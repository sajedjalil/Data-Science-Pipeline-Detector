{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"2e162541-b438-1524-7925-b27b5d57b1fb"},"source":"problems i have is\n\n-tuning ? Am i overfitting ? how do i know ?\n-how do i get the prediction ? see the error below ?"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"af1d2510-716c-7e91-5f81-1c79b72dda86"},"outputs":[],"source":"import numpy as np\nimport pandas as pd\nfrom collections import Counter\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\n# timing function\nimport time   \nstart = time.clock() #_________________ measure efficiency timing\n\ninput_folder='../input/'\ntrain = pd.read_csv(input_folder + 'train.csv',encoding='utf8')[:10000]\ntest  = pd.read_csv(input_folder + 'test.csv',encoding='utf8')[:10000]\n\n# lege opvullen\ntrain.fillna(value='leeg',inplace=True)\ntest.fillna(value='leeg',inplace=True)\n\nprint(\"Original data: trainQ: {}, testQ: {}\".format(train.shape, test.shape) )\nend = time.clock()\nprint('open:',end-start)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3c5d2c42-5a87-99cd-c3b6-665abed8a30e"},"outputs":[],"source":"def cleantxt(x):   \n    # Pad punctuation with spaces on both sides\n    for char in ['.', '\"', ',', '(', ')', '!', '?', ';', ':']:\n        x = x.replace(char, ' ' + char + ' ')\n    return x\n\ntrain['question1']=train['question1'].map(cleantxt)\ntrain['question2']=train['question2'].map(cleantxt)\ntest['question1']=test['question1'].map(cleantxt)\ntest['question2']=test['question2'].map(cleantxt)\n\ntrain_qs = pd.Series(train['question1'].tolist() + train['question2'].tolist())\ntest_qs = pd.Series(test['question1'].tolist() + test['question2'].tolist())\n\ncount_vectorizer = CountVectorizer(binary=True, ngram_range=(1, 2) )\ncount_vectorizer.fit(train_qs.append(test_qs))  #Learn vocabulary and idf, return document freq list.\nprint('lengt dictionary',len(count_vectorizer.vocabulary_))\n\nend = time.clock()\nprint('clean and make freq word dict:',end-start)"},{"cell_type":"markdown","metadata":{"_cell_guid":"ad5ab207-75bd-db41-1d90-c0d543da2aac"},"source":"here i look for equality, differences with tfidf"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"727e3a1d-49cb-8e26-b374-7d90aa77a695"},"outputs":[],"source":"def splitter(dfQ,Dict):\n    eq=[]\n    di1=[]\n    di2=[]\n    for xi in range(0,len(dfQ)):\n        q1words = dfQ.iloc[xi].question1.split()\n        q2words = dfQ.iloc[xi].question2.split()\n        equq1 = [w for w in q1words if w in q2words]\n        difq1 = [w for w in q1words if w not in q2words] \n        difq2 = [w for w in q2words if w not in q1words ]\n        eq.append(' '.join(equq1))\n        di1.append(' '.join(difq1))\n        di2.append(' '.join(difq2))\n    count1_vectorizer = CountVectorizer(vocabulary=Dict, ngram_range=(1, 2),binary=True, min_df=1)\n    count1_vectorizer.fit_transform(dfQ['question1'])  #Learn vocabulary and idf, return term-document matrix.\n    freq1_term_matrix = count_vectorizer.transform(dfQ['question1'])\n    count2_vectorizer = CountVectorizer(vocabulary=Dict, ngram_range=(1, 2),binary=True, min_df=1)\n    count2_vectorizer.fit_transform(dfQ['question2'])\n    freq2_term_matrix = count_vectorizer.transform(dfQ['question2']) #Transform documents to document-term matrix. Uses the vocabulary and document frequencies (df) learned by fit (or fit_transform) This is equivalent to fit followed by transform\n    count3_vectorizer = CountVectorizer(vocabulary=Dict, ngram_range=(1, 2),binary=True, min_df=1)\n    count3_vectorizer.fit_transform(di1)\n    freq3_term_matrix = count_vectorizer.transform(di1) #Transform documents to document-term matrix. Uses the vocabulary and document frequencies (df) learned by fit (or fit_transform) This is equivalent to fit followed by transform\n    count4_vectorizer = CountVectorizer(vocabulary=Dict, ngram_range=(1, 2),binary=True, min_df=1)\n    count4_vectorizer.fit_transform(di2)\n    freq4_term_matrix = count_vectorizer.transform(di2) #Transform documents to document-term matrix. Uses the vocabulary and document frequencies (df) learned by fit (or fit_transform) This is equivalent to fit followed by transform\n    count5_vectorizer = CountVectorizer(vocabulary=Dict, ngram_range=(1, 2),binary=True, min_df=1)\n    count5_vectorizer.fit_transform(eq)\n    freq5_term_matrix = count_vectorizer.transform(eq) #Transform documents to document-term matrix. Uses the vocabulary and document frequencies (df) learned by fit (or fit_transform) This is equivalent to fit followed by transform\n    tfidf1 = TfidfTransformer(norm=\"l2\")\n    tf1_idf_matrix = tfidf1.fit_transform(freq1_term_matrix)\n    tfidf2 = TfidfTransformer(norm=\"l2\")\n    tf2_idf_matrix = tfidf2.fit_transform(freq2_term_matrix)\n    tfidf3 = TfidfTransformer(norm=\"l2\")\n    tf3_idf_matrix = tfidf3.fit_transform(freq3_term_matrix)\n    tfidf4 = TfidfTransformer(norm=\"l2\")\n    tf4_idf_matrix = tfidf4.fit_transform(freq4_term_matrix)\n    tfidf5 = TfidfTransformer(norm=\"l2\")\n    tf5_idf_matrix = tfidf5.fit_transform(freq5_term_matrix)\n    corr1=tf1_idf_matrix[:].dot(tf2_idf_matrix[:].T).diagonal().round(2)\n    corr2=tf1_idf_matrix[:].dot(tf5_idf_matrix[:].T).diagonal().round(2)\n    corr3=tf2_idf_matrix[:].dot(tf5_idf_matrix[:].T).diagonal().round(2)\n    corr4=tf1_idf_matrix[:].dot(tf3_idf_matrix[:].T).diagonal().round(2)\n    corr5=tf2_idf_matrix[:].dot(tf4_idf_matrix[:].T).diagonal().round(2)\n    tf23e=corr2>corr3\n    tf2345=(corr2+corr3)>(corr4+corr5)\n    tf24=corr2>corr4\n    tf35=corr3>corr5\n    tf145=corr1>(corr4/2+corr5/2)\n    \n    return corr1,corr2,corr3,corr4,corr5,tf23e,tf2345,tf24,tf35,tf145\n\n\ntrain['corr1'],train['corr2'],train['corr3'],train['corr4'],train['corr5'],train['tf23e'],train['tf2345'],train['tf24'],train['tf35'],train['tf145']=splitter(train,count_vectorizer.vocabulary_)\n    \nprint(train.head(10))\n\ntest['corr1'],test['corr2'],test['corr3'],test['corr4'],test['corr5'],test['tf23e'],test['tf2345'],test['tf24'],test['tf35'],test['tf145']=splitter(test,count_vectorizer.vocabulary_)\n\nprint(test.head(10))\n\nend = time.clock()\nprint('tfidf - corr:',end-start)         \n\n    \n    "},{"cell_type":"markdown","metadata":{"_cell_guid":"7125b66f-c218-7ad6-6a4a-8bc8856181c1"},"source":"here i have the problem with xgboost optimalisation \nHow do i get that prediction working ?"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"80b83d7a-7a33-e658-7cff-6140eb7089ba"},"outputs":[],"source":"import xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn import cross_validation, metrics\nimport matplotlib.pylab as plt\n%matplotlib inline\nfrom matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 12, 4\n\ndef modelfit(alg, dtrain, predictors,predlabel,useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n    \n    if useTrainCV:\n        xgb_param = alg.get_xgb_params()\n        xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain[predlabel].values)\n        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n            metrics='auc', early_stopping_rounds=early_stopping_rounds)\n        alg.set_params(n_estimators=cvresult.shape[0])\n    \n    #Fit the algorithm on the data\n    alg.fit(dtrain[predictors], dtrain[predlabel],eval_metric='auc')\n        \n    #Predict training set:\n    dtrain_predictions = alg.predict(dtrain[predictors])\n    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n        \n    #Print model report:\n    print(\"\\nModel Report\" )\n    print(\"Accuracy : %.4g\" % metrics.accuracy_score(dtrain[predlabel].values, dtrain_predictions) )\n    print(\"AUC Score (Train): %f\" % metrics.roc_auc_score(dtrain[predlabel], dtrain_predprob) )\n                    \n    feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n    feat_imp.plot(kind='bar', title='Feature Importances')\n    plt.ylabel('Feature Importance Score')\n    return alg\n\npredictors = [x for x in train.columns if x not in ['id','question1','question2','is_duplicate','qid1','qid2']]\n\nfor di in range (5,20,3):\n    # Set our parameters for xgboost\n    for ci in range (1,2,2):\n    # Set our parameters for xgboost\n        print('maxdepth',di,'minchild',ci)\n        xgb1 = XGBClassifier(\n learning_rate =0.1, n_estimators=1000, max_depth=di, min_child_weight=ci,\n gamma=0, subsample=0.8, colsample_bytree=0.8, objective= 'binary:logistic',\n nthread=4, scale_pos_weight=1, seed=27)\n        xgbmodel=modelfit(xgb1,train,predictors,'is_duplicate')\n\ntestcolumn = [x for x in test.columns if x not in ['id','question1','question2']]   \n#print(test[testcolumn])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a5ac5fe0-7b7c-bbc4-9078-d7d805ba7d6c"},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b4df90c6-527c-ed18-a975-c4d3acdb4d30"},"outputs":[],"source":"corrcolumns = [x for x in train.columns if x not in ['question1','question2']]\n\ncorr_mat=train[corrcolumns].corr()\ncorr_mat.head(15)\n\n"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}