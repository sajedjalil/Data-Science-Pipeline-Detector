{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c21e08ac-8968-e25e-8514-420043feca4e"},"outputs":[],"source":"import numpy as np \nimport pandas as pd \nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import log_loss\nimport scipy\nimport xgboost as xgb\nimport difflib\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.metrics import jaccard_distance\n\n\n#Reading and processing of data\ntrain=pd.read_csv('../input/train.csv')[:10000].fillna(\"\")\n#train=pd.read_csv('../input/train.csv').dropna()\nstops = set(stopwords.words(\"english\"))\ny=train['is_duplicate']\ntrain=train.drop(['id', 'qid1', 'is_duplicate','qid2'], axis=1)\n\n#Cleaning up the data\n#Removing ? mark and non ASCII characters\ndef cleanup(data):\n    data['question1'] = data['question1'].apply(lambda x: x.rstrip('?'))\n    data['question2'] = data['question2'].apply(lambda x: x.rstrip('?'))\n    # Removing non ASCII chars\n    data['question1']=data['question1'].apply(lambda x: x.replace(r'[^\\x00-\\x7f]',r' '))\n    data['question2']=data['question2'].apply(lambda x: x.replace(r'[^\\x00-\\x7f]',r' ')) \n    # Pad punctuation with spaces on both sides\n    '''\n    for char in ['.', '\"', ',', '(', ')', '!', '?', ';', ':']:\n        x = x.replace(char, ' ' + char + ' ')\n    '''\n    contractions = {\n      \"ain't\": \"am not\",  \"aren't\": \"are not\",  \"can't\": \"cannot\",  \"can't've\": \"cannot have\",  \"'cause\": \"because\",  \"could've\": \"could have\",  \"couldn't\": \"could not\",\n      \"couldn't've\": \"could not have\",  \"didn't\": \"did not\",  \"doesn't\": \"does not\",  \"don't\": \"do not\",  \"hadn't\": \"had not\",  \"hadn't've\": \"had not have\",\"hasn't\": \"has not\",  \"haven't\": \"have not\",  \"he'd\": \"he would\",  \"he'd've\": \"he would have\",  \"he'll\": \"he will\",  \"he'll've\": \"he will have\",  \"he's\": \"he is\",  \"how'd\": \"how did\",\n      \"how'd'y\": \"how do you\",  \"how'll\": \"how will\",  \"how's\": \"how is\",  \"I'd\": \"I would\",  \"I'd've\": \"I would have\",  \"I'll\": \"I will\",  \"I'll've\": \"I will have\",  \"I'm\": \"I am\",\n      \"I've\": \"I have\",  \"isn't\": \"is not\",  \"it'd\": \"it had\",  \"it'd've\": \"it would have\",  \"it'll\": \"it will\",  \"it'll've\": \"it will have\",  \"it's\": \"it is\",  \"let's\": \"let us\",\"ma'am\": \"madam\",  \"mayn't\": \"may not\",  \"might've\": \"might have\",  \"mightn't\": \"might not\",  \"mightn't've\": \"might not have\",  \"must've\": \"must have\",  \"mustn't\": \"must not\",\n      \"mustn't've\": \"must not have\",  \"needn't\": \"need not\",  \"needn't've\": \"need not have\",  \"o'clock\": \"of the clock\",  \"oughtn't\": \"ought not\",  \"oughtn't've\": \"ought not have\",      \"shan't\": \"shall not\",  \"sha'n't\": \"shall not\",  \"shan't've\": \"shall not have\",  \"she'd\": \"she would\",  \"she'd've\": \"she would have\",  \"she'll\": \"she will\",  \"she'll've\": \"she will have\",\n      \"she's\": \"she is\",  \"should've\": \"should have\",  \"shouldn't\": \"should not\",  \"shouldn't've\": \"should not have\",  \"so've\": \"so have\",  \"so's\": \"so is\",  \"that'd\": \"that would\",  \"that'd've\": \"that would have\",\n      \"that's\": \"that is\",  \"there'd\": \"there had\",  \"there'd've\": \"there would have\",  \"there's\": \"there is\",  \"they'd\": \"they would\",  \"they'd've\": \"they would have\",      \"they'll\": \"they will\",  \"they'll've\": \"they will have\",  \"they're\": \"they are\",  \"they've\": \"they have\",  \"to've\": \"to have\",  \"wasn't\": \"was not\",  \"we'd\": \"we had\",  \"we'd've\": \"we would have\",\n      \"we'll\": \"we will\",  \"we'll've\": \"we will have\",  \"we're\": \"we are\",  \"we've\": \"we have\",  \"weren't\": \"were not\",  \"what'll\": \"what will\",  \"what'll've\": \"what will have\",      \"what're\": \"what are\",  \"what's\": \"what is\",  \"what've\": \"what have\",  \"when's\": \"when is\",  \"when've\": \"when have\",  \"where'd\": \"where did\",  \"where's\": \"where is\",  \"where've\": \"where have\",\n      \"who'll\": \"who will\",  \"who'll've\": \"who will have\",  \"who's\": \"who is\",  \"who've\": \"who have\",  \"why's\": \"why is\",  \"why've\": \"why have\",  \"will've\": \"will have\",  \"won't\": \"will not\",      \"won't've\": \"will not have\",  \"would've\": \"would have\",  \"wouldn't\": \"would not\",  \"wouldn't've\": \"would not have\",  \"y'all\": \"you all\",  \"y'alls\": \"you alls\",  \"y'all'd\": \"you all would\",  \"y'all'd've\": \"you all would have\",\n      \"y'all're\": \"you all are\",  \"y'all've\": \"you all have\",  \"you'd\": \"you had\",  \"you'd've\": \"you would have\",  \"you'll\": \"you you will\",  \"you'll've\": \"you you will have\",  \"you're\": \"you are\",  \"you've\": \"you have\"\n      }\n    contractions_re = re.compile('(%s)' % '|'.join(contractions.keys()))\n    def expand_contractions(s, contractions_dict=contractions):\n        def replace(match):\n            return contractions_dict[match.group(0)]\n        return contractions_re.sub(replace, s)     \n    data['question1']=data['question1'].apply(lambda x: expand_contractions(x.lower()) )\n    data['question2']=data['question2'].apply(lambda x: expand_contractions(x.lower()) )\n    return data\ntrain=cleanup(train)\n\ndef build_dict(sentences):\n    #Dictionary of train words --> word index: word freq\n    print('Building dictionary using train words..')\n    wordcount = dict()\n    #For each worn in each sentence, cummulate frequency\n    for ss in sentences:\n        for w in ss:\n            if w not in wordcount:\n                wordcount[w] = 1\n            else:\n                wordcount[w] += 1    \n    worddict = dict()\n    for idx, w in enumerate(sorted(wordcount.items(), key = lambda x: x[1], reverse=True)):\n        worddict[w[0]] = idx+2  # leave 0 and 1 (UNK)\n    return worddict, wordcount\n\ndef generate_sequence(sentences, dictionary):\n    seqs = [None] * len(sentences)\n    for idx, ss in enumerate(sentences):\n        seqs[idx] = [dictionary[w] if w in dictionary else 1 for w in ss]\n    return seqs\n\ndef tokenize(x):\n    return x.lower().split()\n\nquestions = train['question1'].tolist() + train['question2'].tolist()\ntok_questions = [tokenize(s) for s in questions]\nworddict, wordcount = build_dict(tok_questions)\nprint(np.sum(list(wordcount.values())), ' total words ', len(worddict), ' unique words')\n\n#Metrics for sentence comparison\ndef jc(x):\n    return jaccard_distance(set(x['Q1seq']),set(x['Q2seq']))\n\ndef cosine_d(x):\n    a = set(x['Q1seq'])\n    b = set(x['Q2seq'])\n    d = len(a)*len(b)\n    if (d == 0):\n        return 0\n    else: \n        return len(a.intersection(b))/d\n    \ndef diff_ratios(st1, st2):\n    seq = difflib.SequenceMatcher()\n    seq.set_seqs(str(st1).lower(), str(st2).lower())\n    return seq.quick_ratio()\n\ndef word_match_share(row):\n    q1words = {}\n    q2words = {}\n    for word in str(row['question1']).lower().split():\n        if word not in stops:\n            q1words[word] = 1\n    for word in str(row['question2']).lower().split():\n        if word not in stops:\n            q2words[word] = 1\n    if len(q1words) == 0 or len(q2words) == 0:\n        return 0\n    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n    R = (len(shared_words_in_q1) + len(shared_words_in_q2))/(len(q1words) + len(q2words))\n    return R\n\ndef ngrams_split(lst, n):\n    counts = dict()\n    grams = [''.join(lst[i:i+n]) for i in range(len(lst)-n)]\n    return grams\n\ndef intersect3(x,y):    \n    return set(x).intersection(y)\n\ndef edit_distance(s1, s2):\n    m=len(s1)+1\n    n=len(s2)+1\n\n    tbl = {}\n    for i in range(m): tbl[i,0]=i\n    for j in range(n): tbl[0,j]=j\n    for i in range(1, m):\n        for j in range(1, n):\n            cost = 0 if s1[i-1] == s2[j-1] else 1\n            tbl[i,j] = min(tbl[i, j-1]+1, tbl[i-1, j]+1, tbl[i-1, j-1]+cost)\n\n    return tbl[i,j]\n\ndef leve3(string_1, string_2):\n    len_1 = len(ngrams_split(string_1,3)) + 1\n    len_2 = len(ngrams_split(string_2,3)) + 1\n    d=[0]\n    if len_1>3 and len_2>3:\n        d = [0] * (len_1 * len_2)\n\n        for i in range(len_1):\n            d[i] = i\n        for j in range(len_2):\n            d[j * len_1] = j\n\n        for j in range(1, len_2):\n            for i in range(1, len_1):\n                if string_1[i - 3] == string_2[j - 3]:\n                    d[i + j * len_1] = d[i - 1 + (j - 1) * len_1]\n                else:\n                    d[i + j * len_1] = min(\n                       d[i - 1 + j * len_1] + 1,        # deletion\n                       d[i + (j - 1) * len_1] + 1,      # insertion\n                       d[i - 1 + (j - 1) * len_1] + 1,  # substitution\n                    )\n\n    return d[-1]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ba8a9983-f012-0c33-1d79-b00c13becf16"},"outputs":[],"source":"from sklearn.metrics.pairwise import cosine_similarity,euclidean_distances,laplacian_kernel,sigmoid_kernel,polynomial_kernel,rbf_kernel\nfrom sklearn.decomposition import TruncatedSVD\n\ndef intersecting(a, b):\n    return ' '.join(list(set(a.split()) & set(b.split())))\n\ndef differencing(a, b):\n    return ' '.join(list(set(a.split()) ^ set(b.split())))\n\n\ntfidf = TfidfVectorizer(stop_words='english', ngram_range=(1, 1))\ntfidf.fit_transform(questions)\n\ndef get_features(df_features):    \n    #Question length\n    print('question lengths....')\n    df_features['Qlen1'] = df_features.question1.map(lambda x: len(str(x)))\n    df_features['Qlen2'] = df_features.question2.map(lambda x: len(str(x)))\n    df_features['diffQlen'] = df_features['Qlen1']-df_features['Qlen2']   \n    print('question dist....')\n    df_features['editdis'] = df_features[['question1','question2']].apply(lambda x: edit_distance(*x), axis=1)\n    df_features['levens'] = df_features[['question1','question2']].apply(lambda x: leve3(*x), axis=1)\n    #Question number of words\n    df_features['Qwords1'] = df_features.question1.map(lambda x: len(str(x).split()))\n    df_features['Qwords2'] = df_features.question2.map(lambda x: len(str(x).split()))\n    df_features['diffQword'] = df_features['Qwords1'] -df_features['Qwords2']\n    df_features['isdup'] = df_features.diffQword.map(lambda x: 1-len(str(x)))\n    print('jaccard...')\n    df_features['Q1seq'] = generate_sequence(df_features['question1'].apply(tokenize),worddict)\n    df_features['Q2seq'] = generate_sequence(df_features['question2'].apply(tokenize),worddict)\n    df_features['jaccard'] = df_features.apply(jc,axis = 1)    \n    print('cosine....')\n    df_features['cosine'] = df_features.apply(cosine_d,axis = 1)        \n    #matching the sequences\n    print('difflib...')\n    df_features['SeqMatchRatio'] = df_features.apply(lambda r: diff_ratios(r.question1, r.question2), axis=1)  #takes long\n    #percentage of common words in both questions\n    print('word match...')    \n    df_features['WordMatch'] = df_features.apply(word_match_share, axis=1, raw=True)    \n\n    df_features['interseq'] = df_features[['question1','question2']].apply(lambda x: intersecting(*x), axis=1)\n    df_features['diffseq'] = df_features[['question1','question2']].apply(lambda x: differencing(*x), axis=1)    \n    print('tfidf...')      \n    question1_tfidf = tfidf.transform(df_features.question1.tolist())  #print(question1_tfidf)  sparse matrix \n    question2_tfidf = tfidf.transform(df_features.question2.tolist())    \n    questionI_tfidf = tfidf.transform(df_features.interseq.tolist())    \n    questionD_tfidf = tfidf.transform(df_features.diffseq.tolist()) \n    print('svd...')\n    svd = TruncatedSVD(n_components=50, n_iter=20, random_state=42)\n    df_features=df_features.join(pd.DataFrame(svd.fit_transform(questionI_tfidf)),how='inner')  \n    \n    svd = TruncatedSVD(n_components=30, n_iter=20, random_state=42)\n    temp=pd.DataFrame(svd.fit_transform(questionD_tfidf))\n    temp.rename(columns=lambda x: str(x)+'_d', inplace=True) #nog eens zoeken omcolumns te renamen\n    df_features=df_features.join(temp,how='inner')    \n    svd = TruncatedSVD(n_components=20, n_iter=20, random_state=42)\n    temp=pd.DataFrame(svd.fit_transform(question1_tfidf))\n    temp.rename(columns=lambda x: str(x)+'_q1', inplace=True) #nog eens zoeken omcolumns te renamen\n    df_features=df_features.join(temp,how='inner') \n    svd = TruncatedSVD(n_components=20, n_iter=20, random_state=42)\n    temp=pd.DataFrame(svd.fit_transform(question2_tfidf))\n    temp.rename(columns=lambda x: str(x)+'_q2', inplace=True) #nog eens zoeken omcolumns te renamen\n    df_features=df_features.join(temp,how='inner') \n    \n    df_features['tfidfCo_Si'] = cosine_similarity(question1_tfidf,question2_tfidf).diagonal().T\n    print('Eucl...')    \n    df_features['tfidfEu_Di'] = euclidean_distances(question1_tfidf,question2_tfidf).diagonal().T  \n    print('Sig...')      \n    df_features['tfidfSi_Ke'] = 1-sigmoid_kernel(question1_tfidf,question2_tfidf).diagonal().T  \n    print('rbf...')      \n    df_features['tfidfrbf_Ke'] = rbf_kernel(question1_tfidf,question2_tfidf).diagonal().T*1000-1000       \n    print('poly...')      \n    df_features['tfidfpol_Ke'] = polynomial_kernel(question1_tfidf,question2_tfidf).diagonal().T*1000-1000           \n    #Exactly same questions\n    df_features['exactly_same'] = (df_features['question1'] == df_features['question2']).astype(int)\n    return df_features.fillna(0.0)\n\ndf_train = get_features(train)\nfeats = df_train.columns.values.tolist()\nfeats=[x for x in feats if x not in ['question1','question2','Q1seq','Q2seq','interseq', 'diffseq','id','qid1','qid2','is_duplicate']]\nprint(\"features\",feats)\nprint(df_train.head())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ad3edd8d-0449-3f50-8e1f-f63f6d506b80"},"outputs":[],"source":"x_train, x_valid, y_train, y_valid = train_test_split(df_train[feats], y, test_size=0.3, random_state=0)\n#XGBoost model\nparams = {\"objective\":\"binary:logistic\",'eval_metric':'logloss',\"eta\": 0.11,\n          \"subsample\":0.7,\"min_child_weight\":1,\"colsample_bytree\": 0.7,\n          \"max_depth\":5,\"silent\":1,\"seed\":2017}\n\nd_train = xgb.DMatrix(x_train, label=y_train)\nd_valid = xgb.DMatrix(x_valid, label=y_valid)\nwatchlist = [(d_train, 'train'), (d_valid, 'valid')]\nbst = xgb.train(params, d_train, 2000, watchlist, early_stopping_rounds=200,verbose_eval=25) #change to higher #s\nprint('training done')\n\nprint(\"log loss for training data set\",log_loss(y, bst.predict(xgb.DMatrix(df_train[feats]))))\n#Predicting for test data set\nsub = pd.DataFrame() # Submission data frame\nsub['test_id'] = []\nsub['is_duplicate'] = []\nheader=['test_id','question1','question2','id','qid1','qid2','is_duplicate']\ntest=pd.read_csv('../input/test.csv')[:20000].fillna(\"\")\nprint(\"cleaning test\")\ndf_test=cleanup(test)\nprint(\"feature engineering for test\")\ndf_test = get_features(df_test)\nsub=pd.DataFrame({'test_id':df_test['test_id'], 'is_duplicate':bst.predict(xgb.DMatrix(df_test[feats]))})\nsub.to_csv('quora_submission_xgb_11.csv', index=False)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}