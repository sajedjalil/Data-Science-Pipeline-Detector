{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f3337165-304d-aec5-78c3-5766bcd5b0f4"},"outputs":[],"source":"import pandas as pd\n# timing function\nimport numpy as np\nimport time   \nstart = time.clock() #_________________ measure efficiency timing\n\n# read data\n#test = pd.read_csv('../input/test.csv',encoding='utf8')[:100]\n#test.fillna(value='leeg',inplace=True)\ntrain = pd.read_csv('../input/train.csv',encoding='utf8')[:100]\nprint(train.head(2))\ntrain.fillna(value='leeg',inplace=True)\n\nimport codecs, difflib, Levenshtein\nfrom sklearn.metrics import jaccard_similarity_score\nprint('leven', Levenshtein.ratio(train.iloc[0]['question1'], train.iloc[0]['question2']) )\nprint('diffl', difflib.SequenceMatcher(None, train.iloc[0]['question1'], train.iloc[0]['question2']).ratio() )\nprint('jac', jaccard_similarity_score([10,20,10],[10,20,20],normalize=False) )\n\n\ndef euclidean_distance(x,y):\n    return sqrt(sum(pow(a-b,2) for a, b in zip(x, y)))\ndef manhattan_distance(x,y):\n    return sum(abs(a-b) for a,b in zip(x,y))\n\nprint ( manhattan_distance([10,20,10],[10,20,20]) )\n\nfrom decimal import Decimal\n\ndef nth_root(value, n_root):\n    root_value = 1/float(n_root)\n    return round (Decimal(value) ** Decimal(root_value),3)\n\ndef minkowski_distance(x,y,p_value):\n    return nth_root(sum(pow(abs(a-b),p_value) for a,b in zip(x, y)),p_value)\n\nprint ( minkowski_distance([0,3,4,5],[7,6,3,-1],3) )\ndef square_rooted(x):\n    return round(np.sqrt(sum([a*a for a in x])),3)\n\ndef cosine_similarity(x,y):\n    numerator = sum(a*b for a,b in zip(x,y))\n    denominator = square_rooted(x)*square_rooted(y)\n    \n    return round(numerator/float(denominator),3)\n\nprint ( cosine_similarity([3, 45, 7, 2], [2, 54, 13, 15]) )\n\ndef jaccard_similarity(x,y):\n    intersection_cardinality = len(set.intersection(*[set(x), set(y)]))\n    union_cardinality = len(set.union(*[set(x), set(y)]))\n    return intersection_cardinality/float(union_cardinality)\n\nprint ( jaccard_similarity([0,1,2,5,6],[0,2,3,5,7,9]) )\n\nend = time.clock()\nprint('open:',end-start)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cd9f0e55-1189-f9ec-e47e-fbdafaa45b9b"},"outputs":[],"source":"# -*- coding: utf-8 -*-\nimport numpy as np\nimport pandas as pd\nimport xgboost as xgb\nimport datetime\nimport operator\nfrom sklearn.cross_validation import train_test_split\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nimport matplotlib.pyplot as plt\nfrom pylab import plot, show, subplot, specgram, imshow, savefig\nfrom nltk import word_tokenize\nfrom scipy.spatial import distance\n\nRS = 12357\nROUNDS = 315\n\nprint(\"Started\")\nnp.random.seed(RS)\ninput_folder = '../input/'\n\ndef find_bigrams(input_list):\n    lijst=list(zip(input_list, input_list[1:]))\n    #remove stopwords on second position, do not remove starting stopwords\n    lijst2=[tup for tup in lijst if tup[1] not in stopwords.words('english')]\n    return lijst2\n\ndef BiJaccard(str1, str2):\n    bigram1=set(find_bigrams(str1))\n    bigram2=set(find_bigrams(str2))\n    #print(bigram1,bigram2)\n    return float( len( set(bigram1).intersection(bigram2) ) / len( set(bigram1).union(bigram2)) ) \n\ndef jaro_winkler(ying, yang): #, long_tolerance, winklerize):\n    winklerize=True\n    long_tolerance=True\n    ying_len = len(ying)\n    yang_len = len(yang)\n\n    if not ying_len or not yang_len:\n        return 0.0\n\n    min_len = max(ying_len, yang_len)\n    search_range = (min_len // 2) - 1\n    if search_range < 0:\n        search_range = 0\n\n    ying_flags = [False]*ying_len\n    yang_flags = [False]*yang_len\n\n    # looking only within search range, count & flag matched pairs\n    common_chars = 0\n    for i, ying_ch in enumerate(ying):\n        low = i - search_range if i > search_range else 0\n        hi = i + search_range if i + search_range < yang_len else yang_len - 1\n        for j in range(low, hi+1):\n            if not yang_flags[j] and yang[j] == ying_ch:\n                ying_flags[i] = yang_flags[j] = True\n                common_chars += 1\n                break\n\n    # short circuit if no characters match\n    if not common_chars:\n        return 0.0\n\n    # count transpositions\n    k = trans_count = 0\n    for i, ying_f in enumerate(ying_flags):\n        if ying_f:\n            for j in range(k, yang_len):\n                if yang_flags[j]:\n                    k = j + 1\n                    break\n            if ying[i] != yang[j]:\n                trans_count += 1\n    trans_count /= 2\n\n    # adjust for similarities in nonmatched characters\n    common_chars = float(common_chars)\n    weight = ((common_chars/ying_len + common_chars/yang_len +\n              (common_chars-trans_count) / common_chars)) / 3\n\n    # winkler modification: continue to boost if strings are similar\n    if winklerize and weight > 0.7 and ying_len > 3 and yang_len > 3:\n        # adjust for up to first 4 chars in common\n        j = min(min_len, 4)\n        i = 0\n        while i < j and ying[i] == yang[i] and ying[i]:\n            i += 1\n        if i:\n            weight += i * 0.1 * (1.0 - weight)\n\n        # optionally adjust for long strings\n        # after agreeing beginning chars, at least two or more must agree and\n        # agreed characters must be > half of remaining characters\n        if (long_tolerance and min_len > 4 and common_chars > i+1 and\n                2 * common_chars >= min_len + i):\n            weight += ((1.0 - weight) * (float(common_chars-i-1) / float(ying_len+yang_len-i*2+2)))\n\n    return weight\n\ndef train_xgb(X, y, params):\n\tprint(\"Will train XGB for {} rounds, RandomSeed: {}\".format(ROUNDS, RS))\n\tx, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=RS)\n\n\txg_train = xgb.DMatrix(x, label=y_train)\n\txg_val = xgb.DMatrix(X_val, label=y_val)\n\n\twatchlist  = [(xg_train,'train'), (xg_val,'eval')]\n\treturn xgb.train(params, xg_train, ROUNDS, watchlist)\n\ndef predict_xgb(clr, X_test):\n\treturn clr.predict(xgb.DMatrix(X_test))\n\ndef create_feature_map(features):\n\toutfile = open('xgb.fmap', 'w')\n\ti = 0\n\tfor feat in features:\n\t\toutfile.write('{0}\\t{1}\\tq\\n'.format(i, feat))\n\t\ti = i + 1\n\toutfile.close()\n\ndef add_word_count(x, df, word):\n\tx['q1_' + word] = df['question1'].apply(lambda x: (word in str(x).lower())*1)\n\tx['q2_' + word] = df['question2'].apply(lambda x: (word in str(x).lower())*1)\n\tx[word + '_both'] = x['q1_' + word] * x['q2_' + word]\n\n#starting parameters\nparams = {}\nparams['objective'] = 'binary:logistic'\nparams['eval_metric'] = 'logloss'\nparams['eta'] = 0.11\nparams['max_depth'] = 5\nparams['silent'] = 1\nparams['seed'] = RS\n\ndf_train = pd.read_csv(input_folder + 'train.csv',encoding='utf8') [:50000]\ndf_test  = pd.read_csv(input_folder + 'test.csv',encoding='utf8')  [:50000]\ndf_train.fillna(value='leeg',inplace=True)\ndf_test.fillna(value='leeg',inplace=True)\nprint(\"Original data: X_train: {}, X_test: {}\".format(df_train.shape, df_test.shape))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"efd13edb-63b7-92c9-c26f-3abaad5218e6"},"outputs":[],"source":"\nprint(\"Features processing, be patient...\")\n\n\t# If a word appears only once, we ignore it completely (likely a typo)\n\t# Epsilon defines a smoothing constant, which makes the effect of extremely rare words smaller\ndef get_weight(count, eps=10000, min_count=2):\n\treturn 0 if count < min_count else 1 / (count + eps)\n    \n    \ntrain_qs = pd.Series(df_train['question1'].tolist() + df_train['question2'].tolist()).astype(str)\nwords = (\" \".join(train_qs)).lower().split()\ncounts = Counter(words)\nweights = {word: get_weight(count) for word, count in counts.items()}\n\nstops = set(stopwords.words(\"english\"))\ndef word_shares(row):\n\n    q1_list = word_tokenize(row['question1'])\n    q1 = set(q1_list)\n    q1words = q1.difference(stops)\n    if len(q1words) == 0:\n        return '0:0:0:0:0:0:0:0:0:0:0'\n        print(row['id'])\n        \n    q2_list = word_tokenize(row['question2']) #.lower().split()\n    q2 = set(q2_list)\n    q2words = q2.difference(stops)\n    if len(q2words) == 0:\n        return '0:0:0:0:0:0:0:0:0:0:0'\n    \n    BiJac=BiJaccard(list(q1),list(q2))\n    JaroWink=jaro_winkler(row['question1'],row['question2'])\n    words_hamming = sum(1 for i in zip(q1_list, q2_list) if i[0]==i[1])/max(len(q1_list), len(q2_list))\n\n    #q1stops = q1.intersection(stops)\n    #q2stops = q2.intersection(stops)\n\n    q1_2gram = set([i for i in zip(q1_list, q1_list[1:])])\n    q2_2gram = set([i for i in zip(q2_list, q2_list[1:])])\n    shared_2gram = q1_2gram.intersection(q2_2gram)\n\n\t\t\n    shared_words = q1words.intersection(q2words)\n    differ_words = q1words.symmetric_difference(q2words)\n\t#total_words  = shared_words+differwords\n    \n    shared_weights = [weights.get(w, 0) for w in shared_words]\n    differ_weights = [weights.get(w, 0) for w in differ_words]\n    q1_weights = [weights.get(w, 0) for w in q1words]\n    q2_weights = [weights.get(w, 0) for w in q2words]\n\t\t\n    total_weights = q1_weights + q1_weights\n\t\t\n    R1 = np.sum(shared_weights) / np.sum(total_weights) #tfidf share\n    R2 = len(shared_words) / (len(q1words) + len(q2words) - len(shared_words)) #count share\n    R31 = len(differ_words) / len(q1words) #stops in q1\n    R32 = len(differ_words) / len(q2words) #stops in q2\n    Rcosine_denominator = (np.sqrt(np.dot(q1_weights,q1_weights))*np.sqrt(np.dot(q2_weights,q2_weights)))\n    Rcosine = np.dot(shared_weights, shared_weights)/Rcosine_denominator\n    Rcosdif = np.dot(differ_weights, differ_weights)/Rcosine_denominator    \n    if len(q1_2gram) + len(q2_2gram) == 0:\n        R2gram = 0\n    else:\n        R2gram = len(shared_2gram) / (len(q1_2gram) + len(q2_2gram))\n    return '{}:{}:{}:{}:{}:{}:{}:{}:{}:{}:{}'.format(R1, R2, len(shared_words), R31, R32, R2gram, Rcosine, words_hamming,BiJac,JaroWink,Rcosdif)\n\ndf = pd.concat([df_train, df_test])\ndf['word_shares'] = df.apply(word_shares, axis=1, raw=True)\nx = pd.DataFrame()\n\nx['word_match']       = df['word_shares'].apply(lambda x: float(x.split(':')[0]))\nx['word_match_2root'] = np.sqrt(x['word_match'])\nx['tfidf_word_match'] = df['word_shares'].apply(lambda x: float(x.split(':')[1]))\nx['shared_count']     = df['word_shares'].apply(lambda x: float(x.split(':')[2]))\n\nx['stops1_ratio']     = df['word_shares'].apply(lambda x: float(x.split(':')[3]))\nx['stops2_ratio']     = df['word_shares'].apply(lambda x: float(x.split(':')[4]))\nx['shared_2gram']     = df['word_shares'].apply(lambda x: float(x.split(':')[5]))\nx['cosine']           = df['word_shares'].apply(lambda x: float(x.split(':')[6]))\nx['words_hamming']    = df['word_shares'].apply(lambda x: float(x.split(':')[7]))\nx['bi_jacq']    = df['word_shares'].apply(lambda x: float(x.split(':')[8]))\nx['Jaro_Wink']    = df['word_shares'].apply(lambda x: float(x.split(':')[9]))\nx['Cosdiff']    = df['word_shares'].apply(lambda x: float(x.split(':')[10]))\nx['diff_stops_r']     = x['stops1_ratio'] - x['stops2_ratio']\n\nx['len_q1'] = df['question1'].apply(lambda x: len(str(x)))\nx['len_q2'] = df['question2'].apply(lambda x: len(str(x)))\nx['diff_len'] = x['len_q1'] - x['len_q2']\n\t\nx['caps_count_q1'] = df['question1'].apply(lambda x:sum(1 for i in str(x) if i.isupper()))\nx['caps_count_q2'] = df['question2'].apply(lambda x:sum(1 for i in str(x) if i.isupper()))\nx['diff_caps'] = x['caps_count_q1'] - x['caps_count_q2']\n\nx['len_char_q1'] = df['question1'].apply(lambda x: len(str(x).replace(' ', '')))\nx['len_char_q2'] = df['question2'].apply(lambda x: len(str(x).replace(' ', '')))\nx['diff_len_char'] = x['len_char_q1'] - x['len_char_q2']\n\nx['len_word_q1'] = df['question1'].apply(lambda x: len(str(x).split()))\nx['len_word_q2'] = df['question2'].apply(lambda x: len(str(x).split()))\nx['diff_len_word'] = x['len_word_q1'] - x['len_word_q2']\n\nx['avg_world_len1'] = x['len_char_q1'] / x['len_word_q1']\nx['avg_world_len2'] = x['len_char_q2'] / x['len_word_q2']\nx['diff_avg_word'] = x['avg_world_len1'] - x['avg_world_len2']\n\nx['exactly_same'] = (df['question1'] == df['question2']).astype(int)\nx['duplicated'] = df.duplicated(['question1','question2']).astype(int)\nadd_word_count(x, df,'how')\nadd_word_count(x, df,'what')\nadd_word_count(x, df,'which')\nadd_word_count(x, df,'who')\nadd_word_count(x, df,'where')\nadd_word_count(x, df,'when')\nadd_word_count(x, df,'why')\n\nprint(x.columns)\nprint(x.describe())\n\nfeature_names = list(x.columns.values)\ncreate_feature_map(feature_names)\nprint(\"Features: {}\".format(feature_names))\n\nx_train = x[:df_train.shape[0]]\nx_test  = x[df_train.shape[0]:]\ny_train = df_train['is_duplicate'].values\ndel x, df_train\n\nif 1: # Now we oversample the negative class - on your own risk of overfitting!\n\tpos_train = x_train[y_train == 1]\n\tneg_train = x_train[y_train == 0]\n\tprint(\"Oversampling started for proportion: {}\".format(len(pos_train) / (len(pos_train) + len(neg_train))))\n\tp = 0.165\n\tscale = ((len(pos_train) / (len(pos_train) + len(neg_train))) / p) - 1\n\twhile scale > 1:\n\t\tneg_train = pd.concat([neg_train, neg_train])\n\t\tscale -=1\n\tneg_train = pd.concat([neg_train, neg_train[:int(scale * len(neg_train))]])\n\tprint(\"Oversampling done, new proportion: {}\".format(len(pos_train) / (len(pos_train) + len(neg_train))))\n\tx_train = pd.concat([pos_train, neg_train])\n\ty_train = (np.zeros(len(pos_train)) + 1).tolist() + np.zeros(len(neg_train)).tolist()\n\tdel pos_train, neg_train\n\t"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1eb6980b-59d2-1766-ff09-95ca44112f71"},"outputs":[],"source":"\tprint(\"Training data: X_train: {}, Y_train: {}, X_test: {}\".format(x_train.shape, len(y_train), x_test.shape))\n\tclr = train_xgb(x_train, y_train, params)\n\tpreds = predict_xgb(clr, x_test)\n\n\tprint(\"Writing output...\")\n\tsub = pd.DataFrame()\n\tsub['test_id'] = df_test['test_id']\n\tsub['is_duplicate'] = preds *.75\n\tsub.to_csv(\"xgb_seed{}_n{}.csv\".format(RS, ROUNDS), index=False)\n\n\tprint(\"Features importances...\")\n\timportance = clr.get_fscore(fmap='xgb.fmap')\n\timportance = sorted(importance.items(), key=operator.itemgetter(1))\n\tft = pd.DataFrame(importance, columns=['feature', 'fscore'])\n\n\tft.plot(kind='barh', x='feature', y='fscore', legend=False, figsize=(10, 25))\n\tplt.gcf().savefig('features_importance.png')"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}