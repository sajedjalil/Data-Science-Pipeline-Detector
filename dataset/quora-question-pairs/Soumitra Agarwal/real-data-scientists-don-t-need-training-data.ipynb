{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"3ec0b941-0147-c12f-0b68-8ac6b329ac9a"},"source":"*This script is based on the earlier conclusions from the script here : https://www.kaggle.com/artimous/d/quora/question-pairs-dataset/deciphering-the-quora-bot*\n\n\nGetting common words\n--------------------\n\nNo syntax and semantics analysis here. Going simple to analyse all the common words in the two question sets and visualizing them."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"299e6b9d-c7d1-0809-bb3d-ac116ebccd3b"},"outputs":[],"source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\ntry:\n    t_file = pd.read_csv('../input/test.csv', encoding='ISO-8859-1')\n    tr_file = pd.read_csv('../input/train.csv', encoding ='ISO-8859-1')\n    print('File load: Success')\nexcept:\n    print('File load: Failed')"},{"cell_type":"markdown","metadata":{"_cell_guid":"2effab24-1144-513d-b255-59bd1b8a7068"},"source":"**Removing stop words from ntlk**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7d036380-4b13-f06c-d918-5df92c784e82"},"outputs":[],"source":"from nltk.corpus import stopwords\nstop = stopwords.words('english')\nprint(stop)"},{"cell_type":"markdown","metadata":{"_cell_guid":"cd311de6-c227-215d-ae9c-80e2528858d3"},"source":"CLEANING\n--------\n\nSimply dropped null values, split each of the question strings and removed stops"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"60810b63-7e3e-46b2-ba6f-0c9a9f1853f1"},"outputs":[],"source":"t_file = t_file.dropna()\nt_file['question1'] = t_file['question1'].str.lower().str.split()\nt_file['question2'] = t_file['question2'].str.lower().str.split()\nt_file['question1'] = t_file['question1'].apply(lambda x: [item for item in x if item not in stop])\nt_file['question2'] = t_file['question2'].apply(lambda x: [item for item in x if item not in stop])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c33b8643-9135-6a15-e976-d8c7ae7da47a"},"outputs":[],"source":"tr_file = tr_file.dropna()\ntr_file['question1'] = tr_file['question1'].str.lower().str.split()\ntr_file['question2'] = tr_file['question2'].str.lower().str.split()\ntr_file['question1'] = tr_file['question1'].apply(lambda x: [item for item in x if item not in stop])\ntr_file['question2'] = tr_file['question2'].apply(lambda x: [item for item in x if item not in stop])"},{"cell_type":"markdown","metadata":{"_cell_guid":"c44a5b29-2111-d0ab-4063-22728357c78f"},"source":"**Finding common word percentage and average word lengths**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bf08abe7-a8a9-2441-4537-28436e4d56b6"},"outputs":[],"source":"tr_file['Common'] = tr_file.apply(lambda row: len(list(set(row['question1']).intersection(row['question2']))), axis=1)\ntr_file['Average'] = tr_file.apply(lambda row: 0.5*(len(row['question1'])+len(row['question2'])), axis=1)\ntr_file['Percentage'] = tr_file.apply(lambda row: row['Common']*100.0/(row['Average']+1), axis=1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8c78fccf-b8ba-810c-3024-ce8e190a44ad"},"outputs":[],"source":"t_file['Common'] = t_file.apply(lambda row: len(list(set(row['question1']).intersection(row['question2']))), axis=1)\nt_file['Average'] = t_file.apply(lambda row: 0.5*(len(row['question1'])+len(row['question2'])), axis=1)\nt_file['Percentage'] = t_file.apply(lambda row: 1 if row['Average'] == 0 else row['Common']/(row['Average']), axis=1)"},{"cell_type":"markdown","metadata":{"_cell_guid":"94650a8a-3187-3431-eb4a-8e2a19c5471a"},"source":"**True and False plotting of data**"},{"cell_type":"markdown","metadata":{"_cell_guid":"36a531be-a15d-8d6e-d602-23a1181a4bd3"},"source":"Cheating the title\n------------------\n\nWe can take a look at the training file right? No training still counts as good work? "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dc6e0b1e-6d8d-86b4-96f7-c5ce749b2b63"},"outputs":[],"source":"y = tr_file['Percentage'][tr_file['is_duplicate']==0].values\nx = tr_file['Average'][tr_file['is_duplicate']==0].values\n\nfig, axs = plt.subplots(ncols=2, sharey=True, figsize=(7, 4))\nfig.subplots_adjust(hspace=0.5, left=0.07, right=0.93)\nax = axs[0]\nhb = ax.hexbin(x, y, gridsize=70, bins='log', cmap='inferno')\nax.axis([0, 20, 0, 100])\nax.set_title(\"Duplicates\")\ncb = fig.colorbar(hb, ax=ax)\ncb.set_label('log10(N)')\n\n\ny = tr_file['Percentage'][tr_file['is_duplicate']==1].values\nx = tr_file['Average'][tr_file['is_duplicate']==1].values\nax = axs[1]\nhb = ax.hexbin(x, y, gridsize=70, bins='log', cmap='inferno')\nax.axis([0, 20, 0, 100])\nax.set_title(\"Not duplicates\")\ncb = fig.colorbar(hb, ax=ax)\ncb.set_label('log10(N)')\n\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"0e9e5251-5720-c99d-f5cc-aa317d587118"},"source":"**Scatters are a must**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5e2ee5b6-82c8-9b28-ab77-46e9123aeb5d"},"outputs":[],"source":"x = tr_file['Percentage'][tr_file['is_duplicate']==0].values\ny = tr_file['qid1'][tr_file['is_duplicate']==0].values\narea = tr_file['Average'][tr_file['is_duplicate']==0].values\n\nplt.scatter(x, y, s=area*3, c='r', alpha=0.1)\n\nx = tr_file['Percentage'][tr_file['is_duplicate']==1].values\ny = tr_file['qid1'][tr_file['is_duplicate']==1].values\narea = tr_file['Average'][tr_file['is_duplicate']==1].values\n\nplt.scatter(x, y, s=area*3, c='b', alpha=0.1)\n\nplt.ylabel('Question IDs')\nplt.xlabel('Percentage of common words')\n\nplt.title(\"Percentages of common words in questions\")\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"119d261f-8936-e14b-9da6-7dfea623b986"},"source":"**Observations**\n----------------\n\nFrom the final plot it is pretty clearly visible that the ones that are clustered towards the 100% mark are nearly all blue. This states the fact that questions having a lot of common strings are termed as equivalent more often than not.\nAlso as seen in the hex plot, non duplicates are clustered towards the 100% area more than the duplicate ones.\nHave we decoded the Quora bot? Not at all."},{"cell_type":"markdown","metadata":{"_cell_guid":"f38f8dd3-b3de-894a-0b10-39de25002ee0"},"source":"The final step\n--------------\n\nPenning down the final result into and output file. This approach is just naive."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5d3ebc3a-5d37-8478-6eb1-df860d7a33d4"},"outputs":[],"source":"df2 = pd.DataFrame({'test_id' : range(0,2345796)})\ndf2['is_duplicate']=pd.Series(t_file['Percentage'])\ndf2.fillna(0, inplace = True)\nprint(df2.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c403481e-82d5-4e19-5629-f00d74a506a2"},"outputs":[],"source":"df2.to_csv('submit_naive.csv', index=False)"},{"cell_type":"markdown","metadata":{"_cell_guid":"034b3dba-ea12-9990-e8d8-5d83cfa36070"},"source":" Cosine, Jaccard and Shingling\n--------------------------------\n\nThe first, naive approach towards identifying question pairs -- Strip the stopwords, stem the remaining and do a simple Cosine/Jaccard Test. K-Shingling is also a popular technique, where continuous subsets of \"k\" words are matched between the two documents.\nHowever, a major drawback with the above is that of a lack of semantic understanding -- There might be two questions with a high percentage of common words, but different meanings."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"acbce3bf-b76a-455a-97d2-13b79abdbd22"},"outputs":[],"source":"from collections import Counter\n\ndef get_cosine(vec1, vec2):\n    vec1 = Counter(vec1)\n    vec2 = Counter(vec2)\n    intersection = set(vec1.keys()) & set(vec2.keys())\n    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n    sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n    sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n    if not denominator:\n        return 0.0\n    else:\n        return float(numerator) / denominator"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"135c6d9c-5a27-3f97-9397-7a0be548ac7b"},"outputs":[],"source":"t_file['Cosine'] = t_file.apply(lambda row: get_cosine(row['question1'],row['question2']), axis=1)\nprint(t_file)"},{"cell_type":"markdown","metadata":{"_cell_guid":"e9640aab-7bdc-c0e3-c846-13a6d58e5ee5"},"source":"Jaccard Similarity\n------------------\n\nJaccard Similarity is given by s=p/(p+q+r)\nwhere,\n\n- p = # of attributes positive for both objects \n- q = # of attributes 1 for i and 0 for j \n- r = # of attributes 0 for i and 1 for j \n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5c2b3207-03ff-0376-2970-aea5397ea993"},"outputs":[],"source":"t_file['Jaccard'] = t_file.apply(lambda row: float(row['Common'])/((len(row['question1'])+len(row['question2'])-row['Common'])), axis=1)\nprint(t_file)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}