{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"147802a5-1f0c-80da-6ca5-cc2b69218b5e"},"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2b54c7f9-54a8-9b24-c068-616de394ed2f"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport gc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\nimport gensim, logging\nfrom gensim.models import word2vec\n\nstops = set(stopwords.words(\"english\"))\nsimbolos = ['.','.','...','@','$','(',')','\"',':',';','?']\n%matplotlib inline\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\npal = sns.color_palette()\nfrom subprocess import check_output\n#print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"37956a50-b4b5-e362-1245-e8e517b256cc"},"outputs":[],"source":"df_train = pd.read_csv('../input/train.csv')\ndf_test = pd.read_csv('../input/test.csv')\ndf_part = df_train[0:2500]\nlen(df_train)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e0929273-838d-6aa5-ea25-03e9de557de8"},"outputs":[],"source":"#comparador de palabras y procesado simple de texto: tokenizador, filtrado(lower y alfanumerico) \n# y limpieza(no stops) \n#probar despues con el stemer y el lemmatizador\n\ndef same_word_ratio_2(row):\n    \n    ratiow = []\n    #question1 = row['question1'].values.tolist()\n    #question2 = row['question2'].values.tolist()\n    for i in range(len(row)):\n        n_words = 0     \n        q1 = (pd.Series(row['question1'][i]).astype(str))[0]\n        q2 = (pd.Series(row['question2'][i]).astype(str))[0] \n        if q2 == []: \n            q2 =\"\"\n        if q1 == []:\n            q1 =\"\" \n        q_tokens1 = word_tokenize(q1) #tokenizar frase por frase, las frases con sus tokens separados\n        q_tokens2 = word_tokenize(q2)\n        \n        filtered_tokens1 = [token.lower() for token in q_tokens1 if token.isalnum()]#para limpiar las frases(minusculas y alfanumeric)\n        filtered_tokens2 = [token.lower() for token in q_tokens2 if token.isalnum()]\n        \n        clean_tokens1 = [token for token in filtered_tokens1 if token not in stops] #quitar stopwords\n        clean_tokens2 = [token for token in filtered_tokens2 if token not in stops]\n        \n            #meter lo de word2vec--->>> most_similar() y model.wv.simila\n        \n        if (len(clean_tokens1) + len(clean_tokens2)) == 0:\n            r = len(set(clean_tokens1) & set(clean_tokens2))\n        else:         \n            r = len(set(clean_tokens1) & set(clean_tokens2))/(len(set(clean_tokens1)) + len(set(clean_tokens2)))\n    \n        ratiow.append(r)\n        \n    return ratiow"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ad192701-61eb-28b5-d78f-21e2ef4716cc"},"outputs":[],"source":"def get_clean_tokens(df, question):\n     \n    clean_tokens = []\n    #question1 = row['question1'].values.tolist()\n    #question2 = row['question2'].values.tolist()\n    for i in range(len(df)):    \n        q = (pd.Series(df[question][i]).astype(str))[0] \n        q_tokens = word_tokenize(q) #tokenizar frase por frase, las frases con sus tokens separados        \n        filtered_tokens = [token.lower() for token in q_tokens if token not in simbolos  ]#para limpiar las frases(minusculas y alfanumeric)\n        cleans = [token for token in filtered_tokens if token not in stops] #quitar stopwords\n        clean_tokens.append(cleans)\n        \n        \n    return clean_tokens\n#devuelve una matriz con los clean tokens por frases."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0bd2a658-46dd-6b86-e750-3683cad0e8cd"},"outputs":[],"source":"#same_word_raio_2 reducida->>>>\n\ndef same_word_ratio(df):#df:matriz de datos (df_train)\n    \n    ratiow = []\n    clean_tokens1 = get_clean_tokens(df, 'question1')\n    clean_tokens2 = get_clean_tokens(df, 'question2')\n    for i in range(len(clean_tokens1)):\n        \n        if (len(clean_tokens1[i]) + len(clean_tokens2[i])) == 0:\n            r = len(set(clean_tokens1[i]) & set(clean_tokens2[i]))\n        else:         \n            r = len(set(clean_tokens1[i]) & set(clean_tokens2[i]))/(len(set(clean_tokens1[i])) + len(set(clean_tokens2[i])))\n    \n        ratiow.append(r)\n        \n    return ratiow"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4c7834a5-7f33-3635-d94c-5cd63fba1a33"},"outputs":[],"source":"q1 = (pd.Series(df_part['question1'][42]).astype(str))[0]   \nq2 = (pd.Series(df_part['question2'][42]).astype(str))[0]   \nx = df_part['question1'].values.tolist()\ny = df_part['question2'].values.tolist()\nwords_q1 = (\" \".join(q1)).lower().split()\nprint(x[42])\nq11 = word_tokenize(q1)\nprint(q11)\nfilt1 = [token.lower() for token in q11 if token.isalnum()]\nprint(filt1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b8dc77ed-9918-c715-80fb-b0494b4129af"},"outputs":[],"source":"q_tokens = word_tokenize(q) #tokenizar frase por frase, las frases con sus tokens separados        \nfiltered_tokens = [token.lower() for token in q_tokens if token.isalnum()]#para limpiar las frases(minusculas y alfanumeric)\ncleans = [token for token in filtered_tokens if token not in stops] #quitar stopwords"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e13b4f85-da20-f29f-a819-67a2e0f528ce"},"outputs":[],"source":"clean_tokens1 = get_clean_tokens(df_part, 'question1')\nclean_tokens2 = get_clean_tokens(df_part, 'question2')\n\nprint(clean_tokens1[42])\nprint(clean_tokens2[42])\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"172efc2c-7609-d640-54ed-36c4b1fdb889"},"outputs":[],"source":"num = '50,000'\nif(num.isalnum()):\n    print('siiiiiiiiiiiiiiiiiiiiii')\nelse:\n    print('nooooooooooooooooooo')\nprint(num.lower())\n\nnum.isalnum()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a5bdfdbe-64dd-41a1-9f78-e885f6fe4c6b"},"outputs":[],"source":"#FUNCIÓN DE WORD2VEC\ndef word2vec_sim(df):\n    ratios = []\n    clean1_redu = []\n    clean2_redu = []\n    clean1 = get_clean_tokens(df, 'question1')\n    clean2 = get_clean_tokens(df, 'question2')\n    \n    \n    for c in range(len(clean1)):\n        count = 0\n        sim = 0\n        dif = 0\n        for i in range(len(clean1[c])):\n            for j in range(len(clean2[c])):\n                \n                try:\n                    if(model.similarity(clean1[c][i], clean2[c][j] ) == 1):\n                        clean1.remove(clean1[c][i])\n                        clean2.remove(clean2[c][j])\n\n                except KeyError:\n                    pass \n                    \n        minimo = min(len(clean2[c]), len(clean1[c])) \n        if (minimo == 0):\n            ratio = 0\n            sim = 0\n            dif = 0\n        else:\n            ratio = count / minimo\n            sim = sim /minimo\n            dif = dif / minimo\n        ratios.append(dif)\n        #ratios.append(sim)\n        \n    return clean1 "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"517c689e-2a4d-a15c-dd6c-7abfc77abc64"},"outputs":[],"source":"cleans = word2vec_sim(df_part)\nprint(cleans)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7936fe36-9708-9916-d8a4-c3f157dfa6cd"},"outputs":[],"source":"#pruebas same_word_ratio reducida\nratios = same_word_ratio(df_part) #matriz de ratios para decidir 1 o 0."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4090b083-6a89-78f2-f60d-e3141471963a"},"outputs":[],"source":"print(ratios[53])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"54b3e7a7-b36f-3c12-10df-d8fc4db12767"},"outputs":[],"source":"#PRUEBA PARA EL WORD2VEC\n\"\"\"\ndef word2vec_sim(clean1, clean2):\n    ratios = []\n    count = 0\n    for c in range(len(clean1)):\n        for i in range(len(clean1[c])):\n            for j in range(len(clean2[c])):\n                if(model.wv.similarity(clean1[c][i], clean2[c][j] ) > 0.7):\n                    count = count + 1\n        minimo = min(len(clean2[c]), len(clean1[c])) \n        if (minimo == 0):\n            ratio = 0\n        else:\n            ratio = count / minimo\n            \n        ratios.append(ratio)\n        \n    return ratios \n    \n\"\"\"                "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"54a71d2b-e274-de7c-472a-4e7d0fddb0b0"},"outputs":[],"source":"def decision(matriz_est, umbral):\n    decision = {} #tabla con los datos estimados, duplicados o no.\n    #matriz_est: matriz de \"scores\" de la que se saca una matriz decision (ratiow)\n    for index in range(len(matriz_est)):\n        if (matriz_est[index] > umbral):\n            decision[index] = 1\n        else:\n            decision[index] = 0\n    #print (decision)\n    return decision\n\ndef porcentaje_acierto(matriz_dec, matriz_df): \n    acierto =  {} #tabla para porcentajes de aciertos en la estimación\n    counter = 0\n    for index in range(len(matriz_dec)):\n        if (matriz_dec[index] == matriz_df['is_duplicate'][index]):\n            acierto[index]=1\n            counter = counter +1\n        else:\n            acierto[index]=0\n        \n    porcentaje = (counter*100)/len(matriz_df) \n    return porcentaje"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"158feec3-5940-c891-ab45-2eef2afad7d4"},"outputs":[],"source":"ratiow = same_word_ratio_2(df_train)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e334b164-5d9d-210e-6f80-b4eb2a1341e0"},"outputs":[],"source":"decision = decision(ratiow, 0.25)\nporcentaje = porcentaje_acierto(decision, df_train) \nprint (porcentaje)\n#0.25 es el umbral ideal para df_part"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"68310fd3-cc39-4d72-6dcc-bf9f9f3d976b"},"outputs":[],"source":"x = [0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9]\ny = [0.15,0.16,0.17,0.18,0.19,0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0,3]\nfor w in x:\n    decision = decision(ratiow, w)\n    p = porcentaje_acierto(decision,df_train)\n    print(p)"},{"cell_type":"markdown","metadata":{"_cell_guid":"e551845c-3ab3-94d6-9f72-1eac8e5c822b"},"source":"**FUNCIONES PARA OBTENER LAS NO DETECCIONES Y LAS FALSAS ALARMAS MÁS GRAVES**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"27b2ada1-6475-01ba-eb0c-bf355c8a89c5"},"outputs":[],"source":"\ndef no_detections_graves(ratios, df_x ): #ratios: una matriz de decision con ratios calculados\n                               #df_x: matriz original con las preguntas\n    \n    no_detections = {}\n    no_detections_q = {}\n    no_detections_q1 = {}\n    no_detections_q2 = {}\n    no_detections_q3 = {}\n    counter = 0\n    \n    for index in range(len(ratios)):\n        if ((ratios[index] < 0.15) and (df_x['is_duplicate'][index] == 1)):\n            no_detections[index] = 1\n            #no_detections_q2 = pd.DataFrame({'id': df_x['test_id'][index], 'question1': df_x['question1'][index], 'question2': df_x['question2'][index]})\n            no_detections_q1[index] = df_x['question1'][index]\n            no_detections_q2[index] = df_x['question2'][index]\n            no_detections_q3[index] = ratios[index]\n\n            counter = counter + 1\n        else:\n            no_detections[index] = 0\n            \n    #no_detections_q = pd.DataFrame({'question1': no_detections_q1, 'question2': no_detections_q2, 'ratio': no_detections_q3})   \n    no_detections_q['question1'] = no_detections_q1\n    no_detections_q['question2'] = no_detections_q2\n    no_detections_q['ratio'] = no_detections_q3\n\n    return no_detections_q"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0c040ba5-a819-f2c7-6573-3e687e72e73f"},"outputs":[],"source":"def falsas_alarmas_graves(ratios, df_x ): #ratios: una matriz de decision con ratios calculados\n                               #df_x: matriz original con las preguntas\n    \n    falsa_alarma = {}\n    falsa_alarma_q = {}\n    falsa_alarma_q1 = {}\n    falsa_alarma_q2 = {}\n    falsa_alarma_q3 = {}\n    counter = 0\n    \n    for index in range(len(ratios)):\n        if ((ratios[index] > 0.46) and (df_x['is_duplicate'][index] == 0)):\n            falsa_alarma[index] = 1\n            #falsa_alarma_q2 = pd.DataFrame({'id': df_x['test_id'][index], 'question1': df_x['question1'][index], 'question2': df_x['question2'][index]})\n            falsa_alarma_q1[index] = df_x['question1'][index]\n            falsa_alarma_q2[index] = df_x['question2'][index]\n            falsa_alarma_q3[index] = ratios[index]\n            counter = counter + 1\n        else:\n            falsa_alarma[index]=0\n            \n    #falsa_alarma_q = pd.DataFrame({'question1': falsa_alarma_q1, 'question2': falsa_alarma_q2, 'ratio': falsa_alarma_q3})\n    falsa_alarma_q['question1'] = falsa_alarma_q1\n    falsa_alarma_q['question2'] = falsa_alarma_q2\n    falsa_alarma_q['ratio'] = falsa_alarma_q3\n    return falsa_alarma_q"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"24e12cf6-914e-456c-045b-edd980159f91"},"outputs":[],"source":"no_detections_g = no_detections_graves(ratios , df_part)\nfalsas_alarmas_g = falsas_alarmas_graves(ratios, df_part )"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d0ffaa29-b749-538a-a25f-7e2f2e01d176"},"outputs":[],"source":"#no_detections_g\nfalsas_alarmas_g"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ffc0539f-d9f2-ec8c-d44b-d61946a8c424"},"outputs":[],"source":"ratiow_test = same_word_ratio_2(df_test)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a3c640c7-a751-23d5-4790-f10593601e98"},"outputs":[],"source":"decision_test = decision(ratiow_test, 0.25)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9d27a307-d65e-7d1e-0595-78be82f8474b"},"outputs":[],"source":"decision_test = decision(ratiow_test, 0.25)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"41a11a0f-d6ab-7089-513e-689c64e32767"},"outputs":[],"source":"#submission = pd.DataFrame({'test_id': df_test['id'],'is_duplicate': decision_test})\nsubmission = pd.DataFrame({'test_id': df_train['id'], 'is_duplicate': decision})\n\nsubmission.to_csv(\"submission.csv\", index=False)\n#sub = pd.DataFrame()\n#sub['test_id'] = df_test['id']\n#sub['is_duplicate'] = decision_test\n#sub.to_csv('simple_counw.csv', index=False)\nsubmission.head()"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}