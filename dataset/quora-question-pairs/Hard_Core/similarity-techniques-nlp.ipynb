{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"7448604d-56cc-5eb6-bbd1-8905bf716a60"},"source":"##Predict Duplicate using basic ML + NLP techniques##\n\nI am trying to predict the duplicate sentences using vector similarity calculations and NLP technique in this module and its other forked versions.\n\nBOW + Cosine/Euclidean/Manhattan/Jaccard/Minskowiski"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e7d4aa51-afc3-ef96-11e6-f944b9bdda73"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"markdown","metadata":{"_cell_guid":"542c44bf-ec1d-ba34-7ab0-dae0bfc5e25f"},"source":"##Reading train data, Cleaning##\n\n*Reading Training Data ,\nRemoving duplicates , \nRemoving NULL values*\n\nUsing pandas read_csv command to read data from train files.  And doing some basic cleanup on the data by removing any duplicates or null values that may be present in the data. \n\nNote: Reducing the size of the data set so that the Kernel memory does not run out"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"acafea10-9564-f25a-6849-d976ee90a51a"},"outputs":[],"source":"'''from sklearn.model_selection import train_test_split\n\ndef read_data():\n    df = pd.read_csv(\"../input/train.csv\", nrows=20000)\n    print (\"Shape of base training File = \", df.shape)\n    # Remove missing values and duplicates from training data\n    df.drop_duplicates(inplace=True)\n    df.dropna(inplace=True)\n    print(\"Shape of base training data after cleaning = \", df.shape)\n    return df\n\ndf = read_data()\ndf_train, df_test = train_test_split(df, test_size = 0.02)\nprint (df_train.head(2))\nprint (df_test.shape)'''"},{"cell_type":"markdown","metadata":{"_cell_guid":"b3e20ba9-2e98-37dd-3d95-a3ebf9d9c1fb"},"source":"As we can see from the above output the file contains six columns\n\n- **qid1** & **qid2**  - which contains the unique id assigned to the question\n- **question1** & **question2** - which contains the actual questions\n- **is_duplicate** - which contains information if the question1 and 2 are duplicate or not"},{"cell_type":"markdown","metadata":{"_cell_guid":"7ff92a6d-8a72-cde1-24bb-093fb60fb452"},"source":"## EDA ##\n\nSome EDA on the data to get a look and feel about the data. Here we are trying to see the distribution of output data. Duplicate questions available etc."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ad969c65-aac3-f40c-4650-b50ccc143fa6"},"outputs":[],"source":"''''''from collections import Counter\nimport matplotlib.pyplot as plt\nimport operator\n\ndef eda(df):\n    print (\"Duplicate Count = %s , Non Duplicate Count = %s\" \n           %(df.is_duplicate.value_counts()[1],df.is_duplicate.value_counts()[0]))\n    \n    question_ids_combined = df.qid1.tolist() + df.qid2.tolist()\n    \n    print (\"Unique Questions = %s\" %(len(np.unique(question_ids_combined))))\n    \n    question_ids_counter = Counter(question_ids_combined)\n    sorted_question_ids_counter = sorted(question_ids_counter.items(), key=operator.itemgetter(1))\n    question_appearing_more_than_once = [i for i in question_ids_counter.values() if i > 1]\n    print (\"Count of Quesitons appearing more than once = %s\" %(len(question_appearing_more_than_once)))\n    \n    \neda(df_train)'''"},{"cell_type":"markdown","metadata":{"_cell_guid":"607dec70-9919-5f49-3a56-0cd23c9e9670"},"source":"## Train Dictionary ##\n\nFirst we will tokenize the sentences to extract words from the question. Lets also apply porter stemmer to break down words into their basic form. This should help us increase the accuracy of the system.\n\nThen we use gensims to train a dictionary of words available in the corpus. We are training the dictionary based on the Bag Of Words concept. Gensims dictionary will assign a id to each word which we can use later to convert documents into vectors. \n\nAlso, filter extremes to remove words appearing less than 5times in the corpus or in more than 80% of the questions."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9eb1f3bb-0ba9-bef3-487b-1ad5a440abf9"},"outputs":[],"source":"'''import re\nimport gensim\nfrom gensim import corpora\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import *\n\nwords = re.compile(r\"\\w+\",re.I)\nstopword = stopwords.words('english')\nstemmer = PorterStemmer()\n\ndef tokenize_questions(df):\n    question_1_tokenized = []\n    question_2_tokenized = []\n\n    for q in df.question1.tolist():\n        question_1_tokenized.append([stemmer.stem(i.lower()) for i in words.findall(q) if i not in stopword])\n\n    for q in df.question2.tolist():\n        question_2_tokenized.append([stemmer.stem(i.lower()) for i in words.findall(q) if i not in stopword])\n\n    df[\"Question_1_tok\"] = question_1_tokenized\n    df[\"Question_2_tok\"] = question_2_tokenized\n    \n    return df\n\ndef train_dictionary(df):\n    \n    questions_tokenized = df.Question_1_tok.tolist() + df.Question_2_tok.tolist()\n    \n    dictionary = corpora.Dictionary(questions_tokenized)\n    dictionary.filter_extremes(no_below=5, no_above=0.8)\n    dictionary.compactify()\n    \n    return dictionary\n    \ndf_train = tokenize_questions(df_train)\ndictionary = train_dictionary(df_train)\nprint (\"No of words in the dictionary = %s\" %len(dictionary.token2id))\n\ndf_test = tokenize_questions(df_test)'''"},{"cell_type":"markdown","metadata":{"_cell_guid":"42fe02fe-bf22-fadb-b3b9-812d82e7b60a"},"source":"As we can see that the number of unique words in the dictionary after filtering are 4831. \nThis would be the size of each of the vector in the question set."},{"cell_type":"markdown","metadata":{"_cell_guid":"56bed0e8-b345-d2cd-3816-1748b47c4483"},"source":"##Create Vector##\n\nHere we are using the simple method of Bag Of Words Technique to convert sentences into vectors. There are two vector matrices thus created where each of the matrix is a sparse matrix to save  memory in the system."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c6c05814-8314-8af4-cabc-b39c51a58293"},"outputs":[],"source":"'''def get_vectors(df, dictionary):\n    \n    question1_vec = [dictionary.doc2bow(text) for text in df.Question_1_tok.tolist()]\n    question2_vec = [dictionary.doc2bow(text) for text in df.Question_2_tok.tolist()]\n    \n    question1_csc = gensim.matutils.corpus2csc(question1_vec, num_terms=len(dictionary.token2id))\n    question2_csc = gensim.matutils.corpus2csc(question2_vec, num_terms=len(dictionary.token2id))\n    \n    return question1_csc.transpose(),question2_csc.transpose()\n\n\nq1_csc, q2_csc = get_vectors(df_train, dictionary)\n\nprint (q1_csc.shape)\nprint (q2_csc.shape)'''"},{"cell_type":"markdown","metadata":{"_cell_guid":"0d40f181-547d-8f31-5633-847bff79dde1"},"source":"As we can see each of the matrix is of size \n404288 X 30114   = > (size of the training data) X (no of words in the dictionary)"},{"cell_type":"markdown","metadata":{"_cell_guid":"113ed18c-6634-8ba2-f693-2521cfba2ac7"},"source":"##Define Similarity Calculation Fucntions##\n\nHere we have defined various Distance calculation functions for \n\n - Cosine Distance\n - Euclidean Distance\n - Manhattan Distance\n - Jaccard Distance\n - Minkowski Distance\n\nAs Eucledian, Manhattan and Minkowski Distance may go beyond 1 we must scale them down between0 - 1 , for that we are using MinMaxScaler and training them on training data."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8b90d4fd-5c98-9f58-3e6e-1ed968b6f869"},"outputs":[],"source":"'''from sklearn.metrics.pairwise import cosine_similarity as cs\nfrom sklearn.metrics.pairwise import manhattan_distances as md\nfrom sklearn.metrics.pairwise import euclidean_distances as ed\nfrom sklearn.metrics import jaccard_similarity_score as jsc\nfrom sklearn.neighbors import DistanceMetric\nfrom sklearn.preprocessing import MinMaxScaler\n\nminkowski_dis = DistanceMetric.get_metric('minkowski')\nmms_scale_man = MinMaxScaler()\nmms_scale_euc = MinMaxScaler()\nmms_scale_mink = MinMaxScaler()\n\ndef get_similarity_values(q1_csc, q2_csc):\n    cosine_sim = []\n    manhattan_dis = []\n    eucledian_dis = []\n    jaccard_dis = []\n    minkowsk_dis = []\n    \n    for i,j in zip(q1_csc, q2_csc):\n        sim = cs(i,j)\n        cosine_sim.append(sim[0][0])\n        sim = md(i,j)\n        manhattan_dis.append(sim[0][0])\n        sim = ed(i,j)\n        eucledian_dis.append(sim[0][0])\n        i_ = i.toarray()\n        j_ = j.toarray()\n        try:\n            sim = jsc(i_,j_)\n            jaccard_dis.append(sim)\n        except:\n            jaccard_dis.append(0)\n            \n        sim = minkowski_dis.pairwise(i_,j_)\n        minkowsk_dis.append(sim[0][0])\n    \n    return cosine_sim, manhattan_dis, eucledian_dis, jaccard_dis, minkowsk_dis    \n\n\n# cosine_sim = get_cosine_similarity(q1_csc, q2_csc)\ncosine_sim, manhattan_dis, eucledian_dis, jaccard_dis, minkowsk_dis = get_similarity_values(q1_csc, q2_csc)\nprint (\"cosine_sim sample= \\n\", cosine_sim[0:2])\nprint (\"manhattan_dis sample = \\n\", manhattan_dis[0:2])\nprint (\"eucledian_dis sample = \\n\", eucledian_dis[0:2])\nprint (\"jaccard_dis sample = \\n\", jaccard_dis[0:2])\nprint (\"minkowsk_dis sample = \\n\", minkowsk_dis[0:2])\n\neucledian_dis_array = np.array(eucledian_dis).reshape(-1,1)\nmanhattan_dis_array = np.array(manhattan_dis).reshape(-1,1)\nminkowsk_dis_array = np.array(minkowsk_dis).reshape(-1,1)\n    \nmanhattan_dis_array = mms_scale_man.fit_transform(manhattan_dis_array)\neucledian_dis_array = mms_scale_euc.fit_transform(eucledian_dis_array)\nminkowsk_dis_array = mms_scale_mink.fit_transform(minkowsk_dis_array)\n\neucledian_dis = eucledian_dis_array.flatten()\nmanhattan_dis = manhattan_dis_array.flatten()\nminkowsk_dis = minkowsk_dis_array.flatten()'''"},{"cell_type":"markdown","metadata":{"_cell_guid":"ef529f38-e1c5-4c01-f250-65b37e8efc62"},"source":"##Calculate Log Loss##\n\nHere we will use log loss formula to set a base criteria as to what accuracy our algorithm is able to achieve in terms of log loss which is the competition calucation score.\n\nWe will also use Eucledian, Manhattan , Minkowski and Jaccard to calculate the similarity and then have a look at the log loss from each one of them. These are the five most widely used similarity classes used in Data Science so Lets use each one of them to see which performs best."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8c4e9eb2-f52a-950e-8ddb-14110f64ba7b"},"outputs":[],"source":"''''''from sklearn.metrics import log_loss\n\ndef calculate_logloss(y_true, y_pred):\n    loss_cal = log_loss(y_true, y_pred)\n    return loss_cal\n\nq1_csc_test, q2_csc_test = get_vectors(df_test, dictionary)\ny_pred_cos, y_pred_man, y_pred_euc, y_pred_jac, y_pred_mink = get_similarity_values(q1_csc_test, q2_csc_test)\ny_true = df_test.is_duplicate.tolist()\n\ny_pred_man_array = mms_scale_man.transform(np.array(y_pred_man).reshape(-1,1))\ny_pred_man = y_pred_man_array.flatten()\n\ny_pred_euc_array = mms_scale_euc.transform(np.array(y_pred_euc).reshape(-1,1))\ny_pred_euc = y_pred_euc_array.flatten()\n\ny_pred_mink_array = mms_scale_mink.transform(np.array(y_pred_mink).reshape(-1,1))\ny_pred_mink = y_pred_mink_array.flatten()\n\nlogloss = calculate_logloss(y_true, y_pred_cos)\nprint (\"The calculated log loss value on the test set for cosine sim is = %f\" %logloss)\n\nlogloss = calculate_logloss(y_true, y_pred_man)\nprint (\"The calculated log loss value on the test set for manhattan sim is = %f\" %logloss)\n\nlogloss = calculate_logloss(y_true, y_pred_euc)\nprint (\"The calculated log loss value on the test set for euclidean sim is = %f\" %logloss)\n\nlogloss = calculate_logloss(y_true, y_pred_jac)\nprint (\"The calculated log loss value on the test set for jaccard sim is = %f\" %logloss)\n\nlogloss = calculate_logloss(y_true, y_pred_mink)\nprint (\"The calculated log loss value on the test set for minkowski sim is = %f\" %logloss)'''"},{"cell_type":"markdown","metadata":{"_cell_guid":"6715aaba-6649-6f89-f284-63765995f61f"},"source":"Although this test is run on a small set it indicates that cosine similarity is working as the best parameter for finding duplicate among sentences."},{"cell_type":"markdown","metadata":{"_cell_guid":"82d19c90-b56f-06ff-2d82-68a28dc333b5"},"source":"## Adding Machine Learning Models to improve logloss accuracy ##\n\nNow in order to improve on the accuracy let us feed the results from these similarity coefficients to a Random Forest Regressor and Support Vector Regressor and check if we can improve on the log loss values.\n\nNot concentrating on the hyper parameters of RF and SVM we are just allowing the algorithms to run as it is."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8022d9e6-5b04-9473-821c-7df290040fe4"},"outputs":[],"source":"'''from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\n\nX_train = pd.DataFrame({\"cos\" : cosine_sim, \"man\" : manhattan_dis, \"euc\" : eucledian_dis, \"jac\" : jaccard_dis, \"min\" : minkowsk_dis})\ny_train = df_train.is_duplicate\n\nX_test = pd.DataFrame({\"cos\" : y_pred_cos, \"man\" : y_pred_man, \"euc\" : y_pred_euc, \"jac\" : y_pred_jac, \"min\" : y_pred_mink})\ny_test = y_true\n\nrfr = RandomForestRegressor()\nrfr.fit(X_train, y_train)\n\nsvr = SVR()\nsvr.fit(X_train,y_train)'''"},{"cell_type":"markdown","metadata":{"_cell_guid":"c1b70db8-92e5-f0f9-0374-49bcd4bc8a15"},"source":"Now that we have trained the model . Lets predict duplicate from models and calcualte logloss from them to check if their is any improvement in the logloss values."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"77cc19b8-a2a0-a3c3-2f63-df0496fdba52"},"outputs":[],"source":"''''y_rfr_predicted = rfr.predict(X_test)\ny_svr_predicted = svr.predict(X_test)\n\nlogloss_rfr = calculate_logloss(y_test, y_rfr_predicted)\nlogloss_svr = calculate_logloss(y_test, y_svr_predicted)\n\nprint (\"The calculated log loss value on the test set using RFR is = %f\" %logloss_rfr)\nprint (\"The calculated log loss value on the test set using SVR is = %f\" %logloss_svr)'''"},{"cell_type":"markdown","metadata":{"_cell_guid":"573a925a-ea42-132c-0b75-e51918d6828f"},"source":"As we can see from the above results that we are able to bring down the logloss values to nearly **half** of what was predicted earlier using base similarity techniques. "},{"cell_type":"markdown","metadata":{"_cell_guid":"b5756cac-f00c-f9c8-ece2-141d9c292378"},"source":"## Tuning SVR model and producing output"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7a34e8af-e645-cf11-0260-b726c9dfee01"},"outputs":[],"source":"'''\nPredict Duplicate using basic ML + NLP techniques\nI am trying to predict the duplicate sentences using vector \nsimilarity blnCreateSubmitFile and NLP technique in this module and its other forked versions.\n'''\nfrom scipy.optimize import differential_evolution\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.metrics.pairwise import cosine_similarity as cs\nfrom sklearn.metrics.pairwise import manhattan_distances as md\nfrom sklearn.metrics.pairwise import euclidean_distances as ed\nfrom sklearn.metrics import jaccard_similarity_score as jsc\nfrom sklearn.neighbors import DistanceMetric\nfrom sklearn.preprocessing import MinMaxScaler\n\n\nfrom __future__ import unicode_literals\nfrom sklearn.metrics import log_loss\nimport re\nimport gensim\nfrom gensim import corpora\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import *\nfrom nltk.stem import SnowballStemmer\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.externals import joblib\n\ncoding:'utf-8'\nFILE_PATH='C:\\\\Kaggle\\\\Quora\\\\QuoraComp\\\\'\nFILE_PATH=\"../input/\"\nglobal df_train \nglobal X_train \nglobal y_train\n#global cosine_sim, manhattan_dis,eucledian_dis,jaccard_dis,minkowsk_dis\nglobal y_pred_cos,y_pred_euc,y_pred_jac,y_pred_mink,y_pred_man\nglobal y_true\n\nglobal blnCreateSubmitFile\n\n\ndef calculate_logloss(y_true, y_pred):\n    loss_cal = log_loss(y_true, y_pred)\n    return loss_cal\n\ndef mainPredict():\n\n    global df_train\n    global y_true\n    #global cosine_sim, manhattan_dis,eucledian_dis,jaccard_dis,minkowsk_dis\n    global y_pred_cos,y_pred_euc,y_pred_jac,y_pred_mink,y_pred_man\n    global X_train, y_train\n\n    def read_data():\n        global blnCreateSubmitFile\n        lstrFile='train.csv'\n\n        if blnCreateSubmitFile:\n            lstrFile='test.csv'\n\n        df = pd.read_csv(FILE_PATH+lstrFile) #, nrows=250000)\n        df.reindex(np.random.permutation(df.index))\n        df=df[0:70000]\n        print (\"Shape of base training File = \", df.shape)\n\n        if not blnCreateSubmitFile:\n            # Remove missing values and duplicates from training data\n            df.drop_duplicates(inplace=True)\n            df.dropna(inplace=True)\n\n        print(\"Shape of base training data after cleaning = \", df.shape)\n        return df\n\n    df = read_data()\n\n    if blnCreateSubmitFile:\n        #create submission file, use df_train variable but it holds test data file.\n        df_train =df\n        df_test =df[0:6]\n        del df\n    else:\n        df_train, df_test = train_test_split(df, test_size = 0.14)\n\n        print (df_train.head(2))\n        print (df_test.shape)\n        print (str(len(df_train))+' rows in df_train')\n\n\n    '''Train Dictionary¶\n    First we will tokenize the sentences to extract words from the question. \n    Lets also apply porter stemmer to break down words into their basic form. \n    This should help us increase the accuracy of the system.\n    Then we use gensims to train a dictionary of words available in the corpus.\n    We are training the dictionary based on the Bag Of Words concept. \n    Gensims dictionary will assign a id to each word which we can use \n    later to convert documents into vectors.\n    Also, filter extremes to remove words appearing less than 5 \n    times in the corpus or in more than 80% of the questions.\n    '''\n\n\n    words = re.compile(r\"\\w+\",re.I)\n    stopword = stopwords.words('english')\n    #stemmer = PorterStemmer()\n    stemmer = SnowballStemmer('english')\n    def tokenize_questions(df):\n        question_1_tokenized = []\n        question_2_tokenized = []\n\n        try:\n            for q in df.question1.tolist():\n                question_1_tokenized.append([stemmer.stem(i.lower()) for i in words.findall(q) if i not in stopword ])\n\n            for q in df.question2.tolist():\n                question_2_tokenized.append([stemmer.stem(i.lower()) for i in words.findall(q) if i not in stopword ])\n\n            df[\"Question_1_tok\"] = question_1_tokenized\n            df[\"Question_2_tok\"] = question_2_tokenized\n\n        except Exception as e:\n            #print(len(question_1_tokenized))\n            print ('hit exception')\n            #print (df[78217:78218]['question1'])\n        return df\n\n    def train_dictionary(df):\n    \n        questions_tokenized = df.Question_1_tok.tolist() + df.Question_2_tok.tolist()\n    \n        dictionary = corpora.Dictionary(questions_tokenized)\n        dictionary.filter_extremes(no_below=5, no_above=0.8)\n        dictionary.compactify()\n    \n        return dictionary\n    \n    df_train = tokenize_questions(df_train)\n    dictionary = train_dictionary(df_train)\n    print (\"No of words in the dictionary = %s\" %len(dictionary.token2id))\n\n    df_test = tokenize_questions(df_test)\n\n\n    '''\n    Create Vector\n    Here we are using the simple method of Bag Of Words Technique\n    to convert sentences into vectors.\n    There are two vector matrices thus created where each of\n    the matrix is a sparse matrix to save memory in the system.\n    '''\n    def get_vectors(df, dictionary):\n    \n        question1_vec = [dictionary.doc2bow(text) for text in df.Question_1_tok.tolist()]\n        question2_vec = [dictionary.doc2bow(text) for text in df.Question_2_tok.tolist()]\n    \n        question1_csc = gensim.matutils.corpus2csc(question1_vec, num_terms=len(dictionary.token2id))\n        question2_csc = gensim.matutils.corpus2csc(question2_vec, num_terms=len(dictionary.token2id))\n    \n        return question1_csc.transpose(),question2_csc.transpose()\n\n\n    q1_csc, q2_csc = get_vectors(df_train, dictionary)\n\n    print (q1_csc.shape)\n    print (q2_csc.shape)\n\n    '''\n    Define Similarity Calculation Functions:\n    Here we have defined various Distance calculation functions for\n    Cosine Distance\n    Euclidean Distance\n    Manhattan Distance\n    Jaccard Distance\n    Minkowski Distance\n    As Eucledian, Manhattan and Minkowski Distance \n    may go beyond 1 we must scale them down between0 - 1 ,\n    for that we are using MinMaxScaler and training them on training data.\n    '''\n    minkowski_dis = DistanceMetric.get_metric('minkowski')\n    mms_scale_man = MinMaxScaler()\n    mms_scale_euc = MinMaxScaler()\n    mms_scale_mink = MinMaxScaler()\n\n    def get_similarity_values(q1_csc, q2_csc):\n        #global cosine_sim, manhattan_dis,eucledian_dis,jaccard_dis,minkowsk_dis\n        cosine_sim = []\n        manhattan_dis = []\n        eucledian_dis = []\n        jaccard_dis = []\n        minkowsk_dis = []\n    \n        for i,j in zip(q1_csc, q2_csc):\n            sim = cs(i,j)\n            cosine_sim.append(sim[0][0])\n            sim = md(i,j)\n            manhattan_dis.append(sim[0][0])\n            sim = ed(i,j)\n            eucledian_dis.append(sim[0][0])\n            i_ = i.toarray()\n            j_ = j.toarray()\n            try:\n                sim = jsc(i_,j_)\n                jaccard_dis.append(sim)\n            except:\n                jaccard_dis.append(0)\n            \n            sim = minkowski_dis.pairwise(i_,j_)\n            minkowsk_dis.append(sim[0][0])\n    \n        return cosine_sim, manhattan_dis, eucledian_dis, jaccard_dis, minkowsk_dis    \n\n\n    # cosine_sim = get_cosine_similarity(q1_csc, q2_csc)\n    cosine_sim, manhattan_dis, eucledian_dis, jaccard_dis, minkowsk_dis = get_similarity_values(q1_csc, q2_csc)\n    print (\"cosine_sim sample= \\n\", cosine_sim[0:2])\n    print (\"manhattan_dis sample = \\n\", manhattan_dis[0:2])\n    print (\"eucledian_dis sample = \\n\", eucledian_dis[0:2])\n    print (\"jaccard_dis sample = \\n\", jaccard_dis[0:2])\n    print (\"minkowsk_dis sample = \\n\", minkowsk_dis[0:2])\n\n    eucledian_dis_array = np.array(eucledian_dis).reshape(-1,1)\n    manhattan_dis_array = np.array(manhattan_dis).reshape(-1,1)\n    minkowsk_dis_array = np.array(minkowsk_dis).reshape(-1,1)\n    \n    manhattan_dis_array = mms_scale_man.fit_transform(manhattan_dis_array)\n    eucledian_dis_array = mms_scale_euc.fit_transform(eucledian_dis_array)\n    minkowsk_dis_array = mms_scale_mink.fit_transform(minkowsk_dis_array)\n\n    eucledian_dis = eucledian_dis_array.flatten()\n    manhattan_dis = manhattan_dis_array.flatten()\n    minkowsk_dis = minkowsk_dis_array.flatten()\n\n    '''\n    Calculate Log Loss¶\n    Here we will use log loss formula to set a base criteria as\n    to what accuracy our algorithm is able to achieve in terms \n    of log loss which is the competition calucation score.\n    We will also use Eucledian, Manhattan ,\n    Minkowski and Jaccard to calculate the similarity and then\n    have a look at the log loss from each one of them. These are\n    the five most widely used similarity classes used in Data Science\n    so Lets use each one of them to see which performs best.\n    '''\n\n    q1_csc_test, q2_csc_test = get_vectors(df_test, dictionary)\n    y_pred_cos, y_pred_man, y_pred_euc, y_pred_jac, y_pred_mink = get_similarity_values(q1_csc_test, q2_csc_test)\n    \n    if not blnCreateSubmitFile:\n        y_true = df_test.is_duplicate.tolist()\n\n    y_pred_man_array = mms_scale_man.transform(np.array(y_pred_man).reshape(-1,1))\n    y_pred_man = y_pred_man_array.flatten()\n\n    y_pred_euc_array = mms_scale_euc.transform(np.array(y_pred_euc).reshape(-1,1))\n    y_pred_euc = y_pred_euc_array.flatten()\n\n    y_pred_mink_array = mms_scale_mink.transform(np.array(y_pred_mink).reshape(-1,1))\n    y_pred_mink = y_pred_mink_array.flatten()\n\n    if not blnCreateSubmitFile:\n        logloss = calculate_logloss(y_true, y_pred_cos)\n        print (\"The calculated log loss value on the test set for cosine sim is = %f\" %logloss)\n\n        logloss = calculate_logloss(y_true, y_pred_man)\n        print (\"The calculated log loss value on the test set for manhattan sim is = %f\" %logloss)\n\n        logloss = calculate_logloss(y_true, y_pred_euc)\n        print (\"The calculated log loss value on the test set for euclidean sim is = %f\" %logloss)\n\n        logloss = calculate_logloss(y_true, y_pred_jac)\n        print (\"The calculated log loss value on the test set for jaccard sim is = %f\" %logloss)\n\n        logloss = calculate_logloss(y_true, y_pred_mink)\n        print (\"The calculated log loss value on the test set for minkowski sim is = %f\" %logloss)\n\n    X_train = pd.DataFrame({\"cos\" : cosine_sim, \"man\" : manhattan_dis, \"euc\" : eucledian_dis, \"jac\" : jaccard_dis, \"min\" : minkowsk_dis})\n    \n    if not blnCreateSubmitFile:\n        y_train = df_train.is_duplicate\n\ndef fnRunSVR(*pArgs):\n    '''\n    Adding Machine Learning Models to improve logloss accuracy\n    Now in order to improve on the accuracy let us feed the\n    results from these similarity coefficients to a Random Forest Regressor\n    and Support Vector Regressor and check if we can improve on the log loss values.\n    Not concentrating on the hyper parameters of RF and SVM we \n    are just allowing the algorithms to run as it is.\n    '''\n    print (pArgs)\n    pC=int(pArgs[0][0])\n    pGamma=pArgs[0][1]\n    global blnCreateSubmitFile\n    global df_train\n    #global cosine_sim, manhattan_dis,eucledian_dis,jaccard_dis,minkowsk_dis\n    global y_pred_cos,y_pred_euc,y_pred_jac,y_pred_mink,y_pred_man\n    global y_true\n    global X_train \n    global y_train\n    \n    X_test = pd.DataFrame({\"cos\" : y_pred_cos, \"man\" : y_pred_man, \"euc\" : y_pred_euc, \"jac\" : y_pred_jac, \"min\" : y_pred_mink})\n    \n    if not blnCreateSubmitFile:\n        y_test = y_true\n\n    #rfr = RandomForestRegressor()\n    #rfr.fit(X_train, y_train)\n\n    if not blnCreateSubmitFile:\n        svr = SVR(C=pC, gamma=pGamma)\n        print ('training SVR model')\n        svr.fit(X_train,y_train)\n        joblib.dump(svr, 'Quora_SVR.pkl') \n        print ('saved SVR to pickle')\n    else:\n        #load from pickle saved file\n        svr = joblib.load('Quora_SVR.pkl') \n\n    '''\n    Now that we have trained the model . \n    Lets predict duplicate from models and calculate logloss\n    from them to check if their is any improvement in the logloss values.\n    '''\n    #y_rfr_predicted = rfr.predict(X_test)\n    if not blnCreateSubmitFile:\n        y_svr_predicted = svr.predict(X_test)\n\n        #logloss_rfr = calculate_logloss(y_test, y_rfr_predicted)\n        logloss_svr = calculate_logloss(y_test, y_svr_predicted)\n        print (\"The calculated log loss value on the test set using SVR is = %f\" %logloss_svr)\n    else:\n        y_svr_predicted =svr.predict(X_train)\n    #print (\"The calculated log loss value on the test set using RFR is = %f\" %logloss_rfr)\n    \n    \n    if blnCreateSubmitFile:\n        df_train['is_duplicate']=y_svr_predicted\n        df_train['is_duplicate'] =np.maximum(0,df_train['is_duplicate'] )\n        #submission = pd.DataFrame({'test_id':test_ids, 'is_duplicate':y_svr_predicted})\n        df_train[['test_id','is_duplicate']].to_csv('QuoraSubmission.csv', index=False)\n\n    return logloss_svr\n\nif __name__=='__main__':\n    blnCreateSubmitFile=False\n    mainPredict()\n    lBounds=[(1,100),(.000000001,.25)]\n    fnRunSVR([  5.42448063e+01,   2.37859253e-02])\n    #result=differential_evolution(func=fnRunSVR,bounds=lBounds,disp=1)\n    #print(result)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}