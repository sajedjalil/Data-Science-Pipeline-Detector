{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! unzip \"/kaggle/input/quora-question-pairs/train.csv.zip\"\n! unzip \"/kaggle/input/quora-question-pairs/test.csv.zip\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom subprocess import check_output\n%matplotlib inline\n\n! pip install plotly\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport os\nimport gc\n\nimport re\nfrom nltk.corpus import stopwords\n\n!pip3 install distance\nimport distance\nfrom nltk.stem import PorterStemmer\n! pip3 install bs4\nfrom bs4 import BeautifulSoup","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3.1 Reading and Basic EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv(\"train.csv\")\nprint(\"number of data points\",df.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3.2 Distribution of data points among output classes"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby('is_duplicate')['id'].count().plot.bar()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"total number of question pairs for training {}\".format(len(df)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"question pairs are not similar {}\".format(100-round(df['is_duplicate']).mean()*100,2))\nprint(\"question pairs are similar {}\".format(round(df['is_duplicate']).mean()*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3.2.1  Number of unique questions"},{"metadata":{"trusted":true},"cell_type":"code","source":"qids=pd.Series(df['qid1'].tolist() + df['qid2'].tolist())\nunique_qs = len(np.unique(qids))\nqs_morethan_onetime = np.sum(qids.value_counts()>1)\n\nprint ('Total number of  Unique Questions are: {}\\n'.format(unique_qs))\n\nprint ('Number of unique questions that appear more than one time: {} ({}%)\\n'.format(qs_morethan_onetime,qs_morethan_onetime/unique_qs*100))\n\nprint ('Max number of times a single question is repeated: {}\\n'.format(max(qids.value_counts()))) \n\nq_vals=qids.value_counts()\n\nq_vals=q_vals.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x=[\"unique question\",\"repeated_questions\"]\ny=[unique_qs,qs_morethan_onetime]\n\nplt.figure(figsize=(10,6))\nsns.barplot(x,y)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# check for duplicate"},{"metadata":{"trusted":true},"cell_type":"code","source":"pair_duplicate=df[['qid1','qid2','is_duplicate']].groupby(['qid1','qid2']).count().reset_index()\nprint(\"no of duplicate questions\",(pair_duplicate).shape[0]-df.shape[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# No of occurance of each questions"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 6))\n\nplt.hist(qids.value_counts(), bins=160)\n\nplt.yscale('log', nonposy='clip')\n\nplt.title('Log-Histogram of question appearance counts')\n\nplt.xlabel('Number of occurences of question')\n\nplt.ylabel('Number of questions')\n\nprint ('Maximum number of times a single question is repeated: {}\\n'.format(max(qids.value_counts()))) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Checking for null values"},{"metadata":{"trusted":true},"cell_type":"code","source":"nan_rows=df[df.isnull().any(1)]\nprint(nan_rows)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=df.fillna(\"\")\nnan_rows=df[df.isnull().any(1)]\nprint(nan_rows)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Basic Feature Extraction"},{"metadata":{},"cell_type":"markdown","source":"\n    freq_qid1 = Frequency of qid1's\n    freq_qid2 = Frequency of qid2's\n    q1len = Length of q1\n    q2len = Length of q2\n    q1_n_words = Number of words in Question 1\n    q2_n_words = Number of words in Question 2\n    word_Common = (Number of common unique words in Question 1 and Question 2)\n    word_Total =(Total num of words in Question 1 + Total num of words in Question 2)\n    word_share = (word_common)/(word_Total)\n    freq_q1+freq_q2 = sum total of frequency of qid1 and qid2\n    freq_q1-freq_q2 = absolute difference of frequency of qid1 and qid2\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"if os.path.isfile(\"./df_fe_without_preprocessing_train.csv\"):\n    df=pd.read_csv(\"./df_fe_without_preprocessing_train.csv\")\n    df.head()\nelse:\n    df['freq_qid1']=df.groupby('qid1')['qid1'].transform('count')\n    df['freq_qid2']=df.groupby('qid2')['qid2'].transform('count')\n    df['q1len']=df['question1'].str.len()\n    df['q2len']=df['question2'].str.len()\n    df['q1_n_words'] = df['question1'].apply(lambda row: len(row.split(\" \")))\n    df['q2_n_words'] = df['question2'].apply(lambda row: len(row.split(\" \")))\n    \n    def normalised_word_common(row):\n        w1=set(map(lambda word: word.lower().strip(),row['question1'].split(\" \")))\n        w2=set(map(lambda word: word.lower().strip(),row['question2'].split(\" \")))\n        return 1.0*len(w1 & w2)\n    df['word_common']=df.apply(normalised_word_common,axis=1)\n    \n    def normalized_word_Total(row):\n        w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n        w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n        return 1.0 * (len(w1) + len(w2))\n    df['word_Total'] = df.apply(normalized_word_Total, axis=1)\n    \n    def normalized_word_share(row):\n        w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n        w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n        return 1.0 * len(w1 & w2)/(len(w1) + len(w2))\n    df['word_share'] = df.apply(normalized_word_share, axis=1)\n    \n    df['freq_q1+q2'] = df['freq_qid1']+df['freq_qid2']\n    df['freq_q1-q2'] = abs(df['freq_qid1']-df['freq_qid2'])\n\n    df.to_csv(\"df_fe_without_preprocessing_train.csv\", index=False)\n    \ndf.head()\n    \n    \n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Analysis of extraxted feature **"},{"metadata":{"trusted":true},"cell_type":"code","source":"print (\"Minimum length of the questions in question1 : \" , min(df['q1_n_words']))\n\nprint (\"Minimum length of the questions in question2 : \" , min(df['q2_n_words']))\n\nprint (\"Number of Questions with minimum length [question1] :\", df[df['q1_n_words']== 1].shape[0])\nprint (\"Number of Questions with minimum length [question2] :\", df[df['q2_n_words']== 1].shape[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Word share"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\n\nplt.subplot(1,2,1)\nsns.violinplot(x='is_duplicate',y='word_share',data=df[0:])\n\nplt.subplot(1,2,2)\nsns.distplot(df[df['is_duplicate'] == 1.0]['word_share'][0:] , label = \"1\", color = 'red')\nsns.distplot(df[df['is_duplicate'] == 0.0]['word_share'][0:] , label = \"0\" , color = 'blue' )\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n    The distributions for normalized word_share have some overlap on the far right-hand side, i.e., there are quite a lot of questions with high word similarity\n    The average word share and Common no. of words of qid1 and qid2 is more when they are duplicate(Similar)\n\n"},{"metadata":{},"cell_type":"markdown","source":"Feature: word_Common "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 8))\n\nplt.subplot(1,2,1)\nsns.violinplot(x = 'is_duplicate', y = 'word_common', data = df[0:])\n\nplt.subplot(1,2,2)\nsns.distplot(df[df['is_duplicate'] == 1.0]['word_common'][0:] , label = \"1\", color = 'red')\nsns.distplot(df[df['is_duplicate'] == 0.0]['word_common'][0:] , label = \"0\" , color = 'blue' )\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The distributions of the word_Common feature in similar and non-similar questions are highly overlapping "},{"metadata":{},"cell_type":"markdown","source":"# EDA :Advanced Feature extraction"},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport re\nfrom fuzzywuzzy import fuzz\nfrom sklearn.manifold import TSNE\n\nfrom wordcloud import WordCloud,STOPWORDS\nfrom PIL import Image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing of Text"},{"metadata":{},"cell_type":"markdown","source":"Preprocessing:\n\n    Removing html tags\n    Removing Punctuations\n    Performing stemming\n    Removing Stopwords\n    Expanding contractions etc.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"SAFE_DIV=0.0001\n\nSTOP_WORDS=stopwords.words('english')\n\ndef preprocess(x):\n    x=str(x).lower()\n    x=x.replace(\",000,000\", \"m\").replace(\",000\", \"k\").replace(\"′\", \"'\").replace(\"’\", \"'\")\\\n                           .replace(\"won't\", \"will not\").replace(\"cannot\", \"can not\").replace(\"can't\", \"can not\")\\\n                           .replace(\"n't\", \" not\").replace(\"what's\", \"what is\").replace(\"it's\", \"it is\")\\\n                           .replace(\"'ve\", \" have\").replace(\"i'm\", \"i am\").replace(\"'re\", \" are\")\\\n                           .replace(\"he's\", \"he is\").replace(\"she's\", \"she is\").replace(\"'s\", \" own\")\\\n                           .replace(\"%\", \" percent \").replace(\"₹\", \" rupee \").replace(\"$\", \" dollar \")\\\n                           .replace(\"€\", \" euro \").replace(\"'ll\", \" will\")\n    \n    x = re.sub(r\"([0-9]+)000000\", r\"lm\", x)\n    x = re.sub(r\"([0-9]+)000\", r\"lk\", x)\n    \n    \n    porter=PorterStemmer()\n    pattern=re.compile('\\W')\n    \n    if type(x) == type(''):\n        x = re.sub(pattern, ' ', x)\n    \n    \n    if type(x) == type(''):\n        x = porter.stem(x)\n        example1 = BeautifulSoup(x)\n        x = example1.get_text()\n               \n    return x\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Definition:\n\n    Token: You get a token by splitting sentence a space\n    Stop_Word : stop words as per NLTK.\n    Word : A token that is not a stop_word\n\nFeatures:\n\n    cwc_min : Ratio of common_word_count to min lenghth of word count of Q1 and Q2\n    cwc_min = common_word_count / (min(len(q1_words), len(q2_words))\n\n\n    cwc_max : Ratio of common_word_count to max lenghth of word count of Q1 and Q2\n    cwc_max = common_word_count / (max(len(q1_words), len(q2_words))\n\n\n    csc_min : Ratio of common_stop_count to min lenghth of stop count of Q1 and Q2\n    csc_min = common_stop_count / (min(len(q1_stops), len(q2_stops))\n\n\n    csc_max : Ratio of common_stop_count to max lenghth of stop count of Q1 and Q2\n    csc_max = common_stop_count / (max(len(q1_stops), len(q2_stops))\n\n\n    ctc_min : Ratio of common_token_count to min lenghth of token count of Q1 and Q2\n    ctc_min = common_token_count / (min(len(q1_tokens), len(q2_tokens))\n\n\n    ctc_max : Ratio of common_token_count to max lenghth of token count of Q1 and Q2\n    ctc_max = common_token_count / (max(len(q1_tokens), len(q2_tokens))\n\n\n    last_word_eq : Check if First word of both questions is equal or not\n    last_word_eq = int(q1_tokens[-1] == q2_tokens[-1])\n\n\n    first_word_eq : Check if First word of both questions is equal or not\n    first_word_eq = int(q1_tokens[0] == q2_tokens[0])\n\n\n    abs_len_diff : Abs. length difference\n    abs_len_diff = abs(len(q1_tokens) - len(q2_tokens))\n\n\n    mean_len : Average Token Length of both Questions\n    mean_len = (len(q1_tokens) + len(q2_tokens))/2\n\n\n    fuzz_ratio : https://github.com/seatgeek/fuzzywuzzy#usage http://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/\n\n\n    fuzz_partial_ratio : https://github.com/seatgeek/fuzzywuzzy#usage http://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/\n\n\n    token_sort_ratio : https://github.com/seatgeek/fuzzywuzzy#usage http://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/\n\n    token_set_ratio : https://github.com/seatgeek/fuzzywuzzy#usage http://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/\n\n    longest_substr_ratio : Ratio of length longest common substring to min lenghth of token count of Q1 and Q2\n    longest_substr_ratio = len(longest common substring) / (min(len(q1_tokens), len(q2_tokens))\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_token_features(q1,q2):\n    token_features=[0.0]*10\n    \n    \n    q1_tokens=q1.split()\n    q2_tokens=q2.split()\n    \n    if len(q1_tokens)==0 or len(q2_tokens)==0:\n        return token_features\n    \n    q1_words=set([word for word in q1_tokens if word not in STOP_WORDS])\n    q2_words=set([word for word in q2_tokens if word not in STOP_WORDS])\n    \n    q1_stops=set([word for word in q1_tokens if word in STOP_WORDS])\n    q2_stops=set([word for word in q2_tokens if word in STOP_WORDS])\n    \n    common_word_count=len(q1_words.intersection(q2_words))\n    \n    common_stop_count=len(q1_stops.intersection(q2_stops))\n    \n    common_token_count=len(set(q1_tokens).intersection(set(q2_tokens)))\n    \n    \n    token_features[0]=common_word_count/(min(len(q1_words),len(q2_words))+SAFE_DIV)\n    token_features[1]=common_word_count/(max(len(q1_words),len(q2_words))+SAFE_DIV)\n    token_features[2]=common_stop_count/(min(len(q1_words),len(q2_words))+SAFE_DIV)\n    token_features[3]=common_stop_count/(max(len(q1_words),len(q2_words))+SAFE_DIV)\n    token_features[4]=common_token_count/(min(len(q1_words),len(q2_words))+SAFE_DIV)\n    token_features[5]=common_token_count/(max(len(q1_words),len(q2_words))+SAFE_DIV)\n    \n    \n    token_features[6]=int(q1_tokens[-1]==q2_tokens[-1])\n    token_features[7]=int(q1_tokens[0]==q2_tokens[0])\n    \n    \n    token_features[9]=(len(q1_tokens)+len(q2_tokens))/2\n    \n    return token_features\n\n\n\ndef get_longest_substr_ratio(a, b):\n    strs = list(distance.lcsubstrings(a, b))\n    if len(strs) == 0:\n        return 0\n    else:\n        return len(strs[0]) / (min(len(a), len(b)) + 1)\n    \n    \ndef extract_features(df):\n    df[\"question1\"] = df[\"question1\"].fillna(\"\").apply(preprocess)\n    df[\"question2\"] = df[\"question2\"].fillna(\"\").apply(preprocess)\n    \n    token_features=df.apply(lambda x:get_token_features(x['question1'],x['question2']),axis=1)\n    \n    df[\"cwc_min\"]       = list(map(lambda x:x[0],token_features))\n    df[\"cwc_max\"]       = list(map(lambda x: x[1], token_features))\n    df[\"csc_min\"]       = list(map(lambda x: x[2], token_features))\n    df[\"csc_max\"]       = list(map(lambda x: x[3], token_features))\n    df[\"ctc_min\"]       = list(map(lambda x: x[4], token_features))\n    df[\"ctc_max\"]       = list(map(lambda x: x[5], token_features))\n    df[\"last_word_eq\"]  = list(map(lambda x: x[6], token_features))\n    df[\"first_word_eq\"] = list(map(lambda x: x[7], token_features))\n    df[\"abs_len_diff\"]  = list(map(lambda x: x[8], token_features))\n    df[\"mean_len\"]      = list(map(lambda x: x[9], token_features))\n    \n    df[\"token_set_ratio\"]       = df.apply(lambda x: fuzz.token_set_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n    # The token sort approach involves tokenizing the string in question, sorting the tokens alphabetically, and \n    # then joining them back into a string We then compare the transformed strings with a simple ratio().\n    df[\"token_sort_ratio\"]      = df.apply(lambda x: fuzz.token_sort_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n    df[\"fuzz_ratio\"]            = df.apply(lambda x: fuzz.QRatio(x[\"question1\"], x[\"question2\"]), axis=1)\n    df[\"fuzz_partial_ratio\"]    = df.apply(lambda x: fuzz.partial_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n    df[\"longest_substr_ratio\"]  = df.apply(lambda x: get_longest_substr_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n    return df\n    \n\n\n    \n    \n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nif os.path.isfile(\"./nlp_features_train.csv\"):\n    df=pd.read_csv(\"./nlp_features_train.csv\",encoding='latin-1')\n    df.fillna(\"\")\nelse:\n    print(\"Extracting features for train:\")\n    df = pd.read_csv(\"./train.csv\")\n    df = extract_features(df)\n    df.to_csv(\"nlp_features_train.csv\", index=False)\ndf.head(2)\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Analysis of extracteed features"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_duplicate=df[df['is_duplicate']==1]\ndfp_nonduplicate=df[df['is_duplicate']==0]\n\n# Converting 2d array of q1 and q2 and flatten the array: like {{1,2},{3,4}} to {1,2,3,4}\np = np.dstack([df_duplicate[\"question1\"], df_duplicate[\"question2\"]]).flatten()\nn = np.dstack([dfp_nonduplicate[\"question1\"], dfp_nonduplicate[\"question2\"]]).flatten()\n\nprint (\"Number of data points in class 1 (duplicate pairs) :\",len(p))\nprint (\"Number of data points in class 0 (non duplicate pairs) :\",len(n))\n\n#Saving the np array into a text file\nnp.savetxt('train_p.txt', p, delimiter=' ', fmt='%s')\nnp.savetxt('train_n.txt', n, delimiter=' ', fmt='%s')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from os import path\n\nd = path.dirname('./')\n\ntextp_w = open(path.join(d, 'train_p.txt')).read()\ntextn_w = open(path.join(d, 'train_n.txt')).read()\nstopwords = set(STOPWORDS)\nstopwords.add(\"said\")\nstopwords.add(\"br\")\nstopwords.add(\" \")\nstopwords.remove(\"not\")\n\nstopwords.remove(\"no\")\n#stopwords.remove(\"good\")\n#stopwords.remove(\"love\")\nstopwords.remove(\"like\")\n#stopwords.remove(\"best\")\n#stopwords.remove(\"!\")\nprint (\"Total number of words in duplicate pair questions :\",len(textp_w))\nprint (\"Total number of words in non duplicate pair questions :\",len(textn_w))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wc = WordCloud(background_color=\"white\", max_words=len(textp_w), stopwords=stopwords)\nwc.generate(textp_w)\nprint (\"Word Cloud for Duplicate Question pairs\")\nplt.imshow(wc, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wc = WordCloud(background_color=\"white\", max_words=len(textn_w),stopwords=stopwords)\n# generate word cloud\nwc.generate(textn_w)\nprint (\"Word Cloud for non-Duplicate Question pairs:\")\nplt.imshow(wc, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n = df.shape[0]\nsns.pairplot(df[['ctc_min', 'cwc_min', 'csc_min', 'token_sort_ratio', 'is_duplicate']][0:n], hue='is_duplicate', vars=['ctc_min', 'cwc_min', 'csc_min', 'token_sort_ratio'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of the token_sort_ratio\nplt.figure(figsize=(10, 8))\n\nplt.subplot(1,2,1)\nsns.violinplot(x = 'is_duplicate', y = 'token_sort_ratio', data = df[0:] , )\n\nplt.subplot(1,2,2)\nsns.distplot(df[df['is_duplicate'] == 1.0]['token_sort_ratio'][0:] , label = \"1\", color = 'red')\nsns.distplot(df[df['is_duplicate'] == 0.0]['token_sort_ratio'][0:] , label = \"0\" , color = 'blue' )\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 8))\n\nplt.subplot(1,2,1)\nsns.violinplot(x = 'is_duplicate', y = 'fuzz_ratio', data = df[0:] , )\n\nplt.subplot(1,2,2)\nsns.distplot(df[df['is_duplicate'] == 1.0]['fuzz_ratio'][0:] , label = \"1\", color = 'red')\nsns.distplot(df[df['is_duplicate'] == 0.0]['fuzz_ratio'][0:] , label = \"0\" , color = 'blue' )\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\ndfp_subsampled=df[0:5000]\nX = MinMaxScaler().fit_transform(dfp_subsampled[['cwc_min', 'cwc_max', 'csc_min', 'csc_max' , 'ctc_min' , 'ctc_max' , 'last_word_eq', 'first_word_eq' , 'abs_len_diff' , 'mean_len' , 'token_set_ratio' , 'token_sort_ratio' ,  'fuzz_ratio' , 'fuzz_partial_ratio' , 'longest_substr_ratio']])\ny = dfp_subsampled['is_duplicate'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tsne2d = TSNE(\n    n_components=2,\n    init='random', # pca\n    random_state=101,\n    method='barnes_hut',\n    n_iter=1000,\n    verbose=2,\n    angle=0.5\n).fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame({'x':tsne2d[:,0], 'y':tsne2d[:,1] ,'label':y})\n\n# draw the plot in appropriate place in the grid\nsns.lmplot(data=df, x='x', y='y', hue='label', fit_reg=False, size=8,palette=\"Set1\",markers=['s','o'])\nplt.title(\"perplexity : {} and max_iter : {}\".format(30, 1000))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.manifold import TSNE\ntsne3d = TSNE(\n    n_components=3,\n    init='random', # pca\n    random_state=101,\n    method='barnes_hut',\n    n_iter=1000,\n    verbose=2,\n    angle=0.5\n).fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trace1 = go.Scatter3d(\n    x=tsne3d[:,0],\n    y=tsne3d[:,1],\n    z=tsne3d[:,2],\n    mode='markers',\n    marker=dict(\n        sizemode='diameter',\n        color = y,\n        colorscale = 'Portland',\n        colorbar = dict(title = 'duplicate'),\n        line=dict(color='rgb(255, 255, 255)'),\n        opacity=0.75\n    )\n)\n\ndata=[trace1]\nlayout=dict(height=800, width=800, title='3d embedding with engineered features')\nfig=dict(data=data, layout=layout)\npy.iplot(fig, filename='3DBubble')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Featurizing text data with tfidf weighted word-vectors "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import normalize\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport sys\nfrom tqdm import tqdm\n\nimport spacy\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"train.csv\")\n \n# encode questions to unicode\n# https://stackoverflow.com/a/6812069\n# ----------------- python 2 ---------------------\n# df['question1'] = df['question1'].apply(lambda x: unicode(str(x),\"utf-8\"))\n# df['question2'] = df['question2'].apply(lambda x: unicode(str(x),\"utf-8\"))\n# ----------------- python 3 ---------------------\ndf['question1'] = df['question1'].apply(lambda x: str(x))\ndf['question2'] = df['question2'].apply(lambda x: str(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"question=list(df['question1'])+list(df['question2'])\n\ntfidf=TfidfVectorizer(lowercase=False)\ntfidf.fit_transform(question)\n\n\nword2tfidf = dict(zip(tfidf.get_feature_names(), tfidf.idf_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# en_vectors_web_lg, which includes over 1 million unique vectors.\nnlp = spacy.load('en_core_web_sm')\n\nvecs1 = []\n# https://github.com/noamraph/tqdm\n# tqdm is used to print the progress bar\nfor qu1 in tqdm(list(df['question1'])):\n    doc1 = nlp(qu1) \n    # 384 is the number of dimensions of vectors \n    mean_vec1 = np.zeros([len(doc1), len(doc1[0].vector)])\n    for word1 in doc1:\n        # word2vec\n        vec1 = word1.vector\n        # fetch df score\n        try:\n            idf = word2tfidf[str(word1)]\n        except:\n            idf = 0\n        # compute final vec\n        mean_vec1 += vec1 * idf\n    mean_vec1 = mean_vec1.mean(axis=0)\n    vecs1.append(mean_vec1)\ndf['q1_feats_m'] = list(vecs1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vecs2 = []\nfor qu2 in tqdm(list(df['question2'])):\n    doc2 = nlp(qu2) \n    mean_vec2 = np.zeros([len(doc1), len(doc2[0].vector)])\n    for word2 in doc2:\n        # word2vec\n        vec2 = word2.vector\n        # fetch df score\n        try:\n            idf = word2tfidf[str(word2)]\n        except:\n            #print word\n            idf = 0\n        # compute final vec\n        mean_vec2 += vec2 * idf\n    mean_vec2 = mean_vec2.mean(axis=0)\n    vecs2.append(mean_vec2)\ndf['q2_feats_m'] = list(vecs2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if os.path.isfile('nlp_features_train.csv'):\n    dfnlp = pd.read_csv(\"nlp_features_train.csv\")\nelse:\n    print(\"download nlp_features_train.csv from drive or run previous notebook\")\n\nif os.path.isfile('df_fe_without_preprocessing_train.csv'):\n    dfppro = pd.read_csv(\"df_fe_without_preprocessing_train.csv\",encoding='latin-1')\nelse:\n    print(\"download df_fe_without_preprocessing_train.csv from drive or run previous notebook\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = dfnlp.drop(['qid1','qid2','question1','question2'],axis=1)\ndf2 = dfppro.drop(['qid1','qid2','question1','question2','is_duplicate'],axis=1)\ndf3 = df.drop(['qid1','qid2','question1','question2','is_duplicate'],axis=1)\ndf3_q1 = pd.DataFrame(df3.q1_feats_m.values.tolist(), index= df3.index)\ndf3_q2 = pd.DataFrame(df3.q2_feats_m.values.tolist(), index= df3.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dataframe of nlp features\ndf1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data before preprocessing \ndf2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Questions 1 tfidf weighted word2vec\ndf3_q1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Questions 2 tfidf weighted word2vec\ndf3_q2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of features in nlp dataframe :\", df1.shape[1])\nprint(\"Number of features in preprocessed dataframe :\", df2.shape[1])\nprint(\"Number of features in question1 w2v  dataframe :\", df3_q1.shape[1])\nprint(\"Number of features in question2 w2v  dataframe :\", df3_q2.shape[1])\nprint(\"Number of features in final dataframe  :\", df1.shape[1]+df2.shape[1]+df3_q1.shape[1]+df3_q2.shape[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# storing the final features to csv file\nif not os.path.isfile('final_features.csv'):\n    df3_q1['id']=df1['id']\n    df3_q2['id']=df1['id']\n    df1  = df1.merge(df2, on='id',how='left')\n    df2  = df3_q1.merge(df3_q2, on='id',how='left')\n    result  = df1.merge(df2, on='id',how='left')\n    result.to_csv('final_features.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics.classification import accuracy_score, log_loss\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom collections import Counter\nfrom scipy.sparse import hstack\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import StratifiedKFold \nfrom collections import Counter, defaultdict\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nimport math\nfrom sklearn.metrics import normalized_mutual_info_score\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport sqlite3\nfrom sqlalchemy import create_engine # database connection\nimport datetime as dt\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import SGDClassifier\nfrom mlxtend.classifier import StackingClassifier\n\n\n\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import precision_recall_curve, auc, roc_curve","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating db file from csv\n\"\"\"\nif not os.path.isfile('./train.db'):\n    disk_engine = create_engine('sqlite:///train.db')\n    start = dt.datetime.now()\n    chunksize = 180000\n    j = 0\n    index_start = 1\n    for df in pd.read_csv('./final_features.csv', names=['Unnamed: 0','id','is_duplicate','cwc_min','cwc_max','csc_min','csc_max','ctc_min','ctc_max','last_word_eq','first_word_eq','abs_len_diff','mean_len','token_set_ratio','token_sort_ratio','fuzz_ratio','fuzz_partial_ratio','longest_substr_ratio','freq_qid1','freq_qid2','q1len','q2len','q1_n_words','q2_n_words','word_Common','word_Total','word_share','freq_q1+q2','freq_q1-q2','0_x','1_x','2_x','3_x','4_x','5_x','6_x','7_x','8_x','9_x','10_x','11_x','12_x','13_x','14_x','15_x','16_x','17_x','18_x','19_x','20_x','21_x','22_x','23_x','24_x','25_x','26_x','27_x','28_x','29_x','30_x','31_x','32_x','33_x','34_x','35_x','36_x','37_x','38_x','39_x','40_x','41_x','42_x','43_x','44_x','45_x','46_x','47_x','48_x','49_x','50_x','51_x','52_x','53_x','54_x','55_x','56_x','57_x','58_x','59_x','60_x','61_x','62_x','63_x','64_x','65_x','66_x','67_x','68_x','69_x','70_x','71_x','72_x','73_x','74_x','75_x','76_x','77_x','78_x','79_x','80_x','81_x','82_x','83_x','84_x','85_x','86_x','87_x','88_x','89_x','90_x','91_x','92_x','93_x','94_x','95_x','96_x','97_x','98_x','99_x','100_x','101_x','102_x','103_x','104_x','105_x','106_x','107_x','108_x','109_x','110_x','111_x','112_x','113_x','114_x','115_x','116_x','117_x','118_x','119_x','120_x','121_x','122_x','123_x','124_x','125_x','126_x','127_x','128_x','129_x','130_x','131_x','132_x','133_x','134_x','135_x','136_x','137_x','138_x','139_x','140_x','141_x','142_x','143_x','144_x','145_x','146_x','147_x','148_x','149_x','150_x','151_x','152_x','153_x','154_x','155_x','156_x','157_x','158_x','159_x','160_x','161_x','162_x','163_x','164_x','165_x','166_x','167_x','168_x','169_x','170_x','171_x','172_x','173_x','174_x','175_x','176_x','177_x','178_x','179_x','180_x','181_x','182_x','183_x','184_x','185_x','186_x','187_x','188_x','189_x','190_x','191_x','192_x','193_x','194_x','195_x','196_x','197_x','198_x','199_x','200_x','201_x','202_x','203_x','204_x','205_x','206_x','207_x','208_x','209_x','210_x','211_x','212_x','213_x','214_x','215_x','216_x','217_x','218_x','219_x','220_x','221_x','222_x','223_x','224_x','225_x','226_x','227_x','228_x','229_x','230_x','231_x','232_x','233_x','234_x','235_x','236_x','237_x','238_x','239_x','240_x','241_x','242_x','243_x','244_x','245_x','246_x','247_x','248_x','249_x','250_x','251_x','252_x','253_x','254_x','255_x','256_x','257_x','258_x','259_x','260_x','261_x','262_x','263_x','264_x','265_x','266_x','267_x','268_x','269_x','270_x','271_x','272_x','273_x','274_x','275_x','276_x','277_x','278_x','279_x','280_x','281_x','282_x','283_x','284_x','285_x','286_x','287_x','288_x','289_x','290_x','291_x','292_x','293_x','294_x','295_x','296_x','297_x','298_x','299_x','300_x','301_x','302_x','303_x','304_x','305_x','306_x','307_x','308_x','309_x','310_x','311_x','312_x','313_x','314_x','315_x','316_x','317_x','318_x','319_x','320_x','321_x','322_x','323_x','324_x','325_x','326_x','327_x','328_x','329_x','330_x','331_x','332_x','333_x','334_x','335_x','336_x','337_x','338_x','339_x','340_x','341_x','342_x','343_x','344_x','345_x','346_x','347_x','348_x','349_x','350_x','351_x','352_x','353_x','354_x','355_x','356_x','357_x','358_x','359_x','360_x','361_x','362_x','363_x','364_x','365_x','366_x','367_x','368_x','369_x','370_x','371_x','372_x','373_x','374_x','375_x','376_x','377_x','378_x','379_x','380_x','381_x','382_x','383_x','0_y','1_y','2_y','3_y','4_y','5_y','6_y','7_y','8_y','9_y','10_y','11_y','12_y','13_y','14_y','15_y','16_y','17_y','18_y','19_y','20_y','21_y','22_y','23_y','24_y','25_y','26_y','27_y','28_y','29_y','30_y','31_y','32_y','33_y','34_y','35_y','36_y','37_y','38_y','39_y','40_y','41_y','42_y','43_y','44_y','45_y','46_y','47_y','48_y','49_y','50_y','51_y','52_y','53_y','54_y','55_y','56_y','57_y','58_y','59_y','60_y','61_y','62_y','63_y','64_y','65_y','66_y','67_y','68_y','69_y','70_y','71_y','72_y','73_y','74_y','75_y','76_y','77_y','78_y','79_y','80_y','81_y','82_y','83_y','84_y','85_y','86_y','87_y','88_y','89_y','90_y','91_y','92_y','93_y','94_y','95_y','96_y','97_y','98_y','99_y','100_y','101_y','102_y','103_y','104_y','105_y','106_y','107_y','108_y','109_y','110_y','111_y','112_y','113_y','114_y','115_y','116_y','117_y','118_y','119_y','120_y','121_y','122_y','123_y','124_y','125_y','126_y','127_y','128_y','129_y','130_y','131_y','132_y','133_y','134_y','135_y','136_y','137_y','138_y','139_y','140_y','141_y','142_y','143_y','144_y','145_y','146_y','147_y','148_y','149_y','150_y','151_y','152_y','153_y','154_y','155_y','156_y','157_y','158_y','159_y','160_y','161_y','162_y','163_y','164_y','165_y','166_y','167_y','168_y','169_y','170_y','171_y','172_y','173_y','174_y','175_y','176_y','177_y','178_y','179_y','180_y','181_y','182_y','183_y','184_y','185_y','186_y','187_y','188_y','189_y','190_y','191_y','192_y','193_y','194_y','195_y','196_y','197_y','198_y','199_y','200_y','201_y','202_y','203_y','204_y','205_y','206_y','207_y','208_y','209_y','210_y','211_y','212_y','213_y','214_y','215_y','216_y','217_y','218_y','219_y','220_y','221_y','222_y','223_y','224_y','225_y','226_y','227_y','228_y','229_y','230_y','231_y','232_y','233_y','234_y','235_y','236_y','237_y','238_y','239_y','240_y','241_y','242_y','243_y','244_y','245_y','246_y','247_y','248_y','249_y','250_y','251_y','252_y','253_y','254_y','255_y','256_y','257_y','258_y','259_y','260_y','261_y','262_y','263_y','264_y','265_y','266_y','267_y','268_y','269_y','270_y','271_y','272_y','273_y','274_y','275_y','276_y','277_y','278_y','279_y','280_y','281_y','282_y','283_y','284_y','285_y','286_y','287_y','288_y','289_y','290_y','291_y','292_y','293_y','294_y','295_y','296_y','297_y','298_y','299_y','300_y','301_y','302_y','303_y','304_y','305_y','306_y','307_y','308_y','309_y','310_y','311_y','312_y','313_y','314_y','315_y','316_y','317_y','318_y','319_y','320_y','321_y','322_y','323_y','324_y','325_y','326_y','327_y','328_y','329_y','330_y','331_y','332_y','333_y','334_y','335_y','336_y','337_y','338_y','339_y','340_y','341_y','342_y','343_y','344_y','345_y','346_y','347_y','348_y','349_y','350_y','351_y','352_y','353_y','354_y','355_y','356_y','357_y','358_y','359_y','360_y','361_y','362_y','363_y','364_y','365_y','366_y','367_y','368_y','369_y','370_y','371_y','372_y','373_y','374_y','375_y','376_y','377_y','378_y','379_y','380_y','381_y','382_y','383_y'], chunksize=chunksize, iterator=True, encoding='utf-8', ):\n        df.index += index_start\n        j+=1\n        print('{} rows'.format(j*chunksize))\n        df.to_sql('data', disk_engine, if_exists='append')\n        index_start = df.index[-1] + 1\n \"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n#http://www.sqlitetutorial.net/sqlite-python/create-tables/\ndef create_connection(db_file):\n     create a database connection to the SQLite database\n        specified by db_file\n    :param db_file: database file\n    :return: Connection object or None\n    \n    try:\n        conn = sqlite3.connect(db_file)\n        return conn\n    except Error as e:\n        print(e)\n \n    return None\n\n\ndef checkTableExists(dbcon):\n    cursr = dbcon.cursor()\n    str = \"select name from sqlite_master where type='table'\"\n    table_names = cursr.execute(str)\n    print(\"Tables in the databse:\")\n    tables =table_names.fetchall() \n    print(tables[0][0])\n    return(len(tables))\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n  #read_db = 'train.db'\n#  conn_r = create_connection(read_db)\n # checkTableExists(conn_r)\nconn_r.close()\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n# try to sample data according to the computing power you have\nif os.path.isfile(read_db):\n    conn_r = create_connection(read_db)\n    if conn_r is not None:\n        # for selecting first 1M rows\n        # data = pd.read_sql_query(\"\"\"SELECT * FROM data LIMIT 100001;\"\"\", conn_r\n        \n        # for selecting random points\n        data = pd.read_sql_query(\"SELECT * From data ORDER BY RANDOM() LIMIT 100001;\", conn_r)\n        conn_r.commit()\n        conn_r.close()\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv(\"./final_features.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove the first row \n\ny_true = df['is_duplicate']\ndf.drop(['Unnamed: 0', 'id','is_duplicate'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"-y_true = list(map(int, y_true.values))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  Random train test split( 70:30) "},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test, y_train, y_test = train_test_split(df, y_true, stratify=y_true, test_size=0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of data points in train data :\",X_train.shape)\nprint(\"Number of data points in test data :\",X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"-\"*10, \"Distribution of output variable in train data\", \"-\"*10)\ntrain_distr = Counter(y_train)\ntrain_len = len(y_train)\nprint(\"Class 0: \",int(train_distr[0])/train_len,\"Class 1: \", int(train_distr[1])/train_len)\nprint(\"-\"*10, \"Distribution of output variable in train data\", \"-\"*10)\ntest_distr = Counter(y_test)\ntest_len = len(y_test)\nprint(\"Class 0: \",int(test_distr[1])/test_len, \"Class 1: \",int(test_distr[1])/test_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This function plots the confusion matrices given y_i, y_i_hat.\ndef plot_confusion_matrix(test_y, predict_y):\n    C = confusion_matrix(test_y, predict_y)\n    # C = 9,9 matrix, each cell (i,j) represents number of points of class i are predicted class j\n    \n    A =(((C.T)/(C.sum(axis=1))).T)\n    #divid each element of the confusion matrix with the sum of elements in that column\n    \n    # C = [[1, 2],\n    #     [3, 4]]\n    # C.T = [[1, 3],\n    #        [2, 4]]\n    # C.sum(axis = 1)  axis=0 corresonds to columns and axis=1 corresponds to rows in two diamensional array\n    # C.sum(axix =1) = [[3, 7]]\n    # ((C.T)/(C.sum(axis=1))) = [[1/3, 3/7]\n    #                           [2/3, 4/7]]\n\n    # ((C.T)/(C.sum(axis=1))).T = [[1/3, 2/3]\n    #                           [3/7, 4/7]]\n    # sum of row elements = 1\n    \n    B =(C/C.sum(axis=0))\n    #divid each element of the confusion matrix with the sum of elements in that row\n    # C = [[1, 2],\n    #     [3, 4]]\n    # C.sum(axis = 0)  axis=0 corresonds to columns and axis=1 corresponds to rows in two diamensional array\n    # C.sum(axix =0) = [[4, 6]]\n    # (C/C.sum(axis=0)) = [[1/4, 2/6],\n    #                      [3/4, 4/6]] \n    plt.figure(figsize=(20,4))\n    \n    labels = [1,2]\n    # representing A in heatmap format\n    cmap=sns.light_palette(\"blue\")\n    plt.subplot(1, 3, 1)\n    sns.heatmap(C, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.title(\"Confusion matrix\")\n    \n    plt.subplot(1, 3, 2)\n    sns.heatmap(B, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.title(\"Precision matrix\")\n    \n    plt.subplot(1, 3, 3)\n    # representing B in heatmap format\n    sns.heatmap(A, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.title(\"Recall matrix\")\n    \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building a random model (Finding worst-case log-loss) "},{"metadata":{"trusted":true},"cell_type":"code","source":"# we need to generate 9 numbers and the sum of numbers should be 1\n# one solution is to genarate 9 numbers and divide each of the numbers by their sum\n# ref: https://stackoverflow.com/a/18662466/4084039\n# we create a output array that has exactly same size as the CV data\npredicted_y = np.zeros((test_len,2))\nfor i in range(test_len):\n    rand_probs = np.random.rand(1,2)\n    predicted_y[i] = ((rand_probs/sum(sum(rand_probs)))[0])\nprint(\"Log loss on Test Data using Random Model\",log_loss(y_test, predicted_y, eps=1e-15))\n\npredicted_y =np.argmax(predicted_y, axis=1)\nplot_confusion_matrix(y_test, predicted_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression with hyperparameter tuning "},{"metadata":{"trusted":true},"cell_type":"code","source":"alpha = [10 ** x for x in range(-5, 2)] # hyperparam for SGD classifier.\n\n# read more about SGDClassifier() at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n# ------------------------------\n# default parameters\n# SGDClassifier(loss=’hinge’, penalty=’l2’, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None, \n# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=’optimal’, eta0=0.0, power_t=0.5, \n# class_weight=None, warm_start=False, average=False, n_iter=None)\n\n# some of methods\n# fit(X, y[, coef_init, intercept_init, …])\tFit linear model with Stochastic Gradient Descent.\n# predict(X)\tPredict class labels for samples in X.\n\n#-------------------------------\n# video link: \n#------------------------------\n\n\nlog_error_array=[]\nfor i in alpha:\n    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n    clf.fit(X_train, y_train)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(X_train, y_train)\n    predict_y = sig_clf.predict_proba(X_test)\n    log_error_array.append(log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n\nfig, ax = plt.subplots()\nax.plot(alpha, log_error_array,c='g')\nfor i, txt in enumerate(np.round(log_error_array,3)):\n    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(log_error_array)\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\nclf.fit(X_train, y_train)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(X_train, y_train)\n\npredict_y = sig_clf.predict_proba(X_train)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(X_test)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\npredicted_y =np.argmax(predict_y,axis=1)\nprint(\"Total number of data points :\", len(predicted_y))\nplot_confusion_matrix(y_test, predicted_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Linear SVM with hyperparameter tuning "},{"metadata":{"trusted":true},"cell_type":"code","source":"alpha = [10 ** x for x in range(-5, 2)] # hyperparam for SGD classifier.\n\n# read more about SGDClassifier() at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n# ------------------------------\n# default parameters\n# SGDClassifier(loss=’hinge’, penalty=’l2’, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None, \n# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=’optimal’, eta0=0.0, power_t=0.5, \n# class_weight=None, warm_start=False, average=False, n_iter=None)\n\n# some of methods\n# fit(X, y[, coef_init, intercept_init, …])\tFit linear model with Stochastic Gradient Descent.\n# predict(X)\tPredict class labels for samples in X.\n\n#-------------------------------\n# video link: \n#------------------------------\n\n\nlog_error_array=[]\nfor i in alpha:\n    clf = SGDClassifier(alpha=i, penalty='l1', loss='hinge',max_iter=15,random_state=42)\n    clf.fit(X_train, y_train)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(X_train, y_train)\n    predict_y = sig_clf.predict_proba(X_test)\n    log_error_array.append(log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n\nfig, ax = plt.subplots()\nax.plot(alpha, log_error_array,c='g')\nfor i, txt in enumerate(np.round(log_error_array,3)):\n    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(log_error_array)\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l1', loss='hinge', random_state=42)\nclf.fit(X_train, y_train)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(X_train, y_train)\n\npredict_y = sig_clf.predict_proba(X_train)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(X_test)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\npredicted_y =np.argmax(predict_y,axis=1)\nprint(\"Total number of data points :\", len(predicted_y))\nplot_confusion_matrix(y_test, predicted_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XGBoost "},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nparams = {}\nparams['objective'] = 'binary:logistic'\nparams['eval_metric'] = 'logloss'\nparams['eta'] = 0.02\nparams['max_depth'] = 4\n\nd_train = xgb.DMatrix(X_train, label=y_train)\nd_test = xgb.DMatrix(X_test, label=y_test)\n\nwatchlist = [(d_train, 'train'), (d_test, 'valid')]\n\nbst = xgb.train(params, d_train, 400, watchlist, early_stopping_rounds=20, verbose_eval=10)\n\nxgdmat = xgb.DMatrix(X_train,y_train)\npredict_y = bst.predict(d_test)\nprint(\"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_y =np.array(predict_y>0.5,dtype=int)\nprint(\"Total number of data points :\", len(predicted_y))\nplot_confusion_matrix(y_test, predicted_y)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}