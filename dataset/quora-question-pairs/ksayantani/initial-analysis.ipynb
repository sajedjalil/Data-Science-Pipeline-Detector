{"cells":[{"outputs":[],"metadata":{"_uuid":"21eec13fe15702223fc0913afbb7bc3ec3018aa3","_cell_guid":"4846c062-7350-566d-0693-181f5c6925d1","trusted":true,"_execution_state":"idle"},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":1},{"execution_count":null,"metadata":{"_uuid":"1b53e0d972c0ce18f1284049e9d712d38b37e045","_cell_guid":"bd6bb321-860d-d0fe-059d-622034a0f835"},"cell_type":"markdown","source":"### Import plotting libraries ###","outputs":[]},{"outputs":[],"metadata":{"_uuid":"784292a59e400171a120e7d85dd69fb04dec153c","_cell_guid":"68fe4408-e57e-15c1-4a88-27d9ae7ee7e1","trusted":true,"collapsed":true,"_execution_state":"idle"},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\nsns.set(color_codes=True)\nsns.set_style(\"white\")\n\nfrom plotly.offline import plot\nimport plotly.graph_objs as go\n\nimport sklearn.ensemble\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, log_loss\n\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nfrom collections import Counter\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nstops = set(stopwords.words(\"english\"))","execution_count":2},{"outputs":[],"metadata":{"_uuid":"a364b4cae09525068bfdbc4fbaacaa67287f8a95","_cell_guid":"e9425502-b733-74a2-da78-769cf1e31917","trusted":true,"_execution_state":"idle"},"cell_type":"code","source":"train_set = pd.read_csv('../input/train.csv')\n\ntest_set = pd.read_csv('../input/test.csv')\n\nprint('There are {} records in train'.format(train_set.shape[0]))\nprint('There are {} records in train'.format(test_set.shape[0]))","execution_count":3},{"outputs":[],"metadata":{"_uuid":"1bac82a69039dc97e2c0d1dfecab61bce295142d","_cell_guid":"8adc5737-61ba-2855-ba75-2fdd6378e667","trusted":true,"collapsed":true,"_execution_state":"idle"},"cell_type":"code","source":"target = 'is_duplicate'\nID = 'id'","execution_count":4},{"outputs":[],"metadata":{"trusted":true,"collapsed":true,"_uuid":"a5ce0068a61a0c22591b1c738d32e7fe41dc2f41"},"cell_type":"code","source":"train_set['question1'] = train_set['question1'].fillna('')\ntrain_set['question2'] = train_set['question2'].fillna('')\n\ntest_set['question1'] = test_set['question1'].fillna('')\ntest_set['question2'] = test_set['question2'].fillna('')","execution_count":5},{"outputs":[],"metadata":{"trusted":true,"collapsed":true,"_uuid":"2628282c1349349b49e41f9efefb2edbdbbf52d3"},"cell_type":"code","source":"def clean_text(text):\n    text = re.sub('\\s+', ' ', text)\n    text = re.sub(\"\\.\\s\", '.', text)\n    text = re.sub(':\\)', '.', text)\n    text = re.sub(\"^\\s\\w+\", '\\w+', text)\n    text = re.sub('\\.', ' ', text)\n    return text","execution_count":6},{"outputs":[],"metadata":{"trusted":true,"collapsed":true,"_uuid":"e630bc88a1d06bbb24cda1bb2c73beffa3b805d6"},"cell_type":"code","source":"def replace(sentence):\n    sentence = sentence.replace('what\\'s', 'what is').replace('don\\'t', 'do not')\n    sentence = sentence.replace('i\\'m', 'i am').replace('can\\'t', 'can not')\n    sentence = sentence.replace('doesn\\'t', 'does not').replace('it\\'s', 'it is')\n    sentence = sentence.replace('didn\\'t', 'did not').replace('isn\\'t', 'is not')\n    sentence = sentence.replace('won\\'t', 'will not').replace('aren\\'t', 'are not')\n    sentence = sentence.replace('shouldn\\'t', 'should not').replace('haven\\'t', 'have not')\n    sentence = sentence.replace('hasn\\'t', 'has not').replace('he\\'s', 'he is')\n    sentence = sentence.replace('wouldn\\'t', 'would not').replace('he\\'s', 'he is')\n    sentence = sentence.replace('that\\'s', 'that is').replace('wasn\\'t', 'was not')\n    sentence = sentence.replace('how\\'s', 'how is')\n    sentence = sentence.replace('you\\'ve', 'you have').replace('you\\'re', 'you are')\n    sentence = sentence.replace('i\\'ve', 'i have').replace('they\\'re', 'they are')\n    sentence = sentence.replace('i\\'ll', 'i will').replace('they\\'ve', 'they have')\n    sentence = sentence.replace('we\\'re', 'we are').replace('you\\'ll', 'you will')\n    sentence = sentence.replace('we\\'re', 'we are').replace('we\\'ve', 'we have')\n    sentence = sentence.replace('we\\'ll', 'we will').replace('it\\'ll', 'it will').replace('they\\'ll', 'they will')\n    sentence = sentence.replace('who\\'ll', 'who will').replace('who\\'ve', 'who have')\n    sentence = sentence.replace('he\\'ll', 'he will').replace('that\\'ll', 'that will')\n    sentence = sentence.replace('does\\'nt', 'does not').replace('could\\'ve', 'could have')\n    sentence = sentence.replace('would\\'ve', 'would have').replace('what\\'re', 'what are')\n    sentence = sentence.replace('i\\'am', 'i am').replace('who\\'re', 'who are')\n    sentence = sentence.replace('should\\'ve', 'should have').replace('did\\'nt', 'did not')\n    sentence = sentence.replace('hold\\'em', 'hold them').replace('there\\'re', 'there are')\n    sentence = sentence.replace('do\\'nt', 'do not').replace('could\\'nt', 'could not')\n    return sentence","execution_count":7},{"outputs":[],"metadata":{"_uuid":"43b948617c253bad80712f10ec6ca8021d4a0c3a","_cell_guid":"1758be31-5a90-5fd2-f648-34d19722847e","trusted":true,"collapsed":true,"_execution_state":"idle"},"cell_type":"code","source":"def find_unigrams(question):\n    question = clean_text(question)\n    question = replace(question)\n    \n    word_tokens = question.split(' ')\n    word_tokens = [w for w in word_tokens if not w  in stops]\n    word_tokens = [w for w in word_tokens if not w == '']\n    return word_tokens","execution_count":8},{"outputs":[],"metadata":{"_uuid":"6a26d7eb0874cb7161481ce00b21ffc8908b4093","_cell_guid":"2735b59c-6390-39ef-fc04-2278fcaa78ec","trusted":true,"collapsed":true,"_execution_state":"idle"},"cell_type":"code","source":"def shared_words_in_q2(row):\n    q1_tokens = row['q1_tokens']\n    q2_tokens = row['q2_tokens']\n    \n    matching_words = [w for w in q2_tokens if w in q1_tokens]\n    return len(matching_words) / (len(q1_tokens) + len(q2_tokens))","execution_count":9},{"outputs":[],"metadata":{"_uuid":"f144ed0b942700f4b0d54996f7c891ed22394ec4","_cell_guid":"a5fee247-640c-bce8-ab6f-21646a31b668","trusted":true,"collapsed":true,"_execution_state":"idle"},"cell_type":"code","source":"def shared_words_in_q1(row):\n    q1_tokens = row['q1_tokens']\n    q2_tokens = row['q2_tokens']\n    matching_words = [w for w in q1_tokens if w in q2_tokens]\n    \n    return len(matching_words) / (len(q1_tokens) + len(q2_tokens))","execution_count":10},{"outputs":[],"metadata":{"_uuid":"daca641724cd82d3d2bd9d9f007da1d5141f8a07","_cell_guid":"5f0dfb22-1482-a70a-5ab8-5d021ea239b8","trusted":true,"collapsed":true,"_execution_state":"idle"},"cell_type":"code","source":"train_set['q1_tokens'] = train_set['question1'].map(find_unigrams)\ntrain_set['q2_tokens'] = train_set['question2'].map(find_unigrams)\n\ntrain_set['q1_length'] = train_set['q1_tokens'].apply(len)\ntrain_set['q2_length'] = train_set['q2_tokens'].apply(len)\ntrain_set['len_diff'] = train_set.apply(lambda x: np.abs(x['q1_length'] - x['q2_length']), axis=1)\n\ntrain_set['shared_words_q1'] = train_set.apply(lambda x: shared_words_in_q1(x), axis=1)\ntrain_set['shared_words_q2'] = train_set.apply(lambda x: shared_words_in_q2(x), axis=1)","execution_count":11},{"outputs":[],"metadata":{"_uuid":"4094f1471a58bd7bafc062c6a8d256742983ff12","_execution_state":"idle","trusted":true,"_cell_guid":"ba911d75-0e2e-44d0-97c6-07be9260a5b1"},"cell_type":"code","source":"gb_qid  = train_set.groupby('qid1').filter(lambda x: len(x) > 1).groupby('qid1')\nduplicate_qid1 = sorted(list(gb_qid.groups))","execution_count":12},{"outputs":[],"metadata":{"_uuid":"028ab05f03c250032b4fbd35e46ece4055c4f9e8","_execution_state":"idle","trusted":true,"_cell_guid":"c69b753b-9612-433d-a93c-1e288ebd77e6"},"cell_type":"code","source":"stats = gb_qid['is_duplicate'].agg({np.sum, np.size})\nonly_duplicates = stats.loc[stats['sum'] == stats['size']].sort_values(['size'], ascending=False)\nduplicate_df = train_set.loc[train_set['qid1'].isin(only_duplicates.index)]","execution_count":13},{"outputs":[],"metadata":{"_uuid":"f19934e2d991b46f38120cfdb36811fdbdc09275","_execution_state":"busy","trusted":true,"_cell_guid":"c5a43786-6fb2-4454-8c7c-47841a3d6c27"},"cell_type":"code","source":"train_set.loc[train_set['qid1'].isin(duplicate_qid1), 'graph_root'] = 1\ntrain_set['graph_root'].fillna(0, inplace=True)\ntrain_set['graph_root'] = train_set['graph_root'].astype(int)","execution_count":14},{"outputs":[],"metadata":{"trusted":true,"collapsed":true,"_uuid":"1f505f21869c5119b5596b0c043e218a9483b38a"},"cell_type":"code","source":"for node in only_duplicates.index:\n    group = train_set.loc[train_set['qid1'] == node]\n    group1 = train_set.loc[train_set['qid1'].isin(group['qid2'])]\n    \n    if len(group1) > 0:\n        train_set.loc[train_set['qid1'] == node, 'neighbors'] = len(group1)\n        \ntrain_set['neighbors'].fillna(0, inplace=True)\ntrain_set['neighbors'] = train_set['neighbors'].astype(int)","execution_count":15},{"outputs":[],"metadata":{"_uuid":"e0d0f4e0490f86c02a920c443a115264bbb330c1","_cell_guid":"a9410e58-245f-ab28-76ca-2520db61b848","trusted":true,"collapsed":true,"_execution_state":"busy"},"cell_type":"code","source":"clf = RandomForestClassifier()\ntrain_features = ['len_diff', 'shared_words_q1']\n\ndef train_data(clf, train_features):\n    X = train_set[train_features]\n    y = train_set[target]\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)\n    clf = clf.fit(X_train, y_train)\n\n    y_proba = clf.predict_proba(X_test)\n    log_loss_score = log_loss(y_test, y_proba)\n    metrics.append(log_loss_score)\n    return clf","execution_count":43},{"outputs":[],"metadata":{"_uuid":"7c0e79aad3700044e39464ecf1fe388e67cb3536","_cell_guid":"86bf4262-08f4-a4c6-334a-30d1068c6bff","trusted":true,"_execution_state":"busy"},"cell_type":"code","source":"metrics = []\nfor i in np.arange(5):\n    train_data(clf, train_features)","execution_count":44},{"outputs":[],"metadata":{"trusted":true,"_uuid":"f8586b648ba7206c0c2e037c495592cdfcb5c128"},"cell_type":"code","source":"metrics","execution_count":45},{"outputs":[],"metadata":{"_uuid":"8364657695c2cbb91d88fbe7b0b0c10728ce727f","_cell_guid":"80681035-bde6-016b-6676-c63f01851c99","trusted":true,"collapsed":true,"_execution_state":"busy"},"cell_type":"code","source":"def preprocess():\n    test_set['q1_tokens'] = test_set['question1'].map(find_unigrams)\n    test_set['q2_tokens'] = test_set['question2'].map(find_unigrams)\n\n    test_set['q1_length'] = test_set['q1_tokens'].apply(len)\n    test_set['q2_length'] = test_set['q2_tokens'].apply(len)\n    test_set['len_diff'] = test_set.apply(lambda x: np.abs(x['q1_length'] - x['q2_length']), axis=1)\n\n    test_set['shared_words_q1'] = test_set.apply(lambda x: shared_words_in_q1(x), axis=1)\n    test_set['shared_words_q2'] = test_set.apply(lambda x: shared_words_in_q2(x), axis=1)\n    return test_set","execution_count":20},{"outputs":[],"metadata":{"trusted":true,"collapsed":true,"_uuid":"6f7d7cf5c32511a7dff46fe56cbbcc881756160f"},"cell_type":"code","source":"test_set = preprocess()","execution_count":21},{"outputs":[],"metadata":{"_uuid":"8667fc7869a736195f2229371a58c0bbdf17e306","_cell_guid":"b1bf53b5-aac3-511c-283d-96aa27e2e5d9","trusted":true,"collapsed":true,"_execution_state":"busy"},"cell_type":"code","source":"def generate_predictions(train_features):\n    test_ids = test_set['test_id']\n    predictions = clf.predict_proba(test_set[train_features])\n\n    submission = pd.DataFrame(test_ids)\n\n    prediction_set = []\n    for i in range(len(predictions)):\n        prediction_set.append(predictions[i][1])\n    \n    prediction_set = pd.DataFrame(prediction_set, columns=[target])\n    submission = pd.concat([submission, prediction_set], axis=1)\n    return submission","execution_count":46},{"outputs":[],"metadata":{"_uuid":"e5fc8d9f11413c6bd4af3cb6a95057a3d1fc1a2f","_cell_guid":"8a4544ae-63ff-40e5-926b-813d00f667d1","trusted":true,"_execution_state":"busy"},"cell_type":"code","source":"submission = generate_predictions(['len_diff', 'shared_words_q1'])","execution_count":47},{"outputs":[],"metadata":{"_uuid":"69f3c15af12526dbed212f405c0c69429f4d1fbb","_cell_guid":"9071a76d-599a-f34d-69b4-4f092eb9c850","trusted":true,"_execution_state":"busy"},"cell_type":"code","source":"print(set(pd.isnull(submission[target])))\nsubmission.to_csv(\"submission.csv\", index=False)","execution_count":48}],"nbformat":4,"metadata":{"language_info":{"version":"3.6.1","mimetype":"text/x-python","file_extension":".py","nbconvert_exporter":"python","codemirror_mode":{"version":3,"name":"ipython"},"pygments_lexer":"ipython3","name":"python"},"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"},"_is_fork":false,"_change_revision":0},"nbformat_minor":1}