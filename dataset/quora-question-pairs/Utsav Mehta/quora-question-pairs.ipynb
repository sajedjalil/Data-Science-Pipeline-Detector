{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Quora Question Pairs similarity prediction:"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport zipfile # to read/ extract zip files\nimport matplotlib.pyplot as plt # to plot graphs\nimport seaborn as sns # to plot graphs\n\n# ignoring any/ all types of warnings:\nimport warnings\nwarnings.filterwarnings(action='ignore')\n\nimport gc # to free-up memory occasionally\n\nimport tqdm\nfrom tqdm import notebook\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"scrolled":false},"cell_type":"code","source":"# Reading the zip file:\nzf = zipfile.ZipFile('/kaggle/input/quora-question-pairs/train.csv.zip')\ndf = pd.read_csv(zf.open('train.csv'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Basic Exploratory Data Analysis:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Dataframe info.:\")\nprint(df.info())\nprint(\"Example data points: \")\ndisplay(df.head(5))\nprint(\">> Size of the dataset: \", df.shape[0])\nprint(\">> Shape of the dataset: \", df.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Count of questions that are similar or different:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\">> Total number of questions pairs for training: \", df.shape[0])\nprint(\">> Questions that are similar: {}%\".format(round(df['is_duplicate'].value_counts(normalize='true')[0]*100,2)))\nprint(\">> Questions that are different: {}%\".format(round(df['is_duplicate'].value_counts(normalize='true')[1]*100,2)))\n\nplt.figure(figsize=(10,6))\ndf.groupby('is_duplicate')['id'].count().plot.bar()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Count of questions that are repeated or unique:"},{"metadata":{"trusted":true},"cell_type":"code","source":"qids = pd.Series(df['qid1'].tolist() + df['qid2'].tolist())\nunique_qids = len(np.unique(qids))\nqs_morethan_onetime = np.sum(qids.value_counts()>1)\nprint(\">> Total questions:\", len(qids))\nprint(\">> Total unique Questions:\", unique_qids)\nprint(\">> Total questions that repeat more than once: {} i.e. {:.2%} of unique questions\".format(qs_morethan_onetime,qs_morethan_onetime/unique_qids))\nprint(\">> Maximum no. of times a single question is repeated:\", qids.value_counts().values[0])\n\nx = ['unique questions', 'repeated questions']\ny = [unique_qids, qs_morethan_onetime]\nplt.figure(figsize=(10,6))\nplt.title('Plot representing the unique & repeated questions')\nsns.barplot(x,y)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Occurences of each question:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,6))\nplt.hist(qids.value_counts(), bins=160)\nplt.yscale('log', nonposy='clip')\nplt.title('Log-histogram of question appearance counts')\nplt.xlabel('Number of occurrences of question')\nplt.ylabel('Number of questions')\nprint('Maximum time a single question is repated is: ', qids.value_counts().values[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Checking for null values:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check which rows have null values:\nnan_rows = df[df.isnull().any(1)]\nnan_rows\n\n# Replace Nan values with whitespace and check again if the null values have been dealt with:\ndf = df.fillna(' ')\nnan_rows = df[df.isnull().any(1)]\nprint(nan_rows)\nprint(\">> No more null or NaN values.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Basic featurization:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#del df_basic\ndf_basic = df.copy()\ndisplay(df_basic.head(5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tqdm\nfrom tqdm import notebook\n\nquest1_len=[]\nquest2_len=[]\nquest1_word_count=[]\nquest2_word_count=[]\nword_total=[]\nword_common=[]\nword_shared=[]\n\nfor i in notebook.tqdm(df_basic['id']):\n    # Calculate the values:\n    q1=df_basic['question1'][i]\n    q2=df_basic['question2'][i]\n    q1_words = q1.split()\n    q2_words = q2.split()\n    total = len(set(q1_words)) + len(set(q2_words))\n    unique_common = len(set(q1_words)&set(q2_words))\n    \n    # Append in the lists accordingly:\n    quest1_len.append(len(q1)) # length of question 1\n    quest2_len.append(len(q2)) # length of question 2\n    quest1_word_count.append(len(q1_words)) # no. of words in question 1\n    quest2_word_count.append(len(q2_words)) # no. of words in question 2\n    word_total.append(total) # total number of unique words in question 1 & question 2\n    word_common.append(unique_common) # total number of common unique words in question 1 & question 2\n    word_shared.append(unique_common/ total) # ratio of common unique words & total number of words\n        \ndf_basic['quest1_len'] = quest1_len\ndf_basic['quest2_len'] = quest2_len\ndf_basic['quest1_word_count'] = quest1_word_count\ndf_basic['quest2_word_count'] = quest2_word_count\ndf_basic['word_total'] = word_total\ndf_basic['word_common'] = word_common\ndf_basic['word_shared'] = word_shared\n  \ndisplay(df_basic.head(5))\n\n#freeing up the memory\ngc.collect()\ndel quest1_len, quest2_len, quest1_word_count, quest2_word_count, word_total, word_common, word_shared","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Exploratory data analysis of basic features:\nChecking if any of these features are useful or not using the violin & pdf plots:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(22,8)) # setting the size of the plot\n\n# customizing sns plots, completely optional:\nsns.set_context(\"notebook\", font_scale=1.0)\nsns.set_style(\"whitegrid\")\n\n# 1st subplot - violin plot:\nplt.subplot(1,3,1)\nsns.violinplot(x='is_duplicate', y='word_common', data=df_basic)\n\n# 2nd subplot - box plot:\nplt.subplot(1,3,2)\nsns.boxplot(x='is_duplicate', y='word_common', data=df_basic)\n\n# 3rd subplot - pdf plot:\nplt.subplot(1,3,3)\nsns.distplot(df_basic[df_basic['is_duplicate']==0]['word_common'], label='0') # is_duplicate=0\nsns.distplot(df_basic[df_basic['is_duplicate']==1]['word_common'], label='1') # is_duplicate=1\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(22,8))\n\nsns.set_context(\"notebook\", font_scale=1.0)\nsns.set_style(\"whitegrid\")\n\nplt.subplot(1,3,1)\n#sns.set_theme()\nsns.violinplot(x='is_duplicate', y='word_shared', data=df_basic)\n\nplt.subplot(1,3,2)\nsns.boxplot(x='is_duplicate', y='word_shared', data=df_basic)\n\nplt.subplot(1,3,3)\nsns.distplot(df_basic[df_basic['is_duplicate']==0]['word_shared'], label='0') # is_duplicate=0\nsns.distplot(df_basic[df_basic['is_duplicate']==1]['word_shared'], label='1') # is_duplicate=1\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 'word_shared' definitely seems  more useful feature than 'word_common' by looking at the density plots as 'word_shared' has less area overalapping as compared to the 'word_common'"},{"metadata":{},"cell_type":"markdown","source":"## Pre-processing of the text data:\n1. Removing HTML tags\n2. Removing punctuations\n3. Performing Stemming\n4. Removing stop-words\n5. Expanding contractions, etc."},{"metadata":{"trusted":true},"cell_type":"code","source":"import regex as re\nfrom bs4 import BeautifulSoup\nfrom nltk.stem import *\n\n# Get all the HTML tags, special characters & puctuations and remove them:\ndef text_cleanup(text):\n    #print(\"Original text:\\n\", text)\n    soup = BeautifulSoup(text)\n    clean_text = soup.get_text(strip=True)\n    #print(\"After removing HTML tags:\\n\", clean_text)\n    \n    clean_text = clean_text.lower()\n    \n    clean_text = clean_text.replace(\",000,000\", \"m\").replace(\",000\", \"k\")\\\n                .replace(\"′\", \"'\").replace(\"’\", \"'\").replace(\"won't\", \"will not\")\\\n                .replace(\"cannot\", \"can not\").replace(\"can't\", \"can not\")\\\n                .replace(\"n't\", \" not\").replace(\"what's\", \"what is\").replace(\"it's\", \"it is\")\\\n                .replace(\"'ve\", \" have\").replace(\"i'm\", \"i am\").replace(\"'re\", \" are\")\\\n                .replace(\"he's\", \"he is\").replace(\"she's\", \"she is\")\\\n                .replace(\"%\", \" percent \").replace(\"₹\", \" rupee \").replace(\"$\", \" dollar \")\\\n                .replace(\"€\", \" euro \").replace(\"'ll\", \" will\")\n    clean_text = re.sub(r\"([0-9]+)000000\", r\"\\1m\", clean_text)\n    clean_text = re.sub(r\"([0-9]+)000\", r\"\\1k\", clean_text)\n    clean_text = clean_text.replace(\"'s\", \" own\")\n    \n    #print(\"After expanding the contractions:\\n\", clean_text)\n    \n    pattern = re.compile('\\W')\n    clean_text = re.sub(pattern, ' ', clean_text)\n#     if type(clean_text) == type(''):\n#         clean_text = re.sub(pattern, ' ', clean_text)\n    \n    #print(\"After removing special characters:\\n\", clean_text)\n    \n    \n    port = PorterStemmer()\n    #snow = SnowballStemmer('english')\n    \n#     if type(clean_text) = type(''):\n#         clean_text = port.stem(clean_text)\n    clean_text = port.stem(clean_text)\n    #print(\"After performing Porter Stemming: \", clean_text1)\n\n    #    clean_text = snow.stem(clean_text)\n    #clean_text2 = snow.stem(clean_text)\n    #print(\"After performing Snowball Stemming: \", clean_text2)\n\n    return clean_text","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Advanced featurization:\n\nDefinitions:\n> 1. **Token**: Token is obtained by splitting up the sentence.\n> 2. **Stop words**: Stop words as per NLTK\n> 3. **Word**: A token that is not a stop-word.\n\nFeatures:\n> 1. **cwc_min**: Ratio of common word count to the min(length(Q1),length(Q2))\n        cwc_min = common_word_count / min(length(Q1),length(Q2))\n> 2. **cwc_max**: Ratio of common word count to the max(length(Q1),length(Q2))\n        cwc_max = common_word_count / max(length(Q1),length(Q2))\n> 3. **csc_min**: Ratio of common stop-word count to the min(length(Q1),length(Q2))\n        csc_min = common_stop_count / min(length(Q1),length(Q2))\n> 4. **csc_max**: Ratio of common stop-word count to the max(length(Q1),length(Q2))\n        cwc_max = common_stop_count / max(length(Q1),length(Q2))\n> 5. **ctc_min**: Ratio of common token count to the min(length(Q1),length(Q2))\n        ctc_min = common_token_count / min(length(Q1),length(Q2))\n> 6. **ctc_max**: Ratio of common token count to the max(length(Q1),length(Q2))\n        ctc_min = common_token_count / max(length(Q1),length(Q2))\n> 7. **abs_len_diff**: Absolute token-length difference between both the questions\n        abs_len_diff = abs(length(Q1) - length(Q2))\n> 8. **mean_len**: Average token-length difference between both the questions\n        mean_len = [length(Q1) + length(Q2)] / 2\n> 9. **longest_substr_ratio**: Ratio of length of longest common substring to the   min(length(Q1),length(Q2))\n> 10. **last_word_eq**: Check if last word of both questions are same or not (boolean)\n> 11. **first_word_eq**: Check if first word of both questions are same or not (boolean)\n> 12. We'll be using fuzzywuzzy librarry to get these features - **fuzz_ratio**, **fuzz_partial_ratio**, **token_sort_ratio** & **token_set_ratio**     \nDeveloper's blog :http://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/     \nGithub link: https://github.com/seatgeek/fuzzywuzzy\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"!python -m pip install --upgrade pip\n!pip install fuzzywuzzy\n!pip install distance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from fuzzywuzzy import fuzz # to calculate fuzzy ratios\nfrom nltk.corpus import stopwords # to get the  stop-words\nimport distance # to get the similarity between two words\n\ndf_advanced = df.copy() #  making a copy of the dataframe\nstop_words = set(stopwords.words('english')) # getting the stop-words\n\n# for i in notebook.tqdm(list(set(df_advanced['qid1']))):\n#     quest1 = df_advanced[df_advanced['qid1']==i]['question1']\n#     df_advanced['question1'][df_advanced['qid1']==i] = text_cleanup(quest1.values[0])\n    \n# for i in notebook.tqdm(list(set(df_advanced['qid2']))):\n#     quest2 = df_advanced[df_advanced['qid2']==i]['question2']\n#     df_advanced['question2'][df_advanced['qid2']==i] = text_cleanup(quest2.values[0])\n\nquest1=[]\nquest2=[]\n\n# iterating through question1 column to pre-process the text:\nprint(\"Text-cleanup initiated - Part 1/2\")\nfor i in notebook.tqdm(df_advanced['question1']):\n    quest1.append(text_cleanup(i))\n\n# iterating through question2 column to pre-process the text:\nprint(\"Part 1 completed, Part 2/2 started...\")\nfor i in notebook.tqdm(df_advanced['question2']):\n    quest2.append(text_cleanup(i))\n\n# moving the cleaned questions/ text to the dataframe:\ndf_advanced['question1'] = quest1\ndf_advanced['question2'] = quest2\nprint(\"Part 2 completed, Text-cleanup completed!\")\n\ndisplay(df_advanced)\n\nprint('Advanced featurization initiated...')\n\n# initial values\nsafe_div = 0.0001 # using a constant to avoid division by zero error\ncwc_min=[]\ncwc_max=[]\ncsc_min=[]\ncsc_max=[]\nctc_min=[]\nctc_max=[]\nabs_len_diff=[]\nmean_len=[]\nlongest_substr_ratio=[]\nlast_word_eq=[]\nfirst_word_eq=[]\nlongest_substr_ratio=[]\nfuzz_ratio=[]\nfuzz_partial_ratio=[]\nfuzz_token_sort_ratio=[]\nfuzz_token_set_ratio=[]\n\nfor i in notebook.tqdm(df_advanced['id']):\n    # Storing the lengths & declaring the empty lists\n    q1=df_advanced['question1'][i]\n    q2=df_advanced['question2'][i]\n    q1_len = len(q1)\n    q2_len = len(q2)\n    q1_tokens = q1.split()\n    q2_tokens = q2.split()\n    q1_stop = set()\n    q2_stop = set()\n    q1_words = set()\n    q2_words = set()\n    for token in q1_tokens:\n        if token in stop_words:\n            q1_stop.add(token) # storing the question1 stop-words\n        else:\n            q1_words.add(token) # storing the question1 words\n    for token in q2_tokens:\n        if token in stop_words:\n            q2_stop.add(token) # storing the question2 stop-words\n        else:\n            q2_words.add(token) # storing the question2 words\n\n    unique_common_word = len(q1_words & q2_words) # storing the common words\n    unique_common_stop = len(q1_stop & q2_stop) # storing the common stop-words\n    unique_common_tokens = len(set(q1_tokens) & set(q2_tokens)) # storing the common tokens\n    \n    # Calculating the above mentioned features and storing them in lists:\n    cwc_min.append(unique_common_word/(min(len(q1_words),len(q2_words))+safe_div))\n    cwc_max.append(unique_common_word/(max(len(q1_words),len(q2_words))+safe_div))\n    csc_min.append(unique_common_stop/(min(len(q1_stop),len(q2_stop))+safe_div))\n    csc_max.append(unique_common_stop/(max(len(q1_stop),len(q2_stop))+safe_div))\n    ctc_min.append(unique_common_tokens/(min(len(q1_tokens),len(q2_tokens))+safe_div))\n    ctc_max.append(unique_common_tokens/(max(len(q1_tokens),len(q2_tokens))+safe_div))\n    abs_len_diff.append(abs(len(q1_tokens) - len(q2_tokens)))\n    mean_len.append((len(q1_tokens) + len(q2_tokens))/2)\n    try:\n        longest_substr_ratio.append(len(list(distance.lcsubstrings(q1,q2))[0])/(min(q1_len,q2_len)+1))\n    except IndexError as e:\n        longest_substr_ratio.append(0)\n\n    try:\n        first_word_eq.append(int(q1_tokens[0]==q2_tokens[0]))\n        last_word_eq.append(int(q1_tokens[-1]==q2_tokens[-1]))\n    except IndexError as e:\n        first_word_eq.append(0)\n        last_word_eq.append(0)\n    \n    fuzz_ratio.append(fuzz.ratio(q1,q2))\n    fuzz_partial_ratio.append(fuzz.partial_ratio(q1,q2))\n    fuzz_token_sort_ratio.append(fuzz.token_sort_ratio(q1,q2))\n    fuzz_token_set_ratio.append(fuzz.token_set_ratio(q1,q2))\n    \n# Moving the lists to the below new columns in the dataframe:\ndf_advanced['cwc_min'] = cwc_min\ndf_advanced['cwc_max'] = cwc_max\ndf_advanced['csc_min'] = csc_min\ndf_advanced['csc_max'] = csc_max\ndf_advanced['ctc_min'] = ctc_min\ndf_advanced['ctc_max'] = ctc_max\ndf_advanced['abs_len_diff'] = abs_len_diff\ndf_advanced['mean_length'] = mean_len\ndf_advanced['longest_substr_ratio'] = longest_substr_ratio\ndf_advanced['first_word_eq'] = first_word_eq\ndf_advanced['last_word_eq'] = last_word_eq\ndf_advanced['fuzz_ratio'] = fuzz_ratio\ndf_advanced['fuzz_partial_ratio'] = fuzz_partial_ratio\ndf_advanced['fuzz_token_sort_ratio'] = fuzz_token_sort_ratio\ndf_advanced['fuzz_token_set_ratio'] = fuzz_token_set_ratio\n\ngc.collect()\n\ndisplay(df_advanced)\n#df_advanced.to_csv(\"df_advanced.csv\",index=False)\n\nprint(\"Advanced featurization complete!\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis of the above calculated Advanced features:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n# import zipfile # to read/ extract zip files\n# import matplotlib.pyplot as plt # to plot graphs\n# import seaborn as sns # to plot graphs\n\n# # ignoring any/ all types of warnings:\n# import warnings\n# warnings.filterwarnings(action='ignore')\n\n# import gc # to free-up memory occasionally\n\n# df_advanced = pd.read_csv(\"../input/df-advanced/df_advanced.csv\")\n# display(df_advanced.head(5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Generating Word clouds for all the words that come under duplicate & non-duplicate question pairs:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS\n\nstop_words = set(STOPWORDS) # getting the stop-words\n\n# removing the below stopwords as they might contirbute information in this scenario:\nstop_words.remove('like')\nstop_words.remove('cannot')\nstop_words.remove('no')\nstop_words.remove('not')\n\n# getting all the words in question1 for all non duplicate question pairs:\nwords_in_non_duplicates=\"\"\nfor i in df_advanced[df_advanced['is_duplicate']==0]['question1'].values:\n    try:\n        i=i.strip()\n    except AttributeError as e:\n        i=' '.strip()\n    words_in_non_duplicates += i\n\n# getting all the words in question2 for all non duplicate question pairs:\nfor i in df_advanced[df_advanced['is_duplicate']==0]['question2'].values:\n    try:\n        i=i.strip()\n    except AttributeError as e:\n        i=' '.strip()\n    words_in_non_duplicates += i\n\nprint(\"Generating the word cloud...\")\n\ntotal_nodup_words = len(\" \".join(words_in_non_duplicates))\n\nprint(\"Word Cloud for NON-DUPLICATE question pairs:\")\nwordcloud = WordCloud(background_color='white',width=1200,height=400,max_words=total_nodup_words,stopwords=stop_words)\nwordcloud = wordcloud.generate(words_in_non_duplicates)\n\nplt.figure(figsize=(30,15))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()\nprint(\"Total no. of words in NON-DUPLICATE questions: \",total_nodup_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# getting all the words in question1 for all the duplicate question pairs:\nwords_in_duplicates=\"\"\nfor i in df_advanced[df_advanced['is_duplicate']==1]['question1'].values:\n    try:\n        i=i.strip()\n    except AttributeError as e:\n        i=' '.strip()\n    words_in_duplicates += i\n\n# getting all the words in question2 for all the duplicate question pairs:\nfor i in df_advanced[df_advanced['is_duplicate']==1]['question2'].values:\n    try:\n        i=i.strip()\n    except AttributeError as e:\n        i=' '.strip()\n    words_in_duplicates += i\n\nprint(\"Generating word cloud...\")\n\ntotal_dup_words = len(\" \".join(words_in_duplicates))\n    \nprint(\"Word Cloud for DUPLICATE question pairs:\")\nwordcloud = WordCloud(background_color='white',width=1200,height=400,max_words=total_dup_words,stopwords=stop_words)\nwordcloud = wordcloud.generate(words_in_duplicates)\n\nplt.figure(figsize=(30,15))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()\nprint(\"Total no. of words in NON-DUPLICATE questions: \",total_dup_words)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plotting pair-plots for all the combinations of the advanced features calculated above:"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_palette('muted') # setting the color palette\n#sns.set_style('dark')\nsns.set_context('notebook',font_scale=1.2) # setting the font scale\n#sns.set_style('ticks')\nplot = sns.pairplot(data=df_advanced[['csc_min','csc_max','cwc_min','cwc_max','is_duplicate']], # dataframe\n                    hue='is_duplicate', # set the class variable to colorize the plots\n                    height=3, # size of the plot\n                    corner=True) # remove the other identical diagonal plots \n#           ,plot_kws=dict(marker=\"+\", linewidth=1))\nplot.fig.suptitle(\"Pair-plots among COMMON WORDS & COMMON STOP-WORDS\",y=1.05) # set the title\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot = sns.pairplot(df_advanced[['cwc_min','cwc_max','ctc_min','ctc_max','is_duplicate']],\n             hue='is_duplicate',\n             height=3,\n             corner=True)\nplot.fig.suptitle(\"Pair-plots among COMMON WORDS & COMMON TOKENS\",y=1.05)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot = sns.pairplot(df_advanced[['ctc_min','ctc_max','csc_min','csc_max','is_duplicate']],\n             hue='is_duplicate',\n             height=3,\n             corner=True)\nplot.fig.suptitle(\"Pair-plots among COMMON TOKENS & COMMON STOP-WORDS\",y=1.05)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot = sns.pairplot(df_advanced[['ctc_min','csc_min','cwc_min','fuzz_ratio','is_duplicate']],\n             hue='is_duplicate',\n             height=3,\n             corner=True)\nplot.fig.suptitle(\"Pair-plots among MIN (COMMON TOKENS,WORDS, STOP-WORDS) & FUZZ RATIO\",y=1.05)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot = sns.pairplot(df_advanced[['ctc_max','csc_max','cwc_max','fuzz_ratio','is_duplicate']],\n             hue='is_duplicate',\n             height=3,\n             corner=True)\nplot.fig.suptitle(\"Pair-plots among MAX (COMMON TOKENS,WORDS, STOP-WORDS) & FUZZ RATIO\",y=1.05)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot = sns.pairplot(df_advanced[['ctc_min','csc_min','cwc_min','fuzz_partial_ratio','is_duplicate']],\n             hue='is_duplicate',\n             height=3,\n             corner=True)\nplot.fig.suptitle(\"Pair-plots among MIN (COMMON TOKENS,WORDS, STOP-WORDS) & FUZZ PARTIAL RATIO\",y=1.05)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot = sns.pairplot(df_advanced[['ctc_max','csc_max','cwc_max','fuzz_partial_ratio','is_duplicate']],\n             hue='is_duplicate',\n             height=3,\n             corner=True)\nplot.fig.suptitle(\"Pair-plots among MAX (COMMON TOKENS,WORDS, STOP-WORDS) & FUZZ PARTIAL RATIO\",y=1.05)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot = sns.pairplot(df_advanced[['ctc_min','csc_min','cwc_min','fuzz_token_set_ratio','is_duplicate']],\n             hue='is_duplicate',\n             height=3,\n             corner=True)\nplot.fig.suptitle(\"Pair-plots among MIN (COMMON TOKENS,WORDS, STOP-WORDS) & FUZZ TOKEN SET RATIO\",y=1.05)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot = sns.pairplot(df_advanced[['ctc_max','csc_max','cwc_max','fuzz_token_set_ratio','is_duplicate']],\n             hue='is_duplicate',\n             height=3,\n             corner=True)\nplot.fig.suptitle(\"Pair-plots among MAX (COMMON TOKENS,WORDS, STOP-WORDS) & FUZZ TOKEN SET RATIO\",y=1.05)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot = sns.pairplot(df_advanced[['ctc_min','csc_min','cwc_min','fuzz_token_sort_ratio','is_duplicate']],\n             hue='is_duplicate',\n             height=3,\n             corner=True)\nplot.fig.suptitle(\"Pair-plots among MIN (COMMON TOKENS,WORDS, STOP-WORDS) & FUZZ TOKEN SORT RATIO\",y=1.05)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot = sns.pairplot(df_advanced[['ctc_max','csc_max','cwc_max','fuzz_token_sort_ratio','is_duplicate']],\n             hue='is_duplicate',\n             height=3,\n             corner=True)\nplot.fig.suptitle(\"Pair-plots among MAX (COMMON TOKENS,WORDS, STOP-WORDS) & FUZZ TOKEN SORT RATIO\",y=1.05)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot = sns.pairplot(df_advanced[['fuzz_ratio','fuzz_partial_ratio','fuzz_token_set_ratio','fuzz_token_sort_ratio','is_duplicate']],\n             hue='is_duplicate',\n             height=3,\n             corner=True)\nplot.fig.suptitle(\"Pair-plots among FUZZ RATIO, FUZZ PARTIAL, TOKEN SET & TOKEN SORT RATIOS\",y=1.05)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot = sns.pairplot(df_advanced[['fuzz_ratio','fuzz_partial_ratio','cwc_min','cwc_max','is_duplicate']],\n             hue='is_duplicate',\n             height=3,\n             corner=True)\nplot.fig.suptitle(\"Pair-plots among COMMON WORDS, FUZZ RATIO & FUZZ PARTIAL RATIO\",y=1.05)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot = sns.pairplot(df_advanced[['fuzz_ratio','fuzz_partial_ratio','csc_min','csc_max','is_duplicate']],\n             hue='is_duplicate',\n             height=3,\n             corner=True)\nplot.fig.suptitle(\"Pair-plots among COMMON STOP-WORDS, FUZZ RATIO & FUZZ PARTIAL RATIO\",y=1.05)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot = sns.pairplot(df_advanced[['fuzz_ratio','fuzz_partial_ratio','ctc_min','ctc_max','is_duplicate']],\n             hue='is_duplicate',\n             height=3,\n             corner=True)\nplot.fig.suptitle(\"Pair-plots among COMMON TOKENS, FUZZ RATIO & FUZZ PARTIAL RATIO\",y=1.05)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot = sns.pairplot(df_advanced[['fuzz_token_set_ratio','fuzz_token_sort_ratio','cwc_min','cwc_max','is_duplicate']],\n             hue='is_duplicate',\n             height=3,\n             corner=True)\nplot.fig.suptitle(\"Pair-plots among COMMON WORDS, TOKEN SET RATIO & TOKEN SORT RATIO\",y=1.05)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot = sns.pairplot(df_advanced[['fuzz_token_set_ratio','fuzz_token_sort_ratio','csc_min','csc_max','is_duplicate']],\n             hue='is_duplicate',\n             height=3,\n             corner=True)\nplot.fig.suptitle(\"Pair-plots among COMMON STOP-WORDS, TOKEN SET RATIO & TOKEN SORT RATIO\",y=1.05)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot = sns.pairplot(df_advanced[['fuzz_token_set_ratio','fuzz_token_sort_ratio','ctc_min','ctc_max','is_duplicate']],\n             hue='is_duplicate',\n             height=3,\n             corner=True)\nplot.fig.suptitle(\"Pair-plots among COMMON TOKENS, TOKEN SET RATIO & TOKEN SORT RATIO\",y=1.05)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plotting violin, box & PDF plots of a fuzzy ratios:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(22,8)) # setting the size of the plot\n\n# customizing sns plots, completely optional:\nsns.set_context(\"notebook\", font_scale=1.0)\nsns.set_style(\"whitegrid\")\n\n# 1st subplot - violin plot:\nplt.subplot(1,3,1)\nsns.violinplot(x='is_duplicate', y='fuzz_ratio', data=df_advanced)\n\n# 2nd subplot - box plot:\nplt.subplot(1,3,2)\nsns.boxplot(x='is_duplicate', y='fuzz_ratio', data=df_advanced)\n\n# 3rd subplot - pdf plot:\nplt.subplot(1,3,3)\nsns.distplot(df_advanced[df_advanced['is_duplicate']==0]['fuzz_ratio'], label='0') # is_duplicate=0\nsns.distplot(df_advanced[df_advanced['is_duplicate']==1]['fuzz_ratio'], label='1') # is_duplicate=1\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(22,8)) # setting the size of the plot\n\n# customizing sns plots, completely optional:\nsns.set_context(\"notebook\", font_scale=1.0)\nsns.set_style(\"whitegrid\")\n\n# 1st subplot - violin plot:\nplt.subplot(1,3,1)\nsns.violinplot(x='is_duplicate', y='fuzz_token_set_ratio', data=df_advanced)\n\n# 2nd subplot - box plot:\nplt.subplot(1,3,2)\nsns.boxplot(x='is_duplicate', y='fuzz_token_set_ratio', data=df_advanced)\n\n# 3rd subplot - pdf plot:\nplt.subplot(1,3,3)\nsns.distplot(df_advanced[df_advanced['is_duplicate']==0]['fuzz_token_set_ratio'], label='0') # is_duplicate=0\nsns.distplot(df_advanced[df_advanced['is_duplicate']==1]['fuzz_token_set_ratio'], label='1') # is_duplicate=1\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(22,8)) # setting the size of the plot\n\n# customizing sns plots, completely optional:\nsns.set_context(\"notebook\", font_scale=1.0)\nsns.set_style(\"whitegrid\")\n\n# 1st subplot - violin plot:\nplt.subplot(1,3,1)\nsns.violinplot(x='is_duplicate', y='fuzz_token_sort_ratio', data=df_advanced)\n\n# 2nd subplot - box plot:\nplt.subplot(1,3,2)\nsns.boxplot(x='is_duplicate', y='fuzz_token_sort_ratio', data=df_advanced)\n\n# 3rd subplot - pdf plot:\nplt.subplot(1,3,3)\nsns.distplot(df_advanced[df_advanced['is_duplicate']==0]['fuzz_token_sort_ratio'], label='0') # is_duplicate=0\nsns.distplot(df_advanced[df_advanced['is_duplicate']==1]['fuzz_token_sort_ratio'], label='1') # is_duplicate=1\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### High-level observations from the pair-plots:\nBelow pair plots seem to separate the classes more than the rest of the combinations:\n1. **fuzz_ratio vs ctc_min**\n2. **fuzz_ratio cs cwc_min**\n3. **token_set_ratio vs csc_min**\n4. **token_sort_ratio vs ctc_min**"},{"metadata":{},"cell_type":"markdown","source":"### T-SNE visualizations of the advanced features: "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.manifold import TSNE\nfrom sklearn.preprocessing import MinMaxScaler\n\n#since this is a high-dimensional data, it'd be better to sample some rows and then perform T-SNE visualizations:\nsampled_df = df_advanced[0:10000]\nX_arr = MinMaxScaler().fit_transform(sampled_df.drop(['id','qid1','qid2','question1','question2','is_duplicate'],axis=1))\ny_arr = sampled_df[['is_duplicate']].values\n\nprint(\"Scaling completed!\")\n\nprint(\"Generating T-SNE visualizations, this may take time if the dataset has many rows...\")\n\ntsne2d_df = TSNE(\n    n_components=2,\n    init='random', # pca\n    random_state=101,\n    method='barnes_hut',\n    n_iter=1000,\n    verbose=2,\n    angle=0.5\n).fit_transform(X_arr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a dataframe to plot T-SNE:\ntsne_df = pd.DataFrame({'x':tsne2d_df[:,0], 'y':tsne2d_df[:,1]})\ntsne_df['label']=y_arr\ndisplay(tsne_df.head(5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.FacetGrid(tsne_df, hue='label', height = 8).map(plt.scatter, 'x', 'y').add_legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### In the above T-SNE visualization, there are some areas where the blue points are completely separated from the orange points, thus, we can say that there are definitely some combinations among the advanced features that separate both these classes."},{"metadata":{},"cell_type":"markdown","source":"## Featurization using vectorizers:"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install --upgrade pip\n!pip install spacy\nimport spacy\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nimport tqdm\nfrom tqdm import notebook\n\n# making sure everything is in string format:\ndf_advanced['question1'] = df_advanced['question1'].apply(lambda x: str(x))\ndf_advanced['question2'] = df_advanced['question2'].apply(lambda x: str(x))\n\ndf_advanced","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# merge texts\nquestions = list(df_advanced['question1']) + list(df_advanced['question2'])\n\ntfidf = TfidfVectorizer(lowercase=False,)\ntfidf.fit_transform(questions)\nprint(\"TF-IDF vectorizer created!\")\n\n# creating a dict with key:word and value:tf-idf score\nword2tfidf = dict(zip(tfidf.get_feature_names(), tfidf.idf_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* After we find TF-IDF scores, we convert each question to a weighted average of word2vec vectors by these scores.\n* Here we use a pre-trained GLOVE model which comes free with \"Spacy\". https://spacy.io/usage/vectors-similarity\n* It is trained on Wikipedia and therefore, it is stronger in terms of word semantics."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_advanced.to_pickle('df_advanced.df')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_advanced = pd.read_pickle('../input/df-advanced/df_advanced.df')\ndisplay(df_advanced)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# en_vectors_web_lg, which includes over 1 million unique vectors.\n#!python -m spacy download en_vectors_web_lg\n#nlp = spacy.load('en_vectors_web_lg')\n\nnlp = spacy.load('en_core_web_sm')\n\nquest1_vecs=[]\nprev_mean_vec_quest1=np.zeros((1,1))\n#count=0\nfor question in notebook.tqdm(df_advanced['question1']):\n    document = nlp(question)\n    #count+=1\n    try:\n        mean_vec_quest1 = np.zeros([len(document), len(document[0].vector)])\n        prev_mean_vec_quest1=mean_vec_quest1*0\n        \n        for word in document:\n            vec = word.vector\n            try:\n                tfidf_weight = word2tfidf[str(word)]\n            except:\n                tfidf_weight = 0\n            mean_vec_quest1+= vec * tfidf_weight\n        mean_vec_quest1 = mean_vec_quest1.mean(axis=0)\n        quest1_vecs.append(mean_vec_quest1)\n    except:\n#         print(mean_vec_quest1.shape)\n#         print(mean_vec_quest1)\n#         print(prev_mean_vec_quest1.shape)\n#         print(prev_mean_vec_quest1)\n        mean_vec_quest1 = prev_mean_vec_quest1\n        quest1_vecs.append(mean_vec_quest1)\n\n        \ndf_temp = pd.DataFrame(columns=['quest1_features_mean'])\ndf_temp['quest1_features_mean'] = quest1_vecs\ndf3_quest1 = pd.DataFrame(df_temp.quest1_features_mean.values.tolist(),index=df_advanced.index)\ndel df_temp\ndf3_quest1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# en_vectors_web_lg, which includes over 1 million unique vectors.\n#!python -m spacy download en_vectors_web_lg\n#nlp = spacy.load('en_vectors_web_lg')\n\nnlp = spacy.load('en_core_web_sm')\n\nquest2_vecs=[]\nprev_mean_vec_quest2=np.zeros((1,1))\n\nfor question in notebook.tqdm(df_advanced['question2']):\n    document = nlp(question)\n    try:\n        mean_vec_quest2 = np.zeros([len(document), len(document[0].vector)])\n        prev_mean_vec_quest2=mean_vec_quest2*0\n        for word in document:\n            vec = word.vector\n            try:\n                tfidf_weight = word2tfidf[str(word)]\n            except:\n                tfidf_weight = 0\n            mean_vec_quest2 = mean_vec_quest2 + (vec * tfidf_weight)\n        mean_vec_quest2 = mean_vec_quest2.mean(axis=0)\n        quest2_vecs.append(mean_vec_quest2)\n    except:\n#         print(mean_vec_quest2.shape)\n#         print(mean_vec_quest2)\n#         print(prev_mean_vec_quest2.shape)\n#         print(prev_mean_vec_quest2)\n        mean_vec_quest2 = prev_mean_vec_quest2\n        quest2_vecs.append(mean_vec_quest2)\n        \n    \ndf_temp = pd.DataFrame(columns=['quest2_features_mean'])\ndf_temp['quest2_features_mean'] = quest2_vecs\ndf3_quest2 = pd.DataFrame(df_temp.quest2_features_mean.values.tolist(),index=df_advanced.index)\ndel df_temp\ndf3_quest2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Merging the dataframes:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df3_quest1.index.name='id'\ndf3_quest2.index.name='id'\ndf_basic.set_index('id',inplace=True)\ndf_advanced.set_index('id',inplace=True)\ndf1 = df_basic.drop(['qid1','qid2','question1','question2','is_duplicate'],axis=1)\ndf2 = df_advanced.drop(['qid1','qid2','question1','question2'],axis=1)\n# df3_quest1 = pd.DataFrame(tfidf_weighted_df.quest1_features_mean.values.tolist(), index=tfidf_weighted_df.index)\n# df3_quest2 = pd.DataFrame(tfidf_weighted_df.quest2_features_mean.values.tolist(), index=tfidf_weighted_df.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Basic features: \")\ndisplay(df1.head())\n\nprint(\"Advanced features: \")\ndisplay(df2.head())\n\nprint(\"Question 1 vector features: \")\ndisplay(df3_quest1.head())\n\nprint(\"Question 2 vector features: \")\ndisplay(df3_quest2.head())\n\nprint(\"Total no. of features: \", df1.shape[1]+df2.shape[1]+df3_quest1.shape[1]+df3_quest2.shape[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = df1.merge(df2, on='id',how='left')\ndf1 = df1.merge(df3_quest1, on='id',how='left')\ndf1 = df1.merge(df3_quest2, on='id',how='left')\ndisplay(df1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.to_csv('finaldb.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install --upgrade pip command\n!pip install csv-to-sqlite\nimport csv_to_sqlite\nprint(\"----\")\n# all the usual options are supported\noptions = csv_to_sqlite.CsvOptions() \ninput_files = [\"./finaldb.csv\"] # pass in a list of CSV files\ncsv_to_sqlite.write_csv(input_files, \"finaldb.sqlite\", options)\nprint(\"Sqlite file created successfully!\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sqlite3\n\nconn = sqlite3.connect(\"./finaldb.sqlite\")\nc = conn.cursor()\ntable_names = c.execute(\"SELECT name from sqlite_master where type='table'\")\nprint(\"Tables in the database: \",table_names.fetchall()[0][0])\nconn.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Fetching rows...\")\nconn = sqlite3.connect(\"./finaldb.sqlite\")\nfinaldf = pd.read_sql_query(\n            \"SELECT * FROM finaldb;\",conn)\nconn.close()\nprint(\"Rows fetched successfully from the database!\")\n\nprint(\"Converting strings to numerics...\")\n# reading the data from database converts all the data into string,\n# thus we need to convert them back to numbers\ncols = list(finaldf.columns)\nfor i in notebook.tqdm(cols):\n    try:\n        finaldf[i] = finaldf[i].apply(pd.to_numeric)\n        #count+=1\n        #print(finaldf[i])\n    except ValueError as v:    \n        finaldf[i]=0\n        \nconn.close()\n\n# removing the rows that have any NaN values:\nfinaldf.dropna(axis=0, how=\"any\", thresh=None, subset=None, inplace=True)\n\ndisplay(finaldf)\n\ny = finaldf[['is_duplicate']]\nprint(type(y))\ndisplay(y)\n\nX = finaldf.drop(['is_duplicate'],axis=1)\ndisplay(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(X,y,test_size=0.3)\nprint(\"x_train:\", x_train.shape)\nprint(\"x_test:\", x_test.shape)\nprint(\"y_train:\", y_train.shape)\nprint(\"y_test:\", y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Applying machine learning models:"},{"metadata":{},"cell_type":"markdown","source":"### 1. Random model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This function plots the confusion matrices given y_i, y_i_hat.\ndef plot_confusion_matrix(test_y, predict_y):\n    C = confusion_matrix(test_y, predict_y)\n    # C = 9,9 matrix, each cell (i,j) represents number of points of class i are predicted class j\n    \n    A =(((C.T)/(C.sum(axis=1))).T)\n    #A2 =(C/C.sum(axis=1))\n    #divid each element of the confusion matrix with the sum of elements in that column\n    \n    # C = [[1, 2],\n    #     [3, 4]]\n    # C.T = [[1, 3],\n    #        [2, 4]]\n    # C.sum(axis = 1)  axis=0 corresonds to columns and axis=1 corresponds to rows in two diamensional array\n    # C.sum(axix =1) = [[3, 7]]\n    # ((C.T)/(C.sum(axis=1))) = [[1/3, 3/7]\n    #                           [2/3, 4/7]]\n\n    # ((C.T)/(C.sum(axis=1))).T = [[1/3, 2/3]\n    #                           [3/7, 4/7]]\n    # sum of row elements = 1\n    \n    B =(C/C.sum(axis=0))\n    #divid each element of the confusion matrix with the sum of elements in that row\n    # C = [[1, 2],\n    #     [3, 4]]\n    # C.sum(axis = 0)  axis=0 corresonds to columns and axis=1 corresponds to rows in two diamensional array\n    # C.sum(axix =0) = [[4, 6]]\n    # (C/C.sum(axis=0)) = [[1/4, 2/6],\n    #                      [3/4, 4/6]] \n    plt.figure(figsize=(20,4))\n    \n    labels = [0,1]\n    # representing C in heatmap format\n    cmap=sns.light_palette(\"orange\")\n    plt.subplot(1, 3, 1)\n    sns.heatmap(C, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.title(\"Confusion matrix\")\n    \n    plt.subplot(1, 3, 2)\n    # representing B in heatmap format\n    sns.heatmap(B, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.title(\"Precision matrix\")\n    \n    plt.subplot(1, 3, 3)\n    # representing A in heatmap format\n    sns.heatmap(A, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.title(\"Recall matrix\")\n    \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tqdm\nfrom tqdm import notebook\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics.classification import accuracy_score, log_loss\n\ntest_len = len(y_test)\n\n# Random model:\nrand_y_pred = np.zeros((test_len,2))\nfor i in notebook.tqdm(range(test_len)):\n    rand_probas = np.random.rand(1,2)\n#     print(rand_probas)\n#     print(sum(rand_probas))\n#     print(sum(sum(rand_probas)))\n#     print((rand_probas/sum(sum(rand_probas))))\n#     break\n    rand_y_pred[i] = ((rand_probas/sum(sum(rand_probas)))[0])\nprint(\"Log-loss on test data using a random model: \", log_loss(y_test,rand_y_pred))\nprint(rand_y_pred.shape)\nrand_y_pred = np.argmax(rand_y_pred,axis=1)\nplot_confusion_matrix(y_test,rand_y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Logistic Regression with hyperparameter tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"# HYPER-PARAMETER TUNING:\n\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\n\nalpha = [10 ** x for x in range(-1, 5)] # hyperparam for SGD classifier.\n\n# read more about SGDClassifier() at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n# ------------------------------\n# default parameters\n# SGDClassifier(loss=’hinge’, penalty=’l2’, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None, \n# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=’optimal’, eta0=0.0, power_t=0.5, \n# class_weight=None, warm_start=False, average=False, n_iter=None)\n\n# some of methods\n# fit(X, y[, coef_init, intercept_init, …])\tFit linear model with Stochastic Gradient Descent.\n# predict(X)\tPredict class labels for samples in X.\n\n#-------------------------------\n# video link: \n#------------------------------\n\nprint(\"Checking log loss of the hyperparameter alpha for the values - \",alpha)\nlog_error_array=[]\nfor i in notebook.tqdm(alpha):\n    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n    clf.fit(x_train, y_train)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\",cv='prefit')\n    sig_clf.fit(x_train, y_train)\n    log_pred_y = sig_clf.predict_proba(x_test)\n    loss=log_loss(y_test, log_pred_y, labels=clf.classes_, eps=1e-15)\n    log_error_array.append(loss)\n    print('For value of alpha = ', i, \", the log loss is:\",loss)\n\nprint(\"Generating the plot to determine best alpha...\")\nfig, ax = plt.subplots()\nax.plot(alpha, log_error_array,c='g')\nfor i, txt in enumerate(np.round(log_error_array,3)):\n    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha using log loss (lower is better)\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying Logistic regression with the best alpha value obtained above:\nprint(\"Training for Logistic regression has started...\")\nbest_alpha = np.argmin(log_error_array)\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\nclf.fit(x_train, y_train)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\",cv='prefit')\nsig_clf.fit(x_train, y_train)\nprint(\"Training completed!\")\n\nprint(\"Generating confusion matrices to compare with the random model...\")\nlog_pred_y = sig_clf.predict_proba(x_train)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, log_pred_y, labels=clf.classes_, eps=1e-15))\nlog_pred_y = sig_clf.predict_proba(x_test)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, log_pred_y, labels=clf.classes_, eps=1e-15))\nlog_pred_y =np.argmax(log_pred_y,axis=1)\nprint(\"Total number of data points :\", len(log_pred_y))\nplot_confusion_matrix(y_test, log_pred_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. Linear SVM with hyperparameter tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"# HYPER-PARAMETER TUNING:\n\nalpha = [10 ** x for x in range(-3, 3)] # hyperparam for SGD classifier.\n\n# read more about SGDClassifier() at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n# ------------------------------\n# default parameters\n# SGDClassifier(loss=’hinge’, penalty=’l2’, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None, \n# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=’optimal’, eta0=0.0, power_t=0.5, \n# class_weight=None, warm_start=False, average=False, n_iter=None)\n\n# some of methods\n# fit(X, y[, coef_init, intercept_init, …])\tFit linear model with Stochastic Gradient Descent.\n# predict(X)\tPredict class labels for samples in X.\n\n#-------------------------------\n# video link: \n#------------------------------\n\nprint(\"Checking log-loss of the hyperparameter alpha for the values - \",alpha)\nlog_error_array=[]\nfor i in alpha:\n    clf = SGDClassifier(alpha=i, penalty='l1', loss='hinge', random_state=42)\n    clf.fit(x_train, y_train)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\", cv='prefit')\n    sig_clf.fit(x_train, y_train)\n    linsvm_pred_y = sig_clf.predict_proba(x_test)\n    log_error_array.append(log_loss(y_test, linsvm_pred_y, labels=clf.classes_, eps=1e-15))\n    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_test, linsvm_pred_y, labels=clf.classes_, eps=1e-15))\n\nprint(\"Generating the plot to determine best alpha...\")\nfig, ax = plt.subplots()\nax.plot(alpha, log_error_array,c='g')\nfor i, txt in enumerate(np.round(log_error_array,3)):\n    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha using hinge loss (lower is better)\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying the Logistic regression algorithm with the best alpha obtained above:\nprint(\"Training for Linear SVM has started...\")\nbest_alpha = np.argmin(log_error_array)\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l1', loss='hinge', random_state=42)\n#alpha=0.6597672918027971\n#clf = SGDClassifier(alpha=alpha, penalty='l1', loss='hinge', random_state=42)\nclf.fit(x_train, y_train)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\",cv='prefit')\nsig_clf.fit(x_train, y_train)\nprint(\"Training completed!\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Generating confusion matrices to compare with the random model...\")\nlinsvm_pred_y = sig_clf.predict_proba(x_train)\n#print('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, linsvm_pred_y, labels=clf.classes_, eps=1e-15))\nprint('For values of best alpha = ', alpha, \"The train log loss is:\",log_loss(y_train, linsvm_pred_y, labels=clf.classes_, eps=1e-15))\nlinsvm_pred_y = sig_clf.predict_proba(x_test)\n#print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, linsvm_pred_y, labels=clf.classes_, eps=1e-15))\nprint('For values of best alpha = ', alpha, \"The test log loss is:\",log_loss(y_test, linsvm_pred_y, labels=clf.classes_, eps=1e-15))\nlinsvm_pred_y =np.argmax(linsvm_pred_y,axis=1)\nprint(\"Total number of data points :\", len(linsvm_pred_y))\nplot_confusion_matrix(y_test, linsvm_pred_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4. GBDT using XGBoost with hyperparameter tuning:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\n\nd_train = xgb.DMatrix(x_train,label=y_train)\nd_test = xgb.DMatrix(x_test,label=y_test)\nwatchlist = [(d_train,'train'),(d_test,'valid')]\n#bst = xgb.train(parms,d_train,400,watchlist,early_stopping_rounds=20,verbose_eval=10)\n\neta = [10 ** x for x in range(-3,3)]\n\nprint(\"Checking log-loss of the hyperparameter eta for the values - \",eta)\nlog_error_array=[]\nparms={}\nparms['objective'] = 'binary:logistic'\nparms['eval_metric'] = 'logloss'\nparms['max_depth']=4\nfor i in eta:\n    print(\"For the value of eta = \",i,\", below are the train & test loss: \")\n    parms['eta'] = i\n    xgdmat = xgb.DMatrix(x_train,y_train)\n    bst = xgb.train(parms,d_train,50,watchlist,early_stopping_rounds=0,verbose_eval=10)\n    xgb_pred_y = bst.predict(d_test)\n    #loss=logloss(y_test,xgb_pred_y,labels=clf.classes_,eps=1e-15)\n    #log_error_array.append(loss)\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### From the above output, eta=1 seems the best values as the loss is minimum as compared to the other values. So let's check for even better value:"},{"metadata":{"trusted":true},"cell_type":"code","source":"d_train = xgb.DMatrix(x_train,label=y_train)\nd_test = xgb.DMatrix(x_test,label=y_test)\nwatchlist = [(d_train,'train'),(d_test,'valid')]\n#bst = xgb.train(parms,d_train,400,watchlist,early_stopping_rounds=20,verbose_eval=10)\nparms={}\nparms['objective'] = 'binary:logistic'\nparms['eval_metric'] = 'logloss'\nparms['max_depth']=4\nparms['eta'] = 1\nxgdmat = xgb.DMatrix(x_train,y_train)\nbst = xgb.train(parms,d_train,400,watchlist,early_stopping_rounds=20,verbose_eval=10)\nxgb_pred_y = bst.predict(d_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying the XGBoost algorithm with the best eta obtained above:\nprint(\"Training for XGBoosted Gradient Decision Tree has started...\")\nd_train = xgb.DMatrix(x_train,label=y_train)\nd_test = xgb.DMatrix(x_test,label=y_test)\nwatchlist = [(d_train,'train'),(d_test,'valid')]\n#best_eta = np.argmin(log_error_array)\nparms['objective'] = 'binary:logistic'\nparms['eval_metric'] = 'logloss'\nparms['max_depth']=4\nparms['eta'] = 1\nxgdmat = xgb.DMatrix(x_train,y_train)\nxgb_pred_y = bst.predict(d_test)\n#loss=logloss(y_test,predict_y,labels=clf.classes_,eps=1e-15)\n#clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l1', loss='hinge', random_state=42)\n#alpha=0.6597672918027971\n#clf = SGDClassifier(alpha=alpha, penalty='l1', loss='hinge', random_state=42)\n#clf.fit(x_train, y_train)\n#sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\",cv='prefit')\n#sig_clf.fit(x_train, y_train)\nprint(\"Training completed!\")\n\n# Plotting the confusion matrix:\nxgb_pred_y =np.array(xgb_pred_y>0.5,dtype=int)\nprint(\"Total number of data points :\", len(xgb_pred_y))\nplot_confusion_matrix(y_test, xgb_pred_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We can see that the overall score of confusion matrix, precision & recall is much better obtained by GBDT (using XGBoost), thus GBDT would be the best algorithm to go with in this particular scenario."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}