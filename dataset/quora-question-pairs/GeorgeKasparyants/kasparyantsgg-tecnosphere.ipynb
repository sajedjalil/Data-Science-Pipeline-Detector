{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dfcd3eed-f542-496f-2f6c-5e365c9ca09f"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport nltk\nfrom collections import Counter\nfrom nltk.corpus import stopwords\n\nfrom sklearn.metrics import log_loss\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b93799a9-e8b2-3384-61cd-9e7f366574c8"},"outputs":[],"source":"df_train = pd.read_csv('../input/train.csv')\ndf_test = pd.read_csv('../input/test.csv')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"87f2b3fa-44d9-b446-4edf-3ae0fbef15d1"},"outputs":[],"source":"print(df_train.info())\nprint(df_test.info())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3e7e8f0e-e992-9c90-f511-538687fa6098"},"outputs":[],"source":"from nltk.corpus import stopwords\n\nstops = set(stopwords.words(\"english\"))\n\ndef WordMatch(row):\n    q1 = set(str(row['question1']).split())\n    q2 = set(str(row['question2']).split())\n    a = len(q1.union(q2).difference(stops))\n    if (a == 0):\n        return 0\n    else:\n        return (len(q1.intersection(q2).difference(stops)) + .0) / a"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ac4b3500-c7b5-6387-d71f-a3d5c290cb43"},"outputs":[],"source":"#Чистка\ndf_train.question1 = df_train.question1.map(lambda x : str(x).lower())\ndf_train.question2 = df_train.question2.map(lambda x : str(x).lower())\ndf_train.is_duplicate = df_train.is_duplicate.fillna(0)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"342f7da9-0813-f13b-b978-5149f75519a5"},"outputs":[],"source":"df_train['WordMatch'] = df_train.apply(WordMatch, axis=1, raw=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"84880ccf-42b7-ddd4-3b31-4f49fd4829d4"},"outputs":[],"source":"sns.distplot(df_train[df_train['is_duplicate']==0].WordMatch, kde=False)\nsns.distplot(df_train[df_train['is_duplicate']==1].WordMatch, kde=False)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"eea45b4d-8a2b-ad29-4688-d79ca18705e1"},"outputs":[],"source":"train_qs = pd.Series(df_train['question1'].tolist() + df_train['question2'].tolist()).astype(str)\ntest_qs = pd.Series(df_test['question1'].tolist() + df_test['question2'].tolist()).astype(str)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3b38d6f1-a1b1-83a4-c8d8-aa5cfdf0e6c4"},"outputs":[],"source":"# Считаем повторяемость слов\n\ndef get_weight(count, eps=10000, min_count=2):\n    if count < min_count:\n        return 0\n    else:\n        return 1 / (count + eps)\n    \nwords = (\" \".join(train_qs)).lower().split()\ncounts = Counter(words)\nweights = {word: get_weight(count) for word, count in counts.items()}"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"64809f16-830e-3a06-6ae2-a1fabc09dd6f"},"outputs":[],"source":"def tfidf(row):\n    q1 =  set(str(row['question1']).split()).difference(stops)\n    q2 =  set(str(row['question2']).split()).difference(stops)\n    \n    if len(q1) == 0 or len(q2) == 0:        \n        return 0\n    inter = q1.intersection(q2)\n    \n    shared_weights = [2 * weights.get(w, 0) for w in inter]\n    total_weights = [weights.get(w, 0) for w in q1] + [weights.get(w, 0) for w in q2]\n    \n    R = np.sum(shared_weights) / (np.sum(total_weights) + 0.01)\n    return R"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"daf01bb8-c149-8950-08b5-457abc448af8"},"outputs":[],"source":"df_train['tfidf'] = df_train.apply(tfidf, axis=1, raw=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2921fde4-d4eb-a22e-1153-fd61b7e39273"},"outputs":[],"source":"sns.distplot(df_train[df_train['is_duplicate']==0].tfidf, kde=False)\nsns.distplot(df_train[df_train['is_duplicate']==1].tfidf, kde=False)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ce52d4af-02c7-28f5-cc56-f81e875d1ddb"},"outputs":[],"source":"import xgboost as xgb\n# Параметры\nparams = {}\nparams['objective'] = 'binary:logistic'\nparams['eval_metric'] = 'logloss'\nparams['eta'] = 0.02\nparams['max_depth'] = 4"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2792d90c-1e53-d282-f4b7-98cf316802d7"},"outputs":[],"source":"df_test['tfidf'] = df_test.apply(tfidf, axis=1, raw=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5dbce4b2-4d13-1639-4988-4982b09e4189"},"outputs":[],"source":"df_test['WordMatch'] = df_test.apply(WordMatch, axis=1, raw=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2f7d3eaa-6e7e-714b-ac9c-9842d697dc8a"},"outputs":[],"source":"x_train = df_train.drop(['question1', 'question2', 'qid1', 'qid2', 'is_duplicate', 'id'], axis=1).values\ny_train = df_train.is_duplicate.values\nx_test = df_test.drop(['question1', 'question2', 'test_id'], axis=1).values"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"32163cb4-8d6c-6b39-fa9c-1e252612161e"},"outputs":[],"source":"d_train = xgb.DMatrix(x_train, label=y_train)\nwatchlist = [(d_train, 'train')]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"52c3ab35-9b55-543c-589d-ca12720720c2"},"outputs":[],"source":"bst = xgb.train(params, d_train, 400, watchlist, early_stopping_rounds=50, verbose_eval=10)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e2dc1001-6838-8c86-7999-e62ca788db5a"},"outputs":[],"source":"d_test = xgb.DMatrix(x_test)\ny_test = bst.predict(d_test)\n\nsub = pd.DataFrame()\nsub['test_id'] = df_test['test_id']\nsub['is_duplicate'] = y_test\nsub.to_csv('sub.csv', index=False)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"93b52fc0-a1a1-0b37-4305-532394e95d07"},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8156d5ec-127a-2361-730e-a00d0d0eecef"},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"39dfa8bd-da7c-9005-e8ac-2678d8d967e2"},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"51e72535-28d6-4b71-43a6-a31ad3a6b544"},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4994a279-aea5-f993-c21f-8306bcb20408"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}