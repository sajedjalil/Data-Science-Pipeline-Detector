{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0,"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"548b9ee4-34b0-c9d6-d898-cd04bea118f5","_active":false,"collapsed":false},"source":"practice with the notebook of anokas","execution_count":null,"outputs":[],"execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cd5df4a4-836f-a7be-64bf-9ed3a608894c","_active":false},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","execution_state":"idle"},{"metadata":{"_cell_guid":"812b31af-a0df-e252-72ba-af7dd45bcd1e","_active":false,"collapsed":false},"source":"import numpy as np\nimport pandas as pd\nimport os\nimport gc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\npal = sns.color_palette()\n\n","execution_count":null,"cell_type":"code","outputs":[],"execution_state":"idle"},{"metadata":{"_cell_guid":"348bec2a-8544-962b-ac0e-a5c355868b32","_active":false,"collapsed":false},"source":"df_train = pd.read_csv(\"../input/train.csv\")\ndf_train.head()","execution_count":null,"cell_type":"code","outputs":[],"execution_state":"idle"},{"metadata":{"_cell_guid":"118f96cc-c702-6e2a-683a-003f7a827151","_active":false,"collapsed":false},"source":"print(\"Total number of question pairs for training : {}\".format(len(df_train)))\nprint('Duplicate pairs: {}%'.format(round(df_train['is_duplicate'].mean()*100, 2)))\nqids = pd.Series(df_train['qid1'].tolist() + df_train['qid2'].tolist())\nprint('Total number of questions in the training data: {}'.format(len(np.unique(qids))))\nprint('Number of questions that appear multiple times : {}'.format(np.sum(qids.value_counts() > 1)))\n\nplt.figure(figsize=(12, 5))\nplt.hist(qids.value_counts(), bins=50)\nplt.yscale('log', nonposy='clip')\nplt.title('log-histogram of question appearance counts')\nplt.xlabel('number of occurences of question')\nplt.ylabel('number of quesitons')\nprint()","execution_count":null,"cell_type":"code","outputs":[],"execution_state":"idle"},{"metadata":{"_cell_guid":"b464449c-2f17-984a-1cc9-fd5552df36b4","_active":false,"collapsed":false},"source":"from sklearn.metrics import log_loss\n\np = df_train['is_duplicate'].mean()\nprint('Prediected score : ', log_loss(df_train['is_duplicate'], np.zeros_like(df_train['is_duplicate']) + p ))\n\ndf_test = pd.read_csv('../input/test.csv')\nsub = pd.DataFrame({'test_id': df_test['test_id'], 'is_duplicate': p})\nsub.to_csv('naive_submission.csv', index=False)\nsub.head()\n","execution_count":null,"cell_type":"code","outputs":[],"execution_state":"idle"},{"metadata":{"_cell_guid":"f6fb926c-cfa0-d69e-c80f-436d3f3b612d","_active":false,"collapsed":false},"source":"df_test.head()","execution_count":null,"cell_type":"code","outputs":[],"execution_state":"idle"},{"metadata":{"_cell_guid":"6308679b-bdf0-0334-06d0-94b5b12db701","_active":false,"collapsed":false},"source":"print('Total number of question pairs for testing: {}'.format(len(df_test)))","execution_count":null,"cell_type":"code","outputs":[],"execution_state":"idle"},{"metadata":{"_cell_guid":"d92dd58d-8fcc-4f57-cace-54425cdeb54c","_active":false,"collapsed":false},"source":"train_qs = pd.Series(df_train['question1'].tolist() + df_train['question2'].tolist()).astype(str)\ntest_qs = pd.Series(df_test['question1'].tolist() + df_test['question2'].tolist()).astype(str)\n\ndist_train = train_qs.apply(len)\ndist_test = test_qs.apply(len)\nplt.figure(figsize=(15, 10))\nplt.hist(dist_train, bins=200, range=[0, 200], color=pal[2], normed=True, label='train')\nplt.hist(dist_test, bins=200, range=[0, 200], color=pal[1], normed=True, alpha=0.5, label='test')\nplt.title('Normalised histogram of character count in questions', fontsize=15)\nplt.legend()\nplt.xlabel('Number of characters', fontsize=15)\nplt.ylabel('Probability', fontsize=15)\n\nprint('mean-train {:.2f} std-train {:.2f} mean-test {:.2f} std-test {:.2f} max-train {:.2f} max-test {:.2f}'.format(dist_train.mean(),\n                                                                                                                    dist_train.std(),\n                                                                                                                    dist_test.mean(),\n                                                                                                                    dist_test.std(),\n                                                                                                                    dist_train.max(),\n                                                                                                                    dist_test.max()))","execution_count":null,"cell_type":"code","outputs":[],"execution_state":"idle"},{"metadata":{"_cell_guid":"3c1a6f76-c83b-b593-c97f-7d4bda652c86","_active":false,"collapsed":false},"source":"dist_train = train_qs.apply(lambda x : len(x.split(' ')))\ndist_test = test_qs.apply(lambda x : len(x.split(' ')))\n\nplt.figure(figsize=(15, 10))\nplt.hist(dist_train, bins=50, range=[0, 50], color=pal[2], normed=True, label='train')\nplt.hist(dist_test, bins=50, range=[0, 50], color=pal[1], normed=True, alpha=0.5, label='test')\nplt.title('Normalised histogram of word count in questions', fontsize=15)\nplt.legend()\nplt.xlabel('Number of words', fontsize=15)\nplt.ylabel('Probability', fontsize=15)\n\nprint('mean-train {:.2f} std-train {:.2f} mean-test {:.2f} std-test {:.2f} max-train {:.2f} max-test {:.2f}'.format(dist_train.mean(), dist_train.std(), dist_test.mean(), dist_test.std(), dist_train.max(), dist_test.max()))","execution_count":null,"cell_type":"code","outputs":[],"execution_state":"idle"},{"metadata":{"_cell_guid":"2c05de3e-519d-5ab2-bc1f-f576c1329e5c","_active":false,"collapsed":false},"source":"from wordcloud import WordCloud\ncloud = WordCloud(width = 1440, height=1080).generate(\" \".join(train_qs.astype(str)))\nplt.figure(figsize=(20, 15))\nplt.imshow(cloud)\nplt.axis('off')","execution_count":38,"cell_type":"code","outputs":[],"execution_state":"idle"},{"metadata":{"_cell_guid":"f3bf7390-446f-1990-9120-3b453eb9069b","_active":false,"collapsed":false},"source":"qmarks = np.mean(train_qs.apply(lambda x : '?' in x))\nmath = np.mean(train_qs.apply(lambda x : '[math]' in x))\nfullstop = np.mean(train_qs.apply(lambda x : '.' in x))\ncapital_fist = np.mean(train_qs.apply(lambda x : x[0].isupper()))\ncapitals = np.mean(train_qs.apply(lambda x : max([y.isupper() for y in x])))\nnumbers = np.mean(train_qs.apply(lambda x : max([y.isdigit() for y in x])))\n","execution_count":41,"cell_type":"code","outputs":[],"execution_state":"idle"},{"metadata":{"_cell_guid":"3e582008-514c-28f8-2ae3-3d4125884cd7","_active":false,"collapsed":false},"source":"# np.mean([True, True, False])","execution_count":44,"cell_type":"code","outputs":[],"execution_state":"idle"},{"metadata":{"_cell_guid":"101b26b2-5576-b492-47fa-cb72cb3c24e1","_active":false,"collapsed":false},"source":"from nltk.corpus import stopwords\n\nstops = set(stopwords.words(\"english\"))\n\ndef word_match_share(row):\n    q1words={}\n    q2words={}\n    for word in str(row['question1']).lower().split():\n        if word not in stops:\n            q1words[word] = 1\n    for word in str(row['question2']).lower().split():\n        if word not in stops:\n            q2words[word] = 1\n    if len(q1words) == 0 or len(q2words) == 0:\n        return 0\n    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n    R = (len(shared_words_in_q1) + len(shared_words_in_q2))/(len(q1words) + len(q2words))\n    \n    return R\n\nplt.figure(figsize=(15, 5))\ntrain_word_match = df_train.apply(word_match_share, axis=1, raw=True)\n\nplt.hist(train_word_match[df_train['is_duplicate'] == 0], bins=20, normed=True, label='Not Duplicate')\nplt.hist(train_word_match[df_train['is_duplicate'] == 1], bins=20, normed=True, alpha=0.7, label='Duplicate')\nplt.legend()\nplt.title('Label distribution over word_match_share', fontsize=15)\nplt.xlabel('word_match_share', fontsize=15)","execution_count":52,"cell_type":"code","outputs":[],"execution_state":"idle"},{"metadata":{"_cell_guid":"55c30ed4-1382-4f0d-e2b7-e3a855d68228","_active":true,"collapsed":false},"source":"# find conditional raw with specific index\ntrain_word_match[df_train['is_duplicate'] == 1]","execution_count":55,"cell_type":"code","outputs":[],"execution_state":"idle"},{"metadata":{"_cell_guid":"26ae4719-c5b2-48c5-6fd1-fbf3991a9912","_active":false,"collapsed":false},"source":"# TFIDF","execution_count":null,"cell_type":"code","outputs":[]}]}