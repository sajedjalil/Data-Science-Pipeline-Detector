{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"cef907a7-4698-47c8-fe5a-c8392dc30fe1"},"source":"I plan to use Word2Vec to convert each question into a word vector. Then I will use a Siamese neural network to detect if the pair is duplicate"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0e94f9c4-1d65-c360-0dac-36c835df7444"},"outputs":[],"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport gc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\npal = sns.color_palette()\n\nprint('# File sizes')\nfor f in os.listdir('../input'):\n    if 'zip' not in f:\n        print(f.ljust(30) + str(round(os.path.getsize('../input/' + f) / 1000000, 2)) + 'MB')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3d42e5d7-3234-3f1c-f3dc-8f7b24360017"},"outputs":[],"source":"df_train = pd.read_csv('../input/train.csv')\ndf_train.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"0d83befc-fc2c-01b1-3126-82a8759e4cec"},"source":"We are given a minimal number of data fields here, consisting of:\n\n* **id:** Looks like a simple rowID\n* **qid{1, 2}:** The unique ID of each question in the pair\n* **question{1, 2}:** The actual textual contents of the questions.\n* **is_duplicate:** The label that we are trying to predict - whether the two questions are duplicates of each other.\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"03bfcee8-ec94-0928-c471-5f2b6fc9915d"},"outputs":[],"source":"print('Total number of question pairs for training: {}'.format(len(df_train)))\nprint('Duplicate pairs: {}%'.format(round(df_train['is_duplicate'].mean()*100, 2)))\nqids = pd.Series(df_train['qid1'].tolist() + df_train['qid2'].tolist())\nprint('Total number of questions in the training data: {}'.format(len(\n    np.unique(qids))))\nprint('Number of questions that appear multiple times: {}'.format(np.sum(qids.value_counts() > 1)))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4310c5a5-4faa-ac08-3dd2-e94c074f3bf0"},"outputs":[],"source":"df_test = pd.read_csv('../input/test.csv')\ndf_test.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"1fa11637-27cf-b4d2-b3f2-a39cc1c6457f"},"source":"Encode questions to unicode"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ad0f965c-eb63-93b3-29dd-e42397b702b5"},"outputs":[],"source":"# encode questions to unicode\ndf_train['question1'] = df_train['question1'].apply(lambda x: str(x).encode(\"utf-8\"))\ndf_train['question2'] = df_train['question2'].apply(lambda x: str(x).encode(\"utf-8\"))\ndf_test['question1'] = df_test['question1'].apply(lambda x: str(x).encode(\"utf-8\"))\ndf_test['question2'] = df_test['question2'].apply(lambda x: str(x).encode(\"utf-8\"))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e0dd8608-a0c5-80bb-ef3c-e335882cf1d0"},"outputs":[],"source":"import gensim\nimport sys\nfrom tqdm import tqdm"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"193b9bd9-1e99-4429-ae8c-c4e1057ad089"},"outputs":[],"source":"### Train a GLOVE using Gensim\n\n\nquestions = list(df_train['question1']) + list(df_train['question2'])\n\n# tokenize\nc = 0\nfor question in tqdm(questions):\n    questions[c] = list(gensim.utils.tokenize(question, deacc=True, lower=True))\n    c += 1\n\n# train model\nmodel = gensim.models.Word2Vec(questions, size=300, workers=16, iter=10, negative=20)\n\n# trim memory\nmodel.init_sims(replace=True)\n\n# creta a dict \nw2v = dict(zip(model.wv.index2word, model.wv.syn0))\nprint(\"Number of tokens in Word2Vec:\", len(w2v.keys()))\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0b469433-a2e7-6c5b-171d-7967621b319c"},"outputs":[],"source":"# save model\nmodel.save('3_word2vec.mdl')\nmodel.wv.save_word2vec_format('3_word2vec.bin', binary=True)\nmodel = gensim.models.Word2Vec.load('3_word2vec.mdl')  # you can continue training with the loaded model!\nmodel.wv['computer']"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dc831c21-150b-5dc5-c74c-ceba3eabbd9e"},"outputs":[],"source":"def make_question_vectors(model, sentence): \n    # return numpy document vector by averaging constituent word vectors\n    # sentence is a list of words in same style as iterator makes for entering into word2vec\n    word_vecs = []\n    for word in sentence: \n        try: \n            new_word = model[word]\n        except KeyError:\n            continue\n        # check whether array has nan before appending\n        if not np.isnan(np.sum(new_word)):\n            word_vecs.append(new_word)\n    # if no appropriate word vectors found, return array of zeros\n    if not word_vecs:\n        return np.zeros(model.layer1_size)\n    word_vecs = np.array(word_vecs)\n    return word_vecs.mean(axis=0)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d6399fac-4870-8195-f8e1-f17b772def74"},"outputs":[],"source":"vec1 = df_train['question1'].apply(lambda x: make_question_vectors(model,x.split()))\nvec2 = df_train['question2'].apply(lambda x: make_question_vectors(model,x.split()))\ndf_train['q1_feats'] = list(vec1)\ndf_train['q2_feats'] = list(vec2)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"601362cf-28a3-6004-ddd5-c343314a98b3"},"outputs":[],"source":"df_train.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"6a7bfdd5-4420-872b-44f9-e008cc81721f"},"source":"TIME TO TRAIN"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bdde5cd3-128e-fb96-8638-859a8d4767f6"},"outputs":[],"source":"from __future__ import absolute_import\nfrom __future__ import print_function\nimport numpy as np\n\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Lambda, merge, BatchNormalization, Activation, Input, Merge\nfrom keras import backend as K\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6abca1ce-434f-9218-79dd-86da2f7d7f5a"},"outputs":[],"source":"q1_branch = Sequential()\nq1_branch.add(Dense(1000, input_shape=(300,), activation='relu'))\nq1_branch.add(Dropout(0.2))\n\nq2_branch = Sequential()\nq2_branch.add(Dense(1000, input_shape=(300,), activation='relu'))\nq2_branch.add(Dropout(0.2))\n\nmerged = Merge([q1_branch, q2_branch], mode='concat')\n\nfinal_model = Sequential()\nfinal_model.add(merged)\nfinal_model.add(Dense(500, activation='relu'))\nfinal_model.add(Dropout(0.2))\nfinal_model.add(Dense(500, activation='relu'))\nfinal_model.add(Dropout(0.2))\nfinal_model.add(Dense(2, activation='softmax'))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"69ace45b-d818-91a3-3b5b-c19399d091e0"},"outputs":[],"source":"# compile model - accuracy will be metrix we optimize for\nfinal_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d7a4e49e-9075-75df-8b32-00564fd92448"},"outputs":[],"source":"# shuffle df_train\ndf_train = df_train.reindex(np.random.permutation(df_train.index))\n\n# set number of train and test instances\nnum_train = int(df_train.shape[0] * 0.88)\nnum_test = df_train.shape[0] - num_train                 \nprint(\"Number of training pairs: %i\"%(num_train))\nprint(\"Number of testing pairs: %i\"%(num_test))\n\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"02348000-1f9b-6ea7-292e-f68aadf4afa9"},"outputs":[],"source":"# init data data arrays\nX_train = np.zeros([num_train, 2, 300])\nX_test  = np.zeros([num_test, 2, 300])\nY_train = np.zeros([num_train]) \nY_test = np.zeros([num_test])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"688b22c0-a9fd-b366-26f7-69bf9be9710c"},"outputs":[],"source":"# format data \nb = [a[None,:] for a in list(df_train['q1_feats'].values)]\nq1_feats = np.concatenate(b, axis=0)\n\nb = [a[None,:] for a in list(df_train['q2_feats'].values)]\nq2_feats = np.concatenate(b, axis=0)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a1ded9c5-38ef-1923-0310-21dd80477860"},"outputs":[],"source":"df_train.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"af441e6c-9182-9754-d6c0-fa2ddd34b93f"},"outputs":[],"source":"null"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}