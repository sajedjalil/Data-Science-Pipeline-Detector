{"cells":[{"metadata":{"id":"Z9qQjPL0-lLS"},"cell_type":"markdown","source":"# Looking for my twin question using ðŸ”¥"},{"metadata":{"id":"-S2OVWnX-xed","trusted":true},"cell_type":"code","source":"import os,re,zipfile\nimport pandas as pd\nimport numpy as np\nfrom types import SimpleNamespace\nfrom matplotlib import pyplot as plt\nimport itertools\nplt.style.use('dark_background')\nplt.style.use('seaborn')\n# Torch\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\ntorch.backends.cudnn.deterministic = True  \n\n# metrics\nfrom sklearn import metrics\n\n# Data processing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom tqdm.notebook import tqdm","execution_count":null,"outputs":[]},{"metadata":{"id":"ZM9CFNjwm0o-","outputId":"04775b7a-7019-4371-906b-c538c10e7df8","trusted":true},"cell_type":"code","source":"dev = torch.device('cuda')\ndev","execution_count":null,"outputs":[]},{"metadata":{"id":"1On5dKOv-xdT","outputId":"762d19af-c96a-485d-f3a2-ef8132f1ad32","trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/quora-question-pairs/train.csv.zip')[['question1','question2','is_duplicate']].dropna()\ndups = df[df.is_duplicate == 1].copy()\nno_dups = df[df.is_duplicate == 0].copy()\nsplit_fact = 2\ndf = pd.concat([dups[:int(len(dups) / split_fact)], no_dups[:int(len(no_dups) / split_fact)]],ignore_index=True)\ndf","execution_count":null,"outputs":[]},{"metadata":{"id":"_UCJrVGvRFYV","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_hub as hub\nclass UniversalSentenceEncoder:\n\n    def __init__(self, encoder='universal-sentence-encoder', version='4'):\n        self.version = version\n        self.encoder = encoder\n        self.embd = hub.load(f\"https://tfhub.dev/google/{encoder}/{version}\",)\n\n    def embed(self, sentences):\n        return self.embd(sentences)\n\n    def squized(self, sentences):\n        return np.array(self.embd(tf.squeeze(tf.cast(sentences, tf.string))))","execution_count":null,"outputs":[]},{"metadata":{"id":"vWeLzLZrRHZL","trusted":true},"cell_type":"code","source":"ue = UniversalSentenceEncoder()","execution_count":null,"outputs":[]},{"metadata":{"id":"ARelXfoLRmlX","trusted":true},"cell_type":"code","source":"train,test = train_test_split(df,test_size=0.33,random_state=42,stratify=df['is_duplicate'])","execution_count":null,"outputs":[]},{"metadata":{"id":"WOwhtJcwSQ8m","trusted":true},"cell_type":"code","source":"%%time\nx_q1_train = torch.from_numpy(ue.squized(train['question1'].values)).type(torch.FloatTensor).to(dev)\nx_q2_train = torch.from_numpy(ue.squized(train['question2'].values)).type(torch.FloatTensor).to(dev)\ny_train = torch.from_numpy(train['is_duplicate'].values).type(torch.LongTensor).to(dev)\n\nx_q1_test = torch.from_numpy(ue.squized(test['question1'].values)).type(torch.FloatTensor).to(dev)\nx_q2_test = torch.from_numpy(ue.squized(test['question2'].values)).type(torch.FloatTensor).to(dev)\ny_test = torch.from_numpy(test['is_duplicate'].values).type(torch.LongTensor).to(dev)","execution_count":null,"outputs":[]},{"metadata":{"id":"Fd61q4dnTJ-4","trusted":true},"cell_type":"code","source":"b_size = 256\ntrain_dl = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_q1_train,x_q2_train,y_train), batch_size=b_size, shuffle=True)\ntest_dl = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_q1_test,x_q2_test,y_test), batch_size=b_size, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"6Fi4WCkzXL5E","trusted":true},"cell_type":"code","source":"class Net(nn.Module):\n\n    def __init__(self):\n        super(Net,self).__init__()\n\n        self.q1_lin = nn.Linear(in_features=512,out_features=1024)\n        self.q2_lin = nn.Linear(in_features=512,out_features=1024)\n        self.lin1 = nn.Linear(in_features=2048,out_features=1024)\n        self.lin2 = nn.Linear(in_features=1024,out_features=512)\n        self.lin3 = nn.Linear(in_features=512,out_features=256)\n        self.lin4 = nn.Linear(in_features=256,out_features=128)\n        self.lin5 = nn.Linear(in_features=128,out_features=2)\n\n    # here we take the input data and pass it through the chain of layers\n    def forward(self,q1,q2):\n        q1 = self.q1_lin(q1)\n        q2 = self.q2_lin(q2)\n        x = torch.cat((q1,q2),dim=1)\n        # print(x.size())\n        x = self.lin1(x)\n        x = self.lin2(x)\n        x = self.lin3(x)\n        x = self.lin4(x)\n        x = self.lin5(x)\n        return x\n\n# instance our model\nmodel = Net().to(dev)\n# set the number of epochs\nepochs = 100\n# criterion aka loss function -> find more on pytorch doc\ncriterion = nn.CrossEntropyLoss()\n\n# optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n\n# create 3 lists to store the losses and accuracy at each epoch\ntrain_losses, test_losses, accuracy = [0]*epochs, [0]*epochs,[0]*epochs\n\n\n# in this current case we don't use batches for training and we pass the whole data at each epoch\nfor e in tqdm(range(epochs)):\n\n    for q1,q2,label in train_dl:\n\n        optimizer.zero_grad()\n        q1 = q1.float()\n        q2 = q2.float()\n        # Comput train loss\n        y_pred = model(q1,q2)\n        loss = criterion(y_pred, label)\n        \n        loss.backward()\n\n        optimizer.step()\n\n        # store train loss\n        train_losses[e] = loss.item()\n    \n    for q1,q2,label in test_dl:    \n        # Compute the test stats\n        with torch.no_grad():\n            # Turn on all the nodes\n            model.eval()\n            q1 = q1.float()\n            q2 = q2.float()\n            # Comput test loss\n            ps = model(q1,q2)\n            loss = criterion(ps, label)\n\n            # store test loss\n            test_losses[e] = loss.item()\n            \n            # # Compute accuracy\n            top_p, top_class = ps.topk(1, dim=1)\n        \n            equals = (top_class == label.view(*top_class.shape))\n            \n            # # store accuracy\n            accuracy[e] = torch.mean(equals.type(torch.FloatTensor))\n\n# Print the final information\nprint(f'Accuracy  : {100*accuracy[-1].item():0.2f}%')\nprint(f'Train loss: {train_losses[-1]}')\nprint(f'Test loss : {test_losses[-1]}')\n    \n# Plot the results\nfig,ax = plt.subplots(1,2,figsize=(20,5))\nax[0].set_ylabel('Accuracy')\nax[0].set_xlabel('Epochs')\nax[0].set_title('Model Accuracy')\nax[0].plot(accuracy)\n\nax[1].set_ylabel('Loss')\nax[1].set_xlabel('Epochs')\nax[1].set_title('Train/Test Losses')\nax[1].plot(train_losses, label='train')\nax[1].plot(test_losses, label='test')\nax[1].legend()   \n\nplt.tight_layout()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}