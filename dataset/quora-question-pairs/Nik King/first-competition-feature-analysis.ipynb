{"cells":[{"execution_count":null,"outputs":[],"source":"### Overview\n\nIn this notebook I'll describe my general approach to the Quora machine learning competition https://www.kaggle.com/c/quora-question-pairs sponsered by Quora, a question-and-answer site.\n\nFrom the competition overview:\n\n\"Over 100 million people visit Quora every month, so it's no surprise that many people ask similarly worded questions. Multiple questions with the same intent can cause seekers to spend more time finding the best answer to their question, and make writers feel they need to answer multiple versions of the same question. Quora values canonical questions because they provide a better experience to active seekers and writers, and offer more value to both of these groups in the long term.\"\n\nExciting! So it's my goal to create a machine learning model that uses past Quora data of duplicate questions to predict future duplicate questions.\n\nUsing standard Python libraries and five features I was able to score in the top 50% in my first competition (1370/3307 on the [public leaderboard](https://www.kaggle.com/c/quora-question-pairs/leaderboard)). For some perspective the winning teams for these competitions often have multiple teammembers, specialized hardware, and spend time developing hundreds of different text analysis feature. ","metadata":{"_uuid":"1ac224f793873c3fcb735ff87d731e45249a2a2b","_cell_guid":"3a42d46c-f699-4da8-a556-80ce9c08c4e7"},"cell_type":"markdown"},{"execution_count":null,"outputs":[],"source":"### Setup","metadata":{"_uuid":"5c7b8c0ceeabb94f3ff44ae603443bc2727ff950","_cell_guid":"c56195fd-d2ad-4852-940c-35bc96c5772c"},"cell_type":"markdown"},{"execution_count":null,"outputs":[],"source":"There are two input files: a training dataset and a testing dataset. The training dataset contains Quora questions that having been identified as duplicates. And using that data a machine learning algorithm can be \"trained\" to identify which questions in the testing dataset are duplicates.","metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"b059de0ea1c2db55b79147e5850f6e48da47e65f","_cell_guid":"6a0c8cbf-0ce6-4e5d-8a21-23ff8e083031"},"cell_type":"markdown"},{"execution_count":null,"outputs":[],"source":"# import necessary Python libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os # operating system\nimport gc # garbage collecting\nimport matplotlib.pyplot as plt # plotting\nimport seaborn as sns # statistical data visualization\nfrom __future__ import division # division support in Python 2.7\nimport nltk # Natural Language Processing\nimport codecs # decoding\nimport pickle # for saving off large files\nimport re\n%matplotlib inline","metadata":{"_execution_state":"idle","collapsed":true,"_uuid":"50f838264a19f972f8e7d853094bd3701dfa902a","_cell_guid":"dc34ee33-0e3c-451a-82d9-85ee6104fbea","trusted":false},"cell_type":"code"},{"execution_count":null,"outputs":[],"source":"# setting a color palette for plotting and importing the train and test files\npal = sns.color_palette()\ndf_train = pd.read_csv('../input/train.csv')\ndf_test = pd.read_csv('../input/test.csv')","metadata":{"_execution_state":"idle","collapsed":true,"_uuid":"54c47fa98b81f0134a7f29ffb98bfbd86cac6151","_cell_guid":"c590c498-8dc5-43ac-8230-06ee6a0a5781","trusted":false},"cell_type":"code"},{"execution_count":null,"outputs":[],"source":"Below is the format of the training file. If the **is_duplicate** column equals 1, then **question1** and **question2** have been determined to have a similar meaning.","metadata":{"_uuid":"bb56e8d0d1ea05df4af307526a7dc89005a56022","_cell_guid":"8b7540ea-2dd9-45d9-9813-f953fea57fa7"},"cell_type":"markdown"},{"execution_count":null,"outputs":[],"source":"df_train.head(10)","metadata":{"_execution_state":"idle","_uuid":"55cc91a463e51120ca8a9322ad681cba03737f91","_cell_guid":"75165496-4655-4f6a-b5b6-a1c7012f418f","trusted":false},"cell_type":"code"},{"execution_count":null,"outputs":[],"source":"Below is the format of the test file. Two questions are given, and it will be the model created that determines the likelihood that they have similar meaning.","metadata":{"_uuid":"59228526b4b70bf260eb33917646372a78830739","_cell_guid":"6331f05c-90af-4318-92f9-8c1db472aafc"},"cell_type":"markdown"},{"execution_count":null,"outputs":[],"source":"df_test.head(10)","metadata":{"_execution_state":"idle","_uuid":"056343dd7e3acd4ce39ee3d43ae5b6ae2713e936","_cell_guid":"0daf00f9-0dd4-4bf8-9261-a004aefb3085","trusted":false},"cell_type":"code"},{"execution_count":null,"outputs":[],"source":"### Data Cleaning","metadata":{"_uuid":"f2384b9706863043b18e700b909f3e502808ebf7","_cell_guid":"83beb8b8-a170-4bd1-9cf7-353196127d1a"},"cell_type":"markdown"},{"execution_count":null,"outputs":[],"source":"For my analysis I'll be focusing on semantics, rather than syntax. Therefore it's helpful to remove symbols, numbers, and conjunctions for better results. For example, if you are trying to determine how similar the sentences \"What's Jupyter?\" and \"What is Jupyter?\" it is helpful to replace \"what's\" with \"what is\".","metadata":{"_uuid":"39714ebb7e67eab8e3c30537f472723f6f57995a","_cell_guid":"954d41bc-182e-45ab-9ceb-7c3f883f0930"},"cell_type":"markdown"},{"execution_count":null,"outputs":[],"source":"df_train_clean = df_train\nfor tag in ['question1', 'question2']:\n    df_train_clean[tag] = df_train[tag].str.lower()\n    df_train_clean[tag].replace(to_replace=r\"[^A-Za-z0-9^,!.\\/'+-=]\", value=\" \", regex=True, inplace=True)\n    df_train_clean[tag].replace(to_replace=r\"what's\", value=\"what is \", regex=True, inplace=True)\n    df_train_clean[tag].replace(to_replace=r\"'s\", value=\" \", regex=True, inplace=True)\n    df_train_clean[tag].replace(to_replace=r\"'ve'\", value=\" have \", regex=True, inplace=True)\n    df_train_clean[tag].replace(to_replace=r\"can't\", value=\"cannot \", regex=True, inplace=True)\n    df_train_clean[tag].replace(to_replace=r\"n't\", value=\" not \", regex=True, inplace=True)\n    df_train_clean[tag].replace(to_replace=r\"i'm\", value=\"i am \", regex=True, inplace=True)\n    df_train_clean[tag].replace(to_replace=r\"'re'\", value=\" are \", regex=True, inplace=True)\n    df_train_clean[tag].replace(to_replace=r\"'ll'\", value=\" will \", regex=True, inplace=True)","metadata":{"_execution_state":"idle","collapsed":true,"_uuid":"98824d9724822c9270370422e00d649297354401","_cell_guid":"fbd89488-e223-4c1b-ad81-6d0ac31c80ed","trusted":false},"cell_type":"code"},{"execution_count":null,"outputs":[],"source":"### Feature Analysis","metadata":{"_uuid":"ed5f519c876296b71df7edbe8c1e353acbe88f3a","_cell_guid":"9c8ff0de-39d6-4671-b9ee-2066532412fc"},"cell_type":"markdown"},{"execution_count":null,"outputs":[],"source":"Feature analysis is the process of analyzing different variables in your data with the goal of selecting features that you'll feed into your machine learning model (also see [feature selection](https://en.wikipedia.org/wiki/Feature_selection)). In this dataset we are analyzing the text from questions. Here are just a examples of different features commonly using in text analysis (features I'm using are in bold):\n\n* __Share of words that are contained in both questions__\n* __Share words in that are only contained in one of the two questions__\n* __Sum of total words in both questions__\n* __Share of question words (e.g. \"what\") contained in both questions__\n* __TF-IDF (see below for description__)\n* Length / difference in length\n* [Bag of Words](https://en.wikipedia.org/wiki/Bag-of-words_model)\n* [nGrams](https://en.wikipedia.org/wiki/N-gram) Features (n = 1,2,3) for title and description (Both Words and Characters)\n    * Count of Ngrams (#, Sum, Diff, Max, Min)\n    * Count of Unique Ngrams\n    * Ratio of Intersect Ngrams\n    * Ratio of Unique Intersect Ngrams\n* Distance Features:\n    * Jaccard , Cosine, Levenshtein and Hamming Distance between the titles and descriptions\n* Special Character Counting & Ratio Features:\n    * Counting & Ratio features of Capital Letters in title and description\n    * Counting & Ratio features of Special Letters (digits, punctuations, etc.) in title and description\n* Similarity between sets of words/characters\n* Fuzzywuzzy/jellyfish distances\n* Number of overlapping sets of n words (n=1,2,3)\n* Matching moving windows of strings\n* Cross-matching columns (eg. title1 with description2)\n\nWhen it comes to feature selection, more features are better, but only if they aren't highly correlated. I think of the goal as picking apart the dataset from as many different angles as possible. What the machine learning models excel at is combining those different angles using to find patterns and build a predictive model.\n\nI'm planning on adding building out more features as I tackle future text analysis problems and will probably need to upgrade my CPU as well :)","metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"e4ac1f574a33ef5fe828e0d0736e7d317df9768d","_cell_guid":"c093ee2f-dbdd-4319-922d-a85484fa8a8e"},"cell_type":"markdown"},{"execution_count":null,"outputs":[],"source":"# x_train and x_test will be used to store the different selected features\nx_train = pd.DataFrame()\nx_test = pd.DataFrame()","metadata":{"_execution_state":"idle","_uuid":"cd67476d6965c77d1f120d622e86f16d847a2163","_cell_guid":"87b8b81d-fd11-496e-8745-3149667e655e","trusted":false},"cell_type":"code"},{"execution_count":null,"outputs":[],"source":"#### Feature 1 - Question Match\n\nThis feature calculates the share of question words that have a similar intent. ","metadata":{"_uuid":"be4336fe7d23a0788313e8193f20423cc4a62744","_cell_guid":"3bf03e63-896b-4f2e-863e-c4007e0fe3e0"},"cell_type":"markdown"},{"execution_count":null,"outputs":[],"source":"qdict = {'which': 'determiner', \n         'what': 'determiner',\n         'whose': 'personal determiner',\n         'who': 'personal determiner',\n         'whom': 'personal determiner',\n         'where': 'location',\n         'whither': 'goal',\n         'whence': 'source',\n         'how': 'manner',\n         'why': 'reason',\n         'whatsoever': 'choice',\n         'whether': 'choice'\n        }","metadata":{"_execution_state":"idle","collapsed":true,"_uuid":"68a5dcd12b6154e5ee41f17e965171e31037ab60","_cell_guid":"4bfbbe48-4688-4e5c-b6db-b8acb92da36c","trusted":false},"cell_type":"code"},{"execution_count":null,"outputs":[],"source":"def question_match(row):\n    q1words = {}\n    q2words = {}\n    for word in str(row['question1']).lower().split():\n        if word in qdict.keys():\n            q1words[qdict[word]] = 1\n    for word in str(row['question2']).lower().split():\n        if word in qdict.keys():\n            q2words[qdict[word]] = 1\n    if len(q1words) == 0 or len(q2words) == 0:\n        # The computer-generated chaff includes a few questions that are nothing but stopwords\n        return 0\n    shared = [w for w in q1words.keys() if w in q2words]\n    R = len(shared)*2/(len(q1words) + len(q2words))\n    return R\n\ntrain_q_match = df_train_clean.apply(question_match, axis=1, raw=True)\nplt.figure(figsize=(15, 5))\nplt.hist(train_q_match[df_train_clean['is_duplicate'] == 0], bins=20, normed=True, label='Not Duplicate')\nplt.hist(train_q_match[df_train_clean['is_duplicate'] == 1], bins=20, normed=True, alpha=0.7, label='Duplicate')\nplt.legend()\nplt.title('Label distribution over question_match_share', fontsize=15)\nplt.xlabel('question_match_share', fontsize=15)","metadata":{"_execution_state":"idle","_uuid":"f0b385ba47b78d7445790349550282a843f37a28","_cell_guid":"240b7a33-1292-4c98-b699-284578f7fbef","trusted":false},"cell_type":"code"},{"execution_count":null,"outputs":[],"source":"x_train['q_match'] = train_q_match\nx_test['q_match'] = df_test.apply(question_match, axis=1, raw=True)","metadata":{"_execution_state":"busy","collapsed":true,"_uuid":"7078481025da864eb469c2c76382148d217123d3","_cell_guid":"53937bc7-bb36-4cec-b4ab-b1b2036918b7","trusted":false},"cell_type":"code"},{"execution_count":null,"outputs":[],"source":"#### Feature 2 - Word Match","metadata":{"_uuid":"4919f1f611b14e2cb1a5a977aa233386b78e292e","_cell_guid":"69726da9-9791-4f6a-a7c0-28eba8a8e276"},"cell_type":"markdown"},{"execution_count":null,"outputs":[],"source":"from nltk.corpus import stopwords # stopwords are common words best ignored because the relay no meaning (e.g. \"The\")\n\nstops = set(stopwords.words(\"english\"))\n\ndef word_match_share(row):\n    q1words = {}\n    q2words = {}\n    for word in str(row['question1']).lower().split():\n        if word not in stops:\n            q1words[word] = 1\n    for word in str(row['question2']).lower().split():\n        if word not in stops:\n            q2words[word] = 1\n    if len(q1words) == 0 or len(q2words) == np.nan:\n        # The computer-generated chaff includes a few questions that are nothing but stopwords\n        return 0\n    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n    R = (len(shared_words_in_q1) + len(shared_words_in_q2))/((len(q1words)  + len(q2words)))\n    return R\n\nplt.figure(figsize=(15, 5))\ntrain_word_match = df_train_clean.apply(word_match_share, axis=1, raw=True)\nplt.hist(train_word_match[df_train_clean['is_duplicate'] == 0], bins=20, normed=True, label='Not Duplicate')\nplt.hist(train_word_match[df_train_clean['is_duplicate'] == 1], bins=20, normed=True, alpha=0.7, label='Duplicate')\nplt.legend()\nplt.title('Label distribution over word_match_share', fontsize=15)\nplt.xlabel('word_match_share', fontsize=15)","metadata":{"_execution_state":"busy","_uuid":"21d12a1a64518109bd6e68701d37f2dbe51ebee0","_cell_guid":"27a8296c-cf35-408b-8572-8fbf7a9deae9","trusted":false},"cell_type":"code"},{"execution_count":null,"outputs":[],"source":"x_train['word_match'] = train_word_match\nx_test['word_match'] = df_test.apply(word_match_share, axis=1, raw=True)","metadata":{"_execution_state":"busy","collapsed":true,"_uuid":"da6afb9831c73a01e6dc7d60a621211554887bd0","_cell_guid":"ad51db24-c0fe-4cb7-b153-1bf272db2779","trusted":false},"cell_type":"code"},{"execution_count":null,"outputs":[],"source":"#### Feature 3 - Word Count Total Combined","metadata":{"_uuid":"7b23ba48be128e79f059d330e67bb6783a7ba533","_cell_guid":"c0a92dd8-b9c2-4086-aca7-b02edfd9ad6d"},"cell_type":"markdown"},{"execution_count":null,"outputs":[],"source":"### Total number of words in both questions\ndef word_total_combined(row):\n    q1words = {}\n    q2words = {}\n    R = 0\n    for word in str(row['question1']).lower().split():\n        R = R + 1\n    for word in str(row['question2']).lower().split():\n        R = R + 1\n    return R\n\nplt.figure(figsize=(15, 5))\ntrain_word_total = df_train_clean.apply(word_total_combined, axis=1, raw=True)\nplt.hist(train_word_total[df_train_clean['is_duplicate'] == 0], bins=20, normed=True, label='Not Duplicate')\nplt.hist(train_word_total[df_train_clean['is_duplicate'] == 1], bins=20, normed=True, alpha=0.7, label='Duplicate')\nplt.legend()\nplt.title('Label distribution over word_total_combined', fontsize=15)\nplt.xlabel('word_total_combined', fontsize=15)","metadata":{"_execution_state":"busy","_uuid":"77d95c2af7388ecc392ae930d089bd04629d154c","_cell_guid":"77f29bc4-c887-491b-a97a-aaa838518339","trusted":false},"cell_type":"code"},{"execution_count":null,"outputs":[],"source":"x_train['word_total_combined'] = train_word_total\nx_test['word_total_combined'] = df_test.apply(word_total_combined, axis=1, raw=True)","metadata":{"_execution_state":"busy","collapsed":true,"_uuid":"d0568896d73b14c4dc179b4793bb323994a1ed65","_cell_guid":"9887836d-d7b0-448b-ac12-a49971b8fd96","trusted":false},"cell_type":"code"},{"execution_count":null,"outputs":[],"source":"#### Feature 4 - Word Count Difference","metadata":{"_uuid":"c9a80edf1405f3c9058fe09d53702d7e1f78c01f","_cell_guid":"54a7c164-add8-4870-8ac6-4c73985d0e32"},"cell_type":"markdown"},{"execution_count":null,"outputs":[],"source":"def word_diff(row):\n    q1words = {}\n    q2words = {}\n    for word in str(row['question1']).lower().split():       \n        q1words[word] = 1\n    for word in str(row['question2']).lower().split():\n        q2words[word] = 1\n    if len(q1words) == 0 or len(q2words) == 0:\n        # The computer-generated chaff includes a few questions that are nothing but stopwords\n        return 0\n    R = abs(len(q1words) - len(q2words))\n    return R\n\nplt.figure(figsize=(15, 5))\ntrain_word_diff = df_train_clean.apply(word_diff, axis=1, raw=True)\nplt.hist(train_word_diff[df_train_clean['is_duplicate'] == 0], bins=20, normed=True, label='Not Duplicate')\nplt.hist(train_word_diff[df_train_clean['is_duplicate'] == 1], bins=20, normed=True, alpha=0.7, label='Duplicate')\nplt.legend()\nplt.title('Label distribution over word_diff', fontsize=15)\nplt.xlabel('word_diff', fontsize=15)","metadata":{"_execution_state":"busy","_uuid":"8897b28fc0bfa27f335c00e34922b9e6dca38717","_cell_guid":"c8babe23-0c40-46c7-a073-2249e011d3a7","trusted":false},"cell_type":"code"},{"execution_count":null,"outputs":[],"source":"x_train['word_diff'] = train_word_total\nx_test['word_diff'] = df_test.apply(word_total_combined, axis=1, raw=True)","metadata":{"_execution_state":"busy","collapsed":true,"_uuid":"554bb858a8011615d909bd6047ac1e5656751a2e","_cell_guid":"7ff55bfe-cb08-41fc-9b98-b951789e1de1","trusted":false},"cell_type":"code"},{"execution_count":null,"outputs":[],"source":"x_test.head()","metadata":{"_execution_state":"busy","_uuid":"42213d02cf7cd01206b1ea9389cb01bda8cbb729","_cell_guid":"7b3fcfbe-72eb-4717-a0ff-e7b27591bc06","trusted":false},"cell_type":"code"},{"execution_count":null,"outputs":[],"source":"#### Feature 5 - TF-IDF","metadata":{"collapsed":true,"_uuid":"b94966386ccc7b8f40dfdd087cd9f8855818dcc8","_cell_guid":"c30ade22-e165-4b98-818c-5a9e5a367a24"},"cell_type":"markdown"},{"execution_count":null,"outputs":[],"source":"\"Term frequencyâ€“inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. \"\n\nIn other words, if a rare word (e.g. kitchen) is matched between questions it should mean more than if a common word (e.g. happy) is matched. On a side not, TF-IDF is the general concept behind \"including results for\" suggestions when you google a misspelled word.\n\n\nhttps://en.wikipedia.org/wiki/Tf%E2%80%93idf\n    ","metadata":{"_uuid":"ad58185c31184d003c8774781f61400245b17773","_cell_guid":"c384a235-162a-4909-add2-3af08d307fb2"},"cell_type":"markdown"},{"execution_count":null,"outputs":[],"source":"from collections import Counter\n\ntrain_qs = pd.Series(df_train_clean['question1'].tolist() + df_train_clean['question2'].tolist()).astype(str)\n\n# If a word appears only once, we ignore it completely (likely a typo)\n# Epsilon defines a smoothing constant, which makes the effect of extremely rare words smaller\ndef get_weight(count, eps=10000, min_count=2):\n    if count < min_count:\n        return 0\n    else:\n        return 1 / (count + eps)\n\neps = 5000 \nwords = (\" \".join(train_qs)).lower().split()\ncounts = Counter(words)\nweights = {word: get_weight(count) for word, count in counts.items()}","metadata":{"_execution_state":"busy","collapsed":true,"_uuid":"8c58b45e38d2cc9ad35723bc8d91f8249ffec8eb","_cell_guid":"1a16ca92-1982-497d-abbb-46f9ea69c6bd","trusted":false},"cell_type":"code"},{"execution_count":null,"outputs":[],"source":"print('Most common words and weights: \\n')\nprint(sorted(weights.items(), key=lambda x: x[1] if x[1] > 0 else 9999)[:10])\nprint('\\nLeast common words and weights: ')\n(sorted(weights.items(), key=lambda x: x[1], reverse=True)[:10])","metadata":{"_execution_state":"busy","_uuid":"e5b1760776c1636c57dbc92b922d368548602e53","_cell_guid":"588e5543-a9a9-4a35-9167-76b80f98b782","trusted":false},"cell_type":"code"},{"execution_count":null,"outputs":[],"source":"from nltk.corpus import stopwords\n\nstops = set(stopwords.words(\"english\"))\n\ndef tfidf_word_match_share(row):\n    q1words = {}\n    q2words = {}\n    for word in str(row['question1']).lower().split():\n        if word not in stops:\n            q1words[word] = 1\n    for word in str(row['question2']).lower().split():\n        if word not in stops:\n            q2words[word] = 1\n    if len(q1words) == 0 or len(q2words) == 0:\n        # The computer-generated chaff includes a few questions that are nothing but stopwords\n        return 0\n    \n    missing_weights = np.sum([weights.get(w, 0) for w in q1words.keys() if w not in q2words] + [weights.get(w, 0) for w in q2words.keys() if w not in q1words])\n    #total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\n    \n    #R = np.sum(shared_weights) / np.sum(total_weights)\n    return missing_weights","metadata":{"_execution_state":"busy","collapsed":true,"_uuid":"c02ee15b9453aa46a3bc85b49009b25128f6fae0","_cell_guid":"9348254b-54c6-4f53-bb20-8e5cc4a2dd4f","trusted":false},"cell_type":"code"},{"execution_count":null,"outputs":[],"source":"plt.figure(figsize=(15, 5))\ntfidf_train_word_match = df_train.apply(tfidf_word_match_share, axis=1, raw=True)\nplt.hist(tfidf_train_word_match[df_train['is_duplicate'] == 0].fillna(0), bins=20, normed=True, label='Not Duplicate')\nplt.hist(tfidf_train_word_match[df_train['is_duplicate'] == 1].fillna(0), bins=20, normed=True, alpha=0.7, label='Duplicate')\nplt.legend()\nplt.title('Label distribution over tfidf_word_match_share', fontsize=15)\nplt.xlabel('word_match_share', fontsize=15)","metadata":{"_execution_state":"busy","_uuid":"13bab134e95b3d572b744532be7ee2609eb623c8","_cell_guid":"b5cda7ce-c085-418d-9726-12ad8bf8292d","trusted":false},"cell_type":"code"},{"execution_count":null,"outputs":[],"source":"x_train['tfidf'] = tfidf_train_word_match\nx_test['tfidf'] = df_test.apply(tfidf_word_match_share, axis=1, raw=True)","metadata":{"_execution_state":"busy","collapsed":true,"_uuid":"a776e3fd78241325ef6b10b4122c1f3bd46ce5e3","_cell_guid":"73266f5a-d1af-4541-aa87-fd8a046fc60d","trusted":false},"cell_type":"code"},{"execution_count":null,"outputs":[],"source":"### XGBoost\n\nNow it's time for the machine learning piece. I won't go into any detail here, but for anyone interested in starting out with machine learning http://neuralnetworksanddeeplearning.com/ was my favorite resource.\n\nXGBoost is [Gradient Boosting](https://en.wikipedia.org/wiki/Gradient_boosting) framework and I used this [book](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/) for parameter tuning. ","metadata":{"_uuid":"1d9a4596133e534b64788511664a3ef62f767add","_cell_guid":"472d942a-1fc4-4960-b2a1-0df51ffd0955"},"cell_type":"markdown"},{"execution_count":null,"outputs":[],"source":"y_train = df_train['is_duplicate'].values","metadata":{"_execution_state":"busy","collapsed":true,"_uuid":"7d9dd8ed3ddb44c994e1c4bff6805ccb2b32974f","_cell_guid":"772c9275-d654-4eb3-8289-4f96ecca7b2b","trusted":false},"cell_type":"code"},{"execution_count":null,"outputs":[],"source":"# \npos_train = x_train[y_train == 1]\nneg_train = x_train[y_train == 0]\n\n# Now we oversample the negative class\n# There is likely a much more elegant way to do this...\np = 0.165\nscale = ((len(pos_train) / (len(pos_train) + len(neg_train))) / p) - 1\nwhile scale > 1:\n    neg_train = pd.concat([neg_train, neg_train])\n    scale -=1\n    \nneg_train = pd.concat([neg_train, neg_train[:int(scale * len(neg_train))]])\nprint(len(pos_train) / (len(pos_train) + len(neg_train)))\n\nx_train = pd.concat([pos_train, neg_train])\ny_train = (np.zeros(len(pos_train)) + 1).tolist() + np.zeros(len(neg_train)).tolist()\ndel pos_train, neg_train\n0.19124366100096607","metadata":{"_execution_state":"busy","_uuid":"de5230916dde96475da112667d337671c1890812","scrolled":true,"_cell_guid":"7dfc0d1e-09b0-4c45-9422-e4a01e823e93","trusted":false},"cell_type":"code"},{"execution_count":null,"outputs":[],"source":"# Finally, we split some of the data off for validation\nfrom sklearn.cross_validation import train_test_split\n\nx_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2, random_state=2237)","metadata":{"_execution_state":"busy","_uuid":"238ef92ded5cac66a7d17538fcf98493514d9854","_cell_guid":"fddd62b8-ca61-4fe4-ad38-ef005a25e3e8","trusted":false},"cell_type":"code"},{"execution_count":null,"outputs":[],"source":"import xgboost as xgb\n\n# Set our parameters for xgboost\nparams = {}\nparams['objective'] = 'binary:logistic'\nparams['eval_metric'] = 'logloss'\nparams['eta'] = 0.02\nparams['max_depth'] = 6\nparams['min_child_weight'] = 4\n\n\nd_train = xgb.DMatrix(x_train, label=y_train)\nd_valid = xgb.DMatrix(x_valid, label=y_valid)\n\nwatchlist = [(d_train, 'train'), (d_valid, 'valid')]\n\nbst = xgb.train(params, d_train, 400, watchlist, early_stopping_rounds=50, verbose_eval=10)","metadata":{"_execution_state":"busy","_uuid":"944ee1fdc7a49b8b9bf17004c0f415069e83d47f","_cell_guid":"49004690-dfee-4ad5-b53d-0afc580f655e","trusted":false},"cell_type":"code"},{"execution_count":null,"outputs":[],"source":"One cool aspect of XGBoost is that it's easy to plot how valuable each of your features were after the you've build your model.","metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"0a964bcd265ddc5a4128bbf33e6c42c6d3bde27b","_cell_guid":"3d54f960-0fac-408e-955d-3aaca4031030"},"cell_type":"markdown"},{"execution_count":null,"outputs":[],"source":"import operator\n\ndef ceate_feature_map(features):\n    outfile = open('xgb.fmap', 'w')\n    i = 0\n    for feat in features:\n        outfile.write('{0}\\t{1}\\tq\\n'.format(i, feat))\n        i = i + 1\n\n    outfile.close()\n\nfeatures = list(x_train.columns)\nceate_feature_map(features)\n\nimportance = bst.get_fscore(fmap='xgb.fmap')\nimportance = sorted(importance.items(), key=operator.itemgetter(1))\n\ndf = pd.DataFrame(importance, columns=['feature', 'fscore'])\ndf['fscore'] = df['fscore'] / df['fscore'].sum()\n\nplt.figure()\ndf.plot()\ndf.plot(kind='barh', x='feature', y='fscore', legend=False, figsize=(6, 10))\nplt.title('XGBoost Feature Importance')\nplt.xlabel('relative importance')","metadata":{"_execution_state":"busy","_uuid":"a3c5eb2924f70c1726095ada4449a3f6fa1bef64","_cell_guid":"715a54e3-76b7-4c19-9cbe-82b30ab4d7b8","trusted":false},"cell_type":"code"},{"execution_count":null,"outputs":[],"source":"### Generating a submission file\n\nThis [link](https://www.kaggle.com/c/quora-question-pairs#evaluation) provides the explanation of the submission data format.","metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"28bd1409d625fc4d1700dd74027e3169826145a3","_cell_guid":"d02692dc-3fa7-49ed-b48a-ea63e461bdc0"},"cell_type":"markdown"},{"execution_count":null,"outputs":[],"source":"d_test = xgb.DMatrix(x_test)\np_test = bst.predict(d_test)\n\nsub = pd.DataFrame()\nsub['test_id'] = df_test['test_id']\nsub['is_duplicate'] = p_test\nsub.to_csv('submission.csv', index=False)","metadata":{"_execution_state":"busy","collapsed":true,"_uuid":"1125084e21a18fc2a434f289aecdf195e36697e5","_cell_guid":"2c12f6f5-38a2-4753-884b-90e7cec2ae01","trusted":false},"cell_type":"code"},{"execution_count":null,"outputs":[],"source":"Experimenting with different features and parameter tuning got me to __0.34430__ on the Public Leaderboard, nice!","metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"8a0a2ec7d66b1d5c3fe47cf164b3d5f3b1a28923","_cell_guid":"e22d03cd-189e-42c9-a5ba-1315fa26fd68"},"cell_type":"markdown"}],"nbformat_minor":2,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","pygments_lexer":"ipython3","version":"3.6.1","nbconvert_exporter":"python","file_extension":".py","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3}}},"nbformat":4}