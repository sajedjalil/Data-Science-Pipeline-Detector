{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"9c355c2c-044d-2b87-e1cf-a6732fb9e802"},"source":"# Identifying Duplicate Questions\n\nWelcome to the Quora Question Pairs competition! Here, our goal is to identify which questions asked on [Quora](https://www.quora.com/), a quasi-forum website with over 100 million visitors a month, are duplicates of questions that have already been asked. This could be useful, for example, to instantly provide answers to questions that have already been answered. We are tasked with predicting whether a pair of questions are duplicates or not, and submitting a binary prediction against the logloss metric."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d9f5b5bf-b8a8-a88a-a16c-7666be39bd7e"},"outputs":[],"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0ac4cab6-214b-957b-bb2a-01f7e8d5ed2b"},"outputs":[],"source":"df_train = pd.read_csv('../input/train.csv')\ndf_train.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dc70e18f-f364-0518-1d85-f7409269c754"},"outputs":[],"source":"df_test = pd.read_csv('../input/test.csv')\ndf_test.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1477d9f9-7cbf-09f5-21af-6b1da642ee69"},"outputs":[],"source":"print('Total number of question pairs for testing: {}'.format(len(df_test)))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ef13480a-2522-2d1f-cf8a-56670e0856ab"},"outputs":[],"source":"import re, math, collections\nfrom nltk.corpus import stopwords\n\nstws = set(stopwords.words(\"english\"))\n \ndef tokenize(_str):\n  tokens = collections.defaultdict(lambda: 0.)\n  for m in re.finditer(r\"(\\w+)\", _str, re.UNICODE):\n    m = m.group(1).lower()\n    if len(m) < 2: continue\n    if m in stws: continue\n    tokens[m] += 1  \n  return tokens\n\ndef kldivergence(_s, _t):\n    if (len(_s) == 0):\n        return 1e33\n \n    if (len(_t) == 0):\n        return 1e33\n \n    ssum = 0. + sum(_s.values())\n    slen = len(_s)\n \n    tsum = 0. + sum(_t.values())\n    tlen = len(_t)\n \n    vocabdiff = set(_s.keys()).difference(set(_t.keys()))\n    lenvocabdiff = len(vocabdiff)\n \n    \"\"\" epsilon \"\"\"\n    epsilon = min(min(_s.values())/ssum, min(_t.values())/tsum) * 0.001\n \n    \"\"\" gamma \"\"\"\n    gamma = 1 - lenvocabdiff * epsilon\n \n    \"\"\" Check if distribution probabilities sum to 1\"\"\"\n    sc = sum([v/ssum for v in _s.values()])\n    st = sum([v/tsum for v in _t.values()])\n \n    if sc < 9e-6:\n        sys.exit(2)\n    if st < 9e-6:\n        sys.exit(2)\n \n    div = 0.\n    for t, v in _s.items():\n        pts = v / ssum\n \n        ptt = epsilon\n        if t in _t:\n            ptt = gamma * (_t[t] / tsum)\n \n        ckl = (pts - ptt) * math.log(pts / ptt)\n \n        div +=  ckl\n \n    return div\n\nq1 = \"What are the how best books of all time?\"\nq2 = \"What are some of the military history books of all time?\"\n \nprint(\"KL-divergence between q1 and q2:\", kldivergence(tokenize(q1), tokenize(q2)))\nprint(\"KL-divergence between d2 and d1:\", kldivergence(tokenize(q2), tokenize(q1)))\n    "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3f3ce6c2-0557-fe4e-fe37-ce52741e5296"},"outputs":[],"source":"def kldistance(q1, q2):\n  q1t = tokenize(q1)\n  q2t = tokenize(q2)\n  q1q2div = kldivergence(q1t, q2t)\n  q2q1div = kldivergence(q2t, q1t)\n  divs = q1q2div + q2q1div\n  if divs == 0: return 0\n  return (2 * q1q2div * q2q1div) / divs\n\nprint(kldistance(q1, q2))"},{"cell_type":"markdown","metadata":{"_cell_guid":"28b46d9f-82fe-c37e-010c-a3e115687d41"},"source":"## Rebalancing the Data\nHowever, before I do this, I would like to rebalance the data that XGBoost receives, since we have 37% positive class in our training data, and only 17% in the test data. By re-balancing the data so our training set has 17% positives, we can ensure that XGBoost outputs probabilities that will better match the data on the leaderboard, and should get a better score (since LogLoss looks at the probabilities themselves and not just the order of the predictions like AUC)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0589da36-d092-c951-f3cd-68df3fd91c7a"},"outputs":[],"source":"def kldr(row):\n  return kldistance(str(row['question1']), str(row['question2']))    \n\ntrain_kld = df_train.apply(kldr, axis=1, raw=True)\ntest_kld = df_test.apply(kldr, axis=1, raw=True)\n\n# First we create our training and testing data\nx_train = pd.DataFrame()\nx_test = pd.DataFrame()\nx_train['kld'] = train_kld\nx_test['kld'] = test_kld\n\ny_train = df_train['is_duplicate'].values"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5ccae7db-46e1-cf74-fdd8-4d4758bf460f"},"outputs":[],"source":"pos_train = x_train[y_train == 1]\nneg_train = x_train[y_train == 0]\n\n# Now we oversample the negative class\n# There is likely a much more elegant way to do this...\np = 0.165\nscale = ((len(pos_train) / (len(pos_train) + len(neg_train))) / p) - 1\nwhile scale > 1:\n    neg_train = pd.concat([neg_train, neg_train])\n    scale -=1\nneg_train = pd.concat([neg_train, neg_train[:int(scale * len(neg_train))]])\nprint(len(pos_train) / (len(pos_train) + len(neg_train)))\n\nx_train = pd.concat([pos_train, neg_train])\ny_train = (np.zeros(len(pos_train)) + 1).tolist() + np.zeros(len(neg_train)).tolist()\ndel pos_train, neg_train"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"20684b12-fd83-bad6-b00b-77c51dd14c32"},"outputs":[],"source":"# Finally, we split some of the data off for validation\nfrom sklearn.cross_validation import train_test_split\n\nx_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2, random_state=4242)"},{"cell_type":"markdown","metadata":{"_cell_guid":"9df47145-da6a-ad51-7bfb-1b588c4bab06"},"source":"## XGBoost\n\nNow we can finally run XGBoost on our data, in order to see the score on the leaderboard!"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a5f9f7fc-5ec4-fc9f-7fc0-608f5f25e7c6"},"outputs":[],"source":"import xgboost as xgb\n\n# Set our parameters for xgboost\nparams = {}\nparams['objective'] = 'binary:logistic'\nparams['eval_metric'] = 'logloss'\nparams['eta'] = 0.02\nparams['max_depth'] = 4\n\nd_train = xgb.DMatrix(x_train, label=y_train)\nd_valid = xgb.DMatrix(x_valid, label=y_valid)\n\nwatchlist = [(d_train, 'train'), (d_valid, 'valid')]\n\nbst = xgb.train(params, d_train, 400, watchlist, early_stopping_rounds=50, verbose_eval=10)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"11616998-a057-45e0-ac3c-d129e5cfec36"},"outputs":[],"source":"d_test = xgb.DMatrix(x_test)\np_test = bst.predict(d_test)\n\nsub = pd.DataFrame()\nsub['test_id'] = df_test['test_id']\nsub['is_duplicate'] = p_test\nsub.to_csv('kld_xgb.csv', index=False)\nsub.head()"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}