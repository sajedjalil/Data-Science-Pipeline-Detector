{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b66a89eb-5ec4-096e-70ea-d0565f5c5179"},"outputs":[],"source":"#authors:\n#Shiva Ganga\n#Tharunn Golthi\n#Abhinaya\n#Susmitha\n#importing libraries numpy,pandas,mathplotlib for extracting, modifying and visualizing the data\n\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\n\nfrom subprocess import check_output\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7f6b7179-60b6-be20-abdc-ee8d72d877a9"},"outputs":[],"source":"#loading input train file to train\ntrain = pd.read_csv(\"../input/train.csv\")\n#loading iput test file to test\ntest = pd.read_csv(\"../input/test.csv\")\n#printing the top\ntrain.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"72f8d60e-82ec-b011-f49d-c09b2949a651"},"source":"This is how the training data is given. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8be97387-1777-4016-1f8e-02687c72a3b5"},"outputs":[],"source":"test.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"344a6e0d-f644-f3df-d41d-bf520e4e5177"},"source":"The test data only contains questions but not their id's as in train data, as you can see above. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"23c67ba6-4073-f62f-fa16-20f8bcda8e16"},"outputs":[],"source":"train.info()"},{"cell_type":"markdown","metadata":{"_cell_guid":"9852875c-3834-3555-6a90-9c065071032f"},"source":"The training data has 404290 instances. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"806648cb-0bb3-48a4-9a24-1781dbfad8f0"},"outputs":[],"source":"test.info()"},{"cell_type":"markdown","metadata":{"_cell_guid":"1de3fb26-b036-7be9-f1b6-1320599ac315"},"source":"The test data has 2345796 instances."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f7e95e37-e913-fa85-9b43-e1b1a748793f"},"outputs":[],"source":"train_duplicate_mean = train['is_duplicate'].mean()\nprint (\"mean of train data is_duplicate column\",train_duplicate_mean)"},{"cell_type":"markdown","metadata":{"_cell_guid":"b5c20797-ea81-113c-8f35-7b257f8913d5"},"source":"By finding the mean on the is_duplicate field of train data, we see that about 37% of the train data have pair of questions, which are labeled is_duplicate as 1. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0eac5c39-f63f-1a5a-32a1-393f3ac8c41d"},"outputs":[],"source":"pt = train.groupby('is_duplicate')['id'].count()\npt.plot.bar()"},{"cell_type":"markdown","metadata":{"_cell_guid":"da2b8734-2f4e-34aa-105c-c0666c945ea6"},"source":"The plot shows the is_duplicate distribution in the train data. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0388ecf7-b1fb-a2ba-579d-446156ca0a56"},"outputs":[],"source":"# plotting data for number of questions vs number of occurences of the question \nquestion_id_1 = train['qid1'].tolist()\nquestion_id_2 = train['qid2'].tolist()\n\nquestion_id = pd.Series(question_id_1+question_id_2)\nplt.figure(figsize=(15,6))\nplt.hist(question_id.value_counts(), bins= 30)\nplt.yscale('log', nonposy='clip')"},{"cell_type":"markdown","metadata":{"_cell_guid":"f3e7f0f4-ddd7-c668-db98-56667cd121ce"},"source":"By plotting the no. of questions vs no. of occurences of the question, we observe that most of the questions only appear a few times, except very few. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"74224b66-0638-4beb-fa91-ceaedabf6e53"},"outputs":[],"source":"#using nltk corpus for stopwords\nfrom nltk.corpus import stopwords as st\n#stopwords\nstopwords_set = set(st.words(\"english\"))\n\n#returns total words in a sentence\ndef word_dict(sentence):\n    question_words_dict = {}\n    for word in sentence.lower().split():\n        if word not in stopwords_set:\n            question_words_dict[word] = 1\n    return question_words_dict\n#calculating feature common_word_percentage for each row\ndef common_words_percentage(entry):\n    question_1_words = word_dict(str(entry['question1']))\n    question_2_words = word_dict(str(entry['question2']))\n     \n    if len(question_1_words) == 0 or len(question_2_words) == 0:\n        return 0\n    shared_in_q1 = [word for word in question_1_words.keys() if word in question_2_words]\n    feature_Ratio = ( 2*len(shared_in_q1) )/(len(question_1_words)+len(question_2_words))\n    return feature_Ratio"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a7822518-3d6d-61a5-c4d1-48a0568774a2"},"outputs":[],"source":"#calculating tfidf weights \ndef tfidf_weights(entry):\n    question_1_words = word_dict(str(entry['question1']))\n    question_2_words = word_dict(str(entry['question2']))\n    if len(question_1_words) == 0 or len(question_2_words) == 0:\n        return 0\n    \n    common_wts_1 = [weights.get(w, 0) for w in question_1_words.keys() if w in question_2_words]  \n    common_wts_2 = [weights.get(w, 0) for w in question_2_words.keys() if w in question_2_words]\n    common_wts = common_wts_1 + common_wts_2\n    whole_wts = [weights.get(w, 0) for w in question_1_words] + [weights.get(w, 0) for w in question_2_words]\n    \n    feature_tfidf = np.sum(common_wts) / np.sum(whole_wts)\n    return feature_tfidf"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bf91ac4b-7057-d853-a528-cab8288e58a1"},"outputs":[],"source":"\nlist_of_questions = (train['question1'].str.lower().astype('U').tolist() + train['question2'].str.lower().astype('U').tolist())\n#calcutaing Tfifs feature using inbuilt libraries\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(min_df = 50,max_features = 3000000,ngram_range = (1,10))\nX = vectorizer.fit_transform(list_of_questions)\nidf = vectorizer.idf_\nweights = (dict(zip(vectorizer.get_feature_names(), idf)))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"47131045-01a0-c6d4-fbd6-0d7bb043cbc4"},"outputs":[],"source":"#feature train data frame\nX_TrainData = pd.DataFrame()\n#feature test data frame\nX_TestData = pd.DataFrame()\n# adding common_word_percent feature to train data\nX_TrainData['common_word_percent'] = train.apply(common_words_percentage, axis=1, raw=True)\n# adding feature_ifidf feature to train data\nX_TrainData['feature_ifidf'] = train.apply(tfidf_weights, axis = 1, raw = True)\nY_TrainData = train['is_duplicate'].values\n# adding common_word_percent feature to test data\nX_TestData['common_word_percent'] = test.apply(common_words_percentage, axis = 1, raw = True)\n# adding feature_ifidf feature to test data\nX_TestData['feature_ifidf'] = test.apply(tfidf_weights, axis = 1, raw = True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e583c5e8-fc23-aa11-aeb2-7298a16ea8cf"},"outputs":[],"source":"# calculating jacardian similarity\nimport nltk\ndef jaccard_similarity_coefficient(row):\n    if (type(row['question1']) is str) and (type(row['question2']) is str):\n        words_1 = row['question1'].lower().split()\n        words_2 = row['question2'].lower().split()\n    else:\n        #tokeninzing using nltk\n        words_1 = nltk.word_tokenize(str(row['question1']))\n        words_2 = nltk.word_tokenize(str(row['question2']))\n   \n    joint_words = set(words_1).union(set(words_2))\n    intersection_words = set(words_1).intersection(set(words_2))\n    return len(intersection_words)/len(joint_words)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fd1c25f0-7c2f-3915-e556-329d7afcbc2b"},"outputs":[],"source":"# removing NA values in tarainig data\ntrain = train.fillna(\"\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"029631d4-f834-78b9-330d-d226315a2425"},"outputs":[],"source":"# adding jaccard distance feature to train and test data \nX_TrainData['Jacard_Distance'] = train.apply(jaccard_similarity_coefficient, axis = 1, raw = True)\nX_TestData['Jacard_Distance'] = test.apply(jaccard_similarity_coefficient, axis = 1, raw = True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"24a5bdcd-4819-8e07-0de3-633e18bef49e"},"outputs":[],"source":"\nfrom sklearn.metrics.pairwise import cosine_similarity as cs\nimport re, math\nfrom collections import Counter\n\nWORD = re.compile(r'\\w+')\n# calculating the cosine similarity between two vectors\ndef _cosine_similarity(vector_1, vector_2):\n    \n    common_keys = set(vector_1.keys()) & set(vector_2.keys())\n    array1 = [vector_1[x]**2 for x in vector_1.keys()]\n    array2 = [vector_2[x]**2 for x in vector_2.keys()]\n    \n    if not (math.sqrt(sum(array1)) * math.sqrt(sum(array2))):\n        return 0.0\n    else:\n        return (float(sum([vector_1[x] * vector_2[x] for x in common_keys]))) / (math.sqrt(sum(array1)) * math.sqrt(sum(array2)))\n# making sentence to vector format\ndef sentence_transform(sentence):\n     words = WORD.findall(sentence)\n     return Counter(words)\n#method used to find cosine similarity for each row of data frame\ndef cosine_sim(row):\n    vector1 = sentence_transform(str(row['question1']))\n    vector2 = sentence_transform(str(row['question2']))\n    sim = _cosine_similarity(vector1,vector2)\n    return sim\n\nX_TrainData['cosine_sim'] = train.apply(cosine_sim,axis = 1,raw = True )\nX_TestData['cosine_sim'] = test.apply(cosine_sim,axis = 1,raw = True )"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"15a36c7a-8037-ddaa-2f47-7174da80c64f"},"outputs":[],"source":"import csv, math, random , sys, random\nfrom nltk.corpus import wordnet as wn\nfrom nltk.corpus import brown\nimport math\nimport nltk\nimport sys\nfrom nltk.corpus import stopwords\nimport numpy as np\nimport re\nfrom pandas import read_csv\nfrom nltk.corpus import wordnet as wn\nfrom nltk.corpus import brown\nimport math\nimport nltk\nimport sys\n\n\n\n##################\nALPHA = 0.2\nBETA = 0.45\nETA = 0.4\nPHI = 0.2\nDELTA = 0.85\n\nbrown_freqs = dict()\nN = 0\n\n\n######################### word similarity ##########################\ndef get_best_synset_pair(word_1, word_2):\n    \"\"\" \n    Choose the pair with highest path similarity among all pairs. \n    Mimics pattern-seeking behavior of humans.\n    \"\"\"\n    max_sim = -1.0\n    synsets_1 = wn.synsets(word_1)\n    synsets_2 = wn.synsets(word_2)\n    if len(synsets_1) == 0 or len(synsets_2) == 0:\n        return None, None\n    else:\n        max_sim = -1.0\n        best_pair = None, None\n        for synset_1 in synsets_1:\n            for synset_2 in synsets_2:\n                sim = wn.path_similarity(synset_1, synset_2)\n                if sim != None and sim > max_sim:\n                    max_sim = sim\n                    best_pair = synset_1, synset_2\n        return best_pair\n\n\ndef length_dist(synset_1, synset_2):\n    \"\"\"\n    Return a measure of the length of the shortest path in the semantic \n    ontology (Wordnet in our case as well as the paper's) between two \n    synsets.\n    \"\"\"\n    l_dist = sys.maxsize\n    if synset_1 is None or synset_2 is None:\n        return 0.0\n    if synset_1 == synset_2:\n        # if synset_1 and synset_2 are the same synset return 0\n        l_dist = 0.0\n    else:\n        wset_1 = set([str(x.name()) for x in synset_1.lemmas()])\n        wset_2 = set([str(x.name()) for x in synset_2.lemmas()])\n        if len(wset_1.intersection(wset_2)) > 0:\n            # if synset_1 != synset_2 but there is word overlap, return 1.0\n            l_dist = 1.0\n        else:\n            # just compute the shortest path between the two\n            l_dist = synset_1.shortest_path_distance(synset_2)\n            if l_dist is None:\n                l_dist = 0.0\n    # normalize path length to the range [0,1]\n    return math.exp(-ALPHA * l_dist)\n\n\ndef hierarchy_dist(synset_1, synset_2):\n    \"\"\"\n    Return a measure of depth in the ontology to model the fact that \n    nodes closer to the root are broader and have less semantic similarity\n    than nodes further away from the root.\n    \"\"\"\n    h_dist = sys.maxsize\n    if synset_1 is None or synset_2 is None:\n        return h_dist\n    if synset_1 == synset_2:\n        # return the depth of one of synset_1 or synset_2\n        h_dist = max([x[1] for x in synset_1.hypernym_distances()])\n    else:\n        # find the max depth of least common subsumer\n        hypernyms_1 = {x[0]: x[1] for x in synset_1.hypernym_distances()}\n        hypernyms_2 = {x[0]: x[1] for x in synset_2.hypernym_distances()}\n        lcs_candidates = set(hypernyms_1.keys()).intersection(\n            set(hypernyms_2.keys()))\n        if len(lcs_candidates) > 0:\n            lcs_dists = []\n            for lcs_candidate in lcs_candidates:\n                lcs_d1 = 0\n                if lcs_candidate in hypernyms_1:\n                    lcs_d1 = hypernyms_1[lcs_candidate]\n                lcs_d2 = 0\n                if lcs_candidate in hypernyms_2:\n                    lcs_d2 = hypernyms_2[lcs_candidate]\n                lcs_dists.append(max([lcs_d1, lcs_d2]))\n            h_dist = max(lcs_dists)\n        else:\n            h_dist = 0\n    return ((math.exp(BETA * h_dist) - math.exp(-BETA * h_dist)) /\n            (math.exp(BETA * h_dist) + math.exp(-BETA * h_dist)))\n\n\ndef word_similarity(word_1, word_2):\n    synset_pair = get_best_synset_pair(word_1, word_2)\n    return (length_dist(synset_pair[0], synset_pair[1]) *\n            hierarchy_dist(synset_pair[0], synset_pair[1]))\n\n\n######################### sentence similarity ##########################\n\ndef most_similar_word(word, word_set):\n    \"\"\"\n    Find the word in the joint word set that is most similar to the word\n    passed in. We use the algorithm above to compute word similarity between\n    the word and each word in the joint word set, and return the most similar\n    word and the actual similarity value.\n    \"\"\"\n    max_sim = -1.0\n    sim_word = \"\"\n    for ref_word in word_set:\n        sim = word_similarity(word, ref_word)\n        if sim > max_sim:\n            max_sim = sim\n            sim_word = ref_word\n    return sim_word, max_sim\n\n\ndef info_content(lookup_word):\n    \"\"\"\n    Uses the Brown corpus available in NLTK to calculate a Laplace\n    smoothed frequency distribution of words, then uses this information\n    to compute the information content of the lookup_word.\n    \"\"\"\n    global N\n    if N == 0:\n        # poor man's lazy evaluation\n        for sent in brown.sents():\n            for word in sent:\n                word = word.lower()\n                if word not in brown_freqs:\n                    brown_freqs[word] = 0\n                brown_freqs[word] = brown_freqs[word] + 1\n                N = N + 1\n    lookup_word = lookup_word.lower()\n    n = 0 if lookup_word not in brown_freqs else brown_freqs[lookup_word]\n    return 1.0 - (math.log(n + 1) / math.log(N + 1))\n\n\ndef semantic_vector(words, joint_words, info_content_norm):\n    \"\"\"\n    Computes the semantic vector of a sentence. The sentence is passed in as\n    a collection of words. The size of the semantic vector is the same as the\n    size of the joint word set. The elements are 1 if a word in the sentence\n    already exists in the joint word set, or the similarity of the word to the\n    most similar word in the joint word set if it doesn't. Both values are \n    further normalized by the word's (and similar word's) information content\n    if info_content_norm is True.\n    \"\"\"\n    sent_set = set(words)\n    semvec = np.zeros(len(joint_words))\n    i = 0\n    for joint_word in joint_words:\n        if joint_word in sent_set:\n            # if word in union exists in the sentence, s(i) = 1 (unnormalized)\n            semvec[i] = 1.0\n            if info_content_norm:\n                semvec[i] = semvec[i] * math.pow(info_content(joint_word), 2)\n        else:\n            # find the most similar word in the joint set and set the sim value\n            sim_word, max_sim = most_similar_word(joint_word, sent_set)\n            semvec[i] = PHI if max_sim > PHI else 0.0\n            if info_content_norm:\n                semvec[i] = semvec[i] * info_content(joint_word) * info_content(sim_word)\n        i = i + 1\n    return semvec\n\n\ndef semantic_similarity(row):\n    \"\"\"\n    Computes the semantic similarity between two sentences as the cosine\n    similarity between the semantic vectors computed for each sentence.\n    \"\"\"\n    info_content_norm = True\n    sentence_1 = row['question1']\n    sentence_2 = row['question2']\n    \n    words_1 = nltk.word_tokenize(sentence_1)\n    words_2 = nltk.word_tokenize(sentence_2)\n    joint_words = set(words_1).union(set(words_2))\n    vec_1 = semantic_vector(words_1, joint_words, info_content_norm)\n    vec_2 = semantic_vector(words_2, joint_words, info_content_norm)\n    return np.dot(vec_1, vec_2.T) / (np.linalg.norm(vec_1) * np.linalg.norm(vec_2))\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"33874e71-8b28-ad41-c468-470a1869bf58"},"outputs":[],"source":"#if we implement semantic similarity it will take hours and hours of processing time \n#so we are not including the follwing feature \n# we have included the whole results and description of this feature in the report\n\"\"\"X_TrainData['semantic_sim'] = train.apply(semantic_similarity,axis = 1,raw = True )\nX_TestData['semantic_sim'] = test.apply(semantic_similarity,axis = 1,raw = True )\"\"\"\nX_TrainData"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a1315190-bce4-926e-550f-98e77eb89d05"},"outputs":[],"source":"from sklearn.cross_validation import train_test_split\n# train test split validation data 20% and test data 80%\nX_TrainData, X_ValidData, Y_TrainData, Y_ValidData = train_test_split(X_TrainData, Y_TrainData, test_size=0.20, random_state=4242)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"55a97d52-6a96-3c01-8eda-dc2f102767a3"},"outputs":[],"source":"import xgboost as xgb\n\nxg_TrainData = xgb.DMatrix(X_TrainData, label=Y_TrainData)\nxg_ValidData = xgb.DMatrix(X_ValidData, label=Y_ValidData)\n\nwatchlist = [(xg_TrainData, 'train'), (xg_ValidData, 'valid')]\n#training using XGBoost using evalustion metric as logloss\nbst = xgb.train({'objective':'binary:logistic','eval_metric':'logloss','eta':0.02,'max_depth' :5}, xg_TrainData, 500, watchlist, early_stopping_rounds=50, verbose_eval=10)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4717ecee-0732-9a85-bd4c-d1dc849e6d5d"},"outputs":[],"source":"X_TestData.info()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9377d71b-f652-f139-5079-7f90af56d3c8"},"outputs":[],"source":"\nxg_TestData = xgb.DMatrix(X_TestData)\nxg_ValidData = xgb.DMatrix(X_ValidData)\n#predited values using XG boost\nPredict_TestData = bst.predict(xg_TestData)\nPredict_ValidData = bst.predict(xg_ValidData)\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fe07d703-74fc-1bb6-3428-9eba589a3679"},"outputs":[],"source":"#Roc metric\nfrom sklearn.metrics import precision_recall_curve, auc, roc_curve\nfpr, tpr, _ = roc_curve(Y_ValidData, Predict_ValidData)\nroc_area = auc(fpr, tpr)\nplt.plot(fpr, tpr)\nnp.round(roc_area, 10)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"48df184c-b0cf-ad24-47ef-518bbd84f205"},"outputs":[],"source":"# precision Recall curve\nprecison, recall, _ = precision_recall_curve(Y_ValidData, Predict_ValidData)\nplt.figure(figsize=(10,5))\n\nplt.plot(recall, precison)\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nauc(recall, precison)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b83b33e6-efbc-61e9-4478-2cc49ce1be6e"},"outputs":[],"source":"#final classes to result.csv\nresult = pd.DataFrame()\nresult['test_id'] = test['test_id']\nresult['is_duplicate'] = Predict_TestData\nresult.to_csv('result.csv', index=False)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"08649e93-0809-aea9-eb07-9a20a52e76b5"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}