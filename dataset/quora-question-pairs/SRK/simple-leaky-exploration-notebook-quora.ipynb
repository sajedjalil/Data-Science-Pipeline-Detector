{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"832eebdc-9bd3-7439-eac3-548f4730dc3e"},"source":"In this simple exploration notebook, let us try and explore the dataset given for this competition.\n\n**Update on 25 May 2017: Since there are a couple of leaky features now, let us explore the same as well in the notebook** \n\n**Objective:**\n\nTo classify whether question pairs are duplicate or not. \n\nLet us start with importing the necessary modules for exploring the data."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7f540a2b-39a5-b72c-7cbe-c24456547a89"},"outputs":[],"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize, ngrams\nfrom sklearn import ensemble\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import log_loss\nimport xgboost as xgb\n\neng_stopwords = set(stopwords.words('english'))\ncolor = sns.color_palette()\n\n%matplotlib inline\n\npd.options.mode.chained_assignment = None  # default='warn'"},{"cell_type":"markdown","metadata":{"_cell_guid":"8caac5ad-e5ff-0f1b-88e9-4f4b5095210f"},"source":"Let us read both the train and test dataset and check the number of rows."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cd8b05b9-db86-1544-5ab9-00ce8b925bb0"},"outputs":[],"source":"train_df = pd.read_csv(\"../input/train.csv\")\ntest_df = pd.read_csv(\"../input/test.csv\")\nprint(train_df.shape)\nprint(test_df.shape)"},{"cell_type":"markdown","metadata":{"_cell_guid":"3c313511-0a14-0055-0257-e1461cc183df"},"source":"Okay. So there are about 400K rows in train set and about 2.35M rows in test set.\n\nAlso there are 6 columns in train set but only 3 of them are in test set. So we shall first look at the top few lines to understand the columns that are missing in the test set."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ead95ec8-f4af-4936-c87f-6d764f2fbddf"},"outputs":[],"source":"train_df.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"dd3b36a1-744d-c1e1-7adf-8d9d72aa23e4"},"source":"**Data fields**\n\nid - the id of a training set question pair\n\nqid1, qid2 - unique ids of each question (only available in train.csv)\n\nquestion1, question2 - the full text of each question\n\nis_duplicate - the target variable, set to 1 if question1 and question2 have essentially the same meaning, and 0 otherwise."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e6d497c3-8ad5-c2c7-fd36-318335574910"},"outputs":[],"source":"test_df.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"48238b39-6e93-8419-cd5d-ce3451e9b5cc"},"source":"So we do not have question ids for the test set. I hope the reason is as follows:\n\n*As an anti-cheating measure, Kaggle has supplemented the test set with computer-generated question pairs. Those rows do not come from Quora, and are not counted in the scoring. All of the questions in the training set are genuine examples from Quora.*\n\nSince some questions are not from Quora, question ids are not present I think."},{"cell_type":"markdown","metadata":{"_cell_guid":"1454ab55-ba46-843c-e3cd-6d838c573521"},"source":"**Target Variable Exploration:**\n\nFirst let us look at the target variable distribution."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a3206317-0202-af3a-6bd2-589c2341bb3d"},"outputs":[],"source":"is_dup = train_df['is_duplicate'].value_counts()\n\nplt.figure(figsize=(8,4))\nsns.barplot(is_dup.index, is_dup.values, alpha=0.8, color=color[1])\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('Is Duplicate', fontsize=12)\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f77614e0-9691-d277-3f93-c9845bfaac4b"},"outputs":[],"source":"is_dup / is_dup.sum()"},{"cell_type":"markdown","metadata":{"_cell_guid":"531edf4a-103f-6e8e-31d3-d86681ade14d"},"source":"So we have about 63% non-duplicate questions and 37% duplicate questions in the training data set."},{"cell_type":"markdown","metadata":{"_cell_guid":"8d7fee43-3474-d5bd-40d5-e22a70915f45"},"source":"**Questions Exploration:**\n\nNow let us explore the question fields present in the train data. First let us check the number of words distribution in the questions."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b8f19318-a7f5-70c3-7d5c-909d88c0fe5d"},"outputs":[],"source":"all_ques_df = pd.DataFrame(pd.concat([train_df['question1'], train_df['question2']]))\nall_ques_df.columns = [\"questions\"]\n\nall_ques_df[\"num_of_words\"] = all_ques_df[\"questions\"].apply(lambda x : len(str(x).split()))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ee9ebab8-74ca-eb46-6425-0a8cf2895194"},"outputs":[],"source":"cnt_srs = all_ques_df['num_of_words'].value_counts()\n\nplt.figure(figsize=(12,6))\nsns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, color=color[0])\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('Number of words in the question', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"8978d303-eec5-4623-d617-c4f265abd2b0"},"source":"So the distribution is right skewed with upto 237 words in a question. There are also few questions with 1 or 2 words as well.\n\nNow let us explore the number of characters distribution as well."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7316a117-f34a-913f-6620-4a420649e958"},"outputs":[],"source":"all_ques_df[\"num_of_chars\"] = all_ques_df[\"questions\"].apply(lambda x : len(str(x)))\ncnt_srs = all_ques_df['num_of_chars'].value_counts()\n\nplt.figure(figsize=(50,8))\nsns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, color=color[3])\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('Number of characters in the question', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.show()      \n\ndel all_ques_df"},{"cell_type":"markdown","metadata":{"_cell_guid":"07ae62e4-f063-a5c7-ccec-973b08c20297"},"source":"Number of characters distribution as well is right skewed.\n\nOne interesting point is the sudden dip at the 150 character mark. Not sure why is that so.!\n\nNow let us look at the distribution of common unigrams between the given question pairs."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"912aedd0-224f-3bf1-9d39-bf9cbfc78d67"},"outputs":[],"source":"def get_unigrams(que):\n    return [word for word in word_tokenize(que.lower()) if word not in eng_stopwords]\n\ndef get_common_unigrams(row):\n    return len( set(row[\"unigrams_ques1\"]).intersection(set(row[\"unigrams_ques2\"])) )\n\ndef get_common_unigram_ratio(row):\n    return float(row[\"unigrams_common_count\"]) / max(len( set(row[\"unigrams_ques1\"]).union(set(row[\"unigrams_ques2\"])) ),1)\n\ntrain_df[\"unigrams_ques1\"] = train_df['question1'].apply(lambda x: get_unigrams(str(x)))\ntrain_df[\"unigrams_ques2\"] = train_df['question2'].apply(lambda x: get_unigrams(str(x)))\ntrain_df[\"unigrams_common_count\"] = train_df.apply(lambda row: get_common_unigrams(row),axis=1)\ntrain_df[\"unigrams_common_ratio\"] = train_df.apply(lambda row: get_common_unigram_ratio(row), axis=1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a95823cc-6356-59b2-7dc9-07cd3c653072"},"outputs":[],"source":"cnt_srs = train_df['unigrams_common_count'].value_counts()\n\nplt.figure(figsize=(12,6))\nsns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8)\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('Common unigrams count', fontsize=12)\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"82f3c3a9-d3f9-8b1f-cba8-200aeff0ee5f"},"source":"It is interesting to see that there are very few question pairs with no common words. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"94a5f33f-ca3e-8f82-b404-bf26d43ad587"},"outputs":[],"source":"plt.figure(figsize=(12,6))\nsns.boxplot(x=\"is_duplicate\", y=\"unigrams_common_count\", data=train_df)\nplt.xlabel('Is duplicate', fontsize=12)\nplt.ylabel('Common unigrams count', fontsize=12)\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"d4560efc-0de5-e118-d1cd-34f095227abb"},"source":"There is some good difference between 0 and 1 class using the common unigram count variable. Let us look at the same graph using common unigrams ratio."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"576de972-bf27-0f20-9a5e-ef49210a84c5"},"outputs":[],"source":"plt.figure(figsize=(12,6))\nsns.boxplot(x=\"is_duplicate\", y=\"unigrams_common_ratio\", data=train_df)\nplt.xlabel('Is duplicate', fontsize=12)\nplt.ylabel('Common unigrams ratio', fontsize=12)\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"a9b5933e-2544-10b9-6e37-4114cf3cccb7"},"source":"**Leaky Features Exploration:**\n\nNow let us get into the leaky data exploration part. We have a couple of leaky features which seem to improve the score significantly. \n\n 1. [Frequency based feature by Jared Turkewitz][1]\n 2. [Intersection of common neighbors by Krzysztof Dziedzic implemented by tour1st][2]\n\n\n  [1]: https://www.kaggle.com/jturkewitz/magic-features-0-03-gain\n  [2]: https://www.kaggle.com/tour1st/magic-feature-v2-0-045-gain"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9088fc9c-b0b5-6de5-d5ff-5f8f52d0c85f"},"outputs":[],"source":"ques = pd.concat([train_df[['question1', 'question2']], \\\n        test_df[['question1', 'question2']]], axis=0).reset_index(drop='index')\nques.shape"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"878e55b8-f697-26db-94ee-857263c9722b"},"outputs":[],"source":"from collections import defaultdict\nq_dict = defaultdict(set)\nfor i in range(ques.shape[0]):\n        q_dict[ques.question1[i]].add(ques.question2[i])\n        q_dict[ques.question2[i]].add(ques.question1[i])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7ad019b5-6a57-58c7-4b91-e721c53774a9"},"outputs":[],"source":"def q1_freq(row):\n    return(len(q_dict[row['question1']]))\n    \ndef q2_freq(row):\n    return(len(q_dict[row['question2']]))\n    \ndef q1_q2_intersect(row):\n    return(len(set(q_dict[row['question1']]).intersection(set(q_dict[row['question2']]))))\n\ntrain_df['q1_q2_intersect'] = train_df.apply(q1_q2_intersect, axis=1, raw=True)\ntrain_df['q1_freq'] = train_df.apply(q1_freq, axis=1, raw=True)\ntrain_df['q2_freq'] = train_df.apply(q2_freq, axis=1, raw=True)"},{"cell_type":"markdown","metadata":{"_cell_guid":"02b153ba-f37a-8779-49bd-832fd5fd98f0"},"source":"**Q1-Q2 neighbor intersection count:**\n\nLet us first do simple count plots and see the distribution."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8f6cdc66-9ca8-88db-4ac5-dca332fa0652"},"outputs":[],"source":"cnt_srs = train_df['q1_q2_intersect'].value_counts()\n\nplt.figure(figsize=(12,6))\nsns.barplot(cnt_srs.index, np.log1p(cnt_srs.values), alpha=0.8)\nplt.xlabel('Q1-Q2 neighbor intersection count', fontsize=12)\nplt.ylabel('Log of Number of Occurrences', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"11ecf0d9-e95e-fdd6-718d-c973dd9a3c31"},"outputs":[],"source":"grouped_df = train_df.groupby('q1_q2_intersect')['is_duplicate'].aggregate(np.mean).reset_index()\nplt.figure(figsize=(12,8))\nsns.pointplot(grouped_df[\"q1_q2_intersect\"].values, grouped_df[\"is_duplicate\"].values, alpha=0.8, color=color[2])\nplt.ylabel('Mean is_duplicate', fontsize=12)\nplt.xlabel('Q1-Q2 neighbor intersection count', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"2f02bd69-018c-5af2-8667-70d888bb48f8"},"source":"Wow. This explains why this variable is super predictive.!\n\n**Question1 Frequency:**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8ffd458b-d202-be07-da39-cb239bd214a9"},"outputs":[],"source":"cnt_srs = train_df['q1_freq'].value_counts()\n\nplt.figure(figsize=(12,8))\nsns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8)\nplt.xlabel('Q1 frequency', fontsize=12)\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"6bffdeb6-b70b-3341-ad6c-23caf41d1ee4"},"source":"We could see a long tail here as well. Now let us check the target variable distribution."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ca523363-bd44-745c-9a53-03c7a1b59fa2"},"outputs":[],"source":"plt.figure(figsize=(12,8))\ngrouped_df = train_df.groupby('q1_freq')['is_duplicate'].aggregate(np.mean).reset_index()\nsns.barplot(grouped_df[\"q1_freq\"].values, grouped_df[\"is_duplicate\"].values, alpha=0.8, color=color[4])\nplt.ylabel('Mean is_duplicate', fontsize=12)\nplt.xlabel('Q1 frequency', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"73f9b99b-3221-990a-08c4-2313f1a17180"},"source":"Here as well, we can see an increase in the mean target rate as the frequency increases.! Hopefully this is the case with question 2 as well. \n\nLet us now do a heat map between q1_freq and q2_freq to see the target variable distribution."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e7bfde8b-952a-d0c8-1584-f0119283df63"},"outputs":[],"source":"pvt_df = train_df.pivot_table(index=\"q1_freq\", columns=\"q2_freq\", values=\"is_duplicate\")\nplt.figure(figsize=(12,12))\nsns.heatmap(pvt_df)\nplt.title(\"Mean is_duplicate value distribution across q1 and q2 frequency\")\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"31cbb326-d099-cfbb-b942-cac9e5df067e"},"source":"Let us also check the correlation between the three fields."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"46228caf-e5ac-3331-7a6d-ffb3d647a771"},"outputs":[],"source":"cols_to_use = ['q1_q2_intersect', 'q1_freq', 'q2_freq']\ntemp_df = train_df[cols_to_use]\ncorrmat = temp_df.corr(method='spearman')\nf, ax = plt.subplots(figsize=(8, 8))\n\n# Draw the heatmap using seaborn\nsns.heatmap(corrmat, vmax=1., square=True)\nplt.title(\"Leaky variables correlation map\", fontsize=15)\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"6cb9423e-0e01-c393-c6ae-584c608e6e80"},"source":"Stay tuned.! Yet to complete. Please upvote if you find this useful.!"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}