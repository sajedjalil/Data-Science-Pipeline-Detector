{"nbformat_minor":1,"cells":[{"cell_type":"markdown","source":"#Introduction\nThis project is to help explore the potential pattens behind the **Quora question pairs**. We are going to explore the data and try to find the most obvious patterns then have the ability to predict the unseen questions. In this kernel, the following points will be made:\n\n - *The pipeline to solve this problem*\n - *Data Exploration Analysis*\n - *Feature Engineering*\n - *Construct single best model*\n - *Stacking*\n - *Final Prediction*","metadata":{"_active":false,"_uuid":"9156e02c6de262ed4e981f18453598a0c6a0e24a","_cell_guid":"ad4be0f1-41cc-03d9-41dc-e83f6ae9469e"}},{"cell_type":"markdown","source":"#Pipeline\nThe pipeline can be seen as the plans or the processes to solve the problem. The process can be:\n\n***Raw Data -> Features -> Stacking (Consists of best models) -> Final Prediction***\n\nThe further explanation is:\n\n - ***Raw Data:*** This is the first step to deal with the data. The loading process of data, data exploration analysis as well as the basic analysis will be run.\n - ***Features:*** This is the further step to explore the data. This step can also be called as feature engineering, which deals with how to construct the new features, or how to transfer the previous raw data into the features, which can be used by the models. \n - ***Stacking:*** In this part, there are two kinds of processes. Choose the best single model and stack the best models into a final model. \n      - *Choose best single model:* There are a lot of parameters in the model. We should choose the most suitable parameters for the single model and then stack them together. \n      - *Stacking the models:* Stacking is one method of ensembling algorithm. Easily to say, the results from previous model can be seen as the input of the next model. Then, the combination of several models can be seen as a final model to be used to predict the unseen dataset. ","metadata":{"_active":false,"_uuid":"c71806191c89de927a3fdc04aa54bf624dfb1f10","_cell_guid":"cb4ace18-def3-ba90-d668-78ff4c482a0b"}},{"cell_type":"markdown","source":"#Raw Data","metadata":{"_active":false,"_uuid":"7be461533ed95b394fc1d7fcc76d7772366d991e","_cell_guid":"861d1bf3-03a4-a3aa-a215-0c38a0cc31d2"}},{"cell_type":"code","outputs":[],"source":"#Import the important packages. \nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize, ngrams\neng_stopwords = set(stopwords.words('english'))\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\nfrom nltk.corpus import wordnet as wn\nfrom nltk.corpus import wordnet\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize, ngrams","execution_count":null,"metadata":{"_execution_state":"idle","_active":false,"_uuid":"4bc9506b93c42b7ea185e38ebaf00b4f15a1086a","collapsed":true,"_cell_guid":"37f40ec2-04d0-0b51-142f-fa5cd3f064eb"}},{"cell_type":"markdown","source":"##Import data\nIn the following text is the overview of the **training data set**:","metadata":{"_active":false,"_uuid":"26b8796e1e9a0cd799076f3e1be5dc53ce038559","_cell_guid":"36568bd1-e3ac-171b-aae8-fe71af855a85"}},{"cell_type":"code","outputs":[],"source":"train_data = pd.read_csv(\"../input/q_quora.csv\")\nprint(train_data.shape)\ntrain_data.head()","execution_count":null,"metadata":{"_execution_state":"idle","_active":true,"_uuid":"28dfba54eab84ebb24bf07fc42fdc36325ec2cbf","_cell_guid":"d5162da9-0996-83f3-370c-d42a3c4ac80d"}},{"cell_type":"markdown","source":"In the following text is the overview of the **test data set**:","metadata":{"_active":false,"_uuid":"3bc5d5866d8f4db1a9f425c6283029434b20b7f1","_cell_guid":"b8040a0d-a261-337e-d609-1c89de6d3aaa"}},{"cell_type":"code","outputs":[],"source":"test_data = pd.read_csv(\"../input/test.csv\")\nprint(test_data.shape)\ntest_data.head()","execution_count":null,"metadata":{"_active":false,"_uuid":"8fcd3054f884911d6a460a411be0a1bf436974da","collapsed":true,"_cell_guid":"2a969925-aba9-4faa-8fb5-edce88be0531"}},{"cell_type":"markdown","source":"## Basic EDA","metadata":{"_active":false,"_uuid":"25ea1790dc7d1894cc0d747a6006d72994f3e33e","_cell_guid":"8c190b8b-6549-e2c8-39ed-2c2aa7190b77"}},{"cell_type":"code","outputs":[],"source":"is_dup = train_data['is_duplicate'].value_counts()\nsns.barplot(is_dup.index, is_dup.values)","execution_count":null,"metadata":{"_active":false,"_uuid":"b8d00434d33a124ff81bd7354b1ad1ed22a37fa0","collapsed":true,"_cell_guid":"c0e805f9-04b9-61d5-6d05-ffca55d3de27"}},{"cell_type":"code","outputs":[],"source":"train_q1 = train_data['question1']\ntrain_q2 = train_data['question2']\ntrain_q1_length = [len(i) for i in train_q1]\nsns.histplot(train_q1_length)","execution_count":null,"metadata":{"_execution_state":"idle","_active":false,"_uuid":"73c6f667f3c11e2b1274cc45e4568e8573f9c1da","collapsed":true,"_cell_guid":"18a1275c-dd9b-e40c-fdc7-765f357bbf06"}},{"cell_type":"markdown","source":"#Features\nAs mentioned, we need to transfer the data into the features, which can be used in the further model construction. Just as a summary, the possible interesting features can be:\n\n* **Words of the question**\n\n* **Number of the Noun words**\n\n* **Number of the Capital words - 1**\n\n* **tf**\n\n* **dif**\n\n* **tf/dif**\n\n* **Sentiment analysis**\n\nIn the following part, not all the features will be constructed, only the most important ones will be explained. The codes are shown as following. ","metadata":{"_active":false,"_uuid":"4fa9c67bcb1ace85304b1fd134452292f05e2ed5","_cell_guid":"5393bd9d-9972-6ce2-7278-8ce59fc835e4"}},{"cell_type":"code","outputs":[],"source":"from nltk.corpus import wordnet as wn\nfrom nltk.corpus import wordnet\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize, ngrams\nimport os\nimport numpy as np \nimport pandas as pd \nimport csv\nimport re\nfrom collections import Counter\n\neng_stopwords = set(stopwords.words('english'))\nnouns = {x.name().split('.', 1)[0] for x in wn.all_synsets('n')}\n\ndef getWords(text):\n    return re.compile('\\w+').findall(text)\n\ndef getWords_0(text):\n    temp = re.compile('\\w+').findall(str(text))\n    temp = \" \".join(temp).lower()\n    return(temp)\n\ntrans_Q1_AllWords = pd.Series(train_data['question1'].tolist()).astype(str)\ndf_train_Q1 = train_data['question1'].apply(lambda row: getWords_0(row))\nwords_1 = (\" \".join(df_train_Q1)).split()\ncounts_Q1 = Counter(words_1)\n\n\ntrans_Q2_AllWords = pd.Series(train_data['question2'].tolist()).astype(str)\ndf_train_Q2 = train_data['question2'].apply(lambda row: getWords_0(row))\nwords_2 = (\" \".join(df_train_Q2)).split()\ncounts_Q2 = Counter(words_2)\n\n \ndef feature_extraction(text):\n    \n    que1 = str(text['question1']).lower()\n    que2 = str(text['question2']).lower()\n    que1 = getWords(que1)\n    que2 = getWords(que2)\n    \n    feature = []\n    feature.extend([len(que1),len(que2)])\n    \n    Simplified_que1 = [word for word in que1 if word not in eng_stopwords]\n    Simplified_que2 = [word for word in que2 if word not in eng_stopwords]\n    Length_que1 = len(Simplified_que1)\n    Length_que2 = len(Simplified_que2)\n    feature.extend([Length_que1,Length_que2])\n\n    Unique_que1 = [word for word in Simplified_que1 if word not in Simplified_que2]\n    Unique_que2 = [word for word in Simplified_que2 if word not in Simplified_que1]\n    feature.extend([len(Unique_que1),len(Unique_que2)])\n    \n    Unique_que1_Nouns = [word for word in Unique_que1 if word in nouns]\n    Unique_que2_Nouns = [word for word in Unique_que2 if word in nouns]\n    feature.extend([len(Unique_que1_Nouns), len(Unique_que2_Nouns)])\n    \n    # Tfdif\n    df_Q1_1 = 0\n    df_Q1_2 = 0\n    df_Q2_1 = 0\n    df_Q2_2 = 0\n    for i in Unique_que1:\n        df_Q1_1 = df_Q1_1 + counts_Q1[i] / len(words_1)\n        df_Q1_2 = df_Q1_2 + counts_Q2[i] / len(words_2)\n    for i in Unique_que2:\n        df_Q2_1 = df_Q2_1 + counts_Q1[i] / len(words_1)\n        df_Q2_2 = df_Q2_2 + counts_Q2[i] / len(words_2)\n        \n    feature.extend([df_Q1_1, df_Q1_2, df_Q2_1, df_Q2_2])\n\n    return(feature)\n\ndf_train_Questions = train_data[['question1','question2']]\ntrain_X = np.vstack( np.array(df_train_Questions.apply(lambda row: feature_extraction(row), axis=1)) )\n\ntrain_Y = train_data['is_duplicate']\n\npos_train = train_X[train_Y == 1]\nneg_train = train_X[train_Y == 0]\nindex = np.random.choice(len(pos_train), int(0.17 * 255027), replace = False)\npos_train = pos_train[index]\ntrain_X = np.concatenate([pos_train, neg_train])\ntrain_Y = np.concatenate([np.zeros(len(pos_train)) + 1, np.zeros(len(neg_train))])","execution_count":null,"metadata":{"_active":false,"_uuid":"31d39f3da05106bd38114bb21227328e5ad3600b","collapsed":true,"_cell_guid":"d78074b8-b471-e558-4fff-962e3740992a"}},{"cell_type":"code","outputs":[],"source":"train_X","execution_count":null,"metadata":{"_active":false,"_uuid":"b3727a2efbe9d79bb610a21029ff0dd25cdc1a18","collapsed":true,"_cell_guid":"29a94708-3ce5-fef2-2acd-40b70750ad92"}},{"cell_type":"markdown","source":"# Model Construction\nThere are a lot of models can be used in this problem. We should test them and find the best parameter of these models, then stack them together to give a final prediction of the problem. ","metadata":{"_active":false,"_uuid":"d9b36f052bf903640af9e02287269678600d0949","_cell_guid":"84e599d1-71c2-5937-e41d-a8b7242de939"}},{"cell_type":"markdown","source":"##Single best model \nThe basic things for the model is the single model. We should find the best parameters for the single model and then use the same method to construct the other models. ","metadata":{"_active":false,"_uuid":"c21e81ec6ca0212010eb0e55e753b00546ca12c2","_cell_guid":"56c34e9a-c2d2-b363-f589-a6690972870d"}},{"cell_type":"code","outputs":[],"source":"# Here one simple example will show the basic way to find the best model. The RandomForest algorithm will be used here\nfrom sklearn.ensemble import RandomForestClassifier\nfrom hyperopt import hp, fmin, tpe, STATUS_OK, Trials\nfrom sklearn.model_selection import train_test_split, cross_val_score\n\n# Here the train_X is the features which are generated from the raw data. Train_Y is the results\nX_train, X_valid, y_train, y_valid = train_test_split(train_X, train_Y, test_size=0.2, random_state=1234)\n\ndef objective(space):\n    clf = RandomForestClassifier(**space)\n    logloss = cross_val_score(clf,train_X,train_Y).mean()\n    print (\"SCORE:\", logloss)\n    return{'loss':logloss, 'status': STATUS_OK }\n\nspace4rf = {\n    'max_depth': hp.choice('max_depth', range(1,20)),\n    'max_features': hp.choice('max_features', range(1,5)),\n    'n_estimators': hp.choice('n_estimators', range(1,20)),\n    'criterion': hp.choice('criterion', [\"gini\", \"entropy\"]),\n    'scale': hp.choice('scale', [0, 1]),\n    'normalize': hp.choice('normalize', [0, 1])\n}\n\ntrials = Trials()\nbest = fmin(fn=objective,\n            space=space4rf,\n            algo=tpe.suggest,\n            max_evals=10,\n            trials=trials)\n\nprint (best)","execution_count":null,"metadata":{"_active":false,"_uuid":"768899e8d7aafff629c5c3b11af2aee96caf59a4","collapsed":true,"_cell_guid":"8d0b44f3-8bab-2c32-1685-3fedf4974a5b"}},{"cell_type":"markdown","source":"## Stacking\nAfter we have the basic best model from the previous process, now we can begin to stack them together. Stacking is one method of the ensambling algorithm, which aims to combine several models together to get a better model. The principles of stacking is not so hard, just use the model to create results, which can be seen as the new features for the next stacking. ","metadata":{"_active":false,"_uuid":"c79981db2b7d3b494566c6305e538c118a93cd47","_cell_guid":"f3e1f4aa-3724-5b96-45ec-fc22b031c011"}},{"cell_type":"code","outputs":[],"source":"# The code for the stacking. \n\nclfs = [RandomForestClassifier(n_estimators=100, criterion='gini'),\n        RandomForestClassifier(n_estimators=100, criterion='entropy'),\n        RandomForestClassifier(n_estimators=10, criterion='gini'),\n        RandomForestClassifier(n_estimators=10, criterion='entropy'),\n        ExtraTreesClassifier(n_estimators=100, criterion='gini'),\n        ExtraTreesClassifier(n_estimators=100, criterion='entropy'),\n        ExtraTreesClassifier(n_estimators=10, criterion='gini'),\n        ExtraTreesClassifier(n_estimators=10, criterion='entropy'),\n        GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=50),\n        KNeighborsClassifier(n_neighbors=5),\n        KNeighborsClassifier(n_neighbors=10),\n        GaussianNB(),\n        LogisticRegression()]\n\nn_folds = 5\nskf = list(StratifiedKFold(train_Y, n_folds))\n\ndataset_blend_train = np.zeros((train_X.shape[0], len(clfs)))\ndataset_blend_test = np.zeros((test_X.shape[0], len(clfs)))\n    \nfor j, clf in enumerate(clfs):\n    print (j, clf)\n    dataset_blend_test_j = np.zeros((test_X.shape[0], len(skf)))\n    for i, (train, test) in enumerate(skf):\n        print (\"Fold\", i)\n        X_train = train_X[train]\n        y_train = train_Y[train]\n        X_test = train_X[test]\n        y_test = train_Y[test]\n        clf.fit(X_train, y_train)\n        y_submission = clf.predict_proba(X_test)[:, 1]\n        dataset_blend_train[test, j] = y_submission\n        dataset_blend_test_j[:, i] = clf.predict_proba(test_X)[:, 1]\n    dataset_blend_test[:, j] = dataset_blend_test_j.mean(1)","execution_count":null,"metadata":{"_active":false,"_uuid":"eb499ebeef21362dde0785c1c69bc23f3334ec49","collapsed":true,"_cell_guid":"1fe75d4f-201b-cd95-bedc-e8e561ea7e83"}},{"cell_type":"markdown","source":"###Final Prediction\nAfter the stacking, we have new features, which can be used to make the final prediction. Here I choose XGBoost as the algorithm to make the final prediction. The process is almost similar with the previous steps. ","metadata":{"_active":false,"_uuid":"49e5a8a13757c5298d76fa54fc5b74fe2a2ac40f","_cell_guid":"8dcaa780-7a1e-2408-bd2f-ecb01953b14f"}},{"cell_type":"code","outputs":[],"source":"kf = KFold(n_splits=5, shuffle=True, random_state=2016)\n\nfor dev_index, val_index in kf.split(range(dataset_blend_train_L2.shape[0])):\n    \n    params = {}\n    params[\"objective\"] = \"binary:logistic\"\n    params['eval_metric'] = 'logloss'\n    params[\"eta\"] = 0.3\n    params[\"subsample\"] = 0.7\n    params[\"min_child_weight\"] = 2\n    params[\"colsample_bytree\"] = 0.7\n    params[\"max_depth\"] = 5\n    params[\"silent\"] = 1\n    dev_X, val_X = dataset_blend_train_L2[dev_index,:], dataset_blend_train_L2[val_index,:]\n    dev_y, val_y = train_Y[dev_index], train_Y[val_index]\n    d_train = xgb.DMatrix(dev_X, label = dev_y)\n    d_test = xgb.DMatrix(val_X, label = val_y)\n    watchlist = [ (d_train,'train'), (d_test, 'test') ]\n    bst = xgb.train(params, d_train, 400, watchlist, early_stopping_rounds=50, verbose_eval=10)\n    break\n\n# Prediction\ndataset_blend_test = xgb.DMatrix(dataset_blend_test_L2)\ndata_XGB = xgb.DMatrix(test_X)\npredict_y_XGB = bst.predict(data_XGB)\ndata_prediction = predict_y_XGB\n\ndef Prediction(training_data, testing_data, clfs):\n    for i in clfs:\n        temp_model = i.fit(training_data, train_Y)\n        temp_prediction = temp_model.predict(testing_data)\n        data_prediction = np.vstack([data_prediction,temp_prediction])\n        print(i)\n        \nfor i in clfs_prediction:\n    temp_model = i.fit(dataset_blend_train_L2, train_Y)\n    temp_prediction = temp_model.predict(dataset_blend_test_L2)\n    data_prediction = np.vstack([data_prediction,temp_prediction])\n    print(i)\n\nsub = pd.DataFrame()\nsub['test_id'] = df_test['test_id']\nsub['is_duplicate'] = data_prediction\nsub.to_csv(\"submission.csv\", index=False)","execution_count":null,"metadata":{"_active":false,"_uuid":"9c1c5c6ed3a14c8576d676061994635370a7c3f9","collapsed":true,"_cell_guid":"5fdf14c6-02f4-80fc-f41e-fcb677400e3d"}},{"cell_type":"markdown","source":"#Introduction\nThis project is to help explore the potential pattens behind the **Quora question pairs**. We are going to explore the data and try to find the most obvious patterns then have the ability to predict the unseen questions. In this kernel, the following points will be made:\n\n - *The pipeline to solve this problem*\n - *Data Exploration Analysis*\n - *Feature Engineering*\n - *Construct single best model*\n - *Stacking*\n - *Final Prediction*","metadata":{"_active":false,"_uuid":"71d01fb15ea05c7575fc8ec267df211dd95f09e4","_cell_guid":"ea5ef5eb-88db-8aee-1ffe-31ff148e4981"}},{"cell_type":"markdown","source":"#Pipeline\nThe pipeline can be seen as the plans or the processes to solve the problem. The process can be:\n\n***Raw Data -> Features -> Stacking (Consists of best models) -> Final Prediction***\n\nThe further explanation is:\n\n - ***Raw Data:*** This is the first step to deal with the data. The loading process of data, data exploration analysis as well as the basic analysis will be run.\n - ***Features:*** This is the further step to explore the data. This step can also be called as feature engineering, which deals with how to construct the new features, or how to transfer the previous raw data into the features, which can be used by the models. \n - ***Stacking:*** In this part, there are two kinds of processes. Choose the best single model and stack the best models into a final model. \n      - *Choose best single model:* There are a lot of parameters in the model. We should choose the most suitable parameters for the single model and then stack them together. \n      - *Stacking the models:* Stacking is one method of ensembling algorithm. Easily to say, the results from previous model can be seen as the input of the next model. Then, the combination of several models can be seen as a final model to be used to predict the unseen dataset. ","metadata":{"_active":false,"_uuid":"30ad00160847be931acfe50f3ee7952cf96ea979","_cell_guid":"497a90b8-c5f8-8d5f-4e89-7e56dac34025"}},{"cell_type":"markdown","source":"#Raw Data","metadata":{"_active":false,"_uuid":"af1897b1916ee9f101596b335d65b07632fb2f20","_cell_guid":"d582f0f6-0a2c-5df3-37c6-988afb7b3f88"}},{"cell_type":"code","outputs":[],"source":"#Import the important packages. \nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize, ngrams\neng_stopwords = set(stopwords.words('english'))\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\nfrom nltk.corpus import wordnet as wn\nfrom nltk.corpus import wordnet\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize, ngrams","execution_count":null,"metadata":{"_active":false,"_uuid":"9b3b9ed443cf7ae8a328b016c38937cea4b798f2","collapsed":true,"_cell_guid":"2ce447ec-c34e-320f-f3a5-f65016eb9fe8"}},{"cell_type":"markdown","source":"##Import data\nIn the following text is the overview of the **training data set**:","metadata":{"_active":false,"_uuid":"9887c9a5f035f3daa9fa0c52754c17d96b30e2dd","_cell_guid":"663e2b6b-8455-b07b-afc2-48311ee9e593"}},{"cell_type":"code","outputs":[],"source":"train_data = pd.read_csv(\"../input/train.csv\")\nprint(train_data.shape)\ntrain_data.head()","execution_count":null,"metadata":{"_active":false,"_uuid":"e6c38fec5627d96e473937497c4b19d11e6c0c4a","collapsed":true,"_cell_guid":"f7f6475b-84c2-abf1-9ded-e6709b7cc6c5"}},{"cell_type":"markdown","source":"In the following text is the overview of the **test data set**:","metadata":{"_active":false,"_uuid":"54f6532121f488df68b12171b636b9512fe63e12","_cell_guid":"e4bec55c-68ad-ff15-8950-1620d339e2ae"}},{"cell_type":"code","outputs":[],"source":"test_data = pd.read_csv(\"../input/test.csv\")\nprint(test_data.shape)\ntest_data.head()","execution_count":null,"metadata":{"_active":false,"_uuid":"e1881abfda40e6603baad6705af135d6a69c6e7b","collapsed":true,"_cell_guid":"f05b3776-2681-1a57-7e1c-ef680524550f"}},{"cell_type":"markdown","source":"## Basic EDA","metadata":{"_active":false,"_uuid":"e4c25ecd8e251c8f9d63b3d5ac231f1648cf63b5","_cell_guid":"7df316c6-30c7-ff6e-ebcf-64c045fd6212"}},{"cell_type":"code","outputs":[],"source":"is_dup = train_data['is_duplicate'].value_counts()\nsns.barplot(is_dup.index, is_dup.values)","execution_count":null,"metadata":{"_active":false,"_uuid":"18f80c8d8e67c5601d05ca99a7aa5b5b7258dc87","collapsed":true,"_cell_guid":"9751f7a5-86fd-6285-2e02-dfbd5ed7d843"}},{"cell_type":"code","outputs":[],"source":"train_q1 = train_data['question1']\ntrain_q2 = train_data['question2']\ntrain_q1_length = [len(i) for i in train_q1]\nsns.distplot(train_q1_length)","execution_count":null,"metadata":{"_active":false,"_uuid":"71eccd8a4196f8e3802b5838ac9720701acfadb1","collapsed":true,"_cell_guid":"c2fd2202-4280-7b40-e054-3e054aa95283"}},{"cell_type":"markdown","source":"#Features\nAs mentioned, we need to transfer the data into the features, which can be used in the further model construction. Just as a summary, the possible interesting features can be:\n\n* **Words of the question**\n\n* **Number of the Noun words**\n\n* **Number of the Capital words - 1**\n\n* **tf**\n\n* **dif**\n\n* **tf/dif**\n\n* **Sentiment analysis**\n\nIn the following part, not all the features will be constructed, only the most important ones will be explained. The codes are shown as following. ","metadata":{"_active":false,"_uuid":"fb6ccc5062aabb58614f0314c89ea138c56bf6ea","_cell_guid":"c07ae313-843a-027f-7378-17d4bbe3db46"}},{"cell_type":"code","outputs":[],"source":"from nltk.corpus import wordnet as wn\nfrom nltk.corpus import wordnet\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize, ngrams\nimport os\nimport numpy as np \nimport pandas as pd \nimport csv\nimport re\nfrom collections import Counter\n\neng_stopwords = set(stopwords.words('english'))\nnouns = {x.name().split('.', 1)[0] for x in wn.all_synsets('n')}\n\ndef getWords(text):\n    return re.compile('\\w+').findall(text)\n\ndef getWords_0(text):\n    temp = re.compile('\\w+').findall(str(text))\n    temp = \" \".join(temp).lower()\n    return(temp)\n\ntrans_Q1_AllWords = pd.Series(train_data['question1'].tolist()).astype(str)\ndf_train_Q1 = train_data['question1'].apply(lambda row: getWords_0(row))\nwords_1 = (\" \".join(df_train_Q1)).split()\ncounts_Q1 = Counter(words_1)\n\n\ntrans_Q2_AllWords = pd.Series(train_data['question2'].tolist()).astype(str)\ndf_train_Q2 = train_data['question2'].apply(lambda row: getWords_0(row))\nwords_2 = (\" \".join(df_train_Q2)).split()\ncounts_Q2 = Counter(words_2)\n\n \ndef feature_extraction(text):\n    \n    que1 = str(text['question1']).lower()\n    que2 = str(text['question2']).lower()\n    que1 = getWords(que1)\n    que2 = getWords(que2)\n    \n    feature = []\n    feature.extend([len(que1),len(que2)])\n    \n    Simplified_que1 = [word for word in que1 if word not in eng_stopwords]\n    Simplified_que2 = [word for word in que2 if word not in eng_stopwords]\n    Length_que1 = len(Simplified_que1)\n    Length_que2 = len(Simplified_que2)\n    feature.extend([Length_que1,Length_que2])\n\n    Unique_que1 = [word for word in Simplified_que1 if word not in Simplified_que2]\n    Unique_que2 = [word for word in Simplified_que2 if word not in Simplified_que1]\n    feature.extend([len(Unique_que1),len(Unique_que2)])\n    \n    Unique_que1_Nouns = [word for word in Unique_que1 if word in nouns]\n    Unique_que2_Nouns = [word for word in Unique_que2 if word in nouns]\n    feature.extend([len(Unique_que1_Nouns), len(Unique_que2_Nouns)])\n    \n    # Tfdif\n    df_Q1_1 = 0\n    df_Q1_2 = 0\n    df_Q2_1 = 0\n    df_Q2_2 = 0\n    for i in Unique_que1:\n        df_Q1_1 = df_Q1_1 + counts_Q1[i] / len(words_1)\n        df_Q1_2 = df_Q1_2 + counts_Q2[i] / len(words_2)\n    for i in Unique_que2:\n        df_Q2_1 = df_Q2_1 + counts_Q1[i] / len(words_1)\n        df_Q2_2 = df_Q2_2 + counts_Q2[i] / len(words_2)\n        \n    feature.extend([df_Q1_1, df_Q1_2, df_Q2_1, df_Q2_2])\n\n    return(feature)\n\ndf_train_Questions = train_data[['question1','question2']]\ntrain_X = np.vstack( np.array(df_train_Questions.apply(lambda row: feature_extraction(row), axis=1)) )\n\ntrain_Y = train_data['is_duplicate']\n\npos_train = train_X[train_Y == 1]\nneg_train = train_X[train_Y == 0]\nindex = np.random.choice(len(pos_train), int(0.17 * 255027), replace = False)\npos_train = pos_train[index]\ntrain_X = np.concatenate([pos_train, neg_train])\ntrain_Y = np.concatenate([np.zeros(len(pos_train)) + 1, np.zeros(len(neg_train))])","execution_count":null,"metadata":{"_active":false,"_uuid":"e20a96d5b8ba1d5faf100eb29197b7d72fd97f91","collapsed":true,"_cell_guid":"7c456d4d-bb80-24ec-ef1e-0953e86a5860"}},{"cell_type":"code","outputs":[],"source":"train_X","execution_count":null,"metadata":{"_active":false,"_uuid":"e3e48a8c13bd6cf96a07e0d07126bfcf8791f2d8","collapsed":true,"_cell_guid":"0ba251d3-39fd-2387-c12b-3f7b87aef138"}},{"cell_type":"markdown","source":"# Model Construction\nThere are a lot of models can be used in this problem. We should test them and find the best parameter of these models, then stack them together to give a final prediction of the problem. ","metadata":{"_active":false,"_uuid":"07db1a6f3b685580fe4884bd16314735080d6dbd","_cell_guid":"99ad593b-954a-ef71-e0ff-2cdf69b9ae64"}},{"cell_type":"markdown","source":"##Single best model \nThe basic things for the model is the single model. We should find the best parameters for the single model and then use the same method to construct the other models. ","metadata":{"_active":false,"_uuid":"0c3412adb1f49faf690e30f4dbd2801f71b95ae0","_cell_guid":"a9f64926-c001-e3cd-14d0-45e52c173f72"}},{"cell_type":"code","outputs":[],"source":"# Here one simple example will show the basic way to find the best model. The RandomForest algorithm will be used here\nfrom sklearn.ensemble import RandomForestClassifier\nfrom hyperopt import hp, fmin, tpe, STATUS_OK, Trials\nfrom sklearn.model_selection import train_test_split, cross_val_score\n\n# Here the train_X is the features which are generated from the raw data. Train_Y is the results\nX_train, X_valid, y_train, y_valid = train_test_split(train_X, train_Y, test_size=0.2, random_state=1234)\n\ndef objective(space):\n    clf = RandomForestClassifier(**space)\n    logloss = cross_val_score(clf,train_X,train_Y).mean()\n    print (\"SCORE:\", logloss)\n    return{'loss':logloss, 'status': STATUS_OK }\n\nspace4rf = {\n    'max_depth': hp.choice('max_depth', range(1,20)),\n    'max_features': hp.choice('max_features', range(1,5)),\n    'n_estimators': hp.choice('n_estimators', range(1,20)),\n    'criterion': hp.choice('criterion', [\"gini\", \"entropy\"])\n}\n\ntrials = Trials()\nbest = fmin(fn=objective,\n            space=space4rf,\n            algo=tpe.suggest,\n            max_evals=10,\n            trials=trials)\n\nprint (best)","execution_count":null,"metadata":{"_active":false,"_uuid":"259663d9c218aa14dd74029adaaca860bc9f7887","collapsed":true,"_cell_guid":"5edd62d4-3bba-e64b-dd80-348e7da633a9"}},{"cell_type":"markdown","source":"## Stacking\nAfter we have the basic best model from the previous process, now we can begin to stack them together. Stacking is one method of the ensambling algorithm, which aims to combine several models together to get a better model. The principles of stacking is not so hard, just use the model to create results, which can be seen as the new features for the next stacking. ","metadata":{"_active":false,"_uuid":"84ec713a4cbe9122bf4de387ad01c324c37a7646","_cell_guid":"8f1ab29a-acbd-0845-5c58-ea7f0ea0525d"}},{"cell_type":"code","outputs":[],"source":"# The code for the stacking. It will take long time to train the model, but it works.  \nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier,DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestClassifier,RandomForestRegressor,ExtraTreesClassifier,ExtraTreesRegressor\nfrom sklearn.model_selection import cross_val_predict, GridSearchCV, train_test_split\nfrom sklearn.cross_validation import StratifiedKFold\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import Ridge\n\nclfs = [RandomForestClassifier(n_estimators=100, criterion='gini'),\n        RandomForestClassifier(n_estimators=100, criterion='entropy'),\n        RandomForestClassifier(n_estimators=10, criterion='gini'),\n        RandomForestClassifier(n_estimators=10, criterion='entropy'),\n        ExtraTreesClassifier(n_estimators=100, criterion='gini'),\n        ExtraTreesClassifier(n_estimators=100, criterion='entropy'),\n        ExtraTreesClassifier(n_estimators=10, criterion='gini'),\n        ExtraTreesClassifier(n_estimators=10, criterion='entropy'),\n        GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=50),\n        KNeighborsClassifier(n_neighbors=5),\n        KNeighborsClassifier(n_neighbors=10),\n        GaussianNB(),\n        LogisticRegression()]\n\ntest_X = np.vstack( np.array(test_data.apply(lambda row: feature_extraction(row), axis=1)) )\nX_train, X_test, Y_train, Y_test = train_test_split(train_X, train_Y, test_size = 0.3)\n\n\nn_folds = 5\nskf = list(StratifiedKFold(train_Y, n_folds))\n\ndataset_blend_train = np.zeros((train_X.shape[0], len(clfs)))\ndataset_blend_test = np.zeros((test_X.shape[0], len(clfs)))\n    \nfor j, clf in enumerate(clfs):\n    print (j, clf)\n    dataset_blend_test_j = np.zeros((test_X.shape[0], len(skf)))\n    for i, (train, test) in enumerate(skf):\n        print (\"Fold\", i)\n        X_train = train_X[train]\n        y_train = train_Y[train]\n        X_test = train_X[test]\n        y_test = train_Y[test]\n        clf.fit(X_train, y_train)\n        y_submission = clf.predict_proba(X_test)[:, 1]\n        dataset_blend_train[test, j] = y_submission\n        dataset_blend_test_j[:, i] = clf.predict_proba(test_X)[:, 1]\n    dataset_blend_test[:, j] = dataset_blend_test_j.mean(1)","execution_count":null,"metadata":{"_active":false,"_uuid":"30796566cd15f0e4715df26ac405684af5aab490","collapsed":true,"_cell_guid":"c6aecf5d-acb1-3b52-f4f5-9b7ca7ad58df"}},{"cell_type":"markdown","source":"###Final Prediction\nAfter the stacking, we have new features, which can be used to make the final prediction. Here I choose XGBoost as the algorithm to make the final prediction. The process is almost similar with the previous steps. ","metadata":{"_active":false,"_uuid":"eedd69e0b3a123c2e8803abc717cb73030203605","_cell_guid":"ad14025c-8b4c-7549-00ee-1b2fd6f5cac5"}},{"cell_type":"code","outputs":[],"source":"import xgboost as xgb\nfrom sklearn.model_selection import KFold\nkf = KFold(n_splits=5, shuffle=True, random_state=2016)\nfor dev_index, val_index in kf.split(range(dataset_blend_train.shape[0])):\n    \n    params = {}\n    params[\"objective\"] = \"binary:logistic\"\n    params['eval_metric'] = 'logloss'\n    params[\"eta\"] = 0.3\n    params[\"subsample\"] = 0.7\n    params[\"min_child_weight\"] = 2\n    params[\"colsample_bytree\"] = 0.7\n    params[\"max_depth\"] = 5\n    params[\"silent\"] = 1\n    dev_X, val_X = dataset_blend_train[dev_index,:], dataset_blend_train[val_index,:]\n    dev_y, val_y = train_Y[dev_index], train_Y[val_index]\n    d_train = xgb.DMatrix(dev_X, label = dev_y)\n    d_test = xgb.DMatrix(val_X, label = val_y)\n    watchlist = [ (d_train,'train'), (d_test, 'test') ]\n    bst = xgb.train(params, d_train, 400, watchlist, early_stopping_rounds=50, verbose_eval=10)\n    break\n\n# Prediction\ndataset_blend_test = xgb.DMatrix(dataset_blend_test)\npredict_y_XGB = bst.predict(dataset_blend_test)\ndata_prediction = predict_y_XGB\n\nsub = pd.DataFrame()\nsub['test_id'] = df_test['test_id']\nsub['is_duplicate'] = data_prediction\nsub.to_csv(\"submission.csv\", index=False)","execution_count":null,"metadata":{"_active":false,"_uuid":"49d197180222ebd7bf7929b97e94364cbf70c76c","collapsed":true,"_cell_guid":"2c3cfe2e-c917-4c2b-1fbe-4a38c2cf0cee"}}],"nbformat":4,"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"},"language_info":{"file_extension":".py","pygments_lexer":"ipython3","version":"3.6.3","name":"python","nbconvert_exporter":"python","mimetype":"text/x-python","codemirror_mode":{"version":3,"name":"ipython"}}}}