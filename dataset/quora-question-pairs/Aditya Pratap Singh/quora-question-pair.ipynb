{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!cp /kaggle/input/quora-question-pairs/train.csv.zip .\n!unzip train.csv.zip\n!ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! pip install distance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib import rcParams\nimport nltk\nfrom collections import Counter \nimport distance\nfrom matplotlib import rcParams\nimport plotly.express as px\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.cm as cm\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom subprocess import check_output\n%matplotlib inline\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport os\nimport gc\nimport sys\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport re\nfrom nltk.corpus import stopwords\nfrom tqdm import tqdm\nfrom sklearn.preprocessing import normalize\nfrom nltk.stem import PorterStemmer\nfrom bs4 import BeautifulSoup\nimport collections\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"train.csv\")\n\nprint(\"Number of data points:\",df.shape[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading Data And Getting Basics Stats"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Here we can see that we have some missing value in question1 (1 Question) and question2 (2 Question)"},{"metadata":{},"cell_type":"markdown","source":"<h3> 3.2.1 Distribution of data points among output classes</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking if we have imbalance data or not\ndf.groupby(\"is_duplicate\")['id'].count().plot.bar()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking if dataset is balanced or imbalanced"},{"metadata":{"trusted":true},"cell_type":"code","source":"unique=len(set(list(df['qid1'])+list(df['qid2'])))\nprint ('Total number of  Unique Questions are: {}\\n'.format(unique))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Percentage of Similar question and Non-Similar Question"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('-->> Question pairs are not Similar (is_duplicate = 0):\\n     {}%'.format(100 - round(df['is_duplicate'].mean()*100, 2)))\nprint('-->> Question pairs are Similar (is_duplicate = 1):\\n     {}%'.format(round(df['is_duplicate'].mean()*100, 2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"qids = pd.Series(list(df['qid1']) + list(df['qid2'])) #Creating a dataframe that contains the question id of both quid1 and quid2\nunique_qs = len(np.unique(qids))  #Numpy array to filter down all qniue elements\nqs_morethan_onetime = np.sum(qids.value_counts() > 1)   # Counts any question that have been repeated more than one time\nprint ('Total number of  Unique Questions are: {}\\n'.format(unique_qs))\n#len(set(list(df['qid1'])+list(df['qid2'])))\n\n\nprint ('Number of unique questions that appear more than one time: {} ({}%)\\n'.format(qs_morethan_onetime,round(qs_morethan_onetime/unique_qs*100,2)))\n\nprint ('Max number of times a single question is repeated: {}\\n'.format(max(qids.value_counts()))) # Taking the frequency of all question and printing the max of them\n\nq_vals=qids.value_counts()\nprint(q_vals)\n\nq_vals=q_vals.values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Here we can see questions with qid [2559 ,30182 4044] are some the question that are repeated multiple times\n### *Lets have look at those questions one by one*"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Looking at questions that are asked most frequently \n\nprint(df.loc[df['qid1']==2559]['question1'].head(1).values)\nprint('='*50)\nprint(\" \"*50)\nprint(df.loc[df['qid1']==30182]['question1'].head(1).values)\nprint('='*50)\nprint(\" \"*50)\nprint(df.loc[df['qid1']==4044]['question1'].head(1).values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = [\"unique_questions\" , \"Repeated Questions\"]\ny =  [unique_qs , qs_morethan_onetime]\n\nplt.figure(figsize=(8, 6))\nplt.title (\"Plot representing unique and repeated questions  \")\nsns.barplot(x,y)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking whether there are any repeated pair of questions\n\nduplicateRowsDF = df[df.duplicated(['qid1','qid2'])]   # Collecting all Duplicate data into a dataframe so than we can also see what duplicate value we have\n\nprint (\"Number of duplicate questions : \",duplicateRowsDF.shape[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plotting Questions based on there frequency"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(30, 10))\n\nplt.hist(qids.value_counts(), bins=250)\n\nplt.yscale('log')\n\nplt.title('Log-Histogram of question appearance counts')\n\nplt.xlabel('Number of occurences of question')\n\nplt.ylabel('Number of questions')\n\nprint ('Maximum number of times a single question is repeated: {}\\n'.format(max(qids.value_counts()))) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Checking and Removing Null Value"},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df.isnull().any(1)] # Checking if any value is null in our dataset\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### As we have very few points with NULL value then its better to remove them"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dropna(inplace=True)  # Droping Null Value\ndf[df.isnull().any(1)] \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  Basic Feature Extraction"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Q1_Len']=df['question1'].str.len()  # Finding length of Question 1\ndf['Q2_Len']=df['question2'].str.len()  # Finding length of Question 2\n\ndf['Q1_Words']=df['question1'].apply(lambda row : len(row.split()))   # Finding Number of Words in Question 1\ndf['Q2_Words']=df['question2'].apply(lambda row : len(row.split()))   # Finding Number of Words in Question 2\n\n# Fucntion to find the number of common words in Question 1 and Question 2\n\ndef common(row):\n    '''\n    We are converting both Question 1 and Question 2 to set (and also converting them to lower so that every word have same) and finding there intersection so that we can get common words\n    Then we are simply finding the lenth of those common words\n    '''\n    return len((set(row['question1'].lower().split())).intersection(set(row['question2'].lower().split())))\n\n\n# Creating New Column with Number of Common Words\ndf['common_Word'] = df.apply(common, axis=1)\n\n\n# Total Number of Distict words in both question1 and question2\n\ndef total(row):\n    '''\n    Coverting them to lower form then removing extra spaces and them removing the repeted words by converting them to sets \n    Then finding Length of Both the questions and adding them to find total words in both\n    '''\n    w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n    w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n    return (len(w1) + len(w2))\n\n# Getting total number of (unique) words in both question1 and question2\ndf['word_Total'] = df.apply(total, axis=1)\n\n\ndef word_share(row):\n    '''\n    Here we are finding total number of shared word and dividing by total number of words [ (A intersection B)/A+B]\n    '''\n    w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n    w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n    return 1.0 * len(w1 & w2)/(len(w1) + len(w2))    # Finding the number of common words between question1 and question2 and dividing by total words between both of them\ndf['Shared_Word'] = df.apply(word_share, axis=1)\n\n\n# Saving our dataframe as csv file\ndf.to_csv(\"With_Feature.csv\", index=False)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Analysing our extracted features"},{"metadata":{},"cell_type":"markdown","source":"## Asking Some Basic Question To Our Extracted Feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"print (\"Minimum length of the questions in question1 : \" , min(df['Q1_Words']))\nprint (\"Minimum length of the questions in question2 : \" , min(df['Q2_Words']))\n\nprint (\"Number of Questions with minimum length [question1] :\", df[df['Q1_Words']== 1].shape[0])\nprint (\"Number of Questions with minimum length [question2] :\", df[df['Q2_Words']== 1].shape[0])\n\nprint (\"Maximum length of the questions in question1 : \" , max(df['Q1_Words']))\nprint (\"Maximum length of the questions in question2 : \" , max(df['Q2_Words']))\n\nprint (\"Number of Questions with minimum length [question1] :\", df[df['Q1_Words']>120].shape[0])\nprint (\"Number of Questions with minimum length [question2] :\", df[df['Q2_Words']> 230].shape[0])\n\nprint (\"Maximum number of Common word : \" , max(df['common_Word']))\nprint (\"Maximum number of Shared Word : \" , max(df['Shared_Word']))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Analysing Shared Word"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 8))\n\nplt.subplot(1,2,1)\nsns.violinplot(x = 'is_duplicate', y = 'Shared_Word', data = df[0:])\n\nplt.subplot(1,2,2)\nsns.distplot(df[df['is_duplicate'] == 1.0]['Shared_Word'][0:] , label = \"1\", color = 'red')\nsns.distplot(df[df['is_duplicate'] == 0.0]['Shared_Word'][0:] , label = \"0\" , color = 'green' )\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15, 8))\n\nplt.subplot(1,2,1)\nsns.violinplot(x = 'is_duplicate', y = 'common_Word', data = df[0:])\n\nplt.subplot(1,2,2)\nsns.distplot(df[df['is_duplicate'] == 1.0]['common_Word'][0:] , label = \"1\", color = 'red')\nsns.distplot(df[df['is_duplicate'] == 0.0]['common_Word'][0:] , label = \"0\" , color = 'green' )\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15, 8))\n\nplt.subplot(1,2,1)\nsns.violinplot(x = 'is_duplicate', y = 'word_Total', data = df[0:])\n\nplt.subplot(1,2,2)\nsns.distplot(df[df['is_duplicate'] == 1.0]['word_Total'][0:] , label = \"1\", color = 'red')\nsns.distplot(df[df['is_duplicate'] == 0.0]['word_Total'][0:] , label = \"0\" , color = 'green' )\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"****\n# [2]Text pre-processing\n"},{"metadata":{},"cell_type":"markdown","source":"### Creating our very own stopword list after removing some stop words like how,whom ,not , etc that may be useful to differentiate between questions"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import SnowballStemmer\nsnow=nltk.stem.SnowballStemmer('english')\n\n# Creating custom stopwords\n\nstopwords= set(['the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n            'won', \"won't\", 'wouldn', \"wouldn't\"])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Simple fuction to perform stemming while removing StopWords"},{"metadata":{"trusted":true},"cell_type":"code","source":"def removeStopWord(word):\n  token=word.split(\" \")   ## coverting string to token (list of word) \\\\ like [\"this\",\"is\",\"token\"]\n  removestop=[snow.stem(x) for x in token if x not in stopwords]   ##removing stopwords and also doing Stemming\n  removed=\" \".join(removestop)  ##joing back the list into sentence\n  return removed","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Replacing common words like 1000 to 1k or 1m and many other and removing special characters "},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef preprocess(x):\n    x = str(x).lower()  # Lowering all text to covert all of them to there base form\n    \n    # Replacing commonly use words or numbers like 1,000 to 1k and 1,000,000 to 1m and currency symbol to there respective names and many other symbols to there name\n    \n    x = x.replace(\",000,000\", \"m\").replace(\",000\", \"k\").replace(\"′\", \"'\").replace(\"’\", \"'\")\\\n                           .replace(\"won't\", \"will not\").replace(\"cannot\", \"can not\").replace(\"can't\", \"can not\")\\\n                           .replace(\"n't\", \" not\").replace(\"what's\", \"what is\").replace(\"it's\", \"it is\")\\\n                           .replace(\"'ve\", \" have\").replace(\"i'm\", \"i am\").replace(\"'re\", \" are\")\\\n                           .replace(\"he's\", \"he is\").replace(\"she's\", \"she is\").replace(\"'s\", \" is\")\\\n                           .replace(\"%\", \" percent \").replace(\"₹\", \" rupee \").replace(\"$\", \" dollar \")\\\n                           .replace(\"€\", \" euro \").replace(\"'ll\", \" will\").replace(\"@\",\"at\")\n    \n    # Renaming 1000 to 1k and 1000000 to 1m (the onces which may not be seperated with commans)\n    x = re.sub(r\"([0-9]+)000000\", r\"\\1m\", x)\n    x = re.sub(r\"([0-9]+)000\", r\"\\1k\", x)\n    \n    \n    #Remove any special character like [= , ' ; \"\" ']\n    \n    pattern = re.compile('\\W')\n    \n    if type(x) == type(''):\n        x = re.sub(pattern, ' ', x)\n    \n    #Removing Stopwords And Doing Stemming\n    x=removeStopWord(x)          \n    \n    return x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##  Pre-processing our Questions data (Removing Stop Words, Doing Stemming and more) "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Doing Pre-processing on both question1 and question2\n\ndf['cleanQ1']=df['question1'].apply(preprocess)\ndf['cleanQ2']=df['question2'].apply(preprocess)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lets look how pre-processing changed our question text"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Questions 1 without pre-processing')\nprint(df['question1'][2])\nprint()\nprint('Questions 1 after pre-processing')\nprint(df['cleanQ1'][2])\nprint()\nprint('Questions 2 without pre-processing')\nprint(df['question2'][2])\nprint()\nprint('Questions 2 after pre-processing')\nprint(df['cleanQ2'][2])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Advance Feature Engineering Using NLP and Fuzzy Features\n\n### Simple Ratio : Measurement of edit distance (Minimum number of edits required to convert one sentence to other )\n### Partial Ratio : How much accuratly a part of sentence match to other sentence (\"Chennai Super Kings\", \"Super Kings\")\n### Token Sort Ratio : Tokenizing the string in question, sorting the tokens alphabetically, and then joining them back into a string\n### Token Set Ratio : Tokenize both strings,split the tokens into two groups of intersection and remainder. We use those sets to build up a comparison string."},{"metadata":{},"cell_type":"markdown","source":"### Last_Word : Checks if last word is same in both Q1 and Q2\n### First_Word : Checks if First word is same in both Q1 and Q2\n### Length_diff : Finds the length diffrence between Q1 and Q2\n### StopWord_Ratio : Number of stopwords in both Questions\n### Token_Ratio :  Number of tokens in both Questions\n### Longest_Substr_ratio : Ratio of the Longest Substring that is found in between Q1 and Q2"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from fuzzywuzzy import fuzz\nfrom fuzzywuzzy import process\n\ndf['Simple_Ratio']=df.apply(lambda x: fuzz.ratio(x['cleanQ1'],x['cleanQ2']) ,axis=1)\n\ndf['Partial_Ratio']=df.apply(lambda x: fuzz.partial_ratio(x['cleanQ1'],x['cleanQ2']) ,axis=1)\n\ndf['Token_Sort_Ratio']=df.apply(lambda x: fuzz.token_sort_ratio(x['cleanQ1'],x['cleanQ2']) ,axis=1)\n\ndf['Token_Set_Ratio']=df.apply(lambda x: fuzz.token_set_ratio(x['cleanQ1'],x['cleanQ2']) ,axis=1)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndf['Last_Word']=df.apply(lambda x: int(x['question1'].split()[-1] == x['question2'].split()[-1]),axis=1)\n\n\ndf['First_Word']=df.apply(lambda x: int(x['question1'].split()[0] == x['question2'].split()[0]),axis=1)\n\n\ndf['Length_diff']=df.apply(lambda x: abs(len(x['question1'].split())-len(x['question2'].split())),axis=1)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def common_StopWord_Ratio(q1,q2):\n    q1_token=q1.split() # Splitting Words to make then tokens\n    q2_token=q2.split()\n    \n    # We are takking out all the stopwords in both the Question and finding there intersection (Common stopwords)\n    q1=set([word for word in q1_token if word in stopwords])\n    q2=set([word for word in q2_token if word in stopwords])\n    common=q1.intersection(q2)\n    ratio=len(common)/len(set(q1_token))+len(set(q2_token))\n    \n    return ratio\n\ndef common_Token_Ratio(q1,q2):\n    q1_token=q1.split()\n    q2_token=q2.split()\n    # We are takking out all the token in both the Question and finding there intersection (Common tokens)\n    q1=set([x for x in q1_token if x not in stopwords])\n    q2=set([x for x in q2_token if x not in stopwords])\n    \n    common=q1.intersection(q2)\n    ratio=len(common)/len(set(q1_token))+len(set(q2_token))\n    \n    return ratio\n\ndef get_longest_substr_ratio(a, b):\n    \n    strs = list(distance.lcsubstrings(a, b))\n    if len(strs) == 0:\n        return 0\n    else:\n        return round(len(strs[0]) / (min(len(a), len(b)) + 1),5)\n\ndef common_Characters(q1,q2):\n    q1=\"\".join(q1.split())\n    q2=\"\".join(q2.split())\n    c=0\n    d=dict(Counter(q1))\n    for i in q2:\n        if i in d and d[i]!=0:\n            c+=1;d[i]-=1\n    return c","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndf['StopWord_Ratio']=df.apply(lambda x: common_StopWord_Ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n\n\ndf['Token_Ratio']=df.apply(lambda x: common_Token_Ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n\n\ndf['Longest_Substr_ratio']=df.apply(lambda x: get_longest_substr_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n\n\n# df['Common_Characters']=df.apply(lambda x: common_Characters(x[\"question1\"], x[\"question2\"]), axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Removing Uncleaned Questions"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Deleting question1 from our data frame\ndel df['question1']\n\n# Deleting question2 from our data frame\ndel df['question2']\n\n# Printing dataset after deletion\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# saving our dataframe as csv\ndf.to_csv('nlp_features_train.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Analysing our extracted features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating two dataframe of both duplicate and non duplicate and then extacting all the text and saving then in text file\ndf_duplicate = df[df['is_duplicate'] == 1]\ndfp_nonduplicate = df[df['is_duplicate'] == 0]\n\n# Converting 2d array of q1 and q2 and flatten the array: like {{1,2},{3,4}} to {1,2,3,4}\np = np.dstack([df_duplicate['cleanQ1'], df_duplicate['cleanQ2']]).flatten()\nn = np.dstack([dfp_nonduplicate[\"cleanQ1\"], dfp_nonduplicate[\"cleanQ2\"]]).flatten()\n\nprint (\"Number of data points in class 1 (duplicate pairs) :\",len(p))\nprint (\"Number of data points in class 0 (non duplicate pairs) :\",len(n))\n\n#Saving the np array into a text file\nnp.savetxt('train_duplicate.txt', p, delimiter=' ', fmt='%s')\nnp.savetxt('train_nonduplicate.txt', n, delimiter=' ', fmt='%s')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reading the text files\n\ntextDupli = open('train_duplicate.txt').read()\ntextNon = open('train_nonduplicate.txt').read()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## WordCloud"},{"metadata":{"trusted":true},"cell_type":"code","source":"stopwords = set(STOPWORDS)\nstopwords.add(\"said\")\nstopwords.add(\"br\")\nstopwords.add(\" \")\nstopwords.remove(\"not\")\nstopwords.remove(\"no\")\nstopwords.remove(\"like\")\n  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wc = WordCloud(width = 3000,height = 2000, background_color=\"black\", max_words=50, stopwords=stopwords)\nwc.generate(textDupli)\nprint (\"Word Cloud for Duplicate Question pairs\")\nplt.imshow(wc, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Word Clouds generated from non duplicate pair question's text\nwc = WordCloud(width = 3000,height = 2000, background_color=\"black\", max_words=50, stopwords=stopwords)\nwc.generate(textNon)\nprint (\"Word Cloud for Duplicate Question pairs\")\nplt.imshow(wc, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n = df.shape[0]\nsns.pairplot(df[['Simple_Ratio', 'Partial_Ratio', 'Token_Sort_Ratio', 'Token_Set_Ratio', 'is_duplicate']][0:n], hue='is_duplicate', vars=['Simple_Ratio', 'Partial_Ratio', 'Token_Sort_Ratio', 'Token_Set_Ratio'], palette=\"husl\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of the Simple_Ratio\nplt.figure(figsize=(10, 8))\n\nplt.subplot(1,2,1)\nsns.violinplot(x = 'is_duplicate', y = 'Simple_Ratio', data = df[0:] , )\n\nplt.subplot(1,2,2)\nsns.distplot(df[df['is_duplicate'] == 1.0]['Simple_Ratio'][0:] , label = \"1\", color = 'red')\nsns.distplot(df[df['is_duplicate'] == 0.0]['Simple_Ratio'][0:] , label = \"0\" , color = 'blue' )\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of the Partial_Ratio\nplt.figure(figsize=(10, 8))\n\nplt.subplot(1,2,1)\nsns.violinplot(x = 'is_duplicate', y = 'Partial_Ratio', data = df[0:] , )\n\nplt.subplot(1,2,2)\nsns.distplot(df[df['is_duplicate'] == 1.0]['Partial_Ratio'][0:] , label = \"1\", color = 'red')\nsns.distplot(df[df['is_duplicate'] == 0.0]['Partial_Ratio'][0:] , label = \"0\" , color = 'blue' )\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of the Token_Sort_Ratio\nplt.figure(figsize=(10, 8))\n\nplt.subplot(1,2,1)\nsns.violinplot(x = 'is_duplicate', y = 'Token_Sort_Ratio', data = df[0:] , )\n\nplt.subplot(1,2,2)\nsns.distplot(df[df['is_duplicate'] == 1.0]['Token_Sort_Ratio'][0:] , label = \"1\", color = 'red')\nsns.distplot(df[df['is_duplicate'] == 0.0]['Token_Sort_Ratio'][0:] , label = \"0\" , color = 'blue' )\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of the Token_Set_Ratio\nplt.figure(figsize=(10, 8))\n\nplt.subplot(1,2,1)\nsns.violinplot(x = 'is_duplicate', y = 'Token_Set_Ratio', data = df[0:] , )\n\nplt.subplot(1,2,2)\nsns.distplot(df[df['is_duplicate'] == 1.0]['Token_Set_Ratio'][0:] , label = \"1\", color = 'red')\nsns.distplot(df[df['is_duplicate'] == 0.0]['Token_Set_Ratio'][0:] , label = \"0\" , color = 'blue' )\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Common_Characters does not seem to have much impact**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of the Length_diff\nplt.figure(figsize=(20, 8))\n\nplt.subplot(1,2,1)\nsns.violinplot(x = 'is_duplicate', y = 'Length_diff', data = df[0:] , )\n\nplt.subplot(1,2,2)\nsns.distplot(df[df['is_duplicate'] == 1.0]['Length_diff'][0:] , label = \"1\", color = 'red')\nsns.distplot(df[df['is_duplicate'] == 0.0]['Length_diff'][0:] , label = \"0\" , color = 'green' )\nplt.figlegend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using TSNE for Dimentionality reduction for 15 Features(Generated after cleaning the data) to 3 dimention\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.manifold import TSNE\ndfp_subsampled = df[0:5000]\nX = MinMaxScaler().fit_transform(dfp_subsampled[['Simple_Ratio', 'Partial_Ratio', 'Token_Sort_Ratio', 'Token_Set_Ratio','Q1_Len', 'Q2_Len', 'Q1_Words','Q2_Words', 'common_Word', 'word_Total', 'Shared_Word','Last_Word', 'First_Word', 'Length_diff','StopWord_Ratio', 'Token_Ratio', 'Longest_Substr_ratio' ]])\ny = dfp_subsampled['is_duplicate'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tsne2d = TSNE(\n    n_components=2,\n    init='random', # pca\n    random_state=101,\n    method='barnes_hut',\n    n_iter=1000,\n    verbose=2,\n    angle=0.5\n).fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(11.7,8.27)})\n\ndf = pd.DataFrame({'x':tsne2d[:,0], 'y':tsne2d[:,1] ,'label':y})\n\npalette = sns.color_palette(\"bright\", 2)\n# draw the plot in appropriate place in the grid\nsns.scatterplot(data=df, x='x', y='y', hue='label',legend='full', palette=palette)\nplt.title(\"perplexity : {} and max_iter : {}\".format(30, 1000))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tsne3d = TSNE(\n    n_components=3,\n    init='random', # pca\n    random_state=101,\n    method='barnes_hut',\n    n_iter=1000,\n    verbose=2,\n    angle=0.5\n).fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trace1 = go.Scatter3d(\n    x=tsne3d[:,0],\n    y=tsne3d[:,1],\n    z=tsne3d[:,2],\n    mode='markers',\n    marker=dict(\n        sizemode='diameter',\n        color = y,\n        colorscale = 'Portland',\n        colorbar = dict(title = 'duplicate'),\n        line=dict(color='rgb(255, 255, 255)'),\n        opacity=0.75\n    )\n)\n\ndata=[trace1]\nlayout=dict(height=800, width=800, title='3d embedding with engineered features')\nfig=dict(data=data, layout=layout)\npy.iplot(fig, filename='3DBubble')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Featurizing text data"},{"metadata":{"trusted":true},"cell_type":"code","source":"import spacy\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv('./nlp_features_train.csv')\ndf=df[['cleanQ1','cleanQ2','is_duplicate']]\ndf['cleanQ1'] = df['cleanQ1'].apply(lambda x: str(x))\ndf['cleanQ2'] = df['cleanQ2'].apply(lambda x: str(x))\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# merging questions of both Q1 and Q2 to a single list in which first 404287 index will be of question 1 and then rest of question 2\nquestions = list(df['cleanQ1']) + list(df['cleanQ2'])  # len(questions): 808574  [404287 of q1 and 404287 of q2]\n\ntfidf = TfidfVectorizer() #  Convert a collection of raw documents to a matrix of TF-IDF features\n\ntfidf.fit_transform(questions)  # Converting out text to a matrix of TF-IDF features\n\n# mapping our feature_names with threre resptive tf-idf score  ( dict key:word and value:tf-idf score )\nword2tfidf = dict(zip(tfidf.get_feature_names(), tfidf.idf_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting Key Value Pair\nwrd=list(word2tfidf.keys())[::-1][:10]   # finding keys of dict and then reversing them to get last 10 keys\nfor i in wrd:\n    print(i,word2tfidf[i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### After we find TF-IDF scores, we convert each question to a weighted average of word2vec vectors by these scores.\n### here we use a pre-trained GLOVE model which comes free with \"Spacy\". https://spacy.io/usage/vectors-similarity\n### It is trained on Wikipedia and therefore, it is stronger in terms of word semantics."},{"metadata":{"trusted":true},"cell_type":"code","source":"# en_vectors_web_lg, which includes over 1 million unique vectors.\nnlp = spacy.load('en_core_web_sm')\n\nvecs1 = []\n\nfor qu1 in tqdm(list(df['cleanQ1'])):\n    \n    doc1 = nlp(qu1) #creating object of   GLOVE model  so that we can get vetor representation of our words\n    \n    # Creating a matrix of N x M where N is is number of word is given line and M i.e. 96 which is the vector representaion of 1st word\n    mean_vec1 = np.zeros([len(doc1), len(doc1[0].vector)])\n    \n    # Looping to all words in the given sentence \n    \n    for word1 in doc1:\n        \n       # word2vec ( Creating Vector Representation of every word ) which is 96\n    \n        vec1 = word1.vector\n        \n        # Using try and catch to prefent key error [ For the words that are not there in our word2tfidf dict like empty space ]\n        \n        try:\n            idf = word2tfidf[str(word1)]\n        except:\n            idf = 0\n        \n        # adding up all the words generated in the matrix (word2vec matrix * the word2tfidf Corresponding to that word)\n        mean_vec1 += vec1 * idf\n    mean_vec1 = mean_vec1.mean(axis=0)\n    vecs1.append(mean_vec1)  # Storing the vector representation of every sentence into an array\ndf['q1_feats_m'] = list(vecs1)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# en_vectors_web_lg, which includes over 1 million unique vectors.\nnlp = spacy.load('en_core_web_sm')\n\nvecs2 = []\n\nfor qu2 in tqdm(list(df['cleanQ2'])):\n    \n    doc2 = nlp(qu2) #creating object of   GLOVE model  so that we can get vetor representation of our words\n    \n    # Creating a matrix of N x M where N is is number of word is given line and M i.e. 96 which is the vector representaion of 1st word\n    mean_vec2 = np.zeros([len(doc2), len(doc2[0].vector)])\n    \n    # Looping to all words in the given sentence \n    \n    for word2 in doc2:\n        \n       # word2vec ( Creating Vector Representation of every word ) which is 96\n    \n        vec2 = word2.vector\n        \n        # Using try and catch to prefent key error [ For the words that are not there in our word2tfidf dict like empty space ]\n        \n        try:\n            idf = word2tfidf[str(word2)]\n        except:\n            idf = 0\n        \n        # adding up all the words generated in the matrix (word2vec matrix * the word2tfidf Corresponding to that word)\n        mean_vec2 += vec2 * idf\n    mean_vec2 = mean_vec2.mean(axis=0)\n    vecs2.append(mean_vec2)  # Storing the vector representation of every sentence into an array\ndf['q2_feats_m'] = list(vecs2)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1=pd.read_csv('./nlp_features_train.csv')\ndf1.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1=df1.drop(['Unnamed: 0','qid1', 'qid2','cleanQ1', 'cleanQ2'],axis=1)\ndf1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df3 = df.drop(['cleanQ1', 'cleanQ2', 'is_duplicate'],axis=1)\ndf3_q1 = pd.DataFrame(df3.q1_feats_m.values.tolist(), index= df3.index)\ndf3_q2 = pd.DataFrame(df3.q2_feats_m.values.tolist(), index= df3.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if not os.path.isfile('final_features.csv'):\n    df3_q1['id']=df1['id']\n    df3_q2['id']=df1['id']\n    #df1  = df1.merge(df2, on='id',how='left')\n    df2  = df3_q1.merge(df3_q2, on='id',how='left')\n    result  = df1.merge(df2, on='id',how='left')\n    result.to_csv('final_features.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Machine Learning Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics.classification import accuracy_score, log_loss\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom collections import Counter\nfrom scipy.sparse import hstack\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import StratifiedKFold\nfrom collections import Counter, defaultdict\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nimport math\nfrom sklearn.metrics import normalized_mutual_info_score\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.preprocessing import StandardScaler\n\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import SGDClassifier\nfrom mlxtend.classifier import StackingClassifier\n\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import precision_recall_curve, auc, roc_curve","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data=pd.read_csv('final_features.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_true=data['is_duplicate']\ndata.drop(['Unnamed: 0', 'id','is_duplicate'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\n# transform data\nscaled = scaler.fit_transform(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test, y_train, y_test = train_test_split(scaled, y_true, stratify=y_true, test_size=0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of data points in train data :\",X_train.shape)\nprint(\"Number of data points in test data :\",X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"-\"*10, \"Distribution of output variable in train data\", \"-\"*10)\ntrain_distr = Counter(y_train)\ntrain_len = len(y_train)\nprint(\"Class 0: \",int(train_distr[0])/train_len,\"Class 1: \", int(train_distr[1])/train_len)\nprint(\"-\"*10, \"Distribution of output variable in train data\", \"-\"*10)\ntest_distr = Counter(y_test)\ntest_len = len(y_test)\nprint(\"Class 0: \",int(test_distr[1])/test_len, \"Class 1: \",int(test_distr[1])/test_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This function plots the confusion matrices given y_i, y_i_hat.\ndef plot_confusion_matrix(test_y, predict_y):\n    C = confusion_matrix(test_y, predict_y)\n    # C = 9,9 matrix, each cell (i,j) represents number of points of class i are predicted class j\n    \n    A =(((C.T)/(C.sum(axis=1))).T)\n    #divid each element of the confusion matrix with the sum of elements in that column\n    \n    # C = [[1, 2],\n    #     [3, 4]]\n    # C.T = [[1, 3],\n    #        [2, 4]]\n    # C.sum(axis = 1)  axis=0 corresonds to columns and axis=1 corresponds to rows in two diamensional array\n    # C.sum(axix =1) = [[3, 7]]\n    # ((C.T)/(C.sum(axis=1))) = [[1/3, 3/7]\n    #                           [2/3, 4/7]]\n\n    # ((C.T)/(C.sum(axis=1))).T = [[1/3, 2/3]\n    #                           [3/7, 4/7]]\n    # sum of row elements = 1\n    \n    B =(C/C.sum(axis=0))\n    #divid each element of the confusion matrix with the sum of elements in that row\n    # C = [[1, 2],\n    #     [3, 4]]\n    # C.sum(axis = 0)  axis=0 corresonds to columns and axis=1 corresponds to rows in two diamensional array\n    # C.sum(axix =0) = [[4, 6]]\n    # (C/C.sum(axis=0)) = [[1/4, 2/6],\n    #                      [3/4, 4/6]] \n    plt.figure(figsize=(20,4))\n    \n    labels = [1,2]\n    # representing A in heatmap format\n    cmap=sns.light_palette(\"blue\")\n    plt.subplot(1, 3, 1)\n    sns.heatmap(C, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.title(\"Confusion matrix\")\n    \n    plt.subplot(1, 3, 2)\n    sns.heatmap(B, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.title(\"Precision matrix\")\n    \n    plt.subplot(1, 3, 3)\n    # representing B in heatmap format\n    sns.heatmap(A, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.title(\"Recall matrix\")\n    \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Building a random model (Finding worst-case log-loss)"},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_y = np.zeros((test_len,2))\nfor i in range(test_len):\n    rand_probs = np.random.rand(1,2)\n    predicted_y[i] = ((rand_probs/sum(sum(rand_probs)))[0])\nprint(\"Log loss on Test Data using Random Model\",log_loss(y_test, predicted_y, eps=1e-15))\n\npredicted_y =np.argmax(predicted_y, axis=1)\nplot_confusion_matrix(y_test, predicted_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##  Logistic Regression with hyperparameter tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"alpha = [10 ** x for x in range(-5, 2)] # hyperparam for SGD classifier.\n\n# read more about SGDClassifier() at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n# ------------------------------\n# default parameters\n# SGDClassifier(loss=’hinge’, penalty=’l2’, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None, \n# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=’optimal’, eta0=0.0, power_t=0.5, \n# class_weight=None, warm_start=False, average=False, n_iter=None)\n\n# some of methods\n# fit(X, y[, coef_init, intercept_init, …])\tFit linear model with Stochastic Gradient Descent.\n# predict(X)\tPredict class labels for samples in X.\n\n\n\nlog_error_array=[]\nfor i in alpha:\n    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n    clf.fit(X_train, y_train)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(X_train, y_train)\n    predict_y = sig_clf.predict_proba(X_test)\n    log_error_array.append(log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n\nfig, ax = plt.subplots()\nax.plot(alpha, log_error_array,c='g')\nfor i, txt in enumerate(np.round(log_error_array,3)):\n    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(log_error_array)\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\nclf.fit(X_train, y_train)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(X_train, y_train)\n\npredict_y = sig_clf.predict_proba(X_train)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(X_test)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\npredicted_y =np.argmax(predict_y,axis=1)\nprint(\"Total number of data points :\", len(predicted_y))\nplot_confusion_matrix(y_test, predicted_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Linear SVM with hyperparameter tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"alpha = [10 ** x for x in range(-5, 2)] # hyperparam for SGD classifier.\n\n# read more about SGDClassifier() at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n# ------------------------------\n# default parameters\n# SGDClassifier(loss=’hinge’, penalty=’l2’, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None, \n# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=’optimal’, eta0=0.0, power_t=0.5, \n# class_weight=None, warm_start=False, average=False, n_iter=None)\n\n# some of methods\n# fit(X, y[, coef_init, intercept_init, …])\tFit linear model with Stochastic Gradient Descent.\n# predict(X)\tPredict class labels for samples in X.\n\n#-------------------------------\n# video link: \n#------------------------------\n\n\nlog_error_array=[]\nfor i in alpha:\n    clf = SGDClassifier(alpha=i, penalty='l1', loss='hinge', random_state=42)\n    clf.fit(X_train, y_train)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(X_train, y_train)\n    predict_y = sig_clf.predict_proba(X_test)\n    log_error_array.append(log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n\nfig, ax = plt.subplots()\nax.plot(alpha, log_error_array,c='g')\nfor i, txt in enumerate(np.round(log_error_array,3)):\n    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(log_error_array)\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l1', loss='hinge', random_state=42)\nclf.fit(X_train, y_train)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(X_train, y_train)\n\npredict_y = sig_clf.predict_proba(X_train)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(X_test)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\npredicted_y =np.argmax(predict_y,axis=1)\nprint(\"Total number of data points :\", len(predicted_y))\nplot_confusion_matrix(y_test, predicted_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"parameters = {'nthread':[4], #when use hyperthread, xgboost may become slower\n              'objective':['binary:logistic'],\n              'learning_rate': [0.02], #so called `eta` value\n              'max_depth': [6],\n              'min_child_weight': [11],\n              'silent': [1],\n              'subsample': [0.8],\n              'colsample_bytree': [0.7],\n              'n_estimators': [5], #number of trees, change it to 1000 for better results\n              'missing':[-999],\n              'seed': [1337]}\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\n\nd_train = xgb.DMatrix(X_train, label=y_train)\nd_test = xgb.DMatrix(X_test, label=y_test)\n\nwatchlist = [(d_train, 'train'), (d_test, 'valid')]\n\nbst = xgb.train(params, d_train, 400, watchlist, early_stopping_rounds=20, verbose_eval=10)\n\nxgdmat = xgb.DMatrix(X_train,y_train)\npredict_y = bst.predict(d_test)\nprint(\"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_y =np.array(predict_y>0.5,dtype=int)\nprint(\"Total number of data points :\", len(predicted_y))\nplot_confusion_matrix(y_test, predicted_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}