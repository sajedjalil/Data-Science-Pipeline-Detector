{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"d4282d8b-d5f0-7108-b564-5f92823f9085"},"source":"Quora Questions Pairs"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f95f9bab-2061-23c2-76eb-3fc8be55fa7c"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport nltk\nfrom nltk.corpus import stopwords # Import the stop word list\nfrom collections import Counter\nimport random\nimport string\nimport re\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2f934d44-9c48-fd70-f7ca-f74ccfbaf0cc"},"outputs":[],"source":"#Load and Explore data \n\ntrain_set = pd.read_csv(\"../input/train.csv\")\ntest_set = pd.read_csv(\"../input/test.csv\")\nprint(train_set.shape)\nprint(test_set.shape)\n\nprint(train_set.head())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"835637d2-be8f-8567-2354-2621089b70ca"},"outputs":[],"source":"#Preprocess Text\n\ndef preprocess(question):\n\n    #Remove non-letters        \n    letters_only = re.sub(\"[^a-zA-Z]\", \" \", question) \n    \n    #Convert to lower case\n    text = letters_only.lower()                          \n\n    # Replace punctuation with tokens so we can use them in our model\n    for c in string.punctuation:\n         text = text.replace(c,\"\")\n    \n    #Convert text to words\n    text_to_array = text.split()\n\n     #Convert the stop words to a set\n    stops = set(stopwords.words(\"english\"))                  \n    \n    #Remove stop words\n    meaningful_words = [w for w in text_to_array if not w in stops]    \n    \n    # Remove all words with  5 or fewer occurences\n    #word_counts = Counter(words)\n    #trimmed_words = [word for word in words if word_counts[word] > 5]\n    \n    return meaningful_words"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c6777d60-b125-dbc9-cad0-afdfc9aacc17"},"outputs":[],"source":"#Convert questions to words\n\nwords = []\n\nfor x in range(0,40):#train_set.shape[0]):\n    words += preprocess(train_set.iloc[x,3])\n    words += preprocess(train_set.iloc[x,4])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"53170bf4-c7e5-b192-5994-011e2904f4ff"},"outputs":[],"source":"#Create Lookup Tables\n\ndef create_lookup_tables(words):\n    \"\"\"\n    Create lookup tables for vocabulary\n    :param words: Input list of words\n    :return: A tuple of dicts.  The first dict....\n    \"\"\"\n    word_counts = Counter(words)\n    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n    int_to_vocab = {ii: word for ii, word in enumerate(sorted_vocab)}\n    vocab_to_int = {word: ii for ii, word in int_to_vocab.items()}\n\n    return vocab_to_int, int_to_vocab\n\nvocab_to_int, int_to_vocab = create_lookup_tables(words)\nint_words = [vocab_to_int[word] for word in words]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"24966bf7-d5fa-05dc-1ac1-79a0e2d82241"},"outputs":[],"source":"#Subsampling\n\nthreshold = 1e-5\nword_counts = Counter(int_words)\ntotal_count = len(int_words)\nfreqs = {word: count/total_count for word, count in word_counts.items()}\np_drop = {word: 1 - np.sqrt(threshold/freqs[word]) for word in word_counts}\ntrain_words = [word for word in int_words if random.random() < (1 - p_drop[word])]\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3349c654-d1ad-eaf4-97cb-30a79969664d"},"outputs":[],"source":"# Receives a list of words, an index, and a window size, then returns a list of words in the window around the index\n\ndef get_target(words, idx, window_size=5):\n    ''' Get a list of words in a window around an index. '''\n    \n    R = np.random.randint(1, window_size+1)\n    start = idx - R if (idx - R) > 0 else 0\n    stop = idx + R\n    target_words = set(words[start:idx] + words[idx+1:stop+1])\n    \n    return list(target_words)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c2475f5e-af45-64b0-9beb-5d6162cbcb81"},"outputs":[],"source":"#Get batches for the network\n\ndef get_batches(words, batch_size, window_size=5):\n    ''' Create a generator of word batches as a tuple (inputs, targets) '''\n    \n    n_batches = len(words)//batch_size\n    \n    # only full batches\n    words = words[:n_batches*batch_size]\n    \n    for idx in range(0, len(words), batch_size):\n        x, y = [], []\n        batch = words[idx:idx+batch_size]\n        for ii in range(len(batch)):\n            batch_x = batch[ii]\n            batch_y = get_target(batch, ii, window_size)\n            y.extend(batch_y)\n            x.extend([batch_x]*len(batch_y))\n        yield x, y"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}