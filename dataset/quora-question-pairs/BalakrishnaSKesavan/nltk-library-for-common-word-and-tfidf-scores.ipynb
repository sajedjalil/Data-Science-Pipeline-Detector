{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"pygments_lexer":"ipython3","name":"python","nbconvert_exporter":"python","codemirror_mode":{"version":3,"name":"ipython"},"file_extension":".py","mimetype":"text/x-python","version":"3.6.1"}},"cells":[{"metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"9d0795d187d4c714625614e64f966bdc9fc421ff","_cell_guid":"8af7eb33-d001-4277-9fa7-89bcc70534f0"},"execution_count":null,"source":"This kernel's key feature is the use of Python's NLTK library to produce scores for,\n\n 1.  Common words in question pairs, after excluding stop words \n 2. TFIDF\n\nCode and ideas have been drawn from the kernel submitted by Kaggle user \"Anokas\", who did not use NLTK for word tokenization or TFIDF.","cell_type":"markdown","outputs":[]},{"metadata":{"_execution_state":"idle","trusted":false,"_uuid":"d5d191d0a474f3c3fc695e5b7678f7d2c88143e0","_cell_guid":"c866a108-9e75-44a2-b023-ecf214b88f2d"},"execution_count":null,"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","cell_type":"code","outputs":[]},{"metadata":{"trusted":false,"_execution_state":"idle","collapsed":false,"_uuid":"1d08e40e371c284896404c363c27f3062f94d6e4","_cell_guid":"af624e38-566b-4f60-8791-9f19294509f6"},"execution_count":null,"source":"#These are the additional libraries (the list above are included in the standard Kaggle image)\n\nimport nltk\n#nltk.download() #not needed to run on Kaggle\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n#import pickle #not needed to run on Kaggle, but used to store interim objects while working on a local machine\nimport xgboost as xgb\nfrom sklearn.cross_validation import train_test_split\n","cell_type":"code","outputs":[]},{"metadata":{"trusted":false,"_execution_state":"idle","collapsed":false,"_uuid":"5db79ee67183a079c748444962e15726ac17f54f","_cell_guid":"3f722dd4-3b9a-45e0-8423-844ad240e295"},"execution_count":null,"source":"#reading in the data\ndf_train = pd.read_csv('../input/train.csv')\ndf_test = pd.read_csv('../input/test.csv')","cell_type":"code","outputs":[]},{"metadata":{"trusted":false,"_execution_state":"idle","collapsed":false,"_uuid":"073e0f3c4476a246e6e5926c5e45106bdf23d10c","_cell_guid":"5f86ec6c-66c4-43cc-8924-2441898f5cbd"},"execution_count":null,"source":"#function to identify common words in question pairs\nfrom nltk.corpus import stopwords\n\nstops = set(stopwords.words(\"english\"))\n\ndef common_words(row):\n    q1words = {}\n    q2words = {}\n    \n    for word in word_tokenize(str(row['question1'])):\n        if word not in stops:\n            q1words[word] = 1\n    for word in word_tokenize(str(row['question2'])):\n        if word not in stops:\n            q2words[word] = 1\n    if len(q1words) == 0 or len(q2words) == 0:\n        # The computer-generated chaff includes a few questions that are nothing but stopwords\n        return 'N/A'\n    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n    return shared_words_in_q1","cell_type":"code","outputs":[]},{"metadata":{"trusted":false,"_execution_state":"idle","collapsed":false,"_uuid":"b6310a980b0b82011ffc237ef2aafa7a7933b75a","_cell_guid":"c0da3706-90a8-4fc1-9ea1-e5ab7598a39e"},"execution_count":null,"source":"# function to score common words in question pairs\nfrom nltk.corpus import stopwords\n\nstops = set(stopwords.words(\"english\"))\n\ndef nltk_word_match_share(row):\n    q1words = {}\n    q2words = {}\n    \n    for word in word_tokenize(str(row['question1'])):\n        if word not in stops:\n            q1words[word] = 1\n    \n    for word in word_tokenize(str(row['question2'])):\n        if word not in stops:\n            q2words[word] = 1\n    if len(q1words) == 0 or len(q2words) == 0:\n        # The computer-generated chaff includes a few questions that are nothing but stopwords\n        return 0\n    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n    R = (len(shared_words_in_q1) + len(shared_words_in_q2))/(len(q1words) + len(q2words))\n    return R","cell_type":"code","outputs":[]},{"metadata":{"trusted":false,"_execution_state":"idle","collapsed":false,"_uuid":"5716505f64eb0abba05345de126423e9e1ab1f2b","_cell_guid":"a65d7f81-8877-4c84-87f4-7bec2d1af2c2"},"execution_count":null,"source":"#identifying common words in question pairs\ntrain_common_words_tokenizer = df_train.apply(common_words, axis=1, raw=True)\ntest_common_words_tokenizer = df_test.apply(common_words, axis=1, raw=True) ","cell_type":"code","outputs":[]},{"metadata":{"trusted":false,"_execution_state":"busy","collapsed":false,"_uuid":"5246adb3624249fe095569fe6ed4dde6be65a2a2","_cell_guid":"c5a8774c-ded4-4a14-b74e-2a916d9010f4"},"execution_count":null,"source":"#scoring for common words in question pairs\ntrain_word_match_tokenizer = df_train.apply(nltk_word_match_share, axis=1, raw=True)\ntest_word_match_tokenizer = df_test.apply(nltk_word_match_share, axis=1, raw=True)","cell_type":"code","outputs":[]},{"metadata":{"trusted":false,"_execution_state":"busy","collapsed":false,"_uuid":"0a2c3d32d2079c6fb1769a8cf9fed24b12324a73","_cell_guid":"7698f055-7f80-4afc-8116-f566d408cdae"},"execution_count":null,"source":"#generating TFIDF score for the train corpus \ntf = TfidfVectorizer(analyzer='word', min_df = 0, stop_words = 'english')\ncorpus = train_qs\ntfidf_matrix =  tf.fit_transform(corpus)\nfeature_names = tf.get_feature_names() \nphrase_scores = tf.idf_\n\nnames_scores = pd.DataFrame({'feature_names':feature_names})\nnames_scores['phrase_scores'] = pd.DataFrame(phrase_scores)\n\nwriter = pd.ExcelWriter(\"/home/Bala/Documents/NLP/names_scores.xlsx\")\nnames_scores.to_excel(writer,'Sheet1')\nwriter.save()\n\nscores1=[]\nfor i in range(0,404291):\n    row_scores1 = 0\n    for word in train_common_words_tokenizer[i]:\n            if str.lower(word) in feature_names:\n                row_scores1 = row_scores1 + (phrase_scores[feature_names.index(str.lower(word))])\n                print(i)\n                #print('row_scores', row_scores)\n    scores1.append(row_scores1)\nscores_tfidf_train_nltk = scores1\n","cell_type":"code","outputs":[]},{"metadata":{"trusted":false,"_execution_state":"busy","collapsed":false,"_uuid":"525d12ab42307dc6c24f2984579df77d58081403","_cell_guid":"62031672-3070-4815-9096-5b0cfeca2797"},"execution_count":null,"source":"#generating TFIDF scores for the test corpus\n#IMPORTANT: This code needs optimization/ vectorization\ntf = TfidfVectorizer(analyzer='word', min_df = 0, stop_words = 'english')\ncorpus_test = test_qs\ntfidf_matrix_test =  tf.fit_transform(corpus_test)\nfeature_names_test = tf.get_feature_names() \nphrase_scores_test = tf.idf_\n\nnames_scores_test = pd.DataFrame({'feature_names':feature_names_test})\nnames_scores_test['phrase_scores'] = pd.DataFrame(phrase_scores_test)\n\nscores2=[]\nfor i in range(0,len(df_test)):\n    row_scores2 = 0\n    for word in test_common_words_tokenizer[i]:\n            if str.lower(word) in feature_names_test:\n                row_scores2 = row_scores2 + (phrase_scores_test[feature_names_test.index(str.lower(word))])\n                print(i)\n                #print('row_scores', row_scores)\n    scores2.append(row_scores2)\n#scores1_500000=scores1\nscores_tfidf_test_nltk = pd.DataFrame(scores2)\nscores_tfidf_test_nltk = scores2","cell_type":"code","outputs":[]},{"metadata":{"trusted":false,"_execution_state":"busy","collapsed":false,"_uuid":"5b884990fe018b983171e677771d6b3f34cba421","_cell_guid":"063c92aa-4232-4219-a958-153b9a7bffd9"},"execution_count":null,"source":"#creating dataframes for train and test\nx_train = pd.DataFrame()\nx_test = pd.DataFrame()\n\nx_train['nltk word match'] = train_word_match_tokenizer\nx_train['nltk tfidf']= scores_tfidf_train_nltk\n\ny_train = pd.DataFrame(df_train['is_duplicate'].values)\n\nx_test['nltk word match'] = test_word_match_tokenizer\nx_test['nltk tfidf']= scores_tfidf_test_nltk","cell_type":"code","outputs":[]},{"metadata":{"trusted":false,"_execution_state":"busy","collapsed":false,"_uuid":"109de77306a211bedcb8c285d5375a3e9ff88bf0","_cell_guid":"5bf1b1be-02b9-4b2e-8848-750b7bf38f16"},"execution_count":null,"source":"# final data preparation - Split some of the data off for validation\n\nx_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.1, random_state=3)","cell_type":"code","outputs":[]},{"metadata":{"trusted":false,"_execution_state":"busy","collapsed":false,"_uuid":"f66d637793407aca413fd29a5c74ad273108c8cb","_cell_guid":"e2d88603-916c-4131-8b31-6352b23a7200"},"execution_count":null,"source":"#model training\nnltk_train = x_train.loc[:,['nltk word match', 'nltk tfidf']]\nnltk_valid = x_valid.loc[:,['nltk word match', 'nltk tfidf']]\n\nd_train = xgb.DMatrix(nltk_train, label=y_train)\nd_valid = xgb.DMatrix(nltk_valid, label=y_valid)\n\nwatchlist = [(d_train, 'train'), (d_valid, 'valid')]\n\nbst = xgb.train(params, d_train, 400, watchlist, early_stopping_rounds=50, verbose_eval=10)\n","cell_type":"code","outputs":[]}],"nbformat":4,"nbformat_minor":0}