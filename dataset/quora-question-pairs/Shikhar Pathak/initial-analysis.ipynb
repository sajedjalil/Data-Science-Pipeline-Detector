{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"339acabe-1d23-8782-87d1-07eda2f5049d"},"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4846c062-7350-566d-0693-181f5c6925d1"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"markdown","metadata":{"_cell_guid":"bd6bb321-860d-d0fe-059d-622034a0f835"},"source":"### Import plotting libraries ###"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"68fe4408-e57e-15c1-4a88-27d9ae7ee7e1"},"outputs":[],"source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\nsns.set(color_codes=True)\nsns.set_style(\"white\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"7bf75bf2-242a-7d19-53bc-1c1fad387ae2"},"source":"### Import modeling libraries ###"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9271cd55-625c-6e72-9152-bf0a64b8b4d3"},"outputs":[],"source":"import sklearn.ensemble\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, log_loss"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a2e8c695-e12e-4697-c89c-af2354285ea5"},"outputs":[],"source":"import re\nimport nltk\nfrom nltk.corpus import stopwords\n\nstops = set(stopwords.words(\"english\"))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e9425502-b733-74a2-da78-769cf1e31917"},"outputs":[],"source":"train_set = pd.read_csv('../input/train.csv')\n\ntest_set = pd.read_csv('../input/test.csv')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ef49c77b-34b5-bd65-fb48-79f539b693ff"},"outputs":[],"source":"metrics = []"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"20af0d11-5ec0-f98f-f132-b7a25bdf4fa6"},"outputs":[],"source":"print('There are {} records in train'.format(train_set.shape[0]))\nprint('There are {} records in train'.format(test_set.shape[0]))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8adc5737-61ba-2855-ba75-2fdd6378e667"},"outputs":[],"source":"target = 'is_duplicate'\nID = 'id'"},{"cell_type":"markdown","metadata":{"_cell_guid":"4f5d9248-83b9-2ec5-ce33-c66c9ecb7362"},"source":"### Find nulls in questions ###"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"961af815-d390-2657-d6e2-c9ed12695cf4"},"outputs":[],"source":"def find_nulls(df, column):\n    res = df.ix[pd.isnull(df[column])]\n    percentages = []\n    percentages.append(len(res[column]))\n    percentages.append(df.shape[0] - res.shape[0])\n    percentages = pd.DataFrame(percentages, columns=[column], index=['Nulls', 'Non nulls'])\n    return percentages"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"60512980-382a-760e-4009-4c907070c30d"},"outputs":[],"source":"def plot_bar_chart(data, column):\n    N = data.shape[0]\n    ind = np.arange(N)\n    width = 0.35\n    fig, ax = plt.subplots(figsize=(8,3))\n    rects = ax.bar(ind, data[column])\n    ax.set_xticks(ind + width / 2)\n    ax.set_xticklabels(data.index)\n    for rect in rects:\n        height = rect.get_height()\n        ax.text(rect.get_x() + rect.get_width()/2., 1.05*height,\n                '%d' % int(height),\n                ha='center', va='bottom')\n    plt.show()\n    plt.close()"},{"cell_type":"markdown","metadata":{"_cell_guid":"20fa2ce5-ffb3-d531-3779-a109bfd5ed13"},"source":"### Find nulls in question1 ###"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8a583f5b-5318-0595-0fee-841fe864f631"},"outputs":[],"source":"res = find_nulls(train_set, 'question1')\nres = pd.concat([res, find_nulls(train_set, 'question2')], axis=1)\nres = res.rename(columns={\n    'question1': 'train_q1',\n    'question2': 'train_q2'\n})\nres = pd.concat([res, find_nulls(test_set, 'question1')], axis=1)\nres = pd.concat([res, find_nulls(test_set, 'question2')], axis=1)\nres = res.rename(columns={\n    'question1': 'test_q1',\n    'question2': 'test_q2'\n})\nres"},{"cell_type":"markdown","metadata":{"_cell_guid":"9ddcb242-6c74-19ee-2b3d-bfaa96cb9552"},"source":"### Fill the NA's ###"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bfccc082-ab20-afb7-93da-23cdf861d9a1"},"outputs":[],"source":"train_set['question1'] = train_set['question1'].fillna('')\ntrain_set['question2'] = train_set['question2'].fillna('')\n\ntest_set['question1'] = test_set['question1'].fillna('')\ntest_set['question2'] = test_set['question2'].fillna('')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"acb87ec2-70f3-7c55-1051-3fb52455917e"},"outputs":[],"source":"word_counts = {}\n\ndef find_word_counts(word_counts, tokenlist):\n    for token in tokenlist:\n        if token in word_counts:\n            word_counts[token] = word_counts[token] + 1\n        else:\n            word_counts[token] = 1\n    return word_counts"},{"cell_type":"markdown","metadata":{"_cell_guid":"647bf46d-d36b-11f2-c73f-343fec0bbede"},"source":"### Use regex to find quoted words that can be converted to complete words  ###"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"711cc203-8720-a2b1-f9e3-038bd2b82a85"},"outputs":[],"source":"train_set['q1_quotes'] = train_set['question1'].str.extract('(\\w+\\'\\w+)\\s.*')\ntrain_set['q2_quotes'] = train_set['question2'].str.extract('(\\w+\\'\\w+)\\s.*')"},{"cell_type":"markdown","metadata":{"_cell_guid":"337813be-a188-4789-5816-1914cf7d4892"},"source":"### Pie chart to represent distributions of  sentence with quotes against unquotes ###"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2d3dfb19-af4c-8082-201e-aa4462b7964f"},"outputs":[],"source":"quote_counts = []\nquote_counts.append(train_set.ix[pd.notnull(train_set['q1_quotes'])].shape[0])\nquote_counts.append(train_set.ix[pd.isnull(train_set['q1_quotes'])].shape[0])\nquote_counts = pd.DataFrame(quote_counts, columns=['counts'], \n                            index=['Quotes', 'Unquotes'])\n\ncolors = ['gold', 'yellowgreen']\nexplode = (0.1, 0)\n \nplt.pie(quote_counts['counts'], labels=quote_counts.index, colors=colors, \n        explode=(1, 0), autopct='%.1f%%',)\nplt.axis('equal')\nplt.show()\nplt.close()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f2f128cc-f893-cad8-63fc-9edb43d22b09"},"outputs":[],"source":"quote_counts = []\nquote_counts.append(train_set.ix[pd.notnull(train_set['q2_quotes'])].shape[0])\nquote_counts.append(train_set.ix[pd.isnull(train_set['q2_quotes'])].shape[0])\nquote_counts = pd.DataFrame(quote_counts, columns=['counts'], \n                            index=['Quotes', 'Unquotes'])\n\ncolors = ['lightblue', 'coral']\nexplode = (0.1, 0)\n \nplt.pie(quote_counts['counts'], labels=quote_counts.index, colors=colors, \n        explode=(1, 0), autopct='%.1f%%',)\nplt.axis('equal')\nplt.show()\nplt.close()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"85debfc6-7c16-124c-15b8-dd8de28ee75c"},"outputs":[],"source":"re.match('^(\\w+\\'\\w+)\\s.*', \"What's causing someone to be jealous?\").group(1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5f923ac8-eeed-1e8d-38b3-966a68f240b2"},"outputs":[],"source":"duplicates = train_set.ix[train_set['is_duplicate'] == 1]\nnon_duplicates = train_set.ix[train_set['is_duplicate'] == 0]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a9e507be-bc20-3d53-53e8-96916b26be04"},"outputs":[],"source":"def find_matching_words(x):\n    wq1 = str(x['question1']).lower().split(' ')\n    wq2 = str(x['question2']).lower().split(' ')\n    matches = set(wq1).intersection(set(wq2))\n    return len(matches)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1de480b0-6ca3-5a0b-7777-e6da0ed74903"},"outputs":[],"source":"re.match('.*(\\?\\s).*', \"Is Kristen Stewart a bad actress?  Why or why not?\").group(1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"da534062-4a02-ea81-ed21-42a49b9109f8"},"outputs":[],"source":"def find_bi_grams(question):\n    question = question.replace\n    tokens = question.lower().split(' ')\n    pos_tags = nltk.pos_tag(tokens)\n    bigrams = []\n    \n    print(len(pos_tags))\n    for i in range(len(pos_tags)):\n        if i == len(pos_tags) - 1:\n            continue\n        bigrams.append((pos_tags[i][0], pos_tags[i+1][0]))\n        #if pos_tags[i][1] == 'JJ' and pos_tags[i+1][1] == 'NN':\n        #    bigrams.append((pos_tags[i][0], pos_tags[i+1][0]))\n    print('Done')\n    return bigrams"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"85509f42-afb3-d5cd-4139-e2a5f8cdac9d"},"outputs":[],"source":"train_set['has_questionmark'] = train_set['question1'].str.extract('.*(\\?\\s).*')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4bbb5b21-e629-9640-2534-5bac560b9f0b"},"outputs":[],"source":"train_set.ix[pd.notnull(train_set['has_questionmark'])]['question1'].iloc[1]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c91714dc-b278-d6c0-7889-044c4a09b610"},"outputs":[],"source":"train_set.shape"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1138a6aa-fb84-da8e-2905-0d7c2882a3c0"},"outputs":[],"source":"403717 / 404290"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ce8fc82b-b26d-4ba0-be59-d9a6d98bd121"},"outputs":[],"source":"train_set['q1_bigrams'] = train_set['question1'].map(find_bi_grams)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1010c0b0-29ce-a913-03a8-6085d26e4ce2"},"outputs":[],"source":"copy_frame = train_set.copy(deep=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"72c58922-4c67-dcf7-7ea6-14ada44e53a7"},"outputs":[],"source":"def replace_q1_puncts(sentence):\n    sentence = sentence.replace('What\\'s', 'What is')\n    sentence = sentence.replace('What\\'re', 'What are')\n    sentence = sentence.replace('Who\\'s', 'Who is')\n    sentence = sentence.replace('who\\'re', 'who are')\n    sentence = sentence.replace('How\\'s', 'How is')\n    sentence = sentence.replace('don\\'t', 'do not')\n    sentence = sentence.replace('Don\\'t', 'Do not')\n    sentence = sentence.replace('can\\'t', 'can not')\n    sentence = sentence.replace('doesn\\'t', 'does not')\n    sentence = sentence.replace('does\\'t', 'does not')\n    sentence = sentence.replace('didn\\'t', 'did not')\n    sentence = sentence.replace('isn\\'t', 'is not')\n    sentence = sentence.replace('Isn\\'t', 'Is not')\n    sentence = sentence.replace('won\\'t', 'will not')\n    sentence = sentence.replace('haven\\'t', 'have not')\n    sentence = sentence.replace('aren\\'t', 'are not')\n    sentence = sentence.replace('hasn\\'t', 'has not')\n    sentence = sentence.replace('shouldn\\'t', 'should not')\n    sentence = sentence.replace('Shouldn\\'t', 'Should not')\n    sentence = sentence.replace('wouldn\\'t', 'would not')\n    sentence = sentence.replace('wasn\\'t', 'was not')\n    sentence = sentence.replace('couldn\\'t', 'could not')\n    sentence = sentence.replace('It\\'s', 'It is')\n    sentence = sentence.replace('that\\'s', 'that is')\n    sentence = sentence.replace('I\\'m', 'I am')\n    sentence = sentence.replace('I\\'ve', 'I have')\n    sentence = sentence.replace('I\\'ll', 'I will')\n    sentence = sentence.replace('you\\'ve', 'you have')\n    sentence = sentence.replace('you\\'re', 'you are')\n    sentence = sentence.replace('there\\'s', 'there is')\n    sentence = sentence.replace('they\\'re', 'they are')\n    return sentence"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"43db6b84-013c-5301-4007-90978011f53f"},"outputs":[],"source":"def replace_q2_puncts(sentence):\n    sentence = sentence.replace('What\\'s', 'What is')\n    sentence = sentence.replace('don\\'t', 'do not')\n    sentence = sentence.replace('Don\\'t', 'Do not')\n    sentence = sentence.replace('can\\'t', 'can not')\n    sentence = sentence.replace('I\\'m', 'I am')\n    sentence = sentence.replace('doesn\\'t', 'does not')\n    sentence = sentence.replace('you\\'ve', 'you have')\n    sentence = sentence.replace('didn\\'t', 'did not')\n    sentence = sentence.replace('I\\'ve', 'I have')\n    sentence = sentence.replace('isn\\'t', 'is not')\n    sentence = sentence.replace('you\\'re', 'you are')\n    sentence = sentence.replace('won\\'t', 'will not')\n    sentence = sentence.replace('they\\'re', 'they are')\n    sentence = sentence.replace('haven\\'t', 'have not')\n    sentence = sentence.replace('aren\\'t', 'are not')\n    sentence = sentence.replace('hasn\\'t', 'has not')\n    sentence = sentence.replace('shouldn\\'t', 'should not')\n    sentence = sentence.replace('wouldn\\'t', 'would not')\n    sentence = sentence.replace('wasn\\'t', 'was not')\n    sentence = sentence.replace('couldn\\'t', 'could not')\n    \n    \n    sentence = sentence.replace('Doesn\\'t', 'Does not')\n    sentence = sentence.replace('Wouldn\\'t', 'Would not')\n    sentence = sentence.replace('weren\\'t', 'were not')\n    sentence = sentence.replace('where\\'s', 'where is')\n    return sentence"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"366e693f-acba-7855-54bd-30c0bd31435c"},"outputs":[],"source":"copy_frame['question1'] = copy_frame['question1'].map(replace_q1_puncts)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e5ae55f8-8ed4-1cef-9c4b-dc7c756d248e"},"outputs":[],"source":"copy_frame['question2'] = copy_frame['question2'].map(replace_q2_puncts)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"38266b40-04da-f572-6cac-fe76c74cf48f"},"outputs":[],"source":"copy_frame['q1_quotes'] = copy_frame['question1'].str.extract('(\\w+\\'\\w+)\\s.*')\ncopy_frame['q2_quotes'] = copy_frame['question2'].str.extract('(\\w+\\'\\w+)\\s.*')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b4f1a055-b6bd-6069-976b-a869e2139168"},"outputs":[],"source":"copy_frame['q1_length'] = copy_frame['question1'].apply(lambda x: len(x.split(' ')))\ncopy_frame['q2_length'] = copy_frame['question2'].apply(lambda x: len(x.split(' ')))\ncopy_frame['words_shared'] = copy_frame.apply(lambda x: find_matching_words(x), axis=1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"070bf978-0c08-89c2-4e5f-6ff93aa99a4b"},"outputs":[],"source":"def find_words_not_in_stops(x):\n    q1_not_stops = set(x['question1'].split(' ')).difference(stops)\n    q2_not_stops = set(x['question2'].split(' ')).difference(stops)\n    commons = q1_not_stops.intersection(q2_not_stops)\n    return commons"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"311d4769-8efa-bde6-bc37-a2015a18809d"},"outputs":[],"source":"copy_frame['words_shared_without_stops'] = copy_frame.apply(lambda x:find_words_not_in_stops(x), axis=1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a06f62a2-6ad9-788b-e894-00821b22615e"},"outputs":[],"source":"copy_frame['words_shared_without_stops_len'] = copy_frame['words_shared_without_stops'].apply(len)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"27390a30-9513-59d9-c724-edb078a1fdb9"},"outputs":[],"source":"quote_counts = []\nquote_counts.append(copy_frame.ix[pd.notnull(copy_frame['q1_quotes'])].shape[0])\nquote_counts.append(copy_frame.ix[pd.isnull(copy_frame['q1_quotes'])].shape[0])\nquote_counts = pd.DataFrame(quote_counts, columns=['counts'], \n                            index=['Quotes', 'Unquotes'])\n\ncolors = ['gold', 'yellowgreen']\nexplode = (0.1, 0)\n \nplt.pie(quote_counts['counts'], labels=quote_counts.index, colors=colors, \n        explode=(1, 0), autopct='%.1f%%',)\nplt.axis('equal')\nplt.show()\nplt.close()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"84b6d96c-58ef-aa4a-69a5-99fb27a759f9"},"outputs":[],"source":"quote_counts = []\nquote_counts.append(copy_frame.ix[pd.notnull(copy_frame['q2_quotes'])].shape[0])\nquote_counts.append(copy_frame.ix[pd.isnull(copy_frame['q2_quotes'])].shape[0])\nquote_counts = pd.DataFrame(quote_counts, columns=['counts'], \n                            index=['Quotes', 'Unquotes'])\n\ncolors = ['gold', 'yellowgreen']\nexplode = (0.1, 0)\n \nplt.pie(quote_counts['counts'], labels=quote_counts.index, colors=colors, \n        explode=(1, 0), autopct='%.1f%%',)\nplt.axis('equal')\nplt.show()\nplt.close()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a9410e58-245f-ab28-76ca-2520db61b848"},"outputs":[],"source":"train_features = ['q1_length', 'q2_length', 'words_shared']\nX = copy_frame[train_features]\ny = copy_frame[target]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)\nclf = RandomForestClassifier()\nclf = clf.fit(X_train, y_train)\n\ny_proba = clf.predict_proba(X_test)\nlog_loss_score = log_loss(y_test, y_proba)\nmetrics.append(log_loss_score)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"86bf4262-08f4-a4c6-334a-30d1068c6bff"},"outputs":[],"source":"pd.DataFrame(metrics, columns=['logloss'])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"80681035-bde6-016b-6676-c63f01851c99"},"outputs":[],"source":"def preprocess():\n    test_set['question1'] = test_set['question1'].map(replace_q1_puncts)\n    test_set['question1'] = test_set['question1'].map(replace_q2_puncts)\n    test_set['q1_length'] = test_set['question1'].apply(lambda x: len(x.split(' ')))\n    test_set['q2_length'] = test_set['question2'].apply(lambda x: len(x.split(' ')))\n    test_set['words_shared'] = test_set.apply(lambda x: find_matching_words(x), axis=1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"29f67751-1f9e-76a7-a05b-185427931800"},"outputs":[],"source":"#preprocess()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1bf53b5-aac3-511c-283d-96aa27e2e5d9"},"outputs":[],"source":"def generate_predictions():\n    test_ids = test_set['test_id']\n    predictions = clf.predict_proba(test_set[train_features])\n\n    submission = pd.DataFrame(test_ids)\n\n    prediction_set = []\n    for i in range(len(predictions)):\n        prediction_set.append(predictions[i][1])\n    \n    prediction_set = pd.DataFrame(prediction_set, columns=[target])\n    submission = pd.concat([submission, prediction_set], axis=1)\n    return submission"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8a4544ae-63ff-40e5-926b-813d00f667d1"},"outputs":[],"source":"#submission = generate_predictions()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9071a76d-599a-f34d-69b4-4f092eb9c850"},"outputs":[],"source":"#print(set(pd.isnull(submission[target])))\n#submission.to_csv(\"submission_quotes_replaced.csv\", index=False)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}