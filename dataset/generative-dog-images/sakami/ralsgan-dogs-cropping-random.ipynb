{"cells":[{"metadata":{},"cell_type":"markdown","source":"### reference: "},{"metadata":{},"cell_type":"markdown","source":"[Kernels]  \n- [RaLSGAN dogs 000213](https://www.kaggle.com/titericz/ralsgan-dogs-vladislav-bakhteev-kernel)\n- [Show Annotations and Breeds](https://www.kaggle.com/paulorzp/show-annotations-and-breeds)\n- [RAM DataLoader](https://www.kaggle.com/speedwagon/ram-dataloader)"},{"metadata":{},"cell_type":"markdown","source":"[Discussions]\n- [Error in Starter Code](https://www.kaggle.com/c/generative-dog-images/discussion/99613#latest-574385)\n- [RaLSGAN Explained](https://www.kaggle.com/c/generative-dog-images/discussion/99485#latest-580430)"},{"metadata":{},"cell_type":"markdown","source":"### What I changed:\n- random dogs cropping\n- adding learning rate scheduler\n- truncated trick"},{"metadata":{"trusted":true},"cell_type":"code","source":"from functools import partial\nfrom multiprocessing import Pool\nimport os\nfrom pathlib import Path\nimport random\nimport shutil\nimport time\nimport xml.etree.ElementTree as ET\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nfrom PIL import Image\nfrom scipy.stats import truncnorm\nimport torch\nfrom torch import nn, optim\nfrom torch.optim.optimizer import Optimizer\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import datasets, transforms\nfrom torchvision.datasets.folder import default_loader\nfrom torchvision.utils import save_image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start_time = time.time()\n\nbatch_size = 32\nepochs = 300\nseed = 1029\n\nTRAIN_DIR = Path('../input/all-dogs/')\nANNOTATION_DIR = Path('../input/annotation/Annotation/')\nDOG_DIR = Path('../dogs/dogs/')\nOUT_DIR = Path('../output_images/')\nDOG_DIR.mkdir(parents=True, exist_ok=True)\nOUT_DIR.mkdir(exist_ok=True)\n\ndevice = torch.device('cuda')\n\nlr = 0.0005\nbeta1 = 0.5\nnz = 256\n\nreal_label = 0.95\nfake_label = 0\n\nrandom.seed(seed)\nos.environ['PYTHONHASHSEED'] = str(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Generator(nn.Module):\n    \n    def __init__(self, nz=128, channels=3):\n        super(Generator, self).__init__()\n        self.nz = nz\n        self.channels = channels\n        \n        def convlayer(n_input, n_output, k_size=4, stride=2, padding=0):\n            block = [\n                nn.ConvTranspose2d(n_input, n_output, kernel_size=k_size, stride=stride, padding=padding, bias=False),\n                nn.BatchNorm2d(n_output),\n                nn.ReLU(inplace=True),\n            ]\n            return block\n\n        self.model = nn.Sequential(\n            *convlayer(self.nz, 1024, 4, 1, 0),\n            *convlayer(1024, 512, 4, 2, 1),\n            *convlayer(512, 256, 4, 2, 1),\n            *convlayer(256, 128, 4, 2, 1),\n            *convlayer(128, 64, 4, 2, 1),\n            nn.ConvTranspose2d(64, self.channels, 3, 1, 1),\n            nn.Tanh()\n        )\n\n    def forward(self, z):\n        z = z.view(-1, self.nz, 1, 1)\n        img = self.model(z)\n        return img\n\n\nclass Discriminator(nn.Module):\n    \n    def __init__(self, channels=3):\n        super(Discriminator, self).__init__()\n        self.channels = channels\n\n        def convlayer(n_input, n_output, k_size=4, stride=2, padding=0, bn=False):\n            block = [nn.Conv2d(n_input, n_output, kernel_size=k_size, stride=stride, padding=padding, bias=False)]\n            if bn:\n                block.append(nn.BatchNorm2d(n_output))\n            block.append(nn.LeakyReLU(0.2, inplace=True))\n            return block\n\n        self.model = nn.Sequential(\n            *convlayer(self.channels, 32, 4, 2, 1),\n            *convlayer(32, 64, 4, 2, 1),\n            *convlayer(64, 128, 4, 2, 1, bn=True),\n            *convlayer(128, 256, 4, 2, 1, bn=True),\n            nn.Conv2d(256, 1, 4, 1, 0, bias=False)\n        )\n\n    def forward(self, imgs):\n        out = self.model(imgs)\n        return out.view(-1, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DogsDataset(Dataset):\n    \n    def __init__(self, root, annotation_root, transform=None,\n                 target_transform=None, loader=default_loader, n_process=4):\n        self.root = Path(root)\n        self.annotation_root = Path(annotation_root)\n        self.transform = transform\n        self.target_transform = target_transform\n        self.loader = loader\n        self.imgs = self.cut_out_dogs(n_process)\n\n    def _get_annotation_path(self, img_path):\n        dog = Path(img_path).stem\n        breed = dog.split('_')[0]\n        breed_dir = next(self.annotation_root.glob(f'{breed}-*'))\n        return breed_dir / dog\n    \n    @staticmethod\n    def _get_dog_box(annotation_path):\n        tree = ET.parse(annotation_path)\n        root = tree.getroot()\n        objects = root.findall('object')\n        for o in objects:\n            bndbox = o.find('bndbox')\n            xmin = int(bndbox.find('xmin').text)\n            ymin = int(bndbox.find('ymin').text)\n            xmax = int(bndbox.find('xmax').text)\n            ymax = int(bndbox.find('ymax').text)\n            yield (xmin, ymin, xmax, ymax)\n            \n    def crop_dog(self, path):\n        imgs = []\n        annotation_path = self._get_annotation_path(path)\n        for bndbox in self._get_dog_box(annotation_path):\n            img = self.loader(path)\n            img_ = img.crop(bndbox)\n            if np.sum(img_) != 0:\n                img = img_\n            imgs.append(img)\n        return imgs\n                \n    def cut_out_dogs(self, n_process):\n        with Pool(n_process) as p:\n            imgs = p.map(self.crop_dog, self.root.iterdir())\n        return imgs\n    \n    def __getitem__(self, index):\n        samples = random.choice(self.imgs[index])\n        if self.transform is not None:\n            samples = self.transform(samples)\n        return samples\n    \n    def __len__(self):\n        return len(self.imgs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ParamScheduler(object):\n    \n    def __init__(self, optimizer, scale_fn, step_size):\n        if not isinstance(optimizer, Optimizer):\n            raise TypeError('{} is not an Optimizer'.format(\n                type(optimizer).__name__))\n        \n        self.optimizer = optimizer\n        self.scale_fn = scale_fn\n        self.step_size = step_size\n        self.last_batch_iteration = 0\n        \n    def batch_step(self):\n        for param_group in self.optimizer.param_groups:\n            param_group['lr'] = self.scale_fn(self.last_batch_iteration / self.step_size)\n        \n        self.last_batch_iteration += 1\n\n\ndef combine_scale_functions(scale_fns, phases=None):\n    if phases is None:\n        phases = [1. / len(scale_fns)] * len(scale_fns)\n    phases = [phase / sum(phases) for phase in phases]\n    phases = torch.tensor([0] + phases)\n    phases = torch.cumsum(phases, 0)\n    \n    def _inner(x):\n        idx = (x >= phases).nonzero().max()\n        actual_x = (x - phases[idx]) / (phases[idx + 1] - phases[idx])\n        return scale_fns[idx](actual_x)\n        \n    return _inner\n\n\ndef scale_cos(start, end, x):\n    return start + (1 + np.cos(np.pi * (1 - x))) * (end - start) / 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_transforms = [transforms.RandomRotation(degrees=5)]\ntransform = transforms.Compose([transforms.Resize(64),\n                                transforms.CenterCrop(64),\n                                transforms.RandomHorizontalFlip(p=0.5),\n                                transforms.RandomApply(random_transforms, p=0.3),\n                                transforms.ToTensor(),\n                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\ntrain_data = DogsDataset(TRAIN_DIR / 'all-dogs/', ANNOTATION_DIR, transform=transform)\ntrain_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, num_workers=4)\n\nimgs = next(iter(train_loader))\nimgs = imgs.numpy().transpose(0, 2, 3, 1)\n\nnet_g = Generator(nz).to(device)\nnet_d = Discriminator().to(device)\n\ncriterion = nn.BCELoss()\nscale_fn = combine_scale_functions(\n    [partial(scale_cos, 1e-5, 5e-4), partial(scale_cos, 5e-4, 1e-4)], [0.2, 0.8])\n\noptimizer_g = optim.Adam(net_g.parameters(), lr=lr, betas=(beta1, 0.999))\noptimizer_d = optim.Adam(net_d.parameters(), lr=lr, betas=(beta1, 0.999))\n\nscheduler_g = ParamScheduler(optimizer_g, scale_fn, epochs * len(train_loader))\nscheduler_d = ParamScheduler(optimizer_d, scale_fn, epochs * len(train_loader))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for epoch in range(epochs):\n    for i, real_images in enumerate(train_loader):\n        # --------------------------------------\n        # Update Discriminator network: maximize log(D(x)) + log(1 - D(G(z)))\n        # --------------------------------------\n        net_d.zero_grad()\n        real_images = real_images.to(device)\n        batch_size = real_images.size(0)\n        labels = torch.full((batch_size, 1), real_label, device=device)\n        \n        scheduler_d.batch_step()\n        output_real = net_d(real_images)\n        noise = torch.randn(batch_size, nz, 1, 1, device=device)\n        fake = net_g(noise)\n        output_fake = net_d(fake.detach())\n        err_d = (torch.mean((output_real - torch.mean(output_fake) - labels) ** 2) + \n                 torch.mean((output_fake - torch.mean(output_real) + labels) ** 2)) / 2\n        err_d.backward(retain_graph=True)\n        optimizer_d.step()\n        \n        # --------------------------------------\n        # Update Generator network: maximize log(D(G(z)))\n        # --------------------------------------\n        net_g.zero_grad()\n        scheduler_g.batch_step()\n        output_fake = net_d(fake)   \n        err_g = (torch.mean((output_real - torch.mean(output_fake) + labels) ** 2) +\n                 torch.mean((output_fake - torch.mean(output_real) - labels) ** 2)) / 2\n        err_g.backward()\n        optimizer_g.step()\n        \n    print(f'[{epoch + 1}/{epochs}] Loss_d: {err_d.item():.4f} Loss_g: {err_g.item():.4f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def truncated_normal(size, threshold=1):\n    values = truncnorm.rvs(-threshold, threshold, size=size)\n    return values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"im_batch_size = 50\nn_images = 10000\n\nfor i_batch in range(0, n_images, im_batch_size):\n    z = truncated_normal((im_batch_size, nz, 1, 1), threshold=1)\n    gen_z = torch.from_numpy(z).float().to(device)\n    gen_images = (net_g(gen_z) + 1) / 2\n    images = gen_images.to('cpu').clone().detach()\n    images = images.numpy().transpose(0, 2, 3, 1)\n    for i_image in range(gen_images.size(0)):\n        save_image(gen_images[i_image, :, :, :], OUT_DIR / f'image_{i_batch + i_image:05d}.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(25, 16))\nfor i, j in enumerate(images[:32]):\n    ax = fig.add_subplot(4, 8, i + 1, xticks=[], yticks=[])\n    plt.imshow(j)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shutil.make_archive('images', 'zip', OUT_DIR)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"elapsed_time = time.time() - start_time\nprint(f'All process done in {int(elapsed_time // 3600)} hours {int(elapsed_time % 3600 // 60)} min.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}