{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import clear_output\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nfrom keras.layers import Dropout, BatchNormalization, Reshape, Flatten, RepeatVector\nfrom keras.layers import Lambda, Dense, Input, Conv2D, MaxPool2D, UpSampling2D, concatenate\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.layers import Activation\nfrom keras.models import Model, load_model\n\n\n# Регистрация сессии в keras\nfrom keras import backend as K\nimport tensorflow as tf\nsess = tf.Session()\nK.set_session(sess)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport random\nfrom tqdm import tqdm\nimport shutil\n\ndef rgb2gray(rgb):\n    return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])\n\nComputeLB = False\nDogsOnly = True\n\nimport numpy as np, pandas as pd, os\nimport xml.etree.ElementTree as ET \nimport matplotlib.pyplot as plt, zipfile \nfrom PIL import Image \n\nROOT = '../input/generative-dog-images/'\nif not ComputeLB: ROOT = '../input/'\nIMAGES = os.listdir(ROOT + 'all-dogs/all-dogs/')\nbreeds = os.listdir(ROOT + 'annotation/Annotation/') \n\nidxIn = 0; namesIn = []\nimagesIn = np.zeros((25000,64,64,3))\nimagesIn2 = np.zeros((25000,32,32,3))\n# CROP WITH BOUNDING BOXES TO GET DOGS ONLY\n# https://www.kaggle.com/paulorzp/show-annotations-and-breeds\nif DogsOnly:\n    for breed in breeds:\n        for dog in os.listdir(ROOT+'annotation/Annotation/'+breed):\n            try: img = Image.open(ROOT+'all-dogs/all-dogs/'+dog+'.jpg') \n            except: continue           \n            tree = ET.parse(ROOT+'annotation/Annotation/'+breed+'/'+dog)\n            root = tree.getroot()\n            objects = root.findall('object')\n            for o in objects:\n                bndbox = o.find('bndbox') \n                xmin = int(bndbox.find('xmin').text)\n                ymin = int(bndbox.find('ymin').text)\n                xmax = int(bndbox.find('xmax').text)\n                ymax = int(bndbox.find('ymax').text)\n                w = np.min((xmax - xmin, ymax - ymin))\n                img2 = img.crop((xmin, ymin, xmin+w, ymin+w))\n                #img3 = img2.convert('LA')\n                img3 = img2\n                #img2 = img2.convert('LA')\n                #img2 = img2.resize((64,64), Image.ANTIALIAS)\n                img2 = img2.resize((64,64), Image.ANTIALIAS)\n                img3 = img3.resize((32,32), Image.ANTIALIAS)\n                #img3 = rgb2gray(np.asarray(img2)) \n                #imagesIn[idxIn,:,:] = np.asarray(img2)[:,:,0]\n                imagesIn[idxIn,:,:,:] = np.asarray(img2)\n                #imagesIn2[idxIn,:,:] = np.asarray(img3)[:,:,0]\n                imagesIn2[idxIn,:,:,:] = np.asarray(img3)\n                #if idxIn%1000==0: print(idxIn)\n                namesIn.append(breed)\n                idxIn += 1\n    idx = np.arange(idxIn)\n    np.random.shuffle(idx)\n#     imagesIn = imagesIn[idx,:,:,:]\n#     imagesIn2 = imagesIn2[idx,:,:]\n    imagesIn = imagesIn[idx,:,:]\n    imagesIn2 = imagesIn2[idx,:,:]\n    namesIn = np.array(namesIn)[idx]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plt.imshow(imagesIn[101,:,:], cmap='Greys')\nplt.imshow(Image.fromarray( (imagesIn[101]).astype('uint8').reshape((64,64,3))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(Image.fromarray( (imagesIn2[101]).astype('uint8').reshape((32,32,3))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def gen_batch(x, y):\n    n_batches = x.shape[0] // batch_size\n    while(True):\n        idxs = np.random.permutation(y.shape[0])\n        x = x[idxs]\n        y = y[idxs]\n        for i in range(n_batches):\n            yield x[batch_size*i: batch_size*(i+1)], y[batch_size*i: batch_size*(i+1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.utils import to_categorical\nimport sklearn.preprocessing\nL_enc = sklearn.preprocessing.LabelEncoder()\ny_train_cat = L_enc.fit_transform(namesIn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_cat = to_categorical(y_train_cat).astype(np.float32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = imagesIn2.astype('float32') / 255.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#x_train = np.reshape(x_train, (len(x_train), 32,32, 1))\nx_train = np.reshape(x_train, (len(x_train), 32,32, 3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nbatch_size = 64\nNdim = 32\n\nbatch_shape = (batch_size, Ndim, Ndim, 3)\nlatent_dim = 256\nnum_classes = 120\ndropout_rate = 0.3\ngamma = 1 \n\n\ntrain_batches_it = gen_batch(x_train, y_train_cat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_ = tf.placeholder(tf.float32, shape=(None, Ndim, Ndim, 3),  name='image')\ny_ = tf.placeholder(tf.float32, shape=(None, 120),         name='labels')\nz_ = tf.placeholder(tf.float32, shape=(None, latent_dim), name='z')\n\nimg = Input(tensor=x_)\nlbl = Input(tensor=y_)\nz   = Input(tensor=z_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef add_units_to_conv2d(conv2, units):\n    dim1 = int(conv2.shape[1])\n    dim2 = int(conv2.shape[2])\n    #dim3 = int(conv2.shape[3]) #!\n    dimc = int(units.shape[1])\n    repeat_n = dim1*dim2\n    units_repeat = RepeatVector(repeat_n)(lbl)\n    units_repeat = Reshape((dim1, dim2, dimc))(units_repeat)\n    return concatenate([conv2, units_repeat])\n\n\ndef apply_bn_relu_and_dropout(x, bn=False, relu=True, dropout=True):\n    if bn:\n        x = BatchNormalization(momentum=0.99, scale=False)(x)\n    if relu:\n        x = LeakyReLU()(x)\n    if dropout:\n        x = Dropout(dropout_rate)(x)\n    return x\n\n\nwith tf.variable_scope('encoder'):\n    x = Conv2D(32, kernel_size=(3, 3), strides=(2, 2), padding='same')(img)\n    #x = Conv2D(32, kernel_size=(3), strides=(2, 2), padding='same')(img)\n    x = apply_bn_relu_and_dropout(x)\n    x = MaxPool2D((2, 2), padding='same')(x)\n\n    x = Conv2D(64, kernel_size=(3, 3), padding='same')(x)\n    #x = Conv2D(64, kernel_size=(3), padding='same')(x)\n    x = apply_bn_relu_and_dropout(x)\n    \n    #x = MaxPool2D((2, 2), padding='same')(x)\n\n    #x = Conv2D(64, kernel_size=(3, 3), padding='same')(x)\n    #x = Conv2D(128, kernel_size=(3), padding='same')(x)\n    #x = apply_bn_relu_and_dropout(x)\n\n    x = Flatten()(x)\n    x = concatenate([x, lbl])\n    \n    h = Dense(512)(x) \n    h = apply_bn_relu_and_dropout(h)\n\n    z_mean    = Dense(latent_dim)(h)\n    z_log_var = Dense(latent_dim)(h)\n\n    def sampling(args):\n        z_mean, z_log_var = args\n        epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0., stddev=1.0)\n        return z_mean + K.exp(K.clip(z_log_var/2, -2, 2)) * epsilon\n    l = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\nencoder = Model([img, lbl], [z_mean, z_log_var, l], name='Encoder')\n\n\nwith tf.variable_scope('decoder'):\n    x = concatenate([z, lbl])\n\n    x = Dense(64*64*2)(x)\n    x = apply_bn_relu_and_dropout(x)\n    #x = Reshape((4, 4,16))(x)\n    x = Reshape((16, 16, 32))(x)\n    x = UpSampling2D(size=(2,2))(x)\n\n    x = Conv2D(128, kernel_size=(5,5), padding='same')(x)\n    x = apply_bn_relu_and_dropout(x)\n\n    x = Conv2D(64, kernel_size=(3,3), padding='same')(x)\n    #x = UpSampling2D(size=(2, 2))(x)\n    x = apply_bn_relu_and_dropout(x)\n\n#     x = Conv2D(32, kernel_size=(3,3), padding='same')(x)\n#     x = UpSampling2D(size=(2, 2))(x)\n#     x = apply_bn_relu_and_dropout(x)\n    \n    x = Conv2D(32, kernel_size=(3,3), padding='same')(x)\n    x = apply_bn_relu_and_dropout(x)\n    decoded = Conv2D(3, kernel_size=(5,5), activation='sigmoid', padding='same')(x)\n\ndecoder = Model([z, lbl], decoded, name='Decoder')\n\n\nwith tf.variable_scope('discrim'):\n    x = Conv2D(128, kernel_size=(7, 7), strides=(2, 2), padding='same')(img)\n    x = MaxPool2D((2, 2), padding='same')(x)\n    x = apply_bn_relu_and_dropout(x)\n    x = add_units_to_conv2d(x, lbl)\n\n    x = Conv2D(128, kernel_size=(3, 3), padding='same')(x)\n    x = MaxPool2D((2, 2), padding='same')(x)\n    x = apply_bn_relu_and_dropout(x)\n\n    x = Conv2D(128, kernel_size=(3, 3), padding='same')(x)\n    x = MaxPool2D((2, 2), padding='same')(x)\n    x = apply_bn_relu_and_dropout(x)\n    \n    x = Conv2D(64, kernel_size=(3, 3), padding='same')(x)\n    x = MaxPool2D((2, 2), padding='same')(x)\n    x = apply_bn_relu_and_dropout(x)\n    \n    x = Conv2D(64, kernel_size=(3, 3), padding='same')(x)\n    x = MaxPool2D((2, 2), padding='same')(x)\n    x = apply_bn_relu_and_dropout(x)\n    # l-слой на котором будем сравнивать активации\n    l = Conv2D(32, kernel_size=(3, 3), padding='same')(x)\n    x = apply_bn_relu_and_dropout(x)\n\n    h = Flatten()(x)\n    d = Dense(1, activation='sigmoid')(h)\n\ndiscrim = Model([img, lbl], [d, l], name='Discriminator')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"z_mean, z_log_var, encoded_img = encoder([img, lbl])\n\ndecoded_img = decoder([encoded_img, lbl])\ndecoded_img.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"z_mean, z_log_var, encoded_img = encoder([img, lbl])\n\ndecoded_img = decoder([encoded_img, lbl])\ndecoded_z   = decoder([z,           lbl])\n\ndiscr_img,     discr_l_img     = discrim([img,         lbl])\ndiscr_dec_img, discr_l_dec_img = discrim([decoded_img, lbl])\ndiscr_dec_z,   discr_l_dec_z   = discrim([decoded_z,   lbl])\n\ncvae_model = Model([img, lbl], decoder([encoded_img, lbl]), name='cvae')\ncvae =  cvae_model([img, lbl])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nL_prior = -0.5*tf.reduce_sum(1. + tf.clip_by_value(z_log_var, -2, 2) - tf.square(z_mean) \\\n                             - tf.exp(tf.clip_by_value(z_log_var, -2, 2)))/Ndim/Ndim\n\nlog_dis_img     = tf.log(discr_img + 1e-10)\nlog_dis_dec_z   = tf.log(1. - discr_dec_z + 1e-10)\nlog_dis_dec_img = tf.log(1. - discr_dec_img + 1e-10)\n\nL_GAN = -1/4*tf.reduce_sum(log_dis_img + 2*log_dis_dec_z + log_dis_dec_img)/Ndim/Ndim\n\n# L_dis_llike = tf.reduce_sum(tf.square(discr_l_img - discr_l_dec_img))/28/28\nL_dis_llike = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.sigmoid(discr_l_img),\n                                                                    logits=discr_l_dec_img))/Ndim/Ndim\n\n\n\nL_enc = L_dis_llike + L_prior \nL_dec = gamma * L_dis_llike - L_GAN\nL_dis = L_GAN\n\n\n\noptimizer_enc = tf.train.RMSPropOptimizer(0.001)\noptimizer_dec = tf.train.RMSPropOptimizer(0.0006)#0.0003\noptimizer_dis = tf.train.RMSPropOptimizer(0.001)\n\nencoder_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"encoder\")\ndecoder_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"decoder\")\ndiscrim_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"discrim\")\n\nstep_enc = optimizer_enc.minimize(L_enc, var_list=encoder_vars)\nstep_dec = optimizer_dec.minimize(L_dec, var_list=decoder_vars)\nstep_dis = optimizer_dis.minimize(L_dis, var_list=discrim_vars)\n\n\ndef step(image, label, zp):\n    l_prior, dec_image, l_dis_llike, l_gan, _, _ = sess.run([L_prior, decoded_z, L_dis_llike, L_GAN, step_enc, step_dec],\n                                                            feed_dict={z:zp, img:image, lbl:label, K.learning_phase():1})\n    return l_prior, dec_image, l_dis_llike, l_gan\n\ndef step_d(image, label, zp):\n    l_gan, _ = sess.run([L_GAN, step_dis], feed_dict={z:zp, img:image, lbl:label, K.learning_phase():1})\n    return l_gan","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sess.run(tf.global_variables_initializer())\nsave_periods = list(range(100)) + list(range(100, 1000, 10))\nnb_step = 3 \n\nbatches_per_period = 300\nfor i in range(30000):\n    print('.', end='')\n\n\n    for j in range(nb_step):\n        b0, b1 = next(train_batches_it)\n        zp = np.random.randn(batch_size, latent_dim)\n        l_g = step_d(b0, b1, zp)\n        if l_g < 1.0:\n            break\n        \n\n    for j in range(nb_step):\n        l_p, zx, l_d, l_g = step(b0, b1, zp)\n        if l_g > 0.4:\n            break\n        b0, b1 = next(train_batches_it)\n        zp = np.random.randn(batch_size, latent_dim)\n\n\n    if not i % batches_per_period:\n        period = i // batches_per_period\n\n        print(i, l_p, l_d, l_g)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport sys\nimport random\nimport warnings\n\nimport numpy as np\nimport pandas as pd\nimport cv2\n\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm\nfrom itertools import chain\nimport skimage\nfrom PIL import Image\nfrom skimage.io import imread, imshow, imread_collection, concatenate_images\nfrom skimage.transform import resize\nfrom skimage.util import crop, pad\nfrom skimage.morphology import label\nfrom skimage.color import rgb2gray, gray2rgb, rgb2lab, lab2rgb\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input\nfrom keras.models import Model, load_model,Sequential\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import Input, Dense, UpSampling2D, RepeatVector, Reshape, Embedding\nfrom keras.layers.core import Dropout, Lambda\nfrom keras.layers.convolutional import Conv2D, Conv2DTranspose\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers.merge import concatenate\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras import backend as K\n\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"IMG_WIDTH = 32\nIMG_HEIGHT = 32\nIMG_CHANNELS = 3\nINPUT_SHAPE=(IMG_HEIGHT, IMG_WIDTH, 3)\n\ndef Colorize():\n    in_label = Input(shape=(1,))\n    embed_input = Embedding(120, 1000)(in_label)\n    \n    \n    #Encoder\n    encoder_input = Input(shape=(IMG_WIDTH, IMG_HEIGHT, 3,))\n    encoder_output = Conv2D(32, (3,3), activation='relu', padding='same',strides=1)(encoder_input)\n    encoder_output = MaxPooling2D((2, 2), padding='same')(encoder_output)\n    encoder_output = Conv2D(32, (4,4), activation='relu', padding='same')(encoder_output)\n    encoder_output = Conv2D(32, (3,3), activation='relu', padding='same',strides=1)(encoder_output)\n    encoder_output = MaxPooling2D((2, 2), padding='same')(encoder_output)\n    encoder_output = Conv2D(64, (4,4), activation='relu', padding='same')(encoder_output)\n    encoder_output = Conv2D(64, (3,3), activation='relu', padding='same',strides=1)(encoder_output)\n    encoder_output = MaxPooling2D((2, 2), padding='same')(encoder_output)\n    encoder_output = Conv2D(64, (4,4), activation='relu', padding='same')(encoder_output)\n    encoder_output = Conv2D(64, (3,3), activation='relu', padding='same')(encoder_output)\n    encoder_output = Conv2D(64, (3,3), activation='relu', padding='same')(encoder_output)\n    \n    #Fusion\n    #fusion_output = RepeatVector(2 * 2)(embed_input) \n    fusion_output = Dense(1024, activation='relu')(embed_input) \n    fusion_output = Reshape(([4, 4, 64]))(fusion_output)\n    fusion_output = concatenate([encoder_output, fusion_output], axis=3) \n    fusion_output = Conv2D(64, (1, 1), activation='relu', padding='same')(fusion_output)\n    \n    #Decoder\n    decoder_output = Conv2D(64, (3,3), activation='relu', padding='same')(fusion_output)\n    decoder_output = Conv2D(32, (3,3), activation='relu', padding='same')(decoder_output)\n    decoder_output = UpSampling2D((2, 2))(decoder_output)\n    decoder_output = Conv2D(64, (3,3), activation='relu', padding='same')(decoder_output)\n    decoder_output = UpSampling2D((2, 2))(decoder_output)\n    decoder_output = Conv2D(128, (4,4), activation='relu', padding='same')(decoder_output)\n    decoder_output = Conv2D(128, (3,3), activation='relu', padding='same')(decoder_output)\n    decoder_output = Conv2D(64, (2,2), activation='relu', padding='same')(decoder_output)\n    decoder_output = Conv2D(64, (3, 3), activation='relu', padding='same')(decoder_output)\n    decoder_output = UpSampling2D((2, 2))(decoder_output)\n    \n    decoder_output = Conv2D(128, (4,4), activation='relu', padding='same')(decoder_output)\n    decoder_output = Conv2D(128, (3,3), activation='relu', padding='same')(decoder_output)\n    decoder_output = Conv2D(3, (3, 3), activation='sigmoid', padding='same')(decoder_output)\n    decoder_output = UpSampling2D((2, 2))(decoder_output)\n    return Model(inputs=[encoder_input, in_label], outputs=decoder_output)\n\nmodel = Colorize()\nmodel.compile(optimizer='adam', loss='mean_squared_error')\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.utils import to_categorical\nimport sklearn.preprocessing\nL_enc = sklearn.preprocessing.LabelEncoder()\nlabels = L_enc.fit_transform(namesIn)\nlabels = np.array(labels).reshape((len(labels),1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shift = 6\ny_train = imagesIn / 256.\ndatagen = ImageDataGenerator(\n        shear_range=0.2,\n        zoom_range=0.2,\n        rotation_range=20,\n        #width_shift_range=shift, \n        #height_shift_range=shift,\n        #zca_whitening=True,\n    \n        horizontal_flip=True)\n\ndef image_a_b_gen(dataset=[labels, y_train], batch_size = 20):\n    datagen.fit(y_train)\n    for y_batch, labels_batch  in datagen.flow(y_train, labels, batch_size=batch_size):\n        \n\n        images = [Image.fromarray( (256.*Y).astype('uint8').reshape((64,64,3))) for Y in y_batch]\n        img = [(I.resize((32,32), Image.ANTIALIAS)) for I in images]\n        X_batch = np.array([np.asarray(I)[:,:] for I in img])/256.0\n        \n        yield [X_batch, labels_batch], y_batch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = imagesIn2 / 256.\nlearning_rate_reduction = ReduceLROnPlateau(monitor='loss', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5,\n                                            min_lr=0.00001)\nfilepath = \"Art_Colorization_Model.h5\"\ncheckpoint = ModelCheckpoint(filepath,\n                             save_best_only=True,\n                             monitor='loss',\n                             mode='min')\n\nmodel_callbacks = [learning_rate_reduction,checkpoint]\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 20\nmodel.fit_generator(image_a_b_gen([labels,y_train],BATCH_SIZE),\n            epochs=30,\n            verbose=1,\n            steps_per_epoch=y_train.shape[0]/BATCH_SIZE,\n             callbacks=model_callbacks\n                   )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_batch = X_train\n# y_train = imagesIn / 256.\n# y_train = np.reshape(y_train, (len(y_train), 128,128, 1))\n# model.fit([X_batch,labels],y_train,\n#             epochs=1,\n#             verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_rgb(img):\n    color_me_embed = np.array(np.random.randint(120))\n    img = np.asarray(img)\n    img = img.reshape((32,32,3))/256.0\n    color_me_embed = color_me_embed.reshape(1,1)\n    #print(color_me.shape)\n    output = model.predict([[img], color_me_embed.reshape(1,1)])\n    \n    output = output*256.\n    return Image.fromarray( (output).astype('uint8').reshape((64,64,3)))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import norm\ndef image_gen():\n    label = np.random.randint(120)\n    \n    input_lbl = np.zeros((1, 120))\n    input_lbl[0, label] = 1\n    xi = norm.ppf(np.linspace(0.05, 0.95, 1))\n    yi = norm.ppf(np.linspace(0.05, 0.95, 1))\n\n    z_sample = np.zeros((1, latent_dim))\n    z_sample[:, :2] = np.array([[xi[0], yi[0]]])\n\n    x_decoded = sess.run(decoded_z, feed_dict={z:z_sample, lbl:input_lbl, K.learning_phase():0})\n    img = x_decoded[0].squeeze()\n    return Image.fromarray((256*img).astype('uint8'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img = image_gen()\nimg2 = get_rgb(img)\nplt.imshow(img)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(img2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import zipfile\nmy_zipfile = zipfile.PyZipFile('images.zip', mode='w')\n\nfor k in range(10000):\n\n    img = image_gen()\n    img = get_rgb(img)\n    f = str(k)+'.png'\n    img.save(f,'PNG')\n    my_zipfile.write(f)\n    os.remove(f)\n    #if k % 1000==0: print(k)\nmy_zipfile.close()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"--------------------------------------------------------------------"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}