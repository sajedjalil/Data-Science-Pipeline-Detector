{"cells":[{"metadata":{},"cell_type":"markdown","source":"  --- codes_dim=24, nz=120, trun(0.8), batch_size=32 ---  \n  \n* v6 :  epochs=120, 14M(G),12M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999 : FID=32.91  \n* v7 :  epochs=70, 32M(G),31M(D), lr=2e-4(G),4e-4(D), beta1=0.0, beta2=0.999 : FID=45.10  \n* v8 :  epochs=90, 25M(G),22M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999 : FID=47.23  \n* v9 :  epochs=120, EMA_G(decay_rate=0.9999), 14M(G),12M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999 : FID=39.79  \n* v10:  epochs=140, 10M(G),8M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999 : FID=33.95  \n* v11:  epochs=120, 14M(G),12M(D), lr=1e-4(G),4e-4(D), beta1=0.0, beta2=0.999 : FID=57.68   \n* v12:  epochs=120, 14M(G),12M(D), lr=3e-4(G),5e-4(D), beta1=0.0, beta2=0.999 : FID=38.79   \n* v13:  epochs=180, 8M(G),7M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999 : FID=31.06  \n* v14:  epochs=180, 6M(G),5M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999 : FID=33.95  \n* v16:  epochs=220, 4M(G),3M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999 : FID=35.44  \n* v19:  epochs=170, same as v13 : FID=37.92  \n* v20:  epochs=180, 8M(G),7M(D), lr=3e-4(G),3e-4(D), beta1=0.5, beta2=0.999 : FID=33.57  \n* v21:  epochs=200, 8M(G),7M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999, batch_size=128 : FID=60.34  \n* v23:  epochs=130, 8M(G),7M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999, batch_size=16 : FID=33.85  \n  \n  \n  --- use hypercolumns + scSE layers---  \n  \n* v38: epochs=130, 10M(G),10M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999: FID=50.38  \n  \n  \n  --- scSE layers ---  \n\n* v39: epochs=140, 10M(G),10M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999: FID=34.26  \n  \n  \n  --- hinge loss (remove sigmoid in D) ---  \n  \n* v33: epochs=170, 8M(G),7M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999: FID=34.84  \n  \n  \n  --- hinge loss (remove sigmoid in D) + scSE layers ---  \n  \n* v34: epochs=160, 8M(G),7M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999: FID=32.43  \n* v36: epochs=140, 10M(G),8M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999: FID=34.31  \n  \n  \n  --- hinge loss + use hypercolumns ---  \n  \n* v37: epochs=130, 10M(G),10M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999: FID=42.68  \n  \n  \n \n  \n  --- standard loss, remove bn ---  \n  \n* v24:  epochs=100, 8M(G),7M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999, batch_size=8 : FID=50.77    \n* v25:  epochs=80, 6M(G),5M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999, batch_size=8 : FID=52.29    \n  \n  --- remove bn, corrected self_attention + leaky_relu ---  \n  \n* v26: epochs=90, 6M(G),5M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999, batch_size=8 : FID=49.14  \n* v27: epochs=70, 8M(G),7M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999, batch_size=8 : FID=58.00  \n* v29: epochs=110, 8M(G),7M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999, batch_size=16 : FID=38.77  \n* v30: epochs=150, 8M(G),7M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999, batch_size=32 : FID=38.77\n* v32: epochs=130, 10M(G),8M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999, batch_size=32 : FID=30.95(LB=31.10)  \n* v35: epochs=140, hinge loss, 10M(G),8M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999, batch_size=32 : FID=35.65  \n* v56: epochs=130, 10M(G),8M(D), lr=6e-4(G),6e-4(D), beta1=0.0, beta2=0.999, batch_size=32 : FID=34.42  \n* v57: epochs=140, 10M(G),8M(D), lr=8e-4(G),8e-4(D), beta1=0.0, beta2=0.999, batch_size=32 : FID=35.81  \n* v60: epochs=140, 10M(G),8M(D), lr=12e-4(G),12e-4(D), lr[30,60], beta1=0.0, beta2=0.999, batch_size=32 : FID=34.65  \n* v72: epochs=MAX, same as v32 but with sharpen(1) : FID=  \n\n  \n  --- more attention layers ---  \n  \n* v31: epochs=140, 8M(G),7M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999, batch_size=32 : FID=41.62    \n\n  \n  --- fc with bn and relu ---  \n  \n* v22:  epochs=170, fc(bn+relu), 8M(G),7M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999 : FID=35.34  \n  \n  --- leaky_relu ---  \n  \n* v17:  epochs=170, 8M(G),7M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999 : FID=34.15  \n  \n  --- leaky_relu + label_noise prob 0.1 ---  \n  \n* v18:  epochs=180, 8M(G),7M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999 : FID=33.93   \n  \n  \n  --- shallower ---  \n  \n* v40: epochs=130, 10M(G),10M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999: FID=62.33  \n  \n  \n  \n  --- so far v13 and v32 are the best ---  \n  \n  --- scSE + leaky_relu + pixelshuffle ---  \n  \n* v41: epochs=230, 10M(G),10M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999: FID=42.67  \n* v42: epochs=160, 15M(G),15M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999: FID=36.55  \n* v43: epochs=150, 20M(G),20M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999: FID=37.37  \n  \n  --- Attention + leaky_relu + pixelshuffle ---  \n  \n* v44: epochs=200, 10M(G),10M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999: FID=39.74  \n* v45: epochs=170, 15M(G),15M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999: FID=mode collapse  \n* v46: epochs=150, 20M(G),20M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999: FID=mode collapse  \n  \n  --- + label_noise prob 0.05 ---  \n  \n* v49: epochs=170, 15M(G),15M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999: FID=40.46  \n  \n  \n  \n  --- resize(64,64) ---  \n  --- Attention + leaky_relu ---  \n  \n* v47: epochs=140, 10M(G),8M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999: FID=mode collapse  \n* v48: epochs=120, 10M(G),10M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999: FID=mode collapse  \n  \n  \n  --- G:Attention(64x64) + D:sSE(64x64)+Attention(32x32) + leaky_relu ---  \n  \n* v50: epochs=110, 10M(G),8M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999: FID=36.47  \n* v51: epochs=110, 10M(G),10M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999: FID=36.96  \n* v52: epochs=120, 10M(G),8M(D), lr=3e-4(G),3e-4(D), beta1=0.5, beta2=0.999: FID=48.00  \n  \n  \n  --- resize(64,64) ---  \n  \n* v53: epochs=120, 10M(G),8M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999: FID=36.86  \n* v54: epochs=110, 10M(G),8M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999, label_noise(0.01): FID=37.19  \n  \n  \n  --- v32+bottleneck ---  \n  \n* v55: epochs=80, 10M(G),8M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999, label_noise(0.01): FID=69.72  \n  \n  \n  --- resize(84,84) ---  \n  \n* v58: epochs=140, 10M(G),8M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999: FID=42.46  \n* v59: epochs=130, 10M(G),8M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999, label_noise(0.01): FID=37.15  \n  \n  \n  --- nz=60, codes_dim=10 ---  \n  \n* v61: epochs=140, 10M(G),8M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999, batch_size=32 : FID=34.25  \n* v62: epochs=140, 10M(G),8M(D), lr=12e-4(G),12e-4(D), lr[30,60], beta1=0.0, beta2=0.999, batch_size=32 : FID=32.27  \n* v70: epochs=100, 10M(G),8M(D), lr=12e-4(G),12e-4(D), lr[30,60], beta1=0.0, beta2=0.999, batch_size=32, duplicate good images : FID=40.23  \n* v71: epochs=90, 10M(G),8M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999, batch_size=32, duplicate good images : FID=38.38    \n* v73: epochs=MAX, same with v62 but with sharpen(1) : FID=  \n* v74: epochs=MAX, 8M(G),7M(D), lr=12e-4(G),12e-4(D), lr[30,60], beta1=0.0, beta2=0.999, batch_size=32, sharpen(1) : FID=  \n* v75: epochs=MAX, 6M(G),5M(D), lr=12e-4(G),12e-4(D), lr[30,60], beta1=0.0, beta2=0.999, batch_size=32, sharpen(1) : FID=  \n  \n  \n  --- + res concat ---  \n  \n* v65: epochs=140, 6M(G),5M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999, batch_size=32 : FID=39.76  \n* v66: epochs=130, 7M(G),6M(D), lr=3e-4(G),3e-4(D), beta1=0.0, beta2=0.999, batch_size=32 : FID=36.99  \n* v67: epochs=130, 6M(G),5M(D), lr=12e-4(G),12e-4(D), lr[30,60], beta1=0.0, beta2=0.999, batch_size=32 : FID=34.28  \n  \n  \n  --- DenseBlock ---  \n  \n* v68: epochs=150, 2.2M(G),1.6M(D), lr=12e-4(G),12e-4(D), lr[30,60], beta1=0.0, beta2=0.999, batch_size=32 : FID=too slow  \n* v69: epochs=90, 5M(G),4M(D), lr=12e-4(G),12e-4(D), lr[30,60], beta1=0.0, beta2=0.999, batch_size=32 : FID=too slow  "},{"metadata":{},"cell_type":"markdown","source":"# Import Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n%matplotlib inline\nimport numpy as np\nimport torch\nfrom torch import nn, optim\nfrom torch import autograd\nimport torch.nn.functional as F\nfrom torch.nn import Parameter\nfrom torchvision import datasets, transforms\nfrom torchvision.utils import save_image\nfrom torch.utils.data import Dataset,DataLoader,Subset\nfrom PIL import Image,ImageOps,ImageEnhance\n\nimport cv2\nimport albumentations as A\nfrom albumentations.pytorch import ToTensor\n\nimport glob\nimport xml.etree.ElementTree as ET #for parsing XML\nimport shutil\nfrom tqdm import tqdm\nimport time\nimport random\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TIME_LIMIT = 32400 - 60*10\nstart_time = time.time()\ndef elapsed_time(start_time):\n    return time.time() - start_time","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#random seeds\nseed = 2019\nrandom.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\nnp.random.seed(seed)\nos.environ['PYTHONHASHSEED'] = str(seed)\n\nHINGE_LOSS  = False\nBATCH_SIZE  = 32\nNUM_WORKERS = 4\nEMA = False\nLABEL_NOISE = False #True\nLABEL_NOISE_PROB = 0.01\n\nZ_DIM     = 60 #120\nCODES_DIM = 10 #20\n\nLR_G = 4*3e-4\nLR_D = 4*3e-4\nMILESTONES = [30,60] #None\nSCHEDULER_GAMMA = 0.5\n\nDUPLICATES = False #True\n\nSHARPEN = True\nSHARPEN_MAGNITUDE = 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pre-Processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"PATH = '../input/all-dogs/all-dogs/'\nimg_filenames = os.listdir(PATH)\nlen(img_filenames)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PATH_ANNOTATION = '../input/annotation/Annotation/'\nbreeds = glob.glob(PATH_ANNOTATION+'*')\nannotations = []\nfor breed in breeds:\n    annotations += glob.glob(breed+'/*')\nlen(annotations)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"breed_map = {}\nfor annotation in annotations:\n    breed = annotation.split('/')[-2]\n    index = breed.split('-')[0]\n    breed_map.setdefault(index,breed)\nn_classes = len(breed_map)\nn_classes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"breed_map","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://www.kaggle.com/whizzkid/crop-images-using-bounding-box\ndef bounding_box(img):\n    bpath = PATH_ANNOTATION + str(breed_map[img.split('_')[0]])+'/'+str(img.split('.')[0])\n    tree  = ET.parse(bpath)\n    root  = tree.getroot()\n    objects = root.findall('object')\n    bbxs = []\n    for o in objects:\n        bndbox = o.find('bndbox') #reading bound box\n        xmin = int(bndbox.find('xmin').text)\n        ymin = int(bndbox.find('ymin').text)\n        xmax = int(bndbox.find('xmax').text)\n        ymax = int(bndbox.find('ymax').text)\n        bbxs.append((xmin,ymin,xmax,ymax))\n    return bbxs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def bounding_box_ratio(img):\n    bpath = PATH_ANNOTATION + str(breed_map[img.split('_')[0]])+'/'+str(img.split('.')[0])\n    tree  = ET.parse(bpath)\n    root  = tree.getroot()\n    objects = root.findall('object')\n    bbx_ratios = []\n    for o in objects:\n        bndbox = o.find('bndbox') #reading bound box\n        xmin = int(bndbox.find('xmin').text)\n        ymin = int(bndbox.find('ymin').text)\n        xmax = int(bndbox.find('xmax').text)\n        ymax = int(bndbox.find('ymax').text)\n        xlen = xmax - xmin\n        ylen = ymax - ymin\n        ratio = ylen / xlen\n        bbx_ratios.append((xlen,ylen,ratio))\n    return bbx_ratios","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n#threshold for aspect ratio, at the same time idx for each bbx\nimg_filenames_th = []\nratios_th = []\nfor img in tqdm(img_filenames):\n    bbx_ratios = bounding_box_ratio(img)\n    for i,(xlen,ylen,ratio) in enumerate(bbx_ratios):\n        if ((ratio>0.2)&(ratio<4.0)):\n            img_filenames_th.append(img[:-4]+'_'+str(i)+'.jpg')\n            ratios_th.append(ratio)\nratios_th = np.array(ratios_th)\n\nprint('original : ', len(img_filenames))\nprint('after th : ', len(img_filenames_th))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nif DUPLICATES:\n    #threshold for aspect ratio, at the same time idx for each bbx\n    img_filenames_th2 = []\n    ratios_th2 = []\n    for img in tqdm(img_filenames):\n        bbx_ratios = bounding_box_ratio(img)\n        for i,(xlen,ylen,ratio) in enumerate(bbx_ratios):\n            if ((ratio>0.8)&(ratio<1.2)):\n                img_filenames_th2.append(img[:-4]+'_'+str(i)+'.jpg')\n                ratios_th2.append(ratio)\n    ratios_th2 = np.array(ratios_th2)\n\n    print('original : ', len(img_filenames))\n    print('after th : ', len(img_filenames_th2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from https://www.kaggle.com/korovai/dogs-images-intruders-extraction\nintruders = [\n    #n02088238-basset\n    'n02088238_10870_0.jpg',\n    \n    #n02088466-bloodhound\n    'n02088466_6901_1.jpg',\n    'n02088466_6963_0.jpg',\n    'n02088466_9167_0.jpg',\n    'n02088466_9167_1.jpg',\n    'n02088466_9167_2.jpg',\n    \n    #n02089867-Walker_hound\n    'n02089867_2221_0.jpg',\n    'n02089867_2227_1.jpg',\n    \n    #n02089973-English_foxhound # No details\n    'n02089973_1132_3.jpg',\n    'n02089973_1352_3.jpg',\n    'n02089973_1458_1.jpg',\n    'n02089973_1799_2.jpg',\n    'n02089973_2791_3.jpg',\n    'n02089973_4055_0.jpg',\n    'n02089973_4185_1.jpg',\n    'n02089973_4185_2.jpg',\n    \n    #n02090379-redbone\n    'n02090379_4673_1.jpg',\n    'n02090379_4875_1.jpg',\n    \n    #n02090622-borzoi # Confusing\n    'n02090622_7705_1.jpg',\n    'n02090622_9358_1.jpg',\n    'n02090622_9883_1.jpg',\n    \n    #n02090721-Irish_wolfhound # very small\n    'n02090721_209_1.jpg',\n    'n02090721_1222_1.jpg',\n    'n02090721_1534_1.jpg',\n    'n02090721_1835_1.jpg',\n    'n02090721_3999_1.jpg',\n    'n02090721_4089_1.jpg',\n    'n02090721_4276_2.jpg',\n    \n    #n02091032-Italian_greyhound\n    'n02091032_722_1.jpg',\n    'n02091032_745_1.jpg',\n    'n02091032_1773_0.jpg',\n    'n02091032_9592_0.jpg',\n    \n    #n02091134-whippet\n    'n02091134_2349_1.jpg',\n    'n02091134_14246_2.jpg',\n    \n    #n02091244-Ibizan_hound\n    'n02091244_583_1.jpg',\n    'n02091244_2407_0.jpg',\n    'n02091244_3438_1.jpg',\n    'n02091244_5639_1.jpg',\n    'n02091244_5639_2.jpg',\n    \n    #n02091467-Norwegian_elkhound\n    'n02091467_473_0.jpg',\n    'n02091467_4386_1.jpg',\n    'n02091467_4427_1.jpg',\n    'n02091467_4558_1.jpg',\n    'n02091467_4560_1.jpg',\n    \n    #n02091635-otterhound\n    'n02091635_1192_1.jpg',\n    'n02091635_4422_0.jpg',\n    \n    #n02091831-Saluki\n    'n02091831_1594_1.jpg',\n    'n02091831_2880_0.jpg',\n    'n02091831_7237_1.jpg',\n    \n    #n02092002-Scottish_deerhound\n    'n02092002_1551_1.jpg',\n    'n02092002_1937_1.jpg',\n    'n02092002_4218_0.jpg',\n    'n02092002_4596_0.jpg',\n    'n02092002_5246_1.jpg',\n    'n02092002_6518_0.jpg',\n    \n    #02093256-Staffordshire_bullterrier\n    'n02093256_1826_1.jpg',\n    'n02093256_4997_0.jpg',\n    'n02093256_14914_0.jpg',\n    \n    #n02093428-American_Staffordshire_terrier\n    'n02093428_5662_0.jpg',\n    'n02093428_6949_1.jpg'\n            ]\n\nlen(intruders)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def autocontrast(img, cutoff=1): #cutoff[%]\n    if np.random.rand() < 0.5:\n        img = ImageOps.autocontrast(img, cutoff)\n    return img\n\ndef sharpen(img, magnitude=1):\n    factor = np.random.uniform(1.0-magnitude, 1.0+magnitude)\n    img    = ImageEnhance.Sharpness(img).enhance(factor)\n    return img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def data_preprocessing(img_path,bbx_idx):\n    bbx = bounding_box(img_path)[bbx_idx]\n    img  = Image.open(os.path.join(PATH,img_path))#PILImage format\n    img_cropped  = img.crop(bbx)\n    return img_cropped","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nbreed_map_2 = {}\nfor i,b in enumerate(breed_map.keys()):\n    breed_map_2[b] = i","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DogDataset(Dataset):\n    def __init__(self, path, img_list, transform1=None, transform2=None):\n        self.path      = path\n        self.img_list  = img_list\n        self.transform1 = transform1\n        self.transform2 = transform2\n        \n        self.imgs   = []\n        self.labels = []\n        for i,full_img_path in enumerate(self.img_list):\n            if full_img_path in intruders:\n                continue\n            #img\n            img_path = full_img_path[:-6]+'.jpg'\n            bbx_idx  = int(full_img_path[-5])\n            img = data_preprocessing(img_path,bbx_idx)\n            if self.transform1:\n                img = self.transform1(img) #output shape=(ch,h,w)\n            self.imgs.append(img)\n            #label\n            label = breed_map_2[img_path.split('_')[0]]\n            self.labels.append(label)\n            \n    def __len__(self):\n        return len(self.imgs)\n    \n    def __getitem__(self,idx):\n        img = self.imgs[idx]\n        if self.transform2:\n            img = self.transform2(img)\n        label = self.labels[idx]\n        return {'img':img, 'label':label}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# generate 64x64 images!\n#resize_size = 84\nimg_size    = 64\nbatch_size  = BATCH_SIZE\nMEAN1,MEAN2,MEAN3 = 0.5, 0.5, 0.5\nSTD1,STD2,STD3    = 0.5, 0.5, 0.5\n\ntransform1 = transforms.Compose([transforms.Resize(img_size)])\n#transform1 = transforms.Compose([transforms.Resize((resize_size,resize_size))])\n\ntransform2 = transforms.Compose([transforms.RandomCrop(img_size),\n                                 transforms.RandomHorizontalFlip(p=0.5),\n                                 transforms.ToTensor(),\n                                 transforms.Normalize(mean=[MEAN1, MEAN2, MEAN3],\n                                                      std=[STD1, STD2, STD3]),\n                                ])\n\nif DUPLICATES:\n    img_list = img_filenames_th+img_filenames_th2\nelse:\n    img_list = img_filenames_th\n\ntrain_set = DogDataset(path=PATH,\n                       img_list=img_list,\n                       transform1=transform1,\n                       transform2=transform2,\n                      )\n\ntrain_loader = DataLoader(train_set,\n                          shuffle=True, batch_size=batch_size,\n                          num_workers=NUM_WORKERS, pin_memory=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_set)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img = data_preprocessing(img_filenames_th[1500][:-6]+'.jpg',0)\nimg = transform1(img)\nimg","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Device"},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def conv3x3(in_channel, out_channel): #not change resolusion\n    return nn.Conv2d(in_channel,out_channel,\n                      kernel_size=3,stride=1,padding=1,dilation=1,bias=False)\n\ndef conv1x1(in_channel, out_channel): #not change resolution\n    return nn.Conv2d(in_channel,out_channel,\n                      kernel_size=1,stride=1,padding=0,dilation=1,bias=False)\n\ndef init_weight(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.orthogonal_(m.weight, gain=1)\n        if m.bias is not None:\n            m.bias.data.zero_()\n            \n    elif classname.find('Batch') != -1:\n        m.weight.data.normal_(1,0.02)\n        m.bias.data.zero_()\n    \n    elif classname.find('Linear') != -1:\n        nn.init.orthogonal_(m.weight, gain=1)\n        if m.bias is not None:\n            m.bias.data.zero_()\n    \n    elif classname.find('Embedding') != -1:\n        nn.init.orthogonal_(m.weight, gain=1)\n        \n\nclass cSEBlock(nn.Module):\n    def __init__(self, c, feat):\n        super().__init__()\n        self.attention_fc = nn.Linear(feat,1, bias=False)\n        self.bias         = nn.Parameter(torch.zeros((1,c,1), requires_grad=True))\n        self.sigmoid      = nn.Sigmoid()\n        self.dropout      = nn.Dropout2d(0.1)\n        \n    def forward(self,inputs):\n        batch,c,h,w = inputs.size()\n        x = inputs.view(batch,c,-1)\n        x = self.attention_fc(x) + self.bias\n        x = x.view(batch,c,1,1)\n        x = self.sigmoid(x)\n        x = self.dropout(x)\n        return inputs * x\n\nclass sSEBlock(nn.Module):\n    def __init__(self, c, h, w):\n        super().__init__()\n        self.attention_fc = nn.Linear(c,1, bias=False).apply(init_weight)\n        self.bias         = nn.Parameter(torch.zeros((1,h,w,1), requires_grad=True))\n        self.sigmoid      = nn.Sigmoid()\n        \n    def forward(self,inputs):\n        batch,c,h,w = inputs.size()\n        x = torch.transpose(inputs, 1,2) #(*,c,h,w)->(*,h,c,w)\n        x = torch.transpose(x, 2,3) #(*,h,c,w)->(*,h,w,c)\n        x = self.attention_fc(x) + self.bias\n        x = torch.transpose(x, 2,3) #(*,h,w,1)->(*,h,1,w)\n        x = torch.transpose(x, 1,2) #(*,h,1,w)->(*,1,h,w)\n        x = self.sigmoid(x)\n        return inputs * x\n    \n\nclass scSEBlock(nn.Module):\n    def __init__(self, c, h, w):\n        super().__init__()\n        self.cSE = cSEBlock(c,h*w)\n        self.sSE = sSEBlock(c,h,w)\n    \n    def forward(self, inputs):\n        x1 = self.cSE(inputs)\n        x2 = self.sSE(inputs)\n        return x1+x2\n    \n\nclass Attention(nn.Module):\n    def __init__(self, channels):\n        super().__init__()\n        self.channels = channels\n        self.theta    = nn.utils.spectral_norm(conv1x1(channels, channels//8)).apply(init_weight)\n        self.phi      = nn.utils.spectral_norm(conv1x1(channels, channels//8)).apply(init_weight)\n        self.g        = nn.utils.spectral_norm(conv1x1(channels, channels//2)).apply(init_weight)\n        self.o        = nn.utils.spectral_norm(conv1x1(channels//2, channels)).apply(init_weight)\n        self.gamma    = nn.Parameter(torch.tensor(0.), requires_grad=True)\n        \n    def forward(self, inputs):\n        batch,c,h,w = inputs.size()\n        theta = self.theta(inputs) #->(*,c/8,h,w)\n        phi   = F.max_pool2d(self.phi(inputs), [2,2]) #->(*,c/8,h/2,w/2)\n        g     = F.max_pool2d(self.g(inputs), [2,2]) #->(*,c/2,h/2,w/2)\n        \n        theta = theta.view(batch, self.channels//8, -1) #->(*,c/8,h*w)\n        phi   = phi.view(batch, self.channels//8, -1) #->(*,c/8,h*w/4)\n        g     = g.view(batch, self.channels//2, -1) #->(*,c/2,h*w/4)\n        \n        beta = F.softmax(torch.bmm(theta.transpose(1,2), phi), -1) #->(*,h*w,h*w/4)\n        o    = self.o(torch.bmm(g, beta.transpose(1,2)).view(batch,self.channels//2,h,w)) #->(*,c,h,w)\n        return self.gamma*o + inputs\n        \n    \nclass ConditionalNorm(nn.Module):\n    def __init__(self, in_channel, n_condition):\n        super().__init__()\n        self.bn = nn.BatchNorm2d(in_channel, affine=False) #no learning parameters\n        self.embed = nn.Linear(n_condition, in_channel* 2)\n        \n        nn.init.orthogonal_(self.embed.weight.data[:, :in_channel], gain=1)\n        self.embed.weight.data[:, in_channel:].zero_()\n\n    def forward(self, inputs, label):\n        out = self.bn(inputs)\n        embed = self.embed(label.float())\n        gamma, beta = embed.chunk(2, dim=1)\n        gamma = gamma.unsqueeze(2).unsqueeze(3)\n        beta = beta.unsqueeze(2).unsqueeze(3)\n        out = gamma * out + beta\n        return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #BigGAN + scSEBlock\n# class ResBlock_G(nn.Module):\n#     def __init__(self, in_channel, out_channel, condition_dim, upsample=True):\n#         super().__init__()\n#         self.cbn1 = ConditionalNorm(in_channel, condition_dim)\n#         self.upsample = nn.Sequential()\n#         if upsample:\n#             self.upsample.add_module('upsample',nn.Upsample(scale_factor=2, mode='nearest'))\n#         self.conv3x3_1 = nn.utils.spectral_norm(conv3x3(in_channel, out_channel)).apply(init_weight)\n#         self.cbn2 = ConditionalNorm(out_channel, condition_dim)\n#         self.conv3x3_2 = nn.utils.spectral_norm(conv3x3(out_channel, out_channel)).apply(init_weight) \n#         self.conv1x1   = nn.utils.spectral_norm(conv1x1(in_channel, out_channel)).apply(init_weight)\n        \n#     def forward(self, inputs, condition):\n#         x  = F.relu(self.cbn1(inputs, condition))\n#         x  = self.upsample(x)\n#         x  = self.conv3x3_1(x)\n#         x  = self.conv3x3_2(F.relu(self.cbn2(x, condition)))\n#         x += self.conv1x1(self.upsample(inputs)) #shortcut\n#         return x\n\n# class Generator(nn.Module):\n#     def __init__(self, n_feat, codes_dim=24, n_classes=n_classes):\n#         super().__init__()\n#         self.fc   = nn.Sequential(\n#             nn.utils.spectral_norm(nn.Linear(codes_dim, 16*n_feat*4*4)).apply(init_weight),\n#             #nn.BatchNorm1d(16*n_feat*4*4).apply(init_weight),\n#             #nn.ReLU(),\n#         )\n#         self.res1  = ResBlock_G(16*n_feat, 16*n_feat, codes_dim+n_classes, upsample=True)\n#         self.scse1 = scSEBlock(16*n_feat,8,8)\n#         self.res2  = ResBlock_G(16*n_feat,  8*n_feat, codes_dim+n_classes, upsample=True)\n#         self.scse2 = scSEBlock(8*n_feat,16,16)\n#         self.res3  = ResBlock_G( 8*n_feat,  4*n_feat, codes_dim+n_classes, upsample=True)\n#         self.scse3 = scSEBlock(4*n_feat,32,32)\n#         self.res4  = ResBlock_G( 4*n_feat,  2*n_feat, codes_dim+n_classes, upsample=True)\n#         self.conv  = nn.Sequential(\n#             nn.BatchNorm2d(2*n_feat).apply(init_weight),\n#             nn.ReLU(),\n#             nn.utils.spectral_norm(conv3x3(2*n_feat, 3)).apply(init_weight),\n#         )\n        \n#     def forward(self, z, label_ohe, codes_dim=24):\n#         '''\n#         z.shape = (*,120)\n#         label_ohe.shape = (*,n_classes)\n#         '''\n#         batch = z.size(0)\n#         z = z.squeeze()\n#         label_ohe = label_ohe.squeeze()\n#         codes = torch.split(z, codes_dim, dim=1)\n        \n#         x = self.fc(codes[0]) #->(*,16ch*4*4)\n#         x = x.view(batch,-1,4,4) #->(*,16ch,4,4)\n        \n#         condition = torch.cat([codes[1], label_ohe], dim=1) #(*,codes_dim+n_classes)\n#         x = self.res1(x, condition)#->(*,16ch,8,8)\n#         x = self.scse1(x) #not change shape\n        \n#         condition = torch.cat([codes[2], label_ohe], dim=1)\n#         x = self.res2(x, condition) #->(*,8ch,16,16)\n#         x = self.scse2(x) #not change shape\n        \n#         condition = torch.cat([codes[3], label_ohe], dim=1)\n#         x = self.res3(x, condition) #->(*,4ch,32,32)\n#         x = self.scse3(x) #not change shape\n        \n#         condition = torch.cat([codes[4], label_ohe], dim=1)\n#         x = self.res4(x, condition) #->(*,2ch,64,64)\n        \n#         x = self.conv(x) #->(*,3,64,64)\n#         x = torch.tanh(x)\n#         return x\n    \n\n# class ResBlock_D(nn.Module):\n#     def __init__(self, in_channel, out_channel, downsample=True):\n#         super().__init__()\n#         self.layer = nn.Sequential(\n#             nn.ReLU(),\n#             nn.utils.spectral_norm(conv3x3(in_channel, out_channel)).apply(init_weight),\n#             nn.ReLU(),\n#             nn.utils.spectral_norm(conv3x3(out_channel, out_channel)).apply(init_weight),\n#         )\n#         self.shortcut = nn.Sequential(\n#             nn.utils.spectral_norm(conv1x1(in_channel,out_channel)).apply(init_weight),\n#         )\n#         if downsample:\n#             self.layer.add_module('avgpool',nn.AvgPool2d(kernel_size=2,stride=2))\n#             self.shortcut.add_module('avgpool',nn.AvgPool2d(kernel_size=2,stride=2))\n        \n#     def forward(self, inputs):\n#         x  = self.layer(inputs)\n#         x += self.shortcut(inputs)\n#         return x\n    \n\n# class Discriminator(nn.Module):\n#     def __init__(self, n_feat, n_classes=n_classes):\n#         super().__init__()\n#         self.res1  = ResBlock_D(3, n_feat, downsample=True)\n#         self.scse1 = scSEBlock(n_feat,32,32)\n#         self.res2  = ResBlock_D(  n_feat, 2*n_feat, downsample=True)\n#         self.scse2 = scSEBlock(2*n_feat,16,16)\n#         self.res3  = ResBlock_D(2*n_feat, 4*n_feat, downsample=True)\n#         self.scse3 = scSEBlock(4*n_feat,8,8)\n#         self.res4  = ResBlock_D(4*n_feat, 8*n_feat, downsample=True)\n#         self.res5  = ResBlock_D(8*n_feat,16*n_feat, downsample=False)\n#         self.fc    = nn.utils.spectral_norm(nn.Linear(16*n_feat,1)).apply(init_weight)\n#         self.embedding = nn.Embedding(num_embeddings=n_classes, embedding_dim=16*n_feat).apply(init_weight)\n        \n#     def forward(self, inputs, label):\n#         batch = inputs.size(0) #(*,3,64,64)\n#         h = self.res1(inputs) #->(*,ch,32,32)\n#         h = self.scse1(h) #not change shape\n        \n#         h = self.res2(h) #->(*,2ch,16,16)\n#         h = self.scse2(h) #not change shape\n        \n#         h = self.res3(h) #->(*,4ch,8,8)\n#         h = self.scse3(h) #not change shape\n        \n#         h = self.res4(h) #->(*,8ch,4,4)\n#         h = self.res5(h) #->(*,16ch,4,4)\n#         h = torch.sum((F.relu(h)).view(batch,-1,4*4), dim=2) #GlobalSumPool ->(*,16ch)\n#         outputs = self.fc(h) #->(*,1)\n        \n#         if label is not None:\n#             embed = self.embedding(label) #->(*,16ch)\n#             outputs += torch.sum(embed*h,dim=1,keepdim=True) #->(*,1)\n        \n#         if not HINGE_LOSS:\n#             outputs = torch.sigmoid(outputs)\n            \n#         return outputs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#BigGAN + leaky_relu           \nclass ResBlock_G(nn.Module):\n    def __init__(self, in_channel, out_channel, condition_dim, upsample=True):\n        super().__init__()\n        self.cbn1 = ConditionalNorm(in_channel, condition_dim)\n        self.upsample = nn.Sequential()\n        if upsample:\n            self.upsample.add_module('upsample',nn.Upsample(scale_factor=2, mode='nearest'))\n        self.conv3x3_1 = nn.utils.spectral_norm(conv3x3(in_channel, out_channel)).apply(init_weight)\n        self.cbn2 = ConditionalNorm(out_channel, condition_dim)\n        self.conv3x3_2 = nn.utils.spectral_norm(conv3x3(out_channel, out_channel)).apply(init_weight) \n        self.conv1x1   = nn.utils.spectral_norm(conv1x1(in_channel, out_channel)).apply(init_weight)\n        \n    def forward(self, inputs, condition):\n        x  = F.leaky_relu(self.cbn1(inputs, condition))\n        x  = self.upsample(x)\n        x  = self.conv3x3_1(x)\n        x  = self.conv3x3_2(F.leaky_relu(self.cbn2(x, condition)))\n        x += self.conv1x1(self.upsample(inputs)) #shortcut\n        return x\n\nclass Generator(nn.Module):\n    def __init__(self, n_feat, codes_dim, n_classes=n_classes):\n        super().__init__()\n        self.codes_dim = codes_dim\n        self.fc   = nn.Sequential(\n            nn.utils.spectral_norm(nn.Linear(codes_dim, 16*n_feat*4*4)).apply(init_weight)\n        )\n        self.res1 = ResBlock_G(16*n_feat, 16*n_feat, codes_dim+n_classes, upsample=True)\n        self.res2 = ResBlock_G(16*n_feat,  8*n_feat, codes_dim+n_classes, upsample=True)\n        self.res3 = ResBlock_G( 8*n_feat,  4*n_feat, codes_dim+n_classes, upsample=True)\n        self.attn = Attention(4*n_feat)\n        self.res4 = ResBlock_G( 4*n_feat,  2*n_feat, codes_dim+n_classes, upsample=True)\n        self.conv = nn.Sequential(\n            #nn.BatchNorm2d(2*n_feat).apply(init_weight),\n            nn.LeakyReLU(),\n            nn.utils.spectral_norm(conv3x3(2*n_feat, 3)).apply(init_weight),\n        )\n        \n    def forward(self, z, label_ohe):\n        '''\n        z.shape = (*,Z_DIM)\n        label_ohe.shape = (*,n_classes)\n        '''\n        batch = z.size(0)\n        z = z.squeeze()\n        label_ohe = label_ohe.squeeze()\n        codes = torch.split(z, self.codes_dim, dim=1)\n        \n        x = self.fc(codes[0]) #->(*,16ch*4*4)\n        x = x.view(batch,-1,4,4) #->(*,16ch,4,4)\n        \n        condition = torch.cat([codes[1], label_ohe], dim=1) #(*,codes_dim+n_classes)\n        x = self.res1(x, condition)#->(*,16ch,8,8)\n        \n        condition = torch.cat([codes[2], label_ohe], dim=1)\n        x = self.res2(x, condition) #->(*,8ch,16,16)\n        \n        condition = torch.cat([codes[3], label_ohe], dim=1)\n        x = self.res3(x, condition) #->(*,4ch,32,32)\n        x = self.attn(x) #not change shape\n        \n        condition = torch.cat([codes[4], label_ohe], dim=1)\n        x = self.res4(x, condition) #->(*,2ch,64,64)\n        \n        x = self.conv(x) #->(*,3,64,64)\n        x = torch.tanh(x)\n        return x\n    \n\nclass ResBlock_D(nn.Module):\n    def __init__(self, in_channel, out_channel, downsample=True):\n        super().__init__()\n        self.layer = nn.Sequential(\n            nn.LeakyReLU(0.2),\n            nn.utils.spectral_norm(conv3x3(in_channel, out_channel)).apply(init_weight),\n            nn.LeakyReLU(0.2),\n            nn.utils.spectral_norm(conv3x3(out_channel, out_channel)).apply(init_weight),\n        )\n        self.shortcut = nn.Sequential(\n            nn.utils.spectral_norm(conv1x1(in_channel,out_channel)).apply(init_weight),\n        )\n        if downsample:\n            self.layer.add_module('avgpool',nn.AvgPool2d(kernel_size=2,stride=2))\n            self.shortcut.add_module('avgpool',nn.AvgPool2d(kernel_size=2,stride=2))\n        \n    def forward(self, inputs):\n        x  = self.layer(inputs)\n        x += self.shortcut(inputs)\n        return x\n    \n\nclass Discriminator(nn.Module):\n    def __init__(self, n_feat, n_classes=n_classes):\n        super().__init__()\n        self.res1 = ResBlock_D(3, n_feat, downsample=True)\n        self.attn = Attention(n_feat)\n        self.res2 = ResBlock_D(  n_feat, 2*n_feat, downsample=True)\n        self.res3 = ResBlock_D(2*n_feat, 4*n_feat, downsample=True)\n        self.res4 = ResBlock_D(4*n_feat, 8*n_feat, downsample=True)\n        self.res5 = ResBlock_D(8*n_feat,16*n_feat, downsample=False)\n        self.fc   = nn.utils.spectral_norm(nn.Linear(16*n_feat,1)).apply(init_weight)\n        self.embedding = nn.Embedding(num_embeddings=n_classes, embedding_dim=16*n_feat).apply(init_weight)\n        \n    def forward(self, inputs, label):\n        batch = inputs.size(0) #(*,3,64,64)\n        h = self.res1(inputs) #->(*,ch,32,32)\n        h = self.attn(h) #not change shape\n        h = self.res2(h) #->(*,2ch,16,16)\n        h = self.res3(h) #->(*,4ch,8,8)\n        h = self.res4(h) #->(*,8ch,4,4)\n        h = self.res5(h) #->(*,16ch,4,4)\n        h = torch.sum((F.leaky_relu(h,0.2)).view(batch,-1,4*4), dim=2) #GlobalSumPool ->(*,16ch)\n        outputs = self.fc(h) #->(*,1)\n        \n        if label is not None:\n            embed = self.embedding(label) #->(*,16ch)\n            outputs += torch.sum(embed*h,dim=1,keepdim=True) #->(*,1)\n        \n        if not HINGE_LOSS:\n            outputs = torch.sigmoid(outputs)\n            \n        return outputs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #BigGAN + leaky_relu + res concat     \n# class ResBlock_G(nn.Module):\n#     def __init__(self, in_channel, out_channel, condition_dim, upsample=True):\n#         super().__init__()\n#         self.cbn1 = ConditionalNorm(in_channel, condition_dim)\n#         self.upsample = nn.Sequential()\n#         if upsample:\n#             self.upsample.add_module('upsample',nn.Upsample(scale_factor=2, mode='nearest'))\n#         self.conv3x3_1 = nn.utils.spectral_norm(conv3x3(in_channel, out_channel)).apply(init_weight)\n#         self.cbn2 = ConditionalNorm(out_channel, condition_dim)\n#         self.conv3x3_2 = nn.utils.spectral_norm(conv3x3(out_channel, out_channel)).apply(init_weight) \n#         self.conv1x1   = nn.utils.spectral_norm(conv1x1(in_channel, out_channel)).apply(init_weight)\n        \n#     def forward(self, inputs, condition):\n#         x  = F.leaky_relu(self.cbn1(inputs, condition))\n#         x  = self.upsample(x)\n#         x  = self.conv3x3_1(x)\n#         x  = self.conv3x3_2(F.leaky_relu(self.cbn2(x, condition)))\n#         x1 = self.conv1x1(self.upsample(inputs)) #shortcut\n#         x  = torch.cat([x,x1], dim=1)\n#         return x\n\n# class Generator(nn.Module):\n#     def __init__(self, n_feat, codes_dim, n_classes=n_classes):\n#         super().__init__()\n#         self.codes_dim = codes_dim\n#         self.fc   = nn.Sequential(\n#             nn.utils.spectral_norm(nn.Linear(codes_dim, 16*n_feat*4*4)).apply(init_weight)\n#         )\n#         self.res1 = ResBlock_G(16*n_feat,  8*n_feat, codes_dim+n_classes, upsample=True)\n#         self.res2 = ResBlock_G(16*n_feat,  4*n_feat, codes_dim+n_classes, upsample=True)\n#         self.res3 = ResBlock_G( 8*n_feat,  2*n_feat, codes_dim+n_classes, upsample=True)\n#         self.attn = Attention(4*n_feat)\n#         self.res4 = ResBlock_G( 4*n_feat,  n_feat, codes_dim+n_classes, upsample=True)\n#         self.conv = nn.Sequential(\n#             nn.BatchNorm2d(2*n_feat).apply(init_weight),\n#             nn.LeakyReLU(),\n#             nn.utils.spectral_norm(conv3x3(2*n_feat, 3)).apply(init_weight),\n#         )\n        \n#     def forward(self, z, label_ohe):\n#         '''\n#         z.shape = (*,Z_DIM)\n#         label_ohe.shape = (*,n_classes)\n#         '''\n#         batch = z.size(0)\n#         z = z.squeeze()\n#         label_ohe = label_ohe.squeeze()\n#         codes = torch.split(z, self.codes_dim, dim=1)\n        \n#         x = self.fc(codes[0]) #->(*,16ch*4*4)\n#         x = x.view(batch,-1,4,4) #->(*,16ch,4,4)\n        \n#         condition = torch.cat([codes[1], label_ohe], dim=1) #(*,codes_dim+n_classes)\n#         x = self.res1(x, condition)#->(*,16ch,8,8)\n        \n#         condition = torch.cat([codes[2], label_ohe], dim=1)\n#         x = self.res2(x, condition) #->(*,8ch,16,16)\n        \n#         condition = torch.cat([codes[3], label_ohe], dim=1)\n#         x = self.res3(x, condition) #->(*,4ch,32,32)\n#         x = self.attn(x) #not change shape\n        \n#         condition = torch.cat([codes[4], label_ohe], dim=1)\n#         x = self.res4(x, condition) #->(*,2ch,64,64)\n        \n#         x = self.conv(x) #->(*,3,64,64)\n#         x = torch.tanh(x)\n#         return x\n    \n\n# class ResBlock_D(nn.Module):\n#     def __init__(self, in_channel, out_channel, downsample=True):\n#         super().__init__()\n#         self.layer = nn.Sequential(\n#             nn.LeakyReLU(0.2),\n#             nn.utils.spectral_norm(conv3x3(in_channel, out_channel)).apply(init_weight),\n#             nn.LeakyReLU(0.2),\n#             nn.utils.spectral_norm(conv3x3(out_channel, out_channel)).apply(init_weight),\n#         )\n#         self.shortcut = nn.Sequential(\n#             nn.utils.spectral_norm(conv1x1(in_channel,out_channel)).apply(init_weight),\n#         )\n#         if downsample:\n#             self.layer.add_module('avgpool',nn.AvgPool2d(kernel_size=2,stride=2))\n#             self.shortcut.add_module('avgpool',nn.AvgPool2d(kernel_size=2,stride=2))\n        \n#     def forward(self, inputs):\n#         x  = self.layer(inputs)\n#         x1 = self.shortcut(inputs)\n#         x  = torch.cat([x,x1], dim=1)\n#         return x\n    \n\n# class Discriminator(nn.Module):\n#     def __init__(self, n_feat, n_classes=n_classes):\n#         super().__init__()\n#         self.sse  = sSEBlock(3,64,64)\n#         self.res1 = ResBlock_D(3, n_feat, downsample=True)\n#         self.attn = Attention(2*n_feat)\n#         self.res2 = ResBlock_D(2*n_feat,   n_feat, downsample=True)\n#         self.res3 = ResBlock_D(2*n_feat, 2*n_feat, downsample=True)\n#         self.res4 = ResBlock_D(4*n_feat, 4*n_feat, downsample=True)\n#         self.res5 = ResBlock_D(8*n_feat, 8*n_feat, downsample=False)\n#         self.fc   = nn.utils.spectral_norm(nn.Linear(16*n_feat,1)).apply(init_weight)\n#         self.embedding = nn.Embedding(num_embeddings=n_classes, embedding_dim=16*n_feat).apply(init_weight)\n        \n#     def forward(self, inputs, label):\n#         batch = inputs.size(0) #(*,3,64,64)\n#         h = self.res1(inputs) #->(*,ch,32,32)\n#         h = self.attn(h) #not change shape\n#         h = self.res2(h) #->(*,2ch,16,16)\n#         h = self.res3(h) #->(*,4ch,8,8)\n#         h = self.res4(h) #->(*,8ch,4,4)\n#         h = self.res5(h) #->(*,16ch,4,4)\n#         h = torch.sum((F.leaky_relu(h,0.2)).view(batch,-1,4*4), dim=2) #GlobalSumPool ->(*,16ch)\n#         outputs = self.fc(h) #->(*,1)\n        \n#         if label is not None:\n#             embed = self.embedding(label) #->(*,16ch)\n#             outputs += torch.sum(embed*h,dim=1,keepdim=True) #->(*,1)\n        \n#         if not HINGE_LOSS:\n#             outputs = torch.sigmoid(outputs)\n            \n#         return outputs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #BigGAN + leaky_relu + DenseBlock    \n# class DenseBlock_G(nn.Module):\n#     def __init__(self, in_channel, med_channel=128, growth_rate=32,\n#                  condition_dim=None, upsample=True):\n#         super().__init__()\n#         self.cbn1    = ConditionalNorm(in_channel, condition_dim)\n#         self.conv1x1 = nn.utils.spectral_norm(conv1x1(in_channel, med_channel)).apply(init_weight)\n#         self.cbn2    = ConditionalNorm(med_channel, condition_dim)\n#         self.conv3x3 = nn.utils.spectral_norm(conv3x3(med_channel, growth_rate)).apply(init_weight)\n        \n#     def forward(self, inputs, condition):\n#         x = self.conv1x1(F.leaky_relu(self.cbn1(inputs, condition)))\n#         x = self.conv3x3(F.leaky_relu(self.cbn2(x, condition)))\n#         x = torch.cat([inputs, x], dim=1)\n#         return x\n    \n    \n# class Transition_G(nn.Module):\n#     def __init__(self, in_channel, out_channel):\n#         super().__init__()\n#         self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n#         self.conv1x1  = nn.utils.spectral_norm(conv1x1(in_channel, out_channel)).apply(init_weight)\n        \n#     def forward(self, inputs):\n#         x = self.upsample(inputs)\n#         x = self.conv1x1(x)\n#         return x\n    \n\n# class Generator(nn.Module):\n#     def __init__(self, n_feat, codes_dim, n_classes=n_classes):\n#         super().__init__()\n#         self.codes_dim = codes_dim\n#         self.fc   = nn.Sequential(\n#             nn.utils.spectral_norm(nn.Linear(codes_dim, 16*n_feat*4*4)).apply(init_weight)\n#         )\n#         self.res1 = DenseBlock_G(16*n_feat,128,32, codes_dim+n_classes)\n#         self.res1_2 = DenseBlock_G(16*n_feat+32,128,32, codes_dim+n_classes)\n#         self.tra1 = Transition_G(16*n_feat+64,8*n_feat)\n#         self.res2 = DenseBlock_G(8*n_feat,128,32, codes_dim+n_classes)\n#         self.res2_2 = DenseBlock_G(8*n_feat+32,128,32, codes_dim+n_classes)\n#         self.tra2 = Transition_G(8*n_feat+64,4*n_feat)\n#         self.res3 = DenseBlock_G(4*n_feat,128,32, codes_dim+n_classes)\n#         self.res3_2 = DenseBlock_G(4*n_feat+32,128,32, codes_dim+n_classes)\n#         self.tra3 = Transition_G(4*n_feat+64,2*n_feat)\n#         self.attn = Attention(2*n_feat)\n#         self.res4 = DenseBlock_G(2*n_feat,128,32, codes_dim+n_classes)\n#         self.res4_2 = DenseBlock_G(2*n_feat+32,128,32, codes_dim+n_classes)\n#         self.tra4 = Transition_G(2*n_feat+64,n_feat)\n#         self.conv = nn.Sequential(\n#             nn.BatchNorm2d(n_feat).apply(init_weight),\n#             nn.LeakyReLU(),\n#             nn.utils.spectral_norm(conv3x3(n_feat, 3)).apply(init_weight),\n#         )\n        \n#     def forward(self, z, label_ohe):\n#         '''\n#         z.shape = (*,Z_DIM)\n#         label_ohe.shape = (*,n_classes)\n#         '''\n#         batch = z.size(0)\n#         z = z.squeeze()\n#         label_ohe = label_ohe.squeeze()\n#         codes = torch.split(z, self.codes_dim, dim=1)\n        \n#         x = self.fc(codes[0]) #->(*,16ch*4*4)\n#         x = x.view(batch,-1,4,4) #->(*,16ch,4,4)\n        \n#         condition = torch.cat([codes[1], label_ohe], dim=1) #(*,codes_dim+n_classes)\n#         x = self.res1(x, condition)\n#         x = self.res1_2(x, condition)\n#         x = self.tra1(x)#->(*,16ch,8,8)\n        \n#         condition = torch.cat([codes[2], label_ohe], dim=1)\n#         x = self.res2(x, condition)\n#         x = self.res2_2(x, condition)\n#         x = self.tra2(x) #->(*,8ch,16,16)\n        \n#         condition = torch.cat([codes[3], label_ohe], dim=1)\n#         x = self.res3(x, condition)\n#         x = self.res3_2(x, condition)\n#         x = self.tra3(x) #->(*,4ch,32,32)\n#         x = self.attn(x) #not change shape\n        \n#         condition = torch.cat([codes[4], label_ohe], dim=1)\n#         x = self.res4(x, condition)\n#         x = self.res4_2(x, condition)\n#         x = self.tra4(x) #->(*,2ch,64,64)\n        \n#         x = self.conv(x) #->(*,3,64,64)\n#         x = torch.tanh(x)\n#         return x\n    \n    \n    \n# class DenseBlock_D(nn.Module):\n#     def __init__(self, in_channel, med_channel=128, growth_rate=32,\n#                  downsample=True):\n#         super().__init__()\n#         self.conv1x1 = nn.utils.spectral_norm(conv1x1(in_channel, med_channel)).apply(init_weight)\n#         self.conv3x3 = nn.utils.spectral_norm(conv3x3(med_channel, growth_rate)).apply(init_weight)\n        \n#     def forward(self, inputs):\n#         x = self.conv1x1(F.leaky_relu(inputs, 0.2))\n#         x = self.conv3x3(F.leaky_relu(x, 0.2))\n#         x = torch.cat([inputs, x], dim=1)\n#         return x\n    \n    \n# class Transition_D(nn.Module):\n#     def __init__(self, in_channel, out_channel):\n#         super().__init__()\n#         self.conv1x1    = nn.utils.spectral_norm(conv1x1(in_channel, out_channel)).apply(init_weight)\n#         self.downsample = nn.AvgPool2d(kernel_size=2,stride=2)\n        \n#     def forward(self, inputs):\n#         x = self.conv1x1(inputs)\n#         x = self.downsample(x)\n#         return x\n    \n    \n# class Discriminator(nn.Module):\n#     def __init__(self, n_feat, n_classes=n_classes):\n#         super().__init__()\n#         self.res1 = DenseBlock_D(3,128,n_feat)\n#         self.res1_2 = DenseBlock_D(3+n_feat,228,32)\n#         self.tra1 = Transition_D(3+n_feat+32,2*n_feat)\n#         self.attn = Attention(2*n_feat)\n#         self.res2 = DenseBlock_D(2*n_feat,128,32)\n#         self.res2_2 = DenseBlock_D(2*n_feat+32,128,32)\n#         self.tra2 = Transition_D(2*n_feat+64,4*n_feat)\n#         self.res3 = DenseBlock_D(4*n_feat,128,32)\n#         self.res3_2 = DenseBlock_D(4*n_feat+32,128,32)\n#         self.tra3 = Transition_D(4*n_feat+64,8*n_feat)\n#         self.res4 = DenseBlock_D(8*n_feat,128,32)\n#         self.res4_2 = DenseBlock_D(8*n_feat+32,128,32)\n#         self.tra4 = Transition_D(8*n_feat+64,16*n_feat)\n#         self.res5 = DenseBlock_D(16*n_feat,128,32)\n#         self.fc   = nn.utils.spectral_norm(nn.Linear(16*n_feat+32,1)).apply(init_weight)\n#         self.embedding = nn.Embedding(num_embeddings=n_classes, embedding_dim=16*n_feat+32).apply(init_weight)\n        \n#     def forward(self, inputs, label):\n#         batch = inputs.size(0) #(*,3,64,64)\n#         h = self.tra1(self.res1_2(self.res1(inputs))) #->(*,ch,32,32)\n#         h = self.attn(h) #not change shape\n#         h = self.tra2(self.res2_2(self.res2(h))) #->(*,2ch,16,16)\n#         h = self.tra3(self.res3_2(self.res3(h))) #->(*,4ch,8,8)\n#         h = self.tra4(self.res4_2(self.res4(h))) #->(*,8ch,4,4)\n#         h = self.res5(h) #->(*,16ch,4,4)\n#         h = torch.sum((F.leaky_relu(h,0.2)).view(batch,-1,4*4), dim=2) #GlobalSumPool ->(*,16ch)\n#         outputs = self.fc(h) #->(*,1)\n        \n#         if label is not None:\n#             embed = self.embedding(label) #->(*,16ch)\n#             outputs += torch.sum(embed*h,dim=1,keepdim=True) #->(*,1)\n        \n#         if not HINGE_LOSS:\n#             outputs = torch.sigmoid(outputs)\n            \n#         return outputs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #BigGAN\n# print(count_parameters(model=Generator(n_feat=96, codes_dim=CODES_DIM, n_classes=n_classes))) #z.shape=(*,120)\n# print(count_parameters(model=Discriminator(n_feat=120, n_classes=n_classes)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_img(netG,fixed_noise,fixed_aux_labels=None):\n    if fixed_aux_labels is not None:\n        gen_image = netG(fixed_noise,fixed_aux_labels).to('cpu').clone().detach().squeeze(0)\n    else:\n        gen_image = netG(fixed_noise).to('cpu').clone().detach().squeeze(0)\n    #denormalize\n    gen_image = gen_image*0.5 + 0.5\n    gen_image_numpy = gen_image.numpy().transpose(0,2,3,1)\n    return gen_image_numpy\n\ndef show_generate_imgs(netG,fixed_noise,fixed_aux_labels=None):\n    gen_images_numpy = generate_img(netG,fixed_noise,fixed_aux_labels)\n\n    fig = plt.figure(figsize=(25, 16))\n    # display 10 images from each class\n    for i, img in enumerate(gen_images_numpy):\n        ax = fig.add_subplot(4, 8, i + 1, xticks=[], yticks=[])\n        plt.imshow(img)\n    plt.show()\n    plt.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://www.kaggle.com/osciiart/resnet34-mel-ver3-log-multi-hardaug?scriptVersionId=13887036\ndef cycle(iterable):\n    \"\"\"\n    dataloaderiterator\n    :param iterable:\n    :return:\n    \"\"\"\n    while True:\n        for x in iterable:\n            yield x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#BigGAN\ndef run(lr_G=3e-4,lr_D=3e-4, beta1=0.0, beta2=0.999, nz=120, epochs=2, \n        n_ite_D=1, ema_decay_rate=0.999, show_epoch_list=None, output_freq=10):\n#     #G:4M, D:3M params\n#     netG = Generator(n_feat=22, codes_dim=24, n_classes=n_classes).to(device) #z.shape=(*,120)\n#     netD = Discriminator(n_feat=26, n_classes=n_classes).to(device)\n    #G:6M, D:5M params\n    netG = Generator(n_feat=27, codes_dim=24, n_classes=n_classes).to(device) #z.shape=(*,120)\n    netD = Discriminator(n_feat=33, n_classes=n_classes).to(device)\n#     #G:8M, D:7M params\n#     netG = Generator(n_feat=32, codes_dim=CODES_DIM, n_classes=n_classes).to(device) #z.shape=(*,120)\n#     netD = Discriminator(n_feat=38, n_classes=n_classes).to(device)\n#     #G:10M, D:8M params\n#     netG = Generator(n_feat=36, codes_dim=CODES_DIM, n_classes=n_classes).to(device) #z.shape=(*,120)\n#     netD = Discriminator(n_feat=42, n_classes=n_classes).to(device)\n#     #G:10M, D:10M params\n#     netG = Generator(n_feat=36, codes_dim=24, n_classes=n_classes).to(device) #z.shape=(*,120)\n#     netD = Discriminator(n_feat=46, n_classes=n_classes).to(device)\n#     #G:14M, D:12M params\n#     netG = Generator(n_feat=42, codes_dim=24, n_classes=n_classes).to(device) #z.shape=(*,120)\n#     netD = Discriminator(n_feat=50, n_classes=n_classes).to(device)\n#     #G:25M, D:22M params\n#     netG = Generator(n_feat=56, codes_dim=24, n_classes=n_classes).to(device) #z.shape=(*,120)\n#     netD = Discriminator(n_feat=68, n_classes=n_classes).to(device)\n#     #G:32M, D:31M params\n#     netG = Generator(n_feat=64, codes_dim=24, n_classes=n_classes).to(device) #z.shape=(*,120)\n#     netD = Discriminator(n_feat=80, n_classes=n_classes).to(device)\n\n    #+ res concat\n#     #G:8M, D:7M params\n#     netG = Generator(n_feat=50, codes_dim=CODES_DIM, n_classes=n_classes).to(device) #z.shape=(*,120)\n#     netD = Discriminator(n_feat=66, n_classes=n_classes).to(device)\n#     #G:7M, D:6M params\n#     netG = Generator(n_feat=46, codes_dim=CODES_DIM, n_classes=n_classes).to(device) #z.shape=(*,120)\n#     netD = Discriminator(n_feat=60, n_classes=n_classes).to(device)\n#     #G:6M, D:5M params\n#     netG = Generator(n_feat=43, codes_dim=CODES_DIM, n_classes=n_classes).to(device) #z.shape=(*,120)\n#     netD = Discriminator(n_feat=56, n_classes=n_classes).to(device)\n\n    # dense block\n#     #G:2.2M, D:1.6M params\n#     netG = Generator(n_feat=46, codes_dim=CODES_DIM, n_classes=n_classes).to(device) #z.shape=(*,120)\n#     netD = Discriminator(n_feat=60, n_classes=n_classes).to(device)\n#     #G:5M, D:4M params\n#     netG = Generator(n_feat=96, codes_dim=CODES_DIM, n_classes=n_classes).to(device) #z.shape=(*,120)\n#     netD = Discriminator(n_feat=120, n_classes=n_classes).to(device)\n\n    \n    if EMA:\n        #EMA of G for sampling\n        netG_EMA = Generator(n_feat=42, codes_dim=24, n_classes=n_classes).to(device)\n        netG_EMA.load_state_dict(netG.state_dict())\n        for p in netG_EMA.parameters():\n            p.requires_grad = False\n\n        \n    print(count_parameters(netG))\n    print(count_parameters(netD))\n    \n    real_label = 0.9\n    fake_label = 0\n    \n    D_loss_list = []\n    G_loss_list = []\n    \n    dis_criterion = nn.BCELoss().to(device)\n\n    optimizerD = optim.Adam(netD.parameters(), lr=lr_D, betas=(beta1, beta2))\n    optimizerG = optim.Adam(netG.parameters(), lr=lr_G, betas=(beta1, beta2))\n    \n    if MILESTONES is not None:\n        schedulerD = optim.lr_scheduler.MultiStepLR(optimizerD, milestones=MILESTONES, \n                                                    gamma=SCHEDULER_GAMMA, last_epoch=-1)\n        schedulerG = optim.lr_scheduler.MultiStepLR(optimizerG, milestones=MILESTONES, \n                                                    gamma=SCHEDULER_GAMMA, last_epoch=-1)\n    \n    fixed_noise = torch.randn(32, nz, 1, 1, device=device)\n    #fixed_noise = fixed_noise / fixed_noise.norm(dim=1, keepdim=True)\n    fixed_aux_labels     = np.random.randint(0,n_classes, 32)\n    fixed_aux_labels_ohe = np.eye(n_classes)[fixed_aux_labels]\n    fixed_aux_labels_ohe = torch.from_numpy(fixed_aux_labels_ohe[:,:,np.newaxis,np.newaxis])\n    fixed_aux_labels_ohe = fixed_aux_labels_ohe.float().to(device, non_blocking=True)\n\n    netG.train()\n    netD.train()\n\n    ### training here\n    for epoch in range(1,epochs+1):\n        if elapsed_time(start_time) > TIME_LIMIT:\n            print(f'elapsed_time go beyond {TIME_LIMIT} sec')\n            break\n        \n        if MILESTONES is not None:\n            print('lrG = ', schedulerG.get_lr()[0])\n            print('lrD = ', schedulerD.get_lr()[0])\n        \n        D_running_loss = 0\n        G_running_loss = 0\n        for ii, data in enumerate(train_loader):\n            ############################\n            # (1) Update D network\n            ###########################\n            for _ in range(n_ite_D):\n                \n                if LABEL_NOISE:\n                    real_label = 0.9\n                    fake_label = 0\n                    if np.random.random() < LABEL_NOISE_PROB:\n                        real_label = 0\n                        fake_label = 0.9\n                    \n                # train with real\n                netD.zero_grad()\n                real_images = data['img'].to(device, non_blocking=True) \n                batch_size  = real_images.size(0)\n                dis_labels  = torch.full((batch_size, 1), real_label, device=device) #shape=(*,)\n                aux_labels  = data['label'].long().to(device, non_blocking=True) #shape=(*,)\n                dis_output  = netD(real_images, aux_labels) #dis shape=(*,1)\n                if HINGE_LOSS:\n                    errD_real = torch.mean(F.relu(1-dis_output))\n                else:\n                    errD_real  = dis_criterion(dis_output, dis_labels)\n                errD_real.backward(retain_graph=True)\n\n                # train with fake\n                noise  = torch.randn(batch_size, nz, 1, 1, device=device)\n                #noise = noise / noise.norm(dim=1, keepdim=True)\n                aux_labels     = np.random.randint(0,n_classes, batch_size)\n                aux_labels_ohe = np.eye(n_classes)[aux_labels]\n                aux_labels_ohe = torch.from_numpy(aux_labels_ohe[:,:,np.newaxis,np.newaxis])\n                aux_labels_ohe = aux_labels_ohe.float().to(device, non_blocking=True)\n                aux_labels = torch.from_numpy(aux_labels).long().to(device, non_blocking=True)\n                \n                fake = netG(noise, aux_labels_ohe) #output shape=(*,3,64,64)\n                dis_labels.fill_(fake_label)\n                dis_output = netD(fake.detach(),aux_labels)\n                if HINGE_LOSS:\n                    errD_fake = torch.mean(F.relu(1+dis_output))\n                else:\n                    errD_fake  = dis_criterion(dis_output, dis_labels)\n                errD_fake.backward(retain_graph=True)\n                D_running_loss += (errD_real.item() + errD_fake.item())/len(train_loader)\n                optimizerD.step()\n\n            ############################\n            # (2) Update G network\n            ###########################\n            netG.zero_grad()\n            dis_labels.fill_(real_label)  # fake labels are real for generator cost\n            noise = torch.randn(batch_size, nz, 1, 1, device=device)\n            aux_labels     = np.random.randint(0,n_classes, batch_size)\n            aux_labels_ohe = np.eye(n_classes)[aux_labels]\n            aux_labels_ohe = torch.from_numpy(aux_labels_ohe[:,:,np.newaxis,np.newaxis])\n            aux_labels_ohe = aux_labels_ohe.float().to(device, non_blocking=True)\n            aux_labels = torch.from_numpy(aux_labels).long().to(device, non_blocking=True)\n            fake  = netG(noise, aux_labels_ohe)\n            \n            dis_output = netD(fake, aux_labels)\n            if HINGE_LOSS:\n                errG = - torch.mean(dis_output)\n            else:\n                errG   = dis_criterion(dis_output, dis_labels)\n            errG.backward(retain_graph=True)\n            G_running_loss += errG.item()/len(train_loader)\n            optimizerG.step()\n        \n        if MILESTONES is not None:\n            schedulerD.step()\n            schedulerG.step()\n        \n        if EMA:\n            #update netG_EMA\n            param_itr = cycle(netG.parameters())\n            for i,p_EMA in enumerate(netG_EMA.parameters()):\n                p = next(param_itr)\n                p_EMA.data = (1-ema_decay_rate)*p_EMA.data + ema_decay_rate*p.data\n                p_EMA.requires_grad = False\n        \n        #log\n        D_loss_list.append(D_running_loss)\n        G_loss_list.append(G_running_loss)\n        \n        #output\n        if epoch % output_freq == 0:\n            print('[{:d}/{:d}] D_loss = {:.3f}, G_loss = {:.3f}, elapsed_time = {:.1f} min'.format(epoch,epochs,D_running_loss,G_running_loss,elapsed_time(start_time)/60))\n            \n        if epoch in show_epoch_list:\n            print('epoch = {}'.format(epoch))\n            if not EMA:\n                show_generate_imgs(netG,fixed_noise,fixed_aux_labels_ohe)\n            elif EMA:\n                show_generate_imgs(netG_EMA,fixed_noise,fixed_aux_labels_ohe)\n            \n        if epoch % 100 == 0:\n            if not EMA:\n                torch.save(netG.state_dict(), f'generator_epoch{epoch}.pth')\n            elif EMA:\n                torch.save(netG_EMA.state_dict(), f'generator_epoch{epoch}.pth')\n    \n    if not EMA:\n        torch.save(netG.state_dict(), 'generator.pth')\n    elif EMA:\n        torch.save(netG_EMA.state_dict(), 'generator.pth')\n    torch.save(netD.state_dict(), 'discriminator.pth')\n    \n    res = {'netG':netG,\n           'netD':netD,\n           'nz':nz,\n           'fixed_noise':fixed_noise,\n           'fixed_aux_labels_ohe':fixed_aux_labels_ohe,\n           'D_loss_list':D_loss_list,\n           'G_loss_list':G_loss_list,\n          }\n    if EMA:\n        res['netG_EMA'] = netG_EMA\n        \n    return res","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n#show_epoch_list = np.arange(0,100,1)\nshow_epoch_list = np.arange(0,500+10,10)\n\nres = run(lr_G=LR_G, lr_D=LR_D, beta1=0.0, beta2=0.999, nz=Z_DIM, epochs=500, \n          n_ite_D=1, ema_decay_rate=None, show_epoch_list=show_epoch_list, output_freq=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(res['D_loss_list'], label='D_loss')\nplt.plot(res['G_loss_list'], label='G_loss')\nplt.grid()\nplt.legend()\nplt.title('loss history');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"#truncation_trick\ndef submission_generate_images(res, truncated=None):\n    im_batch_size=50\n    n_images=10000\n    if not EMA:\n        netG = res['netG']\n    elif EMA:\n        netG = res['netG_EMA']\n    nz   = res['nz']\n    if not os.path.exists('../output_images'):\n        os.mkdir('../output_images')\n    for i_batch in range(0, n_images, im_batch_size):\n        if truncated is not None:\n            flag = True\n            while flag:\n                z = np.random.randn(100*im_batch_size*nz)\n                z = z[np.where(abs(z)<truncated)]\n                if len(z)>=im_batch_size*nz:\n                    flag=False\n            gen_z = torch.from_numpy(z[:im_batch_size*nz]).view(im_batch_size,nz,1,1)\n            gen_z = gen_z.float().to(device)\n        else:\n            gen_z = torch.randn(im_batch_size, nz, 1, 1, device=device)\n#         gen_z = gen_z / gen_z.norm(dim=1, keepdim=True)\n        aux_labels     = np.random.randint(0,n_classes, im_batch_size)\n        aux_labels_ohe = np.eye(n_classes)[aux_labels]\n        aux_labels_ohe = torch.from_numpy(aux_labels_ohe[:,:,np.newaxis,np.newaxis])\n        aux_labels_ohe = aux_labels_ohe.float().to(device)\n        \n        gen_images = netG(gen_z,aux_labels_ohe)\n        gen_images = gen_images.to(\"cpu\").clone().detach() #shape=(*,3,h,w), torch.Tensor\n        #denormalize\n        gen_images = gen_images*0.5 + 0.5\n        for i_image in range(gen_images.size(0)):\n            if SHARPEN:\n                img = transforms.ToPILImage()(gen_images[i_image])\n                img = sharpen(img, magnitude=SHARPEN_MAGNITUDE)\n                img = transforms.ToTensor()(img)\n                save_image(img,\n                           os.path.join(f'../output_images', f'image_{i_batch+i_image:05d}.png'))\n            else:\n                save_image(gen_images[i_image, :, :, :],\n                           os.path.join(f'../output_images', f'image_{i_batch+i_image:05d}.png'))\n    shutil.make_archive(f'images', 'zip', f'../output_images')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nsubmission_generate_images(res,truncated=0.8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if not EMA:\n    gen_image_numpy = generate_img(res['netG'],res['fixed_noise'],res['fixed_aux_labels_ohe'])\nelif EMA:\n    gen_image_numpy = generate_img(res['netG_EMA'],res['fixed_noise'],res['fixed_aux_labels_ohe'])\nfor img in gen_image_numpy:\n    plt.imshow(img)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"elapsed_time(start_time)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}