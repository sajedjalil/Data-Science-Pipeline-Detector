{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Params"},{"metadata":{"trusted":false},"cell_type":"code","source":"INPUT_HEIGHT = INPUT_WIDTH = 64\nOUTPUT_HEIGHT = OUTPUT_WIDTH = 64\nCROP_HEIGHT = CROP_WIDTH = 64\nBATCH_SIZE = 32\nANNOTATIONS_PATH = '../input/annotation/Annotation/'\nIMAGES_PATH = '../input/all-dogs/all-dogs/'\nZ_DIM, Z_STD = 64, 1\nCLIP_Z = False\nY_DIM, Y_EMBEDDING_DIM = 120, 10\nC_DIM = 3\nGF_DIM = DF_DIM = 64\nG_LR, D_LR = 1e-4, 1e-4\nADAM_BETA1, ADAM_BETA2, ADAM_EPSILON = 0.5, 0.99, 1e-8\nPERFORM_SN = False\nSN_ITERATIONS = 1\nLOCAL_RUN = False\nINPUT_NOISE_ANNEALING_STEPS = 3e5\n# HINGE | KL | SIGMOID_CROSSENTROPY | LEAKY_HINGE | SQUARED_LEAKY_HINGE | LEAKY_SIGMOID_CROSSENTROPY\nLOSS = 'LEAKY_SIGMOID_CROSSENTROPY' \nNOISY_LOSS_TARGET = True\nINPUTS_NOISE = True\nLABELS_NOISE = True\nCONV_K = 7\nTRAINING_TIME_LIMIT = 32000","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Imports"},{"metadata":{"trusted":false},"cell_type":"code","source":"import cv2\nimport tensorflow as tf\nfrom tensorflow import concat\nimport tensorflow.contrib.slim as slim\nimport imageio\nimport numpy as np\nimport xml.etree.ElementTree as ET\nimport os\nimport time\nimport datetime\nfrom glob import glob\nfrom tqdm import tqdm_notebook as tqdm\nimport zipfile\nfrom itertools import chain, islice, count\nfrom IPython.core.display import display, HTML","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define operations"},{"metadata":{"trusted":false},"cell_type":"code","source":"class batch_norm(object):\n\n    def __init__(self, epsilon=1e-5, momentum=0.9, name=\"batch_norm\"):\n        with tf.variable_scope(name):\n            self.epsilon = epsilon\n            self.momentum = momentum\n            self.name = name\n\n    def __call__(self, x, train=True):\n        return tf.contrib.layers.batch_norm(x,\n                                            decay=self.momentum,\n                                            updates_collections=None,\n                                            epsilon=self.epsilon,\n                                            scale=True,\n                                            is_training=train,\n                                            scope=self.name,\n                                           )\n\n\ndef spectral_normed_weight(w, update_u=True):\n    if PERFORM_SN:\n        def l2normalize(v, eps=1e-12):\n            return v / (tf.reduce_sum(v ** 2) ** 0.5 + eps)\n        w_shape = w.shape.as_list()\n        w_mat = tf.reshape(w, [-1, w_shape[-1]])  # [-1, output_channel]\n        u = tf.get_variable('u', [1, w_shape[-1]], trainable=False,\n                            initializer=tf.truncated_normal_initializer())\n        u_ = u\n        for _ in range(SN_ITERATIONS):\n            v_ = l2normalize(tf.matmul(u_, w_mat, transpose_b=True))\n            u_ = l2normalize(tf.matmul(v_, w_mat))\n        sigma = tf.squeeze(tf.matmul(tf.matmul(v_, w_mat), u_, transpose_b=True))\n        w_mat /= sigma\n        if update_u:\n            with tf.control_dependencies([u.assign(u_)]):\n                w_normed = tf.reshape(w_mat, w_shape)\n        else:\n            w_normed = tf.reshape(w_mat, w_shape)\n        return w_normed\n    else:\n        return w\n\n\ndef conv2d(input_, output_dim,\n           k_h=CONV_K, k_w=CONV_K, d_h=2, d_w=2, stddev=0.02,\n           name=\"conv2d\", update_u=True, add_bias=False):\n    with tf.variable_scope(name):\n        w = tf.get_variable('w', [k_h, k_w, input_.get_shape()[-1], output_dim],\n                            initializer=tf.truncated_normal_initializer(stddev=stddev))\n        w = spectral_normed_weight(w, update_u=update_u)\n        conv = tf.nn.conv2d(input_, w, strides=[1, d_h, d_w, 1], padding='SAME')\n        if add_bias:\n            biases = tf.get_variable('biases', [output_dim], initializer=tf.constant_initializer(0.0))\n            conv = tf.nn.bias_add(conv, biases)\n        return conv\n\n\ndef deconv2d(input_, output_shape,\n             k_h=CONV_K, k_w=CONV_K, d_h=2, d_w=2, stddev=0.02,\n             name=\"deconv2d\", update_u=True, add_bias=False):\n    with tf.variable_scope(name):\n        w = tf.get_variable('w', [k_h, k_w, output_shape[-1], input_.get_shape()[-1]],\n                            initializer=tf.random_normal_initializer(stddev=stddev))\n        w = spectral_normed_weight(w, update_u=update_u)\n        deconv = tf.nn.conv2d_transpose(input_, w, output_shape=output_shape, strides=[1, d_h, d_w, 1])\n        if add_bias:\n            biases = tf.get_variable('biases', [output_shape[-1]], initializer=tf.constant_initializer(0.0))\n            deconv = tf.nn.bias_add(deconv, biases)\n        return deconv\n\n\ndef lrelu(x, leak=0.2, name=\"lrelu\"):\n    return tf.nn.leaky_relu(x, leak, name)\n\n\ndef linear(input_, output_size, scope=None,\n           stddev=0.02, bias_start=0.0, update_u=True):\n    shape = input_.get_shape()\n    with tf.variable_scope(scope or \"Linear\"):\n        matrix = tf.get_variable(\"Matrix\", [shape[1], output_size], tf.float32,\n                                 tf.random_normal_initializer(stddev=stddev))\n        matrix = spectral_normed_weight(matrix, update_u=update_u)\n        bias = tf.get_variable(\"bias\", [output_size], initializer=tf.constant_initializer(bias_start))\n\n        return tf.matmul(input_, matrix) + bias\n\n\ndef add_coordinates(input_tensor):\n    bs, h_dim, w_dim, c = input_tensor.get_shape().as_list()\n\n    x = tf.stack([tf.range(w_dim, dtype=tf.float32)] * h_dim) / (w_dim - 1) * 2 - 1\n    y = tf.transpose(tf.stack([tf.range(h_dim, dtype=tf.float32)] * w_dim)) / (h_dim - 1) * 2 - 1\n    r = ((x**2 + y**2) / 2) ** 0.5\n    coords = tf.stack([tf.stack([x, y, r], axis=-1)] * bs)\n    coords = tf.constant(tf.get_default_session().run(coords))\n    \n    return tf.concat([input_tensor, coords], axis=-1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define utils functions"},{"metadata":{"trusted":false},"cell_type":"code","source":"def timestamp(template='%Y%m%d_%H%M%S', ts=None):\n    return datetime.datetime.fromtimestamp(ts or time.time()).strftime(template)\n\n\ndef conv_out_size_same(size, stride):\n    return int(np.ceil(float(size) / float(stride)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define network"},{"metadata":{},"cell_type":"markdown","source":"## Data Augmentations"},{"metadata":{"trusted":false},"cell_type":"code","source":"def transform_X(x):\n    x = x / 128 - 255 / 256\n    return tf.image.random_flip_left_right(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"class DCGAN(object):\n\n    def __init__(self, sess, data):\n        self.start_time = time.time()\n        out_dir = './gan_out/{}/'.format(timestamp())\n        self.checkpoint_dir = os.path.join(out_dir, 'checkpoints')\n        os.makedirs(self.checkpoint_dir, exist_ok=True)\n        self.sample_dir = os.path.join(out_dir, 'samples')\n        os.makedirs(self.sample_dir, exist_ok=True)\n        self.log_file = open(os.path.join(out_dir, 'log.txt'), 'w')\n        \n        self.sess = sess\n        self.global_step = tf.train.create_global_step()\n        self.load_data(*data)\n        self.build_model()\n\n    def load_data(self, X, Y):\n        dataset = tf.data.Dataset.from_tensor_slices((X, Y))\n        dataset = dataset.map((lambda x, y: (x, tf.one_hot(y, Y_DIM, dtype=tf.float32))))\n        dataset = dataset.shuffle(len(X), reshuffle_each_iteration=True).repeat()\n        dataset = dataset.map((lambda x, y: (transform_X(x), y)), num_parallel_calls=2)\n        dataset = dataset.apply(tf.contrib.data.batch_and_drop_remainder(BATCH_SIZE)).prefetch(2)\n        \n        self.x_batch, self.y_batch = dataset.make_one_shot_iterator().get_next()\n        self.clear_x_batch, self.clear_y_batch = self.x_batch, self.y_batch\n        if INPUTS_NOISE:\n            self.x_batch = self.noisy_inputs(self.x_batch)\n        if LABELS_NOISE:\n            self.y_batch = self.noisy_labels(self.y_batch)\n\n    @staticmethod\n    def x_plus_x_squared(x):\n        return x + x * x\n        \n    def get_d_real_loss(self, logits):\n        if LOSS  == 'SIGMOID_CROSSENTROPY':\n            if NOISY_LOSS_TARGET:\n                target = tf.random.uniform((BATCH_SIZE, 1), minval=0.95, maxval=1.05, dtype=tf.float32) \n            else:\n                target = tf.fill(tf.shape(logits), 1.)\n        elif LOSS  == 'LEAKY_SIGMOID_CROSSENTROPY':\n            target = tf.fill(tf.shape(logits), 0.9)\n            if NOISY_LOSS_TARGET:\n                target += tf.random.uniform([], minval=-0.025, maxval=0.025) \n        elif NOISY_LOSS_TARGET:\n            target = tf.random.uniform((BATCH_SIZE, 1), minval=0.95, maxval=1.05, dtype=tf.float32)           \n        else:\n            target = 1.0\n\n        if LOSS == 'HINGE':\n            loss = tf.nn.relu(target - logits)\n        elif LOSS == 'KL':\n            loss = tf.nn.softplus(-logits)\n        elif LOSS in ('SIGMOID_CROSSENTROPY', 'LEAKY_SIGMOID_CROSSENTROPY'):\n            loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=target)\n        elif LOSS == 'LEAKY_HINGE':\n            loss = tf.nn.leaky_relu(target - logits, -0.2)\n        elif LOSS == 'SQUARED_LEAKY_HINGE':\n            loss = self.x_plus_x_squared(tf.nn.leaky_relu(target - logits, -0.2))\n        return tf.reduce_mean(loss)\n\n    def get_d_fake_loss(self, logits):\n        if LOSS  == 'SIGMOID_CROSSENTROPY':\n            if NOISY_LOSS_TARGET:\n                target = tf.random.uniform((BATCH_SIZE, 1), minval=0.05, maxval=0.05, dtype=tf.float32) \n            else:\n                target = tf.fill(tf.shape(logits), 0.)\n        elif LOSS  == 'LEAKY_SIGMOID_CROSSENTROPY':\n            target = tf.fill(tf.shape(logits), 0.1)\n            if NOISY_LOSS_TARGET:\n                target += tf.random.uniform([], minval=-0.025, maxval=0.025) \n        elif NOISY_LOSS_TARGET:\n            target = tf.random.uniform((BATCH_SIZE, 1), minval=0.95, maxval=1.05, dtype=tf.float32)\n        else:\n            target = 1.0\n            \n        if LOSS == 'HINGE':\n            loss = tf.nn.relu(target + logits)\n        elif LOSS == 'KL':\n            loss = tf.nn.softplus(logits)\n        elif LOSS in ('SIGMOID_CROSSENTROPY', 'LEAKY_SIGMOID_CROSSENTROPY'):\n            loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=target)\n        elif LOSS == 'LEAKY_HINGE':\n            loss = tf.nn.leaky_relu(target + logits, -0.2)\n        elif LOSS == 'SQUARED_LEAKY_HINGE':\n            loss = self.x_plus_x_squared(tf.nn.leaky_relu(target + logits, -0.2))\n        return tf.reduce_mean(loss)\n\n    def get_g_loss(self, logits):\n        if LOSS  == 'SIGMOID_CROSSENTROPY':\n            if NOISY_LOSS_TARGET:\n                target = tf.random.uniform((BATCH_SIZE, 1), minval=0.95, maxval=1.05, dtype=tf.float32) \n            else:\n                target = tf.fill(tf.shape(logits), 1.)\n        elif LOSS  == 'LEAKY_SIGMOID_CROSSENTROPY':\n            target = tf.fill(tf.shape(logits), 0.9)\n            if NOISY_LOSS_TARGET:\n                target += tf.random.uniform([], minval=-0.025, maxval=0.025)\n        elif NOISY_LOSS_TARGET:\n            target = tf.random.uniform((BATCH_SIZE, 1), minval=0.95, maxval=1.05, dtype=tf.float32)\n        else:\n            target = 1.0\n            \n        if LOSS in ('HINGE', 'KL'):\n            loss = -logits\n        elif LOSS in ('SIGMOID_CROSSENTROPY', 'LEAKY_SIGMOID_CROSSENTROPY'):\n            loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=target)\n        elif LOSS == 'LEAKY_HINGE':\n            loss = tf.nn.leaky_relu(target - logits, -0.2)\n        elif LOSS == 'SQUARED_LEAKY_HINGE':\n            loss = self.x_plus_x_squared(tf.nn.leaky_relu(target - logits, -0.2))\n        return tf.reduce_mean(loss)\n\n    def build_model(self):\n        self.increment_global_step = self.global_step.assign_add(1)\n\n        self.z_batch = tf.random.normal((BATCH_SIZE, Z_DIM), 0, Z_STD)\n        if CLIP_Z:\n            self.z_batch = tf.clip_by_value(self.z_batch, -1, 1)\n\n        # batch normalization : deals with poor initialization, helps gradient flow\n        self.d_bn0 = batch_norm(name='d_bn0')\n        self.d_bn1 = batch_norm(name='d_bn1')\n        self.d_bn2 = batch_norm(name='d_bn2')\n        self.d_bn3 = batch_norm(name='d_bn3')\n        self.d_bn4 = batch_norm(name='d_bn4')\n\n        self.g_bn0 = batch_norm(name='g_bn0')\n        self.g_bn1 = batch_norm(name='g_bn1')\n        self.g_bn2 = batch_norm(name='g_bn2')\n        self.g_bn3 = batch_norm(name='g_bn3')\n        self.g_bn4 = batch_norm(name='g_bn4')\n        \n        self.G = self.generator(self.z_batch, self.y_batch)\n        z_sample, y_sample = self.sess.run([self.z_batch, self.y_batch])\n        self.fixed_sampler = tf.cast((self.generator(tf.constant(z_sample), tf.constant(y_sample), update_u=False) + 1) * 127.5, tf.uint8)\n        self.sampler = tf.cast((self.generator(self.z_batch, self.y_batch, update_u=False) + 1) * 127.5, tf.uint8)\n        \n        self.D_logits_real = self.discriminator(self.x_batch, self.y_batch)\n        self.D_logits_fake = self.discriminator(self.G, self.y_batch)\n        \n        self.d_loss_real = self.get_d_real_loss(self.D_logits_real)\n        self.d_loss_fake = self.get_d_fake_loss(self.D_logits_fake)\n        self.d_loss = self.d_loss_real + self.d_loss_fake\n        self.g_loss = self.get_g_loss(self.D_logits_fake)\n        self.get_vars()\n        self.d_optim = tf.train.AdamOptimizer(D_LR, beta1=ADAM_BETA1, beta2=ADAM_BETA2, epsilon=ADAM_EPSILON) \\\n            .minimize(self.d_loss, var_list=self.d_vars)\n        self.g_optim = tf.train.AdamOptimizer(G_LR, beta1=ADAM_BETA1, beta2=ADAM_BETA2, epsilon=ADAM_EPSILON) \\\n            .minimize(self.g_loss, var_list=self.g_vars)\n        self.saver = tf.train.Saver(max_to_keep=1)\n        tf.global_variables_initializer().run(session=self.sess)\n\n    def train(self):\n        info_template = (\n            'step: {}, time: {:5.2f}, d_loss_fake: {:.4f}, '\n            'd_loss_real: {:.4f}, d_loss: {:.5f}, g_loss: {:.5f}')\n        running_time = time.time() - self.start_time\n        while running_time < TRAINING_TIME_LIMIT:\n            d_fake, d_real, d, g, step, _, _ = self.sess.run(\n                [self.d_loss_fake, self.d_loss_real, self.d_loss, self.g_loss, \n                 self.increment_global_step,\n                 self.d_optim, self.g_optim,\n                ],\n            )\n            previous_running_time = running_time\n            running_time = time.time() - self.start_time\n            print(info_template.format(step, running_time, \n                                       d_fake, d_real, d, g),\n                  file=self.log_file)\n            if previous_running_time // 10 != running_time // 10:  # do each 10 seconds\n                print(info_template.format(step, running_time, \n                                           d_fake, d_real, d, g))\n            if previous_running_time // 1000 != running_time // 1000:  # do each 1000 seconds\n                self.save_sample()\n                self.generate_zip(os.path.join(self.sample_dir, f'images{running_time // 1000}.zip'), 1000)\n                self.save(self.checkpoint_dir)\n    \n    def noisy_labels(self, labels, mean=0, std=0.03):\n        '''Add noise to labels'''\n        return tf.add(labels, tf.random_normal(tf.shape(labels), mean, std, dtype=tf.float32))\n\n    def noisy_inputs(self, inputs, mean=0, std=0.03, annealing_steps=INPUT_NOISE_ANNEALING_STEPS):\n        step = tf.cast(self.global_step, tf.float32)\n        return tf.cond(step < annealing_steps,\n                lambda: tf.clip_by_value(tf.add(inputs, tf.random_normal(tf.shape(inputs), \n                                                               mean, \n                                                               std * (annealing_steps - step) / annealing_steps, \n                                                               dtype=tf.float32)), -1, 1),\n                lambda: inputs)\n     \n    def get_vars(self):\n        t_vars = tf.trainable_variables()\n        self.d_vars = [var for var in t_vars if var.name.startswith('discriminator/d_')]\n        self.g_vars = [var for var in t_vars if var.name.startswith('generator/g_')]\n        self.sigma_ratio_vars = [var for var in t_vars if 'sigma_ratio' in var.name]\n        assert (set(self.d_vars) & set(self.g_vars)) == set()\n        assert (set(self.d_vars) | set(self.g_vars)) == set(t_vars)\n        self.all_vars = t_vars\n        \n    def discriminator(self, image, y, update_u=True):\n        with tf.variable_scope(\"discriminator\", reuse=tf.AUTO_REUSE) as scope:\n            h0 = lrelu(self.d_bn0(conv2d(image, DF_DIM, name='d_h0_conv', update_u=update_u))) # 32 * 32\n            h1 = lrelu(self.d_bn1(conv2d(h0, DF_DIM * 2, name='d_h1_conv', update_u=update_u))) # 16 * 16\n            h2 = lrelu(self.d_bn2(conv2d(h1, DF_DIM * 4, name='d_h2_conv', update_u=update_u))) # 8 * 8\n            h3 = lrelu(self.d_bn3(conv2d(h2, DF_DIM * 8, name='d_h3_conv', update_u=update_u))) # 4 * 4\n            y_embedding = lrelu(self.d_bn4(linear(y, Y_EMBEDDING_DIM, 'd_y_embedding', update_u=update_u)))\n            h4 = linear(concat([tf.reshape(h3, [BATCH_SIZE, -1]), y_embedding], 1), 1, 'd_h4_lin', update_u=update_u)\n            return h4\n\n    def generator(self, z, y, is_training=True, update_u=True):\n        with tf.variable_scope(\"generator\", reuse=tf.AUTO_REUSE) as scope:\n            y_embedding = lrelu(self.g_bn0(linear(y, Z_DIM, 'g_y_embedding', update_u=update_u)))\n            z = concat([z, y_embedding], 1)\n            \n            s_h, s_w = OUTPUT_HEIGHT, OUTPUT_WIDTH\n            s_h2, s_w2 = conv_out_size_same(s_h, 2), conv_out_size_same(s_w, 2)\n            s_h4, s_w4 = conv_out_size_same(s_h2, 2), conv_out_size_same(s_w2, 2)\n            s_h8, s_w8 = conv_out_size_same(s_h4, 2), conv_out_size_same(s_w4, 2)\n            s_h16, s_w16 = conv_out_size_same(s_h8, 2), conv_out_size_same(s_w8, 2)\n            \n            # project `z` and reshape\n            z_ = linear(z, GF_DIM * 8 * s_h16 * s_w16, 'g_h0_lin', update_u=update_u)\n            h0 = tf.reshape(z_, [-1, s_h16, s_w16, GF_DIM * 8])\n            h0 = lrelu(self.g_bn1(h0, train=is_training))\n            h1 = deconv2d(h0, [BATCH_SIZE, s_h8, s_w8, GF_DIM * 4], name='g_h1', update_u=update_u) # 8 * 8\n            h1 = lrelu(self.g_bn2(h1, train=is_training))\n            h2 = deconv2d(h1, [BATCH_SIZE, s_h4, s_w4, GF_DIM * 2], name='g_h2', update_u=update_u) # 16 * 16\n            h2 = lrelu(self.g_bn3(h2, train=is_training))\n            h3 = deconv2d(h2, [BATCH_SIZE, s_h2, s_w2, GF_DIM * 1], name='g_h3', update_u=update_u) # 32 * 32\n            h3 = lrelu(self.g_bn4(h3, train=is_training))\n            h4 = deconv2d(h3, [BATCH_SIZE, s_h, s_w, C_DIM], name='g_final_deconv', update_u=update_u) # 64 * 64\n            return tf.nn.tanh(h4)\n  \n    def generate_images(self):\n        images = self.sess.run(\n            self.sampler, \n        )\n        return images\n            \n    def save_sample(self):\n        images, step = self.sess.run(\n            [self.fixed_sampler, self.global_step]\n        )\n        sample = np.vstack([np.hstack(images[i*16:(i+1)*16]) for i in range(max(1, min(8, BATCH_SIZE // 16)))])\n        sample_path = os.path.join(self.sample_dir, '{}.png'.format(step))\n        imageio.imwrite(sample_path, sample)\n        imageio.imwrite('real_dogs_sample.png', sample)\n        display(HTML(f'<img src=\"{sample_path}\" alt=\"Sample of generated dogs.\" width=\"100%\">'))\n        \n    def save(self, checkpoint_dir, filename='model'):\n        if not os.path.exists(checkpoint_dir):\n            os.makedirs(checkpoint_dir)\n        self.saver.save(self.sess, os.path.join(checkpoint_dir, filename))\n        \n    def generate_zip(self, zip_path='images.zip', n_images=10000):\n        start_time = time.time()\n        with zipfile.PyZipFile(zip_path, 'w') as z:\n            for i, image in enumerate(islice(chain.from_iterable(gan.generate_images() for _ in count()), n_images)):\n                path = f'{i}.png'\n                imageio.imwrite(path, image); z.write(path); os.remove(path)\n        print(f'{time.time() - start_time:.1f}s to generate {n_images} images')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load Data"},{"metadata":{"trusted":false},"cell_type":"code","source":"def get_bboxes(annotation_file):\n    '''Extract and return bounding boxes from annotation file.'''\n    bboxes = []\n    objects = ET.parse(annotation_file).getroot().findall('object')\n    for obj in objects:\n        bbox = obj.find('bndbox')\n        bboxes.append(tuple(int(bbox.find(_).text) for _ in ('xmin', 'ymin', 'xmax', 'ymax')))\n    return bboxes\n\n\ndef imread(path):\n    img_bgr = cv2.imread(path)\n    return img_bgr[..., ::-1]\n\n\ndef transform(image, height=CROP_HEIGHT, width=CROP_WIDTH):\n    image = cv2.resize(image, dsize=(width, height), interpolation=cv2.INTER_LANCZOS4)\n    return image\n\n\ndef load_data():\n    print('Loading data, iterating over {} breeds...'.format(Y_DIM))\n    dogs, breeds = [], []\n    for breed_index, breed_path in enumerate(tqdm(sorted(glob(ANNOTATIONS_PATH + '*')))):\n        for annotation_file in glob(breed_path + '/*'):\n            image_path = IMAGES_PATH + '{}.jpg'.format(annotation_file.split('/')[-1])\n            try:\n                image = imread(image_path)\n                bboxes = get_bboxes(annotation_file)\n                # add crops\n                for xmin, ymin, xmax, ymax in bboxes:\n                    dogs.append(transform(image[max(0, ymin-10):ymax+10, max(0, xmin-10):xmax+10, :]))\n                    breeds.append(breed_index)\n            except Exception as e:\n                continue  # there is one annotation file without corresponding image\n    print('Done. {} dogs in total.'.format(len(dogs)))\n    return np.array(dogs, dtype=np.uint8), np.array(breeds, dtype=np.int32)\n\nDOGS, BREEDS = load_data()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_all_variables(verbose=False):\n    model_vars = tf.trainable_variables()\n    stats = slim.model_analyzer.analyze_vars(model_vars, print_info=verbose)\n    if not verbose:\n        print('Total variables: {:.2f}M.'.format(stats[0]/10**6))\n\ndef show_flops():\n    g = tf.get_default_graph()\n    run_meta = tf.RunMetadata()\n    opts = tf.profiler.ProfileOptionBuilder.float_operation()\n    flops = tf.profiler.profile(g, run_meta=run_meta, cmd='op', options=opts)\n    print('{:.3f} TFLOPs in graph.'.format(flops.total_float_ops / 10**12))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.reset_default_graph()\nrun_config = tf.ConfigProto()\nrun_config.gpu_options.allow_growth = True\nsess = tf.Session(config=run_config)\ngan = DCGAN(sess, (DOGS, BREEDS))\nshow_all_variables()\nshow_flops()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gan.train()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Generate Submission ZIP"},{"metadata":{"trusted":true},"cell_type":"code","source":"gan.generate_zip()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"2616a2b133f44794b41df35ac2e727c2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d57713f4522c459db1e012ba3b17a76f","IPY_MODEL_8f198a62491f4c41a015ff8ffc8dab07"],"layout":"IPY_MODEL_75bdd473d3e24ccfa73751611a6eb2a9"}},"74685523b58347808aba8d8184ac2bbe":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"75bdd473d3e24ccfa73751611a6eb2a9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8f198a62491f4c41a015ff8ffc8dab07":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_74685523b58347808aba8d8184ac2bbe","placeholder":"â€‹","style":"IPY_MODEL_9ede190f555f4349bb6a1c4565c8c940","value":"100% 120/120 [03:02&lt;00:00,  1.51s/it]"}},"9ede190f555f4349bb6a1c4565c8c940":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d57713f4522c459db1e012ba3b17a76f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"IntProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"IntProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f8b9fd84ef1b4375b3edcff7c0baed88","max":120,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f3fb266bd21e46b4a6ef04ea6ca1e060","value":120}},"f3fb266bd21e46b4a6ef04ea6ca1e060":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f8b9fd84ef1b4375b3edcff7c0baed88":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":1}