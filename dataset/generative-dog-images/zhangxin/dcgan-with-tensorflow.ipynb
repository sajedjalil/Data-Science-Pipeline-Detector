{"cells":[{"metadata":{},"cell_type":"markdown","source":"The script is based on the book **deep interesing**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\nimport numpy as np\nimport urllib\nimport tarfile\nimport os\nfrom tqdm import tqdm_notebook\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom imageio import imread, imsave, mimsave\nfrom PIL import Image\nimport glob\nimport shutil","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1 Crop image"},{"metadata":{},"cell_type":"markdown","source":"The code below is based on [the amazing script.](https://www.kaggle.com/whizzkid/crop-images-using-bounding-box)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport xml.etree.ElementTree as ET # for parsing XML\nimport matplotlib.pyplot as plt # to show images\nfrom PIL import Image # to read images\nimport os\nimport glob\n\nroot_images=\"../input/all-dogs/all-dogs/\"\nroot_annots=\"../input/annotation/Annotation/\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_images=os.listdir(\"../input/all-dogs/all-dogs/\")\nprint(f\"Total images : {len(all_images)}\")\n\nbreeds = glob.glob('../input/annotation/Annotation/*')\nannotation=[]\nfor b in breeds:\n    annotation+=glob.glob(b+\"/*\")\nprint(f\"Total annotation : {len(annotation)}\")\n\nbreed_map={}\nfor annot in annotation:\n    breed=annot.split(\"/\")[-2]\n    index=breed.split(\"-\")[0]\n    breed_map.setdefault(index,breed)\n    \nprint(f\"Total Breeds : {len(breed_map)}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def bounding_box(image):\n    bpath=root_annots+str(breed_map[image.split(\"_\")[0]])+\"/\"+str(image.split(\".\")[0])\n    tree = ET.parse(bpath)\n    root = tree.getroot()\n    objects = root.findall('object')\n    for o in objects:\n        bndbox = o.find('bndbox') # reading bound box\n        xmin = int(bndbox.find('xmin').text)\n        ymin = int(bndbox.find('ymin').text)\n        xmax = int(bndbox.find('xmax').text)\n        ymax = int(bndbox.find('ymax').text)\n        \n    return (xmin,ymin,xmax,ymax)\n\ndef get_crop_image(image):\n    bbox=bounding_box(image)\n    im=Image.open(os.path.join(root_images,image))\n    im=im.crop(bbox)\n    return im","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nplt.figure(figsize=(10,10))\nfor i,image in enumerate(all_images):\n    im=get_crop_image(image)\n    \n    plt.subplot(3,3,i+1)\n    plt.axis(\"off\")\n    plt.imshow(im)    \n    if(i==8):\n        break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2 DCGAN"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\npath = '../input/all-dogs'\ndataset = 'all-dogs'\ndata_path = os.path.join(path, dataset)\nimages = glob.glob(os.path.join(data_path, '*.*')) \nprint(len(images))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"images[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"z_dim = 1000\n\nWIDTH = 64\nHEIGHT = 64\n\nOUTPUT_DIR = 'samples_dogs'\nif not os.path.exists(OUTPUT_DIR):\n    os.mkdir(OUTPUT_DIR)\n\nGEN_DIR = 'generated_dogs'\nif not os.path.exists(GEN_DIR):\n    os.mkdir(GEN_DIR)\n    \nX = tf.placeholder(dtype=tf.float32, shape=[None, HEIGHT, WIDTH, 3], name='X')\nnoise = tf.placeholder(dtype=tf.float32, shape=[None, z_dim], name='noise')\nis_training = tf.placeholder(dtype=tf.bool, name='is_training')\n\ndef lrelu(x, leak=0.2):\n    return tf.maximum(x, leak * x)\n\ndef sigmoid_cross_entropy_with_logits(x, y):\n    return tf.nn.sigmoid_cross_entropy_with_logits(logits=x, labels=y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def discriminator(image, reuse=None, is_training=is_training):\n    momentum = 0.9\n    with tf.variable_scope('discriminator', reuse=reuse):\n        h0 = lrelu(tf.layers.conv2d(image, kernel_size=5, filters=64, strides=2, padding='same'))\n        \n        h1 = tf.layers.conv2d(h0, kernel_size=5, filters=128, strides=2, padding='same')\n        h1 = lrelu(tf.layers.batch_normalization(h1, training=is_training, momentum=momentum))\n        \n        h2 = tf.layers.conv2d(h1, kernel_size=5, filters=256, strides=2, padding='same')\n        h2 = lrelu(tf.layers.batch_normalization(h2, training=is_training, momentum=momentum))\n        \n        h3 = tf.layers.conv2d(h2, kernel_size=5, filters=512, strides=2, padding='same')\n        h3 = lrelu(tf.layers.batch_normalization(h3, training=is_training, momentum=momentum))\n\n        h4 = tf.layers.flatten(h3)\n        h4 = tf.layers.dense(h4, units=1)\n        return tf.nn.sigmoid(h4), h4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generator(z, is_training=is_training):\n    momentum = 0.9\n    with tf.variable_scope('generator', reuse=None):\n        d = 4\n        h0 = tf.layers.dense(z, units=d * d * 512)\n        h0 = tf.reshape(h0, shape=[-1, d, d, 512])\n        h0 = tf.nn.relu(tf.layers.batch_normalization(h0, training=is_training, momentum=momentum))\n        \n        h1 = tf.layers.conv2d_transpose(h0, kernel_size=5, filters=256, strides=2, padding='same')\n        h1 = tf.nn.relu(tf.layers.batch_normalization(h1, training=is_training, momentum=momentum))\n        \n        h2 = tf.layers.conv2d_transpose(h1, kernel_size=5, filters=128, strides=2, padding='same')\n        h2 = tf.nn.relu(tf.layers.batch_normalization(h2, training=is_training, momentum=momentum))\n        \n        h3 = tf.layers.conv2d_transpose(h2, kernel_size=5, filters=64, strides=2, padding='same')\n        h3 = tf.nn.relu(tf.layers.batch_normalization(h3, training=is_training, momentum=momentum))\n        \n        h4 = tf.layers.conv2d_transpose(h3, kernel_size=5, filters=3, strides=2, padding='same', activation=tf.nn.tanh, name='g')\n        return h4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = generator(noise)\nd_real, d_real_logits = discriminator(X)\nd_fake, d_fake_logits = discriminator(g, reuse=True)\n\nvars_g = [var for var in tf.trainable_variables() if var.name.startswith('generator')]\nvars_d = [var for var in tf.trainable_variables() if var.name.startswith('discriminator')]\n\nloss_d_real = tf.reduce_mean(sigmoid_cross_entropy_with_logits(d_real_logits, tf.ones_like(d_real)))\nloss_d_fake = tf.reduce_mean(sigmoid_cross_entropy_with_logits(d_fake_logits, tf.zeros_like(d_fake)))\nloss_g = tf.reduce_mean(sigmoid_cross_entropy_with_logits(d_fake_logits, tf.ones_like(d_fake)))\nloss_d = loss_d_real + loss_d_fake","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\nwith tf.control_dependencies(update_ops):\n    optimizer_d = tf.train.AdamOptimizer(learning_rate=0.0002, beta1=0.5).minimize(loss_d, var_list=vars_d)\n    optimizer_g = tf.train.AdamOptimizer(learning_rate=0.0002, beta1=0.5).minimize(loss_g, var_list=vars_g)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_image(image_name, height, width):\n    image = get_crop_image(image_name)\n    \n    h = image.size[0]\n    w = image.size[1]\n    \n    image = np.array(image.resize((height, width)))\n    return image / 255.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def montage(images):    \n    if isinstance(images, list):\n        images = np.array(images)\n    img_h = images.shape[1]\n    img_w = images.shape[2]\n    n_plots = int(np.ceil(np.sqrt(images.shape[0])))\n    if len(images.shape) == 4 and images.shape[3] == 3:\n        m = np.ones(\n            (images.shape[1] * n_plots + n_plots + 1,\n             images.shape[2] * n_plots + n_plots + 1, 3)) * 0.5\n    elif len(images.shape) == 4 and images.shape[3] == 1:\n        m = np.ones(\n            (images.shape[1] * n_plots + n_plots + 1,\n             images.shape[2] * n_plots + n_plots + 1, 1)) * 0.5\n    elif len(images.shape) == 3:\n        m = np.ones(\n            (images.shape[1] * n_plots + n_plots + 1,\n             images.shape[2] * n_plots + n_plots + 1)) * 0.5\n    else:\n        raise ValueError('Could not parse image shape of {}'.format(images.shape))\n    for i in range(n_plots):\n        for j in range(n_plots):\n            this_filter = i * n_plots + j\n            if this_filter < images.shape[0]:\n                this_img = images[this_filter]\n                m[1 + i + i * img_h:1 + i + (i + 1) * img_h,\n                  1 + j + j * img_w:1 + j + (j + 1) * img_w] = this_img\n    return m","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 4\nepochs = 500000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tensorflow is dead when running a little time.\nsv = tf.train.Supervisor()\nsaver = sv.saver\nwith sv.managed_session() as sess:\n    z_samples = np.random.uniform(-1.0, 1.0, [batch_size, z_dim]).astype(np.float32)\n    samples = []\n    loss = {'d': [], 'g': []}\n\n    offset = 0\n    for i in tqdm_notebook(range(epochs)):\n        n = np.random.uniform(-1.0, 1.0, [batch_size, z_dim]).astype(np.float32)\n\n        offset = (offset + batch_size) % len(images)\n        batch = np.array([read_image(img, HEIGHT, WIDTH) for img in all_images[offset: offset + batch_size]])\n        batch = (batch - 0.5) * 2\n\n        d_ls, g_ls = sess.run([loss_d, loss_g], feed_dict={X: batch, noise: n, is_training: True})\n        loss['d'].append(d_ls)\n        loss['g'].append(g_ls)\n\n        sess.run(optimizer_d, feed_dict={X: batch, noise: n, is_training: True})\n        sess.run(optimizer_g, feed_dict={X: batch, noise: n, is_training: True})\n        sess.run(optimizer_g, feed_dict={X: batch, noise: n, is_training: True})\n\n        if i % 10000 == 0:\n            print(i, d_ls, g_ls)\n            gen_imgs = sess.run(g, feed_dict={noise: z_samples, is_training: False})\n            gen_imgs = (gen_imgs + 1) / 2\n            imgs = [img[:, :, :] for img in gen_imgs]\n            gen_imgs = montage(imgs)\n            plt.axis('off')\n            plt.imshow(gen_imgs)\n            plt.show()\n\n    plt.plot(loss['d'], label='Discriminator')\n    plt.plot(loss['g'], label='Generator')\n    plt.legend(loc='upper right')\n    plt.show()\n    \n    graph = tf.get_default_graph()\n    g = graph.get_tensor_by_name('generator/g/Tanh:0')\n    noise = graph.get_tensor_by_name('noise:0')\n    is_training = graph.get_tensor_by_name('is_training:0')\n\n    n = np.random.uniform(-1.0, 1.0, [batch_size, z_dim]).astype(np.float32)\n    gen_imgs = sess.run(g, feed_dict={noise: n, is_training: False})\n    gen_imgs = (gen_imgs + 1) / 2\n    imgs = [img[:, :, :] for img in gen_imgs]\n    gen_imgs = montage(imgs)\n    gen_imgs = np.clip(gen_imgs, 0, 1)\n    plt.figure(figsize=(8, 8))\n    plt.axis('off')\n    plt.imshow(gen_imgs)\n    plt.show()\n    \n    # Store data\n    n_batches = 10000 // batch_size\n    last_batch_size = 10000 % batch_size\n\n    for i in tqdm_notebook(range(n_batches)):\n        n = np.random.uniform(-1.0, 1.0, [batch_size, z_dim]).astype(np.float32)\n        gen_imgs = sess.run(g, feed_dict={noise: n, is_training: False})\n        gen_imgs = (gen_imgs + 1) / 2\n        for j in range(batch_size):\n            imsave(os.path.join(GEN_DIR, f'sample_{i}_{j}.png'), gen_imgs[j])\n    \n    for i in range(last_batch_size):\n        n = np.random.uniform(-1.0, 1.0, [batch_size, z_dim]).astype(np.float32)\n        gen_imgs = sess.run(g, feed_dict={noise: n, is_training: False})\n        gen_imgs = (gen_imgs + 1) / 2\n        imsave(os.path.join(GEN_DIR, f'sample_{n_batches}_{i}.png'), gen_imgs[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nshutil.make_archive('images', 'zip', GEN_DIR)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(os.listdir(GEN_DIR))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!rm -rf generated_dogs/\n!rm -rf samples_dogs/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls -l .","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.2"}},"nbformat":4,"nbformat_minor":1}