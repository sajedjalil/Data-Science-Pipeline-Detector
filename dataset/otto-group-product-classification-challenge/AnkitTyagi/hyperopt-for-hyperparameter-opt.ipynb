{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Brief about the notebook.\n\nIn this notebook we will be using Hyperopt for hyperparamter optimization. We will be using [Otto Group production classification Problem](https://www.kaggle.com/c/otto-group-product-classification-challenge). \n\n1. This is multiclass classification problem (9 classes).\n2. Dataset have 93 numerical features and 61878 observations.\n\nWe will be using hyperopt for hyperparamter tuning for our XGBoost model."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n!pip install scikit-optimize==0.8.1\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nimport xgboost as xg\nfrom functools import partial\nfrom hyperopt import hp, fmin, tpe, Trials\nfrom hyperopt.pyll.base import scope","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Reading train dataset in the environment.\ndataset_pd = pd.read_csv(\"/kaggle/input/otto-group-product-classification-challenge/train.csv\", index_col = 0)\nprint(dataset_pd.shape)\n# Reading test dataset in the environment.\ndataset_pd2 = pd.read_csv(\"/kaggle/input/otto-group-product-classification-challenge/test.csv\", index_col = 0)\nprint(dataset_pd2.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a predictor matrix (removing the response variable column)\ndataset_train = dataset_pd.values\nX = dataset_train[:,0:93] # Predictors\ny = dataset_train[:,93] # Response \n\n# XGBoost do not take a categorical variable as input. We can use LabelEncoder to assign labels to categorical variables.\nlabel_encoder = LabelEncoder()\nlabel_encoder = label_encoder.fit(y)\nlabel_encoder_y = label_encoder.transform(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# optimize function \ndef optimize(params, x, y):\n\n    model = xg.XGBClassifier(**params)\n    kf = StratifiedKFold(n_splits = 5)\n    accuracies = []\n    for idx in kf.split(X = x, y = y):\n        train_idx , test_idx = idx[0], idx[1]\n        xtrain = x[train_idx]\n        ytrain = y[train_idx]\n        \n        xtest = x[test_idx]\n        ytest = y[test_idx]\n        \n        model.fit(xtrain, ytrain)\n        preds = model.predict(xtest)\n        fold_acc = accuracy_score(ytest, preds)\n        accuracies.append(fold_acc)\n    \n    return -1.0 * np.mean(accuracies)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Parameter Space for XGBoost\nparam_space = {\n    'max_depth' : scope.int(hp.quniform('max_depth', 3,15, 1)),\n    'n_estimators' : scope.int(hp.quniform('n_estimators', 100, 600, 1)),\n    'criterion' : hp.choice('criterion', ['gini', 'entropy']),\n    'colsample_bytree' : hp.uniform('colsample_bytree', 0.01,1),\n    'learning_rate' : hp.uniform('learning_rate', 0.001,1) \n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Optimization Function\noptimization_function = partial(\n    optimize,\n    x = X,\n    y = label_encoder_y\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trials = Trials()\nresult = fmin(fn = optimization_function,\n                    space = param_space,\n                    algo = tpe.suggest,\n                    max_evals = 15,\n                    trials = trials\n)\nprint(result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train and test split of the data\nX_train, X_test, y_train, y_test = train_test_split(X, label_encoder_y, test_size = 0.33, random_state = 7)\n\nclassifier = xg.XGBClassifier(n_thread = 6, \n                              n_estimators = 396, \n                              max_depth = 6, \n                              colsample_bytree = 0.9292372781188178,\n                              learning_rate = 0.28725052863307404,\n                              criterion = \"gini\")\nclassifier.fit(X_train, y_train)\n\n# Check the accuracy of the model on train and test dataset.\naccuracy_train = accuracy_score(y_train, classifier.predict(X_train))\nprint(\"Accuracy on train dataset %.2f%%\" % (accuracy_train * 100))\n\naccuracy_test = accuracy_score(y_test, classifier.predict(X_test))\nprint(\"Accuracy on test dataset %.2f%%\" % (accuracy_test * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# code for submission file.\ndataset_test = dataset_pd2.values\n\nclassifier = xg.XGBClassifier(n_thread = 6, \n                              n_estimators = 396, \n                              max_depth = 6, \n                              colsample_bytree = 0.9292372781188178,\n                              learning_rate = 0.28725052863307404,\n                              criterion = \"gini\")\nclassifier.fit(X, label_encoder_y)\n\nprediction_sub = classifier.predict(dataset_test)\n\n#dataset_pd2[\"prediction\"] = prediction_sub\nX_sub = np.array(prediction_sub).reshape(-1,1)\nonehot_encoder = OneHotEncoder(sparse = False)\nsubmission_file = onehot_encoder.fit_transform(X_sub)\n\nsubmission_file_df = pd.DataFrame(submission_file, \n                                  columns = ['Class_1','Class_2','Class_3','Class_4','Class_5','Class_6',\n                                            'Class_7','Class_8','Class_9'], index = dataset_pd2.index)\n\n\nsubmission_file_df.to_csv(\"submission_otto_ver2.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}