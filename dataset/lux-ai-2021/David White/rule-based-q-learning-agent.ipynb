{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Here is the implementation for my current top bot. So far it has only had middling success so I thought I would share and see if anyone else might be able to improve on it. My idea was to make rule-based workers that could switch between three different basic behavior patterns:\n1. Build city: Gather wood from the nearest source and build a city tile on the closest open cell.\n2. Gather resources: Gather the most potent fuel currently researched from the nearest available source and bring it to the closest city tile.\n3. Rest: Move to the nearest city tile and stay there.\n\nThe strategy could then be adjusted throughout the match by an RL agent who would alter the proportions of workers assigned each behavior. I implemented a Q-learning agent that takes in a state vector with nine inputs: \n\n> \\[(# of worker units) (# of cart units) (# of city tiles) (# of opponent workers) (# of opponent carts) (# of opponent city tiles) (Research points) (Opponent research points) (Turn #)\\]\n\nand return one of 55 action codes corresponding to a set of worker behavior proportions. My hope was that the neural net would eventually learn strategic patterns like building quickly once night ended, focusing more on resource gathering as night approached, and returning to cities to conserve fuel once night fell but no such patterns ever seemed to emerge.\n\nTo handle mining, I generated Mine objects for each resource cluster at the beginning of each game storing the locations of tiles and resource type for each cluster. When workers were ready to gather resources, they would be assigned a particular resource cell in a mine. ","metadata":{"papermill":{"duration":0.014575,"end_time":"2021-10-05T18:56:22.608733","exception":false,"start_time":"2021-10-05T18:56:22.594158","status":"completed"},"tags":[]}},{"cell_type":"code","source":"!node --version\n!pip install kaggle-environments -U\n!pip install tensorflow","metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2021-10-05T18:56:22.659275Z","iopub.status.busy":"2021-10-05T18:56:22.651023Z","iopub.status.idle":"2021-10-05T18:56:37.400384Z","shell.execute_reply":"2021-10-05T18:56:37.39939Z","shell.execute_reply.started":"2021-10-05T16:34:47.431795Z"},"papermill":{"duration":14.776681,"end_time":"2021-10-05T18:56:37.400533","exception":false,"start_time":"2021-10-05T18:56:22.623852","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cp -r ../input/lux-ai-2021/* .\nfrom kaggle_environments import make","metadata":{"execution":{"iopub.execute_input":"2021-10-05T18:56:37.454825Z","iopub.status.busy":"2021-10-05T18:56:37.454184Z","iopub.status.idle":"2021-10-05T18:56:37.841301Z","shell.execute_reply":"2021-10-05T18:56:37.84171Z","shell.execute_reply.started":"2021-10-05T18:08:48.379084Z"},"papermill":{"duration":0.416034,"end_time":"2021-10-05T18:56:37.841841","exception":false,"start_time":"2021-10-05T18:56:37.425807","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Each worker unit is wrapped in a WorkerAgent object which holds things like their current objective and destination. The workers use a rudimentary sort of collision aversion technique. Workers earlier in the action assignment queue have movement priority over later workers, meaning that if a higher priority worker plans to move into a spot currently occupied or by a lower priority one or a lower priority worker's next step would block one of a higher priority worker, that worker is moved or the planned step is changed.","metadata":{"papermill":{"duration":0.016588,"end_time":"2021-10-05T18:56:37.875353","exception":false,"start_time":"2021-10-05T18:56:37.858765","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%writefile WorkerAgent.py\nfrom enum import Enum\nfrom lux.game_map import RESOURCE_TYPES, Position\nimport sys\nimport math\n\nclass WorkerObjective(Enum):\n    GatherFuel = \"gather fuel\"\n    BuildCity = \"build city\"\n    Rest = \"rest\"\n\n# Wrapper class for worker unit objects\nclass WorkerAgent:\n    def __init__(self, worker_obj, debug):\n        self.debug = debug\n        self.worker = worker_obj\n        self.objective = WorkerObjective.BuildCity\n        self.objective_changed = False\n        self.mine = None\n        self.destination = None\n        \n    # Reset worker object with latest version each turn\n    def update(self, worker_obj):\n        self.worker = worker_obj\n    \n    def _find_open_mine(self, resource_type, mines):\n        mine = mines.place_in_mine(self.worker, resource_type)\n        if mine is not None:\n            self.mine = mine\n        \n    def _get_best_fuel(self, player):\n        resource_type = RESOURCE_TYPES.WOOD\n        if player.researched_coal():\n            resource_type = RESOURCE_TYPES.COAL\n        if player.researched_uranium():\n            resource_type = RESOURCE_TYPES.URANIUM\n            \n        return resource_type\n    \n    def _at_mining_spot(self):\n        mining_spot = self.get_mining_spot()\n        if mining_spot is None:\n            return False\n        \n        return self.worker.pos == mining_spot\n    \n    def _update_mining(self, controller):\n        if self._at_mining_spot():\n            cell = controller.map.get_cell_by_pos(self.worker.pos)\n            if not cell.has_resource():\n                self.mine.report_resource_depleted(self.worker.pos, self.worker)\n                self.mine = None\n         \n        if self.objective == WorkerObjective.GatherFuel:\n            best_fuel = self._get_best_fuel(controller.player)\n            if self.mine is not None and self.mine.resource_type != best_fuel:\n                self.mine.release_worker(self.worker)\n                self._find_open_mine(best_fuel, controller.mines)\n                \n    def _handle_objective_change(self):\n        if self.objective_changed:\n            if self.mine != None:\n                self.mine.release_worker(self.worker)\n                self.mine = None\n            self.destination = None\n            self.objective_changed = False\n            if self.debug:\n                print(\"Worker\", self.worker.id, \"assigned new objective\", file=sys.stderr)\n                \n    def _handle_mine_assignment(self, controller):\n        if self.destination is None and self.mine is None and self.objective != WorkerObjective.Rest:\n            resource_type = RESOURCE_TYPES.WOOD\n            if self.objective == WorkerObjective.GatherFuel:\n                resource_type = self._get_best_fuel(controller.player)\n            self._find_open_mine(resource_type, controller.mines)\n            if self.debug:\n                if self.mine is not None:\n                    print(\"Worker\", self.worker.id, \"assigned to mining spot\", self.get_mining_spot(), file=sys.stderr)\n                else:\n                    print(\"Unable to place worker\", self.worker.id, \"in mine\", file=sys.stderr)\n                \n    def _handle_destination_arrival(self):\n        if self.destination is not None and self.worker.pos == self.destination:\n            self.destination = None  \n            if self.debug:\n                print(\"Worker\", self.worker.id, \"arrived at their destination\", file=sys.stderr)\n        \n    def _handle_destination_assignment(self, controller):\n        if self.destination is not None:\n            return\n        \n        if self.objective == WorkerObjective.Rest and not self.on_city_tile(controller.map):\n            closest_city_tile = controller.cities.get_nearest_city_tile(self.worker.pos)\n            if closest_city_tile is not None:\n                self.destination = closest_city_tile.pos\n            if self.debug:\n                print(\"Worker\", self.worker.id, \"destination set to city tile\", (self.destination.x, self.destination.y), file=sys.stderr)\n            return\n            \n        if self.worker.get_cargo_space_left() == 0: \n            if self.mine is not None:\n                self.mine.release_worker(self.worker)\n                self.mine = None\n\n            if self.debug:\n                print(\"Worker\", self.worker.id, \"is at max cargo\", file=sys.stderr)\n            if self.objective == WorkerObjective.GatherFuel:\n                nearest_city_tile = controller.cities.get_nearest_city_tile(self.worker.pos)\n                if nearest_city_tile is not None:\n                    self.destination = nearest_city_tile.pos\n            elif self.objective == WorkerObjective.BuildCity:\n                # nearest_periph = controller.cities.get_nearest_periph_pos(self.worker.pos, controller.map)\n                # if nearest_periph is not None:\n                #     self.destination = nearest_periph\n                # else:\n                self.destination = self.find_nearest_empty_tile(self.worker.pos, controller.map)\n            if self.debug:\n                print(\"Worker\", self.worker.id, \"destination changed to\", self.destination, file=sys.stderr)\n            return\n                   \n        if not self._at_mining_spot():\n            self.destination = self.get_mining_spot()\n            if self.debug:\n                print(\"Worker\", self.worker.id, \"returning to minining spot\", self.get_mining_spot(), file=sys.stderr)\n            return\n\n    # Converts directions to degrees\n    def _to_degrees(self, direction):\n        directions = [\"w\", \"s\", \"e\", \"n\"]\n        return 90 * directions.index(direction)\n\n    # Converts degrees to directions\n    def _to_dir(self, degrees):\n        directions = [\"w\", \"s\", \"e\", \"n\"]\n        return directions[int((degrees % 360) / 90)]\n        \n\n    # Returns the direction 90 degrees * times clockwise of direction\n    def _rotate_dir(self, direction, times):\n        if direction == \"c\":\n            return \"c\"\n        return self._to_dir(self._to_degrees(direction) + 90 * times)\n                \n        \n    def get_mining_spot(self):\n        if self.mine == None:\n            return None\n        \n        tile = self.mine.get_assigned_spot(self.worker)\n        if tile is not None:\n            return Position(tile[0], tile[1])\n        return None\n        \n    def set_objective(self, objective):\n        if self.objective == objective:\n            return\n        self.objective = objective\n        self.objective_changed = True\n        if self.debug:\n            print(\"Worker\", self.worker.id, \"has new objective\", self.objective, file=sys.stderr)\n        \n    def get_step_direction(self, game_map, steps, avoid_city=False):\n        direction = self.worker.pos.direction_to(self.destination)\n        step = step = self.worker.pos.translate(direction, 1)\n\n        if direction == \"c\" and (step.x, step.y) in steps:              # If the worker plans to stay put but is blocking the step of another worker\n            for i in range(4):\n                new_dir = self._rotate_dir(\"w\", i)\n                step = self.worker.pos.translate(new_dir, 1)\n                if (step.x, step.y) not in steps:\n                    return new_dir\n        \n        if avoid_city or (step.x, step.y) in steps:                     # Get best detour\n            cell = game_map.get_cell(step.x, step.y)\n            if cell.citytile is None and (step.x, step.y) not in steps:\n                return direction\n            \n            shortest_dist = float(\"inf\")\n            best_dir = None\n            for i in range(4):\n                new_dir = self._rotate_dir(direction, i)\n                step = self.worker.pos.translate(new_dir, 1)\n                if step.x < 0 or step.x >= game_map.width or step.y < 0 or step.y >= game_map.height or (step.x, step.y) in steps:\n                    continue\n                cell = game_map.get_cell(step.x, step.y)\n                dist = step.distance_to(self.destination) \n                if cell.citytile is None and dist < shortest_dist:\n                    best_dir = new_dir\n                    \n            if best_dir is not None:\n                return best_dir\n              \n        return direction\n            \n    \n    def on_city_tile(self, game_map):\n        tile = game_map.get_cell_by_pos(self.worker.pos)\n        return tile.citytile is None\n    \n    def find_nearest_empty_tile(self, loc, game_map):\n        if self.tile_is_empty(loc, game_map):\n            return loc\n        \n        searched = set()\n        q = [loc]\n        \n        while len(q) > 0:\n            p = q.pop(0)\n            searched.add((p.x, p.y))\n            \n            if self.tile_is_empty(p, game_map):\n                return p\n            \n            for direction in [(1, 0), (0, 1), (-1, 0), (0, -1)]:\n                neighbor = Position(p.x + direction[0], p.y + direction[1])\n                if neighbor.x >= 0 and neighbor.x < game_map.width and neighbor.y >= 0 and neighbor.y < game_map.height and (neighbor.x, neighbor.y) not in searched:\n                    q.append(neighbor)\n            \n            \n    def tile_is_empty(self, pos, game_map):\n        cell = game_map.get_cell(pos.x, pos.y)\n        return cell.citytile is None and not cell.has_resource()\n    \n    def get_action(self, controller, steps):\n        self._update_mining(controller)\n        \n        if not self.worker.can_act():\n            return None, (self.worker.pos.x, self.worker.pos.y)\n        \n        self._handle_objective_change()\n        self._handle_mine_assignment(controller)\n        self._handle_destination_arrival()\n        \n        if self.destination is None and self.objective == WorkerObjective.BuildCity and self.worker.can_build(controller.map):\n            if self.debug:\n                print(\"Worker\", self.worker.id, \"building city tile at\", self.worker.pos)\n            return self.worker.build_city(), (self.worker.pos.x, self.worker.pos.y)\n        \n        self._handle_destination_assignment(controller)\n                \n        if self.destination is not None:\n            avoid_city = self.objective == WorkerObjective.BuildCity and self.worker.get_cargo_space_left() == 0\n            step_dir = self.get_step_direction(controller.map, steps, avoid_city)\n            if self.debug:\n                print(\"Worker\", self.worker.id, \"step direction:\", step_dir, file=sys.stderr)\n            # step_dir = self.worker.pos.direction_to(self.destination)\n            step = self.worker.pos.translate(step_dir, 1)\n            return self.worker.move(step_dir), (step.x, step.y)\n        \n        return None, (self.worker.pos.x, self.worker.pos.y)\n    \nclass Workers:\n    def __init__(self, worker_list, debug):\n        self.debug = debug\n        self.workers = {}                              # Maps worker ids to WorkerAgent objs\n        self.task_proportions = [0.5, 0.5, 0.0]\n        \n        for worker in worker_list:\n            self.workers[worker.id] = WorkerAgent(worker, self.debug)\n            \n        if self.debug:\n            print(\"Workers object initialized\")\n\n        self._reassign_objectives()\n            \n    def _reassign_objectives(self):\n        num_city_builders = math.ceil(self.task_proportions[0] * len(self.workers))\n        num_fuel_gatherers = math.ceil(self.task_proportions[1] * len(self.workers))\n        worker_ids = self.workers.keys()\n        \n        for i, worker_id in enumerate(worker_ids):\n            if i < num_city_builders:\n                self.workers[worker_id].set_objective(WorkerObjective.BuildCity)\n                continue\n            if i < num_city_builders + num_fuel_gatherers:\n                self.workers[worker_id].set_objective(WorkerObjective.GatherFuel)\n                continue\n            self.workers[worker_id].set_objective(WorkerObjective.Rest)\n        \n    def update(self, worker_list):\n        if self.debug:\n            print(\"Updating workers object\")\n           \n        # Remove workers that were lost last turn\n        lost_workers = set(self.workers.keys()).difference(set([worker.id for worker in worker_list]))\n        for lost_worker in lost_workers:\n            self.workers.pop(lost_worker)\n            \n        for worker in worker_list:\n            if worker.id in self.workers:\n                self.workers[worker.id].update(worker)\n                continue\n                \n            self.workers[worker.id] = WorkerAgent(worker, self.debug)\n            self._reassign_objectives()\n            if self.debug:\n                print(\"Worker added\", file=sys.stderr)\n                \n    def update_task_proportions(self, proportions):\n        self.task_proportions = proportions\n        self._reassign_objectives()\n            \n            \n    def get_actions(self, controller):\n        actions = []\n        steps = set()\n        \n        for worker in self.workers.values():\n            action, step = worker.get_action(controller, steps)\n            steps.add(step)\n            if action is not None:\n                actions.append(action)\n                \n        return actions\n","metadata":{"execution":{"iopub.execute_input":"2021-10-05T18:56:37.920226Z","iopub.status.busy":"2021-10-05T18:56:37.912263Z","iopub.status.idle":"2021-10-05T18:56:37.924327Z","shell.execute_reply":"2021-10-05T18:56:37.923583Z","shell.execute_reply.started":"2021-10-05T16:35:07.88628Z"},"papermill":{"duration":0.032312,"end_time":"2021-10-05T18:56:37.924481","exception":false,"start_time":"2021-10-05T18:56:37.892169","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Cities and CityWrapper classes hold information about city tiles and cities. They are mainly used to provide navigation methods and generate city tile actions.","metadata":{"papermill":{"duration":0.025027,"end_time":"2021-10-05T18:56:37.975132","exception":false,"start_time":"2021-10-05T18:56:37.950105","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%writefile CityWrapper.py\nfrom lux.game_map import Position\nimport sys\n\nclass CityWrapper:\n    def __init__(self, city_obj, debug):\n        self.city = city_obj\n        self.debug = debug\n    \n    def get_nearest_periph_pos(self, loc, game_map):\n        if self.debug:\n            print(\"Searching for city build location\", file=sys.stderr)\n        # Return periphery tile obj closest to loc (Only works if loc is not inside city)\n        \n        # Sort tiles in city according to distance from loc\n        sorted_tiles = sorted(self.city.citytiles, key=lambda tile: tile.pos.distance_to(loc))\n\n        for tile in sorted_tiles:\n            for direction in [(1, 0), (0, 1), (-1, 0), (0, -1)]:\n                neighbor = Position(tile.pos.x + direction[0], tile.pos.y + direction[1])\n                if neighbor.x >= 0 and neighbor.x < game_map.width and neighbor.y >= 0 and neighbor.y < game_map.height:\n                    cell = game_map.get_cell(neighbor.x, neighbor.y)\n                    if cell.citytile == None and not cell.has_resource():\n                        return neighbor\n                    \n        return None\n    \n    def get_nearest_city_tile(self, loc):\n        # Return city tile closest to loc\n        shortest_dist = float(\"inf\")\n        closest = None\n        for tile in self.city.citytiles:\n            dist = tile.pos.distance_to(loc)\n            if dist < shortest_dist:\n                shortest_dist = dist\n                closest = tile\n        return closest\n    \n    def get_actions(self, controller, workers_needed):\n        actions = []\n        workers_built = 0\n        for tile in self.city.citytiles:\n            if tile.can_act():\n                if workers_needed - workers_built > 0:\n                    actions.append(tile.build_worker())\n                    workers_built += 1\n                    if self.debug:\n                        print(\"City tile\", tile.pos, \"creating new worker.\", file=sys.stderr)\n                    continue\n                actions.append(tile.research())\n        return actions, workers_built\n    \nclass CitiesWrapper:\n    def __init__(self, cities_list, debug):\n        self.cities = [CityWrapper(city, debug) for city in cities_list]\n        self.debug = debug\n        \n    def update(self, cities_list):\n        self.cities = [CityWrapper(city, self.debug) for city in cities_list]\n    \n    def get_nearest_city(self, loc):\n        # Return CityWrapper obj closest to loc\n        shortest_dist = float(\"inf\")\n        closest = None\n        \n        for city in self.cities:\n            dist = city.get_nearest_city_tile(loc).pos.distance_to(loc)\n            if dist < shortest_dist:\n                shortest_dist = dist\n                closest = city\n                \n        return closest\n    \n    def get_nearest_city_tile(self, loc):\n        # Return CityTile obj closest to loc\n        shortest_dist = float(\"inf\")\n        closest = None\n        \n        for city in self.cities:\n            tile = city.get_nearest_city_tile(loc)\n            dist = tile.pos.distance_to(loc)\n            if dist < shortest_dist:\n                shortest_dist = dist\n                closest = tile\n                \n        return closest\n    \n    def get_nearest_periph_pos(self, loc, game_map):\n        sorted_cities = sorted(self.cities, key=lambda city: city.get_nearest_city_tile(loc).pos.distance_to(loc))\n        \n        for city in sorted_cities:\n            periph = city.get_nearest_periph_pos(loc, game_map)\n            if periph is not None:\n                return periph\n            \n        return None\n    \n    def get_actions(self, controller):\n        actions = []\n        workers_needed = max(controller.state.num_city_tiles - controller.state.num_workers, 0)\n        \n        for city in self.cities:\n            city_actions, workers_built = city.get_actions(controller, workers_needed)\n            actions += city_actions\n            workers_needed -= workers_built\n            \n        return actions\n","metadata":{"execution":{"iopub.execute_input":"2021-10-05T18:56:38.032732Z","iopub.status.busy":"2021-10-05T18:56:38.032001Z","iopub.status.idle":"2021-10-05T18:56:38.034645Z","shell.execute_reply":"2021-10-05T18:56:38.03517Z","shell.execute_reply.started":"2021-10-05T16:35:07.906416Z"},"papermill":{"duration":0.035086,"end_time":"2021-10-05T18:56:38.035354","exception":false,"start_time":"2021-10-05T18:56:38.000268","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Mine class handles mining operations. Mine objects hold the positions of resource tiles and handle the assignment of worker units to said tiles.","metadata":{"papermill":{"duration":0.027103,"end_time":"2021-10-05T18:56:38.088438","exception":false,"start_time":"2021-10-05T18:56:38.061335","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%writefile Mine.py\nfrom lux.game_map import Position\nimport sys\n\nclass Mine:\n    def __init__(self, game_state, resource_tile_set, resource_type, debug):\n        self.resource_type = resource_type\n        self.resource_tiles = resource_tile_set\n        self.assigned_workers = {}                                      # Maps worker IDs to assigned worker_tile\n        #self.available_resources = 0\n        #self.cart_loc = self.get_cart_loc()\n        #self.available_work_tiles = len(self.worker_tiles)              # Number of available worker tiles\n        self.debug = debug\n    \n    def _find_cart_loc(self):\n        # Find and return the best location to park the cart\n        pass\n    \n    def _get_open_worker_tile(self, worker_pos):\n        available_tiles = list(filter(lambda tile: tile not in self.assigned_workers.values(), self.resource_tiles))\n        available_tiles = sorted(available_tiles, key=lambda tile: Position(tile[0], tile[1]).distance_to(worker_pos))\n        return available_tiles[0]\n    \n    def get_resource_tiles(self):\n        return self.resource_tiles\n    \n    def worker_assigned(self, worker_id):                               # Checks if a given worker is assigned to mine\n        return worker_id in self.assigned_workers\n    \n    def get_dist(self, loc):                                            # Returns the shortest distance between loc and all spots in mine\n        shortest_dist = float(\"inf\")\n        \n        for tile in self.resource_tiles:\n            tile_pos = Position(tile[0], tile[1])\n            dist = tile_pos.distance_to(loc)\n            if dist < shortest_dist:\n                shortest_dist = dist\n                \n        return shortest_dist\n    \n    def has_opening(self):                                              # Checks if there are any available spots in mine\n        return len(self.resource_tiles) > len(self.assigned_workers)\n    \n    def assign_worker(self, worker):\n        self.assigned_workers[worker.id] = self._get_open_worker_tile(worker.pos)\n        \n    def release_worker(self, worker):\n        if worker.id in self.assigned_workers:\n            self.assigned_workers.pop(worker.id)\n        \n    def get_assigned_spot(self, worker):\n        if worker.id in self.assigned_workers:\n            return self.assigned_workers[worker.id]\n        return None\n    \n    def report_resource_depleted(self, pos, assigned_worker):\n        self.resource_tiles.remove((pos.x, pos.y))\n        self.release_worker(assigned_worker)\n\n    \nclass Mines:\n    def __init__(self, game_state, debug):\n        self.mines = []\n        self.debug = debug\n        \n        self._build_mines(game_state)\n        \n    def _is_valid_tile(self, game_state, x, y, w, h, resource_type, searched):\n        if x < 0 or x >= w or y < 0 or y >= h or (x, y) in searched:\n            return False\n        \n        tile = game_state.map.get_cell(x, y)\n        if not tile.has_resource() or tile.resource.type != resource_type:\n            return False\n        \n        return True\n        \n    def _get_resource_cluster(self, game_state, x, y, w, h, resource_type, cluster_tiles=set(), searched=set()):\n        # Given x, y of a starting tile, search game map to find tiles of resource cluster\n        searched.add((x, y))\n        tile = game_state.map.get_cell(x, y)\n        \n        if not tile.has_resource():                             # Add tile to border set and make no recursive calls\n            return cluster_tiles, searched\n        \n        cluster_tiles.add((x, y))\n        \n        for direction in [(1, 0), (0, 1), (-1, 0), (0, -1)]:  # Call function recursively on surrounding tiles\n            new_x, new_y = x + direction[0], y + direction[1]\n            if self._is_valid_tile(game_state, new_x, new_y, w, h, resource_type, searched):\n                new_cluster_tiles, new_searched = self._get_resource_cluster(game_state, new_x, new_y, w, h, resource_type, cluster_tiles, searched)\n                cluster_tiles = cluster_tiles.union(new_cluster_tiles)\n                searched = searched.union(new_searched)\n            \n        return cluster_tiles, searched\n        \n    def _build_mines(self, game_state): \n        # Iterate over map to find clusters of resource tiles\n        w, h = game_state.map.width, game_state.map.height\n        searched = set()\n        clusters = []\n        resource_types = []\n        \n        for x in range(w):\n            for y in range(h):\n                if (x, y) in searched:\n                    continue\n                tile = game_state.map.get_cell(x, y)\n                if tile.has_resource():\n                    resource_types.append(tile.resource.type)\n                    cluster, new_searched = self._get_resource_cluster(game_state, x, y, w, h, tile.resource.type, set(), set())\n                    searched = searched.union(new_searched)\n                    clusters.append(cluster)\n        \n        # ToDo: Merge mines of same resource type that share borders\n        \n        # Build Mine objs from clusters and borders\n        for cluster, resource_type in zip(clusters, resource_types):\n            self.mines.append(Mine(game_state, cluster, resource_type, self.debug))\n        \n        if self.debug:\n            print(\"Clusters:\", clusters, file=sys.stderr)\n                    \n    def update(self, game_state):\n        # Check mines to see if they need updated\n        for mine in self.mines:\n            needs_update = mine.update_mine(game_state)\n            \n            if needs_update:\n                # Find viable cell from mine to seed cluster search\n                mine_tiles = mine.get_resource_tiles()\n                new_cluster = None\n                new_border = None\n                \n                for tile in mine_tiles:\n                    cell = game_state.map.get_cell(tile[0], tile[1])\n                \n                    if cell.has_resource():\n                        new_cluster, new_border, searched = self._get_resource_cluster(game_state, tile[0], tile[1], gamestate.map.width, gamestate.map.height, cell.resource.type, set(), set())\n                        break\n                \n                self.mines.remove(mine)\n                \n                if new_cluster is not None:\n                    self.mines.append(Mine(game_state, new_cluster, new_border, self.debug))\n    \n    def get_closest_mine(self, loc, resource_type):\n        closest_mine = None\n        shortest_dist = float(\"inf\")\n        \n        for mine in self.mines:\n            if mine.resource_type != resource_type:\n                continue\n            for tile in mine.resource_tiles:\n                tile_pos = Position(tile[0], tile[1])\n                dist = tile_pos.distance_to(loc)\n                \n                if dist < shortest_dist:\n                    shortest_dist = dist\n                    closest_mine = mine\n                    \n        return closest_mine\n    \n    def place_in_mine(self, worker, resource_type):\n        sorted_mines = sorted([mine for mine in self.mines if mine.resource_type == resource_type], key=lambda mine: mine.get_dist(worker.pos))\n        \n        for mine in sorted_mines:\n            if mine.has_opening():\n                mine.assign_worker(worker)\n                return mine \n            \n        return None\n","metadata":{"execution":{"iopub.execute_input":"2021-10-05T18:56:38.143997Z","iopub.status.busy":"2021-10-05T18:56:38.1432Z","iopub.status.idle":"2021-10-05T18:56:38.1509Z","shell.execute_reply":"2021-10-05T18:56:38.151408Z","shell.execute_reply.started":"2021-10-05T16:35:07.924012Z"},"papermill":{"duration":0.037092,"end_time":"2021-10-05T18:56:38.15158","exception":false,"start_time":"2021-10-05T18:56:38.114488","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The controller object contains all workers, mines, and cities and provides an interface for the RL agent to modify the current strategy.","metadata":{"papermill":{"duration":0.025703,"end_time":"2021-10-05T18:56:38.203099","exception":false,"start_time":"2021-10-05T18:56:38.177396","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%writefile Controller.py\nfrom Mine import Mines\nfrom WorkerAgent import Workers\nfrom CityWrapper import CitiesWrapper\nimport numpy as np\nimport sys\n\nclass State:\n    def __init__(self, game_state, player, opponent):\n        self._update_state(game_state, player, opponent)\n        \n    def _update_state(self, game_state, player, opponent):\n        self.num_workers = sum([1 if unit.is_worker() else 0 for unit in player.units])\n        self.num_carts = sum([1 if unit.is_cart() else 0 for unit in player.units])\n        self.num_city_tiles = player.city_tile_count\n        self.num_opponent_workers = sum([1 if unit.is_worker() else 0 for unit in opponent.units])\n        self.num_opponent_carts = sum([1 if unit.is_cart() else 0 for unit in opponent.units])\n        self.num_opponent_city_tiles = opponent.city_tile_count\n        self.research_points = player.research_points\n        self.opponent_research_points = opponent.research_points\n        self.turn = game_state.turn\n        \n    def get_state_vector(self):\n        return np.array([\n            self.num_workers,\n            self.num_carts,\n            self.num_city_tiles,\n            self.num_opponent_workers,\n            self.num_opponent_carts,\n            self.num_opponent_city_tiles,\n            self.research_points,\n            self.opponent_research_points,\n            self.turn\n        ]).reshape(1, -1)\n        \n\nclass Controller:\n    def __init__(self, game_state, player, opponent, debug):\n        self.debug = debug\n        self.game_state = game_state\n        self.map = game_state.map\n        self.state = State(game_state, player, opponent)\n        self.player = player\n        self.opponent = opponent\n        self.mines = Mines(game_state, debug)\n        self.workers = Workers([unit for unit in player.units if unit.is_worker()], debug)\n#         self.carts = []\n        self.cities = CitiesWrapper(self.player.cities.values(), debug)\n        \n    def update(self, game_state, player, opponent):\n        self.game_state = game_state\n        self.state._update_state(game_state, player, opponent)\n        self.map = game_state.map\n        self.player = player\n        self.opponent = opponent\n        #self.mines.update(game_state)\n        self.cities.update(self.player.cities.values())\n        self.workers.update([unit for unit in player.units if unit.is_worker()])\n        \n    def get_state_vector(self):\n        return self.state.get_state_vector()\n    \n    def apply_agent_action(self, action):\n        self.workers.update_task_proportions(action)    \n        \n    def get_actions(self):\n        if self.debug:\n            print(\"Turn\", self.game_state.turn, file=sys.stderr)\n        worker_actions = self.workers.get_actions(self)\n        city_actions = self.cities.get_actions(self)\n        return worker_actions + city_actions\n","metadata":{"execution":{"iopub.execute_input":"2021-10-05T18:56:38.258468Z","iopub.status.busy":"2021-10-05T18:56:38.257751Z","iopub.status.idle":"2021-10-05T18:56:38.264559Z","shell.execute_reply":"2021-10-05T18:56:38.264994Z","shell.execute_reply.started":"2021-10-05T16:35:07.944519Z"},"papermill":{"duration":0.036082,"end_time":"2021-10-05T18:56:38.265141","exception":false,"start_time":"2021-10-05T18:56:38.229059","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The RLAgent class defines a Q-learning agent. The agent can either take in a settings and model object or, if none are provided, loads them from files.","metadata":{"papermill":{"duration":0.017534,"end_time":"2021-10-05T18:56:38.300437","exception":false,"start_time":"2021-10-05T18:56:38.282903","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%writefile RLAgent.py\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport numpy as np\nimport random\nimport pickle\n\nfrom tensorflow import keras\n\nclass RLAgent:\n    def __init__(self, settings=None, model=None):\n        if settings is None:\n            with open(\"agent_settings\", \"rb\") as settings_file:\n                settings = pickle.load(settings_file)\n        else:\n            with open(\"agent_settings\", \"wb\") as settings_file:\n                pickle.dump(settings, settings_file)\n                \n        if model is not None:\n            model.save(\"agent_model\")\n        self.action_space = self._get_action_space()\n        self.replays = []\n        self.state_size = 9\n        self.action_size = 55\n        self.gamma = settings[\"gamma\"]\n        self.epsilon = settings[\"epsilon\"]\n        self.training_mode_active = settings[\"training_mode_active\"]\n        self.reward_weights = settings[\"reward_weights\"]\n        self.num_explore_turns = settings[\"num_explore_turns\"]\n        self.explore_timer = 0\n        self.exploring = False\n        self.explore_action = None\n        self.q_net = keras.models.load_model(\"agent_model\")\n        self.target_net = self.q_net\n        \n    def _get_action_space(self):\n        action_space = []\n        for i in range(10):\n            for j in range(10 - i):\n                a, b = i / 10.0, j / 10.0\n                c = (10 - i - j) / 10.0\n                action_space.append([a, b, c])\n                    \n        return action_space\n    \n    def get_action(self, state):\n        if self.exploring:\n            self.explore_timer -= 1\n            if self.explore_timer == 0:\n                self.exploring = False\n            return self.explore_action\n        \n        if self.training_mode_active or np.random.rand() < self.epsilon:\n            self.exploring = True\n            self.explore_timer = self.num_explore_turns\n            self.explore_action = random.randrange(self.action_size)\n            return self.explore_action\n\n        q_vals = self.q_net.predict(state)\n        return np.argmax(q_vals[0])\n    \n    def lookup_action(self, code):\n        return self.action_space[code]\n    \n    def train(self): \n        for state, action, reward, next_state, last_turn in self.replays:\n            target = self.q_net.predict(state)\n            \n            if last_turn:\n                target[0][action] = reward\n            else:\n                t = self.target_net.predict(next_state)\n                target[0][action] = reward + self.gamma * np.amax(t[0])\n            \n            self.q_net.fit(state, target, epochs=1, verbose=0)\n        self.q_net.save(\"agent_model\")\n    \n    def add_replay(self, replay):\n        self.replays.append(replay)\n","metadata":{"execution":{"iopub.execute_input":"2021-10-05T18:56:38.342152Z","iopub.status.busy":"2021-10-05T18:56:38.339481Z","iopub.status.idle":"2021-10-05T18:56:38.347547Z","shell.execute_reply":"2021-10-05T18:56:38.346927Z","shell.execute_reply.started":"2021-10-05T16:35:07.964092Z"},"papermill":{"duration":0.029363,"end_time":"2021-10-05T18:56:38.347748","exception":false,"start_time":"2021-10-05T18:56:38.318385","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#%%writefile agent.py\nfrom lux.game import Game\nfrom lux.game_map import Cell, RESOURCE_TYPES, Position\nfrom lux.constants import Constants\nfrom lux.game_constants import GAME_CONSTANTS\nfrom lux import annotate\nfrom Controller import Controller\nfrom RLAgent import *\nimport math\nimport sys\nimport pickle\n\ndef calculate_reward(s, s_prime, reward_weights):\n        reward_vec = (s_prime[0] - s[0]) * np.array(reward_weights)\n        return np.sum(reward_vec)\n\n# we declare this global game_state object so that state persists across turns so we do not need to reinitialize it all the time\ngame_state = None\ncontroller = None\nstate = None\naction_code = None\nrl_agent = None\ndef agent(observation, configuration):\n    global game_state\n    global controller\n    global state\n    global action_code\n    global rl_agent\n\n    ### Do not edit ###\n    if observation[\"step\"] == 0:\n        game_state = Game()\n        game_state._initialize(observation[\"updates\"])\n        game_state._update(observation[\"updates\"][2:])\n        game_state.id = observation.player\n        \n        player = game_state.players[observation.player]\n        opponent = game_state.players[(observation.player + 1) % 2]\n        controller = Controller(game_state, player, opponent, False)\n        state = controller.get_state_vector()\n        \n        rl_agent = RLAgent(\"test\")\n        action_code = 54\n        reward = 0\n    else:\n        game_state._update(observation[\"updates\"])\n        player = game_state.players[observation.player]\n        opponent = game_state.players[(observation.player + 1) % 2]\n        controller.update(game_state, player, opponent)\n    \n        s_prime = controller.get_state_vector()\n        reward = calculate_reward(state, s_prime, rl_agent.reward_weights)\n        rl_agent.add_replay([state, action_code, reward, s_prime, game_state.turn == 359])\n        state = s_prime\n    \n        action_code = rl_agent.get_action(state)\n    action = rl_agent.lookup_action(action_code)\n#     print(action, state, reward)\n    controller.apply_agent_action(action)  \n    \n    if game_state.turn > 0 and game_state.turn % rl_agent.train_interval == 0:\n        rl_agent.train()\n\n    \n    return controller.get_actions()","metadata":{"execution":{"iopub.execute_input":"2021-10-05T18:56:38.414387Z","iopub.status.busy":"2021-10-05T18:56:38.413699Z","iopub.status.idle":"2021-10-05T18:56:43.912067Z","shell.execute_reply":"2021-10-05T18:56:43.910945Z","shell.execute_reply.started":"2021-10-05T16:35:07.984943Z"},"papermill":{"duration":5.536896,"end_time":"2021-10-05T18:56:43.912215","exception":false,"start_time":"2021-10-05T18:56:38.375319","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile agent.py\nfrom lux.game import Game\nfrom Controller import Controller\nfrom RLAgent import *\n\ndef calculate_reward(s, s_prime, reward_weights):\n        reward_vec = (s_prime[0] - s[0]) * np.array(reward_weights)\n        return np.sum(reward_vec)\n\n# we declare this global game_state object so that state persists across turns so we do not need to reinitialize it all the time\ngame_state = None\ncontroller = None\nstate = None\naction_code = None\nrl_agent = None\ndef agent(observation, configuration):\n    global game_state\n    global controller\n    global state\n    global action_code\n    global rl_agent\n\n    ### Do not edit ###\n    if observation[\"step\"] == 0:\n        game_state = Game()\n        game_state._initialize(observation[\"updates\"])\n        game_state._update(observation[\"updates\"][2:])\n        game_state.id = observation.player\n        \n        player = game_state.players[observation.player]\n        opponent = game_state.players[(observation.player + 1) % 2]\n        controller = Controller(game_state, player, opponent, False)\n        state = controller.get_state_vector()\n        \n        rl_agent = RLAgent()\n        action_code = 54\n        reward = 0\n\n    else:\n        game_state._update(observation[\"updates\"])\n        player = game_state.players[observation.player]\n        opponent = game_state.players[(observation.player + 1) % 2]\n        controller.update(game_state, player, opponent)\n    \n        s_prime = controller.get_state_vector()\n        reward = calculate_reward(state, s_prime, rl_agent.reward_weights)\n        rl_agent.add_replay([state, action_code, reward, s_prime, game_state.turn == 359])\n        state = s_prime\n    \n        action_code = rl_agent.get_action(state)\n    action = rl_agent.lookup_action(action_code)\n    # print(action, state, reward)\n    controller.apply_agent_action(action)\n\n    if (game_state.turn == 359):\n        rl_agent.train()  \n\n    return controller.get_actions()\n","metadata":{"execution":{"iopub.execute_input":"2021-10-05T18:56:43.95508Z","iopub.status.busy":"2021-10-05T18:56:43.954127Z","iopub.status.idle":"2021-10-05T18:56:43.960096Z","shell.execute_reply":"2021-10-05T18:56:43.959545Z","shell.execute_reply.started":"2021-10-05T16:35:14.46451Z"},"papermill":{"duration":0.029514,"end_time":"2021-10-05T18:56:43.960215","exception":false,"start_time":"2021-10-05T18:56:43.930701","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The base agent is used for training. It runs a strictly rule-based version of the agent and to compare the Q-learning agent to it's basic rule-based implementation.","metadata":{"papermill":{"duration":0.018657,"end_time":"2021-10-05T18:56:43.997826","exception":false,"start_time":"2021-10-05T18:56:43.979169","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%writefile base_agent.py\nfrom lux.game import Game\nfrom Controller import Controller\n\n# we declare this global game_state object so that state persists across turns so we do not need to reinitialize it all the time\ngame_state = None\ncontroller = None\n\ndef base_agent(observation, configuration):\n    global game_state\n    global controller\n\n    ### Do not edit ###\n    if observation[\"step\"] == 0:\n        game_state = Game()\n        game_state._initialize(observation[\"updates\"])\n        game_state._update(observation[\"updates\"][2:])\n        game_state.id = observation.player\n        \n        player = game_state.players[observation.player]\n        opponent = game_state.players[(observation.player + 1) % 2]\n        controller = Controller(game_state, player, opponent, False)\n        \n    else:\n        game_state._update(observation[\"updates\"])\n        player = game_state.players[observation.player]\n        opponent = game_state.players[(observation.player + 1) % 2]\n        controller.update(game_state, player, opponent)\n    \n    return controller.get_actions()","metadata":{"execution":{"iopub.execute_input":"2021-10-05T18:56:44.038865Z","iopub.status.busy":"2021-10-05T18:56:44.038345Z","iopub.status.idle":"2021-10-05T18:56:44.043637Z","shell.execute_reply":"2021-10-05T18:56:44.043193Z","shell.execute_reply.started":"2021-10-05T16:35:14.473467Z"},"papermill":{"duration":0.027095,"end_time":"2021-10-05T18:56:44.043788","exception":false,"start_time":"2021-10-05T18:56:44.016693","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The following cell trains the model. By toggling on \"training mode\", it makes the agent always choose a random action. This is to try to fill out the largest number of q-values for different actions taken in different states.\n\nSettings:\n* gamma: The discount rate.\n* epsilon: Controls the probability of performing a random action for exploration purposes.\n* num_explore_turns: Sets the number of turns for which to perform exploration action. This is necessary since the effects of an action can really only be observed over an interval of turns.\n* training_mode_active: Toggles training mode.\n* reward_weights: Sets weights for different state variables when calculating rewards. (See state vector description at top of notebook)","metadata":{"papermill":{"duration":0.019134,"end_time":"2021-10-05T18:56:44.082465","exception":false,"start_time":"2021-10-05T18:56:44.063331","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import pickle\nimport argparse\nfrom RLAgent import *\nfrom agent import *\nfrom base_agent import *\nfrom kaggle_environments import make\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.optimizers import Adam\n\ndef toggle_training_mode():\n    settings = None\n    with open(\"agent_settings\", \"rb\") as settings_file:\n        settings = pickle.load(settings_file)\n    settings[\"training_mode_active\"] = not settings[\"training_mode_active\"] \n    with open(\"agent_settings\", \"wb\") as settings_file:\n        pickle.dump(settings, settings_file)\n\ndef train(num_episodes):\n    toggle_training_mode()\n    for i in range(num_episodes):\n        env = make(\"lux_ai_2021\", configuration={\"seed\": 562124210, \"loglevel\": 1, \"annotations\": True}, debug=True)\n        steps = env.run([agent, base_agent])\n        print(\"Episode\", i + 1, \"of\", num_episodes, \"completed\")\n    toggle_training_mode()\n\nsettings = {\n    \"gamma\": 0.2,\n    \"epsilon\": 0.1,\n    \"num_explore_turns\": 10,\n    \"training_mode_active\": False,\n    \"reward_weights\": (10, 10, 10, 0, 0, 0, 0, 0, -5)\n}\n\noptimizer = Adam(learning_rate=0.2)\nmodel = Sequential()\nmodel.add(Dense(32, activation='relu', input_dim=9))\nmodel.add(Dense(55, activation='linear'))\nmodel.compile(loss='mse', optimizer=optimizer)\n\nrl_agent = RLAgent(settings=settings, model=model)\n\nnum_episodes = 10\ntrain(num_episodes)","metadata":{"execution":{"iopub.execute_input":"2021-10-05T18:56:44.130733Z","iopub.status.busy":"2021-10-05T18:56:44.130199Z","iopub.status.idle":"2021-10-05T19:05:33.421896Z","shell.execute_reply":"2021-10-05T19:05:33.422476Z","shell.execute_reply.started":"2021-10-05T16:35:14.496279Z"},"papermill":{"duration":529.321014,"end_time":"2021-10-05T19:05:33.422871","exception":false,"start_time":"2021-10-05T18:56:44.101857","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"env = make(\"lux_ai_2021\", configuration={\"seed\": 562124210, \"loglevel\": 1, \"annotations\": True}, debug=True)\nsteps = env.run([agent, base_agent])\nenv.render(mode=\"ipython\", width=1200, height=800)","metadata":{"execution":{"iopub.execute_input":"2021-10-05T19:05:33.476798Z","iopub.status.busy":"2021-10-05T19:05:33.47625Z","iopub.status.idle":"2021-10-05T19:06:33.993393Z","shell.execute_reply":"2021-10-05T19:06:33.993974Z","shell.execute_reply.started":"2021-10-05T18:08:56.147876Z"},"papermill":{"duration":60.548166,"end_time":"2021-10-05T19:06:33.994156","exception":false,"start_time":"2021-10-05T19:05:33.44599","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!tar -czf submission.tar.gz *","metadata":{"execution":{"iopub.execute_input":"2021-10-05T19:06:34.428984Z","iopub.status.busy":"2021-10-05T19:06:34.428338Z","iopub.status.idle":"2021-10-05T19:06:34.794342Z","shell.execute_reply":"2021-10-05T19:06:34.79496Z","shell.execute_reply.started":"2021-10-05T16:44:50.168436Z"},"papermill":{"duration":0.583526,"end_time":"2021-10-05T19:06:34.795146","exception":false,"start_time":"2021-10-05T19:06:34.21162","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]}]}