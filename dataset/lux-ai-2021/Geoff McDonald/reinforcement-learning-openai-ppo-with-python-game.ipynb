{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Lux AI Deep Reinforcement Learning Environment Example\nSee https://github.com/glmcdona/LuxPythonEnvGym for environment project and updates.\n\nThis is a python replica of the Lux game engine to speed up training. It reformats the agent problem into making a action decision per-unit for the team.","metadata":{}},{"cell_type":"code","source":"!pip install git+https://github.com/glmcdona/LuxPythonEnvGym.git\n!pip install kaggle-environments -U","metadata":{"execution":{"iopub.status.busy":"2021-10-01T05:52:49.383528Z","iopub.execute_input":"2021-10-01T05:52:49.383921Z","iopub.status.idle":"2021-10-01T05:53:06.819357Z","shell.execute_reply.started":"2021-10-01T05:52:49.383875Z","shell.execute_reply":"2021-10-01T05:53:06.818202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Use GPU if available\nNote: GPU provides very little speedup. I recommend using a CPU-only notebook usually.","metadata":{}},{"cell_type":"code","source":"import torch\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2021-10-01T05:53:06.821457Z","iopub.execute_input":"2021-10-01T05:53:06.821784Z","iopub.status.idle":"2021-10-01T05:53:06.828082Z","shell.execute_reply.started":"2021-10-01T05:53:06.821749Z","shell.execute_reply":"2021-10-01T05:53:06.82694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define the RL agent logic\nEdit this agent logic to implement your own observations, action space, and reward shaping.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"%%writefile agent_policy.py\nfrom luxai2021.game.match_controller import ActionSequence\nimport sys\nimport time\nfrom functools import partial  # pip install functools\n\nimport numpy as np\nfrom gym import spaces\nimport copy\nimport random\n\nfrom luxai2021.env.agent import Agent\nfrom luxai2021.game.actions import *\nfrom luxai2021.game.game_constants import GAME_CONSTANTS\nfrom luxai2021.game.position import Position\n\n\n# https://codereview.stackexchange.com/questions/28207/finding-the-closest-point-to-a-list-of-points\ndef closest_node(node, nodes):\n    dist_2 = np.sum((nodes - node) ** 2, axis=1)\n    return np.argmin(dist_2)\ndef furthest_node(node, nodes):\n    dist_2 = np.sum((nodes - node) ** 2, axis=1)\n    return np.argmax(dist_2)\n\ndef smart_transfer_to_nearby(game, team, unit_id, unit, target_type_restriction=None, **kwarg):\n    \"\"\"\n    Smart-transfers from the specified unit to a nearby neighbor. Prioritizes any\n    nearby carts first, then any worker. Transfers the resource type which the unit\n    has most of. Picks which cart/worker based on choosing a target that is most-full\n    but able to take the most amount of resources.\n\n    Args:\n        team ([type]): [description]\n        unit_id ([type]): [description]\n\n    Returns:\n        Action: Returns a TransferAction object, even if the request is an invalid\n                transfer. Use TransferAction.is_valid() to check validity.\n    \"\"\"\n\n    # Calculate how much resources could at-most be transferred\n    resource_type = None\n    resource_amount = 0\n    target_unit = None\n\n    if unit != None:\n        for type, amount in unit.cargo.items():\n            if amount > resource_amount:\n                resource_type = type\n                resource_amount = amount\n\n        # Find the best nearby unit to transfer to\n        unit_cell = game.map.get_cell_by_pos(unit.pos)\n        adjacent_cells = game.map.get_adjacent_cells(unit_cell)\n\n        \n        for c in adjacent_cells:\n            for id, u in c.units.items():\n                # Apply the unit type target restriction\n                if target_type_restriction == None or u.type == target_type_restriction:\n                    if u.team == team:\n                        # This unit belongs to our team, set it as the winning transfer target\n                        # if it's the best match.\n                        if target_unit is None:\n                            target_unit = u\n                        else:\n                            # Compare this unit to the existing target\n                            if target_unit.type == u.type:\n                                # Transfer to the target with the least capacity, but can accept\n                                # all of our resources\n                                if( u.get_cargo_space_left() >= resource_amount and \n                                    target_unit.get_cargo_space_left() >= resource_amount ):\n                                    # Both units can accept all our resources. Prioritize one that is most-full.\n                                    if u.get_cargo_space_left() < target_unit.get_cargo_space_left():\n                                        # This new target it better, it has less space left and can take all our\n                                        # resources\n                                        target_unit = u\n                                    \n                                elif( target_unit.get_cargo_space_left() >= resource_amount ):\n                                    # Don't change targets. Current one is best since it can take all\n                                    # the resources, but new target can't.\n                                    pass\n                                    \n                                elif( u.get_cargo_space_left() > target_unit.get_cargo_space_left() ):\n                                    # Change targets, because neither target can accept all our resources and \n                                    # this target can take more resources.\n                                    target_unit = u\n                            elif u.type == Constants.UNIT_TYPES.CART:\n                                # Transfer to this cart instead of the current worker target\n                                target_unit = u\n    \n    # Build the transfer action request\n    target_unit_id = None\n    if target_unit is not None:\n        target_unit_id = target_unit.id\n\n        # Update the transfer amount based on the room of the target\n        if target_unit.get_cargo_space_left() < resource_amount:\n            resource_amount = target_unit.get_cargo_space_left()\n    \n    return TransferAction(team, unit_id, target_unit_id, resource_type, resource_amount)\n\n########################################################################################################################\n# This is the Agent that you need to design for the competition\n########################################################################################################################\nclass AgentPolicy(Agent):\n    def __init__(self, mode=\"train\", model=None) -> None:\n        \"\"\"\n        Arguments:\n            mode: \"train\" or \"inference\", which controls if this agent is for training or not.\n            model: The pretrained model, or if None it will operate in training mode.\n        \"\"\"\n        super().__init__()\n        self.model = model\n        self.mode = mode\n        \n        self.stats = None\n        self.stats_last_game = None\n\n        # Define action and observation space\n        # They must be gym.spaces objects\n        # Example when using discrete actions:\n        self.actionSpaceMapUnits = [\n            partial(MoveAction, direction=Constants.DIRECTIONS.CENTER),  # This is the do-nothing action\n            partial(MoveAction, direction=Constants.DIRECTIONS.NORTH),\n            partial(MoveAction, direction=Constants.DIRECTIONS.WEST),\n            partial(MoveAction, direction=Constants.DIRECTIONS.SOUTH),\n            partial(MoveAction, direction=Constants.DIRECTIONS.EAST),\n            smart_transfer_to_nearby, # Transfer to nearby\n            SpawnCityAction,\n            #PillageAction,\n        ]\n        self.actionSpaceMapCities = [\n            SpawnWorkerAction,\n            SpawnCartAction,\n            ResearchAction,\n        ]\n\n        self.action_space = spaces.Discrete(max(len(self.actionSpaceMapUnits), len(self.actionSpaceMapCities)))\n        \n\n        # Observation space: (Basic minimum for a miner agent)\n        # Object:\n        #   1x is worker\n        #   1x is cart\n        #   1x is citytile\n        #\n        #   5x direction_nearest_wood\n        #   1x distance_nearest_wood\n        #   1x amount\n        #\n        #   5x direction_nearest_coal\n        #   1x distance_nearest_coal\n        #   1x amount\n        #\n        #   5x direction_nearest_uranium\n        #   1x distance_nearest_uranium\n        #   1x amount\n        #\n        #   5x direction_nearest_city\n        #   1x distance_nearest_city\n        #   1x amount of fuel\n        #\n        #   28x (the same as above, but direction, distance, and amount to the furthest of each)\n        #\n        #   5x direction_nearest_worker\n        #   1x distance_nearest_worker\n        #   1x amount of cargo\n        # Unit:\n        #   1x cargo size\n        # State:\n        #   1x is night\n        #   1x percent of game done\n        #   2x citytile counts [cur player, opponent]\n        #   2x worker counts [cur player, opponent]\n        #   2x cart counts [cur player, opponent]\n        #   1x research points [cur player]\n        #   1x researched coal [cur player]\n        #   1x researched uranium [cur player]\n        self.observation_shape = (3 + 7 * 5 * 2 + 1 + 1 + 1 + 2 + 2 + 2 + 3,)\n        self.observation_space = spaces.Box(low=0, high=1, shape=\n        self.observation_shape, dtype=np.float16)\n\n        self.object_nodes = {}\n\n    def get_agent_type(self):\n        \"\"\"\n        Returns the type of agent. Use AGENT for inference, and LEARNING for training a model.\n        \"\"\"\n        if self.mode == \"train\":\n            return Constants.AGENT_TYPE.LEARNING\n        else:\n            return Constants.AGENT_TYPE.AGENT\n\n    def get_observation(self, game, unit, city_tile, team, is_new_turn):\n        \"\"\"\n        Implements getting a observation from the current game for this unit or city\n        \"\"\"\n        observation_index = 0\n        if is_new_turn:\n            # It's a new turn this event. This flag is set True for only the first observation from each turn.\n            # Update any per-turn fixed observation space that doesn't change per unit/city controlled.\n\n            # Build a list of object nodes by type for quick distance-searches\n            self.object_nodes = {}\n\n            # Add resources\n            for cell in game.map.resources:\n                if cell.resource.type not in self.object_nodes:\n                    self.object_nodes[cell.resource.type] = np.array([[cell.pos.x, cell.pos.y]])\n                else:\n                    self.object_nodes[cell.resource.type] = np.concatenate(\n                        (\n                            self.object_nodes[cell.resource.type],\n                            [[cell.pos.x, cell.pos.y]]\n                        ),\n                        axis=0\n                    )\n\n            # Add your own and opponent units\n            for t in [team, (team + 1) % 2]:\n                for u in game.state[\"teamStates\"][team][\"units\"].values():\n                    key = str(u.type)\n                    if t != team:\n                        key = str(u.type) + \"_opponent\"\n\n                    if key not in self.object_nodes:\n                        self.object_nodes[key] = np.array([[u.pos.x, u.pos.y]])\n                    else:\n                        self.object_nodes[key] = np.concatenate(\n                            (\n                                self.object_nodes[key],\n                                [[u.pos.x, u.pos.y]]\n                            )\n                            , axis=0\n                        )\n\n            # Add your own and opponent cities\n            for city in game.cities.values():\n                for cells in city.city_cells:\n                    key = \"city\"\n                    if city.team != team:\n                        key = \"city_opponent\"\n\n                    if key not in self.object_nodes:\n                        self.object_nodes[key] = np.array([[cells.pos.x, cells.pos.y]])\n                    else:\n                        self.object_nodes[key] = np.concatenate(\n                            (\n                                self.object_nodes[key],\n                                [[cells.pos.x, cells.pos.y]]\n                            )\n                            , axis=0\n                        )\n\n        # Observation space: (Basic minimum for a miner agent)\n        # Object:\n        #   1x is worker\n        #   1x is cart\n        #   1x is citytile\n        #   5x direction_nearest_wood\n        #   1x distance_nearest_wood\n        #   1x amount\n        #\n        #   5x direction_nearest_coal\n        #   1x distance_nearest_coal\n        #   1x amount\n        #\n        #   5x direction_nearest_uranium\n        #   1x distance_nearest_uranium\n        #   1x amount\n        #\n        #   5x direction_nearest_city\n        #   1x distance_nearest_city\n        #   1x amount of fuel\n        #\n        #   5x direction_nearest_worker\n        #   1x distance_nearest_worker\n        #   1x amount of cargo\n        #\n        #   28x (the same as above, but direction, distance, and amount to the furthest of each)\n        #\n        # Unit:\n        #   1x cargo size\n        # State:\n        #   1x is night\n        #   1x percent of game done\n        #   2x citytile counts [cur player, opponent]\n        #   2x worker counts [cur player, opponent]\n        #   2x cart counts [cur player, opponent]\n        #   1x research points [cur player]\n        #   1x researched coal [cur player]\n        #   1x researched uranium [cur player]\n        obs = np.zeros(self.observation_shape)\n        \n        # Update the type of this object\n        #   1x is worker\n        #   1x is cart\n        #   1x is citytile\n        observation_index = 0\n        if unit is not None:\n            if unit.type == Constants.UNIT_TYPES.WORKER:\n                obs[observation_index] = 1.0 # Worker\n            else:\n                obs[observation_index+1] = 1.0 # Cart\n        if city_tile is not None:\n            obs[observation_index+2] = 1.0 # CityTile\n        observation_index += 3\n        \n        pos = None\n        if unit is not None:\n            pos = unit.pos\n        else:\n            pos = city_tile.pos\n\n        if pos is None:\n            observation_index += 7 * 5 * 2\n        else:\n            # Encode the direction to the nearest objects\n            #   5x direction_nearest\n            #   1x distance\n            for distance_function in [closest_node, furthest_node]:\n                for key in [\n                    Constants.RESOURCE_TYPES.WOOD,\n                    Constants.RESOURCE_TYPES.COAL,\n                    Constants.RESOURCE_TYPES.URANIUM,\n                    \"city\",\n                    str(Constants.UNIT_TYPES.WORKER)]:\n                    # Process the direction to and distance to this object type\n\n                    # Encode the direction to the nearest object (excluding itself)\n                    #   5x direction\n                    #   1x distance\n                    if key in self.object_nodes:\n                        if (\n                                (key == \"city\" and city_tile is not None) or\n                                (unit is not None and str(unit.type) == key and len(game.map.get_cell_by_pos(unit.pos).units) <= 1 )\n                        ):\n                            # Filter out the current unit from the closest-search\n                            closest_index = closest_node((pos.x, pos.y), self.object_nodes[key])\n                            filtered_nodes = np.delete(self.object_nodes[key], closest_index, axis=0)\n                        else:\n                            filtered_nodes = self.object_nodes[key]\n\n                        if len(filtered_nodes) == 0:\n                            # No other object of this type\n                            obs[observation_index + 5] = 1.0\n                        else:\n                            # There is another object of this type\n                            closest_index = distance_function((pos.x, pos.y), filtered_nodes)\n\n                            if closest_index is not None and closest_index >= 0:\n                                closest = filtered_nodes[closest_index]\n                                closest_position = Position(closest[0], closest[1])\n                                direction = pos.direction_to(closest_position)\n                                mapping = {\n                                    Constants.DIRECTIONS.CENTER: 0,\n                                    Constants.DIRECTIONS.NORTH: 1,\n                                    Constants.DIRECTIONS.WEST: 2,\n                                    Constants.DIRECTIONS.SOUTH: 3,\n                                    Constants.DIRECTIONS.EAST: 4,\n                                }\n                                obs[observation_index + mapping[direction]] = 1.0  # One-hot encoding direction\n\n                                # 0 to 1 distance\n                                distance = pos.distance_to(closest_position)\n                                obs[observation_index + 5] = min(distance / 20.0, 1.0)\n\n                                # 0 to 1 value (amount of resource, cargo for unit, or fuel for city)\n                                if key == \"city\":\n                                    # City fuel as % of upkeep for 200 turns\n                                    c = game.cities[game.map.get_cell_by_pos(closest_position).city_tile.city_id]\n                                    obs[observation_index + 6] = min(\n                                        c.fuel / (c.get_light_upkeep() * 200.0),\n                                        1.0\n                                    )\n                                elif key in [Constants.RESOURCE_TYPES.WOOD, Constants.RESOURCE_TYPES.COAL,\n                                             Constants.RESOURCE_TYPES.URANIUM]:\n                                    # Resource amount\n                                    obs[observation_index + 6] = min(\n                                        game.map.get_cell_by_pos(closest_position).resource.amount / 500,\n                                        1.0\n                                    )\n                                else:\n                                    # Unit cargo\n                                    obs[observation_index + 6] = min(\n                                        next(iter(game.map.get_cell_by_pos(\n                                            closest_position).units.values())).get_cargo_space_left() / 100,\n                                        1.0\n                                    )\n\n                    observation_index += 7\n\n        if unit is not None:\n            # Encode the cargo space\n            #   1x cargo size\n            obs[observation_index] = unit.get_cargo_space_left() / GAME_CONSTANTS[\"PARAMETERS\"][\"RESOURCE_CAPACITY\"][\n                \"WORKER\"]\n            observation_index += 1\n        else:\n            observation_index += 1\n\n        # Game state observations\n\n        #   1x is night\n        obs[observation_index] = game.is_night()\n        observation_index += 1\n\n        #   1x percent of game done\n        obs[observation_index] = game.state[\"turn\"] / GAME_CONSTANTS[\"PARAMETERS\"][\"MAX_DAYS\"]\n        observation_index += 1\n\n        #   2x citytile counts [cur player, opponent]\n        #   2x worker counts [cur player, opponent]\n        #   2x cart counts [cur player, opponent]\n        max_count = 30\n        for key in [\"city\", str(Constants.UNIT_TYPES.WORKER), str(Constants.UNIT_TYPES.CART)]:\n            if key in self.object_nodes:\n                obs[observation_index] = len(self.object_nodes[key]) / max_count\n            if (key + \"_opponent\") in self.object_nodes:\n                obs[observation_index + 1] = len(self.object_nodes[(key + \"_opponent\")]) / max_count\n            observation_index += 2\n\n        #   1x research points [cur player]\n        #   1x researched coal [cur player]\n        #   1x researched uranium [cur player]\n        obs[observation_index] = game.state[\"teamStates\"][team][\"researchPoints\"] / 200.0\n        obs[observation_index+1] = float(game.state[\"teamStates\"][team][\"researched\"][\"coal\"])\n        obs[observation_index+2] = float(game.state[\"teamStates\"][team][\"researched\"][\"uranium\"])\n\n        return obs\n\n    def action_code_to_action(self, action_code, game, unit=None, city_tile=None, team=None):\n        \"\"\"\n        Takes an action in the environment according to actionCode:\n            actionCode: Index of action to take into the action array.\n        Returns: An action.\n        \"\"\"\n        # Map actionCode index into to a constructed Action object\n        try:\n            x = None\n            y = None\n            if city_tile is not None:\n                x = city_tile.pos.x\n                y = city_tile.pos.y\n            elif unit is not None:\n                x = unit.pos.x\n                y = unit.pos.y\n            \n            if city_tile != None:\n                action =  self.actionSpaceMapCities[action_code%len(self.actionSpaceMapCities)](\n                    game=game,\n                    unit_id=unit.id if unit else None,\n                    unit=unit,\n                    city_id=city_tile.city_id if city_tile else None,\n                    citytile=city_tile,\n                    team=team,\n                    x=x,\n                    y=y\n                )\n\n                # If the city action is invalid, default to research action automatically\n                if not action.is_valid(game, actions_validated=[]):\n                    action = ResearchAction(\n                        game=game,\n                        unit_id=unit.id if unit else None,\n                        unit=unit,\n                        city_id=city_tile.city_id if city_tile else None,\n                        citytile=city_tile,\n                        team=team,\n                        x=x,\n                        y=y\n                    )\n            else:\n                action =  self.actionSpaceMapUnits[action_code%len(self.actionSpaceMapUnits)](\n                    game=game,\n                    unit_id=unit.id if unit else None,\n                    unit=unit,\n                    city_id=city_tile.city_id if city_tile else None,\n                    citytile=city_tile,\n                    team=team,\n                    x=x,\n                    y=y\n                )\n            \n            return action\n        except Exception as e:\n            # Not a valid action\n            print(e)\n            return None\n\n    def take_action(self, action_code, game, unit=None, city_tile=None, team=None):\n        \"\"\"\n        Takes an action in the environment according to actionCode:\n            actionCode: Index of action to take into the action array.\n        \"\"\"\n        action = self.action_code_to_action(action_code, game, unit, city_tile, team)\n        self.match_controller.take_action(action)\n    \n    def game_start(self, game):\n        \"\"\"\n        This funciton is called at the start of each game. Use this to\n        reset and initialize per game. Note that self.team may have\n        been changed since last game. The game map has been created\n        and starting units placed.\n\n        Args:\n            game ([type]): Game.\n        \"\"\"\n        self.last_generated_fuel = game.stats[\"teamStats\"][self.team][\"fuelGenerated\"]\n        self.last_resources_collected = copy.deepcopy(game.stats[\"teamStats\"][self.team][\"resourcesCollected\"])\n        if self.stats != None:\n            self.stats_last_game =  self.stats\n        self.stats = {\n            \"rew/r_total\": 0,\n            \"rew/r_wood\": 0,\n            \"rew/r_coal\": 0,\n            \"rew/r_uranium\": 0,\n            \"rew/r_research\": 0,\n            \"rew/r_city_tiles_end\": 0,\n            \"rew/r_fuel_collected\":0,\n            \"rew/r_units\":0,\n            \"rew/r_city_tiles\":0,\n            \"game/turns\": 0,\n            \"game/research\": 0,\n            \"game/unit_count\": 0,\n            \"game/cart_count\": 0,\n            \"game/city_count\": 0,\n            \"game/city_tiles\": 0,\n            \"game/wood_rate_mined\": 0,\n            \"game/coal_rate_mined\": 0,\n            \"game/uranium_rate_mined\": 0,\n        }\n        self.is_last_turn = False\n\n        # Calculate starting map resources\n        type_map = {\n            Constants.RESOURCE_TYPES.WOOD: \"WOOD\",\n            Constants.RESOURCE_TYPES.COAL: \"COAL\",\n            Constants.RESOURCE_TYPES.URANIUM: \"URANIUM\",\n        }\n\n        self.fuel_collected_last = 0\n        self.fuel_start = {}\n        self.fuel_last = {}\n        for type, type_upper in type_map.items():\n            self.fuel_start[type] = 0\n            self.fuel_last[type] = 0\n            for c in game.map.resources_by_type[type]:\n                self.fuel_start[type] += c.resource.amount * game.configs[\"parameters\"][\"RESOURCE_TO_FUEL_RATE\"][type_upper]\n\n        self.research_last = 0\n        self.units_last = 0\n        self.city_tiles_last = 0\n\n    def get_reward(self, game, is_game_finished, is_new_turn, is_game_error):\n        \"\"\"\n        Returns the reward function for this step of the game.\n        \"\"\"\n        if is_game_error:\n            # Game environment step failed, assign a game lost reward to not incentivise this\n            print(\"Game failed due to error\")\n            return -1.0\n\n        if not is_new_turn and not is_game_finished:\n            # Only apply rewards at the start of each turn\n            return 0\n\n        # Get some basic stats\n        unit_count = len(game.state[\"teamStates\"][self.team % 2][\"units\"])\n        cart_count = 0\n        for id, u in game.state[\"teamStates\"][self.team % 2][\"units\"].items():\n            if u.type == Constants.UNIT_TYPES.CART:\n                cart_count += 1\n\n        unit_count_opponent = len(game.state[\"teamStates\"][(self.team + 1) % 2][\"units\"])\n        research = min(game.state[\"teamStates\"][self.team][\"researchPoints\"], 200.0) # Cap research points at 200\n        city_count = 0\n        city_count_opponent = 0\n        city_tile_count = 0\n        city_tile_count_opponent = 0\n        for city in game.cities.values():\n            if city.team == self.team:\n                city_count += 1\n            else:\n                city_count_opponent += 1\n\n            for cell in city.city_cells:\n                if city.team == self.team:\n                    city_tile_count += 1\n                else:\n                    city_tile_count_opponent += 1\n        \n        # Basic stats\n        self.stats[\"game/research\"] = research\n        self.stats[\"game/city_tiles\"] = city_tile_count\n        self.stats[\"game/city_count\"] = city_count\n        self.stats[\"game/unit_count\"] = unit_count\n        self.stats[\"game/cart_count\"] = cart_count\n        self.stats[\"game/turns\"] = game.state[\"turn\"]\n\n        rewards = {}\n\n        # Give up to 1.0 reward for each resource based on % of total mined.\n        type_map = {\n            Constants.RESOURCE_TYPES.WOOD: \"WOOD\",\n            Constants.RESOURCE_TYPES.COAL: \"COAL\",\n            Constants.RESOURCE_TYPES.URANIUM: \"URANIUM\",\n        }\n        fuel_now = {}\n        for type, type_upper in type_map.items():\n            fuel_now = game.stats[\"teamStats\"][self.team][\"resourcesCollected\"][type] * game.configs[\"parameters\"][\"RESOURCE_TO_FUEL_RATE\"][type_upper]\n            rewards[\"rew/r_%s\" % type] = (fuel_now - self.fuel_last[type]) / self.fuel_start[type]\n            self.stats[\"game/%s_rate_mined\" % type] = fuel_now / self.fuel_start[type]\n            self.fuel_last[type] = fuel_now\n        \n        # Give more incentive for coal and uranium\n        rewards[\"rew/r_%s\" % Constants.RESOURCE_TYPES.COAL] *= 2\n        rewards[\"rew/r_%s\" % Constants.RESOURCE_TYPES.URANIUM] *= 4\n        \n        # Give a reward based on amount of fuel collected. 1.0 reward for each 20K fuel gathered.\n        fuel_collected = game.stats[\"teamStats\"][self.team][\"fuelGenerated\"]\n        rewards[\"rew/r_fuel_collected\"] = ( (fuel_collected - self.fuel_collected_last) / 20000 )\n        self.fuel_collected_last = fuel_collected\n\n        # Give a reward for unit creation/death. 0.05 reward per unit.\n        rewards[\"rew/r_units\"] = (unit_count - self.units_last) * 0.05\n        self.units_last = unit_count\n\n        # Give a reward for unit creation/death. 0.1 reward per city.\n        rewards[\"rew/r_city_tiles\"] = (city_tile_count - self.city_tiles_last) * 0.1\n        self.city_tiles_last = city_tile_count\n\n        # Tiny reward for research to help. Up to 0.5 reward for this.\n        rewards[\"rew/r_research\"] = (research - self.research_last) / (200 * 2)\n        self.research_last = research\n        \n        # Give a reward up to around 50.0 based on number of city tiles at the end of the game\n        rewards[\"rew/r_city_tiles_end\"] = 0\n        if is_game_finished:\n            self.is_last_turn = True\n            rewards[\"rew/r_city_tiles_end\"] = city_tile_count\n        \n        \n        # Update the stats and total reward\n        reward = 0\n        for name, value in rewards.items():\n            self.stats[name] += value\n            reward += value\n        self.stats[\"rew/r_total\"] += reward\n\n        # Print the final game stats sometimes\n        if is_game_finished and random.random() <= 0.15:\n            stats_string = []\n            for key, value in self.stats.items():\n                stats_string.append(\"%s=%.2f\" % (key, value))\n            print(\",\".join(stats_string))\n\n\n        return reward\n        \n    \n\n    def process_turn(self, game, team):\n        \"\"\"\n        Decides on a set of actions for the current turn. Not used in training, only inference.\n        Returns: Array of actions to perform.\n        \"\"\"\n        start_time = time.time()\n        actions = []\n        new_turn = True\n\n        # Inference the model per-unit\n        units = game.state[\"teamStates\"][team][\"units\"].values()\n        for unit in units:\n            if unit.can_act():\n                obs = self.get_observation(game, unit, None, unit.team, new_turn)\n                action_code, _states = self.model.predict(obs, deterministic=False)\n                if action_code is not None:\n                    actions.append(\n                        self.action_code_to_action(action_code, game=game, unit=unit, city_tile=None, team=unit.team))\n                new_turn = False\n\n        # Inference the model per-city\n        cities = game.cities.values()\n        for city in cities:\n            if city.team == team:\n                for cell in city.city_cells:\n                    city_tile = cell.city_tile\n                    if city_tile.can_act():\n                        obs = self.get_observation(game, None, city_tile, city.team, new_turn)\n                        action_code, _states = self.model.predict(obs, deterministic=False)\n                        if action_code is not None:\n                            actions.append(\n                                self.action_code_to_action(action_code, game=game, unit=None, city_tile=city_tile,\n                                                           team=city.team))\n                        new_turn = False\n\n        time_taken = time.time() - start_time\n        if time_taken > 0.5:  # Warn if larger than 0.5 seconds.\n            print(\"WARNING: Inference took %.3f seconds for computing actions. Limit is 1 second.\" % time_taken,\n                  file=sys.stderr)\n\n        return actions\n\n","metadata":{"execution":{"iopub.status.busy":"2021-10-01T05:53:06.831635Z","iopub.execute_input":"2021-10-01T05:53:06.832123Z","iopub.status.idle":"2021-10-01T05:53:06.850572Z","shell.execute_reply.started":"2021-10-01T05:53:06.83207Z","shell.execute_reply":"2021-10-01T05:53:06.849278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build the environment for training\n\nNotes on metrics:\n* An Episode is a single game between your RL agent and it's opponent. This is generally 360 turns, spanning more than 360 unit + city decision steps.\n* Mean episode length (ep_len_mean) is the number of decision made per game. The larger this gets, means that it is making more unit + city decision per game, meaning that more units and cities were alive for longer during the game.\n* Episode reward mean (ep_rew_mean), is set up as micro-reward funciton for faster learning. Per turn it gets a small reward based on the number of cities and units alive. It gets a really big reward based on the number of cities and units alive at the end of the game.","metadata":{}},{"cell_type":"code","source":"import argparse\nimport glob\nimport os\nimport random\nfrom typing import Callable\n\nfrom stable_baselines3 import PPO  # pip install stable-baselines3\nfrom stable_baselines3.common.callbacks import CheckpointCallback\nfrom stable_baselines3.common.utils import set_random_seed\nfrom stable_baselines3.common.vec_env import SubprocVecEnv\n\nfrom importlib import reload\nimport agent_policy\nreload(agent_policy) # Reload the file from disk incase the above agent-writing cell block was edited\nfrom agent_policy import AgentPolicy\n\nfrom luxai2021.env.agent import Agent\nfrom luxai2021.env.lux_env import LuxEnvironment\nfrom luxai2021.game.constants import LuxMatchConfigs_Default\n\n\n# Default Lux configs\nconfigs = LuxMatchConfigs_Default\n\n# Create a default opponent agent\nopponent = Agent()\n\n# Create a RL agent in training mode\nplayer = AgentPolicy(mode=\"train\")\n\n# Create the Lux environment\nenv = LuxEnvironment(configs=configs,\n                     learning_agent=player,\n                     opponent_agent=opponent)\n\n# Define the model, you can pick other RL algos from Stable Baselines3 instead if you like\nmodel = PPO(\"MlpPolicy\",\n                env,\n                verbose=1,\n                tensorboard_log=\"./lux_tensorboard/\",\n                learning_rate=0.001,\n                gamma=0.999,\n                gae_lambda=0.95,\n                batch_size=2048 * 8,\n                n_steps=2048 * 8\n            )\n\n# Define a learning rate schedule\n# (number of steps, learning_rate)\nschedule = [\n    #(2000000, 0.01),\n    (6000000, 0.001),\n    (6000000, 0.0001),\n]","metadata":{"execution":{"iopub.status.busy":"2021-10-01T05:53:06.852939Z","iopub.execute_input":"2021-10-01T05:53:06.85341Z","iopub.status.idle":"2021-10-01T05:53:06.90787Z","shell.execute_reply.started":"2021-10-01T05:53:06.853365Z","shell.execute_reply":"2021-10-01T05:53:06.902531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train the agent against a dummy opponent","metadata":{}},{"cell_type":"code","source":"from stable_baselines3.common.utils import get_schedule_fn\n\nprint(\"Training model...\")\nrun_id = 1\n\n# Save a checkpoint every 1M steps\ncheckpoint_callback = CheckpointCallback(save_freq=1000000,\n                                         save_path='./models/',\n                                         name_prefix=f'rl_model_{run_id}')\n\n# Train the policy\nfor steps, learning_rate in schedule:\n    model.lr_schedule = get_schedule_fn(learning_rate)\n    model.learn(total_timesteps=steps,\n                callback=checkpoint_callback,\n                reset_num_timesteps = False)\n\n# Save final model\nmodel.save(path=f'models/model.zip')\n\nprint(\"Done training model.\")","metadata":{"execution":{"iopub.status.busy":"2021-10-01T05:53:06.909706Z","iopub.execute_input":"2021-10-01T05:53:06.910205Z","iopub.status.idle":"2021-10-01T05:55:36.032314Z","shell.execute_reply.started":"2021-10-01T05:53:06.91014Z","shell.execute_reply":"2021-10-01T05:55:36.031468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save final model\nmodel.save(path=f'models/model.zip')\n\nprint(\"Done training model.\")","metadata":{"execution":{"iopub.status.busy":"2021-10-01T05:55:36.033374Z","iopub.execute_input":"2021-10-01T05:55:36.033765Z","iopub.status.idle":"2021-10-01T05:55:36.052705Z","shell.execute_reply.started":"2021-10-01T05:55:36.033737Z","shell.execute_reply":"2021-10-01T05:55:36.051593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Set up a Kaggle Submission and lux replay environment for the agent","metadata":{}},{"cell_type":"code","source":"\"\"\"\nThis downloads two required python package dependencies that are not pre-installed\nby Kaggle yet.\n\nThis places the following two packages in the current working directory:\n    luxai2021\n    stable_baselines3\n\"\"\"\n\nimport os\nimport shutil\nimport subprocess\nimport tempfile\n\ndef localize_package(git, branch, folder):\n    if os.path.exists(folder):\n        print(\"Already localized %s\" % folder)\n    else:\n        # https://stackoverflow.com/questions/51239168/how-to-download-single-file-from-a-git-repository-using-python\n        # Create temporary dir\n        t = tempfile.mkdtemp()\n\n        args = ['git', 'clone', '--depth=1', git, t, '-b', branch]\n        res = subprocess.Popen(args, stdout=subprocess.PIPE)\n        output, _error = res.communicate()\n\n        if not _error:\n            print(output)\n        else:\n            print(_error)\n        \n        # Copy desired file from temporary dir\n        shutil.move(os.path.join(t, folder), '.')\n        # Remove temporary dir\n        shutil.rmtree(t, ignore_errors=True)\n\nlocalize_package('https://github.com/glmcdona/LuxPythonEnvGym.git', 'main', 'luxai2021')\nlocalize_package('https://github.com/glmcdona/LuxPythonEnvGym.git', 'main', 'kaggle_submissions')\nlocalize_package('https://github.com/DLR-RM/stable-baselines3.git', 'master', 'stable_baselines3')","metadata":{"execution":{"iopub.status.busy":"2021-10-01T05:55:36.054108Z","iopub.execute_input":"2021-10-01T05:55:36.054416Z","iopub.status.idle":"2021-10-01T05:55:39.25464Z","shell.execute_reply.started":"2021-10-01T05:55:36.054385Z","shell.execute_reply":"2021-10-01T05:55:39.253484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Move the dependent packages into kaggle submissions\n!mv luxai2021 kaggle_submissions\n!mv stable_baselines3 kaggle_submissions\n!rm ./kaggle_submissions/agent_policy.py\n!cp agent_policy.py kaggle_submissions\n\n# Copy the agent and model to the submission \n!cp ./agent_policy.py kaggle_submissions\n!cp ./models/model.zip kaggle_submissions\n\n!ls kaggle_submissions","metadata":{"execution":{"iopub.status.busy":"2021-10-01T05:55:39.258267Z","iopub.execute_input":"2021-10-01T05:55:39.258742Z","iopub.status.idle":"2021-10-01T05:55:44.741181Z","shell.execute_reply.started":"2021-10-01T05:55:39.258693Z","shell.execute_reply":"2021-10-01T05:55:44.739948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from kaggle_environments import make\nimport json\n# run another match but with our empty agent\nenv = make(\"lux_ai_2021\", configuration={\"seed\": 5621242, \"loglevel\": 2, \"annotations\": True}, debug=True)\n\n# Play the environment where the RL agent plays against itself\nsteps = env.run([\"./kaggle_submissions/main.py\", \"./kaggle_submissions/main.py\"])","metadata":{"execution":{"iopub.status.busy":"2021-10-01T05:55:44.745897Z","iopub.execute_input":"2021-10-01T05:55:44.746272Z","iopub.status.idle":"2021-10-01T05:55:50.013376Z","shell.execute_reply.started":"2021-10-01T05:55:44.746237Z","shell.execute_reply":"2021-10-01T05:55:50.012056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Render the match\nenv.render(mode=\"ipython\", width=1200, height=800)","metadata":{"execution":{"iopub.status.busy":"2021-10-01T05:55:50.015399Z","iopub.execute_input":"2021-10-01T05:55:50.015844Z","iopub.status.idle":"2021-10-01T05:55:50.049389Z","shell.execute_reply.started":"2021-10-01T05:55:50.015793Z","shell.execute_reply":"2021-10-01T05:55:50.04832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare and submit the kaggle submission","metadata":{}},{"cell_type":"code","source":"!tar -czf submission.tar.gz -C kaggle_submissions .\n!ls","metadata":{"execution":{"iopub.status.busy":"2021-10-01T05:55:50.050977Z","iopub.execute_input":"2021-10-01T05:55:50.051402Z","iopub.status.idle":"2021-10-01T05:55:51.967847Z","shell.execute_reply.started":"2021-10-01T05:55:50.051369Z","shell.execute_reply":"2021-10-01T05:55:51.966778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}