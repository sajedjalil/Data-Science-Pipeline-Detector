{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!cp -r ../input/lux-ai-2021/* .\n!pip install kaggle-environments -U","metadata":{"execution":{"iopub.status.busy":"2021-12-20T11:26:34.82352Z","iopub.execute_input":"2021-12-20T11:26:34.823792Z","iopub.status.idle":"2021-12-20T11:26:43.042732Z","shell.execute_reply.started":"2021-12-20T11:26:34.823764Z","shell.execute_reply":"2021-12-20T11:26:43.041944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from lux.game import Game\nfrom lux.game_map import Cell, RESOURCE_TYPES, Position\nfrom lux.constants import Constants\nfrom lux.game_constants import GAME_CONSTANTS\nfrom lux import annotate\nimport math\nimport random\nimport numpy as np\nfrom tqdm import tqdm\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras import layers\nfrom keras.models import Sequential, Model\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.compat.v1 import ConfigProto, Session\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow import keras\nfrom kaggle_environments import make\nimport tensorflow as tf\nseed = 562124210","metadata":{"tags":[],"execution":{"iopub.status.busy":"2021-12-20T11:26:43.04467Z","iopub.execute_input":"2021-12-20T11:26:43.044973Z","iopub.status.idle":"2021-12-20T11:26:48.830304Z","shell.execute_reply.started":"2021-12-20T11:26:43.044932Z","shell.execute_reply":"2021-12-20T11:26:48.829387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Solo se le pasan las acciones del jugador aliado\ndef traductor_updates(updates,player_id):\n    research = [0,0] \n    game_map = []\n    cities = {}\n            \n    #inicializamos el array\n    game_map = []\n    for i in range(12):\n        game_map.append([])\n        for e in range(12):\n            game_map[i].append([])\n            for o in range(21):\n                game_map[i][e].append(0)\n    #hacemos la traducci칩n\n    for i in updates:\n        temp = i.split(\" \")\n        if temp[0] == \"rp\":\n            if int(temp[1]) == player_id:\n                research[0] = int(temp[2])\n            else:\n                research[1] = int(temp[2])\n        elif temp[0] == \"r\":\n            if temp[1] == \"wood\":\n                game_map[int(temp[2])][int(temp[3])][0] = int(temp[4])\n            elif temp[1] == \"coal\":\n                game_map[int(temp[2])][int(temp[3])][1] = int(temp[4])\n            elif temp[1] == \"uranium\":\n                game_map[int(temp[2])][int(temp[3])][2] = int(temp[4])\n        #almacenamos las ciudades para obtener su fuel y fuelupkeep cuando aparezcan sus ct\n        elif temp[0] == \"c\":\n            cities[temp[2]] = [int(temp[3]),int(temp[4])]\n        elif temp[0] == \"ct\":\n            if int(temp[1]) == player_id:\n                #tiene ciudad aliada\n                game_map[int(temp[3])][int(temp[4])][3] = 1\n                #Fuel\n                game_map[int(temp[3])][int(temp[4])][4] = cities[temp[2]][0]\n                #FuelUpkeep\n                game_map[int(temp[3])][int(temp[4])][5] = cities[temp[2]][1]\n                #cd\n                game_map[int(temp[3])][int(temp[4])][6] = int(temp[5])\n            else:\n                #tiene ciudad enemiga\n                game_map[int(temp[3])][int(temp[4])][7] = 1\n                #Fuel\n                game_map[int(temp[3])][int(temp[4])][8] = cities[temp[2]][0]\n                #FuelUpkeep\n                game_map[int(temp[3])][int(temp[4])][9] = cities[temp[2]][1]\n                #cd\n                game_map[int(temp[3])][int(temp[4])][10] = int(temp[5])\n        elif temp[0] == \"u\":\n            if int(temp[2]) == player_id:\n                #num unidades aliadas\n                game_map[int(temp[4])][int(temp[5])][11] += 1\n                #cooldown = 0\n                if int(temp[6]) == 0:\n                    game_map[int(temp[4])][int(temp[5])][12] += 1\n                #wood\n                game_map[int(temp[4])][int(temp[5])][13] = int(temp[7])\n                #coal\n                game_map[int(temp[4])][int(temp[5])][14] = int(temp[8])\n                #uranium\n                game_map[int(temp[4])][int(temp[5])][15] = int(temp[9])\n            else:\n                #num unidades enemigas\n                game_map[int(temp[4])][int(temp[5])][16] += 1\n                #cd 0\n                if int(temp[6]) == 0:\n                    game_map[int(temp[4])][int(temp[5])][17] += 1\n                #wood\n                game_map[int(temp[4])][int(temp[5])][18] = int(temp[7])\n                #coal\n                game_map[int(temp[4])][int(temp[5])][19] = int(temp[8])\n                #uranium\n                game_map[int(temp[4])][int(temp[5])][20] = int(temp[9])\n    return research, np.array(game_map)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2021-12-20T11:26:48.831793Z","iopub.execute_input":"2021-12-20T11:26:48.832051Z","iopub.status.idle":"2021-12-20T11:26:48.853202Z","shell.execute_reply.started":"2021-12-20T11:26:48.832021Z","shell.execute_reply":"2021-12-20T11:26:48.852194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def construct_q_network(state_dim=12,features_update = 21,output_dim=8):\n    shape=(state_dim,state_dim,features_update)\n    \n    \n    inputs1 = layers.Input(shape=(3))\n    hidden1 = layers.Dense(6,activation='relu',input_shape=(32,3))(inputs1)\n    hidden2 = layers.Dense(6,activation='relu')(hidden1)\n    hidden3 = layers.Dense(6,activation='linear')(hidden2)\n    \n    inputs2 = layers.Input(shape=(state_dim,state_dim,features_update))\n    conv1 = layers.Conv2D(12, (5, 5), activation='relu', input_shape=(12, 12, 21))(inputs2)\n    conv2 = layers.Conv2D(32, (3, 3), activation='relu', input_shape=(12, 12, 21))(conv1)\n    conv3 = layers.Conv2D(12, (3, 3), activation='relu', input_shape=(12, 12, 21))(conv2)\n    flatten = layers.Flatten()(conv3)\n    \n    combined = layers.concatenate([hidden3,flatten])\n    dense = layers.Dense(1152)(combined)\n    output = layers.Reshape((12,12,8))(dense)\n    \n    model = Model(inputs=[inputs1, inputs2], outputs=output)\n    model.compile(optimizer = \"adam\", loss=\"mse\", metrics = [\"accuracy\"])\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-12-20T11:26:48.855511Z","iopub.execute_input":"2021-12-20T11:26:48.855821Z","iopub.status.idle":"2021-12-20T11:26:48.883959Z","shell.execute_reply.started":"2021-12-20T11:26:48.855784Z","shell.execute_reply":"2021-12-20T11:26:48.883117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"q_network_orig = construct_q_network()\nq_network_temp = construct_q_network()","metadata":{"execution":{"iopub.status.busy":"2021-12-20T11:26:48.885576Z","iopub.execute_input":"2021-12-20T11:26:48.88593Z","iopub.status.idle":"2021-12-20T11:26:49.133387Z","shell.execute_reply.started":"2021-12-20T11:26:48.885892Z","shell.execute_reply":"2021-12-20T11:26:49.132811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"q_network_orig.summary()","metadata":{"execution":{"iopub.status.busy":"2021-12-20T11:26:49.134484Z","iopub.execute_input":"2021-12-20T11:26:49.134967Z","iopub.status.idle":"2021-12-20T11:26:49.146711Z","shell.execute_reply.started":"2021-12-20T11:26:49.134929Z","shell.execute_reply":"2021-12-20T11:26:49.145733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.utils.plot_model(\n    q_network_orig,\n    to_file=\"model.png\",\n    show_shapes=1,\n    show_dtype=1,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T11:26:49.14829Z","iopub.execute_input":"2021-12-20T11:26:49.148692Z","iopub.status.idle":"2021-12-20T11:26:49.982824Z","shell.execute_reply.started":"2021-12-20T11:26:49.148664Z","shell.execute_reply":"2021-12-20T11:26:49.982172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"output: 12x12x8\n(c,n,s,e,w,bc|bw,research)","metadata":{}},{"cell_type":"code","source":"game_state = None\nold_reward = None\nold_state = None\nepsilon = 1.0\nepsilon_decay = 0.995\nepsilon_final = 0.1\ngamma = 0.95\nlearning_rate = 0.01\nold_output = []\nold_research = None\nold_game_map = None\nglobal_actions = None\nmemory=[]\ndef train_q_network(observation, configuration):\n    global game_state\n    global q_network_orig\n    global q_network_temp\n    global old_reward\n    global old_research\n    global old_game_map\n    global epsilon\n    global epsilon_decay\n    global epsilon_final\n    global gamma\n    global learning_rate\n    global old_output\n    global global_actions\n    ### Do not edit ###\n    if observation[\"step\"] == 0:\n        game_state = Game()\n        game_state._initialize(observation[\"updates\"])\n        game_state._update(observation[\"updates\"][2:])\n        game_state.id = observation.player\n        old_reward = 100000+10000\n        old_output = []\n        for i in range(12):\n            old_output.append([])\n            for e in range(12):\n                old_output[i].append([0,0,0,0,0,0,0,0])\n    else:\n        game_state._update(observation[\"updates\"])\n    actions = []\n\n    \n    player = game_state.players[observation.player]\n    opponent = game_state.players[(observation.player + 1) % 2]\n    width, height = game_state.map.width, game_state.map.height\n    reward = 0\n    q_values_units = {}\n    actions_units = {}\n    actions_cities = []\n    new_actions_units = {}\n    new_actions_cities = []\n    new_q_values_units = {}\n    # Se traduce el estado alcanzado\n    research, game_map = traductor_updates(observation[\"updates\"],player.team)\n    research.append(observation[\"step\"])\n    \n    # Se obtienen los q valores para el estado alcanzado\n    actions_matrix = q_network_temp.predict([np.array([research]),np.array([game_map])])\n    \n    # Elegimos las acciones a ejecutar y extraemos sus q valores (m치ximas o aleatorias)\n    sample_epsilon = np.random.rand()\n    if sample_epsilon > epsilon:\n        for unit in player.units:\n            if unit.can_act():\n                # Se almacena la acci칩n elegida para el estado actual\n                actions_units[unit.id] = np.argmax(actions_matrix[0][unit.pos.x][unit.pos.y][:6])\n                # Se almacena el q valor de la acci칩n elegida\n                q_values_units[unit.id] = max(actions_matrix[0][unit.pos.x][unit.pos.y][:6])\n        for city in player.cities:\n            for citytile in player.cities[city].citytiles:\n                if citytile.can_act():\n                    actions_cities.append([citytile.pos,np.argmax(actions_matrix[0][citytile.pos.x][citytile.pos.y][6:]),max(actions_matrix[0][citytile.pos.x][citytile.pos.y][6:])])\n    else:\n        for unit in player.units:\n            if unit.can_act():\n                temp = random.randint(0,5)\n                actions_units[unit.id] = temp\n                q_values_units[unit.id] = actions_matrix[0][unit.pos.x][unit.pos.y][temp]\n        for city in player.cities:\n            for citytile in player.cities[city].citytiles:\n                if citytile.can_act():\n                    temp = random.randint(6,7)\n                    actions_cities.append([citytile.pos,temp,actions_matrix[0][citytile.pos.x][citytile.pos.y][temp]])\n                    \n                    \n    if q_values_units:\n        for action_unit in actions_units.items():\n            if action_unit[1] == 0:\n                actions.append(\"m \"+action_unit[0]+\" c\")\n            elif action_unit[1] == 1:\n                actions.append(\"m \"+action_unit[0]+\" n\")\n            elif action_unit[1] == 2:\n                actions.append(\"m \"+action_unit[0]+\" s\")\n            elif action_unit[1] == 3:\n                actions.append(\"m \"+action_unit[0]+\" e\")\n            elif action_unit[1] == 4:\n                actions.append(\"m \"+action_unit[0]+\" w\")\n            elif action_unit[1] == 5:\n                actions.append(\"bcity \"+action_unit[0])\n                \n    if actions_cities:\n        for action_city in actions_cities:\n            if action_city[1] == 6:\n                actions.append(\"bw \"+str(action_city[0].x)+\" \"+str(action_city[0].y))\n            if action_city[1] == 7:\n                actions.append(\"r \"+str(action_city[0].x)+\" \"+str(action_city[0].y))\n    \n    ########\n                    \n    # Se calcula la recompensa obtenida para el estado alcanzado\n    for unit in player.units:\n        reward += unit.cargo.wood\n        reward += unit.cargo.coal*10\n        reward += unit.cargo.uranium*40\n    for city in player.cities:\n        reward += player.cities[city].fuel\n    reward += len(player.cities)*100000+len(player.units)*10000\n    r = reward-old_reward\n                    \n    #Se almacena en la memoria\n    if observation[\"step\"] != 0:\n        memory.append([old_research,old_game_map,global_actions,r,research, game_map,False])\n    \n    if len(memory)>= 16:\n        samples = random.sample(memory,16)\n        q_values_samples = []\n        for sample in samples:\n            # Calculamos los q valores\n            q_values_matrix = q_network_temp.predict([np.array([sample[0]]),np.array([sample[1]])])\n            new_q_values_matrix = q_network_orig.predict([np.array([sample[4]]),np.array([sample[5]])])\n            # Obtenemos las acciones que se ejecutaron\n            c0 = 0\n            for i in sample[2][0]:\n                c1 = 0\n                for e in i:\n                    c2 = 0\n                    for o in e:\n                        if o >= 1:\n                            if not sample[6]:\n                                q_values_matrix[0][c0][c1][c2] = learning_rate*(r + gamma*new_q_values_matrix[0][c0][c1][c2])\n                            else:\n                                q_values_matrix[0][c0][c1][c2] = r\n                        c2 += 1\n                    c1 += 1\n                c0 += 1\n            q_values_samples.append(q_values_matrix[0])\n        sample_research = np.array(samples)[:,0]\n        sample_game_map = np.array(samples)[:,1]\n        converted_r = []\n        converted_gm = []\n        converted_qv = []\n        for i in sample_research:\n            converted_r.append(np.array(i))\n        for i in sample_game_map:\n            converted_gm.append(np.array(i))\n        for i in q_values_samples:\n            converted_qv.append(np.array(i))\n\n        if observation[\"step\"]%50 == 0:\n            q_network_temp.fit([np.array(converted_r),np.array(converted_gm)],np.array(q_values_samples),epochs=1,verbose=1)\n        else:\n            q_network_temp.fit([np.array(converted_r),np.array(converted_gm)],np.array(q_values_samples),epochs=1,verbose=0)\n\n    # guardamos los valores para el siguiente estado\n    old_research = research\n    old_game_map = game_map\n    old_reward = reward\n    global_actions = actions_matrix\n    # aplicamos el decay a epsilon\n    if epsilon > epsilon_final:\n        epsilon*= epsilon_decay\n\n    return actions","metadata":{"execution":{"iopub.status.busy":"2021-12-20T11:26:49.993138Z","iopub.execute_input":"2021-12-20T11:26:49.993845Z","iopub.status.idle":"2021-12-20T11:26:50.024944Z","shell.execute_reply.started":"2021-12-20T11:26:49.993806Z","shell.execute_reply":"2021-12-20T11:26:50.024146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def last_state(win,state):\n    global game_state\n    global q_network_temp\n    global q_network_orig\n    global old_research\n    global old_game_map\n    global epsilon\n    global epsilon_decay\n    global epsilon_final\n    global gamma\n    global learning_rate\n    global old_output\n    global global_actions\n    \n    actions = []\n    player = game_state.players[1]\n    opponent = game_state.players[0]\n    width, height = game_state.map.width, game_state.map.height\n    reward = 0\n    \n    research, game_map = traductor_updates(state[0][\"observation\"][\"updates\"],player.team)\n    research.append(game_state.turn+1)\n    \n    for unit in player.units:\n        reward += unit.cargo.wood\n        reward += unit.cargo.coal*10\n        reward += unit.cargo.uranium*40\n    for city in player.cities:\n        reward += player.cities[city].fuel\n    reward += len(player.cities)*100000+len(player.units)*10000\n    r = reward-old_reward\n    \n    #Se almacena en la memoria\n    memory.append([old_research,old_game_map,global_actions,r,research,game_map,True])\n    \n    if len(memory)>= 16:\n        samples = random.sample(memory,16)\n        q_values_samples = []\n        for sample in samples:\n            # Calculamos los q valores\n            q_values_matrix = q_network_temp.predict([np.array([sample[0]]),np.array([sample[1]])])\n            new_q_values_matrix = q_network_orig.predict([np.array([sample[4]]),np.array([sample[5]])])\n            # Obtenemos las acciones que se ejecutaron\n            c0 = 0\n            for i in sample[2][0]:\n                c1 = 0\n                for e in i:\n                    c2 = 0\n                    for o in e:\n                        if o >= 1:\n                            if not sample[6]:\n                                q_values_matrix[0][c0][c1][c2] = learning_rate*(r + gamma*new_q_values_matrix[0][c0][c1][c2])\n                            else:\n                                q_values_matrix[0][c0][c1][c2] = r\n                        c2 += 1\n                    c1 += 1\n                c0 += 1\n            q_values_samples.append(q_values_matrix[0])\n        sample_research = np.array(samples)[:,0]\n        sample_game_map = np.array(samples)[:,1]\n        converted_r = []\n        converted_gm = []\n        converted_qv = []\n        for i in sample_research:\n            converted_r.append(np.array(i))\n        for i in sample_game_map:\n            converted_gm.append(np.array(i))\n        for i in q_values_samples:\n            converted_qv.append(np.array(i))\n\n        q_network_temp.fit([np.array(converted_r),np.array(converted_gm)],np.array(q_values_samples),epochs=1,verbose=1)\n\n    # aplicamos el decay a epsilon\n    if epsilon > epsilon_final:\n        epsilon*= epsilon_decay\n    return actions","metadata":{"execution":{"iopub.status.busy":"2021-12-20T11:26:50.02684Z","iopub.execute_input":"2021-12-20T11:26:50.027376Z","iopub.status.idle":"2021-12-20T11:26:50.044097Z","shell.execute_reply.started":"2021-12-20T11:26:50.027347Z","shell.execute_reply":"2021-12-20T11:26:50.043231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"episodes = 300\nlearning_rate = 0.99\nwinrate = 0\nepisodes_played = 0\nepsilon = 1\nenv = make(\"lux_ai_2021\", debug=True, configuration={\"seed\": 562124210,\"annotations\": True, \"width\":12, \"height\":12, \"loglevel\": 0})\nfor episodes in tqdm(range(episodes)):\n    steps = env.run([\"simple_agent\", train_q_network])\n    episodes_played += 1\n    if steps[len(steps)-1][0][\"reward\"] < steps[len(steps)-1][1][\"reward\"]:\n        last_state(True,steps[len(steps)-1])\n        winrate += 1\n        print(\"wins\")\n        print(winrate)\n        print(episodes_played)\n    else:\n        last_state(False,steps[len(steps)-1])\n        print(winrate)\n        print(episodes_played)\n    q_network_orig = tf.keras.models.clone_model(q_network_temp)\n    if learning_rate >= 0.01:\n        learning_rate*=0.995\nq_network_orig.save(\"deep_q_learning.h5\")","metadata":{"execution":{"iopub.status.busy":"2021-12-20T11:26:50.045199Z","iopub.execute_input":"2021-12-20T11:26:50.045562Z","iopub.status.idle":"2021-12-20T11:32:42.194984Z","shell.execute_reply.started":"2021-12-20T11:26:50.045533Z","shell.execute_reply":"2021-12-20T11:32:42.192885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"env.render(mode=\"ipython\", width=1200, height=800)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T11:32:42.196514Z","iopub.status.idle":"2021-12-20T11:32:42.197416Z","shell.execute_reply.started":"2021-12-20T11:32:42.197088Z","shell.execute_reply":"2021-12-20T11:32:42.197143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}