{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Kaggle Ultrasound Nerve Segmentation Competition (2016)<br>as an Undergraduate Research Project."},{"metadata":{},"cell_type":"markdown","source":"## Warning!!!\n\nThe code was run on Kaggle machines in July 2019 - October 2019 using Python 3 interpreter. It might stop working in the future due to the changes to Kaggle virtual environment and the updates of Python libraries used in this code. Please check [Kaggle documentation] and [kaggle/python docker image] for more information.\n\n[Kaggle documentation]: https://www.kaggle.com/docs/kernels#the-kernels-environment\n[kaggle/python docker image]: https://github.com/kaggle/docker-python"},{"metadata":{},"cell_type":"markdown","source":"## About the Project\n\n### Project Details:\n* Title: Machine learning in the service of surgeons\n* Author: George Batchkala, g.batchkala@warwick.ac.uk\n* Supervisor: Dr Sigurd Assing, s.assing@warwick.ac.uk\n* Institution: University of Warwick\n* Department: Statistics\n* Project funding: Undergraduate Research Support Scheme at the University of Warwick\n* Project's official dates: July 1st 2019 - August 29th 2019\n* Project's real dates: July 1st 2019 - August 29th 2019, October 2019\n* Data Set: Kaggle \"Ultrasound Nerve Segmentation\" (2016) <br>https://www.kaggle.com/c/ultrasound-nerve-segmentation/overview\n* Project's GitHub repository: https://github.com/GeorgeBatch/ultrasound-nerve-segmentation\n\n### Motivation\nIn summer 2019, I was conducting an undergraduate research project within the Statistics Department of the University of Warwick. Together with my supervisor, both being interested in machine learning, we chose to work on an old Kaggle competition. The choice can be explained by things we lacked and had at the moment.\n\n**We lacked:**\n* Practical experience with Neural Networks using any software\n* Expertise in Image Segmentation\n\n**We had:**\n* A strong desire to get, what we lacked\n* Decent theoretical understanding of standard machine learning concepts\n* Practical experience with standard statistical machine learning techniques, e.g. k-nearest-neighbours, linear regression and its modifications, support vector machines, etc.\n* Theoretical understanding of how Neural Networks work, at the level of being comfortable with chapters 5-9 of \"Deep Learning\" by Ian Goodfellow, Yoshua Bengio, Aaron Courville (2016). http://www.deeplearningbook.org/\n\n### Project Results:\n* Achieved **top 10%** of the competition's leaderboard (for old competitions the results of late submissions are not displayed)\n* Created a customisable U-net-like neural network with **160 different configurations**\n* Gained practical experience of creating software for Image Segmentation\n* Gained experience of doing independent research and working with academic papers"},{"metadata":{},"cell_type":"markdown","source":"## Initial Research\n\nBefore starting any practical work, a few papers were chosen as preliminary reading.\n\n### Initial papers:\n* Qingqing Cui, Peng Pu, Lu Chen, Wenzheng Zhao, Yu Liu (2018). \"Deep Convolutional Encoder-Decoder Architecture for Neuronal Structure Segmentation\". https://ieeexplore.ieee.org/document/8698405\n* Julián Gil González, Mauricio A. Álvarez, Álvaro A. Orozco (2015). \"Automatic segmentation of nerve structures in ultrasound images using Graph Cuts and Gaussian processes\". https://ieeexplore.ieee.org/document/7319045\n* Julián Gil González, Mauricio A. Álvarez, Álvaro A. Orozco (2016). \"A probabilistic framework based on SLIC-superpixel and Gaussian processes for segmenting nerves in ultrasound images\". https://ieeexplore.ieee.org/document/7591636\n\nHaving little experience in the field, I found myself reading more papers, referenced in the original selection. I list them below for your interest.\n\n### Follow-up papers:\n* Ronneberger, Olaf; Fischer, Philipp; Brox, Thomas (2015). \"U-Net: Convolutional Networks for Biomedical Image Segmentation\". https://arxiv.org/abs/1505.04597\n* Evan Shelhamer, Jonathan Long, Trevor Darrell (2016). \"Fully Convolutional Networks for Semantic Segmentation\". https://arxiv.org/abs/1605.06211\n* Fisher Yu, Vladlen Koltun (2016). \"Multi-Scale Context Aggregation by Dilated Convolutions\". https://arxiv.org/abs/1511.07122\n* Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich (2014). \"Going Deeper with Convolutions\". https://arxiv.org/abs/1409.4842\n* Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, Zbigniew Wojna (2015). \"Rethinking the Inception Architecture for Computer Vision\". https://arxiv.org/abs/1512.00567v3\n* Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi (2016). \"Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning\". https://arxiv.org/abs/1602.07261v2\n\nI also found the article which (in my opinion) very well summarises and explains the main concepts of the last three papers:\n* Bharath Raj (2018), A Simple Guide to the Versions of the Inception Network. https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202"},{"metadata":{},"cell_type":"markdown","source":"## Acknowledgements\n\nDue to the lack of practical experience, I based my project on the code written by Marko Jocić  and Edward Tyantov.\n\n**Marko Jocić's work:** [Kaggle][MJ's Kaggle], [GitHub][MJ's GitHub]\n\n[MJ's Kaggle]: https://www.kaggle.com/c/ultrasound-nerve-segmentation/discussion/21358#latest-651570\n[MJ's GitHub]: https://github.com/jocicmarko/ultrasound-nerve-segmentation\n\n**Edward Tyantov's work:** [Kaggle][ET's Kaggle], [GitHub][ET's GitHub]\n\n[ET's Kaggle]: https://www.kaggle.com/c/ultrasound-nerve-segmentation/discussion/22958#latest-132645\n[ET's GitHub]: https://github.com/EdwardTyantov/ultrasound-nerve-segmentation\n\nMarko Jocić released the code at the very beginning of the competition. I decided that using this code was something many people, including some industry professionals as Edward Tyantov, did during the competition. The code you see here is a modified combination of both sources.\n\n### Reasons for using some blocks of code completely unchanged:\n* My focus for this project was on machine learning, so I decided that I would use the data preparation process, written by professionals, after fully understanding it. The same applies to some other parts.\n* Marko Jocić's code was available from the beginning of the competition, meaning that even the winner could potentially use the data preparation part and proceed to the other parts of the competition straight away.\n\n### Reasons for modifications:\n* I wanted to try different types of U-net-like architectures and not just replicate other people's work.\n* I found Edward's code too complicated at times and wanted to have the code, which I could write myself from the very beginning. Due to this reason, I was simplifying and modifying the code where possible, without changing the final result.\n* Due to some reason, Marko Jocić's code did not give me the 0.57 result, as stated in his Kaggle notebook.\n* Finally, as you hopefully already read in my \"Warning\" section, I needed to be able to run the code on Kaggle servers, which was not possible, given the original code. The code by both authors did not compile. This happened because of the changes made to the Python libraries since 2016 when the competition was held. Even after arranging all the code together and fixing the compilation problem, the code had many bugs, occurring due to other Python library updates. Some of the bugs were left unchanged in author's GitHub versions."},{"metadata":{},"cell_type":"markdown","source":"## Running the code\n\n### Running project's code\n\n#### Running on Kaggle machines\nIf you are reading this in Kaggle notebook, you do not need to follow the next link. If not, to run the code on Kaggle machines, you can fork and run the notebook available through this [link.](https://www.kaggle.com/gbatchkala/urss-2019-project-review)\n\nSome modules take several minutes to run. To execute a specific module, you will either have to set its execute parameter to **True**, or set **execute_all** parameter to **True**, which you can find in the next code block. The former executes a specific module, while the latter allows executing all modules.\n\nYou can also run this code in separate Kaggle script, which is just a concatenated version of all code in this notebook. The script code is available through these links: [Kaggle](https://www.kaggle.com/gbatchkala/urss-final-code-script), [GitHub](https://github.com/GeorgeBatch/ultrasound-nerve-segmentation/blob/master/urss_final_code_script.py)\n\n#### Running on personal machines\nIf you would like to work with the code presented below on your sown machine, I recommend cloning my GitHub repository. This way, you do not need to set up a directory. [Link to project's repository.](https://github.com/GeorgeBatch/ultrasound-nerve-segmentation) "},{"metadata":{},"cell_type":"markdown","source":"## Required set-up\n\n### Setting up your directory\n\nIf you decided to download code in separate files, first, you need to set up your directory structure as shown below. The structure mimics Kaggle's directory structure. On Kaggle, your script/notebook is in the working directory by default, while any data you upload goes inside the input directory.\n\n```\n- working\n  |\n  ---- Edward_Tyantov_edited.py\n  |\n  ---- urss_final_code_script.py\n  |\n  ---- data.py\n  |\n  ----...\n- input\n  |\n  - ultrasound-nerve-segmentation\n   |\n   ---- train\n   |    |\n   |    ---- 1_1.tif\n   |    |\n   |    ---- …\n   |\n   ---- test\n        |\n        ---- 1.tif\n        |\n        ---- …\n```"},{"metadata":{},"cell_type":"markdown","source":"### Requirements:\n\nSee [kaggle/python docker image](https://github.com/kaggle/docker-python)\n\nMinimal information:\n* Python >= 3.5\n* Keras >= 2.0 \n* Tensorflow backend for Keras\n* Working with files: os, sys\n* For run-length-encoding: itertools\n* Working with arrays: numpy\n* Working with images: skimage\n\nIf you are using Theano backend, check that the shape of the data is in the form (samples, rows, cols, channels). Otherwise, the code breaks."},{"metadata":{},"cell_type":"markdown","source":"### Executing files:\n\nTo run this code, you need access to a GPU processing unit. I trained the model on Kaggle's GPU. Otherwise, it can take up to 2.5 days on Intel-i7 processors.\n\nSet model configuration:\n* check_pars.py\n* configuration.py **- configure your network and the learning-rate optimizer**\n\nOrder of file execution:\n* data.py\n* train.py\n* submission.py\n\nAlternatively execute one of:\n* urss_final_code_script.py\n* Edward_Tyantov_edited.py"},{"metadata":{},"cell_type":"markdown","source":"### Configuration\nThere are several versions of the U-net architecture you can try. If you want to try it out, do not change anything in the configuration module (configuration.py on GitHub) and you get the U-net kindly provided by Marko Jocić at the beginning of the competition.\n\nIn case you want to experiment, I list the versions I tried here. To configure your version of the U-net, you need to make decisions on each level of granularity (see below) combining all the top-level decisions into parameters, which you pass to the U-net-generating function. Below you can see the configuration structure:\n\n* Number of outputs\n    * One output\n    * Two outputs\n* Activation\n    * ReLU\n    * ELU\n* Blocks for capturing information\n    * Convolution blocks\n        * Simple convolutions (see [simple implementation][simple u-net implementation])\n            * With batch-normalization\n            * Without batch-normalization\n        * Dilated convolutions (see [referenced paper][Dilated convolutions paper])\n            * With batch-normalization\n            * Without batch-normalization\n    * Inception blocks (see [the article][Inception-blocks article] and two referenced papers [paper 1][Inception v1], [paper 2][Inception v2 and v3])\n         * Inception block v1, versions a, b\n         * Inception block v2, versions a, b, c\n         * Inception block et, versions a, b\n* Skip connections from the down-path to the up-path of the U-net\n    * Standard connections from the [original U-net paper]\n    * Residual connections mimicking ResNet skip connections (see [paper][Inception v4, Inception-ResNet])\n* Pooling layers reducing the size of the image\n    * Non-trainable: Max-pooling layers\n    * Trainable: Normalized Convolution layers with strides\n\n**Optimizer**: Select any available optimizer from [Keras optimizers](https://keras.io/optimizers/)\n    \n[Original U-net paper]: https://arxiv.org/abs/1505.04597\n[simple u-net implementation]: https://github.com/zhixuhao/unet\n\n[Dilated convolutions paper]: https://arxiv.org/abs/1511.07122\n\n[Inception-blocks article]: https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202\n[Inception v1]: https://arxiv.org/abs/1409.4842\n[Inception v2 and v3]: https://arxiv.org/abs/1512.00567v3\n[Inception v4, Inception-ResNet]: https://arxiv.org/abs/1602.07261v2\n\n"},{"metadata":{},"cell_type":"markdown","source":"### Running Edward Tyantov's code\n\nAs a beginner, I found Marko Jocić's code and the instructions for running it accessible. So I did not document the code and do not intend publishing it here in the future. At the same time, I had many problems with trying to make Edward Tyantov's code run correctly. This is why I made a concatenated, simplified, modified, and documented version of Edward Tyantov's original code available on Kaggle and GitHub:\n* Kaggle: https://www.kaggle.com/gbatchkala/edward-tyantov-edited-py\n* GitHub: https://github.com/GeorgeBatch/ultrasound-nerve-segmentation/blob/master/working/Edward_Tyantov_edited.py\n\nIn both versions, you can find the information about the changes, acknowledgements, and licence at the beginning of the python script.\n\nFor GitHub version see LICENCE. If you find any mistakes or want to update the code to satisfy current kaggle environment, please submit your changes to the file via pull request to my GitHub repository."},{"metadata":{},"cell_type":"markdown","source":"# Planned modifications (December 2019 - March 2020)\n\n1. Use multiple cores to construct image arrays instead of doing it sequentially.\n    1. Note that patients 4_41 and 44_1 should have different IDs.\n    2. Record how much time is saved and how much is lost for the overhead.\n2. Write a Learner class for training. It might help to implement all of the further modifications.\n3. Add by-patient option for the train-validations split. Otherwise, the real situation of getting new patients is not modelled.\n4. Average over the forward paths using Dropout for test data prediction (Bayesian use of Dropout).\n    1. Original paper \"Dropout as a Bayesian Approximation\": https://arxiv.org/pdf/1506.02142.pdf\n    2. Dropout(training=True): https://github.com/keras-team/keras/issues/9412\n    3. Example of implementation: https://www.depends-on-the-definition.com/model-uncertainty-in-deep-learning-with-monte-carlo-dropout/\n5. Average over augmented versions of test images:\n    1. Create augumented-images generator. Look up: https://keras.io/preprocessing/image/\n    2. Input the image with its augmented versions for prediction. Look up: keras.Model.predict_generator()\n    3. Invert the augmentation transformations for the masks to get different predictions for the original image.\n    4. Average out these predictions\n6. Average over multiple instances of the same model configuration (Ensemble Methods).\n7. Average over different model configurations, which give the best results (Ensemble Methods).\n8. Find suitable Learning rate parameters for various configurations.\n9. Fecord the results and compare the models.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"execute_all = False","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modules\n\n## Check parameters module\n\nFile name: check_pars.py\n\nInstructions:\n* If used as as separate module, the \"separate-module imports\" part needs to be uncommented\n* Do not change this module unless you want to make modifications to the u-net configuration"},{"metadata":{"trusted":true},"cell_type":"code","source":"########################################################################################################################\n# ======================================================================================================================\n# check_pars\n# ======================================================================================================================\n########################################################################################################################\n# read-only file!!!\n\n# standard-module imports\nfrom keras.optimizers import Adam\n\n\ndef check_dict_subset(subset, superset):\n    \"\"\"Checks if one nested dictionary is a subset of another\n\n    :param subset: subset dictionary\n    :param superset: superset dictionary\n    :return: if failed: gives helpful print statements and assertion error\n             if successful, prints 'Your parameter choice is valid'\n    \"\"\"\n    print(\"superset keys:\", superset.keys())\n    print(\"subset keys:\", subset.keys())\n    assert all(item in superset.keys() for item in subset.keys())\n    print(\"Subset keys is a subset of superset keys\", all(item in superset.keys() for item in subset.keys()))\n    for key in subset.keys():\n        print(\"superset key items:\", superset[key])\n        print(\"subset key items:\", subset[key])\n        if type(superset[key]) == dict:\n            assert type(subset[key]) == type(superset[key])\n            check_dict_subset(subset[key], superset[key])\n        elif type(superset[key]) == list:\n            assert subset[key] in superset[key]\n            print(\"subset[key] item:\", subset[key], \" is in superset[key] items:\", superset[key])\n        else:\n            print(\"Something went wrong. Uncomment the print statements in check_dict_subset() for easier debugging.\")\n            return type(superset[key]), superset[key]\n\n    return 'Your parameter choice is valid'\n\n\n# Only change ALLOWED_PARS if adding new functionality\nALLOWED_PARS = {\n    'outputs': [1, 2],\n    'activation': ['elu', 'relu'],\n    'pooling_block': {\n        'trainable': [True, False]},\n    'information_block': {\n        'inception': {\n            'v1': ['a', 'b'],\n            'v2': ['a', 'b', 'c'],\n            'et': ['a', 'b']},\n        'convolution': {\n            'simple': ['not_normalized', 'normalized'],\n            'dilated': ['not_normalized', 'normalized']}},\n    'connection_block': ['not_residual', 'residual']\n}\n\n# for reference: in combination, these parameter choice showed the best performance\nBEST_OPTIMIZER = Adam(lr=0.0045)\nBEST_PARS = {\n    'outputs': 2,\n    'activation': 'elu',\n    'pooling_block': {'trainable': True},\n    'information_block': {'inception': {'v2': 'b'}},\n    'connection_block': 'residual'\n}\n\nprint(check_dict_subset(BEST_PARS, ALLOWED_PARS))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Configuration module\n\nFile name: configuration.py\n\nInstructions:\n* If used as as separate module, the \"separate-module imports\" part needs to be uncommented\n* Select a network configuration of your choice and record it in PARS (check BEST_PARS in check_pars module for format)\n* Select any available optimizer from [Keras optimizers](https://keras.io/optimizers/)"},{"metadata":{"trusted":true},"cell_type":"code","source":"########################################################################################################################\n# ======================================================================================================================\n# configuration\n# ======================================================================================================================\n########################################################################################################################\n\n# standard-module imports\nfrom keras.optimizers import Adam\n\n# # separate-module imports\n# from check_pars import ALLOWED_PARS, check_dict_subset\n\n# look up the format and the available parameters\nprint(ALLOWED_PARS)\n\n# The result is very sensitive to the choice of the Learning Rate parameter  of the optimizer\n# DO NOT CHANGE THE NAME, you can change the parameters\nOPTIMIZER = Adam(lr=0.0045)\n\n# DO NOT CHANGE THE NAME, you can change the parameters\nPARS = {\n    'outputs': 1,\n    'activation': 'relu',\n    'pooling_block': {'trainable': False},\n    'information_block': {'convolution': {'simple': 'normalized'}},\n    'connection_block': 'not_residual'\n}\n\n# DO NOT REMOVE THESE LINES, they checks if your parameter choice is valid\nassert PARS.keys() == ALLOWED_PARS.keys()\nprint(check_dict_subset(PARS, ALLOWED_PARS))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data module\n\nFile name: data.py\n\nInstructions:\n* if used as as separate module, the \"separate-module imports\" part needs to be uncommented\n* change execute_data to True \n\nCredits: Edward Tyantov\n\nModifications:\n* Add get_nerve_presence(), load_nerve_presence() functions to allow one-output architectures\n* Make appropriate updates, so the code can be run on Kaggle with current library versions\n* Add documentation to all functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"execute_data = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################################################################################################################\n# ======================================================================================================================\n# data\n# ======================================================================================================================\n########################################################################################################################\n\n# ======================================================================================================================\n# Set-up\n# ======================================================================================================================\n\n# standard-module imports\nimport os\nimport numpy as np\nfrom skimage.io import imread\n\n# Input data files are available in the \"../input/\" directory.\n\n_dir = os.path.abspath('')\nos.chdir(_dir)\nprint(_dir)\n\nprint(os.listdir(_dir))\nprint(os.listdir(\"../input\"))\nprint(os.listdir(\"../input/ultrasound-nerve-segmentation\"))\n\n# data\ndata_path = os.path.join('../input/ultrasound-nerve-segmentation', '')\npreprocess_path = os.path.join(_dir, 'np_data')\n\nif not os.path.exists(preprocess_path):\n    os.mkdir(preprocess_path)\nprint(os.listdir(_dir))\n\n# train data\nimg_train_path = os.path.join(preprocess_path, 'imgs_train.npy')\nimg_train_mask_path = os.path.join(preprocess_path, 'imgs_mask_train.npy')\nimg_train_patients = os.path.join(preprocess_path, 'imgs_patient.npy')\nimg_nerve_presence = os.path.join(preprocess_path, 'nerve_presence.npy')\n\n# test data\nimg_test_path = os.path.join(preprocess_path, 'imgs_test.npy')\nimg_test_id_path = os.path.join(preprocess_path, 'imgs_id_test.npy')\n\n# image dimensions\nimage_rows = 420\nimage_cols = 580\n\n\n# ======================================================================================================================\n# Functions for test and train data creation, storage and access\n# ======================================================================================================================\n\ndef load_test_data():\n    \"\"\"Load test data from a .npy file.\n\n    :return: np.array with test data.\n    \"\"\"\n    print('Loading test data from %s' % img_test_path)\n    imgs_test = np.load(img_test_path)\n    return imgs_test\n\n\ndef load_test_ids():\n    \"\"\"Load test ids from a .npy file.\n\n    :return: np.array with test ids. Shape (samples, ).\n    \"\"\"\n    print('Loading test ids from %s' % img_test_id_path)\n    imgs_id = np.load(img_test_id_path)\n    return imgs_id\n\n\ndef load_train_data():\n    \"\"\"Load train data from a .npy file.\n\n    :return: np.array with train data.\n    \"\"\"\n    print('Loading train data from %s and %s' % (img_train_path, img_train_mask_path))\n    imgs_train = np.load(img_train_path)\n    imgs_mask_train = np.load(img_train_mask_path)\n    return imgs_train, imgs_mask_train\n\n\ndef load_patient_num():\n    \"\"\"Load the array with patient numbers from a .npy file\n\n    :return: np.array with patient numbers\n    \"\"\"\n    print('Loading patient numbers from %s' % img_train_patients)\n    return np.load(img_train_patients)\n\n\ndef load_nerve_presence():\n    \"\"\"Load the array with binary nerve presence from a .npy file\n\n    :return: np.array with patient numbers\n    \"\"\"\n    print('Loading nerve presence array from %s' % img_nerve_presence)\n    return np.load(img_nerve_presence)\n\n\ndef get_patient_nums(string):\n    \"\"\"Create a tuple (patient, photo) from image-file name patient_photo.tif\n\n    :param string: image-file name in string format: patient_photo.tif\n    :return: a tuple (patient, photo)\n\n    >>> get_patient_nums('32_50.tif')\n    (32, 50)\n    \"\"\"\n    patient, photo = string.split('_')\n    photo = photo.split('.')[0]\n    return int(patient), int(photo)\n\n\ndef get_nerve_presence(mask_array):\n    \"\"\"Create an array specifying nerve presence on each of the masks in the mask_array\n\n    :param mask_array: 4D tensor of a shape (samples, rows, cols, channels=1) with masks\n    :return:\n    \"\"\"\n    print(\"type(mask_array):\", type(mask_array))\n    print(\"mask_array.shape:\", mask_array.shape)\n    return np.array([int(np.sum(mask_array[i, :, :, 0]) > 0) for i in range(mask_array.shape[0])])\n\n\ndef create_train_data():\n    \"\"\"\n    Create an np.array with patient numbers and save it into a .npy file.\n    Create an np.array with train images and save it into a .npy file.\n    Create an np.array with train masks and save it into a .npy file.\n\n    The np.array with patient numbers will have shape (samples, ).\n        So for each train image saved, the patient number will be recorded exactly in the same order the\n        images were saved.\n    The np.array with train images will have shape (samples, rows, cols, channels).\n    The np.array with train masks will have shape (samples, rows, cols, channels).\n        The masks are saved in the same order as the images.\n    \"\"\"\n    train_data_path = os.path.join(data_path, 'train')\n    images = os.listdir(train_data_path)\n    total = len(images) // 2\n\n    imgs = np.ndarray((total, image_rows, image_cols, 1), dtype=np.uint8)\n    imgs_mask = np.ndarray((total, image_rows, image_cols, 1), dtype=np.uint8)\n    i = 0\n    print('Creating training images...')\n    img_patients = np.ndarray((total,), dtype=np.uint8)\n    for image_name in images:\n\n        # With \"continue\" skip the mask image in the iteration because the mask will be saved together with\n        # the image, when we get the image in one of the next iterations. This guarantees that the images,\n        # masks and corresponding patient numbers are all saved in the correct order.\n        if 'mask' in image_name:\n            continue\n\n        # we got to this point, meaning that image_name is a name of a training image and not a mask.\n\n        # recreate the mask's name fot this image\n        # noinspection PyTypeChecker\n        image_mask_name = image_name.split('.')[0] + '_mask.tif'\n        # get the patient number of the image\n        patient_num = image_name.split('_')[0]\n        # read the image itself to an np.array\n        img = imread(os.path.join(train_data_path, image_name), as_gray=True)\n        # read the corresponding mask to an np.array\n        img_mask = imread(os.path.join(train_data_path, image_mask_name), as_gray=True)\n\n        imgs[i, :, :, 0] = img\n        imgs_mask[i, :, :, 0] = img_mask\n        img_patients[i] = patient_num\n        if i % 100 == 0:\n            print('Done: {0}/{1} images'.format(i, total))\n        i += 1\n    print('Loading done.')\n\n    # saving patient numbers, train images, train masks, nerve presence\n    np.save(img_train_patients, img_patients)\n    np.save(img_train_path, imgs)\n    np.save(img_train_mask_path, imgs_mask)\n    np.save(img_nerve_presence, get_nerve_presence(imgs_mask))\n\n    print('Saving to .npy files done.')\n\n\ndef create_test_data():\n    \"\"\"\n    Create an np.array with test data and save it into a .npy file.\n    Create an np.array with ids for all images and save it into a .npy file.\n\n    The np.array with test data will have shape (samples, rows, cols, channels).\n    The np.array with test data ids will have shape (samples,). Each image id will be a number\n    corresponding to the number in a test image name. For example image '8.tif' will have 8 as its image id.\n    \"\"\"\n    test_data_path = os.path.join(data_path, 'test')\n    images = os.listdir(test_data_path)\n    total = len(images)\n\n    imgs = np.ndarray((total, image_rows, image_cols, 1), dtype=np.uint8)\n    imgs_id = np.ndarray((total,), dtype=np.int32)\n\n    i = 0\n    print('Creating test images...')\n    for image_name in images:\n        img_id = int(image_name.split('.')[0])\n        img = imread(os.path.join(test_data_path, image_name), as_gray=True)\n\n        imgs[i, :, :, 0] = img\n        imgs_id[i] = img_id\n\n        if i % 100 == 0:\n            print('Done: {0}/{1} images'.format(i, total))\n        i += 1\n    print('Loading done.')\n\n    np.save(img_test_path, imgs)\n    np.save(img_test_id_path, imgs_id)\n    print('Saving to .npy files done.')\n\n\n# ----------------------------------------------------------------------------------------------------------------------\n\nif __name__ == '__main__' and (execute_data==True or execute_all==True):\n    create_train_data()\n    create_test_data()\n\n# checking what is in the directory\nprint(os.listdir(preprocess_path))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Metric module\n\nFile name: metric.py\n\nInstructions: if used as as separate module, the \"separate-module imports\" part needs to be uncommented\n\nCredits: Edward Tyantov\n\nModifications:\n* Make appropriate updates, so the code can be run on Kaggle with current library versions\n* Add documentation to all functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"execute_metric = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################################################################################################################\n# ======================================================================================================================\n# metric\n# ======================================================================================================================\n########################################################################################################################\n# needed for u_model\n\n# standard-module imports\nimport numpy as np\nfrom keras import backend as K  # tensorflow backend\n\n\ndef dice_coef(mask_1, mask_2, smooth=1):\n    \"\"\"Compute the dice coefficient between two equal-sized masks.\n\n    Dice Coefficient: https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient\n    We need to add smooth, because otherwise 2 empty (all zeros) masks will throw an error instead of\n    giving 1 as an output.\n\n    :param mask_1: first mask\n    :param mask_2: second mask\n    :param smooth: Smoothing parameter for dice coefficient\n    :return: Smoothened dice coefficient between two equal-sized masks\n    \"\"\"\n    mask_1_flat = K.flatten(mask_1)\n    mask_2_flat = K.flatten(mask_2)\n\n    # for pixel values in {0, 1} multiplication is the intersection of masks\n    intersection = K.sum(mask_1_flat * mask_2_flat)\n    return (2. * intersection + smooth) / (K.sum(mask_1_flat) + K.sum(mask_2_flat) + smooth)\n\n\ndef dice_coef_loss(mask_pred, mask_true):\n    \"\"\"Calculate dice coefficient loss, when comparing predicted mask for an image with the true mask\n\n    :param mask_pred: predicted mask\n    :param mask_true: true mask\n    :return: dice coefficient loss\n    \"\"\"\n    return -dice_coef(mask_pred, mask_true)\n\n\ndef np_dice_coef(mask_1, mask_2, smooth=1):\n    \"\"\"Compute the dice coefficient between two equal-sized masks.\n\n    Used for testing on artificially generated np.arrays\n\n    Dice Coefficient: https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient\n    Need smooth, because otherwise 2 empty (all zeros) masks will throw an error instead of giving 1 as an output.\n\n    :param mask_1: first mask\n    :param mask_2: second mask\n    :param smooth: Smoothing parameter for dice coefficient\n    :return: Smoothened dice coefficient between two equal-sized masks\n    \"\"\"\n    tr = mask_1.flatten()\n    pr = mask_2.flatten()\n    return (2. * np.sum(tr * pr) + smooth) / (np.sum(tr) + np.sum(pr) + smooth)\n\n\n# ----------------------------------------------------------------------------------------------------------------------\n\nif __name__ == '__main__' and (execute_metric==True or execute_all==True):\n    a = np.random.random((420, 100))\n    b = np.random.random((420, 100))\n    res = np_dice_coef(a, b)\n    print(res)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## U-model blocks module\n\nFile name: u_model_blocks.py\n\nInstructions: if used as as separate module, the \"separate-module imports\" part needs to be uncommented\n\nCredits: Edward Tyantov\n\nFunctionality kept from Edward Tyantov's version or insignificantly modified:\n* NConv2D()\n* _shortcut()\n* rblock\n* inception_block_et()\n\nNew functionality:\n* convolution_block()\n* dilated_convolution_block()\n* inception_block_v1()\n* inception_block_v2()\n* pooling_block()\n* information_block()\n* connection_block()"},{"metadata":{"trusted":true},"cell_type":"code","source":"########################################################################################################################\n# ======================================================================================================================\n# u_model_blocks\n# ======================================================================================================================\n########################################################################################################################\n# needed for u_model\n\n# standard-module imports\nfrom keras.layers import add, concatenate, Conv2D, MaxPooling2D\nfrom keras.layers import BatchNormalization, Lambda\nfrom keras.layers.advanced_activations import ELU, LeakyReLU\n\n\n# ======================================================================================================================\n# utility blocks needed for internal performance\n# ======================================================================================================================\n\ndef NConv2D(filters, kernel_size, strides=(1, 1), padding='valid', dilation_rate=1,\n            activation=None, kernel_initializer='glorot_uniform'):\n    \"\"\"Create a (Normalized Conv2D followed by a chosen activation) function\n    Conv2D -> BatchNormalization -> activation()\n\n    :param filters: Integer, the dimensionality of the output space (i.e. the number of output filters in the\n    convolution)\n    :param kernel_size: An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution\n                        window. Can be a single integer to specify the same value for all spatial dimensions.\n    :param strides: An integer or tuple/list of 2 integers, specifying the strides of the convolution along the height\n                    and width. Can be a single integer to specify the same value for all spatial dimensions.\n                    Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1.\n    :param padding: one of 'valid' or 'same' (case-insensitive), 'valid' by default to have the same as Conv2D\n    :param dilation_rate: an integer or tuple/list of a single integer, specifying the dilation rate\n                    to use for dilated convolution. Currently, specifying any dilation_rate value != 1\n                    is incompatible with specifying any strides value != 1\n    :param activation:  string, one of 'elu' or 'relu' or None (case-sensitive),\n                        specifies activation function to be performed after BatchNormalization\n    :param kernel_initializer: Initializer for the kernel weights matrix (see initializers in keras documentation)\n    :return: a function, combined of 2D Convolution, followed by BatchNormalization across filters,\n             and specified activation in that order\n    \"\"\"\n    assert activation in ['relu', 'elu', None]\n    # actv is a function, not a string, like activation\n    actv = activation == 'relu' and (lambda: LeakyReLU(0.0)) or activation == 'elu' and (lambda: ELU(1.0)) or None\n\n    def f(_input):\n        conv = Conv2D(filters=filters, kernel_size=kernel_size, strides=strides, padding=padding,\n                      dilation_rate=dilation_rate, kernel_initializer=kernel_initializer)(_input)\n        norm = BatchNormalization(axis=3)(conv)\n        return actv()(norm)\n\n    return f\n\n\n# needed for rblock (residual block)\ndef _shortcut(_input, residual):\n    stride_width = _input._keras_shape[1] / residual._keras_shape[1]\n    stride_height = _input._keras_shape[2] / residual._keras_shape[2]\n    equal_channels = residual._keras_shape[3] == _input._keras_shape[3]\n\n    shortcut = _input\n    # 1 X 1 conv if shape is different. Else identity.\n    if stride_width > 1 or stride_height > 1 or not equal_channels:\n        shortcut = Conv2D(filters=residual._keras_shape[3], kernel_size=(1, 1),\n                          strides=(stride_width, stride_height),\n                          kernel_initializer=\"he_normal\", padding=\"valid\")(_input)\n\n    return add([shortcut, residual])\n\n\ndef rblock(inputs, filters, kernel_size, padding='valid', activation=None, scale=0.1):\n    \"\"\"Create a scaled Residual block connecting the down-path and the up-path of the u-net architecture\n\n    Activations are scaled by a constant to prevent the network from dying. Usually is set between 0.1 and 0.3. See:\n    https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202\n\n    :param inputs: Input 4D tensor (samples, rows, cols, channels)\n    :param filters: Integer, the dimensionality of the output space (i.e. the number of output convolution filters)\n    :param kernel_size: An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution\n                        window. Can be a single integer to specify the same value for all spatial dimensions.\n    :param padding: one of 'valid' or 'same' (case-insensitive), 'valid' by default to have the same as Conv2D\n    :param activation:  string, one of 'elu' or 'relu' or None (case-sensitive),\n                        specifies activation function to use everywhere in the block\n    :param scale: scaling factor preventing the network from dying out\n    :return: 4D tensor (samples, rows, cols, channels) output of a residual block, given inputs\n    \"\"\"\n    assert activation in ['relu', 'elu', None]\n    # actv is a function, not a string, like activation\n    actv = activation == 'relu' and (lambda: LeakyReLU(0.0)) or activation == 'elu' and (lambda: ELU(1.0)) or None\n\n    residual = Conv2D(filters=filters, kernel_size=kernel_size, padding=padding)(inputs)\n    residual = BatchNormalization(axis=3)(residual)\n    residual = Lambda(lambda x: x * scale)(residual)\n    res = _shortcut(inputs, residual)\n    return actv()(res)\n\n\n# ======================================================================================================================\n# information blocks\n# ======================================================================================================================\n\ndef convolution_block(inputs, filters, kernel_size=(3, 3), padding='valid', activation=None,\n                      version='normalized', pars={}, allowed_pars={}):\n    \"\"\"Create a version of a convolution block.\n\n    Versions: with and without batch-normalization after convolutions.\n\n    :param inputs: Input 4D tensor (samples, rows, cols, channels)\n    :param filters: Integer, the dimensionality of the output space (i.e. the number of output filters in the\n                    convolution).\n    :param kernel_size: An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution\n                        window. Can be a single integer to specify the same value for all spatial dimensions.\n    :param padding: one of 'valid' or 'same' (case-insensitive), 'valid' by default to have the same as Conv2D\n    :param activation: string, specifies activation function to use everywhere in the block\n    :param version: version of the convolution block, one of 'not_normalized', 'normalized' (case sensitive)\n    :param pars: dictionary of parameters passed to u-net, determines the version, if this type of block is chosen\n    :param allowed_pars: dictionary of all allowed to be passed to u-net parameters\n    :return: 4D tensor (samples, rows, cols, channels) output of a convolution block, given inputs\n    \"\"\"\n    assert activation in ['relu', 'elu', None]\n\n    # checking that the allowed version names did not change in ALLOWED_PARS\n    if allowed_pars != {}:\n        assert allowed_pars.get('information_block').get('convolution').get('simple') == ['not_normalized',\n                                                                                          'normalized']\n    # keep version argument if need to use without PARS\n    assert version in ['not_normalized', 'normalized']\n    # setting the version from pars\n    if pars.get('information_block').get('convolution').get('simple') is not None:\n        version = pars.get('information_block').get('convolution').get('simple')\n\n    if version == 'normalized':\n        conv1 = NConv2D(filters=filters, kernel_size=kernel_size, activation=activation, padding=padding)(inputs)\n        return NConv2D(filters=filters, kernel_size=kernel_size, activation=activation, padding=padding)(conv1)\n    else:\n        conv1 = Conv2D(filters=filters, kernel_size=kernel_size, activation=activation, padding=padding)(inputs)\n        return Conv2D(filters=filters, kernel_size=kernel_size, activation=activation, padding=padding)(conv1)\n\n\ndef dilated_convolution_block(inputs, filters, kernel_size=(3, 3), padding='valid', activation=None,\n                              version='normalized', pars={}, allowed_pars={}):\n    \"\"\"Create a version of a dilated-convolution block.\n\n    Versions: with and without batch-normalization after dilated convolutions.\n\n    See more about dilated convolutions:\n    https://towardsdatascience.com/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5\n\n    :param inputs: Input 4D tensor (samples, rows, cols, channels)\n    :param filters: Integer, the dimensionality of the output space (i.e. the number of output filters in the\n                    convolution).\n    :param kernel_size: An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution\n                        window. Can be a single integer to specify the same value for all spatial dimensions.\n    :param padding: one of 'valid' or 'same' (case-insensitive), 'valid' by default to have the same as Conv2D\n    :param activation: string, specifies activation function to use everywhere in the block\n    :param version: version of the dilated-convolution block, one of 'not_normalized', 'normalized' (case sensitive)\n    :param pars: dictionary of parameters passed to u-net, determines the version, if this type of block is chosen\n    :param allowed_pars: dictionary of all allowed to be passed to u-net parameters\n    :return: 4D tensor (samples, rows, cols, channels) output of a dilated-convolution block, given inputs\n    \"\"\"\n    assert activation in ['relu', 'elu', None]\n\n    # checking that the allowed version names did not change in ALLOWED_PARS\n    if allowed_pars != {}:\n        assert allowed_pars.get('information_block').get('convolution').get('dilated') == ['not_normalized',\n                                                                                           'normalized']\n    # keep version argument if need to use without PARS\n    assert version in ['not_normalized', 'normalized']\n    # setting the version from pars\n    if pars.get('information_block').get('convolution') is not None:\n        version = pars.get('information_block').get('convolution')\n\n    if version == 'normalized':\n        conv1 = NConv2D(filters=filters, kernel_size=kernel_size, padding=padding,\n                        dilation_rate=2, activation=activation)(inputs)\n        return NConv2D(filters=filters, kernel_size=kernel_size, padding=padding,\n                       dilation_rate=1, activation=activation)(conv1)\n    else:\n        conv1 = Conv2D(filters=filters, kernel_size=kernel_size, padding=padding,\n                       dilation_rate=2, activation=activation)(inputs)\n        return Conv2D(filters=filters, kernel_size=kernel_size, padding=padding,\n                      dilation_rate=1, activation=activation)(conv1)\n\n\ndef inception_block_v1(inputs, filters, activation=None, version='b', pars={}, allowed_pars={}):\n    \"\"\"Create a version of v1 inception block described in:\n    https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202\n\n    Create an inception block described in v1, sections 'a' (for naive version), or 'b' (with dimension reduction)\n    Each version has 4 verticals in their structure. See the link above.\n\n    For all versions, verticals 1 and 2 of the block start with 2D convolution, which:\n        reduces the number of input filters to next convolutions (to make computation cheaper)\n        uses (1, 1) kernels, no Normalization\n        is NOT normalized\n        is followed by specified activation\n    For all versions, verticals 1, 2, 3:\n        the final convolution layer is not normalised and not activated since it will be dene after concatenation\n    Vertical 4 is just a Conv2D. Its gets normalized and activated after being concatenated with\n        outputs of other verticals.\n    The concatenated output of the verticals is normalised and then activated with a given activation\n\n    :param inputs: Input 4D tensor (samples, rows, cols, channels)\n    :param filters: Integer, the dimensionality of the output space (i.e. the number of output filters in the\n    convolution).\n    :param activation: string, specifies activation function to use everywhere in the block\n    :param version: version of inception block, one of 'a', 'b' (case sensitive)\n    :param pars: dictionary of parameters passed to u-net, determines the version, if this type of block is chosen\n    :param allowed_pars: dictionary of all allowed to be passed to u-net parameters\n    :return: 4D tensor (samples, rows, cols, channels) output of an inception block, given inputs\n    \"\"\"\n\n    assert filters % 16 == 0\n\n    # checking that the allowed version names did not change in ALLOWED_PARS\n    if allowed_pars != {}:\n        assert allowed_pars.get('information_block').get('inception').get('v1') == ['a', 'b']\n    # keep version argument if need to use without PARS\n    assert version in ['a', 'b']\n    # setting the version from pars\n    if pars.get('information_block').get('inception').get('v1') is not None:\n        version = pars.get('information_block').get('inception').get('v1')\n\n    assert activation in ['relu', 'elu', None]\n    # actv is a function, not a string, like activation\n    actv = activation == 'relu' and (lambda: LeakyReLU(0.0)) or activation == 'elu' and (lambda: ELU(1.0)) or None\n\n    # vertical 1\n    if version == 'a':\n        c1 = Conv2D(filters=filters // 8, kernel_size=(5, 5), padding='same', kernel_initializer='he_normal')(inputs)\n    else:\n        c1_1 = Conv2D(filters=filters // 16, kernel_size=(1, 1), padding='same',\n                      activation=activation, kernel_initializer='he_normal')(inputs)\n        c1 = Conv2D(filters=filters // 8, kernel_size=(5, 5), padding='same', kernel_initializer='he_normal')(c1_1)\n\n    # vertical 2\n    if version == 'a':\n        c2 = Conv2D(filters=filters // 2, kernel_size=(3, 3), padding='same', kernel_initializer='he_normal')(inputs)\n    else:\n        c2_1 = Conv2D(filters=filters // 8 * 3, kernel_size=(1, 1), padding='same',\n                      activation=activation, kernel_initializer='he_normal')(inputs)\n        c2 = Conv2D(filters=filters // 2, kernel_size=(3, 3), padding='same', kernel_initializer='he_normal')(c2_1)\n\n    # vertical 3\n    p3_1 = MaxPooling2D(pool_size=(3, 3), strides=(1, 1), padding='same')(inputs)\n    if version == 'b':\n        c3 = Conv2D(filters=filters // 8, kernel_size=(1, 1), padding='same', kernel_initializer='he_normal')(p3_1)\n    else:\n        c3 = p3_1\n\n    # vertical 4\n    c4_1 = Conv2D(filters=filters // 4, kernel_size=(1, 1), padding='same', kernel_initializer='he_normal')(inputs)\n    c4 = c4_1\n\n    # concatenating verticals together, normalizing and applying activation\n    result = concatenate([c1, c2, c3, c4], axis=3)\n    result = BatchNormalization(axis=3)(result)\n    result = actv()(result)\n    return result\n\n\ndef inception_block_v2(inputs, filters, activation=None, version='b', pars={}, allowed_pars={}):\n    \"\"\"Create a version of v1 inception block described in:\n    https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202\n\n    Create an inception block described in v2, sections 'a', 'b', or 'c'\n    Each version has 4 verticals in their structure. See the link above.\n\n    For all versions, verticals 1 and 2 of the block start with 2D convolution, which:\n        reduces the number of input filters to next convolutions (to make computation cheaper)\n        uses (1, 1) kernels, no Normalization\n        is NOT normalized\n        is followed by specified activation\n    For all versions, verticals 1, 2, 3:\n        the middle convolutions use NConv2D with given activation, see its docstring\n        the final convolution layer is not normalised and not activated since it will be dene after concatenation\n    Vertical 4 is just a Conv2D. Its gets normalized and activated after being concatenated with\n        outputs of other verticals.\n    The concatenated output of the verticals is normalised and then activated with a given activation\n\n    :param inputs: Input 4D tensor (samples, rows, cols, channels)\n    :param filters: Integer, the dimensionality of the output space (i.e. the number of output filters in the\n                    convolution).\n    :param activation: string, specifies activation function to use everywhere in the block\n    :param version: version of inception block, one of 'a', 'b', 'c' (case sensitive)\n    :param pars: dictionary of parameters passed to u-net, determines the version, if this type of block is chosen\n    :param allowed_pars: dictionary of all allowed to be passed to u-net parameters\n    :return: 4D tensor (samples, rows, cols, channels) output of an inception block, given inputs\n    \"\"\"\n    assert filters % 16 == 0\n\n    # checking that the allowed version names did not change in ALLOWED_PARS\n    if allowed_pars != {}:\n        assert allowed_pars.get('information_block').get('inception').get('v2') == ['a', 'b', 'c']\n    # keep version argument if need to use without PARS\n    assert version in ['a', 'b', 'c']\n    # setting the version from pars\n    if pars.get('information_block').get('inception').get('v2') is not None:\n        version = pars.get('information_block').get('inception').get('v2')\n\n    assert activation in ['relu', 'elu', None]\n    # actv is a function, not a string, like activation\n    actv = activation == 'relu' and (lambda: LeakyReLU(0.0)) or activation == 'elu' and (lambda: ELU(1.0)) or None\n\n    # vertical 1\n    c1_1 = Conv2D(filters=filters // 16, kernel_size=(1, 1), padding='same',\n                  activation=activation, kernel_initializer='he_normal')(inputs)\n    if version == 'a':\n        c1_2 = NConv2D(filters=filters // 8, kernel_size=3, padding='same',\n                       activation=activation, kernel_initializer='he_normal')(c1_1)\n        c1 = Conv2D(filters=filters // 8, kernel_size=3, padding='same', kernel_initializer='he_normal')(c1_2)\n    elif version == 'b':\n        c1_2 = NConv2D(filters=filters // 8, kernel_size=(1, 3), padding='same',\n                       activation=activation, kernel_initializer='he_normal')(c1_1)\n        c1_3 = NConv2D(filters=filters // 8, kernel_size=(3, 1), padding='same',\n                       activation=activation, kernel_initializer='he_normal')(c1_2)\n        c1_4 = NConv2D(filters=filters // 8, kernel_size=(1, 3), padding='same',\n                       activation=activation, kernel_initializer='he_normal')(c1_3)\n        c1 = Conv2D(filters=filters // 8, kernel_size=(3, 1), padding='same', kernel_initializer='he_normal')(c1_4)\n    else:\n        c1_2 = NConv2D(filters=filters // 8, kernel_size=(1, 3), padding='same',\n                       activation=activation, kernel_initializer='he_normal')(c1_1)\n        c1_3 = NConv2D(filters=filters // 8, kernel_size=3, padding='same',\n                       activation=activation, kernel_initializer='he_normal')(c1_2)\n        c1_41 = Conv2D(filters=filters // 8, kernel_size=(1, 3), padding='same', kernel_initializer='he_normal')(c1_3)\n        c1_42 = Conv2D(filters=filters // 8, kernel_size=(3, 1), padding='same', kernel_initializer='he_normal')(c1_3)\n        c1 = concatenate([c1_41, c1_42], axis=3)\n\n    # vertical 2\n    c2_1 = Conv2D(filters=filters // 8 * 3, kernel_size=(1, 1), padding='same',\n                  activation=activation, kernel_initializer='he_normal')(inputs)\n    if version == 'a':\n        c2 = Conv2D(filters=filters // 2, kernel_size=(3, 3), padding='same', kernel_initializer='he_normal')(c2_1)\n    elif version == 'b':\n        c2_2 = NConv2D(filters=filters // 2, kernel_size=(1, 3), padding='same',\n                       activation=activation, kernel_initializer='he_normal')(c2_1)\n        c2 = Conv2D(filters=filters // 2, kernel_size=(3, 1), padding='same', kernel_initializer='he_normal')(c2_2)\n    else:\n        c2_21 = Conv2D(filters=filters // 2, kernel_size=(1, 3), padding='same', kernel_initializer='he_normal')(c2_1)\n        c2_22 = Conv2D(filters=filters // 2, kernel_size=(3, 1), padding='same', kernel_initializer='he_normal')(c2_1)\n        c2 = concatenate([c2_21, c2_22], axis=3)\n\n    # vertical 3\n    p3_1 = MaxPooling2D(pool_size=(3, 3), strides=(1, 1), padding='same')(inputs)\n    c3 = Conv2D(filters=filters // 8, kernel_size=(1, 1), padding='same', kernel_initializer='he_normal')(p3_1)\n\n    # vertical 4\n    c4 = Conv2D(filters=filters // 4, kernel_size=(1, 1), padding='same', kernel_initializer='he_normal')(inputs)\n\n    # concatenating verticals together, normalizing and applying activation\n    result = concatenate([c1, c2, c3, c4], axis=3)\n    result = BatchNormalization(axis=3)(result)\n    result = actv()(result)\n    return result\n\n\ndef inception_block_et(inputs, filters, activation='relu', version='b', pars={}, allowed_pars={}):\n    \"\"\"Create an inception block with 2 options.\n    For intuition read, parts v1 and v2:\n    https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202\n\n    Each version/option has 4 verticals in their structure. See the link above.\n    Default option: version='b'\n        Create an inception block close to one described in v2, but keeps 5 as a factor for some convolutions\n    Alternative option: version='a'\n        Create an inception block described in v1, section\n\n\n    Function author Edward Tyantov. That's why the name: inception_block_et.\n    My modifications\n\n        use version='a' instead of split=False\n        use version='b' instead of split=True\n\n        change default to version='b', aka split=True\n\n        swap: Conv2D -> BatchNormalization -> activation\n        to:   NConv2D blocks. See NConv2D documentation for them.\n\n        swap: Conv2D -> activation\n        to:   Conv2D -> Conv2D(activation=activation)\n\n        change the order of the verticals to coincide with v2_paper notation\n\n        change names of the outputs of the block verticals to c1, c2, c3, c4\n\n        use 'result' instead of 'res' to avoid confusion with residuals\n\n    :param inputs: Input 4D tensor (samples, rows, cols, channels)\n    :param filters: Integer, the dimensionality of the output space (i.e. the number of output filters in the\n                    convolution).\n    :param activation: activation function to use everywhere in the block\n    :param version: version of inception block\n    :param pars: dictionary of parameters passed to u-net, determines the version, if this type of block is chosen\n    :param allowed_pars: dictionary of all allowed to be passed to u-net parameters\n    :return: 4D tensor (samples, rows, cols, channels) output of an inception block, given inputs\n    \"\"\"\n    assert filters % 16 == 0\n\n    # checking that the allowed version names did not change in ALLOWED_PARS\n    if allowed_pars != {}:\n        assert allowed_pars.get('information_block').get('inception').get('et') == ['a', 'b']\n    # keep version argument if need to use without PARS\n    assert version in ['a', 'b']\n    # setting the version from pars\n    if pars.get('information_block').get('inception').get('et') is not None:\n        version = pars.get('information_block').get('inception').get('et')\n\n    assert activation in ['relu', 'elu', None]\n    # actv is a function, not a string, like activation\n    actv = activation == 'relu' and (lambda: LeakyReLU(0.0)) or activation == 'elu' and (lambda: ELU(1.0)) or None\n\n    # vertical 1\n    c1_1 = Conv2D(filters=filters // 16, kernel_size=(1, 1), padding='same',\n                  activation=activation, kernel_initializer='he_normal')(inputs)\n    if version == 'b':\n        c1_2 = NConv2D(filters=filters // 8, kernel_size=(1, 5), padding='same',\n                       activation=activation, kernel_initializer='he_normal')(c1_1)\n        c1 = Conv2D(filters=filters // 8, kernel_size=(5, 1), kernel_initializer='he_normal', padding='same')(c1_2)\n    else:\n        c1 = Conv2D(filters=filters // 8, kernel_size=(5, 5), kernel_initializer='he_normal', padding='same')(c1_1)\n\n    # vertical 2\n    c2_1 = Conv2D(filters=filters // 8 * 3, kernel_size=(1, 1), padding='same',\n                  activation=activation, kernel_initializer='he_normal')(inputs)\n    if version == 'b':\n        c2_2 = NConv2D(filters=filters // 2, kernel_size=(1, 3), padding='same',\n                       activation=activation, kernel_initializer='he_normal')(c2_1)\n        c2 = Conv2D(filters=filters // 2, kernel_size=(3, 1), kernel_initializer='he_normal', padding='same')(c2_2)\n    else:\n        c2 = Conv2D(filters=filters // 2, kernel_size=(3, 3), kernel_initializer='he_normal', padding='same')(c2_1)\n\n    # vertical 3\n    p3_1 = MaxPooling2D(pool_size=(3, 3), strides=(1, 1), padding='same')(inputs)\n    c3 = Conv2D(filters=filters // 8, kernel_size=(1, 1), padding='same', kernel_initializer='he_normal')(p3_1)\n\n    # vertical 4\n    c4 = Conv2D(filters=filters // 4, kernel_size=(1, 1), padding='same', kernel_initializer='he_normal')(inputs)\n\n    # concatenating verticals together, normalizing and applying activation\n    result = concatenate([c1, c2, c3, c4], axis=3)\n    result = BatchNormalization(axis=3)(result)\n    result = actv()(result)\n    return result\n\n\n# ======================================================================================================================\n# Combining blocks, allowing to use different blocks from before\n# ======================================================================================================================\n\ndef pooling_block(inputs, filters, kernel_size=(3, 3), strides=(2, 2), padding='same', activation=None,\n                  pool_size=(2, 2), trainable=True, pars={}, allowed_pars={}):\n    \"\"\"Function returning the output of one of the pooling blocks.\n\n    Allows not to make different versions of the u-net in terms of how pooling operation is performed:\n        1) trainable (default): through NConv2D custom function, see its documentation\n        2) non-trainable (alternative): through MaxPooling operation\n\n    To get the expected behaviour when changing 'trainable' assert strides == pool_size\n\n    Parameters starting with p_ are only to be used for (trainable=False) MaxPooling2D\n    Parameters starting with c_ are only to be used for (trainable=True) MaxPooling2D\n\n    :param inputs: 4D tensor (samples, rows, cols, channels)\n    :param filters:     NConv2D argument, filters\n    :param kernel_size: NConv2D argument, kernel_size\n    :param strides:     NConv2D argument, strides\n    :param padding:     NConv2D/MaxPooling2D argument, padding\n    :param activation:  NConv2D argument, activation\n    :param pool_size:   MaxPooling2D argument, pool_size\n\n    :param trainable: boolean specifying the version of a pooling block with default behaviour\n        trainable=True: NConv2D(inputs._keras_shape[3], kernel_size=kernel_size, strides=strides, padding=padding)(\n        inputs)\n        trainable=False: MaxPooling2D(pool_size=pool_size)(inputs)\n    :param pars: dictionary of parameters passed to u-net, determines the version of the block\n    :param allowed_pars: dictionary of all allowed to be passed to u-net parameters\n\n    :return: 4D tensor (samples, rows, cols, channels) output of a pooling block\n    \"\"\"\n    # checking that the allowed trainable parameters did not change in ALLOWED_PARS\n    if allowed_pars != {}:\n        assert allowed_pars.get('pooling_block').get('trainable') == [True, False]\n    # keep trainable argument if need to use without PARS\n    assert trainable in [True, False]\n\n    # setting the version from pars\n    if pars.get('pooling_block').get('trainable') is not None:\n        trainable = pars.get('pooling_block').get('trainable')\n\n    # returning block's output\n    if trainable:\n        return NConv2D(filters=filters, kernel_size=kernel_size, strides=strides,\n                       padding=padding, activation=activation)(inputs)\n    else:\n        return MaxPooling2D(pool_size=pool_size, padding=padding)(inputs)\n\n\ndef information_block(inputs, filters, kernel_size=(3, 3), padding='valid', activation=None,\n                      block='inception', block_type='v2', version='b', pars={}, allowed_pars={}):\n    \"\"\"Function returning the output of one of the information blocks.\n\n    :param inputs: 4D tensor (samples, rows, cols, channels)\n    :param filters: Integer, the dimensionality of the output space (i.e. the number of output filters in the\n                    convolution).\n    :param kernel_size: An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution\n                        window. Can be a single integer to specify the same value for all spatial dimensions.\n    :param padding: one of 'valid' or 'same' (case-insensitive), 'valid' by default to have the same as Conv2D\n    :param activation: string, specifies activation function to use everywhere in the block\n\n    Next 3 parameters are there to be able to leave 'pars' and 'allowed_pars' empty\n    :param block:       one of 'inception' or 'convolution' (case-sensitive)\n    :param block_type:  if block == 'inception', one of 'v1', 'v2', 'et' (case-sensitive)\n                        if block == 'convolution': one of 'simple', 'dilated' (case-sensitive)\n    :param version:     version of a block to use\n\n    :param pars: dictionary of parameters passed to u-net, determines the version of the block\n    :param allowed_pars: dictionary of all allowed to be passed to u-net parameters\n\n    :return: 4D tensor (samples, rows, cols, channels) output of a information block\n    \"\"\"\n    # getting which block, block_type, version to use as the information block\n    if pars.get('information_block') is not None:\n        block = list(pars.get('information_block').keys())[0]\n        block_type = list(pars.get('information_block').get(block).keys())[0]\n        version = pars.get('information_block').get(block).get(block_type)\n\n    # inception block\n    if block == 'inception':\n        if block_type == 'v1':\n            return inception_block_v1(inputs=inputs, filters=filters, activation=activation,\n                                      version=version, pars=pars, allowed_pars=allowed_pars)\n        elif block_type == 'v2':\n            return inception_block_v2(inputs=inputs, filters=filters, activation=activation,\n                                      version=version, pars=pars, allowed_pars=allowed_pars)\n        else:\n            return inception_block_et(inputs=inputs, filters=filters, activation=activation,\n                                      version=version, pars=pars, allowed_pars=allowed_pars)\n    # convolution block\n    else:\n        if block_type == 'simple':\n            return convolution_block(inputs=inputs, filters=filters, kernel_size=kernel_size,\n                                     padding=padding, activation=activation,\n                                     version=version, pars=pars, allowed_pars=allowed_pars)\n        else:\n            return dilated_convolution_block(inputs=inputs, filters=filters,\n                                             kernel_size=kernel_size, padding=padding,\n                                             activation=activation, version=version,\n                                             pars=pars, allowed_pars=allowed_pars)\n\n\ndef connection_block(inputs, filters, padding='valid', activation=None,\n                     version='residual', pars={}, allowed_pars={}):\n    \"\"\"Function returning the output of one of the connection block.\n\n    :param inputs: 4D tensor (samples, rows, cols, channels)\n    :param filters: Integer, the dimensionality of the output space (i.e. the number of output filters in the\n                    convolution).\n    :param padding: one of 'valid' or 'same' (case-insensitive), 'valid' by default to have the same as Conv2D\n    :param activation:  string, one of 'elu' or 'relu' or None (case-sensitive),\n                        specifies activation function to use everywhere in the block\n\n    Version parameter is there to be able to leave 'pars' and 'allowed_pars' empty\n    :param version: one of 'not_residual' or 'residual', version of a block to use\n\n    :param pars: dictionary of parameters passed to u-net, determines the version of the block\n    :param allowed_pars: dictionary of all allowed to be passed to u-net parameters\n\n    :return: 4D tensor (samples, rows, cols, channels) output of a connection block\n    \"\"\"\n    # checking that the allowed trainable parameters did not change in ALLOWED_PARS\n    if allowed_pars != {}:\n        assert allowed_pars.get('connection_block') == ['not_residual', 'residual']\n    # keep trainable argument if need to use without PARS\n    assert version in ['not_residual', 'residual']\n    # setting the version from pars\n    if pars.get('connection_block') is not None:\n        version = pars.get('connection_block')\n\n    if version == 'residual':\n        return rblock(inputs=inputs, filters=32, kernel_size=(1, 1), padding='same', activation=activation)\n    else:\n        return Conv2D(filters=filters, kernel_size=(2, 2), padding=padding, kernel_initializer='he_normal')(inputs)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## U-model module\n\nFile name: u_model.py\n\nInstructions: if used as as separate module, the \"separate-module imports\" part needs to be uncommented\n\nCredits: Marko Jocić, Edward Tyantov\n\nModification: Make the architecture fully customisable"},{"metadata":{"trusted":true},"cell_type":"code","source":"execute_u_model = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################################################################################################################\n# ======================================================================================================================\n# u_model\n# ======================================================================================================================\n########################################################################################################################\n# needed for train\n\n# standard-module imports\nimport numpy as np\nfrom keras.layers import Input, concatenate, Conv2D, UpSampling2D, Dense\nfrom keras.layers import Dropout, Flatten\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nfrom keras import backend as K\n\n# # separate-module imports\n# from metric import dice_coef, dice_coef_loss\n# from u_model_blocks import pooling_block, connection_block, information_block\n# from configuration import ALLOWED_PARS, PARS\n\n\nIMG_ROWS, IMG_COLS = 80, 112\nK.set_image_data_format('channels_last')  # (number of images, rows per image, cols per image, channels)\n\n\n# ======================================================================================================================\n# U-net with Inception blocks, Normalised 2D Convolutions instead of Maxpooling\n# ======================================================================================================================\n\ndef get_unet_customised(optimizer, pars=PARS, allowed_pars=ALLOWED_PARS):\n    \"\"\"\n    Creating and compiling the U-net\n\n    This version is fully customisable by choosing pars argument\n\n    :param optimizer: specifies the optimiser for the u-net, e.g. Adam, RMSProp, etc.\n    :param pars: optional, dictionary of parameters passed to customise the U-net\n    :param allowed_pars: optional, dictionary of parameters allowed to be passed to customise the U-net\n    :return: compiled u-net, Keras.Model object\n    \"\"\"\n\n    # string, activation function\n    activation = pars.get('activation')\n\n    # input\n    inputs = Input((IMG_ROWS, IMG_COLS, 1), name='main_input')\n    print('inputs:', inputs._keras_shape)\n\n    #\n    # down the U-net\n    #\n\n    conv1 = information_block(inputs, 32, padding='same', activation=activation, pars=pars, allowed_pars=allowed_pars)\n    print('conv1', conv1._keras_shape)\n    pool1 = pooling_block(inputs=conv1, filters=32, activation=activation, pars=pars, allowed_pars=allowed_pars)\n    print('pool1', pool1._keras_shape)\n    pool1 = Dropout(0.5)(pool1)\n    print('pool1', pool1._keras_shape)\n\n    conv2 = information_block(pool1, 64, padding='same', activation=activation, pars=pars, allowed_pars=allowed_pars)\n    print('conv2', conv2._keras_shape)\n    pool2 = pooling_block(inputs=conv2, filters=64, activation=activation, pars=pars, allowed_pars=allowed_pars)\n    print('pool2', pool2._keras_shape)\n    pool2 = Dropout(0.5)(pool2)\n    print('pool2', pool2._keras_shape)\n\n    conv3 = information_block(pool2, 128, padding='same', activation=activation, pars=pars, allowed_pars=allowed_pars)\n    print('conv3', conv3._keras_shape)\n    pool3 = pooling_block(inputs=conv3, filters=128, activation=activation, pars=pars, allowed_pars=allowed_pars)\n    print('pool3', pool3._keras_shape)\n    pool3 = Dropout(0.5)(pool3)\n    print('pool3', pool3._keras_shape)\n\n    conv4 = information_block(pool3, 256, padding='same', activation=activation, pars=pars, allowed_pars=allowed_pars)\n    print('conv4', conv4._keras_shape)\n    pool4 = pooling_block(inputs=conv4, filters=256, activation=activation, pars=pars, allowed_pars=allowed_pars)\n    print('pool4', pool4._keras_shape)\n    pool4 = Dropout(0.5)(pool4)\n    print('pool4', pool4._keras_shape)\n\n    #\n    # bottom level of the U-net\n    #\n    conv5 = information_block(pool4, 512, padding='same', activation=activation, pars=pars, allowed_pars=allowed_pars)\n    print('conv5', conv5._keras_shape)\n    conv5 = Dropout(0.5)(conv5)\n    print('conv5', conv5._keras_shape)\n\n    #\n    # auxiliary output for predicting probability of nerve presence\n    #\n    if pars['outputs'] == 2:\n        pre = Conv2D(1, kernel_size=(1, 1), kernel_initializer='he_normal', activation='sigmoid')(conv5)\n        pre = Flatten()(pre)\n        aux_out = Dense(1, activation='sigmoid', name='aux_output')(pre)\n\n    #\n    # up the U-net\n    #\n\n    after_conv4 = connection_block(conv4, 256, padding='same', activation=activation,\n                                   pars=pars, allowed_pars=allowed_pars)\n    print('after_conv4', after_conv4._keras_shape)\n    up6 = concatenate([UpSampling2D(size=(2, 2))(conv5), after_conv4], axis=3)\n    conv6 = information_block(up6, 256, padding='same', activation=activation, pars=pars, allowed_pars=allowed_pars)\n    print('conv6', conv6._keras_shape)\n    conv6 = Dropout(0.5)(conv6)\n    print('conv6', conv6._keras_shape)\n\n    after_conv3 = connection_block(conv3, 128, padding='same', activation=activation,\n                                   pars=pars, allowed_pars=allowed_pars)\n    print('after_conv3', after_conv3._keras_shape)\n    up7 = concatenate([UpSampling2D(size=(2, 2))(conv6), after_conv3], axis=3)\n    conv7 = information_block(up7, 128, padding='same', activation=activation, pars=pars, allowed_pars=allowed_pars)\n    print('conv7', conv7._keras_shape)\n    conv7 = Dropout(0.5)(conv7)\n    print('conv7', conv7._keras_shape)\n\n    after_conv2 = connection_block(conv2, 64, padding='same', activation=activation, pars=pars,\n                                   allowed_pars=allowed_pars)\n    print('after_conv2', after_conv2._keras_shape)\n    up8 = concatenate([UpSampling2D(size=(2, 2))(conv7), after_conv2], axis=3)\n    conv8 = information_block(up8, 64, padding='same', activation=activation, pars=pars, allowed_pars=allowed_pars)\n    print('conv8', conv8._keras_shape)\n    conv8 = Dropout(0.5)(conv8)\n    print('conv8', conv8._keras_shape)\n\n    after_conv1 = connection_block(conv1, 32, padding='same', activation=activation,\n                                   pars=pars, allowed_pars=allowed_pars)\n    print('after_conv1', after_conv1._keras_shape)\n    up9 = concatenate([UpSampling2D(size=(2, 2))(conv8), after_conv1], axis=3)\n    conv9 = information_block(up9, 32, padding='same', activation=activation, pars=pars, allowed_pars=allowed_pars)\n    print('conv9', conv9._keras_shape)\n    conv9 = Dropout(0.5)(conv9)\n    print('conv9', conv9._keras_shape)\n\n    # main output\n    conv10 = Conv2D(1, kernel_size=(1, 1), kernel_initializer='he_normal', activation='sigmoid', name='main_output')(\n        conv9)\n    print('conv10', conv10._keras_shape)\n\n    # creating a model\n    # compiling the model\n    if pars['outputs'] == 1:\n        model = Model(inputs=inputs, outputs=conv10)\n        model.compile(optimizer=optimizer,\n                      loss={'main_output': dice_coef_loss},\n                      metrics={'main_output': dice_coef})\n    else:\n        model = Model(inputs=inputs, outputs=[conv10, aux_out])\n        model.compile(optimizer=optimizer,\n                      loss={'main_output': dice_coef_loss, 'aux_output': 'binary_crossentropy'},\n                      metrics={'main_output': dice_coef, 'aux_output': 'acc'},\n                      loss_weights={'main_output': 1., 'aux_output': 0.5})\n\n    return model\n\n\n# ----------------------------------------------------------------------------------------------------------------------\n\n# get_unet() allows to try other versions of the u-net, if more are specified\nget_unet = get_unet_customised\n\nif __name__ == '__main__' and (execute_u_model==True or execute_all==True):\n    # test the u-net without training\n\n    img_rows = IMG_ROWS\n    img_cols = IMG_COLS\n\n    # to check that model works without training, any kind of optimiser can be used\n    model = get_unet(Adam(lr=1e-5), pars=PARS)\n\n    x = np.random.random((1, img_rows, img_cols, 1))\n    result = model.predict(x, 1)\n    print(result)\n    print('params', model.count_params())\n    print('layer num', len(model.layers))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train module\n\nFile name: train.py\n\nCredits: Marko Jocić\n\nModifications: allow for training with 1 or 2 outputs of the U-nel-like architecture\n\nInstructions: If used as as separate module, the \"separate-module imports\" part needs to be uncommented"},{"metadata":{"trusted":true},"cell_type":"code","source":"execute_train = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################################################################################################################\n# ======================================================================================================================\n# train\n# ======================================================================================================================\n########################################################################################################################\n\n# standard-module imports\nimport numpy as np\nfrom skimage.transform import resize\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\n\n# # separate-module imports\n# from u_model import get_unet, IMG_COLS as img_cols, IMG_ROWS as img_rows\n# from data import load_train_data, load_test_data, load_nerve_presence\n# from configuration import PARS, OPTIMIZER\n\n\ndef preprocess(imgs, to_rows=None, to_cols=None):\n    \"\"\"Resize all images in a 4D tensor of images of the shape (samples, rows, cols, channels).\n\n    :param imgs: a 4D tensor of images of the shape (samples, rows, cols, channels)\n    :param to_rows: new number of rows for images to be resized to\n    :param to_cols: new number of rows for images to be resized to\n    :return: a 4D tensor of images of the shape (samples, to_rows, to_cols, channels)\n    \"\"\"\n    if to_rows is None or to_cols is None:\n        to_rows = img_rows\n        to_cols = img_cols\n\n    print(imgs.shape)\n    imgs_p = np.ndarray((imgs.shape[0], to_rows, to_cols, imgs.shape[3]), dtype=np.uint8)\n    for i in range(imgs.shape[0]):\n        imgs_p[i, :, :, 0] = resize(imgs[i, :, :, 0], (to_rows, to_cols), preserve_range=True)\n    return imgs_p\n\n\ndef train_and_predict():\n    print('-' * 30)\n    print('Loading and preprocessing train data...')\n    print('-' * 30)\n    imgs_train, imgs_mask_train = load_train_data()\n    imgs_present = load_nerve_presence()\n\n    imgs_train = preprocess(imgs_train)\n    imgs_mask_train = preprocess(imgs_mask_train)\n\n    # centering and standardising the images\n    imgs_train = imgs_train.astype('float32')\n    mean = np.mean(imgs_train)\n    std = np.std(imgs_train)\n    imgs_train -= mean\n    imgs_train /= std\n\n    imgs_mask_train = imgs_mask_train.astype('float32')\n    imgs_mask_train /= 255.  # scale masks to be in {0, 1} instead of {0, 255}\n\n    print('-' * 30)\n    print('Creating and compiling model...')\n    print('-' * 30)\n\n    # load model - the Learning rate scheduler choice is most important here\n    model = get_unet(optimizer=OPTIMIZER, pars=PARS)\n\n    model_checkpoint = ModelCheckpoint('weights.h5', monitor='val_loss', save_best_only=True)\n    early_stopping = EarlyStopping(patience=5, verbose=1)\n\n    print('-' * 30)\n    print('Fitting model...')\n    print('-' * 30)\n\n    if PARS['outputs'] == 1:\n        imgs_labels = imgs_mask_train\n    else:\n        imgs_labels = [imgs_mask_train, imgs_present]\n\n    model.fit(imgs_train, imgs_labels,\n              batch_size=128, epochs=50,\n              verbose=1, shuffle=True,\n              validation_split=0.2,\n              callbacks=[model_checkpoint, early_stopping])\n\n    print('-' * 30)\n    print('Loading and preprocessing test data...')\n    print('-' * 30)\n    imgs_test = load_test_data()\n    imgs_test = preprocess(imgs_test)\n\n    imgs_test = imgs_test.astype('float32')\n    imgs_test -= mean\n    imgs_test /= std\n\n    print('-' * 30)\n    print('Loading saved weights...')\n    print('-' * 30)\n    model.load_weights('weights.h5')\n\n    print('-' * 30)\n    print('Predicting masks on test data...')\n    print('-' * 30)\n\n    imgs_mask_test = model.predict(imgs_test, verbose=1)\n\n    if PARS['outputs'] == 1:\n        np.save('imgs_mask_test.npy', imgs_mask_test)\n    else:\n        np.save('imgs_mask_test.npy', imgs_mask_test[0])\n        np.save('imgs_mask_test_present.npy', imgs_mask_test[1])\n        \n\n# --------------------------------------------------------------------------------------------------------------------\n\nif __name__ == '__main__' and (execute_train==True or execute_all==True):\n    train_and_predict()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submission module\n\nFile name: submission.py\n\nCredits: Edward Tyantov\n\nModifications: allow for training with 1 or 2 outputs of the U-nel-like architecture\n\nInstructions: If used as as separate module, the \"separate-module imports\" part needs to be uncommented"},{"metadata":{"trusted":true},"cell_type":"code","source":"execute_submission = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################################################################################################################\n# ======================================================================================================================\n# Submission\n# ======================================================================================================================\n########################################################################################################################\n\n# standard-module imports\nimport os\nimport numpy as np\nfrom skimage.transform import resize\nfrom itertools import chain\n\n\n# # separate-module imports\n# from configuration import PARS\n# from data import load_test_ids, image_rows, image_cols, _dir\n\ndef prep(img):\n    \"\"\"Prepare the image for to be used in a submission\n\n    :param img: 2D image\n    :return: resized version of an image\n    \"\"\"\n    img = img.astype('float32')\n    img = resize(img, (image_rows, image_cols), preserve_range=True)\n    img = (img > 0.5).astype(np.uint8)  # threshold\n    return img\n\n\ndef run_length_enc(label):\n    \"\"\"Create a run-length-encoding of an image\n\n    :param label: image to be encoded\n    :return: string with run-length-encoding of an image\n    \"\"\"\n    x = label.transpose().flatten()\n    y = np.where(x > 0)[0]\n\n    # consider empty all masks with less than 10 pixels being greater than 0\n    if len(y) < 10:\n        return ''\n\n    z = np.where(np.diff(y) > 1)[0]\n    start = np.insert(y[z + 1], 0, y[0])\n    end = np.append(y[z], y[-1])\n    length = end - start\n    res = [[s + 1, l + 1] for s, l in zip(list(start), list(length))]\n    res = list(chain.from_iterable(res))\n    return ' '.join([str(r) for r in res])\n\n\ndef submission():\n    \"\"\"Create a submission .csv file.\n\n    The file will have 2 cols: img, pixels.\n        The image column consists of the ids of test images.\n        The pixels column consists of the run-length-encodings of the corresponding images.\n    \"\"\"\n    imgs_id_test = load_test_ids()\n\n    print('Loading imgs_test from imgs_mask_test.npy')\n    imgs_test = np.load('imgs_mask_test.npy')\n    if PARS['outputs'] == 2:\n        print('Loading imgs_exist_test from imgs_mask_test_present.npy')\n        imgs_exist_test = np.load('imgs_mask_test_present.npy')\n\n    argsort = np.argsort(imgs_id_test)\n    imgs_id_test = imgs_id_test[argsort]\n    imgs_test = imgs_test[argsort]\n    if PARS['outputs'] == 2:\n        imgs_exist_test = imgs_exist_test[argsort]\n\n    total = imgs_test.shape[0]\n    ids = []\n    rles = []  # run-length-encodings\n    for i in range(total):\n        img = imgs_test[i, :, :, 0]\n        if PARS['outputs'] == 2:\n            img_exist = imgs_exist_test[i]\n        img = prep(img)\n\n        # only for version with 2 outputs\n        if PARS['outputs'] == 2:\n            # new probability of nerve presence\n            new_prob = (img_exist + min(1, np.sum(img) / 10000.0) * 5 / 3) / 2\n            # setting mask to array of zeros if new probability of nerve presence < 0.5\n            if np.sum(img) > 0 and new_prob < 0.5:\n                img = np.zeros((image_rows, image_cols))\n\n        # producing run-length encoded version of the image\n        rle = run_length_enc(img)\n\n        rles.append(rle)\n        ids.append(imgs_id_test[i])\n\n        if i % 100 == 0:\n            print('{}/{}'.format(i, total))\n\n    # creating a submission file\n    file_name = os.path.join(_dir, 'submission.csv')\n    with open(file_name, 'w+') as f:\n        f.write('img,pixels\\n')\n        for i in range(total):\n            s = str(ids[i]) + ',' + rles[i]\n            f.write(s + '\\n')\n\n\n# --------------------------------------------------------------------------------------------------------------------\nif __name__ == '__main__' and (execute_submission==True or execute_all==True):\n    submission()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[MJ's Kaggle]: https://www.kaggle.com/c/ultrasound-nerve-segmentation/discussion/21358#latest-651570\n[MJ's GitHub]: https://github.com/jocicmarko/ultrasound-nerve-segmentation\n[ET's Kaggle]: https://www.kaggle.com/c/ultrasound-nerve-segmentation/discussion/22958#latest-132645\n[ET's GitHub]: https://github.com/EdwardTyantov/ultrasound-nerve-segmentation"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}