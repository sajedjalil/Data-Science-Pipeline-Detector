{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \nimport os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom PIL import Image\nimport cv2\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\ntrainpaths = []\ntestpaths = []\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        fpath = os.path.join(dirname, filename)\n        if os.path.splitext(fpath)[1] == \".tif\":\n            if \"train\" in fpath and not \"mask\" in fpath:\n                trainpaths.append(fpath)\n            elif \"test\" in fpath and not \"mask\" in fpath:\n                testpaths.append(fpath)\n\ndef show_tif(im, title=None):\n    if title:\n        plt.title(title)\n    plt.imshow(im, cmap=\"gray\")\n    plt.show()\n    \ndef find_mask_file(trf):\n    dir, name = os.path.split (trf)\n    name = os.path.splitext(name)[0] + \"_mask\" + \".tif\"\n    return os.path.join(dir, name)\n\ndef mask_over_image(trf):\n    a = np.array(Image.open(trf))\n    b = (np.array(Image.open(find_mask_file(trf)))/255).astype(np.bool)\n    assert a.shape == b.shape\n    return b * a\n\ndef show_im_and_mask(trf, fs=(10,10)):\n    f, axarr = plt.subplots (1,2, figsize=fs)\n    axarr[0].imshow(Image.open(trf), cmap=\"gray\")\n    axarr[0].set_title(os.path.split(trf)[1])\n    m = mask_over_image(trf)\n    axarr[1].set_title(os.path.split(find_mask_file(trf))[1])\n    axarr[1].imshow(m, cmap=\"gray\")\n    plt.show()\n    \ndef split_into_patches(img, dims, mask, overlap=False):\n    # An image is 420 x 580; #TODO: if wanted, we can make them overlap\n    if img.shape[0] != 420 and img.shape[1] != 580:\n        raise ValueError (\"img shape %s not 420 x 580\"%str(img.shape))\n    patches = []\n    if ((img.shape[0] / dims[0]) != (img.shape[0] // dims[0])) or ((img.shape[1] / dims[1]) != (img.shape[1] // dims[1])): \n        raise ValueError (\"Patch dimension must be evenly divisible\")\n    numys = img.shape[0] // dims[0]\n    numxs = img.shape[1] // dims[1]\n    labs = np.zeros(numys * numxs, dtype=np.float32)\n    for i in range(numys):\n        for j in range(numxs):\n            if len(img.shape) < 3: \n                patches.append(img[i * dims[0]: (i+1) * dims[0], j * dims[1]: (j+1) * dims[1]])\n                labs[i * numxs + j] = np.sum(mask[i * dims[0]: (i+1) * dims[0], j * dims[1]: (j+1) * dims[1]])\n            else: # 3rd dimension is feature space or RGB channels, shouldn't change the piecewise\n                patches.append(img[i * dims[0]: (i+1) * dims[0], j * dims[1]: (j+1) * dims[1], :])\n                labs[i * numxs + j] = np.sum(mask[i * dims[0]: (i+1) * dims[0], j * dims[1]: (j+1) * dims[1]])\n    return np.stack(patches), labs\n    \nprint (len(trainpaths), \" training images\")\nprint (len(testpaths), \" test images\")\ntrain_masks = pd.read_csv(\"/kaggle/input/ultrasound-nerve-segmentation/train_masks.csv\") \nprint (train_masks.head())\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"for j in range(10):\n    show_im_and_mask(trainpaths[j])\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Introduction\nWe adopt a 5-stage philosophy as discussed by [Hadjerci et. al.  ](https://www.sciencedirect.com/science/article/pii/S2352914816300089):\n- 1: Preprocessing\n- 2: Denoising\n- 3: Feature extraction\n- 4: Feature selection\n- 5: Classification and Segementation"},{"metadata":{},"cell_type":"markdown","source":"# Stage 1 and 2: Preprocessing and Noise Reduction\nContrast-enhancement: Contrast-limited adaptive histogram-equalization (CLAHE), to make nerve stand out more in local region. We may take this out if this induces some bias. <br>\n\n\nDespeckling/noise reduction: median filtering: Small median filter can help get rid of few-pixel aberrations in a small neighborhood. This also can be taken out if we find valuable image information is lost as a result."},{"metadata":{"trusted":true},"cell_type":"code","source":"for j in range(10):\n    imf = trainpaths[j]\n    im = cv2.imread(imf)\n    mask = cv2.imread(find_mask_file(imf))\n    if np.sum(mask) == 0:\n        continue\n    mask_outline = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n    mask_outline = cv2.blur(mask, (3,3)) # blur and filter is just to find the border of the labeled region for display\n    mask_outline = mask_outline * ((mask_outline < 255) & (mask_outline > 0))\n    gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n    cl1 = clahe.apply(gray)\n    med = cv2.medianBlur(cl1, 3)\n    f, axarr = plt.subplots(1,3, figsize=(20,20))\n    axarr[0].imshow(im, cmap=\"gray\")\n    axarr[0].imshow(mask_outline, cmap=\"gray\", alpha=0.4)\n    axarr[0].set_title(\"Original Image\")\n    axarr[1].imshow(cl1, cmap=\"gray\")\n    axarr[1].set_title(\"CLAHE-applied\")\n    axarr[1].imshow(mask_outline, cmap=\"gray\", alpha=0.4)\n    axarr[2].imshow(med, cmap=\"gray\")\n    axarr[2].set_title(\"CLAHE followed by median filter\")\n    axarr[2].imshow(mask_outline, cmap=\"gray\", alpha=0.4)\n\n    plt.show()\n\n    xs, ys = np.where(mask)[0], np.where(mask)[1]\n    f, axarr = plt.subplots(1,3, figsize=(20,20))\n    buff = 10\n    \n    axarr[0].imshow(im[min(xs)-buff: max(xs)+buff, min(ys)-buff:max(ys)+buff], cmap=\"gray\")\n    axarr[0].imshow(mask_outline[min(xs)-buff: max(xs)+buff, min(ys)-buff:max(ys)+buff], cmap=\"gray\", alpha=0.4)\n    axarr[0].set_title(\"Original Image\")\n    axarr[1].imshow(cl1[min(xs)-buff: max(xs)+buff, min(ys)-buff:max(ys)+buff], cmap=\"gray\")\n    axarr[1].set_title(\"CLAHE-applied\")\n    axarr[1].imshow(mask_outline[min(xs)-buff: max(xs)+buff, min(ys)-buff:max(ys)+buff], cmap=\"gray\", alpha=0.4)\n    axarr[2].imshow(med[min(xs)-buff: max(xs)+buff, min(ys)-buff:max(ys)+buff], cmap=\"gray\")\n    axarr[2].set_title(\"CLAHE followed by median filter\")\n    axarr[2].imshow(mask_outline[min(xs)-buff: max(xs)+buff, min(ys)-buff:max(ys)+buff], cmap=\"gray\", alpha=0.4)\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Stage 3: Feature Extraction\nCandidate procedure: Local binary patterns (LBPs). See https://scikit-image.org/docs/dev/auto_examples/features_detection/plot_local_binary_pattern.html for a good overview. We aim to isolate the texture(s) of the region of interest (ROI) and use those features. LBP will output a set of values per pixel, and a histogram of those values (ideally) identifies the textures in the image. If we look at the textures in the ROI, hopefully those correspond to the same peaks/intervals on the histograms of given images.\n\nCandidate procedure: Histogram-of-oriented gradients (HoG) transform: Take neighborhoods of pixels and make a histogram of the gradients of the pixels in the neighborhood. Concatenate these histograms to form a feature vector for an image."},{"metadata":{"trusted":true},"cell_type":"code","source":"from skimage.feature import hog, local_binary_pattern\n\nlbp_8_1 = local_binary_pattern(med, P=8, R=1, method=\"ror\") # rotation-invariant by default\nlbp_16_2 = local_binary_pattern(med, P=16, R=2, method=\"ror\")\nfeatures, hogim = hog(med, visualize=True)\n\nf, axarr = plt.subplots(1,4, figsize=(20,20))\naxarr[0].imshow(med, cmap=\"gray\")\naxarr[0].imshow(mask_outline, cmap=\"gray\", alpha=0.4)\naxarr[0].set_title(\"CLAHE-Median filtered Image\")\naxarr[1].imshow(lbp_8_1, cmap=\"gray\")\naxarr[1].imshow(mask_outline, cmap=\"gray\", alpha=0.4)\naxarr[1].set_title(\"LBP R=1, P=8\")\naxarr[2].imshow(lbp_16_2, cmap=\"gray\")\naxarr[2].imshow(mask_outline, cmap=\"gray\", alpha=0.4)\naxarr[2].set_title(\"LBP R=3, P=16\")\naxarr[3].imshow(hogim)\naxarr[3].set_title(\"HOG Feature Visualization\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Stage 4: Feature Selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"xs, ys = np.where(mask)[0], np.where(mask)[1]\nplt.imshow(lbp_8_1[min(xs):max(xs), min(ys):max(ys)])\nplt.imshow(mask[min(xs):max(xs), min(ys):max(ys)], alpha=0.5)\nplt.title(\"LBPs in ROI\")\nplt.colorbar()\nplt.show()\n\nf, axarr = plt.subplots(2, figsize=(12, 5))\nglobal_hist = np.histogram(lbp_8_1.flatten(),bins=256)\nlocal_hist = np.histogram(lbp_8_1[min(xs):max(xs), min(ys):max(ys)].flatten(), bins=256)\n\naxarr[0].hist(lbp_8_1.flatten(), bins=256)\naxarr[0].grid()\naxarr[0].set_title(\"Global Image LBP histogram\")\naxarr[1].hist(lbp_8_1[min(xs):max(xs), min(ys):max(ys)].flatten(), bins=256)\naxarr[1].grid()\naxarr[1].set_title(\"RoI LBP Histogram: KL %f\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like the LBP output is frequent for a few selected parts. We will say any with over 10000 occurrences are salient. TODO: Quantify with Fisher score or other metric"},{"metadata":{"trusted":true},"cell_type":"code","source":"stack, labs = split_into_patches(lbp_8_1, (70,58), np.array(Image.open(find_mask_file(trainpaths[0]))).astype(np.bool))\n\nprint (\"%d stacks made\" %stack.shape[0])\nf, axarr = plt.subplots(6, 10, figsize=(60,40))\nplt.suptitle (\" A bunch of patches\")\nfor i in range(6):\n    for j in range(10):\n        k = i * 10 + j \n        axarr[i,j].set_title(\"(\" + str(i * 70) + \" , \" + str(j * 58) + \"), %f\"%labs[k])\n        axarr[i,j].imshow(stack[k, :, :])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Stage 5: Support-Vector Machine based Detection on Patches\nWhich patches, if any, show the ROI?"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVR\nfrom sklearn.metrics import confusion_matrix\nimport pandas\nxTr = stack.reshape(stack.shape[0], -1)\nyTr = labs\nsvm = SVR(kernel=\"rbf\", gamma=\"scale\", C=1.0)\nsvm.fit(xTr, labs)\npreds = svm.predict(xTr)\none_hot_labs = labs > 0\none_hot_preds = preds > 0.3 # TODO: what is a good way to threshold/round continuous SVR output \nconfusion = confusion_matrix(one_hot_labs, one_hot_preds)\nconfusion ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On this overfit, single-example SVM had 54 true negatives and 6 true positives; a perfect result."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport datetime\n\ndef preprocess (img_path):\n    im = cv2.imread(imf)\n    mask = cv2.imread(find_mask_file(imf))\n    mask_outline = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n    mask_outline = cv2.blur(mask, (3,3))\n    mask_outline = mask_outline * ((mask_outline < 255) & (mask_outline > 0))\n    gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n    cl1 = clahe.apply(gray)\n    med = cv2.medianBlur(cl1, 3)\n    return med\n\ndef generate_features(img):\n    lbp_8_1 = local_binary_pattern(med, P=8, R=1, method=\"ror\") # rotation-invariant by default\n    lbp_16_2 = local_binary_pattern(med, P=16, R=2, method=\"ror\")\n    features, hogim = hog(med, visualize=True)\n    return np.stack([lbp_8_1, lbp_16_2,hogim], axis=-1)\n    \ndef select_features(img):\n    return img\n\ndef prepare_set (xpaths, patch_size):\n    xs, ys = [], []\n    for path in xpaths:\n        med = preprocess(path)\n        features = generate_features(med)\n        features = select_features(features)\n        stack, labs = split_into_patches(features, patch_size, np.array(Image.open(find_mask_file(path))).astype(np.bool))\n        stack = stack.reshape(stack.shape[0], -1)\n        xs.append(stack)\n        ys.append(labs)\n    xs = np.array(xs)\n    ys = np.array(ys)\n    return xs.reshape(-1, xs.shape[2]), ys.flatten()\n\ndef train_model(model, xTr, yTr, patch_size=(70,58),xVal=None,random_seed=0):\n    start = datetime.datetime.now()\n    model = model.fit(xTr, yTr)\n    trpreds = model.predict(xTr)\n    if xVal is not None: \n        valpreds = model.predict(xVal)\n    else:\n        valpreds = None\n    print (\"Model finished training %d patches after %s\"%(xTr.shape[0], str(datetime.datetime.now() - start)))\n    return trpreds, valpreds\n\n\ndef patch_confusion(labs, predictions):\n    one_hot_labs = labs > 0\n    one_hot_preds = predictions > 0.5 # TODO: what is a good way to threshold/round continuous SVR output \n    confusion = confusion_matrix(one_hot_labs, one_hot_preds)\n    return confusion\n\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svm = SVR(kernel=\"linear\", gamma=\"scale\", C=5.0)\nseed = 0\npatch_size = (70, 58)\ntr_paths, val_paths = train_test_split(trainpaths[:150], shuffle=True, test_size=0.2, random_state=seed)\nxTr,yTr = prepare_set(tr_paths, patch_size)\nxVal, yVal = prepare_set(val_paths, patch_size)\n# xTest, yTest = prepare_set(test_paths, patch_size) not used till end\ntp, vp = train_model(svm, xTr,yTr, xVal=xVal)\nprint (\"Training confusion matrix\")\nprint (patch_confusion(yTr, tp))\nprint (\"Validation confusion matrix\")\nprint (patch_confusion(yVal, vp))\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}