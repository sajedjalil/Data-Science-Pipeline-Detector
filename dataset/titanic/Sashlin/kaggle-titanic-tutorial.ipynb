{"nbformat":4,"metadata":{"language_info":{"nbconvert_exporter":"python","pygments_lexer":"ipython3","name":"python","codemirror_mode":{"version":3,"name":"ipython"},"mimetype":"text/x-python","version":"3.6.3","file_extension":".py"},"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"_uuid":"ccef06a134f4426aae32e1e69d9e9cead36fe088","_cell_guid":"d11dd1d4-4d4c-431e-a871-04020eef778f"},"source":"# Kaggle Titanic Supervised Learning Tutorial"},{"cell_type":"markdown","metadata":{"_uuid":"3d99c2f6dbb4a9cb7147b004708f38765ca74b77","_cell_guid":"284e99a4-b5d3-4950-bc29-35217aabccaa"},"source":"## 1. Introduction to Kaggle"},{"cell_type":"markdown","metadata":{"_uuid":"df8c0476ecc3147ceeb4f508133682cb74a4e979","_cell_guid":"93f8158e-5a84-42a2-9331-f88bb58e6074"},"source":"[Kaggle](http://www.kaggle.com) is a site where people create algorithms and compete against machine learning practitioners around the world. Your algorithm wins the competition if it's the most accurate on a particular data set. Kaggle is a fun way to practice your machine learning skills.\n\nIn this mission we're going to learn how to compete in Kaggle competitions. In this introductory mission we'll learn how to:\n\n* Approach a Kaggle competition\n* Explore the competition data and learn about the competition topic\n* Prepare data for machine learning\n* Train a model\n* Measure the accuracy of your model\n* Prepare and make your first Kaggle submission.\n\nKaggle has created a number of competitions designed for beginners. The most popular of these competitions, and the one we'll be looking at, is about predicting which passengers survived the **sinking of the Titanic**.\n\nEach Kaggle competition has two key data files that you will work with - a training set and a testing set."},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"00a01ffd7bbba6caad2d1a5df319ec38c00434e6","_cell_guid":"20fb8f2a-bee9-4968-a056-ef7bcc7bc15f"},"source":"import pandas as pd\n\ntest = pd.read_csv(\"../input/test.csv\")\ntest_shape = test.shape\nprint(test_shape)"},{"cell_type":"markdown","metadata":{"_uuid":"8c186607608719d38cf8c99fa1c44e24a49b817e","_cell_guid":"66778718-5a69-4cba-a096-fac563d409ae"},"source":"### Instructions"},{"cell_type":"markdown","metadata":{"_uuid":"b4d3f451458d237aa56d200199f9b2ed4dfe02ef","_cell_guid":"75996738-d55b-417e-9361-23ab76c8df42"},"source":"Use ```pandas.read_csv()``` to import ```train.csv``` and assign it to the variable ```train```.\nUse ```DataFrame.shape``` to calculate the number of rows and columns in ```train```, and assign the result to ```train_shape```."},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"6fc2acd25cb5072c842da89bcbaf1d24f895ee56","_cell_guid":"a10e8b94-7125-4373-a642-7238849b5d29"},"source":"# INSERT CODE HERE\ntrain = pd.read_csv(\"../input/train.csv\")\ntrain_shape = train.shape\nprint(train_shape)"},{"cell_type":"markdown","metadata":{"_uuid":"d8ded7bcb480f74574cb0174f410e25e69e09046","_cell_guid":"8ecc10e6-f77d-40ad-9a0a-5c8e7881e822"},"source":"## 2. Exploring the data"},{"cell_type":"markdown","metadata":{"_uuid":"1d8b04508654865a8a778d7d9c8bb460307bff6f","_cell_guid":"99e03ddf-2043-46bb-8570-5c4b1de61fb4"},"source":"The files we read in the previous screen are available on the data page for the Titanic competition on Kaggle. That page also has a data dictionary, which explains the various columns that make up the data set. Below are the descriptions contained in that data dictionary:\n\n* *PassengerID* - A column added by Kaggle to identify each row and make submissions easier\n* *Survived* - Whether the passenger survived or not and the value we are predicting (0=No, 1=Yes)\n* *Pclass* - The class of the ticket the passenger purchased (1=1st, 2=2nd, 3=3rd)\n* *Sex* - The passenger's sex\n* *Age* - The passenger's age in years\n* *SibSp* - The number of siblings or spouses the passenger had aboard the Titanic\n* *Parch* - The number of parents or children the passenger had aboard the Titanic\n* *Ticket* - The passenger's ticket number\n* *Fare* - The fare the passenger paid\n* *Cabin* - The passenger's cabin number\n* *Embarked* - The port where the passenger embarked (C=Cherbourg, Q=Queenstown, S=Southampton)"},{"cell_type":"markdown","metadata":{"_uuid":"a0790f959dd864a861288b8c7ca2516a0e03ecfa","_cell_guid":"48ab7d8a-e8e5-4e02-a6e7-484e30c1b1f6"},"source":"### Let's get a view of the actual data"},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"52198316fd2bb160b62cf034212e7a032c994aab","_cell_guid":"5fb11aa8-2639-42b5-bd7d-5d14e9272d8e"},"source":"train.head(10)"},{"cell_type":"markdown","metadata":{"_uuid":"8842075e76806e0d41c2580cc7e32b6684e43f0a","_cell_guid":"b7f83e22-555d-4d4f-beac-7dbc38b2ee16"},"source":"The type of machine learning we will be doing is called classification, because when we make predictions we are classifying each passenger as survived or not. More specifically, we are performing binary classification, which means that there are only two different states we are classifying.\n\nIn any machine learning exercise, thinking about the topic you are predicting is very important. We call this step acquiring domain knowledge, and it's one of the most important determinants for success in machine learning.\n\nIn this case, understanding the Titanic disaster and specifically what variables might affect the outcome of survival is important. Anyone who has watched the movie Titanic would remember that women and children were given preference to lifeboats (as they were in real life). You would also remember the vast class disparity of the passengers.\n\nThis indicates that Age, Sex, and PClass may be good predictors of survival. We'll start by exploring Sex and Pclass by visualizing the data.\n\nBecause the Survived column contains 0 if the passenger did not survive and 1 if they did, we can segment our data by sex and calculate the mean of this column. We can use DataFrame.pivot_table() to easily do this:"},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"33177b28867de369917fc6a183cac2be576b0580","_cell_guid":"b30c2e05-cc81-41d6-b77e-2a4ea7eb5d24"},"source":"import matplotlib.pyplot as plt\n\nsex_pivot = train.pivot_table(index=\"Sex\",values=\"Survived\")\nsex_pivot\n\n# sex_pivot.plot.bar()\n# plt.show()"},{"cell_type":"markdown","metadata":{"_uuid":"22e752af3e8d31c55af33600ec47da0e3113eaed","_cell_guid":"e430584a-c0f9-48a7-b1b6-2cf908b877a4"},"source":"We can immediately see that females survived in much higher proportions than males did.\n\nLet's do the same with the Pclass column."},{"cell_type":"markdown","metadata":{"_uuid":"a89f8dca61310761150c56a341e5dacdef8c2590","_cell_guid":"8092753f-9131-47a6-b753-1b702b57b289"},"source":"### Instructions"},{"cell_type":"markdown","metadata":{"_uuid":"1d352c346ebdcf85420a8bc399019a1e36a3f646","_cell_guid":"cc62f888-ddd1-4ef0-96f2-7467768c83f3"},"source":"* Use ```DataFrame.pivot_table()``` to pivot the ```train``` dataframe:\n    * Use \"Pclass\" for the index parameter.\n    * Use \"Survived\" for the values parameter.\n* Use ```DataFrame.plot.bar()``` to plot the pivot table."},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"cd3fc00d5b7d16629dd60202783d9bf7bb811a9d","_cell_guid":"a764ffaf-ce57-400a-bffe-37b35dbd1811"},"source":"pclass_pivot = train.pivot_table(index=\"Pclass\",values=\"Survived\")\n# pclass_pivot\npclass_pivot.plot.bar()\nplt.show()"},{"cell_type":"markdown","metadata":{"_uuid":"f0161b6e05e13e10eda2b11a014b8211e4982a2b","_cell_guid":"d518c8fb-7834-4695-9965-ff7b5b52df85"},"source":"## 3. Exploring and Converting the age column"},{"cell_type":"markdown","metadata":{"_uuid":"83ab7c68a988faa94db88831400b8e9b77f4fb6f","_cell_guid":"effa13af-608d-473a-8ff5-cf2f84d5d4b7"},"source":"The Sex and PClass columns are what we call categorical features. That means that the values represented a few separate options (for instance, whether the passenger was male or female).\n\nLet's take a look at the Age column using [```Series.describe()```](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.describe.html). "},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"fd39a213c2847f57bee05b7d55e1f23aea3a9686","_cell_guid":"b23178a4-cac6-41e6-b9d8-41082f001568"},"source":"train['Age'].describe()"},{"cell_type":"markdown","metadata":{"_uuid":"e13276a82d84ecb6920239e79e2a05e503d777e9","_cell_guid":"e1f13642-70a1-4e69-b4b1-f865cf72a393"},"source":"The Age column contains numbers ranging from 0.42 to 80.0 (If you look at Kaggle's data page, it informs us that Age is fractional if the passenger is less than one). The other thing to note here is that there are 714 values in this column, fewer than the 814 rows we discovered that the train data set had earlier in this mission which indicates we have some missing values.\n\nAll of this means that the Age column needs to be treated slightly differently, as this is a continuous numerical column. One way to look at distribution of values in a continuous numerical set is to use histograms. We can create two histograms to compare visually the those that survived vs those who died across different age ranges:"},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"0d486d20bf73c9d5aac33c9326ce4cf1c3896b8d","_cell_guid":"89c3da43-2d36-4210-973a-be337054e3cd"},"source":"train[train[\"Survived\"] == 1]"},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"3e43be953fbedf46a783228d461f396327e7bdde","_cell_guid":"9c4f5e45-60b0-4e65-861a-d6dd61e60dc0"},"source":"survived = train[train[\"Survived\"] == 1]\ndied = train[train[\"Survived\"] == 0]\nsurvived[\"Age\"].plot.hist(alpha=0.5,color='red',bins=50)\ndied[\"Age\"].plot.hist(alpha=0.5,color='blue',bins=50)\nplt.legend(['Survived','Died'])\nplt.show()"},{"cell_type":"markdown","metadata":{"_uuid":"3d3eaa464c3bd85bbda4e5b044a8f18135d832b3","_cell_guid":"6d56b478-b85a-4650-9447-d9f7f6918156"},"source":"The relationship here is not simple, but we can see that in some age ranges more passengers survived - where the red bars are higher than the blue bars.\n\nIn order for this to be useful to our machine learning model, we can separate this continuous feature into a categorical feature by dividing it into ranges. We can use the ```pandas.cut()``` function to help us out.\n\nThe ```pandas.cut()``` function has two required parameters - the column we wish to cut, and a list of numbers which define the boundaries of our cuts. We are also going to use the optional parameter labels, which takes a list of labels for the resultant bins. This will make it easier for us to understand our results.\n\nBefore we modify this column, we have to be aware of two things. Firstly, any change we make to the train data, we also need to make to the test data, otherwise we will be unable to use our model to make predictions for our submissions. Secondly, we need to remember to handle the missing values we observed above."},{"cell_type":"markdown","metadata":{"_uuid":"f3bb2a1814f8d52bcaee28b15be1f1f81376821f","_cell_guid":"d511e1d3-7d29-4d9a-b62c-7bd11824de8f"},"source":"We can then use that function on both the ```train``` and ```test``` dataframes.\n\n```python\ndef process_age(df,cut_points,label_names):\n    df[\"Age\"] = df[\"Age\"].fillna(-0.5)\n    df[\"Age_categories\"] = pd.cut(df[\"Age\"],cut_points,labels=label_names)\n    return df\n\ncut_points = [-1,0,18,100]\nlabel_names = [\"Missing\",\"Child\",\"Adult\"]\n\ntrain = process_age(train,cut_points,label_names)\ntest = process_age(test,cut_points,label_names)\n```\n\nThe diagram below shows how the function converts the data:"},{"cell_type":"markdown","metadata":{"_uuid":"bc820399897f31efbede3e84575647de985b0861","_cell_guid":"65cc7138-2ae2-4b2d-821e-3e6d692f3e27"},"source":"![](https://s3.amazonaws.com/dq-content/185/cut.svg)\n\nNote that the cut_points list has one more element than the label_names list, since it needs to define the upper boundary for the last segment."},{"cell_type":"markdown","metadata":{"_uuid":"67607830b87ed28ddb97ba967bd4774ebba675bc","_cell_guid":"4ac20db3-1921-4fcb-bdb5-e82c1878b55a"},"source":"### Instructions"},{"cell_type":"markdown","metadata":{"_uuid":"54eaf5474e1ba358d799f7eb4304ec587be3f54c","_cell_guid":"53b2e5e5-239a-4d7a-856d-754d6ea04d77"},"source":"* Create the ```cut_points``` and ```label_names``` lists to split the ```Age``` column into six categories:\n    * Missing, from -1 to 0\n    * Infant, from 0 to 5\n    * Child, from 5 to 12\n    * Teenager, from 12 to 18\n    * Young Adult, from 18 to 35\n    * Adult, from 35 to 60\n    * Senior, from 60 to 100\n* Apply the ```process_age()``` function on the train dataframe, assigning the result to train.\n* Apply the ```process_age()``` function on the test dataframe, assigning the result to test.\n* Use ```DataFrame.pivot_table()``` to pivot the train dataframe by the Age_categories column.\n* Use ```DataFrame.plot.bar()``` to plot the pivot table."},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"4714bfb7d4b14c0ecf3c066eb063ac38f8a8ced2","_cell_guid":"821160e2-d137-4762-ad1d-d8eaca6d4226"},"source":"def process_age(df,cut_points,label_names):\n    df[\"Age\"] = df[\"Age\"].fillna(-0.5)\n    df[\"Age_categories\"] = pd.cut(df[\"Age\"],cut_points,labels=label_names)\n    return df\n\ncut_points = [-1,0, 5, 12, 18, 35, 60, 100]\nlabel_names = [\"Missing\", 'Infant', \"Child\", 'Teenager', \"Young Adult\", 'Adult', 'Senior']\n\ntrain = process_age(train,cut_points,label_names)\ntest = process_age(test,cut_points,label_names)\n\nage_cat_pivot = train.pivot_table(index=\"Age_categories\",values=\"Survived\")\nage_cat_pivot.plot.bar()\nplt.show()"},{"cell_type":"markdown","metadata":{"_uuid":"aa19941e7c41ef975aa7f5e5aee4b6f12899d194","_cell_guid":"c6465ea0-7181-4997-90fa-47faf06f9b67"},"source":"## 4. Preparing our Data for Machine Learning"},{"cell_type":"markdown","metadata":{"_uuid":"7a3700ec89b216f737c6f59e6c15bf38067b3920","_cell_guid":"5b8caca5-ec4c-4141-886e-3ca1c3d1f1ef"},"source":"So far we have identified three columns that may be useful for predicting survival:\n\n* Sex\n* Pclass\n* Age, or more specifically our newly created Age_categories\n\nBefore we build our model, we need to prepare these columns for machine learning. Most machine learning algorithms can't understand text labels, so we have to convert our values into numbers.\n\nAdditionally, we need to be careful that we don't imply any numeric relationship where there isn't one. If we think of the values in the Pclass column, we know they are 1, 2, and 3."},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"8927831d595abcdae6888f5f928269b9b7e8d5b2","_cell_guid":"0e4e10f2-5d73-40b2-a56d-2e1ce2bd9c83"},"source":"train['Pclass'].value_counts()"},{"cell_type":"markdown","metadata":{"_uuid":"58f07635d8b9aabf6d3f2142897f02bc1fa53a92","_cell_guid":"63be9240-5fad-499e-8954-21a02957c004"},"source":"While the class of each passenger certainly has some sort of ordered relationship, the relationship between each class is not the same as the relationship between the numbers 1, 2, and 3. For instance, class 2 isn't \"worth\" double what class 1 is, and class 3 isn't \"worth\" triple what class 1 is.\n\nIn order to remove this relationship, we can create dummy columns for each unique value in Pclass:"},{"cell_type":"markdown","metadata":{"_uuid":"85590957e0088a433152eea223c504453c1eccfb","_cell_guid":"93161299-8b20-4a28-9b7f-69bba43e4372"},"source":"![](https://s3.amazonaws.com/dq-content/185/kaggle_get_dummies.svg)"},{"cell_type":"markdown","metadata":{"_uuid":"472f80281232c20ad1ff6ee55bb49e15f3bfea34","_cell_guid":"942fa204-1d6b-4b57-abe9-48452535f391"},"source":"Let's use that function to create dummy columns for both the Sex and Age_categories columns."},{"cell_type":"markdown","metadata":{"_uuid":"c2df68e1ba2bec5286720ca7b68b61c5f216dbc5","_cell_guid":"46470179-3fa6-4d12-8040-854536e4daa3"},"source":"### Instructions\n\n* Use the create_dummies() function to create dummy variables for the Sex column:\n    * in the train dataframe.\n    * in the test dataframe.\n* Use the create_dummies() function to create dummy variables for the Age_categories column:\n    * in the train dataframe.\n    * in the test dataframe."},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"93cef9c39b1cd922d1977c7ee353690fd113b279","_cell_guid":"58ad174a-0ff6-4000-9d8d-5ab49d8eac4d"},"source":"column_name = \"Pclass\"\ndf = train\ndummies = pd.get_dummies(df[column_name],prefix=column_name)\ndummies.head()"},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"1e5cc217df58d21b02fdbda9fad4c377e339173c","_cell_guid":"b1bf5db1-aa0c-4a1d-9e1b-9a2ca1b51de5"},"source":"def create_dummies(df,column_name):\n    dummies = pd.get_dummies(df[column_name],prefix=column_name)\n    df = pd.concat([df,dummies],axis=1)\n    return df\n\ntrain = create_dummies(train,\"Pclass\")\ntest = create_dummies(test,\"Pclass\")\ntrain.head()"},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"f64bee8b698ed4c954fee6e6acfaac5525c0ad4e","collapsed":true,"_cell_guid":"51a9e34c-095a-489c-a3b9-d799a8699fc2"},"source":"train = create_dummies(train,\"Sex\")\ntest = create_dummies(test,\"Sex\")\ntrain = create_dummies(train,\"Age_categories\")\ntest = create_dummies(test,\"Age_categories\")"},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"8f25bf63c69b487290851a2de45c4b8c62a04bb2","_cell_guid":"1788dd94-8202-4d52-ac62-d57f2e8754d7"},"source":"train.head()"},{"cell_type":"markdown","metadata":{"_uuid":"31190e72d5132f931ce7a178a97db40510471991","_cell_guid":"7fe7684f-a737-4619-82c6-3d246cc58683"},"source":"## 5. Creating our first machine learning model"},{"cell_type":"markdown","metadata":{"_uuid":"fd120c850b1252e409b8c6e0b8e88731f37a2a8a","_cell_guid":"4133d104-2477-4057-a871-39e59bcf6af3"},"source":"Now that our data has been prepared, we are ready to train our first model. The first model we will use is called Logistic Regression, which is often the first model you will train when performing classification.\n\nWe will be using the scikit-learn library as it has many tools that make performing machine learning easier. The scikit-learn workflow consists of four main steps:\n\n* Instantiate (or create) the specific machine learning model you want to use\n* Fit the model to the training data\n* Use the model to make predictions\n* Evaluate the accuracy of the predictions\n* Each model in scikit-learn is implemented as a separate class and the first step is to identify the class we want to create an instance of. In our case, we want to use the LogisticRegression class.\n\nWe'll start by looking at the first two steps. First, we need to import the class:\n\n```python\nfrom sklearn.linear_model import LogisticRegression\n```\n\nNext, we create a LogisticRegression object:\n\n```python\nlr = LogisticRegression()\n```\n\nLastly, we use the ```LogisticRegression.fit()``` method to train our model. The ```.fit()``` method accepts two arguments: X and y. X must be a two dimensional array (like a dataframe) of the features that we wish to train our model on, and y must be a one-dimensional array (like a series) of our target, or the column we wish to predict.\n\n```python\ncolumns = ['Pclass_2', 'Pclass_3', 'Sex_male']\nlr.fit(train[columns], train['Survived'])\n```\n\nThe code above fits (or trains) our LogisticRegression model using three columns: Pclass_2, Pclass_3, and Sex_male.\n\nLet's train our model using all of the columns we created above."},{"cell_type":"markdown","metadata":{"_uuid":"431dc82bd85e0c044448659e4086c69e672725ad","_cell_guid":"3cc7dda2-18bb-43d9-90d0-86f9368286f8"},"source":"### Instructions\n\n* Instantiate a LogisticRegression object called lr.\n* Use LogisticRegression.fit() to fit the model on the train dataset using:\n    * The columns contained in columns as the first (X) parameter.\n    * The Survived column as the second (y) parameter."},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"2a5f88cdf2ca1d3c8131761e585182975a50d29a","collapsed":true,"_cell_guid":"7f962c4f-205f-4c16-b7ae-63910bf17517"},"source":"columns = ['Pclass_1', 'Pclass_2', 'Pclass_3', 'Sex_female', 'Sex_male',\n       'Age_categories_Missing','Age_categories_Infant',\n       'Age_categories_Child', 'Age_categories_Teenager',\n       'Age_categories_Young Adult', 'Age_categories_Adult',\n       'Age_categories_Senior']\n\nfrom sklearn.linear_model import LogisticRegression"},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"7c211334eaf2081be0f856188f66f7ec2d7837c8","_cell_guid":"58c75f68-6f6f-4b5d-969e-a9904265280f"},"source":"lr.decision_function(train[columns])"},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"de05ce968d010c6a6a7f96166d7aad97e3997822","_cell_guid":"de8b5ce1-3ead-4bad-80a9-c38af62d706c"},"source":"lr.coef_"},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"2c1002c689e8a5211224d8b1d8ad00a858611c12","_cell_guid":"9afff557-fc69-4dff-be13-1f819110b6b0"},"source":"lr = LogisticRegression()\nlr.fit(train[columns], train['Survived'])"},{"cell_type":"markdown","metadata":{"_uuid":"fec795a1a2f6d5fd73ae44614b80abe1f8ed74ad","_cell_guid":"aa7c070b-7277-4a6e-ac70-87c1c500e1db"},"source":"## 6. Splitting our Training Data"},{"cell_type":"markdown","metadata":{"_uuid":"3cb196009407ec4c1f031027661b611fd6aba287","_cell_guid":"efabbdd9-81b3-4882-ae03-131786f12b4d"},"source":"Congratulations, you've trained your first machine learning model! Our next step is to find out how accurate our model is, and to do that, we'll have to make some predictions.\n\nIf you recall from earlier, we do have a test dataframe that we could use to make predictions. We could make predictions on that data set, but because it doesn't have the Survived column we would have to submit it to Kaggle to find out our accuracy. This would quickly become a pain if we had to submit to find out the accuracy every time we optimized our model.\n\nWe could also fit and predict on our train dataframe, however if we do this there is a high likelihood that our model will overfit, which means it will perform well because we're testing on the same data we've trained on, but then perform much worse on new, unseen data.\n\nInstead we can split our train dataframe into two:\n\n* One part to train our model on (often 80% of the observations)\n* One part to make predictions with and test our model (often 20% of the observations)\n\nThe convention in machine learning is to call these two parts train and test. This can become confusing, since we already have our test dataframe that we will eventually use to make predictions to submit to Kaggle. To avoid confusion, from here on, we're going to call this Kaggle 'test' data holdout data, which is the technical name given to this type of data used for final predictions.\n\nThe scikit-learn library has a handy [```model_selection.train_test_split()```](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function that we can use to split our data. train_test_split() accepts two parameters, ```X``` and ```y```, which contain all the data we want to train and test on, and returns four objects: ```train_X, train_y, test_X, test_y```:"},{"cell_type":"markdown","metadata":{"_uuid":"a2ed3abc31b1622f820013f4bacffddb494722e5","_cell_guid":"7fe3e291-fa4a-4698-9b1c-a9530fc65721"},"source":"![](https://s3.amazonaws.com/dq-content/185/kaggle_train_test_split.svg)"},{"cell_type":"markdown","metadata":{"_uuid":"102aa92be2848c2a2ce4d6ca4d37b1c7aa4fcbe1","_cell_guid":"dd08c1c7-c7c1-4d6e-99a7-390234f2d547"},"source":"Here's what the syntax for creating these four objects looks like:\n\n```python\nfrom sklearn.model_selection import train_test_split\n\ncolumns = ['Pclass_2', 'Pclass_3', 'Sex_male']\n\nall_X = train[columns]\nall_y = train['Survived']\n\ntrain_X, test_X, train_y, test_y = train_test_split(\n    all_X, all_y, test_size=0.2,random_state=0)\n```\n\nYou'll notice that there are two other parameters we used: ```test_size```, which lets us control what proportions our data are split into, and ```random_state```. The ```train_test_split()``` function randomizes observations before dividing them, and setting a random seed means that our results will be reproducible, which is important if you are collaborating, or need to produce consistent results each time."},{"cell_type":"markdown","metadata":{"_uuid":"be32178cc446763782d184037e12c5c31901b049","_cell_guid":"8f6b6d7b-2106-4537-9533-94552560ae5c"},"source":"### Instructions\n\n* Use the model_selection.train_test_split() function to split the train dataframe using the following parameters:\n    * test_size of 0.2.\n    * random_state of 0.\n* Assign the four returned objects to train_X, test_X, train_y, and test_y."},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"6184ee18a60bafe1369e62e2ed5faf64ec0e0d9f","collapsed":true,"_cell_guid":"1d88d118-f875-42b1-a898-b25e2d9a2f1f"},"source":"holdout = test # from now on we will refer to this\n               # dataframe as the holdout data\n\nfrom sklearn.model_selection import train_test_split\n\ncolumns = ['Pclass_1', 'Pclass_2', 'Pclass_3', 'Sex_female', 'Sex_male',\n       'Age_categories_Missing','Age_categories_Infant',\n       'Age_categories_Child', 'Age_categories_Teenager',\n       'Age_categories_Young Adult', 'Age_categories_Adult',\n       'Age_categories_Senior']\n\nall_X = train[columns]\nall_y = train['Survived']\n\ntrain_X, test_X, train_y, test_y = train_test_split(\n    all_X, all_y, test_size=0.2,random_state=0)"},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"d80e9d974f73d99dd29378bd1e56889498ae4fc3","_cell_guid":"4a4270c0-8a35-4705-aad5-6ea7b36e8bcd"},"source":"train_X.shape"},{"cell_type":"markdown","metadata":{"_uuid":"cd5c1a42164574d618bf3eb8ce272e2af553ebd3","_cell_guid":"2881b09d-629b-48f1-a32b-20a150b85639"},"source":"## 7. Making Predictions and Measuring their Accuracy"},{"cell_type":"markdown","metadata":{"_uuid":"88f255bb00ef07be0caf49b9ae9636421fc679bd","_cell_guid":"a4868d6a-51eb-4657-9d77-77fa9592e3c3"},"source":"Now that we have our data split into train and test sets, we can fit our model again on our training set, and then use that model to make predictions on our test set.\n\nOnce we have fit our model, we can use the ```LogisticRegression.predict()``` method to make predictions.\n\nThe ```predict()``` method takes a single parameter ```X```, a two dimensional array of features for the observations we wish to predict. ```X``` must have the exact same features as the array we used to fit our model. The method returns single dimensional array of predictions.\n\n```python\nlr = LogisticRegression()\nlr.fit(train_X, train_y)\npredictions = lr.predict(test_X)\n```"},{"cell_type":"markdown","metadata":{"_uuid":"150c731afe7ca4c4da8c2d14d81f9653bd1f96a7","_cell_guid":"0b177159-c0c3-403b-af4e-39f9f41ca2f3"},"source":"There are a number of ways to measure the accuracy of machine learning models, but when competing in Kaggle competitions you want to make sure you use the same method that Kaggle uses to calculate accuracy for that specific competition.\n\nIn this case, [the evaluation section for the Titanic competition on Kaggle](https://www.kaggle.com/c/titanic#evaluation) tells us that our score calculated as \"the percentage of passengers correctly predicted\". This is by far the most common form of accuracy for binary classification.\n\nAs an example, imagine we were predicting a small data set of five observations."},{"cell_type":"markdown","metadata":{"_uuid":"650162608c2c17945085e46d6ee4273446f80359","_cell_guid":"9a8e3b8f-0fb8-4888-ad4d-b9c003ca2dea"},"source":"|Our model's prediction|The actual value|Correct|\n|----------------------|----------------|-------|\n|0                     |0               |Yes    |\n|1                     |0               |No     |\n|0                     |1               |No     |\n|1                     |1               |Yes    |\n|1                     |1               |Yes    |"},{"cell_type":"markdown","metadata":{"_uuid":"db00cc2f3bf44265dc7797a21a08ada72401af9d","_cell_guid":"c360c258-c372-40ee-856b-7aacc23dc0a7"},"source":"In this case, our model correctly predicted three out of five values, so the accuracy based on this prediction set would be 60%.\n\nAgain, scikit-learn has a handy function we can use to calculate accuracy: [```metrics.accuracy_score()```](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html). The function accepts two parameters, ```y_true``` and ```y_pred```, which are the actual values and our predicted values respectively, and returns our accuracy score.\n\n```python \nfrom sklearn.metrics import accuracy_score\naccuracy = accuracy_score(test_y, predictions)\n```\n\nLet's put all of these steps together, and get our first accuracy score."},{"cell_type":"markdown","metadata":{"_uuid":"519d6d101a6ee173b44848b61a5ec42b05d15cc5","_cell_guid":"4cbfb29b-4e2a-407b-87c4-684b73e696d0"},"source":"### Instructions\n\n* Instantiate a new ```LogisticRegression()``` object, ```lr```.\n* Fit the model using ```train_X``` and ```train_y```.\n* Make predictions using ```test_X``` and assign the results to ```predictions```.\n* Use ```accuracy_score()``` to compare ```test_y``` and ```predictions```, assigning the result to ```accuracy```\n* Print the ```accuracy``` variable."},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"e7a6b650cbf06bd3e3ea2eaaf146ed30f6101f96","collapsed":true,"_cell_guid":"4f7b65e7-b92a-492e-81a0-269890602a4c"},"source":"from sklearn.metrics import accuracy_score"},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"d18a458a7a3783ab28f06fb01709a4b30dd070f7","_cell_guid":"ca7fdf3f-84c4-4062-bb77-d57e1240a54b","scrolled":true},"source":"lr = LogisticRegression()\nlr.fit(train_X, train_y)\npredictions = lr.predict(test_X)\naccuracy = accuracy_score(test_y, predictions)\naccuracy"},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"815bba401eb31764d72ecd545fcc47320f9ceaa7","_cell_guid":"881ceab0-1862-433a-8f12-fa09d1c09469"},"source":"from sklearn.metrics import confusion_matrix\n\nconf_matrix = confusion_matrix(test_y, predictions)\npd.DataFrame(conf_matrix, columns=['Survived', 'Died'], index=[['Survived', 'Died']])"},{"cell_type":"markdown","metadata":{"_uuid":"6fc9538ed2a3dfe4ffa2457136e306cd2553e082","_cell_guid":"b391966f-c8da-4377-a82a-e2f29527bde6"},"source":"## Using Cross Validation for More Accurate Error Measurement"},{"cell_type":"markdown","metadata":{"_uuid":"247a6d78b506fc7df36ccd13b831462b64567ace","_cell_guid":"72eef752-4f63-4e34-8519-e48bd075ad1f"},"source":"Our model has an accuracy score of 81.0% when tested against our 20% test set. Given that this data set is quite small, there is a good chance that our model is overfitting, and will not perform as well on totally unseen data.\n\nTo give us a better understanding of the real performance of our model, we can use a technique called **cross validation** to train and test our model on different splits of our data, and then average the accuracy scores."},{"cell_type":"markdown","metadata":{"_uuid":"9a7cffe679420e7e2234c515c8a6cc1f75771dba","_cell_guid":"ca68d862-4920-496a-a5e7-eebd909038f9"},"source":"![](https://s3.amazonaws.com/dq-content/185/kaggle_cross_validation.svg)"},{"cell_type":"markdown","metadata":{"_uuid":"1b3b9b2f8ad9ac6e5d8aa34a091598b43c644f6b","_cell_guid":"cb09a6a4-0f07-41fb-bc13-eb36aaa49f74"},"source":"The most common form of cross validation, and the one we will be using, is called **k-fold** cross validation. 'Fold' refers to each different iteration that we train our model on, and 'k' just refers to the number of folds. In the diagram above, we have illustrated k-fold validation where k is 5.\n\nWe will use scikit-learn's [```model_selection.cross_val_score()``` function](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score) to automate the process. The basic syntax for ```cross_val_score()``` is:"},{"cell_type":"markdown","metadata":{"_uuid":"35831e60ad6738fea5079365126b937f7071a5d6","_cell_guid":"3b6738fc-78c5-49ab-9267-c88682f0fdac"},"source":"```python\ncross_val_score(estimator, X, y, cv=None)\n```"},{"cell_type":"markdown","metadata":{"_uuid":"24856991795248188d7a0551f53129be3b2e09b8","_cell_guid":"29c545eb-a98e-481e-8358-61ea87ec3c0d"},"source":"* estimator is a scikit-learn estimator object, like the LogisticRegression() objects we have been creating.\n* X is all features from our data set.\n* y is the target variables.\n* cv specifies the number of folds.\n\nThe function returns a numpy ndarray of the accuracy scores of each fold.\n\nIt's worth noting, the ```cross_val_score()``` function can use a variety of cross validation techniques and scoring types, but it defaults to k-fold validation and accuracy scores for our input types."},{"cell_type":"markdown","metadata":{"_uuid":"daa12cab3559aa711e2357def11876338a2ad35e","_cell_guid":"a4e65570-94c9-411e-adf3-84fcaa81cf67"},"source":"### Instructions\n\n* Instantiate a new LogisticRegression() object, lr.\n* Use model_selection.cross_val_score() to perform cross-validation on our data and assign the results to scores:\n    * Use the newly created lr as the estimator.\n    * Use all_X and all_y as the input data.\n    * Specify 10 folds to be used.\n* Use the numpy.mean() function to calculate the mean of scores and assign the result to accuracy.\n* Print the variables scores and accuracy."},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"0f5facf07093ae82057dad974a31b7d742ccb8c4","_cell_guid":"9b90797a-29bf-478b-9c47-4ff715dfb8aa"},"source":"from sklearn.model_selection import cross_val_score\nimport numpy as np\n\nlr = LogisticRegression()\nscores = cross_val_score(lr, all_X, all_y, cv=10)\nnp.mean(scores)"},{"cell_type":"markdown","metadata":{"_uuid":"4aa1c3e51c040ec5a948204da5364197715a0de5","_cell_guid":"604e9787-4501-4d8e-b640-0f4ab7688c2a"},"source":"## 9. Making Predictions on Unseen Data\n\nFrom the results of our k-fold validation, you can see that the accuracy number varies with each fold - ranging between 76.4% and 87.6%. This demonstrates why cross validation is important.\n\nAs it happens, our average accuracy score was 80.2%, which is not far from the 81.0% we got from our simple train/test split, however this will not always be the case, and you should always use cross-validation to make sure the error metrics you are getting from your model are accurate.\n\nWe are now ready to use the model we have built to train our final model and then make predictions on our unseen holdout data, or what Kaggle calls the 'test' data set."},{"cell_type":"markdown","metadata":{"_uuid":"4a211474746e0e7483f97c0f2c60e94c7223fe3f","_cell_guid":"d40e593e-8363-4b6f-9c5c-244b4712c3f5"},"source":"### Instructions\n\n* Instantiate a new ```LogisticRegression()``` object, ```lr```.\n* Use the ```fit()``` method to train the model ```lr``` using all of the Kaggle training data: ```all_X``` and ```all_y```.\n* Make predictions using the ```holdout``` data and assign the result to ```holdout_predictions```."},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"2e648299c1347d425ea2ef9ec1aa293f2f1d24a7","collapsed":true,"_cell_guid":"1c4638e3-31eb-42cb-97d4-ccbc077b1921"},"source":"columns = ['Pclass_1', 'Pclass_2', 'Pclass_3', 'Sex_female', 'Sex_male',\n       'Age_categories_Missing','Age_categories_Infant',\n       'Age_categories_Child', 'Age_categories_Teenager',\n       'Age_categories_Young Adult', 'Age_categories_Adult',\n       'Age_categories_Senior']"},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"8483be1f8e813eba167339ae09cfe6dde342eae1","_cell_guid":"d3c3048a-d2b4-4e27-aee1-f654339b7fa5"},"source":"holdout.head()"},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"9711c24e72d7263f50d8f8968a8856f668a71364","_cell_guid":"4aa6c826-26e3-4b5e-8566-4e05e70ea395"},"source":"lr = LogisticRegression()\nlr.fit(all_X, all_y)\nholdout_predictions = lr.predict(holdout[columns])\nholdout_predictions"},{"cell_type":"markdown","metadata":{"_uuid":"1cfca7733018457ca35223b0b423e3e45f7d1af4","_cell_guid":"96438d53-b006-402c-a556-d216e09b01ba"},"source":"## 10. Creating a Submission File"},{"cell_type":"markdown","metadata":{"_uuid":"f46f6670f1122780fc012b4a64e29c4b2e32f11a","_cell_guid":"1dc7f737-02d6-4d19-9fa2-b322e43ae8cd"},"source":"The last thing we need to do is create a submission file. Each Kaggle competition can have slightly different requirements for the submission file. Here's what is specified on the [Titanic competition evaluation page](https://www.kaggle.com/c/titanic#evaluation):\n\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.\n\nThe file should have exactly 2 columns:\n\nPassengerId (sorted in any order)\nSurvived (contains your binary predictions: 1 for survived, 0 for deceased)\nThe table below shows this in a slightly easier to understand format, so we can visualize what we are aiming for."},{"cell_type":"markdown","metadata":{"_uuid":"e60039e5533e1e364a0c477e1a9c777e5892614b","_cell_guid":"df5f06af-f996-4594-b290-778505104498"},"source":"|PassengerId|Survived|\n|-----------|--------|\n|892        |0       |\n|893        |1       |\n|894        |0       |"},{"cell_type":"markdown","metadata":{"_uuid":"7b91fbe68af2effbd4922c7845c7cf7286de3712","_cell_guid":"0d6a7b37-862e-41db-bcbb-c30f6eb87971"},"source":"We will need to create a new dataframe that contains the holdout_predictions we created in the previous screen and the PassengerId column from the holdout dataframe. We don't need to worry about matching the data up, as both of these remain in their original order.\n\nTo do this, we can pass a dictionary to the [```pandas.DataFrame()``` function](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html):"},{"cell_type":"markdown","metadata":{"_uuid":"4a114e6705719010c190bd4830e827542e056b27","_cell_guid":"9c97b44c-28f7-4ab3-9a10-84138ac3dfdb"},"source":"```python\nholdout_ids = holdout[\"PassengerId\"]\nsubmission_df = {\"PassengerId\": holdout_ids,\n                 \"Survived\": holdout_predictions}\nsubmission = pd.DataFrame(submission_df)\n```"},{"cell_type":"markdown","metadata":{"_uuid":"5a6c8bdce2b2f2af5814046cd1890b263364bf39","_cell_guid":"0e265374-0a8b-41f0-b7e2-3a124190d915"},"source":"Finally, we'll use the [```DataFrame.to_csv()``` method](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_csv.html) to save the dataframe to a CSV file. We need to make sure the index parameter is set to False, otherwise we will add an extra column to our CSV."},{"cell_type":"markdown","metadata":{"_uuid":"c7d2bbef96ceab369372406fdd57a07dae85ba56","_cell_guid":"d612b812-1a32-4178-93c0-1ea70a1b28ba"},"source":"### Instructions\n\n* Create a dataframe submission that matches Kaggle's specification.\n* Use the ```to_csv()``` method to save the ```submission``` dataframe using the filename ```submission.csv```, using the documentation to look up the correct syntax."},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"47a1349026f9e239a1e76c517bca4a28f78f7c81","collapsed":true,"_cell_guid":"0c6cce5b-ad8a-4a62-8038-30e6d3fd6460"},"source":"holdout_ids = holdout[\"PassengerId\"]\nsubmission_df = {\"PassengerId\": holdout_ids,\n                 \"Survived\": holdout_predictions}\nsubmission = pd.DataFrame(submission_df)\n\nsubmission.to_csv('titanic_submission.csv', index=False)"},{"cell_type":"markdown","metadata":{"_uuid":"dd7fca4c2569857d6fa6098defdad2ced9100874","_cell_guid":"1366f8ea-969e-4145-99f8-59ecee3e2333"},"source":"## 11. Making our First Submission to Kaggle\n\nNow that we have our submission file, we can start our submission to Kaggle by clicking the blue 'Submit Predictions' button on the competition page."},{"cell_type":"markdown","metadata":{"_uuid":"bdfb416c8ff3fb17aa70b69e52a659ebb457d67d","_cell_guid":"8da78bc0-90a0-4e20-9520-76c82b85a17b"},"source":"![](https://s3.amazonaws.com/dq-content/185/kaggle_submit.jpg)"},{"cell_type":"markdown","metadata":{"_uuid":"fd6f225377362a1803636447a308b33214f4aed1","_cell_guid":"66103d95-1967-402f-a664-0347e86a6834"},"source":"You will then be prompted to upload your CSV file, and add a brief description of your submission. When you make your submission, Kaggle will process your predictions and give you your accuracy for the holdout data and your ranking. When it is finished processing you will see our first submission gets an accuracy score of 0.75598, or 75.6%."},{"cell_type":"markdown","metadata":{"_uuid":"a6fada2e2eed737de0223e738446642035f2a301","_cell_guid":"4fe30bc2-76db-4467-aea4-feb46bec27bb"},"source":"![](https://s3.amazonaws.com/dq-content/185/submission_rank.png)"},{"cell_type":"markdown","metadata":{"_uuid":"d38feee7e5387bee299d6f32b0b245260e26c5f8","_cell_guid":"40e4e5c6-94dd-4293-93fb-615c51f41ca4"},"source":"The fact that our accuracy on the holdout data is 75.6% compared with the 80.2% accuracy we got with cross-validation indicates that our model is overfitting slightly to our training data.\n\nAt the time of writing, accuracy of 75.6% gives a rank of 6,663 out of 7,954. It's easy to look at Kaggle leaderboards after your first submission and get discouraged, but keep in mind that this is just a starting point.\n\nIt's also very common to see a small number of scores of 100% at the top of the Titanic leaderboard and think that you have a long way to go. In reality, anyone scoring about 90% on this competition is likely cheating (it's easy to look up the names of the passengers in the holdout set online and see if they survived).\n\nThere is a great analysis on Kaggle, How am I doing with my score, which uses a few different strategies and suggests a minimum score for this competition is 62.7% (achieved by presuming that every passenger died) and a maximum of around 82%. We are a little over halfway between the minimum and maximum, which is a great starting point."},{"cell_type":"markdown","metadata":{"_uuid":"e0fa4d9a957287bcf781eb503b6fbf6f7b087671","_cell_guid":"7a8d7ae4-c456-43ef-b353-7e6304185037"},"source":"## 12. Next Steps"},{"cell_type":"markdown","metadata":{"_uuid":"19aac309b0cbbcb29f68a14333d39ddac26fdb49","_cell_guid":"a24c472d-6df8-4b25-b6cc-f867174da783"},"source":"There are many things we can do to improve the accuracy of our model. Here are some that we will cover in the next two missions of this course:\n\n* Improving the features:\n    * Feature Engineering: Create new features from the existing data.\n    * Feature Selection: Select the most relevant features to reduce noise and overfitting.\n* Improving the model:\n    * Model Selection: Try a variety of models to improve performance.\n    * Hyperparameter Optimization: Optimize the settings within each particular machine learning model."},{"cell_type":"markdown","metadata":{"_uuid":"b4e588b70e4620241c6f3bc47e5c70a89b4de084","_cell_guid":"e815964c-4a41-4695-bd27-b06563268128"},"source":"## Resources\n\n* https://www.dataquest.io/course/kaggle-fundamentals"},{"cell_type":"markdown","metadata":{"_uuid":"811c05d71fee644d42b73d1ba4eeda8106b637e9","_cell_guid":"da777ede-a5a5-461a-a2aa-eb8b039d4448"},"source":"## Further Reading\n\n* https://github.com/justmarkham/scikit-learn-videos"},{"cell_type":"markdown","metadata":{"_uuid":"bd1a90d94ae4390726b89097c5408e1b4a1f5657","_cell_guid":"d7d31d8e-143d-46da-8c6e-04d24367d79a"},"source":"## THE END, WELL DONE!"},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"eb036811f0162244b4c8265e0df9999490f32167","collapsed":true,"_cell_guid":"9b7e4a04-344b-4e13-b56d-57b6ac28822f"},"source":""}],"nbformat_minor":1}