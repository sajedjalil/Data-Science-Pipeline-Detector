{"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python","mimetype":"text/x-python","pygments_lexer":"ipython3","name":"python","file_extension":".py","version":"3.6.1"},"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"}},"nbformat":4,"nbformat_minor":0,"cells":[{"metadata":{"_uuid":"d7b440678586b6126557b47ec25ff256f5fc730e","collapsed":false,"_cell_guid":"05ce2c27-928a-4443-b055-e300583ef4ca","_execution_state":"idle"},"source":"**A small road map on solving this problem**","outputs":[],"execution_count":null,"cell_type":"markdown"},{"metadata":{"_uuid":"8bae4b9ff6f2671e38f5e3fa19f8e937af08687a","collapsed":false,"_cell_guid":"d9062ad7-8743-467f-9ec9-ec0fc1c74f67","_execution_state":"idle"},"source":"1st you should browse the data,  check the size, the nature of it, and read the requirements.","outputs":[],"execution_count":null,"cell_type":"markdown"},{"metadata":{"_uuid":"c741bfd6ba2c171b3606d30478f3a583445655e5","collapsed":false,"_cell_guid":"862573a8-4d17-4f55-9c1e-711185278afa","_execution_state":"idle"},"source":"2nd you need to to load the data, ","outputs":[],"execution_count":null,"cell_type":"markdown"},{"metadata":{"_uuid":"2b80e536aadf68948ad2d98f4936a003178d9154","collapsed":false,"_cell_guid":"cd5ee564-1989-425b-9f4d-9aeacdf95c43","_execution_state":"idle"},"source":"3rd do a fast cleaning for the data to be able to implement a fast learning algorithm,","outputs":[],"execution_count":null,"cell_type":"markdown"},{"metadata":{"_uuid":"370f7c2e94023769a91a033ea3808e6a0cba8b25","collapsed":false,"_cell_guid":"0e0ad441-8308-4f2a-86af-916e2174a2ab","_execution_state":"idle"},"source":"4th after getting the first result, even if it is a dirty implementation, it will be a good start to go through the problem again check the data cleaning, feature engineering needed, tweak the parameters , try another algorithm to compare the results.","outputs":[],"execution_count":null,"cell_type":"markdown"},{"metadata":{"_uuid":"727f065bd5362dcb3321b2f358cb8a78bc84ca6f","trusted":false,"_cell_guid":"53544c84-5d59-464c-b9ad-670df755f2a1","_execution_state":"idle"},"source":"# Import the needed referances\nimport pandas as pd\nimport numpy as np\nimport csv as csv\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\n\n#Shuffle the datasets\nfrom sklearn.utils import shuffle\n\n#Learning curve\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import ShuffleSplit\n\n#import seaborn as sns\n#Output plots in notebook\n#%matplotlib inline ","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"4cbeff31a28660f5e34a48657d4fe102a1c062ca","collapsed":false,"trusted":false,"_cell_guid":"29134188-3f6d-490f-9b49-d55de341fded","_execution_state":"idle"},"source":"#loading the data sets from the csv files\nprint('--------load train & test file------')\ntrain_dataset = pd.read_csv('../input/train.csv')\ntest_dataset = pd.read_csv('../input/test.csv')\nprint(\"------finish loading --------------------\")","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"68ad42d3818f554d88a1308c547e98206b71c1a1","collapsed":false,"trusted":false,"_cell_guid":"84133a1d-81a7-4568-a62d-b2392dfabbc2","_execution_state":"idle"},"source":"# print data sets information \nprint('----train dataset information-------')\ntrain_dataset.info()\nprint('----test dataset information--------')\ntest_dataset.info()\nprint(\"------------------------------------\")","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"117c47d78c776b36c0749a1810d966a046222aa7","collapsed":false,"trusted":false,"_cell_guid":"cc04a835-15e0-487d-80b4-a1e5fea44479","_execution_state":"idle"},"source":"#Check for missing data & list them \nnas = pd.concat([train_dataset.isnull().sum(), test_dataset.isnull().sum()], axis=1, keys=['Train Dataset', 'Test Dataset'])\nnas[nas.sum(axis=1) > 0]","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"3304f08e385320c48a86363ae5064bb722ade8da","collapsed":false,"trusted":false,"_cell_guid":"9cfb76cd-b88c-4953-94b9-b744b1423f04","_execution_state":"idle"},"source":"# Data sets cleaing, fill nan (null) where needed and delete uneeded columns\nprint('----Strat data cleaning ------------')\n\n#manage Age\ntrain_random_ages = np.random.randint(train_dataset[\"Age\"].mean() - train_dataset[\"Age\"].std(),\n                                          train_dataset[\"Age\"].mean() + train_dataset[\"Age\"].std(),\n                                          size = train_dataset[\"Age\"].isnull().sum())\n\ntest_random_ages = np.random.randint(test_dataset[\"Age\"].mean() - test_dataset[\"Age\"].std(),\n                                          test_dataset[\"Age\"].mean() + test_dataset[\"Age\"].std(),\n                                          size = test_dataset[\"Age\"].isnull().sum())\n\ntrain_dataset[\"Age\"][np.isnan(train_dataset[\"Age\"])] = train_random_ages\ntest_dataset[\"Age\"][np.isnan(test_dataset[\"Age\"])] = test_random_ages\ntrain_dataset['Age'] = train_dataset['Age'].astype(int)\ntest_dataset['Age']    = test_dataset['Age'].astype(int)\n\n# Embarked \ntrain_dataset[\"Embarked\"].fillna('S', inplace=True)\ntest_dataset[\"Embarked\"].fillna('S', inplace=True)\ntrain_dataset['Port'] = train_dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\ntest_dataset['Port'] = test_dataset['Embarked'].map({'S': 0, 'C': 1, 'Q': 2}).astype(int)\ndel train_dataset['Embarked']\ndel test_dataset['Embarked']\n\n# Fare\ntrain_dataset[\"Fare\"].fillna(train_dataset[\"Fare\"].median(), inplace=True)\ntest_dataset[\"Fare\"].fillna(test_dataset[\"Fare\"].median(), inplace=True)\n\ntrain_dataset['Fare']    = train_dataset['Fare'].astype(int)\ntest_dataset['Fare'] = test_dataset['Fare'].astype(int)\n\n# Map Sex to a new column Gender as 'female': 0, 'male': 1\ntrain_dataset['Gender'] = train_dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\ntest_dataset['Gender'] = test_dataset['Sex'].map({'female': 0, 'male': 1}).astype(int)\n# Delete Sex column from datasets\ndel train_dataset['Sex']\ndel test_dataset['Sex']\n\n# Delete Ticket column from datasets  (No need for them in the analysis)\ndel train_dataset['Ticket']\ndel test_dataset['Ticket']\n\n# Cabin has a lot of nan values, so i will remove it\ndel train_dataset['Cabin']\ndel test_dataset['Cabin']","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"fd17b09bdbb4e95aa6a668c1fea3a6819b32b050","collapsed":false,"_cell_guid":"90849a2d-72af-4d4c-bf36-13d59aaa4a66","_execution_state":"idle"},"source":"** Engineer New Features **","outputs":[],"execution_count":null,"cell_type":"markdown"},{"metadata":{"_uuid":"1720e3fe62ca0cfdaccbf44070201f3d19381ed1","collapsed":false,"trusted":false,"_cell_guid":"73e20c44-af4d-47df-bab9-6787495e6b5e","_execution_state":"idle"},"source":"# engineer a new Title feature\n# Get titles from the names\ntrain_dataset['Title'] = train_dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\ntest_dataset['Title'] = test_dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\n# group them\nfull_dataset = [train_dataset, test_dataset]\nfor dataset in full_dataset:\n    dataset['Title'] = dataset['Title'].replace(['Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev'], 'Officer')\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Sir', 'Jonkheer', 'Dona'], 'Royal')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n\n# Get the average survival rate of different titles\ntrain_dataset[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()\n\n##engineer the family size feature\nfor dataset in full_dataset:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n    \n## Create new column \"FamilySizeGroup\" and assign \"Alone\", \"Small\" and \"Big\"\nfor dataset in full_dataset:\n    dataset['FamilySizeGroup'] = 'Small'\n    dataset.loc[dataset['FamilySize'] == 1, 'FamilySizeGroup'] = 'Alone'\n    dataset.loc[dataset['FamilySize'] >= 5, 'FamilySizeGroup'] = 'Big'\n\n## Get the average survival rate of different FamilySizes\ntrain_dataset[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean()\n\n## engineer a new ischild feature\nfor dataset in full_dataset:\n    dataset['IsChild'] = 0\n    dataset.loc[dataset['Age'] <= 10, 'IsChild'] = 1\n    \nfor dataset in full_dataset:\n    dataset.loc[(dataset['Age'] <= 10), 'AgeGroup'] = 1\n    dataset.loc[(dataset['Age'] > 50) & (dataset['Age'] <= 60), 'AgeGroup'] = 2\n    dataset.loc[(dataset['Age'] > 30) & (dataset['Age'] <= 40), 'AgeGroup'] = 3\n    dataset.loc[(dataset['Age'] > 40) & (dataset['Age'] <= 50), 'AgeGroup'] = 4\n    dataset.loc[(dataset['Age'] > 10) & (dataset['Age'] <= 20), 'AgeGroup'] = 5\n    dataset.loc[(dataset['Age'] > 20) & (dataset['Age'] <= 30), 'AgeGroup'] = 6\n    dataset.loc[(dataset['Age'] > 70) & (dataset['Age'] <= 80), 'AgeGroup'] = 7\n    dataset.loc[(dataset['Age'] > 60) & (dataset['Age'] <= 70), 'AgeGroup'] = 8\n    \n# Convert to integer type\n#full_dataset['AgeGroup'] = full_dataset['AgeGroup'].astype(int)\n#full_dataset['AgeGroup'] = full_dataset['AgeGroup'].astype(int)\n\n    ","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"b970c7f9fb7c4691af688d645e323f85b4f935f4","collapsed":false,"trusted":false,"_cell_guid":"c972a955-a0c0-4cf6-bf20-5a2966a43848","_execution_state":"idle"},"source":"# map the new features\ntitle_mapping = {\"Mr\": 0, \"Officer\": 1, \"Master\": 2, \"Miss\": 3, \"Royal\": 4, \"Mrs\": 5}\nfamily_mapping = {\"Small\": 0, \"Alone\": 1, \"Big\": 2}\nfor dataset in full_dataset:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['FamilySizeGroup'] = dataset['FamilySizeGroup'].map(family_mapping)\n    \n\n# Delete Name column from datasets (No need for them in the analysis)\ndel train_dataset['Name']\ndel test_dataset['Name']\n\ndel train_dataset['SibSp']\ndel test_dataset['SibSp']\n\ndel train_dataset['Parch']\ndel test_dataset['Parch']\n\ndel train_dataset['FamilySize']\ndel test_dataset['FamilySize']\n\nprint('----Finish data cleaning ------------')","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"118c2b346dce8d397f3cd1df3a46a78b689c877d","collapsed":false,"trusted":false,"_cell_guid":"bdf01b13-7a42-432c-ac51-da34a6d53728","_execution_state":"idle"},"source":"train_dataset.head()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"46ab8f33947351bf104855cd27578bb6b50ba574","collapsed":false,"trusted":false,"_cell_guid":"4c7f3797-5fea-4ebb-a16f-34c5a4e81119","_execution_state":"idle"},"source":"##Shuffling the datasets\n#train_dataset = shuffle(train_dataset)\n#test_dataset = shuffle(test_dataset)\n#print('Finish Shuffling the datasets')","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"c4c6963558c73f499c884c06c5e8840676ce1078","collapsed":false,"trusted":false,"_cell_guid":"643f5a05-670b-445f-961b-41951ab3b4c7","_execution_state":"idle"},"source":"# create a validation data set arround 20 % \ntrain_dataset, valid_dataset = np.split(train_dataset.sample(frac=1), [int(.8*len(train_dataset))])\nprint(len(train_dataset))\nprint(len(valid_dataset))","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"5ee885be0c2425ccdbc423545bea8f4be042c8c5","collapsed":false,"trusted":false,"_cell_guid":"7b19a752-4e96-40f2-ae19-1dbe0b320b91","_execution_state":"idle"},"source":"del train_dataset['PassengerId']\ndel valid_dataset['PassengerId']\n\nX_train = train_dataset.drop(\"Survived\",axis=1).as_matrix()\nY_train = train_dataset[\"Survived\"].as_matrix()\n\nX_val = valid_dataset.drop(\"Survived\",axis=1).as_matrix()\nY_val = valid_dataset[\"Survived\"].as_matrix()\n\nX_test  = test_dataset.drop(\"PassengerId\",axis=1).copy().as_matrix()\n\nprint(X_train.shape)\nprint(Y_train.shape)\nprint(X_val.shape)\nprint(Y_val.shape)\nprint(X_test.shape)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"ba804030dde8b2a4bd98fed01ba2c5d052ebd3a2","collapsed":false,"trusted":false,"_cell_guid":"d770f3e0-82d2-43e7-bedf-cf71f2362922","_execution_state":"idle"},"source":"# Learning curve\ncv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)\nlogreg_model = LogisticRegression(C=1)\ndef Learning_curve_model(X, Y, model, cv, train_sizes):\n\n    plt.figure()\n    plt.title(\"Learning curve\")\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n\n\n    train_sizes, train_scores, test_scores = learning_curve(model, X, Y, cv=cv, n_jobs=4, train_sizes=train_sizes)\n\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std  = np.std(train_scores, axis=1)\n    test_scores_mean  = np.mean(test_scores, axis=1)\n    test_scores_std   = np.std(test_scores, axis=1)\n    plt.grid()\n    \n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",label=\"Cross-validation score\")\n                     \n    plt.legend(loc=\"best\")\n    return plt\n\nplot_lc      = 1   # 1--display learning curve/ 0 -- don't display\n\n#learn curve\nif plot_lc==1:\n    train_size=np.linspace(.1, 1.0, 15)\n    Learning_curve_model(X_train,Y_train , logreg_model, cv, train_size)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"98699cb1cabecf245bff9f353d104cc0c76f5cd0","collapsed":false,"_cell_guid":"a686ec17-b366-4c2e-a1e2-ed9a674b6815","_execution_state":"idle"},"source":"##Fixing##: Adding features: Fixes high bias , Adding polynomial features: Fixes high bias,Decreasing Î»: Fixes high bias**","outputs":[],"execution_count":null,"cell_type":"markdown"},{"metadata":{"_uuid":"cbbc6de12470fae1b595b7b06d5869a06783df9f","collapsed":false,"trusted":false,"_cell_guid":"b01d1ea1-f312-497e-a42f-ed965fb5d68c","_execution_state":"idle"},"source":"# Support Vector Machines\nsvc = SVC()\nsvc.fit(X_train, Y_train)\nY_pred = svc.predict(X_test)\n\nresult_val =svc.score(X_val, Y_val)\nresult_train = svc.score(X_train, Y_train)\nprint('taring score = %s , while validation score = %s' %(result_train , result_val))\n","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"15a01038a8e7df931a73ec83ee4262984ddd6625","collapsed":false,"trusted":false,"_cell_guid":"fca33ad5-9696-4aca-b490-bc8d12f1ebf2","_execution_state":"idle"},"source":"# Logistic Regression\nlogreg = LogisticRegression(C=1) #(C=0.1, penalty='l1', tol=1e-6)\nlogreg.fit(X_train, Y_train)\nY_pred = logreg.predict(X_test)\n\nresult_val =logreg.score(X_val, Y_val)\nresult_train = logreg.score(X_train, Y_train)\nprint('taring score = %s , while validation score = %s' %(result_train , result_val))","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"b250d05cbf6cf37c6ac8b566aa1085f4e8702b65","collapsed":false,"_execution_state":"idle"},"source":"# Random Forests\nrandom_forest = RandomForestClassifier(criterion='gini', \n                             n_estimators=700,\n                             min_samples_split=10,\n                             min_samples_leaf=1,\n                             max_features='auto',\n                             oob_score=True,\n                             random_state=1,\n                             n_jobs=-1)\n\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(X_test)\n\nresult_val =random_forest.score(X_val, Y_val)\nresult_train = random_forest.score(X_train, Y_train)\nprint('taring score = %s , while validation score = %s' %(result_train , result_val))","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"26eeeb7bb46271c50ea92394b6d1a51bf61655f3","collapsed":false,"trusted":false,"_cell_guid":"84a737f7-3543-417c-bda8-21ac8831fe27","_execution_state":"idle"},"source":"cof_df = pd.DataFrame(train_dataset.columns.delete(0))\ncof_df.columns = ['Features']\ncof_df[\"Coefficient Estimation\"] = pd.Series(logreg.coef_[0])\n\n#PRINT\nprint(cof_df)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"4af68544abca59692cbdd7d90c2a45fb038647cf","collapsed":false,"trusted":false,"_cell_guid":"bf11e1f7-1aae-400d-851c-bdfafd8a839b","_execution_state":"idle"},"source":"submission = pd.DataFrame({\n        \"PassengerId\": test_dataset[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })\nsubmission.to_csv('titanic.csv', index=False)\nprint('Exported')","outputs":[],"execution_count":null,"cell_type":"code"}]}