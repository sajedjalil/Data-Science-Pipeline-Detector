{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Keras and Keras Tuner best practices\n\nThis notebook presents how to use KerasTuner to find a high-performing model in just a few lines of code.\n\nFirst, let's start by installing the latest KerasTuner version:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install git+https://github.com/keras-team/keras-tuner.git -q","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Get the data\n\nNext, we load the data using Pandas.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\n\nfull_train_dataframe = pd.read_csv('../input/titanic/train.csv')\ntest_dataframe = pd.read_csv('../input/titanic/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"full_train_dataframe.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prepare a training and validation dataset\n\nNow, let's split the training data into a training split and a validation split.\n\nWe're going to be using the features Pclass, Sex, Age, SibSp, Parch, Fare.\nWe will drop the features Cabin, Name, PassengerId, Ticket, Embarked.\nBecause the feature Age contains NaN values, we will replace these values with the mean of the values found for this feature in the training data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\ndef fill_nan(df, mean_age):\n    df['Age'].fillna(value=mean_age, inplace=True)\n    \n# Create training and validation datasets\nval_dataframe = full_train_dataframe.sample(frac=0.2, random_state=1337)\ntrain_dataframe = full_train_dataframe.drop(val_dataframe.index)\nmean_age = np.mean(train_dataframe['Age'])\n    \nprint(\"Total number of training samples: %d\" % (len(full_train_dataframe)))\nprint(\"Total number of test samples: %d\" % (len(test_dataframe)))\nprint(\n    \"Using %d samples for training and %d for validation\"\n    % (len(train_dataframe), len(val_dataframe))\n)\n\nfill_nan(train_dataframe, mean_age)\nfill_nan(val_dataframe, mean_age)\nfill_nan(full_train_dataframe, mean_age)\nfill_nan(test_dataframe, mean_age)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We turn the Pandas dataframes into TF Datasets.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\n\ndef dataframe_to_dataset(dataframe, train=True):\n    dataframe = dataframe.copy()\n\n    # Drop useless features\n    dataframe.pop(\"Cabin\")\n    dataframe.pop(\"Name\")\n    dataframe.pop(\"Ticket\")\n    dataframe.pop(\"Embarked\")\n    dataframe.pop(\"PassengerId\")\n    \n    if train:\n        # Set aside labels\n        labels = dataframe.pop(\"Survived\")\n        # Create dataset\n        ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n        ds = ds.shuffle(buffer_size=len(dataframe))\n    else:\n        ds = tf.data.Dataset.from_tensor_slices(dict(dataframe))\n    return ds\n\ntrain_ds = dataframe_to_dataset(train_dataframe)\nval_ds = dataframe_to_dataset(val_dataframe)\ntest_ds = dataframe_to_dataset(test_dataframe, train=False)\nfull_train_ds = dataframe_to_dataset(full_train_dataframe)\n\n# Visualize the names and types of the features in one sample\nfor sample in train_ds.take(1):\n    for key in sample[0].keys():\n        print('Feature:', key, '- dtype:', sample[0][key].dtype.name)\n        \n# Batch the datasets and configure prefetching\ntrain_ds = train_ds.batch(32).prefetch(32)\nval_ds = val_ds.batch(32).prefetch(32)\ntest_ds = test_ds.batch(32).prefetch(32)\nfull_train_ds = full_train_ds.batch(32).prefetch(32)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Encode the features","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We prepare Keras Inputs for the different features in the data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow import keras\n\n# Numerical features\nage = keras.Input(shape=(1,), name='Age')\nfare = keras.Input(shape=(1,), name='Fare')\n\n# Integer categorical features\npclass = keras.Input(shape=(1,), name='Pclass', dtype='int64')\nsibsp = keras.Input(shape=(1,), name='SibSp', dtype='int64')\nparch = keras.Input(shape=(1,), name='Parch', dtype='int64')\n\n# String categorical features\nsex = keras.Input(shape=(1,), name='Sex', dtype='string')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We set up utilities to encode these features, using Keras Preprocessing Layers.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.layers.experimental.preprocessing import Normalization\nfrom tensorflow.keras.layers.experimental.preprocessing import CategoryEncoding\nfrom tensorflow.keras.layers.experimental.preprocessing import StringLookup\nfrom tensorflow.keras.layers.experimental.preprocessing import IntegerLookup\n\n\ndef encode_numerical_feature(feature, name, dataset):\n    # Create a Normalization layer for our feature\n    normalizer = Normalization()\n\n    # Prepare a Dataset that only yields our feature\n    feature_ds = dataset.map(lambda x, y: x[name])\n    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n\n    # Learn the statistics of the data\n    normalizer.adapt(feature_ds)\n\n    # Normalize the input feature\n    encoded_feature = normalizer(feature)\n    return encoded_feature\n\n\ndef encode_categorical_feature(feature, name, dataset):\n    # Create a Lookup layer which will turn strings into integer indices\n    if feature.dtype.name == 'string':\n        index = StringLookup()\n    else:\n        index = IntegerLookup()\n\n    # Prepare a Dataset that only yields our feature\n    feature_ds = dataset.map(lambda x, y: x[name])\n    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n\n    # Learn the set of possible feature values and assign them a fixed integer index\n    index.adapt(feature_ds)\n\n    # Turn the values into integer indices\n    encoded_feature = index(feature)\n\n    # Create a CategoryEncoding for our integer indices\n    encoder = CategoryEncoding(output_mode=\"binary\")\n\n    # Prepare a dataset of indices\n    feature_ds = feature_ds.map(index)\n\n    # Learn the space of possible indices\n    encoder.adapt(feature_ds)\n\n    # Apply one-hot encoding to our indices\n    encoded_feature = encoder(encoded_feature)\n    return encoded_feature\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We encode our features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Numerical features\nencoded_age = encode_numerical_feature(age, name='Age', dataset=train_ds)\nencoded_fare = encode_numerical_feature(fare, name='Fare', dataset=train_ds)\n\n# Integer categorical features\nencoded_pclass = encode_categorical_feature(pclass, name='Pclass', dataset=train_ds)\nencoded_sibsp = encode_categorical_feature(sibsp, name='SibSp', dataset=train_ds)\nencoded_parch = encode_categorical_feature(parch, name='Parch', dataset=train_ds)\n\n# String categorical features\nencoded_sex = encode_categorical_feature(sex, name='Sex', dataset=train_ds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prepare a KerasTuner search space\n\nWe prepare a hyperparameter search space to find the best model to build on top of these features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras import layers\n\ninputs = [age, fare, pclass, sibsp, parch, sex]\nfeatures = layers.concatenate([encoded_age, encoded_fare, encoded_pclass, encoded_sibsp, encoded_parch, encoded_sex])\n\ndef make_model(hp):\n    num_dense = hp.Int('num_dense', min_value=1, max_value=3, step=1)\n    x = features\n    for i in range(num_dense):\n        units = hp.Int('units_{i}'.format(i=i), min_value=8, max_value=256, step=8)\n        x = layers.Dense(units, activation='relu')(x)\n    outputs = layers.Dense(1)(x)\n    model = keras.Model(inputs, outputs)\n\n    learning_rate = hp.Float('learning_rate', min_value=3e-4, max_value=3e-3)\n    optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n    model.compile(loss=keras.losses.BinaryCrossentropy(from_logits=True),\n                  optimizer=optimizer,\n                  metrics=[keras.metrics.BinaryAccuracy(name='acc')])\n    model.summary()\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Run random search over the search space\n\nWe run random search over this hyperparmater search space.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import kerastuner as kt\n\ntuner = kt.tuners.RandomSearch(\n    make_model,\n    objective='val_acc',\n    max_trials=100,\n    overwrite=True)\n\ncallbacks=[keras.callbacks.EarlyStopping(monitor='val_acc', mode='max', patience=3)]\ntuner.search(train_ds, validation_data=val_ds, callbacks=callbacks, epochs=100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On the Kaggle CPU runtime, trying out 100 models takes 2 minutes. At the end of the search, our best validation accuracy is 84.8%.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Find the best epoch\n\nNow, we can retrieve the best hyperparameters, use them to build the best model, and train the model for 100 epochs to find at which epoch training should stop.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"best_hp = tuner.get_best_hyperparameters()[0]\nmodel = make_model(best_hp)\nhistory = model.fit(train_ds, validation_data=val_ds, epochs=100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train the production model\n\nFinally, we can train the best model configuration from scratch for the optimal number of epochs.\n\nThis time, we train on the entirety of the training data -- no validation split. Our model parameters are already validated.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"val_acc_per_epoch = history.history['val_acc']\nbest_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\nprint('Best epoch: %d' % (best_epoch,))\nmodel = make_model(best_hp)\nmodel.fit(full_train_ds, epochs=best_epoch)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\npredictions = tf.nn.sigmoid(model.predict(test_ds)).numpy()\npassenger_ids = test_dataframe.pop(\"PassengerId\")\nsubmission = pd.DataFrame({\"PassengerId\": passenger_ids,\n                           \"Survived\": np.ravel(np.round(predictions))})\nsubmission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}