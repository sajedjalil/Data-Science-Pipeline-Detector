{"cells":[{"metadata":{"_uuid":"08c5693b1c47e47bb5ddd0979f3865b9b0de95c9"},"cell_type":"markdown","source":"# Machine Learning to Predict Titanic Survivors \nHi, I'm a current undergraduate student interested in the Data Science and Machine Learning field. In this Kernel, I will try to step by step build a ML model using sklearn to predict the outcomes of each passenger aboard the titanic. This guide is meant for people starting with data visualization, analysis and Machine Learning. If that sounds like you, then you're in the right place! It is not as difficult as you think to understand.\n\n*Please upvote and share if this helps you!! Also, feel free to fork this kernel to play around with the code and test it for yourself. If you plan to use any part of this code, please reference this kernel!* I will be glad to answer any questions you may have in the comments. Thank You! \n"},{"metadata":{"_uuid":"e5f4bfc42ab2cc96dc924cd12ea144e2ee1d76dd"},"cell_type":"markdown","source":"## Update\nThank you all so much for the support and reading this kernel! I am very inspired to keep learning and I hope you are too. I am in the progress of making more kernels for more competitions as well as ones for data visualization and statistics. Please stay tuned for those, as I will be publishing them very soon! Again, thank you so much and please feel free to contact me or ask any questions! "},{"metadata":{"_uuid":"7e51a8d8c11aeb90bb74e1519b537203318a6ca8"},"cell_type":"markdown","source":"# Contents\n1. [Importing Libraries and Packages](#p1)\n2. [Loading and Viewing Data Set](#p2)\n3. [Dealing with NaN Values (Imputation)](#p3)\n4. [Plotting and Visualizing Data](#p4)\n5. [Feature Engineering](#p5)\n6. [Modeling and Predicting with sklearn](#p6)\n7. [Evaluating Model Performances](#p7)\n8. [Tuning Parameters with GridSearchCV](#p8)\n9. [Submission](#p9)"},{"metadata":{"_uuid":"b5a9e650ed4ee4bcb942881e39476de033684e8f"},"cell_type":"markdown","source":"<a id=\"p1\"></a>\n# 1. Importing Libraries and Packages\nWe will use these packages to help us manipulate the data and visualize the features/labels as well as measure how well our model performed. Numpy and Pandas are helpful for manipulating the dataframe and its columns and cells. We will use matplotlib along with Seaborn to visualize our data."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nsns.set_style(\"whitegrid\")\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9b574000bd7d01fc11bd1a2a129512c420ae9d60"},"cell_type":"markdown","source":"<a id=\"p2\"></a>\n# 2. Loading and Viewing Data Set\nWith Pandas, we can load both the training and testing set that we wil later use to train and test our model. Before we begin, we should take a look at our data table to see the values that we'll be working with. We can use the head and describe function to look at some sample data and statistics. We can also look at its keys and column names."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"scrolled":true},"cell_type":"code","source":"training = pd.read_csv(\"../input/train.csv\")\ntesting = pd.read_csv(\"../input/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"17d595b96b41eecfaeb9185bca85c7b5abd8d2a1"},"cell_type":"code","source":"training.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea5cc5601d9989c520b8269f0d3434fd4b7c4128"},"cell_type":"code","source":"training.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d25ce4c089559faf5ab3b78667e6201a62e57da"},"cell_type":"code","source":"print(training.keys())\nprint(testing.keys())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e6d7efeb33bee9b004b0a309c21feb9c90bde389"},"cell_type":"markdown","source":"<a id=\"p3\"></a>\n# 3. Dealing with NaN Values (Imputation)\nThere are NaN values in our data set in the age column. Furthermore, the Cabin column has too many missing values and isn't useful to be used in predicting survival. We can just drop the column as well as the NaN values which will get in the way of training our model. We also need to fill in the NaN values with replacement values in order for the model to have a complete prediction for every row in the data set. This process is known as **imputation** and we will show how to replace the missing data."},{"metadata":{"trusted":true,"_uuid":"fc61dad51cca67aafca7447fb02b58d8bbb2a226","scrolled":true},"cell_type":"code","source":"def null_table(training, testing):\n    print(\"Training Data Frame\")\n    print(pd.isnull(training).sum()) \n    print(\" \")\n    print(\"Testing Data Frame\")\n    print(pd.isnull(testing).sum())\n\nnull_table(training, testing)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4dfb986a769ce38cb258a3e8f0fc2b4c9490612f"},"cell_type":"code","source":"training.drop(labels = [\"Cabin\", \"Ticket\"], axis = 1, inplace = True)\ntesting.drop(labels = [\"Cabin\", \"Ticket\"], axis = 1, inplace = True)\n\nnull_table(training, testing)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b8cd1cb47f574b46b2058f2e3635d034ce9a57e5"},"cell_type":"markdown","source":"We take a look at the distribution of the Age column to see if it's skewed or symmetrical. This will help us determine what value to replace the NaN values."},{"metadata":{"trusted":true,"_uuid":"b791d174a23371a6a9748906771189bf45051ff2"},"cell_type":"code","source":"copy = training.copy()\ncopy.dropna(inplace = True)\nsns.distplot(copy[\"Age\"])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c04adcb431b42cb574ee0741fdb86925b83863ff"},"cell_type":"markdown","source":"* Looks like the distribution of ages is slightly skewed right. Because of this, we can fill in the null values with the median for the most accuracy."},{"metadata":{"trusted":true,"_uuid":"dfa607948743e80892fdf82a000c7a96873d38ce"},"cell_type":"code","source":"#the median will be an acceptable value to place in the NaN cells\ntraining[\"Age\"].fillna(training[\"Age\"].median(), inplace = True)\ntesting[\"Age\"].fillna(testing[\"Age\"].median(), inplace = True) \ntraining[\"Embarked\"].fillna(\"S\", inplace = True)\ntesting[\"Fare\"].fillna(testing[\"Fare\"].median(), inplace = True)\n\nnull_table(training, testing)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8ae1ecf061805ccaf6663b50bf4c21252f4195a4"},"cell_type":"markdown","source":"<a id=\"p4\"></a>\n# 4. Plotting and Visualizing Data\nIt is very important to understand and visualize any data we are going to use in a machine learning model. By visualizing, we can see the trends and general associations of variables like Sex and Age with survival rate. We can make several different graphs for each feature we want to work with to see the entropy and information gain of the feature. "},{"metadata":{"_uuid":"14ac153fd1d8992bed821c28532da3dd99a3ce2d"},"cell_type":"markdown","source":"**Gender**"},{"metadata":{"trusted":true,"_uuid":"a313f660ca8133f5c625814a6c599f5c527a9b22"},"cell_type":"code","source":"#can ignore the testing set for now\nsns.barplot(x=\"Sex\", y=\"Survived\", data=training)\nplt.title(\"Distribution of Survival based on Gender\")\nplt.show()\n\ntotal_survived_females = training[training.Sex == \"female\"][\"Survived\"].sum()\ntotal_survived_males = training[training.Sex == \"male\"][\"Survived\"].sum()\n\nprint(\"Total people survived is: \" + str((total_survived_females + total_survived_males)))\nprint(\"Proportion of Females who survived:\") \nprint(total_survived_females/(total_survived_females + total_survived_males))\nprint(\"Proportion of Males who survived:\")\nprint(total_survived_males/(total_survived_females + total_survived_males))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e924d85d07f0da76ad085c5ca8f26172a80ad987"},"cell_type":"markdown","source":"> Note that the numbers printed above are the proportion of male/female survivors of all the surviviors ONLY. The graph shows the propotion of male/females out of ALL the passengers including those that didn't survive."},{"metadata":{"_uuid":"69f48dadbaa468414c542c74af8bfe98a68ec20d"},"cell_type":"markdown","source":"Gender appears to be a very good feature to use to predict survival, as shown by the large difference in propotion survived. Let's take a look at how class plays a role in survival as well."},{"metadata":{"_uuid":"8479cb62de593c2945b112b645a527be7aa776f3"},"cell_type":"markdown","source":"**Class**"},{"metadata":{"trusted":true,"_uuid":"d55d78e57c418f2ea9f57c6ae649dc9f044ef553"},"cell_type":"code","source":"sns.barplot(x=\"Pclass\", y=\"Survived\", data=training)\nplt.ylabel(\"Survival Rate\")\nplt.title(\"Distribution of Survival Based on Class\")\nplt.show()\n\ntotal_survived_one = training[training.Pclass == 1][\"Survived\"].sum()\ntotal_survived_two = training[training.Pclass == 2][\"Survived\"].sum()\ntotal_survived_three = training[training.Pclass == 3][\"Survived\"].sum()\ntotal_survived_class = total_survived_one + total_survived_two + total_survived_three\n\nprint(\"Total people survived is: \" + str(total_survived_class))\nprint(\"Proportion of Class 1 Passengers who survived:\") \nprint(total_survived_one/total_survived_class)\nprint(\"Proportion of Class 2 Passengers who survived:\")\nprint(total_survived_two/total_survived_class)\nprint(\"Proportion of Class 3 Passengers who survived:\")\nprint(total_survived_three/total_survived_class)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"205515b33d6dd0962cab4fb825caa45c77e5ec12"},"cell_type":"code","source":"sns.barplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=training)\nplt.ylabel(\"Survival Rate\")\nplt.title(\"Survival Rates Based on Gender and Class\")\n#help(sns.barplot)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ac54f705c3fc462445cad2de978d7077646f933a"},"cell_type":"code","source":"sns.barplot(x=\"Sex\", y=\"Survived\", hue=\"Pclass\", data=training)\nplt.ylabel(\"Survival Rate\")\nplt.title(\"Survival Rates Based on Gender and Class\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"13dde8426cb8a654e6f50cb1855e4369c362ea68"},"cell_type":"markdown","source":"It appears that class also plays a role in survival, as shown by the bar graph. People in Pclass 1 were more likely to survive than people in the other 2 Pclasses."},{"metadata":{"_uuid":"ad10453d5d42100bfb65be57900ff1554508a95c"},"cell_type":"markdown","source":"**Age**"},{"metadata":{"trusted":true,"_uuid":"323af5a70d2033f9fa1b5cc8286a675869ba302e"},"cell_type":"code","source":"survived_ages = training[training.Survived == 1][\"Age\"]\nnot_survived_ages = training[training.Survived == 0][\"Age\"]\nplt.subplot(1, 2, 1)\nsns.distplot(survived_ages, kde=False)\nplt.axis([0, 100, 0, 100])\nplt.title(\"Survived\")\nplt.ylabel(\"Proportion\")\nplt.subplot(1, 2, 2)\nsns.distplot(not_survived_ages, kde=False)\nplt.axis([0, 100, 0, 100])\nplt.title(\"Didn't Survive\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"46374be19c92dbeb146493226b47701820a1bc7f"},"cell_type":"code","source":"sns.stripplot(x=\"Survived\", y=\"Age\", data=training, jitter=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"56bcea2157a75ed314388f1804aa2271cc631f25"},"cell_type":"markdown","source":"It appears as though passengers in the younger range of ages were more likely to survive than those in the older range of ages, as seen by the clustering in the strip plot, as well as the survival distributions of the histogram."},{"metadata":{"_uuid":"60d068f1155860f57f3395bd8bc6b752c050c8e5"},"cell_type":"markdown","source":"Here is one final cumulative graph of a pair plot that shows the relations between all of the different features"},{"metadata":{"trusted":true,"_uuid":"7d98aac15ffac0c0a3282961850dabc42ae79b1c"},"cell_type":"code","source":"sns.pairplot(training)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5a208dd95d6b8546f25d02a55822fb84800e3772"},"cell_type":"markdown","source":"<a id=\"p5\"></a>\n# 5. Feature Engineering\nBecause values in the Sex and Embarked columns are categorical values, we have to represent these strings as numerical values in order to perform our classification with our model. We can also do this process through **One-Hot-Encoding**."},{"metadata":{"trusted":true,"_uuid":"bbf26333a83e030e896203276d0d361ec021ac35","scrolled":false},"cell_type":"code","source":"training.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"66934ee8a0f64f0faa2afce91d9e28a4a1b56864"},"cell_type":"code","source":"testing.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"adbfc2d35676144cea7a3a80775229d51030990d"},"cell_type":"markdown","source":"We change Sex to binary, as either 1 for female or 0 for male. We do the same for Embarked. We do this same process on both the training and testing set to prepare our data for Machine Learning."},{"metadata":{"trusted":true,"_uuid":"18893231d1a1ec187e76095762e9f278d91e0714"},"cell_type":"code","source":"training.loc[training[\"Sex\"] == \"male\", \"Sex\"] = 0\ntraining.loc[training[\"Sex\"] == \"female\", \"Sex\"] = 1\n\ntraining.loc[training[\"Embarked\"] == \"S\", \"Embarked\"] = 0\ntraining.loc[training[\"Embarked\"] == \"C\", \"Embarked\"] = 1\ntraining.loc[training[\"Embarked\"] == \"Q\", \"Embarked\"] = 2\n\ntesting.loc[testing[\"Sex\"] == \"male\", \"Sex\"] = 0\ntesting.loc[testing[\"Sex\"] == \"female\", \"Sex\"] = 1\n\ntesting.loc[testing[\"Embarked\"] == \"S\", \"Embarked\"] = 0\ntesting.loc[testing[\"Embarked\"] == \"C\", \"Embarked\"] = 1\ntesting.loc[testing[\"Embarked\"] == \"Q\", \"Embarked\"] = 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b857a8e6e2e9542d53c01e4699f8d101780d5fee"},"cell_type":"code","source":"testing.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"49a1c9d969495eacc54b5e03086ee4e682f7c472"},"cell_type":"markdown","source":"We can combine SibSp and Parch into one synthetic feature called family size, which indicates the total number of family members on board for each member. "},{"metadata":{"trusted":true,"_uuid":"889c1c9d5c7a42f56562f12dfe9fc76cd4949cd1"},"cell_type":"code","source":"training[\"FamSize\"] = training[\"SibSp\"] + training[\"Parch\"] + 1\ntesting[\"FamSize\"] = testing[\"SibSp\"] + testing[\"Parch\"] + 1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f41eb8b24fa207451ef641776189afb16c811c5d"},"cell_type":"markdown","source":"This IsAlone feature also may work well with the data we're dealing with, telling us whether the passenger was along or not on the ship."},{"metadata":{"trusted":true,"_uuid":"d42df43ea7691877bb3875d15ce2804962f1e70e"},"cell_type":"code","source":"training[\"IsAlone\"] = training.FamSize.apply(lambda x: 1 if x == 1 else 0)\ntesting[\"IsAlone\"] = testing.FamSize.apply(lambda x: 1 if x == 1 else 0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e57b637bc0e7028f1373559b8b8485fb983fa456"},"cell_type":"markdown","source":"Although it may not seem like it, we can also extract some useful information from the name column. Not the actual names themselves, but the title of their names like Ms. or Mr. This may also provide a hint as to whether the passenger survived or not. Therefore we can extract this title and then encode it like we did for Sex and Embarked."},{"metadata":{"trusted":true,"_uuid":"fb584d57bfc2e25e1e106aeb117c87ee9ffa3c75"},"cell_type":"code","source":"for name in training[\"Name\"]:\n    training[\"Title\"] = training[\"Name\"].str.extract(\"([A-Za-z]+)\\.\",expand=True)\n    \nfor name in testing[\"Name\"]:\n    testing[\"Title\"] = testing[\"Name\"].str.extract(\"([A-Za-z]+)\\.\",expand=True)\n    \ntitle_replacements = {\"Mlle\": \"Other\", \"Major\": \"Other\", \"Col\": \"Other\", \"Sir\": \"Other\", \"Don\": \"Other\", \"Mme\": \"Other\",\n          \"Jonkheer\": \"Other\", \"Lady\": \"Other\", \"Capt\": \"Other\", \"Countess\": \"Other\", \"Ms\": \"Other\", \"Dona\": \"Other\", \"Rev\": \"Other\", \"Dr\": \"Other\"}\n\ntraining.replace({\"Title\": title_replacements}, inplace=True)\ntesting.replace({\"Title\": title_replacements}, inplace=True)\n\ntraining.loc[training[\"Title\"] == \"Miss\", \"Title\"] = 0\ntraining.loc[training[\"Title\"] == \"Mr\", \"Title\"] = 1\ntraining.loc[training[\"Title\"] == \"Mrs\", \"Title\"] = 2\ntraining.loc[training[\"Title\"] == \"Master\", \"Title\"] = 3\ntraining.loc[training[\"Title\"] == \"Other\", \"Title\"] = 4\n\ntesting.loc[testing[\"Title\"] == \"Miss\", \"Title\"] = 0\ntesting.loc[testing[\"Title\"] == \"Mr\", \"Title\"] = 1\ntesting.loc[testing[\"Title\"] == \"Mrs\", \"Title\"] = 2\ntesting.loc[testing[\"Title\"] == \"Master\", \"Title\"] = 3\ntesting.loc[testing[\"Title\"] == \"Other\", \"Title\"] = 4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4c82b45238c0405bfa93f028af4f96d0c8b6f760","scrolled":true},"cell_type":"code","source":"print(set(training[\"Title\"]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"382e3a9d9a34fe26c3f0494ff5d3fd48df59038b"},"cell_type":"code","source":"training.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"41de656234ed079e07f5390372c3a9306a17e98a"},"cell_type":"markdown","source":"<a id=\"p6\"></a>\n# 6. Model Fitting and Predicting\nNow that our data has been processed and formmated properly, and that we understand the general data we're working with as well as the trends and associations, we can start to build our model. We can import different classifiers from sklearn. We will try different types of models to see which one gives the best accuracy for its predictions."},{"metadata":{"_uuid":"2c7048c1e2d22005039a58fea15ebadcc27ec2bf"},"cell_type":"markdown","source":"**sklearn Models to Test**"},{"metadata":{"trusted":true,"_uuid":"247cf16dde7c8b0403beae7a93322bf2863ec6f9"},"cell_type":"code","source":"from sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2cd333e04846dcd31b6a622fe901d4f55c958bd6"},"cell_type":"markdown","source":"To evaluate our model performance, we can use the make_scorere and accuracy_score function from sklearn metrics."},{"metadata":{"trusted":true,"_uuid":"63e0f0169267edd3e4c45ae829476f51e4d1df0e"},"cell_type":"code","source":"from sklearn.metrics import make_scorer, accuracy_score ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"92194bb407126ddec5273bb45fdbd5d515854e0a"},"cell_type":"markdown","source":"We can also use a GridSearch cross validation to find the optimal parameters for the model we choose to work with and use to predict on our testing set."},{"metadata":{"trusted":true,"_uuid":"9bd1078ce44840c71b2e4fe6a0651b538d892995"},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"db3842b4fcfcf0d1e7b0c802217b7a38d489ccb8"},"cell_type":"markdown","source":"**Defining Features in Training/Test Set**"},{"metadata":{"trusted":true,"_uuid":"d9484f14efc825d2eb1e5496995d57d6a788de22"},"cell_type":"code","source":"features = [\"Pclass\", \"Sex\", \"Age\", \"Embarked\", \"Fare\", \"FamSize\", \"IsAlone\", \"Title\"]\nX_train = training[features] #define training features set\ny_train = training[\"Survived\"] #define training label set\nX_test = testing[features] #define testing features set\n#we don't have y_test, that is what we're trying to predict with our model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a008d1b7a4a90e34b2844922295f8146823f02ee"},"cell_type":"markdown","source":"**Validation Data Set**\n\nAlthough we already have a test set, it is generally easy to overfit the data with these classifiers. It is therefore useful to have a third data set called the validation data set to ensure that our model doesn't overfit with the data. We can make this third data set with sklearn's train_test_split function. We can also use the validation data set to test the general accuracy of our model."},{"metadata":{"trusted":true,"_uuid":"525f4a8f4119d03e20726b7b594d037e3fa30172"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split #to create validation data set\n\nX_training, X_valid, y_training, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=0) #X_valid and y_valid are the validation sets","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d3597926b012f917a34e9c2369baaa35b4e79452"},"cell_type":"markdown","source":"**SVC Model**"},{"metadata":{"trusted":true,"_uuid":"39fd28f225d9f1a9d283aa441e5e57508cbc7474"},"cell_type":"code","source":"svc_clf = SVC() \nsvc_clf.fit(X_training, y_training)\npred_svc = svc_clf.predict(X_valid)\nacc_svc = accuracy_score(y_valid, pred_svc)\n\nprint(acc_svc)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e6fa08797335fb70e5399aac9d0aa418ddc91416"},"cell_type":"markdown","source":"**LinearSVC Model**"},{"metadata":{"trusted":true,"_uuid":"58b88de30c4a4fa7273cf63e50a54a71fb70e83b"},"cell_type":"code","source":"linsvc_clf = LinearSVC()\nlinsvc_clf.fit(X_training, y_training)\npred_linsvc = linsvc_clf.predict(X_valid)\nacc_linsvc = accuracy_score(y_valid, pred_linsvc)\n\nprint(acc_linsvc)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e2a0d34546d72e87ea6b6cc8515040fd3f315227"},"cell_type":"markdown","source":"**RandomForest Model**"},{"metadata":{"trusted":true,"_uuid":"1a64bcaa1f6beb3fb70f1fe0c8565379a4cce677"},"cell_type":"code","source":"rf_clf = RandomForestClassifier()\nrf_clf.fit(X_training, y_training)\npred_rf = rf_clf.predict(X_valid)\nacc_rf = accuracy_score(y_valid, pred_rf)\n\nprint(acc_rf)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"be2afad3f04bb3317225d689198ce2f817d1634d"},"cell_type":"markdown","source":"**LogisiticRegression Model**"},{"metadata":{"trusted":true,"_uuid":"1fb3aef22962da43e521b386c754c9b2c7ba0e9e"},"cell_type":"code","source":"logreg_clf = LogisticRegression()\nlogreg_clf.fit(X_training, y_training)\npred_logreg = logreg_clf.predict(X_valid)\nacc_logreg = accuracy_score(y_valid, pred_logreg)\n\nprint(acc_logreg)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b171f05d72bdc35ce4bb6c64ac22acb06986c4ed"},"cell_type":"markdown","source":"**KNeighbors Model**"},{"metadata":{"trusted":true,"_uuid":"4db5787d77d70de18781b2b15cd5ad5b16d0e2b0"},"cell_type":"code","source":"knn_clf = KNeighborsClassifier()\nknn_clf.fit(X_training, y_training)\npred_knn = knn_clf.predict(X_valid)\nacc_knn = accuracy_score(y_valid, pred_knn)\n\nprint(acc_knn)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"be52b3ffcc1f25f5c6bc03ff8a94802f6693657b"},"cell_type":"markdown","source":"**GaussianNB Model**"},{"metadata":{"trusted":true,"_uuid":"7c8531ec4857b0fb7ccdfc153fbd7a6ca07e4fef"},"cell_type":"code","source":"gnb_clf = GaussianNB()\ngnb_clf.fit(X_training, y_training)\npred_gnb = gnb_clf.predict(X_valid)\nacc_gnb = accuracy_score(y_valid, pred_gnb)\n\nprint(acc_gnb)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0160729b9da414de164ca5aec33d70302014f7ea"},"cell_type":"markdown","source":"**DecisionTree Model**"},{"metadata":{"trusted":true,"_uuid":"5b66cede005b80b15db98dadfe25453f79cff61c"},"cell_type":"code","source":"dt_clf = DecisionTreeClassifier()\ndt_clf.fit(X_training, y_training)\npred_dt = dt_clf.predict(X_valid)\nacc_dt = accuracy_score(y_valid, pred_dt)\n\nprint(acc_dt)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4a010eb3bd4a277eb2aa21fb5a905b9f7e9a74a9"},"cell_type":"markdown","source":"**XGBoost Model**"},{"metadata":{"trusted":true,"_uuid":"986b1e492627eb16ab64ca42737be67bc401a8d2"},"cell_type":"code","source":"from xgboost import XGBClassifier\n\nxg_clf = XGBClassifier(objective=\"binary:logistic\", n_estimators=10, seed=123)\nxg_clf.fit(X_training, y_training)\npred_xg = xg_clf.predict(X_valid)\nacc_xg = accuracy_score(y_valid, pred_xg)\n\nprint(acc_xg)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"adcda2b37a78d95205f14fbe4c580d4e0174088b"},"cell_type":"markdown","source":"<a id=\"p7\"></a>\n# 7. Evaluating Model Performances\nAfter making so many models and predictions, we should evaluate and see which model performed the best and which model to use on our testing set."},{"metadata":{"trusted":true,"_uuid":"032b969ac47e3ea4fb7b77a0d43411f5184012ca"},"cell_type":"code","source":"model_performance = pd.DataFrame({\n    \"Model\": [\"SVC\", \"Linear SVC\", \"Random Forest\", \n              \"Logistic Regression\", \"K Nearest Neighbors\", \"Gaussian Naive Bayes\",  \n              \"Decision Tree\", \"XGBClassifier\"],\n    \"Accuracy\": [acc_svc, acc_linsvc, acc_rf, \n              acc_logreg, acc_knn, acc_gnb, acc_dt, acc_xg]\n})\n\nmodel_performance.sort_values(by=\"Accuracy\", ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2155e211b1bcc8165f32b444a9122316d9d69e49"},"cell_type":"markdown","source":"It appears that the Random Forest model works the best with our data so we will use it on the test set."},{"metadata":{"_uuid":"294ea2437eec87aa86cd6b4793c88aa374cbbcb2"},"cell_type":"markdown","source":"<a id=\"p8\"></a>\n# 8. Tuning Parameters with GridSearchCV"},{"metadata":{"_uuid":"6e8feefac91cef0a68a90d6c99a498c8e7c0c220"},"cell_type":"markdown","source":"We can improve the accuracy of our model by turning the hyperparameters of our Random Forest model. We will run a GridSearchCV to find the best parameters for the model and use that model to train and test our data."},{"metadata":{"trusted":true,"_uuid":"cee16030035d751cf9c011b562381e892f71835d"},"cell_type":"code","source":"rf_clf = RandomForestClassifier()\n\nparameters = {\"n_estimators\": [4, 5, 6, 7, 8, 9, 10, 15], \n              \"criterion\": [\"gini\", \"entropy\"],\n              \"max_features\": [\"auto\", \"sqrt\", \"log2\"], \n              \"max_depth\": [2, 3, 5, 10], \n              \"min_samples_split\": [2, 3, 5, 10],\n              \"min_samples_leaf\": [1, 5, 8, 10]\n             }\n\ngrid_cv = GridSearchCV(rf_clf, parameters, scoring = make_scorer(accuracy_score))\ngrid_cv = grid_cv.fit(X_train, y_train)\n\nprint(\"Our optimized Random Forest model is:\")\ngrid_cv.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9a59fc2e5923c0f0862a17c8994be909b4e3c8e5"},"cell_type":"markdown","source":"Great, now that we have the optimal parameters for our Random Forest model, we can build a new model with those parameters to fit and use on the test set."},{"metadata":{"trusted":true,"_uuid":"d06da79f1940ec20b1e5207649cfd1b36024fb9f"},"cell_type":"code","source":"rf_clf = grid_cv.best_estimator_\n\nrf_clf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eb41e3a1cd4072d991b28899e8c0898410b706ac"},"cell_type":"markdown","source":"<a id=\"p9\"></a>\n# 9. Submission"},{"metadata":{"_uuid":"6d75b5ea02c6830d422e23b6332dfc81ece3bf54"},"cell_type":"markdown","source":"Let's create a dataframe to submit to the competition with our predictions of our model."},{"metadata":{"trusted":true,"_uuid":"3f003a3580bc74e884ca1525421cffbf15f347b0"},"cell_type":"code","source":"submission_predictions =rf_clf.predict(X_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1305526e29c632cec41f6f290382a25fcdd853c2"},"cell_type":"code","source":"submission = pd.DataFrame({\n        \"PassengerId\": testing[\"PassengerId\"],\n        \"Survived\": submission_predictions\n    })\n\nsubmission.to_csv(\"titanic.csv\", index=False)\nprint(submission.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"36ee8b4c497778afada1cc969e24011799cdc1ad"},"cell_type":"markdown","source":"If you made it this far, congratulations!! You have gotten a glimpse at an introduction to data visualization, analysis and Machine Learning. You are well on your way to become a Data Science expert! Keep learning and trying out new things, as one of the most important things for Data Scientists is to be creative and perform analysis hands-on. Please upvote and share if this kernel helped you!"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}