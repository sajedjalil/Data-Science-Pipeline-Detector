{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Titanic Exploratory Data Analysis\n### Prediction of survival rate using 9 variables","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"markdown","source":"For this analysis we use the <b>Titanic Kaggle Competition</b> dataset found on the Kaggle Data\nRepository at the following location:\n\n<a href=https://www.kaggle.com/c/titanic/data>Titanic Kaggle Competition Dataset</a>\n\nThe objective of the analysis is to classify passengers on the Titanic during the disaster\nof 1912 according to survival\n\nWe aim to achieve this by following the ML pipeline approach of deploying a variety of ML\ntechniques to build a final predictive model with the highest possible accuracy. This\nparticular analysis comprises 4 notebooks as follows:\n\n 1. <i>titanic_baseline</i> - Baseline predictive models (quick and dirty) to\n compare later results against\n 2. <i>titanic_eda</i> - <b>This notebook</b>, Exploratory Descriptive Analysis (EDA)\n 3. <i>titanic_features</i> - Perform feature engineering\n 4. <i>titanic_final_model</i> - Final model\n\nWe hope to gain valuable insights by following this process. The various steps in the\nprocess can be elaborated on as follows (the various notebooks will focus on different parts\n of the process as indicated):\n\n- Load data (<i>all notebooks</i>)\n- Prepare data\n    - Clean data (<i>notebook 2</i>)\n        - Missing values\n        - Outliers\n        - Erroneous values\n    - Explore data (<i>notebook 2</i>)\n        - Exploratory descriptive analysis (EDA)\n        - Correlation analysis\n        - Variable cluster analysis\n    - Transform Data (<i>notebook 3</i>)\n        - Engineer features\n        - Encode data\n        - Scale & normalise data\n        - Impute data (if not done in previous steps)\n        - Feature selection/ importance analysis\n- Build model (<i>notebooks 1 & 4</i>)\n    - Model selection\n    - Data sampling (validation strategy, imbalanced classification)\n    - Hyperparameter optimisation\n- Validate model (<i>notebooks 1 & 4</i>)\n    - Accuracy testing\n- Analysis of results (<i>notebook 1 & 4</i>)\n    - Response curves\n    - Accuracy analysis\n    - Commentary\n\nThe data dictionary for this dataset is as follows:\n\n| Variable | Definition | Key |\n|----------|------------|-----|\n| survival | Survival\t| 0 = No, 1 = Yes |\n| pclass   | Ticket class |\t1 = 1st, 2 = 2nd, 3 = 3rd |\n|sex | Sex | male, female |\n|Age | Age in years | Continuous |\n|sibsp | # of siblings / spouses aboard the Titanic | 0, 1, 2, ..|\n|parch | # of parents / children aboard the Titanic | 0, 1, 2 ..|\n|ticket | Ticket number | PC 17599, STON/O2. 3101282, 330877 |\n|fare | Passenger fare | Continuous |\n|cabin | Cabin number | C123, C85, E46 |\n|embarked | Port of Embarkation\t| C = Cherbourg, Q = Queenstown, S = Southampton |\n\nLet us start the analysis for <b>notebook 2</b>!\n\nThis notebook follows on the previous notebook. We previously concluded that there is a very\n strong signal in the data, even without any pre-processing. We also found that\n several variables have potential for improving the model if pre-processed. The analysis\n also suggests that improvement could be gained by feature engineering. In this notebook we\n will gain an understanding of the data and consider ways of improving our existing score by\n  re-doing our missing data imputation and performing feature engineering.\n\nWe will also learn more about the domain i.e. the events surrounding the accident.\n\nWe will perform an analysis on the entire training dataset i.e. 891 records. We will sense\ncheck our findings against the test set to ensure the two are aligned. We will then make\nsome high level improvements to our models based on our findings and see if our accuracy\nimproves.\n\nWe start by discussing the data with domain experts, to build up an intuition about the data\n. In my case I consulted the most knowledgeable person in our household regarding the matter\n i.e. my daughter whom has watched the movie several times. On asking her the question as to\n  who the most likely people to survive the Titanic accident she came up with the following\n  (without any hesitation or thought):\n\n- The wealthier passengers, as they would have cabins on the higher decks, closer to the\nlifeboats.\n- Woman, children and older passengers, as at the time chivalry still existed, and it is\nlikely that able-bodied younger men would have assisted these passengers with evacuation\n(which is not the case anymore today she however added :( ).\n- She also stated that individuals working on the Titanic staying on the lower decks\nwere less likely to survive (which prompted me to think that these individuals probably also\n did not pay a fare, or at least a very low fare...).\n\nThis is a nice place to start! Let us see what the data says.\n\nAlso, to start off with, thank you to the following authors for the inspiration and ideas (and\nsome code:)) which I have obtained from your excellent work:\n\nGunes Evitan: https://www.kaggle.com/gunesevitan/titanic-advanced-feature-engineering-tutorial\n\nAta Saygın Odabaşı: https://www.kaggle.com/atasaygin/titanic-randomforestclassifier-and-visualization (had to use your map!)","metadata":{}},{"cell_type":"code","source":"# Import libraries\nfrom subprocess import call\n\nimport patsy\nimport folium as folium\nfrom folium import plugins\nfrom folium.plugins import HeatMap\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom IPython.core.display import Image\nfrom matplotlib import pyplot\nfrom numpy import isnan\nfrom patsy.highlevel import dmatrices, dmatrix\nfrom sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif, chi2\nfrom sklearn.impute import KNNImputer\nfrom sklearn.impute._iterative import IterativeImputer\nfrom sklearn.linear_model import LogisticRegression\nfrom visualize_titanic import plot_confusion_matrix, plot_roc_curve, \\\n    plotVar, plotAge, plot_feature_importance, plot_feature_importance_dec\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import accuracy_score, confusion_matrix, roc_curve\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\nfrom scipy.stats import chi2_contingency\nfrom scipy.stats import chi2\nfrom pandasql import sqldf\nfrom mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS\nfrom sklearn.model_selection import GridSearchCV\nimport warnings\nwarnings.filterwarnings('ignore')\n\npd.set_option('display.max_columns', None)\npd.set_option('display.expand_frame_repr', False)\npd.set_option('max_colwidth', -1)","metadata":{"pycharm":{"name":"#%%\n"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">\n<b>Load data</b>\n</div>","metadata":{}},{"cell_type":"code","source":"# Import data\ndf_train = pd.read_csv('../input/titanic/train.csv', header = None,\n                       names = ['passenger_id', 'survived', 'p_class', 'name', 'sex', 'age',\n                                'sib_sp', 'parch', 'ticket', 'fare', 'cabin', 'embarked'],\n                       index_col=False, usecols = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11],\n                       skiprows=1, sep=',', skipinitialspace=True)","metadata":{"pycharm":{"name":"#%%\n"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Train Data","metadata":{}},{"cell_type":"code","source":"df_train.head(20)\nprint(df_train.shape)\n","metadata":{"pycharm":{"name":"#%%\n"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import data\ndf_test = pd.read_csv('../input/titanic/test.csv', header = None,\n                      names = ['passenger_id', 'p_class', 'name', 'sex', 'age', 'sib_sp',\n                               'parch', 'ticket', 'fare', 'cabin', 'embarked'],\n                      index_col=False, usecols = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n                      skiprows=1, sep=',', skipinitialspace=True)","metadata":{"pycharm":{"name":"#%%\n"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Test Data","metadata":{}},{"cell_type":"code","source":"df_test.head(20)\nprint(df_test.shape)\ndf_orig = df_test.copy()","metadata":{"pycharm":{"name":"#%%\n"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will peform our analysis as follows:\n\n- Missing data checks.\n- Model accuracy check.\n- EDA (without missing data replacement).\n- Variable adjustments & missing value imputation.\n- Model accuracy check\n\nFirst thing we will attend to is to bring the data we ignored in the first round, back into our\ndataset i.e. name, ticket and cabin.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">\n<b>Missing values</b>\n</div>","metadata":{}},{"cell_type":"code","source":"# We use will use all of the variables from here onwards.\ndf_train = df_train.loc[:, ['survived', 'p_class', 'name', 'sex', 'age', 'sib_sp', 'parch',\n                            'ticket', 'fare', 'cabin', 'embarked']]","metadata":{"pycharm":{"name":"#%%\n"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = df_test.loc[:, ['p_class', 'name', 'sex', 'age', 'sib_sp', 'parch', 'ticket',\n                          'fare', 'cabin', 'embarked']]","metadata":{"pycharm":{"name":"#%%\n"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We previously noted that there are missing values in the following fields: age, fare and\nembarked.\n\nWe quantify the exact number of missing values in the training set:","metadata":{}},{"cell_type":"code","source":"# Check for null values\nmissing_values_train = df_train.isnull().sum()\nmissing_values_train = missing_values_train.to_frame(name='num_missing')\nmissing_values_train['perc_missing'] = (missing_values_train['num_missing']/df_train.shape[0])*100\nfor index, row in missing_values_train.iterrows():\n    if (row['num_missing'] > 0):\n        print (\"For \\\"%s\\\" the number of missing values are: %d (%.0f%%)\" %  (index,\n                                                                     row['num_missing'],\n                                                                    row['perc_missing']))","metadata":{"pycharm":{"name":"#%%\n"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Consider a sample of missing values from the training set:\n","metadata":{}},{"cell_type":"code","source":"df_train[df_train.isnull().any(axis=1)]\n","metadata":{"pycharm":{"name":"#%%\n"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Quantify the exact number of missing values in the test set:","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"# Check for null values\nmissing_values_test = df_test.isnull().sum()\nmissing_values_test = missing_values_test.to_frame(name='num_missing')\nmissing_values_test['perc_missing'] = (missing_values_test['num_missing']/df_test.shape[0])*100\nfor index, row in missing_values_test.iterrows():\n    if (row['num_missing'] > 0):\n        print (\"For \\\"%s\\\" the number of missing values are: %d (%.0f%%)\" %  (index,\n                                                                     row['num_missing'],\n                                                                    row['perc_missing']))\n","metadata":{"pycharm":{"name":"#%%\n"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Consider a sample of missing values from the test set:","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"# Actual null values\ndf_test[df_test.isnull().any(axis=1)]","metadata":{"pycharm":{"name":"#%%\n"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We observed 177 (20%) null values for <i>age</i>, 687 (77%) for <i>cabin</i> and 2 (0%) for\n<i>embarked</i> for <i>training data</i> and 86 (21%) null values for <i>age</i>, 327 (78%)\n for <i>cabin</i> and 1 (0%) for <i>fare</i> for <i>testing data</i>.\n\nThe visualised missing values look as follows for the training set:","metadata":{}},{"cell_type":"code","source":"#%matplotlib inline\n_ = plt.figure(figsize=(20, 10))\n\n# cubehelix palette is a part of seaborn that produces a colormap\ncmap = sns.cubehelix_palette(light=1, as_cmap=True, reverse=True)\n_ = sns.heatmap(df_train.isnull(), cmap=cmap)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And as follows for the testing set:","metadata":{}},{"cell_type":"code","source":"#%matplotlib inline\n_ = plt.figure(figsize=(20, 10))\n\n# cubehelix palette is a part of seaborn that produces a colormap\ncmap = sns.cubehelix_palette(light=1, as_cmap=True, reverse=True)\n_ = sns.heatmap(df_test.isnull(), cmap=cmap)\n\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the evidence we have thus far we can conclude the following:\n\n- The age (continuous) and cabin (categorical) variables have a significant number of missing\nvalues.\n- The cabin variable has in excess of 70% missing entries, which is substantive.\n- The missing values for these variables seem to be randomly scattered throughout the data.\nWithout more information it is difficult to tell whether these variables are missing at\nrandom or for some systemic reason. We will do some more analysis to try and ascertain this.\n\nIt is important for us to decide on a missing value replacement strategy for age and cabin.\nThe other variables have an insignficant number of missing values and hence simple\nimputation will be performed.\n\nWe will start by analysing the distributions for age first to get a feel for the data.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"# Continuous density plot\nfig_missing, axes = plt.subplots(1, 1, figsize=(15, 12))\n\n# Plot frequency plot/ histogram\n_ = sns.histplot(x=\"age\", kde=True, data=df_train, ax=axes, bins=40);\n_ = axes.set(xlabel=\"Age\", ylabel='Density');\naxes.xaxis.label.set_size(24)\naxes.yaxis.label.set_size(24)\naxes.tick_params('y', labelsize = 20);\naxes.tick_params('x', labelsize = 20);\n\n## Continuous density plot\n#fig_missing, axes = plt.subplots(1, 1, figsize=(15, 12))\n#\n## Plot frequency plot/ histogram\n#_ = sns.histplot(x=\"age\", kde=True, data=df_test, ax=axes, bins=40);\n#_ = axes.set(xlabel=\"Age\", ylabel='Density');\n#axes.xaxis.label.set_size(24)\n#axes.yaxis.label.set_size(24)\n#axes.tick_params('y', labelsize = 20);\n#axes.tick_params('x', labelsize = 20);\n#","metadata":{"pycharm":{"name":"#%%\n"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Age summary statistics (training set):\\n\")\nprint(df_train['age'].describe())\nprint(\"\\nAge summary statistics (testing set):\\n\")\nprint(df_test['age'].describe())","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The age variable is similarly distributed for both the training and test set i.e. there is a\n skew to the right with a slight bump on the left for lower ages. Maximum age was 80 and\n minimum age was around 2 months.\n\nHigh level analysis does not seem to show any specific problematic values within either set\nof data.\n\nNext we consider whether there is correlation between missing age values and the outcome\nvariable i.e. survival. There are a significant number of missing age values, and if the\nmissing values are correlated with the outcome we should consider a more complex mechanism\nfor missing value substitution such as multiple imputation.","metadata":{}},{"cell_type":"code","source":"fig_age, axes = plt.subplots(nrows=1, ncols=1, figsize=(10, 8), squeeze=False)\nlegend_labels = ['Died', 'Survived']\ncolors = [\"lightslategrey\", \"#1F77B4\"]\n\ndf_train['age_missing'] = df_train.apply(lambda row: 1 if np.isnan(row['age']) else 0, axis=1)\n\nage_missing_stacked = df_train.loc[:, [\"survived\", \"age_missing\"]]\nage_missing_stacked.index.name = \"passenger_num\"\nctable_survival_missing = pd.crosstab(age_missing_stacked.survived, age_missing_stacked\n                         .age_missing, colnames=[\"Missing Age\"], rownames=[\"Survived\"])\nctable_survival_missing_perc = pd.crosstab(age_missing_stacked.survived, age_missing_stacked\n                         .age_missing, colnames=[\"Missing Age\"], rownames=[\"Survived\"],\n                                           normalize=\"index\")\n\n#ctable_survival_missing.columns = [\"Not Missing\", \"Missing\"]\n#ctable_survival_missing[\"Survived\"] = [\"Died\", \"Survived\"]\n#ctable_survival_missing = ctable_survival_missing.set_index(\"Survived\")\n#print(ctable_survival_missing)\n\nctable_survival_missing_perc.columns = [\"Not Missing\", \"Missing\"]\nctable_survival_missing_perc[\"Survived\"] = [\"Died\", \"Survived\"]\nctable_survival_missing_perc = ctable_survival_missing_perc.set_index(\"Survived\")\n\naxs = plt.gca()\naxs.set_frame_on(False)\n#age_miss_stack = ctable_survival_missing.plot.bar(stacked=True, ax=axs, color=colors)\nage_miss_stack = ctable_survival_missing_perc.plot.bar(stacked=True, ax=axs, color=colors)\n_ = plt.xlabel('Survival class', fontsize=20)\n#_ = plt.ylabel('Percentage Missing', fontsize=20)\n_ = plt.xticks([0, 1], legend_labels, fontsize=15, alpha=0.8)\n_ = plt.xticks(fontsize=20)\n#_ = plt.yticks(fontsize=20)\n\n# remove all the ticks (both axes), and tick labels on the Y axis\nplt.tick_params(top=False, bottom=False, left=False, right=False, labelleft=False,\n                labelbottom=True)\n\n# Bit of a hack - TODO, clean up here.\n#totals = ctable_survival_missing.iloc[:,0] + ctable_survival_missing_.iloc[:,1]\ntotals = ctable_survival_missing_perc.iloc[:,0] + ctable_survival_missing_perc.iloc[:,1]\n#tot_arr = totals.to_numpy()\n\nfor i, rec in enumerate(age_miss_stack.patches):\n    height = rec.get_height()\n    j = i//2\n    _ = age_miss_stack.text(rec.get_x() + rec.get_width() / 2,\n              rec.get_y() + height / 2,\n              \"{:.0f}%\".format(height/totals[j]*100),\n              ha='center',\n              va='bottom',\n              color=\"w\",\n              fontsize = 15)\n\nplt.tight_layout()","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We observe that there is a larger proportion of missing values in the group of individuals\nthat died compared to the survival group, which suggests correlation between the\nmissing values and outcome variable (survival).\n\nWe will now perform a Chi-Squared test for independence to determine whether there is\ncorrelation between missing age values and survival.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"survived = df_train.loc[df_train[\"survived\"] == 1,:]\ndied = df_train.loc[df_train[\"survived\"] == 0,:]\n\nsurvived_cont = survived[\"age_missing\"].value_counts(normalize=False)\ndied_cont = died[\"age_missing\"].value_counts(normalize=False)\n\n_ = survived_cont.rename(\"survived\", inplace=True)\n_ = died_cont.rename(\"died\", inplace=True)\n\n#cont_table = pd.concat([survived_cont, died_cont], axis=1)\n\nprint(ctable_survival_missing)\n\n#stat, p, dof, expected = chi2_contingency(cont_table)\nstat, p, dof, expected = chi2_contingency(ctable_survival_missing)\nprint('dof=%d' % dof)\nprint(expected)\n# interpret test-statistic\nprob = 0.95\ncritical = chi2.ppf(prob, dof)\nprint('probability=%.3f, critical=%.3f, stat=%.3f' % (prob, critical, stat))\nif abs(stat) >= critical:\n\tprint('Dependent (reject H0)')\nelse:\n\tprint('Independent (fail to reject H0)')\n# interpret p-value\nalpha = 1.0 - prob\nprint('significance=%.3f, p=%.3f' % (alpha, p))\nif p <= alpha:\n\tprint('Dependent (reject H0)')\nelse:\n\tprint('Independent (fail to reject H0)')\n\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see from the results of the Chi-Squared test that the missing values in the age data\nis strongly correlated with the outcome of survival (p=0.008).\n\nThis suggests strongly that we will have to take care when imputing values for age i.e.\ninaccurate imputation might lead to an adverse effect on model accuracy. It is therefore\nimportant to ensure that missing values are imputed as accurately as possible, especially\ntaking other variables into account i.e. missing values in particular groupings of values\nmight have strong correlation with the response. An advance technique such as multiple\nimputation (MICE) or another predictive model such as K-Nearest Neighbour (KNN) is required\nto ensure age is imputed correctly for these sub-groupings of values.\n\nThis also indicates that we could use the missing values later during feature engineering\nand assess possibility of improved model performance based on additional variables built\nusing the missing value information.\n\nNext we will look at the distribution of missing age values in relation to the other\nvariables to gain more insight into the missing values. We start with the continuous variables\nand therefore we consider the fare variable first. We start with some simple distribution\nplots (histograms).","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"fig_age, axes = plt.subplots(nrows=1, ncols=1, figsize=(20, 12), squeeze=False)\n\nax1 = sns.histplot(data = df_train, x = \"fare\", hue = \"age_missing\", ax=axes[0][0],\n                   legend=True)\n\n_ = plt.xlabel('Fare', fontsize=20)\n_ = plt.ylabel('Count', fontsize=20)\n_ = plt.xticks(fontsize=20)\n_ = plt.yticks(fontsize=20)\n#ax1.legend.set_title(\"Missing Age\")\n\nplt.tight_layout()","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig_age, axes = plt.subplots(nrows=1, ncols=1, figsize=(20, 12), squeeze=False)\n\ndf_test['age_missing'] = df_test.apply(lambda row: 1 if np.isnan(row['age']) else 0, axis=1)\n_ = sns.histplot(data = df_test, x = \"fare\", hue = \"age_missing\", ax=axes[0][0])\n\n_ = plt.xlabel('Fare', fontsize=20)\n_ = plt.ylabel('Count', fontsize=20)\n_ = plt.xticks(fontsize=20)\n_ = plt.yticks(fontsize=20)\nplt.tight_layout()","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is evident by inspection that the number of missing age values is higher for lower fares\nin both the testing and training datasets. We now look at the same data plotted on\nhistograms to further explore the relationship between missing values and fare. We now\nhowever add another dimension in that we also consider survival rate as an additional factor.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"sns.set_style(\"white\")\nax = sns.catplot(x=\"survived\", y=\"fare\", hue=\"age_missing\", kind=\"box\", data=df_train,\n                 height = 5, aspect = 1.5, legend=False)\n_ = ax.set(ylim=(0, 300))\n_ = plt.xlabel('Survived', fontsize=20)\n_ = plt.ylabel('Fare', fontsize=20)\n_ = plt.legend(title='Missing Age')\nplt.show()","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It must be noted that we adapted the y-axis to cut off outlier values above 300. There were\na few values clustered at approximately 500.\n\nThe boxplots shows the same pattern we observed when considering the histograms i.e. the\ndistribution of fare values is different for the missing and non-missing (age) groups. For the\nmissing age group values are concentrated around smaller fare values.\n\nAdditionally and interestingly this plot also shows that when taking survival into account\nthe effect is enlarged i.e. for those who survived and paid higher fares age is typically\nnot missing.\n\nAt this point we could do a correlation test between missing values and fare by means of\nusing a logistic regression. We however leave as a later exercise as we have enough evidence\nto include fare in any imputation method used to impute missing values.\n\nNext we look at the categorical variables. We start with passenger class travelled first.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"sns.set_style(\"white\")\nfig_age, axes = plt.subplots(nrows=1, ncols=1, figsize=(10, 8), squeeze=False)\nlegend_labels = ['Third', 'First', 'Second']\ncolors = [\"lightslategrey\", \"#1F77B4\"]\n\nage_pclass_stacked = df_train.loc[:, [\"p_class\", \"age_missing\"]]\nage_pclass_stacked.index.name = \"passenger_num\"\nctable_pclass_missing = pd.crosstab(age_pclass_stacked.p_class, age_pclass_stacked\n                         .age_missing, colnames=[\"Missing Age\"], rownames=[\"Passenger Class\"])\nctable_pclass_missing_perc = pd.crosstab(age_pclass_stacked.p_class, age_pclass_stacked\n                         .age_missing, colnames=[\"Missing Age\"], rownames=[\"Passenger Class\"],\n                                           normalize=\"index\")\n\n#ctable_pclass_missing.columns = [\"Not Missing\", \"Missing\"]\n#ctable_pclass_missing[\"Passenger Class\"] = [\"First\", \"Second\", \"Third\"]\n#ctable_pclass_missing = ctable_pclass_missing.set_index(\"Passenger Class\")\n#ctable_pclass_missing = ctable_pclass_missing.sort_values(by= 'Missing', axis=0,\n#                                                          ascending=False)\nprint(ctable_pclass_missing)\n\nctable_pclass_missing_perc.columns = [\"Not Missing\", \"Missing\"]\nctable_pclass_missing_perc[\"Passenger Class\"] = [\"First\", \"Second\", \"Third\"]\nctable_pclass_missing_perc = ctable_pclass_missing_perc.set_index(\"Passenger Class\")\nctable_pclass_missing_perc = ctable_pclass_missing_perc.sort_values(by= 'Missing', axis=0,\n                                                          ascending=False)\n\naxs = plt.gca()\naxs.set_frame_on(False)\n\nage_pclass_stack = ctable_pclass_missing_perc.plot.bar(stacked=True, ax=axs, color=colors)\n_ = plt.xlabel('Passenger Class', fontsize=20)\n#_ = plt.ylabel('Percentage Missing', fontsize=20)\n_ = plt.xticks([0, 1, 2], legend_labels, fontsize=15, alpha=0.8)\n_ = plt.xticks(fontsize=20)\n#_ = plt.yticks(fontsize=20)\n\n# remove all the ticks (both axes), and tick labels on the Y axis\nplt.tick_params(top=False, bottom=False, left=False, right=False, labelleft=False,\n                labelbottom=True)\n\n# Bit of a hack - TODO, clean up here.\ntotals = [ctable_pclass_missing_perc.iloc[0,:].sum(), ctable_pclass_missing_perc.iloc[1,:].sum(),\n          pd.Series(ctable_pclass_missing_perc.iloc[2,:]).sum()]\n\nfor i, rec in enumerate(age_pclass_stack.patches):\n    height = rec.get_height()\n    j = i%3\n    bar_height = height/totals[j]*100\n    #print (\"Debug\\n j: {}\\nheight: {}\\ntotals: {}\\n\".format(j, height, totals[j]))\n    _ = age_pclass_stack.text(rec.get_x() + rec.get_width()/2,\n              rec.get_y() + height/2,\n              \"{:.0f}%\".format(bar_height),\n              ha='center',\n              va='bottom',\n              color=\"w\",\n              fontsize = 15)\n\nplt.tight_layout()","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is evident by inspection that the number of missing age values is higher in the third\npassenger class than the other two classes.\n\nNext we perform a Chi-Squared test of correlation.","metadata":{}},{"cell_type":"code","source":"#stat, p, dof, expected = chi2_contingency(cont_table)\nstat, p, dof, expected = chi2_contingency(ctable_pclass_missing)\nprint('dof=%d' % dof)\nprint(expected)\n# interpret test-statistic\nprob = 0.95\ncritical = chi2.ppf(prob, dof)\nprint('probability=%.3f, critical=%.3f, stat=%.3f' % (prob, critical, stat))\nif abs(stat) >= critical:\n\tprint('Dependent (reject H0)')\nelse:\n\tprint('Independent (fail to reject H0)')\n# interpret p-value\nalpha = 1.0 - prob\nprint('significance=%.3f, p=%.3f' % (alpha, p))\nif p <= alpha:\n\tprint('Dependent (reject H0)')\nelse:\n\tprint('Independent (fail to reject H0)')\n\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see from the results of the Chi-Squared test that passenger class is strongly\ncorrelated with a value being missing (p=0.000).\n\nAt this point we can conclude that missing values are correlated with several key variables,\nwhich leads us to conclude that replacing missing values by using predictive model is\nnecessary. We will use KNN imputation as it is frequently used for this type of substitution.\nWe won't use MICE as we will impute one variable only i.e. age. MICE is overkill in this\ninstance as its utility derives from the ability to impute several values concurrently. The\nother missing variables in our dataset we will deal with manually.\n\nWe will impute missing values later in this analysis at the time when we split the datasets\ninto training and testing sets in order to avoid data leakage between training and testing\nsets. For now, we will continue with our EDA.\n\n","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"markdown","source":"The number of null values after missing values have been replaced in the test set is:","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"# Replace missing values for test set\ndf_test = df_test.copy()\nmedian = df_test['age'].median()\ndf_test['age'].fillna(median, inplace=True)\nprint(\"Number of null values in age column: {}\".format(df_test['age'].isnull().sum()))\n\nmedian = df_test['fare'].median()\ndf_test['fare'].fillna(median, inplace=True)\nprint(\"Number of null values in fare column: {}\".format(df_test['fare'].isnull().sum()))\nprint(\"Dataframe dimension: {}\".format(df_test.shape))\ndf_test = df_test.copy()","metadata":{"pycharm":{"name":"#%%\n"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We observe that the null values have been removed. We now have a dataset ready for further analysis -\nalbeit a bit of a black box hack :) We will now do some very limited EDA just to get a feel for the data\nas previously discussed.","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">\n<b>Exploration of data</b>\n</div>","metadata":{}},{"cell_type":"markdown","source":"We start by looking at the number of unique records per variable.","metadata":{}},{"cell_type":"code","source":"print(df_train.nunique())","metadata":{"pycharm":{"name":"#%%\n"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are no columns with only one value. We therefore retain all columns for ML purposes as there is\nenough variability to warrant using the data. There are many variables with fewer than 10 levels which\ncould be considered as categorical. Based on our initial assessment of the data we will work with\nlevels of measurement for the data as follows:\n\n- p_class (ordinal) - we will revisit type of encoding later\n- sex (binary) - recode (female - yes or no)\n- age (continuous)\n- sib_sp (ordinal) - check correlation - revisit encoding\n- parch (ordinal) - check correlation - revisit encoding\n- fare (continuous)\n- embarked (nominal) - recode (one hot encode) - probably categorical\n\nWe start by separating continuous and categorical variables for further high level analysis.","metadata":{}},{"cell_type":"code","source":"# We will use datasets for this analysis as follows:\n# df_train, df_test:                Original data, supplemented with extra fields e.g.\n#                                   missing values.\n# df_train_con*, df_train_cat*:     Datasets used for plotting continuous and categorical\n#                                   variables.\n# df_train_trans*, df_test_trans*:  Transformed data, as result of enrichment/ wrangling/\n#                                   feature engineering.\n# X, y, X_train, X_test:            Training and testing sets.\n\n# Separate continuous and categorical variables\nnames_con = ('fare', 'age')\nnames_con_plot = ('survived', 'fare', 'age')\nnames_cat = ('survived', 'p_class', 'sex', 'sib_sp', 'parch', 'embarked')\nnames_cat_test = ('p_class', 'sex', 'sib_sp', 'parch', 'embarked')\nnames_all_orig = ('survived', 'p_class', 'sex', 'sib_sp', 'parch', 'embarked', 'fare', 'age')\n\ndf_train_con = df_train.loc[:, names_con]\ndf_train_con_plot = df_train.loc[:, names_con_plot]\ndf_train_cat = df_train.loc[:, names_cat]\n\ndf_test_con = df_test.loc[:, names_con]\ndf_test_cat = df_test.loc[:, names_cat_test]\n\n# Plotting label dictionary\nplot_con = [('fare', 'Fare'),\n            ('age', 'Age')]\nplot_con_plot = [('survived', 'Survived'),\n            ('fare', 'Fare'),\n            ('age', 'Age')]\nplot_cat = {'survived': ['Died', 'Survived'],\n            'p_class': ['3rd', '1st', '2nd'],\n            'sex': ['Male', 'Female'],\n            'sib_sp': ['0', '1', '2', '4', '3', '8', '5'],\n            'parch': ['0', '1', '2', '3', '5', '4', '6'],\n            'embarked': ['Southampton', 'Cherbourg', 'Queenstown']}\nplot_cat_plot = {'survived': 'Survival Rate',\n            'p_class': 'Passenger Class Travelled',\n            'sex': 'Gender',\n            'sib_sp': '# Siblings or spouses',\n            'parch': '# Parents or children',\n            'embarked': 'Port of Embarkation'}","metadata":{"pycharm":{"name":"#%%\n"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### High level overview\n","metadata":{}},{"cell_type":"markdown","source":"We observe that we have two candidates for continuous variables here (age and fare). With all the\ncategorical variables present, it is likely that a tree model would be better suited to this problem\nunless significant feature engineering on categorical features is performed to ensure features are\noptimally encoded, transformed and scaled for a linear model or neural network.\n\nLet's continue with the high level analysis.\n\nThe overall survival rate was as follows (based on the training dataset):","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"_ = plt.figure()\n\n# Plot outcome counts.a\noutcome_counts = df_train_cat['survived'].value_counts(normalize = True)\nlegend_labels = ['Died', 'Survived']\n\n# change the background bar colors to be light grey\nbars = plt.bar(outcome_counts.index, outcome_counts.values, align='center', linewidth=0,\n               color='lightslategrey')\n# make one bar, the survived bar, a contrasting color\nbars[1].set_color('#1F77B4')\n\n# soften all labels by turning grey\n_ = plt.xticks(outcome_counts.index, legend_labels, fontsize=15, alpha=0.8)\n_ = plt.title('Survival Rate', fontsize=15, pad=30, alpha=0.8)\n\n# remove all the ticks (both axes), and tick labels on the Y axis\nplt.tick_params(top=False, bottom=False, left=False, right=False, labelleft=False,\n                labelbottom=True)\n\n# Remove the frame - my method\nax = plt.gca()\nax.set_frame_on(False)\n\n# Remove the frame of the chart - instructor's method\n#for spine in plt.gca().spines.values():\n#    spine.set_visible(False)\n\n# direct label each bar with Y axis values\nfor bar in bars:\n    _ = plt.gca().text(bar.get_x() + bar.get_width()/2, bar.get_height() - 0.05,\n                       str(round((bar.get_height()*100))) + '%', ha='center', color='w',\n                       fontsize=15)\n\nplt.show()","metadata":{"pycharm":{"name":"#%%\n"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The survival statistics are as follows:\n","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"print(df_train_cat['survived'].value_counts())\nprint(\"\\n\")\n\n#print(df_train_cat['survived'].value_counts(normalize = True).mul(100).round(1).astype(str) + '%')\n#print(\"\\n\")\n#","metadata":{"pycharm":{"name":"#%%\n"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We observe that 38% of passengers survived and 62% died. These statistics correspond with the narrative on survival\nrate quoted in the background information on Kaggle. There it is quoted that around 32% survived and 68% died. The\nsample we are working with is thus representative of the overall population, which is important to note.\n\nWe observe that the target variable contains unbalanced classes. We need to consider revisiting the unbalanced\nclasses at a later stage - depending on the accuracy of our models. For now, we will forge ahead.\n\nNext we will consider class level counts for categorical variables.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"markdown","source":"#### Categorical variable overview\n\nClass percentages:","metadata":{}},{"cell_type":"code","source":"# Bar chart plot of categorical variables.\nfig, ax = plt.subplots(2, 3, figsize=(20, 15));\nbase_color = '#1F77B4'\nfor variable, subplot in zip(names_cat, ax.flatten()):\n    subplot.xaxis.label.set_size(24)\n    subplot.yaxis.label.set_size(24)\n    subplot.tick_params('y', labelsize = 20);\n    subplot.tick_params('x', labelsize = 20);\n    subplot.tick_params(top=False, bottom=False, left=False, right=False, labelleft=False,\n                        labelbottom=True)\n    subplot.set_frame_on(False)\n\n    outcome_counts = df_train_cat[variable].value_counts(normalize=True)\n    bars = subplot.bar(outcome_counts.index.sort_values(), outcome_counts.values, align='center',\n                       linewidth=0,\n               color='lightslategrey')\n    # make one bar, the highest value bar, a contrasting color\n    bars[0].set_color('#1F77B4')\n\n    plt.sca(subplot)\n    _ = plt.xticks(outcome_counts.index.sort_values(), plot_cat[variable], fontsize=15, alpha=0.8)\n    _ = plt.title(plot_cat_plot[variable], fontsize=15, pad=30, alpha=0.8)\n\n    # direct label each bar with Y axis values\n    for bar in bars:\n        _ = plt.gca().text(bar.get_x() + bar.get_width()/2, bar.get_height() - 0.05,\n                           str(round((bar.get_height()*100))) + '%', ha='center', color='w',\n                           fontsize=15)\n\n    plt.tight_layout()\n","metadata":{"pycharm":{"name":"#%%\n"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Class counts:","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"# Class level counts for categorical variables.\nfor variable in names_cat:\n    #print(df_train_cat[variable].value_counts(normalize = True).mul(100).round(1).astype(str) +\n    # '%')\n    print(df_train_cat[variable].value_counts())\n    print(\"\\n\")","metadata":{"pycharm":{"name":"#%%\n"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the categorical variables we observe that there were approximately twice as many passengers in class 3 than either\nclass 1 or 2. We also observe that there were nearly twice as many males as females on the Titanic. We also observe\nthat more than two thirds of passengers did not have any siblings on board. Likewise we observe that more than two\nthirds did not have a father or child on board.\n\nIt is therefore fair to say that the majority of passengers were either couples or single travellers without children\n. In the case where families did travel, the majority of families had one or two children. Very few families with\nmore children were on board the Titanic.\n\nMany of these variables could contribute to correlation with survival at face value e.g. it\nstands to reason that preference would have been given in lifeboats to women and children,\nand that more affluent travellers would have had access to better lifeboats. We will however\n test these hypotheses in this analysis.\n\nWe also see that more than two thirds of passengers departed from Southampton. The relative\ndistribution between the different ports can be observed from the following heatmap.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"count_towns = df_train_cat.groupby(\n    pd.Grouper(key='embarked')).size().reset_index(name='count')\n\nlatitude_embark = ['50.897', '49.6423', ' 51.84914']\nlongitude_embark = ['-1.404', '-1.62551', '-8.2975265']\n\ncount_towns['latitude_embark'] = latitude_embark\ncount_towns['longitude_embark'] = longitude_embark\n\nm = folium.Map([49.922935, -6.068136], zoom_start=6, width='%100', height='%100')\n\nheat_data = count_towns.groupby([\"latitude_embark\", \"longitude_embark\"])['count'].mean().reset_index().values.tolist()\n_ = folium.plugins.HeatMap(heat_data).add_to(m)\nm","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Continuous variable overview\n\nWe start by considering the age distribution of the passengers. At this point we do some limited\nEDA in that we will consider the age profiles of those who survived vs. those who died. As with\nmany natural phenomena we expect age to have some influence on mortality.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"fig_age, axes = plt.subplots(nrows=1, ncols=1, figsize=(10, 6), squeeze=False)\n\n_ = sns.kdeplot(data=df_train_con_plot.loc[(df_train_con_plot['survived'] == 0), 'age'], shade =\nTrue, label = 'Died')\n_ = sns.kdeplot(data=df_train_con_plot.loc[(df_train_con_plot['survived'] == 1), 'age'], shade =\nTrue, label = 'Survived')\n_ = plt.xlabel('Age', fontsize=20)\n_ = plt.ylabel('Density', fontsize=20)\n_ = plt.xticks(fontsize=20)\n_ = plt.yticks(fontsize=20)\n_ = plt.legend(fontsize=15)\nplt.show()\n","metadata":{"pycharm":{"name":"#%%\n"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the age distribution plot we can see that more children under the age of 15 survived than\ndied in the incident. We can see that more individuals between the ages of 20 and 40 died than\nsurvived. We can also see that more individuals above the age of 80 survived compared to dying.\n\nWe can also see that the majority of individuals on the cruise were between the ages of 20 to 40.\nThere were fewer teenagers compared to children under 10. There were comparatively fewer elderly\npeople on board i.e. above 60.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"# 5 number summary.\ndf_train_con.describe()\n","metadata":{"pycharm":{"name":"#%%\n"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We now consider the fare distribution too. As with age, our assumption would be that fare has a\ncorrelation with mortality too.\n\nThe fare distribution is severely skewed to the right. The kurtosis of the plot is very high with\n most values clustered closely around the median value of 14. There was a non-significant but\n relatively smaller number of fares spread between teh values of 30 and 500.\n\nThe age distribution was as previously discussed, with a minimum of 6 months and maximum of 80\nyears old. The distribution is fairly symmetrical with a slight skew to the right. There is a\nyoung child bump to the left of the distribution.\n\nWe now do a more in depth visual analysis of the correlation between survival and fare and age\nrespectively. We do this by creating a set of Violin plots.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"# Continuous density plot\nfig_continuous, axes = plt.subplots(nrows=len(names_con_plot)-1, ncols=2, figsize=(15, 12))\n\n# Plot frequency plot/ histogram\n_ = sns.histplot(x=plot_con[0][0], kde=True, data=df_train_con_plot, ax=axes[0][0], bins=40);\n_ = axes[0][0].set(xlabel=plot_con[0][1], ylabel='Density');\naxes[0][0].xaxis.label.set_size(24)\naxes[0][0].yaxis.label.set_size(24)\naxes[0][0].tick_params('y', labelsize = 20);\naxes[0][0].tick_params('x', labelsize = 20);\n\n# Plot violin plot\n_ = sns.violinplot(x='survived', y=plot_con[0][0], data=df_train_con_plot, ax=axes[0][1]);\n_ = axes[0][1].set(xlabel='', ylabel=plot_con[0][1]);\naxes[0][1].xaxis.label.set_size(24)\naxes[0][1].yaxis.label.set_size(24)\naxes[0][1].tick_params('y', labelsize = 20);\naxes[0][1].tick_params('x', labelsize = 20);\n_ = axes[0][1].set_xticklabels(['Died', 'Survived'])\n\n# Plot frequency plot/ histogram\n_ = sns.histplot(x=plot_con[1][0], kde=True, data=df_train_con_plot, ax=axes[1][0], bins=40);\n_ = axes[1][0].set(xlabel=plot_con[1][1], ylabel='Density');\naxes[1][0].xaxis.label.set_size(24)\naxes[1][0].yaxis.label.set_size(24)\naxes[1][0].tick_params('y', labelsize = 20);\naxes[1][0].tick_params('x', labelsize = 20);\n\n# Plot violin plot\n_ = sns.violinplot(x='survived', y=plot_con[1][0], data=df_train_con_plot, ax=axes[1][1]);\n_ = axes[1][1].set(ylabel=plot_con[1][1], xlabel='');\naxes[1][1].xaxis.label.set_size(24)\naxes[1][1].yaxis.label.set_size(24)\naxes[1][1].tick_params('y', labelsize = 20);\naxes[1][1].tick_params('x', labelsize = 20);\n_ = axes[1][1].set_xticklabels(['Died', 'Survived'])\n\nplt.tight_layout()\n\n","metadata":{"pycharm":{"name":"#%%\n"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The violin plot for <i>fare</i> indicates that there is correlation between fare and survival as\nmore people paying a low fare died and chances of survival increased for higher fares, as well as\n lower fares close to zero (possibly for children travelling at very low cost).\n\nThe plot for <i>age</i> indicates a similar pattern with higher survival for children below 10\nand higher mortality between ages of 20 and 40. The relative likelihood of survival increases\nagain around 40 years of age as you go into the older ages.\n\nWe will investigate these observations in more detail in our next Notebook.\n\nLastly we will look at the Box plots for both age and fare. We do this to get a better feel for\nthe spread of the data and the possibility of outliers. This will be important for us to consider\n whether further analysis and possible processing of variables are required in later stages.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"# Boxplot of continuous variables\nmedianprops = {'color': 'magenta', 'linewidth': 2}\nboxprops = {'color': 'black', 'linestyle': '-', 'linewidth': 2}\nwhiskerprops = {'color': 'black', 'linestyle': '-', 'linewidth': 2}\ncapprops = {'color': 'black', 'linestyle': '-', 'linewidth': 2}\nflierprops = {'color': 'black', 'marker': 'x', 'markersize': 20}\n\n_ = df_train_con.plot(kind='box', subplots=True, figsize=(20, 8), layout=(1,2), fontsize = 20,\n                      medianprops=medianprops, boxprops=boxprops, whiskerprops=whiskerprops,\n                      capprops=capprops, flierprops=flierprops);\n_ = plt.tight_layout();\n_ = plt.show();\n","metadata":{"pycharm":{"name":"#%%\n"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The distributions of the <i>fare</i> and <i>age</i> variables show that fare is skewed heavily to\n the right, with the median skewed to the left of the distribution as expected. The values in the\n  final quintile are spread over wide area with quite a few outliers. This distribution is heavy\n  tailed, as can be expected of many financial distributions. We observe that there are zero\n  values, and there is a large peak in the bin containing zero. We will need to investigate this\n  group of travellers as they could be different from the general population. The heavy tail and\n  many outliers are also good candidates for further processing.\n\nThe <i>age</i> distribution is fairly symmetrical, with a few outliers to the right, but nothing\nout of the ordinary. Most of the values are bundled symmetrically around the median of 28, which\nis quite a young age for the average traveller. The large spike in ages at this interval is also\nconcerning and seems out of place. This will need to be further investigated.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"markdown","source":"### Correlation Analysis","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"markdown","source":"At this point we have learnt the following about the data:\n\n- Ascertained best missing value strategy based on data characteristics:\n    - Use KNN imputation for age variable due to strong correlation between missing age\n    variables and outcome variable.\n    - Use median and mode substitution for remaining missing values, as there are few\n    missing values and more complicated methods are hence not warranted.\n    - Missing values in cabin have to be assessed given a transformation to the variable\n    which we must still decide on.\n- Analysis to date (Decision Tree feature strength) and the fact that a gender only model has\n an accuracy of 77% has shown that gender is very strongly correlated with survival rate.\n- There are many other data elements correlated with survival rate as well as with each other.\n- The majority of variables are categorical.\n- The best performing models are those that have been adjusted for overfitting e.g. MLP\nmodels with regularisation parameters. Given the small dataset, many correlated variables\nand overpowering effect of a few strongly correlated variables the models created to date\nare very prone to overfitting.\n\nThe next step in our analysis is to better understand correlation with the response as well\nas between feature variables. This analysis will enable to decide how to best engineer\nbetter features that capture the correlation without overfitting. The problem we face is\nthat the strong correlation between gender and survival is in effect eclipsing all other\ncorrelations and hence reducing their effect. We need to find a way to combine variables to\ncreate a more balanced representation of the correlations.\n\nAs we have a binary response variable, Pearson's Correlation will be of no use here as it\nassumes a normal distribution and Homoscedasticity which is clearly not the case (I have\nnoticed that many of the Titanic analyses here on Kaggle use Pearson's correlation heatmap\nas implemented by the Pandas <i>corr()</i> function, which has no value or relevance in this\nsetting).\n\nWe will perform correlation analysis as follows:\n\n- Univariate Logistic Regression to measure correlation between categorical response and\ncontinuous feature variables (age, fare)\n- Chi-Squared test for association for correlation between categorical response and\ncategorical feature variables (ticket class, ticket number, # siblings, # parents, fare, cabin\nnumber, port of embarkation).\n\nWe start by first performing some transformations on our dataset to get the data into a\nformat enabling further analysis e.g. encode categorical variables for numeric analysis.\n\nWe start by considering the distribution of values for the \"deck\" variable we create from\nthe original \"cabin\" variable by taking the first character from the \"cabin\" field.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"# We will now transform some variables by grouping categories together based on our EDA\n# analysis. We also encode categorical variables to numeric values in order to do the ML\n# analysis.\n# These transformations would not result in data leakage, and can hence be done before we\n# split the data into training and testing sets.\n\n# Make a copy of original dataset before imputation - we need the original for further\n# analysis.\ndf_train_trans = df_train.copy()\ndf_test_trans = df_test.copy()\n\n# Creating Deck field from the first letter of the cabin field (we create a new category for\n# missing, which is called M). As this is a categorical variable we will leave the missing\n# value field as is.\ndf_train_trans['deck'] = df_train_trans['cabin'].apply(lambda s: s[0] if pd.notnull(s) else 'M')\ndf_test_trans['deck'] = df_test_trans['cabin'].apply(lambda s: s[0] if pd.notnull(s) else 'M')","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_trans['deck'].value_counts()\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We observe that most of the values are missing. Seeing as this is a categorical variable\ncreating a \"missing category might be insightful\". Next we consider the proportion of\nsurvivors within each category.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"_ = plt.title('Survival rate by Deck')\nax = sns.barplot(x='deck', y='survived', data=df_train_trans).set_ylabel ('Survival Rate')\n_ = plt.xlabel('Deck')\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are many values in the smaller deck categories, which we can collapse into other decks\n based on the proportion of survivors in each category.\nThe missing values do have a significantly lower survival rate and will hence be very useful\n information for the model to use.\nThere is only one passenger on deck T, and the test set has no values for deck T. The closest\n category is deck 'A' (based on Googling the deck placement on the ship), so we change the\n single occurrence of T to category A.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"# There is only one passenger on deck T and the test set has no values for deck T.\n# The closest category is deck 'A' (checking on deck arrangements image found via Google), so\n# we change all occurrences of T to A.\nidx = df_train_trans[df_train_trans['deck'] == 'T'].index\ndf_train_trans.loc[idx, 'deck'] = 'A'\n\n## Some of the classes have very few values, we group adjacent classes together.\ndf_train_trans['deck'] = df_train_trans['deck'].replace(['A', 'B', 'C'], 'ABC')\ndf_train_trans['deck'] = df_train_trans['deck'].replace(['D', 'E'], 'DE')\ndf_train_trans['deck'] = df_train_trans['deck'].replace(['F', 'G'], 'FG')\n\ndf_test_trans['deck'] = df_test_trans['deck'].replace(['A', 'B', 'C'], 'ABC')\ndf_test_trans['deck'] = df_test_trans['deck'].replace(['D', 'E'], 'DE')\ndf_test_trans['deck'] = df_test_trans['deck'].replace(['F', 'G'], 'FG')","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_trans['deck'].value_counts()\ndf_train_trans.groupby('deck').survived.mean()","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df_test_trans['deck'].value_counts()\n\n##%% md\n\n#The values for the training and test sets look aligned. We don't have survival data for the\n#test set (obviously!) so cannot check on this distribution. We can however check form the\n#training set.","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_ = plt.title('Survival rate by Deck')\nax = sns.barplot(x='deck', y='survived', data=df_train_trans).set_ylabel ('Survival Rate')\n_ = plt.xlabel('Deck')\n\n\n##%% md\n\n#Not a great split as most of the values are missing, but not bad considering the information\n# we are working with.\n#\n#Next we consider titles extracted from the \"name\" field.","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Next we extract the title variable from the name field\ndf_train_trans['title'] = df_train_trans['name'].str.split(', ', expand=True)[1].str.split('.',\n                                                                            expand=True)[0]\n# Next we extract the title variable from the name field\ndf_test_trans['title'] = df_test_trans['name'].str.split(', ', expand=True)[1].str.split('.',\n                                                                            expand=True)[0]","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_trans['title'].value_counts()\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We observe many values with few occurrences. We will group these into the larger four\ncategories.","metadata":{}},{"cell_type":"code","source":"df_train_trans['is_married'] = 0\ndf_train_trans['is_married'].loc[df_train_trans['title'] == 'Mrs'] = 1\n\ndf_test_trans['is_married'] = 0\ndf_test_trans['is_married'].loc[df_test_trans['title'] == 'Mrs'] = 1","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_trans['title'].replace(['Mme', 'Ms', 'Lady', 'Mlle', 'the Countess', 'Dona'], 'Miss', inplace=True)\ndf_test_trans['title'].replace(['Mme', 'Ms', 'Lady', 'Mlle', 'the Countess', 'Dona'], 'Miss', inplace=True)\n\ndf_train_trans['title'].replace(['Major', 'Col', 'Capt', 'Don', 'Sir', 'Jonkheer', 'Rev',\n                                 'Dr'], 'Mr', inplace=True)\ndf_test_trans['title'].replace(['Major', 'Col', 'Capt', 'Don', 'Sir', 'Jonkheer', 'Rev',\n                                'Dr'], 'Mr', inplace=True)\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_trans['title'].value_counts()\ndf_train_trans.groupby('title').survived.mean()","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_ = plt.title('Survival rate by Title')\nax = sns.barplot(x='title', y='survived', data=df_train_trans).set_ylabel ('Survival Rate')\n_ = plt.xlabel('Title')","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we get a very nice split with a large proportion of values at a considerably lower\nsurvival rate.\n\nWe now encode the gender/ sex value into numeric values.\n\n","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"df_train_trans['sex'].value_counts()\n# Transform sex variable - don't need one hot encoding as variable is binary\ndf_train_trans['sex'] = df_train_trans['sex'].apply(lambda x: 1 if x == 'female' else 0)\n# Same transformation for test set - don't need one hot encoding as variable is binary\ndf_test_trans['sex'] = df_test_trans['sex'].apply(lambda x: 1 if x == 'female' else 0)\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_trans['sex'].value_counts()\ndf_train_trans.groupby('sex').survived.mean()","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"legend_labels = ['Male', 'Female']\n\n_ = plt.title('Survival rate by Gender')\nax = sns.barplot(x='sex', y='survived', data=df_train_trans).set_ylabel ('Survival Rate')\n_ = plt.xlabel('Gender')\n_ = plt.xticks([0, 1], legend_labels)\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We already know that sex is a very strong indicator of survival.\n\nWe now impute the missing values in the embarked field and create a family size feature.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"# Replace embarked with mode training set - no values missing in test set, so not required\n# to further impute. Some leakage takes place here, but only one value so not important -\n# TODO: fix this, just as a matter of principle.\ntrain_emb_mode = df_train_trans['embarked'].mode()\ndf_train_trans['embarked'].fillna(train_emb_mode.iloc[0], inplace=True)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create family size feature\ndf_train_trans['fam_num'] = df_train_trans['sib_sp'] + df_train_trans['parch'] + 1\ndf_test_trans['fam_num'] = df_test_trans['sib_sp'] + df_test_trans['parch'] + 1\n\n# Create family size groupings\ndf_train_trans['fam_size'] = pd.cut(df_train_trans.fam_num, [0,1,4,7,11], labels=['single',\n                           'small', 'large', 'very_large'])\ndf_test_trans['fam_size'] = pd.cut(df_test_trans.fam_num, [0,1,4,7,11], labels=['single',\n                           'small', 'large', 'very_large'])\n\n# Now we One Hot Encode Categorical variables. We leave the dimension variables for now, as\n# we might generate some cross terms later. We don't One Hot Encode variables with missing\n# values e.g. age, as we will impute these during training, and will One Hot Encode at that\n# stage.\n\n# Transform embarked and deck variables for training set\ncategorical_cols = ['embarked', 'deck', 'title', 'p_class', 'fam_size']\ndf_train_trans['dim_embarked'] = df_train_trans['embarked']\ndf_train_trans['dim_deck'] = df_train_trans['deck']\ndf_train_trans['dim_title'] = df_train_trans['title']\ndf_train_trans['dim_p_class'] = df_train_trans['p_class']\ndf_train_trans['dim_fam_size'] = df_train_trans['fam_size']\ndf_train_trans = pd.get_dummies(df_train_trans, columns = categorical_cols, drop_first=True)\n\n# Transform embarked and deck variables for test set\ndf_test_trans['dim_embarked'] = df_test_trans['embarked']\ndf_test_trans['dim_deck'] = df_test_trans['deck']\ndf_test_trans['dim_title'] = df_test_trans['title']\ndf_test_trans['dim_p_class'] = df_test_trans['p_class']\ndf_test_trans['dim_fam_size'] = df_test_trans['fam_size']\ndf_test_trans = pd.get_dummies(df_test_trans, columns = categorical_cols, drop_first=True)\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_trans['dim_fam_size'].value_counts()\ndf_train_trans.groupby('dim_fam_size').survived.mean()\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"legend_labels = ['Single', 'Small', 'Large']\n_ = plt.title('Survival rate by Family size')\nax = sns.barplot(x='dim_fam_size', y='survived', data=df_train_trans).set_ylabel ('Survival '\n                                                                                'Rate')\n_ = plt.xlabel('Family Size')\n_ = plt.xticks([0, 1, 2], legend_labels)\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Fairly good split on family size too. Clearly mid-sized families had a higher survival rate.\n\nNext we consider what information we can extract from the \"ticket\" field. We group tickets\nby name and see if the counts yield any information.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"df_train_trans['ticket'].value_counts()\ndf_train_trans.groupby('ticket').survived.mean()","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We observe that some ticket numbers have duplicates. We also observe that ticket numbers\ntypically either survived or some members perished, suggesting that tickets were sold in\nbatches to families/ groups.\nWe therefore group ticket numbers by frequency and see if there is any value in this grouping.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"# Now we create a variable\ndf_train_trans['ticket_freq'] = df_train_trans.groupby('ticket')['ticket'].transform('count')\ndf_test_trans['ticket_freq'] = df_test_trans.groupby('ticket')['ticket'].transform('count')","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us see what the survival distributions for ticket groups of different sizes are.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"fig, axs = plt.subplots(figsize=(12, 9))\n_ = sns.countplot(x='ticket_freq', hue='survived', data=df_train_trans)\n\n_ = plt.xlabel('Ticket Frequency', size=15, labelpad=20)\n_ = plt.ylabel('Passenger Count', size=15, labelpad=20)\n_ = plt.tick_params(axis='x', labelsize=15)\n_ = plt.tick_params(axis='y', labelsize=15)\n\n_ = plt.legend(['Died', 'Survived'], loc='upper right', prop={'size': 15})\n_ = plt.title('Count of Grouped Tickets', size=15, y=1.05)\n\nplt.show()\n#df_train_trans.head()\n\n##%% md\n\n#We clearly observe that tickets bought in isolation had a much higher death rate. We\n#therefore group the variable as a binary indicator variable.\n#\n#There is not much variation in the lower frequencies, so we group them all together in one\n#category.","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_trans['ticket_freq'] = df_train_trans['ticket_freq'].apply(lambda x: 1 if x == 1\nelse 0)\n# Same transformation for test set - don't need one hot encoding as variable is binary\ndf_test_trans['ticket_freq'] = df_test_trans['ticket_freq'].apply(lambda x: 1 if x == 1\nelse 0)\n\ndf_train_trans['ticket_freq'].value_counts()","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(figsize=(12, 9))\n_ = sns.countplot(x='ticket_freq', hue='survived', data=df_train_trans)\n\n_ = plt.xlabel('Ticket Frequency', size=15, labelpad=20)\n_ = plt.ylabel('Passenger Count', size=15, labelpad=20)\n_ = plt.tick_params(axis='x', labelsize=15)\n_ = plt.tick_params(axis='y', labelsize=15)\n\n_ = plt.legend(['Died', 'Survived'], loc='upper right', prop={'size': 15})\n_ = plt.title('Count of Grouped Tickets', size=15, y=1.05)\n\nplt.show()","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This clearly looks better.\n","metadata":{}},{"cell_type":"code","source":"names_all = list(df_train_trans.columns)\nprint(names_all)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Update dataframe fieldname values\ndrop_cols = ['name', 'sib_sp', 'parch', 'ticket',\n       'cabin', 'age_missing', 'fam_num', 'dim_embarked', 'dim_deck',\n       'dim_title', 'dim_p_class', 'dim_fam_size']\n\n# These stay static\nnames_con = ('fare', 'age')\nnames_con_plot = ('survived', 'fare', 'age')\n\n# These change depending on prior analyses\nnames_cat = names_all.copy()\nfor x in drop_cols:\n    names_cat.remove(x)\nfor x in ['survived', 'age', 'fare']:\n    names_cat.remove(x)\n\nprint(\"names_cat: {}\".format(names_cat))\n\nnames_cat_plot = names_all.copy()\nfor x in drop_cols:\n    names_cat_plot.remove(x)\nfor x in ['age', 'fare']:\n    names_cat_plot.remove(x)\n\nfor x in drop_cols:\n    names_all.remove(x)\nfor x in ['survived']:\n    names_all.remove(x)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next we run a uni-variate Logistic regression for each categorical variable on its own and\nrecord the accuracy scores obtained.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"X_train = df_train_trans.loc[:, names_cat]\ny_train = df_train_trans.loc[:, \"survived\"]\n\nlogval = LogisticRegression(fit_intercept = False)\n\nefs1 = EFS(logval,\n           min_features=1,\n           max_features=1,\n           scoring='accuracy',\n           print_progress=True,\n           cv=5)\n\nefs1 = efs1.fit(X_train, y_train, custom_feature_names=names_cat)\n\nprint('Best accuracy score: %.2f' % efs1.best_score_)\nprint('Best subset (indices):', efs1.best_idx_)\nprint('Best subset (corresponding names):', efs1.best_feature_names_)\n\n#efs1 = efs1.fit(X, y, custom_feature_names=feature_names)\n\ndf_efs = pd.DataFrame.from_dict(efs1.get_metric_dict()).T\ndf_efs.sort_values('avg_score', inplace=True, ascending=False)\n\nmetric_dict = efs1.get_metric_dict()\n\nfig = plt.figure()\nk_feat = sorted(metric_dict.keys())\navg = [metric_dict[k]['avg_score'] for k in k_feat]\n\nupper, lower = [], []\nfor k in k_feat:\n    upper.append(metric_dict[k]['avg_score'] +\n                 metric_dict[k]['std_dev'])\n    lower.append(metric_dict[k]['avg_score'] -\n                 metric_dict[k]['std_dev'])\n\nplt.fill_between(k_feat,\n                 upper,\n                 lower,\n                 alpha=0.2,\n                 color='blue',\n                 lw=1)\n\n_ = plt.plot(k_feat, avg, color='blue', marker='o');\n_ = plt.ylabel('Accuracy +/- Standard Deviation', size = 15)\n_ = plt.xlabel('Feature', size = 15)\nfeature_min = len(metric_dict[k_feat[0]]['feature_idx'])\nfeature_max = len(metric_dict[k_feat[-1]]['feature_idx'])\n_ = plt.xticks(k_feat,\n           [str(metric_dict[k]['feature_names']) for k in k_feat],\n           rotation=90, size = 15)\n_ = plt.yticks(size = 15)\nplt.show();\n\ndf_efs\n\n#eda_all = pd.concat([df_train, df_test], sort=True).reset_index(drop=True)\n\n#sns.pairplot(df_train, vars=names_all, hue='survived', plot_kws = {'alpha': 0.6, 's': 40,\n# 'edgecolor': 'k'}, palette=sns.color_palette(\"hls\", 2), size=8)\n\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We already knew that gender was going to be the winner here. Again it is useful to reflect\non just how strongly this variable is correlated with the outcome. Gender by itself\nobtaining an accuracy of 79% is noteworthy.\n\nOur newly generated title variable does not seem to be adding much to the party here, it\ncomes in second but is likely to contain the same information as gender. We can probably\ndrop this variable, or alternatively attempt to improve gender with information contained in\n the title variable.\n\nAll the other variables show strong correlation with the outcome variable. We will have\n to carefully consider variable interactions to find combinations of some variables\n that avoid multi-collinearity and hence maximises the signal in the data.\n\nNext we will consider correlation between the continuous variables and the response. We have\ntwo continuous variables, <i>age</i> and <i>fare</i>.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"X_train = df_train_trans.loc[:, names_con]\ny_train = df_train_trans.loc[:, \"survived\"]\n\n# Replace missing values for continuous variables - else we cannot compute correlation\n# statistics.\n\n# Replace fare in test set with median from train set - to prevent data leakage.\nmedian_fare = X_train['fare'].median()\nX_train['fare'].fillna(median_fare, inplace=True)\n\n# Replace missing values for training set\n#print(\"Number of null values in age column: {}\".format(X_train['age'].isnull().sum()))\n\n# Define imputer\nimputer = KNNImputer()\n# fit on the dataset\n_ = imputer.fit(X_train)\n# transform the dataset\nX_train_array = imputer.transform(X_train)\nX_train = pd.DataFrame(X_train_array, columns=names_con)\n# summarize total missing\n#print('Missing: %d' % sum(isnan(X_train).flatten()))\n\n# Feature extraction set to retain all - we want to see scores for all variables.\ntest = SelectKBest(score_func=f_classif, k='all')\nfit_kbest = test.fit(X_train, y_train)\nfeatures_kbest = np.array(names_con)\nplot_feature_importance(fit = fit_kbest, features = features_kbest)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is clear from the output of the Chi-Squared test that fare has a higher correlation with\nsurvival rate than age.\n\nIn order to see how feature importances amongst all the variables stack up (categorical and\ncontinuous) we will now analyse correlation and feature importance by considering the output\n of a Random Forest model which deals with categorical and continuous variables in the same\n manner.\n","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"X_train = df_train_trans.loc[:, names_all]\ny_train = df_train_trans.loc[:, \"survived\"]\n\n# Replace fare in test set with median from train set - to prevent data leakage.\nmedian_fare = X_train['fare'].median()\nX_train['fare'].fillna(median_fare, inplace=True)\n\n# Replace missing values for training set\n#print(\"Number of null values in age column: {}\".format(X_train['age'].isnull().sum()))\n\n# Define imputer\nimputer = KNNImputer()\n# fit on the dataset\n_ = imputer.fit(X_train)\n# transform the dataset\nX_train_array = imputer.transform(X_train)\nX_train = pd.DataFrame(X_train_array, columns=names_all)\n# summarize total missing\n#print('Missing: %d' % sum(isnan(X_train).flatten()))\n\nrand_forest = RandomForestClassifier(max_features=0.25, n_estimators=1000, criterion= 'gini',\n                                     random_state=0)\nrand_forest.fit(X_train, y_train)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"importances = np.array(rand_forest.feature_importances_)\nfeature_list = np.array(X_train.columns)\nplot_feature_importance_dec(fit = importances, features = feature_list)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is really interesting. Even though Chi-square ranks fare higher than age in terms of\nimportance, the Random Forest ranks age as the most important feature in this analysis. From\n this we can deduce that age has significant interactions with other variables that result\n survival being classified more accurately.\n\nWe can use this information to engineer more meaningful feature variables.\n\nNext we will analyse the results of the actual Decision Tree.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"estimator = rand_forest.estimators_[1]\n\n# Show the first few levels of the tree\n_ = export_graphviz(estimator, out_file='tree.dot',\n                feature_names = X_train.columns,\n                max_depth = 6,\n                class_names = ['Died', 'Survived'],\n                rounded = True, proportion = True,\n                label='root',\n                precision = 2, filled = True);","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_ = call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\n#_ = call(['dot', '-Tsvg', 'tree.dot', '-o tree.svg', 'tree.svg', '-Gdpi=600'])\nImage(filename = 'tree.png')\n#plt.show()\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the Decision Tree output we can observe many interesting variable interactions\n(combinations of branches that result in clear distinctions of survival or death) such as the\nfollowing:\n\n- gender and age (first 3 levels of splits are mainly on gender and age)\n- gender and deck\n- fare and p_class\n- fare and age\n- fare and embarked\n\nFurther investigation is warranted as there are many meaningful interactions. Let us look at a\nfew more graphs illustrating these interactions.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"chivalry = sns.FacetGrid(df_train, col = 'embarked')\n_ = chivalry.map(sns.pointplot, 'p_class', 'survived', 'sex', ci=95.0, palette = 'deep',\n                 order = [1,2,3], hue_order = ['male','female'])\n_ = chivalry.add_legend()\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">\n<b>Transform variables</b>\n</div>","metadata":{}},{"cell_type":"markdown","source":"It is clear from all our analyses that there are many categorical variables strongly\ncorrelated with survival. It is also clear that there are many strong variable interactions\nin the data.\n\nIt therefore makes sense to experiment with binning of the two continuous variables i.e. age\n and fare and to manually perform some interactions modelling to see if we can obtain more\n consistent results with our predictive models.\n\nWe have already looked at the age variable in detail before, let's have another look at the\nfare variable:","metadata":{}},{"cell_type":"code","source":"fig_fare, axes = plt.subplots(nrows=1, ncols=1, figsize=(10, 6), squeeze=False)\n\n_ = sns.kdeplot(data=df_train_con_plot.loc[(df_train_con_plot['survived'] == 0), 'fare'],\n                shade = True, label = 'Died')\n_ = sns.kdeplot(data=df_train_con_plot.loc[(df_train_con_plot['survived'] == 1), 'fare'],\n                shade = True, label = 'Survived')\n_ = plt.xlabel('Fare', fontsize=20)\n_ = plt.ylabel('Density', fontsize=20)\n_ = plt.xticks(fontsize=20)\n_ = plt.yticks(fontsize=20)\n_ = plt.legend(fontsize=15)\nplt.show()\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can bin this variable quite easily.\n\nAs age and fare have missing values, we need to impute this at time of model building to\navoid data leakage. We will therefore go ahead with the model building process and bin these\n variables after missing values have been replaced.\n\nWe will now build the following models as previously discussed:\n\n 1. Logistic regression\n 2. Multi-layer Perceptron (MLP)\n 3. Decision Tree\n 4. Random Forest\n\nOur strategy is to build our own <i>validation strategy</i> based on the training set for which\nwe have labels. We will do this by splitting this set into training and testing sets according to\na 75%/ 25% split. Any hyper-parameter optimisation will be done by using <i>cross validation</i>\non the 75% test set. The 25% testing set will be used for our final test before we apply the\nresults to the provided test set for submission.\n\nWe therefore now start by splitting the response and features for the training set as previously\ndiscussed.\n\nWe will be using this dataset for all our models from here onwards. We also perform minor\ntransformations such as encoding the <i>sex</i> variable for test and training sets. We also One\nHot Encode the <i>embarked</i> variable. We drop one of the categories for embarked to avoid\nmulti-collinearity (dummy variable trap).\n\n","metadata":{}},{"cell_type":"code","source":"# Group response values to form binary response\ny = df_train_trans.loc[:, 'survived']\n\n# Split data into features (X) and response (y)\nX = df_train_trans.loc[:, names_all]\n\n# Consider using another dataframe for applying testing\ndf_test_trans = df_test_trans.loc[:, names_all]\n\n# Put the response y into an array\ny = np.ravel(y)\n","metadata":{"pycharm":{"name":"#%%\n"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Split, impute missing values and transform the data","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"markdown","source":"We now split the data into training and test sets according to a 75/ 25% split. We next\nimpute missing values without data leakage on the training and test sets.\n\nWe then transform the final continuous variables into categorical variables. The fact that\nwe have transformed all the variables to categorical means that we don't have to scale the\ndata.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n#print('Percentage holdout data: {}%'.format(round(100*(len(X_test)/len(X)),0)))\nnames_train = X.columns\n\n# Replace fare in test set with median from train set - to prevent data leakage.\nmedian_fare = X_train['fare'].median()\ndf_test_trans['fare'].fillna(median_fare, inplace=True)\n\n# Replace missing values for training set\nprint(\"Number of null values in age column: {}\".format(X_train['age'].isnull().sum()))\n\n# Define imputer\nimputer = KNNImputer()\n# fit on the dataset\nimputer.fit(X_train)\n# transform the dataset\nX_train_array = imputer.transform(X_train)\n# summarize total missing\n#print('Missing: %d' % sum(isnan(X_train).flatten()))\nX_train = pd.DataFrame(X_train_array, columns=names_train)\n\nX_test_array = imputer.transform(X_test)\n# summarize total missing\n#print('Missing: %d' % sum(isnan(X_test).flatten()))\nX_test = pd.DataFrame(X_test_array, columns=names_train)\n\n\n# Now we fit and transform for the final model.\n# Fit and apply to the final dataset: TODO: Test whether rebuilding model on complete set\n#  performs better\n#imputer.fit(X)\n\nX_array = imputer.transform(X)\nX = pd.DataFrame(X_array, columns=names_train)\n\n# summarize total missing\n#print('Missing: %d' % sum(isnan(X).flatten()))\ndf_test_trans_array = imputer.transform(df_test_trans)\n# summarize total missing\n#print('Missing: %d' % sum(isnan(df_test_trans).flatten()))\ndf_test_trans = pd.DataFrame(df_test_trans_array, columns=names_train)","metadata":{"pycharm":{"name":"#%%\n"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Binning fare: TODO: fix problem with ranges not understood and undercounting\nfare_bins= [0, 8, 15, 30, 100, 300, 520]\nlabels = ['very_low', 'low', 'average', 'above_average', 'high', 'very_high']\ndf_train['fare_bin'] = pd.cut(df_train['fare'], bins=fare_bins, labels=labels, right=False)\ndf_train_trans['fare_bin'] = pd.cut(df_train_trans['fare'], bins=fare_bins, labels=labels,\n                                    right=False)\nX_train['dim_fare'] = X_train['fare']\nX_train['dim_age'] = X_train['age']\nX_train['dim_ticket_freq'] = X_train['ticket_freq']\nX_test['dim_fare'] = X_test['fare']\nX_test['dim_age'] = X_test['age']\nX_test['dim_ticket_freq'] = X_test['ticket_freq']\ndf_test_trans['dim_fare'] = df_test_trans['fare']\ndf_test_trans['dim_age'] = df_test_trans['age']\ndf_test_trans['dim_ticket_freq'] = df_test_trans['ticket_freq']\n\nX_train['fare_bin'] = pd.cut(X_train['fare'], bins=fare_bins, labels=labels, right=False)\nX_test['fare_bin'] = pd.cut(X_test['fare'], bins=fare_bins, labels=labels,\n                                    right=False)\ndf_train_trans['fare_bin'] = pd.cut(df_train_trans['fare'], bins=fare_bins, labels=labels,\n                                   right=False)\ndf_test_trans['fare_bin'] = pd.cut(df_test_trans['fare'], bins=fare_bins, labels=labels,\n                                   right=False)\n\n#Binning age: TODO: fix problem with ranges not understood and undercounting\nbins= [0, 4, 13, 20, 40, 60, 110]\nlabels = ['infant','child','teen','adult', 'middle_aged', 'elderly']\n#df_train['age_bin'] = pd.cut(df_train['age'], bins=bins, labels=labels, right=False)\n#df_train_trans['age_bin'] = pd.cut(df_train_trans['age'], bins=bins, labels=labels, right=False)\nX_train['age_bin'] = pd.cut(X_train['age'], bins=bins, labels=labels, right=False)\nX_test['age_bin'] = pd.cut(X_test['age'], bins=bins, labels=labels, right=False)\ndf_train_trans['age_bin'] = pd.cut(df_train_trans['age'], bins=bins, labels=labels,\n                                   right=False)\ndf_test_trans['age_bin'] = pd.cut(df_test_trans['age'], bins=bins, labels=labels, right=False)\n\n#\nX_train['dim_age_bin'] = X_train['age_bin']\nX_train['dim_fare_bin'] = X_train['fare_bin']\nX_test['dim_age_bin'] = X_test['age_bin']\nX_test['dim_fare_bin'] = X_test['fare_bin']\n\n# Transform embarked and deck variables for training set - try not binning fare\n#binning_cols = ['fare_bin', 'age_bin', 'ticket_freq']\nbinning_cols = ['age_bin', 'fare_bin']\nX_train = pd.get_dummies(X_train, columns = binning_cols, drop_first=True)\n\n# Transform embarked and deck variables for testing set\n#binning_cols = ['fare_bin', 'age_bin', 'ticket_freq']\nbinning_cols = ['age_bin', 'fare_bin']\nX_test = pd.get_dummies(X_test, columns = binning_cols, drop_first=True)\n\n# Transform embarked and deck variables for test set\ndf_train_trans['dim_age_bin'] = df_train_trans['age_bin']\ndf_train_trans['dim_fare_bin'] = df_train_trans['fare_bin']\n\ndf_test_trans['dim_age_bin'] = df_test_trans['age_bin']\ndf_test_trans['dim_fare_bin'] = df_test_trans['fare_bin']\n\ndf_train_trans = pd.get_dummies(df_train_trans, columns = binning_cols, drop_first=True)\ndf_test_trans = pd.get_dummies(df_test_trans, columns = binning_cols, drop_first=True)\n\n# Hack to fix ticket_freq distribution: TODO: Fix this.\n#df_test_trans['ticket_freq_6.0'] = 0\n#df_test_trans['ticket_freq_7.0'] = 0\n\n#X_train.drop('age', axis=1, inplace=True)\n#X_test.drop('age', axis=1, inplace=True)\n#df_test_trans.drop('age', axis=1, inplace=True)\n#df_train_trans.drop('age', axis=1, inplace=True)\n#X_train.drop('fare', axis=1, inplace=True)\n#X_test.drop('fare', axis=1, inplace=True)\n#df_test_trans.drop('fare', axis=1, inplace=True)\n#df_train_trans.drop('fare', axis=1, inplace=True)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create interaction terms.","metadata":{}},{"cell_type":"code","source":"X_train = pd.merge(X_train, df_train_trans[['dim_deck']],left_index=True, right_index=True)\n\n# create dummy variables, and their interactions: TODO: Check if deck is better cross-term\nX_train_interactions = \\\n    dmatrix('C(dim_deck) * C(sex)', X_train,\n              return_type=\"dataframe\")\nX_train_interactions.drop('Intercept', inplace=True, axis=1)\nX_train = pd.concat([X_train, X_train_interactions], axis=1)\n\nX_train.drop('sex', axis=1, inplace=True)\nX_train.drop('dim_fare', axis=1, inplace=True)\nX_train.drop('dim_age', axis=1, inplace=True)\nX_train.drop('dim_fare_bin', axis=1, inplace=True)\nX_train.drop('dim_age_bin', axis=1, inplace=True)\nX_train.drop('dim_deck', axis=1, inplace=True)\nX_train.drop('dim_ticket_freq', axis=1, inplace=True)\n\nX_test = pd.merge(X_test, df_train_trans[['dim_deck']],left_index=True, right_index=True)\nX_test_interactions = \\\n    dmatrix('C(dim_deck) * C(sex)', X_test,\n              return_type=\"dataframe\")\nX_test_interactions.drop('Intercept', inplace=True, axis=1)\nX_test = pd.concat([X_test, X_test_interactions], axis=1)\n\nX_test.drop('sex', axis=1, inplace=True)\nX_test.drop('dim_fare', axis=1, inplace=True)\nX_test.drop('dim_age', axis=1, inplace=True)\nX_test.drop('dim_fare_bin', axis=1, inplace=True)\nX_test.drop('dim_age_bin', axis=1, inplace=True)\nX_test.drop('dim_deck', axis=1, inplace=True)\nX_test.drop('dim_ticket_freq', axis=1, inplace=True)\n\ndf_test_trans = pd.merge(df_test_trans, df_train_trans[['dim_deck']],left_index=True,\n                         right_index=True)\ndf_test_trans_interactions = \\\n    dmatrix('C(dim_deck) * C(sex)', df_test_trans,\n              return_type=\"dataframe\")\ndf_test_trans_interactions.drop('Intercept', inplace=True, axis=1)\ndf_test_trans = pd.concat([df_test_trans, df_test_trans_interactions], axis=1)\n\ndf_test_trans.drop('sex', axis=1, inplace=True)\ndf_test_trans.drop('dim_fare', axis=1, inplace=True)\ndf_test_trans.drop('dim_age', axis=1, inplace=True)\ndf_test_trans.drop('dim_fare_bin', axis=1, inplace=True)\ndf_test_trans.drop('dim_age_bin', axis=1, inplace=True)\ndf_test_trans.drop('dim_deck', axis=1, inplace=True)\ndf_test_trans.drop('dim_ticket_freq', axis=1, inplace=True)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_selection import chi2\n\n# Separate continuous variables for this step, to be added back afterwards.\nX_train_con = X_train.loc[:, ['age', 'fare', 'ticket_freq']]\nX_train.drop(['age', 'fare', 'ticket_freq'], axis=1, inplace=True)\n# Separate continuous variables for this step, to be added back afterwards.\nX_test_con = X_test.loc[:, ['age', 'fare', 'ticket_freq']]\nX_test.drop(['age', 'fare', 'ticket_freq'], axis=1, inplace=True)\n# Separate continuous variables for this step, to be added back afterwards.\ndf_test_trans_con = df_test_trans.loc[:, ['age', 'fare', 'ticket_freq']]\ndf_test_trans.drop(['age', 'fare', 'ticket_freq'], axis=1, inplace=True)\n\n# Finally we scale our data - separately from categorical variables.\n#scaler = StandardScaler()\n#\n## Fit on training data set\n#names_training = list(X_train_con.columns.values)\n#_ = scaler.fit(X_train_con)\n#X_train_new = scaler.transform(X_train_con)\n#X_train_con = pd.DataFrame(X_train_new, columns=names_training)\n\n# Apply to test data (training)\n#X_test_new = scaler.transform(X_test_con)\n#X_test_con = pd.DataFrame(X_test_new, columns=names_training)\n\n# Scale age and fare on final dataset to final test data\n#df_test_trans_new = scaler.transform(df_test_trans_con)\n#df_test_trans_con = pd.DataFrame(df_test_trans_new, columns=names_training)\n\n# Perform categorical feature selection\nX_train = X_train.astype(float)\ny_train = y_train.astype(float)\nX_test = X_test.astype(float)\ny_test = y_test.astype(float)\ndf_test_trans =  df_test_trans.astype(float)\n\nbest_feat = SelectKBest(chi2, k=\"all\").fit(X_train, y_train)\nmask = X_train.columns.values[best_feat.get_support()]\nX_train_new = best_feat.transform(X_train)\nX_train = pd.DataFrame(X_train_new, columns=mask)\n\nX_test_new = best_feat.transform(X_test)\nX_test = pd.DataFrame(X_test_new, columns=mask)\n\ndf_test_trans_new = best_feat.transform(df_test_trans)\ndf_test_trans = pd.DataFrame(df_test_trans_new, columns=mask)\n\n# What are scores for the features\nfor i in range(len(mask)):\n\tprint('%s: \\t\\t\\t\\t%f' % (mask[i], best_feat.scores_[i]))\n\n# plot the scores\n_ = pyplot.bar([i for i in range(len(best_feat.scores_))], best_feat.scores_)\npyplot.show()\n\n# Add continuous variables back again.\nX_train = pd.merge(X_train, X_train_con['ticket_freq'], left_index=True, right_index=True)\nX_test = pd.merge(X_test, X_test_con['ticket_freq'], left_index=True, right_index=True)\ndf_test_trans = pd.merge(df_test_trans, df_test_trans_con['ticket_freq'], left_index=True,\n                         right_index=True)\n\n# Drop interactions for testing purposes\ninteractions_list = list(X_train_interactions.columns.values)\nX_train.drop(interactions_list, axis=1, inplace=True)\nX_test.drop(interactions_list, axis=1, inplace=True)\ndf_test_trans.drop(interactions_list, axis=1, inplace=True)\n\n# Drop is_married for testing purposes\nX_train.drop(['is_married'], axis=1, inplace=True)\nX_test.drop(['is_married'], axis=1, inplace=True)\ndf_test_trans.drop(['is_married'], axis=1, inplace=True)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#%%\n# Final conversion to float.\nconvert_dict = {'embarked_Q': float,\n                'embarked_S': float,\n                'deck_DE': float,\n                'deck_FG': float,\n                'deck_M': float,\n                'title_Miss': float,\n                'title_Mr': float,\n                'title_Mrs': float,\n                'p_class_2': float,\n                'p_class_3': float,\n                'fam_size_small': float,\n                'fam_size_large': float,\n                'fam_size_very_large': float,\n                'age_bin_child': int,\n                'age_bin_teen': int,\n                'age_bin_adult': int,\n                'age_bin_middle_aged': int,\n                'age_bin_elderly': int,\n                'fare_bin_low': int,\n                'fare_bin_average': int,\n                'fare_bin_above_average': int,\n                'fare_bin_high': int,\n                'fare_bin_very_high': int,\n                'ticket_freq': float}\n\nX_train = X_train.astype(convert_dict)\nX_test = X_test.astype(convert_dict)\ndf_test_trans =  df_test_trans.astype(convert_dict)\n\n# Rearrange columns\ncol_names = ['embarked_Q', 'embarked_S', 'deck_DE', 'deck_FG', 'deck_M', 'title_Miss',\n           'title_Mr', 'title_Mrs', 'p_class_2', 'p_class_3', 'fam_size_small',\n             'fam_size_large', 'fam_size_very_large', 'ticket_freq', 'fare_bin_low',\n             'fare_bin_average', 'fare_bin_above_average', 'fare_bin_high',\n             'fare_bin_very_high', 'age_bin_child', 'age_bin_teen', 'age_bin_adult',\n             'age_bin_middle_aged', 'age_bin_elderly']\n\nX_train = X_train[col_names]\nX_test = X_test[col_names]\ndf_test_trans = df_test_trans[col_names]\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our final dataset for model building looks as follows:","metadata":{}},{"cell_type":"code","source":"X_train.head()","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">\n<b>Build models</b>\n</div>","metadata":{}},{"cell_type":"markdown","source":"We can now start building our first model, yay! We build and test a naive logistic regression \nmodel - without any transformations or optimisations.\n\nThe objective is to ascertain the strength of association between features and responses on \nunprocessed data. ","metadata":{}},{"cell_type":"markdown","source":"#### Naive Logistic Regression","metadata":{}},{"cell_type":"code","source":"# Initial model\nlog_reg = LogisticRegression(max_iter=2000000, fit_intercept = False)\n\n# Probability scores for test set\ny_score_init = log_reg.fit(X_train, y_train).decision_function(X_test)\n\n# False positive Rate and true positive rate\nfpr_roc, tpr_roc, thresholds = roc_curve(y_test, y_score_init)\n\nplot_roc_curve(fpr = fpr_roc, tpr = tpr_roc)","metadata":{"pycharm":{"name":"#%%\n"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = log_reg.predict(X_test)\n# Accuracy before model parameter optimisation\ncnf_matrix = confusion_matrix(y_pred, y_test)\nplot_confusion_matrix(cnf_matrix, classes=[0,1], normalize=True)","metadata":{"pycharm":{"name":"#%%\n"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As can be seen from the accuracy measurements the baseline model performs marginally better\nthan our baseline models. A C-statistic (Area Under the Curve - AUC) of 88% compared to 86%\nfor the baseline model.\n\nThe model has a precision of 83% which is the same as the previously obtained 83%.\n\nThe sensitivity of 77% is up from 68% which means more survivors are being picked up with\nthis model. Specificity us down to 83% from 86% which means the increase in accuracy is at\nthe expense of false negatives.","metadata":{}},{"cell_type":"markdown","source":"#### Naive MLP","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"# Fit and check MSE before regularisation\nmlp_reg = MLPClassifier(max_iter=50000, solver=\"adam\", activation=\"tanh\", hidden_layer_sizes=(5, 5),\n                    random_state=1)\nmlp_reg = mlp_reg.fit(X_train, y_train)\n\n# Predict\ny_pred = mlp_reg.predict(X_test)\n\n# Accuracy before model parameter optimisation\nprint (\"Accuracy Score: %0.5f\" % accuracy_score(y_pred,y_test))\n\n# False positive Rate and true positive rate\nfpr_roc, tpr_roc, thresholds = roc_curve(y_test, y_pred)\n\nplot_roc_curve(fpr = fpr_roc, tpr = tpr_roc)\n\n# Accuracy before model parameter optimisation\ncnf_matrix = confusion_matrix(y_pred, y_test)\nplot_confusion_matrix(cnf_matrix, classes=[0,1], normalize=True)","metadata":{"pycharm":{"name":"#%%\n"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The naive MLP has an AUC score of 85% which is higher than the previous score of\nRegression of 79%.","metadata":{}},{"cell_type":"code","source":"# Optimise numbers of nodes on both layers\nvalidation_scores = {}\nprint(\"Nodes |Validation\")\nprint(\"      | score\")\n\nfor hidden_layer_size in [(i,j) for i in range(2,6) for j in range(2,6)]:\n\n    reg = MLPClassifier(max_iter=1000000, hidden_layer_sizes=hidden_layer_size, random_state=1)\n\n    score = cross_val_score(estimator=reg, X=X_train, y=y_train, cv=2)\n    validation_scores[hidden_layer_size] = score.mean()\n    print(hidden_layer_size, \": %0.5f\" % validation_scores[hidden_layer_size])","metadata":{"pycharm":{"name":"#%%\n"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check scores\nprint(\"The highest validation score is: %0.4f\" % max(validation_scores.values()))\noptimal_hidden_layer_size = [name for name, score in validation_scores.items()\n                              if score==max(validation_scores.values())][0]\nprint(\"This corresponds to nodes\", optimal_hidden_layer_size )\n","metadata":{"pycharm":{"name":"#%%\n"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we optimise the neural network regularisation parameter.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"# Select range over which to find regularisation parameter - exponential used for even\n# distribution of values\nreg_par = [np.e**n for n in np.arange(-2,4,0.5)]\n\nvalidation_scores = {}\nprint(\" alpha  |  Accuracy\")\nfor param in reg_par:\n    reg = MLPClassifier(max_iter=1000000, solver=\"adam\", activation=\"tanh\",\n                        hidden_layer_sizes=optimal_hidden_layer_size, alpha=param, random_state=1)\n    score = cross_val_score(estimator=reg, X=X_train, y=y_train, cv=2, scoring=\"accuracy\")\n    validation_scores[param] = score.mean()\n    print(\"%0.5f |  %s\" % (param, score.mean()))\n\n# Plot the accuracy function against regularisation parameter\nplt.plot([np.log(i) for i in validation_scores.keys()], list(validation_scores.values()));\nplt.xlabel(\"Ln of alpha\");\nplt.ylabel(\"Accuracy\");","metadata":{"pycharm":{"name":"#%%\n"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The highest cross-validation accuracy score and hence the value to use for the `alpha` parameter\nis as follows.","metadata":{}},{"cell_type":"code","source":"max_score = ([np.log(name) for name, score in validation_scores.items() if score==max\n(validation_scores.values())][0])\n# Find lowest value.\nprint(\"The highest accuracy score is: %s\" % (max(validation_scores.values())))\nprint(\"This corresponds to regularisation parameter e**%s\" % max_score)","metadata":{"pycharm":{"name":"#%%\n"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### MSE after regularisation","metadata":{}},{"cell_type":"code","source":"# Fit data with the best parameter\nmlp_reg_optim = MLPClassifier(max_iter=1000000, solver=\"adam\", activation=\"tanh\",\n                    hidden_layer_sizes=optimal_hidden_layer_size, alpha=np.e**(max_score),\n                              random_state=1)\n\nmlp_reg_optim.fit(X_train, y_train)\n\n# Predict\ny_pred = mlp_reg_optim.predict(X_test)\n\n# Accuracy after model parameter optimisation\naccuracy_score(y_pred,y_test)","metadata":{"pycharm":{"name":"#%%\n"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Accuracy analysis","metadata":{}},{"cell_type":"code","source":"# False positive Rate and true positive rate\nfpr_roc, tpr_roc, thresholds = roc_curve(y_test, y_pred)\nplot_roc_curve(fpr = fpr_roc, tpr = tpr_roc)\n\ncnf_matrix = confusion_matrix(y_pred, y_test)\nplot_confusion_matrix(cnf_matrix, classes=[0,1], normalize=True)","metadata":{"pycharm":{"name":"#%%\n"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The optimised MLP has an AUC score of 78% which is higher than the previous score\nof 77%. This is encouraging as this was also the model that fared best on the public data.","metadata":{}},{"cell_type":"markdown","source":"#### Naive Decision Tree\n\nWe now build a naive Decision Tree to see it performs against the previous models. We also use\nthe Decision Tree to calculate feature importance. This will provide us with a better feeling for\n strength of association between features and the response.","metadata":{}},{"cell_type":"code","source":"# Fit a Decision Tree to data\nsamples = [sample for sample in range(1,30)]\nvalidation_scores = []\nfor sample in samples:\n    classifier1 = DecisionTreeClassifier(random_state=1, min_samples_leaf=sample)\n    score = cross_val_score(estimator=classifier1, X=X_train, y=y_train, cv=5)\n    validation_scores.append(score.mean())\n\n# Obtain the minimum leaf samples with the highest validation score\nsamples_optimum = samples[validation_scores.index(max(validation_scores))]\n\nclassifier2 = DecisionTreeClassifier(random_state=0, min_samples_leaf=samples_optimum)\nclassifier2.fit(X_train, y_train)","metadata":{"pycharm":{"name":"#%%\n"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Feature importances for the Decision Tree is as follows:","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"cols_model = X_train.columns.to_list()\n\nimportances = np.array(classifier2.feature_importances_)\nfeature_list = np.array(cols_model)\n\n# summarize feature importance\nfor i,v in enumerate(importances):\n\tprint('Feature: %10s\\tScore:\\t%.5f' % (feature_list[i],v))\n# plot feature importance\nsorted_ID=np.array(np.argsort(importances)[::-1])\nplt.figure(figsize=[10,10])\nplt.xticks(rotation='vertical')\n_ = plt.bar(feature_list[sorted_ID], importances[sorted_ID]);\nplt.show();","metadata":{"pycharm":{"name":"#%%\n"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we have an interesting turn of events. The previously most important features was sex,\nbut has now been replaced by title_Mr. Passenger class and family size are in 2nd and 3rd\nplaces.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"markdown","source":"#### Accuracy analysis","metadata":{}},{"cell_type":"code","source":"# Probability scores for test set\ny_pred = classifier2.predict(X_test)\n\naccuracy_score(y_pred,y_test)\n\n# False positive Rate and true positive rate\nfpr_roc, tpr_roc, thresholds = roc_curve(y_test, y_pred)\nplot_roc_curve(fpr = fpr_roc, tpr = tpr_roc)\n\ncnf_matrix = confusion_matrix(y_pred, y_test)\nplot_confusion_matrix(cnf_matrix, classes=[0,1], normalize=True)\n","metadata":{"pycharm":{"name":"#%%\n"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Decision Tree obtained an AUC score of 75% which is lower than the previous score of\n77%.\n","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"markdown","source":"#### Naive Random Forest\nNow we build a Random Forest and see what happens!","metadata":{}},{"cell_type":"code","source":"rand_forest = RandomForestClassifier(criterion= 'gini', random_state=0)\nrand_forest.fit(X_train, y_train)\n\n# Probability scores for test set\ny_pred = rand_forest.predict(X_test)\n\naccuracy_score(y_pred,y_test)","metadata":{"pycharm":{"name":"#%%\n"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Accuracy analysis","metadata":{}},{"cell_type":"code","source":"# False positive Rate and true positive rate\nfpr_roc, tpr_roc, thresholds = roc_curve(y_test, y_pred)\nplot_roc_curve(fpr = fpr_roc, tpr = tpr_roc)\n\ncnf_matrix = confusion_matrix(y_pred, y_test)\nplot_confusion_matrix(cnf_matrix, classes=[0,1], normalize=True)","metadata":{"pycharm":{"name":"#%%\n"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Random Forest obtained an AUC score of 83% which is higher than the previous score\nobtained.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"markdown","source":"#### Optimised Random Forest","metadata":{}},{"cell_type":"code","source":"rand_forest = RandomForestClassifier(max_features='auto')\nparam_grid = { \"criterion\" : [\"gini\", \"entropy\"], \"min_samples_leaf\" : [1, 5, 10],\n               \"min_samples_split\" : [2, 4, 10, 12], \"n_estimators\": [50, 100, 400, 700]}\ngs = GridSearchCV(estimator=rand_forest, param_grid=param_grid, scoring='accuracy', cv=3,\n                  n_jobs=-1)\ngs = gs.fit(X_train, y_train)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gs.best_params_","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Final prediction - MLP\n#rand_forest = RandomForestClassifier(criterion= 'gini', min_samples_leaf=5,\n#                                     min_samples_split=2, n_estimators=100, random_state=0)\nrand_forest = RandomForestClassifier(criterion= 'entropy', min_samples_leaf=5,\n                                     min_samples_split=4, n_estimators=100, random_state=0)\nrand_forest.fit(X_train, y_train)\n#rand_forest.fit(x_train_prev, y_train)\n\n# Probability scores for test set\ny_pred = rand_forest.predict(X_test)\n#y_pred = rand_forest.predict(x_test_prev)\naccuracy_score(y_pred,y_test)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"importances = np.array(rand_forest.feature_importances_)\nfeature_list = np.array(X_train.columns)\nimportances = np.array(importances)\nsorted_ID=np.array(np.argsort(importances))\nreverse_features = feature_list[sorted_ID][::-1]\nreverse_importances = importances[sorted_ID][::-1]\n\nfor i,v in enumerate(reverse_importances):\n    print('Feature: %20s\\tScore:\\t%.5f' % (reverse_features[i],v))\n\n# Plot feature importance\n#sorted_ID=np.array(np.argsort(scores)[::-1])\n#sns.set(font_scale=1);\n_ = plt.figure(figsize=[10,10]);\n_ = plt.xticks(rotation='horizontal', fontsize=20)\n_ = plt.barh(feature_list[sorted_ID], importances[sorted_ID], align='center');\n_ = plt.yticks(fontsize=20)\n_ = plt.show();","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">\n<b>Conclusion</b>\n</div>","metadata":{}},{"cell_type":"markdown","source":"Previously we chose the optimised (regularised) MLP for submission based on overall\n<i>public accuracy</i> score of 0.7799. This public score was in the top 24% and corresponds\nmore or less with our validation scores, so looks sensible! Not bad for a model with\nabsolutely no fine-tuning.\n\nFor this notebook we performed data cleaning and EDA. We spent a lot of time imputing\nmissing values and calculating correlation. We also spent some time doing some basic feature\n engineering. For this notebook the Random Forest model overtook the MLP in terms of\n performance, and scored a respectable public accuracy score of 0.79665! At this point we\n have used only hand-coded scikit-learn models, with no automated hyperparameter\n optimisation or fancy boosting models. Our approach is also different to other approaches\n on Kaggle in that we opted for the use of dummy variables only, given the fact that most\n variables in this problem are categorical. Seeing as we encoded all variables there was no\n need for scaling of values.\n\nNot bad, but not a huge improvement taking into consideration all the time spent on data\ncleaning and EDA...\n\nWe have however found some useful information from our Decision Trees regarding variable\ninteractions, and are confident that we can build some interesting cross-terms from these in\n our next notebook!\n\nOn to the next notebook on feature engineering and use of ML pipelines!\n\nHopefully we can crack the 80% level target by building some awesome cross-terms.","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"# Gender submission - Score: 0.77\n\n# Final prediction - Random Forest - Score:  0.79665\ny_pred = rand_forest.predict(df_test_trans)\ny_pred = y_pred.astype(int)\n\n#Prepare submission code\nmy_submission = pd.DataFrame({'PassengerId': df_orig.passenger_id, 'Survived': y_pred})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('submission_rand_forest.csv', index=False)\n\n# Final prediction - MLP (optimised) - Score: ?\n#mlp_reg_optim.fit(X, y)\ny_pred = mlp_reg_optim.predict(df_test_trans)\ny_pred = y_pred.astype(int)\n\n#Prepare submission code\nmy_submission = pd.DataFrame({'PassengerId': df_orig.passenger_id, 'Survived': y_pred})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('submission_mlp_optim.csv', index=False)\n\n# TODO: Final prediction - MLP (non-optimised) - Score: ?\ny_pred = mlp_reg.predict(df_test_trans)\ny_pred = y_pred.astype(int)\n\n#Prepare submission code\nmy_submission = pd.DataFrame({'PassengerId': df_orig.passenger_id, 'Survived': y_pred})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('submission_mlp.csv', index=False)\n\n# Final prediction - Logistic Regression - Score: ?\ny_pred = log_reg.predict(df_test_trans)\ny_pred = y_pred.astype(int)\n\n#Prepare submission code\nmy_submission = pd.DataFrame({'PassengerId': df_orig.passenger_id, 'Survived': y_pred})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('submission_logreg.csv', index=False)\n\n# Final prediction - Decision Tree - Score: ?\ny_pred = classifier2.predict(df_test_trans)\ny_pred = y_pred.astype(int)\n\n#Prepare submission code\nmy_submission = pd.DataFrame({'PassengerId': df_orig.passenger_id, 'Survived': y_pred})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('submission_dec_tree.csv', index=False)\n","metadata":{"pycharm":{"name":"#%%\n"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]}]}