{"cells":[{"metadata":{},"cell_type":"markdown","source":"![](https://upload.wikimedia.org/wikipedia/commons/6/6e/St%C3%B6wer_Titanic.jpg)"},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"0.0\"></a>\n# Titanic : Comparison of automatic FE&FS efficiency with Featuretools and tradicional approaches"},{"metadata":{},"cell_type":"markdown","source":"My kernels outline traditional approaches to FE:\n\n1) the consolidated result of EDA and FE optimization from many authors:\n* https://www.kaggle.com/vbmokin/titanic-top-3-cluster-analysis\n* https://www.kaggle.com/vbmokin/titanic-top-3-one-line-of-the-prediction-code\n* https://www.kaggle.com/vbmokin/three-lines-of-code-for-titanic-top-15\n* https://www.kaggle.com/vbmokin/three-lines-of-code-for-titanic-top-20\n\n2) the result of the formation of many features and their processing by 20 models (boosting, regression, simple neural networks, etc.):\n\n* [Titanic (0.83253) - Comparison 20 popular models](https://www.kaggle.com/vbmokin/titanic-0-83253-comparison-20-popular-models)\n\n\nThe kernel [Automated feature engineering for Titanic dataset](https://www.kaggle.com/liananapalkova/automated-feature-engineering-for-titanic-dataset) provides an example of using library Featuretools for automatic FE. Let us analyze whether this application will produce comparable results."},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"0.1\"></a>\n\n## Table of Contents\n\n1. [Import libraries](#1)\n1. [Download datasets](#2)\n1. [Preparing to modeling with manual FE](#3)\n -  [Clearning data and manual FE](#3.1)\n -  [Automatic FE with Featuretools](#3.2)\n -  [Encoding categorical features](#3.3)\n1. [Automatic feature selection (FS)](#4)\n -  [FS with the Pearson correlation](#4.1)\n -  [FS by the SelectFromModel with LinearSVC](#4.2) \n -  [FS by the SelectFromModel with Lasso](#4.3) \n -  [FS by the SelectKBest with Chi-2](#4.4)\n -  [FS by the Recursive Feature Elimination (RFE) with Logistic Regression](#4.5) \n -  [FS by the Recursive Feature Elimination (RFE) with Random Forest](#4.6) \n1. [Modeling](#5)\n -  [The simple rule - very accurate model](#5.1)\n -  [The Random Forest Classifiers for 8 options of selected feature sets](#5.2)\n1. [Comparison of 28 models](#6)\n1. [Conclusions](#7)"},{"metadata":{},"cell_type":"markdown","source":"## 1. Import libraries <a class=\"anchor\" id=\"1\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport featuretools as ft\nfrom featuretools.primitives import *\nfrom featuretools.variable_types import Numeric\n\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\nfrom sklearn.linear_model import LinearRegression, LogisticRegression, LassoCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.metrics import explained_variance_score\nfrom sklearn.svm import LinearSVC\nfrom sklearn.feature_selection import SelectFromModel, SelectKBest, RFE, chi2\n\n# model tuning\nfrom hyperopt import STATUS_OK, Trials, fmin, hp, tpe, space_eval\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Download datasets <a class=\"anchor\" id=\"2\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"traindf = pd.read_csv('../input/titanic/train.csv').set_index('PassengerId')\ntestdf = pd.read_csv('../input/titanic/test.csv').set_index('PassengerId')\ndf = pd.concat([traindf, testdf], axis=0, sort=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Preparing to modeling with manual FE <a class=\"anchor\" id=\"3\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{},"cell_type":"markdown","source":"### 3.1. Clearning data and manual FE <a class=\"anchor\" id=\"3.1\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Thanks to:\n# https://www.kaggle.com/mauricef/titanic\n# https://www.kaggle.com/vbmokin/titanic-top-3-one-line-of-the-prediction-code\n#\ndf = pd.concat([traindf, testdf], axis=0, sort=False)\ndf['Title'] = df.Name.str.split(',').str[1].str.split('.').str[0].str.strip()\ndf['Title'] = df.Name.str.split(',').str[1].str.split('.').str[0].str.strip()\ndf['IsWomanOrBoy'] = ((df.Title == 'Master') | (df.Sex == 'female'))\ndf['LastName'] = df.Name.str.split(',').str[0]\nfamily = df.groupby(df.LastName).Survived\ndf['WomanOrBoyCount'] = family.transform(lambda s: s[df.IsWomanOrBoy].fillna(0).count())\ndf['WomanOrBoyCount'] = df.mask(df.IsWomanOrBoy, df.WomanOrBoyCount - 1, axis=0)\ndf['FamilySurvivedCount'] = family.transform(lambda s: s[df.IsWomanOrBoy].fillna(0).sum())\ndf['FamilySurvivedCount'] = df.mask(df.IsWomanOrBoy, df.FamilySurvivedCount - \\\n                                    df.Survived.fillna(0), axis=0)\ndf['WomanOrBoySurvived'] = df.FamilySurvivedCount / df.WomanOrBoyCount.replace(0, np.nan)\ndf.WomanOrBoyCount = df.WomanOrBoyCount.replace(np.nan, 0)\ndf['Alone'] = (df.WomanOrBoyCount == 0)\n\n#Thanks to https://www.kaggle.com/kpacocha/top-6-titanic-machine-learning-from-disaster\n#\"Title\" improvement\ndf['Title'] = df['Title'].replace('Ms','Miss')\ndf['Title'] = df['Title'].replace('Mlle','Miss')\ndf['Title'] = df['Title'].replace('Mme','Mrs')\n# Embarked\ndf['Embarked'] = df['Embarked'].fillna('S')\n# Cabin, Deck\ndf['Deck'] = df['Cabin'].apply(lambda s: s[0] if pd.notnull(s) else 'M')\ndf.loc[(df['Deck'] == 'T'), 'Deck'] = 'A'\n\n# Thanks to https://www.kaggle.com/erinsweet/simpledetect\n# Fare\nmed_fare = df.groupby(['Pclass', 'Parch', 'SibSp']).Fare.median()[3][0][0]\ndf['Fare'] = df['Fare'].fillna(med_fare)\n#Age\ndf['Age'] = df.groupby(['Sex', 'Pclass', 'Title'])['Age'].apply(lambda x: x.fillna(x.median()))\n# Family_Size\ndf['Family_Size'] = df['SibSp'] + df['Parch'] + 1\n\n# Thanks to https://www.kaggle.com/vbmokin/titanic-top-3-cluster-analysis\ncols_to_drop = ['Name','Ticket','Cabin']\ndf = df.drop(cols_to_drop, axis=1)\n\ndf.WomanOrBoySurvived = df.WomanOrBoySurvived.fillna(0)\ndf.WomanOrBoyCount = df.WomanOrBoyCount.fillna(0)\ndf.FamilySurvivedCount = df.FamilySurvivedCount.fillna(0)\ndf.Alone = df.Alone.fillna(0)\ndf.Alone = df.Alone*1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_optimum = pd.concat([df.WomanOrBoySurvived.fillna(0), df.Alone, df.Sex.replace({'male': 0, 'female': 1})], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Select the main features from which the optimum rule with LB = 0.83253 (see [Titanic Top 3% : one line of the prediction code](https://www.kaggle.com/vbmokin/titanic-top-3-one-line-of-the-prediction-code)) for features were manually selected and **look if the Featuretools library can find them yourself**"},{"metadata":{"trusted":true},"cell_type":"code","source":"target = df.Survived.loc[traindf.index]\ndf = df.drop(['SibSp','Parch','IsWomanOrBoy','WomanOrBoyCount','FamilySurvivedCount','WomanOrBoySurvived','Alone'], axis=1)\ndf['PassengerId'] = df.index\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.2. Automatic FE with Featuretools <a class=\"anchor\" id=\"3.2\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{},"cell_type":"markdown","source":"Thanks to https://www.kaggle.com/liananapalkova/automated-feature-engineering-for-titanic-dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"es = ft.EntitySet(id = 'titanic_data')\nes = es.entity_from_dataframe(entity_id = 'df', dataframe = df.drop(['Survived'], axis=1), \n                              variable_types = \n                              {\n                                  'Embarked': ft.variable_types.Categorical,\n                                  'Sex': ft.variable_types.Boolean,\n                                  'Title': ft.variable_types.Categorical,\n                                  'Family_Size': ft.variable_types.Numeric,\n                                  'LastName': ft.variable_types.Categorical\n                              },\n                              index = 'PassengerId')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"es = es.normalize_entity(base_entity_id='df', new_entity_id='Pclass', index='Pclass')\nes = es.normalize_entity(base_entity_id='df', new_entity_id='Sex', index='Sex')\nes = es.normalize_entity(base_entity_id='df', new_entity_id='Age', index='Age')\nes = es.normalize_entity(base_entity_id='df', new_entity_id='Fare', index='Fare')\nes = es.normalize_entity(base_entity_id='df', new_entity_id='Embarked', index='Embarked')\nes = es.normalize_entity(base_entity_id='df', new_entity_id='Title', index='Title')\nes = es.normalize_entity(base_entity_id='df', new_entity_id='LastName', index='LastName')\nes = es.normalize_entity(base_entity_id='df', new_entity_id='Deck', index='Deck')\nes = es.normalize_entity(base_entity_id='df', new_entity_id='Family_Size', index='Family_Size')\nes = es.normalize_entity(base_entity_id='df', new_entity_id='Title_Sex', index='Sex')\nes = es.normalize_entity(base_entity_id='df', new_entity_id='Sex_LastName', index='LastName')\nes = es.normalize_entity(base_entity_id='df', new_entity_id='Title_LastName', index='LastName')\nes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"primitives = ft.list_primitives()\npd.options.display.max_colwidth = 500\nprimitives[primitives['type'] == 'aggregation'].head(primitives[primitives['type'] == 'aggregation'].shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('max_columns',500)\npd.set_option('max_rows',500)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features, feature_names = ft.dfs(entityset = es, \n                                 target_entity = 'df', \n                                 max_depth = 2)\nlen(feature_names)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.3. Encoding categorical features <a class=\"anchor\" id=\"3.3\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Determination categorical features\nnumerics = ['int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']\ncategorical_columns = []\ncols = features.columns.values.tolist()\nfor col in cols:\n    if features[col].dtype in numerics: continue\n    categorical_columns.append(col)\ncategorical_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encoding categorical features\nfor col in categorical_columns:\n    if col in features.columns:\n        le = LabelEncoder()\n        le.fit(list(features[col].astype(str).values))\n        features[col] = le.transform(list(features[col].astype(str).values))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train, test = features.loc[traindf.index], features.loc[testdf.index]\nX_norm = MinMaxScaler().fit_transform(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Automatic feature selection (FS)<a class=\"anchor\" id=\"4\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{},"cell_type":"markdown","source":"Thanks to https://www.kaggle.com/sz8416/6-ways-for-feature-selection"},{"metadata":{},"cell_type":"markdown","source":"### 4.1. FS with the Pearson correlation <a class=\"anchor\" id=\"4.1\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{},"cell_type":"markdown","source":"Thanks to:\n\n* FE from the https://www.kaggle.com/liananapalkova/automated-feature-engineering-for-titanic-dataset\n\n* Visualization from the https://www.kaggle.com/vbmokin/three-lines-of-code-for-titanic-top-20"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Threshold for removing correlated variables\nthreshold = 0.9\n\ndef highlight(value):\n    if value > threshold:\n        style = 'background-color: pink'\n    else:\n        style = 'background-color: palegreen'\n    return style\n\n# Absolute value correlation matrix\ncorr_matrix = features.corr().abs()\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\nupper.style.applymap(highlight)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Select columns with correlations above threshold\ncollinear_features = [column for column in upper.columns if any(upper[column] > threshold)]\nfeatures_filtered = features.drop(columns = collinear_features)\n#features_positive = features_filtered.loc[:, features_filtered.ge(0).all()]\nprint('The number of features that passed the collinearity threshold: ', features_filtered.shape[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"FE_option0 = features.columns\nFE_option1 = features_filtered.columns\nprint(len(FE_option0), len(FE_option1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.2. FS by the SelectFromModel with LinearSVC <a class=\"anchor\" id=\"4.2\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Thanks to https://www.kaggle.com/liananapalkova/automated-feature-engineering-for-titanic-dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"lsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(train, target)\nmodel = SelectFromModel(lsvc, prefit=True)\nX_new = model.transform(train)\nX_selected_df = pd.DataFrame(X_new, columns=[train.columns[i] for i in range(len(train.columns)) if model.get_support()[i]])\nX_selected_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"FE_option2 = X_selected_df.columns\nFE_option2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.3. FS by the SelectFromModel with Lasso <a class=\"anchor\" id=\"4.3\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso = LassoCV(cv=5).fit(train, target)\nmodel = SelectFromModel(lasso, prefit=True)\nX_new = model.transform(train)\nX_selected_df = pd.DataFrame(X_new, columns=[train.columns[i] for i in range(len(train.columns)) if model.get_support()[i]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"FE_option3 = X_selected_df.columns\nFE_option3","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.4. FS by the SelectKBest with Chi-2 <a class=\"anchor\" id=\"4.4\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{},"cell_type":"markdown","source":"Thanks to:\n* https://www.kaggle.com/sz8416/6-ways-for-feature-selection\n* https://towardsdatascience.com/feature-selection-techniques-in-machine-learning-with-python-f24e7da3f36e"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualization from https://towardsdatascience.com/feature-selection-techniques-in-machine-learning-with-python-f24e7da3f36e\n# but to k='all'\nbestfeatures = SelectKBest(score_func=chi2, k='all')\nfit = bestfeatures.fit(train, target)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(train.columns)\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Feature','Score']  #naming the dataframe columns\nprint(featureScores.nlargest(len(dfcolumns),'Score')) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"FE_option4 = featureScores[featureScores['Score'] > 1000]['Feature']\nlen(FE_option4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"FE_option5 = featureScores[featureScores['Score'] > 100]['Feature']\nlen(FE_option5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### 4.5. FS by the Recursive Feature Elimination (RFE) with Logistic Regression<a class=\"anchor\" id=\"4.5\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{},"cell_type":"markdown","source":"Thanks to:\n* https://www.kaggle.com/sz8416/6-ways-for-feature-selection\n* https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"rfe_selector = RFE(estimator=LogisticRegression(), n_features_to_select=50, step=10, verbose=5)\nrfe_selector.fit(X_norm, target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfe_support = rfe_selector.get_support()\nrfe_feature = train.loc[:,rfe_support].columns.tolist()\nprint(str(len(rfe_feature)), 'selected features')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"FE_option6 = rfe_feature","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.6. FS by the Recursive Feature Elimination (RFE) with Random Forest<a class=\"anchor\" id=\"4.6\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{},"cell_type":"markdown","source":"Thanks to https://www.kaggle.com/sz8416/6-ways-for-feature-selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"embeded_rf_selector = SelectFromModel(RandomForestClassifier(n_estimators=200), threshold='1.25*median')\nembeded_rf_selector.fit(train, target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embeded_rf_support = embeded_rf_selector.get_support()\nembeded_rf_feature = train.loc[:,embeded_rf_support].columns.tolist()\nprint(str(len(embeded_rf_feature)), 'selected features')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"FE_option7 = embeded_rf_feature","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Modeling <a class=\"anchor\" id=\"5\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{},"cell_type":"markdown","source":"### 5.1. The simple rule - very accurate model <a class=\"anchor\" id=\"5.1\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{},"cell_type":"markdown","source":"From my kernel: [Titanic Top 3% : one line of the prediction code](https://www.kaggle.com/vbmokin/titanic-top-3-one-line-of-the-prediction-code)"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_rule = df_optimum.loc[testdf.index]\n# The one line of the code for prediction : LB = 0.83253 (Titanic Top 3%) \ntest_rule['Survived'] = (((test_rule.WomanOrBoySurvived <= 0.238) & (test_rule.Sex > 0.5) & (test_rule.Alone > 0.5)) | \\\n          ((test_rule.WomanOrBoySurvived > 0.238) & \\\n           ~((test_rule.WomanOrBoySurvived > 0.55) & (test_rule.WomanOrBoySurvived <= 0.633))))\n\n# Saving the result\npd.DataFrame({'Survived': test_rule['Survived'].astype(int)}, \\\n             index=testdf.index).reset_index().to_csv('survived.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_simple_rule = 92.7\nLB_simple_rule = 0.83253","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.2. The Random Forest Classifiers for 8 options of selected feature sets <a class=\"anchor\" id=\"5.2\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def RF (features_set,file):\n    # Tuning Random Forest model for features \"features_set\", makes prediction and save it into file  \n    train_fe = train[features_set]\n    test_fe = test[features_set]\n    random_forest = GridSearchCV(estimator=RandomForestClassifier(), param_grid={'n_estimators': [100, 500]}, cv=5).fit(train_fe, target)\n    random_forest.fit(train_fe, target)\n    Y_pred = random_forest.predict(test_fe).astype(int)\n    random_forest.score(train_fe, target)\n    acc_random_forest = round(random_forest.score(train_fe, target) * 100, 2)\n    pd.DataFrame({'Survived': Y_pred}, index=testdf.index).reset_index().to_csv(file, index=False)\n    return acc_random_forest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc0 = RF(FE_option0, 'survived_FT.csv')\nacc1 = RF(FE_option1, 'survived_FE1_Pearson.csv')\nacc2 = RF(FE_option2, 'survived_FE2_LinSVC.csv')\nacc3 = RF(FE_option3, 'survived_FE3_Lasso.csv')\nacc4 = RF(FE_option4, 'survived_FE4_Chi2_1000.csv')\nacc5 = RF(FE_option5, 'survived_FE5_Chi2_100.csv')\nacc6 = RF(FE_option6, 'survived_FE6_RFE_LogR.csv')\nacc7 = RF(FE_option7, 'survived_FE7_RFE_RF.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# After download solutions in Kaggle competition:\n# 2019:\n# LB0 = 0.74641\n# LB1 = 0.73684\n# LB2 = 0.75119\n# LB3 = 0.75598\n# LB4 = 0.76076\n# LB5 = 0.74641\n# LB6 = 0.74641\n# LB7 = 0.74162\n# 2020:\nLB0 = 0.74162\nLB1 = 0.73444\nLB2 = 0.74401\nLB3 = 0.74401\nLB4 = 0.75358\nLB5 = 0.73923\nLB6 = 0.75358\nLB7 = 0.73444","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. Comparison of 28 models <a class=\"anchor\" id=\"6\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{},"cell_type":"markdown","source":"**Comparison of 9 models, including 8 new models**"},{"metadata":{"trusted":true},"cell_type":"code","source":"models = pd.DataFrame({\n    'Model': ['Simple rule',\n              'FT',\n              'FT + Pearson correlation', \n              'FT + SelectFromModel with LinearSVC',\n              'FT + SelectFromModel with Lasso', \n              'FT + SelectKBest with Chi-2 with Score > 1000',\n              'FT + SelectKBest with Chi-2 with Score > 100',\n              'FT + RFE with Logistic Regression',\n              'FT + RFE with Random Forest'],\n    \n    'acc':  [acc_simple_rule, acc0, acc1, acc2, acc3, acc4, acc5, acc6, acc7],\n\n    'LB':   [LB_simple_rule, LB0, LB1, LB2, LB3, LB4, LB5, LB6, LB7]})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models.sort_values(by=['acc', 'LB'], ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models.sort_values(by=['LB', 'acc'], ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 7. Conclusions <a class=\"anchor\" id=\"7\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{},"cell_type":"markdown","source":"The analysis makes the following conclusions:\n\n- The results of the work of the methods of the **Featuretools** library and their post-processing in different ways **did not allow us to come close to the optimal set** of features done manually (from kernel [Titanic Top 3% : one line of the prediction code](https://www.kaggle.com/vbmokin/titanic-top-3-one-line-of-the-prediction-code))\n\nÂ - Among **the methods of post-processing** the Featuretools results in this competition, **the best is SelectKBest with strong filtering** (to only 23 features), the less effective one is the **SelectFromModel** method\n \n- Lack of post-processing of Featuretools results or application of RFE methods is the least effective to predict\n\n- **the Featuretools library has many possibility** - this kernel has not opened them all, so it is advisable to conduct a more in-depth study of this task (other max_depth in *ft.dfs*, other constraints for feature filtering, etc.)"},{"metadata":{},"cell_type":"markdown","source":"I hope you find this kernel useful and enjoyable."},{"metadata":{},"cell_type":"markdown","source":"Your comments and feedback are most welcome."},{"metadata":{},"cell_type":"markdown","source":"[Go to Top](#0.0)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}