{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a class=\"anchor\" id=\"0\"></a>\n# 50 Advanced Tips for Data Science for tabular data (in progress...)\n## Frequently used useful code for:\n* Data cleaning\n* FE\n* Modeling\n* Analysing, and visualization of modeling results\n* Prediction and submitting of modeling results\netc.\n\n### With BONUS - the short solution for Competition [Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic) with LB=0.80382 (Top 2%)\n\n### See also the notebook [50 Tips: Data Science (tabular data) for beginner](https://www.kaggle.com/vbmokin/50-tips-data-science-tabular-data-for-beginner)\n\nLater I will publish another notebook for EDA.","metadata":{"papermill":{"duration":0.064943,"end_time":"2021-03-26T17:17:31.308132","exception":false,"start_time":"2021-03-26T17:17:31.243189","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## It's done 34 tips: 1, 2, 3.1, 3.2, 4.1-4.9, 5.1-5.5, 5.8-5.15, 6.1, 6.2, 7.1-7.4, 7.6, 8\n\n### Added Tip 4.9. Time Series Stationarity Check","metadata":{"papermill":{"duration":0.062348,"end_time":"2021-03-26T17:17:31.433874","exception":false,"start_time":"2021-03-26T17:17:31.371526","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Acknowledgements\n\n### Datasets:\n* for Classification task solutions - competition's dataset [Titanic - Machine Learning from Disaster](https://www.kaggle.com/c/titanic)\n* for Classification task solutions - [Heart Disease UCI](https://www.kaggle.com/ronitf/heart-disease-uci)\n* for Regression task solutions - my dataset [Ammonium prediction in river water](https://www.kaggle.com/vbmokin/ammonium-prediction-in-river-water)\n* from API for Regression task solutions - official data of COVID-19 in Ukraine (https://covid19.rnbo.gov.ua/)\n* for NLP task - [NLP : Reports & News Classification](https://www.kaggle.com/vbmokin/nlp-reports-news-classification)\n\n### Notebooks:\n* [50 Tips: Data Science (tabular data) for beginner](https://www.kaggle.com/vbmokin/50-tips-data-science-tabular-data-for-beginner)\n* [Data Science for tabular data: Advanced Techniques](https://www.kaggle.com/vbmokin/data-science-for-tabular-data-advanced-techniques)\n* [EDA for tabular data: Advanced Techniques](https://www.kaggle.com/vbmokin/eda-for-tabular-data-advanced-techniques)\n* [Heart Disease - Automatic AdvEDA & FE & 20 models](https://www.kaggle.com/vbmokin/heart-disease-automatic-adveda-fe-20-models)\n* [COVID in UA: Prophet with 4, Nd seasonality](https://www.kaggle.com/vbmokin/covid-in-ua-prophet-with-4-nd-seasonality)\n* [Top score : one line of the prediction](https://www.kaggle.com/vbmokin/titanic-top-score-one-line-of-the-prediction)\n* [Data Visualization in just one line of code!!](https://www.kaggle.com/nareshbhat/data-visualization-in-just-one-line-of-code)\n* [DisasterNLP: t-SNE with TFHub, RAPIDS, Plotly](https://www.kaggle.com/xhlulu/disasternlp-t-sne-with-tfhub-rapids-plotly)\n* [MNIST 2D t-SNE with Rapids](https://www.kaggle.com/tunguz/mnist-2d-t-sne-with-rapids)\n* [MNIST Original : 2D tSNE, 3D UMAP with RAPIDS](https://www.kaggle.com/vbmokin/mnist-original-2d-tsne-3d-umap-with-rapids)\n* [ðŸ“Š Automatic EDA Libraries ðŸ“š Comparisson](https://www.kaggle.com/andreshg/automatic-eda-libraries-comparisson/notebook#4.-%F0%9F%93%8A-SweetViz-%F0%9F%93%9A)\n* [Time Series Basic Analysis](https://www.kaggle.com/sandipdatta/time-series-basic-analysis)\n* [PyCaret Introduction (Classification & Regression)](https://www.kaggle.com/frtgnn/pycaret-introduction-classification-regression)\n* https://www.dataschool.io/python-pandas-tips-and-tricks/\n* https://github.com/rougier/numpy-100\n* https://www.kaggle.com/abhi170599/bitcoin-price-prediction-with-machine-learning","metadata":{"papermill":{"duration":0.063205,"end_time":"2021-03-26T17:17:31.559281","exception":false,"start_time":"2021-03-26T17:17:31.496076","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<a class=\"anchor\" id=\"0.1\"></a>\n## Table of Contents\n\n1. [Import main libraries](#1)\n1. [Data download](#2)\n1. [Auxiliary functions](#3)\n    - [Tip 3.1. Time is taken for notebook execution (in H:M:S)](#3.1)\n    - [Tip 3.2. Time is taken for notebook execution (with smart progress meter)](#3.2)\n1. [EDA & Data cleaning](#4)\n    - [Tip 4.1. Automatic EDA (AutoViz)](#4.1)\n    - [Tip 4.2. Automatic EDA (Pandas-profiling)](#4.2)\n    - [Tip 4.3. PCA and 3D plot](#4.3)\n    - [Tip 4.4. Determining the number of clusters (Kmeans)](#4.4)    \n    - [Tip 4.5. TSNE with RAPIDS (2D with matlplotlib and plotly)](#4.5)\n    - [Tip 4.6. UMAP with RAPIDS (3D with plotly)](#4.6)\n    - [Tip 4.7. Automatic EDA (SweetViz)](#4.7)\n    - [Tip 4.8. SweetViz: comparing two subsets of the same dataframe (e.g. Male vs Female)](#4.8)\n    - [Tip 4.9. Time Series Stationarity Check](#4.9)    \n1. [FE](#5)\n    - [Tip 5.1. Reduce memory usage for DataFrame](#5.1)\n    - [Tip 5.2. Unique values for all columns in DataFrame](#5.2)\n    - [Tip 5.3. Data grouping in DataFrame](#5.3)\n    - [Tip 5.4. Creation double and triple features](#5.4)\n    - [Tip 5.5. Search and encoding categorical columns](#5.5)    \n    - [Tip 5.6. Creation feature with pandas.between](#5.6)\n    - [Tip 5.7. Filtering anomalous values](#5.7)\n    - [Tip 5.8 (begin). Feature selection](#5.8)\n     -  [Tip 5.9. FS with the Pearson correlation](#5.8.1)\n     -  [Tip 5.10. FS by the SelectFromModel with LinearSVC](#5.8.2) \n     -  [Tip 5.11. FS by the SelectFromModel with Lasso](#5.8.3) \n     -  [Tip 5.12. FS by the SelectKBest with Chi-2](#5.8.4)\n     -  [Tip 5.13. FS by the Recursive Feature Elimination (RFE) with Logistic Regression](#5.8.5) \n     -  [Tip 5.14. FS by the Recursive Feature Elimination (RFE) with Random Forest](#5.8.6)\n     -  [Tip 5.15. FS by the VarianceThreshold](#5.8.7)             \n    -  [Tip 5.8 (end). Selection the best features](#5.8.8)\n1. [Modeling](#6)\n    - [Tip 6.1. Splitting data with ShuffleSplit & KFold](#6.1)\n    - [Tip 6.2. Xgboost model training with GridSearchCV and Plot tree](#6.2)\n    - [Tip 6.3. Ensemble of models on basic of the xgboost](#6.3)\n    - [Tip 6.4. Modeling with PyCaret](#6.4)\n1. [Analysis and visualization of modeling results](#7)\n    - [Tip 7.1. Drawing confuse matrix](#7.1)\n    - [Tip 7.2. Drawing learning_curve plot](#7.2)\n    - [Tip 7.3. Drawing validation_curve plot](#7.3)\n    - [Tip 7.4. Drawing ROC_AUC plot](#7.4)\n    - [Tip 7.5. Drawing plot with prediction and target data (Plotly)](#7.5)\n    - [Tip 7.6. Explanation model predictions with SHAP](#7.6)    \n1. [BONUS](#8)\n    - [Tip 8.1. Submission data from DataFrame to Kaggle competition](#8.1)","metadata":{"papermill":{"duration":0.063806,"end_time":"2021-03-26T17:17:31.686175","exception":false,"start_time":"2021-03-26T17:17:31.622369","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## 1. Import main libraries<a class=\"anchor\" id=\"1\"></a>\n\n[Back to Table of Contents](#0.1)","metadata":{"papermill":{"duration":0.062234,"end_time":"2021-03-26T17:17:31.812471","exception":false,"start_time":"2021-03-26T17:17:31.750237","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, accuracy_score, confusion_matrix\n\nimport warnings\nwarnings.simplefilter('ignore')","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.953499,"end_time":"2021-03-26T17:17:32.953816","exception":false,"start_time":"2021-03-26T17:17:32.000317","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-12T20:50:38.830418Z","iopub.execute_input":"2021-08-12T20:50:38.830801Z","iopub.status.idle":"2021-08-12T20:50:39.869947Z","shell.execute_reply.started":"2021-08-12T20:50:38.830699Z","shell.execute_reply":"2021-08-12T20:50:39.868935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### See more in the notebook [50 Tips: Data Science (tabular data) for beginner](https://www.kaggle.com/vbmokin/50-tips-data-science-tabular-data-for-beginner)","metadata":{}},{"cell_type":"markdown","source":"## 2. Data download<a class=\"anchor\" id=\"2\"></a>\n\n[Back to Table of Contents](#0.1)","metadata":{"papermill":{"duration":0.073817,"end_time":"2021-03-26T17:17:44.851097","exception":false,"start_time":"2021-03-26T17:17:44.77728","status":"completed"},"tags":[]}},{"cell_type":"code","source":"data_titanic = pd.read_csv('../input/titanic/train.csv')\ndata_titanic.head(3)","metadata":{"papermill":{"duration":0.132066,"end_time":"2021-03-26T17:17:45.204733","exception":false,"start_time":"2021-03-26T17:17:45.072667","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-12T20:50:39.875697Z","iopub.execute_input":"2021-08-12T20:50:39.876082Z","iopub.status.idle":"2021-08-12T20:50:39.930253Z","shell.execute_reply.started":"2021-08-12T20:50:39.876044Z","shell.execute_reply":"2021-08-12T20:50:39.929321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_titanic_num = data_titanic[['Sex', 'Age', 'SibSp', 'Parch', 'Embarked', 'Survived']].copy()\ndata_titanic_num[\"Sex\"] = data_titanic_num[\"Sex\"].map({\"female\":0, \"male\":1})\ndata_titanic_num[\"Embarked\"] = data_titanic_num[\"Embarked\"].map({\"S\":0, \"C\":1, \"Q\": 2})\ndata_titanic_num = data_titanic_num.dropna()  # without NAN \ndata_titanic_num","metadata":{"execution":{"iopub.status.busy":"2021-08-12T20:50:39.934292Z","iopub.execute_input":"2021-08-12T20:50:39.936747Z","iopub.status.idle":"2021-08-12T20:50:39.994352Z","shell.execute_reply.started":"2021-08-12T20:50:39.936702Z","shell.execute_reply":"2021-08-12T20:50:39.993349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_health = pd.read_csv(\"../input/heart-disease-uci/heart.csv\")\ndata_health = data_health.drop_duplicates()\ndata_health.tail(3)","metadata":{"papermill":{"duration":0.103907,"end_time":"2021-03-26T17:17:45.383251","exception":false,"start_time":"2021-03-26T17:17:45.279344","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-12T20:50:39.998765Z","iopub.execute_input":"2021-08-12T20:50:40.001133Z","iopub.status.idle":"2021-08-12T20:50:40.040595Z","shell.execute_reply.started":"2021-08-12T20:50:40.001094Z","shell.execute_reply":"2021-08-12T20:50:40.039837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_water = pd.read_csv('../input/ammonium-prediction-in-river-water/PB_1996_2019_NH4.csv', sep=';')\ndata_water.tail(3)","metadata":{"papermill":{"duration":0.118234,"end_time":"2021-03-26T17:17:45.726489","exception":false,"start_time":"2021-03-26T17:17:45.608255","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-12T20:50:40.045115Z","iopub.execute_input":"2021-08-12T20:50:40.047107Z","iopub.status.idle":"2021-08-12T20:50:40.07859Z","shell.execute_reply.started":"2021-08-12T20:50:40.047066Z","shell.execute_reply":"2021-08-12T20:50:40.077529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_water.info()","metadata":{"papermill":{"duration":0.09846,"end_time":"2021-03-26T17:17:45.899906","exception":false,"start_time":"2021-03-26T17:17:45.801446","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-12T20:50:40.080106Z","iopub.execute_input":"2021-08-12T20:50:40.080434Z","iopub.status.idle":"2021-08-12T20:50:40.094449Z","shell.execute_reply.started":"2021-08-12T20:50:40.080399Z","shell.execute_reply":"2021-08-12T20:50:40.093054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_nlp = pd.read_csv('../input/nlp-reports-news-classification/water_problem_nlp_ua_for_Kaggle_100.csv', delimiter=';', \n                 header=0, encoding='cp1251')\ndata_nlp.tail(3)","metadata":{"papermill":{"duration":0.126121,"end_time":"2021-03-26T17:17:46.293343","exception":false,"start_time":"2021-03-26T17:17:46.167222","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-12T20:50:40.097902Z","iopub.execute_input":"2021-08-12T20:50:40.098176Z","iopub.status.idle":"2021-08-12T20:50:40.121395Z","shell.execute_reply.started":"2021-08-12T20:50:40.098149Z","shell.execute_reply":"2021-08-12T20:50:40.120479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Download one file\nimport requests\nprint('Download daily data from the Portal of RNBO of Ukraine')\nmyfile = requests.get('https://api-covid19.rnbo.gov.ua/charts/main-data?mode=ukraine&fbclid=IwAR1vNXEE0nkmorUmGP4StG4cLrj1Z9VoX3c3Bi8dfltr0elgOj4b0M3ONvk')\nopen('filename', 'wb').write(myfile.content)\ndata_covid = pd.read_json('filename')\ndata_covid[:3]","metadata":{"papermill":{"duration":0.415041,"end_time":"2021-03-26T17:17:47.950362","exception":false,"start_time":"2021-03-26T17:17:47.535321","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-08-12T20:50:40.123094Z","iopub.execute_input":"2021-08-12T20:50:40.123442Z","iopub.status.idle":"2021-08-12T20:50:42.38351Z","shell.execute_reply.started":"2021-08-12T20:50:40.123407Z","shell.execute_reply":"2021-08-12T20:50:42.382617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### See more in the notebook [50 Tips: Data Science (tabular data) for beginner](https://www.kaggle.com/vbmokin/50-tips-data-science-tabular-data-for-beginner)","metadata":{}},{"cell_type":"markdown","source":"## 3. Auxiliary functions<a class=\"anchor\" id=\"3\"></a>\n\n[Back to Table of Contents](#0.1)","metadata":{"papermill":{"duration":0.083794,"end_time":"2021-03-26T17:17:49.318778","exception":false,"start_time":"2021-03-26T17:17:49.234984","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### Tip 3.1. Time is taken for notebook execution (in H:M:S)<a class=\"anchor\" id=\"3.1\"></a>","metadata":{"papermill":{"duration":0.083244,"end_time":"2021-03-26T17:17:49.857212","exception":false,"start_time":"2021-03-26T17:17:49.773968","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"%%time - for one cell - This should be written in the very first line of the cell, even before the comments","metadata":{"papermill":{"duration":0.082857,"end_time":"2021-03-26T17:17:50.023635","exception":false,"start_time":"2021-03-26T17:17:49.940778","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# For any part of the notebook\nimport time\nstart_time = time.time()\nprint('Start time:', time.strftime(\"%H:%M:%S\",time.gmtime(start_time)))\n\n# ... code.... some cells\n\nsecond_time = time.time() - start_time\nprint('Time Taken:', time.strftime(\"%H:%M:%S\",time.gmtime(second_time)))","metadata":{"papermill":{"duration":0.096028,"end_time":"2021-03-26T17:17:50.204477","exception":false,"start_time":"2021-03-26T17:17:50.108449","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tip 3.2. Time is taken for notebook execution (with smart progress meter)<a class=\"anchor\" id=\"3.2\"></a>","metadata":{"papermill":{"duration":0.083605,"end_time":"2021-03-26T17:17:50.372981","exception":false,"start_time":"2021-03-26T17:17:50.289376","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# For the given code in the one cell\nfrom tqdm import tqdm\nx = 0\nfor i in tqdm(range(1000)):\n    x *= np.pi**100","metadata":{"papermill":{"duration":0.098918,"end_time":"2021-03-26T17:17:50.555744","exception":false,"start_time":"2021-03-26T17:17:50.456826","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### See more in the notebook [50 Tips: Data Science (tabular data) for beginner](https://www.kaggle.com/vbmokin/50-tips-data-science-tabular-data-for-beginner)","metadata":{}},{"cell_type":"markdown","source":"## 4. EDA & Data cleaning<a class=\"anchor\" id=\"4\"></a>\n\n[Back to Table of Contents](#0.1)","metadata":{"papermill":{"duration":0.08501,"end_time":"2021-03-26T17:17:51.167774","exception":false,"start_time":"2021-03-26T17:17:51.082764","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### Tip 4.1. Automatic EDA (AutoViz)<a class=\"anchor\" id=\"4.1\"></a>","metadata":{"papermill":{"duration":0.090767,"end_time":"2021-03-26T17:17:54.55817","exception":false,"start_time":"2021-03-26T17:17:54.467403","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"This code for AutoViz used EDA tool from the kernel [Data Visualization in just one line of code!!](https://www.kaggle.com/nareshbhat/data-visualization-in-just-one-line-of-code)","metadata":{}},{"cell_type":"code","source":"# Autoviz for automatic EDA\n!pip install xlrd\n!pip install autoviz\nfrom autoviz.AutoViz_Class import AutoViz_Class","metadata":{"papermill":{"duration":17.641101,"end_time":"2021-03-26T17:18:12.290445","exception":false,"start_time":"2021-03-26T17:17:54.649344","status":"completed"},"tags":[],"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_health.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_health.to_csv('data_EDA.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"AV = AutoViz_Class()\ndata = pd.read_csv('./data_EDA.csv')\ndf = AV.AutoViz(filename=\"\", sep=',', depVar='target', dfte=data, header=0, verbose=1, lowess=False, \n                chart_format='svg',  max_cols_analyzed=15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### See more in the notebook [50 Tips: Data Science (tabular data) for beginner](https://www.kaggle.com/vbmokin/50-tips-data-science-tabular-data-for-beginner)","metadata":{}},{"cell_type":"markdown","source":"### Tip 4.2. Automatic EDA (Pandas-profiling)<a class=\"anchor\" id=\"4.2\"></a>","metadata":{}},{"cell_type":"code","source":"!pip install pandas-profiling==2.11.0","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas_profiling as pp\npp.ProfileReport(data_health)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tip 4.3. PCA and 3D plot <a class=\"anchor\" id=\"4.3\"></a>","metadata":{}},{"cell_type":"code","source":"data_health.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = data_health[['sex', 'cp', 'trestbps', 'fbs', 'restecg', 'target']].copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA(n_components=3)\n#df = data_titanic_num.copy()\n#target = df.pop('Survived')\n#df = data_health.copy()\ntarget = df.pop('target')\npca.fit(df)\npca_samples = pca.transform(df)\nps = pd.DataFrame(pca_samples)\nps.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Thanks to https://www.kaggle.com/tanetboss/user-clustering-for-anime-recommendation (clustering)\nfrom mpl_toolkits.mplot3d import Axes3D\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (6, 4)\nplt.style.use('ggplot')\n%config InlineBackend.figure_formats = {'png', 'retina'}\n\ntocluster = pd.DataFrame(ps[[0,1,2]])\nplt.rcParams['figure.figsize'] = (16, 9)\n\nfig = plt.figure()\nax = Axes3D(fig)\nax.scatter(tocluster[0], tocluster[2], tocluster[1])\n\nplt.title('Data points in 3D PCA axis', fontsize=20)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tip 4.4. Determining the number of clusters (Kmeans)<a class=\"anchor\" id=\"4.4\"></a>","metadata":{}},{"cell_type":"code","source":"# Thanks to https://www.kaggle.com/tanetboss/user-clustering-for-anime-recommendation (clustering)\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nn_max = 15\nn_start = 2\nscores = []\ninertia_list = np.empty(n_max)\n\nfor i in range(n_start,n_max):\n    kmeans = KMeans(n_clusters=i)\n    kmeans.fit(tocluster)\n    inertia_list[i] = kmeans.inertia_\n    scores.append(silhouette_score(tocluster, kmeans.labels_))\n\nn_max_shift = 2  # find maximum after this index of score\nn_clusters = np.argmax(scores[n_max_shift:])+(n_start+n_max_shift) # it's my upgrade\nplt.plot(range(0,n_max),inertia_list,'-o')\nplt.xlabel('Number of cluster')\nplt.axvline(x=n_clusters, color='blue', linestyle='--')\nplt.ylabel('Inertia')\nplt.show()\n\nplt.plot(range(2,n_max), scores);\nplt.title('Results KMeans')\nplt.xlabel('n_clusters');\nplt.axvline(x=n_clusters, color='blue', linestyle='--')\nplt.ylabel('Silhouette Score');\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The maximum value of Silhouette Score should be as close as possible to 1 (at least more than 0.7), otherwise, another clustering method should be used.","metadata":{}},{"cell_type":"code","source":"# Thanks to https://www.kaggle.com/tanetboss/user-clustering-for-anime-recommendation (clustering)\nclusterer = KMeans(n_clusters=n_clusters,random_state=30).fit(tocluster)\ncenters = clusterer.cluster_centers_\nc_preds = clusterer.predict(tocluster)\n\nprint(centers)\n\nfig = plt.figure()\nax = Axes3D(fig)\nax.scatter(tocluster[0], tocluster[2], tocluster[1], c = c_preds)\nplt.title('Data points in 3D PCA axis', fontsize=20)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Thanks to https://www.kaggle.com/tanetboss/user-clustering-for-anime-recommendation (clustering)\nfig = plt.figure(figsize=(10,8))\nplt.scatter(tocluster[1],tocluster[0],c = c_preds)\nfor ci,c in enumerate(centers):\n    plt.plot(c[1], c[0], 'o', markersize=8, color='red', alpha=1)\n\nplt.xlabel('x_values')\nplt.ylabel('y_values')\n\nplt.title('Data points in 2D PCA axis', fontsize=20)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tip 4.5. TSNE with RAPIDS (2D with matlplotlib and plotly)<a class=\"anchor\" id=\"4.5\"></a>","metadata":{}},{"cell_type":"markdown","source":"Thanks to [MNIST Original : 2D tSNE, 3D UMAP with RAPIDS](https://www.kaggle.com/vbmokin/mnist-original-2d-tsne-3d-umap-with-rapids)","metadata":{}},{"cell_type":"code","source":"# Thanks to https://www.kaggle.com/tunguz/mnist-2d-t-sne-with-rapids\n# You need Add Kaggle dataset https://www.kaggle.com/cdeotte/rapids - takes 1 min\nimport sys\n!cp ../input/rapids/rapids.0.18.0 /opt/conda/envs/rapids.tar.gz\n!cd /opt/conda/envs/ && tar -xzvf rapids.tar.gz\nsys.path = [\"/opt/conda/envs/rapids/lib\"] + [\"/opt/conda/envs/rapids/lib/python3.6\"] + [\"/opt/conda/envs/rapids/lib/python3.6/site-packages\"] + sys.path\n!cp /opt/conda/envs/rapids/lib/libxgboost.so /opt/conda/lib/","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 2D with TSNE\nimport cudf, cuml\nfrom cuml.manifold import TSNE","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = data_health.copy()\ndata","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tsne = TSNE(n_components=2)\ndata_2D = tsne.fit_transform(data)\nplt.scatter(data_2D[:,0], data_2D[:,1], c = data['target'], s = 40)","metadata":{"_kg_hide-output":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.express as px\nfig = px.scatter(\n    x=data_2D[:, 0], y=data_2D[:, 1], \n    color=data['target'], hover_name=data['target'].astype('str'), height=700)\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tip 4.6. UMAP with RAPIDS (3D with plotly)<a class=\"anchor\" id=\"4.6\"></a>","metadata":{}},{"cell_type":"markdown","source":"Thanks to [MNIST Original : 2D tSNE, 3D UMAP with RAPIDS](https://www.kaggle.com/vbmokin/mnist-original-2d-tsne-3d-umap-with-rapids)","metadata":{}},{"cell_type":"code","source":"# # Thanks to https://www.kaggle.com/tunguz/mnist-2d-t-sne-with-rapids\n# # You need Add Kaggle dataset https://www.kaggle.com/cdeotte/rapids - takes 1 min\n# import sys\n# !cp ../input/rapids/rapids.0.18.0 /opt/conda/envs/rapids.tar.gz\n# !cd /opt/conda/envs/ && tar -xzvf rapids.tar.gz\n# sys.path = [\"/opt/conda/envs/rapids/lib\"] + [\"/opt/conda/envs/rapids/lib/python3.6\"] + [\"/opt/conda/envs/rapids/lib/python3.6/site-packages\"] + sys.path\n# !cp /opt/conda/envs/rapids/lib/libxgboost.so /opt/conda/lib/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Thanks to https://www.kaggle.com/tunguz/mnist-2d-t-sne-with-rapids\n# Thanks to https://www.kaggle.com/xhlulu/disasternlp-t-sne-with-tfhub-rapids-plotly\n#import cudf, cuml\nfrom cuml.manifold import UMAP\ndata = data_health.copy()\numap3d = UMAP(n_components=3)\ndata_3D = umap3d.fit_transform(data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.express as px\nfig = px.scatter_3d(\n    x=data_3D[:, 0], y=data_3D[:, 1], z=data_3D[:, 2], \n    color=data_health['target'], hover_name=data_health['target'].astype('str'), size_max=3, height=700)\nfig.update_traces(marker=dict(size=3))\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tip 4.7. Automatic EDA (SweetViz)<a class=\"anchor\" id=\"4.7\"></a>","metadata":{}},{"cell_type":"markdown","source":"Thenks to https://www.kaggle.com/andreshg/automatic-eda-libraries-comparisson/notebook#4.-%F0%9F%93%8A-SweetViz-%F0%9F%93%9A","metadata":{}},{"cell_type":"code","source":"!pip install sweetviz","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sweetviz as sv\ndata_report = sv.analyze([data_health, 'Data'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save result to Output section of the notebook\ndata_report.show_html()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tip 4.8. SweetViz: comparing two subsets of the same dataframe (e.g. Male vs Female)<a class=\"anchor\" id=\"4.8\"></a>","metadata":{}},{"cell_type":"markdown","source":"Thanks to [documentation of SweetViz](https://pypi.org/project/sweetviz/)","metadata":{}},{"cell_type":"code","source":"data_health_sex = data_health.copy()\ndata_health_sex['sex'] = data_health_sex['sex'].map({1 : \"male\", 0 : \"female\"})\ndata_health_sex","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_report = sv.compare_intra(data_health_sex, data_health_sex[\"sex\"] == \"male\", [\"Male\", \"Female\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save result to Output section of the notebook\nmy_report.show_html()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tip 4.9. Time Series Stationarity Check<a class=\"anchor\" id=\"4.9\"></a>","metadata":{}},{"cell_type":"markdown","source":"Thanks to [Time Series Basic Analysis](https://www.kaggle.com/sandipdatta/time-series-basic-analysis)","metadata":{}},{"cell_type":"code","source":"# Dickey Fuller Test Function\ndef test_stationarity(timeseries):\n    # Perform Dickey-Fuller test:\n    from statsmodels.tsa.stattools import adfuller\n    print('Results of Dickey-Fuller Test:')\n    print (\"==============================================\")\n    \n    dftest = adfuller(timeseries, autolag='AIC')\n    \n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic', 'p-value', '#lags Used', 'Number of Observations Used'])\n    \n    for key, value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    \n    print(dfoutput)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T20:50:48.728115Z","iopub.execute_input":"2021-08-12T20:50:48.728441Z","iopub.status.idle":"2021-08-12T20:50:48.734833Z","shell.execute_reply.started":"2021-08-12T20:50:48.72841Z","shell.execute_reply":"2021-08-12T20:50:48.733747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"timeseries = data_water['NH4'].replace(np.nan, 0)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T20:50:51.240472Z","iopub.execute_input":"2021-08-12T20:50:51.240814Z","iopub.status.idle":"2021-08-12T20:50:51.24687Z","shell.execute_reply.started":"2021-08-12T20:50:51.240778Z","shell.execute_reply":"2021-08-12T20:50:51.245736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Stationarity Check - Lets do a quick check on Stationarity with Dickey Fuller Test \n# Convert the DF to series first\ntest_stationarity(timeseries)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T20:50:52.983681Z","iopub.execute_input":"2021-08-12T20:50:52.984086Z","iopub.status.idle":"2021-08-12T20:50:53.421884Z","shell.execute_reply.started":"2021-08-12T20:50:52.984052Z","shell.execute_reply":"2021-08-12T20:50:53.420843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Conclusion**\n\nThe Test Statistics value is smaller than critical value. So we can reject the Null Hypothesis.\nHence Statistically (and obviously from the plot) the Time series is Stationary.","metadata":{}},{"cell_type":"code","source":"# Dickey Fuller Test Function with plots\n# Thanks to https://www.kaggle.com/abhi170599/bitcoin-price-prediction-with-machine-learning\ndef evaluate_stationarity(timeseries, timeframe=7):\n    from statsmodels.tsa.stattools import adfuller, acf, pacf\n    roll_mean = timeseries.rolling(window=timeframe).mean()\n    roll_std  = timeseries.rolling(window=timeframe).std()\n    \n    # plot the rolling statistics\n    orig = plt.plot(timeseries,color='blue',label='Original')\n    mean = plt.plot(roll_mean,color='red',label='Rolling Mean')\n    std  = plt.plot(roll_std,color='black',label='Rolling Std')\n    plt.legend(loc='best')\n    plt.title('Rolling Mean and Standard Deviation')\n    plt.show(block=False)\n    \n    # perform ADF test\n    df_test = adfuller(timeseries,autolag='AIC')\n    df_output = pd.Series(df_test[0:4],index=['Test Statistics','p-value','#lags used','Number of Observations Used'])\n    for k,v in df_test[4].items():\n        df_output['Critical Value ({})'.format(k)]=v\n    print(df_output)    ","metadata":{"execution":{"iopub.status.busy":"2021-08-12T20:52:24.425272Z","iopub.execute_input":"2021-08-12T20:52:24.425604Z","iopub.status.idle":"2021-08-12T20:52:24.445376Z","shell.execute_reply.started":"2021-08-12T20:52:24.425575Z","shell.execute_reply":"2021-08-12T20:52:24.441854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_stationarity(timeseries, timeframe=30)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T20:52:28.994938Z","iopub.execute_input":"2021-08-12T20:52:28.99528Z","iopub.status.idle":"2021-08-12T20:52:29.421653Z","shell.execute_reply.started":"2021-08-12T20:52:28.995235Z","shell.execute_reply":"2021-08-12T20:52:29.420731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_stationarity(timeseries, timeframe=7)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T20:52:54.834712Z","iopub.execute_input":"2021-08-12T20:52:54.835062Z","iopub.status.idle":"2021-08-12T20:52:55.254061Z","shell.execute_reply.started":"2021-08-12T20:52:54.835033Z","shell.execute_reply":"2021-08-12T20:52:55.253157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. FE<a class=\"anchor\" id=\"5\"></a>\n\n[Back to Table of Contents](#0.1)","metadata":{"papermill":{"duration":0.188383,"end_time":"2021-03-26T17:18:12.668442","exception":false,"start_time":"2021-03-26T17:18:12.480059","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### Tip 5.1. Reduce memory usage for DataFrame<a class=\"anchor\" id=\"5.1\"></a>","metadata":{"papermill":{"duration":0.193918,"end_time":"2021-03-26T17:18:13.100927","exception":false,"start_time":"2021-03-26T17:18:12.907009","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","metadata":{"papermill":{"duration":0.204876,"end_time":"2021-03-26T17:18:13.494963","exception":false,"start_time":"2021-03-26T17:18:13.290087","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reduce_mem_usage(data_titanic)","metadata":{"papermill":{"duration":0.22998,"end_time":"2021-03-26T17:18:13.911659","exception":false,"start_time":"2021-03-26T17:18:13.681679","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tip 5.2. Unique values for all columns in DataFrame<a class=\"anchor\" id=\"5.2\"></a>","metadata":{"papermill":{"duration":0.190631,"end_time":"2021-03-26T17:18:14.290586","exception":false,"start_time":"2021-03-26T17:18:14.099955","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def rstr(df): \n    return df.apply(lambda x: [x.unique()])\nprint(rstr(data_titanic))","metadata":{"papermill":{"duration":0.216752,"end_time":"2021-03-26T17:18:14.69474","exception":false,"start_time":"2021-03-26T17:18:14.477988","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tip 5.3. Data grouping in DataFrame<a class=\"anchor\" id=\"5.3\"></a>","metadata":{"papermill":{"duration":0.202131,"end_time":"2021-03-26T17:18:20.082425","exception":false,"start_time":"2021-03-26T17:18:19.880294","status":"completed"},"tags":[]}},{"cell_type":"code","source":"data_titanic.groupby(['Sex', 'Pclass'])['Age'].apply(lambda x: x.fillna(x.median()))","metadata":{"papermill":{"duration":0.217624,"end_time":"2021-03-26T17:18:20.499617","exception":false,"start_time":"2021-03-26T17:18:20.281993","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_water[['ID_Station', 'Distance']].groupby(by=['Distance']).mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tip 5.4. Creation double and triple features <a class=\"anchor\" id=\"5.4\"></a>","metadata":{"papermill":{"duration":0.199674,"end_time":"2021-03-26T17:18:24.671134","exception":false,"start_time":"2021-03-26T17:18:24.47146","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def fe_creation(df):\n    df['age2'] = df['age']//10\n    df['trestbps2'] = df['trestbps']//10\n    df['chol2'] = df['chol']//40\n    df['thalach2'] = df['thalach']//40\n    df['oldpeak2'] = df['oldpeak']//0.4\n    for i in ['sex', 'age2', 'fbs', 'restecg', 'exang','thal', ]:\n        for j in ['cp','trestbps2', 'chol2', 'thalach2', 'oldpeak2', 'slope', 'ca']:\n            df[i + \"_\" + j] = df[i].astype('str') + \"_\" + df[j].astype('str')\n    return df","metadata":{"papermill":{"duration":0.343615,"end_time":"2021-03-26T17:18:25.213816","exception":false,"start_time":"2021-03-26T17:18:24.870201","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('max_columns', 70)\ndata_health = fe_creation(data_health)\ndata_health.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tip 5.5. Search and encoding categorical columns <a class=\"anchor\" id=\"5.5\"></a>","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\ndef df_encoding(df):\n    numerics = ['int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    categorical_columns = []\n    features = df.columns.values.tolist()\n    for col in features:\n        if df[col].dtype in numerics: continue\n        categorical_columns.append(col)\n    print('Categorical columns:', categorical_columns)\n\n    for col in categorical_columns:\n        if col in df.columns:\n            le = LabelEncoder()\n            le.fit(list(df[col].astype(str).values))\n            df[col] = le.transform(list(df[col].astype(str).values))\n    \n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_health = df_encoding(data_health)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_health.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_health.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tip 5.6. Creation feature with pandas.between <a class=\"anchor\" id=\"5.6\"></a>","metadata":{"papermill":{"duration":0.199938,"end_time":"2021-03-26T17:18:25.617135","exception":false,"start_time":"2021-03-26T17:18:25.417197","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# In progress... (from my notebook for NY Taxi competition)\n# df ['jrk'] = 0\n# df.loc[(df['pickup_longitude'].between(-73.7841,-73.7721)) & \n#    (df['pickup_latitude'].between(40.6613, 40.6213)),'jrk'] = 1\n# df.loc[(df['dropoff_longitude'].between(-73.7841, -73.7721)) &\n#    (df['dropoff_latitude'].between(40.6613, 40.6213)),'jrk'] = 1","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.209837,"end_time":"2021-03-26T17:18:26.026767","exception":false,"start_time":"2021-03-26T17:18:25.81693","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tip 5.7. Filtering anomalous values <a class=\"anchor\" id=\"5.7\"></a>","metadata":{"papermill":{"duration":0.199457,"end_time":"2021-03-26T17:18:26.426997","exception":false,"start_time":"2021-03-26T17:18:26.22754","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# In progress... (from my notebook for NY Taxi competition)\n# outliers=np.array([False]*len(data))\n# Out = pd.DataFrame()\n# Out = Out.assign(outliers=outliers)\n# outliers[data.fare_amount > 100]=True\n# outliers[data.fare_amount < 2.8]=True\n# outliers[data.distance > 5]=True\n# outliers[data.distance < 0]=True\n# outliers[data.passenger_count < 1]=True\n# outliers[data.passenger_count > 6]=True\n# outliers[data.pickup_longitude > -72.9]=True\n# outliers[data.pickup_longitude < -74.5]=True\n# outliers[data.pickup_latitude > 42]=True\n# outliers[data.pickup_latitude < 40]=True\n# outliers[data.dropoff_longitude > -72.9]=True\n# outliers[data.dropoff_longitude < -74.5]=True\n# outliers[data.pickup_latitude > 42]=True\n# outliers[data.pickup_latitude < 40]=True\n# print('There are total %d entries of ouliers'% sum(outliers))\n# X = data[outliers==False].iloc[:,3:F]\n# z = data[outliers==False]['fare_amount'].values","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.206946,"end_time":"2021-03-26T17:18:26.833959","exception":false,"start_time":"2021-03-26T17:18:26.627013","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tip 5.8 (begin). Feature selection <a class=\"anchor\" id=\"5.8\"></a>","metadata":{}},{"cell_type":"markdown","source":"This tip from the notebook [Heart Disease - Automatic AdvEDA & FE & 20 models](https://www.kaggle.com/vbmokin/heart-disease-automatic-adveda-fe-20-models)","metadata":{}},{"cell_type":"markdown","source":"There are many techniques for **selection features**, see example:\n- [sklearn library documentation](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection)\n- [a collection of notebooks](https://www.kaggle.com/vbmokin/data-science-for-tabular-data-advanced-techniques#3)\n- my notebook [Titanic - Featuretools (automatic FE&FS)](https://www.kaggle.com/vbmokin/titanic-featuretools-automatic-fe-fs)\n- my notebook [Merging FE & Prediction - xgb, lgb, logr, linr](https://www.kaggle.com/vbmokin/merging-fe-prediction-xgb-lgb-logr-linr)","metadata":{}},{"cell_type":"code","source":"train = data_health.copy()\ntarget = train.pop('target')\ntrain.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_features_opt = 25   # the number of features that we need to choose as a result\nnum_features_max = 35   # the somewhat excessive number of features, which we will choose at each stage\nfeatures_best = []","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tip 5.9. FS with the Pearson correlation<a class=\"anchor\" id=\"5.8.1\"></a>","metadata":{}},{"cell_type":"markdown","source":"Thanks to:\n\n* FE from the https://www.kaggle.com/liananapalkova/automated-feature-engineering-for-titanic-dataset\n\n* Visualization from the https://www.kaggle.com/vbmokin/three-lines-of-code-for-titanic-top-20","metadata":{}},{"cell_type":"code","source":"# Threshold for removing correlated variables\nthreshold = 0.9\n\ndef highlight(value):\n    if value > threshold:\n        style = 'background-color: pink'\n    else:\n        style = 'background-color: palegreen'\n    return style\n\n# Absolute value correlation matrix\ncorr_matrix = data_health.corr().abs().round(2)\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\nupper.style.format(\"{:.2f}\").applymap(highlight)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Select columns with correlations above threshold\ncollinear_features = [column for column in upper.columns if any(upper[column] > threshold)]\nfeatures_filtered = data_health.drop(columns = collinear_features)\nprint('The number of features that passed the collinearity threshold: ', features_filtered.shape[1])\nfeatures_best.append(features_filtered.columns.tolist())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tip 5.10. FS by the SelectFromModel with LinearSVC <a class=\"anchor\" id=\"5.8.2\"></a>","metadata":{}},{"cell_type":"markdown","source":"Thanks to https://www.kaggle.com/liananapalkova/automated-feature-engineering-for-titanic-dataset","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import LinearSVC\nfrom sklearn.feature_selection import SelectFromModel\nlsvc = LinearSVC(C=0.1, penalty=\"l1\", dual=False).fit(train, target)\nmodel = SelectFromModel(lsvc, prefit=True)\nX_new = model.transform(train)\nX_selected_df = pd.DataFrame(X_new, columns=[train.columns[i] for i in range(len(train.columns)) if model.get_support()[i]])\nfeatures_best.append(X_selected_df.columns.tolist())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tip 5.11. FS by the SelectFromModel with Lasso <a class=\"anchor\" id=\"5.8.3\"></a>","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LassoCV\nfrom sklearn.feature_selection import SelectFromModel\nlasso = LassoCV(cv=3).fit(train, target)\nmodel = SelectFromModel(lasso, prefit=True)\nX_new = model.transform(train)\nX_selected_df = pd.DataFrame(X_new, columns=[train.columns[i] for i in range(len(train.columns)) if model.get_support()[i]])\nfeatures_best.append(X_selected_df.columns.tolist())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tip 5.12. FS by the SelectKBest with Chi-2 <a class=\"anchor\" id=\"5.8.4\"></a>","metadata":{}},{"cell_type":"markdown","source":"Thanks to:\n* https://www.kaggle.com/sz8416/6-ways-for-feature-selection\n* https://towardsdatascience.com/feature-selection-techniques-in-machine-learning-with-python-f24e7da3f36e","metadata":{}},{"cell_type":"code","source":"# Visualization from https://towardsdatascience.com/feature-selection-techniques-in-machine-learning-with-python-f24e7da3f36e\n# but to k='all'\nfrom sklearn.feature_selection import SelectKBest, chi2\nbestfeatures = SelectKBest(score_func=chi2, k='all')\nfit = bestfeatures.fit(train, target)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(train.columns)\n\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Feature','Score']  #naming the dataframe columns\nfeatures_best.append(featureScores.nlargest(num_features_max,'Score')['Feature'].tolist())\nprint(featureScores.nlargest(len(dfcolumns),'Score')) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tip 5.13. FS by the Recursive Feature Elimination (RFE) with Logistic Regression<a class=\"anchor\" id=\"5.8.5\"></a>","metadata":{}},{"cell_type":"markdown","source":"Thanks to:\n* https://www.kaggle.com/sz8416/6-ways-for-feature-selection\n* https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import RFE\nrfe_selector = RFE(estimator=LogisticRegression(), n_features_to_select=num_features_max, step=10, verbose=5)\nrfe_selector.fit(train, target)\nrfe_support = rfe_selector.get_support()\nrfe_feature = train.loc[:,rfe_support].columns.tolist()\nfeatures_best.append(rfe_feature)\nprint(str(len(rfe_feature)), 'selected features')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tip 5.14. FS by the Recursive Feature Elimination (RFE) with Random Forest<a class=\"anchor\" id=\"5.8.6\"></a>","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import SelectFromModel\nembeded_rf_selector = SelectFromModel(RandomForestClassifier(n_estimators=200), threshold='1.25*median')\nembeded_rf_selector.fit(train, target)\nembeded_rf_support = embeded_rf_selector.get_support()\nembeded_rf_feature = train.loc[:,embeded_rf_support].columns.tolist()\nfeatures_best.append(embeded_rf_feature)\nprint(str(len(embeded_rf_feature)), 'selected features')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tip 5.15. FS by the VarianceThreshold<a class=\"anchor\" id=\"5.8.7\"></a>","metadata":{}},{"cell_type":"markdown","source":"Thanks to [sklearn.feature_selection.VarianceThreshold](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html#sklearn.feature_selection.VarianceThreshold) - Feature selector that removes all low-variance features.","metadata":{}},{"cell_type":"code","source":"# Check whether all features have a sufficiently different meaning\nfrom sklearn.feature_selection import VarianceThreshold\nselector = VarianceThreshold(threshold=10)\nselector.fit_transform(data_health)\nVarianceThreshold_feature = list(np.array(data_health.columns)[selector.get_support(indices=False)])\nfeatures_best.append(VarianceThreshold_feature)\nprint(str(len(VarianceThreshold_feature)), 'selected features')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tip 5.8 (end). Selection the best features<a class=\"anchor\" id=\"5.8.8\"></a>","metadata":{}},{"cell_type":"code","source":"features_best","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The element is in at least one list of optimal features\nmain_cols_max = features_best[0]\nfor i in range(len(features_best)-1):\n    main_cols_max = list(set(main_cols_max) | set(features_best[i+1]))\nmain_cols_max","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The element is in all lists of optimal features\nmain_cols_min = features_best[0]\nfor i in range(len(features_best)-1):\n    main_cols_min = list(set(main_cols_min).intersection(set(features_best[i+1])))\nmain_cols_min","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Most common items in all lists of optimal features\nmain_cols = []\nmain_cols_opt = {feature_name : 0 for feature_name in data_health.columns.tolist()}\nfor i in range(len(features_best)):\n    for feature_name in features_best[i]:\n        main_cols_opt[feature_name] += 1\ndf_main_cols_opt = pd.DataFrame.from_dict(main_cols_opt, orient='index', columns=['Num'])\ndf_main_cols_opt.sort_values(by=['Num'], ascending=False).head(num_features_opt)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"main_cols = df_main_cols_opt.nlargest(num_features_opt, 'Num').index.tolist()\nif not 'target' in main_cols:\n    main_cols.append('target')\nmain_cols","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In progress...","metadata":{}},{"cell_type":"markdown","source":"#### See more in the notebook [50 Tips: Data Science (tabular data) for beginner](https://www.kaggle.com/vbmokin/50-tips-data-science-tabular-data-for-beginner)","metadata":{}},{"cell_type":"markdown","source":"## 6. Modeling<a class=\"anchor\" id=\"6\"></a>\n\n[Back to Table of Contents](#0.1)","metadata":{"papermill":{"duration":0.200438,"end_time":"2021-03-26T17:18:27.233211","exception":false,"start_time":"2021-03-26T17:18:27.032773","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### Tip 6.1. Splitting data with ShuffleSplit & KFold<a class=\"anchor\" id=\"6.1\"></a>","metadata":{"papermill":{"duration":0.201305,"end_time":"2021-03-26T17:18:27.634407","exception":false,"start_time":"2021-03-26T17:18:27.433102","status":"completed"},"tags":[]}},{"cell_type":"code","source":"data_titanic_num.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = data_titanic_num.copy()\nz = X.pop('Survived')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split data to 2 (or more - train/valid/test) subsets\nfrom sklearn.model_selection import train_test_split, ShuffleSplit, KFold\n\n# Split training set to validation subsets\nXtrain, Xval, Ztrain, Zval = train_test_split(X, z, test_size=0.2, random_state=0)\nXtrain.shape","metadata":{"papermill":{"duration":0.225736,"end_time":"2021-03-26T17:18:28.067147","exception":false,"start_time":"2021-03-26T17:18:27.841411","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Random permutation cross-validator\ncv_train = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)","metadata":{"papermill":{"duration":0.209649,"end_time":"2021-03-26T17:18:28.478006","exception":false,"start_time":"2021-03-26T17:18:28.268357","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# K-Folds cross-validator\nkf = KFold(n_splits=5, shuffle = True)","metadata":{"papermill":{"duration":0.209638,"end_time":"2021-03-26T17:18:28.887166","exception":false,"start_time":"2021-03-26T17:18:28.677528","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tip 6.2. Xgboost model training with GridSearchCV and Plot tree<a class=\"anchor\" id=\"6.2\"></a>","metadata":{"papermill":{"duration":0.200315,"end_time":"2021-03-26T17:18:29.28928","exception":false,"start_time":"2021-03-26T17:18:29.088965","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import xgboost as xgb\nfrom xgboost import plot_tree\nfrom matplotlib.pylab import rcParams\nfrom sklearn.model_selection import GridSearchCV\n\ndef xgb_training(train, target_train, cv_train):\n    # XGB Classifier Training\n    eval_metric_model = 'error'\n    xgbr = xgb.XGBClassifier() \n    param_grid_xgb = {'objective': ['binary:hinge'],\n                     'eval_metric': [eval_metric_model],\n                     'random_state': [0]}\n\n    # Training model\n    xgb_CV = GridSearchCV(xgbr, param_grid=param_grid_xgb, cv=cv_train, verbose=False)\n    xgb_CV.fit(train, target_train)\n    xgbp = xgb_CV.best_params_\n    print(xgbp)\n\n    # Feature importance diagrame\n    xgb_model = xgb.XGBClassifier(objective='binary:hinge',\n                                  eval_metric='error',\n                                  random_state=0).fit(train, target_train)\n    fig =  plt.figure(figsize = (15,15))\n    axes = fig.add_subplot(111)\n    xgb.plot_importance(xgb_model,ax = axes,height = 0.5)\n    plt.show();plt.close()\n    \n    # Plot single tree\n    plt.rcParams.update({'font.size': 8})\n    rcParams['figure.figsize'] = 80,40\n    plot_tree(xgb_model, num_trees=0, rankdir='LR')\n    ax = plot_tree(xgb_model, node_attr={'shape': 'record', 'height': '.1'}, show_info = 'split_gain')\n    plt.show()\n    \n    return xgb_CV, xgbp, xgb_model","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.51087,"end_time":"2021-03-26T17:18:30.000139","exception":false,"start_time":"2021-03-26T17:18:29.489269","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_CV, xgbp, xgb_model = xgb_training(Xtrain, Ztrain, cv_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tip 6.3. Ensemble of models on basic of the xgboost<a class=\"anchor\" id=\"6.3\"></a>","metadata":{"papermill":{"duration":0.202392,"end_time":"2021-03-26T17:18:30.418745","exception":false,"start_time":"2021-03-26T17:18:30.216353","status":"completed"},"tags":[]}},{"cell_type":"code","source":"#print(xgbp)","metadata":{"papermill":{"duration":0.208181,"end_time":"2021-03-26T17:18:30.827915","exception":false,"start_time":"2021-03-26T17:18:30.619734","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # In progress...\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.metrics import accuracy_score\n# def vr_training(train, target_train, valid, target_valid, cv_train, result, knnp, rfp, xgbp):\n#     # Voting Classifier Training\n\n#     # Training model\n#     estimators = [('knn', KNeighborsClassifier()),\n#                   ('etr', ExtraTreesClassifier()),\n#                   ('rf', RandomForestClassifier()),\n#                   ('xgb', xgb.XGBClassifier(objective='binary:hinge',\n#                                             eval_metric='error',\n#                                             learning_rate=xgbp['learning_rate'],\n#                                             max_depth=xgbp['max_depth'],\n#                                             n_estimators=xgbp['n_estimators'],\n#                                             reg_lambda=xgbp['reg_lambda'],\n#                                             random_state=random_state))]\n#     model_vr = VotingClassifier(estimators=estimators)\n#     vr = GridSearchCV(estimator=model_vr, param_grid={}, cv=cv_train)\n#     vr.fit(train, target_train)\n\n#     # Prediction for training data\n#     y_train_vr = vr.predict(train)\n\n#     # Accuracy of model\n#     acc = round(accuracy_score(target_train, y_train_vr)*100,1)\n#     print(f'Accuracy of Voting Classifier model training is {acc}')\n\n#     # Print rounded acc to 2 decimal values after the text\n#     y_val_vr = vr.predict(valid)\n#     acc_valid = round(accuracy_score(target_valid, y_val_vr)*100,1)\n#     print(f'Accuracy of Voting Classifier model prediction for valid dataset is {acc_valid} \\n')\n    \n#     return vr","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.404979,"end_time":"2021-03-26T17:18:31.435222","exception":false,"start_time":"2021-03-26T17:18:31.030243","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tip 6.4. Modeling with PyCaret<a class=\"anchor\" id=\"6.4\"></a>","metadata":{}},{"cell_type":"code","source":"#!pip install --upgrade scikit-learn==0.23.2\n#!pip install pycaret","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from pycaret.classification import *","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = setup(data = data_titanic_num, \n#               target = 'Survived',\n#               numeric_imputation = 'mean',\n#               imputation_type='iterative',\n#               categorical_features = ['Sex','Embarked'],\n#               normalize = True,\n#               silent = True\n#              )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#compare_models()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In progress...","metadata":{"papermill":{"duration":0.20346,"end_time":"2021-03-26T17:18:31.841483","exception":false,"start_time":"2021-03-26T17:18:31.638023","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"#### See more in the notebook [50 Tips: Data Science (tabular data) for beginner](https://www.kaggle.com/vbmokin/50-tips-data-science-tabular-data-for-beginner)","metadata":{}},{"cell_type":"markdown","source":"## 7. Analysis and visualization of modeling results<a class=\"anchor\" id=\"7\"></a>\n\n[Back to Table of Contents](#0.1)","metadata":{"papermill":{"duration":0.201867,"end_time":"2021-03-26T17:18:32.244849","exception":false,"start_time":"2021-03-26T17:18:32.042982","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### Tip 7.1. Drawing confuse matrix<a class=\"anchor\" id=\"7.1\"></a>","metadata":{"papermill":{"duration":0.201758,"end_time":"2021-03-26T17:18:32.648111","exception":false,"start_time":"2021-03-26T17:18:32.446353","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# In progress...\nfrom sklearn.metrics import confusion_matrix\ndef plot_cm3(target_train, train_pred, target_valid, valid_pred, target_test, test_pred, title, figsize=(12,3)):\n    # Building the 3 confusion matrices train/valid/test\n    \n    def cm_calc(y_true, y_pred):\n        cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n        cm_sum = np.sum(cm, axis=1, keepdims=True)\n        cm_perc = cm / cm_sum.astype(float) * 100\n        annot = np.empty_like(cm).astype(str)\n        nrows, ncols = cm.shape\n        for i in range(nrows):\n            for j in range(ncols):\n                c = cm[i, j]\n                p = cm_perc[i, j]\n                if i == j:\n                    s = cm_sum[i]\n                    annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n                elif c == 0:\n                    annot[i, j] = ''\n                else:\n                    annot[i, j] = '%.1f%%\\n%d' % (p, c)\n        cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n        cm.index.name = 'Actual'\n        cm.columns.name = 'Predicted'\n        return cm, annot\n\n    \n    # Building the confusion matrices\n    # Tip.\n    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=figsize, sharex=True)\n#     font_size = 11\n#     SMALL_SIZE = 10\n#     MEDIUM_SIZE = 12\n#     BIGGER_SIZE = 14    \n#     plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n#     plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n#     plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n#     plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n#     plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n#     plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n#     plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n#     plt.rc('figure', titlesize=BIGGER_SIZE) \n#     #plt.rcParams.update({'font.size': font_size}) - alternative\n    \n    # Training data\n    ax = axes[0]\n    ax.set_title(\"for training data\")\n    cm0, annot0 = cm_calc(target_train, train_pred)    \n    sns.heatmap(cm0, cmap= \"YlGnBu\", annot=annot0, fmt='', ax=ax)\n    \n    # Validation data\n    ax = axes[1]\n    ax.set_title(\"for validation data\")\n    cm1, annot1 = cm_calc(target_valid, valid_pred)\n    sns.heatmap(cm1, cmap= \"YlGnBu\", annot=annot1, fmt='', ax=ax)\n\n    # Test data\n    ax = axes[2]\n    cm2, annot2 = cm_calc(target_test, test_pred)\n    sns.heatmap(cm2, cmap= \"YlGnBu\", annot=annot2, fmt='', ax=ax)\n    \n    fig.suptitle(title, y=1.05)\n    plt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2021-03-26T17:18:33.078843Z","iopub.status.busy":"2021-03-26T17:18:33.078143Z","iopub.status.idle":"2021-03-26T17:18:33.081477Z","shell.execute_reply":"2021-03-26T17:18:33.080871Z"},"papermill":{"duration":0.222887,"end_time":"2021-03-26T17:18:33.081615","exception":false,"start_time":"2021-03-26T17:18:32.858728","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ndef plot_cm2(target_train, train_pred, target_valid, valid_pred, title, figsize=(8,3)):\n    # Building the 2 confusion matrices train/valid\n    \n    def cm_calc(y_true, y_pred):\n        cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n        cm_sum = np.sum(cm, axis=1, keepdims=True)\n        cm_perc = cm / cm_sum.astype(float) * 100\n        annot = np.empty_like(cm).astype(str)\n        nrows, ncols = cm.shape\n        for i in range(nrows):\n            for j in range(ncols):\n                c = cm[i, j]\n                p = cm_perc[i, j]\n                if i == j:\n                    s = cm_sum[i]\n                    annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n                elif c == 0:\n                    annot[i, j] = ''\n                else:\n                    annot[i, j] = '%.1f%%\\n%d' % (p, c)\n        cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n        cm.index.name = 'Actual'\n        cm.columns.name = 'Predicted'\n        return cm, annot\n\n    \n    # Building the confusion matrices\n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=figsize, sharex=True)\n\n    # Training data\n    ax = axes[0]\n    ax.set_title(\"for training data\")\n    cm0, annot0 = cm_calc(target_train, train_pred)    \n    sns.heatmap(cm0, cmap= \"YlGnBu\", annot=annot0, fmt='', ax=ax)\n    \n    # Validation data\n    ax = axes[1]\n    ax.set_title(\"for validation data\")\n    cm1, annot1 = cm_calc(target_valid, valid_pred)\n    sns.heatmap(cm1, cmap= \"YlGnBu\", annot=annot1, fmt='', ax=ax)\n\n    \n    fig.suptitle(title, y=1.05)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_pred = xgb_model.predict(Xtrain)\nvalid_pred = xgb_model.predict(Xval)\nplot_cm2(Ztrain, train_pred, Zval, valid_pred, \"Confusion matrices for dataset data_titanic_num\", figsize=(8,3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tip 7.2. Drawing learning_curve plot<a class=\"anchor\" id=\"7.2\"></a>","metadata":{"papermill":{"duration":0.203191,"end_time":"2021-03-26T17:18:33.487981","exception":false,"start_time":"2021-03-26T17:18:33.28479","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from sklearn.model_selection import learning_curve\n\n# Thanks to https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html#sphx-glr-auto-examples-model-selection-plot-learning-curve-py\ndef plot_learning_curve(estimator, title, X, y, cv=None, axes=None, ylim=None,\n                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5), random_state=0):\n    \"\"\"\n    Generate 2 plots: \n    - the test and training learning curve, \n    - the training samples vs fit times curve.\n\n    Parameters\n    ----------\n    estimator : object type that implements the \"fit\" and \"predict\" methods\n        An object of that type which is cloned for each validation.\n\n    title : string\n        Title for the chart.\n\n    X : array-like, shape (n_samples, n_features)\n        Training vector, where n_samples is the number of samples and\n        n_features is the number of features.\n\n    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n        Target relative to X for classification or regression;\n        None for unsupervised learning.\n\n    axes : array of 3 axes, optional (default=None)\n        Axes to use for plotting the curves.\n\n    ylim : tuple, shape (ymin, ymax), optional\n        Defines minimum and maximum yvalues plotted.\n\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n          - None, to use the default 5-fold cross-validation,\n          - integer, to specify the number of folds.\n          - :term:`CV splitter`,\n          - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer/None inputs, if ``y`` is binary or multiclass,\n        :class:`StratifiedKFold` used. If the estimator is not a classifier\n        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validators that can be used here.\n\n    train_sizes : array-like, shape (n_ticks,), dtype float or int\n        Relative or absolute numbers of training examples that will be used to\n        generate the learning curve. If the dtype is float, it is regarded as a\n        fraction of the maximum size of the training set (that is determined\n        by the selected validation method), i.e. it has to be within (0, 1].\n        Otherwise it is interpreted as absolute sizes of the training sets.\n        Note that for classification the number of samples usually have to\n        be big enough to contain at least one sample from each class.\n        (default: np.linspace(0.1, 1.0, 5))\n    \n    random_state : random_state\n    \n    \"\"\"\n    plt.figure(figsize=(18,5))\n    plt.title(title)\n    if ylim is not None:\n        plt.set_ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n\n    if cv is None:\n        cv = ShuffleSplit(n_splits=cv_n_split, test_size=0.2, random_state=random_state)\n    \n    train_sizes, train_scores, test_scores = \\\n        learning_curve(estimator=estimator, X=X, y=y, cv=cv,\n                       train_sizes=train_sizes, return_times=False)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n\n    # Plot learning curve\n    plt.grid()\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                         train_scores_mean + train_scores_std, alpha=0.1,\n                         color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=0.1,\n                         color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n                 label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n                 label=\"Cross-validation score\")\n    plt.legend(loc=\"best\")\n\n    plt.show()\n    return","metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2021-03-26T17:18:33.904946Z","iopub.status.busy":"2021-03-26T17:18:33.904226Z","iopub.status.idle":"2021-03-26T17:18:33.907261Z","shell.execute_reply":"2021-03-26T17:18:33.906653Z"},"papermill":{"duration":0.218035,"end_time":"2021-03-26T17:18:33.907398","exception":false,"start_time":"2021-03-26T17:18:33.689363","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Draw plot_learning_curve\nlc_title = \"Learning curve plot for the xgboost model for dataset data_titanic_num\"\nplot_learning_curve(xgb_model, lc_title, Xtrain, Ztrain, cv=cv_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tip 7.3. Drawing validation_curve plot<a class=\"anchor\" id=\"7.3\"></a>","metadata":{"papermill":{"duration":0.202021,"end_time":"2021-03-26T17:18:34.309574","exception":false,"start_time":"2021-03-26T17:18:34.107553","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from sklearn.model_selection import validation_curve\n\n# Thanks to https://scikit-learn.org/stable/auto_examples/model_selection/\ndef plot_validation_curve(estimator, title, X, y, cv=None, axes=None, ylim=None,\n                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5), random_state=0):\n    \"\"\"\n    Generate 2 plots: \n    - the valid and training learning curve, \n    - the training samples vs fit times curve.\n\n    Parameters\n    ----------\n    estimator : object type that implements the \"fit\" and \"predict\" methods\n        An object of that type which is cloned for each validation.\n\n    title : string\n        Title for the chart.\n\n    X : array-like, shape (n_samples, n_features)\n        Training vector, where n_samples is the number of samples and\n        n_features is the number of features.\n\n    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n        Target relative to X for classification or regression;\n        None for unsupervised learning.\n\n    axes : array of 3 axes, optional (default=None)\n        Axes to use for plotting the curves.\n\n    ylim : tuple, shape (ymin, ymax), optional\n        Defines minimum and maximum yvalues plotted.\n\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n          - None, to use the default 5-fold cross-validation,\n          - integer, to specify the number of folds.\n          - :term:`CV splitter`,\n          - An iterable yielding (train, valid) splits as arrays of indices.\n\n        For integer/None inputs, if ``y`` is binary or multiclass,\n        :class:`StratifiedKFold` used. If the estimator is not a classifier\n        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validators that can be used here.\n\n    train_sizes : array-like, shape (n_ticks,), dtype float or int\n        Relative or absolute numbers of training examples that will be used to\n        generate the learning curve. If the dtype is float, it is regarded as a\n        fraction of the maximum size of the training set (that is determined\n        by the selected validation method), i.e. it has to be within (0, 1].\n        Otherwise it is interpreted as absolute sizes of the training sets.\n        Note that for classification the number of samples usually have to\n        be big enough to contain at least one sample from each class.\n        (default: np.linspace(0.1, 1.0, 5))\n    \n    random_state : random_state\n    \n    \"\"\"\n    plt.figure(figsize=(18,5))\n    plt.title(title)\n    if ylim is not None:\n        plt.set_ylim(*ylim)\n    plt.xlabel(\"max_depth from 3 to 7\")\n    plt.ylabel(\"Score\")\n\n    if cv is None:\n        cv = ShuffleSplit(n_splits=cv_n_split, test_size=0.2, random_state=random_state)\n    \n    train_scores, valid_scores = \\\n        validation_curve(estimator=estimator, X=X, y=y, param_name=\"max_depth\", param_range=[3, 4, 5, 6, 7], cv=cv)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    valid_scores_mean = np.mean(valid_scores, axis=1)\n    valid_scores_std = np.std(valid_scores, axis=1)\n\n    # Plot learning curve\n    plt.grid()\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                         train_scores_mean + train_scores_std, alpha=0.1,\n                         color=\"r\")\n    plt.fill_between(train_sizes, valid_scores_mean - valid_scores_std,\n                         valid_scores_mean + valid_scores_std, alpha=0.1,\n                         color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n                 label=\"Training score\")\n    plt.plot(train_sizes, valid_scores_mean, 'o-', color=\"g\",\n                 label=\"Cross-validation score\")\n    plt.legend(loc=\"best\")\n\n    plt.show()\n    return","metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2021-03-26T17:18:34.726341Z","iopub.status.busy":"2021-03-26T17:18:34.72167Z","iopub.status.idle":"2021-03-26T17:18:34.728634Z","shell.execute_reply":"2021-03-26T17:18:34.729133Z"},"papermill":{"duration":0.216971,"end_time":"2021-03-26T17:18:34.729308","exception":false,"start_time":"2021-03-26T17:18:34.512337","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Draw plot_validation_curve\nvc_title = \"Validation curve plot for the xgboost model for dataset data_titanic_num\"\nplot_validation_curve(xgb_model, vc_title, Xtrain, Ztrain, cv=cv_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tip 7.4. Drawing ROC_AUC plot<a class=\"anchor\" id=\"7.4\"></a>","metadata":{"papermill":{"duration":0.203329,"end_time":"2021-03-26T17:18:35.133015","exception":false,"start_time":"2021-03-26T17:18:34.929686","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, classification_report\n\ndef roc_auc_plot(model, train, target_train, valid, target_valid):\n    # Draw ROC-UAC plots and print report: precision, recall, f1-score, support\n\n    def roc_plot(model, target, df, title):\n        # Calc and draw ROC plot for df and target\n        sns.set(font_scale=1.5)\n        sns.set_color_codes(\"muted\")\n        plt.figure(figsize=(5, 4))\n        fpr, tpr, thresholds = roc_curve(target, model.predict_proba(df)[:,1], pos_label=1)\n        plt.plot(fpr, tpr, lw=2, label='ROC curve ')\n        plt.plot([0, 1], [0, 1])\n        plt.xlim([0.0, 1.0])\n        plt.ylim([0.0, 1.05])\n        plt.xlabel('False Positive Rate')\n        plt.ylabel('True Positive Rate')\n        plt.title(title)\n        plt.show()        \n\n    # Report\n    print('Classification report for training data\\n',\n          classification_report(target_train, model.predict(train), target_names=['0', '1']))\n    print('Classification report for validation data\\n',\n          classification_report(target_valid, model.predict(valid), target_names=['0', '1']))\n\n    roc_plot(model, target_train, train, \"ROC curve for training data\")\n    roc_plot(model, target_valid, valid, \"ROC curve for validation data\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2021-03-26T17:18:35.539022Z","iopub.status.busy":"2021-03-26T17:18:35.538335Z","iopub.status.idle":"2021-03-26T17:18:35.548993Z","shell.execute_reply":"2021-03-26T17:18:35.549753Z"},"papermill":{"duration":0.215989,"end_time":"2021-03-26T17:18:35.549944","exception":false,"start_time":"2021-03-26T17:18:35.333955","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Draw ROC-AUC plot\nroc_auc_plot(xgb_model, Xtrain, Ztrain, Xval, Zval)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tip 7.5. Drawing plot with prediction and target data (Plotly)<a class=\"anchor\" id=\"7.5\"></a>","metadata":{"papermill":{"duration":0.201811,"end_time":"2021-03-26T17:18:35.95244","exception":false,"start_time":"2021-03-26T17:18:35.750629","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# In progress...\ndef plot_prediction(df, cols_y_list, target, forecast, log_y=False):\n    # Draws a plot - the features cols_y_list (y) and dates_x dates from the dataframe df\n    # all data are scaling into [0,1]\n    # target data drawing as black wide vertical lines in the dates with target = 1\n    # predicted data drawing as red thin vertical lines in the dates with predicted data = 1\n    # with log_y = False or True\n    \n    # Get data for y with input features data and scaling it into [0,1]\n    df = df.reset_index(drop=False)\n    xf = df.index.tolist()\n    \n    # The feature with the largest importance\n    yf = df[cols_y_list[0]]\n    fig = go.Figure(layout=go.Layout(width=900, height=600))\n\n    # Target\n    target_index_list = [i for i, x in enumerate(target) if x == 1]\n    for i in range(len(target_index_list)):\n        target_index_i =  target_index_list[i]\n        fig.add_shape(dict(type=\"line\", x0=target_index_i, y0=0, x1=target_index_i, y1=1, \n                           line=dict(color=\"black\", width=3)))\n    \n    # Predicted data\n    predicted_index_list = [i for i, x in enumerate(forecast) if x == 1]\n    for i in range(len(predicted_index_list)):\n        pred_index_i =  predicted_index_list[i]\n        fig.add_shape(dict(type=\"line\", x0=pred_index_i, y0=0, x1=pred_index_i, y1=1, \n                           line=dict(color=\"yellow\", width=3, dash=\"dot\")))\n    \n    # Other features\n    for i in range(len(cols_y_list)):\n        yf = df[cols_y_list[i]]\n        fig.add_trace(go.Scatter(x=xf, y=(yf-yf.min())/(yf.max()-yf.min()), mode='lines', name=cols_y_list[i]))\n\n    fig.show()\n\n# Result visualization\n#plot_prediction(data, [], target, prediction)","metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2021-03-26T17:18:36.364637Z","iopub.status.busy":"2021-03-26T17:18:36.363939Z","iopub.status.idle":"2021-03-26T17:18:36.375237Z","shell.execute_reply":"2021-03-26T17:18:36.375735Z"},"papermill":{"duration":0.218904,"end_time":"2021-03-26T17:18:36.37592","exception":false,"start_time":"2021-03-26T17:18:36.157016","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tip 7.6. Explanation model predictions with SHAP<a class=\"anchor\" id=\"7.6\"></a>","metadata":{}},{"cell_type":"code","source":"import shap\nshap_values = shap.TreeExplainer(xgb_model).shap_values(Xtrain)\nshap.summary_plot(shap_values, Xtrain)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shap.dependence_plot(\"Age\", shap_values, Xtrain)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shap_interaction_values = shap.TreeExplainer(xgb_model).shap_interaction_values(Xtrain.iloc[:,:])\nshap.summary_plot(shap_interaction_values, Xtrain.iloc[:,:])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shap_values = shap.TreeExplainer(xgb_model).shap_values(Xval)\nshap_interaction_values = shap.TreeExplainer(xgb_model).shap_interaction_values(Xval)\nshap.summary_plot(shap_values, Xval, plot_type=\"bar\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def data_force_plot_n(model, X):\n\n    shap.initjs()\n    explainer = shap.TreeExplainer(model)\n    shap_values = explainer.shap_values(X)\n    \n    return shap.force_plot(explainer.expected_value, shap_values[0,:], X.iloc[0,:])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_force_plot_n(xgb_model, Xval)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def data_force_plot_all(model, X):\n\n    shap.initjs()\n    explainer = shap.TreeExplainer(model)\n    shap_values = explainer.shap_values(X)\n    \n    return shap.force_plot(explainer.expected_value, shap_values[0:len(Xval),:], X.iloc[0:len(X),:])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_force_plot_all(xgb_model, Xval)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### See more in the notebook [50 Tips: Data Science (tabular data) for beginner](https://www.kaggle.com/vbmokin/50-tips-data-science-tabular-data-for-beginner)","metadata":{}},{"cell_type":"markdown","source":"## 8. BONUS<a class=\"anchor\" id=\"8\"></a>\n\n[Back to Table of Contents](#0.1)","metadata":{"papermill":{"duration":0.213071,"end_time":"2021-03-26T17:18:38.082301","exception":false,"start_time":"2021-03-26T17:18:37.86923","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### Tip 8.1. Submission data from DataFrame to Kaggle competition<a class=\"anchor\" id=\"8.1\"></a>","metadata":{"papermill":{"duration":0.203266,"end_time":"2021-03-26T17:18:38.489144","exception":false,"start_time":"2021-03-26T17:18:38.285878","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### Competition [Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic)","metadata":{"papermill":{"duration":0.203614,"end_time":"2021-03-26T17:18:38.894619","exception":false,"start_time":"2021-03-26T17:18:38.691005","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# From https://www.kaggle.com/vbmokin/titanic-top-score-one-line-of-the-prediction\ntraindf = pd.read_csv('../input/titanic/train.csv').set_index('PassengerId')\ntestdf = pd.read_csv('../input/titanic/test.csv').set_index('PassengerId')\ndf = pd.concat([traindf, testdf], axis=0, sort=False)\ndf['Title'] = df.Name.str.split(',').str[1].str.split('.').str[0].str.strip()\ndf['IsWomanOrBoy'] = ((df.Title == 'Master') | (df.Sex == 'female'))\ndf['LastName'] = df.Name.str.split(',').str[0]\nfamily = df.groupby(df.LastName).Survived\ndf['WomanOrBoyCount'] = family.transform(lambda s: s[df.IsWomanOrBoy].fillna(0).count())\ndf['WomanOrBoyCount'] = df.mask(df.IsWomanOrBoy, df.WomanOrBoyCount - 1, axis=0)\ndf['FamilySurvivedCount'] = family.transform(lambda s: s[df.IsWomanOrBoy].fillna(0).sum())\ndf['FamilySurvivedCount'] = df.mask(df.IsWomanOrBoy, df.FamilySurvivedCount - df.Survived.fillna(0), axis=0)\ndf['WomanOrBoySurvived'] = df.FamilySurvivedCount / df.WomanOrBoyCount.replace(0, np.nan)\ndf['Alone'] = (df.WomanOrBoyCount == 0)\ndf = pd.concat([df.WomanOrBoySurvived.fillna(0), df.Alone, df.Sex.replace({'male': 0, 'female': 1})], axis=1)\ntest_x = df.loc[testdf.index]\ntest_x['Survived'] = (((test_x.WomanOrBoySurvived <= 0.238) & (test_x.Sex > 0.5) & (test_x.Alone > 0.5)) | \\\n          ((test_x.WomanOrBoySurvived > 0.238) & \\\n           ~((test_x.WomanOrBoySurvived > 0.55) & (test_x.WomanOrBoySurvived <= 0.633))))\npd.DataFrame({'Survived': test_x['Survived'].astype(int)}, \\\n             index=testdf.index).reset_index().to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.execute_input":"2021-03-26T17:18:39.304876Z","iopub.status.busy":"2021-03-26T17:18:39.303905Z","iopub.status.idle":"2021-03-26T17:18:40.565057Z","shell.execute_reply":"2021-03-26T17:18:40.565531Z"},"papermill":{"duration":1.467284,"end_time":"2021-03-26T17:18:40.56575","exception":false,"start_time":"2021-03-26T17:18:39.098466","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Result: the file \"submission.csv\" gives LB = 0.80382 (Top 4%)","metadata":{"papermill":{"duration":0.205694,"end_time":"2021-03-26T17:18:40.974289","exception":false,"start_time":"2021-03-26T17:18:40.768595","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"I hope you find this notebook useful and enjoyable.\n\nYour comments and feedback are most welcome.\n\n[Go to Top](#0)","metadata":{"papermill":{"duration":0.201149,"end_time":"2021-03-26T17:18:41.403742","exception":false,"start_time":"2021-03-26T17:18:41.202593","status":"completed"},"tags":[]}}]}