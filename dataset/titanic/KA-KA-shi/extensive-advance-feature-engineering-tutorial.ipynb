{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Hello Everyone , I am Adarsh**\n\n![](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcR6q30jOVrk9-VKiq3cGYC5gAd36D0Heip8W3O2ocMfIOya1_sS&usqp=CAU)\n\n This notebook acts as a tutorial which summarizes the main techniques of feature engineering for solving major problems with their short descriptions and implementation. \n \n** if you like my work and presentaion, please give a Up-vote to the work book as it will motiate me to share more of my learnings !!**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Introduction\n\nWhat is a feature and why we need the engineering of it? Basically, all machine learning algorithms use some input data to create outputs. This input data comprise features, which are usually in the form of structured columns. Algorithms require features with some specific characteristic to work properly. Here, the need for feature engineering arises. I think feature engineering efforts mainly have two goals:\n\n1.     Preparing the proper input dataset, compatible with the machine learning algorithm requirements.\n2.     Improving the performance of machine learning models.\n\n*The features you use influence more than everything else the result. No algorithm alone, to my knowledge, can supplement the information gain given by correct feature engineering.* — Luca Massaron\n\nAccording to Forbes [post](https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says/#791a964b6f63) -> Data preparation accounts for about 80% of the work of data scientists !!\nand unfortunatelly 76% of data scientists view data preparation as the least enjoyable part of their work :(\n\n![](https://thumbor.forbes.com/thumbor/960x0/https%3A%2F%2Fblogs-images.forbes.com%2Fgilpress%2Ffiles%2F2016%2F03%2FTime-1200x511.jpg)\n\nWhen your goal is to get the best possible results from a predictive model, you need to get the most from what you have.\n\nThis includes getting the best results from the algorithms you are using. It also involves getting the most out of the data for your algorithms to work with.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Importance of Feature Engineering\n\nThe features in your data will directly influence the predictive models you use and the results you can achieve.\nYou can say that: the better the features that you prepare and choose, the better the results you will achieve. It is true, but it also misleading.\n\nThe results you achieve are a factor of the model you choose, the data you have available and the features you prepared. Even your framing of the problem and objective measures you’re using to estimate accuracy play a part. Your results are dependent on many inter-dependent properties.\n\nYou need great features that describe the structures inherent in your data.\n\n1. **Better features means flexibility.**\n\nYou can choose “the wrong models” (less than optimal) and still get good results. Most models can pick up on good structure in data. The flexibility of good features will allow you to use less complex models that are faster to run, easier to understand and easier to maintain. This is very desirable.\n\n2. **Better features means simpler models.**\n\nWith well engineered features, you can choose “the wrong parameters” (less than optimal) and still get good results, for much the same reasons. You do not need to work as hard to pick the right models and the most optimized parameters.\n\nWith good features, you are closer to the underlying problem and a representation of all the data you have available and could use to best characterize that underlying problem.\n\n3. **Better features means better results.**\n\n*The algorithms we used are very standard for Kagglers. […]  We spent most of our efforts in feature engineering.*\n— Xavier Conort, on “Q&A with Xavier Conort” on winning the Flight Quest challenge on Kaggle","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### List of problems we will be handling in this tutorial !!\n\n1. [*Numerical and categorical variable*](#1.Numerical_and_categorical_variable)  \n2. [*Cardinality*](#2.Cardinality) \n3. [*RareLabels*](#3.RareLabels)  \n4. [*Outliers*](#4.Outliers)  \n5. [*Date and time variable*](#5.Date_and_Time_variable) \n6. [*Mixed variable*](#6.Mixed_variable) \n7. [*Scaling*](#7.Scaling) \n8. [*Gaussian_transformations*](#8.Gaussian_transformations) ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"9.[*Conclusion*](#Conlcusion)","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"## So Lets Start it !","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 1.Numerical_and_categorical_variable\n\n#### What is a Variable?\n\nA variable is any characteristic, number, or quantity that can be measured or counted. The following are examples of variables:\n\n- Age (21, 35, 62, ...)\n- Gender (male, female)\n- Income (GBP 20000, GBP 35000, GBP 45000, ...)\n- House price (GBP 350000, GBP 570000, ...)\n- Country of birth (China, Russia, Costa Rica, ...)\n- Eye colour (brown, green, blue, ...)\n- Vehicle make (Ford, Volkswagen, ...)\n\nThey are called 'variables' because the value they take may vary (and it usually does) in a population. \n\nMost variables in a data set can be classified into one of two major types:\n\n- **Numerical variables** \n- **Categorical variables**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"===================================================================================\n\n### 1.1 Numerical variables\n\nThe values of a numerical variable are numbers. They can be further classified into discrete and continuous variables.\n\n### 1.1.1 Discrete numerical variable\n\nA variable which values are whole numbers (counts) is called discrete. For example, the number of items bought by a customer in a supermarket is discrete. The customer can buy 1, 25, or 50 items, but not 3.7 items. It is always a round number. The following are examples of discrete variables:\n\n- Number of active bank accounts of a borrower (1, 4, 7, ...)\n- Number of pets in the family\n- Number of children in the family\n\n\n### 1.1.2 Continuous numerical variable\n\nA variable that may contain any value within some range is called continuous. For example, the total amount paid by a customer in a supermarket is continuous. The customer can pay, GBP 20.5, GBP 13.10, GBP 83.20 and so on.\nOther examples of continuous variables are:\n\n- House price (in principle, it can take any value) (GBP 350000, 57000, 1000000, ...)\n- Time spent surfing a website (3.4 seconds, 5.10 seconds, ...)\n- Total debt as percentage of total income in the last month (0.2, 0.001, 0, 0.75, ...)\n\n=============================================================================\n\n#### Real Life example: Peer to peer lending (Finance)\n\n##### Lending Club\n\n**Lending Club** is a peer-to-peer Lending company based in the US. They match people looking to invest money with people looking to borrow money. When investors invest their money through Lending Club, this money is passed onto borrowers, and when borrowers pay their loans back, the capital plus the interest passes on back to the investors. It is a win for everybody as they can get typically lower loan rates and higher investor returns.\n\nIf you want to learn more about Lending Club follow this [link](https://www.lendingclub.com/).\n\nThe Lending Club dataset contains complete loan data for all loans issued through 2007-2015, including the current loan status (Current, Late, Fully Paid, etc.) and latest payment information. Features include credit scores, number of finance inquiries, address including zip codes and state, and collections among others. Collections indicates whether the customer has missed one or more payments and the team is trying to recover their money.\n\nThe file is a matrix of about 890 thousand observations and 75 variables. More detail on this dataset can be found in [Kaggle's website](https://www.kaggle.com/wendykan/lending-club-loan-data)\n\nLet's go ahead and have a look at the variables!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's load the dataset with just a few columns and a few rows\n# to speed the demo\n\n# Variable definitions:\n#-------------------------\n# loan_amnt: loan amount\n# int_rate: interest rate\n# annual_inc: annual income\n# open_acc: open accounts (more on this later)\n# loan_status: loan status(paid, defaulted, etc)\n# open_il_12m: accounts opened in the last 12 months\n\nuse_cols = [\n    'loan_amnt', 'int_rate', 'annual_inc', 'open_acc', 'loan_status',\n    'open_il_12m','id', 'purpose', 'loan_status', 'home_ownership'\n]\n\n# this dataset is very big. To speed things up for the demo\n# I will randomly select 10,000 rows when I load the dataset\n# so I upload just a sample of the total rows\n\ndata = pd.read_csv('../input/lending-club-loan-data/loan.csv', usecols=use_cols).sample(10000, random_state=44)  # set a seed for reproducibility\n\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.1.1 Continious Variable","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's look at the values of the variable loan_amnt\n# this is the amount of money requested by the borrower\n# in US dollars\n\n# this variable is continuous, it can take in principle,\n# any value\n\ndata.open_il_12m.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's make an histogram to get familiar with the\n# distribution of the variable\n\nfig = data.loan_amnt.hist(bins=50)\nfig.set_title('Loan Amount Requested')\nfig.set_xlabel('Loan Amount')\nfig.set_ylabel('Number of Loans')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The values of the variable vary across the entire range of the variable. This is characteristic of continuous variables.\n\nNote: The taller bars correspond to loan sizes of 10000, 15000, 20000, and 35000. There are more loans disbursed for those loan amount values. This indicates that most people tend to ask for these loan amounts. Likely, these particular loan amounts are pre-determined and offered as such in the Lending Club website. Less frequent loan values, like 23,000 or 33,000 could be requested by people who require a specific amount of money for a definite purpose.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 1.1.2 Discrete Variables\n\nLet's explore the variable \"Number of open credit lines in the borrower's credit file\" (open_acc in the dataset). This is, the total number of credit items (for example, credit cards, car loans, mortgages, etc) that is known for that borrower. By definition it is a discrete variable, because a borrower can have 1 credit card, but not 3.5 credit cards.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's inspect the values of the variable\n\n# this is a discrete variable\n\ndata.open_acc.dropna().unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's make an histogram to get familiar with the\n# distribution of the variable\n\nfig = data.open_acc.hist(bins=100)\nfig.set_xlim(0, 30)\nfig.set_title('Number of open accounts')\nfig.set_xlabel('Number of open accounts')\nfig.set_ylabel('Number of Customers')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Histograms of discrete variables have this typical broken shape, as not all the values within the variable range are present in the variable. As I said, the customer can have 3 credit cards, but not 3,5 credit cards.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"===================================================================================\n### 1.2 Categorical variables\n\nThe values of a categorical variable are selected from a group of categories, also called labels. Examples are gender (male or female) and marital status (never married, married, divorced or widowed). Other examples of categorical variables include:\n\n    Intended use of loan (debt-consolidation, car purchase, wedding expenses, ...)\n    Mobile network provider (Vodafone, Orange, ...)\n    Postcode\n\nCategorical variables can be further categorised into ordinal and nominal variables.\n### 1.2.1 Ordinal categorical variables\n\nCategorical variable in which categories can be meaningfully ordered are called ordinal. For example:\n\n    Student's grade in an exam (A, B, C or Fail).\n    Days of the week can be ordinal with Monday = 1 and Sunday = 7.\n    Educational level, with the categories Elementary school, High school, College graduate and PhD ranked from 1 to 4.\n\n### 1.2.2 Nominal categorical variable\n\nThere isn't an intrinsic order of the labels. For example, country of birth (Argentina, England, Germany) is nominal. Other examples of nominal variables include:\n\n    Postcode\n    Vehicle make (Citroen, Peugeot, ...)\n\nThere is nothing that indicates an intrinsic order of the labels, and in principle, they are all equal.\n\nTo be considered:\n\nSometimes categorical variables are coded as numbers when the data are recorded (e.g. gender may be coded as 0 for males and 1 for females). The variable is still categorical, despite the use of numbers.\n\nIn a similar way, individuals in a survey may be coded with a number that uniquely identifies them (for example to avoid storing personal information for confidentiality). This number is really a label, and the variable then categorical. The number has no meaning other than making it possible to uniquely identify the observation (in this case the interviewed subject).\n\nIdeally, when we work with a dataset in a business scenario, the data will come with a dictionary that indicates if the numbers in the variables are to be considered as categories or if they are numerical. And if the numbers are categoriies, the dictionary would explain what they intend to represent.\n\n=============================================================================","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's inspect the variable home ownership,\n# which indicates whether the borrowers own their home\n# or if they are renting for example, among other things.\n\ndata.home_ownership.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's make a bar plot, with the number of loans\n# for each category of home ownership\n\n# the code below counts the number of observations (borrowers)\n# within each category\n# and then makes a plot\n\nfig = data['home_ownership'].value_counts().plot.bar()\nfig.set_title('Home Ownership')\nfig.set_ylabel('Number of customers')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The majority of the borrowers either own their house on a mortgage or rent their property. A few borrowers own their home completely. The category 'Other' seems to be empty. To be completely sure, we could print the numbers as below.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data['home_ownership'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 2 borrowers that have other arrangements for their property. For example, they could live with their parents, or live in a hotel.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# the \"purpose\" variable is another categorical variable\n# that indicates how the borrowers intend to use the\n# money they are borrowing, for example to improve their\n# house, or to cancel previous debt.\n\ndata.purpose.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Debt consolidation means that the borrower would like a loan to cancel previous debts, car means that the borrower is borrowing the money to buy a car, and so on. It gives an idea of the intended use of the loan.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's make a bar plot with the number of borrowers\n# within each category\n\n# the code below counts the number of observations (borrowers)\n# within each category\n# and then makes a plot\n\nfig = data['purpose'].value_counts().plot.bar()\nfig.set_title('Loan Purpose')\nfig.set_ylabel('Number of customers')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The majority of the borrowers intend to use the loan for 'debt consolidation' or to repay their 'credit cards'. This is quite common. What the borrowers intend to do is, to consolidate all the debt that they have on different financial items, in one single debt, the new loan that they will take from Lending Club in this case. This loan will usually provide an advantage to the borrower, either in the form of lower interest rates than a credit card, for example, or longer repayment period.\n\n**That is all for Neumerical and categorical data demonstration. I hope you enjoyed. Please Up-vote this notebook !!**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 2.Cardinality\n\nThe values of a categorical variable are selected from a group of categories, also called labels. For example, in the variable _gender_ the categories or labels are male and female, whereas in the variable _city_ the labels can be London, Manchester, Brighton and so on.\n\nDifferent categorical variables can contain different number of labels or categories. The variable gender contains only 2 labels, but a variable like city or postcode, can contain a huge number of different labels.\n\nThe number of different labels within a categorical variable is known as cardinality. A high number of labels within a variable is known as __high cardinality__.\n\n\n\n### Are multiple labels in a categorical variable a problem?\n\nHigh cardinality may pose the following problems: \n\n- Variables with too many labels tend to dominate over those with only a few labels, particularly in **Tree based** algorithms.\n\n- A big number of labels within a variable may introduce noise with little, if any, information, therefore making machine learning models prone to over-fit.\n\n- Some of the labels may only be present in the training data set, but not in the test set, therefore machine learning algorithms may over-fit to the training set.\n\n- Contrarily, some labels may appear only in the test set, therefore leaving the machine learning algorithms unable to perform a calculation over the new (unseen) observation.\n\n\nIn particular, **tree methods can be biased towards variables with lots of labels** (variables with high cardinality). Thus, their performance may be affected by high cardinality.\n\nBelow, I will show the effect of high cardinality of variables on the performance of different machine learning algorithms, and how a quick fix to reduce the number of labels, without any sort of data insight, already helps to boost performance.\n\n\n\n## Real Life example: \n\n### Predicting Survival on the Titanic: understanding society behaviour and beliefs\n\nPerhaps one of the most infamous shipwrecks in history, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 people on board. Interestingly, by analysing the probability of survival based on few attributes like gender, age, and social status, we can make very accurate predictions on which passengers would survive. Some groups of people were more likely to survive than others, such as women, children, and the upper-class. Therefore, we can learn about the society priorities and privileges at the time.\n\n====================================================================================================\n\nTo download the Titanic data, go ahead to this [website](https://www.kaggle.com/c/titanic/data)\n\nClick on the link 'train.csv', and then click the 'download' blue button towards the right of the screen, to download the dataset. Save it in a folder of your choice.\n\n**Note that you need to be logged in to Kaggle in order to download the datasets**.\n\n====================================================================================================","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### In this Demo:\n\nWe will:\n\n- Learn how to quantify cardinality\n- See examples of high and low cardinality variables\n- Understand the effect of cardinality when preparing train and test sets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's load the titanic dataset\n\ndata = pd.read_csv('../input/titanic/train.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The categorical variables in this dataset are Name, Sex, Ticket, Cabin and Embarked.\n\n---------------\n**Note** that Ticket and Cabin contain both letters and numbers, so they could be treated as Mixed Variables. For this demonstration, I will treat them as categorical.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's inspect the cardinality, this is the number\n# of different labels, for the different categorical variables\n\nprint('Number of categories in the variable Name: {}'.format(\n    len(data.Name.unique())))\n\nprint('Number of categories in the variable Gender: {}'.format(\n    len(data.Sex.unique())))\n\nprint('Number of categories in the variable Ticket: {}'.format(\n    len(data.Ticket.unique())))\n\nprint('Number of categories in the variable Cabin: {}'.format(\n    len(data.Cabin.unique())))\n\nprint('Number of categories in the variable Embarked: {}'.format(\n    len(data.Embarked.unique())))\n\nprint('Total number of passengers in the Titanic: {}'.format(len(data)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"While the variable Sex contains only 2 categories and Embarked 4 (low cardinality), the variables Ticket, Name and Cabin, as expected, contain a huge number of different labels (high cardinality).\n\nTo demonstrate the effect of high cardinality in train and test sets and machine learning performance, I will work with the variable Cabin. I will create a new variable with reduced cardinality.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's explore the values / categories of Cabin\n\n# we know from the previous cell that there are 148\n# different cabins, therefore the variable\n# is highly cardinal\n\ndata.Cabin.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's now reduce the cardinality of the variable. How? instead of using the entire Cabin value, I will capture only the \nfirst letter.\n\n***Rationale***: the first letter indicates the deck on which the cabin was located, and is therefore an indication of both social class status and proximity to the surface of the Titanic. Both are known to improve the probability of survival.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's capture the first letter of Cabin\ndata['Cabin_reduced'] = data['Cabin'].astype(str).str[0]\n\ndata[['Cabin', 'Cabin_reduced']].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We reduced the number of different labels from 148 to 9.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's separate into training and testing set\n# in order to build machine learning models\nfrom sklearn.model_selection import train_test_split\n\nuse_cols = ['Cabin', 'Cabin_reduced', 'Sex']\n\n# this functions comes from scikit-learn\nX_train, X_test, y_train, y_test = train_test_split(\n    data[use_cols], \n    data.Survived,  \n    test_size=0.3,\n    random_state=0)\n\nX_train.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### High cardinality leads to uneven distribution of categories in train and test sets\n\nWhen a variable is highly cardinal, often some categories land only on the training set, or only on the testing set. If present only in the training set, they may lead to over-fitting. If present only on the testing set, the machine learning algorithm will not know how to handle them, as it has not seen them during training.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's find out labels present only in the training set\n\nunique_to_train_set = [\n    x for x in X_train.Cabin.unique() if x not in X_test.Cabin.unique()\n]\n\nlen(unique_to_train_set)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 100 Cabins only present in the training set, and not in the testing set.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's find out labels present only in the test set\n\nunique_to_test_set = [\n    x for x in X_test.Cabin.unique() if x not in X_train.Cabin.unique()\n]\n\nlen(unique_to_test_set)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Variables with high cardinality tend to have values (i.e., categories) present in the training set, that are not present in the test set, and vice versa. This will bring problems at the time of training (due to over-fitting) and scoring of new data (how should the model deal with unseen categories?).\n\nThis problem is almost overcome by reducing the cardinality of the variable. See below.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's find out labels present only in the training set\n# for Cabin with reduced cardinality\n\nunique_to_train_set = [\n    x for x in X_train['Cabin_reduced'].unique()\n    if x not in X_test['Cabin_reduced'].unique()\n]\n\nlen(unique_to_train_set)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's find out labels present only in the test set\n# for Cabin with reduced cardinality\n\nunique_to_test_set = [\n    x for x in X_test['Cabin_reduced'].unique()\n    if x not in X_train['Cabin_reduced'].unique()\n]\n\nlen(unique_to_test_set)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observe how by reducing the cardinality there is now only 1 label in the training set that is not present in the test set. And no label in the test set that is not contained in the training set as well.\n\n**That is all for Cardibnal data demonstration. I hope you enjoyed. Please Up-vote this notebook !!**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 3.RareLabels","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Labels that occur rarely\n\nCategorical variables are those which values are selected from a group of categories, also called labels. Different labels appear in  the dataset with different frequencies. Some categories appear a lot in the dataset, whereas some other categories appear only in a few number of observations.\n\nFor example, in dataset with information about loan applicants where one of the variables is \"city\" where the applicant lives, cities like 'New York' may appear a lot in the data because New York has a huge population, whereas smaller towns like 'Leavenworth' will appear only on a few occasions (if at all, population < 2000 people), because the population there is very small. A borrower is more likely to live in New York, because far more people live in New York.\n\nIn fact, categorical variables often contain a few dominant labels that account for the majority of the observations and a large number of labels that appear only seldomly.\n\n\n### Are Rare Labels in a categorical variable a problem?\n\nRare values can add a lot of information or none at all. For example, consider a stockholder meeting where each person can vote in proportion to their number of shares. One of the shareholders owns 50% of the stock, and the other 999 shareholders own the remaining 50%. The outcome of the vote is largely influenced by the shareholder who holds the majority of the stock. The remaining shareholders may have an impact collectively, but they have almost no impact individually.  \n\nThe same occurs in real life datasets. The label that is over-represented in the dataset tends to dominate the outcome, and those that are under-represented may have no impact individually, but could have an impact if considered collectively.\n\nMore specifically,\n\n- Rare values in categorical variables tend to cause over-fitting, particularly in tree based methods.\n\n- A big number of infrequent labels adds noise, with little information, therefore causing over-fitting.\n\n- Rare labels may be present in training set, but not in test set, therefore causing over-fitting to the train set.\n\n- Rare labels may appear in the test set, and not in the train set. Thus, the machine learning model will not know how to evaluate it. \n\n\n**Note** Sometimes rare values, are indeed important. For example, if we are building a model to predict fraudulent loan applications, which are by nature rare, then a rare value in a certain variable, may be indeed very predictive. This rare value could be telling us that the observation is most likely a fraudulent application, and therefore we would choose not to ignore it.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Real Life example: \n\n### The Mercedes-Benz Greener Manufacturing challenge in Kaggle\n\nDaimler’s Mercedes-Benz cars are leaders in the premium car industry. With a huge selection of car features and options, customers can choose the customized Mercedes-Benz of their dreams.\n\nTo ensure the safety and reliability of each and every unique car configuration before they hit the road, Daimler’s engineers have developed a robust testing system. Testing time varies depending on the different car features and options. The task is to predict the time it takes for a car with certain features to pass the testing. Therefore it is a regression problem: we need to estimate a continuous variable.\n\nThe dataset contains a set of car features, the variable names are masked, so it is impossible to find out what each one of them means. The variable to predict is _y_: time to pass the testing.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's load the dataset with the variables\n# we need for this demo\n\n# 'X1', 'X2', 'X3', 'X6' are the categorical \n# variables in this dataset\n# \"y\" is the target: time to pass the quality tests\n\ndata = pd.read_csv('../input/mercedesbenz-greener-manufacturing/train.csv',\n                   usecols=['X1', 'X2', 'X3', 'X6', 'y'])\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Variable names and variable values are masked intentionally by Mercedes Benz. This is common practice, it is done to protect intellectual property and / or personal information.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's look at the different number of labels\n# in each variable (cardinality)\n\ncols_to_use = ['X1', 'X2', 'X3', 'X6']\n\nfor col in cols_to_use:\n    print('variable: ', col, ' number of labels: ', len(data[col].unique()))\n\nprint('total cars: ', len(data))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"X1 shows 27 different values, X2 shows 44 different categories, and so on.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's plot how frequently each label\n# appears in the dataset\n\n# in other words, the percentage of cars that\n# show each label\n\ntotal_cars = len(data)\n\n# for each categorical variable\nfor col in cols_to_use:\n\n    # count the number of cars per category\n    # and divide by total cars\n\n    # aka percentage of cars per category\n\n    temp_df = pd.Series(data[col].value_counts() / total_cars)\n\n    # make plot with the above percentages\n    fig = temp_df.sort_values(ascending=False).plot.bar()\n    fig.set_xlabel(col)\n\n    # add a line at 5 %\n    fig.axhline(y=0.05, color='red')\n    fig.set_ylabel('percentage of cars')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For each of the above categorical variables, some labels appear in more than 10% of the cars and many appear in less than 10% or even 5% of the cars. These are infrequent labels or **Rare Values** and could cause over-fitting.\n\n### How is the target, \"time to pass testing\", related to these categories?\n\nIn the following cells, I would like to understand the mean time to pass the test that present on average, the cars that display the different categories. Keep on reading, it will become clearer.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# the following function calculates:\n\n# 1) the percentage of cars per category\n# 2) the mean time to pass testing per category\n\n\ndef calculate_perc_and_passtime(df, var):\n\n    # total number of cars\n    total_cars = len(df)\n\n    # percentage of cars per category\n    temp_df = pd.Series(df[var].value_counts() / total_cars).reset_index()\n    temp_df.columns = [var, 'perc_cars']\n\n    # add the mean to pass testing time\n    # the target in this dataset is called 'y'\n    temp_df = temp_df.merge(df.groupby([var])['y'].mean().reset_index(),\n                            on=var,\n                            how='left')\n\n    return temp_df\n\n\n# now we use the function for the variable 'X3'\ntemp_df = calculate_perc_and_passtime(data, 'X3')\ntemp_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above dataframe contains the percentage of cars that show each one of the labels in X3, and the mean time to pass the testing for those cars. In other words, ~46% of cars in the dataset show the label c for X3, and a mean time to pass the test of 101 seconds.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now I create a function to plot of the\n# label frequency and mean time to pass testing.\n\n# This will help us visualise the relationship between the\n# target and the labels\n\ndef plot_categories(df, var):\n    \n    fig, ax = plt.subplots(figsize=(8, 4))\n    plt.xticks(df.index, df[var], rotation=0)\n\n    ax2 = ax.twinx()\n    ax.bar(df.index, df[\"perc_cars\"], color='lightgrey')\n    ax2.plot(df.index, df[\"y\"], color='green', label='Seconds')\n    ax.axhline(y=0.05, color='red')\n    ax.set_ylabel('percentage of cars per category')\n    ax.set_xlabel(var)\n    ax2.set_ylabel('Time to pass testing, in seconds')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_categories(temp_df, 'X3')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cars where X3 is f pass the test quicker, whereas cars with the category d take a longer time to pass the test.\n\nCars where X3 is b, take around 100 seconds to pass the test. However, b is present in less than 5% of the cars. Why is this important? Because if we do not have a lot of cars to learn from, we could be under or over-estimating the effect if b on the time to pass a test.\n\nIn other words, how confident am I to generalise that cars with the label b for X3 take around 100 seconds to pass the test, when I only have a few cars to learn from?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's plot the remaining categorical variables\n\nfor col in cols_to_use:\n    \n    if col !='X3':\n        \n        # re using the functions I created\n        temp_df = calculate_perc_and_passtime(data, col)\n        plot_categories(temp_df, col)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look at variable X2: Most of the categories in X2 are present n less than 5% of cars. In addition, \"y\" varies a lot across those rare categories. The mean value of y goes up and down over the infrequent categories. In fact, it looks quite noisy. This rare labels could indeed be very predictive, or they could be introducing noise rather than information. And because the labels are under-represented, we can't be sure whether they have a true impact on the time to pass the test. We could be under or over-estimating their impact due to the fact that we have information for few cars.\n\n**Note:** This plot would bring more value, if we plotted the errors of the mean time to pass the test. it would give us an idea of how much the mean value of the target varies within each label. Why don't you go ahead and add the standard deviation to the plot?","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Rare labels: grouping under a new label\n\nOne common way of working with rare or infrequent values, is to group them under an umbrella category called 'Rare' or 'Other'. In this way, we are able to understand the \"collective\" affect of the infrequent labels on the target.See below.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# I will replace all the labels that appear in less than 10%\n# of the cars by the label 'rare'\n\ndef group_rare_labels(df, var):\n\n    total_cars = len(df)\n\n    # first I calculate the 10% of cars for each category\n    temp_df = pd.Series(df[var].value_counts() / total_cars)\n\n    # now I create a dictionary to replace the rare labels with the\n    # string 'rare'\n\n    grouping_dict = {\n        k: ('rare' if k not in temp_df[temp_df >= 0.1].index else k)\n        for k in temp_df.index\n    }\n\n    # now I replace the rare categories\n    tmp = df[var].map(grouping_dict)\n\n    return tmp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# group rare labels in X1\n\ndata['X1_grouped'] = group_rare_labels(data, 'X1')\n\ndata[['X1', 'X1_grouped']].head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's plot X1 with the grouped categories\n# re-using the functions I created above\n\ntemp_df = calculate_perc_and_passtime(data, 'X1_grouped')\nplot_categories(temp_df, 'X1_grouped')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\"Rare\" now contains the overall influence of all the infrequent categories on the time to pass the test.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's plot the original X1 for comparison\ntemp_df = calculate_perc_and_passtime(data, 'X1')\nplot_categories(temp_df, 'X1')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Only 4 categories of X1 are relatively common across the different cars. The remaining are now gouped into 'rare' which captures the average time to pass the test for all the infrequent labels.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's group and plot the remaining categorical variables\n\nfor col in cols_to_use[1:]:\n        \n    # re using the functions I created\n    data[col+'_grouped'] = group_rare_labels(data, col)\n    temp_df = calculate_perc_and_passtime(data, col+'_grouped')\n    plot_categories(temp_df, col+'_grouped')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can see for example, that cars with the the label f for variable X3, tend to spend less time in testing, and all the infrequent labels together tend to behave overall like the features c and a, in terms of time to pass the test.\n\n**Note:** Ideally, we would also like to have the standard deviation / interquantile range for the time to pass the test, to get an idea of how variable the time to pass is for each category.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Rare labels lead to uneven distribution of categories in train and test sets\n\nSimilarly to highly cardinal variables, rare or infrequent labels often land only on the training set, or only on the testing set. If present only in the training set, they may lead to over-fitting. If present only on the testing set, the machine learning algorithm will not know how to handle them, as they have not seen the rare labels during training. Let's explore this further.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's separate into training and testing set\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(data[cols_to_use],\n                                                    data.y,\n                                                    test_size=0.3,\n                                                    random_state=0)\n\nX_train.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's find labels present only in the training set\n# I will use X2 as example\n\nunique_to_train_set = [\n    x for x in X_train['X2'].unique() if x not in X_test['X2'].unique()\n]\n\nprint(unique_to_train_set)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 7 categories present in the train set and are not present in the test set.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's find labels present only in the test set\n\nunique_to_test_set = [\n    x for x in X_test['X2'].unique() if x not in X_train['X2'].unique()\n]\n\nprint(unique_to_test_set)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this case, there are 2 rare values present in the test set only.\n\n**That is all for RareLabels demonstration. I hope you enjoyed. Please Up-vote this notebook !!**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 4.Outliers\n\nAn outlier is a data point which is significantly different from the remaining data. \"An outlier is an observation which deviates so much from the other observations as to arouse suspicions that it was generated by a different mechanism.\" [D. Hawkins. Identification of Outliers, Chapman and Hall , 1980.]\n\n\n### Should outliers be removed?\n\nDepending on the context, outliers either deserve special attention or should be completely ignored. Take the example of revenue forecasting: if unusual spikes of revenue are observed, it's probably a good idea to pay extra attention to them and figure out what caused the spike. In the same way, an unusual transaction on a credit card is usually a sign of fraudulent activity, which is what the credit card issuer wants to prevent. So in instances like these, it is useful to look for and investigate further outlier values.\n\nIf outliers are however, introduced due to mechanical error, measurement error or anything else that can't be generalised, it is a good idea to remove these outliers before feeding the data to the modeling algorithm. Why? Because some algorithms are sensitive to outliers. \n\n\n### Which machine learning models are sensitive to outliers?\n\nSome machine learning models are more sensitive to outliers than others. For instance, AdaBoost may treat outliers as \"hard\" cases and put tremendous weights on outliers, therefore producing a model with bad generalisation.\n\nLinear models, in particular Linear Regression, can be also sensitive to outliers.\n\nDecision trees tend to ignore the presence of outliers when creating the branches of their trees. Typically, trees make decisions by asking if variable x >= a certain value, and therefore the outlier will fall on each side of the branch, but it will be treated equally than the remaining values, regardless of its magnitude.\n\nA recent research article suggests that Neural Networks could also be sensitive to outliers, provided the number of outliers is high and the deviation is also high. I would argue that if the number of outliers is high (>15% as suggested in the article), then they are no longer outliers, and rather a fair representation of that variable. A link to this article can be found in the \"Additional reading resources\" lecture within this section of the course.\n\n\n### How can outliers be identified?\n\nOutlier analysis and anomaly detection are a huge field of research devoted to optimise methods and create new algorithms to reliably identify outliers. There are a huge number of ways optimised to detect outliers in different situations. These are mostly targeted to identify outliers when those are the observations that we indeed want to focus on, for example for fraudulent credit card activity.\n\nIn this course, however, I will focus on identifying those outliers introduced by mechanical or measurement error. Those outliers that are indeed a rare case in the population, and that could be ignored. I will show how to identify those outliers, so that in later sections of the course, we can learn how to pre-process them before using the variable to train machine learning algorithms.\n\n\n### Extreme Value Analysis\n\nThe most basic form of outlier detection is **Extreme Value Analysis** of 1-dimensional data. The key for this method is to determine the statistical tails of the underlying distribution of the variable, and then find the values that sit at the very end of the tails.\n\nIf the the variable is Normally distributed (Gaussian), then the values that lie outside the mean plus or minus 3 times the standard deviation of the variable are considered outliers.\n\n- outliers = mean +/- 3* std\n\n\nIf the variable is skewed distributed, a general approach is to calculate the quantiles, and then the inter-quantile range (IQR), as follows:\n\n- IQR = 75th quantile - 25th quantile\n\nAn outlier will sit outside the following upper and lower boundaries:\n\n- Upper boundary = 75th quantile + (IQR * 1.5)\n\n- Lower boundary = 25th quantile - (IQR * 1.5)\n\nor for extreme cases:\n\n- Upper boundary = 75th quantile + (IQR * 3)\n\n- Lower boundary = 25th quantile - (IQR * 3)\n\n\n\n## Datasets for this notebook: \n\n### Predicting Survival on the Titanic: understanding society behaviour and beliefs\n\nPerhaps one of the most infamous shipwrecks in history, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 people on board. Interestingly, by analysing the probability of survival based on few attributes like gender, age, and social status, we can make very accurate predictions on which passengers would survive. Some groups of people were more likely to survive than others, such as women, children, and the upper-class. Therefore, we can learn about the society priorities and privileges at the time.\n\n====================================================================================================\n\nTo download the Titanic data, go ahead to this [website](https://www.kaggle.com/c/titanic/data)\n\nClick on the link 'train.csv', and then click the 'download' blue button towards the right of the screen, to download the dataset. Save it in a folder of your choice.\n\n**Note that you need to be logged in to Kaggle in order to download the datasets**.\n\n====================================================================================================\\\n\n### Boston house prices dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# print information for boston dataset\nfrom sklearn.datasets import load_boston\nprint(load_boston().DESCR)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### In this demo\nWe will:\n\n- Identify outliers using complete case analysis in Normally distributed variables.\n- Identify outliers using complete case analysis in skewed variables.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# for Q-Q plots\nimport pylab\nimport scipy.stats as stats\nimport seaborn as sns\n\n# boston house dataset for the demo\nfrom sklearn.datasets import load_boston","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load the the Boston House price data\n\n# load the boston dataset from sklearn\nboston_dataset = load_boston()\n\n# create a dataframe with the independent variables\n# I will use only 3 of the total variables for this demo\n\nboston = pd.DataFrame(boston_dataset.data,\n                      columns=boston_dataset.feature_names)[[\n                          'RM', 'LSTAT', 'CRIM'\n                      ]]\n\n\nboston.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load the titanic dataset\n\ntitanic = pd.read_csv('../input/titanic/train.csv',\n                      usecols=['Age', 'Fare'])\n\n# The variable age has missing values, I will\n# remove them for this demo\ntitanic.dropna(subset=['Age'], inplace=True)\n\ntitanic.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Identify variable distribution\n\nIn Normally distributed variables, outliers are those values that lie beyond the mean plus or minus 3 times the standard deviation. If the variables are skewed however, we find outliers using the inter-quantile range. In order to decide which method to utilise to detect outliers, we first need to know the distribution of the variable.\n\nWe can use histograms and Q-Q plots to determine if the variable is normally distributed. We can also use boxplots to directly visualise the outliers. Boxplots are a standard way of displaying the distribution of a variable utilising the first quartile, the median, the third quartile and the whiskers.\n\nLooking at a boxplot, you can easily identify:\n\n- The median, indicated by the line within the box.\n- The inter-quantile range (IQR), the box itself.\n- The quantiles, 25th (Q1) is the lower and 75th (Q3) the upper end of the box.\n- The wiskers, which extend to: \n  -- top whisker: Q3 + 1.5 x IQR\n  -- bottom whisker: Q1 -1.5 x IQR\n\nAny value sitting outside the whiskers is considered an outlier. Let's look at the examples below.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# function to create histogram, Q-Q plot and\n# boxplot\n\n\ndef diagnostic_plots(df, variable):\n    # function takes a dataframe (df) and\n    # the variable of interest as arguments\n\n    # define figure size\n    plt.figure(figsize=(16, 4))\n\n    # histogram\n    plt.subplot(1, 3, 1)\n    sns.distplot(df[variable], bins=30)\n    plt.title('Histogram')\n\n    # Q-Q plot\n    plt.subplot(1, 3, 2)\n    stats.probplot(df[variable], dist=\"norm\", plot=pylab)\n    plt.ylabel('RM quantiles')\n\n    # boxplot\n    plt.subplot(1, 3, 3)\n    sns.boxplot(y=df[variable])\n    plt.title('Boxplot')\n\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Normally distributed variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's start with the variable RM from the\n# boston house dataset.\n# RM is the average number of rooms per dwelling\n\ndiagnostic_plots(boston, 'RM')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the histogram and the Q-Q plot, we see that the variable rm approximates a Gaussian distribution quite well. In the boxplot, we see that the variable could have outliers, as there are many dots sitting outside the whiskers, at both tails of the distribution.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's inspect now the variable Age from the titanic\n# refers to the age of the passengers on board\n\ndiagnostic_plots(titanic, 'Age')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the histogram and the Q-Q plot, we see that the variable approximates fairly well a Gaussian distribution. There is a deviation from the distribution towards the smaller values of age. In the boxplot, we can see that the variable could have outliers, as there are many dots sitting outside the whiskers, at the right end of the distribution (top whisker in the boxplot).\n\n### Skewed variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# variable LSTAT from the boston house dataset\n# LSTAT is the % lower status of the population\n\ndiagnostic_plots(boston, 'LSTAT')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"LSTAT is not normally distributed, it is skewed with a tail to the right. According to the boxplot, there are some outliers at the right end of the distribution of the variable.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# variable CRIM from the boston house dataset\n# CRIM is the per capita crime rate by town\n\ndiagnostic_plots(boston, 'CRIM')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"CRIM is heavily skewed, with a tail to the right. There seems to be quite a few outliers as well at the right end of the distribution, according to the boxplot.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# variable Fare from the titanic dataset\n# Fare is the price paid for the ticket by\n# the passengers\n\ndiagnostic_plots(titanic, 'Fare')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fare is also very skewed, and shows some unusual values at the right end of its distribution.\n\nIn the next cells We will identify outliers using the mean and the standard deviation for the variables RM and Age from the boston and titanic datasets, respectively. Then we will use the inter-quantile range to identify outliers for the variables LSTAT, CRIM and Fare from the boston and titanic datasets.\n\n### Complete case analysis for Normally distributed variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# function to find upper and lower boundaries\n# for normally distributed variables\n\n\ndef find_normal_boundaries(df, variable):\n\n    # calculate the boundaries outside which sit the outliers\n    # for a Gaussian distribution\n\n    upper_boundary = df[variable].mean() + 3 * df[variable].std()\n    lower_boundary = df[variable].mean() - 3 * df[variable].std()\n\n    return upper_boundary, lower_boundary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate boundaries for RM\nupper_boundary, lower_boundary = find_normal_boundaries(boston, 'RM')\nupper_boundary, lower_boundary","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above we conclude that values bigger than 8.4 or smaller than 4.2 occur very rarely for the variable RM. Therefore, we can consider them outliers.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# inspect the number and percentage of outliers for RM\n\nprint('total number of houses: {}'.format(len(boston)))\n\nprint('houses with more than 8.4 rooms (right end outliers): {}'.format(\n    len(boston[boston['RM'] > upper_boundary])))\n\nprint('houses with less than 4.2 rooms (left end outliers: {}'.format(\n    len(boston[boston['RM'] < lower_boundary])))\nprint()\nprint('% right end outliers: {}'.format(\n    len(boston[boston['RM'] > upper_boundary]) / len(boston)))\n\nprint('% left end outliers: {}'.format(\n    len(boston[boston['RM'] < lower_boundary]) / len(boston)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using Extreme Value Analysis we identified outliers at both ends of the distribution of RM. The percentage of outliers is small (1.4% considering the 2 tails together), which makes sense, because we are finding precisely outliers. That is, rare values, rare occurrences.\n\nLet's move on to Age in the titanic dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate boundaries for Age in the titanic\n\nupper_boundary, lower_boundary = find_normal_boundaries(titanic, 'Age')\nupper_boundary, lower_boundary","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The upper boundary is 73 years, which means that passengers older than 73 were very few, if any, in the titanic. The lower boundary is negative. Because negative age does not exist, it only makes sense to look for outliers utilising the upper boundary.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets look at the number and percentage of outliers\n\nprint('total passengers: {}'.format(len(titanic)))\n\nprint('passengers older than 73 rooms: {}'.format(\n    len(titanic[titanic['Age'] > upper_boundary])))\nprint()\nprint('% of passengers older than 73 rooms: {}'.format(\n    len(titanic[titanic['Age'] > upper_boundary]) / len(titanic)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There were 2 passengers older than 73 on board of the titanic, which could be considered outliers, as the majority of the population where much younger.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Complete case analysis for skewed variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# function to find upper and lower boundaries\n# for skewed distributed variables\n\n\ndef find_skewed_boundaries(df, variable, distance):\n\n    # Let's calculate the boundaries outside which sit the outliers\n    # for skewed distributions\n\n    # distance passed as an argument, gives us the option to\n    # estimate 1.5 times or 3 times the IQR to calculate\n    # the boundaries.\n\n    IQR = df[variable].quantile(0.75) - df[variable].quantile(0.25)\n\n    lower_boundary = df[variable].quantile(0.25) - (IQR * distance)\n    upper_boundary = df[variable].quantile(0.75) + (IQR * distance)\n\n    return upper_boundary, lower_boundary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# looking for outliers,\n# using the interquantile proximity rule\n# IQR * 1.5, the standard metric\n\n# for LSTAT in the boston house dataset\n\nupper_boundary, lower_boundary = find_skewed_boundaries(boston, 'LSTAT', 1.5)\nupper_boundary, lower_boundary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets look at the number and percentage of outliers\n# for LSTAT\n\nprint('total houses: {}'.format(len(boston)))\n\nprint('houses with LSTAT bigger than 32: {}'.format(\n    len(boston[boston['LSTAT'] > upper_boundary])))\nprint()\nprint('% houses with LSTAT bigger than 32: {}'.format(\n    len(boston[boston['LSTAT'] > upper_boundary])/len(boston)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The upper boundary shows a value of ~32. The lower boundary is negative, however the variable LSTAT does not take negative values. So to calculate the outliers for LSTAT we only use the upper boundary. This coincides with what we observed in the boxplot earlier in the notebook. Outliers sit only at the right tail of LSTAT's distribution.\n\nWe observe 7 houses, 1.3 % of the dataset, with extremely high values for LSTAT.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# looking for outliers,\n# using the interquantile proximity rule\n# IQR * 3, now I am looking for extremely high values\n\nupper_boundary, lower_boundary = find_skewed_boundaries(boston, 'CRIM', 3)\nupper_boundary, lower_boundary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets look at the number and percentage of outliers\n# for CRIM\n\nprint('total houses: {}'.format(len(boston)))\n\nprint('houses with CRIM bigger than 14: {}'.format(\n    len(boston[boston['CRIM'] > upper_boundary])))\nprint()\nprint('% houses with CRIM bigger than 14s: {}'.format(\n    len(boston[boston['CRIM'] > upper_boundary]) / len(boston)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When using the 3  times inter-quantile range to find outliers, we find that ~6% of the houses show unusually high crime rate areas. For CRIM as well, the lower boundary is negative, so it only makes sense to use the upper boundary to calculate outliers, as the variable takes only positive values. This coincides with what we observed in CRIM's boxplot earlier in this notebook.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# finally, identify outliers in Fare in the\n# titanic dataset. I will look again for extreme values\n# using IQR * 3\n\nupper_boundary, lower_boundary = find_skewed_boundaries(titanic, 'Fare', 3)\nupper_boundary, lower_boundary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets look at the number and percentage of passengers\n# who paid extremely high Fares\n\nprint('total passengers: {}'.format(len(titanic)))\n\nprint('passengers older than 73 rooms: {}'.format(\n    len(titanic[titanic['Fare'] > upper_boundary])))\nprint()\nprint('passengers older than 73 rooms: {}'.format(\n    len(titanic[titanic['Fare'] > upper_boundary])/len(titanic)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For Fare, as well as for all the other variables in this notebook which show a tail to the right, the lower boundary is negative. So we will use the upper boundary to determine the outliers. We observe that 6% of the values of the dataset fall above the boundary. \n\n**That is all for Outlier demonstration. I hope you enjoyed. Please Up-vote this notebook !!**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 5.Date_and_Time_variable\n\n## Engineering Dates\n\nDate variables are special type of categorical variable. By their own nature, date variables will contain a multitude of different labels, each one corresponding to a specific date and sometimes time. Date variables, when preprocessed properly can highly enrich a dataset. For example, from a date variable we can extract:\n\n- Month\n- Quarter\n- Semester\n- Day (number)\n- Day of the week\n- Is Weekend?\n- Hr\n- Time differences in years, months, days, hrs, etc.\n\nIt is important to understand that date variables should not be used as the categorical variables we have been working so far when building a machine learning model. Not only because they have a multitude of categories, but also because when we actually use the model to score a new observation, this observation will most likely be in the future, an therefore its date label, will be different than the ones contained in the training set and therefore the ones used to train the machine learning algorithm.\n\n=============================================================================\n\nLet's look at how to pre-process date variables in a real life example.\n\n### Lending Club\n\n**Lending Club** is a peer-to-peer Lending company based in the US. They match people looking to invest money with people looking to borrow money. When investors invest their money through Lending Club, this money is passed onto borrowers, and when borrowers pay their loans back, the capital plus the interest passes on back to the investors. It is a win for everybody as they can get typically lower loan rates and higher investor returns.\n\nIf you want to learn more about Lending Club follow this link:\nhttps://www.lendingclub.com/\n\nThe Lending Club dataset contains complete loan data for all loans issued through the 2007-2015, including the current loan status (Current, Late, Fully Paid, etc.) and latest payment information. Features (aka variables) include credit scores, number of finance inquiries, address including zip codes and state, and collections among others. Collections indicates whether the customer has missed one or more payments and the team is trying to recover their money.\nThe file is a matrix of about 890 thousand observations and 75 variables. More detail on this dataset can be found in Kaggle's website: https://www.kaggle.com/wendykan/lending-club-loan-data\n\nLet's go ahead and have a look!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import datetime\n\n# let's load the Lending Club dataset with a few selected columns\n# just a few rows to speed things up\n\nuse_cols = ['issue_d', 'last_pymnt_d']\ndata = pd.read_csv('../input/lending-club-loan-data/loan.csv', usecols=use_cols, nrows=10000)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now let's parse the dates, currently coded as strings, into datetime format\n\ndata['issue_dt'] = pd.to_datetime(data.issue_d)\ndata['last_pymnt_dt'] = pd.to_datetime(data.last_pymnt_d)\n\ndata[['issue_d','issue_dt','last_pymnt_d', 'last_pymnt_dt']].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extracting Month from date\n\ndata['issue_dt_month'] = data['issue_dt'].dt.month\n\ndata[['issue_dt', 'issue_dt_month']].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extract quarter from date variable\n\ndata['issue_dt_quarter'] = data['issue_dt'].dt.quarter\n\ndata[['issue_dt', 'issue_dt_quarter']].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We could also extract semester\n\ndata['issue_dt_semester'] = np.where(data.issue_dt_quarter.isin([1,2]),1,2)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# day - numeric from 1-31\n\ndata['issue_dt_day'] = data['issue_dt'].dt.day\n\ndata[['issue_dt', 'issue_dt_day']].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# day of the week - from 0 to 6\n\ndata['issue_dt_dayofweek'] = data['issue_dt'].dt.dayofweek\n\ndata[['issue_dt', 'issue_dt_dayofweek']].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# day of the week - name\n\ndata['issue_dt_dayofweek'] = data['issue_dt'].dt.day_name()\n\ndata[['issue_dt', 'issue_dt_dayofweek']].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# was the application done on the weekend?\n\ndata['issue_dt_is_weekend'] = np.where(data['issue_dt_dayofweek'].isin(['Sunday', 'Saturday']), 1,0)\ndata[['issue_dt', 'issue_dt_dayofweek','issue_dt_is_weekend']].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# extract year \n\ndata['issue_dt_year'] = data['issue_dt'].dt.year\n\ndata[['issue_dt', 'issue_dt_year']].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# perhaps more interestingly, extract the date difference between 2 dates\n\n# same as above capturing just the time difference\n(data['last_pymnt_dt']-data['issue_dt']).dt.days.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# or the time difference to today, or any other day of reference\n\n(datetime.datetime.today() - data['issue_dt']).head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Similarly, if this variable had time as well, we could calculate, at what time the application was done in hrs for example, and then segregate it in: morning-afternoon-evening.**\n\nVery common date differences used in the industry include \"age\" using \"date of birth\" and the \"time of application\" (i.e., how old was the applicant when they applied for a loan). Other examples include how long the applicant has lived at th declared address.\n\n### Note\n\nOnce preprocessed in these ways, the variables are ready to be used in machine learning models following typical preprocessing of numerical or categorical variables, as shown in previous and future sections in this course.\n\n**That is all for Date-Time demonstration. I hope you enjoyed. Please Up-vote this notebook !!**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 6.Mixed_variable\n\nMixed variables are those which values contain both numbers and labels.\n\nVariables can be mixed for a variety of reasons. For example, when credit agencies gather and store financial information of users, usually, the values of the variables they store are numbers. However, in some cases the credit agencies cannot retrieve information for a certain user for different reasons. What Credit Agencies do in these situations is to code each different reason due to which they failed to retrieve information with a different code or 'label'. Like this, they generate mixed type variables. These variables contain numbers when the value could be retrieved, or labels otherwise.\n\nAs an example, think of the variable 'number_of_open_accounts'. It can take any number, representing the number of different financial accounts of the borrower. Sometimes, information may not be available for a certain borrower, for a variety of reasons. Each reason will be coded by a different letter, for example: 'A': couldn't identify the person, 'B': no relevant data, 'C': person seems not to have any open account.\n\nAnother example of mixed type variables, is for example the variable missed_payment_status. This variable indicates, whether a borrower has missed a (any) payment in their financial item. For example, if the borrower has a credit card, this variable indicates whether they missed a monthly payment on it. Therefore, this variable can take values of 0, 1, 2, 3 meaning that the customer has missed 0-3 payments in their account. And it can also take the value D, if the customer defaulted on that account.\n\nTypically, once the customer has missed 3 payments, the lender declares the item defaulted (D), that is why this variable takes numerical values 0-3 and then D.\n\n## Engineering mixed variables\n\nWe've seen that mixed variables are those which values contain both numbers and labels (see section 2: types of variables for a re-cap).\n\nHow can we engineer this type of variable to use it in machine learning?\n\nWhat we need to do in these cases is extract the categorical part in one variable, and the numerical part in a different variable. Therefore, we obtain 2 variables from the original one.\n\n**Scenario 1**\nSome mixed variables, will contain among their values either strings or numbers, but not the 2 together. This means that the value of an observation is either a string, or a number, but not a string and a number.\n\nThere are cases, like I explained in section 2, in which the variable is in nature numerical, and whenever the number could not be established for an observation, a code (i.e., a string) is utilised to define that a) information could not be retrieved and b) why it could not be retrieved.\n\nIn situations like this, we can simply extract the string into a categorical variable, and the numbers into a numerical variable. We will end up with 2 variables that contain all the information contained in the single mixed variable. After this, we should engineer the numerical and categorical variables as we would do normally for those types of variables.\n\n**Scenario 2**\nIn other cases, the variable will contain strings and numbers together for the same observation. After inspecting the variable, we can decide whether we should treat these types as categorical variables, or whether it will be more predictive if we separate the categorical and the numerical parts in different variables.\n\n\nWe will see below examples of these 2 scenarios, and how we can go about them.\n\nFor the first scenario I will simulate some data:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's load the Lending Club dataset with the variable \"Number of installment accounts opened in past 24 months\"\n# installment accounts are those that, at the moment of acquiring them, there is a set period and amount\n# of repayments agreed between the lender and borrower. An example of this is a car loan, or a student loan.\n# the borrower knows that they are going to pay a certain, fixed amount over, for example 36 months.\n\ndata = pd.read_csv('../input/lending-club-loan-data/loan.csv', usecols=['id','open_il_24m'])\n\n# let's replace the NaN with the fictitious codes described below:\n# 'A': couldn't identify the person\n# 'B': no relevant data\n# 'C': person seems not to have any account open\n# this is exactly what we did in section 2 of this course for the lecture on mixed types of variables\n\n# select which observations we will replace with each code\nindeces_b = data[data.open_il_24m.isnull()].sample(100000, random_state=44).index\nindeces_c = data[data.open_il_24m.isnull()].sample(300000, random_state=42).index\n\n# replace NA with the fictitious code\ndata.open_il_24m.fillna('A', inplace=True)\ndata.loc[indeces_b, 'open_il_24m']='B'\ndata.loc[indeces_c, 'open_il_24m']='C'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's inspect the mixed variable\n\ndata.open_il_24m.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The variable now is mixed: it contains both numbers and the three codes that we just created for this simulation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# the variable is also discrete in nature. A person can have 1, 2 accounts but not 2.3 accounts\n# let's inspect the number of observations per value of the variable\n\nfig = data.open_il_24m.value_counts().plot.bar()\nfig.set_title('Number of installment accounts open')\nfig.set_ylabel('Number of borrowers')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is how a mixed variable looks like!\n\n### Engineering mixed types of variables\n\nBelow I will demonstrate how to engineer mixed types of variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# we create 2 variables, a numerical one containing the numerical part, and\n# a categorical variable with the codes (strings)\n\ndata['open_il_24m_numerical'] = np.where(data.open_il_24m.str.isdigit(), data.open_il_24m, np.nan)\ndata['open_il_24m_categorical'] = np.where(data.open_il_24m.str.isdigit(), np.nan, data.open_il_24m,)\n\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The categorical variable captures the categorical part of the mixed original variable. Where the categorical variable contains a value, the numerical variable contains NA, and vice versa. See below.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's inspect those instances of the dataset where numerical is not null\n# we can see that when the numerical variable is not null the categorical is null\n# and vice versa\n\ndata.dropna(subset = ['open_il_24m_numerical'], axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And now we are ready to use those variables in machine learning. We should pre-process them as we learnt in previous sections. For example,  we could fill the missing values of the categorical part with the new category 'Missing'. And then we could fill the NA of the numerical  part with a value far out in the distribution.\n\n### Other types of mixed variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's load again the titanic dataset for demonstration\n\ndata = pd.read_csv('../input/titanic/train.csv', usecols = ['Ticket', 'Cabin', 'Survived'])\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The variables Ticket and Cabin, are not strictly mixed in the sense that they do not contain numbers OR strings on different observations. They contains both numbers and strings in the same observation. Perhaps, if we separate them, we can add some information to the dataset. \n\nLet's have a look at what we could do:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# for Cabin, it is relatively straightforward, we can extract the letters and the numbers in different variables\n\ndata['Cabin_numerical'] = data.Cabin.str.extract('(\\d+)') # captures numerical part\ndata['Cabin_categorical'] = data['Cabin'].str[0] # captures the first letter\n\ndata[['Cabin', 'Cabin_numerical', 'Cabin_categorical']].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ticket is not as clear...but we could still capture the first part of the ticket as a code (category)\n# and the second part of the ticket as numeric\n\ndata.Ticket.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# extract the last bit of ticket as number\ndata['Ticket_numerical'] = data.Ticket.apply(lambda s: s.split()[-1])\ndata['Ticket_numerical'] = np.where(data.Ticket_numerical.str.isdigit(), data.Ticket_numerical, np.nan)\n\n# extract the first part of ticket as category\ndata['Ticket_categorical'] = data.Ticket.apply(lambda s: s.split()[0])\ndata['Ticket_categorical'] = np.where(data.Ticket_categorical.str.isdigit(), np.nan, data.Ticket_categorical)\n\ndata[['Ticket', 'Ticket_numerical','Ticket_categorical']].head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's compare the number of categories of the newly designed variables\n\nprint('Ticket_original no of labels: ', len(data.Ticket.unique()))\nprint('Cabin_original no of labels: ', len(data.Cabin.unique()))\n\nprint('Ticket_categorical no of labels: ', len(data.Ticket_categorical.unique()))\nprint('Cabin_categorical no of labels: ', len(data.Cabin_categorical.unique()))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see how we reduced quite a bit the number of categories with this simple pre-processing of the variables. Whether these newly designed variables are or not predictive of survival, remains to be tested. I will come back to this in the last section of this course. Meanwhile, go ahead and built machine learning algorithms with these new variables, and see if they boost the performance.\n\n**That is all for Mixed Variable data demonstration. I hope you enjoyed. Please Up-vote this notebook !!**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 7.Scaling\n\n## Normalisation - Standarisation\n\nThe magnitude of the variables affects different machine learning algorithms for different reasons. In this section, I will cover a few standard ways of setting the magnitude of the variables to the same range of values.\n\n\n#### Normalisation\n\nOne method utilised to bring all the variables to a more homogeneous scale is normalisation. Normalisation is synonym of centering the distribution. This means subtracting the mean of the variable to each observation. This procedure will \"center\" the new distribution at zero (the new mean of the variable will now be zero).\n\n#### Standarisation\n\nStandarisation is also used to bring all the variables to a similar scale. Standarisation means centering the variable at zero, and standarising the variance at 1. The procedure involves subtracting the mean of each observation and then dividing by the standard deviation:\n\nz = (x - x_mean) /  std\n\nFor an overview of the different scaling methods check:\nhttp://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py\n\nLet's demonstrate the procedure of standarisation on the titanic dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# load the numerical variables of the Titanic Dataset\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\n\ndata = pd.read_csv('../input/titanic/train.csv', usecols = ['Pclass', 'Age', 'Fare', 'Survived'])\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's have a look at the values of those variables to get an idea of the magnitudes\ndata.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The different variables present different value ranges, therefore different magnitudes. Not only the minimum and maximum values are different, but they also spread over ranges of different widths.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's look at missing data\n\ndata.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Age contains missing information, so I will fill those observations with the median in the next cell.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's separate into training and testing set\nX_train, X_test, y_train, y_test = train_test_split(data[['Pclass', 'Age', 'Fare']],\n                                                    data.Survived, test_size=0.3,\n                                                    random_state=0)\nX_train.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's fill first the missing data\n\nX_train.Age.fillna(X_train.Age.median(), inplace=True)\nX_test.Age.fillna(X_train.Age.median(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Standarisation\n\nStandardScaler from scikit-learn removes the mean and scales the data to unit variance. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# standarisation: we use the StandardScaler from sklearn\n\nscaler = StandardScaler() # create an object\nX_train_scaled = scaler.fit_transform(X_train) # fit the scaler to the train set, and then transform it\nX_test_scaled = scaler.transform(X_test) # transform the test set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's have a look at the scaled training dataset: mean and standard deviation\n\nprint('means (Pclass, Age and Fare): ', X_train_scaled.mean(axis=0))\nprint('std (Pclass, Age and Fare): ', X_train_scaled.std(axis=0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As expected, the mean of each variable is now around zero and the standard deviation is set to 1. Thus, all the variable  values lie within the same range.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's look at the transformed min and max values\n\nprint('Min values (Pclass, Age and Fare): ', X_train_scaled.min(axis=0))\nprint('Max values (Pclass, Age and Fare): ', X_train_scaled.max(axis=0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"However, the minimum and maximum values vary according to how spread the variable was to begin with and is highly influenced by the presence of outliers. We observed in previous lectures that the variable Fare has a few extreme outliers, that is, people that paid extraordinarily high Fares. Those observations would drive the value of the maximum value far away from the unit variance (a value of 9.9 in this case).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's look at the distribution of the transformed variable Age\n\nplt.hist(X_train_scaled[:,1], bins=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's look at the distribution of the transformed variable Fare\n\nplt.hist(X_train_scaled[:,2], bins=20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The variable Age has a somewhat normal distribution after the transformation, reflecting the approximately Gaussian distribution that shows the original variable. Fare on the other had shows a skewed distribution, which is also evidenced after variable transformation in the previous plot. In fact, we can see that the standarisation of Fare, shrinks the majority of the observations towards the lowest values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's look at how transformed age looks like compared to the original variable\n\nsns.jointplot(X_train.Age, X_train_scaled[:,1], kind='kde')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The transformed variable has a good correlation with the original one.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's look at how transformed Fare looks like compared to the original variable\n\nsns.jointplot(X_train.Fare, X_train_scaled[:,2], kind='kde', xlim=(0,200), ylim=(-1,3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The values are concentrated on the lower left side of the plot, because Fare is highly skewed: the outliers have an influence when computing the empirical mean and standard deviation which shrink the range of the feature values as shown in the left figure above.\n\nOther scaling methods account for the presence of outliers.\n\n**That is all for data scling demonstration. I hope you enjoyed. Please Up-vote this notebook !!**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 8.Gaussian_transformations\n\nSome machine learning models like linear and logistic regression assume that the variables are normally distributed. Others benefit from \"Gaussian-like\" distributions, as in such distributions the observations of X available to predict Y vary across a greater range of values. Thus, Gaussian distributed variables may boost the machine learning algorithm performance.\n\nIf a variable is not normally distributed, sometimes it is possible to find a mathematical transformation so that the transformed variable is Gaussian.\n\n\n### How can we transform variables so that they follow a Gaussian distribution?\n\nThere are a few straightforward methods to transform variables so that they follow a Gaussian distribution. None of them is better than the other, they rather depend on the original distribution of the variables. They are:\n\n- logarithmic transformation\n- reciprocal transformation\n- square root transformation\n- exponential transformation (more general, you can use any exponent)\n- boxcox transformation\n\nTo understand how the **boxcox** transformation works, please refer to these links:\n- http://www.statisticshowto.com/box-cox-transformation/\n- http://www.itl.nist.gov/div898/handbook/eda/section3/eda336.htm\n- http://onlinestatbook.com/2/transformations/box-cox.html\n\n\nIn this notebook I will demonstrate all the transformations on the same variables for comparison, using the titanic dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pylab \nimport scipy.stats as stats","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load the numerical variables of the Titanic Dataset\n\ndata = pd.read_csv('../input/titanic/train.csv', usecols = ['Age', 'Fare', 'Survived'])\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Fill missing data with random sample","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# first I will fill the missing data of the variable age, with a random sample of the variable\n\ndef impute_na(data, variable):\n    # function to fill na with a random sample\n    df = data.copy()\n    \n    # random sampling\n    df[variable+'_random'] = df[variable]\n    \n    # extract the random sample to fill the na\n    random_sample = df[variable].dropna().sample(df[variable].isnull().sum(), random_state=0)\n    \n    # pandas needs to have the same index in order to merge datasets\n    random_sample.index = df[df[variable].isnull()].index\n    df.loc[df[variable].isnull(), variable+'_random'] = random_sample\n    \n    return df[variable+'_random']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fill na\ndata['Age'] = impute_na(data, 'Age')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Age\n### Original distribution\n\nTo visualise the distribution of the variable, Age in this case, we plot a histogram to visualise a bell-shape, and the Q-Qplot. Remember that if the variable is normally distributed, we should see a 45 degree straight line of the values over the theoretical quantiles. See the lecture \"Variable Distribution\" on section 4 for a description of Q-Q plots.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot the histograms to have a quick look at the distributions\n# we can plot Q-Q plots to visualise if the variable is normally distributed\n\ndef diagnostic_plots(df, variable):\n    # function to plot a histogram and a Q-Q plot\n    # side by side, for a certain variable\n    \n    plt.figure(figsize=(15,6))\n    plt.subplot(1, 2, 1)\n    df[variable].hist()\n\n    plt.subplot(1, 2, 2)\n    stats.probplot(df[variable], dist=\"norm\", plot=pylab)\n\n    plt.show()\n    \ndiagnostic_plots(data, 'Age')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The variable Age is almost normally distributed, except for some observations on the lower value tail of the distribution. Note the slight skew to the left in the histogram, and the deviation from the straight line towards the lower values ont he Q-Q- plot. In the following cells, I will apply the above mentioned transformations and compare the distributions of the transformed Age.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Logarithmic transformation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"### Logarithmic transformation\ndata['Age_log'] = np.log(data.Age)\n\ndiagnostic_plots(data, 'Age_log')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The logarithmic transformation, did not render a Gaussian like distribution for Age.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Reciprocal transformation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"### Reciprocal transformation\ndata['Age_reciprocal'] = 1 / data.Age\n\ndiagnostic_plots(data, 'Age_reciprocal')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The reciprocal transformation was also not useful to transform Age into a variable normally distributed.\n\n### Square root transformation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Age_sqr'] =data.Age**(1/2)\n\ndiagnostic_plots(data, 'Age_sqr')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The square root transformation is a bit more succesful that the previous2 transformations, however, the variable is still not Gaussian, and this does not represent an improvement towards normality respect the original distribution of Age.\n\n### Exponential","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Age_exp'] = data.Age**(1/1.2) # you can vary the exponent as needed\n\ndiagnostic_plots(data, 'Age_exp')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The exponential transformation is the best of all the transformations above, at the time of generating a variable that is normally distributed. Comparing the histogram and Q-Q plot of the exponentially transformed Age with the original distribution, I would say that by visual inspection the transformed variable follows more closely a Gaussian distribution. \n\n***Should I transform the variable?***\n\nIt depends on what we are trying to achieve. If this was a situation in a business setting, I would use the original variable without transformation to train the model, as this would represent a simpler situation at the time of asking developers to implement the model in real life, and also it will be easier to interpret. If on the other hand this was an exercise to win a data science competition, I would opt to use the variable that gives me the highest performance.\n\n\n### BoxCox transformation\n\nThe Box-Cox transformation is defined as: \n\nT(Y)=(Y exp(λ)−1)/λ\n\nwhere Y is the response variable and λ is the transformation parameter. λ varies from -5 to 5. In the transformation, all values of λ  are considered and the optimal value for a given variable is selected.\n\nBriefly, for each  λ (the transformation tests several λs), the correlation coefficient of the Probability Plot (Q-Q plot below, correlation between ordered values and theoretical quantiles) is calculated. \n\nThe value of λ corresponding to the maximum correlation on the plot is then the optimal choice for λ.\n\nIn python, we can evaluate and obtain the best λ with the stats.boxcox function from the package scipy.\n\nLet's have a look.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Age_boxcox'], param = stats.boxcox(data.Age) \n\nprint('Optimal λ: ', param)\n\ndiagnostic_plots(data, 'Age_boxcox')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Box Cox transformation was as good as the exponential transformation we performed above to make Age look more Gaussian. Whether we decide to proceed with the original variable or the transformed variable, will depend of the purpose of the exercise as described above.\n\n**That is all for data scling demonstration. I hope you enjoyed. Please Up-vote this notebook !!**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Conlcusion\n\n![](https://miro.medium.com/max/742/0*eySmc2fSF96yXIW0.png)\n\nFeature engineering has tended to be a tedious aspect of solving problems with machine learning and a source of errors preventing solutions from being successfully implemented. \n\nFurthermore, the feature engineering code is not hard-coded for the inputs from prediction engineering which means we can use the same exact code to make features for multiple prediction problems. Lastly, I want to conclude the article with a reminder. These techniques are not magical tools. If your data tiny, dirty and useless, feature engineering may remain incapable. Do not forget “garbage in, garbage out!”","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### More Resources on Feature Engineering\n\nWe have covered a lot of ground in this article and I hope you have a much greater appreciation of what feature engineering is, where it fits in, and how to do it.\n\nThis is really the start of your journey. You need to practice feature engineering and you need to study great practitioners of feature engineering.\n\nThis section provides some resources that might help you on your journey.\n### Books\n\nI cannot find any books or book chapters on the topic.\n\nThere are however some great books on feature extraction. If you are working with digital representations of analog observations like images, video, sound or text, you might like to dive deeper into some feature extraction literature.\n\n*    [ Feature Extraction, Construction and Selection: A Data Mining Perspective](https://www.amazon.com/dp/0792381963?tag=inspiredalgor-20)\n*    [Feature Extraction: Foundations and Applications (I like this book)](https://www.amazon.com/dp/3540354875?tag=inspiredalgor-20)\n*   [Feature Extraction & Image Processing for Computer Vision, Third Edition](https://www.amazon.com/dp/0123965497?tag=inspiredalgor-20)\n\nThere are also lots of books on feature selection. If you are working to reduce your features by removing those that are redundant or irrelevant, dive deeper into feature selection.\n\n*  [Feature Selection for Knowledge Discovery and Data Mining](https://www.amazon.com/dp/079238198X?tag=inspiredalgor-20)\n*  [Computational Methods of Feature Selection](https://www.amazon.com/dp/1584888784?tag=inspiredalgor-20)\n\n### Papers and Slides\n\nIt is a hard topic to find papers on.\n\nAgain, there are plenty of papers of feature extraction and chapters in books of feature selection, but not much of feature engineering. Also feature engineering has a meaning in software engineering as well, one that is not relevant to our discussion.\n\nHere are some generally relevant papers:\n\n*   [JMLR Special Issue on Variable and Feature Selection](jmlr.org/papers/special/feature03.html)\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Refrences:\n[*https://machinelearningmastery.com/*](https://machinelearningmastery.com/)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Please Up-vote this notebook !! it will motivate me a lot to share more of my knowledge and resources !!","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}