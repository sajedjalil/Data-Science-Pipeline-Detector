{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Approach\nThis is a brief introductory walk-through, to **Extremely Randomized Trees**, which performed best among ensemble bagging classifiers with the selected subset of explanatory variables."},{"metadata":{},"cell_type":"markdown","source":"**Two particularly interesting choices in this workbook, are Age inference using first name as a factor, assuming people with the same first name were born more or less the same year;\nThe second advance, is the creation of an Ability to negotiate feature, which is evaluated here as the Fare/Pclass (lower is better), capturing a likely essential skill to survival for passenges to work their way through a life-saving deal.**\n\n"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"from IPython.display import Image\nImage(\"../input/negotiation/negotiation.jpg\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# imports\nfrom sklearn.ensemble import  ExtraTreesClassifier\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport os\n\n# hyperparameter tuning\nfrom sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# input\ntrain = pd.read_csv('/kaggle/input/titanic/train.csv')\ntest = pd.read_csv('/kaggle/input/titanic/test.csv')\n\n# output\nunion = [train, test]\npassenger_id = []","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Studying the correlation among variables, and printing the dataset info for an overview, we understand that we will need to drop some variables, fill some **null** entries, and agreggate them into most relevant numerical buckets."},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix = train.drop(\"PassengerId\", axis=1).corr().round(2)\nsns.heatmap(corr_matrix,annot = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#info\nprint(train.info())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Development\n\nStudying **Age** correlations with other variables (e.g. **Sex** and **Pclass**) to deduct **null** values is one way to fill null variables, another could be to create a probability distribution and randomly pick replacement values from it.\n\nOut of **891/418** passengers in the training/testing sets, **177/86** have a null Age value, approximately **20%** of passengers.\n\nFor this study as an interesting alternative, **Name** is leveraged, by using regular expressions to extract the passengers first names, and then assign the corresponding Age mean in a first name group, to passengers having this very first name and missing **Age**.\n\nUsing this process allows the Age inference of 101/52 passengers in the training/testing sets respectively, leaving approximately 8% of all passengers with **null** Age. The next step is to bucket these into Age groups, which displayed similar patterns, e.g. infants showing the highest survival rate. For simplicity, the remaining null Age people will be classified as working force adults between 18 and 65 years old, which represents the vast majority of passengers."},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, df in enumerate(union):\n\n    # Age inference based on first name\n    name = df['Name'].str.split('.', n=1, expand = True)\n    name = name[1].str.split(expand = True)[0]\n    name.replace(['(\\()','(\\))'],'',regex=True, inplace = True)\n    df['Name'] = name\n    del name\n    mean_age = df[['Name','Age']].groupby(['Name']).mean()\n    df = df.merge(mean_age, on='Name')\n    df['Age'] = df['Age_x'].fillna(df['Age_y'])\n    df = df.drop(['Age_x', 'Age_y', 'Name'], axis=1)\n    \n    # Age fill na as adults\n    df['Age'] = df['Age'].fillna(df['Age'].mean())\n\n    # Age bucketing\n    age_buckets= [0,2,10,18,60,200]\n    age_labels = [0,1,2,3,4]\n    df['AgeGroup'] = pd.cut(df['Age'], bins=age_buckets, labels=age_labels, right=False)\n\n    # Parch bucketing\n    parch_buckets= [0,1,200]\n    parch_labels = [0,1]\n    df['Parch'] = pd.cut(df['Parch'], bins=parch_buckets, labels=parch_labels, right=False)\n\n    # SibSp bucketing\n    sibsp_buckets= [0,1,2,200]\n    sibsp_labels = [0,1,2]\n    df['SibSp'] = pd.cut(df['SibSp'], bins=sibsp_buckets, labels=sibsp_labels, right=False).astype(np.int8)\n    \n    # Fare\n    #df['Fare']= df['Fare'].clip(lower= df['Fare'].quantile(0.00), upper= df['Fare'].quantile(0.01), axis=0)\n    df['Fare'] = df['Fare'].fillna(df['Fare'].mean())\n    \n    # Ability to bargain\n    df['Pclass'] = df['Pclass'].astype(np.int8)    \n    df['Ability'] = df['Fare'] / df['Pclass']\n    \n    # Family size\n    df['Family'] = df['SibSp'].astype(np.int8) + df['Parch'].astype(np.int8) + 1\n    \n    # Fare and ability bucketing quartiles\n    fare_buckets= [0,23,10000]\n    fare_labels = [0,1]\n    df['Fare'] = pd.cut(df['Fare'], bins=fare_buckets, labels=fare_labels, right=False)\n    \n    ab_buckets= [0,4,9,15,20,59,70,10000]\n    ab_labels = [0,1,2,3,4,5,6]\n    df['Ability'] = pd.cut(df['Ability'], bins=ab_buckets, labels=ab_labels, right=False)\n    \n    # Cleaning\n    df = df.sort_values(by=['PassengerId'])\n    passenger_id.append(df[\"PassengerId\"])\n    df['Sex'] = pd.get_dummies(df['Sex'])\n    df['Fare'] = pd.get_dummies(df['Fare'])\n    df['SibSp'] = pd.get_dummies(df['SibSp'])\n    df['Parch'] = pd.get_dummies(df['Parch'])\n    df = df.drop(['Embarked', 'PassengerId', 'Ticket', 'Age', 'Cabin'], axis=1)\n    union[i] = df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Results\nLet's visualise the training dataset at this point, using seaborn sns on the corresponding correlation matrix."},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix = union[0].corr().round(2)\nsns.heatmap(corr_matrix,annot = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The next step will be to split the dataset into the separate subsets, e.g. isolate the \"Survived\" column, to train the Extremely Randomized Trees model."},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = union[0].drop(\"Survived\", axis=1)\ny_train = union[0][\"Survived\"]\nx_test  = union[1]\nx_train.shape, y_train.shape, x_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extremely randomized trees\nex = ExtraTreesClassifier(random_state = 6, bootstrap=True, oob_score=True)\nex.fit(x_train, y_train)\ny_pred = ex.predict(x_test)\nex.score(x_train, y_train)\nscore = round(ex.score(x_train, y_train) * 100, 2)\nprint('Extremely Randomized Trees', score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Features importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i,j in enumerate(x_train.head(1)):\n    print('%s: %s' %(j, int(ex.feature_importances_[i]*100)) + '%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#submission\nsubmission = pd.DataFrame({\n        \"PassengerId\": passenger_id[1],\n        \"Survived\": y_pred\n    })\nsubmission.to_csv('/kaggle/working/submission.csv', index=False)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}