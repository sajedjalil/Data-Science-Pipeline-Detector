{"cells":[{"metadata":{},"cell_type":"markdown","source":"# About this kernel\nThis kernel may (or may not) be helpful in your long and often tedious machine learning journey. This kernel is easily understandable to the beginner like me.  This verbosity tries to explain everything I could possibly know. Once you get through the notebook, you can find this useful and straightforward. I attempted to explain things as simple as possible.\n\nIn this kernel, I'm going to attempt the only Machine learning Algorithms to predict if a passenger survived from the sinking Titanic or not. So it's a binary classification problem. \n\nKeep Learning,\n\nvikas singh\n"},{"metadata":{},"cell_type":"markdown","source":"# CONTEXT\n* [1. Importing packages and Collecting Data](#1)\n* [2. Variable Description and Identification](#2)\n  * [2.1 Categorical and Numerical Variables ](#2.1)\n  * [2.2 Variable Data Types](#2.2)\n  * [2.3 Data Description](#2.3)\n  * [2.4 Missing Variables](#2.4)\n* [3. Feature Engineering Or Data Preprocessing](#3)\n  * [3.1 Process Cabin](#3.1)\n  * [3.2 Process Name](#3.2)\n  * [3.3 Process SibSp & Parch](#3.3)\n  * [3.4 Process Ticket](#3.4)\n  * [3.5 Outliers Detection](#3.5)\n    * [3.5.1 Outliers detection for Fare](#3.5.1)\n    * [3.5.1 Outliers detection for Age](#3.5.2)\n  * [3.6 Imputing Missing Variables](#3.6)\n     * [3.6.1 Imputing Embarked and Fare](#3.6.1)\n     * [3.6.2 Impute Age](#3.6.2)\n  * [3.7 Data Transformation](#3.7)\n      * [3.7.1 Binning Age](#3.7.1)\n      * [3.7.2 Binning Fare](#3.7.2)\n  * [3.8 Correcting Data Type](#3.8)\n  * [3.9 Dropping Features](#3.9)\n  * [3.10 Encoding Categorical Variables](#3.10)\n* [4.Model Building and Evaluation](#4)\n  * [4.1 Training Model](#4.1)\n  * [4.2 Cross-validation: Evaluating estimator performance](#4.2)\n    * [4.2.1 K-Fold Cross Validation](#4.2.1)\n    * [4.2.2 Tuning Hyperparameters](#4.2.2)\n  * [4.3 Retrain and Predict Using Optimized Hyperparameters](#4.3)\n*  [5.Prediction & Submission](#5)\n*  [6. Introduction to Ensemble](#6)\n   * [6.1 Different Ensemble Methods](#6.1)\n     * [6.1.1 Simple Ensemble Methods](#6.1.1)\n     * [6.1.2 Advanced Ensemble Methods](#6.1.2)"},{"metadata":{},"cell_type":"markdown","source":"# 1. Importing packages and Collecting Data <a id=\"1\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data Manipulattion\nimport numpy as np\nimport pandas as pd\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\n# Importing Dependencies\n%matplotlib inline\n\n# Let's be rebels and ignore warnings for now\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read and preview the train data from csv file.\ntrain = pd.read_csv('../input/train.csv')\ntrain.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read and preview the test data from csv file.\ntest = pd.read_csv('../input/test.csv')\ntrain.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Variable Description and Identification <a id=\"2\"></a>\nDescribe what each of the variable indicates and identify our response and predictor variables. Then seperate the categorical variables from numerical variables and finally identify pandas data types (i.e., object, float64 or int64) for every variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merge train and test data together. This eliminates the hassle of handling train and test data seperately for various analysis.\nmerged = pd.concat([train,test], sort = False)\nmerged.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see the shape of the combined data\nmerged.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# variable in the combined data\nmerged.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### So what can we see??\n**We can see total 12 variables. And each variable has 1309 observations (excluding Survived).**\n### Here comes the description of all variables:\n1. **PassengerId** is a unique identifying number assigned to each passenger.\n2. **Survived** is a flag that indicates if a passenger survived or died ( i.e., 0 = No, 1 = Yes).\n3. **Pclass** is the passenger class (i.e., 1 = 1st class, 2 = 2nd class, 3 = 3rd class).\n4. **Name** is the name of the passenger.\n5. **Sex** indicates the gender of the passenger (i.e., Male or female).\n6. **Age** indicates the age of the passenger.\n7. **Sibsp**  is the number of siblings/spouses aboard.\n8. **Parch** is the number of parents/children aboard.\n9. **Ticket** indicates the ticket number issued to the passenger.\n10. **Fare** indicates the amount of money spent on their ticket.\n11. **Cabin** indicates the cabin category occupied by the passenger.\n12. **Embarked** indicates the port where the passenger embarked from (i.e., C = Cherbourg, Q = Queenstown, S = Southampton).\n\n\n### Here, Survived is the target variable and rest of the variables are predictor variables.\n\n## 2.1 Categorical and Numerical Variables  <a id=\"2.1\"></a>\n**Categorical Variable:** Survived, Sex, Pclass (ordinal), Embarked, Cabin, Name, Ticket, SibSp, and Parch.\n\n**Numerical Variable:** Fare, Age, and PassengerId.\n\n## 2.2 Variable Data Types <a id=\"2.1\"></a>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# data types of different variables\nmerged.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"  * Three float variables: - Age, Fare, and Survived.\n  * Four int variables: - Pclass, SibSp, Parch and PassengerId\n  * Five Object(number+strings) variables: -  Name, Sex, Ticket, Cabin,       and Embarked."},{"metadata":{},"cell_type":"markdown","source":"## 2.3 Data Description <a id=\"2.3\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Description of the data variables\nmerged.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.4 Missing Variables <a id=\"2.4\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualization of Missing variables\nplt.figure(figsize=(8,4))\nsns.heatmap(merged.isnull(), yticklabels=False, cbar=False, cmap='plasma')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Count of missing variables\nmerged.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"  * Survived column is showing missing values becuase in test.csv don't       have the survived column.\n  * Age, Cabin, Fare and Embarked has missing values."},{"metadata":{},"cell_type":"markdown","source":"# 3. Feature Engineering Or Data Preprocessing <a id=\"3\"></a>\nIn this section, we transform raw data into understandable format.  We would engineer features like Cabin, Name, SibSp & Parch, and Ticket that could tell us something about survival or death once they're processed.\n\n## 3.1 Process Cabin <a id=\"3.1\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's preview the cabin again.\nmerged['Cabin'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we see that Cabin contains some missing values. let's count it again.\nmerged['Cabin'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's manully understand the Cabin column.\nmerged['Cabin'].value_counts().head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"  * Looks like Cabin is alphanumeric type variable with no special characters (like ., /, % etc) between letters and numbers.\n  * It has also 1014 missing obsevations.\n  * It is reasonable to presume that those NaNs didn't have a cabin.\n  * We will flag NaN as 'X' and keep only the 1st character where Cabin has alphanumeric values."},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's fill all NaNs of cabin as 'X'\nmerged['Cabin'].fillna(value = 'X', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Keeping 1st charater from the Cabin\nmerged['Cabin'] = merged['Cabin'].apply(lambda x: x[0])\nmerged['Cabin'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.2 Process Name <a id=\"3.2\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's see the Name column.\nmerged['Name'].head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"  * This column contains string that furth contains titles such as Mr, Mrs, Master etc.\n  * These title give us useful information about sex and age for example Mr=Male, Mrs=Female and married, miss= Female and young.\n  * Now we want to extract these titles from Name to check if there is any association between these titles and Survived."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extracting title from Name and create a new variable Title.\nmerged['Title'] = merged['Name'].str.extract('([A-Za-z]+)\\.')\nmerged['Title'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's see the different categories of Title from Name column.\nmerged['Title'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"  * We can see there are several titles with the very least frequency. So, it makes sense to put them in fewer buckets.\n  * Professionals like Dr, Rev, Col, Major, Capt will be put into 'Officer' bucket.\n  * Titles such as Dona, Jonkheer, Countess, Sir, Lady, Don were usually entitled to the aristocrats.\n  * We would also replace Mlle and Ms with Miss and Mme by Mrs as these are French titles."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replacing  Dr, Rev, Col, Major, Capt with 'Officer'\nmerged['Title'].replace(to_replace = ['Dr', 'Rev', 'Col', 'Major', 'Capt'], value = 'Officer', inplace=True)\n\n# Replacing Dona, Jonkheer, Countess, Sir, Lady with 'Aristocrate'\nmerged['Title'].replace(to_replace = ['Dona', 'Jonkheer', 'Countess', 'Sir', 'Lady', 'Don'], value = 'Aristocrat', inplace = True)\n\n#  Replace Mlle and Ms with Miss. And Mme with Mrs.\nmerged['Title'].replace({'Mlle':'Miss', 'Ms':'Miss', 'Mme':'Mrs'}, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's see how Tittle looks now\nmerged['Title'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.3 Process SibSp & Parch <a id=\"3.3\"></a>\n Since these two variables together indicate the size of a family, we would create a new variable 'Family_size' from these two variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merging Sibsp and Parch and creating new variable called 'Family_size'\nmerged['Family_size'] = merged.SibSp + merged.Parch + 1  # Adding 1 for single person\nmerged['Family_size'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"  * We see there are several family sizes with the very least frequency like 2,3 and some have large frequency 7,8,11\n  * We will create 4 buckets namely single, small, medium, and large for rest of them."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create buckets of single, small, medium, and large and then put respective values into them.\nmerged['Family_size'].replace(to_replace = [1], value = 'single', inplace = True)\nmerged['Family_size'].replace(to_replace = [2,3], value = 'small', inplace = True)\nmerged['Family_size'].replace(to_replace = [4,5], value = 'medium', inplace = True)\nmerged['Family_size'].replace(to_replace = [6, 7, 8, 11], value = 'large', inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's see how 'Family_size' looks now\nmerged['Family_size'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.4 Process Ticket <a id=\"3.4\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's preview the Ticket variable.\nmerged['Ticket'].head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"  * Ticket variable contains alphanumeric, only numbers and character type variables.\n  *  We will create two groups-one will contain just number and other will only contain character extracted from string.\n  * And assign 'N' to the number type variable.\n  * If a row contains both character and number, we will keep only character.\n  "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Assign N if there is only number and no character. If there is a character, extract the character only.\nticket = []\nfor x in list(merged['Ticket']):\n    if x.isdigit():\n        ticket.append('N')\n    else:\n         ticket.append(x.replace('.','').replace('/','').strip().split(' ')[0])\n# Swap values\nmerged['Ticket'] = ticket","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's count the categories in  Ticket\nmerged['Ticket'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Keeping only the 1st character to reduce the Ticket categories\nmerged['Ticket'] = merged['Ticket'].apply(lambda x : x[0])\nmerged['Ticket'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.5 Outliers Detection <a id=\"3.5\"></a>\nIn statistics, an outlier is a data point that significantly differs from the other data points in a sample. Often, outliers in a data set can alert statisticians to experimental abnormalities or errors in the measurements taken, which may cause them to omit the outliers from the data set. If they do omit outliers from their data set, significant changes in the conclusions drawn from the study may result.\n\n**See the data Description table above for  min, 1st quartile, 2nd quartile(median), 3rd quartile, and max values of a variable.**\n\n**We will use IQR method to detect the outliers for variable Age and Fare though we won't remove them.**\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a function to count total outliers.\ndef outliers(variable):\n    global filtered # Global keyword is used inside a function only when we want to do assignments or when we want to change a variable.\n    \n    # Calculate 1st, 3rd quartiles and iqr.\n    q1, q3 = variable.quantile(0.25), variable.quantile(0.75)\n    iqr = q3 - q1\n    \n    # Calculate lower fence and upper fence for outliers\n    l_fence, u_fence = q1 - 1.5*iqr , q3 + 1.5*iqr   # Any values less than l_fence and greater than u_fence are outliers.\n    \n    # Observations that are outliers\n    outliers = variable[(variable<l_fence) | (variable>u_fence)]\n    print('Total Outliers of', variable.name,':', outliers.count())\n    \n    # Drop obsevations that are outliers\n    filtered = variable.drop(outliers.index, axis = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.5.1 Outliers detection for Fare <a id=\"3.5.1\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Total number of outliers in Fare\noutliers(merged['Fare'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualisation of Fare distribution with outliers\nplt.figure(figsize=(13, 2))\nsns.boxplot(x=merged[\"Fare\"],palette='Blues')\nplt.title('Fare distribution with outliers', fontsize=15 )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualisation of Fare distribution without outliers\nplt.figure(figsize=(13, 2))\nsns.boxplot(x=filtered,palette='Blues')\nplt.title('Fare distribution without outliers', fontsize=15 )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.5.2 Outliers detection for Age <a id=\"3.5.2\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Total number of outliers in Age\noutliers(merged['Age'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualisation of Age distribution with outliers\nplt.figure(figsize=(13, 2))\nsns.boxplot(x=merged[\"Age\"],palette='Blues')\nplt.title('Age distribution with outliers', fontsize=15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualisation of Age distribution without outliers\nplt.figure(figsize=(13, 2))\nsns.boxplot(x=filtered,palette='Blues')\nplt.title('Age distribution without outliers', fontsize=15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.6 Imputing Missing Variables <a id=\"3.6\"></a>\nThere is various techniques to imputing missing variable in the datasets for exmaple linear regession, K-NN, Mean/mode/median, Deep learning etc. The simpliest way to impute missing values of a variable is to impute its missing values with its mean, median or mode depending on its distribution and variable type(categorical or numerical). \n\nHowever, one clear disadvantage of using mean, median or mode to impute missing values is the addition of bias if the amount of missing values is significant (like Age). So simply replacing them with the mean or the median age might not be the best solution since the age may differ by groups and categories of passengers.\n\nTo solve this, we can group our data by some variables that have no missing values and for each subset compute the median age to impute the missing values."},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's count the missing values for each variable\nmerged.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" * Survived column is showing missing values becuase in test.csv don't       have the survived column.\n * Age, Cabin, Fare and Embarked has missing values."},{"metadata":{},"cell_type":"markdown","source":"### 3.6.1 Imputing Embarked and Fare <a id=\"3.6.1\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# imputing Embarked with mode because Embarked is a categorical variable.\nmerged['Embarked'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here S is the most frequent\nmerged['Embarked'].fillna(value = 'S', inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Impute missing values of Fare. Fare is a numerical variable with outliers. Hence it will be imputed by median.'''\nmerged['Fare'].fillna(value = merged['Fare'].median(), inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.6.2 Impute Age <a id=\"3.6.2\"></a>\nTo impute Age with grouped median, we need to know which features are heavily correlated with Age. Let's find out the variables correlated with Age."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's plot correlation heatmap to see which variable is highly correlated with Age. We need to convert categorical variable into numerical to plot correlation heatmap. So convert categorical variables into numerical.\ndf = merged.loc[:, ['Sex', 'Pclass', 'Embarked', 'Title', 'Family_size', 'Parch', 'SibSp', 'Cabin', 'Ticket']]\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndf = df.apply(le.fit_transform) # data is converted.\ndf.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" # Inserting Age in variable correlation.\ndf['Age'] = merged['Age']\n# Move Age at index 0.\ndf = df.set_index('Age').reset_index()\ndf.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now create the heatmap correlation of df\nplt.figure(figsize=(10,6))\nsns.heatmap(df.corr(), cmap ='BrBG',annot = True)\nplt.title('Variables correlated with Age')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"  * Sex, Embarked, and Ticket have the weakest correlation with Age.\n  * Pclass and Title have strong correlation with age.\n  **So the tactic is to impute missing values of Age with the median age of similar rows according to Title and Pclass.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a boxplot to view the correlated and medium of the Pclass and Title variables with Age.\n# Boxplot b/w Pclass and Age\nsns.boxplot(y='Age', x='Pclass', data=merged)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Boxplot b/w Title and Age\nsns.boxplot(y='Age', x='Title', data=merged)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Impute Age with median of respective columns (i.e., Title and Pclass)\nmerged['Age'] = merged.groupby(['Title', 'Pclass'])['Age'].transform(lambda x: x.fillna(x.median()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's check the missing value again.\nmerged.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.7 Data Transformation <a id=\"3.7\"></a>\nIn this section, we will transform our continuous variables. After that, redundant and useless features will be deleted. And finally categorical variables will be encoded into numerical to feed our machine learning models."},{"metadata":{},"cell_type":"markdown","source":"### 3.7.1 Binning Age <a id=\"3.7.1\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating bin categories for Age \nlabel_names = ['infant', 'child', 'teenager','young_adult', 'adult', 'aged']\n\n# Create range for each bin categrories of age\ncut_points = [0,5,12,18,35,60,81]\n\n#Create and view categorized Age with original Age.\nmerged['Age_binned'] = pd.cut(merged['Age'], cut_points, labels = label_names)\n\n#Age with Categorized Age.\nmerged[['Age', 'Age_binned']].head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.7.2 Binning Fare <a id=\"3.7.2\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create bin categories for Fare\ngroups = ['low','medium','high','very_high']\n\n# Create range for each bin categories of Fare\ncut_points = [-1, 130, 260, 390, 520]\n\n#Create and view categorized Fare with original Fare\nmerged['Fare_binned'] = pd.cut(merged.Fare, cut_points, labels = groups)\n\n# Fare with Categorized Fare\nmerged[['Fare', 'Fare_binned']].head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### (Optional) Standarding Fare"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import scaling model\n#from sklearn.preprocessing import MinMaxScaler\n\n#Create a scaler object\n#scaler = MinMaxScaler()\n\n# Fit and transform the merged['Fare']\n#merged['Fare'] = scaler.fit_transform(merged['Fare'].values.reshape(-1,1))\n#merged['Fare'].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** we have option to do binning and standarding of continuous variables but here we did binning for get more accuary in the Desicion tree and Random forest classifier**"},{"metadata":{},"cell_type":"markdown","source":"## 3.8 Correcting Data Type <a id=\"3.8\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking the data type\nmerged.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correcting data types, converting into categorical variables.\nmerged.loc[:, ['Pclass', 'Sex', 'Embarked', 'Cabin', 'Title', 'Family_size', 'Ticket']] = merged.loc[:, ['Pclass', 'Sex', 'Embarked', 'Cabin', 'Title', 'Family_size', 'Ticket']].astype('category')\n\n# Due to merging there are NaN values in Survived for test set observations.\nmerged['Survived'] = merged['Survived'].dropna().astype('int') #Converting without dropping NaN throws an error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check if data types have been corrected\nmerged.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.9 Dropping Features <a id=\"3.9\"></a>\nNow we have both transformed and the original variables transformation have been made from. So we should safely drop the variables that we think would not be useful anymore for our survival analysis since they are very unlikely to be analyzed in their raw forms."},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's see all the variables\nmerged.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# droping the feature that would not be useful anymore\nmerged.drop(columns = ['Name', 'Age','SibSp', 'Parch','Fare'], inplace = True, axis = 1)\nmerged.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.10 Encoding Categorical Variables <a id=\"3.10\"></a>\nWe would like to use one hot encoding instead of label encoding. a one-hot encoding can be applied to the integer representation. This is where the integer encoded variable is removed and a new binary variable is added for each unique integer value."},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert categotical data into dummies variables\nmerged = pd.get_dummies(merged, drop_first=True)\nmerged.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4.Model Building and Evaluation <a id=\"4\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's split the train and test set to feed machine learning algorithm.\ntrain = merged.iloc[:891, :]\ntest  = merged.iloc[891:, :]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Drop passengerid from train set and Survived from test set.'''\ntrain = train.drop(columns = ['PassengerId'], axis = 1)\ntest = test.drop(columns = ['Survived'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# setting the data as input and output for machine learning models\nX_train = train.drop(columns = ['Survived'], axis = 1) \ny_train = train['Survived']\n\n# Extract test set\nX_test  = test.drop(\"PassengerId\", axis = 1).copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# See the dimensions of input and output data set.'''\nprint('Input Matrix Dimension:  ', X_train.shape)\nprint('Output Vector Dimension: ', y_train.shape)\nprint('Test Data Dimension:     ', X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.1 Training Model <a id=\"4.1\"></a>\nWe would train 5 different classifiers for this binary classification problem."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now initialize all the classifiers object.\n\n#1.Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\n\n#2.KNN\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier()\n\n#3.Decision Tree Classifier\nfrom sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier(random_state = 40)\n\n#4.Random Forest Classifier\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(random_state = 40, n_estimators = 100)\n\n#5.Support Vector Machines\nfrom sklearn.svm import SVC\nsvc = SVC(gamma = 'auto')\n\n#6. XGBoost \nfrom xgboost import XGBClassifier\nxgb = XGBClassifier(n_job = -1, random_state = 40)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a function that returns train accuracy of different models.\n\ndef train_accuracy(model):\n    model.fit(X_train, y_train)\n    train_accuracy = model.score(X_train, y_train)\n    train_accuracy = np.round(train_accuracy*100, 2)\n    return train_accuracy\n    \n# making the summary table of train accuracy.\ntrain_accuracy = pd.DataFrame({'Train_accuracy(%)':[train_accuracy(lr), train_accuracy(knn), train_accuracy(dt), train_accuracy(rf), train_accuracy(svc), train_accuracy(xgb)]})\ntrain_accuracy.index = ['LR', 'KNN','DT', 'RF', 'SVC', 'XGB']\nsorted_train_accuracy = train_accuracy.sort_values(by = 'Train_accuracy(%)', ascending = False)\n\n#Training Accuracy of the Classifiers\nsorted_train_accuracy","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**IN the above table, DT, RF, XGB, KNN models have highest train accuracy. But train accuracy of a model is not enough to tell if a model can be able to generalize the unseen data or not.\nwe can't use training accuracy for our model evaluation rather we must know how our model will perform on the data our model is yet to see.**"},{"metadata":{},"cell_type":"markdown","source":"## 4.2 Cross-validation: Evaluating estimator performance <a id=\"4.2\"></a>\nLearning the parameters of a prediction function and testing it on the same data is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data. This situation is called **overfitting.**\n\nOne thing we can do is to split the train set in two groups, usually in 80:20 ratio. That means we would train our model on 80% of the training data and we reserve the rest 20% for evaluating the model since we know the ground truth for this 20% data. This is the first model evaluation technique. In sklearn we have a train_test_split method for that.\n\nBut Train_test split has its drawbacks. Because this approach introduces bias as we are not using all of our observations for testing and also we're reducing the train data size. To overcome this we can use a technique called **cross validation** where all the data is used for training and testing periodically.\n\nHowever, as the train set gets larger, train_test_split has its advantage over k-fold cross validation. Train_test_split is k-times faster than k-fold cross validation. If the training set is very large, both train_test_split and k-fold cross validation perform identically. So for a large training data, train_test_split is prefered over k-fold cross validation to accelerate the training process.\n\n### 4.2.1 K-Fold Cross Validation ¶<a id=\"4.2.1\"></a>\n\nLet's say we will use 10-fold cross validation. So k = 10 and we have total 891 observations. Each fold would have 891/10 = 89.1 observations. So basically k-fold cross validation uses fold-1 (89.1 samples) as the testing set and k-1 (9 folds) as the training sets and calculates test accuracy.This procedure is repeated k times (if k = 10, then 10 times); each time, a different group of observations is treated as a validation or test set. This process results in k estimates of the test accuracy which are then averaged out."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a function that returns mean cross validation score for different models.\ndef val_score(model):\n    from sklearn.model_selection import cross_val_score\n    val_score = cross_val_score(model, X_train, y_train, cv = 10, scoring = 'accuracy').mean()\n    val_score = np.round(val_score*100, 2)\n    return val_score\n\n# making the summary table of cross validation accuracy.\nval_score = pd.DataFrame({'val_score(%)':[val_score(lr), val_score(knn), val_score(dt), val_score(rf), val_score(svc), val_score(xgb)]})\nval_score.index = ['LR', 'KNN','DT', 'RF', 'SVC', 'XGB']\nsorted_val_score = val_score.sort_values(by = 'val_score(%)', ascending = False)\n\n#cross validation accuracy of the Classifiers\nsorted_val_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**In the above tabel, LR and KNN have the highest cross validation accuracy among the remaining models.**"},{"metadata":{},"cell_type":"markdown","source":"### 4.2.2 Tuning Hyperparameters <a id=\"4.2.2\"></a>\nNow let's add Grid Search to all the classifiers with the hopes of optimizing their hyperparameters and thus **improving their accuracy**. Are the default model parameters the best bet? Let's find out.\n\n**For more detail visit on sklearn documention for individual models.**\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# define all the model hyperparameters one by one first\n\n# 1. For logistic regression\nlr_params = {'penalty':['l1', 'l2'],\n             'C': np.logspace(0, 2, 4, 8 ,10)}\n\n# 2. For KNN\nknn_params = {'n_neighbors':[4,5,6,7,8,9,10],\n              'weights':['uniform', 'distance'],\n              'algorithm':['auto', 'ball_tree','kd_tree','brute'],\n              'p':[1,2]}\n\n# 3. For DT\ndt_params = {'max_features': ['auto', 'sqrt', 'log2'],\n             'min_samples_split': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], \n             'min_samples_leaf':[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11],\n             'random_state':[46]}\n# 4. For RF\nrf_params = {'criterion':['gini','entropy'],\n             'n_estimators':[ 10, 30, 200, 400],\n             'min_samples_leaf':[1, 2, 3],\n             'min_samples_split':[3, 4, 6, 7], \n             'max_features':['sqrt', 'auto', 'log2'],\n             'random_state':[46]}\n# 5. For SVC\nsvc_params = {'C': [0.1, 1, 10,100], \n              'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],\n              'gamma': [ 1, 0.1, 0.001, 0.0001]}\n\n#6. For XGB\nxgb_params = xgb_params_grid = {'min_child_weight': [1, 5],\n                   'gamma': [0.04, 0, 0.1, 1.5],\n                   'subsample': [0.6, 0.8, 1.0],\n                   'colsample_bytree': [0.46, 1.0],\n                   'max_depth': [3, 7]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a function to tune hyperparameters of the selected models.\ndef tune_hyperparameters(model, param_grid):\n    from sklearn.model_selection import GridSearchCV\n    global best_params, best_score #if you want to know best parametes and best score\n    \n    # Construct grid search object with 10 fold cross validation.\n    grid = GridSearchCV(model, param_grid, verbose = 3, cv = 10, scoring = 'accuracy', n_jobs = -1)\n    # Fit using grid search.\n    grid.fit(X_train, y_train)\n    best_params, best_score = grid.best_params_, np.round(grid.best_score_*100, 2)\n    return best_params, best_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Appling tune hyperparameters in the created funtion\n\n# Tune LR hyperparameters.\ntune_hyperparameters(lr, param_grid=lr_params)\nlr_best_params, lr_best_score =  best_params, best_score\nprint('LR Best Score:', lr_best_score)\nprint('And Best Parameters:', lr_best_params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tune KNN hyperparameters\ntune_hyperparameters(knn, param_grid=knn_params)\nknn_best_params, knn_best_score =  best_params, best_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tune DT hyperparameters\ntune_hyperparameters(dt, param_grid=dt_params)\ndt_best_params, dt_best_score =  best_params, best_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tune RF hyperparameters\ntune_hyperparameters(rf, param_grid=rf_params)\nrf_best_params, rf_best_score =  best_params, best_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tune SVC hyperparameters\ntune_hyperparameters(svc, param_grid=svc_params)\nsvc_best_params, svc_best_score =  best_params, best_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tune XGB hyperparameters\nxgb_opt = XGBClassifier(learning_rate = 0.04, n_estimators = 500, \n                       silent = 1, nthread = -1, random_state = 101)\ntune_hyperparameters(xgb_opt, param_grid=xgb_params)\nxgb_best_params, xgb_best_score =  best_params, best_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets compares cross validation scores with tunned scores for different models.\n# Create a dataframe of tunned scores and sort them in descending order.'''\ntunned_scores = pd.DataFrame({'Tunned_accuracy(%)': [lr_best_score, knn_best_score, dt_best_score, rf_best_score, svc_best_score, xgb_best_score]})\ntunned_scores.index = ['LR', 'KNN', 'DT', 'RF', 'SVC', 'XGB']\nsorted_tunned_scores = tunned_scores.sort_values(by = 'Tunned_accuracy(%)', ascending = False)\n# Models Accuracy after Optimization\nsorted_tunned_scores","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"when we copares the cross validation scores with tunned scores we can see that all the classifier are improved.  Among the classifiers, RF and SVC have the highest accuracy after tunning hyperparameters."},{"metadata":{},"cell_type":"markdown","source":"## 4.3 Retrain and Predict Using Optimized Hyperparameters <a id=\"4.3\"></a>"},{"metadata":{},"cell_type":"markdown","source":"So we have our best classifiers with their best hyperparameters that produces best accuracy out of a model. That means if we retrain the classifiers using their best hyperparameters, we will be able to get the very same score that we got after tunning hyperparameters (see part 14.4). Let's retrain our classifiers and then use cross validation to calculate the accuracy of the trained model. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Instantiate the models with optimized hyperparameters.\nlr  = LogisticRegression(**lr_best_params)\nknn = KNeighborsClassifier(**knn_best_params)\ndt  = DecisionTreeClassifier(**dt_best_params)\nrf  = RandomForestClassifier(**rf_best_params)\nsvc = SVC(**svc_best_params)\nxgb = XGBClassifier(**xgb_best_params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train all the models with optimised hyperparameters\nmodels = { 'LR': lr, 'KNN':knn,'DT':dt,'RF':rf, 'SVC':svc, 'XGB':xgb}\n\n# 10-fold Cross Validation after Optimization\nscore = []\nfor x, (keys, items) in enumerate(models.items()):\n    # Train the models with optimized parameters using cross validation.\n    # No need to fit the data. cross_val_score does that for us.\n    # But we need to fit train data for prediction in the follow session.\n    from sklearn.model_selection import cross_val_score\n    items.fit(X_train, y_train)\n    scores = cross_val_score(items, X_train, y_train, cv = 10, scoring = 'accuracy')*100\n    score.append(scores.mean())\n    print('Mean Accuracy: %0.4f (+/- %0.4f) [%s]'  % (scores.mean(), scores.std(), keys))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make prediction using all the trained models\nmodel_prediction = pd.DataFrame({'LR':lr.predict(X_test), 'KNN':knn.predict(X_test), 'DT':dt.predict(X_test),'RF':rf.predict(X_test), 'SVC':svc.predict(X_test), 'XGB': xgb.predict(X_test)})\n\n#All the Models Prediction \nmodel_prediction.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Prediction & Submission <a id=\"5\"></a>\nwe will predict using both rf and svc. Then we will create two prediction files in csv format for kaggle submission."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Submission with the most accurate random forest classifier\nsubmission = pd.DataFrame({\n        \"PassengerId\": test[\"PassengerId\"],\n        \"Survived\": rf.predict(X_test)})\nsubmission.to_csv('submission_rf.csv', index = False)\n\n\n# Submission with the most accurate SVC classifier.\nsubmission = pd.DataFrame({\n        \"PassengerId\": test[\"PassengerId\"],\n        \"Survived\": svc.predict(X_test)})\nsubmission.to_csv('submission_svc.csv', index = False)\n\n# Submission with the most accurate XGB classifier.\nsubmission = pd.DataFrame({\n        \"PassengerId\": test[\"PassengerId\"],\n        \"Survived\": xgb.predict(X_test)})\nsubmission.to_csv('submission_xgb.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6. Introduction to Ensemble <a id=\"6\"></a>\nCan we further boost the accuracy of our best models? That's what we will try to do using ensemble method. Ensembles combine predictions from different models to generate a final prediction, and the more models we include the better it performs. Better still, because ensembles combine baseline predictions, they perform at least as well as the best baseline model. Most of the errors from a model’s learning are from three main factors: variance, noise, and bias. By using ensemble methods, we’re able to increase the stability of the final model and reduce the errors caused by bias, variance, and noise. By combining many models, we’re able to (mostly) reduce the variance, even when they are individually not great, as we won’t suffer from random errors from a single source. **The main principle behind ensemble modelling is to group weak learners together to form one strong learner. The most basic ensemble is majority voting rule (where the prediction or vote given by the majority of the models used as final prediction).But there are many other ways to combine predictions, and more generally we can use a model to learn how to best combine predictions.**\n\n**To implement an ensemble we need three basic things:**\n1. A group of base learners that generate predictions.\n2. A meta learner that learns how to best combine these predictions outputed by base learners.\n3. And finally a method for splitting the training data between the base learners and the meta learner.\n\n**An ensemble works best if:**\n1. There is a less correlation in the base models' predictions.\n2. We increase the number of base learners though it might slow the process down.\n\n![](https://cdn-images-1.medium.com/max/1000/0*sOtXk_8ZftGGU00_.png)"},{"metadata":{},"cell_type":"markdown","source":"## 6.1 Different Ensemble Methods <a id=\"6.1\"></a>\nWe would first categorize ensemble methods into two subcategories like 1.Simple Ensemble Methods and 2.Advanced Ensemble Methods\n\n### 6.1.1 Simple Ensemble Methods <a id=\"6.1.1\"></a>\nThey're the simpliest yet so useful form of enselbles. They can be further categorised into \n1. Voting, \n2. Averaging and \n3. Weighted Average. \n\nFirst one is usually used for classification while the later two are used for regression problems.\n\n**Voting Ensemble**  \nVoting ensemble is further classified into \n1. Hard voting and \n2. Soft voting.\n\n#### 6.1.1.1 Hard Voting (or Majority Voting or Max Voting) <a id=\"6.1.1.1\"></a>\nThis hard voting method is usually used for classification problems. The idea is to train multiple models to make predictions for each data point. The predictions by each model are considered as a ‘vote’. The predictions which we get from the majority of the models are used as the final prediction. Say rf and lr predict a class as 1 while knn predicts the same class as 0. Since the majority of the vots is casted in favour of class 1, the voting classifier would predict the very same class as 1. See the table below to understand how hard voting ensemble works.****"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Example : How hard voting works\ndata =[[1, 1, 1, 0, 1],\n       [0, 0, 0, 1, 0]]\ndisplay(pd.DataFrame(data, columns = ['Class', 'RF', 'LR', 'KNN', 'Hard_voting']).set_index('Class'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Correlation among Base Models Predictions:** How base models' predictions are correlated? If base models' predictions are weakly correlated with each other, the ensemble will likely to perform better. On the other hand, for a strong correlation of predictions among the base models, the ensemble will unlikely to perform better. To sumarize, diversity of predictions among the base models is inversely proportional to the ensemble accuracy. Let's make prediction for the test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create a data frame to store base modles prediction \nbase_prediction = model_prediction # we have already make the data frame above of all the models prediction\n\nbase_prediction.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's visualize the correlations among the predictions of base models.\nplt.figure(figsize = (15,8))\nsns.heatmap(base_prediction.corr(), annot=True)\nplt.title('Prediction correlation ammong the Base Models', fontsize = 18)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Findings:** The prediction looks quite similar for the 6 classifiers except when DT is compared to the others classifiers. Now we will create an ensemble with the base models RF, XGB, DT, KNN, and LR. This ensemble can be called heterogeneous ensemble since we have three tree based, one kernel based and one linear models. We would use **EnsembleVotingClassifier method from mlxtend.classifier module** for both hard and soft voting ensembles. The advantage is it requires lesser codes to plot decision regions and I find it a bit faster than sklearn's voting classifier."},{"metadata":{"trusted":true},"cell_type":"code","source":"# We will use mlxtend library to train, predict and plot decision regions of hard voting ensemble classifier\n# Define base models for hard voting ensemble.\nbase_models = [lr, knn, dt, rf, xgb]\n\n# Import ensemble classifier from mlxtend\nfrom mlxtend.classifier import EnsembleVoteClassifier\n\n# Initialize hard voting ensemble\nhard_evc = EnsembleVoteClassifier(clfs= base_models, voting = 'hard')\nprint('Training Hard Voting Emsemble Classification')\ndisplay(hard_evc.fit(X_train, y_train))\nprint('-----Done-----')\n\n# Predict with hard voting ensemble.\ny_pred_hard_ecv = pd.DataFrame(hard_evc.predict(X_test), columns = ['HARD_ECV'])\n\n# Hard voting cross validation score.\nprint('\\nComputing Hard Voting Cross Val Score')\nhard_x_val_score = cross_val_score(hard_evc, X_train, y_train, cv=10,  scoring = 'accuracy')\nhard_x_val_score = np.round(hard_x_val_score.mean()*100,2)\nprint('----Done----')\n\n# Compare hard voting score with best base models scores.\nhard_vs_base_score = pd.DataFrame({'Hard_vs_base_score(%)': [hard_x_val_score, lr_best_score, knn_best_score, dt_best_score, rf_best_score, xgb_best_score]})\nhard_vs_base_score.index = ['HARD_VAL_SCORE', 'LR', 'KNN', 'DT', 'RF', 'XGB']\ndisplay(hard_vs_base_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# See base models prediction with hard voting prediction.\ndf_hard_base = pd.concat([base_prediction.drop('SVC', axis=1),y_pred_hard_ecv], sort = False, axis = 1)\ndisplay(df_hard_base.head(7))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Great! We can see hard voting ensemble is considering majority of the models vote(prediction) to label a particular class. Thus it can reduce prediction errors when predicted by a single base learners.**"},{"metadata":{},"cell_type":"markdown","source":"#### 6.1.1.2 Soft Voting\nOn the other hand, When an ensembles averages based on probabilities we refer to it as soft voting. In an ensemble model, all classifiers (algorithms) are able to estimate class probabilities (i.e., they all have predict_proba() method), then we can specify Scikit-Learn to predict the class with the highest probability, averaged over all the individual classifiers. In a voting classifier setting the voting parameter to 'soft' enables the models to calculate their probability(also known as confidence score) individually and present it to the voting classifier, then the voting classifier averages them and outputs the class with the highest probability. If average probablity of class-1 is greater than class-0, it outputs predicted class is 1 otherwise 0.\n\nNote: This soft-voting classifier often work better than hard-voting as it gives more weight to highly confident votes. We Need to specify voting=”soft” and ensure that all classifiers can estimate class probabilities. One algorithm where we need to be careful is SVC, by default SVC will not give probabilities, we have to specify 'probability' hyperparameter to True. See the table below to understand how soft voting ensemble works."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Example: how soft voting works\ndata = [[0.49, 0.99, 0.49, 0.66, 1],\n        [0.51, 0.01, 0.51, 0.34, 0]]\ndisplay(pd.DataFrame(data, columns=['RF', 'LR', 'KNN', 'Average', 'Soft Voting']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's implement soft voting ensemble in mlxtend.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Base models for soft voting is the base models of hard voting\n# Initialize soft voting ensemble\nbase_model = [lr, knn, dt, rf, xgb]\nsoft_evc = EnsembleVoteClassifier(clfs = base_model, voting = 'soft')\nprint('fitting soft voting ensemble')\ndisplay(soft_evc.fit(X_train, y_train))\n\n#Predict with soft voting ensemble\ny_pred_soft_evc = pd.DataFrame(soft_evc.predict(X_test), columns = ['SOFT_EVC'])\n\n# Hard voting cross validation score\nprint('\\nComputing Soft Voting X Val Score...')\nsoft_x_val_score = cross_val_score(soft_evc, X_train, y_train, cv = 10, scoring = 'accuracy')\nsoft_x_val_score = np.round(soft_x_val_score.mean()*100, 2)\nprint('----Done----')\n\n# Compare Soft voting score with best base models scores.\nsoft_vs_base_score = pd.DataFrame({'Soft_Vs_Base_Score': [soft_x_val_score, lr_best_score, knn_best_score, dt_best_score, rf_best_score, xgb_best_score]})\nsoft_vs_base_score.index = ['SOFT_VAL_SCORE', 'LR', 'KNN', 'DT', 'RF', 'XGB']\ndisplay(hard_vs_base_score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**NICE! Soft voting perform better than all base model.**"},{"metadata":{},"cell_type":"markdown","source":"### 6.1.2 Advanced Ensemble Methods <a id=\"6.1.2\"></a>\nAdvanced ensemble methods can further be classified into\n1. Bagging\n2. Boostoing\n3. Stacking\n4. Blending\n\n#### 6.1.2.1 Bagging \nBagging, is shorthand for the combination of bootstrapping and aggregating. Bootstrapping is a method to help decrease the variance of the classifier and thus reduce overfitting. So the model created should be less overfitted than a single individual model. Bagging is more suitable for high variance low bias models (complex models). Random forest itself is an ensemble machine learning algorithm that follows the bagging technique. We would use rf as the base estimator for bagging instead of default dt. Let's try to implement bagging in sklearn:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize bagging classifier\nfrom sklearn.ensemble import BaggingClassifier\nbagg = BaggingClassifier(base_estimator = rf, verbose = 0, n_jobs = -1, random_state = 45)\nprint('Fitting Bagging Ensemble')\ndisplay(bagg.fit(X_train, y_train))\nprint('---Done----')\n\n# Bagging cross validation score.\nprint('\\nComputing Bagging X Val Score..')\nbagg_x_val_score = cross_val_score(bagg, X_train, y_train, cv = 10, scoring = 'accuracy')\nbagg_x_val_score = np.round(bagg_x_val_score.mean()*100, 2)\nprint('----Done----')\n\n# Compare bagging ensemble score with best base models scores\nbagg_vs_base_score = pd.DataFrame({'Bagging_vs_Base_Score': [bagg_x_val_score,lr_best_score, knn_best_score, dt_best_score, rf_best_score, xgb_best_score]})\nbagg_vs_base_score.index = ['BAGG', 'LR', 'KNN', 'DT', 'RF', 'XGB']\ndisplay(bagg_vs_base_score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Findings:** Bagging can't beat our best base learners."},{"metadata":{},"cell_type":"markdown","source":"#### 6.1.2.2 Boosting \nBoosting refers to any Ensemble method that can combine several weak learners into a strong learner. It does this through a weighted majority vote (classification) or a weighted sum (regression). Ada boost and Gradient boost, and Extreme gradient boost are popular models that uses boosting technique. Boosting is particularly suitable for low variance high bias models (less complex models). Unlike bagging, its a sequential ensemble technique. We will perform a simple voting ensemble of boosting classifiers rather performing boosting ensemble using only a single classifer with a base estimator. **But I used only one boosting model i.e., XGB, so this boosting ensemble method will be ineffective. For appling boosting ensemble method you need three to four boosting models.**\n\n#### 6.1.2.3 Blending \nIn blending, full training data is split into training and prediction sets. The base models (also called level 0 models) are trained on this train set and then predictions are made on this prediction set. These predictions made by base learers are then fed as an input to the meta learner (also called level 1 model). That is meta learner are trained with the output (predictions) of base learners. Blending ensemble uses only a subset of data to train base learners and another subset of data to make predictions. By only fitting every base learner once on a subset of the full training data, Blend ensemble is a fast ensemble that can handle very large datasets simply by only using portion of it at each stage. The cost of this approach is that information is thrown out at each stage, as one layer will not see the training data used by the previous layer. **We will use BlendEnsemble method from mlens.ensemble module to perform blending.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Perform blending in mlens\nfrom mlens.ensemble import BlendEnsemble\n\n# Initialize blend ensembler\nblend = BlendEnsemble(n_jobs = -1, test_size = 0.5, random_state = 45)\n\n# Base models for blending.\nbase_models = [rf, dt, knn, xgb]\nblend.add(base_models)\n\n# Meta learner for blending. We will use lr.'''\nblend.add_meta(lr)\n\n# Train the blend ensemble.\nprint('Fitting Blending...')\ndisplay(blend.fit(X_train, y_train))\nprint('----Done----')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 6.1.2.4 Stacking (Or Stacked Generalization) \nIn blending, we trained the base learners and the meta learner on only half the data, so a lot of information is lost. To prevent this, we need to use a cross-validation strategy. Fitting an ensemble with cross-validation is often referred to as stacking, while the ensemble itself is known as the Super Learner. So basically in stacking, the individual classification models (or base models) are trained on the complete training set; then, the meta-classifier is fitted on the outputs (predictions) of those base learners. The meta-classifier can either be trained on the predicted class labels or probabilities from the ensemble.\n\n**The basic difference between blending and stacking is therefore that stacking allows both base learners and the meta learner to train on the full data set.The outcome of stacking is improved accuracy which is typical for small and medium-sized data sets, where the effect of blending can be severe. As the data set size increases, blending and stacking performs similarly and hence for large data sets blending is preferred over stacking since stacking takes significant amount of time to train the ensemble. We will use package vecstack to perform stacking that can save you from writing a lot of codes if you implement stacking from scratch.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import stacking method from vecstack\nfrom vecstack import stacking\nfrom sklearn.metrics import accuracy_score\n\n# Initialize base models. We will use the same base models as blending\nbase_models = [rf, dt, xgb, knn]\n\n# Perform stacking\nS_train, S_test = stacking(base_models,                # list of base models\n                           X_train, y_train, X_test,   # data\n                           regression = False,         # classification task (if you need \n                                                       # regression - set to True)\n                           mode = 'oof_pred_bag',      # mode: oof for train set, predict test \n                                                       # set in each fold and vote\n                           needs_proba = False,        # predict class labels (if you need \n                                                       # probabilities - set to True) \n                           save_dir = None,            # do not save result and log (to save \n                                                       # in current dir - set to '.')\n                           metric = accuracy_score,    # metric: callable\n                           n_folds = 10,               # number of folds\n                           stratified = True,          # stratified split for folds\n                           shuffle = True,             # shuffle the data\n                           random_state = 45,          # ensure reproducibility\n                           verbose = 1)                # print progress","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**So now we have OOF from base (or 0 level models) models and we can build level 1 model. We have 4 base models (level 0 models), so we expect to get 5 columns in S_train and S_test. S_train will be our input feature to train our meta learner and then prediction will be made on S_test after we train our meta learner. And this prediction on S_test is actually the prediction for our test set (X_test). Before we train our meta learner we can investigate S_train and S_test.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Input features for meta learner')\ndisplay(S_train[:7])\n\nprint('Test/output (prediction set for meta learner')\ndisplay(S_test[:7])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dimension of S_train and S_test\nprint('Dimension of S_train:', S_train.shape)\nprint('Dimension of S_test:', S_test.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize 1st level model that is our meta learner. We will use lr\nsuper_learner = lr \n    \n# Fit meta learner on the output of base learners\nprint('Fitting Stacking...')\nsuper_learner.fit(S_train, y_train)\n\nprint('Done.')\n# Finally predict using super learner.\ny_pred_super = super_learner.predict(S_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting with different ensembles\n\n# Hard voting\nsubmission = pd.DataFrame({\n        \"PassengerId\": test[\"PassengerId\"],\n        \"Survived\": hard_evc.predict(X_test)})\nsubmission.to_csv('submission_hard_evc.csv', index = False)\n\n# Soft voting\nsubmission = pd.DataFrame({\n        \"PassengerId\": test[\"PassengerId\"],\n        \"Survived\": soft_evc.predict(X_test)})\nsubmission.to_csv('submission_soft_evc.csv', index = False)\n\n# Bagging\nsubmission = pd.DataFrame({\n        \"PassengerId\": test[\"PassengerId\"],\n        \"Survived\": bagg.predict(X_test)})\nsubmission.to_csv('submission_bagg.csv', index = False)\n\n# Blending\nsubmission = pd.DataFrame({\n        \"PassengerId\": test[\"PassengerId\"],\n        \"Survived\": blend.predict(X_test).astype(int)})\nsubmission.to_csv('submission_blend.csv', index = False)\n\n# Stacking\nsubmission = pd.DataFrame({\n        \"PassengerId\": test[\"PassengerId\"],\n        \"Survived\": y_pred_super.astype(int)})\nsubmission.to_csv('submission_super.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**My other kernel:- **\n\n***Simple Titanic-Exploratory Data Analysis:*** [Here](https://www.kaggle.com/vikassingh1996/titanic-exploratory-data-analysis-the-beginning)\n\n***Extensive Data Preprocessing And Modeling*** [Here](https://www.kaggle.com/vikassingh1996/extensive-data-preprocessing-and-modeling)\n\n**If you find my kernel useful, Some upvotes will be appreciated.**\n\n--Thank you for reading!"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}