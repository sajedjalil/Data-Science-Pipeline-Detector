{"cells":[{"metadata":{},"cell_type":"markdown","source":"# My Titanic Solution\n## Top 15% Using Simple Classifiers\nHi. Welcome to my very first Kaggle Notebook, solving the classical Titanic problem. It's an interesting question, since it's possible to explore the basic framework necessary to solve a Machine Learning problem, using a small dataset with a easy perspective to test different models and techniques."},{"metadata":{},"cell_type":"markdown","source":"# 1. Libraries\nIt's a good practice to include all the used libraries on the top of the code structure:"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"%matplotlib inline\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport re\nimport os\nimport inspect\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import axes3d, Axes3D\nimport seaborn as sns\nimport statistics as sta\nimport graphviz\nimport xgboost as xgb\n\nfrom plotnine import *\nfrom collections import Counter\n\nfrom sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin\nfrom sklearn.utils.validation import check_X_y, check_array, check_is_fitted\nfrom sklearn.utils.multiclass import unique_labels\nfrom sklearn.metrics import euclidean_distances, silhouette_samples, silhouette_score, accuracy_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier, AdaBoostClassifier\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score, learning_curve, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC \nfrom mlxtend.classifier import StackingClassifier\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans, DBSCAN\n\nfrom xgboost import XGBClassifier\nfrom yellowbrick.cluster import SilhouetteVisualizer\nfrom IPython.display import clear_output, display, FileLinks","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's also change the font color and size in our matplotlib graphics..."},{"metadata":{"trusted":true},"cell_type":"code","source":"COLOR = 'black'\nmpl.rcParams['text.color'] = COLOR\nmpl.rcParams['axes.labelcolor'] = COLOR\nmpl.rcParams['xtick.color'] = COLOR\nmpl.rcParams['ytick.color'] = COLOR\nplt.rcParams.update({'font.size': 18})\nplt.subplots_adjust(wspace = 15, hspace = 15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Feature Engineering\nTime to start! The first step in our solution is to analyse every feature that we have, starting by an overview of the dataset.\n\n# 2.1. Dataset Overview"},{"metadata":{},"cell_type":"markdown","source":"First, let's take a fast looking at the table we have."},{"metadata":{"trusted":true},"cell_type":"code","source":"original_training_df =  pd.read_csv('/kaggle/input/train.csv')\noriginal_training_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then, we can check the general attributes of our data and the non null elements"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"original_training_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"original_training_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"original_training_df.isnull().sum().divide(len(original_training_df.index)).multiply(100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"original_training_df['Name'][1:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2.2. Extract Title from Name\n\nSo, we have a first insight. Time to ask ourselves: would it be possible to extract more informations of this table? And ths answer is: yes, of course! We can, for instance, extract the title of every person from the \"Name\" column"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_title(dataframe_in):\n    dataframe_in['Title'] = dataframe_in['Name'].apply(lambda X: re.search('[A-Z]{1}[a-z]+\\.', X).group(0))\n    return dataframe_in\n    \ndataframe_transformations_test = original_training_df.copy()\ndataframe_transformations_test = get_title(dataframe_transformations_test)\ndataframe_transformations_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can visualize the titles over a histogram. What conclusion can we get? We have many differents types of titles and the size of the sample $(\\approx 800)$ samples is not enough to hangle a column with so many types of categorical information.\n\nA simple approach to solve that problem is to notice that most of people have a \"Mr.\", \"Miss.\", \"Mrs.\" or \"Master.\" title. All the other ones we are going to classify in a common box, that we are going to call \"Rare\"."},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe_transformations_test['Title'].value_counts().plot(kind='bar')\n\ndef classify_title(dataframe_in):\n    dataframe_in.loc[:, ['Title']] = dataframe_in['Title'].apply(lambda X: X if X in ['Mr.', 'Miss.', 'Mrs.', 'Master.'] else 'Rare')\n    return dataframe_in\n    \ndataframe_transformations_test = classify_title(dataframe_transformations_test)\ndataframe_transformations_test['Title'].value_counts().plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can drop out the Passenger ID column"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe_transformations_test.drop('PassengerId', axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can also dummify the sex column: convert the categorical data to zeros and ones:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_sex_to_number(dataframe_in):\n    dataframe_in['Sex'] = dataframe_in['Sex'].apply(lambda X: 0 if X == 'female' else 1)\n    return dataframe_in\n\ndataframe_transformations_test = convert_sex_to_number(dataframe_transformations_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe_transformations_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2.3. Cabin and Ticket Analysis\n* The same problem that we found with the Title attribute can be seen here: we have many cabins. To start our analysis here, let's get just the first letter of the cabin and consider that letter as a classification label.\n* We can get just the prefix of the ticket numbers and create a new attribute"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe_transformations_test.Cabin.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_cabin_letter(dataframe_in):\n    dataframe_in['Cabin'] = dataframe_in['Cabin'].apply(lambda X: re.search('[A-Za-z]{1}', X).group(0).upper() if isinstance(X, str) else '?')\n    return dataframe_in\n\ndef get_ticket_prefix(dataframe_in):\n    dataframe_in['Ticket'] = dataframe_in['Ticket'].apply(lambda X: X.split(' ')[0] if len(X.split(' ')) > 1 else '?')\n    return dataframe_in\n    \ndataframe_transformations_test = get_cabin_letter(dataframe_transformations_test)\ndataframe_transformations_test = get_ticket_prefix(dataframe_transformations_test)\ndataframe_transformations_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"During the data analysis step, we have already seen that there are also many missing data when we are looking at the Cabin column. A first approach to fill the empty vales is to assume that people from the same family are also in the same cabin. So, let's write a function to collect the family name of each passenger and check all the rows without a known cabin. Then, let's fill those fields with a cabin of another member of the family.\n\nWe can drop out the name column after that, as we have already got all the possible information from the passenger names. After this procedure, we are still going to find many missing cabins. Let's simply input \"?\" on those cabins since the fact that this field is unkown can be used as a information itself. If a cabin is unknown for a passenger then, maybe, that cabin was located in a more dangerous place."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_family_name(dataframe_in):\n    dataframe_in['FamilyName'] = dataframe_in['Name'].apply(lambda X: re.search('[A-Z]{1}[a-z ]+', X).group(0))\n    return dataframe_in\n\ndataframe_transformations_test = get_family_name(dataframe_transformations_test)\ndataframe_transformations_test.drop(['Name'], axis = 1, inplace = True)\n\ndict_cabins_per_family = dict()\nfor current_family_name in dataframe_transformations_test.FamilyName.unique().tolist():\n    filter_family_name = (dataframe_transformations_test['FamilyName'] == current_family_name)\n    filter_known_cabin = (dataframe_transformations_test['Cabin'] != '?')\n    listCabinsFromFamily = dataframe_transformations_test.loc[(filter_family_name) &\\\n                                                              (filter_known_cabin)].Cabin.unique().tolist()\n    \n    if len(listCabinsFromFamily) > 0:\n        max_v, mode = 0, None\n        for curr_cabin, v in Counter(listCabinsFromFamily).items():\n            if v > max_v:\n                max_v, mode = v, curr_cabin\n        dict_cabins_per_family[current_family_name] = mode\n\ndef get_family_cabin_per_row(df_row):\n    if df_row.FamilyName in dict_cabins_per_family and df_row.Cabin == '?':\n        out = dict_cabins_per_family[df_row.FamilyName]\n    else:\n        out = df_row.Cabin\n    return out\n\ndef get_family_cabin(df):\n    \n    df['Cabin'] = df.apply(get_family_cabin_per_row, axis = 1)\n    df['Cabin'] = df['Cabin'].fillna('?')\n    \n    return df\n\ndataframe_transformations_test['Cabin'] = get_family_cabin(dataframe_transformations_test)\ndataframe_transformations_test.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We still have two more columns with missing data and our next step is to get the easiest one: the \"Embarked\" column..."},{"metadata":{},"cell_type":"markdown","source":"# 2.4. Embarked Analysis\nFilling this column with the most frequent occurence (the median) would not lead us to a great loss on the quality of out analysis as we have just 2 rows with that problem:"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe_transformations_test['Embarked'].value_counts().plot(kind = 'bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def embarked_fillna_median(df):\n    df.loc[:, ['Embarked']] = df['Embarked'].fillna(value = df['Embarked'].value_counts().idxmax())\n    return df\n\ndataframe_transformations_test = embarked_fillna_median(dataframe_transformations_test)\ndataframe_transformations_test.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2.5.A. Age Analysis - A Simple Approach\nWe can see in the following curves that every people with a \"Master.\" title tends to have a small age at aroung 10 years. Since we have a low variance in that subset of samples, we can just fill the masters column with the average age of the masters.\n\nThen we just repeat the procedure to the non masters. It would lead us to a better approach to threat the missing age data. It's a really simple approach but by doing this and using a convenient structure of classifiers it's possible to reach a good final accuracy as we are going to see later."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(20, 10))\nax1 = fig.add_subplot(311)\nax2 = fig.add_subplot(312)\nax3 = fig.add_subplot(313)\n\ndataframe_transformations_test['FamilyMembers'] = dataframe_transformations_test['SibSp'] + dataframe_transformations_test['Parch']\n\nsns.catplot(x = 'Parch', y = 'Age', hue = 'Title', data = dataframe_transformations_test, kind = \"swarm\", ax = ax1)\nplt.close(2)\nsns.catplot(x = 'SibSp', y = 'Age', hue = 'Title', data = dataframe_transformations_test, kind = \"swarm\", ax = ax2)\nplt.close(2)\nsns.catplot(x = 'FamilyMembers', y = 'Age', hue = 'Title', data = dataframe_transformations_test, kind = \"swarm\", ax = ax3)\nplt.close(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"is_master = (dataframe_transformations_test['Title'] == 'Master.')\nmaster_age_series = dataframe_transformations_test.loc[is_master]['Age']\nprint(master_age_series.mean())\nprint(master_age_series.median())\nprint(master_age_series.std())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fill_age_from_masters(df, strategy_in = 'median'):\n    is_master = (df['Title'] == 'Master.')\n    imp = SimpleImputer(missing_values = np.nan, strategy = strategy_in)\n    df.loc[is_master, 'Age'] = imp.fit_transform(df.loc[is_master][['Age']])\n    return df\n\ndataframe_transformations_test = fill_age_from_masters(dataframe_transformations_test)\ndataframe_transformations_test.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe_transformations_test['Age'].median()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fill_age_from_non_masters(df, strategy_in = 'median'):\n    is_not_master = (df['Title'] != 'Master.')\n    imp = SimpleImputer(missing_values = np.nan, strategy = strategy_in)\n    df.loc[is_not_master, 'Age'] = imp.fit_transform(df.loc[is_not_master][['Age']])\n    return df\n\ndataframe_transformations_test = fill_age_from_non_masters(dataframe_transformations_test)\ndataframe_transformations_test.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe_transformations_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"original_training_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2.5.B. Age Analysis - A Clustering Approach\nFilling the people with a \"Master.\" title seems to be suficient for them since it was possible to observe that they were all young people with an age distribution characterized by a low variance. \n\nBut this assumption doesn't seem to hold really well when we are talking about the \"Non Masters\". So, my approach to solve this problem is exposed in the following sequence of steps:\n\n1. Check the correlation of the \"Age\" column with all the other columns and identify which variables are more correlated with it.\n2. Take these variables and use them to cluster the passenger in different groups - We will have a problem here: categorical variables should be dummified before the clustering algorithm and, after the dummifying procedure, we will obtain many sparse columns on the pre processed matrix.\n3. So, to solve the sparse columns problem, we will run a Principal Component Analysis over the Dummyfied dataset, get the principal componentes obtained and run the clustering algorithm over these principal components.\n4. The clustering algorithm used will be a simple K-Means. The value of K (Number of clusters) will be obtained by a Silhouette Analysis, which will be explained later.\n\nSo, we can't start this analysis at this point, we need to follow these steps after pre processing the data, and we will come back to this approach later."},{"metadata":{},"cell_type":"markdown","source":"# 2.6. Getting Family Features\nWe can infer other informations about the family structure of each passenger:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_family_info(df_in):\n    df_in['FamilyMembers'] = df_in['Parch'] + df_in['SibSp'] + 1\n    df_in['Is_Mother'] = np.where((df_in.Title=='Mrs.') & (df_in.Parch >0), 1, 0)\n    return df_in","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Pre Processing\nAfter the exploratory analysis we can pre process the variables. This part is composed of two steps: first we define the transformations in a modular form, then we blend them in a pipeline."},{"metadata":{},"cell_type":"markdown","source":"# 3.1. Defining Transformations\nThe \"TransformerSignificantData\" has the objective to realize the necessary interpretations over the basic dataset and is strongly based on the exploratory analysis that we have done in the section 2. It's composed by the following steps:\n\n* Get the first letters of each cabin\n* Get the title from each name\n* Classify the title according to the convention defined in 2.\n* Get the family name for each passenger and\n* Use the family names to expand the labels to other people with unknown cabins\n\nThe last step will not fill all the missing cabin data but can, at least, give us a better glimpse over some passengers."},{"metadata":{"trusted":true},"cell_type":"code","source":"class TransformerSignificantData(BaseEstimator, TransformerMixin):\n    \n    def __init__(self):\n        return\n        \n    def fit(self, X, y = None):\n        return self\n    \n    def transform(self, X, y = None):\n        \n        out = get_cabin_letter(X)\n        out = get_ticket_prefix(out)\n        out = get_title(out)\n        out = get_family_info(out)\n        out = classify_title(out)\n        out = get_family_name(out)\n        out = get_family_cabin(out)\n        \n        return out","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The transformer dummify variable will be used to handle the categorical columns. The columns listed in dim_redundant_columns will be removed to avoid the \"Curse of Dimensionality\" problem. It happens when you have more columns than the necessary to code the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"class TransformerDummify(BaseEstimator, TransformerMixin):\n    \n    def __init__(self):\n        return\n    \n    def fit(self, X, y = None):\n        return self\n    \n    def transform(self, X, y = None):\n        \n        columns_to_dummify = ['Sex', 'Cabin', 'Embarked', 'Title', 'Ticket']\n        useless_columns = ['PassengerId', 'Name', 'FamilyName']\n        dim_redundant_cols = ['Sex_male', 'Cabin_?', 'Embarked_S', 'Title_Rare', 'Ticket_?']\n        \n        out_dummies = pd.get_dummies(X[columns_to_dummify], prefix = columns_to_dummify)\n        \n        out = pd.concat([X, out_dummies], axis = 1)\n        out = out.drop(useless_columns + columns_to_dummify + dim_redundant_cols, axis = 1)\n        \n        return out","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The transformer missing data will fill the \"Age\" rows with missing data considering the procedure described in section 2: if the person is a \"Master', we'll input the masters average age and the non masters average age will be placed in the othwe missing age rows. The same procedure will be used to fill the \"Fare\" column"},{"metadata":{"trusted":true},"cell_type":"code","source":"class TransformerMissingData(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, missing_age_masters_strategy = 'mean', missing_age_non_masters_strategy = 'mean'):\n        self.missing_age_masters_strategy = missing_age_masters_strategy\n        self.missing_age_non_masters_strategy = missing_age_non_masters_strategy\n        \n    def fit(self, X, y = None):\n        return self\n    \n    def transform(self, X, y = None):\n\n        is_not_master = (X['Title_Master.'] == 0)\n        is_master = (X['Title_Master.'] == 1)\n        \n        imp_non_master = SimpleImputer(missing_values = np.nan, \n                                       strategy = self.missing_age_non_masters_strategy)\n        \n        imp_master = SimpleImputer(missing_values = np.nan, \n                                   strategy = self.missing_age_masters_strategy)\n        \n        imp_fare = SimpleImputer(missing_values = np.nan,\n                                 strategy = 'mean')\n        \n        X.loc[is_not_master, 'Age'] = imp_non_master.fit_transform(X.loc[is_not_master, ['Age']])\n        X.loc[is_master, 'Age'] = imp_master.fit_transform(X.loc[is_master, ['Age']])\n        X.loc[:, 'Fare'] = imp_fare.fit_transform(X.loc[:, ['Fare']])\n\n        return X ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, it's necessary to normalize the data. We can map the numbers on the interval $[0, 1]$ using the \"MinMaxScaler\" or center the intervals around the mean and divide them by the standard deviation using the \"StandardScaler\" transformation. Let's briefly show each possibility:\n* $X = \\frac{X - b}{a - b}$ - MinMaxScaler using a interval between 0 and 1, where $b$ is the minimum value observed and $a$ is the maximum value observed\n* $X = \\frac{X - \\mu}{\\sigma}$ - Where $\\mu$ is the mean of the known values of the column and $\\sigma$ referes to the standard deviation\nThe categorical values will be mapped between zero and one using the MinMaxScaler and the continuous variables (Age and Fare) will be mapped using a StandardScaler to avoid the high sensibility to outliers present in the MinMaxScaler."},{"metadata":{"trusted":true},"cell_type":"code","source":"class TransformerNormalize(BaseEstimator, TransformerMixin):\n    \n    def __init__(self):\n        return\n    \n    def fit(self, X, y = None):\n        return self\n    \n    def transform(self, X, y = None):\n        \n        out = X\n        out.loc[:, ['Pclass']] = MinMaxScaler().fit_transform(out[['Pclass']])\n        out.loc[:, ['Age', 'Fare']] = StandardScaler().fit_transform(out[['Age', 'Fare']])\n        \n        return out","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"# 3.2. Enhancing the Missing Age Solution\nLet's get our dummy dataset and follow the sequence steps proposed on the section 2.5.B."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_dummy = pd.concat([pd.read_csv('/kaggle/input/train.csv'), pd.read_csv('/kaggle/input/test.csv')], ignore_index = True)\ndummy_pipeline = Pipeline([\n    ('prepare', TransformerSignificantData()),\n    ('dummify', TransformerDummify()),\n    ('normalize', TransformerNormalize())\n])\ndf_dummy = dummy_pipeline.transform(df_dummy)\ndf_dummy.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_dummy = df_dummy[df_dummy['Title_Master.'] == 0]\ndf_dummy.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Which variables are most correlated to the age? We can check the correlation table to get the answer:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_dummy_corr = df_dummy.corr().apply(abs).sort_values(by = 'Age', axis = 0, ascending = False)\nsns.set(rc={'figure.figsize':(12.0,10.0)})\nsns.heatmap(df_dummy_corr, annot=False, cmap='YlGnBu')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_dummy_corr.loc[df_dummy_corr.index.values != 'Age', :]['Age'].plot.bar()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The next step is to get the Principal Components of this"},{"metadata":{},"cell_type":"markdown","source":"We can confirm that if we have a master, then the absolute value of the correlation will be significant and we have already found a simple way to threat it. So, we are going to ignore the \"Title_Master\" column. We can, then, observe some correlation with the age for the following variables:"},{"metadata":{"trusted":true},"cell_type":"code","source":"related_to_age = ['Pclass', 'Title_Miss.', 'SibSp', 'Cabin_C', 'Title_Mr.', 'Fare', 'Parch']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We just selected all the variables with more than $15\\%$ of Pearson Coefficient (ignoring the Ticket Variables to get a simple clustering model). Now, it's time to decompose these columns in Principal Components (PCA - Principal Component Analysis)."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_pca = df_dummy.copy().loc[:, related_to_age]\ndf_pca.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_pca = df_pca.fillna(value = df_pca['Fare'].mean())\ndf_pca.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_age = PCA().fit(df_pca)\ndf_explained_var = pd.DataFrame({\n    'Index': range(0, len(pca_age.explained_variance_ratio_)),\n    'Ratio': pca_age.explained_variance_ratio_,\n    'Cumulative': np.cumsum(pca_age.explained_variance_ratio_)\n})\nggplot(data = df_explained_var, mapping = aes(x = 'Index')) +\\\n    geom_bar(mapping = aes(y = 'Ratio'), stat = 'identity') +\\\n    geom_line(mapping = aes(y = 'Cumulative'), stat = 'identity')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.cumsum(pca_age.explained_variance_ratio_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Clustering over the 3 most important principal components will give us $80%$ of explained variance. Let's plot these variables over a $R^3$ space and check the pattern of the points:"},{"metadata":{"trusted":true},"cell_type":"code","source":"np_pca_significant = pca_age.transform(df_pca.copy())[:, :3]\n\nfig = plt.figure(figsize=(15, 15))\nax = fig.add_subplot(111, projection = '3d')\n\nxs = np_pca_significant[:, 1]\nys = np_pca_significant[:, 2]\nzs = np_pca_significant[:, 0]\nax.scatter(xs, ys, zs, s = 50, alpha = 0.6, edgecolors = 'w')\n\nax.set_xlabel('X1')\nax.set_ylabel('X2')\nax.set_zlabel('X0')\n\nplt.xlim(-3, 2)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that we can aggregate the points in $4$ clusters, divided by planes. Let's try to do so by trying to find an optimal clustering algorithm:\n* We can compare a K-Means algorithm among many different values of K. The optimal value can be calculated by getting the higher average silhouette score.\n* Then, we can try to run a DBSCAN algorithm (Density-Based Spatial Clustering of Applications with Noise)\n\nWe can notice that the best K-Means model is not as good as the DBSCAN one. The hyperparameters of the DBSCAN algorithm were found directly by a inspection over the 3D plot of points."},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans2 = KMeans(n_clusters = 2, random_state = 42, n_init = 500).fit_predict(np_pca_significant)\nkmeans3 = KMeans(n_clusters = 3, random_state = 42, n_init = 500).fit_predict(np_pca_significant)\nkmeans4 = KMeans(n_clusters = 4, random_state = 42, n_init = 500).fit_predict(np_pca_significant)\nkmeans5 = KMeans(n_clusters = 5, random_state = 42, n_init = 500).fit_predict(np_pca_significant)\nkmeans6 = KMeans(n_clusters = 6, random_state = 42, n_init = 500).fit_predict(np_pca_significant)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dbscan_model = DBSCAN(eps = 0.7, min_samples = 20).fit(np_pca_significant)\ndbscan_predict = dbscan_model.fit_predict(np_pca_significant)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"silhouette_avg2 = silhouette_score(np_pca_significant, kmeans2)\nsilhouette_avg3 = silhouette_score(np_pca_significant, kmeans3)\nsilhouette_avg4 = silhouette_score(np_pca_significant, kmeans4)\nsilhouette_avg5 = silhouette_score(np_pca_significant, kmeans5)\nsilhouette_avg6 = silhouette_score(np_pca_significant, kmeans6)\n\ndf_avg_silh = pd.DataFrame({\n    'Avg. Silh. Score': [silhouette_avg2, silhouette_avg3, silhouette_avg4, silhouette_avg5, silhouette_avg6],\n    'K': [2, 3, 4, 5, 6]\n})\n\nggplot(df_avg_silh, aes(x = 'K', y = 'Avg. Silh. Score')) + geom_line(stat = 'identity')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The average silhouette score is maximum when $K = 3$. So, let's color the data points and check what happens:"},{"metadata":{"trusted":true},"cell_type":"code","source":"np_pca_significant = pca_age.transform(df_pca.copy())[:, :3]\n\nfig = plt.figure(figsize=(30, 15))\n\nax = fig.add_subplot(121, projection = '3d')\n\nxs = np_pca_significant[:, 1]\nys = np_pca_significant[:, 2]\nzs = np_pca_significant[:, 0]\ndict_color = {0: 'b', 1: 'r', 2: 'k', 3: 'g', 4: 'c', 5: 'y', 6: 'm'}\nax.scatter(xs, ys, zs, s = 50, alpha = 0.6, edgecolors = 'w', \n           c = [dict_color[i % 7] for i in dbscan_predict])\n\nax.set_xlabel('X1')\nax.set_ylabel('X2')\nax.set_zlabel('X0')\nax.title.set_text('DBSCAN Clusters')\n\nplt.xlim(-3, 2)\n\nax = fig.add_subplot(122, projection = '3d')\n\nxs = np_pca_significant[:, 1]\nys = np_pca_significant[:, 2]\nzs = np_pca_significant[:, 0]\ndict_color = {0: 'b', 1: 'r', 2: 'k', 3: 'g', 4: 'c', 5: 'y', 6: 'm'}\nax.scatter(xs, ys, zs, s = 50, alpha = 0.6, edgecolors = 'w', \n           c = [dict_color[i % 7] for i in kmeans3])\n\nax.set_xlabel('X1')\nax.set_ylabel('X2')\nax.set_zlabel('X0')\nax.title.set_text('Best KNN (K=3) Clusters')\n\nplt.xlim(-3, 2)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, we are going to assign to each row with missing ages the mean value of the age for its respective cluster using the DBSCAN algorithm. Would it be better than just assign the mean value to all non masters missing age rows?\n\nWe can't assure that. That's why we'll slightly change the missing age transformation, adding one more option to the hyperparameter that determines the missing age filling procedure:"},{"metadata":{"trusted":true},"cell_type":"code","source":"class TransformerMissingData(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, missing_age_masters_strategy = 'mean', missing_age_non_masters_strategy = 'cluster'):\n        self.missing_age_masters_strategy = missing_age_masters_strategy\n        self.missing_age_non_masters_strategy = missing_age_non_masters_strategy\n        \n    def fit(self, X, y = None):\n        return self\n    \n    def transform(self, X, y = None):\n\n        is_not_master = (X['Title_Master.'] == 0)\n        is_master = (X['Title_Master.'] == 1)\n        \n        imp_non_master = SimpleImputer(missing_values = np.nan, \n                                       strategy = self.missing_age_non_masters_strategy)\n        \n        imp_master = SimpleImputer(missing_values = np.nan, \n                                   strategy = self.missing_age_masters_strategy)\n        \n        imp_fare = SimpleImputer(missing_values = np.nan,\n                                 strategy = 'mean')\n        \n        X.loc[is_master, 'Age'] = imp_master.fit_transform(X.loc[is_master, ['Age']])\n        X.loc[:, 'Fare'] = imp_fare.fit_transform(X.loc[:, ['Fare']])\n        \n        if self.missing_age_non_masters_strategy in ['mean', 'median']:\n            X.loc[is_not_master, 'Age'] = imp_non_master.fit_transform(X.loc[is_not_master, ['Age']])\n            \n        elif self.missing_age_non_masters_strategy == 'cluster':\n            \n            X_without_age = X.drop(['Age', 'Survived'], axis = 1)\n            print(X_without_age.isnull().sum())\n            pca_fit_vec = PCA(n_components = 3).fit_transform(X_without_age)\n            \n            X['ClusterLabel'] = DBSCAN(eps = 0.7, min_samples = 20).fit_predict(pca_fit_vec)\n            dict_means_per_cluster = dict()\n            for cluster_label in X['ClusterLabel'].tolist():\n                dict_means_per_cluster[cluster_label] = X.loc[X['ClusterLabel'] == cluster_label, :]['Age'].mean()\n            \n            age_list = X['Age'].tolist()\n            cluster_list = X['ClusterLabel'].tolist()\n            for i, curr_age in enumerate(age_list):\n                if np.isnan(curr_age):\n                    age_list[i] = dict_means_per_cluster[cluster_label]\n            X['Age'] = age_list\n                \n        return X","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3.3. Defining the Pre Processing Pipeline"},{"metadata":{},"cell_type":"markdown","source":"Then, we can finally chain all of these transformations in a pipeline, that will be applied over our dataset before training, testing and validating our models. It's important to see that, since we are using a standard scaler, the pipeline must be used over the whole dataset (training + testing) at once. If we ignore this observation, we will have the same value of a non categorical column mapped to two different values over the training and testing sets."},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_engineering_pipeline = Pipeline([\n    ('prepare', TransformerSignificantData()),\n    ('dummify', TransformerDummify()),\n    ('normalize', TransformerNormalize()),\n    ('missing', TransformerMissingData())\n])\n\ndf_all_original = pd.concat([pd.read_csv('/kaggle/input/train.csv'), \n                             pd.read_csv('/kaggle/input/test.csv')], ignore_index = True)\ndf_all_original.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all_processed = feature_engineering_pipeline.transform(df_all_original.copy())\ndf_all_processed.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all_processed.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, we can split our variables in training set, training labels and testing set. The results obtained over the testing set will be submited. Here comes a tricky part: we should never use the validation set to test our results. We will actually compare the validation results with a test set that we will split out of the samples before starting the cross validation step: testing the results in a training set where the training and tunning were not realized can let us get a better information about the generalization power of the estimators.\n\nSo:\n* The parameters of the models are determined over the training set\n* The hyper parameters of the models are determined over the validation set and\n* The best model will be get comparing each one of the obtained models over the testing set"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_ans = df_all_processed[df_all_processed['Survived'].isnull()]\ndf_ans = df_ans.loc[:, df_ans.columns != 'Survived']\n\ndf_cv_and_test = df_all_processed[df_all_processed['Survived'].notnull()]\n\ndf_cv_and_test_X = df_cv_and_test.drop(['Survived', 'ClusterLabel'], axis = 1)\ndf_cv_and_test_Y = df_cv_and_test['Survived']\n\ndf_cv_X, df_test_X, df_cv_Y, df_test_Y = train_test_split(df_cv_and_test_X,\n                                                          df_cv_and_test_Y,\n                                                          test_size = 0.1,\n                                                          random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Applying Different Machine Learning Models\n\n$3$ simple models are goind to be used over our preprocessed dataset:\n* A Random Forest Classifier\n* A Support Vector Machine\n* And a Logistic Regression\nAfter that, we are going to see how to combine these models into a better classifier by using different types of \"Ensemble Learning\" techniques.\n\n# 4.1. Training and Testing the Models\n# 4.1.A. Training and Testing Single Models\nThe models will be trained and tested all over the testing set using a K-Fold Cross Validation. The optimal hyper parameters are sequentially inspected. \nThe Grid Search technique over the Cross Validation folds will be used to find the optimal values for these parameters. Tuning the hyper parameters if important to avoid overfitting: they are responsible for the correct regularization of our models."},{"metadata":{"trusted":true},"cell_type":"code","source":"hyper_rf = dict(\n    n_estimators = [85, 86, 87, 88, 89], # OPT = 88\n    max_depth = [3, 4, 5, 6, 7, 8, 9, 10],\n    random_state = [42]\n)\n\nhyper_svm = dict(\n    C = [6.0, 7.0, 8.0], # 7 = OPT\n    kernel = ['linear', 'poly', 'rbf', 'sigmoid'],\n    gamma = ['auto', 'scale'],\n    probability = [True],\n    random_state = [42]\n)\n\nhyper_logit = dict(\n    C = [0.1, 1.0, 10.0], # 1 = OPT\n    solver = ['lbfgs'],\n    random_state = [42]\n)\n\nhyper_adaboost = dict(\n    n_estimators = [54, 55, 56],# 55 = OPT\n    learning_rate = [0.35, 0.4, 0.45], # 0.4 = OPT\n    random_state = [42]\n)\n\nhyper_xgboost = {\n    'eta': [0, 0.00001], # 0 = OPT\n    'gamma': [0.001, 0.05, 0.1], # 0.05 =OPT\n    'max_depth': [3, 4, 5], # 4 = OPT\n    'probability': [True],\n    'random_state': [42]\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_cv = 10\n\ngscv_rf = GridSearchCV(RandomForestClassifier(), hyper_rf, scoring = 'accuracy', cv = n_cv)\ngscv_svm = GridSearchCV(SVC(), hyper_svm, scoring = 'accuracy', cv = n_cv)\ngscv_logit = GridSearchCV(LogisticRegression(), hyper_logit, scoring = 'accuracy', cv = n_cv)\ngscv_adab = GridSearchCV(AdaBoostClassifier(), hyper_adaboost, scoring = 'accuracy', cv = n_cv)\ngscv_xbt = GridSearchCV(XGBClassifier(), hyper_xgboost, scoring = 'accuracy', cv = n_cv)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Avoid the temptation of testing the score over the validation set - you can get too optimistic results! Then, let's start by splitting our dataset in testing and training / validation dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('1. Fitting Random Forest')\ngscv_rf.fit(df_cv_X.copy(), df_cv_Y.copy())\nprint('2. Fitting SVM')\ngscv_svm.fit(df_cv_X.copy(), df_cv_Y.copy())\nprint('3. Fitting Logit')\ngscv_logit.fit(df_cv_X.copy(), df_cv_Y.copy())\nprint('4. Fitting AdaBoost')\ngscv_adab.fit(df_cv_X.copy(), df_cv_Y.copy())\nprint('5. Fitting XBoostTree')\ngscv_xbt.fit(df_cv_X.copy(), df_cv_Y.copy())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The boxplots represent the training scores and the bars represent the respective scores over the training set. The higher the bar score is, the better is the generalization power of our model."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_CV_score(classifier_name, n_cv):\n    ans = [0] * n_cv\n    for i in range(0, n_cv):\n        command_str = classifier_name + '.cv_results_[\\'split' + str(i) + '_test_score\\']'\n        ans[i] = eval(command_str)[0]\n    return ans\n\ntrain_score_list_rf = get_CV_score('gscv_rf', n_cv)\ntrain_score_list_svm = get_CV_score('gscv_svm', n_cv)\ntrain_score_list_logit = get_CV_score('gscv_logit', n_cv)\ntrain_score_list_adab = get_CV_score('gscv_adab', n_cv)\ntrain_score_list_xbt = get_CV_score('gscv_xbt', n_cv)\n\ndf_train_score = pd.DataFrame(dict(\n    estimator = ['1. rf'] * n_cv + ['2. svm'] * n_cv + ['3. logit'] * n_cv + ['4. adaboost'] * n_cv + ['5. xboosttree'] * n_cv,\n    score = train_score_list_rf + train_score_list_svm + train_score_list_logit + train_score_list_adab + train_score_list_xbt\n))\n\ndf_test_score = pd.DataFrame(dict(\n    estimator = ['1. rf'] + ['2. svm'] + ['3. logit'] + ['4. adaboost'] + ['5. xboosttree'],\n    score = [accuracy_score(gscv_rf.best_estimator_.predict(df_test_X), df_test_Y),\n             accuracy_score(gscv_svm.best_estimator_.predict(df_test_X), df_test_Y),\n             accuracy_score(gscv_logit.best_estimator_.predict(df_test_X), df_test_Y),\n             accuracy_score(gscv_adab.best_estimator_.predict(df_test_X), df_test_Y),\n             accuracy_score(gscv_xbt.best_estimator_.predict(df_test_X), df_test_Y)]\n))\n\nggplot(aes(x = 'estimator', color = 'estimator', y = 'score')) +\\\n    geom_boxplot(data = df_train_score) +\\\n    geom_jitter(data = df_train_score) + coord_cartesian(ylim = [0.7, 1]) +\\\n    geom_bar(data = df_test_score, stat = 'identity', alpha = 0.2, position = \"dodge\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_opt_hyper(dict_hyper, model):\n    for k in dict_hyper.keys():\n        print(k + ': ' + str(model.best_estimator_.get_params()[k]) + ' -- ' + str(dict_hyper[k]))\n\nprint('Optimal Hyperparameters:')\nprint('---')\nprint('1. Random Forest')\nprint_opt_hyper(hyper_rf, gscv_rf)\nprint('---')\nprint('2. SVM')\nprint_opt_hyper(hyper_svm, gscv_svm)\nprint('---')\nprint('3. Logistic Regression')\nprint_opt_hyper(hyper_logit, gscv_logit)\nprint('---')\nprint('4. Ada Boost')\nprint_opt_hyper(hyper_adaboost, gscv_adab)\nprint('---')\nprint('5. XGBTree')\nprint_opt_hyper(hyper_xgboost, gscv_xbt)\nprint('---')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4.1.B. Evaluate Learning Curves of the Single Models\nIt's important to evaluate the learning curve of each model to check if they are underfitted or overfitted by comparing the training score with the validation score."},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_learning_curve(train_scores, valid_scores, train_sizes):\n    \n    size_x_valid, size_y_valid = valid_scores.shape\n    size_x_train, size_y_train = train_scores.shape\n    \n    Y_df_valid = []\n    X_df_valid = []\n    \n    Y_df_train = []\n    X_df_train = []\n    \n    for i in range(0, len(train_sizes)):\n        for j in range(0, size_y_valid):\n            Y_df_valid.append(valid_scores[i][j])\n            X_df_valid.append(train_sizes[i])\n            \n    for i in range(0, len(train_sizes)):\n        for j in range(0, size_y_train):\n            Y_df_train.append(train_scores[i][j])\n            X_df_train.append(train_sizes[i])\n            \n    df_valid = pd.DataFrame({'X': X_df_valid, 'Y': Y_df_valid})\n    df_train = pd.DataFrame({'X': X_df_train, 'Y': Y_df_train})\n    \n    return df_valid, df_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sizes_rf, train_scores_rf, valid_scores_rf = learning_curve(gscv_rf.best_estimator_, df_cv_X.copy(), df_cv_Y.copy(), train_sizes = range(50, 700, 50), cv = 10)\ndf_valid, df_train = plot_learning_curve(train_scores_rf, valid_scores_rf, range(50, 700, 50))\nggplot(mapping = aes(x = 'X', y = 'Y')) + geom_point(data = df_valid, color = 'blue', size = 3, alpha = 0.1) + stat_smooth(data = df_valid, color = 'blue') +\\\n                                          geom_point(data = df_train, color = 'red', size = 3, alpha = 0.1) + stat_smooth(data = df_train, color = 'red') +\\\n                                          ggtitle('Learning Curve - Random Forest')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sizes_svm, train_scores_svm, valid_scores_svm = learning_curve(gscv_svm.best_estimator_, df_cv_X.copy(), df_cv_Y.copy(), train_sizes = range(50, 700, 50), cv = 10)\ndf_valid, df_train = plot_learning_curve(train_scores_svm, valid_scores_svm, range(50, 700, 50))\nggplot(mapping = aes(x = 'X', y = 'Y')) + geom_point(data = df_valid, color = 'blue', size = 3, alpha = 0.1) + stat_smooth(data = df_valid, color = 'blue') +\\\n                                          geom_point(data = df_train, color = 'red', size = 3, alpha = 0.1) + stat_smooth(data = df_train, color = 'red') +\\\n                                          ggtitle('Learning Curve - SVM')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sizes_logit, train_scores_logit, valid_scores_logit = learning_curve(gscv_logit.best_estimator_, df_cv_X.copy(), df_cv_Y.copy(), train_sizes = range(50, 700, 50), cv = 10)\ndf_valid, df_train = plot_learning_curve(train_scores_logit, valid_scores_logit, range(50, 700, 50))\nggplot(mapping = aes(x = 'X', y = 'Y')) + geom_point(data = df_valid, color = 'blue', size = 3, alpha = 0.1) + stat_smooth(data = df_valid, color = 'blue') +\\\n                                          geom_point(data = df_train, color = 'red', size = 3, alpha = 0.1) + stat_smooth(data = df_train, color = 'red') +\\\n                                          ggtitle('Learning Curve - Logistic Regression')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sizes_adab, train_scores_adab, valid_scores_adab = learning_curve(gscv_adab.best_estimator_, df_cv_X.copy(), df_cv_Y.copy(), train_sizes = range(50, 700, 50), cv = 10)\ndf_valid, df_train = plot_learning_curve(train_scores_adab, valid_scores_adab, range(50, 700, 50))\nggplot(mapping = aes(x = 'X', y = 'Y')) + geom_point(data = df_valid, color = 'blue', size = 3, alpha = 0.1) + stat_smooth(data = df_valid, color = 'blue') +\\\n                                          geom_point(data = df_train, color = 'red', size = 3, alpha = 0.1) + stat_smooth(data = df_train, color = 'red') +\\\n                                          ggtitle('Learning Curve - Adaboost')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sizes_xbt, train_scores_xbt, valid_scores_xbt = learning_curve(gscv_xbt.best_estimator_, df_cv_X.copy(), df_cv_Y.copy(), train_sizes = range(50, 700, 50), cv = 10)\ndf_valid, df_train = plot_learning_curve(train_scores_xbt, valid_scores_xbt, range(50, 700, 50))\nggplot(mapping = aes(x = 'X', y = 'Y')) + geom_point(data = df_valid, color = 'blue', size = 3, alpha = 0.1) + stat_smooth(data = df_valid, color = 'blue') +\\\n                                          geom_point(data = df_train, color = 'red', size = 3, alpha = 0.1) + stat_smooth(data = df_train, color = 'red') +\\\n                                          ggtitle('Learning Curve - Extreme Boosting Tree')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, we have compared each one of the optimal single models. It's interesting to ask ourselves: would it be better to blent these models into a single one and check if we can get even better results? The answer is yes and we will see different approaches in the next section.\n\nLooking at the learning curves we can notice that we don't have overfitting the models. Maybe the Random Forest and the XBoost Tree is underfitted, even after tunning their hyper parameters. Since the validation curve ends with a horizontal slope for both cases, the problem doesn't seem to be solvable by just changing the (already tunned) regularization constants.\n\nSo, the curves suggest that we need to extract more significant features from the dataset or increase the training set size to reach an optimal testing score. The bigger sample used during the submission step as well as the use of ensembling models can be used as a solution to this problem."},{"metadata":{},"cell_type":"markdown","source":"# 4.1.C. Training and Testing Ensemble Models\nThe models can be combined in different ways:\n* In a \"Voting Classifier\", each model has a vote and the winning guess will represent the output of the composed estimator. This classifier can be divided in two types:\n    * The \"Hard Voting Classifier\" is composed by models that have, all of them, a vote with weight one - So, in this case, all models have the same contribution over each output\n    * In the \"Soft Voting Classifier\", each model contribute with the estimated final probability, which can be any value in the interval $[0, 1]$. So, the vote of models that are more \"confident\" about the answer will have a bigger priority for each one of the inputs\n* Finally, we can construct a \"Meta Classifier\", in a process known as \"Stacking\", where the outputs of each model serve as input to another model and its output will represent the final answer. Notice that it's also possible to stack models in multiple layers.\n\nThe process of combining multiple models into an ensemble classifier is called \"Stacking\". The quality of this type of model tends to be better when the models are independent. In this case, the law of large numbers hold and the final accuracy will have a tendence to be bigger."},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator_list = [('rf', gscv_rf.best_estimator_), \n                  ('svm', gscv_svm.best_estimator_), \n                  ('logit', gscv_logit.best_estimator_),\n                  ('adab', gscv_adab.best_estimator_),\n                  ('xbt', gscv_xbt.best_estimator_)]\n\nhard_vote_estimator = VotingClassifier(estimator_list)\nsoft_vote_estimator = VotingClassifier(estimator_list)\nmeta_logit_estimator = StackingClassifier(classifiers = [X[1] for X in estimator_list], \n                                          meta_classifier = LogisticRegression())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%capture\n\nhard_cv = GridSearchCV(hard_vote_estimator, param_grid = {'voting': ['hard']}, \n                       scoring = 'accuracy', cv = n_cv).fit(df_cv_X.copy(), df_cv_Y.copy())\n\nsoft_cv = GridSearchCV(soft_vote_estimator, param_grid = {'voting': ['soft']},\n                       scoring = 'accuracy', cv = n_cv).fit(df_cv_X.copy(), df_cv_Y.copy())\n\nstack_cv = GridSearchCV(meta_logit_estimator, param_grid = {'meta_classifier__C': [0.1, 1, 10, 100, 500]},\n                        scoring = 'accuracy', cv = n_cv).fit(df_cv_X.copy(), df_cv_Y.copy())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Hard Voting Score: ' + str(hard_cv.best_score_))\nprint('Soft Voting Score: ' + str(soft_cv.best_score_))\nprint('Stacking Score: ' + str(stack_cv.best_score_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_score_list_hv = get_CV_score('hard_cv', n_cv)\ntrain_score_list_sv = get_CV_score('soft_cv', n_cv)\ntrain_score_list_st = get_CV_score('stack_cv', n_cv)\n\ndf_train_score = pd.DataFrame(dict(\n    estimator = ['1. rf'] * len(train_score_list_rf) + \n                ['2. svm'] * len(train_score_list_svm) + \n                ['3. logit'] * len(train_score_list_logit) +\n                ['4. adaboost'] * len(train_score_list_adab) +\n                ['5. xbtree'] * len(train_score_list_xbt) +\n                ['6. hard voter'] * len(train_score_list_hv) + \n                ['7. soft voter'] * len(train_score_list_sv) +\n                ['8. stacking'] * len(train_score_list_st)\n    , score = train_score_list_rf + train_score_list_svm + train_score_list_logit + train_score_list_adab +\\\n                            train_score_list_xbt + train_score_list_hv + train_score_list_sv + train_score_list_st\n))\n\ndf_test_score = pd.DataFrame(dict(\n    estimator = ['1. rf'] + ['2. svm'] + ['3. logit'] + ['4. adaboost'] + ['5. xbtree'] + ['6. hard voter'] + ['7. soft voter'] + ['8. stacking'],\n    score = [accuracy_score(gscv_rf.best_estimator_.predict(df_test_X), df_test_Y),\n             accuracy_score(gscv_svm.best_estimator_.predict(df_test_X), df_test_Y),\n             accuracy_score(gscv_logit.best_estimator_.predict(df_test_X), df_test_Y),\n             accuracy_score(gscv_adab.best_estimator_.predict(df_test_X), df_test_Y),\n             accuracy_score(gscv_xbt.best_estimator_.predict(df_test_X), df_test_Y),\n             accuracy_score(hard_cv.best_estimator_.predict(df_test_X), df_test_Y),\n             accuracy_score(soft_cv.best_estimator_.predict(df_test_X), df_test_Y),\n             accuracy_score(stack_cv.best_estimator_.predict(df_test_X), df_test_Y),\n            ]\n))\n\nggplot(aes(x = 'estimator', y = 'score', color = 'estimator')) + geom_boxplot(data = df_train_score) +\\\n       geom_jitter(data = df_train_score) + theme(axis_text_x = element_text(angle = 70, \n                                                  hjust = 1, vjust = 0.5)) + coord_cartesian(ylim = [0.7, 1]) +\\\n       geom_bar(data = df_test_score, stat = 'identity', alpha = 0.2, position = \"dodge\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4.1.D. Bonus - Plotting Feature Importances\nSometimes, in data science projects, we can get more results after the first implementation. That's why we can't say that there exists a fixed framework: it's common to work in cycles, in a procedure called CRISP-DM (which stands for \"Cross-Industry Standard Process for Data Mining\". \n\nThen, if we get any insight from the following graphic that will be shown, we can just go back to the feature engineering part and readapt out analysis. With an optimal model of Random Forests, we can plot the feature importances as we can see in the next figure."},{"metadata":{"trusted":true},"cell_type":"code","source":"importance_list = gscv_rf.best_estimator_.feature_importances_.tolist()\ndf_feature_importances = pd.DataFrame({\n    'Feature': df_cv_X.columns,\n    'Importance': importance_list\n})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.barplot(data = df_feature_importances, x = 'Feature', y = 'Importance')\nplt.setp(g.get_xticklabels(), rotation=90)\nprint('Feature Importances:')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And these informations can serve as informations to be used in an enhanced model, considering that the weight of the most important features should have a bigger weight than the weight of the other columns."},{"metadata":{},"cell_type":"markdown","source":"# 5. Final Submission\nWe can notice that the optimal solution is represented by the soft voter model. Which performs slightly better than the support vector machine (SVM). The cell below produces a link with the final result using the soft voter obtained. By using this file we can reach an accuracy of $79.9 \\%$ approximately. And reach the top $15 \\%$. You can use this part of the code as an example of how to get the results easily and then submit them."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_ans.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_ids = ['RandomForest', 'SVM', 'AdaBoost', 'Logistic', 'XBTree', 'SoftVoting', 'HardVoting', 'Stacking']\nfor i, model in enumerate([gscv_rf, gscv_svm, gscv_adab, gscv_logit, gscv_xbt, soft_cv, hard_cv, stack_cv]):\n    submission_estimator = model.best_estimator_.fit(df_cv_and_test_X, df_cv_and_test_Y)\n    survival_ans_col = pd.DataFrame({'Survived': submission_estimator.predict(df_ans.drop('ClusterLabel', axis = 1, inplace = False))})\n\n    df_submission = pd.read_csv('/kaggle/input/test.csv')\n    df_submission['Survived'] = [int(X) for X in survival_ans_col['Survived'].to_list()]\n    df_submission = df_submission.loc[:, ['PassengerId', 'Survived']]\n    df_submission.to_csv('submission.csv')\n\n    os.chdir('/kaggle/working')\n    df_submission.to_csv('submission-' + model_ids[i] + '.csv', index = False)\n    \nFileLinks('.')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}