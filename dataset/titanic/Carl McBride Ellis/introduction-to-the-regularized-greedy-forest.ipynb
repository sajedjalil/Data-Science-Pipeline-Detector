{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction to the Regularized Greedy Forest (RGF)\nThe RGF is a powerful technique developed by Rie Johnson and Tong Zhang in the paper [\"Learning Nonlinear Functions Using Regularized Greedy Forest\"](https://arxiv.org/pdf/1109.0887.pdf). It is on a par with gradient boosting tools like [XGBoost](https://xgboost.ai/). An ensemble of the solutions produced form these methods may well be good enough to win a kaggle competition.\n## Decision Trees\n[Decision trees](https://scikit-learn.org/stable/modules/tree.html) are perhaps one of the most venerable techniques used in machine learning, notably the [ID3](https://link.springer.com/content/pdf/10.1007/BF00116251.pdf) and **C4.5** algorithms by Ross Quinlan. \nDecision trees are simple to implement and to explain, but are prone to [overfitting](https://en.wikipedia.org/wiki/Overfitting).\nThey can be used for both classification, for example see [sklearn.tree.DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html), and regression, see [sklearn.tree.DecisionTreeRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html), and hence the acronym CART. \nDecision trees are what are now known as *weak learners*.\n## Ensembles and the Decision Forest\nIt was shown by that one is able to create a *strong learner* from a collection, or '*ensemble*', of weak learners in a famous paper by Robert Schapire [\"The Strength of Weak Learnability\"](https://link.springer.com/content/pdf/10.1007/BF00116037.pdf). It was from this idea that came the **decision forest**, in which, as the name suggests, one by one a collection of decision trees is created. This goes by the name of [*boosting*](https://en.wikipedia.org/wiki/Boosting_&#40;machine_learning&#41;). The boosting process is what is known as being [*greedy*](https://en.wikipedia.org/wiki/Greedy_algorithm); each individual step is optimal (for example, each tree added to the forest) at the time, but this does not necessarily lead to an overall optimal solution.\n\n## Regularization\n[Regularization](https://en.wikipedia.org/wiki/Regularization_&#40;mathematics&#41;) is a technique designed to prevent [overfitting](https://en.wikipedia.org/wiki/Overfitting). In gradient boosting, an implicit regularization effect is achieved by small step size $s$ or [*shrinkage parameter*](https://en.wikipedia.org/wiki/Shrinkage_&#40;statistics&#41;), which for best results should tend to be infinitesimally small. However, as one can imagine this is not viable in practice. In the end one chooses as small an $s$ as possible, in conjunction with an *early stopping* criteria.\nIn the RGF however,  an explicit regularization is used to prevent overfitting using [structured sparsity](https://en.wikipedia.org/wiki/Structured_sparsity_regularization) where the underlying forest structure is viewed as a graph of sparsity structures. In RGF one has an ensemble of forest nodes rather than individual trees.\n\n## What is RGF?\nIn the words of the authors of RGF:\n\n> \"RGF integrates two ideas: one is to include tree-structured regularization into the learning formulation; and the other is to employ the  fully-corrective  regularized  greedy  algorithm.  Since  in  this  approach  we  are  able  to  take  advantage  of  the special  structure  of  the  decision  forest\"\n\n# Why use the RGF?\nThe regularized greedy forest has been shown to out-perform [gradient boosting decision trees](https://en.wikipedia.org/wiki/Gradient_boosting) (GBDT), which is a technique used by [XGBoost](https://xgboost.ai/), [LightGBM](https://www.microsoft.com/en-us/research/project/lightgbm/), and [CatBoost](https://catboost.ai/). Indeed, RGF was used by the kagglers [infty](https://www.kaggle.com/infty36878) and [random modeler](https://www.kaggle.com/randommodeler) to come 1st in the kaggle [Benchmark Bond Trade Price Challenge](https://www.kaggle.com/c/benchmark-bond-trade-price-challenge) and the [Heritage Health Prize](https://www.kaggle.com/c/hhp/) competitions, and 4th place in the [Predicting a Biological Response](https://www.kaggle.com/c/bioresponse) competition.\n\n# How to use RGF:\nWe shall use the [`rgf_python`](https://github.com/RGF-team/rgf/tree/master/python-package) package, written by the [RGF-team](https://github.com/RGF-team), applied first to a simple classification example; the [Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic) competition data, and then to a regression example, using the [House Prices](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) competition data."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"!pip install rgf_python","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Classification example: Titanic"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas  as pd\nimport numpy   as np\n\n#===========================================================================\n# read in the data\n#===========================================================================\ntrain_data = pd.read_csv('../input/titanic/train.csv')\ntest_data  = pd.read_csv('../input/titanic/test.csv')\nsolution   = pd.read_csv('../input/submission-solution/submission_solution.csv')\n\n#===========================================================================\n# select some features\n#===========================================================================\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Embarked\"]\n\n#===========================================================================\n# for the features that are categorical we use pd.get_dummies:\n# \"Convert categorical variable into dummy/indicator variables.\"\n#===========================================================================\nX_train       = pd.get_dummies(train_data[features])\ny_train       = train_data[\"Survived\"]\nfinal_X_test  = pd.get_dummies(test_data[features])\n\n#===========================================================================\n# perform the classification \n#===========================================================================\nfrom rgf.sklearn import RGFClassifier\n\nclassifier = RGFClassifier(max_leaf=300, algorithm=\"RGF_Sib\", test_interval=100)\n\n#===========================================================================\n# and the fit \n#===========================================================================\nclassifier.fit(X_train, y_train)\n\n#===========================================================================\n# use the model to predict 'Survived' for the test data\n#===========================================================================\npredictions = classifier.predict(final_X_test)\n\n#===========================================================================\n# now calculate our score\n#===========================================================================\nfrom sklearn.metrics import accuracy_score\nprint(\"The score is %.5f\" % accuracy_score( solution['Survived'] , predictions ) )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Regression example: House Prices"},{"metadata":{"trusted":true},"cell_type":"code","source":"#===========================================================================\n# read in the competition data \n#===========================================================================\ntrain_data = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\ntest_data  = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\nsolution   = pd.read_csv('../input/house-prices-advanced-regression-solution-file/solution.csv')\n                         \n#===========================================================================\n# select some features\n#===========================================================================\nfeatures = ['MSSubClass', 'LotArea', 'OverallQual', 'OverallCond', \n        'YearBuilt', 'YearRemodAdd', 'BsmtFinSF1', 'BsmtFinSF2', \n        'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', \n        'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', \n        'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', \n        'TotRmsAbvGrd', 'Fireplaces', 'GarageCars', 'GarageArea', \n        'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', \n        'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold']\n\n#===========================================================================\n#===========================================================================\nX_train       = train_data[features]\ny_train       = train_data[\"SalePrice\"]\nfinal_X_test  = test_data[features]\ny_true        = solution[\"SalePrice\"]\n\n#===========================================================================\n# essential preprocessing: imputation; substitute any 'NaN' with mean value\n#===========================================================================\nX_train      = X_train.fillna(X_train.mean())\nfinal_X_test = final_X_test.fillna(final_X_test.mean())\n\n#===========================================================================\n# perform the regression\n#===========================================================================\nfrom rgf.sklearn import RGFRegressor\n\nregressor = RGFRegressor(max_leaf=300, algorithm=\"RGF_Sib\", test_interval=100, loss=\"LS\")\n\n#===========================================================================\n# and the fit \n#===========================================================================\nregressor.fit(X_train, y_train)\n\n#===========================================================================\n# use the model to predict the prices for the test data\n#===========================================================================\ny_pred = regressor.predict(final_X_test)\n\n#===========================================================================\n# compare your predictions with the 'solution' using the \n# root of the mean_squared_log_error\n#===========================================================================\nfrom sklearn.metrics import mean_squared_log_error\nRMSLE = np.sqrt( mean_squared_log_error(y_true, y_pred) )\nprint(\"The score is %.5f\" % RMSLE )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It almost goes without saying that to produce a medal winning score one does not only need a powerful estimator such as the RGF, but also perform data cleaning, judicious feature selection (perhaps using the new [Boruta-SHAP package](https://www.kaggle.com/carlmcbrideellis/feature-selection-using-borutashap)), if required then also perform  [feature engineering](http://www.feat.engineering/) as well as the necessary hyperparameter tuning and, just maybe, add a little *magic*.\n\n## RGF hyperparameters\nHere we shall mention two of the [RGF parameters](https://github.com/RGF-team/rgf/blob/master/RGF/rgf-guide.rst#432-parameters-to-control-training) that control training:\n\n`algorithm=`\n* `RGF`: RGF with $L_2$ regularization on leaf-only models. (default)\n* `RGF_Opt`: RGF with min-penalty regularization.\n* `RGF_Sib`: RGF with min-penalty regularization with the sum-to-zero sibling constraints.\n\n`loss=`\n* `LS`: square loss (default)\n* `Expo`: exponential loss\n* `Log`: logistic loss\n\n# References\n* [rgf_python](https://github.com/RGF-team/rgf/tree/master/python-package) on GitHub\n* [Regularized Greedy Forest in C++: User Guide](https://github.com/RGF-team/rgf/blob/master/RGF/rgf-guide.rst)\n* [FastRGF](https://github.com/RGF-team/rgf/tree/master/) A variant developed to be used with large (and sparse) datasets.\n* [Rie Johnson and Tong Zhang \"Learning Nonlinear Functions Using Regularized Greedy Forest\", IEEE Transactions on Pattern Analysis and Machine Intelligence, Volume: 36 , Issue: 5  pp. 942-954 (2014)](https://dx.doi.org/10.1109/TPAMI.2013.159) ([arXiv](https://arxiv.org/abs/1109.0887))\n\n# Related reading\n* [Regularized Greedy Forest – The Scottish Play (Act I)](https://www.statworx.com/at/blog/regularized-greedy-forest-the-scottish-play-act-i/) by Fabian Müller\n* [Regularized Greedy Forest – The Scottish Play (Act II)](https://www.statworx.com/de/blog/regularized-greedy-forest-the-scottish-play-act-ii/) by Fabian Müller\n* [An Introductory Guide to Regularized Greedy Forests (RGF) with a case study in Python](https://www.analyticsvidhya.com/blog/2018/02/introductory-guide-regularized-greedy-forests-rgf-python/) by Ankit Choudhary"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}