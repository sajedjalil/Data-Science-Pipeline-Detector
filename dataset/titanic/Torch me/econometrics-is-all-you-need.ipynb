{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# _**<span style=\"color:#1F618D;\"> üéìEconometrics is all you need üéì</span>**_\n\n<span style=\"color:#148F77;\">Author: Kartushov Danil</span> [@kartushovdanil](https://www.kaggle.com/kartushovdanil)   \n<span style=\"color:#148F77;\">Co-Author: Savchenko Stanislav</span> [@savchenkostanislav](https://www.kaggle.com/savchenkostanislav)    \n\n*Last Updated: 06/02/22*\n******\n\n<a id=\"table-of-contents\"></a>\n# **<span style=\"color:#1F618D;\">üìì Table of Content üìì</span>** \n- [1.üòº About author](#1)\n- [2.‚ùî What is econometrics](#2)\n- [3.üíπ Introduction to econometrics](#3)\n- [3.1. üñäÔ∏è Regression](#3.1.)\n    - [3.2. üß® Ordinary Least Squares](#3.2.)\n        - [3.2.1. üéà Precise Analytical Method](#3.2.1.)\n        - [3.2.2. üéâ Approximate Numerical Method](#3.2.2.)\n    - [3.3. üéä Anova](#3.3.)\n    - [3.4. üéÅ Testing Estimation, and Prediction](#3.4.)\n        - [3.4.1. üîÆ F-test](#3.4.1.)\n        - [3.4.2. üßø R2](#3.4.2.)\n        - [3.4.3. üïπÔ∏è Student's t-test](#3.4.3.)\n        - [3.4.4. üîë Why can't we blindly trust R-squared?](#3.4.4.)\n        - [3.4.5. üß∏ Residual Plot](#3.4.5.)\n        - [3.4.6. üñºÔ∏è Confidence Intervals](#3.4.6.)\n    - [3.5. üìª Multicorrelations](#3.5.)\n    - [3.6. ü™ï Regularization](#3.6.)\n    - [3.7. üìû Features Selection Methods](#3.7.)\n    - [3.8. üñ®Ô∏è Other-Regression Methods](#3.8.)\n        - [3.8.1. üíª K-nn Regression](#3.8.1.)\n        - [3.8.2. ‚å®Ô∏è Polynomial Regression](#3.8.2.)\n        - [3.8.3. üñ±Ô∏è Bayesian Regression](#3.8.3.)\n        - [3.8.4. üîå  Regression](#3.8.4.)\n    - [3.9. üé• Bias-Variance trade-off](#3.9.)\n    - [3.10. üìº PCA](#3.10.)\n- [4. üìà Classification](#4)\n\n******\n# <span style=\"color:#1F618D;\">üëÄ Preface of post</span>\nThe idea of creating this post came when Im started to study this topic. Econometrics is what is usual do data scientist, data analyst and etc in workout of business tasks. But while I am start to introduction machine learning, most things about data i was misunderstood. But while you learn econometrics you view to the part of a your knowledge about data from the other side. In this post I wanted to introduce you to econometrics and prove that this skill necessary if you start in data science. \n<a id=\"1.\"></a>\n# <span style=\"color:#1F618D;\">1. üòº About author</span>\nIts time to meet with me. Im alumni of Russian University of Economics, bachelor of applied mathematics and computer science. Typical for me look underneath the hood and see what's going on.  \nMy telegram channal. But there post on my native language https://t.me/notedatasciene.\n\n***By the way I can misleading you, share all information***  \n\n***If you like this notebook or find this notebook helpful, Please feel free to UPVOTE and/or leave a comment.***\n","metadata":{}},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"2.\"></a>\n# **<span style=\"color:#1F618D;\">2. ‚ùî What is econometrics</span>**\n\nEconometrics is called the art of developing forecasts of socio-economic phenomena. It's also about forecasting, about data, but it is some different to machine learning and data science.  \n\nA first difference is econometrics its about interpretable models, sort of insight and inference is a goal, but machine learning may dont care about this. Thats why we can talk about why CEO prefer people with econometrics knowledge, that can understand what's going with the research object and give interpretable or inference.   \nIm tried to draw what is place  econometrics did have in data science.  \n","metadata":{}},{"cell_type":"code","source":"from IPython.display import Image\nImage(\"../input/pictures/econ.png\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-06T17:25:19.807803Z","iopub.execute_input":"2022-02-06T17:25:19.808195Z","iopub.status.idle":"2022-02-06T17:25:19.845277Z","shell.execute_reply.started":"2022-02-06T17:25:19.808162Z","shell.execute_reply":"2022-02-06T17:25:19.844686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"How we propose a forecasting problem using other sciences such as classical statistics, probability theory and mathematical economics?  \nIn statistics, we are limited to methods such as descriptive statistics, histograms, means, variance which one cant give us appropriate results and inference.   \n_fact: Usual the most data science courses end education after statistics._","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom matplotlib.animation import FuncAnimation\nfrom matplotlib import animation\nfrom IPython import display\nfrom cycler import cycler\nimport seaborn as sns","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-02-06T17:25:19.84635Z","iopub.execute_input":"2022-02-06T17:25:19.846567Z","iopub.status.idle":"2022-02-06T17:25:20.98399Z","shell.execute_reply.started":"2022-02-06T17:25:19.846528Z","shell.execute_reply":"2022-02-06T17:25:20.983019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"3.\"></a>\n# **<span style=\"color:#1F618D;\">3. üíπ Introduction to econometrics</span>** \nLike machine learning, the difficulty of econometric modeling lies in the fact that most processes cannot be accurately estimated. We often have to choose one or the other. So let's stick to one golden rule:\n> ¬´Models should be as simple as possible, but not simpler¬ª F. Engels  \n\nI think these are the words of Einstein...\n\n**It is possible to single out the main classes of econometric models:**\n1. Regression Models\n2. Systems of simultaneous equations or regression models\n3. Single Equation Time Series Models\n4. Time series models with multiple equations. Vector autoregression.\n\n**Basic data types in econometric models:**\n1. Time Series\n2. Spatial or cross-sectional data\n3. Panel data","metadata":{}},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"3.1.\"></a>\n# **<span style=\"color:#1F618D;\">3.1. üñäÔ∏è Regression</span>**\nThe first part of this manual we will talk about regressions. Regressions - the simplest linear models, but also it is stronger tool in hand of data scientits. At first time you can be afraid of lot of formuls, but all materials is easy to understand if you will try look into logic of forluma. Let's look at simple regression formula to understand what is it and how its working.   \n$ y= w_{1}\\cdot x_{1}+...+w_{D}\\cdot x_{D}+w_{0} = \\langle x, w \\rangle + w_{0}$  \ny - target variable, $(x_1,...,x_{D})$ - features vector, $w_1,...,w_{D}, w_0$ - model parameters or weight vector. Also $w_0$ - bias or constant. Now we can find weight vector $(w_0,...,w_{D}) \\in \\mathbb{R}^{D+1}$, with methods of optimization ( later we will told about this ).","metadata":{}},{"cell_type":"code","source":"Image(\"../input/pictures/scat.png\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-06T17:25:20.985124Z","iopub.execute_input":"2022-02-06T17:25:20.985339Z","iopub.status.idle":"2022-02-06T17:25:20.999388Z","shell.execute_reply.started":"2022-02-06T17:25:20.985313Z","shell.execute_reply":"2022-02-06T17:25:20.998712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"General view of this regression model:\n$y = w_{0} + w_{1} \\cdot x_{1} + \\varepsilon$\n* $y - \\text{response variable}$\n* $w_{0} - \\text{constant}$\n* $w_{1} - \\text{slope and regression coefficient of } x_1 $\n* $x_1 - \\text{independent variable, regressor}$\n* $\\varepsilon - \\text{residual}$\n\nI gonna show you how work this coefficients\n\nLeets se whats going on while we changing $w_0$:\n$y = w_{0} + w_{1} \\cdot x_{1}$  \nif $w_{1} = 0$, $y = w_{0} + 0 \\cdot x_{1} = w_{0}$  \nif $y \\rightarrow n$ then $ w_{0} \\rightarrow n $  \nAs we can see $w_{0}$ is responsible for level of response variable","metadata":{}},{"cell_type":"code","source":"#%matplotlib notebook\n#%matplotlib widget\n\nplt.rcParams['figure.dpi'] = 150\nfig, ax = plt.subplots(1 ,figsize=(5, 3), facecolor='#f6f5f5')\nfig.subplots_adjust(hspace=0.2, wspace=0.2)\n\nline, = ax.plot([])\n\nax.set_xlim((0,10))\nax.set_ylim((0,30))\nax.set_title('Animation plot of changing regression constant', fontdict={'fontsize': 10, 'fontweight': 'bold'})\ndef animate(coef): \n    x = np.linspace(0, 10, 1000)\n    y=coef\n    line.set_data((x, y))\n    return line\n\nanim = FuncAnimation(fig, animate, frames=35, interval=60)\n#plt.plot(x, y)\n#plt.show()\n\nfor s in [\"top\",\"right\"]:\n    ax.spines[s].set_visible(False)\n    ax.set_facecolor('#f6f5f5')\n    ax.grid(which='major', axis='x', zorder=1, color='#EEEEEE', linewidth=0.4)\n    ax.xaxis.offsetText.set_fontsize(4)\n    ax.yaxis.offsetText.set_fontsize(4)\n    ax.set_ylabel('y')\n    ax.set_xlabel('x')\n    ax.tick_params(labelsize=8, width=1)\n    \nvideo = anim.to_html5_video()\nhtml = display.HTML(video)\ndisplay.display(html)\nplt.close() ","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-06T17:25:21.001117Z","iopub.execute_input":"2022-02-06T17:25:21.001651Z","iopub.status.idle":"2022-02-06T17:25:23.558047Z","shell.execute_reply.started":"2022-02-06T17:25:21.001619Z","shell.execute_reply":"2022-02-06T17:25:23.557019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Leets see whats going on while we changing $w_1$:\n\n$y = w_0 + w_1 \\cdot x_{1}$  \nif $w_0 = 0$ then $y= w_1 \\cdot x_{1} \\Rightarrow \\frac{y}{x_1} = w_1 $\n\nAnd we can see that  $a_1$ is slope of regressor and response variable","metadata":{}},{"cell_type":"code","source":"#%matplotlib notebook\n#%matplotlib widget\n\nfig, ax = plt.subplots(1 ,figsize=(5, 3), facecolor='#f6f5f5')\nfig.subplots_adjust(hspace=0.2, wspace=0.2)\n\nline, = ax.plot([])\n\nax.set_xlim((-10,10))\nax.set_ylim((-10,10))\n\n\ndef animate(coef): \n    x = np.linspace(-10, 10, 1000)\n    y=coef*x\n    line.set_data((x, y))\n    return line\n\nanim = FuncAnimation(fig, animate, frames=40, interval=80)\n#plt.plot(x, y)\n#plt.show()\nax.set_title('Animation plot of changing slope of x and y', fontdict={'fontsize': 10, 'fontweight': 'bold'})\nfor s in [\"top\",\"right\"]:\n    ax.spines[s].set_visible(False)\n    ax.set_facecolor('#f6f5f5')\n    ax.grid(which='major', axis='x', zorder=1, color='#EEEEEE', linewidth=0.4)\n    ax.xaxis.offsetText.set_fontsize(4)\n    ax.yaxis.offsetText.set_fontsize(4)\n    ax.set_ylabel('y')\n    ax.set_xlabel('x')\n    ax.tick_params(labelsize=8, width=1)\n\nvideo = anim.to_html5_video()\nhtml = display.HTML(video)\ndisplay.display(html)\nplt.close() ","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-06T17:25:23.559983Z","iopub.execute_input":"2022-02-06T17:25:23.560316Z","iopub.status.idle":"2022-02-06T17:25:26.197846Z","shell.execute_reply.started":"2022-02-06T17:25:23.560277Z","shell.execute_reply":"2022-02-06T17:25:26.196671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"3.2.\"></a>\n‚Äã\n# **<span style=\"color:#1F618D;\">3.2. üß® Ordinary Least Squares</span>**\nThe most popular method optimization linear regression - OLS. First of all it should be said that optimization regression split of two methods:  __Precise Analytical Method__ and __Approximate Numerical Method__. And in this topic we will talk more about first type. But now let's figure it out what is OLS and why you need to know this. As we know that regression finction look like $ f_{w}(x_i) = \\langle x, w \\rangle + w_{0}$ and you can check out that im told about w_0 is constant, but why? \n\nOften we can ignore $w_0$, because we can achieve the same result if make new feature $x_i$ equal 1. Then new created feature will be have weight equal $w_0$.\n\n$$ \\begin{pmatrix}x_{i1},\\dotsc,x_{iD}\\end{pmatrix} \\cdot \\begin{pmatrix} w_{1}\\\\\\vdots\\\\x_{D}\\\\\\end{pmatrix} + w_{0} = \\begin{pmatrix} 1, x_{i1},\\dotsc,x_{iD}\\end{pmatrix} \\cdot \\begin{pmatrix} w_{0}\\\\ w_{1}\\\\\\vdots\\\\x_{D}\\\\\\end{pmatrix}$$\n\nWe want that our regression model as better as possible approximate our dependent.","metadata":{}},{"cell_type":"code","source":"Image(\"../input/pictures/scat2.png\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-06T17:25:26.200115Z","iopub.execute_input":"2022-02-06T17:25:26.200451Z","iopub.status.idle":"2022-02-06T17:25:26.216308Z","shell.execute_reply.started":"2022-02-06T17:25:26.200408Z","shell.execute_reply":"2022-02-06T17:25:26.215549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So we need to measure how our model is badness and minimize errors. Traditionally function badness named loss function. Important that loss function easy to optimization, smooth function - it is good, partially constant - it is just awful.\n\nLoss functions is different, in this method we will use $L^2$-norm. This loss function use in 90% situations. $L^2$ is euclidean distance $\\Vert y - f_{w}(x) \\Vert_{2}$, so we approximate by simple understanding of distance. So as usual we denote loss function as $L(f, X, y)$, and our loss function look like $L(f, X, y) = \\Vert y - f_{w}(x) \\Vert_{2}^2 = \\Vert y - Xw \\Vert_{2}^2 = \\sum{(y_i - \\langle x_i, w \\rangle)^2}$\n\nBasic loss functions usually have names. Somethimes instead of sum squared distance between points and regression line we used mean distance. This loss function named __MSE__, __Mean Squared Error__ there is no difference other than cosmetic.\n\nSo right now our task is how we can minizize loss function by w?\n\n$$ \\Vert y - Xw \\Vert_{2}^2  \\rightarrow \\min_{w}$$ \n\nOn this questions exist two answer, so lets look to both!","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3.2.1.\"></a>\n‚Äã\n# **<span style=\"color:#1F618D;\">3.2.1. üéà Precise Analytical Method </span>**\n\nTo find the minimum point, we need equate gradient of vectors $w$ to zero in this point and get this formula of optimal weight vector.\n$$w_{*} = (X^{T}X)^{-1}X^{T}y$$\n\nLet's prove this formula. At first we need to find gradient of loss function. For this we will write squared of modulus in inner(scalar) product.\n$$ \\Vert y - Xw \\Vert^2  = \\langle y - Xw, y-Xw \\rangle$$\nLet's apply the product differential formula and take advantage of the symmetry of the inner(scalar) product.","metadata":{}},{"cell_type":"code","source":"Image(\"../input/pictures/formula.jpg\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-06T17:25:26.217782Z","iopub.execute_input":"2022-02-06T17:25:26.218287Z","iopub.status.idle":"2022-02-06T17:25:26.229581Z","shell.execute_reply.started":"2022-02-06T17:25:26.218252Z","shell.execute_reply":"2022-02-06T17:25:26.228655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So we can equate this to zero and get final formula.\n\n$$w_{*} = (X^{T}X)^{-1}X^{T}y$$\n\nBut if we calculate $w_*$, we have to reverse the (square) matrix $X^T X$, which is possible only if it is non-degenerate. What does this mean in terms of data analysis? Why do we believe that this is done in all reasonable situations?\n\nThis first time when we facing with multicorrelation problem. We will talk about problem this later. So if your features are multicorrlation, our matrix $X^T X$ are broken, two or more linearly dependent features dropped and have broked this matrix. But also our features can be approximately linear depended and this problem leads us to various difficulties.\n\n__Result__\n* Computationally hard.\n* The calculation depends on the $X^T X$ matrix. It is often distorted by multicollinearity.\n* This doesn't mean it's a bad method, if you know how to get a pseudo-imagined matrix from SVD or apply QR decomposition it will solve the problem.\n* Used in 90% of cases.","metadata":{}},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"3.2.2.\"></a>\n‚Ä¢\n# **<span style=\"color:#1F618D;\">3.2.2. üéâ Approximate Numerical Method </span>**\n‚Äã\nOur loss function is smooth and convex, which means that we can find the minimum by gradient methods. Gradient direction to side of fastest growth. So if we gonna find minimum we need to take anti gradient with step of previous point. Look at the formula.\n\n$$ w_j \\mapsto w_j - \\alpha \\frac{d}{dw_j}L(f_w,X,y) $$\n\n$\\alpha$ - themp of learning, which responsible for step size.\n\n<img src=\"https://raw.githubusercontent.com/Shathra/gradient-descent-demonstration/master/gradient_descent_local_minima.gif\">\n<img src=\"https://raw.githubusercontent.com/Shathra/gradient-descent-demonstration/master/gradient_descent_1d.gif\">\n<img src=\"https://raw.githubusercontent.com/Shathra/gradient-descent-demonstration/master/gradient_descent_2d.gif\">","metadata":{}},{"cell_type":"markdown","source":"\n[back to top](#table-of-contents)\n<a id=\"3.3.\"></a>\n‚Ä¢\n# **<span style=\"color:#1F618D;\">3.3. üéä Anova </span>**\n\nA new technique called the analysis of variance is used to determine how the different experimental factors affect the average response. As we know regression - average line of features. So ANOVA for regression is way to estimate importance of weight of regression and model in general.\n\n$TSS = \\sum{y_i^2 - \\frac{(\\sum{y_i})^2}{n}}$\n* $SSR$ (sum of squares for regression) measures the amount of variation explained by\nusing the regression equation.\n* $SSE$ (sum of squares for error) measures the residual variation in the data that is not\nexplained by the independent variables.\n\nSo that $TSS = SSR+SSE$  \n\nThe degrees of freedom for these sums of squares are found using the following argu-\nment. There are (n-1) total degrees of freedom. Estimating the regression line requires estimating k unknown coefficients‚Äî $ w_1, w_2,...,w_k $ the constant $w_0$ (which estimates $w_0$) is a function of y and the other estimates. Hence, there are k regression degrees of freedom, leaving ( n-1 ) -k degrees of freedom for error. The mean squares are calculated as MS=SS/df\n\nAlso you can made ANOVA table.   \n\n","metadata":{}},{"cell_type":"code","source":"Image(\"../input/pictures/anova1.png\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-06T17:25:26.477215Z","iopub.execute_input":"2022-02-06T17:25:26.477551Z","iopub.status.idle":"2022-02-06T17:25:26.488373Z","shell.execute_reply.started":"2022-02-06T17:25:26.477513Z","shell.execute_reply":"2022-02-06T17:25:26.487834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"3.4.\"></a>\n\n# **<span style=\"color:#1F618D;\">3.4. üéÅ Testing Estimation, and Prediction </span>**\n\n<a id=\"3.4.1.\"></a>\n\n# **<span style=\"color:#1F618D;\">3.4.1. üîÆ F-test </span>**\nIs the regression equation that uses information provided by the predictor variables\n$x_1,x_2,...,x_k$ substantially better than the simple predictor $\\bar{y}$ that does not rely on any of the\nx-values? This question is answered using an overall F-test with the hypotheses:\n\n$H_0: w_1 = w_0 = ... = w_k = 0$  \nversus  \n$H_1:$ At least one of $w_1, w_2, .., w_k$ is not 0\n\nThe test statistic is found in the ANOVA table.\n\n$$F = \\frac{MSR}{MSE} $$\n\nwhich has an F distribution with $df_1 = k$ and $df_2 = n-k-1$. Since the exact\n\nP-Value = .000, is given in the printout, you can declare the regression to be highly signifi-\ncant. That is, at least one of the predictor variables is contributing significant information for the prediction of the response variable y.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3.4.2.\"></a>  \n# **<span style=\"color:#1F618D;\">3.4.2. üßø R2 </span>**  \n\nHow well does the regression model fit? The regression printout provides a statistical mea-\nsure of the strength of the model in the coefficient of determination, R2‚Äîthe proportion of the total variation that is explained by the regression of y on $x_1, x_2, ..., x_k$‚Äîdefined as\n$$R^2 = \\frac{SSR}{\\text{Total SS}}$$\nThe coefficient of determination is sometimes called multiple R2. Hence, for the real estate example, 97.1% of the total variation has been\nexplained by the regression model. The model fits very well!\n\nIt may be helpful to know that the value of the F statistic is related to R2 by the\nformula\n$$ F = \\frac{R^2 /k}{(1-R^2)(n-k-1)}$$\n\nso that when R2 is large, F is large, and vice versa.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3.4.3.\"></a>  \n# **<span style=\"color:#1F618D;\">3.4.3. üïπÔ∏è Student's t-test </span>**  \nOnce you have determined that the model is useful for predicting y, you should explore\n\nthe nature of the ‚Äúusefulness‚Äù in more detail. Do all of the predictor variables add impor-\ntant information for prediction in the presence of other predictors already in the model?\n\nThe individual t-tests in the first section of the regression printout are designed to test the\nhypotheses\n\n$$H_0: w_i = 0 \\text{ versus } H_1: w_i \\neq 0$$\n\nfor each of the partial regression coefficients, given that the other predictor variables are\nalready in the model. These tests are based on the Student‚Äôs t statistic given by\n\n$y = w_0 + w_1 + ... + w_k + e$  \n$t_{w_i} = \\frac{w_i}{S_{w_i}}$","metadata":{}},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"3.4.4.\"></a>\n\n# **<span style=\"color:#1F618D;\">3.4.4. üîë Why can't we blindly trust R-squared?</span>**\n\nR-square increases when one more regressor is added (this addition may not always be economically justified), in addition, R-squared changes even with the slightest transformation of the dependent variable\n\nInexperienced researchers may have an idea in their heads that it is possible to introduce so many variables into the model that P-2 becomes equal to 1, but this also raises a big problem - these variables may not be justified from an economic point of view.\n\nBut we are smart people :)\nIn order to somehow \"penalize\" the researcher for using a large number of regressors, we introduce the so-called adjusted coefficient\n\n $ R_{adj}^2 = 1 - (1 - R^2)\\frac{n-1}{n-k} $, where k - number of regressors. If k = 1, if k = 1, then $R_{adj}^2 = R^2 $","metadata":{}},{"cell_type":"markdown","source":"* [back to top](#table-of-contents)\n<a id=\"3.4.5.\"></a>\n\n# **<span style=\"color:#1F618D;\">3.4.5. üß∏ Residual plot</span>**\n\nBefore using the regression model for its main purpose‚Äîestimation and prediction of y‚Äî\nyou should look at computer-generated residual plots to make sure that all the regression\nassumptions are valid. The normal probability plot and the plot of residuals versus fit are\nshown for the real estate data. There appear to be three observations that do\n\nnot fit the general pattern. You can see them as outliers in both graphs. These three observa-\ntions should probably be investigated; however, they do not provide strong evidence that\n\nthe assumptions are violated.\n\n","metadata":{}},{"cell_type":"code","source":"Image(\"../input/pictures/anova2.png\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-06T17:25:26.489661Z","iopub.execute_input":"2022-02-06T17:25:26.490049Z","iopub.status.idle":"2022-02-06T17:25:26.501491Z","shell.execute_reply.started":"2022-02-06T17:25:26.490021Z","shell.execute_reply":"2022-02-06T17:25:26.50094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"3.4.6.\"></a>\n‚Äã\n# **<span style=\"color:#1F618D;\">3.4.6. üñºÔ∏è Confidence interval </span>**\n### __In process__","metadata":{}},{"cell_type":"markdown","source":"[back to top](#table-of-contents)\n<a id=\"3.5.\"></a>\n‚Äã\n# **<span style=\"color:#1F618D;\">3.5. üìª Multicollinearity </span>**\n\nIf $det(X^TX) = 0$ then $w^{*} = (X^{T}X)^{-1}X^{T}Y$ is not defined. This is called **strict multicollinearity**. This\nhappens when the columns of X are linearly dependent. Most commonly, this arises when sets of regressors are included which are identically\nrelated. But this situation almost never occurs in practice. \n\nA more difficult issue is **near-multicollinearity**, which is often called **multicollinearity** In this case, we can estimates\n $w^{*}$ and these estimators will still be unbiased and efficient, but the estimators variances will be large, and, consequently, the estimates regression parameters will be insignificant (t-test) \n \n The matrix *R* =  $\\begin{pmatrix}\n    1 & r_{x_1,x_2} & \\dots & r_{x_1,x_m}\\\\\n    r_{x_2,x_1} & 1 & \\dots & r_{x_2,x_m}\\\\\n    \\vdots & \\vdots & \\dots & \\vdots \\\\\n    r_{x_m,x_1} & r_{x_m,x_2} & \\dots & 1\n  \\end{pmatrix} $  $   $ is called the matrix of pair correlation coefficients\n \n**How to recognize?**\n* Estimates of regression parameters are unstable, that is, removing or adding a small number of observations leads to a significant change in the parameters\n* In general, the constructed regression is adequate, $R^2$ is often very high, but at the same time the coefficients for many factors are insignificant (t-test)\n* If $det(R) \\approx 0$ \n* If $\\chi^2_{test} = (1-n+\\frac{(2m + 5)}{6})ln(det(R)) > \\chi^2(\\alpha, \\frac{1}{2}m(m-1)$, m - number of regressors, n - number of observations\n* Variation Inflation Factor (VIF)\n\n**VIF**\n\nLet $ y= w_{0} + w_{1}\\cdot x_{1}+...+w_{m}\\cdot x_{m} = w_{0} +  \\displaystyle\\sum_{i=1}^{m} w_{i}x_{i} $. \n1. Estimate the regressions of each independent variable on all other independent variables: $x_{j} = \\alpha_{0} + \\displaystyle\\sum_{\\substack{\n   i = 1 \\\\\n   i \\neq j\n  }} ^{m}\\alpha_{i}x_{i}  $\n2. For each new regression, we calculate the coefficient of determination $R^2_j$\n3. Calculate m values $VIF_{x_j} = \\frac{1}{1-R^2_j}$\n4. All variables $x_j$ for which $VIF_{x_j} > 10$ are recognized as **multicollinear**.","metadata":{}}]}