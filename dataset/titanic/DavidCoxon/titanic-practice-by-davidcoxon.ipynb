{"cells":[{"metadata":{"_uuid":"721884fd77000b8b4e69c2af9958f4412afd2cff","_cell_guid":"0f2e1b48-de36-4da8-9df9-a2909a92b461"},"cell_type":"markdown","source":"# Predicting Titanic Survivers\nLike Titanic, this is my maiden voyage,  when it comes to Kaggle contest that is!. I've completed the Data Science track on Data Camp, but I'm a relative newbie when it comes to machine learning. I'm going to attempt to work my way through the Titanic: Machine Learning contest. My aim is to submission and initial entry as quickly as possible to get a base line score and then attempt to improve on  on it by first looking at missing data, then engineering key features before establishing a  secondary base line and trying to improve the model itself. I'd like to be able to achieve a score of .80\n\nPlease feel free to post comments or  make suggestions as to what i may be doing wrong or could maybe do better and  consider upvoting if you find the notebook useful!\n\nBecause this notebook has built up over time I have commented out some of the lines that output files so that when i want to output and test a slight change to the code, i don't output files for bit of the notebook that haven't been changed and that i am not especially intereted in. If you are forking this code you can simple remove the hash and output the file. I have also experimented with different models, so the current model in any stage is not necessarily the most efficent (its just the one that i tried last)."},{"metadata":{"_uuid":"13a824268233d8a6a7002be362847e2446aa2da6","_cell_guid":"bb27af35-206d-4da2-8f33-63a6b6671c72"},"cell_type":"markdown","source":"# Import the Libraries and Data"},{"metadata":{"_kg_hide-input":false,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":false,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nfrom sklearn.cross_validation import KFold\nfrom sklearn.ensemble import (AdaBoostClassifier,BaggingClassifier,ExtraTreesClassifier,GradientBoostingClassifier,RandomForestClassifier,VotingClassifier)\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression, Perceptron, SGDClassifier, LogisticRegression, PassiveAggressiveClassifier,RidgeClassifierCV\nfrom sklearn.metrics import accuracy_score,auc,classification_report,confusion_matrix,mean_squared_error, precision_score, recall_score,roc_curve\nfrom sklearn.model_selection import cross_val_score,cross_val_predict,cross_validate,train_test_split,GridSearchCV,KFold,learning_curve,RandomizedSearchCV,StratifiedKFold\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn import ensemble, linear_model,neighbors, svm, tree\n\nfrom scipy.stats import randint\nfrom xgboost import XGBClassifier\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\n\ndf_train=pd.read_csv('../input/train.csv',sep=',')\ndf_test=pd.read_csv('../input/test.csv',sep=',')\ndf_data = df_train.append(df_test) # The entire data: train + test.\n\nPassengerId = df_test['PassengerId']\nSubmission=pd.DataFrame()\nSubmission['PassengerId'] = df_test['PassengerId']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1ce9a1b19b5440668849af3010cefd2c84f1b4f6","_cell_guid":"eb5476cd-91d9-483c-a3b7-1590073da4d8"},"cell_type":"markdown","source":"# Stage 1 : Explore the Data and create a basic model on raw data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true},"cell_type":"markdown","source":"# Explore the data Statistically"},{"metadata":{"_uuid":"bb5839d5b2d1036f98090fb2ec950f03304caec9","_cell_guid":"f191bda6-f63b-420a-8ae0-f2e22607b425"},"cell_type":"markdown","source":"### Number of rows and columns "},{"metadata":{"_uuid":"47cafc06d2881a01e80469d40ed38cbea126652c","_cell_guid":"073e7a38-5e62-427e-ba8d-72a7a5334442","trusted":false,"collapsed":true},"cell_type":"code","source":"# How big are the training and test datasets\nprint(df_train.shape)\nprint(\"----------------------------\")\nprint(df_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7e85c3850f8dd12a0e782456fb1c1986fbaf84b1","_cell_guid":"7bab27ce-09d8-43aa-8842-ee000e2d1e8d"},"cell_type":"markdown","source":"### Column Names"},{"metadata":{"_uuid":"ec6fdc9350bc1e3ce83114c370008528e8109afa","_cell_guid":"4be833c5-e99c-4519-9b29-4d27699cfe12","trusted":false,"collapsed":true},"cell_type":"code","source":"# What are the column names \ndf_train.columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1c3996b06fe7470613faf06c5f0341103ec349e8","_cell_guid":"a340d978-40e3-407d-aadd-78a10c765178"},"cell_type":"markdown","source":"### Data Types"},{"metadata":{"_uuid":"4534fe968fece75089f136f647d5ee9f1408b21e","_cell_guid":"1e27d5dd-1d67-438f-87c3-ea4577f297cd","trusted":false,"collapsed":true},"cell_type":"code","source":"# What type of data object are in each column and how many missing values are there\ndf_data.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f6b8fbba4bb96b7e3aeaa562176e60051846abc1","_cell_guid":"41a8e34c-ee85-4cdb-86d6-5f01e964c409"},"cell_type":"markdown","source":"### Missing Data\n\nHow much Data is missing from the training and test datasets, how important is that data and how much data cleaning might be required."},{"metadata":{"_uuid":"77c81d38447b990a95f21afe3fae8bb5f6d00b4e","scrolled":true,"_cell_guid":"f9bee28e-9b27-4f2b-8bf5-8e7a2b09ebbf","trusted":false,"collapsed":true},"cell_type":"code","source":"#check for any other unusable values\nprint(pd.isnull(df_data).sum())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5bf733a2fc59ab5fd469aa07d107c06ffddc50a2","_cell_guid":"11c1e7bb-6f12-48ca-8807-add46fb447f4"},"cell_type":"markdown","source":"## Observations on missing data.\n\nThere are 144 missing ages in the training data and 86 mssing ages in the test data. Age is an important feature so it is worth spending time to address this properly. \n\nThere are 468 missing Cabin entries in the training data and 326 in the test data, at this stage I'm not sure how important this feature is so I'm going to revisit this when I know more about the feature.\nThere are 2 missing embarked data points in the train data and 1 missing fare in the test data, at this stage this does not represent a problem."},{"metadata":{"_uuid":"9d051a3476f6271b33a955216e947b7aea3de0f3","_cell_guid":"8807eb08-dd68-45b9-ab5e-d810a98a8cbe"},"cell_type":"markdown","source":"## Statistical Overview of the data"},{"metadata":{"_uuid":"10dbf2b79a79bbdcb283fb8237cf9a7eb94bccb2","_cell_guid":"817d81d9-6912-4771-b654-b3d5af85ed46","trusted":false,"collapsed":true},"cell_type":"code","source":"# Get a statistical overview of the training data\ndf_train.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0e37aa2ae972acdbaeb6366bb55c838eef8bc76e","_cell_guid":"e1a5061c-c823-4768-9fde-1eec950c9a75","trusted":false,"collapsed":true},"cell_type":"code","source":"# Get a statistical overview of the data\ndf_test.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7a99c489c6f184666d3e5586db6de3a441c1d0c6","_cell_guid":"53ce0b2e-724d-4530-87e4-9812597c1583"},"cell_type":"markdown","source":"Note: The mean and Std of each of the columns in the 2 datasets are reasonable close together, so its safe to assume that any relationships we discover in the  training data should work similarly in the test data."},{"metadata":{"_uuid":"ec577a60cb08406719037e966a974f29573b57fb","_cell_guid":"37d3c26f-7b3e-411d-b939-ad874ba31eea","trusted":false,"collapsed":true},"cell_type":"code","source":"# Take a look at some sample data\ndf_train.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"30ca0953dbf13567cfccfd96bacf7e76f4474984","_cell_guid":"9dd2bf45-bda2-4dab-bc69-8a8e46c1b72b","trusted":false,"collapsed":true},"cell_type":"code","source":"df_train.tail(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"76fbda700a5f261b31759ade3e8c2c93d7458c01","_cell_guid":"2275818c-6652-4070-be61-b17a7e4b0b5e"},"cell_type":"markdown","source":"# Explore Data Graphically"},{"metadata":{"_uuid":"45b16cde292ad9c45d0143e99a2be8f11ed9608b","_cell_guid":"76f9e3e9-db43-4cc4-a4cd-5f54003fe573"},"cell_type":"markdown","source":"## Survival by Age, Class and Gender"},{"metadata":{"_uuid":"88e91d960cd9f7ec2fa3a635d93e69e54c4e338a","_cell_guid":"158f3ef3-80fd-464d-a837-6b916926bbb2","trusted":false,"collapsed":true},"cell_type":"code","source":"grid = sns.FacetGrid(df_train, col = \"Pclass\", row = \"Sex\", hue = \"Survived\", palette = 'seismic')\ngrid = grid.map(plt.scatter, \"PassengerId\", \"Age\")\ngrid.add_legend()\ngrid","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3d94018b485b98ae9744f17e6ead6f604061cbb6","_cell_guid":"3cf55d17-8577-47cc-b81e-8c8fc89d60dc"},"cell_type":"markdown","source":"## Survival by Age, Port of Embarkation and Gender"},{"metadata":{"_uuid":"706c28dd41dc6ace4ba207bf027f59181cd14f05","_cell_guid":"e5f3871e-6638-426d-b0d9-5ed92d0ff1b5","trusted":false,"collapsed":true},"cell_type":"code","source":"grid = sns.FacetGrid(df_train, col = \"Embarked\", row = \"Sex\", hue = \"Survived\", palette = 'seismic')\ngrid = grid.map(plt.scatter, \"PassengerId\", \"Age\")\ngrid.add_legend()\ngrid","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1687792f3f9915fe72cd2766681000ae74554361","_cell_guid":"be3ae2ae-f77b-4ae4-b8f2-09c5e6138907"},"cell_type":"markdown","source":"This embarkation visualization indicates that a large proportion of passengers embarked at port 'S', with lesser numbers at 'C' and 'Q' it also shows that regardless of embarkation port more women survived than men. It doesn't seem to show any corelation between passenger ID and Embarkation port. Interestingly Embarkation port Q seems to indicate that  only 1 man survived while all women with passenger ID below 500 seem to survive while those above didn't this may be chance but it does look odd compared to 'S' and 'C'."},{"metadata":{"_uuid":"c963f65c9a8f91b04102ce3e4d63ed1062f7d7ee","_cell_guid":"0d5d257f-0b2c-4f43-9090-c74053973f07"},"cell_type":"markdown","source":"## Survival by Age, Number of Siblings and Gender"},{"metadata":{"_uuid":"b365e7b30eb770a2d1e77e4a95d9b4c1ced60c6b","_cell_guid":"82442bd9-cdfc-4f9b-9ebc-9f69238159aa","trusted":false,"collapsed":true},"cell_type":"code","source":"grid = sns.FacetGrid(df_train, col = \"SibSp\", row = \"Sex\", hue = \"Survived\", palette = 'seismic')\ngrid = grid.map(plt.scatter, \"PassengerId\", \"Age\")\ngrid.add_legend()\ngrid","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e7bcda55320028f7e6f8da737e88bba582f560c8","_cell_guid":"10118d5d-4913-4804-93f8-f2d19954c8cd"},"cell_type":"markdown","source":"## Survival by Age, Number of parch and Gender"},{"metadata":{"_uuid":"8c5c3db479f52e13aa80b6025c351eda045e97fb","_cell_guid":"e7e9b15d-5ace-4571-8462-eecb93f7d6ec","trusted":false,"collapsed":true},"cell_type":"code","source":"grid = sns.FacetGrid(df_train, col = \"Parch\", row = \"Sex\", hue = \"Survived\", palette = 'seismic')\ngrid = grid.map(plt.scatter, \"PassengerId\", \"Age\")\ngrid.add_legend()\ngrid","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fb00c6099b7f836f3610e911075f78af6ca724f2","_cell_guid":"f001f636-fc2a-4881-a81e-53edabf53242"},"cell_type":"markdown","source":"# Pairplots\n\nTo get a very basic idea of the relationships between the different features we can use pairplots from seaborn."},{"metadata":{"_uuid":"b81a66434766fce1abfc6ad160ec6a62bee97887","scrolled":false,"_cell_guid":"60f3f81f-4401-4659-b457-4200cabdf2b5","trusted":false,"collapsed":true},"cell_type":"code","source":"g = sns.pairplot(df_train[[u'Survived', u'Pclass', u'Sex', u'Age', u'Parch', u'Fare', u'Embarked']], hue='Survived', palette = 'seismic',size=4,diag_kind = 'kde',diag_kws=dict(shade=True),plot_kws=dict(s=50) )\ng.set(xticklabels=[])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ab4504d0c13948980ed23f80d155fd7d31cd4301","_cell_guid":"1de34739-550f-4384-a3e0-0ea807bb297e"},"cell_type":"markdown","source":"# Create simple model\n\nCreate a baseline score by using old the standard numeric data on on a very basic model, this will be used to see how much any changes we make to the data or model improve performance."},{"metadata":{"_uuid":"ca99023e73866ccccf2b58dadd69242defaea90c","_cell_guid":"dd1b861c-f4d7-4d2f-a131-9b6f59b48624","trusted":false,"collapsed":true},"cell_type":"code","source":"NUMERIC_COLUMNS=['Pclass','Age','SibSp','Parch','Fare']\n\n# create test and training data\ntest = df_test[NUMERIC_COLUMNS].fillna(-1000)\ndata_to_train = df_train[NUMERIC_COLUMNS].fillna(-1000)\ny=df_train['Survived']\nX_train, X_test, y_train, y_test = train_test_split(data_to_train, y, test_size=0.3,random_state=21, stratify=y)\n\nclf = SVC()\nclf.fit(X_train, y_train)\n\n# Print the accuracy\nprint(\"Accuracy: {}\".format(clf.score(X_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a9a81d694ac9b10f1558318ca64ec38d689411e0","_cell_guid":"23cb1c05-31ae-4676-934e-62c0e3f53821"},"cell_type":"markdown","source":"# Create initial predictionsÂ¶"},{"metadata":{"_uuid":"50e1643054e16b31c2234ba18df0176ae4c44696","_cell_guid":"edb5f39c-2c88-4c42-9bde-f6375503cef3","trusted":false,"collapsed":true},"cell_type":"code","source":"Submission['Survived']=clf.predict(test)\nprint(Submission.head())\nprint('predictions generated')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"df612c7f8c676da096cf50c3640b882a9443d330","_cell_guid":"262a3df0-1a62-40bb-be9c-48bbc2787847"},"cell_type":"markdown","source":"# Make first Submission"},{"metadata":{"_uuid":"5b2bf28e6ab4b43d8368df9606f2d3d18d1bd4ab","_kg_hide-output":true,"_cell_guid":"3ab17c71-9324-48ee-815f-e3a92f9cd074","trusted":false,"collapsed":true},"cell_type":"code","source":"# write data frame to csv file\n#Submission.set_index('PassengerId', inplace=True)\n#Submission.to_csv('myfirstsubmission.csv',sep=',')\nprint('file created')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b50c6f50f7d672349871e631132d9f941ba1a02f","_cell_guid":"f66f0981-d9dd-4c9f-ac7a-f63fddb16dd4"},"cell_type":"markdown","source":"The result of this first submission was a score of 0.57894. This constitutes performing just above random, if i'd simply flipped a coin fair coin for each passenger i could have achieved this kind of score. So there is plenty of room for improvement."},{"metadata":{"_uuid":"c4c7faeb87fbe3324c324a01a0dc9bac3e31829d","_cell_guid":"3a0ea6c3-2148-408e-b1eb-12b9b4f5cf6b"},"cell_type":"markdown","source":"# Stage 2 : Clean Data & Engineer features to improve results\n\n \n\n## Cleaning the data : Filling in the blanks\nThere are a number of missing values, including fare, embarked, age and cabin. I started off simply using the average value for fare, embarked and age. However after doing some visual data analysis it became obvious that I could use other factors like title to make better estimates on age by simply using the average for people with the same title, the same applied to embarked where average based on fare would give a better estimate and fare based on embarked. \n\nCabin has so much missing data that it is likely that estimating cabin may add a level of noise to the data that would not be helpful.\n\n## Feature conversion\nSome models work better with with categorical data other numberical data,  while some work best with binaryl data. In some cases this is as simple as changing male and female to numeric data like 0 or 1. We can replace categorical data like embarkation port  's' to values numeric value 1 or title Master to value 3 Values like age that range from 1 to 80 can be scaled so they a represented by a value between 0 and 1. Scaling values means that features are not given a disproportionate importance simply because they are larger, another option for values like Age or Fare are to split them into a more manageable bands which can then be represented as categories so.  Alternately we could put each categorical value into a column of its own, marking each columns with a 0 if they don't apply or a 1 if they do. After doing some initial data eploration i decided it was easiest to convert data into bands and columns, so that I could then compare the models with different options and decide which was best for each before making final predictions.\n\n## Feature Engineering\nHere I attempted to manipulate existing data in order to  try and create new features that i could use in my model, for example family size can be caluculated with the combination of siblings and parents, and title can be extracted from name. "},{"metadata":{"_uuid":"fad3c30cc68e471b84be592166cc2a661c4a357f","_cell_guid":"8cf4c617-baf1-4172-85e0-7ae1754e23a8"},"cell_type":"markdown","source":"## Estimate missing Fare Data based on Embarkation\nWhile there is relatively little missing Fare data, the range of possible values is large, so rather than using simply the media of all fares, we can look at the passenger class or embarkation port in order to use a more appropriate average. We'll start by looking at boxplots for the fares to ensure we are making soon assumptions before we go onto estimating the missing values."},{"metadata":{"_uuid":"7bec995c8fc6a1bdc6634d9eaeafd5f7acd617c4","_cell_guid":"dc5090b5-1bb7-4ede-805e-62d9d4901867","trusted":false,"collapsed":true},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True,figsize=(12,6))\nsns.boxplot(data = df_data, x = \"Pclass\", y = \"Fare\",ax=ax1);\nplt.figure(1)\nsns.boxplot(data = df_data, x = \"Embarked\", y = \"Fare\",ax=ax2);\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"af9325ff33b555d3f2d69495baec14ad59dd19a0","_cell_guid":"2a05e7be-844e-4c77-96a2-bfb146afc694","trusted":false,"collapsed":true},"cell_type":"code","source":"# Fill the na values in Fare based on embarked data\nembarked = ['S', 'C', 'Q']\nfor port in embarked:\n    fare_to_impute = df_data.groupby('Embarked')['Fare'].median()[embarked.index(port)]\n    df_data.loc[(df_data['Fare'].isnull()) & (df_data['Embarked'] == port), 'Fare'] = fare_to_impute\n# Fare in df_train and df_test:\ndf_train[\"Fare\"] = df_data['Fare'][:891]\ndf_test[\"Fare\"] = df_data['Fare'][891:]\nprint('Missing Fares Estimated')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"085e8bc212ff7478117381b570b32a048d69927b","_cell_guid":"58556287-b706-46e8-9e8f-e9289cb0f62d"},"cell_type":"markdown","source":"## FareBand feature"},{"metadata":{"_uuid":"97b3eec73c454f13d81e0ae1c641fcb2e29f2318","scrolled":true,"_cell_guid":"f7b6005b-a173-421c-ac94-ac2f14c9c183","trusted":false,"collapsed":true},"cell_type":"code","source":"#fill in missing Fare value in training set based on mean fare for that Pclass \nfor x in range(len(df_train[\"Fare\"])):\n    if pd.isnull(df_train[\"Fare\"][x]):\n        pclass = df_train[\"Pclass\"][x] #Pclass = 3\n        df_train[\"Fare\"][x] = round(df_train[df_train[\"Pclass\"] == pclass][\"Fare\"].mean(), 8)\n        \n#fill in missing Fare value in test set based on mean fare for that Pclass         \nfor x in range(len(df_test[\"Fare\"])):\n    if pd.isnull(df_test[\"Fare\"][x]):\n        pclass = df_test[\"Pclass\"][x] #Pclass = 3\n        df_test[\"Fare\"][x] = round(df_test[df_test[\"Pclass\"] == pclass][\"Fare\"].mean(), 8)\n        \n#map Fare values into groups of numerical values\ndf_data[\"FareBand\"] = pd.qcut(df_data['Fare'], 8, labels = [1, 2, 3, 4,5,6,7,8]).astype('int')\ndf_train[\"FareBand\"] = pd.qcut(df_train['Fare'], 8, labels = [1, 2, 3, 4,5,6,7,8]).astype('int')\ndf_test[\"FareBand\"] = pd.qcut(df_test['Fare'], 8, labels = [1, 2, 3, 4,5,6,7,8]).astype('int')\ndf_train[[\"FareBand\", \"Survived\"]].groupby([\"FareBand\"], as_index=False).mean()\nprint('FareBand feature created')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"612dd9c3bb41a17fc61cd83c7f887119389b6d72","_cell_guid":"5d9a0816-00c1-459a-92b1-019d91dfdda6"},"cell_type":"markdown","source":"*** Note:*** There are several ways that machine learning can evaluate data, you can use discrete data like fare, or you can make that data categorical by grouping it into bands as i have done here or your can take those categories and turn each category into a column. Different models work, differently depending on how you give them the data. I'm going to create all 3 different structures for some features like fare and age and see how they compare. You shoud not over emphasis a feature by using multiple structures of the same data in a model, we'll therefore filter the differnet stuctures before we evaluate the models. "},{"metadata":{"_uuid":"667caecad9a8266c354211a559f109cc8045eab9","_cell_guid":"b872a2c8-166d-4e3c-842d-cb05d0dbe026"},"cell_type":"markdown","source":"## Embarked Feature"},{"metadata":{"_uuid":"88e9ea060f910b0e6372e50c4e19adb406da6275","_cell_guid":"6562ae96-2a2c-475c-910b-26ee2aacb98e","trusted":false,"collapsed":true},"cell_type":"code","source":"#map each Embarked value to a numerical value\nembarked_mapping = {\"S\": 1, \"C\": 2, \"Q\": 3}\ndf_data[\"Embarked\"] = df_data[\"Embarked\"].map(embarked_mapping)\n# split Embanked into df_train and df_test:\ndf_train[\"Embarked\"] = df_data[\"Embarked\"][:891]\ndf_test[\"Embarked\"] = df_data[\"Embarked\"][891:]\nprint('Embarked feature created')\ndf_data[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"03b822c2b89b94e3ce45d4882c38956a99144696","_cell_guid":"4820654b-3839-4a3b-9795-7edf8b8493ee"},"cell_type":"markdown","source":"## Estimate missing Embarkation Data"},{"metadata":{"_uuid":"fa51fe14a830486e1ed5c23e4fe320f1a3392172","_cell_guid":"f0e3a8b6-d72f-41fd-81c7-7cbd71a859cf","trusted":false,"collapsed":true},"cell_type":"code","source":"# Fill the na values in Embanked based on fareband data\nfareband = [1,2,3,4]\nfor fare in fareband:\n    embark_to_impute = df_data.groupby('FareBand')['Embarked'].median()[fare]\n    df_data.loc[(df_data['Embarked'].isnull()) & (df_data['FareBand'] == fare), 'Embarked'] = embark_to_impute\n# Fare in df_train and df_test:\ndf_train[\"Embarked\"] = df_data['Embarked'][:891]\ndf_test[\"Embarked\"] = df_data['Embarked'][891:]\nprint('Missing Embarkation Estimated')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"08481cfb45e394cfdd55571b7f4848270037e75e","_cell_guid":"e8e87c51-45c4-461e-9377-d671baa59cb6"},"cell_type":"markdown","source":"We will come back to fill in the missing age data a little later. Initially i created an estimate based on the mean age and standard deviation, using random numbers to evenly distribute age estimates, which worked, but actually there is a better way using title. As we have not yet extracted the title data yet, we will wait to estimate ages until we have. "},{"metadata":{"_uuid":"873300197bc688e1350aaf8fe230ad8c57d1c222","_cell_guid":"42133e77-1cd1-416d-9100-7479a80b1d89"},"cell_type":"markdown","source":"##  Gender Feature"},{"metadata":{"_uuid":"d1b98799dea36fd35ebd29efdb9cf71b72aa0fae","_cell_guid":"23e16cb2-8bf9-4904-b9db-9304368cc857","trusted":false,"collapsed":true},"cell_type":"code","source":"# convert categories to Columns\ndummies=pd.get_dummies(df_train[['Sex']], prefix_sep='_') #Gender\ndf_train = pd.concat([df_train, dummies], axis=1) \ntestdummies=pd.get_dummies(df_test[['Sex']], prefix_sep='_') #Gender\ndf_test = pd.concat([df_test, testdummies], axis=1) \nprint('Gender Feature added ')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"922c7dad035637b126e82c3d2f929cef696a642d","_cell_guid":"b4a970b2-3eda-466f-bf17-64998499ef38","trusted":false,"collapsed":true},"cell_type":"code","source":"#map each Gendre value to a numerical value\ngender_mapping = {\"female\": 0, \"male\": 1}\ndf_data[\"Sex\"] = df_data['Sex'].map(gender_mapping)\ndf_data[\"Sex\"]=df_data[\"Sex\"].astype('int')\n\n# Family_Survival in TRAIN_DF and TEST_DF:\ndf_train[\"Sex\"] = df_data[\"Sex\"][:891]\ndf_test[\"Sex\"] = df_data[\"Sex\"][891:]\nprint('Gender Category created')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d6f13d0dda22e4389dedb9f188878897c518ceab","_cell_guid":"29bec7eb-8b58-43b1-9a18-72aff67e10e5"},"cell_type":"markdown","source":"## Name Length"},{"metadata":{"_uuid":"49a521117a7fddb2322e8428fc7d01f788d0110c","_cell_guid":"b5685ab4-35d4-4e99-b161-5968d42fd741","trusted":false,"collapsed":true},"cell_type":"code","source":"df_data['NameLen'] = df_data['Name'].apply(lambda x: len(x))\nprint('Name Length calculated')\n\n# split to test and training\ndf_train[\"NameLen\"] = df_data[\"NameLen\"][:891]\ndf_test[\"NameLen\"] = df_data[\"NameLen\"][891:]\n\ndf_train[\"NameBand\"] = pd.cut(df_train[\"NameLen\"], bins=5, labels = [1,2,3,4,5])\ndf_test[\"NameBand\"] = pd.cut(df_test[\"NameLen\"], bins=5, labels = [1,2,3,4,5])\n\n# convert AgeGroup categories to Columns\ndummies=pd.get_dummies(df_train[[\"NameBand\"]].astype('category'), prefix_sep='_') \ndf_train = pd.concat([df_train, dummies], axis=1) \ndummies=pd.get_dummies(df_test[[\"NameBand\"]].astype('category'), prefix_sep='_')\ndf_test = pd.concat([df_test, dummies], axis=1)\nprint(\"Name Length categories created\")\n\npd.qcut(df_train['NameLen'],5).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ba21b6f61bafeb38d67a873ac096c91b734d39b5","_cell_guid":"73a1e1f3-0c5e-42cc-8cf8-00cecd7d3721"},"cell_type":"markdown","source":"## Title Feature"},{"metadata":{"_uuid":"37e03baec88e410f0b920fe9e9c77b9ccfdba5d9","_cell_guid":"a87030f8-df3f-4e9d-ad2a-3605d3b20b0b","trusted":false,"collapsed":true},"cell_type":"code","source":"#Get titles\ndf_data[\"Title\"] = df_data.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\n#Unify common titles. \ndf_data[\"Title\"] = df_data[\"Title\"].replace('Mlle', 'Miss')\ndf_data[\"Title\"] = df_data[\"Title\"].replace('Master', 'Master')\ndf_data[\"Title\"] = df_data[\"Title\"].replace(['Mme', 'Dona', 'Ms'], 'Mrs')\ndf_data[\"Title\"] = df_data[\"Title\"].replace(['Jonkheer','Don'],'Mr')\ndf_data[\"Title\"] = df_data[\"Title\"].replace(['Capt','Major', 'Col','Rev','Dr'], 'Millitary')\ndf_data[\"Title\"] = df_data[\"Title\"].replace(['Lady', 'Countess','Sir'], 'Honor')\n\n# Age in df_train and df_test:\ndf_train[\"Title\"] = df_data['Title'][:891]\ndf_test[\"Title\"] = df_data['Title'][891:]\n\n# convert Title categories to Columns\ntitledummies=pd.get_dummies(df_train[['Title']], prefix_sep='_')\ndf_train = pd.concat([df_train, titledummies], axis=1) \nttitledummies=pd.get_dummies(df_test[['Title']], prefix_sep='_')\ndf_test = pd.concat([df_test, ttitledummies], axis=1) \nprint('Title categories added')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ce0d55f3dc2247dd4403522bedf6ede3c9929c51","_cell_guid":"6e8ddee2-7992-47e4-acaf-ada16fc2f3fb"},"cell_type":"markdown","source":"## Title Cetegory"},{"metadata":{"_uuid":"1841f1df481ac13a11bc86683bef65b53d49afd0","_cell_guid":"25b58c33-0bb0-4140-b5d2-00db768a0847","trusted":false,"collapsed":true},"cell_type":"code","source":"# Mapping titles\ntitle_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Millitary\": 5, \"Honor\": 6}\ndf_data[\"TitleCat\"] = df_data['Title'].map(title_mapping)\ndf_data[\"TitleCat\"] = df_data[\"TitleCat\"].astype(int)\ndf_train[\"TitleCat\"] = df_data[\"TitleCat\"][:891]\ndf_test[\"TitleCat\"] = df_data[\"TitleCat\"][891:]\nprint('Title Category created')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2bb8ee7bb6ee3f6bc3a9f5fc5d3758f40396ca4e","_cell_guid":"5b65087f-6961-48cc-aefe-0deb4a9b1e83"},"cell_type":"markdown","source":"## Fill age based on title\n\nThe Visualisations of age by title suggests that if  we  create our age estimate by looking at the passengers title and using the average age for that title it may produce a more accurate estimate.  "},{"metadata":{"_uuid":"4c138ced404146c2b4bd2f0e5fdf3d9cd7701531","_cell_guid":"65f34a17-e349-4f58-a0de-5bbdb8516c4d","trusted":false,"collapsed":true},"cell_type":"code","source":"titles = ['Master', 'Miss', 'Mr', 'Mrs', 'Millitary','Honor']\nfor title in titles:\n    age_to_impute = df_data.groupby('Title')['Age'].median()[title]\n    df_data.loc[(df_data['Age'].isnull()) & (df_data['Title'] == title), 'Age'] = age_to_impute\n# Age in df_train and df_test:\ndf_train[\"Age\"] = df_data['Age'][:891]\ndf_test[\"Age\"] = df_data['Age'][891:]\nprint('Missing Ages Estimated')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"294626f5dc7d074884a1234a07ca0516a647a8c0","_cell_guid":"005d5aa4-3bee-48ab-b3b4-17ba672a56a1","collapsed":true},"cell_type":"markdown","source":"## Create AgeBands"},{"metadata":{"_uuid":"d91733b91b7787e5562c21b7d3a1a6061a492edc","_cell_guid":"bf8e352d-a753-420a-a172-7a77a6348c6e","trusted":false,"collapsed":true},"cell_type":"code","source":"# sort Age into band categories\nbins = [0,12,24,45,60,np.inf]\nlabels = ['Child', 'Young Adult', 'Adult','Older Adult','Senior']\ndf_train[\"AgeBand\"] = pd.cut(df_train[\"Age\"], bins, labels = labels)\ndf_test[\"AgeBand\"] = pd.cut(df_test[\"Age\"], bins, labels = labels)\nprint('Age Feature created')\n\n# convert AgeGroup categories to Columns\ndummies=pd.get_dummies(df_train[[\"AgeBand\"]], prefix_sep='_')\ndf_train = pd.concat([df_train, dummies], axis=1) \ndummies=pd.get_dummies(df_test[[\"AgeBand\"]], prefix_sep='_')\ndf_test = pd.concat([df_test, dummies], axis=1)\nprint('AgeBand feature created')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"88c9f8d7fb1a843d36fbb7f60c19d2bfcb57364e","_cell_guid":"278791f7-2e30-4e9e-aced-7646350993e7","collapsed":true},"cell_type":"markdown","source":"## Visualize Age Data"},{"metadata":{"_uuid":"b43d90e1336791cbb92814d9c76b1038f3efc2e1","_cell_guid":"46310750-e237-42ed-8fb4-8e397ed8021e","trusted":false,"collapsed":true},"cell_type":"code","source":"# Visualise Age Data \nfig, (axis1,axis2) = plt.subplots(1,2,figsize=(15,4))\naxis1.set_title('Training Age values - Titanic')\naxis2.set_title('Test Age values - Titanic')\n\n# plot original Age values\ndf_train['Age'].dropna().astype(int).hist(bins=70, ax=axis1)\n#df_test['Age'].dropna().astype(int).hist(bins=70, ax=axis1)\n        \n# plot new Age Values\n#df_train['Age'].hist(bins=70, ax=axis2)\ndf_test['Age'].hist(bins=70, ax=axis2)\n\n# peaks for survived/not survived passengers by their age\nfacet = sns.FacetGrid(df_train, hue=\"Survived\",palette = 'seismic',aspect=4)\nfacet.map(sns.kdeplot,'Age',shade= True)\nfacet.set(xlim=(0, df_train['Age'].max()))\nfacet.add_legend()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4db2538017a5dffd13828396d702fc1466cb0c7f","_cell_guid":"b1d61706-8a04-472c-afcb-4c5bc1cff3b8","trusted":false,"collapsed":true},"cell_type":"code","source":"sns.boxplot(data = df_train, x = \"Title\", y = \"Age\");","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b8bd16e435f03db3e0be3e0e38b4d011c571dc98","_cell_guid":"7d1d8177-c410-40ac-bcc4-3a8a7655a607"},"cell_type":"markdown","source":"## Lone Travellers Feature "},{"metadata":{"_uuid":"94804f474da36936590adebe2bd836c3425dbc05","_cell_guid":"a5e8b97a-c1a0-46a3-ab72-20c9f316fe39","trusted":false,"collapsed":true},"cell_type":"code","source":"df_train[\"Alone\"] = np.where(df_train['SibSp'] + df_train['Parch'] + 1 == 1, 1,0) # People travelling alone\ndf_test[\"Alone\"] = np.where(df_test['SibSp'] + df_test['Parch'] + 1 == 1, 1,0) # People travelling alone\nprint('Lone traveller feature created')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"49125b4d1e12f6ecaf2b999877d2e85fb6809650","_cell_guid":"d33cb358-28e2-4b94-a5bd-895611d96e5d"},"cell_type":"markdown","source":"## Mother\n\nWe know that a higher proportion of women survived than die, but of the women that did not survive a large number of these women were women with families that stayed together, we can add a feature to identify women with children."},{"metadata":{"_uuid":"36d70c77a607636b2a26df8a4542f97b8c69da91","_cell_guid":"03e959a6-e231-41f5-817e-d8224f41184a","trusted":false,"collapsed":true},"cell_type":"code","source":"df_data['Mother'] = (df_data['Title'] == 'Mrs') & (df_data['Parch'] > 0)\ndf_data['Mother'] = df_data['Mother'].astype(int)\n\ndf_train[\"Mother\"] = df_data[\"Mother\"][:891]\ndf_test[\"Mother\"] = df_data[\"Mother\"][891:]\nprint('Mother Category created')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bdc2b37c4ed4794c74e5e1e8a8be30cd0482ad1a","_cell_guid":"48736798-d8b0-4c4f-80ea-c767ddf45cb3"},"cell_type":"markdown","source":"## Family Size Feature\n\nWe know that many families stayed together and that the bigger the less likely that family would be to find a lifeboat together. "},{"metadata":{"_uuid":"b28edbd85b8a8f9711629ce170051e79c11fbff1","_cell_guid":"af0da412-4654-402e-a7cf-66f06a2c26f0","trusted":false,"collapsed":true},"cell_type":"code","source":"df_train[\"Family Size\"] = (df_train['SibSp'] + df_train['Parch'] + 1)\ndf_test[\"Family Size\"] = df_test['SibSp'] + df_test['Parch'] + 1\nprint('Family size feature created')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5ef2960e91f503ce9ae5ef96f4b04909f74d6bd6","_cell_guid":"f4f8b2cf-5e1f-4516-96fa-b4684b54ea93"},"cell_type":"markdown","source":"## Family Survival\n\nThis is based on code taken from from https://www.kaggle.com/shunjiangxu/blood-is-thicker-than-water-friendship-forever"},{"metadata":{"_uuid":"efd286c6c48091714b8a51d6aa4a4577c5b9bdfd","_cell_guid":"d2c8840b-3550-4905-ba71-df5bc9567892","trusted":false,"collapsed":true},"cell_type":"code","source":"# get last name\ndf_data[\"Last_Name\"] = df_data['Name'].apply(lambda x: str.split(x, \",\")[0])\n# Set survival value\nDEFAULT_SURVIVAL_VALUE = 0.5\ndf_data[\"Family_Survival\"] = DEFAULT_SURVIVAL_VALUE\n\n# Find Family groups by Fare\nfor grp, grp_df in df_data[['Survived','Name', 'Last_Name', 'Fare', 'Ticket', 'PassengerId',\n                           'SibSp', 'Parch', 'Age', 'Cabin']].groupby(['Last_Name', 'Fare']):\n    \n    if (len(grp_df) != 1):\n        # A Family group is found.\n        for ind, row in grp_df.iterrows():\n            smax = grp_df.drop(ind)['Survived'].max()\n            smin = grp_df.drop(ind)['Survived'].min()\n            passID = row['PassengerId']\n            if (smax == 1.0):\n                df_data.loc[df_data['PassengerId'] == passID, 'Family_Survival'] = 1\n            elif (smin==0.0):\n                df_data.loc[df_data['PassengerId'] == passID, 'Family_Survival'] = 0\n\nprint(\"Number of passengers with family survival information:\", \n      df_data.loc[df_data['Family_Survival']!=0.5].shape[0])\n\n# Find Family groups by Ticket\nfor _, grp_df in df_data.groupby('Ticket'):\n    if (len(grp_df) != 1):\n        for ind, row in grp_df.iterrows():\n            if (row['Family_Survival'] == 0) | (row['Family_Survival']== 0.5):\n                smax = grp_df.drop(ind)['Survived'].max()\n                smin = grp_df.drop(ind)['Survived'].min()\n                passID = row['PassengerId']\n                if (smax == 1.0):\n                    df_data.loc[df_data['PassengerId'] == passID, 'Family_Survival'] = 1\n                elif (smin==0.0):\n                    df_data.loc[df_data['PassengerId'] == passID, 'Family_Survival'] = 0\n                        \nprint(\"Number of passenger with family/group survival information: \" \n      +str(df_data[df_data['Family_Survival']!=0.5].shape[0]))\n\n# Family_Survival in df_train and df_test:\ndf_train[\"Family_Survival\"] = df_data['Family_Survival'][:891]\ndf_test[\"Family_Survival\"] = df_data['Family_Survival'][891:]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"821395bd7d2b3435f2476723067d04fdeaf1a31d","_cell_guid":"0069ade8-2b34-4be3-8ebe-a29f6b396283"},"cell_type":"markdown","source":"## Cabin feature"},{"metadata":{"_uuid":"9106eaa1504c1be04bc5155fb79711f50e70a477","_cell_guid":"cdf5b923-4275-4bb8-a812-58e385c67184","trusted":false,"collapsed":true},"cell_type":"code","source":"# check if cabin inf exists\ndf_data[\"HadCabin\"] = (df_data[\"Cabin\"].notnull().astype('int'))\n# split Embanked into df_train and df_test:\ndf_train[\"HadCabin\"] = df_data[\"HadCabin\"][:891]\ndf_test[\"HadCabin\"] = df_data[\"HadCabin\"][891:]\nprint('Cabin feature created')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1971f3e584336c047064e7aabf255d1740604076","_cell_guid":"d116a6c7-13e4-49c4-93f9-ebea7a24900f"},"cell_type":"markdown","source":"## Deck feature"},{"metadata":{"_uuid":"1c51d71226a432968085d34b707979fe3c71c2f7","_cell_guid":"5e5c0afe-d43d-40a6-a7ad-305a9c2e6921","trusted":false,"collapsed":true},"cell_type":"code","source":"# Extract Deck\ndf_data[\"Deck\"] = df_data.Cabin.str.extract('([A-Za-z])', expand=False)\ndf_data[\"Deck\"] = df_data[\"Deck\"].fillna(\"N\")\n# Map Deck\ndeck_mapping = {\"N\":0,\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5}\ndf_data['Deck'] = df_data['Deck'].map(deck_mapping)\n#Split to training and test\ndf_train[\"Deck\"] = df_data[\"Deck\"][:891]\ndf_test[\"Deck\"] = df_data[\"Deck\"][891:]\nprint('Deck feature created')\n\n#Map and Create Deck feature for training\ndf_data[\"Deck\"] = df_data.Cabin.str.extract('([A-Za-z])', expand=False)\ndeck_mapping = {\"0\":0,\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5}\ndf_data['Deck'] = df_data['Deck'].map(deck_mapping)\ndf_data[\"Deck\"] = df_data[\"Deck\"].fillna(\"0\")\ndf_data[\"Deck\"]=df_data[\"Deck\"].astype('int')\n\ndf_train[\"Deck\"] = df_data['Deck'][:891]\ndf_test[\"Deck\"] = df_data['Deck'][891:]\nprint('Deck feature created')\n\n# convert categories to Columns\ndummies=pd.get_dummies(df_train[['Deck']].astype('category'), prefix_sep='_')\ndf_train = pd.concat([df_train, dummies], axis=1) \ndummies=pd.get_dummies(df_test[['Deck']].astype('category'), prefix_sep='_')\ndf_test = pd.concat([df_test,dummies], axis=1)\nprint('Deck Categories created')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"829d4586ba6fe716f51296025cc29aab8692b47b","_cell_guid":"6350ce16-49ab-49ee-b608-08c564ac7fb6"},"cell_type":"markdown","source":"## Ticket feature"},{"metadata":{"_uuid":"63c13165a9d5762d6ad49334226f10692ca0af49","_cell_guid":"e911fdda-bc54-43c7-b406-90cc5d3dcf9e","trusted":false,"collapsed":true},"cell_type":"code","source":"## Treat Ticket by extracting the ticket prefix. When there is no prefix it returns X. \n\nTicket = []\nfor i in list(df_data.Ticket):\n    if not i.isdigit() :\n        Ticket.append(i.replace(\".\",\"\").replace(\"/\",\"\").strip().split(' ')[0]) #Take prefix\n    else:\n        Ticket.append(\"X\")\n        \ndf_data[\"Ticket\"] = Ticket\ndf_data[\"Ticket\"].head()\n\ndf_train[\"Ticket\"] = df_data[\"Ticket\"][:891]\ndf_test[\"Ticket\"] = df_data[\"Ticket\"][891:]\nprint('Ticket feature created')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"738c514d454c6c5bb0f9d926f862a67f85f223cb","_cell_guid":"53f96f97-e356-4880-bdc8-5e2bf330b7ef","collapsed":true},"cell_type":"markdown","source":"## Ticket Type Feature"},{"metadata":{"_uuid":"5cbf46257b1bfa19f6fe2a78c6f238cbaf5c8cb3","_cell_guid":"87ce3679-ba5c-455c-afbe-b9ecf9b5d22a","trusted":false,"collapsed":true},"cell_type":"code","source":"# ticket prefix\n\ndf_data['TicketRef'] = df_data['Ticket'].apply(lambda x: str(x)[0])\ndf_data['TicketRef'].value_counts()\n#df_data[\"ticketBand\"] = pd.qcut(df_data['ticket_ref'], 5, labels = [1, 2, 3, 4,5]).astype('int')\n\n# split to test and training\ndf_train[\"TicketRef\"] = df_data[\"TicketRef\"][:891]\ndf_test[\"TicketRef\"] = df_data[\"TicketRef\"][891:]\n\n# convert AgeGroup categories to Columns\ndummies=pd.get_dummies(df_train[[\"TicketRef\"]].astype('category'), prefix_sep='_')\ndf_train = pd.concat([df_train, dummies], axis=1) \ndummies=pd.get_dummies(df_test[[\"TicketRef\"]].astype('category'), prefix_sep='_') \ndf_test = pd.concat([df_test, dummies], axis=1)\nprint(\"TicketBand categories created\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c96ba468fd97df8d900444537d01341b6b91048e","_cell_guid":"2188727b-1381-4c0b-b661-e43ef56dec42"},"cell_type":"markdown","source":"## Passenger Class Feature"},{"metadata":{"_uuid":"d2d1289b491aa8198ed3060124a90c77f4856f50","_cell_guid":"5e734c35-6510-42ed-97e3-aa6726072335","trusted":false,"collapsed":true},"cell_type":"code","source":"# convert AgeGroup categories to Columns\ndummies=pd.get_dummies(df_train[[\"Pclass\"]].astype('category'), prefix_sep='_')\ndf_train = pd.concat([df_train, dummies], axis=1) \ndummies=pd.get_dummies(df_test[[\"Pclass\"]].astype('category'), prefix_sep='_')\ndf_test = pd.concat([df_test, dummies], axis=1)\nprint(\"pclass categories created\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"63284dc7ed668a0675b6cdd7de8a7b137f7477e9","_cell_guid":"f0e20b74-3cda-4122-9e35-9dc7b7fbf46a"},"cell_type":"markdown","source":"## Free Passage\nI noticed that the minimum fare is 0.00 and that the ticket type for some of those is 'LINE' . All of those people with a zero ticket cost seem to be male with no siblings so its possible that these people are in some way associated with 'crew' positions. The majority of the people with a ticket price of 0.00 seemed not to survive, so i'm making free a feature to see whether that makes a difference to the model."},{"metadata":{"_uuid":"bfa77a0ca5638b1ade7493f4268578819e2ce1ee","_cell_guid":"368d1f69-a4b7-474b-95e1-77fb3ce321ac","trusted":false,"collapsed":true},"cell_type":"code","source":"# create free feature based on fare = 0 \ndf_data[\"Free\"] = np.where(df_data['Fare'] ==0, 1,0)\ndf_data[\"Free\"] = df_data['Free'].astype(int)\n\ndf_train[\"Free\"] = df_data[\"Free\"][:891]\ndf_test[\"Free\"] = df_data[\"Free\"][891:]\nprint('Free Category created')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e6dcd7a2aaeb580ba8482d41d8599468ecd4b927","_cell_guid":"c6e98217-e9c5-41b7-b792-15035ec18c0c"},"cell_type":"markdown","source":"## FareBand"},{"metadata":{"_uuid":"60efe33e814b836e8997d62d316380bc0de5fe58","_cell_guid":"e4374ba0-9cb5-4826-b826-7ce966c062f7","trusted":false,"collapsed":true},"cell_type":"code","source":"Pclass = [1,2,3]\nfor aclass in Pclass:\n    fare_to_impute = df_data.groupby('Pclass')['Fare'].median()[aclass]\n    df_data.loc[(df_data['Fare'].isnull()) & (df_data['Pclass'] == aclass), 'Fare'] = fare_to_impute\n        \ndf_train[\"Fare\"] = df_data[\"Fare\"][:891]\ndf_test[\"Fare\"] = df_data[\"Fare\"][891:]        \n\n#map Fare values into groups of numerical values\ndf_train[\"FareBand\"] = pd.qcut(df_train['Fare'], 4, labels = [1, 2, 3, 4]).astype('category')\ndf_test[\"FareBand\"] = pd.qcut(df_test['Fare'], 4, labels = [1, 2, 3, 4]).astype('category')\n\n# convert FareBand categories to Columns\ndummies=pd.get_dummies(df_train[[\"FareBand\"]], prefix_sep='_')\ndf_train = pd.concat([df_train, dummies], axis=1) \ndummies=pd.get_dummies(df_test[[\"FareBand\"]], prefix_sep='_')\ndf_test = pd.concat([df_test, dummies], axis=1)\nprint(\"Fareband categories created\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6cc62f744d74a2890f55d0d9cab2c0485f91d11d","_cell_guid":"6b19d12a-7cd7-4cb6-9026-1da1f27ccf10"},"cell_type":"markdown","source":"## Embarked categories"},{"metadata":{"_uuid":"3cb296a788760a279342f016365ddd3bddd7f123","_cell_guid":"9d6a1b65-e74f-4329-8fd0-9e8d1a9e9904","trusted":false,"collapsed":true},"cell_type":"code","source":"# convert Embarked categories to Columns\ndummies=pd.get_dummies(df_train[[\"Embarked\"]].astype('category'), prefix_sep='_') \ndf_train = pd.concat([df_train, dummies], axis=1) \ndummies=pd.get_dummies(df_test[[\"Embarked\"]].astype('category'), prefix_sep='_')\ndf_test = pd.concat([df_test, dummies], axis=1)\nprint(\"Embarked feature created\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ab9fdd9f1f291415911a6391a371fc787f76794d","_cell_guid":"127a8ac6-0c34-445d-95d5-a772f8c09d59"},"cell_type":"markdown","source":"# Exploring the Engineered data"},{"metadata":{"_uuid":"5f1b6ffb552a69c959a609840fa868baa69f9650","_cell_guid":"a93abe26-c2af-43b5-a54a-c783a612a931"},"cell_type":"markdown","source":"## Missing Data"},{"metadata":{"_uuid":"2cd217d4a38e136a9185c5311cd920355d40323e","_cell_guid":"c51d7f61-2358-4ea6-8b46-fda58f9135fc","trusted":false,"collapsed":true},"cell_type":"code","source":"#check for any other unusable values\nprint(len(df_test.columns))\nprint(pd.isnull(df_test).sum())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4b6d1483c18378c32e1f88f56836232dbe341863","_cell_guid":"b510aa95-ace2-46c1-a45e-bfef2bd966cc"},"cell_type":"markdown","source":"## Statistical Overview"},{"metadata":{"_uuid":"db20ccbf0db2f0c3f47a31b7e759a80dbbcce155","_cell_guid":"ed4cdefb-de12-4f7b-98e5-b532bc2248ae","trusted":false,"collapsed":true},"cell_type":"code","source":"df_train.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f264ea422251e2c02721cd954e2df78c2006ad6a","_cell_guid":"1221771d-4505-4865-8efb-34bd9b643133"},"cell_type":"markdown","source":"# Visualizing age data\nWe could estimate all of the ages based on the mean and standard deviation of the data set, however as age is obviously an important feature in pridicting survival and we need to look at the other features and see if we can work out a way to make a more accurate estimate of age for any given passenger. First lets look at the different age distributions of passengers by title."},{"metadata":{"_uuid":"6316a37319e860411d240ee58e73fe977157a45b","_cell_guid":"16cbd148-bba4-479c-b12e-fd8e4f3d601d","trusted":false,"collapsed":true},"cell_type":"code","source":"# Groupby title\ndf_train[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()\n# plot age distribution by title\nfacet = sns.FacetGrid(data = df_train, hue = \"Title\", legend_out=True, size = 5)\nfacet = facet.map(sns.kdeplot, \"Age\")\nfacet.add_legend();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aad62a7b5e958339fa3daf0ef666bbad8d0abfea","_cell_guid":"7482e53e-c034-42a0-85aa-9a657e90c20e"},"cell_type":"markdown","source":"The age distribution looks slightly suspect and possibly merits further investigation, for example while master generally refers to male's under 16 there a number that are over 40, this might be explained if master is also a title in nautical terms like 'Master Seaman'. You might also expect a quite Normal distribution of ages for any given title, but in many cases this doesn't seem to be the case, this is most likely caused by out estimated numbers skewing the data, one way to avoid this would be to use a random number based on the standard deviation in the estimate for each to get a more natural dataset. We could also use age bands rather than age in the model."},{"metadata":{"_uuid":"cda5452db331d74e319c95053249adc2da8d4477","_cell_guid":"6f24c55a-74e1-4789-b430-ebba2d2a234f"},"cell_type":"markdown","source":"### Survival by FareBand and Gender "},{"metadata":{"_uuid":"2ab0e04ade741513a4a7792e6603ee82ef9a264c","_cell_guid":"34dc8d59-7425-492e-bb37-8d6856b27536","trusted":false,"collapsed":true},"cell_type":"code","source":"grid = sns.FacetGrid(df_train, col = \"FareBand\", row = \"Sex\", hue = \"Survived\", palette = 'seismic')\ngrid = grid.map(plt.scatter, \"PassengerId\", \"Age\")\ngrid.add_legend()\ngrid","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2108bea0ee1fc9d83ad31a1a11f668bf2e24ee38","_cell_guid":"2ba192cc-6293-4050-a742-dcaa67b6416e"},"cell_type":"markdown","source":"### Survival by Deck and Gender "},{"metadata":{"_uuid":"5e0b89f06bff8b5d647e6eb3743abc132c956d62","_cell_guid":"531add0d-4a07-4247-bda2-d07e42b85bef","trusted":false,"collapsed":true},"cell_type":"code","source":"grid = sns.FacetGrid(df_train, col = \"Deck\", row = \"Sex\", hue = \"Survived\", palette = 'seismic')\ngrid = grid.map(plt.scatter, \"PassengerId\", \"Age\")\ngrid.add_legend()\ngrid","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"05756f61c82449b0a2804b940bc5c9b305e39040","_cell_guid":"c6b6a59d-0d55-44f9-a5e4-3dbc78dd213b"},"cell_type":"markdown","source":"### Survival by Family Size and Gender "},{"metadata":{"_uuid":"10b4e8ce607a6297500cc45ae72e1d1a59861bf4","_cell_guid":"14a51b8e-7eea-49c3-91db-59aed6326ff2","trusted":false,"collapsed":true},"cell_type":"code","source":"grid = sns.FacetGrid(df_train, col = \"Family Size\", row = \"Sex\", hue = \"Survived\", palette = 'seismic')\ngrid = grid.map(plt.scatter, \"PassengerId\", \"Age\")\ngrid.add_legend()\ngrid","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"58021ef05643a0238b8d0d5432a7af4d9da7c357","_cell_guid":"63efa126-2a01-4a30-8020-f1aa48110f06"},"cell_type":"markdown","source":"### Survival by Passenger Class and Family Size"},{"metadata":{"_uuid":"5862c89f842661813d0a9a3c55b1ace0d990ad2d","_cell_guid":"5da3b70d-9889-4ce4-9ab7-999052e02b7b","trusted":false,"collapsed":true},"cell_type":"code","source":"fig, (axis1,axis2) = plt.subplots(1,2,figsize=(15,4))\naxis1.set_title('Training Age values - Titanic')\naxis2.set_title('Test Age values - Titanic')\n\nx1=df_train[df_train[\"Survived\"]==0]\nx2=df_train[df_train[\"Survived\"]==1]\n\n# Set up the matplotlib figure\nplt.figure(1)\nsns.jointplot(x=\"Family Size\", y=\"Pclass\", data=x1, kind=\"kde\", color='b');\nplt.figure(2)\nsns.jointplot(x=\"Family Size\", y=\"Pclass\", data=x2, kind=\"kde\", color='r');\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"77d6372808e34a2931e3ffac62880a590b0b1558","_cell_guid":"910ee13d-7181-4e07-9332-7fe0d4c96024"},"cell_type":"markdown","source":"### Fare Jointplot "},{"metadata":{"_uuid":"2fa46d8f2cd4229152dd1bf37bcaf4dbcd1aedf8","_cell_guid":"90ba8a16-086c-4664-a812-a09e7755c1d1","trusted":false,"collapsed":true},"cell_type":"code","source":"sns.jointplot(data=x1, x='PassengerId', y='Age', kind='scatter',color='b')\nplt.figure(4)\nsns.jointplot(data=x2, x='PassengerId', y='Age', kind='scatter',color='r')\n# sns.plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"70b77d246c62e356a9521f8602c844f8ac59c73b","_cell_guid":"9eb86bf1-2838-4aeb-8d63-21e2ec707dca"},"cell_type":"markdown","source":"# Re-train the model on new features"},{"metadata":{"_uuid":"493de5388b78f8a2b12b9d79ab9fbbadad7b9ad0","_cell_guid":"cc55e96d-a719-4383-a1d5-9d1504733201","trusted":false,"collapsed":true},"cell_type":"code","source":"df_train.columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f26148292ca17a671909a1cf1566394aed056c16","_cell_guid":"21c55533-2b89-47c2-b543-443c39de7778","trusted":false,"collapsed":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1cde5b8da29fe2cdf3bb28a27cca13c636d9bfde","_cell_guid":"ef7d08df-a34f-407c-9d7e-5ba8803b3f7e"},"cell_type":"markdown","source":"## Select Columns of Interest"},{"metadata":{"_uuid":"e103c4e81407988ae5ca3783cb042e9eec7a1db1","_cell_guid":"8933e641-cfa4-4c9b-bbc8-b08f8830dcf0","collapsed":true,"trusted":false},"cell_type":"code","source":"# Create list of interesting columns\nSIMPLE_COLUMNS=['Pclass','Age','SibSp','Parch','Family_Survival','Alone','Sex_female','Sex_male','Title_Master', 'Title_Miss','Title_Mr', 'Title_Mrs', 'Title_Millitary','Embarked','FareBand','TicketRef'] #84\nINTERESTING_COLUMNS=['Survived','Pclass','Age','SibSp','Parch','Title','Alone','Mother','Family Size','Family_Survival','Embarked','FareBand','TicketRef']\nCATEGORY_COLUMNS=['Family Size','Family_Survival','Alone','Mother','Sex_female','Sex_male','AgeBand_Child',\n       'AgeBand_Young Adult', 'AgeBand_Adult', 'AgeBand_Older Adult',\n       'AgeBand_Senior','Title_Master', 'Title_Miss','Title_Mr', 'Title_Mrs', 'Title_Millitary','NameBand_1',\n       'NameBand_2', 'NameBand_3', 'NameBand_4', 'NameBand_5','Embarked','TicketRef_A', 'TicketRef_C', 'TicketRef_F', 'TicketRef_L',\n       'TicketRef_P', 'TicketRef_S', 'TicketRef_W', 'TicketRef_X','Pclass_1', 'Pclass_2', 'Pclass_3','HadCabin','Free','FareBand_1', 'FareBand_2', 'FareBand_3', 'FareBand_4'] ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3d597ed6199a01efefeb4d36848c2f321a9a622d","_cell_guid":"afe77855-d9f3-47ec-af05-0da48baa7656"},"cell_type":"markdown","source":"# Re-evaluate the on new features"},{"metadata":{"_uuid":"560b663f936650b7a6c2c844eac395f5de41dd23","scrolled":true,"_cell_guid":"aaa2c9cc-234e-455a-bb25-339d5655a286","trusted":false,"collapsed":true},"cell_type":"code","source":"# create test and training data\ntest = df_test[SIMPLE_COLUMNS].fillna(-1000)\ndata_to_train = df_train[SIMPLE_COLUMNS].fillna(-1000)\nX_train, X_test, y_train, y_test = train_test_split(data_to_train, df_train['Survived'], test_size=0.3,random_state=21, stratify=df_train['Survived'])\n\nRandomForest = RandomForestClassifier(random_state = 0)\nRandomForest.fit(X_train, y_train)\nprint('Evaluation complete')\n# Print the accuracy# Print  \nprint(\"Accuracy: {}\".format(RandomForest.score(X_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7c5f4cc81c76302ca6055d342d8a576033f6ba52","_cell_guid":"a77a118e-e7aa-4af8-803d-20ad04135c34"},"cell_type":"markdown","source":"## Feature Correlation"},{"metadata":{"_uuid":"5e99cf817172444c0f5b868d186c288d85e46d39","_cell_guid":"a0af02a5-6e8f-48fc-9402-c3d81f75e912","trusted":false,"collapsed":true},"cell_type":"code","source":"#map feature correlation\nf,ax = plt.subplots(figsize=(12, 12))\nsns.heatmap(df_train[INTERESTING_COLUMNS].corr(),annot=True, linewidths=.5, fmt= '.1f',ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9faa0d437e4c7943e1639d0acfb33780a8b2b24b","_cell_guid":"d95e4667-d810-46d3-a832-e781e58fdbc0"},"cell_type":"markdown","source":"## Feature Importance (for random forest)"},{"metadata":{"_uuid":"71d23b03f840b1b757245bb3c1aa0641e49eefdd","_cell_guid":"737ac635-c4d8-48a4-858e-797d803b2c14","trusted":false,"collapsed":true},"cell_type":"code","source":"RandomForest_checker = RandomForestClassifier()\nRandomForest_checker.fit(X_train, y_train)\nimportances_df = pd.DataFrame(RandomForest_checker.feature_importances_, columns=['Feature_Importance'],\n                              index=X_train.columns)\nimportances_df.sort_values(by=['Feature_Importance'], ascending=False, inplace=True)\nprint(importances_df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f8195440d641ac964d496f1907bb54f64da5a0be","_cell_guid":"1ea361c4-4126-4e9d-b548-ea56ef5ae083"},"cell_type":"markdown","source":"# Re-forcast predictions based on new features"},{"metadata":{"_uuid":"9fdcd905b7a60429ceb4d62620c388a9faa980a7","_cell_guid":"755a1493-f4be-40ae-be33-9ee302882a6f","trusted":false,"collapsed":true},"cell_type":"code","source":"Submission['Survived']=RandomForest.predict(test)\nprint(Submission.head())\nprint('Submission created')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"50f117aaa9560e10383d0a4b0d891675c0d328a2","_cell_guid":"0d689a14-d161-407b-9fbe-0545d654da7e"},"cell_type":"markdown","source":"# Make revised submission"},{"metadata":{"_uuid":"8971da22e3035e23af2f7b587dcdab60a126d1b9","scrolled":false,"_cell_guid":"f61ee101-471d-4425-9a95-a10e1b269bdf","trusted":false,"collapsed":true},"cell_type":"code","source":"# write data frame to csv file\nSubmission.set_index('PassengerId', inplace=True)\nSubmission.to_csv('randomforestcat02.csv',sep=',')\nprint('file created')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"73fb67722c4b233ff29e491955b752f99eb2a539","_cell_guid":"68850cd4-c427-4f53-8af6-a7f2c9a9df1b"},"cell_type":"markdown","source":"The second revised submission scored 0.75598 which was an improvement of the original revision which scored 0.64593, this used was  is an improvement on the original score of 0.57894. This advanced the submission to 9117 place on the leaderboard, from the starting point of 10599th place! Obviousy a step in the right direction but still needing work."},{"metadata":{"_uuid":"f3425a2b3b51e62d06f9c7550b485ef25b3c7539","_cell_guid":"b4680f97-d9e3-40d8-b5b3-39d914e25965"},"cell_type":"markdown","source":"# Stage 3 : Test Different Models and parameters"},{"metadata":{"_uuid":"443ea01c565176c5b6c059653b680651314f0de9","_cell_guid":"b506b4f4-e3c2-43a4-a070-75822e288fe7"},"cell_type":"markdown","source":"## Split data into test and training"},{"metadata":{"_uuid":"1036293295a5a355f3768ad7fa88457ff8122320","_cell_guid":"73f4510e-9e33-48b7-96f4-08842da93099","trusted":false,"collapsed":true},"cell_type":"code","source":"REVISED_NUMERIC_COLUMNS=['Pclass','Age','SibSp','Parch','Fare','Family_Survival','Alone','Title_Master', 'Title_Miss','Title_Mr', 'Title_Mrs', 'Title_Millitary','Embarked','HadCabin'] #84\nSIMPLE_COLUMNS=['Pclass','Age','SibSp','Parch','Family_Survival','Alone','Sex_female','Sex_male','Title_Master', 'Title_Miss','Title_Mr', 'Title_Mrs', 'Title_Millitary','Embarked','HadCabin','FareBand','TicketRef'] #84\nINTERESTING_COLUMNS=['Survived','Pclass','Age','SibSp','Parch','Title','Alone','Mother','Family Size','Family_Survival','Embarked','HadCabin','FareBand','TicketRef']\nCATEGORY_COLUMNS=['Pclass','SibSp','Parch','Family Size','Family_Survival','Alone','Mother','Sex_female','Sex_male','AgeBand_Child',\n       'AgeBand_Young Adult', 'AgeBand_Adult', 'AgeBand_Older Adult',\n       'AgeBand_Senior','Title_Master', 'Title_Miss','Title_Mr', 'Title_Mrs', 'Title_Millitary','NameBand_1',\n       'NameBand_2', 'NameBand_3', 'NameBand_4', 'NameBand_5','Embarked','TicketRef_A', 'TicketRef_C', 'TicketRef_F', 'TicketRef_L',\n       'TicketRef_P', 'TicketRef_S', 'TicketRef_W', 'TicketRef_X','HadCabin','Free'] \n\n#print(df_test.columns)\n# create test and training data\ndata_to_train = df_train[REVISED_NUMERIC_COLUMNS].fillna(-1000)\nprediction = df_train[\"Survived\"]\ntest = df_test[REVISED_NUMERIC_COLUMNS].fillna(-1000)\nX_train, X_val, y_train, y_val = train_test_split(data_to_train, prediction, test_size = 0.3,random_state=21, stratify=y)\nprint('Data split')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bdf630db7a9b896f599fd673b8becb0c14593778","_cell_guid":"d159a9f6-5a13-4ea0-a452-3d917da0b50f"},"cell_type":"markdown","source":"## AdaBoost"},{"metadata":{"_uuid":"6993d034cad8df9a8c1a407b459157e3739ca294","_cell_guid":"f56ad789-f307-4854-b8cd-5243bd2a4076","trusted":false,"collapsed":true},"cell_type":"code","source":"adaboost=AdaBoostClassifier()\nadaboost.fit(X_train, y_train)\ny_pred = adaboost.predict(X_val)\nacc_adaboost = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_adaboost)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f085bb2f49af3b8d362fbb4a5dce3e6e38910460","_cell_guid":"cdfd7e12-8baf-4147-89c1-5d28e289844d"},"cell_type":"markdown","source":"## Bagging"},{"metadata":{"_uuid":"d971aab992d3949b77549f39bec14ac7df4ea5c2","_cell_guid":"7efa6c80-8458-48c7-9b0f-6072ede54c99","trusted":false,"collapsed":true},"cell_type":"code","source":"bagging=BaggingClassifier()\nbagging.fit(X_train, y_train)\ny_pred = bagging.predict(X_val)\nacc_bagging = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_bagging)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4f5be1e8410b2208897ae8df93f334ee63332805","_cell_guid":"035469fb-4b8d-4d2d-a056-4498f7d4e815"},"cell_type":"markdown","source":"## Decision Tree"},{"metadata":{"_uuid":"21cd723ecb8e0b3c517aa7681523ddde3dab0cd8","_cell_guid":"69e9aa91-4451-47ae-95db-e9c8da605311","trusted":false,"collapsed":true},"cell_type":"code","source":"#Decision Tree\ndecisiontree = DecisionTreeClassifier()\ndecisiontree.fit(X_train, y_train)\ny_pred = decisiontree.predict(X_val)\nacc_decisiontree = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_decisiontree)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"100c4a03188d6fd78942f6e562af56a9cc03efe8","_cell_guid":"f920ed6a-1da2-4c5d-8999-cf8e29436483"},"cell_type":"markdown","source":"## Extra Trees"},{"metadata":{"_uuid":"0c75546741f431d3d548984407a7f635d90f7792","_cell_guid":"aa03cf6a-f762-4b81-a11e-9c7023a189cc","trusted":false,"collapsed":true},"cell_type":"code","source":"# ExtraTreesClassifier\net = ExtraTreesClassifier()\net.fit(X_train, y_train)\ny_pred = et.predict(X_val)\nacc_et = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_et)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1226a76fd4160f2c9a5c83d59d4ba23f6258e8b6","_cell_guid":"73a9b607-c0b3-4579-afe4-625f3c802c85"},"cell_type":"markdown","source":"## Gaussian Naive Bayes"},{"metadata":{"_uuid":"3163db86046533c476eb9c40aaafd0dce63577be","_cell_guid":"b2b7eee6-4bcb-4663-8d06-7faa3047a705","trusted":false,"collapsed":true},"cell_type":"code","source":"# Gaussian Naive Bayes\ngaussian = GaussianNB()\ngaussian.fit(X_train, y_train)\ny_pred = gaussian.predict(X_val)\nacc_gaussian = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_gaussian)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e97caf6ae1e663180b0fce6579a2a70724e6528b","_cell_guid":"cd97481d-ba00-41b7-89e6-02014b794b39"},"cell_type":"markdown","source":"## Gradient Boosting "},{"metadata":{"_uuid":"817368399753a7a1c2c24578f08bdda32b921fe2","_cell_guid":"de1e12f5-6f9a-46ce-b7d1-33c033aabcfa","trusted":false,"collapsed":true},"cell_type":"code","source":"# Gradient Boosting Classifier\ngbk = GradientBoostingClassifier()\ngbk.fit(X_train, y_train)\ny_pred = gbk.predict(X_val)\nacc_gbk = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_gbk)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"26f8526c96d17452f9977cd46e1388a45da47bcf","_cell_guid":"1c5e8de1-2c8c-4ac1-9a04-671da36b521e"},"cell_type":"markdown","source":"## K Nearest Neighbors"},{"metadata":{"_uuid":"2592e81883752e4120b2c7385719e388037195e1","_cell_guid":"428dbbda-7b9f-44c9-8e3e-c2236148be81","trusted":false,"collapsed":true},"cell_type":"code","source":"# KNN or k-Nearest Neighbors\nknn = KNeighborsClassifier()\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_val)\nacc_knn = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_knn)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5d0094b70f5bcd6988eee59ae93035a9d6cf9d36","_cell_guid":"612d3eb0-b0a1-4640-b53b-aab9391b4fb4"},"cell_type":"markdown","source":"## Linear Discriminant Analysis"},{"metadata":{"_uuid":"9a1dfef4085108d1e57f81e0625066486dfdf911","_cell_guid":"62462fec-26fd-4fab-9daf-4323477a0c9a","trusted":false,"collapsed":true},"cell_type":"code","source":"linear_da=LinearDiscriminantAnalysis()\nlinear_da.fit(X_train, y_train)\ny_pred = linear_da.predict(X_val)\nacc_linear_da = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_linear_da)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"da17bdf963a79774138d1dad3a220f4798f5d30d","_cell_guid":"5a35f1ae-9c73-417a-ace0-cd9adfc77327"},"cell_type":"markdown","source":"## LinearSVC"},{"metadata":{"_uuid":"3ec6b8e1a509ff6efe6c4910f123db5d96a3d6df","_cell_guid":"b08b8e33-9289-47d3-b2a9-473fba010f45","trusted":false,"collapsed":true},"cell_type":"code","source":"# Linear SVC\nlinear_svc = LinearSVC()\nlinear_svc.fit(X_train, y_train)\ny_pred = linear_svc.predict(X_val)\nacc_linear_svc = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_linear_svc)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6d165b5f552a2c72edcad498c9eddc890fb937b3","_cell_guid":"bb023848-8e7f-42de-a3df-03258de1a923"},"cell_type":"markdown","source":"## Logistic Regression"},{"metadata":{"_uuid":"72aa9637fd58ced0b2bc4bf19c8f798bbe9bc027","_cell_guid":"f5eb8864-902d-4b53-935c-c5ae3aa5fc3f","trusted":false,"collapsed":true},"cell_type":"code","source":"# Logistic Regression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_val)\nacc_logreg = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_logreg)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c5ee54c9df91861096c09f6199b2a7d581ddd4ae","_cell_guid":"3a011ce7-45d1-49c3-8663-809aab2ab7f7"},"cell_type":"markdown","source":"## MLP"},{"metadata":{"_uuid":"c88c957cd82729def374ed7a5e1db5d955f8021c","_cell_guid":"f74f8e21-23f7-4067-ac22-b31442c0be73","trusted":false,"collapsed":true},"cell_type":"code","source":"MLP = MLPClassifier()\nMLP.fit(X_train, y_train)\ny_pred = MLP.predict(X_val)\nacc_MLP= round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_MLP)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eeddea8e4b638ef80f6aeb880b4c74b492ba6808","_cell_guid":"801d1bd2-dded-4f3e-a711-1a5679c9ed7d"},"cell_type":"markdown","source":"## Passive Aggressive"},{"metadata":{"_uuid":"0f70da04c5fcf96eea9a037aa21189dfae81dbb7","_cell_guid":"2eb23881-42b5-4749-b690-243599da6136","trusted":false,"collapsed":true},"cell_type":"code","source":"passiveaggressive = PassiveAggressiveClassifier()\npassiveaggressive.fit(X_train, y_train)\ny_pred = passiveaggressive.predict(X_val)\nacc_passiveaggressive = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_passiveaggressive)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bc5724a2fe4620d312612afe91049dbe830c16e2","_cell_guid":"4294c1b3-d30d-4304-ba8b-d2340e5d7413"},"cell_type":"markdown","source":"## Perceptron"},{"metadata":{"_uuid":"ea7e1694c96821a6ca3fcbd8bc6039247f519db8","_cell_guid":"2e259321-19d4-429a-bac6-f6e2f4a26477","trusted":false,"collapsed":true},"cell_type":"code","source":"# Perceptron\nperceptron = Perceptron()\nperceptron.fit(X_train, y_train)\ny_pred = perceptron.predict(X_val)\nacc_perceptron = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_perceptron)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"763494c08e2edddef19b49608da458774cb8515f","_cell_guid":"074abaa3-522d-43c7-a979-c370d52ae47f"},"cell_type":"markdown","source":"## Random Forest"},{"metadata":{"_uuid":"790e9ad10cc6cd242fa959ebec9cc63b9a485996","scrolled":true,"_cell_guid":"104a417c-d913-40fe-81d7-97f71a8db89a","trusted":false,"collapsed":true},"cell_type":"code","source":"# Random Forest\nrandomforest = RandomForestClassifier(random_state = 0)\nrandomforest.fit(X_train, y_train)\ny_pred = randomforest.predict(X_val)\nacc_randomforest = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_randomforest)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"251343aba5c0a115510c03b6253b8e668ae74dbc","_cell_guid":"7a74f688-dbbc-4aae-b3fe-32190938fb37"},"cell_type":"markdown","source":"## Ridge Classifier"},{"metadata":{"_uuid":"114a8bde096080444cd4496c0fb3cf96ae192aad","_cell_guid":"feebb201-cb4f-4c9e-b531-751b7b126c17","trusted":false,"collapsed":true},"cell_type":"code","source":"ridge = RidgeClassifierCV()\nridge.fit(X_train, y_train)\ny_pred = ridge.predict(X_val)\nacc_ridge = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_ridge)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8ec249236702ac1e14d9eca31bf9663666b531fa","_cell_guid":"d66a6c84-133f-4600-83bc-dde061dcd523"},"cell_type":"markdown","source":"## Stochastic Gradient Descent"},{"metadata":{"_uuid":"549cc6b5b02f76aab75a3b025005b40876401a33","_cell_guid":"e422b7cc-f6f8-4a35-8de4-119b11409dbe","trusted":false,"collapsed":true},"cell_type":"code","source":"# Stochastic Gradient Descent\nsgd = SGDClassifier()\nsgd.fit(X_train, y_train)\ny_pred = sgd.predict(X_val)\nacc_sgd = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_sgd)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d18a7378e66211672e4317485b4ddf83969f8be4","_cell_guid":"aa24897d-baa0-44f3-a100-b50ed321aab9"},"cell_type":"markdown","source":"## Support Vector Machines\n\nHas more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples.\n1. This class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme."},{"metadata":{"_uuid":"66ae498ded981e6269358cfe6bc60fed3b5faadd","_cell_guid":"d34aa302-fc6d-4787-a8a8-36273d85d01c","trusted":false,"collapsed":true},"cell_type":"code","source":"# instanciate model\nclf = SVC()\n# fit model\nclf.fit(X_train, y_train)\n# predict results\ny_pred = clf.predict(X_val)\n# check accuracy\nacc_clf = round(accuracy_score(y_pred, y_val) * 100, 2)\n#print accuracy\nprint(acc_clf)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b17e4b5b04a14ae84f7f0b1a34d951d44876fe58","_cell_guid":"00f8e497-d81b-4a58-9540-cfd55303a973"},"cell_type":"markdown","source":"## xgboost"},{"metadata":{"_uuid":"a6647116fc05b74500095b33d98810377ca60a4a","_cell_guid":"f82f70dd-912b-4ec3-9dac-fd63aa72f27f","trusted":false,"collapsed":true},"cell_type":"code","source":"# xgboost\nxgb = XGBClassifier(n_estimators=10)\nxgb.fit(X_train, y_train)\ny_pred = xgb.predict(X_val)\nacc_xgb = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_xgb)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e8fadb4412e7274f4fe91daf9f5c899beb9ac80d","_cell_guid":"29196c0a-5f90-4bdb-88bb-72ad07c12732"},"cell_type":"markdown","source":"## Comparing the results"},{"metadata":{"_uuid":"476d4fe076dd263d96070206bb616b2549133a15","_cell_guid":"16e75897-65af-40c4-b236-ea0a3c42c196","trusted":false,"collapsed":true},"cell_type":"code","source":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', 'Ridge Classifier',\n              'Random Forest', 'Naive Bayes', 'Linear SVC', 'MLP','AdaBoost','Linear discriminant','Passive Aggressive',\n              'Decision Tree', 'Gradient Boosting Classifier','Extra Trees','Stochastic Gradient Descent','Perceptron','xgboost'],\n    'Score': [acc_clf, acc_knn, acc_logreg,acc_ridge,acc_randomforest, acc_gaussian,acc_linear_svc, acc_MLP,acc_adaboost,acc_linear_da,acc_passiveaggressive,acc_decisiontree,acc_gbk,acc_et,acc_sgd,acc_perceptron,acc_xgb]})\nmodels.sort_values(by='Score', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c75a06d718bf4e91ab977472870fc922736f75f3","_cell_guid":"e74f1aed-5b01-4a50-a078-2fe91fb4be84"},"cell_type":"markdown","source":"# Reforcast predictions based on best performing model"},{"metadata":{"_uuid":"5a629fec076a8f4a27fc2a785edf4cc728ccc93a","_cell_guid":"7b41f3e8-d98f-44b2-bb76-104a0a2171b7","trusted":false,"collapsed":true},"cell_type":"code","source":"Submission['Survived']=ridge.predict(test)\nprint(Submission.head(5))\nprint('Prediction complete')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d32f3275adef82a72a3d7f237f923c8f6ee35d07","_cell_guid":"0fd0b448-227e-492d-89a3-9cdd5389f34b"},"cell_type":"markdown","source":"# Make model submission"},{"metadata":{"_uuid":"041de4936c148b80686c0e7b346bdd70d0c522b1","_cell_guid":"7bc66dd8-9b1a-4c85-9aa1-37cc03fe930f","trusted":false,"collapsed":true},"cell_type":"code","source":"# write data frame to csv file\nSubmission.set_index('PassengerId', inplace=True)\nSubmission.to_csv('ridgesubmission02.csv',sep=',')\nprint('File created')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e32f127a2a97f13730c1302b8b595b2452e78045","_cell_guid":"3099cd36-3434-46b4-a78e-585875de5b0a"},"cell_type":"markdown","source":"# Stage 4 : Hyper Tuning the Models"},{"metadata":{"_uuid":"dda6888232a077b2da3c2b0ac93e722783327386","_cell_guid":"1df044b1-f8e6-485e-b1cd-9179c8a04595","trusted":false,"collapsed":true},"cell_type":"code","source":"REVISED_NUMERIC_COLUMNS=['Pclass','Age','SibSp','Parch','Family_Survival','Alone','Title_Master', 'Title_Miss','Title_Mr', 'Title_Mrs', 'Title_Millitary','Embarked'] #84\nSIMPLE_COLUMNS=['Pclass','Age','SibSp','Parch','Family_Survival','Alone','Sex_female','Sex_male','Title_Master', 'Title_Miss','Title_Mr', 'Title_Mrs', 'Title_Millitary','Embarked'] #84\nINTERESTING_COLUMNS=['Survived','Pclass','Age','SibSp','Parch','Title','Alone','Mother','Family Size','Family_Survival','Embarked','FareBand','TicketRef']\nCATEGORY_COLUMNS=['Pclass','SibSp','Parch','Family Size','Family_Survival','Alone','Mother','Sex_female','Sex_male','AgeBand_Child',\n       'AgeBand_Young Adult', 'AgeBand_Adult', 'AgeBand_Older Adult',\n       'AgeBand_Senior','Title_Master', 'Title_Miss','Title_Mr', 'Title_Mrs', 'Title_Millitary','NameBand_1',\n       'NameBand_2', 'NameBand_3', 'NameBand_4', 'NameBand_5','Embarked','TicketRef_A', 'TicketRef_C', 'TicketRef_F', 'TicketRef_L',\n       'TicketRef_P', 'TicketRef_S', 'TicketRef_W', 'TicketRef_X','HadCabin','Free'] \n\n#print(df_test.columns)\n# create test and training data\ndata_to_train = df_train[CATEGORY_COLUMNS].fillna(-1000)\nprediction = df_train[\"Survived\"]\ntest = df_test[CATEGORY_COLUMNS].fillna(-1000)\nX_train, X_val, y_train, y_val = train_test_split(data_to_train, prediction, test_size = 0.3,random_state=21, stratify=prediction)\nprint('Data split')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8350b77da7bc61bedc4374f308e09db22ba6df41","_cell_guid":"635b8fd0-3e7b-46b1-bc95-33fe67b36eef"},"cell_type":"markdown","source":"\n"},{"metadata":{"_uuid":"57c4ff97a9449b31e46b75129ebd0e67bec19c9f","_cell_guid":"09934873-663b-4e24-b589-6f9009d4fa5f"},"cell_type":"markdown","source":"## Linear Regression SVC"},{"metadata":{"_uuid":"9eb1635f91e2beb04412c47a6a07a6d141778309","_cell_guid":"29cccffb-38c3-40f3-8adb-55bb9e55f3e4","trusted":false,"collapsed":true},"cell_type":"code","source":"# Support Vector Classifier parameters \nparam_grid = {'C':np.arange(1, 7),\n              'degree':np.arange(1, 7),\n              'max_iter':np.arange(0, 12),\n              'kernel':['rbf','linear'],\n              'shrinking':[0,1]}\n\nclf = SVC()\nsvc_cv=GridSearchCV(clf, param_grid, cv=10)\nsvc_cv.fit(X_train, y_train)\n\nprint(\"Tuned SVC Parameters: {}\".format(svc_cv.best_params_))\nprint(\"Best score is {}\".format(svc_cv.best_score_))\nacc_svc_cv = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_svc_cv)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"acb983e1342ac4f56181a80a8fd53deb28b4729c","_cell_guid":"d3e7fd18-76fa-47cf-bdf6-8c59edc7a46a"},"cell_type":"markdown","source":"## Logistic Regression"},{"metadata":{"_uuid":"0a9d6988a490b93d3c4fa085b6fc326153288b5a","scrolled":true,"_cell_guid":"11836eb4-654d-43a4-a6ad-784992ef2cae","trusted":false,"collapsed":true},"cell_type":"code","source":"# Logistic Regression\n\nfrom sklearn.linear_model import LogisticRegression\n# create parameter grid as a dictionary where the keys are the hyperparameter names and the values are lists of values that we want to try.\nparam_grid = {\"solver\": ['newton-cg','lbfgs','liblinear','sag','saga'],'C': [0.01, 0.1, 1, 10, 100]}\n\n# instanciate classifier\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\n\nlogreg_cv = GridSearchCV(logreg, param_grid, cv=30)\nlogreg_cv.fit(X_train, y_train)\n\ny_pred = logreg_cv.predict(X_val)\nprint(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_))\nprint(\"Best score is {}\".format(logreg_cv.best_score_))\nacc_logreg_cv = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_logreg_cv)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"637dcc57d021d4e98bcd1acec7153fc968c1cc28","_cell_guid":"04a4fb0d-eeb2-4279-bc54-21109f78db3c"},"cell_type":"markdown","source":"## KNN "},{"metadata":{"_uuid":"b4c2895af86e90da19b5c4ee2b2df24d4f6f7275","scrolled":true,"_cell_guid":"ec9d9407-4ee9-47a6-893b-65dd2a5159c5","trusted":false,"collapsed":true},"cell_type":"code","source":"# KNN or k-Nearest Neighbors with GridSearch\n\n# create parameter grid as a dictionary where the keys are the hyperparameter names and the values are lists of values that we want to try.\nparam_grid = {\"n_neighbors\": np.arange(1, 50),\n             \"leaf_size\": np.arange(20, 40),\n             \"algorithm\": [\"ball_tree\",\"kd_tree\",\"brute\"]\n             }\n# instanciate classifier\nknn = KNeighborsClassifier()\nknn.fit(X_train, y_train)\nknn_cv = GridSearchCV(knn, param_grid, cv=10)\nknn_cv.fit(X_train, y_train)\ny_pred = knn_cv.predict(X_val)\nprint(\"Tuned knn Parameters: {}\".format(knn_cv.best_params_))\nprint(\"Best score is {}\".format(knn_cv.best_score_))\nacc_knn_cv = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_knn_cv)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7d8265bd2bfa6b1db4cbaf28614c623f44c94179","_cell_guid":"7e923bac-35be-45c1-b6cb-2e53fec7b86c"},"cell_type":"markdown","source":"## DecisionTree with RandomizedSearch"},{"metadata":{"_uuid":"b65ae78b1dc789edf9d9ca9f4845fd84a2f5e45a","scrolled":true,"_cell_guid":"96aa7acc-4a15-4ed9-862a-2fe9d053882b","collapsed":true,"trusted":false},"cell_type":"code","source":"# DecisionTree with RandomizedSearch\n\n# Setup the parameters and distributions to sample from: param_dist\nparam_dist = {\"random_state\" :  np.arange(0, 10),\n              \"max_depth\": np.arange(1, 10),\n              \"max_features\": np.arange(1, 10),\n              \"min_samples_leaf\": np.arange(1, 10),\n              \"criterion\": [\"gini\",\"entropy\"]}\n\n# Instantiate a Decision Tree classifier: tree\ntree = DecisionTreeClassifier()\n# Instantiate the RandomizedSearchCV object: tree_cv\ntree_cv = RandomizedSearchCV(tree, param_dist, cv=30)\n# Fit it to the data\ntree_cv.fit(X_train,y_train)\ny_pred = tree_cv.predict(X_val)\n# Print the tuned parameters and score\nprint(\"Tuned Decision Tree Parameters: {}\".format(tree_cv.best_params_))\nprint(\"Best score is {}\".format(tree_cv.best_score_))\nacc_tree_cv = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_tree_cv)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6617cd4820b68ba109c6485431bed2f3c74fc4a7","_cell_guid":"0d32116b-5292-436a-8d86-1e93a21c3c76"},"cell_type":"markdown","source":"## Random Forest"},{"metadata":{"_uuid":"57ec97e39d8fc7350e7de3fca33ad0a07077d4c5","_cell_guid":"a4fb80df-9bff-49cf-bc5c-05857954a6a9","collapsed":true,"trusted":false},"cell_type":"code","source":"# Random Forest\n\n# Setup the parameters and distributions to sample from: param_dist\nparam_dist = {\"random_state\" :  np.arange(0, 10),\n              \"n_estimators\" :  np.arange(1, 20),\n              \"max_depth\": np.arange(1, 10),\n              \"max_features\": np.arange(1, 10),\n              \"min_samples_leaf\": np.arange(1, 10),\n              \"criterion\": [\"gini\",\"entropy\"]}\n\n# Instantiate a Decision Tree classifier: tree\nrandomforest = RandomForestClassifier()\n\n# Instantiate the RandomizedSearchCV object: tree_cv\nrandomforest_cv = RandomizedSearchCV(randomforest, param_dist, cv=30)\n\n# Fit it to the data\nrandomforest_cv.fit(X_train,y_train)\ny_pred = randomforest_cv.predict(X_val)\n# Print the tuned parameters and score\nprint(\"Tuned Decision Tree Parameters: {}\".format(randomforest_cv.best_params_))\nprint(\"Best score is {}\".format(randomforest_cv.best_score_))\nacc_randomforest_cv = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_randomforest_cv)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2e3e0cebc18761c6df40a92d0b0d36aec8893539","_cell_guid":"a0d9a1a5-c205-43e8-bad4-b66dd27cea4d"},"cell_type":"markdown","source":"## Gradient Boosting"},{"metadata":{"_uuid":"880a91fc3d7019e480e47f85bc232fc6ce4b8951","scrolled":true,"_cell_guid":"2024cd8a-0e0f-4f95-9041-f8117d91283d","collapsed":true,"trusted":false},"cell_type":"code","source":"# Gradient Boosting Classifier\n\n# Setup the parameters and distributions to sample from: param_dist\nparam_dist = {'max_depth':np.arange(1, 7),\n              'min_samples_leaf': np.arange(1, 6),\n              \"max_features\": np.arange(1, 10),\n             }\n\n# Instantiate Classifier\ngbk = GradientBoostingClassifier()\n\n# Instantiate the RandomizedSearchCV object: tree_cv\ngbk_cv = RandomizedSearchCV(gbk, param_dist, cv=30)\n\ngbk_cv.fit(X_train, y_train)\ny_pred = gbk_cv.predict(X_val)\n\nprint(\"Tuned Gradient Boost Parameters: {}\".format(gbk_cv.best_params_))\nprint(\"Best score is {}\".format(gbk_cv.best_score_))\nacc_gbk_cv = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_gbk_cv)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d3033d56687960b96b3343eb58290dc6b07c6d7a","_cell_guid":"8a136cb4-f569-4fad-a55c-1dc335f4fa6f"},"cell_type":"markdown","source":"## xgboost"},{"metadata":{"_uuid":"aa52db37764db2f4e930416d57d2449b2bfa775b","_cell_guid":"c348c87a-7dba-44f3-b46b-42648405eb65","collapsed":true,"trusted":false},"cell_type":"code","source":"# xgboost\n\n# Setup the parameters and distributions to sample from: param_dist\nparam_dist = {'learning_rate': [.01, .03, .05, .1, .25], #default: .3\n            'max_depth': np.arange(1, 10), #default 2\n            'n_estimators': [10, 50, 100, 300], \n            'booster':['gbtree','gblinear','dart']\n            #'seed': 5  \n             }\n# Instantiate Classifier\nxgb = XGBClassifier()\n\n# Instantiate the RandomizedSearchCV object: tree_cv\nxgb_cv = RandomizedSearchCV(xgb, param_dist, cv=20)\n\n# Fit model\nxgb_cv.fit(X_train, y_train)\n\n# Make prediction\ny_pred = xgb_cv.predict(X_val)\n\n# Print results\nprint(\"xgBoost Parameters: {}\".format(xgb_cv.best_params_))\nprint(\"Best score is {}\".format(xgb_cv.best_score_))\nacc_xgb_cv = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_xgb_cv)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"86f584033cab06766198e953020bede5fe2759aa","_cell_guid":"fe0a6c92-ae79-4b23-9fd3-623d0605fafb"},"cell_type":"markdown","source":"## Comparing the results of the cross validated tuned models (best result)"},{"metadata":{"_uuid":"322232b262fd94c31225fa683ac76f5f706fe744","_cell_guid":"462195e0-3dad-497d-86d4-ac89eb75b462","collapsed":true,"trusted":false},"cell_type":"code","source":"optmodels = pd.DataFrame({\n    'optModel': ['SVC','KNN','Decision Tree','Gradient Boost','Logistic Regression','xgboost'],\n    'optScore': [svc_cv.best_score_,knn_cv.best_score_,tree_cv.best_score_,gbk_cv.best_score_,logreg_cv.best_score_,xgb_cv.best_score_]})\noptmodels.sort_values(by='optScore', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"376c10802e125a57ebc4aa82fb1468c075f3c9a2","_cell_guid":"65fb2739-05e5-4d1b-a756-9b4decc865a1"},"cell_type":"markdown","source":"## Comparing the results of the tuned models (accuracy)"},{"metadata":{"_uuid":"9592eaaa4a63b22f38f7bb9dcaba218af94483cc","_cell_guid":"622c2708-df4e-4c53-b14d-ab9c5020f926","collapsed":true,"trusted":false},"cell_type":"code","source":"optmodels = pd.DataFrame({\n    'optModel': ['Linear Regression','KNearestNieghbours','Decision Tree','Gradient Boost','Logistic Regression','xgboost'],\n    'optScore': [acc_svc_cv,acc_knn_cv,acc_tree_cv,acc_gbk_cv,acc_logreg_cv,acc_xgb_cv]})\noptmodels.sort_values(by='optScore', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"621625ae0008781d70e83a84d469c2a79318c552","_cell_guid":"0ccdbe16-f050-40ee-9030-c2b1e3c15c54"},"cell_type":"markdown","source":"## Plotting Learning Curves"},{"metadata":{"_uuid":"9d2b0f4df1cd56bb799b127e258a7517afc71593","_cell_guid":"e7d777a5-27b0-461a-b561-76f029687821","collapsed":true,"trusted":false},"cell_type":"code","source":"# define function to plot test and training curves\ndef plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"Generate a simple plot of the test and training learning curve\"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n\n# Cross validate model with Kfold stratified cross val\nkfold = StratifiedKFold(n_splits=10)\n# Plot chart for each model\ng = plot_learning_curve(svc_cv.best_estimator_,\"linear regression learning curves\",X_train,y_train,cv=kfold)\ng = plot_learning_curve(logreg_cv.best_estimator_,\"logistic regression learning curves\",X_train,y_train,cv=kfold)\ng = plot_learning_curve(knn_cv.best_estimator_,\"knn learning curves\",X_train,y_train,cv=kfold)\ng = plot_learning_curve(tree_cv.best_estimator_,\"decision tree learning curves\",X_train,y_train,cv=kfold)\ng = plot_learning_curve(randomforest_cv.best_estimator_,\"random forest learning curves\",X_train,y_train,cv=kfold)\ng = plot_learning_curve(gbk_cv.best_estimator_,\"gradient boosting learning curves\",X_train,y_train,cv=kfold)\ng = plot_learning_curve(xgb_cv.best_estimator_,\"xg boost learning curves\",X_train,y_train,cv=kfold)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"29b6c32d918bf9443b011934423c4264d3d67592","_cell_guid":"a6db44f8-1c7e-487a-944a-65d2295e9653"},"cell_type":"markdown","source":"# Optimising the Model\n\nAdding parameters to the basic models generally improved the performance on the training data. These gain on the training data did not always translate to the same increase in performance on the test data, due to over fitting. "},{"metadata":{"_uuid":"51abbe997d1cc5221c5752e804f01e7b29ee50af","_cell_guid":"f337ac46-3fa0-47d3-b9c5-ca96d96f8278"},"cell_type":"markdown","source":"# Predictions based on tuned model"},{"metadata":{"_uuid":"e91b37e071b2ea2ec02bc8efa727ab5831038ca1","_cell_guid":"381fa290-fb7a-4950-8d60-a94d10b4a3e2","collapsed":true,"trusted":false},"cell_type":"code","source":"# Select columns\nX_train = df_train[CATEGORY_COLUMNS].fillna(-1000)\ny_train = df_train[\"Survived\"]\nX_test = df_test[CATEGORY_COLUMNS].fillna(-1000)\n\nfrom sklearn.tree import DecisionTreeClassifier\ntest = df_test[REVISED_NUMERIC_COLUMNS].fillna(-1000)\n# select classifier\n#tree = DecisionTreeClassifier(random_state=0,max_depth=5,max_features=7,min_samples_leaf=2,criterion=\"entropy\") #85,87\n#tree = DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=4,max_features=7, max_leaf_nodes=None, min_impurity_decrease=0.0,min_impurity_split=None, min_samples_leaf=9,min_samples_split=2, min_weight_fraction_leaf=0.0,presort=False, random_state=8, splitter='best')\n#tree = DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,max_features=7, max_leaf_nodes=None, min_impurity_decrease=0.0,min_impurity_split=None, min_samples_leaf=9,min_samples_split=2, min_weight_fraction_leaf=0.0,presort=False, random_state=9, splitter='best')\n#knn = KNeighborsClassifier(algorithm='kd_tree',leaf_size=20,n_neighbors=5)\n#logreg = LogisticRegression(solver='newton-cg')\n#xgboost=XGBClassifier(n_estimators= 300, max_depth= 10, learning_rate= 0.01)\n#gbk=GradientBoostingClassifier(min_samples_leaf=1,max_features=4,max_depth=5)\n#logreg=LogisticRegression(solver='newton-cg',C= 10)\n#gboost=GradientBoostingClassifier(random_state= 7,n_estimators=17,min_samples_leaf= 4, max_features=9,max_depth=5, criterion='gini')\nrandomf=RandomForestClassifier(random_state= 7,n_estimators=17,min_samples_leaf= 4, max_features=9,max_depth=5, criterion='gini')\n\n# train model\nrandomf.fit(data_to_train, prediction)\n# make predictions\nSubmission['Survived']=randomf.predict(X_test)\n#Submission.set_index('PassengerId', inplace=True)\nSubmission.to_csv('randomforestcats01.csv',sep=',')\nprint(Submission.head(5))\nprint('File created')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cf26d67b6c7ec409679aafba18a05a9133871502","_cell_guid":"b95554ff-0b03-4ddb-a252-75a59a9814ad"},"cell_type":"markdown","source":"# Stage 5 : Hyper tuning with confusion matrix\n\nI used a grid search cross validation in the previous stages to estimate the best results, we can use a confusion matrix to find out how well this model works by penalizing incorrect predictions."},{"metadata":{"_uuid":"bc07c290335e1e47761a5ec3d935a3f21c1f4fc7","_cell_guid":"9a65233e-e4ed-45f6-a184-2b09050e90d1","collapsed":true,"trusted":false},"cell_type":"code","source":"# knn Hyper Tunning with confusion Matrix\nREVISED_NUMERIC_COLUMNS=['Pclass','Age','SibSp','Parch','Family_Survival','Alone','Title_Master', 'Title_Miss','Title_Mr', 'Title_Mrs', 'Title_Millitary','Embarked'] #84\nSIMPLE_COLUMNS=['Pclass','Age','SibSp','Parch','Family_Survival','Alone','Sex_female','Sex_male','Title_Master', 'Title_Miss','Title_Mr', 'Title_Mrs', 'Title_Millitary','Embarked'] #84\nINTERESTING_COLUMNS=['Survived','Pclass','Age','SibSp','Parch','Title','Alone','Mother','Family Size','Family_Survival','Embarked','FareBand','TicketRef']\nCATEGORY_COLUMNS=['Pclass','SibSp','Parch','Family Size','Family_Survival','Alone','Mother','Sex_female','Sex_male','AgeBand_Child',\n       'AgeBand_Young Adult', 'AgeBand_Adult', 'AgeBand_Older Adult',\n       'AgeBand_Senior','Title_Master', 'Title_Miss','Title_Mr', 'Title_Mrs', 'Title_Millitary','NameBand_1',\n       'NameBand_2', 'NameBand_3', 'NameBand_4', 'NameBand_5','Embarked','TicketRef_A', 'TicketRef_C', 'TicketRef_F', 'TicketRef_L',\n       'TicketRef_P', 'TicketRef_S', 'TicketRef_W', 'TicketRef_X','HadCabin','Free']  \n\n# create test and training data\ndata_to_train = df_train[CATEGORY_COLUMNS].fillna(-1000)\nX_test2= df_test[CATEGORY_COLUMNS].fillna(-1000)\nprediction  = df_train[\"Survived\"]\nX_train, X_test, y_train, y_test = train_test_split(data_to_train, prediction, test_size = 0.3,random_state=21, stratify=prediction)\nprint('Data Split')\n\nhyperparams = {'algorithm': ['auto'], 'weights': ['uniform', 'distance'] ,'leaf_size': list(range(1,50,5)), \n               'n_neighbors':[6,7,8,9,10,11,12,14,16,18,20,22]}\ngd=GridSearchCV(estimator = KNeighborsClassifier(), param_grid = hyperparams, verbose=True, cv=10, scoring = \"roc_auc\")\ngd.fit(X_train, y_train)\n\ngd.best_estimator_.fit(X_train,y_train)\ny_pred=gd.best_estimator_.predict(X_test)\nSubmission['Survived']=gd.best_estimator_.predict(X_test2)\n\n# Print the results\nprint('Best Score')\nprint(gd.best_score_)\nprint('Best Estimator')\nprint(gd.best_estimator_)\nacc_gd_cv = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint('Accuracy')\nprint(acc_gd_cv)\n\n# Generate the confusion matrix and classification report\nprint('Confusion Matrrix')\nprint(confusion_matrix(y_test, y_pred))\nprint('Classification_report')\nprint(classification_report(y_test, y_pred))\n#Submission.set_index('PassengerId', inplace=True)\nprint('Sample Prediction')\nprint(Submission.head(10))\n#Submission.to_csv('knngridsearch03.csv',sep=',')\nprint('KNN prediction created')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"908b81f5bccc4e6933b4dffa6616778ff2c61d1c","scrolled":false,"_cell_guid":"32d6b403-78f0-4a09-923b-0287b5bdda86","collapsed":true,"trusted":false},"cell_type":"code","source":"# Decision Tree Hyper Tunning with confusion Matrix\n\nREVISED_NUMERIC_COLUMNS=['Pclass','Age','SibSp','Parch','Family_Survival','Alone','Title_Master', 'Title_Miss','Title_Mr', 'Title_Mrs', 'Title_Millitary','Embarked'] #84\nSIMPLE_COLUMNS=['Pclass','Age','SibSp','Parch','Family_Survival','Alone','Sex_female','Sex_male','Title_Master', 'Title_Miss','Title_Mr', 'Title_Mrs', 'Title_Millitary','Embarked'] #84\nINTERESTING_COLUMNS=['Survived','Pclass','Age','SibSp','Parch','Title','Alone','Mother','Family Size','Family_Survival','Embarked','FareBand','TicketRef']\nCATEGORY_COLUMNS=['Pclass','SibSp','Parch','Family Size','Family_Survival','Alone','Mother','Sex_female','Sex_male','AgeBand_Child',\n       'AgeBand_Young Adult', 'AgeBand_Adult', 'AgeBand_Older Adult',\n       'AgeBand_Senior','Title_Master', 'Title_Miss','Title_Mr', 'Title_Mrs', 'Title_Millitary','NameBand_1',\n       'NameBand_2', 'NameBand_3', 'NameBand_4', 'NameBand_5','Embarked','TicketRef_A', 'TicketRef_C', 'TicketRef_F', 'TicketRef_L',\n       'TicketRef_P', 'TicketRef_S', 'TicketRef_W', 'TicketRef_X','HadCabin','Free']  \n\n# create test and training data\ndata_to_train = df_train[CATEGORY_COLUMNS].fillna(-1000)\nX_test2= df_test[CATEGORY_COLUMNS].fillna(-1000)\nprediction  = df_train[\"Survived\"]\nX_train, X_test, y_train, y_test = train_test_split(data_to_train, prediction, test_size = 0.3,random_state=21, stratify=prediction)\nprint('Data Split')\n\nhyperparams = {\"random_state\" :  np.arange(0, 10),\n              \"max_depth\": np.arange(1, 10),\n              \"max_features\": np.arange(1, 10),\n              \"min_samples_leaf\": np.arange(1, 10),\n              \"criterion\": [\"gini\",\"entropy\"]}\n\ngd=GridSearchCV(estimator = DecisionTreeClassifier(), param_grid = hyperparams, verbose=True, cv=10, scoring = \"roc_auc\")\ngd.fit(X_train, y_train)\n\ngd.best_estimator_.fit(X_train,y_train)\ny_pred=gd.best_estimator_.predict(X_test)\nSubmission['Survived']=gd.best_estimator_.predict(X_test2)\n\n# Print the results\nprint('Best Score')\nprint(gd.best_score_)\nprint('Best Estimator')\nprint(gd.best_estimator_)\nacc_gd_cv = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint('Accuracy')\nprint(acc_gd_cv)\n\n# Generate the confusion matrix and classification report\nprint('Confusion Matrrix')\nprint(confusion_matrix(y_test, y_pred))\nprint('Classification_report')\nprint(classification_report(y_test, y_pred))\n#Submission.set_index('PassengerId', inplace=True)\n# print head\nprint(Submission.head(10))\nSubmission.to_csv('Treegridsearch03.csv',sep=',')\nprint('Decision Tree prediction created')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"86260a6164a5e5a54556e688313a9ae5e418e941","_cell_guid":"ef898332-3cb8-4893-9536-68a7e98d2391","collapsed":true,"trusted":false},"cell_type":"code","source":"# Decision Logistic Regression Hyper Tunning with confusion Matrix\n\nREVISED_NUMERIC_COLUMNS=['Pclass','Age','SibSp','Parch','Family_Survival','Alone','Title_Master', 'Title_Miss','Title_Mr', 'Title_Mrs', 'Title_Millitary','Embarked'] \nSIMPLE_COLUMNS=['Pclass','Age','SibSp','Parch','Family_Survival','Alone','Sex_female','Sex_male','Title_Master', 'Title_Miss','Title_Mr', 'Title_Mrs', 'Title_Millitary','Embarked']\nINTERESTING_COLUMNS=['Survived','Pclass','Age','SibSp','Parch','Title','Alone','Mother','Family Size','Family_Survival','Embarked','FareBand','TicketRef']\nCATEGORY_COLUMNS=['Pclass','SibSp','Parch','Family Size','Family_Survival','Alone','Mother','Sex_female','Sex_male','AgeBand_Child',\n       'AgeBand_Young Adult', 'AgeBand_Adult', 'AgeBand_Older Adult',\n       'AgeBand_Senior','Title_Master', 'Title_Miss','Title_Mr', 'Title_Mrs', 'Title_Millitary','NameBand_1',\n       'NameBand_2', 'NameBand_3', 'NameBand_4', 'NameBand_5','Embarked','TicketRef_A', 'TicketRef_C', 'TicketRef_F', 'TicketRef_L',\n       'TicketRef_P', 'TicketRef_S', 'TicketRef_W', 'TicketRef_X','HadCabin','Free']  \n\n# create test and training data\ndata_to_train = df_train[CATEGORY_COLUMNS].fillna(-1000)\nX_test2= df_test[CATEGORY_COLUMNS].fillna(-1000)\nprediction  = df_train[\"Survived\"]\nX_train, X_test, y_train, y_test = train_test_split(data_to_train, prediction, test_size = 0.3,random_state=21, stratify=prediction)\nprint('Data Split')\n\nhyperparams = {'solver':[\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\", \"saga\"],\n              'C': [0.01, 0.1, 1, 10, 100]}\n\ngd=GridSearchCV(estimator = LogisticRegression(), param_grid = hyperparams, verbose=True, cv=10, scoring = \"roc_auc\")\ngd.fit(X_train, y_train)\n\ngd.best_estimator_.fit(X_train,y_train)\ny_pred=gd.best_estimator_.predict(X_test)\nSubmission['Survived']=gd.best_estimator_.predict(X_test2)\n\n# Print the results\nprint('Best Score')\nprint(gd.best_score_)\nprint('Best Estimator')\nprint(gd.best_estimator_)\nacc_gd_cv = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint('Accuracy')\nprint(acc_gd_cv)\n\n# Generate the confusion matrix and classification report\nprint('Confusion Matrrix')\nprint(confusion_matrix(y_test, y_pred))\nprint('Classification_report')\nprint(classification_report(y_test, y_pred))\n#Submission.set_index('PassengerId', inplace=True)\n# print head\nprint(Submission.head(10))\nSubmission.to_csv('Logregwithconfusion01.csv',sep=',')\nprint('Logistic Regression prediction created')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"26230fcaafdebe0652a0581a3169879b81cb1824","_cell_guid":"766ec832-095b-413b-8642-f8cf4a062de1","collapsed":true,"trusted":false},"cell_type":"code","source":"df_train.columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0c46ecd97bce6919c5e6a0f53d1259ec09785385","_cell_guid":"f9c64c91-ca1e-4d25-b082-f69ff912a759","collapsed":true,"trusted":false},"cell_type":"code","source":"# Decision Logistic Regression Hyper Tunning with confusion Matrix\n\n# create test and training data\nX_train = df_train[CATEGORY_COLUMNS].fillna(-1000)\ny_train = df_train[\"Survived\"]\nX_test = df_test[CATEGORY_COLUMNS].fillna(-1000)\n\nrandomf=RandomForestClassifier(criterion='gini', n_estimators=700, min_samples_split=10,min_samples_leaf=1,max_features='auto',oob_score=True,random_state=1,n_jobs=-1)\nrandomf.fit(X_train, y_train)\nSubmission['Survived']=randomf.predict(X_test)\n\n# Print the results\nacc_gd_cv = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint('Accuracy')\nprint(acc_gd_cv)\n#Submission.set_index('PassengerId', inplace=True)\n# print head\nprint(Submission.head(10))\nSubmission.to_csv('finalrandomforest01.csv',sep=',')\nprint('Random Forest prediction created')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"30871fcf5501d9a3205dc43b96a8f33175223903","_cell_guid":"f5ede940-61f5-4bfb-bf66-f3c3d7484783"},"cell_type":"markdown","source":"## Plot Area under ROC"},{"metadata":{"_uuid":"fe08ebd763af662186ca98765361b27e43e106a7","_cell_guid":"ae1b23bc-42dc-470d-b466-df01b934d419","collapsed":true,"trusted":false},"cell_type":"code","source":"# List of Machine Learning Algorithm (MLA)\nMLA = [\n    #Ensemble Methods\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n    \n    #GLM\n    linear_model.LogisticRegressionCV(),\n    \n    #Nearest Neighbor\n    neighbors.KNeighborsClassifier(),\n    \n    #SVM\n    svm.SVC(probability=True),\n    \n    #Trees    \n    #tree.DecisionTreeClassifier(),\n    #tree.ExtraTreeClassifier(), \n    ]\n\nindex = 1\nfor alg in MLA:\n    predicted = alg.fit(X_train, y_train).predict(X_test)\n    fp, tp, th = roc_curve(y_test, predicted)\n    roc_auc_mla = auc(fp, tp)\n    MLA_name = alg.__class__.__name__\n    plt.plot(fp, tp, lw=2, alpha=0.3, label='ROC %s (AUC = %0.2f)'  % (MLA_name, roc_auc_mla))\n    index+=1\n\nplt.title('ROC Curve comparison')\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([0,1])\nplt.ylim([0,1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"28a3aebb2b2b075a0fa0ed4dd6c2b987c72f3794","_cell_guid":"664eb1bd-bd71-41d8-b89e-66625c19adf7"},"cell_type":"markdown","source":"# Stage 6 : Basic Ensemble Modelling\n\nIn the last couple of stages I tried a few different models with  differnet parameters to try and find the one that produced the best results. Another approach would be to use an Ensemble model, that generates results from a selection of the best performing models and then feeds the results into a another model in a second layer.  "},{"metadata":{"_uuid":"397fd8c5fa1b2bed8a8cc39ff18740caac01d0e8","_cell_guid":"6b61eea3-4223-44e6-8e44-3651cf2c9c75","collapsed":true,"trusted":false},"cell_type":"code","source":"REVISED_NUMERIC_COLUMNS=['Pclass','Age','SibSp','Parch','Family_Survival','Alone','Title_Master', 'Title_Miss','Title_Mr', 'Title_Mrs', 'Title_Millitary','Embarked'] \nSIMPLE_COLUMNS=['Pclass','Age','SibSp','Parch','Family_Survival','Alone','Sex_female','Sex_male','Title_Master', 'Title_Miss','Title_Mr', 'Title_Mrs', 'Title_Millitary','Embarked']\nINTERESTING_COLUMNS=['Survived','Pclass','Age','SibSp','Parch','Title','Alone','Mother','Family Size','Family_Survival','Embarked','FareBand','TicketRef']\nCATEGORY_COLUMNS=['Pclass','SibSp','Parch','Family Size','Family_Survival','Alone','Mother','Sex_female','Sex_male','AgeBand_Child',\n       'AgeBand_Young Adult', 'AgeBand_Adult', 'AgeBand_Older Adult',\n       'AgeBand_Senior','Title_Master', 'Title_Miss','Title_Mr', 'Title_Mrs', 'Title_Millitary','NameBand_1',\n       'NameBand_2', 'NameBand_3', 'NameBand_4', 'NameBand_5','Embarked','TicketRef_A', 'TicketRef_C', 'TicketRef_F', 'TicketRef_L',\n       'TicketRef_P', 'TicketRef_S', 'TicketRef_W', 'TicketRef_X','HadCabin','Free']  \n\n# create test and training data\ndata_to_train = df_train[REVISED_NUMERIC_COLUMNS].fillna(-1000)\ndata_to_test = df_test[REVISED_NUMERIC_COLUMNS].fillna(-1000)\nprediction = df_train[\"Survived\"]\nX_train, X_val, y_train, y_val = train_test_split(data_to_train, prediction, test_size = 0.3,random_state=21, stratify=prediction)\nprint('Data Split')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f1c8ddcc4661a1f70fa07316e5e03ffc4e240ab2","_cell_guid":"4f86d505-5400-4bfb-ad8a-d63474ea773b"},"cell_type":"markdown","source":"## Train first layer "},{"metadata":{"_uuid":"1ce24cf6fcb4d36fa33b731178b4adc7e38d4db3","_cell_guid":"cbc46591-6855-4991-9287-9981d25eb0aa","collapsed":true,"trusted":false},"cell_type":"code","source":"#logreg = LogisticRegression()\nlogreg = LogisticRegression(C=10, solver='newton-cg')\nlogreg.fit(X_train, y_train)\ny_pred_train_logreg = cross_val_predict(logreg,X_val, y_val)\ny_pred_test_logreg = logreg.predict(X_test)\nprint('logreg first layer predicted')\n\n#tree = DecisionTreeClassifier()\ntree = DecisionTreeClassifier(random_state=8,min_samples_leaf=6, max_features= 7, max_depth= 4, criterion='gini', splitter='best')\ntree.fit(X_train, y_train)\ny_pred_train_tree = cross_val_predict(tree,X_val,y_val)\ny_pred_test_tree = tree.predict(X_test)\nprint('decision tree first layer predicted')\n\n# randomforest = RandomForestClassifier()\nrandomforest = RandomForestClassifier(random_state=8, n_estimators=15, min_samples_leaf= 4, max_features= 6, max_depth=4,criterion='gini')\nrandomforest.fit(X_train, y_train)\ny_pred_train_randomforest = cross_val_predict(randomforest, X_val, y_val)\ny_pred_test_randomforest = randomforest.predict(X_test)\nprint('random forest first layer predicted')\n\n#gbk\ngbk = GradientBoostingClassifier(min_samples_leaf=3, max_features= 3, max_depth= 3)\ngbk.fit(X_train, y_train)\ny_pred_train_gbk = cross_val_predict(gbk, X_val, y_val)\ny_pred_test_gbk = gbk.predict(X_test)\nprint('gbk first layer predicted')\n\n#knn\nknn = KNeighborsClassifier(algorithm='auto', leaf_size=36, metric='minkowski',metric_params=None, n_jobs=1, n_neighbors=12, p=2,weights='uniform')\nknn.fit(X_train, y_train)\ny_pred_train_knn = cross_val_predict(knn, X_val, y_val)\ny_pred_test_knn = gbk.predict(X_test)\nprint('knn first layer predicted')\n\n#clf = SVC()\nclf = SVC(C=3, degree=1, kernel='linear', max_iter=1, shrinking=0)\nclf.fit(X_train, y_train)\ny_pred_train_clf = cross_val_predict(clf, X_val, y_val)\ny_pred_test_clf = clf.predict(X_test)\nprint('clf first layer predicted')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b97ca5a46057877b60a8f3599c0b67f140498a8c","_cell_guid":"baa694ed-3ebb-4d25-a8a7-899dbf6012e1"},"cell_type":"markdown","source":"## VotingClassifier Ensemble"},{"metadata":{"_uuid":"17fe18de4287122933075303fee668205a27729c","_cell_guid":"9ec35d27-55de-4c11-9b13-b24a6494dc24","collapsed":true,"trusted":false},"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\n\nvotingC = VotingClassifier(estimators=[('logreg', logreg_cv.best_estimator_), ('gbk', gbk_cv.best_estimator_),\n('tree', tree_cv.best_estimator_), ('randomforest',randomforest_cv.best_estimator_),('knn',knn_cv.best_estimator_) ], voting='soft', n_jobs=4)\n\nvotingC = votingC.fit(X_train, y_train)\n\n# write data frame to csv file\nSubmission['Survived'] = votingC.predict(X_test)\n# Submission.set_index('PassengerId', inplace=True)\nSubmission.to_csv('Votingclassifier02.csv',sep=',')\nprint('Voting Classifier Ensemble File created')\nprint(Submission.head())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c4b73f33042736506e78e876c327e0e3b00610a7","_cell_guid":"55a0929c-d6c6-4c53-84bf-02aef849ed45"},"cell_type":"markdown","source":"# Stage 7 : Hyper Tuned Ensemble Modelling"},{"metadata":{"_uuid":"c8a2b6adba0cbc0c9bc11c99799523eb2c739e4e","_cell_guid":"dfcb96e2-ce47-47df-8199-0a8d5c85c1d8","collapsed":true,"trusted":false},"cell_type":"code","source":"# Create Ensemble Model baseline (tuned model!)\nsecond_layer_train = pd.DataFrame( {'Logistic Regression': y_pred_train_logreg.ravel(),\n                                    'Gradient Boosting': y_pred_train_gbk.ravel(),\n                                    'Decision Tree': y_pred_train_tree.ravel(),\n                                    'Random Forest': y_pred_train_randomforest.ravel()\n                                    } )\n\nX_train_second = np.concatenate(( y_pred_train_logreg.reshape(-1, 1), y_pred_train_gbk.reshape(-1, 1), \n                                  y_pred_train_tree.reshape(-1, 1), y_pred_train_randomforest.reshape(-1, 1)),\n                                  axis=1)\nX_test_second = np.concatenate(( y_pred_test_logreg.reshape(-1, 1), y_pred_test_gbk.reshape(-1, 1), \n                                 y_pred_test_tree.reshape(-1, 1), y_pred_test_randomforest.reshape(-1, 1)),\n                                 axis=1)\n#xgb = XGBClassifier(n_estimators= 800,max_depth= 4,min_child_weight= 2,gamma=0.9,subsample=0.8,colsample_bytree=0.8,objective= 'binary:logistic',nthread= -1,scale_pos_weight=1).fit(X_train_second, y_val)\ntree = DecisionTreeClassifier(random_state=8,min_samples_leaf=6, max_depth= 4, criterion='gini').fit(X_train_second,y_val)\n\nSubmission['Survived'] = tree.predict(X_test_second)\nprint(Submission.head())\nprint('Tuned Ensemble model prediction complete')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d94bdb874ae16aaff9a0f2111754b57a79eb8afd","_cell_guid":"d9f64360-c860-4974-a1f7-18e1d754180c","collapsed":true,"trusted":false},"cell_type":"code","source":"# write data frame to csv file\n#Submission.set_index('PassengerId', inplace=True)\nSubmission.to_csv('tunedensemblesubmission04.csv',sep=',')\nprint('tuned Ensemble File created')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ddbacdcb188b189db5cb581c7cd748bb8bf144b5","_cell_guid":"c0162b2e-f110-4d6a-aeea-6493f5e5a907"},"cell_type":"markdown","source":"# Summary\n\nIn this project we have explored the Titanic Data Set, we have identified missing data and filled then as best we could, we have converted categorical data to columns of numeric features that we can use in machine learning and we have engineered new features based on the data we had. We improved our score from base line of 0.57894 to  a score of 0.78.\n\nGoing from a score of 0.57 to 0.77 was the relatively easy part, taking it from 7.8 to 0.8 is a whole different ball game. Its really temping to overwork the data trying to find new features that might improve the score but in really what you gain in new features you loose in the noise you've introduce, its also tempting to keep tweak the parameters of your model to get the best possible score on the test data, but gain what you gain in performance on the training data you loose in overfitting. A better approach is to stick to the features that have the strongest relationships and ensure that any data that you are estimating or engineering is as accurate as you can possibly make it. Using cross validation to hyper tune the model while minimising any over fitting of the data.\n\nWhen I initially created the project I kept the test and training data completely separate but am I am rapidly coming to the conclusion that combining the two datasets,  is possibly a better approach for estimating missing data based on averages across the entire dataset. \n\nI  looked at a range of different models and compared the accuracy of each model on the training data before deciding which model to use for the third submission. I then hyper tuned  a hanful of the best performing to ensure that I submitted the best performing hyper tuned model. \n\nHaving hypertuned a single model the next step in my process was to attempt combining several models in an ensemble. I managed to achieve a result of .803 which was OK but not as good as the best hypertuned models that i'd produced.\n\nI havn't come any where near winning this contest yet, but I survived my first Kaggle contest and got a score of over .8 which has my goal. The main thing is that I had fun and learnt a lot along the way by trying different techniques and looking at what other people were doing.\n\nI've also created a kernal that uses the same data with deep learning, you can find this at https://www.kaggle.com/davidcoxon/deeply-titanic"},{"metadata":{"_uuid":"2cbd72ea1657a90ba05e837969cac1f4d8530bb0","_cell_guid":"ee85a578-c963-42d8-ac8f-0097737527ab"},"cell_type":"markdown","source":"# Credit where credits due\n\nThis competition is predominantly a training exercise and as such I have tried to looks at different approaches and try different techniques to see hw they work.  I have looked at some of the existing entries and adopted some of the tequiques that i have found interesting. So firstly a huge thanks to everyone that look the time to document their code and explain step by step what they did and why.\n\nTo naming names, some of the notebooks that i found most useful and think deserve special mensions are:\n\n### Aldemuro M.A.Haris\nhttps://www.kaggle.com/aldemuro/comparing-ml-algorithms-train-accuracy-90\nInteresting model comparison and ROC graphs\n\n\n### Anisotropic\nhttps://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python/notebook\n\nIntroduction to Ensembling/Stacking in Python is a very useful project on many levels, in I particular I liked how elegantly this code was written.\n\n### Bisaria\nhttps://www.kaggle.com/bisaria/titanic-lasso-ridge-implementation/code\n\nWhile this notebook is based on R and I am working in Python, I found some of the visualizations interesting, specifically the port of embarkation and number of siblings and the mosaic. I also liked the idea of the lone traveller feature and the allocation of the cabin data, based on family.\n\n### CalebCastleberry\nhttps://www.kaggle.com/ccastleberry/titanic-cabin-features\n\nThis notebook explains the importance of the deck feature and proves you can score 70% on the deck feature alone.\n\n### Henrique Mello \nhttps://www.kaggle.com/hrmello/introduction-to-data-exploration-using-seaborn/notebook\n\nThis has some great visualisations of the data and helped me understand the importance of using title in predicting ages when filling in the missing data. \n\n### Konstantin\nhttps://www.kaggle.com/konstantinmasich/titanic-0-82-0-83\n\n### LD Freeman\nhttps://www.kaggle.com/ldfreeman3/a-data-science-framework-to-achieve-99-accuracy\n\nThis not only achieves a fantastic score but is a great tutorial on data science techniques\n\n### Nadin Tamer\nhttps://www.kaggle.com/nadintamer/titanic-survival-predictions-beginner/notebook\n\nI found this another really useful kernel. It is very much a step by step approach, with a particularly good section on different types of model and how they perform for this project.\n\n### Omar El Gabry\nhttps://www.kaggle.com/omarelgabry/a-journey-through-titanic?scriptVersionId=447802/notebook\n\nThis kernal has an interesting section on estimating the missing ages and calculating pearson co-efficients for the features.\n\n### Oscar Takeshita\nhttps://www.kaggle.com/pliptor/divide-and-conquer-0-82296/code\n\nThis kernal was very useful in trying to get over the 0.8 ceiling, its based on R rather than python so i haven't used any of the code, but it helped me focus on the key fearures and to see the benefits of uing the combined training and test dataset for statistics and calculations rather keeping the two at arms length. \n\n### Sina\nhttps://www.kaggle.com/sinakhorami/titanic-best-working-classifier?scriptVersionId=566580\n\nA lot of high scoring kernals reference this notebook, especially the feature engineering discussed in it.\n\n### S.Xu\nhttps://www.kaggle.com/shunjiangxu/blood-is-thicker-than-water-friendship-forever\n\nThis kernal is based on an original kernal by Sina, and it uses the last name and ticket details to find families and firends it then looks at the survival of the group as a whole.\n\n### Yassine Ghouzam\nhttps://www.kaggle.com/yassineghouzam/titanic-top-4-with-ensemble-modeling\n\nThis kernal has an interesting section on learning curves."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}