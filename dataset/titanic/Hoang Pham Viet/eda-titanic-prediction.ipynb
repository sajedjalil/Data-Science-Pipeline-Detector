{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Titanic ensemble modeling classifier\n\n30/7/2021\n* [**1. Introduction**](#1)\n  * [1.1 Import library](#1.1)\n  * [1.2 Loading data](#1.2)\n* [**2. Getting familiar with data (Data analysis)**](#2) \n  * [2.1 Missing value](#2.1)\n     * [2.1.1 Age](#2.1.1)\n     * [2.1.2 Embarked](#2.1.2)\n     * [2.1.3 Fare](#2.1.3)\n     * [2.1.4 Cabin](#2.1.4)\n  * [2.2 Target column distribution](#2.2)\n  * [2.3 Correlations](#2.3)\n* [**3. Feature engineer**](#3)\n  * [3.1 Binning Coutinuous features](#3.1)\n     * [3.1.1 Fare](#3.1.1)\n     * [3.1.2 Age](#3.1.2)\n  * [3.2 Frequency Encoded Binning](#3.2)\n     * [3.2.1 Family size](#3.2.1)\n     * [3.2.2 Ticket frequency](#3.2.2)\n  * [3.3 Title & is Married](#3.3)\n  * [3.4 Feature transformation](#3.4)\n     * [3.4.1 Label encoding non-numerical features](#3.4.1)\n     * [3.4.2 One-hot encoding the categorical features](#3.4.2)\n* [**4. Modeling**](#4)\n  * [4.1 Cross validation simple models](#4.1)\n  * [4.2 Hyper parameters tunning](#4.2)\n  * [4.3 Confusion matrix](#4.3)\n  * [4.4 Ensemble model](#4.4)\n  * [4.5 Submission](#4.5)","metadata":{}},{"cell_type":"markdown","source":"<a name='1'></a>\n# 1. Introduction\nThis project is on the titanic which was on 14 April 1912. Before it was colliding with an ice berg and ended up at the bottom of the Alantic sea. She was the biggest moveable object at the time. She was call \"unsinkable\". Which was not true. Over a thousand people died in the icy sea that night\n\n<img src=\"https://media.istockphoto.com/photos/titanic-and-iceberg-picture-id503132519?k=6&m=503132519&s=612x612&w=0&h=bipCwlzdAUIzTdYdZ6g-CyYWaKJ988T85veDl_3l1Gk=\" style=\"width:600px;height:400px;\">\n\n*By the end of this notebook, we'll go through*\n* **Data analysis** \n    * (Missing value, data visualization, correlation table, categorical features)\n* **Feature engineering** \n    * (Binning continuous features, Frequency encoded binning, Feature transformation)\n* **Modeling** \n    * (Cross validation technique, Hyper parameter tunning, Confusion matrix)","metadata":{}},{"cell_type":"markdown","source":"<a name='1.1'></a>\n## 1.1 Import library","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport missingno as msno\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\nfrom sklearn.model_selection import StratifiedKFold, GridSearchCV\nSEED = 42\n\n#Common Model Algorithms\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\n\nimport warnings\nimport time\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2021-08-03T02:14:45.639294Z","iopub.execute_input":"2021-08-03T02:14:45.639951Z","iopub.status.idle":"2021-08-03T02:14:47.086973Z","shell.execute_reply.started":"2021-08-03T02:14:45.639856Z","shell.execute_reply":"2021-08-03T02:14:47.086136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name='1.2'></a>\n## 1.2 Loading data\n- Training set has 891 rows, testing set has 418 rows\n- There are 12 features in training set and 11 features in testing set\n- One extra feature in training set is \"Survived\" which is the target feature","metadata":{}},{"cell_type":"code","source":"def concat_df(train_data, test_data):\n    # Returns a concatenated df of training and test set\n    return pd.concat([train_data, test_data], sort=True).reset_index(drop=True)\n\ndef divide_df(all_data):\n    # Returns divided dfs of training and test set\n    return all_data.loc[:890], all_data.loc[891:].drop(['Survived'], axis=1)\n\ndf_train = pd.read_csv('../input/titanic/train.csv')\ndf_test = pd.read_csv('../input/titanic/test.csv')\ndf_all = concat_df(df_train, df_test)\n\ndf_train.name = 'Training Set'\ndf_test.name = 'Test Set'\ndf_all.name = 'All Set' \n\ndfs = [df_train, df_test]\n\nprint(f'Number of Training Examples = {df_train.shape[0]}')\nprint(f'Number of Test Examples = {df_test.shape[0]}\\n')\nprint(f'Training X Shape = {df_train.shape}')\nprint(f'Training y Shape = {df_train[\"Survived\"].shape[0]}\\n')\nprint(f'Test X Shape = {df_test.shape}')\nprint(f'Test y Shape = {df_test.shape[0]}\\n')\nprint(df_train.columns)\nprint(df_test.columns)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:47:05.967194Z","iopub.execute_input":"2021-08-02T13:47:05.967576Z","iopub.status.idle":"2021-08-02T13:47:06.016978Z","shell.execute_reply.started":"2021-08-02T13:47:05.967547Z","shell.execute_reply":"2021-08-02T13:47:06.016234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name='2'></a>\n# 2. Getting familiar with data\n- **PassengerId** is the unique id of the row and it doesn't have any effect on target\n- **Survived** is the target variable we are trying to predict (*0=Not Survived* or *1=Survived*)\n- **Pclass** (Passenger class) is the socio-economic status of the passenger and it is a categorical ordinal feature which has 3 unique values (*1=Upper Class*, *2=Middle Class* or *3=Lower Class*)\n- **Name**, **Sex** and **Age** are self-explantory\n- **SipSp** the total number of passenger's siblings and spouses\n- **Parch** the total number of passenger's parent and children\n- **Ticket** the ticket number of passenger\n- **Fare** Passenger's fare\n- **Cabin** is the cabin number of passenger\n- **Embarked** is port of embarkation and it is a categorical feature which has 3 unique values (*C=Cherbourg*, *Q=Queenstown* or *S=Southampton*)","metadata":{}},{"cell_type":"code","source":"print(df_train.info())\ndf_train.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:47:06.018335Z","iopub.execute_input":"2021-08-02T13:47:06.018723Z","iopub.status.idle":"2021-08-02T13:47:06.058534Z","shell.execute_reply.started":"2021-08-02T13:47:06.018694Z","shell.execute_reply":"2021-08-02T13:47:06.057825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name='2.1'></a>\n## 2.1 Missing value & Visualization\n(Which columns contain missing value?)\n<br>As we seen below, the *display_missing()* function count number of missing value in each feature of both training and testing dataset\n- Columns in training set contained missing value: **Age**, **Cabin**, **Embarked**\n- Columns in testing set contained missing value: **Age**, **Fare**, **Cabin**\n<br>\n\n**Remark**\n- It's more convinient to concatunate training and testing set when dealing with features containing missing value.\n- The missing value of \"Age\", \"Embarked\" and \"fare\" is smaller compared to total samples, but roughly 80% of \"Cabin\" value is missing\n- The missing value of \"Age\", \"Embarked\" and \"fare\" can be filled with descriptive statistic (median or mean) but this shouldn't be applied for \"Cabin\" feature","metadata":{}},{"cell_type":"code","source":"msno.matrix(df_train)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:47:06.059752Z","iopub.execute_input":"2021-08-02T13:47:06.060178Z","iopub.status.idle":"2021-08-02T13:47:06.58401Z","shell.execute_reply.started":"2021-08-02T13:47:06.060147Z","shell.execute_reply":"2021-08-02T13:47:06.583312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for df in dfs:\n    print(f'Missing values in {df.name}')\n    print(df.isnull().sum(), \"\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:47:06.58495Z","iopub.execute_input":"2021-08-02T13:47:06.585336Z","iopub.status.idle":"2021-08-02T13:47:06.595025Z","shell.execute_reply.started":"2021-08-02T13:47:06.585308Z","shell.execute_reply":"2021-08-02T13:47:06.594265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name='2.1.1'></a>\n### 2.1.1 Age\n- Use correlation table to find the most appropriate features to groupped by for filling the missing values","metadata":{}},{"cell_type":"code","source":"# Correlation table between features\ndf_all_corr = df_all.corr().abs().unstack().sort_values(kind=\"mergesort\", ascending=False).reset_index()\ndf_all_corr.rename(columns={0:'Correlation'}, inplace=True)\ndf_all_corr.loc[df_all_corr['level_0'] == 'Age']","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:47:06.596172Z","iopub.execute_input":"2021-08-02T13:47:06.596576Z","iopub.status.idle":"2021-08-02T13:47:06.618737Z","shell.execute_reply.started":"2021-08-02T13:47:06.596548Z","shell.execute_reply":"2021-08-02T13:47:06.618086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- **Sex** level can be considered as the second level of groupby while filling the missing value of **Age**\n- By looking at the table, the higher the level of Pclass, the higher the median of Age's values & the Age's value of males tends to be higher than those of females","metadata":{}},{"cell_type":"code","source":"# Fill Age's null val by the median grouped by \"Sex\" and \"Pclass\" features\ndf_all['Age'] = df_all.groupby(['Pclass'])['Age'].apply(lambda x: x.fillna(x.median()))","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:47:06.619707Z","iopub.execute_input":"2021-08-02T13:47:06.620094Z","iopub.status.idle":"2021-08-02T13:47:06.629679Z","shell.execute_reply.started":"2021-08-02T13:47:06.620065Z","shell.execute_reply":"2021-08-02T13:47:06.628877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train, test = divide_df(df_all)\n\ng = sns.FacetGrid(train, hue=\"Survived\", aspect=2, height=5)\ng.map(sns.kdeplot, \"Age\", alpha=.7, shade=True).add_legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:47:06.632203Z","iopub.execute_input":"2021-08-02T13:47:06.632683Z","iopub.status.idle":"2021-08-02T13:47:07.271896Z","shell.execute_reply.started":"2021-08-02T13:47:06.632639Z","shell.execute_reply":"2021-08-02T13:47:07.270736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name='2.1.2'></a>\n### 2.1.2 Embarked","metadata":{}},{"cell_type":"code","source":"# Missing value in \"Embarked\" column\ndf_all.loc[df_all['Embarked'].isnull()]","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:47:07.273714Z","iopub.execute_input":"2021-08-02T13:47:07.274139Z","iopub.status.idle":"2021-08-02T13:47:07.294549Z","shell.execute_reply.started":"2021-08-02T13:47:07.274092Z","shell.execute_reply":"2021-08-02T13:47:07.293585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we see, both passengers are **female**, with **Pclass 1** & paid **\\$80** for a fare. Let's see how fare is distributed Embarked and Pclass features","metadata":{}},{"cell_type":"code","source":"train, test = divide_df(df_all)\n\nfig, axs = plt.subplots(nrows=1, ncols=2, figsize=(15, 10))\nsns.boxplot(x='Embarked', y='Fare', hue='Pclass', data=train, ax=axs[0])\nsns.boxplot(x='Embarked', y='Fare', hue='Pclass', data=test, ax=axs[1])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:47:07.296147Z","iopub.execute_input":"2021-08-02T13:47:07.296536Z","iopub.status.idle":"2021-08-02T13:47:08.049107Z","shell.execute_reply.started":"2021-08-02T13:47:07.296504Z","shell.execute_reply":"2021-08-02T13:47:08.04801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, 1st class is blue box. The mean of money paid for a fare, which is closed to \\$80, belong to Embarked C in both training and testing dataset --> The missing piece is \"C\"","metadata":{}},{"cell_type":"code","source":"# Filling Embarked's missing value with S\ndf_all['Embarked'] = df_all['Embarked'].fillna(\"C\")","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:47:08.050437Z","iopub.execute_input":"2021-08-02T13:47:08.050715Z","iopub.status.idle":"2021-08-02T13:47:08.055996Z","shell.execute_reply.started":"2021-08-02T13:47:08.050688Z","shell.execute_reply":"2021-08-02T13:47:08.054982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name='2.1.3'></a>\n### 2.1.3 Fare\n- Fare is the cost of ticket. There is only 1 missing value in the dataset\n\n--> Just use median of Fare to fill in the missing piece","metadata":{}},{"cell_type":"code","source":"# Missing value in \"Fare\" column\ndf_all.loc[df_all['Fare'].isnull()]","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:47:08.05727Z","iopub.execute_input":"2021-08-02T13:47:08.05759Z","iopub.status.idle":"2021-08-02T13:47:08.081761Z","shell.execute_reply.started":"2021-08-02T13:47:08.057563Z","shell.execute_reply":"2021-08-02T13:47:08.080703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"med_fare = df_all.Fare.median()\ndf_all['Fare'] = df_all['Fare'].fillna(med_fare)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:47:08.082966Z","iopub.execute_input":"2021-08-02T13:47:08.083339Z","iopub.status.idle":"2021-08-02T13:47:08.092371Z","shell.execute_reply.started":"2021-08-02T13:47:08.08331Z","shell.execute_reply":"2021-08-02T13:47:08.091247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train, df_test = divide_df(df_all)\n\nplt.figure(figsize=(12, 8))\ng = sns.FacetGrid(df_train, hue=\"Survived\", aspect=2, height=5)\ng.map(sns.kdeplot, 'Fare', shade= True).add_legend()\ng.set(xlim=(-100, df_train['Fare'].max()))\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:47:08.093775Z","iopub.execute_input":"2021-08-02T13:47:08.094147Z","iopub.status.idle":"2021-08-02T13:47:08.582752Z","shell.execute_reply.started":"2021-08-02T13:47:08.094117Z","shell.execute_reply":"2021-08-02T13:47:08.581846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name='2.1.4'></a>\n### 2.1.4 Cabin\n- The large portion (nearly 80%) of cabin feature is missing and the feature itself can not be ignored completely because cabin is essential in deciding the survival rate of customers.\n\n--> Therefore, It'd be appropriate to separate all the null cabin values as a cabin \"M\"\n\n- The Cabin value looks like C123, it turns out the 1st letter represents the deck in which cabin is located.\n\n--> We need to separate the column \"Cabin\" into the \"Deck\" column only and the remaining number are redundant (should be removed)","metadata":{}},{"cell_type":"code","source":"nan_cabin_percent = round(df_all['Cabin'].isnull().mean()*100, 2)\nprint(f\"Proportion of missing values in \\\"Cabin\\\" feature is {nan_cabin_percent}%\")","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:47:08.583946Z","iopub.execute_input":"2021-08-02T13:47:08.584232Z","iopub.status.idle":"2021-08-02T13:47:08.589739Z","shell.execute_reply.started":"2021-08-02T13:47:08.584203Z","shell.execute_reply":"2021-08-02T13:47:08.589042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create col \"Deck\" contained only 1st letter of column \"Cabin\" & assign \"M\" to null\ndf_all['Deck'] = df_all['Cabin'].apply(lambda s: s[0] if pd.notnull(s) else 'X')\n\n# Transform passenger with deck T to A\nidx = df_all.loc[df_all['Deck'] == \"T\"].index\ndf_all.loc[idx, 'Deck'] = 'A'\n\ndf_all['Deck'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:47:08.590822Z","iopub.execute_input":"2021-08-02T13:47:08.591288Z","iopub.status.idle":"2021-08-02T13:47:08.612218Z","shell.execute_reply.started":"2021-08-02T13:47:08.591245Z","shell.execute_reply":"2021-08-02T13:47:08.611527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train, test = divide_df(df_all)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))\nsns.countplot(train['Deck'], ax=ax1)\nax1.set_title('Count passengers each deck (Train data only)', fontsize=15)\n\nsns.countplot(x='Deck', hue='Survived', data=train, ax=ax2, palette='rocket')\nax2.set_title('Relationship between Survival and Deck (Train data only)', fontsize=15)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:47:08.613387Z","iopub.execute_input":"2021-08-02T13:47:08.613885Z","iopub.status.idle":"2021-08-02T13:47:09.07333Z","shell.execute_reply.started":"2021-08-02T13:47:08.613847Z","shell.execute_reply":"2021-08-02T13:47:09.072189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The survival rate is just one-half of the mortality rate in Deck \"X\". It logically make sense, because deck \"X\" represent the missing infomations & it's hard to derive those values back from the non-survived passengers.","metadata":{}},{"cell_type":"markdown","source":"### 2.1.5 Deck & Survived\n'Decks' is now a high-cardinality feature -> We'll group decks value based on their similarity\n- A, B and C decks are labeled as ABC because all of them have only 1st class passengers\n- D and E decks are labeled as DE because both of them have similar passenger class distribution and same survival rate\n- F and G decks are labeled as FG because of the same reason above\n- M deck doesn't need to be grouped with other decks because it is very different from others and has the lowest survival rate.\n\n(NFM) \"High-cardinality\" feature is the feature which is unique and uncommon (too many unique values)","metadata":{}},{"cell_type":"code","source":"df_all['Deck'] = df_all['Deck'].replace(['A', 'B', 'C'], 'ABC')\ndf_all['Deck'] = df_all['Deck'].replace(['D', 'E'], 'DE')\ndf_all['Deck'] = df_all['Deck'].replace(['F', 'G'], 'FG')\n\ndf_all['Deck'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:47:09.075225Z","iopub.execute_input":"2021-08-02T13:47:09.075659Z","iopub.status.idle":"2021-08-02T13:47:09.090704Z","shell.execute_reply.started":"2021-08-02T13:47:09.075613Z","shell.execute_reply":"2021-08-02T13:47:09.089292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train, test = divide_df(df_all)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))\nsns.countplot(train['Deck'], palette='Set2', ax=ax1)\nax1.set_title('Count passengers each deck (Train data only)', fontsize=15)\n\nsns.countplot(x='Deck', hue='Survived', data=train, ax=ax2, palette='hls')\nax2.set_title('Relationship between Survival and Deck (Train data only)', fontsize=15)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:47:09.092412Z","iopub.execute_input":"2021-08-02T13:47:09.092895Z","iopub.status.idle":"2021-08-02T13:47:09.439331Z","shell.execute_reply.started":"2021-08-02T13:47:09.09283Z","shell.execute_reply":"2021-08-02T13:47:09.438272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.1.6 Pclass & Survived","metadata":{}},{"cell_type":"code","source":"train, test = divide_df(df_all)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 7))\nsns.countplot(train['Pclass'], palette='Set2', ax=ax1)\nax1.set_title(\"Count passengers in each Pclass\")\n\nsns.barplot(x='Sex', y='Survived', hue='Pclass', data=train, ax=ax2, palette='Set1')\nax2.set_title(\"Survived distributed among Sex and Pclass\")\n\nsns.factorplot('Pclass', 'Survived', order=[1, 2, 3], data=train)\n\nfor i in range(1, 4):\n    temp = train.loc[df_train['Pclass']==i]['Survived'].value_counts()\n    print(f'Percentage of Pclass {i} survived over total Pclass {i}: {round(temp[1]/sum(temp), 2)*100}%')\n","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:47:09.440623Z","iopub.execute_input":"2021-08-02T13:47:09.440935Z","iopub.status.idle":"2021-08-02T13:47:10.402234Z","shell.execute_reply.started":"2021-08-02T13:47:09.440904Z","shell.execute_reply":"2021-08-02T13:47:10.401517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The majority of passengers belongs to Pclass 3 which accounts for nearly 55%.\n- By following the plot, the relationship between Pclass and Sex is essential for the survival prediction. Pclass 1 and 2 are almost guaranteed survival for women, while Pclass 2 and 3 are extremely bad for men to survived.\n- The survival percentage of Pclass 1 is around 67% compared to 47% in 2nd class & 24% in 3rd class","metadata":{}},{"cell_type":"code","source":"# Drop feature Cabin\ndf_all.drop('Cabin', axis=1, inplace=True)\n\n# Separate training and testing data\ndf_train, df_test = divide_df(df_all)\n\nprint('Missing values in Training dataset')\nprint(df_train.isnull().sum(), \"\\n\")\nprint('\\n Missing values in Testing dataset')\nprint(df_test.isnull().sum(), \"\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:47:10.403358Z","iopub.execute_input":"2021-08-02T13:47:10.403823Z","iopub.status.idle":"2021-08-02T13:47:10.416703Z","shell.execute_reply.started":"2021-08-02T13:47:10.403789Z","shell.execute_reply":"2021-08-02T13:47:10.415963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name='2.2'></a>\n## 2.2 Target column distribution","metadata":{}},{"cell_type":"code","source":"# Compute survival percentage in training dataset\nnot_survived = df_train['Survived'].loc[df_train['Survived']==0].count()\nsurvived = df_train['Survived'].loc[df_train['Survived']==1].count()\nprint(f'{not_survived} of {not_survived+survived} passengers survived and it is the 38.38% of the training set.')\nprint(f'{survived} of {not_survived+survived} passengers survived and it is the 61.62% of the training set.')","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:47:10.41775Z","iopub.execute_input":"2021-08-02T13:47:10.418272Z","iopub.status.idle":"2021-08-02T13:47:10.426026Z","shell.execute_reply.started":"2021-08-02T13:47:10.41823Z","shell.execute_reply":"2021-08-02T13:47:10.424886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize number of survival and non_survivals in training dataset\nsurvived_rate = [survived, not_survived]\nbar_count = ['Survived', 'Not survived']\n\nfig, axs = plt.subplots(1, 2, figsize=(9, 7))\nsns.countplot(df_train['Survived'], facecolor=(0, 0, 0, 0),\n              linewidth=5, edgecolor=sns.color_palette(\"dark\", 3), ax=axs[0])\nsns.countplot(x='Sex', hue='Survived', data=df_train, ax=axs[1], palette='husl')\n\naxs[0].set_ylabel('Count', size=15, labelpad=5)\naxs[1].set_ylabel('Count', size=15, labelpad=5)\naxs[0].set_xlabel('Survived', size=15, labelpad=5)\naxs[1].set_xlabel('Sex', size=15, labelpad=5)\n\naxs[0].set_title('Training Set Survival Distribution', size=13, y=1.05)\naxs[1].set_title('Survival distributed among sex', size=13, y=1.05)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:47:10.431297Z","iopub.execute_input":"2021-08-02T13:47:10.431563Z","iopub.status.idle":"2021-08-02T13:47:10.665622Z","shell.execute_reply.started":"2021-08-02T13:47:10.431537Z","shell.execute_reply":"2021-08-02T13:47:10.66467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Of 1309 people on the titanic, the majority of 64.4% are male. Within the 1st plot, nearly 550 passengers died compared for only 350 people survived. In the 2nd plot with training data, 81.1% men and 25.8% of women died. Depending on that, \"Sex\" plays an important role in predicting the survival","metadata":{}},{"cell_type":"markdown","source":"<a name='2.3'></a>\n## 2.3 Correlations\n- Correlation is a statistical term which in simple explanation refers to how close two variables having linear relationship with each other\n\nFor example, 2 variables are linearly dependent (x and y are dependent on each other as x=2y). While 2 variable are independent (# of parents & Pclass are not close to each other)\n\n*(Quest ask myself!) Why negative values exist in correlation table?*\n- It means that there is an inverse relationship between 2 variables tested\n- For example, the snowfall-driver is negative correlation b.c the increase the amount of snowfall, the few the number of driver on the street","metadata":{}},{"cell_type":"code","source":"# Construct correlation table for Training data\ndf_train_corr = df_train.drop(['PassengerId'], axis=1).corr().abs().unstack().sort_values(kind=\"quicksort\", ascending=False).reset_index()\ndf_train_corr.rename(columns={\"level_0\": \"Feature 1\", \"level_1\": \"Feature 2\", 0: 'Correlation Coefficient'}, inplace=True)\n# Remove odd row's index b.c they're replicated by the even row's index\ndf_train_corr.drop(df_train_corr.iloc[1::2].index, inplace=True)\n# Drop row having correlation value 1.0\ndf_train_corr_nd = df_train_corr.drop(df_train_corr[df_train_corr['Correlation Coefficient'] == 1.0].index)\n\n# Construct correlation table for Testing data (Steps same as training data)\ndf_test_corr = df_test.corr().abs().unstack().sort_values(kind=\"quicksort\", ascending=False).reset_index()\ndf_test_corr.rename(columns={\"level_0\": \"Feature 1\", \"level_1\": \"Feature 2\", 0: 'Correlation Coefficient'}, inplace=True)\ndf_test_corr.drop(df_test_corr.iloc[1::2].index, inplace=True)\ndf_test_corr_nd = df_test_corr.drop(df_test_corr[df_test_corr['Correlation Coefficient'] == 1.0].index)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:47:10.668194Z","iopub.execute_input":"2021-08-02T13:47:10.668593Z","iopub.status.idle":"2021-08-02T13:47:10.688929Z","shell.execute_reply.started":"2021-08-02T13:47:10.668549Z","shell.execute_reply":"2021-08-02T13:47:10.688118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize correlation table by heatmap\nfig, axs = plt.subplots(nrows=2, figsize=(8, 16))\n\nsns.heatmap(df_train.drop(['PassengerId'], axis=1).corr(), ax=axs[0], annot=True, square=True,\n            cmap='mako', annot_kws={'size': 14})\nsns.heatmap(df_test.drop(['PassengerId'], axis=1).corr(), ax=axs[1], annot=True, square=True,\n            cmap='rocket', annot_kws={'size': 14})\n\nfor i in range(2):\n    axs[i].tick_params(axis='x', labelsize=14)\n    axs[i].tick_params(axis='y', labelsize=14)\n    \naxs[0].set_title('Training correlation set', fontsize=17)\naxs[1].set_title('Testing correlation set', fontsize=17)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:47:10.690036Z","iopub.execute_input":"2021-08-02T13:47:10.690311Z","iopub.status.idle":"2021-08-02T13:47:11.833718Z","shell.execute_reply.started":"2021-08-02T13:47:10.690285Z","shell.execute_reply":"2021-08-02T13:47:11.832801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Take away from correlation plot\n- Following the above plot, there is only few highly correlated features but this signal is a good point. It means that the features don't contain much the redundant or unnecessary values and we're happy that each feature carries with its own unique information. ","metadata":{}},{"cell_type":"markdown","source":"## Remark\n- This relationship can be used to create new features with \"feature transformation\" and \"feature interaction\". \n- Target encoding could be very useful as well because of the high correlations with Survived feature.\n- Categorical features have very distinct distributions with different survival rates. Those features can be one-hot encoded. Some of those features may be combined with each other to make new features.","metadata":{}},{"cell_type":"code","source":"df_all = concat_df(df_train, df_test)\ndf_all.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:47:11.83491Z","iopub.execute_input":"2021-08-02T13:47:11.835197Z","iopub.status.idle":"2021-08-02T13:47:11.859918Z","shell.execute_reply.started":"2021-08-02T13:47:11.835169Z","shell.execute_reply":"2021-08-02T13:47:11.858901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name='3'></a>\n# 3. Feature engineering","metadata":{}},{"cell_type":"markdown","source":"<a name='3.1'></a>\n## 3.1 Binning Continous features\n- Is process of transforming from a continuous or numerical variable to a categorical feature by groupping them into some smaller ranges. We called it \"Binning\" technique\n- Binning of continuous variable is one of feature engineering technique and tends to improve the performance of the model. \n- The binning can be also used to identify missing values or outliers\n- This mechanism divived the continuous variable into specific number of bins which is a smaller range out of total range of the numerous feature ","metadata":{}},{"cell_type":"markdown","source":"<a name='3.1.1'></a>\n### 3.1.1 Fare","metadata":{}},{"cell_type":"code","source":"df_all['Fare'] = pd.qcut(df_all['Fare'], 10)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:47:11.861142Z","iopub.execute_input":"2021-08-02T13:47:11.86141Z","iopub.status.idle":"2021-08-02T13:47:11.87485Z","shell.execute_reply.started":"2021-08-02T13:47:11.861383Z","shell.execute_reply":"2021-08-02T13:47:11.873855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12, 8))\n# Countplot for 2 features\nsns.countplot(x='Fare', hue='Survived', data=df_all, palette='Accent')\n    \nplt.xlabel('Fare', size=15, labelpad=15)\nplt.ylabel('Passenger Count', size=15, labelpad=15)    \nplt.tick_params(axis='x', labelsize=8)\nplt.tick_params(axis='y', labelsize=15)\n    \nplt.legend(['Not Survived', 'Survived'], loc='upper right', prop={'size': 16})\nplt.title('Count of Survival in Fare Feature', size=20, y=1.05)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:47:11.876329Z","iopub.execute_input":"2021-08-02T13:47:11.876987Z","iopub.status.idle":"2021-08-02T13:47:12.122883Z","shell.execute_reply.started":"2021-08-02T13:47:11.876944Z","shell.execute_reply":"2021-08-02T13:47:12.121911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"REMARK\n- \"Fare\" feature and \"Survived\" rate are negatively correlated with each other\n- 13 quantiles based bins are used for 'Fare' feature. They provide a decent amount of information gain\n- The group on the left has the lowest survival rate and the group on the right has the highest survival rate\n- There is an unusual group of fare's values [21.555, 25.52] which has higher the survival rate","metadata":{}},{"cell_type":"markdown","source":"<a name='3.1.2'></a>\n### 3.1.2 Age","metadata":{}},{"cell_type":"code","source":"df_all['Age'] = pd.qcut(df_all['Age'], 10, duplicates='drop')","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:47:12.124017Z","iopub.execute_input":"2021-08-02T13:47:12.124285Z","iopub.status.idle":"2021-08-02T13:47:12.134066Z","shell.execute_reply.started":"2021-08-02T13:47:12.124259Z","shell.execute_reply":"2021-08-02T13:47:12.132794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15, 8))\n# Countplot for 2 features\nsns.countplot(x='Age', hue='Survived', data=df_all, palette='GnBu_r')\n    \nplt.xlabel('Age', size=15, labelpad=15)\nplt.ylabel('Passenger Count', size=15, labelpad=15)    \nplt.tick_params(axis='x', labelsize=8)\nplt.tick_params(axis='y', labelsize=15)\n    \nplt.legend(['Not Survived', 'Survived'], loc='upper right', prop={'size': 16})\nplt.title('Count of Survival in Age Feature', size=20, y=1.05)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:47:12.13553Z","iopub.execute_input":"2021-08-02T13:47:12.13602Z","iopub.status.idle":"2021-08-02T13:47:12.398906Z","shell.execute_reply.started":"2021-08-02T13:47:12.135975Z","shell.execute_reply":"2021-08-02T13:47:12.397747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"REMARK\n- Passenger with ages ranging from [22.0, 25.0] has by far the most lowest survival rate than the other and bin with [0.169, 16] has the highest survival rate\n- There is an age's group [34.0, 40.0] with unusual high survival rate in this process","metadata":{}},{"cell_type":"markdown","source":"<a name='3.2'></a>\n## 3.2 Frequency Encoded Binning","metadata":{}},{"cell_type":"markdown","source":"<a name='3.2.1'></a>\n### 3.2.1 Family size\n- Family_Size is created by adding SibSp, Parch and 1. \n- Those columns are added to compute the total size of family & 1 is added to the end for current passenger himself/herself. The Family_Size table. Graph also clearly show that `Family_size` is the predictor of survival rate, b.c different size turns out different rate\n    \n    - Family size with 1 labeled: Alone\n    - Family Size with 2, 3 and 4 are labeled as Small\n    - Family Size with 5, 6, 7, 8 and 11 are labeled as Large\n- Because the survival rate is highly effected by whether passenger was alone or not. Therefore, a new feature \"Alone\" can be considered as a flag for the lonely passengers","metadata":{}},{"cell_type":"code","source":"# Family_Size feature\ndf_all['Family_Size'] = df_all['SibSp'] + df_all['Parch'] + 1\n# Family_Size_Grouped feature\nfamily_map = {1:'alone', 2:'small', 3:'small', 4:'small', 5:'large', 6:'large', 7:'large', 8:'large', 11:'large'}\ndf_all['Family_Size_Grouped'] = df_all['Family_Size'].map(family_map)\n# Create feature \"Alone\"\ndf_all['Alone'] = 0\ndf_all['Alone'].loc[df_all['Family_Size_Grouped'] == 'alone'] = 1","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:47:12.400527Z","iopub.execute_input":"2021-08-02T13:47:12.400952Z","iopub.status.idle":"2021-08-02T13:47:12.412126Z","shell.execute_reply.started":"2021-08-02T13:47:12.400907Z","shell.execute_reply":"2021-08-02T13:47:12.411191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 7))\nsns.countplot(x=\"Family_Size_Grouped\", hue='Survived', data=df_all, palette='Set2')\nplt.xlabel('Family_Size_Grouped', fontsize=14)\nplt.ylabel('Count', fontsize=14)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:47:12.413531Z","iopub.execute_input":"2021-08-02T13:47:12.413943Z","iopub.status.idle":"2021-08-02T13:47:12.603901Z","shell.execute_reply.started":"2021-08-02T13:47:12.413902Z","shell.execute_reply":"2021-08-02T13:47:12.602964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name='3.2.2'></a>\n### 3.2.2 Ticket frequency\n- Passengers who're comming with their families or their mates bought the same tickets\n- There are too many unique `Ticket` values to analyze, so grouping them up by their frequencies makes things easier.\n\n(Ask myself) **How** is this feature difference from `Family_size`?\n- Many passengers travel along with groups who **can be their friends, team mates** not necessary be a family. Therefore, they are not counted in `Family_size` but instead using **the same ticket number**\n\n(Ask myself) **How** does the family size affect the survival rate?\n- According to graph, passengers traveling with group of 2, 3, 4 have the highest survival rate, while those who travel alone has the lowest survival rate. From group of 5 onwards, the survival rate decreases dramatically. \n- The pattern is really similar to `Family_size` feature --> `Ticket_frequency` can be considered as the predictor","metadata":{}},{"cell_type":"code","source":"df_all['Ticket_Frequency'] = df_all.groupby('Ticket')['Ticket'].transform('count')","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:47:12.605109Z","iopub.execute_input":"2021-08-02T13:47:12.60541Z","iopub.status.idle":"2021-08-02T13:47:12.613717Z","shell.execute_reply.started":"2021-08-02T13:47:12.605379Z","shell.execute_reply":"2021-08-02T13:47:12.612374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15, 8))\n# Countplot for 2 features\nsns.countplot(x='Ticket_Frequency', hue='Survived', data=df_all)\n    \nplt.xlabel('Ticket_Frequency', size=15, labelpad=15)\nplt.ylabel('Survival Count', size=15, labelpad=15)    \nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=15)\n    \nplt.legend(['Not Survived', 'Survived'], loc='upper right', prop={'size': 16})\nplt.title('Count of Survival in Ticket frequency Feature', size=20, y=1.05)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:47:12.615056Z","iopub.execute_input":"2021-08-02T13:47:12.615334Z","iopub.status.idle":"2021-08-02T13:47:12.859597Z","shell.execute_reply.started":"2021-08-02T13:47:12.615309Z","shell.execute_reply":"2021-08-02T13:47:12.858508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name='3.3'></a>\n## 3.3 Title & is Married\n- Title is created by extracting the prefix before Name feature. According to graph below, there are many titles that are occuring very few times. \n- Some of those titles doesn't seem correct and they need to be replaced. \n- **Miss, Mrs, Ms, Mlle, Lady, Mme, the Countess, Dona** titles are replaced with Miss/Mrs/Ms because all of them are female. \n- **Dr, Col, Major, Jonkheer, Capt, Sir, Don and Rev** titles are replaced by **Dr/Sir/Noble/Clergy** because those passengers have similar characteristics. \n- **Master** is a unique title. It is given to male passengers below age 26. They have the highest survival rate among all males.\n\n**Is_Married** is a binary feature based on the **Mrs** title. Mrs title has the highest survival rate among other female titles. This title needs to be a feature because all female titles are grouped with each other.","metadata":{}},{"cell_type":"code","source":"df_all['Name']","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:47:12.860726Z","iopub.execute_input":"2021-08-02T13:47:12.861014Z","iopub.status.idle":"2021-08-02T13:47:12.869059Z","shell.execute_reply.started":"2021-08-02T13:47:12.860987Z","shell.execute_reply":"2021-08-02T13:47:12.868062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract the tile and Is_married signal from the name\ndf_all['Title'] = df_all['Name'].str.split(',', expand=True)[1].str.split('.', expand=True)[0].str.strip()\ndf_all['Married'] = 0\ndf_all['Married'].loc[df_all['Title'] == 'Mrs'] = 1\n\n# Title unique values count\ndf_all['Title'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:47:12.870349Z","iopub.execute_input":"2021-08-02T13:47:12.870653Z","iopub.status.idle":"2021-08-02T13:47:12.893288Z","shell.execute_reply.started":"2021-08-02T13:47:12.870623Z","shell.execute_reply":"2021-08-02T13:47:12.89263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Replace Miss, Mrs, Ms, Mlle, Lady, Mme, the Countess, Dona title as \"Miss/Mrs/Ms\"\ndf_all['Title'] = df_all['Title'].replace(['Miss', 'Mrs','Ms', 'Mlle', 'Lady', 'Mme', 'the Countess', 'Dona'], 'Miss/Mrs/Ms')\n# Replace Dr, Col, Major, Jonkheer, Capt, Sir, Don and Rev title as \"Dr/Military/Noble/Clergy\"\ndf_all['Title'] = df_all['Title'].replace(['Dr', 'Col', 'Major', 'Jonkheer', 'Capt', 'Sir', 'Don', 'Rev', 'Master'], 'Dr/Sir/Military/Master')\n\ndf_all['Title'].unique()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:47:12.895776Z","iopub.execute_input":"2021-08-02T13:47:12.896182Z","iopub.status.idle":"2021-08-02T13:47:12.911214Z","shell.execute_reply.started":"2021-08-02T13:47:12.89615Z","shell.execute_reply":"2021-08-02T13:47:12.910266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name='3.4'></a>\n## 3.4 Feature Transformation\n- After modifying all the features into the appropriate forms, we need to encode the categorical features into numerous so that the data is well-prepared for training model.","metadata":{}},{"cell_type":"code","source":"# Train/Test splitting\ndf_train = df_all.loc[:890]\ndf_test = df_all.loc[891:]\ndfs = [df_train, df_test]","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:47:12.912455Z","iopub.execute_input":"2021-08-02T13:47:12.912873Z","iopub.status.idle":"2021-08-02T13:47:12.918377Z","shell.execute_reply.started":"2021-08-02T13:47:12.912824Z","shell.execute_reply":"2021-08-02T13:47:12.917599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:47:12.919597Z","iopub.execute_input":"2021-08-02T13:47:12.919974Z","iopub.status.idle":"2021-08-02T13:47:12.952084Z","shell.execute_reply.started":"2021-08-02T13:47:12.919944Z","shell.execute_reply":"2021-08-02T13:47:12.95107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name='3.4.1'></a>\n### 3.4.1 Label encoding non-numerical features\n- Norminal data: `Embarked`, `Deck`, `Sex`, `Title`\n- Ordinal data: `Family_Size_Grouped`, `Age`, `Fare`\n\nWe convert them into numerical data type by using `LabelEncoding`. `LabelEncoding` basically transform categorical value to 0 to n class based on their unique values of each features\n\nNote that:\n- **Nominal data** is a type of data that is used as naming or label variables without providing any quantitative value\n- **Ordinal data** contains same features as nominal data but plussing the capability of ordered & measured features","metadata":{}},{"cell_type":"code","source":"non_numeric_features = ['Embarked', 'Sex', 'Deck', 'Title', 'Family_Size_Grouped', 'Age', 'Fare']\n\nfor df in dfs:\n    for feature in non_numeric_features:\n        df[feature] = preprocessing.LabelEncoder().fit_transform(df[feature])","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:47:12.953522Z","iopub.execute_input":"2021-08-02T13:47:12.954017Z","iopub.status.idle":"2021-08-02T13:47:12.982555Z","shell.execute_reply.started":"2021-08-02T13:47:12.953976Z","shell.execute_reply":"2021-08-02T13:47:12.981293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train[non_numeric_features]","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:47:12.984868Z","iopub.execute_input":"2021-08-02T13:47:12.98536Z","iopub.status.idle":"2021-08-02T13:47:13.007941Z","shell.execute_reply.started":"2021-08-02T13:47:12.985292Z","shell.execute_reply":"2021-08-02T13:47:13.006743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name='3.4.2'></a>\n### 3.4.2 One-hot Encoding the categorical features\n- We do NOT encode `Family_Size_Grouped`, `Age`, `Fare` feature using One-hot encoder b.c they are ordinal data type\n- Because the natural ordering between ordinal categories resulting from One-hot encoding may tend to poor performance or unexpected consequence","metadata":{}},{"cell_type":"code","source":"cat_features = ['Sex', 'Deck', 'Embarked', 'Title']\n# encoded_features: list of encoded df of training & testing dataset\nencoded_features = []\n\n# One-hot encoding for both training & testing set\nfor df in dfs:\n    for feature in cat_features:\n        # Change to array after encoding b.c want to add columns when change back to df\n        encoded_feat = preprocessing.OneHotEncoder().fit_transform(df[feature].values.reshape(-1, 1)).toarray()\n        # num of unique value in each feature\n        n = df[feature].nunique()\n        # \"feature_uniqueVal\" are the col's names in df after One-hot encoding\n        cols = ['{}_{}'.format(feature, n) for n in range(1, n + 1)]\n        encoded_df = pd.DataFrame(encoded_feat, columns=cols)\n        encoded_df.index = df.index\n        encoded_features.append(encoded_df)\n        \ndf_train = pd.concat([df_train, *encoded_features[:4]], axis=1)\ndf_test = pd.concat([df_test, *encoded_features[4:]], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:47:13.009744Z","iopub.execute_input":"2021-08-02T13:47:13.01019Z","iopub.status.idle":"2021-08-02T13:47:13.033835Z","shell.execute_reply.started":"2021-08-02T13:47:13.010148Z","shell.execute_reply":"2021-08-02T13:47:13.033133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For submission dataframe\npassengerID = df_test['PassengerId']\n\ndrop_cols = ['Deck', 'Embarked', 'Family_Size', 'Name', 'Parch', \n             'PassengerId', 'Sex', 'SibSp', 'Ticket', 'Title']\n\ndf_train.drop(columns=drop_cols, inplace=True)\ndrop_cols.append(\"Survived\")\ndf_test.drop(columns=drop_cols, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:47:13.035307Z","iopub.execute_input":"2021-08-02T13:47:13.035963Z","iopub.status.idle":"2021-08-02T13:47:13.04606Z","shell.execute_reply.started":"2021-08-02T13:47:13.035918Z","shell.execute_reply":"2021-08-02T13:47:13.045107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:47:13.047669Z","iopub.execute_input":"2021-08-02T13:47:13.048124Z","iopub.status.idle":"2021-08-02T13:47:13.079895Z","shell.execute_reply.started":"2021-08-02T13:47:13.048047Z","shell.execute_reply":"2021-08-02T13:47:13.078851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusion\n- `Age` and `Fare` features are binned. Binning features helps dealing with outliers. `Family_size` is created by summing value of `Parch` as number of parent's member, `SibSp` as number of children's member and plussing 1 as him/herself. `Ticket_Frequency` is created by counting the frequency of each unique ticket\n- `Name` feature is very useful. First, `Title` and `Is_Married` features are created from the title prefix in the names.\n- Finally, the non-numeric type features are label encoded and categorical non-ordinal features are one-hot encoded. Created 5 new features (`Family_Size_Grouped`, `Title`, `Is_Married`, `Deck`, `Ticket_Frequency`) and dropped the useless features after encoding.","metadata":{}},{"cell_type":"markdown","source":"<a name='4'></a>\n# 4. Modeling\nTitanic competition is a binary classification and regression problems. We want to determine the relationship between the features (Age, Fare, Sex,...) and the target column (Survived). Based on the given dataset including this type of problem, a category of machine learning which is called Supervised learning should be useful. Therefore we can narrow down our choice to these below supervised algorithms:\n- K-Nearest Neightbors (KNN)\n- Support Vector Machines\n- Naive Bayes Classifier\n- Decision tree\n- Random forest\n- Extra tree\n- Logistic regression\n- Stochastic gradient Boosting","metadata":{}},{"cell_type":"code","source":"X_train = preprocessing.StandardScaler().fit_transform(df_train.drop(['Survived'], axis=1))\ny_train = df_train['Survived'].values\nX_test = preprocessing.StandardScaler().fit_transform(df_test)\n\nprint('X_train shape: {}'.format(X_train.shape))\nprint('y_train shape: {}'.format(y_train.shape))\nprint('X_test shape: {}'.format(X_test.shape))","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:47:13.081858Z","iopub.execute_input":"2021-08-02T13:47:13.08232Z","iopub.status.idle":"2021-08-02T13:47:13.102698Z","shell.execute_reply.started":"2021-08-02T13:47:13.082244Z","shell.execute_reply":"2021-08-02T13:47:13.101605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name='4.1'></a>\n## 4.1 Cross validate simple models\nThe goal of any supervised algorithm is to best optimize the mapping function (f) for\nthe output variable (Y) with given input data (X)\n- In order to create an efficient model, training and testing dataset should comes from different distributions so that the model is capable of adapting to the real-world data\n- Therefore, in common cases, the performances on training dataset is largely higher than\nthose on testing dataset. This problem is called **\"Variance error\"**\n\n**\"K-fold Cross validation\"** technique provides a smart move to avoid this problem. It split the training data into k equaled groups & perform the prediction process on 1 group, the other groups are used for training the model. The whole process will be repeatedly carried out for each group & averaging the accuracy of them all to get the final result ","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import KFold #for K-fold cross validation\nfrom sklearn.model_selection import cross_val_score #score evaluation\nfrom sklearn.model_selection import cross_val_predict #prediction","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:47:13.104524Z","iopub.execute_input":"2021-08-02T13:47:13.104877Z","iopub.status.idle":"2021-08-02T13:47:13.109012Z","shell.execute_reply.started":"2021-08-02T13:47:13.104845Z","shell.execute_reply":"2021-08-02T13:47:13.108184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cross validate models with K-fold cross validation\nKfold = KFold(n_splits=10, random_state=SEED)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:47:13.110138Z","iopub.execute_input":"2021-08-02T13:47:13.110455Z","iopub.status.idle":"2021-08-02T13:47:13.121581Z","shell.execute_reply.started":"2021-08-02T13:47:13.110428Z","shell.execute_reply":"2021-08-02T13:47:13.120612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ML_algos = {\n    'KNN' : neighbors.KNeighborsClassifier(),\n    'SVC' : svm.SVC(probability=True, random_state=SEED),\n    'GaussianNB' : naive_bayes.GaussianNB(),\n    'DTree' : tree.DecisionTreeClassifier(random_state=SEED),\n    'RFor' : ensemble.RandomForestClassifier(random_state=SEED),\n    'LReg' : linear_model.LogisticRegressionCV(random_state=SEED),\n    'GrB' : ensemble.GradientBoostingClassifier(random_state=SEED),\n    'ExTree' : ensemble.ExtraTreesClassifier(random_state=SEED)\n}\ncv_result = dict()\ncv_mean = dict()\ncv_std = dict()\n\n# Compute the cross validation result of each ML algorithm\nfor name, ML_algo in ML_algos.items():\n    cv_result[name] = cross_val_score(ML_algo, X_train, y=y_train, scoring='accuracy', cv=Kfold, n_jobs=3)\n    cv_mean[name] = cv_result[name].mean()\n    cv_std[name] = cv_result[name].std()\n    ","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:47:13.123182Z","iopub.execute_input":"2021-08-02T13:47:13.123477Z","iopub.status.idle":"2021-08-02T13:47:20.096836Z","shell.execute_reply.started":"2021-08-02T13:47:13.123448Z","shell.execute_reply":"2021-08-02T13:47:20.095523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ML_cv = pd.DataFrame({'cv_mean' : cv_mean, 'cv_std' : cv_std})\nML_cv","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:47:20.098442Z","iopub.execute_input":"2021-08-02T13:47:20.099297Z","iopub.status.idle":"2021-08-02T13:47:20.116875Z","shell.execute_reply.started":"2021-08-02T13:47:20.099247Z","shell.execute_reply":"2021-08-02T13:47:20.115737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(10, 5))\nax = sns.barplot(x=ML_cv['cv_mean'], y=ML_cv.index, orient='h')\nax.set_title('ML algorithms cross validation mean values', fontsize=15)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:47:20.118677Z","iopub.execute_input":"2021-08-02T13:47:20.119495Z","iopub.status.idle":"2021-08-02T13:47:20.380124Z","shell.execute_reply.started":"2021-08-02T13:47:20.119448Z","shell.execute_reply":"2021-08-02T13:47:20.37945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name='4.2'></a>\n## 4.2 Hyper parameters tuning\n- Each model architecture has their own set of parameters which is called \"Hyperparameters\".\n- \"Hyperparameters tuning\" is a process of constructing clear plan to search for the ideal model architectures by choosing the best options for the their set of parameters ","metadata":{}},{"cell_type":"code","source":"# Cleaner & greater ways forhyperparameters\n# Hyperparameter Tune with GridSearchCV\nn_estimators = [10, 50, 100, 300]\n\ngrid_param = [\n    # KNN \n    {'n_neighbors' : range(1, 31),\n     'weights' : ['uniform', 'distance'],\n     'metric' : ['euclidean', 'manhattan', 'minkowski']},\n    \n    # Support vector machine \n    {'kernel' : ['poly', 'rbf', 'sigmoid'],\n     'C' : [50, 10, 1.0, 0.1, 0.01], \n     'gamma' : ['scale']},\n    \n    # Gradient Boosting \n    {'n_estimators' : n_estimators,\n     'learning_rate': [0.001, 0.01, 0.1],\n     'subsample' : [0.5, 0.7, 1.0],\n     'max_depth' : [3, 7, 9] },\n    \n    # Random forest \n    {'n_estimators' : n_estimators,\n    'max_features' : ['sqrt', 'log2'],\n    'criterion' : ['gini', 'entropy'],\n    'oob_score' : [True]},\n    \n    # Logistic regression\n    {'solver' : ['newton-cg', 'lbfgs', 'liblinear'],\n     'Cs' : [10, 1.0, 0.1, 0.01],\n    'fit_intercept': [True, False]}\n]\n\nML_algos = {\n    'KNN' : neighbors.KNeighborsClassifier(),\n    'SVC' : svm.SVC(probability=True, random_state=SEED),\n    'GrB' : ensemble.GradientBoostingClassifier(random_state=SEED),\n    'RFor' : ensemble.RandomForestClassifier(random_state=SEED),\n    'LReg' : linear_model.LogisticRegressionCV(random_state=SEED)\n}","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:47:20.381597Z","iopub.execute_input":"2021-08-02T13:47:20.382242Z","iopub.status.idle":"2021-08-02T13:47:20.392444Z","shell.execute_reply.started":"2021-08-02T13:47:20.3822Z","shell.execute_reply":"2021-08-02T13:47:20.391473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Time performance counter\nstart_time = time.perf_counter()\n\nfor param, ML_algo in zip(grid_param, ML_algos.values()):\n    # Start counting time\n    start = time.perf_counter()\n    grid_search = GridSearchCV(estimator = ML_algo, param_grid=param, n_jobs=2, cv=Kfold, \n                               scoring='accuracy',error_score=0)\n    grid_result = grid_search.fit(X_train, y_train)\n    best_params = grid_result.best_params_\n    best_score = grid_result.best_score_\n    # Compute time for executing each algo\n    run = time.perf_counter() - start\n    \n    print(f'The best parameter for {ML_algo} is {best_params} with the best accuracy score {best_score}')\n    print(f'Computational runtime of this algo: {round(run, 2)} seconds\\n')\n    ML_algo.set_params(**best_params)\n\nrun_total = time.perf_counter() - start_time\nprint('Total optimization time was {:.2f} minutes.'.format(run_total/60))","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:47:20.394083Z","iopub.execute_input":"2021-08-02T13:47:20.394473Z","iopub.status.idle":"2021-08-02T13:52:49.234832Z","shell.execute_reply.started":"2021-08-02T13:47:20.394433Z","shell.execute_reply":"2021-08-02T13:52:49.233403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name='4.3'></a>\n## 4.3 Confusion matrix\n- Confusion matrix is a table describing the performance measurement of ML classification problems. The technique is performed on the set of validation data for which the true values are known\n\n<img src=\"https://2.bp.blogspot.com/-EvSXDotTOwc/XMfeOGZ-CVI/AAAAAAAAEiE/oePFfvhfOQM11dgRn9FkPxlegCXbgOF4QCLcBGAs/s1600/confusionMatrxiUpdated.jpg\" style=\"width:450px;height:300px;\">\n\nFor those who confuse, the \"positive\" or \"negative\" depends on the purpose of the model. This model is used to predict the survival passengers --> \"positive\" means predict 1 as Survived & \"negative\" means predict 0 as not survived\n- **True positive (TP)**: \"True\" means the prediction value is similar to the actual value AND \"positive\" means models predict 1 as a survived passenger\n- **True negative (TN)**: \"True\" means the prediction value is similar to the actual value AND \"negative\" means models predict 0 as a non-survived passenger\n- **False negative (FN)**: \"False\" means the prediction value is different from the actual values AND \"negative\" means models predict 0 as a non-survived passenger\n- **False positive (FN)**: \"False\" means the prediction value is different from the actual values AND \"positive\" means models predict 1 as a survived passenger","metadata":{}},{"cell_type":"code","source":"for name, model in ML_algos.items():\n    y_pred = cross_val_predict(model, X_train, y_train, cv=10)\n    \n    print(f'Confusion matrix of algorithm {name}')\n    print(pd.DataFrame(confusion_matrix(y_train, y_pred),\\\n                       columns=[\"Predicted Not-Survived\", \"Predicted Survived\"],\\\n                       index=[\"Not-Survived\",\"Survived\"]), '\\n')\n    print(classification_report(y_train, y_pred))\n    print('------------------------------\\n\\n')","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:52:49.237099Z","iopub.execute_input":"2021-08-02T13:52:49.23788Z","iopub.status.idle":"2021-08-02T13:52:59.612855Z","shell.execute_reply.started":"2021-08-02T13:52:49.237825Z","shell.execute_reply":"2021-08-02T13:52:59.611826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name='4.4'></a>\n## 4.4 Ensemble model\n- The voting classifier can be a good choice to combine the predictions coming from the 5 classifiers.","metadata":{}},{"cell_type":"code","source":"voteC = ensemble.VotingClassifier(estimators=[('KNN_best', ML_algos['KNN']), ('SVC_best', ML_algos['SVC']),\n                                             ('GrB_best', ML_algos['GrB']), ('RFor_best', ML_algos['RFor']),\n                                             ('LReg_best', ML_algos['LReg'])],\n                                 voting='soft', n_jobs=4)\nvoteC = voteC.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T13:52:59.614432Z","iopub.execute_input":"2021-08-02T13:52:59.615078Z","iopub.status.idle":"2021-08-02T13:53:02.088532Z","shell.execute_reply.started":"2021-08-02T13:52:59.615027Z","shell.execute_reply":"2021-08-02T13:53:02.087435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a name='4.5'></a>\n## 4.5 Submission","metadata":{}},{"cell_type":"code","source":"# Making the prediction\ny_pred = voteC.predict(X_test)\n\nsubmission = pd.DataFrame({\n        'PassengerId' : passengerID,\n        'Survived' : y_pred.astype(int)})\n\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T14:08:38.863413Z","iopub.execute_input":"2021-08-02T14:08:38.863802Z","iopub.status.idle":"2021-08-02T14:08:38.900964Z","shell.execute_reply.started":"2021-08-02T14:08:38.863748Z","shell.execute_reply":"2021-08-02T14:08:38.900004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Credit & Resource\n- Credit should be extended for [Anisotropic](https://www.kaggle.com/arthurtok) and [Gunes Evitan](https://www.kaggle.com/gunesevitan) and all super surportive Kaggler. Send a big thanks to all of them!\n- To [Dataquest](https://www.dataquest.io/?utm_source=google&utm_medium=cpc&utm_campaign&matchtype=e&device=c&campaignid=2083591861&adgroupid=75850930719&adid=482446438765&gclid=CjwKCAjwxo6IBhBKEiwAXSYBs5UgtShJIv7wWV0xdKvnKNUbQ-YyWCpZoUMlTh4fEpyhjU6ButFI0RoCMRAQAvD_BwE) where I start my journey\n- To [Coursera](https://www.coursera.org/) that provided me fundamental skills to get my feets wet\n<br>\n\n**Some great resource:**\n- For excellent materials on stacking or ensembling model, refers to a must-read article for all Kagglers [Kaggle Ensembling Guide](https://mlwave.com/kaggle-ensembling-guide/)\n- [Introduction to machine learning with Python: A Guide to data scientist](https://www.amazon.com/gp/product/1449369413/ref=as_li_tl?ie=UTF8&tag=kaggle-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=1449369413&linkId=740510c3199892cca1632fe738fb8d08) written by core creators of sklearn\n- [Machine learning online course](https://www.coursera.org/learn/machine-learning?) hosted by Andrew Ng _ a pioneer in online education especially on ML & AI","metadata":{}},{"cell_type":"markdown","source":"### I tried my best to finish this notebook with all my current abilities. I honestly hope somehow Kagglers found this notebook helpful. Sharing is caring!","metadata":{}}]}