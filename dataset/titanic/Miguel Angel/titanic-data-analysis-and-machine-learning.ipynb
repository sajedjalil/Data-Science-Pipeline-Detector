{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"43c338e5-638d-593e-5c53-7bbb1052a8a9"},"source":"**This kernel shows:**\n\n - Data exploration with some simple yet useful graphs.\n - Data cleaning.\n - Feature creation.\n - Feature selection.\n - Model score benchmark.\n - Train and test."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d206a1ee-63b3-72a1-051e-525c892f848f"},"outputs":[],"source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import tree\nfrom sklearn import svm\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import fbeta_score\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.ensemble import BaggingClassifier\n\nimport warnings\nwarnings.filterwarnings('ignore')"},{"cell_type":"markdown","metadata":{"_cell_guid":"49f0563f-5246-df8a-2029-8e99c456ce5a"},"source":"**I will work with three datasets:**\n\n - train: contains the information from train.csv. This one will be used to get statistics and graphs.\n - test: contains the information from test.csv. This one will be used to get the predicted labels.\n - data: contains both train and test. This is the one where all data manipulation will be done."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"89a47abc-cfb3-70c2-318d-0352cafc025d"},"outputs":[],"source":"# Read the data:\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\ndata = pd.concat([train,test],ignore_index=True)\nlabels=train[\"Survived\"]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0aac8b15-10bb-3bbb-69e7-5fba155da4e0"},"outputs":[],"source":"# Functions used in the kernel\n\n# Create a graph that groups, counts and check survivors per group\ndef survival_rate(column,t):\n    df=pd.DataFrame()\n    df['total']=train.groupby(column).size()\n    df['survived'] = train.groupby(column).sum()['Survived']\n    df['percentage'] = round(df['survived']/df['total']*100,2)\n    print(df)\n\n    df['survived'].plot(kind=t)\n    df['total'].plot(kind=t,alpha=0.5,title=\"Survivors per \"+str(column))\n    plt.show()\n\n# If age is less than 1, we return 1. Else, we return the original age.\ndef normalize_age_below_one(age):\n    if age < 1:\n        return 1\n    else:\n        return age\n\n# Group ages in buckets\ndef group_age(value):\n    if value <= 10:\n        return \"0-10\"\n    elif value <= 20:\n        return \"10-20\"\n    elif value <= 30:\n        return \"20-30\"\n    elif value <= 40:\n        return \"30-40\"\n    elif value <= 50:\n        return \"40-50\"\n    elif value <= 60:\n        return \"50-60\"\n    elif value <= 70:\n        return \"60-70\"\n    elif value <= 80:\n        return \"70-80\"\n    elif value <= 90:\n        return \"80-90\"\n    else:\n        return \"No data\"\n\n# Change sex type to integers\ndef sex(value):\n    if value == \"male\":\n        return 0\n    else:\n        return 1\n\n# Change embarked type to integers\ndef embarked(value):\n    if value == \"C\":\n        return 0\n    elif value ==\"Q\":\n        return 1\n    else:\n        return 2\n\n# Clean title and convert to numeric.\ndata[\"TitleClean\"] = data[\"Name\"].str.extract('(\\w*\\.)', expand=True)\ndef title_to_int(value):\n    if value == \"Capt.\":\n        return 0\n    elif value == \"Col.\":\n        return 1\n    elif value == \"Countess.\":\n        return 2\n    elif value == \"Don.\":\n        return 3\n    elif value == \"Dr.\":\n        return 4\n    elif value == \"Jonkheer.\":\n        return 5\n    elif value == \"Lady.\":\n        return 6\n    elif value == \"Major.\":\n        return 7\n    elif value == \"Master.\":\n        return 8\n    elif value == \"Miss.\":\n        return 9\n    elif value == \"Mlle.\": #Same as miss\n        return 9\n    elif value == \"Mme.\":\n        return 11\n    elif value == \"Mr.\":\n        return 12\n    elif value == \"Mrs.\":\n        return 13\n    elif value == \"Ms.\":\n        return 14\n    elif value == \"Rev.\":\n        return 15\n    elif value == \"Sir.\":\n        return 16\n    elif value == \"Dona.\": # Same as Mrs\n        return 13\n    else:\n        return np.nan\n    \n# Test a bunch of models. If NL is false, Neural Networks are not tested (they are pretty slow)\ndef lets_try(NL):\n    results={}\n    def test_model(clf):\n        \n        cv = KFold(n_splits=10)\n        fbeta_scorer = make_scorer(fbeta_score, beta=1)\n        cohen_scorer = make_scorer(cohen_kappa_score)\n        accu = cross_val_score(clf, features, labels, cv=cv)\n        fbeta = cross_val_score(clf, features, labels, cv=cv,scoring=fbeta_scorer)\n        cohen = cross_val_score(clf, features, labels, cv=cv,scoring=cohen_scorer)\n        scores=[accu.mean(),fbeta.mean(),cohen.mean()]\n        return scores\n\n    # Decision Tree\n    clf = tree.DecisionTreeClassifier()\n    results[\"Decision Tree\"]=test_model(clf)\n    # Logistic Regression\n    clf = LogisticRegression()\n    results[\"Logistic Regression\"]=test_model(clf)\n    # SVM Linear\n    clf = svm.LinearSVC()\n    results[\"Linear SVM\"]=test_model(clf)\n    # SVM RBF\n    clf = svm.SVC()\n    results[\"RBF SVM\"]=test_model(clf)\n    # Gaussian Bayes\n    clf = GaussianNB()\n    results[\"Gaussian Naive Bayes\"]=test_model(clf)\n    # Random Forest\n    clf=RandomForestClassifier()\n    results[\"Random Forest\"]=test_model(clf)\n    # AdaBoost with Decision Trees\n    clf=AdaBoostClassifier()\n    results[\"AdaBoost\"]=test_model(clf)\n    # SGDC\n    clf=SGDClassifier()\n    results[\"SGDC\"]=test_model(clf)\n    # Bagging\n    clf=BaggingClassifier()\n    results[\"Bagging\"]=test_model(clf)\n    # Neural Networks\n    if NL:\n        clf=MLPClassifier()\n        results[\"Neural Network\"]=test_model(clf)\n    \n    results = pd.DataFrame.from_dict(results,orient='index')\n    results.columns=[\"Accuracy\",\"F-Score\", \"Cohen Kappa\"] \n    results=results.sort(columns=[\"Accuracy\",\"F-Score\", \"Cohen Kappa\"],ascending=False)\n    results.plot(kind=\"bar\",title=\"Model Scores\")\n    axes = plt.gca()\n    axes.set_ylim([0,1])\n    return plt"},{"cell_type":"markdown","metadata":{"_cell_guid":"da50efae-6062-f4ca-2012-fd9a644d91a7"},"source":"# Data Exploration #\n\nIn this section I will do some data exploration to try to find some relations between the survival changes and some of the most important features present in the data."},{"cell_type":"markdown","metadata":{"_cell_guid":"d2a999a3-a431-ff67-3a8b-a5c256074ca9"},"source":"###Data Set's Characteristics###\n\nThe data to analyse includes information from passengers of Titanic, that collided with an iceberg on 15 April 1912. \n\nFirst we get some basic information from the datasets. Things I am interesting in are **rows**, **columns** and **cells without data**. This will help us to have some interesting information like:\n\n 1. The size of our dataset.\n 2. The different features we have.\n 3. How much data is missing from each feature.\n\nFor future reference, these are column's descriptions:\n\n    survival        Survival\n                    (0 = No; 1 = Yes)\n    pclass          Passenger Class\n                    (1 = 1st; 2 = 2nd; 3 = 3rd)\n    name            Name\n    sex             Sex\n    age             Age\n    sibsp           Number of Siblings/Spouses Aboard\n    parch           Number of Parents/Children Aboard\n    ticket          Ticket Number\n    fare            Passenger Fare\n    cabin           Cabin\n    embarked        Port of Embarkation\n                    (C = Cherbourg; Q = Queenstown; S = Southampton)"},{"cell_type":"markdown","metadata":{"_cell_guid":"6dc7eef5-1627-2c23-679f-055c11bd08b2"},"source":"This is the information we have in the training data set."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7bc29c3c-e3cd-b8f1-28b8-ba2714aab2ea"},"outputs":[],"source":"# Count the number of rows\nprint(\"*** Number of rows: \" + str(train.shape[0]))\ntotal = train.shape[0]\nprint(\"\\n\")\n\n# List all the columns\nprint(\"*** Columns: \" + str(train.columns.values), end=\"\\n\")\n\n# Count the number of NaNs each column has.\nprint(\"\\n*** NaNs per column:\")\nprint(pd.isnull(train).sum())"},{"cell_type":"markdown","metadata":{"_cell_guid":"914638dd-0a78-b544-b18f-08ab6c358b24"},"source":"###Passenger's Gender###\n\nThe following section checks the **gender distribution** and the **survival percentage**. Data output:\n\n 1. Passengers per gender and the percentage over total number of passengers\n 2. Passengers that survived per gender and the percentage over the total number of passengers per gender"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5c927efa-f17e-8134-510d-419106fd90a2"},"outputs":[],"source":"# Change gender's text to integers\ndata[\"Sex\"] = data[\"Sex\"].apply(sex)\n\n# Draw survival per sex\nsurvival_rate(\"Sex\",\"barh\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"a986ae66-9f67-e6de-1545-5130051c9ee3"},"source":"###Passengers' Class###\n\nThe following section checks the **class distribution** and the **survival percentage**.  Data output:\n\n 1. Passengers per class and the percentage over total number of passengers.\n 2. Passengers that survived per class and the percentage over the total number of passengers per class."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5d10082d-05e7-1ae6-2062-f2d1a8ba33d7"},"outputs":[],"source":"# Draw survival per Class\nsurvival_rate(\"Pclass\",\"barh\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"392cff4e-3cee-44bf-72f1-91ed746b879d"},"source":"###Passengers' Age###\n\nThe following section checks the **age distribution**. First some data manipulation will be done:\n\n 1. The data set contain ages less than 1 for those people with only months of life. Those will be changed to 1.\n 2. Create new column with buckets of ages, in 10 years ranges.\n\nThen, the usual graph representation showing the ages in groups and the survival rate."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4015590f-9f34-4720-b386-924b2cf50f5b"},"outputs":[],"source":"print(\"*** Number of people with age less than 1 (months):\")\nprint(train[train[\"Age\"] < 1.0].shape[0])\n\n# Those with age <1, changed to 1\ndata['Age'] = data['Age'].apply(normalize_age_below_one)\n\n# Create new feature with data in buckets\ndata[\"AgeGroup\"] = data[\"Age\"].apply(group_age)\ntrain[\"AgeGroup\"] = train[\"Age\"].apply(group_age)\n\n# Draw survival per age group\nsurvival_rate(\"AgeGroup\",\"bar\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"7bb8debe-8601-a4d1-a7db-5d6dd80fceb7"},"source":"###Passengers' Fare###\n\nThe following section checks the **fare distribution** and the **average fare per class**. The analysis shows that fare is mostly related with \"Class\", so no need to check the survival rate since survival per class has been already analysed."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2b239722-69d4-0fbe-330d-a94dbfad2cd2"},"outputs":[],"source":"# Get Fare statistics\nprint(\"*** Fare statistics:\")\nprint(train[\"Fare\"].describe())\n\n# Seems that some people paid nothing:\nprint(\"\\n*** People with fare 0:\")\nnothing = train[train[\"Fare\"] == 0]\nprint(nothing[[\"Name\",\"Sex\",\"Age\",\"Pclass\",\"Survived\"]])\n\n# Graph average Fare per Class\ntrain.groupby(\"Pclass\").mean()['Fare'].plot(kind=\"bar\",title=\"Average Fare per Class\")\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"299a34f2-f835-51b7-1797-376ec182ac59"},"source":"## Passengers' Port of Embarkation ##\n\nThe following section checks the **port distribution** and the **survival percentage**."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3c5175bd-7632-9763-bdbc-da450bdab87d"},"outputs":[],"source":"# Change embarkation data type to integers\ndata[\"Embarked\"] = data[\"Embarked\"].apply(embarked)\n\n# Graph survived per port of embarkation\nsurvival_rate(\"Embarked\",\"bar\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"299e0096-dcd2-4d84-dc83-2b5634cdf8d2"},"source":"## Family members ##\n\nThe following section checks the **family distribution**. First some data manipulation will be done:\n\n - New column, **FamilyMembers** will be added. It counts the number of family members of that particular passenger.\n\nI will also check large families (more than 5 members) and see if there are families where no member survived at all.\n\nThen, the usual graph representation showing the family members and the survival rate."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b43c6e4d-461a-ce01-9ab8-67135e4f20b7"},"outputs":[],"source":"data[\"FamilyMembers\"]=data[\"SibSp\"]+data[\"Parch\"]\ntrain[\"FamilyMembers\"]=train[\"SibSp\"]+data[\"Parch\"]\n\nprint(\"*** Family statistics, members:\")\nprint(\"Min: \" + str(train[\"FamilyMembers\"].min()))\nprint(\"Average: \" + str(round(train[\"FamilyMembers\"].mean(),2)))\nprint(\"Max: \" + str(train[\"FamilyMembers\"].max()), end=\"\\n\\n\")\n\nprint(\"*** Average family members per Class:\")\nprint(train.groupby(\"Pclass\").mean()['FamilyMembers'], end=\"\\n\\n\")\n\n# Families with more than 5 members\nlarge_families=train[train[\"FamilyMembers\"]>= 5]\nlarge_families_by_ticket=large_families.groupby(\"Ticket\").sum()['Survived']\nprint(\"*** Large families by ticket. Did all family die?:\")\nprint(large_families_by_ticket==0, end=\"\\n\\n\")\n\n# Largest family where all members died\nlargest_family_ticket=train[\"Ticket\"][train[\"FamilyMembers\"]==10].iloc[0]\nname=train[\"Name\"][train[\"Ticket\"]==largest_family_ticket].iloc[0]\nprint(\"*** Largest family, all members died: \"+ name.split(\",\")[0], end=\"\\n\\n\")\n# More info: http://www.bbc.com/news/uk-england-cambridgeshire-17596264\n\nsurvival_rate(\"FamilyMembers\",\"bar\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"cbfeb677-35ca-3899-e138-01aae0da704f"},"source":"## Passenger's Tickets ##\n\nAs we have seen in previous section, tickets hide some information that could be valuable. Every member of the same family has the same ticket number. That means that we can use that information to in some way group people by family.\n\nThere is no single ticket identification, but the most common one are numbers. Therefore, a regex will help is to get those and store in a new column."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0c930f55-d56e-c153-b517-f4346926b905"},"outputs":[],"source":"train[\"Ticket\"].head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6e6d569e-afa7-1015-f248-a5a588cb924e"},"outputs":[],"source":"data[\"TicketClean\"] = data[\"Ticket\"].str.extract('(\\d{2,})', expand=True)\ndata[\"TicketClean\"].head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"4d0def15-4a73-c878-3c53-a56df5f55107"},"source":"There are 8 NaN tickets because they didn't have a number in them. We are going to manually assign those."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ddad82f2-b4b0-5503-0953-e6ea141b2a5b"},"outputs":[],"source":"print(\"Rows with NaN: \" + str(pd.isnull(data[\"TicketClean\"]).nonzero()[0]))\nprint(\"Ticket number: \")\nprint(str(data[\"Ticket\"].ix[179]))\nprint(str(data[\"Ticket\"].ix[271]))\nprint(str(data[\"Ticket\"].ix[302]))\nprint(str(data[\"Ticket\"].ix[597]))\nprint(str(data[\"Ticket\"].ix[772]))\nprint(str(data[\"Ticket\"].ix[841]))\nprint(str(data[\"Ticket\"].ix[1077]))\nprint(str(data[\"Ticket\"].ix[1193]))"},{"cell_type":"markdown","metadata":{"_cell_guid":"99ae536d-6531-6502-c3b9-e9dd49c5f54a"},"source":"To cleanup the data I am going to:\n\n - Convert \"TicketClean\" to number.\n - Assign median value to the first group.\n - Assign median+std() to the second group.\n - Assign median-std() to the third group.\n - Then manually update the values."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"55aed2b1-0b44-0dc5-6f4d-af153841b35a"},"outputs":[],"source":"data[\"TicketClean\"] = data[\"Ticket\"].str.extract('(\\d{3,})', expand=True)\ndata[\"TicketClean\"] = data[\"TicketClean\"].apply(pd.to_numeric)\nmed1=data[\"TicketClean\"].median()\nmed2=data[\"TicketClean\"].median()+data[\"TicketClean\"].std()\nmed3=data[\"TicketClean\"].median()-data[\"TicketClean\"].std()\ndata.set_value(179, 'TicketClean', int(med1))\ndata.set_value(271, 'TicketClean', int(med1))\ndata.set_value(302, 'TicketClean', int(med1))\ndata.set_value(597, 'TicketClean', int(med1))\ndata.set_value(772, 'TicketClean', int(med2))\ndata.set_value(841, 'TicketClean', int(med2))\ndata.set_value(1077, 'TicketClean', int(med2))\ndata.set_value(1193, 'TicketClean', int(med2))\ndata[\"TicketClean\"].head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"a8fa882c-eede-2622-597a-e7921568c9d2"},"source":"## Passenger's Name ##\n\nNames also hide some valuable information. Like family name, and also tittle abbreviations like Mr., Miss. and so on. I am going to extract those titles and examine them to see if there is something useful we can do with it."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a9a295e3-4108-e46c-0121-857f662eac30"},"outputs":[],"source":"data[\"TitleClean\"] = data[\"Name\"].str.extract('(\\w*\\.)', expand=True)\ndata.groupby(data[\"TitleClean\"]).size()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"abff5c1c-6356-d3af-d3d4-36e7249d0ce1"},"outputs":[],"source":"data[\"TitleClean\"] = data[\"TitleClean\"].apply(title_to_int)"},{"cell_type":"markdown","metadata":{"_cell_guid":"6170bb9a-f1df-2491-8550-1c915fbf7b94"},"source":"## Machine Learning ##"},{"cell_type":"markdown","metadata":{"_cell_guid":"353db6e3-46db-91aa-4087-a5c0963d8014"},"source":"### Data Balance ###\n\nIt is important to check if our data is balanced. That means that in this binary classification problem we should have the same number of rows for each possible outcome. If the data is imbalanced, our accuracy score could be wrong and the model could have problems to generalise. \n\nThe graph shows that our data is imbalanced, so the things we can do are:\n\n - Use a different score. Apart from Accuracy and F-Score, I will also check Cohen's Kappa.\n\n**Cohen’s kappa**: Classification accuracy normalized by the imbalance of the classes in the data.\nhttp://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/\n\n - **TODO**. Resample the data adding copies of instances from the under-represented class ."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7799566f-1196-171c-fb18-61c9f9aa3aa1"},"outputs":[],"source":"df=pd.DataFrame()\ndf['total']=train.groupby(\"Survived\").size()\ndf=df['total']/train.shape[0]\ndf.plot(kind=\"bar\",title=\"Label's Balance\")\naxes = plt.gca()\naxes.set_ylim([0,1])\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"1a4003e5-5fb5-93a6-2555-90ab998b2684"},"source":"### Data Preparation ###\n\nFirst some data cleaning:\n\n - Drop useless columns.\n - Fill NaN ages and Fare with average from \"Title\" and \"Pclass\" groups.\n - Separate features and labels"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"aa8eb27c-95b5-2f1d-a6ed-b2e80fe1a686"},"outputs":[],"source":"data.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c73ed4af-ad93-f09f-7cba-5264138bb076"},"outputs":[],"source":"remove=['Name','Cabin','Ticket', 'AgeGroup']\nfor column in remove:\n    data = data.drop(column, 1)\n\n# Add missing ages. If there is a NaN, change it with the average for that title group.\nlist_nan=pd.isnull(data[\"Age\"]).nonzero()\n# Get a pd with the mean age for each title\nmeans = data.groupby(\"TitleClean\").mean()['Age']\n# for each row with NaN, we write the average\nfor i in list_nan[0]:\n    temp_title = data[\"TitleClean\"].ix[i]\n    data.set_value(i, 'Age', int(means[temp_title]))\n\n# Add missing fare. If there is a NaN, change it with the average for that Pclass.\nlist_nan=pd.isnull(data[\"Fare\"]).nonzero()\n# Get a pd with the mean age for each title\nmeans = data.groupby(\"Pclass\").mean()['Fare']\n# for each row with NaN, we write the average\nfor i in list_nan[0]:\n    temp_class = data[\"Pclass\"].ix[i]\n    data.set_value(i, 'Fare', int(means[temp_class]))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"41a65867-58d1-91b7-ca38-89b39e5bf32c"},"outputs":[],"source":"# Prepare features\ntrain=data[data['Survived'].isin([0, 1])]\n#labels=train[\"Survived\"]\ntrain=train.drop(\"Survived\", 1)\ntrain=train.drop('PassengerId', 1)\nfeatures=train\n\n# Prepare testing data\ntest=data[~data['Survived'].isin([0, 1])]\ntest=test.drop(\"Survived\", 1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"45c4c218-dfc0-4602-c558-13bf053142cf"},"outputs":[],"source":"lets_try(NL=False).show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"9ac69345-7b5c-4cf7-d0cd-9ba8dcef29ab"},"source":"Seems that Random Forest and AdaBoost perform better. Both allow us to extract information about what are the most important features to take a decision. In the next graph we see which are the most important features for RandomForest."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"061a3cea-37e2-4709-ad1c-feef6c8f5608"},"outputs":[],"source":"def draw_best_features():\n    clf=RandomForestClassifier()\n    clf.fit(features,labels)\n    importances = clf.feature_importances_\n    names=features.columns.values\n\n    pd.Series(importances*100, index=names).plot(kind=\"bar\")\n    plt.show()\n    \ndraw_best_features()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c06a3460-a8ec-df7c-be40-ad0a28f563ad"},"outputs":[],"source":"# Now let's test only with relevant features\n#best_features=[\"Pclass\",\"Sex\",\"Age\",\"Fare\",\"FamilyMembers\", \"TicketClean\", \"TitleClean\"]\nbest_features=[\"Pclass\",\"Sex\",\"Age\",\"Fare\", \"TicketClean\", \"TitleClean\"]\nfeatures=features[best_features]\nfeatures.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"04d9e55a-8233-15e4-ba21-1ea6589785cf"},"source":"Most of the models require the data to be standardised, so I am going to use a scaler and then check the scores again. There should be a huge difference."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6de46884-17af-39de-162d-93d87ab926b1"},"outputs":[],"source":"scaler = MinMaxScaler()\nfeatures_backup=features\nfeatures = scaler.fit_transform(features)\npd.DataFrame(features).head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"da62d333-fd5a-47a3-8d94-ce4e397851b4"},"outputs":[],"source":"lets_try(NL=False).show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7c6170df-f20b-ca3c-e496-412394e572ef"},"outputs":[],"source":"features=features_backup\ncv = KFold(n_splits=5)\n\nparameters = {'n_estimators': [10,20,30,40,50],\n               'min_samples_split' :[2,3,4,5],\n               'min_samples_leaf' : [1,2,3]\n             }\n\nclf = RandomForestClassifier()\ngrid_obj = GridSearchCV(clf, parameters, cv=cv)\ngrid_fit = grid_obj.fit(features, labels)\nbest_clf = grid_fit.best_estimator_ \n\nbest_clf.fit(features,labels)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"02a94a89-dcbe-3d39-f27c-f0df298e23ca"},"outputs":[],"source":"PassengerId=test[\"PassengerId\"]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1dc5fb5a-d00e-2107-f288-230ccffc83f9"},"outputs":[],"source":"#remove=['PassengerId','SibSp', 'Parch', 'Embarked']\n#for column in remove:\n#    test = test.drop(column, 1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b6cc3196-b06b-412d-f825-247f3f7b2aed"},"outputs":[],"source":"test=test[best_features]\ntest.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"594ae932-4abe-31bc-d8dd-4c8a9442f321"},"outputs":[],"source":"predictions=best_clf.predict(test)\n\nsub = pd.DataFrame({\n        \"PassengerId\": PassengerId,\n        \"Survived\": predictions\n    })\nsub.to_csv(\"titanic_submission.csv\", index=False)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ed58d59d-a5d1-9050-5998-874f618d9194"},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b3fc7065-8e03-8a66-24d0-9af08c4c103f"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}