{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <a id = \"stepend\"> Table of contents </a>\n1. [Introduction](#1)\n2. [Kaggle micro-courses study plan](#2)\n3. [The solution of the problem](#3)\n    * [Defining Libraries and Data Path](#4)\n    * [Deal with Missing values](#5)\n    * [Deal with Categorical Variables](#6)\n    * [Defining and testing a model](#7)\n    * [Submit to competiton](#8)\n8. [Conclusion](#9)","metadata":{}},{"cell_type":"markdown","source":"# <a id = \"1\"> 1. Introduction</a>\n### What is it about?\nGood day, dear Kagglers! I wrote this notebook as a kind of logical conclusion to the study of Kaggle micro-courses. It uses a variety of techniques to solve the Titanic problem from all the different Kaggle micro-courses for beginners. You can simply familiarize yourself with this notebook and find those topics that, in your opinion, are worth repeating (i will give all the links to the micro-courses used).\n\nYou can use this solution for training, the main advantage of this solution is its simplicity. It will not use a large number of complex analysis methods or the creation of synthetic variables (such notebooks can be found in sufficient quantities) - this solution is aimed at obtaining decent model accuracy using the basic principles. Thus, you will be able to see the simple logic of data processing and model building.üëç\n### Who is it for?\nThis notebook can be extremely useful for absolutely novice users, as well as those who are interested in mastering some theoretical aspects.\n","metadata":{}},{"cell_type":"markdown","source":"# <a id = \"2\"> 2. Kaggle micro-courses study plan</a>\n### How do I get started on the Kaggle platform?\nFirst, I would like to leave here my subjective view of the order of studying Kaggle micro-courses. In my opinion, these micro-courses are incredibly practical. You can stop anywhere, on any line code in a particular course and start your own exploration on the selected dataset. You can find a complete list of micro-courses available on this platform [here](https://www.kaggle.com/learn/overview).\n### <br/> Here we go:\n1. [Hello, Python!](https://www.kaggle.com/learn/python)\n<br/> One of the main programming languages used in Data Science is Python. The presented course can be started to study even with a zero level of knowledge of the programming language. Actually, it was from this level that I started learning Python.üòä\nThe course is really interesting, and the practical tasks are really challenging.üí™\n2. [Pandas](https://www.kaggle.com/learn/pandas)\n<br/> While Python is the most popular programming language in the data science world, *Pandas* is the most popular Python library for data analysis. In this course, you can learn more about the functions and methods applicable to data frames. How to create a table, how to extract the necessary information from it, and much more.\n3. [Data-visualization](https://www.kaggle.com/learn/data-visualization)\n<br/> In this micro-course, you will learn the basics of data visualization using the *Seaborn* library. The presented course describes in great detail all the tools necessary for work, explaining literally every word in a line of code to you! It is also a very colorful course, because the resulting visuals look amazingly interesting. You can master data visualization without the first two micro-courses, but nevertheless, I recommend that you first study the base of the programming language. Data visualization is very interesting and, if used wisely, can reveal interesting dependencies, tell a previously unknown story about the presented data.\n4. [Intro-to-machine-learning](https://www.kaggle.com/learn/intro-to-machine-learning)\n<br/> This course introduces us to the basic machine learning models that we need to predict key values. The course provides an introduction to classification and regression models such as: *Decision Tree Model* and *Random Forest*. Here we will also get acquainted with the problem of *underfitting* and *overfitting* models, methods of assessing its accuracy. Of course, this micro-course will not give you the mathematical foundations of the presented methods, but as I said earlier, this is pure practice.‚ú® You can immediately apply these models to real data!\n5. [Intermediate-machine-learning](https://www.kaggle.com/learn/intermediate-machine-learning)\n<br/> This course is also a must, as it raises critical issues that are encountered in real-world data. In the Titanic problem, we will encounter data gaps, as well as categorical variables that our model does not want to perceive in their pure form.üôÑ In this course, you will also learn about *pipelines*, advanced model checking techniques (*cross-validation*), the problem of *data leakage*, and explore a new model that is widely used in one form or another in Kaggle's competitions - *XGBoost* üî•.\n6. [Feature-engineering](https://www.kaggle.com/learn/feature-engineering)\n<br/> I strongly recommend starting this course only after all the previous ones)üòÇ Surely you have seen an important part of any analysis of the problem with the title *Feature Engineering*, but what is it? In fact, this is the creation of new, synthetic variables based on the presented ones, which will improve the predictive performance of the model, as well as increase the interpretability of the results.\n\n[\"Return to Monke\"](#stepend)üôâ","metadata":{}},{"cell_type":"markdown","source":"# <a id = \"3\"> 3. The solution of the problem</a>\n\n## <a id = \"4\"> Defining Libraries and Data Path</a>\nIn this cell, we will define all the libraries we need for working with data, methods for filling in the gaps, using various models, and so on. In fact, this block is often filled in during your analysis, when you understand that we need one or another method.\n\nHere we read all our data and write them under the names *train_data* and *test_data*, respectively.\n\nThen we determine the predictive feature *y*. And also we will determine the specific columns with which we will work, I did not use *Ticket* and *Cabin* features here. Although they hide interesting patterns in themselves, they require more complex processing, while giving a slight improvement to the model (you can add or remove some features at your discretionüòâ).","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\ntrain_data = pd.read_csv(\"../input/titanic/train.csv\") \ntest_data = pd.read_csv(\"../input/titanic/test.csv\") \n\ny = train_data[\"Survived\"]\nfeatures = [\"Sex\",\"Pclass\", \"SibSp\", \"Parch\", \"Age\", \"Fare\", \"Embarked\"] #Instead of .dropna i'll use exact features due to the small amount of columns\n\nX_train = train_data[features]\nX_test = test_data[features]\nX_train.head() # Shows the first five lines of our selected data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <a id = \"5\"> Deal with Missing values</a>\nSo, we have defined specific features. Among them there may be *missing values*, which, of course, we will not see in the first lines).The first step in processing the data is to fill in the gaps! (of course, after choosing the main features that you select based on the analysis of all data, for example, with visualization tools, we must first assess the scale of our future actions). Moreover, it is necessary to note an interesting fact:\n>Missing values may not necessarily be lost data, sometimes it may indicate that this feature simply does not exist or it cannot be obtained, which is already some interesting pattern. For example, the column may be called *Pets of passengers* and among *Cats*, *Dogs* and *Parrots*, there may be missing values that are not reasonable to fill in with *Cats* (for example), they will mean that the passenger has no pets ü§∑‚Äç‚ôÄÔ∏è)\n\nTherefore, in this block we will **search for missing values** (more about how these lines of code work can be found in the micro-course [Intermediate-machine-learning. Missing-values](https://www.kaggle.com/alexisbcook/missing-values)):\n","metadata":{}},{"cell_type":"code","source":"# Study missing values:\n\nmissin_v_count_by_columns_train = (X_train.isnull().sum())\nmissin_v_count_by_columns_test = (X_test.isnull().sum())\nprint(\"Missing in train_data:\",'\\n',missin_v_count_by_columns_train[missin_v_count_by_columns_train > 0], '\\nMissing in train_data:','\\n',missin_v_count_by_columns_test[missin_v_count_by_columns_test > 0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we see that there are missing values in both *training* and *test data*, both in columns with *categorical variables* ('Embarked') and with *numerical* ones. That is why, when imputuing values, we must process both *training* and *test data*, otherwise you will train the model without errors, and at the stage of applying the model on all data, you will get an error.ü§î More about how to impute values can be found in the micro-course [Intermediate-machine-learning. Missing-values](https://www.kaggle.com/alexisbcook/missing-values). However, I will immediately note that in this course we are introduced to the *SimpleImputer method*, which is imputed to the entire DataFrame with a single rule for filling in the missing data. However, I want to use different approaches for imputing data to different columns, so I use *.fillna*. You can easily understand how it works from the structure of the code. Now let's **impute the missing values** :","metadata":{}},{"cell_type":"code","source":"# Impute them right there:\nX_train = X_train.fillna(value = {'Age': X_train['Age'].mean(), 'Fare': X_train['Fare'].median(), 'Embarked': 'N'})\nX_test = X_test.fillna(value = {'Age': X_test['Age'].mean(), 'Fare': X_test['Fare'].median(), 'Embarked': 'N'})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's check it out again:\n\nmissin_v_count_by_columns_train = (X_train.isnull().sum())\nmissin_v_count_by_columns_test = (X_test.isnull().sum())\nprint(\"Missing in train_data:\",'\\n',missin_v_count_by_columns_train[missin_v_count_by_columns_train > 0], '\\nMissing in train_data:','\\n',missin_v_count_by_columns_test[missin_v_count_by_columns_test > 0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now there are no missing valuesüòé","metadata":{}},{"cell_type":"markdown","source":"## <a id = \"6\"> Deal with Categorical Variables</a>\n\nAs we noted earlier, categorical data is contained in the 'Embarked' column. However, the 'Sex' column also contains categorical values. More about how to deal with Categorical Variables can be found in the micro-course [Intermediate-machine-learning. Categorical Variables](https://www.kaggle.com/alexisbcook/categorical-variables). In this micro-course, various ways of processing category values are given: simple *Drop* (drop such columns and ignore them), *LabelEncoding* and *OneHotEncoding*. In order to understand the difference between them, be sure to read the presented course. Empirically, I have determined that the best result, in this case, will be when using the *OneHotEncoding* approach to 'Embarked' column and *LabelEncoding* to 'Sex'. However, in order not to use this bulky preprocessing method, I use the alternative *.get_dummies* method (*.factorize()* can be an alternative to *LabelEncoding*). Now let's **preprocess Categorical Variables 'Sex'**:","metadata":{}},{"cell_type":"code","source":"label_encoder = LabelEncoder()\nfor col in ['Sex']:\n    X_train[col] = label_encoder.fit_transform(X_train[col])\n    X_test[col] = label_encoder.transform(X_test[col])\nX_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Compare to the very first output, we replaced the 'male' and 'female' values with '1' and '0' respectively using LabelEncoder(). Now let's apply *.get_dummies*. Please note that I use this method at the very end, when all other data is already numerical! Since the *.get_dummies* method will turn all categorical columns of the DataFrame into sets of columns with corresponding values. **Preprocess Categorical Variables 'Embarked'**:","metadata":{}},{"cell_type":"code","source":"X_train = pd.get_dummies(X_train)\nX_test = pd.get_dummies(X_test)\nX_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It was easy, wasn't it?üòé BUT wait ... let's check the test data:","metadata":{}},{"cell_type":"code","source":"X_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"üò® Have you noticed? We got a different number of columns in test and train data... our statistical model will not like it very much( \n<br/>Guess why it happened? (You can expand the answer below).","metadata":{}},{"cell_type":"markdown","source":"The fact is that, when we checked for missing values, it turned out that there are missing values in the 'Embarked' column in the *training data* (that is, a set of unique values: *'Nan'*, *'C'*, *'Q'* and *'S'*. But in the test there were no such missing values (only *'C'*, *'Q'* and *'S'*) That is why after applying the *OneHotEncoding* method, we received a different number of columns.","metadata":{"_kg_hide-input":true}},{"cell_type":"markdown","source":"It's time to use the knowledge of Pandas!üòÇ Shortly, we need to create such a column entirely of zeros, but in addition to that, put it in the right place! (in order):","metadata":{}},{"cell_type":"code","source":"X_test['Embarked_N'] = 0 # Creating a zeros column","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols2 = X_test.columns.tolist() # List of column names\nlista = [cols2[9]] # Link to missing column \ncols2 = cols2[:7] + lista + cols2[7:9] # Redefine the column numbering order\nX_test = X_test[cols2] # Applying the new order\nX_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Great, now everything is identical!üéâ","metadata":{}},{"cell_type":"markdown","source":"## <a id = \"7\"> Defining and testing a model</a>\nIn this block, we will divide our training data once again into training and test data. More details about this are discussed here: [Intro to Machine Learning. Model Validation](https://www.kaggle.com/dansbecker/model-validation). Short, models can be overfitted and split data allows this to be determined. In this way, we fit the model on a part of the training data, and on the other part we check their accuracy. In the next section, we will fit the model on the selected model on all training data.","metadata":{}},{"cell_type":"code","source":"# Split Data\ns_X_train, s_X_test, s_y_train, s_y_test = train_test_split(X_train, y, train_size=0.75, test_size=0.25, random_state=0)\n\n# Now, let's define and test different models! You can change and test any parametres here.\n#1\nmodel_1 = XGBClassifier(n_estimators = 10000, learning_rate = 0.5, use_label_encoder=False, eval_metric = 'logloss')\nmodel_1.fit(s_X_train, s_y_train, early_stopping_rounds = 9, eval_set = [(s_X_test, s_y_test)], verbose = False)\n#2\nmodel_2 = RandomForestClassifier(n_estimators = 550, max_depth = 6, random_state = 1)\nmodel_2.fit(s_X_train, s_y_train)\n#3\nmodel_3 = DecisionTreeClassifier(max_leaf_nodes = 100, random_state = 1)\nmodel_3.fit(s_X_train, s_y_train)\n\nval_predictions_1 = model_1.predict(s_X_test) # Model_1 prediction\nval_predictions_2 = model_2.predict(s_X_test) # Model_2 prediction\nval_predictions_3 = model_3.predict(s_X_test) # Model_3 prediction\n\n#This class is called 'Dictionaries'\naccuracy_results = {'XGBClassifier': accuracy_score(val_predictions_1, s_y_test), 'RandomForestClassifier': accuracy_score(val_predictions_2, s_y_test),'DecisionTreeClassifier': accuracy_score(val_predictions_3, s_y_test),}\naccuracy_results","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <a id = \"8\"> Submit to competiton</a>\nBased on the validation results (validation based on 25% of the training data), we can see that the XGBClassifier model has the best result, however I submit a random forest model.üôÉ","metadata":{}},{"cell_type":"code","source":"model_2.fit(X_train, y) # Fit model_2 on all train data\npredictions = model_2.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <a id = \"9\"> 4. Conclusion</a>\nHope you enjoyed this notebook. You are free to use it for your solution) I would be extremely grateful for any feedback, recommendations, additions and / or suggestions, as well as for your opinion on the presentation.üòä\n\n[\"Worm-hole\"](#stepend)üí´","metadata":{}}]}