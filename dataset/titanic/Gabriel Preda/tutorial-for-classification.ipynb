{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1><center><font size=\"6\">Tutorial for Classification</font></center></h1>\n\n<h2><center><font size=\"4\">Dataset used: Titanic - Machine Learning from Disaster</font></center></h2>\n\n<br>\n\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/fd/RMS_Titanic_3.jpg/640px-RMS_Titanic_3.jpg\" width=\"450\"></img>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import datetime\nprint(\"Last updated:\")\nprint(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7f8c8e161894460c1b704a60c0d5e1d215dab648"},"cell_type":"markdown","source":"\n\n# <a id='0'>Content</a>\n\n- <a href='#1'>Introduction</a>  \n- <a href='#2'>Prepare the data analysis</a>  \n - <a href='#21'>Load packages</a>  \n - <a href='#22'>Load the data</a>   \n- <a href='#3'>Data exploration</a>   \n - <a href='#31'>Check for missing data</a>  \n - <a href='#32'>Sex, Age, SibSp, Parch</a>   \n - <a href='#33'>Fare, Embarked, Pclass</a>  \n - <a href='#34'>Ticket, Cabin, Name</a>   \n - <a href='#35'>Multiple features visualization</a>   \n - <a href='#36'>Imputation of missing data</a>   \n- <a href='#4'>Feature engineering</a>\n - <a href='#41'>Extract Title from Name</a>\n - <a href='#42'>Build families</a>\n - <a href='#43'>Extract Deck from Ticket</a>  \n - <a href='#44'>Estimate age</a>  \n - <a href='#45'>More features engineering</a>  \n- <a href='#5'>Predictive model for survival</a>\n - <a href='#50'>Split the data</a>  \n - <a href='#51'>Build a baseline model</a>  \n - <a href='#52'>Model evaluation</a>    \n - <a href='#53'>Model refinement</a> \n - <a href='#54'>Submission</a>  \n - <a href='#55'>Hyperparameters optimization</a>\n - <a href='#56'>Submission (model with hyperparameters optimization)</a>  \n- <a href='#6'>Model ensambling</a>\n - <a href='#61'>Create the ensamble framework</a>\n - <a href='#62'>Create the Out-Of-Fold Predictions</a>\n - <a href='#63'>Train the first level models</a>\n - <a href='#64'>Correlation of the results</a>\n - <a href='#65'>Build the second level (ensamble) model</a>\n - <a href='#66'>Submission (ensamble)</a>\n- <a href='#7'>References</a>    "},{"metadata":{"_uuid":"6594845937ca7b3a74dc9c31b0effcf8086f673c"},"cell_type":"markdown","source":"# <a id='1'>Introduction</a>  \n\nThis Kernel will take you through the process of **analyzing the data** to understand the **predictive values** of various **features** and the possible correlation between different features, **selection of features** with predictive value, **features engineering** to create features with higher predictive value, creation of a **baseline model**, succesive **refinement** of the model (we are using **RandomForest**) through selection of features and, at the end, **submission** of the best solution found. \n\nNext, we take the model and define a multi-dimmensional matrix of **hyperparameters** we would like to test. We use Gradient Search and cross-validation to select the best set of hyperparameters. The best model is then used for the **second submission**.\n\nNext, we will use **ensambling** (second level model trained with the output of first level models). We create several models (with the same set of parameters used with the previous model). We use **AdaBoost**, **CatBoost**, **ExtraTrees**, **GradientBoosting**, **RandomForest** and **SupportVectorMachines**. We are using **Out-Of-Folds** to avoid the risk that the base model predictions already having \"seen\" the test set and therefore overfitting when feeding these predictions. The Out-Of-Folds are concatenated and feed to the second level model (XGBoost Classifier) and the prediction using the second level model is submitted (**third submission**) as the solution.\n\nThe dataset used for this tutorial is the famous now **Titanic** dataset.\n\n\n<a href=\"#0\"><font size=\"1\">Go to top</font></a>  "},{"metadata":{"_uuid":"49a4847daa9eea7afc1f005412a574ef86d4f207"},"cell_type":"markdown","source":"# <a id='2'>Prepare the data analysis</a>   \n\n\nBefore starting the analysis, we need to make few preparation: load the packages, load and inspect the data.\n"},{"metadata":{"_uuid":"68cfa776b89a8027e06887ce2993894d75d56f08"},"cell_type":"markdown","source":"## <a id='21'>Load packages</a>\n\nWe load the packages used for the analysis. There are packages for data manipulation, visualization, models, hyperparameter optimization and model metrics.."},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport sys\nimport os\nimport random\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nfrom plotly import tools\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\nimport scipy\nfrom sklearn.model_selection import train_test_split, KFold, GridSearchCV\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.svm import SVC\nimport xgboost as xgb","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a77dd46df07463a57632c7401f34f7ceaea7fa21"},"cell_type":"markdown","source":"<a href=\"#0\"><font size=\"1\">Go to top</font></a>  \n\n\n## <a id='22'>Load the data</a>  \n\nLet's see first what data files do we have in the root directory. "},{"metadata":{"_uuid":"9e9c5345f22b50981187f45c219bcb6bbbe6458c","trusted":true},"cell_type":"code","source":"IS_LOCAL = False\nif(IS_LOCAL):\n    PATH=\"../input/titanic/\"\nelse:\n    PATH=\"../input/\"\nos.listdir(PATH)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"18e369d32c82d3580db42faee68f44402f412d34"},"cell_type":"markdown","source":"There are **train** and **test** data as well as an example **submission** file.  \n\nLet's load the **train** and **test** data."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_kg_hide-input":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_df=pd.read_csv(PATH+'train.csv')\ntest_df=pd.read_csv(PATH+'test.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"55b41e17833da924475d38f9a8c22a1731efb8ab"},"cell_type":"markdown","source":"<a href=\"#0\"><font size=\"1\">Go to top</font></a>  \n\n# <a id='3'>Data exploration</a>  \n\nWe check the shape of train and test dataframes and also show a selection of rows, to have an initial image of the data.\n\n"},{"metadata":{"_kg_hide-input":true,"_uuid":"469fd9a6bba1521569b762a5030a1e1a50bff313","trusted":true},"cell_type":"code","source":"train_df.sample(5).head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"ec12616d56dfbf5a6d40d5c1f692321e7260e461","trusted":true},"cell_type":"code","source":"test_df.sample(5).head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"0f3a03fce399eaeb0226728af285486d6f22abcd","trusted":true},"cell_type":"code","source":"print(\"Train: rows:{} cols:{}\".format(train_df.shape[0], train_df.shape[1]))\nprint(\"Test:  rows:{} cols:{}\".format(test_df.shape[0], test_df.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ccf999716af89faf72b86be24e800f41e38e48ec"},"cell_type":"markdown","source":"Both **train** and **test** files contains the following values:  \n\n* **PassengerID** - the index of the passenger (in the dataset);  \n* **PClass** - the class of the passenger (from 1 to 3);\n* **Name** - the name of the passenger;\n* **Sex** - the sex of the passenger (female or male);  \n* **Age** - the age (where available) of the passenger;  \n* **SibSp** - the number of sibilings / spouses aboard of Titanic;  \n* **Parch** - the number of parents / children aboard of Titanic;  \n* **Ticket** - the ticket number;  \n* **Fare** - the passenger fare (ticket cost);  \n* **Cabin** - the cabin number;  \n* **Embarked** - the place of embarcation of the passenger (C = Cherbourg, Q = Queenstown, S = Southampton).  \n\nThe **train** data has as well the target value, **Survived**.\n\nIt is important, before going to create a model, to have a good understanding of the data. We will therefore explore the various features."},{"metadata":{"_uuid":"0b743ae9ca2a88bce058759520878a2a0a76688a"},"cell_type":"markdown","source":"\nLet's start by checking if there are missing data, unlabeled data or data that is inconsistently labeled. "},{"metadata":{"_uuid":"1acb5da96b0721ab020f34e8ee03862d7d36047d"},"cell_type":"markdown","source":"## <a id='31'>Check for missing data</a>  \n\nLet's create a function that check for missing data in the two datasets (train and test)."},{"metadata":{"_kg_hide-input":true,"_uuid":"0615b432baf1b9f3e98986861ae1c4fb966ba6f9","trusted":true},"cell_type":"code","source":"def missing_data(data):\n    total = data.isnull().sum().sort_values(ascending = False)\n    percent = (data.isnull().sum()/data.isnull().count()*100).sort_values(ascending = False)\n    return pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data(train_df)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"d1f198f06d50726a247df1056fe38b0f50f64605","trusted":true},"cell_type":"code","source":"missing_data(test_df)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"27520c91b12f01e61c1adc4b682ba97b5b20a464"},"cell_type":"markdown","source":"Both in **train** and **test** datasets, `Cabin` has more than 77% missing, `Age` more than 19%. `Embarked` is missing in 2 cases for **train** and `Fare` misses in  1 case for **test**.   \n\nWe will discuss, through this tutorial, the various possible methods to deal with missing data."},{"metadata":{"_uuid":"08afb0eb7021f23df57da4f514e2674b8a8e7167"},"cell_type":"markdown","source":"## <a id='32'>Sex, Age, SibSp, Parch</a>  \n\nLet's check now the data (in **train** and **test**) for `Sex`, `Age`, `SibSp` and `Parch`."},{"metadata":{"_kg_hide-input":true,"_uuid":"baf67bed4b863387ebc45668dba34f08bec2bb57","trusted":true},"cell_type":"code","source":"def get_categories(data, val):\n    tmp = data[val].value_counts()\n    return pd.DataFrame(data={'Number': tmp.values}, index=tmp.index).reset_index()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"291d9d7313de6666745e45a87121fee640f32565","trusted":true},"cell_type":"code","source":"def get_survived_categories(data, val):\n    tmp = data.groupby('Survived')[val].value_counts()\n    return pd.DataFrame(data={'Number': tmp.values}, index=tmp.index).reset_index()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"e0cd0060d230e89f7eab373e67285a49b0202ee0","trusted":true},"cell_type":"code","source":"def draw_trace_bar(data_df,color='Blue'):\n    trace = go.Bar(\n            x = data_df['index'],\n            y = data_df['Number'],\n            marker=dict(color=color),\n            text=data_df['index']\n        )\n    return trace\n\n\ndef plot_bar(data_df, title, xlab, ylab,color='Blue'):\n    trace = draw_trace_bar(data_df, color)\n    data = [trace]\n    layout = dict(title = title,\n              xaxis = dict(title = xlab, showticklabels=True, tickangle=0,\n                          tickfont=dict(\n                            size=10,\n                            color='black'),), \n              yaxis = dict(title = ylab),\n              hovermode = 'closest'\n             )\n    fig = dict(data = data, layout = layout)\n    iplot(fig, filename='draw_trace')\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"6d1c9260105211670fe096654053aeac0a18b94b","trusted":true},"cell_type":"code","source":"def plot_two_bar(data_df1, data_df2, title1, title2, xlab, ylab):\n    trace1 = draw_trace_bar(data_df1, color='Blue')\n    trace2 = draw_trace_bar(data_df2, color='Lightblue')\n    \n    fig = tools.make_subplots(rows=1,cols=2, subplot_titles=(title1,title2))\n    fig.append_trace(trace1,1,1)\n    fig.append_trace(trace2,1,2)\n    \n    fig['layout']['xaxis'].update(title = xlab)\n    fig['layout']['xaxis2'].update(title = xlab)\n    fig['layout']['yaxis'].update(title = ylab)\n    fig['layout']['yaxis2'].update(title = ylab)\n    fig['layout'].update(showlegend=False)\n    \n\n    iplot(fig, filename='draw_trace')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"3e5656ce09c835adb51b7cc989bd80fddce089d8","trusted":true},"cell_type":"code","source":"def plot_survived_bar(data_df, var, ytitle= 'Number of passengers',title= 'Number of survived and not survived passengers by {}'):\n    dfS = data_df[data_df['Survived']==1]\n    dfN = data_df[data_df['Survived']==0]\n\n    traceS = go.Bar(\n        x = dfS[var],y = dfS['Number'],\n        name='Survived',\n        marker=dict(color=\"Blue\"),\n        text=dfS['Number']\n    )\n    traceN = go.Bar(\n        x = dfN[var],y = dfN['Number'],\n        name='Not survived',\n        marker=dict(color=\"Red\"),\n        text=dfS['Number']\n    )\n    \n    data = [traceS, traceN]\n    layout = dict(title = title.format(var),\n          xaxis = dict(title = var, showticklabels=True), \n          yaxis = dict(title = ytitle),\n          hovermode = 'closest'\n    )\n    fig = dict(data=data, layout=layout)\n   \n    iplot(fig, filename='draw_trace')\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"ef956653735c6393f7cc00f21fd88181614b14e3","scrolled":true,"trusted":true},"cell_type":"code","source":"plot_two_bar(get_categories(train_df,'Sex'), get_categories(test_df,'Sex'), \n             'Train data', 'Test data',\n             'Sex', 'Number of passengers')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"12ad593826f751138414efb85bd49b26afdd0523","trusted":true},"cell_type":"code","source":"plot_survived_bar(get_survived_categories(train_df,'Sex'), 'Sex')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"410a8017f065abc6ca809debe6d6314e29d9ec46"},"cell_type":"markdown","source":"From the total female passengers, 74% survived.  \nIn the same time, from the total male passengers, only 18% survived."},{"metadata":{"_kg_hide-input":true,"_uuid":"a9234e51a8ae3a0fa25a1441c752fe32e598d27e","trusted":true},"cell_type":"code","source":"plot_two_bar(get_categories(train_df,'Age'), get_categories(test_df,'Age'), \n             'Train data', 'Test data',\n             'Age', 'Number of passengers')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"43eda2b0a24c47b49e32d93c718c20a46b68741e","trusted":true},"cell_type":"code","source":"plot_survived_bar(get_survived_categories(train_df,'Age'), 'Age')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c96432c1179308eb55ace58d41c8df2722664655"},"cell_type":"markdown","source":"Majority of the passengers were between 20 and 35 years old.\n\nThe survival rate for the age interval 15-35 years was quite small.\n"},{"metadata":{"_kg_hide-input":true,"_uuid":"fa595dad9118aa5607301e030c997c2eca911870","trusted":true},"cell_type":"code","source":"plot_two_bar(get_categories(train_df,'SibSp'), get_categories(test_df,'SibSp'), \n             'Train data', 'Test data',\n             'SibSp', 'Number of passengers')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"56bafb5900498cd0a6976a6b654908f71654b7ff","trusted":true},"cell_type":"code","source":"plot_survived_bar(get_survived_categories(train_df,'SibSp'), 'SibSp')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"65649a08cf438eb245f38deea5336ee8dc4372b5"},"cell_type":"markdown","source":"Most of the passengers traveled alone. From the passengers travelling alone, only 34% survived.\nThe passengers with only one or two sibbilings survived in around 50% of the cases. \nSurvival rates decrease considerably for number of sibilings or spouses of 3, 4, 5 and is practically 0 for 8."},{"metadata":{"_kg_hide-input":true,"_uuid":"a88f2322adf59b09c78bc5c09d6fab21aa38157f","trusted":true},"cell_type":"code","source":"plot_two_bar(get_categories(train_df,'Parch'), get_categories(test_df,'Parch'), \n             'Train data', 'Test data',\n             'Parch', 'Number of passengers')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"f62d0ebd81297ec413df821761398bf149db7490","trusted":true},"cell_type":"code","source":"plot_survived_bar(get_survived_categories(train_df,'Parch'), 'Parch')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7063a42d6a32b91c2513473b30356a2f85ae2632"},"cell_type":"markdown","source":"For the number of children or parents equal to 0, the survival rate is only 34%. \nFor values of 1,2 and 3, the survival rate is 50%, decreasing for larger numbers."},{"metadata":{"_uuid":"beadc271afdbc97fc2d6256a1b33227c72a8d220"},"cell_type":"markdown","source":"<a href=\"#0\"><font size=\"1\">Go to top</font></a>  \n\n\n## <a id='33'>Fare, Embarked, Pclass</a>  \n\nLet's check now the data (in **train** and **test**) for  `Fare`,  `Embarked` and`Pclass`.  \n"},{"metadata":{"_kg_hide-input":true,"_uuid":"a5f2d4023df0f25f83919c529b6c9f31ae14696c","trusted":true},"cell_type":"code","source":"def draw_trace_histogram(data_df,color='Blue'):\n    trace = go.Histogram(\n            x = data_df['index'],\n            y = data_df['Number'],\n            marker=dict(color=color),\n            text=data_df['index']\n        )\n    return trace\n\ndef plot_two_histogram(data_df1, data_df2, title1, title2, xlab, ylab):\n    trace1 = draw_trace_histogram(data_df1, color='Blue')\n    trace2 = draw_trace_histogram(data_df2, color='Lightblue')\n    \n    fig = tools.make_subplots(rows=1,cols=2, subplot_titles=(title1,title2))\n    fig.append_trace(trace1,1,1)\n    fig.append_trace(trace2,1,2)\n    \n    fig['layout']['xaxis'].update(title = xlab)\n    fig['layout']['xaxis2'].update(title = xlab)\n    fig['layout']['yaxis'].update(title = ylab)\n    fig['layout']['yaxis2'].update(title = ylab)\n    fig['layout'].update(showlegend=False)\n    \n\n    iplot(fig, filename='draw_trace')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"f809c3efea796f0f812d941e56c0b6979cadddf4","trusted":true},"cell_type":"code","source":"def plot_survived_histogram(data_df, var):\n    dfS = data_df[data_df['Survived']==1]\n    dfN = data_df[data_df['Survived']==0]\n\n    traceS = go.Histogram(\n        x = dfS[var],y = dfS['Number'],\n        name='Survived',\n        marker=dict(color=\"Blue\"),\n        text=dfS['Number']\n    )\n    traceN = go.Histogram(\n        x = dfN[var],y = dfN['Number'],\n        name='Not survived',\n        marker=dict(color=\"Red\"),\n        text=dfS['Number']\n    )\n    \n    data = [traceS, traceN]\n    layout = dict(title = 'Number of survived and not survived passengers by {}'.format(var),\n          xaxis = dict(title = var, showticklabels=True), \n          yaxis = dict(title = 'Number of passengers'),\n          hovermode = 'closest'\n    )\n    fig = dict(data=data, layout=layout)\n   \n    iplot(fig, filename='draw_trace')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"969931c0d90f219ff6b2dd655ee7d158524a6f8c","trusted":true},"cell_type":"code","source":"plot_two_histogram(get_categories(train_df,'Fare'), get_categories(test_df,'Fare'), \n             'Train data', 'Test data',\n             'Fare', 'Passengers')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2ba46c12c4562381f9170488807efb0de4646158","trusted":true},"cell_type":"code","source":"plot_survived_histogram(get_survived_categories(train_df,'Fare'), 'Fare')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d9cfb82fe1aec0bfd7059fc9acee802e1ad3c8d7"},"cell_type":"markdown","source":"Survival rate increases considerably with the fare value. This confirm the image that richer people survived better. We will validate this observation as well with the class information."},{"metadata":{"_kg_hide-input":true,"_uuid":"6b10b93cd27aaab89a46f3081ccea65755f627f2","trusted":true},"cell_type":"code","source":"plot_two_bar(get_categories(train_df,'Embarked'), get_categories(test_df,'Embarked'), \n             'Train data', 'Test data',\n             'Embarked', 'Number of passengers')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"284086d43e459d034f0253db40011cc0fa9e0ae2","trusted":true},"cell_type":"code","source":"plot_survived_bar(get_survived_categories(train_df,'Embarked'), 'Embarked')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6bf8466810ebf60c7a94a8da50038dc2e69061bb"},"cell_type":"markdown","source":"The best survival rate is for passengers embarked in Cherbourg (more than 50%), the worst for passengers embarked in Southampton."},{"metadata":{"_kg_hide-input":true,"_uuid":"b4dc07259f02a6a54c7e7c78761acb508f5ce20d","trusted":true},"cell_type":"code","source":"plot_two_bar(get_categories(train_df,'Pclass'), get_categories(test_df,'Pclass'), \n             'Train data', 'Test data',\n             'Pclass', 'Number of passengers')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"abfe69f93b3bd771fb0f52b42cc3089f450a2537","trusted":true},"cell_type":"code","source":"plot_survived_bar(get_survived_categories(train_df,'Pclass'), 'Pclass')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6a5a7c3f52c07b5a93667995bc8b986c5217be7b"},"cell_type":"markdown","source":"First class passengers survived in a percent of 63% while less than 50% survived in 2nd class. For passengers in 3rd class, only 24% survived."},{"metadata":{"_uuid":"2128582bebda46da7aa38b77eb301c5ad6349800"},"cell_type":"markdown","source":"<a href=\"#0\"><font size=\"1\">Go to top</font></a>  \n\n\n## <a id='34'>Ticket, Cabin, Name</a>  \n\nLet's check now the data (in **train** and **test**) for  `Ticket`,  `Cabin` and`Name`.\n\nAll these are alphanumeric (contains both letters and numbers), like `Ticket` and `Cabin` or are text fields (`Name`). \n\nWe will have to process them in order to use as features.  \n\nLet's look to `Ticket` first.\n\n"},{"metadata":{"_kg_hide-input":true,"_uuid":"5cfa22b4569176890efec40c5efb6b3a7df3cd17","trusted":true},"cell_type":"code","source":"train_df['Ticket'].value_counts().head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c49a090be88c94feae98fa8027db5746642512dc"},"cell_type":"markdown","source":"`Ticket` has, most probably, little predictive value.  \n\nLet's see also the `Cabin`.\n"},{"metadata":{"_kg_hide-input":true,"_uuid":"0185e98719e1494391f3e25272a6b25db1ed5189","trusted":true},"cell_type":"code","source":"train_df['Cabin'].value_counts().head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1a728117a8f8e6d666770f0f156148891d5d3f7e"},"cell_type":"markdown","source":"`Cabin` has the first letter that is, most probably, giving the information on the deck. This might have a predictive value and we will process further in the next sections.\n\n`Name` might contain multiple information. Let's check few of the `Name` fields."},{"metadata":{"_kg_hide-input":true,"_uuid":"3e5848c8ed21a4bc6dd1d708d05f9d382e786a95","trusted":true},"cell_type":"code","source":"train_df['Name'].head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d87e9cc399d48d852d230bfa49178172ed1080f6"},"cell_type":"markdown","source":"We see that in the `Name` field we have the Family name, the title (which might indicate as well the social status or marital status), and the first name. So, `Name` is actually a quite rich feature column, which can be further exploited (and we will exploit in the next sections). "},{"metadata":{"_uuid":"142771608f9e8acc61cb7ebd5bb22fc7d3415d01"},"cell_type":"markdown","source":"## <a id='35'>Multiple features visualization</a>\n\nLet's show the number of survived/not survived passengers grouped by Class and Sex."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"0b80d493f483b8845df778a1dc834bcf955550fe"},"cell_type":"code","source":"tmp = train_df.groupby(['Pclass', 'Sex'])['Survived'].value_counts()\ndf = pd.DataFrame(data={'Passengers': tmp.values}, index=tmp.index).reset_index()\nhover_text = []\nfor index, row in df.iterrows():\n    hover_text.append(('Pclass: {}<br>'+\n                      'Sex: {}<br>'+\n                      'Survived: {}<br>'+\n                      'Passengers: {}').format(row['Pclass'],\n                                            row['Sex'],\n                                            row['Survived'],\n                                            row['Passengers']))\ndf['hover_text'] = hover_text","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"scrolled":true,"_uuid":"0c1683b71ffe2a9b14b7facdcb77711fc0761b9e"},"cell_type":"code","source":"trace = go.Scatter(\n        x=df['Pclass'],\n        y=df['Sex'],\n        text=df['hover_text'],\n        mode='markers',\n        marker=dict(\n            sizemode='diameter',\n            sizeref=3,\n            size=df['Passengers'],\n            color = df['Survived'],\n            colorscale = 'Bluered',\n        )\n    )\ndata = [trace]\n\nlayout = dict(title = 'Number of surviving/not surviving passengers by class and sex',\n          xaxis = dict(title = 'Class', showticklabels=True, type='category'), \n          yaxis = dict(title = 'Sex', type='category'),            \n          hovermode = 'closest',\n              height=400, width=600, \n         )\nfig=go.Figure(data=data, layout=layout)\niplot(fig, filename='bubble_plot')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d59d0289110f35d431dcb9d95349856e88e75233"},"cell_type":"markdown","source":"\nLet's show the number of survived/not survived passengers grouped by SibSp and Parch."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"2378198815b3be2ead393dd1093540d0a2105d68"},"cell_type":"code","source":"tmp = train_df.groupby(['SibSp', 'Parch'])['Survived'].value_counts()\ndf = pd.DataFrame(data={'Passengers': tmp.values}, index=tmp.index).reset_index()\nhover_text = []\nfor index, row in df.iterrows():\n    hover_text.append(('Sibilings: {}<br>'+\n                      'Parents/Children: {}<br>'+\n                      'Survived: {}<br>'+\n                      'Passengers: {}').format(row['SibSp'],\n                                            row['Parch'],\n                                            row['Survived'],\n                                            row['Passengers']))\ndf['hover_text'] = hover_text","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"1134dfbabe95183c6402d51d7a2bbef87c740368"},"cell_type":"code","source":"trace = go.Scatter(\n        x=df['SibSp'],\n        y=df['Parch'],\n        text=df['hover_text'],\n        mode='markers',\n        marker=dict(\n            sizemode='diameter',\n            sizeref=4,\n            size=df['Passengers'],\n            color = df['Survived'],\n            colorscale = 'Bluered',\n        )\n    )\ndata = [trace]\n\nlayout = dict(title = 'Passengers by number of Sibilings and  parents/children',\n          xaxis = dict(title = 'Sibilings', showticklabels=True, type='category'), \n          yaxis = dict(title = 'Parents/Children', type='category'),            \n          hovermode = 'closest',\n              height=400, width=600, \n         )\nfig=go.Figure(data=data, layout=layout)\niplot(fig, filename='bubble_plot')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id='36'>Imputation of missing data</a>  \n\nWe are creating a model for imputation of **Fare** data.  \n\nFirst, let's create a list with **train** and **test** datasets, to process both in the same time.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df['Survived'] = None\nall_df = pd.concat([train_df, test_df], axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Combined data: {all_df.shape}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's create a Decision Tree model to predict missing Fare value."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\ndef encrypt_single_column(data):\n    le = LabelEncoder()\n    le.fit(data.astype(str))\n    return le.transform(data.astype(str))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = ['Pclass','Sex','Embarked','SibSp','Parch']\n\nfor feature in features:\n    all_df[feature] = encrypt_single_column(all_df[feature])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = all_df.loc[~(all_df.Fare.isna())]\ny = X['Fare'].values\nX = X[features]\nX_test = all_df.loc[all_df.Fare.isna()]    \nX_test = X_test[features]\nprint(f'X: {X.shape} y: {y.shape}, X_text: {X_test.shape}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn import tree\nclf = DecisionTreeRegressor()\nclf.fit(X, y)\ny_test = clf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Fare: {y_test}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We replace the predicted value in the original (combined) data."},{"metadata":{"trusted":true},"cell_type":"code","source":"all_df.loc[all_df.Fare.isna(), 'Fare'] = y_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_df.loc[all_df.Fare.isna()].shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9f0e3c892cf3d82a01bbb8e92effa2cd5c134ccd"},"cell_type":"markdown","source":"<a href=\"#0\"><font size=\"1\">Go to top</font></a>  \n\n\n# <a id='4'>Features engineering</a>\n\nFrom the original features, we will create new features."},{"metadata":{"_uuid":"184fc740108c8b67d26893683cbd96e113694bb0"},"cell_type":"markdown","source":"## <a id='41'>Extract Title from Name</a>\n\nLet's start with processing the names. We will extract the title from the names.  \n\nWe create now a list of datasets (we name it as well all_df, we reuse this name):\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_df = [train_df, test_df]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"92a136c10a3b4fe9e08bb6b1272396d09774a8f0"},"cell_type":"markdown","source":"We apply the rule for extracting the title."},{"metadata":{"_kg_hide-input":true,"_uuid":"f1bc182e85d3179845ea189977dabe644c8d0f38","trusted":true},"cell_type":"code","source":"for dataset in all_df:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"046433853aff13b9224b17e6522561894c39ee6a"},"cell_type":"markdown","source":"Let's verify the relationship between `Title` and `Sex`."},{"metadata":{"_kg_hide-input":true,"_uuid":"fa1cd597441f76531639b5ffff32228f64d5f14c","trusted":true},"cell_type":"code","source":"np.transpose(pd.crosstab(train_df['Title'], train_df['Sex']))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ef91a9a66a23cb7cc19c8301c7a5ccc7df7a2d34"},"cell_type":"markdown","source":"There are male only titles: `Capt`, `Col`, `Don`, `Jonkheer`, `Major`, `Master`, `Mr`, `Rev` and `Sir`. \nAs well, there are female only titles: `Countess`, `Lady`, `Miss`,  `Mlle`,  `Mme`, `Mrs`, `Ms`.\nThere is a female `Dr` (and other 6 males).\n\nMost of these titles are quite rare. We will either group them as `Rare` or correct them (for example, to reunite all young womens with the title `Miss`.\n\nLet's start by grouping all `Miss` and `Mrs` variations under a single name."},{"metadata":{"_kg_hide-input":true,"_uuid":"38b223acf164183892610076ca3fde3b259e4c4a","trusted":true},"cell_type":"code","source":"for dataset in all_df:\n    #unify `Miss`\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    #unify `Mrs`\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"23aa39a21b01e7aec7b1c63d025c84ea97eb01b9"},"cell_type":"markdown","source":"Then, let's set the female Dr as one of the female tipical roles."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"214e6f1e4fec28b1e3f3a55822fc5a9907f20a6b"},"cell_type":"code","source":"train_df[(train_df['Title'] == 'Dr') & (train_df['Sex'] == 'female')]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ccd2a83e213e63d472d97f7c60f72c7716f3c056"},"cell_type":"markdown","source":"She is traveling in Cabin `D17` in 1st class. Let's see if she is alone."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"14785b66e43755e038bb8b4883ef4ea307a998b1"},"cell_type":"code","source":"train_df[train_df['Cabin']=='D17']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f46f4cf0e4b159201772351ac0429a7512ef835a"},"cell_type":"markdown","source":"Because she is traveling with a friend about the same age and with a `Mrs` title, we might want to set her as well as a `Mrs`. Let's do it."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"2dbf305220077f681bcb55e53790e95522d9e864"},"cell_type":"code","source":"train_df.loc[train_df.PassengerId == 797, 'Title'] = 'Mrs'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8e64fd541fd7ff88c90ccaedf44e23ceb4417282"},"cell_type":"markdown","source":"Let's check if this worked well."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"c530102f71c2147b1f975994cc2e73e4c21b488b"},"cell_type":"code","source":"train_df[train_df['Cabin']=='D17']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3d797774cfcb16f4eab6914bcd4f35b26510e239"},"cell_type":"markdown","source":"We succesfully set passenger #797 as a `Mrs`.\n\nLet's also group all the rare titles under a `Rare` title:"},{"metadata":{"_kg_hide-input":true,"_uuid":"e7e26e05723591cffd2b7c56c8dbb30d2d1ddc91","trusted":true},"cell_type":"code","source":"for dataset in all_df:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n     'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6e4318b5049cb82cbb1f9549ed064c65156e1efe"},"cell_type":"markdown","source":"Let's verify the average survival ratio for the passengers with the aggregated titles and sex."},{"metadata":{"_kg_hide-input":true,"_uuid":"550cb26ece596a8d74f5991d4a52f58e38846c37","trusted":true},"cell_type":"code","source":"train_df[['Title', 'Sex', 'Survived']].groupby(['Title', 'Sex'], as_index=False).mean()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"c15ca9e979c04417d03399c812d51968638cde12","trusted":true},"cell_type":"code","source":"plot_survived_bar(get_survived_categories(train_df,'Title'), 'Title')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cce87bab2e5326bdc6aa27ae7824956bdd61e24c"},"cell_type":"markdown","source":"  All rare female titles were saved.   \n  Married females had the highest survival rate besides these very rare cases, of 79%. Young unmarried women followed with 70% survival rate. \n  Lowest survival rate had the men with `Mr` title. \n  Between men, the ones with `Master` title had a much higher survival rate than  these, with 57%."},{"metadata":{"_uuid":"f7dd85a52125024e48ceb50607d38afd265f3eec"},"cell_type":"markdown","source":"## <a id='42'>Build families</a>\n\n\n### Calculate Family Size\n\nFrom `SibSp` and `Parch` we create a new feature, `FamilySize`."},{"metadata":{"_kg_hide-input":true,"_uuid":"8e4fde3d3eed531a8f0cd9b3e225ebe3359ab913","trusted":true},"cell_type":"code","source":"for dataset in all_df:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8838f8e94514e83a7b7d92efd9775fd6f7411d81"},"cell_type":"markdown","source":"Let's check the correlation between family size and survival rate."},{"metadata":{"_kg_hide-input":true,"_uuid":"388f322d49487c11b179b38c75707a7f4d0091f1","trusted":true},"cell_type":"code","source":"train_df[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c252647e56851bff81fe9ccf292c4364c8dbdc36","trusted":true},"cell_type":"code","source":"plot_survived_bar(get_survived_categories(train_df,'FamilySize'), 'FamilySize')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5f346e7c026551bad1e5039f54927656dd5e7a7f"},"cell_type":"markdown","source":"Unmarried passengers and passengers with very large families had the lowest survival rate (30% singles and less than 20% the family members with families larger than 5). This can be explained by the fact that singles were most probably mens in lower classes while families with small number of children most probably saved at least a part of them (for example the mother with the childrens) cooperating between them to ensure salvation. Large families might had lower survival rates due to various reasons, including maybe difficulties to coordinate or more difficult decision on whom to embark on the boats (if not possible for all to embark). Another reason for larger families to survive might be related with the class."},{"metadata":{"_uuid":"c52997ee94789acfbd284a4361341d5675ccd1e1"},"cell_type":"markdown","source":"### Identify families by surname\n\nLet's also try to aggregate families by surname. For this, we will extract the surname from Name."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"2549247af8f87698fa271a1e08376f77d71b6856"},"cell_type":"code","source":"for dataset in all_df:\n    dataset['Surname'] = dataset.Name.str.extract('([A-Za-z]+)\\,', expand=False)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"5298ac133c43dc9566f09fc7beb8246a9a284a04"},"cell_type":"code","source":"tmp = train_df.groupby(['Surname'])['Survived'].value_counts()\ndf = pd.DataFrame(data={'Size of group with same Surname': tmp.values}, index=tmp.index).reset_index().sort_values(['Size of group with same Surname', 'Surname'], ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"a6e0092b464c6fed310054173fa1fe6a283d590e"},"cell_type":"code","source":"tmp = df.groupby(['Size of group with same Surname'])['Survived'].value_counts()\ndf = pd.DataFrame(data={'Number': tmp.values}, index=tmp.index).reset_index().sort_values(['Size of group with same Surname', 'Survived'], ascending=False)\ndf","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"76f7488045b171461fa89987bdd029b691e87ad6"},"cell_type":"code","source":"plot_survived_bar(df, 'Size of group with same Surname', ytitle= 'Number of groups', title= 'Number of survived and not survived groups by {}')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f14779497d7de9f660246a66e0761d6937254474"},"cell_type":"markdown","source":"The above graph is not showing the number of families survived or not survived. It is grouping family members that survived and family members that did not survived by Surname (family name). We did not checked if there are several different families with the same name or a family appears with a part of the family members in the survived lot and with a part of the family in the not survived lot (which we know it happens frequently, or example adult men in inferior class did not survived at all)."},{"metadata":{"_uuid":"92ee3e5caaba78546984b7101ec978dd5e365ef4"},"cell_type":"markdown","source":"## <a id='43'>Extract Deck from Cabin</a>\n\nWe will extract the deck name from the Cabin name by separating the first character from each cabin name. \nUnfortunatelly, a very small number of passengers have `Cabin` information therefore also the `Deck` information will be only available for a reduced number of passengers."},{"metadata":{"_kg_hide-input":true,"_uuid":"a419ab2c6d32ac80ba269377b49857b8cf1ca99d","trusted":true},"cell_type":"code","source":"for dataset in all_df:\n    dataset['Deck'] = dataset.Cabin.str.extract('^([A-Za-z]+)', expand=False)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"8aa5b474eaf677823c22edcc7d25568864c2d58f","trusted":true},"cell_type":"code","source":"train_df[['Deck', 'Survived']].groupby(['Deck'], as_index=False).mean()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"081e381175d120ed11abe654e10276fdfb8be69b","trusted":true},"cell_type":"code","source":"plot_survived_bar(get_survived_categories(train_df,'Deck'), 'Deck')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a7071236ce17086f3b5bfe84428730c2273251ca"},"cell_type":"markdown","source":"The plot shows just the data for the `Deck` information that could be extracted i.e. where a `Cabin` was defined."},{"metadata":{"_uuid":"1a7c7c8bb79f0a309d795dd713a909f625db5638"},"cell_type":"markdown","source":"## <a id='44'>Estimate age</a>\n\nA relativelly large number of passengers have missing Age information. We will try to estimate this missing information from other available data, `Sex` and `Pclass`. Before doing this, we will also map sex values to numeric values."},{"metadata":{"_kg_hide-input":true,"_uuid":"ef45167393ffbb99794eff61a52bd26a22e9f9ac","trusted":true},"cell_type":"code","source":"for dataset in all_df:\n    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"76f66412c21fc9ee877b42ea0f3103e99986598f","trusted":true},"cell_type":"code","source":"age_aprox = np.zeros((2,3))\nfor dataset in all_df:\n    for i in range(0, 2):\n        for j in range(0, 3):\n            aprox_age = dataset[(dataset['Sex'] == i) & \\\n                                  (dataset['Pclass'] == j+1)]['Age'].dropna()\n            age_aprox[i,j] = aprox_age.median()\n            \n    for i in range(0, 2):\n        for j in range(0, 3):\n            dataset.loc[ (dataset.Age.isnull()) & (dataset.Sex == i) & (dataset.Pclass == j+1),'Age'] = \\\n                    age_aprox[i,j]\n\n    dataset['Age'] = dataset['Age'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"99d44b4bdb713a97275c80c52e1e5c4d0838350c"},"cell_type":"markdown","source":"Now we replaced the missing Age values with the one obtained from the approximations."},{"metadata":{"_uuid":"783bfdbe9969e65a7d87cb42818857d0911c3614","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"tmp = train_df.groupby(['Title', 'Pclass'])['Survived'].value_counts()\ndf = pd.DataFrame(data={'Passengers': tmp.values}, index=tmp.index).reset_index()\ndf","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ff547b7feaae02eda18ae500b963d76343e26e40"},"cell_type":"markdown","source":"## <a id='44'>More features engineering</a>  \n\n\nLet's map all titles to numeric values."},{"metadata":{"_kg_hide-input":true,"_uuid":"d794b5e6fafc42751e29ecd6a67009c97793618f","trusted":true},"cell_type":"code","source":"title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in all_df:\n    dataset['Title'] = dataset['Title'].map(title_mapping)   ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ac2c5a925a0cd88257e3be625c9dc0b56ef400be"},"cell_type":"markdown","source":"And let's plot the `Title` grouped by the survived and not survived."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"9afbab6c3e8ee0ea53e14ae66814579e9893440f"},"cell_type":"code","source":"plot_survived_bar(get_survived_categories(train_df,'Title'), 'Title')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8ee6e3bdbc987957325bd55b2049abe37bbbd39d"},"cell_type":"markdown","source":"We also map `Fare` to 4 main fare segments and label them from 0 to 3."},{"metadata":{"_kg_hide-input":true,"_uuid":"8ea642c519381ba180c6b620aa85e575a4efca14","trusted":true},"cell_type":"code","source":"for dataset in all_df:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"894d8d92987f7148ed6e0588e7f0eb1a631ce64d"},"cell_type":"markdown","source":"And let's plot the `Fare` clusters grouped by the survived and not survived."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"b9dca2f2ff6e186b824a35109ad6fbd6fb6036ca"},"cell_type":"code","source":"plot_survived_bar(get_survived_categories(train_df,'Fare'), 'Fare')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fca84485ba5ae82cd5f6f8152319b9ce19f48772"},"cell_type":"markdown","source":"Similarly, we map `Age` to 5 main segments, labeled from 0 to 4. "},{"metadata":{"_kg_hide-input":true,"_uuid":"d3ac26effcb49e1a6b7eb64fcaddf1a4137cbbeb","trusted":true},"cell_type":"code","source":"for dataset in all_df:\n    dataset.loc[ dataset['Age'] <= 16, 'Age']  = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"adfc29772fb34071d6684ed4a15b40729977ef2d"},"cell_type":"markdown","source":"And let's plot the `Age` clusters grouped by the survived and not survived."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"39d58b91efde791ad36091eaf7469a735236b023"},"cell_type":"code","source":"plot_survived_bar(get_survived_categories(train_df,'Age'), 'Age')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fa4a01c2f6e60776f6bd9b74396deaa293d0c99e"},"cell_type":"markdown","source":"Family Size is mapped then to only 3 sizes (0 to 2), the first corresponding to the case when someone is alone."},{"metadata":{"_kg_hide-input":true,"_uuid":"189321fc1891bb6ee1bc3d0d186d14bfb30e885c","trusted":true},"cell_type":"code","source":"for dataset in all_df:\n    dataset.loc[ dataset['FamilySize'] <= 1, 'FamilySize'] = 0\n    dataset.loc[(dataset['FamilySize'] > 1) & (dataset['FamilySize'] <= 4), 'FamilySize'] = 1\n    dataset.loc[ dataset['FamilySize'] > 4, 'FamilySize'] = 2","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7d1f3933eb67f1e312c0c7b19008e33ee70fa204"},"cell_type":"markdown","source":"Let's also add one more feature, `Class*Age`, calculated as `Class` x `Age`."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"e8ed69e45dd18ad41db2329fd941e504b0d3e87e"},"cell_type":"code","source":"for dataset in all_df:\n    dataset['Class*Age'] = dataset['Pclass'] * dataset['Age']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ab14baad7daaf5c57812a23bc132f23f3b15d3c2"},"cell_type":"markdown","source":"And let's plot the `Class*Age` grouped by the survived and not survived."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"550e420ecac54615b8a32dbef7f8d3abb03bc133"},"cell_type":"code","source":"plot_survived_bar(get_survived_categories(train_df,'Class*Age'), 'Class*Age')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"20f992c33156e1ba7590df41bffb346ebf4d861f"},"cell_type":"markdown","source":"Let's see now how looks like train and test set."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"507e6eaef20739ed3f921d1a40732e24c3886015"},"cell_type":"code","source":"train_df.head(3)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"ebb5bf0aad88fd2459775baedcb9408aab46692b"},"cell_type":"code","source":"test_df.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a href=\"#0\"><font size=\"1\">Go to top</font></a>  \n\n"},{"metadata":{"_uuid":"93b243ad54ebfa759aaa57e4c3cd4d4a4edd2feb"},"cell_type":"markdown","source":"\n# <a id='5'>Predictive model for survival</a>  \n\nLet's start with creation of the predictive models. We will create a very simple model for starting."},{"metadata":{"_uuid":"674450c7d711046ece6889cbe2923d4b2217e0ea"},"cell_type":"markdown","source":"\n## <a id='50'>Split the data</a>  \n\nLet's split the training and validation set. We will use a 80-20 split.\nWe also set the matrices for train and validation and the vectors with the target values."},{"metadata":{"_kg_hide-input":true,"_uuid":"b4bbf9a7175f4bf24bc39192037768cf5926d04a","trusted":true},"cell_type":"code","source":"#We are using 80-20 split for train-test\nVALID_SIZE = 0.2\n#We also use random state for reproducibility\nRANDOM_STATE = 2018\n\ntrain, valid = train_test_split(train_df, test_size=VALID_SIZE, random_state=RANDOM_STATE, shuffle=True )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"87f2054b07681134ab6ce19d46f955fe5217a6d5"},"cell_type":"markdown","source":"\n## <a id='51'>Build a baseline model</a>  "},{"metadata":{"_uuid":"13d69dc08c0c8efdc4ae890ada0cd0b17f535125"},"cell_type":"markdown","source":"We will start with a simple model, with just few predictors.\n\nLet's set now the predictors list and the target value. We start with two predictors, the `Sex` and `Pclass`."},{"metadata":{"_kg_hide-input":true,"_uuid":"16c8b6d71bb6a63d21aeac2b40dcdc0028614dfc","trusted":true},"cell_type":"code","source":"predictors = ['Sex', 'Age']\ntarget = 'Survived'","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"396cef911773d5c22a17d1e638fe4ed451edc71b","trusted":true},"cell_type":"code","source":"train_X = train[predictors]\ntrain_Y = train[target].values\nvalid_X = valid[predictors]\nvalid_Y = valid[target].values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c525d396d22d8482bd8dcd967e3b68f4e89a24e7"},"cell_type":"markdown","source":"Let's prepare a simple model, using Random Forest. We set few algorithm parameters and initialize a clasiffier."},{"metadata":{"_kg_hide-input":true,"_uuid":"314bb9eb2332cfb4144b3754dbbbbbe87f64936d","trusted":true},"cell_type":"code","source":"RFC_METRIC = 'gini'  #metric used for RandomForrestClassifier\nNUM_ESTIMATORS = 100 #number of estimators used for RandomForrestClassifier\nNO_JOBS = 4 #number of parallel jobs used for RandomForrestClassifier","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"3e2a8a4dd68c6a2e711e68eb438898f812e2b516","trusted":true},"cell_type":"code","source":"clf = RandomForestClassifier(n_jobs=NO_JOBS, \n                             random_state=RANDOM_STATE,\n                             criterion=RFC_METRIC,\n                             n_estimators=NUM_ESTIMATORS,\n                             verbose=False)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"95b4814c796011e289045320cd846b1e6a8d3278"},"cell_type":"markdown","source":"Then, we fit the classifier with the train data prepared before."},{"metadata":{"_uuid":"8723e6de32203b92e2fac0e67603e97c7a9a1cbe","scrolled":true,"trusted":true},"cell_type":"code","source":"clf.fit(train_X, train_Y)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"aa13d6e5de7f2c25ff372cb4d31354090b3ca9a0","trusted":true},"cell_type":"code","source":"preds = clf.predict(valid_X)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fcd21e80ab0d71eb57c837cf6d7b9cd1e1e7bfe7"},"cell_type":"markdown","source":"Let's plot the features importance. This shows the relative importance of the predictors features for the current model. With this information, we are able to select the features we will use for our gradually refined models."},{"metadata":{"_kg_hide-input":true,"_uuid":"bbd67d5478e207ad61d700f3562e713dd8d703c1","trusted":true},"cell_type":"code","source":"def plot_feature_importance():\n    tmp = pd.DataFrame({'Feature': predictors, 'Feature importance': clf.feature_importances_})\n    tmp = tmp.sort_values(by='Feature importance',ascending=False)\n    plt.figure(figsize = (7,4))\n    plt.title('Features importance',fontsize=14)\n    s = sns.barplot(x='Feature',y='Feature importance',data=tmp)\n    s.set_xticklabels(s.get_xticklabels(),rotation=90)\n    plt.show()   ","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"dc8226b1e855234708f0da0550ba7f0909deccec","trusted":true},"cell_type":"code","source":"plot_feature_importance()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5112a31d0bd5c886c0eb2cd83a837cd5f7a943a6"},"cell_type":"markdown","source":"<a href=\"#0\"><font size=\"1\">Go to top</font></a>  \n\n\n## <a id='52'>Evaluate the model</a>  \n\nLet's evaluate the model performance. \n\nWe are evaluating first the accuracy for the train set.\n\n"},{"metadata":{"_kg_hide-input":true,"_uuid":"51ffe3890dfced2f5b5546bc441a69242cd7bc29","trusted":true},"cell_type":"code","source":"clf.score(train_X, train_Y)\nacc = round(clf.score(train_X, train_Y) * 100, 2)\nprint(\"RandomForest accuracy (train set):\", acc)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c0aa302d865f53de7e99bc4e04e73cf2b1b52136"},"cell_type":"markdown","source":"The result means that the total number of correct predictions divided by the total number of examples in the training set is around 0.77. Because we have a binary classification, this means:\n\n$$Accuracy =\\frac{ \\textrm{True Positives} + \\textrm{True Negatives}}{\\textrm{Number of Examples}}$$ \n\n\nThen we evaluate the accuracy for the validation set. "},{"metadata":{"_kg_hide-input":true,"_uuid":"8d8969996a91a883971aee5186d5e567e0f5c2ad","trusted":true},"cell_type":"code","source":"clf.score(valid_X, valid_Y)\nacc = round(clf.score(valid_X, valid_Y) * 100, 2)\nprint(\"RandomForest accuracy (validation set):\", acc)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e4eedee864f9879da33a3c0d0136af9aac129556"},"cell_type":"markdown","source":"The accuracy for the validation is much better than the accuracy for the training set. \nThis means we have higher bias than variation. The model does not learn too well the training set, we are not overfitting (yet). The validation is better, i.e. we are generalizing well. Before improving the variation, we will try to improve the bias, while looking how this affects the variation as well.\n\nLet's plot now the classification report for validation data."},{"metadata":{"_kg_hide-input":true,"_uuid":"bf61729ac99eadd847591246585420c855edc2f3","trusted":true},"cell_type":"code","source":"print(metrics.classification_report(valid_Y, preds, target_names=['Not Survived', 'Survived']))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4e12c06c3cbab2e217d08d96e12a0b23ab48c6b9"},"cell_type":"markdown","source":"There are few values given in this report / each category in the target (`Survived` or `Not survived`):\n\n* **Precision**  \n* **Recall**  \n* **F1-score**  \n\nLet's explain each of them:\n\n* **Precision** identifies the frequency with which a model was correct when predicting the positive class. That is:\n\n$$Precision = \\frac{\\textrm{True Positives}}{\\textrm{True Positives} + \\textrm{False Positives}}$$\n\n* **Recall** answers the following question: Out of all the possible positive labels, how many did the model correctly identify? That is:\n\n$$Recall = \\frac{\\textrm{True Positives}}{\\textrm{True Positives} + \\textrm{False Negatives}}$$\n\n* **F1-score** is the harmonic mean of **precision** and **recall**.\n\n$$\\textrm{F1-score} = 2  \\frac{Precision * Recall}{Precision + Recall}$$\n\n\nPrecison is better for the `Not survived` as well as Recall and F1-score.\n\nLet's also show the confusion matrix."},{"metadata":{"_kg_hide-input":true,"_uuid":"5b16ca41d3807973a35cc683818f3a7a85a38136","trusted":true},"cell_type":"code","source":"def plot_confusion_matrix():\n    cm = pd.crosstab(valid_Y, preds, rownames=['Actual'], colnames=['Predicted'])\n    fig, (ax1) = plt.subplots(ncols=1, figsize=(5,5))\n    sns.heatmap(cm, \n                xticklabels=['Not Survived', 'Survived'],\n                yticklabels=['Not Survived', 'Survived'],\n                annot=True,ax=ax1,\n                linewidths=.2,linecolor=\"Darkblue\", cmap=\"Blues\")\n    plt.title('Confusion Matrix', fontsize=14)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"9de43dafd912619d9b6ff19aae22e207f962c16d","trusted":true},"cell_type":"code","source":"plot_confusion_matrix()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"5be4ce11403bee48ae79dc29e584b2cfc89526eb"},"cell_type":"markdown","source":"<a href=\"#0\"><font size=\"1\">Go to top</font></a>  \n\n## <a id='53'>Model refinement</a> \n\nLet's rebuild the model and add more features to it."},{"metadata":{"_uuid":"9434088cb662ce0ccb826c2f439e861f5850a892"},"cell_type":"markdown","source":"### Model with {Sex, Age, Pclass, Fare} features"},{"metadata":{"_kg_hide-input":true,"_uuid":"99996c033894e740695320ea4031f9d5450ba34a","trusted":true},"cell_type":"code","source":"predictors = ['Sex', 'Age', 'Pclass', 'Fare']\ntarget = 'Survived'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f4cd30e5eb826f9c34df992aded9847ef04d4b76","trusted":true},"cell_type":"code","source":"train_X = train[predictors]\ntrain_Y = train[target].values\nvalid_X = valid[predictors]\nvalid_Y = valid[target].values","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"_uuid":"835c18c27f03e3d8603363881d04b69f4c9c9365"},"cell_type":"markdown","source":"We fit the model."},{"metadata":{"_kg_hide-input":true,"_uuid":"f7255d67539fa2c39da26a9fa972a466f25c018e","trusted":true},"cell_type":"code","source":"clf.fit(train_X, train_Y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"db06161fd9436880e9defd381e6b770dbf95b699"},"cell_type":"markdown","source":"We predict the validation set."},{"metadata":{"_uuid":"b2c214814d5f4532a6a1b779b567e2e8106cd7de","trusted":true},"cell_type":"code","source":"preds = clf.predict(valid_X)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5efcd7af986b9f047e720d4e40d6b02b66e99d32"},"cell_type":"markdown","source":"Let's plot feature importance."},{"metadata":{"_uuid":"534183c6c6f0b1230cfa540a4a1ef34f2137eaef","trusted":true},"cell_type":"code","source":"plot_feature_importance()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"afaa13f731f88af06e6982d59d95dd528153b730"},"cell_type":"markdown","source":"Let's see the accuracy for the training set and for the validation set."},{"metadata":{"_kg_hide-input":true,"_uuid":"3b3235c28ff96fe18a04b0a6667afed89936f99f","trusted":true},"cell_type":"code","source":"clf.score(train_X, train_Y)\nacc = round(clf.score(train_X, train_Y) * 100, 2)\nprint(\"RandomForest accuracy (train set):\", acc)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"de0845bb56baaaa754090144c8bc86c17e67cc2f"},"cell_type":"code","source":"clf.score(valid_X, valid_Y)\nacc = round(clf.score(valid_X, valid_Y) * 100, 2)\nprint(\"RandomForest accuracy (validation set):\", acc)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ac115ea2ad97feffd6de8d6a5c2df2b29cd137b4"},"cell_type":"markdown","source":"Let's see also the classification report for the validation set."},{"metadata":{"_kg_hide-input":true,"_uuid":"309b6dd183bb2f95c3533f8209f6380b44b319db","trusted":true},"cell_type":"code","source":"print(metrics.classification_report(valid_Y, preds, target_names=['Not Survived', 'Survived']))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"697a726cb56b51803a4a9a095a443b44077ad2e5"},"cell_type":"markdown","source":"Here we plot the confusion matrix."},{"metadata":{"_kg_hide-input":true,"_uuid":"d02cd9c8bc289fbfcf114d016933b677bbbed681","trusted":true},"cell_type":"code","source":"plot_confusion_matrix()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d77f3d34c0f625fd2d8caca727cf55a896ae9165"},"cell_type":"markdown","source":"The model performance with train data improved, i.e. the model is now representing better the training set.  The accuracy and precision with the validation set was as well improved. The recall and f1-score are smaller for `Survived`. \n\nLet's repeat the experiment adding more features."},{"metadata":{"_uuid":"7c33474adf279487d71978e346fb78d1dd9fb027"},"cell_type":"markdown","source":"### Model with {Sex, Age, Pclass, Fare, Parch, SibSp} features"},{"metadata":{"_kg_hide-input":true,"_uuid":"54798ed4e2c9750e57a0e7c8df867ccc29674125","trusted":true},"cell_type":"code","source":"predictors = ['Sex', 'Age', 'Pclass', 'Fare', 'Parch', 'SibSp']\ntarget = 'Survived'","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"ab43add580aeafc6cdc2b79ee2a41aefc1ee8e9b","trusted":true},"cell_type":"code","source":"train_X = train[predictors]\ntrain_Y = train[target].values\nvalid_X = valid[predictors]\nvalid_Y = valid[target].values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"58005c620a0e28626ce513c7403aa0bef3e3e9b4"},"cell_type":"markdown","source":"Let's fit the model with the new predictors."},{"metadata":{"_kg_hide-input":true,"_uuid":"d71f3908acedde795948afc4188cc3a80524c41e","trusted":true},"cell_type":"code","source":"clf.fit(train_X, train_Y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5fb25742357103cff3523d818d7f652326b5e20f"},"cell_type":"markdown","source":"We predict the validation set."},{"metadata":{"_uuid":"3e254cab35a5771948a04a4cd8e42c878193f715","trusted":true},"cell_type":"code","source":"preds = clf.predict(valid_X)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"705806453350c615d94c599d61383d44b843e27f"},"cell_type":"markdown","source":"We plot the feature importance."},{"metadata":{"_kg_hide-input":true,"_uuid":"5a2d40772abb38ceef3cb54f6799086d81d2f713","trusted":true},"cell_type":"code","source":"plot_feature_importance()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"24b994937629d9e93038145963855ecea3764e91"},"cell_type":"markdown","source":"**Sex** is the dominant feature, followed by **Pclass** and **Fare**.  \nLet's see the accuracy for the training set and also for the validation set classification."},{"metadata":{"_kg_hide-input":true,"_uuid":"028354be465fafee0cefb19e4c5e4321a02354d3","trusted":true},"cell_type":"code","source":"clf.score(train_X, train_Y)\nacc = round(clf.score(train_X, train_Y) * 100, 2)\nprint(\"RandomForest accuracy (train set):\", acc)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"c510076530a0b93b8a9d1eb5c19bfc000ba6ad88"},"cell_type":"code","source":"clf.score(valid_X, valid_Y)\nacc = round(clf.score(valid_X, valid_Y) * 100, 2)\nprint(\"RandomForest accuracy (validation set):\", acc)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e282db321874b91a4b1b7afeb13139f6923ab3f4"},"cell_type":"markdown","source":"Let's also plot the classification report for the validation set."},{"metadata":{"_kg_hide-input":true,"_uuid":"9e9361488c91242a1a2e64bbddee682184035176","trusted":true},"cell_type":"code","source":"print(metrics.classification_report(valid_Y, preds, target_names=['Not Survived', 'Survived']))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ca77d1bb13117c4de92a650da889236ca84a6a56","trusted":true},"cell_type":"code","source":"plot_confusion_matrix()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"49760919857fb4ff657d08d450bebf49cadd07e2"},"cell_type":"markdown","source":"The accuracy for the train data classification improved further but the accuracy and precision for validation did not improved.    \n\nLet's add few more engineering features.\n\n\n### Model with {Sex, Age, Pclass, Fare, Parch, SibSp, FamilySize, Title} features"},{"metadata":{"_kg_hide-input":true,"_uuid":"168258ce763e26cf5f339479c18e920396a19bce","trusted":true},"cell_type":"code","source":"predictors = ['Sex', 'Age', 'Pclass', 'Fare', 'Parch', 'SibSp', 'FamilySize', 'Title']\ntarget = 'Survived'","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"eb4eefe1d9daba90e58596dc97b5351f556cdf44","trusted":true},"cell_type":"code","source":"train_X = train[predictors]\ntrain_Y = train[target].values\nvalid_X = valid[predictors]\nvalid_Y = valid[target].values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ec1e45ba8ebe16e0a2f240744e4503926e322f16"},"cell_type":"markdown","source":"Let's fit the model with the new data."},{"metadata":{"_kg_hide-input":true,"_uuid":"9b2d134315335a7347e9a0633ac1f57035318033","trusted":true,"scrolled":true},"cell_type":"code","source":"clf.fit(train_X, train_Y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b5d9f804889ca522c8549ec7881eaa8fc45bd414"},"cell_type":"markdown","source":"We predict the validation set."},{"metadata":{"_uuid":"1205525f9e63d27ff1a23c96de2a8cc4a7483431","trusted":true},"cell_type":"code","source":"preds = clf.predict(valid_X)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1fec0a74c32dd51be9f0278801545f3e5a203a5f"},"cell_type":"markdown","source":"Let's plot also the feature importance."},{"metadata":{"_uuid":"310bbfa5ff058a6c094c6a1f6d27d9831b934a79","trusted":true},"cell_type":"code","source":"plot_feature_importance()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2c154c6398e64e8489d744003864489cc7b42169"},"cell_type":"markdown","source":"Let's see the train set and validation set classification accuracy."},{"metadata":{"_uuid":"b305e7b46812567b14de434606946cd480c37743","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"clf.score(train_X, train_Y)\nacc = round(clf.score(train_X, train_Y) * 100, 2)\nprint(\"RandomForest accuracy (train set):\", acc)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"201120ae98e804f35ef4ba155a6baffa7bea3091"},"cell_type":"code","source":"clf.score(valid_X, valid_Y)\nacc = round(clf.score(valid_X, valid_Y) * 100, 2)\nprint(\"RandomForest accuracy (validation set):\", acc)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"14eb1fc9364047bf58c2f84b5e649fd99ebcb0f2"},"cell_type":"markdown","source":"The classification report for the validation set."},{"metadata":{"_uuid":"f95f5a2759bca4e51c270f4129d74e55230c2204","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(metrics.classification_report(valid_Y, preds, target_names=['Not Survived', 'Survived']))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"046aab8db05c0622dc7b6ad75d3d61e5744909c8"},"cell_type":"markdown","source":"The confusion matrix."},{"metadata":{"_uuid":"e1ef3e18104cd2c30cf0bf47196b9ff1f327ba96","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plot_confusion_matrix()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a92ae021fb8eb4bf8eb6a668abc7f4cdcd85c6e4"},"cell_type":"markdown","source":"The training set accuracy improved even more. In the same time, validation score is not very much improved. This indicates that most probably the model where we added more features is overfitting on the training set. Actually, the best model obtained until now was the one with {Sex, Age, Pclass, Fare} features, where accuracies for training set and validation set were 82% and 86% (with quite good precision for all categories and small recall for `Survived`).\n\nLet's try with a simpler model.\n\n\n### Model with {Title, FamilySize, Pclass} features\n\nThe simple model we will try now, with only three features, is actually using only engineered features."},{"metadata":{"_kg_hide-input":true,"_uuid":"62994428db77c1d23ee4f382607f6ca446cbe79f","trusted":true},"cell_type":"code","source":"predictors = ['FamilySize', 'Title', 'Class*Age']\ntarget = 'Survived'","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"9b2aca0c2ee71d2a2b2d463011f044ae03f744d2","trusted":true},"cell_type":"code","source":"train_X = train[predictors]\ntrain_Y = train[target].values\nvalid_X = valid[predictors]\nvalid_Y = valid[target].values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ec695faff41e2b27a68509a8b0352cf8d4c08b4a"},"cell_type":"markdown","source":"We fit the model."},{"metadata":{"_uuid":"400ecde9344f253da24019cb2e135a3e0929f091","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"rf_clf = clf.fit(train_X, train_Y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a2d6e73d40c3ed1708163d961cb6908909577835"},"cell_type":"markdown","source":"Let's plot the parameters for the classifier."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"70ecbd3c8c0535e91df3b1c7f88e10457acd328c"},"cell_type":"code","source":"rf_clf","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e994d125791f85888e06bda7a77a9db46fc0113f"},"cell_type":"markdown","source":"We predict the validation set."},{"metadata":{"_kg_hide-input":true,"_uuid":"45d6012a316f763bc762deab889e32055d8f3ada","trusted":true},"cell_type":"code","source":"preds = clf.predict(valid_X)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f0d94a34dd1ea7354948f8d1746211b74f5ec453"},"cell_type":"markdown","source":"We check the features importance."},{"metadata":{"_kg_hide-output":true,"_uuid":"55236b749f4ebe5016f9547f21e3968106359357","trusted":true},"cell_type":"code","source":"plot_feature_importance()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"17be7b6d2d7608a662825a94a6b63acf72f79c06","trusted":true},"cell_type":"code","source":"clf.score(train_X, train_Y)\nacc = round(clf.score(train_X, train_Y) * 100, 2)\nprint(\"RandomForest accuracy (train set):\", acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c9afa98e02d14a30d7b03f7ab622b392007b2321"},"cell_type":"code","source":"clf.score(valid_X, valid_Y)\nacc = round(clf.score(valid_X, valid_Y) * 100, 2)\nprint(\"RandomForest accuracy (validation set):\", acc)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9bcb50811e1b58491d99dc8a30ab9eae1d402854"},"cell_type":"markdown","source":"The training set classification accuracy is still good, although lower than that of the previous, more complex model.\nThe validation set accuracy is the best obtained until now.\n\n**Title** is the most important feature.  \n\nLet's check now the classification report for the validation set."},{"metadata":{"_uuid":"bcb994919d835389b8543ffd78189bb5ebf63d12","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(metrics.classification_report(valid_Y, preds, target_names=['Not Survived', 'Survived']))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"184723ba4674071dcdf1f1755529b6a9b6f0d28b"},"cell_type":"markdown","source":"We see that we obtained an substantial improvement of the classification precision for the validation set.   Also the recall is better than in the case of the model with best accuracy and precision until now, besides this one.\n\nLet's also plot the confusion matrix."},{"metadata":{"_kg_hide-input":true,"_uuid":"4ebed7400240957350ddf5a5537d885c4bd0e03b","trusted":true},"cell_type":"code","source":"plot_confusion_matrix()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1c26b200a173e0f5ad0185deada22cbc142a7dfe"},"cell_type":"markdown","source":"We will pick this last model for submission.\nIt doesn't have the best accuracy for train set classification but have one of the best precision and accuracy for the validation set and also the smallest recall. \n\nLet's prepare the submission."},{"metadata":{"_uuid":"06388524ac2f53cebe5ff2b613347223f40d01ce"},"cell_type":"markdown","source":"<a href=\"#0\"><font size=\"1\">Go to top</font></a>  \n\n## <a id='54'>Submission</a>\n\n\nFirst, we predict for test data using the trained model."},{"metadata":{"_kg_hide-input":true,"_uuid":"70411d53d5dcd2ccd00faf4c120da1d0b6881c45","trusted":true},"cell_type":"code","source":"test_X = test_df[predictors]\npred_Y = clf.predict(test_X)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6a5cadfa4c4e2e2d4a50a808dab67d5d77641d29"},"cell_type":"markdown","source":"Then, we prepare the submission dataset and export it in the submission file."},{"metadata":{"_kg_hide-input":true,"_uuid":"65495f0be6ecc98865efd665b900855301d988ec","trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({\"PassengerId\": test_df[\"PassengerId\"],\"Survived\": pred_Y})\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e34d15c9b08f1e2c52e823127ae9ba384158a9b2"},"cell_type":"markdown","source":"The precision obtained for the test set is approx. **0.79**."},{"metadata":{"_uuid":"212581ba2ecf56af8b1bad0996d21aa7315c0bfd"},"cell_type":"markdown","source":"<a href=\"#0\"><font size=\"1\">Go to top</font></a>  \n\n## <a id='55'>Hyperparameters optimization</a>\n\n\nLet's continue with tunning the model hyperparameters.   \nWe define a set of parameters with several values and will run an algorithm called Gradient Search to detect the best combination of parameters for our model.  \nFirst, let's fit the model and assign the output of it to **rf_clf**."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"ab3776e3f6f041fc75a733f4b8f96a5db6265b7b"},"cell_type":"code","source":"rf_clf = clf.fit(train_X, train_Y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"92905a6c1130d8cf7b9ec316bc1a68a33cdc87bd"},"cell_type":"markdown","source":"Let's initialize the GradientSearchCV parameters. We will set only few parameters, as following:\n\n* **n_estimators**: number of trees in the foreset;  \n* **max_features**: max number of features considered for splitting a node;  \n* **max_depth**: max number of levels in each decision tree;  \n* **min_samples_split**: min number of data points placed in a node before the node is split;  \n* **min_samples_leaf**: min number of data points allowed in a leaf node.\n"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"5484c36ce4a98fc2a93b976fa39f0da88b9e7789"},"cell_type":"code","source":"parameters = {\n    'n_estimators': (50, 75,100),\n    'max_features': ('auto', 'sqrt'),\n    'max_depth': (3,4,5),\n    'min_samples_split': (2,5,10),\n    'min_samples_leaf': (1,2,3)\n}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"97568aa6acfe611835c4552ae38bedc9b4f5d02e"},"cell_type":"markdown","source":"We initialize GridSearchCV with the classifier, the set of parameters, number of folds and also the level of verbose for printing out progress."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"1a5601d1b2a0f36419d49b5a417e6f4708df0058"},"cell_type":"code","source":"%%time\ngs_clf = GridSearchCV(rf_clf, parameters, n_jobs=-1, cv = 5, verbose = 5)\ngs_clf = gs_clf.fit(train_X, train_Y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a998474ea3231578b95f5a03e8bd17f0b8024911"},"cell_type":"markdown","source":"Let's see the best parameters."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"c9f53858a732111a2e173804fe7ad1a8674ce65f"},"cell_type":"code","source":"print('Best scores:',gs_clf.best_score_)\nprint('Best params:',gs_clf.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1247fe2da02db685e2d12d2b80a43c8a4a3b96de"},"cell_type":"markdown","source":"Let's predict with the validation data."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"ef4ebc06d9be524405bb2a742b7176c3078e1081"},"cell_type":"code","source":"preds = gs_clf.predict(valid_X)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2b6ff4af16ff8fbb3f29771eecab342f68468962"},"cell_type":"markdown","source":"Let's check the accuracy for the validation set."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"8552a15346e474f654402b4d6dfae04ece158dfd"},"cell_type":"code","source":"clf.score(valid_X, valid_Y)\nacc = round(clf.score(valid_X, valid_Y) * 100, 2)\nprint(\"RandomForest accuracy (validation set):\", acc)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e0ee8b0233b933eed68c00e1894f91367af6a900"},"cell_type":"markdown","source":"Let's check the validation classification report."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"e5eb8cc001544a70e0f429bbdb01e0ba47f2ef2c"},"cell_type":"code","source":"print(metrics.classification_report(valid_Y, preds, target_names=['Not Survived', 'Survived']))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ffee0be047b4b6e29a4ef15746805475520efd2a"},"cell_type":"markdown","source":"## <a id='56'>Submission (model with hyperparameters optimization)</a>\n\n\nFirst, we predict for test data using the trained model.\n"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"4f76f6b6e9c796482d24482d245451da9ec079ab"},"cell_type":"code","source":"test_X = test_df[predictors]\npred_Y = gs_clf.predict(test_X)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c76f42f6d7bbc727c5722bade84df2b9fa006634"},"cell_type":"markdown","source":"Then we prepare the submission dataset and save it to the submission file."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"759ae86b3e62be1203fff191f13ce8a4cc53695a"},"cell_type":"code","source":"submission = pd.DataFrame({\"PassengerId\": test_df[\"PassengerId\"],\"Survived\": pred_Y})\nsubmission.to_csv('submission_hyperparam_optimization.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a href=\"#0\"><font size=\"1\">Go to top</font></a>  "},{"metadata":{"_uuid":"8d0e186d58610f3250d9e5b2b0c7d95aabb1bc35"},"cell_type":"markdown","source":"\n# <a id='6'>Model ensambling</a>\n\n\nLet's continue with creation of second level models. We will train several models and will then use these first level models to train a second level model. This method is powerfull and can enhance the performance of first level models, especially when there is little correlation between the results of first level models.\n"},{"metadata":{"_uuid":"ed0081e2535c00cf053f12843727eb2e353b7f60"},"cell_type":"markdown","source":"## <a id='61'>Create the ensamble framework</a>\n\n\nWe start by creating a generic classifier, that extends the functionality of a simple classifier. This generic classifier will be instanciated with few different first level classifiers and then used in the ensamble. We are also using cross-validation (with KFolds).\n\nFirst step will be to create the folds used in cross validation."},{"metadata":{"trusted":true,"_uuid":"1b4ed3d715babd5126a14173fb94708f787ff068"},"cell_type":"code","source":"NUMBER_KFOLDS = 5\nkf = KFold(n_splits = NUMBER_KFOLDS, random_state = RANDOM_STATE, shuffle = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"76adb43abfc9304fe8cd62871d7f2bd42dc68774"},"cell_type":"code","source":"# Class to extend the Sklearn classifier\nclass SklearnBasicClassifier(object):\n    def __init__(self, clf, seed=2018, params=None):\n        params['random_state'] = seed\n        self.clf = clf(**params)\n\n    def train(self, x_train, y_train):\n        self.clf.fit(x_train, y_train)\n\n    def predict(self, x):\n        return self.clf.predict(x)\n    \n    def fit(self,x,y):\n        return self.clf.fit(x,y)\n    \n    def feature_importances(self,x,y):\n        print(self.clf.fit(x,y).feature_importances_)\n\n    def get_feature_importances(self,x,y):\n        return (self.clf.fit(x,y).feature_importances_)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fc8dbf50c7b111442c0b826363d54560d3fb0c0b"},"cell_type":"markdown","source":"## <a id='62'>Create the Out-of-Fold Predictions</a>\n\nLet's now define out-of-folds predictions. If we would train the base models on the full training data and generate predictions on the full test set and then output these for the second-level training we might go into trouble. The risk here is that the base model predictions would have seen the test set and thus overfitting when feeding those predictions."},{"metadata":{"trusted":true,"_uuid":"6fe8634e9098a432cfc8f9928581fa020bdfe03f"},"cell_type":"code","source":"ntrain = train_df.shape[0]\nntest = test_df.shape[0]\ndef get_oof_predictions(clf, x_train, y_train, x_test):\n    oof_train = np.zeros((ntrain,))\n    oof_test = np.zeros((ntest,))\n    oof_test_skf = np.empty((NUMBER_KFOLDS, ntest))\n    \n    for i, (train_idx, valid_idx) in enumerate(kf.split(train_df)):\n        clf.train(x_train[train_idx], y_train[train_idx])\n        oof_train[valid_idx] = clf.predict(x_train[valid_idx])\n        oof_test_skf[i, :] = clf.predict(x_test)\n\n    oof_test[:] = oof_test_skf.mean(axis=0)\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"25e8daeb6e6e8fa3504f1f58f0e940a67537258f"},"cell_type":"markdown","source":"## <a id='63'>Train the first level models</a>  \n\n\nLet's define and train few first level models.\n\nWe will use the following first level models:\n\n\n* AdaBoost classifer\n* CatBoost Classifier\n* Extra Trees classifier  \n* Gradient Boosting classifer\n* Random Forest classifier  \n* Support Vector Machine\n\n\nLet's define the parameters used for training each classifier:"},{"metadata":{"trusted":true,"_uuid":"a40ac58c7dc07db8e5afc7f21dc6e8db6ae6e2fa"},"cell_type":"code","source":"# AdaBoost parameters\nada_params = {\n    'n_estimators': 200,\n    'learning_rate' : 0.75\n}\n# CatBoost parameters\ncat_params = {\n    'iterations': 150,\n    'learning_rate': 0.02,\n    'depth': 12,\n    'bagging_temperature':0.2,\n    'od_type':'Iter',\n    'metric_period':400,\n}  \n# Extra Trees Parameters\next_params = {\n    'n_jobs': -1,\n    'n_estimators':100,\n    'max_depth': 8,\n    'min_samples_leaf': 3,\n    'verbose': 0\n}\n# Gradient Boosting parameters\ngbm_params = {\n    'n_estimators': 200,\n    'max_depth': 5,\n    'min_samples_leaf': 3,\n    'verbose': 0\n}\n# Random Forest parameters\nrfo_params = {\n    'n_jobs': -1,\n    'n_estimators': 50,\n    'max_depth': 5,\n    'min_samples_leaf': 5,\n    'max_features' : 'auto',\n    'verbose': 0\n}\n# Support Vector Classifier parameters \nsvc_params = {\n    'kernel' : 'linear',\n    'C' : 0.02\n}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"efb8d9465231e560974783c43e636e7e02845ae5"},"cell_type":"markdown","source":"We create 6 objects of type `SklearnBasicClassifier` that represent our 6 models(AdaBoost, CatBoost, ExtraTrees, GradientBoosting, RandomForest and Support Vector Machines)."},{"metadata":{"trusted":true,"_uuid":"34802100a684d863e949a78595ed08a1a96e9358"},"cell_type":"code","source":"# Create 6 objects that represent our 6 models\nada = SklearnBasicClassifier(clf=AdaBoostClassifier, seed=RANDOM_STATE, params=ada_params)\ncat = SklearnBasicClassifier(clf=CatBoostClassifier, seed=RANDOM_STATE, params=cat_params)\next = SklearnBasicClassifier(clf=ExtraTreesClassifier, seed=RANDOM_STATE, params=ext_params)\ngbm = SklearnBasicClassifier(clf=GradientBoostingClassifier, seed=RANDOM_STATE, params=gbm_params)\nrfo = SklearnBasicClassifier(clf=RandomForestClassifier, seed=RANDOM_STATE, params=rfo_params)\nsvc = SklearnBasicClassifier(clf=SVC, seed=RANDOM_STATE, params=svc_params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"73da4925f2a382c3eac05b3da9272bb87f2ec6a7"},"cell_type":"code","source":"predictors = ['FamilySize', 'Title', 'Class*Age']\ntarget = 'Survived'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ea2747a335d7a0992fd86531aee5ec21ebac4b5"},"cell_type":"code","source":"y_train = train_df['Survived'].values\ntrain = train_df[predictors]\ntest = test_df[predictors]\nx_train = train.values\nx_test = test.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6bbcf1730a4d3f44ace78ca8694b5510985f2270"},"cell_type":"code","source":"print(\"Start training\")\nada_oof_train, ada_oof_test = get_oof_predictions(ada, x_train, y_train, x_test) # AdaBoost Classifier\nprint(\"End AdaBoost\")\ncat_oof_train, cat_oof_test = get_oof_predictions(cat, x_train, y_train, x_test) # CatBoost Classifier\nprint(\"End CatBoost\")\next_oof_train, ext_oof_test = get_oof_predictions(ext, x_train, y_train, x_test) # Extra Trees \nprint(\"End ExtraTrees\")\nrfo_oof_train, rfo_oof_test = get_oof_predictions(rfo,x_train, y_train, x_test) # Random Forest Classifier\nprint(\"End RandomForest\")\ngbm_oof_train, gbm_oof_test = get_oof_predictions(gbm,x_train, y_train, x_test) # Gradient Boost Classifier\nprint(\"End GradientBoost\")\nsvc_oof_train, svc_oof_test = get_oof_predictions(svc,x_train, y_train, x_test) # Support Vector Classifier\nprint(\"End training\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"955dbb11b8b9085ee413c1c93810947e8054da19"},"cell_type":"markdown","source":"Let's check the features importance for the 5 out of 6 models. We will not include Support Vector Classifier, this one does not have feature importance available."},{"metadata":{"trusted":true,"_uuid":"38c2d47666df81909e2bc004eadb89ea309c3dac"},"cell_type":"code","source":"ada_feature_importance = ada.get_feature_importances(x_train,y_train)\ncat_feature_importance = cat.get_feature_importances(x_train,y_train)\next_feature_importance = ext.get_feature_importances(x_train,y_train)\ngbm_feature_importance = gbm.get_feature_importances(x_train,y_train)\nrfo_feature_importance = rfo.get_feature_importances(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"33732c84dc2fa96ddf0d5a502c663bcf464d2464"},"cell_type":"code","source":"def plot_feature_importance(feature_importance, classifier):\n    tmp = pd.DataFrame({'Feature': predictors, 'Feature importance': feature_importance[0:len(predictors)]})\n    tmp = tmp.sort_values(by='Feature importance',ascending=False)\n    plt.figure(figsize = (7,4))\n    plt.title('Features importance {}'.format(classifier),fontsize=14)\n    s = sns.barplot(x='Feature',y='Feature importance',data=tmp)\n    s.set_xticklabels(s.get_xticklabels(),rotation=90)\n    plt.show()   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"66d1c412b2b77cb3eac71959cefe9c514fb246eb"},"cell_type":"code","source":"plot_feature_importance(ada_feature_importance, '- AdaBoost')\nplot_feature_importance(cat_feature_importance, '- CatBoost')\nplot_feature_importance(ext_feature_importance, '- ExtraTrees')\nplot_feature_importance(gbm_feature_importance, '- GradientBoosting')\nplot_feature_importance(rfo_feature_importance, '- RandomForest')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d51079d48c47157c364984e44df34bfbc267b6fd"},"cell_type":"markdown","source":"## <a id='64'>Correlation of the results</a>\n\nLet's see the first few results of the predictions using first level models."},{"metadata":{"trusted":true,"_uuid":"d4edc1e2b4f204fa5b3aaf7009d909a1f1994c7b"},"cell_type":"code","source":"base_predictions_train = pd.DataFrame( {\n     'AdaBoost': ada_oof_train.ravel(),\n     'CatBoost': cat_oof_train.ravel(),\n     'ExtraTrees': ext_oof_train.ravel(),\n     'GradientBoost': gbm_oof_train.ravel(),\n     'RandomForest': rfo_oof_train.ravel(),\n     'SVM': svc_oof_train.ravel()\n    })\nbase_predictions_train.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6172dbe72952cb632a71a22270d8e3746725144f"},"cell_type":"markdown","source":"Let's show now the correlation of the predictions using the first level models. The ensamble prediction is best when we have models with good accuracy and less correlated."},{"metadata":{"trusted":true,"_uuid":"3598f5f56cb499a7e5f0a12126a5a15ebc47bdf1"},"cell_type":"code","source":"trace = go.Heatmap(\n        z= base_predictions_train.astype(float).corr().values ,\n        x=base_predictions_train.columns.values,\n        y= base_predictions_train.columns.values,\n          colorscale='Rainbow',\n            showscale=True,\n            reversescale = False\n    )\ndata = [trace]\nlayout = dict(width = 600, height=600)\nfig = dict(data=data, layout=layout)\niplot(fig, filename='heatmap')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9c7159cf936f0b6c8ad02fd03e5d8bc8f2246ef8"},"cell_type":"markdown","source":"## <a id='65'>Build the second level (ensamble) model</a>\n\n\nWe prepare now, using the Out-Of-Folds values, the training and the test set for the second level model. We concatenate the OOFs from the 6 first level models."},{"metadata":{"trusted":true,"_uuid":"a8992e0487890a725f40a4616b11b84bdfb76c28"},"cell_type":"code","source":"x_train = np.concatenate(( ada_oof_train, cat_oof_train, ext_oof_train, gbm_oof_train, rfo_oof_train, svc_oof_train), axis=1)\nx_test = np.concatenate(( ada_oof_test, cat_oof_test, ext_oof_test, gbm_oof_test, rfo_oof_test, svc_oof_test), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e7fa503dc5ae0f371a23ee3d0e044c66b69bbec2"},"cell_type":"markdown","source":"We prepare as well the second level classifier. \n\nWe will use in this case a eXtreme Boost Classifier."},{"metadata":{"trusted":true,"_uuid":"fa0538bfd7f191d00a88445c8492f0e46f3ec08d"},"cell_type":"code","source":"clf = xgb.XGBClassifier(\n learning_rate = 0.02,\n n_estimators= 2000,\n max_depth= 4,\n min_child_weight= 2,\n gamma=0.9,                        \n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread= -1,\n scale_pos_weight=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"37bd28f90ad42efa89d5a43a387b527181a4b72c"},"cell_type":"markdown","source":"We fit the model."},{"metadata":{"trusted":true,"_uuid":"e9eaf22aaf7e6aa995558257b8ff1ce905d19b5e"},"cell_type":"code","source":"xgbm = clf.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fec8e4297813c985da2fd39d55c23300bd0545af"},"cell_type":"markdown","source":"With the fitted second level model we do the prediction for the test data."},{"metadata":{"trusted":true,"_uuid":"f6b6dfe66f0549965f5f729b38750c070c9d5891","_kg_hide-input":true},"cell_type":"code","source":"predictions = xgbm.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1081c979fbf8087380debe5acc024a0a1b222c8d"},"cell_type":"markdown","source":"## <a id='66'>Submission (ensamble)</a>\n\nWe form the submission dataset and save it to the submission file."},{"metadata":{"trusted":true,"_uuid":"62c83c62c181ab3a3e7c26abd2114fa1eecfc781","_kg_hide-input":true},"cell_type":"code","source":"submissionStacking = pd.DataFrame({ 'PassengerId': test_df[\"PassengerId\"],'Survived': predictions })\nsubmissionStacking.to_csv(\"submission_ensamble.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a href=\"#0\"><font size=\"1\">Go to top</font></a>  \n\n"},{"metadata":{"_uuid":"36b9c4d8fc30eff7307a4dec88fe97b4aca1131e"},"cell_type":"markdown","source":"# <a id='7'>References</a>\n\n[1] https://www.kaggle.com/startupsci/titanic-data-science-solutions  \n[2] https://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python  \n[3] https://www.kaggle.com/gpreda/credit-card-fraud-detection-predictive-models  \n[4] https://www.kaggle.com/gpreda/honey-bee-subspecies-classification  "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}