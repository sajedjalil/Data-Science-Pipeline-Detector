{"cells":[{"metadata":{},"cell_type":"markdown","source":"To the Machine Learning Enghutiasts, the Titanic Survival problem statement is a good starter kit. After completing courses from any website, this competition will help you implement your learned knowledge in the realtime.\n\nStay here and you will see a stepwise guide to the solution for your first ML problem.\n\n**Problem statement**:\nWe are given 2 datasets - train and test data with few features (columns in layman language). You have to use the train.csv file to train your ML model and test.csv to check how good is your model performing.\n\nI follow a fixed steps while addressing any new problem statement. You must read the steps once given [here]( (https://datascience.stackexchange.com/a/69647/81999)\n\n**Let's get started**\nFirst, we will import the files into the notebook with the following code.\n\n### Importing the files:","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import cross_val_score\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data analysis\nNow, let's analyze the data and get some insights on it.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_df  = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\nprint (\"*\"*10, \"Dataset information\", \"*\"*10)\nprint (train_df.info())\nprint (\"*\"*10, \"First 5 test rows\", \"*\"*10)\nprint (train_df.head(5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Insights:\nWe can see that the train dataset is of size 891 * 12 where features like Age, Cabin and Embarked have few nulls.\n\nAs we did above, we will read the data of test file as well and print the values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\nprint (\"*\"*10, \"Dataset information\", \"*\"*10)\nprint (test_df.info())\nprint (\"*\"*10, \"Last 5 test rows\", \"*\"*10)\nprint (test_df.tail(5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Handling nulls\n\nNow, first thing that we should address is - NULL values.\nIn order to address nulls, let's find null ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.Cabin.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.Cabin.value_counts()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"Age\"].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Age null fix\ndata = [train_df, test_df]\n\nfor dataset in data:\n    mean = train_df[\"Age\"].mean()\n    #td = test_df[\"Age\"].std()\n    #is_null = dataset[\"Age\"].isnull().sum()\n    # compute random numbers between the mean, std and is_null\n    #rand_age = np.random.randint(mean - std, mean + std, size = is_null)\n    # fill NaN values in Age column with random values generated\n    #age_slice = dataset[\"Age\"].copy()\n    #age_slice[np.isnan(age_slice)] = rand_age\n    #dataset[\"Age\"] = age_slice\n    \n    dataset['Age'] = dataset['Age'].fillna(mean)\n    dataset[\"Age\"] = train_df[\"Age\"].astype(float)\ntrain_df[\"Age\"].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\ntest_df  = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\ndata = [train_df, test_df]\n\nfor dataset in data:\n    dataset['relatives']= dataset['SibSp'] + dataset['Parch']\n    dataset.loc[dataset['relatives'] > 0, 'not_alone'] = 0\n    dataset.loc[dataset['relatives'] == 0, 'not_alone'] = 1\n    dataset['not_alone'] = dataset['not_alone'].astype(int) \n\ntrain_df['not_alone'].value_counts()\n\n#Drop passenger ID as it is not required\ntrain_df = train_df.drop(['PassengerId'], axis=1)\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n#Data processing\n#1. In the cabin variable, create new column and add there only first letters of the column\ndeck = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7, \"U\": 8}\ndata = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Deck'] = dataset['Cabin'].fillna(\"U\")\n    dataset['Deck'] = dataset['Cabin'].astype(str).str[0] \n    dataset['Deck'] = dataset['Deck'].str.capitalize()\n    dataset['Deck'] = dataset['Deck'].map(deck)\n    dataset['Deck'] = dataset['Deck'].fillna(0)\n    dataset['Deck'] = dataset['Deck'].astype(int) \n\ntrain_df['Deck'].value_counts()\n\n#Dropping Cabin feature\ntrain_df = train_df.drop(['Cabin'], axis=1)\ntest_df.drop(['Cabin'], axis=1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (train_df['Deck'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\ntrain_df['Age'].fillna(train_df['Age'].mode(), inplace=True)\ntest_df['Age'].fillna(test_df['Age'].mode(), inplace=True)\n#train_df[\"Age\"].isnull().sum()\nprint (train_df[\"Age\"].isnull().sum())\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.Sex.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print (train_df['Embarked'].value_counts())\n\ncommon_value = 'S'\ndata = [train_df, test_df]\nfor dataset in data:\n    dataset['Embarked'] = dataset['Embarked'].fillna(common_value)\n\ntrain_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = [train_df, test_df] \nembarkedMap = {\"S\": 0, \"C\": 1, \"Q\": 2}\n\ngenderMap = {\"male\": 0, \"female\": 1}\n\nfor dataset in data:\n    dataset['Fare'] = dataset['Fare'].fillna(0)\n    #dataset['Fare'] = dataset['Fare'].astype(int) \n    dataset['Embarked'] = dataset['Embarked'].map(embarkedMap)\n    dataset['Sex'] = dataset['Sex'].map(genderMap)\n    #print (dataset['Embarked'])\n    \ntrain_df['Sex'].describe()\ntrain_df['Embarked'].describe()\ntrain_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#Title extraction\ndata = [train_df, test_df]\ntitles = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n\n#Braund, Mr. Owen Harris\nfor dataset in data:\n    dataset['Title'] = dataset['Name'].str.extract(' ([A-Za-z]+)\\.')\n    #print(dataset['Title'])\n    \n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr',\\\n                                            'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona', 'Mlle', 'Ms', 'Mme'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].map(titles)\n    # filling NaN with 0, to get safe\n    dataset['Title'] = dataset['Title'].fillna(0)\n    \n    \nprint (train_df['Title'].value_counts())\n\n\ntrain_df = train_df.drop(['Name'], axis=1)\ntest_df = test_df.drop(['Name'], axis=1)\ntrain_df['Title'].value_counts()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['Ticket'].value_counts()\ntrain_df = train_df.drop(['Ticket'], axis=1)\ntest_df = test_df.drop(['Ticket'], axis=1) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = [train_df, test_df]\nfor dataset in data:\n    #dataset['Age'] = dataset['Age'].astype(int)\n    dataset.loc[ dataset['Age'] <= 11, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 11) & (dataset['Age'] <= 18), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 18) & (dataset['Age'] <= 22), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 22) & (dataset['Age'] <= 27), 'Age'] = 3\n    dataset.loc[(dataset['Age'] > 27) & (dataset['Age'] <= 33), 'Age'] = 4\n    dataset.loc[(dataset['Age'] > 33) & (dataset['Age'] <= 40), 'Age'] = 5\n    dataset.loc[(dataset['Age'] > 40) & (dataset['Age'] <= 66), 'Age'] = 6\n    dataset.loc[ dataset['Age'] > 66, 'Age'] = 6\n    \n# let's see how it's distributed \ntrain_df['Age'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndata = [train_df, test_df]\n\n#train_df['category_fare'] = pd.qcut(train_df['Fare'], 4)\n\n#train_df['category_fare'].value_counts()\n\nfor dataset in data:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[(dataset['Fare'] > 31) & (dataset['Fare'] <= 99), 'Fare']   = 3\n    dataset.loc[(dataset['Fare'] > 99) & (dataset['Fare'] <= 250), 'Fare']   = 4\n    dataset.loc[ dataset['Fare'] > 250, 'Fare'] = 5\n    dataset['Fare'] = dataset['Fare'].astype(float)\n\ntrain_df['Fare'].value_counts()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\ndata = [train_df, test_df]\nfor dataset in data:\n    dataset['Age_Class']= dataset['Age']* dataset['Pclass']\n    '''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nfor dataset in data:\n    dataset['Fare_Per_Person'] = dataset['Fare']/(dataset['relatives']+1)\n    dataset['Fare_Per_Person'] = dataset['Fare_Per_Person'].astype(int)\n# Let's take a last look at the training set, before we start training the models.\ntrain_df.head(10)\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\ntrain_df  = train_df.drop(\"Ticket\", axis=1)\ntest_df  = test_df.drop(\"Ticket\", axis=1)\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df.drop(['PassengerId'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train_df.drop('Survived', axis=1)\nY_train = train_df['Survived']\n\ntest_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = test_df.drop(\"PassengerId\", axis=1).copy()\nX_test.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\ntrain_df  = train_df.drop(\"not_alone\", axis=1)\ntest_df  = test_df.drop(\"not_alone\", axis=1)\n\ntrain_df  = train_df.drop(\"Parch\", axis=1)\ntest_df  = test_df.drop(\"Parch\", axis=1)\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n\nfrom sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression() \nclf.fit(X_train, Y_train)\n\nY_pred  = clf.predict(X_test)\n\nscores = cross_val_score(clf, X_train, Y_train, cv = 10, scoring = \"accuracy\")\n\n#clf.score(X_train, Y_train)\n#acc_logistic_reg = round(clf.score(X_train, Y_train)*100, 2)\n\nprint (\"Scores: \",scores)\nprint (\"Mean: \", scores.mean())\nprint (\"Standard Deviation: \", scores.std())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2. SVM\nfrom sklearn import svm\nclf_svm = svm.SVC()\nclf_svm.fit(X_train, Y_train)\n\nY_pred_svm  = clf_svm.predict(X_test)\n\nscores_svm = cross_val_score(clf_svm, X_train, Y_train, cv = 10, scoring = \"accuracy\")\nprint (\"Scores: \",scores_svm.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2. Decision tree\n\nfrom sklearn import tree\nclf_dt = tree.DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n                       max_depth=None, max_features=None, max_leaf_nodes=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=5, min_samples_split=25,\n                       min_weight_fraction_leaf=0.0, presort='deprecated',\n                       random_state=None, splitter='best')\nclf_dt.fit(X_train, Y_train)\n\nY_pred_svm  = clf_dt.predict(X_test)\n\nscores_dt = cross_val_score(clf_dt, X_train, Y_train, cv = 10, scoring = \"accuracy\")\nprint (\"Scores: \",scores_dt.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2. Random forest\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nclf_rf = RandomForestClassifier(max_depth=2, random_state=0)\n\nclf_rf.fit(X_train, Y_train)\n\nY_pred_rf  = clf_rf.predict(X_test)\n\nscores_rf = cross_val_score(clf_rf, X_train, Y_train, cv = 10, scoring = \"accuracy\")\nprint (\"Scores: \",scores_rf.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2. Gradient Boosting Classifier\n\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nclf_gbc = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\n\nclf_gbc.fit(X_train, Y_train)\n\nY_pred_rf  = clf_gbc.predict(X_test)\n\nscores_rf = cross_val_score(clf_gbc, X_train, Y_train, cv = 10, scoring = \"accuracy\")\nprint (\"Scores: \",scores_rf.mean())\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2. Bagging Classifier\n\nfrom sklearn.ensemble import BaggingClassifier\n\nclf_bagging = BaggingClassifier()\n\nclf_bagging.fit(X_train, Y_train)\n\nY_pred_rf  = clf_bagging.predict(X_test)\n\nscores_rf = cross_val_score(clf_bagging, X_train, Y_train, cv = 10, scoring = \"accuracy\")\nprint (\"Scores: \",scores_rf.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2. Naive Bayes - Gaussian\n\nfrom sklearn.naive_bayes import GaussianNB\n\nclf_gnb = GaussianNB()\n\nclf_gnb.fit(X_train, Y_train)\n\nY_pred_rf  = clf_gnb.predict(X_test)\n\nscores_rf = cross_val_score(clf_gnb, X_train, Y_train, cv = 10, scoring = \"accuracy\")\nprint (\"Scores: \",scores_rf.mean())\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nfrom xgboost import XGBClassifier\n\nclf_xgb = XGBClassifier().fit(X_train, Y_train)\n\nY_pred  = clf_xgb.predict(X_test)\n\nclf_xgb.score(X_train, Y_train)\n\nscores_rf = cross_val_score(clf_xgb, X_train, Y_train, cv = 10, scoring = \"accuracy\")\nprint (\"Scores: \",scores_rf.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn.model_selection import cross_val_score\n\nrandom_forest = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n                       criterion='gini', max_depth=None, max_features='auto',\n                       max_leaf_nodes=None, max_samples=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=5, min_samples_split=16,\n                       min_weight_fraction_leaf=0.0, n_estimators=700,\n                       n_jobs=-1, oob_score=True, random_state=1, verbose=0,\n                       warm_start=False)\nscores = cross_val_score(random_forest, X_train, Y_train, cv=10, scoring = \"accuracy\")\n\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard Deviation:\", scores.std())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\n\nparam_grid = { \n    \"criterion\" : [\"gini\", \"entropy\"], \n    \"min_samples_leaf\" : [1, 5, 10, 25, 50, 70, 90, 120], \n    \"min_samples_split\" : [2, 4, 10, 12, 16, 18, 25, 35, 45, 55, 65]}\n\ndt = DecisionTreeClassifier()\n\n#clf = GridSearchCV(dt, param_grid=param_grid)\nclf = RandomizedSearchCV(dt, param_grid)\n \nclf.fit(X_train, Y_train)\n \nprint(clf.best_estimator_)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''from sklearn.model_selection import cross_val_score\n\nclf_dt = DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',\n                       max_depth=None, max_features=None, max_leaf_nodes=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=10, min_samples_split=35,\n                       min_weight_fraction_leaf=0.0, presort='deprecated',\n                       random_state=None, splitter='best')\nscores = cross_val_score(clf_dt, X_train, Y_train, cv=10, scoring = \"accuracy\")\n\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard Deviation:\", scores.std())'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''#Hyper parameter tuining - Decision trees\nparam_grid = { \n    \"criterion\" : [\"gini\", \"entropy\"], \n    \"min_samples_leaf\" : [1, 5, 10, 25, 50, 70], \n    \"min_samples_split\" : [2, 4, 10, 12, 16, 18, 25, 35], \n    \"n_estimators\": [100, 400, 700, 1000, 1500]}\n\nrf = RandomForestClassifier(n_estimators=100, max_features='auto', oob_score=True, random_state=1, n_jobs=-1)\n\n#clf = GridSearchCV(rf, param_grid=param_grid, n_jobs=-1)\nclf = RandomizedSearchCV(rf, param_grid, n_jobs=-1)\n \nclf.fit(X_train, Y_train)\nprint(clf.best_estimator_)'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(clf.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''params = {\n        'min_child_weight': [1, 5, 10],\n        'gamma': [0.5, 1, 1.5, 2, 5],\n        'subsample': [0.6, 0.8, 1.0],\n        'colsample_bytree': [0.6, 0.8, 1.0],\n        'max_depth': [3, 4, 5]\n        }\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n\n#param_grid = { \"criterion\" : [\"gini\", \"entropy\"], \"min_samples_leaf\" : [1, 5, 10, 25, 50, 70], \"min_samples_split\" : [2, 4, 10, 12, 16, 18, 25, 35], \"n_estimators\": [100, 400, 700, 1000, 1500]}\n\nxgb = XGBClassifier(learning_rate=0.02, n_estimators=600,\n                    silent=True, nthread=1)\n\n#rf = RandomForestClassifier(n_estimators=100, max_features='auto', oob_score=True, random_state=1, n_jobs=-1)\n\nclf = GridSearchCV(xgb, param_grid=params)\n#clf = RandomizedSearchCV(xgb, param_grid, n_jobs=-1)\n \nclf.fit(X_train, Y_train)\n\nprint(clf.best_estimator_)'''\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''# Logistic regression before HPT\nfrom sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\n\nY_pred = logreg.predict(X_test)\nacc_log = round(logreg.score(X_train, Y_train)*100,2)\n\nprint (acc_log)\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''from sklearn.model_selection import cross_val_score\nlogreg = LogisticRegression()\nscores = cross_val_score(logreg, X_train, Y_train, cv=10, scoring = \"accuracy\")\n\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard Deviation:\", scores.std())'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\noutput = pd.DataFrame({'PassengerId': test_df.PassengerId, 'Survived': Y_pred})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")\n'''","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}