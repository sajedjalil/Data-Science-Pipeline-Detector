{"cells":[{"metadata":{"_cell_guid":"bc64948f-5d6a-078d-085d-1beb58687bd3","_uuid":"e919d1161f20999e599ba1fd66a5a45b9c82f229"},"cell_type":"markdown","source":"# Table of content\n\n1. Introduction - Loading libraries and dataset\n2. Exploratory analysis, engineering and cleaning features - Bi-variate analysis\n3. Correlation analysis - Tri-variate analysis\n4. Predictive modelling, cross-validation, hyperparameters and ensembling\n5. Submitting results\n6. Credits\n\n### Check other Kaggle notebooks from [Yvon Dalat](https://www.kaggle.com/ydalat):\n* [Titanic, a step-by-step intro to Machine Learning](https://www.kaggle.com/ydalat/titanic-a-step-by-step-intro-to-machine-learning): **a practice run ar EDA and ML-classification**\n* [HappyDB, a step-by-step application of Natural Language Processing](https://www.kaggle.com/ydalat/happydb-what-100-000-happy-moments-are-telling-us): **find out what 100,000 happy moments are telling us**\n* [Work-Life Balance survey, an Exploratory Data Analysis of lifestyle best practices](https://www.kaggle.com/ydalat/work-life-balance-best-practices-eda): **key insights into the factors affecting our work-life balance**\n*  [Work-Life Balance survey, a Machine-Learning analysis of best practices to rebalance our lives](https://www.kaggle.com/ydalat/work-life-balance-predictors-and-clustering): **discover the strongest predictors of work-life balance**\n\n**Interested in more facts and data to balance your life, check the [360 Living guide](https://amzn.to/2MFO6Iy) ![360 Living: Practical guidance for a balanced life](https://images-na.ssl-images-amazon.com/images/I/61EhntLIyBL.jpg)**\n\n**Note:** Ever feel burnt out? Missing a deeper meaning? Sometimes life gets off-balance, but with the right steps, we can find the personal path to authentic happiness and balance.\n[Check out how Machine Learning and statistical analysis](https://www.amazon.com/dp/B07BNRRP7J?ref_=cm_sw_r_kb_dp_TZzTAbQND85EE&tag=kpembed-20&linkCode=kpe) sift through 10,000 responses to help us define our unique path to better living.\n\n# 1. Introduction - Loading libraries and dataset\nI created this Python notebook as the learning notes of my first Machine Learning project.\nSo many new terms, new functions, new approaches, but the subject really interested me; so I dived into it, studied one line of code at a time, and captured the references and explanations in this notebook.\n\nThe algorithm itself is a fork from **Anisotropic's Introduction to Ensembling/Stacking in Python**, a great notebook in itself.\nHis notebook was itself based on **Faron's \"Stacking Starter\"**, as well as **Sina's Best Working Classfier**. \nI also used multiple functions from **Yassine Ghouzam**.\nI added many variations and additional features to improve the code (as much as I could) as well as additional visualization.\n\nSome key take away from my personal experiments and what-if analysis over the last couple of weeks:\n\n* **The engineering of the right features is absolutely key**. The goal there is to create the right categories between survived and not survived. They do not have to be the same size or equally distributed. What helped best is to group together passengers with the same survival rates.\n\n* ** I tried many, many different algorightms. Many overfit the training data** (up to 90%) but do not generate more accurate predictions with the test data. What worked better is to use the cross-validation on selected algotirhms. It is OK to select algorithms with various results as there is strenght in diversity. \n\n* **Lastly, the right ensembling was best achieved** with a votingclassifier with soft voting parameter\n\nOne last word: please use this kernel as a first project to practice your ML/Python skills. I shameless ley sotle and learnt from many Kagglers through my learning process, please do the same with the code in this kernel.\n\nI also welcome your comments, questions and feedback.\n\nYvon\n\n## 1.1. Importing Library"},{"metadata":{"_cell_guid":"14630296-b1aa-759e-bafa-b6a73f3896ed","_execution_state":"idle","_kg_hide-input":true,"_kg_hide-output":true,"_uuid":"2e37a274400cfeb472b6405d524325245588dd66","trusted":true,"collapsed":true},"cell_type":"code","source":"# Load libraries for analysis and visualization\nimport pandas as pd # collection of functions for data processing and analysis modeled after R dataframes with SQL like features\nimport numpy as np  # foundational package for scientific computing\nimport re           # Regular expression operations\nimport matplotlib.pyplot as plt # Collection of functions for scientific and publication-ready visualization\n%matplotlib inline\nimport plotly.offline as py     # Open source library for composing, editing, and sharing interactive data visualization \nfrom matplotlib import pyplot\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nfrom collections import Counter\n\n# Machine learning libraries\nimport xgboost as xgb  # Implementation of gradient boosted decision trees designed for speed and performance that is dominative competitive machine learning\nimport seaborn as sns  # Visualization library based on matplotlib, provides interface for drawing attractive statistical graphics\n\nimport sklearn         # Collection of machine learning algorithms\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n                              GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier)\nfrom sklearn.cross_validation import KFold\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score,classification_report, precision_recall_curve, confusion_matrix\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d647b74c-099b-851a-dcd2-3a58c9e8f10c","_uuid":"8b590aafe06a2ac55daae9d2456155e457914f5f"},"cell_type":"markdown","source":"## 1.2. Loading dataset"},{"metadata":{"_cell_guid":"5937fd72-d1ad-f678-cc82-f08a96e4cad0","_execution_state":"idle","_uuid":"b2ad78041b69ce13d1f41bd9bc8c93cafaf7b8ac","trusted":true,"collapsed":true},"cell_type":"code","source":"# Load in the train and test datasets from the CSV files\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\n# Store our passenger ID for easy access\nPassengerId = test['PassengerId']\n\n# Display the first 5 rows of the dataset, a first look at our data\n# 5 first row, 5 sample rows and basic statistics\ntrain.head(50)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b478c2ac-01ee-4f6f-ab5f-9f38e15574f5","_uuid":"5b8c4fe8903fbc4c364cc13ef112990e7c8fb0fe","trusted":true,"collapsed":true},"cell_type":"code","source":"train.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"69b8d571-e89c-492b-9314-31b329f63524","_uuid":"d1cc8b9ae2df4b559f2e3ac1b5f082632d634ece","trusted":true,"collapsed":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8f7edf22-e10f-4504-9642-6c43cbdb7f7e","_uuid":"83baed9972334c9971d82212def66fe8f9671d73"},"cell_type":"markdown","source":"**What are the data types for each feature?**\n* Survived: int\n* Pclass: int\n* Name: string\n* Sex: string\n* Age: float\n* SibSp: int\n* Parch: int\n* Ticket: string\n* Fare: float\n* Cabin: string\n* Embarked: string"},{"metadata":{"_cell_guid":"2ca2f211-5ee3-4d16-a370-a7de6ececa9b","_uuid":"f5243d4a9bed6a1d5424bd60375a0318331f3060"},"cell_type":"markdown","source":"## 1.3. Analysis goal\n**The Survived variable** is the outcome or dependent variable. It is a binary nominal datatype of 1 for \"survived\" and 0 for \"did not survive\".\n**All other variables** are potential predictor or independent variables. The goal is to predict this dependent variable only using the available independent variables. A test dataset has been created to test our algorithm."},{"metadata":{"_cell_guid":"1c8a0f85-2ef2-4d84-bf8a-b6443e8638d2","_uuid":"59d2c0cc264ee98c1b7178c12ffb733ed469be3d"},"cell_type":"markdown","source":"## 1.4. A very first look into the data"},{"metadata":{"_cell_guid":"ccbc1dd8-f6a8-418d-a156-ff42e46e3b25","_uuid":"f38a7f5049a019c2af35a91d7f653ab8ba906f41","trusted":true,"collapsed":true},"cell_type":"code","source":"f,ax = plt.subplots(3,4,figsize=(20,16))\nsns.countplot('Pclass',data=train,ax=ax[0,0])\nsns.countplot('Sex',data=train,ax=ax[0,1])\nsns.boxplot(x='Pclass',y='Age',data=train,ax=ax[0,2])\nsns.countplot('SibSp',hue='Survived',data=train,ax=ax[0,3],palette='husl')\nsns.distplot(train['Fare'].dropna(),ax=ax[2,0],kde=False,color='b')\nsns.countplot('Embarked',data=train,ax=ax[2,2])\n\nsns.countplot('Pclass',hue='Survived',data=train,ax=ax[1,0],palette='husl')\nsns.countplot('Sex',hue='Survived',data=train,ax=ax[1,1],palette='husl')\nsns.distplot(train[train['Survived']==0]['Age'].dropna(),ax=ax[1,2],kde=False,color='r',bins=5)\nsns.distplot(train[train['Survived']==1]['Age'].dropna(),ax=ax[1,2],kde=False,color='g',bins=5)\nsns.countplot('Parch',hue='Survived',data=train,ax=ax[1,3],palette='husl')\nsns.swarmplot(x='Pclass',y='Fare',hue='Survived',data=train,palette='husl',ax=ax[2,1])\nsns.countplot('Embarked',hue='Survived',data=train,ax=ax[2,3],palette='husl')\n\nax[0,0].set_title('Total Passengers by Class')\nax[0,1].set_title('Total Passengers by Gender')\nax[0,2].set_title('Age Box Plot By Class')\nax[0,3].set_title('Survival Rate by SibSp')\nax[1,0].set_title('Survival Rate by Class')\nax[1,1].set_title('Survival Rate by Gender')\nax[1,2].set_title('Survival Rate by Age')\nax[1,3].set_title('Survival Rate by Parch')\nax[2,0].set_title('Fare Distribution')\nax[2,1].set_title('Survival Rate by Fare and Pclass')\nax[2,2].set_title('Total Passengers by Embarked')\nax[2,3].set_title('Survival Rate by Embarked')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"dff1c1dd-1e59-3907-88fa-1a1d699122be","_uuid":"81378834770e55c76751347588322fe32acf5737"},"cell_type":"markdown","source":"This is only a quick of the relationships between features before we start a more detailed analysis.\n\n\n# 2. Exploratory Data Analysis (EDA), Cleaning and Engineering features\n\nWe will start with a standard approach of any kernel: correct, complete, engineer the right features for analysis.\n\n## 2.1. Correcting and completing features\n### Detecting and correcting outliers\nReviewing the data, there does not appear to be any aberrant or non-acceptable data inputs.\n\nThere are potential outliers that we will identify (steps from Yassine Ghouzam):\n* It creates firset a function called detect_outliers, implementing the Tukey method\n* For each column of the dataframe, this function calculates the 25th percentile (Q1) and 75th percentile (Q3) values.\n* The  interquartile range (IQR) is a measure of statistical dispersion, being equal to the difference between the 75th and 25th percentiles, or between upper and lower quartiles.\n* Any data points outside 1.5 time the IQR (1.5 time IQR below Q1, or 1.5 time IQR above Q3), is considered an outlier.\n* The outlier_list_col column captures the indices of these outliers. All outlier data get then pulled into the outlier_indices dataframe.\n* Finally, the detect_outliers function will select only the outliers happening multiple times. This is the datadframe that will be returned."},{"metadata":{"_cell_guid":"98ca04be-4693-4eb7-b25b-cc32e65b45b9","_uuid":"5abddbbf118113d33ed95f10cf73edbd4525df82","trusted":true,"collapsed":true},"cell_type":"code","source":"# Outlier detection \ndef detect_outliers(df,n,features):\n    outlier_indices = []\n    # iterate over features(columns)\n    for col in features:\n        # 1st quartile (25%)\n        Q1 = np.percentile(df[col],25)\n        # 3rd quartile (75%)\n        Q3 = np.percentile(df[col],75)\n        # Interquartile range (IQR)\n        IQR = Q3 - Q1\n        # outlier step\n        outlier_step = 1.5 * IQR\n        # Determine a list of indices of outliers for feature col\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index       \n        # append the found outlier indices for col to the list of outlier indices \n        outlier_indices.extend(outlier_list_col)\n        \n    # select observations containing more than 2 outliers\n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    return multiple_outliers   \n# detect outliers from Age, SibSp , Parch and Fare\nOutliers_to_drop = detect_outliers(train,2,[\"Age\",\"SibSp\",\"Parch\",\"Fare\"])\ntrain.loc[Outliers_to_drop] # Show the outliers rows","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4d70580d-8bcb-434c-8630-8bcb7aada36b","_uuid":"6cf77f7bdf1d38ca620d102e75ce54ed759e7af0"},"cell_type":"markdown","source":"** Observations**\n* The Detect_Outliers function found 10 outliers.\n* PassengerID 28, 89 and 342 passenger have an high Ticket Fare\n* The seven others have very high values of SibSP.\n* I found that dropping the outliers actually lower the prediction. So I decided to keep them.\n\nYou can try to remove them and rerun the prediction to observe the result with the following function:"},{"metadata":{"_cell_guid":"f0acac5f-79ba-4c1a-aab1-f6416f963cc7","_uuid":"1cb4a4a0d63f644850356d29521119025f95a257","collapsed":true,"trusted":true},"cell_type":"code","source":"# Drop outliers\n# train = train.drop(Outliers_to_drop, axis = 0).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3fa8b862-a11f-48f5-b87a-c9bd7114382a","_uuid":"6c82c0056c3492ae9a4f91fac98fb40f7048b252"},"cell_type":"markdown","source":"### Completing features\nThe .info function below shows how complete or incomplete the datasets are.\nThere are null values or missing data in the age, cabin, and embarked field. Missing values can be bad, because some algorithms don't know how-to handle null values and will fail. While others, like decision trees, can handle null values.\n\nThe approach to to complete missing data is to impute using mean, median, or mean + randomized standard deviation. \nWe will do this in section 2.2 with the  **fillna** function:  dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())"},{"metadata":{"_cell_guid":"3cc01b03-52b8-4c55-9dae-ed4c396d8601","_uuid":"3b87698ec3ae2a12fc189a570f8871fd82053245","trusted":true,"collapsed":true},"cell_type":"code","source":"train.info()\nprint('_'*40)\ntest.info()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6188601d-4426-49c1-8048-dfa05e143889","_uuid":"c1a3ee40ac0c78fb443276741e8f71229d14850f"},"cell_type":"markdown","source":"## 2.2. Descriptive analysis (univariate) "},{"metadata":{"_cell_guid":"6948be07-4fa1-456e-876f-e171390303e7","_uuid":"01ad05b95eb87e1d0a179f476339f5dc232501d7","trusted":true,"collapsed":true},"cell_type":"code","source":"full_data = [train, test]\nSurvival = train['Survived']\nSurvival.describe()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1409f12a-e327-47ee-a6d2-980b65ccf2c1","_uuid":"69581914493afab72108ad3e7fcf3185cbb435e1"},"cell_type":"markdown","source":"## 2.3 Feature Engineering - Bi-variate statistical analysis\n\nOne of the first tasks in Data Analytics is to **convert the variables into numerical/ordinal values**.\nThere are multiple types of data\n\n**a) Qualitative data: discrete**\n* Nominal: no natural order between categories. In this case: Name\n* Categorical: Sex\n\n**b) Numeric or quantitative data**\n* Discrete: could be ordinal like Pclass or not like Survived.\n* Continuous. e.g.: age\nMany feature engineering steps were taken from Anisotropic's excellent kernel.\n\n### Pclass"},{"metadata":{"_cell_guid":"3c566817-c659-4ec7-a05b-c868d02d13c9","_uuid":"4451a474e5ce6cd18261e29277eff3fcd0a9754d","trusted":true,"collapsed":true},"cell_type":"code","source":"sns.barplot(x=\"Embarked\", y=\"Survived\", hue=\"Sex\", data=train);","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"af55be9f-804f-443d-b089-dfcbc3c63736","_uuid":"4defc9757a1c64545d410bdbfd59f43f926e772a"},"cell_type":"markdown","source":"Embarked does not seem to have a clear impact on the survival rate. We will analyse it further in the next sections and drop it if we cannot demonstrate a proven relationship to Survived. \n\n### Name_length"},{"metadata":{"_cell_guid":"2f7e788b-5101-4297-9375-dcb8f83375ae","_uuid":"7561e22789e3f2e4e2a3ae3b529d496be0999dcb","trusted":true,"collapsed":true},"cell_type":"code","source":"for dataset in full_data:\n    dataset['Name_length'] = train['Name'].apply(len)\n    # Qcut is a quantile based discretization function to autimatically create categories\n    # dataset['Name_length'] = pd.qcut(dataset['Name_length'], 6, labels=False)\n    # train['Name_length'].value_counts()\n\nsum_Name = train[[\"Name_length\", \"Survived\"]].groupby(['Name_length'],as_index=False).sum()\naverage_Name = train[[\"Name_length\", \"Survived\"]].groupby(['Name_length'],as_index=False).mean()\nfig, (axis1,axis2,axis3) = plt.subplots(3,1,figsize=(18,6))\nsns.barplot(x='Name_length', y='Survived', data=sum_Name, ax = axis1)\nsns.barplot(x='Name_length', y='Survived', data=average_Name, ax = axis2)\nsns.pointplot(x = 'Name_length', y = 'Survived', data=train, ax = axis3)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a084b418-5ece-45be-a71f-75dca192764d","_uuid":"d3b31cd1cb45d6daccca97e90f6216354b0752cf"},"cell_type":"markdown","source":"The first graph shows the amount of people by Name_length.\n\nThe second one, their average survival rates.\n\nThe proposed categories are: less than 23 (mostly men), 24 to 28, 29 to 40, 41 and more (mostly women).\nThe categories are sized to group passengers with similar Survival rates."},{"metadata":{"_cell_guid":"31274424-f19d-4fab-a041-9ad2129da7d3","_uuid":"2c10630f38727745f89cdc3f9ef1e089eee490a6","trusted":true,"collapsed":true},"cell_type":"code","source":"for dataset in full_data:\n    dataset.loc[ dataset['Name_length'] <= 23, 'Name_length'] \t\t\t\t\t\t            = 0\n    dataset.loc[(dataset['Name_length'] > 23) & (dataset['Name_length'] <= 28), 'Name_length']  = 1\n    dataset.loc[(dataset['Name_length'] > 28) & (dataset['Name_length'] <= 40), 'Name_length']  = 2\n    dataset.loc[ dataset['Name_length'] > 40, 'Name_length'] \t\t\t\t\t\t\t        = 3\ntrain['Name_length'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ba18ae99-1947-4429-aeb1-035e8c5d5695","_uuid":"ef01d1f465ac8d9f15e20807a71756ef5e606cdf"},"cell_type":"markdown","source":"### Gender (Sex)"},{"metadata":{"_cell_guid":"faba1caa-0ab6-45a6-aa0f-a6b270e3964c","_uuid":"53345f625d3cdc5d515fa619327bf24fdaade750","collapsed":true,"trusted":true},"cell_type":"code","source":"for dataset in full_data:# Mapping Gender\n    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)    ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2decad09-0ef2-405e-a7a6-0674cdb72f52","_uuid":"9eed1dd267dd07d20edc4c730a1502471891d5be"},"cell_type":"markdown","source":"### Age"},{"metadata":{"_cell_guid":"6b817f31-bbfe-4cd1-ad18-cdbb403c2c09","_uuid":"4906ec9f7249077e67e873d6ec6eeb7d1f073b50","trusted":true,"collapsed":true},"cell_type":"code","source":"#plot distributions of age of passengers who survived or did not survive\na = sns.FacetGrid( train, hue = 'Survived', aspect=6 )\na.map(sns.kdeplot, 'Age', shade= True )\na.set(xlim=(0 , train['Age'].max()))\na.add_legend()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c6d7004d-a67c-48ca-be93-f6c320710c83","_uuid":"c47f9c6044a024d2f1cff9ae2cd3281e95d32add"},"cell_type":"markdown","source":"The best categories for age are:\n* 0:  Less than 14\n* 1:  14 to 30\n* 2:  30 to 40\n* 3:  40 to 50\n* 4:  50 to 60\n* 5:  60 and more"},{"metadata":{"_cell_guid":"f305dc89-836e-4968-820c-9b898d14b934","_uuid":"ba2a81859462878076a50e05cfd4cae890175eb0","trusted":true,"collapsed":true},"cell_type":"code","source":"for dataset in full_data:\n    age_avg = dataset['Age'].mean()\n    age_std = dataset['Age'].std()\n    age_null_count = dataset['Age'].isnull().sum()\n    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\n    dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\n    dataset['Age'] = dataset['Age'].astype(int)\n# Qcut is a quantile based discretization function to autimatically create categories (not used here)\n# dataset['Age'] = pd.qcut(dataset['Age'], 6, labels=False)\n# Using categories as defined above\n    dataset.loc[ dataset['Age'] <= 14, 'Age'] \t\t\t\t\t\t          = 0\n    dataset.loc[(dataset['Age'] > 14) & (dataset['Age'] <= 30), 'Age']        = 5\n    dataset.loc[(dataset['Age'] > 30) & (dataset['Age'] <= 40), 'Age']        = 1\n    dataset.loc[(dataset['Age'] > 40) & (dataset['Age'] <= 50), 'Age']        = 3\n    dataset.loc[(dataset['Age'] > 50) & (dataset['Age'] <= 60), 'Age']        = 2\n    dataset.loc[ dataset['Age'] > 60, 'Age'] \t\t\t\t\t\t\t      = 4\ntrain['Age'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"17f25475-0f53-4019-a76a-2ab4fdac3d09","_uuid":"3df6d74044370293306a3e2f007c104280ae078c","trusted":true,"collapsed":true},"cell_type":"code","source":"train[[\"Age\", \"Survived\"]].groupby(['Age'], as_index=False).mean().sort_values(by='Survived', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3315b27c-eba6-45ce-82ab-958d137a5df6","_uuid":"d861ce6213c63c6f241d3441845e07c3cfdbc86d"},"cell_type":"markdown","source":"### Family: SibSp and Parch\n\nThis section creates a new feature called FamilySize consisting of SibSp and Parch."},{"metadata":{"_cell_guid":"8be96fda-45ea-440c-a3ac-77f28a414fb8","_uuid":"2afc6cb8fc4ac8aa64e7a48444fc22206c4f2906","trusted":true,"collapsed":true},"cell_type":"code","source":"for dataset in full_data:\n# Create new feature FamilySize as a combination of SibSp and Parch\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch']+1\n# Create new feature IsAlone from FamilySize\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n    \n# Create new feature Boys from FamilySize\n    dataset['Boys'] = 0\n    dataset.loc[(dataset['Age'] == 0) & (dataset['Sex']==1), 'Boys'] = 1\n    \nfig, (axis1,axis2) = plt.subplots(1,2,figsize=(18,6))\nsns.barplot(x=\"FamilySize\", y=\"Survived\", hue=\"Sex\", data=train, ax = axis1);\nsns.barplot(x=\"IsAlone\", y=\"Survived\", hue=\"Sex\", data=train, ax = axis2);","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"32f0292d-91ab-4455-a7db-16c95c8d8e26","_uuid":"9177a25e4aafaa52f0d7cdb7244d2ecab6a3a300"},"cell_type":"markdown","source":"IsAlone does not result in a significant difference of survival rate. In addition, the slight difference between men and women go in different direction, i.e. IsAlone alone is not a good predictor of survival. O will drop this feature.\n\n### Fare"},{"metadata":{"trusted":true,"_uuid":"e6fcb6aa36ef79bdb8519ffb4247569a75b8c4bb","collapsed":true},"cell_type":"code","source":"# Interactive chart using cufflinks\nimport cufflinks as cf\ncf.go_offline()\ntrain['Fare'].iplot(kind='hist', bins=30)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c84ce842-21f8-4d32-97f5-03e7fbb659a2","_uuid":"83226906c0e1598b54b49de588407007087e4f89","trusted":true,"collapsed":true},"cell_type":"code","source":"# Remove all NULLS in the Fare column and create a new feature Categorical Fare\nfor dataset in full_data:\n    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\n\n# Explore Fare distribution \ng = sns.distplot(dataset[\"Fare\"], color=\"m\", label=\"Skewness : %.2f\"%(dataset[\"Fare\"].skew()))\ng = g.legend(loc=\"best\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"54bb83d0-ce33-473b-a3ee-9ef7e9b802e3","_uuid":"1aa0af52793efa3a5cd86f8a6e63a9c6e17636e6"},"cell_type":"markdown","source":"**Observations**\n* The Fare distribution is very skewed to the left. This can lead to overweigthing the model with very high values.\n* In this case, it is better to transform it with the log function to reduce the skewness and redistribute the data."},{"metadata":{"_cell_guid":"57516840-ecae-48b8-88f0-b4ecd44ccf6a","_uuid":"2e249c1203e6616e86ea41b0120c5e60e1a4ee4c","trusted":true,"collapsed":true},"cell_type":"code","source":"# Apply log to Fare to reduce skewness distribution\nfor dataset in full_data:\n    dataset[\"Fare\"] = dataset[\"Fare\"].map(lambda i: np.log(i) if i > 0 else 0)\na4_dims = (20, 6)\nfig, ax = pyplot.subplots(figsize=a4_dims)\ng = sns.distplot(train[\"Fare\"][train[\"Survived\"] == 0], color=\"r\", label=\"Skewness : %.2f\"%(train[\"Fare\"].skew()), ax=ax)\ng = sns.distplot(train[\"Fare\"][train[\"Survived\"] == 1], color=\"b\", label=\"Skewness : %.2f\"%(train[\"Fare\"].skew()))\n#g = g.legend(loc=\"best\")\ng = g.legend([\"Not Survived\",\"Survived\"])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4e42f446-91c9-4789-a7d3-3eaa58ca0eb2","_uuid":"9f85e56b9bc0719071d4954ecfa21f576ba387d9"},"cell_type":"markdown","source":"**Observations**\nLog Fare categories are:\n* 0 to 2.7: less survivors\n* More than 2.7 more survivors"},{"metadata":{"_cell_guid":"4bb510c8-ae73-44c2-b3dd-b8582c99cd2d","_uuid":"f932674fd50af20a7962cc48173ca084d9592731","trusted":true,"collapsed":true},"cell_type":"code","source":"for dataset in full_data:\n    dataset.loc[ dataset['Fare'] <= 2.7, 'Fare'] \t\t\t\t\t\t      = 0\n#    dataset.loc[(dataset['Fare'] > 2.7) & (dataset['Fare'] <= 3.2), 'Fare']   = 1\n#    dataset.loc[(dataset['Fare'] > 3.2) & (dataset['Fare'] <= 3.6), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 2.7, 'Fare'] \t\t\t\t\t\t\t  = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\ntrain['Fare'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"73b94ea9-1425-482a-bc80-fd3913d351ce","_uuid":"7b340889434f276651ebc8eabef131a83bf33164"},"cell_type":"markdown","source":"### Cabin"},{"metadata":{"_cell_guid":"c9853a1b-3760-4336-a7fc-c1d79a788dd1","_uuid":"69bdfde97b9fb7392f257640d3713831afc02470","trusted":true,"collapsed":true},"cell_type":"code","source":"# Feature that tells whether a passenger had a cabin on the Titanic (O if no cabin number, 1 otherwise)\nfor dataset in full_data:\n    dataset['Has_Cabin'] = dataset[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n\ntrain[[\"Has_Cabin\", \"Survived\"]].groupby(['Has_Cabin'], as_index=False).sum().sort_values(by='Survived', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2c91ef14-b77b-40b7-9ae7-ec1f1a1af791","_uuid":"a80942192868d63a901ada08b1b6b29d2f540862","trusted":true,"collapsed":true},"cell_type":"code","source":"train[[\"Has_Cabin\", \"Survived\"]].groupby(['Has_Cabin'], as_index=False).mean().sort_values(by='Survived', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d05f03cc-2cf1-4c97-bc5f-950d5e0daf4b","_uuid":"06bcc944b62d3b7a9374bf8abd2bc75593c19075"},"cell_type":"markdown","source":"It appears that Has_Cabin has a strong impact on the Survival rate. We will keep this feature.\n\n### Embarked"},{"metadata":{"_cell_guid":"8e320857-ed76-4c25-a87e-ffba6701e17a","_uuid":"10ae2b5348f8da830360198a7a00429cfe683721","trusted":true,"collapsed":true},"cell_type":"code","source":"for dataset in full_data:\n# Remove all NULLS in the Embarked column\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n# Mapping Embarked\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n    \ntrain_pivot = pd.pivot_table(train, values= 'Survived',index=['Embarked'],columns='Pclass',aggfunc=np.mean, margins=True)\ndef color_negative_red(val):\n    # Takes a scalar and returns a string with the css property 'color: red' if below 0.4, black otherwise.\n    color = 'red' if val < 0.4 else 'black'\n    return 'color: %s' % color\ntrain_pivot = train_pivot.style.applymap(color_negative_red)\ntrain_pivot","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2a110592-a7c2-4cec-83cc-c992927611f3","_uuid":"df1b2bc0562960d6b5de46223e77a4a2461b568f"},"cell_type":"markdown","source":"Irrespective of the class, passengers embarked in 0 (S) and 2 (Q) have lower chance of survival. I will combine those into the first category."},{"metadata":{"_cell_guid":"06aa2b9f-4291-43ba-9acf-638be8f7ef5c","_uuid":"d5a381a5a14fd7cfa7697d783868f4e3b3b05f4d","trusted":true,"collapsed":true},"cell_type":"code","source":"dataset['Embarked'] = dataset['Embarked'].replace(['0', '2'], '0')\ntrain['Fare'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d0e80ec4-b741-4c84-91b9-b9d3272977f5","_uuid":"f8082a689fd95a5c8643ce1746b13a89318f50e9"},"cell_type":"markdown","source":"### Titles"},{"metadata":{"_cell_guid":"55bdf2af-3432-45c2-89d2-38c5852f5f3a","_uuid":"545cdddcf6561fcd180e07d8c51b10993bd7b13a","trusted":true,"collapsed":true},"cell_type":"code","source":"# Define function to extract titles from passenger names\ndef get_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n # If the title exists, extract and return it.\n    if title_search:\n        return title_search.group(1)\n    return \"\"\nfor dataset in full_data:\n# Create a new feature Title, containing the titles of passenger names\n    dataset['Title'] = dataset['Name'].apply(get_title)\n\nfig, (axis1) = plt.subplots(1,figsize=(18,6))\nsns.barplot(x=\"Title\", y=\"Survived\", data=train, ax=axis1);","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d004379b-af09-4c09-85be-1523ea984e74","_uuid":"ce583b77f1d8af8f9784c1d332e696ff94bcde4e"},"cell_type":"markdown","source":"There are 4 types of titles:\n0. Mme, Ms, Lady, Sir, Mlle, Countess: 100%. \n1. Mrs, Miss: around 70% survival\n2. Master: around 60%\n3. Don, Rev, Capt, Jonkheer: no data\n4. Dr, Major, Col: around 40%\n5. Mr: below 20%"},{"metadata":{"_cell_guid":"a042e575-102a-4a5d-b0dd-822d3ea390c5","_uuid":"c030d8811ff9c10554b64451f13920d0afa5fbfc","trusted":true,"collapsed":true},"cell_type":"code","source":"for dataset in full_data:\n    dataset['Title'] = dataset['Title'].replace(['Mrs', 'Miss'], 'MM')\n    dataset['Title'] = dataset['Title'].replace(['Dr', 'Major', 'Col'], 'DMC')\n    dataset['Title'] = dataset['Title'].replace(['Don', 'Rev', 'Capt', 'Jonkheer'],'DRCJ')\n    dataset['Title'] = dataset['Title'].replace(['Mme', 'Ms', 'Lady', 'Sir', 'Mlle', 'Countess'],'MMLSMC' )\n# Mapping titles\n    title_mapping = {\"MM\": 1, \"Master\":2, \"Mr\": 5, \"DMC\": 4, \"DRCJ\": 3, \"MMLSMC\": 0}\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(3)\n    \n# Explore Age vs Survived\ng = sns.FacetGrid(train, col='Survived')\ng = g.map(sns.distplot, \"Age\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"aac81449-bad9-4d4a-a621-dfea7aaeaae7","_uuid":"1d27b00b224a855ace2e45fd2fbeb6b3d2ec03fb","trusted":true,"collapsed":true},"cell_type":"code","source":"train[[\"Title\", \"Survived\"]].groupby(['Title'], as_index=False).mean().sort_values(by='Survived', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1414cb27-994d-4147-bd4b-037d7c93b6e4","_uuid":"f0962278ecad96763bd57279ab27e4b2908be45e"},"cell_type":"markdown","source":"### Extracting deck from cabin\nA cabin number looks like ‘C123’ and the letter refers to the deck: a big thanks to Nikas Donge.\nTherefore we’re going to extract these and create a new feature, that contains a persons deck. Afterwords we will convert the feature into a numeric variable. The missing values will be converted to zero."},{"metadata":{"_cell_guid":"cc6c3f84-55d9-46d4-b5d4-6155017b5758","_uuid":"501ccfdb8e80e7f17cad906c1df68a5fbcaad1e0","trusted":true,"collapsed":true},"cell_type":"code","source":"deck = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7, \"U\": 8}\nfor dataset in full_data:\n    dataset['Cabin'] = dataset['Cabin'].fillna(\"U0\")\n    dataset['Deck'] = dataset['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\n    dataset['Deck'] = dataset['Deck'].map(deck)\n    dataset['Deck'] = dataset['Deck'].fillna(0)\n    dataset['Deck'] = dataset['Deck'].astype(int) \ntrain['Deck'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"aba7fa4f-867a-4caf-9185-27a079f49b7d","_uuid":"fd4d281901a8d460bbdb6d0fdb7fae65906287ed","trusted":true,"collapsed":true},"cell_type":"code","source":"sns.barplot(x = 'Deck', y = 'Survived', order=[1,2,3,4,5,6,7,8], data=train)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0105edb5-86c2-4f4c-af82-f3c55908e211","_uuid":"df0bf2763c869182380da8611b2a3aca2a83fa83"},"cell_type":"markdown","source":"3 types of deck: 1 with 15 passengers, 2 to 6, and 7 to 8 (most passengers)"},{"metadata":{"_cell_guid":"c36e2a86-29cc-4c2a-aeb5-1d9de7efa4d2","_uuid":"9e3df054a01706ac8980a1f673281e8996f74c46","trusted":true,"collapsed":true},"cell_type":"code","source":"for dataset in full_data:\n    dataset.loc[ dataset['Deck'] <= 1, 'Deck'] = 1\n    dataset.loc[(dataset['Deck'] > 1) & (dataset['Deck'] <= 6), 'Deck']  = 3\n    dataset.loc[ dataset['Deck'] > 6, 'Deck'] = 0\ntrain[[\"Deck\", \"Survived\"]].groupby(['Deck'], as_index=False).mean().sort_values(by='Survived', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e9814168-b7cd-d4e4-1b1d-e21c6637a663","_uuid":"dd288776321804d99e4e4a7e88594c1d631e4409"},"cell_type":"markdown","source":"## 2.4 Visualising updated dataset"},{"metadata":{"_cell_guid":"6bd36cde-7b08-42ed-b191-87e27b320fc5","_uuid":"5f8a57d9785370134f2488610dd81a84e9e45096","scrolled":false,"trusted":true,"collapsed":true},"cell_type":"code","source":"test.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"fc426b8f-873d-6f23-4299-99f174956cca","_execution_state":"idle","_uuid":"1f280a1c11dc35a93b57af494938998e6d0b4544","trusted":true,"collapsed":true},"cell_type":"code","source":"train.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"009ce45c-f4db-4718-ba0c-9c3e34d0e8ee","_uuid":"069695c07d229c23cb2edf99d6f5ad901b934e1d"},"cell_type":"markdown","source":"## 2.5. Descriptive statistics"},{"metadata":{"_cell_guid":"522a767e-e466-4f8e-9255-039c391cded8","_uuid":"9c27c628974b2556f03b8bd54fb81771bce4cf25","scrolled":true,"trusted":true,"collapsed":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"25f0e07e-4d62-4f12-abb8-cfcf1ba0a95f","_uuid":"bb72bebe9916f26581bd773823dbe4d410787ac9","trusted":true,"collapsed":true},"cell_type":"code","source":"train[['Pclass', 'Sex', 'Age', 'Parch', 'Fare', 'Embarked', 'Has_Cabin', 'FamilySize', 'Title', 'Survived']].groupby(['Survived'], as_index=False).mean().sort_values(by='Pclass', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6a0bde77-40d6-4b59-8e3c-7a08b219bd30","_uuid":"991534cba51f0387c1cd8a8d5457c7ccd7bafd4b"},"cell_type":"markdown","source":"**Initial observations from the descriptive statistics:**\n* Only 38% survived, a real tragedy :-(\n* Passengers in more expensive classes 1 and 2 had much higher chance of surviving than classes 3 or 4.\n* Also, the higher the fare, the higher the chance. Similarly, having a cabin increases the chance of survival.\n* Women (0) higher chance than men (1)\n* Younger people slightly more chance than older\n* Being alone decreased your chance to survive.\n\nWe will drop unncessary features just before Section 3.1. Pearson Correlation heatmap."},{"metadata":{"_cell_guid":"d51a4163-10c2-443e-bec2-0ec3b2a93b40","_uuid":"d3d1a1236728a132b0bc4e02f85da6e1a7ce2942"},"cell_type":"markdown","source":"# 3. Correlation analysis - Multi-variate analysis\nThis section summarizes  bivariate analysis asthe simplest forms of quantitative (statistical) analysis.\nIt involves the analysis of one or two features, and their relative impact of \"Survived\". \nThis is a useful frist step of our anblaysis in order to determine the empirical relationship between all features."},{"metadata":{"_cell_guid":"2ae4e483-0ae8-422d-abe5-71330366e0a3","_uuid":"58a067cb3eb5edbccb6f9f20fab06c26c1d02da0"},"cell_type":"markdown","source":"## 3.1. Correlation analysis with histograms and pivot-tables"},{"metadata":{"_cell_guid":"3e32fa2e-496f-47ab-a8a0-0a7277d0473f","_uuid":"4a7d5a8d1e764a7f5a7cea45ca373488717382da","trusted":true,"collapsed":true},"cell_type":"code","source":"fig, (axis1,axis2) = plt.subplots(1,2,figsize=(18,6))\nsns.barplot(x=\"Embarked\", y=\"Survived\", hue=\"Sex\", data=train, ax = axis1);\nsns.barplot(x=\"Age\", y=\"Survived\", hue=\"Sex\", data=train, ax = axis2);","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"25e95e2f-5be3-4afe-b23f-47bc8fe80620","_uuid":"de57c4821ae253395ad26821f6eaacfcde79801c"},"cell_type":"markdown","source":"**Observations for Age graph:**\n* 0 or blue represent women; 1 or orange represent men. Gender and age seem to have a stronger influece of the survival rate.\n* We start to find where most survivors are: older women (48 to 64 year old), and younger passengers.\n* What is statistically interesting is that only young boys (Age Category = 0) have  high survival rates, unlike other age groups for men.\n* We will create a new feature called young boys"},{"metadata":{"_cell_guid":"63f1405e-1616-432f-a573-8f838da729d4","_uuid":"d513de669190fa5f4f77e1df27112e03299d4235","collapsed":true,"trusted":true},"cell_type":"code","source":"# for dataset in full_data:\n#    dataset['Boys'] = 0\n#    dataset.loc[(dataset['Age'] == 0) & (dataset['Sex']==1), 'Boys'] = 1\n# dataset['Boys'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"96ad7209-d261-4004-a696-e0747d831ea3","_uuid":"cad712b4c8479d44b80790ea3d30f0e481e4b3a1","trusted":true,"collapsed":true},"cell_type":"code","source":"train[[\"FamilySize\", \"Survived\"]].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ee461836-53d3-4e66-90d3-50bdff72907d","_uuid":"5871dede5537723dc822af51c4b744bd5349dd04","trusted":true,"collapsed":true},"cell_type":"code","source":"for dataset in full_data:\n    dataset['Gender_Embarked'] = 0\n    dataset.loc[(dataset['Sex']==0) & (dataset['Embarked']==0), 'Gender_Embarked'] = 0\n    dataset.loc[(dataset['Sex']==0) & (dataset['Embarked']==2), 'Gender_Embarked'] = 1\n    dataset.loc[(dataset['Sex']==0) & (dataset['Embarked']==1), 'Gender_Embarked'] = 2\n    dataset.loc[(dataset['Sex']==1) & (dataset['Embarked']==2), 'Gender_Embarked'] = 3\n    dataset.loc[(dataset['Sex']==1) & (dataset['Embarked']==0), 'Gender_Embarked'] = 4\n    dataset.loc[(dataset['Sex']==1) & (dataset['Embarked']==1), 'Gender_Embarked'] = 5\ntrain[[\"Gender_Embarked\", \"Survived\"]].groupby(['Gender_Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1ad428d8-c429-40e1-bf63-f1322ab0378f","_uuid":"97d135ceed4cbb2a5df1ae118dff34feb90d3930","trusted":true,"collapsed":true},"cell_type":"code","source":"train_pivot = pd.pivot_table(train, values= 'Survived',index=['Title', 'Pclass'],columns='Sex',aggfunc=np.mean, margins=True)\ndef color_negative_red(val):\n    # Takes a scalar and returns a string with the css property 'color: red' if below 0.4, black otherwise.\n    color = 'red' if val < 0.4 else 'black'\n    return 'color: %s' % color\ntrain_pivot = train_pivot.style.applymap(color_negative_red)\ntrain_pivot","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f6f28a5b-fee9-4f70-bfb1-7f0f026de045","_uuid":"224a3f391434aa77da825c3984324be2c0b17335","trusted":true,"collapsed":true},"cell_type":"code","source":"# grid = sns.FacetGrid(train_df, col='Pclass', hue='Survived')\ngrid = sns.FacetGrid(train, col='Survived', row='Pclass', size=2, aspect=3)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=8)\ngrid.add_legend();","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9585af36-9516-4067-8ba5-a290fea149df","_uuid":"0ae82877c2b4154eb3fe24c94028ee3ad6cef603"},"cell_type":"markdown","source":"**Observations: here are the survivors!**\n1. Family-size of 3 or 4 from first pivot\n2. Women and men alone on first class (second pivot, red showing survival rate below 0.4)\n3. Top-right in the graph above: first class and age categories 1 and 2\n\n** The not-so lucky are mostly in men, Pclass 3 and age category 1 (younger folks)**"},{"metadata":{"_cell_guid":"1388dfc6-3fed-4012-ae0e-d5663f8ca0dc","_uuid":"a3b586bbc09c57c2ff16aed5ebf16c4188400b86","trusted":true,"collapsed":true},"cell_type":"code","source":"#graph distribution of qualitative data: Pclass\nfig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(18,8))\n\nsns.boxplot(x = 'Pclass', y = 'Fare', hue = 'Survived', data = train, ax = axis1)\naxis1.set_title('Pclass vs Fare Survival Comparison')\n\nsns.violinplot(x = 'Pclass', y = 'Age', hue = 'Survived', data = train, split = True, ax = axis2)\naxis2.set_title('Pclass vs Age Survival Comparison')\n\nsns.boxplot(x = 'Pclass', y ='FamilySize', hue = 'Survived', data = train, ax = axis3)\naxis3.set_title('Pclass vs Family Size Survival Comparison')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c1b66928-c57e-430b-b6f3-6156eaeb82cc","_uuid":"29ae992e23cde108c90ee8b0130352590bd507c4","trusted":true,"collapsed":true},"cell_type":"code","source":"fig, saxis = plt.subplots(2, 3,figsize=(18,8))\n\nsns.barplot(x = 'Embarked', y = 'Survived', data=train, ax = saxis[0,0])\nsns.barplot(x = 'Pclass', y = 'Survived', order=[1,2,3], data=train, ax = saxis[0,1])\nsns.barplot(x = 'Deck', y = 'Survived', order=[1,0], data=train, ax = saxis[0,2])\n\nsns.pointplot(x = 'Fare', y = 'Survived',  data=train, ax = saxis[1,0])\nsns.pointplot(x = 'Age', y = 'Survived',  data=train, ax = saxis[1,1])\nsns.pointplot(x = 'FamilySize', y = 'Survived', data=train, ax = saxis[1,2])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5723fb3e-1bfc-435a-9e5f-0586378712d6","_uuid":"479bc09cced41f18fa04843b6cad657de698cb53","trusted":true,"collapsed":true},"cell_type":"code","source":"# grid = sns.FacetGrid(train_df, col='Embarked')\ngrid = sns.FacetGrid(train, row='Has_Cabin', size=2.2, aspect=1.2)\ngrid.map(sns.pointplot, 'Parch', 'Survived', 'Sex', palette='deep')\ngrid.add_legend()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f921f497-9142-44a8-9c75-0cda20c18c35","_uuid":"46df6a79557425480e5121b7a3ecae97778ab2bb"},"cell_type":"markdown","source":"**Observations:**\n* The colors represent: blue=0 is for women, green=1 for men\n* Clearly, women had more chance of surviving, with or without cabin\n* Interesting is that accompanied women without a cabin had less survival chance than women alone without cabin.\n    But this is not true for men. Men alone have less chance than accompanied.\n    \n    **Bottom-line: it would have been better for women without cabin to pretend that they were alone.\n    And lone men should join a family to improve their survival rates.**"},{"metadata":{"_cell_guid":"4cf95e0a-3e2c-4fb9-8b42-ff84dd2f1290","_uuid":"60cd43769a62028eab5a4675b539def728591f6a"},"cell_type":"markdown","source":"## 3.2. Dropping features\nBottom-line of the bi-variate and tri-variate analysis as well as the feature importance analysis (from running the classifiers multiple times), **I decided to drop less-relevant features**. This happened as an iterative process by reviwing the outcome of the feature importance graph in the next section.\nThe problem with less important features is that they create more noice and actually take over the importance of real features like Sex and Pclass.\n\n**The next step after dropping less-relevant features is to scale them, a very good recommendation from Konstantin's kernel**\nIt helps to boost the score. Scaling features is helpful for many ML algorithms like KNN for example, it really boosts their score.\nFeature scaling is a method used to standardize the range of independent variables or features of data. In data processing, it is also known as data normalization.\nFeature standardization makes the values of each feature in the data have zero-mean (when subtracting the mean in the numerator) and unit-variance.\nThe general method of calculation is to determine the distribution mean and standard deviation for each feature. Next we subtract the mean from each feature. Then we divide the values (mean is already subtracted) of each feature by its standard deviation."},{"metadata":{"_cell_guid":"a6f00634-34d5-408b-8e5e-e45102483a62","_uuid":"bf9305cbadc75baad3cefec40ca0da934cfe94a7","collapsed":true,"trusted":true},"cell_type":"code","source":"# Feature selection\ndrop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp', 'Boys', 'IsAlone', 'Embarked']\n\ntrain = train.drop(drop_elements, axis = 1)\ntest  = test.drop(drop_elements, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f4a88f82-63d4-4e40-82f2-b508c9337438","_uuid":"f41988e138dff50dcb8d619ea01f39d5d1da6072"},"cell_type":"markdown","source":"## 3.3. Pearson Correlation Heatmap\n\nThe Seaborn plotting package allows us to plot heatmaps showing the Pearson product-moment correlation coefficient (PPMCC) correlation between features.\nPearson is bivariate correlation, measuring the linear correlation between two features. "},{"metadata":{"_cell_guid":"53b4f1cb-489f-4b86-9764-eaec11d3525c","_uuid":"7c73bf7277d92eff2c85a30184b4547fc2bbf3b5","trusted":true,"collapsed":true},"cell_type":"code","source":"colormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=True)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ccc92745-0680-df4d-d709-10003475d8e3","_uuid":"b6c7a1ea3866d177016a1e13c5d9e7341c49c147"},"cell_type":"markdown","source":"**Observations from the Pearson analysis:** \n* Correlation coefficients with magnitude between 0.5 and 0.7 indicate variables which can be considered **moderately correlated**.\n* We can see from the red cells that many features are \"moderately\" correlated: specifically, IsAlone, Pclass, Name_length, Fare, Sex.\n* This is influenced by the following two factors: 1) Women versus men (and the compounding effect of Name_length) and 2) Passengers paying a high price (Fare) have a higher chance of survival: there are also in first class, have a title. \n\n\n## 3.4. Pairplots\n\nFinally let us generate some pairplots to observe the distribution of data from one feature to the other.\nThe Seaborn pairplot class will help us visualize the distribution of a feature in relationship to each others."},{"metadata":{"_cell_guid":"ea6b0a8f-5a33-666f-8057-c0d689f370f5","_execution_state":"idle","_uuid":"624446543aafd518025fd3f5346d32ee1aab6f9a","trusted":true,"collapsed":true},"cell_type":"code","source":"g = sns.pairplot(train[[u'Survived', u'Pclass', u'Sex', u'Age', u'Fare',\n       u'FamilySize', u'Title']], hue='Survived', palette = 'seismic',size=1.2,diag_kind = 'kde',diag_kws=dict(shade=True),plot_kws=dict(s=10) )\ng.set(xticklabels=[])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2ab49c3a-de30-4bb1-ae56-12465e585fda","_uuid":"10ee628eeb49bd0d6b82bd3731e11c23f8b896a4"},"cell_type":"markdown","source":"**Observations**\n* The pairplot graph all trivariate analysis into one figure.\n* The clustering of red dots indicates the combination of two features results in higher survival rates, or the opposite (clustering of blue dots = lower survival)\nFor example:\n- Smaller family sizes in first and second class\n- Middle age with Pclass in third category = only blue dot\nThis can be used to validate that we extracted the right features or help us define new ones."},{"metadata":{"_cell_guid":"18bd73d2-b436-4500-af02-59274fa97358","_uuid":"4ccf0eb696b2ae00be51777dee3f75f32cd19ad2","collapsed":true,"trusted":true},"cell_type":"code","source":"# X_train (all features for training purpose but excluding Survived),\n# Y_train (survival result of X-Train) and test are our 3 main datasets for the next sections\nX_train = train.drop(\"Survived\", axis=1)\nY_train = train[\"Survived\"]\nX_train.shape, Y_train.shape, test.shape\n\nfrom sklearn.cross_validation import train_test_split\nX_train, x_test, Y_train, y_test = train_test_split(X_train, Y_train, test_size=0.3, random_state=101)\n\nX_test = test.copy() # test data for Kaggle submission\n#std_scaler = StandardScaler()\n#X_train = std_scaler.fit_transform(X_train)\n#X_test = std_scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3eb2dad3-31d7-4367-a0ab-881729c6e8e8","_uuid":"5336a714f678d54e161172ef22827489d1a86a83"},"cell_type":"markdown","source":"# 4. Predictive modelling, cross-validation, hyperparameters and ensembling\n\n* 4.1. Logistic Regression\n* 4.2. Support Vector Machines (supervised)\n* 4.3. k-Nearest Neighbors algorithm (k-NN)\n* 4.4. Naive Bayes classifier\n* 4.5. Perceptron\n* 4.6 Linear SVC\n* 4.7 Stochastic Gradient Descent\n* 4.8. Decision tree\n* 4.9 Random Forrest\n* 4.10 Model summary\n* 4.11. Model cross-validation with K-Fold\n* 4.12 Hyperparameter tuning & learning curves for selected classifiers\n* 4.13 Selecting and combining the best classifiers\n* 4.14 Ensembling\n* 4.15. Summary of most important features\n\n## 4.1. Logistic Regression\nLogistic regression measures the relationship between the categorical dependent feature (in our case Survived) and the other independent features.\nIt estimates probabilities using a cumulative logistic distribution:\n* The first value shows the accuracy of this model\n* The table after this shows the importance of each feature according this classifier."},{"metadata":{"_cell_guid":"d64bab2c-7654-4d0e-a2d3-77103e80e50f","_uuid":"1e8259699d46e14e005b9cb104b4d68848a6d8de","trusted":true,"collapsed":true},"cell_type":"code","source":"logreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\nY_pred1 = logreg.predict(x_test)\nacc_log = round(logreg.score(x_test, y_test) * 100, 2)\nacc_log","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dc82a40522b6352f8119d239e9018cd43dadf303","collapsed":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report\nprint(classification_report(y_test, Y_pred1))\ncm = pd.DataFrame(confusion_matrix(y_test, Y_pred1), ['Actual: NOT', 'Actual: SURVIVED'], ['Predicted: NO', 'Predicted: SURVIVED'])\nprint(cm)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5768d5f9-4d74-4396-a493-64f99e1fba0e","_uuid":"25f194716b1252b1f8c38e4bafc125b508ff400f","collapsed":true,"trusted":true},"cell_type":"code","source":"#coeff_df = pd.DataFrame(X_train.columns.delete(0))\n#coeff_df.columns = ['Feature']\n#coeff_df[\"Correlation\"] = pd.Series(logreg.coef_[0])\n#coeff_df.sort_values(by='Correlation', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"537434f6-8d86-4bab-9828-a92909c14fe9","_uuid":"2e3d6369eb83be933e874845ee87610be012a1cb"},"cell_type":"markdown","source":"**Observation:**\n* This classfier confirms the importance of Name_length\n* FamilySize did not show a strong Pearson correlation with Survived but comes here to the top. This can be due to its strong relationship with other features such as Is_Alone or Parch (Parent-Children).\n\n\n## 4.2. Support Vector Machines (supervised)\nGiven a set of training samples, each sample is marked as belonging to one or the other of two categories.\n\nThe SVM training algorithm builds a model that assigns new test samples to one category or the other, making it a non-probabilistic binary linear classifier."},{"metadata":{"_cell_guid":"0f01682c-4a1d-414c-8905-76897a32ce13","_uuid":"79bdd20f5eb3ef2685384aeea7fe6c9140442163","trusted":true,"collapsed":true},"cell_type":"code","source":"svc=SVC()\nsvc.fit(X_train, Y_train)\nY_pred2 = svc.predict(x_test)\nacc_svc = round(svc.score(x_test, y_test) * 100, 2)\nacc_svc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"014fe936f5dd3734248ba1a81f5a3216a26de9c3","collapsed":true},"cell_type":"code","source":"print(classification_report(y_test, Y_pred2))\ncm = pd.DataFrame(confusion_matrix(y_test, Y_pred2), ['Actual: NOT', 'Actual: SURVIVED'], ['Predicted: NO', 'Predicted: SURVIVED'])\nprint(cm)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a9c6eaa4-bad7-4c48-9d6b-3e02e655a183","_uuid":"be87e04341b24917d87761256e67304c19412cc6"},"cell_type":"markdown","source":"## 4.3. k-Nearest Neighbors algorithm (k-NN)\nThis is a non-parametric method used for classification and regression.\nA sample is classified by a majority vote of its neighbors, with the sample being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor. "},{"metadata":{"_cell_guid":"3e21e1c1-abb6-4c00-a6aa-ce293e0d9ca4","_uuid":"0baf00ea77739487b08035892793c9983c6141d8","trusted":true,"collapsed":true},"cell_type":"code","source":"knn = KNeighborsClassifier(algorithm='auto', leaf_size=26, metric='minkowski', \n                           metric_params=None, n_jobs=1, n_neighbors=10, p=2, \n                           weights='uniform')\nknn.fit(X_train, Y_train)\nknn_predictions = knn.predict(x_test)\nacc_knn = round(knn.score(x_test, y_test) * 100, 2)\n\n# Preparing data for Submission 1\ntest_Survived = pd.Series(knn_predictions, name=\"Survived\")\nSubmission1 = pd.concat([PassengerId,test_Survived],axis=1)\nacc_knn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0d13d4c4159b159181c15eefd24317023f21e8f6","collapsed":true},"cell_type":"code","source":"print(classification_report(y_test, knn_predictions))\ncm = pd.DataFrame(confusion_matrix(y_test, knn_predictions), ['Actual: NOT', 'Actual: SURVIVED'], ['Predicted: NO', 'Predicted: SURVIVED'])\nprint(cm)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"32c4f81d-832a-4260-8f33-df50e172a9b2","_uuid":"8563015000d21eb1fc0f13a5c683dd39062c1a4a","trusted":true,"collapsed":true},"cell_type":"code","source":"Submission1.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e6802255-8522-4de4-b519-6775a2eb31ac","_uuid":"5700969e1be9c7b9a13bc80f57f500b41f9f7589","trusted":true,"collapsed":true},"cell_type":"code","source":"## Selecting the right n_neighbors for the k-NN classifier\nx_trainknn, x_testknn, y_trainknn, y_testknn = train_test_split(X_train,Y_train,test_size = .33, random_state = 0)\nnn_scores = []\nbest_prediction = [-1,-1]\nfor i in range(1,100):\n    knn = KNeighborsClassifier(n_neighbors=i, weights='distance', metric='minkowski', p =2)\n    knn.fit(x_trainknn, y_trainknn)\n    score = accuracy_score(y_testknn, knn.predict(x_testknn))\n    #print i, score\n    if score > best_prediction[1]:\n        best_prediction = [i, score]\n    nn_scores.append(score)\nprint (best_prediction)\nplt.plot(range(1,100),nn_scores)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"08739336-522e-4b96-a51d-b32ca035937b","_uuid":"691639deb7d9cc3a5200a38ccf45c45204b694a1"},"cell_type":"markdown","source":"## 4.4. Naive Bayes classifier\nThis is a family of simple probabilistic classifiers based on applying Bayes' theorem with strong (naive) independence assumptions between the features. Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of features in a learning problem."},{"metadata":{"_cell_guid":"4d363cdb-9f1d-4e43-9750-38b82971f223","_uuid":"f8338165241ba93eb873903e0bf02fe603c5bfb3","trusted":true,"collapsed":true},"cell_type":"code","source":"gaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\nY_pred3 = gaussian.predict(x_test)\nacc_gaussian = round(gaussian.score(x_test, y_test) * 100, 2)\nacc_gaussian","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2dab386c7fc9e86f8c3677732d570030ad6dd203","collapsed":true},"cell_type":"code","source":"print(classification_report(y_test, Y_pred3))\ncm = pd.DataFrame(confusion_matrix(y_test, Y_pred3), ['Actual: NOT', 'Actual: SURVIVED'], ['Predicted: NO', 'Predicted: SURVIVED'])\nprint(cm)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"940ba7a6-109d-447c-8f0f-35d95d751ee7","_uuid":"e32fab307ef58b8e00bcf2f956ed50094a4bb5c9"},"cell_type":"markdown","source":"## 4.5. Perceptron\nThis is an algorithm for supervised learning of binary classifiers: like the other classifiers before, it decides whether an input, represented by a vector of numbers, belongs to some specific class or not. It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector. The algorithm allows for online learning, in that it processes elements in the training set one at a time."},{"metadata":{"_cell_guid":"e1548c66-6c08-4f0a-a8a4-79962ede6cb1","_uuid":"74a727ebe8bd1a1ee7ea25840754e94af455ee2c","trusted":true,"collapsed":true},"cell_type":"code","source":"perceptron = Perceptron()\nperceptron.fit(X_train, Y_train)\nY_pred4 = perceptron.predict(x_test)\nacc_perceptron = round(perceptron.score(x_test, y_test) * 100, 2)\nacc_perceptron","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0775ea4e23a8921457ac1605127640c945941ff1","collapsed":true},"cell_type":"code","source":"print(classification_report(y_test, Y_pred4))\ncm = pd.DataFrame(confusion_matrix(y_test, Y_pred4), ['Actual: NOT', 'Actual: SURVIVED'], ['Predicted: NO', 'Predicted: SURVIVED'])\nprint(cm)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b9a23f92-0bbf-48c4-9ab6-15dbfe5116a7","_uuid":"2a9436f61d14962f8516df1fc1a18b1a3a72c135"},"cell_type":"markdown","source":"## 4.6. Linear SVC\nThis is another implementation of Support Vector Classification (similar to 4.2.) for the case of a linear kernel."},{"metadata":{"_cell_guid":"60611a3f-fd51-4229-acaf-c133ca2e9847","_uuid":"17e366e5f4eb069cbfdbad90453b6e76f05acdef","trusted":true,"collapsed":true},"cell_type":"code","source":"linear_svc = LinearSVC()\nlinear_svc.fit(X_train, Y_train)\nY_pred5 = linear_svc.predict(x_test)\nacc_linear_svc = round(linear_svc.score(x_test, y_test) * 100, 2)\nacc_linear_svc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c03ad12b9cd51cb3db1b352dbbbb8984fbd1e438","collapsed":true},"cell_type":"code","source":"print(classification_report(y_test, Y_pred5))\ncm = pd.DataFrame(confusion_matrix(y_test, Y_pred5), ['Actual: NOT', 'Actual: SURVIVED'], ['Predicted: NO', 'Predicted: SURVIVED'])\nprint(cm)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"244579fa-56f2-4bb6-8b1f-22f4ff061da9","_uuid":"5fce77fff72d983e6e079a9a4a878f200c8249c5"},"cell_type":"markdown","source":"## 4.7. Stochastic Gradient Descent (sgd)\nThis is a stochastic approximation of the gradient descent optimization and iterative method for minimizing an objective function that is written as a sum of differentiable functions. In other words, SGD tries to find minima or maxima by iteration."},{"metadata":{"_cell_guid":"2b46b161-4da8-4652-a250-6173618b8f10","_uuid":"a6ec7ea6c3b3a749ac38e9d54aa3571b14952322","trusted":true,"collapsed":true},"cell_type":"code","source":"sgd = SGDClassifier()\nsgd.fit(X_train, Y_train)\nY_pred6 = sgd.predict(x_test)\nacc_sgd = round(sgd.score(x_test, y_test) * 100, 2)\nacc_sgd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fd97601e09aaa5e728a0694330da63d0184e19f6","collapsed":true},"cell_type":"code","source":"print(classification_report(y_test, Y_pred6))\ncm = pd.DataFrame(confusion_matrix(y_test, Y_pred6), ['Actual: NOT', 'Actual: SURVIVED'], ['Predicted: NO', 'Predicted: SURVIVED'])\nprint(cm)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"56d4b334-9d9b-461a-bcc8-e249647222a2","_uuid":"e5ff90c38f7a875bdc8cc78e3a56204759d6d088"},"cell_type":"markdown","source":"## 4.8. Decision tree\nThis predictive model  maps features (tree branches) to conclusions about the target value (tree leaves).\n\nThe target features  take a finite set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees."},{"metadata":{"_cell_guid":"ff611104-8d18-40ea-a4d9-1cc6ae190312","_uuid":"94fa5a802bfd7692d3f9fb3f10a77840bedc2807","trusted":true,"collapsed":true},"cell_type":"code","source":"decision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\nY_pred7 = decision_tree.predict(x_test)\nacc_decision_tree = round(decision_tree.score(x_test, y_test) * 100, 2)\nacc_decision_tree","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8fa762a1915848f9593403816de0c344aed158aa","collapsed":true},"cell_type":"code","source":"print(classification_report(y_test, Y_pred7))\ncm = pd.DataFrame(confusion_matrix(y_test, Y_pred7), ['Actual: NOT', 'Actual: SURVIVED'], ['Predicted: NO', 'Predicted: SURVIVED'])\nprint(cm)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d2250de2-014f-44fe-8d1e-6802dc4256c9","_uuid":"d2fb348dae0ae80f8d92a7cb352dc476c0df834e"},"cell_type":"markdown","source":"## 4.9. Random Forests\nThis is one of the most popular classfier.\nRandom forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees (n_estimators=100) at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees"},{"metadata":{"_cell_guid":"a23b35cd-e90b-4414-b19a-aab033fc5197","_uuid":"c97d4e9bb57a147d8871ac55ede16c92f94f8937","trusted":true,"collapsed":true},"cell_type":"code","source":"random_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\nrandom_forest_predictions = random_forest.predict(x_test)\nacc_random_forest = round(random_forest.score(x_test, y_test) * 100, 2)\n\n\n# Preparing data for Submission 2\ntest_Survived = pd.Series(random_forest_predictions, name=\"Survived\")\nSubmission2 = pd.concat([PassengerId,test_Survived],axis=1)\n\nacc_random_forest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3fb840a5153d3230b04053d5b20e9d7cf2afadc4","collapsed":true},"cell_type":"code","source":"print(classification_report(y_test, random_forest_predictions))\ncm = pd.DataFrame(confusion_matrix(y_test, random_forest_predictions), ['Actual: NOT', 'Actual: SURVIVED'], ['Predicted: NO', 'Predicted: SURVIVED'])\nprint(cm)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"af2231ef-5396-4dcc-9622-b134f75adcf3","_uuid":"a6c47798f6078f3af1e46714dd9ea2f3afd89662"},"cell_type":"markdown","source":"## 4.10. Model summary\nI found that the picture illustrates the various model better than words.\nThis should be taken with a grain of salt, as the intuition conveyed by these two-dimensional examples does not necessarily carry over to real datasets.\nThe reality os that the algorithms work with many dimensions (11 in our case).\n\nBut it shows how each classifier algorithm partitions the same data in different ways.\nThe three rows represent the three different data set on the right.\nThe plots show training points in solid colors and testing points semi-transparent. The lower right shows the classification accuracy on the test set.\n\nFor instance, the visualization helps understand how RandomForest uses multiple Decision Trees, the linear SVC, or Nearest Neighbors grouping sample by their relative distance to each others.\n\n![image](http://scikit-learn.org/0.15/_images/plot_classifier_comparison_0011.png)\n"},{"metadata":{"_cell_guid":"9235ce64-cec0-496f-a906-9d9fb11ac6eb","_uuid":"03ca3ec943ac68754945b897831809945cc49ba1","trusted":true,"collapsed":true},"cell_type":"code","source":"objects = ('Logistic Regression', 'SVC', 'KNN', 'Gaussian', 'Perceptron', 'linear SVC', 'SGD', 'Decision Tree', 'Random Forest')\nx_pos = np.arange(len(objects))\naccuracies1 = [acc_log, acc_svc, acc_knn, acc_gaussian, acc_perceptron, acc_linear_svc, acc_sgd, acc_decision_tree, acc_random_forest]\n    \nplt.bar(x_pos, accuracies1, align='center', alpha=0.5, color='r')\nplt.xticks(x_pos, objects, rotation='vertical')\nplt.ylabel('Accuracy')\nplt.title('Classifier Outcome')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"04d20d05-98ed-45f6-84d9-aa3e70e9d2af","_uuid":"9fa35c1dda5fe275477b1aac2a23b18ca29a0bbb"},"cell_type":"markdown","source":"**Observations**\n* The above models (classifiers) were applied to a split training and x_test datasets.\n* This results in some classifiers (Decision_tree and Random_Forest) over-fitting the model to the training data. \n* This happens when the classifiers use many input features (to include noise in each feature) on the complete dataset, and ends up “memorizing the noise” instead of finding the signal.\n* This overfit model will then make predictions based on that noise. It performs unusually well on its training data, but will not necessarilyimprove the prediction quality with new data from the test dataset.\n* In the next section, we will cross-validate the models using sample data against each others. We will this by using StratifiedKFold to train and test the models on sample data from the overall dataset.\nStratified K-Folds is a cross validation iterator. It provides train/test indices to split data in train test sets. This cross-validation object is a variation of KFold, which returns stratified folds. The folds are made by preserving the percentage of samples for each class."},{"metadata":{"_cell_guid":"6f4eb75d-3718-4bf9-9bda-ca53fe8f3826","_uuid":"230e8659fb39cf8fdc6e0d531bf9a98c9674d84d"},"cell_type":"markdown","source":"## 4.11. Model cross-validation with K-Fold\n\nThe fitting process applied above optimizes the model parameters to make the model fit the training data as well as possible.\nCross-validation is a way to predict the fit of a model to a hypothetical validation set when an explicit validation set is not available.\nIn simple words, it allows to test how well the model performs on new data.\nIn our case, cross-validation will also be applied to compare the performances of different predictive modeling procedures. \n![Cross-validation process:](https://image.slidesharecdn.com/kagglesharingmarkpeng20151216finalpresented-151216161621/95/general-tips-for-participating-kaggle-competitions-13-638.jpg?cb=1452565877)\n### Cross-validation scores"},{"metadata":{"_cell_guid":"51a3d12e-6709-4c99-8499-e9065de25d21","_uuid":"cd1a76ef5d8a7093e3d05ef0f244591e9ddafe8c","trusted":true,"collapsed":true},"cell_type":"code","source":"# Cross validate model with Kfold stratified cross validation\nfrom sklearn.model_selection import StratifiedKFold\nkfold = StratifiedKFold(n_splits=10)\n# Modeling step Test differents algorithms \nrandom_state = 2\n\nclassifiers = []\nclassifiers.append(LogisticRegression(random_state = random_state))\nclassifiers.append(SVC(random_state=random_state))\nclassifiers.append(KNeighborsClassifier())\nclassifiers.append(GaussianNB())\nclassifiers.append(Perceptron(random_state=random_state))\nclassifiers.append(LinearSVC(random_state=random_state))\nclassifiers.append(SGDClassifier(random_state=random_state))\nclassifiers.append(DecisionTreeClassifier(random_state = random_state))\nclassifiers.append(RandomForestClassifier(random_state = random_state))\n\ncv_results = []\nfor classifier in classifiers :\n    cv_results.append(cross_val_score(classifier, X_train, y = Y_train, scoring = \"accuracy\", cv = kfold, n_jobs=4))\n\ncv_means = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())\n\ncv_res = pd.DataFrame({\"CrossValMeans\":cv_means,\"CrossValerrors\": cv_std,\"Algorithm\":['Logistic Regression',  'KNN', 'Gaussian',\n    'Perceptron', 'linear SVC', 'SGD', 'Decision Tree','SVMC', 'Random Forest']})\n\ng = sns.barplot(\"CrossValMeans\",\"Algorithm\",data = cv_res, palette=\"Set3\",orient = \"h\",**{'xerr':cv_std})\ng.set_xlabel(\"Mean Accuracy\")\ng = g.set_title(\"Cross validation scores\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"056a8fdb-8d59-452a-82b1-86e37e62ea5b","_uuid":"4d268142781fde8a736992af97a30e606094c7a5"},"cell_type":"markdown","source":"## 4.12 Hyperparameter tuning & learning curves for selected classifiers\n\n**1. Adaboost** is used in conjunction with many other types of learning algorithms to improve performance. The output of the other learning algorithms ('weak learners') is combined into a weighted sum that represents the final output of the boosted classifier. AdaBoost is adaptive in the sense that subsequent weak learners are tweaked in favor of those instances misclassified by previous classifiers. AdaBoost is sensitive to noisy data and outliers.\n\n**2. ExtraTrees** implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and use averaging to improve the predictive accuracy and control over-fitting.\n\n**3. RandomForest ** operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random decision forests correct for decision trees' habit of overfitting to their training set.\n\n**4. GradientBoost ** produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. It builds the model in a stage-wise fashion like other boosting methods do, and it generalizes them by allowing optimization of an arbitrary differentiable loss function.\n\n**5. SVMC, or Support Vector Machines.**vGiven a set of training examples, each marked as belonging to one or the other of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier.\n\nAll descripotion adapted from Wikipedia."},{"metadata":{"_cell_guid":"b0263595-bfd7-498c-8161-e76fbc2bdd28","_uuid":"1de7046e7704ee29b4827b3bde077af98bedb4df","trusted":true,"collapsed":true},"cell_type":"code","source":"# Adaboost\nDTC = DecisionTreeClassifier()\nadaDTC = AdaBoostClassifier(DTC, random_state=7)\nada_param_grid = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n              \"base_estimator__splitter\" :   [\"best\", \"random\"],\n              \"algorithm\" : [\"SAMME\",\"SAMME.R\"],\n              \"n_estimators\" :[1,2],\n              \"learning_rate\":  [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3,1.5]}\ngsadaDTC = GridSearchCV(adaDTC,param_grid = ada_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\ngsadaDTC.fit(X_train,Y_train)\nadaDTC_best = gsadaDTC.best_estimator_\ngsadaDTC.best_score_","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b45f6aa5-d003-460e-b051-c412409e946f","_uuid":"1f6162dde2e422d192470675a6e7fa5f8bc3fba4","trusted":true,"collapsed":true},"cell_type":"code","source":"# ExtraTrees \nExtC = ExtraTreesClassifier()\nex_param_grid = {\"max_depth\": [None],\n              \"max_features\": [1, 3, 7],\n              \"min_samples_split\": [2, 3, 7],\n              \"min_samples_leaf\": [1, 3, 7],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[300,600],\n              \"criterion\": [\"gini\"]}\ngsExtC = GridSearchCV(ExtC,param_grid = ex_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\ngsExtC.fit(X_train,Y_train)\nExtC_best = gsExtC.best_estimator_\ngsExtC.best_score_","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"29ca52ff-55d8-4948-aaac-baf1b4be4bf2","_uuid":"d0e880fe0a20338bd6ae7bfd756c192b2424a0a9","trusted":true,"collapsed":true},"cell_type":"code","source":"# Gradient boosting tunning\nGBC = GradientBoostingClassifier()\ngb_param_grid = {'loss' : [\"deviance\"],\n              'n_estimators' : [100,200,300],\n              'learning_rate': [0.1, 0.05, 0.01],\n              'max_depth': [4, 8],\n              'min_samples_leaf': [100,150],\n              'max_features': [0.3, 0.1] }\ngsGBC = GridSearchCV(GBC,param_grid = gb_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\ngsGBC.fit(X_train,Y_train)\nGBC_best = gsGBC.best_estimator_\ngsGBC.best_score_","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2f5ace1f-c5fd-4793-933a-4e0d1b78d68e","_uuid":"cd3db5d04fbcb9b2b672ddc672f008e61ef10af3","trusted":true,"collapsed":true},"cell_type":"code","source":"# SVC classifier\nSVMC = SVC(probability=True)\nsvc_param_grid = {'kernel': ['rbf'], \n                  'gamma': [ 0.001, 0.01, 0.1, 1],\n                  'C': [1,10,50,100,200,300, 1000]}\ngsSVMC = GridSearchCV(SVMC,param_grid = svc_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\ngsSVMC.fit(X_train,Y_train)\nSVMC_best = gsSVMC.best_estimator_\n# Best score\ngsSVMC.best_score_","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f841ae86-1e02-4626-b1e3-f3087cfee304","_uuid":"29f6e95093b5d3d5f54a6042ffe9a1ef2245af2a","scrolled":true,"trusted":true,"collapsed":true},"cell_type":"code","source":"# Random Forest\nrf_param_grid = {\"max_depth\": [None],\n              \"max_features\": [1, 3, 7],\n              \"min_samples_split\": [2, 3, 7],\n              \"min_samples_leaf\": [1, 3, 7],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[300,600],\n              \"criterion\": [\"gini\"]}\ngsrandom_forest = GridSearchCV(random_forest,param_grid = rf_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\ngsrandom_forest.fit(X_train,Y_train)\n# Best score\nrandom_forest_best = gsrandom_forest.best_estimator_\ngsrandom_forest.best_score_","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"eb168d27-5841-4b06-a320-9309c980468e","_uuid":"81070e0011002d63d313a322bd38636d23f48580","trusted":true,"collapsed":true},"cell_type":"code","source":"def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"Generate a simple plot of the test and training learning curve\"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n    plt.legend(loc=\"best\")\n    return plt\ng = plot_learning_curve(gsadaDTC.best_estimator_,\"AdaBoost learning curves\",X_train,Y_train,cv=kfold)\ng = plot_learning_curve(gsExtC.best_estimator_,\"ExtC ExtraTrees learning curves\",X_train,Y_train,cv=kfold)\ng = plot_learning_curve(gsGBC.best_estimator_,\"GBC Gradient Boost learning curves\",X_train,Y_train,cv=kfold)\ng = plot_learning_curve(gsrandom_forest.best_estimator_,\"RandomForest learning curves\",X_train,Y_train,cv=kfold)\ng = plot_learning_curve(gsSVMC.best_estimator_,\"SVMC learning curves\",X_train,Y_train,cv=kfold)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"fc320444-1097-4bd7-995b-908065b188e9","_uuid":"ce75e068abae919bc0c8e8e0d88353d85290cd84"},"cell_type":"markdown","source":"**Observations to fine-tune our models**\n\nFirst, let's compare their best score after fine-tuning their parameters:\n1. Adaboost: 80\n2. ExtraTrees: 83\n3. RandomForest: 82\n4. GradientBoost: 82\n5. SVC: 83\n\nIt appears that GBC and SVMC are doing the best job on the Train data. This is good because we want to keep the model as close to the training data as possible. But not too close!\nThe two major sources of error are bias and variance; as we reduce these two, then we could build more accurate models:\n\n* **Bias**: The less biased a method, the greater its ability to fit data well.\n* **Variance**: with a lower bias comes typically a higher the variance. And therefore the risk that the model will not adapt accurately to new test data.\nThis is the case here with Gradient Boost: high score but cross-validation is very distant.\n\nThe reverse also holds: the greater the bias, the lower the variance. A high-bias method builds simplistic models that generally don't fit well training data. \nWe can see the red and green curves from ExtraTrees, RandomForest and SVC are pretty close.\n**This points to a lower variance, i.e. a stronger ability to apply the model to new data.**\n\nI used the above graphs to optimize the parameters for Adaboost, ExtraTrees, RandomForest, GradientBoost and SVC.\nThis resulted in a significant improvement of the prediction accuracy on the test data (score).\n\nIn addition, I found out that AdaBoost does not do a good job with this dataset as the training score and cross-validation score are quite far apart. \n\n## 4.13 Selecting and combining the best classifiers\nSo, how do we achieve the best trade-off beween bias and variance?\n1. We will first compare in the next section the classifiers; results between themselves and applied to the same test data.\n2. Then \"ensemble\" them together with an automatic function callled *voting*."},{"metadata":{"_cell_guid":"c9f5ad35-050a-4bb1-a3b6-bb88042b2e82","_uuid":"26ccf158bbddbf74058fd2bddb6f54e5afc657c6","trusted":true,"collapsed":true},"cell_type":"code","source":"test_Survived_AdaDTC = pd.Series(adaDTC_best.predict(X_test), name=\"AdaDTC\")\ntest_Survived_ExtC = pd.Series(ExtC_best.predict(X_test), name=\"ExtC\")\ntest_Survived_GBC = pd.Series(GBC_best.predict(X_test), name=\"GBC\")\ntest_Survived_SVMC = pd.Series(SVMC_best.predict(X_test), name=\"SVMC\")\ntest_Survived_random_forest = pd.Series(random_forest_best.predict(X_test), name=\"random_forest\")\n\n# Concatenate all classifier results\nensemble_results = pd.concat([test_Survived_AdaDTC, test_Survived_ExtC, test_Survived_GBC,test_Survived_SVMC,test_Survived_random_forest],axis=1)\ng= sns.heatmap(ensemble_results.corr(),annot=True)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bed2c50a-063f-45e7-b4c7-523f4a5af86d","_uuid":"7f758f0ee9d19608100c393c5b7d0800346ddf05"},"cell_type":"markdown","source":"**Observations:**\n* As indicated before, Adaboost has the lowest correlations when compared to other predictors. This indicates that it predicts differently than the others when it comes to the test data.\n* We will therefore 'ensemble' the remaining four predictors.\n\n## 4.14 Ensembling\nThis is the final step, pulling it together with an amazing 'Voting' function from sklearn.\nAn ensemble is a supervised learning algorithm, that it can be trained and then used to make predictions.\nThe last line applied the \"ensemble predictor\" to the test data for submission."},{"metadata":{"_cell_guid":"d2cc701c-b37f-4745-939a-8062a41f66d7","_uuid":"6b36c683ce6f17cbf9803ac508cb3dabafedf059","trusted":true,"collapsed":true},"cell_type":"code","source":"VotingPredictor = VotingClassifier(estimators=[('ExtC', ExtC_best), ('GBC',GBC_best),\n('SVMC', SVMC_best), ('random_forest', random_forest_best)], voting='soft', n_jobs=4)\nVotingPredictor = VotingPredictor.fit(X_train, Y_train)\nVotingPredictor_predictions = VotingPredictor.predict(test)\ntest_Survived = pd.Series(VotingPredictor_predictions, name=\"Survived\")\n\n# Preparing data for Submission 3\ntest_Survived = pd.Series(VotingPredictor_predictions, name=\"Survived\")\nSubmission3 = pd.concat([PassengerId,test_Survived],axis=1)\nSubmission3.head(15)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"65a21655-0fbf-4806-a783-58498de603d4","_uuid":"860488f8a6cdbc5fe388d5124d4900bd4c8df94a"},"cell_type":"markdown","source":"## 4.15. Summary of most important features"},{"metadata":{"_cell_guid":"9513241c-db0d-4771-80d2-40796b884ebf","_uuid":"660931bb1dad9956f7c8d25397c03be1935f2cf7","trusted":true,"collapsed":true},"cell_type":"code","source":"nrows = ncols = 2\nfig, axes = plt.subplots(nrows = nrows, ncols = ncols, sharex=\"all\", figsize=(15,7))\nnames_classifiers = [(\"AdaBoosting\", adaDTC_best),(\"ExtraTrees\",ExtC_best),\n(\"GradientBoosting\",GBC_best), (\"RandomForest\",random_forest_best)]\n\nnclassifier = 0\nfor row in range(nrows):\n    for col in range(ncols):\n        name = names_classifiers[nclassifier][0]\n        classifier = names_classifiers[nclassifier][1]\n        indices = np.argsort(classifier.feature_importances_)[::-1][:40]\n        g = sns.barplot(y=train.columns[indices][:40],x = classifier.feature_importances_[indices][:40] , orient='h',ax=axes[row][col])\n        g.set_xlabel(\"Relative importance\",fontsize=11)\n        g.set_ylabel(\"Features\",fontsize=11)\n        g.tick_params(labelsize=9)\n        g.set_title(name + \" feature importance\")\n        nclassifier += 1","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4d6b61c0-5d72-b02a-3b37-cbf6518d71b6","_uuid":"4d9ef0298b568e43da6925f385403e0d77bd6e33"},"cell_type":"markdown","source":"Nice graphics, but the obsevation is unclear in my opinion:\n* On one side, we hope as analyst that the models come out with similar patterns. An easy direction to follow.\n* At the same time, \"there have been quite a few articles and Kaggle competition winner stories about the merits of having trained models that are more uncorrelated with one another producing better scores\". As we say in business, diversity brings better results, this seems to be true with algorithms as well!"},{"metadata":{"_cell_guid":"6b4a5c81-e968-d41e-27e4-871481019867","_uuid":"52ac0cd99cee0099d86a180127da42ff7fff960a"},"cell_type":"markdown","source":"# 5. Producing the submission file for Kaggle\n\nFinally having trained and fit all our first-level and second-level models, we can now output the predictions into the proper format for submission to the Titanic competition.\nWhich model to choose? These are the results of my many submissions:\n\n**Submission 1: **The prediction with **KNeighborsClassifier KNN in Section 4.3.** generates a public score of **0.75119**.\n\n**Submission 2:** The prediction with **random_forest in Section 4.9** generates a public score of **0.73684**.\n\n**Submission 3 (Kaggle Version 85):** The prediction with **gsrandom_forest in Section 4.14 ** after stratification and model cross validation, generates a public score of **0.80382**. \n\nDecision: submit #3 as best predictor"},{"metadata":{"_cell_guid":"15484065-8735-4d8e-9e62-a14434de3cbc","_uuid":"cfa4bb7306d46190a37455bd6be5ef6e1c55221d","trusted":true,"collapsed":true},"cell_type":"code","source":"# Submit File \nSubmission3.to_csv(\"StackingSubmission.csv\", index=False)\nprint(\"Completed.\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9a8f83fd-d0e8-035a-cf7f-25c9012e9373","_uuid":"c32d1d64e1a5f8fbe5f51a0a7afd952ccfdec57e"},"cell_type":"markdown","source":"# 6. Credits\n**Huge credits to Anisotropic,  Yassine Ghouzam, Faron and Sina** for pulling together most of the code in this kernel."}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}