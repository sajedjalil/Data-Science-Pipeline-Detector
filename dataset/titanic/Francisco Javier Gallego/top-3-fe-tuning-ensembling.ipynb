{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <b>1 <span style='color:lightseagreen'>|</span> Introduction</b>\n\nThe sinking of the Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the widely considered ‚Äúunsinkable‚Äù RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew. While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others. In this challenge, we ask you to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc).","metadata":{}},{"cell_type":"code","source":"import os\nimport warnings\nfrom pathlib import Path\nfrom IPython.display import clear_output\nfrom IPython.display import display\nfrom pandas.api.types import CategoricalDtype\nfrom IPython.core.display import display, HTML, Javascript\n\n# Basic libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pandas_profiling as pp\nimport seaborn as sns\nimport math\nimport string\n\n# Clustering\nfrom sklearn.cluster import KMeans\n\n# Principal Component Analysis (PCA)\nfrom sklearn.decomposition import PCA\n\n#Mutual Information\nfrom sklearn.feature_selection import mutual_info_regression\n\n# Cross Validation\nfrom sklearn.model_selection import KFold, cross_val_score, StratifiedKFold, learning_curve, train_test_split, GridSearchCV\n\n# Encoders\nfrom category_encoders import MEstimateEncoder\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n\n# Algorithms\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\n\n# Optuna - Bayesian Optimization \nimport optuna\nfrom optuna.samplers import TPESampler\n\n# Plotly\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\nimport plotly.offline as offline\nimport plotly.graph_objs as go\n\n# Metric\nfrom sklearn.metrics import plot_confusion_matrix\n\n# Permutation Importance\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\nwarnings.filterwarnings('ignore')\n\ndf_test = pd.read_csv(\"../input/titanic/test.csv\")\ndf_train = pd.read_csv(\"../input/titanic/train.csv\")\ndf_data = pd.concat([df_train, df_test], sort=True).reset_index(drop=True)\ndfs = [df_train, df_test]\nclear_output()\npp.ProfileReport(df_data)","metadata":{"_uuid":"abb70c97-87d6-4266-80c7-e0d5e9000d6d","_cell_guid":"6b97bf5b-8c16-4bd3-b02d-57129a36f821","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-26T22:36:55.12422Z","iopub.execute_input":"2022-03-26T22:36:55.124559Z","iopub.status.idle":"2022-03-26T22:37:07.006307Z","shell.execute_reply.started":"2022-03-26T22:36:55.12453Z","shell.execute_reply":"2022-03-26T22:37:07.005539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b>2 <span style='color:lightseagreen'>|</span> Exploratory Data Analysis</b>\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>2.1 | General Analysis</b></p>\n</div>","metadata":{}},{"cell_type":"code","source":"# Defining all our palette colours.\nprimary_blue = \"#496595\"\nprimary_blue2 = \"#85a1c1\"\nprimary_blue3 = \"#3f4d63\"\nprimary_grey = \"#c6ccd8\"\nprimary_black = \"#202022\"\nprimary_bgcolor = \"#f4f0ea\"\n\n# \"coffee\" pallette turqoise-gold.\nf1 = \"#a2885e\"\nf2 = \"#e9cf87\"\nf3 = \"#f1efd9\"\nf4 = \"#8eb3aa\"\nf5 = \"#235f83\"\nf6 = \"#b4cde3\"\n\ndef plot_box(fig, feature, r, c):\n    fig.add_trace(go.Box(x=df_data[feature].astype(object), y=df_data.Survived, marker = dict(color= px.colors.sequential.Viridis_r[5])), row =r, col = c)\n    fig.update_xaxes(showgrid = False, showline = True, linecolor = 'gray', linewidth = 2, zeroline = False,row = r, col = c)\n    fig.update_yaxes(showgrid = False, gridcolor = 'gray', gridwidth = 0.5, showline = True, linecolor = 'gray', linewidth = 2, row = r, col = c)\n    \ndef plot_scatter(fig, feature, r, c):\n    fig.add_trace(go.Scatter(x=df_data[feature], y=df_data.SalePrice, mode='markers', marker = dict(color=np.random.randn(10000), colorscale = px.colors.sequential.Viridis)), row = r, col = c)\n    fig.update_xaxes(showgrid = False, showline = True, linecolor = 'gray', linewidth = 2, zeroline = False, row = r, col = c)\n    fig.update_yaxes(showgrid = False, gridcolor = 'gray', gridwidth = 0.5, showline = True, linecolor = 'gray', linewidth = 2, row = r, col = c)\n    \ndef plot_hist(fig, feature, r, c):\n    fig.add_trace(go.Histogram(x=df_data[feature], name='Distribution', marker = dict(color = px.colors.sequential.Viridis_r[5])), row = r, col = c)\n    fig.update_xaxes(showgrid = False, showline = True, linecolor = 'gray', linewidth = 2, row = r, col = c)\n    fig.update_yaxes(showgrid = False, gridcolor = 'gray', gridwidth = 0.5, showline = True, linecolor = 'gray', linewidth = 2, row = r, col = c)\n    \n# chart\nfig = make_subplots(rows=2, cols=3, column_widths=[0.34, 0.33, 0.33], \n                    vertical_spacing=0.1, horizontal_spacing=0.1, subplot_titles=('Age Distribution','PClass Count','Fare Distribution','Parch Count',\n                    'SibSp Count','Survival Count'))\n\nplot_hist(fig, 'Age', 1,1)\n\npclass_df = pd.DataFrame(df_data['Pclass'].value_counts())\nfig.add_trace(go.Bar(x=pclass_df.index, y=pclass_df['Pclass'], marker = dict(color = [primary_blue3, px.colors.sequential.Viridis_r[5], px.colors.sequential.Viridis_r[5]]), name='PClass Count'), row=1, col=2)\n\nfig.update_xaxes(showgrid = False, linecolor='gray', linewidth = 2, zeroline = False, row=1, col=2)\nfig.update_yaxes(showgrid = False, linecolor='gray',linewidth=2, zeroline = False, row=1, col=2)\n\nplot_hist(fig, 'Fare', 1,3)\n\nparch_df = pd.DataFrame(df_data['Parch'].value_counts())\nfig.add_trace(go.Bar(x=parch_df.index, y=parch_df['Parch'], marker = dict(color = [primary_blue3, px.colors.sequential.Viridis_r[5], px.colors.sequential.Viridis_r[5]]), name='Parch Count'), row=2, col=1)\n\nfig.update_xaxes(showgrid = False, linecolor='gray', linewidth = 2, zeroline = False, row=2, col=1)\nfig.update_yaxes(showgrid = False, linecolor='gray',linewidth=2, zeroline = False, row=2, col=1)\n\nplot_hist(fig, 'SibSp',2,2)\n\nsurvived_df = pd.DataFrame(df_data['Survived'].value_counts())\nfig.add_trace(go.Bar(x=survived_df.index, y=survived_df['Survived'], marker = dict(color = [primary_blue3, px.colors.sequential.Viridis_r[5], px.colors.sequential.Viridis_r[5]]), name='Parch Count'), row=2, col=3)\n\nfig.update_xaxes(showgrid = False, linecolor='gray', linewidth = 2, zeroline = False, row=2, col=3)\nfig.update_yaxes(showgrid = False, linecolor='gray',linewidth=2, zeroline = False, row=2, col=3)\n\n# General Styling\nfig.update_layout(height=750, bargap=0.2,\n                  margin=dict(b=50,r=50,l=100),\n                  title = \"<span style='font-size:36px; font-family:Times New Roman'>General Analysis</span>\",                  \n                  plot_bgcolor='rgb(242,242,242)',\n                  paper_bgcolor = 'rgb(242,242,242)',\n                  font=dict(family=\"Times New Roman\", size= 14),\n                  hoverlabel=dict(font_color=\"floralwhite\"),\n                  showlegend=False)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-26T22:37:07.007733Z","iopub.execute_input":"2022-03-26T22:37:07.008017Z","iopub.status.idle":"2022-03-26T22:37:07.13984Z","shell.execute_reply.started":"2022-03-26T22:37:07.007987Z","shell.execute_reply":"2022-03-26T22:37:07.139005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>2.2 | Heatmap</b></p>\n</div>","metadata":{}},{"cell_type":"code","source":"fig = px.imshow(df_data[df_data['Survived'].isnull() == False].corr(), color_continuous_scale='RdBu_r', origin='lower', text_auto=True, aspect='auto')\nfig.update_xaxes(showgrid = False, linecolor='gray', linewidth = 2, zeroline = False)\nfig.update_yaxes(showgrid = True, gridcolor='gray',gridwidth=0.5, linecolor='gray',linewidth=2, zeroline = False)\n\n# General Styling\nfig.update_layout(height=500, bargap=0.2,\n                  margin=dict(b=50,r=30,l=100, t=100),\n                  title = \"<span style='font-size:36px; font-family:Times New Roman'>Heatmap - Numerical Features</span>\",                  \n                  plot_bgcolor='rgb(242,242,242)',\n                  paper_bgcolor = 'rgb(242,242,242)',\n                  font=dict(family=\"Times New Roman\", size= 14),\n                  hoverlabel=dict(font_color=\"floralwhite\"),\n                  showlegend=False)\nfig.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-26T22:37:07.1421Z","iopub.execute_input":"2022-03-26T22:37:07.142388Z","iopub.status.idle":"2022-03-26T22:37:07.225523Z","shell.execute_reply.started":"2022-03-26T22:37:07.142336Z","shell.execute_reply":"2022-03-26T22:37:07.224214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b>2 <span style='color:lightseagreen'>|</span> Missing Values</b>\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>2.1 | Age</b></p>\n</div>\n\nTo address the problem of missing values for the **<span style='color:lightseagreen'>Age</span>** field we will proceed as follows. Since **<span style='color:lightseagreen'>PClass</span>** is the variable that is **<span style='color:lightseagreen'>most correlated</span>** with both Age and Survived, we will group passengers according to the class they belong to. What we will do is replace the missing values with the **<span style='color:lightseagreen'>median</span>** of each group. In fact, what is more, within each of the existing classes we will make a **<span style='color:lightseagreen'>gender distinction</span>**. We do this because, as we will see below, the median of Age varies according to whether the passenger is male or female.","metadata":{"_uuid":"16fa32ad-897b-45a0-8ecf-02fbf59f8e92","_cell_guid":"0a6a608c-cb0d-4855-b853-985aa3e67462","trusted":true}},{"cell_type":"code","source":"df_heatmap = pd.DataFrame(df_data.corr()['Age'].abs())\nf,ax = plt.subplots(figsize=(10,1.5),facecolor='white')\nsns.color_palette(\"rocket\", as_cmap=True)          # Esta paleta es la que viene por defecto\nsns.heatmap(df_heatmap.transpose(),annot = True,square=True, linewidths=1.5, cmap='rocket')","metadata":{"_uuid":"893b8394-f8aa-4ef1-b2d2-b7b333bf8d35","_cell_guid":"acd7a663-909e-492e-9c41-d491e57933ab","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-26T22:37:07.227898Z","iopub.execute_input":"2022-03-26T22:37:07.228349Z","iopub.status.idle":"2022-03-26T22:37:07.49013Z","shell.execute_reply.started":"2022-03-26T22:37:07.228318Z","shell.execute_reply":"2022-03-26T22:37:07.489229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mediana = df_data.groupby(['Sex', 'Pclass']).median()['Age']\nfor i in range(0,mediana.shape[0]):\n    if i<3: \n        print('Edad mediana para mujeres de la clase {}: {}'.format(i+1,mediana[i]))\n    else:\n        print('Edad mediana para hombres de la clase {}: {}'.format(i+1-3,mediana[i]))\ndf_data['Age'] = df_data.groupby(['Sex', 'Pclass'])['Age'].apply(lambda x: x.fillna(x.median()))\nprint('Missing values for Age: {}'.format(df_data.Age.isnull().sum()))","metadata":{"_uuid":"d1581665-f565-4abc-a425-3dd4603293c4","_cell_guid":"fd9739cd-87e3-493a-82fd-e9e8506a4524","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-26T22:37:07.491283Z","iopub.execute_input":"2022-03-26T22:37:07.492448Z","iopub.status.idle":"2022-03-26T22:37:07.511835Z","shell.execute_reply.started":"2022-03-26T22:37:07.492393Z","shell.execute_reply":"2022-03-26T22:37:07.510849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>2.2 | Embarked</b></p>\n</div>\n\nWith respect to Embarked we will replace the missing data by the **<span style='color:lightseagreen'>mode</span>**, i.e. the most repeated value.","metadata":{"_uuid":"6f581672-b225-45b3-a331-eab9a60d2a1f","_cell_guid":"973d6c3b-fc36-4316-a108-c0944975de70","trusted":true}},{"cell_type":"code","source":"df_data.Embarked.value_counts()","metadata":{"_uuid":"6f571044-49b3-4b67-a766-3d252c6bfd73","_cell_guid":"4ce4ec88-d1fc-4b7c-a987-d6e8862fe799","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-26T22:37:07.513272Z","iopub.execute_input":"2022-03-26T22:37:07.514629Z","iopub.status.idle":"2022-03-26T22:37:07.523237Z","shell.execute_reply.started":"2022-03-26T22:37:07.514576Z","shell.execute_reply":"2022-03-26T22:37:07.522283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"moda = 'S'\ndf_data.Embarked = df_data.Embarked.replace(np.nan,moda)\npd.isnull(df_data).sum()","metadata":{"_uuid":"64fc68fe-16fd-4d6a-882e-390fa49c8e63","_cell_guid":"217d0cd4-f9d2-4fa1-89d1-05dd8693e948","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-26T22:37:07.52492Z","iopub.execute_input":"2022-03-26T22:37:07.525182Z","iopub.status.idle":"2022-03-26T22:37:07.542879Z","shell.execute_reply.started":"2022-03-26T22:37:07.525148Z","shell.execute_reply":"2022-03-26T22:37:07.541743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>2.3 | Cabin</b></p>\n</div>\n\n**<span style='color:lightseagreen'>Cabin</span>** feature is little bit tricky and it needs further exploration. The large portion of the Cabin feature is missing and the feature itself **<span style='color:lightseagreen'>can't be ignored completely because some the cabins might have higher survival rates</span>**. It turns out to be the first letter of the Cabin values are the decks in which the cabins are located. Those decks were mainly separated for one passenger class, but some of them were used by multiple passenger classes.\n![alt text](https://vignette.wikia.nocookie.net/titanic/images/f/f9/Titanic_side_plan.png/revision/latest?cb=20180322183733)\n* On the Boat Deck there were **6** rooms labeled as **T, U, W, X, Y, Z** but only the **T** cabin is present in the dataset\n* **A**, **B** and **C** decks were only for 1st class passengers\n* **D** and **E** decks were for all classes\n* **F** and **G** decks were for both 2nd and 3rd class passengers\n* From going **A** to **G**, **<span style='color:lightseagreen'>distance to the staircase increases which might be a factor of survival</span>**","metadata":{"_uuid":"fe29a166-1c3b-46df-8c04-0283ede6e03a","_cell_guid":"2cfc6d51-19be-43e3-997d-b7d3c7bc7fc7","trusted":true}},{"cell_type":"code","source":"# Creating Deck column from the first letter of the Cabin column (M stands for Missing)\ndf_data['Deck'] = df_data['Cabin'].apply(lambda s: s[0] if pd.notnull(s) else 'M')\n\ndf_data_decks = df_data.groupby(['Deck', 'Pclass']).count().drop(columns=['Survived', 'Sex', 'Age', \n                                                                        'Fare', 'Embarked', 'Cabin']).rename(columns={'Name': 'Count'}).transpose()\n\ndef get_pclass_dist(df):\n    \n    # Creating a dictionary for every passenger class count in every deck\n    deck_counts = {'A': {}, 'B': {}, 'C': {}, 'D': {}, 'E': {}, 'F': {}, 'G': {}, 'M': {}, 'T': {}}\n    decks = df.columns.levels[0]    \n    \n    for deck in decks:\n        for pclass in range(1, 4):\n            try:\n                count = df[deck][pclass][0]\n                deck_counts[deck][pclass] = count \n            except KeyError:\n                deck_counts[deck][pclass] = 0\n                \n    df_decks = pd.DataFrame(deck_counts)    \n    deck_percentages = {}\n\n    # Creating a dictionary for every passenger class percentage in every deck\n    for col in df_decks.columns:\n        deck_percentages[col] = [(count / df_decks[col].sum()) * 100 for count in df_decks[col]]\n        \n    return deck_counts, deck_percentages\n\ndef display_pclass_dist(percentages):\n    \n    df_percentages = pd.DataFrame(percentages).transpose()\n    deck_names = ('A', 'B', 'C', 'D', 'E', 'F', 'G', 'M', 'T')\n    bar_count = np.arange(len(deck_names))  \n    bar_width = 0.85\n    \n    pclass1 = df_percentages[0]\n    pclass2 = df_percentages[1]\n    pclass3 = df_percentages[2]\n    \n    plt.figure(figsize=(20, 10))\n    plt.bar(bar_count, pclass1, color='#b5ffb9', edgecolor='white', width=bar_width, label='Passenger Class 1')\n    plt.bar(bar_count, pclass2, bottom=pclass1, color='#f9bc86', edgecolor='white', width=bar_width, label='Passenger Class 2')\n    plt.bar(bar_count, pclass3, bottom=pclass1 + pclass2, color='#a3acff', edgecolor='white', width=bar_width, label='Passenger Class 3')\n\n    plt.xlabel('Deck', size=15, labelpad=20)\n    plt.ylabel('Passenger Class Percentage', size=15, labelpad=20)\n    plt.xticks(bar_count, deck_names)    \n    plt.tick_params(axis='x', labelsize=15)\n    plt.tick_params(axis='y', labelsize=15)\n    \n    plt.legend(loc='upper left', bbox_to_anchor=(1, 1), prop={'size': 15})\n    plt.title('Passenger Class Distribution in Decks', size=18, y=1.05)   \n    \n    plt.show()    \n\nall_deck_count, all_deck_per = get_pclass_dist(df_data_decks)\ndisplay_pclass_dist(all_deck_per)","metadata":{"_uuid":"b23f45d0-42ba-45f8-941f-f739e8f6f1bf","_cell_guid":"11757445-5e9d-4c0a-9fe9-e572b982e57e","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-26T22:37:07.544068Z","iopub.execute_input":"2022-03-26T22:37:07.544499Z","iopub.status.idle":"2022-03-26T22:37:07.868475Z","shell.execute_reply.started":"2022-03-26T22:37:07.544473Z","shell.execute_reply":"2022-03-26T22:37:07.867674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"üìå **Interpret:** for this graph we'll be grouping by **<span style='color:lightseagreen'>Deck</span>** and **<span style='color:lightseagreen'>PClass</span>** atribute. We are able to appreciate that A,B and C are fully occupied by passengers of 1st class. Moreover, as there is just one person in deck T, which class 1 we are going to group it with deck A. Deck of type D is mainly occupied with 1st class passengers, concretely a 85%. The rest are from 2nd class. To conclude, the remaining decks have all passengers from each of the classes.","metadata":{"_uuid":"c1f133bb-f9d2-43a9-ab92-0901dbe9cb47","_cell_guid":"53f19e3b-c3c3-4d6b-be86-2f0c70b047e0","trusted":true}},{"cell_type":"code","source":"# Passenger in the T deck is changed to A\nidx = df_data[df_data['Deck'] == 'T'].index\ndf_data.loc[idx, 'Deck'] = 'A'","metadata":{"_uuid":"334ed3c5-ee6b-4587-bfe5-924ee34fa0d4","_cell_guid":"5bd4189f-fe0f-40d9-b164-cc05a74935a2","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-26T22:37:07.869754Z","iopub.execute_input":"2022-03-26T22:37:07.869951Z","iopub.status.idle":"2022-03-26T22:37:07.876374Z","shell.execute_reply.started":"2022-03-26T22:37:07.869923Z","shell.execute_reply":"2022-03-26T22:37:07.875517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_all_decks_survived = df_data.groupby(['Deck', 'Survived']).count().drop(columns=['Sex', 'Age', 'Fare', \n                                                                                   'Embarked', 'Pclass', 'Cabin']).rename(columns={'Name':'Count'}).transpose()\n\ndef get_survived_dist(df):\n    \n    # Creating a dictionary for every survival count in every deck\n    surv_counts = {'A':{}, 'B':{}, 'C':{}, 'D':{}, 'E':{}, 'F':{}, 'G':{}, 'M':{}}\n    decks = df.columns.levels[0]    \n\n    for deck in decks:\n        for survive in range(0, 2):\n            surv_counts[deck][survive] = df[deck][survive][0]\n            \n    df_surv = pd.DataFrame(surv_counts)\n    surv_percentages = {}\n\n    for col in df_surv.columns:\n        surv_percentages[col] = [(count / df_surv[col].sum()) * 100 for count in df_surv[col]]\n        \n    return surv_counts, surv_percentages\n\ndef display_surv_dist(percentages):\n    \n    df_survived_percentages = pd.DataFrame(percentages).transpose()\n    deck_names = ('A', 'B', 'C', 'D', 'E', 'F', 'G', 'M')\n    bar_count = np.arange(len(deck_names))  \n    bar_width = 0.85    \n\n    not_survived = df_survived_percentages[0]\n    survived = df_survived_percentages[1]\n    \n    plt.figure(figsize=(20, 10))\n    plt.bar(bar_count, not_survived, color='#b5ffb9', edgecolor='white', width=bar_width, label=\"Not Survived\")\n    plt.bar(bar_count, survived, bottom=not_survived, color='#f9bc86', edgecolor='white', width=bar_width, label=\"Survived\")\n \n    plt.xlabel('Deck', size=15, labelpad=20)\n    plt.ylabel('Survival Percentage', size=15, labelpad=20)\n    plt.xticks(bar_count, deck_names)    \n    plt.tick_params(axis='x', labelsize=15)\n    plt.tick_params(axis='y', labelsize=15)\n    \n    plt.legend(loc='upper left', bbox_to_anchor=(1, 1), prop={'size': 15})\n    plt.title('Survival Percentage in Decks', size=18, y=1.05)\n    \n    plt.show()\n\nall_surv_count, all_surv_per = get_survived_dist(df_all_decks_survived)\ndisplay_surv_dist(all_surv_per)","metadata":{"_uuid":"198654b9-d56e-484e-8c98-9c8efc5b772c","_cell_guid":"fa26e90a-0503-4232-95e6-76780b4e4097","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-26T22:37:07.879953Z","iopub.execute_input":"2022-03-26T22:37:07.880145Z","iopub.status.idle":"2022-03-26T22:37:08.163879Z","shell.execute_reply.started":"2022-03-26T22:37:07.880122Z","shell.execute_reply":"2022-03-26T22:37:08.162872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"üìå **Interpret:** for this graph we'll be grouping by **<span style='color:lightseagreen'>Deck</span>** and **<span style='color:lightseagreen'>Survived</span>** atribute, in order to see the survival rate that each Deck has. We are able to appreciate that as expected survival rates are different for every type of Deck. **<span style='color:lightseagreen'>B</span>**, **<span style='color:lightseagreen'>D</span>** and **<span style='color:lightseagreen'>E</span>** are the ones with highest. On the other hand, **<span style='color:lightseagreen'>A</span>** and **<span style='color:lightseagreen'>M</span>** are the ones with lowest. \n\nDue to what we have just seen before, we are going to label decks in the following way: \n* A, B and C decks, as they all have 1st class passengers, are going to be labeled as ABC\n* **D** and **E** decks are labeled as **DE** because both of them have similar passenger class distribution and same survival rate\n* Following the previous criterion we labeled FG\n* M remains equal because it's quite different from the others and it's the one with lowest survival rate.","metadata":{"_uuid":"bcd516cb-6b61-4fec-9cb9-b74b47687b11","_cell_guid":"00c83f89-d02d-4a70-ba40-88548d88440e","trusted":true}},{"cell_type":"code","source":"df_data['Deck'] = df_data['Deck'].replace(['A', 'B', 'C'], 'ABC')\ndf_data['Deck'] = df_data['Deck'].replace(['D', 'E'], 'DE')\ndf_data['Deck'] = df_data['Deck'].replace(['F', 'G'], 'FG')\n\ndf_data['Deck'].value_counts()","metadata":{"_uuid":"98c37d12-cae2-4589-a306-76d846ebf930","_cell_guid":"f9223047-84f2-49d8-8c8c-09894d23d6c3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-26T22:37:08.16555Z","iopub.execute_input":"2022-03-26T22:37:08.165807Z","iopub.status.idle":"2022-03-26T22:37:08.180533Z","shell.execute_reply.started":"2022-03-26T22:37:08.165774Z","shell.execute_reply":"2022-03-26T22:37:08.179416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_data = df_data.drop('Cabin',axis=1)\ndf_data.isnull().sum()","metadata":{"_uuid":"9a34ba89-4287-4798-86ff-cd42e3dc0ccc","_cell_guid":"b10a14ec-65b8-45b1-a5d0-bcf6e0e0bbb7","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-26T22:37:08.181697Z","iopub.execute_input":"2022-03-26T22:37:08.181972Z","iopub.status.idle":"2022-03-26T22:37:08.209627Z","shell.execute_reply.started":"2022-03-26T22:37:08.181939Z","shell.execute_reply":"2022-03-26T22:37:08.208545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>2.4 | Fare</b></p>\n</div>\n\nWe have one missing value for **<span style='color:lightseagreen'>Fare</span>**, belonging to one male of the testing dataset. We can assume that it is related to **<span style='color:lightseagreen'>FamilySize</span>** and **<span style='color:lightseagreen'>PClass</span>**. Median Fare value of a male with a third class ticket and no family is a logical choice to fill the missing value.","metadata":{"_uuid":"0ea64f2f-3f3a-4615-b17b-f0cf4a280abf","_cell_guid":"8e79bf7e-faf9-4bc6-b690-a4e04b782f0a","trusted":true}},{"cell_type":"code","source":"df_data[df_data.Fare.isnull()]","metadata":{"_uuid":"6a48872a-d378-4b9c-bcb0-2276f193b5de","_cell_guid":"8e8ebc93-e137-4016-b597-206d1e856c05","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-26T22:37:08.210874Z","iopub.execute_input":"2022-03-26T22:37:08.211336Z","iopub.status.idle":"2022-03-26T22:37:08.24063Z","shell.execute_reply.started":"2022-03-26T22:37:08.211303Z","shell.execute_reply":"2022-03-26T22:37:08.23873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mediana = df_data.groupby(['Pclass', 'Parch', 'SibSp']).Fare.median()[3][0][0]\ndf_data.Fare = df_data.Fare.fillna(mediana)","metadata":{"_uuid":"ae4c2463-8b8e-4ae7-9831-d45c1cbe91cc","_cell_guid":"8900de69-80bb-4c60-91d7-7da75b06d5df","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-26T22:37:08.242156Z","iopub.execute_input":"2022-03-26T22:37:08.243095Z","iopub.status.idle":"2022-03-26T22:37:08.261449Z","shell.execute_reply.started":"2022-03-26T22:37:08.24306Z","shell.execute_reply":"2022-03-26T22:37:08.260323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b>3 <span style='color:lightseagreen'>|</span> Feature Engineering</b>\n\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>3.1 | Family</b></p>\n</div>\n\nWe will start by creating fields related to the family unit. The first of these will come from the **<span style='color:lightseagreen'>SibSp and Parch</span>** fields, which we can remove later. This will reflect the **<span style='color:lightseagreen'>size of passengers' family</span>**. We will also enter a field to indicate whether the passenger is travelling **<span style='color:lightseagreen'>alone</span>** or not.","metadata":{"_uuid":"0914b377-f53c-4c67-9043-48fb5696fe62","_cell_guid":"d03cbecb-eaf1-4557-a1b2-4f67b6a2c18f","trusted":true}},{"cell_type":"code","source":"df_data['FamilySize'] = df_data.Parch + df_data.SibSp + 1\ndf_data['IsAlone'] = 0\ndf_data.loc[df_data['FamilySize'] == 1, 'IsAlone'] = 1\ndf_data = df_data.drop(['Parch','SibSp'],axis = 1)","metadata":{"_uuid":"cb17e610-5f26-42ff-9292-3033522133a7","_cell_guid":"5cd209fa-fde3-4fd0-b36a-1b25450627e2","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-26T22:37:08.262571Z","iopub.execute_input":"2022-03-26T22:37:08.263983Z","iopub.status.idle":"2022-03-26T22:37:08.281989Z","shell.execute_reply.started":"2022-03-26T22:37:08.263935Z","shell.execute_reply":"2022-03-26T22:37:08.281335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(figsize=(22,8), nrows = 1, ncols = 2)\nax = sns.countplot(x = 'FamilySize', hue='Survived', data = df_data, palette = ['#334550','#6D83AA'], ax = axes[0])\nax.set_title('Survival Rate per Family Size')\nax = sns.countplot(x = 'FamilySize', data = df_data, palette = ['#334550','#334668','#394184','#496595','#6D83AA','#91A2BF','#C8D0DF'], ax = axes[1])\n_ = ax.set_title('Family Size Countplot')","metadata":{"_uuid":"80e8a7cd-a641-4ce4-8571-580bcbac53de","_cell_guid":"bdf1b831-3cdf-435d-9b63-3f00f8731671","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-26T22:37:08.282792Z","iopub.execute_input":"2022-03-26T22:37:08.283823Z","iopub.status.idle":"2022-03-26T22:37:08.753784Z","shell.execute_reply.started":"2022-03-26T22:37:08.283793Z","shell.execute_reply":"2022-03-26T22:37:08.752986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Taking this into account, I have decided to group **<span style='color:lightseagreen'>Family Size</span>** into 4 different groups. They are the following: \n\n* Alone: for people travelling with no member of his/her family. \n* Small: for people travelling with 3 members of family\n* Medium: travelling with 4 or 5 members of family\n* Large: travelling with 6+ members of family","metadata":{"_uuid":"68ea5860-23b5-44f2-89a2-2f4761cea6b9","_cell_guid":"99313300-4547-47b5-b4d0-386735fbf124","trusted":true}},{"cell_type":"code","source":"family_map = {1: 'Alone', 2: 'Small', 3: 'Small', 4: 'Small', 5: 'Medium', 6: 'Medium', 7: 'Large', 8: 'Large', 11: 'Large'}\ndf_data['FamilySizeGrouped'] = df_data['FamilySize'].map(family_map)\ndf_data.head(5)","metadata":{"_uuid":"97e7ca59-3b93-4b67-806c-f6cb90cf1797","_cell_guid":"6feeab71-b296-444b-997f-440d82ebe3b6","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-26T22:37:08.754787Z","iopub.execute_input":"2022-03-26T22:37:08.755051Z","iopub.status.idle":"2022-03-26T22:37:08.772452Z","shell.execute_reply.started":"2022-03-26T22:37:08.755027Z","shell.execute_reply":"2022-03-26T22:37:08.771575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's plot again graphs for previous contents:","metadata":{"_uuid":"36c12623-3a3e-4271-b194-c4c942a94ef3","_cell_guid":"afe1c3b6-7506-4205-8c90-7b6c14b0c8ee","trusted":true}},{"cell_type":"code","source":"fig, axes = plt.subplots(figsize=(22,8), nrows = 1, ncols = 2)\nax = sns.countplot(x = 'FamilySizeGrouped', hue='Survived', data = df_data, palette = ['#334550','#6D83AA'], ax = axes[0])\nax.set_title('Survival Rate per Family Group')\nax = sns.countplot(x = 'FamilySizeGrouped', data = df_data, palette = ['#334550','#394184','#6D83AA','#C8D0DF'], ax = axes[1])\n_ = ax.set_title('Family Group Countplot')","metadata":{"_uuid":"eb233213-e5d6-4713-a144-1b02bd1a6ac6","_cell_guid":"16811fcf-3bcf-4968-8be2-c9a75560dd32","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-26T22:37:08.773394Z","iopub.execute_input":"2022-03-26T22:37:08.773563Z","iopub.status.idle":"2022-03-26T22:37:09.142633Z","shell.execute_reply.started":"2022-03-26T22:37:08.773543Z","shell.execute_reply":"2022-03-26T22:37:09.14155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"üìå **Interpret:** On the left part, on the one hand we can observe that lonely people tend to die. On the other hand, **<span style='color:lightseagreen'>small families members are most likely to survive</span>**. Appreciating the countplot, we can observe that most people is travelling alone or in small families. Travelling as a member of a medium/large family is very unusual.","metadata":{"_uuid":"72f02262-4dd2-40d0-b774-b7152f3a37ec","_cell_guid":"59c91840-aa0d-4d6f-bd2d-7cca354acd38","trusted":true}},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>3.2 | Passenger's Name</b></p>\n</div>\n\nNext we are going to add a column that will be, in part, related to the **<span style='color:lightseagreen'>Name</span>** field:","metadata":{"_uuid":"988c7fdc-7b4d-4bb8-8d70-e356f2c5a1ef","_cell_guid":"7ead1388-9919-4936-b2cf-6fbd08b0f5af","trusted":true}},{"cell_type":"code","source":"df_data.head().style.set_properties(subset=['Name'], **{'background-color': '#F1C40F'})","metadata":{"_uuid":"e2ce4cc6-a149-4398-9e1f-d930a9bfa5fd","_cell_guid":"e9bf5687-e0e0-480f-baa3-13397177eb91","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-26T22:37:09.143739Z","iopub.execute_input":"2022-03-26T22:37:09.143929Z","iopub.status.idle":"2022-03-26T22:37:09.156572Z","shell.execute_reply.started":"2022-03-26T22:37:09.143905Z","shell.execute_reply":"2022-03-26T22:37:09.155239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_data['Title'] = df_data.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\npd.crosstab(df_data['Title'], df_data['Sex']).transpose()","metadata":{"_uuid":"66a0f2fa-34df-4f48-b775-f444abf937fd","_cell_guid":"635ecdf1-142b-4815-933c-211d615aaf55","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-26T22:37:09.157711Z","iopub.execute_input":"2022-03-26T22:37:09.157886Z","iopub.status.idle":"2022-03-26T22:37:09.193506Z","shell.execute_reply.started":"2022-03-26T22:37:09.157862Z","shell.execute_reply":"2022-03-26T22:37:09.192428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can replace many titles with a more common name or classify them as Rare.","metadata":{"_uuid":"b0d7c665-d210-4285-81d8-9304ee92e842","_cell_guid":"3f99bb6b-c35b-45f6-bb06-b3f0c7758748","trusted":true}},{"cell_type":"code","source":"df_data['Title'] = df_data['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\ndf_data['Title'] = df_data['Title'].replace('Mlle', 'Miss')\ndf_data['Title'] = df_data['Title'].replace('Ms', 'Miss')\ndf_data['Title'] = df_data['Title'].replace('Mme', 'Mrs')\n\ndf_data['Is_Married'] = 0\ndf_data['Is_Married'].loc[df_data['Title'] == 'Mme'] = 1\n\ndf_data[['Title', 'Survived']].groupby(['Title'], as_index=False).mean().transpose()","metadata":{"_uuid":"44976603-cbc3-466c-9570-2e524ff357d4","_cell_guid":"2b9eb615-1c35-45ad-8fcc-e0e3da609601","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-26T22:37:09.1946Z","iopub.execute_input":"2022-03-26T22:37:09.194791Z","iopub.status.idle":"2022-03-26T22:37:09.221256Z","shell.execute_reply.started":"2022-03-26T22:37:09.194765Z","shell.execute_reply":"2022-03-26T22:37:09.220053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>3.3 | Fare</b></p>\n</div>\n\nIn order to binning continuous features we are going to use **<span style='v:#F1C40F'>13 quantile base bins</span>**. Even though the bins are too much, they provide a decent amount of information gain, as it would be seen in next section. We'll create `df_data_no_quart` in order to have a DataFrame with the discrete values of Fare, to make it easier to plotting it later.","metadata":{"_uuid":"a8fa46ee-e63f-4d30-a011-d2815474bfab","_cell_guid":"a3131c32-2994-4a7c-8dde-47a0a48c49a6","trusted":true}},{"cell_type":"code","source":"df_data_no_quart = df_data.copy()\nnames = ['1', '2', '3', '4', '5', '6', '7','8','9','10','11','12','13']\ndf_data['Fare'] = pd.qcut(df_data['Fare'], 13, labels = names)\ndf_data.Fare = pd.to_numeric(df_data.Fare, errors = 'coerce')","metadata":{"_uuid":"46847ee6-6483-43eb-a423-7ade6a6f26d6","_cell_guid":"17802424-f264-4757-ba74-bcf555048513","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-26T22:37:09.222431Z","iopub.execute_input":"2022-03-26T22:37:09.222635Z","iopub.status.idle":"2022-03-26T22:37:09.231995Z","shell.execute_reply.started":"2022-03-26T22:37:09.22261Z","shell.execute_reply":"2022-03-26T22:37:09.231507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>3.4 | Age</b></p>\n</div>\n\nLet's keep binning continuous features. For **<span style='color:lightseagreen'>Age</span>** we are going to use **<span style='color:lightseagreen'>10 quantile base bins</span>**. Even though the bins are too much, they provide a decent amount of information gain, as it would be seen in next section.","metadata":{"_uuid":"f9380e43-ebee-4b50-a6c8-eb3f22665206","_cell_guid":"74a32a15-01ab-448b-a4cd-2628a89f5f5d","trusted":true}},{"cell_type":"code","source":"names = ['1','2','3','4','5','6','7','8','9','10']\ndf_data['Age'] = pd.qcut(df_data['Age'], 10, labels = names)","metadata":{"_uuid":"2b0884b5-84d5-46c8-98b4-78f58a2afb2a","_cell_guid":"94f5238e-010a-49c8-af83-7fe5dda5681f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-26T22:37:09.233094Z","iopub.execute_input":"2022-03-26T22:37:09.233728Z","iopub.status.idle":"2022-03-26T22:37:09.252882Z","shell.execute_reply.started":"2022-03-26T22:37:09.233701Z","shell.execute_reply":"2022-03-26T22:37:09.251762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>3.5 | Frequency Encoding</b></p>\n</div>\n\nAs seen in first part of this Feature Engineering section, **<span style='color:lightseagreen'>FamilySize</span>** could have a **<span style='color:lightseagreen'>huge effect on survival prediction</span>**, as rates are quite different between each other.","metadata":{"_uuid":"7a143600-c15f-4a83-a798-954e6a79350b","_cell_guid":"bfb172af-3c6f-4f6c-898e-f58683b0804f","trusted":true}},{"cell_type":"code","source":"df_data['Ticket_Frequency'] = df_data.groupby('Ticket')['Ticket'].transform('count')\ndf_data.head().style.set_properties(subset=['Ticket_Frequency'], **{'background-color': '#F1C40F'})","metadata":{"_uuid":"173e3d3d-770e-41d0-b76a-a3b3efc49ad7","_cell_guid":"832ef14f-83e7-49b9-ab25-f6a2e5025a5d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-26T22:37:09.254555Z","iopub.execute_input":"2022-03-26T22:37:09.254936Z","iopub.status.idle":"2022-03-26T22:37:09.278725Z","shell.execute_reply.started":"2022-03-26T22:37:09.254905Z","shell.execute_reply":"2022-03-26T22:37:09.277884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>3.6 | Surname</b></p>\n</div>\n\nWe are going to group **<span style='color:lightseagreen'>passengers in the same family</span>**. `extract_surname` function is used for extracting surnames of passengers from the Name feature. Family feature is created with the extracted surname.","metadata":{"_uuid":"c24598dc-3410-4d4a-9810-b5a9e421f595","_cell_guid":"407134cf-7a49-4ed5-b5b7-e93539703230","trusted":true}},{"cell_type":"code","source":"def extract_surname(data):    \n    families = []\n    for i in range(len(data)):        \n        name = data.iloc[i]\n        if '(' in name:\n            name_no_bracket = name.split('(')[0] \n        else:\n            name_no_bracket = name\n            \n        family = name_no_bracket.split(',')[0]\n        title = name_no_bracket.split(',')[1].strip().split(' ')[0]\n        \n        for c in string.punctuation:\n            family = family.replace(c, '').strip()\n        families.append(family)\n    return families","metadata":{"_uuid":"8e7d9c7f-4f4a-45bb-97bb-37f903fd433d","_cell_guid":"d4fab6ea-224f-4463-9f1d-77c44c44763b","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-26T22:37:09.27993Z","iopub.execute_input":"2022-03-26T22:37:09.280218Z","iopub.status.idle":"2022-03-26T22:37:09.287953Z","shell.execute_reply.started":"2022-03-26T22:37:09.280194Z","shell.execute_reply":"2022-03-26T22:37:09.286421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_data['Family'] = extract_surname(df_data['Name'])\ndf_train = df_data.loc[:890]\ndf_test = df_data.loc[891:]\ndfs = [df_train, df_test]","metadata":{"_uuid":"038ccfc6-ed32-4637-8e3b-8ac26b301273","_cell_guid":"974ac88c-ee2e-46f7-8310-d864129f764c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-26T22:37:09.288909Z","iopub.execute_input":"2022-03-26T22:37:09.289726Z","iopub.status.idle":"2022-03-26T22:37:09.325922Z","shell.execute_reply.started":"2022-03-26T22:37:09.289697Z","shell.execute_reply":"2022-03-26T22:37:09.324931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**<span style='color:lightseagreen'>Family_Survival_Rate</span>** is calculated from families in training set since there is no Survived feature in test set. A list of family names that are occuring in both training and test set (non_unique_families), is created. The survival rate is calculated for families with more than 1 members in that list, and stored in Family_Survival_Rate feature.\n\nAn extra binary feature **<span style='color:lightseagreen'>Family_Survival_Rate_NA</span>** is created for families that are unique to the test set. This feature is also necessary because there is no way to calculate those families' survival rate. This feature implies that family survival rate is not applicable to those passengers because there is no way to retrieve their survival rate.\n\nTicket_Survival_Rate and Ticket_Survival_Rate_NA features are also created with the same method. Ticket_Survival_Rate and Family_Survival_Rate are averaged and become Survival_Rate, and Ticket_Survival_Rate_NA and Family_Survival_Rate_NA are also averaged and become Survival_Rate_NA.","metadata":{"_uuid":"449feaed-0016-4871-ac1e-e3253f4d3157","_cell_guid":"39732909-92b2-4bb5-839c-00929c144b03","trusted":true}},{"cell_type":"code","source":"# Creating a list of families and tickets that are occuring in both training and test set\nnon_unique_families = [x for x in df_train['Family'].unique() if x in df_test['Family'].unique()]\nnon_unique_tickets = [x for x in df_train['Ticket'].unique() if x in df_test['Ticket'].unique()]\n\ndf_family_survival_rate = df_train.groupby('Family')['Survived', 'Family','FamilySize'].median()\ndf_ticket_survival_rate = df_train.groupby('Ticket')['Survived', 'Ticket','Ticket_Frequency'].median()\n\nfamily_rates = {}\nticket_rates = {}\n\nfor i in range(len(df_family_survival_rate)):\n    # Checking a family exists in both training and test set, and has members more than 1\n    if df_family_survival_rate.index[i] in non_unique_families and df_family_survival_rate.iloc[i, 1] > 1:\n        family_rates[df_family_survival_rate.index[i]] = df_family_survival_rate.iloc[i, 0]\n\nfor i in range(len(df_ticket_survival_rate)):\n    # Checking a ticket exists in both training and test set, and has members more than 1\n    if df_ticket_survival_rate.index[i] in non_unique_tickets and df_ticket_survival_rate.iloc[i, 1] > 1:\n        ticket_rates[df_ticket_survival_rate.index[i]] = df_ticket_survival_rate.iloc[i, 0]\n        \nmean_survival_rate = np.mean(df_train['Survived'])\n\ntrain_family_survival_rate = []\ntrain_family_survival_rate_NA = []\ntest_family_survival_rate = []\ntest_family_survival_rate_NA = []\n\nfor i in range(len(df_train)):\n    if df_train['Family'][i] in family_rates:\n        train_family_survival_rate.append(family_rates[df_train['Family'][i]])\n        train_family_survival_rate_NA.append(1)\n    else:\n        train_family_survival_rate.append(mean_survival_rate)\n        train_family_survival_rate_NA.append(0)\n        \nfor i in range(len(df_test)):\n    if df_test['Family'].iloc[i] in family_rates:\n        test_family_survival_rate.append(family_rates[df_test['Family'].iloc[i]])\n        test_family_survival_rate_NA.append(1)\n    else:\n        test_family_survival_rate.append(mean_survival_rate)\n        test_family_survival_rate_NA.append(0)\n        \ndf_train['Family_Survival_Rate'] = train_family_survival_rate\ndf_train['Family_Survival_Rate_NA'] = train_family_survival_rate_NA\ndf_test['Family_Survival_Rate'] = test_family_survival_rate\ndf_test['Family_Survival_Rate_NA'] = test_family_survival_rate_NA\n\ntrain_ticket_survival_rate = []\ntrain_ticket_survival_rate_NA = []\ntest_ticket_survival_rate = []\ntest_ticket_survival_rate_NA = []\n\nfor i in range(len(df_train)):\n    if df_train['Ticket'][i] in ticket_rates:\n        train_ticket_survival_rate.append(ticket_rates[df_train['Ticket'][i]])\n        train_ticket_survival_rate_NA.append(1)\n    else:\n        train_ticket_survival_rate.append(mean_survival_rate)\n        train_ticket_survival_rate_NA.append(0)\n        \nfor i in range(len(df_test)):\n    if df_test['Ticket'].iloc[i] in ticket_rates:\n        test_ticket_survival_rate.append(ticket_rates[df_test['Ticket'].iloc[i]])\n        test_ticket_survival_rate_NA.append(1)\n    else:\n        test_ticket_survival_rate.append(mean_survival_rate)\n        test_ticket_survival_rate_NA.append(0)\n        \ndf_train['Ticket_Survival_Rate'] = train_ticket_survival_rate\ndf_train['Ticket_Survival_Rate_NA'] = train_ticket_survival_rate_NA\ndf_test['Ticket_Survival_Rate'] = test_ticket_survival_rate\ndf_test['Ticket_Survival_Rate_NA'] = test_ticket_survival_rate_NA\n\nfor df in [df_train, df_test]:\n    df['Survival_Rate'] = (df['Ticket_Survival_Rate'] + df['Family_Survival_Rate']) / 2\n    df['Survival_Rate_NA'] = (df['Ticket_Survival_Rate_NA'] + df['Family_Survival_Rate_NA']) / 2","metadata":{"_uuid":"1121a06d-211f-43c8-b803-fd2d88e08ea0","_cell_guid":"e9fcbea0-d094-46ba-b578-4551a3128375","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-26T22:37:09.327135Z","iopub.execute_input":"2022-03-26T22:37:09.327336Z","iopub.status.idle":"2022-03-26T22:37:09.522787Z","shell.execute_reply.started":"2022-03-26T22:37:09.327311Z","shell.execute_reply":"2022-03-26T22:37:09.521537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b>4 <span style='color:#F1C40F'>|</span> Data Visualization</b>\n\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>4.1 | Heat Map</b></p>\n</div>\n\nüìå **Interpret:** On the one hand, the variables with the highest correlations are: **<span style='color:lightseagreen'>PClass</span>** and **<span style='color:lightseagreen'>Fare</span>**. On the other hand, we can appreciate that both, Age and FamilySize are the ones with less correlation coefficient with respect to Survived.","metadata":{"_uuid":"64b96213-2534-4634-a457-3e75e3393311","_cell_guid":"851abf7b-a32e-4bda-85df-5ad140886b78","trusted":true}},{"cell_type":"code","source":"df_heatmap = pd.DataFrame(df_data.corr()['Survived'].abs())\nf,ax = plt.subplots(figsize=(16,1.5),facecolor='white')\nsns.color_palette(\"rocket\", as_cmap=True)          # Esta paleta es la que viene por defecto\nsns.heatmap(df_heatmap.transpose(),annot = True,square=True, linewidths=1.5, cmap='rocket')","metadata":{"_uuid":"cb1cca8b-ba76-4997-841a-c42566d1b070","_cell_guid":"43d22b90-37aa-4a3b-a606-accab626f88e","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-26T22:37:09.527757Z","iopub.execute_input":"2022-03-26T22:37:09.528165Z","iopub.status.idle":"2022-03-26T22:37:09.817196Z","shell.execute_reply.started":"2022-03-26T22:37:09.52813Z","shell.execute_reply":"2022-03-26T22:37:09.815549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>4.2 | Group of Age Visualization</b></p>\n</div>\n\nüìå **Interpret:** We have created several **<span style='color:lightseagreen'>age groups</span>**, in order to make it easier to make the graphs. On the left size, we have the average fare per each group of age. As observed, **<span style='color:lightseagreen'>older group (48-80)</span>** is the group which has payed more. At the bottom part of the bar graph we can find groups to which teenagers and young adults belong. On the right, I have made a pie plot in order to find which percentage of people from each group survive, and people from which group is more likely to survive. We can see that **<span style='color:lightseagreen'>kids (0.169-16)</span>** are the ones with more survival rate (14.6%). Moreover, we also observe that half of the kids survived. Concretely, a 58.8% of kids survived. On the other hand, older people and youngsters between 21-22 years old bear the brunt as they are the least likely to survive. Indeed, from the whole group of old people just 34% survived aproximately.","metadata":{"_uuid":"b4e08424-4c5d-41ee-9225-5487d5bfee38","_cell_guid":"7da9b2bf-2c2c-48ba-96ca-cb78118b0b3c","trusted":true}},{"cell_type":"code","source":"#names = ['0-8', '9-15', '16-18', '19-25', '26-40', '41-60', '61-100']\noneHot_train_graph = df_data_no_quart.copy()\nnames = ['0.169-16','16-21','21-22','22-25','25-26','26-29.5','29.5-34','34-40','40-48','48-80']\noneHot_train_graph['Age'] = pd.qcut(oneHot_train_graph['Age'], 10, labels = names)\ndf_bar = oneHot_train_graph.groupby('Age').agg({'Fare':'mean'}).reset_index().sort_values(by='Fare',ascending=False).set_index('Age')\ndf_pie = oneHot_train_graph.groupby('Age').agg({\"Survived\" : \"mean\"}).reset_index().sort_values(by='Survived', ascending=False).set_index('Age')\n\nfig = make_subplots(rows=1, cols=2, \n                    specs=[[{\"type\": \"bar\"}, {\"type\": \"pie\"}]],                          \n                    column_widths=[0.7, 0.3], vertical_spacing=0, horizontal_spacing=0.02,\n                    subplot_titles=(\"Average Fare per Group of Age\", \"Survival Percentage per Group of Age\"))\n\nfig.add_trace(go.Bar(x=df_bar['Fare'], y=df_bar.index, marker=dict(color=['#334550','#334550','#394184','#394184','#6D83AA','#6D83AA','#C8D0DF','#C8D0DF','#C8D0DF','#C8D0DF']),\n                     name='Fare', orientation='h'), \n                     row=1, col=1)\nfig.add_trace(go.Pie(values=df_pie['Survived'], labels=df_pie.index, name='Age',\n                     marker=dict(colors=['#334550','#334668','#394184','#496595','#6D83AA','#91A2BF','#C8D0DF']), hole=0.7,\n                     hoverinfo='label+percent+value', textinfo='label'), \n                    row=1, col=2)\n# styling\nfig.update_yaxes(showgrid=False, ticksuffix=' ', categoryorder='total ascending', row=1, col=1)\nfig.update_xaxes(visible=False, row=1, col=1)\nfig.update_layout(height=500, bargap=0.2,\n                  margin=dict(b=0,r=20,l=20), xaxis=dict(tickmode='linear'),\n                  title_text=\"Group of Age Analysis\",\n                  template=\"plotly_white\",\n                  title_font=dict(size=29, color='#8a8d93', family=\"Lato, sans-serif\"),\n                  font=dict(color='#8a8d93'), \n                  hoverlabel=dict(bgcolor=\"#f2f2f2\", font_size=13, font_family=\"Lato, sans-serif\"),\n                  showlegend=False)\nfig.show()","metadata":{"_uuid":"f1f3242b-b30f-406b-8317-a32697ba7b4c","_cell_guid":"4fdb7250-56e9-43c8-baf0-717a51c2d992","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-26T22:37:09.818661Z","iopub.execute_input":"2022-03-26T22:37:09.81891Z","iopub.status.idle":"2022-03-26T22:37:09.917931Z","shell.execute_reply.started":"2022-03-26T22:37:09.818881Z","shell.execute_reply":"2022-03-26T22:37:09.917077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>4.3 | PClass Visualization</b></p>\n</div>\n\nüìå **Interpret:** for this graph we'll be grouping by **<span style='color:lightseagreen'>PClass</span>** atribute. I have plotted 3 different graphs relating PClass with **<span style='color:lightseagreen'>Title</span>**, **<span style='color:lightseagreen'>Loneliness</span>** and **<span style='color:lightseagreen'>Family Size</span>**. We are able to appreciate that **<span style='color:lightseagreen'>lonely people</span>** have chosen class 3 more than any other. Indeed, both Class 3 and Class 2 have been the preference for more lonely people than accompanied. Class 1 as seen, is more likely to be chosen by people accompanied by one familiar. Families with 3 or more people on board are uniformly distributed between different classes.","metadata":{"_uuid":"a93cf7cf-4824-4d26-86cf-fcef8e0bfd24","_cell_guid":"75dc7b0f-2e9d-4b21-ba3a-1dbee4ba5174","trusted":true}},{"cell_type":"code","source":"fig, axes = plt.subplots(nrows=1, ncols=3,figsize=(20, 8))\n\nsns.set_style('whitegrid')\nax = sns.countplot(x = \"Pclass\", hue='Title', data = df_data,ax = axes[0], palette=['#334550','#394184','#6D83AA','#91A2BF','#C8D0DF']);\nax.set_title('PClass per Title')\n\nsns.set_style('whitegrid')\nax = sns.countplot(x = \"Pclass\", hue='IsAlone', data = df_data,ax = axes[1], palette=['#334550','#C8D0DF']);\n_ = ax.set_title('PClass per Loneliness')\n\nsns.set_style('whitegrid')\nax = sns.countplot(x = \"Pclass\", hue='FamilySize', data = df_data,ax = axes[2], palette=['#334550','#394184','#6D83AA','#91A2BF','#C8D0DF']);\n__ = ax.set_title('PClass per Family Size')","metadata":{"_uuid":"e2f82483-d49b-43ad-803a-4c3333b09f68","_cell_guid":"f2d7b9c6-b37c-4054-923a-440080558e9d","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-26T22:37:09.919003Z","iopub.execute_input":"2022-03-26T22:37:09.919195Z","iopub.status.idle":"2022-03-26T22:37:10.517365Z","shell.execute_reply.started":"2022-03-26T22:37:09.919171Z","shell.execute_reply":"2022-03-26T22:37:10.516414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>4.4 | Fare Visualization</b></p>\n</div>\n\nüìå **Interpret:** The groups at the left side of the graph has the lowest survival rate and the groups at the right side of the graph has the highest survival rate. This high survival rate was not visible in the distribution graph. There is also an unusual group (15.742, 23.25] in the middle with high survival rate that is captured in this process.","metadata":{"_uuid":"f1e0eafc-6154-4429-9607-5a5e8abfc3c1","_cell_guid":"6c5e96ff-8818-42b0-82c1-d54cfc37c2f6","trusted":true}},{"cell_type":"code","source":"fig, axs = plt.subplots(figsize=(22, 9))\nsns.countplot(x='Fare', hue='Survived', data=df_data, palette=['#334550','#C8D0DF'])\n\nplt.xlabel('Fare', size=15, labelpad=20)\nplt.ylabel('Passenger Count', size=15, labelpad=20)\nplt.tick_params(axis='x', labelsize=10)\nplt.tick_params(axis='y', labelsize=15)\n\nplt.legend(['Not Survived', 'Survived'], loc='upper right', prop={'size': 15})\nplt.title('Count of Survival in {} Feature'.format('Fare'), size=15, y=1.05)\n\nplt.show()","metadata":{"_uuid":"6e99070d-cf4a-4194-8d0c-ed3bff3e0395","_cell_guid":"5944a299-7ba4-4e76-b127-1761a5c2112e","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-26T22:37:10.518275Z","iopub.execute_input":"2022-03-26T22:37:10.518819Z","iopub.status.idle":"2022-03-26T22:37:10.79645Z","shell.execute_reply.started":"2022-03-26T22:37:10.518794Z","shell.execute_reply":"2022-03-26T22:37:10.795539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b>5 <span style='color:#F1C40F'>|</span> Feature Transformation</b>\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>5.1 | Labeling Non-Numerical Features</b></p>\n</div>\n\nUsing **<span style='color:lightseagreen'>LabelEncoder</span>**, we are going to convert non-numerical features to numerical type. LabelEncoder basically labels the classes from **<span style='color:lightseagreen'>0 to n</span>**. This process is necessary for models to learn from those features.","metadata":{"_uuid":"6e0980b8-106d-4177-b913-769a7a4080a1","_cell_guid":"3292f636-aea8-4838-a3e5-31032709ed9e","trusted":true}},{"cell_type":"code","source":"df_data.dtypes","metadata":{"_uuid":"1f16e9a0-59a1-4362-ac10-0ce154bb6fd1","_cell_guid":"e90045c1-85f1-4d85-be09-8d04cff9e14c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-26T22:37:10.79799Z","iopub.execute_input":"2022-03-26T22:37:10.798216Z","iopub.status.idle":"2022-03-26T22:37:10.807196Z","shell.execute_reply.started":"2022-03-26T22:37:10.79819Z","shell.execute_reply":"2022-03-26T22:37:10.805844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"non_numerical_cols =  [col for col in df_data.columns if df_data[col].dtype == 'object']\nnon_numerical_cols.append('Age')\n\nfor df in dfs:\n    for feature in non_numerical_cols:        \n        df[feature] = LabelEncoder().fit_transform(df[feature])","metadata":{"_uuid":"354d336c-6e4f-4b8c-af88-ac149bdbea20","_cell_guid":"dc16e7d6-979d-4aac-8b57-5ff3e1c0aaea","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-26T22:37:10.808419Z","iopub.execute_input":"2022-03-26T22:37:10.808671Z","iopub.status.idle":"2022-03-26T22:37:10.834814Z","shell.execute_reply.started":"2022-03-26T22:37:10.808647Z","shell.execute_reply":"2022-03-26T22:37:10.833496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>5.2 | One Hot Encoding</b></p>\n</div>\n\nTo finish with, we are going to one hot encoded non-ordinal features. Those features are **<span style='color:lightseagreen'>Embarked, Sex, Deck, Title and PClass</span>**. `Age` and `Fare` as ordinal features are not converted.","metadata":{"_uuid":"e54580d6-c9a0-4c19-9ab6-8e680620d11b","_cell_guid":"b75a9a34-a950-4bd4-9794-93b7877fa273","trusted":true}},{"cell_type":"code","source":"cat_features = ['Pclass', 'Sex', 'Deck', 'Embarked', 'Title', 'FamilySizeGrouped']\nencoded_features = []\n\nfor df in dfs:\n    for feature in cat_features:\n        encoded_feat = OneHotEncoder().fit_transform(df[feature].values.reshape(-1, 1)).toarray()\n        n = df[feature].nunique()\n        cols = ['{}_{}'.format(feature, n) for n in range(1, n + 1)]\n        encoded_df = pd.DataFrame(encoded_feat, columns=cols)\n        encoded_df.index = df.index\n        encoded_features.append(encoded_df)\n\ndf_train = pd.concat([df_train, *encoded_features[:6]], axis=1)\ndf_test = pd.concat([df_test, *encoded_features[6:]], axis=1)","metadata":{"_uuid":"c4903dc7-ac07-4407-b86f-fef14b29021c","_cell_guid":"f312268b-2e48-4cf9-b9d9-4be512271515","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-26T22:37:10.835893Z","iopub.execute_input":"2022-03-26T22:37:10.83617Z","iopub.status.idle":"2022-03-26T22:37:10.860751Z","shell.execute_reply.started":"2022-03-26T22:37:10.836146Z","shell.execute_reply":"2022-03-26T22:37:10.86001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_data_OH = pd.concat([df_train, df_test], sort=True).reset_index(drop=True)\ndrop_cols = ['Deck', 'Embarked', 'Family', 'FamilySize', 'FamilySizeGrouped',\n             'Name', 'PassengerId', 'Pclass', 'Sex', 'Ticket', 'Title',\n            'Ticket_Survival_Rate', 'Family_Survival_Rate', 'Ticket_Survival_Rate_NA', 'Family_Survival_Rate_NA','IsAlone']\n\ndf_data_OH.drop(columns=drop_cols, inplace=True)\n\ndf_data_OH.head()","metadata":{"_uuid":"e93014ea-5d00-4ce7-92ec-06390a42bba4","_cell_guid":"178f0006-64d7-4573-8a7b-bcf27d4f5eae","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-26T22:37:10.861765Z","iopub.execute_input":"2022-03-26T22:37:10.86259Z","iopub.status.idle":"2022-03-26T22:37:10.904426Z","shell.execute_reply.started":"2022-03-26T22:37:10.862549Z","shell.execute_reply":"2022-03-26T22:37:10.903103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b>6 <span style='color:#F1C40F'>|</span> Modeling</b>\n<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>6.1 | Cross Validation</b></p>\n</div>\n\nFor the modeling part we will compare 10 known algorithms, and proceed to evaluate their average accuracy by a **<span style='color:lightseagreen'>stratified kfold cross validation</span>** procedure:\n* SVC\n* Decision Tree\n* AdaBoost\n* Random Forest\n* Extra Trees\n* Gradient Boosting\n* Multiple layer perceprton (neural network)\n* KNN\n* Logistic regression\n* Linear Discriminant Analysis\n* XGBoost Classifier\n\nTo begin with, we are going to create a cross validate model with Kfold stratified. Then we'll test each of the algorithms that I have mentioned before.","metadata":{"_uuid":"47625c82-4a0f-4f46-8f07-681e8ad47826","_cell_guid":"641a5f98-4838-477d-898e-1e24ba449205","trusted":true}},{"cell_type":"code","source":"x_train = df_data_OH[df_data_OH.Survived.isnull() == False].drop('Survived',axis=1)\ny_train = df_data_OH[df_data_OH.Survived.isnull() == False].Survived","metadata":{"_uuid":"95272645-12c6-4a81-b641-f448adc251b5","_cell_guid":"44e1178e-ea38-43ba-872e-efb1a0b15d75","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-26T22:37:10.906314Z","iopub.execute_input":"2022-03-26T22:37:10.906728Z","iopub.status.idle":"2022-03-26T22:37:10.915463Z","shell.execute_reply.started":"2022-03-26T22:37:10.906703Z","shell.execute_reply":"2022-03-26T22:37:10.914014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kfold = StratifiedKFold(n_splits=10)\nrandom_state = 2\nclassifiers = []\nclassifiers.append(SVC(random_state=random_state))\nclassifiers.append(DecisionTreeClassifier(random_state=random_state))\nclassifiers.append(AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state),random_state=random_state,learning_rate=0.1))\nclassifiers.append(RandomForestClassifier(random_state=random_state))\nclassifiers.append(ExtraTreesClassifier(random_state=random_state))\nclassifiers.append(GradientBoostingClassifier(random_state=random_state))\nclassifiers.append(MLPClassifier(random_state=random_state))\nclassifiers.append(KNeighborsClassifier())\nclassifiers.append(LinearDiscriminantAnalysis())\nclassifiers.append(XGBClassifier(random_state = random_state))\n\ncv_results = []\nfor classifier in classifiers :\n    cv_results.append(cross_val_score(classifier, x_train, y = y_train, scoring = \"accuracy\", cv = kfold, n_jobs=4))\n    \ncv_means = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())\n    \ncv_res = pd.DataFrame({\"CrossValMeans\":cv_means,\"CrossValerrors\": cv_std,\"Algorithm\":[\"SVC\",\"DecisionTree\",\"AdaBoost\",\n\"RandomForest\",\"ExtraTrees\",\"GradientBoosting\",\"MultipleLayerPerceptron\",\"KNeighboors\",\"LinearDiscriminantAnalysis\",'XGBClassifier']})\ncv_res = cv_res.sort_values(by='CrossValMeans',ascending = False)","metadata":{"_uuid":"af9433c7-ccd2-4071-adc1-c3696a10334b","_cell_guid":"be109a3f-82b6-4084-8916-df8e66428336","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-26T22:37:10.916776Z","iopub.execute_input":"2022-03-26T22:37:10.916959Z","iopub.status.idle":"2022-03-26T22:39:49.369016Z","shell.execute_reply.started":"2022-03-26T22:37:10.916937Z","shell.execute_reply":"2022-03-26T22:39:49.368518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = make_subplots(rows=1, cols=1, \n                    specs=[[{\"type\": \"bar\"}]])\n\nfig.add_trace(go.Bar(x=cv_res['CrossValMeans'], y=cv_res.Algorithm, marker=dict(color=['#334550','#334550','#334668','#334668','#496595','#496595','#6D83AA','#6D83AA','#91A2BF','#C8D0DF']),\n                     name='Fare', orientation='h'), \n                     row=1, col=1)\n# styling\nfig.update_yaxes(showgrid=True, ticksuffix=' ', categoryorder='total ascending', row=1, col=1)\nfig.update_xaxes(visible=True, row=1, col=1)\nfig.update_layout(height=500, bargap=0.1,\n                  margin=dict(b=0,r=20,l=20), xaxis=dict(tickmode='linear'),\n                  title_text=\"Cross Validation Scores\",\n                  template=\"plotly_white\",\n                  title_font=dict(size=29, color='#8a8d93', family=\"Lato, sans-serif\"),\n                  font=dict(color='#8a8d93'), \n                  hoverlabel=dict(bgcolor=\"#f2f2f2\", font_size=13, font_family=\"Lato, sans-serif\"),\n                  showlegend=False)\nfig.show()","metadata":{"_uuid":"2f8d6546-db85-48d4-a16c-a7d9e0e5432f","_cell_guid":"5bcc0965-b024-4612-b7cc-7d68ddc4887e","collapsed":false,"_kg_hide-input":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-26T22:39:49.372264Z","iopub.execute_input":"2022-03-26T22:39:49.373636Z","iopub.status.idle":"2022-03-26T22:39:49.424511Z","shell.execute_reply.started":"2022-03-26T22:39:49.373591Z","shell.execute_reply":"2022-03-26T22:39:49.42396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>6.2 | Hyperparameter tuning</b></p>\n</div>\n\nFor the ensemble modeling we are going to use: \n* ExtraTreesClassifier\n* SVC\n* AdaBoost \n* RandomForest\n* GradientBoosting\nIn order to make execution quicker, we set **<span style='color:lightseagreen'>n_jobs to -1</span>**. This means that we are going to use every CPU we have in the computer.","metadata":{"_uuid":"e73158cf-f11f-42e5-a987-12ccb0c8fd92","_cell_guid":"087a2663-bf2a-4a07-8c8c-629b9a057002","trusted":true}},{"cell_type":"code","source":"# Adaboost\nDTC = DecisionTreeClassifier()\nadaDTC = AdaBoostClassifier(DTC, random_state=7)\nada_param_grid = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n              \"base_estimator__splitter\" :   [\"best\", \"random\"],\n              \"algorithm\" : [\"SAMME\",\"SAMME.R\"],\n              \"n_estimators\" :[1,2],\n              \"learning_rate\":  [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3,1.5]}\ngsadaDTC = GridSearchCV(adaDTC,param_grid = ada_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= -1, verbose = 1)\ngsadaDTC.fit(x_train,y_train)\nada_best = gsadaDTC.best_estimator_\n\n# Gradient boosting tunning\nGBC = GradientBoostingClassifier()\ngb_param_grid = {'loss' : [\"deviance\"],\n              'n_estimators' : [100,200,300],\n              'learning_rate': [0.1, 0.05, 0.01],\n              'max_depth': [4, 8],\n              'min_samples_leaf': [100,150],\n              'max_features': [0.3, 0.1]}\ngsGBC = GridSearchCV(GBC,param_grid = gb_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= -1, verbose = 1)\ngsGBC.fit(x_train,y_train)\nGBC_best = gsGBC.best_estimator_\n\n# SVC classifier\nSVMC = SVC(probability=True)\nsvc_param_grid = {'kernel': ['rbf'], \n                  'gamma': [ 0.001, 0.01, 0.1, 1],\n                  'C': [1, 10, 50, 100,200,300, 1000]}\ngsSVMC = GridSearchCV(SVMC,param_grid = svc_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= -1, verbose = 1)\ngsSVMC.fit(x_train,y_train)\nSVMC_best = gsSVMC.best_estimator_\n\n#ExtraTrees \nExtC = ExtraTreesClassifier()\nex_param_grid = {\"max_depth\": [None],\n              \"max_features\": [1, 3, 10],\n              \"min_samples_split\": [2, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100,300],\n              \"criterion\": [\"gini\"]}\ngsExtC = GridSearchCV(ExtC,param_grid = ex_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= -1, verbose = 1)\ngsExtC.fit(x_train,y_train)\nExtC_best = gsExtC.best_estimator_\n\n# RandomForest\nrfc_single = RandomForestClassifier(criterion='gini',\n                                           n_estimators=1750,\n                                           max_depth=7,\n                                           min_samples_split=6,\n                                           min_samples_leaf=6,\n                                           max_features='auto',\n                                           oob_score=True,\n                                           random_state=42,\n                                           n_jobs=-1,\n                                           verbose=1) \nrfc_single.fit(x_train, y_train)\nclear_output()","metadata":{"_uuid":"017f15bc-a301-4cdf-91b2-7f0737c18519","_cell_guid":"309e9af2-1b62-4f21-8640-be56783910d5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-26T22:39:49.425629Z","iopub.execute_input":"2022-03-26T22:39:49.425973Z","iopub.status.idle":"2022-03-26T22:41:53.204499Z","shell.execute_reply.started":"2022-03-26T22:39:49.425941Z","shell.execute_reply":"2022-03-26T22:41:53.203479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>6.3 | Ensemble Modeling and Prediction</b></p>\n</div>\n\nFor the final part of this project I have chosen **<span style='color:lightseagreen'>VotingClassifier</span>**. We'll fit the model and then proceed to make the predictions. At the final part of this section you'll find some graphs related to survival predictions made.","metadata":{"_uuid":"2ba4727a-2978-4ea4-8428-efd5a9d77572","_cell_guid":"37250b6f-4a7d-4aec-9d73-5df74aca13fd","trusted":true}},{"cell_type":"code","source":"votingC = VotingClassifier(estimators=[('rfc', rfc_single), ('extc', ExtC_best),\n('svc', SVMC_best), ('adac',ada_best),('gbc',GBC_best)], voting='soft', n_jobs=4)\n\nvotingC.fit(x_train, y_train)\nx_test = df_data_OH[df_data_OH.Survived.isnull() == True].drop('Survived',axis=1)\npredictions_survived = votingC.predict(x_test)\nclear_output()","metadata":{"_uuid":"998d99df-a751-48f9-9602-063c2a520e9e","_cell_guid":"4728eabe-ed90-4aa7-90e2-76a77b4426b5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-26T22:41:53.205815Z","iopub.execute_input":"2022-03-26T22:41:53.206081Z","iopub.status.idle":"2022-03-26T22:42:00.625944Z","shell.execute_reply.started":"2022-03-26T22:41:53.206049Z","shell.execute_reply":"2022-03-26T22:42:00.624848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = pd.DataFrame({'Survived' : predictions_survived},index = df_data[df_data.Survived.isnull() == True].PassengerId)\npredictions['Survived'] = predictions.Survived.astype(int)\n#predictions.to_csv('submission.csv')","metadata":{"_uuid":"e165245b-f630-4e9b-8d8b-dc9d799c7f9b","_cell_guid":"e24c4fdf-91fa-4eaf-8eed-25ce80f81479","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-26T22:42:00.627942Z","iopub.execute_input":"2022-03-26T22:42:00.628145Z","iopub.status.idle":"2022-03-26T22:42:00.638318Z","shell.execute_reply.started":"2022-03-26T22:42:00.628121Z","shell.execute_reply":"2022-03-26T22:42:00.636785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>6.4 | Simple Model</b></p>\n</div>\n\nNow, as an extension, we are going to make predictions with both a single model, **<span style='color:lightseagreen'>RandomForestClassifier</span>**. Hereafter, we will make some studies on **<span style='color:lightseagreen'>features importance</span>** given by model.","metadata":{"_uuid":"bcc59ca9-6bf5-437f-a37d-83e796a31475","_cell_guid":"8a2c6fe1-b6b6-4783-b807-86631687449c","trusted":true}},{"cell_type":"code","source":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\nrfc_single = RandomForestClassifier(criterion='gini',\n                                           n_estimators=1750,\n                                           max_depth=7,\n                                           min_samples_split=6,\n                                           min_samples_leaf=6,\n                                           max_features='auto',\n                                           oob_score=True,\n                                           random_state=42,\n                                           n_jobs=-1,\n                                           verbose=1) \npredictions_train = rfc_single.fit(x_train, y_train)\nperm = PermutationImportance(rfc_single, random_state=1).fit(x_train, y_train)\n#mediana = x_test.Fare.describe()[6]\n#x_test.Fare = x_test.Fare.fillna(mediana)\npredictions_survived = rfc_single.predict(x_test)\npredictions = pd.DataFrame({'Survived' : predictions_survived},index = df_data[df_data.Survived.isnull() == True].PassengerId)\npredictions['Survived'] = predictions.Survived.astype(int)\npredictions.to_csv('submission.csv')\nclear_output()","metadata":{"_uuid":"74421a56-a3d7-4cd6-b89d-52ae3f1b82b4","_cell_guid":"ec0394db-defa-45c9-ad9e-554c2032dad6","collapsed":false,"jupyter":{"outputs_hidden":false},"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-03-26T22:45:04.381578Z","iopub.execute_input":"2022-03-26T22:45:04.38194Z","iopub.status.idle":"2022-03-26T22:46:54.344692Z","shell.execute_reply.started":"2022-03-26T22:45:04.381904Z","shell.execute_reply":"2022-03-26T22:46:54.343655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>6.5 | Permutation Importance</b></p>\n</div>\n\nOne of the most basic questions we might ask of a model is: **<span style='color:lightseagreen'>What features have the biggest impact on predictions?</span>** This concept is called feature importance. There are multiple ways to measure feature importance. Some approaches answer subtly different versions of the question above. Other approaches have documented shortcomings. In this section, we'll focus on permutation importance. Compared to most other approaches, permutation importance is:\n\n* Fast to calculate,\n* Widely used and understood, and\n* Consistent with properties we would want a feature importance measure to have.","metadata":{"_uuid":"b75f8b10-082a-47e5-8612-77c97d105470","_cell_guid":"1de5e2fb-1c6f-4682-83e1-4535607cf2f6","trusted":true}},{"cell_type":"code","source":"eli5.show_weights(perm, feature_names = x_test.columns.tolist())","metadata":{"_uuid":"201492a3-2768-4d57-aa6e-e7b32b24ffc0","_cell_guid":"c360c0d0-2b0a-4817-9911-22b92f5e7624","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-03-26T22:46:54.346293Z","iopub.execute_input":"2022-03-26T22:46:54.34657Z","iopub.status.idle":"2022-03-26T22:46:54.360025Z","shell.execute_reply.started":"2022-03-26T22:46:54.346544Z","shell.execute_reply":"2022-03-26T22:46:54.358603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>6.6 | Partial Plots</b></p>\n</div>\n\nLike permutation importance, partial dependence plots are calculated **<span style='color:lightseagreen'> after a model has been fit </span>**. The model is fit on real data that has not been artificially manipulated in any way. While feature importance shows what variables most affect predictions, partial dependence plots show **<span style='color:lightseagreen'> how a feature affects predictions </span>**.","metadata":{}},{"cell_type":"code","source":"from pdpbox import pdp, get_dataset, info_plots\n\n# Create the data that we will plot\nfeature_names = [i for i in x_test.columns if x_test[i].dtype in [np.int64]]\npdp_goals = pdp.pdp_isolate(model=rfc_single, dataset=x_test, model_features=x_test.columns, feature='Fare')\nclear_output()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-03-26T22:46:54.362401Z","iopub.execute_input":"2022-03-26T22:46:54.362751Z","iopub.status.idle":"2022-03-26T22:47:01.09815Z","shell.execute_reply.started":"2022-03-26T22:46:54.362717Z","shell.execute_reply":"2022-03-26T22:47:01.096881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot it\npdp.pdp_plot(pdp_goals, 'Age')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-26T22:47:01.10049Z","iopub.execute_input":"2022-03-26T22:47:01.101284Z","iopub.status.idle":"2022-03-26T22:47:01.350864Z","shell.execute_reply.started":"2022-03-26T22:47:01.101215Z","shell.execute_reply":"2022-03-26T22:47:01.34942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>6.7 | SHAP Values</b></p>\n</div>\n\nSHAP Values (an acronym from SHapley Additive exPlanations) break down a prediction to show the **<span style='color:lightseagreen'> impact of each feature </span>**. Where could you use this?\n\n* A model says a bank shouldn't loan someone money, and the bank is legally required to explain the basis for each loan rejection\n* A healthcare provider wants to identify what factors are driving each patient's risk of some disease so they can directly address those risk factors with targeted health interventions\n\nWe'll use SHAP Values to explain individual predictions in this lesson. In this section, we'll see how these can be aggregated into powerful model-level insights.","metadata":{}},{"cell_type":"code","source":"X_test = df_data_OH[df_data_OH.Survived.isnull() == True].drop('Survived',axis=1)\nX_test['Survived'] = predictions['Survived']\nX_test = X_test.drop(891,axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-03-26T22:47:01.353391Z","iopub.execute_input":"2022-03-26T22:47:01.35374Z","iopub.status.idle":"2022-03-26T22:47:01.365189Z","shell.execute_reply.started":"2022-03-26T22:47:01.353698Z","shell.execute_reply":"2022-03-26T22:47:01.36378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shap  # package used to calculate Shap values\n\n# Create object that can calculate shap values\nexplainer = shap.TreeExplainer(rfc_single)\n\n# Calculate Shap values\nshap_values = explainer.shap_values(X_test.iloc[48])      # random row\nshap.initjs()\n\nshap.force_plot(explainer.expected_value[1], shap_values[1], X_test.iloc[48])","metadata":{"execution":{"iopub.status.busy":"2022-03-26T22:47:01.366483Z","iopub.execute_input":"2022-03-26T22:47:01.36711Z","iopub.status.idle":"2022-03-26T22:47:01.485616Z","shell.execute_reply.started":"2022-03-26T22:47:01.367077Z","shell.execute_reply":"2022-03-26T22:47:01.484343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#323232;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:white;\"><b>6.8 | Confusion Matrix</b></p>\n</div>","metadata":{}},{"cell_type":"code","source":"plot_confusion_matrix(rfc_single, x_train, y_train)  \nclear_output()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-26T22:47:39.211607Z","iopub.execute_input":"2022-03-26T22:47:39.212171Z","iopub.status.idle":"2022-03-26T22:47:41.247083Z","shell.execute_reply.started":"2022-03-26T22:47:39.212126Z","shell.execute_reply":"2022-03-26T22:47:41.246335Z"},"trusted":true},"execution_count":null,"outputs":[]}]}