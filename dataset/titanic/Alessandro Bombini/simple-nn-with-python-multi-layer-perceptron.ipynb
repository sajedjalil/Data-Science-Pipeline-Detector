{"cells":[{"metadata":{},"cell_type":"markdown","source":"# How to build a simple Neural Network with Python: Multi-layer Perceptron\n\n## Table of Contents\n\n1. [Basics of Artificial Neural Networks](#Basics of Artificial Neural Networks)\n\n    1.1 [Single-layer and Multi-layer perceptron](#Single-layer and Multi-layer perceptron)\n    \n    1.2 [About the dataset](#About the dataset)\n    \n    1.3 [The Data](#The Data)\n    \n2. [Perceptron](#Perceptron)\n\n    2.1 [Activation functions](#Activation functions)\n    \n3. [Neural Network's Layer(s)](#Neural Network's Layer(s))\n\n    3.1 [Backpropagation and Gradien Descent](#Backpropagation and Gradien Descent)\n    \n      3.1.1 [TL;DR: (a.k.a: recap](#TL;DR: (a.k.a: recap))\n   \n   3.2 [Our ANN](#Our ANN)\n   \n4. [Training the model](#Training the model)\n   \n   4.1 [Define the Training as a Function](#Define the Training as a Function)\n   \n5. [Compute Predictions](#Compute Predictions)\n   \n   5.1 [Evaluation report](#Evaluation report)\n   \n   5.2 [Exporting the predictions and submit them](#Exporting the predictions and submit them)\n\n6. [The ANN as a class](#The ANN as a class)\n\n    6.1 [An initialization Improvement](#An initialization Improvement)\n    \n    6.2 [Conclusions](#Conclusions)"},{"metadata":{},"cell_type":"markdown","source":"# Basics of Artificial Neural Networks <a></a>\n\nArtificial neural networks (ANN or NN) are computing systems that are inspired by, but not identical to, biological neural networks that constitute animal brains. Such systems learn to perform tasks by considering examples, generally without being programmed with task-specific rules.\n\nA NN is based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. The basic example is the perceptron [1]. Each connection, like the synapses in a biological brain, can transmit a signal to other neurons. An artificial neuron that receives a signal then processes it and can signal neurons connected to it [2].\n\nIn ANN implementations, the \"signal\" at a connection is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs. The connections are called edges. Neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Neurons may have a threshold such that a signal is sent only if the aggregate signal crosses that threshold. Typically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer), to the last layer (the output layer), possibly after traversing the layers multiple times. [b],\n\n![ANN](https://upload.wikimedia.org/wikipedia/commons/4/46/Colored_neural_network.svg)\n\n### Single-layer and Multi-layer perceptrons\nA single layer perceptron (SLP) is a feed-forward network based on a threshold transfer function. SLP is the simplest type of artificial neural networks and can only classify linearly separable cases with a binary target (1, 0). [c], [d]\n\nBecause SLP is a linear classifier and if the cases are not linearly separable the learning process will never reach a point where all the cases are classified properly. The most famous example of the inability of perceptron to solve problems with linearly non-separable cases is the XOR problem.\n\nA multi-layer perceptron (MLP) has the same structure of a single layer perceptron with one or more hidden layers. The backpropagation algorithm consists of two phases: the forward phase where the activations are propagated from the input to the output layer, and the backward phase, where the error between the observed actual and the requested nominal value in the output layer is propagated backwards in order to modify the weights and bias values.\n\n## About the Dataset\n\n### Overview\nThe dataset is the one from the kaggle competion *[Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic/data)* \n\nThe data has been split into two groups:\n    * training set (train.csv)\n    * test set (test.csv)\n\nThe training set should be used to build your machine learning models. For the training set, we provide the outcome (also known as the “ground truth”) for each passenger. Your model will be based on “features” like passengers’ gender and class. You can also use feature engineering to create new features.\n\nThe test set should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.\n\nWe also include gender_submission.csv, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.\n\n\n\n-------\n[1] https://en.wikipedia.org/wiki/Perceptron\n\n[2] https://en.wikipedia.org/wiki/Artificial_neural_network\n\n[3] https://www.saedsayad.com/artificial_neural_network_bkp.htm\n\n[4] https://iamtrask.github.io/2015/07/12/basic-python-network/\n\n[5] https://www.freecodecamp.org/news/building-a-neural-network-from-scratch/\n\n[6] https://towardsdatascience.com/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6?"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nnp.random.seed(10)\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Required magic to display matplotlib plots in notebooks\n%matplotlib inline\n\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.model_selection import train_test_split\n\n# Input data files are available in the \"../input/\" directory.\n        \ndata = pd.read_csv('../input/titanic/train.csv')\n\ndata.head(4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The Data\n\nSince this is somewhat of a tutorial, we will use a very basic Multi-Layer Perceptron to predict if a set of passengers survived; as a feature, we will only use the *Passenger Class* for sake of simplicity. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# We define a dictionary to transform the 0,1 values in the labels to a String that defines the fate of the passenger\ndict_live = { \n    0 : 'Perished',\n    1 : 'Survived'\n}\n\n# We define a dictionary to binarize the sex\ndict_sex = {\n    'male' : 0,\n    'female' : 1\n}\n\n# We apply the dictionary using a lambda function and the pandas .apply() module\ndata['Bsex'] = data['Sex'].apply(lambda x : dict_sex[x])\n\n\n# Now the features are a 2 column matrix whose entries are the Class (1,2,3) and the Sex (0,1) of the passengers\nfeatures = data[['Pclass', 'Bsex']].to_numpy()\nlabels = data['Survived'].to_numpy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Perceptron\n\nThe perceptron is a basic function that mimics the human neuron. It receives $n$ inputs, associated to the dendrites inputs to the neuron. Each dendrite, due to *lernging*, is weighted by a number that signals its input relevance for the neuron [1]. \n\n![Neuron](https://upload.wikimedia.org/wikipedia/commons/a/a9/Complete_neuron_cell_diagram_en.svg)\n\nThe signal is thus elaborated and passed through the *axon* to others neurons [2]; actually, the neurons *fires* the signal only if the elaborated inputs have surpassed a certain threshold; this is a spiking neuron [3].\n\nThe perceptron wants to mimic it. Receinving a vector (i.e. array) $x_i$ of signals, where $i$ stands for the $i$-th dendrites, it weights each of them by a vector of weights $w_i$. It adds also a *bias* to remove near-zero issues (the bias shifts the decision boundary away from the origin and does not depend on any input value).\n\n![Perceptron](https://miro.medium.com/max/2870/1*n6sJ4yZQzwKL9wnF5wnVNg.png)\n\n### Activation functions\n\nAlso, the perceptron ignite an output through an activation function that is usually a *sigmoid* function [4]\n$$f (x) = \\frac{1}{1+e^{-x}} \\,,$$\n\n![sigmoid](https://upload.wikimedia.org/wikipedia/commons/8/88/Logistic-curve.svg)\n\nor a *rectifier*\n$$\\varphi(x) = \\mathrm{max}[0, x] \\,,$$\n![ReLU](https://upload.wikimedia.org/wikipedia/commons/6/6c/Rectifier_and_softplus_functions.svg)\n\nso that the output $O(x_i)$ of the perceptron is given by\n\n$$O (x_i) = \\varphi \\left( \\Sigma_{i=1}^{n} w_i \\, x_i + b   \\right) ,$$\n\nor, in vectorial representation \n\n$$O(x) = \\varphi \\left(\\mathbf{w}^T \\cdot \\mathbf{x} + b   \\right) $$\n\nBelow we present a simple code implementation of the perceptron.\n\n-----\n[1] https://en.wikipedia.org/wiki/Dendrite\n\n[2] https://en.wikipedia.org/wiki/Neuron\n\n[3] https://icwww.epfl.ch/~gerstner/BUCH.html\n\n[4] https://en.wikipedia.org/wiki/Rectifier_(neural_networks)"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Define the sigmoid activator; we ask if we want the sigmoid or its derivative\ndef sigmoid_act(x, der=False):\n    import numpy as np\n    \n    if (der==True) : #derivative of the sigmoid\n        f = x/(1-x)\n    else : # sigmoid\n        f = 1/(1+ np.exp(-x))\n    \n    return f\n\n# We may employ the Rectifier Linear Unit (ReLU)\ndef ReLU_act(x, der=False):\n    import numpy as np\n    \n    if (der== True):\n        if x>0 :\n            f= 1\n        else :\n            f = 0\n    else :\n        if x>0:\n            f = x\n        else :\n            f = 0\n    return f\n\n# Now we are ready to define the perceptron; \n# it eats a np.array (that may be a list of features )\ndef perceptron(X, act='Sigmoid'): \n    import numpy as np\n    \n    shapes = X.shape # Pick the number of (rows, columns)!\n    n= shapes[0]+shapes[1]\n    # Generating random weights and bias\n    w = 2*np.random.random(shapes) - 0.5 # We want w to be between -1 and 1\n    b = np.random.random(1)\n    \n    # Initialize the function\n    f = b[0]\n    for i in range(0, X.shape[0]-1) : # run over column elements\n        for j in range(0, X.shape[1]-1) : # run over rows elements\n            f += w[i, j]*X[i,j]/n\n    # Pass it to the activation function and return it as an output\n    if act == 'Sigmoid':\n        output = sigmoid_act(f)\n    else :\n        output = ReLU_act(f)\n        \n    return output\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"An example of an output of the Perceptron"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Output with sigmoid activator: ', perceptron(features))\nprint('Output with ReLU activator: ', perceptron(features))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Neural Network's Layer(s)\n\nA standard Artificial Neural Network will be made of multiple layers:\n1. An **Input Layer**, that pass the features to the NN\n2. An arbitrary number of **Hidden Layers**, containing an arbitrary number of neurons for each layer, that receives the inputs and elaborate them. We will introduce Hidden Layers with ReLU activator, since in the *hidden* part of the NN we don't need the output to be contained in the $[0,1]$ range. \n3. An **Output Layer**: these layers contains a number of neurons equal to the number of possible labels we want to have a prediction to; this is because the output of the NN is thus a vector whose dimension is the same as the cardinality of the set of labels, and its entries are the *probability* for each label for the element whose feateures we have passed to the NN. This means that we will use a sigmoid activator to the Output layer, so we squeeze each perceptron's output between 0 and 1. \n\n![ANN](https://miro.medium.com/proxy/1*DW0Ccmj1hZ0OvSXi7Kz5MQ.jpeg)\n\nIn this case, since we have a binary classification (Survived/Perished) we may simply use a single-perceptron Output layer; If the output is smaller than 0.5, the person is perished; otherwise, the person is survived. \n\nFor each layer, we have as an input a matrix made by columns of features (in our example, we have 2 features, i.e. Passenger Class and Passenger Sex), that we label as $I=1,2$. Each of this features will have $n$ entries, so that each feature is a vector $\\{x_I\\}_i$. The layer will have $p$ perceptrons, labelled by $a=1,\\ldots ,p$. Thus the output of the whole layer is a matrix ${O}^{(a)}_{(I)}$ given by\n$${O}^{(a)}_{(I)} = \\varphi\\left( \\mathbf{w}^{(a)}_{(I)} \\cdot  \\mathbf{x}^{(a)}_{(I)} + b^{(a)}_{(I)}   \\right) ,$$\n\nor, explicitly \n$${O}^{(a)}_{(I)} = \\varphi ( {\\large \\Sigma}_{i=1}^{n} \\left({w}^{(a)}_{(I)}\\right)_i \\left(  {x}^{(a)}_{(I)} \\right)_i + \\left( b^{(a)}_{(I)} \\right)_i  )  .$$\n\nIf we start having multiple layers, let us say $N$, we have an additional label $A=1, \\ldots, N$ so that \n$${}^{(A)}{O}^{(a)}_{(I)} = {}^{(A)}\\varphi\\left( {}^{(A)}\\mathbf{w}^{(a)}_{(I)} \\cdot  {}^{(A)}\\mathbf{x}^{(a)}_{(I)} + {}^{(A)}b^{(a)}_{(I)}   \\right) ,$$\n\nwhere we have inserted the label also to the activation function that may depend on which layer we are considering!\n\n## Backpropagation and Gradien Descent\n\nFor adjusting the trainable parameters $\\{w\\}$ and $\\{b\\}$, we need to implement the *backpropagation*. We want to minimize a certain **cost function**\n$$\\mu(y,\\bar{y})=|y-\\bar{y}|^2$$\nwhere $y$ is the output of the output layer while $\\bar{y}$ is the actual label; in order to do so, we start the *gradient descent*, which means that we see the cost function as a function of the trainable parameters $\\mathbf{w}$ such as $\\{w\\}$ and $\\{b\\}$, we compute the gradient - gradient that can be seen as the slope of the multidimensional graph [3] - and we subtract it from the randomly initialized set, as\n$$\\mathbf{w}'_n =  \\mathbf{w}_n - \\eta \\nabla \\mu(\\mathbf{w}_n)  \\,,$$\nmoving thus towards the *optima*, or global minimum, of the cost function. In the formula above $\\eta$ is the **learning rate** of the ANN [4] \n\n\nThis is pictorially represented in the following picture [5]:\n\n![GD](https://hackernoon.com/hn-images/1*f9a162GhpMbiTVTAua_lLQ.png)\n\n**NB:** If you didn't want to commit yourself to the math, you can just jump to the **Our ANN** section below;\n\nExplicitly, the $\\alpha$-th hidden layer is defined by a *matrix* $w_{\\alpha}^{i_{\\alpha} i_{\\alpha-1}}$ and a vector $b_\\alpha^{i_\\alpha}$, and its output is\n$$z_\\alpha^{i_\\alpha} = \\varphi ( w_{\\alpha}^{i_{\\alpha} i_{\\alpha-1}} \\, z^{i_{\\alpha-1}}_{\\alpha-1} + b_\\alpha^{i_\\alpha} ) \\equiv \\varphi (\\zeta^{i_\\alpha}_\\alpha)  .$$\n\nThe output layer, on the contrary, has a binary output with sigmoid activation function\n$$y = f( w_{Out}^{i_n} z_n^{i_n} + b_{Out} ) \\equiv f (\\gamma) .$$\n\nNotice that we have used the greek letter to define the value of the output before employing the activation function, i.e. the greek letters refer to the output of the *soma*, while the latin letter are the output of the *axon*.\n\nWe need thus to compute the gradient $\\nabla \\mu$, where each element is $\\partial \\mu / \\partial z_\\alpha^{i_\\alpha}$; by applying the chain rule repeaditly we get \n$$ \\frac{\\partial \\mu}{\\partial z_{\\alpha}^{i_\\alpha}} = \\frac{\\partial \\mu}{\\partial z_{\\alpha+1}^{i_{\\alpha+1}} } \\cdot \\frac{\\partial z_{\\alpha+1}^{i_{\\alpha+1}} }{\\partial z_{\\alpha}^{i_\\alpha}} = \\frac{\\partial \\mu}{\\partial z_{\\alpha+1}^{i_{\\alpha+1}} } \\cdot w_{\\alpha}^{i_{\\alpha+1} i_\\alpha} \\varphi' (z_{\\alpha}^{i_\\alpha}) \\,.$$\n\nDefininf **the error** for the $\\alpha$-th layer as \n$$\\delta_\\alpha^{i_\\alpha} = \\frac{\\partial \\mu}{\\partial z_{\\alpha}^{i_\\alpha}}$$\nwe see that, working *backwards*, we may obtain it from the previously computed $(\\alpha+1)$-th error via [6]\n$$\\delta_{\\alpha}^{i_\\alpha}  = {\\large \\Sigma}_{{i_\\alpha}}  \\delta_{\\alpha+1}^{i_\\alpha+1}  \\, w_{\\alpha}^{i_{\\alpha+1} i_\\alpha} \\, \\varphi' (z_{\\alpha}^{i_\\alpha}) \\,.$$\n\nThis means that working iteratively, starting from the Output layer, we may easly compute the errors. \n\nNow, the goal of this whole procedure is to go towards a global minimum (or *optima*) of $\\mu$, seen as a function of the parametes $w_\\alpha^{i_{\\alpha+1} i_\\alpha}$ and $b_\\alpha^{i_{\\alpha}}$; \n\nSo, after having shifted the parameters as dictated by our gradient descent equation, we need to compute the variation of the cost function under the variation of parameters, to see if we have moved toward a decreasing cost-function path; in order to do so, we apply again the chain rule\n$$\\frac{\\partial \\mu}{\\partial w_\\alpha^{i_{\\alpha+1} i_\\alpha} } = \\frac{\\partial \\mu}{\\partial z_{\\alpha}^{i_\\alpha}}  \\cdot \\frac{\\partial z_\\alpha^{i_\\alpha} }{\\partial w_\\alpha^{i_{\\alpha+1} i_\\alpha} }  = \\delta_\\alpha^{i_\\alpha} \\cdot z_{\\alpha-1}^{i_\\alpha-1} \\varphi'(\\zeta_\\alpha^{i_\\alpha} ) ,$$\n\n$$\\frac{\\partial \\mu}{\\partial b_\\alpha^{i_{\\alpha}} } =  \\frac{\\partial \\mu}{\\partial z_{\\alpha}^{i_\\alpha}}  \\cdot \\frac{\\partial z_\\alpha^{i_\\alpha} }{\\partial b_\\alpha^{i_{\\alpha}} } =  \\delta_\\alpha^{i_\\alpha} \\cdot  \\varphi'(\\zeta_\\alpha^{i_\\alpha} ) . $$\n\n\n### TL;DR: (a.k.a: recap)\n\nWe now recap what we have shown: \n* **Input**: Set up the inputs $z_0^{i_0}$;\n* **Feed Forward**: Computes the output of the $\\alpha$-th layer $z_\\alpha^{i_\\alpha}$ via the formula \n$$z_\\alpha^{i_\\alpha} = \\varphi ( w_{\\alpha}^{i_{\\alpha} i_{\\alpha-1}} \\, z^{i_{\\alpha-1}}_{\\alpha-1} + b_\\alpha^{i_\\alpha} ) \\equiv \\varphi (\\zeta^{i_\\alpha}_\\alpha) ,$$\nup to the output layer, where the formula is \n$$y = f( w_{Out}^{i_n} z_n^{i_n} + b_{Out} ) .$$\n* **Compute the errors**: compute the error of the last layer via the formula \n$$\\delta^N_{i_N} = \\frac{\\partial \\mu}{\\partial z_N^{i_N} } \\,.$$\nIn our case the last Layer is actually the output layer (that we trat differently), so we have\n$$\\delta^{Out} = \\frac{\\partial \\mu}{\\partial z_{Out} } \\cdot f' (z_{Out}) .$$\n*  **Backpropagate the Error**: For each layer $\\alpha= N-1, \\ldots, 2$ compute \n$$\\delta_{\\alpha}^{i_\\alpha}  = {\\large \\Sigma}_{{i_\\alpha}}  \\delta_{\\alpha+1}^{i_\\alpha+1}  \\, w_{\\alpha}^{i_{\\alpha+1} i_\\alpha} \\, \\varphi' (z_{\\alpha}^{i_\\alpha}) \\,.$$\n* **Output**: the gradient of the cost function is now computed by the formulae\n$$\\frac{\\partial \\mu}{\\partial w_\\alpha^{i_{\\alpha+1} i_\\alpha} } = \\delta_\\alpha^{i_\\alpha} \\cdot z_{\\alpha-1}^{i_\\alpha-1} \\varphi'(\\zeta_\\alpha^{i_\\alpha} ) ,$$\n$$\\frac{\\partial \\mu}{\\partial b_\\alpha^{i_{\\alpha}} } =  \\delta_\\alpha^{i_\\alpha} \\cdot  \\varphi'(\\zeta_\\alpha^{i_\\alpha} ) . $$\n\nNotice that the explicit derivatives of the activation functions are know and easy to compute, since\n$$f'(x) = f(x)(1-f(x)) = y(1-y) \\,,$$\nwhile \n$$\\varphi'(x) = \\Theta(x)\\,,$$\nwhere $\\Theta(x)$ is the *Heaviside Theta*, that outputs 1 for $x\\ge 0$ and $0$ otherwise. \n\nWe need to build now an algorithm that will perform the previous task in the followin manner: \n1. **Input a set of training examples** or *batches*.\n2. **For each training batch** $x_{i_0}$: set the corresponding input activation $z_1^{i_1} = \\varphi(w_1^{i_1 i_0} x_{i_0} + b_1^{i_1})$, and then perform the following steps: \n    1. **Feedforward**: $\\forall \\alpha=2,\\ldots N$ compute \n    $$z_\\alpha^{i_\\alpha} = \\varphi ( w_{\\alpha}^{i_{\\alpha} i_{\\alpha-1}} \\, z^{i_{\\alpha-1}}_{\\alpha-1} + b_\\alpha^{i_\\alpha} ) \\equiv \\varphi (\\zeta^{i_\\alpha}_\\alpha) ,$$\n    $$y = f( w_{Out}^{i_n} z_n^{i_n} + b_{Out} ) $$\n    2. **Compute the outer layer Error**: Compute the vector\n    $$\\delta^{Out} = \\frac{\\partial \\mu}{\\partial z_{Out} } \\cdot f' (z_{Out}) .$$\n    3. **Backpropagate the error**: for each $\\alpha = N, N-1, \\ldots 2$ compute \n    $$\\delta_{\\alpha}^{i_\\alpha}  = {\\large \\Sigma}_{{i_\\alpha}}  \\delta_{\\alpha+1}^{i_\\alpha+1}  \\, w_{\\alpha}^{i_{\\alpha+1} i_\\alpha} \\, \\varphi' (z_{\\alpha}^{i_\\alpha}) \\,.$$\n3. **Gradient Descent**: for each $\\alpha = N, N-1, \\ldots 2$ update the weights $\\{ w_\\alpha^{i_\\alpha i_{\\alpha-1} } \\,, b_\\alpha^{i_\\alpha} \\}$ via the rule\n$$w_\\alpha^{i_\\alpha i_{\\alpha-1} } \\mapsto w_\\alpha^{i_\\alpha i_{\\alpha-1} } - \\eta {\\large \\Sigma}_{i_\\alpha} \\delta_\\alpha^{i_\\alpha} \\cdot z_{\\alpha-1}^{i_{\\alpha-1}} \\,,$$\n$$b_\\alpha^{i_\\alpha  } \\mapsto b_\\alpha^{i_\\alpha  } - \\eta {\\large \\Sigma}_{i_\\alpha} \\delta_\\alpha^{i_\\alpha} \\,.$$\n\n## Our ANN\n \nLet us work out the example for our Neural Network: we will initialize an Artificial Neural Network having\n1. An **input Layer** whose inputs is a 2-entris vector $x_i$ = $[$ Passenger Class, Sex $]$. \n2. **Two Hidden Layers** with respectively $p$ and $q$ neurons *each*, with ReLU activator $\\varphi$, whose outpus are $z_1^{j}$ and $z_2^{k}$ and whose parameters are $\\{ w_1^{j i}, b_1^j\\}$ and $\\{ w_2^{kj}, b_2^k\\}$;\n3. **A single-neuron output layer**, since we want to have a binary classification, that has a sigmoid activator $f$ and outputs a $y\\in [0,1]$ real number and whose parameters are $\\{ w_{Out}^k, b_{Out}\\}$; \n\nThis ANN look somewhat similar to the picture below.\n\n![HL](http://cs231n.github.io/assets/nn1/neural_net2.jpeg)\n\nThis means that our algorithms will have to perform the following steps:\n0. After a train/test split of both training data $\\{x_i\\}_I$ and labels $\\{\\bar{y}\\}_I$, where $I$ runs over the passengers, it has to reassamble the train data into batches to train our Neural Network. For sake of simplicity, and since we are employing *batch gradient descent* [7], we will use the whole training dataset as a batch and thus have only 1 epoch. \n1. Inputs the training data $\\{x_i\\}_I$ and feed them to the first Hidden Layer;\n2. Initialize randomly all the parameters for each layers, i.e. $\\{ w_1^{j i}, b_1^j\\}$, $\\{ w_2^{kj}, b_2^k\\}$ and $\\{ w_{Out}^k, b_{Out}\\}$. Notice that $i=0,1$, $j=0, \\ldots, p-1$ and $k=0, \\ldots, q-1$. Now we can perform the following steps: For each $I$ in the passenger list, we have to\n    1. **Feedforward**: compute *in this order*\n    $$z_1^j = \\varphi( {\\large \\Sigma}_{i=0,1} \\, w_1^{j i} x^i + b_1^j ), $$\n    $$z_2^k = \\varphi( {\\large \\Sigma}_{j=0,\\ldots, p-1} \\, w_2^{k j} z_1^j + b_2^k ), $$\n    $$y = f( {\\large \\Sigma}_{k=0, \\ldots, q-1} w_{Out}^k z_2^k + b_{Out} ) .$$\n    2.  **Compute the outer layer Error**: Compute the vector\n    $$\\delta_{Out} = \\frac{\\partial \\mu}{\\partial y} \\cdot f' (y) = 2(y-\\bar{y}) \\cdot y(1-y) $$\n    3. **Backpropagate the error**: Compute *in this order* the following\n    $$\\delta_2^{k} =   \\delta_{Out}  \\, w_{Out}^{k} \\, \\varphi' (z_{2}^{k}) ,$$\n    $$\\delta_1^j = {\\large \\Sigma}_{k=0, \\ldots, q-1} \\delta_2^{k} w_2^{k j}  \\, \\varphi' (z_{1}^{j}) . $$\n3. **Gradient descent**: When we pass from the $I$-th to the $(I+1)$-th passenger in the training set, we need to update the weights *in this order* following the rules\n    * Output Layer:\n    $${}^{(I+1)}w_{Out}^k = {}^{(I)} w_{Out}^k - \\eta {}^{(I)}\\delta_{Out} \\, z_2^k \\,,$$\n    $${}^{(I+1)}b_{Out} = {}^{(I)} b_{Out} - \\eta {}^{(I)}\\delta_{Out}\\,,$$\n    * Second Layer:\n    $${}^{(I+1)}w_{2}^{kj} = {}^{(I)} w_{2}^{kj} - \\eta \\delta_2^{k} \\, z_1^{j} \\,,$$\n    $${}^{(I+1)}b_{2}^k = {}^{(I)} b_{2}^k - \\eta \\delta_2^{k} \\,, $$\n    * First Layer:\n    $${}^{(I+1)}w_{1}^{ji} = {}^{(I)} w_{1}^{ji} - \\eta \\delta_1^{j} \\, x_i^{i} \\,,$$\n    $${}^{(I+1)}b_{1} = {}^{(I)} b_{1}^j - \\eta \\delta_2^{j} \\,, $$\n\nNow we can iterate over the whole set $I=0, \\ldots \\, \\#$training_set_passengers so that we can **train** our Neural Network!\n\n-------\n[1] http://cs231n.github.io/neural-networks-1/\n\n[2] https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/\n\n[3] https://www.nature.com/articles/323533a0 (original paper) https://en.wikipedia.org/wiki/Gradient_descent (Wiki article)\n\n[4] https://en.wikipedia.org/wiki/Stochastic_gradient_descent\n\n[5] https://hackernoon.com/gradient-descent-aynk-7cbe95a778da\n\n[6] http://neuralnetworksanddeeplearning.com/chap2.html\n\n[7] https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/\n\n[Picture 1] https://medium.com/datadriveninvestor/when-not-to-use-neural-networks-89fb50622429"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\n# Define the sigmoid activator; we ask if we want the sigmoid or its derivative\ndef sigmoid_act(x, der=False):\n    import numpy as np\n    \n    if (der==True) : #derivative of the sigmoid\n        f = 1/(1+ np.exp(- x))*(1-1/(1+ np.exp(- x)))\n    else : # sigmoid\n        f = 1/(1+ np.exp(- x))\n    \n    return f\n\n# We may employ the Rectifier Linear Unit (ReLU)\ndef ReLU_act(x, der=False):\n    import numpy as np\n    \n    if (der == True): # the derivative of the ReLU is the Heaviside Theta\n        f = np.heaviside(x, 1)\n    else :\n        f = np.maximum(x, 0)\n    \n    return f","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train/Test split\n\nWe now split the set of features and labels into a training set and a test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"# split into training and testing sets\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, Y_train, Y_test = train_test_split(features, labels, test_size=0.30)\n\nprint('Training records:',Y_train.size)\nprint('Test records:',Y_test.size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set up the number of perceptron per each layer:\np=4 # Layer 1\nq=4 # Layer 2\n\n# Set up the Learning rate\neta =  1/623\n\n\n# 0: Random initialize the relevant data \nw1 = 2*np.random.rand(p , X_train.shape[1]) - 0.5 # Layer 1\nb1 = np.random.rand(p)\n\nw2 = 2*np.random.rand(q , p) - 0.5  # Layer 2\nb2 = np.random.rand(q)\n\nwOut = 2*np.random.rand(q) - 0.5  # Output Layer\nbOut = np.random.rand(1)\n\nmu = []\nvec_y = []\n\n# Start looping over the passengers, i.e. over I.\n\nfor I in range(0, X_train.shape[0]): #loop in all the passengers:\n    \n    # 1: input the data \n    x = X_train[I]\n    \n    \n    # 2: Start the algorithm\n    \n    # 2.1: Feed forward\n    z1 = ReLU_act(np.dot(w1, x) + b1) # output layer 1 \n    z2 = ReLU_act(np.dot(w2, z1) + b2) # output layer 2\n    y = sigmoid_act(np.dot(wOut, z2) + bOut) # Output of the Output layer\n    \n    #2.2: Compute the output layer's error\n    delta_Out =  (y-Y_train[I]) * sigmoid_act(y, der=True)\n    \n    #2.3: Backpropagate\n    delta_2 = delta_Out * wOut * ReLU_act(z2, der=True) # Second Layer Error\n    delta_1 = np.dot(delta_2, w2) * ReLU_act(z1, der=True) # First Layer Error\n    \n    # 3: Gradient descent \n    wOut = wOut - eta*delta_Out*z2  # Outer Layer\n    bOut = bOut - eta*delta_Out\n    \n    w2 = w2 - eta*np.kron(delta_2, z1).reshape(q,p) # Hidden Layer 2\n    b2 = b2 - eta*delta_2\n    \n    w1 = w1 - eta*np.kron(delta_1, x).reshape(p, x.shape[0]) # Hidden Layer 1\n    b1 = b1 - eta*delta_1\n    \n    # 4. Computation of the loss function\n    mu.append((1/2)*(y-Y_train[I])**2)\n    vec_y.append(y[0])\n\n\n# Plotting the Cost function for each training data     \nplt.figure(figsize=(10,6))\nplt.scatter(np.arange(0, X_train.shape[0]), mu, alpha=0.3, s=4, label='mu')\nplt.title('Loss for each training data point', fontsize=20)\nplt.xlabel('Training data', fontsize=16)\nplt.ylabel('Loss', fontsize=16)\nplt.show()\n\n# Plotting the average cost function over 10 training data    \npino = []\nfor i in range(0, 9):\n    pippo = 0\n    for m in range(0, 59):\n        pippo+=vec_y[60*i+m]/60\n    pino.append(pippo)\n    \n    \n\nplt.figure(figsize=(10,6))\nplt.scatter(np.arange(0, 9), pino, alpha=1, s=10, label='error')\nplt.title('Averege Loss by epoch', fontsize=20)\nplt.xlabel('Epoch', fontsize=16)\nplt.ylabel('Loss', fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define the Training as a Function\n\nSince we have learnt how to train an ANN, we are in the position of define a function that does that, by eating the X_train, Y_train as well as the number of perceptron $p,q$ for the first and second hidden layer, and the learning rate $\\eta$."},{"metadata":{"trusted":true},"cell_type":"code","source":"def ANN_train(X_train, Y_train, p=4, q=4, eta=0.0015):\n    import numpy as np\n    import matplotlib.pyplot as plt\n    \n    # 0: Random initialize the relevant data \n    w1 = 2*np.random.rand(p , X_train.shape[1]) - 0.5 # Layer 1\n    b1 = np.random.rand(p)\n\n    w2 = 2*np.random.rand(q , p) - 0.5  # Layer 2\n    b2 = np.random.rand(q)\n\n    wOut = 2*np.random.rand(q) - 0.5   # Output Layer\n    bOut = np.random.rand(1)\n\n    mu = []\n    vec_y = []\n\n    # Start looping over the passengers, i.e. over I.\n\n    for I in range(0, X_train.shape[0]-1): #loop in all the passengers:\n    \n        # 1: input the data \n        x = X_train[I]\n    \n        # 2: Start the algorithm\n    \n        # 2.1: Feed forward\n        z1 = ReLU_act(np.dot(w1, x) + b1) # output layer 1 \n        z2 = ReLU_act(np.dot(w2, z1) + b2) # output layer 2\n        y = sigmoid_act(np.dot(wOut, z2) + bOut) # Output of the Output layer\n    \n        #2.2: Compute the output layer's error\n        delta_Out = 2 * (y-Y_train[I]) * sigmoid_act(y, der=True)\n    \n        #2.3: Backpropagate\n        delta_2 = delta_Out * wOut * ReLU_act(z2, der=True) # Second Layer Error\n        delta_1 = np.dot(delta_2, w2) * ReLU_act(z1, der=True) # First Layer Error\n    \n        # 3: Gradient descent \n        wOut = wOut - eta*delta_Out*z2  # Outer Layer\n        bOut = bOut - eta*delta_Out\n    \n        w2 = w2 - eta*np.kron(delta_2, z1).reshape(q,p) # Hidden Layer 2\n        b2 = b2 -  eta*delta_2\n    \n        w1 = w1 - eta*np.kron(delta_1, x).reshape(p, x.shape[0])\n        b1 = b1 - eta*delta_1\n    \n        # 4. Computation of the loss function\n        mu.append((y-Y_train[I])**2)\n        vec_y.append(y)\n    \n    batch_loss = []\n    for i in range(0, 10):\n        loss_avg = 0\n        for m in range(0, 60):\n            loss_avg+=vec_y[60*i+m]/60\n        batch_loss.append(loss_avg)\n    \n    \n    plt.figure(figsize=(10,6))\n    plt.scatter(np.arange(1, len(batch_loss)+1), batch_loss, alpha=1, s=10, label='error')\n    plt.title('Averege Loss by epoch', fontsize=20)\n    plt.xlabel('Epoch', fontsize=16)\n    plt.ylabel('Loss', fontsize=16)\n    plt.show()\n    \n    return w1, b1, w2, b2, wOut, bOut, mu","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w1, b1, w2, b2, wOut, bOut, mu = ANN_train(X_train, Y_train, p=8, q=4, eta=0.0015)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Compute Predictions\n\nWe now have to compute predictions from our trained ANN. In order to do so we need to recall the trained parameters $\\{w\\}, \\{b\\}$ and use them to actually get the predictions. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def ANN_pred(X_test, w1, b1, w2, b2, wOut, bOut, mu):\n    import numpy as np\n    \n    pred = []\n    \n    for I in range(0, X_test.shape[0]): #loop in all the passengers\n        # 1: input the data \n        x = X_test[I]\n        \n        # 2.1: Feed forward\n        z1 = ReLU_act(np.dot(w1, x) + b1) # output layer 1 \n        z2 = ReLU_act(np.dot(w2, z1) + b2) # output layer 2\n        y = sigmoid_act(np.dot(wOut, z2) + bOut)  # Output of the Output layer\n        \n        # Append the prediction;\n        # We now need a binary classifier; we this apply an Heaviside Theta and we set to 0.5 the threshold\n        # if y < 0.5 the output is zero, otherwise is 1\n        pred.append( np.heaviside(y - 0.5, 1)[0] )\n    \n    \n    return np.array(pred);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = ANN_pred(X_test, w1, b1, w2, b2, wOut, bOut, mu)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluation report"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the confusion matrix\ncm = confusion_matrix(Y_test, predictions)\n\ndf_cm = pd.DataFrame(cm, index = [dict_live[i] for i in range(0,2)], columns = [dict_live[i] for i in range(0,2)])\nplt.figure(figsize = (7,7))\nsns.heatmap(df_cm, annot=True, cmap=plt.cm.Blues, fmt='g')\nplt.xlabel(\"Predicted Class\", fontsize=18)\nplt.ylabel(\"True Class\", fontsize=18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Of course the outcome is **not** amazing, but we have to keep in mind that this is a very simple (almost naive) implementation of an artificial neural network whose main goal was to explain (mostly to myself) how an ANN works in practice and how it is possible to implement it into *Python* from scratch. "},{"metadata":{},"cell_type":"markdown","source":"# Exporting the predictions and submit them\n\nWe now need to load the 'test.csv' dataset and use the trained ANN to make predictions. "},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = pd.read_csv('../input/titanic/test.csv')\n\ntest_data.head(4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We extract the relevant feature from the test_data as we have done before"},{"metadata":{"trusted":true},"cell_type":"code","source":"# We apply the dictionary using a lambda function and the pandas .apply() module\ntest_data['Bsex'] = test_data['Sex'].apply(lambda x : dict_sex[x])\n\n\nX = test_data[['Pclass', 'Bsex']].to_numpy()\n\ntest_predictions = ANN_pred(X, w1, b1, w2, b2, wOut, bOut, mu)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We thus export the predictions as a *Comma Separated Values* (csv) file. We then create a link that allows us to download such csv [1].\n\n-----\n[1] https://www.kaggle.com/getting-started/58426"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({\n        \"PassengerId\": test_data[\"PassengerId\"],\n        \"Survived\": test_predictions\n    })\n\nsubmission.head(5)\n\n# Export it in a 'Comma Separated Values' (CSV) file\nimport os\nos.chdir(r'../working')\nsubmission.to_csv(r'submission.csv', index=False)\n# Creating a link to download the .csv file we created\nfrom IPython.display import FileLink\nFileLink(r'submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have thus created a csv file containing our predictions for the Titanic 'test.csv' dataframe using our simple (2+1)-layer Artificial Neural Network!"},{"metadata":{},"cell_type":"markdown","source":"# The ANN as a Class\n\nWe want now to implement the code as Class for Python, so that we could easiliy generalize it; the goal is to have a class from which we can instantiate an object \"Neural Network\", and add to it as many hidden layers with as many neurons we want, with the desired activation functions and so on. To generalize further, we define also the *Activation_function* class and the *layers* class, so we may easily add more activation funtions or more different layers (such as Convolutional or Pooling layers for Convolutional Neural Networks).\n\nWithin the ANN class, we define the following methods:\n* **add**: it eats a tuple ( int(number_of_neurons), string(activation_function) ), i.e. the output of the ANN.layer method. It is a void method. It updates the HiddenLayer string defined by the __init__ method. \n* **FeedForward**: it implements the Feed Forward layer by layer.\n* **BackPropagation**: it implements the whole gradient descent mechanism; first, it computes the errors by implementing the backpropagation; then, it updates the ANN parameters by gradient descent. \n* **Fit**: this method eats the training features and labels and fits the ANN by calling iteratively *FeedForward* and *BackPropagation* methods. This allow us to easily modify (or generalize) either *FeedForward* or *BackPropagation* methods without altering the *Fit* method.\n* **predict**: it eats the featurs and spits the label predictions. \n* **set_learning_rate**: by default the learning rate is initialized to be 1, but we can call this method to set it to a different value. \n* **get_accuracy, get_avg_accuracy**: these methods' aim is to return the cost function either at each step of the training process or averaging over 10 passengers, respectively.\n\nIn the **layers** class we have\n* **layer**: it eats two imputs, the number of neurons and the activation function (as a string); it returns a tuple of the two. The idea is to leave room for a generalization of the *layers.layer* method later on by adding multiple layer type (i.e. Pooling or Convolutional layers for the CNN).\n\nIn the **Activation_function** class we have\n* **ReLU_act, sigmoid_act**: these are the activation functions. They can be easily generalized (LeakyReLU, ParametricReLU, etc.)"},{"metadata":{},"cell_type":"markdown","source":"## An initialization Improvement\n\nI recently found an interesting article on the web [1] (see [2] for an easy&fast review) where the authors were able to define a well-performing initialization method for the $\\{w\\}$ and $\\{b\\}$ of the ANN. It relies on the knowledge of the dimension of the previous layer; so they are gaussian-distributed randomly initialized values divided by the squareroot of the dimension of the previous layer, i.e.\n> w = np.random.randn(layer_size[l],layer_size[l-1])*np.sqrt(2/layer_size[l-1])\n>\n> b = np.random.randn(layer_size[l])*np.sqrt(2/layer_size[l-1])\n\nUsing that, as well as *Parametric Rectified Linear Units* (P-ReLU), they were able to obtain a result that\n> [...] *is the first to surpass human-level performance (5.1%, Russakovsky et al.) on this visual recognition challenge.*\n\nThe visual recognition challenge they are referring to is the **ImageNet Large Scale Visual Recognition Competition (ILSVRC) 2014**.\n\n-----\n[1] https://arxiv.org/abs/1502.01852\n\n[2] https://towardsdatascience.com/random-initialization-for-neural-networks-a-thing-of-the-past-bfcdd806bf9e"},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nArtificial Neural Network Class\n'''\nclass ANN:\n    import numpy as np # linear algebra\n    np.random.seed(10)\n    \n    '''\n    Initialize the ANN;\n    HiddenLayer vector : will contain the Layers' info\n    w, b, phi = (empty) arrays that will contain all the w, b and activation functions for all the Layers\n    mu = cost function\n    eta = a standard learning rate initialization. It can be modified by the 'set_learning_rate' method\n    '''\n    def __init__(self) :\n        self.HiddenLayer = []\n        self.w = []\n        self.b = []\n        self.phi = []\n        self.mu = []\n        self.eta = 1 #set up the proper Learning Rate!!\n    \n    '''\n    add method: to add layers to the network\n    '''\n    def add(self, lay = (4, 'ReLU') ):\n        self.HiddenLayer.append(lay)\n    \n    '''\n    FeedForward method: as explained before. \n    '''\n    @staticmethod\n    def FeedForward(w, b, phi, x):\n        return phi(np.dot(w, x) + b)\n        \n    '''\n    BackPropagation algorithm implementing the Gradient Descent \n    '''\n    def BackPropagation(self, x, z, Y, w, b, phi):\n        self.delta = []\n        \n        # We initialize ausiliar w and b that are used only inside the backpropagation algorithm once called        \n        self.W = []\n        self.B = []\n        \n        # We start computing the LAST error, the one for the OutPut Layer \n        self.delta.append(  (z[len(z)-1] - Y) * phi[len(z)-1](z[len(z)-1], der=True) )\n        \n        '''Now we BACKpropagate'''\n        # We thus compute from next-to-last to first\n        for i in range(0, len(z)-1):\n            self.delta.append( np.dot( self.delta[i], w[len(z)- 1 - i] ) * phi[len(z)- 2 - i](z[len(z)- 2 - i], der=True) )\n        \n        # We have the error array ordered from last to first; we flip it to order it from first to last\n        self.delta = np.flip(self.delta, 0)  \n        \n        # Now we define the delta as the error divided by the number of training samples\n        self.delta = self.delta/self.X.shape[0] \n        \n        '''GRADIENT DESCENT'''\n        # We start from the first layer that is special, since it is connected to the Input Layer\n        self.W.append( w[0] - self.eta * np.kron(self.delta[0], x).reshape( len(z[0]), x.shape[0] ) )\n        self.B.append( b[0] - self.eta * self.delta[0] )\n        \n        # We now descend for all the other Hidden Layers + OutPut Layer\n        for i in range(1, len(z)):\n            self.W.append( w[i] - self.eta * np.kron(self.delta[i], z[i-1]).reshape(len(z[i]), len(z[i-1])) )\n            self.B.append( b[i] - self.eta * self.delta[i] )\n        \n        # We return the descended parameters w, b\n        return np.array(self.W), np.array(self.B)\n    \n    \n    '''\n    Fit method: it calls FeedForward and Backpropagation methods\n    '''\n    def Fit(self, X_train, Y_train):            \n        print('Start fitting...')\n        '''\n        Input layer\n        '''\n        self.X = X_train\n        self.Y = Y_train\n        \n        '''\n        We now initialize the Network by retrieving the Hidden Layers and concatenating them \n        '''\n        print('Model recap: \\n')\n        print('You are fitting an ANN with the following amount of layers: ', len(self.HiddenLayer))\n        \n        for i in range(0, len(self.HiddenLayer)) :\n            print('Layer ', i+1)\n            print('Number of neurons: ', self.HiddenLayer[i][0])\n            if i==0:\n                # We now try to use the He et al. Initialization from ArXiv:1502.01852\n                self.w.append( np.random.randn(self.HiddenLayer[i][0] , self.X.shape[1])/np.sqrt(2/self.X.shape[1]) )\n                self.b.append( np.random.randn(self.HiddenLayer[i][0])/np.sqrt(2/self.X.shape[1]))\n                # Old initialization\n                #self.w.append(2 * np.random.rand(self.HiddenLayer[i][0] , self.X.shape[1]) - 0.5)\n                #self.b.append(np.random.rand(self.HiddenLayer[i][0]))\n                \n                # Initialize the Activation function\n                for act in Activation_function.list_act():\n                    if self.HiddenLayer[i][1] == act :\n                        self.phi.append(Activation_function.get_act(act))\n                        print('\\tActivation: ', act)\n\n            else :\n                # We now try to use the He et al. Initialization from ArXiv:1502.01852\n                self.w.append( np.random.randn(self.HiddenLayer[i][0] , self.HiddenLayer[i-1][0] )/np.sqrt(2/self.HiddenLayer[i-1][0]))\n                self.b.append( np.random.randn(self.HiddenLayer[i][0])/np.sqrt(2/self.HiddenLayer[i-1][0]))\n                # Old initialization\n                #self.w.append(2*np.random.rand(self.HiddenLayer[i][0] , self.HiddenLayer[i-1][0] ) - 0.5)\n                #self.b.append(np.random.rand(self.HiddenLayer[i][0]))\n                \n                # Initialize the Activation function\n                for act in Activation_function.list_act():\n                    if self.HiddenLayer[i][1] == act :\n                        self.phi.append(Activation_function.get_act(act))\n                        print('\\tActivation: ', act)\n            \n        '''\n        Now we start the Loop over the training dataset\n        '''  \n        for I in range(0, self.X.shape[0]): # loop over the training set\n            '''\n            Now we start the feed forward\n            '''  \n            self.z = []\n            \n            self.z.append( self.FeedForward(self.w[0], self.b[0], self.phi[0], self.X[I]) ) # First layers\n            \n            for i in range(1, len(self.HiddenLayer)): #Looping over layers\n                self.z.append( self.FeedForward(self.w[i] , self.b[i], self.phi[i], self.z[i-1] ) )\n        \n            \n            '''\n            Here we backpropagate\n            '''      \n            self.w, self.b  = self.BackPropagation(self.X[I], self.z, self.Y[I], self.w, self.b, self.phi)\n            \n            '''\n            Compute cost function\n            ''' \n            self.mu.append(\n                (1/2) * np.dot(self.z[len(self.z)-1] - self.Y[I], self.z[len(self.z)-1] - self.Y[I]) \n            )\n            \n        print('Fit done. \\n')\n        \n\n    \n    '''\n    predict method\n    '''\n    def predict(self, X_test):\n        \n        print('Starting predictions...')\n        \n        self.pred = []\n        self.XX = X_test\n        \n        for I in range(0, self.XX.shape[0]): # loop over the training set\n            \n            '''\n            Now we start the feed forward\n            '''  \n            self.z = []\n            \n            self.z.append(self.FeedForward(self.w[0] , self.b[0], self.phi[0], self.XX[I])) #First layer\n    \n            for i in range(1, len(self.HiddenLayer)) : # loop over the layers\n                self.z.append( self.FeedForward(self.w[i] , self.b[i], self.phi[i], self.z[i-1]))\n       \n            # Append the prediction;\n            # We now need a binary classifier; we this apply an Heaviside Theta and we set to 0.5 the threshold\n            # if y < 0.5 the output is zero, otherwise is zero\n            self.pred.append( np.heaviside(  self.z[-1] - 0.5, 1)[0] ) # NB: self.z[-1]  is the last element of the self.z list\n        \n        print('Predictions done. \\n')\n\n        return np.array(self.pred)\n   \n    '''\n    We need a method to retrieve the accuracy for each training data to follow the learning of the ANN\n    '''\n    def get_accuracy(self):\n        return np.array(self.mu)\n    # This is the averaged version\n    def get_avg_accuracy(self):\n        import math\n        self.batch_loss = []\n        for i in range(0, 10):\n            self.loss_avg = 0\n            # To set the batch in 10 element/batch we use math.ceil method\n            # int(math.ceil((self.X.shape[0]-10) / 10.0))    - 1\n            for m in range(0, (int(math.ceil((self.X.shape[0]-10) / 10.0))   )-1):\n                #self.loss_avg += self.mu[60*i+m]/60\n                self.loss_avg += self.mu[(int(math.ceil((self.X.shape[0]-10) / 10.0)) )*i + m]/(int(math.ceil((self.X.shape[0]-10) / 10.0)) )\n            self.batch_loss.append(self.loss_avg)\n        return np.array(self.batch_loss)\n    \n    '''\n    Method to set the learning rate\n    '''\n    def set_learning_rate(self, et=1):\n        self.eta = et\n        \n        \n'''\nlayers class\n'''\nclass layers :\n    '''\n    Layer method: used to call standar layers to add. \n    Easily generalizable to more general layers (Pooling and Convolutional layers)\n    '''        \n    def layer(p=4, activation = 'ReLU'):\n        return (p, activation)\n\n'''\nActivation functions class\n'''\nclass Activation_function(ANN):\n    import numpy as np\n    \n    def __init__(self) :\n        super().__init__()\n        \n    '''\n    Define the sigmoid activator; we ask if we want the sigmoid or its derivative\n    '''\n    def sigmoid_act(x, der=False):\n        if (der==True) : #derivative of the sigmoid\n            f = 1/(1+ np.exp(- x))*(1-1/(1+ np.exp(- x)))\n        else : # sigmoid\n            f = 1/(1+ np.exp(- x))\n        return f\n\n    '''\n    Define the Rectifier Linear Unit (ReLU)\n    '''\n    def ReLU_act(x, der=False):\n        if (der == True): # the derivative of the ReLU is the Heaviside Theta\n            f = np.heaviside(x, 1)\n        else :\n            f = np.maximum(x, 0)\n        return f\n    \n    def list_act():\n        return ['sigmoid', 'ReLU']\n    \n    def get_act(string = 'ReLU'):\n        if string == 'ReLU':\n            return ReLU_act\n        elif string == 'sigmoid':\n            return sigmoid_act\n        else :\n            return sigmoid_act","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Instantiating the class and Fitting the model\n\nNow we instantiate our model, that will be exactly the same as before, i.e. a two-hidden layer with 8 and 4 neurons respectively and with ReLU activation plus an OutPut layer with a single neuron with sigmoid activation. \n\nWe will set the learning rate and then fit the model; after that we recover the accuracy history (even in the averaged over 10 batches) and finally we compute the predictions."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = ANN()\n\nmodel.add(layers.layer(8, 'ReLU'))\nmodel.add(layers.layer(4, 'ReLU'))\nmodel.add(layers.layer(1, 'sigmoid'))\n\nmodel.set_learning_rate(0.8)\n\nmodel.Fit(X_train, Y_train)\nacc_val = model.get_accuracy()\nacc_avg_val = model.get_avg_accuracy()\n\npredictions = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We plot the accuracy stories we have retrieved:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nplt.scatter(np.arange(1, X_train.shape[0]+1), acc_val, alpha=0.3, s=4, label='mu')\nplt.title('Loss for each training data point', fontsize=20)\nplt.xlabel('Training data', fontsize=16)\nplt.ylabel('Loss', fontsize=16)\nplt.show()\n\nplt.figure(figsize=(10,6))\nplt.scatter(np.arange(1, len(acc_avg_val)+1), acc_avg_val, label='mu')\nplt.title('Averege Loss by epoch', fontsize=20)\nplt.xlabel('Training data', fontsize=16)\nplt.ylabel('Loss', fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And finally see how the system behaves as a classifier:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the confusion matrix\ncm = confusion_matrix(Y_test, predictions)\n\ndf_cm = pd.DataFrame(cm, index = [dict_live[i] for i in range(0,2)], columns = [dict_live[i] for i in range(0,2)])\nplt.figure(figsize = (7,7))\nsns.heatmap(df_cm, annot=True, cmap=plt.cm.Blues, fmt='g')\nplt.xlabel(\"Predicted Class\", fontsize=18)\nplt.ylabel(\"True Class\", fontsize=18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusions\nNotice that, having changed the random initialization following the He et al. procedure, we have improved the classification slightly (recall that this still is a very naive model). Now we can easily add more layers and, in the spirit of Convolutional Neural Network, we can easily modify the class to add for more generic layers like *Convolutional* or *Pooling* layers.\n\nWe can now try a more *deep* neural network for the case at hand:"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = ANN()\n\nmodel.add(layers.layer(24, 'ReLU'))\nmodel.add(layers.layer(12, 'sigmoid'))\nmodel.add(layers.layer(6, 'ReLU'))\nmodel.add(layers.layer(1, 'sigmoid'))\n\nmodel.set_learning_rate(0.8)\n\nmodel.Fit(X_train, Y_train)\nacc_val = model.get_accuracy()\nacc_avg_val = model.get_avg_accuracy()\n\nplt.figure(figsize=(10,6))\nplt.scatter(np.arange(1, len(acc_avg_val)+1), acc_avg_val, label='mu')\nplt.title('Averege Loss by epoch', fontsize=20)\nplt.xlabel('Training data', fontsize=16)\nplt.ylabel('Loss', fontsize=16)\nplt.show()\n\npredictions = model.predict(X_test)\n\n# Plot the confusion matrix\ncm = confusion_matrix(Y_test, predictions)\n\ndf_cm = pd.DataFrame(cm, index = [dict_live[i] for i in range(0,2)], columns = [dict_live[i] for i in range(0,2)])\nplt.figure(figsize = (7,7))\nsns.heatmap(df_cm, annot=True, cmap=plt.cm.Blues, fmt='g')\nplt.xlabel(\"Predicted Class\", fontsize=18)\nplt.ylabel(\"True Class\", fontsize=18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I hope you enjoyed it!\n\nI would love to receive your comments (good AND bad), so feel free to leave a comment! "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}