{"cells":[{"metadata":{},"cell_type":"markdown","source":"![](https://deepsense.ai/wp-content/uploads/2019/02/Keras-or-PyTorch.png)\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<h1 align=\"center\"> INTRODUCTION </h1>\n<hr>\nHello Kagglers! In this kernel I will talk about one of the most confusing questions that is, <b>Which Deep Learning Framework to use?</b> This questions usually comes to a beginner when he/she starts his/her journey in deep learning. Although it not only happens with beginners, sometimes others also get confused which to use. \n\nThere are many deep learning framework options available like:-\n* CNTK\n* TensorFlow\n* Keras\n* DL4J\n* PyTorch\n* Lasagne\n* Mxnet\n\nAnd many more......\n\nBut the most famous frameworks are TensorFlow, Keras and PyTorch. Out of these 3, here I will be comparing PyTorch and Keras in greater depth and proper examples. So let's get started.\n\n<font color='red'><h2>Please do an upvote if you find this kernel useful!</h2></font>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Table of Contents\n<ul style=\"list-style-type:square\">\n    <li><a href=\"#1\">Basic Information</a></li>\n    <li><a href=\"#2\">Factors for Comparison</a></li>\n    <li><a href=\"#4\">Importing Libraries</a></li>\n    <ul>\n        <li><a href=\"#4.1\">Importing Common Libraries</a></li>\n        <li><a href=\"#4.2\">Importing Keras Libraries</a></li>\n        <li><a href=\"#4.3\">Importing PyTorch Libraries</a></li>\n    </ul>\n    <li><a href=\"#5\">Comparison Based on ANN</a></li>\n     <ul>\n        <li><a href=\"#5.1\">Reading the Data</a></li>\n        <li><a href=\"#5.2\">Data Preparation</a></li>\n        <li><a href=\"#5.3\">ANN Model</a></li>\n        <li><a href=\"#5.4\">Optimizers</a></li>\n        <li><a href=\"#5.5\">Training</a></li>\n    </ul>\n    <li><a href=\"#6\">Comparison Based on CNN</a></li>\n     <ul>\n        <li><a href=\"#6.1\">Reading the Data</a></li>\n        <li><a href=\"#6.2\">Data Preparation</a></li>\n        <li><a href=\"#6.3\">CNN Model</a></li>\n        <li><a href=\"#6.4\">Optimizers</a></li>\n        <li><a href=\"#6.5\">Training</a></li>\n        <li><a href=\"#6.6\">Predicting on Test Set</a></li>\n    </ul>\n    <li><a href=\"#7\">When to use which?</a></li>\n    <li><a href=\"#8\">Bonus Part(Digit Recognizer)</a></li>\n    <li><a href=\"#9\">Ending Notes</a></li>\n</ul>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id='1'></a>\n# Basic Information\n\nBefore getting deeper into the comparison, it is very important to know some general information.\n\n<b><font size = \"4.5em\">Keras :-</font></b> Actually Keras is not a framework on it's own. It sits on top of TensorFlow, Theano or CNTK.\n<br>\n<b><font size = \"4.5em\">PyTorch :-</font></b> On the other hand, PyTorch is a Deep Learning Framework developed by Facebook's AI research group.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id='2'></a>\n# Factors for Comparison\n<b><h3>1. Level of API</h3></b>\nKeras has a high level of API as it is capable on running on top of the other frameworks. So it is easier to use and less code are needed.<br>\nPyTorch has a low level API and hence it becomes little difficult to use and more code is required for the similar task but it gives better control to the programmer.<br>\nYou will clearly see the difference in the examples below.\n\n<b><h3>2. Speed</h3></b>\nKeras has comparitively less speed and is much slower than PyTorch. PyTorch has a very high speed due to which it is generally suitable for large datasets.\n\n<b><h3>3. Debugging</h3></b>\nKeras has very less debugging capabilites but PyTorch has very high debugging capabilities. One reason for that is as in PyTorch you have to write each and every step so you know the code properly and can debug the error easily but this is not so in the case of Keras.\n\n<b><h3>4. Ease of code</h3></b>\nAs already told Keras has a high level API so we can do many jobs in just a single line of code and is very easy to read and understand.\nWhereas PyTorch consist many lines of code and it is comparitively difficult to understand.\n\n<b><h3>5. Community</h3></b>\nKeras has a small community support but the good news is you don't have to worry because there are lots of tutorials and solutions available in the net as many people start with the Keras only.\nPyTorch on the other hand has great community support and active development is going on.\n\n<b><h3>6. Popularity</h3></b>\nAlthough both these frameworks are very popular but still if we compare then Keras is much more popular than PyTorch.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://www.stickergiant.com/blog/wp-content/uploads/2017/10/Lets-Code-This-Home-Depot-Kiss-Cut-2017.jpg\" style = \"padding: 2% 0;width:50rem;height:25rem;border-radius:20%\">\nDon't worry if you don't get it because now we will dive deeper into the comparison and lets start coding to explain and verify each of the factors in detail.<br>\nHere I will be comparing these two frameworks by creating one <b>Artificial Neural Network(ANN) Model</b> and one <b>Convolutional Neural Network(CNN) Model</b>. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id='3'></a>\nIn this kernel I will be using data from Titanic: Machine Learning from Disaster competition and Digit Recognizer competition for comparsion but here I will not explain each and every thing which is required for this competiton like data pre processing steps, EDA, and the exact models, I will only explain those things more which needs for comparison. Also please don't consider these models as the final models for these competitions as these are just demo models for comparison and will not give high accuracy.<br>\nBut don't worry, I have created notebooks for these competitions which gives a high accuracy and all the steps are explained in great detail.\n<div class=\"row\">\n    <div class=\"col-sm-6\">\n    <div class = \"card\">\n      <div class = \"card-body\" style = \"width: 20rem; \">\n        <h5 class = \"card-title\"  style = \"font-size: 1.2em;\"align=\"center\">Full Explanation of CNN with high accuracy model</h5>\n          <img src=\"https://storage.googleapis.com/tfds-data/visualization/fig/mnist-3.0.1.png\" class = \"card_img-top\" style = \"padding: 2% 0;width:22rem;height:15rem;\"  alt=\"...\">\n        <p class=\"card-text\" style = \"font-size: 1.0em;text-align: center \"><b>Digit Recognizer: Detailed Explanation of CNN</b></p>\n        <a href = \"https://www.kaggle.com/utcarshagrawal/digit-recognizer-detailed-explanation-of-cnn\" class = \"btn btn-info btn-lg active\"  role = \"button\" style = \"color: white; margin: 0 15% 0 25%\" data-toggle = \"popover\" title = \"Click\">Click here</a>\n      </div>\n    </div>\n  </div>\n    <div class=\"col-sm-6\">\n      <div class=\"card\">\n        <div class=\"card-body\" style=\"width: 20rem;\">\n          <h5 class = \"card-title\"  style = \"font-size: 1.2em; \" align=\"center\" > Non Deep Learning Based method on Titanic Dataset </h5>\n            <img src = \"https://miro.medium.com/max/650/1*mzhf9OccFn7DeVHI9dPiLQ.jpeg\" class = \"card_img-top\" style = \" padding: 2% 0;width:22rem;height:15rem;\"  alt=\"...\">\n          <p class=\"card-text\" style = \"font-size: 1.0em;text-align: center \"><b>Titanic:Spark ML Magic + EDA & Feature Engineering</b></p>\n          <a href = \"https://www.kaggle.com/utcarshagrawal/titanic-spark-ml-magic-eda-feature-engineering\" class = \"btn btn-info btn-lg active\"  role = \"button\" style = \"color: white; margin: 0 15% 0 25%\" data-toggle = \"popover\" title = \"Click\">Click here</a>\n        </div>\n      </div>    \n    </div>\n   </div>    \n    ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Now the first step is to import the required libraries. Here I have separated libraries for Keras, PyTorch and some common libraries. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id = '4.1'></a> \n# Importing Common Libraries","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.preprocessing import StandardScaler\nimport numbers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = '4.2'></a> \n## Importing Keras Libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization\nfrom keras.optimizers import RMSprop, SGD\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.utils.np_utils import to_categorical","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = '4.3'></a> \n## Importing Pytorch Libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nfrom torchvision.utils import make_grid","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='5'></a>\n# 1. COMPARISON BASED ON ANN","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id='5.1'></a>\n# Reading the data\nThe next step is to read the data. As I told earlier I will be creating two models for comparison. The first is ANN Model for which I will be using Titanic: Machine Learning from Disaster competition data. Generally people prefer a non deep learning based approach for this dataset but deep learning based approach also works well.<br> \nFor non deep learning based approach of this competition [go here](#3).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = pd.read_csv(\"../input/titanic/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val = ['Pclass', 'Sex', 'Embarked', 'SibSp', 'Parch']\nplt.figure(figsize=(15,15))\nplt.subplots_adjust(right=1.5)\nfor i in range(5):\n    plt.subplot(2,3,i+1), sns.countplot(x=val[i], hue='Survived', data = df1)\n    plt.legend(['Not Survived', 'Survived'], loc='upper center', prop={'size': 10})\n    plt.title('Count of Survival in {} Feature'.format(val[i]), size=10, y=1.05)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### For more EDA of this dataset [click here](https://www.kaggle.com/utcarshagrawal/titanic-spark-ml-magic-eda-feature-engineering#Exploratory-Data-Analysis).","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id='5.2'></a>\n# Data Preparation\nBefore working on the model data preprocessing is one of the most important step. But preparing the data hugely depend on which type of framework you are using to create a model. There might be few steps which could be exactly same but there could be few different steps also.<br>\nAs we cannot directly apply this data to predict whether the person survived or not, so we will do some data preparation but I will not do all the steps because I am not here interested in increasing the accuracy.<br> \n### If you want to see all the feature engineering steps with explanations [click here](https://www.kaggle.com/utcarshagrawal/titanic-spark-ml-magic-eda-feature-engineering#Feature-Engineering).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sex Encoding\nbinar = LabelBinarizer().fit(df1.loc[:, \"Sex\"])\ndf1[\"Sex\"] = binar.transform(df1[\"Sex\"])\n# Embarked Encoding\ndf1[\"Embarked\"] = df1[\"Embarked\"].fillna('S')\ndf_Embarked = pd.get_dummies(df1.Embarked)\ndf1 = pd.concat([df1, df_Embarked], axis=1)\n#Family\ndf1['Family'] = df1['SibSp'] + df1['Parch'] + 1\ndf1['Alone'] = df1['Family'].apply(lambda x : 0 if x>1 else 1 )\n#Age\ndf1['Age'] = df1['Age'].fillna(-0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = [\"Pclass\",\"Sex\", \"Age\", \"C\", \"Q\", \"S\", \"Alone\"]\n\nX_train = df1[features].values\nY_train = df1[\"Survived\"].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature Scaling\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here in this dataset all the pre processing steps are same for both Keras and PyTorch model. So we don't have anything to compare here but this doesn't always happen. In my next example, you will see the difference.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id='5.3'></a>\n# ANN MODEL\nSo we have applied all the steps and now its time to create our model. Here you will observe the difference clearly. First I will make a Keras model and then a PyTorch model.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## ANN KERAS MODEL","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model1 = Sequential()\nmodel1.add(Dense(100, input_dim=7, activation=\"relu\"))\nmodel1.add(Dropout(0.5))\nmodel1.add(Dense(200, activation=\"relu\"))\nmodel1.add(Dropout(0.5))\nmodel1.add(Dense(1, activation=\"sigmoid\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here you can easily see how each and every step almost took just one line. This is the major advantage of Keras and if you know little bit about Keras models, then you will observe that it is not difficult to understand also. Now lets talk about PyTorch model.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## ANN PYTORCH MODEL","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class ANN(nn.Module):\n    def __init__(self):\n        super(ANN, self).__init__()\n        self.features = nn.Sequential(\n            nn.Linear(7,100),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p=0.5),\n            nn.Linear(100,200),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p=0.5),\n            nn.Linear(200,2),\n        )\n    def forward(self, x):\n        x = self.features(x)\n        \n        return x\n    \nmodel2 = ANN()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here you can observe the difference clearly. Keras on one side is more easily readable and understandable than PyTorch. Both the models are exactly same (Dense layer in Keras is same as Linear layer in PyTorch, just names are different) rest all the parameters are same but syntax of both the models are hugely different. Although for beginners PyTorch initially might be difficult but once you start understanding then it will become your favourite framework. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id='5.4'></a>\n# Optimizers","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#For Keras Model\n\noptimizer1 = SGD(lr = 0.01, momentum = 0.9)\n# Compiling our model\nmodel1.compile(optimizer = optimizer1, loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n#For PyTorch Model\n\n# Define the optimier\noptimizer2 = torch.optim.SGD(model2.parameters(), lr=0.01)\n# Define our loss function\ncriterion = nn.CrossEntropyLoss()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we are using SGD optimizer for both the models just the syntax is different. But in PyTorch we have to also define our loss function whereas in Keras we don't have to explicity define it but we write our loss function when we compile our Keras model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size=64\nepochs=15","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='5.5'></a>\n## So we have completed all the pre processes. Now its time to train our model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The major difference in the code comes here. Training a Keras model is just a one line step and it is very simple as all the things will be printed automatically like loss, accuracy etc. But in PyTorch it is a long method. The advantage of this is you will know your code properly and understand better what the system is doing in the backend. Also you have to give print command to whatever you want to print in your own way.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Keras Model\nmodel1.fit(X_train, Y_train, batch_size = batch_size, epochs = epochs, verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# PyTorch Model\nbatch_no = len(X_train) // batch_size\ntrain_loss_plot, train_acc_plot = [], []\ntrain_loss = 0\ntrain_loss_min = np.Inf\nfor epoch in range(epochs):\n    for i in range(batch_no):\n        start = i*batch_size\n        end = start+batch_size\n        x_ten = Variable(torch.FloatTensor(X_train[start:end]))\n        y_ten = Variable(torch.LongTensor(Y_train[start:end])) \n        \n        # Prevent accumulation of gradients\n        optimizer2.zero_grad()\n        # Make predictions\n        output = model2(x_ten)\n        loss = criterion(output,y_ten)\n        #backprop\n        loss.backward()\n        optimizer2.step()\n        \n        values, labels = torch.max(output, 1)\n        num_right = np.sum(labels.data.numpy() == Y_train[start:end])\n        train_loss += loss.item()*batch_size\n        \n    train_loss = train_loss / len(X_train)\n    train_loss_plot.append(train_loss)\n    train_acc_plot.append((num_right / len(Y_train[start:end])))\n    if train_loss <= train_loss_min:\n        print(\"Validation loss decreased ({:6f} ===> {:6f})\".format(train_loss_min,train_loss))\n        torch.save(model2.state_dict(), \"model.pt\")\n        train_loss_min = train_loss\n\n    print('')\n    print(\"Epoch: {} \\tTrain Loss: {} \\tTrain Accuracy: {}\".format(epoch+1, train_loss,num_right / len(Y_train[start:end]) ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics=pd.DataFrame(model1.history.history)\nplt.plot(metrics['loss'], label='Keras_loss')\nplt.plot(train_loss_plot, label='PyTorch_loss')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(metrics['accuracy'], label='Keras_acc')\nplt.plot(train_acc_plot, label='PyTorch_acc')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is the difference of PyTorch and Keras. Ease of code is one of the factor for choosing Keras but on the other hand PyTorch is very flexible. Here please don't consider the accuracy level as these are not great models and both of these can achieve better accuracy by changing the hyperparameters. Now let us talk about comparison based on CNN where we will cover some left over points.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id='6'></a>\n# 2. COMPARISON BASED ON CNN","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id='6.1'></a>\n# Reading the data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"As I have already told I will be using Digit Recognizer data for comparison based on CNN, so first lets read the data. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df2 = pd.read_csv('../input/digit-recognizer/train.csv')\ndf3 = pd.read_csv('../input/digit-recognizer/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='6.2'></a>\n# Data Preparation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### If you want to know the details of each and every step in Keras Framework [go here](#3).","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Data Preparation for Keras Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_train = df2[\"label\"]\nX_train = df2.drop(labels = [\"label\"],axis = 1).values\n\nX_train = X_train/255.0\nX_test = df3/255.0\n\nX_train = X_train.reshape(-1,28,28,1)\nX_test = X_test.values.reshape(-1,28,28,1)\n\nY_train = to_categorical(Y_train, num_classes = 10)\n\nX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1, random_state=35)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Preparation for Pytorch Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"targets_np = df2.label.values\nfeatures_np = df2.loc[:, df2.columns != 'label'].values/255\ntest_np = df3.values/255\n\nfeatures_np = features_np.reshape(-1,1,28,28)\ntest_np = test_np.reshape(-1,1,28,28)\nfake_labels = np.zeros(test_np.shape)\nfake_labels = torch.from_numpy(fake_labels)\n\nfeatures_train, features_test, target_train, target_test = train_test_split(features_np, targets_np, test_size=0.1, random_state=35)\n\nfeaturesTrain = torch.from_numpy(features_train)\ntargetsTrain = torch.from_numpy(target_train)\n\nfeaturesTest = torch.from_numpy(features_test)\ntargetsTest = torch.from_numpy(target_test)\n\ntest_data = torch.from_numpy(test_np)\n\ntrain = torch.utils.data.TensorDataset(featuresTrain, targetsTrain)\ntest = torch.utils.data.TensorDataset(featuresTest, targetsTest)\nsubmission_data = torch.utils.data.TensorDataset(test_data, fake_labels)\n\ntrain_loader = torch.utils.data.DataLoader(train, batch_size = batch_size, shuffle = True)\ntest_loader = torch.utils.data.DataLoader(test, batch_size = batch_size, shuffle = True)\nsubmission_loader = torch.utils.data.DataLoader(submission_data, batch_size = batch_size, shuffle = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Previously, you saw that the preprocessing steps was same for both the frameworks. But here this is not so. Initially, normalizing, reshaping and splitting the data are same for both but afterthat we have huge difference. In PyTorch we can't apply the numpy data directly but first we have to convert it in tensor then we make a tensor dataset and then combines a dataset and a sampler, and provides single- or multi-process iterators over the dataset.   ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id='6.3'></a>\n# **CNN MODEL COMPARISON**\nAs you saw the difference in ANN Model and now you will clearly see the difference in the CNN Model also.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## KERAS MODEL\n### For detailed explanation of each layer and to know exactly what each layer does in the backend and also to know the perfect model to get high score [go here](#3).","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"model3 = Sequential()\n\nmodel3.add(Conv2D(filters=32, kernel_size=(5,5), padding='Same', activation='relu', input_shape=(28,28,1)))\nmodel3.add(BatchNormalization())\nmodel3.add(Conv2D(filters=32, kernel_size=(5,5), padding='Same', activation='relu'))\nmodel3.add(BatchNormalization())\nmodel3.add(MaxPool2D(pool_size=(2,2)))\nmodel3.add(Dropout(0.25))\n\nmodel3.add(Flatten())\nmodel3.add(Dense(256, activation='relu'))\nmodel3.add(BatchNormalization())\nmodel3.add(Dropout(0.5))\nmodel3.add(Dense(10, activation='softmax'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## PYTORCH MODEL","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class CNN(nn.Module):\n    def __init__(self):\n        super(CNN, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=(5,5), stride=1, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(32, 32, kernel_size=(5,5), stride=1, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n        )\n          \n        self.classifier = nn.Sequential(\n            nn.Dropout(p = 0.5),\n            nn.Linear(32 * 12 * 12, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p = 0.5),\n            nn.Linear(512, 10),\n        )\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        \n        return x\n\nmodel4 = CNN()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='6.4'></a>\n# Optimizer","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#For Keras Model\n\noptimizer3 = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n# Compiling our model\nmodel3.compile(optimizer = optimizer3 , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n\n#For PyTorch Model\n\n# Define the optimier\noptimizer4 = optim.RMSprop(model4.parameters(), lr=0.001)\n# Define our loss function\ncriterion = nn.CrossEntropyLoss()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='6.5'></a>\n## So we have completed all the pre processes. Now its time to train our model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"As I previously told how this step has a huge difference. But here you will observe one difference that shows the flexibility of PyTorch and that is \"cuda()\". If we want a particular computation to be performed on the GPU, we can instruct PyTorch to do so by calling cuda() on our data structures (tensors). So this way we can apply our model on large datasets and the model can be trained on GPU very fastly.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model3.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs, validation_data = (X_val, Y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"if torch.cuda.is_available():\n    model4 = model4.cuda()\n    criterion = criterion.cuda()\n    \nmodel4.train()\n\nsteps = 0\nprint_every = 50\ntrain_losses, test_losses = [], []\n\nfor e in range(epochs):\n    running_loss = 0\n    for images, labels in train_loader:\n        steps += 1\n        if torch.cuda.is_available():\n            images = images.cuda()\n            labels = labels.cuda()\n        # Prevent accumulation of gradients\n        optimizer4.zero_grad()\n        # Make predictions\n        log_ps = model4(images.float())\n        loss = criterion(log_ps, labels.long())\n        #backprop\n        loss.backward()\n        optimizer4.step()\n        \n        running_loss += loss.item()\n        if steps % print_every == 0:\n            test_loss = 0\n            accuracy = 0\n\n            # Turn off gradients for validation\n            with torch.no_grad():\n                model4.eval()\n                for images, labels in test_loader:\n                    if torch.cuda.is_available():\n                        images = images.cuda()\n                        labels = labels.cuda()\n                    log_ps = model4(images.float())\n                    test_loss += criterion(log_ps, labels.long())\n\n                    ps = torch.exp(log_ps)\n                    # Get our top predictions\n                    top_p, top_class = ps.topk(1, dim=1)\n                    equals = top_class == labels.view(*top_class.shape)\n                    accuracy += torch.mean(equals.type(torch.FloatTensor))\n\n    train_losses.append(running_loss/len(train_loader))\n    test_losses.append(test_loss/len(test_loader))\n\n    print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n          \"Training Loss: {:.4f}.. \".format(train_losses[-1]),\n          \"Test Loss: {:.4f}.. \".format(test_losses[-1]),\n          \"Test Accuracy: {:.4f}\".format(accuracy/len(test_loader)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics2=pd.DataFrame(model3.history.history)\nplt.plot(metrics2['loss'], label='Keras_loss')\nplt.plot(train_losses, label='PyTorch_loss')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(metrics2['val_loss'], label='Keras_val_loss')\nplt.plot(test_losses, label='PyTorch_val_loss')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='6.6'></a>\n# Predicting on test set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"results = model3.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_prediction(data):\n    images, labels = next(iter(data))\n    if torch.cuda.is_available():\n        images = images.cuda()\n        labels = labels.cuda()\n\n    img = images\n    # Turn off gradients to speed up this part\n    with torch.no_grad():\n        logps = model4(images.float())\n\n    ps = torch.exp(logps)\nmake_prediction(submission_loader)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Similar to the previous model, here also you can observe the difference of ease of code and simplicity. But in this case you can also observe the flexibility and the speed of PyTorch model. Again here I am saying please don't compare the accuracy as these are not perfect models and we can achieve very high accuracy using any of the frameworks. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id='7'></a>\n# When to use which?\nNow the last thing which is left is when to use which framework.<br>\nAlthough after comparing so many things we still can't say which is better. Each one have its pros and cons and it ultimately depends on your need and expectations.Some suitable situations to use:-\n<ul>\n    <li><h2>Keras:-</h2></li>\n    <ol>\n        <li>Quick Prototype</li>\n        <li>Small dataset</li>\n        <li>Best for begineers</li>\n    </ol>\n    <li><h2>PyTorch:-</h2></li>\n    <ol>\n        <li>Flexibility</li>\n        <li>Large dataset</li>\n        <li>Fast Speed</li>\n        <li>Debugging Capabilities</li>\n    </ol>\n<ul>\n    \n        ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id='8'></a>\n# Bonus Part(Digit Recognizer)\nTo achieve a very high score of this Digit Recognizer competition, if you want to use Keras framework then [go here](#3). For PyTorch Model I have not created any notebook so I have written the code here:-","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class CNN_2(nn.Module):\n    def __init__(self):\n        super(CNN, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n          \n        self.classifier = nn.Sequential(\n            nn.Dropout(p = 0.5),\n            nn.Linear(64 * 7 * 7, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p = 0.5),\n            nn.Linear(512, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p = 0.5),\n            nn.Linear(512, 10),\n        )\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        \n        return x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='9'></a>\n# Ending Notes\nSo here we come to an end to a long comparison between the two most famous deep learning frameworks. I hope now you will be able to select the correct framework according to your need. One more thing I want to say is that I am not an expert in PyTorch so if I have done any mistake in PyTorch code or anywhere else also please comment below so that I can correct it. <br>\n\nI hope that this notebook will not only help you for comparison but also it will help you in the coding part for both the Keras and PyTorch users. <br>\n\nAs I already told for much detailed information for non deep learning based method on Titanic dataset and for CNN explanation using Keras you can refer my other two notebooks, [go here](#3).<br>\n\nAlso if you have any doubt you can comment below and I will definately try to help you.<br>\n\nAlso, please join this discussion and tell your views which framework do you prefer and why:- <a href = \"https://www.kaggle.com/getting-started/175487\" class = \"btn btn-info btn-lg active\"  role = \"button\" style = \"color: white;\" data-toggle = \"popover\" title = \"Click\">Click here</a>\n\n<font color='red'><h3>Thanks a lot for having a look at this notebook.<br> I would like to get an appreciation from you with an upvote. Please upvote and give review if you liked the kernel.</h3></font>","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}