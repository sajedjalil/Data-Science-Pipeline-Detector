{"cells":[{"metadata":{"_uuid":"0ce8368deb46a77d2115dbc3d37859e8aa20c953","_cell_guid":"c986a9fb-e199-40eb-947d-c398d54f6b1e"},"cell_type":"markdown","source":"# EDA To Prediction (DieTanic)\n\n**예측을 위한 탐색적 데이터 분석**"},{"metadata":{},"cell_type":"markdown","source":"* Competition / Dataset : [Titanic: Machine Learning from Disaster](http://https://www.kaggle.com/c/titanic)\n* Date : 10. Aug. 2019 \n* Writer : @[Ashwini Swain](https://www.kaggle.com/ash316)\n* Link : https://www.kaggle.com/ash316/eda-to-prediction-dietanic\n* Translator : @[Isaac_Lee](http://https://www.kaggle.com/isaaclys)"},{"metadata":{},"cell_type":"markdown","source":"***Translator's words / 역자의 말***\n\nI started to translate the kernels written by other person to Korean to study Data Science. Some words can be interpreted randomly in Korean, and there may be some differences from common terms. Please comment if there is such a thing. And please **UPVOTE!** It will motivates me a lot! **Thank You!**\n\n저도 공부할겸 다른분께서 쓰신 커널을 한국어로 번역하기를 시작했습니다. 부족한 가독성과 오역을 너그럽게 이해해주시고, 특정 단어들은 제 임의로 한국어로 번역을 해서 일반적으로 사용하는 용어와 다른 것이 있을 수 있습니다. 그런 것이 있다면 코멘트 해주세요. 그리고 **UPVOTE!** 부탁드려요. 많은 동기부여가 됩니다. 감사합니다!"},{"metadata":{"_uuid":"925765e573c2665df48f766467ed75eaab81190c","_cell_guid":"0bef0e9b-81e0-4737-b972-9cb8a06b6b63"},"cell_type":"markdown","source":"The sinking of the Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. That's why the name **DieTanic**.  This is a very unforgetable disaster that no one in the world can forget.\n- 타이타닉 호의 침몰은 역사상 가장 악명높은 난파선 사건중 하나였습니다. 1912년 4월 15일 첫 항해에서 타이타닉은 빙하에 부딪혀서 승객과 승무원 2224명 중 1502명을 죽였습니다. DieTanic이라는 이름의 이유입니다. 이 사건은 전 세계 사람들이 잊을 수 없는 사건입니다.\n\nIt took about $7.5 million to build the Titanic and it sunk under the ocean due to collision. The Titanic Dataset is a very good dataset for begineers to start a journey in data science and participate in competitions in Kaggle.\n\n- 타이타닉을 건조 비용 750만 달러는 충돌로 인해 바다 밑에 가라 앉았습니다. 타이타닉 데이터는 데이터 사이언스 입문자가 캐글 competition 여행을 시작하기 좋은 자료입니다.\n\nThe Objective of this notebook is to give an **idea how is the workflow in any predictive modeling problem**. How do we check features, how do we add new features and some Machine Learning Concepts. I have tried to keep the notebook as basic as possible so that even newbies can understand every phase of it.\n\n- 이 노트북의 목적은 어떠한 흐름으로 예측 모델링 문제를 풀어가는지 아이디어를 주기 위함입니다. 어떻게 피처(특징)을 확인하고, 어떻게 새로운 피처와 머신러닝(기계학습) 개념을 추가할까요? 저는 이 노트북을 가능한 쉽게 작성하여 한줄한줄 모두 입문자들도 이해할 수 있게 했습니다.\n\nIf You Like the notebook and think that it helped you..**PLEASE UPVOTE**. It will keep me motivated.\n\n- 만약 도움이 되셨다면...UPVOTE 해주시면 감사하겠습니다. 이는 저의 동기부여가 될 것입니다."},{"metadata":{"_uuid":"8fa2571b91b93e0b0a08b7a9e4eedc060ba76c20","_cell_guid":"706b0b7c-19f4-41c9-865f-5b3375253e0a"},"cell_type":"markdown","source":"## Contents of the Notebook: 노트북의 목차\n\n\n#### Part1: Exploratory Data Analysis(EDA): 탐색적 데이터 분석\n1)Analysis of the features. \n\n- 1)피처 분석하기\n\n\n2)Finding any relations or trends considering multiple features.\n\n- 2)복수 피처를 고려한 연관 관계나 동향 찾기\n\n\n#### Part2: Feature Engineering and Data Cleaning: 피처 엔지니어링과 데이터 클리닝\n1)Adding any few features.\n\n- 1)몇개의 피처를 더하기\n\n\n2)Removing redundant features.\n\n- 2)남는 피처를 지우기\n\n\n3)Converting features into suitable form for modeling.\n\n- 3)각 피처를 모델링(머신러닝 모델링)하기 좋게 변환하기\n\n\n#### Part3: Predictive Modeling: 예측 모델링\n1)Running Basic Algorithms.\n\n- 1)기본적인 알고리즘 실행하기\n\n\n2)Cross Validation.\n\n- 2)교차 검증하기\n\n\n3)Ensembling.\n\n- 3)앙상블하기(쉽게말해 여러 모델을 합치기)\n\n\n4)Important Features Extraction.\n\n- 4)중요한 피처 추출하기"},{"metadata":{"_uuid":"18ba4a8f0909fd0a758b8cb8717327de8aeacdc8","_cell_guid":"bf5980c3-b168-4a26-81f3-c7bdcedb6429"},"cell_type":"markdown","source":"## Part1: Exploratory Data Analysis(EDA) / 탐색적 데이터 분석"},{"metadata":{"_uuid":"7bb401b4e2e509cc8a53e9cf645226fa508fa2e2","_cell_guid":"d7601bd6-d22f-499f-97b9-85e01d390f05","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('fivethirtyeight')\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ebc88acccbc14bf28e4e4d49eee1ea82dcc31ab4","_cell_guid":"c12ac199-e3e3-4372-a747-baa6273f4561","trusted":true},"cell_type":"code","source":"data=pd.read_csv('../input/train.csv') ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6bad00c321363076e535c13a1ad70026c17042f3","_cell_guid":"03e86158-c720-48f7-8dde-ee11f79b7893","trusted":true},"cell_type":"code","source":"data.head()  ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1496190095fb1cd2e289c63c986c6eb951046860","_cell_guid":"5ef569cd-e99e-42f0-93ec-abbc6a90c00e","trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fcedf70d9bdab89fb7eb8ff3769b14f0b5036a33","_cell_guid":"31972ab9-edef-49e3-bcd5-120262cf00d8"},"cell_type":"markdown","source":"The **Age, Cabin and Embarked** have null values. I will try to fix them.\n\n나이, 선실 그리고 어디서 탑승했는지에 Null이 있습니다. 제가 조금 고쳐보겠습니다."},{"metadata":{"_uuid":"d433fbf891d9268f60bf395d7db4e61996989d04","_cell_guid":"841dc40d-06b4-4010-b996-8d1e23857341"},"cell_type":"markdown","source":"### How many Survived?? / 얼마나 많이 살아남았나??"},{"metadata":{"_uuid":"c60257aef24e867113873729829c7a1e33f4a0ab","_cell_guid":"fabb7625-a8ef-4f37-99c6-3ec93679ef1f","trusted":true},"cell_type":"code","source":"f,ax=plt.subplots(1,2,figsize=(18,8))\ndata['Survived'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)\nax[0].set_title('Survived')\nax[0].set_ylabel('')\nsns.countplot('Survived',data=data,ax=ax[1])\nax[1].set_title('Survived')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"11fa0236baf6d291f1a1f9325cc42d71fc1b61c1","_cell_guid":"5be22cbd-9b03-4e9f-8eaf-068dd9df401f"},"cell_type":"markdown","source":"It is evident that not many passengers survived the accident. \n- 이것은 사고에서 많은 사람들이 살아남지 못했다는 증거입니다.\n\nOut of 891 passengers in training set, only around 350 survived i.e Only **38.4%** of the total training set survived the crash. We need to dig down more to get better insights from the data and see which categories of the passengers did survive and who didn't.\n\n- 891명의 훈련 데이터들 중에 350명정도, 즉 전체에서 **38.4%** 가 사고에서 살아남았습니다. 우리는 더 나은 인사이트를 위해서 어떤 부류의 사람들이 살아남았고, 그렇지 못했는지 파고들것입니다.\n\nWe will try to check the survival rate by using the different features of the dataset. Some of the features being Sex, Port Of Embarcation, Age,etc.\n\n- 우리는 생존비율을 다른 피처인 성별, 탑승지 나이 등으로 확인할 것입니다.\n\nFirst let us understand the different types of features.\n\n- 먼저 우리는 각기 다른 유형의 피처를 살펴봅시다."},{"metadata":{},"cell_type":"markdown","source":"## Types Of Features / 피처 타입\n\n### Categorical Features: 카테고리 피처\n\nA categorical variable is one that has two or more categories and each value in that feature can be categorised by them. For example, gender is a categorical variable having two categories (male and female). Now we cannot sort or give any ordering to such variables. They are also known as **Nominal Variables**.\n\n- 카테고리 피처는 2개 이상의 카테고리로 각각의 값들을 분류 가능한 피처입니다. 예를 들어 성별은 카테고리 피처로 2개의 카테고리(남자, 여자)를 갖고 있습니다. 명사 피처라고도 불리는 이 피처는 정렬을 하거나 어떤 순서를 부여 할 수 없습니다.\n\n**Categorical Features in the dataset: Sex(성별),Embarked(탑승지).** \n\n### Ordinal Features: 순서 피처\nAn ordinal variable is similar to categorical values, but the difference between them is that we can have relative ordering or sorting between the values. For eg: If we have a feature like **Height** with values **Tall, Medium, Short**, then Height is a ordinal variable. Here we can have a relative sort in the variable.\n\n- 순서 피처는 카테고리 피처와 비슷합니다. 다만 카테고리 피처와 다르게 상대적인 순서를 부여하거나 정렬 할 수 있습니다. 예를 들어 크다, 중간이다, 작다의 값을 가진 \"키\"피처가 있다면 \"키\"는 순서 피처리고 할 수 있습니다. 이때 우리는 변수들을 상대적으로 정렬할 수 있습니다.\n\n**Ordinal Features in the dataset: PClass(일등석인지 이등석인지 등)**\n\n### Continous Feature: 연속 피처\nA feature is said to be continous if it can take values between any two points or between the minimum or maximum values in the features column.\n\n- 어떤 피처가 연속이라면 두지점 사이 또는 최댓값과 최소값 사이에서 값을 하나 정할 수 있습니다.\n\n**Continous Features in the dataset: Age(나이)**"},{"metadata":{"_uuid":"2b36f7862279cf64a76a9950f703bfed4ca220f6","_cell_guid":"ccd13018-e5fb-4022-ac41-cadce1994dbe"},"cell_type":"markdown","source":"## Analysing The Features / 피처 분석"},{"metadata":{"_uuid":"8b5ad1ae98e4aad980f24bbefb489e6ac049768b","_cell_guid":"8d5bd219-61ce-4c88-b0c5-aaffce8cb1cc"},"cell_type":"markdown","source":"## Sex --> Categorical Feature\n## 성별 --> 카테고리 피처"},{"metadata":{"_uuid":"3554e468c8581316a717348689f1d867b3c97f6a","_cell_guid":"428c84fc-9d5e-4022-a9f5-1c8ec7257268","trusted":true},"cell_type":"code","source":"data.groupby(['Sex','Survived'])['Survived'].count()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c06e043424e13b87fcb020322e4869430fd0714f","_cell_guid":"06218a7d-bf3c-40b1-9cfa-2a915f7bc005","trusted":true},"cell_type":"code","source":"f,ax=plt.subplots(1,2,figsize=(18,8))\ndata[['Sex','Survived']].groupby(['Sex']).mean().plot.bar(ax=ax[0])\nax[0].set_title('Survived vs Sex')\nsns.countplot('Sex',hue='Survived',data=data,ax=ax[1])\nax[1].set_title('Sex:Survived vs Dead')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ebf3e75bbc120054b947162e088876bbaca3cf54","_cell_guid":"d97e7fe8-a98c-40f3-98c7-b5118c0295db"},"cell_type":"markdown","source":"This looks interesting. The number of men on the ship is lot more than the number of women. Still the number of women saved is almost twice the number of males saved. The survival rates for a **women on the ship is around 75% while that for men in around 18-19%.**\n- 흥미로운 결과 입니다. 배에 탑승한 남자의 숫자는 여자 보다 많습니다. 그러나 생존한 여자의 숫자는 생존한 남자의 숫자의 두배입니다. 여자의 생존 비율은 75%이지만, 남자는 18~19%밖에 되지 않습니다.\n\nThis looks to be a **very important** feature for modeling. But is it the best??   Lets check other features.\n- 이는 모델링을 할때 매우 중요해 보입니다. 하지만 최선일까요?? 다른 피처들도 살펴봅시다."},{"metadata":{"_uuid":"e3b6327723dedd766d452f55b4056bbd0b37bed2","_cell_guid":"a210b0c8-dd8e-4fd0-a7f2-e597b7ed81e6"},"cell_type":"markdown","source":"## Pclass --> Ordinal Feature\n## Pclass --> 순서피처"},{"metadata":{"_uuid":"4a98fe27c4474296c6b51f4a2b7fb076c228b4b8","_cell_guid":"2477b536-32dd-43a0-8824-be034104b760","trusted":true},"cell_type":"code","source":"pd.crosstab(data.Pclass,data.Survived,margins=True).style.background_gradient(cmap='summer_r')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"592a3d8c24761c3f6e8c5cf875554826d9e308a5","_cell_guid":"c3adaaa2-f675-4273-ba93-8b26c37bacf3","trusted":true},"cell_type":"code","source":"f,ax=plt.subplots(1,2,figsize=(18,8))\ndata['Pclass'].value_counts().plot.bar(color=['#CD7F32','#FFDF00','#D3D3D3'],ax=ax[0])\nax[0].set_title('Number Of Passengers By Pclass')\nax[0].set_ylabel('Count')\nsns.countplot('Pclass',hue='Survived',data=data,ax=ax[1])\nax[1].set_title('Pclass:Survived vs Dead')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ace6b99b7d75b76ecead6f03372c1790fe3aa6c7","collapsed":true,"_cell_guid":"eec00d94-07d2-4e6e-8797-cb6f98341793"},"cell_type":"markdown","source":"People say **Money Can't Buy Everything**. But we can clearly see that Passenegers Of Pclass 1 were given a very high priority while rescue. Even though the the number of Passengers in Pclass 3 were a lot higher, still the number of survival from them is very low, somewhere around **25%**.\n- 사람들은 돈이 모든 것을 살 수 없다고 합니다. 하지만 우리는 1등석에 탑승한 승객들은 매우 높은 우선 순위로 구조되었다는 것을 볼 수 있습니다. 3등석의 승객수가 더 많았지만 생존한 승객은 25%정도로 매우 낮았습니다.\n\nFor Pclass 1 %survived is around **63%** while for Pclass2 is around **48%**. So money and status matters. Such a materialistic world.\n- 1등석은 63% 정도가 생존 했고, 2등석은 48%정도 생존 했습니다. 돈과 생존상태에는 관련이 있습니다. 이런 물질주의적인 세상!(역자: 느낌표는 제가 넣었습니다.)\n\nLets Dive in little bit more and check for other interesting observations. Lets check survival rate with **Sex and Pclass** Together.\n- 또 다른 흥미로운 관찰 결과가 있는지 더 들어가 봅시다. 성별과 Pclass를 같이 확인해 봅시다."},{"metadata":{"_uuid":"1308ec5a68849984dfd1e05b53c52b6192363a18","_cell_guid":"7d413d16-2861-4aca-9042-38e374eddef3","trusted":true},"cell_type":"code","source":"pd.crosstab([data.Sex,data.Survived],data.Pclass,margins=True).style.background_gradient(cmap='summer_r')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"710111beaace27f0e85958a0639f2b2175b0892c","_cell_guid":"1fd41001-f153-4a78-806b-72b16a34f88f","trusted":true},"cell_type":"code","source":"sns.factorplot('Pclass','Survived',hue='Sex',data=data)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1dbe07636bbd6d44fb0854c9c9cec03529117042","_cell_guid":"7ae5251a-8bd9-4638-85c8-b0e0ed91420e"},"cell_type":"markdown","source":"We use **FactorPlot** in this case, because they make the seperation of categorical values easy.\n- 우리는 이번 케이스에서 FactorPlot을 이용했는데, 카테고리 값들의 분리를 잘 보여주기 때문입니다.\n\nLooking at the **CrossTab** and the **FactorPlot**, we can easily infer that survival for **Women from Pclass1** is about **95-96%**, as only 3 out of 94 Women from Pclass1 died. \n- Cross Tab과 FactorPlot을 보면 우리는 1등석의 여자들은 94명중 3명만 생존하지 못해 약 95~96%가 생존한것을 볼 수 있다.\n\nIt is evident that irrespective of Pclass, Women were given first priority while rescue. Even Men from Pclass1 have a very low survival rate. \n- 이것은 Pclass에 상관없이 여자는 구조에서 우선 순위를 차지한다는 증거가 됩니다. 1등석의 남자도 여전히 생존 비율은 낮습니다.\n\nLooks like Pclass is also an important feature. Lets analyse other features.\n- Pclass도 중요한 피처인것 같군요. 다른 피처들도 살펴 봅시다."},{"metadata":{"_uuid":"da1710d88cdb726d4c74d1580eb4650823f8e1a9","_cell_guid":"b9a8739f-9bfa-48a0-8b55-85694f8c7b36"},"cell_type":"markdown","source":"## Age --> Continous Feature\n## 나이 --> 연속 피처\n"},{"metadata":{"_uuid":"58e1110e104a4628f2852fa284905525997c2c44","_cell_guid":"d8c1dc5a-2f74-4c88-9101-6c98abaf9878","trusted":true},"cell_type":"code","source":"print('Oldest Passenger was of:',data['Age'].max(),'Years')\nprint('Youngest Passenger was of:',data['Age'].min(),'Years')\nprint('Average Age on the ship:',data['Age'].mean(),'Years')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a43082dc51717bbcc0a036db2d78b7732eb41ef1","_cell_guid":"cf3e9729-799e-4142-84ff-0dbb6906136e","trusted":true},"cell_type":"code","source":"f,ax=plt.subplots(1,2,figsize=(18,8))\nsns.violinplot(\"Pclass\",\"Age\", hue=\"Survived\", data=data,split=True,ax=ax[0])\nax[0].set_title('Pclass and Age vs Survived')\nax[0].set_yticks(range(0,110,10))\nsns.violinplot(\"Sex\",\"Age\", hue=\"Survived\", data=data,split=True,ax=ax[1])\nax[1].set_title('Sex and Age vs Survived')\nax[1].set_yticks(range(0,110,10))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"33c3ffbed55b33a51666b95a46c9f8aaa634bb73","_cell_guid":"b6f98f68-46c3-4aac-a4c7-ef350bf8ccd7"},"cell_type":"markdown","source":"#### Observations: 관찰결과\n\n1)The number of children increases with Pclass and the survival rate for passenegers below Age 10(i.e children) looks to be good irrespective of the Pclass.\n- 1) 어린이의 숫자는 Pclass가 높아질수록(3등석에 가까울 수록) 높아졌고, 10살 이하의 승객들(예: 어린이)의 생존 비율은 Pclass와 상관 없다는 것을 잘 보여줌.\n\n2)Survival chances for Passenegers aged 20-50 from Pclass1 is high and is even better for Women.\n- 2) 1등석에 탑승한 20~50살의 승객들의 생존 기회는 매우 높았는데 이는 여자일 수록 더 높음.\n\n3)For males, the survival chances decreases with an increase in age.\n- 3) 남자들은 나이가 많을 수록 생존 기회가 낮아짐."},{"metadata":{"_uuid":"d347a55f54b1ee15ce39ad0e22d873a2c32fc736","_cell_guid":"23be39a5-be98-422b-8116-90cb0fd120ba"},"cell_type":"markdown","source":"As we had seen earlier, the Age feature has **177** null values. To replace these NaN values, we can assign them the mean age of the dataset.\n- 우리가 앞서 봤듯이 나이값에는 177개의 Null값이 있습니다. 우리는 나이 데이터의 평균값으로 Null값을 대체 할 수 있습니다.\n\n\nBut the problem is, there were many people with many different ages. We just cant assign a 4 year kid with the mean age that is 29 years. Is there any way to find out what age-band does the passenger lie??\n- 그러나 문제가 있습니다. 많은 사람이 다양한 나이를 가지고 있습니다. 우리가 4살의 아이를 29살이라고 작성할 수는 없습니다. 어떠한 나이대에 승객이 속해있는지 아는 방법이 없을까요?\n\n\n**Bingo!!!!**, we can check the **Name**  feature. Looking upon the feature, we can see that the names have a salutation like Mr or Mrs. Thus we can assign the mean values of Mr and Mrs to the respective groups.\n- 옳거니!(역자: 개그포인트 입니다.ㅎㅎ) 우리는 이름 피처를 살펴 볼 수 있습니다. 우리가 이름 피처를 볼 때 우리는 Mr나 Mrs와 같은 이니셜이 붙는 것을 알 수 있습니다. 따라서 Mr와 Mrs의 평균값을 각 그룹에 할당 할 수 있습니다.\n\n**''What's In A Name??''**---> **Feature**  :p\n- **이름에는 뭐가 있죠??**---> **\"피처\"**  :P"},{"metadata":{"_uuid":"e92f7a0ef7ea07abb81e602fc700b382c64fca96","_cell_guid":"0af8b99e-0d4b-4844-a146-da2a359ca2ff","trusted":true},"cell_type":"code","source":"data['Initial']=0\nfor i in data:\n    data['Initial']=data.Name.str.extract('([A-Za-z]+)\\.') #lets extract the Salutations","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"413c7cdc7469c86bd9d13fd54c731ecb58704197","_cell_guid":"8efd06d8-dee6-4892-a986-25944cf2bf61"},"cell_type":"markdown","source":"Okay so here we are using the Regex: **[A-Za-z]+)\\.**. So what it does is, it looks for strings which lie between **A-Z or a-z** and followed by a **.(dot)**. So we successfully extract the Initials from the Name.\n- 우리는 여기서 명령어 **[A-Za-z]+)\\.**를 사용합니다. 이 명령어가 하는 것은 **A-Z** **a-z**의 문자열 중에 **.(마침표)**가 뒤에 붙어있는 것들을 찾습니다. 우리는 성공적으로 그 문자열들의 이니셜을 추출할 수 있습니다."},{"metadata":{"_uuid":"cf32ee39ef64840facd833f7f5d7616ffbaaa97c","_cell_guid":"e87e0415-43e7-4717-a6dc-43e60e71460e","trusted":true},"cell_type":"code","source":"pd.crosstab(data.Initial,data.Sex).T.style.background_gradient(cmap='summer_r') #Checking the Initials with the Sex","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f0514c0b00c34f72bd77eb597a4e08a1c9edf982","_cell_guid":"1d7d94e4-240e-4cfc-ba21-d036b3bd7869"},"cell_type":"markdown","source":"Okay so there are some misspelled Initials like Mlle or Mme that stand for Miss. I will replace them with Miss and same thing for other values.\n- 여기에는 Mlle와 Mme과 같이 Miss를 잘못 표기한 것들이 있습니다. 저는 그것을 다른 Miss로 바꿔 줄겁니다. 다른 값들도 마찬가지로 해줍니다."},{"metadata":{"_uuid":"99a86205c88ad2c8fd96fc18225cd10ed91620dd","_cell_guid":"55b6028e-948c-4a98-a214-e86212481af4","trusted":true},"cell_type":"code","source":"data['Initial'].replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don'],['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr'],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f09f5aefc80a89f0f3af69cd9fdf30afb576f456","_cell_guid":"c1d0c1dd-ac10-4360-99e1-dfea7418ad0e","trusted":true},"cell_type":"code","source":"data.groupby('Initial')['Age'].mean() #lets check the average age by Initials","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d2bbefba6442fcd47e04c90daa43f58fc001e47b","_cell_guid":"57ec5300-f0e3-46ce-a920-6c1846901b6d"},"cell_type":"markdown","source":"### Filling NaN Ages\n### Null 나이값을 채워주기"},{"metadata":{"_uuid":"8bd3c34f7f539bc3d4720531da6405e2d0e96b46","_cell_guid":"f006b4b0-a8aa-432c-9bdb-040a435e77f8","trusted":true},"cell_type":"code","source":"## Assigning the NaN Values with the Ceil values of the mean ages\ndata.loc[(data.Age.isnull())&(data.Initial=='Mr'),'Age']=33\ndata.loc[(data.Age.isnull())&(data.Initial=='Mrs'),'Age']=36\ndata.loc[(data.Age.isnull())&(data.Initial=='Master'),'Age']=5\ndata.loc[(data.Age.isnull())&(data.Initial=='Miss'),'Age']=22\ndata.loc[(data.Age.isnull())&(data.Initial=='Other'),'Age']=46","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0fd9e749a4eefeb0b57c0fe97de9b0ee9815c279","_cell_guid":"534ab487-2e49-4df1-93b4-4aac64c52bc1","trusted":true},"cell_type":"code","source":"data.Age.isnull().any() #So no null values left finally ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dff9fcc871c21b4949d0082f3609151bb6f3e726","_cell_guid":"b2ed1983-50d5-405c-8bad-c61a087758f5","trusted":true},"cell_type":"code","source":"f,ax=plt.subplots(1,2,figsize=(20,10))\ndata[data['Survived']==0].Age.plot.hist(ax=ax[0],bins=20,edgecolor='black',color='red')\nax[0].set_title('Survived= 0')\nx1=list(range(0,85,5))\nax[0].set_xticks(x1)\ndata[data['Survived']==1].Age.plot.hist(ax=ax[1],color='green',bins=20,edgecolor='black')\nax[1].set_title('Survived= 1')\nx2=list(range(0,85,5))\nax[1].set_xticks(x2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e78651053e3b3641da3ded5bac87be689c7df259","_cell_guid":"b4c65724-4641-4e8c-be59-c9d41bfd088b"},"cell_type":"markdown","source":"### Observations: 관찰결과\n1)The Toddlers(age<5) were saved in large numbers(The Women and Child First Policy).\n- 1) 유아(5세미만)는 더 큰 숫자로 구조 됨(여자와 아이 먼저 법칙).\n\n2)The oldest Passenger was saved(80 years).\n- 2) 가장 연장한 사람도 구조 됨(80세).\n\n3)Maximum number of deaths were in the age group of 30-40.\n- 3) 가장 사망자 숫자가 높은 그룹은 30-40세 그룹."},{"metadata":{"_uuid":"e81923d749fef3cfc374b9b2dcbc9f27e8cc1ecc","_cell_guid":"82ec9949-9681-42d0-959d-02fe4ff2675c","trusted":true},"cell_type":"code","source":"sns.factorplot('Pclass','Survived',col='Initial',data=data)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4a0486c4864f225d54bbc392fa01ec068e9d89d5","_cell_guid":"2e547f77-4b30-4e76-baf8-1ece2f74074d"},"cell_type":"markdown","source":"The Women and Child first policy thus holds true irrespective of the class.\n- 여자와 아이 먼저 법칙은 Pclass와 상관없습니다."},{"metadata":{"_uuid":"a24b323daf19e8fd6cacc83f549523b49a0789e7","_cell_guid":"8be8b82f-8d91-471e-adc0-80dc6d3def0b"},"cell_type":"markdown","source":"## Embarked --> Categorical Value\n## 탑승장소 --> 카테고리 값"},{"metadata":{"_uuid":"0d9307f18fcc510c7615e73080f23ba6cb80c3ae","_cell_guid":"ec30e8cc-471d-4616-be83-b01bd45d51a7","trusted":true},"cell_type":"code","source":"pd.crosstab([data.Embarked,data.Pclass],[data.Sex,data.Survived],margins=True).style.background_gradient(cmap='summer_r')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5a99134fe7128642103b9b859172b6db85da3514","_cell_guid":"1966db49-4dd7-4b34-98c1-5387f9c3fb70"},"cell_type":"markdown","source":"### Chances for Survival by Port Of Embarkation\n### 승선장소에 따른 생존 확률"},{"metadata":{"_uuid":"8dce8ddd858624321e8a69f2e9a5a30ade19aa12","_cell_guid":"4193b498-a67c-49f1-b6ec-71c4bf0300a1","trusted":true},"cell_type":"code","source":"sns.factorplot('Embarked','Survived',data=data)\nfig=plt.gcf()\nfig.set_size_inches(5,3)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dc3ba850739bf7c24af92b37976d17ac05a668f4","_cell_guid":"d678b40e-ea1a-4340-9c98-562a8550860d"},"cell_type":"markdown","source":"The chances for survival for Port C is highest around 0.55 while it is lowest for S.\n- C포트에서 승선한 승객이 0.55로 가장 높은 확률로 생존했고, S에서 탑승한 승객이 가장 낮았습니다."},{"metadata":{"_uuid":"dfcab3effc1ebf8653e3a4b61149b44fc146fdfb","_cell_guid":"51ff68c3-ffa2-4ac7-95ee-04ecb7d9da64","trusted":true},"cell_type":"code","source":"f,ax=plt.subplots(2,2,figsize=(20,15))\nsns.countplot('Embarked',data=data,ax=ax[0,0])\nax[0,0].set_title('No. Of Passengers Boarded')\nsns.countplot('Embarked',hue='Sex',data=data,ax=ax[0,1])\nax[0,1].set_title('Male-Female Split for Embarked')\nsns.countplot('Embarked',hue='Survived',data=data,ax=ax[1,0])\nax[1,0].set_title('Embarked vs Survived')\nsns.countplot('Embarked',hue='Pclass',data=data,ax=ax[1,1])\nax[1,1].set_title('Embarked vs Pclass')\nplt.subplots_adjust(wspace=0.2,hspace=0.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"57bba2f20422b2db70f0f84e8feb8ba0ace410a0","collapsed":true,"_cell_guid":"6a5b59d7-4886-4b28-9a6d-8cb266f6f0eb"},"cell_type":"markdown","source":"### Observations: 관찰결과\n1)Maximum passenegers boarded from S. Majority of them being from Pclass3.\n- 1) S에서 가장 많은 승객이 탑승함. 주된 승객들을 3등석.\n\n2)The Passengers from C look to be lucky as a good proportion of them survived. The reason for this maybe the rescue of all the Pclass1 and Pclass2 Passengers.\n- 2) C에서 탑승한 그들의 대부분이 살아남은걸로 봐서 운이 좋다고 생각할 수 있음. 사실은 그들은 모두 1등석과 2등석 승객이었음.\n\n3)The Embark S looks to the port from where majority of the rich people boarded. Still the chances for survival is low here, that is because many passengers from Pclass3 around **81%** didn't survive. \n- 3) S에서 승선한 사람들도 대부분이 부자들이었음. 그래도 여전히 그들의 생존 확률은 낮음. 3등석의 81%가 살아남지 못했기 때문임.\n\n4)Port Q had almost 95% of the passengers were from Pclass3.\n- 4) Q에서 탑승한 95%의 승객들은 3등석임."},{"metadata":{"_uuid":"2546b1329d2f46bbfdc4b6ac3728747c436ee3f1","_cell_guid":"566e32f9-eaa9-44b1-b904-b71f242f7c6e","trusted":true},"cell_type":"code","source":"sns.factorplot('Pclass','Survived',hue='Sex',col='Embarked',data=data)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a82c6f302df8fb1227ed024f38f2e8a73376d113","collapsed":true,"_cell_guid":"e694b26b-ccca-4405-9fa5-af4f91527d80"},"cell_type":"markdown","source":"### Observations: 관찰결과\n\n1)The survival chances are almost 1 for women for Pclass1 and Pclass2 irrespective of the Pclass.\n- 1) 1등석과 2등석에 있던 여자들은 승선장소와 관계없이 생존 확률이 거의 1.\n\n2)Port S looks to be very unlucky for Pclass3 Passenegers as the survival rate for both men and women is very low.**(Money Matters)**\n- 2) S에서 승선한 3등석 승객들은 남자와 여자 모두 생존 확률이 낮음. (돈은 중요합니다)\n\n3)Port Q looks to be unlukiest for Men, as almost all were from Pclass 3.\n- 3) Q에서 승선한 사람들이 거의 3등석이었다는 것을 생각하면 남자들에게 가장 불운함."},{"metadata":{"_uuid":"c124234128b669e41546daab16bfb85d14b5dc03","collapsed":true,"_cell_guid":"9d2984b4-c9a7-44bf-ada3-78afc83bcd26"},"cell_type":"markdown","source":"### Filling Embarked NaN / 승선 Null값 채우기\n\nAs we saw that maximum passengers boarded from Port S, we replace NaN with S.\n- 우리는 S에서 가장 많은 승객이 탑승한것을 보았음으로 Null값을 S로 채워넣을 겁니다."},{"metadata":{"_uuid":"c77ed7f842ec862326ca6b9986e21a0a7d69acff","_cell_guid":"62309104-404b-4f79-a50b-1f1747fde9f5","trusted":true},"cell_type":"code","source":"data['Embarked'].fillna('S',inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"16f31e0c60dc64d638d9505b2eaa855fb20205cc","_cell_guid":"56d6a590-9ab2-4be6-8a90-f0bb9e908cae","trusted":true},"cell_type":"code","source":"data.Embarked.isnull().any()# Finally No NaN values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"838c230d6e4027c9f87b702ffd5805284c74ca4d","collapsed":true,"_cell_guid":"05194e42-445e-41a5-9124-e4ec29d1ac2a"},"cell_type":"markdown","source":"## SibSip --> Discrete Feature\n## SibSip --> 가산(셀 수 있는) 피처\nThis feature represents whether a person is alone or with his family members.\n- 이 피처는 어떤 사람이 혼자인지 가족과 같이 있는지를 나타냅니다.\n\nSibling = brother, sister, stepbrother, stepsister\n- 형제자매 = 형제, 자매, 이복형제, 이복자매\n\nSpouse = husband, wife \n- 배우자 = 남편, 아내"},{"metadata":{"_uuid":"56069ce478b75673fab78145fb6a6741cad28d76","_cell_guid":"ae7b6019-3162-400f-9746-8d8239049751","trusted":true},"cell_type":"code","source":"pd.crosstab([data.SibSp],data.Survived).style.background_gradient(cmap='summer_r')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"565dcbd1acdb973ccfacb41d1d509ee3c59cd126","_cell_guid":"e464b8ab-e642-4666-a701-059c1bd3b77b","trusted":true},"cell_type":"code","source":"f,ax=plt.subplots(1,2,figsize=(20,8))\nsns.barplot('SibSp','Survived',data=data,ax=ax[0])\nax[0].set_title('SibSp vs Survived')\nsns.factorplot('SibSp','Survived',data=data,ax=ax[1])\nax[1].set_title('SibSp vs Survived')\nplt.close(2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fcdc00224417620a8805e0e5d0b6e83c81119981","_cell_guid":"c0ce45f8-0b08-4631-ade7-b3ddd3978414","trusted":true},"cell_type":"code","source":"pd.crosstab(data.SibSp,data.Pclass).style.background_gradient(cmap='summer_r')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"485d132cdff1171a3c5853572f6150ad1e4f92cc","collapsed":true,"_cell_guid":"8fd6eeb9-7aed-4bbb-b13d-2d491bdbdd6a"},"cell_type":"markdown","source":"### Observations: 관찰결과\n\n\nThe barplot and factorplot shows that if a passenger is alone onboard with no siblings, he have 34.5% survival rate. The graph roughly decreases if the number of siblings increase. This makes sense. That is, if I have a family on board, I will try to save them instead of saving myself first. Surprisingly the survival for families with 5-8 members is **0%**. The reason may be Pclass??\n- barplot과 factorplot을 보면 혼자 탑승한 승객인 경우 34.5%의 생존 비율을 보였고, 그래프는 형제자매의 숫자가 커질 수록 roughl하게 감소했습니다. 일리가 있는 변화입니다. 저도 가족이 있는 승객이라면, 제가 살기보다는 먼저 제 가족을 살릴 것입니다. 놀랍게도 5-8명의 구성원을 가진 가족의 생존 비율은 **0%**입니다. Pclass가 이유일 수 있겠네요?\n\nThe reason is **Pclass**. The crosstab shows that Person with SibSp>3 were all in Pclass3. It is imminent that all the large families in Pclass3(>3) died.\n- 그 이유는 Pclass가 맞았습니다. crosstab은 3명 이상의 동승자가 있는 사람의 경우 모두 3등석이었습니다. 이것은 3등석에 탑승한 대가족은 모두 살아남지 못한것에 가깝습니다."},{"metadata":{"_uuid":"5f4af4fa5a1708815b12e4e0c330a6762647f44b","collapsed":true,"_cell_guid":"4abd6f2c-0b9e-48a8-ba09-97ef75b3499a"},"cell_type":"markdown","source":"## Parch"},{"metadata":{"_uuid":"f426753939cc958e1c358e3cf165c5915a0fcc2d","_cell_guid":"84e405d3-cd3d-4a00-840d-f3e51bbfd45f","trusted":true},"cell_type":"code","source":"pd.crosstab(data.Parch,data.Pclass).style.background_gradient(cmap='summer_r')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ecc5aebc2c638b3a6bbde9338b588d3d803c1af2","_cell_guid":"d07b3740-3af3-442a-b640-54926597c999"},"cell_type":"markdown","source":"The crosstab again shows that larger families were in Pclass3.\n- crosstab은 다시금 대가족은 3등석에 있었다는 사실을 알려줍니다."},{"metadata":{"_uuid":"13b42065a19f14e2ce10dcf597377ea59c9bfc2d","_cell_guid":"fb77d798-a7dc-4483-8ce9-9cf8349934f1","trusted":true},"cell_type":"code","source":"f,ax=plt.subplots(1,2,figsize=(20,8))\nsns.barplot('Parch','Survived',data=data,ax=ax[0])\nax[0].set_title('Parch vs Survived')\nsns.factorplot('Parch','Survived',data=data,ax=ax[1])\nax[1].set_title('Parch vs Survived')\nplt.close(2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b55cfb2450f70f31a52c4ed64b89118b7b74ccec","_cell_guid":"4b40e791-0817-4e70-a8e6-c74e121cdf45"},"cell_type":"markdown","source":"### Observations: 관찰결과\n\nHere too the results are quite similar. Passengers with their parents onboard have greater chance of survival. It however reduces as the number goes up.\n- 다시 여기서도 결과는 비슷합니다. 가족이 함께 승선한 승객들은 더 큰 생존의 기회가 있었습니다. 어찌되었든 이 기회도 가족구성원이 늘어날수록 감소했습니다.\n\nThe chances of survival is good for somebody who has 1-3 parents on the ship. Being alone also proves to be fatal and the chances for survival decreases when somebody has >4 parents on the ship.\n- 1-3명의 가족이 함께 탑승한 승객의 경우 생존의 기회가 가장 좋았습니다. 혼자인 경우도 누군가 4명이상의 가족을 동반할 때와 같이 생존의 기회가 감소했습니다."},{"metadata":{"_uuid":"d873672610a96daa00522c850bd1b96013f92856","_cell_guid":"ce242dd4-c537-40f9-8223-76740512e966"},"cell_type":"markdown","source":"## Fare --> Continous Feature\n## 비용 --> 연속 피처"},{"metadata":{"_uuid":"fbd3e42723ae1447bc2f1b91204ffa609b4d07c5","_cell_guid":"db19b152-af6d-41f7-a545-fe41e4be18a8","trusted":true},"cell_type":"code","source":"print('Highest Fare was:',data['Fare'].max())\nprint('Lowest Fare was:',data['Fare'].min())\nprint('Average Fare was:',data['Fare'].mean())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9cce63092c5f0f6864e45e43197a9b9af78a6bd7","_cell_guid":"5ab99107-6bd5-47be-b512-120eab64e7ef"},"cell_type":"markdown","source":"The lowest fare is **0.0**. Wow!! a free luxorious ride. \n- 가장 낮은 요금은 **0.0**이었습니다. 와우! 무료로 호화로운 크루즈 여행이네요."},{"metadata":{"_uuid":"cea989cb8581e2d563009339e8194b81c531afde","scrolled":true,"_cell_guid":"e9c4559e-913c-4a2d-9e96-6385e7ed6a06","trusted":true},"cell_type":"code","source":"f,ax=plt.subplots(1,3,figsize=(20,8))\nsns.distplot(data[data['Pclass']==1].Fare,ax=ax[0])\nax[0].set_title('Fares in Pclass 1')\nsns.distplot(data[data['Pclass']==2].Fare,ax=ax[1])\nax[1].set_title('Fares in Pclass 2')\nsns.distplot(data[data['Pclass']==3].Fare,ax=ax[2])\nax[2].set_title('Fares in Pclass 3')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2f85d68e7c648ffde18f55b6f49701e159256478","_cell_guid":"757f65b4-5a89-4385-9422-a98b2c3999cd"},"cell_type":"markdown","source":"There looks to be a large distribution in the fares of Passengers in Pclass1 and this distribution goes on decreasing as the standards reduces. As this is also continous, we can convert into discrete values by using binning.\n- 1등석에 탑승한 승객이 가장 큰 분포를 보였고, 이 분포는 표준 편차가 줄어들수록 감소했습니다. 이것 역시 연속된 자료이기 때문에 binning을 이용해서 이산적인 값으로 바꿀 수 있습니다."},{"metadata":{"_uuid":"3918cdff2761844f861290010cac76797c2499eb","collapsed":true,"_cell_guid":"de570fa8-3b6c-48b9-908c-6bdb9978bda1"},"cell_type":"markdown","source":"## Observations in a Nutshell for all features: 모든 피처에 대한 간단한 분석 결과\n\n**Sex:** The chance of survival for women is high as compared to men.\n- **성별:** 여자의 생존 확률은 남자에 비해 높음.\n\n**Pclass:**There is a visible trend that being a **1st class passenger** gives you better chances of survival. The survival rate for **Pclass3 is very low**. For **women**, the chance of survival from **Pclass1** is almost 1 and is high too for those from **Pclass2**.   **Money Wins!!!**. \n_ **Pclass:**1등석 승객**이 더 높은 생존 기회를 제공하는 흐름을 볼수 있음.**3등석**의 승객의 경우 생존 확률이 **매우 낮음**. 1등석에 탑승한 **여성**승객의 경우 2등석과 다르게 생존비율이 매우높은 거의 100% 수준임. **돈 많은 놈이 이긴다!!!**\n\n**Age:** Children less than 5-10 years do have a high chance of survival. Passengers between age group 15 to 35 died a lot.\n- **나이:** 5-10살 보다 어린 아이들의 경우 높은 확률로 생존 함. 15-35세의 승객이 가장 많이 살아남지 못함.\n\n**Embarked:** This is a very interesting feature. **The chances of survival at C looks to be better than even though the majority of Pclass1 passengers got up at S.** Passengers at Q were all from **Pclass3**. \n- **승선장소:** 매우 흥미로운 피처임. **S에서 승선한 주된 승객이 1등석임에도 불구하고 C에서 탑승한 사람들의 생존확률이 더 좋음.** Q에서 탑승한 승객은 모두 3등석임.\n\n**Parch+SibSp:** Having 1-2 siblings,spouse on board or 1-3 Parents shows a greater chance of probablity rather than being alone or having a large family travelling with you.\n- **배우자+가족:** 1-2명의 형제와 동승한 경우 또는 1-3명의 가족과 동승한 경우가 많은 가족과 함께 탑승한 경우보다 더 좋은 확률로 생존함."},{"metadata":{"_uuid":"3b7ef048f72c226d996c6ad955b2b171d1780b93","_cell_guid":"410e5ca6-2aa4-42a9-9875-d5aeb87d831f"},"cell_type":"markdown","source":"## Correlation Between The Features / 피처들 사이의 관계"},{"metadata":{"_uuid":"afa990766959d5cafd155c0c10c8c2d5afab2919","scrolled":true,"_cell_guid":"88547a8f-28bb-469e-b3b8-5e5fbc4a1e30","trusted":true},"cell_type":"code","source":"sns.heatmap(data.corr(),annot=True,cmap='RdYlGn',linewidths=0.2) #data.corr()-->correlation matrix\nfig=plt.gcf()\nfig.set_size_inches(10,8)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0b4721abbc77dcd5dd75616fd0b6f2e9ba198ff4","collapsed":true,"_cell_guid":"e42e7713-4b01-4429-94d9-3dd514227933"},"cell_type":"markdown","source":"### Interpreting The Heatmap / Heatmap 분석\n\nThe first thing to note is that only the numeric features are compared as it is obvious that we cannot correlate between alphabets or strings. Before understanding the plot, let us see what exactly correlation is.\n- 일단 우리는 문자들 또는 문자열 사이의 관계를 알아볼 수 없기 때문에 숫자피처들만 비교할겁니다. 그래프를 이해하기 전에 정확히 어떤 관계가 있는 지 살펴봅시다.\n\n**POSITIVE CORRELATION:** If an **increase in feature A leads to increase in feature B, then they are positively correlated**. A value **1 means perfect positive correlation**.\n- **양성 상호관계:**: 어떤 피처A의 증가가 피처B의 증가로 이어진다면, 그 둘은 양성 상호관계에 있다고 합니다. 1은 완벽한 양성 상호관계입니다.\n\n**NEGATIVE CORRELATION:** If an **increase in feature A leads to decrease in feature B, then they are negatively correlated**. A value **-1 means perfect negative correlation**.\n- **음성 상호관계:** 어떤 피처A의 증가가 피처B의 감소로 이어진다면, 그 둘은 음성 상호관계에 있다고 합니다. -1은 완벽한 음성 상호관계입니다.\n\nNow lets say that two features are highly or perfectly correlated, so the increase in one leads to increase in the other. This means that both the features are containing highly similar information and there is very little or no variance in information. This is known as **MultiColinearity** as both of them contains almost the same information.\n- 어떤 두 피처가 서로 완벽하게 또는 높은 상호관계에 있다고 합시다. 그래서 한 피처의 증가가 다른 하나의 증가를 가져올때 그 둘은 서로 비슷한 정보를 갖고 있고 그 둘은 변화가 적거나 거의 없습니다. 이것은 다중공선성(역자: 직역하자면 그렇습니다.)이라고 둘이 거의 같은 정보를 담고 있음을 뜻합니다.\n\nSo do you think we should use both of them as **one of them is redundant**. While making or training models, we should try to eliminate redundant features as it reduces training time and many such advantages.\n- 우리는 둘중 하나가 불필요하다는 것을 알면서도 둘다 사용해야할까요. 우리가 훈련 모델을 만들면서 우리는 그런 불필요한 피처들을 제거함으로서 훈련시간을 감소시키고 다른 이점을 얻을 수 있습니다.\n\nNow from the above heatmap,we can see that the features are not much correlated. The highest correlation is between **SibSp and Parch i.e 0.41**. So we can carry on with all features.\n-  이제 위에 heatmap에서 우리는 각각의 피처들이 서로 상호관계가 강하지 않다는 것을 볼 수 있습니다. 가장 높은 상호관계는 형제와 배우자로 **0.41**입니다. 따라서 우리는 모든 피처들을 가지고 갈 수 있습니다."},{"metadata":{"_uuid":"08132891de3a44c38d099573a8693270b565bb1b","_cell_guid":"734d8511-e8ed-423a-913a-ccaa1ebda241"},"cell_type":"markdown","source":"## Part2: Feature Engineering and Data Cleaning / 피처 엔지니어링과 데이터 정제\n\nNow what is Feature Engineering?\n- 피처 엔지니어링은 무엇이냐?\n\nWhenever we are given a dataset with features, it is not necessary that all the features will be important. There maybe be many redundant features which should be eliminated. Also we can get or add new features by observing or extracting information from other features.\n- 우리가 피처를 가진 데이터를 얻게 되면, 모든 피처가 필요하거니 중요하지 않습니다. 제거가 필요한 불필요한 피처가 있을 수 있습니다. 또한 우리는 관찰과 다른 피처들에서부터 추출한 정보들로 새로운 피처를 만들 수 있습니다.\n\nAn example would be getting the Initals feature using the Name Feature. Lets see if we can get any new features and eliminate a few. Also we will tranform the existing relevant features to suitable form for Predictive Modeling.\n- 그 예시로 이름피처에서 이니셜 피처를 추출한것을 들 수 있습니다. 한번 어떤 피처를 더하거나 제거할 수 있을 지 봅시다. 그리고 기존의 관련 피처들을 예측 모델링을 하기에 적합하게 변환할 것입니다."},{"metadata":{"_uuid":"488a35e1cce6d5d3a50327ec2dfd9d0961f1abaf","_cell_guid":"f2fe673f-b1e4-4c2b-828c-091f27bbc3f5"},"cell_type":"markdown","source":"## Age_band / 나이_대\n\n#### Problem With Age Feature: 나이 피처의 문제\nAs I have mentioned earlier that **Age is a continous feature**, there is a problem with Continous Variables in Machine Learning Models.\n- 제가 전에도 언급했듯이 **나이는 연속 피처** 입니다. 머신 러닝 모델에서 연속피처는 문제가 있습니다.\n\n**Eg:**If I say to group or arrange Sports Person by **Sex**, We can easily segregate them by Male and Female.\n- **예시:** 만약 제가 스포츠인을 성별로 그룹짓거나 분류하라 하면, 우리는 쉽게 남성과 여성으로 분류할 수 있습니다.\n\nNow if I say to group them by their **Age**, then how would you do it? If there are 30 Persons, there may be 30 age values. Now this is problematic.\n- 이제 그들을 **나이**로 나눠보라고 하면, 어떻게 할 것인가요? 만약 30명의 사람이 있다면 30개의 값들이 있습니다. 이제 문제가 생겼습니다.\n\nWe need to convert these **continous values into categorical values** by either Binning or Normalisation. I will be using binning i.e group a range of ages into a single bin or assign them a single value.\n- 우리는 Binning or Normalisation(정규화)를 통해 **연속 피처**를 **카테고리 피처**로 바꿀 필요가 있습니다. 저는 binning을 이용할 것인데, 예를 들어 특정 나이 영역에 하나의 큰 값을 할당하는 방식입니다.\n\nOkay so the maximum age of a passenger was 80. So lets divide the range from 0-80 into 5 bins. So 80/5=16.\nSo bins of size 16.\n- 최고령자는 80세 였습니다. 따라서 0-80의 영역을 5개의 큰 덩어리로 나눕시다. 80/5=16이므로 각각 16년의 크기를 가진 덩어리가 나오겠네요."},{"metadata":{"_uuid":"f79d6021bde433c229d180a82038bfc061b05093","_cell_guid":"4b641963-d34e-4a4e-972f-df400c21c62d","trusted":true},"cell_type":"code","source":"data['Age_band']=0\ndata.loc[data['Age']<=16,'Age_band']=0\ndata.loc[(data['Age']>16)&(data['Age']<=32),'Age_band']=1\ndata.loc[(data['Age']>32)&(data['Age']<=48),'Age_band']=2\ndata.loc[(data['Age']>48)&(data['Age']<=64),'Age_band']=3\ndata.loc[data['Age']>64,'Age_band']=4\ndata.head(2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f03e02b9ffd6a00041f18f2e34a36f76637101fe","_cell_guid":"2f187870-e106-4852-acf0-9fa2e4115789","trusted":true},"cell_type":"code","source":"data['Age_band'].value_counts().to_frame().style.background_gradient(cmap='summer')#checking the number of passenegers in each band","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b314354514550835885788e4653c4747ed19bd35","_cell_guid":"ba93248d-da3c-4843-947e-b8fab18e8ab9","trusted":true},"cell_type":"code","source":"sns.factorplot('Age_band','Survived',data=data,col='Pclass')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"10a137ebe2c56afc515342642c35d4daa9b78497","_cell_guid":"b432f71d-6fd1-42be-8533-1b7114276004"},"cell_type":"markdown","source":"True that..the survival rate decreases as the age increases irrespective of the Pclass.\n- Pclass와 관계 없이 나이가 증가함에 따라 생존 비율이 감소했습니다.\n\n## Family_Size and Alone / 가족_크기 그리고 혼자승선\nAt this point, we can create a new feature called \"Family_size\" and \"Alone\" and analyse it. This feature is the summation of Parch and SibSp. It gives us a combined data so that we can check if survival rate have anything to do with family size of the passengers. Alone will denote whether a passenger is alone or not.\n- 이 시점에서 우리는 \"가족크기\" 와 \"혼자승선(이하 혼자)\" 라는 피처를 분석을 통해 추가할 수 있습니다. 이 피처는 배우자와 형제자매의 요약된 버전입니다. 이것은 우리에게 합쳐진 데이터를 보여주는데 이는 승객의 가족 크기에 따라 변화하는 생존 비율을 체크할 수 있게 해줍니다. 혼자는 말그대로 승객이 혼자인지 아닌지 표시합니다."},{"metadata":{"_uuid":"89d9a7057f35aaf778a38057bcb6de2cbf6cd1dd","_cell_guid":"c676363a-a754-4fcb-aff3-62422bbc4924","trusted":true},"cell_type":"code","source":"data['Family_Size']=0\ndata['Family_Size']=data['Parch']+data['SibSp']#family size\ndata['Alone']=0\ndata.loc[data.Family_Size==0,'Alone']=1#Alone\n\nf,ax=plt.subplots(1,2,figsize=(18,6))\nsns.factorplot('Family_Size','Survived',data=data,ax=ax[0])\nax[0].set_title('Family_Size vs Survived')\nsns.factorplot('Alone','Survived',data=data,ax=ax[1])\nax[1].set_title('Alone vs Survived')\nplt.close(2)\nplt.close(3)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"81bcb9835ff850fc400183c0c6b599fd4aecd222","_cell_guid":"1ba180b0-c78a-4f9e-a179-6f89464d1c7d"},"cell_type":"markdown","source":"**Family_Size=0 means that the passeneger is alone.** Clearly, if you are alone or family_size=0,then chances for survival is very low. For family size > 4,the chances decrease too. This also looks to be an important feature for the model. Lets examine this further.\n- **가족_크기=0의 의미는 승객이 혼자라는 뜻입니다.** 분명하게, 만약 당신이 혼자거나 가족크기가 0이라면, 당신의 생존 확률은 많이 낮습니다. 4인 이상의 가족인 경우도 마찬가지로 감소합니다. 이것도 모델에서 중요한 피처들처럼 보입니다. 뒤에서 조금 더 다뤄 봅시다."},{"metadata":{"_uuid":"17d8e84e2f6acb5e3b4ae72712002ff6ec267037","_cell_guid":"be014085-aa05-4bd4-8835-76413ada128c","trusted":true},"cell_type":"code","source":"sns.factorplot('Alone','Survived',data=data,hue='Sex',col='Pclass')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"12d1a4fa19df5691a4ecca4662031a125e98ed02","_cell_guid":"6cf47ab0-5172-499d-ae63-1e45f72175f2"},"cell_type":"markdown","source":"It is visible that being alone is harmful irrespective of Sex or Pclass except for Pclass3 where the chances of females who are alone is high than those with family.\n- 성별과 Pclass와 상관없이 혼자인것은 3등석에 탑승한 여성승객의 경우 가족과 함께 승선한 여성승객에 비해 혼자인 경우가 좋지 않다는 것을 볼 수 있습니다.\n\n## Fare_Range / 비용_범위\n\nSince fare is also a continous feature, we need to convert it into ordinal value. For this we will use **pandas.qcut**.\n- 비용도 연속 피처이기 때문에, 우리는 순서가 있는 값으로 바꿔주어야 합니다. 이때 우리는 **pandas.qcut**를 사용합니다.\n\nSo what **qcut** does is it splits or arranges the values according the number of bins we have passed. So if we pass for 5 bins, it will arrange the values equally spaced into 5 seperate bins or value ranges.\n- **qcut**는 우리가 넘겨준 단위의 크기에 따라 값들을 나누거나 모아 줍니다. 만약 우리가 5의 단위 를 넘겨줬다면, 5의 단위나 값의 범위로 나눠줍니다."},{"metadata":{"_uuid":"4aec257e46ab71667aed9571663e74f7e1f27d9b","_cell_guid":"f46d820c-e81e-448f-837d-d3dfd63b94e6","trusted":true},"cell_type":"code","source":"data['Fare_Range']=pd.qcut(data['Fare'],4)\ndata.groupby(['Fare_Range'])['Survived'].mean().to_frame().style.background_gradient(cmap='summer_r')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"07f991646cc077f42cab42c41942138109af346d","_cell_guid":"357e3fd7-465f-4bd4-83f1-ac78bcdf7538"},"cell_type":"markdown","source":"As discussed above, we can clearly see that as the **fare_range increases, the chances of survival increases.**\n- 위에서 알아봤듯이 우리는 **비용의 증가는 생존확률을 높인다**는 것을 분명하게 볼 수 있습니다.\n\nNow we cannot pass the Fare_Range values as it is. We should convert it into singleton values same as we did in **Age_Band**\n    - 이제 우리는 비용_범위 값을 그대로 넘길 수 없습니다. 우리는 \"나이_범위\"에서와 같은 싱글 톤 값으로 변환해 주어야 합니다. (연속 피처를 카테고리 피처로 변환)"},{"metadata":{"_uuid":"f22ee32844ba93a6162d14ca801385064a996101","_cell_guid":"ba80366f-8f8c-421e-bfb9-f3121402e5b6","trusted":true},"cell_type":"code","source":"data['Fare_cat']=0\ndata.loc[data['Fare']<=7.91,'Fare_cat']=0\ndata.loc[(data['Fare']>7.91)&(data['Fare']<=14.454),'Fare_cat']=1\ndata.loc[(data['Fare']>14.454)&(data['Fare']<=31),'Fare_cat']=2\ndata.loc[(data['Fare']>31)&(data['Fare']<=513),'Fare_cat']=3","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4390d6f0ea444a0a86ce72a592fdb3bd858ac725","_cell_guid":"c3446d11-2151-4cfa-aa43-2637ac793525","trusted":true},"cell_type":"code","source":"sns.factorplot('Fare_cat','Survived',data=data,hue='Sex')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2e4dc43d07d286ef9e4e8995d77cddb892973c86","_cell_guid":"2cd0333d-a2dd-4deb-b103-8e2e0fc5da4e"},"cell_type":"markdown","source":"Clearly, as the Fare_cat increases, the survival chances increases. This feature may become an important feature during modeling along with the Sex.\n- 분명하게, Fare_cat(비용_카테고리) 증가할수록 생존 확률이 증가합니다. 이 피처는 성별과 함께 모델링에서 중요할 것입니다.\n\n## Converting String Values into Numeric / 문자열 값을 숫자 값으로 변환\n\nSince we cannot pass strings to a machine learning model, we need to convert features loke Sex, Embarked, etc into numeric values.\n- 우리가 문자열을 머신러닝 모델에 넘겨줄 수 없기 때문에, 우리는 성별, 승선 장소 등 을 숫자 값으로 변환해야합니다."},{"metadata":{"_uuid":"e76a3ec186753e9217584419945e6654072819a7","_cell_guid":"c9a75f9d-9a0c-4ffc-88ec-5996ee589121","trusted":true},"cell_type":"code","source":"data['Sex'].replace(['male','female'],[0,1],inplace=True)\ndata['Embarked'].replace(['S','C','Q'],[0,1,2],inplace=True)\ndata['Initial'].replace(['Mr','Mrs','Miss','Master','Other'],[0,1,2,3,4],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"30b66ebbf65acebdefe20a520d9a62762b959590","_cell_guid":"89a1862f-f0ca-48e9-aeaf-460950538c7f"},"cell_type":"markdown","source":"### Dropping UnNeeded Features / 불필요한 피처 버리기\n\n**Name**--> We don't need name feature as it cannot be converted into any categorical value.\n- **이름**--> 이름: 어떠한  카테고리 값으로 변환 할 수 업기 때문에 필요하지 않습니다.\n\n**Age**--> We have the Age_band feature, so no need of this.\n- **나이**--> 우리는 나이_대 피처가 있음으로 필요하지 않습니다.\n\n**Ticket**--> It is any random string that cannot be categorised.\n- **티켓**--> 이건 랜덤한 문자열 자료이므로 카테고리화 시킬 수 없습니다.\n\n**Fare**--> We have the Fare_cat feature, so unneeded\n- **비용** 이건 랜덤한 문자열 자료이므로 카테고리화 시킬 수 없습니다.\n\n**Cabin**--> A lot of NaN values and also many passengers have multiple cabins. So this is a useless feature.\n- **선실**--> 많은 null값들이 있고 많은 승객들이 복수의 선실을 갖고 있기에 불필요합니다\n\n**Fare_Range**--> We have the fare_cat feature.\n- **비용_범위**--> 우리는 비용카테고리가 있습니다.\n\n**PassengerId**--> Cannot be categorised.\n- **승객ID**--> 카테고리화 할 수 없습니다."},{"metadata":{"_uuid":"210bdb7650161cbe09a381dc89c0fc4485b2646c","_cell_guid":"bc04b292-b453-4b45-868b-ab3f99b776dc","trusted":true},"cell_type":"code","source":"data.drop(['Name','Age','Ticket','Fare','Cabin','Fare_Range','PassengerId'],axis=1,inplace=True)\nsns.heatmap(data.corr(),annot=True,cmap='RdYlGn',linewidths=0.2,annot_kws={'size':20})\nfig=plt.gcf()\nfig.set_size_inches(18,15)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d3247092c61752d3330e7a4e4f5e039402f11285","_cell_guid":"ff2e04fa-1213-4183-8f03-d78889bb3aba"},"cell_type":"markdown","source":"Now the above correlation plot, we can see some positively related features. Some of them being **SibSp and Family_Size** and **Parch and Family_Size** and some negative ones like **Alone and Family_Size.**\n- 위에 상호관계 그래프를 보면 몇가지의 양성 관계 피처들을 볼 수 있습니다. 형제와 가족크기, 배우자와 가족크기 등이 있고 음성 관계로 혼자와 가족크기가 있습니다. "},{"metadata":{"_uuid":"fb6f48f1da808adbb03cd22b9a823ed2bd3374ae","_cell_guid":"7110f2af-1002-4d78-942b-19318b0d90c3"},"cell_type":"markdown","source":"# Part3: Predictive Modeling / 예측 모델링\n\nWe have gained some insights from the EDA part. But with that, we cannot accurately predict or tell whether a passenger will survive or die. So now we will predict the whether the Passenger will survive or not using some great Classification Algorithms.\n- 위에 상호관계 그래프를 보면 몇가지의 양성 관계 피처들을 볼 수 있습니다. 형제와 가족크기, 배우자와 가족크기 등이 있고 음성 관계로 혼자와 가족크기가 있습니다. 그랴서 우리는 승객의 생사를 몇가지 분류 알고리즘을 이용해서 예측 할것입니다.\n\nFollowing are the algorithms I will use to make the model:\n- 다음은 제가 모델을 만들때 사용할 알고리즘입니다:\n\n1)Logistic Regression / 로지스틱회귀\n\n2)Support Vector Machines(Linear and radial) / 서포트 벡터 머신\n\n3)Random Forest / 랜덤 포레스트\n\n4)K-Nearest Neighbours / K-근접 이웃들\n\n5)Naive Bayes / 나이브 베이즈\n\n6)Decision Tree / 결정 트리\n\n7)Logistic Regression / 로지스틱 회귀"},{"metadata":{"_uuid":"c39d18a3cbffb9b7d71b746f5f873ef8e4f7ecb3","_cell_guid":"e099ab49-b0b3-40a1-b372-1f006774d641","trusted":true},"cell_type":"code","source":"#importing all the required ML packages\nfrom sklearn.linear_model import LogisticRegression #logistic regression\nfrom sklearn import svm #support vector Machine\nfrom sklearn.ensemble import RandomForestClassifier #Random Forest\nfrom sklearn.neighbors import KNeighborsClassifier #KNN\nfrom sklearn.naive_bayes import GaussianNB #Naive bayes\nfrom sklearn.tree import DecisionTreeClassifier #Decision Tree\nfrom sklearn.model_selection import train_test_split #training and testing data split\nfrom sklearn import metrics #accuracy measure\nfrom sklearn.metrics import confusion_matrix #for confusion matrix","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"03896d10e6364121c9ffe2a4cfe280353637bcfe","_cell_guid":"c01597f4-4536-41d6-8f9a-dad8f955524e","trusted":true},"cell_type":"code","source":"train,test=train_test_split(data,test_size=0.3,random_state=0,stratify=data['Survived'])\ntrain_X=train[train.columns[1:]]\ntrain_Y=train[train.columns[:1]]\ntest_X=test[test.columns[1:]]\ntest_Y=test[test.columns[:1]]\nX=data[data.columns[1:]]\nY=data['Survived']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"737c474fe05198067e38559bf0c554af594da5bc","_cell_guid":"6618cea9-bbdc-45d3-b506-12c6000ca7e3"},"cell_type":"markdown","source":"### Radial Support Vector Machines(rbf-SVM)"},{"metadata":{"_uuid":"36939717ad06a4590276530754414bd5a281dde2","_cell_guid":"278c175b-553b-4b79-9e1a-9f4f86b385fd","trusted":true},"cell_type":"code","source":"model=svm.SVC(kernel='rbf',C=1,gamma=0.1)\nmodel.fit(train_X,train_Y)\nprediction1=model.predict(test_X)\nprint('Accuracy for rbf SVM is ',metrics.accuracy_score(prediction1,test_Y))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a2a394ac6de1d870ff4a68f46c7a14356ed65770","_cell_guid":"095e2680-800c-42fe-bff3-6ff7431e41a4"},"cell_type":"markdown","source":"### Linear Support Vector Machine(linear-SVM)"},{"metadata":{"_uuid":"0d8b19eaf5d0ca739d80d264b5a8dec2747a808b","_cell_guid":"a61bf24a-1952-474f-8596-31bf61140e17","trusted":true},"cell_type":"code","source":"model=svm.SVC(kernel='linear',C=0.1,gamma=0.1)\nmodel.fit(train_X,train_Y)\nprediction2=model.predict(test_X)\nprint('Accuracy for linear SVM is',metrics.accuracy_score(prediction2,test_Y))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c7e40b35d7a0c8cdbfab6da2c0f40f062e63e4e6","_cell_guid":"05d907c6-7751-458b-b475-e95f689b9590"},"cell_type":"markdown","source":"### Logistic Regression"},{"metadata":{"_uuid":"c3f3229fede1a5869f5c36503acf9dad53dcbcfa","_cell_guid":"80a062ce-b946-4213-9125-559c21ea409d","trusted":true},"cell_type":"code","source":"model = LogisticRegression()\nmodel.fit(train_X,train_Y)\nprediction3=model.predict(test_X)\nprint('The accuracy of the Logistic Regression is',metrics.accuracy_score(prediction3,test_Y))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ad97a5c8cd433ec3f0d757b73e163c47efb1d8e8","_cell_guid":"301bb743-9c81-4228-936c-32b17b57aa96"},"cell_type":"markdown","source":"### Decision Tree"},{"metadata":{"_uuid":"da62e904b8e149cc3239daf8f209a9472ccb13cc","_cell_guid":"bedf7f6c-29b1-4724-9977-67f23414d320","trusted":true},"cell_type":"code","source":"model=DecisionTreeClassifier()\nmodel.fit(train_X,train_Y)\nprediction4=model.predict(test_X)\nprint('The accuracy of the Decision Tree is',metrics.accuracy_score(prediction4,test_Y))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2b39e134858428fdbc7ed40a1be057f3c2e20418","_cell_guid":"16a910b2-fcc6-4204-871f-0fc524bba888"},"cell_type":"markdown","source":"### K-Nearest Neighbours(KNN)"},{"metadata":{"_uuid":"ae81fa1b7a11605a1a7366da0b08af05ab6a2662","_cell_guid":"4a840c02-24c5-4e77-98f0-6d972ce08707","trusted":true},"cell_type":"code","source":"model=KNeighborsClassifier() \nmodel.fit(train_X,train_Y)\nprediction5=model.predict(test_X)\nprint('The accuracy of the KNN is',metrics.accuracy_score(prediction5,test_Y))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d74d9689faf92c6e44f73dbde7e4992ecc53d765","_cell_guid":"8968f37a-0e0f-48b0-94f4-5018b9db6f08"},"cell_type":"markdown","source":"Now the accuracy for the KNN model changes as we change the values for **n_neighbours** attribute. The default value is **5**. Lets check the accuracies over various values of n_neighbours.\n- 이제 n_neighbours 속성의 값을 변경하면 KNN 모델의 정확도가 변경됩니다. 기본값은 5입니다. n_neighbours의 다양한 값에 대한 정확도를 확인해봅시다."},{"metadata":{"_uuid":"8920914151d70a6231389b86e609aec66a5b88c0","_cell_guid":"9633dca4-eaa2-4d59-9590-d03fe1510cde","trusted":true},"cell_type":"code","source":"a_index=list(range(1,11))\na=pd.Series()\nx=[0,1,2,3,4,5,6,7,8,9,10]\nfor i in list(range(1,11)):\n    model=KNeighborsClassifier(n_neighbors=i) \n    model.fit(train_X,train_Y)\n    prediction=model.predict(test_X)\n    a=a.append(pd.Series(metrics.accuracy_score(prediction,test_Y)))\nplt.plot(a_index, a)\nplt.xticks(x)\nfig=plt.gcf()\nfig.set_size_inches(12,6)\nplt.show()\nprint('Accuracies for different values of n are:',a.values,'with the max value as ',a.values.max())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ef98b8d1030a9f68a68a5064c2980e38a448b8fe","_cell_guid":"bb36dadf-0e74-4372-adf8-751a31951570"},"cell_type":"markdown","source":"### Gaussian Naive Bayes"},{"metadata":{"_uuid":"66889b223ce7c42880b23b6d2ec13c7a8ca8b028","_cell_guid":"fe14b93b-7732-44a3-a8fd-667abd2713fc","trusted":true},"cell_type":"code","source":"model=GaussianNB()\nmodel.fit(train_X,train_Y)\nprediction6=model.predict(test_X)\nprint('The accuracy of the NaiveBayes is',metrics.accuracy_score(prediction6,test_Y))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a7d1e5161b35601aeb5b44a4fcf6b88b243a6151","_cell_guid":"54cd59cb-854d-40d4-ac81-2d86a290977a"},"cell_type":"markdown","source":"### Random Forests"},{"metadata":{"_uuid":"2adaf51a647e1a2ec1d89189da53c1d8124e3d10","_cell_guid":"4a3fb4ef-213b-42f2-864d-9b758088f296","trusted":true},"cell_type":"code","source":"model=RandomForestClassifier(n_estimators=100)\nmodel.fit(train_X,train_Y)\nprediction7=model.predict(test_X)\nprint('The accuracy of the Random Forests is',metrics.accuracy_score(prediction7,test_Y))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"271705b29ca8f22c789072720fde34203cf3fd1b","_cell_guid":"9d87ba93-aeaa-456b-857d-ce03229f5e29"},"cell_type":"markdown","source":"The accuracy of a model is not the only factor that determines the robustness of the classifier. Let's say that a classifier is trained over a training data and tested over the test data and it scores an accuracy of 90%.\n- 모델의 정확도가 모델의 분류의 견고성(?)을 결정짓는 유일한 요소는 아닙니다. 분류기가 훈련데이터로 훈련 받고 테스트 데이터로 테스트한 후 그 점수가 90%의 정확도라고 합시다.\n\nNow this seems to be very good accuracy for a classifier, but can we confirm that it will be 90% for all the new test sets that come over??. The answer is **No**, because we can't determine which all instances will the classifier will use to train itself. As the training and testing data changes, the accuracy will also change. It may increase or decrease. This is known as **model variance**.\n- 이게 분류기로써 매우 좋은 정확도로 보입니다. 하지만 모든 새로운 테스트 데이터 셋에세도 그럴까요? 답은 No입니다. 왜냐하면 우리는 분류기가 자기자신을 훈련하기 위해 사용할 모든 사례들을 정할 수 없기 때문입니다. 하지만 모든 새로운 테스트 데이터 셋에세도 그럴까요? 답은 No입니다. 왜냐하면 우리는 분류기가 자기자신을 훈련하기 위해 사용할 모든 사례들을 정할 수 없기 때문입니다. \n\nTo overcome this and get a generalized model,we use **Cross Validation**.\n- 하지만 모든 새로운 테스트 데이터 셋에세도 그럴까요? 답은 No입니다. 왜냐하면 우리는 분류기가 자기자신을 훈련하기 위해 사용할 모든 사례들을 정할 수 없기 때문입니다.\n\n\n# Cross Validation / 교차 검증\n\nMany a times, the data is imbalanced, i.e there may be a high number of class1 instances but less number of other class instances. Thus we should train and test our algorithm on each and every instance of the dataset. Then we can take an average of all the noted accuracies over the dataset. \n- 하지만 모든 새로운 테스트 데이터 셋에세도 그럴까요? 답은 No입니다. 왜냐하면 우리는 분류기가 자기자신을 훈련하기 위해 사용할 모든 사례들을 정할 수 없기 때문입니다. 그렇지만 우리는 모든 경우와 대이터 셋에 대해서 훈련과 테스트를 진행해야합니다. 그런 다음에 모든 대이터셋에 대한 기록된 정확도의 평균값을 구할 수 있습니다.\n\n1)The K-Fold Cross Validation works by first dividing the dataset into k-subsets.\n- 1) K겹 교차검증은 데이터셋을 k서브 데이터셋으로 나누는 것에서 시작합니다.\n\n2)Let's say we divide the dataset into (k=5) parts. We reserve 1 part for testing and train the algorithm over the 4 parts.\n- 2) K=5의 파트로 데이터셋을 나눈다고 합시다. 우리는 1개의 파트를 알고리즘 테스트를 위해 사용하고 나머지 4개의 파트는 훈련을 위해 사용하는 것으로 분배합니다.\n\n3)We continue the process by changing the testing part in each iteration and training the algorithm over the other parts. The accuracies and errors are then averaged to get a average accuracy of the algorithm. This is called K-Fold Cross Validation.\n- 3) 우리는 각각의 훈련과 태스트 파트를 바꿔가면서 진행합니다. 그러면 정확도와 오류가 평균화 되어 특정 알고리즘의 정확도가 측정됩니다. 이것이 K-겹 교차검증 입니다.\n\n4)An algorithm may underfit over a dataset for some training data and sometimes also overfit the data for other training set. Thus with cross-validation, we can achieve a generalised model.\n- 4) 알고리즘은 어떤 데이터 셋에 대하여 과소적합이 될 수도 있고, 과적합이 될 수도 있습니다. 우리는 교차 검증을 통해서 일반적인 모델을 얻을 수 있습니다."},{"metadata":{"_uuid":"0008f647edc90d8da7811a77394b7f97dc4084c5","_cell_guid":"6055a9d4-60c3-4f05-80ad-7b24aa7905f6","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold #for K-fold cross validation\nfrom sklearn.model_selection import cross_val_score #score evaluation\nfrom sklearn.model_selection import cross_val_predict #prediction\nkfold = KFold(n_splits=10, random_state=22) # k=10, split the data into 10 equal parts\nxyz=[]\naccuracy=[]\nstd=[]\nclassifiers=['Linear Svm','Radial Svm','Logistic Regression','KNN','Decision Tree','Naive Bayes','Random Forest']\nmodels=[svm.SVC(kernel='linear'),svm.SVC(kernel='rbf'),LogisticRegression(),KNeighborsClassifier(n_neighbors=9),DecisionTreeClassifier(),GaussianNB(),RandomForestClassifier(n_estimators=100)]\nfor i in models:\n    model = i\n    cv_result = cross_val_score(model,X,Y, cv = kfold,scoring = \"accuracy\")\n    cv_result=cv_result\n    xyz.append(cv_result.mean())\n    std.append(cv_result.std())\n    accuracy.append(cv_result)\nnew_models_dataframe2=pd.DataFrame({'CV Mean':xyz,'Std':std},index=classifiers)       \nnew_models_dataframe2","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"806ea60471bde0ee16e59826fb53dfbb1c2e33e9","_cell_guid":"34b4b0c2-8a1a-4ab4-bc3d-bc90ced71dd7","trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(12,6))\nbox=pd.DataFrame(accuracy,index=[classifiers])\nbox.T.boxplot()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"547adc3bcf5e0b6548046e1f1554c51b70dcd610","_cell_guid":"f165a7fd-e614-4577-8bc5-0903d066d22e","trusted":true},"cell_type":"code","source":"new_models_dataframe2['CV Mean'].plot.barh(width=0.8)\nplt.title('Average CV Mean Accuracy')\nfig=plt.gcf()\nfig.set_size_inches(8,5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aa34471632a4a7449881f8dfedc4f4c3575f1506","_cell_guid":"37935965-7042-4828-983b-bea15524551c"},"cell_type":"markdown","source":"The classification accuracy can be sometimes misleading due to imbalance. We can get a summarized result with the help of confusion matrix, which shows where did the model go wrong, or which class did the model predict wrong.\n- 불균형으로 인해 분류 정확도가 오도 될 수 있습니다. 컨퓨전 행렬을 통해 요약 된 결과를 얻을 수 있습니다. 컨퓨전 행렬은 모델이 잘못되었거나 모델이 잘못 예측 한 클래스를 나타냅니다.\n\n## Confusion Matrix / 컨퓨전 행렬\n\nIt gives the number of correct and incorrect classifications made by the classifier.\n- 불균형으로 인해 분류 정확도가 오도 될 수 있습니다. 컨퓨전 행렬을 통해 요약 된 결과를 얻을 수 있습니다. 컨퓨전 행렬은 모델이 잘못되었거나 모델이 잘못 예측 한 클래스를 나타냅니다."},{"metadata":{"_uuid":"9d1a7c6efcb76a121f29a861f77e0e32e4904d4a","_cell_guid":"d6ae1291-7a55-4cc0-a039-0b9cbf2deedb","trusted":true},"cell_type":"code","source":"f,ax=plt.subplots(3,3,figsize=(12,10))\ny_pred = cross_val_predict(svm.SVC(kernel='rbf'),X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred),ax=ax[0,0],annot=True,fmt='2.0f')\nax[0,0].set_title('Matrix for rbf-SVM')\ny_pred = cross_val_predict(svm.SVC(kernel='linear'),X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred),ax=ax[0,1],annot=True,fmt='2.0f')\nax[0,1].set_title('Matrix for Linear-SVM')\ny_pred = cross_val_predict(KNeighborsClassifier(n_neighbors=9),X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred),ax=ax[0,2],annot=True,fmt='2.0f')\nax[0,2].set_title('Matrix for KNN')\ny_pred = cross_val_predict(RandomForestClassifier(n_estimators=100),X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred),ax=ax[1,0],annot=True,fmt='2.0f')\nax[1,0].set_title('Matrix for Random-Forests')\ny_pred = cross_val_predict(LogisticRegression(),X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred),ax=ax[1,1],annot=True,fmt='2.0f')\nax[1,1].set_title('Matrix for Logistic Regression')\ny_pred = cross_val_predict(DecisionTreeClassifier(),X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred),ax=ax[1,2],annot=True,fmt='2.0f')\nax[1,2].set_title('Matrix for Decision Tree')\ny_pred = cross_val_predict(GaussianNB(),X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,y_pred),ax=ax[2,0],annot=True,fmt='2.0f')\nax[2,0].set_title('Matrix for Naive Bayes')\nplt.subplots_adjust(hspace=0.2,wspace=0.2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"635c65dbd043e5e46e828c9101bf397d25717671","_cell_guid":"d8c66c23-c57c-4d3c-8742-4baa551f9ab1"},"cell_type":"markdown","source":"### Interpreting Confusion Matrix / 컨퓨전 매트릭스 해석\n\nThe left diagonal shows the number of correct predictions made for each class while the right diagonal shows the number of wrong prredictions made. \n- 왼쪽 대각선은 각 클래스에 대하여 맞는 예측을 한 경우이고 그리고 오른쪽 대각선은 잘못된 예측의 숫자를 보여줍니다.\nLets consider the first plot for rbf-SVM:\n- 먼저 rbf-SVM 그래프부터 봅시다.\n1)The no. of correct predictions are **491(for dead) + 247(for survived)** with the mean CV accuracy being **(491+247)/891 = 82.8%** which we did get earlier.\n- 1) 맞는 예측은 **491(사망자)+247(생존자)** 이고, 평균 교차검증(이하 CV) 정확도는 앞써 얻었듯 **(491+247)/891 = 82.8%** 입니다.\n\n2)**Errors**-->  Wrongly Classified 58 dead people as survived and 95 survived as dead. Thus it has made more mistakes by predicting dead as survived.\n- 2) **오차들**--> 58명의 생존했다고 분류된 사망자와 95명의 사망자로 분류된 생존자가 발생했습니다. 따라서 이는 사망자를 생존자로 분류하는 과정에서 더 많은 실수를 범했습니다.\n\nBy looking at all the matrices, we can say that rbf-SVM has a higher chance in correctly predicting dead passengers but NaiveBayes has a higher chance in correctly predicting passengers who survived.\n- 모든 행렬들을 볼때, 우리는 rdf-SVM이 사망자 예측을 정확히 하는데 더 좋고, 나이브 베이즈가 생존자를 예측하는데 좋다는 것을 "},{"metadata":{"_uuid":"fbfd27e3e1feae00fe22f517d2c4ebc8247c936c","_cell_guid":"d264a15a-fcb1-49ea-a374-4b5cd9501738"},"cell_type":"markdown","source":"### Hyper-Parameters Tuning / 하이퍼 파라미터 튜닝(최적화)\n\nThe machine learning models are like a Black-Box. There are some default parameter values for this Black-Box, which we can tune or change to get a better model. Like the C and gamma in the SVM model and similarly different parameters for different classifiers, are called the hyper-parameters, which we can tune to change the learning rate of the algorithm and get a better model. This is known as Hyper-Parameter Tuning.\n- 머신러닝 모델은 마치 블랙박스와 같습니다. 보통 블랙박스에 우리가 튜닝하거나 바꿔서 더 좋은 모델을 얻을 수 있는 몇개의 기본 피라미터 값이 있습니다. SVM 모델에 C와 감마와 다른 모델들의 비슷하지만 다른, 역시 우리가 좋은 모델을 얻기 위해 튜닝해서 알고리즘의 학습률을 바꿀 수 있는 하이퍼 파라미터가 있습니다. 이것은 하이퍼파라미터 튜닝이라고 알려져있습니다.\n\nWe will tune the hyper-parameters for the 2 best classifiers i.e the SVM and RandomForests.\n- 우리는 이제 2개의 좋은 분류기인 SVM과 RandomForests의 하이퍼 파라밑터를 튜닝해볼 것입니다.\n#### SVM"},{"metadata":{"_uuid":"27cec77e7540f00f14f16ad654dbc3b285979450","_cell_guid":"b55abfe7-c4be-4712-b4ed-3447f18b9503","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nC=[0.05,0.1,0.2,0.3,0.25,0.4,0.5,0.6,0.7,0.8,0.9,1]\ngamma=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\nkernel=['rbf','linear']\nhyper={'kernel':kernel,'C':C,'gamma':gamma}\ngd=GridSearchCV(estimator=svm.SVC(),param_grid=hyper,verbose=True)\ngd.fit(X,Y)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f5139677175baefbada84f8e8b080ab9ca82cad9","_cell_guid":"0105b9ed-30bd-45b6-81ab-058d4b24055a"},"cell_type":"markdown","source":"#### Random Forests"},{"metadata":{"_uuid":"73b496e28b85d890b03290352264fa5eb32e2075","_cell_guid":"651ec70e-9823-401f-8e7d-62eb29c1c3a7","trusted":true},"cell_type":"code","source":"n_estimators=range(100,1000,100)\nhyper={'n_estimators':n_estimators}\ngd=GridSearchCV(estimator=RandomForestClassifier(random_state=0),param_grid=hyper,verbose=True)\ngd.fit(X,Y)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a37e3a5077f20b3fc2fb55141a6ca4adcd58a053","_cell_guid":"849fb470-3b99-4a8f-99ca-1117bd913347"},"cell_type":"markdown","source":"The best score for Rbf-Svm is **82.82% with C=0.05 and gamma=0.1**.\nFor RandomForest, score is abt **81.8% with n_estimators=900**.\n- Rbf-SVM의 가장 좋은 점수는 **82.82%로 C=0.05, gamma=0.1**일때 였습니다. RandomForest의 경우 **n_esstimators=900일때 81.8%**로 가장 좋았습니다."},{"metadata":{"_uuid":"ddf9e42f2103a765a2b9ce0f52ac67478c36164c","_cell_guid":"108536a3-68e9-4abc-8dc7-c98b801a3386"},"cell_type":"markdown","source":"# Ensembling / 앙상블\n\nEnsembling is a good way to increase the accuracy or performance of a model. In simple words, it is the combination of various simple models to create a single powerful model.\n- 앙상블은 모델의 정확성과 성능을 향상시키는데 좋은 방법입니다. 간단하게 간단한 모델을 합쳐서 하나의 강력한 모델을 만드는 것입니다.\n\nLets say we want to buy a phone and ask many people about it based on various parameters. So then we can make a strong judgement about a single product after analysing all different parameters. This is **Ensembling**, which improves the stability of the model. \n- 만약 우리가 핸드폰을 구매하고 싶어 많은 사람들에게 여러가지 파라미터에 근거해서 물어볼 것입니다. 모두 다른 파라미터를 분석한 뒤에 우리는 한 제품에 대해 강한 판단을 내릴 수 있습니다. 이것이 모델의 견고성을 개선하는 \"앙상블\"입니다.\n\nEnsembling can be done in ways like:\n- 앙상블은 다음의 순서로 진행됩니다:\n\n1)Voting Classifier\n- 1) 보팅 분류기\n\n2)Bagging\n- 2) 배깅\n\n3)Boosting.\n- 3) 부스팅"},{"metadata":{"_uuid":"22670e16c173f051f1c2eba96a4faa83fc87053b","_cell_guid":"8c7f49d7-8986-4c75-816d-a82e1e000e34"},"cell_type":"markdown","source":"## Voting Classifier / 보팅 분류기\n\nIt is the simplest way of combining predictions from many different simple machine learning models. It gives an average prediction result based on the prediction of all the submodels. The submodels or the basemodels are all of diiferent types.\n- 이는 가장 간단하게 여러 간단한 기계학습 모델들의 예측을 합하는 방법입니다. 이는 하위모델들의 예측 결과를 바탕으로 평균 예측 결과를 제공합니다. 하위모델 또는 베이스모델은 모두 다른 타입입니다."},{"metadata":{"_uuid":"3fde83a97a3f8ec941901886b1694941a77b740c","_cell_guid":"0fb4987a-e837-4dd6-89a5-ac8d9c47bb8c","trusted":true},"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\nensemble_lin_rbf=VotingClassifier(estimators=[('KNN',KNeighborsClassifier(n_neighbors=10)),\n                                              ('RBF',svm.SVC(probability=True,kernel='rbf',C=0.5,gamma=0.1)),\n                                              ('RFor',RandomForestClassifier(n_estimators=500,random_state=0)),\n                                              ('LR',LogisticRegression(C=0.05)),\n                                              ('DT',DecisionTreeClassifier(random_state=0)),\n                                              ('NB',GaussianNB()),\n                                              ('svm',svm.SVC(kernel='linear',probability=True))\n                                             ], \n                       voting='soft').fit(train_X,train_Y)\nprint('The accuracy for ensembled model is:',ensemble_lin_rbf.score(test_X,test_Y))\ncross=cross_val_score(ensemble_lin_rbf,X,Y, cv = 10,scoring = \"accuracy\")\nprint('The cross validated score is',cross.mean())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8b1d6e720f77c19f785cb28d5bcda57eeb57e10b","_cell_guid":"79afa502-8bfd-416e-95dc-1297b9e42a39"},"cell_type":"markdown","source":"## Bagging / 배깅\n\nBagging is a general ensemble method. It works by applying similar classifiers on small partitions of the dataset and then taking the average of all the predictions. Due to the averaging,there is reduction in variance. Unlike Voting Classifier, Bagging makes use of similar classifiers.\n- 배깅은 일반적인 앙상블 모델입니다. 이 모델은 데이터 세트의 작은 부분에 비슷한 분류기를 적용한 다음 모든 예측의 평균을 취함으로써 작동합니다. 평균화에 의해, 분산이 감소합니다. 보팅 분류기와 다르게, 배깅은 비슷한슷한 분류기를 사용합니다.\n\n#### Bagged KNN / 배깅된 KNN\n\nBagging works best with models with high variance. An example for this can be Decision Tree or Random Forests. We can use KNN with small value of **n_neighbours**, as small value of n_neighbours.\n- 배깅은 높은 분산을 갖고있는 모델에서 가장 잘 작동합니다. 그 예로 결정트리와 Random Forests 모델이 있습니다. 우리는 작은 n_이웃들의 값을 갖고 있는 KNN을 적은 값의 n_이웃들로 사용할 수 있습니다."},{"metadata":{"_uuid":"60c5490b5804a9679629dc819caa2d8e18a7893b","_cell_guid":"aa9aa59c-417e-430a-90be-ff28f463c124","trusted":true},"cell_type":"code","source":"from sklearn.ensemble import BaggingClassifier\nmodel=BaggingClassifier(base_estimator=KNeighborsClassifier(n_neighbors=3),random_state=0,n_estimators=700)\nmodel.fit(train_X,train_Y)\nprediction=model.predict(test_X)\nprint('The accuracy for bagged KNN is:',metrics.accuracy_score(prediction,test_Y))\nresult=cross_val_score(model,X,Y,cv=10,scoring='accuracy')\nprint('The cross validated score for bagged KNN is:',result.mean())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"76e78fa11fc31aba7f0840fdfd837c1ada8b14c9","_cell_guid":"56a42f5c-da5e-4568-8c81-214353999328"},"cell_type":"markdown","source":"#### Bagged DecisionTree\n"},{"metadata":{"_uuid":"5b31f702c3ba63fefab40b3367b22fbf6f1a8f6d","_cell_guid":"477cf946-fb66-42c0-a824-63dced016235","trusted":true},"cell_type":"code","source":"model=BaggingClassifier(base_estimator=DecisionTreeClassifier(),random_state=0,n_estimators=100)\nmodel.fit(train_X,train_Y)\nprediction=model.predict(test_X)\nprint('The accuracy for bagged Decision Tree is:',metrics.accuracy_score(prediction,test_Y))\nresult=cross_val_score(model,X,Y,cv=10,scoring='accuracy')\nprint('The cross validated score for bagged Decision Tree is:',result.mean())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8d05183dac7f4c41ddeda980fe5fcf83caf3ef45","_cell_guid":"eeed9cf6-c417-44f8-8feb-6f79611e5efe"},"cell_type":"markdown","source":"## Boosting / 부스팅\n\nBoosting is an ensembling technique which uses sequential learning of classifiers. It is a step by step enhancement of a weak model.Boosting works as follows:\n- 부스팅은 절차적인 학습과 분류기를 사용하는 앙상블 기술입니다. 이것은 약한 모델들의 한단계 한단계 개선입니다. 부스팅은 다음에 따라 작동합니다:\n\nA model is first trained on the complete dataset. Now the model will get some instances right while some wrong. Now in the next iteration, the learner will focus more on the wrongly predicted instances or give more weight to it. Thus it will try to predict the wrong instance correctly. Now this iterative process continous, and new classifers are added to the model until the limit is reached on the accuracy.\n- 모델은 가장 먼저 완성된 데이터셋으로 훈련됩니다. 이제 모델은 몇몇 인스턴스는 맞게 취할 것이고, 몇몇은 틀리게 취할것입니다. 다음 반복에서는 잘못 예측한 인스턴스에 집중하거나 더 많은 가중치를 부여합니다. 모델은 잘못 예측한 인스턴스를 맞게 예측하려 할 것입니다. 이런 반복 과정이 계속 되면, 정확도의 한계에 도달할때까지 새로운 분류기 모델이 추가됩니다."},{"metadata":{"_uuid":"1b36f078809acf52842a01c7de5afbbe8000badc","_cell_guid":"5fd768e5-78ce-45b4-a015-2994cd003de9"},"cell_type":"markdown","source":"#### AdaBoost(Adaptive Boosting) / 에이다부스트(적응형 부스팅)\n\nThe weak learner or estimator in this case is a Decsion Tree.  But we can change the dafault base_estimator to any algorithm of our choice.\n- 여기서 약한 학습자와 평가자는 결정 트리입니다. 우리는 기본 base_estimator를 우리가 정하는 알고리즘으로 바꿀 수 있습니다."},{"metadata":{"_uuid":"32f72bdb2b9b054f7b7ef839dab62e430a2d050d","_cell_guid":"a0a08d75-57bd-4c7d-a3b6-68cba2ba915b","trusted":true},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\nada=AdaBoostClassifier(n_estimators=200,random_state=0,learning_rate=0.1)\nresult=cross_val_score(ada,X,Y,cv=10,scoring='accuracy')\nprint('The cross validated score for AdaBoost is:',result.mean())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b89e596f6b3fbc3441abf75e676060c12289b26e","_cell_guid":"898b737c-37a4-4c5b-a745-e39351a0b790"},"cell_type":"markdown","source":"#### Stochastic Gradient Boosting / 확률적 그라디언트 부스팅 \n\nHere too the weak learner is a Decision Tree.\n- 여기서도 약한 학습자는 결정 트리 입니다."},{"metadata":{"_uuid":"a7aaea0c740932c4d7248fab80012553feefb0e2","_cell_guid":"b41cd90a-04d4-4e8b-afe3-c50a66dda140","trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\ngrad=GradientBoostingClassifier(n_estimators=500,random_state=0,learning_rate=0.1)\nresult=cross_val_score(grad,X,Y,cv=10,scoring='accuracy')\nprint('The cross validated score for Gradient Boosting is:',result.mean())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f1357e5586025b3ed8facde947ca86802f4815b5","_cell_guid":"21565e7d-82bc-462f-b50c-4486a2a1dc69"},"cell_type":"markdown","source":"#### XGBoost / XG부스트"},{"metadata":{"_uuid":"a0a6b1f8b56a578a1a5e90da72e7e7a31f5ef1f3","_cell_guid":"a0f54823-8c1a-4287-845a-cf2749bbd243","trusted":true},"cell_type":"code","source":"import xgboost as xg\nxgboost=xg.XGBClassifier(n_estimators=900,learning_rate=0.1)\nresult=cross_val_score(xgboost,X,Y,cv=10,scoring='accuracy')\nprint('The cross validated score for XGBoost is:',result.mean())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"750f3084480a868530e2b788edc91c54dfc5ac8d","_cell_guid":"6c163453-2257-487a-b9f1-df4cc159f2ae"},"cell_type":"markdown","source":"We got the highest accuracy for AdaBoost. We will try to increase it with Hyper-Parameter Tuning\n- 우리는 에이다부스트에서 가장 높은 정확도를 보였습니다. 하이퍼-파리미터 튜닝을 통해서 더 높여보도록 하겠습니다.\n\n#### Hyper-Parameter Tuning for AdaBoost / 에이다부스트 하이퍼-파리미터 튜닝"},{"metadata":{"_uuid":"4fcae3c275c3a8a5618727b2d526c996aacb7c5e","_cell_guid":"5cf49532-71b9-4d67-911a-581ddfc47730","trusted":true},"cell_type":"code","source":"n_estimators=list(range(100,1100,100))\nlearn_rate=[0.05,0.1,0.2,0.3,0.25,0.4,0.5,0.6,0.7,0.8,0.9,1]\nhyper={'n_estimators':n_estimators,'learning_rate':learn_rate}\ngd=GridSearchCV(estimator=AdaBoostClassifier(),param_grid=hyper,verbose=True)\ngd.fit(X,Y)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7cbeb738e5cb865d90753e77f12dae18d3d06a65","_cell_guid":"3736f533-0a16-4881-99d2-3a7275935d47"},"cell_type":"markdown","source":"The maximum accuracy we can get with AdaBoost is **83.16% with n_estimators=200 and learning_rate=0.05**\n- 에이다 부스트로 얻은 최대 정확도는 **83.16%로 n_estimator=200 그리고 learning_rate=0.05**일때 였습니다."},{"metadata":{"_uuid":"107b5a93285d783a08beb89795c21a665d7dbb7c","_cell_guid":"e1cb4442-2ddd-4854-9e62-7ff4543e8c9c"},"cell_type":"markdown","source":"### Confusion Matrix for the Best Model / 베스트 모델의 컨퓨전 행렬"},{"metadata":{"_uuid":"3d9debcdbd9d7b9c4e88c214385e8a6b5ba7f8fa","_cell_guid":"337cee47-1ebb-4ac3-866e-f087bed5c4a3","trusted":true},"cell_type":"code","source":"ada=AdaBoostClassifier(n_estimators=200,random_state=0,learning_rate=0.05)\nresult=cross_val_predict(ada,X,Y,cv=10)\nsns.heatmap(confusion_matrix(Y,result),cmap='winter',annot=True,fmt='2.0f')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dbc679469ae2c8d73b1d42d56a36c390fc12017e","_cell_guid":"1daa7ac2-03e6-4b87-b627-f73a03eb6c35"},"cell_type":"markdown","source":"## Feature Importance / 피처 중요도"},{"metadata":{"_uuid":"1c11bc8983157d3041b2144fffe0bb27992aa9a1","_cell_guid":"1aa1f4b8-f625-4fad-9f70-8b61c39a7f1a","trusted":true},"cell_type":"code","source":"f,ax=plt.subplots(2,2,figsize=(15,12))\nmodel=RandomForestClassifier(n_estimators=500,random_state=0)\nmodel.fit(X,Y)\npd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[0,0])\nax[0,0].set_title('Feature Importance in Random Forests')\nmodel=AdaBoostClassifier(n_estimators=200,learning_rate=0.05,random_state=0)\nmodel.fit(X,Y)\npd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[0,1],color='#ddff11')\nax[0,1].set_title('Feature Importance in AdaBoost')\nmodel=GradientBoostingClassifier(n_estimators=500,learning_rate=0.1,random_state=0)\nmodel.fit(X,Y)\npd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[1,0],cmap='RdYlGn_r')\nax[1,0].set_title('Feature Importance in Gradient Boosting')\nmodel=xg.XGBClassifier(n_estimators=900,learning_rate=0.1)\nmodel.fit(X,Y)\npd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[1,1],color='#FD0F00')\nax[1,1].set_title('Feature Importance in XgBoost')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"03663c4786700a6d4083b4aa1a0f6d913725fefb","_cell_guid":"57661993-cb09-4eac-b4f4-8666f3e1a1b7"},"cell_type":"markdown","source":"We can see the important features for various classifiers like RandomForests, AdaBoost,etc.\n- 우리는 각각의 분류기별로 중요한 피처들을 볼 수 있습니다. (랜덤 포레스트, 에이다부스트 등)\n\n#### Observations: 분석결과\n\n1)Some of the common important features are Initial,Fare_cat,Pclass,Family_Size.\n- 1)몇가지 중요한 피처들은 이니셜, 비용_카테고리, Pclass(등석), 가족크기임.\n\n2)The Sex feature doesn't seem to give any importance, which is shocking as we had seen earlier that Sex combined with Pclass was giving a very good differentiating factor. Sex looks to be important only in RandomForests.\n- 2) 성별 피처는 놀랍게도 그렇게 중요도가 높지 않음.. 우리가 앞서 Pclass와 결합된 성별은 좋은 차별화된 요소를 제공한다는 것을 봤기 때문. 성별은 랜덤 포레스트에서만 중요한것 같음.\n \nHowever, we can see the feature Initial, which is at the top in many classifiers. We had already seen the positive correlation between Sex and Initial, so they both refer to the gender.\n- 그나저나, 우리는 많은 분류기에서 이니셜 피처가 중요하다고 하는 것을 볼 수 있음. 우리는 이미 이니셜과 성별의 상호관계성을 보았기 때문에 두가지 모두 성별을 나타냄을 알 수 있음.\n\n3)Similarly the Pclass and Fare_cat refer to the status of the passengers and Family_Size with Alone,Parch and SibSp.\n- 3) 비슷하게 Pclass와 비용_카테고리는 승객의 상태를 나타내고, 가족_크기는 혼자, 배우자, 형제자매를 나타냄."},{"metadata":{"_uuid":"9b10931b25a97196ebc0496f6d72246a75ad1349","_cell_guid":"bef630bd-03ef-476e-a805-ac81b9ca3f54"},"cell_type":"markdown","source":"I hope all of you did gain some insights to Machine Learning. Some other great notebooks for Machine Learning are:\n- 모두가 머신 러닝에 인사이트가 있었으면 좋겠습니다. 머신러닝과 관련된 새로운 노트북들:\n\n1) For R:[Divide and Conquer by Oscar Takeshita](https://www.kaggle.com/pliptor/divide-and-conquer-0-82297/notebook)\n\n2)For Python:[Pytanic by Heads and Tails](https://www.kaggle.com/headsortails/pytanic)\n\n3)For Python:[Introduction to Ensembling/Stacking by Anisotropic](https://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python)\n\n### Thanks a lot for having a look at this notebook. If you found this notebook useful, **Do Upvote**.\n### 이 노트북을 봐주셔서 감사합니다. 만약 도움이 되었다면 **Upvote** 부탁드립니다.\n"}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}