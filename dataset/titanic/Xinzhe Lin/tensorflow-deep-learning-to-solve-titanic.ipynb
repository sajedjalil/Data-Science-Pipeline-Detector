{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e09f3e86-e94b-b5a7-d051-586de4898f7d"},"outputs":[],"source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"009db3a9-6481-d7a3-9895-2827bdab5aed"},"outputs":[],"source":"# load data\ntrain_data = pd.read_csv(r\"../input/train.csv\")\ntest_data = pd.read_csv(r\"../input/test.csv\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8e070496-f890-da5b-819d-ecae41505416"},"outputs":[],"source":"train_data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4690d5f5-1d18-b9d0-7fca-31494a4132fd"},"outputs":[],"source":"test_data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0e2e2685-776a-3e19-f6f3-1188363752c3"},"outputs":[],"source":"# Feature Engineering\nfrom sklearn.preprocessing import Imputer\n\ndef nan_padding(data, columns):\n    for column in columns:\n        imputer=Imputer()\n        data[column]=imputer.fit_transform(data[column].values.reshape(-1,1))\n    return data\n\n\nnan_columns = [\"Age\", \"SibSp\", \"Parch\"]\n\ntrain_data = nan_padding(train_data, nan_columns)\ntest_data = nan_padding(test_data, nan_columns)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"724e35ee-418d-0ef6-e3d6-25dd158a1203"},"outputs":[],"source":"train_data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d9e85bf0-a7eb-71f0-5102-ba0c66a9f36f"},"outputs":[],"source":"#save PassengerId for evaluation\ntest_passenger_id=test_data[\"PassengerId\"]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"69f17817-2a9e-9c5d-e460-559588c4d316"},"outputs":[],"source":"def drop_not_concerned(data, columns):\n    return data.drop(columns, axis=1)\n\nnot_concerned_columns = [\"PassengerId\",\"Name\", \"Ticket\", \"Fare\", \"Cabin\", \"Embarked\"]\ntrain_data = drop_not_concerned(train_data, not_concerned_columns)\ntest_data = drop_not_concerned(test_data, not_concerned_columns)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b642846b-31a6-1619-5b88-17ca2c908d6a"},"outputs":[],"source":"train_data.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"27ad4c12-bcde-78ff-032b-ea58ae86fed6"},"outputs":[],"source":"test_data.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a50e8d60-454e-b402-6953-21cd95d82f33"},"outputs":[],"source":"def dummy_data(data, columns):\n    for column in columns:\n        data = pd.concat([data, pd.get_dummies(data[column], prefix=column)], axis=1)\n        data = data.drop(column, axis=1)\n    return data\n\n\ndummy_columns = [\"Pclass\"]\ntrain_data=dummy_data(train_data, dummy_columns)\ntest_data=dummy_data(test_data, dummy_columns)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3ce32d22-6e51-ce31-fe73-5b349aa6b894"},"outputs":[],"source":"test_data.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"93cd9d05-6ab1-42c2-6222-f16ae21be706"},"outputs":[],"source":"from sklearn.preprocessing import LabelEncoder\ndef sex_to_int(data):\n    le = LabelEncoder()\n    le.fit([\"male\",\"female\"])\n    data[\"Sex\"]=le.transform(data[\"Sex\"]) \n    return data\n\ntrain_data = sex_to_int(train_data)\ntest_data = sex_to_int(test_data)\ntrain_data.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a6d562ac-cfe1-9ce2-53fa-4b2789812d9f"},"outputs":[],"source":"from sklearn.preprocessing import MinMaxScaler\n\ndef normalize_age(data):\n    scaler = MinMaxScaler()\n    data[\"Age\"] = scaler.fit_transform(data[\"Age\"].values.reshape(-1,1))\n    return data\ntrain_data = normalize_age(train_data)\ntest_data = normalize_age(test_data)\ntrain_data.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6de2351b-7efb-f681-7b7b-74a6e9d5883d"},"outputs":[],"source":"from sklearn.preprocessing import LabelBinarizer\nfrom sklearn.model_selection import train_test_split\n\ndef split_valid_test_data(data, fraction=(1 - 0.8)):\n    data_y = data[\"Survived\"]\n    lb = LabelBinarizer()\n    data_y = lb.fit_transform(data_y)\n\n    data_x = data.drop([\"Survived\"], axis=1)\n\n    train_x, valid_x, train_y, valid_y = train_test_split(data_x, data_y, test_size=fraction)\n\n    return train_x.values, train_y, valid_x, valid_y\n\ntrain_x, train_y, valid_x, valid_y = split_valid_test_data(train_data)\nprint(\"train_x:{}\".format(train_x.shape))\nprint(\"train_y:{}\".format(train_y.shape))\nprint(\"train_y content:{}\".format(train_y[:3]))\n\nprint(\"valid_x:{}\".format(valid_x.shape))\nprint(\"valid_y:{}\".format(valid_y.shape))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b18da52c-396a-466c-b13c-3d9c55ab7ea5"},"outputs":[],"source":"# Build Neural Network\nfrom collections import namedtuple\n\ndef build_neural_network(hidden_units=10):\n    tf.reset_default_graph()\n    inputs = tf.placeholder(tf.float32, shape=[None, train_x.shape[1]])\n    labels = tf.placeholder(tf.float32, shape=[None, 1])\n    learning_rate = tf.placeholder(tf.float32)\n    is_training=tf.Variable(True,dtype=tf.bool)\n    \n    initializer = tf.contrib.layers.xavier_initializer()\n    fc = tf.layers.dense(inputs, hidden_units, activation=None,kernel_initializer=initializer)\n    fc=tf.layers.batch_normalization(fc, training=is_training)\n    fc=tf.nn.relu(fc)\n    \n    logits = tf.layers.dense(fc, 1, activation=None)\n    cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=labels, logits=logits)\n    cost = tf.reduce_mean(cross_entropy)\n    \n    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n\n    predicted = tf.nn.sigmoid(logits)\n    correct_pred = tf.equal(tf.round(predicted), labels)\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n\n    # Export the nodes \n    export_nodes = ['inputs', 'labels', 'learning_rate','is_training', 'logits',\n                    'cost', 'optimizer', 'predicted', 'accuracy']\n    Graph = namedtuple('Graph', export_nodes)\n    local_dict = locals()\n    graph = Graph(*[local_dict[each] for each in export_nodes])\n\n    return graph\n\nmodel = build_neural_network()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"55b103de-7ae0-389a-7b8a-0452fbef28c1"},"outputs":[],"source":"def get_batch(data_x,data_y,batch_size=32):\n    batch_n=len(data_x)//batch_size\n    for i in range(batch_n):\n        batch_x=data_x[i*batch_size:(i+1)*batch_size]\n        batch_y=data_y[i*batch_size:(i+1)*batch_size]\n        \n        yield batch_x,batch_y"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"27030dd0-a904-b53e-6165-2d7d7e4ee608"},"outputs":[],"source":"epochs = 200\ntrain_collect = 50\ntrain_print=train_collect*2\n\nlearning_rate_value = 0.001\nbatch_size=16\n\nx_collect = []\ntrain_loss_collect = []\ntrain_acc_collect = []\nvalid_loss_collect = []\nvalid_acc_collect = []\n\nsaver = tf.train.Saver()\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    iteration=0\n    for e in range(epochs):\n        for batch_x,batch_y in get_batch(train_x,train_y,batch_size):\n            iteration+=1\n            feed = {model.inputs: train_x,\n                    model.labels: train_y,\n                    model.learning_rate: learning_rate_value,\n                    model.is_training:True\n                   }\n\n            train_loss, _, train_acc = sess.run([model.cost, model.optimizer, model.accuracy], feed_dict=feed)\n            \n            if iteration % train_collect == 0:\n                x_collect.append(e)\n                train_loss_collect.append(train_loss)\n                train_acc_collect.append(train_acc)\n\n                if iteration % train_print==0:\n                     print(\"Epoch: {}/{}\".format(e + 1, epochs),\n                      \"Train Loss: {:.4f}\".format(train_loss),\n                      \"Train Acc: {:.4f}\".format(train_acc))\n                        \n                feed = {model.inputs: valid_x,\n                        model.labels: valid_y,\n                        model.is_training:False\n                       }\n                val_loss, val_acc = sess.run([model.cost, model.accuracy], feed_dict=feed)\n                valid_loss_collect.append(val_loss)\n                valid_acc_collect.append(val_acc)\n                \n                if iteration % train_print==0:\n                    print(\"Epoch: {}/{}\".format(e + 1, epochs),\n                      \"Validation Loss: {:.4f}\".format(val_loss),\n                      \"Validation Acc: {:.4f}\".format(val_acc))\n                \n\n    saver.save(sess, \"./titanic.ckpt\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8ed01203-e9b6-423d-045d-11e6194a59f9"},"outputs":[],"source":"plt.plot(x_collect, train_loss_collect, \"r--\")\nplt.plot(x_collect, valid_loss_collect, \"g^\")\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"26749357-f1eb-8112-2f5f-7b15134f31e6"},"outputs":[],"source":"plt.plot(x_collect, train_acc_collect, \"r--\")\nplt.plot(x_collect, valid_acc_collect, \"g^\")\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e1336e1c-7d5e-c200-bcbb-24a7bb8d6bb7"},"outputs":[],"source":"model=build_neural_network()\nrestorer=tf.train.Saver()\nwith tf.Session() as sess:\n    restorer.restore(sess,\"./titanic.ckpt\")\n    feed={\n        model.inputs:test_data,\n        model.is_training:False\n    }\n    test_predict=sess.run(model.predicted,feed_dict=feed)\n    \ntest_predict[:10]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cc7801f4-88e2-9140-5313-17c444506dde"},"outputs":[],"source":"from sklearn.preprocessing import Binarizer\nbinarizer=Binarizer(0.5)\ntest_predict_result=binarizer.fit_transform(test_predict)\ntest_predict_result=test_predict_result.astype(np.int32)\ntest_predict_result[:10]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"461ee38a-4b8f-a1af-4448-d2503ccc9be3"},"outputs":[],"source":"passenger_id=test_passenger_id.copy()\nevaluation=passenger_id.to_frame()\nevaluation[\"Survived\"]=test_predict_result\nevaluation[:10]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"15867098-96d8-9878-83b1-9741711928cc"},"outputs":[],"source":"evaluation.to_csv(\"evaluation_submission.csv\",index=False)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}