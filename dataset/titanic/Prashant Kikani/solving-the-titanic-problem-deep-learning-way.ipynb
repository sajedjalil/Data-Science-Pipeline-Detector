{"cells":[{"metadata":{},"cell_type":"markdown","source":"Today in this lesson, we will try to solve the same Titanic problem using Deep Learning i.e. using artificial neural networks.<br>\n\nTo recap the problem definition,<br>\nIn this problem, we have some data about each passenger that were into Titanic ship.<br>\n**Our problem is to predict or forecast, whether this person will survive the ship sinking or not**.<br><br>\n\n## Goal of this lesson is to learn basics of deep learning with hands-on experience.\n\nWe will follow the same steps as previous lesson to keep it simple.<br>\n1. Open the data files.\n2. Understand the data. What each column in the table means.\n3. Preprocess data\n    * Remove the outliers.\n    * Fill `NaN` or `null` values. Sometimes, we also remove all the rows with `NaN` values.\n    * Feature engineering - Create new columns out of existing columns using our understanding.\n    * Converting data into numeric form if it's not.\n4. Train a machine learning model.\n5. Validate the trained model i.e. checking it's performance on unseen data.\n6. If it performs good in validation, use model to predict future real world data.\n\nFirst of all, let's get familier with Deep Learning.<br>\n\n# What is Deep Learning?\n> Deep learning is a subfield of machine learning; which only uses one specific type of model called artificial neural networks to capture patterns & remember them.\n\n![ANN](https://blog.knoldus.com/wp-content/uploads/2017/06/ann-vs-bnn.jpg)\n<center>A biological neuron on the left & a simple artificial neural network on the right.</center>\n<br><br>\nIn above figure,\nLeft part contains our biological brain‚Äôs neuron. <br>\nOur brain is made from billions of those.<br>\nRight part is a sample artificial neural network.<br>\nEach circle is a neuron.<br><br>\n\n> In artificial neural networks, each neuron is fully or partially connected with other neurons.<br>\n\nGoing from left to right in artificial neural network from the above figure,<br>\nFirst layer is called the ‚Äú**input**‚Äù layer,and the last layer is called the‚Äú**output**‚Äù layer.<br>\nEasy enough.\n\nAnd middle layers are called ‚Äúhidden‚Äù layers.<br><br>\n\n### Why hidden?<br>\nBecause they are not visible from outside. Only input & output layers are visible. They are in between input & output layers.<br>\n\n## So, what are artificial neural networks then?\nIt's kind of a copy of the human brain itself, but on a very tiny scale.<br>\nYes. You read it right.<br>\nCore idea of neural networks is inspired by our own biological brain.<br>\nNeural networks are, well, network of neurons !<br>\nThey are called ‚Äúartificial‚Äù as it‚Äôs created by humans not biology.<br>\nThey are not natural.<br>\n\nEach neuron is connected with some other neurons & by this, a network is formed from neurons. Which is called a neural network.\n\n## What is ‚Äúdeep‚Äù in deep learning?\nWe use the term ‚Äúdeep‚Äù as there can be multiple middle / hidden layers in a neural network. <br>\nSo, because of those multiple layers, our network becomes deep.<br>\nThat‚Äôs why people call those neural networks ‚Äúdeep‚Äù neural networks.<br>\n\nDeep learning is responsible for exciting technologies like image recognition, natural language processing, speech recognition etc.<br>\n\n### Roots of deep learning field are highly dependent on a branch of mathematics called ‚Äú[calculus](https://en.wikipedia.org/wiki/Calculus)‚Äù.\nYou might have heard terms like ‚Äú**differentiation**‚Äù or ‚Äú**integration**‚Äù etc. <br>\nThose are some things which come under this calculus field.<br>\nIn deep learning we use differentiation to teach our neural network. <br>\nThings get a bit mathematical from this point.<br><br>\n\n### Enough of verbal learning.<br>\nLet's learn by code !<br>\n\n## We'll follow the same steps as previous lesson to keep it simple & to allow us to focus on deep learning part.<br>\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# import necessary libraries first\n\n# pandas to open data files & processing it.\nimport pandas as pd\n# to see all columns\npd.set_option('display.max_columns', None)\n\n# numpy for numeric data processing\nimport numpy as np\n\n# sklearn to do preprocessing & ML models\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# keras for deep learning model creation\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Dropout, BatchNormalization, Activation\nfrom keras.optimizers import Adam\nfrom keras.utils import plot_model\n\n# Matplotlob & seaborn to plot graphs & visulisation\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\n# for fixing the random seed\nimport random\nimport os, tensorflow as tf\nimport torch\n\n# ignore warnings\nimport warnings\nwarnings.simplefilter(action='ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Open the data files."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"titanic_data = pd.read_csv(\"../input/titanic/train.csv\")\ntitanic_data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, we have total of 891 rows & 12 columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"titanic_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Understand the data. What each column in the table means.\n\n**Each row in above table contains data of a passenger.<br>**\nThose details include following columns.<br>\n\nHere is all the columns of above table mean.<br>\n\n`PassengerId` : Unique ID for each passenger.<br><br>\n`Survived` : Whether that passenger survived or not. (0 = No, 1 = Yes)<br><br>\n`Pclass` : Ticket class of passenger. (1 = Upper, 2 = Middle, 3 = Lower)<br><br>\n`Name` : Name of passenger<br><br>\n`Sex` : Gender of passenger<br><br>\n`Age` : Age of passenger<br><br>\n`SibSp` : # of siblings / spouses aboard the Titanic of passenger<br><br>\n`Parch` : # of parents / children aboard the Titanic of passenger<br><br>\n`Ticket` : Ticket number of passenger<br><br>\n`Fare` : Ticket amount / passenger fare.<br><br>\n`Cabin` : Cabin number of passenger<br><br>\n`Embarked` : Port of Embarkation of passenger. (C = Cherbourg, Q = Queenstown, S = Southampton)<br><br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"titanic_data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Let's check unique values for each column"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Survival\ntitanic_data['Survived'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, total 335 people have survived & 547 people have died in the Titanic."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ticket class\ntitanic_data['Pclass'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This tells `3` value occurs 491 times, `1` value occurs 207 times etc."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Gender\ntitanic_data['Sex'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Siblings\ntitanic_data['SibSp'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Parent or Childs\ntitanic_data['Parch'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Embarked station\ntitanic_data['Embarked'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most of the passengers have embarked from \"Cherbourg\" & \"Southampton\""},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(titanic_data['Sex']);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(titanic_data['Survived'], titanic_data['Sex']);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wow !<br>\n~75% of females have survived.<br>\nEven if total number of females are less than males.<br><br>\n\nMay be because, females were given more priority in lifeboats than males. May be."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(titanic_data['Survived'], titanic_data['Fare'], titanic_data['Pclass']);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"People with higher class have higher chances of survival !"},{"metadata":{},"cell_type":"markdown","source":"# 3. Preprocess data\n\nIn preprocessing step, we detect outliers & remove them from our data.\n\n## 3.1\n\n## What is an outlier in data? Why does it occur?\nOutliers are as the name suggests, very different from general / normal trend.<br>\nThey occur in data because of some faults in data collection pipeline.<br><br>\n\n## Why we generally remove outliers?\nBecause one big outlier can mess up whole model's performance.<br>\nEven if all other contributions might be of a low value, one high outlier value already shifts the entire gradient towards higher values as well.<br>\n\nMost of the time, we remove outliers so that, we can train our model only from general trends.<br>\nLet's see some examples using our titanic data.<br>\n\n### One common practice followed to detect outliers is BoxPlot."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x=titanic_data[\"Fare\"])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see, for majority of passengers, `Fare` price is less than 250.<br>\nSo, let's only keep the rows with `Fare` < 250."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Only take rows which have \"Fare\" value less than 250.\ntitanic_data = titanic_data[titanic_data['Fare'] < 250]\ntitanic_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"So, we have removed 9 rows.<br>Originally, there were 891 rows."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x=titanic_data[\"Age\"])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.2 Fill NaN or null values in data\n\n### Why NaN (not a number) values occur in data?\nSometimes, while collecting data, if some information is missing for some rows, it's filled as NaN.<br>\nIt means nothing is there.<br>\nIt's empty.<br>\n\n### How NaN values can be handled?\nThere are several methods.\n* Fill a specified value like \"EMPTY\" or -1 for all the NaN values.\n    * This option is good for categorical type columns / features.\n* If column is numeric in nature, fill with mean or median of that specific column.\n    * This option is good for numerical type columns / features.\n* Remove all the raws who have atleast 1 NaN value in any column.\n    * If total number of raws with NaN values is less, we can just remove those rows from our data.\n\nLet's look if there are any missing values in our data."},{"metadata":{"trusted":true},"cell_type":"code","source":"titanic_data.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 177 NaN values in Age & 686 NaN values in Cabin column.<br>\nIn Cabin more than 75% values are empty.<br>\nSo, we will just remove that column."},{"metadata":{"trusted":true},"cell_type":"code","source":"titanic_data.drop(\"Cabin\", axis=1, inplace=True)\ntitanic_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"titanic_data.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see `Cabin` column is removed from our data.<br><br>\nNow, `Age` is a numeric column.<br>\nSo, let's fill NaN values by mean of all the other non-NaN values."},{"metadata":{"trusted":true},"cell_type":"code","source":"age_mean = titanic_data['Age'].mean()\nprint(age_mean)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can fill all the NaN values using `fillna` "},{"metadata":{"trusted":true},"cell_type":"code","source":"titanic_data['Age'].fillna(age_mean, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"titanic_data.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are just 2 NaN values in `Embarked` column.<br>\nWe handle NaN values in `Embarked` column by filling most occuring value in that column."},{"metadata":{"trusted":true},"cell_type":"code","source":"titanic_data['Embarked'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"titanic_data['Embarked'].fillna(\"S\", inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"titanic_data.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we can see, no NaN values are there in our whole data."},{"metadata":{},"cell_type":"markdown","source":"Next step is **Feature Engineering**\n\n## 3.3\n\n### What is Feature Engineering?\n> Feature Engineering is creating more meaningful data out of existing data using our domain knowledge & comman sense.<br>\n\nIn other words, we try to create more relevant information for our ML models. <br>\nSo, that our model can capture patterns in faster & better ways.\n\n### Now, this is a creative step. We need to use brain to create relevant features in the data.\n\nLet's think."},{"metadata":{"trusted":true},"cell_type":"code","source":"titanic_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's once again look at what we have at hand.<br><br><br>\n`PassengerId` : Unique ID for each passenger.<br><br>\n`Survived` : Whether that passenger survived or not. (0 = No, 1 = Yes)<br><br>\n`Pclass` : Ticket class of passenger. (1 = Upper, 2 = Middle, 3 = Lower)<br><br>\n`Name` : Name of passenger<br><br>\n`Sex` : Gender of passenger<br><br>\n`Age` : Age of passenger<br><br>\n`SibSp` : # of siblings / spouses aboard the Titanic of passenger<br><br>\n`Parch` : # of parents / children aboard the Titanic of passenger<br><br>\n`Ticket` : Ticket number of passenger<br><br>\n`Fare` : Ticket amount / passenger fare.<br><br>\n`Embarked` : Port of Embarkation of passenger. (C = Cherbourg, Q = Queenstown, S = Southampton)<br><br>\n\n### How can we use these columns to create more relevant information?\n\nLet's use `SibSp` & `Parch` to create a `total_family_members` feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"titanic_data['total_family_members'] = titanic_data['Parch'] + titanic_data['SibSp'] + 1\n\n# if total family size is 1, person is alone.\ntitanic_data['is_alone'] = titanic_data['total_family_members'].apply(lambda x: 0 if x > 1 else 1)\n\ntitanic_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(titanic_data['total_family_members'], titanic_data['Survived'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Interesting.<br>\nPeople with total_family_members = 4 have more than 70% chances of survival !<br><br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(titanic_data['is_alone'], titanic_data['Survived'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"People with family have 20% higher chance of survival than people travelling alone !!\n\n`Age` column also can be used to create partitions.<br>\nWe can use `apply` function to `Age` column to create new column `age_group`<br>\nLike.."},{"metadata":{"trusted":true},"cell_type":"code","source":"def age_to_group(age):\n    if 0 < age < 12:\n        # children\n        return 0\n    elif 12 <= age < 50:\n        # adult\n        return 1\n    elif age >= 50:\n        # elderly people\n        return 2\n    \ntitanic_data['age_group'] = titanic_data['Age'].apply(age_to_group)\ntitanic_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Why this age_group feature is useful ?\nLet's see.."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(titanic_data['age_group'], titanic_data['Survived']);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`0` i.e. children have higher survival rate compared to adults & elderly people.<br>\nThis data may become useful to our model.<br>\n\n### Can you think of any way we can use `name` column ?\nWe can capture name title like Mr. Ms. Miss. etc."},{"metadata":{"trusted":true},"cell_type":"code","source":"titanic_data['name_title'] = titanic_data['Name'].str.extract('([A-Za-z]+)\\.', expand=False)\ntitanic_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"titanic_data['name_title'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_name_title(val):\n    if val in ['Rev', 'Col', 'Mlle', 'Mme', 'Ms', 'Sir', 'Lady', 'Don', 'Jonkheer', 'Countess', 'Capt']:\n        return 'RARE'\n    else:\n        return val\n\ntitanic_data['name_title'] = titanic_data['name_title'].apply(clean_name_title)\ntitanic_data['name_title'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(titanic_data['name_title'], titanic_data['Survived']);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"People with `Mrs` & `Miss` titles i.e. females have high chances of survival.<br>\nBut in males, with `Master` title, you have higher chances of survival !<br><br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"titanic_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's drop columns which are not useful to us as of now.<br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# save the target column \ntarget = titanic_data['Survived'].tolist()\n\ntitanic_data.drop(['PassengerId', 'Survived', 'Name', 'Ticket'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"titanic_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.4 Convert all the data into numeric form\n\nWe can see, `Sex`, `Embarked` & `name_title` are not in numeric form.<br>\nLet's convert them via LabelEncoder from sci-kit learn."},{"metadata":{"trusted":true},"cell_type":"code","source":"le = preprocessing.LabelEncoder()\ntitanic_data['Sex'] = le.fit_transform(titanic_data['Sex'])\ntitanic_data['Embarked'] = le.fit_transform(titanic_data['Embarked'])\ntitanic_data['name_title'] = le.fit_transform(titanic_data['name_title'])\ntitanic_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## One-hot encoding\nFor deep learning models, it's a commmmon practice to convert `categorical` features / columns to one-hot encoding format.<br>\n\n![one-hot](https://i.imgur.com/mtimFxh.png)\n\nOne-hot encoding makes data sparse.<br>\nIt helps neural networks to look at features individually.<br><br>\n\nWe have `Pclass`, `Sex`, `SibSp`, `Parch`, `Embarked`, `total_family_members`, \t`is_alone`, `age_group`, `name_title` categorical features.<br>\nLet's do one-hot encoding for them."},{"metadata":{"trusted":true},"cell_type":"code","source":"titanic_data = pd.get_dummies(titanic_data, columns=[\"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Embarked\", \"total_family_members\", \"is_alone\", \"age_group\", \"name_title\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"titanic_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see above, we have a seprate column for each unique values for each `categorical` feature.<br><br>\n\nNow, let's scale numerical features.<br>\n\n## Feature scaling\n\nFeature scaling becomes useful in deep learning for neumerical features.<br>\nWe do feature scaling to make all the columns of uniform scaling.<br>\nIn other words, to convert all the feature in same scale by removing unit part of it.<br><br>\nSometimes, it also helps in speeding up the calculations in an algorithm."},{"metadata":{"trusted":true},"cell_type":"code","source":"mm = preprocessing.MinMaxScaler(feature_range=(-1, 1))\ntitanic_data['Age'] = mm.fit_transform(titanic_data['Age'].to_numpy().reshape(-1, 1))\ntitanic_data['Fare'] = mm.fit_transform(titanic_data['Fare'].to_numpy().reshape(-1, 1))\ntitanic_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we have everything in numbers !!"},{"metadata":{},"cell_type":"markdown","source":"# 4. Train a machine learning model.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data, val_data, train_target, val_target = train_test_split(titanic_data, target, test_size=0.2)\ntrain_data.shape, val_data.shape, len(train_target), len(val_target)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have our training data & validation data.<br>\nWe have randomly choosen 20% of the all the rows on which we will check our model's performance.<br>\n\n### Now, let's create a neural network.."},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    tf.random.set_seed(seed)\n\n# We fix all the random seed so that, we can reproduce the results.\nseed_everything(2020)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_layer = Input(shape=(titanic_data.shape[1],),name='input_layer')\nhidden_layer_1 = Dense(32, activation = 'relu')(input_layer)\nhidden_layer_2 = Dense(16, activation = 'relu')(hidden_layer_1)\noutput_layer = Dense(1, activation = 'sigmoid')(hidden_layer_2)\n\nmodel = Model(input=input_layer, output=output_layer)\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are total `2017` parameters in our model which we will tune while back-propagation."},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_model(model, show_shapes=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is our simple neural network.<br><br>\n\nWe have 1 input layer, 2 hidden layer, 1 output layer.<br>\n\n### Via input_layer, data will go in & from output_layer, we will get the prediction from our neural network.\n\n### Let's give training to our neural network now.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# We will give training 10 times with the same data.\nEPOCHS = 10\n\n# We will process 64 rows at a time.\nBATCH_SIZE = 32\n\nmodel.fit(\n        train_data, train_target,\n        nb_epoch=EPOCHS,\n        batch_size=BATCH_SIZE,\n        validation_data=(val_data, val_target),\n        verbose = 1,\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Training is done.<br>\nWe have trainied our neural network model.<br><br>\n\n# 5. Validate the trained model i.e. checking it's performance on unseen data.\n\nIt's called \"unseen\" because our ML model have never seen this data.<br>\nIt's kind of a test for it.<br>\nWhere it's performance will be checked on data which it have never seen or train."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict labels on Validation data which model have never seen before.\n\nval_predictions = model.predict(val_data)\nlen(val_predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# first 10 values of validation_predictions\nval_predictions[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see our predictions are in float.<br>\nLet's convert them into integers.<br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"val_predictions1 = [1 if x >= 0.5 else 0 for x in val_predictions]\nval_predictions1[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate the accuracy score on validation data.\n# We already have correct target information for them.\n\naccuracy = accuracy_score(val_target, val_predictions1)\naccuracy","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cool !!"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"We got %.3f percent accuracy on our validation unseen data !!\"%(accuracy*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There's a lot can be done to improve this neural network model.<br>\nWe have build a very basic neural network model to solve this problem.<br>\nBut for an introduction lesson, this much is enough as of now."},{"metadata":{},"cell_type":"markdown","source":"# 6. If it performs good in validation, use model to predict future real world data.\n\n### Now, we can use this model to other people & predict if they were on Titanic ship in 1912 !! "},{"metadata":{},"cell_type":"markdown","source":"# Summary\n\nSo, in this lesson, we saw, what a typical pipeline looks like in solving a machine learning(ML) problem.\n\n1. Open the data files.\n2. Understand the data. What each column in the table means.\n3. Preprocess data\n    * Remove the outliers.\n    * Fill `NaN` or `null` values. Sometimes, we also remove all the rows with `NaN` values.\n    * Feature engineering - Create new columns out of existing columns using our understanding.\n    * Converting data into numeric form if it's not.\n4. Train a machine learning model.\n5. Validate the trained model i.e. checking it's performance on unseen data.\n6. If it performs good in validation, use model to predict future real world data.\n\n## Upvote this kernel if you have learned something from it.\n## Tell me if you have any kind of doubts / questions in comment section below.\n\n## In next lesson we will understand Convolutional Neural Networks(CNNs) from scratch in detail with full Python code.\n## See you in the [next lesson](https://www.kaggle.com/prashantkikani/is-it-a-cat-or-dog-convolutional-nn) üëã"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}