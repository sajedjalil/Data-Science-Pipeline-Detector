{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1><center>Titanic: efficient ensembling and optimization</center></h1>\n\n<center><img src=\"https://www.dlt.travel/immagine/33923/magazine-titanic2.jpg\"></center>"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"top\"></a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:Black; border:0' role=\"tab\" aria-controls=\"home\"><center>Quick navigation</center></h3>\n\n* [1. Feature engineering](#1)\n* [2. Single models training and optimization](#2)\n* [3. SuperLearner training and optimization](#3)\n* [4. Final submission](#4)\n    \n    \n## Best LB score is in Version 80.\n    \n    \n#### Keras neural network for Titanic classification problem: <a href=\"https://www.kaggle.com/isaienkov/keras-neural-network-architecture-optimization\">Titanic: Keras Neural Network architecture optimization</a>\n    \n#### Hyperparameter tunning methods for Titanic classification problem: <a href=\"https://www.kaggle.com/isaienkov/hyperparameters-tuning-techniques\">Titanic: hyperparameters tuning techniques</a>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom mlens.ensemble import SuperLearner\n\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier, ExtraTreesClassifier, AdaBoostClassifier, RandomForestClassifier, BaggingClassifier\nfrom sklearn.linear_model import RidgeClassifier, Perceptron, PassiveAggressiveClassifier, LogisticRegression, SGDClassifier\n\nimport optuna\nfrom optuna.samplers import TPESampler\n\nimport matplotlib.pyplot as plt\nimport plotly.express as px\n\nimport warnings\nfrom sklearn.exceptions import ConvergenceWarning","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# To see optuna progress you need to comment this row\noptuna.logging.set_verbosity(optuna.logging.WARNING)\nwarnings.filterwarnings(action='ignore', category=ConvergenceWarning)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1\"></a>\n<h2 style='background:black; border:0; color:white'><center>1. Feature engineering<center><h2>"},{"metadata":{},"cell_type":"markdown","source":"#### In this notebook I will not focus on preprocessing and feature engineering steps, just show how to build your efficient ensemble in few lines of code. I use almost the same features as in the most of kernels in current competition."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/titanic/train.csv')\ntest = pd.read_csv('/kaggle/input/titanic/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets see percent of NaNs for every column in training set"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"for col in train.columns:\n    print(col, str(round(100* train[col].isnull().sum() / len(train), 2)) + '%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here is some basic preprocessing to get fast training and test datasets."},{"metadata":{"trusted":true},"cell_type":"code","source":"train['LastName'] = train['Name'].str.split(',', expand=True)[0]\ntest['LastName'] = test['Name'].str.split(',', expand=True)[0]\nds = pd.concat([train, test])\n\nsur = list()\ndied = list()\n\nfor index, row in ds.iterrows():\n    s = ds[(ds['LastName']==row['LastName']) & (ds['Survived']==1)]\n    d = ds[(ds['LastName']==row['LastName']) & (ds['Survived']==0)]\n    s=len(s)\n    if row['Survived'] == 1:\n        s-=1\n    d=len(d)\n    if row['Survived'] == 0:\n        d-=1\n    sur.append(s)\n    died.append(d)\n    \nds['FamilySurvived'] = sur\nds['FamilyDied'] = died\nds['FamilySize'] = ds['SibSp'] + ds['Parch'] + 1\nds['IsAlone'] = 0\nds.loc[ds['FamilySize'] == 1, 'IsAlone'] = 1\nds['Fare'] = ds['Fare'].fillna(train['Fare'].median())\nds['Embarked'] = ds['Embarked'].fillna('Q')\n\ntrain = ds[ds['Survived'].notnull()]\ntest = ds[ds['Survived'].isnull()]\ntest = test.drop(['Survived'], axis=1)\n\ntrain['rich_woman'] = 0\ntest['rich_woman'] = 0\ntrain['men_3'] = 0\ntest['men_3'] = 0\n\ntrain.loc[(train['Pclass']<=2) & (train['Sex']=='female'), 'rich_woman'] = 1\ntest.loc[(test['Pclass']<=2) & (test['Sex']=='female'), 'rich_woman'] = 1\ntrain.loc[(train['Pclass']==3) & (train['Sex']=='male'), 'men_3'] = 1\ntest.loc[(test['Pclass']==3) & (test['Sex']=='male'), 'men_3'] = 1\n\ntrain['rich_woman'] = train['rich_woman'].astype(np.int8)\ntest['rich_woman'] = test['rich_woman'].astype(np.int8)\n\ntrain[\"Cabin\"] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in train['Cabin']])\ntest['Cabin'] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in test['Cabin']])\n\nfor cat in ['Pclass', 'Sex', 'Embarked', 'Cabin']:\n    train = pd.concat([train, pd.get_dummies(train[cat], prefix=cat)], axis=1)\n    train = train.drop([cat], axis=1)\n    test = pd.concat([test, pd.get_dummies(test[cat], prefix=cat)], axis=1)\n    test = test.drop([cat], axis=1)\n    \ntrain = train.drop(['PassengerId', 'Ticket', 'LastName', 'SibSp', 'Parch', 'Sex_male', 'Name'], axis=1)\ntest =  test.drop(['PassengerId', 'Ticket', 'LastName', 'SibSp', 'Parch', 'Sex_male', 'Name'], axis=1)\n\ntrain = train.fillna(-1)\ntest = test.fillna(-1)\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's do some visualization."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = px.box(\n    train, \n    x=\"Survived\", \n    y=\"Age\", \n    points='all',\n    title='Age & Survived box plot',\n    width=700,\n    height=500\n)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see from training set that almost all people with Age higher than 63 years didn't survive. Can use these information in modeling post processing."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.box(\n    train, \n    x=\"Survived\", \n    y=\"Fare\", \n    points='all',\n    title='Fare & Survived box plot',\n    width=700,\n    height=500\n)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = px.box(\n    train, \n    x=\"Survived\", \n    y=\"FamilySize\", \n    points='all',\n    title='Family Size & Survived box plot',\n    width=700,\n    height=500\n)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Another one thing. People with family size more than 7 didn't survive."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = px.box(\n    train, \n    x=\"Survived\", \n    y=\"FamilyDied\", \n    points='all',\n    title='Family Died & Survived box plot',\n    width=700,\n    height=500\n)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f = plt.figure(\n    figsize=(19, 15)\n)\n\nplt.matshow(\n    train.corr(), \n    fignum=f.number\n)\n\nplt.xticks(\n    range(train.shape[1]), \n    train.columns, \n    fontsize=14, \n    rotation=75\n)\n\nplt.yticks(\n    range(train.shape[1]), \n    train.columns, \n    fontsize=14\n)\n\ncb = plt.colorbar()\ncb.ax.tick_params(\n    labelsize=14\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets create train and test dataset and create holdout set for validation."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train['Survived']\nX = train.drop(['Survived', 'Cabin_T'], axis=1)\nX_test = test.copy()\n\nX, X_val, y, y_val = train_test_split(X, y, random_state=0, test_size=0.2, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"2\"></a>\n<h2 style='background:black; border:0; color:white'><center>2. Single models training and optimization<center><h2>"},{"metadata":{},"cell_type":"markdown","source":"Lets create some separate single models and check accuracy score. We also try to optimize every single model using optuna framework. As we can see we can get some better results with it."},{"metadata":{"trusted":true},"cell_type":"code","source":"class Optimizer:\n    def __init__(self, metric, trials=30):\n        self.metric = metric\n        self.trials = trials\n        self.sampler = TPESampler(seed=666)\n        \n    def objective(self, trial):\n        model = create_model(trial)\n        model.fit(X, y)\n        preds = model.predict(X_val)\n        if self.metric == 'acc':\n            return accuracy_score(y_val, preds)\n        else:\n            return f1_score(y_val, preds)\n            \n    def optimize(self):\n        study = optuna.create_study(direction=\"maximize\", sampler=self.sampler)\n        study.optimize(self.objective, n_trials=self.trials)\n        return study.best_params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(\n    random_state=666\n)\nrf.fit(X, y)\npreds = rf.predict(X_val)\n\nprint('Random Forest accuracy: ', accuracy_score(y_val, preds))\nprint('Random Forest f1-score: ', f1_score(y_val, preds))\n\ndef create_model(trial):\n    max_depth = trial.suggest_int(\"max_depth\", 2, 6)\n    n_estimators = trial.suggest_int(\"n_estimators\", 2, 150)\n    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 10)\n    model = RandomForestClassifier(\n        min_samples_leaf=min_samples_leaf, \n        n_estimators=n_estimators, \n        max_depth=max_depth, \n        random_state=666\n    )\n    return model\n\noptimizer = Optimizer('f1')\nrf_f1_params = optimizer.optimize()\nrf_f1_params['random_state'] = 666\nrf_f1 = RandomForestClassifier(\n    **rf_f1_params\n)\nrf_f1.fit(X, y)\npreds = rf_f1.predict(X_val)\n\nprint('Optimized on F1 score')\nprint('Optimized Random Forest: ', accuracy_score(y_val, preds))\nprint('Optimized Random Forest f1-score: ', f1_score(y_val, preds))\n\noptimizer = Optimizer('acc')\nrf_acc_params = optimizer.optimize()\nrf_acc_params['random_state'] = 666\nrf_acc = RandomForestClassifier(\n    **rf_acc_params\n)\nrf_acc.fit(X, y)\npreds = rf_acc.predict(X_val)\n\nprint('Optimized on accuracy')\nprint('Optimized Random Forest: ', accuracy_score(y_val, preds))\nprint('Optimized Random Forest f1-score: ', f1_score(y_val, preds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb = XGBClassifier(\n    random_state=666\n)\nxgb.fit(X, y)\npreds = xgb.predict(X_val)\n\nprint('XGBoost accuracy: ', accuracy_score(y_val, preds))\nprint('XGBoost f1-score: ', f1_score(y_val, preds))\n\ndef create_model(trial):\n    max_depth = trial.suggest_int(\"max_depth\", 2, 6)\n    n_estimators = trial.suggest_int(\"n_estimators\", 1, 150)\n    learning_rate = trial.suggest_uniform('learning_rate', 0.0000001, 1)\n    gamma = trial.suggest_uniform('gamma', 0.0000001, 1)\n    subsample = trial.suggest_uniform('subsample', 0.0001, 1.0)\n    model = XGBClassifier(\n        learning_rate=learning_rate, \n        n_estimators=n_estimators, \n        max_depth=max_depth, \n        gamma=gamma, \n        subsample=subsample,\n        random_state=666\n    )\n    return model\n\noptimizer = Optimizer('f1')\nxgb_f1_params = optimizer.optimize()\nxgb_f1_params['random_state'] = 666\nxgb_f1 = XGBClassifier(\n    **xgb_f1_params\n)\nxgb_f1.fit(X, y)\npreds = xgb_f1.predict(X_val)\n\nprint('Optimized on F1 score')\nprint('Optimized XGBoost accuracy: ', accuracy_score(y_val, preds))\nprint('Optimized XGBoost f1-score: ', f1_score(y_val, preds))\n\noptimizer = Optimizer('acc')\nxgb_acc_params = optimizer.optimize()\nxgb_acc_params['random_state'] = 666\nxgb_acc = XGBClassifier(\n    **xgb_acc_params\n)\nxgb_acc.fit(X, y)\npreds = xgb_acc.predict(X_val)\n\nprint('Optimized on accuracy')\nprint('Optimized XGBoost accuracy: ', accuracy_score(y_val, preds))\nprint('Optimized XGBoost f1-score: ', f1_score(y_val, preds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb = LGBMClassifier(\n    random_state=666\n)\nlgb.fit(X, y)\npreds = lgb.predict(X_val)\n\nprint('LightGBM accuracy: ', accuracy_score(y_val, preds))\nprint('LightGBM f1-score: ', f1_score(y_val, preds))\n\ndef create_model(trial):\n    max_depth = trial.suggest_int(\"max_depth\", 2, 6)\n    n_estimators = trial.suggest_int(\"n_estimators\", 1, 150)\n    learning_rate = trial.suggest_uniform('learning_rate', 0.0000001, 1)\n    num_leaves = trial.suggest_int(\"num_leaves\", 2, 3000)\n    min_child_samples = trial.suggest_int('min_child_samples', 3, 200)\n    model = LGBMClassifier(\n        learning_rate=learning_rate, \n        n_estimators=n_estimators, \n        max_depth=max_depth, \n        num_leaves=num_leaves, \n        min_child_samples=min_child_samples,\n        random_state=666\n    )\n    return model\n\noptimizer = Optimizer('f1')\nlgb_f1_params = optimizer.optimize()\nlgb_f1_params['random_state'] = 666\nlgb_f1 = LGBMClassifier(\n    **lgb_f1_params\n)\nlgb_f1.fit(X, y)\npreds = lgb_f1.predict(X_val)\n\nprint('Optimized on F1-score')\nprint('Optimized LightGBM accuracy: ', accuracy_score(y_val, preds))\nprint('Optimized LightGBM f1-score: ', f1_score(y_val, preds))\n\noptimizer = Optimizer('acc')\nlgb_acc_params = optimizer.optimize()\nlgb_acc_params['random_state'] = 666\nlgb_acc = LGBMClassifier(\n    **lgb_acc_params\n)\nlgb_acc.fit(X, y)\npreds = lgb_acc.predict(X_val)\n\nprint('Optimized on accuracy')\nprint('Optimized LightGBM accuracy: ', accuracy_score(y_val, preds))\nprint('Optimized LightGBM f1-score: ', f1_score(y_val, preds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression(\n    random_state=666\n)\nlr.fit(X, y)\npreds = lr.predict(X_val)\n\nprint('Logistic Regression: ', accuracy_score(y_val, preds))\nprint('Logistic Regression f1-score: ', f1_score(y_val, preds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt = DecisionTreeClassifier(\n    random_state=666\n)\ndt.fit(X, y)\npreds = dt.predict(X_val)\n\nprint('Decision Tree accuracy: ', accuracy_score(y_val, preds))\nprint('Decision Tree f1-score: ', f1_score(y_val, preds))\n\ndef create_model(trial):\n    max_depth = trial.suggest_int(\"max_depth\", 2, 6)\n    min_samples_split = trial.suggest_int('min_samples_split', 2, 16)\n    min_weight_fraction_leaf = trial.suggest_uniform('min_weight_fraction_leaf', 0.0, 0.5)\n    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n    model = DecisionTreeClassifier(\n        min_samples_split=min_samples_split, \n        min_weight_fraction_leaf=min_weight_fraction_leaf, \n        max_depth=max_depth, \n        min_samples_leaf=min_samples_leaf, \n        random_state=666\n    )\n    return model\n\noptimizer = Optimizer('f1')\ndt_f1_params = optimizer.optimize()\ndt_f1_params['random_state'] = 666\ndt_f1 = DecisionTreeClassifier(\n    **dt_f1_params\n)\ndt_f1.fit(X, y)\npreds = dt_f1.predict(X_val)\n\nprint('Optimized on F1-score')\nprint('Optimized Decision Tree accuracy: ', accuracy_score(y_val, preds))\nprint('Optimized Decision Tree f1-score: ', f1_score(y_val, preds))\n\noptimizer = Optimizer('acc')\ndt_acc_params = optimizer.optimize()\ndt_acc_params['random_state'] = 666\ndt_acc = DecisionTreeClassifier(\n    **dt_acc_params\n)\ndt_acc.fit(X, y)\npreds = dt_acc.predict(X_val)\n\nprint('Optimized on accuracy')\nprint('Optimized Decision Tree accuracy: ', accuracy_score(y_val, preds))\nprint('Optimized Decision Tree f1-score: ', f1_score(y_val, preds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bc = BaggingClassifier(\n    random_state=666\n)\nbc.fit(X, y)\npreds = bc.predict(X_val)\n\nprint('Bagging Classifier accuracy: ', accuracy_score(y_val, preds))\nprint('Bagging Classifier f1-score: ', f1_score(y_val, preds))\n\ndef create_model(trial):\n    n_estimators = trial.suggest_int('n_estimators', 2, 200)\n    max_samples = trial.suggest_int('max_samples', 1, 100)\n    model = BaggingClassifier(\n        n_estimators=n_estimators, \n        max_samples=max_samples, \n        random_state=666\n    )\n    return model\n\noptimizer = Optimizer('f1')\nbc_f1_params = optimizer.optimize()\nbc_f1_params['random_state'] = 666\nbc_f1 = BaggingClassifier(\n    **bc_f1_params\n)\nbc_f1.fit(X, y)\npreds = bc_f1.predict(X_val)\n\nprint('Optimized on F1-score')\nprint('Optimized Bagging Classifier accuracy: ', accuracy_score(y_val, preds))\nprint('Optimized Bagging Classifier f1-score: ', f1_score(y_val, preds))\n\noptimizer = Optimizer('acc')\nbc_acc_params = optimizer.optimize()\nbc_acc_params['random_state'] = 666\nbc_acc = BaggingClassifier(\n    **bc_acc_params\n)\nbc_acc.fit(X, y)\npreds = bc_acc.predict(X_val)\n\nprint('Optimized on accuracy')\nprint('Optimized Bagging Classifier accuracy: ', accuracy_score(y_val, preds))\nprint('Optimized Bagging Classifier f1-score: ', f1_score(y_val, preds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier()\nknn.fit(X, y)\npreds = knn.predict(X_val)\n\nprint('KNN accuracy: ', accuracy_score(y_val, preds))\nprint('KNN f1-score: ', f1_score(y_val, preds))\n\nsampler = TPESampler(seed=0)\ndef create_model(trial):\n    n_neighbors = trial.suggest_int(\"n_neighbors\", 2, 25)\n    model = KNeighborsClassifier(n_neighbors=n_neighbors)\n    return model\n\noptimizer = Optimizer('f1')\nknn_f1_params = optimizer.optimize()\nknn_f1 = KNeighborsClassifier(\n    **knn_f1_params\n)\nknn_f1.fit(X, y)\npreds = knn_f1.predict(X_val)\n\nprint('Optimized on F1-score')\nprint('Optimized KNN accuracy: ', accuracy_score(y_val, preds))\nprint('Optimized KNN f1-score: ', f1_score(y_val, preds))\n\noptimizer = Optimizer('acc')\nknn_acc_params = optimizer.optimize()\nknn_acc = KNeighborsClassifier(\n    **knn_acc_params\n)\nknn_acc.fit(X, y)\npreds = knn_acc.predict(X_val)\n\nprint('Optimized on accuracy')\nprint('Optimized KNN accuracy: ', accuracy_score(y_val, preds))\nprint('Optimized KNN f1-score: ', f1_score(y_val, preds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"abc = AdaBoostClassifier(\n    random_state=666\n)\nabc.fit(X, y)\npreds = abc.predict(X_val)\n\nprint('AdaBoost accuracy: ', accuracy_score(y_val, preds))\nprint('AdaBoost f1-score: ', f1_score(y_val, preds))\n\ndef create_model(trial):\n    n_estimators = trial.suggest_int(\"n_estimators\", 2, 150)\n    learning_rate = trial.suggest_uniform('learning_rate', 0.0005, 1.0)\n    model = AdaBoostClassifier(\n        n_estimators=n_estimators, \n        learning_rate=learning_rate, \n        random_state=666\n    )\n    return model\n\noptimizer = Optimizer('f1')\nabc_f1_params = optimizer.optimize()\nabc_f1_params['random_state'] = 666\nabc_f1 = AdaBoostClassifier(\n    **abc_f1_params\n)\nabc_f1.fit(X, y)\npreds = abc_f1.predict(X_val)\n\nprint('Optimized on F1-score')\nprint('Optimized AdaBoost accuracy: ', accuracy_score(y_val, preds))\nprint('Optimized AdaBoost f1-score: ', f1_score(y_val, preds))\n\noptimizer = Optimizer('acc')\nabc_acc_params = optimizer.optimize()\nabc_acc_params['random_state'] = 666\nabc_acc = AdaBoostClassifier(\n    **abc_acc_params\n)\nabc_acc.fit(X, y)\npreds = abc_acc.predict(X_val)\n\nprint('Optimized on accuracy')\nprint('Optimized AdaBoost accuracy: ', accuracy_score(y_val, preds))\nprint('Optimized AdaBoost f1-score: ', f1_score(y_val, preds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"et = ExtraTreesClassifier(\n    random_state=666\n)\net.fit(X, y)\npreds = et.predict(X_val)\n\nprint('ExtraTreesClassifier accuracy: ', accuracy_score(y_val, preds))\nprint('ExtraTreesClassifier f1-score: ', f1_score(y_val, preds))\n\ndef create_model(trial):\n    n_estimators = trial.suggest_int(\"n_estimators\", 2, 150)\n    max_depth = trial.suggest_int(\"max_depth\", 2, 6)\n    model = ExtraTreesClassifier(\n        n_estimators=n_estimators, \n        max_depth=max_depth, \n        random_state=0\n    )\n    return model\n\noptimizer = Optimizer('f1')\net_f1_params = optimizer.optimize()\net_f1_params['random_state'] = 666\net_f1 = ExtraTreesClassifier(\n    **et_f1_params\n)\net_f1.fit(X, y)\npreds = et_f1.predict(X_val)\n\nprint('Optimized on F1-score')\nprint('Optimized ExtraTreesClassifier accuracy: ', accuracy_score(y_val, preds))\nprint('Optimized ExtraTreesClassifier f1-score: ', f1_score(y_val, preds))\n\noptimizer = Optimizer('acc')\net_acc_params = optimizer.optimize()\net_acc_params['random_state'] = 666\net_acc = ExtraTreesClassifier(\n    **et_acc_params\n)\net_acc.fit(X, y)\npreds = et_acc.predict(X_val)\n\nprint('Optimized on accuracy')\nprint('Optimized ExtraTreesClassifier accuracy: ', accuracy_score(y_val, preds))\nprint('Optimized ExtraTreesClassifier f1-score: ', f1_score(y_val, preds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3\"></a>\n<h2 style='background:black; border:0; color:white'><center>3. SuperLearner training and optimization<center><h2>"},{"metadata":{},"cell_type":"markdown","source":"Now we will create ensemble model named SuperLearner from mlens package. For details check https://machinelearningmastery.com/super-learner-ensemble-in-python/"},{"metadata":{},"cell_type":"markdown","source":"We are going to use our single models in the first layer and LogisticRegressor as metalearner."},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"model = SuperLearner(\n    folds=5, \n    random_state=666\n)\n\nmodel.add(\n    [\n        bc, \n        lgb, \n        xgb, \n        rf, \n        dt, \n        knn\n    ]\n)\n\nmodel.add_meta(\n    LogisticRegression()\n)\n\nmodel.fit(X, y)\n\npreds = model.predict(X_val)\n\nprint('SuperLearner accuracy: ', accuracy_score(y_val, preds))\nprint('SuperLearner f1-score: ', f1_score(y_val, preds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's optimize SuperLearner"},{"metadata":{"trusted":true},"cell_type":"code","source":"mdict = {\n    'RF': RandomForestClassifier(random_state=666),\n    'XGB': XGBClassifier(random_state=666),\n    'LGBM': LGBMClassifier(random_state=666),\n    'DT': DecisionTreeClassifier(random_state=666),\n    'KNN': KNeighborsClassifier(),\n    'BC': BaggingClassifier(random_state=666),\n    'OARF': RandomForestClassifier(**rf_acc_params),\n    'OFRF': RandomForestClassifier(**rf_f1_params),\n    'OAXGB': XGBClassifier(**xgb_acc_params),\n    'OFXGB': XGBClassifier(**xgb_f1_params),\n    'OALGBM': LGBMClassifier(**lgb_acc_params),\n    'OFLGBM': LGBMClassifier(**lgb_f1_params),\n    'OADT': DecisionTreeClassifier(**dt_acc_params),\n    'OFDT': DecisionTreeClassifier(**dt_f1_params),\n    'OAKNN': KNeighborsClassifier(**knn_acc_params),\n    'OFKNN': KNeighborsClassifier(**knn_f1_params),\n    'OABC': BaggingClassifier(**bc_acc_params),\n    'OFBC': BaggingClassifier(**bc_f1_params),\n    'OAABC': AdaBoostClassifier(**abc_acc_params),\n    'OFABC': AdaBoostClassifier(**abc_f1_params),\n    'OAET': ExtraTreesClassifier(**et_acc_params),\n    'OFET': ExtraTreesClassifier(**et_f1_params),\n    'LR': LogisticRegression(random_state=666),\n    'ABC': AdaBoostClassifier(random_state=666),\n    'SGD': SGDClassifier(random_state=666), \n    'ET': ExtraTreesClassifier(random_state=666),\n    'MLP': MLPClassifier(random_state=666),\n    'GB': GradientBoostingClassifier(random_state=666),\n    'RDG': RidgeClassifier(random_state=666),\n    'PCP': Perceptron(random_state=666),\n    'PAC': PassiveAggressiveClassifier(random_state=666)\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model(trial):\n    model_names = list()\n    models_list = [\n        'RF', 'XGB', 'LGBM', 'DT', \n        'KNN', 'BC', 'OARF', 'OFRF', \n        'OAXGB', 'OFXGB', 'OALGBM', \n        'OFLGBM', 'OADT', 'OFDT', \n        'OAKNN', 'OFKNN', 'OABC', \n        'OFBC', 'OAABC', 'OFABC', \n        'OAET', 'OFET', 'LR', \n        'ABC', 'SGD', 'ET', \n        'MLP', 'GB', 'RDG', \n        'PCP', 'PAC'\n    ]\n    \n    head_list = [\n        'RF', \n        'XGB', \n        'LGBM', \n        'DT', \n        'KNN', \n        'BC', \n        'LR', \n        'ABC', \n        'SGD', \n        'ET', \n        'MLP', \n        'GB', \n        'RDG', \n        'PCP', \n        'PAC'\n    ]\n    \n    n_models = trial.suggest_int(\"n_models\", 2, 6)\n    for i in range(n_models):\n        model_item = trial.suggest_categorical('model_{}'.format(i), models_list)\n        if model_item not in model_names:\n            model_names.append(model_item)\n    \n    folds = trial.suggest_int(\"folds\", 2, 6)\n    \n    model = SuperLearner(\n        folds=folds, \n        random_state=666\n    )\n    \n    models = [\n        mdict[item] for item in model_names\n    ]\n    model.add(models)\n    head = trial.suggest_categorical('head', head_list)\n    model.add_meta(\n        mdict[head]\n    )\n        \n    return model\n\ndef objective(trial):\n    model = create_model(trial)\n    model.fit(X, y)\n    preds = model.predict(X_val)\n    score = accuracy_score(y_val, preds)\n    return score\n\nstudy = optuna.create_study(\n    direction=\"maximize\", \n    sampler=sampler\n)\n\nstudy.optimize(\n    objective, \n    n_trials=50\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = study.best_params\n\nhead = params['head']\nfolds = params['folds']\ndel params['head'], params['n_models'], params['folds']\nresult = list()\nfor key, value in params.items():\n    if value not in result:\n        result.append(value)\n        \nresult","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = SuperLearner(\n    folds=folds, \n    random_state=666\n)\n\nmodels = [\n    mdict[item] for item in result\n]\nmodel.add(models)\nmodel.add_meta(mdict[head])\n\nmodel.fit(X, y)\n\npreds = model.predict(X_val)\n\nprint('Optimized SuperLearner accuracy: ', accuracy_score(y_val, preds))\nprint('Optimized SuperLearner f1-score: ', f1_score(y_val, preds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see we improved our best single score only in a few lines of code. Feel free to add new features and try different models inside superlearner."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4\"></a>\n<h2 style='background:black; border:0; color:white'><center>4. Final submission<center><h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = model.predict(X_test)\npreds = preds.astype(np.int16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/titanic/gender_submission.csv')\nsubmission['Survived'] = preds\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}