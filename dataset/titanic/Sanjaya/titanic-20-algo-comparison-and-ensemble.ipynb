{"cells":[{"metadata":{},"cell_type":"markdown","source":"### ML Solution pipeline\n\n[Problem](#Problem)  -> [Acquire Data](#Acquire) -> [Prepare Data](#Prepare) ->  [Visualize](#Visualize) -> [FeatureEnginering](#Feature) -> [Model](#Model) -> [Predict](#Predict) -> [Evaluate](#Evaluate) -> [HyperParams Tuning](#Tune) -> [Submission](#Submission)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Problem[^](#Problem)<a id=\"Problem\" ></a><br>\nThe problem definition for Titanic Survival competition is described [here](https://www.kaggle.com/c/titanic) at Kaggle.\n\n**Summary:** \"what sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc).  In simple words, based on passenger details we need to predict that person survive or not?","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Acquire Data[^](#Acquire)<a id=\"Acquire\" ></a><br>\n\nData definition can be found in [here.](https://www.kaggle.com/c/titanic/data)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\n\ninput_train_df = pd.read_csv('../input/titanic/train.csv')\ninput_test_df = pd.read_csv('../input/titanic/test.csv')\ninput_train_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Prepare Data[^](#Prepare)<a id=\"Prepare\" ></a><br>\nCleaning -> Fill Missing Data -> Create New Features\n\nCleaning:  remove unwanted features, this will remove unwanted weight we are carring in dataframe","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#Cleaning - select required features\ncleaned_train_df = input_train_df.drop(['Ticket'],axis=1)\ncleaned_train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fill missing data\nprint(cleaned_train_df.isnull().sum())\ncleaned_train_df['Embarked'] = cleaned_train_df['Embarked'].fillna(cleaned_train_df.Embarked.dropna().mode()[0]) \ncleaned_train_df['Age'] = cleaned_train_df['Age'].fillna(cleaned_train_df.Age.dropna().mode()[0]) \ncleaned_train_df['Cabin'] = cleaned_train_df['Cabin'].fillna('NONE')\nprint(cleaned_train_df.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#remove outliers\nselectedFetaureForOutliers = ['Pclass','Age','Fare','Parch'] #numeric feilds\ndfSelected = cleaned_train_df[selectedFetaureForOutliers]\nQ1 = dfSelected.quantile(0.25)\nQ3 = dfSelected.quantile(0.75)\nIQR = Q3-Q1\nprint(IQR)\nprint(\"Before Remove outlier shape : \"+str(dfSelected.shape))\nnoOutlier_df = dfSelected[~((dfSelected < (Q1 - 1.5 * IQR)) |(dfSelected > (Q3 + 1.5 * IQR))).any(axis=1)]\nprint(\"After Remove outlier shape : \"+str(noOutlier_df.shape))\nnoOutlier_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Normalize\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nnoOutlier_df[['Age','Fare']] = scaler.fit_transform(noOutlier_df[['Age','Fare']])\nnoOutlier_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cleaned_train_df = cleaned_train_df.iloc[noOutlier_df.index,:]\ncleaned_train_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Enginering[^](#Feature)<a id=\"Feature\" ></a><br>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(cleaned_train_df['Cabin'].unique())\ncleaned_train_df['CabinGroup'] = cleaned_train_df['Cabin'].str.slice(0,1)\n\ncleaned_train_df['Title'] = cleaned_train_df['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\ncleaned_train_df = cleaned_train_df.drop('Name',axis=1)\ncleaned_train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Construct feature AgeCat\ncleaned_train_df['AgeCat']=0\ncleaned_train_df.loc[cleaned_train_df['Age']<=16,'AgeCat']=0\ncleaned_train_df.loc[(cleaned_train_df['Age']>16)&(cleaned_train_df['Age']<=32),'AgeCat']=1\ncleaned_train_df.loc[(cleaned_train_df['Age']>32)&(cleaned_train_df['Age']<=48),'AgeCat']=2\ncleaned_train_df.loc[(cleaned_train_df['Age']>48)&(cleaned_train_df['Age']<=64),'AgeCat']=3\ncleaned_train_df.loc[cleaned_train_df['Age']>64,'AgeCat']=4\n\ninput_test_df['AgeCat']=0\ninput_test_df.loc[input_test_df['Age']<=16,'AgeCat']=0\ninput_test_df.loc[(input_test_df['Age']>16)&(input_test_df['Age']<=32),'AgeCat']=1\ninput_test_df.loc[(input_test_df['Age']>32)&(input_test_df['Age']<=48),'AgeCat']=2\ninput_test_df.loc[(input_test_df['Age']>48)&(input_test_df['Age']<=64),'AgeCat']=3\ninput_test_df.loc[input_test_df['Age']>64,'AgeCat']=4\n\n# Construct feature FareCat\ncleaned_train_df['FareCat']=0\ncleaned_train_df.loc[cleaned_train_df['Fare']<=7.775,'FareCat']=0\ncleaned_train_df.loc[(cleaned_train_df['Fare']>7.775)&(cleaned_train_df['Fare']<=8.662),'FareCat']=1\ncleaned_train_df.loc[(cleaned_train_df['Fare']>8.662)&(cleaned_train_df['Fare']<=14.454),'FareCat']=2\ncleaned_train_df.loc[(cleaned_train_df['Fare']>14.454)&(cleaned_train_df['Fare']<=26.0),'FareCat']=3\ncleaned_train_df.loc[(cleaned_train_df['Fare']>26.0)&(cleaned_train_df['Fare']<=52.369),'FareCat']=4\ncleaned_train_df.loc[cleaned_train_df['Fare']>52.369,'FareCat']=5\n\ninput_test_df['FareCat']=0\ninput_test_df.loc[input_test_df['Fare']<=7.775,'FareCat']=0\ninput_test_df.loc[(input_test_df['Fare']>7.775)&(input_test_df['Fare']<=8.662),'FareCat']=1\ninput_test_df.loc[(input_test_df['Fare']>8.662)&(input_test_df['Fare']<=14.454),'FareCat']=2\ninput_test_df.loc[(input_test_df['Fare']>14.454)&(input_test_df['Fare']<=26.0),'FareCat']=3\ninput_test_df.loc[(input_test_df['Fare']>26.0)&(input_test_df['Fare']<=52.369),'FareCat']=4\ninput_test_df.loc[input_test_df['Fare']>52.369,'FareCat']=5\n\n# Construct feature FamilySize\ncleaned_train_df['FamilySize'] = cleaned_train_df['Parch'] + cleaned_train_df['SibSp']\ninput_test_df['FamilySize'] = input_test_df['Parch'] + input_test_df['SibSp']\n\ncleaned_train_df.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualize Data[^](#Visualize)<a id=\"Visualize\" ></a><br>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\nfig,axes = plt.subplots(nrows=4,ncols=2)\nfig.tight_layout(pad=1.0)\ndef plot_stat(data,column,ax_pos):\n    grouped = data.groupby([column]).agg({'Survived':['count','sum']})\n    grouped.plot(kind='bar',title=column+' Total Vs Survived',ax=ax_pos,figsize=(15,10))\n    grouped.columns = [\"_\".join(x) for x in grouped.columns.ravel()]\n    stat = pd.DataFrame({'Survival_rate':grouped['Survived_sum'].divide(grouped['Survived_count'])*100})\n    stat = stat.set_index(column+'_'+stat.index.astype(str))\n    return stat\n\nsexBaseSurviveRate = plot_stat(cleaned_train_df,'Sex',axes[0,0])\npclassBaseSurviveRate = plot_stat(cleaned_train_df,'Pclass',axes[0,1])\nparchBaseSurviveRate = plot_stat(cleaned_train_df,'Parch',axes[1,0])\nembarkedBaseSurviveRate = plot_stat(cleaned_train_df,'Embarked',axes[1,1])\nsibSpBaseSurviveRate = plot_stat(cleaned_train_df,'SibSp',axes[2,0])\ncabinGroupBaseSurviveRate = plot_stat(cleaned_train_df,'CabinGroup',axes[2,1])\ntitleBaseSurviveRate = plot_stat(cleaned_train_df,'Title',axes[3,0])\nAgeCatBaseSurviveRate = plot_stat(cleaned_train_df,'AgeCat',axes[3,1])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"survival_rate_df=sexBaseSurviveRate.append(pclassBaseSurviveRate).append(parchBaseSurviveRate).append(embarkedBaseSurviveRate).append(sibSpBaseSurviveRate).append(cabinGroupBaseSurviveRate).append(titleBaseSurviveRate)\nsurvival_rate_df.sort_values(by=['Survival_rate'],ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see some cabin groups seems to be having high survival rate\n\n### Modeling[^](#Model)<a id=\"Model\" ></a><br>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\nX = cleaned_train_df.drop(['Survived','PassengerId','Cabin',\"Age\",\"Fare\",\"Parch\",\"SibSp\"],axis=1)\nX['CabinGroup'] = le.fit_transform(X['CabinGroup'])\nX['Embarked'] = le.fit_transform(X['Embarked'])\nX['Sex'] = le.fit_transform(X['Sex'])\nX['Title'] = le.fit_transform(X['Title'])\n\nY = cleaned_train_df['Survived']\n\ntest_altered = input_test_df.drop(['PassengerId','Ticket',\"Parch\",\"SibSp\"],axis=1)\ntest_altered['Cabin'] = test_altered['Cabin'].fillna('NONE')\ntest_altered['CabinGroup'] = test_altered['Cabin'].str.slice(0,1)\ntest_altered['CabinGroup'] = le.fit_transform(test_altered['CabinGroup'])\ntest_altered['Embarked'] = le.fit_transform(test_altered['Embarked'])\ntest_altered['Sex'] = le.fit_transform(test_altered['Sex'])\ntest_altered['Title'] = test_altered['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\ntest_altered['Title'] = le.fit_transform(test_altered['Title'])\ntest_altered = test_altered.drop('Name',axis=1)\nX_predict_test = test_altered.drop(['Cabin','Age','Fare'],axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process, neural_network\nfrom xgboost import XGBClassifier\n\nrandom_state = 20\nalgorithums = [\n    linear_model.LogisticRegressionCV(max_iter = 50000,random_state=random_state),\n    linear_model.PassiveAggressiveClassifier(random_state=random_state),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(random_state=random_state),\n    linear_model.Perceptron(random_state=random_state),\n    \n    svm.SVC(max_iter = 500000,probability=True,kernel='linear',C=0.025),\n    svm.NuSVC(max_iter = 500000,probability=True),\n    svm.LinearSVC(max_iter = 500000),\n    \n    ensemble.AdaBoostClassifier(random_state=random_state,n_estimators=500,learning_rate=0.75),\n    ensemble.BaggingClassifier(random_state=random_state),\n    ensemble.ExtraTreesClassifier(random_state=random_state,max_depth=6,min_samples_leaf=2),\n    ensemble.GradientBoostingClassifier(random_state=random_state,n_estimators=500,max_depth=6,min_samples_leaf=2),\n    ensemble.RandomForestClassifier(random_state=random_state,n_estimators=500,warm_start=True,max_depth=6,min_samples_leaf=2,max_features='sqrt'),\n    \n    discriminant_analysis.LinearDiscriminantAnalysis(),\n    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n    \n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    \n    neighbors.KNeighborsClassifier(),\n    \n    tree.DecisionTreeClassifier(),\n    tree.ExtraTreeClassifier(),\n    \n    gaussian_process.GaussianProcessClassifier(),\n    \n    XGBClassifier(),\n    \n    neural_network.MLPClassifier(random_state=random_state)\n]    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split,cross_validate,ShuffleSplit\n\nresult_table_columns = ['Algo','Train Accuracy','Test Accuracy','Fit Time','Score Time']\n\nfeatures = ['Pclass', 'Sex','Embarked', 'AgeCat','FareCat', 'FamilySize', 'CabinGroup','Title']\n\nresults = pd.DataFrame(columns=result_table_columns)\n\ndef model_stats(algo,X,Y,features,row_index):    \n    cv_split = ShuffleSplit(n_splits = 10, test_size = .3, train_size = .7, random_state = 0 ) \n    cv_results = cross_validate(algo,X[features],Y,cv=cv_split,return_train_score=True)\n    \n    algo.fit(X[features],Y)\n    \n    results.loc[row_index,'Algo'] = algo.__class__.__name__\n    results.loc[row_index,'Fit Time'] = cv_results['fit_time'].mean()\n    results.loc[row_index,'Score Time'] = cv_results['score_time'].mean()\n    results.loc[row_index,'Train Accuracy'] = cv_results['train_score'].mean() *100\n    results.loc[row_index,'Test Accuracy'] = cv_results['test_score'].mean()  *100  \n    \nrow_index = 0\nfor algo in algorithums:\n    model_stats(algo,X,Y,features,row_index)\n    row_index += 1\n\nresults.sort_values(by='Test Accuracy',ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ensemble modeling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"vote_est = [\n    ('bnb', naive_bayes.BernoulliNB()),\n    ('gbc', ensemble.GradientBoostingClassifier(random_state=random_state,n_estimators=500,max_depth=6,min_samples_leaf=2)),\n    ('xgb', XGBClassifier()),\n    ('ada', ensemble.AdaBoostClassifier(random_state=random_state,n_estimators=500,learning_rate=0.75)),\n    ('bc', ensemble.BaggingClassifier(random_state=random_state)),\n    ('etc',ensemble.ExtraTreesClassifier(random_state=random_state,max_depth=6,min_samples_leaf=2)), \n    ('rfc', ensemble.RandomForestClassifier(random_state=random_state,n_estimators=500,warm_start=True,max_depth=6,min_samples_leaf=2,max_features='sqrt')),\n    ('gpc', gaussian_process.GaussianProcessClassifier()),  \n    ('lr', linear_model.LogisticRegressionCV(max_iter = 50000)),  \n    ('gnb', naive_bayes.GaussianNB()), \n    ('knn', neighbors.KNeighborsClassifier()), \n    ('svc', svm.SVC(probability=True))\n]\n\n#Hard Vote\ncv_split = ShuffleSplit(n_splits = 10, test_size = .3, train_size = .7, random_state = 0 ) \nvote_hard = ensemble.VotingClassifier(estimators = vote_est , voting = 'hard')\nvote_hard_cv = cross_validate(vote_hard, X[features],Y, cv  = cv_split,return_train_score=True)\nvote_hard.fit(X[features],Y)\n\nprint(\"Hard Voting Training w/bin score mean: {:.2f}\". format(vote_hard_cv['train_score'].mean()*100)) \nprint(\"Hard Voting Test w/bin score mean: {:.2f}\". format(vote_hard_cv['test_score'].mean()*100))\nprint(\"Hard Voting Test w/bin score 3*std: +/- {:.2f}\". format(vote_hard_cv['test_score'].std()*100*3))\nprint('-'*10)\n\n\n#Soft Vote\nvote_soft = ensemble.VotingClassifier(estimators = vote_est , voting = 'soft')\nvote_soft_cv = cross_validate(vote_soft, X[features],Y, cv=cv_split,return_train_score=True)\nvote_soft.fit(X[features],Y)\n\nprint(\"Soft Voting Training w/bin score mean: {:.2f}\". format(vote_soft_cv['train_score'].mean()*100)) \nprint(\"Soft Voting Test w/bin score mean: {:.2f}\". format(vote_soft_cv['test_score'].mean()*100))\nprint(\"Soft Voting Test w/bin score 3*std: +/- {:.2f}\". format(vote_soft_cv['test_score'].std()*100*3))\nprint('-'*10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Predict[^](#Predict)<a id=\"Predict\" ></a><br>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# bestAlgo = ensemble.RandomForestClassifier(random_state=random_state,n_estimators=500,warm_start=True,max_depth=6,min_samples_leaf=2,max_features='sqrt')\n# bestAlgo.fit(X[features],Y)\nbestAlgo = vote_hard\n\nY_trainPredict = bestAlgo.predict(X[features])\n\nY_predict = bestAlgo.predict(X_predict_test)\n\nprint(bestAlgo.score(X[features],Y))\n\nconfusion_matrix(Y,Y_trainPredict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Sumbission[^](#Submission)<a id=\"Submission\" ></a><br>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({\n        \"PassengerId\": input_test_df[\"PassengerId\"],\n        \"Survived\": Y_predict\n    })\n#submission.to_csv('submissionVotingHard.csv', index=False)\nsubmission.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}