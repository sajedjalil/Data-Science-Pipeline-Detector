{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Titanic: Machine Learning from Disaster\n\n## Table of Contents\n\n1. [Import Libraries](#import-libraries)\n1. [Get the Data](#get-the-data)\n    1. [Take a quick look at data structure](#quick-look)\n    1. [Create a test set](#create-test)\n1. [Discover and Visualize the Data to Gain Insights (EDA)](#eda)\n    1. [Discover and Visualize](#eda-main)\n    1. [Looking for Correlations](#cor)\n    1. [EDA Results](#eda-res)\n1. [Feature Engineering](#feature)\n1. [Machine Learning](#ml)\n1. [Fine-Tune the Model](#evaluate)"},{"metadata":{},"cell_type":"markdown","source":"<a name='import-libraries'></a>\n# Import libraries\n\nThis notebook is using the end to end machine learning technique from [Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/) (CH2 or Appendix B) book.\n\nFirst part is skipped because this problem is not a business problem etc.\n\nNow we can import all libraries for this problem. Making this in the first step will led you to making the plan for further steps."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Essential libraries\nimport pandas as pd\nimport numpy as np\nimport time\n\n# Data Viz libraries\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Consistent plots\nfrom pylab import rcParams\n\nrcParams['figure.figsize'] = 20,5\nrcParams['xtick.labelsize'] = 9\nrcParams['ytick.labelsize'] = 9\nrcParams['axes.labelsize'] = 10\n\n# Feature engineering libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\nfrom sklearn.impute import KNNImputer,SimpleImputer\nfrom sklearn.compose import ColumnTransformer\n\n# Classifiers\nfrom sklearn.linear_model import SGDClassifier, LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Evaluation libraries\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/titanic/train.csv\")\ntest = pd.read_csv(\"../input/titanic/test.csv\")\ndf = pd.concat([train,test])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a name='get-the-data'></a>\n# Get the Data\n\n<a name='quick-look'></a>\n### Take a Quick Look at the Data Structure"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that there are null values in the 'Age', 'Cabin', and 'Embarked' columns, we will take care of it later. Also, we can see that there are 891 instances, which means a fairly small dataset. Now, we may investigate all categorical attributes in order to secure the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Survived.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Target values seems fairly distributed, no needs for dummy values.\n* Now let's look at the summary of the numerical data."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Another quick way to get a feel of the type of data you are dealing with is to plot a histogram for each numerical attribute.\n* We may need to transform 'Fare' attribute or cut it to batches"},{"metadata":{},"cell_type":"markdown","source":"<a name='create-test'></a>\n## Create a test set\n\nWe need to separate the df dataset into the test and train set to block data snooping."},{"metadata":{"trusted":true},"cell_type":"code","source":"eda, df_test = train_test_split(train, test_size=0.25, random_state=42)\n\neda.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a name='eda'></a>\n# Discover and Visualize the Data to Gain Insights (EDA)\n\nSo far we have only taken a quick glance at the data to get a general understanding of\nthe kind of data we are manipulating. Now the goal is to go into a little more depth.\n<a name='eda-main'></a>\n## Discover and Visualize the Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"eda.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we are going to analyze each attribute for their,\n* Name\n* Type\n* % of missing values\n* Noisiness and type of noise\n* Usefulness for the task\n* Type of distribution\n\nWe can easily say `PassengerId`, `Name` and `Ticket` attributes are not useful for the task. Thus, we can skip analyzing these columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Easy to use function to plot each categorical data\ndef catplot(col):\n    f, axes = plt.subplots(1, 3, sharex=True)\n    sns.stripplot(\n        data = eda,\n        x = col,\n        y = 'Age',\n        hue = 'Survived',\n        jitter = True,\n        ax  = axes[0]\n    )\n    sns.countplot(\n        data = eda,\n        x = col,\n        hue = 'Survived',\n        ax  = axes[1]\n    )\n    sns.violinplot(\n        data=eda,\n        x=col,\n        y='Age',\n        hue='Survived',\n        ax=axes[2]\n    )\n    print('Name:', col)\n    print('Type:', type(col))\n    print('% of missing values:', df[col].isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* `Pclass` attr,"},{"metadata":{"trusted":true},"cell_type":"code","source":"catplot('Pclass')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* `Sex` attr,"},{"metadata":{"trusted":true},"cell_type":"code","source":"catplot('Sex')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* `SibSp` attr,"},{"metadata":{"trusted":true},"cell_type":"code","source":"catplot('SibSp')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* `Parch` attr,"},{"metadata":{"trusted":true},"cell_type":"code","source":"catplot('Parch')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* `Embarked` attr,"},{"metadata":{"trusted":true},"cell_type":"code","source":"catplot('Embarked')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It can be seen that third class has the most death rate in contrast first class has least death rate."},{"metadata":{},"cell_type":"markdown","source":"<a name='cor'></a>\n## Looking for Correlations"},{"metadata":{"trusted":true},"cell_type":"code","source":"attributes = [\"Age\", \"Fare\",'Survived']\n\nsns.pairplot(eda[attributes],hue='Survived')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Obviously, there is no correlation."},{"metadata":{},"cell_type":"markdown","source":"<a name='eda-res'></a>\n## EDA Results\n* `PassengerId`,`Name` and `Ticket` have no effect on exploring the data. They should be removed.\n* There is huge correlation between class of passenger and whether he survived or not `Pclass`.\n* Same as with `Pclass` attribute `Sex` has correlated with surviving chance of passenger.\n* `SibSp` and `Parch` attributes can be combined as number of family member aboard.\n* There is 2 missing values for `Embarked` and lot more for `Age` attribute, they can filled in [Feature Engineering](#feature).\n* There are also lot of missing values in `Cabin` attribute, however attribute is not explanatory to predict whether passenger survived or not. Thus, it can be dropped from `df`.\n* `Fare` is not distributed very  well as continous variable cutting them into the bins would help.\n"},{"metadata":{},"cell_type":"markdown","source":"<a name ='feature'></a>\n# Feature Engineering\nItâ€™s time to prepare the data for your Machine Learning algorithms. Instead of doing this manually, you should write functions for this purpose, for several good reasons.\n\n\n* Simple imput for `Embarked` and `Fare`"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_fill = df.copy()\ndf_fill['Fare'].fillna(df_fill['Fare'].median(), inplace = True)\ndf_fill['Embarked'].fillna(df_fill['Embarked'].mode().iloc[0], inplace = True)\ndf_fill.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Set index as `PassengerId`"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set index as passengerId\ndf_index = df_fill.set_index('PassengerId')\ndf_index.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Impute `Age` according to title of each passenger"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split Name into 3 part\ndf_split_name = df_index.copy()\ndf_split_name.insert(1,'Title',df_split_name['Name'].str.extract('([A-Za-z]+)\\.', expand=True)[0])\n\n# Replacing rare titles with more common ones\nmapping = {'Mlle': 'Miss', 'Major': 'Mr', 'Col': 'Mr', 'Sir': 'Mr', 'Don': 'Mr', 'Mme': 'Miss',\n          'Jonkheer': 'Mr', 'Lady': 'Mrs', 'Capt': 'Mr', 'Countess': 'Mrs', 'Ms': 'Miss', 'Dona': 'Mrs'}\ndf_split_name.replace({'Title': mapping}, inplace=True)\n\n# Iterate for each title\nfor ind, title in enumerate(df_split_name.Title.value_counts().index):\n    median_age = df_split_name.groupby('Title').Age.median()[title]\n    df_split_name.loc[ (df_split_name.Age.isnull()) & (df_split_name.Title==title),'Age'] = median_age\ndf_split_name","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Cut `Fare` and `Age` into bins"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cut = df_split_name.copy()\n\n# Cut label into 5 piece\ndf_cut['FareBin'] = pd.qcut(df_cut.Fare, 5)\n\n# Transform it by encoding\nlabel = LabelEncoder()\ndf_cut['FareBin_Code'] = label.fit_transform(df_cut['FareBin'])\n\n# Drop unnecessary attrs.\ndf_cut.drop(['FareBin'], 1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cut label into 5 piece\ndf_cut['AgeBin'] = pd.qcut(df_cut.Age, 5)\n\n# Transform it by encoding\nlabel = LabelEncoder()\ndf_cut['AgeBin_code'] = label.fit_transform(df_cut['AgeBin'])\n\n# Drop unnecessary attrs.\ndf_cut.drop(['Age','AgeBin'], 1, inplace=True)\n\ndf_cut.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* `SibSp` and `Parch` attributes can be combined as number of family member aboard."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Combine two attributes\ndf_comb = df_cut.copy()\ndf_comb['Family_members_aboard'] = df_comb['SibSp'] + df_comb['Parch']\ndf_comb.drop(['SibSp','Parch'], axis=1, inplace=True)\ndf_comb.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Try to extract family information"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_extr_family = df_comb.copy()\ndf_extr_family.insert(2,'Surname',df_extr_family['Name'].str.extract('([A-Za-z]+)\\,', expand=True)[0])\n\nDEFAULT_SURVIVAL_VALUE = 0.5\ndf_extr_family['Family_Survival'] = DEFAULT_SURVIVAL_VALUE\ndf_extr_family.reset_index(inplace=True)\nfor surname, sur_group in df_extr_family[df_extr_family['Family_members_aboard'] > 0].groupby(['Surname','Fare']):\n    for ind, row in sur_group.iterrows():\n        smax = sur_group.drop(ind).Survived.max()\n        smin = sur_group.drop(ind).Survived.min()\n        passID = row['PassengerId']\n        if smax == 1:\n            df_extr_family.loc[df_extr_family['PassengerId'] == passID, 'Family_Survival'] = 1\n        elif smin == 0:\n            df_extr_family.loc[df_extr_family['PassengerId'] == passID, 'Family_Survival'] = 0\ndf_extr_family[df_extr_family.Family_Survival != 0.5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Encode `Embarked` and `Sex` columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_enc = df_extr_family.copy()\n\n# Encode Embarked\nlabel = LabelEncoder()\ndf_enc['Embarked_code'] = label.fit_transform(df_enc['Embarked'])\n\n# Encode Sex\nlabel = LabelEncoder()\ndf_enc['Sex_code'] = label.fit_transform(df_enc['Sex'])\n\ndf_enc.drop(['Sex', 'Embarked'], 1, inplace=True)\ndf_enc.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Drop unnecessary columns which are `Name`, `Ticket`, `Surname`, `Fare` and `Cabin`"},{"metadata":{"trusted":true},"cell_type":"code","source":"def drop_cols(cols):\n    return df_enc.drop(cols, axis=1)\n\nattr_to_drop = ['Title', 'Surname', 'Name', 'Ticket', 'Cabin', 'Fare']\ndf_prepared = drop_cols(attr_to_drop)\ndf_prepared.set_index('PassengerId',inplace=True)\ntrain_ready = df_prepared[:891]\nsubmission = df_prepared[891:]\ntrain_ready","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a name ='ml'></a>\n# Machine Learning "},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train_ready.drop('Survived',1)\ny = train_ready['Survived']\n\nX_submission = submission.drop('Survived',1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Scale the data and submission attributes with `StandardScaler`"},{"metadata":{"trusted":true},"cell_type":"code","source":"std_scaler = StandardScaler()\nX = std_scaler.fit_transform(X)\nX_submission = std_scaler.transform(X_submission)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* from sklearn.linear_model import SGDClassifier, LogisticRegression\n* from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n* from sklearn.neighbors import KNeighborsClassifier\n* from sklearn.naive_bayes import GaussianNB\n* from sklearn.tree import DecisionTreeClassifier\n* from sklearn.svm import SVC"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef compare_clf(classifiers):\n    rows = []\n    for clf in classifiers:\n        start = time.time()\n        score_arr = cross_val_score(clf,X,y,cv=5,scoring='roc_auc')\n        end = time.time()\n        for i, score in enumerate(score_arr):\n            score_dict = {\n                'fold':i+1,\n                'Classifier':clf.__class__.__name__,\n                'Score':score,\n                'Time (sec)':end-start\n            }\n            rows.append(score_dict)\n    return pd.DataFrame(rows)\n            \nclassifiers = [\n    SGDClassifier(),\n    LogisticRegression(),\n    LinearDiscriminantAnalysis(),\n    GaussianNB(),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    SVC(),\n    KNeighborsClassifier()\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"compare_clf(classifiers).groupby('Classifier').agg({'mean','median','std'}).drop('fold',1).sort_values(('Score','mean'),ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Linear Discriminant Analysis Selected"},{"metadata":{},"cell_type":"markdown","source":"<a name ='evaluate'></a>\n# Fine-Tune the Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_neighbors = [6,7,8,9,10,11,12,14,16,18,20,22]\nalgorithm = ['auto']\nweights = ['uniform', 'distance']\nleaf_size = list(range(1,50,5))\nhyperparams = {'algorithm': algorithm, 'weights': weights, 'leaf_size': leaf_size, \n               'n_neighbors': n_neighbors}\n\ngd=GridSearchCV(estimator = KNeighborsClassifier(), param_grid = hyperparams, verbose=True, \n                cv=5, scoring = \"roc_auc\")\ngd.fit(X, y)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gd.best_estimator_.fit(X, y)\ny_pred = gd.best_estimator_.predict(X_submission)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit=pd.DataFrame(data=y_pred, index=submission.index, columns=['Survived'], dtype='int')\nsubmit.to_csv('submission.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}