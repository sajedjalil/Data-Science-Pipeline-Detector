{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Titanic Prediction Step by Step (Beginner)\n\nAs a newcomer to data science and machine learning, i did an initial level analysis and some predictions. If you liked the work and i was able to help, please don't forget to upvote :)"},{"metadata":{},"cell_type":"markdown","source":"## 1st Step: Import Libraries\n\nIn the first step, we will import the necessary libraries for us."},{"metadata":{"trusted":true},"cell_type":"code","source":"## Some visualization tools and tools we mostly use.\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom pandas import read_csv\nimport seaborn as sns\n\n## This part is optional, if you don't want to see \n## the warnings, you can turn them off in this way.\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n## Machine learning tools.\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.svm import LinearSVC","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2nd Step: Upload and Reading the Data\n\nIn the second step we will load the data. Then we can take a first look at our data. \n\na) Let's make some comments about our data: there are some variables in our study that are not important to us like \"Name\" and \"Ticket\". We will clean them in the next stage and continue on our way."},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/titanic/train.csv\")\ntest = pd.read_csv(\"../input/titanic/test.csv\")\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"b) Now we are looking at our missing datas for our train and test data. As far as we can see, we can make the following conclusion: There is a level of missing data in the \"Age\" and \"Cabin\" variables that we can call serious. Since there are too many missing data that we cannot work with the \"Cabin\" variable, we will delete it. However, since the \"Age\" variable is both an important variable and we have the ability to correct it, we can delete or replace the missing datas in this variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(pd.isnull(train).sum(),\"\\n------------\\n\",pd.isnull(test).sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3rd Step: Data Analysis and Some Visiuality\n\nThis part, especially the visualization part, is a part that I am still working on. The data visualization and analysis part is the most crucial and important part in such studies. I couldn't keep this part too wide, but don't do what i do, filter and analyze the data as much as possible. And try to improve yourself on this subject because if you are going to work on a data, you must first warm up with that data."},{"metadata":{"trusted":true},"cell_type":"code","source":"train[[\"Sex\", \"Survived\"]].groupby([\"Sex\"]).mean()*100\n## Survival rates by gender.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[[\"Pclass\", \"Survived\"]].groupby([\"Pclass\"]).mean()*100\n## Apparently the socioeconomic situation had an impact on the chance to survive.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[[\"Embarked\", \"Survived\"]].groupby([\"Embarked\"]).mean()*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### This image gives us the correlation analysis. We use it to explain the relationship between variables, and we can say that the relationship increases as the ratio on boxes approaches 1."},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax = plt.subplots(figsize = (5,5))\nsns.heatmap(train.corr(), annot=True)\n\n## this part is optional. I had to do it because the \n## plot had been disproportionate.\nbottom, top = ax.get_ylim()\nax.set_ylim(bottom + 0.5, top - 0.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"survived_map = {1:\"Survived\", 0:\"Death\"}\n\nsns.swarmplot(x=\"Sex\", y=\"Age\", \n              hue=train[\"Survived\"].map(survived_map).copy(), \n              data=train)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"survived_map = {1:\"Survived\", 0:\"Death\"}\n\nsns.swarmplot(x=\"Sex\", y=\"Fare\", \n              hue=train[\"Survived\"].map(survived_map).copy(), \n              data=train)\nplt.show()\n\n## Socioeconomics status was important for survive. We can see that \n## 3 person who status of passangers with the highest ticket fee.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"survived_map = {1:\"Survived\", 0:\"Death\"}\n\nsns.swarmplot(x=\"Embarked\", y=\"Age\", \n              hue=train[\"Survived\"].map(survived_map).copy(), \n              data=train)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4th Step: Data Cleaning and Editing\n\nIn this section, we will make our data ready to predict and work on. This part is also important. We will find and clear our missing datas, remove the variables that we will not use from the data set. And group them by putting them into data types that we can operate on.\n\n\nAs always, let's first take a look at our raw data. And let's decide what we should work on. As I said from the beginning, some variables are useless, so we will delete them. And we will fill in some of the missing data in a way that works for us. We will also need to change the dtypes of some variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.info(),test.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## We will delete unimportant variables for us.\ntrain.drop(\"PassengerId\",axis=1,inplace=True)\ntrain.drop([\"Cabin\",\"Name\",\"Ticket\"], axis=1, inplace=True)\ntest.drop([\"Cabin\",\"Name\",\"Ticket\"], axis=1, inplace=True)\n\n## In this section we will group the sibsp and patch variables into \n## one variable and group them together and delete the rest.\n## First, let's find out who the traveler is traveling with how many people.\ntrain[\"Alone?\"] = train[\"SibSp\"]+train[\"Parch\"]+1\ntrain.drop([\"SibSp\",\"Parch\"], axis=1, inplace=True)\n\ntest[\"Alone?\"] = test[\"SibSp\"]+test[\"Parch\"]+1\ntest.drop([\"SibSp\",\"Parch\"], axis=1, inplace=True)\n\n## Now we will classify our passengers and apply mapping.\nbins = [0,1,4,11]\nlabels = [\"Alone\",\"NotAlone\",\"Crowd\"]\ntrain[\"Person\"] = pd.cut(train[\"Alone?\"],bins, labels = labels, include_lowest = True)\n\nperson_map = {\"Alone\":1, \"NotAlone\":2, \"Crowd\":3}\ntrain[\"Person\"] = train[\"Person\"].map(person_map)\ntrain.drop(\"Alone?\", axis=1, inplace=True)\n\nbins = [0,1,4,11]\nlabels = [\"Alone\",\"NotAlone\",\"Crowd\"]\ntest[\"Person\"] = pd.cut(test[\"Alone?\"],bins, labels = labels, include_lowest = True)\n\nperson_map = {\"Alone\":1, \"NotAlone\":2, \"Crowd\":3}\ntest[\"Person\"] = test[\"Person\"].map(person_map)\ntest.drop(\"Alone?\", axis=1, inplace=True)\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### In this section, we will randomly fill the missing data in our age variable with a value between the average of the age variable and the standard error."},{"metadata":{"trusted":true},"cell_type":"code","source":"def age_train(x):\n    train_mean = train[x].mean()\n    train_std = train[x].std()\n    train_null = train[x].isnull().sum()\n    return np.random.randint(train_mean - train_std, train_mean + train_std, size = train_null)\ndef age_test(x):\n    test_mean = test[x].mean()\n    test_std = test[x].std()\n    test_null = test[x].isnull().sum()\n    return np.random.randint(test_mean - test_std, test_mean + test_std, size = test_null)\n\ntrain[\"Age\"][np.isnan(train[\"Age\"])] = age_train(\"Age\")\ntest[\"Age\"][np.isnan(test[\"Age\"])] = age_test(\"Age\")\n\n\ntrain[\"Age\"] = train[\"Age\"].astype(\"int64\")\ntest[\"Age\"] = test[\"Age\"].astype(\"int64\")\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We fill in the missing data and categorize the remaining variables."},{"metadata":{},"cell_type":"markdown","source":"#### Fare"},{"metadata":{"trusted":true},"cell_type":"code","source":"test[\"Fare\"].fillna(test[\"Fare\"].median(), inplace = True)\ntrain[\"Fare\"] = train[\"Fare\"].astype(\"int64\")\ntest[\"Fare\"] = test[\"Fare\"].astype(\"int64\")\n\ntrain[\"FareGroup\"] = pd.qcut(train[\"Fare\"], 4, labels = [1,2,3,4])\ntrain.drop(\"Fare\", axis=1, inplace=True)\n\ntest[\"FareGroup\"] = pd.qcut(test[\"Fare\"], 4, labels = [1,2,3,4])\ntest.drop(\"Fare\", axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Embarked"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.fillna({\"Embarked\": \"S\"})\n\nembarked_map = {\"S\": 0, \"C\": 1, \"Q\":2}\ntrain[\"Embarked\"] = train[\"Embarked\"].map(embarked_map)\n\nembarked_map = {\"S\": 0, \"C\": 1, \"Q\":2}\ntest[\"Embarked\"] = test[\"Embarked\"].map(embarked_map)\n\ntrain[\"Embarked\"] = train[\"Embarked\"].astype(\"int64\")\ntest[\"Embarked\"] = test[\"Embarked\"].astype(\"int64\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Sex"},{"metadata":{"trusted":true},"cell_type":"code","source":"sex_map = {\"male\": 0, \"female\": 1}\ntrain['Sex'] = train['Sex'].map(sex_map)\n\nsex_map = {\"male\": 0, \"female\": 1}\ntest[\"Sex\"] = test[\"Sex\"].map(sex_map)\n\ntrain[\"Sex\"] = train[\"Sex\"].astype(\"int64\")\ntest[\"Sex\"] = test[\"Sex\"].astype(\"int64\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Age"},{"metadata":{"trusted":true},"cell_type":"code","source":"bins = [0,18,30,50,70,120]\nlabels = [\"0-17\",\"18-29\",\"30-49\",\"50-69\",\"70+\"]\ntrain[\"AgeGroup\"] = pd.cut(train[\"Age\"],bins, labels = labels, include_lowest = True)\n\nage_mapping = {\"0-17\":0, \"18-29\":1, \"30-49\":2, \"50-69\":3, \"70+\":4}\ntrain[\"AgeGroup\"] = train[\"AgeGroup\"].map(age_mapping)\ntrain.drop(\"Age\", axis=1, inplace=True)\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bins = [0,18,30,50,70,120]\nlabels = [\"0-17\",\"18-29\",\"30-49\",\"50-69\",\"70+\"]\ntest[\"AgeGroup\"] = pd.cut(test[\"Age\"],bins, labels = labels, include_lowest = True)\n\nage_mapping = {\"0-17\":0, \"18-29\":1, \"30-49\":2, \"50-69\":3, \"70+\":4}\ntest[\"AgeGroup\"] = test[\"AgeGroup\"].map(age_mapping)\ntest.drop(\"Age\", axis=1, inplace=True)\n\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5th Step: Predictions\n\n### We can now implement our machine learning algorithms.\n\n##### First of all, we will separate our data set as train and test."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nindependent_var = train.drop([\"Survived\"], axis=1)\ndependent_var = train[\"Survived\"]\nx_train, x_test, y_train, y_test = train_test_split(independent_var, dependent_var, \n                                                  test_size = 20, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### And finally we can implement our models. I applied all of them here at the same time and sorted the accuracy scores of the models in a dataframe. You can implement it one by one."},{"metadata":{"trusted":true},"cell_type":"code","source":"models = []\nmodels.append(SVC())\nmodels.append(LinearSVC())\nmodels.append(Perceptron())\nmodels.append(GaussianNB())\nmodels.append(SGDClassifier())\nmodels.append(LogisticRegression())\nmodels.append(KNeighborsClassifier())\nmodels.append(RandomForestClassifier())\nmodels.append(DecisionTreeClassifier())\nmodels.append(GradientBoostingClassifier())\n\naccuracy_list = []\nfor model in models:\n    model.fit(x_train, y_train)\n    y_pred = model.predict(x_test)\n    accuracy = (accuracy_score(y_pred, y_test, normalize=True)*100)\n    accuracy_list.append(accuracy)\n\n\nmodel_name_list = [\"SVM\",\"Linear SVC\",\"Perceptron\",\"Gaussian NB\",\"SGD Classifier\",\"Logistic Regression\",\n                   \"K-Neighbors Classifier\",\"Random Forest Classifier\",\"Decision Tree\",\"Gradient Boosting\"]\n\nbest_model = pd.DataFrame({\"Model\": model_name_list, \"Score\": accuracy_list})\nbest_model.sort_values(by=\"Score\", ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission Part"},{"metadata":{"trusted":true},"cell_type":"code","source":"DT = DecisionTreeClassifier()\nDT.fit(x_train, y_train)\n\npassanger_id = test[\"PassengerId\"]\npred = DT.predict(test.drop(\"PassengerId\", axis=1))\npredictions = pd.DataFrame({ \"PassengerId\" : passanger_id, \"Survived\": pred })\n\n## predictions.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"**References**\n\n* [A Journey through Titanic](http://https://www.kaggle.com/omarelgabry/a-journey-through-titanic)\n* [Titanic Data Science Solutions](http://https://www.kaggle.com/startupsci/titanic-data-science-solutions)\n* [Titanic Survival Predictions (Beginner)](http://https://www.kaggle.com/nadintamer/titanic-survival-predictions-beginner)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":4}