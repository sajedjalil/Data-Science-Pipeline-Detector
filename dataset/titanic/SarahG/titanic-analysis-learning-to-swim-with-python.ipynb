{"cells":[{"metadata":{"_uuid":"c3b9226e142667d6b96e34daf7d6e42bea0ea1e2","_cell_guid":"cb19f71d-51c8-417f-829e-3179d3319dcd"},"cell_type":"markdown","source":"# **Introduction**\n\nThis is my first Kaggle, and my first foray into data analysis using python.  The following kernel contains the steps enumerated below for assessing the Titanic survival dataset:\n\n1. Import data and python packages\n2. Assess Data Quality & Missing Values\n    * 2.1 Age - Missing Values\n    * 2.2 Cabin - Missing Values\n    * 2.3 Embarked - Missing Values\n    * 2.4 Final Adjustments to Data\n    * 2.4.1 Additional Variables\n3. Exploratory Data Analysis\n4. Logistic Regression \n5. Hold-Out Testing & Model Assessment\n    * 5.1 Kaggle \"Test\" Dataset\n    * 5.2 Re-run Logistic Regression w/ 80-20 Split\n    * 5.3 Out-of-sample test results\n6. Logistic Regression Conclusions<br>\n7. Alternate Approach: Random Forest Estimation\n<br>\n*References are provided at the bottom of each section.*"},{"metadata":{"_uuid":"0a395fd25f20834b070ef55cb8987c8c1f9b55f9","_cell_guid":"33c91cae-2ff8-45a6-b8cb-671619e9c933"},"cell_type":"markdown","source":"## 1. Import Data & Python Packages"},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"d8bdd5f0320e244e4702ed8ec1c2482b022c51cd","_cell_guid":"de05512e-6991-44df-9599-da92a7e459ac","collapsed":true},"source":"import numpy as np \nimport pandas as pd \n\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt \nplt.rc(\"font\", size=14)\nimport seaborn as sns\nsns.set(style=\"white\") #white background style for seaborn plots\nsns.set(style=\"whitegrid\", color_codes=True)\n\n#sklearn imports source: https://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"7964157913fbcff581fc1929eed487708e81ac9c","_cell_guid":"e0a17223-f682-45fc-89a5-667af9782bbe","collapsed":true},"source":"# get titanic & test csv files as a DataFrame\n\n#developmental data (train)\ntitanic_df = pd.read_csv(\"../input/train.csv\")\n\n#cross validation data (hold-out testing)\ntest_df    = pd.read_csv(\"../input/test.csv\")\n\n# preview developmental data\ntitanic_df.head(5)\n","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"bff38fcf31baf67493513c06f0c2f6e50576ff09","_cell_guid":"1d969b76-ea88-4d32-a58e-f22a070258bf","collapsed":true},"source":"test_df.head(5)","outputs":[]},{"metadata":{"_uuid":"2b1d45128663b9466fe9ac0059a13cfd4bd43657","_cell_guid":"4cd08f1e-9cb9-4d8e-99d0-3a9ce91b91c3"},"cell_type":"markdown","source":"<font color=red>  Note: There is no target variable for the hold out data (i.e. \"Survival\" column is missing), so there's no way to use this as our cross validation sample.  Refer to Section 5.</font>"},{"metadata":{"_uuid":"8660e63a62c2fcdb4f7633380166438caf5edae9","_cell_guid":"6578c0da-7bcf-433d-9f28-a66d8dfa6fa3"},"cell_type":"markdown","source":"## 2. Data Quality & Missing Value Assessment"},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"d4fe22ead7e187724ca6f3ba7ba0e6412ae0e874","_cell_guid":"29dddd33-d995-4b0f-92ea-a361b368cc42","collapsed":true},"source":"# check missing values in train dataset\ntitanic_df.isnull().sum()","outputs":[]},{"metadata":{"_uuid":"696b428bd3ca49421f650665267ce7ca1b358814","_cell_guid":"7776faeb-6a8f-4460-a367-4b087d2cc089"},"cell_type":"markdown","source":"### 2.1    Age - Missing Values"},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"233138d201b240f1e689a6cb9c63e527cb1cd929","_cell_guid":"81c65784-d043-47f4-bd1f-18ed772d902b","collapsed":true},"source":"sum(pd.isnull(titanic_df['Age']))","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"029ed252aa0e628f5c888d81efff8d98be14d2c3","_cell_guid":"260af420-4345-44ae-aaef-cb7b10e0d54a","collapsed":true},"source":"# proportion of \"Age\" missing\nround(177/(len(titanic_df[\"PassengerId\"])),4)","outputs":[]},{"metadata":{"_uuid":"c8fff460fb532a063f6944450809014ca831ca52","_cell_guid":"951f7bb8-779c-4eac-85a2-3fdcfdcd293e"},"cell_type":"markdown","source":"~20% of entries for passenger age are missing. Let's see what the 'Age' variable looks like in general."},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"c6fd60f15d5e803d4dffc89e782c6fbc72445a83","_cell_guid":"6d65fcfa-52bf-45ab-b959-64a32c1c1976","collapsed":true},"source":"titanic_df[\"Age\"].hist(bins=15, color='teal', alpha=0.8)","outputs":[]},{"metadata":{"_uuid":"24c201948b9c8c8076ab01271a4790d9db9096b5","_cell_guid":"e62d6951-d968-43ba-aabf-add90524d042"},"cell_type":"markdown","source":"Since \"Age\" is (right) skewed, using the mean might give us biased results by filling in ages that are older than desired.  To deal with this, we'll use the median to impute the missing values. "},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"92c608b8f196e3eb7f75183a0d4ed61f3f914806","_cell_guid":"76fa4f61-cbd1-4638-8c36-b73430f4f40c","collapsed":true},"source":"# median age is 28 (as compared to mean which is ~30)\ntitanic_df[\"Age\"].median(skipna=True)","outputs":[]},{"metadata":{"_uuid":"e1a08114e302ddc90266e5f065b3f0b5a200bc89","_cell_guid":"dea7b01c-c8c1-401f-a336-36ee73de2222"},"cell_type":"markdown","source":"### 2.2 Cabin - Missing Values"},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"1acbce9c6bc5d586dda3e47b7506067a85524e66","_cell_guid":"1a1ad808-0a63-43ac-b757-71195880ed4f","collapsed":true},"source":"# proportion of \"cabin\" missing\nround(687/len(titanic_df[\"PassengerId\"]),4)","outputs":[]},{"metadata":{"_uuid":"b6e037c7ac5ec476516031a06b042d8b9999ba44","_cell_guid":"eda8c434-63ff-4875-8566-2e194c0d3f66"},"cell_type":"markdown","source":"77% of records are missing, which means that imputing information and using this variable for prediction is probably not wise.  We'll ignore this variable in our model."},{"metadata":{"_uuid":"d575319b1f528c7a153d8ab680282048cb163b14","_cell_guid":"0e696cff-ca80-4cb5-862c-ee80f4b1ab1f"},"cell_type":"markdown","source":"### 2.3 Embarked - Missing Values"},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"92ab9e62fb62f2a0fb9972baf6ada444187540e6","_cell_guid":"f21c2b55-2126-439d-8b1d-e96dafc97d81","collapsed":true},"source":"# proportion of \"Embarked\" missing\nround(2/len(titanic_df[\"PassengerId\"]),4)","outputs":[]},{"metadata":{"_uuid":"dc97b80524057522f024d0ae6f1abe77cb994903","_cell_guid":"d03a4187-c527-4f71-8260-0495f4523e9e"},"cell_type":"markdown","source":"There are only 2 missing values for \"Embarked\", so we can just impute with the port where most people boarded."},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"7561cc74de105488d349832545367d13d562b67d","_cell_guid":"7f3b91bc-f388-4fa3-b908-61c82292eab0","collapsed":true},"source":"sns.countplot(x='Embarked',data=titanic_df,palette='Set2')\nplt.show()","outputs":[]},{"metadata":{"_uuid":"19cfaae8c484dcb1d00f69b2771e86dc249e9793","_cell_guid":"c4c55f99-ce99-44f9-b7a8-d4d623ae9295"},"cell_type":"markdown","source":"By far the most passengers boarded in Southhampton, so we'll impute those 2 NaN's w/ \"S\"."},{"metadata":{"_uuid":"86e3fee2a3413531703d3ec86f1a0d5463755bf0","_cell_guid":"d9e14c34-e6c2-46ad-a9a5-121f6a236b8b"},"cell_type":"markdown","source":"*References for graph creation:*<br>\nhttps://matplotlib.org/1.2.1/examples/pylab_examples/histogram_demo.html <br>\nhttps://seaborn.pydata.org/generated/seaborn.countplot.html"},{"metadata":{"_uuid":"3609e785d210d5a8110f7ce550e61007d066449b","_cell_guid":"684c308f-25ae-4039-9332-ddb58953a054"},"cell_type":"markdown","source":"### 2.4 Final Adjustments to Data (Train & Test)"},{"metadata":{"_uuid":"06d2762ccec3f11564870fe941fc9ac45d71662f","_cell_guid":"b3025cdc-fe9f-43b6-bda1-e45c1f25e77c"},"cell_type":"markdown","source":"Based on my assessment of the missing values in the dataset, I'll make the following changes to the data:\n* If \"Age\" is missing for a given row, I'll impute with 28 (median age).\n* If \"Embark\" is missing for a riven row, I'll impute with \"S\" (the most common boarding port).\n* I'll ignore \"Cabin\" as a variable.  There are too many missing values for imputation.  Based on the information available, it appears that this value is associated with the passenger's class and fare paid."},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"feeed4b6775f88edf5de12b0ee6ee73c16eba61d","_cell_guid":"bc0d7121-1008-4890-9043-07eba1524e15","collapsed":true},"source":"train_data = titanic_df\ntrain_data[\"Age\"].fillna(28, inplace=True)\ntrain_data[\"Embarked\"].fillna(\"S\", inplace=True)\ntrain_data.drop('Cabin', axis=1, inplace=True)","outputs":[]},{"metadata":{"_uuid":"d8280757e6bc627821fb0540c87ccd6ca110f1e0","_cell_guid":"6925fcc2-977b-4369-85e1-77a9210326a7"},"cell_type":"markdown","source":"### 2.4.1 Additional Variables"},{"metadata":{"_uuid":"3bfdee842f11d27ca490f466c45ef9bf3673e7ae","_cell_guid":"5cf98f33-fdd5-4a16-b6bf-fa36bc8b84e0"},"cell_type":"markdown","source":"According to the Kaggle data dictionary, both SibSp and Parch relate to traveling with family.  For simplicity's sake (and to account for possible multicollinearity), I'll combine the effect of these variables into one categorical predictor: whether or not that individual was traveling alone."},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"d1f5815ba663f7e8cc17d7efcff73653af5b1bdb","_cell_guid":"759c3c8e-8db6-41d9-a1a2-058a15b338a6","collapsed":true},"source":"## Create categorical variable for traveling alone\n\ntrain_data['TravelBuds']=train_data[\"SibSp\"]+train_data[\"Parch\"]\ntrain_data['TravelAlone']=np.where(train_data['TravelBuds']>0, 0, 1)","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"b19aa93fada9cb5fc4b4b306cdc73f8e08a68b7c","_cell_guid":"df667f94-7e24-46ec-9d5e-61aa0f457f08","collapsed":true},"source":"train_data.drop('SibSp', axis=1, inplace=True)\ntrain_data.drop('Parch', axis=1, inplace=True)\ntrain_data.drop('TravelBuds', axis=1, inplace=True)","outputs":[]},{"metadata":{"_uuid":"ca53796bf788bd3b015f1a79a97e050bafa2c770","_cell_guid":"e4a22367-b719-4204-952f-d2e9a3b8075e"},"cell_type":"markdown","source":"I'll also create categorical variables for Passenger Class (\"Pclass\"), Gender (\"Sex\"), and Port Embarked (\"Embarked\"). "},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"4addc27f7e8fd27cbae8f07e39c9c14862a472c2","_cell_guid":"6cbd962e-30de-4dfa-8e0e-ce9bfb939524","collapsed":true},"source":"#create categorical variable for Pclass\n\ntrain2 = pd.get_dummies(train_data, columns=[\"Pclass\"])","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"52f6f57ba2345e963e313d8f2209b645b0c7f14f","_cell_guid":"afe4460b-234f-426e-9a6a-8bd35c9f650b","collapsed":true},"source":"train3 = pd.get_dummies(train2, columns=[\"Embarked\"])","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"0bae4e78eba150b7381adbe2aa2c28f4f813dd29","_cell_guid":"feaf5b34-d06d-4880-b476-a21bd74ba0fc","collapsed":true},"source":"train4=pd.get_dummies(train3, columns=[\"Sex\"])\ntrain4.drop('Sex_female', axis=1, inplace=True)","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"78bc585360fa00590dc7094fd3d57568e1457a51","_cell_guid":"d9977027-13df-4641-b9ea-cb20d1fe3927","collapsed":true},"source":"train4.drop('PassengerId', axis=1, inplace=True)\ntrain4.drop('Name', axis=1, inplace=True)\ntrain4.drop('Ticket', axis=1, inplace=True)\ntrain4.head(5)","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"8c35a63b2fcb5d042a2e1f0f904fc24e8edf3053","_cell_guid":"25fb2fc9-a1ca-4d44-baa5-affa995da74a","collapsed":true},"source":"df_final = train4","outputs":[]},{"metadata":{"_uuid":"6a8e533e77c7f1f1a68d136119f93972447a31cf","_cell_guid":"768cf074-6ecb-47eb-9b42-8b9079ffb811"},"cell_type":"markdown","source":"### Now, apply the same changes to the test data. <br>\nI will apply to same imputation for \"Age\" in the Test data as I did for my Training data (if missing, Age = 28).  <br> I'll also remove the \"Cabin\" variable from the test data, as I've decided not to include it in my analysis. <br> There were no missing values in the \"Embarked\" port variable. <br> I'll add the dummy variables to finalize the test set.  <br> Finally, I'll impute the 1 missing value for \"Fare\" with the median, 14.45."},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"145675b90aa2befa533c640aaedd4bf8069b12d4","_cell_guid":"8b9ef076-3669-4339-8d10-0d8783a92e07","collapsed":true},"source":"test_df[\"Age\"].fillna(28, inplace=True)\ntest_df[\"Fare\"].fillna(14.45, inplace=True)\ntest_df.drop('Cabin', axis=1, inplace=True)","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"746e1803237da93829db0c6bf926a83b7dada86a","_cell_guid":"f848ce97-6644-485d-b6e9-ed18f79b34ce","collapsed":true},"source":"test_df['TravelBuds']=test_df[\"SibSp\"]+test_df[\"Parch\"]\ntest_df['TravelAlone']=np.where(test_df['TravelBuds']>0, 0, 1)\n\ntest_df.drop('SibSp', axis=1, inplace=True)\ntest_df.drop('Parch', axis=1, inplace=True)\ntest_df.drop('TravelBuds', axis=1, inplace=True)\n\ntest2 = pd.get_dummies(test_df, columns=[\"Pclass\"])\ntest3 = pd.get_dummies(test2, columns=[\"Embarked\"])\n\ntest4=pd.get_dummies(test3, columns=[\"Sex\"])\ntest4.drop('Sex_female', axis=1, inplace=True)\n\ntest4.drop('PassengerId', axis=1, inplace=True)\ntest4.drop('Name', axis=1, inplace=True)\ntest4.drop('Ticket', axis=1, inplace=True)\nfinal_test = test4","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"bb3cc11f5e5b138f31a58fd7e242a43f73e14f0f","_cell_guid":"760a79a0-4fa2-4f20-8745-b38b8015f919","collapsed":true},"source":"final_test.head(5)","outputs":[]},{"metadata":{"_uuid":"0d43f878d627789df29855c46832eedef63681a7","_cell_guid":"46c9d50c-d2ae-45d4-b8a2-6be16ad61d30"},"cell_type":"markdown","source":"*References for categorical variable creation: <br>\nhttp://pbpython.com/categorical-encoding.html <br>\nhttps://chrisalbon.com/python/data_wrangling/pandas_create_column_using_conditional/*"},{"metadata":{"_uuid":"4e26c19bf719b7086addc0e1981c00836a19f189","_cell_guid":"1430d510-1c8d-4544-8009-3911fff7afbb"},"cell_type":"markdown","source":"## 3. Exploratory Data Analysis"},{"metadata":{"_uuid":"32e9c04a3281fb1aa8c77e1406c56cd820459202","_cell_guid":"2655428b-d69d-4c0f-85ff-e31ada8e37b9"},"cell_type":"markdown","source":"## 3.1 Exploration of Age"},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"790e8d7ca89d19e276b3398e299c42893a796b79","_cell_guid":"9f9ca9e5-50a0-4487-ba53-815dda90af1c","collapsed":true},"source":"plt.figure(figsize=(15,8))\nsns.kdeplot(titanic_df[\"Age\"][df_final.Survived == 1], color=\"darkturquoise\", shade=True)\nsns.kdeplot(titanic_df[\"Age\"][df_final.Survived == 0], color=\"lightcoral\", shade=True)\nplt.legend(['Survived', 'Died'])\nplt.title('Density Plot of Age for Surviving Population and Deceased Population')\nplt.show()\n","outputs":[]},{"metadata":{"_uuid":"6c5625b454f5e01dd6b6d843d801851c14c64d1e","_cell_guid":"8e304d72-27f3-41cf-863f-63872f4c37df"},"cell_type":"markdown","source":"The age distribution for survivors and deceased is actually very similar.  One notable difference is that, of the survivors, a larger proportion were children.  The passengers evidently made an attempt to save children by giving them a place on the life rafts. "},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"9cf1794d9db2fdc314c20ca97a76e9470e81a354","_cell_guid":"d2aa9f59-c433-4258-b8db-225b63a5eab6","collapsed":true},"source":"plt.figure(figsize=(20,4))\navg_survival_byage = round(df_final[[\"Age\", \"Survived\"]].groupby(['Age'],as_index=False).mean(),1)\ng = sns.barplot(x='Age', y='Survived', data=avg_survival_byage, color=\"LightSeaGreen\")\n","outputs":[]},{"metadata":{"_uuid":"67051cf653243b3103c9f8015c501d89d92bd3bc","_cell_guid":"0b636440-ab38-46a8-8cc9-9421683d5c0b"},"cell_type":"markdown","source":"Considering the survival rate of passengers under 16, I'll also include another categorical variable in my dataset: \"Minor\""},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"8918defa6e17b83c700ea45357ebd67a3a22f02f","_cell_guid":"1655b49b-b33f-4236-8b31-d995ef26c6f6","collapsed":true},"source":"df_final['IsMinor']=np.where(train_data['Age']<=16, 1, 0)","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"8773bbf9277fd1baec3e4becd11d7b5bf32c2463","_cell_guid":"45a48287-86eb-4853-be6a-e5be295ce9ed","collapsed":true},"source":"final_test['IsMinor']=np.where(final_test['Age']<=16, 1, 0)","outputs":[]},{"metadata":{"_uuid":"337b3ced0c6423cf1d126f23a7e60c0181af6a47","_cell_guid":"a643b196-91c6-4b12-9463-0f984fbfc91a"},"cell_type":"markdown","source":"## 3.2 Exploration of Fare"},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"4a1c521f08460f6983eca0c4e01294fb7c86e4f9","_cell_guid":"9f31ffe1-7cd8-4169-b193-ed44e56d0bd4","collapsed":true},"source":"plt.figure(figsize=(15,8))\nsns.kdeplot(df_final[\"Fare\"][titanic_df.Survived == 1], color=\"darkturquoise\", shade=True)\nsns.kdeplot(df_final[\"Fare\"][titanic_df.Survived == 0], color=\"lightcoral\", shade=True)\nplt.legend(['Survived', 'Died'])\nplt.title('Density Plot of Fare for Surviving Population and Deceased Population')\n# limit x axis to zoom on most information. there are a few outliers in fare. \nplt.xlim(-20,200)\nplt.show()","outputs":[]},{"metadata":{"_uuid":"2717310b6c443d675c7342be0c2c18b265723273","_cell_guid":"346b7322-a3e4-48df-bbb1-4d8ec7716f3f"},"cell_type":"markdown","source":"As the distributions are clearly different for the fares of survivors vs. deceased, it's likely that this would be a significant predictor in our final model.  Passengers who paid lower fare appear to have been less likely to survive.  This is probably strongly correlated with Passenger Class, which we'll look at next."},{"metadata":{"_uuid":"4524affda51265ea23fa923e2ea7f93d7bb91875","_cell_guid":"cf585311-4029-4be4-8af2-3eea8258801a"},"cell_type":"markdown","source":"## 3.3 Exploration of Passenger Class"},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"08fd677214959e0b938a0f8a94b63ab548673ea5","_cell_guid":"676548e8-6dd4-4180-800c-7b164acb3877","collapsed":true},"source":"sns.barplot('Pclass', 'Survived', data=titanic_df, color=\"darkturquoise\")\nplt.show()","outputs":[]},{"metadata":{"_uuid":"8ddb19191253a6e09dfcb0beff2b3690f1052d52","_cell_guid":"193233f8-b220-4cae-aa0f-f822316d5623"},"cell_type":"markdown","source":"Unsurprisingly, being a first class passenger was safest."},{"metadata":{"_uuid":"2fc06b75321946b721852f78431435f9ba5fef39","_cell_guid":"c59f8e8f-e8c2-40fb-b9c8-12dddd6d318f"},"cell_type":"markdown","source":"## 3.4 Exploration of Embarked Port"},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"a9f0598701c7c5224eaa73dafa869af73beffe18","_cell_guid":"6e5bec50-2f5e-433e-9130-c56956fddad3","collapsed":true},"source":"sns.barplot('Embarked', 'Survived', data=titanic_df, color=\"teal\")\nplt.show()","outputs":[]},{"metadata":{"_uuid":"2f6a0329cf0c7b771a707ec790efc065924e1ee2","_cell_guid":"88d78820-35a5-48fd-a234-9f3ca3fca779"},"cell_type":"markdown","source":"Passengers who boarded in Cherbourg, France, appear to have the highest survival rate.  Passengers who boarded in Southhampton were marginally less likely to survive than those who boarded in Queenstown.  This is probably related to passenger class, or maybe even the order of room assignments (e.g. maybe earlier passengers were more likely to have rooms closer to deck). <br> It's also worth noting the size of the whiskers in these plots.  Because the number of passengers who boarded at Southhampton was highest, the confidence around the survival rate is the highest.  The whisker of the Queenstown plot includes the Southhampton average, as well as the lower bound of its whisker.  It's possible that Queenstown passengers were equally, or even more, ill-fated than their Southhampton counterparts."},{"metadata":{"_uuid":"92bacce85a7dec5509217b9570bc2a2fea6a8452","_cell_guid":"9e6dc87e-ba59-4004-8145-79709328fe27"},"cell_type":"markdown","source":"## 3.5 Exploration of Traveling Alone vs. With Family"},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"e0c3dc16292ef0bcabf0fc680d821ef654084ab4","_cell_guid":"67017a88-93d4-412b-9adf-8b4d1d9b9db0","collapsed":true},"source":"sns.barplot('TravelAlone', 'Survived', data=df_final, color=\"mediumturquoise\")\nplt.show()","outputs":[]},{"metadata":{"_uuid":"f160bd7399e024ae669d55f09caf6e7902768851","_cell_guid":"e9e68cef-5e74-46aa-8343-39afbbf00efe"},"cell_type":"markdown","source":"Individuals traveling without family were more likely to die in the disaster than those with family aboard.  Given the era, it's likely that individuals traveling alone were likely male."},{"metadata":{"_uuid":"693c25c25f3590f0b027725471ddd74d56f154af","_cell_guid":"201b4c9d-b9f0-4ae9-8580-0b4e24ee62be"},"cell_type":"markdown","source":"## 3.6 Exploration of Gender Variable"},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"d264b874ec1e4b2dea1ab61ede322fe903e3d864","_cell_guid":"29d59a53-075d-4f3f-b8c6-de3f4efc5c95","collapsed":true},"source":"sns.barplot('Sex', 'Survived', data=titanic_df, color=\"aquamarine\")\nplt.show()","outputs":[]},{"metadata":{"_uuid":"80c02b9fe2151c443f189cbe44c9cacf7e5c44a4","_cell_guid":"490ed298-f0e4-466b-acc8-81280315e6a2"},"cell_type":"markdown","source":"This is a very obvious difference.  Clearly being female greatly increased your chances of survival."},{"metadata":{"_uuid":"99d25477fb5f2b8e086568a807a689a09692ef53","_cell_guid":"5de685c6-decc-41e9-abde-383733ff9871"},"cell_type":"markdown","source":"References: <br>\nhttps://seaborn.pydata.org/generated/seaborn.barplot.html <br>\nhttps://seaborn.pydata.org/generated/seaborn.kdeplot.html"},{"metadata":{"_uuid":"39dbc095f99dcec6d25a7a4561e81bb641078622","_cell_guid":"c833cbf5-74db-44ff-90fa-b600ff0a09d7"},"cell_type":"markdown","source":"## 4. Logistic Regression and Results"},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"279069df8023b368c30efa3304e0dcd0afe65a88","_cell_guid":"2f86f579-4666-4b03-8da6-80eeb48cb4e8","collapsed":true},"source":"df_final.head(10)","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"69ff389cd4ae7702c907ba252080ac81350ca477","_cell_guid":"fdc40c40-81fb-4bab-bbb3-562d5416dc57","collapsed":true},"source":"cols=[\"Age\", \"Fare\", \"TravelAlone\", \"Pclass_1\", \"Pclass_2\",\"Embarked_C\",\"Embarked_S\",\"Sex_male\",\"IsMinor\"] \nX=df_final[cols]\nY=df_final['Survived']","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"81fbdbb236fba1be540ecb6903505a2b6457f855","_cell_guid":"9f2a9b35-d553-4321-8e64-4d8a21cba48d","collapsed":true},"source":"import statsmodels.api as sm\nfrom scipy import stats\nstats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df)\nlogit_model=sm.Logit(Y,X)\nresult=logit_model.fit()\nprint(result.summary())","outputs":[]},{"metadata":{"_uuid":"e9d52d5b182c0a01218982e844e53d5278e0d98a","_cell_guid":"b1b3b56f-2f5f-47d6-9375-62c11e49ce79"},"cell_type":"markdown","source":"Nearly all variables are significant at the 0.05 alpha level, but we'll run the model again without Fare and TravelAlone (removed one at a time, results didn't change much.  In the end removed both).  I also removed \"IsMinor\" from this regression, as the information provided is redundant to the Age variable."},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"9d87357aa13931e9f0a0379a480d262b198a227f","_cell_guid":"febd185a-bc4b-4210-b1f6-a10954bbdfaa","collapsed":true},"source":"cols2=[\"Age\", \"Pclass_1\", \"Pclass_2\",\"Embarked_C\",\"Embarked_S\",\"Sex_male\"]  \nX2=df_final[cols2]\nY=df_final['Survived']\n\nlogit_model=sm.Logit(Y,X2)\nresult=logit_model.fit()\n\nprint(result.summary())","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"e0b25290f527ae75c125fe07d175504ae762bf84","_cell_guid":"9cef2c68-0004-4581-9474-391318843cec","collapsed":true},"source":"from sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression()\nlogreg.fit(X2, Y)\n\nlogreg.score(X2, Y)","outputs":[]},{"metadata":{"_uuid":"bd8aa37cd432126d450d0ef6d86d37e39f7f6279","_cell_guid":"3557c70f-a3d7-445c-942a-e7b73a34c0c0"},"cell_type":"markdown","source":"## Model's Predictive Score: 0.7935"},{"metadata":{"_uuid":"436611f27bbf7285cf5369430378dbb36070f50b","_cell_guid":"2a6fd9d3-042d-4b93-956b-5d15095dec1f"},"cell_type":"markdown","source":"*References:* <br>\nhttps://github.com/statsmodels/statsmodels/issues/3931 <br>\nhttps://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8"},{"metadata":{"_uuid":"9c28028e41ac498b143e79aa25df54c14099de12","_cell_guid":"8d1ef95d-fdac-49a1-8a68-329c7a3fa799"},"cell_type":"markdown","source":"## 5. Hold-Out Testing"},{"metadata":{"_uuid":"6865f4c059afed78e15668fc31ae6a2fc4e238b5","_cell_guid":"7b5a614e-a254-4965-b4d6-8400047fa1fb"},"cell_type":"markdown","source":"### 5.1 Using Kaggle's Titanic \"Test\" Data"},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"dca69540f9e48bb06b000c3590efaf2a1c6e9fe9","_cell_guid":"d91d3ba4-36ce-46f4-9b8e-66ea9d887d1b","collapsed":true},"source":"#from sklearn.linear_model import LogisticRegression\n#from sklearn import metrics\n#logreg = LogisticRegression()\n#logreg.fit(X2, Y)\n\n#X_test = final_test[cols2]\n#y_test = final_test['Survived']\n\n#y_pred = logreg.predict(X_test)\n#print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))\n","outputs":[]},{"metadata":{"_uuid":"662f9a432fd132fc4bb2937c4a922570cb8bdf16","_cell_guid":"572d7a3b-eaf2-41d7-8bb7-07cb111a4011"},"cell_type":"markdown","source":"</div>\n <div class=\"alert alert-block alert-danger\">\n<font color=red> **Cross Validation: Turns out the test data doesn't have \"survived\" information, so this isn't helpful for our out-of-sample analysis.** </font>\n\n"},{"metadata":{"_uuid":"907d61b84f6f76ef3f8efb9a62f68a03d5de04b8","_cell_guid":"fec69951-fd95-45d8-ba9c-8735721831b1"},"cell_type":"markdown","source":"### 5.2 Using 80-20 Split for Cross Validation"},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"7ee14c32600b2a42f365d671f146d30ebe474452","_cell_guid":"8cb5c46c-41c4-4748-971c-f820bde62e55","collapsed":true},"source":"from sklearn.model_selection import train_test_split\ntrain, test = train_test_split(df_final, test_size=0.2)","outputs":[]},{"metadata":{"_uuid":"b0df7b99dd6a91edaaa3f1990c84f599979a1517","_cell_guid":"dc5c8814-84be-405a-a947-ffec08104359"},"cell_type":"markdown","source":"*References:* <br>\nhttps://stackoverflow.com/questions/24147278/how-do-i-create-test-and-train-samples-from-one-dataframe-with-pandas"},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"0f05e45e32cb8e81cd14db4e33bd532d011f8a75","_cell_guid":"677e898c-6efb-43cd-a31c-12e219cae53b","collapsed":true},"source":"#re-fit logistic regression on new train sample\n\ncols2=[\"Age\", \"Pclass_1\", \"Pclass_2\",\"Embarked_C\",\"Embarked_S\",\"Sex_male\"] \nX3=train[cols2]\nY3=train['Survived']\nlogit_model3=sm.Logit(Y3,X3)","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"14f0d5843c0e7d9685538436ae9a65745cdc41cf","_cell_guid":"9010d0f8-a9a7-4174-984c-ee948581bab8","collapsed":true},"source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\n\nlogreg = LogisticRegression()\nlogreg.fit(X3, Y3)\nlogreg.score(X3, Y3)","outputs":[]},{"metadata":{"_uuid":"e2d06f6339c48f50f4cd42cbe8186597b8c5b59e","_cell_guid":"85a23b6a-7506-4c0b-9e88-c782543672f3"},"cell_type":"markdown","source":"The score for the new training sample (80% of original) is very close to the original performance, which is good!<br>\nLet's assess how well it scores on the 20% hold-out sample."},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"837ef204f480adcebad0ccfdc5508bcad82ca3bd","_cell_guid":"f9246036-0c71-4875-b5fb-2e6db5b17076","collapsed":true},"source":"from sklearn import metrics\nlogreg.fit(X3, Y3)\n\nX3_test = test[cols2]\nY3_test = test['Survived']\n\nY3test_pred = logreg.predict(X3_test)\nprint('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X3_test, Y3_test)))","outputs":[]},{"metadata":{"_uuid":"9deb4949ca11fbffcaec0231a9d80b7a01edbfaf","_cell_guid":"b8979c69-14c6-438d-8d7a-479a242a4f8a"},"cell_type":"markdown","source":"The model's out of sample performance does not show any deterioration.<br>\n*Resources:* <br>\nhttp://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html <br>\nhttps://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8"},{"metadata":{"_uuid":"ec44fbaddbc23f03a8ac470391f41ce35f40cf72","_cell_guid":"f485ca4c-2172-4383-979e-21e78a66192c"},"cell_type":"markdown","source":"## Assessing the model's performance based on ROC/AUC "},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"354d8663b9219459737c651e57a968bcd97e7843","_cell_guid":"5404f653-701a-48a6-a0e0-624a40149d74","collapsed":true},"source":"# Model's in sample AUC\n\nfrom sklearn.metrics import roc_auc_score\nlogreg.fit(X3, Y3)\nY3_pred = logreg.predict(X3)\n\ny_true = Y3\ny_scores = Y3_pred\nroc_auc_score(y_true, y_scores)","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"a9172d2bffa265ea5fadfbf548f70d7e13f84e8b","_cell_guid":"85546d4a-b9ed-4427-b565-445fe9219ab5","collapsed":true},"source":"#Visualizing the model's ROC curve (**source for graph code given below the plot)\nfrom sklearn.metrics import roc_curve, auc\nlogreg.fit(X3, Y3)\n\ny_test = Y3_test\nX_test = X3_test\n \n# Determine the false positive and true positive rates\nFPR, TPR, _ = roc_curve(y_test, logreg.predict_proba(X_test)[:,1])\n \n# Calculate the AUC\n\nroc_auc = auc(FPR, TPR)\nprint ('ROC AUC: %0.3f' % roc_auc )\n \n# Plot of a ROC curve\nplt.figure(figsize=(10,10))\nplt.plot(FPR, TPR, label='ROC curve (area = %0.3f)' % roc_auc)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([-0.05, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve (Test Sample Performance)')\nplt.legend(loc=\"lower right\")\nplt.show()","outputs":[]},{"metadata":{"_uuid":"fb2a0c6234fb538fa0be9c090a58b4a277a7ce5a","_cell_guid":"68b656b0-66f2-4418-9013-f243dc386bfc"},"cell_type":"markdown","source":"An AUC score of 0.5 is effectively as good as the flip of a coin, and means that the model really has no classification power at all between the positive and negative occurences. The AUC for both the test and train samples when run on my logistic regression demonstrates relatively strong power of separation between positive and negative occurences (survived - 1, died - 0).\n\n> ### \"AUC of a classifier is equivalent to the probability that the classifier will rank a randomly chosen positive instance higher than a randomly chosen negative instance.\" -Majnik, Bosnic, 2011<br> \n\n<br> *References*: <br>\nROC Analysis of Classifiers in Machine Learning: A Survey, Matjaz Majnik, Zoran Bosnic, 2011: <br>\nhttp://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.301.969&rep=rep1&type=pdf<br>\n\nhttps://stackoverflow.com/questions/25009284/how-to-plot-roc-curve-in-python\nhttp://www.ultravioletanalytics.com/2014/12/16/kaggle-titanic-competition-part-x-roc-curves-and-auc/"},{"metadata":{"_uuid":"f29821f4daae995de78bfa338688fba748a9980c","_cell_guid":"f8225667-0fe0-4641-98a5-fbba2eaa3d6f"},"cell_type":"markdown","source":"# 6. Logistic Regression Conclusion<br>\n<br> \nBased on my analysis, if you were to be aboard the Titanic, your chances of survival were best if you fit the following criteria:<br>\n* Female\n* Young\n* In First Class \n* Embarked in Cherbourg France\n"},{"metadata":{"_uuid":"740bc96266c96981ada150cc9d50a4056267aca1","_cell_guid":"82d95e6d-a700-4c77-bf29-04f9d58777ea"},"cell_type":"markdown","source":"## 7. Random Forest Estimation"},{"metadata":{},"cell_type":"markdown","source":"Our Logistic Regression is effective and easy to interpret, but there are other ML techniques which could provide a more accurate prediction.  Random forests, a tree-based machine learning technique, often provide more accurate results than Logistic Regression classifier models.  There is a tradeoff for the additional complexity: growing your number of trees too much can subject your model to overfitting and reduce the predictive power of the model. <br> <br>\nI conducted several iterations of a Random Forest model by adjusting the number of trees (n_estimators parameter) and submitted by results for scoring on Kaggle. I tested 40, 80, 100, and 120 trees, and the best out-of-sample predictictive power was achieved with 100 trees."},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"f25ecf956a5b9447f743c8901b9359299fea1ed8","_cell_guid":"f3cc0e9e-b508-4fb2-8159-af6541137bfe","collapsed":true},"source":"from sklearn.ensemble import RandomForestClassifier\n\ncols=[\"Age\", \"Fare\", \"TravelAlone\", \"Pclass_1\", \"Pclass_2\",\"Embarked_C\",\"Embarked_S\",\"Sex_male\",\"IsMinor\"] \nX=df_final[cols]\nY=df_final['Survived']\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X, Y)\nrandom_forest.score(X, Y)","outputs":[]},{"metadata":{"_uuid":"7401b956bdee1419c72702f1dd730b81700ad468","_cell_guid":"55fa70fe-27fe-419a-8ac3-99458fb7c066"},"cell_type":"markdown","source":"*References*:<br>\nhttp://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html <br>\nhttps://stats.stackexchange.com/questions/260460/optimization-of-a-random-forest-model<br>\nhttps://en.wikipedia.org/wiki/Random_forest <br>\nhttps://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm"},{"metadata":{},"cell_type":"markdown","source":"## Final Submission"},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"d3686a38c1e0859fe53bc070d602151a75a9890c","_cell_guid":"73e9ad95-6696-475b-a2eb-27650f8e58e0","collapsed":true},"source":"final_test_RF=final_test[cols]\nY_pred_RF = random_forest.predict(final_test_RF)","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"_kg_hide-output":true,"_uuid":"f92595b431f087b36d88401424663a6f78a70dba","_cell_guid":"419771ee-93ed-4dfa-b1b8-2f18db7a16d9","collapsed":true},"source":"submission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": Y_pred_RF\n    })\nsubmission.to_csv('titanic.csv', index=False)","outputs":[]},{"metadata":{"_uuid":"ac970e8ddb0a6ca633db81ce6f5de22c1fb318f5","_cell_guid":"39d43a7f-4fb0-4cee-bad1-27b99d6ec26a","collapsed":true},"cell_type":"markdown","source":"**Final References:** <br>\n*Editing Markdowns*: https://medium.com/ibm-data-science-experience/markdown-for-jupyter-notebooks-cheatsheet-386c05aeebed<br>\n*Matplotlib color library:* https://matplotlib.org/examples/color/named_colors.html"},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"de71da1b92d4bf8e32ee677fb294aae82ce42981","_cell_guid":"54d27743-346e-4e30-8dbe-50c154eb66cf","collapsed":true},"source":"","outputs":[]}],"metadata":{"language_info":{"version":"3.6.4","pygments_lexer":"ipython3","file_extension":".py","nbconvert_exporter":"python","codemirror_mode":{"version":3,"name":"ipython"},"name":"python","mimetype":"text/x-python"},"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":1}