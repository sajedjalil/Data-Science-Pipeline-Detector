{"cells":[{"cell_type":"markdown","metadata":{},"source":"# Machine Learning from Start to Finish with Scikit-Learn\n\nThis notebook covers the basic Machine Learning process in Python step-by-step. Go from raw data to at least 78% accuracy on the Titanic Survivors dataset. \n\n### Steps Covered\n\n\n1. Importing  a DataFrame\n2. Visualize the Data\n3. Cleanup and Transform the Data\n4. Encode the Data\n5. Split Training and Test Sets\n6. Fine Tune Algorithms\n7. Cross Validate with KFold\n8. Upload to Kaggle"},{"cell_type":"markdown","metadata":{},"source":"## CSV to DataFrame\n\nCSV files can be loaded into a dataframe by calling `pd.read_csv` . After loading the training and test files, print a `sample` to see what you're working with."},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\ndata_train = pd.read_csv('../input/train.csv')\ndata_test = pd.read_csv('../input/test.csv')\n\ndata_train.sample(3)"},{"cell_type":"markdown","metadata":{},"source":"## Visualizing Data\n\nVisualizing data is crucial for recognizing underlying patterns to exploit in the model. "},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"sns.barplot(x=\"Embarked\", y=\"Survived\", hue=\"Sex\", data=data_train);"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"sns.pointplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=data_train,\n              palette={\"male\": \"blue\", \"female\": \"pink\"},\n              markers=[\"*\", \"o\"], linestyles=[\"-\", \"--\"]);"},{"cell_type":"markdown","metadata":{},"source":"## Transforming Features\n\n1. Aside from 'Sex', the 'Age' feature is second in importance. To avoid overfitting, I'm grouping people into logical human age groups. \n2. Each Cabin starts with a letter. I bet this letter is much more important than the number that follows, let's slice it off. \n3. Fare is another continuous value that should be simplified. I ran `data_train.Fare.describe()` to get the distribution of the feature, then placed them into quartile bins accordingly. \n4. Extract information from the 'Name' feature. Rather than use the full name, I extracted the last name and name prefix (Mr. Mrs. Etc.), then appended them as their own features. \n5. Lastly, drop useless features. (Ticket and Name)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"def simplify_ages(df):\n    df.Age = df.Age.fillna(-0.5)\n    bins = (-1, 0, 5, 12, 18, 25, 35, 60, 120)\n    group_names = ['Unknown', 'Baby', 'Child', 'Teenager', 'Student', 'Young Adult', 'Adult', 'Senior']\n    categories = pd.cut(df.Age, bins, labels=group_names)\n    df.Age = categories\n    return df\n\ndef simplify_cabins(df):\n    df.Cabin = df.Cabin.fillna('N')\n    df.Cabin = df.Cabin.apply(lambda x: x[0])\n    return df\n\ndef simplify_fares(df):\n    df.Fare = df.Fare.fillna(-0.5)\n    bins = (-1, 0, 8, 15, 31, 1000)\n    group_names = ['Unknown', '1_quartile', '2_quartile', '3_quartile', '4_quartile']\n    categories = pd.cut(df.Fare, bins, labels=group_names)\n    df.Fare = categories\n    return df\n\ndef format_name(df):\n    df['Lname'] = df.Name.apply(lambda x: x.split(' ')[0])\n    df['NamePrefix'] = df.Name.apply(lambda x: x.split(' ')[1])\n    return df    \n    \ndef drop_features(df):\n    return df.drop(['Ticket', 'Name', 'Embarked'], axis=1)\n\ndef transform_features(df):\n    df = simplify_ages(df)\n    df = simplify_cabins(df)\n    df = simplify_fares(df)\n    df = format_name(df)\n    df = drop_features(df)\n    return df\n\ndata_train = transform_features(data_train)\ndata_test = transform_features(data_test)\ndata_train.head()"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"sns.barplot(x=\"Age\", y=\"Survived\", hue=\"Sex\", data=data_train);"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"sns.barplot(x=\"Cabin\", y=\"Survived\", hue=\"Sex\", data=data_train);"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"sns.barplot(x=\"Fare\", y=\"Survived\", hue=\"Sex\", data=data_train);"},{"cell_type":"markdown","metadata":{},"source":"## Some Final Encoding\n\nThe last part of the preprocessing phase is to normalize labels. The LabelEncoder in Scikit-learn will convert each unique string value into a number, making out data more flexible for various algorithms. \n\nThe result is a table of numbers that looks scary to humans, but beautiful to machines. "},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"from sklearn import preprocessing\ndef encode_features(df_train, df_test):\n    features = ['Fare', 'Cabin', 'Age', 'Sex', 'Lname', 'NamePrefix']\n    df_combined = pd.concat([df_train[features], df_test[features]])\n    \n    for feature in features:\n        le = preprocessing.LabelEncoder()\n        le = le.fit(df_combined[feature])\n        df_train[feature] = le.transform(df_train[feature])\n        df_test[feature] = le.transform(df_test[feature])\n    return df_train, df_test\n    \ndata_train, data_test = encode_features(data_train, data_test)\ndata_train.head()"},{"cell_type":"markdown","metadata":{},"source":"## Splitting up the Training Data\n\nNow its time for some Machine Learning. \n\nFirst, separate the features(X) from the labels(y). \n\n**X_all:** All features minus the value we want to predict (Survived).\n\n**y_all:** Only the value we want to predict. \n\nSecond, use Scikit-learn to randomly shuffle this data into four variables. In this case, I'm training 80% of the data, then testing against the other 20%.  \n\nLater, this data will be reorganized into a KFold pattern to validate the effectiveness of a trained algorithm. "},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"from sklearn.model_selection import train_test_split\n\nX_all = data_train.drop(['Survived', 'PassengerId'], axis=1)\ny_all = data_train['Survived']\n\nnum_test = 0.20\nX_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=num_test, random_state=23)"},{"cell_type":"markdown","metadata":{},"source":"## Fitting and Tuning an Algorithm\n\nNow it's time to figure out which algorithm is going to deliver the best model. I'm going with the RandomForestClassifier, but you can drop any other classifier here, such as Support Vector Machines or Naive Bayes. "},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import make_scorer, accuracy_score\nfrom sklearn.model_selection import GridSearchCV\n\n# Choose the type of classifier. \nclf = RandomForestClassifier()\n\n# Choose some parameter combinations to try\nparameters = {'n_estimators': [4, 6, 9], \n              'max_features': ['log2', 'sqrt','auto'], \n              'criterion': ['entropy', 'gini'],\n              'max_depth': [2, 3, 5, 10], \n              'min_samples_split': [2, 3, 5],\n              'min_samples_leaf': [1,5,8]\n             }\n\n# Type of scoring used to compare parameter combinations\nacc_scorer = make_scorer(accuracy_score)\n\n# Run the grid search\ngrid_obj = GridSearchCV(clf, parameters, scoring=acc_scorer)\ngrid_obj = grid_obj.fit(X_train, y_train)\n\n# Set the clf to the best combination of parameters\nclf = grid_obj.best_estimator_\n\n# Fit the best algorithm to the data. \nclf.fit(X_train, y_train)\n\n"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"predictions = clf.predict(X_test)\nprint(accuracy_score(y_test, predictions))"},{"cell_type":"markdown","metadata":{},"source":"## Validate with KFold\n\nIs this model actually any good? It helps to verify the effectiveness of the algorithm using KFold. This will split our data into 10 buckets, then run the algorithm using a different bucket as the test set for each iteration. "},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"from sklearn.cross_validation import KFold\n\ndef run_kfold(clf):\n    kf = KFold(891, n_folds=10)\n    outcomes = []\n    fold = 0\n    for train_index, test_index in kf:\n        fold += 1\n        X_train, X_test = X_all.values[train_index], X_all.values[test_index]\n        y_train, y_test = y_all.values[train_index], y_all.values[test_index]\n        clf.fit(X_train, y_train)\n        predictions = clf.predict(X_test)\n        accuracy = accuracy_score(y_test, predictions)\n        outcomes.append(accuracy)\n        print(\"Fold {0} accuracy: {1}\".format(fold, accuracy))     \n    mean_outcome = np.mean(outcomes)\n    print(\"Mean Accuracy: {0}\".format(mean_outcome)) \n\nrun_kfold(clf)\n"},{"cell_type":"markdown","metadata":{},"source":"## Predict the Actual Test Data\n\nAnd now for the moment of truth. Make the predictions, export the CSV file, and upload them to Kaggle."},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"ids = data_test['PassengerId']\npredictions = clf.predict(data_test.drop('PassengerId', axis=1))\n\n\noutput = pd.DataFrame({ 'PassengerId' : ids, 'Survived': predictions })\n# output.to_csv('titanic-predictions.csv', index = False)\noutput.head()"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0}