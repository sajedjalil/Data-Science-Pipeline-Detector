{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# KEY NOTE\n\nThis notebook is complete guide to pyspark. Since most technologies and industries now a days are working on cloud, it is good to have knowledge on how to apply ML and Datascience when it comes to \"Big Data\". I will be modifying the code and content already available so that we can use it with simple dataset like \"Titanic\" which is the hello world of ML.\n\nThese are just my personal notes. I am sharing these so that it helps others too, who are trying to learn the similar concepts. Any Feedback is appreciated.\n\nThis is the first notebook in pyspark series."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"<a id='top'></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\">Notebook Navigation</h3>\n\n[1. What is Spark?](#1)     \n[2. Using Spark in Python](#2)    \n[3. Using DataFrames](#3)     \n[4. Creating a SparkSession](#4)     \n[5. Viewing Tables](#5)     \n[6. Querying the Data](#6)     \n[7. Pandafy a Spark DataFrame](#7)         \n[8. Upside Down with Spark](#8)     \n[9. Dropping the Middle Man](#9)     \n[10. Creating Columns](#10)         "},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1\"></a>\n# 1. What is Spark?\n\nSpark is a platform for cluster computing. Spark lets you spread data and computations over clusters with multiple nodes (think of each node as a separate computer). Splitting up your data makes it easier to work with very large datasets because each node only works with a small amount of data.\n\nAs each node works on its own subset of the total data, it also carries out a part of the total calculations required, so that both data processing and computation are performed in parallel over the nodes in the cluster. It is a fact that parallel computation can make certain types of programming tasks much faster.\n\nHowever, with greater computing power comes greater complexity.\n\nIf you are deciding whether or not Spark is the best solution for your problem you can consider questions like:\n* Is my data too big to work with on a single machine?\n* Can my calculations be easily parallelized?"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"2\"></a>\n# 2. Using Spark in Python\n\nThe first step in using Spark is connecting to a cluster.\n\nIn practice, the cluster will be hosted on a remote machine that's connected to all other nodes. There will be one computer, called the master that manages splitting up the data and the computations. The master is connected to the rest of the computers in the cluster, which are called worker. The master sends the workers data and calculations to run, and they send their results back to the master.\n\nWhen just getting started with Spark it's simpler to just run a cluster locally. Thus for now, instead of connecting to another computer, all computations will be run on Kaggle's servers in a simulated cluster.\n\nCreating the connection is as simple as creating an instance of the SparkContext class. The class constructor takes a few optional arguments that allow us to specify the attributes of the cluster we're connecting to.\n\nAn object holding all these attributes can be created with the SparkConf() constructor. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# installing pyspark in container\n!pip install pyspark","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3\"></a>\n# 3. Using DataFrames\n\nSpark's core data structure is the Resilient Distributed Dataset (RDD). This is a low level object that splits data across multiple nodes in the cluster. However, RDDs are hard to work with directly, so we'll be using the Spark DataFrame abstraction built on top of RDDs.\n\nThe Spark DataFrame was designed to behave a lot like a SQL table (a table with variables in the columns and observations in the rows). Not only are they easier to understand, DataFrames are also more optimized for complicated operations than RDDs.\n\nWhen we start modifying and combining columns and rows of data, there are many ways to arrive at the same result, but some often take much longer than others. When using RDDs, it's up to the data scientist to figure out the right way to optimize the query, but the DataFrame implementation has much of this optimization built in!\n\nTo start working with Spark DataFrames, we first have to create a SparkSession object from your SparkContext. You can think of the SparkContext as your connection to the cluster and the SparkSession as your interface with that connection."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4\"></a>\n# 4. Creating a SparkSession\n\nCreating multiple SparkSessions and SparkContexts can cause issues, so it's best practice to use the SparkSession.builder.getOrCreate() method. This returns an existing SparkSession if there's already one in the environment, or creates a new one if required"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import SparkSession from pyspark.sql\nfrom pyspark.sql import SparkSession\n\n# Create my_spark\nspark_ex = SparkSession.builder.getOrCreate()\n\n# Print my_spark\nprint(spark_ex)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"5\"></a>\n# 5. Viewing Tables\nOnce we've created a SparkSession, we can start poking around to see what data is in your cluster!\n\nSparkSession has method called catalog which lists all the data inside the cluster.Further .listTables() method, returns the names of all the tables in your cluster as a list."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print the tables in the catalog\nprint(spark_ex.catalog.listTables())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here there are no predefined tables in our local cluster"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"6\"></a>\n# 6. Querying the Data\n\nOne of the advantages of the DataFrame interface is that you can run SQL queries on the tables in your Spark cluster.\n\n*If we had tables in our cluster,we would directly start firing our queries but since we have no tables in our cluster. I will just create one here and you can skip that part and focus more on queries for now. we ll cover it later. (I have hidden those code lines but you can always expand and see it!)*\n\nRunning a query on this table is as easy as using the .sql() method on your SparkSession. Lets try this."},{"metadata":{"trusted":true},"cell_type":"code","source":"#registering a table in catalog\nimport pandas as pd\ndf1 = pd.read_csv(\"../input/titanic/train.csv\")\ndf2=df1.iloc[:,0:4]\nspark_df = spark_ex.createDataFrame(df2)\nspark_df.registerTempTable(\"sample_table\")\n#spark_df.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Don't change this query\nquery = \"FROM sample_table SELECT * LIMIT 10\"\n\n# Get the first 10 rows of flights\ntitanic10 = spark_ex.sql(query)\n\n# Show the results\ntitanic10.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"7\"></a>\n# 7. Pandafy a Spark DataFrame\n\nSuppose we've run a query on your huge dataset and aggregated it down to something a little more manageable. Sometimes it makes sense to then take that table and work with it locally using a tool like pandas.We can do that wirh .toPandas() method. \n\nThis time the query counts the number of survived passengers when grouped by Pclass"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Don't change this query\nquery = \"SELECT Pclass,COUNT(*) as Survived_Count FROM sample_table GROUP BY Pclass\"\n\n# Run the query\ntitanic_counts = spark_ex.sql(query)\n\n# Convert the results to a pandas DataFrame\npd_counts = titanic_counts.toPandas()\n\n# Print the head of pd_counts\nprint(pd_counts.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"8\"></a>\n# 8. Upside Down with Spark\n\nLets now put a pandas DataFrame into a Spark cluster. The SparkSession class has a method for this as well.\n\nThe .createDataFrame() method takes a pandas DataFrame and returns a Spark DataFrame.\n\nSince it is stored locally and not in spark catalog we can do use the .createTempView() Spark DataFrame method, which takes as its only argument the name of the temporary table you'd like to register. This method registers the DataFrame as a table in the catalog, but as this table is temporary, it can only be accessed from the specific SparkSession used to create the Spark DataFrame.\n\nThere is also the method .createOrReplaceTempView(). This safely creates a new temporary table if nothing was there before, or updates an existing table if one was already defined. We'll use this method to avoid running into problems with duplicate tables.\n\nCheck out the diagram to see all the different ways your Spark data structures interact with each other.\n![](https://s3.amazonaws.com/assets.datacamp.com/production/course_4452/datasets/spark_figure.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\npd_temp = pd.DataFrame(np.random.random(10))\nspark_temp = spark_ex.createDataFrame(pd_temp)\nprint(spark_ex.catalog.listTables())\nspark_temp.createOrReplaceTempView(\"temp\")\n\n# Examine the tables in the catalog again\nprint(spark_ex.catalog.listTables())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"9\"></a>\n# 9. Dropping the Middle Man\n\nWe can also read the text file straight into Spark instead of first converting into pandas dataframe.\n\nSparkSession has a .read attribute which has several methods for reading different data sources into Spark DataFrames. Using these you can create a DataFrame from a .csv file"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Don't change this file path\nfile_path = \"../input/titanic/test.csv\"\n\n# Read in the titanic data\ntitanic = spark_ex.read.csv(file_path,header=True)\n\n# Show the data\ntitanic.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"10\"></a>\n# 10. Creating Columns\n\nWe wil now use the methods defined by Spark's DataFrame class to perform common data operations.\n\nLet's look at performing column-wise operations. In Spark we can do this using the .withColumn() method, which takes two arguments. First, a string with the name of your new column, and second the new column itself.\n\nThe new column must be an object of class Column. Creating one of these is as easy as extracting a column from your DataFrame using df.colName.\n\nUpdating a Spark DataFrame is somewhat different than working in pandas because the **Spark DataFrame is immutable**. This means that it can't be changed, and so columns can't be updated in place.Hence all these methods return a new DataFrame. To overwrite the original DataFrame you must reassign the returned DataFrame using the method like so:\n`\ndf = df.withColumn(\"newCol\", df.oldCol + 1)`"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add Fare x100 Column\ntitanic = titanic.withColumn(\"Fare x 100\",titanic.Fare*100)\ntitanic.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Please check out the next notebook in continuation [here](https://www.kaggle.com/amritvirsinghx/scalable-data-science-pyspark-dataframes-nb2)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}