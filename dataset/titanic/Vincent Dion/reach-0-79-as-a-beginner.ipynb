{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Hello wanderer !\n\nWelcome to the new version of my notebook, previously known as \"Reach > 0.80 as a noob\", and now named \"Reach > 0.79 as a beginner\". You may have noticed a drastic reduce in the score displayed in the title, I have to admit I'm a bit sad by this, 0.79+ is not as near cool as 0.80+ is, especially since I ascended from complete noob to humble beginner in the meantime, you would thought I learnt how to do better in score. Well, no and yes actually. Let's jump in !\n\nEarly note to the potential readers, this notebook is loooooooooong, 'cause I talk a looooooooot, but if you're a real beginner like me, I hope I'll make your reading worth (at least for the code part, you can skip the markdown stuff). Also, the code you gonna see is not super clean, not because I don't care, but because I have to be better at it, sorry in advance if some of this is hard to watch (at least it works). Last thing,  I don't use EDA nor ML advanced stuff in this notebook, you can find plenty of that on other notebooks, I'd rather want to focus on simple things sometimes overlooked :\n- Transformers and Pipelines in pre-processing\n- Ensembling\n- Common mistakes of beginners, learnt the hard way\n\n*Once again, no seaborn nor matplotlib here. Don't get me wrong, those are essentials, super useful even, and I would definitely use them on another notebook to increase my score, but not on this one. This one goes straight to the point.*\n\n# **Why the downgrade in score ??**\n\nThis new notebook, although it achieves not so good of a score compare to my previous one (which was 0.815 !) is actually future proof ! To understand what does this mean, Kaggle has a neat system where the test set of the Titanic competition is often changed, it's really good against cheaters on top of the leaderboard (the shameless people with more than 0.95 in score), but it cas also drastically reduce the score of other contestants who submit a model that generalize very poorly ..... yeah you get it now, my precious 0.815 score went down to 0.77 overnight. ML adepts will say what kind of disease my model carried, and you migth have heard it too, **OVERFITTING**. \n\nIf you red anything in Machine Learning, you red about overfitting at least once, generally in training phase. The thing I learnt with this competition though, it's that you can overfit in \"submission phase\", meaning overfitting the test data ! In my case, this overfitting happens because I actually submitted too much, tweaking my model bit by bit in order to increase the final score. I didn't realize back then, but that's overfitting, and in real world situation it is dangerous. My new objective with this notebook, reach a decent score in one submission !! Let's do this !\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport warnings\nwarnings.filterwarnings('ignore')\npd.options.display.max_columns = 40","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Let's load the data\ntrain_data = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/titanic/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We concat the two dataset in order to work on the missing values\nfull_dataset = pd.concat([train_data, test_data])\n# Let's then have a look at the features\nfull_dataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pre-processing - Part One\n\nIn order to practice, I wanted to try the custom transformers. It looked kinda neat to be able to create my own custom pipelines with custom transformers, I thought it would take me a long time to learn it and surprise, it's actually rather easy to have simple transformers ! I definitely invite you to try.\n\nSince it was interesting to make those, I decided to create a lot of transformers. The idea was simple, at the end of it, I wanted a super-mega-giga pipeline that apply a bunch of changes to the dataset. To my susprise (another one!), not only does this work really well, it also works extremely fast, changes are applied in few seconds it's awesome.\n\nI'll explain every transformers with in-code commentaries, but since there is a lot of them, I chose to only display 2 of them. Click the code button if you want to see more of them. The idea behind this Part One is to handle missing data and create meaningful features for every initial features but Age (see Part Two). Part Three will focus on one last feature that I couldn't put into a transformer 'cause I'm still a beginner\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass KnownCabinTransformer(BaseEstimator, TransformerMixin):\n    \"\"\"\n    This first transformer is rather easy, goal is to create\n    a new column, 'KnownCabin' where value is 0 when we don't\n    know the cabin location, and 1 when we do.\n    For this notebook, this is the only thing I do with cabin,\n    but I'm sure there is plenty more to do with this information.\n    \"\"\"\n    def __init__(self):\n        print(\"- KnownCabin transformer initiated -\")\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        X.Cabin.fillna(0, inplace=True)\n        X[\"KnownCabin\"] = 0\n        X.loc[X.Cabin != 0, 'KnownCabin'] = 1\n        \n        print(\"- KnownCabin transformer applied -\")\n             \n        return X\n            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TitleTransformer(BaseEstimator, TransformerMixin):\n    \"\"\" \n    A way more interesting Transformer, when I extract the Title \n    of every passenger thanks to a regular expression, and create\n    dummies out of them. Those Titles also gonna be useful for \n    other features later on.\n    \"\"\"\n    def __init__(self):\n        print(\"- Title transformer initiated -\")\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        X['Title'] = X.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n        \n        #Different variations of nobility/professions, all called 'Rare'\n        X['Title'] = X['Title'].replace(['Lady', 'Countess','Capt', 'Col',\n                                         'Don', 'Dr', 'Major', 'Rev', \n                                         'Sir', 'Jonkheer', 'Dona'], \n                                         'Rare')\n        \n        #Different (French) variations of Miss and Mrs\n        X['Title'] = X['Title'].replace(['Mlle', 'Ms'], 'Miss')\n        X['Title'] = X['Title'].replace('Mme', 'Mrs')\n        \n        #If a title is not treated, I count it as rare\n        X['Title'] = X['Title'].fillna('Rare')\n        \n        title_dummies = pd.get_dummies(X['Title'], prefix=\"Title\")\n        \n        X = pd.concat([X, title_dummies], axis=1)\n        \n        # Drop of Title column for redundancy\n        X = X.drop('Title', axis=1)\n        \n        \n        print(\"- Title transformer applied -\")\n             \n        return X","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From now on, the rest of my transformers are in one big chunk of code just below. Feel free to look at them too by clicking on the \"code\" button or directly go to Part Two.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"class MissingFareTransformer(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Simple transformer to handle missing values in Fare. It looks\n    at the class of the passenger before giving the median fare \n    associated with this class.\n    It's not optimal as you would also like to look at the number \n    of people under the same ticket, but it's a beginning.    \n    \"\"\"\n    def __init__(self):\n        print(\"- MissingFare transformer initiated -\")\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        X.loc[(X.Pclass == 1) & (X.Fare.isnull()), 'Fare'] = X.loc[X.Pclass == 1][\"Fare\"].median()\n        X.loc[(X.Pclass == 2) & (X.Fare.isnull()), 'Fare'] = X.loc[X.Pclass == 2][\"Fare\"].median()\n        X.loc[(X.Pclass == 3) & (X.Fare.isnull()), 'Fare'] = X.loc[X.Pclass == 3][\"Fare\"].median()\n        \n        print(\"- MissingFare transformer applied -\")\n        \n        return X\n    \n\nclass EmbarkedTransformer(BaseEstimator, TransformerMixin):\n    \"\"\"\n    An other simple transformer, giving to the missing values in\n    Embarked the most common value, S in that case. We also use\n    this transformer to create dummies of Embarked.\n    \"\"\"\n    def __init__(self):\n        print(\"- MissingEmbarked transformer initiated -\")\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        self.most_frequent = X[\"Embarked\"].value_counts().index[0]\n        X[\"Embarked\"].fillna(self.most_frequent, inplace=True)\n        \n        embarked_dummies = pd.get_dummies(X['Embarked'], prefix=\"Embarked\")\n        \n        X = pd.concat([X, embarked_dummies], axis=1)\n        \n        # Drop of Embarked column for redundancy\n        X = X.drop('Embarked', axis=1)\n        \n        print(\"- MissingEmbarked transformer applied -\")\n        \n        return X\n    \n\nclass HypotheticalMissingsTransformer(BaseEstimator, TransformerMixin):\n    \"\"\"\n    This one is actually useless, but future-proof !! If the test set\n    changes again, this transformer will deal with new missing values \n    in features without missing values at the time of this notebook.\n    The only column I don't deal with is the Ticket, being a tad to\n    difficult to generate random Ticket or finding families that would\n    share the same number.\n    \"\"\"\n    def __init__(self):\n        print(\"- HypotheticalMissings transformer initiated -\")\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        #Pclass is handled by frequency\n        self.most_frequent_class = X[\"Pclass\"].value_counts().index[0]\n        \n        self.unknown_name = \"unknown\"\n        \n        # Assuming people with NaN in SibSp and Parch got no family with them\n        self.horizontal_family = 0\n        self.vertical_family = 0     \n        \n        X[\"Pclass\"].fillna(self.most_frequent_class, inplace=True)\n        X[\"Name\"].fillna(self.unknown_name, inplace=True)\n        \n        X[\"SibSp\"].fillna(self.horizontal_family, inplace=True)\n        X[\"Parch\"].fillna(self.vertical_family, inplace=True)\n        \n        # We go a little further with sex, kinda important in our models\n        # Mr., Master. and Rare are men, others are female\n        X.loc[(X.Sex.isnull()) & (X.Title_Master == 1), \"Sex\"] == \"male\"\n        X.loc[(X.Sex.isnull()) & (X.Title_Mr == 1), \"Sex\"] == \"male\"\n        # We assume 'rare' title holders are more likely to be male than female\n        X.loc[(X.Sex.isnull()) & (X.Title_Rare == 1), \"Sex\"] == \"male\"\n        \n        X.loc[(X.Sex.isnull()) & (X.Title_Miss == 1), \"Sex\"] == \"female\"\n        X.loc[(X.Sex.isnull()) & (X.Title_Mrs == 1), \"Sex\"] == \"female\"\n        \n        print(\"- HypotheticalMissings transformer applied -\")\n        \n        return X\n\n    \nclass GenderTransformer(BaseEstimator, TransformerMixin):\n    \"\"\"\n    This transformer could have been a simple use of dummies,\n    but at this point I was full-on in the use of transformers,\n    transformers are cool, Michael Bay approves !\n    \"\"\"\n    def __init__(self):\n        print(\"- Gender transformer initiated -\")\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        X['Sex'] = X['Sex'].map({'male': 0, 'female': 1})\n        \n        print(\"- Gender transformer applied -\")\n        \n        return X\n\n\nclass PclassTransformer(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Cf GenderTransformer commentaries, transformers rule!\n    \"\"\"\n    def __init__(self):\n        print(\"- Pclass transformer initiated -\")\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        pclass_dummies = pd.get_dummies(X['Pclass'], prefix='Pclass')\n        \n        X = pd.concat([X, pclass_dummies], axis=1)\n        \n        X = X.drop('Pclass', axis=1)\n        \n        print(\"- Pclass transformer applied -\")\n        \n        return X\n    \n\nclass FamilySizeTransformer(BaseEstimator, TransformerMixin):\n    \"\"\"\n    A simple feature created thanks to other existing features.\n    FamilySize is used by many other notebooks and yield some\n    decent results.\n    \"\"\"\n    def __init__(self):\n        print(\"- FamilySize transformer initiated -\")\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        X[\"FamilySize\"] = X[\"SibSp\"] + X[\"Parch\"] + 1\n        \n        print(\"- FamilySize transformer applied -\")\n        \n        return X","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Pre-processing - Part Two**\n\nI decided to split the handling of the \"Age\" missing values in a second part. I saw some notebook using a mean() or median() method, but I wanted to go a little further and use some basic deduction. The idea was essentialy to recognize children within the passengers with missing data in Age, children being more likely to survive (\"Children and women first\" is a popular memory of tragedies such as the Titanic). A simple example then, every passenger with the Title Master (or Title_Master == 1 in my case) are children. Same logic, a lot of Miss 'might' be children too, when Mr. and Mrs are definitely not. The following transformers will then focus directly or indirectly on the age of the passengers.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class AgeStatusTransformer(BaseEstimator, TransformerMixin):\n    \"\"\"\n    I wanted to distinguish people from whom we know for sure their age,\n    those from whom the age is estimated (floating value over 1 yo), and\n    those from whom we guessed the age (missing values at the start of the\n    exercice)\n    \"\"\"\n    def __init__(self):\n        print(\"- AgeStatus transformer initiated -\")\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        X[\"AgeStatus\"] = \"known\"\n        #Toddlers under 1 have their age displayed in float, it's not a estimation\n        X.loc[(X[\"Age\"] > 1) & ((X[\"Age\"]*2)%2 != 0), \"AgeStatus\"] = \"estimated\"\n        X.loc[X[\"Age\"].isnull(), \"AgeStatus\"] = \"guessed\"\n        \n        age_status_dummies = pd.get_dummies(X['AgeStatus'], prefix=\"AgeStatus\")\n        \n        X = pd.concat([X, age_status_dummies], axis=1)\n        \n        # Drop of AgeStatus column for redundancy\n        X = X.drop('AgeStatus', axis=1)\n        \n        print(\"- AgeStatus transformer applied -\")\n             \n        return X\n            ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Ticket_grouping**\n\nThis transformer achieves a little bit the same work than the FamilySize one, but I found it more accurate to work with for some other transformers later on (Age and Tragedy). The idea is that there is a decent number of people travelling under the same ticket but are not family members, this column can show it.\n\nIt is far from ideal, and someone who wants to have better results might use a more precised transformer looking at the name and ticket for example, to really show the different groups travelling together. This one I used is just the easy way.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import category_encoders as ce\n\nclass TicketGroupingTransformer(BaseEstimator, TransformerMixin):\n    \"\"\"\n    I apply a category encoder on the Ticket, if 6 people share the\n    same ticket, the value of the new column will be 6.\n    \"\"\"\n    def __init__(self):\n        print(\"- TicketGrouping transformer initiated -\")\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        cat_feature = ['Ticket']\n        count_enc = ce.CountEncoder(cols=cat_feature)\n        count_enc.fit(X[cat_feature])\n        grouping_col = count_enc.transform(X[cat_feature]).add_suffix(\"_grouping\")\n        X = pd.concat([X, grouping_col], axis=1)\n        \n        print(\"- TicketGrouping transformer applied -\")\n        \n        return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class AgeGuessingTransformer(BaseEstimator, TransformerMixin):\n    \"\"\"\n    The transformer that guess the ages way more precisely than mean or\n    median, and I made the previous change in part two. Every change is\n    explained but it is rather intuitive.\n    \n    I never apply hard-coded value of course, I just use the median of\n    a more precise subgroup.\n    \"\"\"\n    def __init__(self):\n        print(\"- AgeGuessing transformer initiated -\")\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        # Every 'Master' is a child, we apply the median of all Master (4yo)\n        X.loc[(X.Age.isnull() == True) & (X.Title_Master == 1), \"Age\"] = X.loc[X.Title_Master == 1][\"Age\"].median()\n        \n        # Every Miss with at least a sibling (no spouse obviously), is more \n        # likely to be a child too (it's not sure, just more likely) (13yo)\n        X.loc[(X.Age.isnull() == True) & (X.SibSp > 0) & (X.Title_Miss == 1), \"Age\"] = X.loc[(X.Title_Miss == 1) & (X.SibSp > 0)][\"Age\"].median()\n        \n        # People alone are way more likely to be adult (28yo)\n        X.loc[(X.Age.isnull() == True) & (X.Ticket_grouping == 1), \"Age\"] = X.loc[(X.Ticket_grouping == 1)][\"Age\"].median()\n        \n        # People with more than 1 sibling/spouse are more likely to be children (13yo)\n        X.loc[(X.Age.isnull() == True) & (X.SibSp > 1), \"Age\"] = X.loc[(X.Age.isnull() == False) & (X.SibSp > 1)][\"Age\"].median()\n        \n        # The rest is more likely to be adults (27yo)\n        X.loc[(X.Age.isnull()), \"Age\"] = X.loc[(X.Age.isnull() == False) & (X.Ticket_grouping > 1)][\"Age\"].median()\n        \n        \n        print(\"- AgeGuessing transformer applied -\")\n        \n        return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class AgeBinningTransformer(BaseEstimator, TransformerMixin):\n    \"\"\"\n    I use a binning technique to create 6 categories of age,\n    a better solution would be to look at the distribution of\n    age before applying the bins, I was just lazy.\n    \"\"\"\n    def __init__(self):\n        print(\"- AgeBinning transformer initiated -\")\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        self.bins = [0, 2, 16, 25, 40, 50, np.inf]\n        self.age_cat = [\"0_2\", \"2_16\", \"16_25\", \"25_40\", \"30_50\", \"50+\"]\n        \n        X[\"AgeBins\"] = pd.cut(X[\"Age\"], bins=self.bins, labels=self.age_cat)\n        \n        age_bins_dummies = pd.get_dummies(X['AgeBins'], prefix=\"AgeBins\")\n        \n        X = pd.concat([X, age_bins_dummies], axis=1)  \n        \n        print(\"- AgeBinning transformer applied -\")\n        \n        return X","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pre-processing - Pipeline joy\n\nThis is my biggest joy of this notebook, and it was a great occasion for me to practice transformers. It all comes to this, this badass pipeline that transform the initial dataset into a way more meaningful in just a few seconds. Almost got me to cry a little, of joy.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\n\ncustom_pipeline = Pipeline([\n        (\"cabin_trans\", KnownCabinTransformer()),\n        (\"title_trans\", TitleTransformer()),\n        (\"hypo_missing_trans\", HypotheticalMissingsTransformer()),\n        (\"missing_fare_trans\", MissingFareTransformer()),\n        (\"embarked_trans\", EmbarkedTransformer()),\n        (\"gender_trans\", GenderTransformer()),\n        (\"class_trans\", PclassTransformer()),\n        (\"family_size_trans\", FamilySizeTransformer()),\n        (\"ticket_grp_trans\", TicketGroupingTransformer()),\n        (\"age_status_trans\", AgeStatusTransformer()),\n        (\"age_guessing_trans\", AgeGuessingTransformer()),\n        (\"age_binning_trans\", AgeBinningTransformer()),\n    ])\n\nfull_dataset = custom_pipeline.fit_transform(full_dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# LOOK AT THIS, SUCH A BEAUTY !!!\nfull_dataset.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pre-processing - Part Three\n\nIt needed to be a trilogy. Why ? Because I still have some troubles with transformers, that's why. The following pre-modeling change is huge to me, but I don't know why putting it in a transformers caused me many bugs, so here it is, a stupid ass loop, sooooo sad.\n\nThe idea behind this last transformation is to create a column called \"TS_Tragedy\" for Ticket Sharing Tragedy. More specifically, I want to know if at least 2/3 of the passengers of a given group (sharing the same ticket) died. It took many a long time of reflexion to come up with this, at first I just wanted to create a feature that looked at \"vertical survivability\", meaning does both the parents of a child died (resulting in the child being more likely to not survive either) ? And does every child of a parent died (resulting in the parent being more likely to not have survived). \n\nI settled for this \"TS_Tragedy\" although more general, because it gave me good enough results for children (traditional models of this competition tend to over estimate the survivability of children). Around 300 passengers are concerned, and a hundred got a \"yes\" as value, meaning 2/3 of their group are reported dead.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# I set up the column as null, staying this way for people in group of 2 or less\nfull_dataset[\"TS_Tragedy\"] = \"null\"\ni = 0\nunique_tickets_subset = full_dataset.loc[full_dataset[\"Ticket_grouping\"] > 2][\"Ticket\"].unique()\nwhile i < len(unique_tickets_subset):\n    subset = full_dataset.loc[full_dataset.Ticket == unique_tickets_subset[i]]\n    try:\n        # I chose to have the number of people in training set as denominator rather than total on ticket\n        ratio = len(subset.loc[subset[\"Survived\"] == 0]) / len(subset.loc[subset.Survived.isnull() == False])\n        if ratio > 0.65 :\n            full_dataset.loc[full_dataset.Ticket == unique_tickets_subset[i], \"TS_Tragedy\"] = \"yes\"\n        else:\n            full_dataset.loc[full_dataset.Ticket == unique_tickets_subset[i], \"TS_Tragedy\"] = \"no\"\n        i += 1\n    \n    #ZeroDivisionError caused by a full group in test set, I pass on it\n    except ZeroDivisionError:\n        i += 1\n        \nTST_dummies = pd.get_dummies(full_dataset['TS_Tragedy'], prefix=\"TST\")\n        \nfull_dataset = pd.concat([full_dataset, TST_dummies], axis=1)\n        \nfull_dataset = full_dataset.drop('TS_Tragedy', axis=1)\n\nfull_dataset.head(3)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We reindex the full dataset, before dropping the PassengerId col later on\nfull_dataset.set_index(full_dataset.PassengerId, verify_integrity = True, inplace=True)\nfull_dataset.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# MODELLING - INTRO\n\nLet's go on the fun part, creating the model. Many other changes could have been done in pre-processing, but I have enough for this notebook. A good practicionner would also make some neat EDA at this point, good looking graph and stuff, but I'm not a good practicionner (kidding aside, plenty other notebooks provide amazing graphs and figures, give it a look !)\n\nSPOILER ALERT : I use an ensemble model that actually performs not that great (hence the under 0.8 score). I just wanted to practice on ensembling and it was amazingly fun. I strongly invite you to give it a try if you're a beginner too.[](http://)\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"First thing first, we split back the data into training and testing set, and we drop some non-used feature.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# split back into train and test\n\nX_train = full_dataset.loc[full_dataset.Survived.isnull() == False]\ny_train = X_train[\"Survived\"].astype(int)\nX_test = full_dataset.loc[full_dataset.Survived.isnull()]\n\n### Dropping non used features (and target Survived)\nuseless_features = [\"PassengerId\", \"Survived\", \"Name\", \"Age\", \"Ticket\", \"Fare\", \"Cabin\", \"AgeBins\"]\n\nX_train.drop(useless_features, axis=1, inplace=True)\nX_test.drop(useless_features, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Because I'm using a SVM model in my ensembling, I need to apply some standard scaling to my data, the following code use a scikit transformer design for this effect and apply it on the data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nX_train_scaled = pd.DataFrame(scaler.fit_transform(X_train[[\"SibSp\",\"Parch\",\"FamilySize\",\"Ticket_grouping\"]]), columns=[\"sc_SibSp\",\"sc_Parch\",\"sc_FamilySize\",\"sc_Ticket_grouping\"], index=X_train.index)\nX_test_scaled = pd.DataFrame(scaler.fit_transform(X_test[[\"SibSp\",\"Parch\",\"FamilySize\",\"Ticket_grouping\"]]), columns=[\"sc_SibSp\",\"sc_Parch\",\"sc_FamilySize\",\"sc_Ticket_grouping\"], index=X_test.index)\n\nX_train = pd.concat([X_train, X_train_scaled], axis=1)\nX_train.drop([\"SibSp\",\"Parch\",\"FamilySize\",\"Ticket_grouping\"], axis=1, inplace=True)\nX_test = pd.concat([X_test, X_test_scaled], axis=1)\nX_test.drop([\"SibSp\",\"Parch\",\"FamilySize\",\"Ticket_grouping\"], axis=1, inplace=True)\n\nX_train.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# MODELLING - MODELS PARAMETERS\n\nThis following part is the heart of the ensembling model, I fit every model after using a GridSearchSV to find some good parameters. There is a lot so I chose to display only the KNeighbors example, but as for the transformers, you can expand the code by clicking on the code \"button\".","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Various imports\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# KNeighbors example\n\nparam_grid = [\n    {'n_neighbors':[2,3,4,5,6,7,8], 'weights':['uniform', 'distance'], 'n_jobs':[-1] }\n]\n\nneigh = KNeighborsClassifier()\n\ngrid_search = GridSearchCV(neigh, param_grid, cv=5, scoring='accuracy')\n\ngrid_search.fit(X_train, y_train)\n\ngrid_search.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"neigh = KNeighborsClassifier(n_neighbors = 5, weights = 'uniform')\nneigh.fit(X_train, y_train)\nscores = cross_val_score(neigh, X_train, y_train, scoring=\"accuracy\", cv=10)\nprint(scores.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"0.82, not bad at all for a KNeighbors model on such complicated dataset. \n\nJust below I apply the same technique for the rest of the models, nothing much to say. Funny enough to me, it's the SVC that yields the best results (even better than the ensembling in fact, I should improve on this but it's fine enough for me for a start, I'm happy I practiced the ensembling).\n\nI put the GridSearchCV parameters under commentaries, you can delete those or applied them, but some take an awful amount of time, be warned.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\"\"\"\n# Logistic Regression\n\nparam_grid = [\n    {'penalty':['l1','l2','elacticnet','none'], 'C':[0.001, 0.01, 0.1,1, 10, 100, 1000], 'n_jobs':[-1] }\n]\n\nlog_reg = LogisticRegression()\n\ngrid_search = GridSearchCV(log_reg, param_grid, cv=5, scoring='accuracy')\n\ngrid_search.fit(X_train, y_train)\n\ngrid_search.best_params_\n\"\"\"\nlog_reg = LogisticRegression(C = 0.001, penalty='none')\nlog_reg.fit(X_train, y_train)\nscores = cross_val_score(log_reg, X_train, y_train, scoring=\"accuracy\", cv=10)\nprint(\"Logistic Regression mean score : \", scores.mean())\n\n\"\"\"\n# Random Forest\n\nparam_grid = [\n    {'n_estimators':[100,200,300,500,700,1000], 'max_depth':[2,3,4,5,6,7,8],\n     'min_samples_split':[1,2,3,4,5], 'min_samples_leaf':[1,2,3,4] ,'n_jobs':[-1] }\n]\n\nforest_clf = RandomForestClassifier()\n\ngrid_search = GridSearchCV(forest_clf, param_grid, cv=5, scoring='accuracy')\n\ngrid_search.fit(X_train, y_train)\n\ngrid_search.best_params_\n\"\"\"\n\nforest_clf = RandomForestClassifier(max_depth = 6, min_samples_leaf=1, min_samples_split=2, n_estimators=100)\nforest_clf.fit(X_train, y_train)\nscores = cross_val_score(forest_clf, X_train, y_train, scoring=\"accuracy\", cv=10)\nprint(\"Random Forest mean score : \", scores.mean())\n\n\"\"\"\n# SVC\n\nparam_grid = [\n    {'kernel':['linear','poly','rbf','sigmoid'], 'C':[0.001, 0.01, 0.1,1, 10, 100, 1000], 'degree':[2,3,4], 'gamma':['auto']}\n]\n\nsvc = SVC()\n\ngrid_search = GridSearchCV(svc, param_grid, cv=5, scoring='accuracy')\n\ngrid_search.fit(X_train, y_train)\n\ngrid_search.best_params_\n\"\"\"\n\n# Despite this 0.847 score, real submission is 0.794, not good enough\nsvc = SVC(C = 10, kernel='rbf', gamma='auto', probability=True)\nsvc.fit(X_train, y_train)\nscores = cross_val_score(svc, X_train, y_train, scoring=\"accuracy\", cv=10)\nprint(\"SVC mean score: \", scores.mean())\n\n# GradientBoostingClassifier\ngbc = GradientBoostingClassifier(n_estimators=500, max_depth=3, learning_rate=0.01)\ngbc.fit(X_train, y_train)\nscores = cross_val_score(gbc, X_train, y_train, scoring=\"accuracy\", cv=10)\nprint(\"Gradient Boosting mean score : \", scores.mean())\n\n# XGB\nxgb_clf = xgb.XGBClassifier(n_estimators=500, max_depth=3, learning_rate=0.01)\nxgb_clf.fit(X_train, y_train)\nscores = cross_val_score(xgb_clf, X_train, y_train, scoring=\"accuracy\", cv=10)\nprint(\"XGB mean score : \", scores.mean())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Voting ensembling**\n\nI discovered this method really recently and I found it to be really cool, so I gave it a go. I chose a 'hard' voting system purely arbitrarely, you can have almost the same results with a 'soft'","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Voting ensembling\n\nvoting_clf = VotingClassifier(estimators=[('gbc', gbc), ('xgb', xgb_clf), ('forest', forest_clf), ('svc', svc), ('lrc', log_reg), ('knc', neigh)],\n                             voting='hard')\n\nvoting_clf.fit(X_train, y_train)\n\nscores = cross_val_score(voting_clf, X_train, y_train, scoring=\"accuracy\", cv=10)\n\nprint(scores.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"0.838, good enough for me, I'll wrap this up and make the predictions out of it !","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = voting_clf.predict(X_test)\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': preds})\noutput.to_csv(\"submission.csv\", index=False)\noutput.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tadaaaa!\n\nUnfortunately the 0.80 will not be broken with this notebook (you should have something like 0.795). \n\nBut here's a lesson of my journey : I got over 0.79 in a single try ! It took me something like 50 submissions to get a 0.80 few month ago, only to painfully realize what Overfitting really meant. Confidently, I can say this notebook will give me around 0.79/0.80 for EVERY changes in the test set, and for me that's awesome. I started ML few months ago and those simple exercices (and especially coming back at them with more practice) can really show you how much progress we made.\n\nThere is plenty other things to improve on this notebook, and plenty other things to try in order to reach a great score. But, for me, that will be enough for now. I sincerely hope you may have learn something from my mistakes. don't hesitate to leave a comment if you need to know anything about how I chose to code my notebook, always happy to talk with the Kaggle community.\n\nGood day to all of you !","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}