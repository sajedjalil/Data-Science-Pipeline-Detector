{"cells":[{"metadata":{"_uuid":"c2b778944fbb524c9722ec085985c5e142e1ecca"},"cell_type":"markdown","source":"# A First Crack: Tools, Tips & 3 Cipher Solutions\n\nTo get started on this Kaggle competition, we will need some tools for cracking simple ciphers. This kernel explores the data, creates some helpful functionality for all ciphers, and applies them to cracking the first three ciphers.\n0. Loading the Source Data (Important!)\n1. Utility Functions\n2. Characteristics of Plaintext Data\n    * Length of the newsgroup documents (before being split into 300 char strings)\n    * Distribution of ASCII characters in plaintext\n    * Dictionary of most common words in the plaintext source\n3. Cracking Cipher#1\n    * Matching the observed character frequencies\n    * Optimizing the decryption with word frequency similarity (or ngrams)\n    * Finding actual plaintext in the source and fine-tuning\n4. Cracking Cipher #2\n5. Cracking Cipher #3\n5. Make Predictions\n\nIn working on this, many thanks is due to:\n - [Practical Cryptography](http://practicalcryptography.com/ciphers/classical-era/) for a summary of classic ciphers\n - [The scikit-learn tutorial for text data](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html) using the same \"20 Newgroups\" dataset\n\n** Hope you found this public kernel to be helpful! **"},{"metadata":{"_uuid":"104c877cc1d1d5f531a4464ed061b9d67a11a81b"},"cell_type":"markdown","source":"### Preliminary Setup\nIf not being run on a Kaggle kernel, you need to [create an API Token](https://github.com/Kaggle/kaggle-api#api-credentials), upload it, and make sure it is in the directory (usually ~/.kaggle/kaggle.json)"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true,"_uuid":"f04a2ddb5b4db10a6b5b241375f17d30c896beb2"},"cell_type":"code","source":"# if using from the AISE.ai machine image on Google Cloud Platform,\n# upload the kaggle.json file to /jet/prs/workspace, and then from SSH:\n# cd /home/jet\n# mkdir .kaggle\n# cd /jet/prs/workspace\n# cp kaggle.json /home/jet/.kaggle\n#  chmod 600 /home/jet/.kaggle/kaggle.json\nnewdownloads = False\nif newdownloads:\n    # install missing packages\n    !pip install matplotlib\n    !pip install scikit-learn\n    !pip install fuzzywuzzy\n    !pip install python-Levenshtein\n    !pip install tqdm\n    !pip install kaggle\n    !pip install \"dask[complete]\"\n    !pip install graphviz\n    \n    # download the kaggle data\n    import os\n    import zipfile\n    DATA_DIR = '/jet/prs/workspace/data'\n    os.makedirs(DATA_DIR)\n    os.chdir(DATA_DIR)\n    !kaggle competitions download -c 20-newsgroups-ciphertext-challenge\n    #unzip\n    trainzip = zipfile.ZipFile('train.csv.zip')\n    trainzip.extractall(path=DATA_DIR)\n    testzip = zipfile.ZipFile('test.csv.zip')\n    testzip.extractall(path=DATA_DIR)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"77bc570695295fa267a655f5e4980277287e5d72"},"cell_type":"markdown","source":"### Load Python packages\nWe will import the usual `numpy`/`pandas`/`matplotlib`. Also `CountVectorizer` from `sklearn` for dealing with text analysis, `fuzzywuzzy` and `Levenshtein` for string matching, `itertools` to help with lists, and `dask` for  parallel processing."},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import os\n\nimport numpy as np\nnp.random.seed(seed=42) # fix seed for reproduceability\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom scipy import stats\n\nfrom sklearn.feature_extraction.text import CountVectorizer\ntoken_pattern = r\"[a-zA-Z0-9!@#$&()<>'=\\-]+\" # CountVectorizer word token\n\nfrom fuzzywuzzy import fuzz, process\nimport Levenshtein as leven\n\nfrom itertools import compress\nfrom tqdm import tqdm\n\n# distributed computing\nfrom dask import delayed, compute\nfrom dask.diagnostics import ProgressBar\nProgressBar().register()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9deeb108c0efc922c0f7364e22e21b397cb7744e"},"cell_type":"markdown","source":"### Load Kaggle data"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true,"_uuid":"92a00c0e898096febfcf769509af02e7cf1d3e98"},"cell_type":"code","source":"# Kaggle test and training data\nonkaggle = True\nif onkaggle:\n    train_datafile = '../input/train.csv'\n    test_datafile = '../input/test.csv'\nelse:\n    train_datafile = '/jet/prs/workspace/data/train.csv'\n    test_datafile = '/jet/prs/workspace/data/test.csv'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d5de26744f65f4c133323d877191ad9f19bd8a80","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(train_datafile)\ntest_df =  pd.read_csv(test_datafile)\ntest_df['length'] = np.array([len(test_df['ciphertext'].iloc[idx]) for idx in range(len(test_df))],dtype=int)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2cbd78585149e3482748586ccc1ae104715cedb4","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(\"Train data\")\ntrain_df.tail(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dd120b1f2c8f15125b06ba0709893b4a728553b5","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(\"Test data\")\ntest_df.tail(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6f4208935e4ff3af0a229bb01ce029ebcdef749f"},"cell_type":"markdown","source":"# 0. Loading the Source Data (Important!)\nIt will be helpful to reference the plaintext data from the **20 Newsgroups** data set, which is conveniently one of the `sklearn.datasets`. We will download this using `sklearn`, which brings some conveniences relative to the primary source on Jason Rennie's homepage ( [http://qwone.com/~jason/20Newsgroups/](http://qwone.com/~jason/20Newsgroups/) ).\n\nNote that the `fetch_20newsgroups` function used here returns a scikit-learn \"bunch\" object (see details [here](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html) ). \n\nIt will be extremely helpful to break this up into 300 character chunks **in exactly the same way as the Kaggle organizer. ** To do this, the key piece of code modifies the line breaks and trailing white space:\n\n`textstring.replace('\\r\\n','\\n').replace('\\r','\\n').replace('\\n','\\n ').rstrip(' ')`"},{"metadata":{"_kg_hide-input":true,"_uuid":"c2bde2da862e902445f93123a258354e1cbc936f","trusted":true},"cell_type":"code","source":"%%time\nfrom sklearn.datasets import fetch_20newsgroups\ntwenty_databunch = fetch_20newsgroups(subset='all', download_if_missing=True)\n\n# CORRECTION TO MATCH KAGGLE DATA\ndef sourcetransform(textstring):\n    return textstring.replace('\\r\\n','\\n').replace('\\r','\\n').replace('\\n','\\n ').rstrip(' ')\n\nsourcetext = twenty_databunch.data\nfor i,textstring in enumerate(sourcetext):\n    sourcetext[i] = sourcetransform(textstring)\n    \ntwenty_databunch.data = sourcetext\n\ntwenty_datalengths = [len(datastring) for datastring in twenty_databunch.data]\n\ncategory_names = twenty_databunch.target_names\n\n\nchunks_plaintext = []\nchunks_target = []\nchunks_length = []\nfor i in range(len(twenty_databunch.target)):\n    strlength = len(twenty_databunch.data[i])\n    if strlength > 300:\n        for j in range(strlength // 300):\n            chunks_plaintext.append(twenty_databunch.data[i][300*j:300*(j+1)])\n            chunks_target.append(twenty_databunch.target[i])\n            chunks_length.append(300)\n        if strlength%300 > 0:\n            chunks_plaintext.append(twenty_databunch.data[i][300*(strlength // 300):(300*(strlength // 300)+strlength%300)])\n            chunks_target.append(twenty_databunch.target[i])\n            chunks_length.append(strlength%300)\n    else:\n        chunks_plaintext.append(twenty_databunch.data[i])\n        chunks_target.append(twenty_databunch.target[i])\n        chunks_length.append(strlength)\n        \nchunk_df = pd.DataFrame({'plaintext':chunks_plaintext,\n                         'length':np.array(chunks_length,dtype=int), \n                         'target':np.array(chunks_target,dtype=int)})\nchunk_df['testref'] = np.nan\nchunk_df['trainref'] = np.nan\n\n\n# is is very helpful to have a copy of the data in dictionary form for fuzzy wuzzy lookup\nsourcetext_dict = {idx: el for idx, el in enumerate(chunks_plaintext)}\nchunk_df.tail()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"},"cell_type":"markdown","source":"# 1. Utility Functions\n\nThe functions in the code section below may be helpful:\n* translating between strings of text and 8-bit ascii representations (`string2ascii` and `ascii2string`)\n* calculating vectors where each element is the frequency of character per million characters of text ( `char_per_million`)\n* decipher function for a substitution cipher given a mapping between ascii characters (`decipher_subst`)\n* trimming a longer text to match a shorter one with the same fuzzy similarity (`trim_matchedsource`)\n* matrix of ascii character replacement operations needed for Levenshtein distance (`leven_replace_matrix`)\n\n\nAnd for later use:\n* calculating vectors where each element is the frequency of word/ngram per million characters of text (`word_per_million` and `ngram_per_million`). We can use these calculate similarity to properties of a source language.text (`similarity_score`)"},{"metadata":{"_uuid":"c4eecd98a4fbddcb47dc16e60aba25078a7d3b45","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# translating a string of text to an array of 8-bit integers representing ASCII values\ndef string2ascii(textstring):\n    return np.array([ord(char) for char in textstring], dtype=np.int8)\n\n# translating an array of 8-bit integers representing ASCII values to a string\ndef ascii2string(nparray):\n    return ''.join(chr(npint) for npint in nparray)\n\n# calculate ascii character frequency per million characters\ndef char_per_million(stringsarray):\n    asciicount = np.zeros((128,), dtype=int)\n    for k,textdata in enumerate(stringsarray):\n        asciicount += np.histogram(string2ascii(textdata),np.arange(129))[0]\n    totalchars = np.sum(asciicount)\n    return np.multiply(asciicount,np.divide(1000000.0,totalchars))\n\n# substitution decipher using the input asciimap dataframe (index= ciphered ascii integer)\ndef decipher_subst(textstring,asciimap):\n    inarray = string2ascii(textstring)\n    outarray = np.zeros(len(inarray),dtype=np.int8)\n    for asciival in asciimap.index:\n        outarray[inarray==asciival] = asciimap['decipher'].loc[asciival]\n    return ascii2string(outarray)\n\n# For fine-tuning a cipher, this identifies letter character replacements needed\n# The output is a 128x128 matrix with rows and columns corresponding to ascii characters \n# The element in element [a,b] is the number of times the ascii character in texta had to \n# be switched for the ascii character in textb\ndef leven_replace_matrix(textA,textB):\n    # initialize output matrix\n    asciiswitchmat = np.zeros([128,128],dtype=int)\n    # calculate edits for Levenshtein distance\n    lops = leven.editops(textA,textB)\n    # count the character replacements needed\n    replacetf = [editop[0] is 'replace' for editop in lops]\n    replacerefA =  np.array([editop[1] for editop in lops],dtype=np.int8)\n    replacerefB =  np.array([editop[2] for editop in lops],dtype=np.int8)\n    goodidx = np.logical_and(replacetf,np.logical_and(replacerefA<128,replacerefB<128))\n    replacerefA = replacerefA[replacetf]\n    replacerefB = replacerefB[replacetf]\n    for i, refA in enumerate(replacerefA):\n        asciiswitchmat[ord(textA[refA]),ord(textB[replacerefB[i]])] += 1\n    return asciiswitchmat","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"71d018dd267b3f613e0c94e67b8b960cd7c92236","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def char_count(textstring):\n    return np.histogram(string2ascii(textstring),np.arange(129))[0]\n    \n# calculate ascii character frequency per million characters\ndef char_per_300(stringsarray):\n    asciicount = np.zeros((128,), dtype=int)\n    for k,textdata in enumerate(stringsarray):\n        asciicount += char_count(textdata)\n    totalchars = np.sum(asciicount)\n    return np.multiply(asciicount,np.divide(300.0,totalchars))\n\n# number of words\ndef num_recognized_words(stringsarray,worddictionary):\n    vectorizer = CountVectorizer(analyzer='word',vocabulary=worddictionary2)\n    X = vectorizer.fit_transform(stringsarray)\n    return np.sum(X)\n\n# calculate words per million characters using a given dictionary list of ngrams\n# the worddictionary is generated on a previous run of CountVectorizer using English text\ndef word_per_million(stringsarray,worddictionary):\n    totalchars = sum([len(cipherstring) for cipherstring in stringsarray])\n    vectorizer = CountVectorizer(analyzer='word',token_pattern=token_pattern,vocabulary=worddictionary)\n    #vectorizer = CountVectorizer(vocabulary=worddictionary)\n    X = vectorizer.fit_transform(stringsarray)\n    return np.squeeze(np.asarray(np.sum(X,axis=0)*(1000000.0/totalchars)))\n\n# calculate ngrams per million characters using a given dictionary list of ngrams\n# the ngramdictionary is generated on a previous run of CountVectorizer using English text\ndef ngram_per_million(stringsarray,ngramdictionary):\n    ngramlength = len(next(iter(ngramdictionary)))\n    totalchars = sum([len(cipherstring) for cipherstring in stringsarray])\n    vectorizer = CountVectorizer(analyzer='char', \n                             max_features=1000,\n                             lowercase = False,\n                             ngram_range = (n_ngram,n_ngram),\n                             vocabulary=ngramdictionary)\n    X = vectorizer.fit_transform(stringsarray)\n    return np.squeeze(np.asarray(np.sum(X,axis=0)*(1000000.0/totalchars)))\n\ndef similarity_score(x, y):\n    return 1.0 - np.divide(np.linalg.norm(x - y),np.linalg.norm(y))\n\n# goodnes of fit\ndef goodness_of_fit(cipher_array,asciimap,verbose=True):\n    decipher_array = [decipher_subst(ciphertext,asciimap) \n     for ciphertext in cipher_array]\n    w_score = num_recognized_words(decipher_array, worddictionary)\n    wpm_candidate = word_per_million( decipher_array, worddictionary)\n    npm_candidate = ngram_per_million( decipher_array, ngramdictionary)\n    wpm_score = similarity_score(wpm_candidate, wpm_plaintext)\n    npm_score = similarity_score(npm_candidate, npm_plaintext)\n    if verbose:\n        print('   Goodness-of-Fit vs Source Text   ')\n        print('====================================')\n        print(\"Number of Recognized Words = \" + str(w_score))\n        print(\"Word-per-Million Similarity = \" + str(wpm_score))\n        print(\"Ngram-per-Million Similarity = \" + str(npm_score))\n    return w_score,wpm_score, npm_score","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dcb4e795eab099cd2ac5e72a4098c51c48da59d5"},"cell_type":"markdown","source":"# 2. Characteristics of Plaintext Data\nFirst, let's look at the actual, plaintext data to get a feel for what we should expect to see in the postings from the 20 newsgroups. Knowing the characteristics of the text will make it much easier to decipher.\n\nEach of the newsgroup documents looks something like this:"},{"metadata":{"_kg_hide-input":true,"_uuid":"2c00275909919a1ec501a076b0ef6d0cfdc628b1","trusted":true},"cell_type":"code","source":"# Display an example data item along with its associated category and filename\ndataitem = 0\nprint(twenty_databunch.data[dataitem])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"43498fc4a019dd9a6468501ffdbe3282bd5ce1dc"},"cell_type":"markdown","source":"There are 18,846 data items distributed across the 20 categories."},{"metadata":{"_kg_hide-input":true,"_uuid":"0be97785769bf6bb3fb06abfad4c1023e1df5d8c","trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(9, 12))\nax.barh(np.arange(len(category_names)),\n        [sum(twenty_databunch.target==k) for k in range(len(category_names))],\n        tick_label = category_names)\nax.tick_params(axis='both',labelsize=14)\nplt.title('# of Newsgroup Postings',fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7bc03f9988245d2034c96c763778b41818fd969a"},"cell_type":"markdown","source":"## Length of the newsgroup documents (after being split into 300 char strings)\nBeyond a general understanding of the dataset, the length of the documents can be helpful in decrypting the ciphers as you try to match plaintext to ciphertext--especially in trying to puzzle through the more complicated 3rd and 4th ciphers."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"19a5f6c48fa36c2eadbbbf6a8b8c66b46e0958d0"},"cell_type":"code","source":"print(str((100*sum(chunk_df['length']==300))//len(chunk_df))+'% of the data items have a size = 300')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"94abac28f4b3b507a5d1a06d1d60f50f3be1f294"},"cell_type":"code","source":"print(str(sum(chunk_df['length']<5))+' of the data items have a size < 5 characters!')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7298c5f42d492e595393e6bf9622848f31a86cd6"},"cell_type":"markdown","source":"## Distribution of ASCII characters in plaintext\nIn hacking the cipher, it will be helpful to see what the true distribution of ASCII characters is within the plaintext. The traditional letters will probably be fairly similar to the general distribution of alphabet letters in written English documents, but the newsgroup format of these postings could change the frequency with which we see capitalization and special characters."},{"metadata":{"_kg_hide-input":false,"_uuid":"407cf718c2fbe852cc84d33ffebe955db6a4f69d","trusted":true},"cell_type":"code","source":"%%time\ncpm_plaintext = char_per_million(chunks_plaintext)\nasciitop = np.argsort(-1*cpm_plaintext)\nasciitop_df = pd.DataFrame({'char':[chr(asciinum) for asciinum in asciitop],'char_per_million':cpm_plaintext[asciitop]},index=asciitop)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"8410105ac9a349bf38d1e2cf471d0faa348268f7","trusted":true},"cell_type":"code","source":"# create a bar chart to highlight the top 40 most common characters\nfig, ax = plt.subplots(figsize=(15, 5))\nax.plot(np.arange(30),asciitop_df['char_per_million'].iloc[0:30],color='red', marker='*', markersize=10)\nax.set_xticklabels([\"'\"+chr(asciinum)+\"'\" for asciinum in asciitop_df.index[0:30]])\nax.set_xticks(np.arange(30))\nax.tick_params(axis='both',labelsize=16)\nplt.ylabel('Occurrence per Million Characters',fontsize=16)\nplt.title('Top 30 ASCII Characters: Plaintext Source',fontsize=18)\nplt.show()\nprint(\"Of the 128 possible ASCII values, only \"+ str(sum(cpm_plaintext>1)) +\" are used more than once per million\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f0b0a7869f8bacf76c28eab3eca806f89664e979"},"cell_type":"markdown","source":"## Dictionaries of most common words and ngrams\n\nNote that we are not only using the default definion of a word (at least two alphanumeric characters) but are also considering an alternative dictionary allowing for one-letter words and special characters. This is done when loading the Python packages, defining the alternative CountVector word token: \n\n`token_pattern = r\"[a-zA-Z0-9!@#$&()<>'=\\-]+\" `"},{"metadata":{"_kg_hide-input":false,"_uuid":"7edd5cf384bfa882153c87cf76747ba6296d3db6","trusted":true},"cell_type":"code","source":"# the 5000 most common traditional words\ntotalchars = sum([len(cipherstring) for cipherstring in twenty_databunch.data])\nvectorizer = CountVectorizer(analyzer='word', max_features=5000)\nsource_words = vectorizer.fit_transform(twenty_databunch.data)\nworddictionary2 = vectorizer.vocabulary_\n\n# the 5000 most common words, including one-letter words and symbols\ntotalchars = sum([len(cipherstring) for cipherstring in twenty_databunch.data])\nvectorizer = CountVectorizer(analyzer='word', max_features=5000, token_pattern=token_pattern)\nsource_words = vectorizer.fit_transform(twenty_databunch.data)\nworddictionary = vectorizer.vocabulary_\n# the word-per-million score from plaintext English that we try to match in deciphering\nwpm_plaintext = word_per_million(twenty_databunch.data,worddictionary)\n\n# the 1000 most common English ngrams\nn_ngram = 3\ntotalchars = sum([len(cipherstring) for cipherstring in twenty_databunch.data])\nvectorizer = CountVectorizer(analyzer='char', \n                             max_features=1000,\n                             lowercase = False,\n                             ngram_range = (n_ngram,n_ngram) )\nsource_words = vectorizer.fit_transform(twenty_databunch.data)\nngramdictionary = vectorizer.vocabulary_\n# the word-per-million score from plaintext English that we try to match in deciphering\nnpm_plaintext = ngram_per_million(twenty_databunch.data,ngramdictionary)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"a7ed2440cc8e790848e61fb91ae900ae75180f1c","trusted":true},"cell_type":"code","source":"# this dictionary dataframe will show the common words\nworddict_df = pd.DataFrame.from_dict(worddictionary, orient='index')\nworddict_df.reset_index(inplace=True)\nworddict_df.rename(index=str, columns={\"index\": \"word\", 0: \"dict_index\"},inplace=True)\nworddict_df.set_index('dict_index',inplace=True)\nworddict_df.sort_index(inplace=True)\nworddict_df['words_per_million'] = wpm_plaintext\nworddict_df.sort_values('words_per_million',ascending=False,inplace=True)\nprint(\"Top 10 Most Common Words/Symbols\")\nprint(worddict_df.head(10))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e187418d502fb0dbe9b0905d5f361057aa78b8ba"},"cell_type":"markdown","source":"# 3. Cracking Cipher#1\n\nLet's get started on cracking the cipher for `difficulty=1`.\n\n## Observed character frequencies\n\nMany classic ciphers can be grouped into one of two categories; (1) substitution, where each character is mapped to another, and (2) transposition., where the ordering of the characters is switched.\n\nWe can use the observed frequency of characters to identify them. In a transposition ciphertext, the characters show the same frequency of occurrence as normal English text. In contrast, a substitution cipher will produce ciphertext where each character has a different frequency than it would in English. But importantly, the probability distribution should look about the same--just with different characters.\n\nIf the shape of the probability distribution changes, the cipher is more complicated. This could likely fall in the category of a \"polyalphabetic substitution cipher\", where the substitutions mapping changes from one letter to the next. \n\nSo let's look at the distribution of observed characters per million."},{"metadata":{"_uuid":"3e015b77e4dadf8b3347cbe028b8f7110f60715b","trusted":true},"cell_type":"code","source":"# we combine test and training data for greatest precision\ncpm_train = char_per_million(train_df[train_df['difficulty']==1]['ciphertext'])\ncpm_test =  char_per_million(test_df[test_df['difficulty']==1]['ciphertext'])\ncpm_cipher = 0.67*cpm_test + 0.33*cpm_train\n\nc1asciitop = np.argsort(-1*cpm_cipher)\nc1asciitop_df = pd.DataFrame({'char':[chr(asciinum) for asciinum in c1asciitop],\n                            'char_per_million':cpm_cipher[c1asciitop]},index=c1asciitop)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"29725254db9b5c0fa71891dc123c4dad80ce77a0","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# create a bar chart to highlight the top 40 most common characters\nfig, ax = plt.subplots(figsize=(15, 5))\nax.bar(np.arange(30),\n       c1asciitop_df['char_per_million'].iloc[0:30],\n       tick_label = [\"'\"+chr(asciinum)+\"'\" for asciinum in c1asciitop_df.index[0:30]] )\nax.plot(np.arange(30),asciitop_df['char_per_million'].iloc[0:30],color='red', marker='*', markersize=10)\nax.tick_params(axis='both',labelsize=16)\nplt.ylabel('Occurrence per Million Characters',fontsize=16)\nplt.title('Top 30 ASCII Characters: Ciphertext Difficulty=1',fontsize=18)\nplt.legend(['Distribution for Top Characters in Source Data','Distribution for Difficulty=1'],fontsize=16)\nplt.show()\nprint(\"Of the 128 possible ASCII values, only \"+ str(sum(cpm_cipher>1)) +\" are used more than once per million\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e529c13fbd00ba84ea32f64d19361894fd7cf80d"},"cell_type":"markdown","source":"This is what we expect to see from a substitution cipher. Note the red line showing the shape of the distribution for the top characters in the source data: different letters but almost exactly the same distribution!\n \nLet's start deciphering by trying out a mapping between the characters of the plaintext and the ciphertext with the most similar frequency of occurrence. To do this, we will create an `asciimap` dataframe that connects each ciphertext characer in the index to a plaintext character in the `decipher` column. Then, if we just line up the frequency of letters ocurring in the plaintext English with the ciphertext, we can generate a first guess on a potential substitution mapping. Let's see how legible this is for the sample training data at `iloc[0]`:"},{"metadata":{"_uuid":"a481fb06c014d6c9fc404270b702024ff89b08df","trusted":true},"cell_type":"code","source":"asciimap = pd.DataFrame(asciitop, index=c1asciitop,columns=['decipher'])","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"ff23b31051abfb9b2b9b2b62b77fc83b13ff30f5","trusted":true},"cell_type":"code","source":"textstring = train_df[train_df['difficulty']==1]['ciphertext'].iloc[0]\nprint('SAMPLE DECIPHER ATTEMPT:')\nprint(decipher_subst(textstring,asciimap))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a25ffadd18d37c829f1a171a769e271c7c7386b5"},"cell_type":"markdown","source":"Not bad! At this point, the text is already legible enough to understand. We could finish deciphering manually by making a few corrections to the asciimap (i.e. we could turn the  >'s sideways to makes v's!). \n\n... but manual deciphering is no fun!  And automating and optimizing may create tools that we will need for deciphering the later ciphers."},{"metadata":{"_uuid":"31b8ab3a912d10584e90a0147abf03e012f26783"},"cell_type":"markdown","source":"## Finding actual plaintext in the source and fine-tuning\nThe current `asciimap` substitution deciphering--while not perfect--is close enough to easily find the plaintext source and perfect the final substitutions in the cipher. Note that it takes about 25 seconds for `fuzzy wuzy` to find a match on a typical CPU."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"221d1124d1e8cf28463ad215242810936bf87f51"},"cell_type":"code","source":"%%time\n# Go through the plain text from the category and find the matching entry\ntextstring = decipher_subst(train_df[train_df['difficulty']==1]['ciphertext'].iloc[0],asciimap)\ntextcategory = train_df[train_df['difficulty']==1]['target'].iloc[0]\nsourcematch = process.extractOne(textstring, sourcetext_dict, scorer = fuzz.ratio)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"a194a180a1f47039760b48092a2ca9d863e0dfd6"},"cell_type":"code","source":"print('Found a match in source data with \"fuzz.ratio\" score of ' + str(fuzz.ratio(textstring,sourcematch[0])))\nprint('Source data found in category = ' + category_names[chunk_df['target'].iloc[sourcematch[2]]])\nprint('Matching text below:')\nprint('----------------')\nprint(sourcematch[0])\nprint(' ')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"19a9efa3581c6d054eec9f2d6db0f2f55b0dfa08"},"cell_type":"markdown","source":" While this level of accuracy is probably fine for identifying most of the `difficulty=1` ciphers, we may want to perfect the decryption. Getting the first cipher completely correct could help with the others.\n \n To fine-tune the mapping, let's take the identified matches and figure out which characters are not mapped correctly. We need a fairly large subset to find the rare characters, like \"K\", but going through so many example ciphers to find the rare characters eats significant time! For this step, it is helpful to use parallel processing (done here via `dask`)."},{"metadata":{"trusted":true,"_uuid":"e198eaaf007d4eacc10657b48facb9ded8f0aa89"},"cell_type":"code","source":"# subsample of ciphertext\nsubset_size = 2000\ntune_df = train_df[np.logical_and(train_df['difficulty']==1,\n                                    np.array([len(train_df['ciphertext'].iloc[idx]) for idx in range(len(train_df))])>100)]\ntuneindices = tune_df.index[:subset_size]\n        \n# function to match to source data for same category and length >100 chars\ndef finetunematch(idx):\n    textstring = decipher_subst(tune_df['ciphertext'].loc[idx],asciimap)\n    selectindices = np.logical_and(chunk_df['target']==tune_df['target'].loc[idx],chunk_df['length']>250)\n    sourcematch = process.extractOne(textstring, \n                                     list(compress(chunks_plaintext, selectindices)), \n                                     scorer = fuzz.ratio)\n    return sourcematch[0], textstring, sourcematch[1]\n\n# parallel evaluation using Dask (big benefits for more CPUs)\npar_compute = [delayed(finetunematch)(idx) for idx in tuneindices]\noutput_arrays = compute(*par_compute, scheduler='processes')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e5699b3ea1acaeb856f23bdc8b0c520ff5e552b1"},"cell_type":"markdown","source":"To help ensure the matches are high quality, let's only use matches where the text similarity, as measured by the `fuzz.ratio`, is higher than 80."},{"metadata":{"trusted":true,"_uuid":"5d9c4a3f7a966845a8a0726f80e8df290854836f","_kg_hide-input":true},"cell_type":"code","source":"# select those with relatively high fuzzy.ratio scores\nminfuzzyscore = 80\nsourcematch_array = [arrays[0] for arrays in output_arrays]\ndecipher_array = [arrays[1] for arrays in output_arrays]\nfuzzyscores_array = np.array([arrays[2] for arrays in output_arrays])\nsourcematch_array = list(compress(sourcematch_array, (fuzzyscores_array>=minfuzzyscore) ))\ndecipher_array = list(compress(decipher_array, (fuzzyscores_array>=minfuzzyscore) ))\ncipher_array =  tune_df[:subset_size]['ciphertext']\ncipher_array = list(compress(cipher_array, (fuzzyscores_array>=minfuzzyscore) ))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"03b1d6595ff37218a2e013c2cd5c990a12a09784"},"cell_type":"code","source":"print('In subsample, ' + str(np.sum(fuzzyscores_array>=minfuzzyscore)) + ' of the ' + str(subset_size) + ' have fuzzy.ratios > ' \n       + str(minfuzzyscore) + ' and will be used for fine-tuning')\nprint(' ')\nprint('Before fine-tuning the sub-sample:')\ngof = goodness_of_fit(cipher_array,asciimap=asciimap)\nprint('Average fuzzy.ratio = ' + str(np.mean(fuzzyscores_array[fuzzyscores_array>=minfuzzyscore])))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7c965ab08d1d4115a761cbbe4c109fb18476b01d"},"cell_type":"markdown","source":"Using those matches, some of the most common character replacements needed to match the proposed decipher with the source text (using Levenshtein distance edits) are:"},{"metadata":{"trusted":true,"_uuid":"154571606c303d8ded66ee1924498989f8603d71","_kg_hide-input":true},"cell_type":"code","source":"# initialize asciiswitchmat, a 128x128 matrix with rows numbers corresponding to ascii characters\n# that should be replaced by the ascii character of the column number\nasciiswitchmat = np.zeros([128,128],dtype=int)\nasciicount = np.zeros((128,), dtype=int)\n\nfor i, textstring in enumerate(decipher_array):\n    asciiswitchmat += leven_replace_matrix(textstring,sourcematch_array[i])\n    asciicount += np.histogram(string2ascii(textstring),np.arange(129))[0]\n\ndeciphererror_dict = {'ascii_replacement': [np.argmax(asciiswitchmat[i,...]) for i in range(128)],\n                      'char_decipher': [chr(i) for i in range(128)],\n                      'char_replacement': [chr(np.argmax(asciiswitchmat[i,...])) for i in range(128)],\n                      'numError': np.sum(asciiswitchmat,axis=1),\n                      'numReplace': [asciiswitchmat[i,np.argmax(asciiswitchmat[i,...])] for i in range(128)],\n                      'totalObs': asciicount}\n\ndeciphererror_df = pd.DataFrame.from_dict(deciphererror_dict)\ndeciphererror_df['pctError'] = deciphererror_df['numError'] / deciphererror_df['totalObs']\ndeciphererror_df['pctBestReplace'] = deciphererror_df['numReplace'] / deciphererror_df['numError']  \ndeciphererror_df.sort_values(by=['pctError'],ascending=False,inplace=True)\ndeciphererror_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0f7f38abed7b326b3f718f010a4aaca9d047ef55"},"cell_type":"code","source":"# correct the identified errors in character substitution\nerrorprone = np.logical_and(deciphererror_df['totalObs']>=1,\n                            np.logical_and(deciphererror_df['pctError']>=0.01*(100-minfuzzyscore),\n                                           deciphererror_df['pctBestReplace']>=0.5)),\ndecipherchars = deciphererror_df.index[errorprone]\nimprovementchars = deciphererror_df['ascii_replacement'].loc[errorprone].values\ntmpasciimap = asciimap.copy()\nfor i,decipherchar in enumerate(decipherchars):\n    asciimap['decipher'].loc[tmpasciimap['decipher'].values==decipherchar] = improvementchars[i]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f02168fb31d0e5c3b7a016b2b69e6d5d7d54fc18"},"cell_type":"markdown","source":"Let's run through some more to try to correctly map characters that are less frequently observed."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"d310b710190feec997fb17222439b74eb35f7feb"},"cell_type":"code","source":"# Repeating the previous code\n# subsample of ciphertext\nsubset2_size = 2000\ntuneindices = tune_df.index[subset_size:(subset2_size+subset_size)]\n\n# parallel evaluation using Dask (big benefits for more CPUs)\npar_compute2 = [delayed(finetunematch)(idx) for idx in tuneindices]\noutput_arrays = compute(*par_compute2, scheduler='processes')\nsourcematch_array = [arrays[0] for arrays in output_arrays]\ndecipher_array = [arrays[1] for arrays in output_arrays]\nfuzzyscores_array = np.array([arrays[2] for arrays in output_arrays])\nsourcematch_array = list(compress(sourcematch_array, (fuzzyscores_array>=minfuzzyscore) ))\ndecipher_array = list(compress(decipher_array, (fuzzyscores_array>=minfuzzyscore) ))\ncipher_array =  tune_df[subset_size:(subset_size+subset2_size)]['ciphertext']\ncipher_array = list(compress(cipher_array, (fuzzyscores_array>=minfuzzyscore) ))\n\nprint('In 2nd subsample, ' + str(np.sum(fuzzyscores_array>=minfuzzyscore)) + ' of the ' + str(subset2_size) + ' have fuzzy.ratios > ' \n       + str(minfuzzyscore) + ' and will be used for fine-tuning')\n\n# initialize asciiswitchmat, a 128x128 matrix with rows numbers corresponding to ascii characters\n# that should be replaced by the ascii character of the column number\nasciiswitchmat = np.zeros([128,128],dtype=int)\nasciicount = np.zeros((128,), dtype=int)\n\nfor i, textstring in enumerate(decipher_array):\n    asciiswitchmat += leven_replace_matrix(textstring,sourcematch_array[i])\n    asciicount += np.histogram(string2ascii(textstring),np.arange(129))[0]\n\ndeciphererror_dict = {'ascii_replacement': [np.argmax(asciiswitchmat[i,...]) for i in range(128)],\n                      'char_decipher': [chr(i) for i in range(128)],\n                      'char_replacement': [chr(np.argmax(asciiswitchmat[i,...])) for i in range(128)],\n                      'numError': np.sum(asciiswitchmat,axis=1),\n                      'numReplace': [asciiswitchmat[i,np.argmax(asciiswitchmat[i,...])] for i in range(128)],\n                      'totalObs': asciicount}\n\ndeciphererror_df = pd.DataFrame.from_dict(deciphererror_dict)\ndeciphererror_df['pctError'] = deciphererror_df['numError'] / deciphererror_df['totalObs']\ndeciphererror_df['pctBestReplace'] = deciphererror_df['numReplace'] / deciphererror_df['numError']\n\n# correct the identified errors in character substitution\nerrorprone = np.logical_and(deciphererror_df['totalObs']>=1,\n                            np.logical_and(deciphererror_df['pctError']>=0.1,\n                                           deciphererror_df['pctBestReplace']>=0.8)),\ndecipherchars = deciphererror_df.index[errorprone]\nimprovementchars = deciphererror_df['ascii_replacement'].loc[errorprone].values\ntmpasciimap = asciimap.copy()\nfor i,decipherchar in enumerate(decipherchars):\n    asciimap['decipher'].loc[tmpasciimap['decipher'].values==decipherchar] = improvementchars[i]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ed80b5f1b65ea860499dbc7ea6b4c614b048cec3"},"cell_type":"markdown","source":"Now we should have a fairly accurate solution. Let's check goodness-of-fit statistics as well as our decryption of the sample cipher at `iloc[0]`"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"bfd4a6c0b9cd679f6b5322a76e2bb9421a3a5863"},"cell_type":"code","source":"print('After fine-tuning the sub-sample:')\ngof = goodness_of_fit(cipher_array,asciimap=asciimap)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"b20f81518b9903bc3beb615b19d1da1cb15b3412"},"cell_type":"code","source":"textstring = train_df[train_df['difficulty']==1]['ciphertext'].iloc[0]\nprint('FINAL DECIPHER:')\nprint(decipher_subst(textstring,asciimap))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4a232085b908ef42efc046968228b47bae464213"},"cell_type":"markdown","source":"This looks great and exactly matches the source. Let's now decipher all the test data for difficulty=1."},{"metadata":{"trusted":true,"_uuid":"fc30ca96433b365495e83cfde52220a1d0bafc28"},"cell_type":"code","source":"%%time\ntest_df['plaintext'] = ['']*len(test_df)\n# fill in plaintext for test data\nc1indices = test_df[test_df['difficulty']==1].index\ntest_df.loc[c1indices,'plaintext'] = [decipher_subst(test_df['ciphertext'].loc[idx],asciimap) for idx in c1indices]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8596adf73128a0c3e81e3e7e67b1ddd47434f941"},"cell_type":"markdown","source":"# 4. Cracking Cipher #2\nWe can follow the exact same process in examining cipher #2. First let's look at the distribution of the ascii characters in the ciphertext."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"5f2c4982f734fbbd0452194c6d23ea72bc2f77d9"},"cell_type":"code","source":"%%time\ncpm_plaintext = char_per_million(chunks_plaintext)\nasciitop = np.argsort(-1*cpm_plaintext)\nasciitop_df = pd.DataFrame({'char':[chr(asciinum) for asciinum in asciitop],'char_per_million':cpm_plaintext[asciitop]},index=asciitop)\n\n# we combine test and training data for greatest precision\ncpm_train = char_per_million(train_df[train_df['difficulty']==2]['ciphertext'])\ncpm_test =  char_per_million(test_df[test_df['difficulty']==2]['ciphertext'])\ncpm_cipher = 0.67*cpm_test + 0.33*cpm_train\n\nc2asciitop = np.argsort(-1*cpm_cipher)\nc2asciitop_df = pd.DataFrame({'char':[chr(asciinum) for asciinum in c1asciitop],\n                            'char_per_million':cpm_cipher[c2asciitop]},index=c2asciitop)\n\n# create a bar chart to highlight the top 40 most common characters\nfig, ax = plt.subplots(figsize=(15, 5))\nax.bar(np.arange(30),\n       c2asciitop_df['char_per_million'].iloc[0:30],\n       tick_label = [\"'\"+chr(asciinum)+\"'\" for asciinum in c2asciitop_df.index[0:30]] )\nax.plot(np.arange(30),asciitop_df['char_per_million'].iloc[0:30],color='red', marker='*', markersize=10)\nax.tick_params(axis='both',labelsize=16)\nplt.ylabel('Occurrence per Million Characters',fontsize=16)\nplt.title('Top 30 ASCII Characters: Ciphertext Difficulty=2',fontsize=18)\nplt.show()\nprint(\"Of the 128 possible ASCII values, only \"+ str(sum(cpm_cipher>1)) +\" are used more than once per million\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b5491bf6a20769680b3f774944fb1d351aac5a7e"},"cell_type":"markdown","source":"Looks like another substititution cipher. And even though this Kaggle competition creates the ciphertext by encoding with cipher #1 and then with cipher #2 to create the difficulty=2 ciphers, we can simply solve for the composition mapping between the plaintext in the source and the ciphertext. \n\n(However, you might find some interesting insight into the exact deciphering by looking at the patterns in going from cipher 1 to cipher 2. This would be helpful in correctly deciphering rare characters)."},{"metadata":{"trusted":true,"_uuid":"b291ba67d30a19f42ade96564a23ca0c5e37a22a","_kg_hide-input":true},"cell_type":"code","source":"%%time\nasciimap2 = pd.DataFrame(asciitop, index=c2asciitop,columns=['decipher'])\n# Go through the plain text from the category and find the matching entry\ntextstring = decipher_subst(train_df[train_df['difficulty']==2]['ciphertext'].iloc[0],asciimap2)\ntextcategory = train_df[train_df['difficulty']==2]['target'].iloc[0]\nsourcematch = process.extractOne(textstring, sourcetext_dict, scorer = fuzz.ratio)\n\nprint('Found a match in source data with \"fuzz.ratio\" score of ' + str(fuzz.ratio(textstring,sourcematch[0])))\nprint('Source data found in category = ' + category_names[chunk_df['target'].iloc[sourcematch[2]]])\nprint('Original deciphering using character frequency:')\nprint('----------------')\nprint(textstring)\nprint(' ')\nprint('Matching text below:')\nprint('----------------')\nprint(sourcematch[0])\nprint(' ')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"36b5d12ef047ad190db65a1d1d7c9e0d9fbff9b9"},"cell_type":"markdown","source":"Again, just using the character frequency is not a bad place to start. We can then fine-tune by matching to the source text."},{"metadata":{"trusted":true,"_uuid":"bfc00ce904b674caaea37a7ece57082d93f75998","_kg_hide-input":true},"cell_type":"code","source":"# subsample of ciphertext\nsubset_size = 2000\ntune_df = train_df[np.logical_and(train_df['difficulty']==2,\n                                    np.array([len(train_df['ciphertext'].iloc[idx]) for idx in range(len(train_df))])>100)]\ntuneindices = tune_df.index[:subset_size]\n        \n# function to match to source data for same category and length greater than 100 characters\ndef finetunematch2(idx):\n    textstring = decipher_subst(tune_df['ciphertext'].loc[idx],asciimap2)\n    selectindices = np.logical_and(chunk_df['target']==tune_df['target'].loc[idx],chunk_df['length']==len(textstring))\n    sourcematch = process.extractOne(textstring, \n                                     list(compress(chunks_plaintext, selectindices)), \n                                     scorer = fuzz.ratio)\n    return sourcematch[0], textstring, sourcematch[1]\n\n# parallel evaluation using Dask (big benefits for more CPUs)\npar_compute = [delayed(finetunematch2)(idx) for idx in tuneindices]\noutput_arrays = compute(*par_compute, scheduler='processes')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"74b072a3bf9af721dcbfb830dc294c7845f0bfe5","_kg_hide-input":true},"cell_type":"code","source":"# select those with relatively high fuzzy.ratio scores\nminfuzzyscore = 80\nsourcematch_array = [arrays[0] for arrays in output_arrays]\ndecipher_array = [arrays[1] for arrays in output_arrays]\nfuzzyscores_array = np.array([arrays[2] for arrays in output_arrays])\nsourcematch_array = list(compress(sourcematch_array, (fuzzyscores_array>=minfuzzyscore) ))\ndecipher_array = list(compress(decipher_array, (fuzzyscores_array>=minfuzzyscore) ))\ncipher_array =  tune_df[:subset_size]['ciphertext']\ncipher_array = list(compress(cipher_array, (fuzzyscores_array>=minfuzzyscore) ))\nprint('In subsample, ' + str(np.sum(fuzzyscores_array>=minfuzzyscore)) + ' of the ' + str(subset_size) + ' have fuzzy.ratios > ' \n       + str(minfuzzyscore) + ' and will be used for fine-tuning')\nprint(' ')\nprint('Before fine-tuning the sub-sample:')\ngof = goodness_of_fit(cipher_array,asciimap=asciimap2)\nprint('Average fuzzy.ratio = ' + str(np.mean(fuzzyscores_array[fuzzyscores_array>=minfuzzyscore])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b23ca6773ae7583d8f38d78eac7a56bf3649f364","_kg_hide-input":true},"cell_type":"code","source":"# initialize asciiswitchmat, a 128x128 matrix with rows numbers corresponding to ascii characters\n# that should be replaced by the ascii character of the column number\nasciiswitchmat = np.zeros([128,128],dtype=int)\nasciicount = np.zeros((128,), dtype=int)\n\nfor i, textstring in enumerate(decipher_array):\n    asciiswitchmat += leven_replace_matrix(textstring,sourcematch_array[i])\n    asciicount += np.histogram(string2ascii(textstring),np.arange(129))[0]\n\ndeciphererror_dict2 = {'ascii_replacement': [np.argmax(asciiswitchmat[i,...]) for i in range(128)],\n                      'char_decipher': [chr(i) for i in range(128)],\n                      'char_replacement': [chr(np.argmax(asciiswitchmat[i,...])) for i in range(128)],\n                      'numError': np.sum(asciiswitchmat,axis=1),\n                      'numReplace': [asciiswitchmat[i,np.argmax(asciiswitchmat[i,...])] for i in range(128)],\n                      'totalObs': asciicount}\n\ndeciphererror_df2 = pd.DataFrame.from_dict(deciphererror_dict2)\ndeciphererror_df2['pctError'] = deciphererror_df2['numError'] / deciphererror_df2['totalObs']\ndeciphererror_df2['pctBestReplace'] = deciphererror_df2['numReplace'] / deciphererror_df2['numError']  \ndeciphererror_df2.sort_values(by=['numError'],ascending=False,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"923b752f5f20e7d0ae1b08419fcaf7532e84a5d3","_kg_hide-input":true},"cell_type":"code","source":"# correct the identified errors in character substitution\nerrorprone = np.logical_and(deciphererror_df2['totalObs']>=1,\n                            np.logical_and(deciphererror_df2['pctError']>=0.1,\n                                           deciphererror_df2['pctBestReplace']>=0.5)),\ndecipherchars = deciphererror_df2.index[errorprone]\nimprovementchars = deciphererror_df2['ascii_replacement'].loc[errorprone].values\ntmpasciimap = asciimap2.copy()\nfor i,decipherchar in enumerate(decipherchars):\n    asciimap2['decipher'].loc[tmpasciimap['decipher'].values==decipherchar] = improvementchars[i]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"314ce88dbec63aee94d9fc1540edd3f9d1ab3082"},"cell_type":"code","source":"subset2_size = 2000\ntuneindices = tune_df.index[subset_size:(subset2_size+subset_size)]\n\n# parallel evaluation using Dask (big benefits for more CPUs)\npar_compute2 = [delayed(finetunematch2)(idx) for idx in tuneindices]\noutput_arrays = compute(*par_compute2, scheduler='processes')\n\n\n# select those with relatively high fuzzy.ratio scores\nminfuzzyscore = 80\nsourcematch_array = [arrays[0] for arrays in output_arrays]\ndecipher_array = [arrays[1] for arrays in output_arrays]\nfuzzyscores_array = np.array([arrays[2] for arrays in output_arrays])\nsourcematch_array = list(compress(sourcematch_array, (fuzzyscores_array>=minfuzzyscore) ))\ndecipher_array = list(compress(decipher_array, (fuzzyscores_array>=minfuzzyscore) ))\ncipher_array =  tune_df[subset_size:(subset_size+subset2_size)]['ciphertext']\ncipher_array = list(compress(cipher_array, (fuzzyscores_array>=minfuzzyscore) ))\n\n\nprint('In 2nd subsample, ' + str(np.sum(fuzzyscores_array>=minfuzzyscore)) + ' of the ' + str(subset2_size) + ' have fuzzy.ratios > ' \n       + str(minfuzzyscore) + ' and will be used for fine-tuning')\n\n# initialize asciiswitchmat, a 128x128 matrix with rows numbers corresponding to ascii characters\n# that should be replaced by the ascii character of the column number\nasciiswitchmat = np.zeros([128,128],dtype=int)\nasciicount = np.zeros((128,), dtype=int)\n\nfor i, textstring in enumerate(decipher_array):\n    asciiswitchmat += leven_replace_matrix(textstring,sourcematch_array[i])\n    asciicount += np.histogram(string2ascii(textstring),np.arange(129))[0]\n\ndeciphererror_dict = {'ascii_replacement': [np.argmax(asciiswitchmat[i,...]) for i in range(128)],\n                      'char_decipher': [chr(i) for i in range(128)],\n                      'char_replacement': [chr(np.argmax(asciiswitchmat[i,...])) for i in range(128)],\n                      'numError': np.sum(asciiswitchmat,axis=1),\n                      'numReplace': [asciiswitchmat[i,np.argmax(asciiswitchmat[i,...])] for i in range(128)],\n                      'totalObs': asciicount}\n\ndeciphererror_df = pd.DataFrame.from_dict(deciphererror_dict)\ndeciphererror_df['pctError'] = deciphererror_df['numError'] / deciphererror_df['totalObs']\ndeciphererror_df['pctBestReplace'] = deciphererror_df['numReplace'] / deciphererror_df['numError']\n\n# correct the identified errors in character substitution\nerrorprone = np.logical_and(deciphererror_df['totalObs']>=2,\n                            np.logical_and(deciphererror_df['pctError']>=0.2,\n                                           deciphererror_df['pctBestReplace']>=0.66)),\ndecipherchars = deciphererror_df.index[errorprone]\nimprovementchars = deciphererror_df['ascii_replacement'].loc[errorprone].values\ntmpasciimap = asciimap2.copy()\nfor i,decipherchar in enumerate(decipherchars):\n    asciimap2['decipher'].loc[tmpasciimap['decipher'].values==decipherchar] = improvementchars[i]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7c171e064cb1c20f388d9a226f0e3e1292801025","_kg_hide-input":true},"cell_type":"code","source":"print('After fine-tuning the sub-sample:')\ngof = goodness_of_fit(cipher_array,asciimap=asciimap2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f875f4e8e4bec8955ded767dea498eb101393108","_kg_hide-input":true},"cell_type":"code","source":"textstring = train_df[train_df['difficulty']==2]['ciphertext'].iloc[0]\nprint('FINAL DECIPHER:')\nprint(decipher_subst(textstring,asciimap2))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"61b22d1e92747ce55c13295f29530e48d3b9149d"},"cell_type":"markdown","source":"And we can now decipher difficulty=2. "},{"metadata":{"trusted":true,"_uuid":"fd2e234ef82e74e80baf94182cf17e3f4ff38517","_kg_hide-input":true},"cell_type":"code","source":"%%time\nc2indices = test_df[test_df['difficulty']==2].index\ntest_df.loc[c2indices,'plaintext'] = [decipher_subst(test_df['ciphertext'].loc[idx],asciimap2) for idx in c2indices]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e6990937243de97874e2f9d3531456de690edefd"},"cell_type":"markdown","source":"# 5. Cracking Cipher #3\nAs before let's start by looking at the distribution of characters in ciphertext for difficulty =3. "},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"bcde2ba0bf00268a0b96a40686a7864c9e9a05c7"},"cell_type":"code","source":"# we combine test and training data for greatest precision\ncpm_train = char_per_million(train_df[train_df['difficulty']==3]['ciphertext'])\ncpm_test =  char_per_million(test_df[test_df['difficulty']==3]['ciphertext'])\ncpm_cipher = 0.67*cpm_test + 0.33*cpm_train\n\nc3asciitop = np.argsort(-1*cpm_cipher)\nc3asciitop_df = pd.DataFrame({'char':[chr(asciinum) for asciinum in c3asciitop],\n                            'char_per_million':cpm_cipher[c3asciitop]},index=c3asciitop)\n\n# create a bar chart to highlight the top 40 most common characters\nfig, ax = plt.subplots(figsize=(15, 5))\nax.bar(np.arange(30),\n       c3asciitop_df['char_per_million'].iloc[0:30],\n       tick_label = [\"'\"+chr(asciinum)+\"'\" for asciinum in c3asciitop_df.index[0:30]] )\nax.plot(np.arange(30),asciitop_df['char_per_million'].iloc[0:30],color='red', marker='*', markersize=10)\nax.tick_params(axis='both',labelsize=16)\nplt.ylabel('Occurrence per Million Characters',fontsize=16)\nplt.title('Top 30 ASCII Characters: Ciphertext Difficulty=3',fontsize=18)\nplt.legend(['Distribution for Top Characters in Source Data','Distribution for Difficulty=3'],fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"31cc4d02ec9afff17aec6be05c9cf7f8ed364592"},"cell_type":"markdown","source":"Of course it would be too easy if cipher number three were another substitution cipher. Looking at the distribution of characters, we can see that the probability distribution of characters does not match the frequencies we would observe in typical plaintext for most characters...though it seems that the space character matches. \n\nInteresting. It must be the case that plaintext characters are not always mapped to the same ciphertext characters. \n\nLet's see what it looks like if we try to decipher ciphertext from difficulty #3 using our mapping from cipher #2."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"87240799ab4f06b8e9945911911a252856290b2d"},"cell_type":"code","source":"# Go through the plain text from the category and find the matching entry\ntextstring = decipher_subst(train_df[train_df['difficulty']==3]['ciphertext'].iloc[0],asciimap2)\ntextcategory = train_df[train_df['difficulty']==3]['target'].iloc[0]\nsourcematch = process.extractOne(textstring, sourcetext_dict, scorer = fuzz.ratio)\n\nprint('Found a match in source data with \"fuzz.ratio\" score of ' + str(fuzz.ratio(textstring,sourcematch[0])))\nprint('Source data found in category = ' + category_names[chunk_df['target'].iloc[sourcematch[2]]])\nprint('Original deciphering of difficulty #3 ciphertext of using mapping from cipher #2:')\nprint('----------------')\nprint(textstring)\nprint(' ')\nprint('Matching text below:')\nprint('----------------')\nprint(sourcematch[0])\nprint(' ')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9e5547d52415fd4b9b9c9f5137e7267d1f39e081"},"cell_type":"markdown","source":"Strangely, we (and fuzzywuzzy) are able to just barely figure out the matching plaintext when using the cipher#2 mapping. But the unmatched plaintext characters don't consistently correspond to the same ciphertext characters. \n\nLet's reverse engineer which ciphertext characters are the ones that we can't seem to consistently match."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"98507602350de00522892cca2bdc58cfb25ac7c4"},"cell_type":"code","source":"# subsample of ciphertext\nsubset_size = 400\n\nreversemap2 = asciimap2.copy()\nreversemap2.drop_duplicates('decipher',inplace=True)\nreversemap2.reset_index(inplace=True)\nreversemap2.rename(index=str,columns={\"decipher\":\"index\",\"index\":\"decipher\"},inplace=True)\nreversemap2.set_index('index',inplace=True)\nreversemap2.head()\n\ntransform_ct = np.zeros((128,), dtype=int)\ntransform_out = np.zeros((128,), dtype=int)\nstable_ct = np.zeros((128,), dtype=int)\nfor idx in tqdm(range(subset_size)):\n    textstring = decipher_subst(train_df[train_df['difficulty']==3]['ciphertext'].iloc[idx],asciimap2)\n    textcategory = train_df[train_df['difficulty']==3]['target'].iloc[idx]\n    sourcematch = process.extractOne(textstring, \n                                     list(compress(chunks_plaintext,chunks_target==textcategory)), \n                                     scorer = fuzz.ratio)\n    s1 = string2ascii(decipher_subst(sourcematch[0],reversemap2))\n    s2 = string2ascii(train_df[train_df['difficulty']==3]['ciphertext'].iloc[idx])\n    if len(s1)==len(s2):\n        transform_ct += np.histogram(np.array(s1[(s1-s2)>0],dtype=int),np.arange(129))[0]\n        transform_out += np.histogram(np.array(s2[(s1-s2)>0],dtype=int),np.arange(129))[0]\n        stable_ct += np.histogram(np.array(s1[(s1-s2)==0],dtype=int),np.arange(129))[0]\n\ntransform_pct = np.divide(transform_ct, (transform_ct + stable_ct+1) )\n\nenoughobs = np.arange(128)[np.logical_and((transform_ct+stable_ct)>5,np.arange(128)>40)]\nfig, ax = plt.subplots(figsize=(20, 4))\nax.bar(np.arange(len(enoughobs)),\n        transform_pct[enoughobs],\n              tick_label = [\"'\"+chr(asciinum)+\"'\" for asciinum in enoughobs] )\nplt.title('Characters Transformed by Cipher Number 3',fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b8b5e94dd7391456df296c3864ee1f1bbf8ace18"},"cell_type":"markdown","source":"Ah ha! These seem to all be alphabetic characters (upper and lowercase) that are being modified. We'll define this set of alphabetic characters as `c3mod`."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"ae244c51439d485c71fa8876291afd94eb9fe295"},"cell_type":"code","source":"c3mod = set()\nmodascii = np.union1d(np.arange(65,91),np.arange(97,123))\nfor i in modascii:\n    c3mod.add(i)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"80bb87a89579d5a1e1d3da60192178a121e202b0"},"cell_type":"markdown","source":" It seems that cipher #3 is doing a transformation that only applies to these aphabetic characters, shifting them by varying amounts. Let's now try to find some pattern in how they are being shifted."},{"metadata":{"trusted":true,"_uuid":"054e94799f49ee5c1b0532a6807959e68519548b","_kg_hide-input":true},"cell_type":"code","source":"%%time\ntrain_lengths = np.array([len(cipherstring) for cipherstring in train_df['ciphertext']])\nc3indices = train_df.index[np.logical_and(train_df['difficulty']==3,train_lengths>290)]\n\nnumsamples = 50\nc3modmat = np.nan*np.zeros((numsamples,300))\ns2startmat = np.nan*np.zeros((numsamples,300)) \nfor i,idx in enumerate(c3indices[0:numsamples]):\n    textstring = decipher_subst(train_df['ciphertext'].loc[idx],asciimap2)\n    textcategory = train_df['target'].loc[idx]\n    selectindices = np.logical_and(chunk_df['target']==textcategory,chunk_df['length']>290)\n    sourcematch = process.extractOne(textstring,\n                                     list(compress(chunks_plaintext, selectindices)),\n                                     scorer = fuzz.ratio)\n    # Look at the differences for the modified characters\n    s1 = string2ascii(decipher_subst(sourcematch[0],reversemap2))\n    s2 = string2ascii(train_df['ciphertext'].loc[idx])\n    s2mods = [(snum in c3mod) for snum in s2]\n    ds = (s1[s2mods]-s2[s2mods])\n    # append to matrix\n    c3modmat[i,0:len(ds)] = ds\n    s2startmat[i,0:len(ds)] = s2[s2mods]\n    \n    \n# with only 26 letters, we cycle back to the beginning\nc3modmat[c3modmat<0] = c3modmat[c3modmat<0]+26\n\nfig, ax = plt.subplots(figsize=(15, 5))\n#ax.plot(np.arange(300),c3modmat.transpose())\nax.matshow(c3modmat[0:40,0:80])\nplt.xlabel('Character Changes',fontsize=16)\nplt.ylabel('Ciphertext Samples',fontsize=16)\nplt.title(\"EUREKA! A CONSISTENT PATTERN!\",fontsize=22)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5996df4fae9000ca42ef51235c22b275f001d116"},"cell_type":"markdown","source":"If assign the numerical shifts to alphabetic characters, the pattern reveals a secret message that repeats every 19 characters! A fitting tribute to [Helen Fouch Gaines](https://en.wikipedia.org/wiki/Helen_Fouch%C3%A9_Gaines)"},{"metadata":{"trusted":true,"_uuid":"9db92f30c5572765a1004816f1843191ddeee952"},"cell_type":"code","source":"modeadjust,_ = stats.mode(c3modmat,axis=0,nan_policy='omit')\nmodeadjust = np.squeeze(modeadjust.data)\nmodeadjust = np.trim_zeros(modeadjust)\nstartadj = 91\nprint('Modal adjustment to alphabetic characters')\nprint(ascii2string(startadj-np.array(modeadjust,dtype=int)))\nprint('Chars 0-19')\nprint(ascii2string(startadj-np.array(modeadjust[0:19],dtype=int)))\nprint('Chars 19-38')\nprint(ascii2string(startadj-np.array(modeadjust[19:37],dtype=int)))\nprint('Key = ')\nmodeadjust0 = [ 19.,  22.,  15.,  22.,  13.,   1.,  21.,  12.,   6.,  24.,  19.,\n        22.,   1.,  20.,   26.,  18.,  13.,  22.,   8.]\nprint(ascii2string(startadj-np.array(modeadjust0,dtype=int)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4f9e4ed712ca6afa7a920619aa91c523bf537cad"},"cell_type":"markdown","source":"We have a polyalphabetic substitution cipher with a rolling key (i.e. a Vigenre cipher). Let's now decipher our example text and then all of difficulty = 3."},{"metadata":{"_kg_hide-input":false,"trusted":true,"_uuid":"67dde3995357e64442eaa5cd7fc7c87164d24377"},"cell_type":"code","source":"def decipher_rolling(textstring,c3mod,c3adjust):\n    inarray = string2ascii(textstring)\n    outarray = np.zeros(len(inarray),dtype=np.int8)\n    outarray[:] = inarray\n    # check to see if any substitutions are needed\n    modchars = [(snum in c3mod) for snum in inarray]\n    nmods = sum(modchars)\n    if nmods>0:\n        modchars = np.squeeze(np.argwhere(modchars))\n        lowermods = outarray[modchars] > 95\n        charadj = outarray[modchars] - 65 - 32*lowermods\n        charadj = (charadj + c3adjust[:nmods])%26\n        outarray[modchars] = 65+32*lowermods+charadj\n    return ascii2string(outarray)\n\nc3adjust = np.tile(modeadjust0,16)\nc3adjust = c3adjust[0:300]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"9dde515fcca8d5dab7addf71e9ff2c56cb9f12b8"},"cell_type":"code","source":"textstring = train_df[train_df['difficulty']==3]['ciphertext'].iloc[0]\nprint('FINAL DECIPHER:')\nprint(decipher_subst(decipher_rolling(textstring,c3mod,c3adjust),asciimap2))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b6116760dc11c6d817dd609c9f65fa47e051fccb"},"cell_type":"markdown","source":"And we can now decipher difficulty=3 (and we will also try to use this to attempt difficulty=4)."},{"metadata":{"trusted":true,"_uuid":"56fd52de28456f3d7814c738fcda37072c98568c"},"cell_type":"code","source":"%%time\nc3indices = test_df[test_df['difficulty']>2].index\ntest_df.loc[c3indices,'plaintext'] = [decipher_subst(decipher_rolling(test_df['ciphertext'].loc[idx],c3mod,c3adjust),asciimap2) for idx in c3indices]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"446311444395e8b91ff8dc6848d54f0dde75b696"},"cell_type":"markdown","source":"# 6. Make Predictions\nAfter having cracked some/all of the ciphers, a big challenge is to efficiently process this all. Obvious methods, like fuzzy wuzzy lookups, take 10-30 seconds for each data item--which eats up a lot of CPU time. \n\n**Important tip: find fast/efficient ways to do this. My method uses a \"psuedo hash\" that I made up which is called `lookupscore`. ** I match deciphered text to the source for cases where they have the same lookup score, and then uses slower comparisons for the cases where there are no exact matches. The more accuraate you have your deciphering, the fewer \"slow comparisons\" you need to make. "},{"metadata":{"trusted":true,"_uuid":"874533ebf6827177453035f6ad543b27be6bb0d9","_kg_hide-input":true},"cell_type":"code","source":"def minleven(textstring,selectdataframe):\n    arrayofstrings = selectdataframe['plaintext'].values\n    ldistances = [leven.distance(textstring,compstring) for compstring in arrayofstrings]\n    bestfitref = np.argmin(ldistances)\n    bestfittext = selectdataframe['plaintext'].iloc[bestfitref]\n    bestfitindex = selectdataframe.index[bestfitref]\n    return bestfittext, bestfitindex\n\ndef char_count(textstring):\n    return np.histogram(string2ascii(textstring),np.arange(129))[0]\n    \n# calculate ascii character frequency per million characters\ndef char_per_300(stringsarray):\n    asciicount = np.zeros((128,), dtype=int)\n    for k,textdata in enumerate(stringsarray):\n        asciicount += char_count(textdata)\n    totalchars = np.sum(asciicount)\n    return np.multiply(asciicount,np.divide(300.0,totalchars))\n\ndef similarity_score(x, y):\n    #ss = 1.0 - np.divide(np.linalg.norm(x - y),np.linalg.norm(y))\n    ss = np.sum(np.multiply(x,y))\n    return ss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b72e7361cb3f9e36066e1289059f258cc3436ff5","_kg_hide-input":true},"cell_type":"code","source":"%%time\n# major speedup with a pseudo-hash\ncp300 = char_per_300(chunks_plaintext)\ntest_df['Predicted'] = 1\ntest_df['Levenshtein'] = np.nan\ntest_df['lookupscore'] = [similarity_score(char_count(textstring),cp300) for textstring in test_df['plaintext'].values ]\nchunk_df['lookupscore'] = [similarity_score(char_count(textstring),cp300) for textstring in chunk_df['plaintext'].values ]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b815dd8b8b2cfe386924a883c412c47a8c13ea39"},"cell_type":"markdown","source":"To make this kernel run for less time, we will only look for exact matches."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"c1ad5e102fc5a87eb390696f7886245188b7c702"},"cell_type":"code","source":"def exacttestmatch(idx):\n    textstring = test_df['plaintext'].loc[idx]\n    selectindices = np.argwhere(chunk_df['lookupscore']==test_df['lookupscore'].loc[idx])\n    if len(selectindices)==1:\n        matchtext = chunk_df['plaintext'].loc[selectindices[0]].values[0]\n        matchtarget = chunk_df['target'].loc[selectindices[0]].values[0]\n        matchdist = leven.distance(textstring,matchtext)\n    else:\n        matchtarget = np.nan\n        matchdist = 300\n    return matchtarget, matchdist\n\n\nfor idx in tqdm(test_df.index):\n    matchtarget, matchdist = exacttestmatch(idx)\n    if matchdist == 0:\n        test_df.loc[idx,'Predicted'] = matchtarget\n        test_df.loc[idx,'Levenshtein'] = matchdist\n        \nprint(str(np.sum(test_df['Levenshtein']==0))+\"/\"+str(len(test_df))+\" exact matches\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0d0ecba0caac335dcc43d4ba0fdfa23b972674ff"},"cell_type":"markdown","source":"And we can submit this for scoring on the Leaderboard...and only cipher #4 remains to be solved!"},{"metadata":{"trusted":true,"_uuid":"04822299ac56004d640683bb91023622b25821d6"},"cell_type":"code","source":"submission_df = test_df.copy()\nsubmission_df.set_index('Id',inplace=True)\nsubmission_df.drop(['difficulty','plaintext','ciphertext','length','Levenshtein','lookupscore'],\n                   axis=1,inplace=True)\nsubmission_df['Predicted'] = pd.to_numeric(submission_df['Predicted'],downcast='integer')\nsubmission_df.to_csv('submission.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}