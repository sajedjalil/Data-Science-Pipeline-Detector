{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **UWMGI: Unet PyTorch [Train] with EDA**","metadata":{"papermill":{"duration":0.068881,"end_time":"2022-05-01T14:21:23.855197","exception":false,"start_time":"2022-05-01T14:21:23.786316","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"#### Thanks AWSAF for a great [notebook](https://www.kaggle.com/code/awsaf49/uwmgi-unet-train-pytorch), I used some parts directly from this notebook","metadata":{"papermill":{"duration":0.050763,"end_time":"2022-05-01T14:21:23.958536","exception":false,"start_time":"2022-05-01T14:21:23.907773","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### Please if this kernel is useful, <font color='red'>please upvote !!</font>","metadata":{"papermill":{"duration":0.07339,"end_time":"2022-05-01T14:21:24.083052","exception":false,"start_time":"2022-05-01T14:21:24.009662","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Install Libraries","metadata":{"papermill":{"duration":0.095034,"end_time":"2022-05-01T14:21:24.295469","exception":false,"start_time":"2022-05-01T14:21:24.200435","status":"completed"},"tags":[]}},{"cell_type":"code","source":"!pip install -q segmentation_models_pytorch","metadata":{"papermill":{"duration":19.068405,"end_time":"2022-05-01T14:21:43.451917","exception":false,"start_time":"2022-05-01T14:21:24.383512","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-06T23:10:08.212632Z","iopub.execute_input":"2022-05-06T23:10:08.212921Z","iopub.status.idle":"2022-05-06T23:10:17.332775Z","shell.execute_reply.started":"2022-05-06T23:10:08.21289Z","shell.execute_reply":"2022-05-06T23:10:17.331883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import Libraries ","metadata":{"papermill":{"duration":0.051758,"end_time":"2022-05-01T14:21:43.555685","exception":false,"start_time":"2022-05-01T14:21:43.503927","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\npd.options.plotting.backend = \"plotly\"\nimport random\nfrom glob import glob\nimport seaborn as sns\nimport os, shutil\nfrom tqdm import tqdm\ntqdm.pandas()\nimport time\nimport copy\nimport joblib\nfrom collections import defaultdict\nimport gc\nfrom IPython import display as ipd\n\n# visualization\nimport cv2\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle\n\n# Sklearn\nfrom sklearn.model_selection import StratifiedKFold, KFold, StratifiedGroupKFold\n\n# PyTorch \nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda import amp\nimport segmentation_models_pytorch as smp\nimport timm\n\n# Albumentations for augmentations\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nimport rasterio\nfrom joblib import Parallel, delayed\n\n# For colored terminal text\nfrom colorama import Fore, Back, Style\nc_  = Fore.GREEN\nsr_ = Style.RESET_ALL\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# For descriptive error messages\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"","metadata":{"_kg_hide-input":false,"papermill":{"duration":11.235988,"end_time":"2022-05-01T14:21:54.843577","exception":false,"start_time":"2022-05-01T14:21:43.607589","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-06T23:10:17.335598Z","iopub.execute_input":"2022-05-06T23:10:17.336094Z","iopub.status.idle":"2022-05-06T23:10:17.347458Z","shell.execute_reply.started":"2022-05-06T23:10:17.336052Z","shell.execute_reply":"2022-05-06T23:10:17.346723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CFG ","metadata":{"papermill":{"duration":0.050231,"end_time":"2022-05-01T14:21:54.946677","exception":false,"start_time":"2022-05-01T14:21:54.896446","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class CFG:\n    seed          = 42\n    debug         = False \n    model_name    = 'Unet'\n    train_bs      = 4\n    valid_bs      = train_bs*2\n    img_size      = (512, 512)\n    epochs        = 20\n    lr            = 2e-3\n    scheduler     = 'CosineAnnealingLR'\n    min_lr        = 1e-6\n    T_max         = int(30000/train_bs*epochs)+50\n    T_0           = 25\n    warmup_epochs = 0\n    wd            = 1e-6\n    n_accumulate  = max(1, 32//train_bs)\n    n_fold        = 5\n    fold_selected = 1\n    num_classes   = 3\n    device        = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","metadata":{"papermill":{"duration":0.136117,"end_time":"2022-05-01T14:21:55.134348","exception":false,"start_time":"2022-05-01T14:21:54.998231","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-06T23:10:17.349162Z","iopub.execute_input":"2022-05-06T23:10:17.349736Z","iopub.status.idle":"2022-05-06T23:10:17.358759Z","shell.execute_reply.started":"2022-05-06T23:10:17.349698Z","shell.execute_reply":"2022-05-06T23:10:17.357979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed = 42):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    print('> SEEDING DONE')\n    \nset_seed(CFG.seed)","metadata":{"papermill":{"duration":0.067447,"end_time":"2022-05-01T14:21:55.253013","exception":false,"start_time":"2022-05-01T14:21:55.185566","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-06T23:10:17.361372Z","iopub.execute_input":"2022-05-06T23:10:17.361821Z","iopub.status.idle":"2022-05-06T23:10:17.388486Z","shell.execute_reply.started":"2022-05-06T23:10:17.361779Z","shell.execute_reply":"2022-05-06T23:10:17.380916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/uw-madison-gi-tract-image-segmentation/train.csv')\nprint(df.shape)","metadata":{"papermill":{"duration":0.546814,"end_time":"2022-05-01T14:21:55.852788","exception":false,"start_time":"2022-05-01T14:21:55.305974","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-06T23:10:17.392143Z","iopub.execute_input":"2022-05-06T23:10:17.392835Z","iopub.status.idle":"2022-05-06T23:10:17.667328Z","shell.execute_reply.started":"2022-05-06T23:10:17.392796Z","shell.execute_reply":"2022-05-06T23:10:17.666551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.rename(columns = {'class':'class_name'}, inplace = True)\n#--------------------------------------------------------------------------\ndf[\"case\"] = df[\"id\"].apply(lambda x: int(x.split(\"_\")[0].replace(\"case\", \"\")))\ndf[\"day\"] = df[\"id\"].apply(lambda x: int(x.split(\"_\")[1].replace(\"day\", \"\")))\ndf[\"slice\"] = df[\"id\"].apply(lambda x: x.split(\"_\")[3])\n#--------------------------------------------------------------------------\nTRAIN_DIR=\"../input/uw-madison-gi-tract-image-segmentation/train\"\nall_train_images = glob(os.path.join(TRAIN_DIR, \"**\", \"*.png\"), recursive=True)\nx = all_train_images[0].rsplit(\"/\", 4)[0] ## ../input/uw-madison-gi-tract-image-segmentation/train\n\npath_partial_list = []\nfor i in range(0, df.shape[0]):\n    path_partial_list.append(os.path.join(x,\n                          \"case\"+str(df[\"case\"].values[i]),\n                          \"case\"+str(df[\"case\"].values[i])+\"_\"+ \"day\"+str(df[\"day\"].values[i]),\n                          \"scans\",\n                          \"slice_\"+str(df[\"slice\"].values[i])))\ndf[\"path_partial\"] = path_partial_list\n#--------------------------------------------------------------------------\npath_partial_list = []\nfor i in range(0, len(all_train_images)):\n    path_partial_list.append(str(all_train_images[i].rsplit(\"_\",4)[0]))\n    \ntmp_df = pd.DataFrame()\ntmp_df['path_partial'] = path_partial_list\ntmp_df['path'] = all_train_images\n\n#--------------------------------------------------------------------------\ndf = df.merge(tmp_df, on=\"path_partial\").drop(columns=[\"path_partial\"])\n#--------------------------------------------------------------------------\ndf[\"width\"] = df[\"path\"].apply(lambda x: int(x[:-4].rsplit(\"_\",4)[1]))\ndf[\"height\"] = df[\"path\"].apply(lambda x: int(x[:-4].rsplit(\"_\",4)[2]))\n#--------------------------------------------------------------------------\ndel x,path_partial_list,tmp_df\n#--------------------------------------------------------------------------\ndf.head(5)\n","metadata":{"papermill":{"duration":8.127953,"end_time":"2022-05-01T14:22:04.034952","exception":false,"start_time":"2022-05-01T14:21:55.906999","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-06T23:10:17.668737Z","iopub.execute_input":"2022-05-06T23:10:17.668982Z","iopub.status.idle":"2022-05-06T23:10:21.807419Z","shell.execute_reply.started":"2022-05-06T23:10:17.668953Z","shell.execute_reply":"2022-05-06T23:10:21.806721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# RESTRUCTURE  DATAFRAME\ndf_train = pd.DataFrame({'id':df['id'][::3]})\n\ndf_train['large_bowel'] = df['segmentation'][::3].values\ndf_train['small_bowel'] = df['segmentation'][1::3].values\ndf_train['stomach'] = df['segmentation'][2::3].values\n\ndf_train['path'] = df['path'][::3].values\ndf_train['case'] = df['case'][::3].values\ndf_train['day'] = df['day'][::3].values\ndf_train['slice'] = df['slice'][::3].values\ndf_train['width'] = df['width'][::3].values\ndf_train['height'] = df['height'][::3].values\n\n\ndf_train.reset_index(inplace=True,drop=True)\ndf_train.fillna('',inplace=True); \ndf_train['count'] = np.sum(df_train.iloc[:,1:4]!='',axis=1).values\ndf_train.sample(5)","metadata":{"papermill":{"duration":0.137681,"end_time":"2022-05-01T14:22:04.227564","exception":false,"start_time":"2022-05-01T14:22:04.089883","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-06T23:10:21.80871Z","iopub.execute_input":"2022-05-06T23:10:21.809133Z","iopub.status.idle":"2022-05-06T23:10:21.902755Z","shell.execute_reply.started":"2022-05-06T23:10:21.809094Z","shell.execute_reply":"2022-05-06T23:10:21.902063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{"papermill":{"duration":0.053448,"end_time":"2022-05-01T14:22:04.335224","exception":false,"start_time":"2022-05-01T14:22:04.281776","status":"completed"},"tags":[]}},{"cell_type":"code","source":"df_train['count'].value_counts().plot.bar()","metadata":{"execution":{"iopub.status.busy":"2022-05-06T23:10:21.90376Z","iopub.execute_input":"2022-05-06T23:10:21.903973Z","iopub.status.idle":"2022-05-06T23:10:21.970939Z","shell.execute_reply.started":"2022-05-06T23:10:21.903949Z","shell.execute_reply":"2022-05-06T23:10:21.9701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,4))\nbar = plt.bar([1,2,3],100*np.mean( df_train.iloc[:,1:4]!='',axis=0))\nplt.title('Percent Training Images with Mask', fontsize=16)\nplt.ylabel('Percent of Images'); plt.xlabel('Class Type')\nplt.xticks([1,2,3])\nlabels=[\"large bowel\",\"small bowel\",\"stomach\"]\nfor rect,lbl in zip(bar,labels):\n    height = rect.get_height()\n    plt.text(rect.get_x() + rect.get_width()/3, height,  lbl,\n             ha='center', va='bottom',fontsize=16)\n    plt.text(rect.get_x() + rect.get_width()/1.3, height, '%.1f %%' % height,\n             ha='center', va='bottom',fontsize=13)\n\nplt.ylim((0,50)); plt.show()","metadata":{"papermill":{"duration":0.308924,"end_time":"2022-05-01T14:22:04.697458","exception":false,"start_time":"2022-05-01T14:22:04.388534","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-06T23:10:21.972295Z","iopub.execute_input":"2022-05-06T23:10:21.972552Z","iopub.status.idle":"2022-05-06T23:10:22.168596Z","shell.execute_reply.started":"2022-05-06T23:10:21.972515Z","shell.execute_reply":"2022-05-06T23:10:22.167902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# RLE","metadata":{"papermill":{"duration":0.057618,"end_time":"2022-05-01T14:22:04.811513","exception":false,"start_time":"2022-05-01T14:22:04.753895","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ref: https://www.kaggle.com/paulorzp/run-length-encode-and-decode\ndef rle_decode(mask_rle, shape):\n    '''\n    mask_rle: run-length as string formated (start length)\n    shape: (height,width) of array to return \n    Returns numpy array, 1 - mask, 0 - background\n\n    '''\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape(shape)  # Needed to align to RLE direction\n\n\n# ref.: https://www.kaggle.com/stainsby/fast-tested-rle\ndef rle_encode(img):\n    '''\n    img: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    '''\n    pixels = img.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)\n\n\ndef show_img(img, mask=None):\n    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n    plt.imshow(img, cmap='bone')\n    \n    if mask is not None:\n        plt.imshow(mask, alpha=0.5)\n        handles = [Rectangle((0,0),1,1, color=_c) for _c in [(0.667,0.0,0.0), (0.0,0.667,0.0), (0.0,0.0,0.667)]]\n        labels = [\"Large Bowel\", \"Small Bowel\", \"Stomach\"]\n        plt.legend(handles,labels)\n    plt.axis('off')","metadata":{"papermill":{"duration":0.071865,"end_time":"2022-05-01T14:22:04.941636","exception":false,"start_time":"2022-05-01T14:22:04.869771","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-06T23:10:22.171813Z","iopub.execute_input":"2022-05-06T23:10:22.172024Z","iopub.status.idle":"2022-05-06T23:10:22.182742Z","shell.execute_reply.started":"2022-05-06T23:10:22.171999Z","shell.execute_reply":"2022-05-06T23:10:22.182094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Folds","metadata":{"papermill":{"duration":0.053777,"end_time":"2022-05-01T14:22:05.050418","exception":false,"start_time":"2022-05-01T14:22:04.996641","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# SAMPLES\ntrain_mask = list(df_train[df_train['large_bowel']!=''].index)\ntrain_mask += list(df_train[df_train['small_bowel']!=''].index)\ntrain_mask += list(df_train[df_train['stomach']!=''].index)\n\ndf_train=df_train[df_train.index.isin(train_mask)]     \ndf_train.reset_index(inplace=True,drop=True)\nprint(df_train.shape)","metadata":{"papermill":{"duration":0.104198,"end_time":"2022-05-01T14:22:05.210468","exception":false,"start_time":"2022-05-01T14:22:05.10627","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-06T23:10:22.184044Z","iopub.execute_input":"2022-05-06T23:10:22.18447Z","iopub.status.idle":"2022-05-06T23:10:22.238111Z","shell.execute_reply.started":"2022-05-06T23:10:22.184433Z","shell.execute_reply":"2022-05-06T23:10:22.237336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skf = StratifiedGroupKFold(n_splits=CFG.n_fold, shuffle=True, random_state=42)\nfor fold, (_, val_idx) in enumerate(skf.split(X=df_train, y=df_train['case'],groups =df_train['case']), 1):\n    df_train.loc[val_idx, 'fold'] = fold\n    \ndf_train['fold'] = df_train['fold'].astype(np.uint8)\n\ntrain_ids = df_train[df_train[\"fold\"]!=CFG.fold_selected].index\nvalid_ids = df_train[df_train[\"fold\"]==CFG.fold_selected].index\n\ndf_train.groupby('fold').size()","metadata":{"papermill":{"duration":0.180251,"end_time":"2022-05-01T14:22:05.445752","exception":false,"start_time":"2022-05-01T14:22:05.265501","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-06T23:10:22.239444Z","iopub.execute_input":"2022-05-06T23:10:22.23986Z","iopub.status.idle":"2022-05-06T23:10:22.354176Z","shell.execute_reply.started":"2022-05-06T23:10:22.239824Z","shell.execute_reply":"2022-05-06T23:10:22.353289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(df_train.groupby(['fold','count'])['id'].count())","metadata":{"papermill":{"duration":0.07296,"end_time":"2022-05-01T14:22:05.57516","exception":false,"start_time":"2022-05-01T14:22:05.5022","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-06T23:10:22.355654Z","iopub.execute_input":"2022-05-06T23:10:22.355941Z","iopub.status.idle":"2022-05-06T23:10:22.369807Z","shell.execute_reply.started":"2022-05-06T23:10:22.355905Z","shell.execute_reply":"2022-05-06T23:10:22.368996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{"papermill":{"duration":0.056645,"end_time":"2022-05-01T14:22:05.689182","exception":false,"start_time":"2022-05-01T14:22:05.632537","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class BuildDataset(torch.utils.data.Dataset):\n    def __init__(self, df, subset=\"train\", transforms=None):\n        self.df = df\n        self.subset = subset\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.df)\n    \n    \n    def __getitem__(self, index): \n        masks = np.zeros((CFG.img_size[0],CFG.img_size[1], 3), dtype=np.float32)\n        img_path=self.df['path'].iloc[index]\n        w=self.df['width'].iloc[index]\n        h=self.df['height'].iloc[index]\n        img = self.__load_img(img_path)\n        if self.subset == 'train':\n            for k,j in zip([0,1,2],[\"large_bowel\",\"small_bowel\",\"stomach\"]):\n                rles=self.df[j].iloc[index]\n                mask = rle_decode(rles, shape=(h, w, 1))\n                mask = cv2.resize(mask, CFG.img_size)\n                masks[:,:,k] = mask\n        \n        masks = masks.transpose(2, 0, 1)\n        img = img.transpose(2, 0, 1)\n\n        if self.subset == 'train': return torch.tensor(img), torch.tensor(masks)\n        else: return torch.tensor(img)\n        \n    def __load_gray_img(self, img_path):\n        img = cv2.imread(img_path, cv2.IMREAD_ANYDEPTH)\n        img = (img - img.min())/(img.max() - img.min())*255.0 \n        img = cv2.resize(img, CFG.img_size)\n        img = np.expand_dims(img, axis=-1)\n        img = img.astype(np.float32) / 255.\n        return img\n    def __load_img(self, img_path):\n        img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n        img = (img - img.min())/(img.max() - img.min())*255.0 \n        img = cv2.resize(img, CFG.img_size)\n        img = np.tile(img[...,None], [1, 1, 3]) # gray to rgb\n        img = img.astype(np.float32) /255.\n        return img\n    \n\n","metadata":{"papermill":{"duration":0.079661,"end_time":"2022-05-01T14:22:05.825296","exception":false,"start_time":"2022-05-01T14:22:05.745635","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-06T23:10:22.371494Z","iopub.execute_input":"2022-05-06T23:10:22.371823Z","iopub.status.idle":"2022-05-06T23:10:22.388351Z","shell.execute_reply.started":"2022-05-06T23:10:22.371784Z","shell.execute_reply":"2022-05-06T23:10:22.387438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Augmentations","metadata":{"papermill":{"duration":0.058218,"end_time":"2022-05-01T14:22:05.941496","exception":false,"start_time":"2022-05-01T14:22:05.883278","status":"completed"},"tags":[]}},{"cell_type":"code","source":"data_transforms = {\n    \"train\": A.Compose([\n        A.Resize(*CFG.img_size, interpolation=cv2.INTER_NEAREST),\n        A.VerticalFlip(),\n        A.OneOf([\n                A.RandomBrightness(),\n                ], p=0.5),\n        ], p=1.0),\n    \"valid\": A.Compose([\n        A.Resize(*CFG.img_size, interpolation=cv2.INTER_NEAREST),\n        ], p=1.0)\n}","metadata":{"_kg_hide-output":true,"papermill":{"duration":0.0678,"end_time":"2022-05-01T14:22:06.066101","exception":false,"start_time":"2022-05-01T14:22:05.998301","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-06T23:10:22.389809Z","iopub.execute_input":"2022-05-06T23:10:22.390377Z","iopub.status.idle":"2022-05-06T23:10:22.401567Z","shell.execute_reply.started":"2022-05-06T23:10:22.390334Z","shell.execute_reply":"2022-05-06T23:10:22.400466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DataLoader","metadata":{"papermill":{"duration":0.056669,"end_time":"2022-05-01T14:22:06.179941","exception":false,"start_time":"2022-05-01T14:22:06.123272","status":"completed"},"tags":[]}},{"cell_type":"code","source":"\ntrain_dataset = BuildDataset(df_train[df_train.index.isin(train_ids)], transforms=data_transforms['train'])\nvalid_dataset = BuildDataset(df_train[df_train.index.isin(valid_ids)], transforms=data_transforms['valid'])\n\ntrain_loader = DataLoader(train_dataset, batch_size=CFG.train_bs, num_workers=4, shuffle=True, pin_memory=True, drop_last=False)\nvalid_loader = DataLoader(valid_dataset, batch_size=CFG.valid_bs,num_workers=4, shuffle=False, pin_memory=True)\n    \n\nimgs, msks = next(iter(train_loader))\nimgs.size(), msks.size()","metadata":{"papermill":{"duration":7.702453,"end_time":"2022-05-01T14:22:13.938855","exception":false,"start_time":"2022-05-01T14:22:06.236402","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-06T23:10:22.403489Z","iopub.execute_input":"2022-05-06T23:10:22.40382Z","iopub.status.idle":"2022-05-06T23:10:23.441657Z","shell.execute_reply.started":"2022-05-06T23:10:22.403781Z","shell.execute_reply":"2022-05-06T23:10:23.440912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Visualization","metadata":{"papermill":{"duration":0.078956,"end_time":"2022-05-01T14:22:14.076464","exception":false,"start_time":"2022-05-01T14:22:13.997508","status":"completed"},"tags":[]}},{"cell_type":"code","source":"imgs, msks = next(iter(train_loader))\nimgs.size(), msks.size()\n\ndef plot_batch(imgs, msks, size=3):\n    plt.figure(figsize=(4*4, 4))\n    for idx in range(size):\n        plt.subplot(1, 4, idx+1)\n        img = imgs[idx,].permute((1, 2, 0)).numpy()\n        msk = msks[idx,].permute((1, 2, 0)).numpy()\n        show_img(img, msk)\n    plt.tight_layout()\n    plt.show()\n\nplot_batch(imgs, msks, size=4)","metadata":{"_kg_hide-input":true,"papermill":{"duration":3.628283,"end_time":"2022-05-01T14:22:17.817956","exception":false,"start_time":"2022-05-01T14:22:14.189673","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-06T23:10:23.443508Z","iopub.execute_input":"2022-05-06T23:10:23.443791Z","iopub.status.idle":"2022-05-06T23:10:25.163178Z","shell.execute_reply.started":"2022-05-06T23:10:23.443754Z","shell.execute_reply":"2022-05-06T23:10:25.162299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ngc.collect()","metadata":{"papermill":{"duration":0.343671,"end_time":"2022-05-01T14:22:18.227067","exception":false,"start_time":"2022-05-01T14:22:17.883396","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-06T23:10:25.165082Z","iopub.execute_input":"2022-05-06T23:10:25.165363Z","iopub.status.idle":"2022-05-06T23:10:25.54852Z","shell.execute_reply.started":"2022-05-06T23:10:25.165327Z","shell.execute_reply":"2022-05-06T23:10:25.547796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# UNet Model\n","metadata":{"papermill":{"duration":0.065659,"end_time":"2022-05-01T14:22:18.364383","exception":false,"start_time":"2022-05-01T14:22:18.298724","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class DoubleConv(nn.Module):\n    \"\"\"(Conv3D -> BN -> ReLU) * 2\"\"\"\n    def __init__(self, in_channels, out_channels, num_groups=8):\n        super().__init__()\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n          )     \n        \n    def forward(self,x):\n        return self.double_conv(x)\n\n    \nclass Down(nn.Module):\n\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.encoder = nn.Sequential(\n            nn.MaxPool2d(2, 2),\n            DoubleConv(in_channels, out_channels)\n        )\n    def forward(self, x):\n        return self.encoder(x)\n\n    \nclass Up(nn.Module):\n\n    def __init__(self, in_channels, out_channels, bilinear=True):\n        super().__init__()\n        \n        if bilinear:\n            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n        else:\n            self.up = nn.ConvTranspose2d(in_channels // 2, in_channels // 2, kernel_size=2, stride=2)\n            \n        self.conv = DoubleConv(in_channels, out_channels)\n        \n    def forward(self, x1, x2):\n        x1 = self.up(x1)\n\n        diffY = x2.size()[2] - x1.size()[2]\n        diffX = x2.size()[3] - x1.size()[3]\n\n        x1 = F.pad(x1, (diffX // 2, diffX - diffX // 2,\n                        diffY // 2, diffY - diffY // 2))\n        x = torch.cat([x2, x1], dim=1)\n        x = self.conv(x)\n        return x       \n\n    \nclass Out(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size = 1)\n\n    def forward(self, x):\n        return self.conv(x)\n\n\nclass UNet(nn.Module):\n    def __init__(self, in_channels, n_classes, n_channels):\n        super().__init__()\n        self.in_channels = in_channels\n        self.n_classes = n_classes\n        self.n_channels = n_channels\n\n        self.conv = DoubleConv(in_channels, n_channels)\n        self.enc1 = Down(n_channels, 2 * n_channels)\n        self.enc2 = Down(2 * n_channels, 4 * n_channels)\n        self.enc3 = Down(4 * n_channels, 8 * n_channels)\n        self.enc4 = Down(8 * n_channels, 16 * n_channels)\n        self.enc5 = Down(16 * n_channels, 16 * n_channels)\n        \n        self.dec1 = Up(32 * n_channels, 8 * n_channels)\n        self.dec2 = Up(16 * n_channels, 4 * n_channels)\n        self.dec3 = Up(8 * n_channels, 2 * n_channels)\n        self.dec4 = Up(4 * n_channels, n_channels)\n        self.dec5 = Up(2 * n_channels, n_channels)\n        self.out = Out(n_channels, n_classes)\n\n    def forward(self, x):\n        x1 = self.conv(x)\n        x2 = self.enc1(x1)\n        x3 = self.enc2(x2)\n        x4 = self.enc3(x3)\n        x5 = self.enc4(x4)\n        x6 = self.enc5(x5)\n        \n        mask = self.dec1(x6, x5)\n        mask = self.dec2(mask, x4)\n        mask = self.dec3(mask, x3)\n        mask = self.dec4(mask, x2)\n        mask = self.dec5(mask, x1)\n        mask = self.out(mask)\n        return mask","metadata":{"papermill":{"duration":0.092656,"end_time":"2022-05-01T14:22:18.520966","exception":false,"start_time":"2022-05-01T14:22:18.42831","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-06T23:10:25.550959Z","iopub.execute_input":"2022-05-06T23:10:25.551521Z","iopub.status.idle":"2022-05-06T23:10:25.572506Z","shell.execute_reply.started":"2022-05-06T23:10:25.551484Z","shell.execute_reply":"2022-05-06T23:10:25.571833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model():\n    model = UNet(in_channels=3, n_classes=3, n_channels=32)\n    model.to(CFG.device)\n    return model\n\n\ndef load_model(path):\n    model = build_model()\n    model.load_state_dict(torch.load(path))\n    model.eval()\n    return model","metadata":{"papermill":{"duration":0.075448,"end_time":"2022-05-01T14:22:18.660525","exception":false,"start_time":"2022-05-01T14:22:18.585077","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-06T23:10:25.573528Z","iopub.execute_input":"2022-05-06T23:10:25.574618Z","iopub.status.idle":"2022-05-06T23:10:25.58711Z","shell.execute_reply.started":"2022-05-06T23:10:25.574577Z","shell.execute_reply":"2022-05-06T23:10:25.586284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ðŸ”§ Loss Function","metadata":{"papermill":{"duration":0.065476,"end_time":"2022-05-01T14:22:18.791235","exception":false,"start_time":"2022-05-01T14:22:18.725759","status":"completed"},"tags":[]}},{"cell_type":"code","source":"JaccardLoss = smp.losses.JaccardLoss(mode='multilabel')\nDiceLoss    = smp.losses.DiceLoss(mode='multilabel')\nBCELoss     = smp.losses.SoftBCEWithLogitsLoss()\nLovaszLoss  = smp.losses.LovaszLoss(mode='multilabel', per_image=False)\nTverskyLoss = smp.losses.TverskyLoss(mode='multilabel', log_loss=False)\n\ndef dice_coef(y_true, y_pred, thr=0.5, dim=(2,3), epsilon=0.001):\n    y_true = y_true.to(torch.float32)\n    y_pred = (y_pred>thr).to(torch.float32)\n    inter = (y_true*y_pred).sum(dim=dim)\n    den = y_true.sum(dim=dim) + y_pred.sum(dim=dim)\n    dice = ((2*inter+epsilon)/(den+epsilon)).mean(dim=(1,0))\n    return dice\n\ndef iou_coef(y_true, y_pred, thr=0.5, dim=(2,3), epsilon=0.001):\n    y_true = y_true.to(torch.float32)\n    y_pred = (y_pred>thr).to(torch.float32)\n    inter = (y_true*y_pred).sum(dim=dim)\n    union = (y_true + y_pred - y_true*y_pred).sum(dim=dim)\n    iou = ((inter+epsilon)/(union+epsilon)).mean(dim=(1,0))\n    return iou\n\ndef criterion(y_pred, y_true):\n    return 0.6*BCELoss(y_pred, y_true) + 0.4*DiceLoss(y_pred, y_true)","metadata":{"papermill":{"duration":0.082752,"end_time":"2022-05-01T14:22:18.940904","exception":false,"start_time":"2022-05-01T14:22:18.858152","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-06T23:10:25.588625Z","iopub.execute_input":"2022-05-06T23:10:25.589234Z","iopub.status.idle":"2022-05-06T23:10:25.602964Z","shell.execute_reply.started":"2022-05-06T23:10:25.589193Z","shell.execute_reply":"2022-05-06T23:10:25.602141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Function","metadata":{"papermill":{"duration":0.099375,"end_time":"2022-05-01T14:22:19.108356","exception":false,"start_time":"2022-05-01T14:22:19.008981","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n    model.train()\n    scaler = amp.GradScaler()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    \n    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc='Train ')\n    for step, (images, masks) in pbar:         \n        images = images.to(device, dtype=torch.float)\n        masks  = masks.to(device, dtype=torch.float)\n        \n        batch_size = images.size(0)\n        \n        with amp.autocast(enabled=True):\n            y_pred = model(images)\n            loss   = criterion(y_pred, masks)\n            loss   = loss / CFG.n_accumulate\n            \n        scaler.scale(loss).backward()\n    \n        if (step + 1) % CFG.n_accumulate == 0:\n            scaler.step(optimizer)\n            scaler.update()\n\n            # zero the parameter gradients\n            optimizer.zero_grad()\n\n            if scheduler is not None:\n                scheduler.step()\n                \n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n        \n        epoch_loss = running_loss / dataset_size\n        \n        mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0\n        current_lr = optimizer.param_groups[0]['lr']\n        pbar.set_postfix(train_loss=f'{epoch_loss:0.4f}',\n                        lr=f'{current_lr:0.5f}',\n                        gpu_mem=f'{mem:0.2f} GB')\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    return epoch_loss","metadata":{"papermill":{"duration":0.137479,"end_time":"2022-05-01T14:22:19.392561","exception":false,"start_time":"2022-05-01T14:22:19.255082","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-06T23:10:25.605401Z","iopub.execute_input":"2022-05-06T23:10:25.607549Z","iopub.status.idle":"2022-05-06T23:10:25.61951Z","shell.execute_reply.started":"2022-05-06T23:10:25.607514Z","shell.execute_reply":"2022-05-06T23:10:25.61868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Validation Function","metadata":{"papermill":{"duration":0.110867,"end_time":"2022-05-01T14:22:19.616474","exception":false,"start_time":"2022-05-01T14:22:19.505607","status":"completed"},"tags":[]}},{"cell_type":"code","source":"@torch.no_grad()\ndef valid_one_epoch(model, dataloader, device, epoch):\n    model.eval()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    \n    val_scores = []\n    \n    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc='Valid ')\n    for step, (images, masks) in pbar:        \n        images  = images.to(device, dtype=torch.float)\n        masks   = masks.to(device, dtype=torch.float)\n        \n        batch_size = images.size(0)\n        \n        y_pred  = model(images)\n        loss    = criterion(y_pred, masks)\n        \n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n        \n        epoch_loss = running_loss / dataset_size\n        \n        y_pred = nn.Sigmoid()(y_pred)\n        val_dice = dice_coef(masks, y_pred).cpu().detach().numpy()\n        val_jaccard = iou_coef(masks, y_pred).cpu().detach().numpy()\n        val_scores.append([val_dice, val_jaccard])\n        \n        mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0\n        current_lr = optimizer.param_groups[0]['lr']\n        pbar.set_postfix(valid_loss=f'{epoch_loss:0.4f}',\n                        lr=f'{current_lr:0.5f}',\n                        gpu_memory=f'{mem:0.2f} GB')\n    val_scores  = np.mean(val_scores, axis=0)\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    return epoch_loss, val_scores","metadata":{"papermill":{"duration":0.081551,"end_time":"2022-05-01T14:22:19.808831","exception":false,"start_time":"2022-05-01T14:22:19.72728","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-06T23:10:25.621126Z","iopub.execute_input":"2022-05-06T23:10:25.621805Z","iopub.status.idle":"2022-05-06T23:10:25.634518Z","shell.execute_reply.started":"2022-05-06T23:10:25.621764Z","shell.execute_reply":"2022-05-06T23:10:25.633746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{"papermill":{"duration":0.065922,"end_time":"2022-05-01T14:22:19.942521","exception":false,"start_time":"2022-05-01T14:22:19.876599","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def run_training(model, optimizer, scheduler, device, num_epochs):\n    # To automatically log gradients\n\n    \n    if torch.cuda.is_available():\n        print(\"cuda: {}\\n\".format(torch.cuda.get_device_name()))\n    \n    start = time.time()\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_dice      = -np.inf\n    best_epoch     = -1\n    history = defaultdict(list)\n    \n    for epoch in range(1, num_epochs + 1): \n        gc.collect()\n        print(f'Epoch {epoch}/{num_epochs}', end='')\n        train_loss = train_one_epoch(model, optimizer, scheduler, \n                                           dataloader=train_loader, \n                                           device=CFG.device, epoch=epoch)\n        \n        val_loss, val_scores = valid_one_epoch(model, valid_loader, \n                                                 device=CFG.device, \n                                                 epoch=epoch)\n        val_dice, val_jaccard = val_scores\n    \n        history['Train Loss'].append(train_loss)\n        history['Valid Loss'].append(val_loss)\n        history['Valid Dice'].append(val_dice)\n        history['Valid Jaccard'].append(val_jaccard)\n        \n        # Log the metrics\n\n        \n        print(f'Valid Dice: {val_dice:0.4f} | Valid Jaccard: {val_jaccard:0.4f}')\n        \n        # deep copy the model\n        if val_dice >= best_dice:\n            print(f\"{c_}Valid Score Improved ({best_dice:0.4f} ---> {val_dice:0.4f})\")\n            best_dice    = val_dice\n            best_jaccard = val_jaccard\n            best_epoch   = epoch\n            #run.summary[\"Best Dice\"]    = best_dice\n           # run.summary[\"Best Jaccard\"] = best_jaccard\n           # run.summary[\"Best Epoch\"]   = best_epoch\n            best_model_wts = copy.deepcopy(model.state_dict())\n            PATH = f\"best_epoch-{fold:02d}.bin\"\n            torch.save(model.state_dict(), PATH)\n            # Save a model file from the current directory\n            print(f\"Model Saved{sr_}\")\n            \n        last_model_wts = copy.deepcopy(model.state_dict())\n        PATH = f\"last_epoch-{fold:02d}.bin\"\n        torch.save(model.state_dict(), PATH)\n            \n        print(); print()\n    \n    end = time.time()\n    time_elapsed = end - start\n    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n        time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n    print(\"Best Score: {:.4f}\".format(best_jaccard))\n    \n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    \n    return model, history","metadata":{"papermill":{"duration":0.081613,"end_time":"2022-05-01T14:22:20.088511","exception":false,"start_time":"2022-05-01T14:22:20.006898","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-06T23:10:25.63626Z","iopub.execute_input":"2022-05-06T23:10:25.636546Z","iopub.status.idle":"2022-05-06T23:10:25.652328Z","shell.execute_reply.started":"2022-05-06T23:10:25.63651Z","shell.execute_reply":"2022-05-06T23:10:25.651572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fetch_scheduler(optimizer):\n    if CFG.scheduler == 'CosineAnnealingLR':\n        scheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=CFG.T_max, \n                                                   eta_min=CFG.min_lr)\n    elif CFG.scheduler == 'CosineAnnealingWarmRestarts':\n        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=CFG.T_0, \n                                                             eta_min=CFG.min_lr)\n    elif CFG.scheduler == 'ReduceLROnPlateau':\n        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer,\n                                                   mode='min',\n                                                   factor=0.1,\n                                                   patience=7,\n                                                   threshold=0.0001,\n                                                   min_lr=CFG.min_lr,)\n    elif CFG.scheduer == 'ExponentialLR':\n        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.85)\n    elif CFG.scheduler == None:\n        return None\n        \n    return scheduler","metadata":{"papermill":{"duration":0.075722,"end_time":"2022-05-01T14:22:20.229967","exception":false,"start_time":"2022-05-01T14:22:20.154245","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-06T23:10:25.653717Z","iopub.execute_input":"2022-05-06T23:10:25.654684Z","iopub.status.idle":"2022-05-06T23:10:25.664992Z","shell.execute_reply.started":"2022-05-06T23:10:25.654644Z","shell.execute_reply":"2022-05-06T23:10:25.66421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = build_model()\noptimizer = optim.Adam(model.parameters(), lr=CFG.lr, weight_decay=CFG.wd)\nscheduler = fetch_scheduler(optimizer)","metadata":{"papermill":{"duration":0.229539,"end_time":"2022-05-01T14:22:20.523289","exception":false,"start_time":"2022-05-01T14:22:20.29375","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-06T23:10:25.666465Z","iopub.execute_input":"2022-05-06T23:10:25.667372Z","iopub.status.idle":"2022-05-06T23:10:25.804575Z","shell.execute_reply.started":"2022-05-06T23:10:25.667277Z","shell.execute_reply":"2022-05-06T23:10:25.803824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Start Training","metadata":{"papermill":{"duration":0.065997,"end_time":"2022-05-01T14:22:20.65523","exception":false,"start_time":"2022-05-01T14:22:20.589233","status":"completed"},"tags":[]}},{"cell_type":"code","source":"for fold in range(1):\n    print(f'#'*35)\n    print(f'######### Fold: {fold}')\n    print(f'#'*35)\n    model     = build_model()\n    optimizer = optim.Adam(model.parameters(), lr=CFG.lr, weight_decay=CFG.wd)\n    scheduler = fetch_scheduler(optimizer)\n    model, history = run_training(model, optimizer, scheduler,\n                                  device=CFG.device,\n                                  num_epochs=CFG.epochs)\n    ","metadata":{"_kg_hide-output":true,"papermill":{"duration":5604.638418,"end_time":"2022-05-01T15:55:45.360588","exception":false,"start_time":"2022-05-01T14:22:20.72217","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-06T23:10:25.80604Z","iopub.execute_input":"2022-05-06T23:10:25.806388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loss, Dice,Jaccard Curves","metadata":{}},{"cell_type":"code","source":"# PLOT TRAINING\nplt.figure(figsize=(15,5))\nplt.plot(range(CFG.epochs),history['Valid Dice'],label='Valid Dice')\nplt.plot(range(CFG.epochs),history['Valid Jaccard'],label='Valid Jaccard')\nplt.title('Dice & Jaccard'); plt.xlabel('Epoch'); plt.ylabel('');plt.legend(); \nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PLOT TRAINING\nplt.figure(figsize=(15,5))\nplt.plot(range(CFG.epochs),history['Train Loss'],label='Train Loss')\nplt.plot(range(CFG.epochs),history['Valid Loss'],label='Valid Loss')\nplt.title('LOSS'); plt.xlabel('Epoch'); plt.ylabel('loss');plt.legend(); \nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction","metadata":{"papermill":{"duration":7.69462,"end_time":"2022-05-01T15:56:01.266413","exception":false,"start_time":"2022-05-01T15:55:53.571793","status":"completed"},"tags":[]}},{"cell_type":"code","source":"test_dataset = BuildDataset(df_train[df_train.index.isin(valid_ids)], \n                            transforms=data_transforms['valid'])\ntest_loader  = DataLoader(test_dataset, batch_size=5, \n                          num_workers=4, shuffle=False, pin_memory=True)\n\nimgs, msks =  next(iter(test_loader))\n\nimgs = imgs.to(CFG.device, dtype=torch.float)\n\npreds = []\nfor fold in range(1):\n    model = load_model(f\"best_epoch-{fold:02d}.bin\")\n    with torch.no_grad():\n        pred = model(imgs)\n        pred = (nn.Sigmoid()(pred)>0.5).double()\n    preds.append(pred)\n    \nimgs  = imgs.cpu().detach()\npreds = torch.mean(torch.stack(preds, dim=0), dim=0).cpu().detach()","metadata":{"papermill":{"duration":8.540461,"end_time":"2022-05-01T15:56:17.83046","exception":false,"start_time":"2022-05-01T15:56:09.289999","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_batch(imgs, preds, size=5)","metadata":{"papermill":{"duration":8.588373,"end_time":"2022-05-01T15:56:34.25364","exception":false,"start_time":"2022-05-01T15:56:25.665267","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Computing Dice & Jaccard for Classes","metadata":{"papermill":{"duration":7.781034,"end_time":"2022-05-01T15:56:50.578607","exception":false,"start_time":"2022-05-01T15:56:42.797573","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def dice_coef_metric_per_classes(probabilities: np.ndarray,\n                                    truth: np.ndarray,\n                                    treshold: float = 0.5,\n                                    eps: float = 1e-9,\n                                    classes: list = [\"Large Bowel\", \"Small Bowel\", \"Stomach\"]) -> np.ndarray:\n    \"\"\"\n    Calculate Dice score for data batch and for each class.\n    Params:\n        probobilities: model outputs after activation function.\n        truth: model targets.\n        threshold: threshold for probabilities.\n        eps: additive to refine the estimate.\n        classes: list with name classes.\n        Returns: dict with dice scores for each class.\n    \"\"\"\n    scores = {key: list() for key in classes}\n    num = probabilities.shape[0]\n    num_classes = probabilities.shape[1]\n    predictions = (probabilities >= treshold).astype(np.float32)\n    assert(predictions.shape == truth.shape)\n\n    for i in range(num):\n        for class_ in range(num_classes):\n            prediction = predictions[i][class_]\n            truth_ = truth[i][class_]\n            intersection = 2.0 * (truth_ * prediction).sum()\n            union = truth_.sum() + prediction.sum()\n            if truth_.sum() == 0 and prediction.sum() == 0:\n                 scores[classes[class_]].append(1.0)\n            else:\n                scores[classes[class_]].append((intersection + eps) / union)\n                \n    return scores\n\n\ndef jaccard_coef_metric_per_classes(probabilities: np.ndarray,\n               truth: np.ndarray,\n               treshold: float = 0.5,\n               eps: float = 1e-9,\n               classes: list = [\"Large Bowel\", \"Small Bowel\", \"Stomach\"]) -> np.ndarray:\n    \"\"\"\n    Calculate Jaccard index for data batch and for each class.\n    Params:\n        probobilities: model outputs after activation function.\n        truth: model targets.\n        threshold: threshold for probabilities.\n        eps: additive to refine the estimate.\n        classes: list with name classes.\n        Returns: dict with jaccard scores for each class.\"\n    \"\"\"\n    scores = {key: list() for key in classes}\n    num = probabilities.shape[0]\n    num_classes = probabilities.shape[1]\n    predictions = (probabilities >= treshold).astype(np.float32)\n    assert(predictions.shape == truth.shape)\n\n    for i in range(num):\n        for class_ in range(num_classes):\n            prediction = predictions[i][class_]\n            truth_ = truth[i][class_]\n            intersection = (prediction * truth_).sum()\n            union = (prediction.sum() + truth_.sum()) - intersection + eps\n            if truth_.sum() == 0 and prediction.sum() == 0:\n                 scores[classes[class_]].append(1.0)\n            else:\n                scores[classes[class_]].append((intersection + eps) / union)\n\n    return scores","metadata":{"papermill":{"duration":7.467532,"end_time":"2022-05-01T15:57:05.820825","exception":false,"start_time":"2022-05-01T15:56:58.353293","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_scores_per_classes(model,\n                               dataloader,\n                               classes):\n    \"\"\"\n    Compute Dice and Jaccard coefficients for each class.\n    Params:\n        model: neural net for make predictions.\n        dataloader: dataset object to load data from.\n        classes: list with classes.\n        Returns: dictionaries with dice and jaccard coefficients for each class for each slice.\n    \"\"\"\n\n    dice_scores_per_classes = {key: list() for key in classes}\n    iou_scores_per_classes = {key: list() for key in classes}\n    with torch.no_grad():\n        for i, data in enumerate(dataloader):\n            imgs, targets = data[0], data[1]\n            imgs, targets = imgs.to(CFG.device), targets.to(CFG.device)\n            logits = model(imgs)\n            logits = logits.detach().cpu().numpy()\n            targets = targets.detach().cpu().numpy()\n            \n            dice_scores = dice_coef_metric_per_classes(logits, targets)\n            iou_scores = jaccard_coef_metric_per_classes(logits, targets)\n\n            \n            for key in dice_scores.keys():\n                dice_scores_per_classes[key].extend(dice_scores[key])\n\n            for key in iou_scores.keys():\n                iou_scores_per_classes[key].extend(iou_scores[key])\n                \n    return dice_scores_per_classes, iou_scores_per_classes","metadata":{"papermill":{"duration":7.756741,"end_time":"2022-05-01T15:57:21.974034","exception":false,"start_time":"2022-05-01T15:57:14.217293","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dice_scores_per_classes, iou_scores_per_classes = compute_scores_per_classes(\n    model, test_loader, [\"Large Bowel\", \"Small Bowel\", \"Stomach\"]\n    )","metadata":{"papermill":{"duration":53.529015,"end_time":"2022-05-01T15:58:23.240052","exception":false,"start_time":"2022-05-01T15:57:29.711037","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dice_df = pd.DataFrame(dice_scores_per_classes)\ndice_df.columns = ['Large Bowel Dice', 'Small Bowel Dice', 'Stomach Dice']\n\niou_df = pd.DataFrame(iou_scores_per_classes)\niou_df.columns = ['Large Bowel Jaccard', 'Small Bowel Jaccard', 'Stomach Jaccard']\nval_metics_df = pd.concat([dice_df, iou_df], axis=1, sort=True)\nval_metics_df = val_metics_df.loc[:, ['Large Bowel Dice', 'Large Bowel Jaccard', \n                                      'Small Bowel Dice', 'Small Bowel Jaccard', \n                                      'Stomach Dice', 'Stomach Jaccard']]\nval_metics_df.head(3)","metadata":{"papermill":{"duration":7.769347,"end_time":"2022-05-01T15:58:38.628641","exception":false,"start_time":"2022-05-01T15:58:30.859294","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"colors = ['#008899', '#aaaa00', '#008899', '#aaaa00', '#008899', '#aaaa00']\npalette = sns.color_palette(colors, 6)\n\nfig, ax = plt.subplots(figsize=(20, 7));\nsns.barplot(x=val_metics_df.mean().index, y=val_metics_df.mean(), palette=palette, ax=ax);\nax.set_xticklabels(val_metics_df.columns, fontsize=12, rotation=0);\nax.set_title(\"Dice and IoU \", fontsize=20)\n\nfor idx, p in enumerate(ax.patches):\n        percentage = '{:.2f}%'.format(100 * val_metics_df.mean().values[idx])\n        x = p.get_x() + p.get_width() / 2 - 0.15\n        y = p.get_y() + p.get_height() + 0.005\n        ax.annotate(percentage, (x, y), fontsize=15, fontweight=\"bold\")\n","metadata":{"papermill":{"duration":8.043052,"end_time":"2022-05-01T15:58:54.591679","exception":false,"start_time":"2022-05-01T15:58:46.548627","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]}]}