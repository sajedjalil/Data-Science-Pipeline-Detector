{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# GI-Seg PyTorch âš¡ Train & Infer\n- Downloaded weight and requirements come from [GI-Seg Downloads](https://www.kaggle.com/clemchris/gi-seg-download)\n- Dataset: [UW-Madison GI Tract Image Segmentation Masks](https://www.kaggle.com/datasets/clemchris/uw-madison-gi-tract-image-segmentation-masks)\n\n\n## Sources --> please upvote them if you find this notebook useful\n- Awsaf's [UWMGI: Unet [Train] [PyTorch]](https://www.kaggle.com/code/awsaf49/uwmgi-unet-train-pytorch)\n- Awsaf's [UWMGI: 2.5D stride=2 Data](https://www.kaggle.com/code/awsaf49/uwmgi-2-5d-stride-2-data)\n- Awsaf's [UWMGI: Unet [Infer] [PyTorch]](https://www.kaggle.com/code/awsaf49/uwmgi-unet-infer-pytorch)\n\n## Scores\n- V13: 0.775 (`arch=\"Unet\"`, `encoder_name=\"efficientnet-b2\"`, `batch_size=64\"`, `img_size=256`, `max_epochs=3`)\n- V14: 0.748 (`arch=\"Unet\"`, `encoder_name=\"efficientnet-b0\"`, `batch_size=128\"`, `img_size=256`, `max_epochs=3`)\n- V15 - V19: Memory errors \n- V20: 0.778 (`arch=\"Unet\"`, `encoder_name=\"efficientnet-b4\"`, `batch_size=64\"`, `img_size=256`, `max_epochs=5`)\n- V21 - V24: Inference bug fixes\n- V25: 0.812 (`arch=\"Unet\"`, `encoder_name=\"efficientnet-b1\"`, `batch_size=128\"`, `img_size=224`, `max_epochs=15`)\n- V26: 0.827 (2.5D Data, `arch=\"Unet\"`, `encoder_name=\"efficientnet-b1\"`, `batch_size=128\"`, `img_size=224`, `max_epochs=15`)\n- V27 - V29: Memory errors\n- V30: 0.841 (2.5D Data, use scheduler every step, `arch=\"Unet\"`, `encoder_name=\"efficientnet-b1\"`, `batch_size=128\"`, `img_size=224`, `max_epochs=15`)\n- V31: 0.840 (2.5D Data, use scheduler every step, `arch=\"Unet\"`, `encoder_name=\"efficientnet-b1\"`, `batch_size=96\"`, `img_size=256`, `max_epochs=15`)\n- V32: 0.XXX (2.5D Data, use scheduler every step, `arch=\"Unet\"`, `encoder_name=\"timm-efficientnet-b1\"`, `encoder_weights=\"noisy-student\"`, `batch_size=96\"`, `img_size=256`, `max_epochs=15`)","metadata":{}},{"cell_type":"markdown","source":"# Setup Pretrained Model Checkpoint","metadata":{}},{"cell_type":"code","source":"!mkdir -p /root/.cache/torch/hub/checkpoints\n!cp ../input/gi-seg-downloads/efficientnet-b1-f1951068.pth /root/.cache/torch/hub/checkpoints/efficientnet-b1-f1951068.pth\n!cp ../input/gi-seg-downloads/tf_efficientnet_b1_ns-99dd0c41.pth /root/.cache/torch/hub/checkpoints/tf_efficientnet_b1_ns-99dd0c41.pth\n","metadata":{"execution":{"iopub.status.busy":"2022-05-09T14:09:05.471507Z","iopub.execute_input":"2022-05-09T14:09:05.47187Z","iopub.status.idle":"2022-05-09T14:09:09.393701Z","shell.execute_reply.started":"2022-05-09T14:09:05.471776Z","shell.execute_reply":"2022-05-09T14:09:09.392505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Installs","metadata":{}},{"cell_type":"code","source":"!cd ../input/gi-seg-downloads && \\\npip install -q efficientnet_pytorch-0.6.3.tar.gz pretrainedmodels-0.7.4.tar.gz timm-0.4.12-py3-none-any.whl  segmentation_models_pytorch-0.2.1-py3-none-any.whl && \\\npip install -q monai-0.8.1-202202162213-py3-none-any.whl && \\\npip install -q torchmetrics-0.8.2-py3-none-any.whl","metadata":{"execution":{"iopub.status.busy":"2022-05-09T14:09:09.39677Z","iopub.execute_input":"2022-05-09T14:09:09.397125Z","iopub.status.idle":"2022-05-09T14:10:45.068612Z","shell.execute_reply.started":"2022-05-09T14:09:09.397073Z","shell.execute_reply":"2022-05-09T14:10:45.067511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\nfrom typing import Callable\nfrom typing import List\nfrom typing import Optional\nfrom typing import Tuple\n\nimport albumentations as A\nimport cupy as cp\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pytorch_lightning as pl\nimport seaborn as sns\nimport segmentation_models_pytorch as smp\nimport torch\nfrom albumentations.pytorch import ToTensorV2\nfrom monai.metrics.utils import get_mask_edges\nfrom monai.metrics.utils import get_surface_distance\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nfrom torchmetrics import Metric\nfrom torchmetrics import MetricCollection\nfrom torch.utils.data import DataLoader\nfrom tqdm.notebook import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-09T14:10:45.071973Z","iopub.execute_input":"2022-05-09T14:10:45.072704Z","iopub.status.idle":"2022-05-09T14:10:55.651812Z","shell.execute_reply.started":"2022-05-09T14:10:45.07266Z","shell.execute_reply":"2022-05-09T14:10:55.650793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Paths & Settings","metadata":{}},{"cell_type":"code","source":"KAGGLE_DIR = Path(\"/\") / \"kaggle\"\nINPUT_DIR = KAGGLE_DIR / \"input\"\nOUTPUT_DIR = KAGGLE_DIR / \"working\"\n\nINPUT_DATA_DIR = INPUT_DIR / \"uw-madison-gi-tract-image-segmentation\"\nINPUT_DATA_NPY_DIR = INPUT_DIR / \"uw-madison-gi-tract-image-segmentation-masks\"\n\nN_SPLITS = 5\nRANDOM_SEED = 2022\nIMG_SIZE = 256\nVAL_FOLD = 0\nLOAD_IMAGES = True # True for 2.5D data\nUSE_AUGS = True\nBATCH_SIZE = 96\nNUM_WORKERS = 2\nARCH = \"Unet\"\nENCODER_NAME = \"timm-efficientnet-b1\"\nENCODER_WEIGHTS = \"noisy-student\"\nLOSS = \"bce_tversky\"\nOPTIMIZER = \"Adam\"\nLEARNING_RATE = 2e-3\nWEIGHT_DECAY = 1e-6\nSCHEDULER = \"CosineAnnealingLR\"\nMIN_LR = 1e-6\n\nFAST_DEV_RUN = False # Debug training\nGPUS = 1\nMAX_EPOCHS = 15\nPRECISION = 16\n\nCHANNELS = 3\nSTRIDE = 2\nDEVICE = \"cuda\"\nTHR = 0.45\n\nDEBUG = False # Debug complete pipeline\n","metadata":{"execution":{"iopub.status.busy":"2022-05-09T14:10:55.655176Z","iopub.execute_input":"2022-05-09T14:10:55.655515Z","iopub.status.idle":"2022-05-09T14:10:55.667976Z","shell.execute_reply.started":"2022-05-09T14:10:55.655469Z","shell.execute_reply":"2022-05-09T14:10:55.66515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"class Dataset(torch.utils.data.Dataset):\n    def __init__(self, df: pd.DataFrame, load_images: bool, load_mask: bool, transforms: Optional[Callable] = None):\n        self.df = df\n        self.load_images = load_images\n        self.load_mask = load_mask\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx: int):\n        row = self.df.iloc[idx]\n\n        if self.load_images:\n            image = self._load_images(eval(row[\"image_paths\"]))\n        else:\n            image = self._load_image(row[\"image_path\"])\n\n        if self.load_mask:\n            mask = self._load_mask(row[\"mask_path\"])\n\n            if self.transforms:\n                data = self.transforms(image=image, mask=mask)\n                image, mask = data[\"image\"], data[\"mask\"]\n\n            return image, mask\n        else:\n            id_ = row[\"id\"]\n            h, w = image.shape[:2]\n\n            if self.transforms:\n                data = self.transforms(image=image)\n                image = data[\"image\"]\n\n            return image, id_, h, w\n        \n    def _load_images(self, paths):\n        images = [self._load_image(path, tile=False) for path in paths]\n        image = np.stack(images, axis=-1)\n        return image\n\n    @staticmethod\n    def _load_image(path, tile: bool = True):\n        image = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n        image = image.astype(\"float32\")  # original is uint16\n        \n        if tile:\n            image = np.tile(image[..., None], [1, 1, 3])  # gray to rgb\n            \n        image /= image.max()\n\n        return image\n\n    @staticmethod\n    def _load_mask(path):\n        return np.load(path).astype(\"float32\") / 255.0\n","metadata":{"execution":{"iopub.status.busy":"2022-05-09T14:10:55.669876Z","iopub.execute_input":"2022-05-09T14:10:55.670295Z","iopub.status.idle":"2022-05-09T14:10:55.689488Z","shell.execute_reply.started":"2022-05-09T14:10:55.670219Z","shell.execute_reply":"2022-05-09T14:10:55.688456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LitDataModule","metadata":{}},{"cell_type":"code","source":"class LitDataModule(pl.LightningDataModule):\n    def __init__(\n        self,\n        train_csv_path: str,\n        test_csv_path: Optional[str],\n        img_size: int,\n        use_augs: bool,\n        val_fold: int,\n        load_images: bool,\n        batch_size: int,\n        num_workers: int,\n    ):\n        super().__init__()\n\n        self.save_hyperparameters()\n\n        self.train_df = pd.read_csv(train_csv_path)\n\n        if test_csv_path is not None:\n            self.test_df = pd.read_csv(test_csv_path)\n        else:\n            self.test_df = None\n\n        self.train_transforms, self.val_test_transforms = self._init_transforms()\n\n    def _init_transforms(self):\n        img_size = self.hparams.img_size\n\n        train_transforms = [\n            A.Resize(img_size, img_size, interpolation=cv2.INTER_NEAREST),\n        ]\n        if self.hparams.use_augs:\n            train_transforms += [\n                A.HorizontalFlip(p=0.5),\n                A.VerticalFlip(p=0.5),\n                A.OneOf(\n                    [\n                        A.GridDistortion(num_steps=5, distort_limit=0.05, p=1.0),\n                        A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=1.0),\n                    ],\n                    p=0.25,\n                ),\n            ]\n        train_transforms += [ToTensorV2(transpose_mask=True)]\n        train_transforms = A.Compose(train_transforms, p=1.0)\n\n        val_test_transforms = [\n            A.Resize(img_size, img_size, interpolation=cv2.INTER_NEAREST),\n            ToTensorV2(transpose_mask=True),\n        ]\n        val_test_transforms = A.Compose(val_test_transforms, p=1.0)\n\n        return train_transforms, val_test_transforms\n\n    def setup(self, stage: Optional[str] = None):\n        train_df = self.train_df[self.train_df.fold != self.hparams.val_fold].reset_index(drop=True)\n        val_df = self.train_df[self.train_df.fold == self.hparams.val_fold].reset_index(drop=True)\n\n        if stage == \"fit\" or stage is None:\n            self.train_dataset = self._dataset(train_df, load_mask=True, transform=self.train_transforms)\n            self.val_dataset = self._dataset(val_df, load_mask=True, transform=self.val_test_transforms)\n\n        if stage == \"test\" or stage is None:\n            if self.test_df is not None:\n                self.test_dataset = self._dataset(self.test_df, load_mask=False, transform=self.val_test_transforms)\n            else:\n                self.test_dataset = self._dataset(val_df, load_mask=True, transform=self.val_test_transforms)\n\n    def _dataset(self, df: pd.DataFrame, load_mask: bool, transform: Callable) -> Dataset:\n        return Dataset(df=df, load_images=self.hparams.load_images, load_mask=load_mask, transforms=transform)\n\n    def train_dataloader(self) -> DataLoader:\n        return self._dataloader(self.train_dataset, train=True)\n\n    def val_dataloader(self) -> DataLoader:\n        return self._dataloader(self.val_dataset)\n\n    def test_dataloader(self) -> DataLoader:\n        return self._dataloader(self.test_dataset)\n\n    def _dataloader(self, dataset: Dataset, train: bool = False) -> DataLoader:\n        return DataLoader(\n            dataset,\n            batch_size=self.hparams.batch_size,\n            shuffle=train,\n            num_workers=self.hparams.num_workers,\n            pin_memory=True,\n        )\n","metadata":{"execution":{"iopub.status.busy":"2022-05-09T14:10:55.691444Z","iopub.execute_input":"2022-05-09T14:10:55.691872Z","iopub.status.idle":"2022-05-09T14:10:55.717595Z","shell.execute_reply.started":"2022-05-09T14:10:55.691825Z","shell.execute_reply":"2022-05-09T14:10:55.716413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Metrics","metadata":{}},{"cell_type":"code","source":"class CompetitionMetric(Metric):\n    def __init__(self, thr=THR):\n        super().__init__(compute_on_step=False)\n\n        self.thr = thr\n\n        self.add_state(\"y_pred\", default=[])\n        self.add_state(\"y_true\", default=[])\n\n    def update(self, y_pred: torch.Tensor, y_true: torch.Tensor):\n        y_pred = torch.nn.Sigmoid()(y_pred)\n        y_pred = (y_pred > self.thr).to(\"cpu\").detach().to(torch.float32)\n\n        y_true = y_true.to(\"cpu\").detach().to(torch.float32)\n\n        self.y_pred.append(y_pred)\n        self.y_true.append(y_true)\n\n    def compute(self):\n        y_pred = torch.cat(self.y_pred).numpy()\n        y_true = torch.cat(self.y_true).numpy()\n\n        return compute_competition_metric(y_pred, y_true)[0]\n\n\ndef compute_competition_metric(preds: np.ndarray, targets: np.ndarray) -> float:\n    dice_ = compute_dice(preds, targets)\n    hd_dist_ = compute_hd_dist(preds, targets)\n    return 0.4 * dice_ + 0.6 * hd_dist_, dice_, hd_dist_\n\n\n# Slightly adapted from https://www.kaggle.com/code/carnozhao?scriptVersionId=93589877&cellId=2\ndef compute_dice(preds: np.ndarray, targets: np.ndarray) -> float:\n    preds = preds.astype(np.uint8)\n    targets = targets.astype(np.uint8)\n\n    I = (targets & preds).sum((2, 3))  # noqa: E741\n    U = (targets | preds).sum((2, 3))  # noqa: E741\n\n    return np.mean((2 * I / (U + I + 1) + (U == 0)).mean(1))\n\n\ndef compute_hd_dist(preds: np.ndarray, targets: np.ndarray) -> float:\n    return 1 - np.mean([hd_dist_batch(preds[:, i, ...], targets[:, i, ...]) for i in range(3)])\n\n\ndef hd_dist_batch(preds: np.ndarray, targets: np.ndarray) -> float:\n    return np.mean([hd_dist(pred, target) for pred, target in zip(preds, targets)])\n\n\n# From https://www.kaggle.com/code/yiheng?scriptVersionId=93883465&cellId=4\ndef hd_dist(pred: np.ndarray, target: np.ndarray) -> float:\n    if np.all(pred == target):\n        return 0.0\n\n    edges_pred, edges_gt = get_mask_edges(pred, target)\n    surface_distance = get_surface_distance(edges_pred, edges_gt, distance_metric=\"euclidean\")\n\n    if surface_distance.shape == (0,):\n        return 0.0\n\n    dist = surface_distance.max()\n    max_dist = np.sqrt(np.sum(np.array(pred.shape) ** 2))\n\n    if dist > max_dist:\n        return 1.0\n\n    return dist / max_dist\n","metadata":{"execution":{"iopub.status.busy":"2022-05-09T14:10:55.719681Z","iopub.execute_input":"2022-05-09T14:10:55.720159Z","iopub.status.idle":"2022-05-09T14:10:55.742982Z","shell.execute_reply.started":"2022-05-09T14:10:55.720086Z","shell.execute_reply":"2022-05-09T14:10:55.741816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LitModule","metadata":{}},{"cell_type":"code","source":"class LitModule(pl.LightningModule):\n    LOSS_FNS = {\n        \"bce\": smp.losses.SoftBCEWithLogitsLoss(),\n        \"dice\": smp.losses.DiceLoss(mode=\"multilabel\"),\n        \"focal\": smp.losses.FocalLoss(mode=\"multilabel\"),\n        \"jaccard\": smp.losses.JaccardLoss(mode=\"multilabel\"),\n        \"lovasz\": smp.losses.LovaszLoss(mode=\"multilabel\"),\n        \"tversky\": smp.losses.TverskyLoss(mode=\"multilabel\"),\n    }\n\n    def __init__(\n        self,\n        arch: str,\n        encoder_name: str,\n        encoder_weights: str,\n        loss: str,\n        optimizer: str,\n        learning_rate: float,\n        weight_decay: float,\n        scheduler: Optional[str],\n        T_max: int,\n        T_0: int,\n        min_lr: int,\n    ):\n        super().__init__()\n\n        self.save_hyperparameters()\n\n        self.model = self._init_model()\n\n        self.loss_fn = self._init_loss_fn()\n\n        self.metrics = self._init_metrics()\n\n    def _init_model(self):\n        return smp.create_model(\n            self.hparams.arch,\n            encoder_name=self.hparams.encoder_name,\n            encoder_weights=self.hparams.encoder_weights,\n            in_channels=3,\n            classes=3,\n            activation=None,\n        )\n\n    def _init_loss_fn(self):\n        losses = self.hparams.loss.split(\"_\")\n        loss_fns = [self.LOSS_FNS[loss] for loss in losses]\n\n        def criterion(y_pred, y_true):\n            return sum(loss_fn(y_pred, y_true) for loss_fn in loss_fns) / len(loss_fns)\n\n        return criterion\n\n    def _init_metrics(self):\n        val_metrics = MetricCollection({\"val_comp_metric\": CompetitionMetric()})\n        test_metrics = MetricCollection({\"test_comp_metric\": CompetitionMetric()})\n\n        return torch.nn.ModuleDict(\n            {\n                \"val_metrics\": val_metrics,\n                \"test_metrics\": test_metrics,\n            }\n        )\n\n    def configure_optimizers(self):\n        optimizer_kwargs = dict(\n            params=self.parameters(), lr=self.hparams.learning_rate, weight_decay=self.hparams.weight_decay\n        )\n        if self.hparams.optimizer == \"Adadelta\":\n            optimizer = torch.optim.Adadelta(**optimizer_kwargs)\n        elif self.hparams.optimizer == \"Adagrad\":\n            optimizer = torch.optim.Adagrad(**optimizer_kwargs)\n        elif self.hparams.optimizer == \"Adam\":\n            optimizer = torch.optim.Adam(**optimizer_kwargs)\n        elif self.hparams.optimizer == \"AdamW\":\n            optimizer = torch.optim.AdamW(**optimizer_kwargs)\n        elif self.hparams.optimizer == \"Adamax\":\n            optimizer = torch.optim.Adamax(**optimizer_kwargs)\n        elif self.hparams.optimizer == \"SGD\":\n            optimizer = torch.optim.SGD(**optimizer_kwargs)\n        else:\n            raise ValueError(f\"Unknown optimizer: {self.hparams.optimizer}\")\n\n        if self.hparams.scheduler is not None:\n            if self.hparams.scheduler == \"CosineAnnealingLR\":\n                scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n                    optimizer, T_max=self.hparams.T_max, eta_min=self.hparams.min_lr\n                )\n            elif self.hparams.scheduler == \"CosineAnnealingWarmRestarts\":\n                scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n                    optimizer, T_0=self.hparams.T_0, eta_min=self.hparams.min_lr\n                )\n            elif self.hparams.scheduler == \"ExponentialLR\":\n                scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n            elif self.hparams.scheduler == \"ReduceLROnPlateau\":\n                scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5)\n            else:\n                raise ValueError(f\"Unknown scheduler: {self.hparams.scheduler}\")\n\n            return {\"optimizer\": optimizer, \"lr_scheduler\": {\"scheduler\": scheduler, \"interval\": \"step\"}}\n        else:\n            return {\"optimizer\": optimizer}\n\n    def forward(self, images):\n        return self.model(images)\n\n    def training_step(self, batch, batch_idx):\n        return self.shared_step(batch, \"train\")\n\n    def validation_step(self, batch, batch_idx):\n        self.shared_step(batch, \"val\")\n\n    def test_step(self, batch, batch_idx):\n        self.shared_step(batch, \"test\")\n\n    def shared_step(self, batch, stage, log=True):\n        images, masks = batch\n        y_pred = self(images)\n\n        loss = self.loss_fn(y_pred, masks)\n\n        if stage != \"train\":\n            metrics = self.metrics[f\"{stage}_metrics\"](y_pred, masks)\n        else:\n            metrics = None\n\n        if log:\n            self._log(loss, metrics, stage)\n\n        return loss\n\n    def _log(self, loss, metrics, stage):\n        on_step = True if stage == \"train\" else False\n\n        self.log(f\"{stage}_loss\", loss, on_step=on_step, on_epoch=True, prog_bar=True)\n\n        if metrics is not None:\n            self.log_dict(metrics, on_step=on_step, on_epoch=True)\n\n    @classmethod\n    def load_eval_checkpoint(cls, checkpoint_path, device):\n        module = cls.load_from_checkpoint(checkpoint_path=checkpoint_path).to(device)\n        module.eval()\n\n        return module\n","metadata":{"execution":{"iopub.status.busy":"2022-05-09T14:10:55.745102Z","iopub.execute_input":"2022-05-09T14:10:55.745441Z","iopub.status.idle":"2022-05-09T14:10:55.7811Z","shell.execute_reply.started":"2022-05-09T14:10:55.745395Z","shell.execute_reply":"2022-05-09T14:10:55.780131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"def train(\n    random_seed: int = RANDOM_SEED,\n    train_csv_path: str = str(INPUT_DATA_NPY_DIR / \"train_preprocessed.csv\"),\n    img_size: int = IMG_SIZE,\n    use_augs: bool = USE_AUGS,\n    val_fold: int = VAL_FOLD,\n    load_images: bool = LOAD_IMAGES,\n    batch_size: int = BATCH_SIZE,\n    num_workers: int = NUM_WORKERS,\n    arch: str = ARCH,\n    encoder_name: str = ENCODER_NAME,\n    encoder_weights: str = ENCODER_WEIGHTS,\n    loss: str = LOSS,\n    optimizer: str = OPTIMIZER,\n    learning_rate: float = LEARNING_RATE,\n    weight_decay: float = WEIGHT_DECAY,\n    scheduler: str = SCHEDULER,\n    min_lr: float = MIN_LR,\n    gpus: int = GPUS,\n    fast_dev_run: bool = FAST_DEV_RUN,\n    max_epochs: int = MAX_EPOCHS,\n    precision: int = PRECISION,\n    debug: bool = DEBUG,\n):\n    pl.seed_everything(random_seed)\n\n    if debug:\n        num_workers = 0\n        max_epochs = 2\n\n    data_module = LitDataModule(\n        train_csv_path=train_csv_path,\n        test_csv_path=None,\n        img_size=img_size,\n        use_augs=use_augs,\n        val_fold=val_fold,\n        load_images=load_images,\n        batch_size=batch_size,\n        num_workers=num_workers,\n    )\n\n    module = LitModule(\n        arch=arch,\n        encoder_name=encoder_name,\n        encoder_weights=encoder_weights,\n        loss=loss,\n        optimizer=optimizer,\n        learning_rate=learning_rate,\n        weight_decay=weight_decay,\n        scheduler=scheduler,\n        T_max=int(30_000 / batch_size * max_epochs) + 50,\n        T_0=25,\n        min_lr=min_lr,\n    )\n\n    trainer = pl.Trainer(\n        fast_dev_run=fast_dev_run,\n        gpus=gpus,\n        limit_train_batches=0.02 if debug else 1.0,\n        limit_val_batches=0.02 if debug else 1.0,\n        limit_test_batches=0.02 if debug else 0.5, # Metric computation takes too much memory otherwise\n        logger=pl.loggers.CSVLogger(save_dir='logs/'),\n        log_every_n_steps=10,\n        max_epochs=max_epochs,\n        precision=precision,\n    )\n\n    trainer.fit(module, datamodule=data_module)\n    \n    \n    if not fast_dev_run:\n        trainer.test(module, datamodule=data_module)\n    \n    return trainer\n","metadata":{"execution":{"iopub.status.busy":"2022-05-09T14:10:55.783403Z","iopub.execute_input":"2022-05-09T14:10:55.784034Z","iopub.status.idle":"2022-05-09T14:10:55.800631Z","shell.execute_reply.started":"2022-05-09T14:10:55.783989Z","shell.execute_reply":"2022-05-09T14:10:55.799413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = train()","metadata":{"execution":{"iopub.status.busy":"2022-05-09T14:10:55.80501Z","iopub.execute_input":"2022-05-09T14:10:55.805329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# From https://www.kaggle.com/code/jirkaborovec?scriptVersionId=93358967&cellId=22\nmetrics = pd.read_csv(f\"{trainer.logger.log_dir}/metrics.csv\")[[\"epoch\", \"train_loss_epoch\", \"val_loss\"]]\nmetrics.set_index(\"epoch\", inplace=True)\n\nsns.relplot(data=metrics, kind=\"line\", height=5, aspect=1.5)\nplt.grid()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Infer","metadata":{}},{"cell_type":"markdown","source":"### Load Test Data","metadata":{}},{"cell_type":"code","source":"def extract_metadata_from_id(df):\n    df[[\"case\", \"day\", \"slice\"]] = df[\"id\"].str.split(\"_\", n=2, expand=True)\n\n    df[\"case\"] = df[\"case\"].str.replace(\"case\", \"\").astype(int)\n    df[\"day\"] = df[\"day\"].str.replace(\"day\", \"\").astype(int)\n    df[\"slice\"] = df[\"slice\"].str.replace(\"slice_\", \"\").astype(int)\n\n    return df\n\n\ndef extract_metadata_from_path(path_df):\n    path_df[[\"parent\", \"case_day\", \"scans\", \"file_name\"]] = path_df[\"image_path\"].str.rsplit(\"/\", n=3, expand=True)\n\n    path_df[[\"case\", \"day\"]] = path_df[\"case_day\"].str.split(\"_\", expand=True)\n    path_df[\"case\"] = path_df[\"case\"].str.replace(\"case\", \"\")\n    path_df[\"day\"] = path_df[\"day\"].str.replace(\"day\", \"\")\n\n    path_df[[\"slice\", \"width\", \"height\", \"spacing\", \"spacing_\"]] = (\n        path_df[\"file_name\"].str.replace(\"slice_\", \"\").str.replace(\".png\", \"\").str.split(\"_\", expand=True)\n    )\n    path_df = path_df.drop(columns=[\"parent\", \"case_day\", \"scans\", \"file_name\", \"spacing_\"])\n\n    numeric_cols = [\"case\", \"day\", \"slice\", \"width\", \"height\", \"spacing\"]\n    path_df[numeric_cols] = path_df[numeric_cols].apply(pd.to_numeric)\n\n    return path_df\n\n\ndef add_image_paths(df, channels, stride):\n    for i in range(channels):\n        df[f\"image_path_{i:02}\"] = df.groupby([\"case\", \"day\"])[\"image_path\"].shift(-i * stride).fillna(method=\"ffill\")\n    \n    image_path_columns = [f\"image_path_{i:02d}\" for i in range(channels)]\n    df[\"image_paths\"] = df[image_path_columns].values.tolist()\n    df = df.drop(columns=image_path_columns)\n    \n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df = pd.read_csv(INPUT_DATA_DIR / \"sample_submission.csv\")\ntest_set_hidden = not bool(len(sub_df))\n\nif test_set_hidden:\n    test_df = pd.read_csv(INPUT_DATA_DIR / \"train.csv\")[: 1000 * 3]\n    test_df = test_df.drop(columns=[\"class\", \"segmentation\"]).drop_duplicates()\n    image_paths = [str(path) for path in (INPUT_DATA_DIR / \"train\").rglob(\"*.png\")]\nelse:\n    test_df = sub_df.drop(columns=[\"class\", \"predicted\"]).drop_duplicates()\n    image_paths = [str(path) for path in (INPUT_DATA_DIR / \"test\").rglob(\"*.png\")]\n\ntest_df = extract_metadata_from_id(test_df)\n\npath_df = pd.DataFrame(image_paths, columns=[\"image_path\"])\npath_df = extract_metadata_from_path(path_df)\n\ntest_df = test_df.merge(path_df, on=[\"case\", \"day\", \"slice\"], how=\"left\")\ntest_df = add_image_paths(test_df, CHANNELS, STRIDE)\n\nprint(len(test_df))\ntest_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Save Test DataFrame","metadata":{}},{"cell_type":"code","source":"test_df.to_csv(\"test_preprocessed.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Run inference","metadata":{}},{"cell_type":"code","source":"def mask2rle(mask):\n    \"\"\"\n    img: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    \"\"\"\n    mask = cp.array(mask)\n    pixels = mask.flatten()\n    pad = cp.array([0])\n    pixels = cp.concatenate([pad, pixels, pad])\n    runs = cp.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n\n    return \" \".join(str(x) for x in runs)\n\n\ndef masks2rles(masks, ids, heights, widths):\n    pred_strings = []\n    pred_ids = []\n    pred_classes = []\n\n    for idx in range(masks.shape[0]):\n        height = heights[idx].item()\n        width = widths[idx].item()\n        mask = cv2.resize(masks[idx], dsize=(width, height), interpolation=cv2.INTER_NEAREST)  # back to original shape\n\n        rle = [None] * 3\n        for midx in [0, 1, 2]:\n            rle[midx] = mask2rle(mask[..., midx])\n\n        pred_strings.extend(rle)\n        pred_ids.extend([ids[idx]] * len(rle))\n        pred_classes.extend([\"large_bowel\", \"small_bowel\", \"stomach\"])\n\n    return pred_strings, pred_ids, pred_classes\n\n\n@torch.no_grad()\ndef infer(img_size, load_images, batch_size, num_workers, model_paths, device, thr):\n    data_module = LitDataModule(\n        train_csv_path=str(INPUT_DATA_NPY_DIR / \"train_preprocessed.csv\"),\n        test_csv_path=\"test_preprocessed.csv\",\n        img_size=img_size,\n        use_augs=False,\n        val_fold=0,\n        load_images=load_images,\n        batch_size=batch_size,\n        num_workers=num_workers,\n    )\n    data_module.setup(stage=\"test\")\n    test_dataloader = data_module.test_dataloader()\n    \n    pred_strings = []\n    pred_ids = []\n    pred_classes = []\n\n    for imgs, ids, heights, widths in tqdm(test_dataloader):\n        imgs = imgs.to(device, dtype=torch.float)\n        size = imgs.size()\n\n        masks = []\n        masks = torch.zeros((size[0], 3, size[2], size[3]), device=device, dtype=torch.float32)\n\n        for path in model_paths:\n            model = LitModule.load_eval_checkpoint(path, device=device)\n            out = model(imgs)\n            out = torch.nn.Sigmoid()(out)\n            masks += out / len(model_paths)\n\n        masks = (masks.permute((0, 2, 3, 1)) > thr).to(torch.uint8).cpu().detach().numpy()  # shape: (n, h, w, c)\n\n        result = masks2rles(masks, ids, heights, widths)\n        pred_strings.extend(result[0])\n        pred_ids.extend(result[1])\n        pred_classes.extend(result[2])\n\n    pred_df = pd.DataFrame({\"id\": pred_ids, \"class\": pred_classes, \"predicted\": pred_strings})\n\n    return pred_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_paths = list((Path(trainer.logger.log_dir) / \"checkpoints\").glob(\"*.ckpt\"))\nmodel_paths","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_df = infer(IMG_SIZE, LOAD_IMAGES, 32, NUM_WORKERS, model_paths, DEVICE, THR)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submit","metadata":{}},{"cell_type":"code","source":"if not test_set_hidden:\n    sub_df = pd.read_csv(\"../input/uw-madison-gi-tract-image-segmentation/sample_submission.csv\")\n    del sub_df[\"predicted\"]\nelse:\n    sub_df = pd.read_csv(\"../input/uw-madison-gi-tract-image-segmentation/train.csv\")[: 1000 * 3]\n    del sub_df[\"segmentation\"]\n\nsub_df = sub_df.merge(pred_df, on=[\"id\", \"class\"])\nsub_df.to_csv(\"submission.csv\", index=False)\ndisplay(sub_df.head(5))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ","metadata":{}}]}