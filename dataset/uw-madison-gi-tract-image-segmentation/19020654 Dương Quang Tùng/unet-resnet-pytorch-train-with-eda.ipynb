{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **UWMGI: Unet PyTorch [Train] with EDA**","metadata":{"papermill":{"duration":0.068881,"end_time":"2022-05-01T14:21:23.855197","exception":false,"start_time":"2022-05-01T14:21:23.786316","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Import Libraries ","metadata":{"papermill":{"duration":0.051758,"end_time":"2022-05-01T14:21:43.555685","exception":false,"start_time":"2022-05-01T14:21:43.503927","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\npd.options.plotting.backend = \"plotly\"\nimport random\nfrom glob import glob\nimport seaborn as sns\nimport os, shutil\nfrom tqdm import tqdm\ntqdm.pandas()\nimport time\nimport copy\nimport joblib\nfrom collections import defaultdict\nimport gc\nfrom IPython import display as ipd\n\n# visualization\nimport cv2\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle\n\n# Sklearn\nfrom sklearn.model_selection import StratifiedKFold, KFold, StratifiedGroupKFold\n\n# PyTorch \nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda import amp\n\n# Albumentations for augmentations\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nimport rasterio\nfrom joblib import Parallel, delayed\n\n# For colored terminal text\nfrom colorama import Fore, Back, Style\nc_  = Fore.GREEN\nsr_ = Style.RESET_ALL\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# # For descriptive error messages\n# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"","metadata":{"_kg_hide-input":false,"papermill":{"duration":11.235988,"end_time":"2022-05-01T14:21:54.843577","exception":false,"start_time":"2022-05-01T14:21:43.607589","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-16T10:54:17.387709Z","iopub.execute_input":"2022-05-16T10:54:17.388212Z","iopub.status.idle":"2022-05-16T10:54:21.760884Z","shell.execute_reply.started":"2022-05-16T10:54:17.388098Z","shell.execute_reply":"2022-05-16T10:54:21.760162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cupy as cp\n\ndef mask2rle(msk, thr=0.5):\n    '''\n    img: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    '''\n    msk    = cp.array(msk)\n    pixels = msk.flatten()\n    pad    = cp.array([0])\n    pixels = cp.concatenate([pad, pixels, pad])\n    runs   = cp.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)\n\ndef masks2rles(msks, ids, heights, widths):\n    pred_strings = []; pred_ids = []; pred_classes = [];\n    for idx in range(msks.shape[0]):\n        height = heights[idx].item()\n        width = widths[idx].item()\n        msk = cv2.resize(msks[idx], \n                         dsize=(width, height), \n                         interpolation=cv2.INTER_NEAREST) # back to original shape\n        rle = [None]*3\n        for midx in [0, 1, 2]:\n            rle[midx] = mask2rle(msk[...,midx])\n        pred_strings.extend(rle)\n        pred_ids.extend([ids[idx]]*len(rle))\n        pred_classes.extend(['large_bowel', 'small_bowel', 'stomach'])\n    return pred_strings, pred_ids, pred_classes","metadata":{"execution":{"iopub.status.busy":"2022-05-16T10:54:21.762348Z","iopub.execute_input":"2022-05-16T10:54:21.762582Z","iopub.status.idle":"2022-05-16T10:54:22.946811Z","shell.execute_reply.started":"2022-05-16T10:54:21.762549Z","shell.execute_reply":"2022-05-16T10:54:22.94607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CFG ","metadata":{"papermill":{"duration":0.050231,"end_time":"2022-05-01T14:21:54.946677","exception":false,"start_time":"2022-05-01T14:21:54.896446","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class CFG:\n    seed          = 42\n    debug         = False \n    model_name    = 'Unet'\n    train_bs      = 32\n    valid_bs      = train_bs*2\n    img_size      = (224, 224)\n    epochs        = 20\n    lr            = 2e-3\n    scheduler     = 'CosineAnnealingLR'\n    min_lr        = 5e-5\n    T_max         = 12\n    T_0           = 12\n    warmup_epochs = 0\n    wd            = 1e-6\n    n_accumulate  = max(1, 32//train_bs)\n    n_fold        = 5\n    fold_selected = 1\n    num_classes   = 3\n    device        = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","metadata":{"papermill":{"duration":0.136117,"end_time":"2022-05-01T14:21:55.134348","exception":false,"start_time":"2022-05-01T14:21:54.998231","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-16T10:54:22.947984Z","iopub.execute_input":"2022-05-16T10:54:22.948227Z","iopub.status.idle":"2022-05-16T10:54:23.013354Z","shell.execute_reply.started":"2022-05-16T10:54:22.948184Z","shell.execute_reply":"2022-05-16T10:54:23.012401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed = 42):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    print('> SEEDING DONE')\n    \nset_seed(CFG.seed)","metadata":{"papermill":{"duration":0.067447,"end_time":"2022-05-01T14:21:55.253013","exception":false,"start_time":"2022-05-01T14:21:55.185566","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-16T10:54:23.015616Z","iopub.execute_input":"2022-05-16T10:54:23.02159Z","iopub.status.idle":"2022-05-16T10:54:23.049015Z","shell.execute_reply.started":"2022-05-16T10:54:23.021521Z","shell.execute_reply":"2022-05-16T10:54:23.040176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/uw-madison-gi-tract-image-segmentation/train.csv')\nprint(df.shape)","metadata":{"papermill":{"duration":0.546814,"end_time":"2022-05-01T14:21:55.852788","exception":false,"start_time":"2022-05-01T14:21:55.305974","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-16T10:54:23.050154Z","iopub.execute_input":"2022-05-16T10:54:23.050406Z","iopub.status.idle":"2022-05-16T10:54:23.64478Z","shell.execute_reply.started":"2022-05-16T10:54:23.050373Z","shell.execute_reply":"2022-05-16T10:54:23.644095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.rename(columns = {'class':'class_name'}, inplace = True)\n#--------------------------------------------------------------------------\ndf[\"case\"] = df[\"id\"].apply(lambda x: int(x.split(\"_\")[0].replace(\"case\", \"\")))\ndf[\"day\"] = df[\"id\"].apply(lambda x: int(x.split(\"_\")[1].replace(\"day\", \"\")))\ndf[\"slice\"] = df[\"id\"].apply(lambda x: x.split(\"_\")[3])\n#--------------------------------------------------------------------------\nTRAIN_DIR=\"../input/uw-madison-gi-tract-image-segmentation/train\"\nall_train_images = glob(os.path.join(TRAIN_DIR, \"**\", \"*.png\"), recursive=True)\nx = all_train_images[0].rsplit(\"/\", 4)[0] ## ../input/uw-madison-gi-tract-image-segmentation/train\n\npath_partial_list = []\nfor i in range(0, df.shape[0]):\n    path_partial_list.append(os.path.join(x,\n                          \"case\"+str(df[\"case\"].values[i]),\n                          \"case\"+str(df[\"case\"].values[i])+\"_\"+ \"day\"+str(df[\"day\"].values[i]),\n                          \"scans\",\n                          \"slice_\"+str(df[\"slice\"].values[i])))\ndf[\"path_partial\"] = path_partial_list\n#--------------------------------------------------------------------------\npath_partial_list = []\nfor i in range(0, len(all_train_images)):\n    path_partial_list.append(str(all_train_images[i].rsplit(\"_\",4)[0]))\n    \ntmp_df = pd.DataFrame()\ntmp_df['path_partial'] = path_partial_list\ntmp_df['path'] = all_train_images\n\n#--------------------------------------------------------------------------\ndf = df.merge(tmp_df, on=\"path_partial\").drop(columns=[\"path_partial\"])\n#--------------------------------------------------------------------------\ndf[\"width\"] = df[\"path\"].apply(lambda x: int(x[:-4].rsplit(\"_\",4)[1]))\ndf[\"height\"] = df[\"path\"].apply(lambda x: int(x[:-4].rsplit(\"_\",4)[2]))\n#--------------------------------------------------------------------------\ndel x,path_partial_list,tmp_df\n#--------------------------------------------------------------------------\ndf.head(5)\n","metadata":{"papermill":{"duration":8.127953,"end_time":"2022-05-01T14:22:04.034952","exception":false,"start_time":"2022-05-01T14:21:55.906999","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-16T10:54:23.649295Z","iopub.execute_input":"2022-05-16T10:54:23.649545Z","iopub.status.idle":"2022-05-16T10:54:31.366821Z","shell.execute_reply.started":"2022-05-16T10:54:23.649513Z","shell.execute_reply":"2022-05-16T10:54:31.366147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# RESTRUCTURE  DATAFRAME\ndf_train = pd.DataFrame({'id':df['id'][::3]})\n\ndf_train['large_bowel'] = df['segmentation'][::3].values\ndf_train['small_bowel'] = df['segmentation'][1::3].values\ndf_train['stomach'] = df['segmentation'][2::3].values\n\ndf_train['path'] = df['path'][::3].values\ndf_train['case'] = df['case'][::3].values\ndf_train['day'] = df['day'][::3].values\ndf_train['slice'] = df['slice'][::3].values\ndf_train['width'] = df['width'][::3].values\ndf_train['height'] = df['height'][::3].values\n\n\ndf_train.reset_index(inplace=True,drop=True)\ndf_train.fillna('',inplace=True); \ndf_train['count'] = np.sum(df_train.iloc[:,1:4]!='',axis=1).values\ndf_train.sample(5)","metadata":{"papermill":{"duration":0.137681,"end_time":"2022-05-01T14:22:04.227564","exception":false,"start_time":"2022-05-01T14:22:04.089883","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-16T10:54:31.368138Z","iopub.execute_input":"2022-05-16T10:54:31.368383Z","iopub.status.idle":"2022-05-16T10:54:31.453043Z","shell.execute_reply.started":"2022-05-16T10:54:31.36835Z","shell.execute_reply":"2022-05-16T10:54:31.452209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{"papermill":{"duration":0.053448,"end_time":"2022-05-01T14:22:04.335224","exception":false,"start_time":"2022-05-01T14:22:04.281776","status":"completed"},"tags":[]}},{"cell_type":"code","source":"df_train['count'].value_counts().plot.bar()","metadata":{"execution":{"iopub.status.busy":"2022-05-16T10:54:31.454543Z","iopub.execute_input":"2022-05-16T10:54:31.454818Z","iopub.status.idle":"2022-05-16T10:54:32.955829Z","shell.execute_reply.started":"2022-05-16T10:54:31.454781Z","shell.execute_reply":"2022-05-16T10:54:32.95516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,4))\nbar = plt.bar([1,2,3],100*np.mean( df_train.iloc[:,1:4]!='',axis=0))\nplt.title('Percent Training Images with Mask', fontsize=16)\nplt.ylabel('Percent of Images'); plt.xlabel('Class Type')\nplt.xticks([1,2,3])\nlabels=[\"large bowel\",\"small bowel\",\"stomach\"]\nfor rect,lbl in zip(bar,labels):\n    height = rect.get_height()\n    plt.text(rect.get_x() + rect.get_width()/3, height,  lbl,\n             ha='center', va='bottom',fontsize=16)\n    plt.text(rect.get_x() + rect.get_width()/1.3, height, '%.1f %%' % height,\n             ha='center', va='bottom',fontsize=13)\n\nplt.ylim((0,50)); plt.show()","metadata":{"papermill":{"duration":0.308924,"end_time":"2022-05-01T14:22:04.697458","exception":false,"start_time":"2022-05-01T14:22:04.388534","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-16T10:54:32.957194Z","iopub.execute_input":"2022-05-16T10:54:32.957622Z","iopub.status.idle":"2022-05-16T10:54:33.175686Z","shell.execute_reply.started":"2022-05-16T10:54:32.957585Z","shell.execute_reply":"2022-05-16T10:54:33.174996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# RLE","metadata":{"papermill":{"duration":0.057618,"end_time":"2022-05-01T14:22:04.811513","exception":false,"start_time":"2022-05-01T14:22:04.753895","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ref: https://www.kaggle.com/paulorzp/run-length-encode-and-decode\ndef rle_decode(mask_rle, shape):\n    '''\n    mask_rle: run-length as string formated (start length)\n    shape: (height,width) of array to return \n    Returns numpy array, 1 - mask, 0 - background\n\n    '''\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape(shape)  # Needed to align to RLE direction\n\n\n# ref.: https://www.kaggle.com/stainsby/fast-tested-rle\ndef rle_encode(img):\n    '''\n    img: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    '''\n    pixels = img.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)\n\n\ndef show_img(img, mask=None):\n    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n    plt.imshow(img, cmap='bone')\n    \n    if mask is not None:\n        plt.imshow(mask, alpha=0.5)\n        handles = [Rectangle((0,0),1,1, color=_c) for _c in [(0.667,0.0,0.0), (0.0,0.667,0.0), (0.0,0.0,0.667)]]\n        labels = [\"Large Bowel\", \"Small Bowel\", \"Stomach\"]\n        plt.legend(handles,labels)\n    plt.axis('off')","metadata":{"papermill":{"duration":0.071865,"end_time":"2022-05-01T14:22:04.941636","exception":false,"start_time":"2022-05-01T14:22:04.869771","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-16T10:54:33.178833Z","iopub.execute_input":"2022-05-16T10:54:33.179079Z","iopub.status.idle":"2022-05-16T10:54:33.190398Z","shell.execute_reply.started":"2022-05-16T10:54:33.179045Z","shell.execute_reply":"2022-05-16T10:54:33.189298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Folds","metadata":{"papermill":{"duration":0.053777,"end_time":"2022-05-01T14:22:05.050418","exception":false,"start_time":"2022-05-01T14:22:04.996641","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# SAMPLES\ntrain_mask = list(df_train[df_train['large_bowel']!=''].index)\ntrain_mask += list(df_train[df_train['small_bowel']!=''].index)\ntrain_mask += list(df_train[df_train['stomach']!=''].index)\n\ndf_train=df_train[df_train.index.isin(train_mask)]     \ndf_train.reset_index(inplace=True,drop=True)\nprint(df_train.shape)","metadata":{"papermill":{"duration":0.104198,"end_time":"2022-05-01T14:22:05.210468","exception":false,"start_time":"2022-05-01T14:22:05.10627","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-16T10:54:33.192166Z","iopub.execute_input":"2022-05-16T10:54:33.192573Z","iopub.status.idle":"2022-05-16T10:54:33.246465Z","shell.execute_reply.started":"2022-05-16T10:54:33.192534Z","shell.execute_reply":"2022-05-16T10:54:33.245676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skf = StratifiedGroupKFold(n_splits=CFG.n_fold, shuffle=True, random_state=42)\nfor fold, (_, val_idx) in enumerate(skf.split(X=df_train, y=df_train['count'],groups =df_train['case']), 1):\n    df_train.loc[val_idx, 'fold'] = fold\n    \ndf_train['fold'] = df_train['fold'].astype(np.uint8)\n\ntrain_ids = df_train[df_train[\"fold\"]!=CFG.fold_selected].index\nvalid_ids = df_train[df_train[\"fold\"]==CFG.fold_selected].index\n\ndf_train.groupby('fold').size()","metadata":{"papermill":{"duration":0.180251,"end_time":"2022-05-01T14:22:05.445752","exception":false,"start_time":"2022-05-01T14:22:05.265501","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-16T10:54:33.247955Z","iopub.execute_input":"2022-05-16T10:54:33.248233Z","iopub.status.idle":"2022-05-16T10:54:33.355586Z","shell.execute_reply.started":"2022-05-16T10:54:33.248199Z","shell.execute_reply":"2022-05-16T10:54:33.3548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(df_train.groupby(['fold','count'])['id'].count())","metadata":{"papermill":{"duration":0.07296,"end_time":"2022-05-01T14:22:05.57516","exception":false,"start_time":"2022-05-01T14:22:05.5022","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-16T10:54:33.356995Z","iopub.execute_input":"2022-05-16T10:54:33.357254Z","iopub.status.idle":"2022-05-16T10:54:33.37064Z","shell.execute_reply.started":"2022-05-16T10:54:33.357218Z","shell.execute_reply":"2022-05-16T10:54:33.369973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{"papermill":{"duration":0.056645,"end_time":"2022-05-01T14:22:05.689182","exception":false,"start_time":"2022-05-01T14:22:05.632537","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class BuildDataset(torch.utils.data.Dataset):\n    def __init__(self, df, subset=\"train\", transforms=None):\n        self.df = df\n        self.subset = subset\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.df)\n    \n    \n    def __getitem__(self, index): \n        masks = np.zeros((CFG.img_size[0],CFG.img_size[1], 3), dtype=np.float32)\n        id_ = self.df['id'].iloc[index]\n        img_path=self.df['path'].iloc[index]\n        w=self.df['width'].iloc[index]\n        h=self.df['height'].iloc[index]\n        img = self.__load_img(img_path)\n        if self.subset == 'train':\n            for k,j in zip([0,1,2],[\"large_bowel\",\"small_bowel\",\"stomach\"]):\n                rles=self.df[j].iloc[index]\n                mask = rle_decode(rles, shape=(h, w, 1))\n                mask = cv2.resize(mask, CFG.img_size)\n                masks[:,:,k] = mask\n        \n        masks = masks.transpose(2, 0, 1)\n        img = img.transpose(2, 0, 1)\n\n        if self.subset == 'train': return torch.tensor(img), torch.tensor(masks)\n        else: return torch.tensor(img), id_, h, w\n        \n\n    def __load_img(self, img_path):\n        img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n        img = (img - img.min())/(img.max() - img.min())*255.0 \n        img = cv2.resize(img, CFG.img_size)\n        img = np.tile(img[...,None], [1, 1, 3]) # gray to rgb\n        img = img.astype(np.float32) /255.\n        return img\n    \n\n","metadata":{"papermill":{"duration":0.079661,"end_time":"2022-05-01T14:22:05.825296","exception":false,"start_time":"2022-05-01T14:22:05.745635","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-16T10:54:33.373447Z","iopub.execute_input":"2022-05-16T10:54:33.373675Z","iopub.status.idle":"2022-05-16T10:54:33.387078Z","shell.execute_reply.started":"2022-05-16T10:54:33.373639Z","shell.execute_reply":"2022-05-16T10:54:33.38625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Augmentations","metadata":{"papermill":{"duration":0.058218,"end_time":"2022-05-01T14:22:05.941496","exception":false,"start_time":"2022-05-01T14:22:05.883278","status":"completed"},"tags":[]}},{"cell_type":"code","source":"data_transforms = {\n    \"train\": A.Compose([\n        A.Resize(*CFG.img_size, interpolation=cv2.INTER_NEAREST),\n        A.HorizontalFlip(),\n        A.VerticalFlip(),\n        A.OneOf([\n                A.RandomContrast(),\n                A.RandomGamma(),\n                A.RandomBrightness(),\n                ], p=0.2),\n\n        ], p=1.0),\n    \"valid\": A.Compose([\n        A.Resize(*CFG.img_size, interpolation=cv2.INTER_NEAREST),\n        ], p=1.0)\n}","metadata":{"_kg_hide-output":true,"papermill":{"duration":0.0678,"end_time":"2022-05-01T14:22:06.066101","exception":false,"start_time":"2022-05-01T14:22:05.998301","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-16T10:54:33.388334Z","iopub.execute_input":"2022-05-16T10:54:33.38922Z","iopub.status.idle":"2022-05-16T10:54:33.397049Z","shell.execute_reply.started":"2022-05-16T10:54:33.38918Z","shell.execute_reply":"2022-05-16T10:54:33.396196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DataLoader","metadata":{"papermill":{"duration":0.056669,"end_time":"2022-05-01T14:22:06.179941","exception":false,"start_time":"2022-05-01T14:22:06.123272","status":"completed"},"tags":[]}},{"cell_type":"code","source":"\ntrain_dataset = BuildDataset(df_train, transforms=data_transforms['train'])\nvalid_dataset = BuildDataset(df_train[df_train.index.isin(valid_ids)], transforms=data_transforms['valid'])\n\ntrain_loader = DataLoader(train_dataset, batch_size=CFG.train_bs, num_workers=4, shuffle=True, pin_memory=True, drop_last=False)\nvalid_loader = DataLoader(valid_dataset, batch_size=CFG.valid_bs,num_workers=4, shuffle=False, pin_memory=True)\n    \n\nimgs, msks = next(iter(train_loader))\nimgs.size(), msks.size()","metadata":{"papermill":{"duration":7.702453,"end_time":"2022-05-01T14:22:13.938855","exception":false,"start_time":"2022-05-01T14:22:06.236402","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-16T10:54:33.398534Z","iopub.execute_input":"2022-05-16T10:54:33.399109Z","iopub.status.idle":"2022-05-16T10:54:38.1065Z","shell.execute_reply.started":"2022-05-16T10:54:33.399049Z","shell.execute_reply":"2022-05-16T10:54:38.105794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Visualization","metadata":{"papermill":{"duration":0.078956,"end_time":"2022-05-01T14:22:14.076464","exception":false,"start_time":"2022-05-01T14:22:13.997508","status":"completed"},"tags":[]}},{"cell_type":"code","source":"imgs, msks = next(iter(train_loader))\nimgs.size(), msks.size()\n\ndef plot_batch(imgs, msks, size=3):\n    plt.figure(figsize=(5*5, 5))\n    for idx in range(size):\n        plt.subplot(1, 5, idx+1)\n        img = imgs[idx,].permute((1, 2, 0)).numpy()\n        msk = msks[idx,].permute((1, 2, 0)).numpy()\n#         for midx in [0, 1, 2]:\n#             print(mask2rle(msk[...,midx]))\n        show_img(img, msk)\n    plt.tight_layout()\n    plt.show()\n\nplot_batch(imgs, msks, size=5)","metadata":{"_kg_hide-input":true,"papermill":{"duration":3.628283,"end_time":"2022-05-01T14:22:17.817956","exception":false,"start_time":"2022-05-01T14:22:14.189673","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-16T10:54:38.109427Z","iopub.execute_input":"2022-05-16T10:54:38.109638Z","iopub.status.idle":"2022-05-16T10:54:40.569524Z","shell.execute_reply.started":"2022-05-16T10:54:38.109611Z","shell.execute_reply":"2022-05-16T10:54:40.568746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"papermill":{"duration":0.343671,"end_time":"2022-05-01T14:22:18.227067","exception":false,"start_time":"2022-05-01T14:22:17.883396","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-16T10:54:40.570597Z","iopub.execute_input":"2022-05-16T10:54:40.570868Z","iopub.status.idle":"2022-05-16T10:54:40.755278Z","shell.execute_reply.started":"2022-05-16T10:54:40.570827Z","shell.execute_reply":"2022-05-16T10:54:40.754582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# UNet Model\n","metadata":{"papermill":{"duration":0.065659,"end_time":"2022-05-01T14:22:18.364383","exception":false,"start_time":"2022-05-01T14:22:18.298724","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import torch.nn as nn\n\ndef conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=dilation, groups=groups, bias=False, dilation=dilation).cuda()\n\n\ndef conv1x1(in_planes, out_planes, stride=1):\n    \"\"\"1x1 convolution\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False).cuda()\n\ndef upsample(scale_factor, mode='bilinear', align_corners=True):\n    return nn.Upsample(scale_factor=scale_factor, mode=mode, align_corners=align_corners).cuda()\n\ndef convrelu(in_channels, out_channels, kernel, padding):\n    return nn.Sequential(\n        nn.Conv2d(in_channels, out_channels, kernel, padding=padding),\n        nn.ReLU(inplace=True)\n    ).cuda()\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n                 base_width=64, dilation=1, norm_layer=None):\n        super(BasicBlock, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        if groups != 1 or base_width != 64:\n            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n        if dilation > 1:\n            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = norm_layer(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = norm_layer(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n    # This variant is also known as ResNet V1.5 and improves accuracy according to\n    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n                 base_width=64, dilation=1, norm_layer=None):\n        super(Bottleneck, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        width = int(planes * (base_width / 64.)) * groups\n        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv1x1(inplanes, width)\n        self.bn1 = norm_layer(width)\n        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n        self.bn2 = norm_layer(width)\n        self.conv3 = conv1x1(width, planes * self.expansion)\n        self.bn3 = norm_layer(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self, block, layers, num_classes=3, zero_init_residual=False,\n                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n                 norm_layer=None):\n        super(ResNet, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        self._norm_layer = norm_layer\n\n        self.inplanes = 64\n        self.dilation = 1\n        if replace_stride_with_dilation is None:\n            # each element in the tuple indicates if we should replace\n            # the 2x2 stride with a dilated convolution instead\n            replace_stride_with_dilation = [False, False, False]\n        if len(replace_stride_with_dilation) != 3:\n            raise ValueError(\"replace_stride_with_dilation should be None \"\n                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n        self.groups = groups\n        self.base_width = width_per_group\n        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = norm_layer(self.inplanes)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n#         self.layer0 = nn.Sequential(\n#             nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False),\n#             norm_layer(self.inplanes),\n#             nn.ReLU(inplace=True),\n#             nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n#         )\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n                                       dilate=replace_stride_with_dilation[0])\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n                                       dilate=replace_stride_with_dilation[1])\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n                                       dilate=replace_stride_with_dilation[2])\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n        # Zero-initialize the last BN in each residual branch,\n        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n        if zero_init_residual:\n            for m in self.modules():\n                if isinstance(m, Bottleneck):\n                    nn.init.constant_(m.bn3.weight, 0)\n                elif isinstance(m, BasicBlock):\n                    nn.init.constant_(m.bn2.weight, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n        norm_layer = self._norm_layer\n        downsample = None\n        previous_dilation = self.dilation\n        if dilate:\n            self.dilation *= stride\n            stride = 1\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                conv1x1(self.inplanes, planes * block.expansion, stride),\n                norm_layer(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n                            self.base_width, previous_dilation, norm_layer))\n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes, groups=self.groups,\n                                base_width=self.base_width, dilation=self.dilation,\n                                norm_layer=norm_layer))\n\n        return nn.Sequential(*layers)\n\n    def _forward_impl(self, x):\n        # See note [TorchScript super()]\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n#         x = self.layer0(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n\n        return x\n\n    def forward(self, x):\n        return self._forward_impl(x)\n\n\ndef _resnet(arch, block, layers, **kwargs):\n    model = ResNet(block, layers, **kwargs)\n    return model\n\n\ndef resnet18(progress=True, **kwargs):\n    r\"\"\"ResNet-18 model from\n    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    \"\"\"\n    return _resnet('resnet18', BasicBlock, [2, 2, 2, 2],\n                   **kwargs)\n\ndef resnet50(progress=True, **kwargs):\n    r\"\"\"ResNet-50 model from\n    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    \"\"\"\n    return _resnet('resnet50', Bottleneck, [3, 4, 6, 3],\n                   **kwargs)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T10:54:40.756707Z","iopub.execute_input":"2022-05-16T10:54:40.757116Z","iopub.status.idle":"2022-05-16T10:54:40.79981Z","shell.execute_reply.started":"2022-05-16T10:54:40.757076Z","shell.execute_reply":"2022-05-16T10:54:40.799028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# UNet resnet18\ndef uBasicBlock(in_channels, out_channels, k, p):\n    return nn.Sequential(\n        nn.Conv2d(in_channels, out_channels, kernel_size=k, padding=p),\n        nn.ReLU(inplace=True)\n    )\nclass UNet(nn.Module):\n    def __init__(self, num_classes=3):\n        super(UNet, self).__init__()\n        self.n_class = num_classes\n        \n        self.base_model = resnet18()\n        self.base_layers = list(self.base_model.children())\n\n        self.down0 = nn.Sequential(*self.base_layers[:3]) # size=(N, 64, x.H/2, x.W/2)\n        self.out0 = uBasicBlock(64, 64, 1, 0)\n        self.down1 = nn.Sequential(*self.base_layers[3:5]) # size=(N, 128, x.H/4, x.W/4)\n        self.out1 = uBasicBlock(64, 64, 1, 0)\n        self.down2 = self.base_layers[5]  # size=(N, 256, x.H/8, x.W/8)\n        self.out2 = uBasicBlock(128, 128, 1, 0)\n        self.down3 = self.base_layers[6]  # size=(N, 512, x.H/16, x.W/16)\n        self.out3 = uBasicBlock(256, 256, 1, 0)\n        self.down4 = self.base_layers[7]  # size=(N, 512, x.H/32, x.W/32)\n        self.out4 = uBasicBlock(512, 512, 1, 0)\n\n        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n\n        self.up3 = uBasicBlock(256 + 512, 512, 3, 1)\n        self.up2 = uBasicBlock(128 + 512, 256, 3, 1)\n        self.up1 = uBasicBlock(64 + 256, 256, 3, 1)\n        self.up0 = uBasicBlock(64 + 256, 128, 3, 1)\n\n        self.ori2 = uBasicBlock(64 + 128, 64, 3, 1)\n        self.ori1 = uBasicBlock(64, 64, 3, 1)\n        self.ori0 = uBasicBlock(3, 64, 3, 1)\n\n        self.last = nn.Conv2d(64, self.n_class, 1)\n\n    def forward(self, x):\n        ori = self.ori0(x)\n        ori = self.ori1(ori)\n        down0 = self.down0(x)\n        down1 = self.down1(down0)\n        down2 = self.down2(down1)\n        down3 = self.down3(down2)\n        down4 = self.down4(down3)\n        out4 = self.out4(down4)\n        x = self.upsample(out4)\n        out3 = self.out3(down3)\n        x = torch.cat([x, out3], dim=1)\n        x = self.up3(x)\n        x = self.upsample(x)\n        out2 = self.out2(down2)\n        x = torch.cat([x, out2], dim=1)\n        x = self.up2(x)\n        x = self.upsample(x)\n        out1 = self.out1(down1)\n        x = torch.cat([x, out1], dim=1)\n        x = self.up1(x)\n        x = self.upsample(x)\n        out0 = self.out0(down0)\n        x = torch.cat([x, out0], dim=1)\n        x = self.up0(x)\n        x = self.upsample(x)\n        x = torch.cat([x, ori], dim=1)\n        x = self.ori2(x)\n\n        x = self.last(x)\n        # x = torch.softmax(x, dim=1)\n        return x","metadata":{"papermill":{"duration":0.092656,"end_time":"2022-05-01T14:22:18.520966","exception":false,"start_time":"2022-05-01T14:22:18.42831","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-16T10:54:40.801197Z","iopub.execute_input":"2022-05-16T10:54:40.801461Z","iopub.status.idle":"2022-05-16T10:54:40.822141Z","shell.execute_reply.started":"2022-05-16T10:54:40.801426Z","shell.execute_reply":"2022-05-16T10:54:40.821477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = UNet(num_classes=CFG.num_classes).cuda()\nimg = torch.randn(1, 3, 224, 224).cuda()\nout = model(img)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T10:54:40.823502Z","iopub.execute_input":"2022-05-16T10:54:40.82394Z","iopub.status.idle":"2022-05-16T10:54:47.670071Z","shell.execute_reply.started":"2022-05-16T10:54:40.823903Z","shell.execute_reply":"2022-05-16T10:54:47.669335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ðŸ”§ Loss Function","metadata":{"papermill":{"duration":0.065476,"end_time":"2022-05-01T14:22:18.791235","exception":false,"start_time":"2022-05-01T14:22:18.725759","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# class DiceLoss(nn.Module):\n#     def __init__(self):\n#         super(DiceLoss, self).__init__()\n\n#     def forward(self, inputs, targets, smooth=1e-5):\n        \n#         #comment out if your model contains a sigmoid or equivalent activation layer\n#         inputs = F.sigmoid(inputs)       \n        \n#         #flatten label and prediction tensors\n#         inputs = inputs.view(-1)\n#         targets = targets.view(-1)\n        \n#         intersection = (inputs * targets).sum()                            \n#         dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n        \n#         return 1 - dice","metadata":{"execution":{"iopub.status.busy":"2022-05-16T10:54:47.671437Z","iopub.execute_input":"2022-05-16T10:54:47.671705Z","iopub.status.idle":"2022-05-16T10:54:47.675404Z","shell.execute_reply.started":"2022-05-16T10:54:47.671655Z","shell.execute_reply":"2022-05-16T10:54:47.674733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def FocalLoss(\n#     inputs: torch.Tensor,\n#     targets: torch.Tensor,\n#     alpha: float = 0.25,\n#     gamma: float = 2,\n#     reduction: str = \"sum\",\n# ):\n#     p = torch.sigmoid(inputs)\n#     ce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction=reduction)\n#     p_t = p * targets + (1 - p) * (1 - targets)\n#     loss = ce_loss * ((1 - p_t) ** gamma)\n\n#     if alpha >= 0:\n#         alpha_t = alpha * targets + (1 - alpha) * (1 - targets)\n#         loss = alpha_t * loss\n\n#     if reduction == \"mean\":\n#         loss = loss.mean()\n#     elif reduction == \"sum\":\n#         loss = loss.sum()\n\n#     return loss","metadata":{"execution":{"iopub.status.busy":"2022-05-16T10:54:47.676629Z","iopub.execute_input":"2022-05-16T10:54:47.677045Z","iopub.status.idle":"2022-05-16T10:54:47.686993Z","shell.execute_reply.started":"2022-05-16T10:54:47.677006Z","shell.execute_reply":"2022-05-16T10:54:47.686263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# DiceLoss    = DiceLoss()\nBCELoss     = torch.nn.BCEWithLogitsLoss()\n\ndef dice_coef(y_true, y_pred, thr=0.5, dim=(2,3), epsilon=0.001):\n    y_true = y_true.to(torch.float32)\n    y_pred = (y_pred>thr).to(torch.float32)\n    inter = (y_true*y_pred).sum(dim=dim)\n    den = y_true.sum(dim=dim) + y_pred.sum(dim=dim)\n    dice = ((2*inter+epsilon)/(den+epsilon)).mean(dim=(1,0))\n    return dice\n\ndef iou_coef(y_true, y_pred, thr=0.5, dim=(2,3), epsilon=0.001):\n    y_true = y_true.to(torch.float32)\n    y_pred = (y_pred>thr).to(torch.float32)\n    inter = (y_true*y_pred).sum(dim=dim)\n    union = (y_true + y_pred - y_true*y_pred).sum(dim=dim)\n    iou = ((inter+epsilon)/(union+epsilon)).mean(dim=(1,0))\n    return iou\n\ndef criterion(y_pred, y_true):\n    return BCELoss(y_pred, y_true)","metadata":{"papermill":{"duration":0.082752,"end_time":"2022-05-01T14:22:18.940904","exception":false,"start_time":"2022-05-01T14:22:18.858152","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-16T10:54:47.688238Z","iopub.execute_input":"2022-05-16T10:54:47.688684Z","iopub.status.idle":"2022-05-16T10:54:47.698657Z","shell.execute_reply.started":"2022-05-16T10:54:47.688634Z","shell.execute_reply":"2022-05-16T10:54:47.698018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Function","metadata":{"papermill":{"duration":0.099375,"end_time":"2022-05-01T14:22:19.108356","exception":false,"start_time":"2022-05-01T14:22:19.008981","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n    model.train()\n    model.to(device)\n    scaler = amp.GradScaler()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    \n    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc='Train ')\n    for step, (images, masks) in pbar:         \n        images = images.to(device, dtype=torch.float)\n        masks  = masks.to(device, dtype=torch.float)\n        \n        batch_size = images.size(0)\n        \n        with amp.autocast(enabled=True):\n            y_pred = model(images)\n            loss   = criterion(y_pred, masks)\n            loss   = loss / CFG.n_accumulate\n            \n        scaler.scale(loss).backward()\n    \n        if (step + 1) % CFG.n_accumulate == 0:\n            scaler.step(optimizer)\n            scaler.update()\n\n            # zero the parameter gradients\n            optimizer.zero_grad()\n                \n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n        \n        epoch_loss = running_loss / dataset_size\n        \n        mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0\n        current_lr = optimizer.param_groups[0]['lr']\n        pbar.set_postfix(train_loss=f'{epoch_loss:0.4f}',\n                        lr=f'{current_lr:0.5f}',\n                        gpu_mem=f'{mem:0.2f} GB')\n    if scheduler is not None:\n        scheduler.step()\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    return epoch_loss","metadata":{"papermill":{"duration":0.137479,"end_time":"2022-05-01T14:22:19.392561","exception":false,"start_time":"2022-05-01T14:22:19.255082","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-16T10:54:47.699817Z","iopub.execute_input":"2022-05-16T10:54:47.700205Z","iopub.status.idle":"2022-05-16T10:54:47.712132Z","shell.execute_reply.started":"2022-05-16T10:54:47.700168Z","shell.execute_reply":"2022-05-16T10:54:47.711466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Validation Function","metadata":{"papermill":{"duration":0.110867,"end_time":"2022-05-01T14:22:19.616474","exception":false,"start_time":"2022-05-01T14:22:19.505607","status":"completed"},"tags":[]}},{"cell_type":"code","source":"@torch.no_grad()\ndef valid_one_epoch(model, dataloader, device, epoch):\n    model.eval()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    \n    val_scores = []\n    \n    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc='Valid ')\n    for step, (images, masks) in pbar:        \n        images  = images.to(device, dtype=torch.float)\n        masks   = masks.to(device, dtype=torch.float)\n        \n        batch_size = images.size(0)\n        \n        y_pred  = model(images)\n        loss    = criterion(y_pred, masks)\n        \n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n        \n        epoch_loss = running_loss / dataset_size\n        \n        y_pred = nn.Sigmoid()(y_pred)\n        val_dice = dice_coef(masks, y_pred).cpu().detach().numpy()\n        val_jaccard = iou_coef(masks, y_pred).cpu().detach().numpy()\n        val_scores.append([val_dice, val_jaccard])\n        \n        mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0\n        current_lr = optimizer.param_groups[0]['lr']\n        pbar.set_postfix(valid_loss=f'{epoch_loss:0.4f}',\n                        lr=f'{current_lr:0.5f}',\n                        gpu_memory=f'{mem:0.2f} GB')\n    val_scores  = np.mean(val_scores, axis=0)\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    return epoch_loss, val_scores","metadata":{"papermill":{"duration":0.081551,"end_time":"2022-05-01T14:22:19.808831","exception":false,"start_time":"2022-05-01T14:22:19.72728","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-16T10:54:47.713255Z","iopub.execute_input":"2022-05-16T10:54:47.713658Z","iopub.status.idle":"2022-05-16T10:54:47.725086Z","shell.execute_reply.started":"2022-05-16T10:54:47.713621Z","shell.execute_reply":"2022-05-16T10:54:47.724313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{"papermill":{"duration":0.065922,"end_time":"2022-05-01T14:22:19.942521","exception":false,"start_time":"2022-05-01T14:22:19.876599","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def run_training(model, optimizer, scheduler, device, num_epochs):\n    # To automatically log gradients\n\n    if torch.cuda.is_available():\n        print(\"cuda: {}\\n\".format(torch.cuda.get_device_name()))\n    \n    start = time.time()\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_dice      = -np.inf\n    best_epoch     = -1\n    history = defaultdict(list)\n    \n    for epoch in range(1, num_epochs + 1): \n        gc.collect()\n        print(f'Epoch {epoch}/{num_epochs}', end='')\n        train_loss = train_one_epoch(model, optimizer, scheduler, \n                                           dataloader=train_loader, \n                                           device=CFG.device, epoch=epoch)\n        \n        val_loss, val_scores = valid_one_epoch(model, valid_loader, \n                                                 device=CFG.device, \n                                                 epoch=epoch)\n        val_dice, val_jaccard = val_scores\n    \n        history['Train Loss'].append(train_loss)\n        history['Valid Loss'].append(val_loss)\n        history['Valid Dice'].append(val_dice)\n        history['Valid Jaccard'].append(val_jaccard)\n        \n        # Log the metrics\n\n        \n        print(f'Valid Dice: {val_dice:0.4f} | Valid Jaccard: {val_jaccard:0.4f}')\n        \n        # deep copy the model\n        if val_dice >= best_dice:\n            print(f\"{c_}Valid Score Improved ({best_dice:0.4f} ---> {val_dice:0.4f})\")\n            best_dice    = val_dice\n            best_jaccard = val_jaccard\n            best_epoch   = epoch\n            #run.summary[\"Best Dice\"]    = best_dice\n           # run.summary[\"Best Jaccard\"] = best_jaccard\n           # run.summary[\"Best Epoch\"]   = best_epoch\n            best_model_wts = copy.deepcopy(model.state_dict())\n            PATH = f\"best_epoch-{fold:02d}.bin\"\n            torch.save(model.state_dict(), PATH)\n            # Save a model file from the current directory\n            print(f\"Model Saved{sr_}\")\n            \n        last_model_wts = copy.deepcopy(model.state_dict())\n        PATH = f\"last_epoch-{fold:02d}.bin\"\n        torch.save(model.state_dict(), PATH)\n            \n        print(); print()\n    \n    end = time.time()\n    time_elapsed = end - start\n    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n        time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n    print(\"Best Score: {:.4f}\".format(best_jaccard))\n    \n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    \n    return model, history","metadata":{"papermill":{"duration":0.081613,"end_time":"2022-05-01T14:22:20.088511","exception":false,"start_time":"2022-05-01T14:22:20.006898","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-16T10:54:47.726341Z","iopub.execute_input":"2022-05-16T10:54:47.726748Z","iopub.status.idle":"2022-05-16T10:54:47.740875Z","shell.execute_reply.started":"2022-05-16T10:54:47.72671Z","shell.execute_reply":"2022-05-16T10:54:47.740222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fetch_scheduler(optimizer):\n    if CFG.scheduler == 'CosineAnnealingLR':\n        scheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=CFG.T_max, \n                                                   eta_min=CFG.min_lr)\n    elif CFG.scheduler == 'CosineAnnealingWarmRestarts':\n        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=CFG.T_0, \n                                                             eta_min=CFG.min_lr)\n    elif CFG.scheduler == 'ReduceLROnPlateau':\n        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer,\n                                                   mode='min',\n                                                   factor=0.1,\n                                                   patience=7,\n                                                   threshold=0.0001,\n                                                   min_lr=CFG.min_lr,)\n    elif CFG.scheduer == 'ExponentialLR':\n        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.85)\n    elif CFG.scheduler == None:\n        return None\n        \n    return scheduler","metadata":{"papermill":{"duration":0.075722,"end_time":"2022-05-01T14:22:20.229967","exception":false,"start_time":"2022-05-01T14:22:20.154245","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-16T10:54:47.74687Z","iopub.execute_input":"2022-05-16T10:54:47.74726Z","iopub.status.idle":"2022-05-16T10:54:47.754927Z","shell.execute_reply.started":"2022-05-16T10:54:47.747232Z","shell.execute_reply":"2022-05-16T10:54:47.75424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Start Training","metadata":{"papermill":{"duration":0.065997,"end_time":"2022-05-01T14:22:20.65523","exception":false,"start_time":"2022-05-01T14:22:20.589233","status":"completed"},"tags":[]}},{"cell_type":"code","source":"for fold in range(1):\n    print(f'#'*35)\n    print(f'######### Fold: {fold}')\n    print(f'#'*35)\n    model = UNet(num_classes=CFG.num_classes)\n#     model.load_state_dict(torch.load('./best_epoch-00.bin'))\n    optimizer = optim.Adam(model.parameters(), lr=CFG.lr, weight_decay=CFG.wd)\n    scheduler = fetch_scheduler(optimizer)\n    model, history = run_training(model, optimizer, scheduler,\n                                  device=CFG.device,\n                                  num_epochs=CFG.epochs)\n    ","metadata":{"_kg_hide-output":true,"papermill":{"duration":5604.638418,"end_time":"2022-05-01T15:55:45.360588","exception":false,"start_time":"2022-05-01T14:22:20.72217","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-16T10:54:47.755818Z","iopub.execute_input":"2022-05-16T10:54:47.758852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loss, Dice,Jaccard Curves","metadata":{}},{"cell_type":"code","source":"# PLOT TRAINING\nplt.figure(figsize=(15,5))\nplt.plot(range(CFG.epochs),history['Valid Dice'],label='Valid Dice')\nplt.plot(range(CFG.epochs),history['Valid Jaccard'],label='Valid Jaccard')\nplt.title('Dice & Jaccard'); plt.xlabel('Epoch'); plt.ylabel('');plt.legend(); \nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PLOT TRAINING\nplt.figure(figsize=(15,5))\nplt.plot(range(CFG.epochs),history['Train Loss'],label='Train Loss')\nplt.plot(range(CFG.epochs),history['Valid Loss'],label='Valid Loss')\nplt.title('LOSS'); plt.xlabel('Epoch'); plt.ylabel('loss');plt.legend(); \nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction","metadata":{"papermill":{"duration":7.69462,"end_time":"2022-05-01T15:56:01.266413","exception":false,"start_time":"2022-05-01T15:55:53.571793","status":"completed"},"tags":[]}},{"cell_type":"code","source":"test_dataset = BuildDataset(df_train[df_train.index.isin(valid_ids)], \n                            transforms=data_transforms['valid'])\ntest_loader  = DataLoader(test_dataset, batch_size=5, \n                          num_workers=4, shuffle=False, pin_memory=True)\n\nimgs, msks =  next(iter(test_loader))\n\nimgs = imgs.to(CFG.device, dtype=torch.float)\n\npreds = []\nfor fold in range(1):\n    model.load_state_dict(torch.load(f\"best_epoch-{fold:02d}.bin\"))\n    model.cuda()\n    model.eval()\n    with torch.no_grad():\n        pred = model(imgs)\n        pred = (nn.Sigmoid()(pred)>0.5).double()\n    preds.append(pred)\n    \nimgs  = imgs.cpu().detach()\npreds = torch.mean(torch.stack(preds, dim=0), dim=0).cpu().detach()","metadata":{"papermill":{"duration":8.540461,"end_time":"2022-05-01T15:56:17.83046","exception":false,"start_time":"2022-05-01T15:56:09.289999","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_batch(imgs, preds, size=5)","metadata":{"papermill":{"duration":8.588373,"end_time":"2022-05-01T15:56:34.25364","exception":false,"start_time":"2022-05-01T15:56:25.665267","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_batch(imgs, msks, size=5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Computing Dice & Jaccard for Classes","metadata":{"papermill":{"duration":7.781034,"end_time":"2022-05-01T15:56:50.578607","exception":false,"start_time":"2022-05-01T15:56:42.797573","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def dice_coef_metric_per_classes(probabilities: np.ndarray,\n                                    truth: np.ndarray,\n                                    treshold: float = 0.5,\n                                    eps: float = 1e-9,\n                                    classes: list = [\"Large Bowel\", \"Small Bowel\", \"Stomach\"]) -> np.ndarray:\n    \"\"\"\n    Calculate Dice score for data batch and for each class.\n    Params:\n        probobilities: model outputs after activation function.\n        truth: model targets.\n        threshold: threshold for probabilities.\n        eps: additive to refine the estimate.\n        classes: list with name classes.\n        Returns: dict with dice scores for each class.\n    \"\"\"\n    scores = {key: list() for key in classes}\n    num = probabilities.shape[0]\n    num_classes = probabilities.shape[1]\n    predictions = (probabilities >= treshold).astype(np.float32)\n    assert(predictions.shape == truth.shape)\n\n    for i in range(num):\n        for class_ in range(num_classes):\n            prediction = predictions[i][class_]\n            truth_ = truth[i][class_]\n            intersection = 2.0 * (truth_ * prediction).sum()\n            union = truth_.sum() + prediction.sum()\n            if truth_.sum() == 0 and prediction.sum() == 0:\n                 scores[classes[class_]].append(1.0)\n            else:\n                scores[classes[class_]].append((intersection + eps) / union)\n                \n    return scores\n\n\ndef jaccard_coef_metric_per_classes(probabilities: np.ndarray,\n               truth: np.ndarray,\n               treshold: float = 0.5,\n               eps: float = 1e-9,\n               classes: list = [\"Large Bowel\", \"Small Bowel\", \"Stomach\"]) -> np.ndarray:\n    \"\"\"\n    Calculate Jaccard index for data batch and for each class.\n    Params:\n        probobilities: model outputs after activation function.\n        truth: model targets.\n        threshold: threshold for probabilities.\n        eps: additive to refine the estimate.\n        classes: list with name classes.\n        Returns: dict with jaccard scores for each class.\"\n    \"\"\"\n    scores = {key: list() for key in classes}\n    num = probabilities.shape[0]\n    num_classes = probabilities.shape[1]\n    predictions = (probabilities >= treshold).astype(np.float32)\n    assert(predictions.shape == truth.shape)\n\n    for i in range(num):\n        for class_ in range(num_classes):\n            prediction = predictions[i][class_]\n            truth_ = truth[i][class_]\n            intersection = (prediction * truth_).sum()\n            union = (prediction.sum() + truth_.sum()) - intersection + eps\n            if truth_.sum() == 0 and prediction.sum() == 0:\n                 scores[classes[class_]].append(1.0)\n            else:\n                scores[classes[class_]].append((intersection + eps) / union)\n\n    return scores","metadata":{"papermill":{"duration":7.467532,"end_time":"2022-05-01T15:57:05.820825","exception":false,"start_time":"2022-05-01T15:56:58.353293","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_scores_per_classes(model,\n                               dataloader,\n                               classes):\n    \"\"\"\n    Compute Dice and Jaccard coefficients for each class.\n    Params:\n        model: neural net for make predictions.\n        dataloader: dataset object to load data from.\n        classes: list with classes.\n        Returns: dictionaries with dice and jaccard coefficients for each class for each slice.\n    \"\"\"\n\n    dice_scores_per_classes = {key: list() for key in classes}\n    iou_scores_per_classes = {key: list() for key in classes}\n    with torch.no_grad():\n        for i, data in enumerate(dataloader):\n            imgs, targets = data[0], data[1]\n            imgs, targets = imgs.to(CFG.device), targets.to(CFG.device)\n            logits = model(imgs)\n            logits = logits.detach().cpu().numpy()\n            targets = targets.detach().cpu().numpy()\n            \n            dice_scores = dice_coef_metric_per_classes(logits, targets)\n            iou_scores = jaccard_coef_metric_per_classes(logits, targets)\n\n            \n            for key in dice_scores.keys():\n                dice_scores_per_classes[key].extend(dice_scores[key])\n\n            for key in iou_scores.keys():\n                iou_scores_per_classes[key].extend(iou_scores[key])\n                \n    return dice_scores_per_classes, iou_scores_per_classes","metadata":{"papermill":{"duration":7.756741,"end_time":"2022-05-01T15:57:21.974034","exception":false,"start_time":"2022-05-01T15:57:14.217293","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dice_scores_per_classes, iou_scores_per_classes = compute_scores_per_classes(\n    model, test_loader, [\"Large Bowel\", \"Small Bowel\", \"Stomach\"]\n    )","metadata":{"papermill":{"duration":53.529015,"end_time":"2022-05-01T15:58:23.240052","exception":false,"start_time":"2022-05-01T15:57:29.711037","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dice_df = pd.DataFrame(dice_scores_per_classes)\ndice_df.columns = ['Large Bowel Dice', 'Small Bowel Dice', 'Stomach Dice']\n\niou_df = pd.DataFrame(iou_scores_per_classes)\niou_df.columns = ['Large Bowel Jaccard', 'Small Bowel Jaccard', 'Stomach Jaccard']\nval_metics_df = pd.concat([dice_df, iou_df], axis=1, sort=True)\nval_metics_df = val_metics_df.loc[:, ['Large Bowel Dice', 'Large Bowel Jaccard', \n                                      'Small Bowel Dice', 'Small Bowel Jaccard', \n                                      'Stomach Dice', 'Stomach Jaccard']]\nval_metics_df.head(3)","metadata":{"papermill":{"duration":7.769347,"end_time":"2022-05-01T15:58:38.628641","exception":false,"start_time":"2022-05-01T15:58:30.859294","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"colors = ['#008899', '#aaaa00', '#008899', '#aaaa00', '#008899', '#aaaa00']\npalette = sns.color_palette(colors, 6)\n\nfig, ax = plt.subplots(figsize=(20, 7));\nsns.barplot(x=val_metics_df.mean().index, y=val_metics_df.mean(), palette=palette, ax=ax);\nax.set_xticklabels(val_metics_df.columns, fontsize=12, rotation=0);\nax.set_title(\"Dice and IoU \", fontsize=20)\n\nfor idx, p in enumerate(ax.patches):\n        percentage = '{:.2f}%'.format(100 * val_metics_df.mean().values[idx])\n        x = p.get_x() + p.get_width() / 2 - 0.15\n        y = p.get_y() + p.get_height() + 0.005\n        ax.annotate(percentage, (x, y), fontsize=15, fontweight=\"bold\")\n","metadata":{"papermill":{"duration":8.043052,"end_time":"2022-05-01T15:58:54.591679","exception":false,"start_time":"2022-05-01T15:58:46.548627","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"def get_metadata(row):\n    data = row['id'].split('_')\n    case = int(data[0].replace('case',''))\n    day = int(data[1].replace('day',''))\n    slice_ = int(data[-1])\n    row['case'] = case\n    row['day'] = day\n    row['slice'] = slice_\n    return row\n\ndef path2info(row):\n    path = row['path']\n    data = path.split('/')\n    slice_ = int(data[-1].split('_')[1])\n    case = int(data[-3].split('_')[0].replace('case',''))\n    day = int(data[-3].split('_')[1].replace('day',''))\n    width = int(data[-1].split('_')[2])\n    height = int(data[-1].split('_')[3])\n    row['height'] = height\n    row['width'] = width\n    row['case'] = case\n    row['day'] = day\n    row['slice'] = slice_\n#     row['id'] = f'case{case}_day{day}_slice_{slice_}'\n    return row","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df = pd.read_csv('../input/uw-madison-gi-tract-image-segmentation/sample_submission.csv')\nif not len(sub_df):\n    debug = True\n    sub_df = pd.read_csv('../input/uw-madison-gi-tract-image-segmentation/train.csv')[:1000*3]\n    sub_df = sub_df.drop(columns=['class','segmentation']).drop_duplicates()\nelse:\n    debug = False\n    sub_df = sub_df.drop(columns=['class','predicted']).drop_duplicates()\nsub_df = sub_df.progress_apply(get_metadata,axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if debug:\n    paths = glob(f'/kaggle/input/uw-madison-gi-tract-image-segmentation/train/**/*png',recursive=True)\n#     paths = sorted(paths)\nelse:\n    paths = glob(f'/kaggle/input/uw-madison-gi-tract-image-segmentation/test/**/*png',recursive=True)\n#     paths = sorted(paths)\npath_df = pd.DataFrame(paths, columns=['path'])\npath_df = path_df.progress_apply(path2info, axis=1)\npath_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = sub_df.merge(path_df, on=['case','day','slice'], how='left')\ntest_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = BuildDataset(test_df, subset='test', transforms=data_transforms['valid'])\ntest_loader  = DataLoader(test_dataset, batch_size=CFG.valid_bs, \n                          num_workers=4, shuffle=False, pin_memory=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\ndef infer(test_loader, num_log=1, thr=0.5):\n    msks = []; imgs = [];\n    pred_strings = []; pred_ids = []; pred_classes = [];\n    model = UNet()\n    model.load_state_dict(torch.load(f\"best_epoch-00.bin\"))\n    model.cuda()\n    model.eval()\n    for idx, (img, ids, heights, widths) in enumerate(tqdm(test_loader, total=len(test_loader), desc='Infer')):\n        img = img.to(CFG.device, dtype=torch.float) # .squeeze(0)\n        size = img.size()\n        msk = []\n        msk = torch.zeros((size[0], 3, size[2], size[3]), device=CFG.device, dtype=torch.float32)\n        out   = model(img) # .squeeze(0) # removing batch axis\n        out   = nn.Sigmoid()(out) # removing channel axis\n        msk = (out.permute((0,2,3,1))>thr).to(torch.uint8).cpu().detach().numpy() # shape: (n, h, w, c)\n        result = masks2rles(msk, ids, heights, widths)\n        pred_strings.extend(result[0])\n        pred_ids.extend(result[1])\n        pred_classes.extend(result[2])\n        del img, msk, out, result\n        gc.collect()\n        torch.cuda.empty_cache()\n    return pred_strings, pred_ids, pred_classes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_strings, pred_ids, pred_classes = infer(test_loader)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_df = pd.DataFrame({\n    \"id\":pred_ids,\n    \"class\":pred_classes,\n    \"predicted\":pred_strings\n})\nif not debug:\n    sub_df = pd.read_csv('../input/uw-madison-gi-tract-image-segmentation/sample_submission.csv')\n    del sub_df['predicted']\nelse:\n    sub_df = pd.read_csv('../input/uw-madison-gi-tract-image-segmentation/train.csv')[:1000*3]\n    del sub_df['segmentation']\n    \nsub_df = sub_df.merge(pred_df, on=['id','class'])\nsub_df.to_csv('submission.csv',index=False)\ndisplay(sub_df.head(5))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}