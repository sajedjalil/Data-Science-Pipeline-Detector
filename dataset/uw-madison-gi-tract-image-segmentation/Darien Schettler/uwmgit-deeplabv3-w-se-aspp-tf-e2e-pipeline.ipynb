{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<br>\n\n<br><center><img src=\"https://storage.googleapis.com/kaggle-competitions/kaggle/27923/logos/header.png?t=2021-06-02-20-30-25\" width=100%></center>\n\n<h2 style=\"text-align: center; font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: underline; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">UWM - GI Tract Image Segmentation Challenge<br>End-to-End Pipeline w/ DeepLab3</h2>\n<h5 style=\"text-align: center; font-family: Verdana; font-size: 12px; font-style: normal; font-weight: bold; text-decoration: None; text-transform: none; letter-spacing: 1px; color: black; background-color: #ffffff;\">CREATED BY: DARIEN SCHETTLER</h5>\n\n<br>\n\n---\n\n<br>\n\n<center><div class=\"alert alert-block alert-warning\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n    <b style=\"font-size: 18px;\">üëè &nbsp; IF YOU FORK THIS OR FIND THIS HELPFUL &nbsp; üëè</b><br><br><b style=\"font-size: 22px; color: darkorange\">PLEASE UPVOTE!</b><br><br>This was a lot of work for me and while it may seem silly, it makes me feel appreciated when others like my work. üòÖ\n</div></center>\n\n<center><div class=\"alert alert-block alert-info\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n    <b style=\"font-size: 18px;\">üôè &nbsp; THIS NOTEBOOK HEAVILY REFERENCES <a href=\"https://keras.io/examples/vision/deeplabv3_plus/#building-the-deeplabv3-model\">THIS TUTORIAL</a>  &nbsp; üôè</b></div></center>\n\n\n","metadata":{}},{"cell_type":"markdown","source":"<p id=\"toc\"></p>\n\n<br><br>\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: teal; background-color: #ffffff;\">TABLE OF CONTENTS</h1>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#imports\">0&nbsp;&nbsp;&nbsp;&nbsp;IMPORTS</a></h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#setup\">1&nbsp;&nbsp;&nbsp;&nbsp;SETUP</a></h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#helper_functions\">2&nbsp;&nbsp;&nbsp;&nbsp;HELPER FUNCTIONS</a></h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#create_dataset\">3&nbsp;&nbsp;&nbsp;&nbsp;DATASET CREATION</a></h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#model_training\">4&nbsp;&nbsp;&nbsp;&nbsp;MODEL TRAINING</a></h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#model_inference\">5&nbsp;&nbsp;&nbsp;&nbsp;MODEL INFERENCE</a></h3>\n\n---","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n<a id=\"imports\"></a>\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #ffffff; color: teal;\" id=\"imports\">0&nbsp;&nbsp;IMPORTS&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;</a></h1>","metadata":{}},{"cell_type":"code","source":"print(\"\\n... IMPORTS STARTING ...\\n\")\n\nimport os\nos.environ[\"TF_GPU_ALLOCATOR\"]=\"cuda_malloc_async\"\n\nprint(\"\\n\\tVERSION INFORMATION\")\n# Machine Learning and Data Science Imports\nimport tensorflow as tf; print(f\"\\t\\t‚Äì TENSORFLOW VERSION: {tf.__version__}\");\nimport tensorflow_hub as tfhub; print(f\"\\t\\t‚Äì TENSORFLOW HUB VERSION: {tfhub.__version__}\");\nimport tensorflow_addons as tfa; print(f\"\\t\\t‚Äì TENSORFLOW ADDONS VERSION: {tfa.__version__}\");\nimport pandas as pd; pd.options.mode.chained_assignment = None;\nimport numpy as np; print(f\"\\t\\t‚Äì NUMPY VERSION: {np.__version__}\");\nimport sklearn; print(f\"\\t\\t‚Äì SKLEARN VERSION: {sklearn.__version__}\");\nfrom sklearn.preprocessing import RobustScaler, PolynomialFeatures\nfrom pandarallel import pandarallel; pandarallel.initialize();\nfrom sklearn.model_selection import GroupKFold, StratifiedKFold\n\n# # RAPIDS\n# import cudf, cupy, cuml\n# from cuml.neighbors import NearestNeighbors\n# from cuml.manifold import TSNE, UMAP\n\n# Built In Imports\nfrom kaggle_datasets import KaggleDatasets\nfrom collections import Counter\nfrom datetime import datetime\nfrom glob import glob\nimport warnings\nimport requests\nimport hashlib\nimport imageio\nimport IPython\nimport sklearn\nimport urllib\nimport zipfile\nimport pickle\nimport random\nimport shutil\nimport string\nimport json\nimport math\nimport time\nimport gzip\nimport ast\nimport sys\nimport io\nimport os\nimport gc\nimport re\n\n# Visualization Imports\nfrom matplotlib.colors import ListedColormap\nfrom matplotlib.patches import Rectangle\nimport matplotlib.patches as patches\nimport plotly.graph_objects as go\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm; tqdm.pandas();\nimport plotly.express as px\nimport seaborn as sns\nfrom PIL import Image, ImageEnhance\nimport matplotlib; print(f\"\\t\\t‚Äì MATPLOTLIB VERSION: {matplotlib.__version__}\");\nfrom matplotlib import animation, rc; rc('animation', html='jshtml')\nimport plotly\nimport PIL\nimport cv2\n\nimport plotly.io as pio\nprint(pio.renderers)\n\ndef seed_it_all(seed=7):\n    \"\"\" Attempt to be Reproducible \"\"\"\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\n    \nprint(\"\\n\\n... IMPORTS COMPLETE ...\\n\")\n\nprint(\"\\n... SEEDING FOR DETERMINISTIC BEHAVIOUR ...\\n\")\nseed_it_all()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n<a id=\"background_information\"></a>\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: teal; background-color: #ffffff;\" id=\"setup\">1&nbsp;&nbsp;SETUP&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;</a></h1>\n\n---\n","metadata":{}},{"cell_type":"markdown","source":"<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">1.1 ACCELERATOR DETECTION</h3>\n\n---\n\nIn order to use **`TPU`**, we use **`TPUClusterResolver`** for the initialization which is necessary to connect to the remote cluster and initialize cloud TPUs. Let's go over two important points\n\n1. When using TPU on Kaggle, you don't need to specify arguments for **`TPUClusterResolver`**\n2. However, on **G**oogle **C**ompute **E**ngine (**GCE**), you will need to do the following:\n\n<br>\n\n```python\n# The name you gave to the TPU to use\nTPU_WORKER = 'my-tpu-name'\n\n# or you can also specify the grpc path directly\n# TPU_WORKER = 'grpc://xxx.xxx.xxx.xxx:8470'\n\n# The zone you chose when you created the TPU to use on GCP.\nZONE = 'us-east1-b'\n\n# The name of the GCP project where you created the TPU to use on GCP.\nPROJECT = 'my-tpu-project'\n\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=TPU_WORKER, zone=ZONE, project=PROJECT)\n```\n\n<div class=\"alert alert-block alert-danger\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n    <b style=\"font-size: 16px;\">üõë &nbsp; WARNING:</b><br><br>- Although the Tensorflow documentation says it is the <b>project name</b> that should be provided for the argument <b><code>`project`</code></b>, it is actually the <b>Project ID</b>, that you should provide. This can be found on the GCP project dashboard page.<br>\n</div>\n\n<div class=\"alert alert-block alert-info\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n    <b style=\"font-size: 16px;\">üìñ &nbsp; REFERENCES:</b><br><br>\n    - <a href=\"https://www.tensorflow.org/guide/tpu#tpu_initialization\"><b>Guide - Use TPUs</b></a><br>\n    - <a href=\"https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/TPUClusterResolver\"><b>Doc - TPUClusterResolver</b></a><br>\n\n</div>","metadata":{}},{"cell_type":"code","source":"print(f\"\\n... ACCELERATOR SETUP STARTING ...\\n\")\n\n# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    TPU = tf.distribute.cluster_resolver.TPUClusterResolver()  \nexcept ValueError:\n    TPU = None\n\nif TPU:\n    print(f\"\\n... RUNNING ON TPU - {TPU.master()}...\")\n    tf.config.experimental_connect_to_cluster(TPU)\n    tf.tpu.experimental.initialize_tpu_system(TPU)\n    strategy = tf.distribute.experimental.TPUStrategy(TPU)\nelse:\n    print(f\"\\n... RUNNING ON CPU/GPU ...\")\n    physical_devices = tf.config.list_physical_devices('GPU')\n    try:\n        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n    except:\n        # Invalid device or cannot modify virtual devices once initialized.\n        pass\n    \n    # Yield the default distribution strategy in Tensorflow\n    #   --> Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\n# What Is a Replica?\n#    --> A single Cloud TPU device consists of FOUR chips, each of which has TWO TPU cores. \n#    --> Therefore, for efficient utilization of Cloud TPU, a program should make use of each of the EIGHT (4x2) cores. \n#    --> Each replica is essentially a copy of the training graph that is run on each core and \n#        trains a mini-batch containing 1/8th of the overall batch size\nN_REPLICAS = strategy.num_replicas_in_sync\n    \nprint(f\"... # OF REPLICAS: {N_REPLICAS} ...\\n\")\n\nprint(f\"\\n... ACCELERATOR SETUP COMPLTED ...\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-04-28T20:18:37.047755Z","iopub.execute_input":"2022-04-28T20:18:37.048093Z","iopub.status.idle":"2022-04-28T20:18:37.154463Z","shell.execute_reply.started":"2022-04-28T20:18:37.048052Z","shell.execute_reply":"2022-04-28T20:18:37.153722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">1.2 COMPETITION DATA ACCESS</h3>\n\n---\n\nTPUs read data must be read directly from **G**oogle **C**loud **S**torage **(GCS)**. Kaggle provides a utility library ‚Äì¬†**`KaggleDatasets`** ‚Äì which has a utility function **`.get_gcs_path`** that will allow us to access the location of our input datasets within **GCS**.<br><br>\n\n<div class=\"alert alert-block alert-info\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n    <b style=\"font-size: 16px;\">üìå &nbsp; TIPS:</b><br><br>- If you have multiple datasets attached to the notebook, you should pass the name of a specific dataset to the <b><code>`get_gcs_path()`</code></b> function. <i>In our case, the name of the dataset is the name of the directory the dataset is mounted within.</i><br><br>\n</div>","metadata":{}},{"cell_type":"code","source":"print(\"\\n... DATA ACCESS SETUP STARTED ...\\n\")\n\nif TPU:\n    # Google Cloud Dataset path to training and validation images\n    DATA_DIR = KaggleDatasets().get_gcs_path('uw-madison-gi-tract-image-segmentation')\n    save_locally = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n    load_locally = tf.saved_model.LoadOptions(experimental_io_device='/job:localhost')\nelse:\n    # Local path to training and validation images\n    DATA_DIR = \"/kaggle/input/uw-madison-gi-tract-image-segmentation\"\n    save_locally = None\n    load_locally = None\n\nprint(f\"\\n... DATA DIRECTORY PATH IS:\\n\\t--> {DATA_DIR}\")\n\nprint(f\"\\n... IMMEDIATE CONTENTS OF DATA DIRECTORY IS:\")\nfor file in tf.io.gfile.glob(os.path.join(DATA_DIR, \"*\")): print(f\"\\t--> {file}\")\n\nprint(\"\\n\\n... DATA ACCESS SETUP COMPLETED ...\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-04-28T20:18:37.15584Z","iopub.execute_input":"2022-04-28T20:18:37.156097Z","iopub.status.idle":"2022-04-28T20:18:37.179321Z","shell.execute_reply.started":"2022-04-28T20:18:37.156061Z","shell.execute_reply":"2022-04-28T20:18:37.178433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">1.3 LEVERAGING XLA OPTIMIZATIONS</h3>\n\n---\n\n\n**XLA** (Accelerated Linear Algebra) is a domain-specific compiler for linear algebra that can accelerate TensorFlow models with potentially no source code changes. **The results are improvements in speed and memory usage**.\n\n<br>\n\nWhen a TensorFlow program is run, all of the operations are executed individually by the TensorFlow executor. Each TensorFlow operation has a precompiled GPU/TPU kernel implementation that the executor dispatches to.\n\nXLA provides us with an alternative mode of running models: it compiles the TensorFlow graph into a sequence of computation kernels generated specifically for the given model. Because these kernels are unique to the model, they can exploit model-specific information for optimization.<br><br>\n\n<div class=\"alert alert-block alert-danger\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n    <b style=\"font-size: 16px;\">üõë &nbsp; WARNING:</b><br><br>- XLA can not currently compile functions where dimensions are not inferrable: that is, if it's not possible to infer the dimensions of all tensors without running the entire computation<br>\n</div>\n\n<div class=\"alert alert-block alert-info\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n    <b style=\"font-size: 16px;\">üìå &nbsp; NOTE:</b><br><br>- XLA compilation is only applied to code that is compiled into a graph (in <b>TF2</b> that's only a code inside <b><code>tf.function</code></b>).<br>- The <b><code>jit_compile</code></b> API has must-compile semantics, i.e. either the entire function is compiled with XLA, or an <b><code>errors.InvalidArgumentError</code></b> exception is thrown)\n</div>\n\n<div class=\"alert alert-block alert-info\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n    <b style=\"font-size: 16px;\">üìñ &nbsp; REFERENCE:</b><br><br>    - <a href=\"https://www.tensorflow.org/xla\"><b>XLA: Optimizing Compiler for Machine Learning</b></a><br>\n</div>","metadata":{}},{"cell_type":"code","source":"print(f\"\\n... XLA OPTIMIZATIONS STARTING ...\\n\")\n\nprint(f\"\\n... CONFIGURE JIT (JUST IN TIME) COMPILATION ...\\n\")\n# enable XLA optmizations (10% speedup when using @tf.function calls)\ntf.config.optimizer.set_jit(False)\n\nprint(f\"\\n... XLA OPTIMIZATIONS COMPLETED ...\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-04-28T20:18:37.18155Z","iopub.execute_input":"2022-04-28T20:18:37.182267Z","iopub.status.idle":"2022-04-28T20:18:37.190014Z","shell.execute_reply.started":"2022-04-28T20:18:37.182224Z","shell.execute_reply":"2022-04-28T20:18:37.189002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">1.4 BASIC DATA DEFINITIONS & INITIALIZATIONS</h3>\n\n---\n","metadata":{}},{"cell_type":"code","source":"print(\"\\n... BASIC DATA SETUP STARTING ...\\n\\n\")\n\n# Open the training dataframe and display the initial dataframe\nTRAIN_DIR = os.path.join(DATA_DIR, \"train\")\nTRAIN_CSV = os.path.join(DATA_DIR, \"train.csv\")\ntrain_df = pd.read_csv(TRAIN_CSV)\n\n# Get all training images\nall_train_images = glob(os.path.join(TRAIN_DIR, \"**\", \"*.png\"), recursive=True)\n\nprint(\"\\n... ORIGINAL TRAINING DATAFRAME... \\n\")\ndisplay(train_df)\n\nTEST_DIR = os.path.join(DATA_DIR, \"test\")\nSS_CSV   = os.path.join(DATA_DIR, \"sample_submission.csv\")\nss_df = pd.read_csv(SS_CSV)\n\n# Get all testing images if there are any\nall_test_images = glob(os.path.join(TEST_DIR, \"**\", \"*.png\"), recursive=True)\n\nprint(\"\\n\\n\\n... ORIGINAL SUBMISSION DATAFRAME... \\n\")\ndisplay(ss_df)\n\n# For debugging purposes when the test set hasn't been substituted we will know\nDEBUG=len(all_test_images)==0\n\nif DEBUG:\n    TEST_DIR = TRAIN_DIR\n    all_test_images = all_train_images\n    first_50_cases = train_df.id.apply(lambda x: x.split(\"_\", 1)[0]).unique()[:50]\n    ss_df = train_df[train_df.id.apply(lambda x: x.split(\"_\", 1)[0]).isin(first_50_cases)]\n    ss_df = ss_df[[\"id\", \"class\"]]\n    ss_df[\"predicted\"] = \"\"\n    \n    print(\"\\n\\n\\n... DEBUG SUBMISSION DATAFRAME... \\n\")\n    display(ss_df)\n\nclasses = [\"Large Bowel\", \"Small Bowel\", \"Stomach\"]\nsf_classes = [\"lb\", \"sb\", \"st\"]\nSF2LF = {_sf:_lf for _sf,_lf in zip(sf_classes, classes)}\nLF2SF = {_lf:_sf for _sf,_lf in zip(sf_classes, classes)}\n\nprint(f\"\\n\\n\\n... ARE WE DEBUGGING: {DEBUG}... \\n\")\n\nprint(\"\\n... BASIC DATA SETUP FINISHED ...\\n\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-04-28T20:18:37.192185Z","iopub.execute_input":"2022-04-28T20:18:37.192569Z","iopub.status.idle":"2022-04-28T20:18:41.096022Z","shell.execute_reply.started":"2022-04-28T20:18:37.19253Z","shell.execute_reply":"2022-04-28T20:18:41.095206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">1.5 UPDATE DATAFRAMES WITH ACCESSIBLE EXTRA INFORMATION</h3>\n\n---\n\nI wrapped the logic in a preprocessing function","metadata":{}},{"cell_type":"code","source":"def get_filepath_from_partial_identifier(_ident, file_list):\n    return [x for x in file_list if _ident in x][0]\n\ndef df_preprocessing(df, globbed_file_list, is_test=False):\n    \"\"\" The preprocessing steps applied to get column information \"\"\"\n    # 1. Get Case-ID as a column (str and int)\n    df[\"case_id_str\"] = df[\"id\"].apply(lambda x: x.split(\"_\", 2)[0])\n    df[\"case_id\"] = df[\"id\"].apply(lambda x: int(x.split(\"_\", 2)[0].replace(\"case\", \"\")))\n\n    # 2. Get Day as a column\n    df[\"day_num_str\"] = df[\"id\"].apply(lambda x: x.split(\"_\", 2)[1])\n    df[\"day_num\"] = df[\"id\"].apply(lambda x: int(x.split(\"_\", 2)[1].replace(\"day\", \"\")))\n\n    # 3. Get Slice Identifier as a column\n    df[\"slice_id\"] = df[\"id\"].apply(lambda x: x.split(\"_\", 2)[2])\n\n    # 4. Get full file paths for the representative scans\n    df[\"_partial_ident\"] = (globbed_file_list[0].rsplit(\"/\", 4)[0]+\"/\"+ # /kaggle/input/uw-madison-gi-tract-image-segmentation/train/\n                           df[\"case_id_str\"]+\"/\"+ # .../case###/\n                           df[\"case_id_str\"]+\"_\"+df[\"day_num_str\"]+ # .../case###_day##/\n                           \"/scans/\"+df[\"slice_id\"]) # .../slice_#### \n    _tmp_merge_df = pd.DataFrame({\"_partial_ident\":[x.rsplit(\"_\",4)[0] for x in globbed_file_list], \"f_path\":globbed_file_list})\n    df = df.merge(_tmp_merge_df, on=\"_partial_ident\").drop(columns=[\"_partial_ident\"])\n\n    # 5. Get slice dimensions from filepath (int in pixels)\n    df[\"slice_h\"] = df[\"f_path\"].apply(lambda x: int(x[:-4].rsplit(\"_\",4)[1]))\n    df[\"slice_w\"] = df[\"f_path\"].apply(lambda x: int(x[:-4].rsplit(\"_\",4)[2]))\n\n    # 6. Pixel spacing from filepath (float in mm)\n    df[\"px_spacing_h\"] = df[\"f_path\"].apply(lambda x: float(x[:-4].rsplit(\"_\",4)[3]))\n    df[\"px_spacing_w\"] = df[\"f_path\"].apply(lambda x: float(x[:-4].rsplit(\"_\",4)[4]))\n\n    if not is_test:\n        # 7. Merge 3 Rows Into A Single Row (As This/Segmentation-RLE Is The Only Unique Information Across Those Rows)\n        l_bowel_df = df[df[\"class\"]==\"large_bowel\"][[\"id\", \"segmentation\"]].rename(columns={\"segmentation\":\"lb_seg_rle\"})\n        s_bowel_df = df[df[\"class\"]==\"small_bowel\"][[\"id\", \"segmentation\"]].rename(columns={\"segmentation\":\"sb_seg_rle\"})\n        stomach_df = df[df[\"class\"]==\"stomach\"][[\"id\", \"segmentation\"]].rename(columns={\"segmentation\":\"st_seg_rle\"})\n        df = df.merge(l_bowel_df, on=\"id\", how=\"left\")\n        df = df.merge(s_bowel_df, on=\"id\", how=\"left\")\n        df = df.merge(stomach_df, on=\"id\", how=\"left\")\n        df = df.drop_duplicates(subset=[\"id\",]).reset_index(drop=True)\n        df[\"lb_seg_flag\"] = df[\"lb_seg_rle\"].apply(lambda x: not pd.isna(x))\n        df[\"sb_seg_flag\"] = df[\"sb_seg_rle\"].apply(lambda x: not pd.isna(x))\n        df[\"st_seg_flag\"] = df[\"st_seg_rle\"].apply(lambda x: not pd.isna(x))\n        df[\"n_segs\"] = df[\"lb_seg_flag\"].astype(int)+df[\"sb_seg_flag\"].astype(int)+df[\"st_seg_flag\"].astype(int)\n\n    # 8. Reorder columns to the a new ordering (drops class and segmentation as no longer necessary)\n    new_col_order = [\"id\", \"f_path\", \"n_segs\",\n                     \"lb_seg_rle\", \"lb_seg_flag\",\n                     \"sb_seg_rle\", \"sb_seg_flag\", \n                     \"st_seg_rle\", \"st_seg_flag\",\n                     \"slice_h\", \"slice_w\", \"px_spacing_h\", \n                     \"px_spacing_w\", \"case_id_str\", \"case_id\", \n                     \"day_num_str\", \"day_num\", \"slice_id\", \"predicted\"]\n    if is_test: new_col_order.insert(1, \"class\")\n    new_col_order = [_c for _c in new_col_order if _c in df.columns]\n    df = df[new_col_order]\n    \n    return df\n\nprint(\"\\n... UPDATING DATAFRAMES WITH ACCESSIBLE INFORMATION STARTED ...\\n\\n\")\n\nprint(\"\\n... UPDATED TRAINING DATAFRAME... \\n\")\ntrain_df = df_preprocessing(train_df, all_train_images)\ndisplay(train_df)\n\nss_df = df_preprocessing(ss_df, all_test_images, is_test=True)\nprint(\"\\n\\n\\n... UPDATED SUBMISSION DATAFRAME... \\n\")\ndisplay(ss_df)\n\nprint(\"\\n... UPDATING DATAFRAMES WITH ACCESSIBLE INFORMATION FINISHED ...\\n\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-04-28T20:18:41.09765Z","iopub.execute_input":"2022-04-28T20:18:41.097909Z","iopub.status.idle":"2022-04-28T20:18:43.288362Z","shell.execute_reply.started":"2022-04-28T20:18:41.097873Z","shell.execute_reply":"2022-04-28T20:18:43.287528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n\n<a id=\"helper_functions\"></a>\n\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: teal; background-color: #ffffff;\" id=\"helper_functions\">\n    2&nbsp;&nbsp;HELPER FUNCTION & CLASSES&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;</a>\n</h1>\n\n---","metadata":{}},{"cell_type":"code","source":"# ref: https://www.kaggle.com/paulorzp/run-length-encode-and-decode\n# modified from: https://www.kaggle.com/inversion/run-length-decoding-quick-start\ndef rle_decode(mask_rle, shape, color=1):\n    \"\"\" TBD\n    \n    Args:\n        mask_rle (str): run-length as string formated (start length)\n        shape (tuple of ints): (height,width) of array to return \n    \n    Returns: \n        Mask (np.array)\n            - 1 indicating mask\n            - 0 indicating background\n\n    \"\"\"\n    # Split the string by space, then convert it into a integer array\n    s = np.array(mask_rle.split(), dtype=int)\n\n    # Every even value is the start, every odd value is the \"run\" length\n    starts = s[0::2] - 1\n    lengths = s[1::2]\n    ends = starts + lengths\n\n    # The image image is actually flattened since RLE is a 1D \"run\"\n    if len(shape)==3:\n        h, w, d = shape\n        img = np.zeros((h * w, d), dtype=np.float32)\n    else:\n        h, w = shape\n        img = np.zeros((h * w,), dtype=np.float32)\n\n    # The color here is actually just any integer you want!\n    for lo, hi in zip(starts, ends):\n        img[lo : hi] = color\n        \n    # Don't forget to change the image back to the original shape\n    return img.reshape(shape)\n\n# https://www.kaggle.com/namgalielei/which-reshape-is-used-in-rle\ndef rle_decode_top_to_bot_first(mask_rle, shape):\n    \"\"\" TBD\n    \n    Args:\n        mask_rle (str): run-length as string formated (start length)\n        shape (tuple of ints): (height,width) of array to return \n    \n    Returns:\n        Mask (np.array)\n            - 1 indicating mask\n            - 0 indicating background\n\n    \"\"\"\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape((shape[1], shape[0]), order='F').T  # Reshape from top -> bottom first\n\n# ref.: https://www.kaggle.com/stainsby/fast-tested-rle\ndef rle_encode(img):\n    \"\"\" TBD\n    \n    Args:\n        img (np.array): \n            - 1 indicating mask\n            - 0 indicating background\n    \n    Returns: \n        run length as string formated\n    \"\"\"\n    \n    pixels = img.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)\n\ndef flatten_l_o_l(nested_list):\n    \"\"\" Flatten a list of lists \"\"\"\n    return [item for sublist in nested_list for item in sublist]\n\ndef load_json_to_dict(json_path):\n    \"\"\" Helper function to load a json file into a dictionary \"\"\"\n    with open(json_path) as json_file:\n        data = json.load(json_file)\n    return data\n\ndef tf_load_png(img_path):\n    \"\"\" Helper to load an image using pure tf \"\"\"\n    return tf.image.decode_png(tf.io.read_file(img_path), channels=3)\n\ndef open_gray16(_path, normalize=True, to_rgb=False):\n    \"\"\" Helper to open competition specific files from path\n    \n    Args:\n        _path (str): Path to the image on the LOCAL file system\n        normalize (bool, optional): Whether or not to coerce image to be between 0-1\n        to_rgb (bool, optional): Whether or not to tile the grayscale image to produce a pseudo RGB image\n        \n    Returns:\n        The image as a numpy array\n    \"\"\"\n    if normalize:\n        if to_rgb:\n            return np.tile(np.expand_dims(cv2.imread(_path, cv2.IMREAD_ANYDEPTH)/65535., axis=-1), 3)\n        else:\n            return cv2.imread(_path, cv2.IMREAD_ANYDEPTH)/65535.\n    else:\n        if to_rgb:\n            return np.tile(np.expand_dims(cv2.imread(_path, cv2.IMREAD_ANYDEPTH), axis=-1), 3)\n        else:\n            return cv2.imread(_path, cv2.IMREAD_ANYDEPTH)\n        \ndef rle_decode_tf(mask_rle, shape):\n    \"\"\" Pure tensorflow RLE decoding function for easy pipelining\n    \n    Args:\n        mask_rle (str): The Run Length Encoded mask\n        shape (tuple): The shape of the mask we are decoding\n    \n    Returns:\n        A 1D tf.constant represending the decoded mask\n    \"\"\"\n    \n    shape = tf.convert_to_tensor(shape, tf.int64)\n    size = tf.math.reduce_prod(shape)\n    \n    # Split string\n    s = tf.strings.split(mask_rle)\n    s = tf.strings.to_number(s, tf.int64)\n    \n    # Get starts and lengths\n    starts = s[::2] - 1\n    lens = s[1::2]\n    \n    # Make ones to be scattered\n    total_ones = tf.reduce_sum(lens)\n    ones = tf.ones([total_ones], tf.uint8)\n    \n    # Make scattering indices\n    r = tf.range(total_ones)\n    lens_cum = tf.math.cumsum(lens)\n    s = tf.searchsorted(lens_cum, r, 'right')\n    idx = r + tf.gather(starts - tf.pad(lens_cum[:-1], [(1, 0)]), s)\n    \n    # Scatter ones into flattened mask\n    mask_flat = tf.scatter_nd(tf.expand_dims(idx, 1), ones, [size])\n    \n    # Reshape into mask\n    return tf.reshape(mask_flat, shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-28T20:18:43.289807Z","iopub.execute_input":"2022-04-28T20:18:43.290214Z","iopub.status.idle":"2022-04-28T20:18:43.31253Z","shell.execute_reply.started":"2022-04-28T20:18:43.290175Z","shell.execute_reply":"2022-04-28T20:18:43.311759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n\n<a id=\"create_dataset\"></a>\n\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: teal; background-color: #ffffff;\" id=\"create_dataset\">\n    3&nbsp;&nbsp;DATASET CREATION&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;</a>\n</h1>\n\n---","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">3.0 CREATE DATASET FOLDS</h3>\n\n---\n\n* ~~Remove rows with no segmentation masks for now...~~ (this resulted in way too many false positives)\n* Remove erroneous files\n* 8 folds\n* Grouped by case\n* Stratified on Unique Segmentation String (built to indicate which combination of seg masks are present)\n\n**NOTE: We are just taking a single fold for the purposes of this notebook...**","metadata":{}},{"cell_type":"code","source":"if DEBUG:\n    N_FOLDS = 8\n    gkf = GroupKFold(n_splits=N_FOLDS) \n    \n    remove_ids = [\"case7_day0\", \"case81_day30\"]\n    for _id in remove_ids:\n        train_df = train_df[~train_df.id.str.contains(_id)].reset_index(drop=True)\n    \n    # train_df = train_df[train_df.n_segs>0].reset_index(drop=True)\n    train_df[\"which_segs\"] = train_df.lb_seg_flag.astype(int).astype(str)+\\\n                             train_df.sb_seg_flag.astype(int).astype(str)+\\\n                             train_df.st_seg_flag.astype(int).astype(str)\n\n    for train_idxs, val_idxs in gkf.split(train_df[\"id\"], train_df[\"which_segs\"], train_df[\"case_id\"]):\n        sub_train_df=train_df.iloc[train_idxs]\n        N_TRAIN = len(sub_train_df)\n        sub_train_df=sub_train_df.sample(N_TRAIN).reset_index(drop=True)\n\n        sub_val_df=train_df.iloc[val_idxs]\n        N_VAL = len(sub_val_df)\n        sub_val_df=sub_val_df.sample(N_VAL).reset_index(drop=True)\n\n        break\n\n    # Fix the way we handled nan\n    sub_train_df.lb_seg_rle.fillna(\"\", inplace=True)\n    sub_train_df.sb_seg_rle.fillna(\"\", inplace=True)\n    sub_train_df.st_seg_rle.fillna(\"\", inplace=True)\n    \n    # Fix the way we handled nan\n    sub_val_df.lb_seg_rle.fillna(\"\", inplace=True)\n    sub_val_df.sb_seg_rle.fillna(\"\", inplace=True)\n    sub_val_df.st_seg_rle.fillna(\"\", inplace=True)\n\n    print(\"\\nFOLD 1: TRAIN DF\\n\\n\")\n    display(sub_train_df)\n\n    print(\"\\n\\n\\n\\nFOLD 1: VAL DF\\n\\n\")\n    display(sub_val_df)","metadata":{"execution":{"iopub.status.busy":"2022-04-28T20:18:43.313676Z","iopub.execute_input":"2022-04-28T20:18:43.313987Z","iopub.status.idle":"2022-04-28T20:18:43.60379Z","shell.execute_reply.started":"2022-04-28T20:18:43.313952Z","shell.execute_reply":"2022-04-28T20:18:43.603106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">3.1 CREATE MASK DATASET</h3>\n\n---\n\n* We will create our masks to have a shape of $W \\times H \\times 3$, where each channel is binary mask for a particular segmentation class in the order\n    * Channel 0 --> \"Large Bowel\"\n    * Channel 1 --> \"Small Bowel\"\n    * Channel 2 --> \"Stomach\"\n* We will first frame this problem as simple categorical segmentation and simply overlap values from 2-->0\n    * i.e. if we have overlapping Stomach, and Small Bowel... the Small Bowel mask will overwrite the Stomach mask\n    * i.e. if we have overlapping Large Bowel and Small Bowel... the Large Bowel mask will overwrite the Small Bowel mask\n* I will save both version of this dataset so we can try experimenting later\n    * NOTE: I think I should just be able to change the loss and we'll be good to go... but for now I will try and score with what I have...\n\n**NOTE: At this point we have to determine the size of our dataset... as most images are fairly small, let's target a size of <font color=\"blue\">$256 \\times 256$</font>**","metadata":{}},{"cell_type":"code","source":"IMAGE_SHAPE = SEG_SHAPE = (256,256)\n\ndef make_seg_mask(row, output_dir, resize_to):\n    \"\"\" Make a segmentation mask from a dataframe row\n    \n    Args:\n        row (pd.Series): The respective row to convert\n        output_dir (str): Path to the output directory for which we will dump the mask\n        resize_to (tuple): The shape to resize our mask to following creation\n        \n    Returns:\n        A string pointing to the newly generated numpy mask\n    \"\"\"\n    _output_style = \"multiclass\" if \"multiclass\" in output_dir else \"multilabel\"\n    _slice_shape = (row.slice_w, row.slice_h)\n    \n    if not pd.isna(row.lb_seg_rle):\n        lb_mask = rle_decode(row.lb_seg_rle, _slice_shape, )\n    else:\n        lb_mask = np.zeros(_slice_shape)\n        \n    if not pd.isna(row.sb_seg_rle):\n        sb_mask = rle_decode(row.sb_seg_rle, _slice_shape)\n    else:\n        sb_mask = np.zeros(_slice_shape)\n        \n    if not pd.isna(row.st_seg_rle):\n        st_mask = rle_decode(row.st_seg_rle, _slice_shape)\n    else:\n        st_mask = np.zeros(_slice_shape)\n    \n    if _output_style==\"multiclass\":\n        mask_arr = st_mask*3                         # stomach     = 3\n        mask_arr = np.where(sb_mask==1, 2, mask_arr) # small bowel = 2\n        mask_arr = np.where(lb_mask==1, 1, mask_arr) # large bowel = 1\n    else:\n        mask_arr = np.stack([lb_mask, sb_mask, st_mask], axis=-1)\n    \n    mask_arr = cv2.resize(mask_arr, resize_to, interpolation=cv2.INTER_NEAREST).astype(np.uint8)\n    mask_path = os.path.join(output_dir, f\"{row.id}_mask\")\n    np.save(mask_path, mask_arr)\n    return mask_path+\".npy\"\n\nSTYLE = \"multiclass\"\nif DEBUG: \n    _output_dir = f\"/kaggle/working/{STYLE}/npy_files\"\n    if not os.path.isdir(_output_dir): os.makedirs(_output_dir, exist_ok=True)\n    train_df[f\"{STYLE}_mask_path\"] = train_df.progress_apply(lambda _row: make_seg_mask(_row, _output_dir, resize_to=SEG_SHAPE), axis=1)\n    sub_train_df = sub_train_df.merge(train_df[[\"id\", f\"{STYLE}_mask_path\"]], on=\"id\")\n    sub_val_df = sub_val_df.merge(train_df[[\"id\", f\"{STYLE}_mask_path\"]], on=\"id\")","metadata":{"execution":{"iopub.status.busy":"2022-04-28T20:18:43.605057Z","iopub.execute_input":"2022-04-28T20:18:43.60547Z","iopub.status.idle":"2022-04-28T20:19:28.004046Z","shell.execute_reply.started":"2022-04-28T20:18:43.605431Z","shell.execute_reply":"2022-04-28T20:19:28.003233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">3.2 CREATE TF.DATA.DATASET</h3>\n\n---\n\n**INPUT**\n* Raw Image (256x256x3)\n\n**OUTPUT/TARGET**\n* Segmented Image (256x256x[3|1])\n\n---\n","metadata":{}},{"cell_type":"markdown","source":"**TEST SOME AUGMENTATIONS**|","metadata":{}},{"cell_type":"code","source":"dummy_id = 80\n\n_dummy_img = tf.constant(cv2.imread(train_df.f_path[dummy_id], -1))\n_dummy_img = _dummy_img/tf.reduce_max(_dummy_img)\n_dummy_mask = rle_decode(train_df.lb_seg_rle[dummy_id], (train_df.slice_w[dummy_id], train_df.slice_h[dummy_id]))\n\n_dummy_batch = tf.repeat(tf.expand_dims(tf.repeat(tf.expand_dims(_dummy_img, axis=-1), 3, axis=-1), axis=0), 8, axis=0)\n_dummy_mask_batch = tf.cast(tf.repeat(tf.expand_dims(tf.repeat(tf.expand_dims(_dummy_mask, axis=-1), 3, axis=-1), axis=0), 8, axis=0), tf.uint8)\n\n_degree_rot = tf.random.uniform([8,], minval=tf.constant(-0.4), maxval=tf.constant(0.4))\n_dummy_aug_batch = tfa.image.rotate(_dummy_batch, _degree_rot, interpolation=\"bilinear\")\n_dummy_aug_mask_batch = tfa.image.rotate(_dummy_mask_batch, _degree_rot, interpolation=\"bilinear\")\n\n\nplt.figure(figsize=(20,10))\nfor i in range(8):\n    plt.subplot(1,8,i+1)\n    plt.imshow(_dummy_batch[i])\n    plt.axis(False)\nplt.tight_layout()\nplt.show()\n\nplt.figure(figsize=(20,10))\nfor i in range(8):\n    plt.subplot(1,8,i+1)\n    plt.imshow(tf.cast(_dummy_mask_batch[i], tf.float32))\n    plt.axis(False)\nplt.tight_layout()\nplt.show()\n\nplt.figure(figsize=(20,10))\nfor i in range(8):\n    plt.subplot(1,8,i+1)\n    plt.imshow(_dummy_aug_batch[i])\n    plt.axis(False)\nplt.tight_layout()\nplt.show()\n\nplt.figure(figsize=(20,10))\nfor i in range(8):\n    plt.subplot(1,8,i+1)\n    plt.imshow(tf.cast(_dummy_aug_mask_batch[i], tf.float32))\n    plt.axis(False)\nplt.tight_layout()\nplt.show()\n\n_t_mag = tf.random.uniform([8,2], minval=tf.constant(-50.0), maxval=tf.constant(50.0))\n_dummy_aug_batch = tfa.image.translate(_dummy_aug_batch, translations=_t_mag, interpolation=\"bilinear\")\n_dummy_aug_mask_batch = tfa.image.translate(_dummy_aug_mask_batch, translations=_t_mag, interpolation=\"nearest\")\n\nplt.figure(figsize=(20,10))\nfor i in range(8):\n    plt.subplot(1,8,i+1)\n    plt.imshow(_dummy_aug_batch[i])\n    plt.axis(False)\nplt.tight_layout()\nplt.show()\n\nplt.figure(figsize=(20,10))\nfor i in range(8):\n    plt.subplot(1,8,i+1)\n    plt.imshow(tf.cast(_dummy_aug_mask_batch[i], tf.float32))\n    plt.axis(False)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-28T20:19:28.006521Z","iopub.execute_input":"2022-04-28T20:19:28.006934Z","iopub.status.idle":"2022-04-28T20:19:34.412751Z","shell.execute_reply.started":"2022-04-28T20:19:28.006896Z","shell.execute_reply":"2022-04-28T20:19:34.412023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tf_load_image(path):\n    \"\"\" Load an image with the correct shape using only TF\n    \n    Args:\n        path (tf.string): Path to the image to be loaded\n        resize_to (tuple, optional): Size to reshape image\n    \n    Returns:\n        3 channel tf.Constant image ready for training/inference\n    \n    \"\"\"\n    img_bytes = tf.io.read_file(path)\n    img = tf.image.decode_png(img_bytes, channels=3, dtype=tf.uint16)\n    # img = 255.*(img/tf.constant(32767, dtype=tf.uint16))\n    img = 255.*(img/tf.reduce_max(img))\n    img = tf.image.resize(img, (tf.constant(IMAGE_SHAPE[0]), tf.constant(IMAGE_SHAPE[1])))\n    return img\n\n\ndef tf_load_mask(rle_strs, root_shape, style=\"multiclass\"):\n    \"\"\" Pure tensorflow function to load multiple rles into an RGB array based on output style \"\"\"\n    tf_masks = [tf.cast(tf.image.resize(tf.expand_dims(rle_decode_tf(rle_str, root_shape), axis=-1), size=(tf.constant(SEG_SHAPE[0]), tf.constant(SEG_SHAPE[1])), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR), tf.uint8) for rle_str in rle_strs]\n    \n    if style==\"multilabel\":\n        return tf.concat(tf_masks, axis=-1)\n    else:        \n        _tf_masks = tf.zeros((*SEG_SHAPE, 1), dtype=tf.uint8)\n        _tf_masks = tf_masks[2]*tf.constant(3, dtype=tf.uint8) # small bowel = 3\n        _tf_masks = tf.where(tf_masks[1]==tf.constant(1, dtype=tf.uint8), tf.constant(2, dtype=tf.uint8), _tf_masks) # small bowel = 2\n        _tf_masks = tf.where(tf_masks[0]==tf.constant(1, dtype=tf.uint8), tf.constant(1, dtype=tf.uint8), _tf_masks) # large bowel = 1\n        return _tf_masks\n\ndef augment_batch(img_batch, mask_batch):\n    \"\"\" Pipeline augmentation \n    \n        - Right-Left Flipping (1/3 probability)\n        - Rotation (2/3 probability)\n        - Translation (2/3 probability)\n    \"\"\"\n    # Simple augmentation\n    if tf.random.uniform([])<=tf.constant(0.3333):\n        img_batch = tf.image.flip_left_right(img_batch)\n        mask_batch = tf.image.flip_left_right(mask_batch)\n        \n    # Random rotation\n    if tf.random.uniform([])<=tf.constant(0.6666):\n        degree_rot = tf.random.uniform([BATCH_SIZE,], minval=tf.constant(-0.4), maxval=tf.constant(0.4))\n        img_batch = tfa.image.rotate(img_batch, degree_rot, interpolation=\"bilinear\")\n        mask_batch = tfa.image.rotate(mask_batch, degree_rot, interpolation=\"nearest\")\n    \n    # Random translation\n    if tf.random.uniform([])<=tf.constant(0.6666):\n        _t_mag = tf.random.uniform([BATCH_SIZE,2], minval=tf.constant(-30.0), maxval=tf.constant(30.0))\n        img_batch = tfa.image.translate(img_batch, translations=_t_mag, interpolation=\"bilinear\")\n        mask_batch = tfa.image.translate(mask_batch, translations=_t_mag, interpolation=\"nearest\")\n        \n    return img_batch, mask_batch\n\ndef add_sample_weights(image_batch, mask_batch, _multiplier=1.5, _exp=0.25):\n    \"\"\"\n    Incorporate class weighting as a third term in tf.data.Dataset\n        \n        BACKGROUND TRAINING DATA PIXEL COUNT (%)  : %98.3326\n        LARGE BOWEL TRAINING DATA PIXEL COUNT (%) : %0.6883\n        SMALL BOWEL TRAINING DATA PIXEL COUNT (%) : %0.6301\n        STOMACH TRAINING DATA PIXEL COUNT (%)     : %0.3490\n\n    \"\"\"    \n    # Add class weighting\n    likelihood = tf.constant([0.983326, 0.06883, 0.06301, 0.03490])\n    class_weights = tf.constant(_multiplier)*((tf.constant(1.0)-likelihood)**_exp)\n\n    # Create an image of `sample_weights` by using the label at each pixel as an index into the `class weights`\n    sample_weights_batch = tf.gather(class_weights, indices=tf.cast(mask_batch, tf.int32))\n    \n    return image_batch, mask_batch, sample_weights_batch\n\ndef model_preprocessing_train(img_batch, mask_batch):\n    \"\"\" Model specific preprocessing for DeepLabV3 (training)\"\"\"\n    img_batch = img_batch/tf.constant(127.5)-tf.constant(1.0)\n    return img_batch, mask_batch\n    \ndef model_preprocessing_test(img_batch):\n    \"\"\" Model specific preprocessing for DeepLabV3 (testing)\"\"\"    \n    img_batch = img_batch/tf.constant(127.5)-tf.constant(1.0)\n    return img_batch\n\n# Hyperparameters\nBATCH_SIZE = 24\nSHUFFLE_BUFFER = max(BATCH_SIZE*25, 500)\nAUTOTUNE = tf.data.AUTOTUNE\n\n# Whether or not to train?\nDO_TRAIN=False\nif not DEBUG: DO_TRAIN=False\n\nif DEBUG:\n    train_ds = tf.data.Dataset.from_tensor_slices((sub_train_df.f_path, (sub_train_df.lb_seg_rle, sub_train_df.sb_seg_rle, sub_train_df.st_seg_rle), (sub_train_df.slice_w,sub_train_df.slice_h)))\n    val_ds = tf.data.Dataset.from_tensor_slices((sub_val_df.f_path, (sub_val_df.lb_seg_rle, sub_val_df.sb_seg_rle, sub_val_df.st_seg_rle), (sub_val_df.slice_w,sub_val_df.slice_h)))\n\n    train_ds = train_ds.map(lambda x,y,z: (tf_load_image(x), tf_load_mask(y,z,style=STYLE)), num_parallel_calls=AUTOTUNE)\n    val_ds = val_ds.map(lambda x,y,z: (tf_load_image(x), tf_load_mask(y,z,style=STYLE)), num_parallel_calls=AUTOTUNE)\n\n    train_ds = train_ds.shuffle(SHUFFLE_BUFFER)\\\n                       .batch(BATCH_SIZE, drop_remainder=True)\\\n                       .map(augment_batch, num_parallel_calls=AUTOTUNE)\\\n                       .map(model_preprocessing_train, num_parallel_calls=AUTOTUNE)\\\n                       .map(add_sample_weights, num_parallel_calls=AUTOTUNE)\\\n                       .prefetch(AUTOTUNE)    \n    \n    # we only shuffle the validation a little because we don't want \n    # drop_remainder to hit the same images over and over...\n    val_ds = val_ds.shuffle(SHUFFLE_BUFFER//5)\\\n                   .batch(BATCH_SIZE, drop_remainder=True)\\\n                   .map(model_preprocessing_train, num_parallel_calls=AUTOTUNE)\\\n                   .prefetch(AUTOTUNE)\n    \n    for _img_batch, _mask_batch in val_ds.take(1):\n        print(_img_batch.shape, _mask_batch.shape)\n        _img=_img_batch[0]\n        _mask=_mask_batch[0]\n        plt.figure(figsize=(15,5))\n        plt.subplot(1,2,1)\n        plt.imshow(tf.cast(_mask, tf.float32))\n\n        plt.subplot(1,2,2)\n        plt.imshow(tf.cast((_img+1)*127.5, tf.uint8))\n\n        plt.tight_layout()\n        plt.show()\n        \n# We only need every third row (hence the iloc[::3])\ntest_ds = tf.data.Dataset.from_tensor_slices(ss_df.iloc[::3].f_path.tolist())\ntest_ds = test_ds.map(lambda x: tf_load_image(x), num_parallel_calls=AUTOTUNE)\n\n# This should be deterministic... i.e. the order of images will match the order of IDs\ntest_ds = test_ds.batch(BATCH_SIZE)\\\n                 .map(model_preprocessing_test, num_parallel_calls=AUTOTUNE)\\\n                 .prefetch(AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2022-04-28T20:19:34.414123Z","iopub.execute_input":"2022-04-28T20:19:34.414384Z","iopub.status.idle":"2022-04-28T20:19:39.493684Z","shell.execute_reply.started":"2022-04-28T20:19:34.414341Z","shell.execute_reply":"2022-04-28T20:19:39.492971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n\n<a id=\"model_training\"></a>\n\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: teal; background-color: #ffffff;\" id=\"model_training\">\n    4&nbsp;&nbsp;MODEL TRAINING&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;</a>\n</h1>\n\n---","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">4.1 BUILDING THE DEEPLABV3+ MODEL</h3>\n\n<a href=\"https://keras.io/examples/vision/deeplabv3_plus/#building-the-deeplabv3-model\"><b>[REF]</b></a>\n\n---\n\n<a href=\"https://arxiv.org/abs/1802.02611\">DeepLabv3+</a> extends DeepLabv3 by adding an encoder-decoder structure. The encoder module processes multiscale contextual information by applying dilated convolution at multiple scales, while the decoder module refines the segmentation results along object boundaries. \n\n<center><img src=\"https://github.com/lattice-ai/DeepLabV3-Plus/raw/master/assets/deeplabv3_plus_diagram.png\"></center>\n\n**Dilated Convolution**: \n* With dilated convolution, as we go deeper in the network, we can keep the stride constant but with larger field-of-view without increasing the number of parameters or the amount of computation.\n* Besides, it enables larger output feature maps, which is useful for semantic segmentation.\n\n**Dilated Spatial Pyramid Pooling**\n* The reason for using Dilated Spatial Pyramid Pooling is that it was shown that as the sampling rate becomes larger, the number of valid filter weights (i.e., weights that are applied to the valid feature region, instead of padded zeros) becomes smaller.\n\n**The Encoder**\n* The encoder features are first bilinearly upsampled by a factor 4, and then concatenated with the corresponding low-level features from the network backbone that have the same spatial resolution. \n* For this example, we use a **ResNet50** pretrained on ImageNet as the backbone model, and we use the low-level features from the **`conv4_block6_2_relu`** block (shown below) of the backbone.","metadata":{}},{"cell_type":"code","source":"def convolution_block(block_input, num_filters=256, kernel_size=3,\n                      dilation_rate=1, padding=\"same\", use_bias=False,):\n    \"\"\" TBD \"\"\"\n    x = tf.keras.layers.Conv2D(filters=num_filters, \n                               kernel_size=kernel_size, \n                               dilation_rate=dilation_rate, \n                               padding=padding, \n                               use_bias=use_bias, \n                               kernel_initializer=tf.keras.initializers.HeNormal())(block_input)\n    x = tf.keras.layers.BatchNormalization()(x)\n    return tf.keras.layers.Activation(\"relu\")(x)\n\ndef SqueezeAndExcite(inputs, ratio=8):\n    init = inputs\n    filters = init.shape[-1]\n    se_shape = (1, 1, filters)\n    \n    se = tf.keras.layers.GlobalAveragePooling2D()(init)\n    se = tf.keras.layers.Reshape(se_shape)(se)\n    se = tf.keras.layers.Dense(filters // ratio, activation='relu', kernel_initializer='he_normal', use_bias=False)(se)\n    se = tf.keras.layers.Dense(filters, activation='sigmoid', kernel_initializer='he_normal', use_bias=False)(se)\n    return init*se\n\ndef ASPP(inputs):\n    \"\"\" Image Pooling \"\"\"\n    shape = inputs.shape\n    y1 = tf.keras.layers.AveragePooling2D(pool_size=(shape[1], shape[2]))(inputs)\n    y1 = tf.keras.layers.Conv2D(256, 1, padding=\"same\", use_bias=False)(y1)\n    y1 = tf.keras.layers.BatchNormalization()(y1)\n    y1 = tf.keras.layers.Activation(\"relu\")(y1)\n    y1 = tf.keras.layers.UpSampling2D((shape[1], shape[2]), interpolation=\"bilinear\")(y1)\n\n    \"\"\" 1x1 conv \"\"\"\n    y2 = tf.keras.layers.Conv2D(256, 1, padding=\"same\", use_bias=False)(inputs)\n    y2 = tf.keras.layers.BatchNormalization()(y2)\n    y2 = tf.keras.layers.Activation(\"relu\")(y2)\n\n    \"\"\" 3x3 conv rate=6 \"\"\"\n    y3 = tf.keras.layers.Conv2D(256, 3, padding=\"same\", use_bias=False, dilation_rate=6)(inputs)\n    y3 = tf.keras.layers.BatchNormalization()(y3)\n    y3 = tf.keras.layers.Activation(\"relu\")(y3)\n\n    \"\"\" 3x3 conv rate=12 \"\"\"\n    y4 = tf.keras.layers.Conv2D(256, 3, padding=\"same\", use_bias=False, dilation_rate=12)(inputs)\n    y4 = tf.keras.layers.BatchNormalization()(y4)\n    y4 = tf.keras.layers.Activation(\"relu\")(y4)\n\n    \"\"\" 3x3 conv rate=18 \"\"\"\n    y5 = tf.keras.layers.Conv2D(256, 3, padding=\"same\", use_bias=False, dilation_rate=18)(inputs)\n    y5 = tf.keras.layers.BatchNormalization()(y5)\n    y5 = tf.keras.layers.Activation(\"relu\")(y5)\n\n    y = tf.keras.layers.Concatenate()([y1, y2, y3, y4, y5])\n    y = tf.keras.layers.Conv2D(256, 1, padding=\"same\", use_bias=False)(y)\n    y = tf.keras.layers.BatchNormalization()(y)\n    y = tf.keras.layers.Activation(\"relu\")(y)\n\n    return y\n\ndef DilatedSpatialPyramidPooling(dspp_input):\n    \"\"\" TBD \"\"\"\n    x = tf.keras.layers.AveragePooling2D(pool_size=(HIGH_FEAT_LAYER_OUTPUT_SHAPE[-3], \n                                                    HIGH_FEAT_LAYER_OUTPUT_SHAPE[-2]))(dspp_input)\n    x = convolution_block(x, kernel_size=1, use_bias=True)\n    \n    # Get layers to concatenate\n    out_pool = tf.keras.layers.UpSampling2D(size=(HIGH_FEAT_LAYER_OUTPUT_SHAPE[-3]//x.shape[1], \n                                                  HIGH_FEAT_LAYER_OUTPUT_SHAPE[-2]//x.shape[2]), \n                                            interpolation=\"bilinear\")(x)\n    _out_layers = [out_pool,]+\\\n                  [convolution_block(dspp_input, 256, _k, _d) for _k, _d in zip((1,3,3,3), (1,6,12,18))]\n    \n    output = convolution_block(tf.keras.layers.Concatenate(axis=-1)(_out_layers), kernel_size=1)\n\n    return output\n\n\ndef DeeplabV3Plus(backbone, low_feat_layer, high_feat_layer, n_classes, weights=\"imagenet\", dropout=0.3):\n    \n    _inputs = tf.keras.layers.Input(shape=(*IMAGE_SHAPE, 3))\n    encoder_bb = backbone(weights=weights, include_top=False, input_tensor=_inputs)\n    \n    x = encoder_bb.get_layer(high_feat_layer).output\n    x = tf.keras.layers.Dropout(dropout)(x)\n    x = ASPP(x)    \n    \n    input_a = tf.keras.layers.UpSampling2D(size=(IMAGE_SHAPE[0]//4//x.shape[1], \n                                                 IMAGE_SHAPE[1]//4//x.shape[2]), \n                                           interpolation=\"bilinear\")(x)\n    input_b = encoder_bb.get_layer(low_feat_layer).output\n    input_b = tf.keras.layers.Dropout(dropout)(input_b)\n    input_b = convolution_block(input_b, num_filters=48, kernel_size=1)\n\n    x = tf.keras.layers.Concatenate(axis=-1)([input_a, input_b])\n    \n    x = SqueezeAndExcite(x)\n    x = convolution_block(x)\n    x = convolution_block(x)\n    x = SqueezeAndExcite(x)\n    \n    x = tf.keras.layers.UpSampling2D(size=(IMAGE_SHAPE[0]//x.shape[1], \n                                           IMAGE_SHAPE[1]//x.shape[2]), \n                                     interpolation=\"bilinear\",)(x)\n    x = tf.keras.layers.Dropout(dropout/2)(x)\n    _outputs = tf.keras.layers.Conv2D(n_classes, kernel_size=(1, 1), padding=\"same\", activation=\"sigmoid\" if STYLE==\"multilabel\" else \"softmax\")(x)\n    return tf.keras.Model(inputs=_inputs, outputs=_outputs)\n\n# If you change the backbone you will need to adjust this accordingly\nBACKBONE = tf.keras.applications.ResNet50\nRES_HIGH_FEAT_LAYER = \"conv4_block6_2_relu\"\nRES_LOW_FEAT_LAYER = \"conv2_block3_2_relu\"\n_dummy_model = BACKBONE(include_top=False, weights=None, input_shape=(*IMAGE_SHAPE, 3))\nHIGH_FEAT_LAYER_OUTPUT_SHAPE = _dummy_model.get_layer(RES_HIGH_FEAT_LAYER).output_shape[1:]\nLOW_FEAT_LAYER_OUTPUT_SHAPE = _dummy_model.get_layer(RES_LOW_FEAT_LAYER).output_shape[1:]\n\nSUB_NODEBUG_MODEL_WT_PATH = \"/kaggle/input/deeplav3-224-seaspp-public-weights/resnet50_256x256x3_multiclass\"\n\nif STYLE==\"multiclass\":\n    N_CLASSES = len(classes)+1 # n_classses+background\nelse:\n    N_CLASSES = len(classes) # n_classses (binary so background is 0 in each channel)\n\nMODEL_INSPECT = \"summary\"\n\n# We need this locally if we want to do all of this stuff without internet...\nWEIGHT_PATH = \"/kaggle/input/tf-keras-pretrained-model-weights/No Top/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n\nif DO_TRAIN:\n    deeplabv3plus = DeeplabV3Plus(backbone=BACKBONE, weights=WEIGHT_PATH,\n                                  low_feat_layer=RES_LOW_FEAT_LAYER, \n                                  high_feat_layer=RES_HIGH_FEAT_LAYER, \n                                  n_classes=N_CLASSES)\n\n    if MODEL_INSPECT==\"plot\":\n        display(tf.keras.utils.plot_model(deeplabv3plus))\n    elif MODEL_INSPECT==\"summary\":\n        print(deeplabv3plus.summary())\nelse:\n    deeplabv3plus = tf.keras.models.load_model(SUB_NODEBUG_MODEL_WT_PATH, compile=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-28T20:19:39.495201Z","iopub.execute_input":"2022-04-28T20:19:39.495452Z","iopub.status.idle":"2022-04-28T20:19:43.252039Z","shell.execute_reply.started":"2022-04-28T20:19:39.495416Z","shell.execute_reply":"2022-04-28T20:19:43.251341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">4.2 TRAINING THE MODEL</h3>\n\n<a href=\"https://keras.io/examples/vision/deeplabv3_plus/#building-the-deeplabv3-model\"><b>[REF]</b></a>\n\n---","metadata":{}},{"cell_type":"code","source":"# Custom Callback To Include in Callbacks List At Training Time\nclass GarbageCollectorCallback(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs=None):\n        gc.collect()\n        tf.keras.backend.clear_session()\n\ndef plot_history(_history, fold_num=\"1\", metrics=(\"acc\",)):\n    \"\"\" TBD \"\"\"\n    fig = px.line(_history.history, \n                  x=range(len(_history.history[\"loss\"])), \n                  y=[\"loss\", \"val_loss\"],\n                  labels={\"value\":\"Loss (log-axis)\", \"x\":\"Epoch #\"},\n                  title=f\"<b>FOLD {fold_num} MODEL - LOSS</b>\", log_y=True\n                  )\n    fig.show()\n\n    for _m in metrics:\n        fig = px.line(_history.history, \n                      x=range(len(_history.history[_m])), \n                      y=[_m, f\"val_{_m}\"],\n                      labels={\"value\":f\"{_m} (log-axis)\", \"x\":\"Epoch #\"},\n                      title=f\"<b>FOLD {fold_num} MODEL - {_m}</b>\", log_y=True)\n        fig.show()    \n\nN_EPOCH = 20\nif DO_TRAIN:\n    OPTIMIZER = tf.keras.optimizers.Adam(0.0006666)\n    if STYLE==\"multiclass\":\n        LOSS = tf.keras.losses.SparseCategoricalCrossentropy()\n    else:\n        LOSS = tfa.losses.SigmoidFocalCrossEntropy()\n        \n    METRICS = [\"acc\"]\n               \n\n    _lr_cb = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.75, \n                                                  patience=2, verbose=1, mode=\"min\")\n    _es_cb = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",  patience=4, \n                                              verbose=1, mode=\"min\",\n                                              restore_best_weights=True)\n    _ckpt_cb = tf.keras.callbacks.ModelCheckpoint(f'./resnet50_{IMAGE_SHAPE[0]}x{IMAGE_SHAPE[1]}x3_{STYLE}', \n                                                  monitor='val_loss', mode=\"min\",\n                                                  save_best_only=True, options=save_locally)\n    _gc_cb = GarbageCollectorCallback()\n    CB_LIST = [_es_cb, _ckpt_cb, _lr_cb, _gc_cb]\n\n    deeplabv3plus.compile(optimizer=OPTIMIZER, loss=LOSS, metrics=METRICS,)\n    history = deeplabv3plus.fit(train_ds, validation_data=val_ds, epochs=N_EPOCH, callbacks=CB_LIST)\n    plot_history(history)","metadata":{"execution":{"iopub.status.busy":"2022-04-28T20:19:43.253358Z","iopub.execute_input":"2022-04-28T20:19:43.253606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: teal; background-color: #ffffff;\">4.3 VALIDATE AND VISUALIZE</h3>\n\n---","metadata":{}},{"cell_type":"code","source":"def get_overlay(img, mask, _alpha=0.999, _beta=0.45, _gamma=0):\n    \n    # Normalize to be between 0-1 (float32)\n    img = (img/img.max()).astype(np.float32)\n    \n    # Make mask RGB and float32\n    if len(mask.shape)==2:\n        mask_rgb = np.zeros_like(img, dtype=np.float32)\n        mask_rgb[..., 2] = np.where(mask==3, 1.0, 0.0)\n        mask_rgb[..., 1] = np.where(mask==2, 1.0, 0.0)\n        mask_rgb[..., 0] = np.where(mask==1, 1.0, 0.0)\n    else:\n        mask_rgb=mask.astype(np.float32)\n    \n    # overlay\n    seg_overlay = cv2.addWeighted(src1=img, alpha=_alpha, \n                                  src2=mask_rgb, beta=_beta, gamma=_gamma)\n    return seg_overlay\n\ndef get_miss_overlay(gt_mask, pred_mask, _alpha=0.9, _beta=0.25, _gamma=0):\n    \n    # Make mask RGB and float32\n    miss_rgb = np.zeros((*pred_mask.shape[:2],3), dtype=np.float32)\n    \n    if len(pred_mask.shape)==2:\n        miss_rgb[..., 1] = np.where((gt_mask==pred_mask)&(gt_mask!=0), 0.8, 0.0)\n        miss_rgb[..., 0] = np.where((gt_mask!=pred_mask), 0.8, 0.0)\n    else:\n        \n        miss_rgb = np.where((gt_mask==pred_mask)&(gt_mask!=0.0), (0.0,0.8,0.0), (0.0,0.0,0.0))\n        miss_rgb = np.where((gt_mask!=pred_mask), (0.8,0.0,0.0), miss_rgb)\n        \n    return miss_rgb\n\ndef plot_preds(img, pred_mask, gt_mask):\n    gt_overlay = get_overlay(img, gt_mask)\n    pred_overlay = get_overlay(img, pred_mask)\n    miss_overlay = get_miss_overlay(gt_mask, pred_mask)\n    \n    plt.figure(figsize=(20,12))\n    \n    for i, (_desc, _img) in enumerate(zip([\"Original\", \"Prediction Mask\", \"Ground-Truth Mask\", \"Miss Mask\"], [img, pred_overlay, gt_overlay, miss_overlay])):        \n        plt.subplot(1,4,i+1)\n        plt.imshow(_img)\n        plt.title(f\"{_desc} Image\", fontweight=\"bold\")        \n        plt.axis(False)\n        \n        if i in [1,2]:\n            handles = [Rectangle((0,0),1,1, color=_c) for _c in [(0.667,0.0,0.0), (0.0,0.667,0.0), (0.0,0.0,0.667)]]\n            labels = [\"Large Bowel Segmentation Map\", \"Small Bowel Segmentation Map\", \"Stomach Segmentation Map\"]\n            plt.legend(handles,labels)\n        elif i==3:\n            handles = [Rectangle((0,0),1,1, color=_c) for _c in [(0.0,0.8,0.0), (0.8,0.0,0.0), (0.0, 0.0, 0.0)]]\n            labels = [\"Agreement\", \"Disagreement\", \"Background\"]\n            plt.legend(handles,labels)\n    plt.tight_layout()\n    plt.show()\n\nif DEBUG:\n    for img_batch, mask_batch in val_ds.take(1):\n        pred_batch = deeplabv3plus(img_batch)\n\n        if STYLE==\"multilabel\":\n            pred_batch = np.where(pred_batch>=0.5, 1.0, 0.0)\n        else:\n            pred_batch = np.argmax(pred_batch, axis=-1)\n\n        img_batch = ((img_batch+1)*127.5).numpy().astype(np.int32)\n        mask_batch = mask_batch.numpy().squeeze().astype(np.float32)\n        break\n\n    for _img, _pred, _mask in zip(img_batch, pred_batch, mask_batch):\n        plot_preds(_img, _pred, _mask)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T16:12:58.694518Z","iopub.execute_input":"2022-04-22T16:12:58.694762Z","iopub.status.idle":"2022-04-22T16:13:13.043718Z","shell.execute_reply.started":"2022-04-22T16:12:58.694729Z","shell.execute_reply":"2022-04-22T16:13:13.043009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n\n<a id=\"model_inference\"></a>\n\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: teal; background-color: #ffffff;\" id=\"model_inference\">\n    5&nbsp;&nbsp;MODEL INFERENCE&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;</a>\n</h1>\n\n---\n\n**Only run this when things are done as masks will be deleted**","metadata":{}},{"cell_type":"code","source":"!rm -rf ./multi*","metadata":{"execution":{"iopub.status.busy":"2022-04-22T16:13:43.178938Z","iopub.execute_input":"2022-04-22T16:13:43.179213Z","iopub.status.idle":"2022-04-22T16:13:44.521338Z","shell.execute_reply.started":"2022-04-22T16:13:43.179162Z","shell.execute_reply":"2022-04-22T16:13:44.520371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pred_2_rle(pred_arr, root_shape):\n    \n    # Get correct size pred array based on initial slice size\n    pred_arr = cv2.resize(pred_arr, root_shape, interpolation=cv2.INTER_NEAREST)\n    \n    # Get individual segmentation masks\n    lb_mask = np.where(pred_arr==1,1,0)\n    sb_mask = np.where(pred_arr==2,1,0)\n    st_mask = np.where(pred_arr==3,1,0)\n    \n    return rle_encode(lb_mask), rle_encode(sb_mask), rle_encode(st_mask)\n\nN_TEST = int(np.ceil((len(ss_df)//3)/BATCH_SIZE))\n\n# Loop over batches and get prediction\nfor i, img_batch in tqdm(enumerate(test_ds), total=N_TEST):\n    \n    # Cleanup every so often\n    if i%100==0:\n        gc.collect(); gc.collect(); tf.keras.backend.clear_session(); gc.collect()    \n    \n    # Get predictions\n    pred_batch = tf.argmax(deeplabv3plus(img_batch, training=False), axis=-1).numpy()\n    \n    # Loop over prediction and determine submission dataframe index (3*individual-count because of reduced inference size)\n    for j, _pred in enumerate(pred_batch):\n        df_idx = 3*(i*BATCH_SIZE+j)\n        pred_rles = pred_2_rle(_pred, (ss_df.iloc[df_idx][\"slice_h\"], ss_df.iloc[df_idx][\"slice_w\"]))\n        \n        # Loop over rles and assign the correct row of the submission dataframe\n        for k,pred_rle in enumerate(pred_rles):\n            ss_df.loc[df_idx+k, \"predicted\"] = pred_rle","metadata":{"execution":{"iopub.status.busy":"2022-04-22T16:14:08.970598Z","iopub.execute_input":"2022-04-22T16:14:08.970886Z","iopub.status.idle":"2022-04-22T16:21:00.385639Z","shell.execute_reply.started":"2022-04-22T16:14:08.970856Z","shell.execute_reply":"2022-04-22T16:21:00.38145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Helper functions\ndef fix_empty_slices(_row):\n    if int(_row[\"slice_id\"].rsplit(\"_\", 1)[-1]) in remove_seg_slices[_row[\"class\"]]:\n        _row[\"predicted\"] = \"\"\n    return _row\n\ndef is_isolated(_row):\n    return (_row[\"predicted\"]!=\"\" and _row[\"prev_predicted\"]==\"\" and _row[\"next_predicted\"]==\"\")\n\ndef fix_nc_slices(_row):\n    if _row[\"seg_isolated\"]:\n        _row[\"predicted\"] = \"\"\n    return _row\n\n# No segmentation exists at these slices\nremove_seg_slices = {\n     \"large_bowel\": [1, 138, 139, 140, 141, 142, 143, 144],\n    \"small_bowel\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 138, 139, 140, 141, 142, 143, 144],\n    \"stomach\": [1, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144],\n}\n\n# Heuristic processing\nss_df = ss_df.apply(fix_empty_slices, axis=1)\nss_df[\"prev_predicted\"] = ss_df.shift(3, fill_value=\"\")[\"predicted\"]\nss_df[\"next_predicted\"] = ss_df.shift(-3, fill_value=\"\")[\"predicted\"]\nss_df[\"seg_isolated\"] = ss_df.apply(is_isolated, axis=1)\nss_df = ss_df.apply(fix_nc_slices, axis=1)\n\n# Submit\nss_df = ss_df[[\"id\", \"class\", \"predicted\"]]\nss_df.to_csv(\"submission.csv\", index=False)\ndisplay(ss_df)","metadata":{},"execution_count":null,"outputs":[]}]}