{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":false},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os, gc\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as ctb\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom datetime import date, datetime, timedelta\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\nfrom sklearn.neighbors import KNeighborsRegressor, NearestNeighbors\nfrom sklearn.linear_model import Ridge\nfrom scipy.optimize import nnls\npd.set_option('display.max_columns', 10)\npd.set_option('display.max_rows', 10)\nnp.set_printoptions(precision=6, suppress=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# note: data update 2020-04-07, blend\nmname = 'gbt3n'\npath = '/kaggle/input/gbt1n-external/'\npathk = '/kaggle/input/covid19-global-forecasting-week-3/'\nnhorizon = 30\nskip = 0\nkv = [6,11]\nval_scheme = 'forward'\nprev_test = False\ntrain_full = True\nsave_data = True\n\nbooster = ['lgb','xgb','ctb','rdg']\n# booster = ['cas']\n\nteams = []\n# teams = ['psi0c','cpmp0c','ahmet0c']\n\n# if using updated daily data, also update time-varying external data\n# in COVID-19 and covid-19-data, git pull origin master \n# ecdc wget https://opendata.ecdc.europa.eu/covid19/casedistribution/csv\n# weather: https://www.kaggle.com/davidbnn92/weather-data/output?scriptVersionId=31103959\n# google trends: pytrends0b.ipynb\n# run teams models","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"code","source":"train = pd.read_csv(pathk+'train.csv')\n\n# use week2 test set in order to compare with week 2 leaderboard\nif prev_test:\n    test = pd.read_csv('../week2/test.csv')\n    ss = pd.read_csv('../week2/submission.csv')\nelse:\n    test = pd.read_csv(pathk+'test.csv')\n    ss = pd.read_csv(pathk+'submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# tmax and dmax are the last day of training\ntmax = train.Date.max()\ndmax = datetime.strptime(tmax,'%Y-%m-%d').date()\nprint(tmax, dmax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"fmax = test.Date.max()\nfdate = datetime.strptime(fmax,'%Y-%m-%d').date()\nfdate","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"tmin = train.Date.min()\nfmin = test.Date.min()\ntmin, fmin","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"dmin = datetime.strptime(tmin,'%Y-%m-%d').date()\nprint(dmin)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"ynames = ['ConfirmedCases','Fatalities']\nny = len(ynames)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# train['ForecastId'] = train.Id - train.Id.max()\ncp = ['Country_Region','Province_State']\ncpd = cp + ['Date']\ntrain = train.merge(test[cpd+['ForecastId']], how='left', on=cpd)\ntrain['ForecastId'] = train['ForecastId'].fillna(0).astype(int)\ntrain['y0_pred'] = np.nan\ntrain['y1_pred'] = np.nan\n\ntest['Id'] = test.ForecastId + train.Id.max()\ntest['ConfirmedCases'] = np.nan\ntest['Fatalities'] = np.nan\n# use zeros here instead of nans so monotonic adjustment fills final dates if necessary\ntest['y0_pred'] = 0.0\ntest['y1_pred'] = 0.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# concat non-overlapping part of test to train for feature engineering\nd = pd.concat((train,test[test.Date > train.Date.max()])).reset_index(drop=True)\nd","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"(dmin + timedelta(30)).isoformat()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d['Date'].value_counts().std()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# fill missing province with blank, must also do this with external data before merging\nd[cp] = d[cp].fillna('')\n\n# create single location variable\nd['Loc'] = d['Country_Region'] + ' ' + d['Province_State']\nd['Loc'] = d['Loc'].str.strip()\nd['Loc'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# drop new regions in order to compare with week 2 leaderboard\nif prev_test:\n    test2 = pd.read_csv('../week2/test.csv')\n    test2[cp] = test2[cp].fillna('')\n    test2 = test2.drop(['ForecastId','Date'], axis=1).drop_duplicates()\n    # test2['week2'] = 1\n    test2\n\n    d = d.merge(test2, how='inner', on=cp)\n    # d = d[d.week2.notnull()]\n    d.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# log1p transform both targets\nyv = []\nfor i in range(ny):\n    v = 'y'+str(i)\n    d[v] = np.log1p(d[ynames[i]])\n    yv.append(v)\nprint(d[yv].describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# merge predictions from other teams\n# right now these are based only on public lb training set < 2020-03-26\n# need to also use predictions from full set\n# teams = ['dott0b','psi0b','cpmp0b']\ntfeats = [[],[]]\nfor ti in teams:\n    td = pd.read_csv('sub/'+ti+'.csv')\n    t = ti[:-2]\n    print(td.head(), td.shape, ti, t)\n    td[t+'0'] = np.log1p(td.ConfirmedCases)\n    td[t+'1'] = np.log1p(td.Fatalities)\n    td.drop(ynames, axis=1, inplace=True)\n    if 'ForecastId' in list(td.columns):\n        d = d.merge(td, how='left', on='ForecastId')\n    else:\n        d = d.merge(td, how='left', on='Id')\n    print(d.shape)\n    tfeats[0].append(t+'0')\n    tfeats[1].append(t+'1')\ntf2 = len(tfeats[0])\nprint(tfeats, tf2)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# sort by location then date\nd = d.sort_values(['Loc','Date']).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d['Country_Region'].value_counts(dropna=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d['Province_State'].value_counts(dropna=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# # data scraped from https://www.worldometers.info/coronavirus/, including past daily snapshots\n# # 12 new features, all log1p transformed, must be lagged\n# wm = pd.read_csv('wmcp.csv')\n# wm.drop('Country',axis=1,inplace=True)\n# wmf = [c for c in wm.columns if c not in cpd]\n# wm[cp] = wm[cp].fillna('')\n# d = d.merge(wm, how='left', on=cpd)\n# d.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# d[wmf].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# google trends\ngt = pd.read_csv(path+'google_trends.csv')\ngt[cp] = gt[cp].fillna('')\ngt","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# since trends data lags behind a day or two, shift the date to make it contemporaneous\ngmax = gt.Date.max()\ngmax = datetime.strptime(gmax,'%Y-%m-%d').date()\ngoff = (dmax - gmax).days\nprint(dmax, gmax, goff)\ngt['Date'] = (pd.to_datetime(gt.Date) + timedelta(goff)).dt.strftime('%Y-%m-%d')\ngt['google_covid'] = gt['coronavirus'] + gt['covid-19'] + gt['covid19']\ngt.drop(['coronavirus','covid-19','covid19'], axis=1, inplace=True)\ngoogle = ['google_covid']\ngt","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d = d.merge(gt, how='left', on=['Country_Region','Province_State','Date'])\nd","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d['google_covid'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# merge country info\ncountry = pd.read_csv(path+'covid19countryinfo1.csv')\n# country[\"pop\"] = country[\"pop\"].str.replace(\",\",\"\").astype(float)\ncountry","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"country.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# first merge by country\nd = d.merge(country.loc[country.medianage.notnull(),['country','pop','testpop','medianage']],\n            how='left', left_on='Country_Region', right_on='country')\nd","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# then merge by province\nc1 = country.loc[country.medianage.isnull(),['country','pop','testpop']]\nprint(c1.shape)\nc1.columns = ['Province_State','pop1','testpop1']\n# d.update(c1)\nd = d.merge(c1,how='left',on='Province_State')\nd.loc[d.pop1.notnull(),'pop'] = d.loc[d.pop1.notnull(),'pop1']\nd.loc[d.testpop1.notnull(),'testpop'] = d.loc[d.testpop1.notnull(),'testpop1']\nd.drop(['pop1','testpop1'], axis=1, inplace=True)\nprint(d.shape)\nprint(d.loc[(d.Date=='2020-03-25') & (d['Province_State']=='New York')])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# testing data time series, us states only, would love to have this for all countries\nct = pd.read_csv(path+'states_daily_4pm_et.csv')\nsi = pd.read_csv(path+'states_info.csv')\nsi = si.rename(columns={'name':'Province_State'})\nct = ct.merge(si[['state','Province_State']], how='left', on='state')\nct['Date'] = ct['date'].apply(str).transform(lambda x: '-'.join([x[:4], x[4:6], x[6:]]))\nct.loc[ct.Province_State=='US Virgin Islands','Province_State'] = 'Virgin Islands'\nct.loc[ct.Province_State=='District Of Columbia','Province_State'] = 'District of Columbia'\npd.set_option('display.max_rows', 20)\nct\n# ct = ct['Date','state','total']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"ckeep = ['positive','negative','totalTestResults']\nfor c in ckeep: ct[c] = np.log1p(ct[c])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d = d.merge(ct[['Province_State','Date']+ckeep], how='left',\n            on=['Province_State','Date'])\nd","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# weather data from from davide bonine\nw = pd.read_csv(path+'training_data_with_weather_info_week_3.csv')\nw.drop(['Id','ConfirmedCases','Fatalities','country+province','day_from_jan_first'], axis=1, inplace=True)\nw[cp] = w[cp].fillna('')\nwf = list(w.columns[5:])\nw","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"w.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# replace values\nw['ah'] = w['ah'].replace(to_replace={np.inf:np.nan})\nw['wdsp'] = w['wdsp'].replace(to_replace={999.9:np.nan})\nw['prcp'] = w['prcp'].replace(to_replace={99.99:np.nan})\nw.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"w[['Country_Region','Province_State']].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"w[['Country_Region','Province_State']].drop_duplicates().shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# since weather data may lag behind a day or two, adjust the date to make it contemporaneous\nwmax = w.Date.max()\nwmax = datetime.strptime(wmax,'%Y-%m-%d').date()\nwoff = (dmax - wmax).days\nprint(dmax, wmax, woff)\nw['Date'] = (pd.to_datetime(w.Date) + timedelta(woff)).dt.strftime('%Y-%m-%d')\nw","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# merge Lat and Long for all times and the time-varying weather data based on date\nd = d.merge(w[cp+['Lat','Long']].drop_duplicates(), how='left', on=cp)\nw.drop(['Lat','Long'],axis=1,inplace=True)\nd = d.merge(w, how='left', on=cpd)\nd","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# combine ecdc and nytimes data as extra y0 and y1\necdc = pd.read_csv(path+'ecdc.csv', encoding = 'latin')\necdc","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# https://opendata.ecdc.europa.eu/covid19/casedistribution/csv\necdc['Date'] = pd.to_datetime(ecdc[['year','month','day']]).dt.strftime('%Y-%m-%d')\necdc = ecdc.rename(mapper={'countriesAndTerritories':'Country_Region'}, axis=1)\necdc['Country_Region'] = ecdc['Country_Region'].replace('_',' ',regex=True)\necdc['Province_State'] = ''\necdc['cc'] = ecdc.groupby(cp)['cases'].cummax()\necdc['extra_y0'] = np.log1p(ecdc.cc)\necdc['cd'] = ecdc.groupby(cp)['deaths'].cummax()\necdc['extra_y1'] = np.log1p(ecdc.cd)\necdc = ecdc[cpd + ['extra_y0','extra_y1']]\necdc[::63]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"dmax","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"ecdc = ecdc[(ecdc.Date >= '2020-01-22')]\necdc","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# https://github.com/nytimes/covid-19-data\nnyt = pd.read_csv(path+'us-states.csv')\nnyt['extra_y0'] = np.log1p(nyt.cases)\nnyt['extra_y1'] = np.log1p(nyt.deaths)\nnyt['Country_Region'] = 'US'\nnyt = nyt.rename(mapper={'date':'Date','state':'Province_State'},axis=1)\nnyt.drop(['fips','cases','deaths'],axis=1,inplace=True)\nnyt","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"extra = pd.concat([ecdc,nyt], sort=True)\nextra","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d = d.merge(extra, how='left', on=cpd)\nd","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# # enforce monotonicity\n# d = d.sort_values(['Loc','Date']).reset_index(drop=True)\n# for y in yv:\n#     ey = 'extra_'+y\n#     d[ey] = d[ey].fillna(0.)\n#     d[ey] = d.groupby('Loc')[ey].cummax()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d[['y0','y1','extra_y0','extra_y1']].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# impute us state data prior to march 10\nfor i in range(ny):\n    ei = 'extra_'+yv[i]\n    qm = (d.Country_Region == 'US') & (d.Date < '2020-03-10') & (d[ei].notnull())\n    print(i,sum(qm))\n    d.loc[qm,yv[i]] = d.loc[qm,ei]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d[['y0','y1']].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.plot(d.loc[d.Province_State=='New York','y0'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# log rates\nd['rate0'] = d.y0 - np.log(d['pop'])\nd['rate1'] = d.y1 - np.log(d['pop'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# recovered data from hopkins, https://github.com/CSSEGISandData/COVID-19\nrecovered = pd.read_csv(path+'time_series_covid19_recovered_global.csv')\nrecovered = recovered.rename(mapper={'Country/Region':'Country_Region','Province/State':'Province_State'}, axis=1)\nrecovered[cp] = recovered[cp].fillna('')\nrecovered = recovered.drop(['Lat','Long'], axis=1)\nrecovered","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# replace US row with identical rows for every US state\nusp = d.loc[d.Country_Region=='US','Province_State'].unique()\nprint(usp, len(usp))\nrus = recovered[recovered.Country_Region=='US']\nrus","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rus = rus.reindex(np.repeat(rus.index.values,len(usp)))\nrus.loc[:,'Province_State'] = usp\nrus","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"recovered =  recovered[recovered.Country_Region!='US']\nrecovered = pd.concat([recovered,rus]).reset_index(drop=True)\nrecovered","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# melt and merge\nrm = pd.melt(recovered, id_vars=cp, var_name='d', value_name='recov')\nrm","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rm['Date'] = pd.to_datetime(rm.d)\nrm.drop('d',axis=1,inplace=True)\nrm['Date'] = rm['Date'].dt.strftime('%Y-%m-%d')\nrm","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d = d.merge(rm, how='left', on=['Country_Region','Province_State','Date'])\nd","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d['recov'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# approximate US state recovery via proportion of confirmed cases\nd['ccsum'] = d.groupby(['Country_Region','Date'])['ConfirmedCases'].transform(lambda x: x.sum())\nd.loc[d.Country_Region=='US','recov'] = d.loc[d.Country_Region=='US','recov'] * \\\n                                        d.loc[d.Country_Region=='US','ConfirmedCases'] / \\\n                                        (d.loc[d.Country_Region=='US','ccsum'] + 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d.loc[:,'recov'] = np.log1p(d.recov)\n# d.loc[:,'recov'] = d['recov'].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# # enforce monotonicity\n# d = d.sort_values(['Loc','Date']).reset_index(drop=True)\n# d['recov'] = d['recov'].fillna(0.)\n# d['recov'] = d.groupby('Loc')['recov'].cummax()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d.loc[d.Province_State=='North Carolina','recov'][45:55]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d = d.sort_values(['Loc','Date']).reset_index(drop=True)\nd.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# compute nearest neighbors\nregions = d[['Loc','Lat','Long']].drop_duplicates('Loc').reset_index(drop=True)\nregions","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# regions.to_csv('regions.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# knn max features\nk = kv[0]\nnn = NearestNeighbors(k)\nnn.fit(regions[['Lat','Long']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# first matrix is distances, second indices to nearest neighbors including self\n# note two cruise ships are replicated and have identical lat, long values\nknn = nn.kneighbors(regions[['Lat','Long']])\nknn","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"ns = d['Loc'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# time series matrix\nky = d['y0'].values.reshape(ns,-1)\nprint(ky.shape)\n\nprint(ky[0])\n\n# use knn indices to create neighbors\nknny = ky[knn[1]]\nprint(knny.shape)\n\nknny = knny.transpose((0,2,1)).reshape(-1,k)\nprint(knny.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# knn max features\nnk = len(kv)\nkp = []\nkd = []\nns = regions.shape[0]\nfor k in kv:\n    nn = NearestNeighbors(k)\n    nn.fit(regions[['Lat','Long']])\n    knn = nn.kneighbors(regions[['Lat','Long']])\n    kp.append('knn'+str(k)+'_')\n    kd.append('kd'+str(k)+'_')\n    for i in range(ny):\n        yi = 'y'+str(i)\n        kc = kp[-1]+yi\n        # time series matrix\n        ky = d[yi].values.reshape(ns,-1)\n        # use knn indices to create neighbor matrix\n        km = ky[knn[1]].transpose((0,2,1)).reshape(-1,k)\n        \n        # take maximum value over all neighbors to approximate spreading\n        d[kc] = np.amax(km, axis=1)\n        print(d[kc].describe())\n        print()\n        \n        # distance to max\n        kc = kd[-1]+yi\n        ki = np.argmax(km, axis=1).reshape(ns,-1)\n        kw = np.zeros_like(ki).astype(float)\n        # inefficient indexing, surely some way to do it faster\n        for j in range(ns): \n            kw[j] = knn[0][j,ki[j]]\n        d[kc] = kw.flatten()\n        print(d[kc].describe())\n        print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"ki[j]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# range of dates for training\n# dates = d[~d.y0.isnull()]['Date'].drop_duplicates()\ndates = d[d.y0.notnull()]['Date'].drop_duplicates()\ndates","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# correlations for knn features\ncols = []\nfor i in range(ny):\n    yi = yv[i]\n    cols.append(yi)\n    for k in kp:\n        cols.append(k+yi)\nd.loc[:,cols].corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d['Date'] = pd.to_datetime(d['Date'])\nd['Date'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# days since beginning\n# basedate = train['Date'].min()\n# train['dint'] = train.apply(lambda x: (x.name.to_datetime() - basedate).days, axis=1)\nd['dint'] = (d['Date'] - d['Date'].min()).dt.days\nd['dint'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# reference days since exp(j)th occurrence\nfor i in range(ny):\n    \n    for j in range(3):\n\n        ij = str(i)+'_'+str(j)\n        \n        cut = 2**j if i==0 else j\n        \n        qd1 = (d[yv[i]] > cut) & (d[yv[i]].notnull())\n        d1 = d.loc[qd1,['Loc','dint']]\n        # d1.shape\n        # d1.head()\n\n        # get min for each location\n        d1['dmin'] = d1.groupby('Loc')['dint'].transform(lambda x: x.min())\n        # dintmax = d1['dint'].max()\n        # print(i,j,'dintmax',dintmax)\n        # d1.head()\n\n        d1.drop('dint',axis=1,inplace=True)\n        d1 = d1.drop_duplicates()\n        d = d.merge(d1,how='left',on=['Loc'])\n \n        # if dmin is missing then the series had no occurrences in the training set\n        # go ahead and assume there will be one at the beginning of the test period\n        # the average time between first occurrence and first death is 14 days\n        # if j==0: d[dmi] = d[dmi].fillna(dintmax + 1 + i*14)\n\n        # ref day is days since dmin, must clip at zero to avoid leakage\n        d['ref_day'+ij] = np.clip(d.dint - d.dmin, 0, None)\n        d['ref_day'+ij] = d['ref_day'+ij].fillna(0)\n        d.drop('dmin',axis=1,inplace=True)\n\n        # asymptotic curve may bin differently\n        d['recip_day'+ij] = 1 / (1 + (1 + d['ref_day'+ij])**(-1.0))\n    \n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d['dint'].value_counts().std()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# diffs and rolling means\n# note lags are taken dynamically at run time\ne = 1\nr = 5\nfor i in range(ny):\n    yi = 'y'+str(i)\n    dd = '_d'+str(e)\n    rr = '_r'+str(r)\n    \n    for j in range(3):\n        d[yi+'_d'+str(1+j)] = d.groupby('Loc')[yi].transform(lambda x: x.diff(1+j))\n    \n    d[yi+rr] = d.groupby('Loc')[yi].transform(lambda x: x.rolling(r).mean())\n    d['rate'+str(i)+dd] = d.groupby('Loc')['rate'+str(i)].transform(lambda x: x.diff(e))\n    d['rate'+str(i)+rr] = d.groupby('Loc')['rate'+str(i)].transform(lambda x: x.rolling(r).mean())\n    d['extra_y'+str(i)+dd] = d.groupby('Loc')['extra_y'+str(i)].transform(lambda x: x.diff(e))\n    d['extra_y'+str(i)+rr] = d.groupby('Loc')['extra_y'+str(i)].transform(lambda x: x.rolling(r).mean())\n\n    for k in kp:\n        d[k+yi+dd] = d.groupby('Loc')[k+yi].transform(lambda x: x.diff(e))\n        d[k+yi+rr] = d.groupby('Loc')[k+yi].transform(lambda x: x.rolling(r).mean())\n\n    for k in kd:\n        d[k+yi+dd] = d.groupby('Loc')[k+yi].transform(lambda x: x.diff(e))\n        d[k+yi+rr] = d.groupby('Loc')[k+yi].transform(lambda x: x.rolling(r).mean())\n        \nvlist = ['recov'] + google + wf\n\nfor v in vlist:\n    d[v+dd] = d.groupby('Loc')[v].transform(lambda x: x.diff(e))\n    d[v+rr] = d.groupby('Loc')[v].transform(lambda x: x.rolling(r).mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# final sort before training\nd = d.sort_values(['Loc','dint']).reset_index(drop=True)\nd.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# initial continuous and categorical features\n# dogs = tfeats\n# ref_day0_0 is no longer leaky since every location has at least one confirmed case\ndogs = ['ref_day0_0']\ncats = ['Loc']\nprint(dogs, len(dogs))\nprint(cats, len(cats))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# one-hot encode categorical features\nohef = []\nfor i,c in enumerate(cats):\n    print(c, d[c].nunique())\n    ohe = pd.get_dummies(d[c], prefix=c)\n    ohec = [f.translate({ord(c): \"_\" for c in \" !@#$%^&*()[]{};:,./<>?\\|`~-=_+\"}) for f in list(ohe.columns)]\n    ohe.columns = ohec\n    d = pd.concat([d,ohe],axis=1)\n    ohef = ohef + ohec","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d['Loc_US_North_Carolina'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d['Loc_US_Colorado'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# must start cas server from gevmlax02 before running this cell\n# ssh rdcgrd001 /opt/vb025/laxnd/TKGrid/bin/caslaunch stat -mode mpp -cfg /u/sasrdw/config.lua\nif 'cas' in booster:\n    from swat import *\n    s = CAS('rdcgrd001.unx.sas.com', 16695)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# boosting hyperparameters\nparams = {}\n\n# from vopani\nSEED = 345\nLGB_PARAMS = {\"objective\": \"regression\",\n              \"num_leaves\": 5,\n              \"learning_rate\": 0.013,\n              \"bagging_fraction\": 0.91,\n              \"feature_fraction\": 0.81,\n              \"reg_alpha\": 0.13,\n              \"reg_lambda\": 0.13,\n              \"metric\": \"rmse\",\n              \"seed\": SEED\n             }\n\nparams[('lgb','y0')] = LGB_PARAMS\nparams[('lgb','y1')] = LGB_PARAMS\n# params[('lgb','y0')] = {'lambda_l2': 1.9079933811271934, 'max_depth': 5}\n# params[('lgb','y1')] = {'lambda_l2': 1.690407455211948, 'max_depth': 3}\nparams[('xgb','y0')] = {'lambda_l2': 1.9079933811271934, 'max_depth': 5}\nparams[('xgb','y1')] = {'lambda_l2': 1.690407455211948, 'max_depth': 3}\nparams[('ctb','y0')] = {'l2_leaf_reg': 1.9079933811271934, 'max_depth': 5}\nparams[('ctb','y1')] = {'l2_leaf_reg': 1.690407455211948, 'max_depth': 3}","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# booster = ['rdg','lgb','xgb','ctb']\n# booster = ['lgb','xgb']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# single horizon validation using one day at a time for 28 days\nnb = len(booster)\nnls = np.zeros((nhorizon-skip,ny,nb+tf2))\nrallv = np.zeros((nhorizon-skip,ny,nb))\niallv = np.zeros((nhorizon-skip,ny,nb)).astype(int)\nyallv = []\npallv = []\nimps = []\n \n# loop over horizons\nfor horizon in range(1+skip,nhorizon+1):\n# for horizon in range(4,5):\n    \n    print()\n#     print('*'*20)\n#     print(f'horizon {horizon}')\n#     print('*'*20)\n    \n    gc.collect()\n    \n    hs = str(horizon)\n    if horizon < 10: hs = '0' + hs\n    \n    # build lists of features\n    lags = []\n    # must lag reference days to avoid validation leakage\n    for i in range(ny):\n        for j in range(3):\n            # omit ref_day0_0 since it is no longer leaky\n            if (i > 0) | (j > 0): lags.append('ref_day'+str(i)+'_'+str(j))\n            \n    # lag all time-varying features\n    for i in range(ny):\n        yi = 'y'+str(i)\n        lags.append(yi)\n        lags.append('extra_'+yi)\n        lags.append('rate'+str(i))\n        for j in range(3):\n            lags.append(yi+'_d'+str(1+j))\n        lags.append('extra_'+yi+dd)\n        lags.append('rate'+str(i)+dd)\n        lags.append(yi+rr)\n        lags.append('extra_'+yi+rr)\n        lags.append('rate'+str(i)+rr)\n        for k in kp:\n            lags.append(k+yi)\n            lags.append(k+yi+dd)\n            lags.append(k+yi+rr)\n        for k in kd:\n            lags.append(k+yi)\n            lags.append(k+yi+dd)\n            lags.append(k+yi+rr)\n       \n    lags.append('recov')\n    \n#     lags = lags + wmf + google + wf + ckeep\n#     lags = lags + google + wf + ckeep\n\n    lags = lags + google + wf + ckeep\n    \n#     cinfo = ['pop', 'tests', 'testpop', 'density', 'medianage',\n#        'urbanpop', 'hospibed', 'smokers']\n    cinfo0 = ['testpop']\n    cinfo1 = ['testpop','medianage']\n    \n    f0 = dogs + lags + cinfo0 + ohef\n    f1 = dogs + lags + cinfo1 + ohef\n    \n    # remove some features based on validation experiments\n#     f0 = [f for f in f0 if not f.startswith('knn11') and not f.startswith('kd') \\\n#          and not f.startswith('rate') and not f.endswith(dd) and not f.endswith(rr)]\n\n    f0 = [f for f in f0 if not f.startswith('knn11') and not f.startswith('kd11')]\n    f1 = [f for f in f1 if not f.startswith('knn6') and not f.startswith('kd6')]\n    \n    # remove any duplicates\n    # f0 = list(set(f0))\n    # f1 = list(set(f1))\n    \n    features = []\n    features.append(f0)\n    features.append(f1)\n    \n    nf = []\n    for i in range(ny):\n        nf.append(len(features[i]))\n        # print(nf[i], features[i][:10])\n     \n    if val_scheme == 'forward':\n        # ddate is the last day of validation training\n        # training data stays constant\n        ddate = dmax - timedelta(days=nhorizon)\n        qtrain = d['Date'] <= ddate.isoformat()\n        # validation day moves forward\n        vdate = ddate + timedelta(days=horizon)\n        qval = d['Date'] == vdate.isoformat()\n        # lag day is last day of training\n        qvallag = d['Date'] == ddate.isoformat()\n        # for saving predictions into main table\n        qsave = qval\n    else: \n        # ddate is the last day of validation training\n        # training data moves backwards\n        ddate = dmax - timedelta(days=horizon)\n        qtrain = d['Date'] <= ddate.isoformat()\n        # validate using the last day with data\n        # validation day stays constant\n        vdate = dmax\n        qval = d['Date'] == vdate.isoformat()\n        # lag day is last day of training\n        qvallag = d['Date'] == ddate.isoformat()\n        # for saving predictions into table, expected rise going backwards\n        sdate = dmax - timedelta(days=horizon-1)\n        qsave = d['Date'] == sdate.isoformat()\n\n    \n    x_train = d[qtrain].copy()\n    # make y training data monotonic nondecreasing\n    y_train = []\n    yd_train = []    \n    for i in range(ny):\n        y_train.append(pd.Series(d.loc[qtrain,['Loc',yv[i]]].groupby('Loc')[yv[i]].cummax()).values)\n        ylag = pd.Series(d.loc[qtrain,['Loc',yv[i]]].groupby('Loc')[yv[i]].cummax().shift(horizon).values)\n        yd_train.append(y_train[i] - ylag)\n        # yd_train[i] = yd_train[i].fillna(0)\n        yd_train[i] = np.nan_to_num(yd_train[i])\n        yd_train[i] = np.clip(yd_train[i], 0, None)\n        \n    x_val = d[qval].copy()\n    \n#     y_val = [d.loc[qval,'y0'].copy(), d.loc[qval,'y1'].copy()]\n#     y_vallag = [d.loc[qvallag,'y0'].copy(), d.loc[qvallag,'y1'].copy()]\n    y_val = [d.loc[qval,'y0'].values, d.loc[qval,'y1'].values]\n    y_vallag = [d.loc[qvallag,'y0'].values, d.loc[qvallag,'y1'].values]\n    yd_val = [y_val[0] - y_vallag[0], y_val[1] - y_vallag[1]]\n    yallv.append(y_val)\n    \n    # lag features\n    x_train.loc[:,lags] = x_train.groupby('Loc')[lags].transform(lambda x: x.shift(horizon))\n    x_val.loc[:,lags] = d.loc[qvallag,lags].values\n\n    print()\n    print(horizon, 'x_train', x_train.shape)\n    print(horizon, 'x_val', x_val.shape)\n    \n    if train_full:\n        \n        qfull = (d['Date'] <= tmax)\n        \n        tdate = dmax + timedelta(days=horizon)\n        qtest = d['Date'] == tdate.isoformat()\n        qtestlag = d['Date'] == dmax.isoformat()\n    \n        x_full = d[qfull].copy()\n        \n        # make y training data monotonic nondecreasing\n        y_full = []\n        yd_full = []\n        for i in range(ny):\n            y_full.append(pd.Series(d.loc[qfull,['Loc',yv[i]]].groupby('Loc')[yv[i]].cummax()).values)\n            ylag = pd.Series(d.loc[qfull,['Loc',yv[i]]].groupby('Loc')[yv[i]].cummax().shift(horizon).values)\n            yd_full.append(y_full[i] - ylag)\n            # yd_full[i] = yd_full[i].fillna(0)\n            yd_full[i] = np.nan_to_num(yd_full[i])\n            yd_full[i] = np.clip(yd_full[i], 0, None)\n        \n        x_test = d[qtest].copy()\n        y_fulllag = [d.loc[qtestlag,'y0'].values, d.loc[qtestlag,'y1'].values]\n        \n        # lag features\n        x_full.loc[:,lags] = x_full.groupby('Loc')[lags].transform(lambda x: x.shift(horizon))\n        x_test.loc[:,lags] = d.loc[qtestlag,lags].values\n\n        print(horizon, 'x_full', x_full.shape)\n        print(horizon, 'x_test', x_test.shape)\n\n    train_set = []\n    val_set = []\n    ny = len(y_train)\n\n#     for i in range(ny):\n#         train_set.append(xgb.DMatrix(x_train[features[i]], y_train[i]))\n#         val_set.append(xgb.DMatrix(x_val[features[i]], y_val[i]))\n\n    gc.collect()\n\n    # loop over multiple targets\n    mod = []\n    pred = []\n    rez = []\n    iters = []\n    \n    for i in range(ny):\n#     for i in range(1):\n        print()\n        print('*'*40)\n        print(f'horizon {horizon} {yv[i]} {ynames[i]} {vdate}')\n        print('*'*40)\n        \n        # use catboost only for y1\n        # nb = 2 if i==0 else 3\n       \n        # matrices to store predictions\n        vpm = np.zeros((x_val.shape[0],nb))\n        tpm = np.zeros((x_test.shape[0],nb))\n        \n        # x_train[features[i]] = x_train[features[i]].fillna(0)\n        # x_val[features[i]] = x_val[features[i]].fillna(0)\n        \n        for b in range(nb):\n            \n            restore_features = False\n                       \n            if booster[b] == 'cas':\n                \n                x_train['Partition'] = 1\n                x_val['Partition'] = 0\n                x_cas_all = pd.concat([x_train, x_val], axis=0)\n                # make copy of target since it is also used for lags\n                x_cas_all['target'] = pd.concat([y_train[i], y_val[i]], axis=0).values\n                s.upload(x_cas_all, casout=\"x_cas_val\")\n\n                target = 'target'\n                inputs = features[i]\n                inputs.append(target)\n\n                s.loadactionset(\"autotune\")\n                res=s.autotune.tuneGradientBoostTree (\n                    trainOptions = {\n                        \"table\":{\"name\":'x_cas_val',\"where\":\"Partition=1\"},\n                        \"target\":target,\n                        \"inputs\":inputs,\n                        \"casOut\":{\"name\":\"model\", \"replace\":True}\n                    },\n                    scoreOptions = {\n                        \"table\":{\"name\":'x_cas_val', \"where\":\"Partition=0\"},\n                        \"model\":{\"name\":'model'},\n                        \"casout\":{\"name\":\"x_valid_preds\",\"replace\":True},\n                        \"copyvars\": ['Id','Loc','Date']\n                    },\n                    tunerOptions = {\n                        \"seed\":54321,  \n                        \"objective\":\"RASE\", \n                        \"userDefinedPartition\":True \n                    }\n                )\n                print()\n                print(res.TunerSummary)\n                print()\n                print(res.BestConfiguration)        \n\n                TunerSummary=pd.DataFrame(res['TunerSummary'])\n                TunerSummary[\"Value\"]=pd.to_numeric(TunerSummary[\"Value\"])\n                BestConf=pd.DataFrame(res['BestConfiguration'])\n                BestConf[\"Value\"]=pd.to_numeric(BestConf[\"Value\"])\n                vpt = s.CASTable(\"x_valid_preds\").to_frame()\n                #FG: resort the CAS predictions by Id\n                vpt = vpt.sort_values(['Loc','Date']).reset_index(drop=True)\n                vp = vpt['P_target'].values\n\n                s.dropTable(\"x_cas_val\")\n                s.dropTable(\"x_valid_preds\")\n                \n            else:\n                # scikit interface automatically uses best model for predictions\n                # params[(booster[b],yv[i])]['n_estimators'] = 5000\n                \n                kwargs = {'verbose':False}\n                if booster[b]=='lgb':\n                    params[(booster[b],yv[i])]['n_estimators'] = 100\n                    model = lgb.LGBMRegressor(**params[(booster[b],yv[i])]) \n                elif booster[b]=='xgb':\n                    params[(booster[b],yv[i])]['n_estimators'] = 50\n                    params[(booster[b],yv[i])]['base_score'] = np.mean(y_train[i])\n                    model = xgb.XGBRegressor(**params[(booster[b],yv[i])])\n                elif booster[b]=='ctb':\n                    params[(booster[b],yv[i])]['n_estimators'] = 350\n                    # change feature list for categorical features\n                    features_save = features[i].copy()\n                    features[i] = [f for f in features[i] if not f.startswith('Loc_')] + ['Loc']\n                    params[(booster[b],yv[i])]['cat_features'] = ['Loc']\n                    restore_features = True\n                    model = ctb.CatBoostRegressor(**params[(booster[b],yv[i])])\n                elif booster[b]=='rdg':\n                    # alpha from cpmp\n                    model = Ridge(alpha=3, fit_intercept=True)\n                    kwargs = {}\n                else:\n                    raise ValueError(f'Unrecognized booster {booster[b]}')\n                    \n                xtrn = x_train[features[i]].copy()\n                xval = x_val[features[i]].copy()\n                if booster[b]=='rdg':\n                    s = StandardScaler()\n                    xtrn = s.fit_transform(xtrn)\n                    xval = s.transform(xval)\n                    xtrn = np.nan_to_num(xtrn)\n                    xval = np.nan_to_num(xval)\n                    xtrn = pd.DataFrame(xtrn, columns=features[i])\n                    xval = pd.DataFrame(xval, columns=features[i])\n                \n                # fit cumulative target\n                model.fit(xtrn, y_train[i],\n#                                   eval_set=[(x_train[features[i]], yd_train[i]),\n#                                             (x_val[features[i]], yd_val[i])],\n#                                   eval_set=[(x_val[features[i]], yd_val[i])],\n#                                   eval_set=[(x_val[features[i]], y_val[i])],\n#                                   early_stopping_rounds=30,\n                                    **kwargs\n                         )\n\n                vp = model.predict(xval)\n\n                # fit diffs from last training y\n                kwargs = {'verbose':False}\n                if booster[b]=='lgb':\n                    # params[(booster[b],yv[i])]['n_estimators'] = 100\n                    model = lgb.LGBMRegressor(**params[(booster[b],yv[i])]) \n                elif booster[b]=='xgb':\n                    params[(booster[b],yv[i])]['n_estimators'] = 50\n                    params[(booster[b],yv[i])]['base_score'] = np.mean(yd_train[i])\n                    model = xgb.XGBRegressor(**params[(booster[b],yv[i])])\n                elif booster[b]=='ctb':\n                    params[(booster[b],yv[i])]['n_estimators'] = 350\n                    # hack for categorical features, ctb must be last in booster list\n                    # features[i] = [f for f in features[i] if not f.startswith('Loc_')] + ['Loc']\n                    # params[(booster[b],yv[i])]['cat_features'] = ['Loc']\n                    model = ctb.CatBoostRegressor(**params[(booster[b],yv[i])])\n                elif booster[b]=='rdg':\n                    # alpha from cpmp\n                    model = Ridge(alpha=3, fit_intercept=True)\n                    kwargs = {}\n                else:\n                    raise ValueError(f'Unrecognized booster {booster[b]}')\n\n                model.fit(xtrn, yd_train[i],\n#                                   eval_set=[(x_train[features[i]], yd_train[i]),\n#                                             (x_val[features[i]], yd_val[i])],\n#                                   eval_set=[(x_val[features[i]], yd_val[i])],\n#                                   eval_set=[(x_val[features[i]], y_val[i])],\n#                                   early_stopping_rounds=30,\n                                  **kwargs\n                         )\n\n                vpd = model.predict(xval)\n                vpd = np.clip(vpd,0,None)\n                vpd = y_vallag[i] + vpd\n                \n                # blend two predictions based on horizon\n                alpha = 0.1 + 0.8*(horizon-1)/29\n                vp = alpha*vp + (1-alpha)*vpd\n\n#                 iallv[horizon-skip-1,i,b] = model._best_iteration if booster[b]=='lgb' else \\\n#                                             model.best_iteration if booster[b]=='xgb' else \\\n#                                             model.best_iteration_\n\n                gain = np.abs(model.coef_) if booster[b]=='rdg' else model.feature_importances_\n        #         gain = model.get_score(importance_type='gain')\n        #         split = model.get_score(importance_type='weight')   \n            #     gain = model.feature_importance(importance_type='gain')\n            #     split = model.feature_importance(importance_type='split').astype(float)  \n            #     imp = pd.DataFrame({'feature':features,'gain':gain,'split':split})\n                imp = pd.DataFrame({'feature':features[i],'gain':gain})\n        #         imp = pd.DataFrame({'feature':features[i]})\n        #         imp['gain'] = imp['feature'].map(gain)\n        #         imp['split'] = imp['feature'].map(split)\n\n                imp.set_index(['feature'],inplace=True)\n\n                imp.gain /= np.sum(imp.gain)\n        #         imp.split /= np.sum(imp.split)\n\n                imp.sort_values(['gain'], ascending=False, inplace=True)\n\n                print()\n                print(imp.head(n=10))\n                # print(imp.shape)\n\n                imp.reset_index(inplace=True)\n                imp['horizon'] = horizon\n                imp['target'] = yv[i]\n                imp['set'] = 'valid'\n                imp['booster'] = booster[b]\n\n                mod.append(model)\n                imps.append(imp)\n                \n            # china rule, last observation carried forward, set to zero here\n            qcv = (x_val['Country_Region'] == 'China') & \\\n                  (x_val['Province_State'] != 'Hong Kong') & \\\n                  (x_val['Province_State'] != 'Macau')\n            vp[qcv] = 0.0\n\n            # make sure horizon 1 prediction is not smaller than first lag\n            # because we know series is monotonic\n            # if horizon==1+skip:\n            if True:\n                a = np.zeros((len(vp),2))\n                a[:,0] = vp\n                # note yv is lagged here\n                a[:,1] = x_val[yv[i]].values\n                vp = np.nanmax(a,axis=1)\n            \n            val_score = np.sqrt(mean_squared_error(vp, y_val[i]))\n            vpm[:,b] = vp\n            \n            print()\n            print(f'{booster[b]} validation rmse {val_score:.6f}')\n            rallv[horizon-skip-1,i,b] = val_score\n\n            gc.collect()\n    \n#             break\n\n            if train_full:\n                \n                print()\n                print(f'{booster[b]} training with full data and predicting', tdate.isoformat())\n                    \n                # x_full[features[i]] = x_full[features[i]].fillna(0)\n                # x_test[features[i]] = x_test[features[i]].fillna(0)\n        \n                if booster[b] == 'cas':\n                    \n                    x_full['target'] = y_full[i].values\n                    s.upload(x_full, casout=\"x_full\")\n                    # use hyperparameters from validation fit\n                    s.loadactionset(\"decisionTree\")\n                    result = s.gbtreetrain(\n                        table={\"name\":'x_full'},\n                        target=target,\n                        inputs= inputs,\n                        varimp=True,\n                        ntree=BestConf.iat[0,2], \n                        m=BestConf.iat[1,2],\n                        learningRate=BestConf.iat[2,2],\n                        subSampleRate=BestConf.iat[3,2],\n                        lasso=BestConf.iat[4,2],\n                        ridge=BestConf.iat[5,2],\n                        nbins=BestConf.iat[6,2],\n                        maxLevel=BestConf.iat[7,2],\n                        #quantileBin=True,\n                        seed=326146718,\n                        #savestate={\"name\":\"aStore\",\"replace\":True}\n                        casOut={\"name\":'fullmodel', \"replace\":True}\n                        ) \n\n                    s.upload(x_test, casout=\"x_test_cas\")\n\n                    s.decisionTree.gbtreeScore(\n                        modelTable={\"name\":\"fullmodel\"},        \n                        table={\"name\":\"x_test_cas\"},\n                        casout={\"name\":\"x_test_preds\",\"replace\":True},\n                        copyvars= ['Loc','Date']\n                        ) \n                    # save test predictions back into main table\n                    forecast = s.CASTable(\"x_test_preds\").to_frame()\n                    forecast = forecast.sort_values(['Loc','Date']).reset_index(drop=True)\n                    tp = forecast['_GBT_PredMean_'].values\n                    \n                    s.dropTable(\"x_full\")\n                    s.dropTable(\"x_test_cas\")\n                     \n                else:\n                    \n                    # use number of iterations from validation fit\n                    kwargs = {'verbose':False}\n                    # params[(booster[b],yv[i])]['n_estimators'] = iallv[horizon-skip-1,i,b]\n                    if booster[b]=='lgb':\n                        model = lgb.LGBMRegressor(**params[(booster[b],yv[i])])\n                    elif booster[b]=='xgb':\n                        params[(booster[b],yv[i])]['base_score'] = np.mean(y_full[i])\n                        model = xgb.XGBRegressor(**params[(booster[b],yv[i])])\n                    elif booster[b]=='ctb':\n                        model = ctb.CatBoostRegressor(**params[(booster[b],yv[i])])\n                    elif booster[b]=='rdg':\n                        # alpha from cpmp\n                        model = Ridge(alpha=3, fit_intercept=True)\n                        kwargs = {}\n                    else:\n                        raise ValueError(f'Unrecognized booster {booster[b]}')\n                    \n                    xfull = x_full[features[i]].copy()\n                    xtest = x_test[features[i]].copy()\n                    if booster[b]=='rdg':\n                        s = StandardScaler()\n                        xfull = s.fit_transform(xfull)\n                        xtest = s.transform(xtest)\n                        xfull = np.nan_to_num(xfull)\n                        xtest = np.nan_to_num(xtest)\n                        xfull = pd.DataFrame(xfull, columns=features[i])\n                        xtest = pd.DataFrame(xtest, columns=features[i])\n                        \n                    model.fit(xfull, y_full[i], **kwargs)\n                    \n                    # params[(booster[b],yv[i])]['n_estimators'] = 5000\n\n                    tp = model.predict(xtest)\n                \n                    # use number of iterations from validation fit\n                    # params[(booster[b],yv[i])]['n_estimators'] = iallv[horizon-skip-1,i,b]\n                    kwargs = {'verbose':False}\n                    if booster[b]=='lgb':\n                        model = lgb.LGBMRegressor(**params[(booster[b],yv[i])])\n                    elif booster[b]=='xgb':\n                        params[(booster[b],yv[i])]['base_score'] = np.mean(yd_full[i])\n                        model = xgb.XGBRegressor(**params[(booster[b],yv[i])])\n                    elif booster[b]=='ctb':\n                        model = ctb.CatBoostRegressor(**params[(booster[b],yv[i])])\n                    elif booster[b]=='rdg':\n                        # alpha from cpmp\n                        model = Ridge(alpha=3, fit_intercept=True)\n                        kwargs = {}\n                    else:\n                        raise ValueError(f'Unrecognized booster {booster[b]}')\n                    \n                    # model.fit(x_full[features[i]], y_full[i], verbose=False)\n                    model.fit(xfull, yd_full[i], **kwargs)\n                    \n                    # params[(booster[b],yv[i])]['n_estimators'] = 5000\n\n                    tpd = model.predict(xtest)\n                    tpd = np.clip(tpd,0,None)\n                    tpd = y_fulllag[i] + tpd\n                    \n                    tp = alpha*tp + (1-alpha)*tpd\n                \n                    gain = np.abs(model.coef_) if booster[b]=='rdg' else model.feature_importances_\n            #         gain = model.get_score(importance_type='gain')\n            #         split = model.get_score(importance_type='weight')   \n                #     gain = model.feature_importance(importance_type='gain')\n                #     split = model.feature_importance(importance_type='split').astype(float)  \n                #     imp = pd.DataFrame({'feature':features,'gain':gain,'split':split})\n                    imp = pd.DataFrame({'feature':features[i],'gain':gain})\n            #         imp = pd.DataFrame({'feature':features[i]})\n            #         imp['gain'] = imp['feature'].map(gain)\n            #         imp['split'] = imp['feature'].map(split)\n\n                    imp.set_index(['feature'],inplace=True)\n\n                    imp.gain /= np.sum(imp.gain)\n            #         imp.split /= np.sum(imp.split)\n\n                    imp.sort_values(['gain'], ascending=False, inplace=True)\n\n                    print()\n                    print(imp.head(n=10))\n                    # print(imp.shape)\n\n                    imp.reset_index(inplace=True)\n                    imp['horizon'] = horizon\n                    imp['target'] = yv[i]\n                    imp['set'] = 'full'\n                    imp['booster'] = booster[b]\n\n                    imps.append(imp)\n\n                # china rule, last observation carried forward, set to zero here\n                qct = (x_test['Country_Region'] == 'China') & \\\n                      (x_test['Province_State'] != 'Hong Kong') & \\\n                      (x_test['Province_State'] != 'Macau')\n                tp[qct] = 0.0\n\n                # make sure first horizon prediction is not smaller than first lag\n                # because we know series is monotonic\n                # if horizon==1+skip:\n                if True:\n                    a = np.zeros((len(tp),2))\n                    a[:,0] = tp\n                    # note yv is lagged here\n                    a[:,1] = x_test[yv[i]].values\n                    tp = np.nanmax(a,axis=1)\n\n                tpm[:,b] = tp\n                \n                gc.collect()\n                \n            # restore feature list\n            if restore_features:\n                features[i] = features_save\n                restore_features = False\n                \n        # concat team predictions\n        if len(tfeats[i]):\n            vpm = np.concatenate([vpm,d.loc[qval,tfeats[i]].values], axis=1)\n            tpm = np.concatenate([tpm,d.loc[qtest,tfeats[i]].values], axis=1)\n                \n        # nonnegative least squares to estimate ensemble weights\n        # x, rnorm = nnls(vpm, y_val[i])\n        \n        # smooth weights by shrinking towards all equal\n        # x = (x + np.ones(3)/3.)/2\n        \n        # simple averaging to avoid overfitting\n        # drop ridge from y0\n        if i==0:\n            x = np.array([1., 1., 1., 0.])/3.\n        else:\n            nm = vpm.shape[1]\n            x = np.ones(nm)/nm\n        \n#         # drop catboost from y0\n#         if i == 0:  \n#             x = np.array([0.5, 0.5, 0.0])\n#         else: \n#             nm = vpm.shape[1]\n#             x = np.ones(nm)/nm\n\n        # smooth weights with rolling mean, ewma\n        # alpha = 0.1\n        # if horizon-skip > 1: x = alpha * x + (1 - alpha) * nls[horizon-skip-2,i]\n\n        nls[horizon-skip-1,i] = x\n        \n        val_pred = np.matmul(vpm, x)\n        test_pred = np.matmul(tpm, x)\n        \n        # china rule in case weights do not sum to 1\n        # val_pred[qcv] = vpm[:,0][qcv]\n        # test_pred[qcv] = tpm[:,0][qct]\n        \n        # save validation and test predictions back into main table\n        d.loc[qsave,yv[i]+'_pred'] = val_pred\n        d.loc[qtest,yv[i]+'_pred'] = test_pred\n\n        # ensemble validation score\n        # val_score = np.sqrt(rnorm/vpm.shape[0])\n        val_score = np.sqrt(mean_squared_error(val_pred, y_val[i]))\n        \n        rez.append(val_score)\n        pred.append(val_pred)\n\n    pallv.append(pred)\n    \n    # construct strings of nnls weights for printing\n    w0 = ''\n    w1 = ''\n    for b in range(nb+tf2):\n        w0 = w0 + f' {nls[horizon-skip-1,0,b]:.2f}'\n        w1 = w1 + f' {nls[horizon-skip-1,1,b]:.2f}'\n        \n    print()\n    print('         Validation RMSLE  ', ' '.join(booster), ' '.join(tfeats[0]))\n    print(f'{ynames[0]} \\t {rez[0]:.6f}  ' + w0)\n    print(f'{ynames[1]} \\t {rez[1]:.6f}  ' + w1)\n    print(f'Mean \\t \\t {np.mean(rez):.6f}')\n\n#     # break down RMSLE by day\n#     rp = np.zeros((2,7))\n#     for i in range(ny):\n#         for di in range(50,57):\n#             j = di - 50\n#             qf = x_val.dint == di\n#             rp[i,j] = np.sqrt(mean_squared_error(pred[i][qf], y_val[i][qf]))\n#             print(i,di,f'{rp[i,j]:.6f}')\n#         print(i,f'{np.mean(rp[i,:]):.6f}')\n#         plt.plot(rp[i])\n#         plt.title(ynames[i] + ' RMSLE')\n#         plt.show()\n        \n    # plot actual vs predicted\n    plt.figure(figsize=(10, 5))\n    for i in range(ny):\n        plt.subplot(1,2,i+1)\n        # plt.plot([0, 12], [0, 12], 'black')\n        plt.plot(pred[i], y_val[i], '.')\n        plt.xlabel('Predicted')\n        plt.ylabel('Actual')\n        plt.title(ynames[i])\n        plt.grid()\n    plt.show()\n        \n# save one big table of importances\nimpall = pd.concat(imps)\n\n# remove number suffixes from lag names to aid in analysis\n# impall['feature1'] = impall['feature'].replace(to_replace='lag..', value='lag', regex=True)\n\nos.makedirs('imp', exist_ok=True)\nfname = 'imp/' + mname + '_imp.csv'\nimpall.to_csv(fname, index=False)\nprint()\nprint(fname, impall.shape)\n\n# save scores and weights\nos.makedirs('rez', exist_ok=True)\nfname = 'rez/' + mname+'_rallv.npy'\nnp.save(fname, rallv)\nprint(fname, rallv.shape)\n\nfname = 'rez/' + mname+'_nnls.npy'\nnp.save(fname, nls)\nprint(fname, nls.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"x_train[features[i][42:52]].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"if 'cas' in booster: s.shutdown()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"tdate.isoformat()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rf = [f for f in features[0] if f.startswith('ref')]\nd[rf].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"np.mean(iallv, axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(10, 8))\nfor i in range(ny):\n    plt.subplot(2,2,1+i)\n    plt.plot(rallv[:,i])\n    plt.title(ynames[i] + ' RMSLE vs Horizon')\n    plt.grid()\n    plt.legend(booster)\n    \n    plt.subplot(2,2,3+i)\n    plt.plot(nls[:,i])\n    plt.title(ynames[i] + ' Ensemble Weights')\n    plt.grid()\n    plt.legend(booster+tfeats[i])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# compute validation rmsle\nm = 0\nlocs = d.loc[:,['Loc']].drop_duplicates().reset_index(drop=True)\n# locs = x_val.copy().reset_index(drop=True)\n# print(locs.shape)\ny_truea = []\ny_preda = []\n\nprint(f'# {mname}')\nfor i in range(ny):\n    y_true = []\n    y_pred = []\n    for j in range(nhorizon-skip):\n        y_true.append(yallv[j][i])\n        y_pred.append(pallv[j][i])\n    y_true = np.stack(y_true)\n    y_pred = np.stack(y_pred)\n    # print(y_pred.shape)\n    # make each series monotonic increasing\n    for j in range(y_pred.shape[1]): \n        y_pred[:,j] = np.maximum.accumulate(y_pred[:,j])\n    # copy updated predictions into main table\n    for horizon in range(1+skip,nhorizon+1):\n        vdate = ddate + timedelta(days=horizon)\n        qval = d['Date'] == vdate.isoformat()\n        d.loc[qval,yv[i]+'_pred'] = y_pred[horizon-1-skip]\n    rmse = np.sqrt(mean_squared_error(y_pred, y_true))\n    print(f'# {rmse:.6f}')\n    m += rmse/2\n    locs['rmse'+str(i)] = np.sqrt(np.mean((y_true-y_pred)**2, axis=0))\n    y_truea.append(y_true)\n    y_preda.append(y_pred)\nprint(f'# {m:.6f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# gbt3n\n# 1.848530\n# 1.452883\n# 1.650706","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"ddate","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# sort to find worst predictions of y0\nlocs = locs.sort_values('rmse0', ascending=False)\nlocs[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# plot worst fits of y0\nfor i in range(5):\n    li = locs.index[i]\n    plt.plot(y_truea[0][:,li])\n    plt.plot(y_preda[0][:,li])\n    plt.title(locs.loc[li,'Loc'])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# plt.plot(d.loc[d.Loc=='Belgium','y0'][39:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# sort to find worst predictions of y1\nlocs = locs.sort_values('rmse1', ascending=False)\nlocs[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# plot worst fits of y1\nfor i in range(5):\n    li = locs.index[i]\n    plt.plot(y_truea[1][:,li])\n    plt.plot(y_preda[1][:,li])\n    plt.title(locs.loc[li,'Loc'])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"tmax","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"dmax","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# enforce monotonicity of forecasts in test set after last date in training\n# loc = d['Loc'].unique()\nlocs = d['Loc'].drop_duplicates()\nfor loc in locs:\n    # q = (d.Loc==loc) & (d.ForecastId > 0)\n    q = (d.Loc==loc) & (d.Date > tmax)\n    # if skip, fill in last observed value\n    if skip: qs0 = (d.Loc==loc) & (d.Date == dmax.isoformat())\n    for yi in yv:\n        yp = yi+'_pred'\n        d.loc[q,yp] = np.maximum.accumulate(d.loc[q,yp])\n        if skip:\n            for j in range(skip):\n                qs1 = (d.Loc==loc) & (d.Date == (dmax + timedelta(1+j)).isoformat())\n                d.loc[qs1,yp] = d.loc[qs0,yi].values","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"# plot actual and predicted curves over time for specific locations\n# locs = ['China Tibet','China Xinjiang','China Hong Kong', 'China Macau',\n#         'Spain','Italy','India',\n#         'US Washington','US New York','US California',\n#         'US North Carolina','US Ohio']\n# xlab = ['03-12','03-18','03-25','04-01','04-08','04-15','04-22']\n# plot all locations\nlocs = d['Loc'].drop_duplicates()\nfor loc in locs:\n    plt.figure(figsize=(14,2))\n    \n    # fig, ax = plt.subplots()\n    # fig.autofmt_xdate()\n    \n    for i in range(ny):\n    \n        plt.subplot(1,2,i+1)\n        plt.plot(d.loc[d.Loc==loc,[yv[i],'Date']].set_index('Date'))\n        plt.plot(d.loc[d.Loc==loc,[yv[i]+'_pred','Date']].set_index('Date'))\n        # plt.plot(d.loc[d.Loc==loc,[yv[i]]])\n        # plt.plot(d.loc[d.Loc==loc,[yv[i]+'_pred']])\n        # plt.xticks(np.arange(len(xlab)), xlab, rotation=-45)\n        # plt.xticks(np.arange(12), calendar.month_name[3:5], rotation=20)\n        # plt.xticks(rotation=-45)\n        plt.xticks([])\n        plt.title(loc + ' ' + ynames[i])\n       \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# pd.set_option('display.max_rows', 100)\n# loc = 'China Xinjiang'\n# d.loc[d.Loc==loc,['Date',yv[0],yv[0]+'_pred']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"tmax","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"fmin","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# compute public lb score\nif not prev_test:\n    q = (d.Date >= fmin) & (d.Date <= tmax)\n    # q = (d.Date >= tmax) & (d.Date <= tmax)\n    print(f'# {fmin} {tmax} {sum(q)/ns} {mname}')\n    s0 = np.sqrt(mean_squared_error(d.loc[q,'y0'],d.loc[q,'y0_pred']))\n    s1 = np.sqrt(mean_squared_error(d.loc[q,'y1'],d.loc[q,'y1_pred']))\n    print(f'# CC \\t {s0:.6f}')\n    print(f'# Fa \\t {s1:.6f}')\n    print(f'# Mean \\t {(s0+s1)/2:.6f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# 2020-03-26 2020-04-07 13.0 gbt3l\n# CC \t 2.469711\n# Fa \t 2.050545\n# Mean \t 2.260128","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# 2020-03-26 2020-04-07 13.0 gbt3j\n# CC \t 2.469110\n# Fa \t 2.047875\n# Mean \t 2.258493","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# 2020-03-26 2020-04-07 13.0 gbt3e\n# CC \t 2.471136\n# Fa \t 2.080902\n# Mean \t 2.276019","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# create submission\nsub = d.loc[d.ForecastId > 0, ['ForecastId','y0_pred','y1_pred','dint']]\nsub['dint'] = sub['dint'] - sub['dint'].min()\nprint(sub.shape)\nprint(sub['dint'].describe())\nhmax = np.max(sub.dint.values) + 1\nprint(hmax)\n# blend with beluga\nbs = pd.read_csv(path+'beluga0g.csv')\nprint(bs.shape)\nbs['b0g0'] = np.log1p(bs.ConfirmedCases)\nbs['b0g1'] = np.log1p(bs.Fatalities)\n# bs.drop(['ConfirmedCases','Fatalities'],axis=1,inplace=True)\nsub = sub.merge(bs, how='left', on='ForecastId')\n\nbs = pd.read_csv(path+'vop0e.csv')\nprint(bs.shape)\nbs['v0e0'] = np.log1p(bs.ConfirmedCases)\nbs['v0e1'] = np.log1p(bs.Fatalities)\n# bs.drop(['ConfirmedCases','Fatalities'],axis=1,inplace=True)\nsub = sub.merge(bs, how='left', on='ForecastId')\n\nbs = pd.read_csv(path+'oscii0e.csv')\nprint(bs.shape)\nbs['o0e0'] = np.log1p(bs.ConfirmedCases)\nbs['o0e1'] = np.log1p(bs.Fatalities)\n# bs.drop(['ConfirmedCases','Fatalities'],axis=1,inplace=True)\nsub = sub.merge(bs, how='left', on='ForecastId')\n\n# 80/20 linear blend over future horizons\nfor h in range(hmax):\n    # a = 0.5 if h < 14 else 0.8 - 0.6 * (h - 14) / (hmax - 15)\n    # print(h,a)\n    q = sub['dint'] == h\n    sub.loc[q,'ConfirmedCases'] = np.expm1((sub.loc[q,'y0_pred'] + sub.loc[q,'b0g0'] + \\\n                                          0.5*sub.loc[q,'v0e0'] + 0.5*sub.loc[q,'o0e0'])/3.0)\n    sub.loc[q,'Fatalities'] = np.expm1((sub.loc[q,'y1_pred'] + sub.loc[q,'b0g1'] + \\\n                                      0.5*sub.loc[q,'v0e1'] + 0.5*sub.loc[q,'o0e1'])/3.0)\n\nsub = sub[['ForecastId','ConfirmedCases','Fatalities']]\n\nos.makedirs('sub',exist_ok=True)\nfname = 'sub/' + mname + '.csv'\nsub.to_csv(fname, index=False)\nprint(fname, sub.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sub.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# sub[30:50]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# merge final predictions back into main table\nsub1 = sub.copy()\nfor i in range(ny): \n    mi = mname + str(i)\n    if mi in d.columns: d.drop(mi, axis=1, inplace=True)\n    sub1[mi] = np.log1p(sub1[ynames[i]])\n    sub1.drop(ynames[i],axis=1,inplace=True)\nd = d.merge(sub1, how='left', on='ForecastId')\n\n# compute public lb score after averaging with others\nif not prev_test:\n    q = (d.Date >= fmin) & (d.Date <= tmax)\n    # q = (d.Date >= tmax) & (d.Date <= tmax)\n    print(f'# {fmin} {tmax} {sum(q)/ns} {mname}')\n    s0 = np.sqrt(mean_squared_error(d.loc[q,'y0'],d.loc[q,mname+'0']))\n    s1 = np.sqrt(mean_squared_error(d.loc[q,'y1'],d.loc[q,mname+'1']))\n    print(f'# CC \\t {s0:.6f}')\n    print(f'# Fa \\t {s1:.6f}')\n    print(f'# Mean \\t {(s0+s1)/2:.6f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# load actual submission, previous code should generate something very close to it\nsub = pd.read_csv(\"/kaggle/input/gbt3nx/gbt3n.csv\")\nprint(sub.describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fname = 'submission.csv'\nsub.to_csv(fname, index=False)\nprint(fname, sub.shape)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"2f8637a583ba4bfa8946994db32cde94":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"624f68aaf5684d2ca6f88c940feb59e0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6827a0694afe427ba904c7fe40befda3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b66678a5c7b64b90a555ccfeabb305c2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"b937a9e2638e4fb4932fc16802ad725f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_624f68aaf5684d2ca6f88c940feb59e0","placeholder":"","style":"IPY_MODEL_cc713908016240288dac63ce30f32779","value":" 163/163 [00:05&lt;00:00, 31.15it/s]"}},"ba5137f29e61409db2c60bf628df8bcd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"100%","description_tooltip":null,"layout":"IPY_MODEL_6827a0694afe427ba904c7fe40befda3","max":163,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b66678a5c7b64b90a555ccfeabb305c2","value":163}},"cc713908016240288dac63ce30f32779":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ebb54321a1de47a1900c2b6f56668c6c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ba5137f29e61409db2c60bf628df8bcd","IPY_MODEL_b937a9e2638e4fb4932fc16802ad725f"],"layout":"IPY_MODEL_2f8637a583ba4bfa8946994db32cde94"}}},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":4}