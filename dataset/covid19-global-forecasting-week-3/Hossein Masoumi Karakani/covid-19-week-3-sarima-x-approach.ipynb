{"cells":[{"metadata":{},"cell_type":"markdown","source":"One of the most widely used forecasting approaches for univariate time series data forecasting is Autoregressive Integrated Moving Average (ARIMA). Although the method can take into account time series data with existing trends, however, it does not support time series data with a seasonal component. In order to combat this drawback, an extension to ARIMA model that supports the direct modeling of the seasonal component will be used, denoted by S(Seasonal)ARIMA.\n\nIn my approach for the COVID-19 global forecasting competition (week 3), I will discuss the following:\n\nARIMA Model Limitations SARIMA Model SARIMA Model in Python Grid Search for SARIMA Hyperparameters\n\nARIMA Model Limitations\nARIMA model supports both an autoregressive and moving average components. The integrated element refers to differencing method to support time series data with trend. However, the limitation of this model is that it does not support seasonal data and expects time series data that is either not seasonal or has the seasonal component removed, e.g., seasonally adjusted through seasonal differencing method.The ARIMA model is denoted by ARIMA(p,d,q), where p is the number of lag observations included in the model (also referred to as the lag order), d is the number of times that the raw observations are differenced (also called the the degree of differencing), and q that is the size of the moving average window (also called the order of moving average).\n\nSARIMA Model\nIn order to overcome the drawbacks of the ARIMA model, SARIMA or Seasonal ARIMA is considered as the modeling approach that explicitly supports univariate time series data with a seasonal component. The flexibility of this model is that it adds three new hyperparameters to specify the autoregression (AR), differencing (I), and moving average (MA) for the seasonal component of the time series data, as well as an additional parameter for the period of seasonality. The SARIMA model is denoted by SARIMA(p,d,q)(P,D,Q)m, where p is the trend autoregression order, d is the trend difference order, q is the trend moving average order, P is the seasonal autoregressive order, D is the seasonal difference order, Q is the seasonal moving average order, and m is the numbe of time steps for a single seasonal period. Configuring a SARIMA model requires selecting hyperparameters for both the seasonal and the trend components of the series.\n\nSARIMA Model in Python\nThe SARIMA model in Python is supported by the Statsmodels library.To use SARIMA model there are three steps, they are as follows:\n\nDefine the model An instance of the SARIMAX class can be created by providing the training data and the selection of model configuration parameters (also called hyperparameters of the model). The implementation is called SARIMAX instead of SARIMA since the 'X' addition means that the implementation also supports 'exogenous' variable(s).\n\nFit Model Make Prediction\n\nGrid Search for SARIMA Hyperparameters\nThe SARIMA model requires careful analysis and domain expertise in order to configure the model hyperparameters. An alternative approach to configuring the model is to \"grid search\" a suite of hyperparameter configurations in order to discover an ideal scenario.\n\nThe SARIMA model can subsume the ARIMA, ARMA, AR, and MA models through model configuration parameters. The trend and seasonal hyperparameters of the model can be configured by analyzing autocorrelation and partial autocorrelation plots (ACF & PACF), and this can take some expertise. An alternative approach is to grid search a suite of model configurations and discover which configuration work best for a specific univariate time series. For more information, please refer to the following link (https://machinelearningmastery.com/how-to-grid-search-sarima-model-hyperparameters-for-time-series-forecasting-in-python/)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import the necessary libraris #\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport warnings\nwarnings.filterwarnings(action='ignore')\nimport plotly.express as px\nimport plotly.graph_objs as go\nfrom plotly.subplots import make_subplots\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.tsa.arima_model import ARIMA\n# Define the directory for the input files (train + test + submission) #\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define the criteria to evaluate the accuracy for the final forecasting model #\ndef RMSLE(pred,actual):\n    return np.sqrt(np.mean(np.power((np.log(pred+1)-np.log(actual+1)),2)))\npd.set_option('mode.chained_assignment', None)\n# Import the train & test data for COVID-19 (Week 3) #\ntest = pd.read_csv(\"../input/covid19-global-forecasting-week-3/test.csv\")\ntrain = pd.read_csv(\"../input/covid19-global-forecasting-week-3/train.csv\")\n# Replace the missing values in the train & test data sets #\ntrain['Province_State'].fillna('', inplace=True)\ntest['Province_State'].fillna('', inplace=True)\n# Convert the \"Date\" Variable in the training & test sets #\ntrain['Date'] =  pd.to_datetime(train['Date'])\ntest['Date'] =  pd.to_datetime(test['Date'])\n# Sort values in the training & test sets #\ntrain = train.sort_values(['Country_Region','Province_State','Date'])\ntest = test.sort_values(['Country_Region','Province_State','Date'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining key dates for reference purposes #\nfeature_day = [1,20,50,100,200,500,1000]\ndef CreateInput(data):\n    feature = []\n    for day in feature_day:\n        data.loc[:,'Number day from ' + str(day) + ' case'] = 0\n        if (train[(train['Country_Region'] == country) & (train['Province_State'] == province) & (train['ConfirmedCases'] < day)]['Date'].count() > 0):\n            fromday = train[(train['Country_Region'] == country) & (train['Province_State'] == province) & (train['ConfirmedCases'] < day)]['Date'].max()        \n        else:\n            fromday = train[(train['Country_Region'] == country) & (train['Province_State'] == province)]['Date'].min()       \n        for i in range(0, len(data)):\n            if (data['Date'].iloc[i] > fromday):\n                day_denta = data['Date'].iloc[i] - fromday\n                data['Number day from ' + str(day) + ' case'].iloc[i] = day_denta.days \n        feature = feature + ['Number day from ' + str(day) + ' case']\n    \n    return data[feature]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"pred_data_all = pd.DataFrame()\nfor country in train['Country_Region'].unique():\n    for province in train[(train['Country_Region'] == country)]['Province_State'].unique():\n        df_train = train[(train['Country_Region'] == country) & (train['Province_State'] == province)]\n        df_test = test[(test['Country_Region'] == country) & (test['Province_State'] == province)]\n        X_train = CreateInput(df_train)\n        y_train_confirmed = df_train['ConfirmedCases'].ravel()\n        y_train_fatalities = df_train['Fatalities'].ravel()\n        X_pred = CreateInput(df_test)        \n        for day in sorted(feature_day,reverse = True):\n            feature_use = 'Number day from ' + str(day) + ' case'\n            idx = X_train[X_train[feature_use] == 0].shape[0]     \n            if (X_train[X_train[feature_use] > 0].shape[0] >= 20):\n                break      \n        adjusted_X_train = X_train[idx:][feature_use].values.reshape(-1, 1)\n        adjusted_y_train_confirmed = y_train_confirmed[idx:]\n        adjusted_y_train_fatalities = y_train_fatalities[idx:] #.values.reshape(-1, 1)\n        idx = X_pred[X_pred[feature_use] == 0].shape[0]    \n        adjusted_X_pred = X_pred[idx:][feature_use].values.reshape(-1, 1)\n        pred_data = test[(test['Country_Region'] == country) & (test['Province_State'] == province)]\n        max_train_date = train[(train['Country_Region'] == country) & (train['Province_State'] == province)]['Date'].max()\n        min_test_date = pred_data['Date'].min()\n        model = SARIMAX(adjusted_y_train_confirmed, order=(1,1,0), \n                        measurement_error=True).fit(disp=False)\n        y_hat_confirmed = model.forecast(pred_data[pred_data['Date'] > max_train_date].shape[0])\n        y_train_confirmed = train[(train['Country_Region'] == country) & (train['Province_State'] == province) & (train['Date'] >=  min_test_date)]['ConfirmedCases'].values\n        y_hat_confirmed = np.concatenate((y_train_confirmed,y_hat_confirmed), axis = 0)\n               \n        model = SARIMAX(adjusted_y_train_fatalities, order=(1,1,0), \n                        measurement_error=True).fit(disp=False)\n        y_hat_fatalities = model.forecast(pred_data[pred_data['Date'] > max_train_date].shape[0])\n        y_train_fatalities = train[(train['Country_Region'] == country) & (train['Province_State'] == province) & (train['Date'] >=  min_test_date)]['Fatalities'].values\n        y_hat_fatalities = np.concatenate((y_train_fatalities,y_hat_fatalities), axis = 0)\n        pred_data['ConfirmedCases_hat'] =  y_hat_confirmed\n        pred_data['Fatalities_hat'] = y_hat_fatalities\n        pred_data_all = pred_data_all.append(pred_data)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"code","source":"df_val = pd.merge(pred_data_all,train[['Date','Country_Region','Province_State','ConfirmedCases','Fatalities']],on=['Date','Country_Region','Province_State'], how='left')\ndf_val.loc[df_val['Fatalities_hat'] < 0,'Fatalities_hat'] = 0\ndf_val.loc[df_val['ConfirmedCases_hat'] < 0,'ConfirmedCases_hat'] = 0\ndf_val_3 = df_val.copy()\nsubmission = df_val[['ForecastId','ConfirmedCases_hat','Fatalities_hat']]\nsubmission.columns = ['ForecastId','ConfirmedCases','Fatalities']\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}