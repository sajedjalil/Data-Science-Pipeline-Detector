{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport plotly.express as px\nfrom sklearn.linear_model import LinearRegression, BayesianRidge\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\nfrom xgboost import XGBRegressor\nimport pycountry\nimport matplotlib.pyplot as plt\n\nfrom scipy.optimize import curve_fit\nfrom scipy.integrate import odeint\n\n# Calculate the Root Mean Squared Logarithmic Error (RMSLE)\nfrom sklearn.metrics import mean_squared_log_error\n\nfrom fbprophet import Prophet\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define the fuction to calculte the \"root mean squared logarithmic error\"\n\ndef rmsle(y_true, y_predict):\n    return np.sqrt(np.mean(np.square(np.log1p(y_true) - np.log1p(y_predict))))\n#log1p: natural logarithmic value of x+1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Explore the dataset \nExploring the dataset to see what information we have."},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(\"../input/covid19-global-forecasting-week-3/test.csv\")\ntrain = pd.read_csv(\"../input/covid19-global-forecasting-week-3/train.csv\")\ntrain['unique_id'] = train['Country_Region'].astype(str) + '_' + train['Province_State'].astype(str)\n\nprint('Total Number of Country in Training Data: ', train['Country_Region'].nunique())\nprint('Has in total number of Province or States: ', train['Province_State'].nunique())\nprint('Date range: ', min(train['Date']), max(train['Date']), 'Today number of days: ', train['Date'].nunique())\n\nprint('Total Number of Country in Test Data: ', test['Country_Region'].nunique())\nprint('Has in total number of Province or States: ', test['Province_State'].nunique())\nprint('Date range: ', min(test['Date']), max(test['Date']), 'Today number of days: ', test['Date'].nunique())\n\nprint('For the training dataset, the number of regions on the first day ', min(train['Date']), ' are ', train[train['Date'] == min(train['Date'])]['Country_Region'].nunique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The training dataset has all 180 countries confirmed case and fatalities regardless whether that was the first day the country found its first case or not. Our training dataset will be driving by a lot of zero values from the earlier days.\n\nLets see globally, how the coronavirus cases number change. I will use plotly here."},{"metadata":{"trusted":true},"cell_type":"code","source":"tot_confirmed = train.groupby(['Date']).agg({'ConfirmedCases':['sum']})\ntot_fatalities = train.groupby(['Date']).agg({'Fatalities':['sum']})\ntot_case_bydate = tot_confirmed.join(tot_fatalities)\ntot_case_bydate.reset_index(inplace = True)\ntot_case_bydate.head()\n\n# Later need to put into one figure\nfig = px.scatter(tot_case_bydate, x = 'Date', y = 'ConfirmedCases',\n                width=800, height=400)\nfig.update_layout(title='Global Confirmed Cases - Cumulative')\nfig.show()\n\nfig = px.scatter(tot_case_bydate, x = 'Date', y = 'Fatalities',\n                width=800, height=400)\nfig.update_layout(title='Global Fatalities Cases - Cumulative')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_map = train.copy()\ndf_map['Date'] = df_map['Date'].astype(str)\ndf_map = df_map.groupby(['Date','Country_Region'], as_index=False)['ConfirmedCases','Fatalities'].sum()\n\ndef get_iso3_util(country_name):\n    try:\n        country = pycountry.countries.get(name=country_name)\n        return country.alpha_3\n    except:\n        if 'Congo' in country_name:\n            country_name = 'Congo'\n        elif country_name == 'Diamond Princess' or country_name == 'Laos' or country_name == 'MS Zaandam':\n            return country_name\n        elif country_name == 'Korea, South':\n            country_name = 'Korea, Republic of'\n        elif country_name == 'Taiwan*':\n            country_name = 'Taiwan'\n        elif country_name == 'Burma':\n            country_name = 'Myanmar'\n        elif country_name == 'West Bank and Gaza':\n            country_name = 'Gaza'\n        country = pycountry.countries.search_fuzzy(country_name)\n        return country[0].alpha_3\n\nd = {}\ndef get_iso3(country):\n    if country in d:\n        return d[country]\n    else:\n        d[country] = get_iso3_util(country)\n    \ndf_map['iso_alpha'] = df_map.apply(lambda x: get_iso3(x['Country_Region']), axis=1)\n\ndf_map['ln(ConfirmedCases)'] = np.log(df_map.ConfirmedCases + 1)\ndf_map['ln(Fatalities)'] = np.log(df_map.Fatalities + 1)\n\npx.choropleth(df_map, \n              locations=\"iso_alpha\", \n              color=\"ln(ConfirmedCases)\", \n              hover_name=\"Country_Region\", \n              hover_data=[\"ConfirmedCases\"] ,\n              animation_frame=\"Date\",\n              color_continuous_scale=px.colors.sequential.dense, \n              title='Total Confirmed Cases growth(Logarithmic Scale)')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Machine Learning Model\n## 1. Linear Regression\nWe start with the basic linear regression model. Because the increase of confirmed cases and fatalities are not linear (we can tell from above graphs), and it is more like exponential growth, we use the log value of confirmed cases and fatalities for the model. Be careful: since the predicted value is log value, do not forget to convert back to nomral number. "},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(\"../input/covid19-global-forecasting-week-3/test.csv\")\ntrain = pd.read_csv(\"../input/covid19-global-forecasting-week-3/train.csv\")\ntest['Province_State'].fillna('', inplace = True)\ntrain['Province_State'].fillna('', inplace = True)\ntest['Date'] = pd.to_datetime(test['Date'])\ntrain['Date'] = pd.to_datetime(train['Date'])\n\ntrain['unique_id'] = train['Country_Region'].astype(str) + '_' + train['Province_State'].astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"days = 11\n\npred_confirm = []\npred_fatality = []\ntest_length = test['Date'].nunique()\n\n# Prepare to calculate the loss function:\ny_true_c = []\ny_predict_c = []\ny_true_f = []\ny_predict_f = []\n\n# Using Linear Regression Model to Predict Confirmed Cases and Fatalities.\nfor uid in train['unique_id'].unique():\n    df = train[train['unique_id'] == uid]\n    \n    # use log transformed value\n    y_c = df.set_index('Date')['ConfirmedCases'].values.flatten()\n    # Append the last 7 days to y_true data for loss function calculation later\n    y_true_c.append(y_c[-days:])\n    index = len(y_c) - days\n    y_confirm = y_c[:index]\n    y_confirm = np.log(y_confirm)\n    y_confirm[y_confirm == np.inf] = 0\n    y_confirm[y_confirm == -np.inf] = 0\n    \n    y_f = df.set_index('Date')['Fatalities'].values.flatten()\n    index = len(y_f) - days\n    y_true_f.append(y_f[-days:])\n    y_fatality = y_f[:index]\n    y_fatality = np.log(y_fatality)\n    y_fatality[y_fatality == np.inf] = 0\n    y_fatality[y_fatality == -np.inf] = 0\n    \n    x = np.arange(0, len(y_confirm), 1)\n    x = x.reshape(-1,1)\n    x_test = np.arange(max(x) + 1, max(x) + test_length + 1, 1)\n    x_test = x_test.reshape(-1,1)\n    \n    lreg = LinearRegression()\n    \n    lreg.fit(x,y_confirm)\n    predict_c = lreg.predict(x_test)\n    pred_confirm.append(predict_c)\n    y_predict_c.append(predict_c[:days])\n    \n    lreg.fit(x,y_fatality)\n    predict_f = lreg.predict(x_test)\n    pred_fatality.append(predict_f)\n    y_predict_f.append(predict_f[:days])\n\npred_confirm = [item for sublist in pred_confirm for item in sublist]\npred_fatality = [item for sublist in pred_fatality for item in sublist]\n\n# Convert log to normal value:\npredict_c = np.exp(pred_confirm).astype(int)\npredict_f = np.exp(pred_fatality).astype(int)\n\n# Replace any negative value with 0\npredict_c[predict_c < 0] = 0\npredict_f[predict_f < 0] = 0\n\nsubmission = pd.DataFrame({'ForecastId': test['ForecastId'], \n                           'ConfirmedCases': predict_c, \n                           'Fatalities': predict_f})\n# submission.to_csv('submission.csv', index = False)\n\n\n####### Calculate the loss function: RMSLE\ny_true_c = [item for sublist in y_true_c for item in sublist]\ny_predict_c = [item for sublist in y_predict_c for item in sublist]\ny_true_f = [item for sublist in y_true_f for item in sublist]\ny_predict_f = [item for sublist in y_predict_f for item in sublist]\n\n# Convert log to normal value:\ny_predict_c2 = np.exp(y_predict_c).astype(int)\ny_predict_f2 = np.exp(y_predict_f).astype(int)\n\n# Replace any negative value with 0\ny_predict_c2[y_predict_c2 < 0] = 0\ny_predict_c2 = y_predict_c2.tolist()\ny_predict_f2[y_predict_f2 < 0] = 0\ny_predict_f2 = y_predict_f2.tolist()\n\nY_true = [*y_true_c, *y_true_f]\nY_predict =[*y_predict_c2, *y_predict_f2]\n\n# Calculate the lost: 1.8981\nrmsle(Y_true, Y_predict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. XGBoost Model\nThree classes of boosting: \n1. Adaptive Boosting\n2. Gradient Boosting\n3. XGBoost: has the tendency to fill in the missing values. It is an advanced version of gradient boosting (extreme gradient boosting). It is faster and more efficient.\n    - It is faster and can build more efficient model compare with Gradient Boosting;\n    - Supports parallelization by creating decision trees;\n    - Weight each input based on what is the most relevant optimal.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"days = 0\n\npred_confirm = []\npred_fatality = []\ntest_length = test['Date'].nunique()\n\n# Prepare to calculate the loss function:\ny_true_c = []\ny_predict_c = []\ny_true_f = []\ny_predict_f = []\n\n# Using Linear Regression Model to Predict Confirmed Cases and Fatalities.\nfor uid in train['unique_id'].unique():\n    df = train[train['unique_id'] == uid]\n    \n    # use log transformed value\n    y_c = df.set_index('Date')['ConfirmedCases'].values.flatten()\n    # Append the last 7 days to y_true data for loss function calculation later\n    y_true_c.append(y_c[-days:])\n    index = len(y_c) - days\n    y_confirm = y_c[:index]\n    \n    y_f = df.set_index('Date')['Fatalities'].values.flatten()\n    index = len(y_f) - days\n    y_true_f.append(y_f[-days:])\n    y_fatality = y_f[:index]\n    \n    x = np.arange(0, len(y_confirm), 1)\n    x = x.reshape(-1,1)\n    x_test = np.arange(max(x) + 1, max(x) + test_length + 1, 1)\n    x_test = x_test.reshape(-1,1)\n    \n    # xgboost\n    model =  XGBRegressor(n_estimators=1000)\n    model.fit(x,y_confirm)                \n    predict_c = model.predict(x_test)\n    pred_confirm.append(predict_c)\n    y_predict_c.append(predict_c[:days])\n    \n    model.fit(x,y_fatality)\n    predict_f = model.predict(x_test)\n    pred_fatality.append(predict_f)\n    y_predict_f.append(predict_f[:days])    \n    \npred_confirm = [item for sublist in pred_confirm for item in sublist]\npred_fatality = [item for sublist in pred_fatality for item in sublist]\n\n\nsubmission = pd.DataFrame({'ForecastId': test['ForecastId'], \n                           'ConfirmedCases': pred_confirm,\n                           'Fatalities': pred_fatality})\n\nsubmission = submission.round(0)\n\nsubmission.to_csv('submission.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ####### Calculate the loss function: RMSLE\n# # Calculate the loss\n# y_true_c = [item for sublist in y_true_c for item in sublist]\n# y_predict_c = [item for sublist in y_predict_c for item in sublist]\n# y_predict_c = [round(elem, 2) for elem in y_predict_c]\n# y_true_f = [item for sublist in y_true_f for item in sublist]\n# y_predict_f = [item for sublist in y_predict_f for item in sublist]\n# y_predict_f = [round(elem, 2) for elem in y_predict_f]\n\n\n# Y_true = [*y_true_c, *y_true_f]\n# Y_predict =[*y_predict_c, *y_predict_f]\n\n# # Calculate the loss: 0.9131976\n# rmsle(Y_true, Y_predict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. SIR Model\nThe resulst for SIR model is not that accurate. We did not use it after all."},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(\"../input/covid19-global-forecasting-week-3/test.csv\")\ntrain = pd.read_csv(\"../input/covid19-global-forecasting-week-3/train.csv\")\ntest['Date'] = pd.to_datetime(test['Date'])\ntrain['Date'] = pd.to_datetime(train['Date'])\ntrain['Province_State'] = train['Province_State'].fillna('None')\ntrain['unique_id'] = train['Country_Region'].astype(str) + '_' + train['Province_State'].astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_hubei = train[train['Province_State']=='Hubei']\ntrain_hubei['Date'] = train_hubei['Date'].astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def deriv(y, t, N,beta,gamma):\n\n            S,I,R = y\n            dSdt = -beta * S * I/N\n            dIdt = beta * S * I/N  - gamma * I\n            dRdt = gamma * I\n            return dSdt,dIdt,dRdt\n        \ndef odeint_func(t,N,I0,R0,beta,gamma):\n    \n    S0 = (N - I0 - R0)\n    y0 = S0, I0, R0\n    ret = odeint(deriv, y0, t, args=(N, beta, gamma))\n\n    return np.ravel(np.vstack((ret[:,1],ret[:,2])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_curve_fit_diff_S(y1,y2,N):\n   \n    I0 = y1[0]\n    R0 = y2[0]\n    def deriv(y, t,N, beta,gamma,tau):\n\n        S,I,R = y\n#         dNdt = -(1/10**tau)*N\n        dSdt = -beta * S * I/N\n        dIdt = beta * S * I/N  - gamma * I\n        dRdt = gamma * I\n        return dSdt,dIdt,dRdt\n    \n    def odeint_func(t,N0,beta,gamma,tau,I0,R0):\n        \n        S0 = (N - I0 - R0)\n        y0 = S0, I0, R0\n        ret = odeint(deriv, y0, t, args=(N,beta, gamma,tau))\n        return np.ravel(np.vstack((ret[:,1],ret[:,2])))\n\n    t = np.arange(0,len(y1),1)\n    y_t = np.vstack((y1,y2))\n\n    values , pcov = curve_fit(lambda t,beta,gamma,tau: odeint_func(t,N,beta,gamma,tau,I0,R0), \n                          t, np.ravel(y_t) ,bounds=((0,0,-np.inf),(1,1,np.inf)),maxfev=999999)\n        \n    return values[0],values[1],values[2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_SIR_data_diff_S(beta,gamma,tau, y_active,N, days):\n    \n    I0 = y_active[0]\n    R0 = 0\n    t = np.arange(0,len(y_active)+days,1)\n    def deriv(y, t,N, beta,gamma,tau):\n\n        S,I,R = y\n#         dNdt = -(1/10**tau)*N\n        dSdt = -beta * S * I/N\n        dIdt = beta * S * I/N  - gamma * I\n        dRdt = gamma * I\n        return dSdt,dIdt,dRdt\n    \n    S0 = (N - I0 - R0)\n    y0 = S0, I0, R0\n    ret = odeint(deriv, y0, t, args=(N,beta, gamma,tau))\n        \n    return ret.T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import datetime\npop = 58500000\ny_conf  = train_hubei.set_index('Date')['ConfirmedCases'].values.flatten()\ny_death = train_hubei.set_index('Date')['Fatalities'].values.flatten()\n\nidx = np.argwhere(y_conf>0)[0][0]\n    \ny_active = y_conf[idx:]\ny_deaths = y_death[idx:]\nbeta,gamma,tau = run_curve_fit_diff_S(y_active,y_deaths,pop)\nprint(beta,gamma,tau)\nfig = plt.figure()\nax = plt.gca()\nstart_date =  datetime.datetime.strptime(train_hubei['Date'].values[idx], '%Y-%m-%d')\nfirst_date = datetime.datetime.strptime(train_hubei['Date'].values[0], '%Y-%m-%d')\n\ndate_line = [first_date + datetime.timedelta(days=x) for x in range(len(train_hubei))]\n\nax.plot(date_line,y_conf,'r-', label='Active Cases')\nax.plot(date_line,y_death,'g-', label='Active Cases')\nN_days = 360\ndate_line_ext = [first_date + datetime.timedelta(days=x) for x in range(len(y_conf)+N_days)]\n\nS,I,R = get_SIR_data_diff_S(beta,gamma,tau,y_active,pop,N_days)\n#N  = np.append(np.ones(idx)*max(N),N)\nS = np.append(np.ones(idx)*max(S),S)\nI = np.append(np.zeros(idx),I)\nR = np.append(np.zeros(idx),R)\n#ax.plot(date_line_ext,N,'k--',label='Total Population Fit')\n\nax.plot(date_line_ext,S,'b--',label='Susceptible Cases (Fit)')\nax.plot(date_line_ext,I,'r--',label='Active Cases (Fit)')\nax.plot(date_line_ext,R,'g--',label = 'Death Cases (Fit)')\nplt.yscale('linear')\nplt.legend()\n\n#plt.ylim(10,1e8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Prophet (Base model)\nThe loss function is not great for the world prediction (2.7939650495039494), so we did not use this model eventually and it takes forever to run."},{"metadata":{"trusted":true},"cell_type":"code","source":"# test = pd.read_csv(\"../input/covid19-global-forecasting-week-3/test.csv\")\n# train = pd.read_csv(\"../input/covid19-global-forecasting-week-3/train.csv\")\n# train['unique_id'] = train['Country_Region'].astype(str) + '_' + train['Province_State'].astype(str)\n# train['Date'] = pd.to_datetime(train['Date'])\n# test['Date'] = pd.to_datetime(test['Date'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Using New York as an example\n# # I want to predict any date after March 26th\n# days = 7\n\n# df = train[train['unique_id'] == 'US_New York']\n# index = len(df) - days\n\n# df = df[:index]\n\n# confirmed = df[['Date','ConfirmedCases']]\n# fatalities = df[['Date','Fatalities']]\n\n# confirmed.columns = ['ds','y']\n# fatalities.columns = ['ds','y']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model = Prophet(interval_width = 0.95)\n# model.fit(confirmed)\n# furture = model.make_future_dataframe(periods = days)\n# furture.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Set up upper value and lower value (set up tolerance)\n# forecast = model.predict(furture)\n# forecast[['ds','yhat','yhat_lower','yhat_upper']].tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# confirmed_forecast_plot = model.plot(forecast)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# confirmed_forecast_plot = model.plot_components(forecast)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# days = 12\n\n# # Prepare to calculate the loss function:\n# y_true_c = []\n# y_true_f = []\n# y_predict_c = []\n# y_predict_f = []\n\n# # Using Linear Regression Model to Predict Confirmed Cases and Fatalities.\n# for uid in train['unique_id'].unique():\n#     df = train[train['unique_id'] == uid]\n   \n#     # Append the last 7 days to y_true data for loss function calculation later    \n#     y_true_c.append(df['ConfirmedCases'][-days:])\n#     y_true_f.append(df['Fatalities'][-days:])\n    \n#     index = len(df) - days\n#     df = df[:index]\n    \n#     confirmed = df[['Date','ConfirmedCases']]\n#     fatalities = df[['Date','Fatalities']]\n\n#     confirmed.columns = ['ds','y']\n#     fatalities.columns = ['ds','y']\n    \n#     # Predict the ConfirmedCases\n#     model_c = Prophet(interval_width = 0.95)\n#     model_c.fit(confirmed) \n#     furture_c = model.make_future_dataframe(periods=days)\n#     forecast_c = model.predict(furture_c)\n#     y_predict_c.append(forecast_c[-days:]['yhat'])\n    \n#     # Predict Fatalities\n#     model_f = Prophet(interval_width = 0.95)\n#     model_f.fit(fatalities)\n#     furture_f = model.make_future_dataframe(periods=days)\n#     forecast_f = model.predict(furture_f)\n#     y_predict_f.append(forecast_f[-days:]['yhat'])    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Y_true = [*y_true_c, *y_true_f]\n# Y_predict =[*y_predict_c, *y_predict_f]\n\n# # Calculate the lostt\n# rmsle(Y_true, Y_predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-3/train.csv')\n# test = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-3/test.csv')\n\n# train = train.fillna('NA')\n# test = test.fillna('NA')\n\n# train['Date'] = pd.to_datetime(train['Date'])\n# train['Date'] = train['Date'].dt.strftime('%m%d')   # Change to only month and day. Because years are all the same. \n# test['Date'] = pd.to_datetime(test['Date'])\n# test['Date'] = test['Date'].dt.strftime('%m%d')\n\n# country_list = train['Country_Region'].unique()\n\n# sub = []\n# for country in country_list:\n    \n#     province_list = train.loc[train['Country_Region'] == country].Province_State.unique()  #Get the province list by country\n    \n#     for province in province_list:\n        \n#         X_train = train.loc[(train['Country_Region'] == country) & (train['Province_State'] == province),['Date']].astype('int')  # Date\n#         Y_train_c = train.loc[(train['Country_Region'] == country) & (train['Province_State'] == province),['ConfirmedCases']]    # Confirmed Cases \n#         Y_train_f = train.loc[(train['Country_Region'] == country) & (train['Province_State'] == province),['Fatalities']]        # Fatalities\n#         X_test = test.loc[(test['Country_Region'] == country) & (test['Province_State'] == province), ['Date']].astype('int')     # Date\n#         X_forecastId = test.loc[(test['Country_Region'] == country) & (test['Province_State'] == province), ['ForecastId']]       # ForecastId\n#         X_forecastId = X_forecastId.values.tolist()      # Put ForecastId into an array (nested)\n#         X_forecastId = [v[0] for v in X_forecastId]      # Open the nested array to one list\n        \n#         # Use XGBRegressor to fit and predict\n#         model_c = XGBRegressor(n_estimators=1000)\n#         model_c.fit(X_train, Y_train_c)\n#         Y_pred_c = model_c.predict(X_test)\n        \n#         model_f = XGBRegressor(n_estimators=1000)\n#         model_f.fit(X_train, Y_train_f)\n#         Y_pred_f = model_f.predict(X_test)\n        \n#         for j in range(len(Y_pred_c)):\n#             dic = { 'ForecastId': X_forecastId[j], 'ConfirmedCases': Y_pred_c[j], 'Fatalities': Y_pred_f[j]}\n#             sub.append(dic)\n\n# submission = pd.DataFrame(sub)\n# submission = submission.round(0)\n# # submission[['ForecastId','ConfirmedCases','Fatalities']].to_csv(path_or_buf='submission.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}