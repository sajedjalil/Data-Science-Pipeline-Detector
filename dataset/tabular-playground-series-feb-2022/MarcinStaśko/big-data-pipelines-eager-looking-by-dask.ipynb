{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üìñ1. Dask Introduction\n\nDask is composed of two parts:\n\nDynamic task scheduling optimized for computation. This is similar to Airflow, Luigi, Celery, or Make, but optimized for interactive computational workloads.\n\n‚ÄúBig Data‚Äù collections like parallel arrays, dataframes, and lists that extend common interfaces like NumPy, Pandas, or Python iterators to larger-than-memory or distributed environments. These parallel collections run on top of dynamic task schedulers.\n\nThe tips and trics how to handle the bigger datasets\n\n![dask](https://miro.medium.com/max/744/0*IZmDXucl3oksi6oF.png)\n\n[10min to dask](https://docs.dask.org/en/latest/10-minutes-to-dask.html)","metadata":{}},{"cell_type":"markdown","source":"# üî® Loading Operation\n","metadata":{}},{"cell_type":"markdown","source":">Let`s check how long pandas dataframe will be loaded","metadata":{}},{"cell_type":"code","source":"import dask.dataframe as dd\nimport pandas as pd\nimport numpy as np\n\nTRAIN_DATA = '../input/tabular-playground-series-feb-2022/train.csv'\nTEST_DATA = '../input/tabular-playground-series-feb-2022/test.csv'\n","metadata":{"execution":{"iopub.status.busy":"2022-02-07T12:04:47.392356Z","iopub.execute_input":"2022-02-07T12:04:47.392687Z","iopub.status.idle":"2022-02-07T12:04:49.983078Z","shell.execute_reply.started":"2022-02-07T12:04:47.392593Z","shell.execute_reply":"2022-02-07T12:04:49.98237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%timeit\ndf_pandas = pd.read_csv(TRAIN_DATA)\n# Almost 16 second in lazy mode. We have to test eager mode","metadata":{"execution":{"iopub.status.busy":"2022-02-07T12:04:49.985725Z","iopub.execute_input":"2022-02-07T12:04:49.986771Z","iopub.status.idle":"2022-02-07T12:06:48.582553Z","shell.execute_reply.started":"2022-02-07T12:04:49.986729Z","shell.execute_reply":"2022-02-07T12:06:48.580887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_pandas = pd.read_csv(TRAIN_DATA)\ndf_pandas.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-07T12:06:48.583831Z","iopub.execute_input":"2022-02-07T12:06:48.584077Z","iopub.status.idle":"2022-02-07T12:07:01.749183Z","shell.execute_reply.started":"2022-02-07T12:06:48.584042Z","shell.execute_reply":"2022-02-07T12:07:01.748479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4>Let`s check how long it will take for the dask </h4>","metadata":{}},{"cell_type":"code","source":"%%timeit\ndf_dask = dd.read_csv(TRAIN_DATA,blocksize=50e6) # 50 mb blocks\n# 36 ms, more than 400 times faster!!","metadata":{"execution":{"iopub.status.busy":"2022-02-07T12:07:01.751217Z","iopub.execute_input":"2022-02-07T12:07:01.751469Z","iopub.status.idle":"2022-02-07T12:07:04.033358Z","shell.execute_reply.started":"2022-02-07T12:07:01.751435Z","shell.execute_reply":"2022-02-07T12:07:04.032665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_dask = dd.read_csv(TRAIN_DATA,blocksize=50e6) # 50 mb blocks\ndf_dask.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-07T12:07:04.034545Z","iopub.execute_input":"2022-02-07T12:07:04.034803Z","iopub.status.idle":"2022-02-07T12:07:04.766128Z","shell.execute_reply.started":"2022-02-07T12:07:04.03477Z","shell.execute_reply":"2022-02-07T12:07:04.765474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#If you want to change eager to lazy mode \n\n#dask2pandas = df_dask.compute()\n\n# I recomend to compute it at the final step and make querring using the dask and compute onlu results","metadata":{"execution":{"iopub.status.busy":"2022-02-07T12:07:04.767468Z","iopub.execute_input":"2022-02-07T12:07:04.767731Z","iopub.status.idle":"2022-02-07T12:07:04.77142Z","shell.execute_reply.started":"2022-02-07T12:07:04.767695Z","shell.execute_reply":"2022-02-07T12:07:04.770507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%timeit\n# You can use Dask only for loading the data and converting to the pandas. It is still faster than reading in pandas !!\ndf_dask = dd.read_csv(TRAIN_DATA,blocksize=50e6) # 50 mb blocks, \ndask2pandas = df_dask.compute()\n\n# Pandas .read_csv takes 16 seconds\n# Dask .read_csv and compute together takes 11.4 that is around 30% faster\n# You can play with the blocksize and share in comment with blocksize is the best for kaggle enviroment","metadata":{"execution":{"iopub.status.busy":"2022-02-07T12:07:04.772764Z","iopub.execute_input":"2022-02-07T12:07:04.773031Z","iopub.status.idle":"2022-02-07T12:08:46.131122Z","shell.execute_reply.started":"2022-02-07T12:07:04.772996Z","shell.execute_reply":"2022-02-07T12:08:46.130325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ‚öô Data Handling - Examples\n","metadata":{}},{"cell_type":"code","source":"# Dask code looks pretty the same like in pandas\nmissing_data = df_dask['target'].value_counts().compute() # Use the compute wisely because it take the most of time of dask script\n\nmissing_data = missing_data.reset_index()\nmissing_data['target'] = missing_data['target'].max() - missing_data['target'] # To count missing\n","metadata":{"execution":{"iopub.status.busy":"2022-02-07T12:08:46.13249Z","iopub.execute_input":"2022-02-07T12:08:46.13281Z","iopub.status.idle":"2022-02-07T12:08:52.024408Z","shell.execute_reply.started":"2022-02-07T12:08:46.132766Z","shell.execute_reply":"2022-02-07T12:08:52.023668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.express as px\n\nfig = px.bar(missing_data, x='index', y='target',title=\"Missing data count\")\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-07T12:11:19.894058Z","iopub.execute_input":"2022-02-07T12:11:19.894308Z","iopub.status.idle":"2022-02-07T12:11:19.951998Z","shell.execute_reply.started":"2022-02-07T12:11:19.89428Z","shell.execute_reply":"2022-02-07T12:11:19.951242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%timeit\nmissing_data_pandas = df_pandas['target'].value_counts()\n#print(missing_data_pandas)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T12:08:53.40501Z","iopub.execute_input":"2022-02-07T12:08:53.405424Z","iopub.status.idle":"2022-02-07T12:08:55.633631Z","shell.execute_reply.started":"2022-02-07T12:08:53.405384Z","shell.execute_reply":"2022-02-07T12:08:55.632102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%timeit\n\n# Let`s check how the example operations are made:\nmissing_data_dask = df_dask['target'].value_counts()\n\n# WOW around 30 times faster than pandas","metadata":{"execution":{"iopub.status.busy":"2022-02-07T12:08:55.634946Z","iopub.execute_input":"2022-02-07T12:08:55.635269Z","iopub.status.idle":"2022-02-07T12:09:02.953224Z","shell.execute_reply.started":"2022-02-07T12:08:55.635229Z","shell.execute_reply":"2022-02-07T12:09:02.952509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Why we are using the .compute() ?\nmissing_data_dask = df_dask['target'].value_counts()\n\n# The returned object is the Dask Series Structure. We can perform multiple operations but it stores \"logic\". When we want to get the value we have to make the compute\nmissing_data_dask","metadata":{"execution":{"iopub.status.busy":"2022-02-07T12:09:02.954634Z","iopub.execute_input":"2022-02-07T12:09:02.954883Z","iopub.status.idle":"2022-02-07T12:09:02.962275Z","shell.execute_reply.started":"2022-02-07T12:09:02.954848Z","shell.execute_reply":"2022-02-07T12:09:02.961595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%timeit\nmissing_data_dask.compute()\n# The compute \"generate\" the data according to the logic we created.","metadata":{"execution":{"iopub.status.busy":"2022-02-07T12:09:02.963657Z","iopub.execute_input":"2022-02-07T12:09:02.964137Z","iopub.status.idle":"2022-02-07T12:09:49.959115Z","shell.execute_reply.started":"2022-02-07T12:09:02.964099Z","shell.execute_reply":"2022-02-07T12:09:49.958366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üî≠ Code Profiling \n\n>The %%timeit is just simple measurement of the time using the magic functions in the jupyter.<br>\nI will show you how to profile the code to analyse the ETL pipeline. This is usefull for big dataset especially exceedig the RAM capa.<br>\nOur Kaggle dataset fits in the RAM capa but I will show techniques how to handle very big datasets.\n\n![profiling](https://static1.smartbear.co/smartbearbrand/files/13/139b726c-f193-4785-b1be-abd4be7980d7.png)","metadata":{}},{"cell_type":"code","source":"# We have to create some profiling tools\n\nimport time\nimport os\nimport psutil\nimport inspect\n\n\ndef elapsed_since(start):\n    #return time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - start))\n    elapsed = time.time() - start\n    if elapsed < 1:\n        return str(round(elapsed*1000,2)) + \"ms\"\n    if elapsed < 60:\n        return str(round(elapsed, 2)) + \"s\"\n    if elapsed < 3600:\n        return str(round(elapsed/60, 2)) + \"min\"\n    else:\n        return str(round(elapsed / 3600, 2)) + \"hrs\"\n\n\ndef get_process_memory():\n    process = psutil.Process(os.getpid())\n    mi = process.memory_info()\n    return mi.rss, mi.vms, mi.shared\n\n\ndef format_bytes(bytes):\n    if abs(bytes) < 1000:\n        return str(bytes)+\"B\"\n    elif abs(bytes) < 1e6:\n        return str(round(bytes/1e3,2)) + \"kB\"\n    elif abs(bytes) < 1e9:\n        return str(round(bytes / 1e6, 2)) + \"MB\"\n    else:\n        return str(round(bytes / 1e9, 2)) + \"GB\"\n\n\ndef profile(func, *args, **kwargs):\n    def wrapper(*args, **kwargs):\n        rss_before, vms_before, shared_before = get_process_memory()\n        start = time.time()\n        result = func(*args, **kwargs)\n        elapsed_time = elapsed_since(start)\n        rss_after, vms_after, shared_after = get_process_memory()\n        print(\"Profiling: {:>20}  RSS: {:>8} | VMS: {:>8} | SHR {\"\n              \":>8} | time: {:>8}\"\n            .format(\"<\" + func.__name__ + \">\",\n                    format_bytes(rss_after - rss_before),\n                    format_bytes(vms_after - vms_before),\n                    format_bytes(shared_after - shared_before),\n                    elapsed_time))\n        return result\n    if inspect.isfunction(func):\n        return wrapper\n    elif inspect.ismethod(func):\n        return wrapper(*args,**kwargs)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T12:09:49.960054Z","iopub.execute_input":"2022-02-07T12:09:49.960245Z","iopub.status.idle":"2022-02-07T12:09:49.973515Z","shell.execute_reply.started":"2022-02-07T12:09:49.960221Z","shell.execute_reply":"2022-02-07T12:09:49.972183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@profile\ndef dask_loader(src):\n    df_dask = dd.read_csv(src,blocksize=50e6)\n    return df_dask\n\n@profile\ndef pandas_loader(src):\n    df_pandas = pd.read_csv(src) # 50 mb blocks\n    return df_pandas\n\n@profile\ndef dask_loader_compute(src):\n    df_dask2pandas = dask_loader(src).compute()\n    return df_dask2pandas\n\n# The time will be longer because the profiling is attached to it\n    \npandas_loader(TRAIN_DATA)\n_ = dask_loader_compute(TRAIN_DATA)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T12:09:49.974874Z","iopub.execute_input":"2022-02-07T12:09:49.975131Z","iopub.status.idle":"2022-02-07T12:10:16.444584Z","shell.execute_reply.started":"2022-02-07T12:09:49.975096Z","shell.execute_reply":"2022-02-07T12:10:16.443798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see the data in dask loader before compute is not highly ram consumable. We can preform multiple ETL steps before we finally generate the data.\nI will also show how to store the results in the specyfic format to allow fast readout.","metadata":{}},{"cell_type":"code","source":"#!pip install dask_ml\n#!pip install scikit-learn==0.23.1","metadata":{"execution":{"iopub.status.busy":"2022-02-07T12:15:47.63978Z","iopub.execute_input":"2022-02-07T12:15:47.640442Z","iopub.status.idle":"2022-02-07T12:15:47.644008Z","shell.execute_reply.started":"2022-02-07T12:15:47.640403Z","shell.execute_reply":"2022-02-07T12:15:47.643363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\n#from dask_ml import preprocessing\n#from dask_ml.model_selection import train_test_split\n\n","metadata":{"execution":{"iopub.status.busy":"2022-02-07T12:16:21.22347Z","iopub.execute_input":"2022-02-07T12:16:21.223744Z","iopub.status.idle":"2022-02-07T12:16:21.22772Z","shell.execute_reply.started":"2022-02-07T12:16:21.223712Z","shell.execute_reply":"2022-02-07T12:16:21.227036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = dd.read_csv(TRAIN_DATA,blocksize=50e6).dropna(how='any').compute()\ntest_df = dd.read_csv(TEST_DATA,blocksize=50e6).compute()","metadata":{"execution":{"iopub.status.busy":"2022-02-07T12:18:29.096333Z","iopub.execute_input":"2022-02-07T12:18:29.096604Z","iopub.status.idle":"2022-02-07T12:18:47.84957Z","shell.execute_reply.started":"2022-02-07T12:18:29.096573Z","shell.execute_reply":"2022-02-07T12:18:47.848803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use the dask to preprocess whole data and just compute at the last step.\ntarget_encoder = LabelEncoder()\ntrain_df[\"target\"] = target_encoder.fit_transform(train_df[\"target\"])\n\nX = train_df.drop([\"target\"], axis=1)\ny = train_df[\"target\"]","metadata":{"execution":{"iopub.status.busy":"2022-02-07T12:19:05.735082Z","iopub.execute_input":"2022-02-07T12:19:05.735909Z","iopub.status.idle":"2022-02-07T12:19:05.933572Z","shell.execute_reply.started":"2022-02-07T12:19:05.735874Z","shell.execute_reply":"2022-02-07T12:19:05.932668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üöÖ Training Ensembled Meta Classifier\n\nThe ensemble technique works best when the base models are not correlated. We have 3 basics concept of ensembling techniques\nMax Voting\nThe prediction from each model is a vote. In max voting the final prediction come from the most votes\n\n* classifier 1 ‚Äì class A\n* classifier 2 ‚Äì class B\n* classifier 3 ‚Äì class B\n* Output:        class B (Averaging)\n<br>\nThe final output is an average of all predictons (regression problems)\n\n* regressor 1 ‚Äì 200\n* regressor 2 ‚Äì 300 \n* regressor 3 ‚Äì 400\n* Output:    300  (Weighted Averaging)\n<br>\nThe base model with higher predictive power is more important.\n","metadata":{}},{"cell_type":"markdown","source":"## ‚öô Training Configuration","metadata":{}},{"cell_type":"code","source":"# Training routine\nSEED = 1992\nlgbm_params = {\n    'metric': 'softmax',\n    'n_estimators': 4000,\n    'objective': 'multiclass',\n    'random_state': SEED,\n    'learning_rate': 0.025,\n    'min_child_samples': 150,\n    'reg_alpha': 3e-5,\n    'reg_lambda': 9e-2,\n    'num_leaves': 20,\n    'max_depth': 16,\n    'colsample_bytree': 0.8,\n    'subsample': 0.65,\n    'subsample_freq': 2,\n    'max_bin': 240,\n    'device':'gpu'\n}\n\n\nETC_params = {\n    'bootstrap':True,\n    'criterion': 'entropy',\n    'max_features': 0.55,\n    'min_samples_leaf': 8,\n    'min_samples_split': 4,\n    'n_estimators': 150\n}\n\ncat_params = {#'iterations': 5000,\n          'eval_metric': 'AUC',\n          'loss_function':'Logloss',\n          'od_type':'Iter',\n          'num_trees':50000,\n          'max_depth': 6, \n          'l2_leaf_reg': 3,\n          'bootstrap_type': 'Bayesian',\n          'max_bin': 254,\n          'grow_policy': \"Lossguide\",\n          'random_seed': 314,\n          'min_data_in_leaf': 64,\n          'verbose': None,\n          'logging_level': 'Silent',\n          'task_type': 'GPU'\n}","metadata":{"execution":{"iopub.status.busy":"2022-02-07T12:19:22.492309Z","iopub.execute_input":"2022-02-07T12:19:22.492851Z","iopub.status.idle":"2022-02-07T12:19:22.499036Z","shell.execute_reply.started":"2022-02-07T12:19:22.492811Z","shell.execute_reply":"2022-02-07T12:19:22.498094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## üìÅ Training Imports","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.simplefilter('ignore')\n\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom mlxtend.classifier import StackingCVClassifier,EnsembleVoteClassifier","metadata":{"execution":{"iopub.status.busy":"2022-02-07T12:19:23.598972Z","iopub.execute_input":"2022-02-07T12:19:23.59949Z","iopub.status.idle":"2022-02-07T12:19:23.831081Z","shell.execute_reply.started":"2022-02-07T12:19:23.59945Z","shell.execute_reply":"2022-02-07T12:19:23.830385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"## üî® Initialize Meta Classification","metadata":{}},{"cell_type":"code","source":"# Let`s train 2 models and make hard voting\n\ncl1 = LGBMClassifier(**lgbm_params)\ncl2 = ExtraTreesClassifier(**ETC_params)\ncl3 = CatBoostClassifier(**cat_params)\n# Hard Voting Ensemble\nS_eclf = EnsembleVoteClassifier(clfs=[cl1, cl2, cl3],\n                              weights=[1, 1, 1], voting='hard')\n","metadata":{"execution":{"iopub.status.busy":"2022-02-07T12:19:43.861732Z","iopub.execute_input":"2022-02-07T12:19:43.862509Z","iopub.status.idle":"2022-02-07T12:19:43.867694Z","shell.execute_reply.started":"2022-02-07T12:19:43.862457Z","shell.execute_reply":"2022-02-07T12:19:43.866874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_probs = []\nscores = []\n\nfolds = StratifiedKFold(n_splits=8, shuffle=True)\n\nfor fold, (train_id, test_id) in enumerate(folds.split(X, y)):  \n    X_train = X.iloc[train_id]\n    y_train = y.iloc[train_id]\n    X_valid = X.iloc[test_id]\n    y_valid = y.iloc[test_id]\n    \n    \n    S_eclf.fit(X_train, y_train) \n\n    \n    #model.fit(X_train, y_train)\n    \n    valid_pred = S_eclf.predict(X_valid)\n    valid_score = accuracy_score(y_valid, valid_pred)\n    \n    print(\"Fold:\", fold + 1, \"Accuracy:\", valid_score)\n    \n    scores.append(valid_score)\n    \n    # Save predictions to later submit the mean values\n    #if submission: \n    y_probs.append(S_eclf.predict_proba(test_df))\n    ","metadata":{"execution":{"iopub.status.busy":"2022-02-07T12:20:17.498164Z","iopub.execute_input":"2022-02-07T12:20:17.498704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Mean Accuracy Score\nprint(\"Mean accuracy score:\", np.array(scores).mean())","metadata":{"execution":{"iopub.status.busy":"2022-02-05T10:45:07.491904Z","iopub.execute_input":"2022-02-05T10:45:07.492761Z","iopub.status.idle":"2022-02-05T10:45:07.498429Z","shell.execute_reply.started":"2022-02-05T10:45:07.492715Z","shell.execute_reply":"2022-02-05T10:45:07.497711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üìñ Submission","metadata":{}},{"cell_type":"code","source":"y_prob = sum(y_probs) / len(y_probs)\n# The explanations for these numbers are in AMBROSM's code\ny_prob += np.array([0, 0, 0.01, 0.03, 0, 0, 0, 0, 0, 0])\ny_pred_tuned = target_encoder.inverse_transform(np.argmax(y_prob, axis=1))\npd.Series(y_pred_tuned, index=test_df.index).value_counts().sort_index() / len(test_df) * 100","metadata":{"execution":{"iopub.status.busy":"2022-02-05T10:45:27.473308Z","iopub.execute_input":"2022-02-05T10:45:27.473741Z","iopub.status.idle":"2022-02-05T10:45:27.540825Z","shell.execute_reply.started":"2022-02-05T10:45:27.473693Z","shell.execute_reply":"2022-02-05T10:45:27.539913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = dd.read_csv(\"../input/tabular-playground-series-feb-2022/sample_submission.csv\").compute()\n\nsub[\"target\"] = y_pred_tuned\nsub.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-05T10:46:36.862076Z","iopub.execute_input":"2022-02-05T10:46:36.862394Z","iopub.status.idle":"2022-02-05T10:46:37.374535Z","shell.execute_reply.started":"2022-02-05T10:46:36.86236Z","shell.execute_reply":"2022-02-05T10:46:37.373254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4> Notebook in progress Upvote if you like :) </h4>\n","metadata":{}}]}