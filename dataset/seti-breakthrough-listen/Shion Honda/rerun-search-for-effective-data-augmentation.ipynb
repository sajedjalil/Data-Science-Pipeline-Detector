{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Search for Effective Data Augmentation & TTA\nI don't know anything about data augmentation for spectograms, so I tried different types of augmentations and checked if it improves the validation score.\n\n![](https://images.unsplash.com/photo-1613744450985-fc6372fe6a12?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=1350&q=80)\n\n# TL;DR\nI experimented with these augmentations and also measured the effect of test time augmentation:\n- Mixup\n- CutMix\n- Horizontal flip\n- Vertical flip\n- Shift scale rotate\n- Random resized crop\n- Motion blur\n- SpecAugment\n- Random brightness contrast\n- Gaussian noise\n\nComments are welcome!\n\n# Notes\nI'd like to mention these notebooks/discussions that helped me a lot!\n- https://www.kaggle.com/micheomaano/efficientnet-b4-mixup-cv-0-98-lb-0-97\n- https://www.kaggle.com/c/seti-breakthrough-listen/discussion/242644\n- https://www.kaggle.com/yasufuminakama/seti-nfnet-l0-starter-training\n\n**UPDATE on June 5th**: Added CutMix, SpecAugment, TTA","metadata":{}},{"cell_type":"markdown","source":"# Setups","metadata":{}},{"cell_type":"code","source":"!pip install git+https://github.com/rwightman/pytorch-image-models\nimport timm","metadata":{"execution":{"iopub.status.busy":"2021-08-10T22:19:50.351521Z","iopub.execute_input":"2021-08-10T22:19:50.35188Z","iopub.status.idle":"2021-08-10T22:19:59.748516Z","shell.execute_reply.started":"2021-08-10T22:19:50.351848Z","shell.execute_reply":"2021-08-10T22:19:59.747386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nimport os\nimport math\nimport time\nimport random\nimport gc\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom tqdm.notebook import tqdm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom albumentations.core.transforms_interface import ImageOnlyTransform","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-10T22:19:59.752217Z","iopub.execute_input":"2021-08-10T22:19:59.752506Z","iopub.status.idle":"2021-08-10T22:19:59.761423Z","shell.execute_reply.started":"2021-08-10T22:19:59.752474Z","shell.execute_reply":"2021-08-10T22:19:59.760229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    seed = 46\n    debug = False\n    model_name = \"tf_efficientnet_b0\"\n    n_epoch = 10\n    n_tta = 3\n    size = 256\n    lr = 5e-4\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nprint(f\"Using device {CFG.device}\")\n\ndef seed_torch(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nseed_torch(CFG.seed)","metadata":{"execution":{"iopub.status.busy":"2021-08-10T22:19:59.763367Z","iopub.execute_input":"2021-08-10T22:19:59.76388Z","iopub.status.idle":"2021-08-10T22:19:59.775903Z","shell.execute_reply.started":"2021-08-10T22:19:59.763837Z","shell.execute_reply":"2021-08-10T22:19:59.774843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/seti-breakthrough-listen/train_labels.csv')\ntrain['file_path'] = train['id'].apply(lambda x:\n                                      f\"../input/seti-breakthrough-listen/train/{x[0]}/{x}.npy\")\n\nif CFG.debug:\n    CFG.n_epoch = 1\n    train = train.sample(n=128, random_state=CFG.seed).reset_index(drop=True)\nelse:\n    train = train.sample(frac=0.25, random_state=CFG.seed).reset_index(drop=True)\n\ntrain, valid = train_test_split(train, test_size=0.25, random_state=CFG.seed)\ndisplay(train)\ndisplay(valid)","metadata":{"execution":{"iopub.status.busy":"2021-08-10T22:19:59.778044Z","iopub.execute_input":"2021-08-10T22:19:59.77851Z","iopub.status.idle":"2021-08-10T22:19:59.884249Z","shell.execute_reply.started":"2021-08-10T22:19:59.778475Z","shell.execute_reply":"2021-08-10T22:19:59.883415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Augmentations","metadata":{}},{"cell_type":"code","source":"def spec_augment(x, alpha=0.1):\n    t0 = np.random.randint(0, x.shape[0])\n    delta = np.random.randint(0, int(x.shape[0]*alpha))\n    x[t0:min(t0+delta, x.shape[0])] = 0\n    t0 = np.random.randint(0, x.shape[1])\n    delta = np.random.randint(0, int(x.shape[1]*alpha))\n    x[:, t0:min(t0+delta, x.shape[1])] = 0\n    return x\n\nclass SpecAugment(ImageOnlyTransform):\n    def apply(self, img, **params):\n        return spec_augment(img)\n\np = 0.5\nDA_DICT = {\n    \"spec_augment\": SpecAugment(p=p),\n    \"hflip\": A.HorizontalFlip(p=p),\n    \"vflip\": A.VerticalFlip(p=p),\n    \"shift_scale_rotate\": A.ShiftScaleRotate(rotate_limit=0, p=p),\n    \"random_resized_crop\": A.RandomResizedCrop(height=CFG.size, width=CFG.size, p=p),\n    \"motion_blur\": A.MotionBlur(p=p),\n    \"random_brightness_contrast\": A.RandomBrightnessContrast(p=p),\n    \"gauss_noise\": A.GaussNoise(var_limit=(0.1, 1), p=0.5)\n}","metadata":{"execution":{"iopub.status.busy":"2021-08-10T22:19:59.885575Z","iopub.execute_input":"2021-08-10T22:19:59.885915Z","iopub.status.idle":"2021-08-10T22:19:59.896885Z","shell.execute_reply.started":"2021-08-10T22:19:59.885878Z","shell.execute_reply":"2021-08-10T22:19:59.895942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset with Different Transforms","metadata":{}},{"cell_type":"code","source":"class SETIDataset(Dataset):\n    def __init__(self, df, transform):\n        self.df = df\n        self.file_names = df['file_path'].values\n        self.labels = df[\"target\"].values\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        image = np.load(self.file_names[idx]).astype(np.float32) # (6, 273, 256)\n        image = np.vstack(image).transpose((1, 0)) # (256, 1638)\n        image = self.transform(image=image)['image']\n        label = torch.tensor(self.labels[idx]).float()\n        return image, label\n\ndef get_transforms(da):\n    if da not in DA_DICT:\n        return A.Compose([\n            A.Resize(CFG.size, CFG.size),\n            ToTensorV2(),\n        ])\n\n    else:\n        return A.Compose([\n            A.Resize(CFG.size, CFG.size),\n            DA_DICT[da],\n            ToTensorV2(),\n        ])\n    \ndef mixup(x, y, alpha=1.0):\n    '''Returns mixed inputs, pairs of targets, and lambda'''\n    if alpha > 0:\n        lam = np.random.beta(alpha, alpha)\n    else:\n        lam = 1\n\n    batch_size = x.size()[0]\n    index = torch.randperm(batch_size).to(CFG.device)\n\n    mixed_x = lam * x + (1 - lam) * x[index, :]\n    y_a, y_b = y, y[index]\n    return mixed_x, y_a, y_b, lam\n\ndef rand_bbox(W, H, lam):\n    cut_rat = np.sqrt(1. - lam)\n    cut_w = np.int(W * cut_rat)\n    cut_h = np.int(H * cut_rat)\n\n    # uniform\n    cx = np.random.randint(cut_w // 2, W - cut_w // 2)\n    cy = np.random.randint(cut_h // 2, H - cut_h // 2)\n\n    bbx1 = cx - cut_w // 2\n    bby1 = cy - cut_h // 2\n    bbx2 = cx + cut_w // 2\n    bby2 = cy + cut_h // 2\n\n    return bbx1, bby1, bbx2, bby2\n\ndef cutmix(x, y, alpha=1.0):\n    if alpha > 0:\n        lam = np.random.beta(alpha, alpha)\n    else:\n        lam = 1\n\n    batch_size = x.size()[0]\n    index = torch.randperm(batch_size).to(CFG.device)\n\n    bbx1, bby1, bbx2, bby2 = rand_bbox(x.size()[1], x.size()[2], lam)\n    x[:, bbx1:bbx2, bby1:bby2] = x[index, bbx1:bbx2, bby1:bby2]\n    y_a, y_b = y, y[index]\n    return x, y_a, y_b, lam","metadata":{"execution":{"iopub.status.busy":"2021-08-10T22:19:59.898411Z","iopub.execute_input":"2021-08-10T22:19:59.898809Z","iopub.status.idle":"2021-08-10T22:19:59.916229Z","shell.execute_reply.started":"2021-08-10T22:19:59.898752Z","shell.execute_reply":"2021-08-10T22:19:59.915099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training & Evaluation","metadata":{}},{"cell_type":"code","source":"def criterion(outputs, targets):\n    return nn.BCEWithLogitsLoss()(outputs, targets)\n\ndef mix_criterion(pred, y_a, y_b, lam):\n    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n\ndef train_one_epoch(model, optimizer, dataloader, mix=None):\n    model.train()\n    \n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    for step, (images, labels) in bar:         \n        images = images.to(CFG.device)\n        labels = labels.to(CFG.device)\n        \n        if mix==\"mixup\":\n            images, targets_a, targets_b, lam = mixup(images, labels.view(-1, 1))\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = mix_criterion(outputs, targets_a, targets_b, lam)\n        elif mix==\"cutmix\":\n            images, targets_a, targets_b, lam = cutmix(images, labels.view(-1, 1))\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = mix_criterion(outputs, targets_a, targets_b, lam)\n        else:\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs.view(-1), labels)\n            \n        loss.backward()\n        optimizer.step()\n\ndef valid_one_epoch(model, dataloader):\n    model.eval()\n    targets = []\n    preds = []\n    \n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    for step, (images, labels) in bar:        \n        images = images.to(CFG.device)\n        labels = labels.to(CFG.device)        \n        with torch.no_grad():\n            outputs = model(images)\n        preds.append(outputs.sigmoid().cpu().detach().numpy())\n        targets.append(labels.view(-1).cpu().detach().numpy())\n    \n    targets = np.concatenate(targets)\n    preds = np.concatenate(preds)\n    return roc_auc_score(targets, preds)\n\ndef valid_one_epoch_tta(model, dataloader):\n    model.eval()\n    PREDS = np.zeros(len(dataloader.dataset))\n    for _ in range(CFG.n_tta):\n        targets = []\n        preds = []\n        bar = tqdm(enumerate(dataloader), total=len(dataloader))\n        for step, (images, labels) in bar:        \n            images = images.to(CFG.device)\n            labels = labels.to(CFG.device)        \n            with torch.no_grad():\n                outputs = model(images)\n            preds.append(outputs.sigmoid().cpu().detach().numpy())\n            targets.append(labels.view(-1).cpu().detach().numpy())\n        targets = np.concatenate(targets)\n        PREDS += np.concatenate(preds).reshape(-1)\n    PREDS /= CFG.n_tta\n    return roc_auc_score(targets, PREDS)\n\ndef run(da):\n    train_dataset = SETIDataset(train, transform=get_transforms(da))\n    valid_dataset = SETIDataset(valid, transform=get_transforms(None))\n    tta_dataset = SETIDataset(valid, transform=get_transforms(da))\n    train_loader = DataLoader(train_dataset, batch_size=32, \n                              num_workers=4, shuffle=True, pin_memory=True)\n    valid_loader = DataLoader(valid_dataset, batch_size=64, \n                              num_workers=4, shuffle=False, pin_memory=True)\n    tta_loader = DataLoader(tta_dataset, batch_size=64, \n                              num_workers=4, shuffle=False, pin_memory=True)\n    del train_dataset, valid_dataset, tta_dataset\n    model = timm.create_model(CFG.model_name, pretrained=True, in_chans=1, num_classes=1)\n    model.to(CFG.device)\n    optimizer = torch.optim.Adam(\n        model.parameters(), lr=CFG.lr\n    )\n    \n    valid_scores = []\n    tta_scores = []\n    for epoch in range(1, CFG.n_epoch + 1):\n        train_one_epoch(model, optimizer, train_loader, da)\n        print(da, \"Train\")\n        gc.collect()\n        torch.cuda.empty_cache()\n        score = valid_one_epoch(model, valid_loader)\n        valid_scores.append(score)\n        print(da, \"Valid\", score)\n        gc.collect()\n        torch.cuda.empty_cache()\n        if da in DA_DICT:\n            score = valid_one_epoch_tta(model, tta_loader)\n            tta_scores.append(score)\n            print(da, \"TTA\", score)\n            gc.collect()\n            torch.cuda.empty_cache()\n    return valid_scores, tta_scores","metadata":{"execution":{"iopub.status.busy":"2021-08-10T22:19:59.91781Z","iopub.execute_input":"2021-08-10T22:19:59.918247Z","iopub.status.idle":"2021-08-10T22:19:59.945219Z","shell.execute_reply.started":"2021-08-10T22:19:59.918211Z","shell.execute_reply":"2021-08-10T22:19:59.944166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Result","metadata":{}},{"cell_type":"code","source":"das = list(DA_DICT.keys()) + [\"mixup\", \"cutmix\"]\n\nbase_scores, _ = run(None)\n\nda_adopt = []\nfor da in das:\n    valid_scores, tta_scores = run(da)\n\n    plt.figure()\n    plt.plot(range(1, CFG.n_epoch + 1), base_scores, label=\"Baseline\", marker=\".\")\n    plt.plot(range(1, CFG.n_epoch + 1), valid_scores, label=\"+DA\", marker=\".\")\n    if da in DA_DICT:\n        plt.plot(range(1, CFG.n_epoch + 1), tta_scores, label=\"+DA +TTA\", marker=\".\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Valid AUC\")\n    plt.legend()\n    plt.title(da)\n    plt.show()\n\n    if max(valid_scores) > max(base_scores):\n        print(f\"{da} improves AUC!\")\n        da_adopt.append(da)\n    elif tta_scores and max(tta_scores) > max(base_scores):\n        print(f\"{da} improves AUC!\")\n        da_adopt.append(da)\n    else:\n        print(f\"{da} does not improve AUC.\")\n    \n    gc.collect()\n    torch.cuda.empty_cache()\n\nprint(\"\\n===== Result Summary =====\")\nprint(\"The following data augmentation improves the validation score\")\nfor da in da_adopt:\n    print(da)","metadata":{"execution":{"iopub.status.busy":"2021-08-10T22:19:59.947498Z","iopub.execute_input":"2021-08-10T22:19:59.948Z","iopub.status.idle":"2021-08-10T22:33:06.559235Z","shell.execute_reply.started":"2021-08-10T22:19:59.947963Z","shell.execute_reply":"2021-08-10T22:33:06.552285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Next Directions\n- [RandAugment](https://arxiv.org/abs/1909.13719)\n  - As far as I understand, it automatically chooses the best set of augmentations with the optimal parameters\n- Tuning augmentation probability `p`\n  - [The EfficientNetV2 paper](https://arxiv.org/abs/2104.00298) says larger input images need stronger regularization and hence heavier augmentations","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}