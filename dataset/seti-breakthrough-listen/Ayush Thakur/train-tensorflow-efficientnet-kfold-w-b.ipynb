{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Starter kernel using TensorFlow to train a model (EfficientNet) using K-fold cross validation. Weights and Biases is used for experiment tracking. \n\nIdea:\n\n* Each data point consist of 6 spectrograms where the first, third and fifth spectrograms can potentially have a UFO signal. \n* Stack these spectrograms to get a three channel image followed by resizing to 224x224 resolution.\n* Train an EfficientNet-B0 as a baseline. \n\nThe model was trained on a single V100 GPU (GCP instance). \n\n> I will update the kernel with the CV score and LB score along with W&B dashboard to show how the training progressed. \n\n| Method        | CV Score           | LB Score  | W&B Dashboard |\n| ------------- |:-------------:| :-----:| -------: |\n| Single Model     | 0.95 | 0.91 | [W&B Run Page](https://wandb.ai/ayush-thakur/kaggle-seti/runs/314vuphr) |\n| 5 Fold training  | 0.93      |   0.93 | [W&B Project Page](https://wandb.ai/ayush-thakur/kaggle-seti)\n\nI am publisizing this kernel so that I can be of use if you are biased towards using TensorFlow/Keras. If you want to train the baseline on a Kaggle kernel consider reducing the batch size. ","metadata":{}},{"cell_type":"markdown","source":"# üß∞ Imports and Setups","metadata":{}},{"cell_type":"code","source":"!pip install -q --upgrade wandb \n\nimport wandb\nprint(wandb.__version__)\nfrom wandb.keras import WandbCallback\n\nwandb.login()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nprint(tf.__version__)\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import models\nimport tensorflow_addons as tfa\nfrom tensorflow.keras import mixed_precision\n\nimport os\nimport gc\nimport json\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.utils.class_weight import compute_class_weight","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gpus = tf.config.list_physical_devices('GPU')\nif gpus:\n  try:\n    # Currently, memory growth needs to be the same across GPUs\n    for gpu in gpus:\n      tf.config.experimental.set_memory_growth(gpu, True)\n    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n  except RuntimeError as e:\n    # Memory growth must be set before GPUs have been initialized\n    print(e)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üìÄ Hyperparameters","metadata":{}},{"cell_type":"code","source":"TRAIN_PATH = 'train/'\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n\nCONFIG = dict (\n    seed = 42,\n    num_labels = 1,\n    num_folds = 5,\n    train_val_split = 0.2,\n    img_width = 224,\n    img_height = 224,\n    batch_size = 64,\n    epochs = 100,\n    learning_rate = 4e-4,\n    wandb_kernel = True,\n    architecture = \"CNN\",\n    infra = \"Kaggle\",\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üî® Build Input Pipeline","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/seti-breakthrough-listen/train_labels.csv')\nprint(f'Number of train images: {len(df)}')\ndf['img_path'] = df['id'].apply(lambda x: f'../input/seti-breakthrough-listen/train/{x[0]}/{x}.npy')\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_weights = compute_class_weight('balanced', \n                                    classes=np.unique(df['target'].values),\n                                    y=df['target'].values)\n\nclass_weights_dict = {key: val for key, val in zip(np.unique(df['target'].values), class_weights)}\nclass_weights_dict                                                            ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Fold = StratifiedKFold(n_splits=CONFIG['num_folds'], shuffle=True, random_state=CONFIG['seed'])\nfor n, (train_index, val_index) in enumerate(Fold.split(df, df['target'])):\n    df.loc[val_index, 'fold'] = int(n)\ndf['fold'] = df['fold'].astype(int)\ndf.groupby(['fold', 'target']).size()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_npy(path):\n    # load npy data\n    data = np.load(path.numpy()).astype(np.float32)\n    # stack \n    data = np.dstack((data[0], data[2], data[4]))\n    # Normalize\n    \n    return data\n\n@tf.function\ndef load_resize_spec(df_dict):\n    # Load image\n    [image,] = tf.py_function(load_npy, [df_dict['img_path']], [tf.float32])\n    image.set_shape((273, 256, 3))\n    \n    # Resize image\n    image = tf.image.resize(image, (CONFIG['img_height'], CONFIG['img_width']))\n    \n    # Parse label\n    label = df_dict['target']\n    label = tf.cast(label, tf.float32)\n    \n    return image, label","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"AUTOTUNE = tf.data.AUTOTUNE\n\ndef get_dataloaders(train_df, valid_df):\n    trainloader = tf.data.Dataset.from_tensor_slices(dict(train_df))\n    validloader = tf.data.Dataset.from_tensor_slices(dict(valid_df))\n\n    trainloader = (\n        trainloader\n        .shuffle(1024)\n        .map(load_resize_spec, num_parallel_calls=AUTOTUNE)\n        .batch(CONFIG['batch_size'])\n        .prefetch(AUTOTUNE)\n    )\n\n    validloader = (\n        validloader\n        .map(load_resize_spec, num_parallel_calls=AUTOTUNE)\n        .batch(CONFIG['batch_size'])\n        .prefetch(AUTOTUNE)\n    )\n    \n    return trainloader, validloader","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üê§ Model","metadata":{}},{"cell_type":"code","source":"def get_model():\n    base_model = tf.keras.applications.EfficientNetB0(include_top=False, weights='imagenet')\n    base_model.trainabe = True\n\n    inputs = layers.Input((CONFIG['img_height'], CONFIG['img_width'], 3))\n    x = base_model(inputs, training=True)\n    x = layers.GlobalAveragePooling2D()(x)\n    x = layers.Dropout(0.5)(x)\n    \n    outputs = layers.Dense(CONFIG['num_labels'])(x)\n    outputs = layers.Activation('sigmoid', dtype='float32', name='predictions')(outputs)\n    \n    return models.Model(inputs, outputs)\n\ntf.keras.backend.clear_session() \nmodel = get_model()\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CONFIG['model_name'] = 'efficientnetb0-folds'\nCONFIG['group'] = 'K-Fold-EnetB0'\nCONFIG['run_name'] = 'baseline-k-fold'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Callbacks","metadata":{}},{"cell_type":"code","source":"# Callbacks\nearlystopper = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', patience=5, verbose=0, mode='min',\n    restore_best_weights=True\n)\n\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n                              patience=3, min_lr=CONFIG['learning_rate'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" ","metadata":{}},{"cell_type":"markdown","source":"# üöÑ Train","metadata":{}},{"cell_type":"code","source":"def get_predictions(model, validloader, valid_df):\n    y_pred = []\n    for image_batch, label_batch in validloader:\n        preds = model.predict(image_batch)\n        y_pred.extend(preds)\n        \n    valid_df['preds'] = y_pred\n    \n    return valid_df ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oof_df = pd.DataFrame()\n\nfor fold in range(CONFIG['num_folds']):\n    # Prepare train and valid df\n    train_df = df.loc[df.fold != fold].reset_index(drop=True)\n    valid_df = df.loc[df.fold == fold].reset_index(drop=True)\n\n    # Prepare dataloaders\n    trainloader, validloader = get_dataloaders(train_df, valid_df)\n    \n    \n    # Initialize model\n    tf.keras.backend.clear_session()\n    model = get_model()\n\n    # Compile model\n    optimizer = tf.keras.optimizers.Adam(learning_rate=CONFIG['learning_rate'])\n    model.compile(optimizer, \n                  loss=tfa.losses.SigmoidFocalCrossEntropy(), \n                  metrics=[tf.keras.metrics.AUC(curve='ROC')])\n\n\n    # Update CONFIG dict with the name of the model.\n    print('Training configuration: ', CONFIG)\n\n    # Initialize W&B run\n    run = wandb.init(project='kaggle-seti', \n                     config=CONFIG,\n                     group=CONFIG['group'], \n                     job_type='train',\n                     name=CONFIG['run_name'])\n\n    # Train\n    _ = model.fit(trainloader, \n                  epochs=CONFIG['epochs'],\n                  validation_data=validloader,\n                  class_weight=class_weights_dict,\n                  callbacks=[WandbCallback(),\n                             earlystopper,\n                             reduce_lr])\n    \n    \n    # Evaluate\n    loss, auc = model.evaluate(validloader)\n    wandb.log({'Val AUC-ROC': auc})\n    \n    # Save model\n    model_name = CONFIG['model_name']\n    MODEL_PATH = f'models/{model_name}'\n    os.makedirs(MODEL_PATH, exist_ok=True)\n    count_models = len(os.listdir(MODEL_PATH))\n    \n    model.save(f'{MODEL_PATH}/{model_name}_{count_models}.h5')\n\n    # Get Prediction on validation set\n    _oof_df = get_predictions(model, validloader, valid_df)\n    oof_df = pd.concat([oof_df, _oof_df])\n\n    # Close W&B run\n    run.finish()\n    \n    del model, trainloader, validloader, _oof_df\n    _ = gc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![img](https://i.imgur.com/QW7uCAK.gif)","metadata":{}},{"cell_type":"code","source":"def correct_preds(row):\n    return row.preds[0]\n\noof_df['preds'] = oof_df.apply(lambda row: correct_preds(row), axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metric = tf.keras.metrics.AUC()\nmetric.update_state(oof_df.target.values, oof_df.preds.values)\nprint(f'CV Score: {metric.result().numpy()}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}