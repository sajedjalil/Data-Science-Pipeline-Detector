{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip -q install timm\n","metadata":{"execution":{"iopub.status.busy":"2021-10-13T13:37:02.020256Z","iopub.execute_input":"2021-10-13T13:37:02.020599Z","iopub.status.idle":"2021-10-13T13:37:08.84167Z","shell.execute_reply.started":"2021-10-13T13:37:02.020498Z","shell.execute_reply":"2021-10-13T13:37:08.84078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nfrom tqdm import tqdm\nimport time\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.cuda.amp import autocast, GradScaler\nfrom torch.optim import Adam, SGD\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau, OneCycleLR\nfrom torch.optim.optimizer import Optimizer\nimport torchvision.utils as vutils\n\n# import pytorch_lightning as pl\n# from pytorch_lightning import seed_everything\n# from pytorch_lightning.metrics.functional import accuracy, f1, auroc\n# from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n\nimport timm\nimport albumentations as A\nfrom albumentations.core.transforms_interface import ImageOnlyTransform\nfrom albumentations.pytorch import ToTensorV2","metadata":{"execution":{"iopub.status.busy":"2021-10-13T13:37:20.894488Z","iopub.execute_input":"2021-10-13T13:37:20.895313Z","iopub.status.idle":"2021-10-13T13:37:24.743261Z","shell.execute_reply.started":"2021-10-13T13:37:20.895275Z","shell.execute_reply":"2021-10-13T13:37:24.741756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SEED = 42\nN_FOLDS = 5\nTRAIN_FOLD = 4\nTARGET_COL = 'target'\nN_EPOCHS = 17\nBATCH_SIZE = 32\nDIM1 = 256\nDIM2 = 256\nLR = 1e-4\nMAX_LR = 5e-4\nPRECISION = 16\nGRADIENT_ACCUMULATION = 1\nEARLY_STOP = 3\nMODEL = 'efficientnet_b1'\n# backbone = 'vit_deit_base_distilled_patch16_384'\nNEW_HEAD = False\nCHANNELS = [0, 2, 4]\n\n\nLR *= BATCH_SIZE / 32\nMAX_LR *= BATCH_SIZE / 32","metadata":{"execution":{"iopub.status.busy":"2021-10-13T13:37:38.434159Z","iopub.execute_input":"2021-10-13T13:37:38.434471Z","iopub.status.idle":"2021-10-13T13:37:38.441778Z","shell.execute_reply.started":"2021-10-13T13:37:38.434439Z","shell.execute_reply":"2021-10-13T13:37:38.44069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed = int):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    random_state = np.random.RandomState(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    return random_state\n\nrandom_state = set_seed(SEED)","metadata":{"execution":{"iopub.status.busy":"2021-10-13T13:38:03.226062Z","iopub.execute_input":"2021-10-13T13:38:03.22633Z","iopub.status.idle":"2021-10-13T13:38:03.236609Z","shell.execute_reply.started":"2021-10-13T13:38:03.226301Z","shell.execute_reply":"2021-10-13T13:38:03.235894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    print(\"GPU is available.\")\nelse:\n    device = torch.device(\"cpu\")\n    print(\"GPU not available, going to use CPU instead.\")","metadata":{"execution":{"iopub.status.busy":"2021-10-13T13:38:11.91004Z","iopub.execute_input":"2021-10-13T13:38:11.910758Z","iopub.status.idle":"2021-10-13T13:38:11.945335Z","shell.execute_reply.started":"2021-10-13T13:38:11.91071Z","shell.execute_reply":"2021-10-13T13:38:11.944341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/seti-breakthrough-listen/train_labels.csv')\ntest = pd.read_csv('../input/seti-breakthrough-listen/sample_submission.csv')\n\ndef get_train_file_path(image_id):\n    return \"../input/seti-breakthrough-listen/train/{}/{}.npy\".format(image_id[0], image_id)\n\ndef get_test_file_path(image_id):\n    return \"../input/seti-breakthrough-listen/test/{}/{}.npy\".format(image_id[0], image_id)\n\ntrain['file_path'] = train['id'].apply(get_train_file_path)\ntest['file_path'] = test['id'].apply(get_test_file_path)\n\ndisplay(train.sample(5))","metadata":{"execution":{"iopub.status.busy":"2021-10-13T13:39:07.602379Z","iopub.execute_input":"2021-10-13T13:39:07.603036Z","iopub.status.idle":"2021-10-13T13:39:07.808853Z","shell.execute_reply.started":"2021-10-13T13:39:07.602996Z","shell.execute_reply":"2021-10-13T13:39:07.808114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TrainDataset(Dataset):\n    def __init__(self, df, test=False, transform=None, use_vit=False):\n        self.df = df\n        self.test = test\n        self.file_names = df['file_path'].values\n        if not self.test:\n            self.labels = df[TARGET_COL].values\n        self.transform = transform\n        self.use_vit = use_vit\n        \n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        file_path = self.file_names[idx]\n        \n        image = np.load(file_path)[CHANNELS]\n        image = image.astype(np.float32)\n        image = np.vstack(image).T\n        if self.transform:\n            image = self.transform(image=image)['image']\n            image = self.inv_stem(image)\n        else:\n            image = image[np.newaxis,:,:]\n            image = torch.from_numpy(image).float()\n            image = self.inv_stem(image)\n        if not self.test:\n            label = torch.unsqueeze(torch.tensor(self.labels[idx]).float(),-1)\n            return {\"spect\":image, \"target\": label}\n        else:\n            return {\"spect\":image}\n        \n    def inv_stem(self, x):\n        if self.use_vit:\n            x1 = x.transpose(0, 1).view(24, 24, 16, 16)\n            y = torch.zeros(384, 384, dtype=x.dtype)\n            for i in range(24):\n                for j in range(24):\n                    y[i*16:(i+1)*16, j*16:(j+1)*16] = x1[i, j]\n            return y\n        else:\n            return x","metadata":{"execution":{"iopub.status.busy":"2021-10-13T13:39:19.012272Z","iopub.execute_input":"2021-10-13T13:39:19.01301Z","iopub.status.idle":"2021-10-13T13:39:19.024349Z","shell.execute_reply.started":"2021-10-13T13:39:19.012972Z","shell.execute_reply":"2021-10-13T13:39:19.023692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def spec_augment(x, alpha=0.1):\n    t0 = np.random.randint(0, x.shape[0])\n    delta = np.random.randint(0, int(x.shape[0]*alpha))\n    x[t0:min(t0+delta, x.shape[0])] = 0\n    t0 = np.random.randint(0, x.shape[1])\n    delta = np.random.randint(0, int(x.shape[1]*alpha))\n    x[:, t0:min(t0+delta, x.shape[1])] = 0\n    return x\n\nclass SpecAugment(ImageOnlyTransform):\n    def apply(self, img, **params):\n        return spec_augment(img)\n    \n# https://www.kaggle.com/shionhonda/search-for-effective-data-augmentation\ndef get_transforms(*, data):\n    if data == 'train':\n        return A.Compose([\n            A.Resize(DIM1,\n                    DIM2),\n            A.VerticalFlip(p=0.5),\n            #A.HorizontalFlip(p=0.5),\n            A.ShiftScaleRotate(rotate_limit=0, p=0.3),\n            A.MotionBlur(p=0.3),\n            SpecAugment(p=0.3),\n            ToTensorV2(),\n        ])\n    \n    elif data == 'valid':\n        return A.Compose([\n            A.Resize(DIM1, DIM2),\n            ToTensorV2(),\n        ])","metadata":{"execution":{"iopub.status.busy":"2021-10-13T13:39:29.91217Z","iopub.execute_input":"2021-10-13T13:39:29.912457Z","iopub.status.idle":"2021-10-13T13:39:29.92442Z","shell.execute_reply.started":"2021-10-13T13:39:29.912426Z","shell.execute_reply":"2021-10-13T13:39:29.923672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_dataloader():\n    return DataLoader(train_dataset, batch_size=4, num_workers=2,\n                      drop_last=False, shuffle=False, pin_memory=True)\ntrain_dataset = TrainDataset(train[train['target']==0], transform=get_transforms(data='train'), use_vit=False)\ntrain_batch = next(iter(train_dataloader()))\nbatch, targets = train_batch[\"spect\"], train_batch[\"target\"]\nprint(batch.shape)\nplt.figure(figsize=(16, 16))\nplt.axis(\"off\")\nplt.title(\"Target = 0\")\nplt.imshow(vutils.make_grid(\n    batch, nrow=1, padding=10, normalize=True).permute(1,2,0).cpu().numpy())\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-13T13:39:49.437568Z","iopub.execute_input":"2021-10-13T13:39:49.438277Z","iopub.status.idle":"2021-10-13T13:39:56.114783Z","shell.execute_reply.started":"2021-10-13T13:39:49.438238Z","shell.execute_reply":"2021-10-13T13:39:56.114064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = TrainDataset(train[train['target']==1], transform=None)\ntrain_batch = next(iter(train_dataloader()))\nbatch, targets = train_batch[\"spect\"], train_batch[\"target\"]\n\nplt.figure(figsize=(16, 16))\nplt.axis(\"off\")\nplt.title(\"Target = 1\")\nplt.imshow(vutils.make_grid(\n    batch, nrow=1, padding=10, normalize=True).permute(1,2,0).cpu().numpy())\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-13T13:40:15.219391Z","iopub.execute_input":"2021-10-13T13:40:15.220228Z","iopub.status.idle":"2021-10-13T13:40:16.228174Z","shell.execute_reply.started":"2021-10-13T13:40:15.220189Z","shell.execute_reply":"2021-10-13T13:40:16.227441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def mixup_data(x, y, alpha=1.0):\n    '''Returns mixed inputs, pairs of targets, and lambda'''\n    if alpha > 0:\n        lam = torch.distributions.Beta(alpha, alpha)\n    else:\n        lam = 1\n\n    batch_size = x.size()[0]\n\n    index = torch.randperm(batch_size)\n\n    mixed_x = lam * x + (1 - lam) * x[index, :]\n    y_a, y_b = y, y[index]\n    return mixed_x, y_a, y_b, lam\n\n\ndef mixup_criterion(criterion, pred, y_a, y_b, lam):\n    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)","metadata":{"execution":{"iopub.status.busy":"2021-10-13T13:40:34.355922Z","iopub.execute_input":"2021-10-13T13:40:34.356237Z","iopub.status.idle":"2021-10-13T13:40:34.364344Z","shell.execute_reply.started":"2021-10-13T13:40:34.356204Z","shell.execute_reply":"2021-10-13T13:40:34.363643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.distributions import Beta\n\nclass Mixup(nn.Module):\n    def __init__(self, mix_beta=1.0):\n\n        super(Mixup, self).__init__()\n        self.beta_distribution = Beta(mix_beta, mix_beta)\n\n    def forward(self, x, y):\n        lam = self.beta_distribution.sample().to(device)\n        batch_size = x.shape[0]\n        index = torch.randperm(batch_size)\n        mixed_x = lam * x + (1 - lam) * x[index, :]\n        y_a, y_b = y, y[index]\n        return mixed_x, y_a, y_b, lam","metadata":{"execution":{"iopub.status.busy":"2021-10-13T13:40:49.353793Z","iopub.execute_input":"2021-10-13T13:40:49.354538Z","iopub.status.idle":"2021-10-13T13:40:49.361376Z","shell.execute_reply.started":"2021-10-13T13:40:49.354498Z","shell.execute_reply":"2021-10-13T13:40:49.360578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\nfrom typing import TYPE_CHECKING, Any, Callable, Optional\n\nif TYPE_CHECKING:\n    from torch.optim.optimizer import _params_t\nelse:\n    _params_t = Any\n\nclass MADGRAD(Optimizer):\n\n    def __init__(\n        self, params: _params_t, lr: float = 1e-2, momentum: float = 0.9, weight_decay: float = 0, eps: float = 1e-6,\n    ):\n        if momentum < 0 or momentum >= 1:\n            raise ValueError(f\"Momentum {momentum} must be in the range [0,1]\")\n        if lr <= 0:\n            raise ValueError(f\"Learning rate {lr} must be positive\")\n        if weight_decay < 0:\n            raise ValueError(f\"Weight decay {weight_decay} must be non-negative\")\n        if eps < 0:\n            raise ValueError(f\"Eps must be non-negative\")\n\n        defaults = dict(lr=lr, eps=eps, momentum=momentum, weight_decay=weight_decay)\n        super().__init__(params, defaults)\n\n    @property\n    def supports_memory_efficient_fp16(self) -> bool:\n        return False\n\n    @property\n    def supports_flat_params(self) -> bool:\n        return True\n\n    def step(self, closure: Optional[Callable[[], float]] = None) -> Optional[float]:\n\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        if 'k' not in self.state:\n            self.state['k'] = torch.tensor([0], dtype=torch.long)\n        k = self.state['k'].item()\n\n        for group in self.param_groups:\n            eps = group[\"eps\"]\n            lr = group[\"lr\"] + eps\n            decay = group[\"weight_decay\"]\n            momentum = group[\"momentum\"]\n\n            ck = 1 - momentum\n            lamb = lr * math.pow(k + 1, 0.5)\n\n            for p in group[\"params\"]:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                state = self.state[p]\n\n                if \"grad_sum_sq\" not in state:\n                    state[\"grad_sum_sq\"] = torch.zeros_like(p.data).detach()\n                    state[\"s\"] = torch.zeros_like(p.data).detach()\n                    if momentum != 0:\n                        state[\"x0\"] = torch.clone(p.data).detach()\n\n                if momentum != 0.0 and grad.is_sparse:\n                    raise RuntimeError(\"momentum != 0 is not compatible with sparse gradients\")\n\n                grad_sum_sq = state[\"grad_sum_sq\"]\n                s = state[\"s\"]\n\n                # Apply weight decay\n                if decay != 0:\n                    if grad.is_sparse:\n                        raise RuntimeError(\"weight_decay option is not compatible with sparse gradients\")\n\n                    grad.add_(p.data, alpha=decay)\n\n                if grad.is_sparse:\n                    grad = grad.coalesce()\n                    grad_val = grad._values()\n\n                    p_masked = p.sparse_mask(grad)\n                    grad_sum_sq_masked = grad_sum_sq.sparse_mask(grad)\n                    s_masked = s.sparse_mask(grad)\n\n                    # Compute x_0 from other known quantities\n                    rms_masked_vals = grad_sum_sq_masked._values().pow(1 / 3).add_(eps)\n                    x0_masked_vals = p_masked._values().addcdiv(s_masked._values(), rms_masked_vals, value=1)\n\n                    # Dense + sparse op\n                    grad_sq = grad * grad\n                    grad_sum_sq.add_(grad_sq, alpha=lamb)\n                    grad_sum_sq_masked.add_(grad_sq, alpha=lamb)\n\n                    rms_masked_vals = grad_sum_sq_masked._values().pow_(1 / 3).add_(eps)\n\n                    s.add_(grad, alpha=lamb)\n                    s_masked._values().add_(grad_val, alpha=lamb)\n\n                    # update masked copy of p\n                    p_kp1_masked_vals = x0_masked_vals.addcdiv(s_masked._values(), rms_masked_vals, value=-1)\n                    # Copy updated masked p to dense p using an add operation\n                    p_masked._values().add_(p_kp1_masked_vals, alpha=-1)\n                    p.data.add_(p_masked, alpha=-1)\n                else:\n                    if momentum == 0:\n                        # Compute x_0 from other known quantities\n                        rms = grad_sum_sq.pow(1 / 3).add_(eps)\n                        x0 = p.data.addcdiv(s, rms, value=1)\n                    else:\n                        x0 = state[\"x0\"]\n\n                    # Accumulate second moments\n                    grad_sum_sq.addcmul_(grad, grad, value=lamb)\n                    rms = grad_sum_sq.pow(1 / 3).add_(eps)\n\n                    # Update s\n                    s.data.add_(grad, alpha=lamb)\n\n                    # Step\n                    if momentum == 0:\n                        p.data.copy_(x0.addcdiv(s, rms, value=-1))\n                    else:\n                        z = x0.addcdiv(s, rms, value=-1)\n\n                        # p is a moving average of z\n                        p.data.mul_(1 - ck).add_(z, alpha=ck)\n\n\n        self.state['k'] += 1\n        return loss","metadata":{"execution":{"iopub.status.busy":"2021-10-13T13:41:13.687588Z","iopub.execute_input":"2021-10-13T13:41:13.687925Z","iopub.status.idle":"2021-10-13T13:41:13.712289Z","shell.execute_reply.started":"2021-10-13T13:41:13.687894Z","shell.execute_reply":"2021-10-13T13:41:13.711299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion = nn.BCEWithLogitsLoss()","metadata":{"execution":{"iopub.status.busy":"2021-10-13T13:41:27.228365Z","iopub.execute_input":"2021-10-13T13:41:27.228682Z","iopub.status.idle":"2021-10-13T13:41:27.232805Z","shell.execute_reply.started":"2021-10-13T13:41:27.228652Z","shell.execute_reply":"2021-10-13T13:41:27.232027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_activation(activ_name: str=\"relu\"):\n    \"\"\"\"\"\"\n    act_dict = {\n        \"relu\": nn.ReLU(inplace=True),\n        \"tanh\": nn.Tanh(),\n        \"sigmoid\": nn.Sigmoid(),\n        \"identity\": nn.Identity()}\n    if activ_name in act_dict:\n        return act_dict[activ_name]\n    else:\n        raise NotImplementedError\n\nclass Conv2dBNActiv(nn.Module):\n    \"\"\"Conv2d -> (BN ->) -> Activation\"\"\"\n\n    def __init__(\n        self, in_channels: int, out_channels: int,\n        kernel_size: int, stride: int=1, padding: int=0,\n        bias: bool=False, use_bn: bool=True, activ: str=\"relu\"\n    ):\n        \"\"\"\"\"\"\n        super(Conv2dBNActiv, self).__init__()\n        layers = []\n        layers.append(nn.Conv2d(\n            in_channels, out_channels,\n            kernel_size, stride, padding, bias=bias))\n        if use_bn:\n            layers.append(nn.BatchNorm2d(out_channels))\n\n        layers.append(get_activation(activ))\n        self.layers = nn.Sequential(*layers)\n\n    def forward(self, x):\n        \"\"\"Forward\"\"\"\n        return self.layers(x)\n\n\nclass SSEBlock(nn.Module):\n    \"\"\"channel `S`queeze and `s`patial `E`xcitation Block.\"\"\"\n\n    def __init__(self, in_channels: int):\n        \"\"\"Initialize.\"\"\"\n        super(SSEBlock, self).__init__()\n        self.channel_squeeze = nn.Conv2d(\n            in_channels=in_channels, out_channels=1,\n            kernel_size=1, stride=1, padding=0, bias=False)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        \"\"\"Forward.\"\"\"\n        # # x: (bs, ch, h, w) => h: (bs, 1, h, w)\n        h = self.sigmoid(self.channel_squeeze(x))\n        # # x, h => return: (bs, ch, h, w)\n        return x * h\n\n\nclass SpatialAttentionBlock(nn.Module):\n    \"\"\"Spatial Attention for (C, H, W) feature maps\"\"\"\n\n    def __init__(\n        self, in_channels,\n        out_channels_list,\n    ):\n        \"\"\"Initialize\"\"\"\n        super(SpatialAttentionBlock, self).__init__()\n        self.n_layers = len(out_channels_list)\n        channels_list = [in_channels] + out_channels_list\n        assert self.n_layers > 0\n        assert channels_list[-1] == 1\n\n        for i in range(self.n_layers - 1):\n            in_chs, out_chs = channels_list[i: i + 2]\n            layer = Conv2dBNActiv(in_chs, out_chs, 3, 1, 1, activ=\"relu\")\n            setattr(self, f\"conv{i + 1}\", layer)\n\n        in_chs, out_chs = channels_list[-2:]\n        layer = Conv2dBNActiv(in_chs, out_chs, 3, 1, 1, activ=\"sigmoid\")\n        setattr(self, f\"conv{self.n_layers}\", layer)\n\n    def forward(self, x):\n        \"\"\"Forward\"\"\"\n        h = x\n        for i in range(self.n_layers):\n            h = getattr(self, f\"conv{i + 1}\")(h)\n\n        h = h * x\n        return h","metadata":{"execution":{"iopub.status.busy":"2021-10-13T13:41:39.706686Z","iopub.execute_input":"2021-10-13T13:41:39.707128Z","iopub.status.idle":"2021-10-13T13:41:39.72408Z","shell.execute_reply.started":"2021-10-13T13:41:39.707092Z","shell.execute_reply":"2021-10-13T13:41:39.723245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Backbone(nn.Module):\n\n    def __init__(self, name='resnet18', pretrained=True):\n        super(Backbone, self).__init__()\n        self.net = timm.create_model(name, pretrained=pretrained)\n\n        if 'regnet' in name:\n            self.out_features = self.net.head.fc.in_features\n        elif 'vit' in name:\n            self.out_features = self.net.head.in_features\n        elif name == 'deit_base_distilled_patch16_384':\n            self.out_features = 768\n        elif 'csp' in name:\n            self.out_features = self.net.head.fc.in_features\n        elif 'res' in name:  # works also for resnest\n            self.out_features = self.net.fc.in_features\n        elif 'efficientnet' in name:\n            self.out_features = self.net.classifier.in_features\n        elif 'densenet' in name:\n            self.out_features = self.net.classifier.in_features\n        elif 'senet' in name:\n            self.out_features = self.net.fc.in_features\n        elif 'inception' in name:\n            self.out_features = self.net.last_linear.in_features\n\n        else:\n            self.out_features = self.net.classifier.in_features\n\n    def forward(self, x):\n        x = self.net.forward_features(x)\n\n        return x","metadata":{"execution":{"iopub.status.busy":"2021-10-13T13:41:53.118832Z","iopub.execute_input":"2021-10-13T13:41:53.119611Z","iopub.status.idle":"2021-10-13T13:41:53.130166Z","shell.execute_reply.started":"2021-10-13T13:41:53.119565Z","shell.execute_reply":"2021-10-13T13:41:53.129044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SETINet(nn.Module):\n    def __init__(self, backbone, out_dim, embedding_size=512,\n                 loss=False, pretrained=True, use_mixup=True):\n        super(SETINet, self).__init__()\n        self.backbone_name = backbone\n        self.loss = loss\n        self.out_dim = out_dim\n        self.use_mixup = use_mixup\n\n        self.mixup = Mixup()\n        self.backbone = Backbone(backbone, pretrained=pretrained)\n        if int(embedding_size) != int(self.backbone.out_features):\n            self.embedding_size = self.backbone.out_features // 2\n        else:\n            self.embedding_size = embedding_size\n\n        self.neck = nn.Sequential(\n                SpatialAttentionBlock(self.backbone.out_features, [64, 32, 16, 1]),\n                nn.AdaptiveAvgPool2d(output_size=1),\n                nn.Flatten(start_dim=1),\n                nn.Linear(self.backbone.out_features, self.embedding_size),\n                nn.ReLU(inplace=True),\n                nn.Dropout(0.5),\n        )\n\n        self.head = nn.Linear(self.embedding_size, out_dim)\n\n    def forward(self, input_dict, training=True, get_embeddings=False, get_attentions=False):\n\n        x = input_dict['spect']\n        if self.use_mixup and training==True:\n            x, y_a, y_b, lam = self.mixup(x, input_dict['target'])\n            \n            \n        if 'deit_base_distilled_patch16_384' == self.backbone_name:\n            x = x.unsqueeze(1)\n        x = x.expand(-1, 3, -1, -1)\n\n        x = self.backbone(x)\n        x = self.neck(x)\n\n        logits = self.head(x)\n\n        output_dict = {'logits': logits}\n        \n        if self.loss and self.use_mixup and training==True:\n            target = input_dict['target']\n            loss = mixup_criterion(criterion, logits, y_a, y_b, lam)\n            output_dict['loss'] = loss\n        elif self.loss:\n            target = input_dict['target']\n            loss = criterion(logits, target)\n            output_dict['loss'] = loss\n\n        return output_dict","metadata":{"execution":{"iopub.status.busy":"2021-10-13T13:42:08.110465Z","iopub.execute_input":"2021-10-13T13:42:08.111264Z","iopub.status.idle":"2021-10-13T13:42:08.126376Z","shell.execute_reply.started":"2021-10-13T13:42:08.111223Z","shell.execute_reply":"2021-10-13T13:42:08.124672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_epoch(loader, model, optimizer, scheduler, scaler, device):\n\n    model.train()\n    model.zero_grad()\n    train_loss = []\n    bar = tqdm(range(len(loader)))\n    load_iter = iter(loader)\n    batch = load_iter.next()\n    batch = {k: batch[k].to(device, non_blocking=True) for k in batch.keys()}\n\n    for i in bar:\n        input_dict = batch.copy()\n        if i + 1 < len(loader):\n            batch = load_iter.next()\n            batch = {k: batch[k].to(device, non_blocking=True)\n                     for k in batch.keys()}\n\n        with autocast():\n            out_dict = model(input_dict)\n        loss = out_dict['loss']\n        loss_np = loss.detach().cpu().numpy()\n        # loss.backward()\n        scaler.scale(loss).backward()\n\n        if (i+1) % GRADIENT_ACCUMULATION == 0 or i == len(loader) - 1:\n            # optimizer.step()\n            scaler.step(optimizer)\n            scaler.update()\n            model.zero_grad()\n            scheduler.step()\n\n        train_loss.append(loss_np)\n        smooth_loss = sum(train_loss[-100:]) / min(len(train_loss), 100)\n        bar.set_description('loss: %.4f, smth: %.4f' % (loss_np, smooth_loss))\n    return train_loss","metadata":{"execution":{"iopub.status.busy":"2021-10-13T13:42:23.830059Z","iopub.execute_input":"2021-10-13T13:42:23.830377Z","iopub.status.idle":"2021-10-13T13:42:23.840424Z","shell.execute_reply.started":"2021-10-13T13:42:23.830346Z","shell.execute_reply":"2021-10-13T13:42:23.839562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def val_epoch(loader, model, device):\n\n    model.eval()\n    val_loss = []\n    LOGITS = []\n    TARGETS = []\n\n    with torch.no_grad():\n        if 1:\n            bar = tqdm(range(len(loader)))\n            load_iter = iter(loader)\n            batch = load_iter.next()\n            batch = {k: batch[k].to(device, non_blocking=True)\n                     for k in batch.keys()}\n\n            for i in bar:\n                input_dict = batch.copy()\n                if i + 1 < len(loader):\n                    batch = load_iter.next()\n                    batch = {k: batch[k].to(device, non_blocking=True)\n                             for k in batch.keys()}\n\n                out_dict = model(input_dict,training=False)\n                logits = out_dict['logits'].sigmoid()\n                loss = out_dict['loss']\n                target = input_dict['target']\n                loss_np = loss.detach().cpu().numpy()\n                LOGITS.append(logits.detach())\n                TARGETS.append(target.detach())\n                val_loss.append(loss_np)\n\n                smooth_loss = sum(val_loss[-100:]) / min(len(val_loss), 100)\n                bar.set_description('loss: %.4f, smth: %.4f' %\n                                    (loss_np, smooth_loss))\n\n            val_loss = np.mean(val_loss)\n\n    LOGITS = torch.cat(LOGITS)\n    TARGETS = torch.cat(TARGETS)\n    # auc_score = fast_auc_torch(TARGETS, LOGITS).detach().cpu().numpy()\n    sklearn_auc = roc_auc_score(TARGETS.detach().cpu(), LOGITS.detach().cpu()) \n    return val_loss, LOGITS, 0, sklearn_auc","metadata":{"execution":{"iopub.status.busy":"2021-10-13T13:42:36.392809Z","iopub.execute_input":"2021-10-13T13:42:36.393505Z","iopub.status.idle":"2021-10-13T13:42:36.405924Z","shell.execute_reply.started":"2021-10-13T13:42:36.39347Z","shell.execute_reply":"2021-10-13T13:42:36.403677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def save_checkpoint(model, optimizer, scheduler, scaler, epoch, fold, seed, fname=\"stft_tranformer\"):\n    checkpoint = {\n        'model': model.state_dict(),\n        'optimizer': optimizer.state_dict(),\n        'scheduler': scheduler.state_dict(),\n        'scaler': scaler.state_dict(),\n        'epoch': epoch,\n    }\n    torch.save(checkpoint, '%s_%d_%d_%d.pt' %\n               (fname, fold, seed, epoch))","metadata":{"execution":{"iopub.status.busy":"2021-10-13T13:42:44.732947Z","iopub.execute_input":"2021-10-13T13:42:44.733563Z","iopub.status.idle":"2021-10-13T13:42:44.740977Z","shell.execute_reply.started":"2021-10-13T13:42:44.73351Z","shell.execute_reply":"2021-10-13T13:42:44.738241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_checkpoint(backbone, epoch, fold, seed, fname):\n    model = SETINet(backbone,\n                      out_dim=len(BIRD_CODE),\n                      loss=True,\n                      pretrained=False,\n                      ).to(device)\n    optimizer = MADGRAD(model.parameters(), lr=LR)\n    scheduler = OneCycleLR(\n          optimizer,\n          epochs = N_EPOCHS,\n          max_lr = MAX_LR,\n          total_steps = n_training_steps,\n          steps_per_epoch = steps_per_epoch\n        )\n    \n    scaler = GradScaler()\n    checkpoint = torch.load('%s_%d_%d_%d.pt' %\n                            (fname, fold, seed, epoch))\n    model.load_state_dict(checkpoint['model'])\n    optimizer.load_state_dict(checkpoint['optimizer'])\n    scheduler.load_state_dict(checkpoint['scheduler'])\n    scaler.load_state_dict(checkpoint['scaler'])\n    return model, optimizer, scheduler, scaler","metadata":{"execution":{"iopub.status.busy":"2021-10-13T13:42:54.068993Z","iopub.execute_input":"2021-10-13T13:42:54.06993Z","iopub.status.idle":"2021-10-13T13:42:54.078766Z","shell.execute_reply.started":"2021-10-13T13:42:54.069884Z","shell.execute_reply":"2021-10-13T13:42:54.077988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"steps_per_epoch=(len(train)// N_EPOCHS) // BATCH_SIZE\nn_training_steps = steps_per_epoch * N_EPOCHS\n\nskf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\ntrain[\"fold\"] = -1\nfor fold_id, (_, val_idx) in enumerate(skf.split(train[\"id\"], train[\"target\"])):\n    train.loc[val_idx, \"fold\"] = fold_id","metadata":{"execution":{"iopub.status.busy":"2021-10-13T13:43:03.169087Z","iopub.execute_input":"2021-10-13T13:43:03.169357Z","iopub.status.idle":"2021-10-13T13:43:03.196767Z","shell.execute_reply.started":"2021-10-13T13:43:03.169328Z","shell.execute_reply":"2021-10-13T13:43:03.195984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for seed in [0]:\n    for fold in range(N_FOLDS):\n        if fold != TRAIN_FOLD:\n            continue\n            \n        train_fold = train[train['fold']!=TRAIN_FOLD]\n        train_dataset = TrainDataset(\n            train_fold,\n            transform=get_transforms(data='train'),\n            use_vit=False\n        )\n        train_data_loader = DataLoader(\n            train_dataset,\n            batch_size=BATCH_SIZE,\n            num_workers=4,\n            shuffle=True,\n            pin_memory=True,\n        )\n        \n        val_df = train[train['fold']==TRAIN_FOLD]\n        val_dataset = TrainDataset(          \n            val_df,\n            transform=get_transforms(data='valid'),\n            use_vit=False\n        )\n\n        valid_data_loader_orig = DataLoader(\n            val_dataset,\n            batch_size=BATCH_SIZE*2,\n            num_workers=4,\n            shuffle=False,\n            pin_memory=True,\n        )\n        \n        model = SETINet(backbone=MODEL,\n                      out_dim=1,\n                      loss=True,\n                      pretrained=True,\n                      ).to(device)\n        optimizer = MADGRAD(model.parameters(), lr=LR)\n        scheduler = OneCycleLR(\n              optimizer,\n              epochs = N_EPOCHS,\n              max_lr = MAX_LR,\n              steps_per_epoch=int(np.ceil(len(train_data_loader)/GRADIENT_ACCUMULATION)))\n        scaler = GradScaler()\n        \n        roc_auc_max = 0.\n        loss_min = 99999\n        not_improving = 0\n        \n        for epoch in range(N_EPOCHS):\n            print(time.ctime(), 'Epoch:', epoch, flush=True)\n            # train_loss = 0.0\n            train_loss = train_epoch(train_data_loader, model, optimizer, scheduler, scaler, device)\n\n            (val_loss, _ , auc_score, sklearn_auc) = val_epoch(valid_data_loader_orig, model, device)\n            content = 'Orig %d Ep %d, lr: %.7f, train loss: %.5f, val loss: %.5f, sklearnAUC: %.4f'\n            values = (fold,\n                      epoch,\n                      optimizer.param_groups[0][\"lr\"],\n                      np.mean(train_loss),\n                      np.mean(val_loss),\n                      \n                      sklearn_auc\n                      )\n            print(content % values, flush=True)\n            \n            not_improving += 1\n            if sklearn_auc > roc_auc_max:\n                save_checkpoint(model, optimizer, scheduler,\n                                scaler, epoch, fold, seed)\n                roc_auc_max = sklearn_auc\n                not_improving = 0\n                \n            if not_improving == EARLY_STOP:\n                print('Early Stopping...')\n                break","metadata":{"execution":{"iopub.status.busy":"2021-10-13T13:43:16.833711Z","iopub.execute_input":"2021-10-13T13:43:16.834215Z","iopub.status.idle":"2021-10-13T16:03:19.858392Z","shell.execute_reply.started":"2021-10-13T13:43:16.834167Z","shell.execute_reply":"2021-10-13T16:03:19.85638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}