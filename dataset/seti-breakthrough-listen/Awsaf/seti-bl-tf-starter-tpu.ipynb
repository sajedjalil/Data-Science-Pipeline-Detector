{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# [SETI Breakthrough Listen - E.T. Signal Search](https://www.kaggle.com/c/seti-breakthrough-listen)\n>Find extraterrestrial signals in data from deep space \n\n![](https://storage.googleapis.com/kaggle-competitions/kaggle/23652/logos/header.png?t=2021-02-24-19-15-30)","metadata":{}},{"cell_type":"markdown","source":"# A. Version Info üö©\n* `v9`  : moved from **StratifiedKFold** by group to **KFold** as those folder(groups) don't have any signifance.\n* `v10` : **KFold** to **StratifiedKFold**\n* `v11` : no resize, augmentation updated for non-square image","metadata":{}},{"cell_type":"markdown","source":"# B. Resource üç∞\n\n* Notebook for Dataset\n    * [SETI-BL: 256x256 tfrec Data](https://www.kaggle.com/awsaf49/seti-bl-256x256-tfrec-data)\n\n\n* Datasets\n    * [128x128](https://www.kaggle.com/awsaf49/setibl-128x128-tfrec-dataset)\n    * [256x256](https://www.kaggle.com/awsaf49/setibl-256x256-tfrec-dataset)\n    * [384x384](https://www.kaggle.com/awsaf49/setibl-384x384-tfrec-dataset)","metadata":{}},{"cell_type":"markdown","source":"# 0. Methodology üìå\n* This notebook demonstrates how can we use **CNN** to classify the signals.\n* Even though at first glance this problems seems like **signal** detection problem, but we can convert this to be a image classification problem. We can convert 1D signal to 2D signal(image) using different methods such as **Fourier Transform** which basically converts data from **time** domain to **frequency** domain. So, from `[time, intensity]` we get `[time, frequency, intensity]`. Luckily, we are given such 2D info; so we don't have to convert anything.\n\n* This notebook uses pre-computed **tfrecord** for training. **TFRecord** can scale up training upto 2x. With **TPU** as our device, this can save our good amount of time.\n* For augmentation, **Rotation, Flip, Shear, Zoom, Shift, Dropout, RandomJitter** is used.\n* This notebook uses **EfficientNet** as backbone model (classifier).\n* Then, we calculate **Out of Fold (OOF)** score of this notebook.\n* Finally, we make our **Submission**.","metadata":{}},{"cell_type":"markdown","source":"# 1. Install Libraries üõ†","metadata":{}},{"cell_type":"code","source":"!pip install -q efficientnet >> /dev/null","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-07-16T11:45:21.209207Z","iopub.execute_input":"2021-07-16T11:45:21.209896Z","iopub.status.idle":"2021-07-16T11:45:30.894791Z","shell.execute_reply.started":"2021-07-16T11:45:21.209804Z","shell.execute_reply":"2021-07-16T11:45:30.89369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Import Libraries üìö","metadata":{}},{"cell_type":"code","source":"import os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # to avoid too many logging messages\nimport shutil\nfrom glob import glob\nfrom kaggle_datasets import KaggleDatasets\n\nimport tensorflow as tf, re, math\nimport tensorflow.keras.backend as K\nimport efficientnet.tfkeras as efn\nimport tensorflow_addons as tfa\n\nimport pandas as pd\nimport numpy as np\nimport random\nimport sklearn\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2021-07-16T11:45:30.896458Z","iopub.execute_input":"2021-07-16T11:45:30.896793Z","iopub.status.idle":"2021-07-16T11:45:38.122941Z","shell.execute_reply.started":"2021-07-16T11:45:30.896759Z","shell.execute_reply":"2021-07-16T11:45:38.121704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Libraries' Version","metadata":{}},{"cell_type":"code","source":"print('np:',np.__version__)\nprint('pd:',pd.__version__)\nprint('sklearn:',sklearn.__version__)\nprint('tf:',tf.__version__)\nprint('tfa:', tfa.__version__)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Notebook Config ‚öôÔ∏è","metadata":{}},{"cell_type":"code","source":"# USE VERBOSE=0 for silent, VERBOSE=1 for interactive, VERBOSE=2 for commit\nVERBOSE      = 0\nDISPLAY_PLOT = True\n\nDEVICE = \"TPU\" #or \"GPU\"\n\n# USE DIFFERENT SEED FOR DIFFERENT STRATIFIED KFOLD\nSEED = 42\n\n# NUMBER OF FOLDS. USE 2, 5, 10\nFOLDS = 5\n\n# WHICH IMAGE SIZES TO LOAD EACH FOLD\nIMG_SIZES = [[273, 256]]*FOLDS\n\n# BATCH SIZE AND EPOCHS\nBATCH_SIZES = [32]*FOLDS\nEPOCHS      = [12]*FOLDS\n\n# WHICH EFFICIENTNET B? TO USE\nEFF_NETS = [6]*FOLDS\n\n# Augmentations\nAUGMENT   = True\nTRANSFORM = True\n\n# AFFINE TRANSFORMATION\nROT_    = 0.0 # ROTATION\nSHR_    = 2.0 # SHEAR\nHZOOM_  = 8.0 # HORIZONTAL ZOOM\nWZOOM_  = 8.0 # VERTICAL ZOOM\nHSHIFT_ = 8.0 # HORIZONTAL SHIFT\nWSHIFT_ = 8.0 # VERTICAL SHIFT\n\n# COARSE DROPOUT - MORE ABOUT THIS IS LATER\nPROBABILITY = 0.50 # PROBABILITY OF DROPOUT\nCT          = 8 # NUMBER OF DROPOUTS\nSZ          = 0.05 # SIZE OF THE DROPOUT - CALCULATED AS PERCENT OF IMAGE-DIMENSION\n\n#bri, contrast\nsat  = (0.7, 1.3) # SATURATION\ncont = (0.8, 1.2) # CONTRAST\nbri  =  0.1 # BRIGHTNESS\n\n# WEIGHTS FOR FOLD MODELS WHEN PREDICTING TEST\nWGTS = [1/FOLDS]*FOLDS\n\n# TEST TIME AUGMENTATION STEPS\nTTA = 11","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2021-07-16T11:45:38.124709Z","iopub.execute_input":"2021-07-16T11:45:38.125009Z","iopub.status.idle":"2021-07-16T11:45:38.133573Z","shell.execute_reply.started":"2021-07-16T11:45:38.124982Z","shell.execute_reply":"2021-07-16T11:45:38.132293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Reproducibility ","metadata":{}},{"cell_type":"code","source":"def seeding(SEED):\n    np.random.seed(SEED)\n    random.seed(SEED)\n    os.environ['PYTHONHASHSEED'] = str(SEED)\n    os.environ['TF_CUDNN_DETERMINISTIC'] = str(SEED)\n    tf.random.set_seed(SEED)\n    print('seeding done!!!')\nseeding(SEED)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T11:45:38.134986Z","iopub.execute_input":"2021-07-16T11:45:38.135265Z","iopub.status.idle":"2021-07-16T11:45:38.149087Z","shell.execute_reply.started":"2021-07-16T11:45:38.135238Z","shell.execute_reply":"2021-07-16T11:45:38.148336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Set Up Device üì±\nFollowing codes automatically detects hardware(tpu or gpu or cpu). \n* **What is TPU ?**: They are hardware accelerators specialized in deep learning tasks. They are equipped with 128GB of high-speed memory allowing larger batches, larger models and also larger training inputs. To know more about tpu, checkout this [doc](https://www.kaggle.com/docs/tpu).\n\n<img src=\"https://storage.googleapis.com/kaggle-media/tpu/tpu_cores_and_chips.png\" width=600>","metadata":{}},{"cell_type":"code","source":"if DEVICE == \"TPU\":\n    print(\"connecting to TPU...\")\n    try:\n        # detect and init the TPU\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        print('Running on TPU ', tpu.master())\n    except ValueError:\n        print(\"Could not connect to TPU\")\n        tpu = None\n\n    if tpu:\n        try:\n            print(\"initializing  TPU ...\")\n            tf.config.experimental_connect_to_cluster(tpu)\n            tf.tpu.experimental.initialize_tpu_system(tpu)\n            # instantiate a distribution strategy\n            strategy = tf.distribute.experimental.TPUStrategy(tpu)\n            print(\"TPU initialized\")\n        except _:\n            print(\"failed to initialize TPU\")\n    else:\n        DEVICE = \"GPU\"\n\nif DEVICE != \"TPU\":\n    print(\"Using default strategy for CPU and single GPU\")\n    # instantiate a distribution strategy\n    strategy = tf.distribute.get_strategy()\n\nif DEVICE == \"GPU\":\n    print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n    \n\nAUTO     = tf.data.experimental.AUTOTUNE\nREPLICAS = strategy.num_replicas_in_sync\nprint(f'REPLICAS: {REPLICAS}')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-16T11:45:38.150442Z","iopub.execute_input":"2021-07-16T11:45:38.150759Z","iopub.status.idle":"2021-07-16T11:45:43.937622Z","shell.execute_reply.started":"2021-07-16T11:45:38.150708Z","shell.execute_reply":"2021-07-16T11:45:43.936508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Preprocess üßÇ\n**NOTE:**\n* Original Signal had **6** channels. I only took **3** as it was mention here,\n```\nNot all of the ‚Äúneedle‚Äù signals look like diagonal lines, and they may not be present for the entirety of all three ‚ÄúA‚Äù observations, but what they do have in common is that they are only present in some or all of the ‚ÄúA‚Äù observations (panels 1, 3, and 5 in the cadence snippets).\n```\n*  Didn't do any **Signal Processing** so far. Left it for the model.\n*  Used simple augmentations, some of them may hurt the model.","metadata":{}},{"cell_type":"code","source":"GCS_PATH = [None]*FOLDS\nfor i,k in enumerate(IMG_SIZES):\n    GCS_PATH[i] = KaggleDatasets().get_gcs_path('setibl-%ix%i-tfrec-dataset'%(k[0],k[1]))\nfiles_train = np.sort(np.array(tf.io.gfile.glob(GCS_PATH[0] + '/train*.tfrec')))\nfiles_test  = np.sort(np.array(tf.io.gfile.glob(GCS_PATH[0] + '/test*.tfrec')))\nnum_train_files = len(files_train)\nnum_test_files  = len(files_test)\nprint('train_files:',num_train_files)\nprint('test_files:',num_test_files)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-16T11:45:43.939298Z","iopub.execute_input":"2021-07-16T11:45:43.939931Z","iopub.status.idle":"2021-07-16T11:45:56.826158Z","shell.execute_reply.started":"2021-07-16T11:45:43.939885Z","shell.execute_reply":"2021-07-16T11:45:56.825045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Meta Data\n* `train/` - a training set of cadence snippet files stored in **numpy float16** format (v1.20.1), one file per cadence snippet id, with corresponding labels found in the train_labels.csv file. Each file has dimension `(6, 273, 256)`, with the 1st dimension representing the 6 positions of the cadence, and the 2nd and 3rd dimensions representing the 2D spectrogram.\n* `test/` - the test set cadence snippet files; you must predict whether or not the cadence contains a \"needle\", which is the target for this competition\n* `sample_submission.csv` - a sample submission file in the correct format\n* `train_labels` - targets corresponding (by id) to the cadence snippet files found in the train/ folder\n* `old_leaky_data` - full pre-relaunch data, including test labels; you should not assume this data is helpful (it may or may not be).\n","metadata":{}},{"cell_type":"code","source":"train_label_df = pd.read_csv('../input/c/seti-breakthrough-listen/train_labels.csv')\ntest_label_df  = pd.read_csv('../input/c/seti-breakthrough-listen/sample_submission.csv')\n\ntrain_paths = glob('../input/c/seti-breakthrough-listen/train/**/*.npy')\ntest_paths = glob('../input/c/seti-breakthrough-listen/test/**/*.npy')\n\ntrain_df = pd.DataFrame({'filepath':train_paths})\ntrain_df['id'] = train_df.filepath.map(lambda x: x.split('/')[-1].split('.')[0])\ntrain_df['group'] = train_df.filepath.map(lambda x: x.split('/')[-2])\ntrain_df = pd.merge(train_df, train_label_df, on='id', how='left')\n\nprint(f'num_train: {len(train_paths)}\\nnum_test : {len(test_paths)}')","metadata":{"execution":{"iopub.status.busy":"2021-07-16T11:45:56.827461Z","iopub.execute_input":"2021-07-16T11:45:56.827772Z","iopub.status.idle":"2021-07-16T11:46:00.902266Z","shell.execute_reply.started":"2021-07-16T11:45:56.827741Z","shell.execute_reply":"2021-07-16T11:46:00.901332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Class Distribution\n* **Tfrecord** dataset has total **20** files and for proper validatoin scheme, each has been made stratifing `target` & `group` feature. ","metadata":{}},{"cell_type":"code","source":"from plotly.offline import init_notebook_mode, iplot, plot\nimport plotly as py\nimport plotly.express as px\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\nfig = go.Figure(data=[\n    go.Bar(name='haystack', \n           y=train_df.target.value_counts().values[0:1],\n           x=['haystack'],\n           text = train_df.target.value_counts()[0:1],\n           orientation='v',\n           textposition='outside',),\n    go.Bar(name='needle', \n           y=train_df.target.value_counts().values[1:],\n           x=['needle'],\n           text = train_df.target.value_counts()[1:],\n           orientation='v',\n           textposition='outside',)\n])\n# Change the bar mode\nfig.update_layout(\n                  width=800,\n                  height=600,\n                  title=f'Class Distribution',\n                  yaxis_title='Number of Images',\n                  xaxis_title='Class Name',)\niplot(fig)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-16T11:46:00.904687Z","iopub.execute_input":"2021-07-16T11:46:00.904972Z","iopub.status.idle":"2021-07-16T11:46:03.423391Z","shell.execute_reply.started":"2021-07-16T11:46:00.904945Z","shell.execute_reply":"2021-07-16T11:46:03.422301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Data Augmentation üåà\nUsed simple augmentations, some of them may hurt the model.\n* RandomFlip (Left-Right)\n* No Rotation\n* RandomBrightness\n* RndomContrast\n* Shear\n* Zoom\n* Coarsee Dropout/Cutout\n\nAs this is not typical **image** data rather **signal** typical augmantation for **image** may do some damage","metadata":{}},{"cell_type":"code","source":"# Augmentations by @cdeotte\ndef get_mat(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n    # returns 3x3 transformmatrix which transforms indicies\n        \n    # CONVERT DEGREES TO RADIANS\n    rotation = math.pi * rotation / 180.\n    shear    = math.pi * shear    / 180.\n\n    def get_3x3_mat(lst):\n        return tf.reshape(tf.concat([lst],axis=0), [3,3])\n    \n    # ROTATION MATRIX\n    c1   = tf.math.cos(rotation)\n    s1   = tf.math.sin(rotation)\n    one  = tf.constant([1],dtype='float32')\n    zero = tf.constant([0],dtype='float32')\n    \n    rotation_matrix = get_3x3_mat([c1,   s1,   zero, \n                                   -s1,  c1,   zero, \n                                   zero, zero, one])    \n    # SHEAR MATRIX\n    c2 = tf.math.cos(shear)\n    s2 = tf.math.sin(shear)    \n    \n    shear_matrix = get_3x3_mat([one,  s2,   zero, \n                                zero, c2,   zero, \n                                zero, zero, one])        \n    # ZOOM MATRIX\n    zoom_matrix = get_3x3_mat([one/height_zoom, zero,           zero, \n                               zero,            one/width_zoom, zero, \n                               zero,            zero,           one])    \n    # SHIFT MATRIX\n    shift_matrix = get_3x3_mat([one,  zero, height_shift, \n                                zero, one,  width_shift, \n                                zero, zero, one])\n    \n    return K.dot(K.dot(rotation_matrix, shear_matrix), \n                 K.dot(zoom_matrix,     shift_matrix))\n\n\ndef transform(image, DIM=IMG_SIZES[0]):    \n    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n    # output - image randomly rotated, sheared, zoomed, and shifted\n    \n    # fixed for non-square image thanks to Chris Deotte\n    \n    if DIM[0]!=DIM[1]:\n        pad = (DIM[0]-DIM[1])//2\n        image = tf.pad(image, [[0, 0], [pad, pad+1],[0, 0]])\n        \n    NEW_DIM = DIM[0]\n    \n    XDIM = NEW_DIM%2 #fix for size 331\n    \n    rot = ROT_ * tf.random.normal([1], dtype='float32')\n    shr = SHR_ * tf.random.normal([1], dtype='float32') \n    h_zoom = 1.0 + tf.random.normal([1], dtype='float32') / HZOOM_\n    w_zoom = 1.0 + tf.random.normal([1], dtype='float32') / WZOOM_\n    h_shift = HSHIFT_ * tf.random.normal([1], dtype='float32') \n    w_shift = WSHIFT_ * tf.random.normal([1], dtype='float32') \n\n    # GET TRANSFORMATION MATRIX\n    m = get_mat(rot,shr,h_zoom,w_zoom,h_shift,w_shift) \n\n    # LIST DESTINATION PIXEL INDICES\n    x   = tf.repeat(tf.range(NEW_DIM//2, -NEW_DIM//2,-1), NEW_DIM)\n    y   = tf.tile(tf.range(-NEW_DIM//2, NEW_DIM//2), [NEW_DIM])\n    z   = tf.ones([NEW_DIM*NEW_DIM], dtype='int32')\n    idx = tf.stack( [x,y,z] )\n    \n    # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n    idx2 = K.dot(m, tf.cast(idx, dtype='float32'))\n    idx2 = K.cast(idx2, dtype='int32')\n    idx2 = K.clip(idx2, -NEW_DIM//2+XDIM+1, NEW_DIM//2)\n    \n    # FIND ORIGIN PIXEL VALUES           \n    idx3 = tf.stack([NEW_DIM//2-idx2[0,], NEW_DIM//2-1+idx2[1,]])\n    d    = tf.gather_nd(image, tf.transpose(idx3))\n    \n    if DIM[0]!=DIM[1]:\n        image = tf.reshape(d,[NEW_DIM, NEW_DIM,3])\n        image = image[:, pad:DIM[1]+pad,:]\n    image = tf.reshape(image, [*DIM, 3])\n        \n    return image","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-16T11:46:03.425632Z","iopub.execute_input":"2021-07-16T11:46:03.426071Z","iopub.status.idle":"2021-07-16T11:46:03.449272Z","shell.execute_reply.started":"2021-07-16T11:46:03.426028Z","shell.execute_reply":"2021-07-16T11:46:03.448081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dropout\ncheck this [notebook](https://www.kaggle.com/c/siim-isic-melanoma-classification/discussion/169721) by chris for more explanation on **Coarse Dropout and Cutout**\n\n![](http://playagricola.com/Kaggle/drop-7-24.jpg)\n\nCoarse Dropout and Cutout augmentation are techniques to prevent **overfitting** and encourage generalization. They randomly remove rectangles from training images. By removing portions of the images, we challenge our models to pay attention to the entire image because it never knows what part of the image will be present. (This is similar and different to dropout layer within a **CNN**).\n\n* Cutout is the technique of removing 1 large rectangle of random size\n* Coarse dropout is the technique of removing many small rectanges of similar size.\n\nBy changing the parameters below, we can have either coarse dropout or cutout. (For cutout, you'll need to add tf.random.uniform for random size. I leave this as an exercise for the reader.","metadata":{}},{"cell_type":"code","source":"# by cdotte\ndef dropout(image,DIM=IMG_SIZES[0], PROBABILITY = 0.6, CT = 5, SZ = 0.1):\n    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n    # output - image with CT squares of side size SZ*DIM removed\n    \n    # DO DROPOUT WITH PROBABILITY DEFINED ABOVE\n    P = tf.cast( tf.random.uniform([],0,1)<PROBABILITY, tf.int32)\n    if (P==0)|(CT==0)|(SZ==0): \n        return image\n    \n    for k in range(CT):\n        # CHOOSE RANDOM LOCATION\n        x = tf.cast( tf.random.uniform([],0,DIM[1]),tf.int32)\n        y = tf.cast( tf.random.uniform([],0,DIM[0]),tf.int32)\n        # COMPUTE SQUARE \n        WIDTH = tf.cast( SZ*min(DIM),tf.int32) * P\n        ya = tf.math.maximum(0,y-WIDTH//2)\n        yb = tf.math.minimum(DIM[0],y+WIDTH//2)\n        xa = tf.math.maximum(0,x-WIDTH//2)\n        xb = tf.math.minimum(DIM[1],x+WIDTH//2)\n        # DROPOUT IMAGE\n        one = image[ya:yb,0:xa,:]\n        two = tf.zeros([yb-ya,xb-xa,3], dtype = image.dtype) \n        three = image[ya:yb,xb:DIM[1],:]\n        middle = tf.concat([one,two,three],axis=1)\n        image = tf.concat([image[0:ya,:,:],middle,image[yb:DIM[0],:,:]],axis=0) \n\n    image = tf.reshape(image,[*DIM,3])\n    return image","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-16T11:46:03.450853Z","iopub.execute_input":"2021-07-16T11:46:03.451157Z","iopub.status.idle":"2021-07-16T11:46:03.466815Z","shell.execute_reply.started":"2021-07-16T11:46:03.451125Z","shell.execute_reply":"2021-07-16T11:46:03.465631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Reading TFRecord Data\n**So, ever wondered why people use TFRecord?**\n\n* The **TFRecord** (`.tfrecord`/`.tfrec`) format is TensorFlow's custom data format which is used for storing a sequence of binary records. \n* TFRecord data takes up **less space on the storage disk**, and **takes less time to read and write from the disk**.\n* There are a number of advantages to using TFRecords: \n    * More efficient storage\n    * Fast I/O\n    * Self-contained files\n    * TPUs require that you pass data to them in TFRecord format\n    \n## tf.data\nTo build data pipeline using `tfrecrod` we need to use `tf.data` API. **So, what is `tf.data` ?**\n\n* The `tf.data` API enables us to build complex input pipelines from simple, reusable pieces. For example, the pipeline for an image model might aggregate data from files in a distributed file system, apply random transformation to each image, and merge randomly selected images into a batch for training.\n\n* The `tf.data` API provides a `tf.data.Dataset` abstraction that represents a sequence of elements where each element comprises one or more components. For instances, in an image pipeline, an element might be a single training example, with a pair of tensor components representing the image and its label.\n\nCheckout this [doc](https://www.tensorflow.org/guide/data) if you want to learn more about `tf.data`.\n","metadata":{}},{"cell_type":"code","source":"def read_labeled_tfrecord(example):\n    tfrec_format = {\n        'image'                        : tf.io.FixedLenFeature([], tf.string),\n        'image_id'                     : tf.io.FixedLenFeature([], tf.string),\n        'target'                       : tf.io.FixedLenFeature([], tf.int64)\n    }           \n    example = tf.io.parse_single_example(example, tfrec_format)\n    return example['image'], example['target']\n\n\ndef read_unlabeled_tfrecord(example, return_image_id):\n    tfrec_format = {\n        'image'                        : tf.io.FixedLenFeature([], tf.string),\n        'image_id'                     : tf.io.FixedLenFeature([], tf.string),\n    }\n    example = tf.io.parse_single_example(example, tfrec_format)\n    return example['image'], example['image_id'] if return_image_id else 0\n\n \ndef prepare_image(img, augment=True, dim=IMG_SIZES[0]):    \n    img = tf.image.decode_png(img, channels=3)\n    img = tf.cast(img, tf.float32)\n    img = img/255.0 # rescale image\n    \n    if augment:\n        img = transform(img,DIM=dim) if TRANSFORM else img\n        img = tf.image.random_flip_left_right(img)\n        #img = tf.image.random_hue(img, 0.01)\n        img = tf.image.random_saturation(img, sat[0], sat[1])\n        img = tf.image.random_contrast(img, cont[0], cont[1])\n        img = tf.image.random_brightness(img, bri)     \n                      \n    img = tf.reshape(img, [*dim, 3])\n            \n    return img\n\ndef count_data_items(fileids):\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(fileid).group(1)) \n         for fileid in fileids]\n    return np.sum(n)","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-07-16T11:46:03.468463Z","iopub.execute_input":"2021-07-16T11:46:03.468806Z","iopub.status.idle":"2021-07-16T11:46:03.48371Z","shell.execute_reply.started":"2021-07-16T11:46:03.468743Z","shell.execute_reply":"2021-07-16T11:46:03.482822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. Data Pipeline üçö\n* Read **TFRecord** files.\n* `cache` data to speed up the training.\n* `repeat` the data stream (for training only & test-time augmentation).\n* `shuffle` the data (for training only).\n* Unparse **tfrecord** data.\n* Decode data from ByteString to Image Data.\n* Process Image Data (Rescale).\n* Apply Augmentations.\n* Batch Data.\n","metadata":{}},{"cell_type":"code","source":"def get_dataset(files, augment = False, shuffle = False, repeat = False, \n                labeled=True, return_image_ids=True, batch_size=16, dim=IMG_SIZES[0]):\n    \n    ds = tf.data.TFRecordDataset(files, num_parallel_reads=AUTO) # read tfrecord\n    ds = ds.cache() # cache data for speedup\n    \n    if repeat:\n        ds = ds.repeat() # repeat the data (for training only)\n    \n    if shuffle: \n        ds = ds.shuffle(1024*2, seed=SEED) # shuffle the data (for training only)\n        opt = tf.data.Options()\n        opt.experimental_deterministic = False # order won't be maintained when we shuffle\n        ds = ds.with_options(opt)\n        \n    if labeled: \n        ds = ds.map(read_labeled_tfrecord, num_parallel_calls=AUTO) # unparse tfrecord data with labels\n    else:\n        ds = ds.map(lambda example: read_unlabeled_tfrecord(example, return_image_ids), \n                    num_parallel_calls=AUTO)      # unparse tfrecord data without labels\n    \n    ds = ds.map(lambda img, imgid_or_label: (prepare_image(img, augment=augment, dim=dim), \n                                               imgid_or_label),  # get img from bytestring, augmentations\n                num_parallel_calls=AUTO)\n    if labeled and augment:\n        ds = ds.map(lambda img, label: (dropout(img, DIM=dim, PROBABILITY = PROBABILITY, CT = CT, SZ = SZ), label),\n                    num_parallel_calls=AUTO)  # use dropout only in training\n    \n    ds = ds.batch(batch_size * REPLICAS) # batch the data\n    ds = ds.prefetch(AUTO) # prefatch data for speedup\n    return ds","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-07-16T11:46:03.485103Z","iopub.execute_input":"2021-07-16T11:46:03.485416Z","iopub.status.idle":"2021-07-16T11:46:03.500656Z","shell.execute_reply.started":"2021-07-16T11:46:03.485388Z","shell.execute_reply":"2021-07-16T11:46:03.499488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualization","metadata":{}},{"cell_type":"code","source":"def display_batch(batch, size=3):\n    imgs, tars = batch\n    for img_idx in range(size):\n        plt.figure(figsize=(5*2, 15*2))\n        for idx in range(3):\n            plt.subplot(size, 3, idx+1)\n            plt.title(f'id:{tars[img_idx].numpy().decode(\"utf-8\")}')\n            plt.imshow(imgs[img_idx,:, :, idx])\n            plt.text(5, 15, str(idx), bbox={'facecolor': 'white'})\n            plt.xticks([])\n            plt.yticks([])\n        plt.tight_layout()\n        plt.show() ","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-16T11:46:03.502131Z","iopub.execute_input":"2021-07-16T11:46:03.502548Z","iopub.status.idle":"2021-07-16T11:46:03.517451Z","shell.execute_reply.started":"2021-07-16T11:46:03.502509Z","shell.execute_reply":"2021-07-16T11:46:03.516548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fold = 0\nds = get_dataset(files_train, augment=True, shuffle=False, repeat=True,labeled=False,return_image_ids=True,\n                dim=IMG_SIZES[fold], batch_size = BATCH_SIZES[fold])\nds = ds.unbatch().batch(20)\nbatch = next(iter(ds))\ndisplay_batch(batch, 3);","metadata":{"execution":{"iopub.status.busy":"2021-07-16T11:46:03.518451Z","iopub.execute_input":"2021-07-16T11:46:03.518714Z","iopub.status.idle":"2021-07-16T11:46:08.97792Z","shell.execute_reply.started":"2021-07-16T11:46:03.518687Z","shell.execute_reply":"2021-07-16T11:46:08.977081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 8. Build Model üì¶\n>Though there have been some amazing development, **EFficientNet** always come up with a decent score.\n\n![EffNet](https://1.bp.blogspot.com/-MQO5qKuTT8c/XpdE8_IwpsI/AAAAAAAAFtg/mSjhF2ws5FYxwcHN6h9_l5DqYzQlNYJwwCLcBGAsYHQ/s1600/image1.png)\n\nYou can try other models like, \n* Vision Transformer (ViT)\n* ResNet\n* InceptionNet\n* XceptionNet","metadata":{}},{"cell_type":"code","source":"EFNS = [efn.EfficientNetB0, efn.EfficientNetB1, efn.EfficientNetB2, efn.EfficientNetB3, \n        efn.EfficientNetB4, efn.EfficientNetB5, efn.EfficientNetB6, efn.EfficientNetB7]\n\ndef build_model(dim=IMG_SIZES[0], ef=0):\n    inp = tf.keras.layers.Input(shape=(*dim,3)) # input layer with propoer img-dimension\n    base = EFNS[ef](input_shape=(*dim,3),weights='imagenet',include_top=False) # get base model (efficientnet), use imgnet weights\n    x = base(inp)\n    x = tf.keras.layers.GlobalAveragePooling2D()(x) # use GAP to get pooling result form conv outputs\n    x = tf.keras.layers.Dense(32, activation = 'relu')(x) # use activation to apply non-linearity\n    x = tf.keras.layers.Dense(1,activation='sigmoid')(x) # use sigmoid to convert predictions to [0-1]\n    model = tf.keras.Model(inputs=inp,outputs=x)\n    opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n    loss = tf.keras.losses.BinaryCrossentropy(label_smoothing=0.01)  # label smoothing for robustness\n    model.compile(optimizer=opt,loss=loss,metrics=['AUC'])\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-07-16T11:46:08.979156Z","iopub.execute_input":"2021-07-16T11:46:08.979649Z","iopub.status.idle":"2021-07-16T11:46:08.987807Z","shell.execute_reply.started":"2021-07-16T11:46:08.979615Z","shell.execute_reply":"2021-07-16T11:46:08.986998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 8. LR Schedule ‚öì\n* This is a common train schedule for transfer learning. \n* The learning rate starts near zero, then increases to a maximum, then decays over time. Consider changing the schedule and/or learning rates. Note how the learning rate max is larger with larger batches sizes.","metadata":{}},{"cell_type":"code","source":"def get_lr_callback(batch_size=8, plot=False):\n    lr_start   = 0.000005\n    lr_max     = 0.00000125 * REPLICAS * batch_size\n    lr_min     = 0.000001\n    lr_ramp_ep = 5\n    lr_sus_ep  = 0\n    lr_decay   = 0.8\n   \n    def lrfn(epoch):\n        if epoch < lr_ramp_ep:\n            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n            \n        elif epoch < lr_ramp_ep + lr_sus_ep:\n            lr = lr_max\n            \n        else:\n            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n            \n        return lr\n    if plot:\n        plt.figure(figsize=(10,5))\n        plt.plot(np.arange(EPOCHS[0]), [lrfn(epoch) for epoch in np.arange(EPOCHS[0])], marker='o')\n        plt.xlabel('epoch'); plt.ylabel('learnig rate')\n        plt.title('Learning Rate Scheduler')\n        plt.show()\n\n    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=False)\n    return lr_callback\n\n_=get_lr_callback(BATCH_SIZES[0], plot=True )","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-16T11:46:08.989413Z","iopub.execute_input":"2021-07-16T11:46:08.989889Z","iopub.status.idle":"2021-07-16T11:46:09.37488Z","shell.execute_reply.started":"2021-07-16T11:46:08.989857Z","shell.execute_reply":"2021-07-16T11:46:09.374141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 9. Training üöÖ\n* Our model will be trained for the number of `FOLDS` and `EPOCHS` you chose in the configuration above. Each fold the model with lowest validation `AUC` will be saved and used to predict OOF and test. ","metadata":{}},{"cell_type":"code","source":"skf = KFold(n_splits=FOLDS,shuffle=True,random_state=SEED)\noof_pred = []; oof_tar = []; oof_val = []; oof_f1 = []; oof_ids = []; oof_folds = [] \npreds = np.zeros((count_data_items(files_test),1))\n\nfor fold,(idxT,idxV) in enumerate(skf.split(np.arange(num_train_files))):\n    # DISPLAY FOLD INFO\n    if DEVICE=='TPU':\n        if tpu: tf.tpu.experimental.initialize_tpu_system(tpu)\n    \n    # CREATE TRAIN AND VALIDATION SUBSETS\n    files_train = tf.io.gfile.glob([GCS_PATH[fold] + '/train%.2i*.tfrec'%x for x in idxT])\n    np.random.shuffle(files_train);\n    files_valid = tf.io.gfile.glob([GCS_PATH[fold] + '/train%.2i*.tfrec'%x for x in idxV])\n    files_test = np.sort(np.array(tf.io.gfile.glob(GCS_PATH[fold] + '/test*.tfrec')))\n    \n    print('#'*25); print('#### FOLD',fold+1)\n    print('#### Image Size: (%i, %i) | model: %s | batch_size %i'%\n          (IMG_SIZES[fold][0],IMG_SIZES[fold][1],EFNS[EFF_NETS[fold]].__name__,BATCH_SIZES[fold]*REPLICAS))\n    train_images = count_data_items(files_train)\n    val_images   = count_data_items(files_valid)\n    print('#### Training: %i | Validation: %i'%(train_images, val_images))\n    \n    # BUILD MODEL\n    K.clear_session()\n    with strategy.scope():\n        model = build_model(dim=IMG_SIZES[fold],ef=EFF_NETS[fold])\n    print('#'*25)   \n    # SAVE BEST MODEL EACH FOLD\n    sv = tf.keras.callbacks.ModelCheckpoint(\n        'fold-%i.h5'%fold, monitor='val_auc', verbose=0, save_best_only=True,\n        save_weights_only=True, mode='max', save_freq='epoch')\n   \n    # TRAIN\n    print('Training...')\n    history = model.fit(\n        get_dataset(files_train, augment=AUGMENT, shuffle=True, repeat=True,\n                dim=IMG_SIZES[fold], batch_size = BATCH_SIZES[fold]), \n        epochs=EPOCHS[fold], \n        callbacks = [sv,get_lr_callback(BATCH_SIZES[fold])], \n        steps_per_epoch=count_data_items(files_train)/BATCH_SIZES[fold]//REPLICAS,\n        validation_data=get_dataset(files_valid,augment=False,shuffle=False,\n                repeat=False,dim=IMG_SIZES[fold]), \n        #class_weight = {0:1,1:2},\n        verbose=VERBOSE\n    )\n    \n    # Loading best model for inference\n    print('Loading best model...')\n    model.load_weights('fold-%i.h5'%fold)  \n    \n    # PREDICT OOF USING TTA\n    print('Predicting OOF with TTA...')\n    ds_valid = get_dataset(files_valid,labeled=False,return_image_ids=False,augment=AUGMENT,\n            repeat=True,shuffle=False,dim=IMG_SIZES[fold],batch_size=BATCH_SIZES[fold]*2)\n    ct_valid = count_data_items(files_valid); STEPS = TTA * ct_valid/BATCH_SIZES[fold]/2/REPLICAS\n    pred = model.predict(ds_valid,steps=STEPS,verbose=VERBOSE)[:TTA*ct_valid,] \n    oof_pred.append( np.mean(pred.reshape((ct_valid,TTA),order='F'),axis=1) )                 \n    \n    # GET OOF TARGETS AND idS\n    ds_valid = get_dataset(files_valid, augment=False, repeat=False, dim=IMG_SIZES[fold],\n            labeled=True, return_image_ids=True)\n    oof_tar.append( np.array([target.numpy() for img, target in iter(ds_valid.unbatch())]) )\n    oof_folds.append( np.ones_like(oof_tar[-1],dtype='int8')*fold )\n    ds = get_dataset(files_valid, augment=False, repeat=False, dim=IMG_SIZES[fold],\n                labeled=False, return_image_ids=True)\n    oof_ids.append( np.array([img_id.numpy().decode(\"utf-8\") for img, img_id in iter(ds.unbatch())]))\n    \n    # PREDICT TEST USING TTA\n    print('Predicting Test with TTA...')\n    ds_test = get_dataset(files_test,labeled=False,return_image_ids=False,augment=AUGMENT,\n            repeat=True,shuffle=False,dim=IMG_SIZES[fold],batch_size=BATCH_SIZES[fold]*2)\n    ct_test = count_data_items(files_test); STEPS = TTA * ct_test/BATCH_SIZES[fold]/2/REPLICAS\n    pred = model.predict(ds_test,steps=STEPS,verbose=VERBOSE)[:TTA*ct_test,] \n    preds[:,0] += np.mean(pred.reshape((ct_test,TTA),order='F'),axis=1) * WGTS[fold]\n    \n    # REPORT RESULTS\n    auc = roc_auc_score(oof_tar[-1],oof_pred[-1])\n    oof_val.append(np.max( history.history['val_auc'] ))\n    print('#### FOLD %i OOF AUC without TTA = %.3f, with TTA = %.3f'%(fold+1,oof_val[-1],auc))\n    \n    # PLOT TRAINING\n    if DISPLAY_PLOT:\n        plt.figure(figsize=(15,5))\n        plt.plot(np.arange(len(history.history['auc'])),history.history['auc'],'-o',label='Train auc',color='#ff7f0e')\n        plt.plot(np.arange(len(history.history['auc'])),history.history['val_auc'],'-o',label='Val auc',color='#1f77b4')\n        x = np.argmax( history.history['val_auc'] ); y = np.max( history.history['val_auc'] )\n        xdist = plt.xlim()[1] - plt.xlim()[0]; ydist = plt.ylim()[1] - plt.ylim()[0]\n        plt.scatter(x,y,s=200,color='#1f77b4'); plt.text(x-0.03*xdist,y-0.13*ydist,'max auc\\n%.2f'%y,size=14)\n        plt.ylabel('auc',size=14); plt.xlabel('Epoch',size=14)\n        plt.legend(loc=2)\n        plt2 = plt.gca().twinx()\n        plt2.plot(np.arange(len(history.history['auc'])),history.history['loss'],'-o',label='Train Loss',color='#2ca02c')\n        plt2.plot(np.arange(len(history.history['auc'])),history.history['val_loss'],'-o',label='Val Loss',color='#d62728')\n        x = np.argmin( history.history['val_loss'] ); y = np.min( history.history['val_loss'] )\n        ydist = plt.ylim()[1] - plt.ylim()[0]\n        plt.scatter(x,y,s=200,color='#d62728'); plt.text(x-0.03*xdist,y+0.05*ydist,'min loss',size=14)\n        plt.ylabel('Loss',size=14)\n        plt.title('FOLD %i - Image Size (%i, %i), %s'%\n                (fold+1,IMG_SIZES[fold][0],IMG_SIZES[fold][1],EFNS[EFF_NETS[fold]].__name__),size=18)\n        plt.legend(loc=3)\n        plt.savefig(f'fig{fold}.png')\n        plt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-16T11:46:09.376058Z","iopub.execute_input":"2021-07-16T11:46:09.376506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 10. Calculate OOF üëÄ\nThe **OOF** (out of fold) predictions are saved to disk. If you wish to ensemble multiple models, use the **OOF** to determine what are the best weights to blend your models with. Choose weights that maximize **OOF** `CV` score when used to blend **OOF**. Then use those same weights to blend your test predictions.\n\n**Note**:\n* Don't do blending just to climb **LB**, because most of the time it ends up getting overfitted.\n* Try improving the **CV** by blending different model and you can keep an eye on the **LB**.\n* As only`20%` data will be used for calculating **LB** score, relying on **CV** should be a safe option","metadata":{}},{"cell_type":"code","source":"# COMPUTE OVERALL OOF AUC\noof = np.concatenate(oof_pred); true = np.concatenate(oof_tar);\nids = np.concatenate(oof_ids); folds = np.concatenate(oof_folds)\nauc = roc_auc_score(true,oof)\nprint('Overall OOF AUC with TTA = %.3f'%auc)\n\n# SAVE OOF TO DISK\ndf_oof = pd.DataFrame(dict(image_id = ids, target=true, pred = oof, fold=folds))\ndf_oof.to_csv('oof.csv',index=False)\ndf_oof.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 11. Post process üçß\nThere are ways to modify predictions based on patient information to increase **CV**-**LB**. You can experiment with that here on your **OOF**.","metadata":{}},{"cell_type":"markdown","source":"# 12. Submission üë£","metadata":{}},{"cell_type":"code","source":"ds = get_dataset(files_test, augment=False, repeat=False, dim=IMG_SIZES[fold],\n                 labeled=False, return_image_ids=True)\n\nimage_ids = np.array([img_id.numpy().decode(\"utf-8\") \n                        for img, img_id in iter(ds.unbatch())])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame({'id':image_ids, 'target':preds[:,0]})\nsubmission = submission.sort_values('id') \nsubmission.to_csv('submission.csv', index=False)\nsubmission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prediction Distribution","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,5))\nplt.hist(submission.target,bins=100);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 13. Reference üí°\n* Please Check this amazing notebook, [Triple Stratified KFold with TFRecords](https://www.kaggle.com/cdeotte/triple-stratified-kfold-with-tfrecords) by [Chris Deotte](https://www.kaggle.com/cdeotte)\n","metadata":{}},{"cell_type":"markdown","source":"## Please **Upvote** if you find this helpful üëΩ","metadata":{}}]}