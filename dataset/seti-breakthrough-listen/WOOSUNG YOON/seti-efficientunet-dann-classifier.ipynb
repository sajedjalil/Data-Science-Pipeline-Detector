{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport sys\nsys.path.append('../input/timm-pytorch-image-models/pytorch-image-models-master/')\nfrom glob import glob\nimport pickle\nfrom tqdm.notebook import tqdm\n\nimport numpy as np\nimport pandas as pd\nimport random\nimport math\n\nimport cv2\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nfrom sklearn import metrics\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import train_test_split\n\n!pip install efficientunet-pytorch\nfrom efficientunet import *\nimport timm\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Function\nfrom torchvision import transforms\nimport itertools","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Air conditionor + GPU \nelectric fee...\n\nSETI test 5","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('../input/seti-breakthrough-listen/train_labels.csv')\ntrain_df_old = pd.read_csv('../input/seti-breakthrough-listen/old_leaky_data/train_labels_old.csv')\ntest_df = pd.read_csv('../input/seti-breakthrough-listen/sample_submission.csv')\n\ntrain_df['filepath'] = train_df.id.apply(lambda x: f'../input/seti-breakthrough-listen/train/{x[0]}/{x}.npy')\ntrain_df_old['filepath'] = train_df_old.id.apply(lambda x : f'../input/seti-breakthrough-listen/old_leaky_data/train_old/{x[0]}/{x}.npy')\ntest_df['filepath'] = test_df.id.apply(lambda x : f'../input/seti-breakthrough-listen/test/{x[0]}/{x}.npy')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class setiDataset(Dataset):\n    def __init__(self, df): \n        self.df = df\n    \n    def fileinfo(self, idx):\n        return self.df.filepath.iloc[idx], self.df.target.iloc[idx]\n    \n    def __getitem__(self, idx):\n        filepath, target = self.fileinfo(idx)\n        image = np.load(filepath).astype('float32')\n        image = np.stack([image[0], image[2], image[4]]).transpose(1,2,0)\n        image = cv2.resize(image, (320,320), interpolation=cv2.INTER_LINEAR)\n        \n        image = ToTensorV2()(image=image)['image'] \n        return image, torch.tensor(target, dtype=torch.long)\n    \n    def __len__(self):\n        return len(self.df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class new_old_Dataset(Dataset):\n    def __init__(self, df, df_old): \n        N = min(len(df[df.target==1]), len(df_old[df_old.target==1]))\n        self.df = df[df.target==1].sample(N).reset_index(drop=True)\n        self.df_old = df_old[df_old.target==1].sample(N).reset_index(drop=True)\n        \n    def fileinfo(self, idx):\n        return self.df.filepath.iloc[idx], self.df_old.filepath.iloc[idx]\n    \n    def __getitem__(self, idx):\n        filepath, old_filepath = self.fileinfo(idx)\n        image = np.load(filepath).astype('float32')\n        image = np.stack([image[0], image[2], image[4]]).transpose(1,2,0)\n        image = cv2.resize(image, (320,320), interpolation=cv2.INTER_LINEAR)\n        image = ToTensorV2()(image=image)['image'] \n        \n        old_image = np.load(old_filepath).astype('float32')\n        old_image = np.stack([old_image[0], old_image[2], old_image[4]]).transpose(1,2,0)\n        old_image = cv2.resize(old_image, (320,320), interpolation=cv2.INTER_LINEAR)\n        old_image = ToTensorV2()(image=old_image)['image']\n        \n        return image, old_image\n    \n    def __len__(self):\n        return len(self.df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class GradReverse(Function):\n    @staticmethod\n    def forward(ctx, x, lamda):\n        ctx.lamda = lamda\n        return x.view_as(x)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        output = (grad_output.neg() * ctx.lamda)\n        return output, None\n\nclass domain_classifier(nn.Module):\n    def __init__(self):\n        super(domain_classifier, self).__init__()\n        self.main = nn.Sequential(\n            nn.Conv2d(1, 64, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(512, 512, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(512, 512, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(512, 512, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(512, 1, 2, bias=False),\n        )\n        \n    def forward(self, x, lam=0.0):\n        x = GradReverse.apply(x, lam)\n        return self.main(x).view(-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(data_loader, \n              model, label_classifier, \n              optimizer, device):\n    model.train()\n    label_classifier.train()\n    for data in tqdm(data_loader, position=0, leave=True, desc='Training'):\n        images, targets = data\n        images = images.to(device, dtype=torch.float)\n        targets = targets.to(device, dtype=torch.float)\n        def closure():\n            optimizer.zero_grad()\n            output = model(images)\n            output = label_classifier(output)\n            loss = nn.BCEWithLogitsLoss()(output, targets.view(-1,1))\n            loss.backward()\n            return loss\n        optimizer.step(closure)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def adjust_lambda(pos, current_epoch, data_loader_size, total_epochs):\n    p = (pos + current_epoch*data_loader_size)/(total_epochs*data_loader_size)\n    lam = 2/(1+np.exp(-10*p)) - 1\n    return lam\n\ndef train_dann(data_loader, epoch,\n               model, label_classifier, domain_discriminator, \n               optimizer, optimizer_D, device):\n    model.train()\n    label_classifier.train()\n    domain_discriminator.train()\n    \n    N = len(data_loader)\n    for pos, data in tqdm(enumerate(data_loader), position=0, leave=True, total=N, desc='DANN Training'):\n        images, images_old = data\n        batch_size = images.size(0)\n        targets = torch.ones(batch_size)\n        targets_old = torch.zeros(batch_size)\n        \n        images = images.to(device, dtype=torch.float)\n        images_old = images_old.to(device, dtype=torch.float)\n        targets = targets.to(device, dtype=torch.float)\n        targets_old = targets_old.to(device, dtype=torch.float)\n        \n        \n        optimizer_D.zero_grad()\n        optimizer.zero_grad()\n        output = model(images)\n        predict = label_classifier(output)\n        loss = nn.BCEWithLogitsLoss()(predict, targets.view(-1,1))\n        \n        output_ = model(images_old)\n        output = torch.cat([output,output_], dim=0) \n        lam = adjust_lambda(pos, epoch, N, total_epochs=5)   \n        predict = domain_discriminator(output, lam)\n        targets_old = torch.cat([targets, targets_old])\n        loss += nn.BCEWithLogitsLoss()(predict, targets_old)\n        loss.backward()\n        \n        optimizer.step()\n        optimizer_D.step()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(data_loader, epoch, \n             model, label_classifier, device):\n    model.eval()\n    label_classifier.eval()\n    final_targets = []\n    final_outputs = []\n    with torch.no_grad():\n        \n        for data in tqdm(data_loader, position=0, leave=True, desc='Evaluating'):\n            images, targets = data\n\n            images = images.to(device, dtype=torch.float)\n            targets = targets.to(device, dtype=torch.float)\n            \n            output = model(images)\n            output = label_classifier(output)\n            \n            targets = targets.detach().cpu().numpy().tolist()\n            output = output.detach().cpu().numpy().tolist()\n            \n            final_targets.extend(targets)\n            final_outputs.extend(output)\n        \n    roc_auc = metrics.roc_auc_score(final_targets, final_outputs)\n    print(f\"Epoch={epoch}, Valid ROC AUC={roc_auc}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device('cpu')\n\nmodel = get_efficientunet_b0(out_channels=1, concat_input=True, pretrained=True)\nlabel_classifier = timm.create_model('mobilenetv3_large_100', in_chans = 1, pretrained=True, num_classes=1)\ndomain_discriminator = domain_classifier()\nmodel.to(device)\nlabel_classifier.to(device)\ndomain_discriminator.to(device)\n\noptimizer = torch.optim.Adam(list(model.parameters()) + list(label_classifier.parameters()), lr=5e-4, weight_decay=1e-5)\noptimizer_D = torch.optim.Adam(domain_discriminator.parameters(), lr=5e-6, weight_decay=1e-7)\n\nBATCH_C = 16\nBATCH_D = 8\n\ntrn_df, val_df = train_test_split(train_df, test_size=0.15, random_state=42)\ntrn_df = trn_df.reset_index(drop=True)\nval_df = val_df.reset_index(drop=True)\n\ntrain_dataset = setiDataset(trn_df)\ntrain_loader= torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_C, shuffle=True)\n\nvalid_dataset = setiDataset(val_df)\nvalid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_C, shuffle=False)\n\ntrain_dataset_old = new_old_Dataset(trn_df, train_df_old)\ntrain_loader_old= torch.utils.data.DataLoader(train_dataset_old, batch_size=BATCH_D, shuffle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(3):\n    train(train_loader, \n              model, label_classifier, \n              optimizer, device\n    )\n    evaluate(valid_loader, epoch, \n             model, label_classifier, device\n    )\n    \n\nfor epcoh in range(3):\n    train_dann(train_loader_old, epoch,\n               model, label_classifier, domain_discriminator, \n               optimizer, optimizer_D, device\n    )\n    evaluate(valid_loader, epoch, \n             model, label_classifier, device\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}