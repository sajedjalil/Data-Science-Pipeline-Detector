{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom IPython.core.display import HTML\ndef css_styling():\n    styles = open(\"../input/titlestyle/style2.css\", \"r\").read()\n    return HTML(styles)\ncss_styling()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-07-17T19:48:45.148921Z","iopub.execute_input":"2021-07-17T19:48:45.149294Z","iopub.status.idle":"2021-07-17T19:48:45.162241Z","shell.execute_reply.started":"2021-07-17T19:48:45.14926Z","shell.execute_reply":"2021-07-17T19:48:45.161059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"heading\">\n   <h1><span style=\"color: white\">Intro</span></h1>\n</div>\n<div class=\"content\">\n\n<u>üìî Public notebooks - 180+</u><br>\n\n<u>ü•á Gold medals - 18+</u><br>\n\n<u>ü•à Silver medals - 23+</u><br>\n\n<u>ü•â Bronze medals - 49+</u><br>\n\n    \nWordcloud made of notebook titles:\n<img src=\"https://i.imgur.com/kmmFe4D.jpeg\" alt=\"img1\"/>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div style = \"font-family: Arial;font-size:1.6em;color: #0a6121;background: #ace6bc;padding:5px;border-style: solid;border-color:#0a6121;\">\n<b>Summa summarum:</b> Pretrained neural networks and data augumentation.\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div class=\"heading\">\n   <h1><span style=\"color: white\">Goal</span></h1>\n</div>\n<div class='content'>\n    <h3><b>üõ∏ Identify anomalous signals, where the signal is a radio signal represented by spectrogram with a 2d array.</b></h3>\n    <h3><b>üõ∏ The general goal of this competition is to find new ways for analyzing signals from the space in order to detect artificial ones.</b></h3>\n</div>","metadata":{}},{"cell_type":"code","source":"def get_train_filename_by_id(_id: str) -> str:\n    return f\"../input/seti-breakthrough-listen/train/{_id[0]}/{_id}.npy\"\n\ndef show_cadence(filename: str, label: int) -> None:\n    plt.figure(figsize=(16, 10))\n    arr = np.load(filename)\n    for i in range(6):\n        plt.subplot(6, 1, i + 1)\n        if i == 0:\n            plt.title(f\"ID: {os.path.basename(filename)} TARGET: {label}\\\n            {'(anomalous)' if label==1 else '(nonanomalous)'}\", fontsize=18)\n        plt.imshow(arr[i].astype(float), interpolation='nearest', aspect='auto')\n        plt.text(5, 100, [\"ON\", \"OFF\"][i % 2], bbox={'facecolor': 'white'})\n        plt.xticks([])\n    plt.show()\n\ndf_train = pd.read_csv(\"../input/seti-breakthrough-listen/train_labels.csv\")\n\ndf_tmp = df_train[df_train[\"target\"] == 0].sample(1)\nfor ind, row in df_tmp.iterrows():\n    show_cadence(get_train_filename_by_id(row[\"id\"]), row[\"target\"])\n\ndf_tmp = df_train[df_train[\"target\"] == 1].sample(3)\nfor ind, row in df_tmp.iterrows():\n    show_cadence(get_train_filename_by_id(row[\"id\"]), row[\"target\"])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-17T18:15:15.544932Z","iopub.execute_input":"2021-07-17T18:15:15.545373Z","iopub.status.idle":"2021-07-17T18:15:18.137795Z","shell.execute_reply.started":"2021-07-17T18:15:15.545339Z","shell.execute_reply":"2021-07-17T18:15:18.136982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"heading\">\n   <h1><span style=\"color: white\">Pretrained neural networks</span></h1>\n</div>\n<div class='content'>\n    \nThe process of training a neural network is a difficult and time-consuming process and for most of the users not even feasible. CNN's are an architecture typically used for image recognition and classification, but they can be applied for other tasks as well. It is very unlikely that some of the CNN models trained from scratch can be comparable with pre-trained ones. Thus, the rule of thumb when it is any image data classification task is the use of existing, <b>pretrained models</b>.<br><br>\n\nIn that sense, currently, state of the art achievements holds <b>NFNet</b> models <a href=\"https://arxiv.org/pdf/2102.06171.pdf\">https://arxiv.org/pdf/2102.06171.pdf</a>.\nUser-friendly notebook with PyTorch implementation of this model can be found here <a href=\"https://www.kaggle.com/yasufuminakama/seti-nfnet-l0-starter-training\">https://www.kaggle.com/yasufuminakama/seti-nfnet-l0-starter-training</a>.<br>\n\n<img src=\"https://i.imgur.com/y7mCw5f.png\" alt=\"img1\"/>\n<center><i>NFNet comparison, Source: High-Performance Large-Scale Image Recognition Without Normalization.</i></center>\n<br>\n<b>Data augmentation</b> is a method used to increase the data set by adding synthetic samples generated from observations of the original data. It is a technique to improve machine learning models by introducing noise to the training set. Methods such as <b>rotation</b> and <b>cropping</b> are used as they can add variation and diversity in images. The example using `albumentations` package with <b>ResNet18d in PyTorch</b> are shown here <a href=\"https://www.kaggle.com/ttahara/seti-e-t-resnet18d-baseline\">https://www.kaggle.com/ttahara/seti-e-t-resnet18d-baseline</a>.<br><br>\n\nInteresting data augmentation techniques to prevent overfitting and encourage generalization are <b>coarse dropout and cutout</b>. They randomly remove rectangles from the images and gives a similar effect as the dropout layer in CNN. These methods with <b>EfficientNet in TF</b> are presented here <a href=\"https://www.kaggle.com/awsaf49/seti-bl-tf-starter-tpu\">https://www.kaggle.com/awsaf49/seti-bl-tf-starter-tpu</a>.<br><br>\n\n<b>EfficientNet with Keras</b> <a href=\"https://www.kaggle.com/usharengaraju/seti-eda-baseline-tensorflow-and-tpu\">https://www.kaggle.com/usharengaraju/seti-eda-baseline-tensorflow-and-tpu</a> and even simpler here <a href=\"https://www.kaggle.com/mrigendraagrawal/tf-spatial-seti-breakthrough-listen-starter\">https://www.kaggle.com/mrigendraagrawal/tf-spatial-seti-breakthrough-listen-starter</a>.<br><br>\n\nSimple <b>EfficientNet starter in PyTorch</b> <a href=\"https://www.kaggle.com/micheomaano/simple-yet-effectiveb0-lb-0-96\">https://www.kaggle.com/micheomaano/simple-yet-effectiveb0-lb-0-96</a>.<br><br>\n\nUsing <b>PyTorch Image Models (timm)</b> <a href=\"https://fastai.github.io/timmdocs/\">https://fastai.github.io/timmdocs/</a> can be easily created pretrained model with several lines of code.<br><br>\n\n<blockquote style=\"background: #f9f9f9;border-left: 10px solid #ccc;margin: 1.5em 10px;padding: 0.5em 10px;\">Timm is a deep-learning library created by Ross Wightman and is a collection of SOTA computer vision models, layers, utilities, optimizers, schedulers, data-loaders, augmentations and also training/validating scripts with ability to reproduce ImageNet training results</blockquote>\n<blockquote style=\"background: #f9f9f9;border-left: 10px solid #ccc;margin: 1.5em 10px;padding: 0.5em 10px;\">It is that simple to create a model using timm. The create_model function is a factory method that can be used to create over 300 models that are part of the timm library.</blockquote><br><br>\n\nExample using `timm` with the <b>SwimNet</b> <a href=\"https://www.kaggle.com/ligtfeather/swinformer-cutmix-amp-accelerate-w-b\">https://www.kaggle.com/ligtfeather/swinformer-cutmix-amp-accelerate-w-b</a>.<br><br>\n\n\nOne more interesting image augmentation technique is <b>mixup</b>. The original paper states that this technique improves the generalization error of state-of-the-art models on ImageNet and CIFAR. Also, it helps to combat memorization of corrupt labels, sensitivity to adversarial examples, and instability in adversarial training (adversarial examples are inputs to machine learning models that have been intentionally designed to cause the model to make a mistake). Usage of <b>mixup with NFNet</b> <a href=\"https://www.kaggle.com/ligtfeather/eca-nfnet-sam-opt-mixup-k-folds-w-b\">https://www.kaggle.com/ligtfeather/eca-nfnet-sam-opt-mixup-k-folds-w-b</a>. From the comparative study, it seems that the <b>mixup</b> can give a significant increase in the score <a href=\"https://www.kaggle.com/c/seti-breakthrough-listen/discussion/245152\">https://www.kaggle.com/c/seti-breakthrough-listen/discussion/245152</a>.<br>\n\n<img src=\"https://i.imgur.com/j6R66v9.png\" alt=\"img1\"/>\n<center><i>Mixup augumentation, Source: <a href=\"https://amitness.com/2020/05/data-augmentation-for-nlp/\">https://amitness.com/2020/05/data-augmentation-for-nlp/</a></i></center>\n<br>\nWhile CNN's are standard models with tasks on images, transformers are not yet common in this field. However, there are some promising works trying to utilize transformers for image-based tasks. <b>Vision Transformer (ViT)</b> is one of them. First, it divides a given input image into patches. These patches are then sequentially passed through a linear projection layer that has a role of an embedding layer. After that, position information is added\nto the embedding vectors and fed into the standard transformer. <b>ViT with PyTorch and timm</b> is presented here <a href=\"https://www.kaggle.com/heyytanay/pytorch-training-augments-vit-kfolds\">https://www.kaggle.com/heyytanay/pytorch-training-augments-vit-kfolds</a>.<br>\n\n<img src=\"https://i.imgur.com/i2kJ29F.jpg\" alt=\"img1\"/>\n<center><i>ViT, Source: <a href=\"https://www.youtube.com/watch?v=3B6q4xnuFUE\">https://www.youtube.com/watch?v=3B6q4xnuFUE</a></i></center>\n<br>\nBesides CNNs and transformers, one more architecture that is proposed in this competition is <b>bidirectional GRU</b>. This is a recurrent neural network that has a slightly simpler mechanism than LSTM. Classic implementation with Keras can be found here <a href=\"https://www.kaggle.com/xhlulu/seti-bidirectional-gru-in-keras\">https://www.kaggle.com/xhlulu/seti-bidirectional-gru-in-keras</a>.<br><br>\n\nUsually what brings high scores in competitions is using ensemble models. This was also proved here <a href=\"https://www.kaggle.com/reighns/lb-0-98-cv-0-9915-forward-ensembling-technique\">https://www.kaggle.com/reighns/lb-0-98-cv-0-9915-forward-ensembling-technique</a> using <b>forward OOF ensemble</b>. Great explanation and tutorial of <b>CV and forward ensemble (hill climbing)</b> has been discussed here <a href=\"https://www.kaggle.com/c/siim-isic-melanoma-classification/discussion/175614\">https://www.kaggle.com/c/siim-isic-melanoma-classification/discussion/175614</a>.<br><br>\n    \nYes, it is possible to train <b>PyTorch model on the TPU</b> and the way how to do it is explained here <a href=\"https://www.kaggle.com/tanlikesmath/seti-pytorch-xla-tpu-starter\">https://www.kaggle.com/tanlikesmath/seti-pytorch-xla-tpu-starter</a>. There is provided an in-depth look into how you can use <b>PyTorch XLA</b> (Accelerated Linear Algebra) interface with TPUs by compiling the PyTorch code as XLA programs to run on TPU devices.<br><br>\n    \nUnusually for this competition, <b>Rapids KNN</b> fitted with <b>embeddings</b> from fine-tuned <b>EfficientNet</b> <a href=\"https://www.kaggle.com/tuckerarrants/seti-rapids-knn\">https://www.kaggle.com/tuckerarrants/seti-rapids-knn</a>\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"<div class=\"body\">\n   <h1><span style=\"color: white\">Data augumentation</span></h1>\n</div>\n<div class=\"content\">\n    \nBesides the above-mentioned image augmentation techniques, we will point out some of them here. Comparison of several data augmentation techniques where the best results had <b>mixup, vflip, shift scale rotate, motion blur and SpecAugment</b> <a href=\"https://www.kaggle.com/shionhonda/search-for-effective-data-augmentation\">https://www.kaggle.com/shionhonda/search-for-effective-data-augmentation</a><br><br>\n    \nBefore passing images into CNNs, we usually resize them to the compatible size. This procedure can result in information loss because we are usually relying on some interpolation techniques for shrinking the images. Thus, the author of this notebook <a href=\"https://www.kaggle.com/tuckerarrants/seti-learned-image-resizing\">https://www.kaggle.com/tuckerarrants/seti-learned-image-resizing</a> proposed to train <b>CNN-based resizer</b>, that is recently published in this paper <a href=\"https://arxiv.org/pdf/2103.09950.pdf\">https://arxiv.org/pdf/2103.09950.pdf</a> by Google Research team.\n<img src=\"https://i.imgur.com/R5YtQWN.jpg\" alt=\"img1\"/>\n<center><i>CNN resizer, Source: <a href=\"https://arxiv.org/pdf/2103.09950.pdf\">https://arxiv.org/pdf/2103.09950.pdf</a></i></center>\n<br>\nVery clean notebook with <b>resize, horizontal flip, vertical flip, random resized crop, shift scale rotate, cutout and mixup</b> followed by <b>Resnet34d in Pytorch</b> can be found here <a href=\"https://www.kaggle.com/manabendrarout/inference-5x-ensemble-vanilla-resnet34d-seti\">https://www.kaggle.com/manabendrarout/inference-5x-ensemble-vanilla-resnet34d-seti</a><br><br>\n    \n<b>AugLy - a data augmentations library</b> by Facebook that supports 4 modalities (audio, image, text and video). <a href=\"https://www.kaggle.com/shanmukh05/augly-fair-s-new-data-augmentation-library\">https://www.kaggle.com/shanmukh05/augly-fair-s-new-data-augmentation-library</a>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div class=\"body\">\n   <h1><span style=\"color: white\">Worth mentioning</span></h1>\n</div>\n<div class=\"content\">\n‚≠ê Extensive <b>PyTorch model tutorial</b> with the explanation of each step\n<a href=\"https://www.kaggle.com/reighns/tutorial-pytorch-inference-flashcard-questions\">https://www.kaggle.com/reighns/tutorial-pytorch-inference-flashcard-questions</a><br><br>\n    \n‚≠ê <b>Fast AUC</b> function <a href=\"https://www.kaggle.com/c/seti-breakthrough-listen/discussion/244387\">https://www.kaggle.com/c/seti-breakthrough-listen/discussion/244387</a>\n</div>","metadata":{}},{"cell_type":"markdown","source":"### Code reference\n\n* üõ∏ Signal Search üëΩ - Exploratory Data Analysis, https://www.kaggle.com/ihelon/signal-search-exploratory-data-analysis","metadata":{}}]}