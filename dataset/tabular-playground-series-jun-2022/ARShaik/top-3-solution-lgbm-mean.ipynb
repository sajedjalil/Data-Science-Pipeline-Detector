{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <div style=\"padding:20px;color:white;margin:0;font-size:200%;text-align:center;display:fill;border-radius:5px;background-color:#AF601A;overflow:hidden;font-weight:500\">TPS June 2022\n</div>\n\n\n","metadata":{}},{"cell_type":"markdown","source":" ### If you are a beginner, see my other notebook for imputation tutorial [notebook](https://www.kaggle.com/code/abdulravoofshaik/quick-eda-and-missing-values-tutorial). \n ### The following cartoon depicts the overall framework for applying advanced regression technique for imputation.\n","metadata":{}},{"cell_type":"markdown","source":"<img src=\"https://i.postimg.cc/90BdtTjq/imputer.gif\">\n\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#E59866;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:black;\"><b>1.0 | Load data and Preprocessing</b></p>\n</div>","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport missingno as msno\npd.set_option('display.max_columns', None)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.impute import SimpleImputer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-18T22:19:05.123235Z","iopub.execute_input":"2022-06-18T22:19:05.123712Z","iopub.status.idle":"2022-06-18T22:19:05.749419Z","shell.execute_reply.started":"2022-06-18T22:19:05.123623Z","shell.execute_reply":"2022-06-18T22:19:05.748476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(\"../input/tabular-playground-series-jun-2022/data.csv\")\nTarget = pd.read_csv(\"../input/tabular-playground-series-jun-2022/sample_submission.csv\", index_col='row-col')","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-18T22:19:05.750993Z","iopub.execute_input":"2022-06-18T22:19:05.751353Z","iopub.status.idle":"2022-06-18T22:19:23.292208Z","shell.execute_reply.started":"2022-06-18T22:19:05.75132Z","shell.execute_reply":"2022-06-18T22:19:23.291288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose:\n        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df\n\ndata = reduce_mem_usage(data)\nTarget = reduce_mem_usage(Target)","metadata":{"execution":{"iopub.status.busy":"2022-06-18T22:19:23.293468Z","iopub.execute_input":"2022-06-18T22:19:23.293816Z","iopub.status.idle":"2022-06-18T22:19:27.969997Z","shell.execute_reply.started":"2022-06-18T22:19:23.293787Z","shell.execute_reply":"2022-06-18T22:19:27.968774Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get the number of missing data points per column\nmissing_values_count = data.isnull().sum()\n# look at the # of missing points in the first ten columns\nmissing_values_count[0:30]","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-18T22:19:27.971774Z","iopub.execute_input":"2022-06-18T22:19:27.972126Z","iopub.status.idle":"2022-06-18T22:19:28.130155Z","shell.execute_reply.started":"2022-06-18T22:19:27.972096Z","shell.execute_reply":"2022-06-18T22:19:28.129386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# how many total missing values do we have?\ntotal_cells = np.product(data.shape)\ntotal_missing = missing_values_count.sum()\n\n# percent of data that is missing\n(total_missing/total_cells) * 100","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-18T22:19:28.1313Z","iopub.execute_input":"2022-06-18T22:19:28.131756Z","iopub.status.idle":"2022-06-18T22:19:28.139316Z","shell.execute_reply.started":"2022-06-18T22:19:28.131726Z","shell.execute_reply":"2022-06-18T22:19:28.138158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt #data viz\n# v Sets matplotlib figure size defaults to 25x20\nplt.rcParams[\"figure.figsize\"] = (25,20)\n\nfig, ax = plt.subplots(#This functions lets us place many plots within a single figure\n    9, #number of rows\n    9  #number of columns\n)\n\n#adds title to figure            \nfig.text(\n    0.35, # text position along x axis\n    1, # text position along y axis\n    'EDA of Features', #title text\n    {'size': 24} #Increase font size to 35\n         )\n\ni = 0 # subplot column index\nj = 0 # subplot row index\nfor col in data.columns: #iterate thru all dataset columns\n    if col not in ['row_id']: \n        ax[j, i].hist(data[col], bins=100) #plots histogram on subplot [j, i]\n        ax[j, i].set_title(col, #adds a title to the subplot\n                           {'size': '14', 'weight': 'bold'}) \n        if i == 8: #if we reach the last column of the row, drop down a row and reset\n            i = 0\n            j += 1\n        else: #if not at the end of the row, move over a column\n            i += 1\n\nplt.rcParams.update({'axes.facecolor':'lightgreen'})\nplt.figure(facecolor='red') \nplt.show() \n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-18T22:19:28.140868Z","iopub.execute_input":"2022-06-18T22:19:28.141242Z","iopub.status.idle":"2022-06-18T22:19:51.015291Z","shell.execute_reply.started":"2022-06-18T22:19:28.14121Z","shell.execute_reply":"2022-06-18T22:19:51.014203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#E59866;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:black;\"><b>2.0 | Preprocessing</b></p>\n</div>","metadata":{}},{"cell_type":"markdown","source":"### Lets start with column F_1_0 and try to replace the NaN values. First we need to find out where are the missing values located in this column. As shown below figure, we need to split our data into two sets. \n### Training set: It consists of known values for F_1_0 column, which means all the rows with non-NaN value in F_1_0 column. \n### Test set: It consists of Unknown values for F_1_0 column, which means all the rows with  NaN value in F_1_0 column.\n### we apply the same concept to individual column and develop 80 individual models. Missing vlaue plot and correlation plots are shown below\n","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.colors\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom plotly.offline import init_notebook_mode\nimport seaborn as sns\nplt.figure(figsize=(20,8))\nsns.heatmap(data.isnull(), yticklabels=False, cbar=False, cmap='crest')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-18T22:19:51.017002Z","iopub.execute_input":"2022-06-18T22:19:51.0183Z","iopub.status.idle":"2022-06-18T22:21:07.795301Z","shell.execute_reply.started":"2022-06-18T22:19:51.018243Z","shell.execute_reply":"2022-06-18T22:21:07.794293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### As we can see above plot, there are not missing values for columns starting with F_2","metadata":{}},{"cell_type":"code","source":"## we apply the same concept to individual column and develop 80 individual models. As we have noticed earlier, dataset has four different subsets. Lets divide them.\n# this code snippet is taken from https://www.kaggle.com/code/martynovandrey/tps-jun-22-splitted-dataset-24x-faster. Consider upvoting the original author also\nfeatures = list(data.columns)\nfeatures_1, features_2, features_3, features_4 = [], [], [], []\nF = [[], [], [], [], []]\nfor feature in features:\n    for i in [1, 2, 3, 4]:\n        if feature.split('_')[1] == str(i):\n            F[i].append(feature)\ndf = [[], [], [], [], []]\n\nfig, axs = plt.subplots(nrows=4, ncols=1, figsize=(18, 30))\n\nfor i in [1, 2, 3, 4]:\n    df[i] = data[F[i]]\n    corr = df[i].corr()\n    sns.heatmap(corr, ax=axs[i-1], annot=True)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-18T22:21:07.796811Z","iopub.execute_input":"2022-06-18T22:21:07.797422Z","iopub.status.idle":"2022-06-18T22:21:24.365386Z","shell.execute_reply.started":"2022-06-18T22:21:07.797371Z","shell.execute_reply":"2022-06-18T22:21:24.364194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Columns starting with F_4 are correlated with each other, we do not see such pattern in other subsets.","metadata":{}},{"cell_type":"code","source":"# lets take look at one of the subset\ndf[4].head(2)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-18T22:21:24.366853Z","iopub.execute_input":"2022-06-18T22:21:24.367292Z","iopub.status.idle":"2022-06-18T22:21:24.395923Z","shell.execute_reply.started":"2022-06-18T22:21:24.367252Z","shell.execute_reply":"2022-06-18T22:21:24.394908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get the number of missing data points per column in this subset\nmissing_values_count = df[4].isnull().sum()\nmissing_values_count [0:30]","metadata":{"execution":{"iopub.status.busy":"2022-06-18T22:21:24.398903Z","iopub.execute_input":"2022-06-18T22:21:24.399323Z","iopub.status.idle":"2022-06-18T22:21:24.431721Z","shell.execute_reply.started":"2022-06-18T22:21:24.399291Z","shell.execute_reply":"2022-06-18T22:21:24.430773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Check the number of NaN values per sample\nn_null = pd.DataFrame(df[4].isna().sum(axis=1))\nplot = sns.histplot(data=n_null, bins=10, stat=\"percent\")\nplot.set_xlabel('The number of NaN values')\nprint(f'The max number of NaN values per sample is {n_null.max()}.')","metadata":{"execution":{"iopub.status.busy":"2022-06-18T22:38:13.730115Z","iopub.execute_input":"2022-06-18T22:38:13.730569Z","iopub.status.idle":"2022-06-18T22:38:14.405502Z","shell.execute_reply.started":"2022-06-18T22:38:13.730532Z","shell.execute_reply":"2022-06-18T22:38:14.404714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### as we can see above ~22 rows has NaN value and 4% of rows has two NaN values. We need to find some special way to manage two NaN values in the same row. @ehekatlact has noticed that number of NaN values per rows is critical. One idea is to create new column which contain the number of rows and treat it as the categorical data. We will use this information in future.","metadata":{}},{"cell_type":"markdown","source":"## Feature interaction \n### To investiage the feature interaction, simple LGBM model is selected. ","metadata":{}},{"cell_type":"code","source":"# this part of code snipped is taken from https://www.kaggle.com/code/vishalbajaj2000/santander-lightgbm-xgb-feature-interactions \n# upvote the original work also\ndef get_splits_gain(tree_num=0, parent=-1, tree=None, lev=0, node_name=None, split_gain=None, reclimit=50000):\n    '''\n    Function to recusively walk thru a single decision tree (only LIGHTGBM for now) and extract GAIN values and Feature interactions. \n    Since it uses YIELD the user of the function needs to walk through the function in a for loop to extract values. \n    ---Arguments---\n    tree_num : The number of the tree node to analyze used only in output.\n    parent : DO NOT PASS A VALUE, it used by the function for recusion to keep track of the interactions.\n    tree : A single decision tree as a DICT. Required.\n    lev : DO NOT PASS A VALUE, it used by the function for recusion to keep track of the level of the node/interaction.\n    node_name : DO NOT PASS A VALUE, it used by the function for recusion to keep track of the interactions.\n    split_gain : DO NOT PASS A VALUE, it used by the function for recusion to keep track of the gain values.\n    inter : DO NOT PASS A VALUE, it used by the function for recusion to keep track of the interactions.\n    reclimit: this sets the max recusive limit higher incase the model is very deep. USe with caution, I have no idea on how the system beaves with very large values!\n    \n    ---YIELD---\n    A single line per recursion:\n    tree_num : tree number\n    tag : 'split_feature', the tag/key for which the value is being extracted for the split.\n    old_parent : The actual parent for the column that is splitted on, for the first node of the tree it is '-1' by default.\n    parent : The child node under the old_parent. Note: for the first node the value is passed here\n    lev : The depth/Level of the node, for the first node the level is 1.\n    node_name : The node from where the info was extracted.\n    split_gain : the gain value at that level\n    '''\n    sys.setrecursionlimit(reclimit)\n    if tree == None:\n        raise Exception('No tree present to analyze!')\n    for k, v in tree.items():\n        if type(v) != dict and k in ['split_feature']:\n            old_parent = parent\n            parent = v\n            tag = k\n            yield tree_num, tag, old_parent, parent, lev, node_name, split_gain\n        elif isinstance(v, dict):\n            if v.get('split_gain') == None:\n                continue\n            else:\n                tree = v\n                lev_inc = lev + 1\n                node_name = k\n                split_gain = v['split_gain']\n                for result in get_splits_gain(tree_num, parent, tree, lev_inc, node_name, split_gain):\n                    yield result\n        else:\n            continue\n            \n#Creates a feature dictionary based on the features present in the LGBM model\ndef lgbm_create_feat_dict(model):\n    feat_dict = dict(enumerate(model['feature_names']))\n    feat_dict[-1] = 'base'\n    return feat_dict\n\ndef analyze_model(model):\n    '''\n    Take a JSON dump of LGBM model, calls the recursive function to analyse all trees in the model, interprets feature index/names and returns a dataframe with teh model analysis and a feature interactions\n    ---Arguments---\n    model :  LGBM JSON model\n    ---Returns---\n    tree_info_df : pandas DF with model summarized and feature interactions.\n    '''\n    tree_info = []\n    for j in range(0,len(model['tree_info'])):\n        for i in get_splits_gain(tree_num=j, tree=model['tree_info'][j]):\n            tree_info.append(list(i))\n    tree_info_df = pd.DataFrame(tree_info, columns=['TreeNo','Type','ParentFeature', 'SplitOnfeature','Level','TreePos','Gain'])\n    lgbm_feat_dict = lgbm_create_feat_dict(model_lgb)\n    tree_info_df['ParentFeature'].replace(lgbm_feat_dict, inplace=True)\n    tree_info_df['SplitOnfeature'].replace(lgbm_feat_dict, inplace=True)\n    tree_info_df['Interactions'] = tree_info_df['ParentFeature'].map(str) + ' - ' + tree_info_df['SplitOnfeature'].map(str)\n    return tree_info_df\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-18T22:21:24.433214Z","iopub.execute_input":"2022-06-18T22:21:24.433837Z","iopub.status.idle":"2022-06-18T22:21:24.454016Z","shell.execute_reply.started":"2022-06-18T22:21:24.433801Z","shell.execute_reply":"2022-06-18T22:21:24.453216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nfrom sklearn.model_selection import train_test_split\nTARGET=['F_4_11']  # vary the target to observe the feature interactions\nX_train, X_val, y_train, y_val = train_test_split(df[4].drop(TARGET,axis=1), df[4][TARGET], test_size=0.3, shuffle=True)\nfrom lightgbm import LGBMRegressor\nlgbm_params = {'random_state': 22,\n#           'device' : 'gpu',\n          'n_estimators': 200, # you can increase the n_splits value to >10000, to minimize the runtime I have used 200\n          'learning_rate' : 0.1,\n          'metric' : 'rmse'}\nlgbm_model = LGBMRegressor(**lgbm_params)\nlgbm_model.fit(X_train,y_train,eval_set=[(X_val,y_val),(X_train,y_train)],verbose=10000)\n#Produces a JSON model dump for LightGBM\nmodel_lgb = lgbm_model.booster_.dump_model()\n\nlgb_df = analyze_model(model_lgb)\nlgb_df= round(lgb_df, 2)\nlgb_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-18T22:21:24.455128Z","iopub.execute_input":"2022-06-18T22:21:24.45595Z","iopub.status.idle":"2022-06-18T22:21:36.316061Z","shell.execute_reply.started":"2022-06-18T22:21:24.455916Z","shell.execute_reply":"2022-06-18T22:21:36.315005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Produce some calculations for easier plotting\nlgb_inter_calc = lgb_df.groupby('Interactions')['Gain'].agg(['count','sum','min','max','mean','std']).sort_values(by='sum', ascending=False).reset_index('Interactions').fillna(0)\nlgb_inter_calc = round(lgb_inter_calc, 2) #if i dont round sns.barplot fails due to too large a precision.\n#Created 2 datasets as i see that BASE (the first node of the tree) has a very hight gains and thus dilutes the interactions\nlgb_inter_calc_nobase = lgb_inter_calc[lgb_inter_calc['Interactions'].str.contains('base')==False]\nlgb_inter_calc.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-18T22:22:39.366369Z","iopub.execute_input":"2022-06-18T22:22:39.366937Z","iopub.status.idle":"2022-06-18T22:22:39.399158Z","shell.execute_reply.started":"2022-06-18T22:22:39.366895Z","shell.execute_reply":"2022-06-18T22:22:39.398298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_data = lgb_inter_calc_nobase.sort_values('sum', ascending=False).iloc[0:75].reset_index(drop=True)\ndef plot_feat_interaction(data=None):\n    plt.figure(figsize=(20, 14))\n    ax = plt.subplot(121)\n    sns.barplot(x='sum', y='Interactions', data=data.sort_values('sum', ascending=False), ax=ax)\n    ax.set_title('Total Gain for Feature Interaction', fontweight='bold', fontsize=14)\n    # Plot Gain importances\n    ax = plt.subplot(122)\n    sns.barplot(x='count', y='Interactions', data=data.sort_values('sum', ascending=False), ax=ax)\n    ax.set_title('No. of times Feature interacted', fontweight='bold', fontsize=14)\n    plt.tight_layout()\nplot_feat_interaction(plot_data)","metadata":{"execution":{"iopub.status.busy":"2022-06-18T22:26:45.642406Z","iopub.execute_input":"2022-06-18T22:26:45.642981Z","iopub.status.idle":"2022-06-18T22:26:48.002509Z","shell.execute_reply.started":"2022-06-18T22:26:45.642908Z","shell.execute_reply":"2022-06-18T22:26:48.001386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Similar to @WTI200 a good feature interaction can be observed between different features. Need to find sophesticated approach to include this informaiton into the training purposes. For now we will ignore this information and build a model in the next section.","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#E59866;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:black;\"><b>3.0 | Modeling</b></p>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana;\">\n    ðŸ“Œ Important note: Based on the correlation plot it can be seen that subset-four (start with F_4_) has good correlation with each other. Rest of the subsets are weekly correlated.<br>\n    The strategy is use LGBM regression for subset-4 and mean regression for rest of the subsets.\n</div>","metadata":{}},{"cell_type":"code","source":"%%time\nfrom lightgbm import LGBMRegressor\nfrom sklearn.metrics import r2_score\nfor i in [4]:\n    dummy_df = pd.DataFrame()\n    dummy_df2 = pd.DataFrame()\n    col_train = pd.DataFrame()\n    col_test = pd.DataFrame()\n    dummy_df=df[i].copy()\n    dummy_df2=df[i].copy()\n    for column in dummy_df.columns: \n        print('Processing Colunm Name : ', column)\n        if dummy_df[column].isnull().sum() == 0:\n            print(dummy_df[column].isnull().sum())\n            continue    # continue as no NaN values found in this column\n        col_nan_ix = dummy_df[dummy_df[column].isnull()].index  # identify the rows which has NaN in column F_1_0\n        col_train = dummy_df.drop(col_nan_ix, axis = 0)  #training set which has F_1_0 fixed value but other columns might have NaN values\n        col_test = dummy_df[dummy_df.index.isin(col_nan_ix)] \n        X = col_train.drop([column],axis=1)\n        y = col_train[column]\n        model = LGBMRegressor(n_estimators=20000,metric='r2')\n        model.fit(X,y)\n        score=model.score(X, y)\n        print('R2 of this column : ', score)\n        dummy_df2[column][col_nan_ix] = model.predict(col_test.drop([column],axis=1))\n    df[i]=dummy_df2.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lets take a quick look at the subset-4\ndf[4].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Now we use simple meanimputer for subset-1 and subset-3. Note that subset-2 has no missing values","metadata":{}},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\nimp = SimpleImputer(\n        missing_values=np.nan,\n        strategy='mean') \nfor i in [1,3]:    \n    df[i][:] = imp.fit_transform(df[i])","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Merged_Subsets = pd.concat([df[1], df[2], df[3], df[4]], axis=1)\nsubmission = pd.read_csv('../input/tabular-playground-series-jun-2022/sample_submission.csv', index_col='row-col')\nfor i in tqdm(submission.index):\n    row = int(i.split('-')[0])\n    col = i.split('-')[1]\n    submission.loc[i, 'value'] = Merged_Subsets.loc[row, col]\n\nsubmission.to_csv('submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## The LB is 0.87724, top 2% as of now.","metadata":{}},{"cell_type":"markdown","source":"<img src=\"https://31.media.tumblr.com/tumblr_lptgqw2SE81qj2mh7o1_500.gif\">","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;\n            background-color:#E59866;font-size:150%;\n            font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 8px;color:black;\"><b>3.0 | References</b></p>\n</div>","metadata":{}},{"cell_type":"markdown","source":"https://towardsdatascience.com/using-the-missingno-python-library-to-identify-and-visualise-missing-data-prior-to-machine-learning-34c8c5b5f009 <br>\nhttps://towardsdatascience.com/how-to-handle-missing-data-8646b18db0d4 <br>\nhttps://www.kaggle.com/code/parulpandey/a-guide-to-handling-missing-values-in-python/notebook <br>\nhttps://www.kaggle.com/code/residentmario/using-missingno-to-diagnose-data-sparsity/notebook <br>\nhttps://www.analyticsvidhya.com/blog/2021/05/dealing-with-missing-values-in-python-a-complete-guide/ <br>\nhttps://www.kaggle.com/code/calebreigada/getting-started-eda-preprocessing <br>\nhttps://medium.com/swlh/impute-missing-values-the-right-way-c63735fccccd <br>\n\n","metadata":{}}]}