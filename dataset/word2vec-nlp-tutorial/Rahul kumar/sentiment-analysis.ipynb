{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"pip install bs4  # Installing bs4 package to use BeautifulSoup","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Importing important libraries and functions\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom bs4 import BeautifulSoup\nimport re\nimport nltk\nfrom nltk.corpus import stopwords","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Uploading training data, data to be predicted and sample submission file\ntrain_data = pd.read_csv('../input/word2vec-nlp-tutorial/labeledTrainData.tsv.zip',  delimiter=\"\\t\", quoting=3)\ntest_data = pd.read_csv('../input/word2vec-nlp-tutorial/testData.tsv.zip', delimiter = \"\\t\", quoting= 3 )\nsubmission1 = pd.read_csv(\"../input/word2vec-nlp-tutorial/sampleSubmission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analyzing the training data\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cheking for any missing values in training data\ntrain_data.isnull().sum().sum()     # Zero missing values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Since we have three columns in which the sentiment column is target variable and id is not required for training so we will pre process the training data containing reviews only\n# Each review is in the form of a paragraph with HTML tags, punctuations, numbers and stopwords such as is, are am etc so we need to clean them \n# For cleaning below is the function which uses beautifulsoup, regular expression and natural language toolkit to achieve the above \n\n# creating train_y as target variable in the form of an array\ntrain_y = np.array(train_data[\"sentiment\"])\n\n# To eliminate stop word we need to download its vacab from nltk\nnltk.download('stopwords')\n\ndef process(review):\n   # review without HTML tags\n   review = BeautifulSoup(review).get_text()\n   # review without punctuation and numbers\n   review = re.sub(\"[^a-zA-Z]\",' ',review)\n   # converting into lowercase and splitting to eliminate stopwords\n   review = review.lower()\n   review = review.split()\n   # review without stopwords\n   swords = set(stopwords.words(\"english\"))                      # conversion into set for fast searching\n   review = [w for w in review if w not in swords]               \n   # joining of splitted paragraph by spaces and return\n   return(\" \".join(review))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# processing the training data with the help of function defined above\n\ntrain_x = []\nfor r in range(len(train_data[\"review\"])):\n  if (r+1)%1000 == 0:\n    print(\"No of reviews processed =\", r+1)\n  train_x.append(process(train_data[\"review\"][r]))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now we have our processed and cleaned training set but it is in the form of text but to train we need to convert it into numerical data\n# For that we will use bag of words which is based on the frequency that each word occur in a review within training data and we will go for 5000 most common words\n\n# Initializing the countvectorizer which is a sklearn tool for bag of words\nvectorizer = CountVectorizer( max_features = 5000 )\n# Now we will use fit_transform which fits the model to learn vocabulary for 5000 most common words and then transform the training data into feature vectors\ntrain_x = vectorizer.fit_transform(train_x)\n# conversion into array\ntrain_x = train_x.toarray()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Final train_x and train_y\ntrain_x.shape, train_y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analyzing data to be predicted\ntest_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking for any missing values\ntest_data.isnull().sum().sum()   # No missing values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# processing the data to be predicted with the help of function defined above\n\ntest = []\nfor r in range(len(test_data[\"review\"])):\n  if (r+1)%1000 == 0:\n    print(\"No of reviews processed =\", r+1)\n  test.append(process(test_data[\"review\"][r]))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting the text data in data to be predicted into bag of words feature vectors\ntest = vectorizer.transform(test)\n# conversion into array\ntest = test.toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Final data to be predicted\ntest.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a model, fitting and seeing the performance on training data\n\nmodel = RandomForestClassifier(n_estimators = 100)\nmodel.fit(train_x, train_y)\ntrain_predict = model.predict(train_x)\nAUC = roc_auc_score(train_y, train_predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# printing AUC\nAUC   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prediction on data to be predicted\nsubmission2 = model.predict(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Conversion of predicted arry to dataframe and combining with sample submission\nsubmission2 = pd.DataFrame(submission2)\nsubmission = pd.concat([submission1, submission2], axis =1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing the sentiment column from sample submission and renaming the predicted column from 0 to \"sentiment\"\nsubmission.drop(columns=[\"sentiment\"], inplace=True)\nsubmission = submission.rename(columns = {0 :\"sentiment\"})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Final submission file\nsubmission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# saving the final dubmission file in csv format\nsubmission.to_csv(\"Submission.csv\", index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}