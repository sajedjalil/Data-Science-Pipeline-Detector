{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\n\nDIR='/kaggle/input/word2vec-nlp-tutorial/'\n# Read data from files \ntrain = pd.read_csv( DIR+\"labeledTrainData.tsv\", header=0, \n delimiter=\"\\t\", quoting=3 )\n\ntest = pd.read_csv( DIR+\"testData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\nunlabeled_train = pd.read_csv( DIR+\"unlabeledTrainData.tsv\", header=0, \n delimiter=\"\\t\", quoting=3 )\n\n# Verify the number of reviews that were read (100,000 in total)\nprint( \"Read %d labeled train reviews, %d labeled test reviews, \" \\\n\"and %d unlabeled reviews\\n\" % (train[\"review\"].size,  \n test[\"review\"].size, unlabeled_train[\"review\"].size ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import various modules for string cleaning\nfrom bs4 import BeautifulSoup\nimport re\nfrom nltk.corpus import stopwords\n\ndef review_to_wordlist( review, remove_stopwords=False ):\n    # Function to convert a document to a sequence of words,\n    # optionally removing stop words.  Returns a list of words.\n    \n    # 1. Remove HTML\n    review_text = BeautifulSoup(review).get_text()\n    \n    # 2. Remove non-letters\n    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n    \n    # 3. Convert words to lower case and split them\n    words = review_text.lower().split()\n    \n    # 4. Optionally remove stop words (false by default)\n    if remove_stopwords:\n        stops = set(stopwords.words(\"english\"))\n        words = [w for w in words if not w in stops]\n    \n    # 5. Return a list of words\n    return(words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Download the punkt tokenizer for sentence splitting\nimport nltk.data\nnltk.download('punkt') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the punkt tokenizer\ntokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n\n# Define a function to split a review into parsed sentences\ndef review_to_sentences( review, tokenizer, remove_stopwords=False ):\n    # Function to split a review into parsed sentences. Returns a list of sentences, where each sentence is a list of words\n    \n    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n    raw_sentences = tokenizer.tokenize(review.strip())\n    \n    # 2. Loop over each sentence\n    sentences = []\n    for raw_sentence in raw_sentences:\n        # If a sentence is empty, skip it\n        if len(raw_sentence) > 0:\n            # Otherwise, call review_to_wordlist to get a list of words\n            sentences.append( review_to_wordlist( raw_sentence,remove_stopwords ))\n    \n    # Return the list of sentences (each sentence is a list of words,so this returns a list of lists\n    return sentences","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences = []  # Initialize an empty list of sentences\n\nprint (\"Parsing sentences from training set\")\nfor review in train[\"review\"]:\n    sentences += review_to_sentences(review, tokenizer)\n\nprint (\"Parsing sentences from unlabeled set\")\nfor review in unlabeled_train[\"review\"]:\n    sentences += review_to_sentences(review, tokenizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import the built-in logging module and configure it so that Word2Vec \n# creates nice output messages\nimport logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n    level=logging.INFO)\n\n# Set values for various parameters\nnum_features = 300    # Word vector dimensionality                      \nmin_word_count = 40   # Minimum word count                        \nnum_workers = 4       # Number of threads to run in parallel\ncontext = 10          # Context window size                                                                                    \ndownsampling = 1e-3   # Downsample setting for frequent words\n\n# Initialize and train the model (this will take some time)\nfrom gensim.models import word2vec\nprint (\"Training model...\")\nmodel = word2vec.Word2Vec(sentences, workers=num_workers, \\\n            size=num_features, min_count = min_word_count, \\\n            window = context, sample = downsampling)\n\n# If you don't plan to train the model any further, calling \n# init_sims will make the model much more memory-efficient.\nmodel.init_sims(replace=True)\n\n# It can be helpful to create a meaningful model name and \n# save the model for later use. You can load it later using Word2Vec.load()\nmodel_name = \"300features_40minwords_10context\"\nmodel.save(model_name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.doesnt_match(\"man woman child kitchen\".split())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.most_similar(\"man\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" model.most_similar(\"queen\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.wv.syn0.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model[\"flower\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np  # Make sure that numpy is imported\n\ndef makeFeatureVec(words, model, num_features):\n    # Function to average all of the word vectors in a given\n    # paragraph\n    #\n    # Pre-initialize an empty numpy array (for speed)\n    featureVec = np.zeros((num_features,),dtype=\"float32\")\n    #\n    nwords = 0.\n    # \n    # Index2word is a list that contains the names of the words in \n    # the model's vocabulary. Convert it to a set, for speed \n    index2word_set = set(model.wv.index2word)\n    #\n    # Loop over each word in the review and, if it is in the model's\n    # vocaublary, add its feature vector to the total\n    for word in words:\n        if word in index2word_set: \n            nwords = nwords + 1.\n            featureVec = np.add(featureVec,model[word])\n    # \n    # Divide the result by the number of words to get the average\n    featureVec = np.divide(featureVec,nwords)\n    return featureVec\n\n\ndef getAvgFeatureVecs(reviews, model, num_features):\n    # Given a set of reviews (each one a list of words), calculate \n    # the average feature vector for each one and return a 2D numpy array \n    # \n    # Initialize a counter\n    counter = 0\n    # \n    # Preallocate a 2D numpy array, for speed\n    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n    # \n    # Loop through the reviews\n    for review in reviews:\n       #\n       # Print a status message every 1000th review\n        if counter%1000 == 0:\n            print (\"Review %d of %d\" % (counter, len(reviews)))\n        \n       # Call the function (defined above) that makes average feature vectors\n        reviewFeatureVecs[counter] = makeFeatureVec(review, model,num_features)\n       #\n       # Increment the counter\n        counter = counter + 1\n    return reviewFeatureVecs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ****************************************************************\n# Calculate average feature vectors for training and testing sets,\n# using the functions we defined above. Notice that we now use stop word\n# removal.\n\nclean_train_reviews = []\nfor review in train[\"review\"]:\n    clean_train_reviews.append( review_to_wordlist( review,remove_stopwords=True ))\n\ntrainDataVecs = getAvgFeatureVecs( clean_train_reviews, model, num_features )\n\nprint(\"Creating average feature vecs for test reviews\")\n\nclean_test_reviews = []\nfor review in test[\"review\"]:\n    clean_test_reviews.append( review_to_wordlist( review, \\\n        remove_stopwords=True ))\n\ntestDataVecs = getAvgFeatureVecs( clean_test_reviews, model, num_features )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit a random forest to the training data, using 100 trees\nfrom sklearn.ensemble import RandomForestClassifier\nforest = RandomForestClassifier( n_estimators = 100 )\n\nprint (\"Fitting a random forest to labeled training data...\")\nforest = forest.fit( trainDataVecs, train[\"sentiment\"] )\n\n# Test & extract results \nresult = forest.predict( testDataVecs )\n\n# Write the test results \noutput = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\noutput.to_csv( \"Word2Vec_AverageVectors.csv\", index=False, quoting=3 )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From Words to Paragraphs, Attempt 2: Clustering "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\nimport time\n\nstart = time.time() # Start time\n\n# Set \"k\" (num_clusters) to be 1/5th of the vocabulary size, or an\n# average of 5 words per cluster\nword_vectors = model.wv.syn0\nnum_clusters = int(word_vectors.shape[0] / 5)\n\n# Initalize a k-means object and use it to extract centroids\nkmeans_clustering = KMeans( n_clusters = num_clusters )\nidx = kmeans_clustering.fit_predict( word_vectors )\n\n# Get the end time and print how long the process took\nend = time.time()\nelapsed = end - start\nprint (\"Time taken for K Means clustering: \", elapsed, \"seconds.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a Word / Index dictionary, mapping each vocabulary word to\n# a cluster number                                                                                            \nword_centroid_map = dict(zip( model.wv.index2word, idx ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For the first 10 clusters\nfor cluster in range(0,10):\n    #\n    # Print the cluster number  \n    print( \"\\nCluster %d\" % cluster)\n    #\n    # Find all of the words for that cluster number, and print them out\n    words = []\n    for i in range(0,len(word_centroid_map.values())):\n        val=list(word_centroid_map.values())\n        #print(val)\n        if( val == cluster ):\n            words.append(word_centroid_map.keys()[i])\n    print(words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_bag_of_centroids( wordlist, word_centroid_map ):\n    #\n    # The number of clusters is equal to the highest cluster index\n    # in the word / centroid map\n    num_centroids = max( word_centroid_map.values() ) + 1\n    #\n    # Pre-allocate the bag of centroids vector (for speed)\n    bag_of_centroids = np.zeros( num_centroids, dtype=\"float32\" )\n    #\n    # Loop over the words in the review. If the word is in the vocabulary,\n    # find which cluster it belongs to, and increment that cluster count \n    # by one\n    for word in wordlist:\n        if word in word_centroid_map:\n            index = word_centroid_map[word]\n            bag_of_centroids[index] += 1\n    #\n    # Return the \"bag of centroids\"\n    return bag_of_centroids","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pre-allocate an array for the training set bags of centroids (for speed)\ntrain_centroids = np.zeros( (train[\"review\"].size, num_clusters),dtype=\"float32\" )\n\n# Transform the training set reviews into bags of centroids\ncounter = 0\nfor review in clean_train_reviews:\n    train_centroids[counter] = create_bag_of_centroids( review,word_centroid_map )\n    counter += 1\n\n# Repeat for test reviews \ntest_centroids = np.zeros(( test[\"review\"].size, num_clusters),dtype=\"float32\" )\n\ncounter = 0\nfor review in clean_test_reviews:\n    test_centroids[counter] = create_bag_of_centroids( review,word_centroid_map )\n    counter += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit a random forest and extract predictions \nforest = RandomForestClassifier(n_estimators = 100)\n\n# Fitting the forest may take a few minutes\nprint (\"Fitting a random forest to labeled training data...\")\nforest = forest.fit(train_centroids,train[\"sentiment\"])\nresult = forest.predict(test_centroids)\n\n# Write the test results \noutput = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":result})\noutput.to_csv( \"BagOfCentroids.csv\", index=False, quoting=3 )","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}