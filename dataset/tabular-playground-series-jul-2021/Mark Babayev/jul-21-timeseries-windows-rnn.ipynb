{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport math\nimport datetime\nimport numpy as np \nimport pandas as pd\nimport scipy as sc\nimport matplotlib.pyplot as plt\nimport numexpr\nfrom PIL import Image\nimport seaborn as sns\n\nRANDOM_SEED = 111\n\nnp.random.seed(RANDOM_SEED)\n\nfrom numpy.random import default_rng\nrng = default_rng(RANDOM_SEED)\n\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score, accuracy_score, mean_squared_log_error\nfrom sklearn.preprocessing import OrdinalEncoder, MinMaxScaler, StandardScaler, OneHotEncoder, Binarizer, KBinsDiscretizer, QuantileTransformer, PolynomialFeatures\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV, KFold, StratifiedKFold, StratifiedShuffleSplit, ShuffleSplit\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn import set_config\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom tensorflow.python.keras.losses import mean_squared_logarithmic_error\n\ntf.random.set_seed(RANDOM_SEED)\n\nINPUT_DIR = '/kaggle/input/tabular-playground-series-jul-2021'\nBATCH_SIZE = 1024","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-09T18:21:37.465109Z","iopub.execute_input":"2021-08-09T18:21:37.465895Z","iopub.status.idle":"2021-08-09T18:21:44.879706Z","shell.execute_reply.started":"2021-08-09T18:21:37.465787Z","shell.execute_reply":"2021-08-09T18:21:44.878486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Upload dataset","metadata":{}},{"cell_type":"code","source":"def season(month):\n  if (month == 12 or month == 1 or month == 2):   #winter\n        return 0        \n  elif(month == 3 or month == 4 or month == 5):   #spring\n        return 1       \n  elif(month == 6 or month == 7 or month == 8):   #summer\n        return 2       \n  else:                                           #outemn\n        return 3 \n\ndef daytime(hour):\n  if (hour > 5 and hour < 17):      #light\n    return 0\n  else:                             #darkness\n    return 1\n\ntrain_df = pd.read_csv(os.path.join(INPUT_DIR, 'train.csv'), index_col='date_time')\ntest_df = pd.read_csv(os.path.join(INPUT_DIR,'test.csv'), index_col='date_time')\n\ntrain_df.index = pd.to_datetime(train_df.index)\ntest_df.index = pd.to_datetime(test_df.index)\n\nlabels = train_df[['target_carbon_monoxide','target_benzene','target_nitrogen_oxides']]\n\ntrain_df.drop(labels.columns, axis=1, inplace=True)\ntotal_df = train_df.append(test_df)     #pd.concat()\n\ntotal_df['dew_point'] = total_df['deg_C'].apply(lambda x: (17.27 * x) / (237.7 + x)) + total_df['absolute_humidity'].apply(lambda x: math.log (x))\ntotal_df['partial_pressure'] = (total_df['deg_C'].apply(lambda x: (237.7 + x) * 286.8) * total_df['absolute_humidity']) / 100000\ntotal_df['saturated_wvd'] = (total_df['absolute_humidity'] * 100) / total_df['relative_humidity']\n\ntotal_df['dt_hour'] = [x.hour for x in total_df.index]\ntotal_df['dt_weekday'] = [x.weekday() for x in total_df.index]\ntotal_df['dt_month'] = [x.month for x in total_df.index]\ntotal_df['dt_season'] = [season(x.month) for x in total_df.index]\ntotal_df['dt_lights'] = [daytime(x.hour) for x in total_df.index]\ntotal_df['dt_month_s'] = np.sin(np.pi * (total_df['dt_month']-1)/6)\ntotal_df['dt_month_c'] = np.cos(np.pi * (total_df['dt_month']-1)/6)\n\ntotal_df['dt_month_s'] = total_df['dt_month_s'].astype('category').cat.codes\ntotal_df['dt_month_c'] = total_df['dt_month_c'].astype('category').cat.codes\n\ntotal_df[\"dt_working_hours\"] = total_df[\"dt_hour\"].isin(np.arange(8, 21, 1)).astype(\"int\")\ntotal_df[\"dt_weekend\"] = (total_df[\"dt_weekday\"] >= 5).astype(\"int\")\n\nsensors = ['sensor_1', 'sensor_2', 'sensor_3', 'sensor_4', 'sensor_5']\n#total_df[[x+'_1h' for x in sensors]] = total_df[sensors].diff(1).fillna(0)\n\ncat_cols = np.array([col for col in total_df.columns if 'dt_' in col])\nnum_cols = np.array([col for col in total_df.columns if not 'dt_' in col])\ntotal_cols = np.concatenate([num_cols,cat_cols])\ncat_cols_idx = [np.where(total_df.columns == x)[0][0] for x in cat_cols]","metadata":{"execution":{"iopub.status.busy":"2021-08-09T18:21:44.881194Z","iopub.execute_input":"2021-08-09T18:21:44.8815Z","iopub.status.idle":"2021-08-09T18:21:45.28027Z","shell.execute_reply.started":"2021-08-09T18:21:44.881473Z","shell.execute_reply":"2021-08-09T18:21:45.279224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Analyse features","metadata":{}},{"cell_type":"code","source":"pd.concat((total_df.min(), total_df.max(), total_df.mean(), total_df.std(), total_df.nunique()), axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T18:21:45.282401Z","iopub.execute_input":"2021-08-09T18:21:45.282726Z","iopub.status.idle":"2021-08-09T18:21:45.335727Z","shell.execute_reply.started":"2021-08-09T18:21:45.282696Z","shell.execute_reply":"2021-08-09T18:21:45.334442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.clustermap(total_df.corr(), annot=True, square=True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T18:21:45.337802Z","iopub.execute_input":"2021-08-09T18:21:45.338257Z","iopub.status.idle":"2021-08-09T18:21:47.884903Z","shell.execute_reply.started":"2021-08-09T18:21:45.338214Z","shell.execute_reply":"2021-08-09T18:21:47.883791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All input features, expesially temperature (`deg_C`) are seasonal, so we can use here windowed sequences with RNN models. ","metadata":{}},{"cell_type":"code","source":"total_df[num_cols].plot(subplots=True, layout=(3,4), figsize=(20,5))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T18:21:47.886293Z","iopub.execute_input":"2021-08-09T18:21:47.886607Z","iopub.status.idle":"2021-08-09T18:21:50.511759Z","shell.execute_reply.started":"2021-08-09T18:21:47.886576Z","shell.execute_reply":"2021-08-09T18:21:50.510476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_df[num_cols].pct_change().plot(subplots=True, layout=(3,4), figsize=(20,5))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T18:21:50.513412Z","iopub.execute_input":"2021-08-09T18:21:50.513805Z","iopub.status.idle":"2021-08-09T18:21:53.288725Z","shell.execute_reply.started":"2021-08-09T18:21:50.513765Z","shell.execute_reply":"2021-08-09T18:21:53.287745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"NOTE: There is a temperature anomaly at \"2011-02-06\". The very low morning temperature and a shart jump to the normal midday temperature.<br/>\nI tried to use feature value returns instead of the absolute values but received worse results.","metadata":{}},{"cell_type":"code","source":"total_df.loc['2011-02-06','deg_C'].pct_change().plot();","metadata":{"execution":{"iopub.status.busy":"2021-08-09T18:21:53.28982Z","iopub.execute_input":"2021-08-09T18:21:53.290099Z","iopub.status.idle":"2021-08-09T18:21:53.492881Z","shell.execute_reply.started":"2021-08-09T18:21:53.290072Z","shell.execute_reply":"2021-08-09T18:21:53.492168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_df.loc['2011-02-06T08:00':'2011-02-06T14:00','deg_C']","metadata":{"execution":{"iopub.status.busy":"2021-08-09T18:21:53.495865Z","iopub.execute_input":"2021-08-09T18:21:53.496483Z","iopub.status.idle":"2021-08-09T18:21:53.508986Z","shell.execute_reply.started":"2021-08-09T18:21:53.496431Z","shell.execute_reply":"2021-08-09T18:21:53.507814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_df.loc['2011-02-05T08:00':'2011-02-05T14:00','deg_C']","metadata":{"execution":{"iopub.status.busy":"2021-08-09T18:21:53.510687Z","iopub.execute_input":"2021-08-09T18:21:53.511285Z","iopub.status.idle":"2021-08-09T18:21:53.531245Z","shell.execute_reply.started":"2021-08-09T18:21:53.511247Z","shell.execute_reply":"2021-08-09T18:21:53.530371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Correlation between seasonalities","metadata":{}},{"cell_type":"code","source":"seasonality_dict = {ts: seasonal_decompose(total_df[ts], period=255).seasonal for ts in num_cols}\nseasonality_corr = pd.DataFrame(seasonality_dict).corr()\n\nsns.clustermap(seasonality_corr, annot=True)\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-08-09T18:21:53.532313Z","iopub.execute_input":"2021-08-09T18:21:53.532732Z","iopub.status.idle":"2021-08-09T18:21:54.784964Z","shell.execute_reply.started":"2021-08-09T18:21:53.532701Z","shell.execute_reply":"2021-08-09T18:21:54.783938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, in both cases the autocorrelation is highest in a period of 24h","metadata":{}},{"cell_type":"code","source":"from statsmodels.graphics import tsaplots\ntsaplots.plot_acf(total_df[\"deg_C\"], lags=30, title='deg_C')\ntsaplots.plot_acf(labels[labels.columns[0]], lags=30, title=labels.columns[0])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T18:21:54.786479Z","iopub.execute_input":"2021-08-09T18:21:54.787107Z","iopub.status.idle":"2021-08-09T18:21:55.359568Z","shell.execute_reply.started":"2021-08-09T18:21:54.787059Z","shell.execute_reply":"2021-08-09T18:21:55.358594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The rolling STD for 24h windows is not constant, so it is not a \"white noise\".","metadata":{}},{"cell_type":"code","source":"total_df[\"deg_C\"].rolling(24).std().std()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T18:21:55.360839Z","iopub.execute_input":"2021-08-09T18:21:55.361149Z","iopub.status.idle":"2021-08-09T18:21:55.368643Z","shell.execute_reply.started":"2021-08-09T18:21:55.361107Z","shell.execute_reply":"2021-08-09T18:21:55.367581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"...and it is not a \"random walk\" - p<0.05","metadata":{}},{"cell_type":"code","source":"from statsmodels.tsa.stattools import adfuller\n\nresults = adfuller(labels[labels.columns[0]])\n\nprint(f\"ADF Statistic: {results[0]}\")\nprint(f\"p-value: {results[1]}\")\nprint(\"Critical Values:\")\nfor key, value in results[4].items():\n    print(\"\\t%s: %.3f\" % (key, value))","metadata":{"execution":{"iopub.status.busy":"2021-08-09T18:21:55.370037Z","iopub.execute_input":"2021-08-09T18:21:55.370366Z","iopub.status.idle":"2021-08-09T18:21:55.870999Z","shell.execute_reply.started":"2021-08-09T18:21:55.370335Z","shell.execute_reply":"2021-08-09T18:21:55.869904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.concat((labels.min(), labels.max(), labels.mean(), labels.nunique()), axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T18:21:55.872585Z","iopub.execute_input":"2021-08-09T18:21:55.873239Z","iopub.status.idle":"2021-08-09T18:21:55.902261Z","shell.execute_reply.started":"2021-08-09T18:21:55.873191Z","shell.execute_reply":"2021-08-09T18:21:55.901119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Target labels are highly correlated.","metadata":{}},{"cell_type":"code","source":"sns.clustermap(labels.corr(), annot=True, figsize=(5,5))\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-08-09T18:21:55.904088Z","iopub.execute_input":"2021-08-09T18:21:55.904936Z","iopub.status.idle":"2021-08-09T18:21:56.291673Z","shell.execute_reply.started":"2021-08-09T18:21:55.904884Z","shell.execute_reply":"2021-08-09T18:21:56.290808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will split the dataset into the 24h windows","metadata":{}},{"cell_type":"code","source":"fft = tf.signal.rfft(total_df['deg_C'])\nf_per_dataset = np.arange(0, len(fft))\n\nn_samples_h = len(total_df['deg_C'])\nhours_per_year = 24*365.2524\nyears_per_dataset = n_samples_h/(hours_per_year)\n\nf_per_year = f_per_dataset/years_per_dataset\nplt.step(f_per_year, np.abs(fft))\nplt.xscale('log')\nplt.ylim(0, 50000)\nplt.xlim([0.1, max(plt.xlim())])\nplt.xticks([1, 365.2524], labels=['1/Year', '1/day'])\n_ = plt.xlabel('Frequency (log scale)')","metadata":{"execution":{"iopub.status.busy":"2021-08-09T18:21:56.292858Z","iopub.execute_input":"2021-08-09T18:21:56.293308Z","iopub.status.idle":"2021-08-09T18:21:56.731499Z","shell.execute_reply.started":"2021-08-09T18:21:56.293264Z","shell.execute_reply":"2021-08-09T18:21:56.730778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocess features","metadata":{}},{"cell_type":"code","source":"pipe_pre = ColumnTransformer([\n  #('poly', PolynomialFeatures(interaction_only=True, include_bias=False), sensors),\n  ('num', Pipeline([\n      ('sensors', ColumnTransformer([\n        ('poly', PolynomialFeatures(interaction_only=True, include_bias=False), sensors)\n      ], remainder='passthrough')),\n      #('scale', StandardScaler()),\n      #('gauss', QuantileTransformer(output_distribution=\"normal\")),\n      #('minmax', MinMaxScaler()),\n      ('kbins', KBinsDiscretizer(n_bins=32, encode='ordinal')),  #strategy='uniform'\n      #('onehot', OneHotEncoder(sparse=False))\n  ]), num_cols),\n  #('cat', OrdinalEncoder(), cat_cols)\n  ('cat', OneHotEncoder(sparse=False), cat_cols)\n  #('cat', MyVectorizer(cols=cat_cols, hashing=16), cat_cols)\n], remainder='passthrough')\n\npipe_pre.fit(total_df)\ntotal_data = pipe_pre.transform(total_df).astype('float')\n\ntrain_data, test_data = total_data[:train_df.index.shape[0]], total_data[train_df.index.shape[0]:]","metadata":{"execution":{"iopub.status.busy":"2021-08-09T18:21:56.73277Z","iopub.execute_input":"2021-08-09T18:21:56.733345Z","iopub.status.idle":"2021-08-09T18:21:56.838637Z","shell.execute_reply.started":"2021-08-09T18:21:56.733303Z","shell.execute_reply":"2021-08-09T18:21:56.837553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Example of how the windowing is working.<br/>\nThere ar emany methods that do windowing. Each of them has pros and cons. \n\n`test_data = skimage.util.view_as_windows(test_data, (16, 2000)).reshape((-1, 16, 2000, 1))`\n\n`tf.keras.preprocessing.timeseries_dataset_from_array(train_data, exp_labels[1], sequence_length=window_size, batch_size=BATCH_SIZE)`\n\n`tf.keras.preprocessing.sequence.TimeseriesGenerator`\n\n```exp_test_data = [np.roll(test_data, -i, axis=0) for i in range(window_size)]\nexp_test_data = np.moveaxis(np.stack(exp_test_data),0,-1)```\n  \nWe will use the simplest:","metadata":{}},{"cell_type":"code","source":"wsize = 3\ndata = [1,2,3,4,5,6,7,8,9,10,11,12]\ntarget = [-1,-2,-3,-4,-5,-6,-7,-8,-9,-10,-11,-12]\n\nexp_data = [data[i:-wsize+i+1] for i in range(wsize-1)]\nexp_data.append(data[wsize-1:])\nexp_data = np.dstack(exp_data)[0]\n\nexp_target = target[wsize-1:]\n\nprint(exp_data)\nprint(exp_target)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T18:21:56.840055Z","iopub.execute_input":"2021-08-09T18:21:56.840526Z","iopub.status.idle":"2021-08-09T18:21:56.850039Z","shell.execute_reply.started":"2021-08-09T18:21:56.840479Z","shell.execute_reply":"2021-08-09T18:21:56.848741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# after reducing LR continue from the largest score weights\nclass CustomReduceLROnPlateau(tf.keras.callbacks.ReduceLROnPlateau):\n  def on_epoch_end(self, epoch, logs=None):\n    current = (logs or {}).get(self.monitor)\n    if not self.monitor_op(current, self.best) and not self.in_cooldown():\n      if self.wait+1 >= self.patience:\n        self.model.load_weights(\"filepath.h5\")\n    \n    super().on_epoch_end(epoch, logs)\n\n\ndef create_windows(data, window_size):\n  exp_data = [data[i:-window_size+i+1] for i in range(window_size-1)]\n  exp_data.append(data[window_size-1:])\n  exp_data = np.moveaxis(np.dstack(exp_data),1,2)\n  return exp_data\n\n\ndef append_label(data, target):\n  target = np.moveaxis(np.expand_dims([target for x in range(264)], axis=0), 2,0)\n  data = np.append(data, target, axis=1)\n  return data\n\n\ndef RMSLE(y_true, y_pred):\n  return tf.keras.backend.sqrt(tf.keras.backend.mean(tf.keras.backend.square(tf.keras.backend.log(1+y_pred) - tf.keras.backend.log(1+y_true))))\n\n\ndef train_model(train_tensor, test_tensor, model):\n  tf.keras.backend.clear_session()\n\n  early_stop  = tf.keras.callbacks.EarlyStopping(patience=3, monitor='val_loss', mode='min', min_delta=0.00001)\n  check_point = tf.keras.callbacks.ModelCheckpoint(filepath='filepath.h5', monitor='val_loss', mode='min', save_best_only=True, save_weights_only=True)\n\n  model.compile(loss=RMSLE, optimizer=tfa.optimizers.SWA(tf.keras.optimizers.Adam(0.01)))\n  model.fit(train_tensor, validation_data=train_tensor, epochs=10, callbacks=[early_stop, check_point], verbose=1)\n  model.load_weights('filepath.h5')\n    \n  score = model.evaluate(train_tensor)\n  test_predict = model.predict(test_tensor)\n    \n  return score, test_predict\n\n\nwindow_size = 24\nBATCH_SIZE = 1024\ndims = train_data.shape[1]","metadata":{"execution":{"iopub.status.busy":"2021-08-09T18:36:10.686389Z","iopub.execute_input":"2021-08-09T18:36:10.68675Z","iopub.status.idle":"2021-08-09T18:36:10.703836Z","shell.execute_reply.started":"2021-08-09T18:36:10.686718Z","shell.execute_reply":"2021-08-09T18:36:10.702531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build 2D model\n\nThis is the simplest, flat DNN model that doesn't use windows.<br/>\nAll models are split to 3 identical branches that return 1 target label each (corresponds to the sklearn's `MultiOutputRegressor`) because I recieved worse results when was training with 1 shared branch:<br/>\n`tf.keras.layers.Dense(3, activation=\"sigmoid\")`<br/>\nThe activation function is always `relu` because when I was training all models with `sigmoid` and `softmax` they returned the worse results.<br/>\nThe 2D model returned the worst result.","metadata":{}},{"cell_type":"code","source":"img_array = np.array(Image.open('../input/jul21plot/model_2d.png'))\nplt.figure(figsize = (20,10))\nplt.imshow(img_array)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T18:56:28.970781Z","iopub.execute_input":"2021-08-09T18:56:28.971287Z","iopub.status.idle":"2021-08-09T18:56:29.419046Z","shell.execute_reply.started":"2021-08-09T18:56:28.971236Z","shell.execute_reply":"2021-08-09T18:56:29.417787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_model_2d(dims):\n  inp = tf.keras.layers.Input(shape=(dims,))\n\n  branch = [None] * 3\n  for i in range(3):\n    branch[i] = tf.keras.layers.Dense(dims, activation=\"relu\")(inp)\n    branch[i] = tf.keras.layers.Dense(dims//2, activation=\"relu\")(branch[i])\n    branch[i] = tf.keras.layers.Dense(dims//4, activation=\"relu\")(branch[i])\n    branch[i] = tf.keras.layers.Dense(1, activation=\"relu\")(branch[i])\n\n  y = tf.keras.layers.Concatenate()(branch)\n  model = tf.keras.Model(inputs=inp, outputs=y)\n  #tf.keras.utils.plot_model(model, show_shapes=True, rankdir=\"TB\")\n  #print(model.summary())\n  return model\n\n\ntrain_tensor = tf.data.Dataset.from_tensor_slices((train_data, labels)).batch(BATCH_SIZE, drop_remainder=True).cache()\ntest_tensor = tf.data.Dataset.from_tensor_slices(test_data).batch(BATCH_SIZE).cache()\n\nmodel = create_model_2d(dims)\nscore, test_predict = train_model(train_tensor, test_tensor, model)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T18:21:57.296538Z","iopub.execute_input":"2021-08-09T18:21:57.296874Z","iopub.status.idle":"2021-08-09T18:22:04.886042Z","shell.execute_reply.started":"2021-08-09T18:21:57.296842Z","shell.execute_reply":"2021-08-09T18:22:04.884898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build 3D model\n\nThis is the RNN model that uses 24h windows for input features and one-step-prediction for target labels.<br/>\nI tried other window sizes, like: 8h, 12h, 7d, 11d, 30d; and they all returned worse results.<br/>\nThis model returned the best local result of `0.1564716398715973` (`0.2` in Private Score) after about the 150 steps.","metadata":{}},{"cell_type":"code","source":"img_array = np.array(Image.open('../input/jul21plot/model_3d.png'))\nplt.figure(figsize = (20,10))\nplt.imshow(img_array)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T18:56:34.78051Z","iopub.execute_input":"2021-08-09T18:56:34.781071Z","iopub.status.idle":"2021-08-09T18:56:35.35591Z","shell.execute_reply.started":"2021-08-09T18:56:34.78102Z","shell.execute_reply":"2021-08-09T18:56:35.355203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def simple_model_3d(dims, wsize):\n  inp = tf.keras.layers.Input(shape=(wsize, dims))\n  x = tf.keras.layers.SimpleRNN(wsize, return_sequences=True)(inp)\n  x = tf.keras.layers.SimpleRNN(wsize//2)(x)\n  y = tf.keras.layers.Dense(1, activation=\"relu\")(x)\n  model = tf.keras.Model(inputs=inp, outputs=y)\n  print(model.summary())\n  return model\n\n\ndef create_model_3d(dims, wsize):\n  inp = tf.keras.layers.Input(shape=(wsize, dims))\n\n  branch = [None] * 3\n  for i in range(3):\n    branch[i] = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(wsize, return_sequences=True))(inp)\n    branch[i] = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(wsize))(branch[i])\n    branch[i] = tf.keras.layers.Dense(wsize, activation=\"relu\")(branch[i])\n    branch[i] = tf.keras.layers.Dense(wsize//2, activation=\"relu\")(branch[i])\n    branch[i] = tf.keras.layers.Dense(1, activation=\"relu\")(branch[i])\n\n  y = tf.keras.layers.Concatenate()(branch)\n  model = tf.keras.Model(inputs=inp, outputs=y)\n  tf.keras.utils.plot_model(model, show_shapes=True, rankdir=\"TB\")\n  #print(model.summary())\n  return model\n\n\nexp_labels = labels.values[window_size-1:]\nexp_train_data = create_windows(train_data, window_size)\n\nexp_test_data = np.concatenate([train_data[-window_size:,:], test_data[1:]])\nexp_test_data = create_windows(exp_test_data, window_size)\n\ntrain_tensor = tf.data.Dataset.from_tensor_slices((exp_train_data, exp_labels)).batch(BATCH_SIZE, drop_remainder=True).cache()\ntest_tensor = tf.data.Dataset.from_tensor_slices(exp_test_data).batch(BATCH_SIZE).cache()\n\nmodel = create_model_3d(dims, window_size)\nscore, test_predict = train_model(train_tensor, test_tensor, model)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T18:22:05.483165Z","iopub.execute_input":"2021-08-09T18:22:05.483469Z","iopub.status.idle":"2021-08-09T18:23:20.518932Z","shell.execute_reply.started":"2021-08-09T18:22:05.48344Z","shell.execute_reply":"2021-08-09T18:23:20.518175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_array = np.array(Image.open('../input/jul21plot/model_4d.png'))\nplt.figure(figsize = (20,10))\nplt.imshow(img_array)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T18:55:53.988508Z","iopub.execute_input":"2021-08-09T18:55:53.988925Z","iopub.status.idle":"2021-08-09T18:55:54.461835Z","shell.execute_reply.started":"2021-08-09T18:55:53.988888Z","shell.execute_reply":"2021-08-09T18:55:54.461087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build 4D model\n\nThis is the RNN model that uses 24h windows for input features and the 24h multi-step-prediction for target labels.<br/>\nAn example is described here: https://mobiarch.wordpress.com/2020/11/13/preparing-time-series-data-for-rnn-in-tensorflow/<br/>\nThis model returned the local result of `0.3389951288700104` after about the 150 steps.","metadata":{}},{"cell_type":"code","source":"def create_model_4d(dims, wsize):\n  inp = tf.keras.layers.Input(shape=(wsize, dims))\n  #x = tf.keras.layers.Embedding(dims, dims//2)(inp)\n  #x = tf.keras.layers.Reshape(target_shape=(dims//2))(x)\n  #x = tf.keras.layers.GlobalAveragePooling2D()(x)\n  #x = tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1))(x)\n  #x = tf.keras.layers.BatchNormalization()(inp)\n\n  branch = [None] * 3\n  for i in range(3):\n    branch[i] = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(wsize//2, return_sequences=True))(inp)\n    branch[i] = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(wsize//2, return_sequences=True))(branch[i])\n    branch[i] = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(wsize//2))(branch[i])\n    branch[i] = tf.keras.layers.Dense(1, activation=\"relu\")(branch[i])\n\n  y = tf.keras.layers.Concatenate()(branch)\n  model = tf.keras.Model(inputs=inp, outputs=y)\n\n  #tf.keras.utils.plot_model(model, show_shapes=True, rankdir=\"TB\")\n  #print(model.summary())\n  return model\n\n\nexp_labels = create_windows(labels, window_size) \nexp_train_data = create_windows(train_data, window_size)\n\nexp_test_data = np.concatenate([train_data[-window_size:,:], test_data[1:]])\nexp_test_data = create_windows(exp_test_data, window_size)\n\ntrain_tensor = tf.data.Dataset.from_tensor_slices((exp_train_data, exp_labels)).batch(BATCH_SIZE, drop_remainder=True).cache()\ntest_tensor = tf.data.Dataset.from_tensor_slices(exp_test_data).batch(BATCH_SIZE).cache()\n\nmodel = create_model_4d(dims, window_size)\nscore, test_predict = train_model(train_tensor, test_tensor, model)\ntest_predict = test_predict[:,:,0]","metadata":{"execution":{"iopub.status.busy":"2021-08-09T18:51:17.740679Z","iopub.execute_input":"2021-08-09T18:51:17.741045Z","iopub.status.idle":"2021-08-09T18:52:15.113568Z","shell.execute_reply.started":"2021-08-09T18:51:17.74101Z","shell.execute_reply":"2021-08-09T18:52:15.112602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Save results to CSV file","metadata":{}},{"cell_type":"code","source":"output_res = pd.DataFrame(index=test_df.index, data={'date_time':test_df.index.values})\noutput_res[labels.columns] = test_predict\noutput_res.to_csv('./submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T18:23:22.743431Z","iopub.status.idle":"2021-08-09T18:23:22.745358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Validate the 3D model residuals\n\nAs we can see, the residuals still contain the autocorrelations and trend/seasonality patterns.","metadata":{}},{"cell_type":"code","source":"exp_labels = labels.values[window_size-1:]\ntrain_predict = pd.read_csv('../input/predicted-train-ds/train_predict.csv')\nresiduals = pd.DataFrame(data=train_predict-exp_labels, columns=labels.columns)\n\nprint('mean: \\n', residuals.mean(axis=0))\nprint('std: \\n', residuals.std(axis=0))","metadata":{"execution":{"iopub.status.busy":"2021-08-09T19:05:07.272868Z","iopub.execute_input":"2021-08-09T19:05:07.273263Z","iopub.status.idle":"2021-08-09T19:05:07.302731Z","shell.execute_reply.started":"2021-08-09T19:05:07.273227Z","shell.execute_reply":"2021-08-09T19:05:07.301556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss = np.sqrt(mean_squared_log_error(train_predict, exp_labels))\nprint('loss function:', loss)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T19:05:10.156561Z","iopub.execute_input":"2021-08-09T19:05:10.157043Z","iopub.status.idle":"2021-08-09T19:05:10.16637Z","shell.execute_reply.started":"2021-08-09T19:05:10.15701Z","shell.execute_reply":"2021-08-09T19:05:10.165258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"residuals.plot(subplots=True, figsize=(20,5))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T19:05:15.769337Z","iopub.execute_input":"2021-08-09T19:05:15.769681Z","iopub.status.idle":"2021-08-09T19:05:16.24911Z","shell.execute_reply.started":"2021-08-09T19:05:15.769652Z","shell.execute_reply":"2021-08-09T19:05:16.248218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tsaplots.plot_acf(residuals[residuals.columns[0]], lags=30, title=residuals.columns[2])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T19:05:43.653521Z","iopub.execute_input":"2021-08-09T19:05:43.65388Z","iopub.status.idle":"2021-08-09T19:05:43.868357Z","shell.execute_reply.started":"2021-08-09T19:05:43.653851Z","shell.execute_reply":"2021-08-09T19:05:43.867319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seasonal_decompose(residuals[residuals.columns[0]], period=255).seasonal.plot()\nseasonal_decompose(residuals[residuals.columns[0]], period=255).trend.plot()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T19:05:46.858099Z","iopub.execute_input":"2021-08-09T19:05:46.858529Z","iopub.status.idle":"2021-08-09T19:05:47.111979Z","shell.execute_reply.started":"2021-08-09T19:05:46.858495Z","shell.execute_reply":"2021-08-09T19:05:47.110512Z"},"trusted":true},"execution_count":null,"outputs":[]}]}