{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.lines import Line2D\nfrom catboost import CatBoostRegressor\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit, LeaveOneGroupOut\nfrom sklearn.ensemble import IsolationForest\n\n# Pandas setting to display more dataset rows and columns\npd.set_option('display.max_rows', 100)\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_colwidth', None)\npd.set_option('display.float_format', lambda x: '%.5f' % x)\n\n# import warnings\n# warnings.simplefilter(action='ignore', category=UserWarning)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-29T06:09:38.287953Z","iopub.execute_input":"2021-07-29T06:09:38.288318Z","iopub.status.idle":"2021-07-29T06:09:38.766187Z","shell.execute_reply.started":"2021-07-29T06:09:38.288237Z","shell.execute_reply":"2021-07-29T06:09:38.765017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Data import**","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/tabular-playground-series-jul-2021/train.csv\", low_memory=False)#, nrows=10000)\ntrain[\"date_time\"] = pd.to_datetime(train[\"date_time\"], format=\"%Y-%m-%d %H:%M:%S\")\ntest = pd.read_csv(\"/kaggle/input/tabular-playground-series-jul-2021/test.csv\", low_memory=False)\ntest[\"date_time\"] = pd.to_datetime(test[\"date_time\"], format=\"%Y-%m-%d %H:%M:%S\")\ntrain.info(memory_usage=\"deep\")","metadata":{"execution":{"iopub.status.busy":"2021-07-29T06:09:38.768428Z","iopub.execute_input":"2021-07-29T06:09:38.768755Z","iopub.status.idle":"2021-07-29T06:09:38.821129Z","shell.execute_reply.started":"2021-07-29T06:09:38.768722Z","shell.execute_reply":"2021-07-29T06:09:38.820332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.info(memory_usage=\"deep\")","metadata":{"execution":{"iopub.status.busy":"2021-07-29T06:09:38.822852Z","iopub.execute_input":"2021-07-29T06:09:38.823449Z","iopub.status.idle":"2021-07-29T06:09:38.836689Z","shell.execute_reply.started":"2021-07-29T06:09:38.8234Z","shell.execute_reply":"2021-07-29T06:09:38.835479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T06:09:38.838662Z","iopub.execute_input":"2021-07-29T06:09:38.839044Z","iopub.status.idle":"2021-07-29T06:09:38.87214Z","shell.execute_reply.started":"2021-07-29T06:09:38.839008Z","shell.execute_reply":"2021-07-29T06:09:38.870833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **EDA**","metadata":{}},{"cell_type":"code","source":"targets = [\"target_carbon_monoxide\", \"target_benzene\", \"target_nitrogen_oxides\"]\ntarget_names = [\"Carbon monoxide\", \"Benzene\", \"Nitrogen oxides\"]","metadata":{"execution":{"iopub.status.busy":"2021-07-29T06:09:38.874042Z","iopub.execute_input":"2021-07-29T06:09:38.874429Z","iopub.status.idle":"2021-07-29T06:09:38.88381Z","shell.execute_reply.started":"2021-07-29T06:09:38.874365Z","shell.execute_reply":"2021-07-29T06:09:38.882401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The idea of SMC feature below was taken from this [notebook](https://www.kaggle.com/junhyeok99/automl-pycaret).\n\nTaking into account temperature changes was suggested by [@lukaszborecki](https://www.kaggle.com/lukaszborecki) [here](https://www.kaggle.com/c/tabular-playground-series-jul-2021/discussion/250931#1380107).","metadata":{}},{"cell_type":"code","source":"def add_new_plot_features(df):\n    \"\"\"\n    Adds new features to a given dataset for plotting\n    \"\"\"\n    df[\"month\"] = df[\"date_time\"].dt.month\n    df[\"day_of_week\"] = df[\"date_time\"].dt.dayofweek\n    df[\"day_of_year\"] = df[\"date_time\"].dt.dayofyear\n    df[\"hour\"] = df[\"date_time\"].dt.hour\n    df[\"quarter\"] = df[\"date_time\"].dt.quarter\n    df[\"week_of_year\"] = df[\"date_time\"].dt.isocalendar().week.astype(\"int\")\n#     df[\"is_winter\"] = df[\"month\"].isin([1, 2, 12])\n#     df[\"is_sprint\"] = df[\"month\"].isin([3, 4, 5])\n#     df[\"is_summer\"] = df[\"month\"].isin([6, 7, 8])\n#     df[\"is_autumn\"] = df[\"month\"].isin([9, 10, 11])\n    df[\"working_hours\"] =  df[\"hour\"].isin(np.arange(8, 21, 1)).astype(\"int\")\n    df[\"is_weekend\"] = (df[\"date_time\"].dt.dayofweek >= 5).astype(\"int\")\n    return df\n\n# def add_new_heatmap_features(df):\n#     \"\"\"\n#     Adds new features to a given dataset for correlation heatmap\n#     \"\"\"\n  \n#     df[\"hour\"] = df[\"date_time\"].dt.hour\n#     df[\"working_hours\"] =  df[\"hour\"].isin(np.arange(8, 21, 1)).astype(\"int\")\n#     df[\"maximum_hours\"] =  df[\"hour\"].isin([8, 9, 17, 18, 19, 20]).astype(\"int\")\n#     df[\"is_weekend\"] = (train[\"date_time\"].dt.dayofweek >= 5).astype(\"int\")\n#     df[\"SMC\"] = (df[\"absolute_humidity\"] * 100) / df[\"relative_humidity\"]\n#     df[\"temp-6\"] = df[\"deg_C\"] - df[\"deg_C\"].shift(periods=6, fill_value=0)\n#     df[\"temp-3\"] = df[\"deg_C\"] - df[\"deg_C\"].shift(periods=3, fill_value=0)\n#     df[\"temp-24\"] = df[\"deg_C\"] - df[\"deg_C\"].shift(periods=24, fill_value=0)\n#     df[\"abshum-6\"] = df[\"absolute_humidity\"] - df[\"absolute_humidity\"].shift(periods=6, fill_value=0)\n#     df[\"abshum-3\"] = df[\"absolute_humidity\"] - df[\"absolute_humidity\"].shift(periods=3, fill_value=0)\n#     df[\"relhum-6\"] = df[\"relative_humidity\"] - df[\"relative_humidity\"].shift(periods=6, fill_value=0)\n#     df[\"relhum-3\"] = df[\"relative_humidity\"] - df[\"relative_humidity\"].shift(periods=3, fill_value=0)\n#     df[\"s1-6\"] = df[\"sensor_1\"] - df[\"sensor_1\"].shift(periods=6, fill_value=0)\n#     df[\"s2-6\"] = df[\"sensor_2\"] - df[\"sensor_2\"].shift(periods=6, fill_value=0)\n#     df[\"s3-6\"] = df[\"sensor_3\"] - df[\"sensor_3\"].shift(periods=6, fill_value=0)\n#     df[\"s4-6\"] = df[\"sensor_4\"] - df[\"sensor_4\"].shift(periods=6, fill_value=0)\n#     df[\"s5-6\"] = df[\"sensor_5\"] - df[\"sensor_5\"].shift(periods=6, fill_value=0)\n    \n#     df.drop([\"hour\"], axis=1, inplace=True)\n    \n#     return df.copy()\n\ndef add_new_ml_features(df, i=3): # i=3 is for heatmap plot\n    \"\"\"\n    Adds new features to a given dataset for training\n    \"\"\"\n\n    df[\"hour\"] = df[\"date_time\"].dt.hour\n    df[\"working_hours\"] =  df[\"hour\"].isin(np.arange(8, 21, 1)).astype(\"int\")\n    df[\"maximum_hours\"] =  df[\"hour\"].isin([8, 9, 17, 18, 19, 20]).astype(\"int\")\n    \n    # Marking weekends because they usually have lower target values\n    df[\"is_weekend\"] = (df[\"date_time\"].dt.dayofweek >= 5).astype(\"int\")\n    df[\"SMC\"] = (df[\"absolute_humidity\"] * 100) / df[\"relative_humidity\"]\n    \n    # A list of features to generate shifted and lagged values\n    shift_features = [[\"SMC\", \"absolute_humidity\", \"deg_C\",\n                      \"sensor_1\", \"sensor_2\", \"sensor_3\", \"sensor_4\", \"sensor_5\"],\n                      [\"SMC\", \"absolute_humidity\", \"target_carbon_monoxide_preds\",\n                      \"sensor_1\", \"sensor_2\", \"sensor_3\", \"sensor_4\", \"sensor_5\"],\n                      [\"SMC\", \"absolute_humidity\", \"target_carbon_monoxide_preds\", \"target_benzene_preds\",\n                      \"sensor_1\", \"sensor_2\", \"sensor_3\", \"sensor_5\"],\n                      # Features for heatmap plot\n                      [\"SMC\", \"absolute_humidity\", \"deg_C\",\n                      \"sensor_1\", \"sensor_2\", \"sensor_3\", \"sensor_4\", \"sensor_5\"]]\n    \n    # Amounts of hour shifts and lags\n    shifts = [1, 2, 3, 4, 5, 6, 12, 24]\n    \n    for feature in shift_features[i]:\n        for shift in shifts:\n            df[feature+\"-\"+str(shift)+\"abs_shfit\"] = df[feature] - df[feature].shift(periods=shift, fill_value=0)\n            df[feature+\"+\"+str(shift)+\"abs_shfit\"] = df[feature] - df[feature].shift(periods=-shift, fill_value=0)\n    \n    df.drop([\"hour\"], axis=1, inplace=True)\n    \n    return df.copy()","metadata":{"execution":{"iopub.status.busy":"2021-07-29T06:09:38.885467Z","iopub.execute_input":"2021-07-29T06:09:38.886164Z","iopub.status.idle":"2021-07-29T06:09:38.905971Z","shell.execute_reply.started":"2021-07-29T06:09:38.88611Z","shell.execute_reply":"2021-07-29T06:09:38.904572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_copy = train.copy()\ntest_copy = test.copy()\ntrain = add_new_plot_features(train)\ntest = add_new_plot_features(test)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T06:09:38.907545Z","iopub.execute_input":"2021-07-29T06:09:38.907898Z","iopub.status.idle":"2021-07-29T06:09:38.965769Z","shell.execute_reply.started":"2021-07-29T06:09:38.907862Z","shell.execute_reply":"2021-07-29T06:09:38.964703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The datasets have timestamps. Let's compare which dates are in each dataset.","metadata":{}},{"cell_type":"code","source":"# Plot dataframe\ndf = pd.concat([train[\"date_time\"], test[\"date_time\"]], axis=0).reset_index(drop=True)\n\nfig, ax = plt.subplots(figsize=(16, 1.5))\nbar1 =  ax.barh(0, 7111+2247, color=\"salmon\", height=0.2)\nbar2 =  ax.barh(0, 7111, color=\"teal\", height=0.2)\nax.set_title(\"Train and test datasets size comparison\", fontsize=20, pad=5)\nax.bar_label(bar1, [\"Test dataset\"], label_type=\"edge\", padding=-170,\n             fontsize=20, color=\"white\", weight=\"bold\")\nax.bar_label(bar2, [\"Train dataset\"], label_type=\"center\",\n             fontsize=20, color=\"white\", weight=\"bold\")\nax.set_xticks([0, 7111, 7111+2247])\nax.set_xticklabels([\"2010-03-10\", \"2011-01-01\", \"2011-04-04\"])\nax.set_yticks([])\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-07-29T06:09:38.968514Z","iopub.execute_input":"2021-07-29T06:09:38.969084Z","iopub.status.idle":"2021-07-29T06:09:39.101108Z","shell.execute_reply.started":"2021-07-29T06:09:38.969031Z","shell.execute_reply":"2021-07-29T06:09:39.099985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Target plots","metadata":{}},{"cell_type":"markdown","source":"The datasets also have three target columns that the model have to predict. Let's see how each target is changing in time.","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(figsize=(16, 18), ncols=1, nrows=3, sharex=False)\n\nplt.subplots_adjust(hspace = 0.3)\n\ncolors = [\"palevioletred\", \"deepskyblue\", \"mediumseagreen\"]\n\nfor i in [0, 1, 2]:\n    axs[i].plot(train[\"date_time\"], train[targets[i]], color=colors[i])\n    axs[i].set_title(f\"{target_names[i]} (target #{i+1}) levels across time\", fontsize=20, pad=5)\n    axs[i].set_ylabel(f\"{target_names[i]} level\", fontsize=14, labelpad=5)\n    axs[i].set_xlabel(\"Date\", fontsize=14, labelpad=5)\n    axs[i].grid(axis=\"both\")\n\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-07-29T06:09:39.103587Z","iopub.execute_input":"2021-07-29T06:09:39.103941Z","iopub.status.idle":"2021-07-29T06:09:40.022123Z","shell.execute_reply.started":"2021-07-29T06:09:39.103907Z","shell.execute_reply":"2021-07-29T06:09:40.020656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's see mean target values per day of year.","metadata":{}},{"cell_type":"code","source":"# Dataframe copy excluding the last row which is the only one representing January\ndf = train.drop([7110], axis=0).copy()\ndf[\"day\"] = df[\"date_time\"].dt.dayofyear\ndf[\"weekday\"] = df[\"date_time\"].dt.dayofweek\n\ncolors = [\"palevioletred\", \"deepskyblue\", \"mediumseagreen\"]\n\n# An array of number of days of year (i.e. from 1 to 365) which are mondays to mark week starts\nmondays = df.loc[df[\"weekday\"] == 0][\"day\"].value_counts(sort=False).index\n# An array of number of weeks of year to be used as label ticks\nweeks = df[\"date_time\"].dt.isocalendar().week.unique()[1:]\n\nfig, axs = plt.subplots(figsize=(16, 18), ncols=1, nrows=3, sharex=False)\n\nplt.subplots_adjust(hspace = 0.3)\n\nfor i in [0, 1, 2]:\n    axs[i].plot(df.groupby(\"day\")[targets[i]].mean().index,\n                df.groupby(\"day\")[targets[i]].mean().values, color=colors[i])\n    axs[i].set_title(f\"{target_names[i]} (target #{i+1}) mean levels across time\", fontsize=20, pad=5)\n    axs[i].set_ylabel(f\"{target_names[i]} level\", fontsize=14, labelpad=5)\n    axs[i].set_xlabel(\"Week starts\", fontsize=14, labelpad=5)\n    axs[i].set_xticks(mondays)\n    axs[i].set_xticklabels(weeks)\n    axs[i].grid(axis=\"both\")\n\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-07-29T06:09:40.024167Z","iopub.execute_input":"2021-07-29T06:09:40.024707Z","iopub.status.idle":"2021-07-29T06:09:41.303112Z","shell.execute_reply.started":"2021-07-29T06:09:40.024646Z","shell.execute_reply":"2021-07-29T06:09:41.300248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see, all target values usually go down at the end of each week (i.e. during weekends). \n\nLet's check targets distribution along each month.","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(ncols=2, nrows=5, figsize=(16, 20))\nplt.subplots_adjust(hspace = 0.3)\nfig.suptitle(target_names[0], fontsize=20)\n\ni=3\nfor r in np.arange(5):\n    for c in [0, 1]:\n        axs[r, c].plot(train.loc[train[\"month\"]==i, targets[0]], color=\"steelblue\")\n        axs[r, c].set_title(f\"Month #{i}\", fontsize=15)\n        axs[r, c].legend(fontsize=13)\n        i+=1","metadata":{"execution":{"iopub.status.busy":"2021-07-29T06:09:41.305827Z","iopub.execute_input":"2021-07-29T06:09:41.306312Z","iopub.status.idle":"2021-07-29T06:09:43.014993Z","shell.execute_reply.started":"2021-07-29T06:09:41.306261Z","shell.execute_reply":"2021-07-29T06:09:43.013539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(ncols=2, nrows=5, figsize=(16, 20))\nplt.subplots_adjust(hspace = 0.3)\nfig.suptitle(target_names[1], fontsize=20)\n\ni=3\nfor r in np.arange(5):\n    for c in [0, 1]:\n        axs[r, c].plot(train.loc[train[\"month\"]==i, targets[1]], color=\"palevioletred\")\n        axs[r, c].set_title(f\"Month #{i}\", fontsize=15)\n        axs[r, c].legend(fontsize=13)\n        i+=1","metadata":{"execution":{"iopub.status.busy":"2021-07-29T06:09:43.01669Z","iopub.execute_input":"2021-07-29T06:09:43.017187Z","iopub.status.idle":"2021-07-29T06:09:44.812724Z","shell.execute_reply.started":"2021-07-29T06:09:43.01713Z","shell.execute_reply":"2021-07-29T06:09:44.811802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are some near zero flat areas at 4th, 6th, 8th, 12th month plots. Need to figure out what is so special about these days. It also may be a garbage data which sould be deleted before machine learning.","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(ncols=2, nrows=5, figsize=(16, 20))\nplt.set_cmap(\"Set2\")\nplt.subplots_adjust(hspace = 0.3)\nfig.suptitle(target_names[2], fontsize=20)\n\ni=3\nfor r in np.arange(5):\n    for c in [0, 1]:\n        axs[r, c].plot(train.loc[train[\"month\"]==i, targets[2]], color=\"goldenrod\")\n        axs[r, c].set_title(f\"Month #{i}\", fontsize=15)\n        axs[r, c].legend(fontsize=13)\n        i+=1","metadata":{"execution":{"iopub.status.busy":"2021-07-29T06:09:44.813871Z","iopub.execute_input":"2021-07-29T06:09:44.814339Z","iopub.status.idle":"2021-07-29T06:09:46.478836Z","shell.execute_reply.started":"2021-07-29T06:09:44.814307Z","shell.execute_reply":"2021-07-29T06:09:46.477399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check each target value distribution.","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(figsize=(15, 6), ncols=3, nrows=1, sharey=False)\n\nfig.suptitle(\"Target values distribution\", fontsize=20)\n\ncolors = [\"mediumorchid\", \"lightseagreen\", \"cornflowerblue\"]\n\nfor i in [0, 1, 2]:\n    axs[i].hist(train[targets[i]], bins=60, edgecolor=\"black\", color=colors[i])\n    axs[i].set_title(f\"{target_names[i]} (target #{i+1})\", fontsize=15, pad=5)\n    axs[i].set_ylabel(\"Amount of values\", fontsize=13, labelpad=5)\n    axs[i].set_xlabel(f\"{target_names[i]} level\", fontsize=13, labelpad=5)\n    axs[i].grid(axis=\"y\")\n\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-07-29T06:09:46.480701Z","iopub.execute_input":"2021-07-29T06:09:46.481029Z","iopub.status.idle":"2021-07-29T06:09:47.316546Z","shell.execute_reply.started":"2021-07-29T06:09:46.480997Z","shell.execute_reply":"2021-07-29T06:09:47.315415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check how each target value chenges depending on the time of day, day of week, and month.","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(figsize=(16, 18), ncols=1, nrows=3, sharex=False)\n\nplt.subplots_adjust(hspace = 0.3)\nwidth=0.35\nx = train.groupby(\"hour\")[\"target_carbon_monoxide\"].mean().index\n\nfor i in np.arange(3):\n    bars1 = axs[i].bar(x-width/2, train.groupby(\"hour\")[targets[i]].mean(),\n                        width=width, edgecolor=\"black\", label=\"Mean\", color=\"cornflowerblue\")\n    bars2 = axs[i].bar(x+width/2, train.groupby(\"hour\")[targets[i]].median(),\n                        width=width, edgecolor=\"black\", label=\"Median\", color=\"palevioletred\")\n    axs[i].set_title(f\"{target_names[i]} (target #{i+1})\", fontsize=15, pad=10)\n    axs[i].set_ylabel(\"Target value\", fontsize=13, labelpad=5)\n    axs[i].set_xlabel(\"Day hours\", fontsize=13, labelpad=5)\n    axs[i].set_xticks(x)\n    axs[i].grid(axis=\"y\")\n    axs[i].legend(fontsize=13)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-29T06:09:47.318006Z","iopub.execute_input":"2021-07-29T06:09:47.318324Z","iopub.status.idle":"2021-07-29T06:09:48.708064Z","shell.execute_reply.started":"2021-07-29T06:09:47.318291Z","shell.execute_reply":"2021-07-29T06:09:48.70687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dataframe copy excluding the last row which is the only one representing January\ndf = train.drop([7110], axis=0).copy()\n\nfig, axs = plt.subplots(figsize=(16, 19), ncols=2, nrows=3, sharex=False,\n                        gridspec_kw={'width_ratios': [1, 1.5]})\n\nfig.suptitle(\"Target values distribution per month and day of week\", fontsize=20)\n\nplt.subplots_adjust(hspace = 0.25)\nwidth=0.35\nx = df.groupby(\"day_of_week\")[\"target_carbon_monoxide\"].mean().index + 1\n\nfor i in np.arange(3):\n    bars1 = axs[i, 0].bar(x-width/2, df.groupby(\"day_of_week\")[targets[i]].mean(),\n                        width=width, edgecolor=\"black\", label=\"Mean\", color=\"salmon\")\n    bars2 = axs[i, 0].bar(x+width/2, df.groupby(\"day_of_week\")[targets[i]].median(),\n                        width=width, edgecolor=\"black\", label=\"Median\", color=\"teal\")\n    axs[i, 0].set_title(f\"{target_names[i]} (target #{i+1})\", fontsize=15, pad=10)\n    axs[i, 0].set_ylabel(\"Target value\", fontsize=13, labelpad=5)\n    axs[i, 0].set_xlabel(\"Day of week\", fontsize=13, labelpad=5)\n    axs[i, 0].set_xticks(x)\n    axs[i, 0].grid(axis=\"y\")\n    axs[i, 0].legend(fontsize=13)\n\nx = df.groupby(\"month\")[\"target_carbon_monoxide\"].mean().index\nfor i in np.arange(3):\n    bars1 = axs[i, 1].bar(x-width/2, df.groupby(\"month\")[targets[i]].mean(),\n                        width=width, edgecolor=\"black\", label=\"Mean\", color=\"salmon\")\n    bars2 = axs[i, 1].bar(x+width/2, df.groupby(\"month\")[targets[i]].median(),\n                        width=width, edgecolor=\"black\", label=\"Median\", color=\"teal\")\n    axs[i, 1].set_title(f\"{target_names[i]} (target #{i+1})\", fontsize=15, pad=10)\n    axs[i, 1].set_ylabel(\"Target value\", fontsize=13, labelpad=5)\n    axs[i, 1].set_xlabel(\"Month\", fontsize=13, labelpad=5)\n    axs[i, 1].set_xticks(x)\n    axs[i, 1].grid(axis=\"y\")\n    axs[i, 1].legend(fontsize=13)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-29T06:09:48.70954Z","iopub.execute_input":"2021-07-29T06:09:48.709852Z","iopub.status.idle":"2021-07-29T06:09:50.062003Z","shell.execute_reply.started":"2021-07-29T06:09:48.709821Z","shell.execute_reply":"2021-07-29T06:09:50.06103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Day hours which will be used for plotting data\nhours = [0, 5, 8, 14, 19]\n# Dataframe copy excluding the last row which is the only one representing January\ndf = train.loc[train[\"hour\"].isin(hours)].drop([7110], axis=0).copy()\n\nfig, axs = plt.subplots(figsize=(16, 18), ncols=2, nrows=3, sharex=False,\n                        gridspec_kw={'width_ratios': [1, 1.5]})\n\nfig.suptitle(\"Target values distribution per month and day of week at given hours\", fontsize=20)\n\nplt.subplots_adjust(hspace = 0.3)\nwidth=0.15\nx = np.sort(df[\"day_of_week\"].unique()) + 1\n\nfor i in np.arange(3):\n    bars1 = axs[i, 0].bar(x-width*2, df.loc[df[\"hour\"] == 0].groupby(\"day_of_week\")[targets[i]].mean(),\n                        width=width, edgecolor=\"black\", label=\"00:00\", color=\"salmon\")\n    bars2 = axs[i, 0].bar(x-width, df.loc[df[\"hour\"] == 5].groupby(\"day_of_week\")[targets[i]].mean(),\n                        width=width, edgecolor=\"black\", label=\"05:00\", color=\"sandybrown\")\n    bars3 = axs[i, 0].bar(x, df.loc[df[\"hour\"] == 8].groupby(\"day_of_week\")[targets[i]].mean(),\n                        width=width, edgecolor=\"black\", label=\"08:00\", color=\"teal\")\n    bars4 = axs[i, 0].bar(x+width, df.loc[df[\"hour\"] == 14].groupby(\"day_of_week\")[targets[i]].mean(),\n                        width=width, edgecolor=\"black\", label=\"14:00\", color=\"palevioletred\")\n    bars5 = axs[i, 0].bar(x+width*2, df.loc[df[\"hour\"] == 19].groupby(\"day_of_week\")[targets[i]].mean(),\n                        width=width, edgecolor=\"black\", label=\"19:00\", color=\"mediumslateblue\")\n    axs[i, 0].set_title(f\"{target_names[i]} (target #{i+1})\", fontsize=15, pad=10)\n    axs[i, 0].set_ylabel(\"Target value\", fontsize=13, labelpad=5)\n    axs[i, 0].set_xlabel(\"Day of week\", fontsize=13, labelpad=5)\n    axs[i, 0].set_xticks(x)\n    axs[i, 0].grid(axis=\"y\")\n    axs[i, 0].legend(fontsize=10)\n\nx = df[\"month\"].unique()\nfor i in np.arange(3):\n    bars1 = axs[i, 1].bar(x-width*2, df.loc[df[\"hour\"] == 0].groupby(\"month\")[targets[i]].mean(),\n                        width=width, edgecolor=\"black\", label=\"00:00\", color=\"salmon\")\n    bars2 = axs[i, 1].bar(x-width, df.loc[df[\"hour\"] == 5].groupby(\"month\")[targets[i]].mean(),\n                        width=width, edgecolor=\"black\", label=\"05:00\", color=\"sandybrown\")\n    bars3 = axs[i, 1].bar(x, df.loc[df[\"hour\"] == 8].groupby(\"month\")[targets[i]].mean(),\n                        width=width, edgecolor=\"black\", label=\"08:00\", color=\"teal\")\n    bars4 = axs[i, 1].bar(x+width, df.loc[df[\"hour\"] == 14].groupby(\"month\")[targets[i]].mean(),\n                        width=width, edgecolor=\"black\", label=\"14:00\", color=\"palevioletred\")\n    bars5 = axs[i, 1].bar(x+width*2, df.loc[df[\"hour\"] == 19].groupby(\"month\")[targets[i]].mean(),\n                        width=width, edgecolor=\"black\", label=\"19:00\", color=\"mediumslateblue\")\n    axs[i, 1].set_title(f\"{target_names[i]} (target #{i+1})\", fontsize=15, pad=10)\n    axs[i, 1].set_ylabel(\"Target value\", fontsize=13, labelpad=5)\n    axs[i, 1].set_xlabel(\"Month\", fontsize=13, labelpad=5)\n    axs[i, 1].set_xticks(x)\n    axs[i, 1].grid(axis=\"y\")\n    axs[i, 1].legend(fontsize=10)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T06:09:50.063033Z","iopub.execute_input":"2021-07-29T06:09:50.063306Z","iopub.status.idle":"2021-07-29T06:09:52.199722Z","shell.execute_reply.started":"2021-07-29T06:09:50.063278Z","shell.execute_reply":"2021-07-29T06:09:52.192261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature plots","metadata":{}},{"cell_type":"code","source":"# Lists of feature names to be used for plots below\nall_features = [\"deg_C\", \"relative_humidity\", \"absolute_humidity\", \"sensor_1\", \"sensor_2\", \"sensor_3\", \"sensor_4\", \"sensor_5\"]\nall_feature_names = [\"Temperature (deg. C)\", \"Relative humidity\", \"Absolute humidity\", \"Sensor 1\", \"Sensor_2\", \"Sensor 3\", \"Sensor 4\", \"Sensor 5\"]\n\nweather_features = [\"deg_C\", \"relative_humidity\", \"absolute_humidity\"]\nweather_feature_names = [\"Temperature (deg. C)\", \"Relative humidity\", \"Absolute humidity\"]\n\nsensor_features = [\"sensor_1\", \"sensor_2\", \"sensor_3\", \"sensor_4\", \"sensor_5\"]\nsensor_feature_names = [\"Sensor 1\", \"Sensor_2\", \"Sensor 3\", \"Sensor 4\", \"Sensor 5\"]","metadata":{"execution":{"iopub.status.busy":"2021-07-29T06:09:52.201245Z","iopub.execute_input":"2021-07-29T06:09:52.201698Z","iopub.status.idle":"2021-07-29T06:09:52.211596Z","shell.execute_reply.started":"2021-07-29T06:09:52.201651Z","shell.execute_reply":"2021-07-29T06:09:52.210216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's compare our train and test feature data.","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(figsize=(16, 30), ncols=1, nrows=8, sharex=False)\n\nplt.subplots_adjust(hspace = 0.4)\n\ncolors = [\"lightcoral\", \"sandybrown\", \"darkorange\", \"mediumseagreen\",\n          \"lightseagreen\", \"cornflowerblue\", \"mediumpurple\", \"palevioletred\",\n          \"lightskyblue\", \"sandybrown\", \"yellowgreen\", \"indianred\",\n          \"lightsteelblue\", \"mediumorchid\", \"deepskyblue\"]\n\nfor i in np.arange(8):\n    legend_lines = [Line2D([0], [0], color=colors[i], lw=10),\n                    Line2D([0], [0], color=\"black\", lw=10)]\n    axs[i].plot(train[\"date_time\"], train[all_features[i]], color=colors[i], label=\"Train data\")\n    axs[i].plot(test[\"date_time\"], test[all_features[i]], color=\"black\", label=\"Test data\")\n    axs[i].set_title(f\"{all_feature_names[i]} levels across time\", fontsize=20, pad=5)\n    axs[i].set_ylabel(f\"{all_feature_names[i]} level\", fontsize=14, labelpad=5)\n    axs[i].set_xlabel(\"Date\", fontsize=14, labelpad=5)\n    axs[i].legend(legend_lines, [\"Train data\", \"Test data\"], fontsize=12, loc=1)\n    axs[i].grid(axis=\"both\")","metadata":{"execution":{"iopub.status.busy":"2021-07-29T06:09:52.213349Z","iopub.execute_input":"2021-07-29T06:09:52.21382Z","iopub.status.idle":"2021-07-29T06:09:54.059122Z","shell.execute_reply.started":"2021-07-29T06:09:52.213771Z","shell.execute_reply":"2021-07-29T06:09:54.058074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot dataframe creation\ndf = pd.concat([train_copy, test_copy], axis=0)\ndf.reset_index(drop=True, inplace=True)\ndf[\"week_of_year\"] = df[\"date_time\"].dt.isocalendar().week.astype(\"int\")\ndf[\"day_of_year\"] = df[\"date_time\"].dt.dayofyear\n\nfig, axs = plt.subplots(figsize=(16, 18), ncols=2, nrows=3, sharex=False)\n\nplt.subplots_adjust(hspace = 0.4)\n\ncolors = [\"palevioletred\", \"deepskyblue\", \"mediumseagreen\"]\n\nfor i in [0, 1, 2]:\n    # New year days start from 7110th row\n    data = df.iloc[:7110].groupby(\"day_of_year\")[weather_features[i]].mean()\n    axs[i, 0].plot(data.index, data.values, color=colors[i], label=\"Train data\")\n    data = df.iloc[7110:].groupby(\"day_of_year\")[weather_features[i]].mean()\n    axs[i, 0].plot(data.index, data.values, color=\"black\", alpha=0.7, label=\"Test data\")\n    axs[i, 0].set_title(f\"Mean dayly {weather_feature_names[i]} levels\", fontsize=20, pad=5)\n    axs[i, 0].set_ylabel(f\"{weather_feature_names[i]} level\", fontsize=14, labelpad=5)\n    axs[i, 0].set_xlabel(\"Day of year\", fontsize=14, labelpad=5)\n    axs[i, 0].grid(axis=\"both\")\n    axs[i, 0].legend(fontsize=12)\n\n\nfor i in [0, 1, 2]:\n    # New year weeks start from 7159th row. \n    # Because of Jan 1st and 2nd from the test dataset are counted as 52nd week of 2010,\n    # the colored plotline contains some test data. \n    data = df.iloc[:7159].groupby(\"week_of_year\")[weather_features[i]].mean()\n    axs[i, 1].plot(data.index, data.values, color=colors[i], label=\"Train data\")\n    data = df.iloc[7159:].groupby(\"week_of_year\")[weather_features[i]].mean()\n    axs[i, 1].plot(data.index, data.values, color=\"black\", alpha=0.7, label=\"Test data\")\n    axs[i, 1].set_title(f\"Mean weekly {weather_feature_names[i]} levels\", fontsize=20, pad=5)\n    axs[i, 1].set_ylabel(f\"{weather_feature_names[i]} level\", fontsize=14, labelpad=5)\n    axs[i, 1].set_xlabel(\"Week of year\", fontsize=14, labelpad=5)\n    axs[i, 1].grid(axis=\"both\")\n    axs[i, 1].legend(fontsize=12)\n\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-07-29T06:09:54.060395Z","iopub.execute_input":"2021-07-29T06:09:54.060898Z","iopub.status.idle":"2021-07-29T06:09:55.323662Z","shell.execute_reply.started":"2021-07-29T06:09:54.06086Z","shell.execute_reply":"2021-07-29T06:09:55.32257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot dataframe creation\ndf = pd.concat([train_copy, test_copy], axis=0)\ndf.reset_index(drop=True, inplace=True)\ndf[\"week_of_year\"] = df[\"date_time\"].dt.isocalendar().week.astype(\"int\")\ndf[\"day_of_year\"] = df[\"date_time\"].dt.dayofyear\n\nfig, axs = plt.subplots(figsize=(16, 30), ncols=2, nrows=5, sharex=False)\n\nplt.subplots_adjust(hspace = 0.4)\n\ncolors = [\"palevioletred\", \"deepskyblue\", \"mediumseagreen\", \"goldenrod\", \"indianred\"]\n\nfor i in np.arange(5):\n    data = df.iloc[:7110].groupby(\"day_of_year\")[sensor_features[i]].mean()\n    axs[i, 0].plot(data.index, data.values, color=colors[i], label=\"Train data\")\n    data = df.iloc[7110:].groupby(\"day_of_year\")[sensor_features[i]].mean()\n    axs[i, 0].plot(data.index, data.values, color=\"black\", alpha=0.7, label=\"Test data\")\n    axs[i, 0].set_title(f\"Mean dayly {sensor_feature_names[i]} levels\", fontsize=20, pad=5)\n    axs[i, 0].set_ylabel(f\"{sensor_feature_names[i]} level\", fontsize=14, labelpad=5)\n    axs[i, 0].set_xlabel(\"Day of year\", fontsize=14, labelpad=5)\n    axs[i, 0].grid(axis=\"both\")\n    axs[i, 0].legend(fontsize=12)\n\n\nfor i in np.arange(5):\n    data = df.iloc[:7159].groupby(\"week_of_year\")[sensor_features[i]].mean()\n    axs[i, 1].plot(data.index, data.values, color=colors[i], label=\"Train data\")\n    data = df.iloc[7159:].groupby(\"week_of_year\")[sensor_features[i]].mean()\n    axs[i, 1].plot(data.index, data.values, color=\"black\", alpha=0.7, label=\"Test data\")\n    axs[i, 1].set_title(f\"Mean dayly {sensor_feature_names[i]} levels\", fontsize=20, pad=5)\n    axs[i, 1].set_ylabel(f\"{sensor_feature_names[i]} level\", fontsize=14, labelpad=5)\n    axs[i, 1].set_xlabel(\"Week of year\", fontsize=14, labelpad=5)\n    axs[i, 1].grid(axis=\"both\")\n    axs[i, 1].legend(fontsize=12)\n\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-07-29T06:09:55.325322Z","iopub.execute_input":"2021-07-29T06:09:55.325668Z","iopub.status.idle":"2021-07-29T06:09:57.379136Z","shell.execute_reply.started":"2021-07-29T06:09:55.325639Z","shell.execute_reply":"2021-07-29T06:09:57.378075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check feature correlation.","metadata":{}},{"cell_type":"code","source":"# Plot dataframe\ndf = train_copy.copy()\ndf = pd.concat([df[targets], df.drop(targets, axis=1)], axis=1).corr().round(2)\n\n# Mask to hide upper-right part of plot as it is a duplicate\nmask = np.zeros_like(df)\nmask[np.triu_indices_from(mask)] = True\n\n# Making a plot\nplt.figure(figsize=(12,12))\nax = sns.heatmap(df, annot=True, mask=mask, cmap=\"RdBu\", linewidth=1,\n                 annot_kws={\"weight\": \"bold\", \"fontsize\":13})\nax.set_title(\"Original dataset correlation heatmap\", fontsize=17)\nplt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n         rotation_mode=\"anchor\", weight=\"bold\")\nplt.setp(ax.get_yticklabels(), weight=\"bold\")\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-07-29T06:09:57.380433Z","iopub.execute_input":"2021-07-29T06:09:57.380731Z","iopub.status.idle":"2021-07-29T06:09:58.118942Z","shell.execute_reply.started":"2021-07-29T06:09:57.380703Z","shell.execute_reply":"2021-07-29T06:09:58.117564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot dataframe\ndf = add_new_ml_features(train_copy.copy())\ndf = pd.concat([df[targets], df.drop(targets, axis=1)], axis=1).corr().round(2)\n\n# Mask to hide upper-right part of plot as it is a duplicate\nmask = np.zeros_like(df)\nmask[np.triu_indices_from(mask)] = True\n\n# Making a plot\nplt.figure(figsize=(16,12))\nax = sns.heatmap(df, annot=False, mask=mask, cmap=\"RdBu\", annot_kws={\"weight\": \"bold\", \"fontsize\": 9})\nax.set_title(\"Original and engineered features correlation heatmap\", fontsize=17)\nplt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n         rotation_mode=\"anchor\", weight=\"bold\")\nplt.setp(ax.get_yticklabels(), weight=\"bold\")\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-07-29T06:09:58.12044Z","iopub.execute_input":"2021-07-29T06:09:58.120755Z","iopub.status.idle":"2021-07-29T06:10:00.353682Z","shell.execute_reply.started":"2021-07-29T06:09:58.120724Z","shell.execute_reply":"2021-07-29T06:10:00.352517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Machine learning**","metadata":{}},{"cell_type":"markdown","source":"The datetime to int conversion shown below was found in this [notebook](https://www.kaggle.com/jarupula/eda-rf-model-tps-july-21).","metadata":{}},{"cell_type":"code","source":"def prepare_dataset(train_copy, test_copy, i):\n\n    X = add_new_ml_features(train_copy.copy(), i)\n\n    # Dropping the last row which is 2011-01-01 00:00:00\n    if X.index[-1] == 7110:\n        X.drop([7110], axis=0, inplace=True)\n\n    # Resetting dataframe index\n    X.reset_index(drop=True, inplace=True)\n\n    X_test = add_new_ml_features(test_copy.copy(), i)\n\n    y = np.log1p(X[[\"target_carbon_monoxide\", \"target_benzene\", \"target_nitrogen_oxides\"]])\n    X.drop([\"target_carbon_monoxide\", \"target_benzene\", \"target_nitrogen_oxides\"], axis=1, inplace=True)\n    \n    X['date_time'] = X['date_time'].astype('datetime64[ns]').astype(np.int64)/10**9\n    X_test['date_time'] = X_test['date_time'].astype('datetime64[ns]').astype(np.int64)/10**9\n\n#     display(X.head())\n#     display(X_test.head())\n#     display(y.head())\n\n    # features = X.drop(\"date_time\", axis=1).columns\n    # X_test[features] = np.log1p(X_test[features])\n    # X[features] = np.log1p(X[features])\n    \n    return X, X_test, y","metadata":{"execution":{"iopub.status.busy":"2021-07-29T06:10:00.354895Z","iopub.execute_input":"2021-07-29T06:10:00.355319Z","iopub.status.idle":"2021-07-29T06:10:00.363034Z","shell.execute_reply.started":"2021-07-29T06:10:00.355286Z","shell.execute_reply":"2021-07-29T06:10:00.362268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sets of hyperparameters optimized by Optuna for each target\n# cb_params = [\n#                 {'learning_rate': 0.010169009412219588,\n#                  'l2_leaf_reg': 8.908337085912136,\n#                  'bagging_temperature': 8.384477224270551,\n#                  'random_strength': 1.950237493637981,\n#                  'depth': 6,\n#                  'grow_policy': 'Lossguide',\n#                  'leaf_estimation_method': 'Newton'},\n#                 {'learning_rate': 0.166394867169309,\n#                  'l2_leaf_reg': 8.704675157564441,\n#                  'bagging_temperature': 3.340826164726799,\n#                  'random_strength': 1.538518016574368,\n#                  'depth': 2,\n#                  'grow_policy': 'Depthwise',\n#                  'leaf_estimation_method': 'Newton'},\n#                 {'learning_rate': 0.028141156076957437,\n#                  'l2_leaf_reg': 3.116523267336638,\n#                  'bagging_temperature': 4.420661209459851,\n#                  'random_strength': 1.8011752694610028,\n#                  'depth': 6,\n#                  'grow_policy': 'Depthwise',\n#                  'leaf_estimation_method': 'Newton'},\n#             ]\ncb_params = [\n                {'learning_rate': 0.04094650317955774,\n                 'l2_leaf_reg': 8.555213318408395,\n                 'bagging_temperature': 4.188124681571345,\n                 'random_strength': 1.444399265342111,\n                 'depth': 8,\n                 'grow_policy': 'Lossguide',\n                 'leaf_estimation_method': 'Gradient'},\n                {'learning_rate': 0.010499552543881853,\n                 'l2_leaf_reg': 2.630654006362146,\n                 'bagging_temperature': 4.824439111895089,\n                 'random_strength': 1.3480005087465852,\n                 'depth': 4,\n                 'grow_policy': 'Lossguide',\n                 'leaf_estimation_method': 'Newton'},\n               {'learning_rate': 0.010202325317933652,\n                'l2_leaf_reg': 0.9134009064920859,\n                'bagging_temperature': 8.535456442729302,\n                'random_strength': 1.353469950151128,\n                'depth': 10,\n                'grow_policy': 'Lossguide',\n                'leaf_estimation_method': 'Newton'},\n            ]\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-29T06:10:00.364187Z","iopub.execute_input":"2021-07-29T06:10:00.364634Z","iopub.status.idle":"2021-07-29T06:10:00.384286Z","shell.execute_reply.started":"2021-07-29T06:10:00.364601Z","shell.execute_reply":"2021-07-29T06:10:00.383486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# A list of feature importances\nall_fi = []\n\nsplits = 10\n\n# Initializing and filling predictions dataframe with datetime values\npreds = pd.DataFrame()\npreds[\"date_time\"] = test_copy[\"date_time\"].copy()\n\n# The months will be used for folds split\nmonths = train_copy.drop(7110, axis=0)[\"date_time\"].dt.month\n\ntotal_mean_rmsle = 0\n\nX = pd.DataFrame()\n\ntrain_preds_df = pd.DataFrame(index=np.arange(7110))\ntest_preds_df = pd.DataFrame(index=test.index)\n\nfor i, target in enumerate(targets):\n    print(f\"\\nTraining for {target}...\")\n\n    # Getting dataset for a current target in case different datasets should be used for different targets\n    X, X_test, y = prepare_dataset(pd.concat([train_copy, train_preds_df], axis=1), pd.concat([test_copy, test_preds_df], axis=1), i)\n    \n#     skf = StratifiedKFold(n_splits=splits, shuffle=True, random_state=42)\n    skf = LeaveOneGroupOut()\n    oof_preds = np.zeros((X.shape[0],))\n    model_preds = 0\n    model_fi = 0\n    for num, (train_idx, valid_idx) in enumerate(skf.split(X=X, groups=months)):\n        X_train, X_valid = X.loc[train_idx], X.loc[valid_idx]\n        y_train, y_valid = y.loc[train_idx, target], y.loc[valid_idx, target]\n        model = CatBoostRegressor(random_state=42,\n                                 thread_count=4,\n                                 verbose=False,\n                                 loss_function='RMSE',\n                                 eval_metric='RMSE',\n                                 od_type=\"Iter\",\n                                 early_stopping_rounds=500,\n                                 use_best_model=True,\n                                 iterations=10000,\n                                 **cb_params[i])\n        model.fit(X_train, y_train,\n                  eval_set=(X_valid, y_valid),\n                  verbose=False,\n                  cat_features=[\"working_hours\", \"is_weekend\", \"maximum_hours\"])\n        model_preds += np.expm1(model.predict(X_test)) / splits\n        model_fi += model.feature_importances_\n        oof_preds[valid_idx] = np.expm1(model.predict(X_valid))\n        oof_preds[oof_preds < 0] = 0\n        print(f\"Fold {num} RMSLE: {np.sqrt(mean_squared_log_error(np.expm1(y_valid), oof_preds[valid_idx]))}\")\n#         print(f\"Trees: {model.tree_count_}\")\n\n    train_preds_df[target+\"_preds\"] = np.log1p(oof_preds)\n    test_preds_df[target+\"_preds\"] = np.log1p(model_preds)\n    \n    target_rmsle = np.sqrt(mean_squared_log_error(np.expm1(y[target]), oof_preds))\n    total_mean_rmsle += target_rmsle / len(targets)\n    print(f\"\\nOverall {target} RMSLE: {target_rmsle}\")    \n    preds[target] = model_preds\n    all_fi.append(dict(zip(X_test.columns, model_fi)))\nprint(f\"\\n\\nTotal RMSLE is {total_mean_rmsle}\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-07-29T06:10:00.387629Z","iopub.execute_input":"2021-07-29T06:10:00.388078Z","iopub.status.idle":"2021-07-29T06:10:29.030404Z","shell.execute_reply.started":"2021-07-29T06:10:00.388045Z","shell.execute_reply":"2021-07-29T06:10:29.029488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Feature importances**","metadata":{}},{"cell_type":"code","source":"# Creating feature list from feature importance dictionaries\nfeature_list = set()\nfor i in np.arange(len(all_fi)):\n    feature_list = set.union(feature_list, set(all_fi[i].keys()))\nprint(f\"There are {len(feature_list)} unique features used for training: {feature_list}\")","metadata":{"execution":{"iopub.status.busy":"2021-07-29T06:10:29.032247Z","iopub.execute_input":"2021-07-29T06:10:29.033124Z","iopub.status.idle":"2021-07-29T06:10:29.040319Z","shell.execute_reply.started":"2021-07-29T06:10:29.033085Z","shell.execute_reply":"2021-07-29T06:10:29.03911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Combining feature importances of different models into one dataframe\ndf = pd.DataFrame(columns=[\"Feature\"])\ndf[\"Feature\"] = list(feature_list)\nfor i in np.arange(len(all_fi)):\n    for key in all_fi[i].keys():\n        df.loc[df[\"Feature\"] == key, \"Importance_\" + str(i+1)] = all_fi[i][key] / 1000\ndf.fillna(0, inplace=True)\ndf[\"Overall_importance\"] = df[\"Importance_1\"] + df[\"Importance_2\"] + df[\"Importance_3\"]\ndf.sort_values(\"Overall_importance\", ascending=False, inplace=True)\ndf.reset_index(drop=True, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T06:10:29.041806Z","iopub.execute_input":"2021-07-29T06:10:29.04213Z","iopub.status.idle":"2021-07-29T06:10:29.143926Z","shell.execute_reply.started":"2021-07-29T06:10:29.042099Z","shell.execute_reply":"2021-07-29T06:10:29.142108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = np.arange(0, len(df[\"Feature\"]))\nheight = 0.3\n\nfig, ax = plt.subplots(figsize=(12, 50))\nbars1 = ax.barh(x-height, df[\"Importance_1\"], height=height,\n                color=\"cornflowerblue\",\n                edgecolor=\"black\",\n                label=target_names[0])\nbars2 = ax.barh(x, df[\"Importance_2\"], height=height,\n                color=\"palevioletred\",\n                edgecolor=\"black\",\n                label=target_names[1])\nbars3 = ax.barh(x+height, df[\"Importance_3\"], height=height,\n                color=\"mediumseagreen\",\n                edgecolor=\"black\",\n                label=target_names[2])\nax.set_title(\"Feature importances\", fontsize=20, pad=5)\nax.set_ylabel(\"Feature names\", fontsize=15, labelpad=5)\nax.set_xlabel(\"Feature importance\", fontsize=15, labelpad=5)\nax.set_yticks(x)\nax.set_yticklabels(df[\"Feature\"], fontsize=12)\n# ax.set_xlim(0, 0.25)\n# ax.set_xticks(np.arange(0, 0.275, 0.025))\nax.tick_params(axis=\"x\", labelsize=12)\nax.grid(axis=\"x\")\n# ax2 = ax.secondary_xaxis('top')\n# ax2.set_xlabel(\"Feature importance\", fontsize=20, labelpad=15)\n# # ax2.set_xlim(0, 0.25)\n# # ax2.set_xticks(np.arange(0, 0.275, 0.025))\n# ax2.tick_params(axis=\"x\", labelsize=15)\n# ax.legend(legend_lines, [\"Original features\", \"Custom features\"], fontsize=15, loc=1, bbox_to_anchor=(0, 0, 1, 0.97))\nax.legend(fontsize=13, loc=\"lower right\")\nplt.margins(0.04, 0.01)\nplt.gca().invert_yaxis()","metadata":{"execution":{"iopub.status.busy":"2021-07-29T06:10:29.145043Z","iopub.status.idle":"2021-07-29T06:10:29.145733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Predictions analysis and submission**","metadata":{}},{"cell_type":"code","source":"preds.to_csv('submission.csv', index=False)\npreds.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-29T06:10:29.14676Z","iopub.status.idle":"2021-07-29T06:10:29.147406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's compare predictions with the closest months from the train datasets.","metadata":{}},{"cell_type":"code","source":"targets = train_copy[[\"target_carbon_monoxide\", \"target_benzene\", \"target_nitrogen_oxides\"]]","metadata":{"execution":{"iopub.status.busy":"2021-07-29T06:10:29.148646Z","iopub.status.idle":"2021-07-29T06:10:29.149469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(ncols=1, nrows=3, figsize=(16, 8))\nplt.set_cmap(\"Set2\")\nplt.subplots_adjust(hspace = 0.3)\n\nfor i, target in enumerate(targets.columns):\n    axs[i].plot(np.arange(0, 744, 1), targets.loc[train[\"month\"]==12, target], label=\"Train, 12th month\")\n    axs[i].plot(np.arange(0, 744, 1), preds.loc[preds[\"date_time\"].dt.month==1, target],\n                label=\"Test, 1th month\")\n    axs[i].set_title(target_names[i], fontsize=15)\n    axs[i].legend(fontsize=13)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T06:10:29.151009Z","iopub.status.idle":"2021-07-29T06:10:29.151908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(ncols=1, nrows=3, figsize=(16, 8))\nplt.set_cmap(\"Set2\")\nplt.subplots_adjust(hspace = 0.3)\n\nfor i, target in enumerate(targets.columns):\n    axs[i].plot(np.arange(0, 720, 1), targets.loc[train[\"month\"]==11, target], label=\"Train, 11th month\")\n    axs[i].plot(np.arange(0, 744, 1), preds.loc[preds[\"date_time\"].dt.month==1, target],\n                label=\"Test, 1th month\")\n    axs[i].set_title(target_names[i], fontsize=15)\n    axs[i].legend(fontsize=13)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T06:10:29.153172Z","iopub.status.idle":"2021-07-29T06:10:29.15403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(ncols=1, nrows=3, figsize=(16, 8))\nplt.set_cmap(\"Set2\")\nplt.subplots_adjust(hspace = 0.3)\n\nfor i, target in enumerate(targets.columns):\n    axs[i].plot(np.arange(0, 598, 1), targets.loc[:597, target], label=\"Train, from 10.3 to 4.4\")\n    axs[i].plot(np.arange(0, 596, 1), preds.loc[1651: , target],\n                label=\"Test, from 10.3 to 4.4\")\n    axs[i].set_title(target_names[i], fontsize=15)\n    axs[i].legend(fontsize=13)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T06:10:29.155211Z","iopub.status.idle":"2021-07-29T06:10:29.156037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see, the predictions are the closest to the training set in the overlapping months (from March 10 to April 4). ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}