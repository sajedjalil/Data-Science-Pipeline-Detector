{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# INFO ABOUT COMPETITION\n![books](https://cdn.pixabay.com/photo/2016/08/24/16/20/books-1617327_960_720.jpg)\n<br>Image by Marisa Sias from Pixabay\n\n### Goal \nThe goal is to build algorithms to rate the complexity of reading passages for grade 3-12 classroom use. \n\n### Data\nFiles <br>\n* train.csv - the training set\n* test.csv - the test set\n* sample_submission.csv - a sample submission file in the correct format<br>\n\nColumns<br>\n* id - unique ID for excerpt\n* url_legal - URL of source - this is blank in the test set.\n* license - license of source material - this is blank in the test set.\n* excerpt - text to predict reading ease of\n* target - reading ease\n* standard_error - measure of spread of scores among multiple raters for each excerpt. Not included for test data.<br>\n\nNote: \nData includes excerpts from several time periods and a wide range of reading ease scores.<br>\nTest set includes a slightly larger proportion of modern texts (the type of texts model should generalize to) than the training set.<br>\nWhile licensing information is provided for the public test set, the hidden private test set includes only blank license/legal information.\n\n### Evaluation\nSubmissions are scored on the RMSE - root mean squared error. <br>\n* example of submission file:<br>\nid,target<br>\neaf8e7355,0.0<br>\n60ecc9777,0.5<br>\nc0f722661,-2.0<br>\netc.<br>","metadata":{}},{"cell_type":"markdown","source":"# IMPORTS","metadata":{}},{"cell_type":"code","source":"# basic imports \nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt # visualization\nimport seaborn as sns # visualization\nsns.set()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# image manipulation for word cloud\nfrom PIL import Image \nfrom wordcloud import WordCloud\nfrom nltk.corpus import stopwords\nSTOP_WORDS = set(stopwords.words('english'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# neural networks\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, BatchNormalization","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# getting image from url\nfrom io import BytesIO \nimport requests","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  Vectorizer for text data - Counts and Tfidf\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import PCA","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing re for text preprocessing\nimport re","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# spacy for text preprocessing (lemmatization, tokenization, NER, POS)\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extreme Gradient Boosting Models\nimport xgboost as xgb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Imports from sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, make_scorer\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.linear_model import Lasso, Ridge\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CONSTANTS","metadata":{}},{"cell_type":"markdown","source":"#### Paths to files with data and image for mask in wordcloud. Dictionary with NER and POS Tags for Spacy","metadata":{}},{"cell_type":"code","source":"# data paths\ntrain_path = '../input/commonlitreadabilityprize/train.csv'\ntest_path = '../input/commonlitreadabilityprize/test.csv'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# images to use as mask in wordclouds generator\nbook_path = 'https://cdn.pixabay.com/photo/2013/04/01/21/30/book-99132_960_720.png'\nbook_path_2 = 'https://cdn.icon-icons.com/icons2/2622/PNG/512/book_icon_158035.png'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dictionary of POS Tags and NER Tags along with explanation used by spacy\nGLOSSARY_POS = {\n    # POS tags\n    # Universal POS Tags\n    # http://universaldependencies.org/u/pos/\n    \"ADJ\": \"adjective\",\n    \"ADP\": \"adposition\",\n    \"ADV\": \"adverb\",\n    \"AUX\": \"auxiliary\",\n    \"CONJ\": \"conjunction\",\n    \"CCONJ\": \"coordinating conjunction\",\n    \"DET\": \"determiner\",\n    \"INTJ\": \"interjection\",\n    \"NOUN\": \"noun\",\n    \"NUM\": \"numeral\",\n    \"PART\": \"particle\",\n    \"PRON\": \"pronoun\",\n    \"PROPN\": \"proper noun\",\n    \"PUNCT\": \"punctuation\",\n    \"SCONJ\": \"subordinating conjunction\",\n    \"SYM\": \"symbol\",\n    \"VERB\": \"verb\",\n    \"X\": \"other\",\n    \"EOL\": \"end of line\",\n    \"SPACE\": \"space\"}\n\nGLOSSARY_NER = {\n    # Named Entity Recognition\n    # OntoNotes 5\n    # https://catalog.ldc.upenn.edu/docs/LDC2013T19/OntoNotes-Release-5.0.pdf\n    \"PERSON\": \"People, including fictional\",\n    \"NORP\": \"Nationalities or religious or political groups\",\n    \"FACILITY\": \"Buildings, airports, highways, bridges, etc.\",\n    \"FAC\": \"Buildings, airports, highways, bridges, etc.\",\n    \"ORG\": \"Companies, agencies, institutions, etc.\",\n    \"GPE\": \"Countries, cities, states\",\n    \"LOC\": \"Non-GPE locations, mountain ranges, bodies of water\",\n    \"PRODUCT\": \"Objects, vehicles, foods, etc. (not services)\",\n    \"EVENT\": \"Named hurricanes, battles, wars, sports events, etc.\",\n    \"WORK_OF_ART\": \"Titles of books, songs, etc.\",\n    \"LAW\": \"Named documents made into laws.\",\n    \"LANGUAGE\": \"Any named language\",\n    \"DATE\": \"Absolute or relative dates or periods\",\n    \"TIME\": \"Times smaller than a day\",\n    \"PERCENT\": 'Percentage, including \"%\"',\n    \"MONEY\": \"Monetary values, including unit\",\n    \"QUANTITY\": \"Measurements, as of weight or distance\",\n    \"ORDINAL\": '\"first\", \"second\", etc.',\n    \"CARDINAL\": \"Numerals that do not fall under another type\",\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#dictionary with english contractions like don't isn't for function explanding contractions\ncontractions = { \n\"ain't\": \"am not / are not / is not / has not / have not\",\n\"aren't\": \"are not / am not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he had / he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he shall / he will\",\n\"he'll've\": \"he shall have / he will have\",\n\"he's\": \"he has / he is\",\n\"how'd\": \"how did\",\n\"how'd'y\": \"how do you\",\n\"how'll\": \"how will\",\n\"how's\": \"how has / how is / how does\",\n\"I'd\": \"I had / I would\",\n\"I'd've\": \"I would have\",\n\"I'll\": \"I shall / I will\",\n\"I'll've\": \"I shall have / I will have\",\n\"I'm\": \"I am\",\n\"I've\": \"I have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it had / it would\",\n\"it'd've\": \"it would have\",\n\"it'll\": \"it shall / it will\",\n\"it'll've\": \"it shall have / it will have\",\n\"it's\": \"it has / it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"mustn't've\": \"must not have\",\n\"needn't\": \"need not\",\n\"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\",\n\"oughtn't've\": \"ought not have\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"shan't've\": \"shall not have\",\n\"she'd\": \"she had / she would\",\n\"she'd've\": \"she would have\",\n\"she'll\": \"she shall / she will\",\n\"she'll've\": \"she shall have / she will have\",\n\"she's\": \"she has / she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"so've\": \"so have\",\n\"so's\": \"so as / so is\",\n\"that'd\": \"that would / that had\",\n\"that'd've\": \"that would have\",\n\"that's\": \"that has / that is\",\n\"there'd\": \"there had / there would\",\n\"there'd've\": \"there would have\",\n\"there's\": \"there has / there is\",\n\"they'd\": \"they had / they would\",\n\"they'd've\": \"they would have\",\n\"they'll\": \"they shall / they will\",\n\"they'll've\": \"they shall have / they will have\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"to've\": \"to have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we had / we would\",\n\"we'd've\": \"we would have\",\n\"we'll\": \"we will\",\n\"we'll've\": \"we will have\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what shall / what will\",\n\"what'll've\": \"what shall have / what will have\",\n\"what're\": \"what are\",\n\"what's\": \"what has / what is\",\n\"what've\": \"what have\",\n\"when's\": \"when has / when is\",\n\"when've\": \"when have\",\n\"where'd\": \"where did\",\n\"where's\": \"where has / where is\",\n\"where've\": \"where have\",\n\"who'll\": \"who shall / who will\",\n\"who'll've\": \"who shall have / who will have\",\n\"who's\": \"who has / who is\",\n\"who've\": \"who have\",\n\"why's\": \"why has / why is\",\n\"why've\": \"why have\",\n\"will've\": \"will have\",\n\"won't\": \"will not\",\n\"won't've\": \"will not have\",\n\"would've\": \"would have\",\n\"wouldn't\": \"would not\",\n\"wouldn't've\": \"would not have\",\n\"y'all\": \"you all\",\n\"y'all'd\": \"you all would\",\n\"y'all'd've\": \"you all would have\",\n\"y'all're\": \"you all are\",\n\"y'all've\": \"you all have\",\n\"you'd\": \"you had / you would\",\n\"you'd've\": \"you would have\",\n\"you'll\": \"you shall / you will\",\n\"you'll've\": \"you shall have / you will have\",\n\"you're\": \"you are\",\n\"you've\": \"you have\"\n}\n\ncontractions_re = re.compile('(%s)' % '|'.join(contractions.keys()))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# FUNCTIONS","metadata":{}},{"cell_type":"markdown","source":"#### Preparing functions that will make text processing and data exploration easier","metadata":{}},{"cell_type":"code","source":"# Retrieving website address from url_legal column\ndef clean_link(link):\n    \"\"\"Function that retrieves main website address from the link\"\"\"\n    if pd.isnull(link):\n        return link\n    \n    link = link.replace(\"https://\",'')\n    link = link.replace(\"http://\",'')\n    link = link.split('/')\n    if isinstance(link,list):\n        return link[0]\n    else:\n        return link","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reading image from url path for wordcloud generation\ndef read_img_from_url(url):\n    \"\"\"Returns np.array from url leading to image\"\"\"\n    response = requests.get(url)\n    img = Image.open(BytesIO(response.content))\n    img_matrix = np.array(img)\n    return img_matrix","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# constants and functions that allow counting syllables in the word\n\nVOWEL_RUNS = re.compile(\"[aeiouy]+\", flags=re.I)\nEXCEPTIONS = re.compile(\n    # fixes trailing e issues:\n    # smite, scared\n    \"[^aeiou]e[sd]?$|\"\n    # fixes adverbs:\n    # nicely\n    + \"[^e]ely$\",\n    flags=re.I\n)\nADDITIONAL = re.compile(\n    # fixes incorrect subtractions from exceptions:\n    # smile, scarred, raises, fated\n    \"[^aeioulr][lr]e[sd]?$|[csgz]es$|[td]ed$|\"\n    # fixes miscellaneous issues:\n    # flying, piano, video, prism, fire, evaluate\n    + \".y[aeiou]|ia(?!n$)|eo|ism$|[^aeiou]ire$|[^gq]ua\",\n    flags=re.I\n)\n\ndef count_syllables(word):\n    \"\"\"Returns number of syllables in the word based on string\"\"\"\n    vowel_runs = len(VOWEL_RUNS.findall(word))\n    exceptions = len(EXCEPTIONS.findall(word))\n    additional = len(ADDITIONAL.findall(word))\n    return max(1, vowel_runs - exceptions + additional)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cleaning text\ndef lemma_txt(doc):\n    \"Function that returns lowercase, lemmatized text without punctuation\"\n    lemma_list = [token.lemma_ for token in doc if token.is_alpha and token.lemma_ != '-PRON-']\n    return ' '.join(lemma_list)\n\n# Calculating basic text statistics\ndef counter(doc):    \n    \"\"\" Function that returns dictionary with text statistics:\n        - count of chars\n        - count of words\n        - count of sentences\n        - count of syllables\n        - avg. count of words per sentence\n        - avg. count of syllables per word\n        - count of difficult words (with more than 2 syllables)\n        - percentage of words with more than two syllables in the text\"\"\"\n    char_list = np.array([len(token.text) for token in doc if token.is_alpha])\n    syll_list = np.array([count_syllables(token.text) for token in doc if token.is_alpha])\n    word_count = len(char_list)\n    sent_count = len(list(doc.sents))\n    char_count = char_list.sum()\n    diff_word_count = np.sum(syll_list>3)\n    syll_count = syll_list.sum()\n    diff_word_perc = round(diff_word_count/word_count,2)*100\n    syll_per_word = round(syll_count/word_count,2)\n    word_per_sent = round(word_count/sent_count,2)\n    \n    counter_dict = {\n        'Words' : word_count,\n        'Sentences' : sent_count,\n        'Chars' : char_count,\n        'Syllables' : syll_count,\n        'Diff_Words' : diff_word_count,\n        'Diff_Words_Perc' : diff_word_perc,\n        'Words_Per_Sent' : word_per_sent,\n        'Syll_Per_Word' : syll_per_word,\n    }\n    \n    return counter_dict\n\n# Conting different Parts of Speech in text\ndef pos_counter(doc):\n    \"\"\"Functions that returns dictionary with count of different parts of speech in the text.\n    - POS tags based on spacy package\"\"\"\n    counts_dict = doc.count_by(spacy.attrs.IDS['TAG'])\n\n    pos_dict = {}\n    \n    for i in nlp.tokenizer.vocab.morphology.tag_map.keys():\n            if i != '_SP':\n                pos_dict[spacy.explain(i)]=0\n    \n    # Create dict with the human readable part of speech tags\n    for pos, count in counts_dict.items():\n        tag = spacy.explain(doc.vocab[pos].text)\n        pos_dict[tag] = count\n        \n    return pos_dict\n\n# Counting number of different named entities in text\ndef ner_counter(doc):\n    \"\"\"Functions that returns dictionary with count of different named entities in the text.\n    - NER tags based on spacy package\"\"\"\n    ner_dict = {}\n    for ner in GLOSSARY_NER:\n        ner_dict[ner]=0\n    \n    for ent in doc.ents:\n        ner_dict[ent.label_] += 1\n    \n    return ner_dict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#define function to expand contractions and showcase\ndef expand_contractions(s, contractions = contractions):\n    def replace(match):\n        return contractions[match.group(0)]\n    return contractions_re.sub(replace, s)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function that expands dataframe including text column with text statistics, POS counts, NER counts and \"Clean Text\"\ndef preprocess_text(df_data, txt_col='excerpt'):\n    \"\"\"Functions that preprocess text and expands DataFrame with relevant columns.\"\"\"\n    docs = nlp.pipe(df_data[txt_col].tolist())\n    df_data['DOCS']=[doc for doc in list(docs)]\n    df_data['Clean_Text']= df_data['DOCS'].apply(lemma_txt)\n    \n    df_data['Text_Stats']= df_data['DOCS'].apply(counter)\n    df_data['POS_Stats'] = df_data['DOCS'].apply(pos_counter)\n    df_data['NER_Stats'] = df_data['DOCS'].apply(ner_counter)\n    \n    dfs = [df_data.drop(labels=['Text_Stats','POS_Stats','NER_Stats','DOCS'],axis=1),\n           pd.DataFrame(df_data['Text_Stats'].tolist()),\n           pd.DataFrame(df_data['POS_Stats'].tolist()),\n           pd.DataFrame(df_data['NER_Stats'].tolist())]\n    \n    return pd.concat(dfs, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lists with columns created using preprocess_text function\nbasic_stats = ['Chars','Words','Syllables','Sentences','Syll_Per_Word','Words_Per_Sent','Diff_Words']\nPOS_stats = [spacy.explain(i) for i in nlp.tokenizer.vocab.morphology.tag_map.keys() if i != '_SP']\nNER_stats = list(GLOSSARY_NER.keys())\n\nall_stats = basic_stats + POS_stats + NER_stats","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LOADING DATA","metadata":{}},{"cell_type":"markdown","source":"#### Let's load the training data and take a look at first few rows. I will also check the data shape and if we are dealing with any nulls. ","metadata":{}},{"cell_type":"code","source":"# Loading data\ndata_train = pd.read_csv(train_path)\ndisplay(data_train.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking size of data\nprint('TRAIN DATA')\nprint('Samples:',data_train.shape[0])\nprint('Columns:',data_train.shape[1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking data types and amount of nulls\ndisplay(data_train.info())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Let's load the test data and also take a look in first few rows\n","metadata":{}},{"cell_type":"code","source":"# Loading test data and showing first 5 rows\ndata_test = pd.read_csv(test_path)\ndisplay(data_test.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking size of test data\nprint(\"TEST DATA:\")\nprint('Samples:',data_test.shape[0])\nprint('Columns:',data_test.shape[1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Preprocessing the train and creating columns with text statistics","metadata":{}},{"cell_type":"code","source":"# cleaning the text and preparing different statistics\ndata_train = preprocess_text(data_train)\n\n# Retrieving main website from url link\ndata_train['website'] = data_train['url_legal'].apply(clean_link)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EXPLORATORY DATA ANALYSIS","metadata":{}},{"cell_type":"markdown","source":"#### I will start the exploaratory analysis by reviewing the source website and license of available texts","metadata":{}},{"cell_type":"code","source":"# Count of source websites and license for texts in training data\nfig, axis = plt.subplots(2, figsize=(10,12))\n\nsns.countplot(y='website',hue='website', data=data_train, dodge=False, ax=axis[0])\naxis[0].set_title('Source Website Count',fontsize=16)\naxis[0].get_legend().remove()\n\nsns.countplot(y='license', hue='license',data=data_train, dodge=False, ax=axis[1])\naxis[1].set_title('License Count',fontsize=16)\naxis[1].get_legend().remove()\n\nfig.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Then I will explore target distribution and standard error","metadata":{}},{"cell_type":"code","source":"# Inspecting targer variable and standard error\ndisplay(data_train[['target','standard_error']].describe())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualizing Target and Standard Error\nfig, axis = plt.subplots(1,3,figsize=(14,5))\n\nsns.histplot(x='target',kde=True, data=data_train,bins=100, ax=axis[0])\naxis[0].set_title('Target Distribution', fontsize=16)\n\nsns.histplot(x='standard_error',kde=True, data=data_train, ax=axis[1], color='darkred')\naxis[1].set_title('Standard Error Distribution', fontsize=16)\n\nsns.histplot(x='standard_error',kde=True, data=data_train.query('standard_error > 0.01'), ax=axis[2], color='darkred')\naxis[2].set_title('Standard Error Distribution', fontsize=16)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems like in standard error there is one untypical observation equal 0 and rest of them lays between 0.4 and 0.7. <br> \nTarget have distribution close to normal with mean -1 and is slightly skewed towards lower values","metadata":{}},{"cell_type":"code","source":"print('Std error above 0:',data_train[data_train['standard_error']>0].shape[0], 'samples')\nprint('Std error equal or below 0:',data_train[data_train['standard_error']<=0].shape[0], 'samples')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Let's review relationship between target and std error. It seems like the biggest std error is on the extremes.","metadata":{}},{"cell_type":"code","source":"# Plot of standard_error versus target - points colored by length of text\nfig, ax = plt.subplots(figsize=(7,7))\nsns.scatterplot(x='target', y='standard_error',hue='Chars', data=data_train,\n                alpha=0.5, ax=ax, palette='viridis_r')\nax.set_title('Standard Error vs Target', fontsize=16)\nax.set_ylim([0.4,0.7])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Let's review the examples for the lowest and highest values of target ","metadata":{}},{"cell_type":"code","source":"# The most difficult to read text\ndisplay(data_train.sort_values(by='target')[['target','excerpt']].head(1).values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The easiest to read text\ndisplay(data_train.sort_values(by='target', ascending=False)[['target','excerpt']].head(1).values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Let's take a look at correlation between target and statistics ","metadata":{}},{"cell_type":"code","source":"# Correlation between POS stats and target\nfig,ax = plt.subplots(figsize=(4,8))\nax.set_title('Correlation of Text Stats and Target', fontsize=16)\nsns.heatmap(data_train[basic_stats+['target']].corr()[['target']].sort_values(by='target'),\n            annot=True, fmt='.2f',\n            vmin=-1, vmax=1,\n            cmap='RdBu_r',\n            ax=ax)\nfig.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Correlation between POS stats and target\nfig,ax = plt.subplots(figsize=(3,20))\nax.set_title('Correlation of POS and Target', fontsize=16)\nsns.heatmap(data_train[POS_stats+['target']].corr()[['target']].sort_values(by='target'),\n            annot=True, fmt='.2f',\n            vmin=-1, vmax=1,\n            cmap='RdBu_r',\n            ax=ax)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Correlation between NER stats and target\nfig,ax = plt.subplots(figsize=(3,10))\nax.set_title('Correlation of NER and Target', fontsize=16)\nsns.heatmap(data_train[NER_stats+['target']].corr()[['target']].sort_values(by='target'),\n            annot=True, fmt='.2f',\n            vmin=-1, vmax=1,\n            cmap='RdBu_r',\n            ax=ax)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Let's plot relationship between basic text stats and target and review their distribution","metadata":{}},{"cell_type":"code","source":"# Plot of text length versus target \nfig, ax = plt.subplots( figsize=(5,5))\n\nfig.suptitle(\"Text Statistics\", fontsize=18)\n\nsns.scatterplot(y='Words_Per_Sent', x='Syll_Per_Word',hue='target',\n                data=data_train, ax=ax)\nax.set_title('Avg. Word Length vs Avg Sentence Length', fontsize=16)\n\nfig.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# statistics of text data\ndisplay(data_train[basic_stats].describe())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Variance over mean - index of dispersion\n(data_train[basic_stats].var()/data_train[basic_stats].mean()).abs()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Variance over mean - index of dispersion\nfig, ax = plt.subplots()\nsns.barplot(x=(data_train[basic_stats].var()/data_train[basic_stats].mean()).abs(),ax=ax,\n                  y=basic_stats)\nfig.suptitle(\"Index of Dispersion - Variance Over Mean\", fontsize=16)\nax.set_yticklabels(labels=basic_stats)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,ax = plt.subplots(1,2, figsize=(10,5))\n\nfig.suptitle('Distribution of Avg. Word and Sentence Length in Texts', fontsize=18)\nax[0].set_title('Avg. Sentence Length', fontsize=16)\nsns.histplot(x='Words_Per_Sent', data=data_train, bins=100,kde=True, ax=ax[0], color='green')\n\nax[1].set_title('Avg. Word Length', fontsize=16)\nsns.histplot(x='Syll_Per_Word', data=data_train,bins=100,kde=True, ax=ax[1], color='blue')\n\nfig.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,ax = plt.subplots(1,4, figsize=(20,5))\n\nfig.suptitle('Distribution of Chars, Words and Sentences Count in Texts', fontsize=18)\n\nax[0].set_title('Word Count', fontsize=16)\nsns.histplot(x='Words', data=data_train, bins=40,kde=True, ax=ax[0], color='red')\n\nax[1].set_title('Sentence Count', fontsize=16)\nsns.histplot(x='Sentences', data=data_train, bins=40,kde=True, ax=ax[1], color='yellow')\n\nax[2].set_title('Syllable Count', fontsize=16)\nsns.histplot(x='Syllables', data=data_train, bins=40,kde=True, ax=ax[2], color='violet')\n\nax[3].set_title('Char Count', fontsize=16)\nsns.histplot(x='Chars', data=data_train, bins=40,kde=True, ax=ax[3], color='blue')\n\nfig.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TEXT CONTENT","metadata":{}},{"cell_type":"markdown","source":"#### Now I'm gonna take closer look at content of texts - calculating word frequency and POS, NER frequency for whole corpus","metadata":{}},{"cell_type":"markdown","source":"## Word Clouds with word frequency","metadata":{}},{"cell_type":"markdown","source":"Word Cloud is useful and pretty way to see most common words","metadata":{}},{"cell_type":"code","source":"# Joining whole corpus to generate wordcloud\nwc_data = ' '.join(data_train['Clean_Text'].tolist()).upper()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# instantiate a word cloud object\nexcerpt_cloud = WordCloud(\n    background_color='white',\n    max_words=2000,\n    stopwords=stopwords.words('english'),\n)\n# generate the word cloud\nexcerpt_cloud.generate(wc_data);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# display the word cloud\nfig, ax = plt.subplots(figsize=(14,7))\nfig.suptitle('Word Cloud with Most Frequent Words', fontsize=20)\nplt.imshow(excerpt_cloud, interpolation='bilinear')\nplt.axis('off')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Word Frequency","metadata":{}},{"cell_type":"markdown","source":"#### Let's calculate frequency for different ngrams","metadata":{}},{"cell_type":"code","source":"# Calculating word frequency - top 20 unigrams, bigrams and trigrams\n\n# Unigrams\nCV = CountVectorizer(stop_words=stopwords.words('english'),\n                    ngram_range=(1,1),\n                    max_features=20)\nCV_excerpt = CV.fit_transform([wc_data])\ntop_uni = pd.DataFrame({'unigram':CV.get_feature_names(),'count':CV_excerpt.toarray()[0]})\n\n# Bigrams\nCV = CountVectorizer(stop_words=stopwords.words('english'),\n                    ngram_range=(2,2),\n                    max_features=20)\nCV_excerpt = CV.fit_transform([wc_data])\ntop_bi = pd.DataFrame({'bigram':CV.get_feature_names(),'count':CV_excerpt.toarray()[0]})\n\n# Trigrams\nCV = CountVectorizer(stop_words=stopwords.words('english'),\n                    ngram_range=(3,3),\n                    max_features=20)\nCV_excerpt = CV.fit_transform([wc_data])\ntop_tri = pd.DataFrame({'trigram':CV.get_feature_names(),'count':CV_excerpt.toarray()[0]})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(1,3, figsize=(16,7))\nfig.suptitle('Most Frequent n-grams', fontsize=20)\n\nsns.barplot(y='unigram',x='count', data=top_uni.sort_values(by='count'), palette='viridis', ax=ax[0])\nax[0].set_title('Unigrams', fontsize=16)\nsns.barplot(y='bigram',x='count', data=top_bi.sort_values(by='count'), palette='magma', ax=ax[1])\nax[1].set_title('Bigrams', fontsize=16)\nsns.barplot(y='trigram',x='count', data=top_tri.sort_values(by='count'), palette='inferno', ax=ax[2])\nax[2].set_title('Trigrams', fontsize=16)\n\nfig.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Part of Speech Statistics","metadata":{}},{"cell_type":"code","source":"# Display POS Statistics for dataset\nfig, ax = plt.subplots(figsize=(10,14))\nax.set_title('Part of Speech Statistics for Training Data', fontsize=16)\ndata_train[POS_stats].sum().sort_values().plot.barh(ax=ax)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Named Entity Statistics","metadata":{}},{"cell_type":"code","source":"# Display NER Statistics for dataset\nfig, ax = plt.subplots(figsize=(10,12))\nfig.suptitle('Part of Speech Statistics for Training Data')\ndata_train[NER_stats].sum().sort_values().plot.barh(ax=ax)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's calculate sample weights based on standard error\ndata_train['sample_weight']= 1.6 - data_train['standard_error']\n\nsns.scatterplot(y='sample_weight', x='standard_error', data=data_train)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# NEURAL NETWORK","metadata":{}},{"cell_type":"markdown","source":"#### Building basic neural network that will consider text stats and Tfidf Vectors","metadata":{}},{"cell_type":"code","source":"# Preparing Tfidf Vectorizer\nTfidf = TfidfVectorizer(stop_words = stopwords.words('english'), max_df=0.995, min_df=0.005)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Splitting the data into train and test\nX = data_train[basic_stats+NER_stats+POS_stats+['Clean_Text']]\ny = data_train[['target','sample_weight']]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fitting vectorizer\ntfidf_train = Tfidf.fit_transform(X_train['Clean_Text'])\ntfidf_test = Tfidf.transform(X_test['Clean_Text'])\n\n# Preparing dataframes with features\ntf_train_df = pd.concat([X_train.drop(labels='Clean_Text', axis=1),\n                         pd.DataFrame(tfidf_train.toarray(),index=X_train.index, columns=Tfidf.get_feature_names())],\n                         ignore_index=True,axis=1)\n\ntf_test_df = pd.concat([X_test.drop(labels='Clean_Text', axis=1),\n                        pd.DataFrame(tfidf_test.toarray(), index=X_test.index, columns=Tfidf.get_feature_names())],\n                       ignore_index=True, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scale = StandardScaler()\npca = PCA(n_components=0.99)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_sc = scale.fit_transform(X_train[all_stats])\ntrain_pca = pca.fit_transform(train_sc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_sc = scale.transform(X_test[all_stats])\ntest_pca = pca.transform(test_sc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_train = np.hstack([train_pca, tfidf_train.toarray()])\nmy_test = np.hstack([test_pca, tfidf_test.toarray()])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.bar(x=range(pca.explained_variance_ratio_.shape[0]), height=pca.explained_variance_ratio_)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Building simple Sequential NN model\nmodel = Sequential()\nmodel.add(Dense(256, activation='relu', input_shape=(my_train.shape[1],)))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.25))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.25))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.25))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.25))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.25))\nmodel.add(Dense(256, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compiling the model with adam optimizer and huber loss\nmodel.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001),\n              loss=tf.keras.losses.huber,\n              metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse')])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This callback will reduce learning rate if the model will get stuck\ncallbacks = [tf.keras.callbacks.ReduceLROnPlateau(patience=5, factor=0.5),\n             tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training the model\nhist = model.fit(my_train, y_train['target'],\n          batch_size=200,\n          validation_data=(my_test, y_test['target']),\n          sample_weight= y_train['sample_weight'],\n          callbacks=callbacks,\n          verbose=1,\n          epochs=200)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics_v = pd.DataFrame(hist.history)[['rmse','val_rmse']]\nmetrics_v.plot()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_v = pd.DataFrame(hist.history)[['loss','val_loss']]\nloss_v.plot()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(my_test).reshape(-1)\nRMSE = np.sqrt(mean_squared_error(y_test['target'], y_pred))\nprint('RMSE: ',RMSE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,ax = plt.subplots()\nsns.scatterplot(x=y_test['target'],y=y_pred, ax =ax , alpha=0.5)\nax.plot([-3.5,0,1.75],[-3.5,0,1.75],color='darkred')\nax.set_xlim([-4,2])\nax.set_ylim([-4,2])\nplt.axis('Equal')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.jointplot(x=y_test['target'],y=y_pred, kind='resid')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DUMMY REGRESSOR","metadata":{}},{"cell_type":"markdown","source":"#### For comparison I created dummy regressor which predicts mean","metadata":{}},{"cell_type":"code","source":"tr_mean = y_train['target'].mean()\ny_brute_pred = np.ones_like(y_pred)*tr_mean","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"RMSE = np.sqrt(mean_squared_error(y_brute_pred, y_pred))\nprint('RMSE: ',RMSE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# GENERATING SUBMISSION FILES","metadata":{}},{"cell_type":"code","source":"# Loading test data\ndata_test = pd.read_csv(test_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predicting based on test set\n\n# Text preprocessing\ndata_test = preprocess_text(data_test)\n\n# Generating tfidf\nX_new = data_test[basic_stats+NER_stats+POS_stats+['Clean_Text']]\nnew_test = Tfidf.transform(X_new['Clean_Text'])\n\n# Preparing dataframes with features\nnew_sc = scale.transform(X_new[all_stats])\nnew_pca = pca.transform(new_sc)\n\nnew_feat = np.hstack([new_pca, new_test.toarray()])\n\npreds = model.predict(new_feat).reshape(-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generating submission file\ndata_test['target']  = np.round(preds,2)\ndata_test[['id','target']].to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_test[['id','target']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}