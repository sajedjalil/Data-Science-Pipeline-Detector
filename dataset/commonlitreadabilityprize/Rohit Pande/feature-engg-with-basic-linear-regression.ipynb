{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# CommonLit Readability\nThe object of this kernel is to investigate feature engineering methods for this dataset rather than attaining a high RMSE score. This is implemented primarily through two packages, namely `textstat` and `nltk`. Features identified are contextual and reflect the reading levels, difficulty levels and parts of speech for each record. The text data is thereby converted into tabular format to allow the implementation of shallow learning algorithms like Linear Regression. This method provides an RMSE score of 0.7201, an MSE score of 0.5186 and an $R^2$ score of 0.5047.\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":false}},{"cell_type":"markdown","source":"# Initialisation\n## Imports","metadata":{}},{"cell_type":"code","source":"# Using the internet\n#!pip install textstat\n#!pip install rich\n\n# Without the internet\n#!pip download textstat -d ./textstat/\n#!pip download rich -d ./rich/","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import os\n# from zipfile import ZipFile\n\n# dirName = \"./\"\n# zipName = \"packages.zip\"\n\n# # Create a ZipFile Object\n# with ZipFile(zipName, 'w') as zipObj:\n#     # Iterate over all the files in directory\n#     for folderName, subfolders, filenames in os.walk(dirName):\n#         for filename in filenames:\n#             if (filename != zipName):\n#                 # create complete filepath of file in directory\n#                 filePath = os.path.join(folderName, filename)\n#                 # Add file to zip\n#                 zipObj.write(filePath)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install ../input/additionalpackages/textstat/Pyphen-0.10.0-py3-none-any.whl\n!pip install ../input/additionalpackages/textstat/textstat-0.7.0-py3-none-any.whl\n!pip install ../input/additionalpackages/rich/Pygments-2.9.0-py3-none-any.whl\n!pip install ../input/additionalpackages/rich/colorama-0.4.4-py2.py3-none-any.whl\n!pip install ../input/additionalpackages/rich/commonmark-0.9.1-py2.py3-none-any.whl\n!pip install ../input/additionalpackages/rich/rich-10.2.0-py3-none-any.whl\n!pip install ../input/additionalpackages/rich/typing_extensions-3.10.0.0-py3-none-any.whl","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport textstat\nimport rich\nimport nltk\nimport random\nimport missingno as msno\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing, tree, metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.utils import shuffle\n#from sklearn.metrics import mean_squared_error as mse\nfrom rich.console import Console\nfrom rich import print\nfrom rich.theme import Theme\n#from IPython.display import display","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Definitions\n\nI have defined my own metrics as I was having issues with sklearn's implementation.","metadata":{}},{"cell_type":"code","source":"def custom_palette(custom_colors):\n    customPalette = sns.set_palette(sns.color_palette(custom_colors))\n    sns.palplot(sns.color_palette(custom_colors),size=0.8)\n    plt.tick_params(axis='both', labelsize=0, length = 0)\n\npalette = [\"#7209B7\",\"#3F88C5\",\"#136F63\",\"#F72585\",\"#FFBA08\"]\npalette2 = sns.diverging_palette(120, 220, n=20)\ncustom_palette(palette)\n\ncustom_theme = Theme({\n    \"info\" : \"italic bold cyan\",\n    \"warning\": \"italic bold magenta\",\n    \"danger\": \"bold blue\"\n})\nconsole = Console(theme=custom_theme)\n\ndef rmse(x,y): \n    # Define Root Mean Squared Error calculation\n    return np.sqrt(((x-y)**2).mean())\n\ndef mse(x,y): \n    # Define Mean Squared Error calculation\n    return ((x-y)**2).mean()\n\ndef print_score(m):\n    # Define method to print the model score\n    res = [rmse(m.predict(X_train), y_train), \n           rmse(m.predict(X_test), y_test),\n           #rmse(m.predict(X_valid), y_valid),\n           mse(m.predict(X_train), y_train), \n           mse(m.predict(X_test), y_test),\n           m.score(X_train, y_train), \n           m.score(X_test, y_test),\n           #m.score(X_valid, y_valid)\n          ]\n    console.print(\"RMSE SCORES (lower the better):\",style=\"info\")\n    print('RMSE (train): {}\\nRMSE (test): {}'\n          .format(res[0],res[1]))\n    console.print(\"MSE SCORES (lower the better):\",style=\"info\")\n    print('MSE (train): {}\\nMSE (test): {}'\n          .format(res[2],res[3]))\n    console.print(\"R-square SCORES (higher the better):\",style=\"info\")\n    print('R-square (train): {}\\nR-square (test): {}'\n          .format(res[4],res[5]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Read data","metadata":{}},{"cell_type":"code","source":"DATA_DIR_INPUT = '/kaggle/input/commonlitreadabilityprize'\nDATA_DIR_OUTPUT = '/kaggle/working'\ndata_train = pd.read_csv(f'{DATA_DIR_INPUT}/train.csv')\ndata_val = pd.read_csv(f'{DATA_DIR_INPUT}/test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis\n## Missing values\nCheck if any missing values are present.","metadata":{}},{"cell_type":"code","source":"msno.matrix(data_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`url_legal` and `license` fields are being dropped as they are largely empty and do not add much information.","metadata":{}},{"cell_type":"code","source":"# Drop url_legal and licence columns from training data\ndata_train.drop(columns=['url_legal', 'license'], inplace=True)\ndata_val.drop(columns=['url_legal', 'license'], inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering\n## textstat\nThe `textstat` package helps in identifying a number of different readability indices. It also helps in the identification of statistics like the number of difficult words. As a result, all of the indices currently implemented within `textstat` are being added as individual features for each record.","metadata":{}},{"cell_type":"code","source":"def add_features(df):\n    df['lexicon_count'] = df.excerpt.apply(lambda x: textstat.lexicon_count(x))\n    df['flesch_reading_ease'] = df.excerpt.apply(lambda x: textstat.flesch_reading_ease(x))\n    df['flesch_kincaid_grade'] = df.excerpt.apply(lambda x: textstat.flesch_kincaid_grade(x))\n    df['smog_index'] = df.excerpt.apply(lambda x: textstat.smog_index(x))\n    df['coleman_liau_index'] = df.excerpt.apply(lambda x: textstat.coleman_liau_index(x))\n    df['automated_readability_index'] = df.excerpt.apply(lambda x: textstat.automated_readability_index(x))\n    df['dale_chall_readability_score'] = df.excerpt.apply(lambda x: textstat.dale_chall_readability_score(x))\n    df['linsear_write_formula'] = df.excerpt.apply(lambda x: textstat.linsear_write_formula(x))\n    df['gunning_fog'] = df.excerpt.apply(lambda x: textstat.gunning_fog(x))\n    df['fernandez_huerta'] = df.excerpt.apply(lambda x: textstat.fernandez_huerta(x))\n    df['szigriszt_pazos'] = df.excerpt.apply(lambda x: textstat.szigriszt_pazos(x))\n    df['gutierrez_polini'] = df.excerpt.apply(lambda x: textstat.gutierrez_polini(x))\n    df['spache_readability'] = df.excerpt.apply(lambda x: textstat.spache_readability(x))\n    df['crawford'] = df.excerpt.apply(lambda x: textstat.crawford(x))\n    df['difficult_words'] = df.excerpt.apply(lambda x: textstat.difficult_words(x))\n    df['syllable_count'] = df.excerpt.apply(lambda x: textstat.syllable_count(x))\n    df['sentence_count'] = df.excerpt.apply(lambda x: textstat.sentence_count(x))\n    df['polysyllabcount'] = df.excerpt.apply(lambda x: textstat.polysyllabcount(x))\n    df['char_count'] = df.excerpt.apply(lambda x: textstat.char_count(x))\n    df['letter_count'] = df.excerpt.apply(lambda x: textstat.letter_count(x))\n    df['avg_character_per_word'] = df.excerpt.apply(lambda x: textstat.avg_character_per_word(x))\n    df['avg_letter_per_word'] = df.excerpt.apply(lambda x: textstat.avg_letter_per_word(x))\n    df['avg_sentence_length'] = df.excerpt.apply(lambda x: textstat.avg_sentence_length(x))\n    df['avg_sentence_per_word'] = df.excerpt.apply(lambda x: textstat.avg_sentence_per_word(x))\n    df['avg_syllables_per_word'] = df.excerpt.apply(lambda x: textstat.avg_syllables_per_word(x))\n    df['lix'] = df.excerpt.apply(lambda x: textstat.lix(x))\n    df['rix'] = df.excerpt.apply(lambda x: textstat.rix(x))\n    df['reading_time'] = df.excerpt.apply(lambda x: textstat.reading_time(x))\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The cell below takes a long time to execute (approximately 22 minutes on the Kaggle kernel). Therefore, the output has been saved in the following cell. This is then loaded thereafter to save time.","metadata":{}},{"cell_type":"code","source":"# Run this only if needed, pickles have been provided in the cell below to save time.\n\n# # Add indices and paragraph features\n# %time df_train = add_features(data_train)\n# df_val = add_features(data_val)\n# #df_train.head().T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load / Save DFs","metadata":{}},{"cell_type":"code","source":"# Save df_train and df_val as pickles to save time in the future\n#import pickle \n#file_df_train = open(f'{DATA_DIR_OUTPUT}/df_train.pkl', 'wb')\n#with open('df.pkl', 'wb') as file:\n#file_df_val = open(f'{DATA_DIR_OUTPUT}/df_val.pkl', 'wb')\n#pickle.dump(df_train, file_df_train)\n#pickle.dump(df_val, file_df_val)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Load saved DFs to save time in the future\nimport pickle \nfile_df_train = open('../input/pickled-dfs/df_train.pkl', 'rb') \nfile_df_val = open('../input/pickled-dfs/df_val.pkl', 'rb') \ndf_train = pickle.load(file_df_train)\ndf_val = pickle.load(file_df_val)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## nltk\nThe `nltk` package is useful for several NLP tasks. However, the current implementation uses its 'parts of speech' tagging feature. Here, the parts of speech are identified and counted for each record.","metadata":{}},{"cell_type":"code","source":"def count_pos_tags(df):\n    for index, row in df.iterrows():\n        tag_list = []\n        paragraph = row.excerpt\n        tokens = nltk.word_tokenize(paragraph)\n        tagged = nltk.pos_tag(tokens)\n        for tag in tagged:\n            tag_list.append(tag[1])\n        tag_counts = pd.Series(tag_list).value_counts()\n        tag_indices = tag_counts.index\n        for i, tag in enumerate(tag_counts):\n            df.at[index, tag_indices[i]] = tag\n    return df.fillna(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add parts-of-speech features to training set\ndf_train = count_pos_tags(df_train)\n\n# Add parts-of-speech features to validation set\ndf_val = count_pos_tags(df_val)\n# Add columns not present in df_val\ndifferences = list(set(df_train.columns).difference(set(df_val.columns)))\n# Remove target column names\ndifferences.remove('target')\ndifferences.remove('standard_error')\n# Add missing columns as zero columns\ndf_val[differences] = 0\n\ndf_train.shape, df_val.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Normalisation\nNormalisation is being done for best practice. Not all models need the data to be normalised. However, if you wish to test other models, this may be useful.","metadata":{}},{"cell_type":"code","source":"# Manual identification of features calculated by textstat\ncontinuous_features = [\n    'lexicon_count',\n       'flesch_reading_ease', 'flesch_kincaid_grade', 'smog_index',\n       'coleman_liau_index', 'automated_readability_index',\n       'dale_chall_readability_score', 'linsear_write_formula', 'gunning_fog',\n       'fernandez_huerta', 'szigriszt_pazos', 'gutierrez_polini',\n       'spache_readability', 'crawford', 'difficult_words', 'syllable_count',\n       'sentence_count', 'polysyllabcount', 'char_count', 'letter_count',\n       'avg_character_per_word', 'avg_letter_per_word', 'avg_sentence_length',\n       'avg_sentence_per_word', 'avg_syllables_per_word', 'lix', 'rix',\n       'reading_time'\n]\n\n# Automatic identification of features calculated by nltk\nadditional_features = list(set(df_train.columns).difference(set(continuous_features)))\nadditional_features.remove('standard_error')\nadditional_features.remove('target')\nadditional_features.remove('excerpt')\nadditional_features.remove('id')\ncontinuous_features.extend(additional_features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Normalisation\nnormalizer = preprocessing.Normalizer()\n# Normalise training set\nnormalized_train_X = pd.DataFrame(normalizer.fit_transform(df_train[continuous_features]))\nnormalized_train_X.columns = df_train[continuous_features].columns\n# Normalise validation set\nnormalized_val_X = pd.DataFrame(normalizer.transform(df_val[continuous_features]))\nnormalized_val_X.columns = df_val[continuous_features].columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = normalized_train_X.copy()\ny = df_train.target\nX.shape, y.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train-Test Split\nSplit the data into 80% for training and 20% for testing purposes.","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X,y, \n                                                    test_size=0.2,\n                                                    random_state=42)\nX_valid = normalized_val_X.copy()\n\nX_train.shape, y_train.shape, X_test.shape, y_test.shape, X_valid.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Training\nThis is a base Linear Regression model.","metadata":{}},{"cell_type":"code","source":"# Linear Regression\nlr_model = LinearRegression()\nlr_model.fit(X_train, y_train)\nprint_score(lr_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predictions\nObtain predictions for submission.","metadata":{}},{"cell_type":"code","source":"y_pred = lr_model.predict(X_valid)\npredictions = pd.DataFrame()\npredictions['id'] = df_val.id\npredictions['target'] = y_pred\n#predictions.to_csv(\"/kaggle/working/submission.csv\", index=False)\npredictions.to_csv(f'{DATA_DIR_OUTPUT}/submission.csv', index=False)\npredictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}