{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# CommonLit Mean Pooled Regression\n\nThis notebook builds an ensemble of 4 base models (DeBERTa, ELECTRA, MPNet and RoBERTa). While the large models will have better scores, this notebook is focused on simplicity and solid performance with the tradeoff of slightly worse accuracy. The models were externally trained and uploaded as a dataset but the training code is included in this notebook.","metadata":{}},{"cell_type":"markdown","source":"## Model layout\n\nThe models are all defined below and use the same mean pooling logic. Each output head is straightforward given the small training set and varies slightly based on what worked best. Models are defined to maximize compatibility with the HuggingFace Trainer API.","metadata":{}},{"cell_type":"code","source":"import torch\n\nfrom torch.nn import LayerNorm, Linear, Module, MSELoss, Sequential, Tanh\n\nfrom transformers import AutoModelForSequenceClassification\nfrom transformers.modeling_outputs import SequenceClassifierOutput\nfrom transformers.models.deberta.modeling_deberta import DebertaModel, DebertaPreTrainedModel\nfrom transformers.models.electra.modeling_electra import ElectraModel, ElectraPreTrainedModel\nfrom transformers.models.mpnet.modeling_mpnet import MPNetModel, MPNetPreTrainedModel\nfrom transformers.models.roberta.modeling_roberta import RobertaModel, RobertaPreTrainedModel\n\nclass MeanPooledRegression(Module):\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None\n    ):\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        # Get language model\n        model = self.model()\n\n        # Run inputs through language model\n        outputs = model(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict\n        )\n\n        # pylint: disable=E1101\n        # Build mean pooled vector using outputs with input attention mask set\n        mask = attention_mask.unsqueeze(-1).expand(outputs[0].size()).float()\n        pooling = torch.sum(outputs[0] * mask, 1) / torch.clamp(mask.sum(1), min=1e-9)\n\n        # Run mean pooled vector through regression function\n        logits = self.regressor(pooling)\n\n        # Calculate loss\n        loss = None\n        if labels is not None:\n            loss_fct = MSELoss()\n            loss = loss_fct(logits.squeeze(), labels.squeeze())\n\n        # Return outputs\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return SequenceClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions\n        )\n\nclass DebertaForReadability(DebertaPreTrainedModel, MeanPooledRegression):\n    _keys_to_ignore_on_load_missing = [\"regressor\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n\n        self.deberta = DebertaModel(config)\n        self.regressor = Sequential(\n            Linear(config.hidden_size, config.hidden_size),\n            Tanh(),\n            Linear(config.hidden_size, 1)\n        )\n\n    def model(self):\n        return self.deberta\n\nclass ElectraForReadability(ElectraPreTrainedModel, MeanPooledRegression):\n    _keys_to_ignore_on_load_missing = [\"regressor\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n\n        self.electra = ElectraModel(config)\n        self.regressor = Linear(config.hidden_size, 1)\n\n    def model(self):\n        return self.electra\n\nclass MPNetForReadability(MPNetPreTrainedModel, MeanPooledRegression):\n    _keys_to_ignore_on_load_missing = [\"regressor\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n\n        self.mpnet = MPNetModel(config)\n        self.regressor = Sequential(\n            LayerNorm(config.hidden_size),\n            Linear(config.hidden_size, 1)\n        )\n\n    def model(self):\n        return self.mpnet\n\nclass RobertaForReadability(RobertaPreTrainedModel, MeanPooledRegression):\n    _keys_to_ignore_on_load_missing = [\"regressor\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n\n        self.roberta = RobertaModel(config)\n        self.regressor = Linear(config.hidden_size, 1)\n\n    def model(self):\n        return self.roberta\n\nclass AutoModelForReadability:\n    @staticmethod\n    def from_pretrained(path, config=None):\n        if \"deberta\" in path:\n            return DebertaForReadability.from_pretrained(path, config=config)\n        elif \"electra\" in path:\n            return ElectraForReadability.from_pretrained(path, config=config)\n        elif \"mpnet\" in path:\n            return MPNetForReadability.from_pretrained(path, config=config)\n        elif \"roberta\" in path:\n            return RobertaForReadability.from_pretrained(path, config=config)\n\n        return AutoModelForSequenceClassification.from_pretrained(path, config=config)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T15:23:23.431896Z","iopub.execute_input":"2021-08-03T15:23:23.432317Z","iopub.status.idle":"2021-08-03T15:23:29.852048Z","shell.execute_reply.started":"2021-08-03T15:23:23.432207Z","shell.execute_reply":"2021-08-03T15:23:29.851141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cross-validation sets\n\nUse 5 folds for model cross-validation.","metadata":{}},{"cell_type":"code","source":"import csv\n\nfrom sklearn.model_selection import KFold\n\ndef buildcv():\n    # Read training data and labels\n    data, targets = [], []\n    with open(\"/kaggle/input/commonlitreadabilityprize/train.csv\", \"r\", newline='') as csvf:\n        for row in csv.DictReader(csvf):\n            # Include complete entries\n            if float(row[\"target\"]) != 0 and float(row[\"standard_error\"]) != 0:\n                data.append(row[\"excerpt\"])\n                targets.append(float(row[\"target\"]))\n\n    fold = 0\n    for train_index, test_index in KFold(n_splits=5).split(data):\n        X_train = [x for i, x in enumerate(data) if i in train_index]\n        X_test = [x for i, x in enumerate(data) if i in test_index]\n\n        y_train = [x for i, x in enumerate(targets) if i in train_index]\n        y_test = [x for i, x in enumerate(targets) if i in test_index]\n\n        with open(\"train-%d.csv\" % fold, \"w\", newline='') as csvfile:\n            writer = csv.writer(csvfile)\n\n            writer.writerow([\"label\", \"text\"])\n\n            for i, x in enumerate(X_train):\n                writer.writerow([y_train[i], x])\n\n        with open(\"valid-%d.csv\" % fold, \"w\", newline='') as csvfile:\n            writer = csv.writer(csvfile)\n\n            writer.writerow([\"label\", \"text\"])\n\n            for i, x in enumerate(X_test):\n                writer.writerow([y_test[i], x])\n\n        fold += 1","metadata":{"execution":{"iopub.status.busy":"2021-08-03T15:23:29.853511Z","iopub.execute_input":"2021-08-03T15:23:29.853837Z","iopub.status.idle":"2021-08-03T15:23:30.415655Z","shell.execute_reply.started":"2021-08-03T15:23:29.853802Z","shell.execute_reply":"2021-08-03T15:23:30.4148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training\n\nThe section below trains each model. As noted above, this code was executed externally but it's included here for clarity. The HuggingFace Trainer API is used to with 5 folds.\n\n*Note that running the training code below requires the datasets package to be installed.*\n```python\n!pip install datasets\nfrom datasets import load_dataset\n```","metadata":{}},{"cell_type":"code","source":"import gc\n\nimport numpy as np\nimport torch\n\nfrom sklearn.metrics import mean_squared_error\nfrom transformers import AutoTokenizer, AutoConfig\nfrom transformers import Trainer, TrainerCallback, TrainingArguments, set_seed\n\nEVAL_STEPS = 50\n\nclass EvalCallback(TrainerCallback):\n    def on_evaluate(self, args, state, control, **kwargs):\n        # Dynamically set evaluation schedule based on current rmse\n        rmse = state.log_history[-1][\"eval_rmse\"]\n        prmse = state.log_history[-2][\"eval_rmse\"] if len(state.log_history) > 1 and \\\n                \"eval_rmse\" in state.log_history[-2] else None\n\n        if rmse > 0.51 or (prmse and rmse > prmse):\n            args.eval_steps = EVAL_STEPS\n        elif rmse <= 0.48:\n            args.eval_steps = 1\n        elif rmse <= 0.49: \n            args.eval_steps = 2\n        elif rmse <= 0.50:\n            args.eval_steps = 4\n        elif rmse <= 0.51:\n            args.eval_steps = 8\n\ndef metrics(pred):\n    return {\"rmse\": mean_squared_error(pred.label_ids, pred.predictions, squared=False)}\n\ndef regression(options):\n    path, epochs, lrate, decay = options\n    print(\"Parameters -\", options)\n\n    # Get model short name\n    name = path.lower().split(\"/\")[-1].split(\"-\")[0]\n\n    rmse = []\n    for b in range(5):\n        # Initialize training arguments\n        args = TrainingArguments(\"%s-%d\" % (name, b), overwrite_output_dir=True, \n                                 num_train_epochs=epochs, learning_rate=lrate, \n                                 weight_decay=decay, evaluation_strategy=\"steps\",\n                                 eval_steps=EVAL_STEPS, save_total_limit=1,\n                                 load_best_model_at_end=True, metric_for_best_model=\"rmse\",\n                                 greater_is_better=False)\n\n        files = {\"train\": \"train-%d.csv\" % b, \"validation\": \"valid-%d.csv\" % b}\n\n        # Set seed before initializing model.\n        set_seed(args.seed)\n\n        # Model config\n        config = AutoConfig.from_pretrained(path)\n        config.update({\"hidden_dropout_prob\": 0.0, \"layer_norm_eps\": 1e-7, \"num_labels\": 1})\n\n        # Load tokenizer\n        tokenizer = AutoTokenizer.from_pretrained(path)\n\n        # Load model\n        model = AutoModelForReadability.from_pretrained(path, config=config)\n\n        # Load datasets\n        datasets = load_dataset(\"csv\", data_files=files)\n        datasets = datasets.map(lambda data: tokenizer(data[\"text\"], padding=\"max_length\",\n                                                       max_length=248, truncation=True), batched=True)\n\n        trainer = Trainer(model=model, tokenizer=tokenizer, args=args,\n                          train_dataset=datasets[\"train\"],\n                          eval_dataset=datasets[\"validation\"],\n                          compute_metrics=metrics, \n                          callbacks=[EvalCallback()] if EVAL_STEPS < 50 else None)\n\n        trainer.train()\n        trainer.save_model()\n        trainer.save_state()\n\n        results = trainer.evaluate()\n        rmse.append(results[\"eval_rmse\"])\n\n        # Clear memory\n        tokenizer, model, datasets, trainer, results = None, None, None, None, None\n        gc.collect()\n        torch.cuda.empty_cache()\n\n        # Early exit\n        if rmse[0] >= 0.48:\n            return 1.0\n\n    print(options, rmse, \"=\", np.mean(rmse))\n\n    return np.mean(rmse)\n\n# Training run externally \n#buildcv()\n#regression((\"roberta-base\", 3, 2e-05, 0.01))\n#regression((\"microsoft/deberta-base\", 4, 2e-05, 0))\n#regression((\"google/electra-base-discriminator\", 3, 3e-05, 0.01))\n#regression((\"microsoft/mpnet-base\", 4, 4e-05, 0))","metadata":{"execution":{"iopub.status.busy":"2021-08-03T15:23:30.417434Z","iopub.execute_input":"2021-08-03T15:23:30.417761Z","iopub.status.idle":"2021-08-03T15:23:30.580792Z","shell.execute_reply.started":"2021-08-03T15:23:30.417726Z","shell.execute_reply":"2021-08-03T15:23:30.579949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference\n\nLoad models and run inference against test set.","metadata":{}},{"cell_type":"code","source":"import csv\n\nimport numpy as np\nimport torch\n\nfrom transformers import AutoTokenizer\n\ndef batch(texts, size):\n    return [texts[x : x + size] for x in range(0, len(texts), size)]\n\ndef encode(chunk):\n    embeddings = None\n\n    for tokenizer, model in models:\n        # Tokenize sentences\n        encoded_input = tokenizer(chunk, max_length=248, padding=\"max_length\",\n                                  truncation=True, return_tensors=\"pt\")        \n        encoded_input.to(device)\n\n        # Compute token embeddings\n        with torch.no_grad():\n            model_output = model(**encoded_input)\n\n        # Transformer model logits as features\n        outputs = model_output.logits.cpu().numpy()\n\n        if embeddings is not None:\n            embeddings = np.concatenate((embeddings, outputs), axis=1)\n        else:\n            embeddings = outputs\n\n    return embeddings","metadata":{"execution":{"iopub.status.busy":"2021-08-03T15:23:30.582274Z","iopub.execute_input":"2021-08-03T15:23:30.582587Z","iopub.status.idle":"2021-08-03T15:23:30.591158Z","shell.execute_reply.started":"2021-08-03T15:23:30.582553Z","shell.execute_reply":"2021-08-03T15:23:30.590163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set device\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nids, data = [], []\nwith open(\"/kaggle/input/commonlitreadabilityprize/test.csv\", mode=\"r\") as csvfile:\n    for row in csv.DictReader(csvfile):\n        ids.append(row[\"id\"])\n        data.append(row[\"excerpt\"])\n\nvalues = None\nmodels = None\nfor mtype in [\"deberta\", \"electra\", \"mpnet\", \"roberta\"]:\n    models = []\n    predictions = None\n\n    for b in range(5):\n        path = \"/kaggle/input/commonlit-transformers-models/%s-%d\" % (mtype, b)\n        print(path)\n        tokenizer = AutoTokenizer.from_pretrained(path)\n\n        model = AutoModelForReadability.from_pretrained(path)\n        model.to(device)\n\n        models.append((tokenizer, model))\n\n    for chunk in batch(data, 32):\n        outputs = encode(chunk)\n        outputs = np.mean(outputs, axis=1).reshape(-1, 1)\n        \n        if predictions is not None:\n            predictions = np.concatenate((predictions, outputs), axis=0)\n        else:\n            predictions = outputs\n\n    if values is not None:\n        values = np.concatenate((values, predictions), axis=1)\n    else:\n        values = predictions\n\n# Write test predictions\npredictions = np.average(values, weights=[0.21, 0.17, 0.23, 0.39], axis=1)\nwith open(\"submission.csv\", \"w\") as output:\n    output.write(\"id,target\\n\")\n\n    for x, p in enumerate(predictions):\n        output.write(\"%s,%.8f\\n\" % (ids[x], p))","metadata":{"execution":{"iopub.status.busy":"2021-08-03T15:23:30.592742Z","iopub.execute_input":"2021-08-03T15:23:30.593278Z","iopub.status.idle":"2021-08-03T15:26:26.372961Z","shell.execute_reply.started":"2021-08-03T15:23:30.593078Z","shell.execute_reply":"2021-08-03T15:26:26.37215Z"},"trusted":true},"execution_count":null,"outputs":[]}]}