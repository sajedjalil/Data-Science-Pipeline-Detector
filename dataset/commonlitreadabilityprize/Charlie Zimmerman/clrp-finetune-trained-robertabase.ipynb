{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Solution Overview:**\n\nTrain Roberta-Base and RobertaLarge models on the contest data along with supplmemental sources similar to that data.  Fine tune the models using cross-validation folds. Inference weights all 10 models (two trained models * five fine-tuned models [five folds] per model) equally.\n\n**Notebook Sequence:**\n* [Train Roberta Base Model](https://www.kaggle.com/charliezimmerman/clrp-train-robertabase-maskedlm-model)\n* [Train Roberta Large Model](https://www.kaggle.com/charliezimmerman/clrp-train-robertalarge-masked-lm-model/)\n* [Fine Tune Trained Roberta-Base Model -- **This Notebook**](https://www.kaggle.com/charliezimmerman/clrp-finetune-trained-robertabase)\n* [Fine Tune Trained Roberta Large Model](https://www.kaggle.com/charliezimmerman/clrp-finetune-trained-robertalarge)\n* [Inference Notebook](https://www.kaggle.com/charliezimmerman/clrp-inference-robertabase-robertalarge-ensemble)\n\n**This Notebook influenced by:**\n\n* [https://www.kaggle.com/chamecall/clrp-finetune-single-roberta-base?scriptVersionId=68893027](https://www.kaggle.com/chamecall/clrp-finetune-single-roberta-base?scriptVersionId=68893027)\n* [https://www.kaggle.com/maunish/clrp-pytorch-roberta-finetune](https://www.kaggle.com/maunish/clrp-pytorch-roberta-finetune)","metadata":{}},{"cell_type":"code","source":"import transformers\nimport pandas as pd\nimport os\nimport gc\nimport sys\nimport math\nimport time\nimport tqdm\nimport random\nimport numpy as np\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.nn.functional import mse_loss\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data import RandomSampler, SequentialSampler#, Sampler\nfrom transformers import AutoModel,AutoTokenizer,get_cosine_schedule_with_warmup, AutoConfig, AdamW\nfrom time import time\nfrom tqdm import tqdm\nimport warnings\nfrom pathlib import Path\nwarnings.filterwarnings('ignore')\nfrom sklearn.metrics import mean_squared_error","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class configuration:\n    tokenizer_path = '../input/roberta-base'\n    clrp_data_path= '../input/commonlitreadabilityprize'\n    pretrained_model_path = '../input/clrp-trained-robertabase/robertabase_clrp_model'\n    output_path='/kaggle/working/clrp-robertabase-modelweights')\n    output_hidden_states = True\n    epochs = 3\n    evaluate_interval = 10\n    batch_size = 16\n    device = 'cuda'\n    seed = 42\n    max_len = 256\n    lr = 2e-5\n    wd = 0.01\n    eval_schedule = [(float('inf'), 16), (0.5, 8), (0.49, 4), (0.48, 2), (0.47, 1), (0, 0)]\n    num_folds=5\n    base_seed=1000\n    fold_seeds=[9183,4309,4071,98,4071]\n    max_length = 300\n    train_batch_size = 8\n    val_batch_size = 32\n    num_warmup_steps=50\n   ","metadata":{"execution":{"iopub.status.busy":"2021-08-14T00:17:51.162659Z","iopub.execute_input":"2021-08-14T00:17:51.163077Z","iopub.status.idle":"2021-08-14T00:17:51.176783Z","shell.execute_reply.started":"2021-08-14T00:17:51.163041Z","shell.execute_reply":"2021-08-14T00:17:51.175964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = torch.cuda.amp.GradScaler() \nDEVICE = torch.device(configuration.device if torch.cuda.is_available() else 'cpu')\nn_gpu = torch.cuda.device_count()\ntorch.cuda.get_device_name(0)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-14T00:16:21.84614Z","iopub.execute_input":"2021-08-14T00:16:21.846457Z","iopub.status.idle":"2021-08-14T00:16:21.908025Z","shell.execute_reply.started":"2021-08-14T00:16:21.846427Z","shell.execute_reply":"2021-08-14T00:16:21.907237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(configuration.clrp_data_path + \"/train.csv\")\ntest = pd.read_csv(configuration.clrp_data_path + \"/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-08-14T00:16:30.894334Z","iopub.execute_input":"2021-08-14T00:16:30.8947Z","iopub.status.idle":"2021-08-14T00:16:31.001708Z","shell.execute_reply.started":"2021-08-14T00:16:30.894666Z","shell.execute_reply":"2021-08-14T00:16:31.000837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models_dir = Path(configuration.output_path)\nmodels_dir.mkdir(exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-14T00:27:07.177778Z","iopub.execute_input":"2021-08-14T00:27:07.178103Z","iopub.status.idle":"2021-08-14T00:27:07.182744Z","shell.execute_reply.started":"2021-08-14T00:27:07.178073Z","shell.execute_reply":"2021-08-14T00:27:07.181661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed=configuration.base_seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    \ndef get_data_loaders(data, fold):\n    \n    x_train = data.loc[data.fold != fold, 'excerpt'].tolist()\n    y_train = data.loc[data.fold != fold, 'target'].values\n    x_val = data.loc[data.fold == fold, 'excerpt'].tolist()\n    y_val = data.loc[data.fold == fold, 'target'].values\n    \n    tokenizer = AutoTokenizer.from_pretrained(configuration.tokenizer_path)\n    \n    encoded_train = tokenizer.batch_encode_plus(\n        x_train, \n        add_special_tokens=True, \n        return_attention_mask=True, \n        padding='max_length', \n        truncation=True,\n        max_length=configuration.max_length, \n        return_tensors='pt'\n    )\n    \n    encoded_val = tokenizer.batch_encode_plus(\n        x_val, \n        add_special_tokens=True, \n        return_attention_mask=True, \n        padding='max_length', \n        truncation=True,\n        max_length=configuration.max_length, \n        return_tensors='pt'\n    )\n    \n    dataset_train = TensorDataset(\n        encoded_train['input_ids'],\n        encoded_train['attention_mask'],\n        torch.tensor(y_train)\n    )\n    dataset_val = TensorDataset(\n        encoded_val['input_ids'],\n        encoded_val['attention_mask'],\n        torch.tensor(y_val)\n    )\n    \n    dataloader_train = DataLoader(\n        dataset_train,\n        sampler = RandomSampler(dataset_train),\n        batch_size=configuration.train_batch_size\n    )\n\n    dataloader_val = DataLoader(\n        dataset_val,\n        sampler = SequentialSampler(dataset_val),\n        batch_size=configuration.val_batch_size\n    )\n\n    return dataloader_train, dataloader_val","metadata":{"execution":{"iopub.status.busy":"2021-08-14T00:16:33.090396Z","iopub.execute_input":"2021-08-14T00:16:33.090877Z","iopub.status.idle":"2021-08-14T00:16:33.101369Z","shell.execute_reply.started":"2021-08-14T00:16:33.09084Z","shell.execute_reply":"2021-08-14T00:16:33.10012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#create folds\nseed = 1000\nseed_everything(seed=seed)\nx=train.index.to_list()\nrand_idx=random.sample(x, len(x))\ntrain.loc[:,'fold'] = pd.cut(rand_idx, bins=configuration.num_folds,labels=False)\ntarget = train.target.to_numpy()\n","metadata":{"execution":{"iopub.status.busy":"2021-08-14T00:16:42.025076Z","iopub.execute_input":"2021-08-14T00:16:42.025404Z","iopub.status.idle":"2021-08-14T00:16:42.052183Z","shell.execute_reply.started":"2021-08-14T00:16:42.025372Z","shell.execute_reply":"2021-08-14T00:16:42.051122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_examples_to_features(text, tokenizer, max_len):\n\n    tok = tokenizer.encode_plus(\n        text, \n        max_length=max_len, \n        truncation=True,\n        padding='max_length',\n    )\n    return tok\n\n\nclass CLRPDataset(Dataset):\n    def __init__(self, data, tokenizer, max_len, is_test=False):\n        self.data = data\n        self.excerpts = self.data.excerpt.tolist()\n        if not is_test:\n            self.targets = self.data.target.tolist()\n            \n        self.tokenizer = tokenizer\n        self.is_test = is_test\n        self.max_len = max_len\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, item):\n        if not self.is_test:\n            excerpt = self.excerpts[item]\n            label = self.targets[item]\n            features = convert_examples_to_features(\n                excerpt, self.tokenizer, self.max_len\n            )\n            return {\n                'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n                'label':torch.tensor(label, dtype=torch.float),\n            }\n        else:\n            excerpt = self.excerpts[item]\n            features = convert_examples_to_features(\n                excerpt, self.tokenizer, self.max_len\n            )\n            return {\n                'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n            }","metadata":{"execution":{"iopub.status.busy":"2021-08-14T00:16:44.132111Z","iopub.execute_input":"2021-08-14T00:16:44.132432Z","iopub.status.idle":"2021-08-14T00:16:44.142524Z","shell.execute_reply.started":"2021-08-14T00:16:44.132403Z","shell.execute_reply":"2021-08-14T00:16:44.141317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AttentionHead(nn.Module):\n    def __init__(self, h_size, hidden_dim=512):\n        super().__init__()\n        self.W = nn.Linear(h_size, hidden_dim)\n        self.V = nn.Linear(hidden_dim, 1)\n        \n    def forward(self, features):\n        att = torch.tanh(self.W(features))\n        score = self.V(att)\n        attention_weights = torch.softmax(score, dim=1)\n        context_vector = attention_weights * features\n        context_vector = torch.sum(context_vector, dim=1)\n\n        return context_vector\n\nclass CLRPModel(nn.Module):\n    def __init__(self,transformer,config):\n        super(CLRPModel,self).__init__()\n        self.h_size = config.hidden_size\n        self.transformer = transformer\n        self.head = AttentionHead(self.h_size)\n        self.linear = nn.Linear(self.h_size, 1)\n              \n    def forward(self, input_ids, attention_mask):\n        transformer_out = self.transformer(input_ids, attention_mask)\n        x = self.head(transformer_out.last_hidden_state)\n        x = self.linear(x)\n        return x\n","metadata":{"execution":{"iopub.status.busy":"2021-08-14T00:16:51.72408Z","iopub.execute_input":"2021-08-14T00:16:51.724403Z","iopub.status.idle":"2021-08-14T00:16:51.733044Z","shell.execute_reply.started":"2021-08-14T00:16:51.724371Z","shell.execute_reply":"2021-08-14T00:16:51.731962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_optimizer(model):\n    parameters = []\n    lr = configuration.lr\n    multiplier=.990\n    classifier_lr=lr\n    for layer in range(11,-1,-1):\n        layer_params = {\n            'params': [p for n,p in model.named_parameters() if f'encoder.layer.{layer}.' in n],\n            'lr': lr\n        }\n        parameters.append(layer_params)\n        lr *= multiplier\n    classifier_params = {\n        'params': [p for n,p in model.named_parameters() if 'layer_norm' in n or 'linear' in n \n                   or 'pooling' in n],\n        'lr': classifier_lr\n    }\n    parameters.append(classifier_params)\n    \n    return optim.AdamW(parameters)","metadata":{"execution":{"iopub.status.busy":"2021-08-14T00:16:56.193721Z","iopub.execute_input":"2021-08-14T00:16:56.194245Z","iopub.status.idle":"2021-08-14T00:16:56.202147Z","shell.execute_reply.started":"2021-08-14T00:16:56.194204Z","shell.execute_reply":"2021-08-14T00:16:56.201161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"   model_config = AutoConfig.from_pretrained(configuration.pretrained_model_path)\n   model_config.update({\n            \"hidden_dropout_prob\": 0.0,\n            \"layer_norm_eps\": 1e-7\n            }) \n\n   transformer = AutoModel.from_pretrained(configuration.pretrained_model_path, config=model_config) \n   model = CLRPModel(transformer, model_config)\n   model = model.to(configuration.device) ","metadata":{"execution":{"iopub.status.busy":"2021-08-14T00:17:59.235376Z","iopub.execute_input":"2021-08-14T00:17:59.235744Z","iopub.status.idle":"2021-08-14T00:18:12.665353Z","shell.execute_reply.started":"2021-08-14T00:17:59.235711Z","shell.execute_reply":"2021-08-14T00:18:12.664551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AvgCounter:\n    def __init__(self):\n        self.reset()\n        \n    def update(self, loss, n_samples):\n        self.loss += loss * n_samples\n        self.n_samples += n_samples\n        \n    def avg(self):\n        return self.loss / self.n_samples\n    \n    def reset(self):\n        self.loss = 0\n        self.n_samples = 0\n\nclass EvaluationScheduler:\n    def __init__(self, evaluation_schedule, penalize_factor=1, max_penalty=8):\n        self.evaluation_schedule = evaluation_schedule\n        self.evaluation_interval = self.evaluation_schedule[0][1]\n        self.last_evaluation_step = 0\n        self.prev_loss = float('inf')\n        self.penalize_factor = penalize_factor\n        self.penalty = 0\n        self.prev_interval = -1\n        self.max_penalty = max_penalty\n\n    def step(self, step):\n        # should we to make evaluation right now\n        if step >= self.last_evaluation_step + self.evaluation_interval:\n            self.last_evaluation_step = step\n            return True\n        else:\n            return False\n        \n            \n    def update_evaluation_interval(self, last_loss):\n        # set up evaluation_interval depending on loss value\n        cur_interval = -1\n        for i, (loss, interval) in enumerate(self.evaluation_schedule[:-1]):\n            if self.evaluation_schedule[i+1][0] < last_loss < loss:\n                self.evaluation_interval = interval\n                cur_interval = i\n                break\n           \n        self.prev_loss = last_loss\n        self.prev_interval = cur_interval\n        \n          \n        \ndef make_dataloader(data, tokenizer, is_train=True):\n    dataset = CLRPDataset(data, tokenizer=tokenizer, max_len=configuration.max_len)\n    if is_train:\n        sampler = RandomSampler(dataset)\n    else:\n        sampler = SequentialSampler(dataset)\n\n    batch_dataloader = DataLoader(dataset, sampler=sampler, batch_size=configuration.batch_size, pin_memory=True)\n    return batch_dataloader\n                   \n            \nclass CLRPTrainer:\n    def __init__(self, train_dl, val_dl, model, optimizer, scheduler, criterion, model_num):\n        self.train_dl = train_dl\n        self.val_dl = val_dl\n        self.model = model\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n        self.device =configuration.device\n        self.batches_per_epoch = len(self.train_dl)\n        self.criterion = criterion\n        self.model_num = model_num\n                \n    def run(self):\n        record_info = {\n            'train_loss': [],\n            'val_loss': [],\n        }\n        \n        best_val_loss = float('inf')\n        evaluation_scheduler = EvaluationScheduler(configuration.eval_schedule)\n        train_loss_counter = AvgCounter()\n        step = 0\n        \n        for epoch in range(configuration.epochs):\n            \n            print(f'Epoch: {epoch+1}/{configuration.epochs}')\n            start_epoch_time = time()\n            \n            for batch_num, batch in enumerate(self.train_dl):\n                train_loss = self.train(batch)\n                train_loss_counter.update(train_loss, len(batch))\n                record_info['train_loss'].append((step, train_loss.item()))\n\n                if evaluation_scheduler.step(step):\n                    val_loss = self.evaluate()\n                    \n                    record_info['val_loss'].append((step, val_loss.item()))        \n                    print(f'\\t\\t{epoch+1}#[{batch_num+1}/{self.batches_per_epoch}]: train loss - {train_loss_counter.avg()} | val loss - {val_loss}',)\n                    train_loss_counter.reset()\n\n                    if val_loss < best_val_loss:\n                        best_val_loss = val_loss\n                        print(f\"Val loss decreased from {best_val_loss} to {val_loss}\")\n                        torch.save(self.model, f'{configuration.output_path}/model_{self.model_num}.bin')\n                        \n                    evaluation_scheduler.update_evaluation_interval(val_loss.item())\n                        \n\n                step += 1\n            end_epoch_time = time()\n            print(f'The epoch took {end_epoch_time - start_epoch_time} sec..')\n\n        return record_info, best_val_loss\n            \n\n    def train(self, batch):\n        self.model.train()\n        sent_id, mask, labels = batch['input_ids'].to(self.device), batch['attention_mask'].to(self.device), batch['label'].to(self.device), \n        self.model.zero_grad() \n        preds = self.model(sent_id, mask)\n        train_loss = self.criterion(preds, labels.unsqueeze(1))\n        \n        train_loss.backward()\n        self.optimizer.step()\n        self.scheduler.step()\n        return torch.sqrt(train_loss)\n\n    def evaluate(self):\n        self.model.eval()\n        val_loss_counter = AvgCounter()\n\n        for step,batch in enumerate(self.val_dl):\n            sent_id, mask, labels = batch['input_ids'].to(self.device), batch['attention_mask'].to(self.device), batch['label'].to(self.device)\n            with torch.no_grad():\n                preds = self.model(sent_id, mask)\n                loss = self.criterion(preds,labels.unsqueeze(1))\n                val_loss_counter.update(torch.sqrt(loss), len(labels))\n        return val_loss_counter.avg()\n    \n    \ndef mse_loss(y_true,y_pred):\n\n    return nn.functional.mse_loss(y_true,y_pred)","metadata":{"execution":{"iopub.status.busy":"2021-08-14T00:18:35.460395Z","iopub.execute_input":"2021-08-14T00:18:35.460748Z","iopub.status.idle":"2021-08-14T00:18:35.484814Z","shell.execute_reply.started":"2021-08-14T00:18:35.460717Z","shell.execute_reply":"2021-08-14T00:18:35.483966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"  seed_everything(seed)\n  for model_num in range(configuration.num_folds): \n    best_loss=999\n    seed=configuration.fold_seeds[model_num]\n    print(f'seed={seed} , Model#{model_num+1}')\n      \n    tokenizer = AutoTokenizer.from_pretrained(configuration.tokenizer_path)\n    config = AutoConfig.from_pretrained(configuration.pretrained_model_path)\n    config.update({\n            \"hidden_dropout_prob\": 0.0,\n            \"layer_norm_eps\": 1e-7\n            }) \n\n\n    train_dl = make_dataloader(train[train.fold!=model_num], tokenizer)\n    val_dl = make_dataloader(train[train.fold==model_num], tokenizer, is_train=False)\n\n    transformer = AutoModel.from_pretrained(configuration.pretrained_model_path, config=config)  \n    model = CLRPModel(transformer, config)\n    model = model.to(configuration.device)\n    optimizer = create_optimizer(model)\n    scheduler = get_cosine_schedule_with_warmup(\n            optimizer,\n            num_training_steps=configuration.epochs * len(train_dl),\n            num_warmup_steps=configuration.num_warmup_steps)  \n\n    criterion = mse_loss\n\n    clrp_trainer =CLRPTrainer(train_dl, val_dl, model, optimizer, scheduler, criterion, model_num)\n    \n    record_info, best_val_loss = clrp_trainer.run()\n    steps, train_losses = list(zip(*record_info['train_loss']))\n    steps, val_losses = list(zip(*record_info['val_loss']))\n\n!date '+%A %W %Y %X' > execution_time\n","metadata":{"execution":{"iopub.status.busy":"2021-08-14T00:27:26.998142Z","iopub.execute_input":"2021-08-14T00:27:26.998457Z","iopub.status.idle":"2021-08-14T02:03:34.377652Z","shell.execute_reply.started":"2021-08-14T00:27:26.998429Z","shell.execute_reply":"2021-08-14T02:03:34.376653Z"},"trusted":true},"execution_count":null,"outputs":[]}]}