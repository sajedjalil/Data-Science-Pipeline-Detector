{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from pathlib import Path\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport random\n\nfrom sklearn import linear_model\nfrom sklearn import metrics\nfrom math import sqrt\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nimport pickle\n\nimport torch\nfrom torch.utils.data import random_split,DataLoader, Dataset\nfrom torchtext.data import Field, TabularDataset, BucketIterator, Iterator\n\nfrom transformers import (BertTokenizer, BertModel, AdamW,\n                          get_linear_schedule_with_warmup,\n                          RobertaTokenizerFast,RobertaModel,\n                          RobertaConfig,PreTrainedModel,\n                          get_constant_schedule_with_warmup,\n                          AutoModelForSequenceClassification,\n                          AutoModel,AutoConfig,\n                          AutoTokenizer,get_cosine_schedule_with_warmup\n                         )\nimport warnings\nwarnings.simplefilter('ignore')\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed = 0):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    random_state = np.random.RandomState(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    return random_state\n\nseed = 82\nrandom_state = set_seed(seed)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dir = Path('../input/commonlitreadabilityprize')\ntrain_file = data_dir / 'train.csv'\ntest_file = data_dir / 'test.csv'\nsample_file = data_dir / 'sample_submission.csv'\nsubmission_file = 'submission.csv'\noutput_path = \"./\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.makedirs('./model_preds')\nos.makedirs('./models')\nos.makedirs('./test')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set random seed and set device to GPU.\n#torch.manual_seed(17)\n\nif torch.cuda.is_available():\n    device = torch.device('cuda')\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\nelse:\n    device = torch.device('cpu')\n\nprint(device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(train_file)\ndf.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create fold","metadata":{}},{"cell_type":"code","source":"k_folds = 5","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import model_selection\n\ndf.loc[:,\"kfold\"] = -1\ndf = df.sample(frac=1).reset_index(drop=True)\n\ny = df.target.values\nskf = model_selection.KFold(n_splits=k_folds)\n\nfor f, (t_, v_) in enumerate(skf.split(X=df, y=y)):\n    df.loc[v_, \"kfold\"]=f\n    \ndf.to_csv(\"./train_folds.csv\", index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# text pre-processing for Sklearn models","metadata":{}},{"cell_type":"code","source":"from nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\n\ndef remove_stopwords(text):\n    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n\n\", \".join(stopwords.words('english'))\nSTOPWORDS = set(stopwords.words('english'))\n\ndef preprop(df):\n    df[\"excerpt\"]=df.excerpt.str.lower()\n    \n    ps = PorterStemmer()\n    df[\"excerpt\"] = df.excerpt.apply(ps.stem)\n    \n    wnl = WordNetLemmatizer()\n    df[\"excerpt\"] = df.excerpt.apply(wnl.lemmatize)\n\n    df[\"excerpt\"] = df.excerpt.apply(lambda text: remove_stopwords(text))\n    \n    return df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train\ndf = pd.read_csv(\"./train_folds.csv\")\ndf=preprop(df)\n\ndf.to_csv(\"./train_folds_preprop.csv\", index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create models","metadata":{}},{"cell_type":"markdown","source":"## Word embedding with TfIDF","metadata":{}},{"cell_type":"code","source":"#linear model with TfidfVectorizer on Excerpt\ndef run_Tfidf(fold):\n    df = pd.read_csv(\"./train_folds.csv\") #better results\n    df.excerpt = df.excerpt.apply(str)\n    df_train = df[df.kfold != fold].reset_index(drop=True)\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n    \n    tfv = TfidfVectorizer()\n    tfv.fit(df_train.excerpt.values)\n    \n    Xtrain = tfv.transform(df_train.excerpt.values)\n    Xvalid = tfv.transform(df_valid.excerpt.values)\n    \n    ytrain = df_train.target.values\n    yvalid = df_valid.target.values\n    \n    clf = linear_model.LinearRegression()\n    clf.fit(Xtrain, ytrain)\n    pred = clf.predict(Xvalid)\n        \n    rmse = sqrt(metrics.mean_squared_error(yvalid, pred))\n    print(f\"fold={fold}, RMSE={rmse}\")\n    \n    if fold == (k_folds-1):\n        filename = './models/clf_lr_Tfidf.sav'\n        pickle.dump(clf, open(filename, 'wb'))\n        print(\"model saved\")\n    \n    df_valid.loc[:,\"lr_Tfidf_pred\"] = pred\n    \n    return df_valid[[\"id\",\"target\",\"kfold\",\"lr_Tfidf_pred\"]]\n\ndfs = []\nfor j in range(k_folds):\n    temp_df = run_Tfidf(j)\n    dfs.append(temp_df)  \nfin_valid_df = pd.concat(dfs)\nfin_valid_df.to_csv(\"./model_preds/lr_Tfidf_excerpt.csv\", index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#linear model with CountVectorizer on Excerpt\nfrom sklearn import linear_model\nfrom sklearn import metrics\nfrom math import sqrt\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ndef run_lr_cnt(fold):\n    #df = pd.read_csv(\"./train_folds.csv\")\n    df = pd.read_csv(\"./train_folds_preprop.csv\") #Better results\n    df.excerpt = df.excerpt.apply(str)\n    df_train = df[df.kfold != fold].reset_index(drop=True)\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n    \n    tfv = CountVectorizer()\n    tfv.fit(df_train.excerpt.values)\n    \n    Xtrain = tfv.transform(df_train.excerpt.values)\n    Xvalid = tfv.transform(df_valid.excerpt.values)\n    \n    ytrain = df_train.target.values\n    yvalid = df_valid.target.values\n    \n    clf = linear_model.LinearRegression()\n    clf.fit(Xtrain, ytrain)\n    pred = clf.predict(Xvalid)\n    \n    rmse = sqrt(metrics.mean_squared_error(yvalid, pred))\n    print(f\"fold={fold}, RMSE={rmse}\")\n    \n    if fold == (k_folds-1):\n        filename = './models/clf_lr_cnt.sav'\n        pickle.dump(clf, open(filename, 'wb'))\n        print(\"model saved\")\n    \n    df_valid.loc[:,\"lr_cnt_pred\"] = pred\n    \n    return df_valid[[\"id\",\"target\",\"kfold\",\"lr_cnt_pred\"]]\n\ndfs = []\nfor j in range(k_folds):\n    temp_df = run_lr_cnt(j)\n    dfs.append(temp_df)  \nfin_valid_df = pd.concat(dfs)\nfin_valid_df.to_csv(\"./model_preds/lr_cnt_excerpt.csv\", index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#rf_svd\nfrom sklearn import decomposition\nfrom sklearn import ensemble\nfrom sklearn import metrics\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ndef run_rf_svd(fold):\n    #df = pd.read_csv(\"./train_folds.csv\")\n    df = pd.read_csv(\"./train_folds_preprop.csv\")\n    df.review = df.excerpt.apply(str)\n    df_train = df[df.kfold != fold].reset_index(drop=True)\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n    \n    tfv = TfidfVectorizer()\n    tfv.fit(df_train.excerpt.values)\n    \n    Xtrain = tfv.transform(df_train.excerpt.values)\n    Xvalid = tfv.transform(df_valid.excerpt.values)\n    \n    svd = decomposition.TruncatedSVD(n_components=120)\n    svd.fit(Xtrain)\n    xtrain_svd = svd.transform(Xtrain)\n    xvalid_svd = svd.transform(Xvalid)\n    \n    ytrain = df_train.target.values\n    yvalid = df_valid.target.values\n    \n    clf = ensemble.RandomForestRegressor(n_estimators=500, n_jobs=-1)\n    clf.fit(xtrain_svd, ytrain)\n    pred = clf.predict(xvalid_svd)\n    \n    rmse = sqrt(metrics.mean_squared_error(yvalid, pred))\n    print(f\"fold={fold}, RMSE={rmse}\")\n    \n    if fold == (k_folds-1):\n        filename = './models/clf_rfr.sav'\n        pickle.dump(clf, open(filename, 'wb'))\n        print(\"model saved\")\n    \n    df_valid.loc[:,\"rf_svd_Tfidf_pred\"] = pred\n    \n    return df_valid[[\"id\",\"target\",\"kfold\",\"rf_svd_Tfidf_pred\"]]\n\ndfs = []\nfor j in range(k_folds):\n    temp_df = run_rf_svd(j)\n    dfs.append(temp_df)  \nfin_valid_df = pd.concat(dfs)\nfin_valid_df.to_csv(\"./model_preds/rf_svd_Tfidf_excerpt.csv\", index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#xgboost\nfrom sklearn import decomposition\nfrom sklearn import ensemble\nfrom sklearn import metrics\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom xgboost import XGBRegressor\n\ndef run_xgboost_Tfidf(fold):\n    df = pd.read_csv(\"./train_folds.csv\") #better results\n    #df = pd.read_csv(\"./train_folds_preprop.csv\")\n    df.review = df.excerpt.apply(str)\n    df_train = df[df.kfold != fold].reset_index(drop=True)\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n    \n    tfv = TfidfVectorizer()\n    tfv.fit(df_train.excerpt.values)\n    \n    Xtrain = tfv.transform(df_train.excerpt.values)\n    Xvalid = tfv.transform(df_valid.excerpt.values)\n    \n    svd = decomposition.TruncatedSVD(n_components=120)\n    svd.fit(Xtrain)\n    xtrain_svd = svd.transform(Xtrain)\n    xvalid_svd = svd.transform(Xvalid)\n    \n    ytrain = df_train.target.values\n    yvalid = df_valid.target.values\n    \n    clf = XGBRegressor(n_estimators=500, n_jobs=-1, learning_rate=0.05)\n    clf.fit(xtrain_svd, ytrain)\n    pred = clf.predict(xvalid_svd)  \n    \n    rmse = sqrt(metrics.mean_squared_error(yvalid, pred))\n    print(f\"fold={fold}, RMSE={rmse}\")\n        \n    if fold == (k_folds-1):\n        filename = './models/clf_xgboost.sav'\n        pickle.dump(clf, open(filename, 'wb'))\n        print(\"model saved\")\n    \n    df_valid.loc[:,\"xgboost_Tfidf_pred\"] = pred\n    \n    return df_valid[[\"id\",\"target\",\"kfold\",\"xgboost_Tfidf_pred\"]]\n\ndfs = []\nfor j in range(k_folds):\n    temp_df = run_xgboost_Tfidf(j)\n    dfs.append(temp_df)  \nfin_valid_df = pd.concat(dfs)\nfin_valid_df.to_csv(\"./model_preds/xgboost_Tfidf_excerpt.csv\", index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Word embedding with Glove","metadata":{}},{"cell_type":"code","source":"EMBEDDING_FILE = '../input/glove6b/glove.6B.100d.txt'\n\nembeddings_dict = {}\nfor line in open(EMBEDDING_FILE):\n    values = line.split()\n    word = values[0]\n    # print(word)\n    vector = np.asarray(values[1:], \"float32\")\n    # print(vector)\n    embeddings_dict[word] = vector","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_feature_vectors(sentence):\n    words = sentence.split()\n    feature_vec = np.zeros((100,),dtype=\"float32\")\n    i=0\n    for word in words:\n        try:\n            feature_vec = np.add(feature_vec, embeddings_dict.get(word))\n        except:\n            i = i + 1\n    if len(words) > 0:\n        feature_vec = np.divide(feature_vec, len(words)- i)\n    return feature_vec","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#randomforest with Glove on excerpt\ndef run_rf_glove(fold):\n    #df = pd.read_csv(\"./train_folds.csv\")\n    df = pd.read_csv(\"./train_folds_preprop.csv\") #Better results\n    df_train = df[df.kfold != fold].reset_index(drop=True)\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n    \n    Xtrain_glove = np.array([get_feature_vectors(sentence) for sentence in df_train.excerpt.values])\n    Xvalid_glove = np.array([get_feature_vectors(sentence) for sentence in df_valid.excerpt.values])\n    ytrain = df_train.target.values\n    yvalid = df_valid.target.values\n    \n    clf = ensemble.RandomForestRegressor(n_estimators=500, n_jobs=-1)\n    clf.fit(Xtrain_glove, ytrain)\n    pred = clf.predict(Xvalid_glove)\n    \n    rmse = sqrt(metrics.mean_squared_error(yvalid, pred))\n    print(f\"fold={fold}, RMSE={rmse}\")\n    \n    df_valid.loc[:,\"rf_glove_pred\"] = pred\n    \n    return df_valid[[\"id\",\"target\",\"kfold\",\"rf_glove_pred\"]]\n\ndfs = []\nfor j in range(k_folds):\n    temp_df = run_rf_glove(j)\n    dfs.append(temp_df)  \nfin_valid_df = pd.concat(dfs)\nfin_valid_df.to_csv(\"./model_preds/rf_glove_excerpt.csv\", index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#xgboost with Glove on excerpt\ndef run_XGBR(fold):\n    #df = pd.read_csv(\"./train_folds.csv\")\n    df = pd.read_csv(\"./train_folds_preprop.csv\") #Better results\n    df_train = df[df.kfold != fold].reset_index(drop=True)\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n    \n    Xtrain_glove = np.array([get_feature_vectors(sentence) for sentence in df_train.excerpt.values])\n    Xvalid_glove = np.array([get_feature_vectors(sentence) for sentence in df_valid.excerpt.values])\n    ytrain = df_train.target.values\n    yvalid = df_valid.target.values\n    \n\n    clf = XGBRegressor(n_estimators=600,\n                       objective = 'reg:squarederror',\n                       eval_metric = 'rmse',\n                       n_jobs=-1,\n                       subsample = 1.0,\n                       learning_rate=0.05,\n                       max_depth = 5,\n                       early_stopping_rounds = 10,\n                       gamma = 1,\n                       colsample_bytree=0.9,\n                       verbosity = 0,\n                       random_state=seed\n                      )\n    clf.fit(Xtrain_glove, ytrain)\n    pred = clf.predict(Xvalid_glove)\n    \n    rmse = sqrt(metrics.mean_squared_error(yvalid, pred))\n    print(f\"fold={fold}, RMSE={rmse}\")\n    \n    df_valid.loc[:,\"XGBR_glove_pred\"] = pred\n    \n    return df_valid[[\"id\",\"target\",\"kfold\",\"XGBR_glove_pred\"]]\n\ndfs = []\nfor j in range(k_folds):\n    temp_df = run_XGBR(j)\n    dfs.append(temp_df)  \nfin_valid_df = pd.concat(dfs)\nfin_valid_df.to_csv(\"./model_preds/xgboost_glove_excerpt.csv\", index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Word embedding with Roberta","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\n\nclass CLRPDataset(torch.nn.Module):\n    def __init__(self, df, tokenizer, max_len=256):\n        self.excerpt = df['excerpt'].to_numpy()\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n    \n    def __getitem__(self,idx):\n        encode = self.tokenizer(self.excerpt[idx],\n                                return_tensors='pt',\n                                max_length=self.max_len,\n                                padding='max_length',\n                                truncation=True)\n        return encode\n    \n    def __len__(self):\n        return len(self.excerpt)\n    \n\ndef get_embeddings(df, path, plot_losses=True, verbose=True):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"{device} is used\")\n            \n    MODEL_PATH = path\n    model = AutoModel.from_pretrained(MODEL_PATH,output_hidden_states = True,)\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n    model.to(device)\n    model.eval()\n    \n    ds = CLRPDataset(df, tokenizer, 256)\n    dl = DataLoader(ds,\n                    batch_size=128,\n                    shuffle=False,\n                    num_workers = 4,\n                    pin_memory=True,\n                    drop_last=False)\n    \n    embeddings = list()\n    with torch.no_grad():\n        for i, inputs in tqdm(enumerate(dl)):\n            inputs = {key:val.reshape(val.shape[0], -1).to(device) for key, val in inputs.items()}\n            outputs = model(**inputs)\n#https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/#31-running-bert-on-our-text\n            cat = torch.cat(tuple([outputs[2][i] for i in [-4, -3, -2, -1]]), dim=-1)\n            outputs = cat[:, 0, :].detach().cpu().numpy()\n            embeddings.extend(outputs)\n            \n    del model\n    \n    return np.array(embeddings)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# source: https://www.kaggle.com/maunish/clrp-roberta-lgbm\n#https://www.kaggle.com/c/commonlitreadabilityprize/discussion/237795\n#https://www.kaggle.com/abhishek/modelf1\n\nMODEL_NAME = Path('../input/modelf1')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"./train_folds.csv\")\ntrain_embeddings =  get_embeddings(df,MODEL_NAME)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#import xgboost as xgb\n#xgboost with roberta on excerpt\ndef run_XGBR_Roberta(fold):\n    df = pd.read_csv(\"./train_folds.csv\")\n    df_train = df[df.kfold != fold].reset_index(drop=True)\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n    \n    #df = pd.read_csv(\"./train_folds_preprop.csv\")\n    train_idx = df[df.kfold != fold].index\n    valid_idx = df[df.kfold == fold].index\n    #target = df['target'].to_numpy()\n    \n    Xtrain = train_embeddings[train_idx]\n    Xvalid = train_embeddings[valid_idx]\n    ytrain = df_train.target.values\n    yvalid = df_valid.target.values\n    \n\n    clf = XGBRegressor(n_estimators=500,\n                       objective = 'reg:squarederror',\n                       eval_metric = 'rmse',\n                       n_jobs=-1,\n                       subsample = 0.7,\n                       learning_rate=0.05,\n                       max_depth = 4,\n                       early_stopping_rounds = 15,\n                       gamma = 1,\n                       colsample_bytree=1,\n                       verbosity = 0,\n                       random_state=seed,\n                       tree_method='gpu_hist'\n                      )\n    clf.fit(Xtrain, ytrain)\n    pred = clf.predict(Xvalid)\n    \n    rmse = sqrt(metrics.mean_squared_error(yvalid, pred))\n    print(f\"fold={fold}, RMSE={rmse}\")\n    \n    filename = f\"./models/XGBR_roberta_fold_{fold}.sav\"\n    pickle.dump(clf, open(filename, 'wb'))\n    print(\"model saved\")\n    \n    df_valid.loc[:,\"XGBR_roberta_pred\"] = pred\n    \n    return df_valid[[\"id\",\"target\",\"kfold\",\"XGBR_roberta_pred\"]]\n\ndfs = []\nfor j in range(k_folds):\n    temp_df = run_XGBR_Roberta(j)\n    dfs.append(temp_df)  \nfin_valid_df = pd.concat(dfs)\nfin_valid_df.to_csv(\"./model_preds/xgboost_roberta_excerpt.csv\", index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# SEARCH BEST PARAMS XGBOOST\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import make_scorer\n#import warnings\n\ndef rmse(predict, actual):\n    predict = np.array(predict)\n    actual = np.array(actual)\n    distance = predict - actual\n    square_distance = distance ** 2\n    mean_square_distance = square_distance.mean()\n    score = np.sqrt(mean_square_distance)\n\n    return score\n\nrmse_score = make_scorer(rmse, greater_is_better = False)\n\ny_train = df.target.values\n\n\nx_fit,x_test,y_fit,y_test = train_test_split(train_embeddings, y_train, train_size =0.15, \n                            random_state=seed)\nclf = XGBRegressor(\n        eval_metric = 'rmse',\n        #nthread = 4,\n        eta = 0.1,\n        n_estimators = 500,\n        max_depth = 5,\n        subsample = 0.5,\n        gamma = 1,\n        colsample_bytree = 1.0,\n        #silent = 1,\n        verbosity = 0,\n        random_state=seed,\n        tree_method='gpu_hist'\n        )\nparameters = {\n    'n_estimators': [200, 300, 400, 500],\n    'eta': [0.01, 0.05, 0.1],\n    'early_stopping_rounds':[10,15,20],\n    'gamma':[0, 1, 10],\n    'max_depth': [3, 4, 5],\n    'subsample': [0.7, 0.8, 0.9, 1.0],\n    'colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n}\n\nclf1 = RandomizedSearchCV(clf, parameters, n_jobs=-1, scoring=rmse_score, cv=5, return_train_score=True)\nclf1.fit(x_fit, y_fit)\nprint('Best Params: \\n', clf1.best_params_ )\n\nResult = pd.DataFrame(clf1.cv_results_)\nResult","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Transformers model","metadata":{}},{"cell_type":"code","source":"MODEL_NAME = Path('../input/modelf1')\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, model_max_length=256) ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Data(Dataset):\n    def __init__(self, data):\n        super().__init__()\n        self.data = data\n\n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):       \n        excerpt = self.data.excerpt[idx]\n        target = self.data.target[idx]\n        return excerpt, target","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Roberta(PreTrainedModel):\n   def __init__(self, conf):\n       super(Roberta, self).__init__(conf)\n       self.roberta = AutoModel.from_pretrained(MODEL_NAME, config=conf)\n    \n       self.W = torch.nn.Linear(4*self.roberta.config.hidden_size, self.roberta.config.hidden_size) \n       self.drop_out = torch.nn.Dropout(0.4)\n       self.V = torch.nn.Linear(self.roberta.config.hidden_size, 1)\n    \n       self.linear = torch.nn.Linear(4*self.roberta.config.hidden_size, 1)\n\n   def forward(self, input_ids, attention_mask):\n       out = self.roberta(input_ids=input_ids, \n                             attention_mask=attention_mask\n                            )\n       cat = torch.cat(tuple([out[2][i] for i in [-4, -3, -2, -1]]), dim=-1)\n       att = torch.tanh(self.W(cat))\n       score = self.V(att)\n       attention_weights = torch.softmax(score, dim=1)\n       context_vector = attention_weights * cat #* out[2]\n       context_vector = torch.sum(context_vector, dim=1)\n       out = self.drop_out(context_vector)\n       out = self.linear(out)\n        \n       return out","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training Function\n\ndef train(model,\n          optimizer,\n          train_iter,\n          valid_iter,\n          loss_fct,\n          fold,\n          valid_period,\n          num_epochs = 5,\n          scheduler = None,\n          output_path = output_path):\n    \n    # Initialize losses and loss histories\n    train_loss = 0.0\n    valid_loss = 0.0\n\n    train_loss_list = []\n    valid_loss_list = []\n\n    best_valid_loss = float('Inf')\n    \n    global_step = 0\n    global_steps_list = []\n    \n    model.train()\n        # Train loop\n    for epoch in range(num_epochs):\n        for (excerpts, target) in train_iter:\n            \n            batch = tokenizer(list(excerpts), truncation=True, padding=True, return_tensors='pt', add_special_tokens=True)\n            input_ids = batch['input_ids']\n            input_ids = input_ids.to(device, dtype=torch.long)\n            attention_mask = batch['attention_mask']\n            attention_mask = attention_mask.to(device, dtype=torch.long)\n            \n            target=torch.tensor(target).to(device, dtype=torch.float)\n\n            logits = model(input_ids=input_ids,  \n                           attention_mask=attention_mask)\n            loss = loss_fct(torch.squeeze(logits), target)\n            loss.backward()\n            \n            # Optimizer and scheduler step\n            optimizer.step()    \n            scheduler.step()\n            \n            optimizer.zero_grad()\n\n            # Update train loss and global step\n            train_loss += loss.item()\n            global_step += 1\n                        # Validation loop. Save progress and evaluate model performance.\n            if global_step % valid_period == 0:\n                all_preds=[]\n                model.eval()\n                with torch.no_grad():\n                    for (excerpts, target) in valid_iter:\n                        batch = tokenizer(list(excerpts), truncation=True, padding=True, return_tensors='pt', add_special_tokens=True)\n                        input_ids = batch['input_ids']\n                        input_ids = input_ids.to(device, dtype=torch.long)\n                        attention_mask = batch['attention_mask']\n                        attention_mask = attention_mask.to(device, dtype=torch.long)\n\n                        target=torch.tensor(target).to(device, dtype=torch.float)\n\n                        logits = model(input_ids=input_ids, \n                                       attention_mask=attention_mask)\n\n                        preds = torch.squeeze(logits)\n                        loss = loss_fct(preds, target)\n                        valid_loss += loss.item()\n                        all_preds.append(preds.detach().cpu())\n\n                # Store train and validation loss history\n                train_loss = train_loss / valid_period\n                valid_loss = valid_loss / len(valid_iter)\n                train_loss_list.append(train_loss)\n                valid_loss_list.append(valid_loss)\n                global_steps_list.append(global_step)\n\n                # print summary\n                print('Epoch [{}/{}], global step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'\n                      .format(epoch+1, num_epochs, global_step, num_epochs*len(train_iter),\n                              train_loss, valid_loss))\n                                \n                # checkpoint\n                if best_valid_loss > valid_loss:\n                    best_valid_loss = valid_loss\n                    best_pred = np.array(all_preds)\n                    save_path = f'./models/RobertaBase_model-fold-{fold}.pth'\n                    torch.save(model.state_dict(), save_path)\n                        \n                train_loss = 0.0                \n                valid_loss = 0.0\n\n                model.train()\n    return best_pred\n    print('Training done!')\n\n\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_config = RobertaConfig.from_pretrained(MODEL_NAME)\nmodel_config.output_hidden_states = True","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test(model, test_loader):\n    \n    model.eval()\n    all_preds=[]\n    with torch.no_grad():\n        for (excerpts, _) in test_loader:\n            batch = tokenizer(list(excerpts), truncation=True, padding=True, return_tensors='pt', add_special_tokens=False)\n            input_ids = batch['input_ids']\n            input_ids = input_ids.to(device, dtype=torch.long)\n            attention_mask = batch['attention_mask']\n            attention_mask = attention_mask.to(device, dtype=torch.float)\n\n            output = model(input_ids, attention_mask)\n            preds = torch.squeeze(output)\n            all_preds.append(preds.detach().cpu())\n\n    torch.cuda.empty_cache()\n    return np.array(all_preds)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#https://www.kaggle.com/chamecall/clrp-finetune\n\ndef create_optimizer(model):\n    named_parameters = list(model.named_parameters())    \n    \n    roberta_parameters = named_parameters[:197]    \n    attention_parameters = named_parameters[199:203]\n    regressor_parameters = named_parameters[203:]\n        \n    attention_group = [params for (name, params) in attention_parameters]\n    regressor_group = [params for (name, params) in regressor_parameters]\n\n    parameters = []\n    parameters.append({\"params\": attention_group})\n    parameters.append({\"params\": regressor_group})\n\n    for layer_num, (name, params) in enumerate(roberta_parameters):\n        weight_decay = 0.0 if \"bias\" in name else 0.01\n\n        lr = 2e-5\n\n        if layer_num >= 69:        \n            lr = 5e-5\n\n        if layer_num >= 133:\n            lr = 1e-4\n\n        parameters.append({\"params\": params,\n                           \"weight_decay\": weight_decay,\n                           \"lr\": lr})\n\n    return AdamW(parameters)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Main training loop\n# Configuration options\n\ndef run_Roberta(fold):\n    # Configuration options\n    NUM_EPOCHS = 3\n    loss_fct = torch.nn.MSELoss()\n    dfs = pd.read_csv(\"./train_folds.csv\")\n    \n    \n    print('--------------------------------')\n    print(f'FOLD {fold}')\n    print('--------------------------------')\n    df_train = dfs[dfs.kfold != fold].reset_index(drop=True)\n    df_valid = dfs[dfs.kfold == fold].reset_index(drop=True)\n    train_data = Data(data = df_train)\n    valid_data = Data(data = df_valid)\n    yvalid = df_valid.target.values\n    \n    train_loader = torch.utils.data.DataLoader(\n                      train_data, \n                      batch_size=16,\n                      shuffle=True)\n    \n    val_loader = torch.utils.data.DataLoader(\n                      valid_data)\n    \n    model = Roberta(model_config)\n    model = model.to(device)\n\n    optimizer = create_optimizer(model)\n    \n    print(\"======================= Start training =================================\")\n\n    scheduler = get_cosine_schedule_with_warmup(optimizer,\n                                                num_warmup_steps=50,\n                                                num_training_steps= NUM_EPOCHS * len(train_loader))\n\n    pred = train(model=model, \n                  train_iter=train_loader, \n                  valid_iter=val_loader, \n                  optimizer=optimizer, \n                  scheduler=scheduler,\n                  loss_fct =loss_fct,\n                  num_epochs=NUM_EPOCHS,\n                  fold =fold,\n                  valid_period = len(train_loader))\n    \n    \n    rmse = sqrt(metrics.mean_squared_error(yvalid, pred))\n    print(f\"fold={fold}, RMSE={rmse}\")\n    \n    df_valid.loc[:,\"Roberta_pred\"] = pred\n    #torch.cuda.empty_cache() # PyTorch empty cache\n    \n    return df_valid[[\"id\",\"target\",\"kfold\",\"Roberta_pred\"]]\n\ndfs = []\nfor j in range(k_folds):\n    temp_df = run_Roberta(j)\n    dfs.append(temp_df)  \nfin_valid_df = pd.concat(dfs)\nfin_valid_df.to_csv(\"./model_preds/Roberta.csv\", index=False)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import glob\nimport numpy as np\n\nfiles = glob.glob(\"./model_preds/*.csv\")\ndf = None\nfor f in files :\n    if df is None:\n        df = pd.read_csv(f)\n    else:\n        temp_df  = pd.read_csv(f)\n        df = df.merge(temp_df, on=\"id\", how=\"left\")\n\nprint(list(df.columns))\ntargets = df.iloc[:,1].values\n\npred_cols = [\"XGBR_roberta_pred\", \"Roberta_pred\"]\n\nfor col in pred_cols:\n    rmse = sqrt(metrics.mean_squared_error(targets, df[col].values))\n    print(f\"{col}, overall_rmse={rmse}\")\n\nprint(\"average\")\navg_pred = np.mean(df[[\"XGBR_roberta_pred\", \"Roberta_pred\"]].values, axis=1)\nprint(sqrt(metrics.mean_squared_error(targets, avg_pred)))\n\nprint(\"weighted average\")\n#rf_glove_pred = df.rf_glove_pred.values\nXGB_roberta_pred = df.XGBR_roberta_pred.values\nRoberta_pred = df.Roberta_pred.values\n\navg_pred = (Roberta_pred + 2*XGB_roberta_pred)/3\nprint(sqrt(metrics.mean_squared_error(targets, avg_pred)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Blending","metadata":{}},{"cell_type":"code","source":"#optimal weights\nimport glob\nimport numpy as np\nfrom functools import partial\nfrom scipy.optimize import fmin\n\nclass OptimizeRMSE:\n    def __init__(self):\n        self.coef_ =0\n        \n    def _rmse(self, coef, X, y):\n        x_coef = X*coef\n        predictions = np.sum(x_coef, axis=1)\n        rmse_score = sqrt(metrics.mean_squared_error(y, predictions))\n        return 1.0 * rmse_score\n    \n    def fit(self, X, y):\n        partial_loss = partial(self._rmse, X=X, y=y)\n        init_coef = np.random.dirichlet(np.ones(X.shape[1]))\n        self.coef_ = fmin(partial_loss, init_coef, disp=True)\n        \n    def predict(self, X):\n        x_coef = X*self.coef_\n        predictions = np.sum(x_coef, axis=1)\n        return predictions\n    \ndef run_training(pred_df, fold):\n    \n    train_df = pred_df[pred_df.iloc[:,2] != fold].reset_index(drop=True)\n    valid_df = pred_df[pred_df.iloc[:,2] == fold].reset_index(drop=True)\n    \n    xtrain = train_df[[\"XGBR_roberta_pred\", \"Roberta_pred\"]].values\n    xvalid = valid_df[[\"XGBR_roberta_pred\", \"Roberta_pred\"]].values\n    \n    opt = OptimizeRMSE()\n    opt.fit(xtrain, train_df.iloc[:,1].values)\n    return opt.coef_\n     \nfiles = glob.glob(\"./model_preds/*.csv\")\ndf = None\nfor f in files :\n    if df is None:\n        df = pd.read_csv(f)\n    else:\n        temp_df  = pd.read_csv(f)\n        df = df.merge(temp_df, on=\"id\", how=\"left\")\n        \ntargets = df.iloc[:,1].values\npred_cols = [\"XGBR_roberta_pred\", \"Roberta_pred\"]\n\ncoefs = []\nfor j in range(k_folds):\n          coefs.append(run_training(df, j))\ncoefs = np.array(coefs)\nprint(coefs)\n\ncoefs = np.mean(coefs, axis=0)\nprint(coefs)\n\nwt_avg = (coefs[0]*df.XGBR_roberta_pred.values \n          + coefs[1]*df.Roberta_pred.values\n         )\nprint(\"optimal rmse after finding optimal coefs\")\nprint(sqrt(metrics.mean_squared_error(targets, wt_avg)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test","metadata":{}},{"cell_type":"code","source":"#Data preprocessing\n\n#Load Data\ntest_df = pd.read_csv(test_file)\n\n#add fake label in the test only in the aim to use the evalution function\ntest_df.insert(1,'target', 0, False)\ntest_df.to_csv(\"./test/test_with_col_target.csv\", index=False)\n\ntest_preprop=preprop(test_df)\ntest_preprop.to_csv(\"./test/test_preprop.csv\", index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load models","metadata":{}},{"cell_type":"code","source":"df_test = pd.read_csv(\"./test/test_with_col_target.csv\")\nMODEL_NAME = Path('../input/modelf1')\ntest_embeddings = get_embeddings(df_test,MODEL_NAME)\n\ndf_xgboost_roberta= pd.DataFrame({\n     \"id\" : df_test.id.values\n  })\n\n\nfor j in range(k_folds):\n    xgb_roberta_reloaded = pickle.load(open(f'./models/XGBR_roberta_fold_{j}.sav','rb'))\n    df_xgboost_roberta[f\"fold_{j}\"] = xgb_roberta_reloaded.predict(test_embeddings)\n    \ndf_xgboost_roberta['target_xgb_roberta'] = df_xgboost_roberta.iloc[:, 1:].mean(axis=1)\ndf_xgboost_roberta = df_xgboost_roberta[['id','target_xgb_roberta']]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ROBERTA_Base\ndf_test = pd.read_csv(\"./test/test_with_col_target.csv\") #Better results\ntest_data = Data(data = df_test) \ntest_loader = DataLoader(dataset = test_data, shuffle=False)#, batch_size = 64)\n\ndf_Roberta= pd.DataFrame({\n     \"id\" : df_test.id.values\n  })\nfor j in range(k_folds):\n    torch.cuda.empty_cache() # PyTorch empty cache\n    \n    model = Roberta(model_config)    \n    load_path = f'./models/RobertaBase_model-fold-{j}.pth'\n    model.load_state_dict(torch.load(load_path))\n    model = model.to(device)\n    \n    # Print about testing\n    print(f'Starting testing - fold_{j}')    \n    model_return = test(model=model,\n        test_loader = test_loader\n        )\n    \n    \n    df_Roberta[f\"fold_{j}\"] = model_return\n    \ndf_Roberta['target_Roberta'] = df_Roberta.iloc[:, 1:].mean(axis=1)\ndf_Roberta = df_Roberta[['id','target_Roberta']]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_avg = (coefs[0]*df_xgboost_roberta.target_xgb_roberta.values\n          + coefs[1]*df_Roberta.target_Roberta.values\n         )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df= pd.DataFrame({\n     \"id\" : df_test.id.values\n  })\ndf['target']=test_avg\n\ndf.to_csv(f\"{output_path}/submission.csv\",index=False)\n","metadata":{},"execution_count":null,"outputs":[]}]}