{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","scrolled":true,"execution":{"iopub.status.busy":"2021-06-11T09:19:36.808687Z","iopub.execute_input":"2021-06-11T09:19:36.809249Z","iopub.status.idle":"2021-06-11T09:19:37.888168Z","shell.execute_reply.started":"2021-06-11T09:19:36.809165Z","shell.execute_reply":"2021-06-11T09:19:37.88697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.path.append('../input/sentence-transformers/sentence-transformers-master')\nsys.path.append('../input/huggingface-bert-variants')\nsys.path.append('../input/huggingface-roberta-variants')","metadata":{"execution":{"iopub.status.busy":"2021-06-11T09:19:37.88956Z","iopub.execute_input":"2021-06-11T09:19:37.889881Z","iopub.status.idle":"2021-06-11T09:19:37.89437Z","shell.execute_reply.started":"2021-06-11T09:19:37.889851Z","shell.execute_reply":"2021-06-11T09:19:37.893282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom tqdm import tqdm\ntqdm.pandas()\nfrom collections import Counter\nimport statsmodels.api as sm\nfrom scipy.stats import skew, iqr, kurtosis\n\n#notebook formatting\n#from rich.jupyter import print\n#from rich.console import Console\n#from rich.theme import Theme\n#from rich import pretty\n\n#visualization imports \nimport seaborn as sns\nimport matplotlib.pyplot as plt \n%matplotlib inline\n\n#sklearn imports\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression, Ridge, BayesianRidge\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\n#nlp imports \nimport re\nimport string\nfrom nltk import pos_tag\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem.snowball import SnowballStemmer\n\nimport spacy\nfrom textblob import TextBlob","metadata":{"execution":{"iopub.status.busy":"2021-06-11T09:19:37.896584Z","iopub.execute_input":"2021-06-11T09:19:37.897022Z","iopub.status.idle":"2021-06-11T09:19:41.158448Z","shell.execute_reply.started":"2021-06-11T09:19:37.89698Z","shell.execute_reply":"2021-06-11T09:19:41.157539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import transformers\nimport sentence_transformers\nfrom sentence_transformers import SentenceTransformer, models","metadata":{"execution":{"iopub.status.busy":"2021-06-11T09:19:41.160129Z","iopub.execute_input":"2021-06-11T09:19:41.16043Z","iopub.status.idle":"2021-06-11T09:19:48.835532Z","shell.execute_reply.started":"2021-06-11T09:19:41.160403Z","shell.execute_reply":"2021-06-11T09:19:48.834636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nlp_eng_emb = spacy.load(\"en_core_web_lg\")","metadata":{"execution":{"iopub.status.busy":"2021-06-11T09:19:48.83672Z","iopub.execute_input":"2021-06-11T09:19:48.836998Z","iopub.status.idle":"2021-06-11T09:19:58.21666Z","shell.execute_reply.started":"2021-06-11T09:19:48.836971Z","shell.execute_reply":"2021-06-11T09:19:58.215887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##seed everything\ndef seed_everything(seed=0):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\n\nseed = 42\nseed_everything(seed)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T09:19:58.217659Z","iopub.execute_input":"2021-06-11T09:19:58.218035Z","iopub.status.idle":"2021-06-11T09:19:58.22234Z","shell.execute_reply.started":"2021-06-11T09:19:58.218008Z","shell.execute_reply":"2021-06-11T09:19:58.221628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#define your own rmse and set greater_is_better=False\ndef rmse_custom(y_true, y_pred):\n    return np.sqrt(((y_true - y_pred) ** 2).mean())\n\nrmse = make_scorer(rmse_custom, greater_is_better=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T09:19:58.223233Z","iopub.execute_input":"2021-06-11T09:19:58.223686Z","iopub.status.idle":"2021-06-11T09:19:58.237187Z","shell.execute_reply.started":"2021-06-11T09:19:58.223657Z","shell.execute_reply":"2021-06-11T09:19:58.236159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/commonlitreadabilityprize/train.csv')\nprint(f\"Shape df train: {df_train.shape}\")\ndf_test = pd.read_csv('/kaggle/input/commonlitreadabilityprize/test.csv')\ntest_id = df_test[['id']]\nprint(f\"Shape df test: {df_test.shape}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-11T09:19:58.241804Z","iopub.execute_input":"2021-06-11T09:19:58.242079Z","iopub.status.idle":"2021-06-11T09:19:58.338641Z","shell.execute_reply.started":"2021-06-11T09:19:58.242053Z","shell.execute_reply":"2021-06-11T09:19:58.337674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#drop columns that we are not going to use in training:\ncolumns_drop = ['url_legal', 'license', 'id', 'standard_error']\nfor df in [df_train, df_test]:\n    for col in columns_drop:\n        try: df = df.drop(col, axis=1)\n        except: pass","metadata":{"execution":{"iopub.status.busy":"2021-06-11T09:19:58.340192Z","iopub.execute_input":"2021-06-11T09:19:58.34047Z","iopub.status.idle":"2021-06-11T09:19:58.351095Z","shell.execute_reply.started":"2021-06-11T09:19:58.340443Z","shell.execute_reply":"2021-06-11T09:19:58.350376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_pos(excerpt):\n    '''Returns number of nouns, adj and verbs in the text'''\n    nouns = ['NN', 'NNS', 'NNP', 'NNPS']\n    adjectives = ['JJ', 'JJR', 'JJS']\n    verbs = ['VB','VBD','VBG','VBN','VBP','VBZ']\n    #first tokenize in words\n    text_tokens = word_tokenize(excerpt)\n    pos_lst = pos_tag(text_tokens)\n    num_nouns = len([noun[0] for noun in pos_lst if noun[1] in nouns])\n    num_adj = len([adj[0] for adj in pos_lst if adj[1] in adjectives])\n    num_verbs = len([verb[0] for verb in pos_lst if verb[1] in verbs])\n    \n    return [num_nouns, num_adj, num_verbs]","metadata":{"execution":{"iopub.status.busy":"2021-06-11T09:19:58.352274Z","iopub.execute_input":"2021-06-11T09:19:58.352774Z","iopub.status.idle":"2021-06-11T09:19:58.366143Z","shell.execute_reply.started":"2021-06-11T09:19:58.352744Z","shell.execute_reply":"2021-06-11T09:19:58.365439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#get polarity and subjecivity \n\ntext_blob_obj = TextBlob('I hate football')\n","metadata":{"execution":{"iopub.status.busy":"2021-06-11T09:19:58.367321Z","iopub.execute_input":"2021-06-11T09:19:58.367816Z","iopub.status.idle":"2021-06-11T09:19:58.378078Z","shell.execute_reply.started":"2021-06-11T09:19:58.367783Z","shell.execute_reply":"2021-06-11T09:19:58.377019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_blob_obj.sentiment","metadata":{"execution":{"iopub.status.busy":"2021-06-11T09:19:58.379417Z","iopub.execute_input":"2021-06-11T09:19:58.379837Z","iopub.status.idle":"2021-06-11T09:19:58.456418Z","shell.execute_reply.started":"2021-06-11T09:19:58.379807Z","shell.execute_reply":"2021-06-11T09:19:58.455705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_blob_obj.sentiment.polarity","metadata":{"execution":{"iopub.status.busy":"2021-06-11T09:19:58.457729Z","iopub.execute_input":"2021-06-11T09:19:58.458291Z","iopub.status.idle":"2021-06-11T09:19:58.464044Z","shell.execute_reply.started":"2021-06-11T09:19:58.458236Z","shell.execute_reply":"2021-06-11T09:19:58.463079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_blob_obj.sentiment.subjectivity","metadata":{"execution":{"iopub.status.busy":"2021-06-11T09:19:58.465466Z","iopub.execute_input":"2021-06-11T09:19:58.466063Z","iopub.status.idle":"2021-06-11T09:19:58.476668Z","shell.execute_reply.started":"2021-06-11T09:19:58.466018Z","shell.execute_reply":"2021-06-11T09:19:58.475709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#First let´s generate new features in the train and test set dataset\n\nfor df in [df_train, df_test]:\n    print(f\" ====== Generating Counting features..... =======\")\n    df['excerpt_length'] = df['excerpt'].apply(lambda x: len(x))\n    df['excerpt_num_words'] = df['excerpt'].apply(lambda x: len(word_tokenize(x)))\n    df['excerpt_num_sentences'] = df['excerpt'].apply(lambda x: len(sent_tokenize(x)))\n    df['count_exclamation_mark'] = df['excerpt'].apply(lambda x: x.count('!'))\n    df['count_question_mark'] = df['excerpt'].apply(lambda x: x.count('?'))\n    df['count_punctuation'] =  df['excerpt'].apply(lambda x: sum([x.count(punct) for punct in '.,;:']))\n\n    #POS features\n    print(f\" ====== Generating POS features..... =======\")\n    df['num_nouns'], df['num_adj'], df['num_verbs'] = zip(*df['excerpt'].apply(lambda x: get_pos(x)))\n    # proportion of nouns, adj and verbs with respect to the total number of words\n    df['nouns_proportion'] = df['num_nouns'] / df['excerpt_num_words']\n    df['adj_proportion'] = df['num_adj'] / df['excerpt_num_words']\n    df['verbs_proportion'] = df['num_verbs'] / df['excerpt_num_words']\n    \n    #Text Blob features: polarity and subjectivity\n    print(f\" ====== Generating Text Blob features..... =======\")\n    df['polarity'] = df['excerpt'].apply(lambda x: TextBlob(x).sentiment.polarity)\n    df['subjectivity'] = df['excerpt'].apply(lambda x: TextBlob(x).sentiment.subjectivity)\n\n    #More additional features\n    print(f\" ====== Generating Additional Features.... =======\")\n    df['num_words_capital'] = df['excerpt'].apply(lambda x: len([word for word in x.split() if word.istitle()]))\n    #average lenght of tokens (words) and sentences in each excerpt\n    df['avg_len_words'] = df['excerpt'].apply(lambda x: np.mean([len(word) for word in x.split()]))\n    df['avg_len_sentences'] = df['excerpt'].apply(lambda x: np.mean([len(sent) for sent in sent_tokenize(x)]))","metadata":{"execution":{"iopub.status.busy":"2021-06-11T09:19:58.47821Z","iopub.execute_input":"2021-06-11T09:19:58.478717Z","iopub.status.idle":"2021-06-11T09:20:48.80488Z","shell.execute_reply.started":"2021-06-11T09:19:58.478676Z","shell.execute_reply":"2021-06-11T09:20:48.803967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_text(excerpt, remove_stopwords=False, lemmatizer=False):\n    #remove punctuation '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n    excerpt = re.sub(f\"[{string.punctuation}]\", '', excerpt)\n    weird_characters = ['\\n', '—', '”', '“', '\\xad', '…', '½', '¼', \n                            'æ', '°', '±', '·', '‘', '´', '–', '÷']\n    for character in weird_characters:\n        excerpt = excerpt.replace(character, \"\")\n    #remove numbers\n    excerpt = re.sub('[0-9]', '', excerpt)\n    text_token = word_tokenize(excerpt)\n    \n    clean_words_list = [w for w in text_token if len(w) > 2]\n    \n    #remove stopwords\n    if remove_stopwords:\n        stop_words = [w for w in stopwords.words('english')]\n        clean_words_list = [w for w in clean_words_list if w not in set(stop_words) and len(w) > 2]\n    \n    ##TODO: add lemmatizer##\n    if lemmatizer:\n        lemmatizer = WordNetLemmatizer()\n        clean_words_list = [lemmatizer.lemmatize(word) for word in clean_words_list]\n        lemmatizer_snow_ball = SnowballStemmer(\"english\")\n        clean_words_list = [lemmatizer_snow_ball.stem(word) for word in clean_words_list]\n        \n    clean_text = ' '.join(clean_words_list)\n\n    return clean_text.lower()","metadata":{"execution":{"iopub.status.busy":"2021-06-11T09:20:48.805887Z","iopub.execute_input":"2021-06-11T09:20:48.806149Z","iopub.status.idle":"2021-06-11T09:20:48.815447Z","shell.execute_reply.started":"2021-06-11T09:20:48.806115Z","shell.execute_reply":"2021-06-11T09:20:48.814499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_text_sbert(excerpt, remove_stopwords=False, lemmatizer=False):\n    #remove punctuation '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n    remove_punctuation = '\"#$%&\\'()*+,-/<=>?@[\\\\]^_`{|}~'\n    excerpt = re.sub(f\"[{remove_punctuation}]\", '', excerpt)\n    excerpt = excerpt.lower()\n    weird_characters = ['\\n', '—', '”', '“', '\\xad', '…', '½', '¼', \n                            'æ', '°', '±', '·', '‘', '´', '–', '÷']\n    for character in weird_characters:\n        excerpt = excerpt.replace(character, \"\")\n    #remove numbers\n    excerpt = re.sub('[0-9]', '', excerpt)\n    \n    #clean_words_list = [w for w in text_token if len(w) > 2]\n    \n    #clean_text = ' '.join(clean_words_list)\n\n    return excerpt","metadata":{"execution":{"iopub.status.busy":"2021-06-11T09:20:48.816693Z","iopub.execute_input":"2021-06-11T09:20:48.816999Z","iopub.status.idle":"2021-06-11T09:20:48.830326Z","shell.execute_reply.started":"2021-06-11T09:20:48.816971Z","shell.execute_reply":"2021-06-11T09:20:48.829271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_train['excerpt'] = df_train['excerpt'].apply(lambda x: clean_text_sbert(x, remove_stopwords=False, lemmatizer=False))\n# df_test['excerpt'] = df_test['excerpt'].apply(lambda x: clean_text_sbert(x, remove_stopwords=False, lemmatizer=False))","metadata":{"execution":{"iopub.status.busy":"2021-06-11T09:20:48.831817Z","iopub.execute_input":"2021-06-11T09:20:48.83213Z","iopub.status.idle":"2021-06-11T09:20:48.840288Z","shell.execute_reply.started":"2021-06-11T09:20:48.832103Z","shell.execute_reply":"2021-06-11T09:20:48.839355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ner_entities_lst = ['PERSON', 'NORP', 'FAC', 'ORG', 'GPE', 'LOC', 'PRODUCT', 'EVENT', 'WORK_OF_ART',\n                    'LAW', 'LANGUAGE', 'DATE', 'TIME', 'PERCENT', 'MONEY', 'QUANTITY', 'ORDINAL', 'CARDINAL']\n\nfor ent in ner_entities_lst:\n    df_train['count_' + ent] = 0.0\n    df_test['count_' + ent] = 0.0","metadata":{"execution":{"iopub.status.busy":"2021-06-11T09:20:48.841475Z","iopub.execute_input":"2021-06-11T09:20:48.841803Z","iopub.status.idle":"2021-06-11T09:20:48.867705Z","shell.execute_reply.started":"2021-06-11T09:20:48.841776Z","shell.execute_reply":"2021-06-11T09:20:48.866785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_ner_tags(excerpt):\n    entities = []\n    doc = nlp_eng_emb(excerpt)\n    for ent in doc.ents:\n        entities.append(ent.label_)\n    return dict(Counter(entities))","metadata":{"execution":{"iopub.status.busy":"2021-06-11T09:20:48.869122Z","iopub.execute_input":"2021-06-11T09:20:48.869511Z","iopub.status.idle":"2021-06-11T09:20:48.880161Z","shell.execute_reply.started":"2021-06-11T09:20:48.869482Z","shell.execute_reply":"2021-06-11T09:20:48.879162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['ner'] = df_train['excerpt'].apply(get_ner_tags)\ndf_test['ner'] = df_test['excerpt'].apply(get_ner_tags)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T09:20:48.881673Z","iopub.execute_input":"2021-06-11T09:20:48.882069Z","iopub.status.idle":"2021-06-11T09:22:29.782764Z","shell.execute_reply.started":"2021-06-11T09:20:48.882005Z","shell.execute_reply":"2021-06-11T09:22:29.78171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for df in [df_train, df_test]:\n    for index, row in tqdm(df.iterrows()):\n        for key, value in row['ner'].items():\n            df.loc[index, 'count_' + key] = value\n    del df['ner']    ","metadata":{"execution":{"iopub.status.busy":"2021-06-11T09:22:29.784021Z","iopub.execute_input":"2021-06-11T09:22:29.784333Z","iopub.status.idle":"2021-06-11T09:22:32.673545Z","shell.execute_reply.started":"2021-06-11T09:22:29.784304Z","shell.execute_reply":"2021-06-11T09:22:32.672878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Modeling","metadata":{}},{"cell_type":"code","source":"##SELECT FETAURES \n\nfeatures_training = []\n\ncol_vec_dim = [i for i in range(256)] #dimensions of the vector\nfeatures_training.extend(col_vec_dim)\n\nfeatures_eda = ['excerpt_length', 'excerpt_num_words', 'excerpt_num_sentences','num_nouns', 'num_adj', \n                   'num_verbs', 'nouns_proportion','adj_proportion', 'verbs_proportion', 'num_words_capital',\n                   'count_exclamation_mark', 'count_question_mark', 'count_punctuation', 'avg_len_words', 'avg_len_sentences']\nfeatures_training.extend(features_eda)\n\nfeatures_training.extend(['polarity', 'subjectivity'])\n\nfeatures_ner = ['count_' + ent for ent in ner_entities_lst] #features NER\nfeatures_training.extend(features_ner)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T09:22:32.675964Z","iopub.execute_input":"2021-06-11T09:22:32.676222Z","iopub.status.idle":"2021-06-11T09:22:32.682953Z","shell.execute_reply.started":"2021-06-11T09:22:32.676197Z","shell.execute_reply":"2021-06-11T09:22:32.681921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch import nn","metadata":{"execution":{"iopub.status.busy":"2021-06-11T09:22:32.684345Z","iopub.execute_input":"2021-06-11T09:22:32.684635Z","iopub.status.idle":"2021-06-11T09:22:32.694272Z","shell.execute_reply.started":"2021-06-11T09:22:32.684589Z","shell.execute_reply":"2021-06-11T09:22:32.693504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#BASE_MODEL = '/kaggle/input/huggingface-bert-variants/bert-base-uncased/bert-base-uncased'\nBASE_MODEL = '/kaggle/input/huggingface-roberta-variants/roberta-base/roberta-base'\nword_embedding_model = models.Transformer(BASE_MODEL)\npooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\ndense_model = models.Dense(in_features=pooling_model.get_sentence_embedding_dimension(), \n                           out_features=256, \n                           activation_function=nn.Tanh())\nmodel_sbert = SentenceTransformer(modules=[word_embedding_model, pooling_model, dense_model])","metadata":{"execution":{"iopub.status.busy":"2021-06-11T09:22:32.695144Z","iopub.execute_input":"2021-06-11T09:22:32.695393Z","iopub.status.idle":"2021-06-11T09:22:41.373515Z","shell.execute_reply.started":"2021-06-11T09:22:32.695368Z","shell.execute_reply":"2021-06-11T09:22:41.372717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(columns=[i for i in range(256)])\nfor excerpt in tqdm(list(df_train['excerpt'])):\n#     vec = nlp_eng_emb(excerpt).vector\n    vec =  model_sbert.encode(excerpt)\n    vec_series = pd.Series(list(vec), index=df.columns)\n    df = df.append(vec_series, ignore_index=True)\n    \n    \ntrain = pd.concat([df_train, df], axis=1)\ntrain.head()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-06-11T09:22:41.374685Z","iopub.execute_input":"2021-06-11T09:22:41.375126Z","iopub.status.idle":"2021-06-11T09:44:37.396392Z","shell.execute_reply.started":"2021-06-11T09:22:41.375081Z","shell.execute_reply":"2021-06-11T09:44:37.395642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.DataFrame(columns=[i for i in range(256)])\nfor excerpt in tqdm(list(df_test['excerpt'])):\n#     vec = nlp_eng_emb(excerpt).vector\n    vec =  model_sbert.encode(excerpt)\n    vec_series = pd.Series(list(vec), index=df.columns)\n    test = test.append(vec_series, ignore_index=True)\n\ndf_pred = pd.concat([df_test, test], axis=1)\ncolumns_drop = ['url_legal', 'license', 'id', 'excerpt']\nfor col in columns_drop:\n    df_pred = df_pred.drop(col, axis=1)\ndf_pred.head()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-06-11T09:44:37.397377Z","iopub.execute_input":"2021-06-11T09:44:37.39781Z","iopub.status.idle":"2021-06-11T09:44:40.640177Z","shell.execute_reply.started":"2021-06-11T09:44:37.397778Z","shell.execute_reply":"2021-06-11T09:44:40.639156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nX_train, X_test, y_train, y_test = train_test_split(train, train['target'], test_size=0.01,\n                                                   random_state=seed)\n\nprint(f\"Shape X_train: {X_train.shape}\")\nprint(f\"Shape X_test: {X_test.shape}\")\n\nX_train = train[features_training]\ny_train = train['target']\n#filter X_train by columns training","metadata":{"execution":{"iopub.status.busy":"2021-06-11T09:44:40.64149Z","iopub.execute_input":"2021-06-11T09:44:40.641914Z","iopub.status.idle":"2021-06-11T09:44:40.675565Z","shell.execute_reply.started":"2021-06-11T09:44:40.64187Z","shell.execute_reply":"2021-06-11T09:44:40.674542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipe_spacy = Pipeline([('scaler', MinMaxScaler()),\n                        ('bayesian_ridge', BayesianRidge())]\n                        )\n\nparam_grid_spacy = {\n\n}\n\nmodel_spacy = GridSearchCV(estimator=pipe_spacy,\n                                param_grid=param_grid_spacy,\n                                scoring=rmse,\n                                cv=10,\n                                verbose=3)\n\n\nmodel_spacy.fit(X_train, y_train)\nprint(f'Best params are : {model_spacy.best_params_}')\nprint(f'Best training score: {round(model_spacy.best_score_, 5)}')\n\n#y_pred = model_spacy.predict(X_test[features_training])\n#print(f\"RMSE baseline with testing set: {round(rmse_custom(y_test, y_pred), 5)}\")","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-06-11T09:44:40.676662Z","iopub.execute_input":"2021-06-11T09:44:40.676925Z","iopub.status.idle":"2021-06-11T09:44:43.240384Z","shell.execute_reply.started":"2021-06-11T09:44:40.676899Z","shell.execute_reply":"2021-06-11T09:44:43.239273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### XGBOOST REGRESSOR\n\n- gamma: The default is 0. Values of less than 10 are standard. Increasing the value prevents overfitting.\n- reg_alpha: L1 regularization on leaf weights. Larger values mean more regularization and prevent overfitting. The default is 0.\n- reg_lambda: L2 regularization on leaf weights. Increasing the value prevents overfitting. The default is 1\n- booster: gbtree","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb","metadata":{"execution":{"iopub.status.busy":"2021-06-11T10:37:11.769267Z","iopub.execute_input":"2021-06-11T10:37:11.769692Z","iopub.status.idle":"2021-06-11T10:37:11.774007Z","shell.execute_reply.started":"2021-06-11T10:37:11.769659Z","shell.execute_reply":"2021-06-11T10:37:11.772944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_regressor = xgb.XGBRegressor(booster='gbtree', \n                                reg_lambda=10,\n                                max_depth=4)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T10:37:13.368522Z","iopub.execute_input":"2021-06-11T10:37:13.369077Z","iopub.status.idle":"2021-06-11T10:37:13.373105Z","shell.execute_reply.started":"2021-06-11T10:37:13.369032Z","shell.execute_reply":"2021-06-11T10:37:13.372353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_regressor.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T10:37:57.701512Z","iopub.execute_input":"2021-06-11T10:37:57.702102Z","iopub.status.idle":"2021-06-11T10:38:03.438264Z","shell.execute_reply.started":"2021-06-11T10:37:57.702051Z","shell.execute_reply":"2021-06-11T10:38:03.437207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train","metadata":{"execution":{"iopub.status.busy":"2021-06-11T10:41:38.338802Z","iopub.execute_input":"2021-06-11T10:41:38.33928Z","iopub.status.idle":"2021-06-11T10:41:38.346755Z","shell.execute_reply.started":"2021-06-11T10:41:38.33925Z","shell.execute_reply":"2021-06-11T10:41:38.346118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean_squared_error(np.array(y_train), xgb_regressor.predict(X_train), squared=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T10:42:35.705518Z","iopub.execute_input":"2021-06-11T10:42:35.705877Z","iopub.status.idle":"2021-06-11T10:42:35.738109Z","shell.execute_reply.started":"2021-06-11T10:42:35.705848Z","shell.execute_reply":"2021-06-11T10:42:35.737115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pipe_spacy = Pipeline([('scaler', MinMaxScaler()),\n#                         ('svr', SVR())]\n#                         )\n\n# param_grid_spacy = {\n#     'svr__C': [1, 2.2, 4, 8],\n#     'svr__gamma': [0.1, 0.08, 0.01],\n#     'svr__kernel': ['rbf'], \n#     'svr__epsilon': [1, 0.1, 0.01]\n\n# }\n\n# model_spacy = GridSearchCV(estimator=pipe_spacy,\n#                                 param_grid=param_grid_spacy,\n#                                 scoring=rmse,\n#                                 cv=10,\n#                                 verbose=3)\n\n\n# model_spacy.fit(X_train[features_training], y_train)\n# print(f'Best params are : {model_spacy.best_params_}')\n# print(f'Best training score: {round(model_spacy.best_score_, 5)}')\n\n#y_pred = model_spacy.predict(X_test[features_training])\n#print(f\"RMSE baseline with testing set: {round(rmse_custom(y_test, y_pred), 5)}\")","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-06-11T07:31:24.946239Z","iopub.status.idle":"2021-06-11T07:31:24.946711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### CROSS VALIDATION STRATEGY\n\n- out of fold (oof) score based on predictions made by data not used to train a model, using the validations folds.\n- Traget follows a normal distribution, we are going to stratify the training dataset using that column","metadata":{"execution":{"iopub.status.busy":"2021-06-02T09:16:01.490126Z","iopub.execute_input":"2021-06-02T09:16:01.493815Z","iopub.status.idle":"2021-06-02T09:16:01.497943Z","shell.execute_reply.started":"2021-06-02T09:16:01.493768Z","shell.execute_reply":"2021-06-02T09:16:01.4967Z"}}},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold","metadata":{"execution":{"iopub.status.busy":"2021-06-11T07:34:00.262708Z","iopub.execute_input":"2021-06-11T07:34:00.26309Z","iopub.status.idle":"2021-06-11T07:34:00.266952Z","shell.execute_reply.started":"2021-06-11T07:34:00.263057Z","shell.execute_reply":"2021-06-11T07:34:00.266018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#create groups/bins in the target \ntrain['target_bin'] = pd.cut(train['target'], bins=10, labels=[i + 1 for i in range(10)])","metadata":{"execution":{"iopub.status.busy":"2021-06-11T07:34:00.584868Z","iopub.execute_input":"2021-06-11T07:34:00.585247Z","iopub.status.idle":"2021-06-11T07:34:00.597152Z","shell.execute_reply.started":"2021-06-11T07:34:00.585212Z","shell.execute_reply":"2021-06-11T07:34:00.596067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T07:34:00.65403Z","iopub.execute_input":"2021-06-11T07:34:00.654395Z","iopub.status.idle":"2021-06-11T07:34:00.659678Z","shell.execute_reply.started":"2021-06-11T07:34:00.654363Z","shell.execute_reply":"2021-06-11T07:34:00.658707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = train[features_training]\ny_train = train['target']","metadata":{"execution":{"iopub.status.busy":"2021-06-11T07:34:01.046627Z","iopub.execute_input":"2021-06-11T07:34:01.047201Z","iopub.status.idle":"2021-06-11T07:34:01.056242Z","shell.execute_reply.started":"2021-06-11T07:34:01.047166Z","shell.execute_reply":"2021-06-11T07:34:01.055026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import MaxAbsScaler","metadata":{"execution":{"iopub.status.busy":"2021-06-11T07:34:01.444447Z","iopub.execute_input":"2021-06-11T07:34:01.44505Z","iopub.status.idle":"2021-06-11T07:34:01.449295Z","shell.execute_reply.started":"2021-06-11T07:34:01.445016Z","shell.execute_reply":"2021-06-11T07:34:01.448125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = []\ntraining_scores = []\n\ndf_out_of_fold = train.copy()\ndf_out_of_fold['out_of_fold'] = 0\n\nfor fold, (train_idx, val_idx) in enumerate(list(skfold.split(X=X_train, y=train['target_bin']))):\n    print(f\"\\n Training FOLD: {fold + 1} / {10}\")\n    \n    #bayesian_ridge = BayesianRidge()\n    pipe_br = make_pipeline(MinMaxScaler(), BayesianRidge())\n    pipe_br.fit(X_train.loc[train_idx], y_train.loc[train_idx])\n    train_rmse = mean_squared_error(y_train.loc[train_idx], pipe_br.predict(X_train.loc[train_idx]), squared=False)\n    training_scores.append(train_rmse)\n    print(f'Fold {fold + 1}: Training score: {round(train_rmse, 4)}')\n    \n    #precit traget for each fold >> submission values\n    #print(pipe_br.predict(df_pred))\n    predictions.append(pipe_br.predict(df_pred[features_training]))\n    #now let´s predict the results with the validation (not used for training) set of each fold\n    pred_oof = pipe_br.predict(X_train.loc[val_idx])\n    df_out_of_fold['out_of_fold'].iloc[val_idx] += pred_oof\n\nprint(f'Training score: {round(np.mean(training_scores), 4)}, Training STD: {round(np.std(training_scores), 4)}')\nprint(f'Oout of fold score across folds: {round(mean_squared_error(df_out_of_fold.target, df_out_of_fold.out_of_fold, squared=False), 5)}')","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-06-11T07:34:01.509372Z","iopub.execute_input":"2021-06-11T07:34:01.509733Z","iopub.status.idle":"2021-06-11T07:34:04.400694Z","shell.execute_reply.started":"2021-06-11T07:34:01.5097Z","shell.execute_reply":"2021-06-11T07:34:04.399158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### INFERIENCE","metadata":{}},{"cell_type":"code","source":"predictions = xgb_regressor.predict(df_pred)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T10:57:39.864716Z","iopub.execute_input":"2021-06-11T10:57:39.865132Z","iopub.status.idle":"2021-06-11T10:57:39.889894Z","shell.execute_reply.started":"2021-06-11T10:57:39.865091Z","shell.execute_reply":"2021-06-11T10:57:39.888956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_id['target'] = np.mean(predictions, axis=0)\ndf_submission = test_id[['id', 'target']]\ndf_submission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-03T15:18:01.850769Z","iopub.execute_input":"2021-06-03T15:18:01.851089Z","iopub.status.idle":"2021-06-03T15:18:01.998035Z","shell.execute_reply.started":"2021-06-03T15:18:01.851057Z","shell.execute_reply":"2021-06-03T15:18:01.997298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_submission","metadata":{"execution":{"iopub.status.busy":"2021-06-03T15:18:03.98269Z","iopub.execute_input":"2021-06-03T15:18:03.983008Z","iopub.status.idle":"2021-06-03T15:18:03.992611Z","shell.execute_reply.started":"2021-06-03T15:18:03.982979Z","shell.execute_reply":"2021-06-03T15:18:03.991333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}