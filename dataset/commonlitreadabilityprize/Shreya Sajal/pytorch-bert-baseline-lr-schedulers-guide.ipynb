{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":" <h1 style=\"font-family:verdana;\"> <center>BERT BASELINE AND A WALKTHROUGH THE LEARNING RATE SCHEDULERS </center> </h1>","metadata":{}},{"cell_type":"markdown","source":"\n<p style=\"color:#159364; font-family:cursive;\">INSTALL THE TRANSFORMERS PACKAGE FROM THE HUGGING FACE LIBRARY</center></p>\n","metadata":{}},{"cell_type":"code","source":"!pip install transformers","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"color:#159364; font-family:cursive;\">IMPORT THE LIBRARIES</center></p>","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport copy\nimport datetime\nimport time\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda import amp\nimport transformers\nfrom transformers import BertTokenizer,BertForSequenceClassification, BertModel, BertConfig\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nfrom tqdm import tqdm\nfrom collections import defaultdict\nimport plotly.graph_objects as go\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold, KFold\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"color:#159364; font-family:cursive;\">LOOK AT THE DATA</center></p>","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/commonlitreadabilityprize/train.csv\")\ntest_df = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\",usecols=[\"id\",\"excerpt\"])\nprint('Number of training sentences: {:,}\\n'.format(df.shape[0]))\ndf.sample(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"color:#159364; font-family:cursive;\">A BIT OF PREPROCESSING</center></p>","metadata":{}},{"cell_type":"code","source":"def prep_text(text_df):\n    text_df = text_df.str.replace(\"\\n\",\"\",regex=False) \n    return text_df.str.replace(\"\\'s\",r\"s\",regex=True).values\ndf[\"excerpt\"] = prep_text(df[\"excerpt\"])\ntest_df[\"excerpt\"] = prep_text(test_df[\"excerpt\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"color:#159364; font-family:cursive;\">CREATE FOLDS</center></p>","metadata":{}},{"cell_type":"markdown","source":"Code taken from:https://www.kaggle.com/abhishek/step-1-create-folds","metadata":{}},{"cell_type":"code","source":"def create_folds(data, num_splits):\n    # we create a new column called kfold and fill it with -1\n    data[\"kfold\"] = -1\n    \n    # the next step is to randomize the rows of the data\n    data = data.sample(frac=1).reset_index(drop=True)\n\n    # calculate number of bins by Sturge's rule\n    # I take the floor of the value, you can also\n    # just round it\n    num_bins = int(np.floor(1 + np.log2(len(data))))\n    \n    # bin targets\n    data.loc[:, \"bins\"] = pd.cut(\n        data[\"target\"], bins=num_bins, labels=False\n    )\n    \n    # initiate the kfold class from model_selection module\n    kf = StratifiedKFold(n_splits=num_splits)\n    \n    # fill the new kfold column\n    # note that, instead of targets, we use bins!\n    for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)):\n        data.loc[v_, 'kfold'] = f\n    \n    # drop the bins column\n    data = data.drop(\"bins\", axis=1)\n\n    # return dataframe with folds\n    return data\n\n\n# create folds\ndf = create_folds(df, num_splits=5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"color:#159364; font-family:cursive;\">TRAINING CONFIGURATION</center></p>","metadata":{}},{"cell_type":"code","source":"sen_length = []\n\nfor sentence in tqdm(df[\"excerpt\"]):\n   \n    token_words = CONFIG.tokenizer.encode_plus(sentence)[\"input_ids\"]\n    sen_length.append(len(token_words))\n    \nprint('maxlenth of all sentences are  ', max(sen_length))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CONFIG:\n    seed = 42\n    max_len = 331\n    train_batch = 16\n    valid_batch = 32\n    epochs = 10\n    learning_rate = 2e-5\n    splits = 5\n    scaler = amp.GradScaler()\n    model='bert-base-cased'\n    tokenizer = BertTokenizer.from_pretrained(model, do_lower_case=True)\n    tokenizer.save_pretrained('./tokenizer')\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"color:#159364; font-family:cursive;\">REPRODUCIBILITY</center></p>","metadata":{}},{"cell_type":"code","source":"def set_seed(seed = CONFIG.seed):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \nset_seed(CONFIG.seed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"color:#159364; font-family:cursive;\">DEFINE THE DATASET CLASS</center></p>","metadata":{}},{"cell_type":"code","source":"class BERTDataset(Dataset):\n    def __init__(self,df):\n        self.text = df['excerpt'].values\n        self.target = df['target'].values\n        self.max_len = CONFIG.max_len\n        self.tokenizer = CONFIG.tokenizer\n        \n    def __len__(self):\n        return len(self.text)\n    \n    def __getitem__(self, index):\n        text = self.text[index]\n        text = ' '.join(text.split())\n        inputs = self.tokenizer.encode_plus(\n            text,\n            None,\n            truncation=True,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            return_token_type_ids=True\n        )\n\n        return {\n            'ids': torch.tensor(inputs['input_ids'], dtype=torch.long),\n            'mask': torch.tensor(inputs['attention_mask'], dtype=torch.long),\n            'token_type_ids': torch.tensor(inputs[\"token_type_ids\"], dtype=torch.long),\n            'target': torch.tensor(self.target[index], dtype=torch.float)\n        }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"color:#159364; font-family:cursive;\">MODEL:BERT FOR SEQUENCE CLASSIFICATION from ü§ó </center></p>","metadata":{}},{"cell_type":"code","source":"model = BertForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\",\n    num_labels = 1,\n    output_attentions = False,\n    output_hidden_states = False, \n)\n\nmodel.cuda()","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"color:#159364; font-family:cursive;\">OPTIMIZER</center></p>","metadata":{}},{"cell_type":"code","source":"param_optimizer = list(model.named_parameters())\nno_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\noptimizer_parameters = [\n    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \n     'weight_decay': 0.0001},\n    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \n     'weight_decay': 0.0}\n    ]  \n\noptimizer = AdamW(optimizer_parameters, lr=CONFIG.learning_rate)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"color:#159364; font-family:cursive;\">GET THE PREPARED DATA</center></p>","metadata":{}},{"cell_type":"code","source":"def get_data(fold):\n    df_train = df[df.kfold != fold].reset_index(drop=True)\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n    \n    train_dataset = BERTDataset(df_train)\n    valid_dataset = BERTDataset(df_valid)\n\n    train_loader = DataLoader(train_dataset, batch_size=CONFIG.train_batch, \n                              num_workers=4, shuffle=True, pin_memory=True)\n    valid_loader = DataLoader(valid_dataset, batch_size=CONFIG.valid_batch, \n                              num_workers=4, shuffle=False, pin_memory=True)\n    \n    return train_loader, valid_loader","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"color:#159364; font-family:cursive;\">FOLD:0</center></p>","metadata":{}},{"cell_type":"code","source":"train_dataloader,validation_dataloader=get_data(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"color:#159364; font-family:cursive;\">DEFINE LOSS AND TIME FUNCTIONS</center></p>","metadata":{}},{"cell_type":"code","source":"def loss_fn(output,target):\n     return torch.sqrt(nn.MSELoss()(output,target))\ndef format_time(elapsed):\n    elapsed_rounded = int(round((elapsed)))\n    return str(datetime.timedelta(seconds=elapsed_rounded))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"color:#159364; font-family:cursive;\">DEFINE THE FUNCTION FOR TRAINING,VALIDATION AND RUNNING</center></p>","metadata":{}},{"cell_type":"code","source":"def run(optimizer,scheduler):\n    set_seed(40)\n    scaler=CONFIG.scaler\n    training_stats = []\n    total_t0 = time.time()\n    epochs=CONFIG.epochs\n    for epoch_i in range(0, epochs):\n        print(\"\")\n        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n        print('Training...')\n        t0 = time.time()\n        total_train_loss = 0\n        data_size=0\n        model.train()\n        for step, batch in enumerate(train_dataloader):\n            tr_loss=[]\n            b_input_ids = batch['ids'].to(CONFIG.device)\n            b_token_type_ids=batch['token_type_ids'].to(CONFIG.device)\n            b_input_mask = batch['mask'].to(CONFIG.device)\n            b_labels = batch['target'].to(CONFIG.device)\n            batch_size = b_input_ids.size(0)\n            model.zero_grad() \n            with amp.autocast(enabled=True):\n                output= model(b_input_ids,attention_mask=b_input_mask)          \n                output=output[\"logits\"].squeeze(-1)\n                loss = loss_fn(output,b_labels)\n                tr_loss.append(loss.item()/len(output))\n            scheduler.step()\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n\n        avg_train_loss = np.mean(tr_loss)    \n        training_time = format_time(time.time() - t0)\n        gc.collect()\n        print(\"\")\n        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n        print(\"  Training epoch took: {:}\".format(training_time))\n        print(\"\")\n        print(\"Running Validation...\")\n\n        t0 = time.time()\n        model.eval()\n        val_loss = 0\n        allpreds = []\n        alltargets = []\n        for batch in validation_dataloader:\n            losses = []\n            with torch.no_grad():\n                device=CONFIG.device\n                ids = batch[\"ids\"].to(device)\n                mask = batch[\"mask\"].to(device)\n                tokentype = batch[\"token_type_ids\"].to(device)\n                output = model(ids,mask)\n                output = output[\"logits\"].squeeze(-1)\n                target = batch[\"target\"].to(device)\n                loss = loss_fn(output,target)\n                losses.append(loss.item()/len(output))\n                allpreds.append(output.detach().cpu().numpy())\n                alltargets.append(target.detach().squeeze(-1).cpu().numpy())\n        allpreds = np.concatenate(allpreds)\n        alltargets = np.concatenate(alltargets)\n        losses = np.mean(losses)\n        gc.collect() \n        validation_time = format_time(time.time() - t0)\n        print(\"  Validation Loss: {0:.2f}\".format(losses))\n        print(\"  Validation took: {:}\".format(validation_time))\n        training_stats.append(\n        {\n            'epoch': epoch_i + 1,\n            'Training Loss': avg_train_loss,\n            'Valid. Loss': losses,\n            'Training Time': training_time,\n            'Validation Time': validation_time\n        }\n    ) \n    print(\"\")\n    print(\"Training complete!\")\n    return training_stats  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"color:#159364; font-family:cursive;\">VISUALIZATION FUNCTION </center></p>","metadata":{}},{"cell_type":"code","source":"def Visualizations(training_stats):\n    pd.set_option('precision', 2)\n    df_stats = pd.DataFrame(data=training_stats)\n    df_stats = df_stats.set_index('epoch')\n    layout = go.Layout(template= \"plotly_dark\")\n    fig = go.Figure(layout=layout)\n    fig.add_trace(go.Scatter(x=df_stats.index, y=df_stats['Training Loss'],\n                    mode='lines+markers',\n                    name='Training Loss'))\n    fig.add_trace(go.Scatter(x=df_stats.index, y=df_stats['Valid. Loss'],\n                    mode='lines+markers',\n                    name='Validation Loss'))\n    fig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"color:#159364; font-family:cursive;\">LEARNING RATE SCHEDULER </center></p>","metadata":{}},{"cell_type":"markdown","source":"**LINEAR SCHEDULE WITH WARMUP**","metadata":{}},{"cell_type":"code","source":"# Defining LR Scheduler\nscheduler = get_linear_schedule_with_warmup(\n    optimizer, \n    num_warmup_steps=0, \n    num_training_steps=len(train_dataloader)*CONFIG.epochs\n)\nlrs = []\nfor epoch in range(1, CONFIG.epochs + 1):\n    if scheduler is not None:\n        scheduler.step()\n    lrs.append(optimizer.param_groups[0][\"lr\"])\nlayout = go.Layout(template= \"plotly_dark\",title='Learning_rate')\nfig = go.Figure(layout=layout)\n\nfig.add_trace(go.Scatter(x=list(range(10)), y=lrs,\n                    mode='lines+markers',\n                    name='Learning_rate'))\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**RUN ON THE SET SCHEDULER**","metadata":{}},{"cell_type":"code","source":"df1=run(optimizer,scheduler)\nVisualizations(df1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n# <p><center style=\"color:#159364; font-family:cursive;\">ANALYZE LEARNING RATE VARIATION IN OTHER LEARNING RATE SCHEDULERS</center></p>","metadata":{}},{"cell_type":"markdown","source":"# 1.Reduce LR On Plateau:","metadata":{}},{"cell_type":"markdown","source":"* Reduces learning rate when a metric has stopped improving. Models often benefit from reducing the learning rate by a factor of 2-10 once learning stagnates. This scheduler reads a metrics quantity and if no improvement is seen for a ‚Äòpatience‚Äô number of epochs, the learning rate is reduced.\n* In this scheduler,the scheduler.step() requires the loss argument to identify any stagnation in improvement of metric,so we changed the run function","metadata":{}},{"cell_type":"code","source":"scheduler=torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.01, patience=1, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=False)\ndef run_model(optimizer,scheduler,epochs):\n    set_seed(40)\n    scaler=CONFIG.scaler\n    training_stats = []\n    total_t0 = time.time()\n\n    for epoch_i in range(0, epochs):\n        print(\"\")\n        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n        print('Training...')\n        t0 = time.time()\n        total_train_loss = 0\n        data_size=0\n        model.train()\n        for step, batch in enumerate(train_dataloader):\n            tr_loss=[]\n            b_input_ids = batch['ids'].to(CONFIG.device)\n            b_token_type_ids=batch['token_type_ids'].to(CONFIG.device)\n            b_input_mask = batch['mask'].to(CONFIG.device)\n            b_labels = batch['target'].to(CONFIG.device)\n            batch_size = b_input_ids.size(0)\n            model.zero_grad() \n            with amp.autocast(enabled=True):\n                output= model(b_input_ids,attention_mask=b_input_mask)          \n                output=output[\"logits\"].squeeze(-1)\n                loss = loss_fn(output,b_labels)\n                tr_loss.append(loss.item()/len(output))\n            scheduler.step(loss)\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n        avg_train_loss = np.mean(tr_loss)    \n        training_time = format_time(time.time() - t0)\n\n        gc.collect()\n        print(\"\")\n        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n        print(\"  Training epoch took: {:}\".format(training_time))\n        print(\"\")\n        print(\"Running Validation...\")\n\n        t0 = time.time()\n        model.eval()\n        val_loss = 0\n        allpreds = []\n        alltargets = []\n\n        for batch in validation_dataloader:\n            losses = []\n            with torch.no_grad():\n                device=CONFIG.device\n                ids = batch[\"ids\"].to(device)\n                mask = batch[\"mask\"].to(device)\n                tokentype = batch[\"token_type_ids\"].to(device)\n                output = model(ids,mask)\n                output = output[\"logits\"].squeeze(-1)\n                target = batch[\"target\"].to(device)\n                loss = loss_fn(output,target)\n                losses.append(loss.item()/len(output))\n                allpreds.append(output.detach().cpu().numpy())\n                alltargets.append(target.detach().squeeze(-1).cpu().numpy())\n        allpreds = np.concatenate(allpreds)\n        alltargets = np.concatenate(alltargets)\n        losses = np.mean(losses)\n        gc.collect() \n        validation_time = format_time(time.time() - t0)\n        print(\"  Validation Loss: {0:.2f}\".format(losses))\n        print(\"  Validation took: {:}\".format(validation_time))\n        training_stats.append(\n        {\n            'epoch': epoch_i + 1,\n            'Training Loss': avg_train_loss,\n            'lr':optimizer.param_groups[0]['lr'],\n            'Valid. Loss': losses,\n            'Training Time': training_time,\n            'Validation Time': validation_time\n        }\n    ) \n    print(\"\")\n    print(\"Training complete!\")\n    return training_stats\n    \n\n\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LAMBDA LR","metadata":{}},{"cell_type":"markdown","source":"Sets the learning rate of each parameter group to the initial lr times a given function(here it is 0.65^epoch). When last_epoch=-1, sets initial lr as lr.","metadata":{}},{"cell_type":"code","source":"optimizer = AdamW(optimizer_parameters, lr=CONFIG.learning_rate)\nlambda1 = lambda epoch: 0.65 ** epoch\nscheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1)\nlrs = []\nfor i in range(10):\n    optimizer.step()\n    lrs.append(optimizer.param_groups[0][\"lr\"])\n    scheduler.step()\nlayout = go.Layout(template= \"plotly_dark\",title='Learning_rate')\nfig = go.Figure(layout=layout)\n\nfig.add_trace(go.Scatter(x=list(range(10)), y=lrs,\n                    mode='lines+markers',\n                    name='Learning_rate'))\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MULTIPLICATIVE LR","metadata":{}},{"cell_type":"markdown","source":"Multiplies the learning rate of each parameter group by the factor given in the specified function. When last_epoch=-1, sets initial lr as lr.","metadata":{}},{"cell_type":"code","source":"optimizer = AdamW(optimizer_parameters, lr=CONFIG.learning_rate)\n\nlmbda = lambda epoch: 0.65 ** epoch\nscheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)\nlrs = []\n\nfor i in range(10):\n    optimizer.step()\n    lrs.append(optimizer.param_groups[0][\"lr\"])\n    scheduler.step()\n\nlayout = go.Layout(template= \"plotly_dark\",title='Learning_rate')\nfig = go.Figure(layout=layout)\n\nfig.add_trace(go.Scatter(x=list(range(10)), y=lrs,\n                    mode='lines+markers',\n                    name='Learning_rate'))\nfig.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# STEP LR","metadata":{}},{"cell_type":"markdown","source":"Decays the learning rate of each parameter group by gamma every step_size epochs","metadata":{}},{"cell_type":"code","source":"\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\nlrs = []\n\nfor i in range(10):\n    optimizer.step()\n    lrs.append(optimizer.param_groups[0][\"lr\"])\n    scheduler.step()\n\nlayout = go.Layout(template= \"plotly_dark\",title='Learning_rate')\nfig = go.Figure(layout=layout)\n\nfig.add_trace(go.Scatter(x=list(range(10)), y=lrs,\n                    mode='lines+markers',\n                    name='Learning_rate'))\nfig.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Multi Step LR","metadata":{}},{"cell_type":"markdown","source":"Decays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones.","metadata":{}},{"cell_type":"code","source":"scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[6,8,9], gamma=0.1)\nlrs = []\n\nfor i in range(10):\n    optimizer.step()\n    lrs.append(optimizer.param_groups[0][\"lr\"])\n    scheduler.step()\n\nlayout = go.Layout(template= \"plotly_dark\",title='Learning_rate')\nfig = go.Figure(layout=layout)\n\nfig.add_trace(go.Scatter(x=list(range(10)), y=lrs,\n                    mode='lines+markers',\n                    name='Learning_rate'))\nfig.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ExponentialLR","metadata":{}},{"cell_type":"markdown","source":"Decays the learning rate of each parameter group by gamma every epoch.","metadata":{}},{"cell_type":"code","source":"scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1)\nlrs = []\nfor i in range(10):\n    optimizer.step()\n    lrs.append(optimizer.param_groups[0][\"lr\"])\n    scheduler.step()\n\nlayout = go.Layout(template= \"plotly_dark\",title='Learning_rate')\nfig = go.Figure(layout=layout)\n\nfig.add_trace(go.Scatter(x=list(range(10)), y=lrs,\n                    mode='lines+markers',\n                    name='Learning_rate'))\nfig.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cosine Annealing LR","metadata":{}},{"cell_type":"markdown","source":"Sets the learning rate of each parameter group using a cosine annealing schedule.\nIf the learning rate is set solely by this scheduler, the learning rate at each step becomes:\n\n$$\n\\eta_{t}=\\eta_{\\min }+\\frac{1}{2}\\left(\\eta_{\\max }-\\eta_{\\min }\\right)\\left(1+\\cos \\left(\\frac{T_{c u r}}{T_{\\max }} \\pi\\right)\\right)\n$$\n\nIt has been proposed in SGDR: Stochastic Gradient Descent with Warm Restarts. Note that this only implements the cosine annealing part of SGDR, and not the restarts.https://arxiv.org/abs/1608.03983","metadata":{}},{"cell_type":"code","source":"scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=0)\nlrs = []\nfor i in range(100):\n    optimizer.step()\n    lrs.append(optimizer.param_groups[0][\"lr\"])\n    scheduler.step()\n\nlayout = go.Layout(template= \"plotly_dark\",title='Learning_rate')\nfig = go.Figure(layout=layout)\n\nfig.add_trace(go.Scatter(x=list(range(100)), y=lrs,\n                    mode='lines+markers',\n                    name='Learning_rate'))\nfig.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CYCLIC LR\nSets the learning rate of each parameter group according to cyclical learning rate policy (CLR). The policy cycles the learning rate between two boundaries with a constant frequency, as detailed in the paper [Cyclical Learning Rates for Training Neural Networks](https://arxiv.org/abs/1506.01186). The distance between the two boundaries can be scaled on a per-iteration or per-cycle basis.\n\nCyclical learning rate policy changes the learning rate after every batch. step should be called after a batch has been used for training.\nThis class has three built-in policies, as put forth in the paper:\n\n* **‚Äútriangular‚Äù**: A basic triangular cycle without amplitude scaling.\n* **‚Äútriangular2‚Äù**: A basic triangular cycle that scales initial amplitude by half each cycle.\n* **‚Äúexp_range‚Äù**: A cycle that scales initial amplitude by \\text{gamma}^{\\text{cycle iterations}}gamma cycle iterations at each cycle iteration.","metadata":{}},{"cell_type":"markdown","source":"# CYCLIC LR:TRIANGULAR","metadata":{}},{"cell_type":"code","source":"optimizer = torch.optim.SGD(model.parameters(), lr=100)\nscheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.001, max_lr=0.1,step_size_up=5,mode=\"triangular\")\nlrs = []\n\n\nfor i in range(100):\n    optimizer.step()\n    lrs.append(optimizer.param_groups[0][\"lr\"])\n    scheduler.step()\n\nlayout = go.Layout(template= \"plotly_dark\",title='Learning_rate')\nfig = go.Figure(layout=layout)\n\nfig.add_trace(go.Scatter(x=list(range(100)), y=lrs,\n                    mode='lines+markers',\n                    name='Learning_rate'))\nfig.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CYCLIC LR:TRIANGUALR2","metadata":{}},{"cell_type":"code","source":"optimizer = torch.optim.SGD(model.parameters(), lr=100)\nscheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.001, max_lr=0.1,step_size_up=5,mode=\"triangular2\")\nlrs = []\n\n\nfor i in range(100):\n    optimizer.step()\n    lrs.append(optimizer.param_groups[0][\"lr\"])\n    scheduler.step()\n\nlayout = go.Layout(template= \"plotly_dark\",title='Learning_rate')\nfig = go.Figure(layout=layout)\n\nfig.add_trace(go.Scatter(x=list(range(100)), y=lrs,\n                    mode='lines+markers',\n                    name='Learning_rate'))\nfig.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**RUNNING OUR MODEL WITH THIS SCHEDULER**","metadata":{}},{"cell_type":"code","source":"df9=run(optimizer,scheduler)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Visualizations(df9)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CYCLIC LR:EXPONENTIAL RANGE","metadata":{}},{"cell_type":"code","source":"optimizer = torch.optim.SGD(model.parameters(), lr=100)\nscheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.001, max_lr=0.1,step_size_up=5,mode=\"exp_range\",gamma=0.85)\nlrs = []\n\n\nfor i in range(100):\n    optimizer.step()\n    lrs.append(optimizer.param_groups[0][\"lr\"])\n    scheduler.step()\n\nlayout = go.Layout(template= \"plotly_dark\",title='Learning_rate')\nfig = go.Figure(layout=layout)\n\nfig.add_trace(go.Scatter(x=list(range(100)), y=lrs,\n                    mode='lines+markers',\n                    name='Learning_rate'))\nfig.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ONE CYCLE LR:COSINE ANNEALING (DEFAULT)","metadata":{}},{"cell_type":"markdown","source":"* Sets the learning rate of each parameter group according to the 1 cycle learning rate policy.\n* The 1 cycle policy anneals the learning rate from an initial learning rate to some maximum learning rate and then from that maximum learning rate to some minimum learning rate much lower than the initial learning rate.","metadata":{}},{"cell_type":"code","source":"optimizer = torch.optim.SGD(model.parameters(), lr=100)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=10, epochs=10)\nlrs = []\n\n\nfor i in range(100):\n    optimizer.step()\n    lrs.append(optimizer.param_groups[0][\"lr\"])\n    scheduler.step()\n\nlayout = go.Layout(template= \"plotly_dark\",title='Learning_rate')\nfig = go.Figure(layout=layout)\n\nfig.add_trace(go.Scatter(x=list(range(100)), y=lrs,\n                    mode='lines+markers',\n                    name='Learning_rate'))\nfig.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ONE CYCLE LR:LINEAR ANNEALING","metadata":{}},{"cell_type":"code","source":"optimizer = torch.optim.SGD(model.parameters(), lr=100)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=10, epochs=10,anneal_strategy='linear')\nlrs = []\n\n\nfor i in range(100):\n    optimizer.step()\n    lrs.append(optimizer.param_groups[0][\"lr\"])\n    scheduler.step()\nlayout = go.Layout(template= \"plotly_dark\",title='Learning_rate')\nfig = go.Figure(layout=layout)\n\nfig.add_trace(go.Scatter(x=list(range(100)), y=lrs,\n                    mode='lines+markers',\n                    name='Learning_rate'))\nfig.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# COSINE ANNEALING WARM RESTARTS","metadata":{}},{"cell_type":"markdown","source":"Set the learning rate of each parameter group using a cosine annealing schedule, and restarts after Ti epochs.\n\n\n$$\n\\eta_{t}=\\eta_{\\min }+\\frac{1}{2}\\left(\\eta_{\\max }-\\eta_{\\min }\\right)\\left(1+\\cos \\left(\\frac{T_{\\operatorname{cur}}}{T_{i}} \\pi\\right)\\right)\n$$\n","metadata":{}},{"cell_type":"code","source":"lr_sched = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=1, eta_min=0.001, last_epoch=-1)\n\n\nlrs = []\n\nfor i in range(100):\n    lr_sched.step()\n    lrs.append(\n        optimizer.param_groups[0][\"lr\"]\n    )\n\nlayout = go.Layout(template= \"plotly_dark\",title='Learning_rate')\nfig = go.Figure(layout=layout)\n\nfig.add_trace(go.Scatter(x=list(range(100)), y=lrs,\n                    mode='lines+markers',\n                    name='Learning_rate'))\nfig.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_sched = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=0.01, last_epoch=-1)\n\n\nlrs = []\n\nfor i in range(300):\n    lr_sched.step()\n    lrs.append(\n        optimizer.param_groups[0][\"lr\"]\n    )\n\nlayout = go.Layout(template= \"plotly_dark\",title='Learning_rate')\nfig = go.Figure(layout=layout)\n\nfig.add_trace(go.Scatter(x=list(range(300)), y=lrs,\n                    mode='lines+markers',\n                    name='Learning_rate'))\nfig.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![Upvote!](https://img.shields.io/badge/Upvote-If%20you%20like%20my%20work-07b3c8?style=for-the-badge&logo=kaggle)\n","metadata":{}}]}