{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Data Extraction\nGet the training and testing dataset.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport sklearn\n\nimport sys\nsys.path = [\n    '../input/readability-package',\n] + sys.path\nimport readability    \n\nimport nltk\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \nfrom nltk import pos_tag, pos_tag_sents\n\nfrom sklearn import model_selection\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import model_selection\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import Ridge, BayesianRidge\nimport spacy\nimport pickle\nimport joblib\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"Python Version: \", sys.version)\nprint(\"Spacy Version: \", spacy.__version__)\nprint(\"SkLearn Version: \", sklearn.__version__)\nprint(\"NLTK Version: \", nltk.__version__)\nprint(\"Pandas Version: \", pd.__version__)\nprint(\"Numpy Version: \", np.__version__)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-23T04:47:09.765499Z","iopub.execute_input":"2021-09-23T04:47:09.765952Z","iopub.status.idle":"2021-09-23T04:47:12.279594Z","shell.execute_reply.started":"2021-09-23T04:47:09.76586Z","shell.execute_reply":"2021-09-23T04:47:12.278315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(\"../input/commonlitreadabilityprize/train.csv\")\ntest_df = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-09-23T04:47:12.281444Z","iopub.execute_input":"2021-09-23T04:47:12.281983Z","iopub.status.idle":"2021-09-23T04:47:12.409288Z","shell.execute_reply.started":"2021-09-23T04:47:12.281935Z","shell.execute_reply":"2021-09-23T04:47:12.408216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-23T04:47:12.411696Z","iopub.execute_input":"2021-09-23T04:47:12.412188Z","iopub.status.idle":"2021-09-23T04:47:12.443076Z","shell.execute_reply.started":"2021-09-23T04:47:12.412142Z","shell.execute_reply":"2021-09-23T04:47:12.442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-23T04:47:12.445038Z","iopub.execute_input":"2021-09-23T04:47:12.445349Z","iopub.status.idle":"2021-09-23T04:47:12.459283Z","shell.execute_reply.started":"2021-09-23T04:47:12.44532Z","shell.execute_reply":"2021-09-23T04:47:12.457989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preparation\nClean the data and get them ready for model training.","metadata":{}},{"cell_type":"code","source":"# source: https://www.kaggle.com/ravishah1/readability-feature-engineering-non-nn-baseline/notebook\n\n\"\"\"\nThis function uses the readability library for feature engineering.\nIt includes textual statistics, readability scales and metric, and some pos stats\n\"\"\"\ndef readability_measurements(passage: str):\n    results = readability.getmeasures(passage, lang='en')\n    \n    chars_per_word = results['sentence info']['characters_per_word']\n    syll_per_word = results['sentence info']['syll_per_word']\n    words_per_sent = results['sentence info']['words_per_sentence']\n    \n    kincaid = results['readability grades']['Kincaid']\n    ari = results['readability grades']['ARI']\n    coleman_liau = results['readability grades']['Coleman-Liau']\n    flesch = results['readability grades']['FleschReadingEase']\n    gunning_fog = results['readability grades']['GunningFogIndex']\n    lix = results['readability grades']['LIX']\n    smog = results['readability grades']['SMOGIndex']\n    rix = results['readability grades']['RIX']\n    dale_chall = results['readability grades']['DaleChallIndex']\n    \n    tobeverb = results['word usage']['tobeverb']\n    auxverb = results['word usage']['auxverb']\n    conjunction = results['word usage']['conjunction']\n    pronoun = results['word usage']['pronoun']\n    preposition = results['word usage']['preposition']\n    nominalization = results['word usage']['nominalization']\n    \n    pronoun_b = results['sentence beginnings']['pronoun']\n    interrogative = results['sentence beginnings']['interrogative']\n    article = results['sentence beginnings']['article']\n    subordination = results['sentence beginnings']['subordination']\n    conjunction_b = results['sentence beginnings']['conjunction']\n    preposition_b = results['sentence beginnings']['preposition']\n\n    \n    return [chars_per_word, syll_per_word, words_per_sent,\n            kincaid, ari, coleman_liau, flesch, gunning_fog, lix, smog, rix, dale_chall,\n            tobeverb, auxverb, conjunction, pronoun, preposition, nominalization,\n            pronoun_b, interrogative, article, subordination, conjunction_b, preposition_b]","metadata":{"execution":{"iopub.status.busy":"2021-09-23T04:47:12.461327Z","iopub.execute_input":"2021-09-23T04:47:12.461792Z","iopub.status.idle":"2021-09-23T04:47:12.476089Z","shell.execute_reply.started":"2021-09-23T04:47:12.461744Z","shell.execute_reply":"2021-09-23T04:47:12.474999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# source: https://www.kaggle.com/ravishah1/readability-feature-engineering-non-nn-baseline/notebook\n\n\"\"\"\nThis function generates features using spacy en_core_wb_lg\nUseful resources:\nhttps://www.kaggle.com/konradb/linear-baseline-with-cv\nhttps://www.kaggle.com/anaverageengineer/comlrp-baseline-for-complete-beginners\n\"\"\"\n\ndef spacy_features(df: pd.DataFrame):  \n    nlp = spacy.load('en_core_web_lg')\n    with nlp.disable_pipes():\n        vectors = np.array([nlp(text).vector for text in df.excerpt])\n        \n    return vectors\n\ndef get_spacy_col_names():\n    names = list()\n    for i in range(300):\n        names.append(f\"spacy_{i}\")\n        \n    return names","metadata":{"execution":{"iopub.status.busy":"2021-09-23T04:47:12.47773Z","iopub.execute_input":"2021-09-23T04:47:12.47813Z","iopub.status.idle":"2021-09-23T04:47:12.488303Z","shell.execute_reply.started":"2021-09-23T04:47:12.478096Z","shell.execute_reply":"2021-09-23T04:47:12.487306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# source: https://www.kaggle.com/ravishah1/readability-feature-engineering-non-nn-baseline/notebook\n\ndef pos_tag_features(passage: str):\n    \"\"\"\n    This function counts the number of times different parts of speech occur in an excerpt\n    \"\"\"\n    pos_tags = [\"CC\", \"CD\", \"DT\", \"EX\", \"FW\", \"IN\", \"JJ\", \"JJR\", \"JJS\", \"LS\", \"MD\", \n                \"NN\", \"NNS\", \"NNP\", \"NNPS\", \"PDT\", \"POS\", \"PRP\", \"RB\", \"RBR\", \"RBS\", \"RP\", \"TO\", \"UH\",\n                \"VB\", \"VBD\", \"VBG\", \"VBZ\", \"WDT\", \"WP\", \"WRB\"]\n    \n    tags = pos_tag(word_tokenize(passage))\n    tag_list= list()\n    \n    for tag in pos_tags:\n        tag_list.append(len([i[0] for i in tags if i[1] == tag]))\n    \n    return tag_list","metadata":{"execution":{"iopub.status.busy":"2021-09-23T04:47:12.48951Z","iopub.execute_input":"2021-09-23T04:47:12.489801Z","iopub.status.idle":"2021-09-23T04:47:12.49947Z","shell.execute_reply.started":"2021-09-23T04:47:12.489773Z","shell.execute_reply":"2021-09-23T04:47:12.498554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# source: https://www.kaggle.com/ravishah1/readability-feature-engineering-non-nn-baseline/notebook\n\n\"\"\"\nThis function is where I test miscellaneous features\nThis is experimental\nCurrently checks sentence status\n\"\"\"\ndef generate_other_features(passage: str):\n    # punctuation count\n    periods = passage.count(\".\")\n    commas = passage.count(\",\")\n    semis = passage.count(\";\")\n    exclaims = passage.count(\"!\")\n    questions = passage.count(\"?\")\n    \n    # Some other stats\n    num_char = len(passage)\n    num_words = len(passage.split(\" \"))\n    unique_words = len(set(passage.split(\" \") ))\n    word_diversity = unique_words/num_words\n    \n    word_len = [len(w) for w in passage.split(\" \")]\n    longest_word = np.max(word_len)\n    avg_len_word = np.mean(word_len)\n    \n    return [periods, commas, semis, exclaims, questions,\n            num_char, num_words, unique_words, word_diversity,\n            longest_word, avg_len_word]","metadata":{"execution":{"iopub.status.busy":"2021-09-23T04:47:12.502048Z","iopub.execute_input":"2021-09-23T04:47:12.502379Z","iopub.status.idle":"2021-09-23T04:47:12.516452Z","shell.execute_reply.started":"2021-09-23T04:47:12.50232Z","shell.execute_reply":"2021-09-23T04:47:12.515371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_folds(data, num_splits):\n    # we create a new column called kfold and fill it with -1\n    data[\"kfold\"] = -1\n    \n    # the next step is to randomize the rows of the data\n    data = data.sample(frac=1).reset_index(drop=True)\n\n    # calculate number of bins by Sturge's rule\n    # I take the floor of the value, you can also\n    # just round it\n    num_bins = int(np.floor(1 + np.log2(len(data))))\n    \n    # bin targets\n    data.loc[:, \"bins\"] = pd.cut(\n        data[\"target\"], bins=num_bins, labels=False\n    )\n    \n    # initiate the kfold class from model_selection module\n    kf = model_selection.StratifiedKFold(n_splits=num_splits)\n    \n    # fill the new kfold column\n    # note that, instead of targets, we use bins!\n    for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)):\n        data.loc[v_, 'kfold'] = f\n    \n    # drop the bins column\n    data = data.drop(\"bins\", axis=1)\n\n    # return dataframe with folds\n    return data","metadata":{"execution":{"iopub.status.busy":"2021-09-23T04:47:12.519102Z","iopub.execute_input":"2021-09-23T04:47:12.5198Z","iopub.status.idle":"2021-09-23T04:47:12.528506Z","shell.execute_reply.started":"2021-09-23T04:47:12.519752Z","shell.execute_reply":"2021-09-23T04:47:12.52739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Check\nChecking for statistics for making the body of said book/topics.","metadata":{}},{"cell_type":"code","source":"'''\nSource for feature check section: \n1.) https://www.kaggle.com/ravishah1/readability-feature-engineering-non-nn-baseline/notebook\n'''\n\nclass CLRDataset:\n    \"\"\"\n    This is my CommonLit Readability Dataset.\n    By calling the get_df method on an object of this class,\n    you will have a fully feature engineered dataframe\n    \"\"\"\n    def __init__(self, df: pd.DataFrame, train: bool, n_folds=2):\n        self.df = df\n        self.excerpts = df[\"excerpt\"]\n        \n        self._extract_features()\n        \n        if train:\n            self.df = create_folds(self.df, n_folds)\n        \n    def _extract_features(self):\n        scores_df = pd.DataFrame(self.df[\"excerpt\"].apply(lambda p : readability_measurements(p)).tolist(), \n                                 columns=[\"chars_per_word\", \"syll_per_word\", \"words_per_sent\",\n                                          \"kincaid\", \"ari\", \"coleman_liau\", \"flesch\", \"gunning_fog\", \"lix\", \"smog\", \"rix\", \"dale_chall\",\n                                          \"tobeverb\", \"auxverb\", \"conjunction\", \"pronoun\", \"preposition\", \"nominalization\",\n                                          \"pronoun_b\", \"interrogative\", \"article\", \"subordination\", \"conjunction_b\", \"preposition_b\"])\n        self.df = pd.merge(self.df, scores_df, left_index=True, right_index=True)\n        \n        spacy_df = pd.DataFrame(spacy_features(self.df), columns=get_spacy_col_names())\n        self.df = pd.merge(self.df, spacy_df, left_index=True, right_index=True)\n        \n        pos_df = pd.DataFrame(self.df[\"excerpt\"].apply(lambda p : pos_tag_features(p)).tolist(),\n                              columns=[\"CC\", \"CD\", \"DT\", \"EX\", \"FW\", \"IN\", \"JJ\", \"JJR\", \"JJS\", \"LS\", \"MD\", \n                                       \"NN\", \"NNS\", \"NNP\", \"NNPS\", \"PDT\", \"POS\", \"PRP\", \"RB\", \"RBR\", \"RBS\", \"RP\", \"TO\", \"UH\",\n                                       \"VB\", \"VBD\", \"VBG\", \"VBZ\", \"WDT\", \"WP\", \"WRB\"])\n        self.df = pd.merge(self.df, pos_df, left_index=True, right_index=True)\n        \n        other_df = pd.DataFrame(self.df[\"excerpt\"].apply(lambda p : generate_other_features(p)).tolist(),\n                                columns=[\"periods\", \"commas\", \"semis\", \"exclaims\", \"questions\",\n                                         \"num_char\", \"num_words\", \"unique_words\", \"word_diversity\",\n                                         \"longest_word\", \"avg_len_word\"])\n        self.df = pd.merge(self.df, other_df, left_index=True, right_index=True)\n        \n    def get_df(self):\n        return self.df\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx: int):\n        pass","metadata":{"execution":{"iopub.status.busy":"2021-09-23T04:47:12.530291Z","iopub.execute_input":"2021-09-23T04:47:12.530728Z","iopub.status.idle":"2021-09-23T04:47:12.549482Z","shell.execute_reply.started":"2021-09-23T04:47:12.530684Z","shell.execute_reply":"2021-09-23T04:47:12.548177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Results for training dataset.\ndataset = CLRDataset(train_df, train=True)\ndf = dataset.get_df()\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-23T04:47:12.550718Z","iopub.execute_input":"2021-09-23T04:47:12.551086Z","iopub.status.idle":"2021-09-23T04:49:53.227563Z","shell.execute_reply.started":"2021-09-23T04:47:12.551055Z","shell.execute_reply":"2021-09-23T04:49:53.226776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Results for testing dataset.\ntest_dataset = CLRDataset(test_df, train=False)\ntest_df = test_dataset.get_df()\ntest_df.head(2)","metadata":{"execution":{"iopub.status.busy":"2021-09-23T04:49:53.22885Z","iopub.execute_input":"2021-09-23T04:49:53.229168Z","iopub.status.idle":"2021-09-23T04:50:00.926319Z","shell.execute_reply.started":"2021-09-23T04:49:53.229135Z","shell.execute_reply":"2021-09-23T04:50:00.925574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modelling\nMake the model and train it.","metadata":{}},{"cell_type":"code","source":"def set_seed(seed=42):\n    \"\"\" Sets the Seed \"\"\"\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    \nset_seed(42)","metadata":{"execution":{"iopub.status.busy":"2021-09-23T04:50:00.927459Z","iopub.execute_input":"2021-09-23T04:50:00.927858Z","iopub.status.idle":"2021-09-23T04:50:00.931972Z","shell.execute_reply.started":"2021-09-23T04:50:00.927829Z","shell.execute_reply":"2021-09-23T04:50:00.931165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = [\"chars_per_word\", \"syll_per_word\", \"words_per_sent\",\n            \"kincaid\", \"ari\", \"coleman_liau\", \"flesch\", \"gunning_fog\", \"lix\", \"smog\", \"rix\", \"dale_chall\",\n            \"tobeverb\", \"auxverb\", \"conjunction\", \"pronoun\", \"preposition\", \"nominalization\", \n            \"pronoun_b\", \"interrogative\", \"article\", \"subordination\", \"conjunction_b\", \"preposition_b\"]\nfeatures+=get_spacy_col_names()\nfeatures+=[\"CC\", \"CD\", \"DT\", \"EX\", \"FW\", \"IN\", \"JJ\", \"JJR\", \"JJS\", \"LS\", \"MD\", \n            \"NN\", \"NNS\", \"NNP\", \"NNPS\", \"PDT\", \"POS\", \"PRP\", \"RB\", \"RBR\", \"RBS\", \"RP\", \"TO\", \"UH\",\n            \"VB\", \"VBD\", \"VBG\", \"VBZ\", \"WDT\", \"WP\", \"WRB\"]","metadata":{"execution":{"iopub.status.busy":"2021-09-23T04:50:00.933075Z","iopub.execute_input":"2021-09-23T04:50:00.933501Z","iopub.status.idle":"2021-09-23T04:50:00.945669Z","shell.execute_reply.started":"2021-09-23T04:50:00.933462Z","shell.execute_reply":"2021-09-23T04:50:00.944902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\" I normalize the data here, could be useful depending on your model\"\"\"\nscaler = MinMaxScaler()\ndf[features] = scaler.fit_transform(df[features])\ntest_df[features] = scaler.transform(test_df[features])","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-09-23T04:50:00.946982Z","iopub.execute_input":"2021-09-23T04:50:00.947343Z","iopub.status.idle":"2021-09-23T04:50:01.513446Z","shell.execute_reply.started":"2021-09-23T04:50:00.947311Z","shell.execute_reply":"2021-09-23T04:50:01.512298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training\nDefine the model to train.","metadata":{}},{"cell_type":"code","source":"def train_pred_one_fold(model_name: str, fold: int, df: pd.DataFrame, test_df: pd.DataFrame, features: list, rmse: list):\n    \"\"\"\n    This function trains and predicts on one fold of your selected model\n    df is the train df, test_df is the test_df\n    X features are defined in features\n    y output is target\n    oof score is printed and stored in the rmse list\n    \"\"\"\n    train = df[df.kfold == fold]\n    X_train = train[features]\n    y_train = train[\"target\"]\n \n    valid = df[df.kfold != fold]\n    X_valid = valid[features]\n    y_valid = valid[\"target\"]\n    \n    X_test = test_df[features]\n\n    # Ridge model\n    if model_name == 'ridge' or model_name == 'bayesian_ridge':\n        model.fit(X_train, y_train)\n        oof = model.predict(X_valid)\n        print(np.sqrt(mean_squared_error(y_valid, oof)))\n        rmse.append(np.sqrt(mean_squared_error(y_valid, oof)))\n        test_preds = model.predict(X_test)\n#         with open(f\"model_{fold}.pkl\", \"wb\") as file:\n#             pickle.dump(model, file)\n        if not os.path.isfile(f\"model_{fold}.joblib\"):\n            joblib.dump(model, f\"model_{fold}.joblib\")\n    \n    else:\n        test_preds = 0\n        raise Exception(\"Not Implemented\")\n        \n    return test_preds","metadata":{"execution":{"iopub.status.busy":"2021-09-23T04:50:01.514837Z","iopub.execute_input":"2021-09-23T04:50:01.515151Z","iopub.status.idle":"2021-09-23T04:50:01.524394Z","shell.execute_reply.started":"2021-09-23T04:50:01.51512Z","shell.execute_reply":"2021-09-23T04:50:01.523336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_pred(model_name: str, df: pd.DataFrame, test_df: pd.DataFrame, features: list):\n    \"\"\"\n    This function trains and predicts multiple fold using train_pred_one_fold\n    The average rmse is printed the the test data predictions are returned\n    The last column is the average result from all folds to be submitted\n    \"\"\"\n    global model\n    if model_name == 'ridge':\n        model = Ridge(alpha=3, max_iter=10000)\n        \n    elif model_name == 'bayesian_ridge':\n        model = BayesianRidge(n_iter=10000, tol=0.8) \n        \n    print(f\"model_name: {model_name}\")\n    all_preds = pd.DataFrame()\n    rmse = list()\n    for f in range(2):\n        all_preds[f\"{model_name}_{f}\"] = train_pred_one_fold(model_name, f, df, test_df, features, rmse)\n\n    all_preds[f\"{model_name}\"] = all_preds.mean(axis=1)\n    print(\"---------\")\n    print(f\"avg rmse: {np.mean(rmse)}\")\n    return all_preds","metadata":{"execution":{"iopub.status.busy":"2021-09-23T04:50:01.525514Z","iopub.execute_input":"2021-09-23T04:50:01.525876Z","iopub.status.idle":"2021-09-23T04:50:01.541736Z","shell.execute_reply.started":"2021-09-23T04:50:01.525836Z","shell.execute_reply":"2021-09-23T04:50:01.540566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prep_sub(preds: pd.DataFrame, col_name: str):\n    \"\"\"\n    This function takes an output prediction df from train_pred\n    and sets it to a format that can be submitted to the competition\n    \"\"\"\n    sub = pd.read_csv(\"../input/commonlitreadabilityprize/sample_submission.csv\")\n    sub[\"target\"] = preds[col_name]\n    sub.to_csv(\"submission.csv\", index=False)\n    print(sub)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-09-23T04:50:01.543124Z","iopub.execute_input":"2021-09-23T04:50:01.543457Z","iopub.status.idle":"2021-09-23T04:50:01.55672Z","shell.execute_reply.started":"2021-09-23T04:50:01.543412Z","shell.execute_reply":"2021-09-23T04:50:01.555718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ridge_preds = train_pred('bayesian_ridge', df, test_df, features)\nridge_preds","metadata":{"execution":{"iopub.status.busy":"2021-09-23T04:50:01.558216Z","iopub.execute_input":"2021-09-23T04:50:01.558652Z","iopub.status.idle":"2021-09-23T04:50:02.132382Z","shell.execute_reply.started":"2021-09-23T04:50:01.55861Z","shell.execute_reply":"2021-09-23T04:50:02.131293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prep_sub(ridge_preds, 'bayesian_ridge')\n","metadata":{"execution":{"iopub.status.busy":"2021-09-23T04:50:02.133883Z","iopub.execute_input":"2021-09-23T04:50:02.134485Z","iopub.status.idle":"2021-09-23T04:50:02.158169Z","shell.execute_reply.started":"2021-09-23T04:50:02.134431Z","shell.execute_reply":"2021-09-23T04:50:02.156949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Scraping\nGet at least the top 100 results from a book site called Project Gutenberg.\n\nSince the site did not recommend scraping, I can do the cheeky and only get from this link: https://www.gutenberg.org/browse/scores/top#books-last1\n\nSorry MSPs but I need to enable internet for this one.","metadata":{}},{"cell_type":"code","source":"# For web scraping, charrrrgeeeee !!!!!\n\nimport requests\nfrom bs4 import BeautifulSoup as bs # Totally not BS\nimport re\nimport pandas as pd\nimport numpy as np\nimport itertools","metadata":{"execution":{"iopub.status.busy":"2021-09-23T04:50:02.160001Z","iopub.execute_input":"2021-09-23T04:50:02.160825Z","iopub.status.idle":"2021-09-23T04:50:02.341894Z","shell.execute_reply.started":"2021-09-23T04:50:02.160754Z","shell.execute_reply":"2021-09-23T04:50:02.340851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"the_only_url_worth_scraping = \"https://www.gutenberg.org/browse/scores/top#books-last1\"\nresponse = requests.get(the_only_url_worth_scraping)\nbook_soup_is_delicious = bs(response.text, 'html.parser')","metadata":{"execution":{"iopub.status.busy":"2021-09-23T04:50:02.343266Z","iopub.execute_input":"2021-09-23T04:50:02.343697Z","iopub.status.idle":"2021-09-23T04:50:02.536786Z","shell.execute_reply.started":"2021-09-23T04:50:02.343656Z","shell.execute_reply":"2021-09-23T04:50:02.535712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract book urls\nbooks_list = book_soup_is_delicious.ol.find_all('a', attrs={'class': None})\nbooks_list = [tag.attrs['href'] for tag in books_list \n              if tag.attrs['href'].startswith('/ebooks/')]\nbooks_lists = list(dict.fromkeys(books_list))\n\n# print(\"In total we have \" + str(len(books_list)) + \" books\") # Comment out afterwards\n# print(books_list) # Comment out afterwards","metadata":{"execution":{"iopub.status.busy":"2021-09-23T04:50:02.539497Z","iopub.execute_input":"2021-09-23T04:50:02.539802Z","iopub.status.idle":"2021-09-23T04:50:02.548799Z","shell.execute_reply.started":"2021-09-23T04:50:02.539773Z","shell.execute_reply":"2021-09-23T04:50:02.547548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make the test dataset. This gonna take long af.\nj = 0\ntitle_dict = {}\n\ncsv_data = pd.DataFrame([], columns=['id', 'url_legal', 'license', 'excerpt'])\nfor book in books_list:\n    book_id = book[8:]\n\n    # HTTP link constructor because main site does not like being scraped so I used mirror.\n    url_html_book = \"https://gutenberg.pglaf.org/\"\n    if len(book_id) == 1:\n        url_html_book += \"0/\"\n    else:\n        for i in range(len(book_id) - 1):\n            url_html_book += book_id[i] + \"/\"\n    url_html_book += book_id + \"/\" + book_id + \"-h/\" + book_id + \"-h.htm\"\n    \n    # Get all the paragraphs.\n    book_response = requests.get(url_html_book, headers={\"Accept\":\"text/html;charset=utf-8\"})\n    single_book = bs(book_response.text, 'html.parser')\n    single_book_paragraphs = single_book.find_all('p', attrs={'class': None})\n    single_book_title = single_book.title.get_text().strip()\n    \n    if(single_book_title.lower().startswith(\"the project gutenberg ebook of \")):\n        title_dict.update({book_id: single_book_title[31:]})\n    else:\n        title_dict.update({book_id: single_book_title})\n\n    paragraphs = \"\"\n    \n    # Process each paragraph.\n    for paragraph in single_book_paragraphs:\n        text = paragraph.get_text().strip()\n        \n        short_text = text[:12]\n        short_text_lower = short_text.lower()\n        short_text_upper = short_text.upper()\n        \n        # Remove \"table of contents\" parts.\n        excluded_start = short_text_lower.startswith('chapter') or short_text_lower.startswith(\"drawn by\")\n        \n        # Remove couple of multiple capital letters.\n        excluded_start = excluded_start or short_text_upper.startswith(short_text)\n        \n        # Filter off licensing text.\n        excluded_start = excluded_start or short_text_lower.startswith(\"copyright\")\n        excluded_start = excluded_start or short_text_lower.startswith(\"gnu free\")\n        excluded_start = excluded_start or short_text_lower.startswith(\"note:\")\n        excluded_start = excluded_start or text.find(\"Project Gutenberg eBook\") >= 0\n        excluded_start = excluded_start or short_text_lower.startswith(\"produced by:\")\n        \n        # Ignore empty paragraphs and short ones. Also do not include chapter listings.\n        if len(text) > 50 and not excluded_start:\n\n            # Clean the paragraph.\n            text = text.replace('  ', '').replace('\\n', ' ').replace('\\r', '').strip() + \" \\n\"\n            paragraphs += text\n            \n            if(len(paragraphs.split(' ')) >= 175):\n                paragraphs = ' '.join(paragraphs.split(' ')[:175])\n                break\n    \n    if(len(paragraphs.split(' ')) >= 75):\n        csv_form = pd.DataFrame([[book_id, url_html_book, \"Public domain in the USA.\" , \n                                  paragraphs]], columns=['id', 'url_legal', 'license', 'excerpt'], index=[j])\n        csv_data = csv_data.append(csv_form)\n    j = j + 1\n    print(\"Progress: {} of {}\".format(j, len(books_list))) \n\ncsv_data.to_csv(\"guthenberg.csv\", index=True) # Raw data to be used for processing.","metadata":{"execution":{"iopub.status.busy":"2021-09-23T04:50:02.550554Z","iopub.execute_input":"2021-09-23T04:50:02.550885Z","iopub.status.idle":"2021-09-23T04:51:22.094376Z","shell.execute_reply.started":"2021-09-23T04:50:02.550853Z","shell.execute_reply":"2021-09-23T04:51:22.093125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Results for guthenberg dataset.\n# But for some reason the pre-processing disliked a few of the excerpts so can result in less data processed.\ntest_dataset = CLRDataset(csv_data, train=False) \ntest_df = test_dataset.get_df()\ntest_df.fillna(0) # Ensure all NaNs or empty values are 0 since model only processes numbers.\n# test_df.to_csv(\"sample_gutenberg_dataset.csv\") \ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-23T04:51:22.095964Z","iopub.execute_input":"2021-09-23T04:51:22.096283Z","iopub.status.idle":"2021-09-23T04:51:34.827938Z","shell.execute_reply.started":"2021-09-23T04:51:22.096254Z","shell.execute_reply":"2021-09-23T04:51:34.826914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prepare to use model actually.\ntest_df[features] = scaler.transform(test_df[features])\nridge_preds = train_pred('bayesian_ridge', df, test_df, features)\n\n# Temp save bayesian ridge results.\n# ridge_preds.to_csv(\"model_results.csv\", columns = [\"bayesian_ridge\"], index=False)\nridge_preds","metadata":{"execution":{"iopub.status.busy":"2021-09-23T04:51:34.829324Z","iopub.execute_input":"2021-09-23T04:51:34.829628Z","iopub.status.idle":"2021-09-23T04:51:35.364176Z","shell.execute_reply.started":"2021-09-23T04:51:34.829598Z","shell.execute_reply":"2021-09-23T04:51:35.363086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make a list that matches submission format.\nguthenb_list = []\nridge_list = ridge_preds[\"bayesian_ridge\"].to_list()\ntest_ids = test_df[\"id\"].to_list()\nfor i in range(len(ridge_list)):\n    guthenb_list.append([test_ids[i], ridge_list[i], title_dict[test_ids[i]]])\n# print(guthenb_list)\nguthenb = pd.DataFrame(guthenb_list, columns=[\"id\", \"target\", \"title\"])\nguthenb.to_csv(\"guthenberg-results.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-23T04:51:35.365744Z","iopub.execute_input":"2021-09-23T04:51:35.366409Z","iopub.status.idle":"2021-09-23T04:51:35.381568Z","shell.execute_reply.started":"2021-09-23T04:51:35.366328Z","shell.execute_reply":"2021-09-23T04:51:35.38006Z"},"trusted":true},"execution_count":null,"outputs":[]}]}