{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Pytorch inference ensemble notebook\n\nthanks to https://www.kaggle.com/andretugan/pre-trained-roberta-solution-in-pytorch\n\nThis comptition summary.\n\n\n### preprocess\n\n[make NLP](https://www.kaggle.com/kunihikofurugori/make-nlpdataset)\n\n→ not effective (score is not improved.)\n\n[exclude anomaly data](https://www.kaggle.com/kunihikofurugori/step1-exclude-anomaly)\n\n→ score slightly is improved....?\n\n### transformer training\n\n[transformer tpu8fold](https://www.kaggle.com/kunihikofurugori/tpu8fold-transformer)\n\n→ The score is improved because I tried to many seed pattern.\n\n(Please see Section 4.2 in [Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping](https://arxiv.org/pdf/2002.06305.pdf))\n\n\n[Add attention and early stopping](https://www.kaggle.com/kunihikofurugori/tpu8fold-transformer-attention)\n→ attention and early stopping is powerful method. these method is effective.\n\n(Please see Section 5 in [Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping](https://arxiv.org/pdf/2002.06305.pdf))\n\n[Add multi dropout](https://www.kaggle.com/kunihikofurugori/step2-1-tpu8foldtraining-rbase)\n\n→ score slightly is improved....?\n\n### inference\n\nxgb,lgb inference model is slightly imporved. \n\nsvm inference model is not imporved.\n\nLB score in my model ensembleing is 0.461 and andretugan model is model 0.467 and then 2 pair ensemble is then LB 0.457.","metadata":{}},{"cell_type":"code","source":"import torch\nimport pandas as pd\nimport numpy as np\n#torch\nimport torch\nimport torch.nn as nn\nfrom torch.nn import Parameter\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import _LRScheduler\nfrom torch.optim import Adam, lr_scheduler\n\nimport xgboost as xgb\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_squared_error\n\nimport transformers\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer,AutoModel\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntorch.backends.cudnn.benchmark = True\n\nimport warnings\nwarnings.simplefilter('ignore')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class testDataset:\n    def __init__(self, excerpt, tokenizer, max_len):\n        self.excerpt = excerpt\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.excerpt)\n\n    def __getitem__(self, item):\n        text = str(self.excerpt[item])\n        inputs = self.tokenizer(\n            text, \n            max_length=self.max_len, \n            padding=\"max_length\", \n            truncation=True\n        )\n\n        ids = inputs[\"input_ids\"]\n        mask = inputs[\"attention_mask\"]\n\n        return {\n            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n            \"attention_mask\": torch.tensor(mask, dtype=torch.long),\n        }","metadata":{"execution":{"iopub.status.busy":"2021-07-20T11:51:36.27344Z","iopub.execute_input":"2021-07-20T11:51:36.273933Z","iopub.status.idle":"2021-07-20T11:51:36.282301Z","shell.execute_reply.started":"2021-07-20T11:51:36.273894Z","shell.execute_reply":"2021-07-20T11:51:36.281158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AttentionHead(nn.Module):\n    def __init__(self, in_features, hidden_dim, num_targets):\n        super().__init__()\n        self.in_features = in_features\n        self.middle_features = hidden_dim\n\n        self.W = nn.Linear(in_features, hidden_dim)\n        self.V = nn.Linear(hidden_dim, 1)\n        self.out_features = hidden_dim\n        self._init_params()    \n    def _init_params(self):\n        nn.init.xavier_normal_(self.W.weight)\n        nn.init.constant_(self.W.bias, 0)\n        nn.init.xavier_normal_(self.V.weight)\n        nn.init.constant_(self.V.bias, 0)\n\n    def forward(self, features):\n        att = torch.tanh(self.W(features))\n\n        score = self.V(att)\n\n        attention_weights = torch.softmax(score, dim=1)\n\n        context_vector = attention_weights * features\n        context_vector = torch.sum(context_vector, dim=1)\n\n        return context_vector\n\nclass dModel(nn.Module):\n    def __init__(self,modelpath, leng):\n        super(dModel, self).__init__()\n        self.roberta = transformers.AutoModel.from_pretrained(modelpath)#,num_labels=1)\n        self.head = AttentionHead(leng,leng,1)\n        self.fc = nn.Linear(leng, 1)\n        self.dropout = nn.Dropout(p=0.1)\n        self.dropouts = nn.ModuleList([\n                nn.Dropout(p=0.1) for _ in range(5)\n            ])\n        self._init_params()    \n    def _init_params(self):\n        nn.init.xavier_normal_(self.fc.weight)\n        nn.init.constant_(self.fc.bias, 0)\n    def forward(self, **x):\n        x = self.roberta(**x)[0]\n        x = self.head(x)\n        for i, dropout in enumerate(self.dropouts):\n            if i == 0:\n                logits = self.fc(dropout(x))\n            else:\n                logits += self.fc(dropout(x))\n        \n        logits /= len(self.dropouts)\n        return logits","metadata":{"execution":{"iopub.status.busy":"2021-07-20T11:51:36.289637Z","iopub.execute_input":"2021-07-20T11:51:36.290357Z","iopub.status.idle":"2021-07-20T11:51:36.305948Z","shell.execute_reply.started":"2021-07-20T11:51:36.290302Z","shell.execute_reply":"2021-07-20T11:51:36.305114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_embeddings(model_path,weight_path, max_len,leng):\n    model = dModel(model_path,leng)\n    model.load_state_dict(torch.load(weight_path,map_location=device))\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    model = model.to(device)\n    model.eval()\n    df = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")\n    dataset = testDataset(excerpt=df.excerpt.values, tokenizer=tokenizer, max_len=max_len)\n    data_loader = torch.utils.data.DataLoader(\n        dataset, batch_size=16, num_workers=2, pin_memory=True, shuffle=False, drop_last=False\n    )\n    final_output = np.empty((0,leng))\n\n    for b_idx, (data) in enumerate(data_loader):\n        with torch.no_grad():\n            for key, value in data.items():\n                data[key] = value.to(device)\n            x = model.roberta(**data)[0]\n            output = model.head(x)\n            final_output = np.append(final_output,output.clone().detach().cpu().numpy(),axis=0)\n    \n    torch.cuda.empty_cache()\n    return final_output\n\ndef generate_predictions_md(model_path,weight_path, max_len,leng):\n    #model = AutoModelForSequenceClassification.from_pretrained(model_path,num_labels=1)\n    model = dModel(model_path,leng)\n    model.load_state_dict(torch.load(weight_path,map_location=device))\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n\n    model = model.to(device)\n    model.eval()\n    df = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")\n    dataset = testDataset(excerpt=df.excerpt.values, tokenizer=tokenizer, max_len=max_len)\n    data_loader = torch.utils.data.DataLoader(\n        dataset, batch_size=8, num_workers=2, pin_memory=True, shuffle=False, drop_last=False\n    )\n    final_output = []\n\n    for b_idx, data in enumerate(data_loader):\n        with torch.no_grad():\n            for key, value in data.items():\n                data[key] = value.to(device)\n            output = model(**data)\n            #output = output.logits.squeeze(-1).detach().cpu().numpy()\n            output = output.squeeze(-1).detach().cpu().numpy()\n            final_output = np.append(final_output,output)\n    \n    torch.cuda.empty_cache()\n    return final_output","metadata":{"execution":{"iopub.status.busy":"2021-07-20T11:51:36.307368Z","iopub.execute_input":"2021-07-20T11:51:36.307875Z","iopub.status.idle":"2021-07-20T11:51:36.325726Z","shell.execute_reply.started":"2021-07-20T11:51:36.307836Z","shell.execute_reply":"2021-07-20T11:51:36.324729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#0:LB 0.483 to 0.487 to 0.486\n\n#1:LB 0.487 to 0.489 to 0.488\n\n#2:LB 0.489 to 0.489　to 0.489\n\n#3:LB 0.498 to 0.492 to 0.485\n\n#4:LB 0.498 to 0.492 to 0.491\n\n#5:LB 0.497 to 0.504 \n\n#6:LB 0.500 to 0.509 \n\nmodel_paths = [\"../input/roberta-transformers-pytorch/roberta-large/\",\n                   \"../input/roberta-transformers-pytorch/roberta-large/\",\n                   #\"../input/roberta-transformers-pytorch/roberta-large/\",\n                   #\"../input/roberta-transformers-pytorch/distilroberta-base/\",\n                   #\"../input/roberta-transformers-pytorch/roberta-base/\",\n                   #\"../input/roberta-transformers-pytorch/roberta-large/\",\n                   #\"../input/roberta-transformers-pytorch/roberta-base/\",\n                   #\"../input/roberta-transformers-pytorch/roberta-base/\"\n              ]\nweight_paths = [\"../input/commonlitreadability-weight/simplelarge_0.469_fold0_mlen250.pt\",\n                    \"../input/commonlitreadability-weight/simplelarge_0.455_fold1_mlen250.pt\",\n                    #\"../input/commonlitreadability-weight/simplelarge_0.464_fold3_mlen250.pt\",\n                    #\"../input/commonlitreadability-weight/bestmodel_0.469_fold1_mlen250.pt\",\n                    #\"../input/commonlitreadability-weight/bestbase_0.443_fold4_mlen250.pt\",\n                    #\"../input/commonlitreadability-weight/multipledropoutrobertalarge_LB0.500_mlen250.pt\",\n                    #\"../input/commonlitreadability-weight/basebestmodel_0.428_fold4_mlen250.pt\",\n                    #\"../input/commonlitreadability-weight/multipledropoutrobertabase_LB0.504_mlen250.pt\"\n               ]\nlengs = [1024,1024]#,1024]#,768,768,1024,768]#,768]\n\n\nmodel_paths_emb = [#\"../input/roberta-transformers-pytorch/roberta-large/\",\n                   #\"../input/roberta-transformers-pytorch/roberta-large/\",\n                   \"../input/roberta-transformers-pytorch/roberta-large/\",\n                   \"../input/roberta-transformers-pytorch/distilroberta-base/\",\n                   \"../input/roberta-transformers-pytorch/roberta-base/\",\n                   #\"../input/roberta-transformers-pytorch/roberta-large/\",]\n                   #\"../input/roberta-transformers-pytorch/roberta-base/\",]\n                   #\"../input/roberta-transformers-pytorch/roberta-base/\"]\n                  ]\nweight_paths_emb = [#\"../input/commonlitreadability-weight/simplelarge_0.469_fold0_mlen250.pt\",\n                    #\"../input/commonlitreadability-weight/simplelarge_0.455_fold1_mlen250.pt\",\n                    \"../input/commonlitreadability-weight/simplelarge_0.464_fold3_mlen250.pt\",\n                    \"../input/commonlitreadability-weight/bestmodel_0.469_fold1_mlen250.pt\",\n                    \"../input/commonlitreadability-weight/bestbase_0.443_fold4_mlen250.pt\",\n                    #\"../input/commonlitreadability-weight/multipledropoutrobertalarge_LB0.500_mlen250.pt\",\n                    #\"../input/commonlitreadability-weight/basebestmodel_0.428_fold4_mlen250.pt\",\n                    #\"../input/commonlitreadability-weight/multipledropoutrobertabase_LB0.504_mlen250.pt\"\n                   ]\nlengs = [1024,768,768]#[1024,1024,1024,768,768,1024,768]#,768]","metadata":{"execution":{"iopub.status.busy":"2021-07-20T11:51:36.327338Z","iopub.execute_input":"2021-07-20T11:51:36.327744Z","iopub.status.idle":"2021-07-20T11:51:36.340972Z","shell.execute_reply.started":"2021-07-20T11:51:36.327705Z","shell.execute_reply":"2021-07-20T11:51:36.340085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#single LB 0.483\npreds0 = generate_predictions_md(model_path=\"../input/roberta-transformers-pytorch/roberta-large/\",\n                              weight_path=\"../input/commonlitreadability-weight/simplelarge_0.469_fold0_mlen250.pt\",\n                              max_len=250,leng=1024)\n\n#single LB 0.487\npreds1 = generate_predictions_md(model_path=\"../input/roberta-transformers-pytorch/roberta-large/\",\n                              weight_path=\"../input/commonlitreadability-weight/simplelarge_0.455_fold1_mlen250.pt\",\n                              max_len=250,leng=1024)\n\n#single LB 0.489\npreds2 = generate_predictions_md(model_path=\"../input/roberta-transformers-pytorch/roberta-large/\",\n                              weight_path=\"../input/commonlitreadability-weight/simplelarge_0.464_fold3_mlen250.pt\",\n                              max_len=250,leng=1024)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T11:51:36.342565Z","iopub.execute_input":"2021-07-20T11:51:36.343278Z","iopub.status.idle":"2021-07-20T11:52:51.512088Z","shell.execute_reply.started":"2021-07-20T11:51:36.343236Z","shell.execute_reply":"2021-07-20T11:52:51.510924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_embeddings = []\n\nfor mpath,wpath, leng in zip(model_paths_emb,weight_paths_emb, lengs):\n    emb = generate_embeddings(mpath,wpath, max_len=250,leng=leng)\n    test_embeddings.append(emb)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T11:52:51.513938Z","iopub.execute_input":"2021-07-20T11:52:51.514337Z","iopub.status.idle":"2021-07-20T11:53:16.09087Z","shell.execute_reply.started":"2021-07-20T11:52:51.514291Z","shell.execute_reply":"2021-07-20T11:53:16.089802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_embeddings[0].shape","metadata":{"execution":{"iopub.status.busy":"2021-07-20T11:53:16.094472Z","iopub.execute_input":"2021-07-20T11:53:16.094787Z","iopub.status.idle":"2021-07-20T11:53:16.103993Z","shell.execute_reply.started":"2021-07-20T11:53:16.094757Z","shell.execute_reply":"2021-07-20T11:53:16.102883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import lightgbm as lgb\n\nparams = {\n    'objective': 'reg:squarederror',\n    'eval_metric': 'rmse',\n    \n    'eta': 0.05,\n    'max_depth': 2,\n    \n    'gamma': 1,\n    'subsample': 0.9,\n    \n    'nthread': 2,\n    'tree_method' : 'gpu_hist'\n}\n\nparams_l = {\n    'boosting_type': 'gbdt',\n    'metric': 'rmse',\n    'objective': 'regression',\n    'seed': 2048,\n    'learning_rate': 0.05,\n    \"n_jobs\": -1,\n    'max_depth': 4,\n    \"verbose\": -1\n}\n\nnfolds = 5\nkf = KFold(n_splits=nfolds, shuffle=True, random_state=2048)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T11:53:16.106294Z","iopub.execute_input":"2021-07-20T11:53:16.10671Z","iopub.status.idle":"2021-07-20T11:53:18.373103Z","shell.execute_reply.started":"2021-07-20T11:53:16.106672Z","shell.execute_reply":"2021-07-20T11:53:18.372197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_pred(train_embeddings,test_emb, targets, train_df):\n    best_iterations = []\n    oof_rmses = []\n    preds = np.zeros(test.shape[0])\n    pred_list = []\n\n    for k, (train_idx, valid_idx) in enumerate(kf.split(train_df)):    \n\n        dtrain = xgb.DMatrix(train_embeddings[train_idx], targets[train_idx])\n        dvalid = xgb.DMatrix(train_embeddings[valid_idx], targets[valid_idx])\n        evals_result = dict()\n        booster = xgb.train(params,\n                            dtrain,\n                            evals=[(dtrain, 'train'), (dvalid, 'valid')],\n                            num_boost_round=300,\n                            early_stopping_rounds=20,\n                            evals_result=evals_result,\n                            verbose_eval=False)\n\n        best_iteration = np.argmin(evals_result['valid']['rmse'])\n        best_iterations.append(best_iteration)\n        oof_rmse = evals_result['valid']['rmse'][best_iteration]\n        oof_rmses.append(oof_rmse)\n        \n        preds += booster.predict(xgb.DMatrix(test_emb), ntree_limit=int(best_iteration+1)) / nfolds\n        pred_list.append(booster.predict(xgb.DMatrix(test_emb), ntree_limit=int(best_iteration+1)))\n    pred_list = np.array(pred_list)[np.argpartition(oof_rmses, 3)[:2]]\n    preds = pred_list.mean(axis=0)\n    evals_df = pd.DataFrame()\n    evals_df['fold'] = range(1, nfolds+1)\n    evals_df['best_iteration'] = best_iterations\n    evals_df['oof_rmse'] = oof_rmses\n\n    display(evals_df)\n    print('mean oof rmse = {}'.format(np.mean(oof_rmses)))\n    \n    torch.cuda.empty_cache()\n    return preds\n\ndef get_pred_lgb(train_embeddings,test_emb, targets, train_df):\n\n    pred = np.zeros(test.shape[0])\n    pred_list = []\n    rmses = []\n\n    for k, (train_idx, valid_idx) in enumerate(kf.split(train_df)):    \n\n        dtrain = lgb.Dataset(train_embeddings[train_idx], targets[train_idx])\n        dvalid = lgb.Dataset(train_embeddings[valid_idx], targets[valid_idx], reference=dtrain)\n\n        model = lgb.train(\n            params_l,\n            dtrain, \n            num_boost_round=300,\n            early_stopping_rounds=20,\n            valid_sets=[dtrain, dvalid], \n            verbose_eval=-1\n        )\n\n        y_pred = model.predict(train_embeddings[valid_idx])\n        rmse = rmse_score(targets[valid_idx], y_pred)\n        rmses.append(rmse)\n\n        tmp_pred = model.predict(test_emb)\n        pred += tmp_pred / 5\n    \n    print(\"\\n\", \"Mean Fold RMSE:\", np.mean(rmses))\n    return pred","metadata":{"execution":{"iopub.status.busy":"2021-07-20T11:53:18.374286Z","iopub.execute_input":"2021-07-20T11:53:18.37465Z","iopub.status.idle":"2021-07-20T11:53:18.390119Z","shell.execute_reply.started":"2021-07-20T11:53:18.374614Z","shell.execute_reply":"2021-07-20T11:53:18.38918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SVM prediction","metadata":{}},{"cell_type":"code","source":"def rmse_score(y_true,y_pred):\n    return np.sqrt(mean_squared_error(y_true,y_pred))\n\ndef get_pred_svm(X,y,X_test,bins,nfolds=5,C=10,kernel='rbf'):\n    scores = list()\n    preds = np.zeros((X_test.shape[0]))\n    \n    kfold = StratifiedKFold(n_splits=5,shuffle=True,random_state=2048)\n    for k, (train_idx,valid_idx) in enumerate(kfold.split(X,bins)):\n        model = SVR(C=C,kernel=kernel,gamma='auto')\n        X_train,y_train = X[train_idx], y[train_idx]\n        X_valid,y_valid = X[valid_idx], y[valid_idx]\n        \n        model.fit(X_train,y_train)\n        prediction = model.predict(X_valid)\n        score = rmse_score(prediction,y_valid)\n        print(f'Fold {k} , rmse score: {score}')\n        scores.append(score)\n        preds += model.predict(X_test)\n        \n    print(\"mean rmse\",np.mean(scores))\n    return np.array(preds)/nfolds","metadata":{"execution":{"iopub.status.busy":"2021-07-20T11:53:18.391715Z","iopub.execute_input":"2021-07-20T11:53:18.392328Z","iopub.status.idle":"2021-07-20T11:53:18.405462Z","shell.execute_reply.started":"2021-07-20T11:53:18.392249Z","shell.execute_reply":"2021-07-20T11:53:18.404551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")\ntarget = np.load(\"../input/embedding-featurevector/target.npy\")\ntrain_df = pd.read_csv(\"../input/step1-exclude-anomaly/train.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-07-20T11:53:18.406731Z","iopub.execute_input":"2021-07-20T11:53:18.407093Z","iopub.status.idle":"2021-07-20T11:53:18.504238Z","shell.execute_reply.started":"2021-07-20T11:53:18.407055Z","shell.execute_reply":"2021-07-20T11:53:18.50344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_bins = int(np.floor(1 + np.log2(len(train_df))))\ntrain_df.loc[:,'bins'] = pd.cut(train_df['target'],bins=num_bins,labels=False)\nbins = train_df.bins.to_numpy()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T11:53:18.505479Z","iopub.execute_input":"2021-07-20T11:53:18.505849Z","iopub.status.idle":"2021-07-20T11:53:18.516966Z","shell.execute_reply.started":"2021-07-20T11:53:18.505811Z","shell.execute_reply":"2021-07-20T11:53:18.515963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_comb_svm = np.empty((0,len(test)))\npred_comb = np.empty((0,len(test)))\npred_comb_lgb = np.empty((0,len(test)))\nfor i in range(len(test_embeddings)):\n    embeddings = np.load(\"../input/embedding-featurevector/embeddings\" + str(i+2) + \".npy\")\n    print(embeddings.shape)\n    preds = get_pred(embeddings,test_embeddings[i], target, train_df)\n    preds_lgb = get_pred_lgb(embeddings,test_embeddings[i], target, train_df)\n    pred_comb = np.append(pred_comb,preds[None,:],axis=0)\n    pred_comb_lgb = np.append(pred_comb_lgb,preds_lgb[None,:],axis=0)\n    #preds_svm = get_pred_svm(embeddings, target, test_embeddings[i], bins = bins)\n    #pred_comb_svm = np.append(pred_comb_svm,preds_svm[None,:],axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T11:53:18.518748Z","iopub.execute_input":"2021-07-20T11:53:18.519307Z","iopub.status.idle":"2021-07-20T11:54:24.797476Z","shell.execute_reply.started":"2021-07-20T11:53:18.519262Z","shell.execute_reply":"2021-07-20T11:54:24.796509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_ori = (preds0 + preds1 + preds2)/3\n#pred_ori = (preds0 + preds1)/2\npred_svm = pred_comb_svm.sum(axis=0)/len(pred_comb_svm)\npred_xgb = pred_comb.sum(axis=0)/len(pred_comb)\npred_lgb = pred_comb_lgb.sum(axis=0)/len(pred_comb_lgb)\npredf = (pred_ori*3 + pred_xgb*3)/6 # +pred_comb_svm[1])/6","metadata":{"execution":{"iopub.status.busy":"2021-07-20T11:54:24.798941Z","iopub.execute_input":"2021-07-20T11:54:24.799518Z","iopub.status.idle":"2021-07-20T11:54:24.80612Z","shell.execute_reply.started":"2021-07-20T11:54:24.799481Z","shell.execute_reply":"2021-07-20T11:54:24.80522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# model2","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 32\nMAX_LEN = 248\nEVAL_SCHEDULE = [(0.50, 16), (0.49, 8), (0.48, 4), (0.47, 2), (-1., 1)]\nROBERTA_PATH = \"../input/roberta-transformers-pytorch/roberta-base\"\nTOKENIZER_PATH = \"../input/roberta-transformers-pytorch/roberta-base\"\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\ntest_df = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/test.csv\")\nsubmission_df = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/sample_submission.csv\")\ntokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T11:54:24.807544Z","iopub.execute_input":"2021-07-20T11:54:24.808346Z","iopub.status.idle":"2021-07-20T11:54:24.988455Z","shell.execute_reply.started":"2021-07-20T11:54:24.808305Z","shell.execute_reply":"2021-07-20T11:54:24.987593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LitModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n        config.update({\"output_hidden_states\":True, \n                       \"hidden_dropout_prob\": 0.0,\n                       \"layer_norm_eps\": 1e-7})                       \n        \n        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config)  \n            \n        self.attention = nn.Sequential(            \n            nn.Linear(768, 512),            \n            nn.Tanh(),                       \n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )        \n\n        self.regressor = nn.Sequential(                        \n            nn.Linear(768, 1)                        \n        )\n        \n\n    def forward(self, input_ids, attention_mask):\n        roberta_output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)        \n\n        # There are a total of 13 layers of hidden states.\n        # 1 for the embedding layer, and 12 for the 12 Roberta layers.\n        # We take the hidden states from the last Roberta layer.\n        last_layer_hidden_states = roberta_output.hidden_states[-1]\n\n        # The number of cells is MAX_LEN.\n        # The size of the hidden state of each cell is 768 (for roberta-base).\n        # In order to condense hidden states of all cells to a context vector,\n        # we compute a weighted average of the hidden states of all cells.\n        # We compute the weight of each cell, using the attention neural network.\n        weights = self.attention(last_layer_hidden_states)\n                \n        # weights.shape is BATCH_SIZE x MAX_LEN x 1\n        # last_layer_hidden_states.shape is BATCH_SIZE x MAX_LEN x 768        \n        # Now we compute context_vector as the weighted average.\n        # context_vector.shape is BATCH_SIZE x 768\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)        \n        \n        # Now we reduce the context vector to the prediction score.\n        return self.regressor(context_vector)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T11:54:24.989822Z","iopub.execute_input":"2021-07-20T11:54:24.990159Z","iopub.status.idle":"2021-07-20T11:54:25.000364Z","shell.execute_reply.started":"2021-07-20T11:54:24.990125Z","shell.execute_reply":"2021-07-20T11:54:24.999402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LitDataset(Dataset):\n    def __init__(self, df, inference_only=False):\n        super().__init__()\n\n        self.df = df        \n        self.inference_only = inference_only\n        self.text = df.excerpt.tolist()\n        #self.text = [text.replace(\"\\n\", \" \") for text in self.text]\n        \n        if not self.inference_only:\n            self.target = torch.tensor(df.target.values, dtype=torch.float32)        \n    \n        self.encoded = tokenizer.batch_encode_plus(\n            self.text,\n            padding = 'max_length',            \n            max_length = MAX_LEN,\n            truncation = True,\n            return_attention_mask=True\n        )        \n \n\n    def __len__(self):\n        return len(self.df)\n\n    \n    def __getitem__(self, index):        \n        input_ids = torch.tensor(self.encoded['input_ids'][index])\n        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n        \n        if self.inference_only:\n            return (input_ids, attention_mask)            \n        else:\n            target = self.target[index]\n            return (input_ids, attention_mask, target)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T11:54:25.001891Z","iopub.execute_input":"2021-07-20T11:54:25.002353Z","iopub.status.idle":"2021-07-20T11:54:25.014055Z","shell.execute_reply.started":"2021-07-20T11:54:25.002311Z","shell.execute_reply":"2021-07-20T11:54:25.013168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(model, data_loader):\n    \"\"\"Returns an np.array with predictions of the |model| on |data_loader|\"\"\"\n    model.eval()\n\n    result = np.zeros(len(data_loader.dataset))    \n    index = 0\n    \n    with torch.no_grad():\n        for batch_num, (input_ids, attention_mask) in enumerate(data_loader):\n            input_ids = input_ids.to(DEVICE)\n            attention_mask = attention_mask.to(DEVICE)\n                        \n            pred = model(input_ids, attention_mask)                        \n\n            result[index : index + pred.shape[0]] = pred.flatten().to(\"cpu\")\n            index += pred.shape[0]\n\n    return result","metadata":{"execution":{"iopub.status.busy":"2021-07-20T11:54:25.015428Z","iopub.execute_input":"2021-07-20T11:54:25.015813Z","iopub.status.idle":"2021-07-20T11:54:25.026322Z","shell.execute_reply.started":"2021-07-20T11:54:25.015779Z","shell.execute_reply":"2021-07-20T11:54:25.025425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\nfrom transformers import AutoModel\nfrom transformers import AutoConfig\nimport gc\ngc.enable()\n\ntest_dataset = LitDataset(test_df, inference_only=True)\n\nNUM_MODELS = 5\n\nall_predictions = np.zeros((NUM_MODELS, len(test_df)))\n\n\n\ntest_dataset = LitDataset(test_df, inference_only=True)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n                         drop_last=False, shuffle=False, num_workers=2)\n\nfor model_index in range(NUM_MODELS):            \n    model_path = f\"../input/commonlit-roberta-0467/model_{model_index + 1}.pth\"\n    print(f\"\\nUsing {model_path}\")\n                        \n    model = LitModel()\n    model.load_state_dict(torch.load(model_path, map_location=DEVICE))    \n    model.to(DEVICE)\n        \n    all_predictions[model_index] = predict(model, test_loader)\n            \n    del model\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T11:54:25.027779Z","iopub.execute_input":"2021-07-20T11:54:25.02815Z","iopub.status.idle":"2021-07-20T11:54:58.682503Z","shell.execute_reply.started":"2021-07-20T11:54:25.028113Z","shell.execute_reply":"2021-07-20T11:54:58.681489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_predictions = (all_predictions[0] + all_predictions[2] + all_predictions[3])/3\nmodel_predictions5 = all_predictions.mean(axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T12:02:54.81779Z","iopub.execute_input":"2021-07-20T12:02:54.818181Z","iopub.status.idle":"2021-07-20T12:02:54.824902Z","shell.execute_reply.started":"2021-07-20T12:02:54.818149Z","shell.execute_reply":"2021-07-20T12:02:54.822162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_2sigma = (pred_ori*3 + model_predictions5*5)/8","metadata":{"execution":{"iopub.status.busy":"2021-07-20T12:03:24.018573Z","iopub.execute_input":"2021-07-20T12:03:24.01895Z","iopub.status.idle":"2021-07-20T12:03:24.023306Z","shell.execute_reply.started":"2021-07-20T12:03:24.01892Z","shell.execute_reply":"2021-07-20T12:03:24.022289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#predfinal = (predf + model_predictions)/2","metadata":{"execution":{"iopub.status.busy":"2021-07-20T11:54:58.690439Z","iopub.execute_input":"2021-07-20T11:54:58.691048Z","iopub.status.idle":"2021-07-20T11:54:58.701396Z","shell.execute_reply.started":"2021-07-20T11:54:58.69101Z","shell.execute_reply":"2021-07-20T11:54:58.700457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predfinal_x = (pred_ori*3 + pred_xgb*2 + model_predictions*5)/10\npredfinal_l = (pred_ori*3 + pred_lgb*2 + model_predictions*5)/10","metadata":{"execution":{"iopub.status.busy":"2021-07-20T11:54:58.702642Z","iopub.execute_input":"2021-07-20T11:54:58.703021Z","iopub.status.idle":"2021-07-20T11:54:58.712082Z","shell.execute_reply.started":"2021-07-20T11:54:58.702984Z","shell.execute_reply":"2021-07-20T11:54:58.711111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ex = pd.read_csv(\"../input/commonlitreadabilityprize/train.csv\")\ntrain_ex = train_ex[train_ex.target!=0]\nupper = train_ex.target.mean()+1.0*train_ex.target.std()\nlower = train_ex.target.mean()-1.0*train_ex.target.std()\n\n#plt.scatter(train_ex.target.values,train_ex.standard_error.values)\n#print(lower,upper)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T11:54:58.713259Z","iopub.execute_input":"2021-07-20T11:54:58.715528Z","iopub.status.idle":"2021-07-20T11:54:58.791106Z","shell.execute_reply.started":"2021-07-20T11:54:58.715496Z","shell.execute_reply":"2021-07-20T11:54:58.790203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2021-07-20T12:02:34.225262Z","iopub.execute_input":"2021-07-20T12:02:34.225587Z","iopub.status.idle":"2021-07-20T12:02:34.231474Z","shell.execute_reply.started":"2021-07-20T12:02:34.225556Z","shell.execute_reply":"2021-07-20T12:02:34.230698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test[\"best_pred\"] = (pred_ori*3 + model_predictions*3)/6\ntest['pred2sigma'] =pred_2sigma\ntest['predfull'] = predfinal_l\ntest = test.sort_values(\"best_pred\")\nsigma2 = int(len(test)*0.05)\nheadindex = test.head(sigma2).index\ntailindex = test.tail(sigma2).index\nbestpredindex = test[(test.best_pred > lower) & (test.best_pred < upper)].index\nfullindex = test[~((test.best_pred > lower) & (test.best_pred < upper))].index\nmeansindex = test[(test.best_pred > train_ex.target.max()) | (test.best_pred < train_ex.target.min())].index\ntest[\"prediction\"] = 0\ntest.loc[fullindex,\"prediction\"] = test.loc[fullindex,\"predfull\"]\ntest.loc[bestpredindex,\"prediction\"] = test.loc[bestpredindex,\"best_pred\"]\ntest.loc[meansindex,\"prediction\"] = train_ex.target.mean()\ntest.loc[headindex,\"prediction\"] = test.loc[headindex,'pred2sigma']\ntest.loc[tailindex,\"prediction\"] = test.loc[tailindex,'pred2sigma']\ntest","metadata":{"execution":{"iopub.status.busy":"2021-07-20T12:05:00.798946Z","iopub.execute_input":"2021-07-20T12:05:00.79931Z","iopub.status.idle":"2021-07-20T12:05:00.834979Z","shell.execute_reply.started":"2021-07-20T12:05:00.799277Z","shell.execute_reply":"2021-07-20T12:05:00.833591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame()\nsubmission['id'] = test['id'].copy()\nsubmission['target'] = test['prediction'].copy()\nsubmission.to_csv('submission.csv', index=False)\nsubmission","metadata":{"execution":{"iopub.status.busy":"2021-07-20T12:05:49.467412Z","iopub.execute_input":"2021-07-20T12:05:49.46775Z","iopub.status.idle":"2021-07-20T12:05:49.485111Z","shell.execute_reply.started":"2021-07-20T12:05:49.467719Z","shell.execute_reply":"2021-07-20T12:05:49.484276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}