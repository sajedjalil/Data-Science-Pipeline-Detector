{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import re\nimport os\nimport gc\nimport numpy as np\nimport pandas as pd\nimport random\nimport math\nimport tensorflow as tf\nimport logging\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nfrom tensorflow.keras import backend as K\nimport transformers\nfrom transformers import RobertaTokenizer, TFRobertaModel\nfrom kaggle_datasets import KaggleDatasets\ntf.get_logger().setLevel(logging.ERROR)\nfrom kaggle_datasets import KaggleDatasets\nfrom cloud_tpu_client import Client\nfrom sklearn import datasets\nfrom sklearn import model_selection\n\nfrom numba import cuda\n\n\n\nfrom IPython.display import SVG\n\n\nfrom tensorflow.keras.callbacks import Callback\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n\n\nimport tensorflow_addons as tfa\n\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n\nimport matplotlib.pyplot as plt\n\nfrom transformers import AutoTokenizer,AutoModelForSeq2SeqLM,AutoModel","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-23T20:01:30.69373Z","iopub.execute_input":"2021-06-23T20:01:30.694098Z","iopub.status.idle":"2021-06-23T20:01:39.315443Z","shell.execute_reply.started":"2021-06-23T20:01:30.694022Z","shell.execute_reply":"2021-06-23T20:01:39.314652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![talkIsCheap](https://quotefancy.com/media/wallpaper/3840x2160/1700728-Linus-Torvalds-Quote-Talk-is-cheap-Show-me-the-code.jpg)","metadata":{}},{"cell_type":"code","source":"#Client().configure_tpu_version(tf.__version__, restart_type='ifNeeded')","metadata":{"execution":{"iopub.status.busy":"2021-06-23T20:01:39.316917Z","iopub.execute_input":"2021-06-23T20:01:39.317232Z","iopub.status.idle":"2021-06-23T20:01:39.321533Z","shell.execute_reply.started":"2021-06-23T20:01:39.317197Z","shell.execute_reply":"2021-06-23T20:01:39.320546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.__version__","metadata":{"execution":{"iopub.status.busy":"2021-06-23T20:01:39.323553Z","iopub.execute_input":"2021-06-23T20:01:39.323926Z","iopub.status.idle":"2021-06-23T20:01:39.334976Z","shell.execute_reply.started":"2021-06-23T20:01:39.323889Z","shell.execute_reply":"2021-06-23T20:01:39.334097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformers.__version__","metadata":{"execution":{"iopub.status.busy":"2021-06-23T20:01:39.336588Z","iopub.execute_input":"2021-06-23T20:01:39.337045Z","iopub.status.idle":"2021-06-23T20:01:39.344426Z","shell.execute_reply.started":"2021-06-23T20:01:39.337009Z","shell.execute_reply":"2021-06-23T20:01:39.343383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"for training on tpu, use tpu_on = 1. i tried hard on tpu but couldn't get better result after tpu training,compared to gpu training","metadata":{}},{"cell_type":"code","source":"# Configurations\nEPOCHS = 100\n\ntpu_on = 0\ndebug = 1\n# Seed\nSEED = 12345\n\n# Learning rate\nLR = 1e-4\nLR1 = 1e-3\n\n# Verbosity\nVERBOSE = 2\n\n\n# For tf.dataset\nAUTO = tf.data.experimental.AUTOTUNE\n\n# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-06-23T20:01:39.345791Z","iopub.execute_input":"2021-06-23T20:01:39.346297Z","iopub.status.idle":"2021-06-23T20:01:39.360044Z","shell.execute_reply.started":"2021-06-23T20:01:39.34626Z","shell.execute_reply":"2021-06-23T20:01:39.358138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if(not debug):\n    print(\"saving tokenizer,model for offline inference...\\n\")\n    save_path = '/kaggle/working/'\n    tokenizer.save_pretrained(save_path)\n    def build_roberta_base_model():\n        transformer = TFRobertaModel.from_pretrained(MODEL)\n\n        return transformer\n\n\n    modelweight = build_roberta_base_model()\n    modelweight.save_weights('tf_model.h5')\n\n    configuration = modelweight.config\n\n    configuration.save_pretrained(save_path)\n\n    del modelweight\n    if(tpu_on):\n        print(\"\\ntpu on, clearing memory for next fold........\\n\\n\\n\")\n        tf.tpu.experimental.initialize_tpu_system(tpu) \n    gc.collect()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-23T20:01:39.36152Z","iopub.execute_input":"2021-06-23T20:01:39.361986Z","iopub.status.idle":"2021-06-23T20:01:39.368647Z","shell.execute_reply.started":"2021-06-23T20:01:39.361952Z","shell.execute_reply":"2021-06-23T20:01:39.367577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tr = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\ntr.excerpt[118]","metadata":{"execution":{"iopub.status.busy":"2021-06-23T20:01:39.370264Z","iopub.execute_input":"2021-06-23T20:01:39.37085Z","iopub.status.idle":"2021-06-23T20:01:39.444489Z","shell.execute_reply.started":"2021-06-23T20:01:39.370815Z","shell.execute_reply":"2021-06-23T20:01:39.443563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max = 0\nfor i in range(len(tr)):\n    if(len(tr.excerpt[i]) > max):\n        max = len(tr.excerpt[i])\n    \nmax","metadata":{"execution":{"iopub.status.busy":"2021-06-23T20:01:39.44702Z","iopub.execute_input":"2021-06-23T20:01:39.447376Z","iopub.status.idle":"2021-06-23T20:01:39.485747Z","shell.execute_reply.started":"2021-06-23T20:01:39.447349Z","shell.execute_reply":"2021-06-23T20:01:39.484578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://www.kaggle.com/abhishek/step-1-create-folds/notebook\n\ndef create_folds(data, num_splits):\n    # we create a new column called kfold and fill it with -1\n    data[\"kfold\"] = -1\n    \n    # the next step is to randomize the rows of the data\n    data = data.sample(frac=1).reset_index(drop=True)\n\n    # calculate number of bins by Sturge's rule\n    # I take the floor of the value, you can also\n    # just round it\n    num_bins = int(np.floor(1 + np.log2(len(data))))\n    \n    # bin targets\n    data.loc[:, \"bins\"] = pd.cut(\n        data[\"target\"], bins=num_bins, labels=False\n    )\n    \n    # initiate the kfold class from model_selection module\n    kf = model_selection.StratifiedKFold(n_splits=num_splits)\n    \n    # fill the new kfold column\n    # note that, instead of targets, we use bins!\n    for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)):\n        data.loc[v_, 'kfold'] = f\n    \n    # drop the bins column\n    data = data.drop(\"bins\", axis=1)\n\n    # return dataframe with folds\n    return data","metadata":{"execution":{"iopub.status.busy":"2021-06-23T20:01:39.48745Z","iopub.execute_input":"2021-06-23T20:01:39.488044Z","iopub.status.idle":"2021-06-23T20:01:39.495192Z","shell.execute_reply.started":"2021-06-23T20:01:39.488006Z","shell.execute_reply":"2021-06-23T20:01:39.493875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#LR = 0.000040\ndef build_lrfn(lr_start=0.000410, lr_max=0.00001, \n               lr_min=0.00001, lr_rampup_epochs=7, \n               lr_sustain_epochs=0, lr_exp_decay=.87):\n    lr_max = lr_max * strategy.num_replicas_in_sync\n\n    def lrfn(epoch):\n        if epoch < lr_rampup_epochs:\n            lr = (lr_max - lr_start) / lr_rampup_epochs * epoch + lr_start\n        elif epoch < lr_rampup_epochs + lr_sustain_epochs:\n            lr = lr_max\n        else:\n            lr = (lr_max - lr_min) * lr_exp_decay**(epoch - lr_rampup_epochs - lr_sustain_epochs) + lr_min\n        return lr\n    \n    return lrfn\n\nplt.figure(figsize=(10, 7))\n\nlrfn = build_lrfn()\nplt.plot([i for i in range(35)], [lrfn(i) for i in range(35)]);","metadata":{"execution":{"iopub.status.busy":"2021-06-23T20:01:39.496674Z","iopub.execute_input":"2021-06-23T20:01:39.497202Z","iopub.status.idle":"2021-06-23T20:01:39.667126Z","shell.execute_reply.started":"2021-06-23T20:01:39.497166Z","shell.execute_reply":"2021-06-23T20:01:39.666333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import *\nfrom tensorflow.keras import backend as K\nimport numpy as np\n\nclass CyclicLR(Callback):\n    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n    The method cycles the learning rate between two boundaries with\n    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n    The amplitude of the cycle can be scaled on a per-iteration or \n    per-cycle basis.\n    This class has three built-in policies, as put forth in the paper.\n    \"triangular\":\n        A basic triangular cycle w/ no amplitude scaling.\n    \"triangular2\":\n        A basic triangular cycle that scales initial amplitude by half each cycle.\n    \"exp_range\":\n        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n        cycle iteration.\n    For more detail, please see paper.\n    \n    # Example\n        ```python\n            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n                                step_size=2000., mode='triangular')\n            model.fit(X_train, Y_train, callbacks=[clr])\n        ```\n    \n    Class also supports custom scaling functions:\n        ```python\n            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n                                step_size=2000., scale_fn=clr_fn,\n                                scale_mode='cycle')\n            model.fit(X_train, Y_train, callbacks=[clr])\n        ```    \n    # Arguments\n        base_lr: initial learning rate which is the\n            lower boundary in the cycle.\n        max_lr: upper boundary in the cycle. Functionally,\n            it defines the cycle amplitude (max_lr - base_lr).\n            The lr at any cycle is the sum of base_lr\n            and some scaling of the amplitude; therefore \n            max_lr may not actually be reached depending on\n            scaling function.\n        step_size: number of training iterations per\n            half cycle. Authors suggest setting step_size\n            2-8 x training iterations in epoch.\n        mode: one of {triangular, triangular2, exp_range}.\n            Default 'triangular'.\n            Values correspond to policies detailed above.\n            If scale_fn is not None, this argument is ignored.\n        gamma: constant in 'exp_range' scaling function:\n            gamma**(cycle iterations)\n        scale_fn: Custom scaling policy defined by a single\n            argument lambda function, where \n            0 <= scale_fn(x) <= 1 for all x >= 0.\n            mode paramater is ignored \n        scale_mode: {'cycle', 'iterations'}.\n            Defines whether scale_fn is evaluated on \n            cycle number or cycle iterations (training\n            iterations since start of cycle). Default is 'cycle'.\n    \"\"\"\n\n    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n                 gamma=1., scale_fn=None, scale_mode='cycle'):\n        super(CyclicLR, self).__init__()\n\n        self.base_lr = base_lr\n        self.max_lr = max_lr\n        self.step_size = step_size\n        self.mode = mode\n        self.gamma = gamma\n        if scale_fn == None:\n            if self.mode == 'triangular':\n                self.scale_fn = lambda x: 1.\n                self.scale_mode = 'cycle'\n            elif self.mode == 'triangular2':\n                self.scale_fn = lambda x: 1/(2.**(x-1))\n                self.scale_mode = 'cycle'\n            elif self.mode == 'exp_range':\n                self.scale_fn = lambda x: gamma**(x)\n                self.scale_mode = 'iterations'\n        else:\n            self.scale_fn = scale_fn\n            self.scale_mode = scale_mode\n        self.clr_iterations = 0.\n        self.trn_iterations = 0.\n        self.history = {}\n\n        self._reset()\n\n    def _reset(self, new_base_lr=None, new_max_lr=None,\n               new_step_size=None):\n        \"\"\"Resets cycle iterations.\n        Optional boundary/step size adjustment.\n        \"\"\"\n        if new_base_lr != None:\n            self.base_lr = new_base_lr\n        if new_max_lr != None:\n            self.max_lr = new_max_lr\n        if new_step_size != None:\n            self.step_size = new_step_size\n        self.clr_iterations = 0.\n        \n    def clr(self):\n        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n        if self.scale_mode == 'cycle':\n            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n        else:\n            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n        \n    def on_train_begin(self, logs={}):\n        logs = logs or {}\n\n        if self.clr_iterations == 0:\n            K.set_value(self.model.optimizer.lr, self.base_lr)\n        else:\n            K.set_value(self.model.optimizer.lr, self.clr())        \n            \n    def on_batch_end(self, epoch, logs=None):\n        \n        logs = logs or {}\n        self.trn_iterations += 1\n        self.clr_iterations += 1\n\n        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n        self.history.setdefault('iterations', []).append(self.trn_iterations)\n\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n        \n        K.set_value(self.model.optimizer.lr, self.clr())","metadata":{"execution":{"iopub.status.busy":"2021-06-23T20:01:39.668398Z","iopub.execute_input":"2021-06-23T20:01:39.668722Z","iopub.status.idle":"2021-06-23T20:01:39.686114Z","shell.execute_reply.started":"2021-06-23T20:01:39.668694Z","shell.execute_reply":"2021-06-23T20:01:39.685327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import TFRobertaForSequenceClassification\n# Function to seed everything\ndef seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.random.set_seed(seed)\n\n# This function tokenize the text according to a transformers model tokenizer\ndef regular_encode(texts, tokenizer = None, maxlen = 512):\n    enc_di = tokenizer.batch_encode_plus(\n        texts,\n        padding = 'max_length',\n        truncation = True,\n        max_length = maxlen,\n    )\n    \n    return np.array(enc_di['input_ids'])\n\n# This function encode our training sentences\ndef encode_texts(x_train,tokenizer, x_val, MAX_LEN):\n    x_train = regular_encode(x_train.tolist(), tokenizer = tokenizer, maxlen = MAX_LEN)\n    x_val = regular_encode(x_val.tolist(), tokenizer = tokenizer, maxlen = MAX_LEN)\n    return x_train, x_val\n\n# Function to transform arrays to tensors\ndef transform_to_tensors(x_train, x_val, y_train, y_val,BATCH_SIZE):\n    \n    train_dataset = (\n        tf.data.Dataset\n        .from_tensor_slices((x_train, y_train))\n        .repeat()\n        .shuffle(2048)\n        .batch(BATCH_SIZE)\n        .prefetch(AUTO)\n    )\n    \n    valid_dataset = (\n        tf.data.Dataset\n        .from_tensor_slices((x_val, y_val))\n        .batch(BATCH_SIZE)\n        .prefetch(AUTO)\n    )\n    \n    return train_dataset, valid_dataset\n\n# Function to build our model\ndef build_roberta_model(max_len = 512,MODEL = None):\n    #transformer = TFRobertaModel.from_pretrained('../input/training-tf-roberta-large-on-mlm/mlm_tf-roberta-large/')\n    transformer = TFRobertaModel.from_pretrained(MODEL)\n    #transformer = AutoModel.from_pretrained(MODEL)\n\n    input_word_ids = tf.keras.layers.Input(shape = (max_len, ), dtype = tf.int32, name = 'input_word_ids')\n    sequence_output = transformer(input_word_ids)[0]\n  \n    # We only need the cls_token, resulting in a 2d array\n    cls_token = sequence_output[:, 0, :]\n    #cls_token = tf.keras.layers.Dropout(0.3)(cls_token)\n\n    output = tf.keras.layers.Dense(1, activation = 'linear', dtype = 'float32')(cls_token)\n    model = tf.keras.models.Model(inputs = [input_word_ids], outputs = [output])\n    ''' \n    cycle = tfa.optimizers.CyclicalLearningRate(\n            initial_learning_rate = LR,\n            maximal_learning_rate =  1e-3,\n            step_size =  200,\n            scale_fn = lambda x: 1.,\n            scale_mode = 'cycle',\n            name = 'CyclicalLearningRate'\n    )\n    '''\n    optimizer = tf.keras.optimizers.Adam(lr = LR)\n    opt = tfa.optimizers.SWA(optimizer, start_averaging=8, average_period=3)\n    model.compile(optimizer = opt ,  #tfa.optimizers.RectifiedAdam(lr = LR)\n                  loss = [tf.keras.losses.MeanSquaredError()],\n                  metrics = [tf.keras.metrics.RootMeanSquaredError()])\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2021-06-23T20:01:39.687516Z","iopub.execute_input":"2021-06-23T20:01:39.688112Z","iopub.status.idle":"2021-06-23T20:01:39.702992Z","shell.execute_reply.started":"2021-06-23T20:01:39.688058Z","shell.execute_reply":"2021-06-23T20:01:39.702135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.listdir('../')","metadata":{"execution":{"iopub.status.busy":"2021-06-23T20:01:39.704335Z","iopub.execute_input":"2021-06-23T20:01:39.704819Z","iopub.status.idle":"2021-06-23T20:01:39.718361Z","shell.execute_reply.started":"2021-06-23T20:01:39.704748Z","shell.execute_reply":"2021-06-23T20:01:39.717501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to build our model\nfrom transformers import RobertaConfig\ndef build_model(max_len = 512,MODEL = None):\n    print(\"extract embedding... \")\n    ''' \n    comment\n    '''\n    config = RobertaConfig(dropout=0.4, attention_dropout=0.4)\n    config.output_hidden_states = False\n    #transformer_model = TFRobertaModel.from_pretrained(MODEL, config = config)\n    \n    transformer_model = TFRobertaModel.from_pretrained('../input/distilroberta-base-pretrained-on-commonlit/mlm_tfdistilroberta-base/', config = config)\n\n    #transformer_model = TFRobertaModel.from_pretrained(MODEL)\n\n    #input_ids_in = tf.keras.layers.Input(shape=(128,), name='input_token', dtype='int32')\n    #input_masks_in = tf.keras.layers.Input(shape=(128,), name='masked_token', dtype='int32') \n    #embedding_layer = transformer_model(input_ids_in, attention_mask=input_masks_in)[0]\n\n    input_word_ids = tf.keras.layers.Input(shape = (max_len, ), dtype = tf.int32, name = 'input_word_ids')\n    embedding_layer = transformer_model(input_word_ids)[0]\n    \n    \n    cls_token = embedding_layer[:,0,:]\n    X =  tfa.layers.GroupNormalization()(cls_token)\n    \n \n    X = tf.keras.layers.Dense(4096, activation='relu')(X)\n   \n   \n    X = tf.keras.layers.Dense(1, activation='linear')(X)\n  \n\n    model = tf.keras.models.Model(inputs = [input_word_ids], outputs = [X])\n    ''' \n    for layer in model.layers[:2]:\n        layer.trainable = False\n    '''\n    optimizer = tf.keras.optimizers.Adam(lr = LR)\n    optimizer = tfa.optimizers.SWA(optimizer, start_averaging=30, average_period=3)\n    model.compile(optimizer = optimizer ,  #tfa.optimizers.RectifiedAdam(lr = LR)\n                  loss = [tf.keras.losses.MeanSquaredError()],\n                  metrics = [tf.keras.metrics.RootMeanSquaredError()])\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2021-06-23T20:14:02.664046Z","iopub.execute_input":"2021-06-23T20:14:02.66436Z","iopub.status.idle":"2021-06-23T20:14:02.673378Z","shell.execute_reply.started":"2021-06-23T20:14:02.66433Z","shell.execute_reply":"2021-06-23T20:14:02.672012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Function to train and evaluate our model\ndef train_and_evaluate(trainStrategy = \"stratifiedkfold\",modelType = \"xlm\"):\n    \n    # Read our training data\n    #df = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\n    \n    # read training data\n    df = pd.read_csv(\"../input/commonlitreadabilityprize/train.csv\")\n    # Create out of folds array to store predictions\n    oof_predictions = np.zeros(len(df))\n    \n    print(trainStrategy,modelType)\n    \n    if(modelType == \"distilroberta\"):\n        print(\"distilroberta base model training........\")\n        # Number of folds for training\n        FOLDS = 5\n        # Max length\n        MAX_LEN = 300\n        # Get the trained model we want to use\n        MODEL = 'distilroberta-base' #jplu/tf-xlm-roberta-base\n\n        # Let's load our model tokenizer\n        tokenizer = RobertaTokenizer.from_pretrained('../input/distilroberta-base-pretrained-on-commonlit/mlm_tfdistilroberta-base/')\n        #tokenizer = AutoTokenizer.from_pretrained(MODEL)\n      \n        BATCH_SIZE = 32 * strategy.num_replicas_in_sync \n        \n        # Learning rate\n        LR = 1e-4\n        LR1 = 1e-3\n\n\n    else:\n        print(\"xlmr base model training........\")\n        # Number of folds for training\n        FOLDS = 5\n        # Max length\n        MAX_LEN = 192\n        # Get the trained model we want to use\n        MODEL = 'jplu/tf-xlm-roberta-base' #jplu/tf-xlm-roberta-large google/mt5-large\n        # Let's load our model tokenizer\n        #tokenizer = RobertaTokenizer.from_pretrained(MODEL)\n        tokenizer = AutoTokenizer.from_pretrained(MODEL)\n        BATCH_SIZE = 16 * strategy.num_replicas_in_sync \n        \n       \n\n    print(MAX_LEN)\n        \n\n    #df = clean_data(df, input_columns ) \n    if (trainStrategy == \"stratifiedkfold\"):\n        \n        # create folds\n        df = create_folds(df, num_splits=FOLDS)\n\n\n        # Seed everything\n        seed_everything(SEED)\n\n        # Initiate kfold object with shuffle and a specific seed\n        #kfold = KFold(n_splits = FOLDS, shuffle = True, random_state = SEED)\n        \n        for fold in range(FOLDS):\n         \n            trn_ind = np.where((df['kfold'] != fold))[0]\n            val_ind = np.where((df['kfold'] == fold))[0]\n\n            print(len(trn_ind),'=total training indexes \\n \\n',len(val_ind),'=total validation indexes \\n')\n            print(\"........................\\n\")\n            #print(val_ind)\n            print(f'Training fold {fold + 1}')\n            K.clear_session()\n            # Get text features and target\n            x_train, x_val = df['excerpt'].iloc[trn_ind], df['excerpt'].iloc[val_ind]\n            y_train, y_val = df['target'].iloc[trn_ind].values, df['target'].iloc[val_ind].values\n            # Encode our text with Roberta tokenizer\n            x_train, x_val = encode_texts(x_train,tokenizer, x_val, MAX_LEN)\n            # Function to transform our numpy array to a tf Dataset\n            train_dataset, valid_dataset = transform_to_tensors(x_train, x_val, y_train, y_val,BATCH_SIZE)\n            # Build model\n            with strategy.scope():\n                #model = build_roberta_model(max_len = MAX_LEN,MODEL = MODEL)\n                model =  build_model(max_len = MAX_LEN,MODEL = MODEL)\n            # Model checkpoint\n            ''' \n            count = 0\n            for layer in model.layers:\n                count +=1\n                if count < 5: #freezing first 4 layers\n                    print(\"freezing layers...\")\n                    layer.trainable = False\n            \n            for w in model.get_layer(MODEL).weights:\n                print(\"freezing .......\")\n                w._trainable = False\n            '''\n            \n            '''\n            for layer in model.layer[:2]:\n                layer.trainable = False\n            '''\n            checkpoint = tf.keras.callbacks.ModelCheckpoint(f'Roberta_{modelType}_{trainStrategy}_{SEED}_{fold + 1}.h5', \n                                                            monitor = 'val_root_mean_squared_error', \n                                                            verbose = VERBOSE, \n                                                            save_best_only = True,\n                                                            save_weights_only = True, \n                                                            mode = 'min')\n      \n            es = EarlyStopping(monitor='val_root_mean_squared_error', mode='min', patience=20, \n                       restore_best_weights=True, verbose=1)\n            lr_callback = LearningRateScheduler(lrfn, verbose=1)\n\n            #LR = 0.000040\n\n            reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss',  \n                                        factor=0.15, patience=15, \n                                        verbose=1, mode='min', \n                                        epsilon=0.0001, cooldown=1, min_lr=0.00001)\n\n\n            steps = x_train.shape[0] // BATCH_SIZE \n            print(\"total steps\",steps)\n            # Set CLR options\n            clr_step_size = steps\n            base_lr = 1e-4\n            max_lr = 1e-3\n            mode='triangular'\n            clr = CyclicLR(base_lr=base_lr, max_lr=max_lr, step_size=clr_step_size, mode=mode)\n\n\n            # Training phase\n            callback_list = [checkpoint,clr] #lr_callback reduceLROnPlat es\n            #model.load_weights(f'Roberta_{modelType}_{trainStrategy}_{SEED}_{fold + 1}.h5')\n            history = model.fit(train_dataset,\n                                batch_size = BATCH_SIZE,\n                                epochs = EPOCHS,\n                                verbose = VERBOSE,\n                                callbacks = [callback_list],\n                                validation_data = valid_dataset,\n                                steps_per_epoch = steps)\n            \n            # Load best epoch weights\n            model.load_weights(f'Roberta_{modelType}_{trainStrategy}_{SEED}_{fold + 1}.h5')\n            # Predict validation set to save them in the out of folds array\n            val_pred = model.predict(valid_dataset)\n            oof_predictions[val_ind] = val_pred.reshape(-1)\n            print(\"\\n\\n\\n\\n\")\n            print('-'*50)\n            del train_dataset, valid_dataset\n            del model\n            if(tpu_on):\n                print(\"\\ntpu on, clearing memory for next fold........\\n\\n\\n\")\n                tf.tpu.experimental.initialize_tpu_system(tpu) \n            gc.collect()\n         \n\n    else:\n        print(trainStrategy,'\\n')\n        # Initiate kfold object with shuffle and a specific seed\n        kfold = KFold(n_splits = FOLDS, shuffle = True, random_state = SEED)\n\n\n        for fold, (trn_ind, val_ind) in enumerate(kfold.split(df)):\n\n            print('-'*50)\n            print(f'Training fold {fold + 1}')\n            K.clear_session()\n            # Get text features and target\n            x_train, x_val = df['excerpt'].iloc[trn_ind], df['excerpt'].iloc[val_ind]\n            y_train, y_val = df['target'].iloc[trn_ind].values, df['target'].iloc[val_ind].values\n            # Encode our text with Roberta tokenizer\n            x_train, x_val = encode_texts(x_train,tokenizer, x_val, MAX_LEN)\n            # Function to transform our numpy array to a tf Dataset\n            train_dataset, valid_dataset = transform_to_tensors(x_train, x_val, y_train, y_val,BATCH_SIZE)\n            # Build model\n            \n            with strategy.scope():\n                model = build_roberta_model(max_len = MAX_LEN,MODEL = MODEL)\n            # Model checkpoint\n            checkpoint = tf.keras.callbacks.ModelCheckpoint(f'Roberta_{modelType}_{trainStrategy}_{SEED}_{fold + 1}.h5', \n                                                            monitor = 'val_root_mean_squared_error', \n                                                            verbose = VERBOSE, \n                                                            save_best_only = True,\n                                                            save_weights_only = True, \n                                                            mode = 'min')\n            steps = x_train.shape[0] // BATCH_SIZE \n            # Set CLR options\n            clr_step_size = steps\n            base_lr = LR\n            max_lr = LR1\n            mode='triangular'\n            clr = CyclicLR(base_lr=base_lr, max_lr=max_lr, step_size=clr_step_size, mode=mode)\n            \n            es = EarlyStopping(monitor='val_root_mean_squared_error', mode='min', patience=20, \n                       restore_best_weights=True, verbose=1)\n            \n            reduceLROnPlat = ReduceLROnPlateau(monitor='val_root_mean_squared_error',  \n                                        factor=0.07, patience=10, \n                                        verbose=1, mode='min', \n                                        epsilon=0.0001, cooldown=1, min_lr=0.00001)\n\n\n            # Training phase\n            callback_list = [checkpoint,reduceLROnPlat] #lr_callback reduceLROnPlat\n            #model.load_weights('../input/commonlit-readability-roberta-tf/Roberta_Base_123_1.h5')\n         \n            # Training phase\n            history = model.fit(train_dataset,\n                                batch_size = BATCH_SIZE,\n                                epochs = EPOCHS,\n                                verbose = VERBOSE,\n                                callbacks = [callback_list],\n                                validation_data = valid_dataset,\n                                steps_per_epoch = steps)\n\n\n            # Load best epoch weights\n            model.load_weights(f'Roberta_{modelType}_{trainStrategy}_{SEED}_{fold + 1}.h5')\n            # Predict validation set to save them in the out of folds array\n            val_pred = model.predict(valid_dataset)\n            oof_predictions[val_ind] = val_pred.reshape(-1)\n            print(\"\\n\\n\\n\\n\")\n            print('-'*50)\n            del train_dataset, valid_dataset\n            del model\n            if(tpu_on):\n                print(\"\\ntpu on, clearing memory for next fold........\\n\\n\\n\")\n                tf.tpu.experimental.initialize_tpu_system(tpu) \n            gc.collect()\n         \n\n\n    \n\n    print('\\n')\n    print('-'*50)\n    # Calculate out of folds root mean squared error\n    oof_rmse = np.sqrt(mean_squared_error(df['target'], oof_predictions))\n    print(f'Our out of folds RMSE is {oof_rmse}')\n    ","metadata":{"execution":{"iopub.status.busy":"2021-06-23T20:14:02.805834Z","iopub.execute_input":"2021-06-23T20:14:02.80608Z","iopub.status.idle":"2021-06-23T20:14:02.833891Z","shell.execute_reply.started":"2021-06-23T20:14:02.806056Z","shell.execute_reply":"2021-06-23T20:14:02.833156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Experiment - 1**\n\ntrain distilroberta base 5 stratifiedkfold ","metadata":{}},{"cell_type":"code","source":"%%time\n\ntrain_and_evaluate(trainStrategy=\"kfold\",modelType = \"distilroberta\")#stratifiedkfold\n","metadata":{"execution":{"iopub.status.busy":"2021-06-23T20:14:03.389056Z","iopub.execute_input":"2021-06-23T20:14:03.389432Z","iopub.status.idle":"2021-06-23T20:31:57.487753Z","shell.execute_reply.started":"2021-06-23T20:14:03.389391Z","shell.execute_reply":"2021-06-23T20:31:57.486923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Experiment - 2**\n\ntrain xlmr base 5 kfolds\n\n","metadata":{}},{"cell_type":"code","source":"%%time\n\n#train_and_evaluate(trainStrategy=\"stratifiedkfold\")\n","metadata":{"execution":{"iopub.status.busy":"2021-06-23T20:13:54.190082Z","iopub.execute_input":"2021-06-23T20:13:54.190407Z","iopub.status.idle":"2021-06-23T20:13:54.195935Z","shell.execute_reply.started":"2021-06-23T20:13:54.190372Z","shell.execute_reply":"2021-06-23T20:13:54.194847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Experiment - 3**\n\ntrain xlm roberta base 8 stratified kfolds  ","metadata":{}},{"cell_type":"code","source":"\n%%time\n\n\n#train_and_evaluate(trainStrategy=\"stratifiedkfold\")\n","metadata":{"execution":{"iopub.status.busy":"2021-06-23T20:09:20.496654Z","iopub.execute_input":"2021-06-23T20:09:20.497224Z","iopub.status.idle":"2021-06-23T20:09:20.504313Z","shell.execute_reply.started":"2021-06-23T20:09:20.497185Z","shell.execute_reply":"2021-06-23T20:09:20.503366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# References : \n1. https://www.kaggle.com/ragnar123/commonlit-readability-roberta-tf\n2. https://www.kaggle.com/abhishek/step-1-create-folds\n3. https://github.com/mhmoodlan/cyclic-learning-rate/blob/master/clr.py\n4. https://www.kaggle.com/ajax0564/training-tf-roberta-on-mlm","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}