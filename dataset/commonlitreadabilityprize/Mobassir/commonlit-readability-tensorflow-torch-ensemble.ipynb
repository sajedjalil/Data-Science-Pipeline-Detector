{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#!pip install tokenizers==0.9.4\n#!pip install transformers==4.2.1\nimport sys\n#sys.modules['tokenizers'].__version__\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-20T09:46:38.980738Z","iopub.execute_input":"2021-06-20T09:46:38.981067Z","iopub.status.idle":"2021-06-20T09:46:38.985735Z","shell.execute_reply.started":"2021-06-20T09:46:38.981031Z","shell.execute_reply":"2021-06-20T09:46:38.98477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport importlib\nimportlib.reload(sys.modules['pkg_resources'])\n\nsys.modules['pkg_resources'].get_distribution('tokenizers').version\n","metadata":{"execution":{"iopub.status.busy":"2021-06-20T09:46:39.087392Z","iopub.execute_input":"2021-06-20T09:46:39.087652Z","iopub.status.idle":"2021-06-20T09:46:39.394207Z","shell.execute_reply.started":"2021-06-20T09:46:39.087626Z","shell.execute_reply":"2021-06-20T09:46:39.39336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom numba import cuda \nimport pandas as pd\n\nimport re\nimport os\nimport numpy as np\nimport pandas as pd\nimport random\nimport math\nimport tensorflow as tf\nimport logging\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nfrom tensorflow.keras import backend as K\nfrom transformers import RobertaTokenizer, TFRobertaModel\nfrom kaggle_datasets import KaggleDatasets\ntf.get_logger().setLevel(logging.ERROR)\nfrom kaggle_datasets import KaggleDatasets\nfrom transformers import AutoTokenizer\n\nimport numpy as np\nimport pandas as pd \nimport os\nimport gc\nimport sys\nimport time\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import KFold, StratifiedKFold\nimport xgboost as xgb\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import mean_squared_error\n\nfrom transformers import AutoModel, AutoTokenizer\nimport json\nfrom tensorflow.keras.models import load_model\nimport re\nimport pandas as pd\nimport string\nimport keras\nfrom sklearn.svm import SVR\n\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport pickle\nfrom tqdm import tqdm\nimport gc\n\nfrom sklearn.model_selection import KFold,StratifiedKFold,train_test_split\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.linear_model import LinearRegression, Ridge\n\nimport tensorflow as tf \nfrom tensorflow.keras.layers import Input,LSTM,Bidirectional,Embedding,Dense, Conv1D, Dropout , MaxPool1D , MaxPooling1D, GlobalAveragePooling2D , GlobalAveragePooling1D , GlobalMaxPooling1D , concatenate , Flatten\nfrom tensorflow.keras.metrics import RootMeanSquaredError\nfrom tensorflow.keras.models import Model,load_model,save_model , model_from_json\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau,ModelCheckpoint, EarlyStopping ,LearningRateScheduler\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras import backend as K\n\nfrom transformers import TFBertModel, BertTokenizerFast , BertTokenizer , RobertaTokenizerFast , TFRobertaModel , RobertaConfig , TFAutoModel , AutoTokenizer\n","metadata":{"execution":{"iopub.status.busy":"2021-06-20T09:46:39.395666Z","iopub.execute_input":"2021-06-20T09:46:39.396159Z","iopub.status.idle":"2021-06-20T09:46:47.845572Z","shell.execute_reply.started":"2021-06-20T09:46:39.396122Z","shell.execute_reply":"2021-06-20T09:46:47.844691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.modules['tokenizers'].__version__","metadata":{"execution":{"iopub.status.busy":"2021-06-20T09:46:47.848861Z","iopub.execute_input":"2021-06-20T09:46:47.84913Z","iopub.status.idle":"2021-06-20T09:46:47.853932Z","shell.execute_reply.started":"2021-06-20T09:46:47.849104Z","shell.execute_reply":"2021-06-20T09:46:47.853063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import importlib\nimportlib.reload(sys.modules['pkg_resources'])\n\nsys.modules['pkg_resources'].get_distribution('tokenizers').version","metadata":{"execution":{"iopub.status.busy":"2021-06-20T09:46:47.855614Z","iopub.execute_input":"2021-06-20T09:46:47.856176Z","iopub.status.idle":"2021-06-20T09:46:48.146693Z","shell.execute_reply.started":"2021-06-20T09:46:47.85614Z","shell.execute_reply":"2021-06-20T09:46:48.145743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.__version__","metadata":{"execution":{"iopub.status.busy":"2021-06-20T09:46:48.148113Z","iopub.execute_input":"2021-06-20T09:46:48.148486Z","iopub.status.idle":"2021-06-20T09:46:48.155274Z","shell.execute_reply.started":"2021-06-20T09:46:48.148449Z","shell.execute_reply":"2021-06-20T09:46:48.154489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import transformers\ntransformers.__version__","metadata":{"execution":{"iopub.status.busy":"2021-06-20T09:46:48.156669Z","iopub.execute_input":"2021-06-20T09:46:48.157261Z","iopub.status.idle":"2021-06-20T09:46:48.162799Z","shell.execute_reply.started":"2021-06-20T09:46:48.157225Z","shell.execute_reply":"2021-06-20T09:46:48.161937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls '../input/commonlitdistilroberta'\n","metadata":{"execution":{"iopub.status.busy":"2021-06-20T09:46:48.16501Z","iopub.execute_input":"2021-06-20T09:46:48.165694Z","iopub.status.idle":"2021-06-20T09:46:48.812717Z","shell.execute_reply.started":"2021-06-20T09:46:48.165655Z","shell.execute_reply":"2021-06-20T09:46:48.811767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model 0 :\nhttps://www.kaggle.com/bharadwajvedula/clr-lb-0-475-lazy-way-to-get-good-score\n","metadata":{}},{"cell_type":"code","source":"gpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    for gpu in gpus:\n        tf.config.experimental.set_memory_growth(gpu,True)\n        \nmax_len = 250\nbatch_size = 32\nAUTOTUNE = tf.data.AUTOTUNE\n\nMODEL=['bert-base-uncased' , 'roberta-base']\n\nmodel_name = MODEL[1]\n\npath=[\n    \"../input/commonlitreadabilityprize/sample_submission.csv\",\n    \"../input/commonlitreadabilityprize/test.csv\",\n    \"../input/commonlitreadabilityprize/train.csv\"\n]\n\ndf_train = pd.read_csv(path[2])\ndf_test = pd.read_csv(path[1])\ndf_ss = pd.read_csv(path[0])\n                         \ndf_train = df_train.drop(['url_legal','license','standard_error'],axis='columns')\ndf_test = df_test.drop(['url_legal','license'],axis='columns')\nX= df_train['excerpt']\ny=df_train['target'].values\n\nX_test = df_test['excerpt']\n\ntokenizer1 = AutoTokenizer.from_pretrained(\"../input/huggingface-roberta-variants/roberta-base/roberta-base\")\n\nprint('tokenization')\ntrain_embeddings = tokenizer1(X.to_list(), truncation = True , padding = 'max_length' , max_length=max_len)\ntest_embeddings = tokenizer1(X_test.to_list() , truncation = True , padding = 'max_length' , max_length = max_len)\n                         \n@tf.function\ndef map_function(encodings):\n    input_ids = encodings['input_ids']\n    \n    return {'input_word_ids': input_ids}\n\nprint(\"generating train and test\")    \ntrain = tf.data.Dataset.from_tensor_slices((train_embeddings))\ntrain = (\n            train\n            .map(map_function, num_parallel_calls=AUTOTUNE)\n            .batch(16)\n            .prefetch(AUTOTUNE)\n        )\n\n\ntest = tf.data.Dataset.from_tensor_slices((test_embeddings))\ntest = (\n        test\n        .map(map_function, num_parallel_calls = AUTOTUNE)\n        .batch(16)\n        .prefetch(AUTOTUNE)\n    )\n                         \n                         \ndef build_roberta_base_model(max_len=max_len ):\n    \n    transformer = TFAutoModel.from_pretrained(\"../input/huggingface-roberta-variants/roberta-base/roberta-base\")\n    \n    input_word_ids = tf.keras.layers.Input(shape = (max_len, ), dtype = tf.int32, name = 'input_word_ids')\n    sequence_output = transformer(input_word_ids)[0]\n    \n    # We only need the cls_token, resulting in a 2d array\n    cls_token = sequence_output[:, 0, :]\n    output = tf.keras.layers.Dense(1, activation = 'linear', dtype = 'float32')(cls_token)\n    \n    model = tf.keras.models.Model(inputs = [input_word_ids], outputs = output)\n    \n    return model\n                         \nragnar_model = build_roberta_base_model()\ndef feature_extractor(path):\n    print(\"loading weights\")\n    ragnar_model.load_weights(path)\n    x= ragnar_model.layers[-3].output\n    model = Model(inputs = ragnar_model.inputs , outputs = x)\n    return model\n                         \ndef get_preds(model,train,test):\n    print(\"Extracting Features from train data\")\n    train_features = model.predict( train , verbose =1)\n    train_features = train_features.last_hidden_state\n    train_features = train_features[: , 0 , :]\n    print(\"Extracting Features from train data\")\n    test_features = model.predict( test , verbose =1)\n    test_features = test_features.last_hidden_state\n    test_features = test_features[: , 0 , :]\n    \n    return np.array(train_features , dtype= np.float16) , np.array(test_features , dtype= np.float16) \n                         \n#model weight paths\npaths=[\"../input/commonlit-readability-roberta-base/Roberta_Base_123_1.h5\",\n       \"../input/commonlit-readability-roberta-base/Roberta_Base_123_2.h5\",\n       \"../input/commonlit-readability-roberta-base/Roberta_Base_123_3.h5\",\n       \"../input/commonlit-readability-roberta-base/Roberta_Base_123_4.h5\",\n       \"../input/commonlit-readability-roberta-base/Roberta_Base_123_5.h5\"\n      ]\n                         \n#1\nextraction_model = feature_extractor(paths[0])\ntrain_embeddings1 , test_embeddings1 = get_preds(extraction_model , train , test)\n                         \n#2\nextraction_model = feature_extractor(paths[1])\ntrain_embeddings2 , test_embeddings2 = get_preds(extraction_model , train , test)\n                         \n#3\nextraction_model = feature_extractor(paths[2])\ntrain_embeddings3 , test_embeddings3 = get_preds(extraction_model , train , test)\n                         \n#4\nextraction_model = feature_extractor(paths[3])\ntrain_embeddings4 , test_embeddings4 = get_preds(extraction_model , train , test)\n                         \n#5\nextraction_model = feature_extractor(paths[4])\ntrain_embeddings5 , test_embeddings5 = get_preds(extraction_model , train , test)\n                         \ndef get_preds(train_embeddings , test_embeddings):\n    scores=[]\n    kfold = KFold(n_splits=5, shuffle= True , random_state=2021)\n    iteration=1\n    preds = np.zeros((test_embeddings.shape[0]))\n    for train_idx, test_idx in kfold.split(train_embeddings,y):\n        print(f'running iteration {iteration}')\n        X_train = train_embeddings[train_idx]\n        X_test = train_embeddings[test_idx]\n        y_train = y[train_idx]\n        y_test = y[test_idx]\n\n        regression_model = Ridge()\n        \n        regression_model.fit(X_train,y_train)\n        y_pred = regression_model.predict(X_test)\n\n        score = np.sqrt(mse(y_pred,y_test))\n        scores.append(score)\n        print(f'Fold {iteration} , rmse score: {score}')\n        y_preds = regression_model.predict(test_embeddings)\n        y_preds=y_preds.reshape(-1)\n        preds+=y_preds  \n        iteration += 1\n\n    print(f\"the average rmse is {np.mean(scores)}\")\n    return np.array(preds)/5  \n                         \n                         \nprint(\"***********predicting***********\")\npreds1 = get_preds(train_embeddings1,test_embeddings1)\nprint(\"***********predicting***********\")\npreds2 = get_preds(train_embeddings2,test_embeddings2)\nprint(\"***********predicting***********\")\npreds3 = get_preds(train_embeddings3,test_embeddings3)\nprint(\"***********predicting***********\")\npreds4 = get_preds(train_embeddings4,test_embeddings4)\nprint(\"***********predicting***********\")\npreds5 = get_preds(train_embeddings5,test_embeddings5)\n\npreds=(preds1+preds2+preds3+preds4+preds5)/5\npreds = preds.tolist()\nsub0=pd.DataFrame({'id':df_ss['id'],'target':preds})\n#sub.to_csv('submission.csv',index=False)\nsub0.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T09:48:33.726443Z","iopub.execute_input":"2021-06-20T09:48:33.726782Z","iopub.status.idle":"2021-06-20T09:52:32.418994Z","shell.execute_reply.started":"2021-06-20T09:48:33.726751Z","shell.execute_reply":"2021-06-20T09:52:32.416243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# model 1 : \nhttps://www.kaggle.com/mobassir/readability-tensorflow-roberta-training-baseline","metadata":{}},{"cell_type":"code","source":"# Read our test data\ndf = pd.read_csv('../input/commonlitreadabilityprize/test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#generator = AutoTokenizer.from_pretrained(MODEL,  use_fast=True) ","metadata":{"execution":{"iopub.status.busy":"2021-06-14T18:04:55.36035Z","iopub.execute_input":"2021-06-14T18:04:55.360715Z","iopub.status.idle":"2021-06-14T18:04:55.365289Z","shell.execute_reply.started":"2021-06-14T18:04:55.360675Z","shell.execute_reply":"2021-06-14T18:04:55.36405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.listdir('../input/readability-tensorflow-roberta-training-baseline/')","metadata":{"execution":{"iopub.status.busy":"2021-06-14T18:04:55.366688Z","iopub.execute_input":"2021-06-14T18:04:55.367117Z","iopub.status.idle":"2021-06-14T18:04:55.37717Z","shell.execute_reply.started":"2021-06-14T18:04:55.367081Z","shell.execute_reply":"2021-06-14T18:04:55.376104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = [\n    \n# fold 1 -> 0.50380\n#fold 2 -> 0.51644\n#fold 3 -> 0.49793\n#fold 4 -> 0.53571 \n#fold 5 -> 0.50805\n    \n    '../input/commonlitdistilroberta/Roberta_distilroberta_stratifiedkfold_12345_5.h5',\n    #'../input/commonlitdistilroberta/Roberta_distilroberta_stratifiedkfold_12345_4.h5',\n    '../input/commonlitdistilroberta/Roberta_distilroberta_stratifiedkfold_12345_3.h5',\n    #'../input/commonlitdistilroberta/Roberta_distilroberta_stratifiedkfold_12345_2.h5',\n    '../input/commonlitdistilroberta/Roberta_distilroberta_stratifiedkfold_12345_1.h5',\n  \n  \n]","metadata":{"execution":{"iopub.status.busy":"2021-06-14T18:04:55.378448Z","iopub.execute_input":"2021-06-14T18:04:55.378833Z","iopub.status.idle":"2021-06-14T18:04:55.384732Z","shell.execute_reply.started":"2021-06-14T18:04:55.378797Z","shell.execute_reply":"2021-06-14T18:04:55.383809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This function tokenize the text according to a transformers model tokenizer\ndef regular_encode(texts, tokenizer, maxlen = 350):\n    enc_di = tokenizer.batch_encode_plus(\n        texts,\n        padding = 'max_length',\n        truncation = True,\n        max_length = maxlen,\n    )\n    \n    return np.array(enc_di['input_ids'])\n\n# This function encode our training sentences\ndef encode_texts(x_test, MAX_LEN = 350):\n    x_test = regular_encode(x_test.tolist(), tokenizer, maxlen = MAX_LEN)\n    return x_test\n\n# Function to build our model\ndef build_roberta_model(max_len = 350,MODEL=None):\n    transformer = TFRobertaModel.from_pretrained(MODEL)\n    input_word_ids = tf.keras.layers.Input(shape = (max_len, ), dtype = tf.int32, name = 'input_word_ids')\n    sequence_output = transformer(input_word_ids)[0]\n    # We only need the cls_token, resulting in a 2d array\n    cls_token = sequence_output[:, 0, :]\n    output = tf.keras.layers.Dense(1, activation = 'linear', dtype = 'float32')(cls_token)\n    model = tf.keras.models.Model(inputs = [input_word_ids], outputs = [output])\n    return model\n\n# Function for inference\ndef distilroberta_base_inference():\n    # Configurations\n\n    # Max length\n    MAX_LEN = 350\n    # Get the trained model we want to use\n\n    MODEL = '../input/huggingface-roberta-variants/distilroberta-base/distilroberta-base'\n\n    #MODEL = '../input/distilrobertapretrained'\n\n    # Let's load our model tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n    #tokenizer = AutoTokenizer.from_pretrained(\"distilroberta-base\") \n    \n\n    # Get text features\n    x_test = df['excerpt']\n    # Encode our text with Roberta tokenizer\n    x_test = encode_texts(x_test, MAX_LEN)\n    # Initiate an empty vector to store prediction\n    predictions = np.zeros(len(df))\n    # Predict with the 5 models (5 folds training)\n    for i in range(len(models)):\n        print('\\n')\n        print('-'*50)\n        print(f'Predicting with model {i + 1}')\n        # Build model\n        model = build_roberta_model(max_len = MAX_LEN,MODEL = MODEL)\n        # Load pretrained weights\n        print(models[i])\n        model.load_weights(models[i])\n        # Predict\n        fold_predictions = model.predict(x_test).reshape(-1)\n        # Add fold prediction to the global predictions\n        predictions += fold_predictions / len(models)\n    # Save submissions\n    #df['target'] = predictions\n    #df[['id', 'target']].to_csv('submissiontensorflow.csv', index = False)\n    print(\"donee\")\n    return predictions\n\n#tfdf = distilroberta_base_inference()","metadata":{"execution":{"iopub.status.busy":"2021-06-14T18:04:55.386041Z","iopub.execute_input":"2021-06-14T18:04:55.386445Z","iopub.status.idle":"2021-06-14T18:04:55.402728Z","shell.execute_reply.started":"2021-06-14T18:04:55.386407Z","shell.execute_reply":"2021-06-14T18:04:55.401929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# model 2:\nhttps://www.kaggle.com/ragnar123/commonlit-readability-roberta-tf-inference","metadata":{}},{"cell_type":"code","source":"# Configurations\n# Number of folds for training\nFOLDS = 5\n# Max length\nMAX_LEN = 250\n# Get the trained model we want to use\nMODEL = '../input/tfroberta-base'\n# Let's load our model tokenizer\ntokenizer = RobertaTokenizer.from_pretrained(MODEL)\n\n\n# This function tokenize the text according to a transformers model tokenizer\ndef regular_encode(texts, tokenizer, maxlen = MAX_LEN):\n    enc_di = tokenizer.batch_encode_plus(\n        texts,\n        padding = 'max_length',\n        truncation = True,\n        max_length = maxlen,\n    )\n    \n    return np.array(enc_di['input_ids'])\n\n# This function encode our training sentences\ndef encode_texts(x_test, MAX_LEN = 350):\n    x_test = regular_encode(x_test.tolist(), tokenizer, maxlen = MAX_LEN)\n    return x_test\n\n# Function to build our model\ndef build_roberta_base_model(max_len = MAX_LEN):\n    transformer = TFRobertaModel.from_pretrained(MODEL)\n    input_word_ids = tf.keras.layers.Input(shape = (max_len, ), dtype = tf.int32, name = 'input_word_ids')\n    sequence_output = transformer(input_word_ids)[0]\n    # We only need the cls_token, resulting in a 2d array\n    cls_token = sequence_output[:, 0, :]\n    output = tf.keras.layers.Dense(1, activation = 'linear', dtype = 'float32')(cls_token)\n    model = tf.keras.models.Model(inputs = [input_word_ids], outputs = [output])\n    return model\n\n# Function for inference\ndef roberta_base_inference():\n    #predictions1 = distilroberta_base_inference()\n    # Read our test data\n    df = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\n    # Get text features\n    x_test = df['excerpt']\n    # Encode our text with Roberta tokenizer\n    x_test = encode_texts(x_test, MAX_LEN)\n    # Initiate an empty vector to store prediction\n    predictions = np.zeros(len(df))\n    # Predict with the 5 models (5 folds training)\n    for i in range(FOLDS):\n        print('\\n')\n        print('-'*50)\n        print(f'Predicting with model {i + 1}')\n        # Build model\n        model = build_roberta_base_model(max_len = MAX_LEN)\n        # Load pretrained weights\n        model.load_weights(f'../input/commonlit-readability-roberta-base/Roberta_Base_123_{i + 1}.h5')\n        # Predict\n        fold_predictions = model.predict(x_test).reshape(-1)\n        # Add fold prediction to the global predictions\n        predictions += fold_predictions / FOLDS\n    # Save submissions\n    \n    \n    df['target'] = predictions\n    #df[['id', 'target']].to_csv('submission2.csv', index = False)\n    return df\n\nragdf = roberta_base_inference()\nragdf.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-14T18:04:55.405905Z","iopub.execute_input":"2021-06-14T18:04:55.40619Z","iopub.status.idle":"2021-06-14T18:06:27.960385Z","shell.execute_reply.started":"2021-06-14T18:04:55.406166Z","shell.execute_reply":"2021-06-14T18:06:27.959458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = cuda.get_current_device()\ndevice.reset()\n!nvidia-smi","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# torch","metadata":{}},{"cell_type":"markdown","source":"# model 3: https://www.kaggle.com/rhtsingh/commonlit-readability-prize-roberta-torch-infer-3","metadata":{}},{"cell_type":"code","source":"%matplotlib inline\nfrom glob import glob\nimport os\nimport matplotlib.pyplot as plt\nimport json\nfrom collections import defaultdict\nimport gc\ngc.enable()\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim.optimizer import Optimizer\nimport torch.optim.lr_scheduler as lr_scheduler\nfrom torch.utils.data import (\n    Dataset, DataLoader, \n    SequentialSampler, RandomSampler\n)\nfrom transformers import RobertaConfig\nfrom transformers import (\n    get_cosine_schedule_with_warmup, \n    get_cosine_with_hard_restarts_schedule_with_warmup\n)\nfrom transformers import RobertaTokenizer\nfrom transformers import RobertaModel\nfrom IPython.display import clear_output\nfrom tqdm import tqdm, trange\n\nimport pandas as pd\nimport numpy as np\n\nimport os\nimport gc\nimport sys\nimport cv2\nimport math\nimport time\nimport tqdm\nimport random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.svm import SVR\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold,StratifiedKFold\n\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.optim import Adam, lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom transformers import (AutoModel, AutoTokenizer, \n                          AutoModelForSequenceClassification)\n\nimport plotly.express as px\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\n\n\nfrom colorama import Fore, Back, Style\n\n\ntrain = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\ntest = pd.read_csv('../input/commonlitreadabilityprize/test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_examples_to_features(data, tokenizer, max_len, is_test=False):\n    data = data.replace('\\n', '')\n    tok = tokenizer.encode_plus(\n        data, \n        max_length=max_len, \n        truncation=True,\n        return_attention_mask=True,\n        return_token_type_ids=True\n    )\n    curr_sent = {}\n    padding_length = max_len - len(tok['input_ids'])\n    curr_sent['input_ids'] = tok['input_ids'] + ([0] * padding_length)\n    curr_sent['token_type_ids'] = tok['token_type_ids'] + \\\n        ([0] * padding_length)\n    curr_sent['attention_mask'] = tok['attention_mask'] + \\\n        ([0] * padding_length)\n    return curr_sent","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DatasetRetriever(Dataset):\n    def __init__(self, data, tokenizer, max_len, is_test=False):\n        self.data = data\n        self.excerpts = self.data.excerpt.values.tolist()\n        self.tokenizer = tokenizer\n        self.is_test = is_test\n        self.max_len = max_len\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, item):\n        if not self.is_test:\n            excerpt, label = self.excerpts[item], self.targets[item]\n            features = convert_examples_to_features(\n                excerpt, self.tokenizer, \n                self.max_len, self.is_test\n            )\n            return {\n                'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n                'token_type_ids':torch.tensor(features['token_type_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n                'label':torch.tensor(label, dtype=torch.double),\n            }\n        else:\n            excerpt = self.excerpts[item]\n            features = convert_examples_to_features(\n                excerpt, self.tokenizer, \n                self.max_len, self.is_test\n            )\n            return {\n                'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n                'token_type_ids':torch.tensor(features['token_type_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n            }\n     ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CommonLitModel(nn.Module):\n    def __init__(\n        self, \n        model_name, \n        config,  \n        multisample_dropout=False,\n        output_hidden_states=False\n    ):\n        super(CommonLitModel, self).__init__()\n        self.config = config\n        self.roberta = RobertaModel.from_pretrained(\n            model_name, \n            output_hidden_states=output_hidden_states\n        )\n        self.layer_norm = nn.LayerNorm(config.hidden_size)\n        if multisample_dropout:\n            self.dropouts = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n        else:\n            self.dropouts = nn.ModuleList([nn.Dropout(0.3)])\n        self.regressor = nn.Linear(config.hidden_size, 1)\n        self._init_weights(self.layer_norm)\n        self._init_weights(self.regressor)\n \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n \n    def forward(\n        self, \n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        labels=None\n    ):\n        outputs = self.roberta(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n        )\n        sequence_output = outputs[1]\n        sequence_output = self.layer_norm(sequence_output)\n \n        # multi-sample dropout\n        for i, dropout in enumerate(self.dropouts):\n            if i == 0:\n                logits = self.regressor(dropout(sequence_output))\n            else:\n                logits += self.regressor(dropout(sequence_output))\n        \n        logits /= len(self.dropouts)\n \n        # calculate loss\n        loss = None\n        if labels is not None:\n            loss_fn = torch.nn.MSELoss()\n            logits = logits.view(-1).to(labels.dtype)\n            loss = torch.sqrt(loss_fn(logits, labels.view(-1)))\n        \n        output = (logits,) + outputs[1:]\n        return ((loss,) + output) if loss is not None else output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_model(model_name, num_labels=1):\n    tokenizer = RobertaTokenizer.from_pretrained(model_name)\n    config = RobertaConfig.from_pretrained(model_name)\n    config.update({'num_labels':num_labels})\n    model = CommonLitModel(model_name, config=config)\n    return model, tokenizer\n\ndef make_loader(\n    data, \n    tokenizer, \n    max_len,\n    batch_size,\n):\n    \n    test_dataset = DatasetRetriever(data, tokenizer, max_len, is_test=True)\n    test_sampler = SequentialSampler(test_dataset)\n    test_loader = DataLoader(\n        test_dataset, \n        batch_size=batch_size // 2, \n        sampler=test_sampler, \n        pin_memory=False, \n        drop_last=False, \n        num_workers=0\n    )\n\n    return test_loader","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Evaluator:\n    def __init__(self, model, scalar=None):\n        self.model = model\n        self.scalar = scalar\n\n    def evaluate(self, data_loader, tokenizer):\n        preds = []\n        self.model.eval()\n        total_loss = 0\n        with torch.no_grad():\n            for batch_idx, batch_data in enumerate(data_loader):\n                input_ids, attention_mask, token_type_ids = batch_data['input_ids'], \\\n                    batch_data['attention_mask'], batch_data['token_type_ids']\n                input_ids, attention_mask, token_type_ids = input_ids.cuda(), \\\n                    attention_mask.cuda(), token_type_ids.cuda()\n                \n                if self.scalar is not None:\n                    with torch.cuda.amp.autocast():\n                        outputs = self.model(\n                            input_ids=input_ids,\n                            attention_mask=attention_mask,\n                            token_type_ids=token_type_ids\n                        )\n                else:\n                    outputs = self.model(\n                        input_ids=input_ids,\n                        attention_mask=attention_mask,\n                        token_type_ids=token_type_ids\n                    )\n                \n                logits = outputs[0].detach().cpu().numpy().squeeze().tolist()\n                preds += logits\n        return preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" def config(fold, model_name, load_model_path):\n    torch.manual_seed(2021)\n    torch.cuda.manual_seed(2021)\n    torch.cuda.manual_seed_all(2021)\n    \n    max_len = 250\n    batch_size = 8\n\n    model, tokenizer = make_model(\n        model_name=model_name, \n        num_labels=1\n    )\n    model.load_state_dict(\n        torch.load(f'{load_model_path}/model{fold}.bin')\n    )\n    test_loader = make_loader(\n        test, tokenizer, max_len=max_len,\n        batch_size=batch_size\n    )\n\n    if torch.cuda.device_count() >= 1:\n        print('Model pushed to {} GPU(s), type {}.'.format(\n            torch.cuda.device_count(), \n            torch.cuda.get_device_name(0))\n        )\n        model = model.cuda() \n    else:\n        raise ValueError('CPU training is not supported')\n\n    # scaler = torch.cuda.amp.GradScaler()\n    scaler = None\n    return (\n        model, tokenizer, \n        test_loader, scaler\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run(fold=0, model_name=None, load_model_path=None):\n    model, tokenizer, \\\n        test_loader, scaler = config(fold, model_name, load_model_path)\n    \n    import time\n\n    evaluator = Evaluator(model, scaler)\n\n    test_time_list = []\n\n    torch.cuda.synchronize()\n    tic1 = time.time()\n\n    preds = evaluator.evaluate(test_loader, tokenizer)\n\n    torch.cuda.synchronize()\n    tic2 = time.time() \n    test_time_list.append(tic2 - tic1)\n    \n    del model, tokenizer, test_loader, scaler\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    return preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\npred_df1 = pd.DataFrame()\npred_df2 = pd.DataFrame()\npred_df3 = pd.DataFrame()\nfor fold in tqdm(range(5)):\n    pred_df1[f'fold{fold}'] = run(fold, '../input/roberta-base/', '../input/commonlit-roberta-base-i/')\n    pred_df2[f'fold{fold+5}'] = run(fold, '../input/robertalarge/', '../input/roberta-large-itptfit/')\n    pred_df3[f'fold{fold+10}'] = run(fold, '../input/robertalarge/', '../input/commonlit-roberta-large-ii/')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# model 4 : https://www.kaggle.com/maunish/clrp-roberta-svm","metadata":{}},{"cell_type":"code","source":"\ny_ = Fore.YELLOW\nr_ = Fore.RED\ng_ = Fore.GREEN\nb_ = Fore.BLUE\nm_ = Fore.MAGENTA\nc_ = Fore.CYAN\nsr_ = Style.RESET_ALL\n\n\ntrain_data = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\ntest_data = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\nsample = pd.read_csv('../input/commonlitreadabilityprize/sample_submission.csv')\n\nnum_bins = int(np.floor(1 + np.log2(len(train_data))))\ntrain_data.loc[:,'bins'] = pd.cut(train_data['target'],bins=num_bins,labels=False)\n\ntarget = train_data['target'].to_numpy()\nbins = train_data.bins.to_numpy()\n\ndef rmse_score(y_true,y_pred):\n    return np.sqrt(mean_squared_error(y_true,y_pred))\n\n\nconfig = {\n    'batch_size':128,\n    'max_len':256,\n    'nfolds':5,\n    'seed':42,\n}\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONASSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(seed=config['seed'])\n\n\nclass CLRPDataset(Dataset):\n    def __init__(self,df,tokenizer):\n        self.excerpt = df['excerpt'].to_numpy()\n        self.tokenizer = tokenizer\n    \n    def __getitem__(self,idx):\n        encode = self.tokenizer(self.excerpt[idx],return_tensors='pt',\n                                max_length=config['max_len'],\n                                padding='max_length',truncation=True)\n        return encode\n    \n    def __len__(self):\n        return len(self.excerpt)\n    \n    \n    \nclass AttentionHead(nn.Module):\n    def __init__(self, in_features, hidden_dim, num_targets):\n        super().__init__()\n        self.in_features = in_features\n        self.middle_features = hidden_dim\n\n        self.W = nn.Linear(in_features, hidden_dim)\n        self.V = nn.Linear(hidden_dim, 1)\n        self.out_features = hidden_dim\n\n    def forward(self, features):\n        att = torch.tanh(self.W(features))\n\n        score = self.V(att)\n\n        attention_weights = torch.softmax(score, dim=1)\n\n        context_vector = attention_weights * features\n        context_vector = torch.sum(context_vector, dim=1)\n\n        return context_vector\n    \nclass Model(nn.Module):\n    def __init__(self):\n        super(Model,self).__init__()\n        self.roberta = AutoModel.from_pretrained('../input/roberta-base')    \n        self.head = AttentionHead(768,768,1)\n        self.dropout = nn.Dropout(0.1)\n        self.linear = nn.Linear(self.head.out_features,1)\n\n    def forward(self,**xb):\n        x = self.roberta(**xb)[0]\n        x = self.head(x)\n        return x\n    \n    \ndef get_embeddings(df,path,plot_losses=True, verbose=True):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"{device} is used\")\n            \n    model = Model()\n    model.load_state_dict(torch.load(path))\n    model.to(device)\n    model.eval()\n    \n    tokenizer = AutoTokenizer.from_pretrained('../input/roberta-base')\n    \n    ds = CLRPDataset(df,tokenizer)\n    dl = DataLoader(ds,\n                  batch_size = config[\"batch_size\"],\n                  shuffle=False,\n                  num_workers = 4,\n                  pin_memory=True,\n                  drop_last=False\n                 )\n        \n    embeddings = list()\n    with torch.no_grad():\n        for i, inputs in tqdm(enumerate(dl)):\n            inputs = {key:val.reshape(val.shape[0],-1).to(device) for key,val in inputs.items()}\n            outputs = model(**inputs)\n            outputs = outputs.detach().cpu().numpy()\n            embeddings.extend(outputs)\n    return np.array(embeddings)\n\n\ntrain_embeddings1 =  get_embeddings(train_data,'../input/clr-roberta/model0/model0.bin')\ntest_embeddings1 = get_embeddings(test_data,'../input/clr-roberta/model0/model0.bin')\n\ntrain_embeddings2 =  get_embeddings(train_data,'../input/clr-roberta/model1/model1.bin')\ntest_embeddings2 = get_embeddings(test_data,'../input/clr-roberta/model1/model1.bin')\n\ntrain_embeddings3 =  get_embeddings(train_data,'../input/clr-roberta/model2/model2.bin')\ntest_embeddings3 = get_embeddings(test_data,'../input/clr-roberta/model2/model2.bin')\n\ntrain_embeddings4 =  get_embeddings(train_data,'../input/clr-roberta/model3/model3.bin')\ntest_embeddings4 = get_embeddings(test_data,'../input/clr-roberta/model3/model3.bin')\n\ntrain_embeddings5 =  get_embeddings(train_data,'../input/clr-roberta/model4/model4.bin')\ntest_embeddings5 = get_embeddings(test_data,'../input/clr-roberta/model4/model4.bin')\nfrom sklearn.linear_model import Ridge\ndef get_preds_svm(X,y,X_test,RidgeReg=0,bins=bins,nfolds=5,C=20,kernel='rbf'):\n    scores = list()\n    preds = np.zeros((X_test.shape[0]))\n    \n    kfold = StratifiedKFold(n_splits=config['nfolds'],shuffle=True,random_state=config['seed'])\n    for k, (train_idx,valid_idx) in enumerate(kfold.split(X,bins)):\n        if(RidgeReg):\n            print(\"ridge...\")\n            model = Ridge(alpha=50.0)\n        else:\n            model = SVR(C=C,kernel=kernel,gamma='auto')\n        X_train,y_train = X[train_idx], y[train_idx]\n        X_valid,y_valid = X[valid_idx], y[valid_idx]\n        \n        model.fit(X_train,y_train)\n        prediction = model.predict(X_valid)\n        score = rmse_score(prediction,y_valid)\n        print(f'Fold {k} , rmse score: {score}')\n        scores.append(score)\n        preds += model.predict(X_test)\n        \n    print(\"mean rmse\",np.mean(scores))\n    return np.array(preds)/nfolds\n\n\nsvm_preds1 = get_preds_svm(train_embeddings1,target,test_embeddings1)\nsvm_preds2 = get_preds_svm(train_embeddings2,target,test_embeddings2)\nsvm_preds3 = get_preds_svm(train_embeddings3,target,test_embeddings3)\nsvm_preds4 = get_preds_svm(train_embeddings4,target,test_embeddings4)\nsvm_preds5 = get_preds_svm(train_embeddings5,target,test_embeddings5)\n\nridge_preds1 = get_preds_svm(train_embeddings1,target,test_embeddings1,RidgeReg=1)\nridge_preds2 = get_preds_svm(train_embeddings2,target,test_embeddings2,RidgeReg=1)\nridge_preds3 = get_preds_svm(train_embeddings3,target,test_embeddings3,RidgeReg=1)\nridge_preds4 = get_preds_svm(train_embeddings4,target,test_embeddings4,RidgeReg=1)\nridge_preds5 = get_preds_svm(train_embeddings5,target,test_embeddings5,RidgeReg=1)\n\n\n#all_preds = (svm_preds1 + svm_preds2 + svm_preds3 + svm_preds4 + svm_preds5+ridge_preds1+ridge_preds2+ridge_preds3+ridge_preds4+ridge_preds5)/10\nall_preds = (svm_preds1 + svm_preds2 + svm_preds3 + svm_preds4 + svm_preds5)/5\n\nsample.target = all_preds\n#sample.to_csv('submission.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# model 5 : https://www.kaggle.com/abhishek/fork-of-fork-of-yum-yum-yum-93f968","metadata":{}},{"cell_type":"code","source":"\n\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\nclass Dataset:\n    def __init__(self, excerpt, tokenizer, max_len):\n        self.excerpt = excerpt\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.excerpt)\n\n    def __getitem__(self, item):\n        text = str(self.excerpt[item])\n        inputs = self.tokenizer(\n            text, \n            max_length=self.max_len, \n            padding=\"max_length\", \n            truncation=True\n        )\n\n        ids = inputs[\"input_ids\"]\n        mask = inputs[\"attention_mask\"]\n\n        return {\n            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n            \"attention_mask\": torch.tensor(mask, dtype=torch.long),\n        }\n    \n    \ndef generate_predictions(model_path, max_len):\n    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n\n    model.to(\"cuda\")\n    model.eval()\n    \n    df = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")\n    \n    dataset = Dataset(excerpt=df.excerpt.values, tokenizer=tokenizer, max_len=max_len)\n    data_loader = torch.utils.data.DataLoader(\n        dataset, batch_size=32, num_workers=4, pin_memory=True, shuffle=False\n    )\n\n    final_output = []\n\n    for b_idx, data in enumerate(data_loader):\n        with torch.no_grad():\n            for key, value in data.items():\n                data[key] = value.to(\"cuda\")\n            output = model(**data)\n            output = output.logits.detach().cpu().numpy().ravel().tolist()\n            final_output.extend(output)\n    \n    torch.cuda.empty_cache()\n    return np.array(final_output)\n\n\npreds1 = generate_predictions(\"../input/a81653/\", max_len=256)\npreds2 = generate_predictions(\"../input/a81656/\", max_len=256)\npreds3 = generate_predictions(\"../input/a81657/\", max_len=256)\npreds4 = generate_predictions(\"../input/a81660/\", max_len=256)\npreds5 = generate_predictions(\"../input/a81675/\", max_len=192)\npreds6 = generate_predictions(\"../input/a87832/\", max_len=256)\n\n#https://www.kaggle.com/ankur310794/crp-mix-6-models-inference\n\nweights_pos = [2.29301865e-08 ,9.18492143e-02 ,3.56011564e-01, 5.34926853e-09,\n 8.52853500e-02 ,4.66853844e-01]\n\nweights = weights_pos\npreds = preds1*weights[0] + preds2*weights[1] + preds3*weights[2]+ preds4*weights[3] + preds5*weights[4]+ preds6*weights[5]\n\n#preds = (preds1 + preds2 + preds3 + preds4 + preds5 + preds6) / 6\n\nsubmission = pd.read_csv(\"../input/commonlitreadabilityprize/sample_submission.csv\")\nsubmission.target = preds\n#submission.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# model 6 : https://www.kaggle.com/datafan07/eda-simple-bayesian-ridge-with-sentence-embeddings","metadata":{}},{"cell_type":"code","source":"# adding wordcloud in offline mode\nimport sys\nsys.path.append('../input/wcloud/word_cloud-master')\nsys.path.append('../input/sentence-transformers/sentence-transformers-master')\nimport wordcloud\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import Ridge\nfrom scipy import stats\nfrom scipy.stats import norm\n\nimport matplotlib.gridspec as gridspec\nfrom matplotlib.ticker import MaxNLocator\n\n# setting some globl config\n\nplt.style.use('ggplot')\norange_black = [\n    '#fdc029', '#df861d', '#FF6347', '#aa3d01', '#a30e15', '#800000', '#171820'\n]\nplt.rcParams['figure.figsize'] = (16,9)\nplt.rcParams[\"figure.facecolor\"] = '#FFFACD'\nplt.rcParams[\"axes.facecolor\"] = '#FFFFE0'\nplt.rcParams[\"axes.grid\"] = True\nplt.rcParams[\"grid.color\"] = orange_black[3]\nplt.rcParams[\"grid.alpha\"] = 0.5\nplt.rcParams[\"grid.linestyle\"] = '--'\n\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# loding train and test data\n\ntrain_df = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\ntest_df = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\n\n# dropping some columns\n\ntrain_df=train_df[['id','excerpt','target','standard_error']]\n\n# creating corpus\n\nfrom nltk import FreqDist\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nstopwords = set(stopwords.words('english'))\n\nnew = train_df['excerpt'].str.split()\nnew = new.values.tolist()\ncorpus = [word.lower() for i in new for word in i if word.lower() not in stopwords]\n          \n\ncommon_words = [i[0] for i in FreqDist(corpus).most_common(20)]\ncommon_words_count = [i[1] for i in FreqDist(corpus).most_common(20)]\n\nimport sentence_transformers\nfrom sentence_transformers import SentenceTransformer, models\n\nmodel_path = '../input/finetuned-model1/checkpoint-568'\nword_embedding_model = models.Transformer(model_path, max_seq_length=275)\npooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\nmodel = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n\nX_train = model.encode(train_df.excerpt, device='cuda')\nX_test = model.encode(test_df.excerpt, device='cuda')\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom datetime import datetime\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import BayesianRidge\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import VotingRegressor\n\ntrain_df['Character Count'] = train_df['excerpt'].apply(lambda x: len(str(x)))\npreds = []\n\n\ndf_oof=train_df.copy()\ndf_oof['oof'] = 0\n\nfeature_importance_df = pd.DataFrame()\n\n\nskf = StratifiedKFold(10, shuffle=True, random_state=42)\n\nsplits = list(skf.split(X=X_train, y=train_df['Character Count']))\nfor i, (train_idx, val_idx) in enumerate(splits):\n    print(f'\\n------------- Training Fold {i + 1} / {10}')\n    print(\"Current Time =\", datetime.now().strftime(\"%H:%M:%S\"))\n    r1 = LinearRegression()\n    r2 = RandomForestRegressor(n_estimators=30, random_state=43)\n    ridge = Ridge(alpha=50.0)\n    br = BayesianRidge(n_iter=30, verbose=True)\n\n    clf =   BayesianRidge(n_iter=30, verbose=True) #VotingRegressor([('r2', r2), ('br', br)])\n    clf.fit(X_train[train_idx],train_df.target[train_idx])\n    \n    preds.append(clf.predict(X_test))\n    x=clf.predict(X_train[val_idx])\n    df_oof['oof'].iloc[val_idx]+= x\n\nprint(f'Training score: {mean_squared_error(train_df.target, clf.predict(X_train), squared=False)}')\nprint(f'OOF score across folds: {mean_squared_error(df_oof.target, df_oof.oof, squared=False)}')\n\n# getting mean prediction across 5 folds\ny_pred = np.mean(preds,0)\n\n# creating submission csv\n\nmysub = test_df[[\"id\"]].copy()\nmysub[\"target\"] = y_pred\n\n\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model 7 : https://www.kaggle.com/maunish/clrp-pytorch-roberta-inference","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport sys\nimport cv2\nimport math\nimport time\nimport tqdm\nimport random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import KFold,StratifiedKFold\n\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader,Sampler\n\nimport multiprocessing\nimport more_itertools\n\n\nfrom transformers import (AutoModel, AutoTokenizer, AutoModelForSequenceClassification)\n\nfrom colorama import Fore, Back, Style\ny_ = Fore.YELLOW\nr_ = Fore.RED\ng_ = Fore.GREEN\nb_ = Fore.BLUE\nm_ = Fore.MAGENTA\nc_ = Fore.CYAN\nsr_ = Style.RESET_ALL\n\ntrain_data = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\ntest_data = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\nsample1 = pd.read_csv('../input/commonlitreadabilityprize/sample_submission.csv')\n\nconfig = {\n    'learning_rate':2e-5,\n    'batch_size':32,\n    'epochs':10,\n    'nfolds':5,\n    'seed':42,\n    'max_len':256,\n}\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONASSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(seed=config['seed'])\n\nclass CLRPDataset(Dataset):\n    def __init__(self,df,tokenizer):\n        self.excerpt = df['excerpt'].to_numpy()\n        self.tokenizer = tokenizer\n    \n    def __getitem__(self,idx):\n        encode = self.tokenizer(self.excerpt[idx],return_tensors='pt',\n                                max_length=config['max_len'],\n                                padding='max_length',truncation=True)\n        return encode\n    \n    def __len__(self):\n        return len(self.excerpt)\nclass AttentionHead(nn.Module):\n    def __init__(self, in_features, hidden_dim, num_targets):\n        super().__init__()\n        self.in_features = in_features\n        self.middle_features = hidden_dim\n\n        self.W = nn.Linear(in_features, hidden_dim)\n        self.V = nn.Linear(hidden_dim, 1)\n        self.out_features = hidden_dim\n\n    def forward(self, features):\n        att = torch.tanh(self.W(features))\n\n        score = self.V(att)\n\n        attention_weights = torch.softmax(score, dim=1)\n\n        context_vector = attention_weights * features\n        context_vector = torch.sum(context_vector, dim=1)\n\n        return context_vector\n    \n    \nclass Model(nn.Module):\n    def __init__(self):\n        super(Model,self).__init__()\n        self.roberta = AutoModel.from_pretrained('../input/roberta-base')    \n        self.head = AttentionHead(768,768,1)\n        self.dropout = nn.Dropout(0.1)\n        self.linear = nn.Linear(768,1)\n\n    def forward(self,**xb):\n        x = self.roberta(**xb)[0]\n        x = self.head(x)\n        x = self.dropout(x)\n        x = self.linear(x)\n        return x\n\n\ndef get_prediction(df,path,device='cuda'):        \n    model = Model()\n    model.load_state_dict(torch.load(path))\n    model.to(device)\n    model.eval()\n\n    tokenizer = AutoTokenizer.from_pretrained('../input/roberta-base')\n\n    test_ds = CLRPDataset(df,tokenizer)\n    test_dl = DataLoader(test_ds,\n                        batch_size = config[\"batch_size\"],\n                        shuffle=False,\n                        num_workers = 4,\n                        pin_memory=True)\n\n    predictions = list()\n    for i, (inputs) in tqdm(enumerate(test_dl)):\n        inputs = {key:val.reshape(val.shape[0],-1).to(device) for key,val in inputs.items()}\n        outputs = model(**inputs)\n        outputs = outputs.cpu().detach().numpy().ravel().tolist()\n        predictions.extend(outputs)\n\n    torch.cuda.empty_cache()\n    return np.array(predictions)\n\npred1 = get_prediction(test_data,'../input/clr-roberta/model0/model0.bin')\npred2 = get_prediction(test_data,'../input/clr-roberta/model1/model1.bin')\npred3 = get_prediction(test_data,'../input/clr-roberta/model2/model2.bin')\npred4 = get_prediction(test_data,'../input/clr-roberta/model3/model3.bin')\npred5 = get_prediction(test_data,'../input/clr-roberta/model4/model4.bin')\n\npredictions = (pred1 + pred2 + pred3 + pred4 + pred5)/5\n\nsample1['target'] = predictions\n#sample.to_csv('submission.csv',index=False)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-06-16T18:23:46.932167Z","iopub.execute_input":"2021-06-16T18:23:46.932495Z","iopub.status.idle":"2021-06-16T18:24:21.652489Z","shell.execute_reply.started":"2021-06-16T18:23:46.932465Z","shell.execute_reply":"2021-06-16T18:24:21.650256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.read_csv('../input/commonlitreadabilityprize/sample_submission.csv')\n\nsub['target'] = ((pred_df2.mean(axis=1)) + (pred_df1.mean(axis=1))+ (pred_df3.mean(axis=1)) +sample['target']+ragdf['target']+sub0['target'])/6\nsub.to_csv('submission.csv', index=False)\nsub","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}