{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:350%;text-align:center;border-radius: 15px 50px;\">Introduction</h1>\n\n","metadata":{}},{"cell_type":"markdown","source":"In the following notebook, we dive into some more refined analysis on text-derived features and create two baseline models.\nA small experiment will be performed to uncover some of the differences between the hardest texts and the easiest ones.\nAbout the experiment:\nWe will divide the texts into the given data set into two groups which we will call \"Hard\" and \"Easy,\" the \"Easy\" group will contain all the texts whose reading ease score is below two standard deviations, and the \"Hard\" group will include those who are above two standard deviations.\nOur goal will be to analyze and determine which features differ the most between the two groups, an attribute that increases the importance of such a feature when predicting a reading ease score.\n\nIn the process of creating and evaluating the models, we will engage the problem from 2 perspectives; the first will be a correlation-based approach where we will use the insight gained through the data analysis to construct simple models and test their performance.\nThe second approach will be a deep neural network based on the embeddings of the text in our dataset.\nThe neural network will be a simple linear descent dense head connected to an embeddings layer.\n","metadata":{}},{"cell_type":"markdown","source":"<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Libraries and Data Loading</h1>\n\n","metadata":{}},{"cell_type":"code","source":"import scipy\nimport re\nimport string\nimport nltk\nimport random\nimport os\nimport pymc3                                                as pm\nimport tensorflow                                           as tf\nimport numpy                                                as np \nimport pandas                                               as pd \nimport matplotlib.pyplot                                    as plt\nimport matplotlib.cm                                        as cm\nimport seaborn                                              as sns\nimport plotly.express                                       as ex\nimport plotly.graph_objs                                    as go\nimport plotly.offline                                       as pyo\nimport spacy                                                as sp\nfrom plotly.subplots                                        import make_subplots\nfrom sklearn.decomposition                                  import TruncatedSVD,PCA\nfrom sklearn.cluster                                        import DBSCAN\nfrom sklearn.manifold                                       import Isomap\nfrom sklearn.feature_extraction.text                        import CountVectorizer,TfidfVectorizer\nfrom sklearn.cluster                                        import KMeans\nfrom nltk.sentiment.vader                                   import SentimentIntensityAnalyzer as SIA\nfrom wordcloud                                              import WordCloud,STOPWORDS\nfrom nltk.util                                              import ngrams\nfrom nltk                                                   import word_tokenize\nfrom nltk.stem                                              import PorterStemmer\nfrom nltk.stem                                              import WordNetLemmatizer\nfrom tqdm.notebook                                          import tqdm\nfrom keras                                                  import backend as K\nimport warnings\nwarnings.filterwarnings(\"ignore\")\ntqdm.pandas()\nsns.set_style('darkgrid')\n#nltk.download('vader_lexicon')\npyo.init_notebook_mode()\nnlps = sp.load('en')\nplt.rc('figure',figsize=(16,8))\nsns.set_context('paper',font_scale=1.5)\ndef set_seed(seed=31415):\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\nset_seed()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/commonlitreadabilityprize/train.csv')\ntest  = pd.read_csv('/kaggle/input/commonlitreadabilityprize/test.csv')\n\nc_df = pd.concat([train,test])[['excerpt','target']]\nc_df.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Data Preprocessing and Feature Engineering</h1>\n\n","metadata":{}},{"cell_type":"code","source":"#Preprocessing and Feature Engineering \n\n\n\n#===========================Feature Engineering =========================================================================================\n\n#Naive Features\nc_df['# Of Words']              = c_df['excerpt'].apply(lambda x: len(x.split(' ')))\nc_df['# Of StopWords']          = c_df['excerpt'].apply(lambda x: len([word for word in x.split(' ') if word in list(STOPWORDS)]))\nc_df['# Of Sentences']          = c_df['excerpt'].apply(lambda x: len(re.findall('\\.',x)))\nc_df['Average Word Length']     = c_df['excerpt'].apply(lambda x: np.mean(np.array([len(va) for va in x.split(' ') if va not in list(STOPWORDS)])))\nc_df['Average Sentence Length'] = c_df['excerpt'].apply(lambda x: np.mean(np.array([len(va) for va in x.split('.')])))\n\n\n#Recored Entities\nENTS_PER_TEXT = []\n\nfor E in tqdm(c_df['excerpt']):\n    \n    TAGGED_ENTS = { 'GPE'    : [],\n                    'MONEY'  : [],\n                    'PERSON' : [],\n                    'EVENT'  : [],\n                    'FAC'    : []\n              }\n    for tok in nlps(E).ents:\n        if tok.label_ in ['GPE','MONEY','PERSON','EVENT','FAC']:\n            TAGGED_ENTS[tok.label_].append(tok)\n    ENTS_PER_TEXT.append(TAGGED_ENTS)   \n\n#Named Entity Extraction (\"CAN BE SKIPPED BY USING DATA COLLECTED IN THE PERIVOUS FOR LOOP\")\nc_df['# Of Different Countries Mentioned']    = c_df['excerpt'].progress_apply(lambda x: len([tok for tok in nlps(x).ents if tok.label_ == 'GPE' ]))\nc_df['# Of Times Money Was Mentioned']        = c_df['excerpt'].progress_apply(lambda x: len([tok for tok in nlps(x).ents if tok.label_ == 'MONEY' ]))\nc_df['# Of Different People Mentioned']       = c_df['excerpt'].progress_apply(lambda x: len([tok for tok in nlps(x).ents if tok.label_ == 'PERSON' ]))\nc_df['# Of Different Events Mentioned']       = c_df['excerpt'].progress_apply(lambda x: len([tok for tok in nlps(x).ents if tok.label_ == 'EVENT' ]))\nc_df['# Of Different Facilities Mentioned']   = c_df['excerpt'].progress_apply(lambda x: len([tok for tok in nlps(x).ents if tok.label_ == 'FAC' ]))\n\n\n        \n\n\n#Sentiment Analysis\nsid = SIA()\nc_df['sentiments']           = c_df['excerpt'].progress_apply(lambda x: sid.polarity_scores(x))\nc_df['Positive Sentiment']   = c_df['sentiments'].apply(lambda x: x['pos']+1*(10**-6)) \nc_df['Neutral Sentiment']    = c_df['sentiments'].apply(lambda x: x['neu']+1*(10**-6))\nc_df['Negative Sentiment']   = c_df['sentiments'].apply(lambda x: x['neg']+1*(10**-6))\n\nc_df.drop(columns=['sentiments'],inplace=True)\n\n#===========================Data Preprocessing=========================================================================================\n\n# Remove all the special characters\nc_df.excerpt              = c_df.excerpt.apply(lambda x: ''.join(re.sub(r'\\W', ' ', x))) \n# Substituting multiple spaces with single space \nc_df.excerpt              = c_df.excerpt.apply(lambda x: ''.join(re.sub(r'\\s+', ' ', x, flags=re.I)))\n# Converting to Lowercase \nc_df.excerpt              = c_df.excerpt.str.lower() \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"c_df.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:350%;text-align:center;border-radius: 15px 50px;\">Exploratory Data Analysis</h1>\n\n","metadata":{}},{"cell_type":"markdown","source":"<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Global Analysis</h1>\n\n","metadata":{}},{"cell_type":"code","source":"plt.subplot(211)\nplt.title('Distribution of Reading Ease Score in Our Data')\nax = sns.kdeplot(c_df.target,lw=2)\nplt.vlines(c_df.target.mean(),0,0.36,color='red',label='Mean',lw=3,ls='--')\nplt.vlines(c_df.target.median(),0,0.36,color='tab:green',label='Median',lw=3,ls='-.')\nplt.vlines(scipy.stats.mode(c_df.target)[0],0,0.36,color='tab:Purple',label='Mode',lw=3,ls='-')\n\nplt.xlabel('')\nplt.legend()\nplt.subplot(212)\nsns.kdeplot(c_df.target,cumulative=True,lw=3,color='tab:orange')\nplt.xlabel('Reading Ease')\nplt.tight_layout()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mask = np.zeros_like(c_df.corr())\nmask[np.abs(c_df.corr()) < 0.1] = 1\nsns.heatmap(c_df.corr(),cmap='coolwarm',annot=True,mask=mask,linewidth=2)\n","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Text Based Analysis</h1>\n\n","metadata":{}},{"cell_type":"code","source":"ALL_GPS    = []\nALL_MONEY  = []\nALL_PERSON = []\nALL_EVENT  = []\nALL_FAC    = []\nfor dic in ENTS_PER_TEXT:\n    for key in ['GPE','MONEY','PERSON','EVENT','FAC']:\n        if len(dic['GPE']) != 0:\n            ALL_GPS+=list(dic['GPE'])\n        if len(dic['MONEY']) != 0:\n            ALL_MONEY+=(dic['MONEY'])\n        if len(dic['PERSON']) != 0:\n            ALL_PERSON+=(dic['PERSON'])\n        if len(dic['EVENT']) != 0:\n            ALL_EVENT+=(dic['EVENT'])\n        if len(dic['FAC']) != 0:\n            ALL_FAC+=(dic['FAC'])\ntop_10_places = pd.Series([i.text for i in ALL_GPS]).value_counts()[:10]\ntop_10_money = pd.Series([i.text for i in ALL_MONEY]).value_counts()[:10]\ntop_10_person = pd.Series([i.text for i in ALL_PERSON]).value_counts()[:10]\ntop_10_event = pd.Series([i.text for i in ALL_EVENT]).value_counts()[:10]\ntop_10_fac = pd.Series([i.text for i in ALL_FAC]).value_counts()[:10]","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.subplot(211)\nplt.title('Top 10 Most Frequently Mentioned Locations')\nsns.barplot(x=top_10_places.index,y=top_10_places.values,palette=cm.twilight(top_10_places.values))\nplt.subplot(212)\nplt.title('Top 10 Most Frequently Currency Related Words/Phrases')\nsns.barplot(x=top_10_money.index,y=top_10_money.values,palette=cm.twilight(top_10_money.values*10))\n","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.subplot(311)\nplt.title('Top 10 Most Frequently Mentioned Pepole')\nsns.barplot(x=top_10_person.index,y=top_10_person.values,palette=cm.twilight(top_10_person.values))\nplt.subplot(312)\nplt.title('Top 10 Most Frequently Mentioned Events')\nax = sns.barplot(x=top_10_event.index,y=top_10_event.values,palette=cm.twilight(top_10_event.values*10))\nax.set_xticklabels(ax.get_xticklabels(),rotation=90)\nplt.subplot(313)\nplt.title('Top 10 Most Frequently Mentioned Facilites')\nsns.barplot(x=top_10_fac.index,y=top_10_fac.values,palette=cm.twilight(top_10_fac.values*10))\n\nplt.tight_layout()","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NUMBER_OF_COMPONENTS = 1200\n\nCV = CountVectorizer(stop_words='english',ngram_range=(1,1))\ncv_df = CV.fit_transform(c_df.excerpt)\ncv_df = pd.DataFrame(cv_df.toarray(),columns=CV.vocabulary_)\n\nsvd = TruncatedSVD(NUMBER_OF_COMPONENTS)\ndecomposed = svd.fit_transform(cv_df)\n\nevr = svd.explained_variance_ratio_\ntotal_var = evr.sum() * 100\ncumsum_evr = np.cumsum(evr)\n\ntrace1 = {\n    \"name\": \"individual explained variance\", \n    \"type\": \"bar\", \n    'y':evr}\ntrace2 = {\n    \"name\": \"cumulative explained variance\", \n    \"type\": \"scatter\", \n     'y':cumsum_evr}\ndata = [trace1, trace2]\nlayout = {\n    \"xaxis\": {\"title\": \"Principal components\"}, \n    \"yaxis\": {\"title\": \"Explained variance ratio\"},\n  }\nfig = go.Figure(data=data, layout=layout)\nfig.update_layout(     title='{:.2f}% of the Song Lyrics Variance Can Be Explained Using {} Words out of {} Unique words'.format(np.sum(evr)*100,NUMBER_OF_COMPONENTS,len(CV.vocabulary_)))\nfig.show()","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf_idf = TfidfVectorizer(stop_words='english',ngram_range=(1,2))\n\ntrans_df = tf_idf.fit_transform(c_df.excerpt[:-7])\n\nisomap = Isomap(n_components=3)\n\nisomap_dec = isomap.fit_transform(trans_df)\n\ntemp  =  pd.DataFrame(isomap_dec)\ntemp = temp.rename(columns={0:'Dim1',1:'Dim2',2:'Dim3'})\ntemp['target'] = c_df.target[:-7].values\n\nex.scatter_3d(temp,x='Dim1',y='Dim2',z='Dim3',color='target',title='Spread of Tfidf Vectorized Samples in R^3')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"db = DBSCAN(eps=1.2,min_samples=80)\ndb.fit(temp)\ntemp['cluster'] =db.labels_\n\nex.scatter_3d(temp,x='Dim1',y='Dim2',z='Dim3',color='cluster',title='Tfidf Vectorized Samples in R^3 Clusterd By Density')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as grid_spec\nfrom sklearn.neighbors import KernelDensity\ndec_df= temp.copy()\nr = lambda: np.random.randint(0,255)\ndef random_color_hex():\n    return ('#%02X%02X%02X' % (r(),r(),r()))\ncountries = pd.Series(db.labels_).unique()\ncolors = [random_color_hex() for _ in range(0,len(countries))]\ngs = grid_spec.GridSpec(len(countries),1)\nfig = plt.figure(figsize=(16,9))\n\ni = 0\n\nax_objs = []\nfor cluster in countries:\n    x = np.array(dec_df[dec_df.cluster == cluster]['target'])\n    x_d = np.linspace(-4,4, 1000)\n\n    kde = KernelDensity(bandwidth=0.1, kernel='gaussian')\n    kde.fit(x[:, None])\n\n    logprob = kde.score_samples(x_d[:, None])\n\n    # creating new axes object\n    ax_objs.append(fig.add_subplot(gs[i:i+1, 0:]))\n\n    # plotting the distribution\n    ax_objs[-1].plot(x_d, np.exp(logprob),color=\"#f0f0f0\",lw=1)\n    ax_objs[-1].fill_between(x_d, np.exp(logprob), alpha=1,color=colors[i])\n\n    #ax_objs[-1].set_xlim(0,1.)\n    #ax_objs[-1].set_ylim(0,2.5)\n\n    # make background transparent\n    rect = ax_objs[-1].patch\n    rect.set_alpha(0)\n\n    # remove borders, axis ticks, and labels\n    ax_objs[-1].set_yticklabels([])\n    ax_objs[-1].grid(False)\n    if i == len(countries)-1:\n        ax_objs[-1].set_xlabel(\"target\", fontsize=16,fontweight=\"bold\")\n    else:\n        ax_objs[-1].set_xticklabels([])\n\n    spines = [\"top\",\"right\",\"left\",\"bottom\"]\n    for s in spines:\n        ax_objs[-1].spines[s].set_visible(False)\n\n    #adj_country = country.replace(\" \",\"\\n\")\n    ax_objs[-1].text(-4,0,f'Cluster: {cluster}',fontweight=\"bold\",fontsize=14,ha=\"right\")\n\n\n    i += 1\n\ngs.update(hspace=-0.6)\n\nfig.text(0.07,0.89,\"Distribution Reading Ease Score in Each Cluster\",fontsize=20)\n\nplt.grid(False)\nplt.tight_layout()\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Hardest/Easiest Expirement</h1>\n\n","metadata":{}},{"cell_type":"code","source":"plt.title('Distribution of Reading Ease Score in Our Data')\nax = sns.kdeplot(c_df.target,lw=2)\nkde_x, kde_y = ax.lines[0].get_data()\np1 = plt.axvline(x=c_df.target.mean()+2*c_df.target.std(),color='tab:red')\np2 = plt.axvline(x=c_df.target.mean()-2*c_df.target.std(),color='tab:green')\n\n\nax.fill_between(kde_x, kde_y, where=((kde_x<=c_df.target.mean()-2*c_df.target.std()) ) , interpolate=True, color='tab:green',label='Easiest to Read')\nax.fill_between(kde_x, kde_y, where=((kde_x>=c_df.target.mean()+2*c_df.target.std())) , interpolate=True, color='tab:red',label='Hardest to Read')\n\nplt.xlabel('')\nplt.legend()","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"e_df = c_df[c_df.target <= c_df.target.mean()-2*c_df.target.std() ].copy()\nh_df = c_df[c_df.target >= c_df.target.mean()+2*c_df.target.std() ].copy()\na_df = c_df[(c_df.target > c_df.target.mean()-2*c_df.target.std()) & (c_df.target < c_df.target.mean()+2*c_df.target.std())].copy()\n\n_e_ENTS = pd.DataFrame(ENTS_PER_TEXT,index=c_df.index).loc[e_df.index,:]\n_h_ENTS = pd.DataFrame(ENTS_PER_TEXT,index=c_df.index).loc[h_df.index,:]\n\nfor col in e_df.columns[1:]:\n    e_df[col] = (e_df[col]-a_df[col].mean())/a_df[col].std()\n    h_df[col] = (h_df[col]-a_df[col].mean())/a_df[col].std()\n    \nE_WC,H_WC = None,None\nE_WC = WordCloud(background_color='white',width=800,height=400,stopwords=STOPWORDS,collocations=False).generate(' '.join(e_df.excerpt))\nH_WC = WordCloud(background_color='white',width=800,height=400,stopwords=STOPWORDS,collocations=False).generate(' '.join(h_df.excerpt))\n\nplt.subplot(121)\nplt.title('Most Common Words in Easiest Samples')\nplt.imshow(E_WC)\nplt.axis('off')\nplt.subplot(122)\nplt.title('Most Common Words in Hardest Samples')\nplt.imshow(H_WC)\nplt.axis('off')\n\nplt.tight_layout()","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"medf = e_df.iloc[:,1:].melt()\nmhdf = h_df.iloc[:,1:].melt()\nmedf['Set'] = 'Easy'\nmhdf['Set'] = 'Hard'\nax = sns.boxplot(x='variable',y='value',hue='Set',data=pd.concat([medf,mhdf]),showfliers = False,notch=True)\n\nax.set_xticklabels(ax.get_xticklabels(),rotation=90)\nplt.show()","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observation: Looking at the boxplot above, we can see that in terms of z-score, we have some significant differences between the two groups.\nFeatures like \"Number of Sentences,\" \"Average Sentence Length,\" and Positive Sentiment differ by at least one standard deviation from each other; such an analysis saves us the time of performing t-tests or ANOVA as we can see a significant difference between the means.","metadata":{}},{"cell_type":"code","source":"EASY_GPS    = []\nEASY_MONEY  = []\nEASY_PERSON = []\nEASY_EVENT  = []\nEASY_FAC    = []\n\nHARD_GPS    = []\nHARD_MONEY  = []\nHARD_PERSON = []\nHARD_EVENT  = []\nHARD_FAC    = []\nfor index,row in _e_ENTS.iterrows():\n    dic = {'GPE':row['GPE'],'MONEY':row['MONEY'],'PERSON':row['PERSON'],'EVENT':row['EVENT'],'FAC':row['FAC']}\n    for key in ['GPE','MONEY','PERSON','EVENT','FAC']:\n        if len(dic['GPE']) != 0:\n            EASY_GPS+=list(dic['GPE'])\n        if len(dic['MONEY']) != 0:\n            EASY_MONEY+=(dic['MONEY'])\n        if len(dic['PERSON']) != 0:\n            EASY_PERSON+=(dic['PERSON'])\n        if len(dic['EVENT']) != 0:\n            EASY_EVENT+=(dic['EVENT'])\n        if len(dic['FAC']) != 0:\n            EASY_FAC+=(dic['FAC'])\n            \nfor index,row in _h_ENTS.iterrows():\n    dic = {'GPE':row['GPE'],'MONEY':row['MONEY'],'PERSON':row['PERSON'],'EVENT':row['EVENT'],'FAC':row['FAC']}\n    for key in ['GPE','MONEY','PERSON','EVENT','FAC']:\n        if len(dic['GPE']) != 0:\n            HARD_GPS+=list(dic['GPE'])\n        if len(dic['MONEY']) != 0:\n            HARD_MONEY+=(dic['MONEY'])\n        if len(dic['PERSON']) != 0:\n            HARD_PERSON+=(dic['PERSON'])\n        if len(dic['EVENT']) != 0:\n            HARD_EVENT+=(dic['EVENT'])\n        if len(dic['FAC']) != 0:\n            HARD_FAC+=(dic['FAC'])\n            \nEASY_top_10_places  = pd.Series([i.text for i in EASY_GPS]).value_counts()[:10]\nEASY_top_10_money   = pd.Series([i.text for i in EASY_MONEY]).value_counts()[:10]\nEASY_top_10_person  = pd.Series([i.text for i in EASY_PERSON]).value_counts()[:10]\nEASY_top_10_event   = pd.Series([i.text for i in EASY_EVENT]).value_counts()[:10]\nEASY_top_10_fac     = pd.Series([i.text for i in EASY_FAC]).value_counts()[:10]\n\nHARD_top_10_places  = pd.Series([i.text for i in HARD_GPS]).value_counts()[:10]\nHARD_top_10_money   = pd.Series([i.text for i in HARD_MONEY]).value_counts()[:10]\nHARD_top_10_person  = pd.Series([i.text for i in HARD_PERSON]).value_counts()[:10]\nHARD_top_10_event   = pd.Series([i.text for i in HARD_EVENT]).value_counts()[:10]\nHARD_top_10_fac     = pd.Series([i.text for i in HARD_FAC]).value_counts()[:10]","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.subplot(211)\nplt.title('Top 10 Most Frequently Mentioned Locations in \"Easiest\" Text')\nsns.barplot(x=EASY_top_10_places.index,y=EASY_top_10_places.values,palette=cm.twilight(EASY_top_10_places.values*10))\nplt.subplot(212)\nplt.title('Top 10 Most Frequently Currency Related Words/Phrases in \"Easiest\" Text')\nax = sns.barplot(x=EASY_top_10_money.index,y=EASY_top_10_money.values,palette=cm.twilight(EASY_top_10_money.values*10))\nax.set_xticklabels(ax.get_xticklabels(),rotation=90)\n\nplt.tight_layout()","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.subplot(311)\nplt.title('Top 10 Most Frequently Mentioned Pepole in \"Easiest\" Text')\nsns.barplot(x=EASY_top_10_person.index,y=EASY_top_10_person.values,palette=cm.twilight(EASY_top_10_person.values*20))\nplt.subplot(312)\nplt.title('Top 10 Most Frequently Mentioned Events in \"Easiest\" Text')\nax = sns.barplot(x=EASY_top_10_event.index,y=EASY_top_10_event.values,palette=cm.twilight(EASY_top_10_event.values*20))\nax.set_xticklabels(ax.get_xticklabels(),rotation=90)\nplt.subplot(313)\nplt.title('Top 10 Most Frequently Mentioned Facilites in \"Easiest\" Text')\nsns.barplot(x=EASY_top_10_fac.index,y=EASY_top_10_fac.values,palette=cm.twilight(EASY_top_10_fac.values*10))\n\nplt.tight_layout()","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.subplot(211)\nplt.title('Top 10 Most Frequently Mentioned Locations in \"Hardest\" Text')\nsns.barplot(x=HARD_top_10_places.index,y=HARD_top_10_places.values,palette=cm.twilight(HARD_top_10_places.values*30))\nplt.tight_layout()\nplt.subplot(212)\nplt.title('Top 10 Most Frequently Mentioned Pepole in \"Hardest\" Text')\nsns.barplot(x=HARD_top_10_person.index,y=HARD_top_10_person.values,palette=cm.twilight(HARD_top_10_person.values*20))\nplt.tight_layout()","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fixed_df = c_df[['Average Word Length','target']].copy()\nfixed_df['Average Word Length'] = np.round(fixed_df['Average Word Length'],1)\nfixed_df = fixed_df.sort_values(by='Average Word Length')\nns_df = fixed_df.groupby(by='Average Word Length').mean().reset_index()\n\nplt.title('Average Score For Every Average Word Length Bin Scaled To 1 Decimel Point')\nax= sns.barplot(x=ns_df['Average Word Length'],y=ns_df['target'])\nax.set_xticklabels(ax.get_xticklabels(),rotation=-45)\nplt.grid()\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Probabilistic Inference</h1>\n\n","metadata":{}},{"cell_type":"code","source":"n_df = c_df.iloc[:-8,:].copy()\n#n_df['Average Word Length'] = (n_df['Average Word Length']-n_df['Average Word Length'].min())/(n_df['Average Word Length'].max()-n_df['Average Word Length'].min())+0.0001\n#n_df['target'] = (n_df['target']-n_df['target'].min())/(n_df['target'].max()-n_df['target'].min())+0.0001\n\nfig = ex.scatter(n_df,x='Average Word Length',y='target',trendline='ols')\nfig.update_layout(title='<b>The linear relationship between average word length in a given text and the reading ease score<b>')\nfig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In general, the classic frequentists way to think about Linear Regression as follows:\n$$Y=Xβ+ϵ$$\nwhere $Y$ is the output we want to predict (or dependent variable), X is our predictor (or independent variable), and $β$ are the coefficients of the model we want to estimate. $ϵ$ is an error term which is assumed to be normally distributed.\n\nWe can then use Ordinary Least Squares or Maximum Likelihood to find the best fitting $β$.\n\nWe will preform the Bayesian equivalent which takes a probabilistic view of the problem and express this model in terms of probability distributions:\n\n$$Y∼N(Xβ,σ^{2})$$\nIn our case $Y$ is a random variable of which each entry is distributed according to a Normal distribution. The mean of this normal distribution is provided by our linear predictor with variance $σ^{2}$.\n","metadata":{}},{"cell_type":"code","source":"with pm.Model() as breg: \n    sigma = pm.Uniform(\"sigma\", 0,10)\n    intercept = pm.Normal(\"Intercept\", 0, sigma=20)\n    x_coeff = pm.Normal(\"x\", 0, sigma=20)\n\n    likelihood = pm.Normal(\"y\", mu=(intercept + x_coeff * n_df.iloc[:,5]), sigma=sigma, observed=n_df.iloc[:,1])\n    \n    step = pm.Metropolis()\n    trace = pm.sample(5000, cores=2,step=step)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In our Bayesian model, we have four random variables that origin in different distributions, those four random variables construct our 4D posterior landscape from which we will sample using MCMC.\n ","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15, 10))\npm.plot_trace(trace[3000:])\nplt.tight_layout();","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(17, 10))\nplt.plot(n_df.iloc[:,5], n_df.iloc[:,1], \"x\", label=\"data\")\npm.plot_posterior_predictive_glm(trace[3000:], samples=3000, label=\"posterior predictive regression lines\",eval=n_df.iloc[:,5])\n\nplt.title(\"Posterior predictive regression lines\")\nplt.legend(loc=0)\nplt.xlabel(\"Average Word Length\")\nplt.ylabel(\"Reading Ease\");","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation**: After constructing a bayesian model to sample from the posterior space of all possible linear regression intercepts and coefficients, we learn that when using the intercept and coefficient of the average word length to predict the reading ease score, we can be the most confident in our estimate when the average word length is around six letters wide.\nAs we go further away from six in both directions, we lose confidence in our estimate as the variance grows larger and the average error in our estimate increases.","metadata":{}},{"cell_type":"markdown","source":"<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:350%;text-align:center;border-radius: 15px 50px;\">Baseline Model Testing</h1>\n\n","metadata":{}},{"cell_type":"markdown","source":"<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Correlation Oriented</h1>\n\n","metadata":{}},{"cell_type":"code","source":"from sklearn.pipeline        import Pipeline\nfrom sklearn.linear_model    import LinearRegression\nfrom sklearn.tree            import DecisionTreeRegressor\nfrom sklearn.ensemble        import RandomForestRegressor\nfrom sklearn.preprocessing   import StandardScaler\nfrom sklearn.model_selection import train_test_split,cross_val_score\n\nLR_MODEL = Pipeline(steps=[('Scale',StandardScaler()),('model',LinearRegression())])\nDT_MODEL = Pipeline(steps=[('Scale',StandardScaler()),('model',DecisionTreeRegressor())])\nRF_MODEL = Pipeline(steps=[('Scale',StandardScaler()),('model',RandomForestRegressor())])\n\nx_train,x_test,y_train,y_test = train_test_split(c_df.iloc[:-8,:][['Average Word Length','# Of Sentences','# Of Different People Mentioned']],c_df.iloc[:-8,1])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LR_RESULTS = -1*cross_val_score(LR_MODEL,c_df.iloc[:-8,:][['Average Word Length','# Of Sentences','# Of Different People Mentioned']],\n                   c_df.iloc[:-8,:][['target']],cv=10,scoring='neg_root_mean_squared_error')\nDT_RESULTS = -1*cross_val_score(DT_MODEL,c_df.iloc[:-8,:][['Average Word Length','# Of Sentences','# Of Different People Mentioned']],\n                   c_df.iloc[:-8,:][['target']],cv=10,scoring='neg_root_mean_squared_error')\nRF_RESULTS = -1*cross_val_score(RF_MODEL,c_df.iloc[:-8,:][['Average Word Length','# Of Sentences','# Of Different People Mentioned']],\n                   c_df.iloc[:-8,:]['target'].values,cv=10,scoring='neg_root_mean_squared_error')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = go.Figure()\nlr_trace =  go.Scatter(x=list(range(0,len(LR_RESULTS))),y=LR_RESULTS,name='Linear Regression')\ndt_trace =  go.Scatter(x=list(range(0,len(DT_RESULTS))),y=DT_RESULTS,name='Decision Tree')\nrf_trace =  go.Scatter(x=list(range(0,len(RF_RESULTS))),y=RF_RESULTS,name='Random Forest')\n\nfig.add_trace(lr_trace)\nfig.add_trace(dt_trace)\nfig.add_trace(rf_trace)\n\nfig.update_layout(title='Different Baseline Model 10 Fold Cross Validation')\nfig.update_yaxes(title_text=\"RMSE\")\nfig.update_xaxes(title_text=\"Fold #\")\nfig.show()","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Text Oriented</h1>\n\n","metadata":{}},{"cell_type":"code","source":"from keras.preprocessing.text import one_hot\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers.embeddings import Embedding\nvalidation = c_df.iloc[:c_df.shape[0]-8,0].sample(20,random_state=42)\ntarin_df = c_df.iloc[:c_df.shape[0]-8,0].drop(index=validation.index)\n\nvocab = [i for i in list(nltk.FreqDist(' '.join(c_df.excerpt).split(' ')).keys()) ]\nvocab_size = len(vocab)\nencoded_docs = [one_hot(d, vocab_size) for d in tarin_df]\nmax_length = max(tarin_df.str.len())\npadded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n\n\nencoded_valid= [one_hot(d, vocab_size) for d in validation]\npadded_docs_valid = pad_sequences(encoded_valid, maxlen=max_length, padding='post')\n\n\nencoded_test= [one_hot(d, vocab_size) for d in test.excerpt]\npadded_docs_test = pad_sequences(encoded_test, maxlen=max_length, padding='post')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(vocab_size, 950, input_length=max_length))\nmodel.add(Flatten())\nmodel.add(Dense(250, activation='linear'))\nmodel.add(Dense(100, activation='linear'))\nmodel.add(Dense(50, activation='linear'))\nmodel.add(Dense(25, activation='linear'))\nmodel.add(Dense(5, activation='linear'))\nmodel.add(Dense(1, activation='linear'))\n\n\ndef root_mean_squared_error(y_true, y_pred):\n        return K.sqrt(K.mean(K.square(y_pred - y_true))) \nmodel.compile(optimizer='adam', loss=root_mean_squared_error, metrics=['mse'])\n\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(padded_docs,\n          c_df.iloc[tarin_df.index,1],\n          epochs=5,\n          batch_size=25,\n          validation_data=(padded_docs_valid,c_df.iloc[validation.index,1]),\n          verbose=1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.subplot(211)\nplt.title('Model loss over epochs')\npd.DataFrame(history.history).loss.plot()\nplt.subplot(212)\nplt.title('Model MSE over epochs')\npd.DataFrame(history.history).mse.plot(color='red')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction = model.predict(padded_docs_test,verbose=0)\nprediction = prediction.flatten()\nLR_MODEL.fit(x_train,y_train)\nRF_MODEL.fit(x_train,y_train)\nfinal_prediction = prediction*0.8+RF_MODEL.predict(c_df.iloc[-7:,:][['Average Word Length','# Of Sentences','# Of Different People Mentioned']])*0.1\nfinal_prediction += LR_MODEL.predict(c_df.iloc[-7:,:][['Average Word Length','# Of Sentences','# Of Different People Mentioned']])*0.1\nsubmit_df = pd.DataFrame({'id':test.id,'target':final_prediction})\nsubmit_df","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit_df.to_csv(\"submission.csv\", index = False)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Conclusions</h1>\n\n","metadata":{}},{"cell_type":"markdown","source":"<p style=\"color: rgb(14, 16, 26); background: transparent; margin-top:0pt; margin-bottom:0pt;\"><span data-preserver-spaces=\"true\" style='color: rgb(14, 16, 26); background: transparent; margin-top: 0pt; margin-bottom: 0pt; font-size: 22px; font-family: \"Times New Roman\", Times, serif;'>During the work on the following notebook, we derived several key points worth taking into account in feature works.</span></p>\n<p style=\"color: rgb(14, 16, 26); background: transparent; margin-top:0pt; margin-bottom:0pt;\"><span style=\"font-family: 'Times New Roman', Times, serif;\"><span style=\"font-size: 22px;\"><br></span></span></p>\n<ul style=\"color: rgb(14, 16, 26); background: transparent; margin-top:0pt; margin-bottom:0pt;\">\n    <li style=\"color: rgb(14, 16, 26); background: transparent; margin-top:0pt; margin-bottom:0pt; list-style-type:disc;\"><span style=\"font-family: 'Times New Roman', Times, serif;\"><span style=\"font-size: 22px;\"><span data-preserver-spaces=\"true\" style=\"color: rgb(14, 16, 26); background: transparent; margin-top:0pt; margin-bottom:0pt;\">Basic text structure features such as &quot;Average Word Length&quot; significantly affect the overall reading ease compared to more fine detail text attributes such as the sentiments or the different elements of the text&apos;s theme.</span></span></span></li>\n    <li style=\"color: rgb(14, 16, 26); background: transparent; margin-top:0pt; margin-bottom:0pt; list-style-type:disc;\"><span style=\"font-family: 'Times New Roman', Times, serif;\"><span style=\"font-size: 22px;\"><span data-preserver-spaces=\"true\" style=\"color: rgb(14, 16, 26); background: transparent; margin-top:0pt; margin-bottom:0pt;\">When looking at the most frequent entities from a list of types, we see that many labels reappear and that the distribution of those labels is far from uniform (&apos;a penny&apos;, for example, is the most frequent currency mentioned in our texts).</span></span></span></li>\n    <li style=\"color: rgb(14, 16, 26); background: transparent; margin-top:0pt; margin-bottom:0pt; list-style-type:disc;\"><span style=\"font-family: 'Times New Roman', Times, serif;\"><span style=\"font-size: 22px;\"><span data-preserver-spaces=\"true\" style=\"color: rgb(14, 16, 26); background: transparent; margin-top:0pt; margin-bottom:0pt;\">From the conducted experiment, I was surprised to see that the &quot;Hardest&quot; texts &quot;Event.&quot; labeled entities are not existing and that the &quot;French Revolution&quot; has been the most frequent event in the &quot;Easiest&quot; texts. One thing to note for future work is that there is a topic and theme difference between the two groups, as shown by the different distribution of top 10 entities in all labels.</span></span></span></li>\n    <li style=\"color: rgb(14, 16, 26); background: transparent; margin-top:0pt; margin-bottom:0pt; list-style-type:disc;\"><span style=\"font-family: 'Times New Roman', Times, serif;\"><span style=\"font-size: 22px;\"><span data-preserver-spaces=\"true\" style=\"color: rgb(14, 16, 26); background: transparent; margin-top:0pt; margin-bottom:0pt;\">When comparing different &quot;naive&quot; models, surprisingly, the Linear Regression model has scored the lowest total RMSE when fitting on different folds of the data.</span></span></span></li>\n    <li style=\"color: rgb(14, 16, 26); background: transparent; margin-top:0pt; margin-bottom:0pt; list-style-type:disc;\"><span data-preserver-spaces=\"true\" style='color: rgb(14, 16, 26); background: transparent; margin-top: 0pt; margin-bottom: 0pt; font-size: 22px; font-family: \"Times New Roman\", Times, serif;'>The most simple NN based on word embeddings has converged in about four iterations and has overscored the &quot;naive&quot; models.</span></li>\n</ul>","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}