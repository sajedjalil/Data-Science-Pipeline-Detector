{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<font color='#3498DB'><center><h2>On Stability of Few-Sample Transformer Fine-Tuning</h2></center></font>\n<br>\n\n<font color='#3498DB'><h3>Introduction</h3></font>\nFine-tuning Transformer models tend to exhibit training instability. Even with the same hyperparameter values (learning rate, batch size, etc.), distinct random seeds can lead to substantially different results. The issue is even more apparent, especially when using the large variants of Transformers on small datasets.\n\n*This notebook will dive into different aspects of the few-sample fine-tuning optimization process and techniques. The goal is to better understand the different remedies we have to deal with a few sample finetuning problems.*\n<br>\n\n<font color='#3498DB'><h3>Problem</h3></font>\n\nThe instability of the Transformer fine-tuning process has been known since the introduction of BERT, and from then various methods have been proposed to address it.\n\nE.g. in this competition, we have only ~2.8k samples. When divided into folds each model only receives ~2.2k examples and the data has noisy labels as well. So, one way we've all figured for stability is evaluating more within each epoch rather than after each epoch. \n<br>\n\n<font color='#3498DB'><h3>Solution</h3></font>\nThere are many recently proposed methods to increase few-sample fine-tuning stability and they show a significant performance improvement over simple finetuning methods. \n - Debiasing Omission In BertADAM  \n - Re-Initializing Transformer Layers \n - Utilizing Intermediate Layers  \n - Layer-wise Learning Rate Decay (LLRD)\n - Mixout Regularization  \n - Pre-trained Weight Decay  \n - Stochastic Weight Averaging   \n \n*Note 1: These methods are independent and it's not recommended to use all of them at once. Even though mixing two or more techniques might result in improvement but this may not be always true.*\n<br>\n\n<font color='#3498DB'><h3>Contents</h3></font>\n- [**Debiasing Omission In BertADAM**](#section1)\n  - [Intoduction](#section1a)\n  - [Adam Pseudocode](#section1b)\n  - [Implementation](#section1b)\n  - [Resources and References](#section1b)\n- [**Re-Initializing Transformer Layers**](#section2)\n  - [Intoduction](#section2a)\n  - [Implementation](#section2b)\n    - [Pooler Re-init](#section2a1)\n    - [RoBERTa Re-init](#section2a2)\n    - [XLNet Re-init](#section2a3)\n    - [BART Re-init](#section2a3)\n  - [Sensitivity to number of layers Re-Initialized](#section2c)\n  - [Resources and References](#section2d)\n- [**Utilizing Intermediate Layers**](#section3)\n  - [Intoduction](#section3a)\n  - [Idea](#section3b)\n  - [Implementation](#section3c)\n      - [Weighted Layer Pooling](#section3c1)\n  - [Pooling Strategy and Layer Choice](#section3d)\n  - [Resources and References](#section3e)\n- [**Layer-wise Learning Rate Decay (LLRD)**](#section4)\n  - [Intoduction](#section4a)\n  - [Implementation](#section4b)\n  - [Visualization](#section4c)\n  - [Resources and References](#section4d)\n- [**Mixout Regularization**](#section5)\n  - [Intoduction](#section5a)\n  - [Idea](#section5b)\n  - [Implementation](#section5c)\n  - [Conclusion](#section5d)\n  - [Resources and References](#section5e)\n- [**Pre-trained Weight Decay**](#section6)\n  - [Intoduction](#section6a)\n  - [Implementation](#section6b) \n  - [Resources and References](#section6c)\n- [**Stochastic Weight Averaging**](#section7)\n  - [Intoduction](#section7a)\n  - [Idea](#section7b)\n  - [Implementation](#section7c)\n  - [Resources and References](#section7d)   \n- [**Ending Notes**](#section8)\n\n<font color='#3498DB'><h3>What's New?</h3></font>\n1. [SWA, Apex AMP & Interpreting Transformers in Torch](https://www.kaggle.com/rhtsingh/swa-apex-amp-interpreting-transformers-in-torch) notebook is an implementation of the Stochastic Weight Averaging technique with NVIDIA Apex on transformers using PyTorch. The notebook also implements how to interactively interpret Transformers using LIT (Language Interpretability Tool) a platform for NLP model understanding.   \nIt has in-depth explanations and code implementations for,\n - SWA \n - Apex AMP\n - Weighted Layer Pooling\n - MADGRAD Optimizer\n - Grouped LLRD\n - Language Interpretibility Tool\n    - Attention Visualization\n    - Saliency Maps\n    - Integrated Gradients\n    - LIME \n    - Embedding Space (UMAP & PCA)\n    - Counterfactual generation\n    - And many more ...\n\n\n2. [Utilizing Transformer Representations Efficiently](https://www.kaggle.com/rhtsingh/utilizing-transformer-representations-efficiently) notebook will show many different ways these outputs and hidden representations can be utilized to do much more than just adding an output layer. It has code implementations and detailed explanations for all the below techniques,\n - Pooler Output  \n - Last Hidden State Output  \n    - CLS Embeddings  \n    - Mean Pooling  \n    - Max Pooling  \n    - Mean + Max Pooling  \n    - Conv1D Pooling  \n - Hidden Layers Output  \n    - Layerwise CLS Embeddings  \n    - Concatenate Pooling  \n    - Weighted Layer Pooling  \n    - LSTM / GRU Pooling  \n    - Attention Pooling  \n    - WKPooling  \n \n3. [Speeding up Transformer w/ Optimization Strategies](https://www.kaggle.com/rhtsingh/speeding-up-transformer-w-optimization-strategies) notebook explains in-depth 5 optimization strategies with code. All these techniques are promising and can improve the model performance both in terms of speed and accuracy.\n   - Dynamic Padding and Uniform Length Batching\n   - Gradient Accumulation\n   - Freeze Embedding\n   - Numeric Precision Reduction\n   - Gradient Checkpointing  ","metadata":{}},{"cell_type":"code","source":"import gc\ngc.enable()","metadata":{"execution":{"iopub.status.busy":"2021-06-12T13:38:12.167931Z","iopub.execute_input":"2021-06-12T13:38:12.168335Z","iopub.status.idle":"2021-06-12T13:38:12.176513Z","shell.execute_reply.started":"2021-06-12T13:38:12.168246Z","shell.execute_reply":"2021-06-12T13:38:12.1753Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color='#3498DB'><a id=\"section1\"><h2>Debiasing Omission In BERTAdam</h2></a></font>\n\n<font color='#3498DB'><a id=\"section10\"><h3>Introduction</h3></a></font>\n\nBERTAdam is the most commonly used optimizer for fine-tuning Transformer which is a modified version of the ADAM optimizer.\n\nIt differs from the original ADAM algorithm (Kingma & Ba, 2014) in omitting a bias correction step. This change was introduced in the BERT paper, and subsequently made its way into common open source libraries, including the official implementation HuggingFace Transformers.\n\n<font color='#3498DB'><a id=\"section10\"><h3>The Adam pseudocode</h3></a></font>\n\nRequire: *α*: learning rate; *β1, β2 ∈ [0, 1):* exponential decay rates for the moment estimates; f(θ): stochastic objective function with parameters θ; θ0: initial parameter vector; λ ∈ [0, 1): decoupled weight decay.\n\n01: $m0 ← 0$ (Initialize first moment vector)  \n02: $v0 ← 0 $ (Initialize second moment vector)  \n03: $t ← 0 $ (Initialize timestep)  \n04: **while** *θt* not converged **do** (Initialize timestep)  \n05:   $ t ← t + 1$  \n06:  $ gt ← ∇θft(θt−1)$ (Get gradients w.r.t. stochastic objective at timestep t)  \n07:  $ mt ← β1 · mt−1 + (1 − β1) · gt$ (Update biased first moment estimate)  \n08:  $ vt ← β2 · vt−1 + (1 − β2) · g^2t$ (Update biased second raw moment estimate)  \n<font color='#F1948A'>09: $ mt ← mt/(1 − βt1)$ (Compute bias-corrected first moment estimate)</font>  \n<font color='#F1948A'>10:$ vt ← vt/(1 − βt2)$ (Compute bias-corrected second raw moment estimate)</font>  \n11:$ θt ← θt−1 − α · ~mt/(√~vt + e)$ (Update parameters)  \n12: **end while**  \n13: **return** θt (Resulting parameters)  \n<br>\nAbove shows the ADAM algorithm and highlights the omitted line in the non-standard BERTAdam implementation. Without the bias correction results in degenerate runs and at times for few samples, models fine-tuned fail to outperform the random baseline. \n\nModels trained with BERTAdam on small models result in underfitting and to keep it simple, this correction is crucial for Transformer finetuning on small datasets i.e. with fewer than 10k training samples.\n\n<font color='#3498DB'><a id=\"section10\"><h3>Implementation</h3></a></font>\n\nHere we will implement bias corrected Adam with HuggingFace Transformers library. This is relatively straightforward using HuggingFace AdamW optimizer by setting `correct_bias` parameter to true.\n\n*Note: HuggingFace Transformers AdamW has `correct_bias` parameter set to True by default. Still it's worth noting the importance this parameter serves.*","metadata":{}},{"cell_type":"code","source":"from transformers import (\n    AdamW,\n    AutoConfig,\n    AutoModelForSequenceClassification\n)\nfrom transformers import logging\nlogging.set_verbosity_warning()\nlogging.set_verbosity_error()\n\n_pretrained_model = 'roberta-base'\nlr = 2e-5\nepsilon = 1e-6\nweight_decay = 0.01\nuse_bertadam = False\n\nconfig = AutoConfig.from_pretrained(_pretrained_model)\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    _pretrained_model, \n    config=config\n)\n\nno_decay = [\"bias\", \"LayerNorm.weight\"]\noptimizer_grouped_parameters = [{\n    \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n    \"weight_decay\": weight_decay,\n    \"lr\": lr,\n},\n{\n    \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n    \"weight_decay\": 0.0,\n    \"lr\": lr,\n}]\n\noptimizer = AdamW(\n    optimizer_grouped_parameters,\n    lr=lr,\n    eps=epsilon,\n    correct_bias=not use_bertadam # bias correction step\n)\n\ndel model, optimizer_grouped_parameters, optimizer\ngc.collect();","metadata":{"execution":{"iopub.status.busy":"2021-06-12T13:40:35.873285Z","iopub.execute_input":"2021-06-12T13:40:35.873693Z","iopub.status.idle":"2021-06-12T13:40:40.645685Z","shell.execute_reply.started":"2021-06-12T13:40:35.873661Z","shell.execute_reply":"2021-06-12T13:40:40.644372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color='#3498DB'><a id=\"section112\"><h3>References and Resources</h3></a></font>\n \n - [REVISITING FEW-SAMPLE BERT FINE-TUNING](https://arxiv.org/pdf/2006.05987.pdf)\n - [ON THE STABILITY OF FINE-TUNING BERT: MISCONCEPTIONS, EXPLANATIONS, AND STRONG BASELINES](https://arxiv.org/pdf/2006.04884.pdf)","metadata":{}},{"cell_type":"markdown","source":"<font color='#3498DB'><a id=\"section1\"><h2>Reinitializing Transformer Layers</h2></a></font>\n\n<font color='#3498DB'><a id=\"section10\"><h3>Introduction</h3></a></font>\n\nThis is a very interesting technique where instead of using the pretrained weights for all layers, we re-initialize the pooler layers and the top Transformer blocks using the original Transformer initialization. The layers reinitialized results in destruction of gained pretrained knowledge for those specific blocks.\n\n<font color='#3498DB'><a id=\"section10\"><h3>Idea</h3></a></font>\n\nThe idea is motivated by computer vision transfer learning results where we know that lower pre-trained layers learn more general features while higher layers closer to the output specialize more to the pre-training tasks. \nExisting methods using Transformer show that using the complete network is not always the most effective choice and usually slows down training and hurts performance. \n\n<font color='#3498DB'><a id=\"section10\"><h3>Implementation</h3></a></font>\n\nThe implementation varies for various transformers depending upon the type of Transformer they are (Autoencoding, Autoregressive, etc.). \n\nWe will be implementing pooler reinitialization and block initialization for 3 architectures RoBERTa, XLNet, BART.\n\n<font color='#3498DB'><a id=\"section10\"><h4>Pooler Reinitialization</h4></a></font>\n\n-  We \"pool\" the model by simply taking the hidden state corresponding to the first token.","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import RobertaModel, RobertaConfig\nfrom transformers.models.roberta.modeling_roberta import RobertaClassificationHead\n\n_model_type = 'roberta'\n_pretrained_model = 'roberta-base'\nconfig = RobertaConfig.from_pretrained(_pretrained_model)\nadd_pooler = True\nreinit_pooler = True\n\nclass Net(nn.Module):\n    def __init__(self, config, _pretrained_model, add_pooler):\n        super(Net, self).__init__()\n        self.roberta = RobertaModel.from_pretrained(_pretrained_model, add_pooling_layer=add_pooler)\n        self.classifier = RobertaClassificationHead(config)\n        \n    def forward(self, input_ids, attention_mask):\n        outputs = self.roberta(\n            input_ids,\n            attention_mask=attention_mask,\n        )\n        sequence_output = outputs[0]\n        logits = self.classifier(sequence_output)\n        return logits\n        \nmodel = Net(config, _pretrained_model, add_pooler)\n\nif reinit_pooler:\n    print('Reinitializing Pooler Layer ...')\n    encoder_temp = getattr(model, _model_type)\n    encoder_temp.pooler.dense.weight.data.normal_(mean=0.0, std=encoder_temp.config.initializer_range)\n    encoder_temp.pooler.dense.bias.data.zero_()\n    for p in encoder_temp.pooler.parameters():\n        p.requires_grad = True\n    print('Done.!')\n    \ndel model\ngc.collect();","metadata":{"execution":{"iopub.status.busy":"2021-06-12T13:41:31.527111Z","iopub.execute_input":"2021-06-12T13:41:31.527438Z","iopub.status.idle":"2021-06-12T13:41:36.344213Z","shell.execute_reply.started":"2021-06-12T13:41:31.527412Z","shell.execute_reply":"2021-06-12T13:41:36.343294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color='#3498DB'><a id=\"section10\"><h4>Layer Reinitialization - RoBERTa</h4></a></font>\n\n- RoBERTa builds on BERT and modifies key hyperparameters, removing the next-sentence pretraining objective and training with much larger mini-batches and learning rates.\n\n- RoBERTa has the same architecture as BERT, but uses a byte-level BPE as a tokenizer (same as GPT-2) and uses a different pretraining scheme.\n\n- RoBERTa doesn’t have token_type_ids, you don’t need to indicate which token belongs to which segment. Just separate your segments with the separation token tokenizer.sep_token.\n\n*Note 1: TF version uses truncated_normal for initialization.*\n\n*Note 2: To check wether the weights are being re-initialized, run this block of code before and after re-initialization*\n\n```python\nfor layer in model.roberta.encoder.layer[-reinit_layers:]:\n    for module in layer.modules():\n        if isinstance(module, nn.Linear):\n            print(module.weight.data)\n```","metadata":{}},{"cell_type":"code","source":"from transformers import AutoConfig\nfrom transformers import AutoModelForSequenceClassification\nfrom transformers import logging\nlogging.set_verbosity_warning()\nlogging.set_verbosity_error()\n\nreinit_layers = 2\n_model_type = 'roberta'\n_pretrained_model = 'roberta-base'\nconfig = AutoConfig.from_pretrained(_pretrained_model)\nconfig.update({'num_labels':1})\nmodel = AutoModelForSequenceClassification.from_pretrained(_pretrained_model)\n\nif reinit_layers > 0:\n    print(f'Reinitializing Last {reinit_layers} Layers ...')\n    encoder_temp = getattr(model, _model_type)\n    for layer in encoder_temp.encoder.layer[-reinit_layers:]:\n        for module in layer.modules():\n            if isinstance(module, nn.Linear):\n                module.weight.data.normal_(mean=0.0, std=config.initializer_range)\n                if module.bias is not None:\n                    module.bias.data.zero_()\n            elif isinstance(module, nn.Embedding):\n                module.weight.data.normal_(mean=0.0, std=config.initializer_range)\n                if module.padding_idx is not None:\n                    module.weight.data[module.padding_idx].zero_()\n            elif isinstance(module, nn.LayerNorm):\n                module.bias.data.zero_()\n                module.weight.data.fill_(1.0)\n    print('Done.!')\n\ndel model\ngc.collect();","metadata":{"execution":{"iopub.status.busy":"2021-06-12T13:42:33.102796Z","iopub.execute_input":"2021-06-12T13:42:33.103299Z","iopub.status.idle":"2021-06-12T13:42:40.450672Z","shell.execute_reply.started":"2021-06-12T13:42:33.103271Z","shell.execute_reply":"2021-06-12T13:42:40.449285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color='#3498DB'><a id=\"section10\"><h4>Layer Reinitialization - XLNet</h4></a></font>\n\n- XLNet is one of the few models that has no sequence length limit.\n\n- XLnet is an extension of the Transformer-XL model pre-trained using an autoregressive method to learn bidirectional contexts.\n\n*Note: TF version uses truncated_normal for initialization.*","metadata":{}},{"cell_type":"code","source":"from transformers import AutoConfig\nfrom transformers import AutoModelForSequenceClassification\nfrom transformers import logging\nfrom transformers.models.xlnet.modeling_xlnet import XLNetRelativeAttention\nlogging.set_verbosity_warning()\nlogging.set_verbosity_error()\n\nreinit_layers = 2\n_model_type = 'xlnet'\n_pretrained_model = 'xlnet-base-cased'\nconfig = AutoConfig.from_pretrained(_pretrained_model)\nconfig.update({'num_labels':1})\nmodel = AutoModelForSequenceClassification.from_pretrained(_pretrained_model)\n\nif reinit_layers > 0:\n    print(f'Reinitializing Last {reinit_layers} Layers ...')\n    for layer in model.transformer.layer[-reinit_layers :]:\n        for module in layer.modules():\n            if isinstance(module, (nn.Linear, nn.Embedding)):\n                module.weight.data.normal_(mean=0.0, std=model.transformer.config.initializer_range)\n                if isinstance(module, nn.Linear) and module.bias is not None:\n                    module.bias.data.zero_()\n            elif isinstance(module, nn.LayerNorm):\n                module.bias.data.zero_()\n                module.weight.data.fill_(1.0)\n            elif isinstance(module, XLNetRelativeAttention):\n                for param in [\n                    module.q,\n                    module.k,\n                    module.v,\n                    module.o,\n                    module.r,\n                    module.r_r_bias,\n                    module.r_s_bias,\n                    module.r_w_bias,\n                    module.seg_embed,\n                ]:\n                    param.data.normal_(mean=0.0, std=model.transformer.config.initializer_range)\n    print('Done.!')\n    \ndel model\ngc.collect();","metadata":{"execution":{"iopub.status.busy":"2021-06-12T13:42:48.517034Z","iopub.execute_input":"2021-06-12T13:42:48.517386Z","iopub.status.idle":"2021-06-12T13:43:03.249568Z","shell.execute_reply.started":"2021-06-12T13:42:48.517349Z","shell.execute_reply":"2021-06-12T13:43:03.248219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color='#3498DB'><a id=\"section10\"><h4>Layer Reinitialization - BART</h4></a></font>\n\n - Bart uses a standard seq2seq/machine translation architecture with a bidirectional encoder (like BERT) and a left-to-right decoder (like GPT).\n\n - The pretraining task involves randomly shuffling the order of the original sentences and a novel in-filling scheme, where spans of text are replaced with a single mask token.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoConfig\nfrom transformers import AutoModelForSequenceClassification\nfrom transformers import logging\nfrom transformers.models.xlnet.modeling_xlnet import XLNetRelativeAttention\nlogging.set_verbosity_warning()\nlogging.set_verbosity_error()\n\nreinit_layers = 2\n_model_type = 'bart'\n_pretrained_model = 'facebook/bart-base'\nconfig = AutoConfig.from_pretrained(_pretrained_model)\nconfig.update({'num_labels':1})\nmodel = AutoModelForSequenceClassification.from_pretrained(_pretrained_model)\n\nif reinit_layers > 0:\n    print(f'Reinitializing Last {reinit_layers} Layers ...')\n    for layer in model.model.decoder.layers[-reinit_layers :]:\n        for module in layer.modules():\n            model.model._init_weights(module)\n    print('Done.!')\n\ndel model\ngc.collect();","metadata":{"execution":{"iopub.status.busy":"2021-06-12T08:48:00.631935Z","iopub.execute_input":"2021-06-12T08:48:00.63232Z","iopub.status.idle":"2021-06-12T08:48:07.77923Z","shell.execute_reply.started":"2021-06-12T08:48:00.632289Z","shell.execute_reply":"2021-06-12T08:48:07.778167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color='#3498DB'><a id=\"section10\"><h4>Sensitivity to Number of Layers Re-initialized </h4></a></font>\n\nExperiments show that Re-init is more robust to unfavorable random seed. Improvements is seen when only the pooler layer is re-initialized. Re-initializing further layers helps more. \n\nHowever it is not suggested to reinit more than top 6 layers as the performance plateaus and even decreases as futher re-initialization destroys pre-trained layers with general important features. The best number of reinit layers varies across datasets.\n\n<font color='#3498DB'><a id=\"section112\"><h3>References and Resources</h3></a></font>\n \n - [Investigating Transferability in Pretrained Language Models](https://arxiv.org/pdf/2004.14975.pdf)\n - [REVISITING FEW-SAMPLE BERT FINE-TUNING](https://arxiv.org/pdf/2006.05987.pdf)\n - [RIFLE: Backpropagation in Depth for Deep Transfer Learning through\nRe-Initializing the Fully-connected LayEr](https://arxiv.org/pdf/2007.03349.pdf) \n - [Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping](https://arxiv.org/pdf/2002.06305.pdf)","metadata":{}},{"cell_type":"markdown","source":"<font color='#3498DB'><a id=\"section1\"><h2>Utilizing Intermediate Layers</h2></a></font>\n\n<font color='#3498DB'><a id=\"section1\"><h3>Introduction</h3></a></font>\nThis is one of the best techniques that has been widely studied using probing methods which shows that the pre-trained features from intermediate layers are more transferable. \n\nIn HuggingFace Transformers there are 2 main outputs and 3 if configured; that we receive after giving `input_ids` and `attention_mask` as input.\n\n - **last hidden state** (batch size, seq Len, hidden size) which is the sequence of hidden states at the output of the last layer.\n - **pooler output** (batch size, hidden size) - Last layer hidden-state of the first token of the sequence\n - **all hidden states** (n layers, batch size, seq Len, hidden size) - Hidden states for all layers and for all ids.\n \n \n<font color='#3498DB'><a id=\"section1\"><h3>Idea</h3></a></font>\nAs we have discussed before in reinitialization section, the output of the last layer may not always be the best representation of the input text during the fine-tuning for downstream\ntasks. \n\nFor pre-trained language models, including Transformer, the most transferable contextualized representations of input text tend to occur in the middle layers, while the top layers specialize for language modeling. Therefore, the onefold use of the last layer’s output may restrict the power of the pre-trained representation.\n\n<font color='#3498DB'><a id=\"section1\"><h3>Implementation</h3></a></font>\nWe have multiple application-dependent strategies for fetching intermediate representations and not all of them can be shared in this notebook. But, I will do share here the most useful one and which helps in improvement for almost any type of problem. \n\n**WeightedLayerPooling** - Token embeddings are the weighted mean of their different hidden layer representations.","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport pandas as pd\nfrom transformers import (\n    AutoConfig, \n    AutoModel, \n    AutoTokenizer\n)\n\n_pretrained_model = 'roberta-base'\nbatch_size = 16\nmax_seq_length = 256\n\ntrain = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\ntexts = train['excerpt'][:batch_size].tolist()\n\nconfig = AutoConfig.from_pretrained(_pretrained_model)\n# configure to output all hidden states as well\nconfig.update({'output_hidden_states':True}) \nmodel = AutoModel.from_pretrained(_pretrained_model, config=config)\ntokenizer = AutoTokenizer.from_pretrained(_pretrained_model)\n\nfeatures = tokenizer.batch_encode_plus( \n    texts, \n    max_length=max_seq_length,\n    padding='max_length', \n    truncation=True, \n    add_special_tokens=True,\n    return_attention_mask=True, \n    return_tensors='pt'\n)\nprint(features['input_ids'].shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T11:45:29.268299Z","iopub.execute_input":"2021-06-12T11:45:29.268598Z","iopub.status.idle":"2021-06-12T11:45:38.690198Z","shell.execute_reply.started":"2021-06-12T11:45:29.268575Z","shell.execute_reply":"2021-06-12T11:45:38.68641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outputs = model(features['input_ids'], attention_mask=features['attention_mask'])\nprint(\"Total number of outputs: \", len(outputs))\nprint('Shape of 1st output', outputs[0].shape)\nprint('Shape of 2nd output', outputs[1].shape)\nprint('Length of 3rd output', len(outputs[2]))","metadata":{"execution":{"iopub.status.busy":"2021-06-12T11:45:40.369345Z","iopub.execute_input":"2021-06-12T11:45:40.369797Z","iopub.status.idle":"2021-06-12T11:45:59.016683Z","shell.execute_reply.started":"2021-06-12T11:45:40.369767Z","shell.execute_reply":"2021-06-12T11:45:59.015753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We can see after setting `output_hidden_states` to `True` that we now receive three different outputs. \n\n- We have 13 hidden layers outputs despite 12 hidden layers in the model because we also receive outputs for the embedding layers.","metadata":{}},{"cell_type":"code","source":"class WeightedLayerPooling(nn.Module):\n    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights = None):\n        super(WeightedLayerPooling, self).__init__()\n        self.layer_start = layer_start\n        self.num_hidden_layers = num_hidden_layers\n        self.layer_weights = layer_weights if layer_weights is not None \\\n            else nn.Parameter(\n                torch.tensor([1] * (num_hidden_layers+1 - layer_start), dtype=torch.float)\n            )\n\n    def forward(self, features):\n        ft_all_layers = features['all_layer_embeddings']\n\n        all_layer_embedding = torch.stack(ft_all_layers)\n        all_layer_embedding = all_layer_embedding[self.layer_start:, :, :, :]\n\n        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n\n        features.update({'token_embeddings': weighted_average})\n        return features","metadata":{"execution":{"iopub.status.busy":"2021-06-12T12:04:07.637215Z","iopub.execute_input":"2021-06-12T12:04:07.637734Z","iopub.status.idle":"2021-06-12T12:04:07.652081Z","shell.execute_reply.started":"2021-06-12T12:04:07.637708Z","shell.execute_reply":"2021-06-12T12:04:07.650875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we will add our hidden layers outputs to features with key `all_layer_embeddings` for convenience and pass that to WeightedLayerPooling operation. We will be using hidden states from our last 4 hidden layers. We will add the weighted layer pooling outputs to our features dict with key - `token_embeddings`.","metadata":{}},{"cell_type":"code","source":"layer_start = 9\npooler = WeightedLayerPooling(\n    config.num_hidden_layers, \n    layer_start=layer_start, layer_weights=None\n)\nfeatures.update({'all_layer_embeddings':outputs[2]})\nfeatures = pooler(features)\nprint(\"Weighted Layer Pooling Embeddings Shape: \", features['token_embeddings'].shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T11:58:10.148241Z","iopub.execute_input":"2021-06-12T11:58:10.148704Z","iopub.status.idle":"2021-06-12T11:58:10.249806Z","shell.execute_reply.started":"2021-06-12T11:58:10.148678Z","shell.execute_reply":"2021-06-12T11:58:10.2488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we have a combined final representation of last four layers. We can now simply take the cls token outputs, concatenate. \nThe standard pooling operation as implemented in HuggingFace Transformer for BERT, RoBERTa etc. can also be appled here. Below we simply take the `cls` token outputs and pass it from a Linear layer.","metadata":{}},{"cell_type":"code","source":"sequence_output = features['token_embeddings'][:, 0]\noutputs = nn.Linear(config.hidden_size, 1)(sequence_output)\nprint(\"Outputs Shape: \", outputs.shape)\n\ndel model, tokenizer\ngc.collect();","metadata":{"execution":{"iopub.status.busy":"2021-06-12T12:06:55.137918Z","iopub.execute_input":"2021-06-12T12:06:55.138461Z","iopub.status.idle":"2021-06-12T12:06:55.145685Z","shell.execute_reply.started":"2021-06-12T12:06:55.13843Z","shell.execute_reply":"2021-06-12T12:06:55.144219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color='#3498DB'><a id=\"section1\"><h3>Pooling Strategy and Layer Choice</h3></a></font>\n\nThe BERT authors tested word-embedding strategies by feeding different vector combinations as input features to a BiLSTM used on a named entity recognition task and observing the resulting F1 scores.\n\nThis is partially demonstrated by noting that the different layers of BERT encode very different kinds of information, so the appropriate pooling strategy will change depending on the application because different layers encode different kinds of information.\n\n![embedding_layers](http://jalammar.github.io/images/bert-feature-extraction-contextualized-embeddings.png)\n\nHan Xiao created an open-source project named bert-as-service on GitHub which is intended to create word embeddings for your text using BERT `bert-as-service`, by default, uses the outputs from the second-to-last layer of the model.\n\nHis observations are - \n - The embeddings start in the first layer as having no contextual information.\n - As the embeddings move deeper into the network, they pick up more and more contextual information with each layer.\n - As you approach the final layer, however, you start picking up information that is specific to BERT’s pre-training tasks (the “Masked Language Model” (MLM) and “Next Sentence Prediction” (NSP)).\n    - What we want are embeddings that encode the word meaning well…\n    - BERT is motivated to do this, but it is also motivated to encode anything else that would help it determine what a missing word is (MLM), or whether the second sentence came after the first (NSP).\n - The second-to-last layer is what Han settled on as a reasonable sweet spot.","metadata":{}},{"cell_type":"markdown","source":"<font color='#3498DB'><a id=\"section112\"><h3>References and Resources</h3></a></font>\n \n - [How to Fine-Tune BERT for Text Classification?](https://arxiv.org/pdf/1905.05583.pdf)\n - [Deepening Hidden Representations from Pre-trained Language Models](https://arxiv.org/pdf/1911.01940.pdf)\n - [WHAT DO YOU LEARN FROM CONTEXT? PROBING FOR SENTENCE STRUCTURE IN CONTEXTUALIZED WORD REPRESENTATIONS](https://openreview.net/pdf?id=SJzSgnRcKX)\n - [Linguistic Knowledge and Transferability of Contextual Representations](https://www.aclweb.org/anthology/N19-1112.pdf)\n - [BERT Word Embeddings Tutorial](https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/)\n - [Sentence Transformers: Multilingual Sentence, Paragraph, and Image Embeddings using BERT](https://github.com/UKPLab/sentence-transformers)","metadata":{}},{"cell_type":"markdown","source":"<font color='#3498DB'><a id=\"section1\"><h2>LLRD - Layerwise Learning Rate Decay</h2></a>\n\n<font color='#3498DB'><a id=\"section1\"><h3>Introduction</h3></a></font>\n    \nLLRD  is a method that applies higher learning rates for top layers and lower learning rates for bottom layers. This is accomplished by setting the learning rate of the top layer and using a multiplicative decay rate to decrease the learning rate layer-by-layer from top to bottom. \n    \nThe goal is to modify the lower layers that encode more general information less than the top layers that are more specific to the pre-training task. This method is adopted in fine-tuning several recent pre-trained models, including XLNet and ELECTRA.\n\n<font color='#3498DB'><a id=\"section1\"><h3>Implementation</h3></a></font> \n    \n[Guide to HuggingFace Schedulers & Differential LRs](https://www.kaggle.com/rhtsingh/guide-to-huggingface-schedulers-differential-lrs) notebook introduces various differential learning rate strategies but not this one. We will implement official LLRD here and visualize how learning rate changes for various layers. \n    \nFirst we import our necessary modules, define model params, optimizer params, scheduler params then, create model and config.","metadata":{}},{"cell_type":"code","source":"from transformers import (\n    AdamW, \n    AutoConfig, \n    AutoModelForSequenceClassification,\n    get_cosine_schedule_with_warmup,\n    get_linear_schedule_with_warmup\n)\nfrom transformers import logging\nlogging.set_verbosity_warning()\nlogging.set_verbosity_error()\n\n_model_type = 'roberta'\n_pretrained_model = 'roberta-base'\n# optimizer params\nlearning_rate = 5e-5\nlayerwise_learning_rate_decay = 0.9\nweight_decay = 0.01\nadam_epsilon = 1e-6\nuse_bertadam = False\n# scheduler params\nnum_epochs = 20\nnum_warmup_steps = 0\n\nconfig = AutoConfig.from_pretrained(_pretrained_model)\nconfig.update({'num_labels':1})\nmodel = AutoModelForSequenceClassification.from_pretrained(_pretrained_model)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T06:32:08.427462Z","iopub.execute_input":"2021-06-13T06:32:08.42784Z","iopub.status.idle":"2021-06-13T06:32:15.214417Z","shell.execute_reply.started":"2021-06-13T06:32:08.427804Z","shell.execute_reply":"2021-06-13T06:32:15.213422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The below is our LLRD function, we will first initialize our task specific head. Then we multiply our `learning rate` with `layerwise learning rate decay` and assign it to each transformer block. \n\nAs we will see the top layers closer to task-specific head have higher learning rate than the bottom ones.","metadata":{}},{"cell_type":"code","source":"def get_optimizer_grouped_parameters(\n    model, model_type, \n    learning_rate, weight_decay, \n    layerwise_learning_rate_decay\n):\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    # initialize lr for task specific layer\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if \"classifier\" in n or \"pooler\" in n],\n            \"weight_decay\": 0.0,\n            \"lr\": learning_rate,\n        },\n    ]\n    # initialize lrs for every layer\n    num_layers = model.config.num_hidden_layers\n    layers = [getattr(model, model_type).embeddings] + list(getattr(model, model_type).encoder.layer)\n    layers.reverse()\n    lr = learning_rate\n    for layer in layers:\n        lr *= layerwise_learning_rate_decay\n        optimizer_grouped_parameters += [\n            {\n                \"params\": [p for n, p in layer.named_parameters() if not any(nd in n for nd in no_decay)],\n                \"weight_decay\": weight_decay,\n                \"lr\": lr,\n            },\n            {\n                \"params\": [p for n, p in layer.named_parameters() if any(nd in n for nd in no_decay)],\n                \"weight_decay\": 0.0,\n                \"lr\": lr,\n            },\n        ]\n    return optimizer_grouped_parameters","metadata":{"execution":{"iopub.status.busy":"2021-06-13T06:32:15.216249Z","iopub.execute_input":"2021-06-13T06:32:15.21664Z","iopub.status.idle":"2021-06-13T06:32:15.225439Z","shell.execute_reply.started":"2021-06-13T06:32:15.216596Z","shell.execute_reply":"2021-06-13T06:32:15.224436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We create our grouped parameters, initialize our optimizer and scheduler.","metadata":{}},{"cell_type":"code","source":"grouped_optimizer_params = get_optimizer_grouped_parameters(\n    model, _model_type, \n    learning_rate, weight_decay, \n    layerwise_learning_rate_decay\n)\noptimizer = AdamW(\n    grouped_optimizer_params,\n    lr=learning_rate,\n    eps=adam_epsilon,\n    correct_bias=not use_bertadam\n)\nscheduler = get_cosine_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=num_warmup_steps,\n    num_training_steps=num_epochs\n)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T06:32:15.227086Z","iopub.execute_input":"2021-06-13T06:32:15.227422Z","iopub.status.idle":"2021-06-13T06:32:15.250732Z","shell.execute_reply.started":"2021-06-13T06:32:15.227385Z","shell.execute_reply":"2021-06-13T06:32:15.249617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color='#3498DB'><a id=\"section1\"><h3>Visualization</h3></a></font> \nWe will now perform `optimizer.step()` and `scheduler.step()` like any other normal training and collect our learning rate for each layer in each epoch. Then we will visualize the learning rates.\n\n*Note: The visualization has been done using plotly and has been hidden.*","metadata":{}},{"cell_type":"code","source":"(learning_rates1, learning_rates2, learning_rates3, learning_rates4,\nlearning_rates5, learning_rates6, learning_rates7, learning_rates8,\nlearning_rates9, learning_rates10, learning_rates11, learning_rates12, \nlearning_rates13, learning_rates14) = [[] for i in range(14)]\n\ndef collect_lr(optimizer):\n    learning_rates1.append(optimizer.param_groups[0][\"lr\"])\n    learning_rates2.append(optimizer.param_groups[2][\"lr\"])\n    learning_rates3.append(optimizer.param_groups[4][\"lr\"])\n    learning_rates4.append(optimizer.param_groups[6][\"lr\"])\n    learning_rates5.append(optimizer.param_groups[8][\"lr\"])\n    learning_rates6.append(optimizer.param_groups[10][\"lr\"])\n    learning_rates7.append(optimizer.param_groups[12][\"lr\"])\n    learning_rates8.append(optimizer.param_groups[14][\"lr\"])\n    learning_rates9.append(optimizer.param_groups[16][\"lr\"])\n    learning_rates10.append(optimizer.param_groups[18][\"lr\"])\n    learning_rates11.append(optimizer.param_groups[20][\"lr\"])\n    learning_rates12.append(optimizer.param_groups[22][\"lr\"])\n    learning_rates13.append(optimizer.param_groups[24][\"lr\"])\n    learning_rates14.append(optimizer.param_groups[26][\"lr\"])\n\ncollect_lr(optimizer)\nfor epoch in range(num_epochs):\n    optimizer.step()\n    scheduler.step()\n    collect_lr(optimizer)","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-06-13T06:32:15.25267Z","iopub.execute_input":"2021-06-13T06:32:15.253134Z","iopub.status.idle":"2021-06-13T06:32:15.279112Z","shell.execute_reply.started":"2021-06-13T06:32:15.253091Z","shell.execute_reply":"2021-06-13T06:32:15.278214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly\nimport plotly.graph_objs as go\nimport plotly.express as px\nimport plotly.io as pio\nimport plotly.offline as pyo\npio.templates.default='plotly_white'\n\ndef get_default_layout(title):\n    font_style = 'Courier New'\n    layout = {}\n    #layout['height'] = 400\n    #layout['width'] = 1200\n    layout['template'] = 'plotly_white'\n    layout['dragmode'] = 'zoom'\n    layout['hovermode'] = 'x'\n    layout['hoverlabel'] = {\n        'font_size': 14,\n        'font_family':font_style\n    }\n    layout['font'] = {\n        'size':14,\n        'family':font_style,\n        'color':'rgb(128, 128, 128)'\n    }\n    layout['xaxis'] = {\n        'title': 'Epochs',\n        'showgrid': True,\n        'type': 'linear',\n        'categoryarray': None,\n        'gridwidth': 1,\n        'ticks': 'outside',\n        'showline': True, \n        'showticklabels': True,\n        'tickangle': 0,\n        'tickmode': 'array'\n    }\n    layout['yaxis'] = {\n        'title': 'Learning Rate',\n        'exponentformat':'none',\n        'showgrid': True,\n        'type': 'linear',\n        'categoryarray': None,\n        'gridwidth': 1,\n        'ticks': 'outside',\n        'showline': True, \n        'showticklabels': True,\n        'tickangle': 0,\n        'tickmode': 'array'\n    }\n    layout['title'] = {\n        'text':title,\n        'x': 0.5,\n        'y': 0.95,\n        'xanchor': 'center',\n        'yanchor': 'top',\n        'font': {\n            'family':font_style,\n            'size':14,\n            'color':'black'\n        }\n    }\n    layout['showlegend'] = True\n    layout['legend'] = {\n        'x':0.1,\n        'y':1.1,\n        'orientation':'h',\n        'itemclick': 'toggleothers',\n        'font': {\n            'family':font_style,\n            'size':14,\n            'color':'black'\n        }\n    }\n    return go.Layout(layout)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-13T06:32:15.280487Z","iopub.execute_input":"2021-06-13T06:32:15.280782Z","iopub.status.idle":"2021-06-13T06:32:15.325439Z","shell.execute_reply.started":"2021-06-13T06:32:15.280754Z","shell.execute_reply":"2021-06-13T06:32:15.324475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_trace(learning_rates, num_epochs, name, color):\n    return go.Scatter(\n        x=list(range(0, num_epochs, 1)), \n        y=learning_rates, \n        texttemplate=\"%{y:.6f}\",\n        mode='markers+lines',\n        name=name,\n        marker=dict(color=color),\n    )\n\ntrace1 = build_trace(learning_rates1, num_epochs, name='Regressor', color='#83c8d2')\ntrace2 = build_trace(learning_rates2, num_epochs, name='Layer 12', color='#82c9d2')\ntrace3 = build_trace(learning_rates3, num_epochs, name='Layer 11', color='#85c7cf')\ntrace4 = build_trace(learning_rates4, num_epochs, name='Layer 10', color='#88c4cc')\ntrace5 = build_trace(learning_rates5, num_epochs, name='Layer 9', color='#8cc1c8')\ntrace6 = build_trace(learning_rates6, num_epochs, name='Layer 8', color='#8fbfc5')\ntrace7 = build_trace(learning_rates7, num_epochs, name='Layer 7', color='#92bcc2')\ntrace8 = build_trace(learning_rates8, num_epochs, name='Layer 6', color='#96babe')\ntrace9 = build_trace(learning_rates9, num_epochs, name='Layer 5', color='#99b7bb')\ntrace10 = build_trace(learning_rates10, num_epochs, name='Layer 4', color='#9cb4b8')\ntrace11 = build_trace(learning_rates11, num_epochs, name='Layer 3', color='#a0b2b4')\ntrace12 = build_trace(learning_rates12, num_epochs, name='Layer 2', color='#a3afb1')\ntrace13 = build_trace(learning_rates13, num_epochs, name='Layer 1', color='#a7adad')\ntrace14 = build_trace(learning_rates14, num_epochs, name='Embeddings', color='#aaa')\n\nlayout=get_default_layout('Layer Wise Learning Rate Decay')\nfig = go.Figure(\n    data=[\n        trace1, trace2, trace3, trace4, trace5, trace6, \n        trace7, trace8, trace9, trace10, trace11, trace12, \n        trace13, trace14\n    ], \n    layout=layout.update({'showlegend':False})\n)\n\nfig.show()\n\ndel model, grouped_optimizer_params, optimizer, scheduler\ngc.collect();","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-13T06:32:15.326527Z","iopub.execute_input":"2021-06-13T06:32:15.326789Z","iopub.status.idle":"2021-06-13T06:32:15.414946Z","shell.execute_reply.started":"2021-06-13T06:32:15.326761Z","shell.execute_reply":"2021-06-13T06:32:15.413591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color='#3498DB'><a id=\"section112\"><h3>References and Resources</h3></a></font>\n \n - [Universal language model fine-tuning for text classification](https://arxiv.org/pdf/1801.06146.pdf)\n - [Xlnet: Generalized autoregressive pretraining for language understanding](https://arxiv.org/pdf/1906.08237.pdf)\n - [ELECTRA: PRE-TRAINING TEXT ENCODERS AS DISCRIMINATORS RATHER THAN GENERATORS](https://arxiv.org/pdf/2003.10555.pdf)\n","metadata":{}},{"cell_type":"markdown","source":"<font color='#3498DB'><a id=\"section1\"><h2>Mixout Regularization</h2></a>\n\n<font color='#3498DB'><a id=\"section112\"><h3>Introduction</h3></a></font>\n    \nMixout is a stochastic regularization technique motivated by Dropout and DropConnect. At each training iteration, each model parameter is replaced with its pre-trained value with probability p. The goal is to prevent catastrophic forgetting, and proves it constrains the fine-tuned model from deviating too much from the pre-trained initialization.\n    \n<font color='#3498DB'><a id=\"section112\"><h3>Idea</h3></a></font>\n\n![mixout](https://d3i71xaburhd42.cloudfront.net/7fb48d00f44771e061c34d9e83415487cf538110/2-Figure1-1.png)\n\n<font color='#000000'>Suppose that `u` is target model parameter and `w` is current model parameter. \n- We first memorize the parameters of the vanilla network at u. \n- In the dropout network, we randomly choose an input neuron to be dropped (a dotted\nneuron) with a probability of p. That is, all outgoing parameters from the dropped neuron are\neliminated (dotted connections). \n- In the mixout(u) network, the eliminated parameters in (b)\nare replaced by the corresponding parameters in (a). In other words, the mixout(u) network at w is the mixture of the vanilla network at u and the dropout network at w with a probability of p.</font>\n    \n<font color='#3498DB'><a id=\"section112\"><h3>Implementation</h3></a></font>\n    \nHere we will implement Mixout. The code has been taken from https://github.com/bloodwass/mixout","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.init as init\nimport torch.nn.functional as F\nfrom torch.nn import Parameter\nfrom torch.autograd.function import InplaceFunction\n\nclass Mixout(InplaceFunction):\n    @staticmethod\n    def _make_noise(input):\n        return input.new().resize_as_(input)\n\n    @classmethod\n    def forward(cls, ctx, input, target=None, p=0.0, training=False, inplace=False):\n        if p < 0 or p > 1:\n            raise ValueError(\"A mix probability of mixout has to be between 0 and 1,\" \" but got {}\".format(p))\n        if target is not None and input.size() != target.size():\n            raise ValueError(\n                \"A target tensor size must match with a input tensor size {},\"\n                \" but got {}\".format(input.size(), target.size())\n            )\n        ctx.p = p\n        ctx.training = training\n\n        if ctx.p == 0 or not ctx.training:\n            return input\n\n        if target is None:\n            target = cls._make_noise(input)\n            target.fill_(0)\n        target = target.to(input.device)\n\n        if inplace:\n            ctx.mark_dirty(input)\n            output = input\n        else:\n            output = input.clone()\n\n        ctx.noise = cls._make_noise(input)\n        if len(ctx.noise.size()) == 1:\n            ctx.noise.bernoulli_(1 - ctx.p)\n        else:\n            ctx.noise[0].bernoulli_(1 - ctx.p)\n            ctx.noise = ctx.noise[0].repeat(input.size()[0], 1)\n        ctx.noise.expand_as(input)\n\n        if ctx.p == 1:\n            output = target\n        else:\n            output = ((1 - ctx.noise) * target + ctx.noise * output - ctx.p * target) / (1 - ctx.p)\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        if ctx.p > 0 and ctx.training:\n            return grad_output * ctx.noise, None, None, None, None\n        else:\n            return grad_output, None, None, None, None\n\n\ndef mixout(input, target=None, p=0.0, training=False, inplace=False):\n    return Mixout.apply(input, target, p, training, inplace)\n\n\nclass MixLinear(torch.nn.Module):\n    __constants__ = [\"bias\", \"in_features\", \"out_features\"]\n    def __init__(self, in_features, out_features, bias=True, target=None, p=0.0):\n        super(MixLinear, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = Parameter(torch.Tensor(out_features, in_features))\n        if bias:\n            self.bias = Parameter(torch.Tensor(out_features))\n        else:\n            self.register_parameter(\"bias\", None)\n        self.reset_parameters()\n        self.target = target\n        self.p = p\n\n    def reset_parameters(self):\n        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        if self.bias is not None:\n            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n            bound = 1 / math.sqrt(fan_in)\n            init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, input):\n        return F.linear(input, mixout(self.weight, self.target, self.p, self.training), self.bias)\n\n    def extra_repr(self):\n        type = \"drop\" if self.target is None else \"mix\"\n        return \"{}={}, in_features={}, out_features={}, bias={}\".format(\n            type + \"out\", self.p, self.in_features, self.out_features, self.bias is not None\n        )","metadata":{"execution":{"iopub.status.busy":"2021-06-12T22:30:05.572354Z","iopub.execute_input":"2021-06-12T22:30:05.572942Z","iopub.status.idle":"2021-06-12T22:30:05.605797Z","shell.execute_reply.started":"2021-06-12T22:30:05.572888Z","shell.execute_reply":"2021-06-12T22:30:05.605069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Above we have defined Mixout Regularization. Now we will be adding this to our model.","metadata":{}},{"cell_type":"code","source":"import math\nfrom transformers import AutoModelForSequenceClassification, AutoConfig\nfrom transformers import logging\nlogging.set_verbosity_warning()\nlogging.set_verbosity_error()\n\n_pretrained_model = 'roberta-base'\nmixout = 0.7\n\nconfig = AutoConfig.from_pretrained(_pretrained_model)\nconfig.update({'num_labels':1})\nmodel = AutoModelForSequenceClassification.from_pretrained(_pretrained_model)\n\nif mixout > 0:\n    print('Initializing Mixout Regularization')\n    for sup_module in model.modules():\n        for name, module in sup_module.named_children():\n            if isinstance(module, nn.Dropout):\n                module.p = 0.0\n            if isinstance(module, nn.Linear):\n                target_state_dict = module.state_dict()\n                bias = True if module.bias is not None else False\n                new_module = MixLinear(\n                    module.in_features, module.out_features, bias, target_state_dict[\"weight\"], mixout\n                )\n                new_module.load_state_dict(target_state_dict)\n                setattr(sup_module, name, new_module)\n    print('Done.!')\n\ndel model\ngc.collect();","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And we're done, this can now be used for downstream fine-tuning tasks and Mixout will do its work.\n\n<font color='#3498DB'><a id=\"section112\"><h3>Conclusions</h3></a></font>\nMixout is an adaptive L2-regularizer toward  optimization trajectory in the sense that its regularization coefficient adapts along the optimization path. Mixout improves the stability of finetuning a big, pretrained language model even with only a few training examples of a target task. This is well known technique for improving stability in Transformer finetuning.\n\n<font color='#3498DB'><a id=\"section112\"><h3>References and Resources</h3></a></font>\n \n - [MIXOUT: EFFECTIVE REGULARIZATION TO FINETUNE\nLARGE-SCALE PRETRAINED LANGUAGE MODELS](https://arxiv.org/pdf/1909.11299.pdf)\n - [MIXOUT Code Implementation](https://github.com/bloodwass/mixout)","metadata":{}},{"cell_type":"markdown","source":"<font color='#3498DB'><a id=\"section1\"><h2>Pre-trained Weight Decay</h2></a></font>\n\n<font color='#3498DB'><a id=\"section112\"><h3>Introduction</h3></a></font>\nWeight decay (WD) is a common regularization technique. At each optimization iteration, λw is subtracted from the model parameters, where λ is a hyperparameter for the regularization strength and w is the model parameters. Pre-trained weight decay adapts this method for fine-tuning pre-trained models by subtracting λ(w − wˆ ) from the objective, where wˆ is the pre-trained parameters. The Mixout paper has shown Pre-trained weight decay works better than conventional weight decay in Transformer fine-tuning and can stabilize fine-tuning. \n\n<font color='#3498DB'><a id=\"section112\"><h3>Implementation</h3></a></font>\nHere we will be implementing the pretrained weight decay.","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.optim import Optimizer\nfrom transformers import (\n    AdamW, \n    AutoConfig, \n    AutoModelForSequenceClassification,\n    get_cosine_schedule_with_warmup,\n    get_linear_schedule_with_warmup\n)\nfrom transformers import logging\nlogging.set_verbosity_warning()\nlogging.set_verbosity_error()\n\n_model_type = 'roberta'\n_pretrained_model = 'roberta-base'\n\n# optimizer params\nlearning_rate = 5e-5\nweight_decay = 0.01\nadam_epsilon = 1e-6\nuse_bertadam = False\nuse_prior_wd = True\n\nconfig = AutoConfig.from_pretrained(_pretrained_model)\nconfig.update({'num_labels':1})\nmodel = AutoModelForSequenceClassification.from_pretrained(_pretrained_model)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T08:18:51.292306Z","iopub.execute_input":"2021-06-13T08:18:51.292571Z","iopub.status.idle":"2021-06-13T08:18:57.577444Z","shell.execute_reply.started":"2021-06-13T08:18:51.292548Z","shell.execute_reply":"2021-06-13T08:18:57.576419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Below is our code for wdecay, the code is pretty intuitive to understand and does exactly what we described above. This will be a wrapper around our optimizer.","metadata":{}},{"cell_type":"code","source":"class PriorWD(Optimizer):\n    def __init__(self, optim, use_prior_wd=False, exclude_last_group=True):\n        super(PriorWD, self).__init__(optim.param_groups, optim.defaults)\n        self.param_groups = optim.param_groups\n        self.optim = optim\n        self.use_prior_wd = use_prior_wd\n        self.exclude_last_group = exclude_last_group\n        self.weight_decay_by_group = []\n        for i, group in enumerate(self.param_groups):\n            self.weight_decay_by_group.append(group[\"weight_decay\"])\n            group[\"weight_decay\"] = 0\n\n        self.prior_params = {}\n        for i, group in enumerate(self.param_groups):\n            for p in group[\"params\"]:\n                self.prior_params[id(p)] = p.detach().clone()\n\n    def step(self, closure=None):\n        if self.use_prior_wd:\n            for i, group in enumerate(self.param_groups):\n                for p in group[\"params\"]:\n                    if self.exclude_last_group and i == len(self.param_groups):\n                        p.data.add_(-group[\"lr\"] * self.weight_decay_by_group[i], p.data)\n                    else:\n                        p.data.add_(\n                            -group[\"lr\"] * self.weight_decay_by_group[i], p.data - self.prior_params[id(p)],\n                        )\n        loss = self.optim.step(closure)\n\n        return loss\n\n    def compute_distance_to_prior(self, param):\n        assert id(param) in self.prior_params, \"parameter not in PriorWD optimizer\"\n        return (param.data - self.prior_params[id(param)]).pow(2).sum().sqrt()","metadata":{"execution":{"iopub.status.busy":"2021-06-13T08:18:57.579344Z","iopub.execute_input":"2021-06-13T08:18:57.579609Z","iopub.status.idle":"2021-06-13T08:18:57.593606Z","shell.execute_reply.started":"2021-06-13T08:18:57.579582Z","shell.execute_reply":"2021-06-13T08:18:57.592351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we create our optimizer with simple grouped param intialization and optimizer params as defined above.","metadata":{}},{"cell_type":"code","source":"def get_optimizer_grouped_parameters(model, learning_rate, weight_decay):\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n            \"weight_decay\": weight_decay,\n            \"lr\": learning_rate,\n        },\n        {\n            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n            \"weight_decay\": 0.0,\n            \"lr\": learning_rate,\n        },\n    ]\n    return optimizer_grouped_parameters\n\noptimizer_grouped_parameters = get_optimizer_grouped_parameters(model, learning_rate, weight_decay)\noptimizer = AdamW(\n    optimizer_grouped_parameters,\n    lr=learning_rate,\n    eps=adam_epsilon,\n    correct_bias=not use_bertadam\n)\n\noptimizer = PriorWD(optimizer, use_prior_wd=use_prior_wd)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T08:20:18.86284Z","iopub.execute_input":"2021-06-13T08:20:18.863096Z","iopub.status.idle":"2021-06-13T08:20:18.947654Z","shell.execute_reply.started":"2021-06-13T08:20:18.863074Z","shell.execute_reply":"2021-06-13T08:20:18.946829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This can now be used directly in training and the prior weight decay will do its work.\n\n<font color='#3498DB'><a id=\"section112\"><h3>References and Resources</h3></a></font>\n \n - [MIXOUT: EFFECTIVE REGULARIZATION TO FINETUNE\nLARGE-SCALE PRETRAINED LANGUAGE MODELS](https://arxiv.org/pdf/1909.11299.pdf)\n - [MIXOUT Code Implementation](https://github.com/bloodwass/mixout)","metadata":{}},{"cell_type":"markdown","source":"<font color='#3498DB'><a id=\"section1\"><h2>Stochastic Weight Averaging</h2></a></font>\n\n<font color='#3498DB'><a id=\"section112\"><h3>Introduction</h3></a></font>\n\nSnapshot ensembling is a technique where we take weights snapshot while training the same network and then after training create an ensemble of nets with the same architecture but different weights. This allows to improve test performance, and it is a very cheap way too because you just train one model once, just saving weights from time to time.\n\nIn SWA (Stochastic Weight Averaging) the authors propose to use a novel ensembling in the weights space. This method produces an ensemble by combining weights of the same network at different stages of training and then uses this model with combined weights to make predictions. There are 2 benefits from this approach:\n - when combining weights, we still get one model at the end, which speeds up predictions\n - it can be applied to any architecture and data set and shows good result in all of them.\n \n<font color='#3498DB'><a id=\"section112\"><h3>Idea</h3></a></font>\n![swa](https://miro.medium.com/max/1766/1*_USiR_z8PKaDuIcAs9xomw.png)\n\nIntuition for SWA comes from empirical observation that local minima at the end of each learning rate cycle tend to accumulate at the border of areas on loss surface where loss value is low (points W1, W2 and W3 are at the border of the red area of low loss in the left panel of figure above). \nBy taking the average of several such points, it is possible to achieve a wide, generalizable solution with even lower loss (Wswa in the left panel of the figure above).\n\nHere is how it works. Instead of an ensemble of many models, you only need two models:\n - the first model that stores the running average of model weights (w_swa in the formula). This will be the final model after the end of the training which will be used for predictions.\n - the second model (w in the formula) that will be traversing the weight space, exploring it by using a cyclical learning rate schedule.\n \n![swa2](https://miro.medium.com/max/502/1*Afu2bqxzC6p1BpIRTDWJtg.png)\n \nAt the end of each learning rate cycle, the current weights of the second model will be used to update the weight of the running average model by taking weighted mean between the old running average weights and the new set of weights from the second model (formula provided in the figure on the left). \n By following this approach, you only need to train one model, and store only two models in memory during training. For prediction, you only need the running average model and predicting on it is much faster than using ensemble described above, where you use many models to predict and then average results.","metadata":{}},{"cell_type":"markdown","source":"<font color='#3498DB'><a id=\"section112\"><h3>Implementation</h3></a></font>\n\nI won't be implementing it here, since this requires its own separate kernel. The main code will look something like below,\n\n```python\nfrom torch.optim.swa_utils import AveragedModel, SWALR\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\n\nloader, optimizer, model, loss_fn = ...\nswa_start = 5\nswa_model = AveragedModel(model)\nswa_scheduler = SWALR(optimizer, swa_lr=0.05)\nscheduler = CosineAnnealingLR(optimizer, T_max=100)\n\nfor epoch in range(100):\n      for input, target in loader:\n          optimizer.zero_grad()\n          loss_fn(model(input), target).backward()\n          optimizer.step()\n      if epoch > swa_start:\n          swa_model.update_parameters(model)\n          swa_scheduler.step()\n      else:\n          scheduler.step()\n\n# Update bn statistics for the swa_model at the end\ntorch.optim.swa_utils.update_bn(loader, swa_model)\n# Use swa_model to make predictions on test data \npreds = swa_model(test_input)\n```\n\n<font color='#3498DB'><a id=\"section112\"><h3>References and Resources</h3></a></font>\n - [Jigsaw Unintended Bias in Toxicity Classification - 1st Place Solution](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/discussion/103280)\n - [Averaging Weights Leads to Wider Optima and Better Generalization](https://arxiv.org/pdf/1803.05407.pdf)\n - [Torch SWA Example](https://github.com/izmailovpavel/torch_swa_examples)\n - [Google QUEST Q&A Labeling - How to use SWA in PyTorch](https://www.kaggle.com/c/google-quest-challenge/discussion/129936)\n - [Stochastic Weight Averaging — a New Way to Get State of the Art Results in Deep Learning](https://towardsdatascience.com/stochastic-weight-averaging-a-new-way-to-get-state-of-the-art-results-in-deep-learning-c639ccf36a)\n - [PyTorch 1.6 now includes Stochastic Weight Averaging](https://pytorch.org/blog/pytorch-1.6-now-includes-stochastic-weight-averaging/)\n - [Fast.ai SWA](https://github.com/fastai/fastai/pull/276/commits)\n ","metadata":{}},{"cell_type":"markdown","source":"<font color='#3498DB'><a id=\"section6\"><h2>Ending Notes</h2></a></font>\n\n- There are many more stable training strategies which I haven't covererd and which one do further research on,\n    - Early Stopping\n    - Training Iterations: Longer Fine-Tuning\n    - Transferring via an Intermediate Task - STILTs Training\n    - Weight initialization and data order\n    - Mixed Precision Training\n    \n- I will be sharing a FineTuning kernel with all of the above idea and results soon.\n\n- More comprehensive repository for learning and implementing Transformers for various tasks can be found [here](https://notebooks.quantumstat.com/), [here](https://huggingface.co/transformers/master/community.html#community-notebooks) and [here](https://huggingface.co/transformers/notebooks.html)  \n\n- I want to acknowledge once more that this kernel has code implementations from the potpourri of best papers out there on Stable and Robust Transformer Fine-Tuning Strategies.\n\n - [REVISITING FEW-SAMPLE BERT FINE-TUNING](https://arxiv.org/pdf/2006.05987.pdf)\n - [ON THE STABILITY OF FINE-TUNING BERT](https://arxiv.org/pdf/2006.04884.pdf)\n - [SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models](https://arxiv.org/pdf/1911.03437.pdf)\n - [Fine-Tuning Pretrained Language Models:Weight Initializations, Data Orders, and Early Stopping](https://arxiv.org/pdf/2002.06305.pdf)\n - [MIXOUT: EFFECTIVE REGULARIZATION TO FINETUNE LARGE-SCALE PRETRAINED LANGUAGE MODELS](https://arxiv.org/pdf/1909.11299.pdf)\n - [How to Fine-Tune BERT for Text Classification?](https://arxiv.org/pdf/1905.05583.pdf)\n - [Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks](https://arxiv.org/pdf/1811.01088.pdf)\n \n<font color='#3498DB'><a id=\"section2\"><h2>Thanks & Please Do Upvote!</h2></a></font>","metadata":{}}]}