{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<font color='#2980B9'><center><h2>Speeding up Transformer with Optimization Strategies</h2></center></font>\n<br>\n\n<font color='#3498DB'><h3>Introduction</h3></font>\n\nFor training the state-of-the-art or SOTA models, GPU is a big necessity. And even if we can procure one from **Google Colab** or **Kaggle**, there still comes the problem of memory constraints.   \n\nWe are more or less accustomed to seeing the **OOM (Out of Memory)** or **GPU Runtime Limit Reached** errors whenever we throw a large batch, large model, longer epochs, etc. to train. \n\n*This notebook will foray into how we can solve these issues using various optimization strategies with in-depth analysis and code.* \n<br>\n\n<font color='#3498DB'><h3>Problem</h3></font>\n\nThe problem is far more **apparent** when we talk about **transformers**. Transformers are extremely memory intensive. Hence, there is quite a high probability that we will run out of memory or above the runtime limit while training larger models or for longer epochs.\n\nLike in this competition the **loss** is **oscillating/unstable**. For finding the best performing models the trick is to **evaluate** more **frequently** within each epoch than after every epoch. This is an intensive process and if the training is done in **folds** might result in GPU Runtime Limit Reached.\n<br>\n\n<font color='#3498DB'><h3>Solution</h3></font>\n\nThere are some promising **well-known** out of the box **strategies** to solve these problems and each strategy comes with its own benefits. \n - Dyanmic Padding and Uniform Length Batching  \n - Gradient Accumulation  \n - Freeze Embedding  \n - Numeric Precision Reduction \n - Gradient Checkpointing\n \n*Note 1: All these optimizations are focussed on single-GPU only. These optimizations will work on multi-GPU as well but multi-GPU setting has more advanced techniques like DataParallel, DataParallelModel, ZeRO etc. which work much better for accelarating Transformers.*\n\n*Note 2: Gradient Checkpointing increases computation time but it helps to fit larger batch size in single pass. Thus, I've included this.*\n\n<br>\n\n<font color='#3498DB'><h3>Contents</h3></font>\n- [**Dynamic Padding and Uniform Length Batching (Smart Batching)**](#section1)\n  - [Introduction](#section10)\n      - [Dyanmic Padding](#section11)\n      - [Uniform Length Batching](#section12)\n  - [Read Dataset](#section13)\n  - [Load Tokenizer](#section14)\n  - [Tokenize without Padding](#section15)\n  - [Sort by Length](#section16)\n  - [Random Batch Selection](#section17)\n  - [Add Padding](#section18)\n    - [Comparison I](#section18a)\n  - [Unified Function - **make_smart_batches**](#section19)\n  - [Smart Batching with DataLoader, Collater, Sampler](#section110)\n    - [Components](#section110a)\n    - [Comparison II](#section110b)\n  - [Conclusion](#section111)\n  - [References & Resources](#section112)\n- [**Freeze Embedding**](#section2)\n  - [Introduction](#section20)\n  - [Idea](#section21)\n  - [Splits Analysis](#section22)\n  - [Implementation](#section23)\n  - [Conclusion](#section24)\n  - [References & Resources](#section25)\n- [**Numeric Precision Reduction**](#section3)\n  - [Introduction](#section30)\n  - [Floating Point Representation](#section31)\n  - [How to Use?](#section32)\n  - [Special Case](#section33)\n  - [References & Resources](#section34)\n- [**Gradient Accumulation**](#section4)\n  - [Introduction](#section40)\n  - [Implementation](#section41)\n  - [References & Resources](#section42)\n- [**Gradient Checkpointing**](#section5)\n  - [Introduction](#section50)\n  - [Idea](#section51)\n  - [References & Resources](#section52)\n- [**Ending Notes**](#section6)\n\n<font color='#3498DB'><h3>What's New?</h3></font>\n1. [SWA, Apex AMP & Interpreting Transformers in Torch](https://www.kaggle.com/rhtsingh/swa-apex-amp-interpreting-transformers-in-torch) notebook is an implementation of the Stochastic Weight Averaging technique with NVIDIA Apex on transformers using PyTorch. The notebook also implements how to interactively interpret Transformers using LIT (Language Interpretability Tool) a platform for NLP model understanding.   \nIt has in-depth explanations and code implementations for,\n - SWA \n - Apex AMP\n - Weighted Layer Pooling\n - MADGRAD Optimizer\n - Grouped LLRD\n - Language Interpretibility Tool\n    - Attention Visualization\n    - Saliency Maps\n    - Integrated Gradients\n    - LIME \n    - Embedding Space (UMAP & PCA)\n    - Counterfactual generation\n    - And many more ...\n\n2. [Utilizing Transformer Representations Efficiently](https://www.kaggle.com/rhtsingh/utilizing-transformer-representations-efficiently) notebook will show many different ways these outputs and hidden representations can be utilized to do much more than just adding an output layer. It has code implementations and detailed explanations for all the below techniques,\n - Pooler Output  \n - Last Hidden State Output  \n    - CLS Embeddings  \n    - Mean Pooling  \n    - Max Pooling  \n    - Mean + Max Pooling  \n    - Conv1D Pooling  \n - Hidden Layers Output  \n    - Layerwise CLS Embeddings  \n    - Concatenate Pooling  \n    - Weighted Layer Pooling  \n    - LSTM / GRU Pooling  \n    - Attention Pooling  \n    - WKPooling  \n\n3. [On Stability of Few-Sample Transformer Fine-Tuning](https://www.kaggle.com/rhtsingh/on-stability-of-few-sample-transformer-fine-tuning) notebook goes over various remedies to increase few-sample fine-tuning stability and they show a significant performance improvement over simple finetuning methods. The methods explained in the notebook are - \n - Debiasing Omission In BertADAM\n - Re-Initializing Transformer Layers\n - Utilizing Intermediate Layers\n - Layer-wise Learning Rate Decay (LLRD) \n - Mixout Regularization\n - Pre-trained Weight Decay\n - Stochastic Weight Averaging. \n \n<font color='#3498DB'><h3>Fixes</h3></font>\n\n - <font color='#3498DB'>6/6/2021 - Fixed Dynamic Padding + Uniform Length Bacthing with DataLoader</font>\n   - Issue: Once the SmartBatchingSampler is initalized it returns the same sequence of randomly shuffled batches due to which randomness is reduced.\n   ","metadata":{}},{"cell_type":"markdown","source":"<font color='#3498DB'><a id=\"section1\"><h2>Dynamic Padding and Uniform Length Batching</h2></a></font>\n\n<font color='#3498DB'><a id=\"section10\"><h3>Introduction</h3></a></font>\nTraining neural networks on a batch of sequences requires them to have the exact same length to build the batch matrix representation. Because real life NLP datasets are always made of texts of variable lengths, we often need to make some sequences shorter by truncating them, and some others longer by adding at the end a repeated fake token called “pad” token. \n\nBecause the pad token doesn’t represent a real word, when most computations are done, before computing the loss, we erase the pad token signal by multiplying it by 0 through the “attention mask” matrix for each sample, which identifies the [PAD] tokens and tells Transformer to ignore them.\n\n![Fixed Padding](https://drive.google.com/uc?export=view&id=1UaVW3gxTSAD0E9CrCX48kn6QF_L_slP9)\n\nFor e.g., In this competition, in order to feed multiple samples into RoBERTa at once, I am padding out all of the sentences to a \"fixed length\" of 250. \nThis is the standard approach that I’ve used in all of my examples, and it’s certainly the simplest to implement, code-wise.\n\n<font color='#3498DB'><a id=\"section11\"><h3>Dynamic Padding</h3></a></font>\nHere we limit the number of added pad tokens to reach the length of the longest sequence of each mini batch instead of a fixed value set for the whole train set Because the number of added tokens changes across mini batches, we call it \"dynamic\" padding.\n\n![Dyanmic Padding](https://drive.google.com/uc?export=view&id=1UfJJ2sj1w6prvwQPwyqbQCgJbJ9qIC9V)\n\n\n<font color='#3498DB'><a id=\"section12\"><h3>Uniform Length Batching</h3></a></font>\n\nWe push the logic futher by generating batches made of similar length sequences so we avoid extreme cases where most sequences in the mini batch are short and we are required to add lots of pad tokens to each of them because 1 sequence of the same mini batch is very long.\n\n![Uniform Length Batching](https://drive.google.com/uc?export=view&id=1UjQaOAkN-zPh10fSoXmZ-68jMdFLq2y_)\n\n*Note: In the above illustration the selected batches are in sequence, but we’ll actually be selecting them more randomly, to allow for more randomness to the order of the training data.*\n\n**That's it, we are done with theory and from here onwards, we will understand by implementing in code.**","metadata":{}},{"cell_type":"markdown","source":"<font color='#3498DB'><a id=\"section13\"><h3>Read Dataset</h3></a></font>\n\nHere we will read our competition dataset.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom transformers import AutoTokenizer\nfrom IPython.display import clear_output\n\ntrain = pd.read_csv('../input/commonlitreadabilityprize/train.csv', low_memory=False)\ntest = pd.read_csv('../input/commonlitreadabilityprize/sample_submission.csv', low_memory=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-01T19:21:20.343389Z","iopub.execute_input":"2021-06-01T19:21:20.343703Z","iopub.status.idle":"2021-06-01T19:21:20.42112Z","shell.execute_reply.started":"2021-06-01T19:21:20.343673Z","shell.execute_reply":"2021-06-01T19:21:20.420197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color='#3498DB'><a id=\"section14\"><h3>Load Tokenizer</h3></a></font>\n\nWe'll be using RoBERTa-base tokenizer.","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('roberta-base')\nclear_output()","metadata":{"execution":{"iopub.status.busy":"2021-06-01T19:21:20.422417Z","iopub.execute_input":"2021-06-01T19:21:20.422673Z","iopub.status.idle":"2021-06-01T19:21:21.332291Z","shell.execute_reply.started":"2021-06-01T19:21:20.422648Z","shell.execute_reply":"2021-06-01T19:21:21.331247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color='#3498DB'><a id=\"section15\"><h3>Tokenize Without Padding</h3></a></font>\n\n<font color='#3498DB'><a id=\"section15a\"><h4>Peak GPU Memory Use</h4></a></font>\nEven when applying this technique, we may still want to truncate our inputs to a certain maximum length. All it takes is one batch that's too long to fit on the GPU, and our training will fail! Hence, it makes sense to truncate to something lower than 512, even with smart batching. \n\n*Note: We don't need to be concerned about this during training in this competition since the highest sequence length is less than 350.*\n\n<font color='#3498DB'><a id=\"section15b\"><h4>Tokenize, but don't pad</h4></a></font>\n\nWe’re going to start by tokenizing all of the samples and mapping the tokens to their IDs. We’re also going to truncate the sequences to our chosen max_len, and we’re going to add the special tokens. But we are **not padding** yet!","metadata":{}},{"cell_type":"code","source":"def good_update_interval(total_iters, num_desired_updates):\n    # find intervals for printing updates\n    exact_interval = total_iters / num_desired_updates\n    order_of_mag = len(str(total_iters)) - 1\n    round_mag = order_of_mag - 1\n    update_interval = int(round(exact_interval, -round_mag))\n    if update_interval == 0:\n        update_interval = 1\n    return update_interval","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-01T19:21:21.334198Z","iopub.execute_input":"2021-06-01T19:21:21.334477Z","iopub.status.idle":"2021-06-01T19:21:21.339381Z","shell.execute_reply.started":"2021-06-01T19:21:21.334448Z","shell.execute_reply":"2021-06-01T19:21:21.338461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_ids = []\n\ntrain_text = train.excerpt.values.tolist()\ntrain_targets = train.target.values.tolist()\nmax_len = 350\n\nprint('Tokenizing {:,} training samples...'.format(len(train_text)))\n\nupdate_interval = good_update_interval(total_iters=len(train_text), num_desired_updates=5)\n\nfor text in train_text:\n    if ((len(input_ids) % update_interval) == 0):\n        print('  Tokenized {:,} samples.'.format(len(input_ids)))\n\n    input_id = tokenizer.encode(\n        text=text,           \n        add_special_tokens=True, \n        max_length=max_len,  \n        truncation=True,     \n        padding=False\n    )       \n                                 \n    input_ids.append(input_id)\n    \nprint('DONE.')\nprint('{:>10,} samples'.format(len(input_ids)))","metadata":{"execution":{"iopub.status.busy":"2021-06-01T19:21:21.340809Z","iopub.execute_input":"2021-06-01T19:21:21.34113Z","iopub.status.idle":"2021-06-01T19:21:23.965615Z","shell.execute_reply.started":"2021-06-01T19:21:21.3411Z","shell.execute_reply":"2021-06-01T19:21:23.963556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color='#3498DB'><a id=\"section16\"><h3>Sort by length</h3></a></font>\n\nLet’s look at the lengths of the samples in their original, unsorted order.\nThe below plot simply confirms that the sample lengths do vary significantly, and that they are unsorted.","metadata":{}},{"cell_type":"code","source":"unsorted_lengths = [len(x) for x in input_ids]\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set(style='darkgrid')\n\nsns.set(font_scale=1.5)\nplt.rcParams[\"figure.figsize\"] = (12,6)\n\nplt.scatter(range(0, len(unsorted_lengths)), unsorted_lengths, marker=\"|\")\n\nplt.xlabel('Sample Number')\nplt.ylabel('Sequence Length')\nplt.title('Samples BEFORE Sorting')\n\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-01T19:21:23.967151Z","iopub.execute_input":"2021-06-01T19:21:23.967554Z","iopub.status.idle":"2021-06-01T19:21:24.252905Z","shell.execute_reply.started":"2021-06-01T19:21:23.967511Z","shell.execute_reply":"2021-06-01T19:21:24.252026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we’ll sort the examples by length so that we can create batches with equal (or at least similar) lengths and generate the same plot again.","metadata":{}},{"cell_type":"code","source":"sorted_input_ids = sorted(zip(input_ids, train_targets), key=lambda x: len(x[0]))\nprint('Shortest sample:', len(sorted_input_ids[0][0]))\nprint('Longest sample:', len(sorted_input_ids[-1][0]))\nsorted_lengths = [len(s[0]) for s in sorted_input_ids]","metadata":{"execution":{"iopub.status.busy":"2021-06-01T19:21:24.254117Z","iopub.execute_input":"2021-06-01T19:21:24.254387Z","iopub.status.idle":"2021-06-01T19:21:24.262316Z","shell.execute_reply.started":"2021-06-01T19:21:24.25436Z","shell.execute_reply":"2021-06-01T19:21:24.261406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Use plot styling from seaborn.\nsns.set(style='darkgrid')\n\n# Increase the plot size and font size.\nsns.set(font_scale=1.5)\nplt.rcParams[\"figure.figsize\"] = (12,6)\n\nplt.plot(range(0, len(sorted_lengths)), sorted_lengths)\n\nplt.xlabel('Sample Number')\nplt.ylabel('Sequence Length')\nplt.title('Samples after Sorting')\n\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-01T19:21:24.263602Z","iopub.execute_input":"2021-06-01T19:21:24.263919Z","iopub.status.idle":"2021-06-01T19:21:24.525996Z","shell.execute_reply.started":"2021-06-01T19:21:24.263859Z","shell.execute_reply":"2021-06-01T19:21:24.524998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color='#3498DB'><a id=\"section17\"><h3>Random Batch Selection</h3></a></font>\n\nNow we’re ready to select our batches.\nRather than dividing the batches up in order, we will still add a degree of **randomness** to our selection.\n\nHere’s the process:\n\n - Pick a random starting point in the (sorted!) list of samples.\n - Grab a contiguous batch of samples starting from that point.\n - Delete those samples from the list, and repeat until all of the samples have been grabbed.\n \nThis will result in some **fragmentation** of the list, which means it won’t be quite as efficient as if we just sliced up the batches in sorted order.","metadata":{}},{"cell_type":"code","source":"batch_size = 24\nimport random\n\nbatch_ordered_sentences = []\nbatch_ordered_labels = []\n\nprint('Creating training batches of size {:}'.format(batch_size))\n\nwhile len(sorted_input_ids) > 0:  \n    if ((len(batch_ordered_sentences) % 50) == 0):\n        print('  Selected {:,} batches.'.format(len(batch_ordered_sentences)))\n\n    to_take = min(batch_size, len(sorted_input_ids))\n    select = random.randint(0, len(sorted_input_ids) - to_take)\n    batch = sorted_input_ids[select:(select + to_take)]\n    batch_ordered_sentences.append([s[0] for s in batch])\n    batch_ordered_labels.append([s[1] for s in batch])\n    del sorted_input_ids[select:select + to_take]\n\nprint('\\n  DONE - {:,} batches.'.format(len(batch_ordered_sentences)))","metadata":{"execution":{"iopub.status.busy":"2021-06-01T19:21:24.528052Z","iopub.execute_input":"2021-06-01T19:21:24.528335Z","iopub.status.idle":"2021-06-01T19:21:24.549007Z","shell.execute_reply.started":"2021-06-01T19:21:24.528304Z","shell.execute_reply":"2021-06-01T19:21:24.548064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color='#3498DB'><a id=\"section18\"><h3>Add Padding</h3></a></font>\n\nWe’ve created our batches, but many of them will contain sequences of different lengths. In order to leverage the GPUs parallel processing of batches, all of the sequences within a batch need to be the same length.\n\nThis means we need to do some **padding!**\n\nWe’ll also create our attention masks here, and cast everything to PyTorch tensors in preparation for our fine-tuning step.","metadata":{}},{"cell_type":"code","source":"import torch\n\ninputs = []\nattn_masks = []\ntargets = []\n\nfor (batch_inputs, batch_labels) in zip(batch_ordered_sentences, batch_ordered_labels):\n    batch_padded_inputs = []\n    batch_attn_masks = []\n    max_size = max([len(sen) for sen in batch_inputs])\n    for sen in batch_inputs:\n        num_pads = max_size - len(sen)\n        padded_input = sen + [tokenizer.pad_token_id]*num_pads\n        attn_mask = [1] * len(sen) + [0] * num_pads\n        batch_padded_inputs.append(padded_input)\n        batch_attn_masks.append(attn_mask)\n    inputs.append(torch.tensor(batch_padded_inputs))\n    attn_masks.append(torch.tensor(batch_attn_masks))\n    targets.append(torch.tensor(batch_labels))","metadata":{"execution":{"iopub.status.busy":"2021-06-01T19:21:24.550752Z","iopub.execute_input":"2021-06-01T19:21:24.551057Z","iopub.status.idle":"2021-06-01T19:21:24.644782Z","shell.execute_reply.started":"2021-06-01T19:21:24.551026Z","shell.execute_reply":"2021-06-01T19:21:24.643852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color='#3498DB'><a id=\"section18a\"><h4>Comparison</h4></a></font>\nNow that our data is ready, we can calculate the total number of tokens in the training data after using smart batching.","metadata":{}},{"cell_type":"code","source":"padded_lengths = []\nfor batch in inputs:\n    for s in batch:\n        padded_lengths.append(len(s))\n\nsmart_token_count = np.sum(padded_lengths)\nfixed_token_count = len(train_text) * max_len\n\nprcnt_reduced = (fixed_token_count - smart_token_count) / float(fixed_token_count) \n\nprint('Total tokens:')\nprint('   Fixed Padding: {:,}'.format(fixed_token_count))\nprint('  Smart Batching: {:,}  ({:.2%} less)'.format(smart_token_count, prcnt_reduced))","metadata":{"execution":{"iopub.status.busy":"2021-06-01T19:21:24.646048Z","iopub.execute_input":"2021-06-01T19:21:24.646319Z","iopub.status.idle":"2021-06-01T19:21:24.663094Z","shell.execute_reply.started":"2021-06-01T19:21:24.646293Z","shell.execute_reply":"2021-06-01T19:21:24.661872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color='#3498DB'><a id=\"section19\"><h3>All in One - make_smart_batches</h3></a></font>\n\nThis function combines all of the steps from the “Smart Batching” section into a single (re-usable) function. You can use this in your own Notebook for applying smart batching to both your training and test sets.\n\nClick *show hidden code.*","metadata":{}},{"cell_type":"code","source":"def make_smart_batches(text_samples, labels, batch_size):\n    '''\n    This function combines all of the required steps to prepare batches.\n    '''\n\n    print('Creating Smart Batches from {:,} examples with batch size {:,}...\\n'.format(len(text_samples), batch_size))\n\n    # =========================\n    #   Tokenize & Truncate\n    # =========================\n\n    full_input_ids = []\n\n    # Tokenize all training examples\n    print('Tokenizing {:,} samples...'.format(len(labels)))\n\n    # Choose an interval on which to print progress updates.\n    update_interval = good_update_interval(total_iters=len(labels), num_desired_updates=10)\n\n    # For each training example...\n    for text in text_samples:\n        \n        # Report progress.\n        if ((len(full_input_ids) % update_interval) == 0):\n            print('  Tokenized {:,} samples.'.format(len(full_input_ids)))\n\n        # Tokenize the sample.\n        input_ids = tokenizer.encode(text=text,              # Text to encode.\n                                    add_special_tokens=True, # Do add specials.\n                                    max_length=max_len,      # Do Truncate!\n                                    truncation=True,         # Do Truncate!\n                                    padding=False)           # DO NOT pad.\n                                    \n        # Add the tokenized result to our list.\n        full_input_ids.append(input_ids)\n        \n    print('DONE.')\n    print('{:>10,} samples\\n'.format(len(full_input_ids)))\n\n    # =========================\n    #      Select Batches\n    # =========================    \n\n    # Sort the two lists together by the length of the input sequence.\n    samples = sorted(zip(full_input_ids, labels), key=lambda x: len(x[0]))\n\n    print('{:>10,} samples after sorting\\n'.format(len(samples)))\n\n    import random\n\n    # List of batches that we'll construct.\n    batch_ordered_sentences = []\n    batch_ordered_labels = []\n\n    print('Creating batches of size {:}...'.format(batch_size))\n\n    # Choose an interval on which to print progress updates.\n    update_interval = good_update_interval(total_iters=len(samples), num_desired_updates=10)\n    \n    # Loop over all of the input samples...    \n    while len(samples) > 0:\n        \n        # Report progress.\n        if ((len(batch_ordered_sentences) % update_interval) == 0 \\\n            and not len(batch_ordered_sentences) == 0):\n            print('  Selected {:,} batches.'.format(len(batch_ordered_sentences)))\n\n        # `to_take` is our actual batch size. It will be `batch_size` until \n        # we get to the last batch, which may be smaller. \n        to_take = min(batch_size, len(samples))\n\n        # Pick a random index in the list of remaining samples to start\n        # our batch at.\n        select = random.randint(0, len(samples) - to_take)\n\n        # Select a contiguous batch of samples starting at `select`.\n        #print(\"Selecting batch from {:} to {:}\".format(select, select+to_take))\n        batch = samples[select:(select + to_take)]\n\n        #print(\"Batch length:\", len(batch))\n\n        # Each sample is a tuple--split them apart to create a separate list of \n        # sequences and a list of labels for this batch.\n        batch_ordered_sentences.append([s[0] for s in batch])\n        batch_ordered_labels.append([s[1] for s in batch])\n\n        # Remove these samples from the list.\n        del samples[select:select + to_take]\n\n    print('\\n  DONE - Selected {:,} batches.\\n'.format(len(batch_ordered_sentences)))\n\n    # =========================\n    #        Add Padding\n    # =========================    \n\n    print('Padding out sequences within each batch...')\n\n    py_inputs = []\n    py_attn_masks = []\n    py_labels = []\n\n    # For each batch...\n    for (batch_inputs, batch_labels) in zip(batch_ordered_sentences, batch_ordered_labels):\n\n        # New version of the batch, this time with padded sequences and now with\n        # attention masks defined.\n        batch_padded_inputs = []\n        batch_attn_masks = []\n        \n        # First, find the longest sample in the batch. \n        # Note that the sequences do currently include the special tokens!\n        max_size = max([len(sen) for sen in batch_inputs])\n\n        # For each input in this batch...\n        for sen in batch_inputs:\n            \n            # How many pad tokens do we need to add?\n            num_pads = max_size - len(sen)\n\n            # Add `num_pads` padding tokens to the end of the sequence.\n            padded_input = sen + [tokenizer.pad_token_id]*num_pads\n\n            # Define the attention mask--it's just a `1` for every real token\n            # and a `0` for every padding token.\n            attn_mask = [1] * len(sen) + [0] * num_pads\n\n            # Add the padded results to the batch.\n            batch_padded_inputs.append(padded_input)\n            batch_attn_masks.append(attn_mask)\n\n        # Our batch has been padded, so we need to save this updated batch.\n        # We also need the inputs to be PyTorch tensors, so we'll do that here.\n        # Todo - Michael's code specified \"dtype=torch.long\"\n        py_inputs.append(torch.tensor(batch_padded_inputs))\n        py_attn_masks.append(torch.tensor(batch_attn_masks))\n        py_labels.append(torch.tensor(batch_labels))\n    \n    print('  DONE.')\n\n    # Return the smart-batched dataset!\n    return (py_inputs, py_attn_masks, py_labels)\n\n(py_inputs, py_attn_masks, py_labels) = make_smart_batches(train_text, train_targets, batch_size)\n\nclear_output()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-01T19:21:24.664464Z","iopub.execute_input":"2021-06-01T19:21:24.664737Z","iopub.status.idle":"2021-06-01T19:21:27.166126Z","shell.execute_reply.started":"2021-06-01T19:21:24.664709Z","shell.execute_reply":"2021-06-01T19:21:27.165074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color='#3498DB'><a id=\"section110\"><h3>Smart Batching with DataLoader, Collator, Sampler</h3></a></font>\n\nThis section is an attempt to implement the above methodology of “smart batching” in a more formal way with the PyTorch **DataLoader** class.\n<br>\n\n<font color='#3498DB'><a id=\"section110a\"><h4>Components</h4></a></font>\nWe will have 4 major components that will work together,\n  - Dataset - Stores the samples and their corresponding labels. \n  - DataLoader - Wrap an iterable around the Dataset to enable easy access to the samples.\n  - Sampler -  Specify the sequence of indices that will be used in data loading.\n  - Collator -  Collate lists of samples into batches.\n   \n*Note: To make the code compact the dataloader is inside \"SmartBatchingDataset\" and one can easily take it outside.*","metadata":{}},{"cell_type":"code","source":"import random\nimport numpy as np\nimport multiprocessing\nimport more_itertools\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Sampler, Dataset, DataLoader","metadata":{"execution":{"iopub.status.busy":"2021-06-01T19:21:27.16767Z","iopub.execute_input":"2021-06-01T19:21:27.168076Z","iopub.status.idle":"2021-06-01T19:21:27.173591Z","shell.execute_reply.started":"2021-06-01T19:21:27.168036Z","shell.execute_reply":"2021-06-01T19:21:27.172923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**SmartBatchingDataset** will store the samples in by tokenzing excerpts and converting into sequences.","metadata":{}},{"cell_type":"code","source":"class SmartBatchingDataset(Dataset):\n    def __init__(self, df, tokenizer):\n        super(SmartBatchingDataset, self).__init__()\n        self._data = (\n            f\"{tokenizer.bos_token} \" + df.excerpt + f\" {tokenizer.eos_token}\" \n        ).apply(tokenizer.tokenize).apply(tokenizer.convert_tokens_to_ids).to_list()\n        self._targets = None\n        if 'target' in df.columns:\n            self._targets = df.target.tolist()\n        self.sampler = None\n\n    def __len__(self):\n        return len(self._data)\n\n    def __getitem__(self, item):\n        if self._targets is not None:\n            return self._data[item], self._targets[item]\n        else:\n            return self._data[item]\n\n    def get_dataloader(self, batch_size, max_len, pad_id):\n        self.sampler = SmartBatchingSampler(\n            data_source=self._data,\n            batch_size=batch_size\n        )\n        collate_fn = SmartBatchingCollate(\n            targets=self._targets,\n            max_length=max_len,\n            pad_token_id=pad_id\n        )\n        dataloader = DataLoader(\n            dataset=self,\n            batch_size=batch_size,\n            sampler=self.sampler,\n            collate_fn=collate_fn,\n            num_workers=(multiprocessing.cpu_count()-1),\n            pin_memory=True\n        )\n        return dataloader","metadata":{"execution":{"iopub.status.busy":"2021-06-01T19:21:27.174467Z","iopub.execute_input":"2021-06-01T19:21:27.174694Z","iopub.status.idle":"2021-06-01T19:21:27.194546Z","shell.execute_reply.started":"2021-06-01T19:21:27.17467Z","shell.execute_reply":"2021-06-01T19:21:27.193737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**SmartBatchingSampler** will sort the sequences by length, make batches of size \"batch_size\", shuffle the batch and return indices.","metadata":{}},{"cell_type":"code","source":"class SmartBatchingSampler(Sampler):\n    def __init__(self, data_source, batch_size):\n        super(SmartBatchingSampler, self).__init__(data_source)\n        self.len = len(data_source)\n        sample_lengths = [len(seq) for seq in data_source]\n        argsort_inds = np.argsort(sample_lengths)\n        self.batches = list(more_itertools.chunked(argsort_inds, n=batch_size))\n        self._backsort_inds = None\n    \n    def __iter__(self):\n        if self.batches:\n            last_batch = self.batches.pop(-1)\n            np.random.shuffle(self.batches)\n            self.batches.append(last_batch)\n        self._inds = list(more_itertools.flatten(self.batches))\n        yield from self._inds\n\n    def __len__(self):\n        return self.len\n    \n    @property\n    def backsort_inds(self):\n        if self._backsort_inds is None:\n            self._backsort_inds = np.argsort(self._inds)\n        return self._backsort_inds","metadata":{"execution":{"iopub.status.busy":"2021-06-01T19:21:27.196017Z","iopub.execute_input":"2021-06-01T19:21:27.196564Z","iopub.status.idle":"2021-06-01T19:21:27.209908Z","shell.execute_reply.started":"2021-06-01T19:21:27.196524Z","shell.execute_reply":"2021-06-01T19:21:27.208733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**SmartBatchingCollate** will add padding upto highest sequence length, make attention masks, targets for each sample in batch. ","metadata":{}},{"cell_type":"code","source":"class SmartBatchingCollate:\n    def __init__(self, targets, max_length, pad_token_id):\n        self._targets = targets\n        self._max_length = max_length\n        self._pad_token_id = pad_token_id\n        \n    def __call__(self, batch):\n        if self._targets is not None:\n            sequences, targets = list(zip(*batch))\n        else:\n            sequences = list(batch)\n        \n        input_ids, attention_mask = self.pad_sequence(\n            sequences,\n            max_sequence_length=self._max_length,\n            pad_token_id=self._pad_token_id\n        )\n        \n        if self._targets is not None:\n            output = input_ids, attention_mask, torch.tensor(targets)\n        else:\n            output = input_ids, attention_mask\n        return output\n    \n    def pad_sequence(self, sequence_batch, max_sequence_length, pad_token_id):\n        max_batch_len = max(len(sequence) for sequence in sequence_batch)\n        max_len = min(max_batch_len, max_sequence_length)\n        padded_sequences, attention_masks = [[] for i in range(2)]\n        attend, no_attend = 1, 0\n        for sequence in sequence_batch:\n            # As discussed above, truncate if exceeds max_len\n            new_sequence = list(sequence[:max_len])\n            \n            attention_mask = [attend] * len(new_sequence)\n            pad_length = max_len - len(new_sequence)\n            \n            new_sequence.extend([pad_token_id] * pad_length)\n            attention_mask.extend([no_attend] * pad_length)\n            \n            padded_sequences.append(new_sequence)\n            attention_masks.append(attention_mask)\n        \n        padded_sequences = torch.tensor(padded_sequences)\n        attention_masks = torch.tensor(attention_masks)\n        return padded_sequences, attention_masks","metadata":{"execution":{"iopub.status.busy":"2021-06-01T19:21:27.211067Z","iopub.execute_input":"2021-06-01T19:21:27.211567Z","iopub.status.idle":"2021-06-01T19:21:27.229914Z","shell.execute_reply.started":"2021-06-01T19:21:27.211525Z","shell.execute_reply":"2021-06-01T19:21:27.228736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we create **Dataset** and **DataLoader**, iterate over our dataloader, check the total number of tokens and, do comparison study.\n","metadata":{"execution":{"iopub.status.busy":"2021-05-31T21:28:52.537512Z","iopub.execute_input":"2021-05-31T21:28:52.537861Z","iopub.status.idle":"2021-05-31T21:28:52.543765Z","shell.execute_reply.started":"2021-05-31T21:28:52.537832Z","shell.execute_reply":"2021-05-31T21:28:52.542059Z"}}},{"cell_type":"code","source":"dataset = SmartBatchingDataset(train, tokenizer)\ndataloader = dataset.get_dataloader(batch_size=24, max_len=max_len, pad_id=tokenizer.pad_token_id)\n\npadded_lengths = []\nfor batch_idx, (input_ids, attention_mask, targets) in enumerate(dataloader):\n    for s in input_ids:\n        padded_lengths.append(len(s))\n\nsmart_token_count = np.sum(padded_lengths)\nfixed_token_count = len(train_text) * max_len\n\nprcnt_reduced = (fixed_token_count - smart_token_count) / float(fixed_token_count) \n\nprint('Total tokens:')\nprint('   Fixed Padding: {:,}'.format(fixed_token_count))\nprint('  Smart Batching: {:,}  ({:.2%} less)'.format(smart_token_count, prcnt_reduced))","metadata":{"execution":{"iopub.status.busy":"2021-06-01T19:21:27.231208Z","iopub.execute_input":"2021-06-01T19:21:27.231594Z","iopub.status.idle":"2021-06-01T19:21:31.018283Z","shell.execute_reply.started":"2021-06-01T19:21:27.231554Z","shell.execute_reply":"2021-06-01T19:21:31.015953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color='#3498DB'><a id=\"section110b\"><h4>Comparison II</h4></a></font>\n\nIn comparison to above technique of random batch selection we don't have any framentation in case of DataLoader but, randomness is reduced.\n\n - **Fixed Padding (MaxLen-350 & BS-24):**  991,900 tokens   \n - **Smart Batching with Random Batch:** 623,780  (37.11% less)\n - **Smart Batching with DataLoader:**   621,694  (37.32% less)  \n\n<font color='#3498DB'><a id=\"section111\"><h3>Conclusion</h3></a></font>\nIt has been shown that this technique constantly provides significant time reduction without reducing (in some cases even improves) accuracy. This technique is a low hanging fruit that should be widely used by Transformer users.\n\n<font color='#3498DB'><a id=\"section112\"><h3>References and Resources</h3></a></font>\n \n - [**Smart Batching Tutorial - Speed Up BERT Training**](http://mccormickml.com/2020/07/29/smart-batching-tutorial/#42-tokenize-without-padding) by Chris McCormick. This is the main source of almost everything you're seeing above except for DataLoader part. Suggest to check his YouTube channel and other Blog articles as well.\n - [Divide Hugging Face Transformers training time by 2 or more](https://towardsdatascience.com/divide-hugging-face-transformers-training-time-by-2-or-more-21bf7129db9q-21bf7129db9e) by Michaël Benesty. The inspration of Chris McCormick Smart Batching tutorial.\n   - [Colab Notebook](https://github.com/ELS-RD/transformers-notebook/blob/master/Divide_Hugging_Face_Transformers_training_time_by_2_or_more.ipynb) \n   - [GitHub Gist](https://gist.github.com/pommedeterresautee/1a334b665710bec9bb65965f662c94c8)\n - [Accelerating recurrent neural network training using sequence bucketing and multi-GPU data parallelization](https://arxiv.org/abs/1708.05604)\n - [Jigsaw Multilingual Toxic Comment Classification - 4th Place Solution](https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification/discussion/160980)\n - [Speed up your RNN with Sequence Bucketing](https://www.kaggle.com/bminixhofer/speed-up-your-rnn-with-sequence-bucketing)\n - [Better Batches with PyTorchText BucketIterator](https://gmihaila.medium.com/better-batches-with-pytorchtext-bucketiterator-12804a545e2a#:~:text=%F0%9F%8D%87%20Better%20Batches%20with%20PyTorchText%20BucketIterator,-How%20to%20use&text=This%20allows%20us%20to%20provide,gpt2%2C%20xlnet%2C%20etc.)\n - [Tensorflow-esque bucket by sequence length](https://discuss.pytorch.org/t/tensorflow-esque-bucket-by-sequence-length/41284)","metadata":{}},{"cell_type":"markdown","source":"<font color='#3498DB'><a id=\"section2\"><h2>Freeze Embedding</h2></a></font>\n\n<font color='#3498DB'><a id=\"section20\"><h3>Introduction</h3></a></font>\nFreezing the embedding matrix during the fine-tuning of a pre-trained language model, yes its that simple. Freezing Embedding Layer of transformers can save the GPU memory so that one can use bigger batchsize when training, while speeding up the training process.\n\n<font color='#3498DB'><a id=\"section21\"><h3>Idea</h3></a></font>\n - The intuition behind this is that the embedding layer contains an independent representation of each token and the vectors for tokens with similar meanings should be learned to be close in the embeddings space during pre-training to be used in a pretty similar way by the further layers. But when you have not a good amount of labeled data or a small corpus that is common for real-world use cases, most probably the data on which you will do the inference will contain some tokens that were not presented in the train set. Probably some of these new tokens will have the synonym pairs in your labeled data, but gradient updates can destroy this type of connection learned by the language model.\n \n - The other is since we are freezing the embedding matrix which itself contains huge number of parameters, gradient updates will not take place and hence will result in lesser computation times and one can utilize larger batch size.\n \n<font color='#3498DB'><a id=\"section22\"><h3>Splits Analysis</h3></a></font>\nFirst of all, we will split the data using the KFold strategy that I have been using in my other notebooks and do some analysis for better understanding the first idea.","metadata":{}},{"cell_type":"code","source":"!pip install -q matplotlib_venn\n\nfrom itertools import chain\nimport numpy as np\nimport pandas as pd\nfrom sklearn import model_selection\nimport matplotlib.pyplot as plt\nfrom matplotlib_venn import venn2\n%config InlineBackend.figure_format = 'retina'\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2021-06-01T19:21:31.021805Z","iopub.execute_input":"2021-06-01T19:21:31.022192Z","iopub.status.idle":"2021-06-01T19:21:36.849244Z","shell.execute_reply.started":"2021-06-01T19:21:31.022156Z","shell.execute_reply":"2021-06-01T19:21:36.847959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/commonlitreadabilityprize/train.csv', low_memory=False)\ntest = pd.read_csv('../input/commonlitreadabilityprize/test.csv', low_memory=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-01T19:21:36.851534Z","iopub.execute_input":"2021-06-01T19:21:36.851995Z","iopub.status.idle":"2021-06-01T19:21:36.911922Z","shell.execute_reply.started":"2021-06-01T19:21:36.851944Z","shell.execute_reply":"2021-06-01T19:21:36.910719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We convert our excerpts to corresponding input ids using our roberta tokenizer,","metadata":{}},{"cell_type":"code","source":"input_ids = (\n    f\"{tokenizer.bos_token} \" + train.excerpt + f\" {tokenizer.eos_token}\"\n).apply(tokenizer.tokenize).apply(tokenizer.convert_tokens_to_ids).to_list()\n\ntrain['input_ids'] = input_ids","metadata":{"execution":{"iopub.status.busy":"2021-06-01T19:21:36.913291Z","iopub.execute_input":"2021-06-01T19:21:36.913563Z","iopub.status.idle":"2021-06-01T19:21:40.181088Z","shell.execute_reply.started":"2021-06-01T19:21:36.913536Z","shell.execute_reply":"2021-06-01T19:21:40.180106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And we create 5-Kfold split using random state - 2021","metadata":{}},{"cell_type":"code","source":"def create_folds(data, num_splits):\n    data[\"kfold\"] = -1\n    kf = model_selection.KFold(n_splits=num_splits, shuffle=True, random_state=2021)\n    for fold, (train_index, valid_index) in enumerate(kf.split(X=data)):\n        data.loc[valid_index, 'kfold'] = fold\n    return data\ntrain = create_folds(train, num_splits=5)","metadata":{"execution":{"iopub.status.busy":"2021-06-01T19:21:40.182457Z","iopub.execute_input":"2021-06-01T19:21:40.182775Z","iopub.status.idle":"2021-06-01T19:21:40.193866Z","shell.execute_reply.started":"2021-06-01T19:21:40.182745Z","shell.execute_reply":"2021-06-01T19:21:40.19292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are going to check the tokens overlap of train and validation set tokens for each of our folds. \n\nClick *show hidden code* to view code.","metadata":{}},{"cell_type":"code","source":"def create_features_targets(train, fold):\n    # function to separate train and validation data for a fold\n    features_train = train[train.kfold!=fold]['input_ids'].tolist()\n    features_val = train[train.kfold==fold]['input_ids'].tolist()\n    targets_train = train[train.kfold!=fold]['target'].tolist()\n    targets_val = train[train.kfold==fold]['target'].tolist()\n    return features_train, targets_train, features_val, targets_val\n\n# create train and validation features for each fold\nfeatures_train1, targets_train1, features_val1, targets_val1 = create_features_targets(train, 0)\nfeatures_train2, targets_train2, features_val2, targets_val2 = create_features_targets(train, 1)\nfeatures_train3, targets_train3, features_val3, targets_val3 = create_features_targets(train, 2)\nfeatures_train4, targets_train4, features_val4, targets_val4 = create_features_targets(train, 3)\nfeatures_train5, targets_train5, features_val5, targets_val5 = create_features_targets(train, 4)\n\n# create set of unique input ids for each train and validation data\nfeatures_train1_set, features_val1_set = set(chain.from_iterable(features_train1)), set(chain.from_iterable(features_val1))\nfeatures_train2_set, features_val2_set = set(chain.from_iterable(features_train2)), set(chain.from_iterable(features_val2))\nfeatures_train3_set, features_val3_set = set(chain.from_iterable(features_train3)), set(chain.from_iterable(features_val3))\nfeatures_train4_set, features_val4_set = set(chain.from_iterable(features_train4)), set(chain.from_iterable(features_val4))\nfeatures_train5_set, features_val5_set = set(chain.from_iterable(features_train5)), set(chain.from_iterable(features_val5))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-01T19:21:40.195Z","iopub.execute_input":"2021-06-01T19:21:40.195257Z","iopub.status.idle":"2021-06-01T19:21:40.371357Z","shell.execute_reply.started":"2021-06-01T19:21:40.19523Z","shell.execute_reply":"2021-06-01T19:21:40.370191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12, 8))\n\nplt.suptitle(\"Intersection of train tokens set and validation tokens set for each fold\")\n\nplt.subplot(231)\nvenn2(\n    [features_train1_set, features_val1_set],\n    set_labels=(\"Train\", \"Validation\"),\n    set_colors=(\"blue\", \"plum\")\n)\nplt.title('Fold 0')\n\nplt.subplot(232)\nvenn2(\n    [features_train2_set, features_val2_set],\n    set_labels=(\"Train\", \"Validation\"),\n    set_colors=(\"blue\", \"plum\")\n)\nplt.title('Fold 1')\n\nplt.subplot(233)\nvenn2(\n    [features_train3_set, features_val3_set], \n    set_labels=(\"Train\", \"Validation\"),\n    set_colors=(\"blue\", \"plum\")\n)\nplt.title('Fold 2')\n\nplt.subplot(234)\nvenn2(\n    [features_train4_set, features_val4_set], \n    set_labels=(\"Train\", \"Validation\"), \n    set_colors=(\"blue\", \"plum\")\n)\nplt.title('Fold 3')\n\nplt.subplot(235)\nvenn2(\n    [features_train5_set, features_val5_set], \n    set_labels=(\"Train\", \"Validation\"), \n    set_colors=(\"blue\", \"plum\")\n)\nplt.title('Fold 4')\n\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-01T19:21:40.372679Z","iopub.execute_input":"2021-06-01T19:21:40.373003Z","iopub.status.idle":"2021-06-01T19:21:40.85148Z","shell.execute_reply.started":"2021-06-01T19:21:40.372978Z","shell.execute_reply":"2021-06-01T19:21:40.850575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that for each fold number of unseen tokens is greater than 1700 for each split. This means that there are lots of unseen tokens in the validation set than the train set and in cases like these Freeze Embedding truly shines.","metadata":{}},{"cell_type":"markdown","source":"<font color='#3498DB'><a id=\"section23\"><h3>Implementation</h3></a></font>\n\nHere we are going to look at how we can implement Freeze Embedding. The implementation is pretty straightforward and only requires us to freeze base model embeddings.","metadata":{}},{"cell_type":"code","source":"import transformers\nfrom transformers import AutoConfig, AutoModelForSequenceClassification\n\nfreeze_embedding = True\n_pretrained_model = 'roberta-base'\n\nconfig = AutoConfig.from_pretrained(_pretrained_model)\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    _pretrained_model, config=config\n)\nmodel.base_model.embeddings.requires_grad_(not freeze_embedding)\n\nclear_output()","metadata":{"execution":{"iopub.status.busy":"2021-06-01T19:21:40.8542Z","iopub.execute_input":"2021-06-01T19:21:40.854497Z","iopub.status.idle":"2021-06-01T19:21:46.019257Z","shell.execute_reply.started":"2021-06-01T19:21:40.854467Z","shell.execute_reply":"2021-06-01T19:21:46.017821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color='#3498DB'><a id=\"section24\"><h3>Conclusion</h3></a></font>\n\nIt’s a good idea to freeze the embedding matrix when you fine-tune some pre-trained language models, especially with a large vocabulary. It won't give you superior improvements but you can try it for some Kaggle competition. \n\nThis setup looks the same as with no freezing and you can save your time for training and include larger batch sizes which is pretty important.\n\n<font color='#3498DB'><a id=\"section25\"><h3>Resources and References</h3></a></font>\n\n - [Jigsaw Multilingual Toxic Comment Classification 10th Place Solution](https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification/discussion/161100)\n - [Semi-Frozen Embeddings for NLP Transfer Learning](https://medium.com/commetric/semi-frozen-embeddings-for-nlp-transfer-learning-49fcfeec3f1b)\n - [To Tune or Not to Tune?\nAdapting Pretrained Representations to Diverse Tasks](https://arxiv.org/pdf/1903.05987.pdf)\n - [What Would Elsa Do? Freezing Layers During Transformer Fine-Tuning](https://arxiv.org/pdf/1911.03090.pdf)\n ","metadata":{}},{"cell_type":"markdown","source":"<font color='#3498DB'><a id=\"section3\"><h2>Numeric Precision Reduction</h2></a></font>\n\n<font color='#3498DB'><a id=\"section30\"><h3>Introduction</h3></a></font>\nNumeric Precision Reduction means yielding speedups through the use of floating point reduction and quantization. This is perhaps the most-general method for yielding prediction-time speedups. \n \n In past years poor support for float16 operations on GPU hardware meant that reducing the precision of weights and activations was often counter-productive, but the introduction of the NVIDIA Volta and Turing architectures with Tensor Cores means modern GPUs are now well equipped for efficient float16 arithmetic.\n\n<font color='#3498DB'><a id=\"section31\"><h3>Floating Point Representation</h3></a></font>\n Floating point types store numeric information of three types – the sign, exponent, and fraction.  Traditional float32 representations have 8 bits and 23 bits respectively to represent the exponent and fraction.  Traditional float16 representations (the format used for NVIDIA hardware) roughly halve both the exponent and fraction components of the representation. TPUs use a variant called bfloat16. \n \nMost of a transformer network can be naively converted to float16 weights and activations with no accuracy penalty.\n \n![fp16](https://www.pragmatic.ml/content/images/2020/04/image-2.png)\n\n  Small portions of the network – in particular, portions of the softmax operation – must remain in float32.  This is because the sum of a large number of small values (our logits) can be a source of accumulated error. Because both float16 and float32 values are used, this method is often referred to as \"mixed-precision\" training. \n  \nLess precise numeric representations enable speedups from two sources.\n - Native half-precision instructions\n - Larger batch sizes thanks to more compact representations \n \n<font color='#3498DB'><a id=\"section32\"><h3>How to Use?</h3></a></font>\nMixed precision primarily benefits Tensor Core-enabled architectures (Volta, Turing, Ampere). AMP shows significant (2-3X) speedup on those architectures. On earlier architectures (Kepler, Maxwell, Pascal), one may observe a modest speedup. One can run `!nvidia-smi` to display the GPU’s architecture.\n\n - [NVIDIA-apex](https://github.com/NVIDIA/apex) - NVIDIA has published a rather extensive suite of benchmarks relating to floating point precision reduction – in practice this method yields speedups up to 3x.\n - [Torch - torch.cuda.amp](https://pytorch.org/docs/stable/amp.html) - In PyTorch 1.6 release, developers at NVIDIA and Facebook moved mixed precision functionality into PyTorch core as the AMP package, torch.cuda.amp. torch.cuda.amp is more flexible and intuitive compared to apex.amp.\n \n<font color='#3498DB'><a id=\"section33\"><h3>Special Case</h3></a></font>\nOn a dataset with small batches, one should be careful with mixed precision, because it can lead to unexpected slower training if there is not enough computation to perform.\n\nFor e.g. I tried using Native AMP in this competition dataset and my results didn't improve. The reason is probably that it adds overhead and doesn’t help that much as most batches are only made of short sequences. Mixed precision helps the most with big matrix operations.\n\n<font color='#3498DB'><a id=\"section34\"><h3>Resources and References</h3></a></font>\n\n - [CommonLit Readability Prize - RoBERTa Torch|FIT](https://www.kaggle.com/rhtsingh/commonlit-readability-prize-roberta-torch-fit) - FineTuning Notebook of mine implements this for both training and validation.\n - [Tips N Tricks # 8: Using automatic mixed precision training with PyTorch 1.6\n](https://www.youtube.com/watch?v=X7iOkhGePXg) - Abhishek's YouTube Tutorial.\n - [NVIDIA Deep Learning Examples for Tensor Cores](https://github.com/NVIDIA/DeepLearningExamples) - Tons of state-of-the-Art Deep Learning examples.\n - [Torch AMP Examples](https://pytorch.org/docs/stable/notes/amp_examples.html)\n - [Torch AMP Recipes](https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html)","metadata":{}},{"cell_type":"markdown","source":"<font color='#3498DB'><a id=\"section4\"><h2>Gradient Accumulation</h2></a></font>\n![5-steps](https://miro.medium.com/max/700/1*ySxl2GQu0g07R7gWF4rizg.gif)\n<center>The 5-steps of a gradient descent optimization algorithm</center>\n<br>\nThe PyTorch code equivalent of these 5 steps can also be written in 5 lines:\n\n```python \npredictions = model(inputs)               # Forward pass  \nloss = loss_function(predictions, labels) # Compute loss function  \nloss.backward()                           # Backward pass  \noptimizer.step()                          # Optimizer step  \npredictions = model(inputs)               # Forward pass with new parameters  \n```\n\nDuring the loss.backward() operation, gradients are computed for each parameter (in green on our animation) and stored in a tensor associated to each parameter: parameter.grad (the middle graph on our animation).\n\n<font color='#3498DB'><a id=\"section40\"><h3>Introduction</h3></a></font>\nGradient Accumulation just means that, before calling optimizer.step() to perform a step of gradient descent, we will sum the gradients of several backward operations in the parameter.grad tensors. \n\nThis is straightforward to do in PyTorch as the gradient tensors are not reset unless we call `model.zero_grad()` or `optimizer.zero_grad()`.\n\nWe’ll also need to divide by the number of accumulation steps if our loss is averaged over the training samples.\n\n<font color='#3498DB'><a id=\"section41\"><h3>Implementation</h3></a></font>\nCoding the gradient accumulation part is also ridiculously easy on PyTorch. All you need to do is to store the loss at each batch and then update the model parameters only after a set number of batches that you choose.\n\nWe hold onto `optimizer.step()` which updates the parameters for `accumulation_steps` number of batches. Also, `model.zero_grad()` is called at the same time to reset the accumulated gradients.\n\nHere is a simple gist for training a model using gradient accumulation.\n\n```python \noptimizer.zero_grad()                               # Reset gradients tensors\nfor i, (inputs, labels) in enumerate(training_set):\n    predictions = model(inputs)                     # Forward pass\n    loss = loss_function(predictions, labels)       # Compute loss function\n    loss = loss / accumulation_steps                # Normalize our loss (if averaged)\n    loss.backward()                                 # Backward pass\n    if (i+1) % accumulation_steps == 0:             # Wait for several backward steps\n        optimizer.step()                            # Now we can do an optimizer step\n        optimizer.zero_grad()                           # Reset gradients tensors\n        if (i+1) % evaluation_steps == 0:           # Evaluate the model when we...\n            evaluate_model()                        # ...have no gradients accumulated\n```\n\n<font color='#3498DB'><a id=\"section42\"><h3>Resources and References</h3></a></font>\n \n - [CommonLit Readability Prize - RoBERTa Torch|ITPT](https://www.kaggle.com/rhtsingh/commonlit-readability-prize-roberta-torch-itpt) - Pretraining Kernel of mine specific to this comptition implements Accumulated Gradients.\n - [HugginFace Examples](https://github.com/huggingface/transformers/tree/master/examples) - The official repository of HuggingFace Examples which implements Gradient Accumulation in all PyTorch scripts.\n - [Gradient Accumulation: Overcoming Memory Constraints in Deep Learning](https://towardsdatascience.com/gradient-accumulation-overcoming-memory-constraints-in-deep-learning-36d411252d01)\n - [Gradient Accumulation in TensorFlow](https://gchlebus.github.io/2018/06/05/gradient-averaging.html)","metadata":{}},{"cell_type":"markdown","source":"<font color='#3498DB'><a id=\"section5\"><h2>Gradient Checkpointing</h2></a></font>\n\n<font color='#3498DB'><a id=\"section50\"><h3>Introduction</h3></a></font>\nGradient Checkpointing is a method used for reducing the memory footprint when training deep neural networks, at the cost of having a small increase in computation time.It allows trading compute for memory and hence allows training bigger/wider models and use large minibatch sizes.\n\n<font color='#3498DB'><a id=\"section51\"><h3>Idea</h3></a></font>\nThe idea is to back-propagate the gradients in small chunks along the model, trading the memory needed to store a full back propagation graph with the additional compute of a partial forward pass associated to each chunk. \n\nThis is a rather slow method as we add additional compute to reduce the memory requirements but helps in selecting larger batch size and it can be interesting in some settings.\n\n![checkpoint](https://miro.medium.com/proxy/0*udMSiPD0kZHum-sZ.)\n <center>“Memory-poor” strategy that needs O(1) memory (but requires O(n²) computation steps)</center>\n \n<font color='#3498DB'><a id=\"section52\"><h3>References and Resources</h3></a></font>\n - Official Paper - [Training Deep Nets with Sublinear Memory Cost](https://arxiv.org/abs/1604.06174v2)\n - PyTorch Doc - [torch.utils.checkpoint.checkpoint](https://pytorch.org/docs/master/checkpoint.html)\n - PyTorch Memory optimizations via gradient checkpointing - [pytorch_memonger](https://github.com/prigoyal/pytorch_memonger)\n   - Notebook - [Trading compute for memory in PyTorch models using Checkpointing](https://github.com/prigoyal/pytorch_memonger/blob/master/tutorial/Checkpointing_for_PyTorch_models.ipynb)\n - TensorFlow - [OpenAI Gradient Checkpointing](https://medium.com/tensorflow/fitting-larger-networks-into-memory-583e3c758ff9)\n - [PyTorch Checkpoint](https://github.com/csrhddlam/pytorch-checkpoint#:~:text=Gradient%20checkpointing%20is%20a%20technique%20to%20reduce%20GPU%20memory%20cost.)\n - [Training Neural Nets on Larger Batches](https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255)","metadata":{}},{"cell_type":"markdown","source":"<font color='#3498DB'><a id=\"section6\"><h2>Ending Notes</h2></a></font>\n\n- There are many more optimization strategies (in multi-GPU setups specifically) but I've found these are the most impactful ones. \n\n- As a futher research, one can explore [ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/abs/1910.02054) and subsequent papers which contains a plethora of ingenious new ideas on how one could make their hardware do much more than what it was thought possible before. The techniques described in this paper is implemented in libraries [DeepSpeed](https://github.com/microsoft/deepspeed) and [FairScale](https://github.com/facebookresearch/fairscale/) which was later on intergrated in HuggingFace v4.2.0.\n\n- HugginFace recently introduced [Accelerate](https://huggingface.co/blog/accelerate-library) which provides out of the box simplicity and support for distributed training and mixed precision.\n\n- More comprehensive repository for learning and implementing Transformers for various tasks can be found [here](https://notebooks.quantumstat.com/), [here](https://huggingface.co/transformers/master/community.html#community-notebooks) and [here](https://huggingface.co/transformers/notebooks.html)  ","metadata":{}},{"cell_type":"markdown","source":"*Note: I haven't tried all of these methods or have benchmarked for this competition yet. This notebook serves more like a guide than a solution and I'll update the results for my experiments here soon.*\n\n<font color='#3498DB'><a id=\"section2\"><h2>Thanks & Please Do Upvote!</h2></a></font>","metadata":{}}]}