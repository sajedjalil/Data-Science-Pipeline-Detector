{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<img src=\"https://images.unsplash.com/photo-1532012197267-da84d127e765?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=634&q=80\">","metadata":{}},{"cell_type":"markdown","source":"# Dependencies","metadata":{}},{"cell_type":"code","source":"%matplotlib inline \n\nimport matplotlib as mpl\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport scipy as sp\n\n#from pandas.tools.plotting import scatter_matrix\npd.set_option('display.width', 500)\npd.set_option('display.max_columns', 100)\npd.set_option('display.notebook_repr_html', True)\nimport seaborn as sns\n\nsns.set(style=\"whitegrid\")\nimport warnings\n\nwarnings.filterwarnings('ignore')\nimport math\nimport string\nimport sys\n\nimport sklearn\nfrom IPython.core.interactiveshell import InteractiveShell\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\n\nInteractiveShell.ast_node_interactivity = \"all\"\n\nimport scikitplot as skplt  # conda install -c conda-forge scikit-plot\nimport statsmodels.formula.api as sm\n\nfrom sklearn.linear_model import (ElasticNet, LassoCV, LassoLarsCV,\n                                  LinearRegression, LogisticRegression,\n                                  Perceptron, Ridge, RidgeCV, SGDClassifier)\n\nfrom sklearn.metrics import (accuracy_score, auc, confusion_matrix,\n                             mean_absolute_error, roc_curve)\nfrom sklearn.model_selection import (GridSearchCV, KFold, StratifiedKFold,\n                                     cross_val_score, learning_curve,\n                                     train_test_split)\n\n#importing from sklearn\nfrom sklearn.pipeline import make_pipeline\n\n!pip install -q textstat\n!pip install -q rich\n\nimport os\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\nimport re\nimport nltk\nimport textstat\nimport time\nimport wandb\nimport rich\nimport spacy\n\nfrom pandas import DataFrame\nfrom matplotlib.lines import Line2D\nfrom rich.console import Console\nfrom rich import print\nfrom rich.theme import Theme\nfrom nltk.corpus import stopwords\nfrom nltk import pos_tag\nfrom collections import Counter\nfrom wordcloud import WordCloud,STOPWORDS\nfrom spacy import displacy\nfrom nltk.tokenize import sent_tokenize, word_tokenize \nfrom sklearn.feature_extraction.text import CountVectorizer as CV\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error as mse\n\nnltk.download('stopwords')","metadata":{"ExecuteTime":{"end_time":"2020-02-07T04:33:05.173377Z","start_time":"2020-02-07T04:33:05.157419Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Warning! This notebook focuses on some linear algebra and basic statistics; I have never taken a course in regression, so here is all my self-learned notes, wishing to benefit others.**","metadata":{}},{"cell_type":"markdown","source":"#  Some Preliminaries","metadata":{}},{"cell_type":"markdown","source":"# Terminologies\n\n\nWe first establish that our regression model is defined as \n\n$$\n\\left|\n\\begin{array}{l}\n\\mathbf{y} = \\mathbf{X} \\mathbf{\\beta} + \\mathbf{\\varepsilon} \\\\\n \\mathbf{\\varepsilon} \\sim N(0, \\sigma^2 \\mathbf{I})\n\\end{array}\n\\right.$$\n\n\n\nConsider a data set with $m$ independent observations, $$(\\mathbf{x}^{(1)}, y^{(1)}), (\\mathbf{x}^{(2)}, y^{(2)}),..., (\\mathbf{x}^{(m)}, y^{(m)})$$\n\nAssume for a moment that we know the true model that explains the relationship between the input variables $X$ and $y$: a *multivariate linear regression* problem where the representation is $$\\mathbf{y} = \\beta_0 + \\beta_1x_1 + ...+\\beta_n x_n + \\varepsilon~~~ \\text{where } \\varepsilon \\sim^{i.i.d} N(0, \\sigma^2)$$\n\n\n\n\n\nSince there exists $m$ observations, we can write an equation for each observation:\n\n\n$$y^{(1)} = \\beta_0 + \\beta_1x_1^{(1)} + ...+\\beta_n x_n^{(1)} + \\varepsilon^{(1)}$$\n\n$$y^{(2)} = \\beta_0 + \\beta_1x_1^{(2)} + ...+\\beta_n x_n^{(2)} + \\varepsilon^{(2)}$$\n\n$$\\vdots$$\n\n$$y^{(m)} = \\beta_0 + \\beta_1x_1^{(m)} + ...+\\beta_n x_n^{(m)} + \\varepsilon^{(m)}$$\n\n\nSince linear regression model usually have a intercept term, it is necessary to include a constant variable term  $x_0 = 1$  such that our linear regression can be expressed compactly in matrix algebra form. Adding the intercept term, we have the following: \n\n\n$$y^{(1)} = \\beta_0x_0^{(1)} + \\beta_1x_1^{(1)} + ...+\\beta_n x_n^{(1)} + \\varepsilon^{(1)}$$\n\n$$y^{(2)} = \\beta_0x_0^{(2)} + \\beta_1x_1^{(2)} + ...+\\beta_n x_n^{(2)} + \\varepsilon^{(2)}$$\n\n$$\\vdots$$\n\n$$y^{(m)} = \\beta_0x_0^{(m)} + \\beta_1x_1^{(m)} + ...+\\beta_n x_n^{(m)} + \\varepsilon^{(m)}$$\n\n\nWe transform the above system of linear equations into matrix form as follows:\n\n\n\n\n\n$\\begin{bmatrix} y^{(1)}  \\\\ y^{(2)} \\\\ y^{(3)} \\\\ \\vdots \\\\y^{(m)} \\end{bmatrix}_{m \\times 1} = \\begin{bmatrix} 1 &  x_1^{(1)} & x_2^{(1)} & \\cdots & x_n^{(1)} \\\\\n                1 &  x_1^{(2)} & x_2^{(2)} & \\cdots & x_n^{(2)} \\\\ \n                \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n                1 &  x_1^{(m)} & x_2^{(m)} & \\cdots & x_n^{(m)} \\end{bmatrix}_{m \\times (n+1)} \\begin{bmatrix} \\beta_0 \\\\ \\beta_ 1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_n\\end{bmatrix}_{(n+1) \\times 1} + \\begin{bmatrix} \\varepsilon^{(1)} \\\\ \\varepsilon^{(2)} \\\\ \\varepsilon^{(3)} \\\\ \\vdots \\\\ \\varepsilon^{(m)} \\end{bmatrix}_{m \\times 1}$\n\n\n\n\n\nWe then write the above system of linear equations more compactly as $$\\mathbf{y} = \\mathbf{X}\\mathbf{\\beta} + \\mathbf{\\varepsilon} ~~~ \\text{where } \\varepsilon \\sim^{i.i.d} N(0, \\sigma^2)$$\n\n\n\nThis is assumed to be an accurate reflection of the real world. The model has a systematic component $\\mathbf{X}\\mathbf{\\beta}$ and a stochastic component $\\mathbf{\\varepsilon}$ \\footnote{Stanford: OLS in Matrix Form}. Our goal is to obtain estimates of the population parameters in the $\\mathbf{\\beta}$ vector\\footnote{do not forget that the residuals $e$ is an estimate for $\\varepsilon$.}.\n\n\n\n\n\n\n\n**Terminologies**\n\n$\\mathbf{x^{(i)}}$ **the i-th training sample:** This column vector is $$\\mathbf{x^{(i)}} = \\begin{bmatrix} x_1^{(i)} \\\\ x_2^{(i)} \\\\ \\vdots \\\\ x_n^{(i)} \\end{bmatrix}$$ where $x_j^{(i)}$ is the value of feature $j$ in the i-th training instance.\n\n\n\n\n$\\mathbf{X}$ **the Design Matrix:** Let $\\mathbf{X}$ be the design matrix of dimensions $m \\times (n+1)$ where $m$ is the number of observations (training samples) and $n$ independent feature/input variables. As seen previously, \n\n$$\\mathbf{X} = \\begin{bmatrix} (\\mathbf{x^{(1)}})^{T} \\\\ (\\mathbf{x^{(2)}})^{T} \\\\ \\vdots \\\\ (\\mathbf{x^{(m)}})^{T}\\end{bmatrix}$$\n\n\n\n\n$\\mathbf{\\beta}$ **the vector of coefficients/parameters:** The column vector $\\mathbf{\\beta}$ contains all the coefficients of the linear model.\n\n\n\n\n$\\mathbf{\\varepsilon}$ **the random vector of the error terms:** The column vector $\\mathbf{\\varepsilon}$ contains $m$ error terms corresponding to the $m$ observations. $$\\mathbf{e} = \\begin{bmatrix} \\varepsilon^{(1)} \\\\ \\varepsilon^{(2)} \\\\ \\vdots \\\\ \\varepsilon^{(m)} \\end{bmatrix}$$\n\n\n\n\n$\\mathbf{y}$ **the output vector:** The column vector $\\mathbf{y}$ contains the output for the $m$ observations. $$\\mathbf{y} = \\begin{bmatrix} y^{(1)} \\\\ y^{(2)} \\\\ \\vdots \\\\ y^{(m)} \\end{bmatrix}$$\n\n","metadata":{}},{"cell_type":"markdown","source":"## **$\\hat \\beta$ is multivariate normally distributed**\n\nWith the assumption that the error term $\\varepsilon | X \\sim N[0, \\sigma^2\\mathbf{I}]$, we derive the weaker condition \\footnote{Note that we have proved $\\mathbf{e}[\\mathbf{\\varepsilon}] = 0$ and $\\mathrm{Var}[\\mathbf{\\varepsilon}] = \\sigma^2\\mathbf{I}$ earlier by the Law of Iterated Expectation on $\\mathbf{\\varepsilon}|\\mathbf{X}$, but it is necessary to further assume that $\\mathbf{\\varepsilon}$ itself has a normal distribution, or else we will not know the distribution of $\\mathbf{\\varepsilon}$}  of $\\varepsilon \\sim N[0, \\sigma^2\\mathbf{I}]$. It is necessary to make this assumption for us to ***know the distribution of*** $\\hat{\\mathbf{\\beta}}$, because without this assumption, we will be at a loss of what distribution $\\hat{\\mathbf{\\beta}}$ can take on. We claim first that since $\\mathbf{\\varepsilon}|\\mathbf{X} \\sim N[0, \\sigma^2\\mathbf{I}]$ follows a multivariate normal distribution of mean $0$ and variance $\\sigma^2\\mathbf{I}$, then it implies that $\\hat{\\mathbf{\\beta}}$ also follows a multivariate normal distribution of mean $\\mathbf{\\beta}$ and variance $\\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1}$.","metadata":{}},{"cell_type":"markdown","source":"To justify this, we need to recall the variance-covariance matrix for $\\hat{\\mathbf{\\beta}}$ to have variance $\\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1}$\\footnote{it is necessary to know the mean and variance of $\\mathbf{\\varepsilon}$ in order to find the variance-covariance of $\\hat{\\mathbf{\\beta}}$.}, and earlier on we also established the proof of $\\hat{\\mathbf{\\beta}}$ being an unbiased estimator, which simply states that $\\mathrm{E}[\\hat{\\mathbf{\\beta}}] = \\mathbf{\\beta}$. So we know  the mean and variance of our OLS estimators $\\hat{\\mathbf{\\beta}}$, but one last question, why does $\\mathbf{\\varepsilon}$ taking on a multivariate normal distribution imply that $\\hat{\\mathbf{\\beta}}$ must also follow a multivariate normal distribution?","metadata":{}},{"cell_type":"markdown","source":"This is a consequence of that $\\hat{\\mathbf{\\beta}}$ is a linear estimator, recall that we are treating $\\mathbf{\\beta}$ and $\\mathbf{X}$ as constant and non-stochastic, and consequently $\\hat{\\mathbf{\\beta}}$ is linear function of the error vector $\\mathbf{\\varepsilon}$. $$\\hat{\\mathbf{\\beta}} = \\mathbf{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{\\varepsilon} = g(\\mathbf{\\varepsilon})$$\n\nAnd using the property that ***a linear transform of a normal random variable is normal***, we can easily see that $\\hat{\\mathbf{\\beta}}$ must be normal as well since $\\hat{\\mathbf{\\beta}}$ is a linear transformation of $\\mathbf{\\varepsilon}$.","metadata":{}},{"cell_type":"markdown","source":"Thus so far we established the least squares parameters $\\hat{\\mathbf{\\beta}}$ is normally distributed with mean $\\mathbf{\\beta}$ and variance $\\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1}$. More formally, we say that $\\hat{\\mathbf{\\beta}}$ follows a **multi-variate normal distribution** because $\\hat{\\mathbf{\\beta}}$ is a random vector, the distinction is that the variance of $\\hat{\\mathbf{\\beta}}$ is now a variance-covariance matrix instead of just a variance vector. But recall we do not know the real value of $\\sigma^2$ and therefore we replace $\\sigma^2$ with the **unbiased variance estimator of the error variance, which we define and name it to be the Mean Squared Error (MSE)** It is useful to remind yourself that we are learning ML, although MSE has a slightly different formula from the MSE in sklearn, the end goal is the same, refer to [Confusion on MSE](https://stats.stackexchange.com/questions/448005/confusion-on-mean-squared-error-for-regression).","metadata":{}},{"cell_type":"markdown","source":"# CommonLit Readability Prize\n\nI took the pre-processing function from my teammate's notebook [here](https://www.kaggle.com/ruchi798/commonlit-readability-prize-eda-baseline), go give her notebook a look if you haven't for all the EDA you need!!! I won't talk more about it here as hers has a full coverage!","metadata":{}},{"cell_type":"code","source":"sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n\ndef custom_palette(custom_colors):\n    customPalette = sns.set_palette(sns.color_palette(custom_colors))\n    sns.palplot(sns.color_palette(custom_colors),size=0.8)\n    plt.tick_params(axis='both', labelsize=0, length = 0)\n\npalette = [\"#7209B7\",\"#3F88C5\",\"#136F63\",\"#F72585\",\"#FFBA08\"]\npalette2 = sns.diverging_palette(120, 220, n=20)\ncustom_palette(palette)\n\ncustom_theme = Theme({\n    \"info\" : \"italic bold cyan\",\n    \"warning\": \"italic bold magenta\",\n    \"danger\": \"bold blue\"\n})\n\nconsole = Console(theme=custom_theme)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(\"../input/commonlitreadabilityprize/train.csv\")\ntest_df = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")\n#====== Preprocessing function ======\ndef preprocess(data):\n    excerpt_processed=[]\n    for e in data['excerpt']:\n        \n        # find alphabets\n        e = re.sub(\"[^a-zA-Z]\", \" \", e)\n        \n        # convert to lower case\n        e = e.lower()\n        \n        # tokenize words\n        e = nltk.word_tokenize(e)\n        \n        # remove stopwords\n        e = [word for word in e if not word in set(stopwords.words(\"english\"))]\n        \n        # lemmatization\n        lemma = nltk.WordNetLemmatizer()\n        e = [lemma.lemmatize(word) for word in e]\n        e=\" \".join(e)\n        \n        excerpt_processed.append(e)\n        \n    return excerpt_processed\n        \ntrain_df[\"excerpt_preprocessed\"] = preprocess(train_df)\ntest_df[\"excerpt_preprocessed\"] = preprocess(test_df)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import functools\nimport numpy as np\nfrom sklearn.exceptions import NotFittedError\n\n\ndef NotFitted(func):\n    @functools.wraps(func)\n    def wrapper(self, *args, **kwargs):\n        if not self._fitted:\n            print(self._fitted)\n            raise NotFittedError  # or you can define custom error NotFittedError()\n        else:\n            print(self._fitted)\n            return func(self, *args, **kwargs)\n\n    return wrapper\n\n\ndef MSE(y_true: np.ndarray, y_pred: np.ndarray):\n    mse = np.sum(np.square(y_true - y_pred))\n    return mse\n\n\nclass MyLinearRegression:\n    \"\"\"\n    Linear Regression class generalized to n-features. For description, read the method fit.\n\n    ...\n\n    Attributes\n    ----------\n    coef_ : float\n        the coefficient vector\n\n    intercept_ : float\n        the intercept value\n\n    has_intercept : bool\n        whether to include intercept or not\n\n    _fitted: bool\n        a flag to turn to true once we called fit on the data\n\n    Methods\n    -------\n    fit(self, X: np.ndarray = None, y_true: np.ndarray = None):\n        fits the model and calculates the coef and intercept.\n    \"\"\"\n\n    def __init__(\n        self,\n        has_intercept: bool = True,\n        solver: str = \"Closed Form Solution\",\n        learning_rate: float = 0.1,\n        num_epochs: int = 1000,\n    ):\n        super().__init__()\n        \"\"\"\n        Constructs all the necessary attributes for the LinearRegression object.\n\n        Parameters\n        ----------\n            has_intercept : bool\n                whether to include intercept or not\n            \n            solver: str\n                {\"Closed Form Solution\", \"Gradient Descent\"}\n                if Closed Form Solution: closed form solution for finding optimal parameters of beta\n                                         recall \\vec{beta} = (X'X)^{-1}X'Y ; note scikit-learn uses a slightly different way.\n                                         https://stackoverflow.com/questions/66881829/implementation-of-linear-regression-closed-form-solution/66886954#66886954\n        \"\"\"\n\n        self.solver = solver\n        self.has_intercept = has_intercept\n        self.learning_rate = learning_rate\n        self.num_epochs = num_epochs\n\n        self.coef_ = None\n        self.intercept_ = None\n        self._fitted = False\n        self.optimal_betas = None\n\n    def _init_weights(self, X: np.ndarray):\n        \"\"\"\n        To be included for Gradient Descent\n        \"\"\"\n        n_features = X.shape[1]\n        ### init with all 0s\n        initial_weights = np.zeros(shape=(n_features))  # 1d array\n        return initial_weights\n\n    def check_shape(self, X: np.ndarray, y_true: np.ndarray):\n        \"\"\"\n        Check the shape of the inputs X & y_true\n\n        if X is 1D array, then it is simple linear regression, reshape to 2D\n        [1,2,3] -> [[1],[2],[3]] to fit the data\n\n                Parameters:\n                        X (np.ndarray): 2D numpy array (n_samples, n_features). Input Matrix of size m by n; where m is the number of samples, and n the number of features.\n\n                        y_true (np.ndarray): 1D numpy array (n_samples,). Input ground truth, also referred to as y_true of size m by 1.\n\n                Returns:\n                        self: Method for chaining\n\n                Examples:\n                --------\n                        >>> see main\n\n                Explanation:\n                -----------\n\n        \"\"\"\n\n        if X is not None and len(X.shape) == 1:\n            X = X.reshape(-1, 1)\n        return X, y_true\n\n    def degrees_of_freedom(self, X: np.ndarray, y_true: np.ndarray):\n        # degrees of freedom of population dependent variable variance\n        self._dft = self._features.shape[0] - 1\n        # degrees of freedom of population error variance\n        self._dfe = self._features.shape[0] - self._features.shape[1] - 1\n\n    def fit(self, X: np.ndarray = None, y_true: np.ndarray = None):\n        \"\"\"\n        Does not return anything. Instead it calculates the optimal beta coefficients for the Linear Regression Model. The default solver will be the closed formed solution\n        B = (XtX)^{-1}Xty where we guarantee that this closed solution is unique, provided the invertibility of XtX. This is also called the Ordinary Least Squares Estimate\n        where we minimze the Mean Squared Loss function to get the best beta coefficients which gives rise to the least loss function.\n\n\n                Parameters:\n                        X (np.ndarray): 2D numpy array (n_samples, n_features). Input Matrix of size m by n; where m is the number of samples, and n the number of features.\n\n                        y_true (np.ndarray): 1D numpy array (n_samples,). Input ground truth, also referred to as y_true of size m by 1.\n\n                Returns:\n                        self (MyLinearRegression): Method for chaining, as you must call .fit first on the LinearRegression class.\n                                                   https://stackoverflow.com/questions/36250990/return-self-in-python\n\n                Examples:\n                --------\n                        >>> see main\n\n                Explanation:\n                -----------\n\n        \"\"\"\n        #print(X.shape)\n        X, y_true = self.check_shape(X, y_true)\n        n_samples, n_features = X.shape[0], X.shape[1]\n\n        # add a column of ones if there exists an intercept: recall this is needed for intercept beta_0 whereby each sample is y_i = b1x1+b2x2+...+b0(1)\n        if self.has_intercept:\n            #print(X)\n            X = np.insert(X, 0, 1, axis=1)  # np.c_[np.ones(n_samples), X]\n\n        if self.solver == \"Closed Form Solution\":\n\n            XtX = np.transpose(X, axes=None) @ X\n            det = np.linalg.det(XtX)\n            if det == 0:\n                print(\"Singular Matrix, Recommend to use SVD\")\n\n            XtX_inv = np.linalg.inv(XtX)\n            Xty = np.transpose(X, axes=None) @ y_true\n            self.optimal_betas = XtX_inv @ Xty\n\n        elif self.solver == \"Batch Gradient Descent\":\n            self.optimal_betas = self._init_weights(X)\n            for epoch in range(self.num_epochs):\n                y_pred = X @ self.optimal_betas\n                MSE_LOSS = MSE(y_true, y_pred)\n                GRADIENT_VECTOR = (2 / n_samples) * -(y_true - y_pred) @ X\n                # yet another vectorized operation\n                self.optimal_betas -= self.learning_rate * GRADIENT_VECTOR\n                if epoch % 100 == 0:\n                    print(\"EPOCH: {} | MSE_LOSS : {}\".format(epoch, MSE_LOSS))\n\n        # set attributes from None to the optimal ones\n        self.coef_ = self.optimal_betas[1:]\n        self.intercept_ = self.optimal_betas[0]\n        self._fitted = True\n\n        return self\n\n    @NotFitted\n    def predict(self, X: np.ndarray):\n        \"\"\"\n        Predicts the y_true value given an input of X.\n\n\n                Parameters:\n                        X (np.ndarray): 2D numpy array (n_samples, n_features).\n\n                Returns:\n                        y_hat: y_pred\n\n                Examples:\n                --------\n                        >>> see main\n\n                Explanation:\n                -----------\n\n        \"\"\"\n        if self.has_intercept:\n            # y_pred = self.intercept_ + X @ self.coef_\n            X = np.insert(X, 0, 1, axis=1)\n            y_pred = X @ self.optimal_betas\n        else:\n            y_pred = X @ self.coef_\n\n        return y_pred\n\n    @NotFitted\n    def residuals(self, X: np.ndarray, y_true: np.ndarray):\n        self._residuals = y_true - self.predict(X)\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_HONGNAN = MyLinearRegression(learning_rate=0.1, has_intercept=True, solver=\"Batch Gradient Descent\", num_epochs=6666)\nSKLEARN_LR = LinearRegression()\n\nX = train_df[\"excerpt_preprocessed\"]\ny = train_df['target']\nvectorizer = TfidfVectorizer(binary=True, ngram_range=(1,1))\nX = vectorizer.fit_transform(X)\nimport scipy\nX=scipy.sparse.csr_matrix.toarray(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SKLEARN_LR.fit(X_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = SKLEARN_LR.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MSE = mse(y_test, y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MSE","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_HONGNAN = MyLinearRegression(learning_rate=0.1, has_intercept=True, solver=\"Batch Gradient Descent\", num_epochs=66666).fit(X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TEST_vectorizer = TfidfVectorizer(binary=True, ngram_range=(1,1))\nTEST = vectorizer.fit_transform(test_df[\"excerpt_preprocessed\"])\nTEST=scipy.sparse.csr_matrix.toarray(TEST)\nTEST_PRED = lr_HONGNAN.predict(TEST)\npredictions = pd.DataFrame()\npredictions['id'] = test_df['id']\npredictions['target'] = test_pred\npredictions.to_csv(\"/kaggle/working/submission.csv\", index=False)\npredictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def training_all(model,X,y):\n    \n    model = make_pipeline(\n        TfidfVectorizer(binary=True, ngram_range=(1,1)),\n        model,\n    )\n    print(X.shape)\n    model.fit(X, y)\n    print(X.shape)\n    print(test_df[\"excerpt_preprocessed\"].shape)\n    y_pred = model.predict(test_df[\"excerpt_preprocessed\"])\n    \n    return y_pred\n\nX = train_df[\"excerpt_preprocessed\"]\ny = train_df['target']\nlr = LinearRegression()\ntest_pred = training_all(lr,X,y)\npredictions = pd.DataFrame()\npredictions['id'] = test_df['id']\npredictions['target'] = test_pred\npredictions.to_csv(\"/kaggle/working/submission.csv\", index=False)\npredictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# House Pricing Example\n\nStill a good read if you are looking to gain an intuition on the core idea, hypothesis testing for Linear Regression.","metadata":{}},{"cell_type":"markdown","source":"##  **The MSE and Distribution of** $\\hat{\\mathbf{\\beta}}$\n\n\n$\n\\begin{align}\n\\text{MSE, the unbiased estimator of error term} & = \\hat{\\sigma}^2  \\\\\n & = \\dfrac{1}{m-(n+1)}\\sum_{i=1}^m\\left( y^{(i)}-\\hat y^{(i)}\\right)^2\\\\\n &= \\dfrac{1}{m-(n+1)}(\\mathbf{y} - X\\mathbf{\\beta})^T(y-X\\mathbf{\\beta})\\\\\n &= \\dfrac{1}{m-(n+1)}(\\mathbf{e})^T(\\mathbf{e})\n\\end{align}\n$\n\n\n<br>\n\nConsequently, \n\n\nDistribution of $\\hat{\\mathbf{\\beta}}$ is given by:\n\n\n\n**Distribution of** $\\hat{\\mathbf{\\beta}}$\n$$\\hat{\\mathbf{\\beta}} \\sim N\\left(\\mathbf{\\beta},  \\hat{\\sigma}^2(\\mathbf{X}^T\\mathbf{X})^{-1}\\right)$$\n\n\n\n\n\nNow that we know the distribution of $\\mathbf{\\beta}$, we can conduct our favourite hypothesis tests and the likes! ","metadata":{}},{"cell_type":"markdown","source":"# Simple Linear Regression Approach:  Conducting the Hypothesis Test on the Population Parameter $\\mathbf{\\beta}$\n\nAs with any textbooks, we usually look at the simple regression form and gain a good understanding before we generalize to the multi-variate regression, because it is easier to ingest the intuition with only 2 coefficients, and later on we can generalize to the n-dimensional space of multi-variate regression with ease.\n\n\nDisclaimer: This topic is mostly based on the explanations from the course from [PSU STAT 462](https://online.stat.psu.edu/stat462), I also take, explain, copy some ideas from various textbooks including the famous book [Introduction to Statistical Learning](http://faculty.marshall.usc.edu/gareth-james/ISL/). I have very little statistics background, but in an attempt to really understand ML, I gathered and compiled valuable information from different books and wrote them down. So, if there are some paragraphs that are copied verbatim, it is entirely credited to these textbooks, I really want to share some of the very important chunk of information written by them, so much that I do not want to distort their words in my own ones.\n\n","metadata":{}},{"cell_type":"markdown","source":"Recall that we are ultimately always ***interested in drawing conclusions about the population, not the particular sample we observed.*** \\footnote{This is an important sentence to understand, the reason we are testing our hypothesis on the population parameter instead of the estimated parameter is because we are interested in knowing our real population parameter, and we are using the estimated parameter to provide some statistical gauge, this will be explained more later.} In the simple regression setting, we are often interested in learning about the population intercept $\\beta_0$ and the population slope $\\beta_1$. As you know, confidence intervals and hypothesis tests are two related, but different, ways of learning about the values of population parameters. Here, we will learn how to calculate confidence intervals and conduct ***different hypothesis tests*** for both $\\beta_0$ and $\\beta_1$.","metadata":{}},{"cell_type":"markdown","source":"## A look at the dataset","metadata":{}},{"cell_type":"markdown","source":"Let us look at the dataset concerning ***only*** the relationship between `GrLivArea` (the predictor variable) and the `SalePrice` (the response variable). From now on I will refer `GrLivArea` as just `house area` for simplicity.\n\nThe response variable $\\mathbf{y}$ is the the property's sale price in dollars. This is the target variable that you're trying to predict.\n\nThe predictor variable $\\mathbf{X}$ is the Above grade (ground) living area square feet, in other words, it is the house's area (excluding the basement). \n\nLet us look at the data below.","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/train.csv\")\n\n\ntrain.loc[:, ['GrLivArea', 'SalePrice']]","metadata":{"ExecuteTime":{"end_time":"2020-02-07T09:01:13.212076Z","start_time":"2020-02-07T09:01:13.156226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We easily plot the OLS regression line as follows using the python code.","metadata":{}},{"cell_type":"code","source":"# let our predictor be LotArea in square feet, basically it your house's area above ground level, non americans can treat\n# this predictor as house's area since most of the world don't have basements -.-\nX = train.loc[:,['GrLivArea']]\n\n# let our target be the sale price, be careful that y must be a series of values extracted this way\n\ny = train['SalePrice']\n\n\n\nlr = LinearRegression()\nlr.fit(X,y)\n\n\n  \n\nfigure, lotarea = plt.subplots(figsize=(20,10), dpi = 100)\nlotarea.scatter(X,y,color='red',s = 5)\nlotarea.plot(X, lr.predict(X), color = 'blue')\nlotarea.set_title('Fitted Line for House Area vs Sale Price', \n        fontsize = 20)\nlotarea.set_xlabel('GrLivArea/House Area (sq feet)', fontsize = 15)\nlotarea.set_ylabel('SalePrice ($))', fontsize = 15)","metadata":{"ExecuteTime":{"end_time":"2020-02-07T09:01:15.116792Z","start_time":"2020-02-07T09:01:14.739776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import display, Math\n\nprint('The intercept term \\N{GREEK SMALL LETTER BETA}\\N{SUBSCRIPT ZERO}\\u0302 is:', lr.intercept_)\nprint('The intercept term \\N{GREEK SMALL LETTER BETA}\\N{SUBSCRIPT ONE}\\u0302 is:', lr.coef_)\n\nols_parameters = [lr.intercept_, lr.coef_[0]]","metadata":{"ExecuteTime":{"end_time":"2020-02-07T09:01:41.37201Z","start_time":"2020-02-07T09:01:41.365049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## A little intuition on why we need to use hypothesis testing","metadata":{}},{"cell_type":"markdown","source":"As we can see above from both the fitted plot and the OLS coefficients, there does seem to be a linear relationship between the two. Furthermore, the OLS regression line's equation can be easily calculated and given by $$\\hat{\\mathbf{y}} = 18569 + 107\\mathbf{X}$$\n\nAnd so we know the estimated slope parameter $\\hat\\beta_1$ is $107$, and apparently there exhibits a \"relationship\" betwen $\\mathbf{\\varepsilon}$ and the $\\mathbf{X}$ and $\\mathbf{y}$. Remember, if there is no relationship, $\\hat{\\beta}$ should be 0. But be careful, although we can be certain that there is a relationship between `house area` and the `sale price`, but it is only limited to the 1460 samples that we have!\n\nIn fact, we want to know if there is a relationship between the ***population*** of all of the `house area` and its corresponding `sale price`. That is, we want to know if the true population slope $\\beta_1$ is unlikely to be 0 as well. \\footnote{Recall that 0 is a common benchmark we use in linear regression, but it, in fact can be any number.}. This is why we have to draw inferences from $\\hat\\beta_1$ to make substantiate conclusion on the true population slope $\\beta_1$.\n","metadata":{}},{"cell_type":"markdown","source":"Let us formulate our question/hypothesis by asking the question: **Do our `house area` and `sale price` exhibit a true linear relationship in our population? Can we make inferences of our true population parameters based on the estimated parameters (OLS estimates)?**\n\nThus, we can use the infamous scientific method Hypothesis Testing by defining our null hypothesis to be $\\beta_1 = 0$ and the alternative hypothesis to be $\\beta_1 \\neq 0$.\n\n\n\n\\begin{equation}\\label{eq:}\n\\text{Null Hypothesis: } H_0: \\beta_1 = 0\\\\\n\\text{Alternative Hypothesis: } H_1: \\beta_1 \\neq 0\n\\end{equation}\n\n\nBasically, the null hypothesis says that $\\beta_1 = 0$, indicating that there is no relationship between $X$ and $y$. Indeed, if $\\beta_1 = 0$, our original model reduces to $y = \\beta_0 + \\varepsilon$, and this shows $X$ does not depend on $y$ at all. To test the null hypothesis, we need to determine whether $\\hat\\beta_1$, our OLS estimate for $\\beta_1$, is ***sufficiently far from 0*** so that we are \\textbf{confident} that the real parameter $\\beta_1$ is non-zero.","metadata":{}},{"cell_type":"markdown","source":"### A little more intuition","metadata":{}},{"cell_type":"markdown","source":"Now here is the intuition, yet again, we need to gain an understanding first before we go into further mathematical details. We already established subtly that even if we can easily calculate $\\hat\\beta_1$, and it turns out to be non-zero, how can we feel **\"safe\"** that our true parameter $\\beta_1$ is also non-zero? Well if you think deeper, it all boils down to whether our $\\hat{\\beta_1}$ is a \\textbf{good estimate} of $\\beta_1$; and this can be quantified by checking the standard error of $\\hat{\\beta_1}$. By definition, we should remember the standard error of $\\hat{\\beta_1}$ means the amount of dispersion of the various $\\hat\\beta_1$ around its mean (recall the mean of $\\hat\\beta_1$ is $\\beta_1$). So if $\\text{SE}(\\hat{\\beta_1})$ is small, this means $\\hat{\\beta_1}$ does not vary much around its mean and recall that the mean of $\\hat{\\beta_1}$ is none other than $\\beta_1$ due to the unbiasedness property of OLS estimator; consequently, it is quite \"safe\" to say that which ever sample you take from the population, the $\\hat\\beta_1$ you calculated should not fluctuate much from the mean $\\beta_1$, indicating that $\\beta_1$ and $\\hat\\beta_1$ are quite \"close to each other\".\n\n<br>\n\n\nBut here is the catch yet again, how small is small? If for example, $\\hat{\\beta_1} = 0.5$ but if $\\text{SE}(\\hat{\\beta_1}) = 0.499$, then this standard error is not small relative to the value of $\\hat{\\mathbf{\\beta}}$. $\\hat{\\beta_1}$. One should understand in a rough sense that the standard error of $\\hat{\\mathbf{\\beta}}$ is the **average amount that this particular $\\hat{\\mathbf{\\beta}}$ differs from the actual value of $\\mathbf{\\beta}$.** So if for example, the above example says that the average amount that this $\\hat{\\beta_1}$ differs from the real $\\beta_1$ is 0.499, and this is not very promising because it means that our real $\\beta_1$ could be $0.5-0.499 = 0.001$, which is very close to $0$, we do not really feel safe about our $\\hat{\\beta_1}$ since it can still suggest that our real $\\beta_1$ is near 0 and we cannot reject the null hypothesis that $\\beta_1 = 0$.\n\n\n\nConversely, if our standard error of $\\hat{\\beta_1}$ is very large, say equals to 10000, this does not necessarily mean our $\\hat{\\beta_1}$ is an inaccurate estimate of $\\beta_1$, because if our $\\beta_1$ is so large relative to 10000, say our $\\beta_1 = 1000000000000$, then this standard error is small relatively and we still have strong evidence to reject the null hypothesis. \\footnote{In a similar vein as the previous example, on average, we should expect our $\\beta_1$ to be around $1000000000000 \\pm 10000$, which is nowhere close to 0.}.\n\n\n\nSo we also must find a metric to calculate how small is small, or conversely, how big is big. Therefore, we n\\mathbf{\\varepsilon}d to understand that if $\\text{SE}(\\hat{\\beta_1}) <<< \\hat{\\beta_1}$, that is, standard error of the estimator $\\hat{\\beta_1}$ is way smaller than $\\hat{\\beta_1}$, then it is safe to say that our $\\hat{\\beta_1}$ provides strong evidence that the real parameter is non-zero as well.  \n\n\n\n\nFor example, if $\\hat{\\beta_1} = 0.5$ and $\\text{SE}(\\hat{\\beta_1}) = 0.0001$, then even though $\\hat{\\beta_1}$ is quite near $0$, we can still feel confident to say that the real parameter $\\beta_1$ is not 0, simply because our estimate $\\hat{\\beta_1}$ has such a small standard error/variation that it is very unlikely for our $\\beta_1$ to be 0.\n\n\nWhat I have described above is merely an intuition, talk is cheap and we need to use scientific methods to quantify it. To quantify what I have said above, we use t-statistics to calculate the number of standard deviations that $\\hat{\\beta_1}$ is away from $0$. In fact the formula allows any number, not limited to 0.","metadata":{}},{"cell_type":"markdown","source":"## T-Statistics","metadata":{}},{"cell_type":"markdown","source":"**T-Statistics**\n\nIn statistics, the t-statistic is the ratio of the difference of the estimated value of a parameter from its hypothesized value to its standard error. A good intuitive of [explanation of t-statistics can be read here](https://blog.minitab.com/blog/statistics-and-quality-data-analysis/what-is-a-t-test-and-why-is-it-like-telling-a-kid-to-clean-up-that-mess-in-the-kitchen).\n\n\nLet $\\hat{\\mathbf{\\beta}}$ be an estimator of $\\mathbf{\\beta}$ in some statistical model. Then a t-statistic for this parameter $\\mathbf{\\beta}$ is any quantity of the form $$t_{\\hat{\\mathbf{\\beta}}} = \\dfrac{\\hat{\\mathbf{\\beta}} - \\mathbf{\\beta}_H}{\\text{SE}(\\hat{\\mathbf{\\beta}})}$$\n\nwhere $\\mathbf{\\beta}_H$ is the value we want to test in the hypothesis. By default, statistical software sets $\\mathbf{\\beta}_H = 0$.\n\n\n\nIn the regression setting, we further take note that each individual coefficient is given by $$t_{\\hat{\\mathbf{\\beta}}_i} = [t_{\\hat{\\mathbf{\\beta}}}]_{(i+1) \\times (i+1)}$$\n\n\n","metadata":{}},{"cell_type":"markdown","source":"In our setting, our $\\beta_H = 0$ since we are testing whether our slope $\\beta_1 = 0$ or not. To reiterate, our t statistics reduces to $$t_{\\hat{\\mathbf{\\beta_1}}} = \\dfrac{\\hat{\\mathbf{\\beta_1}}}{\\text{SE}(\\hat{\\mathbf{\\beta_1}})}$$\nwhich is simply the ratio of $\\hat{\\mathbf{\\beta}}$ to its standard error $\\text{SE}(\\hat{\\mathbf{\\beta_1}})$. This ratio measures the number of standard deviations that $\\hat{\\beta_1}$ is away from $\\beta_H = 0$. Consequently, ***If our null hypothesis is really true, that $\\beta_1 = 0$, then if we calculate our t-value to be 0 (i.e. we can understand it as the number of standard deviations that $\\hat{\\beta_1}$ is 0, which means the $\\hat{\\beta_1}$ is 0. In which case we accept the null hypothesis; on the other hand, if our t-value is non 0, it means that $\\hat{\\beta_1} \\neq 0$) ***\n\nConsequently, we can conclude that greater the magnitude of $|t|$ (t can be either positive\nor negative), the greater the evidence to reject the null hypothesis. The closer $t$ is to 0,\nthe more likely there isn’t a significant evidence to reject the null hypothesis.","metadata":{}},{"cell_type":"markdown","source":"### Implementing T-value with Python","metadata":{}},{"cell_type":"markdown","source":"To find the t-value of $\\hat{\\beta_1}$, we need to first know what is the standard error of $\\hat{\\beta_1}$, which is $\\text{SE}(\\hat{\\mathbf{\\beta_1}})$. The formula is given by \n\n**Standard Error**\n\n$\\text{SE}(\\hat{\\beta_1}) = \\sqrt{\\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1}}$\n\\end{definition}\n\nBut since we do not know the value of the population variance $\\sigma^2$ of the error term, we estimate it with $$\\hat{\\sigma^2} = \\dfrac{\\text{Residual Sum of Squares (RSS)}}{m-(n+1)}$$\n\nThe python code is implemented as follows.","metadata":{}},{"cell_type":"code","source":"from scipy import stats\n\nm = X.shape[0]      # number of training samples\nn = X.shape[1]      # number of features\nX_with_intercept = np.c_[np.ones(X.shape[0]), X]\nnp.linalg.inv(np.dot(X_with_intercept.T, X_with_intercept)).shape\n\n# RSE = sqrt(RSS/(m-n))\n# thus, sigma square estimate is RSS/(m-n)\n\nsigma_square_hat = np.linalg.norm(y - lr.predict(X)) ** 2 / (m-(n+1)) \nvar_beta_hat = sigma_square_hat * np.linalg.inv(np.dot(X_with_intercept.T, X_with_intercept))\n\nfor i in range(n+1):\n    standard_error = var_beta_hat[i,i] ** 0.5   # standard error for beta_0 and beta_1\n    print(f\"Standard Error of (beta_hat[{i}]): {standard_error}\")\n    \n\n    \n    t_values = ols_parameters[i]/standard_error\n    print(f\"t_value of (beta_hat[{i}]): {t_values}\")\n    \n        \n    print(\"━\"*60)\n    print(\"━\"*60)\n","metadata":{"ExecuteTime":{"end_time":"2020-02-07T09:03:04.703202Z","start_time":"2020-02-07T09:03:04.691255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Well, we are only interested in the t-value of $\\hat{\\beta_1}$, which shows a value of 38.36. Quoting from [the author Jim Frost's book, he mentioned that](https://statisticsbyjim.com/hypothesis-testing/t-tests-t-values-t-distributions-probabilities/) the tricky thing about t-values is that they are a unitless statistic, which makes them difficult to interpret on their own. We performed a t-test and it produced a t-value of 38.36. What does this t-value mean exactly? We know that the sample mean doesn’t equal the null hypothesis value because this t-value doesn’t equal zero. However, we don’t know how exceptional our value is if the null hypothesis is correct.\n\nThis is why we need to look at the P-values of the t-test. But before that, we look at t-distributions first.","metadata":{}},{"cell_type":"markdown","source":"## T-distribution","metadata":{}},{"cell_type":"markdown","source":"How T-distribution is formed:\n\n1. Define our null and alternative hypothesis. In Simple Linear Regression, we simply set: $$H_0: \\beta_1  = 0$$ $$H_{A}: \\beta_1 \\neq 0$$\n\n\n2. Determine the test statistic that is most suitable for this parameter $\\beta_1$. Note that $\\beta_1$ is actually a constant and not a random variable, and we cannot know the distribution of $\\beta_1$ anyways since population is unknown. However, we can make use of the OLS estimator of $\\beta_1$ to draw inferences about it. So although we do not know $\\beta_1$ we are able to derive the sampling distribution of $\\hat{\\beta_1}$ which we have shown in the first section to be $$\\hat{\\mathbf{\\beta}} \\sim N\\left(\\mathbf{\\beta},  \\hat{\\sigma}^2(\\mathbf{X}^T\\mathbf{X})^{-1}\\right)$$\n\n\nWe could jolly well define $\\hat{\\beta_1}$ to be our test statistic and calculate $P(\\hat{\\beta_1} \\geq B_H)$ where $B_H$ is the hypothesis we want to test, if not for the fact that we do not even know the value of $\\sigma^2$ - which we desperately need because it appeared in the variance of $\\hat{\\beta_1}$. So this is the reason why we need to use an equivalent test statistic $$t_{\\hat{\\mathbf{\\beta_1}}} = \\dfrac{\\hat{\\mathbf{\\beta_1}} - \\mathbf{\\beta_1}_H}{\\text{SE}(\\hat{\\mathbf{\\beta_1}})} \\sim t(df)$$ \n\nThis means that if our $\\hat{\\beta_1}$ follows a normal distribution of $\\hat{\\mathbf{\\beta}} \\sim N\\left(\\mathbf{\\beta},  \\hat{\\sigma}^2(\\mathbf{X}^T\\mathbf{X})^{-1}\\right)$, then an equivalent distribution is $t_{\\hat{\\mathbf{\\beta_1}}} \\sim t(df)$. \n\nSo it is clear that for any $a, b \\in \\mathbb{R}$, finding $P(a \\leq \\hat{\\beta_1} \\leq b)$ is the same as finding $P\\left(\\dfrac{a}{\\text{SE}(\\hat{\\beta_1})} \\leq t_{\\hat{\\mathbf{\\beta_1}}} \\leq \\dfrac{b}{\\text{SE}(\\hat{\\beta_1})} \\right)$.\n\n\n3. So finally, we have formed a sampling distribution of t-statistics, hinged on the assumption that ***our null hypothesis is true, $\\beta_1 = 0$ (this is important).***\n\n\n\n\n\n[Jim Statistics](https://statisticsbyjim.com/hypothesis-testing/t-tests-t-values-t-distributions-probabilities/) provides an intuitive idea.","metadata":{}},{"cell_type":"markdown","source":"## p-values","metadata":{}},{"cell_type":"markdown","source":"\n\nWe use the resulting test statistic to calculate the p-value. As always, the p-value is the answer to the question \"how likely is it that we’d get a test statistic $t_{\\hat{\\mathbf{\\beta}}}$ as extreme as we did if the null hypothesis were true?\" The p-value is determined by referring to a t-distribution with $m-(n+1) = m-2$ degrees of freedom where we recall that $m$ is the number of data, and $n$ is the number of variables. The $n+1$ is because we are actually fitting 2 parameters, so we add on 1 intercept term which has a dummy variable $x_0$.\n\n\n\n\nFinally, we make a decision:\n\n\n1. If the p-value is smaller than the significance level $\\alpha$, we reject the null hypothesis in favor of the alternative. We conclude \"there is sufficient evidence at the $\\alpha$ level to conclude that there is a linear relationship in the population between the predictor x and response y.\"\n    \n    \n2. If the p-value is larger than the significance level $\\alpha$, we fail to reject the null hypothesis. We conclude \"there is not enough evidence at the $\\alpha$ level to conclude that there is a linear relationship in the population between the predictor x and response y.\"\n\n\n\n\nLet us implement our p-values.","metadata":{}},{"cell_type":"code","source":"from scipy import stats\n\nm = X.shape[0]      # number of training samples\nn = X.shape[1]      # number of features\nX_with_intercept = np.c_[np.ones(X.shape[0]), X]\n\n\n# RSE = sqrt(RSS/(m-n))\n# thus, sigma square estimate is RSS/(m-n)\n\n\n\n\n\nsigma_square_hat = np.linalg.norm(y - lr.predict(X)) ** 2 / (m-(n+1))\nvar_beta_hat = sigma_square_hat * np.linalg.inv(np.dot(X_with_intercept.T, X_with_intercept))\n\nfor i in range(n+1):\n    standard_error = var_beta_hat[i,i] ** 0.5   # standard error for beta_0 and beta_1\n    print(f\"Standard Error of (beta_hat[{i}]): {standard_error}\")\n    \n\n    \n    t_values = ols_parameters[i]/standard_error\n    print(f\"t_value of (beta_hat[{i}]): {t_values}\")\n    \n\n    \n    p_values = 1 - stats.t.cdf(abs(t_values), df= X.shape[0] -(X.shape[1] + 1))\n    print(f\"p_value of (beta_hat[{i}]): {p_values}\")   \n    \n    print(\"━\"*60)\n    print(\"━\"*60)\n\n\n","metadata":{"ExecuteTime":{"end_time":"2020-02-07T09:05:14.440959Z","start_time":"2020-02-07T09:05:14.432005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We easily use python to calculate the statistics for us, as shown above.\n\n\nBecause the p-value is so small (usually less than $0.05$, this means that if our null hypothesis was actually true that $\\beta_1 = 0$, then there is only a $0.0\\%$ chance to get this $t = 38.348$ value. In other words, it is unlikely to observe such a substantial association between the predictor and the response due to chance, in the absence of any real association between the predictor and the response. We can reject the null hypothesis and conclude that $\\beta_1$ does not equal 0. There is sufficient evidence, at the $\\alpha = 0.05$ level, to conclude that there is a linear relationship in the population between house area and the sale price.\n\n\n\nSimilarly, we can see the p-value for $\\hat{\\beta_0}$ to be less than $0.05$ as well. Consequently, we conclude that $\\beta_0 \\neq 0$ and $\\beta_1 \\neq 0$.\n","metadata":{}},{"cell_type":"markdown","source":"Since the above is a self implemented function, it is always good to check with the statsmodels.api from python to check if I am correct.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport math\nfrom statsmodels.api import OLS\n\nX_with_intercept = np.c_[np.ones(X.shape[0]), X]\nOLS(y,X_with_intercept).fit().summary()","metadata":{"ExecuteTime":{"end_time":"2020-02-07T09:00:52.614304Z","start_time":"2020-02-07T09:00:52.598346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To be continued, in the next section, we will generalize to multiple regression, and explain why when the model has so many variables, say 100, then just by reading the t-test's p-value for each of the variable is not enough - and that is where F-test come in, which is a better test when there are more variables.","metadata":{}}]}