{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This is my first experience with NLP\n\nHere I experimenting with different models (roberta, distilbert, xlnet) + different heads (LSTM, 1D-CNN, Attention, Transformer) + FC layer.\n\nThe pipeline is very flexible, you can change any of components\n\nI run here only single fold, but you can add the loop and train with cross-validation.","metadata":{}},{"cell_type":"code","source":"pip install --upgrade -q pytorch-lightning==1.3.1","metadata":{"execution":{"iopub.status.busy":"2021-05-21T20:35:06.046415Z","iopub.execute_input":"2021-05-21T20:35:06.046765Z","iopub.status.idle":"2021-05-21T20:35:16.376842Z","shell.execute_reply.started":"2021-05-21T20:35:06.046683Z","shell.execute_reply":"2021-05-21T20:35:16.375958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport pathlib\n\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom matplotlib import pyplot as plt\n\nimport os, random, gc\nimport re, time, json, pickle\n\nfrom sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n\nfrom sklearn.model_selection import KFold\n\nfrom tqdm.notebook import tqdm\nimport pytorch_lightning as pl\n\nfrom transformers import AutoTokenizer, AutoConfig, AutoModelForTokenClassification\nimport joblib\n\nfrom typing import Optional\nfrom collections import defaultdict","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-21T20:35:16.379812Z","iopub.execute_input":"2021-05-21T20:35:16.380067Z","iopub.status.idle":"2021-05-21T20:35:20.314087Z","shell.execute_reply.started":"2021-05-21T20:35:16.380038Z","shell.execute_reply":"2021-05-21T20:35:20.313261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True","metadata":{"execution":{"iopub.status.busy":"2021-05-21T20:35:20.315859Z","iopub.execute_input":"2021-05-21T20:35:20.31618Z","iopub.status.idle":"2021-05-21T20:35:20.32193Z","shell.execute_reply.started":"2021-05-21T20:35:20.316146Z","shell.execute_reply":"2021-05-21T20:35:20.320578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CONFIG:\n    max_length = 300\n    num_targets = 1\n    SEED = 321\n    loader_params = dict(\n        trn=dict(batch_size=4,\n                 num_workers=0,\n                 shuffle=True,\n                 pin_memory=True),\n        val=dict(batch_size=5,\n                 num_workers=0,\n                 shuffle=False,\n                 pin_memory=True),\n        tst=dict(batch_size=5,\n                 num_workers=0,\n                 shuffle=False,\n                 pin_memory=True),\n        all=dict(batch_size=5,\n                 num_workers=0,\n                 shuffle=False,\n                 pin_memory=True)\n    )\n    learning_rate = 5e-5\n    n_folds = 5\n    \nseed_everything(CONFIG.SEED)\n\nroot_path = pathlib.Path('../input/commonlitreadabilityprize')\n\nsave_path = pathlib.Path('.')\n","metadata":{"execution":{"iopub.status.busy":"2021-05-21T20:35:20.323691Z","iopub.execute_input":"2021-05-21T20:35:20.324272Z","iopub.status.idle":"2021-05-21T20:35:20.338006Z","shell.execute_reply.started":"2021-05-21T20:35:20.324226Z","shell.execute_reply":"2021-05-21T20:35:20.337228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CommonLitDataset(Dataset):\n    def __init__(self, inputs, masks, targets):\n        assert inputs.shape == masks.shape\n        self.inputs = inputs\n        self.masks = masks\n        self.targets = targets.type(torch.float32)\n\n    def __len__(self):\n        return len(self.inputs)\n\n    def __getitem__(self, idx):\n        return dict(inputs=self.inputs[idx], masks=self.masks[idx], targets=self.targets[[idx]])\n","metadata":{"execution":{"iopub.status.busy":"2021-05-21T20:35:20.339166Z","iopub.execute_input":"2021-05-21T20:35:20.339569Z","iopub.status.idle":"2021-05-21T20:35:20.34595Z","shell.execute_reply.started":"2021-05-21T20:35:20.339532Z","shell.execute_reply":"2021-05-21T20:35:20.345096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PrintCallback(pl.callbacks.Callback):\n    \"\"\"\n    callback for pytorch lightning which saves and prints out results\n    \"\"\"\n\n    def __init__(self):\n        self.metrics = {}\n\n    def on_epoch_end(self, trainer, pl_module):\n        metrics_dict = {k: v for k, v in trainer.callback_metrics.items() if 'step' not in k}\n        self.metrics[trainer.current_epoch] = metrics_dict\n        msg = f'epoch: {str(trainer.current_epoch).rjust(4)}\\t'\n        msg += '\\t'.join([f'{k}: {v:.5f}' for k, v in metrics_dict.items()])\n        print(msg)\n\n    def to_df(self):\n        return pd.DataFrame(\n            {epoch: {k: v.item() for k, v in metrics.items()} for epoch, metrics in self.metrics.items()}).T\n\n    def plot(self):\n        df = self.to_df()\n        fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n        for column in df.columns:\n            df[column].plot(ax=ax, legend=column)\n        return fig\n","metadata":{"execution":{"iopub.status.busy":"2021-05-21T20:35:20.347337Z","iopub.execute_input":"2021-05-21T20:35:20.34785Z","iopub.status.idle":"2021-05-21T20:35:20.359071Z","shell.execute_reply.started":"2021-05-21T20:35:20.347811Z","shell.execute_reply":"2021-05-21T20:35:20.358205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Squeeze(nn.Module):\n    def __init__(self, dims=-1):\n        super().__init__()\n        self.dims = dims\n        \n    def forward(self, x):\n        return x.squeeze(self.dims)\n        \nclass AttentionHead(nn.Module):\n    def __init__(self, in_features, hidden_dim, num_targets):\n        super().__init__()\n        self.in_features = in_features\n        self.middle_features = hidden_dim\n\n        self.W = nn.Linear(in_features, hidden_dim)\n        self.V = nn.Linear(hidden_dim, 1)\n        self.out_features = hidden_dim\n\n    def forward(self, features):\n        att = torch.tanh(self.W(features))\n\n        score = self.V(att)\n\n        attention_weights = torch.softmax(score, dim=1)\n\n        context_vector = attention_weights * features\n        context_vector = torch.sum(context_vector, dim=1)\n\n        return context_vector\n\n    \n\nclass CNNHead(nn.Module):\n    def __init__(self, in_features, hidden_dim, kernel_size=10, num_targets=1):\n        super().__init__() \n        self.head = nn.Sequential(nn.Conv1d(in_features, hidden_dim, kernel_size=kernel_size),\n                                     nn.AdaptiveMaxPool1d(1),\n                                     Squeeze()\n                                    )\n        self.out_features = hidden_dim\n        \n    def forward(self, x):\n        return self.head(x.permute(0,2,1))\n        \n\nclass LSTMHead(nn.Module):\n    def __init__(self, in_features, hidden_dim, n_layers, num_targets=1):\n        super().__init__()\n        self.lstm = nn.LSTM(in_features,\n                            hidden_dim,\n                            n_layers,\n                            batch_first=True,\n                            bidirectional=False,\n                            dropout=0.2)\n        self.out_features = hidden_dim\n\n    def forward(self, x):\n        self.lstm.flatten_parameters()\n        _, (hidden, _) = self.lstm(x)\n        out = hidden[-1]\n        return out\n        \n        \nclass TransformerHead(nn.Module):\n    def __init__(self, in_features, max_length, num_layers=1, nhead=8, num_targets=1):\n        super().__init__()\n\n        self.transformer = nn.TransformerEncoder(encoder_layer=nn.TransformerEncoderLayer(d_model=in_features,\n                                                                                          nhead=nhead),\n                                                 num_layers=num_layers)\n        self.row_fc = nn.Linear(in_features, 1)\n        self.out_features = max_length\n\n    def forward(self, x):\n        out = self.transformer(x)\n        out = self.row_fc(out).squeeze(-1)\n        return out","metadata":{"execution":{"iopub.status.busy":"2021-05-21T20:35:20.360421Z","iopub.execute_input":"2021-05-21T20:35:20.360822Z","iopub.status.idle":"2021-05-21T20:35:20.377455Z","shell.execute_reply.started":"2021-05-21T20:35:20.360787Z","shell.execute_reply":"2021-05-21T20:35:20.376654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CommonLitModel(pl.LightningModule):\n    def __init__(self,\n                 model_name: str,\n                 fold_id: int,\n                 head,\n                 models_path: pathlib.Path = pathlib.Path('./models'),\n                 data_path: pathlib.Path = pathlib.Path('../input/commonlitreadabilityprize')\n                 ):\n        super().__init__()\n        self.model_name = model_name\n        self.num_targets = CONFIG.num_targets\n        self.criterion = nn.MSELoss()\n        self.fold_id = fold_id\n        self.save_hyperparameters()\n        self.use_attn_block = True\n\n        self.model_path = pathlib.Path(models_path) / model_name\n        self.state_dict_path = self.model_path / f'{head}_fold{fold_id}.pt'\n        self.config_path = self.model_path / 'config.pkl'\n        self.tokenizer_path = self.model_path / 'tokenizer.pkl'\n\n        self.is_fine_tuned = self.config_path.exists() and self.state_dict_path.exists()\n        if self.is_fine_tuned:\n            with open(self.config_path, 'rb') as f:\n                self.config = pickle.load(f)\n            self.feature_extractor = AutoModelForTokenClassification.from_config(self.config)\n            \n            with open(self.tokenizer_path, 'rb') as f:\n                self.tokenizer = pickle.load(f)\n\n        else:\n            self.feature_extractor = AutoModelForTokenClassification.from_pretrained(model_name)\n            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n            self.config = AutoConfig.from_pretrained(model_name)\n            \n\n        in_features = self.feature_extractor.classifier.in_features\n        if head == 'AttentionHead':\n            self.head = AttentionHead(in_features=in_features, hidden_dim=in_features, num_targets=1)\n        if head == 'CNNHead':\n            self.head = CNNHead(in_features=in_features, hidden_dim=in_features//4, kernel_size=10, num_targets=1)\n        if head == 'LSTMHead':\n            self.head = LSTMHead(in_features=in_features, hidden_dim=in_features//4, n_layers=1, num_targets=1)\n        if head == 'TransformerHead':\n            self.head = TransformerHead(in_features=in_features, max_length=CONFIG.max_length, num_layers=1, nhead=8, num_targets=1)\n        self.feature_extractor.classifier = nn.Identity()\n        self.fc = nn.Linear(self.head.out_features, self.num_targets)\n        \n        if self.is_fine_tuned:\n            self.load_state_dict(torch.load(self.state_dict_path, map_location=self.device))\n        self.data_path = data_path\n\n        self.data = self.prepare_data()\n\n        self.masks = dict(trn=(self.data.fold_id.ne(self.fold_id) & self.data.fold_id.ge(0)).values,\n                          val=self.data.fold_id.eq(self.fold_id).values,\n                          tst=self.data.fold_id.eq(-1).values\n                          )\n\n    @staticmethod\n    def set_folds(in_data, cv_obj=KFold(n_splits=5, random_state=CONFIG.SEED, shuffle=True)):\n        df = in_data.copy()\n        df[\"fold_id\"] = -1\n        for fold_id, (_, val_set) in enumerate(cv_obj.split(np.arange(df.index.size))):\n            df.loc[val_set, \"fold_id\"] = fold_id\n        return df\n\n    def prepare_data(self):\n        train_df = (pd.read_csv(self.data_path / \"train.csv\")\n                    .pipe(self.set_folds, cv_obj=KFold(n_splits=CONFIG.n_folds, random_state=CONFIG.SEED, shuffle=True))\n                    )\n        test_df = (pd.read_csv(self.data_path / \"test.csv\")\n                   .assign(fold_id=-1))\n        df = pd.concat([train_df, test_df], sort=False).set_index('id')\n\n        inputs_ids = dict()\n        attention_masks = dict()\n        for idx, excerpt in zip(df.index, df.excerpt):\n            out = self.tokenizer(excerpt,\n                                 add_special_tokens=True,\n                                 return_tensors=\"pt\",\n                                 max_length=CONFIG.max_length,\n                                 padding=\"max_length\",\n                                 truncation=True)\n            inputs_ids[idx] = out['input_ids']\n            attention_masks[idx] = out['attention_mask']\n\n        df = (df\n                  .join(pd.Series(inputs_ids).to_frame('inputs'))\n                  .join(pd.Series(attention_masks).to_frame('attention_masks'))\n                  .loc[:, ['fold_id', 'inputs', 'attention_masks', 'target']])\n        return df\n\n    def fetch_dataloader(self, typ: str):\n        if typ == 'all':\n            dt = self.data\n        else:\n            dt = self.data.loc[self.masks[typ]]\n\n        ds = CommonLitDataset(inputs=torch.cat(dt['inputs'].tolist()),\n                              masks=torch.cat(dt['attention_masks'].tolist()),\n                              targets=torch.from_numpy(dt['target'].values)\n                              )\n        return DataLoader(ds, **CONFIG.loader_params[typ])\n\n    def train_dataloader(self):\n        return self.fetch_dataloader(typ='trn')\n\n    def val_dataloader(self):\n        return self.fetch_dataloader(typ='val')\n\n    def test_dataloader(self):\n        return self.fetch_dataloader(typ='tst')\n\n    def configure_optimizers(self):\n        optimizer = optim.Adam(self.parameters(), lr=CONFIG.learning_rate)\n        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=1e-5, T_max=20)\n        return dict(optimizer=optimizer, scheduler=scheduler)\n\n    def forward(self, inputs, masks):\n        x = self.feature_extractor(inputs, masks)[\"logits\"]\n        x = self.head(x)\n        x = self.fc(x)\n        return x\n\n    def shared_step(self, batch, typ):\n        inputs = batch['inputs']\n        masks = batch['masks']\n        targets = batch['targets']\n        outputs = self(inputs, masks)\n        loss = self.criterion(outputs, targets)\n        self.log(f'{typ}_loss', loss, on_step=False, on_epoch=True)\n        return loss\n\n    def training_step(self, batch, batch_idx):\n        return self.shared_step(batch, typ='trn')\n\n    def validation_step(self, batch, batch_idx):\n        return self.shared_step(batch, typ='val')\n\n    def save(self):\n        torch.save(self.state_dict(), self.state_dict_path)\n        with open(self.tokenizer_path, 'wb') as f:\n            pickle.dump(self.tokenizer, f)\n        with open(self.config_path, 'wb') as f:\n            pickle.dump(self.config, f)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-05-21T20:35:20.379683Z","iopub.execute_input":"2021-05-21T20:35:20.380058Z","iopub.status.idle":"2021-05-21T20:35:20.412052Z","shell.execute_reply.started":"2021-05-21T20:35:20.380022Z","shell.execute_reply":"2021-05-21T20:35:20.411035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nresults = []\nheads = ['AttentionHead', 'CNNHead', 'LSTMHead', 'TransformerHead']\nmodels = [\n#     'roberta-base',\n    'distilbert-base-uncased' \n#     'xlnet-base-cased'\n]\n\n\ndir_path = pathlib.Path('./models')\n\nfold_id = 0\nmodels_dict = defaultdict(lambda: defaultdict(dict))\nfor model_name in models:\n    model_name = model_name\n    model_path = dir_path / model_name\n    model_path.mkdir(exist_ok=True, parents=True)\n    for head in heads:\n        try:        \n            checkpoint_callback = pl.callbacks.ModelCheckpoint()\n            print_callback = PrintCallback()\n            es_callback = pl.callbacks.EarlyStopping(monitor='val_loss', patience=2) \n\n            model = CommonLitModel(model_name, fold_id, head=head)\n            \n            print(model_name, head)\n            if model.is_fine_tuned:\n                model = model.cuda()\n                model.eval()\n                print('\\talready fine tuned')\n                lst = [(model(batch['inputs'].cuda(), batch['masks'].cuda()) - batch['targets'].cuda()).pow(2).cpu().detach().numpy().ravel() for batch in model.val_dataloader()]\n                score = np.sqrt(np.concatenate(lst).mean())\n                models_dict[model_name][head] = model\n                print(model_name, head, score)\n                continue\n            trainer = pl.Trainer(gpus=1 if torch.cuda.is_available() else 0, max_epochs=50, fast_dev_run=False, callbacks=[checkpoint_callback, print_callback, es_callback])\n\n            trainer.fit(model, model.train_dataloader(), model.val_dataloader())\n\n            print_callback.plot()\n            plt.show()\n            \n            model = model.load_from_checkpoint(checkpoint_callback.best_model_path)\n            model = model.cuda()\n            model.save()\n            model.eval()\n            lst = [(model(batch['inputs'].cuda(), batch['masks'].cuda()) - batch['targets'].cuda()).pow(2).cpu().detach().numpy().ravel() for batch in model.val_dataloader()]\n            score = np.sqrt(np.concatenate(lst).mean())\n            torch.cuda.empty_cache()\n            models_dict[model_name][head] = model\n            print(model_name, head, score)\n            results.append([model_name, head, score])\n        except Exception as e:\n            print(e)\n            pass","metadata":{"execution":{"iopub.status.busy":"2021-05-21T20:35:20.414867Z","iopub.execute_input":"2021-05-21T20:35:20.415496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(results, columns=['model_name', 'head_name', 'score']).groupby(['head_name', 'model_name']).max().unstack()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -rf ./lightning_logs","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}