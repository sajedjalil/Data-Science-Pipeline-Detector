{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Acknowledgement: \nThis is just a cleaner version for https://www.kaggle.com/maunish/clrp-pytorch-train-tpu to finetune RoberTa with 5-fold CV. Output models can be found in https://www.kaggle.com/maunish/clrp-roberta-svm/data. I do this just to solve some confusion for code. If you like, just give credit to [Maunish](https://www.kaggle.com/maunish). \n\nThe confusion is this line of code `inputs = {key:val.reshape(val.shape[0], -1).to(device) for key, val in inputs.items()}`. Finally, I figure out that pytorch dataset will output extra dimension due to huggingface tokenizer with `return_tensors='pt'`.","metadata":{}},{"cell_type":"markdown","source":"## Imports ðŸ“—","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport sys\nimport cv2\nimport math\nimport time\nimport tqdm\nimport random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold,StratifiedKFold, GroupKFold\n\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.nn import Parameter\nimport torch.nn.functional as F\nfrom torch.optim import Adam, lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim.optimizer import Optimizer\nfrom torch.optim.lr_scheduler import _LRScheduler\nfrom torch.optim.lr_scheduler import (CosineAnnealingWarmRestarts, CosineAnnealingLR, \n                                      ReduceLROnPlateau)\n\nfrom transformers import (AutoModel, AutoTokenizer, \n                          AutoModelForSequenceClassification,\n                          get_constant_schedule_with_warmup,get_cosine_schedule_with_warmup)\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-05-22T03:43:33.143805Z","iopub.execute_input":"2021-05-22T03:43:33.144093Z","iopub.status.idle":"2021-05-22T03:43:40.836311Z","shell.execute_reply.started":"2021-05-22T03:43:33.144067Z","shell.execute_reply":"2021-05-22T03:43:40.835506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Getting Data ðŸ’¾","metadata":{}},{"cell_type":"code","source":"# data reading\ntrain_data = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\ntest_data = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\nsample = pd.read_csv('../input/commonlitreadabilityprize/sample_submission.csv')\n\nclass CLRPDataset(nn.Module):\n    def __init__(self,df,tokenizer,max_len=128):\n        self.excerpt = df['excerpt'].to_numpy()\n        self.targets = df['target'].to_numpy()\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n    \n    def __getitem__(self,idx):\n        encode = self.tokenizer(self.excerpt[idx],\n                                return_tensors='pt', # this will add a extra dimension\n                                max_length=self.max_len,\n                                padding='max_length',\n                                truncation=True)\n        \n        target = torch.tensor(self.targets[idx],dtype=torch.float)\n        return encode, target\n    \n    def __len__(self):\n        return len(self.excerpt)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T03:43:40.837732Z","iopub.execute_input":"2021-05-22T03:43:40.838035Z","iopub.status.idle":"2021-05-22T03:43:40.938391Z","shell.execute_reply.started":"2021-05-22T03:43:40.838001Z","shell.execute_reply":"2021-05-22T03:43:40.937555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# finetune setting\nconfig = {\n    'lr': 1e-5,\n    'wd':1e-1,\n    'batch_size':16,\n    'max_len':256,\n    'epochs':4,\n    'nfolds':5,\n    'seed':42,\n}\n\ndef rmse_score(y_true,y_pred):\n    return np.sqrt(mean_squared_error(y_true,y_pred))\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONASSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(seed=config['seed'])\n\ndef loss_fn(outputs,targets):\n    outputs = outputs.logits.squeeze(-1)\n    return torch.sqrt(nn.MSELoss()(outputs,targets))\n\ndef train_loop(train_loader, model, loss_fn, device,optimizer,lr_scheduler=None):\n    model.train()\n    total_loss = 0\n    for i, (inputs,targets) in enumerate(train_loader):\n        optimizer.zero_grad()\n        inputs = {key:val.reshape(val.shape[0],-1).to(device) for key,val in inputs.items()}\n        targets = targets.to(device)\n        outputs = model(**inputs)\n        loss = loss_fn(outputs,targets)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n    total_loss /= len(train_loader)\n    return total_loss\n\ndef valid_loop(valid_loader, model, loss_fn, device):\n    model.eval()\n    total_loss = 0\n    valid_predictions = list()\n    with torch.no_grad():\n        for i, (inputs,targets) in enumerate(valid_loader):\n            inputs = {key:val.reshape(val.shape[0],-1).to(device) for key,val in inputs.items()}\n            targets = targets.to(device)\n\n            outputs = model(**inputs)\n            loss = loss_fn(outputs,targets)\n            total_loss += loss.item()\n            outputs = outputs.logits.squeeze(-1).cpu().detach().numpy().tolist()\n#                 outputs = outputs.cpu().detach().numpy().tolist()\n            valid_predictions.extend(outputs)\n        total_loss /= len(valid_loader)\n    return total_loss ,valid_predictions\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2021-05-22T03:45:25.663497Z","iopub.execute_input":"2021-05-22T03:45:25.663853Z","iopub.status.idle":"2021-05-22T03:45:25.67906Z","shell.execute_reply.started":"2021-05-22T03:45:25.663823Z","shell.execute_reply":"2021-05-22T03:45:25.67822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# training\ntrain = train_data\nkfold = KFold(n_splits=config['nfolds'],shuffle=True,random_state=config['seed'])\nfor k , (train_idx,valid_idx) in enumerate(kfold.split(X=train)):\n    x_train,x_valid = train.loc[train_idx],train.loc[valid_idx]\n\n    MODEL_PATH = 'roberta-large'\n    model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH,num_labels=1)\n    model.to(device)\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n\n    train_ds = CLRPDataset(x_train,tokenizer,config['max_len'])\n    train_dl = DataLoader(train_ds,\n                          batch_size = config[\"batch_size\"],\n                          shuffle=True,\n                          num_workers = 4,\n                          pin_memory=True,\n                          drop_last=False\n                         )\n\n    valid_ds = CLRPDataset(x_valid,tokenizer,config['max_len'])\n    valid_dl = DataLoader(valid_ds,\n                          batch_size = config[\"batch_size\"],\n                          shuffle=False,\n                          num_workers = 4,\n                          pin_memory=True,\n                          drop_last=False,\n                         )\n\n    optimizer = optim.AdamW(model.parameters(),lr=config['lr'],weight_decay=config['wd'])\n    lr_scheduler = None\n\n    best_loss = 99999\n    best_valid_predictions = list()\n    \n    for i in range(config[\"epochs\"]):\n        train_loss = train_loop(train_dl,model,loss_fn,device,optimizer,lr_scheduler=lr_scheduler)\n        valid_loss,valid_predictions = valid_loop(valid_dl,model,loss_fn,device)\n\n        valid_targets = x_valid['target'].to_list()\n\n        if lr_scheduler:\n            lr_scheduler.step()\n            \n        if valid_loss <= best_loss:\n            best_loss = valid_loss\n            best_valid_predictions = valid_predictions\n#                 torch.save(model.state_dict(),f'./model{k}/model{k}.bin')\n            model.save_pretrained(f'./model{k}')\n            tokenizer.save_pretrained(f'./model{k}')\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}