{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <p style=\"background-color:gray; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 10px 100px; color:black; hight:max\"> Upvote my work if you found it useful.üéØ </p>\n\n# <p style=\"background-color:#CCE3F2; font-family:newtimeroman; font-size:175%; text-align:center; border-radius: 15px 50px;\">CommonLit Readability Prize üìñ</p>\n\n\n\n<img src=\"https://image.slidesharecdn.com/80edc8ea-6538-4fab-a6b6-fb2c9714fd24-160328160353/95/contentreadability-1-638.jpg?cb=1459181240\" alt=\"Readability\" hight=50 width=800></img>\n\n------------------------------------------\n------------------------------------------\n","metadata":{}},{"cell_type":"markdown","source":"<p style=\"background-color:skyblue; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 10px 500px;\"><b>Introduction</b></p>\nThe objective of this project is to build a Machine Learning model to rate the complexity of reading passages for grade 3-12 classroom use based on pattern extracted from analysing 6 descriptive features.<br>\n<p style=\"background-color:skyblue; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 10px 500px;\"><b>We have 79 descriptive features:</b></p>\n<ul>\n    <li>id : unique ID for excerpt.</li>\n    <li>url_legal : URL of source.</li>\n    <li>license : license of source material.</li>\n    <li>excerpt : text to predict reading ease of.</li>\n    <li>target : reading ease</li>\n    <li>standard_error : measure of spread of scores among multiple raters for each excerpt.</li>  \n</ul>\n<p style=\"background-color:skyblue; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 10px 100px;\"><b>Outline:</b></p>\n<ul>\n    <li><a href=\"#Phase I\"><b>Phase I</b></a>\n        <ul>\n            <li><a href=\"#head-1\">Data Pre-processing</a>\n            <li><a href=\"#head-2\">EDA</a>","metadata":{}},{"cell_type":"markdown","source":"### <b>Installing and importing packages</b>","metadata":{}},{"cell_type":"code","source":"# We gonna use these packages for our analysis.\n\n# importing\nimport os\nimport re\nimport pandas as pd\nimport matplotlib as plt\nimport seaborn as sns\nimport itertools\nimport nltk \nimport string\nfrom wordcloud import WordCloud\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\nfrom nltk.stem import WordNetLemmatizer\nfrom textblob import TextBlob,Word\nfrom collections import Counter\n\n#visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly \nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nimport plotly.io as pio\nfrom plotly.subplots import make_subplots\nimport plotly.express as px\nfrom plotly.offline import iplot, init_notebook_mode\nimport cufflinks as cf\nimport plotly.figure_factory as ff \nfrom plotly.offline import iplot\nfrom plotly import tools\ncolors = px.colors.qualitative.Prism\npio.templates.default = \"plotly_white\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# see our files:\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <h3>Reading Data</h3>\nDownloading and reading dataset from Kaggle. Right now, only considering 2834 rows for the exploration.","metadata":{}},{"cell_type":"code","source":"#reading data\ntrain = pd.read_csv('/kaggle/input/commonlitreadabilityprize/train.csv')\ntest = pd.read_csv('/kaggle/input/commonlitreadabilityprize/test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"head-1\"></a>\n<a id=\"head-1-3\"></a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">Data Preprocessing üîß</p>\nThere are various tasks that need to perform in data processing like lowercasing, removing stop words, removing digits, removing URLs, HTML tags and many more.<br>\nFor basic cleaning and lemmatization, I am using the texthero and nltk package which offers features related to text preprocessing as well as data exploration. Code is as follows:","metadata":{}},{"cell_type":"code","source":"#filtering the unwanted symbols, spaces, ....etc\nto_replace_by_space = re.compile('[/(){}\\[\\]|@,;]')\npunctuation = re.compile(f'([{string.punctuation}‚Äú‚Äù¬®¬´¬ª¬Æ¬¥¬∑¬∫¬Ω¬æ¬ø¬°¬ß¬£‚Ç§‚Äò‚Äô])')\nbad_symbols = re.compile('[^0-9a-z #+_]')\nstopwords = set(stopwords.words('english'))\n\ndef text_prepare(text):\n    '''\n    text: a string\n    returna modified version of the string\n    '''\n    text = text.lower() # lowercase text\n    text = re.sub(punctuation, '',text)\n    text = re.sub(to_replace_by_space, \" \", text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n    text = re.sub(bad_symbols, \"\", text)         # delete symbols which are in BAD_SYMBOLS_RE from text\n    text = \" \".join([word for word in text.split(\" \") if word not in stopwords]) # delete stopwords from text\n    text = re.sub(' +', ' ', text)\n    return text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['excerpt'] = train['excerpt'].apply(text_prepare)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"head-2\"></a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">Data Exploration and Analysis üîç</p>\n<p>It is important to know enough insights about your data before implementing any machine learning model or performing statistical hypothesis testing. Exploratory data analysis provide that insight about the data to you.\nIn this article, we will be discussing the respective python packages available and different exploratory task in the natural language processing domain one should perform.\n    <b>So, let's start.....","metadata":{}},{"cell_type":"code","source":"# lets define a function to plot a bar plot easily\n\ndef bar_plot(df,x,x_title,y,title,colors=None,text=None):\n    fig = px.bar(x=x,\n                 y=y,\n                 text=text,\n                 labels={x: x_title.title()},          # replaces default labels by column name\n                 data_frame=df,\n                 color=colors,\n                 barmode='group',\n                 template=\"simple_white\",\n                 color_discrete_sequence=px.colors.qualitative.Prism)\n    \n    texts = [df[col].values for col in y]\n    for i, t in enumerate(texts):\n        fig.data[i].text = t\n        fig.data[i].textposition = 'inside'\n        \n    fig['layout'].title=title\n\n    for trace in fig.data:\n        trace.name = trace.name.replace('_',' ').title()\n\n    fig.update_yaxes(tickprefix=\"\", showgrid=True)\n\n    fig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lets define a function to plot a histogram plot easily\n\ndef hist_plot(df,x,title):\n    fig = px.histogram(x=df[x],\n                       color_discrete_sequence=colors,\n                       opacity=0.8)\n\n    fig['layout'].title=title\n    fig.update_yaxes(tickprefix=\"\", showgrid=True)\n    fig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ### Target ","metadata":{}},{"cell_type":"code","source":"title='Target distribution'\nhist_plot(train, 'target' ,title)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ### standard error","metadata":{}},{"cell_type":"code","source":"title='Standard error distribution'\nhist_plot(train, 'standard_error' ,title)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ### License","metadata":{}},{"cell_type":"code","source":"temp = train['license'].dropna()\ntemp = temp.value_counts().to_frame().reset_index()\ntitle = 'Unique licenses count'\nbar_plot(temp, \n         'index',\n         'License',\n         ['license'],\n          title=title)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ### Url legal","metadata":{}},{"cell_type":"code","source":"# Using this iteratively I was able to get a full list of titles.\nurl_unique_list = train['url_legal'].dropna().apply(lambda x : re.findall('https?://([A-Za-z_0-9.-]+).*',x)[0]).unique()\nurl_unique_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"url_count = {}\nfor sent in train['url_legal'].dropna().values:\n    url = re.findall('https?://([A-Za-z_0-9.-]+).*',sent)[0]\n    if url in url_count:\n        url_count[url] += 1\n    else:\n        url_count[url] = 1\n    \nurl_count_df = pd.DataFrame(data=url_count.items())\nurl_count_df = url_count_df.sort_values(by=1,ascending=False).rename(columns={0:'Site',1:'Count'})\nurl_count_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"title = 'Unique Sites count'\nbar_plot(url_count_df, \n         'Site',\n         'Site',\n         ['Count'],\n          title=title)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Words level analysis","metadata":{}},{"cell_type":"code","source":"#Find words spreading (each word frequency)\nfreq_d = pd.Series(' '.join(train['excerpt']).split()).value_counts()\n#Plot the words distribution\nfig = px.line(freq_d,\n              title='The word frequency visualization')\nfig.update_layout(showlegend=False) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prepared_as_text = [line for line in train['excerpt']]\ntext_prepared_results = '/n'.join(prepared_as_text)\n\ntext= ' '.join(t for t in train['excerpt'])\nwords_list= text.split()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_freq= {}\n\nfor word in set(words_list):\n    word_freq[word]= words_list.count(word)\n    \n#sorting the dictionary \nword_freq = dict(sorted(word_freq.items(), reverse=True, key=lambda item: item[1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#sort the data and put it in a data frame for the visualization\nword_freq_temp = dict(itertools.islice(word_freq.items(), 25))\nword_freq_df = pd.DataFrame(word_freq_temp.items(),columns=['word','count']).sort_values('count',ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bar_plot(word_freq_df.reset_index(),\n         'word',\n         'Words',\n         ['count'],\n         title='Top 20 frequent words')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def wordcloud(text,stopwords,ngram=1):\n    # text: if ngram>1, text should be a dictionary\n    wordcloud = WordCloud(width=1400, \n                          height=800,\n                          random_state=2021,\n                          background_color='black',\n                          stopwords=stop)\n    if ngram ==1:\n        wordc = wordcloud.generate(' '.join(text))\n    else:\n        wordc = wordcloud.generate_from_frequencies(text)\n    plt.figure(figsize=(20,10), facecolor='k')\n    plt.imshow(wordcloud)\n    plt.axis('off')\n    plt.tight_layout(pad=0)\n    \nwordcloud(train['excerpt'],stop)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ### Bigrams","metadata":{}},{"cell_type":"code","source":"from nltk.util import ngrams    \n\ndef get_n_grans_count(text, n_grams, min_freq):\n    output = {}\n    tokens = nltk.word_tokenize(text)\n\n    #Create the n_gram\n    if n_grams == 2:\n        gs = nltk.bigrams(tokens)\n        \n    elif n_grams == 3:\n        gs = nltk.trigrams(tokens)\n\n    else:\n        return 'Only 2_grams and 3_grams are supported'\n    #compute frequency distribution for all the bigrams in the text\n    fdist = nltk.FreqDist(gs)\n    for k,v in fdist.items():\n        if v > min_freq:\n            index = ' '.join(k)\n            output[index] = v\n    \n    return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"two_grams = get_n_grans_count(text, n_grams=2, min_freq=10)\ntwo_grams_df = pd.DataFrame(data=two_grams.items())\ntwo_grams_df = two_grams_df.sort_values(by=1,ascending=False).rename(columns={0:'Two grams',1:'Count'})\ntwo_grams_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bar_plot(two_grams_df.iloc[:20],\n         'Two grams',\n         'Two grams',\n         ['Count'],\n         title='Top 20 frequent bigram')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"two_grams_temp = {j.replace(' ','_'):k for j,k in two_grams.items()}\n\nwordcloud(two_grams_temp,stop,ngram=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ### Trigrams","metadata":{}},{"cell_type":"code","source":"three_grams = get_n_grans_count(text, n_grams=3, min_freq=0)\nthree_grams_df = pd.DataFrame(data=three_grams.items())\nthree_grams_df = three_grams_df.sort_values(by=1,ascending=False).rename(columns={0:'Three grams',1:'Count'})\nthree_grams_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bar_plot(three_grams_df.iloc[:20],\n         'Three grams',\n         'Three grams',\n         ['Count'],\n         title='Top 20 frequent trigram')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"three_grams_temp = {j.replace(' ','_') : k for j, k in three_grams.items()}\n\nwordcloud(three_grams_temp,stop,ngram=3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ### Words length","metadata":{}},{"cell_type":"code","source":"words_length = {}\n\nfor word in set(words_list):\n    words_length[word] = len(word)\n    \nwords_length = dict(sorted(words_length.items(), reverse=True, key=lambda item: item[1]))\n#sort the data and put it in a data frame for the visualization\nword_length_temp = dict(itertools.islice(words_length.items(), 25))\nwords_length_df = pd.DataFrame(words_length.items(),columns=['word','count']).sort_values('count',ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hist_plot(words_length_df,\n          'count',\n          title='Target distribution')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> <b>So most of words have length between 4-11.<br> \nBut Wait!, something is weird here. There are words with a length of more than 15. We have to verify, are these actually English words or not?","metadata":{}},{"cell_type":"code","source":"longer_23 = words_length_df[words_length_df['count'] > 23]\nlonger_23","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>Sentence level analysis</h2><br>\nSentence level analysis Text statistics include sentence length distribution, minimum, maximum, and average length. To check the sentence length distribution. Code and output are as follows:","metadata":{}},{"cell_type":"code","source":"train['sentence_len']= train['excerpt'].str.len()\nprint('Max length     : {} \\nMin length     : {} \\nAverage Length : {}'.\\\n      format(max(train['sentence_len']),min(train['sentence_len']),train['sentence_len'].mean()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#the longest sentence we have\ntrain[train['sentence_len']==max(train['sentence_len'])]['excerpt'].values[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#the shortest sentence we have\ntrain[train['sentence_len']==min(train['sentence_len'])]['excerpt'].values[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hist_plot(train,\n          'sentence_len',\n          title='Sentences lengh distribution with spaces')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> <b>According to distribution, most of the article lengths are in the range of 500‚Äì650 and maximum length, minimum length, and average length are 1049, 306, 617.3712.<br>The point to note here is, sentence length includes spaces between word. If you want distribution without space, you can use the below code.","metadata":{}},{"cell_type":"code","source":"train['sentence_len_no_sp']= train['excerpt'].str.split().map(lambda x: len(x))\n\nhist_plot(train,\n          'sentence_len_no_sp',\n          title='Sentences lengh distribution without spaces')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <h2 align=\"center\" style='color:red' > If you liked the notebook or learned something please <b>Upvote</b>! </h2>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"background-color:skyblue; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 10px 100px;\">You can also see:</p>\n<ul>\n<li><b><a href='https://www.kaggle.com/alaasedeeq/house-price-prediction-top-8'>‚úî House Price prediction(Top 8%)</a>     \n<li><b><a href='https://www.kaggle.com/alaasedeeq/predicting-the-survival-of-titanic-top-6'>Predicting the Survival of Titanic (Top 6%)</a>    \n<li><b><a href='https://www.kaggle.com/alaasedeeq/prediction-of-heart-disease-machine-learning'>Prediction of Heart Disease (Machine Learning)</a>\n<li><b><a href='https://www.kaggle.com/alaasedeeq/data-exploration-and-visualization-uber-data'>Data exploration and visualization(Uber Data)</a><br>\n<li><b><a href='https://www.kaggle.com/alaasedeeq/hotel-booking-eda-cufflinks-and-plotly'>Hotel booking EDA (Cufflinks and plotly)\n</a><br>\n<li><b><a href='https://www.kaggle.com/alaasedeeq/suicide-rates-visualization-and-geographic-maps/edit/run/53135916'>Suicide Rates visualization and Geographic maps</a>\n<li><b><a href='https://www.kaggle.com/alaasedeeq/superstore-data-analysis-with-plotly-clustering'>Superstore Data Analysis With Plotly(Clustering)</a>\n<li><b><a href='https://www.kaggle.com/alaasedeeq/superstore-analysis-with-cufflinks-and-pandas'>Superstore Analysis With Cufflinks and pandas</a>\n<li><b><a href='https://www.kaggle.com/alaasedeeq/learn-data-analysis-using-sql-and-pandas'>Learn Data Analysis using SQL and Pandas</a>\n<li><b><a href='https://www.kaggle.com/alaasedeeq/european-soccer-database-with-sqlite3'>European soccer database with sqlite3</a>\n<li><b><a href='https://www.kaggle.com/alaasedeeq/chinook-questions-with-sqlite'>Chinook data questions with sqlite3</a>","metadata":{}},{"cell_type":"markdown","source":"### <h1 align=\"center\" style=\"color:red \">Thanks for reading</h1>","metadata":{}}]}