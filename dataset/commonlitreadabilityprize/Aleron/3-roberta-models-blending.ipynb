{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Overview\n\nThis notebook combines three models.","metadata":{}},{"cell_type":"code","source":"!pip install ../input/lama-whl/efficientnet_pytorch-0.7.0/dist/efficientnet_pytorch-0.7.0.tar ../input/lama-whl/log_calls-0.3.2/log_calls-0.3.2/ ../input/lama-whl/sphinxcontrib_devhelp-1.0.2-py2.py3-none-any.whl ../input/lama-whl/sphinxcontrib_htmlhelp-1.0.3-py2.py3-none-any.whl ../input/lama-whl/sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl ../input/lama-whl/sphinxcontrib_qthelp-1.0.3-py2.py3-none-any.whl ../input/lama-whl/sphinxcontrib_applehelp-1.0.2-py2.py3-none-any.whl ../input/lama-whl/sphinxcontrib_serializinghtml-1.1.4-py2.py3-none-any.whl ../input/lama-whl/importlib_metadata-1.7.0-py2.py3-none-any.whl ../input/lama-whl/poetry_core-1.0.3-py2.py3-none-any.whl ../input/lama-whl/imagesize-1.2.0-py2.py3-none-any.whl ../input/lama-whl/docutils-0.16-py2.py3-none-any.whl ../input/lama-whl/alabaster-0.7.12-py2.py3-none-any.whl ../input/lama-whl/snowballstemmer-2.1.0-py2.py3-none-any.whl ../input/lama-whl/Sphinx-3.5.4-py3-none-any.whl ../input/lama-whl/sphinx_autodoc_typehints-1.11.1-py3-none-any.whl ../input/lama-whl/nbsphinx-0.8.0-py3-none-any.whl ../input/lama-whl/nbsphinx_link-1.3.0-py2.py3-none-any.whl ../input/lama-whl/cssselect-1.1.0-py2.py3-none-any.whl ../input/lama-whl/pyquery-1.4.3-py3-none-any.whl ../input/lama-whl/chuanconggao-html2json-0.2.4.1-0-g99d7fbb/chuanconggao-html2json-99d7fbb/ ../input/lama-whl/json2html-1.3.0/json2html-1.3.0 ../input/lama-whl/lightgbm-2.3.1-py2.py3-none-manylinux1_x86_64.whl ../input/lama-whl/AutoWoE-1.2.1-py3-none-any.whl ../input/lama-whl/LightAutoML-0.2.14-py3-none-any.whl > /dev/null","metadata":{"execution":{"iopub.status.busy":"2021-08-01T09:47:37.973765Z","iopub.execute_input":"2021-08-01T09:47:37.974127Z","iopub.status.idle":"2021-08-01T09:48:14.791452Z","shell.execute_reply.started":"2021-08-01T09:47:37.974095Z","shell.execute_reply":"2021-08-01T09:48:14.790345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport math\nimport random\nimport time\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModel\nfrom transformers import AutoConfig\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.svm import SVR\n\nimport gc\ngc.enable()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-01T09:48:14.795676Z","iopub.execute_input":"2021-08-01T09:48:14.79595Z","iopub.status.idle":"2021-08-01T09:48:14.805219Z","shell.execute_reply.started":"2021-08-01T09:48:14.795919Z","shell.execute_reply":"2021-08-01T09:48:14.804428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import transformers\nfrom transformers import BertTokenizer\n\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.model_selection import KFold\n\nimport lightgbm as lgb\n\nfrom fastprogress.fastprogress import  progress_bar\n \nfrom sklearn.metrics import mean_squared_error\nfrom lightautoml.automl.presets.text_presets import TabularAutoML\nfrom lightautoml.tasks import Task","metadata":{"execution":{"iopub.status.busy":"2021-08-01T09:48:14.807166Z","iopub.execute_input":"2021-08-01T09:48:14.807547Z","iopub.status.idle":"2021-08-01T09:48:14.815951Z","shell.execute_reply.started":"2021-08-01T09:48:14.807508Z","shell.execute_reply":"2021-08-01T09:48:14.814975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 32\nMAX_LEN = 248\nEVAL_SCHEDULE = [(0.5, 16), (0.49, 8), (0.48, 4), (0.47, 2), (-1, 1)]\nROBERTA_PATH = \"../input/roberta-transformers-pytorch/roberta-base\"\nTOKENIZER_PATH = \"../input/roberta-transformers-pytorch/roberta-base\"\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nDEVICE","metadata":{"execution":{"iopub.status.busy":"2021-08-01T09:48:14.817742Z","iopub.execute_input":"2021-08-01T09:48:14.818339Z","iopub.status.idle":"2021-08-01T09:48:14.829954Z","shell.execute_reply.started":"2021-08-01T09:48:14.818302Z","shell.execute_reply":"2021-08-01T09:48:14.829011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/test.csv\")\nsubmission_df = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-08-01T09:48:14.831294Z","iopub.execute_input":"2021-08-01T09:48:14.831913Z","iopub.status.idle":"2021-08-01T09:48:14.916326Z","shell.execute_reply.started":"2021-08-01T09:48:14.831876Z","shell.execute_reply":"2021-08-01T09:48:14.915525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T09:48:14.917676Z","iopub.execute_input":"2021-08-01T09:48:14.918029Z","iopub.status.idle":"2021-08-01T09:48:15.137035Z","shell.execute_reply.started":"2021-08-01T09:48:14.917992Z","shell.execute_reply":"2021-08-01T09:48:15.136147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"class LitDataset(Dataset):\n    def __init__(self, df, inference_only=False):\n        super().__init__()\n\n        self.df = df        \n        self.inference_only = inference_only\n        self.text = df.excerpt.tolist()\n        #self.text = [text.replace(\"\\n\", \" \") for text in self.text]\n        \n        if not self.inference_only:\n            self.target = torch.tensor(df.target.values, dtype=torch.float32)        \n    \n        self.encoded = tokenizer.batch_encode_plus(\n            self.text,\n            padding = 'max_length',            \n            max_length = MAX_LEN,\n            truncation = True,\n            return_attention_mask=True\n        )        \n \n\n    def __len__(self):\n        return len(self.df)\n\n    \n    def __getitem__(self, index):        \n        input_ids = torch.tensor(self.encoded['input_ids'][index])\n        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n        \n        if self.inference_only:\n            return (input_ids, attention_mask)            \n        else:\n            target = self.target[index]\n            return (input_ids, attention_mask, target)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T09:48:15.138475Z","iopub.execute_input":"2021-08-01T09:48:15.138857Z","iopub.status.idle":"2021-08-01T09:48:15.150133Z","shell.execute_reply.started":"2021-08-01T09:48:15.138818Z","shell.execute_reply":"2021-08-01T09:48:15.14904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model 1\nInspired from: https://www.kaggle.com/maunish/clrp-roberta-svm","metadata":{}},{"cell_type":"code","source":"class LitModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n        config.update({\"output_hidden_states\":True, \n                       \"hidden_dropout_prob\": 0.25,\n                       \"layer_norm_eps\": 1e-7})                       \n        \n        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config)  \n            \n        self.attention = nn.Sequential(            \n            nn.Linear(768, 512),            \n            nn.Tanh(),                       \n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )        \n\n        self.regressor = nn.Sequential(                        \n            nn.Linear(768, 1)                        \n        )\n        \n\n    def forward(self, input_ids, attention_mask):\n        roberta_output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)        \n\n        # There are a total of 13 layers of hidden states.\n        # 1 for the embedding layer, and 12 for the 12 Roberta layers.\n        # We take the hidden states from the last Roberta layer.\n        last_layer_hidden_states = roberta_output.hidden_states[-1]\n\n        # The number of cells is MAX_LEN.\n        # The size of the hidden state of each cell is 768 (for roberta-base).\n        # In order to condense hidden states of all cells to a context vector,\n        # we compute a weighted average of the hidden states of all cells.\n        # We compute the weight of each cell, using the attention neural network.\n        weights = self.attention(last_layer_hidden_states)\n                \n        # weights.shape is BATCH_SIZE x MAX_LEN x 1\n        # last_layer_hidden_states.shape is BATCH_SIZE x MAX_LEN x 768        \n        # Now we compute context_vector as the weighted average.\n        # context_vector.shape is BATCH_SIZE x 768\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)        \n        \n        # Now we reduce the context vector to the prediction score.\n        return self.regressor(context_vector)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T09:48:15.153493Z","iopub.execute_input":"2021-08-01T09:48:15.153871Z","iopub.status.idle":"2021-08-01T09:48:15.164254Z","shell.execute_reply.started":"2021-08-01T09:48:15.153835Z","shell.execute_reply":"2021-08-01T09:48:15.163395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(model, data_loader):\n    \"\"\"Returns an np.array with predictions of the |model| on |data_loader|\"\"\"\n    model.eval()\n\n    result = np.zeros(len(data_loader.dataset))    \n    index = 0\n    \n    with torch.no_grad():\n        for batch_num, (input_ids, attention_mask) in enumerate(data_loader):\n            input_ids = input_ids.to(DEVICE)\n            attention_mask = attention_mask.to(DEVICE)\n                        \n            pred = model(input_ids, attention_mask)                        \n\n            result[index : index + pred.shape[0]] = pred.flatten().to(\"cpu\")\n            index += pred.shape[0]\n\n    return result","metadata":{"execution":{"iopub.status.busy":"2021-08-01T09:48:15.166171Z","iopub.execute_input":"2021-08-01T09:48:15.166706Z","iopub.status.idle":"2021-08-01T09:48:15.177011Z","shell.execute_reply.started":"2021-08-01T09:48:15.166669Z","shell.execute_reply":"2021-08-01T09:48:15.17604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"### train ###\n\nNUM_MODELS = 5\n\ntrain_all_predictions = np.zeros((NUM_MODELS, len(train_df)))\n\ntrain_dataset = LitDataset(train_df, inference_only=True)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n                         drop_last=False, shuffle=False, num_workers=2)\n\nfor model_index in tqdm(range(NUM_MODELS)):            \n    model_path = f\"../input/commonlit-roberta-0467/model_{model_index + 1}.pth\"\n    print(f\"\\nUsing {model_path}\")\n                        \n    model = LitModel()\n    model.load_state_dict(torch.load(model_path, map_location=DEVICE))    \n    model.to(DEVICE)\n        \n    train_all_predictions[model_index] = predict(model, train_loader)\n            \n    del model\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-08-01T09:48:15.178385Z","iopub.execute_input":"2021-08-01T09:48:15.179076Z","iopub.status.idle":"2021-08-01T09:49:05.980172Z","shell.execute_reply.started":"2021-08-01T09:48:15.179031Z","shell.execute_reply":"2021-08-01T09:49:05.979193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_model1_predictions = train_all_predictions.mean(axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T09:49:05.981803Z","iopub.execute_input":"2021-08-01T09:49:05.982381Z","iopub.status.idle":"2021-08-01T09:49:05.987181Z","shell.execute_reply.started":"2021-08-01T09:49:05.98234Z","shell.execute_reply":"2021-08-01T09:49:05.986306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### test ###\n\nNUM_MODELS = 5\n\nall_predictions = np.zeros((NUM_MODELS, len(test_df)))\n\ntest_dataset = LitDataset(test_df, inference_only=True)\n\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n                         drop_last=False, shuffle=False, num_workers=2)\n\nfor model_index in tqdm(range(NUM_MODELS)):            \n    model_path = f\"../input/commonlit-roberta-0467/model_{model_index + 1}.pth\"\n    print(f\"\\nUsing {model_path}\")\n                        \n    model = LitModel()\n    model.load_state_dict(torch.load(model_path, map_location=DEVICE))    \n    model.to(DEVICE)\n        \n    all_predictions[model_index] = predict(model, test_loader)\n            \n    del model\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-08-01T09:49:05.988636Z","iopub.execute_input":"2021-08-01T09:49:05.989297Z","iopub.status.idle":"2021-08-01T09:49:47.385008Z","shell.execute_reply.started":"2021-08-01T09:49:05.98926Z","shell.execute_reply":"2021-08-01T09:49:47.384085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model1_predictions = all_predictions.mean(axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T09:49:47.386405Z","iopub.execute_input":"2021-08-01T09:49:47.38692Z","iopub.status.idle":"2021-08-01T09:49:47.391947Z","shell.execute_reply.started":"2021-08-01T09:49:47.386879Z","shell.execute_reply":"2021-08-01T09:49:47.390672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model 2\nInspired from: [https://www.kaggle.com/rhtsingh/commonlit-readability-prize-roberta-torch-infer-3](https://www.kaggle.com/rhtsingh/commonlit-readability-prize-roberta-torch-infer-3)","metadata":{}},{"cell_type":"code","source":"test = test_df\ntrain = train_df\n\nfrom glob import glob\nimport os\nimport matplotlib.pyplot as plt\nimport json\nfrom collections import defaultdict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim.optimizer import Optimizer\nimport torch.optim.lr_scheduler as lr_scheduler\nfrom torch.utils.data import (\n    Dataset, DataLoader, \n    SequentialSampler, RandomSampler\n)\nfrom transformers import RobertaConfig\nfrom transformers import (\n    get_cosine_schedule_with_warmup, \n    get_cosine_with_hard_restarts_schedule_with_warmup\n)\nfrom transformers import RobertaTokenizer\nfrom transformers import RobertaModel\nfrom IPython.display import clear_output","metadata":{"execution":{"iopub.status.busy":"2021-08-01T09:49:47.393304Z","iopub.execute_input":"2021-08-01T09:49:47.39385Z","iopub.status.idle":"2021-08-01T09:49:47.406175Z","shell.execute_reply.started":"2021-08-01T09:49:47.39381Z","shell.execute_reply":"2021-08-01T09:49:47.405277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"def convert_examples_to_features(data, tokenizer, max_len, is_test=False):\n    data = data.replace('\\n', '')\n    tok = tokenizer.encode_plus(\n        data, \n        max_length=max_len, \n        truncation=True,\n        return_attention_mask=True,\n        return_token_type_ids=True\n    )\n    curr_sent = {}\n    padding_length = max_len - len(tok['input_ids'])\n    curr_sent['input_ids'] = tok['input_ids'] + ([0] * padding_length)\n    curr_sent['token_type_ids'] = tok['token_type_ids'] + \\\n        ([0] * padding_length)\n    curr_sent['attention_mask'] = tok['attention_mask'] + \\\n        ([0] * padding_length)\n    return curr_sent\n\nclass DatasetRetriever(Dataset):\n    def __init__(self, data, tokenizer, max_len, is_test=False):\n        self.data = data\n        self.excerpts = self.data.excerpt.values.tolist()\n        self.tokenizer = tokenizer\n        self.is_test = is_test\n        self.max_len = max_len\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, item):\n        if not self.is_test:\n            excerpt, label = self.excerpts[item], self.targets[item]\n            features = convert_examples_to_features(\n                excerpt, self.tokenizer, \n                self.max_len, self.is_test\n            )\n            return {\n                'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n                'token_type_ids':torch.tensor(features['token_type_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n                'label':torch.tensor(label, dtype=torch.double),\n            }\n        else:\n            excerpt = self.excerpts[item]\n            features = convert_examples_to_features(\n                excerpt, self.tokenizer, \n                self.max_len, self.is_test\n            )\n            return {\n                'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n                'token_type_ids':torch.tensor(features['token_type_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n            }","metadata":{"execution":{"iopub.status.busy":"2021-08-01T09:49:47.407737Z","iopub.execute_input":"2021-08-01T09:49:47.408186Z","iopub.status.idle":"2021-08-01T09:49:47.423606Z","shell.execute_reply.started":"2021-08-01T09:49:47.408148Z","shell.execute_reply":"2021-08-01T09:49:47.422477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CommonLitModel(nn.Module):\n    def __init__(\n        self, \n        model_name, \n        config,  \n        multisample_dropout=False,\n        output_hidden_states=False\n    ):\n        \n        super(CommonLitModel, self).__init__()\n        self.config = config\n        self.roberta = RobertaModel.from_pretrained(\n            model_name, \n            output_hidden_states=output_hidden_states\n        )\n        \n        self.layer_norm = nn.LayerNorm(config.hidden_size)\n        \n        if multisample_dropout:\n            self.dropouts = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n        else:\n            self.dropouts = nn.ModuleList([nn.Dropout(0.3)])\n            \n        self.regressor = nn.Linear(config.hidden_size, 1)\n        self._init_weights(self.layer_norm)\n        self._init_weights(self.regressor)\n \n    def _init_weights(self, module):\n        \n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n                \n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n                \n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n \n    def forward(\n        self, \n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        labels=None\n    ):\n        outputs = self.roberta(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n        )\n        sequence_output = outputs[1]\n        sequence_output = self.layer_norm(sequence_output)\n \n        # multi-sample dropout\n        for i, dropout in enumerate(self.dropouts):\n            if i == 0:\n                logits = self.regressor(dropout(sequence_output))\n            else:\n                logits += self.regressor(dropout(sequence_output))\n        \n        logits /= len(self.dropouts)\n \n        # calculate loss\n        loss = None\n        \n        if labels is not None:\n            loss_fn = torch.nn.MSELoss()\n            logits = logits.view(-1).to(labels.dtype)\n            loss = torch.sqrt(loss_fn(logits, labels.view(-1)))\n        \n        output = (logits,) + outputs[1:]\n        return ((loss,) + output) if loss is not None else output","metadata":{"execution":{"iopub.status.busy":"2021-08-01T09:49:47.425278Z","iopub.execute_input":"2021-08-01T09:49:47.42576Z","iopub.status.idle":"2021-08-01T09:49:47.443999Z","shell.execute_reply.started":"2021-08-01T09:49:47.425727Z","shell.execute_reply":"2021-08-01T09:49:47.443125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_model(model_name, num_labels=1):\n    tokenizer = RobertaTokenizer.from_pretrained(model_name)\n    config = RobertaConfig.from_pretrained(model_name)\n    config.update({'num_labels':num_labels})\n    model = CommonLitModel(model_name, config=config)\n    return model, tokenizer\n\ndef make_loader(\n    data, \n    tokenizer, \n    max_len,\n    batch_size,\n):\n    \n    test_dataset = DatasetRetriever(data, tokenizer, max_len, is_test=True)\n    test_sampler = SequentialSampler(test_dataset)\n    \n    test_loader = DataLoader(\n        test_dataset, \n        batch_size=batch_size // 2, \n        sampler=test_sampler, \n        pin_memory=False, \n        drop_last=False, \n        num_workers=0\n    )\n\n    return test_loader","metadata":{"execution":{"iopub.status.busy":"2021-08-01T09:49:47.445134Z","iopub.execute_input":"2021-08-01T09:49:47.445786Z","iopub.status.idle":"2021-08-01T09:49:47.457186Z","shell.execute_reply.started":"2021-08-01T09:49:47.445702Z","shell.execute_reply":"2021-08-01T09:49:47.456256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Evaluator:\n    def __init__(self, model, scalar=None):\n        self.model = model\n        self.scalar = scalar\n\n    def evaluate(self, data_loader, tokenizer):\n        preds = []\n        self.model.eval()\n        total_loss = 0\n        with torch.no_grad():\n            for batch_idx, batch_data in enumerate(data_loader):\n                input_ids, attention_mask, token_type_ids = batch_data['input_ids'], \\\n                    batch_data['attention_mask'], batch_data['token_type_ids']\n                input_ids, attention_mask, token_type_ids = input_ids.cuda(), \\\n                    attention_mask.cuda(), token_type_ids.cuda()\n                \n                if self.scalar is not None:\n                    with torch.cuda.amp.autocast():\n                        outputs = self.model(\n                            input_ids=input_ids,\n                            attention_mask=attention_mask,\n                            token_type_ids=token_type_ids\n                        )\n                else:\n                    outputs = self.model(\n                        input_ids=input_ids,\n                        attention_mask=attention_mask,\n                        token_type_ids=token_type_ids\n                    )\n                \n                logits = outputs[0].detach().cpu().numpy().squeeze().tolist()\n                preds += logits\n        return preds\n\ndef config(fold, model_name, load_model_path, test_flag = True):\n    torch.manual_seed(2021)\n    torch.cuda.manual_seed(2021)\n    torch.cuda.manual_seed_all(2021)\n    \n    max_len = 250\n    batch_size = 8\n\n    model, tokenizer = make_model(\n        model_name=model_name, \n        num_labels=1\n    )\n    \n    model.load_state_dict(\n        torch.load(f'{load_model_path}/model{fold}.bin')\n    )\n    \n    if test_flag:\n        test_loader = make_loader(\n            test, tokenizer, max_len=max_len,\n            batch_size=batch_size\n        )\n    else:\n        test_loader = make_loader(\n            train, tokenizer, max_len=max_len,\n            batch_size=batch_size\n        )\n\n    if torch.cuda.device_count() >= 1:\n        print('Model pushed to {} GPU(s), type {}.'.format(\n            torch.cuda.device_count(), \n            torch.cuda.get_device_name(0))\n        )\n        model = model.cuda() \n    else:\n        raise ValueError('CPU training is not supported')\n\n    # scaler = torch.cuda.amp.GradScaler()\n    scaler = None\n    return (\n        model, tokenizer, \n        test_loader, scaler\n    )","metadata":{"execution":{"iopub.status.busy":"2021-08-01T09:49:47.4588Z","iopub.execute_input":"2021-08-01T09:49:47.459339Z","iopub.status.idle":"2021-08-01T09:49:47.474697Z","shell.execute_reply.started":"2021-08-01T09:49:47.459285Z","shell.execute_reply":"2021-08-01T09:49:47.473683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{"execution":{"iopub.status.busy":"2021-07-25T09:11:47.357351Z","iopub.execute_input":"2021-07-25T09:11:47.35772Z","iopub.status.idle":"2021-07-25T09:11:47.362915Z","shell.execute_reply.started":"2021-07-25T09:11:47.357634Z","shell.execute_reply":"2021-07-25T09:11:47.361981Z"}}},{"cell_type":"code","source":"import time\n\ndef run(fold=0, model_name=None, load_model_path=None, test_flag = True):\n    model, tokenizer, \\\n        test_loader, scaler = config(fold, model_name, load_model_path, test_flag)\n\n    evaluator = Evaluator(model, scaler)\n\n    test_time_list = []\n\n    torch.cuda.synchronize()\n    tic1 = time.time()\n\n    preds = evaluator.evaluate(test_loader, tokenizer)\n\n    torch.cuda.synchronize()\n    tic2 = time.time() \n    test_time_list.append(tic2 - tic1)\n    \n    del model, tokenizer, test_loader, scaler\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    return preds","metadata":{"execution":{"iopub.status.busy":"2021-08-01T09:49:47.47605Z","iopub.execute_input":"2021-08-01T09:49:47.476611Z","iopub.status.idle":"2021-08-01T09:49:47.487251Z","shell.execute_reply.started":"2021-08-01T09:49:47.476553Z","shell.execute_reply":"2021-08-01T09:49:47.486198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_df1 = pd.DataFrame()\npred_df2 = pd.DataFrame()\npred_df3 = pd.DataFrame()\n\nfor fold in tqdm(range(5)):\n    pred_df1[f'fold{fold}'] = run(fold%5, '../input/roberta-transformers-pytorch/roberta-base/', '../input/commonlit-roberta-base-i/')\n    pred_df2[f'fold{fold+5}'] = run(fold%5, '../input/roberta-transformers-pytorch/roberta-large', '../input/roberta-large-itptfit/')\n    pred_df3[f'fold{fold+10}'] = run(fold%5, '../input/roberta-transformers-pytorch/roberta-large', '../input/commonlit-roberta-large-ii/')\n\npred_df1 = np.array(pred_df1)\npred_df2 = np.array(pred_df2)\npred_df3 = np.array(pred_df3)\n\nmodel2_predictions = (pred_df2.mean(axis=1) * 0.5) + (pred_df1.mean(axis=1) * 0.3) + (pred_df3.mean(axis=1) * 0.2)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T09:49:47.488699Z","iopub.execute_input":"2021-08-01T09:49:47.489066Z","iopub.status.idle":"2021-08-01T09:54:03.447587Z","shell.execute_reply.started":"2021-08-01T09:49:47.489029Z","shell.execute_reply":"2021-08-01T09:54:03.446698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### train ###\n\ntrain_pred_df1 = pd.DataFrame()\ntrain_pred_df2 = pd.DataFrame()\ntrain_pred_df3 = pd.DataFrame()\n\nfor fold in tqdm(range(5)):\n    train_pred_df1[f'fold{fold}'] = run(fold%5, '../input/roberta-transformers-pytorch/roberta-base/', '../input/commonlit-roberta-base-i/', test_flag=False)\n    train_pred_df2[f'fold{fold+5}'] = run(fold%5, '../input/roberta-transformers-pytorch/roberta-large', '../input/roberta-large-itptfit/', test_flag=False)\n    train_pred_df3[f'fold{fold+10}'] = run(fold%5, '../input/roberta-transformers-pytorch/roberta-large', '../input/commonlit-roberta-large-ii/', test_flag=False)\n\ntrain_pred_df1 = np.array(train_pred_df1)\ntrain_pred_df2 = np.array(train_pred_df2)\ntrain_pred_df3 = np.array(train_pred_df3)\n\ntrain_model2_predictions = (train_pred_df2.mean(axis=1) * 0.5) + (train_pred_df1.mean(axis=1) * 0.3) + (train_pred_df3.mean(axis=1) * 0.2)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T09:54:03.448886Z","iopub.execute_input":"2021-08-01T09:54:03.449243Z","iopub.status.idle":"2021-08-01T09:58:10.333531Z","shell.execute_reply.started":"2021-08-01T09:54:03.449207Z","shell.execute_reply":"2021-08-01T09:58:10.332388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model 3 \n\nInspired from: https://www.kaggle.com/jcesquiveld/best-transformer-representations","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport random\n\nfrom transformers import AutoConfig, AutoModel, AutoTokenizer, AdamW, get_linear_schedule_with_warmup, logging\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, TensorDataset, SequentialSampler, RandomSampler, DataLoader\n\nfrom tqdm.notebook import tqdm\n\nimport gc; gc.enable()\nfrom IPython.display import clear_output\n\nfrom sklearn.model_selection import StratifiedKFold\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style('whitegrid')\nlogging.set_verbosity_error()","metadata":{"execution":{"iopub.status.busy":"2021-08-01T09:58:10.335231Z","iopub.execute_input":"2021-08-01T09:58:10.335656Z","iopub.status.idle":"2021-08-01T09:58:10.345957Z","shell.execute_reply.started":"2021-08-01T09:58:10.335614Z","shell.execute_reply":"2021-08-01T09:58:10.344846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"INPUT_DIR = '../input/commonlitreadabilityprize'\nMODEL_DIR = '../input/roberta-transformers-pytorch/roberta-large'\nCHECKPOINT_DIR = '../input/clrp-mean-pooling/'\n\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nMAX_LENGTH = 248\nTEST_BATCH_SIZE = 1\nHIDDEN_SIZE = 1024\n\nNUM_FOLDS = 5\nSEEDS = [113]\n\ntest = pd.read_csv(os.path.join(INPUT_DIR, 'test.csv'))\ntrain = pd.read_csv(os.path.join(INPUT_DIR, 'train.csv'))","metadata":{"execution":{"iopub.status.busy":"2021-08-01T09:58:33.960315Z","iopub.execute_input":"2021-08-01T09:58:33.960647Z","iopub.status.idle":"2021-08-01T09:58:34.001338Z","shell.execute_reply.started":"2021-08-01T09:58:33.960615Z","shell.execute_reply":"2021-08-01T09:58:34.000549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MeanPoolingModel(nn.Module):\n    \n    def __init__(self, model_name):\n        super().__init__()\n        \n        config = AutoConfig.from_pretrained(model_name)\n        self.model = AutoModel.from_pretrained(model_name, config=config)\n        self.linear = nn.Linear(HIDDEN_SIZE, 1)\n        self.loss = nn.MSELoss()\n        \n    def forward(self, input_ids, attention_mask, labels=None):\n        \n        outputs = self.model(input_ids, attention_mask)\n        last_hidden_state = outputs[0]\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        logits = self.linear(mean_embeddings)\n        \n        preds = logits.squeeze(-1).squeeze(-1)\n        \n        if labels is not None:\n            loss = self.loss(preds.view(-1).float(), labels.view(-1).float())\n            return loss\n        else:\n            return preds","metadata":{"execution":{"iopub.status.busy":"2021-08-01T09:58:34.442436Z","iopub.execute_input":"2021-08-01T09:58:34.442786Z","iopub.status.idle":"2021-08-01T09:58:34.450931Z","shell.execute_reply.started":"2021-08-01T09:58:34.442754Z","shell.execute_reply":"2021-08-01T09:58:34.449947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_test_loader(data):\n\n    x_test = data.excerpt.tolist()\n    \n    tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n\n    encoded_test = tokenizer.batch_encode_plus(\n        x_test, \n        add_special_tokens=True, \n        return_attention_mask=True, \n        padding='max_length', \n        truncation=True,\n        max_length=MAX_LENGTH, \n        return_tensors='pt'\n    )\n\n    dataset_test = TensorDataset(\n        encoded_test['input_ids'],\n        encoded_test['attention_mask']\n    )\n\n    dataloader_test = DataLoader(\n        dataset_test,\n        sampler = SequentialSampler(dataset_test),\n        batch_size=TEST_BATCH_SIZE\n    )\n    \n    return dataloader_test\n\ntest_dataloader = get_test_loader(test)\ntrain_dataloader = get_test_loader(train)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T09:58:34.981205Z","iopub.execute_input":"2021-08-01T09:58:34.981514Z","iopub.status.idle":"2021-08-01T09:58:35.39685Z","shell.execute_reply.started":"2021-08-01T09:58:34.981483Z","shell.execute_reply":"2021-08-01T09:58:35.395957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_predictions = []\nfor seed in SEEDS:\n    \n    fold_predictions = []\n    \n    for fold in tqdm(range(NUM_FOLDS)):\n        model_path = f\"model_{seed + 1}_{fold + 1}.pth\"\n        \n        print(f\"\\nUsing {model_path}\")\n        \n        model_path = CHECKPOINT_DIR + f\"model_{seed + 1}_{fold + 1}.pth\"\n        model = MeanPoolingModel(MODEL_DIR)\n        model.load_state_dict(torch.load(model_path)) \n        model.to(DEVICE)\n        model.eval()\n\n        predictions = []\n        for batch in test_dataloader:\n\n            batch = tuple(b.to(DEVICE) for b in batch)\n\n            inputs = {'input_ids':      batch[0],\n                      'attention_mask': batch[1],\n                      'labels':         None,\n                     }\n\n     \n            preds = model(**inputs).item()\n            predictions.append(preds)\n            \n        del model \n        gc.collect()\n            \n        fold_predictions.append(predictions)\n    all_predictions.append(np.mean(fold_predictions, axis=0).tolist())\n    \nmodel3_predictions = np.mean(all_predictions,axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T09:58:38.617776Z","iopub.execute_input":"2021-08-01T09:58:38.618094Z","iopub.status.idle":"2021-08-01T10:00:19.108076Z","shell.execute_reply.started":"2021-08-01T09:58:38.618065Z","shell.execute_reply":"2021-08-01T10:00:19.1014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_all_predictions = []\nfor seed in SEEDS:\n    \n    fold_predictions = []\n    \n    for fold in tqdm(range(NUM_FOLDS)):\n        model_path = f\"model_{seed + 1}_{fold + 1}.pth\"\n        \n        print(f\"\\nUsing {model_path}\")\n        \n        model_path = CHECKPOINT_DIR + f\"model_{seed + 1}_{fold + 1}.pth\"\n        model = MeanPoolingModel(MODEL_DIR)\n        model.load_state_dict(torch.load(model_path)) \n        model.to(DEVICE)\n        model.eval()\n\n        predictions = []\n        for batch in train_dataloader:\n\n            batch = tuple(b.to(DEVICE) for b in batch)\n\n            inputs = {'input_ids':      batch[0],\n                      'attention_mask': batch[1],\n                      'labels':         None,\n                     }\n\n     \n            preds = model(**inputs).item()\n            predictions.append(preds)\n            \n        del model \n        gc.collect()\n            \n        fold_predictions.append(predictions)\n    train_all_predictions.append(np.mean(fold_predictions, axis=0).tolist())\n    \ntrain_model3_predictions = np.mean(train_all_predictions,axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T10:00:19.109946Z","iopub.execute_input":"2021-08-01T10:00:19.110347Z","iopub.status.idle":"2021-08-01T10:02:17.765088Z","shell.execute_reply.started":"2021-08-01T10:00:19.11031Z","shell.execute_reply":"2021-08-01T10:02:17.763381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # predictions = model1_predictions * 0.5 + model2_predictions * 0.3 + model3_predictions * 0.2  # 0.461\n# # predictions = model1_predictions * 0.45 + model2_predictions * 0.35 + model3_predictions * 0.2  # 0.461\n# predictions = model1_predictions * 0.40 + model2_predictions * 0.25 + model3_predictions * 0.35   #\n# predictions","metadata":{"execution":{"iopub.status.busy":"2021-08-01T10:02:17.768979Z","iopub.execute_input":"2021-08-01T10:02:17.769476Z","iopub.status.idle":"2021-08-01T10:02:17.772728Z","shell.execute_reply.started":"2021-08-01T10:02:17.769439Z","shell.execute_reply":"2021-08-01T10:02:17.77188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_predictions = train_model1_predictions * 0.40 + train_model2_predictions * 0.25 + train_model3_predictions * 0.35   #\n# train_predictions\n","metadata":{"execution":{"iopub.status.busy":"2021-08-01T10:02:17.774034Z","iopub.execute_input":"2021-08-01T10:02:17.774531Z","iopub.status.idle":"2021-08-01T10:02:17.785112Z","shell.execute_reply.started":"2021-08-01T10:02:17.774494Z","shell.execute_reply":"2021-08-01T10:02:17.784236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_results = pd.DataFrame(np.vstack((train_model1_predictions, train_model2_predictions, train_model3_predictions)).transpose(), \n                       columns=['model1','model2','model3'])\n\ntrain_results['target'] = train_df['target']\ntrain_results.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-01T10:02:17.786448Z","iopub.execute_input":"2021-08-01T10:02:17.786979Z","iopub.status.idle":"2021-08-01T10:02:17.828825Z","shell.execute_reply.started":"2021-08-01T10:02:17.786941Z","shell.execute_reply":"2021-08-01T10:02:17.827827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_results = pd.DataFrame(np.vstack((model1_predictions, model2_predictions, model3_predictions)).transpose(), \n                       columns=['model1','model2','model3'])\n","metadata":{"execution":{"iopub.status.busy":"2021-08-01T10:02:17.830224Z","iopub.execute_input":"2021-08-01T10:02:17.830619Z","iopub.status.idle":"2021-08-01T10:02:17.836182Z","shell.execute_reply.started":"2021-08-01T10:02:17.830579Z","shell.execute_reply":"2021-08-01T10:02:17.834959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\n \n# Create linear regression object\nregr = LinearRegression()\n\n# Train the model using the training sets\nregr.fit(train_results.drop('target', axis=1), train_results.target)\n\n# Make predictions using the testing set\ny_pred = regr.predict(test_results)\n\n# The coefficients\nprint('Coefficients: \\n', regr.coef_)\nprint('Coefficients Sum: \\n', sum(regr.coef_))","metadata":{"execution":{"iopub.status.busy":"2021-08-01T10:05:11.303507Z","iopub.execute_input":"2021-08-01T10:05:11.303954Z","iopub.status.idle":"2021-08-01T10:05:11.4135Z","shell.execute_reply.started":"2021-08-01T10:05:11.30392Z","shell.execute_reply":"2021-08-01T10:05:11.412495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Coefficients: \\n', regr.coef_)\nM1, M2, M3 = regr.coef_\nprint(M1, M2, M3)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T10:06:17.835845Z","iopub.execute_input":"2021-08-01T10:06:17.836289Z","iopub.status.idle":"2021-08-01T10:06:17.846101Z","shell.execute_reply.started":"2021-08-01T10:06:17.836249Z","shell.execute_reply":"2021-08-01T10:06:17.844783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Stacking with LAMA","metadata":{}},{"cell_type":"code","source":"# TIMEOUT = 15_000 # Time in seconds for automl run\n# TARGET_NAME = 'target' # Target column name\n\n# def rmse(x, y): return np.sqrt(mean_squared_error(x, y))\n# task = Task('reg', metric=rmse)\n# roles = {\n#     'target': TARGET_NAME,\n#         }","metadata":{"execution":{"iopub.status.busy":"2021-08-01T09:58:12.590434Z","iopub.status.idle":"2021-08-01T09:58:12.591208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# automl = TabularAutoML(task=task,\n#                           timeout=TIMEOUT,\n#                           general_params={'nested_cv': False, 'use_algos': [['linear_l2']]},\n#                           reader_params={'cv': 5},\n#                           selection_params={'mode': 1},\n#                           )\n\n# oof_pred = automl.fit_predict(train_results, roles=roles)\n# print('')\n# print(rmse(train_results[TARGET_NAME], oof_pred.data[:, 0]))","metadata":{"execution":{"iopub.status.busy":"2021-08-01T09:58:12.592594Z","iopub.status.idle":"2021-08-01T09:58:12.593282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predictions = automl.predict(test_results).data[:, 0]","metadata":{"execution":{"iopub.status.busy":"2021-08-01T09:58:12.594509Z","iopub.status.idle":"2021-08-01T09:58:12.595236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.target = y_pred # predictions\nsubmission_df","metadata":{"execution":{"iopub.status.busy":"2021-08-01T10:07:28.612306Z","iopub.execute_input":"2021-08-01T10:07:28.612657Z","iopub.status.idle":"2021-08-01T10:07:28.627795Z","shell.execute_reply.started":"2021-08-01T10:07:28.612624Z","shell.execute_reply":"2021-08-01T10:07:28.626634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T10:07:31.579888Z","iopub.execute_input":"2021-08-01T10:07:31.580211Z","iopub.status.idle":"2021-08-01T10:07:31.881807Z","shell.execute_reply.started":"2021-08-01T10:07:31.580181Z","shell.execute_reply":"2021-08-01T10:07:31.880754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}