{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Being a novice in Natural Language Processing based DL tasks, this Kaggle competition seems like a great opportunity to break the ice and be comfortable with this subdomain of Deep Learning. \n\nHaving said that I have some prior experience using HuggingFace Transformers in TensorFlow/Keras ecosystem. I have also written this W&B report on [How to Fine-Tune HuggingFace Tranformer with W&B?](https://wandb.ai/ayush-thakur/huggingface/reports/How-to-Fine-Tune-HuggingFace-Tranformer-with-W-B---Vmlldzo0MzQ2MDc) that might come in handy to few.\n\nThis kernel is about training a HuggingFace transformer using TensorFlow/Keras. I have tried to make it as intuitive as possible for a regular Keras users and is trying out NLP or HuggingFace for the first time. This Kernel can be divided into few blocks:\n\n* **Import and Setups** - here we will import relevant libraries and setup Weights and Biases related steps for tracking experiments. \n\n* **Hyperparameters** - configuration dictionary for all hyperparameters.\n\n* **Prepare Dataset** - here we will create K-fold split of the training data based on this easy to understand [kernel](https://www.kaggle.com/abhishek/step-1-create-folds) by [Abhishek Thakur](https://www.kaggle.com/abhishek). This is followed by building an input pipeline using `tf.data` API.\n\n* **Build Model** - here we will build our model definition. Note that you can use any Transformer of your choice. \n\n* **Train with W&B** - here we do a K (5) fold training and will use Weights and Biases for experiment tracking. \n\n* **Evaluate** - here we will evaluate the model for local CV score.","metadata":{}},{"cell_type":"markdown","source":"# üß∞ Imports and Setups","metadata":{}},{"cell_type":"code","source":"# TensorFlow related\nimport tensorflow as tf\nprint(tf.__version__)\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import * \n\nimport os\nimport gc\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.model_selection import StratifiedKFold\n\n# HuggingFace related\nfrom transformers import DistilBertTokenizerFast\nfrom transformers import TFDistilBertModel\n\nfrom kaggle_secrets import UserSecretsClient","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the latest version of W&B\n!pip install -q wandb\n\n# Weights and Biases related imports\nimport wandb\nfrom wandb.keras import WandbCallback\n\n# W&B login - please visit wandb.ai/authorize to get your auth key\nwandb.login()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> üìå Learn more about why and how to use Weights and Biases in this kernel: [Experiment Tracking with Weights and Biases](https://www.kaggle.com/ayuraj/experiment-tracking-with-weights-and-biases)","metadata":{}},{"cell_type":"markdown","source":"# üìÄ Hyperparameters","metadata":{}},{"cell_type":"code","source":"CONFIG = dict(\n    # Dataset related \n    num_splits = 5, \n    \n    # Model related\n    model_name = 'DistilBERT',\n    max_token_length = 256,\n    \n    # Training related\n    batch_size = 64,\n    epochs = 100,\n    init_lr = 1e-4,\n    earlys_patience = 10,\n    reduce_lr_plateau = 5,\n    \n    # Misc\n    seed = 42,\n    wandb_kernel = True,\n    competition = 'commonlit'\n)\n\nsave_dir = 'trained/'\nos.makedirs(save_dir, exist_ok=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üî® Prepare Dataset","metadata":{}},{"cell_type":"markdown","source":"## 1. Create Folds\n\nThis is based on the [Step 1: Create Folds](https://www.kaggle.com/abhishek/step-1-create-folds) kernel by [Abhishek Thakur](https://www.kaggle.com/abhishek). Even though the dataset is small this competition is not that easy to crack.","metadata":{}},{"cell_type":"code","source":"# Ref: https://www.kaggle.com/abhishek/step-1-create-folds\ndef create_folds(data, num_splits):\n    # we create a new column called kfold and fill it with -1\n    data[\"kfold\"] = -1\n    \n    # the next step is to randomize the rows of the data\n    data = data.sample(frac=1).reset_index(drop=True)\n\n    # calculate number of bins by Sturge's rule\n    # I take the floor of the value, you can also\n    # just round it\n    num_bins = int(np.floor(1 + np.log2(len(data))))\n    \n    # bin targets\n    data.loc[:, \"bins\"] = pd.cut(\n        data[\"target\"], bins=num_bins, labels=False\n    )\n    \n    # initiate the kfold class from model_selection module\n    kf = StratifiedKFold(n_splits=num_splits)\n    \n    # fill the new kfold column\n    # note that, instead of targets, we use bins!\n    for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)):\n        data.loc[v_, 'kfold'] = f\n    \n    # drop the bins column\n    data = data.drop(\"bins\", axis=1)\n\n    # return dataframe with folds\n    return data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# read training data\ndf = pd.read_csv(\"../input/commonlitreadabilityprize/train.csv\")\n\n# create folds\ndf = create_folds(df, num_splits=CONFIG['num_splits'])\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Create Dataloader\n\n**Two words on Tokenization**: Tokenizing a text is splitting it into words or subwords, which then are converted to ids through a look-up table. The conversion of tokens to ids through a look-up table depends on the vocabulary(the set of all unique words and tokens used) which depends on the dataset, the task, and the resulting pre-trained model. **HuggingFace tokenizer automatically downloads the vocab used during pretraining or fine-tuning a given model. We need not create our own vocab from the dataset for fine-tuning.**","metadata":{}},{"cell_type":"code","source":"# Use the tokenizer of your choice\ntokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n# Save the tokenizer so that you can download the files and move it to a Kaggle dataset.\ntokenizer.save_pretrained(save_dir)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> üìå The name of the tokenizer will depend on the choice of your Transformer model. ","metadata":{}},{"cell_type":"markdown","source":"### For explaination purposes\n\nFew of the cells below are just for explaination pusposes. Since we are training using K-Folds, the input pipeline will be created inside the for loop. The cells below will give you an insight on how I am using `tf.data` and HuggingFace's Tokenizer to build the input pipeline.\n\n1. Here (K-1)th split is taken as validation data. We will train on the rest of the data. ","metadata":{}},{"cell_type":"code","source":"def get_train_val_split(fold_num):\n    # Get training split\n    train_df = df.loc[df['kfold']!=fold_num]\n    train_df = train_df[['excerpt', 'target']]\n    \n    # Get validation split\n    val_df = df.loc[df['kfold']==fold_num]\n    val_df = val_df[['excerpt', 'target']]\n    \n    # Extract texts and labels.\n    train_text, train_label = list(train_df.excerpt.values), list(train_df.target.values)\n    val_text, val_label = list(val_df.excerpt.values), list(val_df.target.values)\n\n    return train_text, train_label, val_text, val_label\n\ntrain_text, train_label, val_text, val_label = get_train_val_split(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2. Pass in the raw text to the tokenizer. The output of this process is a dictionary `*_encodings` with `input_ids` and `attention_mask` as keys. For some tokenizers you will have `token_ids` as another key. Please modify the training pipeline and model accordingly. It will be as easy as cooking noodles. \n","metadata":{}},{"cell_type":"code","source":"train_encodings = tokenizer(train_text, truncation=True, padding=True, max_length=CONFIG['max_token_length'])\nval_encodings = tokenizer(val_text, truncation=True, padding=True, max_length=CONFIG['max_token_length'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"3. We will use `tf.data` wrap the encoding and label. `tf.data` can create highly efficient input pipeline. In case your tokenizer's output also has a key `token_ids` consider modifying the `parse_data` appropriately. ","metadata":{}},{"cell_type":"code","source":"AUTOTUNE = tf.data.AUTOTUNE\n\n# Note that some tokenizers also returns 'token_id'. Modify this function accordingly. \n@tf.function\ndef parse_data(from_tokenizer, target):\n    input_ids = from_tokenizer['input_ids']\n    attention_mask = from_tokenizer['attention_mask']\n    \n    target = tf.cast(target, tf.float32)\n    \n    return {'input_ids': input_ids,\n            'attention_mask': attention_mask}, target\n\n# Utility function to build dataloaders\ndef get_dataloaders(train_encodings, train_label, val_encodings, val_label):\n    trainloader = tf.data.Dataset.from_tensor_slices((dict(train_encodings), list(train_label)))\n    validloader = tf.data.Dataset.from_tensor_slices((dict(val_encodings), list(val_label)))\n\n    trainloader = (\n        trainloader\n        .shuffle(1024)\n        .map(parse_data, num_parallel_calls=AUTOTUNE)\n        .batch(CONFIG['batch_size'])\n        .prefetch(AUTOTUNE)\n    )\n\n    validloader = (\n        validloader\n        .map(parse_data, num_parallel_calls=AUTOTUNE)\n        .batch(CONFIG['batch_size'])\n        .prefetch(AUTOTUNE)\n    )\n    \n    return trainloader, validloader\n\ntrainloader, validloader = get_dataloaders(train_encodings, train_label, val_encodings, val_label)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize a batch of data\nnext(iter(trainloader))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üê§ Build Model","metadata":{}},{"cell_type":"code","source":"# You can use a Transformer model of your choice.\ntransformer_model = TFDistilBertModel.from_pretrained('distilbert-base-uncased')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def CommonLitModel():\n    # Input layers\n    input_ids = Input(shape=(CONFIG['max_token_length'],), dtype=tf.int32, name=\"input_ids\")\n    attention_mask = Input(shape=(CONFIG['max_token_length'],), dtype=tf.int32, name=\"attention_mask\")\n    \n    # Transformer backbone to extract features\n    sequence_output = transformer_model(input_ids=input_ids, attention_mask=attention_mask)[0]\n    clf_output = sequence_output[:, 0, :]\n    \n    # Dropout to regularize \n    clf_output = Dropout(0.1)(clf_output)\n    \n    # Output layer with linear activation as we are doing regression. \n    out = Dense(1, activation='linear')(clf_output)\n    \n    # Build model \n    model = Model(inputs=[input_ids, attention_mask], outputs=out)\n    \n    return model\n\n# Sanity check model\ntf.keras.backend.clear_session()\nmodel = CommonLitModel()\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üöÑ Train with W&B\n\nSince it's going to be a K-fold training we will have a for loop that will iterate `CONFIG['num_splits']` number of times. In the loop the following steps will be repeated:\n\n1. Get the training and validation dataset.\n2. Pass the dataset to the tokenizer.\n3. Prepare training and validation dataloader.\n4. Initialize model.\n5. Train the model.\n6. Evaluate on validation dataset.\n7. Save model.\n\nWe will use W&B to log all the metrics. We will use a `group` argument when initializing a W&B run (`wandb.init`) that will enable us to group all the runs in the W&B dashboard. \n\n### [Check out the W&B dashboard $\\rightarrow$](https://wandb.ai/ayush-thakur/commonlit?workspace=user-ayush-thakur)\n![img](https://i.imgur.com/WYo0b4q.gif)\n","metadata":{}},{"cell_type":"code","source":"# Early stopping \nearlystopper = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', patience=CONFIG['earlys_patience'], verbose=0, mode='min',\n    restore_best_weights=True\n)\n\n# Reduce LR on Plateau\nreducelrplateau = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss', factor=0.2, patience=CONFIG['reduce_lr_plateau']\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for fold in range(CONFIG['num_splits']):\n    # 1. Get the training and validation dataset.\n    train_text, train_label, val_text, val_label = get_train_val_split(fold)\n    \n    # 2. Pass the dataset to the tokenizer.\n    train_encodings = tokenizer(train_text, truncation=True, padding=True, max_length=CONFIG['max_token_length'])\n    val_encodings = tokenizer(val_text, truncation=True, padding=True, max_length=CONFIG['max_token_length'])\n    \n    # 3. Prepare training and validation dataloader.\n    trainloader, validloader = get_dataloaders(train_encodings, train_label, val_encodings, val_label)\n    \n    # 4. Initialize model\n    tf.keras.backend.clear_session()\n    model = CommonLitModel()\n    \n    # Compile\n    optimizer = tf.keras.optimizers.Adam(lr=1e-5)\n    model.compile(optimizer, loss='mean_squared_error', metrics=[tf.keras.metrics.RootMeanSquaredError()])    \n    \n    # Initialize Weights and Biases run\n    run = wandb.init(project='commonlit', \n                     config=CONFIG,\n                     group='DistilBERT-K-Fold',\n                     job_type='train_kfold')\n    \n    # 5. Train the model\n    _ = model.fit(trainloader, \n              epochs=CONFIG['epochs'], \n              validation_data=validloader,\n              callbacks=[WandbCallback(),\n                         reducelrplateau,\n                         earlystopper])\n    \n    # 6. Evaluate on validation dataset.\n    loss, rmse = model.evaluate(validloader)\n    wandb.log({'valid_rmse': rmse})\n    \n    # 7. Save model\n    model.save(f'{save_dir}/distil-bert_{fold}')\n    \n    # Close W&B run\n    run.finish()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}