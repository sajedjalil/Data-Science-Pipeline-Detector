{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 3- Learning ðŸ¤—  - Out-of-the-box Electra [LB: 0.58]\n\nHi, and welcome! This is the second kernel of the series `Learning ðŸ¤—`, a personal project I'm currently working on. I am an experienced data scientist diving into the hugging face transformers library and this series or kernels is a \"working diary\", as I do it. The approach I'm taking is the following: \n1. Explore various out-of-the-box models, without digging into their technical details. \n2. After that, I'll start going over the best ranked public kernels, understand their ideas, and reproduce them by myself. \n\nYou are invited to follow me in this journey. In this short kernel  we fine-tune an out-of-the-box cased RoBERTa, with just the minimal set up required for it to run in this competition, obtaining a leaderboard score of `0.53`. \n\n\nThis is an ongoing project, so expect more notebooks to be added to the series soon. Actually, we are currently working on the following ones:\n\n\n1. [Learning ðŸ¤—  - Out-of-the-box BERT [LB: 0.577]](https://www.kaggle.com/julian3833/1-learning-out-of-the-box-bert-lb-0-577)\n2. [Learning ðŸ¤— - Out-of-the-box RoBERTa [LB: 0.53]](https://www.kaggle.com/julian3833/2-learning-out-of-the-box-roberta-lb-0-53)\n3. [Learning ðŸ¤— - Out-of-the-box Electra [LB: 0.58]](https://www.kaggle.com/julian3833/3-learning-out-of-the-box-electra-lb/) (this notebook)\n4. _Learning ðŸ¤— - Minimal fine tuning (WIP)_\n5. _Learning ðŸ¤— - Preprocessing (WIP)_\n6. _Learning ðŸ¤— - Reviewing public kernels (WIP)_\n7. _Learning ðŸ¤— - Intra-domain pre training RoBERTa (WIP)_\n\n\n## This notebook\n\nThe code below is just a copy of the code in [1- Learning ðŸ¤—  - Out-of-the-box BERT [LB: 0.577]](https://www.kaggle.com/julian3833/1-learning-out-of-the-box-bert-lb-0-577) with just 2 changes, which are the following ones. Refer to that notebook for a more detailed description of the process and a more verbose, commented code.","metadata":{}},{"cell_type":"code","source":"MODEL_NAME = \"../input/electra/base-discriminator\"\nEPOCHS = 1","metadata":{"execution":{"iopub.status.busy":"2021-06-27T05:06:38.625742Z","iopub.execute_input":"2021-06-27T05:06:38.626137Z","iopub.status.idle":"2021-06-27T05:06:38.630136Z","shell.execute_reply.started":"2021-06-27T05:06:38.626055Z","shell.execute_reply":"2021-06-27T05:06:38.629295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification, AutoTokenizer\n\ndef load_dfs():\n    train_csv = '../input/commonlitreadabilityprize/train.csv'\n    test_csv = '../input/commonlitreadabilityprize/test.csv'\n    df_train = pd.read_csv(train_csv)[[\"excerpt\", \"target\"]].rename(columns={\"target\": \"label\", \"excerpt\": \"text\"})\n    df_test = pd.read_csv(test_csv)[[\"id\", \"excerpt\"]].rename(columns={ \"excerpt\": \"text\"})\n    return df_train, df_test\n\ndef rmse(y_true, y_pred): return np.sqrt(((y_true - y_pred) ** 2).mean().item())\n    \ndef compute_metrics(pred_results):\n    y_pred = pred_results.predictions.squeeze()\n    y_true = pred_results.label_ids\n    return {\"rmse\": rmse(y_true, y_pred)}\n\ndef submit(trainer, ds_test):\n    sample_sub_csv = '../input/commonlitreadabilityprize/sample_submission.csv'\n    pred_csv = '/kaggle/working/submission.csv'\n    pred_results = trainer.predict(ds_test)\n    y_pred = pred_results.predictions.squeeze()\n    df_res = pd.read_csv(sample_sub_csv)\n    df_res['target'] = y_pred.tolist()\n    df_res.to_csv(pred_csv, index=False)\n\ndef tokenize(tokenizer, df_train, df_val, df_test):    \n    train_tokenized = tokenizer(df_train['text'].tolist(), padding=\"max_length\", truncation=True, max_length=512)\n    val_tokenized = tokenizer(df_val['text'].tolist(), padding=\"max_length\", truncation=True, max_length=512)\n    test_tokenized = tokenizer(df_test['text'].tolist(), padding=\"max_length\", truncation=True, max_length=512)\n    train_tokenized['label'] = df_train['label'].tolist()\n    val_tokenized['label'] = df_val['label'].tolist()\n    ds_train = [dict(zip(train_tokenized,t)) for t in zip(*train_tokenized.values())]\n    ds_val = [dict(zip(val_tokenized,t)) for t in zip(*val_tokenized.values())]\n    ds_test = [dict(zip(test_tokenized,t)) for t in zip(*test_tokenized.values())]\n    return ds_train, ds_val, ds_test\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=1)\ndf_base, df_test = load_dfs()\ndf_train, df_val = train_test_split(df_base, test_size=0.066)\nds_train, ds_val, ds_test = tokenize(tokenizer, df_train, df_val, df_test)\nargs = TrainingArguments(\"/kaggle/working/model/\", num_train_epochs=EPOCHS, \n                         evaluation_strategy=\"steps\", eval_steps=100, report_to=\"none\")\ntrainer = Trainer(model=model, args=args, train_dataset=ds_train, eval_dataset=ds_val, \n                  compute_metrics=compute_metrics)\ntrainer.train()\nsubmit(trainer, ds_test)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-27T05:06:38.902989Z","iopub.execute_input":"2021-06-27T05:06:38.903295Z","iopub.status.idle":"2021-06-27T05:09:52.416455Z","shell.execute_reply.started":"2021-06-27T05:06:38.903267Z","shell.execute_reply":"2021-06-27T05:09:52.415532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ðŸ¤—ðŸ¤— Thanks for reading this notebook! Remember to upvote if you found it useful, and stay tuned for the next deliveries! ðŸ¤—ðŸ¤—","metadata":{}}]}