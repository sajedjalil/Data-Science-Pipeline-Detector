{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import libraries","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport gc\nimport pickle\nimport optuna\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import QuantileTransformer\n\nimport re\nimport nltk\nimport string\nfrom textblob import TextBlob\nfrom nltk.corpus import wordnet\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nimport lightgbm as lgb\nfrom xgboost import XGBRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom sklearn.linear_model import BayesianRidge, Lasso, ElasticNet","metadata":{"execution":{"iopub.status.busy":"2021-06-07T04:59:35.308502Z","iopub.execute_input":"2021-06-07T04:59:35.309028Z","iopub.status.idle":"2021-06-07T04:59:39.091354Z","shell.execute_reply.started":"2021-06-07T04:59:35.308914Z","shell.execute_reply":"2021-06-07T04:59:39.090249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load source datasets","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(\"../input/commonlitreadabilityprize/train.csv\")\ntrain_df.drop(['url_legal','license','standard_error'], inplace=True, axis=1)\ntrain_df.set_index(\"id\", inplace=True)\nprint(f\"train_df: {train_df.shape}\\n\")\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-07T04:59:39.092803Z","iopub.execute_input":"2021-06-07T04:59:39.093126Z","iopub.status.idle":"2021-06-07T04:59:39.232478Z","shell.execute_reply.started":"2021-06-07T04:59:39.093095Z","shell.execute_reply":"2021-06-07T04:59:39.231433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")\ntest_df.drop(['url_legal','license'], inplace=True, axis=1)\ntest_df.set_index(\"id\", inplace=True)\nprint(f\"test_df: {test_df.shape}\\n\")\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-07T04:59:39.234544Z","iopub.execute_input":"2021-06-07T04:59:39.234865Z","iopub.status.idle":"2021-06-07T04:59:39.257081Z","shell.execute_reply.started":"2021-06-07T04:59:39.234816Z","shell.execute_reply":"2021-06-07T04:59:39.256037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Extract target label","metadata":{}},{"cell_type":"code","source":"_, ax = plt.subplots(1, 2, figsize=(15, 5))\nsns.boxplot(x='target', data=train_df, ax=ax[0])\nsns.histplot(x='target', data=train_df, ax=ax[1])\nax[0].title.set_text('Box Plot - target')\nax[1].title.set_text('Hist Plot - target')","metadata":{"execution":{"iopub.status.busy":"2021-06-07T04:59:39.258562Z","iopub.execute_input":"2021-06-07T04:59:39.259038Z","iopub.status.idle":"2021-06-07T04:59:39.71525Z","shell.execute_reply.started":"2021-06-07T04:59:39.259001Z","shell.execute_reply":"2021-06-07T04:59:39.7145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Ytrain = train_df['target'].values\nYtrain_strat = pd.qcut(train_df['target'].values, q=5, labels=range(0,5))\ntrain_df.drop(['target'], inplace=True, axis=1)\nprint(\"Ytrain: {}\".format(Ytrain.shape))","metadata":{"execution":{"iopub.status.busy":"2021-06-07T04:59:39.716362Z","iopub.execute_input":"2021-06-07T04:59:39.716814Z","iopub.status.idle":"2021-06-07T04:59:39.725222Z","shell.execute_reply.started":"2021-06-07T04:59:39.716758Z","shell.execute_reply":"2021-06-07T04:59:39.724216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Engineering","metadata":{}},{"cell_type":"code","source":"def decontraction(phrase):\n    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase","metadata":{"execution":{"iopub.status.busy":"2021-06-07T04:59:39.726348Z","iopub.execute_input":"2021-06-07T04:59:39.726622Z","iopub.status.idle":"2021-06-07T04:59:39.735488Z","shell.execute_reply.started":"2021-06-07T04:59:39.726595Z","shell.execute_reply":"2021-06-07T04:59:39.734359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dialog_parser(text):\n    \n    tokenized = nltk.word_tokenize(text)\n    \n    # let's set up some lists to hold our pieces of narrative and dialog\n    parsed_dialog = []\n    parsed_narrative = []\n    \n    # and this list will be a bucket for the text we're currently exploring\n    current = []\n\n    # now let's set up values that will help us loop through the text\n    length = len(tokenized)\n    found_q = False\n    counter = 0\n    quote_open, quote_close = '``', \"''\"\n\n    # now we'll start our loop saying that as long as our sentence is...\n    while counter < length:\n        word = tokenized[counter]\n\n        # until we find a quotation mark, we're working with narrative\n        if quote_open not in word and quote_close not in word:\n            current.append(word)\n\n        # here's what we do when we find a closed quote\n        else:\n            # we append the narrative we've collected & clear our our\n            # current variable\n            parsed_narrative.append(current)\n            current = []\n            \n            # now current is ready to hold dialog and we're working on\n            # a piece of dialog\n            current.append(word)\n            found_q = True\n\n            # while we're in the quote, we're going to increment the counter\n            # and append to current in this while loop\n            while found_q and counter < length-1:\n                counter += 1\n                if quote_close not in tokenized[counter]:\n                    current.append(tokenized[counter])\n                else:\n                    # if we find a closing quote, we add our dialog to the\n                    # appropriate list, clear current and flip our found_q\n                    # variable to False\n                    current.append(tokenized[counter])\n                    parsed_dialog.append(current)\n                    current = []\n                    found_q = False\n\n        # increment the counter to move us through the text\n        counter += 1\n    \n    if len(parsed_narrative) == 0:\n        parsed_narrative.append(current)\n    \n    mean_dialog_word_len = 0\n    \n    if len(parsed_dialog) > 0:\n        for text in parsed_dialog:\n            join_text = \" \".join(text)\n            join_text = join_text.replace('\"','')\n            join_text = join_text.replace(\"''\",\"\")\n            mean_dialog_word_len += len(join_text.split())\n        \n        mean_dialog_word_len /= float(len(parsed_dialog))\n    \n    mean_narrative_word_len = 0\n    \n    if len(parsed_narrative) > 0:\n        for text in parsed_narrative:\n            join_text = \" \".join(text)\n            join_text = join_text.replace('\"','')\n            join_text = join_text.replace(\"''\",\"\")\n            mean_narrative_word_len += len(join_text.split())\n        \n        mean_narrative_word_len /= float(len(parsed_narrative))\n\n    return len(parsed_dialog), len(parsed_narrative), mean_dialog_word_len, mean_narrative_word_len","metadata":{"execution":{"iopub.status.busy":"2021-06-07T04:59:39.737068Z","iopub.execute_input":"2021-06-07T04:59:39.737775Z","iopub.status.idle":"2021-06-07T04:59:39.752751Z","shell.execute_reply.started":"2021-06-07T04:59:39.737721Z","shell.execute_reply":"2021-06-07T04:59:39.75194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_punctuations(text):\n    punct = []\n    punct += list(string.punctuation)\n    punct += '’'\n    punct += '-'\n    punct += ','\n    punct += '.'\n    punct += '?'\n    punct += '!'\n    punct.remove('\"')\n    \n    for punctuation in punct:\n        text = text.replace(punctuation, '')\n    return text","metadata":{"execution":{"iopub.status.busy":"2021-06-07T04:59:39.755536Z","iopub.execute_input":"2021-06-07T04:59:39.756171Z","iopub.status.idle":"2021-06-07T04:59:39.769702Z","shell.execute_reply.started":"2021-06-07T04:59:39.756128Z","shell.execute_reply":"2021-06-07T04:59:39.7687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def lemmatize_words(text):\n    lemmatizer = WordNetLemmatizer()\n    wordnet_map = {\n        \"N\": wordnet.NOUN, \n        \"V\": wordnet.VERB, \n        \"J\": wordnet.ADJ, \n        \"R\": wordnet.ADV\n    }\n    pos_tagged_text = nltk.pos_tag(text.split())\n    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])","metadata":{"execution":{"iopub.status.busy":"2021-06-07T04:59:39.771229Z","iopub.execute_input":"2021-06-07T04:59:39.771515Z","iopub.status.idle":"2021-06-07T04:59:39.782736Z","shell.execute_reply.started":"2021-06-07T04:59:39.771479Z","shell.execute_reply":"2021-06-07T04:59:39.78155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combined_df = train_df.append(test_df, sort=False, ignore_index=False)\n\ndel train_df\ndel test_df\ngc.collect()\n\ncombined_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-07T04:59:39.783893Z","iopub.execute_input":"2021-06-07T04:59:39.784363Z","iopub.status.idle":"2021-06-07T04:59:39.939494Z","shell.execute_reply.started":"2021-06-07T04:59:39.784328Z","shell.execute_reply":"2021-06-07T04:59:39.938385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"punct = []\npunct += list(string.punctuation)\npunct += '’'\npunct += '-'\npunct += ','\npunct += '.'\npunct += '?'\npunct += '!'\n\n\ncombined_df[\"excerpt_num_words\"] = combined_df[\"excerpt\"].apply(lambda x: len(str(x).split()))\ncombined_df[\"excerpt_num_unique_words\"] = combined_df[\"excerpt\"].apply(lambda x: len(set(str(x).split())))\ncombined_df[\"excerpt_num_chars\"] = combined_df[\"excerpt\"].apply(lambda x: len(str(x)))\ncombined_df[\"excerpt_num_stopwords\"] = combined_df[\"excerpt\"].apply(lambda x: len([w for w in str(x).lower().split() if w in set(stopwords.words('english'))]))\ncombined_df[\"excerpt_num_punctuations\"] =combined_df['excerpt'].apply(lambda x: len([c for c in str(x) if c in punct]))\ncombined_df[\"excerpt_num_words_upper\"] = combined_df[\"excerpt\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\ncombined_df[\"excerpt_num_words_title\"] = combined_df[\"excerpt\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\ncombined_df[\"excerpt_mean_word_len\"] = combined_df[\"excerpt\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\ncombined_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-07T04:59:39.941039Z","iopub.execute_input":"2021-06-07T04:59:39.941444Z","iopub.status.idle":"2021-06-07T05:00:44.647698Z","shell.execute_reply.started":"2021-06-07T04:59:39.941401Z","shell.execute_reply":"2021-06-07T05:00:44.646653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert to lower case\ncombined_df['Processed_excerpt'] = combined_df['excerpt'].apply(lambda x: str(x).lower().replace('\\\\', '').replace('_', ' '))\n\n# Remove double spaces\ncombined_df['Processed_excerpt'] = combined_df['Processed_excerpt'].apply(lambda x: re.sub('\\s+',  ' ', x))\n\n# Replace contractions (\"don't\" with \"do not\" and \"we've\" with \"we have\")\ncombined_df['Processed_excerpt'] = combined_df['Processed_excerpt'].apply(lambda x: decontraction(x))\n\n# Remove punctuations\ncombined_df['Processed_excerpt'] = combined_df['Processed_excerpt'].apply(remove_punctuations)\n\n# Lemmatize words\ncombined_df['Processed_excerpt'] = combined_df['Processed_excerpt'].apply(lambda text: lemmatize_words(text))\n\ncombined_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:00:44.648863Z","iopub.execute_input":"2021-06-07T05:00:44.649133Z","iopub.status.idle":"2021-06-07T05:01:15.800598Z","shell.execute_reply.started":"2021-06-07T05:00:44.649106Z","shell.execute_reply":"2021-06-07T05:01:15.799349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combined_df[\"excerpt_num_dialog\"] = combined_df[\"Processed_excerpt\"].apply(lambda x: dialog_parser(x)[0])\ncombined_df[\"excerpt_num_narrative\"] = combined_df[\"Processed_excerpt\"].apply(lambda x: dialog_parser(x)[1])\ncombined_df[\"excerpt_dialog_mean_word_len\"] = combined_df[\"Processed_excerpt\"].apply(lambda x: dialog_parser(x)[2])\ncombined_df[\"excerpt_narrative_mean_word_len\"] = combined_df[\"Processed_excerpt\"].apply(lambda x: dialog_parser(x)[3])\ncombined_df['excerpt_polarity'] = combined_df['Processed_excerpt'].apply(lambda x: TextBlob(x).sentiment[0])\ncombined_df['excerpt_subjectivity'] = combined_df['Processed_excerpt'].apply(lambda x: TextBlob(x).sentiment[1])\ncombined_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:01:15.802037Z","iopub.execute_input":"2021-06-07T05:01:15.802326Z","iopub.status.idle":"2021-06-07T05:01:39.175925Z","shell.execute_reply.started":"2021-06-07T05:01:15.802299Z","shell.execute_reply":"2021-06-07T05:01:39.174902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = combined_df[:Ytrain.shape[0]].copy()\ndf['target'] = Ytrain\nplt.figure(figsize=(15,12))\nsns.heatmap(df.corr(), annot=True, cmap=\"RdYlGn\")","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:01:39.177427Z","iopub.execute_input":"2021-06-07T05:01:39.1778Z","iopub.status.idle":"2021-06-07T05:01:40.764693Z","shell.execute_reply.started":"2021-06-07T05:01:39.17775Z","shell.execute_reply":"2021-06-07T05:01:40.763658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfidf = TfidfVectorizer(stop_words='english', ngram_range=(1,5), \n                        analyzer='word', max_df=0.95, min_df=3, \n                        use_idf=1, sublinear_tf=1, \n                        max_features=500, strip_accents='ascii')\n\nfeatures = tfidf.fit_transform(combined_df.Processed_excerpt).toarray()\n\nfeatures_df = pd.DataFrame(features, \n                           columns=tfidf.get_feature_names(), \n                           index=combined_df.index)\n\ncombined_df = pd.merge(combined_df, \n                       features_df, \n                       left_index=True, \n                       right_index=True)\n\ncombined_df.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:01:40.765941Z","iopub.execute_input":"2021-06-07T05:01:40.766234Z","iopub.status.idle":"2021-06-07T05:01:45.675724Z","shell.execute_reply.started":"2021-06-07T05:01:40.766206Z","shell.execute_reply":"2021-06-07T05:01:45.674666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(3):\n    print(f\"\\nOriginal Excerpt: \\n{combined_df.iloc[i]['excerpt']} \\n\\nProcessed Excerpt: \\n{combined_df.iloc[i]['Processed_excerpt']}\\n\")\n    print(\"=\"*150)","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:01:45.6773Z","iopub.execute_input":"2021-06-07T05:01:45.67794Z","iopub.status.idle":"2021-06-07T05:01:45.691241Z","shell.execute_reply.started":"2021-06-07T05:01:45.677891Z","shell.execute_reply":"2021-06-07T05:01:45.690019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Glove Embeddings","metadata":{}},{"cell_type":"code","source":"def sent2vec(text):\n    words = nltk.word_tokenize(text)\n    words = [w for w in words if w.isalpha()]\n    \n    M = []\n    for w in words:\n        try:\n            M.append(embeddings_index[w])\n        except:\n            continue\n    \n    M = np.array(M)\n    v = M.sum(axis=0)\n    if type(v) != np.ndarray:\n        return np.zeros(300)\n    \n    return v / np.sqrt((v ** 2).sum())","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:01:45.694307Z","iopub.execute_input":"2021-06-07T05:01:45.694777Z","iopub.status.idle":"2021-06-07T05:01:45.707368Z","shell.execute_reply.started":"2021-06-07T05:01:45.694729Z","shell.execute_reply":"2021-06-07T05:01:45.706005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(\"../input/gloveembeddings/Glove_840B_300d_Embeddings.txt\", 'rb') as handle: \n    data = handle.read()\n\nprocessed_data = pickle.loads(data)\nembeddings_index = processed_data['embeddings_index']\nprint('Word vectors found: {}'.format(len(embeddings_index)))\n\ndel processed_data\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:01:45.709485Z","iopub.execute_input":"2021-06-07T05:01:45.709823Z","iopub.status.idle":"2021-06-07T05:02:47.801222Z","shell.execute_reply.started":"2021-06-07T05:01:45.709792Z","shell.execute_reply":"2021-06-07T05:02:47.800224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"glove_vec = [sent2vec(x) for x in tqdm(combined_df[\"Processed_excerpt\"].values)]\ncol_list = ['glove_'+str(i) for i in range(300)]\nglove_vec_df = pd.DataFrame(np.array(glove_vec), columns=col_list, index=combined_df.index)\nprint(f\"glove_vec_df: {glove_vec_df.shape}\\n\")\nglove_vec_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:02:47.803242Z","iopub.execute_input":"2021-06-07T05:02:47.803689Z","iopub.status.idle":"2021-06-07T05:02:53.012085Z","shell.execute_reply.started":"2021-06-07T05:02:47.803643Z","shell.execute_reply":"2021-06-07T05:02:53.01105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combined_df = pd.merge(combined_df, glove_vec_df, how=\"inner\", on=\"id\", sort=False)\ncombined_df.drop(['excerpt','Processed_excerpt'], inplace=True, axis=1)\nprint(f\"combined_df: {combined_df.shape}\\n\")\ncombined_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:02:53.013313Z","iopub.execute_input":"2021-06-07T05:02:53.013588Z","iopub.status.idle":"2021-06-07T05:02:53.28629Z","shell.execute_reply.started":"2021-06-07T05:02:53.013562Z","shell.execute_reply":"2021-06-07T05:02:53.28528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Xtrain = combined_df[:Ytrain.shape[0]].copy()\nXtest = combined_df[Ytrain.shape[0]:].copy()\nprint(f\"Xtrain: {Xtrain.shape} \\nXtest: {Xtest.shape}\")\n\ndel combined_df\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:02:53.288019Z","iopub.execute_input":"2021-06-07T05:02:53.288702Z","iopub.status.idle":"2021-06-07T05:02:53.577172Z","shell.execute_reply.started":"2021-06-07T05:02:53.288653Z","shell.execute_reply":"2021-06-07T05:02:53.576029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Quantile Transformation","metadata":{}},{"cell_type":"code","source":"for col in tqdm(Xtrain.columns):\n    transformer = QuantileTransformer(n_quantiles=1000, \n                                      random_state=10, \n                                      output_distribution=\"normal\")\n    \n    vec_len = len(Xtrain[col].values)\n    vec_len_test = len(Xtest[col].values)\n\n    raw_vec = Xtrain[col].values.reshape(vec_len, 1)\n    test_vec = Xtest[col].values.reshape(vec_len_test, 1)\n    transformer.fit(raw_vec)\n    \n    Xtrain[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n    Xtest[col] = transformer.transform(test_vec).reshape(1, vec_len_test)[0]\n\nprint(\"Xtrain: {} \\nYtrain: {} \\nXtest: {}\".format(Xtrain.shape, Ytrain.shape, Xtest.shape))","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:02:53.578595Z","iopub.execute_input":"2021-06-07T05:02:53.578932Z","iopub.status.idle":"2021-06-07T05:03:03.383996Z","shell.execute_reply.started":"2021-06-07T05:02:53.57889Z","shell.execute_reply":"2021-06-07T05:03:03.38304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Base Models\n\n* **BayesianRidge** (Unused)\n* **Lasso**\n* **ElasticNet**\n* **HistGradientBoostingRegressor** (Unused)\n* **LightGBM**\n* **XGBoost**","metadata":{}},{"cell_type":"code","source":"FOLD = 5\nNUM_SEED = 3\nCOUNTER = 0\n\nnp.random.seed(0)\nseeds = np.random.randint(0, 100, size=NUM_SEED)\n\n#oof_score_ridge = 0\noof_score_lasso = 0\noof_score_enet = 0\n#oof_score_gbr = 0\noof_score_lgb = 0\noof_score_xgb = 0\n\n#y_pred_final_ridge = np.zeros((Xtest.shape[0], NUM_SEED))\ny_pred_final_lasso = np.zeros((Xtest.shape[0], NUM_SEED))\ny_pred_final_enet = np.zeros((Xtest.shape[0], NUM_SEED))\n#y_pred_final_gbr = np.zeros((Xtest.shape[0], NUM_SEED))\ny_pred_final_lgb = np.zeros((Xtest.shape[0], NUM_SEED))\ny_pred_final_xgb = np.zeros((Xtest.shape[0], NUM_SEED))\n\n#y_pred_meta_ridge = np.zeros((Ytrain.shape[0], NUM_SEED))\ny_pred_meta_lasso = np.zeros((Ytrain.shape[0], NUM_SEED))\ny_pred_meta_enet = np.zeros((Ytrain.shape[0], NUM_SEED))\n#y_pred_meta_gbr = np.zeros((Ytrain.shape[0], NUM_SEED))\ny_pred_meta_lgb = np.zeros((Ytrain.shape[0], NUM_SEED))\ny_pred_meta_xgb = np.zeros((Ytrain.shape[0], NUM_SEED))","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:03:12.873358Z","iopub.execute_input":"2021-06-07T05:03:12.873717Z","iopub.status.idle":"2021-06-07T05:03:12.88546Z","shell.execute_reply.started":"2021-06-07T05:03:12.873684Z","shell.execute_reply":"2021-06-07T05:03:12.884458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Model Name \\t\\t\\tSeed \\t\\tFold \\t\\tOOF Score \\t\\tAggregate OOF Score\")\n\nfor sidx, seed in enumerate(seeds):\n    #seed_score_ridge = 0\n    seed_score_lasso = 0\n    seed_score_enet = 0\n    #seed_score_gbr = 0\n    seed_score_lgb = 0\n    seed_score_xgb = 0\n    \n    kfold = StratifiedKFold(n_splits=FOLD, shuffle=True, random_state=seed)\n\n    for idx, (train, val) in enumerate(kfold.split(Xtrain, Ytrain_strat)):\n        COUNTER += 1\n\n        train_x, train_y = Xtrain.iloc[train], Ytrain[train]\n        val_x, val_y = Xtrain.iloc[val], Ytrain[val]\n        \n        \n        #====================================================================\n        #                           Bayesian Ridge\n        #====================================================================\n        '''\n        ridge_model = BayesianRidge()\n        ridge_model.fit(train_x, train_y)\n        \n        y_pred = ridge_model.predict(val_x)\n        y_pred_meta_ridge[val, sidx] = y_pred\n        y_pred_final_ridge[:, sidx] += ridge_model.predict(Xtest)\n        \n        score = np.sqrt(mean_squared_error(val_y, y_pred))\n        oof_score_ridge += score\n        seed_score_ridge += score\n        print(f\"\\nBayesian Ridge \\t\\t\\t{seed} \\t\\t{idx+1} \\t\\t{score}\")\n        '''\n        \n        #====================================================================\n        #                                 Lasso\n        #====================================================================\n        \n        lasso_model = Lasso(alpha=0.025, max_iter=2000, random_state=0)\n        lasso_model.fit(train_x, train_y)\n        \n        y_pred = lasso_model.predict(val_x)\n        y_pred_meta_lasso[val, sidx] = y_pred\n        y_pred_final_lasso[:, sidx] += lasso_model.predict(Xtest)\n        \n        score = np.sqrt(mean_squared_error(val_y, y_pred))\n        oof_score_lasso += score\n        seed_score_lasso += score\n        print(f\"Lasso         \\t\\t\\t{seed} \\t\\t{idx+1} \\t\\t{score}\")\n        \n        \n        #====================================================================\n        #                              ElasticNet\n        #====================================================================\n        \n        enet_model = ElasticNet(alpha=0.025, max_iter=2000, random_state=0)\n        enet_model.fit(train_x, train_y)\n        \n        y_pred = enet_model.predict(val_x)\n        y_pred_meta_enet[val, sidx] = y_pred\n        y_pred_final_enet[:, sidx] += enet_model.predict(Xtest)\n        \n        score = np.sqrt(mean_squared_error(val_y, y_pred))\n        oof_score_enet += score\n        seed_score_enet += score\n        print(f\"ElasticNet \\t\\t\\t{seed} \\t\\t{idx+1} \\t\\t{score}\")\n        \n        \n        #====================================================================\n        #                     HistGradientBoostingRegressor\n        #====================================================================\n        '''\n        gbr_model = HistGradientBoostingRegressor(max_depth=6, max_leaf_nodes=52, random_state=0)\n        gbr_model.fit(train_x, train_y)\n        \n        y_pred = gbr_model.predict(val_x)\n        y_pred_meta_gbr[val, sidx] = y_pred\n        y_pred_final_gbr[:, sidx] += gbr_model.predict(Xtest)\n        \n        score = np.sqrt(mean_squared_error(val_y, y_pred))\n        oof_score_gbr += score\n        seed_score_gbr += score\n        print(f\"GradientBoostingRegressor \\t{seed} \\t\\t{idx+1} \\t\\t{score}\")\n        '''\n        \n        #====================================================================\n        #                                LightGBM\n        #====================================================================\n        \n        params = {}\n        params[\"objective\"] = 'regression'\n        params[\"metric\"] = 'rmse'\n        params[\"boosting\"] = 'gbdt'\n        params[\"learning_rate\"] = 0.0204\n        params[\"lambda_l2\"] = 0.225\n        params[\"num_leaves\"] = 52\n        params[\"max_depth\"] = 6\n        params[\"feature_fraction\"] = 0.75\n        params[\"bagging_fraction\"] = 0.65\n        params[\"bagging_freq\"] = 10\n        params[\"min_data_in_leaf\"] = 15\n        params[\"verbosity\"] = -1\n        num_rounds = 5000\n        \n        lgtrain = lgb.Dataset(train_x, label=train_y.ravel())\n        lgvalidation = lgb.Dataset(val_x, label=val_y.ravel())\n\n        lgb_model = lgb.train(params, lgtrain, num_rounds, \n                              valid_sets=[lgtrain, lgvalidation], \n                              early_stopping_rounds=100, verbose_eval=False)\n\n        y_pred = lgb_model.predict(val_x, num_iteration=lgb_model.best_iteration)\n        y_pred_meta_lgb[val, sidx] = y_pred\n        y_pred_final_lgb[:, sidx] += lgb_model.predict(Xtest, num_iteration=lgb_model.best_iteration)\n        \n        score = np.sqrt(mean_squared_error(val_y, y_pred))\n        oof_score_lgb += score\n        seed_score_lgb += score\n        print(f\"LightGBM       \\t\\t\\t{seed} \\t\\t{idx+1} \\t\\t{score}\")\n        \n        \n        #====================================================================\n        #                                XGBoost\n        #====================================================================\n        \n        xgb_model = XGBRegressor(\n            objective='reg:squarederror',\n            eval_metric='rmse',\n            booster='gbtree',\n            sample_type='uniform',\n            tree_method='hist',\n            grow_policy='lossguide',\n            num_round=5000,\n            #max_depth=11, \n            max_leaves=55,\n            learning_rate=0.074,\n            subsample=0.74,\n            colsample_bytree=0.675,\n            min_child_weight=7,\n            reg_lambda=0.152,\n            verbosity=0\n        )\n\n        xgb_model.fit(train_x, train_y, eval_set=[(train_x, train_y), (val_x, val_y)], \n                      early_stopping_rounds=200, verbose=False)\n\n        y_pred = xgb_model.predict(val_x, iteration_range=(0, xgb_model.best_iteration))\n        y_pred_meta_xgb[val, sidx] += y_pred\n        y_pred_final_xgb[:, sidx] += xgb_model.predict(Xtest, iteration_range=(0, xgb_model.best_iteration))\n        \n        score = np.sqrt(mean_squared_error(val_y, y_pred))\n        oof_score_xgb += score\n        seed_score_xgb += score\n        print(f\"XGBoost     \\t\\t\\t{seed} \\t\\t{idx+1} \\t\\t{score}\\n\")\n        \n    print(\"=\"*110)\n    #print(f\"Bayesian Ridge \\t\\t\\t{seed} \\t\\t\\t\\t\\t\\t\\t{(seed_score_ridge / FOLD)}\")\n    print(f\"Lasso         \\t\\t\\t{seed} \\t\\t\\t\\t\\t\\t\\t{(seed_score_lasso / FOLD)}\")\n    print(f\"ElasticNet \\t\\t\\t{seed} \\t\\t\\t\\t\\t\\t\\t{(seed_score_enet / FOLD)}\")\n    #print(f\"GradientBoostingRegressor \\t{seed} \\t\\t\\t\\t\\t\\t\\t{(seed_score_gbr / FOLD)}\")\n    print(f\"LightGBM       \\t\\t\\t{seed} \\t\\t\\t\\t\\t\\t\\t{(seed_score_lgb / FOLD)}\")\n    print(f\"XGBoost     \\t\\t\\t{seed} \\t\\t\\t\\t\\t\\t\\t{(seed_score_xgb / FOLD)}\")\n    print(\"=\"*110)","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:03:14.03308Z","iopub.execute_input":"2021-06-07T05:03:14.033425Z","iopub.status.idle":"2021-06-07T05:04:06.474952Z","shell.execute_reply.started":"2021-06-07T05:03:14.033395Z","shell.execute_reply":"2021-06-07T05:04:06.473342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#y_pred_final_ridge = y_pred_final_ridge / float(FOLD)\ny_pred_final_lasso = y_pred_final_lasso / float(FOLD)\ny_pred_final_enet = y_pred_final_enet / float(FOLD)\n#y_pred_final_gbr = y_pred_final_gbr / float(FOLD)\ny_pred_final_lgb = y_pred_final_lgb / float(FOLD)\ny_pred_final_xgb = y_pred_final_xgb / float(FOLD)\n\n#oof_score_ridge /= float(COUNTER)\noof_score_lasso /= float(COUNTER)\noof_score_enet /= float(COUNTER)\n#oof_score_gbr /= float(COUNTER)\noof_score_lgb /= float(COUNTER)\noof_score_xgb /= float(COUNTER)\n\n#print(f\"Bayesian Ridge | Aggregate OOF Score: {oof_score_ridge}\")\nprint(f\"Lasso | Aggregate OOF Score: {oof_score_lasso}\")\nprint(f\"ElasticNet | Aggregate OOF Score: {oof_score_enet}\")\n#print(f\"GradientBoostingRegressor | Aggregate OOF Score: {oof_score_gbr}\")\nprint(f\"LightGBM | Aggregate OOF Score: {oof_score_lgb}\")\nprint(f\"XGBoost | Aggregate OOF Score: {oof_score_xgb}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-06T14:41:34.798513Z","iopub.execute_input":"2021-06-06T14:41:34.799355Z","iopub.status.idle":"2021-06-06T14:41:34.811932Z","shell.execute_reply.started":"2021-06-06T14:41:34.799291Z","shell.execute_reply":"2021-06-06T14:41:34.810338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create submission file","metadata":{}},{"cell_type":"code","source":"y_pred_final = (y_pred_final_lgb * 0.65) + (y_pred_final_enet * 0.15) + (y_pred_final_xgb * 0.1) + (y_pred_final_lasso * 0.1) \n\nsubmit_df = pd.read_csv(\"../input/commonlitreadabilityprize/sample_submission.csv\")\nsubmit_df['target'] = y_pred_final\nsubmit_df.to_csv(\"./submission.csv\", index=False)\nsubmit_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-06T14:42:56.532243Z","iopub.execute_input":"2021-06-06T14:42:56.53265Z","iopub.status.idle":"2021-06-06T14:42:56.563838Z","shell.execute_reply.started":"2021-06-06T14:42:56.532615Z","shell.execute_reply":"2021-06-06T14:42:56.562681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}