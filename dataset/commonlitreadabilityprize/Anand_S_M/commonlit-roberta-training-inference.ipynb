{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## **CommonLit Readability Prize**\n\nIn this competition, youâ€™ll build algorithms to rate the complexity of reading passages for grade 3-12 classroom use. \n\nThe dataset that includes readers from a wide variety of age groups and a large collection of texts taken from various domains. Winning models will be sure to incorporate text cohesion and semantics.\n\nInstead using the regular BERT implementation lets try out DistilBERT\n\nDistilBERT is a transformers model, smaller and faster than BERT, which was pretrained on the same corpus in a self-supervised fashion, using the BERT base model as a teacher. \n\n![BERT](https://blog.rasa.com/content/images/2019/09/pruning_bert.png)","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nimport seaborn as sns \nfrom sklearn import metrics\nimport torch\nimport torch.nn as nn \nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport transformers\nfrom transformers import BertForSequenceClassification\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\n\nimport warnings\nwarnings.simplefilter('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#config\nTRAIN_PATH = '../input/commonlitreadabilityprize/train.csv'\nTEST_PATH = '../input/commonlitreadabilityprize/test.csv'\nEPOCHS = 10\nLR = 3e-4\nMAX_LEN = 200\nBERT_MODEL = '../input/distilbertbaseuncased'\nTOKENIZER = transformers.BertTokenizer.from_pretrained(\n    BERT_MODEL,\n    do_lower_case=True\n)\nTRAIN_BS = 16\nVALID_BS = 32\nCOLUMNS = ['excerpt', 'target']\nDEVICE = device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nDEVICE","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets have a quick glance at the data\ntrain = pd.read_csv(TRAIN_PATH)\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#we need to check if the data is clean and complete\ntrain.info()\n#we will see the all required columns contains no NaN values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#to check how many unique values are there in each columns\n#this also gives us an idea of what sort of a problem we are going to face\nfor col in train.columns:\n    print(f\"{col}: {len(train[col].unique())}\")\n    \n#from this we can see that the target is not a categorical field cause it has \n#same number of unique values as the number of rows\n#So its kind of a regression problem ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CommonLitDataset(Dataset):\n    def __init__(self, excerpt, target=None, test=False):\n        self.excerpt = excerpt\n        self.target = target\n        self.test = test \n        self.tokenizer = TOKENIZER\n        self.max_len = MAX_LEN\n        \n    def __len__(self):\n        return len(self.excerpt)\n    \n    def __getitem__(self, index):\n        excerpt = str(self.excerpt[index])\n        excerpt = ' '.join(excerpt.split())\n        \n        inputs = self.tokenizer.encode_plus(\n            excerpt,\n            None,\n            truncation=True,\n            add_special_tokens=True,\n            max_length = self.max_len,\n            pad_to_max_length = True\n        )\n        \n        ids = torch.tensor(inputs['input_ids'], dtype=torch.long)\n        mask = torch.tensor(inputs['attention_mask'], dtype=torch.long)\n        token_type_ids = torch.tensor(inputs['token_type_ids'], dtype=torch.long)\n        \n        if self.test:\n            return {\n                'ids': ids,\n                'mask': mask,\n                'token_type_ids': token_type_ids\n            }\n        else:\n            targets = torch.tensor(self.target[index], dtype=torch.float)\n            return {\n                'ids': ids,\n                'mask': mask,\n                'token_type_ids': token_type_ids,\n                'targets': targets\n            }      ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train and Evaluation Engine","metadata":{}},{"cell_type":"code","source":"class Engine:\n    def __init__(self,model,optimizer,train_dataloader,\n                 valid_dataloader,device):\n        self.model = model\n        self.optimizer = optimizer\n        self.train_data = train_dataloader\n        self.valid_data = valid_dataloader\n        self.device = device\n        \n    def loss_fn(self,outputs, targets):\n        return torch.sqrt(nn.MSELoss()(outputs, targets))\n    \n    def train_fn(self):\n        self.model.train()\n        i = 0\n        size = len(self.train_data)\n        for data in self.train_data:\n            ids = data['ids'].to(self.device,dtype=torch.long)\n            token_type_ids = data['token_type_ids'].to(self.device,dtype=torch.long)\n            mask = data['mask'].to(self.device,dtype=torch.long)\n            targets = data['targets'].to(self.device,dtype=torch.float)\n            \n            self.optimizer.zero_grad()\n            outputs = model(\n                    ids=ids,\n                    mask=mask,\n                    token_type_ids = token_type_ids\n            )\n            loss = self.loss_fn(outputs, targets)\n            loss.backward()\n            optimizer.step()\n            \n            return loss\n    \n    def eval_fn(self):\n        self.model.eval()\n        _targets = []\n        _outputs = []\n        with torch.no_grad():\n            for data in self.valid_data:\n                ids = data['ids'].to(self.device,dtype=torch.long)\n                token_type_ids = data['token_type_ids'].to(self.device,dtype=torch.long)\n                mask = data['mask'].to(self.device,dtype=torch.long)\n                targets = data['targets'].to(self.device,dtype=torch.float)\n            \n                self.optimizer.zero_grad()\n                outputs = model(\n                        ids=ids,\n                        mask=mask,\n                        token_type_ids = token_type_ids\n                        )\n                val_loss = self.loss_fn(outputs, targets)\n                targets = targets.cpu().detach()\n                _targets.extend(targets.numpy().tolist())\n                outputs = outputs.cpu().detach()\n                _outputs.extend(outputs.numpy().tolist())\n                \n            return val_loss, _outputs, _targets\n        \n    def inference_fn(self, test_dl, infer_model):\n        outputs = []\n        infer_model.eval()\n        with torch.no_grad():\n            for i,data in enumerate(test_dl):\n                ids = data['ids'].to(self.device,dtype=torch.long)\n                token_type_ids = data['token_type_ids'].to(self.device,dtype=torch.long)\n                mask = data['mask'].to(self.device,dtype=torch.long)\n            \n                self.optimizer.zero_grad()\n                out = infer_model(\n                        ids=ids,\n                        mask=mask,\n                        token_type_ids = token_type_ids\n                        )\n                out = out.cpu().detach().numpy()\n                if i==0:\n                    outputs = out\n                else:\n                    outputs = np.concatenate((outputs,out), axis=None)\n                \n        return outputs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### DistilBERT Model","metadata":{}},{"cell_type":"code","source":"#model\nclass DistilBERT(nn.Module):\n    def __init__(self):\n        super(DistilBERT, self).__init__()\n        self.bert = transformers.BertModel.from_pretrained(BERT_MODEL)\n        self.dropout1 = nn.Dropout(0.3)\n        self.out = nn.Linear(768, 1)\n        \n    def forward(self, ids, mask, token_type_ids):\n        _, output = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids, return_dict=False)\n        output = self.dropout1(output)\n        output = self.out(output)\n        return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Training","metadata":{}},{"cell_type":"code","source":"#Model training\ndata = pd.read_csv(TRAIN_PATH)\ndata = data.sample(frac=1).reset_index(drop=True)\ndata = data[COLUMNS]\n\ntrain_df = data[:2500].sample(frac=1).reset_index(drop=True)\nvalid_df = data[2500:].sample(frac=1).reset_index(drop=True)\nprint(f\"Train : {train_df.shape}\\nValidation: {valid_df.shape}\")\n\ntrain_ds = CommonLitDataset(\n    excerpt=train_df['excerpt'].values,\n    target = train_df['target'].values\n)\nvalid_ds = CommonLitDataset(\n    excerpt=valid_df['excerpt'].values,\n    target = valid_df['target'].values\n)\n\ntrain_dl = DataLoader(\n    train_ds,\n    batch_size = TRAIN_BS,\n    shuffle = True,\n    num_workers = 4\n)\nvalid_dl = DataLoader(\n    valid_ds,\n    batch_size= VALID_BS,\n    shuffle= True,\n    num_workers= 4 \n)\nmodel = DistilBERT().to(DEVICE);\noptimizer = transformers.AdamW(model.parameters(),lr=LR)\nengine = Engine(model=model,optimizer=optimizer,\n                train_dataloader=train_dl,\n                valid_dataloader=valid_dl,\n                device=DEVICE\n               )\nbest_loss = 10\nfor epoch in range(EPOCHS):\n    train_loss = engine.train_fn()\n    val_loss, outputs, targets = engine.eval_fn()\n    print(f\"epoch: {epoch}, train loss: {train_loss}, val_loss: {val_loss}\")\n    if val_loss < best_loss:\n        print(f\"saving model with loss: {val_loss}\")\n        torch.save(model.state_dict(),f\"CommonLit_{val_loss}.bin\")\n        best_loss = val_loss\n        \nprint(f\"final Report\\nValidation RMSE Loss: {best_loss}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Inference and Submission","metadata":{}},{"cell_type":"code","source":"infer_model = DistilBERT()\ninfer_model.load_state_dict(torch.load(f\"CommonLit_{best_loss}.bin\"))\ninfer_model.to(device)\ninfer_model.eval()\n\ntest_df = pd.read_csv(TEST_PATH)\ntest_dataset = CommonLitDataset(\n    excerpt=test_df['excerpt'].values,\n    test=True\n)\ntest_dl = DataLoader(test_dataset,batch_size=16,shuffle=False,num_workers=4)\noutput = engine.inference_fn(test_dl,infer_model)\nsubmission_df = pd.DataFrame({'id': test_df.id, 'target': output.reshape(-1).tolist()})\nsubmission_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.to_csv('/kaggle/working/submission.csv', index=False)\nsubmission_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **If you find it useful please upvote** :) ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}