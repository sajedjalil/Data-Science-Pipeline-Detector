{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# [ReadNet is SOTA for WeeBit](https://paperswithcode.com/sota/text-classification-on-weebit-readability) (Readability Assessment dataset).\n# This code is just a ReadNet implementation for CommonLit competition.","metadata":{"_uuid":"ee41b934-7d58-4f84-af15-c0674f1f4ae3","_cell_guid":"7d9a8fce-66bf-4b78-ad3a-5341a427326b","trusted":true}},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom torch import Tensor, nn, tensor\nimport math\nimport pandas as pd\nimport csv\nimport nltk\nnltk.download('punkt')\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom pathlib import Path\nfrom fastai.vision.all import *\nfrom fastai.text.all import *\n\n\n# Make sure to have your glove embeddings stored here\nroot_dir = '.'","metadata":{"_uuid":"a896e240-d0d3-486d-a86d-d5bc1acf0f6a","_cell_guid":"a4e047a4-b796-4b0a-baa4-c2a6198f0025","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-27T11:32:32.556943Z","iopub.execute_input":"2021-07-27T11:32:32.557347Z","iopub.status.idle":"2021-07-27T11:32:57.170994Z","shell.execute_reply.started":"2021-07-27T11:32:32.55726Z","shell.execute_reply":"2021-07-27T11:32:57.170173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## MODEL CODE ##\n\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads, masked):\n        super().__init__()\n        assert d_model % num_heads == 0, \"num_heads must evenly chunk d_model\"\n        self.num_heads = num_heads\n        self.wq = nn.Linear(d_model, d_model, bias=False)  # QQ what if bias=True?\n        self.wk = nn.Linear(d_model, d_model, bias=False)\n        self.wv = nn.Linear(d_model, d_model, bias=False)\n        self.masked = masked\n        self.softmax = nn.Softmax(dim=2)\n\n    def forward(self, q, k, v):\n        qs = self.wq(q).chunk(self.num_heads, dim=2)\n        ks = self.wk(k).chunk(self.num_heads, dim=2)\n        vs = self.wv(v).chunk(self.num_heads, dim=2)\n        outs = []\n        # TODO Use einsum instead of for loop\n        for qi, ki, vi in zip(qs, ks, vs):\n            attns = qi.bmm(ki.transpose(1, 2)) / (ki.shape[2] ** 0.5)\n            if self.masked:\n                attns = attns.tril()  # Zero out upper triangle so it can't look ahead\n            attns = self.softmax(attns)\n            outs.append(attns.bmm(vi))\n        return torch.cat(outs, dim=2)","metadata":{"_uuid":"8017c2bc-1607-425a-acb3-f3432c14e9b9","_cell_guid":"ed7ddb26-8c53-44f8-aa54-997c667afcb0","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-27T11:32:57.172358Z","iopub.execute_input":"2021-07-27T11:32:57.172668Z","iopub.status.idle":"2021-07-27T11:32:57.181607Z","shell.execute_reply.started":"2021-07-27T11:32:57.172633Z","shell.execute_reply":"2021-07-27T11:32:57.18066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AddNorm(nn.Module):\n    def __init__(self, d_model):\n        super().__init__()\n        self.ln = nn.LayerNorm(d_model)\n\n    def forward(self, x1, x2):\n        return self.ln(x1+x2)\n\n\nclass FeedForward(nn.Module):\n    def __init__(self, d_model):\n        super().__init__()\n        self.l1 = nn.Linear(d_model, d_model)\n        self.relu = nn.ReLU()\n        self.l2 = nn.Linear(d_model, d_model)\n    def forward(self, x):\n        return self.l2(self.relu(self.l1(x)))\n\n\ndef pos_encode(x):\n    pos, dim = torch.meshgrid(torch.arange(x.shape[1]), torch.arange(x.shape[2]))\n    dim = 2 * (dim // 2)\n    enc_base = pos/(10_000**(dim / x.shape[2]))\n    addition = torch.zeros_like(x)\n    for d in range(x.shape[2]):\n        enc_func = torch.sin if d % 2 == 0 else torch.cos\n        addition[:,:,d] = enc_func(enc_base[:,d])\n    if x.is_cuda:\n        addition = addition.cuda()\n    return x + addition","metadata":{"_uuid":"91b6370d-bd9a-473b-8de4-0ad32d6b6040","_cell_guid":"58062cc6-93d5-4444-8244-5f8c68a8bab7","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-27T11:32:57.183732Z","iopub.execute_input":"2021-07-27T11:32:57.184089Z","iopub.status.idle":"2021-07-27T11:32:57.195854Z","shell.execute_reply.started":"2021-07-27T11:32:57.184052Z","shell.execute_reply":"2021-07-27T11:32:57.195019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EncoderBlock(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        self.mha = MultiHeadAttention(d_model=d_model, num_heads=num_heads, masked=False)\n        self.an1 = AddNorm(d_model)\n        self.ff = FeedForward(d_model)\n        self.an2 = AddNorm(d_model)\n\n    def forward(self, x):\n        x = self.an1(x, self.mha(q=x, k=x, v=x))\n        return self.an2(x, self.ff(x))\n\n\nclass AttentionAggregation(nn.Module):\n    def __init__(self, d_model):\n        super().__init__()\n        self.query = nn.Linear(d_model, 1, bias=False)\n\n    def forward(self, x):  # (b, s, m)\n        attns = self.query(x).softmax(dim=1)  # (b, s, 1)\n        enc = torch.bmm(attns.transpose(1, 2), x)  # (b, 1, m)\n        return enc.squeeze(1)\n\n\nclass LinTanh(nn.Module):\n    def __init__(self, d_model):\n        super().__init__()\n        self.lin = nn.Linear(d_model, d_model)\n        self.tanh = nn.Tanh()\n\n    def forward(self, x):\n        return self.tanh(self.lin(x))","metadata":{"_uuid":"e937062a-346f-437f-83ad-dbad5a30e815","_cell_guid":"c883bff9-98a5-4da9-a57d-d98994c73ddd","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-27T11:32:57.197419Z","iopub.execute_input":"2021-07-27T11:32:57.197822Z","iopub.status.idle":"2021-07-27T11:32:57.208855Z","shell.execute_reply.started":"2021-07-27T11:32:57.197784Z","shell.execute_reply":"2021-07-27T11:32:57.20769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LinFeatConcat(nn.Module):\n    def __init__(self, d_model, n_feats, n_out):\n        super().__init__()\n        self.lin = nn.Linear(d_model + n_feats, n_out, bias=False)  # TODO what if True?\n\n    def forward(self, x, feats):\n        return self.lin(torch.cat([x, feats], dim=1))\n\n\nclass ReadNetBlock(nn.Module):\n    def __init__(self, d_model, n_heads, n_blocks, n_feats, n_out):\n        super().__init__()\n        self.blocks = nn.Sequential(*[EncoderBlock(d_model=d_model, num_heads=n_heads) for _ in range(n_blocks)])\n        self.lin_tanh = LinTanh(d_model=d_model)\n        self.attn_agg = AttentionAggregation(d_model=d_model)\n        self.lin_feat_concat = LinFeatConcat(d_model=d_model, n_feats=n_feats, n_out=n_out)\n\n    def forward(self, x, feats):  # (b, s, m), (b, f)\n        x = pos_encode(x)\n        x = self.blocks(x)\n        x = self.lin_tanh(x)\n        x = self.attn_agg(x)\n        return self.lin_feat_concat(x, feats)","metadata":{"_uuid":"8cab79a4-98c1-4d12-89a0-73c48312cac2","_cell_guid":"be66e401-ddfc-4fe2-9af2-fb27e02d4577","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-27T11:32:57.21013Z","iopub.execute_input":"2021-07-27T11:32:57.210476Z","iopub.status.idle":"2021-07-27T11:32:57.220575Z","shell.execute_reply.started":"2021-07-27T11:32:57.210438Z","shell.execute_reply":"2021-07-27T11:32:57.219469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class GloveEmbedding(nn.Module):\n    def __init__(self, num):\n        super().__init__()\n        # Make embedding\n        self.embed = nn.Embedding(400_000 + 1, num)\n        # found GloveEmbedding on kaggle and set here.\n        emb_w = pd.read_csv(\n            '../input/glove-embeddings/glove.6B.200d.txt', header=None, sep=\" \", quoting=csv.QUOTE_NONE\n        ).values[:, 1:].astype('float64')\n        emb_w = Tensor(emb_w)\n        emb_w = torch.cat([emb_w, torch.zeros(1, num)], dim=0)\n        self.embed.weight = nn.Parameter(emb_w)\n\n    def forward(self, x):\n        return self.embed(x.to(torch.long))","metadata":{"_uuid":"c82522ce-5ed8-4772-b4f1-5a9348d8c1fe","_cell_guid":"fa291a4e-19b9-466d-801c-f2a4d8f6d025","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-27T11:32:57.221779Z","iopub.execute_input":"2021-07-27T11:32:57.222269Z","iopub.status.idle":"2021-07-27T11:32:57.231609Z","shell.execute_reply.started":"2021-07-27T11:32:57.222231Z","shell.execute_reply":"2021-07-27T11:32:57.230797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ReadNet(nn.Module):\n    def __init__(self, embed, d_model, n_heads, n_blocks, n_feats_sent, n_feats_doc):\n        super().__init__()\n        self.embed = embed\n        self.sent_block = ReadNetBlock(\n            d_model=d_model, n_heads=n_heads, n_blocks=n_blocks, n_feats=n_feats_sent, n_out=d_model\n        )\n        self.doc_block = ReadNetBlock(\n            d_model=d_model, n_heads=n_heads, n_blocks=n_blocks, n_feats=n_feats_doc, n_out=d_model + n_feats_doc\n        )\n        self.head = nn.Sequential(\n            nn.Linear(d_model + n_feats_doc, 1),\n        )\n\n    def forward(self, x, feats_sent=None, feats_doc=None):  # (b, d, s) tokens, (b, d, n_f_s), (b, n_f_d)\n        if feats_sent is None: feats_sent = Tensor([])\n        if feats_doc is None: feats_doc = Tensor([])\n        if x.is_cuda:\n            feats_sent = feats_sent.cuda()\n            feats_doc = feats_doc.cuda()\n        x = self.embed(x)\n        b, d, s, m = x.shape\n        x = x.reshape(b * d, s, m)\n        sents_enc = self.sent_block(x, feats_sent.reshape(b * d, -1))  # (b*d, m)\n        docs = sents_enc.reshape(b, d, m)\n        docs_enc = self.doc_block(docs, feats_doc)\n        out = self.head(docs_enc)\n        return out.squeeze(1)","metadata":{"_uuid":"487d86c5-9bff-4433-8344-75ffd3b94a7e","_cell_guid":"3a92e6c6-e888-4243-8a66-5139de77b92c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-27T11:32:57.232788Z","iopub.execute_input":"2021-07-27T11:32:57.233408Z","iopub.status.idle":"2021-07-27T11:32:57.244313Z","shell.execute_reply.started":"2021-07-27T11:32:57.233371Z","shell.execute_reply":"2021-07-27T11:32:57.243155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## DATA PREPARATION ##\n\nclass GloveTokenizer:\n    def __init__(self, num):\n        # found GloveEmbedding on kaggle and set here.\n        words = pd.read_csv(\n            '../input/glove-embeddings/glove.6B.200d.txt', header=None, sep=\" \", quoting=csv.QUOTE_NONE, usecols=[0]\n        ).values\n        words = [word[0] for word in words]\n        self.word2idx = {w: i for i, w in enumerate(words)}\n\n    def __call__(self, sent):\n        toks = [self.word2idx.get(w.lower()) for w in word_tokenize(sent)]\n        return [self.unk_token if t is None else t for t in toks]\n\n    @property\n    def unk_token(self):\n        return 400_000  # We appended this to the end of the embedding to return all zeros\n\n    @property\n    def pad_token(self):\n        return self.unk_token  # Seems that this is the best option for GLOVE\n\n\ndef prepare_txts(txts, tokenizer):\n    # Input: (bs,) str, Output: (bs, max_doc_len, max_sent_len)\n    # We choose to elongate all docs and sentences to the max rather than truncate some of them\n    # TODO: Do this better later:\n    # (1) Truncate smartly (if there is one very long outlier sentence or doc)\n    # (2) Group together docs of similar lengths (in terms of num_sents)\n    docs = [[tokenizer(sent) for sent in sent_tokenize(txt)] for txt in txts]\n    # pkl_save(root_dir/\"doc_lens\", pd.Series([len(doc) for doc in docs]))\n    max_doc_len = max([len(doc) for doc in docs])\n    docs = [doc + [[]] * (max_doc_len - len(doc)) for doc in docs]\n    # pkl_save(root_dir/\"sent_lens\", pd.Series([len(sent) for doc in docs for sent in doc]))\n    max_sent_len = max([len(sent) for doc in docs for sent in doc])\n    docs = [[s + [tokenizer.pad_token] * (max_sent_len - len(s)) for s in doc] for doc in docs]\n    return Tensor(docs)\n\n\ndef prepare_txts_cut(txts, tokenizer, max_doc_len=18, max_sent_len=49):\n    docs = [[tokenizer(sent)[:max_sent_len] for sent in sent_tokenize(txt)[:max_doc_len]] for txt in txts]\n    docs = [doc + [[]] * (max_doc_len - len(doc)) for doc in docs]\n    docs = [[s + [tokenizer.pad_token] * (max_sent_len - len(s)) for s in doc] for doc in docs]\n    return Tensor(docs)","metadata":{"_uuid":"baa4a83a-26d6-4fa4-9f40-6d023b46b094","_cell_guid":"d31b302c-7194-48bf-b927-ff9a712dc6a3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-27T11:32:57.246538Z","iopub.execute_input":"2021-07-27T11:32:57.246962Z","iopub.status.idle":"2021-07-27T11:32:57.260482Z","shell.execute_reply.started":"2021-07-27T11:32:57.246927Z","shell.execute_reply":"2021-07-27T11:32:57.259597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocess\nPreprocessing improved training results.\nbelow code is from [How To: Preprocessing for GloVe Part2: Usage](https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part2-usage)","metadata":{}},{"cell_type":"code","source":"symbols_to_isolate = '.,?!-;*\"â€¦:â€”()%#$&_/@ï¼¼ãƒ»Ï‰+=â€â€œ[]^â€“>\\\\Â°<~â€¢â‰ â„¢ËˆÊŠÉ’âˆžÂ§{}Â·Ï„Î±â¤â˜ºÉ¡|Â¢â†’Ì¶`â¥â”â”£â”«â”—ï¼¯â–ºâ˜…Â©â€•Éªâœ”Â®\\x96\\x92â—Â£â™¥âž¤Â´Â¹â˜•â‰ˆÃ·â™¡â—â•‘â–¬â€²É”Ëâ‚¬Û©Ûžâ€ Î¼âœ’âž¥â•â˜†ËŒâ—„Â½Ê»Ï€Î´Î·Î»ÏƒÎµÏÎ½Êƒâœ¬ï¼³ï¼µï¼°ï¼¥ï¼²ï¼©ï¼´â˜»Â±â™ÂµÂºÂ¾âœ“â—¾ØŸï¼Žâ¬…â„…Â»Ð’Ð°Ð²â£â‹…Â¿Â¬â™«ï¼£ï¼­Î²â–ˆâ–“â–’â–‘â‡’â­â€ºÂ¡â‚‚â‚ƒâ§â–°â–”â—žâ–€â–‚â–ƒâ–„â–…â–†â–‡â†™Î³Ì„â€³â˜¹âž¡Â«Ï†â…“â€žâœ‹ï¼šÂ¥Ì²Ì…Ìâˆ™â€›â—‡âœâ–·â“â—Â¶ËšË™ï¼‰ÑÐ¸Ê¿âœ¨ã€‚É‘\\x80â—•ï¼ï¼…Â¯âˆ’ï¬‚ï¬â‚Â²ÊŒÂ¼â´â„â‚„âŒ â™­âœ˜â•ªâ–¶â˜­âœ­â™ªâ˜”â˜ â™‚â˜ƒâ˜ŽâœˆâœŒâœ°â†â˜™â—‹â€£âš“å¹´âˆŽâ„’â–ªâ–™â˜â…›ï½ƒï½ï½“Ç€â„®Â¸ï½—â€šâˆ¼â€–â„³â„â†â˜¼â‹†Ê’âŠ‚ã€â…”Â¨Í¡à¹âš¾âš½Î¦Ã—Î¸ï¿¦ï¼Ÿï¼ˆâ„ƒâ©â˜®âš æœˆâœŠâŒâ­•â–¸â– â‡Œâ˜â˜‘âš¡â˜„Ç«â•­âˆ©â•®ï¼Œä¾‹ï¼žÊ•ÉÌ£Î”â‚€âœžâ”ˆâ•±â•²â–â–•â”ƒâ•°â–Šâ–‹â•¯â”³â”Šâ‰¥â˜’â†‘â˜É¹âœ…â˜›â™©â˜žï¼¡ï¼ªï¼¢â—”â—¡â†“â™€â¬†Ì±â„\\x91â €Ë¤â•šâ†ºâ‡¤âˆâœ¾â—¦â™¬Â³ã®ï½œï¼âˆµâˆ´âˆšÎ©Â¤â˜œâ–²â†³â–«â€¿â¬‡âœ§ï½ï½–ï½ï¼ï¼’ï¼ï¼˜ï¼‡â€°â‰¤âˆ•Ë†âšœâ˜'\nsymbols_to_delete = '\\nðŸ•\\rðŸµðŸ˜‘\\xa0\\ue014\\t\\uf818\\uf04a\\xadðŸ˜¢ðŸ¶ï¸\\uf0e0ðŸ˜œðŸ˜ŽðŸ‘Š\\u200b\\u200eðŸ˜Ø¹Ø¯ÙˆÙŠÙ‡ØµÙ‚Ø£Ù†Ø§Ø®Ù„Ù‰Ø¨Ù…ØºØ±ðŸ˜ðŸ’–ðŸ’µÐ•ðŸ‘ŽðŸ˜€ðŸ˜‚\\u202a\\u202cðŸ”¥ðŸ˜„ðŸ»ðŸ’¥á´ÊÊ€á´‡É´á´…á´á´€á´‹Êœá´œÊŸá´›á´„á´˜Ê™Ò“á´Šá´¡É¢ðŸ˜‹ðŸ‘×©×œ×•××‘×™ðŸ˜±â€¼\\x81ã‚¨ãƒ³ã‚¸æ•…éšœ\\u2009ðŸšŒá´µÍžðŸŒŸðŸ˜ŠðŸ˜³ðŸ˜§ðŸ™€ðŸ˜ðŸ˜•\\u200fðŸ‘ðŸ˜®ðŸ˜ƒðŸ˜˜××¢×›×—ðŸ’©ðŸ’¯â›½ðŸš„ðŸ¼à®œðŸ˜–á´ ðŸš²â€ðŸ˜ŸðŸ˜ˆðŸ’ªðŸ™ðŸŽ¯ðŸŒ¹ðŸ˜‡ðŸ’”ðŸ˜¡\\x7fðŸ‘Œá¼á½¶Î®Î¹á½²Îºá¼€Î¯á¿ƒá¼´Î¾ðŸ™„ï¼¨ðŸ˜ \\ufeff\\u2028ðŸ˜‰ðŸ˜¤â›ºðŸ™‚\\u3000ØªØ­ÙƒØ³Ø©ðŸ‘®ðŸ’™ÙØ²Ø·ðŸ˜ðŸ¾ðŸŽ‰ðŸ˜ž\\u2008ðŸ¾ðŸ˜…ðŸ˜­ðŸ‘»ðŸ˜¥ðŸ˜”ðŸ˜“ðŸ½ðŸŽ†ðŸ»ðŸ½ðŸŽ¶ðŸŒºðŸ¤”ðŸ˜ª\\x08â€‘ðŸ°ðŸ‡ðŸ±ðŸ™†ðŸ˜¨ðŸ™ƒðŸ’•ð˜Šð˜¦ð˜³ð˜¢ð˜µð˜°ð˜¤ð˜ºð˜´ð˜ªð˜§ð˜®ð˜£ðŸ’—ðŸ’šåœ°ç„è°·ÑƒÐ»ÐºÐ½ÐŸÐ¾ÐÐðŸ¾ðŸ•ðŸ˜†×”ðŸ”—ðŸš½æ­Œèˆžä¼ŽðŸ™ˆðŸ˜´ðŸ¿ðŸ¤—ðŸ‡ºðŸ‡¸Ð¼Ï…Ñ‚Ñ•â¤µðŸ†ðŸŽƒðŸ˜©\\u200aðŸŒ ðŸŸðŸ’«ðŸ’°ðŸ’ŽÑÐ¿Ñ€Ð´\\x95ðŸ–ðŸ™…â›²ðŸ°ðŸ¤ðŸ‘†ðŸ™Œ\\u2002ðŸ’›ðŸ™ðŸ‘€ðŸ™ŠðŸ™‰\\u2004Ë¢áµ’Ê³Ê¸á´¼á´·á´ºÊ·áµ—Ê°áµ‰áµ˜\\x13ðŸš¬ðŸ¤“\\ue602ðŸ˜µÎ¬Î¿ÏŒÏ‚Î­á½¸×ª×ž×“×£× ×¨×š×¦×˜ðŸ˜’ÍðŸ†•ðŸ‘…ðŸ‘¥ðŸ‘„ðŸ”„ðŸ”¤ðŸ‘‰ðŸ‘¤ðŸ‘¶ðŸ‘²ðŸ”›ðŸŽ“\\uf0b7\\uf04c\\x9f\\x10æˆéƒ½ðŸ˜£âºðŸ˜ŒðŸ¤‘ðŸŒðŸ˜¯ÐµÑ…ðŸ˜²á¼¸á¾¶á½ðŸ’žðŸš“ðŸ””ðŸ“šðŸ€ðŸ‘\\u202dðŸ’¤ðŸ‡\\ue613å°åœŸè±†ðŸ¡â”â‰\\u202fðŸ‘ ã€‹à¤•à¤°à¥à¤®à¤¾ðŸ‡¹ðŸ‡¼ðŸŒ¸è”¡è‹±æ–‡ðŸŒžðŸŽ²ãƒ¬ã‚¯ã‚µã‚¹ðŸ˜›å¤–å›½äººå…³ç³»Ð¡Ð±ðŸ’‹ðŸ’€ðŸŽ„ðŸ’œðŸ¤¢ÙÙŽÑŒÑ‹Ð³Ñä¸æ˜¯\\x9c\\x9dðŸ—‘\\u2005ðŸ’ƒðŸ“£ðŸ‘¿à¼¼ã¤à¼½ðŸ˜°á¸·Ð—Ð·â–±Ñ†ï¿¼ðŸ¤£å–æ¸©å“¥åŽè®®ä¼šä¸‹é™ä½ å¤±åŽ»æ‰€æœ‰çš„é’±åŠ æ‹¿å¤§åç¨Žéª—å­ðŸãƒ„ðŸŽ…\\x85ðŸºØ¢Ø¥Ø´Ø¡ðŸŽµðŸŒŽÍŸá¼”æ²¹åˆ«å…‹ðŸ¤¡ðŸ¤¥ðŸ˜¬ðŸ¤§Ð¹\\u2003ðŸš€ðŸ¤´Ê²ÑˆÑ‡Ð˜ÐžÐ Ð¤Ð”Ð¯ÐœÑŽÐ¶ðŸ˜ðŸ–‘á½á½»Ïç‰¹æ®Šä½œæˆ¦ç¾¤Ñ‰ðŸ’¨åœ†æ˜Žå›­×§â„ðŸˆðŸ˜ºðŸŒâá»‡ðŸ”ðŸ®ðŸðŸ†ðŸ‘ðŸŒ®ðŸŒ¯ðŸ¤¦\\u200dð“’ð“²ð“¿ð“µì•ˆì˜í•˜ì„¸ìš”Ð–Ñ™ÐšÑ›ðŸ€ðŸ˜«ðŸ¤¤á¿¦æˆ‘å‡ºç”Ÿåœ¨äº†å¯ä»¥è¯´æ™®é€šè¯æ±‰è¯­å¥½æžðŸŽ¼ðŸ•ºðŸ¸ðŸ¥‚ðŸ—½ðŸŽ‡ðŸŽŠðŸ†˜ðŸ¤ ðŸ‘©ðŸ–’ðŸšªå¤©ä¸€å®¶âš²\\u2006âš­âš†â¬­â¬¯â–æ–°âœ€â•ŒðŸ‡«ðŸ‡·ðŸ‡©ðŸ‡ªðŸ‡®ðŸ‡¬ðŸ‡§ðŸ˜·ðŸ‡¨ðŸ‡¦Ð¥Ð¨ðŸŒ\\x1fæ€é¸¡ç»™çŒ´çœ‹Êð—ªð—µð—²ð—»ð˜†ð—¼ð˜‚ð—¿ð—®ð—¹ð—¶ð˜‡ð—¯ð˜ð—°ð˜€ð˜…ð—½ð˜„ð—±ðŸ“ºÏ–\\u2000Ò¯Õ½á´¦áŽ¥Ò»Íº\\u2007Õ°\\u2001É©ï½™ï½…àµ¦ï½ŒÆ½ï½ˆð“ð¡ðžð«ð®ððšðƒðœð©ð­ð¢ð¨ð§Æ„á´¨×Ÿá‘¯à»Î¤á§à¯¦Ð†á´‘Üð¬ð°ð²ð›ð¦ð¯ð‘ð™ð£ð‡ð‚ð˜ðŸŽÔœÐ¢á—žà±¦ã€”áŽ«ð³ð”ð±ðŸ”ðŸ“ð…ðŸ‹ï¬ƒðŸ’˜ðŸ’“Ñ‘ð˜¥ð˜¯ð˜¶ðŸ’ðŸŒ‹ðŸŒ„ðŸŒ…ð™¬ð™–ð™¨ð™¤ð™£ð™¡ð™®ð™˜ð™ ð™šð™™ð™œð™§ð™¥ð™©ð™ªð™—ð™žð™ð™›ðŸ‘ºðŸ·â„‹ð€ð¥ðªðŸš¶ð™¢á¼¹ðŸ¤˜Í¦ðŸ’¸Ø¬íŒ¨í‹°ï¼·ð™‡áµ»ðŸ‘‚ðŸ‘ƒÉœðŸŽ«\\uf0a7Ð‘Ð£Ñ–ðŸš¢ðŸš‚àª—à«àªœàª°àª¾àª¤à«€á¿†ðŸƒð“¬ð“»ð“´ð“®ð“½ð“¼â˜˜ï´¾Ì¯ï´¿â‚½\\ue807ð‘»ð’†ð’ð’•ð’‰ð’“ð’–ð’‚ð’ð’…ð’”ð’Žð’—ð’ŠðŸ‘½ðŸ˜™\\u200cÐ›â€’ðŸŽ¾ðŸ‘¹âŽŒðŸ’â›¸å…¬å¯“å…»å® ç‰©å—ðŸ„ðŸ€ðŸš‘ðŸ¤·æ“ç¾Žð’‘ð’šð’ð‘´ðŸ¤™ðŸ’æ¬¢è¿Žæ¥åˆ°é˜¿æ‹‰æ–¯×¡×¤ð™«ðŸˆð’Œð™Šð™­ð™†ð™‹ð™ð˜¼ð™…ï·»ðŸ¦„å·¨æ”¶èµ¢å¾—ç™½é¬¼æ„¤æ€’è¦ä¹°é¢áº½ðŸš—ðŸ³ðŸðŸðŸ–ðŸ‘ðŸ•ð’„ðŸ—ð ð™„ð™ƒðŸ‘‡é”Ÿæ–¤æ‹·ð—¢ðŸ³ðŸ±ðŸ¬â¦ãƒžãƒ«ãƒãƒ‹ãƒãƒ­æ ªå¼ç¤¾â›·í•œêµ­ì–´ã„¸ã…“ë‹ˆÍœÊ–ð˜¿ð™”â‚µð’©â„¯ð’¾ð“ð’¶ð“‰ð“‡ð“Šð“ƒð“ˆð“…â„´ð’»ð’½ð“€ð“Œð’¸ð“Žð™Î¶ð™Ÿð˜ƒð—ºðŸ®ðŸ­ðŸ¯ðŸ²ðŸ‘‹ðŸ¦Šå¤šä¼¦ðŸ½ðŸŽ»ðŸŽ¹â›“ðŸ¹ðŸ·ðŸ¦†ä¸ºå’Œä¸­å‹è°Šç¥è´ºä¸Žå…¶æƒ³è±¡å¯¹æ³•å¦‚ç›´æŽ¥é—®ç”¨è‡ªå·±çŒœæœ¬ä¼ æ•™å£«æ²¡ç§¯å”¯è®¤è¯†åŸºç£å¾’æ›¾ç»è®©ç›¸ä¿¡è€¶ç¨£å¤æ´»æ­»æ€ªä»–ä½†å½“ä»¬èŠäº›æ”¿æ²»é¢˜æ—¶å€™æˆ˜èƒœå› åœ£æŠŠå…¨å ‚ç»“å©šå­©ææƒ§ä¸”æ —è°“è¿™æ ·è¿˜â™¾ðŸŽ¸ðŸ¤•ðŸ¤’â›‘ðŸŽæ‰¹åˆ¤æ£€è®¨ðŸðŸ¦ðŸ™‹ðŸ˜¶ì¥ìŠ¤íƒ±íŠ¸ë¤¼ë„ì„ìœ ê°€ê²©ì¸ìƒì´ê²½ì œí™©ì„ë µê²Œë§Œë“¤ì§€ì•Šë¡ìž˜ê´€ë¦¬í•´ì•¼í•©ë‹¤ìºë‚˜ì—ì„œëŒ€ë§ˆì´ˆì™€í™”ì•½ê¸ˆì˜í’ˆëŸ°ì„±ë¶„ê°ˆë•ŒëŠ”ë°˜ë“œì‹œí—ˆëœì‚¬ìš©ðŸ”«ðŸ‘å‡¸á½°ðŸ’²ðŸ—¯ð™ˆá¼Œð’‡ð’ˆð’˜ð’ƒð‘¬ð‘¶ð•¾ð–™ð–—ð–†ð–Žð–Œð–ð–•ð–Šð–”ð–‘ð–‰ð–“ð–ð–œð–žð–šð–‡ð•¿ð–˜ð–„ð–›ð–’ð–‹ð–‚ð•´ð–Ÿð–ˆð•¸ðŸ‘‘ðŸš¿ðŸ’¡çŸ¥å½¼ç™¾\\uf005ð™€ð’›ð‘²ð‘³ð‘¾ð’‹ðŸ’ðŸ˜¦ð™’ð˜¾ð˜½ðŸð˜©ð˜¨á½¼á¹‘ð‘±ð‘¹ð‘«ð‘µð‘ªðŸ‡°ðŸ‡µðŸ‘¾á“‡á’§á”­áƒá§á¦á‘³á¨á“ƒá“‚á‘²á¸á‘­á‘Žá“€á£ðŸ„ðŸŽˆðŸ”¨ðŸŽðŸ¤žðŸ¸ðŸ’ŸðŸŽ°ðŸŒðŸ›³ç‚¹å‡»æŸ¥ç‰ˆðŸ­ð‘¥ð‘¦ð‘§ï¼®ï¼§ðŸ‘£\\uf020ã£ðŸ‰Ñ„ðŸ’­ðŸŽ¥ÎžðŸ´ðŸ‘¨ðŸ¤³ðŸ¦\\x0bðŸ©ð‘¯ð’’ðŸ˜—ðŸðŸ‚ðŸ‘³ðŸ—ðŸ•‰ðŸ²Ú†ÛŒð‘®ð—•ð—´ðŸ’êœ¥â²£â²ðŸ‘â°é‰„ãƒªäº‹ä»¶Ñ—ðŸ’Šã€Œã€\\uf203\\uf09a\\uf222\\ue608\\uf202\\uf099\\uf469\\ue607\\uf410\\ue600ç‡»è£½ã‚·è™šå½å±ç†å±ˆÐ“ð‘©ð‘°ð’€ð‘ºðŸŒ¤ð—³ð—œð—™ð—¦ð—§ðŸŠá½ºá¼ˆá¼¡Ï‡á¿–Î›â¤ðŸ‡³ð’™ÏˆÕÕ´Õ¥Õ¼Õ¡ÕµÕ«Õ¶Ö€Ö‚Õ¤Õ±å†¬è‡³á½€ð’ðŸ”¹ðŸ¤šðŸŽð‘·ðŸ‚ðŸ’…ð˜¬ð˜±ð˜¸ð˜·ð˜ð˜­ð˜“ð˜–ð˜¹ð˜²ð˜«Ú©Î’ÏŽðŸ’¢ÎœÎŸÎÎ‘Î•ðŸ‡±â™²ðˆâ†´ðŸ’’âŠ˜È»ðŸš´ðŸ–•ðŸ–¤ðŸ¥˜ðŸ“ðŸ‘ˆâž•ðŸš«ðŸŽ¨ðŸŒ‘ðŸ»ðŽððŠð‘­ðŸ¤–ðŸŽŽðŸ˜¼ðŸ•·ï½‡ï½’ï½Žï½”ï½‰ï½„ï½•ï½†ï½‚ï½‹ðŸ°ðŸ‡´ðŸ‡­ðŸ‡»ðŸ‡²ð—žð—­ð—˜ð—¤ðŸ‘¼ðŸ“‰ðŸŸðŸ¦ðŸŒˆðŸ”­ã€ŠðŸŠðŸ\\uf10aáƒšÚ¡ðŸ¦\\U0001f92f\\U0001f92aðŸ¡ðŸ’³á¼±ðŸ™‡ð—¸ð—Ÿð— ð—·ðŸ¥œã•ã‚ˆã†ãªã‚‰ðŸ”¼'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.tokenize.treebank import TreebankWordTokenizer\ntb_tokenizer = TreebankWordTokenizer()\n\n\nisolate_dict = {ord(c):f' {c} ' for c in symbols_to_isolate}\nremove_dict = {ord(c):f'' for c in symbols_to_delete}\n\n\ndef handle_punctuation(x):\n    x = x.translate(remove_dict)\n    x = x.translate(isolate_dict)\n    return x\n\ndef handle_contractions(x):\n    x = tb_tokenizer.tokenize(x)\n    return x\n\ndef fix_quote(x):\n    x = [x_[1:] if x_.startswith(\"'\") else x_ for x_ in x]\n    x = ' '.join(x)\n    return x\n\ndef preprocess(x):\n    x = handle_punctuation(x)\n    x = handle_contractions(x)\n    x = fix_quote(x)\n    return x","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\ncleaned_text = []\nfor text in data['excerpt']:\n    cleaned_text.append(preprocess(text)) \ndata['cleaned_text'] = cleaned_text","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## TRAIN ## (using fastai)\n\ntokenizer = GloveTokenizer(200)\nembed = GloveEmbedding(200)\n\ndef get_splits(data):\n  num = len(data)\n  idx = list(range(num))\n  random.seed(42)\n  random.shuffle(idx)\n    # changed from 0.75\n  split = int(num*0.8)\n  return idx[:split], idx[split:]\n\n\ndef get_dls(bs):\n    # changed from original code because of preprocess\n  txts = data.cleaned_text.tolist()\n  x = prepare_txts_cut(txts, tokenizer)\n  y = data.target.tolist()\n\n  ds = TfmdLists(\n      zip(x, y),\n      tfms=[],\n      splits=get_splits(data),\n  )\n\n  dls = ds.dataloaders(batch_size=bs)\n\n  return dls\n\n\ndef get_model():\n    # d_model=200 was better than 100 or 300.\n    readnet = ReadNet(\n        embed=embed,\n        d_model=200,\n        n_heads=4,\n        n_blocks=6,\n        n_feats_sent=0,\n        n_feats_doc=0,\n    )\n    readnet = readnet.cuda()\n\n    # Automatically freeze the embedding. We should not be learning this\n    for p in readnet.embed.parameters():\n        p.requires_grad = False\n\n    return readnet\n\n# added rmse for metrics (basic indicator for Public Score)\nmetrics = [rmse]\nlearn = Learner(dls=get_dls(32), model=get_model(), metrics=metrics, loss_func=MSELossFlat())\nlearn.lr_find()\n\n# Result MSE is about 0.40","metadata":{"_uuid":"daba3a26-63c6-4c15-9a96-1cf82bb3dd2b","_cell_guid":"530e10ef-f2fe-4008-b2ee-c39970c20cfb","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-27T11:34:45.613231Z","iopub.execute_input":"2021-07-27T11:34:45.613624Z","iopub.status.idle":"2021-07-27T11:36:06.690353Z","shell.execute_reply.started":"2021-07-27T11:34:45.613579Z","shell.execute_reply":"2021-07-27T11:36:06.689522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Fixed momentum=0.9 improved training. Cyclical momentum like (0.95, 0.85, 0.95) didn't.<br>\nIncreasing batch size & learning rate, Weight decay didn't work well.","metadata":{}},{"cell_type":"code","source":"cbs=[SaveModelCallback(monitor='_rmse', fname='model_0', comp=np.less, reset_on_fit=False), GradientAccumulation(32)]\nlearn.fit_one_cycle(50, 3e-5, moms=(0.9, 0.9, 0.9), cbs=cbs)","metadata":{"_uuid":"3e88c624-f760-49ef-bb9c-a47d8cca13fa","_cell_guid":"a3a21bcb-dc11-493a-bc16-f135b465da90","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"test_df = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\n\ncleaned_text = []\nfor text in test_df['excerpt']:\n    cleaned_text.append(preprocess(text)) \ntest_df['cleaned_text'] = cleaned_text\n\ntest_txts = test_df.cleaned_text.tolist()\ntest_cut_txts = prepare_txts_cut(test_txts, tokenizer)\ntest_cut_txts_zip = zip(test_cut_txts, [0 for i in range(len(test_cut_txts))])\n\ntest_dl = learn.dls.test_dl(test_cut_txts_zip, 128)\npreds,_  = learn.get_preds(dl=test_dl)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_df = pd.read_csv('../input/commonlitreadabilityprize/sample_submission.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_df.target = preds","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_df.to_csv(\"submission.csv\", index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Thank you for reading. I expected more from ReadNet. Maybe I did some big mistakes to implement or the model itself isn't great for this competition. Comment and upvote would be very much apppreciated.**","metadata":{}}]}