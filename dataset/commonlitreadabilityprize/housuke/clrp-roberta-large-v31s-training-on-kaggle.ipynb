{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"ENV = 'kaggle'\nassert ENV in ['colab', 'kaggle']\n \nPHASE = 'train'\nassert PHASE in ['train', 'eval_oof', 'inference']","metadata":{"id":"thick-significance","papermill":{"duration":0.035928,"end_time":"2021-07-21T03:59:06.032257","exception":false,"start_time":"2021-07-21T03:59:05.996329","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-30T14:22:11.387444Z","iopub.execute_input":"2021-07-30T14:22:11.387785Z","iopub.status.idle":"2021-07-30T14:22:11.39647Z","shell.execute_reply.started":"2021-07-30T14:22:11.387707Z","shell.execute_reply":"2021-07-30T14:22:11.395469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install transformers==4.5.1\n!pip install torch==1.9.0","metadata":{"id":"secondary-spice","papermill":{"duration":0.02897,"end_time":"2021-07-21T03:59:06.083838","exception":false,"start_time":"2021-07-21T03:59:06.054868","status":"completed"},"tags":[],"outputId":"679b385a-e381-437a-dce8-f65a1d0b18f4","execution":{"iopub.status.busy":"2021-07-30T14:22:11.40009Z","iopub.execute_input":"2021-07-30T14:22:11.400409Z","iopub.status.idle":"2021-07-30T14:23:44.836618Z","shell.execute_reply.started":"2021-07-30T14:22:11.400381Z","shell.execute_reply":"2021-07-30T14:23:44.835658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"id":"completed-entrance","papermill":{"duration":0.717563,"end_time":"2021-07-21T03:59:06.823305","exception":false,"start_time":"2021-07-21T03:59:06.105742","status":"completed"},"tags":[],"outputId":"3a192bcd-9497-4165-81b8-3b7fed4386b4","execution":{"iopub.status.busy":"2021-07-30T14:23:44.838592Z","iopub.execute_input":"2021-07-30T14:23:44.838962Z","iopub.status.idle":"2021-07-30T14:23:45.508233Z","shell.execute_reply.started":"2021-07-30T14:23:44.83892Z","shell.execute_reply":"2021-07-30T14:23:45.507184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if ENV=='colab':\n    from google.colab import drive\n    drive.mount('/content/drive')","metadata":{"id":"documentary-calvin","papermill":{"duration":0.035815,"end_time":"2021-07-21T03:59:06.892126","exception":false,"start_time":"2021-07-21T03:59:06.856311","status":"completed"},"tags":[],"outputId":"ca9e9007-d576-4f7f-ece9-3bc316e5d362","execution":{"iopub.status.busy":"2021-07-30T14:23:45.527506Z","iopub.execute_input":"2021-07-30T14:23:45.528218Z","iopub.status.idle":"2021-07-30T14:23:45.540005Z","shell.execute_reply.started":"2021-07-30T14:23:45.528174Z","shell.execute_reply":"2021-07-30T14:23:45.539231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport math\nimport random\nimport time\n \nimport numpy as np\nimport pandas as pd\n \nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n \nimport transformers\nfrom transformers import AdamW\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModel\nfrom transformers import AutoConfig\nfrom transformers import get_cosine_schedule_with_warmup\n \nfrom sklearn.model_selection import KFold\n \nimport gc, json, pickle, shutil\ngc.enable()\n\nfrom tqdm.auto import tqdm\nfrom matplotlib import pyplot as plt","metadata":{"id":"immune-racing","papermill":{"duration":7.66669,"end_time":"2021-07-21T03:59:14.581517","exception":false,"start_time":"2021-07-21T03:59:06.914827","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-30T14:23:45.541995Z","iopub.execute_input":"2021-07-30T14:23:45.546641Z","iopub.status.idle":"2021-07-30T14:23:51.773636Z","shell.execute_reply.started":"2021-07-30T14:23:45.546603Z","shell.execute_reply":"2021-07-30T14:23:51.772748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(transformers.__version__)","metadata":{"id":"fourth-scope","papermill":{"duration":0.029705,"end_time":"2021-07-21T03:59:14.634544","exception":false,"start_time":"2021-07-21T03:59:14.604839","status":"completed"},"tags":[],"outputId":"a021da87-d286-460a-9eb6-1215071dc722","execution":{"iopub.status.busy":"2021-07-30T14:23:51.774883Z","iopub.execute_input":"2021-07-30T14:23:51.775235Z","iopub.status.idle":"2021-07-30T14:23:51.782598Z","shell.execute_reply.started":"2021-07-30T14:23:51.775201Z","shell.execute_reply":"2021-07-30T14:23:51.781617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(torch.__version__)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T14:23:51.784213Z","iopub.execute_input":"2021-07-30T14:23:51.784558Z","iopub.status.idle":"2021-07-30T14:23:51.792403Z","shell.execute_reply.started":"2021-07-30T14:23:51.784522Z","shell.execute_reply":"2021-07-30T14:23:51.791512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_random_seed(random_seed):\n    random.seed(random_seed)\n    np.random.seed(random_seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(random_seed)\n\n    torch.manual_seed(random_seed)\n    torch.cuda.manual_seed(random_seed)\n    torch.cuda.manual_seed_all(random_seed)\n\n    torch.backends.cudnn.deterministic = True","metadata":{"id":"handmade-chamber","papermill":{"duration":0.029795,"end_time":"2021-07-21T03:59:14.686854","exception":false,"start_time":"2021-07-21T03:59:14.657059","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-30T14:23:51.793874Z","iopub.execute_input":"2021-07-30T14:23:51.79466Z","iopub.status.idle":"2021-07-30T14:23:51.804427Z","shell.execute_reply.started":"2021-07-30T14:23:51.794623Z","shell.execute_reply":"2021-07-30T14:23:51.801877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{"id":"russian-wyoming","papermill":{"duration":0.022406,"end_time":"2021-07-21T03:59:14.731991","exception":false,"start_time":"2021-07-21T03:59:14.709585","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class LitDataset(Dataset):\n    def __init__(self, df, inference_only=False):\n        super().__init__()\n\n        self.df = df        \n        self.inference_only = inference_only\n        self.text = df.excerpt.tolist()\n        #self.text = [text.replace(\"\\n\", \" \") for text in self.text]\n        \n        if not self.inference_only:\n            self.target = torch.tensor(df.target.values, dtype=torch.float32)\n            self.bins = torch.tensor(df.bins.values, dtype=torch.long)\n    \n        self.encoded = tokenizer.batch_encode_plus(\n            self.text,\n            padding = 'max_length',            \n            max_length = MAX_LEN,\n            truncation = True,\n            return_attention_mask = True,\n            return_token_type_ids = not NO_TOKEN_TYPE\n        )\n\n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):        \n        input_ids = torch.tensor(self.encoded['input_ids'][index])\n        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n        \n        out_dict = {'input_ids':input_ids, 'attention_mask':attention_mask}\n        \n        if not NO_TOKEN_TYPE:\n            out_dict['token_type_ids'] = torch.tensor(self.encoded['token_type_ids'][index])\n        \n        if sa_complex is not None:\n            if sa_complex == 'hdd':\n                with open(f'SelfAttComplex/{str(index).zfill(4)}.pkl','rb') as f:\n                    out_dict['sa_complex'] = pickle.load(f)\n            else:\n                out_dict['sa_complex'] = sa_complex[index]\n\n        if not self.inference_only:\n            out_dict['target'] = self.target[index]\n            out_dict['bins'] = self.bins[index]\n\n        return out_dict","metadata":{"id":"dependent-respondent","papermill":{"duration":0.034107,"end_time":"2021-07-21T03:59:14.788842","exception":false,"start_time":"2021-07-21T03:59:14.754735","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-30T14:23:51.805941Z","iopub.execute_input":"2021-07-30T14:23:51.808219Z","iopub.status.idle":"2021-07-30T14:23:51.828745Z","shell.execute_reply.started":"2021-07-30T14:23:51.808171Z","shell.execute_reply":"2021-07-30T14:23:51.827606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Self Attention Complexity in Pretrained Model","metadata":{"id":"tgz9F5nPUQQl"}},{"cell_type":"code","source":"def SelfAttention_Complexity(df: pd.DataFrame, output_device):\n    pre_dataset = LitDataset(df, inference_only=True)\n    pre_loader = DataLoader(pre_dataset, batch_size=BATCH_SIZE,\n                            drop_last=False, shuffle=False)\n    \n    if output_device == 'hdd':\n        os.makedirs('SelfAttComplex', exist_ok=True)\n\n    cfg_update = {\"output_attentions\":True, \"hidden_dropout_prob\": 0.0,\n                  \"layer_norm_eps\": 1e-7}\n    if PHASE=='train':\n        config = AutoConfig.from_pretrained(MODEL_NAME)\n        config.update(cfg_update)\n        backbone = AutoModel.from_pretrained(MODEL_NAME, config=config).to(DEVICE)\n    elif PHASE=='eval_oof' or PHASE=='inference':\n        config = AutoConfig.from_pretrained(LOAD_BACKBONE_DIR)\n        config.update(cfg_update)\n        backbone = AutoModel.from_pretrained(LOAD_BACKBONE_DIR, config=config).to(DEVICE)\n\n    backbone.resize_token_embeddings(len(tokenizer))\n\n    output_sa_complex = []\n    backbone.eval()\n    idx = 0\n    with torch.no_grad():\n        for batch_num, dsargs in enumerate(tqdm(pre_loader)):\n\n            kwargs = {}\n            kwargs['input_ids'] = dsargs['input_ids'].to(DEVICE)\n            if not NO_TOKEN_TYPE:\n                kwargs['token_type_ids'] = dsargs['token_type_ids'].to(DEVICE)\n            kwargs['attention_mask'] = dsargs['attention_mask'].to(DEVICE)\n\n            if 't5' in MODEL_NAME.lower() and HAS_DECODER:\n                # shift to right\n                kwargs['decoder_input_ids'] = torch.cat([tokenizer.pad_token_id * torch.ones(kwargs['input_ids'].size(0), 1).long().to(DEVICE),\n                                                        kwargs['input_ids'][:,:-1]], dim=1)\n            \n            # self attention\n            output_backbone = backbone(**kwargs)\n            self_att = torch.stack(output_backbone.attentions, dim=1) #[batch, layer, head, seq, seq]\n            seq_len = self_att.size(-1)\n            self_att = self_att.view(self_att.size(0), -1, seq_len, seq_len) #[batch, layer*head, seq, seq]\n            self_att *= kwargs['attention_mask'].unsqueeze(1).unsqueeze(-1)\n\n            # self attention complexity\n            distance_from_diag = (torch.arange(seq_len).view(1, -1) - torch.arange(seq_len).view(-1, 1)) / (seq_len - 1)\n            distance_from_diag = distance_from_diag.to(DEVICE)\n            sa_complex = []\n            temp = self_att * distance_from_diag.unsqueeze(0).unsqueeze(1).clip(min=0)\n            temp = temp.sum(dim=-1) #[batch, layer*head, seq]\n            sa_complex.append(temp)\n            temp = self_att * distance_from_diag.unsqueeze(0).unsqueeze(1).clip(max=0).abs()\n            temp = temp.sum(dim=-1) #[batch, layer*head, seq]\n            sa_complex.append(temp)\n            sa_complex = torch.cat(sa_complex, dim=1).transpose(-2,-1) #[batch, seq, layer*head*2]\n\n            if output_device == 'hdd':\n                for batch_item in sa_complex:\n                    with open(f'SelfAttComplex/{str(idx).zfill(4)}.pkl','wb') as f:\n                        pickle.dump(batch_item, f)\n                    idx += 1\n            else:\n                output_sa_complex.append(sa_complex)\n    \n    if output_device == 'hdd':\n        return 'hdd'\n    else:\n        output_sa_complex = torch.cat(output_sa_complex, dim=0)\n        return output_sa_complex.to(output_device)","metadata":{"id":"jIkqwsBLJJs-","execution":{"iopub.status.busy":"2021-07-30T14:23:51.830069Z","iopub.execute_input":"2021-07-30T14:23:51.830601Z","iopub.status.idle":"2021-07-30T14:23:51.856382Z","shell.execute_reply.started":"2021-07-30T14:23:51.830564Z","shell.execute_reply":"2021-07-30T14:23:51.85537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model\nThe model is inspired by the one from [Maunish](https://www.kaggle.com/maunish/clrp-roberta-svm).","metadata":{"id":"economic-brooks","papermill":{"duration":0.022482,"end_time":"2021-07-21T03:59:14.834036","exception":false,"start_time":"2021-07-21T03:59:14.811554","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class LitModel(nn.Module):\n    def __init__(self, benchmark_token=None, use_max_pooling=False, sa_complex_dim=0):\n        super().__init__()\n \n        self.benchmark_token = benchmark_token\n        self.use_max_pooling = use_max_pooling\n        self.sa_complex_dim = sa_complex_dim\n        \n        cfg_update = {\"output_hidden_states\":True, \"hidden_dropout_prob\": 0.0,\n                      \"layer_norm_eps\": 1e-7}\n        if PHASE=='train':\n            config = AutoConfig.from_pretrained(MODEL_NAME)\n            config.save_pretrained(f'{SAVE_DIR}/backbone')\n            config.update(cfg_update)                       \n            self.backbone = AutoModel.from_pretrained(MODEL_NAME, config=config)\n            self.backbone.save_pretrained(f'{SAVE_DIR}/backbone')\n        elif PHASE=='eval_oof' or PHASE=='inference':\n            config = AutoConfig.from_pretrained(LOAD_BACKBONE_DIR)\n            config.update(cfg_update)                       \n            self.backbone = AutoModel.from_pretrained(LOAD_BACKBONE_DIR, config=config)\n            \n        self.hidden_layer_weights = nn.Parameter(torch.zeros(NUM_HIDDEN_LAYERS).view(-1, 1, 1, 1))\n \n        # Dropout layers\n        self.dropouts_regr = nn.ModuleList([\n            nn.Dropout(0.5) for _ in range(5)\n        ])\n        self.dropouts_clsi = nn.ModuleList([\n            nn.Dropout(0.5) for _ in range(5)\n        ])\n \n        if self.use_max_pooling:\n            num_pool = 2\n        else:\n            num_pool = 1\n        self.attention_layer_norm = nn.LayerNorm(HIDDEN_SIZE * num_pool + sa_complex_dim)\n        self.attention = nn.Sequential(            \n            nn.Linear(HIDDEN_SIZE * num_pool + sa_complex_dim, 512 * num_pool),            \n            nn.Tanh(),                       \n            nn.Linear(512 * num_pool, 1),\n            nn.Softmax(dim=1)\n            )        \n        self.head_regressor = nn.Linear(HIDDEN_SIZE * num_pool + sa_complex_dim, 1)\n        self.head_classifier = nn.Linear(HIDDEN_SIZE * num_pool + sa_complex_dim, NUM_BINS)                   \n \n    def forward(self, input_ids, token_type_ids, attention_mask, self_att_complex):\n\n        kwargs = {}\n        if self.benchmark_token is None:\n            kwargs['input_ids'] = input_ids\n            if not NO_TOKEN_TYPE:\n                kwargs['token_type_ids'] = token_type_ids\n            kwargs['attention_mask'] = attention_mask\n        else:\n            benchmark_input_ids, benchmark_token_type_ids, benchmark_attention_mask = self.benchmark_token\n            kwargs['input_ids'] = torch.cat((input_ids, benchmark_input_ids), dim = 0)\n            if not NO_TOKEN_TYPE:\n                kwargs['token_type_ids'] = torch.cat((token_type_ids, benchmark_token_type_ids), dim = 0)\n            kwargs['attention_mask'] = torch.cat((attention_mask, benchmark_attention_mask), dim = 0)\n\n        if 't5' in MODEL_NAME.lower() and HAS_DECODER:\n            # shift to right\n            kwargs['decoder_input_ids'] = torch.cat([tokenizer.pad_token_id * torch.ones(kwargs['input_ids'].size(0), 1).long().to(DEVICE),\n                                                     kwargs['input_ids'][:,:-1]], dim=1)\n        output_backbone = self.backbone(**kwargs)\n        \n        # Extract output\n        if HAS_DECODER:\n            hidden_states = output_backbone.encoder_hidden_states + output_backbone.decoder_hidden_states[1:]\n        else:\n            hidden_states = output_backbone.hidden_states\n \n        # Mean/max pooling (over hidden layers), concatenate with pooler\n        hidden_states = torch.stack(tuple(hidden_states[-i-1] for i in range(len(hidden_states) - 1)), dim = 0)\n        layer_weight = nn.functional.softmax(self.hidden_layer_weights, dim = 0)\n        output_backbone = torch.sum(hidden_states * layer_weight, dim = 0)\n        if self.use_max_pooling:\n            out_max, _ = torch.max(hidden_states, dim = 0)\n            output_backbone = torch.cat((output_backbone, out_max), dim = -1)\n        if self.sa_complex_dim != 0:\n            self_att_complex = torch.cat((self_att_complex, benchmark_sa_complex), dim = 0)\n            output_backbone = torch.cat((output_backbone, self_att_complex), dim = -1)\n        \n        output_backbone = self.attention_layer_norm(output_backbone)\n \n        # Attention Pooling\n        weights = self.attention(output_backbone)\n        context_vector = torch.sum(weights * output_backbone, dim=1)        \n \n        # Multiple dropout\n        for i, dropout in enumerate(self.dropouts_regr):\n            if i == 0:\n                output_regr = self.head_regressor(dropout(context_vector))\n                output_clsi = self.head_classifier(self.dropouts_clsi[i](context_vector))\n            else:\n                output_regr += self.head_regressor(dropout(context_vector))\n                output_clsi += self.head_classifier(self.dropouts_clsi[i](context_vector))\n \n        output_regr /= len(self.dropouts_regr)\n        output_clsi /= len(self.dropouts_clsi)\n\n        if self.benchmark_token is not None:\n            output_regr = output_regr[:-1] - output_regr[-1]\n            output_clsi = output_clsi[:-1]\n\n        # Now we reduce the context vector to the prediction score.\n        return output_regr, nn.functional.softmax(output_clsi, dim=-1)","metadata":{"id":"patent-being","papermill":{"duration":0.045739,"end_time":"2021-07-21T03:59:14.902472","exception":false,"start_time":"2021-07-21T03:59:14.856733","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-30T14:23:51.857922Z","iopub.execute_input":"2021-07-30T14:23:51.858539Z","iopub.status.idle":"2021-07-30T14:23:51.891027Z","shell.execute_reply.started":"2021-07-30T14:23:51.858501Z","shell.execute_reply":"2021-07-30T14:23:51.889992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# loss function","metadata":{"id":"intelligent-seating","papermill":{"duration":0.02252,"end_time":"2021-07-21T03:59:14.948212","exception":false,"start_time":"2021-07-21T03:59:14.925692","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class QuadraticWeightedKappaLoss(nn.Module):\n    def __init__(self, num_cat, device = 'cpu'):\n        super(QuadraticWeightedKappaLoss, self).__init__()\n        self.num_cat = num_cat\n        cats = torch.arange(num_cat).to(device)\n        self.weights = (cats.view(-1,1) - cats.view(1,-1)).pow(2) / (num_cat - 1)**2\n        \n    def _confusion_matrix(self, pred_smax, true_cat):\n        confusion_matrix = torch.zeros((self.num_cat, self.num_cat)).to(pred_smax.device)\n        for t, p in zip(true_cat.view(-1), pred_smax):\n            confusion_matrix[t.long()] += p\n        return confusion_matrix\n        \n    def forward(self, pred_smax, true_cat):\n        # Confusion matrix\n        O = self._confusion_matrix(pred_smax, true_cat)\n        \n        # Count elements in each category\n        true_hist = torch.bincount(true_cat, minlength = self.num_cat)\n        pred_hist = pred_smax.sum(dim = 0)\n        \n        # Expected values\n        E = torch.outer(true_hist, pred_hist)\n        \n        # Normlization\n        O = O / torch.sum(O)\n        E = E / torch.sum(E)\n        \n        # Weighted Kappa\n        numerator = torch.sum(self.weights * O)\n        denominator = torch.sum(self.weights * E)\n        \n        return COEF_QWK * numerator / denominator","metadata":{"id":"surprised-beads","papermill":{"duration":0.034319,"end_time":"2021-07-21T03:59:15.005276","exception":false,"start_time":"2021-07-21T03:59:14.970957","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-30T14:23:51.892554Z","iopub.execute_input":"2021-07-30T14:23:51.893156Z","iopub.status.idle":"2021-07-30T14:23:51.904745Z","shell.execute_reply.started":"2021-07-30T14:23:51.893103Z","shell.execute_reply":"2021-07-30T14:23:51.903841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BradleyTerryLoss(nn.Module):\n    def __init__(self):\n        super(BradleyTerryLoss, self).__init__()\n\n    def forward(self, pred_mean, true_mean):\n        batch_size = len(pred_mean)\n        true_comparison = true_mean.view(-1,1) - true_mean.view(1,-1)\n        pred_comparison = pred_mean.view(-1,1) - pred_mean.view(1,-1)\n        \n        return COEF_BT * (torch.log(1 + torch.tril(torch.exp(-true_comparison * pred_comparison))).sum()\n                          / (batch_size * (batch_size - 1) / 2))","metadata":{"id":"limited-drama","papermill":{"duration":0.030369,"end_time":"2021-07-21T03:59:15.058733","exception":false,"start_time":"2021-07-21T03:59:15.028364","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-30T14:23:51.906014Z","iopub.execute_input":"2021-07-30T14:23:51.906631Z","iopub.status.idle":"2021-07-30T14:23:51.91822Z","shell.execute_reply.started":"2021-07-30T14:23:51.906592Z","shell.execute_reply":"2021-07-30T14:23:51.917205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def eval_mse(model, data_loader):\n    \"\"\"Evaluates the mean squared error of the |model| on |data_loader|\"\"\"\n    model.eval()            \n    mse_sum = 0\n\n    all_pred_r = []\n    with torch.no_grad():\n        for batch_num, dsargs in enumerate(data_loader):\n            input_ids = dsargs['input_ids'].to(DEVICE)\n            attention_mask = dsargs['attention_mask'].to(DEVICE)\n            target = dsargs['target'].to(DEVICE)\n            bins = dsargs['bins'].to(DEVICE)\n\n            token_type_ids = None\n            if not NO_TOKEN_TYPE:\n                token_type_ids = dsargs['token_type_ids'].to(DEVICE)\n\n            self_att_complex = None\n            if USE_SELF_ATT:\n                self_att_complex = dsargs['sa_complex'].to(DEVICE)\n\n            pred_r, _ = model(input_ids, token_type_ids, attention_mask, self_att_complex)                       \n\n            mse_sum += nn.MSELoss(reduction=\"sum\")(pred_r.flatten(), target).item()\n            all_pred_r.append(pred_r)\n\n    return mse_sum / len(data_loader.dataset), torch.cat(all_pred_r, dim=0).squeeze()","metadata":{"id":"military-mathematics","papermill":{"duration":0.032195,"end_time":"2021-07-21T03:59:15.114402","exception":false,"start_time":"2021-07-21T03:59:15.082207","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-30T14:23:51.919597Z","iopub.execute_input":"2021-07-30T14:23:51.920215Z","iopub.status.idle":"2021-07-30T14:23:51.930965Z","shell.execute_reply.started":"2021-07-30T14:23:51.920181Z","shell.execute_reply":"2021-07-30T14:23:51.929944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training, Validation","metadata":{"id":"optical-boating","papermill":{"duration":0.022508,"end_time":"2021-07-21T03:59:15.159426","exception":false,"start_time":"2021-07-21T03:59:15.136918","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def train(model, model_path, train_loader, val_loader,\n          optimizer, num_epochs, fold, scheduler=None):    \n    best_val_rmse = None\n    best_epoch = 0\n    step = 0\n    last_eval_step = 0\n    eval_period = EVAL_SCHEDULE[0][1]    \n\n    start = time.time()\n\n    history = {'step':[], 'epoch':[], 'batch_num':[], 'val_rmse':[],\n               'trn_rmse':[], 'trn_qwk':[], 'trn_bt':[]}\n    \n    for epoch in range(num_epochs):\n        val_rmse = None         \n\n        epoch_target, epoch_bins, epoch_pred_r, epoch_pred_c = (torch.tensor([]),)*4\n        epoch_bins = epoch_bins.long()\n    \n        for batch_num, dsargs in enumerate(train_loader):\n            input_ids = dsargs['input_ids'].to(DEVICE)\n            attention_mask = dsargs['attention_mask'].to(DEVICE)\n            target = dsargs['target'].to(DEVICE)\n            bins = dsargs['bins'].to(DEVICE)\n\n            token_type_ids = None\n            if not NO_TOKEN_TYPE:\n                token_type_ids = dsargs['token_type_ids'].to(DEVICE)\n\n            self_att_complex = None\n            if USE_SELF_ATT:\n                self_att_complex = dsargs['sa_complex'].to(DEVICE)\n\n            optimizer.zero_grad()\n            \n            model.train()\n\n            pred_r, pred_c = model(input_ids, token_type_ids, attention_mask, self_att_complex)\n                                                        \n            loss = (nn.MSELoss(reduction=\"mean\")(pred_r.flatten(), target)\n                    + QWKloss(pred_c, bins) + BTloss(pred_r.flatten(), target))\n                        \n            loss.backward()\n            \n            epoch_target = torch.cat([epoch_target.to(DEVICE), target.clone().detach()], dim=0)\n            epoch_bins = torch.cat([epoch_bins.to(DEVICE), bins.clone().detach()], dim=0)\n            epoch_pred_r = torch.cat([epoch_pred_r.to(DEVICE), pred_r.clone().detach()], dim=0)\n            epoch_pred_c = torch.cat([epoch_pred_c.to(DEVICE), pred_c.clone().detach()], dim=0)\n\n            optimizer.step()\n            if scheduler:\n                scheduler.step()\n            \n            if step >= last_eval_step + eval_period:\n                # Evaluate the model on val_loader.\n                elapsed_seconds = time.time() - start\n                num_steps = step - last_eval_step\n                print(f\"\\n{num_steps} steps took {elapsed_seconds:0.3} seconds\")\n                last_eval_step = step\n                \n                mse, _ = eval_mse(model, val_loader)\n                val_rmse = math.sqrt(mse)\n                trn_rmse = nn.MSELoss(reduction=\"mean\")(epoch_pred_r.flatten(), epoch_target).item()\n                trn_qwk  = QWKloss(epoch_pred_c, epoch_bins).item()\n                trn_bt  = BTloss(epoch_pred_r.flatten(), epoch_target).item()\n\n                print(f\"Epoch: {epoch} batch_num: {batch_num}\", \n                      f\"val_rmse: {val_rmse:0.4}\", f\"train_rmse: {trn_rmse:0.4}\",\n                      f\"train_qwk: {trn_qwk:0.4}\", f\"train_bt: {trn_bt:0.4}\")\n\n                for rmse, period in EVAL_SCHEDULE:\n                    if val_rmse >= rmse:\n                        eval_period = period\n                        break\n                percent = step / (num_epochs * len(train_loader))\n                if 0.5 <= percent and percent <= 0.8:\n                    eval_period = min([eval_period, 8])\n                \n                if not best_val_rmse or val_rmse < best_val_rmse:                    \n                    best_val_rmse = val_rmse\n                    best_epoch = epoch\n                    torch.save(model.state_dict(), model_path)\n                    print(f\"New best_val_rmse: {best_val_rmse:0.4}\")\n                else:       \n                    print(f\"Still best_val_rmse: {best_val_rmse:0.4}\",\n                          f\"(from epoch {best_epoch})\")\n\n                ''' history json dump '''\n                history['step'].append(step)\n                history['epoch'].append(epoch)\n                history['batch_num'].append(batch_num)\n                history['val_rmse'].append(val_rmse)\n                history['trn_rmse'].append(trn_rmse)\n                history['trn_qwk'].append(trn_qwk)\n                history['trn_bt'].append(trn_bt)\n                with open(f'{SAVE_DIR}/{MODEL_VER}_fold{fold+1}_history.json', 'w') as f:\n                    json.dump(history, f, indent=4)\n                    \n                start = time.time()\n                                            \n            step += 1\n\n        del epoch_target, epoch_bins, epoch_pred_r, epoch_pred_c\n        \n        print('\\nHidden Layer Weights:')\n        print(model.hidden_layer_weights.squeeze())\n        print(nn.functional.softmax(model.hidden_layer_weights.squeeze(),dim=0))\n    \n    return best_val_rmse","metadata":{"id":"acceptable-stationery","papermill":{"duration":0.045642,"end_time":"2021-07-21T03:59:15.227759","exception":false,"start_time":"2021-07-21T03:59:15.182117","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-30T14:23:51.932326Z","iopub.execute_input":"2021-07-30T14:23:51.932901Z","iopub.status.idle":"2021-07-30T14:23:51.964505Z","shell.execute_reply.started":"2021-07-30T14:23:51.932863Z","shell.execute_reply":"2021-07-30T14:23:51.963429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(model, data_loader):\n    \"\"\"Returns an np.array with predictions of the |model| on |data_loader|\"\"\"\n    model.eval()\n\n    result = np.zeros(len(data_loader.dataset))    \n    index = 0\n    \n    with torch.no_grad():\n        for batch_num, dsargs in enumerate(data_loader):\n            input_ids = dsargs['input_ids'].to(DEVICE)\n            attention_mask = dsargs['attention_mask'].to(DEVICE)\n\n            token_type_ids = None\n            if not NO_TOKEN_TYPE:\n                token_type_ids = dsargs['token_type_ids'].to(DEVICE)\n\n            self_att_complex = None\n            if USE_SELF_ATT:\n                self_att_complex = dsargs['sa_complex'].to(DEVICE)\n                        \n            pred_r, _ = model(input_ids, token_type_ids, attention_mask, self_att_complex)                        \n\n            result[index : index + pred_r.shape[0]] = pred_r.flatten().to(\"cpu\")\n            index += pred_r.shape[0]\n\n    return result","metadata":{"id":"earned-ministry","papermill":{"duration":0.031383,"end_time":"2021-07-21T03:59:15.28167","exception":false,"start_time":"2021-07-21T03:59:15.250287","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-30T14:23:51.965951Z","iopub.execute_input":"2021-07-30T14:23:51.966509Z","iopub.status.idle":"2021-07-30T14:23:51.97603Z","shell.execute_reply.started":"2021-07-30T14:23:51.96647Z","shell.execute_reply":"2021-07-30T14:23:51.974964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_optimizer(model):\n    named_parameters = list(model.named_parameters())\n    \n    backbone_parameters = [(n, p) for n, p in named_parameters if n.startswith('backbone')]\n    attention_parameters = [(n, p) for n, p in named_parameters if n.startswith('attention')]\n    hidden_wts_parameters = [(n, p) for n, p in named_parameters if n.startswith ('hidden_layer_weights')]\n    head_parameters = [(n, p) for n, p in named_parameters if n.startswith('head')]\n        \n    attention_group = [params for (name, params) in attention_parameters]\n    hidden_wts_group = [params for (name, params) in hidden_wts_parameters]\n    head_group = [params for (name, params) in head_parameters]\n \n    parameters = []\n    parameters.append({\"params\": attention_group})\n    parameters.append({\"params\": hidden_wts_group, 'weight_decay': 0.0, 'lr': HIDDEN_WTS_LR})\n    parameters.append({\"params\": head_group})\n \n    no_decay = ['bias', 'LayerNorm.weight', 'layer_norm']\n \n    if 'roberta' in MODEL_NAME.lower() or 'electra' in MODEL_NAME.lower():\n        layers = [getattr(model, 'backbone').embeddings] + list(getattr(model, 'backbone').encoder.layer)\n    elif 'gpt2' in MODEL_NAME.lower():\n        layers = [getattr(model, 'backbone').wte] + list(getattr(model, 'backbone').h)\n    elif 'xlnet' in MODEL_NAME.lower():\n        layers = [getattr(model, 'backbone').word_embedding] + list(getattr(model, 'backbone').layer)\n    elif 'bart' in MODEL_NAME.lower():\n        enc_layers = ([getattr(model, 'backbone').encoder.embed_positions] +\n                      list(getattr(model, 'backbone').encoder.layers) +\n                      [getattr(model, 'backbone').encoder.layernorm_embedding])\n        dec_layers = ([getattr(model, 'backbone').decoder.embed_positions] +\n                      list(getattr(model, 'backbone').decoder.layers) + \n                      [getattr(model, 'backbone').decoder.layernorm_embedding])\n        assert len(enc_layers)==len(dec_layers)\n        layers = [getattr(model, 'backbone').shared]\n        for e, d in zip(enc_layers, dec_layers):\n            layers += [e, d]\n    elif 't5' in MODEL_NAME.lower():\n        enc_layers = (list(getattr(model, 'backbone').encoder.block) +\n                      [getattr(model, 'backbone').encoder.final_layer_norm])\n        dec_layers = (list(getattr(model, 'backbone').decoder.block) + \n                      [getattr(model, 'backbone').decoder.final_layer_norm])\n        assert len(enc_layers)==len(dec_layers)\n        layers = [getattr(model, 'backbone').shared]\n        for e, d in zip(enc_layers, dec_layers):\n            layers += [e, d]\n    else:\n        raise RuntimeError('specify the parameters for backbone.')\n \n    layers.reverse()\n    layerwise_learning_rate_decay = LAYERWISE_LR_DECAY**(1.0/len(layers))\n    lr = BACKBONE_LR\n    for i, layer in enumerate(layers):\n        lr *= layerwise_learning_rate_decay\n        parameters += [\n            {\n                'params': [p for n, p in layer.named_parameters() if not any(nd in n for nd in no_decay)],\n                'weight_decay': 0.01,\n                'lr': lr,\n            },\n            {\n                'params': [p for n, p in layer.named_parameters() if any(nd in n for nd in no_decay)],\n                'weight_decay': 0.0,\n                'lr': lr,\n            },\n        ]\n \n    return AdamW(parameters)","metadata":{"id":"noted-float","papermill":{"duration":0.042397,"end_time":"2021-07-21T03:59:15.346831","exception":false,"start_time":"2021-07-21T03:59:15.304434","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-30T14:23:51.977338Z","iopub.execute_input":"2021-07-30T14:23:51.978065Z","iopub.status.idle":"2021-07-30T14:23:52.005499Z","shell.execute_reply.started":"2021-07-30T14:23:51.977969Z","shell.execute_reply":"2021-07-30T14:23:52.00421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_examples_to_features(text, tokenizer, max_len, is_test = False, return_tensor = False):\n    # Take from https://www.kaggle.com/rhtsingh/commonlit-readability-prize-roberta-torch-fit\n    text = text.replace('\\n', '')\n    if return_tensor:\n        tok = tokenizer.encode_plus(\n            text, \n            max_length = max_len, \n            padding = 'max_length', \n            return_tensors = 'pt',\n            truncation = True,\n            return_attention_mask = True,\n            return_token_type_ids = not NO_TOKEN_TYPE\n        )\n    else:\n        tok = tokenizer.encode_plus(\n            text, \n            max_length = max_len, \n            padding = 'max_length', \n            truncation = True,\n            return_attention_mask = True,\n            return_token_type_ids = not NO_TOKEN_TYPE\n        )\n    return tok","metadata":{"id":"champion-session","papermill":{"duration":0.030472,"end_time":"2021-07-21T03:59:15.399841","exception":false,"start_time":"2021-07-21T03:59:15.369369","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-30T14:23:52.00682Z","iopub.execute_input":"2021-07-30T14:23:52.00756Z","iopub.status.idle":"2021-07-30T14:23:52.01494Z","shell.execute_reply.started":"2021-07-30T14:23:52.007452Z","shell.execute_reply":"2021-07-30T14:23:52.013872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Train_or_Validation():\n    list_val_rmse = []\n \n    oof = []\n    for fold in range(NUM_FOLDS):\n        print(f\"\\nFold {fold + 1}/{NUM_FOLDS}\")\n            \n        set_random_seed(SEED + fold)\n        \n        train_dataset = LitDataset(train_df[train_df['kfold'] != fold])\n        val_dataset = LitDataset(train_df[train_df['kfold'] == fold])\n        val_df = train_df[train_df['kfold'] == fold].copy()\n            \n        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n                                  drop_last=True, shuffle=True, num_workers=0)    \n        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE,\n                                drop_last=False, shuffle=False, num_workers=0)    \n        \n        sa_complex_dim = 0\n        if USE_SELF_ATT:\n            sa_complex_dim = benchmark_sa_complex.size(-1)\n        \n        model = LitModel(benchmark_token = benchmark_token, use_max_pooling = USE_MAX_POOLING,\n                         sa_complex_dim = sa_complex_dim).to(DEVICE)\n        \n        # Update vocabulary size\n        model.backbone.resize_token_embeddings(len(tokenizer))\n \n        if PHASE=='train':\n            model_path = f\"{SAVE_DIR}/model_{fold + 1}.bin\"\n            set_random_seed(SEED + fold)    \n \n            optimizer = create_optimizer(model)                        \n            scheduler = get_cosine_schedule_with_warmup(\n                optimizer,\n                num_training_steps = NUM_EPOCHS * len(train_loader) * 11//10,\n                num_warmup_steps = 50)\n            \n            list_val_rmse.append(train(model, model_path, train_loader, val_loader, optimizer, \n                                       num_epochs=NUM_EPOCHS, fold=fold, scheduler=scheduler, ))\n        \n        elif PHASE=='eval_oof':\n            model_path = f\"{MODEL_DIR}/model_{fold + 1}.bin\"\n            model.load_state_dict(torch.load(model_path))\n            model.to(DEVICE)\n            \n            mse, pred_r = eval_mse(model, val_loader)\n            val_df['pred'] = pred_r.to('cpu').detach().numpy().copy()\n            oof.append(val_df)\n            list_val_rmse.append(math.sqrt(mse))\n \n        del model\n        gc.collect()\n        \n        print(\"\\nPerformance estimates:\")\n        print(list_val_rmse)\n        print(\"Mean:\", np.array(list_val_rmse).mean())\n\n    if PHASE=='eval_oof':\n        oof = pd.concat(oof)\n\n    return oof","metadata":{"id":"plain-stake","papermill":{"duration":0.035803,"end_time":"2021-07-21T03:59:15.458042","exception":false,"start_time":"2021-07-21T03:59:15.422239","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-30T14:23:52.019357Z","iopub.execute_input":"2021-07-30T14:23:52.019941Z","iopub.status.idle":"2021-07-30T14:23:52.03692Z","shell.execute_reply.started":"2021-07-30T14:23:52.019902Z","shell.execute_reply":"2021-07-30T14:23:52.036041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Inference():\n    all_predictions = np.zeros((NUM_FOLDS, len(test_df)))\n\n    test_dataset = LitDataset(test_df, inference_only=True)\n    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n                             drop_last=False, shuffle=False, num_workers=0)\n\n    for fold in range(NUM_FOLDS):            \n\n        sa_complex_dim = 0\n        if USE_SELF_ATT:\n            sa_complex_dim = benchmark_sa_complex.size(-1)\n\n        model = LitModel(benchmark_token = benchmark_token, use_max_pooling = USE_MAX_POOLING,\n                         sa_complex_dim = sa_complex_dim).to(DEVICE)\n\n        # Update vocabulary size\n        model.backbone.resize_token_embeddings(len(tokenizer))\n\n        model_path = f\"{MODEL_DIR}/model_{fold + 1}.bin\"\n        print(f\"\\nUsing {model_path}\")\n                            \n        model.load_state_dict(torch.load(model_path))    \n        \n        all_predictions[fold] = predict(model, test_loader)\n        \n        del model\n        gc.collect()\n\n    predictions = all_predictions.mean(axis=0)\n    output_df = submission_df.copy()\n    output_df.target = predictions\n    print(output_df)\n\n    return output_df","metadata":{"id":"certain-platform","papermill":{"duration":0.031555,"end_time":"2021-07-21T03:59:15.512158","exception":false,"start_time":"2021-07-21T03:59:15.480603","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-30T14:23:52.044503Z","iopub.execute_input":"2021-07-30T14:23:52.044973Z","iopub.status.idle":"2021-07-30T14:23:52.054718Z","shell.execute_reply.started":"2021-07-30T14:23:52.044936Z","shell.execute_reply":"2021-07-30T14:23:52.053755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Main","metadata":{"papermill":{"duration":0.022876,"end_time":"2021-07-21T03:59:15.829228","exception":false,"start_time":"2021-07-21T03:59:15.806352","status":"completed"},"tags":[],"id":"dated-valley"}},{"cell_type":"code","source":"DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nif ENV=='colab':\n    BASE_DIR = '/content/drive/MyDrive/Colab Notebooks/CLR/input'\n    TRAIN_DATA_DIR = BASE_DIR\nelif ENV=='kaggle':\n    BASE_DIR = '../input/commonlitreadabilityprize'\n    TRAIN_DATA_DIR = '../input/step-1-create-folds'\n\ntrain_df = pd.read_csv(f'{TRAIN_DATA_DIR}/train_folds.csv')\nbenchmark = train_df[(train_df.target == 0) & (train_df.standard_error == 0)].copy()\ntrain_df.drop(train_df[(train_df.target == 0) & (train_df.standard_error == 0)].index,\n              inplace=True)\ntrain_df.reset_index(drop=True, inplace=True)\n\ntest_df = pd.read_csv(f\"{BASE_DIR}/test.csv\")\nsubmission_df = pd.read_csv(f\"{BASE_DIR}/sample_submission.csv\")","metadata":{"id":"pleasant-fraud","papermill":{"duration":0.156481,"end_time":"2021-07-21T03:59:15.78347","exception":false,"start_time":"2021-07-21T03:59:15.626989","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-30T14:23:52.056246Z","iopub.execute_input":"2021-07-30T14:23:52.057007Z","iopub.status.idle":"2021-07-30T14:23:52.247539Z","shell.execute_reply.started":"2021-07-30T14:23:52.056972Z","shell.execute_reply":"2021-07-30T14:23:52.246603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SEED = 1000\nNUM_FOLDS = 5\nNUM_EPOCHS = 4\nBATCH_SIZE = 8\nMAX_LEN = 248\nEVAL_SCHEDULE = [(0.52, 32), (0.49, 16), (0.48, 8), (0.47, 4), (-1., 2)]\nMODEL_NAME = 'roberta-large'\nMODEL_VER = 'CLRP_LightBase_031s_RoBERTaL'\n \nNUM_HIDDEN_LAYERS = 24\nHIDDEN_SIZE = 1024\nNUM_BINS = 29\nCOEF_QWK = 0.0 # coefficient of QWK loss\nCOEF_BT = 1.0 # coefficient of Bradley-Terry loss\n\nUSE_MAX_POOLING = True\nUSE_SELF_ATT = True\nNO_TOKEN_TYPE = False\nHAS_DECODER = False\n\nBACKBONE_LR = 2e-5\nHIDDEN_WTS_LR = 1e-2\nLAYERWISE_LR_DECAY = 0.1\n\nif ENV=='colab':\n    MODEL_DIR = f'/content/drive/MyDrive/Colab Notebooks/CLR/{MODEL_VER}'\n    SAVE_DIR = MODEL_DIR\n    LOAD_BACKBONE_DIR = f'{MODEL_DIR}/backbone'\nelif ENV=='kaggle':\n    MODEL_DIR = '../input/clrp-lightbase-031s-robertal-dat'\n    SAVE_DIR = '.'\n    LOAD_BACKBONE_DIR = '../input/robertalarge'\n\nQWKloss = QuadraticWeightedKappaLoss(num_cat=NUM_BINS, device=DEVICE)\nBTloss = BradleyTerryLoss()\ntrain_df['bins'] = pd.cut(train_df['target'], bins=NUM_BINS, labels=False)\n\n# Setup Tokenizer\nif PHASE=='train':\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n    tokenizer.save_pretrained(f'{SAVE_DIR}/backbone')\nelif PHASE=='eval_oof' or PHASE=='inference':\n    tokenizer = AutoTokenizer.from_pretrained(LOAD_BACKBONE_DIR)\nif 'gpt2' in MODEL_NAME.lower():\n    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n\n# Tokenize the benchmark text\nbenchmark_token = convert_examples_to_features(benchmark['excerpt'].iloc[0], tokenizer, MAX_LEN, return_tensor = True)\nif NO_TOKEN_TYPE:\n    benchmark_token = (benchmark_token['input_ids'].to(DEVICE), None, benchmark_token['attention_mask'].to(DEVICE))\nelse:\n    benchmark_token = (benchmark_token['input_ids'].to(DEVICE), benchmark_token['token_type_ids'].to(DEVICE), benchmark_token['attention_mask'].to(DEVICE))\n\n# Main\nif PHASE=='train' or PHASE=='eval_oof':\n    sa_complex = None # Self-Attention Complexity in Pretrained Model\n    if USE_SELF_ATT:\n        sa_complex = SelfAttention_Complexity(train_df, 'cpu')\n        benchmark_sa_complex = SelfAttention_Complexity(benchmark, DEVICE)\n    oof_df = Train_or_Validation()\n\nif PHASE=='eval_oof':\n    oof_df.to_csv(f'oof_{MODEL_VER}.csv')\n\nif PHASE=='inference':\n    sa_complex = None # Self-Attention Complexity in Pretrained Model\n    if USE_SELF_ATT:\n        sa_complex = SelfAttention_Complexity(test_df, 'cpu')\n        benchmark_sa_complex = SelfAttention_Complexity(benchmark, DEVICE)\n    submission_df = Inference()\n    submission_df.to_csv(\"submission.csv\", index=False)\n\nif os.path.isdir('SelfAttComplex'):\n    shutil.rmtree('SelfAttComplex')","metadata":{"id":"statistical-blues","papermill":{"duration":175.708893,"end_time":"2021-07-21T04:02:11.56077","exception":false,"start_time":"2021-07-21T03:59:15.851877","status":"completed"},"tags":[],"outputId":"fa7d3150-336f-472a-e7c3-8fa4e744156e","execution":{"iopub.status.busy":"2021-07-30T14:23:52.248815Z","iopub.execute_input":"2021-07-30T14:23:52.249156Z","iopub.status.idle":"2021-07-30T14:30:20.60351Z","shell.execute_reply.started":"2021-07-30T14:23:52.249101Z","shell.execute_reply":"2021-07-30T14:30:20.601016Z"},"trusted":true},"execution_count":null,"outputs":[]}]}