{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# NLP from bag of words to transformer","metadata":{"execution":{"iopub.status.busy":"2021-06-16T20:06:13.217669Z","iopub.execute_input":"2021-06-16T20:06:13.218399Z","iopub.status.idle":"2021-06-16T20:06:13.222639Z","shell.execute_reply.started":"2021-06-16T20:06:13.218348Z","shell.execute_reply":"2021-06-16T20:06:13.221767Z"}}},{"cell_type":"markdown","source":"\n\n__This notebook is using commonlitreadibilityprize__ data to showcase the different possible approaches to solve an NLP regression problem.\nIn real word application this would be just the last part of the long data science pipeline.\nIn this notebook we will only focus on the most common modelling techniques, going from the most basic to the most complex one","metadata":{}},{"cell_type":"markdown","source":"## Models :\n1. Bag of words \n2. TF-IDF \n3. Word2Vec\n3. Decision tree & ensamble\n4. Support Vector machine\n5. Transformer","metadata":{}},{"cell_type":"markdown","source":"## Pre Processing","metadata":{}},{"cell_type":"markdown","source":"The first step, obviously, is to import all the necessary packages to implement the NLP prediction process. \nSpecifically, to carry out the different machine learning tasks we import:\n\n* NLTK (data preprocessing)\n* scikitlearn (models implementation) \n* re (Regex)\n* gensim(word2vec)\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport nltk\nfrom nltk.tokenize import word_tokenize as wt \nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nstemmer = PorterStemmer()\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nimport gensim\nfrom gensim.models import Word2Vec\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2021-06-19T10:32:37.40845Z","iopub.execute_input":"2021-06-19T10:32:37.408829Z","iopub.status.idle":"2021-06-19T10:32:38.120652Z","shell.execute_reply.started":"2021-06-19T10:32:37.408797Z","shell.execute_reply":"2021-06-19T10:32:38.11976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Loading data from the challenge:","metadata":{}},{"cell_type":"code","source":"dataset=pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/train.csv\")\ndataset=dataset[['target','excerpt']]\ndataset.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-19T10:13:50.641932Z","iopub.execute_input":"2021-06-19T10:13:50.642409Z","iopub.status.idle":"2021-06-19T10:13:50.689006Z","shell.execute_reply.started":"2021-06-19T10:13:50.64238Z","shell.execute_reply":"2021-06-19T10:13:50.688422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The first step in our process is cleaning the data and making them machine-readable. \nTo this purpose, we perform:\n\n* Stemming \n* Stop Words Removing\n* Tokenization\n* Lowercase Standardization","metadata":{}},{"cell_type":"code","source":"data = []\n\nfor i in range(dataset.shape[0]):\n\n    sms = dataset.iloc[i, 1]\n\n    # remove non alphabatic characters\n    sms = re.sub('[^A-Za-z]', ' ', sms)\n\n    # make words lowercase, because Go and go will be considered as two words\n    sms = sms.lower()\n\n    # tokenising\n    tokenized_sms = wt(sms)\n\n    # remove stop words and stemming\n \n    sms_processed = []\n    for word in tokenized_sms:\n        if word not in set(stopwords.words('english')):\n            sms_processed.append(stemmer.stem(word))\n\n    sms_text = \" \".join(sms_processed)\n    data.append(sms_text)","metadata":{"execution":{"iopub.status.busy":"2021-06-19T08:11:23.886273Z","iopub.execute_input":"2021-06-19T08:11:23.886606Z","iopub.status.idle":"2021-06-19T08:12:41.500829Z","shell.execute_reply.started":"2021-06-19T08:11:23.886578Z","shell.execute_reply":"2021-06-19T08:12:41.500133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Most importantly, we build a matrix that allows the subsequent models to be implemented. In fact, machine learning models receive as an input a matrix that should represent the underlying data in vectors.\n\nIt's worth noticing that we don't limit our approach to one single vectorization. That is, we use three different approaches as elements of the matrix in order to allow model comparison and to reduce the risk of having biased models.\nSpecifically, we use:\n    \n1. COUNT\n2. TF/IDF\n3. WORD2VEC\n\nNotice that word2vec doesn't need stemming or other pre-processing techniques.\n\nWe will now compare the different vectorization approaches using a simple linear regression.\n","metadata":{}},{"cell_type":"markdown","source":"## 1. Bag of words & linear regression\n","metadata":{"execution":{"iopub.status.busy":"2021-06-19T07:29:18.406196Z","iopub.execute_input":"2021-06-19T07:29:18.406614Z","iopub.status.idle":"2021-06-19T07:29:18.411476Z","shell.execute_reply.started":"2021-06-19T07:29:18.406515Z","shell.execute_reply":"2021-06-19T07:29:18.410624Z"}}},{"cell_type":"markdown","source":"The first model is a simple linear regression trained on the bag of words representation.\nFirst we transform the data into vectors, then we proceed with the classical train/test split.","metadata":{}},{"cell_type":"code","source":"# creating the feature matrix \nfrom sklearn.feature_extraction.text import CountVectorizer\n\nvectorizer = CountVectorizer(max_features=1000) #Bag of words\nX = vectorizer.fit_transform(data).toarray()\ny = dataset.iloc[:, 0]\n\n# split train and test data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-06-19T08:12:41.501885Z","iopub.execute_input":"2021-06-19T08:12:41.502218Z","iopub.status.idle":"2021-06-19T08:12:41.838836Z","shell.execute_reply.started":"2021-06-19T08:12:41.502193Z","shell.execute_reply":"2021-06-19T08:12:41.838118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then we initialized the linear regression, trained it and evaluated it on the test set.","metadata":{}},{"cell_type":"code","source":"regr = linear_model.LinearRegression()\n\n# Train the model using the training sets\nregr.fit(X_train, y_train)\n\n# Make predictions using the testing set\ny_pred = regr.predict(X_test)\n\n# RMSE volue on test set\nscores = cross_val_score(regr, X_train, y_train, cv=5,scoring='neg_root_mean_squared_error')\n-(scores.mean())","metadata":{"execution":{"iopub.status.busy":"2021-06-19T08:35:04.09254Z","iopub.execute_input":"2021-06-19T08:35:04.092917Z","iopub.status.idle":"2021-06-19T08:35:07.048898Z","shell.execute_reply.started":"2021-06-19T08:35:04.092887Z","shell.execute_reply":"2021-06-19T08:35:07.047845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the linear regression we can extract the coefficient that represents the feature importance of each word.","metadata":{}},{"cell_type":"code","source":"coef_table = pd.DataFrame(list(vectorizer.get_feature_names())).copy()\ncoef_table.insert(len(coef_table.columns),\"Coefs\",regr.coef_.transpose())\ncoef_table.nlargest(25,'Coefs')","metadata":{"execution":{"iopub.status.busy":"2021-06-19T08:12:44.95705Z","iopub.execute_input":"2021-06-19T08:12:44.957428Z","iopub.status.idle":"2021-06-19T08:12:45.005321Z","shell.execute_reply.started":"2021-06-19T08:12:44.957391Z","shell.execute_reply":"2021-06-19T08:12:45.004178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Error distribution:","metadata":{}},{"cell_type":"code","source":"print(f' RMSE: {mean_squared_error(y_test, y_pred, squared=False):.4f}')\nfig, ax = plt.subplots(1, 1, figsize=(20, 6))\nsns.distplot(y_test, ax=ax, label='Label')\nsns.distplot(y_pred, ax=ax, label='Prediction')\nax.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-19T08:12:45.00749Z","iopub.execute_input":"2021-06-19T08:12:45.009475Z","iopub.status.idle":"2021-06-19T08:12:45.339232Z","shell.execute_reply.started":"2021-06-19T08:12:45.009426Z","shell.execute_reply":"2021-06-19T08:12:45.338348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. TF-IDF\n","metadata":{}},{"cell_type":"markdown","source":"The second model is a linear regression trained on the TF-IDF representation.\nWe follow the same approach as the previous model","metadata":{}},{"cell_type":"code","source":"# creating the feature matrix \nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer(max_features=1000) #TFIDF\n\nX = vectorizer.fit_transform(data).toarray()\ny = dataset.iloc[:, 0]\n\n# split train and test data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-06-19T08:36:40.402618Z","iopub.execute_input":"2021-06-19T08:36:40.402961Z","iopub.status.idle":"2021-06-19T08:36:40.711452Z","shell.execute_reply.started":"2021-06-19T08:36:40.402931Z","shell.execute_reply":"2021-06-19T08:36:40.710593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"regr = linear_model.LinearRegression()\n\n# Train the model using the training sets\nregr.fit(X_train, y_train)\n\n# Make predictions using the testing set\ny_pred = regr.predict(X_test)\n\n# RMSE volue on test set\nscores = cross_val_score(regr, X_train, y_train, cv=5,scoring='neg_root_mean_squared_error')\n-(scores.mean())","metadata":{"execution":{"iopub.status.busy":"2021-06-19T08:37:00.840463Z","iopub.execute_input":"2021-06-19T08:37:00.840782Z","iopub.status.idle":"2021-06-19T08:37:03.739084Z","shell.execute_reply.started":"2021-06-19T08:37:00.840756Z","shell.execute_reply":"2021-06-19T08:37:03.738127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f' RMSE: {mean_squared_error(y_test, y_pred, squared=False):.4f}')\nfig, ax = plt.subplots(1, 1, figsize=(20, 6))\nsns.distplot(y_test, ax=ax, label='Label')\nsns.distplot(y_pred, ax=ax, label='Prediction')\nax.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-19T08:38:21.025394Z","iopub.execute_input":"2021-06-19T08:38:21.025738Z","iopub.status.idle":"2021-06-19T08:38:21.335963Z","shell.execute_reply.started":"2021-06-19T08:38:21.025706Z","shell.execute_reply":"2021-06-19T08:38:21.335196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Word2Vec\n","metadata":{}},{"cell_type":"markdown","source":"The final linear regression presented is based on the word2vec representation.\n\nFirst of all, we load the pretrained word embedding model.","metadata":{}},{"cell_type":"code","source":"word2vecModel = gensim.models.KeyedVectors.load_word2vec_format(\"/kaggle/input/google-pretrain-model/GoogleNews-vectors-negative300.bin.gz\", binary=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-19T08:56:31.264162Z","iopub.execute_input":"2021-06-19T08:56:31.264684Z","iopub.status.idle":"2021-06-19T08:57:55.44756Z","shell.execute_reply.started":"2021-06-19T08:56:31.264617Z","shell.execute_reply":"2021-06-19T08:57:55.446682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then we create a function to compute the vector that will represent each excerpt.","metadata":{}},{"cell_type":"code","source":"def avg_feature_vector(sentence, model, num_features):\n    words = sentence.replace('\\n',\" \").replace(',',' ').replace('.',\" \").split()\n    feature_vec = np.zeros((num_features,),dtype=\"float32\")\n    i=0\n    for word in words:\n        try:\n            feature_vec = np.add(feature_vec, model[word])\n        except KeyError as error:\n            feature_vec \n            i = i + 1\n    if len(words) > 0:\n        feature_vec = np.divide(feature_vec, len(words)- i)\n    return feature_vec","metadata":{"execution":{"iopub.status.busy":"2021-06-19T08:44:34.680145Z","iopub.execute_input":"2021-06-19T08:44:34.680477Z","iopub.status.idle":"2021-06-19T08:44:34.687315Z","shell.execute_reply.started":"2021-06-19T08:44:34.680451Z","shell.execute_reply":"2021-06-19T08:44:34.686368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then we compute the vectorization on the training data.","metadata":{}},{"cell_type":"code","source":"word2vec_train = np.zeros((len(dataset.index),300),dtype=\"float32\")\n\nfor i in range(len(dataset.index)):\n    word2vec_train[i] = avg_feature_vector(dataset[\"excerpt\"][i],word2vecModel, 300)","metadata":{"execution":{"iopub.status.busy":"2021-06-19T08:57:55.449016Z","iopub.execute_input":"2021-06-19T08:57:55.449367Z","iopub.status.idle":"2021-06-19T08:57:57.193551Z","shell.execute_reply.started":"2021-06-19T08:57:55.449322Z","shell.execute_reply":"2021-06-19T08:57:57.192755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We proceed with the classical train/test split.","metadata":{}},{"cell_type":"code","source":"# creating the feature matrix \n\nX = word2vec_train\ny = dataset.iloc[:, 0]\n\n# split train and test data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-06-19T09:05:55.234568Z","iopub.execute_input":"2021-06-19T09:05:55.234954Z","iopub.status.idle":"2021-06-19T09:05:55.244253Z","shell.execute_reply.started":"2021-06-19T09:05:55.234922Z","shell.execute_reply":"2021-06-19T09:05:55.243266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally we compute the last linear regression on the word2vec data.","metadata":{}},{"cell_type":"code","source":"regr = linear_model.LinearRegression()\n\n# Train the model using the training sets\nregr.fit(X_train, y_train)\n\n# Make predictions using the testing set\ny_pred = regr.predict(X_test)\n\n# RMSE volue on test set\nscores = cross_val_score(regr, X_train, y_train, cv=5,scoring='neg_root_mean_squared_error')\n-(scores.mean())","metadata":{"execution":{"iopub.status.busy":"2021-06-19T09:07:00.897003Z","iopub.execute_input":"2021-06-19T09:07:00.897446Z","iopub.status.idle":"2021-06-19T09:07:01.699233Z","shell.execute_reply.started":"2021-06-19T09:07:00.897397Z","shell.execute_reply":"2021-06-19T09:07:01.698077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f' RMSE: {mean_squared_error(y_test, y_pred, squared=False):.4f}')\nfig, ax = plt.subplots(1, 1, figsize=(20, 6))\nsns.distplot(y_test, ax=ax, label='Label')\nsns.distplot(y_pred, ax=ax, label='Prediction')\nax.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-19T09:07:04.323629Z","iopub.execute_input":"2021-06-19T09:07:04.323981Z","iopub.status.idle":"2021-06-19T09:07:04.648551Z","shell.execute_reply.started":"2021-06-19T09:07:04.323951Z","shell.execute_reply":"2021-06-19T09:07:04.647885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It's clear that the word2vec representation of the data is superior compared to the Bag of words and the TF-IDF. Therefore, that would be the representation of choice for the next models.\n","metadata":{}},{"cell_type":"markdown","source":"## 4. Decision tree & ensamble\n","metadata":{}},{"cell_type":"markdown","source":"After having decided that Word2Vec is the best technique to pre-process the data we start going further with the linear regression.\nIn this section we will evaluate the tree-based models.","metadata":{}},{"cell_type":"markdown","source":"### Regression Tree\n","metadata":{}},{"cell_type":"markdown","source":"Training and evaluation of a regression tree.","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor \n  \n# create a regressor object\nTree = DecisionTreeRegressor(random_state = 42) \n  \n# Train the model using the training sets\nTree.fit(X_train, y_train)\n\n# Make predictions using the testing set\ny_predT = Tree.predict(X_test)\n\nscores = cross_val_score(Tree, X_train, y_train, cv=5,scoring='neg_root_mean_squared_error')\n-(scores.mean())","metadata":{"execution":{"iopub.status.busy":"2021-06-19T09:13:21.681171Z","iopub.execute_input":"2021-06-19T09:13:21.681491Z","iopub.status.idle":"2021-06-19T09:13:24.331412Z","shell.execute_reply.started":"2021-06-19T09:13:21.681465Z","shell.execute_reply":"2021-06-19T09:13:24.330701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f' RMSE: {mean_squared_error(y_test, y_pred, squared=False):.4f}')\nfig, ax = plt.subplots(1, 1, figsize=(20, 6))\nsns.distplot(y_test, ax=ax, label='Label')\nsns.distplot(y_predT, ax=ax, label='Prediction')\nax.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-19T09:15:08.447574Z","iopub.execute_input":"2021-06-19T09:15:08.447977Z","iopub.status.idle":"2021-06-19T09:15:08.725962Z","shell.execute_reply.started":"2021-06-19T09:15:08.447945Z","shell.execute_reply":"2021-06-19T09:15:08.725139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Regression Forest\n","metadata":{}},{"cell_type":"markdown","source":"Training and evaluation of a regression forest.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor \n  \n# create a regressor object\nForest = RandomForestRegressor(random_state = 42) \n  \n# Train the model using the training sets\nForest.fit(X_train, y_train)\n\n# Make predictions using the testing set\ny_predF = Forest.predict(X_test)\n\nscores = cross_val_score(Forest, X_train, y_train, cv=5,scoring='neg_root_mean_squared_error')\n-(scores.mean())","metadata":{"execution":{"iopub.status.busy":"2021-06-19T09:20:15.3355Z","iopub.execute_input":"2021-06-19T09:20:15.335861Z","iopub.status.idle":"2021-06-19T09:22:48.177136Z","shell.execute_reply.started":"2021-06-19T09:20:15.335828Z","shell.execute_reply":"2021-06-19T09:22:48.176264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f' RMSE: {mean_squared_error(y_test, y_predF, squared=False):.4f}')\n\nfig, ax = plt.subplots(1, 1, figsize=(20, 6))\nsns.distplot(y_test, ax=ax, label='Label')\nsns.distplot(y_predF, ax=ax, label='Prediction')\nax.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-19T09:22:48.178289Z","iopub.execute_input":"2021-06-19T09:22:48.178547Z","iopub.status.idle":"2021-06-19T09:22:48.44978Z","shell.execute_reply.started":"2021-06-19T09:22:48.178523Z","shell.execute_reply":"2021-06-19T09:22:48.44886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Boosted Tree\n","metadata":{}},{"cell_type":"markdown","source":"Training and evaluation of a boosted tree.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor \n  \n# create a regressor object\nBoost = GradientBoostingRegressor(random_state = 42) \n  \n# Train the model using the training sets\nBoost.fit(X_train, y_train)\n\n# Make predictions using the testing set\ny_predB = Forest.predict(X_test)\n\nscores = cross_val_score(Boost, X_train, y_train, cv=5,scoring='neg_root_mean_squared_error')\n-(scores.mean())","metadata":{"execution":{"iopub.status.busy":"2021-06-19T09:22:48.451239Z","iopub.execute_input":"2021-06-19T09:22:48.451498Z","iopub.status.idle":"2021-06-19T09:24:11.788622Z","shell.execute_reply.started":"2021-06-19T09:22:48.451474Z","shell.execute_reply":"2021-06-19T09:24:11.787677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Error Distribution\nprint(f' RMSE: {mean_squared_error(y_test, y_predB, squared=False):.4f}')\n\nsns.scatterplot(\n    x=y_test, y= y_predB,\n    palette=sns.color_palette(\"hls\", 10),\n    legend=\"full\")","metadata":{"execution":{"iopub.status.busy":"2021-06-19T09:24:11.790001Z","iopub.execute_input":"2021-06-19T09:24:11.790268Z","iopub.status.idle":"2021-06-19T09:24:11.9249Z","shell.execute_reply.started":"2021-06-19T09:24:11.790243Z","shell.execute_reply":"2021-06-19T09:24:11.924008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(20, 6))\nsns.distplot(y_test, ax=ax, label='Label')\nsns.distplot(y_predB, ax=ax, label='Prediction')\nax.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-19T09:24:11.926026Z","iopub.execute_input":"2021-06-19T09:24:11.926293Z","iopub.status.idle":"2021-06-19T09:24:12.211296Z","shell.execute_reply.started":"2021-06-19T09:24:11.926266Z","shell.execute_reply":"2021-06-19T09:24:12.210595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. Support Vector machine","metadata":{}},{"cell_type":"markdown","source":"Training and evaluation of a support vector machine.","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVR\n  \n# create a regressor object\nSupport = SVR(kernel = 'rbf') \n  \n# Train the model using the training sets\nSupport.fit(X_train, y_train)\n\n# Make predictions using the testing set\ny_predS = Support.predict(X_test)\n\nscores = cross_val_score(Support, X_train, y_train, cv=5,scoring='neg_root_mean_squared_error')\n-(scores.mean())","metadata":{"execution":{"iopub.status.busy":"2021-06-19T09:27:56.616432Z","iopub.execute_input":"2021-06-19T09:27:56.616777Z","iopub.status.idle":"2021-06-19T09:28:01.234031Z","shell.execute_reply.started":"2021-06-19T09:27:56.616749Z","shell.execute_reply":"2021-06-19T09:28:01.233058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f' RMSE: {mean_squared_error(y_test, y_predS, squared=False):.4f}')\n\nfig, ax = plt.subplots(1, 1, figsize=(20, 6))\nsns.distplot(y_test, ax=ax, label='Label')\nsns.distplot(y_predS, ax=ax, label='Prediction')\nax.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-19T09:28:18.028033Z","iopub.execute_input":"2021-06-19T09:28:18.028387Z","iopub.status.idle":"2021-06-19T09:28:18.326926Z","shell.execute_reply.started":"2021-06-19T09:28:18.028357Z","shell.execute_reply":"2021-06-19T09:28:18.325764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The support vector machine seems to be the model that perform better in this task.\n\nSo we are going to optimize the hyperparameter using the grid search technique.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n  \n#defining parameter range\nparam_grid = {'C': [0.1, 1, 10, 100, 1000], \n              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n              'kernel': ['rbf']} \n  \ngrid = GridSearchCV(SVR(), param_grid, refit = True, verbose = 0)\n  \n#fitting the model for grid search\ngrid.fit(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-06-19T09:33:42.852401Z","iopub.execute_input":"2021-06-19T09:33:42.852766Z","iopub.status.idle":"2021-06-19T09:33:45.041408Z","shell.execute_reply.started":"2021-06-19T09:33:42.852731Z","shell.execute_reply":"2021-06-19T09:33:45.039284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print best parameter after tuning\nprint(grid.best_params_)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-19T09:33:28.843141Z","iopub.execute_input":"2021-06-19T09:33:28.843501Z","iopub.status.idle":"2021-06-19T09:33:28.847769Z","shell.execute_reply.started":"2021-06-19T09:33:28.843467Z","shell.execute_reply":"2021-06-19T09:33:28.84683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"SVM with best parameter found by the grid search and to further optimize the model we train it with cross validation with five fold.","metadata":{}},{"cell_type":"code","source":"#KFold ã€€n_splits=5\nfrom sklearn.model_selection import KFold\ny_train_num=dataset.target.to_numpy()\n\nfold = KFold(n_splits=5, shuffle=True, random_state=42)\ncv=list(fold.split(word2vec_train, y_train_num))","metadata":{"execution":{"iopub.status.busy":"2021-06-19T10:32:29.734123Z","iopub.execute_input":"2021-06-19T10:32:29.734478Z","iopub.status.idle":"2021-06-19T10:32:29.774766Z","shell.execute_reply.started":"2021-06-19T10:32:29.734446Z","shell.execute_reply":"2021-06-19T10:32:29.773363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rmses = []\nfor tr_idx, val_idx in cv: \n    x_tr, x_va = word2vec_train[tr_idx], word2vec_train[val_idx]\n    y_tr, y_va = y_train_num[tr_idx], y_train_num[val_idx]\n        \n    # Training\n    model = SVR(kernel = 'rbf',gamma=1,C=1) \n    model.fit(x_tr, y_tr)    \n    y_pred = model.predict(x_va)\n    rmse =  np.sqrt(mean_squared_error(y_va, y_pred))\n    rmses.append(rmse)\n    \n    \n    fig, ax = plt.subplots(1, 1, figsize=(20, 6))\n    sns.distplot(y_va, ax=ax, label='Label')\n    sns.distplot(y_pred, ax=ax, label='Prediction')\n    ax.legend()\n    plt.show()\n        \nprint(\"\\n\", \"Mean Fold RMSE:\", np.mean(rmses))    ","metadata":{"execution":{"iopub.status.busy":"2021-06-19T09:46:07.524529Z","iopub.execute_input":"2021-06-19T09:46:07.524903Z","iopub.status.idle":"2021-06-19T09:46:14.0135Z","shell.execute_reply.started":"2021-06-19T09:46:07.524871Z","shell.execute_reply":"2021-06-19T09:46:14.012895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"SVM with best parameter found by the grid search and to further optimize the model we train it with cross validation with five fold.","metadata":{}},{"cell_type":"markdown","source":"## 5. Transfomer\n","metadata":{}},{"cell_type":"markdown","source":"After training the algorithm on our current dataset we push further with new models based on transfer learning.\nIn this case we will use the \"RoBERTa-base\" model, which is a smaller implementation of  [Roberta](https://arxiv.org/abs/1907.11692).","metadata":{}},{"cell_type":"markdown","source":"#### Import libraries from transformes and define useful function for the training phase.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras import optimizers, losses, metrics, Model\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\nfrom transformers import TFAutoModelForSequenceClassification, TFAutoModel, AutoTokenizer\nfrom transformers import RobertaTokenizer, RobertaModel\nfrom sklearn.metrics import mean_squared_error\n\n# Utility functions\ndef custom_standardization(text):\n    text = text.lower() # if encoder is uncased\n    text = text.strip()\n    return text\n\n\ndef sample_target(features, target):\n    mean, stddev = target\n    sampled_target = tf.random.normal([], mean=tf.cast(mean, dtype=tf.float32), \n                                      stddev=tf.cast(stddev, dtype=tf.float32), dtype=tf.float32)\n    \n    return (features, sampled_target)\n    \n\ndef get_dataset(pandas_df, tokenizer, labeled=True, ordered=False, repeated=False, \n                is_sampled=False, batch_size=32, seq_len=128):\n    \"\"\"\n        Return a Tensorflow dataset ready for training or inference.\n    \"\"\"\n    text = [custom_standardization(text) for text in pandas_df['excerpt']]\n    \n    # Tokenize inputs\n    tokenized_inputs = tokenizer(text, max_length=seq_len, truncation=True, \n                                 padding='max_length', return_tensors='tf')\n    \n    if labeled:\n        dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': tokenized_inputs['input_ids'], \n                                                      'attention_mask': tokenized_inputs['attention_mask']}, \n                                                      (pandas_df['target'], pandas_df['standard_error'])))\n        if is_sampled:\n            dataset = dataset.map(sample_target, num_parallel_calls=tf.data.AUTOTUNE)\n    else:\n        dataset = tf.data.Dataset.from_tensor_slices({'input_ids': tokenized_inputs['input_ids'], \n                                                      'attention_mask': tokenized_inputs['attention_mask']})\n        \n    if repeated:\n        dataset = dataset.repeat()\n    if not ordered:\n        dataset = dataset.shuffle(1024)\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n    \n    return dataset\n\n\ndef plot_metrics(history):\n    metric_list = list(history.keys())\n    size = len(metric_list)//2\n    fig, axes = plt.subplots(size, 1, sharex='col', figsize=(20, size * 5))\n    axes = axes.flatten()\n    \n    for index in range(len(metric_list)//2):\n        metric_name = metric_list[index]\n        val_metric_name = metric_list[index+size]\n        axes[index].plot(history[metric_name], label='Train %s' % metric_name)\n        axes[index].plot(history[val_metric_name], label='Validation %s' % metric_name)\n        axes[index].legend(loc='best', fontsize=16)\n        axes[index].set_title(metric_name)\n\n    plt.xlabel('Epochs', fontsize=16)\n    sns.despine()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-19T10:32:46.697129Z","iopub.execute_input":"2021-06-19T10:32:46.697518Z","iopub.status.idle":"2021-06-19T10:32:46.713776Z","shell.execute_reply.started":"2021-06-19T10:32:46.697485Z","shell.execute_reply":"2021-06-19T10:32:46.712655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Learning strategy","metadata":{}},{"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print(f'Running on TPU {tpu.master()}')\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n\nAUTO = tf.data.experimental.AUTOTUNE\nREPLICAS = strategy.num_replicas_in_sync","metadata":{"execution":{"iopub.status.busy":"2021-06-19T10:30:42.964943Z","iopub.execute_input":"2021-06-19T10:30:42.965272Z","iopub.status.idle":"2021-06-19T10:30:42.979224Z","shell.execute_reply.started":"2021-06-19T10:30:42.965237Z","shell.execute_reply":"2021-06-19T10:30:42.978423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Hyperparameters","metadata":{}},{"cell_type":"code","source":"#Ml parameter\nBATCH_SIZE = 16\nLEARNING_RATE = 1e-5\nEPOCHS = 5\nSEQ_LEN = 256 \n#Cv numbers\nN_FOLDS = 5\nBASE_MODEL = \"/kaggle/input/huggingface-roberta/roberta-base\" ","metadata":{"execution":{"iopub.status.busy":"2021-06-19T10:30:48.236868Z","iopub.execute_input":"2021-06-19T10:30:48.237214Z","iopub.status.idle":"2021-06-19T10:30:48.242576Z","shell.execute_reply.started":"2021-06-19T10:30:48.237185Z","shell.execute_reply":"2021-06-19T10:30:48.241614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Model","metadata":{}},{"cell_type":"code","source":"def model_fn(encoder, seq_len=256):\n    input_ids = L.Input(shape=(seq_len,), dtype=tf.int32, name='input_ids')\n    input_attention_mask = L.Input(shape=(seq_len,), dtype=tf.int32, name='attention_mask')\n    \n    outputs = encoder({'input_ids': input_ids, \n                       'attention_mask': input_attention_mask})\n    \n    model = Model(inputs=[input_ids, input_attention_mask], outputs=outputs)\n\n    optimizer = optimizers.Adam(lr=LEARNING_RATE)\n    model.compile(optimizer=optimizer, \n                  loss=losses.MeanSquaredError(), \n                  metrics=[metrics.RootMeanSquaredError()])\n    \n    return model\n\n\nwith strategy.scope():\n    encoder = TFAutoModelForSequenceClassification.from_pretrained(BASE_MODEL, num_labels=1)\n    model = model_fn(encoder, SEQ_LEN)\n    \nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-06-19T10:30:50.085234Z","iopub.execute_input":"2021-06-19T10:30:50.085593Z","iopub.status.idle":"2021-06-19T10:31:09.253625Z","shell.execute_reply.started":"2021-06-19T10:30:50.085563Z","shell.execute_reply":"2021-06-19T10:31:09.252639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Training","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\nskf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\noof_pred = []; oof_labels = []; history_list = []; test_pred = []\ntrain=pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/train.csv\")\ntest=pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/test.csv\")\ntrain.drop(['url_legal', 'license'], axis=1, inplace=True)\ntest.drop(['url_legal', 'license'], axis=1, inplace=True)\n\nfor fold,(idxT, idxV) in enumerate(skf.split(train)):\n    if tpu: tf.tpu.experimental.initialize_tpu_system(tpu)\n    print(f'\\nFOLD: {fold+1}')\n    print(f'TRAIN: {len(idxT)} VALID: {len(idxV)}')\n\n    # Model\n    K.clear_session()\n    with strategy.scope():\n        encoder = TFAutoModelForSequenceClassification.from_pretrained(BASE_MODEL, num_labels=1,hidden_dropout_prob=0.1)\n        model = model_fn(encoder, SEQ_LEN)\n        \n    model_path = f'model_{fold}.h5'\n    es = EarlyStopping(monitor='val_root_mean_squared_error', mode='min', \n                       patience=2, restore_best_weights=True, verbose=1)\n    checkpoint = ModelCheckpoint(model_path, monitor='val_root_mean_squared_error', mode='min', \n                                 save_best_only=True, save_weights_only=True)\n\n    # Train\n    history = model.fit(x=get_dataset(train.loc[idxT], tokenizer, repeated=True, is_sampled=True, \n                                      batch_size=BATCH_SIZE, seq_len=SEQ_LEN), \n                        validation_data=get_dataset(train.loc[idxV], tokenizer, ordered=True, \n                                                    batch_size=BATCH_SIZE, seq_len=SEQ_LEN), \n                        steps_per_epoch=100, \n                        callbacks=[es, checkpoint], \n                        epochs=EPOCHS,  \n                        verbose=2).history\n      \n    history_list.append(history)\n    # Save last model weights\n    model.load_weights(model_path)\n    \n    # Results\n    print(f\"#### FOLD {fold+1} OOF RMSE = {np.min(history['val_root_mean_squared_error']):.4f}\")\n\n    # OOF predictions\n    valid_ds = get_dataset(train.loc[idxV], tokenizer, ordered=True, batch_size=BATCH_SIZE, seq_len=SEQ_LEN)\n    oof_labels.append([target[0].numpy() for sample, target in iter(valid_ds.unbatch())])\n    x_oof = valid_ds.map(lambda sample, target: sample)\n    oof_pred.append(model.predict(x_oof)['logits'])\n\n    # Test predictions\n    test_ds = get_dataset(test, tokenizer, labeled=False, ordered=True, batch_size=BATCH_SIZE, seq_len=SEQ_LEN)\n    x_test = test_ds.map(lambda sample: sample)\n    test_pred.append(model.predict(x_test)['logits'])","metadata":{"execution":{"iopub.status.busy":"2021-06-19T10:32:57.106303Z","iopub.execute_input":"2021-06-19T10:32:57.106675Z","iopub.status.idle":"2021-06-19T11:04:22.790222Z","shell.execute_reply.started":"2021-06-19T10:32:57.106643Z","shell.execute_reply":"2021-06-19T11:04:22.789365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Training performance","metadata":{}},{"cell_type":"code","source":"y_true = np.concatenate(oof_labels)\ny_preds = np.concatenate(oof_pred)\n\n\nfor fold, history in enumerate(history_list):\n    print(f\"FOLD {fold+1} RMSE: {np.min(history['val_root_mean_squared_error']):.4f}\")\n    \nprint(f'OOF RMSE: {mean_squared_error(y_true, y_preds, squared=False):.4f}')","metadata":{"execution":{"iopub.status.busy":"2021-06-19T11:04:22.791823Z","iopub.execute_input":"2021-06-19T11:04:22.79215Z","iopub.status.idle":"2021-06-19T11:04:22.80129Z","shell.execute_reply.started":"2021-06-19T11:04:22.792115Z","shell.execute_reply":"2021-06-19T11:04:22.800249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for fold, history in enumerate(history_list):\n    print(f'\\nFOLD: {fold+1}')\n    plot_metrics(history)","metadata":{"execution":{"iopub.status.busy":"2021-06-19T11:04:22.803345Z","iopub.execute_input":"2021-06-19T11:04:22.803781Z","iopub.status.idle":"2021-06-19T11:04:24.90353Z","shell.execute_reply.started":"2021-06-19T11:04:22.803744Z","shell.execute_reply":"2021-06-19T11:04:24.902084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\n\nThe performance of the transformer is superior to the Support vector machine so we are going to push that model to the challenge submission.\nObviously there is room for further improvement by fine tuning the transformer model with techniques such as:\n\n1. Testing different hyperparameters \n2. Gradient Clipping\n3. Differential Learning Rate\n4. Scaling up the the full Roberta model\n5. Ensamble learning on multiples istances of the model","metadata":{}},{"cell_type":"markdown","source":"#### Submission","metadata":{}},{"cell_type":"code","source":"submission = test[['id']]\nsubmission['target'] = np.mean(test_pred, axis=0)\nsubmission.to_csv('./submission.csv', index=False)\ndisplay(submission.head(10))","metadata":{"execution":{"iopub.status.busy":"2021-06-19T11:04:47.684135Z","iopub.execute_input":"2021-06-19T11:04:47.684469Z","iopub.status.idle":"2021-06-19T11:04:47.977569Z","shell.execute_reply.started":"2021-06-19T11:04:47.684438Z","shell.execute_reply":"2021-06-19T11:04:47.976798Z"},"trusted":true},"execution_count":null,"outputs":[]}]}