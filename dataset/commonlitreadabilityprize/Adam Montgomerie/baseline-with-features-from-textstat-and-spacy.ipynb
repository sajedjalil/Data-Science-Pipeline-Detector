{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Textstat\n\n[Textstat](https://pypi.org/project/textstat/) has a bunch of metrics for calculating text complexity like flesch-kincaid, gunning-fog, etc.","metadata":{}},{"cell_type":"code","source":"!pip install ../input/textstat/Pyphen-0.10.0-py3-none-any.whl\n!pip install ../input/textstat/textstat-0.7.0-py3-none-any.whl","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from textstat import textstat\nimport re\nimport os\nimport en_core_web_sm\nimport numpy as np\nimport pandas as pd\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nfrom sklearn.linear_model import LinearRegression, Ridge, ElasticNet\n\nRANDOM_SEED = 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Generation\n\nThe features below are mostly copied from a project I did before trying to predict the CEFR difficulty level of some texts. You can find the full code repo [here](https://github.com/AMontgomerie/CEFR-English-Level-Predictor).","metadata":{}},{"cell_type":"code","source":"nlp = en_core_web_sm.load()\nSTOPWORDS = stopwords.words(\"english\")\nPUNCTUATION = list(string.punctuation)\nPOS_TAGS = [\"ADJ\",\"ADP\",\"ADV\",\"AUX\",\"CONJ\",\"CCONJ\",\"DET\",\"INTJ\",\"NOUN\",\"NUM\",\"PART\",\"PRON\",\"PROPN\",\"PUNCT\",\"SCONJ\",\"SYM\",\"VERB\",\"X\",\"SPACE\"]\n\n\ndef generate_features(data):\n    feature_data = []\n\n    for text in data:\n        features = preprocess_text(text)\n        feature_data.append(features)\n\n    return pd.DataFrame(feature_data)\n\n\ndef preprocess_text(text):\n    simplified_text = simplify_punctuation(text)\n\n    features = {\n        \"flesch_reading_ease\": textstat.flesch_reading_ease(simplified_text),\n        \"smog_index\": textstat.smog_index(simplified_text),\n        \"flesch_kincaid_grade\": textstat.flesch_kincaid_grade(simplified_text),\n        \"coleman_liau_index\": textstat.coleman_liau_index(simplified_text),\n        \"automated_readability_index\": textstat.automated_readability_index(\n            simplified_text\n        ),\n        \"dale_chall_readability_score\": textstat.dale_chall_readability_score(\n            simplified_text\n        ),\n        \"difficult_words\": textstat.difficult_words(simplified_text),\n        \"linsear_write_formula\": textstat.linsear_write_formula(simplified_text),\n        \"gunning_fog\": textstat.gunning_fog(simplified_text),\n        \"text_standard\": textstat.text_standard(simplified_text, float_output=True),\n        \"mean_parse_tree_depth\": get_mean_parse_tree_depth(text),\n        \"mean_ents_per_sentence\": get_mean_ents_per_sentence(text),\n        \"total_ents\": get_total_ents(text),\n        \"total_chars\": get_num_chars(text),\n        \"total_words\": get_num_words(text),\n        \"chars_per_word\": get_mean_chars_per_word(text),\n        \"total_sentences\": get_num_sentences(text),\n        \"words_per_sentence\": get_mean_words_per_sentence(text),\n        \"nonstop_word_count\": get_mean_nonstop_word_count(text),\n        \"nonstop_char_count\": get_mean_nonstop_char_length(text),\n        \"nonstop_token_proportion\": get_nonstop_proportion(text),\n    }\n\n    features.update(get_mean_pos_tags(text))\n\n    return features\n\n\ndef simplify_punctuation(text):\n    # from https://github.com/shivam5992/textstat/issues/77\n\n    text = re.sub(r\"[,:;()\\-]\", \" \", text)  # Override commas, colons, etc to spaces/\n    text = re.sub(r\"[\\.!?]\", \".\", text)  # Change all terminators like ! and ? to \".\"\n    text = re.sub(r\"^\\s+\", \"\", text)  # Remove white space\n    text = re.sub(r\"[ ]*(\\n|\\r\\n|\\r)[ ]*\", \" \", text)  # Remove new lines\n    text = re.sub(r\"([\\.])[\\. ]+\", \".\", text)  # Change all \"..\" to \".\"\n    text = re.sub(r\"[ ]*([\\.])\", \". \", text)  # Normalize all \".\"`\n    text = re.sub(r\"\\s+\", \" \", text)  # Remove multiple spaces\n    text = re.sub(r\"\\s+$\", \"\", text)  # Remove trailing spaces\n    return text\n\n\ndef get_mean_parse_tree_depth(text):\n    sentences = text.split(\".\")\n    depths = []\n    for doc in list(nlp.pipe(sentences)):\n        depths += get_parse_tree_depths(doc)\n    return np.mean(depths)\n\n\ndef get_parse_tree_depths(doc):\n    return [get_depth(token) for token in doc]\n\n\ndef get_depth(token, depth=0):\n    depths = [get_depth(child, depth + 1) for child in token.children]\n    return max(depths) if len(depths) > 0 else depth\n\n\ndef get_mean_pos_tags(text):\n    sentences = text.split(\".\")\n    sentence_counts = make_pos_tag_count_lists(sentences)\n    num_sentences = textstat.sentence_count(text)\n    mean_pos_tags = calculate_mean_per_tag(sentence_counts, num_sentences)\n    return mean_pos_tags\n\n\ndef make_pos_tag_count_lists(sentences):\n    sentence_counts = {}\n    for doc in list(nlp.pipe(sentences)):\n        pos_counts = get_pos_tag_counts(doc)\n        for key in pos_counts:\n            if key in sentence_counts:\n                sentence_counts[key].append(pos_counts[key])\n            else:\n                sentence_counts[key] = [pos_counts[key]]\n    return sentence_counts\n\n\ndef get_pos_tag_counts(doc):\n    pos_counts = {}\n    pos_tags = [token.pos_ for token in doc]\n    for tag in pos_tags:\n        if tag in pos_counts:\n            pos_counts[tag] += 1\n        else:\n            pos_counts[tag] = 1\n    return pos_counts\n\n\ndef calculate_mean_per_tag(counts, num_sentences):\n    mean_pos_tags = {f\"mean_{tag.lower()}\": 0 for tag in POS_TAGS}\n    for key in counts:\n        if len(counts[key]) < num_sentences:\n            counts[key] += [0] * (num_sentences - len(counts[key]))\n        mean_value = round(np.mean(counts[key]), 2)\n        mean_pos_tags[\"mean_\" + key.lower()] = mean_value\n    return mean_pos_tags\n\n\ndef get_total_ents(text):\n    return len(nlp(text).doc.ents)\n\n\ndef get_mean_ents_per_sentence(text):\n    return get_total_ents(text) / textstat.sentence_count(text)\n\n\ndef get_mean_chars_per_word(text):\n    return get_num_chars(text) / get_num_words(text)\n\n\ndef get_mean_words_per_sentence(text):\n    return get_num_words(text) / get_num_sentences(text)\n\n\ndef get_mean_nonstop_char_length(text):\n    spans = tokenize_on_stopwords(text)\n    return sum([get_num_chars(span) for span in spans]) / len(spans)\n\n\ndef get_mean_nonstop_word_count(text):\n    spans = tokenize_on_stopwords(text)\n    return sum([get_num_words(span) for span in spans]) / len(spans)\n\n\ndef get_nonstop_proportion(text):\n    tokens = nltk.word_tokenize(text)\n    nonstop_tokens = [token for token in tokens if token not in STOPWORDS + PUNCTUATION]\n    return len(nonstop_tokens) / len(tokens)\n\n\ndef tokenize_on_stopwords(text):\n    tokens = nltk.word_tokenize(text)\n    spans = []\n    current_span = []\n    for token in tokens:\n        if token not in STOPWORDS + PUNCTUATION:\n            current_span.append(token)\n        else:\n            if len(current_span) > 0:\n                spans.append(\" \".join(current_span))\n            current_span = []\n    return spans\n\n\ndef get_num_chars(text):\n    return len(text)\n\n\ndef get_num_words(text):\n    return len(text.split())\n\n\ndef get_num_sentences(text):\n    total = text.count(\".\") + text.count(\"?\") + text.count(\"!\")\n    if total == 0:\n        return 1\n    else:\n        return total","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"../input/commonlitreadabilityprize/train.csv\")\ntrain","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_features = generate_features(train.excerpt.to_list())\ntrain_features[\"target\"] = train[\"target\"]\ntrain_features","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cross Validation\n\nWe're trying out XGBoost, Ridge Regression, Linear Regression, and Elastic Net on 5-fold CV with the features we just generated. All models are trained with default parameters.","metadata":{}},{"cell_type":"code","source":"def cross_validate(model_type):\n    scores = []\n    kf = KFold(random_state=RANDOM_SEED, shuffle=True)\n    \n    for i, (train_index, eval_index) in enumerate(kf.split(train_features)):\n        fold_train = train_features.loc[train_index]\n        fold_eval = train_features.loc[eval_index]\n        y_train = fold_train.pop(\"target\")\n        y_eval = fold_eval.pop(\"target\")\n        fold_train = np.ascontiguousarray(fold_train.to_numpy())\n        fold_eval = np.ascontiguousarray(fold_eval.to_numpy())\n        model = model_type()\n        model.fit(fold_train, y_train)\n        preds = model.predict(fold_eval)\n        rmse = mean_squared_error(y_eval, preds, squared=False)\n        scores.append(rmse)\n        print(f\"Fold: {i+1} | RMSE: {rmse}\")\n    \n    print(f\"CV: {sum(scores)/len(scores)}\\n\")\n\n\nprint(\"XGBRegressor:\")\ncross_validate(XGBRegressor)\n\nprint(\"Ridge:\")\ncross_validate(Ridge)\n\nprint(\"Linear Regression:\")\ncross_validate(LinearRegression)\n\nprint(\"Elastic Net:\")\ncross_validate(ElasticNet)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The best model seemes to be Ridge, so let's go with that.","metadata":{}},{"cell_type":"code","source":"target = train_features.pop(\"target\")\nmodel = Ridge()\nmodel.fit(train_features, target)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test Set Prediction\n\n","metadata":{}},{"cell_type":"code","source":"test = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")\ntest_features = generate_features(test.excerpt.to_list())\npreds = model.predict(test_features)\nsubmission = pd.DataFrame({\"id\": test.id, \"target\": preds})\nsubmission.to_csv(\"submission.csv\", index=False)\nsubmission","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}