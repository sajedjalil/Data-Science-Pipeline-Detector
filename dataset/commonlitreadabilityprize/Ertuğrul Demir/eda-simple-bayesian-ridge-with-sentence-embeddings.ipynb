{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Starter Code & EDA, Roberta Sentence Embeddings with Bayesian Model\n\n### Hello all! In this notebook we're going to examine the data we're given, gonna do some exploratory data analysis and visualizations, aiming to get some insights and find some patterns about the data.\n\n### In second part we're going to implement some simple baseline model to make a starting point for the competition.\n\n### Well that's enough of me talking, let's get started:","metadata":{}},{"cell_type":"markdown","source":"# Loading Libraries\n\n### Here we load some packages which we're going to need them in future cells. ","metadata":{}},{"cell_type":"code","source":"# adding wordcloud in offline mode\n\nimport sys\nsys.path.append('../input/wcloud/word_cloud-master')\nsys.path.append('../input/sentence-transformers/sentence-transformers-master')\nimport wordcloud","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loading libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom scipy import stats\nfrom scipy.stats import norm\n\nimport matplotlib.gridspec as gridspec\nfrom matplotlib.ticker import MaxNLocator\n\n# setting some globl config\n\nplt.style.use('ggplot')\norange_black = [\n    '#fdc029', '#df861d', '#FF6347', '#aa3d01', '#a30e15', '#800000', '#171820'\n]\nplt.rcParams['figure.figsize'] = (16,9)\nplt.rcParams[\"figure.facecolor\"] = '#FFFACD'\nplt.rcParams[\"axes.facecolor\"] = '#FFFFE0'\nplt.rcParams[\"axes.grid\"] = True\nplt.rcParams[\"grid.color\"] = orange_black[3]\nplt.rcParams[\"grid.alpha\"] = 0.5\nplt.rcParams[\"grid.linestyle\"] = '--'\n\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading Data\n\n### We load our train and test data. We do have small dataset with few amount of features. Most important one seems to be excerpt which is the feature that covers most of the useful data. Since this is \"Code Competition\" our test set seems to be small but it's going to load actual test set when we commit the notebook.","metadata":{}},{"cell_type":"code","source":"# loding train and test data\n\ntrain_df = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\ntest_df = pd.read_csv('../input/commonlitreadabilityprize/test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking dataframes and their shape\n\nprint('\\033[1m'+'TRAIN COLUMNS:')\ndisplay(train_df.columns)\nprint('\\033[1m''\\033[1m''TEST COLUMNS:')\ndisplay(test_df.columns)\nprint('\\033[1m'+'TRAIN SHAPE:')\ndisplay(train_df.shape)\nprint('\\033[1m'+'TEST SHAPE:')\ndisplay(test_df.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Target Distribution\n\n### Let's start with what kind of a problem we have given and what metrics we can use. As you can see below we have continous target labels so we have \"Regression\" problem. When we check the distribution of targets we do see kinda normal distribution, which is good.","metadata":{}},{"cell_type":"code","source":"def plot_dist3(df, feature, title):\n    \n    # Creating a customized chart. and giving in figsize and everything.\n    \n    fig = plt.figure(constrained_layout=True)\n    \n    # creating a grid of 3 cols and 3 rows.\n    \n    grid = gridspec.GridSpec(ncols=3, nrows=2, figure=fig)\n\n    # Customizing the histogram grid.\n    \n    ax1 = fig.add_subplot(grid[0, :2])\n    \n    # Set the title.\n    \n    ax1.set_title('Histogram')\n    \n    # plot the histogram.\n    \n    sns.distplot(df.loc[:, feature],\n                 hist=True,\n                 kde=True,\n                 fit=norm,\n                  hist_kws={\n                 'rwidth': 0.85,\n                 'edgecolor': 'black',\n                 'alpha': 0.8},\n                 ax=ax1,\n                 color=orange_black[1])\n    \n    ax1.axvline(df.loc[:, feature].mean(), color='Green', linestyle='dashed', linewidth=3)\n\n    min_ylim, max_ylim = plt.ylim()\n    ax1.text(df.loc[:, feature].mean()*1.95, max_ylim*0.95, 'Mean: {:.2f}'.format(df.loc[:, feature].mean()), color='Green', fontsize='12',\n             bbox=dict(boxstyle='round',facecolor='red', alpha=0.5))\n    ax1.legend(labels=['Actual','Normal'])\n    ax1.xaxis.set_major_locator(MaxNLocator(nbins=24))\n\n    # customizing the QQ_plot.\n    \n    ax2 = fig.add_subplot(grid[1, :2])\n    \n    # Set the title.\n    \n    ax2.set_title('Probability Plot')\n    \n    # Plotting the QQ_Plot.\n    stats.probplot(df.loc[:, feature].fillna(np.mean(df.loc[:, feature])),\n                   plot=ax2)\n    ax2.get_lines()[0].set_markerfacecolor('#e74c3c')\n    ax2.get_lines()[0].set_markersize(12.0)\n    ax2.xaxis.set_major_locator(MaxNLocator(nbins=24))\n\n    # Customizing the Box Plot:\n    \n    ax3 = fig.add_subplot(grid[:, 2])\n    # Set title.\n    \n    ax3.set_title('Box Plot')\n    \n    # Plotting the box plot.\n    \n    sns.boxplot(y=feature, data=df, ax=ax3, color=orange_black[2])\n    ax3.yaxis.set_major_locator(MaxNLocator(nbins=24))\n\n    plt.suptitle(f'{title}', fontsize=24)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_dist3(train_df, 'target', 'Readability Score Distribution')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking df again\ntrain_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Missing Values\n\n### We have many missing values for url's and licences features therefore they're useless for now and I think dropping them would be good choice.","metadata":{}},{"cell_type":"code","source":"# loading missingno package and plotting nan values\n\nimport missingno as msno\n\nmsno.bar(train_df, color=(orange_black[1]))\n\nplt.title('Missing Values', fontsize=24)\n\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dropping some columns\n\ntrain_df=train_df[['id','excerpt','target','standard_error']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Meta Features\n\n### These simple features extracted directly from strings themselves. We can't be sure if they mean anything at all unless we examine them so let's take a look:","metadata":{}},{"cell_type":"markdown","source":"## Character Counts\n\n### For the distribution part we have nicely spread data points. With the median character count around 970...","metadata":{}},{"cell_type":"code","source":"# Creating a new feature for the visualization.\n\ntrain_df['Character Count'] = train_df['excerpt'].apply(lambda x: len(str(x)))\nfrom matplotlib.ticker import MaxNLocator\nimport matplotlib.gridspec as gridspec\nimport matplotlib.patches as mpatches\n\ndef plot_dist4(df, feature, title, color):\n    # Creating a customized chart. and giving in figsize and everything.\n    fig = plt.figure(constrained_layout=True, figsize=(18, 8))\n    # Creating a grid of 3 cols and 3 rows.\n    grid = gridspec.GridSpec(ncols=3, nrows=2, figure=fig)\n\n    # Customizing the histogram grid.\n    ax1 = fig.add_subplot(grid[0, :2])\n    # Set the title.\n    ax1.set_title('Histogram')\n    # plot the histogram.\n    sns.histplot(df.loc[:, feature],\n                 ax=ax1,\n                 color=color)\n    ax1.set_ylim([0, 275])\n    ax1.axvline(df.loc[:, feature].mean(), color=orange_black[1], linestyle='dashed', linewidth=3)\n\n    min_ylim, max_ylim = plt.ylim()\n    ax1.text(df.loc[:, feature].mean()*0.9, max_ylim*0.9, 'Mean: {:.2f}'.format(df.loc[:, feature].mean()), color='Green', fontsize='12',\n             bbox=dict(boxstyle='round',facecolor=orange_black[1]))\n    \n    ax1.set(ylabel='Frequency')\n    ax1.xaxis.set_major_locator(MaxNLocator(nbins=24))\n\n    # Customizing the ecdf_plot.\n    ax2 = fig.add_subplot(grid[1, :2])\n    # Set the title.\n    ax2.set_title('Empirical CDF')\n    # Plotting the ecdf_Plot.\n    sns.histplot(df.loc[:, feature],\n                 ax=ax2,\n                 cumulative=True,\n                 color=orange_black[4])\n    ax2.xaxis.set_major_locator(MaxNLocator(nbins=20))\n    ax2.set(ylabel='Cumulative')\n\n    # Customizing the Box Plot.\n    ax3 = fig.add_subplot(grid[:, 2])\n    # Set title.\n    ax3.set_title('Box Plot')\n    # Plotting the box plot.\n    sns.boxplot(y=feature, data=df, orient='v', ax=ax3, color=orange_black[1])\n    ax3.yaxis.set_major_locator(MaxNLocator(nbins=25))\n\n    plt.suptitle(f'{title}', fontsize=24)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plotting character distribution\nplot_dist4(train_df, 'Character Count',\n           'Character Distribution ', orange_black[6])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### When we plot character counts we can see some linear relation there. With increasing number of character counts our target score decreases, interesting...","metadata":{}},{"cell_type":"code","source":"# regplot for character counts\n\nsns.regplot(x=train_df['Character Count'], y=train_df.target, order=1,\n                    \n                    color=orange_black[2],\n                    line_kws={'color': orange_black[6]},\n                    scatter_kws={'alpha':0.6})\nplt.title('Character Counts vs Target', fontsize=24)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Word Counts\n\n### Word Counts distribution looks more evenly (not normally!) not sure what it means for now, let's take a closer look with scatterplot...","metadata":{}},{"cell_type":"code","source":"def plot_word_number_histogram(text):\n    \n    \n    \"\"\"A function for displaying word distribuition\"\"\"\n    \n\n    fig, axes = plt.subplots(figsize=(18, 6))\n    sns.histplot(text.str.split().map(lambda x: len(x)), color=orange_black[3],stat='density', bins=50)\n    \n    \n    plt.xlabel('Word Count')\n    plt.ylabel('Frequency')    \n    fig.suptitle('Word Counts', fontsize=24, va='baseline')\n    plt.xticks(np.arange(140, 210, 5))\n    \n    # plotting\n    \n    fig.tight_layout()\n\n\n\nplot_word_number_histogram(train_df.excerpt)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Yeah... Doesn't look much meaningful to me, there's slight decrease in scores when the word count increses. Well let's check other meta features then...","metadata":{}},{"cell_type":"code","source":"# regplot for word counts\n\ntrain_df['Word Count']=train_df.excerpt.str.split().map(lambda x: len(x))\nsns.regplot(x=train_df['Word Count'], y=train_df.target, order=1,\n                    \n                    color=orange_black[2],\n                    line_kws={'color': orange_black[6]},\n                    scatter_kws={'alpha':0.4})\nplt.title('Word Counts vs Target', fontsize=24)\n\n# plotting\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Word Lenghts\n\n### Does length of the words affect readability score? Let's check how's the distribution: It looks like decently spread with little bit of right skew...","metadata":{}},{"cell_type":"code","source":"def plot_word_len_histogram(text):\n    \n    \"\"\"A function for comparing average word length\"\"\"\n    \n    fig, axes = plt.subplots(figsize=(18, 6))\n    sns.histplot(text.str.split().apply(lambda x: [len(i) for i in x]).map(\n        lambda x: np.mean(x)),\n                 color=orange_black[4])   \n\n    \n    fig.suptitle('Mean Word Lengths', fontsize=24, va='baseline')\n    \n    # plotting\n    \n    fig.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plotting word len hist\n\nplot_word_len_histogram(train_df.excerpt)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Oh that's interesting... There's strong linear relation between mean word lenghts and readability. Looks like longer/complicated words decreases the score.","metadata":{}},{"cell_type":"code","source":"# regplot for word lengths\n\ntrain_df['Mean Word Length'] = train_df.excerpt.str.split().apply(lambda x: [len(i) for i in x]).map(\n        lambda x: np.mean(x))\n\nsns.regplot(x=train_df['Mean Word Length'], y=train_df.target, order=1,\n                    \n                    color=orange_black[2],\n                    line_kws={'color': orange_black[6]},\n                    scatter_kws={'alpha':0.4})\nplt.title('Mean Word Lenghts vs Target', fontsize=24)\n\n# plotting\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Number of Digits\n\n### We can observe that having lots of digits slightly decreasing readability score...","metadata":{}},{"cell_type":"code","source":"# regplot for word lengths\n\n\ntrain_df['Number of Digits'] =  train_df['excerpt'].apply(lambda s: sum(c.isdigit() for c in s))\n\nsns.regplot(x=train_df['Number of Digits'], y=train_df['target'], order=1,\n                    color=orange_black[2],\n                    line_kws={'color': orange_black[6]},\n                    scatter_kws={'alpha':0.4})\nplt.title('Number of Digits vs Target', fontsize=24)\n\n# plotting\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Common N-Grams\n\n### Here we check some common words. They're not directly giving us info but they can help us summarizing what kind of texts we have here and understand them.","metadata":{}},{"cell_type":"code","source":"# creating corpus\n\nfrom nltk import FreqDist\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\n\n# setting stopwords \n\nstopwords = set(stopwords.words('english'))\n\nnew = train_df['excerpt'].str.split()\nnew = new.values.tolist()\ncorpus = [word.lower() for i in new for word in i if word.lower() not in stopwords]\n          \n#getting top 20 common words\n    \ncommon_words = [i[0] for i in FreqDist(corpus).most_common(20)]\ncommon_words_count = [i[1] for i in FreqDist(corpus).most_common(20)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Unigrams","metadata":{}},{"cell_type":"code","source":"# plotting unigrams\n\nfig, ax = plt.subplots(figsize=(18, 8))\nsns.barplot(x=common_words_count, y=common_words, palette='cividis')\nfor n, i in enumerate(common_words):    \n    ax.text(common_words_count[n]-0.003, \n            n, #Y location\n            s=f'{round(common_words_count[n],3)}', \n            va='center', \n            ha='right', \n            color='white', \n            fontsize=8,\n            bbox=dict(boxstyle='round',facecolor='red', alpha=0.5))\nplt.title('Most Common Unigrams', fontsize=24)\nsns.despine()\nplt.xticks([])\nfig.patch.set_facecolor('#FFFACD')\nax.patch.set_facecolor('#FFFFE0')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Bigrams","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\ndef ngrams(n, title, loc):\n    \"\"\"A Function to plot most common ngrams\"\"\"\n    fig, ax = plt.subplots(figsize=(18, 8))\n\n    new = train_df.excerpt.str.split()\n    new = new.values.tolist()\n    corpus = [word.lower() for i in new for word in i if word.lower()]\n\n    def _get_top_ngram(corpus, n=None):\n        #getting top ngrams\n        vec = CountVectorizer(ngram_range=(n, n),\n                              max_df=0.9,\n                              stop_words='english').fit(corpus)\n        bag_of_words = vec.transform(corpus)\n        sum_words = bag_of_words.sum(axis=0)\n        words_freq = [(word, sum_words[0, idx])\n                      for word, idx in vec.vocabulary_.items()]\n        words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n        return words_freq[:15]\n\n    top_n_bigrams = _get_top_ngram(train_df.excerpt, n)[:15]\n    x, y = map(list, zip(*top_n_bigrams))\n    sns.barplot(x=y, y=x, palette='cividis')  \n    \n    \n    for n, i in enumerate(x):    \n            ax.text(y[n]-loc, \n            n, #Y location\n            s=f'{round(y[n],3)}', \n            va='center', \n            ha='right', \n            color='white', \n            fontsize=8,\n            bbox=dict(boxstyle='round',facecolor='red', alpha=0.5))\n\n    fig.suptitle(title, fontsize=24, va='baseline')\n    sns.despine()\n    plt.xticks([])\n    fig.patch.set_facecolor('#FFFACD')\n    ax.patch.set_facecolor('#FFFFE0')\n    plt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plotting bigrams\nngrams(2, 'Most Common Bigrams', 0.8)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Trigrams","metadata":{}},{"cell_type":"code","source":"# plotting trigrams\nngrams(3, 'Most Common Trigrams', 0.15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Wordcloud\n\n## Again most common words in more visual format, not giving us extra feature but looks cool right? :)","metadata":{}},{"cell_type":"code","source":"# make worldcloud\nfrom wordcloud import WordCloud, ImageColorGenerator\nfrom PIL import Image\n\n# getting tokens\ntexts = \" \".join(token for token in corpus)\n# setting stopwords\nstopwords_wc = set(stopwords)\n# loading custom font\nfont_path = \"../input/wcloud/acetone_font.otf\"\n\n# generating wordcloud\nwordcloud = WordCloud(stopwords=stopwords_wc, font_path=font_path,\n                      max_words=1500,\n                      max_font_size=350, random_state=42,\n                      width=2000, height=1000,\n                      colormap = \"gist_stern\")\nwordcloud.generate(texts)\n\n# plotting\nplt.figure(figsize = (24, 13))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Getting Sentence Embeddings\n\n### We're given small number of training instances. With huge models especially neural networks we're always in risk of overfitting. For this and baseline purposes I implemented a simple sentence embedding model based on fine tuned roberta. After getting embedding matrix we are going to train that using Bayesian Ridge Regression...","metadata":{}},{"cell_type":"code","source":"# loading model using sentence transformers\n\nimport sentence_transformers\nfrom sentence_transformers import SentenceTransformer, models\n\n# setting model path for fine-tuned roberta weights\n\nmodel_path = '../input/finetuned-model1/checkpoint-568'\nword_embedding_model = models.Transformer(model_path, max_seq_length=275)\npooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\nmodel = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Vectorizing Texts\n\n### First we're going to vectorize our sentences to get numerical representations so we can feed them into our traditional machine learning algorithms...","metadata":{}},{"cell_type":"code","source":"# encoding train and test strings\n\nX_train = model.encode(train_df.excerpt, device='cuda')\nX_test = model.encode(test_df.excerpt, device='cuda')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking shapes of embeddings\n\ndisplay(X_train.shape)\ndisplay(X_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cross Validation\n\n### As I said we used really simple regressor to get results, I haven't used our extra features, neither optimized the model parameters. I think it's possible to get better score by using more advanced models with optimized parameters but for now we'll leave it like that. But for Bayesian Ridge we got decent starting point! I stratified the folds across the character counts which I believe  it gives more balanced stratification...","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nfrom datetime import datetime\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import BayesianRidge\n\npreds = []\ntrain_scores = []\n\ndf_oof=train_df.copy()\ndf_oof['oof'] = 0\n\nskf = StratifiedKFold(10, shuffle=True, random_state=42)\n\nsplits = list(skf.split(X=X_train, y=train_df['Character Count']))\n\n# predicting out of fold scores for each fold and doing predictions for each training set\n\nfor i, (train_idx, val_idx) in enumerate(splits):\n    print(f'\\n------------- Training Fold {i + 1} / {10}')\n    print(\"Current Time =\", datetime.now().strftime(\"%H:%M:%S\"))\n\n    clf = BayesianRidge(n_iter=300, verbose=True)\n    clf.fit(X_train[train_idx],train_df.target[train_idx])\n    train_score=mean_squared_error(train_df.target[train_idx], clf.predict(X_train[train_idx]), squared=False)\n    train_scores.append(train_score)\n    print(f\"Fold {i} train RMSE: {train_score}\")\n    \n    \n    preds.append(clf.predict(X_test))\n    x=clf.predict(X_train[val_idx])\n    df_oof['oof'].iloc[val_idx]+= x\n\nprint(f'Training score: {np.mean(train_scores)}, Training STD: {np.std(train_scores)}')\nprint(f'OOF score across folds: {mean_squared_error(df_oof.target, df_oof.oof, squared=False)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Results\n\n### Not a bad score for such a simple approach! More importantly I got decent CV/LB correlation. It's still early parts of the competition and I believe we gonna get more complicated models on the course but I thik this is decent starting point for starters. I hope you find it useful, **if you do please dont forget to upvote!** Also feel free to ask if you have any questions in comments section.\n\n### Happy coding!","metadata":{}},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"# getting mean prediction across 5 folds\ny_pred = np.mean(preds,0)\ny_pred.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating submission csv\n\nsub = test_df[[\"id\"]].copy()\nsub[\"target\"] = y_pred\nsub.to_csv('submission.csv', index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking submission file\n\nsub.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}