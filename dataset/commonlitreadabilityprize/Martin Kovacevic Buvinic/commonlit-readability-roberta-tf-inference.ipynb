{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Comments\n\nHere you can find a simple baseline model: https://www.kaggle.com/ragnar123/commonlit-readability-roberta-tf\n\nThis model has an out of folds root mean squared error of 0.5097\n\nHere is the training script:\n\nI tried to use tpu for training but the results are no good, hope this tensorflow baseline help to start this competition.","metadata":{}},{"cell_type":"code","source":"import re\nimport os\nimport numpy as np\nimport pandas as pd\nimport random\nimport math\nimport tensorflow as tf\nimport logging\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nfrom tensorflow.keras import backend as K\nfrom transformers import RobertaTokenizer, TFRobertaModel\nfrom kaggle_datasets import KaggleDatasets\ntf.get_logger().setLevel(logging.ERROR)\nfrom kaggle_datasets import KaggleDatasets","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Configurations\n# Number of folds for training\nFOLDS = 5\n# Max length\nMAX_LEN = 250\n# Get the trained model we want to use\nMODEL = '../input/tfroberta-base'\n# Let's load our model tokenizer\ntokenizer = RobertaTokenizer.from_pretrained(MODEL)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This function tokenize the text according to a transformers model tokenizer\ndef regular_encode(texts, tokenizer, maxlen = MAX_LEN):\n    enc_di = tokenizer.batch_encode_plus(\n        texts,\n        padding = 'max_length',\n        truncation = True,\n        max_length = maxlen,\n    )\n    \n    return np.array(enc_di['input_ids'])\n\n# This function encode our training sentences\ndef encode_texts(x_test, MAX_LEN):\n    x_test = regular_encode(x_test.tolist(), tokenizer, maxlen = MAX_LEN)\n    return x_test\n\n# Function to build our model\ndef build_roberta_base_model(max_len = MAX_LEN):\n    transformer = TFRobertaModel.from_pretrained(MODEL)\n    input_word_ids = tf.keras.layers.Input(shape = (max_len, ), dtype = tf.int32, name = 'input_word_ids')\n    sequence_output = transformer(input_word_ids)[0]\n    # We only need the cls_token, resulting in a 2d array\n    cls_token = sequence_output[:, 0, :]\n    output = tf.keras.layers.Dense(1, activation = 'linear', dtype = 'float32')(cls_token)\n    model = tf.keras.models.Model(inputs = [input_word_ids], outputs = [output])\n    return model\n\n# Function for inference\ndef roberta_base_inference():\n    # Read our test data\n    df = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\n    # Get text features\n    x_test = df['excerpt']\n    # Encode our text with Roberta tokenizer\n    x_test = encode_texts(x_test, MAX_LEN)\n    # Initiate an empty vector to store prediction\n    predictions = np.zeros(len(df))\n    # Predict with the 5 models (5 folds training)\n    for i in range(FOLDS):\n        print('\\n')\n        print('-'*50)\n        print(f'Predicting with model {i + 1}')\n        # Build model\n        model = build_roberta_base_model(max_len = MAX_LEN)\n        # Load pretrained weights\n        model.load_weights(f'../input/commonlit-readability-roberta-base/Roberta_Base_123_{i + 1}.h5')\n        # Predict\n        fold_predictions = model.predict(x_test).reshape(-1)\n        # Add fold prediction to the global predictions\n        predictions += fold_predictions / FOLDS\n    # Save submissions\n    df['target'] = predictions\n    df[['id', 'target']].to_csv('submission.csv', index = False)\n    return df\n\ndf = roberta_base_inference()\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}