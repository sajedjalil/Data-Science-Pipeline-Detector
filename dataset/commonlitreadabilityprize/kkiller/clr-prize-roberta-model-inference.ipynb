{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Notes","metadata":{}},{"cell_type":"markdown","source":"This kernel if only for inference, the training one is on its road.  \nThe experimental model is **roBERTa**. But, as we're using the **huggingface**'s **AutoModel** interface, you can easily choose whatever you want.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\n\nimport torch\nfrom torch import nn\nfrom  torch.utils.data import Dataset, DataLoader\n\nimport pickle\n\n\nfrom tqdm.notebook import tqdm\n\nfrom transformers import AutoTokenizer, AutoConfig, AutoModelForTokenClassification, AutoModelForSequenceClassification","metadata":{"id":"2dt7oG43VAqc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_LENGTH = 300\nNUM_TARGETS = 1\n\nSEED = 321\n\nMODEL_NAME = \"roberta-base\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TEST_BATCH_SIZE = 32\nTEST_NUM_WORKERS = 2\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint(\"Device:\", DEVICE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndf = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")\n\nprint(df.shape)\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(\"../input/crp-roberta-model-kkiller-public-ds/crp-roberta-model-kkiller-private/roberta-base-tokenizer.pkl\", \"rb\") as f:\n    TOKENIZER = pickle.load(f)\n    \nwith open(\"../input/crp-roberta-model-kkiller-public-ds/crp-roberta-model-kkiller-private/roberta-base-config.pkl\", \"rb\") as f:\n    CONFIG = pickle.load(f)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CRPDataset(Dataset):\n    def __init__(self, df, tokenizer=None):\n        self.df = df\n        \n        self.tokenizer = TOKENIZER if tokenizer is None else tokenizer\n        \n        self.tokenizer_kwargs = dict(\n            add_special_tokens=True,\n            return_tensors=\"pt\",\n            max_length=MAX_LENGTH,\n            padding=\"max_length\",\n            truncation=True,\n        )\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def tokenize(self, txt):\n        return self.tokenizer(txt, **self.tokenizer_kwargs)\n    \n    def __getitem__(self, idx):\n        d = self.tokenize(df.excerpt.iloc[idx])\n        input_ids, masks =  d[\"input_ids\"].squeeze(0), d[\"attention_mask\"].squeeze(0)\n        return input_ids, masks","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds = CRPDataset(df)\nlen(ds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x, x_mask = ds[0]\nx.shape, x_mask.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{"id":"vigGc55XxIFr"}},{"cell_type":"code","source":"def get_model(model_name=None, task=\"token_classification\", num_targets=NUM_TARGETS, config=None):\n    task = task.lower()\n        \n    if \"token\" in task:\n        model_instance = AutoModelForTokenClassification\n    elif \"sequence\" in task:\n        model_instance = AutoModelForSequenceClassification\n        \n    if config:\n        model = model_instance.from_config(config)\n        tokenizer = None\n    else:\n        model = model_instance.from_pretrained(model_name)\n        tokenizer = AutoTokenizer.from_pretrained(model_name)\n        config = AutoConfig.from_pretrained(model_name)\n    \n    if hasattr(model, \"classifier\"):\n        model.classifier = nn.Linear(model.classifier.in_features, NUM_TARGETS)\n        \n    return config,tokenizer, model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AttentionBlock(nn.Module):\n  def __init__(self, in_features, middle_features, out_features):\n    super().__init__()\n    self.in_features = in_features\n    self.middle_features = middle_features\n    self.out_features = out_features\n\n    self.W = nn.Linear(in_features, middle_features)\n    self.V = nn.Linear(middle_features, out_features)\n\n  def forward(self, features):\n    att = torch.tanh(self.W(features))\n\n    score = self.V(att)\n\n    attention_weights = torch.softmax(score, dim=1)\n\n    context_vector = attention_weights * features\n    context_vector = torch.sum(context_vector, dim=1)\n\n    return context_vector","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CRPTokenModel(nn.Module):\n    def __init__(self, config, num_targets=NUM_TARGETS):\n        super().__init__()\n        self.num_targets = num_targets\n        \n        config,tokenizer, model = get_model(config=config, task=\"token_classification\", num_targets=1)\n        \n        self.in_features =  model.classifier.in_features\n        model.classifier = nn.Identity()\n        \n        self.config = config\n        self.tokenizer = tokenizer\n        self.model = model\n        \n        self.att = AttentionBlock(self.in_features, self.in_features, 1)\n        self.fc = nn.Linear(self.in_features, self.num_targets)\n        \n    def forward(self, *args, **kwargs):\n        \n        x = self.model(*args, **kwargs)[\"logits\"]\n        x = self.att(x)\n        \n        x = self.fc(x)\n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_net(checkpoint_path=None, num_targets=NUM_TARGETS, config=None):\n    config = CONFIG if config is None else config\n\n    net = CRPTokenModel(config)\n    net = net.to(DEVICE)\n    if checkpoint_path is not None:\n        net.load_state_dict(torch.load(checkpoint_path, map_location=DEVICE))\n    net = net.eval()\n    return net","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\ndef _predict(nets, xb):\n    pred = 0\n    for net in nets:\n        pred += net(input_ids=xb[0], attention_mask=xb[1])\n\n    pred /= len(nets)\n\n    return pred\n\n@torch.no_grad()\ndef predict(nets, test_data):\n    preds = []\n    for xb in  test_data:\n        xb = (xb[0].to(DEVICE), xb[1].to(DEVICE))\n        \n        preds.append(_predict(nets, xb).cpu().numpy())\n\n    preds = np.concatenate(preds)\n    return preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data = CRPDataset(df)\ntest_loader = DataLoader(test_data, batch_size=TEST_BATCH_SIZE, num_workers=TEST_NUM_WORKERS, shuffle=False)\nlen(test_data), len(test_loader)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checkpoint_paths = list(Path(\"../input/crp-roberta-model-kkiller-public-ds/crp-roberta-model-kkiller-private/roberta-base_maxlen300_seed666\").glob(\"*.pth\"))\n\n\ncheckpoint_paths = [\n    '../input/crp-roberta-model-kkiller-public-ds/crp-roberta-model-kkiller-private/roberta-base_maxlen300_seed666/crp_roberta-base_fold0_epoch_00_rmse_val_-0.5064_20210504223412.pth',\n    '../input/crp-roberta-model-kkiller-public-ds/crp-roberta-model-kkiller-private/roberta-base_maxlen300_seed666/crp_roberta-base_fold1_epoch_03_rmse_val_-0.5388_20210504225149.pth',\n    '../input/crp-roberta-model-kkiller-public-ds/crp-roberta-model-kkiller-private/roberta-base_maxlen300_seed666/crp_roberta-base_fold2_epoch_05_rmse_val_-0.5622_20210504230806.pth',\n    '../input/crp-roberta-model-kkiller-public-ds/crp-roberta-model-kkiller-private/roberta-base_maxlen300_seed666/crp_roberta-base_fold3_epoch_00_rmse_val_-0.5852_20210504231457.pth',\n    '../input/crp-roberta-model-kkiller-public-ds/crp-roberta-model-kkiller-private/roberta-base_maxlen300_seed666/crp_roberta-base_fold4_epoch_02_rmse_val_-0.5352_20210504233113.pth',\n]\n\nnets = [\n    load_net(str(ckpt)) for ckpt in checkpoint_paths\n]\n\nprint(len(nets))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = predict(nets, test_loader)\nprint(preds.shape)\npreds[:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = df[[\"id\"]].copy()\nsub[\"target\"] = preds\n\nprint(sub.shape)\nsub.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}