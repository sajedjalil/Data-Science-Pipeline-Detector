{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Bag of Words and Word2Vec on CommonLit Readability Prize Competition\nIn this notebook, I will create a Bag of Words model and a Word Vectors model with Google's Word2Vec, and I will train multiple models on each.\n\nHere are the basic imports.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom bs4 import BeautifulSoup # for removing HTML tags\nimport re # removing punctuation and numbers\nimport nltk # removing stop words\nimport gensim # word vectors (word2vec)\nimport cython # speeding up training\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-07-01T03:32:45.762475Z","iopub.execute_input":"2021-07-01T03:32:45.762817Z","iopub.status.idle":"2021-07-01T03:32:45.770391Z","shell.execute_reply.started":"2021-07-01T03:32:45.762787Z","shell.execute_reply":"2021-07-01T03:32:45.769557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Imports\nFirst, I'll import the required data.","metadata":{}},{"cell_type":"code","source":"# reading in the train and test data\ntrain_data = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\ntest_data = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\nprint(train_data.shape, test_data.shape)\nprint(train_data[\"excerpt\"][0])\ntrain_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-01T03:28:09.679624Z","iopub.execute_input":"2021-07-01T03:28:09.679988Z","iopub.status.idle":"2021-07-01T03:28:09.751385Z","shell.execute_reply.started":"2021-07-01T03:28:09.679959Z","shell.execute_reply":"2021-07-01T03:28:09.750393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# splitting data (not using train_test_split since that would mess up the index)\nX_train = train_data[\"excerpt\"].iloc[:(train_data[\"excerpt\"].size-200)]\nX_val = train_data[\"excerpt\"].iloc[(train_data[\"excerpt\"].size-200):]\ny_train = train_data[\"target\"].iloc[:(train_data[\"target\"].size-200)]\ny_val = train_data[\"target\"].iloc[(train_data[\"target\"].size-200):]","metadata":{"execution":{"iopub.status.busy":"2021-07-01T03:28:12.718555Z","iopub.execute_input":"2021-07-01T03:28:12.718898Z","iopub.status.idle":"2021-07-01T03:28:12.725833Z","shell.execute_reply.started":"2021-07-01T03:28:12.71887Z","shell.execute_reply":"2021-07-01T03:28:12.724312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocessing for Bag of Words\nNext, I'll do the data preprocessing to get the sentences ready for a Bag of Words (also partially used later for Word2Vec model).","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer\n\ndef excerpt_to_wordlist(excerpt, remove_stop_words=False):\n    '''Creates a clean excerpt by removing\n    HTML tags, punctuation, numbers, and stop words'''\n    # remove HTML tags\n    excerpt1 = BeautifulSoup(excerpt).get_text()\n    # remove punctuation\n    excerpt2 = re.sub(\"[^a-zA-Z0-9]\", \" \", excerpt1)\n    # removing unneccesary newlines/spaces + lower case\n    excerpt3 = excerpt2.lower().split()\n    if remove_stop_words:\n        # removing stop words\n        stops = set(stopwords.words(\"english\")) # faster search through set than list\n        excerpt4 = [word for word in excerpt3 if word not in stops]\n        # lemmatization and stemming\n        lemmatizer = WordNetLemmatizer()\n        porterStemmer = PorterStemmer()\n        excerpt5 = [lemmatizer.lemmatize(porterStemmer.stem(word)) for \\\n                   word in excerpt4]\n        # return final review (joined by spaces)\n        return \" \".join(excerpt5)\n    else:\n        return excerpt3","metadata":{"execution":{"iopub.status.busy":"2021-07-01T03:28:23.572931Z","iopub.execute_input":"2021-07-01T03:28:23.573271Z","iopub.status.idle":"2021-07-01T03:28:23.580412Z","shell.execute_reply.started":"2021-07-01T03:28:23.573242Z","shell.execute_reply":"2021-07-01T03:28:23.579594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clean_train_excerpts = []\nclean_valid_excerpts = []\nclean_test_excerpts = []\nfor i in range(0, X_train.size):\n    clean_train_excerpts.append(excerpt_to_wordlist(X_train[i], True))\n    if (i+1)%500 == 0:\n        print(f\"{i+1} finished for train data\")\nfor i in range(X_train.size, X_train.size+X_val.size):\n    clean_valid_excerpts.append(excerpt_to_wordlist(X_val[i], True))\nfor i in range(0, test_data[\"excerpt\"].size):\n    clean_test_excerpts.append(excerpt_to_wordlist(test_data[\"excerpt\"][i], True))","metadata":{"execution":{"iopub.status.busy":"2021-07-01T03:28:26.289565Z","iopub.execute_input":"2021-07-01T03:28:26.290058Z","iopub.status.idle":"2021-07-01T03:28:37.333247Z","shell.execute_reply.started":"2021-07-01T03:28:26.290027Z","shell.execute_reply":"2021-07-01T03:28:37.332239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Bag of Words Representation\nHere, I will create a Bag of Words representation for the excerpts.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n# creating the vectorizer for creating the bag of words\nvectorizer = CountVectorizer(analyzer = \"word\", \\\n                             tokenizer = None, \\\n                             preprocessor = None, \\\n                             stop_words = None, \\\n                             max_features = 5000)\n\ntrain_data_features = vectorizer.fit_transform(clean_train_excerpts)\ntrain_data_features = train_data_features.toarray()\nvalid_data_features = vectorizer.transform(clean_valid_excerpts)\nvalid_data_features = valid_data_features.toarray()\ntest_data_features = vectorizer.transform(clean_test_excerpts)\ntest_data_features = test_data_features.toarray()","metadata":{"execution":{"iopub.status.busy":"2021-07-01T03:28:37.334858Z","iopub.execute_input":"2021-07-01T03:28:37.33514Z","iopub.status.idle":"2021-07-01T03:28:37.733196Z","shell.execute_reply.started":"2021-07-01T03:28:37.335113Z","shell.execute_reply":"2021-07-01T03:28:37.732311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Models for Bag of Words\nNow, I will create the following models to test on the Bag of Words:\n1. Random Forest Regressor\n2. XGBoost Regressor","metadata":{}},{"cell_type":"markdown","source":"### Random Forest Regressor","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\nmodelbow1 = RandomForestRegressor(n_estimators=100, random_state=1)\nmodelbow1.fit(train_data_features, y_train)\nval_predictions_bow1 = modelbow1.predict(valid_data_features)\nprint(mean_squared_error(y_val,val_predictions_bow1))","metadata":{"execution":{"iopub.status.busy":"2021-07-01T03:28:43.003918Z","iopub.execute_input":"2021-07-01T03:28:43.00444Z","iopub.status.idle":"2021-07-01T03:28:53.433125Z","shell.execute_reply.started":"2021-07-01T03:28:43.004408Z","shell.execute_reply":"2021-07-01T03:28:53.431084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### XGBoost Regressor","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBRegressor\n\nmodelbow2 = XGBRegressor(n_estimators=100, learning_rate=0.005, n_jobs=3, random_state=1)\nmodelbow2.fit(train_data_features, y_train)\nval_predictions_bow2 = modelbow2.predict(valid_data_features)\nprint(mean_squared_error(y_val,val_predictions_bow2))","metadata":{"execution":{"iopub.status.busy":"2021-07-01T02:54:01.633301Z","iopub.execute_input":"2021-07-01T02:54:01.633844Z","iopub.status.idle":"2021-07-01T02:54:23.158936Z","shell.execute_reply.started":"2021-07-01T02:54:01.633797Z","shell.execute_reply":"2021-07-01T02:54:23.15772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As seen from above, it seems that Random Forest did the best on the Bag of Words representation of the data.","metadata":{}},{"cell_type":"markdown","source":"## Data Preprocessing for Word Embeddings\nNext, I'll do the data preprocessing for word embeddings.","metadata":{}},{"cell_type":"code","source":"# Load punkt tokenizer\ntokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n\n# Function to split review into sentences\ndef excerpt_to_sentences(excerpt, tokenizer, remove_stopwords=False):\n    '''Creates a clean text split into sentences'''\n    # Using tokenizer to split paragraph into sentences\n    sentences1 = tokenizer.tokenize(excerpt.strip())\n    # go over each sentence\n    sentences = []\n    for sentence in sentences1:\n        if len(sentence) > 0:\n            # call review_to_wordlist\n            sentences.append(excerpt_to_wordlist(sentence, remove_stopwords))\n    return sentences","metadata":{"execution":{"iopub.status.busy":"2021-07-01T02:30:38.267653Z","iopub.execute_input":"2021-07-01T02:30:38.268026Z","iopub.status.idle":"2021-07-01T02:30:38.274592Z","shell.execute_reply.started":"2021-07-01T02:30:38.267996Z","shell.execute_reply":"2021-07-01T02:30:38.273316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentences = []\n\n# adding train sentences in a list for training the word2vec model (not removing stop words for better model training)\nfor i in range(0, X_train.size):\n    sentences += excerpt_to_sentences(X_train[i], tokenizer)\n    if (i+1)%500 == 0:\n        print(f\"{i+1} sentences finished for train data\")\n    if (i+1) == X_train.size:\n        print(\"All done\")","metadata":{"execution":{"iopub.status.busy":"2021-07-01T02:30:44.557991Z","iopub.execute_input":"2021-07-01T02:30:44.558371Z","iopub.status.idle":"2021-07-01T02:30:51.91109Z","shell.execute_reply.started":"2021-07-01T02:30:44.55834Z","shell.execute_reply":"2021-07-01T02:30:51.909816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Word Embedding Model\nNow, I will be using a Word2Vec model to create the word embeddings.\n\nWord embeddings are a way of coding words into a list of numbers, and this vector effectively stores the word's information. This can help for operations such as the following one:\n> king - man + woman = queen\n\nThis should work because the relation between a king and a man is very similar to the relation between a queen and a woman.\n\nFirst, I'll create the word embedding model.","metadata":{}},{"cell_type":"code","source":"# Import the built-in logging module; configure it for Word2Vec to create nice output messages (next three lines from Bag of Words meets Bags of Popcorn tutorial)\nimport logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n                    level=logging.INFO)\n\n# Setting values for parameters\nnum_features = 300     # Word vector dimensionality                      \nmin_word_count = 20    # Minimum word count                        \nnum_workers = 4        # Number of threads to run in parallel\ncontext = 20           # Context window size                                                                                    \ndownsampling = 0.0001  # Downsample setting for frequent words\n\n# initializing and training the word2vec model (number 1 from the list)\nfrom gensim.models import word2vec\nwv = word2vec.Word2Vec(sentences, workers=num_workers, vector_size=num_features, min_count=min_word_count, \\\n                        window=context, sample=downsampling)\nwv = wv.wv","metadata":{"execution":{"iopub.status.busy":"2021-07-01T02:18:51.466532Z","iopub.execute_input":"2021-07-01T02:18:51.466922Z","iopub.status.idle":"2021-07-01T02:18:53.426098Z","shell.execute_reply.started":"2021-07-01T02:18:51.466891Z","shell.execute_reply":"2021-07-01T02:18:53.425054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating Word Embeddings\nNow, I'll create the word embeddings (average over the whole excerpt).","metadata":{}},{"cell_type":"code","source":"def excerpt_to_vector(excerpt, model, num_features):\n    '''Function to average all word vectors in an excerpt'''\n    # initializing an empty np array\n    feature_vector = np.zeros((num_features,),dtype=\"float32\")\n    num_words = 0\n    # set of words in model's vocabulary\n    set_of_words = set(model.index_to_key)\n    for word in excerpt:\n        if word in set_of_words:\n            num_words += 1\n            feature_vector = np.add(feature_vector,model[word])\n    feature_vector = np.divide(feature_vector, num_words)\n    return feature_vector\n\ndef get_feature_vectors(excerpts, model, num_features):\n    '''Function to create feature vectors for all excerpts'''\n    count = 0\n    excerpt_vectors = np.zeros((len(excerpts),num_features),dtype=\"float32\")\n    for excerpt in excerpts:\n        excerpt_vectors[count] = excerpt_to_vector(excerpt, model, num_features)\n        count += 1\n        if count%1000 == 0:\n            print(f\"{count} reviews converted\")\n    return excerpt_vectors","metadata":{"execution":{"iopub.status.busy":"2021-07-01T02:18:56.722503Z","iopub.execute_input":"2021-07-01T02:18:56.722838Z","iopub.status.idle":"2021-07-01T02:18:56.729975Z","shell.execute_reply.started":"2021-07-01T02:18:56.72281Z","shell.execute_reply":"2021-07-01T02:18:56.7291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clean_train_excerpts_vec = []\nclean_valid_excerpts_vec = []\nclean_test_excerpts_vec = []\nfor i in range(0, X_train.size):\n    clean_train_excerpts_vec.append(excerpt_to_wordlist(X_train[i]))\n    if (i+1)%500 == 0:\n        print(f\"{i+1} finished for train data\")\nfor i in range(X_train.size, X_train.size+X_val.size):\n    clean_valid_excerpts_vec.append(excerpt_to_wordlist(X_val[i]))\nfor i in range(0, test_data[\"excerpt\"].size):\n    clean_test_excerpts_vec.append(excerpt_to_wordlist(test_data[\"excerpt\"][i]))\n\ntrain_data_vectors = get_feature_vectors(clean_train_excerpts_vec, wv, num_features)\nvalid_data_vectors = get_feature_vectors(clean_valid_excerpts_vec, wv, num_features)\ntest_data_vectors = get_feature_vectors(clean_test_excerpts_vec, wv, num_features)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T02:19:27.953892Z","iopub.execute_input":"2021-07-01T02:19:27.954384Z","iopub.status.idle":"2021-07-01T02:19:30.831703Z","shell.execute_reply.started":"2021-07-01T02:19:27.954352Z","shell.execute_reply":"2021-07-01T02:19:30.830625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training Models\nNow, I will train the following models for Word Embeddings and try to find the best one:\n1. Random Forest Regressor\n2. XGBoost Regressor","metadata":{}},{"cell_type":"markdown","source":"### Random Forest Regressor","metadata":{}},{"cell_type":"code","source":"modelvec1 = RandomForestRegressor(n_estimators=100, random_state=1)\nmodelvec1.fit(train_data_vectors, y_train)\nval_predictions_vec1 = modelvec1.predict(valid_data_vectors)\nprint(mean_squared_error(y_val,val_predictions_vec1))","metadata":{"execution":{"iopub.status.busy":"2021-07-01T02:22:46.012153Z","iopub.execute_input":"2021-07-01T02:22:46.01258Z","iopub.status.idle":"2021-07-01T02:23:30.54301Z","shell.execute_reply.started":"2021-07-01T02:22:46.012547Z","shell.execute_reply":"2021-07-01T02:23:30.542236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### XGBoost Regressor","metadata":{}},{"cell_type":"code","source":"modelvec2 = XGBRegressor(n_estimators=100, learning_rate=0.005, n_jobs=3, random_state=1)\nmodelvec2.fit(train_data_vectors, y_train)\nval_predictions_vec2 = modelvec2.predict(valid_data_vectors)\nprint(mean_squared_error(y_val,val_predictions_vec2))","metadata":{"execution":{"iopub.status.busy":"2021-07-01T02:23:30.544945Z","iopub.execute_input":"2021-07-01T02:23:30.545273Z","iopub.status.idle":"2021-07-01T02:23:38.453761Z","shell.execute_reply.started":"2021-07-01T02:23:30.545244Z","shell.execute_reply":"2021-07-01T02:23:38.451649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems like RandomForestRegressor does the best for Word Embeddings. It does slightly better on the Bag of Words, so we will use the first model.","metadata":{}},{"cell_type":"markdown","source":"## Final Training\n\nHere, we'll do the final training on all of the data, and we will use this for the final submission.","metadata":{}},{"cell_type":"code","source":"# creating final X and y\nfinal_train_features = np.concatenate([train_data_features, valid_data_features], axis=0)\nfinal_y_train = pd.concat([y_train, y_val], axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T03:04:52.241656Z","iopub.execute_input":"2021-07-01T03:04:52.241985Z","iopub.status.idle":"2021-07-01T03:04:52.296568Z","shell.execute_reply.started":"2021-07-01T03:04:52.241957Z","shell.execute_reply":"2021-07-01T03:04:52.295628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# final training\nfinal_model = RandomForestRegressor(n_estimators=100, random_state=1)\nfinal_model.fit(final_train_features, final_y_train)\n\n# prediction and output to csv file\npredictions = final_model.predict(test_data_features)\noutput = pd.DataFrame(data={\"id\":test_data[\"id\"], \"target\": predictions})\noutput.to_csv(\"submission.csv\", index=False)\nprint(\"Submission file created!\")","metadata":{"execution":{"iopub.status.busy":"2021-07-01T03:07:44.057686Z","iopub.execute_input":"2021-07-01T03:07:44.058072Z","iopub.status.idle":"2021-07-01T03:09:45.230689Z","shell.execute_reply.started":"2021-07-01T03:07:44.05804Z","shell.execute_reply":"2021-07-01T03:09:45.229782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Thank you for reading! Any feedback on the notebook would be appreciated.","metadata":{}}]}