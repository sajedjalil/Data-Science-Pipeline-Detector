{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\n## for data\nimport pandas as pd\nimport collections\nimport json\nimport string \n## for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n## for text processing\nimport re\nimport nltk\nfrom nltk.tokenize import word_tokenize\n## for sentiment\nfrom textblob import TextBlob\n## for ner, pos\nimport spacy\nnlp = spacy.load(\"en_core_web_lg\")\nnltk.download('wordnet')\n## parameters searching\nfrom sklearn.model_selection import GridSearchCV\n## rmse\nfrom sklearn.metrics import mean_squared_error\n## pickle\nimport dill as pickle\n\nfrom sklearn.base import BaseEstimator\n\nfrom sklearn.svm import SVR\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2021-06-14T20:06:07.523306Z","iopub.execute_input":"2021-06-14T20:06:07.523636Z","iopub.status.idle":"2021-06-14T20:06:12.609317Z","shell.execute_reply.started":"2021-06-14T20:06:07.523605Z","shell.execute_reply":"2021-06-14T20:06:12.608395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/commonlitreadabilityprize/train.csv')","metadata":{"execution":{"iopub.status.busy":"2021-06-14T20:06:12.610738Z","iopub.execute_input":"2021-06-14T20:06:12.61103Z","iopub.status.idle":"2021-06-14T20:06:12.640004Z","shell.execute_reply.started":"2021-06-14T20:06:12.611Z","shell.execute_reply":"2021-06-14T20:06:12.639231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df.drop(['target', 'standard_error'], axis=1)\n# X = train[['excerpt']]\ny = df['target']","metadata":{"execution":{"iopub.status.busy":"2021-06-14T20:06:12.641804Z","iopub.execute_input":"2021-06-14T20:06:12.64214Z","iopub.status.idle":"2021-06-14T20:06:12.647457Z","shell.execute_reply.started":"2021-06-14T20:06:12.642105Z","shell.execute_reply":"2021-06-14T20:06:12.646537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.preprocessing import KBinsDiscretizer\n\nclass regressor_stratified_cv:\n    def __init__(self, n_splits = 10, n_repeats = 2, group_count = 10,\n                 random_state = 0, strategy = 'quantile'):\n        self.group_count = group_count\n        self.strategy = strategy\n        self.cvkwargs = dict(n_splits = n_splits, n_repeats = n_repeats, \n                             random_state = random_state)\n        self.cv = RepeatedStratifiedKFold(**self.cvkwargs)\n        self.discretizer = KBinsDiscretizer(n_bins = self.group_count, encode = 'ordinal',\n                                            strategy = self.strategy)  \n            \n    def split(self, X, y, groups = None):\n        kgroups=self.discretizer.fit_transform(y[:, None])[:, 0]\n        return self.cv.split(X, kgroups, groups)\n    \n    def get_n_splits(self, X, y, groups = None):\n        return self.cv.get_n_splits(X, y, groups)","metadata":{"execution":{"iopub.status.busy":"2021-06-14T20:06:12.648929Z","iopub.execute_input":"2021-06-14T20:06:12.6494Z","iopub.status.idle":"2021-06-14T20:06:12.658726Z","shell.execute_reply.started":"2021-06-14T20:06:12.649363Z","shell.execute_reply":"2021-06-14T20:06:12.657588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import the BaseEstimator\nfrom sklearn.base import BaseEstimator\n\n# define the class OutletTypeEncoder\n# This will be our custom transformer that will create 3 new binary columns\n# custom transformer must have methods fit and transform\nclass FeatureGenerator(BaseEstimator):\n\n    def __init__(self):\n        pass\n\n    def fit(self, x_dataset, y=None):\n        return self\n\n    def transform(self, x_dataset):\n        \n        # utils function to count the element of a list\n        def utils_lst_count(lst):\n            dic_counter = collections.Counter()\n            for x in lst:\n                dic_counter[x] += 1\n            dic_counter = collections.OrderedDict(\n                       sorted(dic_counter.items(),\n                       key=lambda x: x[1], reverse=True))\n            lst_count = [ {key:value} for key,value in dic_counter.items() ]\n            return lst_count\n        \n        # utils function create new column for each tag category\n        def utils_new_features(lst_dics_tuples, tag):\n            if len(lst_dics_tuples) > 0:\n                tag_type = []\n                for dic_tuples in lst_dics_tuples:\n                    for tuple in dic_tuples:\n                        type, n = tuple[1], dic_tuples[tuple]\n                        tag_type = tag_type + [type]*n\n                        dic_counter = collections.Counter()\n                        for x in tag_type:\n                            dic_counter[x] += 1\n                return dic_counter[tag]\n            else:\n                return 0\n\n            \n        # num of words in excerpt\n        x_dataset['word_count'] = x_dataset[\"excerpt\"].apply(lambda x: len(str(x).split(\" \")))\n        # num of chars in excerpt\n        x_dataset['char_count'] = x_dataset[\"excerpt\"].apply(lambda x: sum(len(word) for word in str(x).split(\" \")))\n        # num of sentences in excerpt\n        x_dataset['sentence_count'] = x_dataset[\"excerpt\"].apply(lambda x: len(str(x).split(\".\")))\n        # avg word len in excerpt\n        x_dataset['avg_word_length'] = x_dataset['char_count'] / x_dataset['word_count']\n        # avg sentence len in excerpt\n        x_dataset['avg_sentence_lenght'] = x_dataset['word_count'] / x_dataset['sentence_count']\n        # sentiment index of excerpt\n        x_dataset[\"sentiment\"] = x_dataset[\"excerpt\"].apply(lambda x: TextBlob(x).sentiment.polarity)\n        x_dataset[\"nlp_text\"] = x_dataset[\"excerpt\"].apply(lambda x: nlp(x) )\n        # ner tag text and exctract tags into a list\n        x_dataset[\"ner_tags\"] = x_dataset[\"nlp_text\"].apply(lambda x: [(tag.text, tag.label_) \n                                for tag in x.ents] )\n        # count ner tags\n        x_dataset[\"ner_tags\"] = x_dataset[\"ner_tags\"].apply(lambda x: utils_lst_count(x))\n        # extract ner features\n        ner_tags_set = ['QUANTITY', 'MONEY', 'GPE',\n                    'NORP', 'CARDINAL', 'LOC',\n                    'ORDINAL', 'PRODUCT', 'FAC',\n                    'LANGUAGE', 'TIME', 'LAW',\n                    'EVENT', 'ORG', 'PERCENT',\n                    'WORK_OF_ART', 'PERSON', 'DATE']\n        for feature in ner_tags_set:\n            x_dataset[\"ner_tags_\" + feature] = x_dataset[\"ner_tags\"].apply(lambda x:\n                                                                 utils_new_features(x, feature))\n        \n        # pos tag text and exctract tags into a list\n        x_dataset[\"pos_tags\"] = x_dataset[\"nlp_text\"].apply(lambda x: [(token.text, token.tag_) \n                                for token in x] )\n        # count pos tags\n        x_dataset[\"pos_tags\"] = x_dataset[\"pos_tags\"].apply(lambda x: utils_lst_count(x))\n        # extract pos features\n        pos_tags_set = ['CC', 'POS', 'WDT', 'VBP', 'FW', ':', 'PRP$',\n                    'WRB', 'PRP', 'RP', 'RBS', 'NNP', 'CD', 'EX', 'PDT',\n                    'VBN', 'WP$', 'JJ', 'SYM', 'VBG', 'VB', 'JJS', 'VBD',\n                    'WP', ',', 'NNS', 'NN', 'VBZ', 'MD', 'RB', 'DT',\n                    'JJR', 'UH', 'NNPS', 'TO', 'RBR']\n    \n        for feature in pos_tags_set:\n            x_dataset[\"pos_tags_\" + feature] = x_dataset[\"pos_tags\"].apply(lambda x:\n                                                                 utils_new_features(x, feature))\n    \n        return x_dataset","metadata":{"execution":{"iopub.status.busy":"2021-06-14T20:06:12.660107Z","iopub.execute_input":"2021-06-14T20:06:12.660464Z","iopub.status.idle":"2021-06-14T20:06:12.680126Z","shell.execute_reply.started":"2021-06-14T20:06:12.660427Z","shell.execute_reply":"2021-06-14T20:06:12.679207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define the class IDFVectorizer\n# to generate new feature with mean of idf\nclass IDFVectorizer(BaseEstimator):\n\n    def __init__(self):\n        pass\n\n    def fit(self, x_dataset, y=None):\n        return self\n\n    def transform(self, x_dataset):\n        \n        # removal of punctuation\n        PUNCT_TO_REMOVE = string.punctuation\n        def remove_punctuation(text):\n            return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n        \n        # removal of stopwords\n        from nltk.corpus import stopwords\n        \", \".join(stopwords.words('english'))\n        STOPWORDS = set(stopwords.words('english'))\n        def remove_stopwords(text):\n            return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n        \n        # lemmatization \n        from nltk.stem import WordNetLemmatizer\n        lemmatizer = WordNetLemmatizer()\n        def lemmatize_words(text):\n            return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n        \n        # word frequency in docs\n        def doc_freq(word):\n            c = 0\n            try:\n                c = DF[word]\n            except:\n                pass\n            return c\n        \n        # idf vector generation\n        def mean_of_vector(tokens):\n            idf_vec = []\n            for token in np.unique(tokens):\n                df = doc_freq(token)\n                idf = np.log(N/(df + 1))\n                try:\n                    idf_vec.append(idf)\n                except:\n                    pass\n            \n            return np.mean(idf_vec)\n        \n        # lower casing\n        x_dataset[\"excerpt_proc\"] = x_dataset[\"excerpt\"].str.lower()\n        # removal of punctuation\n        x_dataset[\"excerpt_proc\"] = x_dataset[\"excerpt_proc\"].apply(lambda text: remove_punctuation(text))\n        # removal of stopwords\n        x_dataset[\"excerpt_proc\"] = x_dataset[\"excerpt_proc\"].apply(lambda text: remove_stopwords(text))\n        # lemmatization \n        x_dataset[\"excerpt_proc\"] = x_dataset[\"excerpt_proc\"].apply(lambda text: lemmatize_words(text))    \n        # tokenizetion\n        x_dataset[\"excerpt_proc\"] = x_dataset[\"excerpt_proc\"].apply(lambda x: [token for token in word_tokenize(x)])\n        \n        N = len(x_dataset[\"excerpt\"])\n        DF = {}\n        for i in range(N):\n            tokens = x_dataset[\"excerpt_proc\"].iloc[i]\n            for w in tokens:\n                try:\n                    DF[w].add(i)\n                except:\n                    DF[w] = {i}\n            \n\n        for i in DF:\n            DF[i] = len(DF[i]) \n\n        x_dataset['idf_vec'] = x_dataset[\"excerpt_proc\"].apply(lambda x:  mean_of_vector(x))\n        \n        return x_dataset","metadata":{"execution":{"iopub.status.busy":"2021-06-14T20:06:12.681453Z","iopub.execute_input":"2021-06-14T20:06:12.681868Z","iopub.status.idle":"2021-06-14T20:06:12.698255Z","shell.execute_reply.started":"2021-06-14T20:06:12.681836Z","shell.execute_reply":"2021-06-14T20:06:12.697481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pre-processsing step\n# Drop the columns \nfrom sklearn.compose import ColumnTransformer\npre_process = ColumnTransformer(remainder='passthrough',\n                                transformers=[('drop_columns', 'drop', ['id', \n                                                                        'url_legal', \n                                                                        'license',\n                                                                        'excerpt',\n                                                                        'ner_tags',\n                                                                        'pos_tags',\n                                                                        'excerpt_proc',\n                                                                        'nlp_text'\n                                                                       ])])","metadata":{"execution":{"iopub.status.busy":"2021-06-14T20:06:13.875171Z","iopub.execute_input":"2021-06-14T20:06:13.875477Z","iopub.status.idle":"2021-06-14T20:06:13.881316Z","shell.execute_reply.started":"2021-06-14T20:06:13.875447Z","shell.execute_reply":"2021-06-14T20:06:13.880277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\npipe = Pipeline([ #('get_new_columns', FeatureGenerator()),\n                  ('idf_vect', IDFVectorizer()),\n                  ('pre_processing', pre_process),\n                 ('svr', SVR(C = 0.01, kernel = 'linear'))\n                ])","metadata":{"execution":{"iopub.status.busy":"2021-06-14T20:06:15.963046Z","iopub.execute_input":"2021-06-14T20:06:15.963351Z","iopub.status.idle":"2021-06-14T20:06:15.969155Z","shell.execute_reply.started":"2021-06-14T20:06:15.963324Z","shell.execute_reply":"2021-06-14T20:06:15.968264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preproc_pipe = Pipeline([ ('get_new_columns', FeatureGenerator())\n                ])","metadata":{"execution":{"iopub.status.busy":"2021-06-14T20:06:24.343945Z","iopub.execute_input":"2021-06-14T20:06:24.344263Z","iopub.status.idle":"2021-06-14T20:06:24.348045Z","shell.execute_reply.started":"2021-06-14T20:06:24.344233Z","shell.execute_reply":"2021-06-14T20:06:24.346946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_proc = preproc_pipe.fit_transform(X)","metadata":{"execution":{"iopub.status.busy":"2021-06-14T20:06:25.485245Z","iopub.execute_input":"2021-06-14T20:06:25.485554Z","iopub.status.idle":"2021-06-14T20:14:25.282697Z","shell.execute_reply.started":"2021-06-14T20:06:25.485518Z","shell.execute_reply":"2021-06-14T20:14:25.2818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_splits = 5\nn_repeats = 2\ngroup_count = 10\ncv = regressor_stratified_cv(n_splits = n_splits, n_repeats = n_repeats,\n                           group_count = group_count, random_state = 0, strategy = 'quantile')\n\n\n# logger.info(\"Train SVR\")\ni = 0\nfor train_index, test_index in cv.split(X_proc, y):\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    pipe.fit(X_train, y_train)\n    predict = pipe.predict(X_test)\n    rmse = mean_squared_error(y_test, predict, squared=False)\n    print(rmse)\n#     logger.info(\"The rmse for SVR iteration {}: {:.3f}\".format(i, rmse))\n#     logger.info(\"-------------------------------\")\n    i += 1","metadata":{"execution":{"iopub.status.busy":"2021-06-14T20:17:38.367574Z","iopub.execute_input":"2021-06-14T20:17:38.367929Z","iopub.status.idle":"2021-06-14T20:18:51.31924Z","shell.execute_reply.started":"2021-06-14T20:17:38.367899Z","shell.execute_reply":"2021-06-14T20:18:51.317778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv('../input/commonlitreadabilityprize/test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-06-14T20:18:51.320758Z","iopub.execute_input":"2021-06-14T20:18:51.321096Z","iopub.status.idle":"2021-06-14T20:18:51.334427Z","shell.execute_reply.started":"2021-06-14T20:18:51.321059Z","shell.execute_reply":"2021-06-14T20:18:51.333692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = preproc_pipe.transform(test)","metadata":{"execution":{"iopub.status.busy":"2021-06-14T20:20:45.481766Z","iopub.execute_input":"2021-06-14T20:20:45.482084Z","iopub.status.idle":"2021-06-14T20:20:46.631318Z","shell.execute_reply.started":"2021-06-14T20:20:45.482056Z","shell.execute_reply":"2021-06-14T20:20:46.630504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = pipe.predict(test)","metadata":{"execution":{"iopub.status.busy":"2021-06-14T20:20:46.632853Z","iopub.execute_input":"2021-06-14T20:20:46.633351Z","iopub.status.idle":"2021-06-14T20:20:46.657382Z","shell.execute_reply.started":"2021-06-14T20:20:46.633308Z","shell.execute_reply":"2021-06-14T20:20:46.656647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame({'id':test['id'],'target':predictions})","metadata":{"execution":{"iopub.status.busy":"2021-06-14T20:20:49.20554Z","iopub.execute_input":"2021-06-14T20:20:49.205899Z","iopub.status.idle":"2021-06-14T20:20:49.210915Z","shell.execute_reply.started":"2021-06-14T20:20:49.20587Z","shell.execute_reply":"2021-06-14T20:20:49.209726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-14T20:20:55.610371Z","iopub.execute_input":"2021-06-14T20:20:55.610709Z","iopub.status.idle":"2021-06-14T20:20:55.752565Z","shell.execute_reply.started":"2021-06-14T20:20:55.610672Z","shell.execute_reply":"2021-06-14T20:20:55.751733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# C = [.01, 1]\n# degree = [3, 4]\n# gamma = ['scale']\n# coef0 = [.01, 1]\n\n# parameters = [\n#   {'svr__C': C, 'svr__kernel':['linear']},\n#   {'svr__C': C, 'svr__kernel':['poly'], 'svr__degree':degree, 'svr__gamma':gamma, 'svr__coef0': coef0},\n#   {'svr__C': C, 'svr__kernel':['rbf'], 'svr__gamma':gamma},\n#   {'svr__C': C, 'svr__kernel':['sigmoid'], 'svr__gamma':gamma, 'svr__coef0': coef0},\n# ]","metadata":{"execution":{"iopub.status.busy":"2021-06-14T20:04:06.579031Z","iopub.status.idle":"2021-06-14T20:04:06.579626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# grid = GridSearchCV(pipe, param_grid=parameters, cv=3, scoring=\"neg_mean_absolute_error\", n_jobs=-1, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-14T20:04:06.580652Z","iopub.status.idle":"2021-06-14T20:04:06.58134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# grid.fit(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-06-14T20:04:06.582408Z","iopub.status.idle":"2021-06-14T20:04:06.583032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(\"Best parameters: {}\".format(grid.best_params_))","metadata":{"execution":{"iopub.status.busy":"2021-06-14T20:04:06.584163Z","iopub.status.idle":"2021-06-14T20:04:06.584753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # get importance\n# importance = pipe.steps[3][1].feature_importances_\n# # summarize feature importance\n# for i,v in enumerate(importance):\n# \tprint('Feature: %0d, Score: %.5f' % (i,v))\n# # plot feature importance\n# pyplot.bar([x for x in range(len(importance))], importance)\n# pyplot.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-14T20:04:06.585793Z","iopub.status.idle":"2021-06-14T20:04:06.586328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # n_estimators\n# n_estimators = [int(x) for x in np.linspace(start = 200, stop = 1000, num = 5)]\n\n# # max_features\n# max_features = ['auto', 'sqrt']\n\n# # max_depth\n# max_depth = [int(x) for x in np.linspace(20, 100, num = 5)]\n# max_depth.append(None)\n\n# # min_samples_split\n# min_samples_split = [2, 5, 10]\n\n# # min_samples_leaf\n# min_samples_leaf = [1, 2, 4]\n\n# # bootstrap\n# bootstrap = [True, False]\n\n# # Create the random grid\n# random_grid = {'rrandom_forest__n_estimators': n_estimators,\n#                'random_forest__max_features': max_features,\n#                'random_forest__max_depth': max_depth,\n#                'random_forest__min_samples_split': min_samples_split,\n#                'random_forest__min_samples_leaf': min_samples_leaf,\n#                'random_forest__bootstrap': bootstrap}\n\n# random_search = RandomizedSearchCV(estimator=pipe,\n#                                    param_distributions=random_grid,\n#                                    n_iter=5,\n#                                    scoring='neg_root_mean_squared_error',\n#                                    cv=3, \n#                                    verbose=1, \n#                                    random_state=8)","metadata":{"execution":{"iopub.status.busy":"2021-06-14T20:04:06.587313Z","iopub.status.idle":"2021-06-14T20:04:06.587922Z"},"trusted":true},"execution_count":null,"outputs":[]}]}