{"cells":[{"metadata":{},"cell_type":"markdown","source":"**This notebook is used for learning language and profanity weights.**\nTo avoid overfitting, I randomly choose one set from the top 10 best weights and accept it if it leads to an increase in my public leaderboard score.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport numpy\nfrom sklearn import metrics\nimport pandas as pd\nfrom sklearn.metrics import roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**1. Learn language weights.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"wr = open('results.txt', \"w\", encoding='utf8')\nmybest = pd.read_csv('/kaggle/input/mybest/sub9523.csv')\nsub = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-test-translated/jigsaw_miltilingual_test_translated.csv')\nsub1 = pd.read_csv('/kaggle/input/mysub42/sub9458.csv')\nsub['prd'] = sub1['toxic']\n\n#Ranges here may be manually reset to save computing time.\n#p = [1.30,0.60,0.80,0.50,0.60,0.60]\nfor p0 in numpy.arange(1.2, 1.3, 0.1):\n    for p1 in numpy.arange(0.6, 0.7, 0.1):\n        for p2 in numpy.arange(0.8, 0.9, 0.1):\n            for p3 in numpy.arange(0.6, 0.7, 0.1):\n                for p4 in numpy.arange(0.6, 0.7, 0.1):\n                    for p5 in numpy.arange(0.6, 0.7, 0.1):\n                        out = []\n                        for _, row in sub.iterrows():\n                            item = [row['id'], row['prd'], row['lang']]\n                            if(item[2]=='es'):\n                                if(item[1]<0.7):\n                                    item[1] *= p0#1.250    99.9455    99.8233\n                            elif(item[2]=='fr'):\n                                if(item[1]<0.7):\n                                    item[1] *= p1#0.950    99.9436    99.8294\n                            elif(item[2]=='ru'):\n                                if(item[1]<0.7):\n                                    item[1] *= p2#0.900    99.9409    99.8379\n                            elif(item[2]=='it'):\n                                if(item[1]<0.7):\n                                    item[1] *= p3#0.750    99.9480    99.8688\n                            elif(item[2]=='tr'):\n                                if(item[1]<0.7):\n                                    item[1] *= p4#0.900    99.9486    99.8715\n                            elif(item[2]=='pt'):\n                                if(item[1]<0.7):\n                                    item[1] *= p5#-3 0.9991403473465919 0.9981593\n\n                            out.append(item)\n\n                        of = pd.DataFrame(out, columns=['id', 'toxic', 'lang'])\n                        score1 = roc_auc_score(mybest.toxic.round().astype(int), of.toxic.values)\n                        score2 = roc_auc_score(of.toxic.round().astype(int), mybest.toxic.values)\n                        line = '%1.2f,%1.2f,%1.2f,%1.2f,%1.2f,%1.2f\\t%2.4f\\t%2.4f'%(p0, p1,p2,p3,p4,p5, 100*score1, 100*score2)\n                        print(line)\n                        wr.write(line+'\\n')\nwr.close()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**2. Learn profanity weights.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"wr = open('results.txt', \"w\", encoding='utf8')\nmybest = pd.read_csv('/kaggle/input/mybest/sub9523.csv')\nsub = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-test-translated/jigsaw_miltilingual_test_translated.csv')\nsub1 = pd.read_csv('/kaggle/input/mysub42/sub9458.csv')\nsub['prd'] = sub1['toxic']\n\ndic = {}#Profanity dictionary.\noft = open('/kaggle/input/profanity/Profanity.txt', \"r\", encoding='utf8')\nfor l in oft:\n    ele = l.strip().lower().split(':')\n    dic[ele[0]] = ele[1]\noft.close()\n\n#I set all weights as 1 and tune them one by one to save computing time.\nlen, les, lit, ltr, lfr, lru, lpt = 1., 1., 1., 1., 1., 1., 1.\n\n#len 1052 1.4: 99.9771    99.9512\n#les 101 1.00:    99.9771    99.9512\n#lit 235 1.10:    99.9770    99.9515\n#ltr 41 1.00:    99.9770    99.9515\n#lfr 202 1.00:    99.9770    99.9515\n#lru 1.30:    99.9680    99.7974\n#lpt 66 1.30:    99.9771    99.9519\n\nenpros = dic['en'].split(',')\nfor len in numpy.arange(1.3, 1.4, 0.1):\n    out = []\n    found = 0\n    for _, row in sub.iterrows():\n        if(row['lang']=='es'):\n            lmd = les#99.4925    99.2043\n        elif(row['lang']=='it'):\n            lmd = lit\n        elif(row['lang']=='tr'):\n            lmd = ltr\n        elif(row['lang']=='fr'):\n            lmd = lfr\n        elif(row['lang']=='ru'):\n            lmd = lru\n        else:\n            lmd = lpt\n\n        item = [row['id'], row['prd']]\n        if(item[1]<0.5):\n            for w in enpros:\n                if(str(row['translated']).lower().find(w)>=0):\n                    item[1] *= len\n                    found += 1\n                    break\n\n            ws = dic[row['lang']].split(',')\n            for w in ws:\n                if(str(row['content']).lower().find(w)>=0):\n                    item[1] *= lmd\n                    #if(row['lang']=='pt'):\n                        #found += 1\n                    break\n        out.append(item)\n\n\n    of = pd.DataFrame(out, columns=['id', 'toxic'])\n    \n    score1 = roc_auc_score(mybest.toxic.round().astype(int), of.toxic.values)\n    score2 = roc_auc_score(of.toxic.round().astype(int), mybest.toxic.values)\n    l = '%1.2f:\\t%2.4f\\t%2.4f'%(len, 100*score1, 100*score2)\n    print(found, l)\n    wr.write(l+'\\n')\nwr.close()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}