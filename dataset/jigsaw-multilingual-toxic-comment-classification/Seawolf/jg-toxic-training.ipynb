{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **This notebook is used for training.**\n\nI adopted a lot from xhlulu's notebook at Link. Many thanks!\nXLM-Roberta was used.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\n\nimport numpy as np\nfrom sklearn import metrics\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input, Concatenate\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom kaggle_datasets import KaggleDatasets\nimport transformers\nfrom transformers import TFAutoModel, AutoTokenizer\nfrom tqdm.notebook import tqdm\nfrom tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors\n\n# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Define encoder.\ndef regular_encode(texts, tokenizer, maxlen=512):\n    enc_di = tokenizer.batch_encode_plus(\n        texts, \n        return_attention_masks=False, \n        return_token_type_ids=False,\n        pad_to_max_length=True,\n        max_length=maxlen\n    )\n    \n    return np.array(enc_di['input_ids'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#'''\ndef build_model(transformer, max_len=512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_lang_tags = Input(shape=(4,), dtype=tf.float32, name=\"input_lang_tags\")\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    x = Concatenate()([cls_token, input_lang_tags])\n    out = Dense(1, activation='sigmoid')(x)\n    \n    model = Model(inputs=[input_word_ids, input_lang_tags], outputs=out)\n    \n    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC()])\n    \n    return model\n\n'''\n#Build a pure text model where language information is not considered.\ndef build_model_PT(transformer, max_len=512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(cls_token)\n    \n    model = Model(inputs=input_word_ids, outputs=out)\n    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC()])\n    \n    return model\n#'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"AUTO = tf.data.experimental.AUTOTUNE\n# Configuration\nEPOCHS = 2\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync#16\nMAX_LEN = 192","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# First load the real tokenizer\nMODEL = 'jplu/tf-xlm-roberta-large'\ntokenizer = AutoTokenizer.from_pretrained(MODEL)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Load text data into memory**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def lang_embed(lang, tran):\n    lang_codes = {'en':'000', 'es':'100', 'fr':'010',\n                  'it':'001', 'pt':'110', 'ru':'101',\n                  'tr':'011'}\n    tran_codes = {'orig':'0', 'tran':'1'}\n    vec = lang_codes[lang]+tran_codes[tran]\n    vec = [int(v) for v in vec]\n    return vec","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def text_process(text):\n    ws = text.split(' ')\n    if(len(ws)>160):\n        text = ' '.join(ws[:160]) + ' ' + ' '.join(ws[-32:])\n    return text\n\n#Build the original validation corpus.\nvalid = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation.csv')\nvalid['comment_text'] = valid['comment_text'].apply(lambda x: text_process(x))\n\nx_valid = regular_encode(valid.comment_text.values, tokenizer, maxlen=MAX_LEN)\nlang_tag_valid = np.array([lang_embed(row['lang'], 'orig') for _, row in valid.iterrows()])\ny_valid = valid.toxic.values\n\ngap = 128#valid.shape[0]%BATCH_SIZE\nx_valid = np.concatenate((x_valid, x_valid[-gap:]))\nlang_tag_valid = np.concatenate((lang_tag_valid, lang_tag_valid[-gap:]))\ny_valid = np.concatenate((y_valid, y_valid[-gap:]))\nprint(y_valid.shape)\n'''\n#Build the translated validation corpus.\nvalid = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-test-translated/jigsaw_miltilingual_valid_translated.csv')\nvalid['comment_text'] = valid['comment_text'].apply(lambda x: text_process(x))\n\nx_tran_valid = regular_encode(valid.comment_text.values, tokenizer, maxlen=MAX_LEN)\nlang_tag_tran_valid = np.array([lang_embed('en', 'tran') for _, row in valid.iterrows()])\ny_tran_valid = valid.toxic.values\n\ngap = 128#valid.shape[0]%BATCH_SIZE\nx_tran_valid = np.concatenate((x_tran_valid, x_tran_valid[-gap:]))\nlang_tag_tran_valid = np.concatenate((lang_tag_tran_valid, lang_tag_tran_valid[-gap:]))\ny_tran_valid = np.concatenate((y_tran_valid, y_tran_valid[-gap:]))\nprint(y_tran_valid.shape)\n'''\nsub = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Build the original and translated test corpora.\ntest = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test.csv')\ntest['content'] = test['content'].apply(lambda x: text_process(x))\n\n#tran_test = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-test-translated/jigsaw_miltilingual_test_translated.csv')\n#tran_test['translated'] = tran_test['translated'].apply(lambda x: text_process(x))\n                       \nx_test = regular_encode(test.content.values, tokenizer, maxlen=MAX_LEN)\nlang_tag_test = np.array([lang_embed(row['lang'], 'orig') for _, row in test.iterrows()])\n\n#x_tran_test = regular_encode(tran_test.translated.values, tokenizer, maxlen=MAX_LEN)\n#lang_tag_tran_test = np.array([lang_embed('en', 'tran') for _, row in tran_test.iterrows()])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. **1. Gather pseudal labelled corpus for fine train.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"mybest = pd.read_csv('/kaggle/input/mybest/sub9521.csv')\nmybest['orig'] = test['content']\nmybest['lang'] = test['lang']\nmybest['tran'] = ''#tran_test['translated']\n\nout = []\nfor _, row in mybest.iterrows():\n    #if row['lang'] not in ('fr', 'ru', 'pt'):#Only gather those not in validation?\n        #continue\n    if(row['toxic']>=0.5):\n        out.append([row['orig'], row['tran'], row['lang'], 1])\n    elif(row['toxic']<0.5):\n        out.append([row['orig'], row['tran'], row['lang'], 0])\n\ntrain = pd.DataFrame(out, columns=['orig', 'tran', 'lang', 'toxic'])\ntrain['orig'] = train['orig'].apply(lambda x: text_process(x))\n#train['tran'] = train['tran'].apply(lambda x: text_process(x))\nprint(train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time \nx_orig_train = regular_encode(train.orig.values, tokenizer, maxlen=MAX_LEN)\nlang_tag_orig_train = np.array([lang_embed(row['lang'], 'orig') for _, row in train.iterrows()])\n\n#x_tran_train = regular_encode(train.tran.values, tokenizer, maxlen=MAX_LEN)\n#lang_tag_tran_train = np.array([lang_embed('en', 'tran') for _, row in train.iterrows()])\n\ny_train = train.toxic.values\n\ngap = 128#train.shape[0]%BATCH_SIZE\nx_orig_train = np.concatenate((x_orig_train, x_orig_train[-gap:]))\nlang_tag_orig_train = np.concatenate((lang_tag_orig_train, lang_tag_orig_train[-gap:]))\n\n#x_tran_train = np.concatenate((x_tran_train, x_tran_train[-gap:]))\n#lang_tag_tran_train = np.concatenate((lang_tag_tran_train, lang_tag_tran_train[-gap:]))\n\ny_train = np.concatenate((y_train, y_train[-gap:]))\nprint(y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**2.Load model into the TPU**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nwith strategy.scope():\n    transformer_layer = TFAutoModel.from_pretrained(MODEL)\n    #model = build_model_PT(transformer_layer, max_len=MAX_LEN)\n    model = build_model(transformer_layer, max_len=MAX_LEN)\n#model.summary()\n#model.load_weights('/kaggle/input/basemodels/mg2m.h5')\nmodel.load_weights('/kaggle/input/basemodels/mixmoriggen1.h5')\n#model.load_weights('/kaggle/input/en2m1211/en2m1211.h5')\n#model.load_weights('/kaggle/input/mixmodel0/mixm0.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**3. Fine train on pseudo labelled corpus.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"***3.1 Train on original data.***","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#'''#First train on best orignal data.\nhistory = model.fit([x_orig_train[:63872],lang_tag_orig_train[:63872]], y_train[:63872],\n                    validation_data=([x_valid[:8064],lang_tag_valid[:8064]], y_valid[:8064]),\n                    batch_size=BATCH_SIZE, epochs=1, verbose=1)\n'''\n#First train on best orignal data.\nhistory = model.fit(x_orig_train[:63872], y_train[:63872],\n                    validation_data=(x_valid[:8064], y_valid[:8064]),\n                    batch_size=BATCH_SIZE, epochs=1, verbose=1)\n#'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#'''#Fine train on validation data.#\nhistory = model.fit([x_valid[:8064],lang_tag_valid[:8064]], y_valid[:8064],\n                    validation_data=([x_valid[:8064],lang_tag_valid[:8064]], y_valid[:8064]),\n                    batch_size=BATCH_SIZE, epochs=1, verbose=1)\n'''\nhistory = model.fit(x_valid[:8064], y_valid[:8064],\n                    validation_data=(x_valid[:8064], y_valid[:8064]),\n                    batch_size=BATCH_SIZE, epochs=2, verbose=1)\n#'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***3.3 Save model.***","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save_weights(\"/kaggle/working/mixgn2mp4.h5\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***3.4 Predict.***","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nsub['toxic'] = model.predict([x_test, lang_tag_test], verbose=1)\n#sub['toxic'] = model.predict(x_test, verbose=1)\nsub.to_csv('submission.csv', index=False)\nscore1 = roc_auc_score(mybest.toxic.round().astype(int), sub.toxic.values)\nscore2 = roc_auc_score(sub.toxic.round().astype(int), mybest.toxic.values)\nprint('p: %2.4f %2.4f'%(100*score1, 100*score2))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}