{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Importing Required Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import math\nimport numpy as np\nimport pandas as pd\nimport logging\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.model_selection import KFold\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB,  BernoulliNB\nfrom sklearn.metrics import accuracy_score, log_loss,jaccard_similarity_score\nfrom sklearn.preprocessing import normalize\nfrom sklearn import svm\nfrom sklearn.model_selection import GridSearchCV\n#xboost and heamy\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom heamy.dataset import Dataset\nfrom heamy.estimator import Classifier\nfrom heamy.pipeline import ModelsPipeline\n#Keras modules\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.utils import to_categorical\nfrom keras.datasets import mnist\nfrom keras.utils.vis_utils import model_to_dot\nfrom IPython.display import SVG\nfrom keras.utils import np_utils","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Path"},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_DIR = \"../input\"\nSUBMISSION_FILE = \"{0}/sample_submission.csv\".format(DATA_DIR)\nTRAIN_FILE = \"{0}/train.csv\".format(DATA_DIR)\nTEST_FILE = \"{0}/test.csv\".format(DATA_DIR)\nid_test = pd.read_csv('../input/test.csv').Id","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_distplots(train_path, colum_num_from=1, colum_num_to=11):\n    train = pd.read_csv(train_path)\n    _ = plt.figure(figsize=(20, 20))\n    i = 0\n    for feature in train.columns[colum_num_from:colum_num_to]:\n        i += 1\n        plt.subplot(5, 5, i)\n        sns.distplot(train[train.Cover_Type == 1][feature], hist=False, label='1')\n        sns.distplot(train[train.Cover_Type == 2][feature], hist=False, label='2')\n        sns.distplot(train[train.Cover_Type == 3][feature], hist=False, label='3')\n        sns.distplot(train[train.Cover_Type == 4][feature], hist=False, label='4')\n        sns.distplot(train[train.Cover_Type == 5][feature], hist=False, label='5')\n        sns.distplot(train[train.Cover_Type == 6][feature], hist=False, label='6')\n        sns.distplot(train[train.Cover_Type == 7][feature], hist=False, label='7')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Distribution plots of each feature with each Cover_Type**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_distplots(TRAIN_FILE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Pre defined values for later use**"},{"metadata":{},"cell_type":"markdown","source":"Log-Loss is used. Also called as cross entropy loss.\n> -(yt log(yp) + (1 - yt) log(1 - yp))"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"CACHE = False\nNFOLDS = 5\nSEED = 1337\n\n#cross_entropy loss\n#-(yt log(yp) + (1 - yt) log(1 - yp))\nMETRIC = log_loss\n\nID = 'Id'\nTARGET = 'Cover_Type'\n\n#set precision to 5 decimal places\nnp.set_printoptions(precision=5)\nnp.set_printoptions(suppress=True)\n\n#seed value is set\nnp.random.seed(SEED)\nlogging.basicConfig(level=logging.WARNING)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering\n> Creating new features from existing features."},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_feats(df):\n    #Hydrology - Fire Points\n    df['HF1'] = (df['Horizontal_Distance_To_Hydrology'] + df['Horizontal_Distance_To_Fire_Points'])\n    df['HF2'] = (df['Horizontal_Distance_To_Hydrology'] - df['Horizontal_Distance_To_Fire_Points'])\n    \n    #Hydrology - Roadways\n    df['HR1'] = (df['Horizontal_Distance_To_Hydrology'] + df['Horizontal_Distance_To_Roadways'])\n    df['HR2'] = (df['Horizontal_Distance_To_Hydrology'] - df['Horizontal_Distance_To_Roadways'])\n    \n    #Firepoints - Roadways\n    df['FR1'] = (df['Horizontal_Distance_To_Fire_Points'] + df['Horizontal_Distance_To_Roadways'])\n    df['FR2'] = (df['Horizontal_Distance_To_Fire_Points'] - df['Horizontal_Distance_To_Roadways'])\n    \n    #Elevation & Vertical_Distance_To_Hydrology\n    df['EV1'] = (df['Elevation'] + df['Vertical_Distance_To_Hydrology'])\n    df['EV2'] = (df['Elevation'] - df['Vertical_Distance_To_Hydrology'])\n    \n    #Mean of created features\n    df['Mean_HF1'] = df.HF1 / 2\n    df['Mean_HF2'] = df.HF2 / 2\n    df['Mean_HR1'] = df.HR1 / 2\n    df['Mean_HR2'] = df.HR2 / 2\n    df['Mean_FR1'] = df.FR1 / 2\n    df['Mean_FR2'] = df.FR2 / 2\n    df['Mean_EV1'] = df.EV1 / 2\n    df['Mean_EV2'] = df.EV2 / 2    \n    \n    #Oblique Distance\n    df['Elevation_Vertical'] = df['Elevation'] + df['Vertical_Distance_To_Hydrology']    \n    df['Neg_Elevation_Vertical'] = df['Elevation'] - df['Vertical_Distance_To_Hydrology']\n    \n    #Given the horizontal & vertical distance to hydrology, \n    #it will be more intuitive to obtain the euclidean distance: sqrt{(verticaldistance)^2 + (horizontaldistance)^2}    \n    df['slope_hyd_sqrt'] = (df['Horizontal_Distance_To_Hydrology']**2+df['Vertical_Distance_To_Hydrology']**2)**0.5\n    \n    #remove infinite value if any\n    df['slope_hyd_sqrt'] = df.slope_hyd_sqrt.map(lambda x: 0 if np.isinf(x) else x)\n    \n    df['slope_hyd2'] = np.sqrt(df['Horizontal_Distance_To_Hydrology']**2+df['Vertical_Distance_To_Hydrology']**2)\n    df['slope_hyd2'] = df.slope_hyd2.map(lambda x: 0 if np.isinf(x) else x) # remove infinite value if any\n    \n    #Mean distance to Amenities \n    df['Mean_Amenities'] = (df.Horizontal_Distance_To_Fire_Points + df.Horizontal_Distance_To_Hydrology + df.Horizontal_Distance_To_Roadways) / 3 \n    \n    #Mean Distance to Fire and Water \n    df['Mean_Fire_Hyd1'] = (df.Horizontal_Distance_To_Fire_Points + df.Horizontal_Distance_To_Hydrology) / 2\n    df['Mean_Fire_Hyd2'] = (df.Horizontal_Distance_To_Fire_Points + df.Horizontal_Distance_To_Roadways) / 2\n    \n    #Shadiness\n    df['Shadiness_morn_noon'] = df.Hillshade_9am / (df.Hillshade_Noon+1)\n    df['Shadiness_noon_3pm'] = df.Hillshade_Noon / (df.Hillshade_3pm+1)\n    df['Shadiness_morn_3'] = df.Hillshade_9am / (df.Hillshade_3pm+1)\n    df['Shadiness_morn_avg'] = (df.Hillshade_9am + df.Hillshade_Noon)/2\n    df['Shadiness_afternoon'] = (df.Hillshade_Noon + df.Hillshade_3pm)/2\n    df['Shadiness_mean_hillshade'] =  (df['Hillshade_9am']  + df['Hillshade_Noon'] + df['Hillshade_3pm'] ) / 3    \n    \n    #Shade Difference\n    df[\"Hillshade-9_Noon_diff\"] = df[\"Hillshade_9am\"] - df[\"Hillshade_Noon\"]\n    df[\"Hillshade-noon_3pm_diff\"] = df[\"Hillshade_Noon\"] - df[\"Hillshade_3pm\"]\n    df[\"Hillshade-9am_3pm_diff\"] = df[\"Hillshade_9am\"] - df[\"Hillshade_3pm\"]\n\n    # Mountain Trees\n    df[\"Slope*Elevation\"] = df[\"Slope\"] * df[\"Elevation\"]\n    # Only some trees can grow on steep montain\n    \n    ### More features\n    df['Neg_HorizontalHydrology_HorizontalFire'] = (df['Horizontal_Distance_To_Hydrology']-df['Horizontal_Distance_To_Fire_Points'])\n    df['Neg_HorizontalHydrology_HorizontalRoadways'] = (df['Horizontal_Distance_To_Hydrology']-df['Horizontal_Distance_To_Roadways'])\n    df['Neg_HorizontalFire_Points_HorizontalRoadways'] = (df['Horizontal_Distance_To_Fire_Points']-df['Horizontal_Distance_To_Roadways'])\n    \n    df['MeanNeg_Mean_HorizontalHydrology_HorizontalFire'] = (df['Horizontal_Distance_To_Hydrology']-df['Horizontal_Distance_To_Fire_Points'])/2\n    df['MeanNeg_HorizontalHydrology_HorizontalRoadways'] = (df['Horizontal_Distance_To_Hydrology']-df['Horizontal_Distance_To_Roadways'])/2\n    df['MeanNeg_HorizontalFire_Points_HorizontalRoadways'] = (df['Horizontal_Distance_To_Fire_Points']-df['Horizontal_Distance_To_Roadways'])/2   \n    df[\"Vertical_Distance_To_Hydrology\"] = abs(df['Vertical_Distance_To_Hydrology'])\n    \n    df['Neg_Elev_Hyd'] = df.Elevation-df.Horizontal_Distance_To_Hydrology*0.2\n    \n    # Bin Features\n    bin_defs = [\n        # col name, bin size, new name\n        ('Elevation', 200, 'Binned_Elevation'), # Elevation is different in train vs. test!?\n        ('Aspect', 45, 'Binned_Aspect'),\n        ('Slope', 6, 'Binned_Slope'),\n        ('Horizontal_Distance_To_Hydrology', 140, 'Binned_Horizontal_Distance_To_Hydrology'),\n        ('Horizontal_Distance_To_Roadways', 712, 'Binned_Horizontal_Distance_To_Roadways'),\n        ('Hillshade_9am', 32, 'Binned_Hillshade_9am'),\n        ('Hillshade_Noon', 32, 'Binned_Hillshade_Noon'),\n        ('Hillshade_3pm', 32, 'Binned_Hillshade_3pm'),\n        ('Horizontal_Distance_To_Fire_Points', 717, 'Binned_Horizontal_Distance_To_Fire_Points')\n    ]\n    \n    for col_name, bin_size, new_name in bin_defs:\n        df[new_name] = np.floor(df[col_name] / bin_size)\n        \n    print('Total number of features : %d' % (df.shape)[1])\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Loading and preprocessing\n> Data preprocessing and loading is done"},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_and_process_dataset():\n    train = pd.read_csv(TRAIN_FILE)\n    test = pd.read_csv(TEST_FILE)\n\n    # XGB needs labels starting with 0!\n    # now 7 become 6, 6 become 5 and so on ..\n    y_train = train[TARGET].ravel() - 1\n    \n    classes = train.Cover_Type.unique()\n    num_classes = len(classes)\n    print(\"There are {0} classes: {1} \".format(num_classes, classes))        \n\n    train.drop([ID, TARGET], axis=1, inplace=True)\n    test.drop([ID], axis=1, inplace=True)\n    \n    train = add_feats(train)    \n    test = add_feats(test)    \n    \n    cols_to_normalize = [ 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n                       'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n                       'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', \n                       'Horizontal_Distance_To_Fire_Points', \n                       'Shadiness_morn_noon', 'Shadiness_noon_3pm', 'Shadiness_morn_3',\n                       'Shadiness_morn_avg',\n                       'Shadiness_afternoon', \n                       'Shadiness_mean_hillshade',\n                       'HF1', 'HF2', \n                       'HR1', 'HR2', \n                       'FR1', 'FR2'\n                       ]\n\n    train[cols_to_normalize] = normalize(train[cols_to_normalize])\n    test[cols_to_normalize] = normalize(test[cols_to_normalize])\n\n    # elevation was found to have very different distributions on test and training sets\n    # lets just drop it for now to see if we can implememnt a more robust classifier!\n    train = train.drop('Elevation', axis=1)\n    test = test.drop('Elevation', axis=1)    \n    \n    x_train = train.values\n    x_test = test.values\n\n    return {'X_train': x_train, 'X_test': x_test, 'y_train': y_train}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Converting dataset to 'Dataset' object using Heamy**\n> Dataset oject is used in every heamy object including classifiers."},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = Dataset(preprocessor=load_and_process_dataset, use_cache=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ensemble Learning\n> Ensemble learning is implemented. Model is trained in two stages.\n> 1. In stage 1, training data is fed into multiple classifiers with k-cross folds. The output is out-of-fold predictions.\n> 2. In stage 2 training, out-of-fold predictions are fed to another single classifier (in our case svm and neural networks).\n> 3. In the end, the output of stage 2 classifier is final output."},{"metadata":{},"cell_type":"markdown","source":"**Initializing the classifiers with their Hyperparameters for Ensemble Learning**\n> * Classifier object is used from Heamy library.\n> * Classifiers from different modules including sklearn are implemented into Classifier object of Heamy.\n> * It is convenient to work with Heamy."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Multiple Classifiers are used for multiple results\n\n#RandomForestClassifier\nrf_params = {'n_estimators': 200, 'criterion': 'entropy', 'random_state': 0}\nrf = Classifier(dataset=dataset, estimator=RandomForestClassifier, \n                use_cache=False, parameters=rf_params, name='rf')\n\n#RandomForestClassifier\nrf1_params = {'n_estimators': 200, 'criterion': 'gini', 'random_state': 0}\nrf1 = Classifier(dataset=dataset, estimator=RandomForestClassifier, \n                 use_cache=False, parameters=rf1_params,name='rf1')\n\n#ExtraTreesClassifier\net_params = {'n_estimators': 200, 'criterion': 'entropy', 'random_state': 0}\net = Classifier(dataset=dataset, estimator=ExtraTreesClassifier, \n                use_cache=False, parameters=et_params,name='et')\n\n#ExtraTreesClassifier\net1_params = {'n_estimators': 200, 'criterion': 'gini', 'random_state': 0}\net1 = Classifier(dataset=dataset, use_cache=False, estimator=ExtraTreesClassifier,\n                 parameters=et1_params,name='et1')\n\n#LGBMClassifier\nlgb_params = {'n_estimators': 200, 'learning_rate':0.1}\nlgbc = Classifier(dataset=dataset, estimator=LGBMClassifier, \n                  use_cache=False, parameters=lgb_params,name='lgbc')\n\n#LogisticRegression\nlogr_params = {'solver' : 'liblinear', 'multi_class' : 'ovr', 'C': 1, 'random_state': 0}\nlogr = Classifier(dataset=dataset, estimator=LogisticRegression, \n                  use_cache=False, parameters=logr_params,name='logr')\n\n#Naive Bayes\ngnb = Classifier(dataset=dataset,estimator=GaussianNB, use_cache=False, name='gnb')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**XGB Classifier**\n> * Classifiers can also work through a defined function. A classifier should have four inputs of data and then it can easily be used with Heamy."},{"metadata":{"trusted":true},"cell_type":"code","source":"def xgb_classifier(X_train, y_train, X_test, y_test=None):\n    xg_params = {'seed': 0,\n                'colsample_bytree': 0.7,\n                'silent': 1,\n                'subsample': 0.7,\n                'learning_rate': 0.1,\n                'objective': 'multi:softprob',   \n                'num_class': 7,\n                'max_depth': 4,\n                'min_child_weight': 1,\n                'eval_metric': 'mlogloss',\n                'nrounds': 200}\n    \n    X_train = xgb.DMatrix(X_train, label=y_train)\n    model = xgb.train(xg_params, X_train, xg_params['nrounds'])\n    return model.predict(xgb.DMatrix(X_test))\n\nxgb_first = Classifier(estimator=xgb_classifier, dataset=dataset, use_cache=CACHE, name='xgb_classifier')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Setting up the pipeline to train using multiple models**\n> * Stack the models and returns new dataset with out-of-fold predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline = ModelsPipeline(rf, et, et1, lgbc, logr, gnb, xgb_first)\nstack_ds = pipeline.stack(k=NFOLDS,seed=SEED)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Out-of-folds predictions**"},{"metadata":{"trusted":true},"cell_type":"code","source":"stack_ds.X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stack_ds.X_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Shape of out-of-fold predictions:\", \"X shape: \", stack_ds.X_train.shape, \"y shape: \", stack_ds.y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_outfold = stack_ds.X_train.values\nX_test_outfold = stack_ds.X_test.values\nX = X_train_outfold\ny_train_sv = stack_ds.y_train + 1\ny = y_train_sv","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Stage 2.1: Logistic Regression\n> Training on out-of-fold predictions with LogisticRegression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train LogisticRegression on stacked data (second stage)\nlr = LogisticRegression\nlr_params = {'C': 5, 'random_state' : SEED, 'solver' : 'liblinear', 'multi_class' : 'ovr',}\nstacker = Classifier(dataset=stack_ds, estimator=lr, use_cache=False, parameters=lr_params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_proba = stacker.predict()\n# Note: labels starting with 0 in xgboost, therefore adding +1!\npredictions = np.round(np.argmax(preds_proba, axis=1)).astype(int) + 1\n\nsubmission = pd.read_csv(SUBMISSION_FILE)\nsubmission[TARGET] = predictions\nsubmission.to_csv('Stage_2_1_logregr_out_of_fold.csv', index=None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Stage 2.2: SVM\n> Now, training with SVM"},{"metadata":{},"cell_type":"markdown","source":"**Grid Search for svc**"},{"metadata":{"trusted":true},"cell_type":"code","source":"Cs = [0.001, 0.01, 0.1, 1, 10]\ngammas = [0.001, 0.01, 0.1, 1]\nparam_grid = {'C': Cs, 'gamma' : gammas}\ngrid_search = GridSearchCV(svm.SVC(kernel='rbf'), param_grid, cv=5, verbose=True)\n#grid_search.fit(X, y)\n#grid_search.best_params_\n#best_svc = grid_search.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_params_svc = {'C': 10, 'gamma': 0.01}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_svc = svm.SVC(**best_params_svc)\nbest_svc.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_svc = best_svc.predict(X_test_outfold)\nsub_svc = pd.DataFrame({\"Id\": id_test.values,\"Cover_Type\": preds_svc})\nsub_svc.to_csv(\"Stage_2_2_svc_out_of_fold.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Stage 2.3: Neural Networks\n> Training with artificial neural networks"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_nn = np_utils.to_categorical(stack_ds.y_train + 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Dense(1024, input_dim=49, kernel_initializer='uniform', activation='selu'))\nmodel.add(Dense(512, kernel_initializer='uniform', activation='softplus'))\nmodel.add(Dense(256, kernel_initializer='uniform', activation='elu'))\nmodel.add(Dense(128, kernel_initializer='uniform', activation='selu'))\nmodel.add(Dense(64, kernel_initializer='uniform', activation='softplus'))\nmodel.add(Dense(32, kernel_initializer='uniform', activation='elu'))\nmodel.add(Dense(16, kernel_initializer='uniform', activation='softplus'))\nmodel.add(Dense(8, kernel_initializer='uniform', activation='softmax'))\n# Compile model\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())\n# Fit model\nmodel.fit(X, y_train_nn, epochs=10, batch_size=32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_nn = model.predict(X_test_outfold)\nsub_nn = pd.DataFrame({\"Id\": id_test.values,\"Cover_Type\": np.argmax(preds_nn,axis=1)})\nsub_nn.to_csv(\"Stage_2_3_ann_out_of_fold.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Took reference from:\nhttps://www.kaggle.com/justfor/ensembling-and-stacking-with-heamy"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}