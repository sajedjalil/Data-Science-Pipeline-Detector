{"cells":[{"metadata":{"_uuid":"282de4e4a4be38327e0ebeddfb51571345ca2611"},"cell_type":"markdown","source":"<h1>Ensemble Learning with ExtraTrees</h1>\n\nIn this Tutorial I am going to explain in details how to classify the cover_type dataset using Ensemble learning. In many cases machine learning algorithms don't perform well without feature engineering which is the process of filling NaNs and missing values , creating new features and etc... . I will be performing some exploratory data analysis to perform feature engineering before implementing the suitable model."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nprint(os.listdir(\"../input\"))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d5c9260341f59916e7b13e946a55deb10002e89a"},"cell_type":"markdown","source":"Now we should load the train and test data into two seperate dataframes"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")\n\n# The following two lines determines the number of visible columns and \n#the number of visible rows for dataframes and that doesn't affect the code\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 500)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9779f47110b06f8b4d074dc92a676a4bc011f9ed"},"cell_type":"markdown","source":"<h1>**Data Exploration and Analysis**</h1>"},{"metadata":{"_uuid":"f94ac150f8a77111372bdfca111a8d77eb31751c"},"cell_type":"markdown","source":"Now we should go further to explore our data to be able to know which features to use and if we can synthesize new features. Now i will show the first 5 rows. "},{"metadata":{"trusted":true,"_uuid":"16fa1c06f2910a63aa6369ba75eede063b5c3d99"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1bb2dc24ddd365f71718ab70a0e24c9514ffc0f3"},"cell_type":"markdown","source":"Let's now see how many data points we have for training."},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"a9f35db9e241fbaffb6adfdf0b019a007d57f9bb"},"cell_type":"code","source":"print(\"The number of traning examples(data points) = %i \" % train.shape[0])\nprint(\"The number of features we have = %i \" % train.shape[1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1faada8ab307447da4bf8c75797d5c0b6c0a18b0"},"cell_type":"markdown","source":"Let's check if any of the columns contains NaNs or Nulls so that we can fill those values if they are insignificant or drop them. We may drop a whole column if most of its values are NaNs or fill its value according to its relation with other columns in the dataframe. Nones can also be 0 in some datasets and that is why i am going to use the describe of the train to see if the range of numbers is not reasonable or not. if you are dropping rows with NaNs and you notice that you need to drop a large portion of your dataset then you should think about filling the NaN values or drop a column that has most of its values missing."},{"metadata":{"trusted":true,"_uuid":"333ab6d0ba59277d4bc5b46d6af9752286d2edf5"},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7b4d2f7edb2d280e34bb2c6f479554f4653d6a4c"},"cell_type":"code","source":"train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"69edb39bdca020f68c4a9b068dadd4862c126284"},"cell_type":"markdown","source":"It seems we don't have any NaN or Null value among the dataset we are trying to classify. Let's now discover the correlation matrix for this dataset and see if we can combine features or drop some according to its correlation with the output labels."},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"370d9012a10f74a9587b60e18c0b338ff0c9301e"},"cell_type":"code","source":"import seaborn as sns\n\n\nimport matplotlib.pyplot as plt\n\n\ncorr = train.corr()\nf, ax = plt.subplots(figsize=(25, 25))\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(corr, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cc04710ccdaa02440eb2872217be9d3cf9189144","scrolled":true},"cell_type":"code","source":"corr","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0cfd5638410be5d8f8030eabdd723909a69bd0e1"},"cell_type":"markdown","source":"From the above results it seems that soil_Type7 and soil_Type15 doesn't haveany correlation with the output cover_Type so we can easily drop them from the data we have. Also Soil_Type9, Soil_Type36, Soil_Type27, Soil_Type25, Soil_Type8 have weak correlation, but when a feature has a weak correlation tht doesn't mean it is useful cuz combined with other feature it may make a good impact.  I choose those columns after experimenting many times with the data i have from the Extratrees, correlation matrix and the heatmap."},{"metadata":{"trusted":true,"_uuid":"8480bc76c93a34ecc04caa289e378d3cae4ebc61"},"cell_type":"code","source":"train.drop(['Id'], inplace = True, axis = 1 )\ntrain.drop(['Soil_Type15' , \"Soil_Type7\"], inplace = True, axis = 1 )\ntest.drop(['Soil_Type15' , \"Soil_Type7\"], inplace = True, axis = 1 )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"669f24bc28e6eda1feaa714b51daabfe598b0aff"},"cell_type":"markdown","source":"Let's now explore some relations between features that we can add later to make the algorithm perform better."},{"metadata":{"trusted":true,"_uuid":"30502b1a4f4759c3e09680ae26eade57073aa6b5"},"cell_type":"code","source":"import matplotlib.pyplot as plt\nclasses = np.array(list(train.Cover_Type.values))\n\ndef plotRelation(first_feature, sec_feature):\n    \n    plt.scatter(first_feature, sec_feature, c = classes, s=10)\n    plt.xlabel(first_feature.name)\n    plt.ylabel(sec_feature.name)\n    \nf = plt.figure(figsize=(25,20))\nf.add_subplot(331)\nplotRelation(train.Horizontal_Distance_To_Hydrology, train.Horizontal_Distance_To_Fire_Points)\nf.add_subplot(332)\nplotRelation(train.Horizontal_Distance_To_Hydrology, train.Horizontal_Distance_To_Roadways)\nf.add_subplot(333)\nplotRelation(train.Elevation, train.Vertical_Distance_To_Hydrology)\nf.add_subplot(334)\nplotRelation(train.Hillshade_9am, train.Hillshade_3pm)\nf.add_subplot(335)\nplotRelation(train.Horizontal_Distance_To_Fire_Points, train.Horizontal_Distance_To_Hydrology)\nf.add_subplot(336)\nplotRelation(train.Horizontal_Distance_To_Hydrology, train.Vertical_Distance_To_Hydrology)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ec47eafa0174498d963616ce2a9d4502377151c2"},"cell_type":"markdown","source":"As you can see there are some important relations that the model can infere from these new features according to the plots and also the correlation matrix and the heatmap. I will now add these features to the training data and the test data. I have read many resources as this [study](https://rstudio-pubs-static.s3.amazonaws.com/160297_f7bcb8d140b74bd19b758eb328344908.html), this grat [course](https://www.coursera.org/learn/competitive-data-science) and from that great [kernel](https://www.kaggle.com/codename007/forest-cover-type-eda-baseline-model).\n\nAlso it seems that the vertical distance contain some negative number and it gave me better performance when taken the absolute for the column. It is really important to notice that Tree based models only fits vertical and horizontal lines so it is very important to engineer some oblique or tilted features like slope and etc... ."},{"metadata":{"trusted":true,"_uuid":"8dd3f518259bbeef62494d3d7505dbe895c9f838"},"cell_type":"code","source":"# train.head()\ntrain['HorizontalHydrology_HorizontalFire'] = (train['Horizontal_Distance_To_Hydrology']+train['Horizontal_Distance_To_Fire_Points'])\ntrain['Neg_HorizontalHydrology_HorizontalFire'] = (train['Horizontal_Distance_To_Hydrology']-train['Horizontal_Distance_To_Fire_Points'])\ntrain['HorizontalHydrology_HorizontalRoadways'] = (train['Horizontal_Distance_To_Hydrology']+train['Horizontal_Distance_To_Roadways'])\ntrain['Neg_HorizontalHydrology_HorizontalRoadways'] = (train['Horizontal_Distance_To_Hydrology']-train['Horizontal_Distance_To_Roadways'])\ntrain['HorizontalFire_Points_HorizontalRoadways'] = (train['Horizontal_Distance_To_Fire_Points']+train['Horizontal_Distance_To_Roadways'])\ntrain['Neg_HorizontalFire_Points_HorizontalRoadways'] = (train['Horizontal_Distance_To_Fire_Points']-train['Horizontal_Distance_To_Roadways'])\n\ntrain['Neg_Elevation_Vertical'] = train['Elevation']-train['Vertical_Distance_To_Hydrology']\ntrain['Elevation_Vertical'] = train['Elevation']+train['Vertical_Distance_To_Hydrology']\n\ntrain['mean_hillshade'] =  (train['Hillshade_9am']  + train['Hillshade_Noon'] + train['Hillshade_3pm'] ) / 3\n\ntrain['Mean_HorizontalHydrology_HorizontalFire'] = (train['Horizontal_Distance_To_Hydrology']+train['Horizontal_Distance_To_Fire_Points'])/2\ntrain['Mean_HorizontalHydrology_HorizontalRoadways'] = (train['Horizontal_Distance_To_Hydrology']+train['Horizontal_Distance_To_Roadways'])/2\ntrain['Mean_HorizontalFire_Points_HorizontalRoadways'] = (train['Horizontal_Distance_To_Fire_Points']+train['Horizontal_Distance_To_Roadways'])/2\n\ntrain['MeanNeg_Mean_HorizontalHydrology_HorizontalFire'] = (train['Horizontal_Distance_To_Hydrology']-train['Horizontal_Distance_To_Fire_Points'])/2\ntrain['MeanNeg_HorizontalHydrology_HorizontalRoadways'] = (train['Horizontal_Distance_To_Hydrology']-train['Horizontal_Distance_To_Roadways'])/2\ntrain['MeanNeg_HorizontalFire_Points_HorizontalRoadways'] = (train['Horizontal_Distance_To_Fire_Points']-train['Horizontal_Distance_To_Roadways'])/2\n\ntrain['Slope2'] = np.sqrt(train['Horizontal_Distance_To_Hydrology']**2+train['Vertical_Distance_To_Hydrology']**2)\ntrain['Mean_Fire_Hydrology_Roadways']=(train['Horizontal_Distance_To_Fire_Points'] + train['Horizontal_Distance_To_Hydrology'] + train['Horizontal_Distance_To_Roadways']) / 3\ntrain['Mean_Fire_Hyd']=(train['Horizontal_Distance_To_Fire_Points'] + train['Horizontal_Distance_To_Hydrology']) / 2 \n\ntrain[\"Vertical_Distance_To_Hydrology\"] = abs(train['Vertical_Distance_To_Hydrology'])\n\ntrain['Neg_EHyd'] = train.Elevation-train.Horizontal_Distance_To_Hydrology*0.2\n\n\ntest['HorizontalHydrology_HorizontalFire'] = (test['Horizontal_Distance_To_Hydrology']+test['Horizontal_Distance_To_Fire_Points'])\ntest['Neg_HorizontalHydrology_HorizontalFire'] = (test['Horizontal_Distance_To_Hydrology']-test['Horizontal_Distance_To_Fire_Points'])\ntest['HorizontalHydrology_HorizontalRoadways'] = (test['Horizontal_Distance_To_Hydrology']+test['Horizontal_Distance_To_Roadways'])\ntest['Neg_HorizontalHydrology_HorizontalRoadways'] = (test['Horizontal_Distance_To_Hydrology']-test['Horizontal_Distance_To_Roadways'])\ntest['HorizontalFire_Points_HorizontalRoadways'] = (test['Horizontal_Distance_To_Fire_Points']+test['Horizontal_Distance_To_Roadways'])\ntest['Neg_HorizontalFire_Points_HorizontalRoadways'] = (test['Horizontal_Distance_To_Fire_Points']-test['Horizontal_Distance_To_Roadways'])\n\ntest['Neg_Elevation_Vertical'] = test['Elevation']-test['Vertical_Distance_To_Hydrology']\ntest['Elevation_Vertical'] = test['Elevation'] + test['Vertical_Distance_To_Hydrology']\n\ntest['mean_hillshade'] = (test['Hillshade_9am']  + test['Hillshade_Noon']  + test['Hillshade_3pm'] ) / 3\n\ntest['Mean_HorizontalHydrology_HorizontalFire'] = (test['Horizontal_Distance_To_Hydrology']+test['Horizontal_Distance_To_Fire_Points'])/2\ntest['Mean_HorizontalHydrology_HorizontalRoadways'] = (test['Horizontal_Distance_To_Hydrology']+test['Horizontal_Distance_To_Roadways'])/2\ntest['Mean_HorizontalFire_Points_HorizontalRoadways'] = (test['Horizontal_Distance_To_Fire_Points']+test['Horizontal_Distance_To_Roadways'])/2\n\ntest['MeanNeg_Mean_HorizontalHydrology_HorizontalFire'] = (test['Horizontal_Distance_To_Hydrology']-test['Horizontal_Distance_To_Fire_Points'])/2\ntest['MeanNeg_HorizontalHydrology_HorizontalRoadways'] = (test['Horizontal_Distance_To_Hydrology']-test['Horizontal_Distance_To_Roadways'])/2\ntest['MeanNeg_HorizontalFire_Points_HorizontalRoadways'] = (test['Horizontal_Distance_To_Fire_Points']-test['Horizontal_Distance_To_Roadways'])/2\n\ntest['Slope2'] = np.sqrt(test['Horizontal_Distance_To_Hydrology']**2+test['Vertical_Distance_To_Hydrology']**2)\ntest['Mean_Fire_Hydrology_Roadways']=(test['Horizontal_Distance_To_Fire_Points'] + test['Horizontal_Distance_To_Hydrology'] + test['Horizontal_Distance_To_Roadways']) / 3 \ntest['Mean_Fire_Hyd']=(test['Horizontal_Distance_To_Fire_Points'] + test['Horizontal_Distance_To_Hydrology']) / 2\n\n\ntest['Vertical_Distance_To_Hydrology'] = abs(test[\"Vertical_Distance_To_Hydrology\"])\n\ntest['Neg_EHyd'] = test.Elevation-test.Horizontal_Distance_To_Hydrology*0.2","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"278a7807c8638cc6f6774d5666977377c768cba4"},"cell_type":"markdown","source":"Now we should seperate the training set from the labels and name them x and y then we will split them into training and test sets to be able to see how well it would do on unseen data which will give anestimate on how well it will do when testing on Kaggle test data. I will use the convention of using 80% of the data as training set and 20% for the test set."},{"metadata":{"trusted":true,"_uuid":"f9ee27e349f745f85c32132cb9dbc047eff9e0ba"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"8284a1b7b2b3ab724e256ca2797c7d210da431e6"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx = train.drop(['Cover_Type'], axis = 1)\ny = train['Cover_Type']\nprint( y.head() )\n\nx_train, x_test, y_train, y_test = train_test_split( x.values, y.values, test_size=0.05, random_state=42 )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b6e180b0511b577a844cd93cc135e9d11d945a4c"},"cell_type":"markdown","source":"It is important to know if the number of points in the classes are balanced. If the data is skewed then we will not be able to use accuracy as a performance metric since it will be misleading but if it is skewed we may use F-beta score or precision and recall.  Precision or recall or F1 score. the choice depends on the problem itself. Where high recall means low number of false negatives , High precision means low number of false positives and     F1 score is a trade off between them. You can refere to this article for more about precision and recall http://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html"},{"metadata":{"trusted":true,"_uuid":"b851644f8c0ac6df69a35daade502864c1a33120"},"cell_type":"code","source":"unique, count= np.unique(y_train, return_counts=True)\nprint(\"The number of occurances of each class in the dataset = %s \" % dict (zip(unique, count) ), \"\\n\" )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9e1c61793c3f011867abd6b0251c76bd3d26cda6"},"cell_type":"markdown","source":"It seems the data points in each class are almost balanced so it will be okay to use accuracy as a metric to measure how well the ML model performs"},{"metadata":{"_uuid":"e1a95c5d1179e36b78f27db9605ecf36d7d7c5c7"},"cell_type":"markdown","source":"Lets now see if the new features we added have any segnificance for the extra tee model or not and how important are our features. We can check that through the Extra trees algorithm which can predict the useful features internally usign \"feature_importances\""},{"metadata":{"trusted":true,"_uuid":"88335b9d2361c90917e2a3e533c522b2e9dd5530"},"cell_type":"code","source":"from sklearn import datasets\nfrom sklearn import metrics\nfrom sklearn.ensemble import ExtraTreesClassifier\n# load the iris datasets\ndataset = datasets.load_iris()\n# fit an Extra Trees model to the data\nclf = ExtraTreesClassifier()\nclf.fit(x_train,y_train)\n# display the relative importance of each attribute\nz = clf.feature_importances_\n#make a dataframe to display every value and its column name\ndf = pd.DataFrame()\nprint(len(z))\nprint(len(list(x.columns.values)))\n\ndf[\"values\"] = z\ndf['column'] = list(x.columns.values)\n# Sort then descendingly to get the worst features at the end\ndf.sort_values(by='values', ascending=False, inplace = True)\ndf.head(100)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"17f1aee2236f13ec6c51358a18558d8084e725df"},"cell_type":"markdown","source":"Since we have only 15120 training examples then I have tried **SVMs** but it didn't give me great performance so i tried **Ensemble learning using Extra trees** instead and it gave me much better results than the SVM algorithm . If you don't know which estimator or algorithm to use you can check the Scikit Learn Cheat sheet below.\n![](http://scikit-learn.org/stable/_static/ml_map.png)"},{"metadata":{"_uuid":"c6d641d997b1dfc8d27e383d8fb3b4e0d35534ad"},"cell_type":"markdown","source":"When using ExtraTrees or even any machine learning algorithm it is very important to remember to perform feature scaling to make the model converge faster. Also if you plan to use SVM classifier it would perform better with compression techniques like [PCA](https://www.coursera.org/lecture/machine-learning/principal-component-analysis-algorithm-ZYIPa) ."},{"metadata":{"trusted":true,"_uuid":"31ea6f953b6037edce4e2362d67178da4bb217fe"},"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn import decomposition\n\nscaler = StandardScaler()\nscaler.fit(x_train)\nx_train = scaler.transform(x_train)\nx_test = scaler.transform(x_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9d38421ba7af29bb5533d9a05f23f98e0356037f"},"cell_type":"markdown","source":"Now it is time to fit the Extratrees classifier algorithm and for that we will use Scikit learn ExtraTreesClassifier and try to tune the parameters to reach the best performance. For the parameter tuning i will use gridsearchCV from Scikit-Learn to tune the parameters instead of manual tuning. To perform grid search uncomment the commented code and comment the uncommented code in the following cell. Gridsearch is an exhaustive algorithm that tries all combinations of hyperparameters specified in the param_grid, you can know more about how it [works here](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)."},{"metadata":{"trusted":true,"_uuid":"4de9c79f0e1fbb67b537481643263696060f2691"},"cell_type":"code","source":"train.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"76b0e7c0ac88f1cf8bdeee53324556f34d175f1c"},"cell_type":"code","source":"###### from sklearn.svm import SVC\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n#uncomment the commented code and uncomment the commented to perform gridsearchCV\nfrom xgboost import XGBClassifier\n\nclf = ExtraTreesClassifier(n_estimators=950, random_state=0)\n\nclf.fit(x_train, y_train)\nprint('Accuracy of classifier on training set: {:.2f}'.format(clf.score(x_train, y_train) * 100))\nprint('Accuracy of classifier on test set: {:.2f}'.format(clf.score(x_test, y_test) * 100))\n\n# n_estimators = np.linspace(start = 600 , stop = 1000, num = 8, dtype= int )\n# n_estimators = [500, 550, 600, 650, 700, 750, 800 , 850, 900, 950]\n\n# param_grid = {'n_estimators': n_estimators}\n# grid = GridSearchCV(clf, param_grid =param_grid, cv=3, n_jobs=-1, scoring='accuracy')\n# grid.fit(x_train, y_train)\n\n# print(\"The best parameters are %s with a score of %0.0f\" % (grid.best_params_, grid.best_score_ * 100 ))\n# print( \"Best estimator accuracy on test set {:.2f} \".format(grid.best_estimator_.score(x_test, y_test) * 100 ) )\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"158e22b0436e06245743926705b896ee4c88ded5"},"cell_type":"markdown","source":"The last thing to do now is to predict Kaggle test set to get the results and submit the result csv file."},{"metadata":{"trusted":true,"_uuid":"3463f1c44c3745492e8d0f278ac99c5d04bb6690"},"cell_type":"code","source":"test.head()\n\nid = test['Id']\ntest.drop(['Id'] , inplace = True , axis = 1)\n\ntest = scaler.transform(test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"029a8a08c3776df62dd35ba04f7f05da93f68f8e"},"cell_type":"markdown","source":"And now let's see the predictions using the predict function in sklearn"},{"metadata":{"trusted":true,"_uuid":"cebbeb049f98d1719addc1d7a7777d887cc41106"},"cell_type":"code","source":"#Uncomment the commented code and comment the other line to run the grid search predict\n\n# predictions = grid.best_estimator_.predict(test)\npredictions = clf.predict(test)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5fb3f0f195217cd6b9c8c77452fc044edbddaf6f"},"cell_type":"markdown","source":"Finally we should output the predictions in the format they want in the competition."},{"metadata":{"trusted":true,"_uuid":"afe93e3cd020bc43d3df80e5be4b621781117fda"},"cell_type":"code","source":"out = pd.DataFrame()\nout['Id'] = id\nout['Cover_Type'] = predictions\nout.to_csv('xyn_submission.csv', index=False)\nout.head(5)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}