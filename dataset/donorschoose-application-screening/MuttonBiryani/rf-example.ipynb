{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"# Basic Data manipulation and math functions\nimport pandas as pd\nimport numpy as np\nimport random\nimport scipy\n\n# File listing, creating directory paths etc. and memory management\nimport os\nimport gc\n\n# Garphing\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n# NLP specific and string functionalities\nimport re, string\nfrom nltk.tokenize import RegexpTokenizer\nfrom stop_words import get_stop_words\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.tokenize import TweetTokenizer\n\n# Iporting fucntions from the popular sklearn ML module\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import GridSearchCV, train_test_split","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Constants\nrun_gridsearch__=False\nnrows__ = 1000\nfit_sample_size__=1.1\nrf_n_estimators__=1000\ntrain_inloc_ = '../input/train.csv'\ntest_inloc_ = '../input/test.csv'\nlabels_ = []\nen_stop_ = get_stop_words('en')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c639761e58d70c66089af98fd30dd1ddc82791a3"},"cell_type":"code","source":"p_stemmer = PorterStemmer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"52bca4ebb4ed529946ace37bd65dc1b49632752c","collapsed":true},"cell_type":"code","source":"rsrc = pd.read_csv(\"../input/resources.csv\")\ntrain = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2c29db3f5f0cdd046b5e622b862e3680ccb8d605","collapsed":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4d5f35ccf281bef7e3ec605d43f87164fe3b003f","collapsed":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"53b55ad9b20847ec14c3bb08ec4a9167b5e1c7f7","collapsed":true},"cell_type":"code","source":"rsrc.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6fc30a5a5da2b8bf4220545d15d59cc900b733c6","collapsed":true},"cell_type":"code","source":"print(\"Train:\", train.shape)\nprint(\"Test:\", test.shape)\nprint(\"Resource:\", rsrc.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e55136e0bfb077eb02fce49c7d72637e921f01b3"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c84145afb768320f025b2cf9f7cf68055909b3ab"},"cell_type":"markdown","source":"### Check for Missing Values"},{"metadata":{"trusted":true,"_uuid":"be54ff33a0c808cddca537b6bbf6e414c48c0118","collapsed":true},"cell_type":"code","source":"train.isnull().sum()[train.isnull().sum()>0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"573833d8c74e7b8a4482f155fc5bda759deb2624","collapsed":true},"cell_type":"code","source":"test.isnull().sum()[test.isnull().sum()>0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8351ea7ad7a5dd585b7dc9f487bbed50832f503f","collapsed":true},"cell_type":"code","source":"rsrc.isnull().sum()[rsrc.isnull().sum()>0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c3cd08183ef097e0e96d99a6526c3606f317e7b0"},"cell_type":"markdown","source":"### Treat misisng values\nThere are various ways in which missing values can be treated. Given that here values are missing for textual columns, we just replace it with '' (an empty string)\n"},{"metadata":{"trusted":true,"_uuid":"c4ad40a71efaedef775dcc3db1a7eb2375928bde","collapsed":true},"cell_type":"code","source":"train = train.fillna('')\ntest = test.fillna('')\nrsrc = rsrc.fillna('')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d37f85783c637c49239d1097836cfeaaaf244379"},"cell_type":"markdown","source":"### Combine the 4 essays"},{"metadata":{"trusted":true,"_uuid":"820d3f045d3366e1ec1d724319268416b816fc76","collapsed":true},"cell_type":"code","source":"# Combining the project essay 1, 2, 3, 4 \nfor df in [train, test]:\n    df[\"essays\"] = df[\"project_essay_1\"] + df[\"project_essay_2\"] + df[\"project_essay_3\"] + df[\"project_essay_4\"]\n    df.drop(['project_essay_1', 'project_essay_2', 'project_essay_3', 'project_essay_4'], axis=1, inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"efc9868d7ed97b26fd90e1449fbf5d7f0f574ffa"},"cell_type":"markdown","source":"### Let's do some quick data exploration"},{"metadata":{"trusted":true,"_uuid":"04d8d8a00bb18b1d21fa60e140c4155661df0609","collapsed":true},"cell_type":"code","source":"def data_preview_str(df, name):\n    tot_feats = 0\n    if name=='train':\n        for col in [i for i in df.columns if (df[i].dtypes=='O') & (i.find('_id')<0)]:\n            tot_feats = tot_feats + len(df[col].unique())-1\n            print(\" %s \"% col, '# unique: ',len(df[col].unique()))\n            xdf = df.groupby(col).agg({'id':'count','project_is_approved':'sum'})\n            xdf.columns = ['#Records','Response_Rate']\n            xdf['Response_Rate'] = xdf['Response_Rate']/sum(xdf['Response_Rate'])\n            xdf['%Records'] = xdf['#Records']*100/sum(xdf['#Records'])\n            xdf.sort_values('#Records', ascending=[0], inplace=True)\n            print(xdf.loc[:,['#Records','%Records','Response_Rate']].head(10))\n            print(\"%d of %d\"%(min(10,xdf.shape[0]), xdf.shape[0]))\n            del xdf\n            print(\"-\"*50)        \n    else:\n        for col in [i for i in df.columns if (df[i].dtypes=='O') or (len(df[i].unique())<=15)]:\n            tot_feats = tot_feats + len(df[col].unique())-1\n            print(\" %s \"% col, '# unique: ',len(df[col].unique()))\n            xdf = pd.DataFrame(df[col].value_counts())\n            xdf['%Records'] = xdf[col]*100/sum(xdf[col])\n            xdf.columns = ['#Records','%Records']\n            print(xdf.head())\n            del xdf\n            print(\"-\"*50)\n    ll=[i for i in df.columns if (df[i].dtypes!='O') and (len(df[i].unique())>15)]\n    tot_feats= tot_feats + len(ll)\n    print(\"Creating dummies will lead to a total of %d features\"% tot_feats)\n    return 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7b0f7bdfc9e33c86bfca37bcc97fc4a9c980b077","scrolled":true,"collapsed":true},"cell_type":"code","source":"data_preview_str(train, 'train')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9fc5ff433c974c18ded309ab75f177e150d0f9fe","collapsed":true},"cell_type":"code","source":"data_preview_str(test, 'test')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c623305d3a140610342cc1e19d206a301d4741ab"},"cell_type":"markdown","source":"### Let us build some simple features based on the EDA above"},{"metadata":{"trusted":true,"_uuid":"8f751ffa123238c908fdca8a0da2139ca2deb552","collapsed":true},"cell_type":"code","source":"# Combine Train and Test ( for easier submissions)\ntrain['train_flag'] = 1\ntest['train_flag'] = 0\nfull_data = pd.concat([train,test], sort=False)\nfull_data.groupby('train_flag')['id','project_is_approved'].count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"43c400fbe75134ddba3d576e80b4f69a2ff0a9d6","collapsed":true},"cell_type":"code","source":"# Dummy or Flag Features\nfull_data = pd.get_dummies(full_data, columns =['teacher_prefix','school_state','project_grade_category'])\n# full_data.drop(['teacher_prefix','school_state','project_grade_category'], axis=1, inplace=True)\nfull_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"501ffd50b4f224a34af5e8269e3ad394b3c9f602","scrolled":true,"collapsed":true},"cell_type":"code","source":"# Date features\nfull_data['project_submitted_datetime'] = pd.to_datetime(full_data['project_submitted_datetime'])\nfull_data['day'] = full_data['project_submitted_datetime'].dt.day\nfull_data['dayofweek'] = full_data['project_submitted_datetime'].dt.dayofweek\nfull_data['month'] = full_data['project_submitted_datetime'].dt.month\nfull_data['year'] = full_data['project_submitted_datetime'].dt.year\nfull_data.drop('project_submitted_datetime', axis=1, inplace=True)\nfull_data[['day','dayofweek','month','year']].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d70b5f43cb816da4a4c29a805aa16fdffd6e266e","collapsed":true},"cell_type":"code","source":"# Project Title attributes : First impression is a Last-ing one\nfull_data['pt_caps'] = full_data['project_title'].str.findall(r'[A-Z]').str.len()/full_data['project_title'].str.len()\nfull_data['pt_special_chars'] = full_data['project_title'].str.findall(r'[^A-Za-z0-9]').str.len()/full_data['project_title'].str.len()\nfull_data['pt_len'] = full_data['project_title'].str.len()\nfull_data['pt_words'] = full_data['project_title'].str.findall(r'[\\s]').str.len()/full_data['project_title'].str.len()\n\n# project_resource_summary attributes\nfull_data['prs_caps'] = full_data['project_resource_summary'].str.findall(r'[A-Z]').str.len()/full_data['project_title'].str.len()\nfull_data['prs_special_chars'] = full_data['project_resource_summary'].str.findall(r'[^A-Za-z0-9]').str.len()/full_data['project_title'].str.len()\nfull_data['prs_len'] = full_data['project_resource_summary'].str.len()\nfull_data['prs_words'] = full_data['project_resource_summary'].str.findall(r'[\\s]').str.len()/full_data['project_title'].str.len()\n\n# essays\nfull_data['ess_caps'] = full_data['essays'].str.findall(r'[A-Z]').str.len()/full_data['project_title'].str.len()\nfull_data['ess_special_chars'] = full_data['essays'].str.findall(r'[^A-Za-z0-9]').str.len()/full_data['project_title'].str.len()\nfull_data['ess_len'] = full_data['essays'].str.len()\nfull_data['ess_words'] = full_data['essays'].str.findall(r'[\\s]').str.len()/full_data['project_title'].str.len()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e97d27edab947e7c49c746eefc7913c772fb0a28","collapsed":true},"cell_type":"code","source":"# aggregating it based on the project id\n\nrsrc[\"description\"].fillna(\"\", inplace = True)\n\nrsrc_grp = pd.DataFrame(rsrc.groupby(\"id\").agg({\"description\" : lambda x : \"\".join(x),\n                   \"quantity\": [\"sum\", \"mean\"],\n                   \"price\" : [\"sum\", \"mean\"]}))\n\nrsrc_grp.reset_index(inplace = True)\nrsrc_grp.columns.droplevel(0)\nrsrc_grp.columns= [\"id\", \"description\", \"quantity_sum\",\n                \"quantity_mean\", \"price_sum\", \"price_mean\"]\n# Merge the aggregated data with the combined Data frame\nprint(\"Before merge:\",full_data.shape)\nfull_data = pd.merge(full_data, rsrc_grp, on = \"id\", how = \"left\")\nprint(\"After merge:\",full_data.shape)\ndel rsrc_grp","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2c0edac61b36969c15692ce2bd46ec3a1781e0a8"},"cell_type":"markdown","source":"### Let us create TFiDF features from our texts\n**tf–idf** or **TFIDF**, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.[1] It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The tf-idf value increases proportionally to the number of times a word appears in the document and is offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appear more frequently in general. Tf-idf is one of the most popular term-weighting schemes today; 83% of text-based recommender systems in digital libraries use tf-idf."},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"108542d231d245b14238c9cd89dbd86101919a7f","collapsed":true},"cell_type":"code","source":"text_cols = [\n    'project_title', \n    'essays', \n    'project_resource_summary',\n    'description'\n]\nnfeats=5\n\nprint(full_data[text_cols].head(1).T)\nfrom tqdm import tqdm\n\nfor c in text_cols:\n    tfidf = TfidfVectorizer(\n        max_features=nfeats,\n        norm='l2',\n        sublinear_tf = True,\n        stop_words = 'english',\n        analyzer = 'word',\n        min_df = 5,\n        max_df = .9,\n        smooth_idf = False)\n    \n    print(\"*** %s ***\"% c)\n    print(tfidf)\n    \n    tfidf.fit(full_data[c])\n    tfidf_train = np.array(tfidf.transform(full_data[c]).toarray(), dtype=np.float16)\n    for i in range(nfeats):\n        full_data[c + '_tfidf_' + str(i)] = tfidf_train[:, i]\n    del tfidf, tfidf_train\n    gc.collect()\n    \nprint('Done.')\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dd3b6dc936267a4212cf70de2c04e78328dd13db"},"cell_type":"markdown","source":"### Let us now get rid of extra column sand prep the data for modeling"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"676edb6def9bcc788f03499d5862f1b6c14f71c5","collapsed":true},"cell_type":"code","source":"remcols = [\n    'id',\n    'teacher_id',\n    'project_title', \n    'essays', \n    'project_resource_summary',\n    'description',\n    'project_subject_categories', 'project_subject_subcategories', 'project_is_approved'\n]\n\n\ntrain_idcol = full_data.loc[full_data['train_flag']==1,'id']\ntrain_teacher_idcol = full_data.loc[full_data['train_flag']==1,:'teacher_id']\n\ntest_idcol = full_data.loc[full_data['train_flag']==1,'id']\ntest_teacher_idcol = full_data.loc[full_data['train_flag']==1,:'teacher_id']\n\n\nfull_data.drop(remcols, axis = 1, inplace = True)\n\nX = full_data.loc[full_data['train_flag']==1, :].reset_index()\ny = train[\"project_is_approved\"].reset_index()\n\nprint(X.shape, y.shape)\n\nX_test = full_data.loc[full_data['train_flag']==0, :].reset_index()\n\ndel train, test\n\n#  Data Splitting for validation\n\nxTrain, xTest, yTrain, yTest = train_test_split(X, y, test_size = 0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"48dd7e33f9457778102ef71da5e12e4258e38213","collapsed":true},"cell_type":"code","source":"yTrain.drop('index', axis=1, inplace=True)\nyTest.drop('index', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"88f268c2cb66320d2bcb4a13c1e72642a4b45b1d"},"cell_type":"markdown","source":"## Usually we perform a large amount of feature engineering to understand the problem and build better models. With the limited time we stop here and start fitting an RF model"},{"metadata":{"trusted":true,"_uuid":"d112459a38312ad198b35e1da10382b84c65def4","collapsed":true},"cell_type":"code","source":"xTrain.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"df30fb327726429fe97cd23f440ae4a26d734458","collapsed":true},"cell_type":"code","source":"first_rf = RandomForestClassifier(n_estimators=1,random_state=1)\nfirst_rf.fit(xTrain, yTrain.values.ravel())\nimportance = first_rf.feature_importances_\nimportance = pd.DataFrame(importance, index=xTrain.columns, \n                          columns=[\"Importance\"])\n\nimportance[\"Std\"] = np.std([tree.feature_importances_\n                            for tree in first_rf.estimators_], axis=0)\n\nimportance.sort_values('Importance', ascending=[0], inplace=True)\n\nx = importance.index\ny = importance['Importance']\nyerr = importance['Std']\nplt.figure(figsize=(50,5))\nplt.bar(x, y, yerr=yerr, align=\"center\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bf954f7dc550a84a01f66f2c0584242f8f16d6fc"},"cell_type":"markdown","source":"### There i smore to it, let us tune this model"},{"metadata":{"trusted":true,"_uuid":"5d9cb5407cbfeca028c2ab5716e0732ea3fe51da","collapsed":true},"cell_type":"code","source":"score_dict = {'AUC': 'roc_auc'}\n# clf = RandomForestClassifier(max_depth=2, random_state=0)\nclf = GridSearchCV(\n    RandomForestClassifier(random_state=0),\n    param_grid={\n        'n_estimators':[1],\n        'max_depth':[2, 4, 8, 10, 12, 14, 16, 18, 20, 22, 24]},\n    scoring = score_dict,\n    cv=5,refit='AUC')\n\nclfs = clf.fit(xTrain,yTrain.values.ravel())\n\nresults = clfs.cv_results_\n\n\npd.DataFrame(results['mean_test_AUC']).rename(columns= {0:'AUC'}).plot()\n\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}