{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importing Library\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\") #for Ignoring Warnings\n\nimport sqlite3                 #To get the data from Database\nimport pandas as pd            #\nimport numpy as np\nimport nltk\nimport string\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc\nfrom nltk.stem.porter import PorterStemmer\n\nimport re\n# Tutorial about Python regular expressions: https://pymotw.com/2/re/\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nfrom gensim.models import Word2Vec\nfrom gensim.models import KeyedVectors\nimport pickle\n\nfrom tqdm import tqdm\nimport os\n\nfrom plotly import plotly\nimport plotly.offline as offline\nimport plotly.graph_objs as go\noffline.init_notebook_mode()\nfrom collections import Counter\n\n#from sklearn.cross_validation import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score\n#from collections import counter\n#from sklearn.model_selection import cross_Validate\n\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importing the project data and resource data\nproject_data = pd.read_csv('../input/donorschoose-application-screening/train.csv')\nresource_data = pd.read_csv('../input/donorschoose-application-screening/resources.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Shape of the project and resource data\nprint(\"project data shape\" , project_data.shape)\nprint(\"resource data shape\", resource_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#display 1st five columns of project data\ndisplay(project_data.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#display 1st five columns of resource data\ndisplay(resource_data.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Class value count in project data\nproject_data['project_is_approved'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''#Downsampling (project_is_approved==1) class data to balance the class\n#https://elitedatascience.com/imbalanced-classes\n\nfrom sklearn.utils import resample\nfrom sklearn.utils import shuffle\nmajority = project_data[project_data['project_is_approved']==1]\nminority = project_data[project_data['project_is_approved']==0]\n\nmajority_downsample = resample(majority,replace=False,n_samples=27734,random_state=123)\n\nproject_data = pd.concat([majority_downsample,minority])\n\nproject_data['project_is_approved'].value_counts()\n\nproject_data = shuffle(project_data)  '''\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#merge resource data with project data\n# https://stackoverflow.com/questions/22407798/how-to-reset-a-dataframes-indexes-for-all-groups-in-one-step\nprice_data = resource_data.groupby('id').agg({'price':'sum','quantity':'sum'}).reset_index()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#merge all essay\nproject_data[\"essay\"] = project_data[\"project_essay_1\"].map(str) +\\\n                        project_data[\"project_essay_2\"].map(str) + \\\n                        project_data[\"project_essay_3\"].map(str) + \\\n                        project_data[\"project_essay_4\"].map(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"project_data.drop(['project_essay_1','project_essay_2','project_essay_3','project_essay_4'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#join two dataframes \nproject_data =pd.merge(project_data,price_data,on='id',how='left',copy=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"project_data.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**TEXT PREPROCESSING**"},{"metadata":{},"cell_type":"markdown","source":"*ESSAY*"},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://stackoverflow.com/a/47091490/4084039\nimport re\n\ndef decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    phrase = re.sub('[^A-Za-z0-9]+', ' ', phrase)\n    return phrase","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n            'won', \"won't\", 'wouldn', \"wouldn't\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"project_data = project_data[:100]\nproject_data['project_is_approved'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reading glove vectors in python: https://stackoverflow.com/a/38230349/4084039\ndef loadGloveModel(gloveFile):\n    print (\"Loading Glove Model\")\n    f = open(gloveFile,'r', encoding=\"utf8\")\n    model = {}\n    for line in tqdm(f):\n        splitLine = line.split()\n        word = splitLine[0]\n        embedding = np.array([float(val) for val in splitLine[1:]])\n        model[word] = embedding\n    print (\"Done.\",len(model),\" words loaded!\")\n    return model\nmodel = loadGloveModel('../input/glove2word2vec/glove_w2v.txt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Combining all the above statemennts \nfrom tqdm import tqdm\npreprocessed_essays = []\n# tqdm is for printing the status bar\nfor sentance in tqdm(project_data['essay'].values):\n    sent = decontracted(sentance)\n    sent = sent.replace('\\\\r', ' ')\n    sent = sent.replace('\\\\\"', ' ')\n    sent = sent.replace('\\\\n', ' ')\n    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n    #'[^A-Za-z0-9]+'\n    sent1=[]\n    for e in sent.split():\n        if e not in stopwords:\n            \n            if e.isdigit():\n                sent1.append('')\n            else:\n                sent1.append(e)\n        s= ' '.join(sent1)\n    preprocessed_essays.append(s.lower())\n        \n   \n    \n\n    \n  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_cat_list=[]\nfor i in project_data['project_subject_subcategories']:\n    temp = \"\"\n    for j in i.split(','):\n        if 'The' in j.split():\n            j=j.replace('The','')\n        j=j.replace(' ','')\n        j=j.replace('&','_')\n        temp= temp+\" \"+j\n        #print(temp)\n    sub_cat_list.append(temp)\n    \n#print(sub_cat_list)\nproject_data['clean_subcategories']=sub_cat_list\n#project_data.drop(['project_subject_subcategories'],axis=1,inplace=True)\n\n\n    \n\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_list=[]\nfor i in project_data['project_subject_categories']:\n    temp = \"\"\n    for j in i.split(','):\n        if 'The' in j.split():\n            j=j.replace('The','')\n        j=j.replace(' ','')\n        j=j.replace('&','_')\n        temp= temp+\" \"+j\n        #print(temp)\n    cat_list.append(temp)\n    \n#print(sub_cat_list)\nproject_data['clean_categories']=cat_list\n#project_data.drop(['project_subject_categories'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"project_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"project_data['essay']=preprocessed_essays\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_data = pd.DataFrame(columns=['clean_categories', 'clean_subcategories', 'title','price','essay'])\nknn_data_label = pd.DataFrame(columns=['label'])\nknn_data['clean_categories']=project_data['clean_categories']\nknn_data['clean_subcategories']=project_data['clean_subcategories']\nknn_data['title']=project_data['project_title']\nknn_data['price']=project_data['price']\nknn_data['essay']=project_data['essay']\nknn_data_label['label']= project_data['project_is_approved']\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils import resample\nfrom sklearn.utils import shuffle\nx_1,x_test,y_1,y_test = train_test_split(knn_data,knn_data_label,test_size=0.3,random_state=0)\nx_tr,x_cv,y_tr,y_cv = train_test_split(x_1,y_1,test_size=0.3)\n\n\n\nx_tr['label']= y_tr['label']\ntrain_data_0 = x_tr[x_tr['label']==0]\ntrain_data_1 = x_tr[x_tr['label']==1]\ncount_class_1, count_class_0 = x_tr.label.value_counts()\ntrain_data_0 = train_data_0.sample(count_class_1,replace=True)\ntrain_data = pd.concat([train_data_0,train_data_1])\ntrain_data = shuffle(train_data)\ny_tr=train_data['label']\ny_tr.value_counts()\nx_tr= train_data.drop(columns=\"label\")\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#y_tr.label.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#BOW#\n#essay\n#TRAIN\ncount_vect = CountVectorizer() \ncount_vect.fit(x_tr['essay'])\nbow_essay_tr = count_vect.transform(x_tr['essay'])\ndisplay(bow_essay_tr.shape)\n\n#essay\n#cv\nbow_essay_cv = count_vect.transform(x_cv['essay'])\ndisplay(bow_essay_cv.shape)\n\n#essay\n#test\nbow_essay_test = count_vect.transform(x_test['essay'])\ndisplay(bow_essay_test.shape)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#BOW#\n#title\n#TRAIN\ncount_vect = CountVectorizer() \ncount_vect.fit(x_tr['title'])\nbow_title_tr = count_vect.transform(x_tr['title'])\ndisplay(bow_title_tr.shape)\n\n#title\n#cv\nbow_title_cv = count_vect.transform(x_cv['title'])\ndisplay(bow_title_cv.shape)\n\n#title\n#test\nbow_title_test = count_vect.transform(x_test['title'])\ndisplay(bow_title_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#BOW#\n#categories\n#TRAIN\ncount_vect = CountVectorizer() \ncount_vect.fit(x_tr['clean_categories'])\nbow_categories_tr = count_vect.transform(x_tr['clean_categories'])\ndisplay(bow_categories_tr.shape)\n\n#categories\n#cv\nbow_categories_cv = count_vect.transform(x_cv['clean_categories'])\ndisplay(bow_categories_cv.shape)\n\n#categories\n#test\nbow_categories_test = count_vect.transform(x_test['clean_categories'])\ndisplay(bow_categories_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#BOW#\n#subcategories\n#TRAIN\ncount_vect = CountVectorizer() \ncount_vect.fit(x_tr['clean_subcategories'])\nbow_subcategories_tr = count_vect.transform(x_tr['clean_subcategories'])\ndisplay(bow_subcategories_tr.shape)\n\n#subcategories\n#cv\nbow_subcategories_cv = count_vect.transform(x_cv['clean_subcategories'])\ndisplay(bow_subcategories_cv.shape)\n\n#subcategories\n#test\nbow_subcategories_test = count_vect.transform(x_test['clean_subcategories'])\ndisplay(bow_subcategories_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#TFIDF\n#title\n#train\n\ntf_idf_vect = TfidfVectorizer(min_df=10)\ntf_idf_vect.fit(x_tr['title'])\ntfidf_title_tr = tf_idf_vect.transform(x_tr['title'])\nprint(tfidf_title_tr.shape)\n\n#TFIDF\n#title\n#cv\n\ntfidf_title_cv = tf_idf_vect.transform(x_cv['title'])\nprint(tfidf_title_cv.shape)\n\n#TFIDF\n#title\n#test\n\ntfidf_title_test = tf_idf_vect.transform(x_test['title'])\nprint(tfidf_title_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#TFIDF\n#essay\n#train\n\ntf_idf_vect = TfidfVectorizer(min_df=10)\ntf_idf_vect.fit(x_tr['essay'])\ntfidf_essay_tr = tf_idf_vect.transform(x_tr['essay'])\nprint(tfidf_essay_tr.shape)\n\n#TFIDF\n#essay\n#cv\n\ntfidf_essay_cv = tf_idf_vect.transform(x_cv['essay'])\nprint(tfidf_essay_cv.shape)\n\n#TFIDF\n#essay\n#test\n\ntfidf_essay_test = tf_idf_vect.transform(x_test['essay'])\nprint(tfidf_essay_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#TFIDF\n#categories\n#train\n\ntf_idf_vect = TfidfVectorizer(min_df=10)\ntf_idf_vect.fit(x_tr['clean_categories'])\ntfidf_categories_tr = tf_idf_vect.transform(x_tr['clean_categories'])\nprint(tfidf_categories_tr.shape)\n\n#TFIDF\n#categories\n#cv\n\ntf_idf_vect = TfidfVectorizer(min_df=10)\ntf_idf_vect.fit(x_cv['clean_categories'])\ntfidf_categories_cv = tf_idf_vect.transform(x_cv['clean_categories'])\nprint(tfidf_categories_cv.shape)\n\n#TFIDF\n#categories\n#test\n\ntf_idf_vect = TfidfVectorizer(min_df=10)\ntf_idf_vect.fit(x_test['clean_categories'])\ntfidf_categories_test = tf_idf_vect.transform(x_test['clean_categories'])\nprint(tfidf_categories_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#TFIDF\n#subcategories\n#train\n\ntf_idf_vect = TfidfVectorizer(min_df=10)\ntf_idf_vect.fit(x_tr['clean_subcategories'])\ntfidf_subcategories_tr = tf_idf_vect.transform(x_tr['clean_subcategories'])\nprint(tfidf_subcategories_tr.shape)\n\n#TFIDF\n#subcategories\n#cv\n\n\ntfidf_subcategories_cv = tf_idf_vect.transform(x_cv['clean_subcategories'])\nprint(tfidf_subcategories_cv.shape)\n\n#TFIDF\n#subacategories\n#test\n\n\ntfidf_subcategories_test = tf_idf_vect.transform(x_test['clean_subcategories'])\nprint(tfidf_subcategories_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# average Word2Vec\n# compute average word2vec for each essay.\n#Train\navg_w2v_essay_tr = []; # the avg-w2v for each sentence/review is stored in this list\nfor sentence in tqdm(x_tr['essay']): # for each review/sentence\n    vector = np.zeros(200) # as word vectors are of zero length\n    cnt_words =0; # num of words with a valid vector in the sentence/review\n    for word in sentence.split(): # for each word in a review/sentence\n        if word in model :\n            vector += model[word]\n            cnt_words += 1\n    if cnt_words != 0:\n        vector /= cnt_words\n    avg_w2v_essay_tr.append(vector)\n    \navg_w2v_essay_tr=np.asarray(avg_w2v_essay_tr)  #converting list into array\ndisplay(avg_w2v_essay_tr.shape)\n\n#CV\navg_w2v_essay_cv = []; # the avg-w2v for each sentence/review is stored in this list\nfor sentence in tqdm(x_cv['essay']): # for each review/sentence\n    vector = np.zeros(200) # as word vectors are of zero length\n    cnt_words =0; # num of words with a valid vector in the sentence/review\n    for word in sentence.split(): # for each word in a review/sentence\n        if word in model :\n            vector += model[word]\n            cnt_words += 1\n    if cnt_words != 0:\n        vector /= cnt_words\n    avg_w2v_essay_cv.append(vector)\n    \navg_w2v_essay_cv=np.asarray(avg_w2v_essay_cv)\ndisplay(avg_w2v_essay_cv.shape)\n\n\n#TEST\navg_w2v_essay_test = []; # the avg-w2v for each sentence/review is stored in this list\nfor sentence in tqdm(x_test['essay']): # for each review/sentence\n    vector = np.zeros(200) # as word vectors are of zero length\n    cnt_words =0; # num of words with a valid vector in the sentence/review\n    for word in sentence.split(): # for each word in a review/sentence\n        if word in model :\n            vector += model[word]\n            cnt_words += 1\n    if cnt_words != 0:\n        vector /= cnt_words\n    avg_w2v_essay_test.append(vector)\navg_w2v_essay_test=np.asarray(avg_w2v_essay_test)\ndisplay(avg_w2v_essay_test.shape)\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# average Word2Vec\n# compute average word2vec for each title.\n#Train\navg_w2v_title_tr = []; # the avg-w2v for each sentence/review is stored in this list\nfor sentence in tqdm(x_tr['title']): # for each review/sentence\n    vector = np.zeros(200) # as word vectors are of zero length\n    cnt_words =0; # num of words with a valid vector in the sentence/review\n    for word in sentence.split(): # for each word in a review/sentence\n        if word in model :\n            vector += model[word]\n            cnt_words += 1\n    if cnt_words != 0:\n        vector /= cnt_words\n    avg_w2v_title_tr.append(vector)\n    \navg_w2v_title_tr=np.asarray(avg_w2v_title_tr)  #converting list into array\ndisplay(avg_w2v_title_tr.shape)\n\n#CV\navg_w2v_title_cv = []; # the avg-w2v for each sentence/review is stored in this list\nfor sentence in tqdm(x_cv['title']): # for each review/sentence\n    vector = np.zeros(200) # as word vectors are of zero length\n    cnt_words =0; # num of words with a valid vector in the sentence/review\n    for word in sentence.split(): # for each word in a review/sentence\n        if word in model :\n            vector += model[word]\n            cnt_words += 1\n    if cnt_words != 0:\n        vector /= cnt_words\n    avg_w2v_title_cv.append(vector)\n    \navg_w2v_title_cv=np.asarray(avg_w2v_title_cv)\ndisplay(avg_w2v_title_cv.shape)\n\n\n#TEST\navg_w2v_title_test = []; # the avg-w2v for each sentence/review is stored in this list\nfor sentence in tqdm(x_test['title']): # for each review/sentence\n    vector = np.zeros(200) # as word vectors are of zero length\n    cnt_words =0; # num of words with a valid vector in the sentence/review\n    for word in sentence.split(): # for each word in a review/sentence\n        if word in model :\n            vector += model[word]\n            cnt_words += 1\n    if cnt_words != 0:\n        vector /= cnt_words\n    avg_w2v_title_test.append(vector)\navg_w2v_title_test=np.asarray(avg_w2v_title_test)\ndisplay(avg_w2v_title_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# average Word2Vec\n# compute average word2vec for each subcategories.\n#Train\navg_w2v_clean_subcategories_tr = []; # the avg-w2v for each sentence/review is stored in this list\nfor sentence in tqdm(x_tr['clean_subcategories']): # for each review/sentence\n    vector = np.zeros(200) # as word vectors are of zero length\n    cnt_words =0; # num of words with a valid vector in the sentence/review\n    for word in sentence.split(): # for each word in a review/sentence\n        if word in model :\n            vector += model[word]\n            cnt_words += 1\n    if cnt_words != 0:\n        vector /= cnt_words\n    avg_w2v_clean_subcategories_tr.append(vector)\n    \navg_w2v_clean_subcategories_tr=np.asarray(avg_w2v_clean_subcategories_tr)  #converting list into array\ndisplay(avg_w2v_clean_subcategories_tr.shape)\n\n#CV\navg_w2v_clean_subcategories_cv = []; # the avg-w2v for each sentence/review is stored in this list\nfor sentence in tqdm(x_cv['clean_subcategories']): # for each review/sentence\n    vector = np.zeros(200) # as word vectors are of zero length\n    cnt_words =0; # num of words with a valid vector in the sentence/review\n    for word in sentence.split(): # for each word in a review/sentence\n        if word in model :\n            vector += model[word]\n            cnt_words += 1\n    if cnt_words != 0:\n        vector /= cnt_words\n    avg_w2v_clean_subcategories_cv.append(vector)\n    \navg_w2v_clean_subcategories_cv=np.asarray(avg_w2v_clean_subcategories_cv)\ndisplay(avg_w2v_clean_subcategories_cv.shape)\n\n\n#TEST\navg_w2v_clean_subcategories_test = []; # the avg-w2v for each sentence/review is stored in this list\nfor sentence in tqdm(x_test['clean_subcategories']): # for each review/sentence\n    vector = np.zeros(200) # as word vectors are of zero length\n    cnt_words =0; # num of words with a valid vector in the sentence/review\n    for word in sentence.split(): # for each word in a review/sentence\n        if word in model :\n            vector += model[word]\n            cnt_words += 1\n    if cnt_words != 0:\n        vector /= cnt_words\n    avg_w2v_clean_subcategories_test.append(vector)\navg_w2v_clean_subcategories_test=np.asarray(avg_w2v_clean_subcategories_test)\ndisplay(avg_w2v_clean_subcategories_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# average Word2Vec\n# compute average word2vec for each categories.\n#Train\navg_w2v_clean_categories_tr = []; # the avg-w2v for each sentence/review is stored in this list\nfor sentence in tqdm(x_tr['clean_categories']): # for each review/sentence\n    vector = np.zeros(200) # as word vectors are of zero length\n    cnt_words =0; # num of words with a valid vector in the sentence/review\n    for word in sentence.split(): # for each word in a review/sentence\n        if word in model :\n            vector += model[word]\n            cnt_words += 1\n    if cnt_words != 0:\n        vector /= cnt_words\n    avg_w2v_clean_categories_tr.append(vector)\n    \navg_w2v_clean_categories_tr=np.asarray(avg_w2v_clean_categories_tr)  #converting list into array\ndisplay(avg_w2v_clean_categories_tr.shape)\n\n#CV\navg_w2v_clean_categories_cv = []; # the avg-w2v for each sentence/review is stored in this list\nfor sentence in tqdm(x_cv['clean_categories']): # for each review/sentence\n    vector = np.zeros(200) # as word vectors are of zero length\n    cnt_words =0; # num of words with a valid vector in the sentence/review\n    for word in sentence.split(): # for each word in a review/sentence\n        if word in model :\n            vector += model[word]\n            cnt_words += 1\n    if cnt_words != 0:\n        vector /= cnt_words\n    avg_w2v_clean_categories_cv.append(vector)\n    \navg_w2v_clean_categories_cv=np.asarray(avg_w2v_clean_categories_cv)\ndisplay(avg_w2v_clean_categories_cv.shape)\n\n\n#TEST\navg_w2v_clean_categories_test = []; # the avg-w2v for each sentence/review is stored in this list\nfor sentence in tqdm(x_test['clean_categories']): # for each review/sentence\n    vector = np.zeros(200) # as word vectors are of zero length\n    cnt_words =0; # num of words with a valid vector in the sentence/review\n    for word in sentence.split(): # for each word in a review/sentence\n        if word in model :\n            vector += model[word]\n            cnt_words += 1\n    if cnt_words != 0:\n        vector /= cnt_words\n    avg_w2v_clean_categories_test.append(vector)\navg_w2v_clean_categories_test=np.asarray(avg_w2v_clean_categories_test)\ndisplay(avg_w2v_clean_categories_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#TFIDF weighted W2v\n#Essay\n#train\ntfidf_model = TfidfVectorizer()\ntfidf_model.fit(x_tr['essay'])\n# we are converting a dictionary with word as a key, and the idf as a value\ndictionary = dict(zip(tfidf_model.get_feature_names(), list(tfidf_model.idf_)))\ntfidf_words = set(tfidf_model.get_feature_names())\n\n\ntfidf_w2v_essay_tr = []; # the avg-w2v for each sentence/review is stored in this list\nfor sentence in tqdm(x_tr['essay']): # for each review/sentence\n    vector = np.zeros(200) # as word vectors are of zero length\n    tf_idf_weight =0; # num of words with a valid vector in the sentence/review\n    for word in sentence.split(): # for each word in a review/sentence\n        if (word in model) and (word in tfidf_words):\n            vec = model[word] # getting the vector for each word\n            # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)/len(sentence.split())))\n            tf_idf = dictionary[word]*(sentence.count(word)/len(sentence.split())) # getting the tfidf value for each word\n            vector += (vec * tf_idf) # calculating tfidf weighted w2v\n            tf_idf_weight += tf_idf\n    if tf_idf_weight != 0:\n        vector /= tf_idf_weight\n    tfidf_w2v_essay_tr.append(vector)\n\n\ntfidf_w2v_essay_tr=np.asarray(tfidf_w2v_essay_tr)\ndisplay(tfidf_w2v_essay_tr.shape)\n\n#CV\n\ntfidf_w2v_essay_cv = []; # the avg-w2v for each sentence/review is stored in this list\nfor sentence in tqdm(x_cv['essay']): # for each review/sentence\n    vector = np.zeros(200) # as word vectors are of zero length\n    tf_idf_weight =0; # num of words with a valid vector in the sentence/review\n    for word in sentence.split(): # for each word in a review/sentence\n        if (word in model) and (word in tfidf_words):\n            vec = model[word] # getting the vector for each word\n            # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)/len(sentence.split())))\n            tf_idf = dictionary[word]*(sentence.count(word)/len(sentence.split())) # getting the tfidf value for each word\n            vector += (vec * tf_idf) # calculating tfidf weighted w2v\n            tf_idf_weight += tf_idf\n    if tf_idf_weight != 0:\n        vector /= tf_idf_weight\n    tfidf_w2v_essay_cv.append(vector)\n\n\ntfidf_w2v_essay_cv=np.asarray(tfidf_w2v_essay_cv)\ndisplay(tfidf_w2v_essay_cv.shape)\n\n#Test\n\ntfidf_w2v_essay_test = []; # the avg-w2v for each sentence/review is stored in this list\nfor sentence in tqdm(x_test['essay']): # for each review/sentence\n    vector = np.zeros(200) # as word vectors are of zero length\n    tf_idf_weight =0; # num of words with a valid vector in the sentence/review\n    for word in sentence.split(): # for each word in a review/sentence\n        if (word in model) and (word in tfidf_words):\n            vec = model[word] # getting the vector for each word\n            # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)/len(sentence.split())))\n            tf_idf = dictionary[word]*(sentence.count(word)/len(sentence.split())) # getting the tfidf value for each word\n            vector += (vec * tf_idf) # calculating tfidf weighted w2v\n            tf_idf_weight += tf_idf\n    if tf_idf_weight != 0:\n        vector /= tf_idf_weight\n    tfidf_w2v_essay_test.append(vector)\n\n\ntfidf_w2v_essay_test=np.asarray(tfidf_w2v_essay_test)\ndisplay(tfidf_w2v_essay_test.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#TFIDF weighted W2v\n#Title\n#train\ntfidf_model = TfidfVectorizer()\ntfidf_model.fit(x_tr['title'])\n# we are converting a dictionary with word as a key, and the idf as a value\ndictionary = dict(zip(tfidf_model.get_feature_names(), list(tfidf_model.idf_)))\ntfidf_words = set(tfidf_model.get_feature_names())\n\n\ntfidf_w2v_title_tr = []; # the avg-w2v for each sentence/review is stored in this list\nfor sentence in tqdm(x_tr['title']): # for each review/sentence\n    vector = np.zeros(200) # as word vectors are of zero length\n    tf_idf_weight =0; # num of words with a valid vector in the sentence/review\n    for word in sentence.split(): # for each word in a review/sentence\n        if (word in model) and (word in tfidf_words):\n            vec = model[word] # getting the vector for each word\n            # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)/len(sentence.split())))\n            tf_idf = dictionary[word]*(sentence.count(word)/len(sentence.split())) # getting the tfidf value for each word\n            vector += (vec * tf_idf) # calculating tfidf weighted w2v\n            tf_idf_weight += tf_idf\n    if tf_idf_weight != 0:\n        vector /= tf_idf_weight\n    tfidf_w2v_title_tr.append(vector)\n\n\ntfidf_w2v_title_tr=np.asarray(tfidf_w2v_title_tr)\ndisplay(tfidf_w2v_title_tr.shape)\n\n#CV\n\ntfidf_w2v_title_cv = []; # the avg-w2v for each sentence/review is stored in this list\nfor sentence in tqdm(x_cv['title']): # for each review/sentence\n    vector = np.zeros(200) # as word vectors are of zero length\n    tf_idf_weight =0; # num of words with a valid vector in the sentence/review\n    for word in sentence.split(): # for each word in a review/sentence\n        if (word in model) and (word in tfidf_words):\n            vec = model[word] # getting the vector for each word\n            # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)/len(sentence.split())))\n            tf_idf = dictionary[word]*(sentence.count(word)/len(sentence.split())) # getting the tfidf value for each word\n            vector += (vec * tf_idf) # calculating tfidf weighted w2v\n            tf_idf_weight += tf_idf\n    if tf_idf_weight != 0:\n        vector /= tf_idf_weight\n    tfidf_w2v_title_cv.append(vector)\n\n\ntfidf_w2v_title_cv=np.asarray(tfidf_w2v_title_cv)\ndisplay(tfidf_w2v_title_cv.shape)\n\n#Test\n\ntfidf_w2v_title_test = []; # the avg-w2v for each sentence/review is stored in this list\nfor sentence in tqdm(x_test['title']): # for each review/sentence\n    vector = np.zeros(200) # as word vectors are of zero length\n    tf_idf_weight =0; # num of words with a valid vector in the sentence/review\n    for word in sentence.split(): # for each word in a review/sentence\n        if (word in model) and (word in tfidf_words):\n            vec = model[word] # getting the vector for each word\n            # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)/len(sentence.split())))\n            tf_idf = dictionary[word]*(sentence.count(word)/len(sentence.split())) # getting the tfidf value for each word\n            vector += (vec * tf_idf) # calculating tfidf weighted w2v\n            tf_idf_weight += tf_idf\n    if tf_idf_weight != 0:\n        vector /= tf_idf_weight\n    tfidf_w2v_title_test.append(vector)\n\n\ntfidf_w2v_title_test=np.asarray(tfidf_w2v_title_test)\ndisplay(tfidf_w2v_title_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#TFIDF weighted W2v\n#subcategories\n#train\ntfidf_model = TfidfVectorizer()\ntfidf_model.fit(x_tr['clean_subcategories'])\n# we are converting a dictionary with word as a key, and the idf as a value\ndictionary = dict(zip(tfidf_model.get_feature_names(), list(tfidf_model.idf_)))\ntfidf_words = set(tfidf_model.get_feature_names())\n\n\ntfidf_w2v_clean_subcategories_tr = []; # the avg-w2v for each sentence/review is stored in this list\nfor sentence in tqdm(x_tr['clean_subcategories']): # for each review/sentence\n    vector = np.zeros(200) # as word vectors are of zero length\n    tf_idf_weight =0; # num of words with a valid vector in the sentence/review\n    for word in sentence.split(): # for each word in a review/sentence\n        if (word in model) and (word in tfidf_words):\n            vec = model[word] # getting the vector for each word\n            # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)/len(sentence.split())))\n            tf_idf = dictionary[word]*(sentence.count(word)/len(sentence.split())) # getting the tfidf value for each word\n            vector += (vec * tf_idf) # calculating tfidf weighted w2v\n            tf_idf_weight += tf_idf\n    if tf_idf_weight != 0:\n        vector /= tf_idf_weight\n    tfidf_w2v_clean_subcategories_tr.append(vector)\n\n\ntfidf_w2v_clean_subcategories_tr=np.asarray(tfidf_w2v_clean_subcategories_tr)\ndisplay(tfidf_w2v_clean_subcategories_tr.shape)\n\n#CV\n\ntfidf_w2v_clean_subcategories_cv = []; # the avg-w2v for each sentence/review is stored in this list\nfor sentence in tqdm(x_cv['clean_subcategories']): # for each review/sentence\n    vector = np.zeros(200) # as word vectors are of zero length\n    tf_idf_weight =0; # num of words with a valid vector in the sentence/review\n    for word in sentence.split(): # for each word in a review/sentence\n        if (word in model) and (word in tfidf_words):\n            vec = model[word] # getting the vector for each word\n            # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)/len(sentence.split())))\n            tf_idf = dictionary[word]*(sentence.count(word)/len(sentence.split())) # getting the tfidf value for each word\n            vector += (vec * tf_idf) # calculating tfidf weighted w2v\n            tf_idf_weight += tf_idf\n    if tf_idf_weight != 0:\n        vector /= tf_idf_weight\n    tfidf_w2v_clean_subcategories_cv.append(vector)\n\n\ntfidf_w2v_clean_subcategories_cv=np.asarray(tfidf_w2v_clean_subcategories_cv)\ndisplay(tfidf_w2v_clean_subcategories_cv.shape)\n\n#Test\n\ntfidf_w2v_clean_subcategories_test = []; # the avg-w2v for each sentence/review is stored in this list\nfor sentence in tqdm(x_test['clean_subcategories']): # for each review/sentence\n    vector = np.zeros(200) # as word vectors are of zero length\n    tf_idf_weight =0; # num of words with a valid vector in the sentence/review\n    for word in sentence.split(): # for each word in a review/sentence\n        if (word in model) and (word in tfidf_words):\n            vec = model[word] # getting the vector for each word\n            # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)/len(sentence.split())))\n            tf_idf = dictionary[word]*(sentence.count(word)/len(sentence.split())) # getting the tfidf value for each word\n            vector += (vec * tf_idf) # calculating tfidf weighted w2v\n            tf_idf_weight += tf_idf\n    if tf_idf_weight != 0:\n        vector /= tf_idf_weight\n    tfidf_w2v_clean_subcategories_test.append(vector)\n\n\ntfidf_w2v_clean_subcategories_test=np.asarray(tfidf_w2v_clean_subcategories_test)\ndisplay(tfidf_w2v_clean_subcategories_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#TFIDF weighted W2v\n#subcategories\n#train\ntfidf_model = TfidfVectorizer()\ntfidf_model.fit(x_tr['clean_categories'])\n# we are converting a dictionary with word as a key, and the idf as a value\ndictionary = dict(zip(tfidf_model.get_feature_names(), list(tfidf_model.idf_)))\ntfidf_words = set(tfidf_model.get_feature_names())\n\n\ntfidf_w2v_clean_categories_tr = []; # the avg-w2v for each sentence/review is stored in this list\nfor sentence in tqdm(x_tr['clean_categories']): # for each review/sentence\n    vector = np.zeros(200) # as word vectors are of zero length\n    tf_idf_weight =0; # num of words with a valid vector in the sentence/review\n    for word in sentence.split(): # for each word in a review/sentence\n        if (word in model) and (word in tfidf_words):\n            vec = model[word] # getting the vector for each word\n            # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)/len(sentence.split())))\n            tf_idf = dictionary[word]*(sentence.count(word)/len(sentence.split())) # getting the tfidf value for each word\n            vector += (vec * tf_idf) # calculating tfidf weighted w2v\n            tf_idf_weight += tf_idf\n    if tf_idf_weight != 0:\n        vector /= tf_idf_weight\n    tfidf_w2v_clean_categories_tr.append(vector)\n\n\ntfidf_w2v_clean_categories_tr=np.asarray(tfidf_w2v_clean_categories_tr)\ndisplay(tfidf_w2v_clean_categories_tr.shape)\n\n#CV\n\ntfidf_w2v_clean_categories_cv = []; # the avg-w2v for each sentence/review is stored in this list\nfor sentence in tqdm(x_cv['clean_categories']): # for each review/sentence\n    vector = np.zeros(200) # as word vectors are of zero length\n    tf_idf_weight =0; # num of words with a valid vector in the sentence/review\n    for word in sentence.split(): # for each word in a review/sentence\n        if (word in model) and (word in tfidf_words):\n            vec = model[word] # getting the vector for each word\n            # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)/len(sentence.split())))\n            tf_idf = dictionary[word]*(sentence.count(word)/len(sentence.split())) # getting the tfidf value for each word\n            vector += (vec * tf_idf) # calculating tfidf weighted w2v\n            tf_idf_weight += tf_idf\n    if tf_idf_weight != 0:\n        vector /= tf_idf_weight\n    tfidf_w2v_clean_categories_cv.append(vector)\n\n\ntfidf_w2v_clean_categories_cv=np.asarray(tfidf_w2v_clean_categories_cv)\ndisplay(tfidf_w2v_clean_categories_cv.shape)\n\n#Test\n\ntfidf_w2v_clean_categories_test = []; # the avg-w2v for each sentence/review is stored in this list\nfor sentence in tqdm(x_test['clean_categories']): # for each review/sentence\n    vector = np.zeros(200) # as word vectors are of zero length\n    tf_idf_weight =0; # num of words with a valid vector in the sentence/review\n    for word in sentence.split(): # for each word in a review/sentence\n        if (word in model) and (word in tfidf_words):\n            vec = model[word] # getting the vector for each word\n            # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)/len(sentence.split())))\n            tf_idf = dictionary[word]*(sentence.count(word)/len(sentence.split())) # getting the tfidf value for each word\n            vector += (vec * tf_idf) # calculating tfidf weighted w2v\n            tf_idf_weight += tf_idf\n    if tf_idf_weight != 0:\n        vector /= tf_idf_weight\n    tfidf_w2v_clean_categories_test.append(vector)\n\n\ntfidf_w2v_clean_categories_test=np.asarray(tfidf_w2v_clean_categories_test)\ndisplay(tfidf_w2v_clean_categories_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#price standardized\n#train\n\n# check this one: https://www.youtube.com/watch?v=0HOqOcln3Z4&t=530s\n# standardization sklearn: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\nfrom sklearn.preprocessing import StandardScaler\n\n# price_standardized = standardScalar.fit(project_data['price'].values)\n# this will rise the error\n# ValueError: Expected 2D array, got 1D array instead: array=[725.05 213.03 329.   ... 399.   287.73   5.5 ].\n# Reshape your data either using array.reshape(-1, 1)\n\nprice_scalar = StandardScaler()\nprice_scalar.fit(x_tr['price'].values.reshape(-1,1)) # finding the mean and standard deviation of this data\nprint(f\"Mean : {price_scalar.mean_[0]}, Standard deviation : {np.sqrt(price_scalar.var_[0])}\")\n\n# Now standardize the data with above maen and variance.\nprice_standardized_tr = price_scalar.transform(x_tr['price'].values.reshape(-1, 1))\n\n#cv\n\nprice_scalar = StandardScaler()\nprice_scalar.fit(x_cv['price'].values.reshape(-1,1)) \nprint(f\"Mean : {price_scalar.mean_[0]}, Standard deviation : {np.sqrt(price_scalar.var_[0])}\")\n\nprice_standardized_cv = price_scalar.transform(x_cv['price'].values.reshape(-1, 1))\n\n#test\n\nprice_scalar = StandardScaler()\nprice_scalar.fit(x_test['price'].values.reshape(-1,1))\nprint(f\"Mean : {price_scalar.mean_[0]}, Standard deviation : {np.sqrt(price_scalar.var_[0])}\")\n\nprice_standardized_test = price_scalar.transform(x_test['price'].values.reshape(-1, 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#SET1::\n\n# merge two sparse matrices: https://stackoverflow.com/a/19710648/4084039\nfrom scipy.sparse import hstack\n# with the same hstack function we are concatinating a sparse matrix and a dense matirx :)\nX_tr =hstack((bow_categories_tr, bow_subcategories_tr,bow_essay_tr, price_standardized_tr,bow_title_tr))\nX_cv=hstack((bow_categories_cv, bow_subcategories_cv,bow_essay_cv, price_standardized_cv,bow_title_cv))\nX_test=hstack((bow_categories_test, bow_subcategories_test,bow_essay_test,price_standardized_test,bow_title_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://www.geeksforgeeks.org/confusion-matrix-machine-learning/\n\nfrom sklearn.metrics import confusion_matrix \nfrom sklearn.metrics import accuracy_score \nfrom sklearn.metrics import classification_report \n\nAccuracy_cv=[]\nAccuracy_tr=[]\nk = []\nf1 = []\nAuc_value = []\nprob = []\nfor i in range(1,30,2):\n    #instantiate learning model (k=30)\n    knn= KNeighborsClassifier(n_neighbors=i)\n    \n    \n    #fitting the model on crossvalidation train\n    knn.fit(X_tr,np.ravel(y_tr))\n    \n    #predicting the response of the crossvalidation train \n    pred_cv = knn.predict(X_cv)\n    pred_tr = knn.predict(X_tr)\n    \n    \n    \n    #evaluate the Cv accuracy\n    acc_cv = accuracy_score(y_cv,pred_cv,normalize=True)*float(100)\n    acc_tr = accuracy_score(y_tr,pred_tr,normalize=True)*float(100)\n    Accuracy_cv.append(acc_cv)\n    Accuracy_tr.append(acc_tr)\n    k.append(i)\n    \n    Results = confusion_matrix(y_cv,pred_cv) \n    prec = Results[1][1]/(Results[1][1]+Results[0][1])\n    rec =  Results[1][1]/(Results[1][1]+Results[1][0])\n    mult = prec*rec\n    add =  prec+rec\n    result = (mult/add)\n    result = 2*result\n    f1.append(result)\n    \n    prob_score= knn.predict_proba(X_cv)\n    prob_psv = prob_score[:,1]\n    fpr,tpr,threshold = metrics.roc_curve(y_cv,prob_psv)\n    value = auc(fpr,tpr)\n    Auc_value.append(value)\n       \n    \nAcc_table = pd.DataFrame(columns=['K-value','Accuracy_cv','Accuracy_tr','F1_score_cv','Auc_value'])\nAcc_table['K-value']=k\nAcc_table['Accuracy_cv']=Accuracy_cv\nAcc_table['Accuracy_tr']=Accuracy_tr\nAcc_table['F1_score_cv']=f1\nAcc_table['Auc_value']=Auc_value\nAcc_table.sort_values([\"Auc_value\"],axis=0,ascending=False,inplace= True)\n\ndisplay(Acc_table)\n########################    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Accuracy plot of Training and CV Data vs K-value\nAcc_table1=Acc_table.sort_values([\"K-value\"],axis=0,ascending=False,inplace=False)\na=Acc_table1['K-value']\nb=Acc_table1['Accuracy_tr']\nc=Acc_table1['K-value']\nd=Acc_table1['Accuracy_cv']\nplt.title('Accuracy plot of Training and test Data vs K-value')\nplt.plot(a ,b,label='training accuracy' )\nplt.plot(c,d,label='CV accuracy')\nplt.legend()\nplt.xlabel('Number of Neighbours')\nplt.ylabel('Accuracy')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint(Acc_table['Accuracy_tr'].sort_values().values)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"    I = int(Acc_table.iloc[0][0])\n    print(I)\n    \n    knn= KNeighborsClassifier(n_neighbors=I)\n    \n    #fitting the model on crossvalidation train\n    knn.fit(X_tr,y_tr)\n    \n    #predicting the response of the crossvalidation train\n    pred = knn.predict(X_test)\n    \n    \n    #evaluate the Cv accuracy\n    acc = accuracy_score(y_test,pred,normalize=True)*float(100)\n    print(acc)\n    \n    confusion_mat = confusion_matrix(y_test,pred)\n    print(confusion_mat)\n    df_cm = pd.DataFrame(confusion_mat,range(2),range(2))\n    sns.heatmap(df_cm,annot=True,fmt='g')\n    plt.title('Confusion Matrix of the Test Data')\n    plt.xlabel('Actual Class label')\n    plt.ylabel('Predicted Class label')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"ROC CURVE"},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculating probability of the class\nfrom sklearn import metrics\nprob_score = []\nprob_score= knn.predict_proba(X_test)\nprob_psv = prob_score[:,1]\n\n#ROC curve\n\nfpr,tpr,threshold = metrics.roc_curve(y_test,prob_psv)\nroc_auc = auc(fpr,tpr)\n\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr,label='knn')\nplt.xlabel('fpr')\nplt.ylabel('tpr')\nplt.title('knn ROC CURVE')\nplt.legend()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prob_score\nprob_psv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#SET2::\nX_tr = hstack((bow_categories_tr, bow_subcategories_tr,bow_essay_tr, price_standardized_tr,tfidf_title_tr,tfidf_essay_tr))\nX_cv=hstack((bow_categories_cv, bow_subcategories_cv,bow_essay_cv, price_standardized_cv,tfidf_title_cv,tfidf_essay_cv))\nX_test=hstack((bow_categories_test, bow_subcategories_test,bow_essay_test,price_standardized_test,tfidf_title_test,tfidf_essay_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Accuracy_cv=[]\nAccuracy_tr=[]\nk = []\nf1 = []\nAuc_value = []\nprob = []\nfor i in range(1,30,2):\n    #instantiate learning model (k=30)\n    knn= KNeighborsClassifier(n_neighbors=i)\n    \n    \n    #fitting the model on crossvalidation train\n    knn.fit(X_tr,np.ravel(y_tr))\n    \n    #predicting the response of the crossvalidation train \n    pred_cv = knn.predict(X_cv)\n    pred_tr = knn.predict(X_tr)\n    \n    \n    \n    #evaluate the Cv accuracy\n    acc_cv = accuracy_score(y_cv,pred_cv,normalize=True)*float(100)\n    acc_tr = accuracy_score(y_tr,pred_tr,normalize=True)*float(100)\n    Accuracy_cv.append(acc_cv)\n    Accuracy_tr.append(acc_tr)\n    k.append(i)\n    \n    Results = confusion_matrix(y_cv,pred_cv) \n    prec = Results[1][1]/(Results[1][1]+Results[0][1])\n    rec =  Results[1][1]/(Results[1][1]+Results[1][0])\n    mult = prec*rec\n    add =  prec+rec\n    result = (mult/add)\n    result = 2*result\n    f1.append(result)\n    \n    prob_score= knn.predict_proba(X_cv)\n    prob_psv = prob_score[:,1]\n    fpr,tpr,threshold = metrics.roc_curve(y_cv,prob_psv)\n    value = auc(fpr,tpr)\n    Auc_value.append(value)\n       \n    \nAcc_table = pd.DataFrame(columns=['K-value','Accuracy_cv','Accuracy_tr','F1_score_cv','Auc_value'])\nAcc_table['K-value']=k\nAcc_table['Accuracy_cv']=Accuracy_cv\nAcc_table['Accuracy_tr']=Accuracy_tr\nAcc_table['F1_score_cv']=f1\nAcc_table['Auc_value']=Auc_value\nAcc_table.sort_values([\"Auc_value\"],axis=0,ascending=False,inplace= True)\n\ndisplay(Acc_table)\n######################## \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Accuracy plot of Training and CV Data vs K-value\nAcc_table1=Acc_table.sort_values([\"K-value\"],axis=0,ascending=False,inplace=False)\na=Acc_table1['K-value']\nb=Acc_table1['Accuracy_tr']\nc=Acc_table1['K-value']\nd=Acc_table1['Accuracy_cv']\nplt.title('Accuracy plot of Training and test Data vs K-value')\nplt.plot(a ,b,label='training accuracy' )\nplt.plot(c,d,label='CV accuracy')\nplt.legend()\nplt.xlabel('Number of Neighbours')\nplt.ylabel('Accuracy')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"    I = int(Acc_table.iloc[0][0])\n    print(I)\n    \n    knn= KNeighborsClassifier(n_neighbors=I)\n    \n    #fitting the model on crossvalidation train\n    knn.fit(X_tr,y_tr)\n    \n    #predicting the response of the crossvalidation train\n    pred = knn.predict(X_test)\n    \n    \n    #evaluate the Cv accuracy\n    acc = accuracy_score(y_test,pred,normalize=True)*float(100)\n    print(acc)\n    \n    confusion_mat = confusion_matrix(y_test,pred)\n    print(confusion_mat)\n    df_cm = pd.DataFrame(confusion_mat,range(2),range(2))\n    sns.heatmap(df_cm,annot=True,fmt='g')\n    plt.title('Confusion Matrix of the Test Data')\n    plt.xlabel('Actual Class label')\n    plt.ylabel('Predicted Class label')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculating probability of the class\nfrom sklearn import metrics\nprob_score = []\nprob_score= knn.predict_proba(X_test)\nprob_psv = prob_score[:,1]\n\n#ROC curve\n\nfpr,tpr,threshold = metrics.roc_curve(y_test,prob_psv)\nroc_auc = auc(fpr,tpr)\n\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr,label='knn')\nplt.xlabel('fpr')\nplt.ylabel('tpr')\nplt.title('knn ROC CURVE')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Taking Top 2000 feature and appling on SET2\n#https://www.geeksforgeeks.org/ml-chi-square-test-for-feature-selection/\nfrom sklearn.datasets import load_digits\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.feature_selection import SelectKBest, f_classif\n\nchi2_features = SelectKBest(f_classif,k=2000)\nX_tr = X_tr.astype(int)\nX_cv = X_cv.astype(int)\nX_test = X_test.astype(int)\n\nX_tr = chi2_features.fit_transform(X_tr,y_tr)\nX_cv = chi2_features.fit_transform(X_cv,y_cv)\nX_test = chi2_features.fit_transform(X_test,y_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Accuracy_cv=[]\nAccuracy_tr=[]\nk = []\nf1 = []\nAuc_value = []\nprob = []\nfor i in range(1,30,2):\n    #instantiate learning model (k=30)\n    knn= KNeighborsClassifier(n_neighbors=i)\n    \n    \n    #fitting the model on crossvalidation train\n    knn.fit(X_tr,np.ravel(y_tr))\n    \n    #predicting the response of the crossvalidation train \n    pred_cv = knn.predict(X_cv)\n    pred_tr = knn.predict(X_tr)\n    \n    \n    \n    #evaluate the Cv accuracy\n    acc_cv = accuracy_score(y_cv,pred_cv,normalize=True)*float(100)\n    acc_tr = accuracy_score(y_tr,pred_tr,normalize=True)*float(100)\n    Accuracy_cv.append(acc_cv)\n    Accuracy_tr.append(acc_tr)\n    k.append(i)\n    \n    Results = confusion_matrix(y_cv,pred_cv) \n    prec = Results[1][1]/(Results[1][1]+Results[0][1])\n    rec =  Results[1][1]/(Results[1][1]+Results[1][0])\n    mult = prec*rec\n    add =  prec+rec\n    result = (mult/add)\n    result = 2*result\n    f1.append(result)\n    \n    prob_score= knn.predict_proba(X_cv)\n    prob_psv = prob_score[:,1]\n    fpr,tpr,threshold = metrics.roc_curve(y_cv,prob_psv)\n    value = auc(fpr,tpr)\n    Auc_value.append(value)\n       \n    \nAcc_table = pd.DataFrame(columns=['K-value','Accuracy_cv','Accuracy_tr','F1_score_cv','Auc_value'])\nAcc_table['K-value']=k\nAcc_table['Accuracy_cv']=Accuracy_cv\nAcc_table['Accuracy_tr']=Accuracy_tr\nAcc_table['F1_score_cv']=f1\nAcc_table['Auc_value']=Auc_value\nAcc_table.sort_values([\"Auc_value\"],axis=0,ascending=False,inplace= True)\n\ndisplay(Acc_table)\n########################","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Accuracy plot of Training and CV Data vs K-value\nAcc_table1=Acc_table.sort_values([\"K-value\"],axis=0,ascending=False,inplace=False)\na=Acc_table1['K-value']\nb=Acc_table1['Accuracy_tr']\nc=Acc_table1['K-value']\nd=Acc_table1['Accuracy_cv']\nplt.title('Accuracy plot of Training and test Data vs K-value')\nplt.plot(a ,b,label='training accuracy' )\nplt.plot(c,d,label='CV accuracy')\nplt.legend()\nplt.xlabel('Number of Neighbours')\nplt.ylabel('Accuracy')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"    I = int(Acc_table.iloc[0][0])\n    print(I)\n    \n    knn= KNeighborsClassifier(n_neighbors=I)\n    \n    #fitting the model on crossvalidation train\n    knn.fit(X_tr,y_tr)\n    \n    #predicting the response of the crossvalidation train\n    pred = knn.predict(X_test)\n    \n    \n    #evaluate the Cv accuracy\n    acc = accuracy_score(y_test,pred,normalize=True)*float(100)\n    print(acc)\n    \n    confusion_mat = confusion_matrix(y_test,pred)\n    print(confusion_mat)\n    df_cm = pd.DataFrame(confusion_mat,range(2),range(2))\n    sns.heatmap(df_cm,annot=True,fmt='g')\n    plt.title('Confusion Matrix of the Test Data')\n    plt.xlabel('Actual Class label')\n    plt.ylabel('Predicted Class label')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculating probability of the class\nfrom sklearn import metrics\nprob_score = []\nprob_score= knn.predict_proba(X_test)\nprob_psv = prob_score[:,1]\n\n#ROC curve\n\nfpr,tpr,threshold = metrics.roc_curve(y_test,prob_psv)\nroc_auc = auc(fpr,tpr)\n\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr,label='knn')\nplt.xlabel('fpr')\nplt.ylabel('tpr')\nplt.title('knn ROC CURVE')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#SET3::\nX_tr = hstack((bow_categories_tr, bow_subcategories_tr,bow_essay_tr, price_standardized_tr,avg_w2v_title_tr,avg_w2v_essay_tr))\nX_cv=hstack((bow_categories_cv, bow_subcategories_cv,bow_essay_cv, price_standardized_cv,avg_w2v_title_cv,avg_w2v_essay_cv))\nX_test=hstack((bow_categories_test, bow_subcategories_test,bow_essay_test,price_standardized_test,avg_w2v_title_test,avg_w2v_essay_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Accuracy_cv=[]\nAccuracy_tr=[]\nk = []\nf1 = []\nAuc_value = []\nprob = []\nfor i in range(1,30,2):\n    #instantiate learning model (k=30)\n    knn= KNeighborsClassifier(n_neighbors=i)\n    \n    \n    #fitting the model on crossvalidation train\n    knn.fit(X_tr,np.ravel(y_tr))\n    \n    #predicting the response of the crossvalidation train \n    pred_cv = knn.predict(X_cv)\n    pred_tr = knn.predict(X_tr)\n    \n    \n    \n    #evaluate the Cv accuracy\n    acc_cv = accuracy_score(y_cv,pred_cv,normalize=True)*float(100)\n    acc_tr = accuracy_score(y_tr,pred_tr,normalize=True)*float(100)\n    Accuracy_cv.append(acc_cv)\n    Accuracy_tr.append(acc_tr)\n    k.append(i)\n    \n    Results = confusion_matrix(y_cv,pred_cv) \n    prec = Results[1][1]/(Results[1][1]+Results[0][1])\n    rec =  Results[1][1]/(Results[1][1]+Results[1][0])\n    mult = prec*rec\n    add =  prec+rec\n    result = (mult/add)\n    result = 2*result\n    f1.append(result)\n    \n    prob_score= knn.predict_proba(X_cv)\n    prob_psv = prob_score[:,1]\n    fpr,tpr,threshold = metrics.roc_curve(y_cv,prob_psv)\n    value = auc(fpr,tpr)\n    Auc_value.append(value)\n       \n    \nAcc_table = pd.DataFrame(columns=['K-value','Accuracy_cv','Accuracy_tr','F1_score_cv','Auc_value'])\nAcc_table['K-value']=k\nAcc_table['Accuracy_cv']=Accuracy_cv\nAcc_table['Accuracy_tr']=Accuracy_tr\nAcc_table['F1_score_cv']=f1\nAcc_table['Auc_value']=Auc_value\nAcc_table.sort_values([\"Auc_value\"],axis=0,ascending=False,inplace= True)\n\ndisplay(Acc_table)\n######################## ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Accuracy plot of Training and CV Data vs K-value\nAcc_tabel1 = Acc_table.sort_values([\"K-value\"],axis=0,ascending=False,inplace=False)\na=Acc_table1['K-value']\nb=Acc_table1['Accuracy_tr']\nc=Acc_table1['K-value']\nd=Acc_table1['Accuracy_cv']\nplt.title('Accuracy plot of Training and test Data vs K-value')\nplt.plot(a ,b,label='training accuracy' )\nplt.plot(c,d,label='CV accuracy')\nplt.legend()\nplt.xlabel('Number of Neighbours')\nplt.ylabel('Accuracy')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"    I = int(Acc_table.iloc[0][0])\n    print(I)\n    \n    knn= KNeighborsClassifier(n_neighbors=I)\n    \n    #fitting the model on crossvalidation train\n    knn.fit(X_tr,y_tr)\n    \n    #predicting the response of the crossvalidation train\n    pred = knn.predict(X_test)\n    \n    \n    #evaluate the Cv accuracy\n    acc = accuracy_score(y_test,pred,normalize=True)*float(100)\n    print(acc)\n    \n    confusion_mat = confusion_matrix(y_test,pred)\n    print(confusion_mat)\n    df_cm = pd.DataFrame(confusion_mat,range(2),range(2))\n    sns.heatmap(df_cm,annot=True,fmt='g')\n    plt.title('Confusion Matrix of the Test Data')\n    plt.xlabel('Actual Class label')\n    plt.ylabel('Predicted Class label')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculating probability of the class\nfrom sklearn import metrics\nprob_score = []\nprob_score= knn.predict_proba(X_test)\nprob_psv = prob_score[:,1]\n\n#ROC curve\n\nfpr,tpr,threshold = metrics.roc_curve(y_test,prob_psv)\nroc_auc = auc(fpr,tpr)\n\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr,label='knn')\nplt.xlabel('fpr')\nplt.ylabel('tpr')\nplt.title('knn ROC CURVE')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_tr = hstack((bow_categories_tr, bow_subcategories_tr,bow_essay_tr, price_standardized_tr,tfidf_w2v_title_tr,tfidf_w2v_essay_tr))\nX_cv=hstack((bow_categories_cv, bow_subcategories_cv,bow_essay_cv, price_standardized_cv,tfidf_w2v_title_cv,tfidf_w2v_essay_cv))\nX_test=hstack((bow_categories_test, bow_subcategories_test,bow_essay_test,price_standardized_test,tfidf_w2v_title_test,tfidf_w2v_essay_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Accuracy_cv=[]\nAccuracy_tr=[]\nk = []\nf1 = []\nAuc_value = []\nprob = []\nfor i in range(1,30,2):\n    #instantiate learning model (k=30)\n    knn= KNeighborsClassifier(n_neighbors=i)\n    \n    \n    #fitting the model on crossvalidation train\n    knn.fit(X_tr,np.ravel(y_tr))\n    \n    #predicting the response of the crossvalidation train \n    pred_cv = knn.predict(X_cv)\n    pred_tr = knn.predict(X_tr)\n    \n    \n    \n    #evaluate the Cv accuracy\n    acc_cv = accuracy_score(y_cv,pred_cv,normalize=True)*float(100)\n    acc_tr = accuracy_score(y_tr,pred_tr,normalize=True)*float(100)\n    Accuracy_cv.append(acc_cv)\n    Accuracy_tr.append(acc_tr)\n    k.append(i)\n    \n    Results = confusion_matrix(y_cv,pred_cv) \n    prec = Results[1][1]/(Results[1][1]+Results[0][1])\n    rec =  Results[1][1]/(Results[1][1]+Results[1][0])\n    mult = prec*rec\n    add =  prec+rec\n    result = (mult/add)\n    result = 2*result\n    f1.append(result)\n    \n    prob_score= knn.predict_proba(X_cv)\n    prob_psv = prob_score[:,1]\n    fpr,tpr,threshold = metrics.roc_curve(y_cv,prob_psv)\n    value = auc(fpr,tpr)\n    Auc_value.append(value)\n       \n    \nAcc_table = pd.DataFrame(columns=['K-value','Accuracy_cv','Accuracy_tr','F1_score_cv','Auc_value'])\nAcc_table['K-value']=k\nAcc_table['Accuracy_cv']=Accuracy_cv\nAcc_table['Accuracy_tr']=Accuracy_tr\nAcc_table['F1_score_cv']=f1\nAcc_table['Auc_value']=Auc_value\nAcc_table.sort_values([\"Auc_value\"],axis=0,ascending=False,inplace= True)\n\ndisplay(Acc_table)\n######################## ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Accuracy plot of Training and CV Data vs K-value\nAcc_tabel1 = Acc_table.sort_values([\"K-value\"],axis=0,ascending=False,inplace=False)\na=Acc_table1['K-value']\nb=Acc_table1['Accuracy_tr']\nc=Acc_table1['K-value']\nd=Acc_table1['Accuracy_cv']\nplt.title('Accuracy plot of Training and test Data vs K-value')\nplt.plot(a ,b,label='training accuracy' )\nplt.plot(c,d,label='CV accuracy')\nplt.legend()\nplt.xlabel('Number of Neighbours')\nplt.ylabel('Accuracy')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"    I = int(Acc_table.iloc[0][0])\n    print(I)\n    \n    knn= KNeighborsClassifier(n_neighbors=I)\n    \n    #fitting the model on crossvalidation train\n    knn.fit(X_tr,np.ravel(y_tr))\n    \n    #predicting the response of the crossvalidation train\n    pred = knn.predict(X_test)\n    \n    \n    #evaluate the Cv accuracy\n    acc = accuracy_score(y_test,pred,normalize=True)*float(100)\n    print(acc)\n    \n    confusion_mat = confusion_matrix(y_test,pred)\n    print(confusion_mat)\n    df_cm = pd.DataFrame(confusion_mat,range(2),range(2))\n    sns.heatmap(df_cm,annot=True,fmt='g')\n    plt.title('Confusion Matrix of the Test Data')\n    plt.xlabel('Actual Class label')\n    plt.ylabel('Predicted Class label')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculating probability of the class\nfrom sklearn import metrics\nprob_score = []\nprob_score= knn.predict_proba(X_test)\nprob_psv = prob_score[:,1]\n\n#ROC curve\n\nfpr,tpr,threshold = metrics.roc_curve(y_test,prob_psv)\nroc_auc = auc(fpr,tpr)\n\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr,label='knn')\nplt.xlabel('fpr')\nplt.ylabel('tpr')\nplt.title('knn ROC CURVE')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}