{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"},"cell_type":"markdown","source":"# <center>Beginner's Guide to Capsule Networks</center>\n\n_Author: Zafar_\n\n_Last Updated: 03/30/18_\n\n--------\n\nIn the recently concluded [Toxic Comments Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge), Capsule Network (aka CapsNet) proved to be a huge success. This notebook introduces and implements a Capsule Network in Keras and evaluates its performance in the DonorsChoose.Org Application Screening Competition.\n\n## Contents\n1. [Introduction to Capsule Networks](#introduction)\n    * 1.1 [Human Visual Recognition](#human)\n    * 1.2 [Capsules](#capsules)\n    * 1.3 [Routing by Agreement](#routing)\n    * 1.4 [Mathematics behind CapsNet](#maths)\n    * 1.5 [The Dyanmic Routing Algorithm](#algo)\n    * 1.6 [A word about squash function](#squash)\n    * 1.7 [The advantage of Capsule Networks](#advantage)\n2. [Boilerplate Code](#boilerplate)\n3. [CapsNet implementation](#capsnet_model)\n4. [Training](#training)\n5. [Submission](#submission)\n6. [Conclusion](#conclusion)\n7. [References](#references)\n\nBefore you read further, it is important to note that the examples presented here are just for the purpose of understanding."},{"metadata":{"_cell_guid":"7a089293-2c3b-4bcb-bdfd-dd48854555b6","_uuid":"95390dc737aa3971e40034808790c153f7ed7398"},"cell_type":"markdown","source":"<a id=\"introduction\"></a>\n## Introduction to Capsule Networks\n<a id=\"human\"></a>\n#### Human Visual Recognition\nAny real object is made up of several smaller objects. For example, a tree consists of a *trunk*, a *crown* and *roots*. These parts form a hierarchy. The crown of a tree further consists of branches and branches have leaves.\n\n![Parts of a tree](https://study.com/cimages/multimages/16/tree_parts_diagram.png)\n\nWhenever we see some object, our eyes make some **fixation points** and the relative positions and natures of these fixation points help our brain in recognizing that object. By doing so, our brain does not have to process every detail. Just by seeing some leaves and branches, our brain recognizes there is a crown of a tree. And the crown is standing on a trunk below which are some roots. Combining this hierarchical information, our brain knows that there is a tree. From now on, we will call **the parts of the objects** as entities.\n\n![Parts of a tree](https://raw.githubusercontent.com/zaffnet/images/master/images/tree.png)\n\n>*Each complex object can be thought of a hierarchy of simpler objects.*\n<a id=\"capsules\"></a>\n#### Capsules\n\nThe assumption behind CapsNet is that there are capsules (groups of neurons) that tell whether certain objects (**entities**) are present in an image. Corresponding to each entity, there is a capsule which gives:\n1. the probability that the entity exists\n2. The **instantiation parameters** of that entity.\n\nInstantiation parameters are the properties of that entity in an image (like \"position\", \"size\", \"position\", \"hue\", etc). For example, a **rectangle** is a simple geometric object. The capsule corresponding to a rectangle will tell us about its instantiation parameters. \n\n![Rectangle capsule](https://raw.githubusercontent.com/zaffnet/images/master/images/rectangle.png)\n\n\nFrom the figure above, our imaginary capsule consists of 6 neurons each corresponding to some property of the rectangle. The length of this vector will give us the probability of the presence of a rectangle. So, the probability that a rectangle is present will be: $$\\sqrt[]{1.3^2 + 0.6^2 + 7.4^2 + 6.5^2 + 0.5^2 + 1.4^2} = 10.06$$\n\nBut wait a minute! If the length of the output vector represents the probability of the existence of an entity, shouldn't it be less than or equal to 1 (i.e., $0 \\leq P \\leq 1$)? Yes, and that is why we transform the capsule output $s$ like this:\n\n![squashing function](https://raw.githubusercontent.com/zaffnet/images/master/images/squash.png)\n\n\nThis non-linear transformation is called **squashing** function and it serves as an activation function for capsule networks (just like ReLU is used in CNNs).\n\n>*A capsule is a group of neurons whose activation $v = <v_1, v_2, ..., v_n>$ represents the instantiation parameters of an entity and whose length represents the probability of the existence of that entity.*\n<a id=\"routing\"></a>\n#### Routing by agreement\nA CapsNet consists of several layers. Capsules in the lower layer correspond to simple entities (like rectangles, triangles, circles, etc). These low-level capsules bet on the presence of more complex entities and their bets are \"combined\" to get the output of high-level capsules (doors, windows, etc). For example, the presence of a *rectangle* (angle with x-axis = 0, size = 5, position = 0,...) and a *triangle* (angle with x-axis = 6, size = 5, position = 5,...) work together to bet on the presence of a *house* (a higher-level entity). \n\nThere is a **coupling effect** too. When some low-level capsules agree on the presence of a high-level entity, the high-level capsule corresponding to that entity sends a feedback to these low-level capsules which *increases* their bet on that high-level capsule. To understand this, let's assume we have two levels of capsules: \n1. Lower level corresponds to rectangles, triangles and circles\n2. High level corresponds to houses, boats, and cars\n\nIf there is an image of a house, the capsules corresponding to rectangles and triangles will have large activation vectors. Their relative positions (coded in their instantiation parameters) will bet on the presence of high-level objects. Since they will agree on the presence of house, the output vector of the house capsule will become large. This, in turn, will make the predictions by the rectangle and the traingle capsules larger. This cycle will repeat 4-5 times after which the bets on the presence of a house will be considerably larger than the bets on the presence of a boat or a car.\n<a id=\"maths\"></a>\n#### Mathematics behind CapsNet\nSuppose layer $l$ and $l+1$ have $m$ and $n$ capsules respectively. Our task is to calculate the activations of the capsules at layer $l+1$ given the activations at layer $l$. Let $u$ denotes the activations of capsules at layer $l$. We have to calculate $v$, the activations of capsules at layer $l+1$. \n\nFor a capsule $j$ at layer $l+1$, \n\n1. We first calculate the **prediction vectors** by the capsules at layer $l$. The prediction vector by a capsule $i$ (of layer $l$) for the capsule $j$ (of layer $l+1$) is given by:\n    $$\\boldsymbol{\\hat{\\textbf{u}}}_{j|i} = \\boldsymbol{\\textbf{W}}_{ij}\\boldsymbol{\\textbf{u}}_i$$ $\\textbf{W}_{ij}$ is the weight matrix.\n\n2. We then calculate the **output vector** for the capsule $j$. The output vector is the weighted sum of all the prediction vectors given by the capsules of layer $l$ for the capsule $j$:\n    $$s_j = \\sum_{i=1}^{m}{c_{ij}\\boldsymbol{\\hat{\\textbf{u}}}_{j|i}}$$ The scalar $\\textbf{c}_{ij}$ is called **coupling coefficient** between capsule $i$ (of layer $l$) and $j$ (of layer $l+1$). These coefficients are determied by an algorithm called the **iterative dynamic routing algorithm**.\n\n3. We apply the **squashing** function on the output vector to get the activation $\\textbf{v}_j$ of the capsule $j$:\n    $$\\textbf{v}_j = \\textbf{squash}(\\textbf{s}_j)$$\n    \n<a id=\"algo\"></a>\n#### The dynamic routing algorithm\nThe activation vectors of layer $l+1$ send feedback signals to the capsules at layer $l$. If the prediction vector of capsule $i$ (of layer $l$) for a capsule $j$ (of layer $l+1$) is in agreement with the activation vector of capsule $j$, their dot product should be high. Hence the \"weight\" of the prediction vector $\\boldsymbol{\\hat{\\textbf{u}}}_{j|i}$ is increased in the output vector of $j$. In other words, those prediction vectors that helped the activation vector have a lot more weight in the output vector (and consequently the activation vector). This cycle of mutual help continues for 4-5 rounds. \n\nBut the predictions of a low-level capsule for high-level capsules should sum to one. That is why for a capsule $i$ (of layer $l$), \n$$c_{ij} = \\frac{\\exp(b_{ij})}{\\sum_{k}{\\exp(b_{ik})}}$$ Clearly, $$\\sum_{k}{c_{ik}} = 1$$ The logit $b_{ij}$ indicates whether capsules $i$ (of layer $l$) and $j$ (of layer $l+1$) have strong coupling. In other words, it is a measure of how much the presence of the capsule $j$ is explained by the capsule $i$. Initially, all $b_{ij}$ should be equal.\n\n**Routing algorithm:**\n>Given: Prediction vectors $\\boldsymbol{\\hat{\\textbf{u}}}_{j|i}$, number of routing iterations $r$\n\n>for all capsule $i$ in layer $l$ and capsule $j$ in layer $l+1$: $b_{ij} = 0$ \n\n>for $r$ iterations do:\n\n>>for all capsules $i$ in the layer $l$: $c_i = softmax(b_i)$ \n>>**(the bets of a capsule on high-level capsules should sum to 1)**\n\n>>for all capsules $j$ in the layer $l+1$: $s_j = \\sum_{i=1}^{m}{c_{ij}\\boldsymbol{\\hat{\\textbf{u}}}_{j|i}}$\n>>**(the output vector is the weighted sum of prediction vectors)**\n\n>>for all capsules $j$ in the layer $l+1$: $\\textbf{v}_j = \\textbf{squash}(\\textbf{s}_j)$\n>>**(apply the activation function)**\n\n>> for all capsule $i$ in layer $l$ and capsule $j$ in layer $l+1$: $b_{ij} = b_{ij} + \\boldsymbol{\\hat{\\textbf{u}}}_{j|i} \\cdot \\textbf{v}_j$\n\n> return $\\textbf{v}_j$\n\nThe last line in the loop is very important. It is here that the routing happens. If the product has $\\boldsymbol{\\hat{\\textbf{u}}}_{j|i} \\cdot \\textbf{v}_j$ is large, it will increase $b_{ij}$ which will increase the corresponding coupling coefficient $c_{ij}$, which in turn, will make the product $\\boldsymbol{\\hat{\\textbf{u}}}_{j|i} \\cdot \\textbf{v}_j$ even larger.\n\nThis is how CapsNet works. At this point, you will find no difficulty in reading the [original paper](https://arxiv.org/pdf/1710.09829.pdf) by Hinton.\n<a id=\"squash\"></a>\n#### A word about squashing function:\nThe derivative of $\\|\\mathbf{s}\\|$ is undefined when $\\|\\mathbf{s}\\|=0$, and it may blow up during training: if a vector is zero, the gradients will be `nan`, so when the optimizer updates the variables, they will also become `nan`. The solution is to implement the norm manually by computing the square root of the sum of squares plus a tiny epsilon value: $\\|\\mathbf{s}\\| \\approx \\sqrt{\\sum\\limits_i{{s_i}^2}\\,\\,+ \\epsilon}$.\n<a id=\"advantage\"></a>\n#### What is the advantage?\nIn a CNN, there are pooling layers. We generally use MaxPool which is a very primitive type of routing mechanism. The most active feature in a local pool (say 4x4 grid) is routed to the higher layer and the higher-level detectors don't have a say in the routing. Compare this with the routing-by-agreement mechanism introduced in the CapsNet. Only those features that agree with high-level detectors are routed. This is the advantage of CapsNet over CNN. It has a superior dynamic routing mechanism (dynamic because the information to be routed is determined in real time)."},{"metadata":{"_cell_guid":"1661d3c5-b9d6-4340-a268-26c6e7fdabfe","_uuid":"62359e3ac6e9ebf7c96d5ece77d74051cb655593"},"cell_type":"markdown","source":"<a id=\"boilerplate\"></a>\n## Boilerplate Code\n#### Essential imports"},{"metadata":{"_cell_guid":"4c52bea6-9dcf-46b3-b07a-d6bc156182b6","_kg_hide-output":true,"collapsed":true,"_uuid":"e900d3cf3df9e77110ab7d79f1ccbe3dd068b8f9","trusted":false},"cell_type":"code","source":"import gc\nimport os\nimport nltk\nimport tqdm\nimport numpy as np\nimport pandas as pd\nnltk.download(\"punkt\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"449b057d-d61f-462b-a1a4-3b80b43106c0","collapsed":true,"_uuid":"1f1ad2cc343f8e2c74bade9b631e9af7680e5ddd","trusted":false},"cell_type":"code","source":"def tokenize_sentences(sentences, words_dict):\n    tokenized_sentences = []\n    for sentence in tqdm.tqdm(sentences):\n        if hasattr(sentence, \"decode\"):\n            sentence = sentence.decode(\"utf-8\")\n        tokens = nltk.tokenize.word_tokenize(sentence)\n        result = []\n        for word in tokens:\n            word = word.lower()\n            if word not in words_dict:\n                words_dict[word] = len(words_dict)\n            word_index = words_dict[word]\n            result.append(word_index)\n        tokenized_sentences.append(result)\n    return tokenized_sentences, words_dict","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"07acd58d-93f6-4a65-8b2b-bdc20b0f1fb2","collapsed":true,"_uuid":"b4715a55f35ba98018ab948f2f6859fbcb9da505","trusted":false},"cell_type":"code","source":"def read_embedding_list(file_path):\n    embedding_word_dict = {}\n    embedding_list = []\n    f = open(file_path)\n\n    for index, line in enumerate(f):\n        if index == 0:\n            continue\n        values = line.split()\n        word = values[0]\n        try:\n            coefs = np.asarray(values[1:], dtype='float32')\n        except:\n            continue\n        embedding_list.append(coefs)\n        embedding_word_dict[word] = len(embedding_word_dict)\n    f.close()\n    embedding_list = np.array(embedding_list)\n    return embedding_list, embedding_word_dict","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8e09726c-5bd1-4344-8290-69f6b87fda18","collapsed":true,"_uuid":"4a0c79a8e741d3f4f6a829c79e0ce2dd16db1593","trusted":false},"cell_type":"code","source":"def clear_embedding_list(embedding_list, embedding_word_dict, words_dict):\n    cleared_embedding_list = []\n    cleared_embedding_word_dict = {}\n\n    for word in words_dict:\n        if word not in embedding_word_dict:\n            continue\n        word_id = embedding_word_dict[word]\n        row = embedding_list[word_id]\n        cleared_embedding_list.append(row)\n        cleared_embedding_word_dict[word] = len(cleared_embedding_word_dict)\n\n    return cleared_embedding_list, cleared_embedding_word_dict","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"eded41b7-677b-4079-aa27-64e532c9bc52","collapsed":true,"_uuid":"4fe8621eb24bd507cafbdd31c4c2146110c656ae","trusted":false},"cell_type":"code","source":"def convert_tokens_to_ids(tokenized_sentences, words_list, embedding_word_dict, sentences_length):\n    words_train = []\n\n    for sentence in tokenized_sentences:\n        current_words = []\n        for word_index in sentence:\n            word = words_list[word_index]\n            word_id = embedding_word_dict.get(word, len(embedding_word_dict) - 2)\n            current_words.append(word_id)\n\n        if len(current_words) >= sentences_length:\n            current_words = current_words[:sentences_length]\n        else:\n            current_words += [len(embedding_word_dict) - 1] * (sentences_length - len(current_words))\n        words_train.append(current_words)\n    return words_train","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"681bea70-8a75-46af-aaed-b9726b4d0bda","_uuid":"d438dd177f790e46e2bbf82574fb7d63b1416669"},"cell_type":"markdown","source":"<a id=\"capsnet_model\"></a>\n### Capsule Network Model\nThe Architecture of our CapsNet is very similar to general architecture, except for an addition Capsule Layer.\n\n![Text Classification](https://raw.githubusercontent.com/zaffnet/images/master/images/comparison.jpg)\n\n\n#### Advantage of Capsule Layer in Text Classification\nAs you can see, we have used Capsule layer instead of Pooling layer. Capsule Layer eliminates the need for forced pooling layers like MaxPool. In many cases, this is desired because we get translational invariance without losing minute details."},{"metadata":{"_cell_guid":"a631c494-99ae-41db-a119-cd8f98209e03","_kg_hide-output":true,"collapsed":true,"_uuid":"327232d13ddd4549edec2b185bf96c0bf6b1ed87","trusted":false},"cell_type":"code","source":"from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\nfrom keras.engine import Layer\nfrom keras.layers import Activation, Add, Bidirectional, Conv1D, Dense, Dropout, Embedding, Flatten\nfrom keras.layers import concatenate, GRU, Input, K, LSTM, MaxPooling1D\nfrom keras.layers import GlobalAveragePooling1D,  GlobalMaxPooling1D, SpatialDropout1D\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nfrom keras.preprocessing import text, sequence\nfrom sklearn.metrics import accuracy_score, roc_auc_score, log_loss\nfrom sklearn.model_selection import train_test_split\nfrom keras import initializers, regularizers, constraints, optimizers, layers, callbacks","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4758b588-6cf9-41d9-97f6-85ea9c7e59ab","_uuid":"88116eab2cbc77c61dc4776d40e01f5ef5c62f5c"},"cell_type":"markdown","source":"#### CapsNet parameters"},{"metadata":{"_cell_guid":"08637271-0dce-4d53-a807-cd4516cb567d","collapsed":true,"_uuid":"a7f0a6791da9b5d0c4a57b9ae08d7667e37ad40d","trusted":false},"cell_type":"code","source":"gru_len = 128\nRoutings = 5\nNum_capsule = 10\nDim_capsule = 16\ndropout_p = 0.3\nrate_drop_dense = 0.3","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"aa7b6069-897e-4e50-b3a9-14f9fe2de59d","collapsed":true,"_uuid":"57bd4165c2cf1af5e268bd253c6d201d222a5f8a","trusted":false},"cell_type":"code","source":"def squash(x, axis=-1):\n    s_squared_norm = K.sum(K.square(x), axis, keepdims=True)\n    scale = K.sqrt(s_squared_norm + K.epsilon())\n    return x / scale","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"46eb605f-9281-4171-82da-ed4f541db6b6","_uuid":"8be15963a074dde02ace1a1d3ca103bb28f2eb26"},"cell_type":"markdown","source":"#### Capsule Layer"},{"metadata":{"_cell_guid":"182cd19b-cdb3-4897-95d6-a32af7eddb94","collapsed":true,"_uuid":"6fe7d0b616cd46e74c72004f9ad90b1aec2bab22","trusted":false},"cell_type":"code","source":"class Capsule(Layer):\n    def __init__(self, num_capsule, dim_capsule, routings=3, kernel_size=(9, 1), share_weights=True,\n                 activation='default', **kwargs):\n        super(Capsule, self).__init__(**kwargs)\n        self.num_capsule = num_capsule\n        self.dim_capsule = dim_capsule\n        self.routings = routings\n        self.kernel_size = kernel_size\n        self.share_weights = share_weights\n        if activation == 'default':\n            self.activation = squash\n        else:\n            self.activation = Activation(activation)\n\n    def build(self, input_shape):\n        super(Capsule, self).build(input_shape)\n        input_dim_capsule = input_shape[-1]\n        if self.share_weights:\n            self.W = self.add_weight(name='capsule_kernel',\n                                     shape=(1, input_dim_capsule,\n                                            self.num_capsule * self.dim_capsule),\n                                     # shape=self.kernel_size,\n                                     initializer='glorot_uniform',\n                                     trainable=True)\n        else:\n            input_num_capsule = input_shape[-2]\n            self.W = self.add_weight(name='capsule_kernel',\n                                     shape=(input_num_capsule,\n                                            input_dim_capsule,\n                                            self.num_capsule * self.dim_capsule),\n                                     initializer='glorot_uniform',\n                                     trainable=True)\n\n    def call(self, u_vecs):\n        if self.share_weights:\n            u_hat_vecs = K.conv1d(u_vecs, self.W)\n        else:\n            u_hat_vecs = K.local_conv1d(u_vecs, self.W, [1], [1])\n\n        batch_size = K.shape(u_vecs)[0]\n        input_num_capsule = K.shape(u_vecs)[1]\n        u_hat_vecs = K.reshape(u_hat_vecs, (batch_size, input_num_capsule,\n                                            self.num_capsule, self.dim_capsule))\n        u_hat_vecs = K.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n        # final u_hat_vecs.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n\n        b = K.zeros_like(u_hat_vecs[:, :, :, 0])  # shape = [None, num_capsule, input_num_capsule]\n        for i in range(self.routings):\n            b = K.permute_dimensions(b, (0, 2, 1))  # shape = [None, input_num_capsule, num_capsule]\n            c = K.softmax(b)\n            c = K.permute_dimensions(c, (0, 2, 1))\n            b = K.permute_dimensions(b, (0, 2, 1))\n            outputs = self.activation(K.batch_dot(c, u_hat_vecs, [2, 2]))\n            if i < self.routings - 1:\n                b = K.batch_dot(outputs, u_hat_vecs, [2, 3])\n\n        return outputs\n\n    def compute_output_shape(self, input_shape):\n        return (None, self.num_capsule, self.dim_capsule)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4b606082-b041-4c50-aab1-305295534758","collapsed":true,"_uuid":"4f4444cbe95d966d5e632f5af88ebb9b932dfed7","trusted":false},"cell_type":"code","source":"def get_model(embedding_matrix, sequence_length, dropout_rate, recurrent_units, dense_size):\n    input1 = Input(shape=(sequence_length,))\n    embed_layer = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1],\n                                weights=[embedding_matrix], trainable=False)(input1)\n    embed_layer = SpatialDropout1D(rate_drop_dense)(embed_layer)\n\n    x = Bidirectional(\n        GRU(gru_len, activation='relu', dropout=dropout_p, recurrent_dropout=dropout_p, return_sequences=True))(\n        embed_layer)\n    capsule = Capsule(num_capsule=Num_capsule, dim_capsule=Dim_capsule, routings=Routings,\n                      share_weights=True)(x)\n    capsule = Flatten()(capsule)\n    capsule = Dropout(dropout_p)(capsule)\n    output = Dense(1, activation='sigmoid')(capsule)\n    model = Model(inputs=input1, outputs=output)\n    model.compile(\n        loss='binary_crossentropy',\n        optimizer='adam',\n        metrics=['accuracy'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"34865130-068a-41f7-887f-b9c5920586e2","collapsed":true,"_uuid":"6e612062548efc64ad675350650482c59c868a39","trusted":false},"cell_type":"code","source":"def _train_model(model, batch_size, train_x, train_y, val_x, val_y):\n    num_labels = train_y.shape[1]\n    patience = 5\n    best_loss = -1\n    best_weights = None\n    best_epoch = 0\n    \n    current_epoch = 0\n    \n    while True:\n        model.fit(train_x, train_y, batch_size=batch_size, epochs=1)\n        y_pred = model.predict(val_x, batch_size=batch_size)\n\n        total_loss = 0\n        for j in range(num_labels):\n            loss = log_loss(val_y[:, j], y_pred[:, j])\n            total_loss += loss\n\n        total_loss /= num_labels\n\n        print(\"Epoch {0} loss {1} best_loss {2}\".format(current_epoch, total_loss, best_loss))\n\n        current_epoch += 1\n        if total_loss < best_loss or best_loss == -1:\n            best_loss = total_loss\n            best_weights = model.get_weights()\n            best_epoch = current_epoch\n        else:\n            if current_epoch - best_epoch == patience:\n                break\n\n    model.set_weights(best_weights)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"13a836bc-d251-438a-b135-73fa4428b193","collapsed":true,"_uuid":"1f15bfa8b13df55230ace8c32177810a95ce04e0","trusted":false},"cell_type":"code","source":"def train_folds(X, y, X_test, fold_count, batch_size, get_model_func):\n    print(\"=\"*75)\n    fold_size = len(X) // fold_count\n    models = []\n    result_path = \"predictions\"\n    if not os.path.exists(result_path):\n        os.mkdir(result_path)\n    for fold_id in range(0, fold_count):\n        fold_start = fold_size * fold_id\n        fold_end = fold_start + fold_size\n\n        if fold_id == fold_size - 1:\n            fold_end = len(X)\n\n        train_x = np.concatenate([X[:fold_start], X[fold_end:]])\n        train_y = np.concatenate([y[:fold_start], y[fold_end:]])\n\n        val_x = np.array(X[fold_start:fold_end])\n        val_y = np.array(y[fold_start:fold_end])\n\n        model = _train_model(get_model_func(), batch_size, train_x, train_y, val_x, val_y)\n        train_predicts_path = os.path.join(result_path, \"train_predicts{0}.npy\".format(fold_id))\n        test_predicts_path = os.path.join(result_path, \"test_predicts{0}.npy\".format(fold_id))\n        train_predicts = model.predict(X, batch_size=512, verbose=1)\n        test_predicts = model.predict(X_test, batch_size=512, verbose=1)\n        np.save(train_predicts_path, train_predicts)\n        np.save(test_predicts_path, test_predicts)\n\n    return models","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"078ec9ba-e9a8-4868-a5b3-42675d2d00d0","_uuid":"6f385e91ebc684e8aa7a5bcc95c915fc07b98abc"},"cell_type":"markdown","source":"<a id=\"training\"></a>\n### Training\n\n#### IMPORTANT\nDue to time limit in Kaggle kernels, I have restricted the model size and trained it on a small part of the  dataset. The commented values are those for which this model is trained.\n\n"},{"metadata":{"_cell_guid":"cd4771aa-4355-4edd-82bd-07f9779ca9b3","collapsed":true,"_uuid":"7a6033641b32dd560b545b6b4fbba034997e486b","trusted":false},"cell_type":"code","source":"# train_file_path = \"../input/donorschooseorg-preprocessed-data/train_preprocessed.csv\"\ntrain_file_path = \"../input/donorschooseorg-preprocessed-data/train_small.csv\"\n\n# test_file_path = \"../input/donorschooseorg-preprocessed-data/test_preprocessed.csv\"\ntest_file_path = \"../input/donorschooseorg-preprocessed-data/test_small.csv\"\n\n# embedding_path = \"../input/fatsttext-common-crawl/crawl-300d-2M/crawl-300d-2M.vec\"\nembedding_path = \"../input/donorschooseorg-preprocessed-data/embeddings_small.vec\"\n\nbatch_size = 128 # 256\nrecurrent_units = 16 # 64\ndropout_rate = 0.3 \ndense_size = 8 # 32\nsentences_length = 10 # 300\nfold_count = 2 # 10","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"66f3990e-32b3-4428-bbb8-51be54b493cb","collapsed":true,"_uuid":"30d622993e434548e6b26bcf46317c168a160206","trusted":false},"cell_type":"code","source":"UNKNOWN_WORD = \"_UNK_\"\nEND_WORD = \"_END_\"\nNAN_WORD = \"_NAN_\"\nCLASSES = [\"project_is_approved\"]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"63f8cf0d-9d52-41df-8a91-eb008aec7fb3","_kg_hide-output":true,"collapsed":true,"_uuid":"66a9d49708a013d1da00b945e1dddaf87fe340fc","trusted":false},"cell_type":"code","source":"# Load data\nprint(\"Loading data...\")\ntrain_data = pd.read_csv(train_file_path)\ntest_data = pd.read_csv(test_file_path)\nlist_sentences_train = train_data[\"application_text\"].fillna(NAN_WORD).values\nlist_sentences_test = test_data[\"application_text\"].fillna(NAN_WORD).values\ny_train = train_data[CLASSES].values","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"991fa3fc-bd61-44db-9c9e-8b064c4480ba","_kg_hide-output":true,"collapsed":true,"_uuid":"f0c0539daa59465f541c4b9628f4fd2ad4f88a1c","trusted":false},"cell_type":"code","source":"print(\"Tokenizing sentences in train set...\")\ntokenized_sentences_train, words_dict = tokenize_sentences(list_sentences_train, {})\nprint(\"Tokenizing sentences in test set...\")\ntokenized_sentences_test, words_dict = tokenize_sentences(list_sentences_test, words_dict)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"427256fc-d559-42a5-a11f-a245f46762a7","_kg_hide-output":true,"collapsed":true,"_uuid":"527fe0400d6baf2c35e7371234a3ae01385bfd5d","trusted":false},"cell_type":"code","source":"# Embedding\nwords_dict[UNKNOWN_WORD] = len(words_dict)\nprint(\"Loading embeddings...\")\nembedding_list, embedding_word_dict = read_embedding_list(embedding_path)\nembedding_size = len(embedding_list[0])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1457f67c-b4a1-4f57-8cf7-e10845a34ff7","_kg_hide-output":true,"collapsed":true,"_uuid":"8af39d90468df68ca6566422dba9b05458c153ef","trusted":false},"cell_type":"code","source":"print(\"Preparing data...\")\nembedding_list, embedding_word_dict = clear_embedding_list(embedding_list, embedding_word_dict, words_dict)\n\nembedding_word_dict[UNKNOWN_WORD] = len(embedding_word_dict)\nembedding_list.append([0.] * embedding_size)\nembedding_word_dict[END_WORD] = len(embedding_word_dict)\nembedding_list.append([-1.] * embedding_size)\n\nembedding_matrix = np.array(embedding_list)\n\nid_to_word = dict((id, word) for word, id in words_dict.items())\ntrain_list_of_token_ids = convert_tokens_to_ids(\n    tokenized_sentences_train,\n    id_to_word,\n    embedding_word_dict,\n    sentences_length)\ntest_list_of_token_ids = convert_tokens_to_ids(\n    tokenized_sentences_test,\n    id_to_word,\n    embedding_word_dict,\n    sentences_length)\nX_train = np.array(train_list_of_token_ids)\nX_test = np.array(test_list_of_token_ids)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c8222729-dba3-4e74-80ea-a53e45a7d1ba","_kg_hide-output":true,"collapsed":true,"_uuid":"1e3081e1c8e3fa5644e28639f3908519a270bea0","trusted":false},"cell_type":"code","source":"get_model_func = lambda: get_model(\n    embedding_matrix,\n    sentences_length,\n    dropout_rate,\n    recurrent_units,\n    dense_size)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4b24361b-e012-499b-98f4-ddd964cf74c9","_kg_hide-output":true,"collapsed":true,"_uuid":"c60cae2220beb0190fd1f41d685e30a811ed8f7d","trusted":false},"cell_type":"code","source":"del train_data, test_data, list_sentences_train, list_sentences_test\ndel tokenized_sentences_train, tokenized_sentences_test, words_dict\ndel embedding_list, embedding_word_dict\ndel train_list_of_token_ids, test_list_of_token_ids\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"065825da-aa27-4ba4-a460-1bc5c87b111b","_kg_hide-output":true,"collapsed":true,"_uuid":"abea6ac5481a36615f7cc83851de848c193c5334","trusted":false},"cell_type":"code","source":"print(\"Starting to train models...\")\nmodels = train_folds(X_train, y_train, X_test, fold_count, batch_size, get_model_func)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0d4d100f-f7ca-4ce4-886e-24ffcae3457b","_uuid":"0814f5a555073ef039a74b853270af2f2dd11f72"},"cell_type":"markdown","source":"<a id=\"submission\"></a>\n### Submission\n\nWe trained the model for 10 folds using default parameters. We will make a rank-averaged submission."},{"metadata":{"_cell_guid":"e56f9fd2-d532-41e0-9d89-0786575303e1","_kg_hide-output":true,"collapsed":true,"_uuid":"0baf1585fd832f28798c803dd7066f113e883f50","trusted":false},"cell_type":"code","source":"from scipy.stats import rankdata\n\nLABELS = [\"project_is_approved\"]\n\nbase = \"../input/donorschooseorg-application-screening-predictions/predictions/predictions/\"\npredict_list = []\nfor j in range(10):\n    predict_list.append(np.load(base + \"predictions_001/test_predicts%d.npy\"%j))\n    \nprint(\"Rank averaging on \", len(predict_list), \" files\")\npredcitions = np.zeros_like(predict_list[0])\nfor predict in predict_list:\n    predcitions = np.add(predcitions.flatten(), rankdata(predict)/predcitions.shape[0])  \npredcitions /= len(predict_list)\n\nsubmission = pd.read_csv('../input/donorschoose-application-screening/sample_submission.csv')\nsubmission[LABELS] = predcitions\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b5073f22-4b02-458f-8346-b72a5a5f0828","_uuid":"13c1c13f73a41bf1bdf6df7caca8efc62ad02665"},"cell_type":"markdown","source":"<a id=\"conclusion\"></a>\n### Conclusion\nWe can see that a Capsule Network can be helpful in Text Classification. Even without any hyperparameter tuning, one can build a strong baseline using CapsNet. \n<a id=\"references\"></a>\n### References\n* [Dynamic Routing Between Capsules](https://arxiv.org/abs/1710.09829)\n* [Understanding Hinton’s Capsule Networks](https://medium.com/ai%C2%B3-theory-practice-business/understanding-hintons-capsule-networks-part-i-intuition-b4b559d1159b)\n* [Capsule Networks (CapsNets) – Tutorial](https://www.youtube.com/watch?v=pPN8d0E3900)"},{"metadata":{"_kg_hide-input":true,"_cell_guid":"afe4dbf8-491a-4869-bf1d-ffbb25df8e9f","collapsed":true,"_uuid":"951d44d36e9a89315e8d647c8db99476ade5a688","trusted":false},"cell_type":"code","source":"from IPython.lib.display import YouTubeVideo\nYouTubeVideo('pPN8d0E3900', width=800, height=450)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"931ddf0c-7026-41ec-8bd2-479bcd18cddf","collapsed":true,"_uuid":"6a402e8e6e57959942ed3d82db7d61035854e29e","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}