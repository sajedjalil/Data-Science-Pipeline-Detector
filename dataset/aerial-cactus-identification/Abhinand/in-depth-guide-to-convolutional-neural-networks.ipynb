{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nIn this notebook we'll go over defining, training and testing as well as the pre-processing steps needed to feed an image into CNNs to get state of the art results. We'll be using PyTorch for this tutorial. PyTorch is a powerful deep learning framework which is rising in popularity, and it is thoroughly at home in Python which makes it really easy to learn and use. This tutorial wonâ€™t assume much in regards to prior knowledge of PyTorch, but it might be helpful to checkout [my previous introductory CV tutorial](https://www.kaggle.com/abhinand05/mnist-introduction-to-computervision-with-pytorch). \n\n\n![FeaturedImage](https://i.ibb.co/ws3htpn/2088474-6a86-3.jpg)\n\n\n\nIn this notebook, we'll train a CNN to classify images based on whether they have a columnar cactus or not. We'll use the Aerial Cactus Dataset from [this currently running Kaggle competition](https://www.kaggle.com/c/aerial-cactus-identification/overview). For more information about the dataset visit [this page](https://www.kaggle.com/c/aerial-cactus-identification/data). I picked this competition because I felt it is the best place for beginners to practice their new found skills with CNNs as MNIST is just way too simple to bring CNNs into play, a regular Multi-layer perceptron may well do the job. So, this is a perfect beginners competition as someone rightly said in the discussion forums. \n\n### **If you like this kernel or wish to fork it please give it an UPVOTE to show your appreciation.**"},{"metadata":{},"cell_type":"markdown","source":"# Import Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nprint(os.listdir('../input/'))\n\nimport matplotlib.pyplot as plt\nplt.style.use(\"ggplot\")\n\n# OpenCV Image Library\nimport cv2\n\n# Import PyTorch\nimport torchvision.transforms as transforms\nfrom torch.utils.data.sampler import SubsetRandomSampler\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader, Dataset\nimport torchvision\nimport torch.optim as optim","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading Training Data + EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/train.csv\")\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The train data contains 17500 images which can be found in a seperate directory also we have a csv file but we can't directly visualize them as we'll see later.\n\nThe test data has 4000 images and it is stored in a seperate directory. Note that it doesn't have a csv file as we saw for the train data."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Train Size: {len(os.listdir('../input/train/train'))}\")\nprint(f\"Test Size: {len(os.listdir('../input/test/test'))}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we inspect the data to see the amount of training samples for each class. As we can see about 2/3 of the training data belongs to one class. If the dataset you are using contains almost or above 90% of the training data belonging to one single class then it will greatly impact your results. This is called skewed classes, we can use data augmentation, sampling and several ways to overcome this. However I don't believe that's a problem here as we have enough data for CNNs to get great results. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Counting the number of sample data for each class\nvalue_counts = train_df.has_cactus.value_counts()\n%matplotlib inline\nplt.pie(value_counts, labels=['Has Cactus', 'No Cactus'], autopct='%1.1f', colors=['green', 'red'], shadow=True)\nplt.figure(figsize=(5,5))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Setting up Train Data for Pytorch \nWe cannot simply use the raw image data to make predictions using PyTorch. There are several pre-processing steps involved which we'll discuss in detail in this section.\n\n**Step 1:** \n\nFirst up we define a custom class that extends PyTorch's `torch.utils.data.Dataset` class. I think everything in there is pretty straight-forward. We define our constructors and add two different methods `len` and `getitem` which essentially replaces the parent definitions.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data paths\ntrain_path = '../input/train/train/'\ntest_path = '../input/test/test/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Our own custom class for datasets\nclass CreateDataset(Dataset):\n    def __init__(self, df_data, data_dir = './', transform=None):\n        super().__init__()\n        self.df = df_data.values\n        self.data_dir = data_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        img_name,label = self.df[index]\n        img_path = os.path.join(self.data_dir, img_name)\n        image = cv2.imread(img_path)\n        if self.transform is not None:\n            image = self.transform(image)\n        return image, label","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Step 2:**\n\nNow that we've defined our class it is time to pass the raw data and convert it to PyTorch understandable form. \n\n**Transforms** - We can use the transforms feature in PyTorch to apply Data Augmentations which help us improve the accuracy of our model when done right. There are sevaral transformations that can be applied for which you may want to have a look at the [documentation here](https://pytorch.org/docs/stable/torchvision/transforms.html). Here we conert it to a PIL image first. Random horizontal flipping of sample images is applied along with random rotation of 10 degrees for random training examples. We then convert the images into a PyTorch tensor then we normalize the them.\n\n**Creating our Dataset -** We then use our `CreateDataset` class to covert the raw data in the way PyTorch expects. We have also applied the transformations there. "},{"metadata":{"trusted":true},"cell_type":"code","source":"transforms_train = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(10),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\ntrain_data = CreateDataset(df_data=train_df, data_dir=train_path, transform=transforms_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Step 3:** \n\nBatch size is set. The batch size is usually set between 64 and 256. The batch size does have an effect on the final test accuracy. One way to think about it is that smaller batches means that the number of parameter updates per epoch is greater. \n\nThen the percentage of data needed for validation is set to 20%, which almost always seem to work for me. But in the end it's just another hyperparameter you can tune.\n\nIn the next steps we use the `torch.utils.data.samplerSubsetRandomSampler` function to split our data into training and validation sets, which is similar to `train_test_split` fuction of scikit-learn.\n\nWe have the training data passed into the trainloader. We can make an iterator with iter(trainloader) that can help us grab data. Later, we'll use this to loop through the dataset for training. Each time we can pull out data of the size of the batch_size defined."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set Batch Size\nbatch_size = 64\n\n# Percentage of training set to use as validation\nvalid_size = 0.2\n\n# obtain training indices that will be used for validation\nnum_train = len(train_data)\nindices = list(range(num_train))\nnp.random.shuffle(indices)\nsplit = int(np.floor(valid_size * num_train))\ntrain_idx, valid_idx = indices[split:], indices[:split]\n\n# Create Samplers\ntrain_sampler = SubsetRandomSampler(train_idx)\nvalid_sampler = SubsetRandomSampler(valid_idx)\n\n# prepare data loaders (combine dataset and sampler)\ntrain_loader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\nvalid_loader = DataLoader(train_data, batch_size=batch_size, sampler=valid_sampler)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Setting up Test Data\nThis section of code should make sense to you now. The same steps used above are repeated for the test data as well. \n\nNote that we don't augment our data on the training set. This is because augmentation is only done on the training set to improve the performance by giving our model complex variations that might make it generalize well to new samples on the test set, so it doesn't make sense to do data augmentation on the test data as well. However we still need to convert it to a tensor and normalize it."},{"metadata":{"trusted":true},"cell_type":"code","source":"transforms_test = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\n# creating test data\nsample_sub = pd.read_csv(\"../input/sample_submission.csv\")\ntest_data = CreateDataset(df_data=sample_sub, data_dir=test_path, transform=transforms_test)\n\n# prepare the test loader\ntest_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualize Images"},{"metadata":{},"cell_type":"markdown","source":"Visualizing the images and inspecting them to get a better idea of what they are is always helpful before going on the construct our model to make predictions.\n\nWe can see aerial images of cactus. The images contain color channels, we should keep in mind and they are 32x32 images. Note that these images are very low  inresolution yet as humans we have evolved in such a way to make sense of even these kind of pictures. We're going to build a CNN in the next few sections to ultimately achieve just that or even better performance."},{"metadata":{"trusted":true},"cell_type":"code","source":"classes = [ 'No Cactus','Cactus']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def imshow(img):\n    '''Helper function to un-normalize and display an image'''\n    # unnormalize\n    img = img / 2 + 0.5\n    # convert from Tensor image and display\n    plt.imshow(np.transpose(img, (1, 2, 0)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# obtain one batch of training images\ndataiter = iter(train_loader)\nimages, labels = dataiter.next()\nimages = images.numpy() # convert images to numpy for display\n\n# plot the images in the batch, along with the corresponding labels\nfig = plt.figure(figsize=(25, 4))\n# display 20 images\nfor idx in np.arange(20):\n    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n    imshow(images[idx])\n    ax.set_title(classes[labels[idx]])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Viewing an Image in More Detail\nHere, we look at the normalized red, green, and blue (RGB) color channels as three separate, grayscale intensity images for illustration purposes. This helps me explain CNNs later.\n\nEach little square you can see are pixels with their normalized values. Larger value means brighter, lower means darker. This is how images are represented in our computers. When we combine all three channels we get a color image. This RGB color scheme can represent about 16.77 million colors, which is awesome. \n\nAs you might have guessed already color images are actually three dimensional, where Greyscale images for example are 1D."},{"metadata":{"trusted":true},"cell_type":"code","source":"rgb_img = np.squeeze(images[3])\nchannels = ['red channel', 'green channel', 'blue channel']\n\nfig = plt.figure(figsize = (36, 36)) \nfor idx in np.arange(rgb_img.shape[0]):\n    ax = fig.add_subplot(3, 1, idx + 1)\n    img = rgb_img[idx]\n    ax.imshow(img, cmap='gray')\n    ax.set_title(channels[idx])\n    width, height = img.shape\n    thresh = img.max()/2.5\n    for x in range(width):\n        for y in range(height):\n            val = round(img[x][y],2) if img[x][y] !=0 else 0\n            ax.annotate(str(val), xy=(y,x),\n                    horizontalalignment='center',\n                    verticalalignment='center', size=8,\n                    color='white' if img[x][y]<thresh else 'black')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Multi-Layer Perceptrons (MLP) Vs Convolutional Neural Networks (CNN)\nAs you might have seen in [my previous tutorial](https://www.kaggle.com/abhinand05/mnist-introduction-to-computervision-with-pytorch) on MNIST dataset, MLPs were good enough to score more than 90% accuracy. It turns out that they're nowhere near good enough to achieve similar results on complex datasets mainly because they expect a flattened vector of the original image like the one below. MNIST dataset is an exception as it comes already well processed.\n\n![Example](https://github.com/abhinand5/CNNs-in-PyTorch/blob/master/cifar-cnn/FireShot%20Capture%20084%20-%20Intro%20to%20Deep%20Learning%20with%20PyTorch%20-%20Udacity%20-%20classroom.udacity.com.png?raw=true)\n\nSo all it sees is a vector and treats it just as a vector with no special structure. It has no knowledge whatsoever of the fact that it was previously arranged in a grid, losing important information. If we do the same for color images we throw away vital information by flattening it, which almost never works for real-world images. CNNs on the other hand work exactly the other way around, capable of working with and elucidating patterns from multi-dimensional data, that is why they are so powerful. Unlike MLPs CNNs actually understand the information from pixels that are in close proximity to each other and are more related to each other than those pixels that are far apart and unrelated. \n"},{"metadata":{},"cell_type":"markdown","source":"## Understanding CNNs\n\nConolutional Neural Networks make use of three different classes of layers which distinctly differ with each other. But when combined they give extraordinary results. \n\n* Convolutional Layers\n* Pooling Layers\n* Fully Connected Layers\n\nWe'll go over each one of there layers in great detail. \n\nI'll use an example of classifying a car here to explain different layers of a Convolutional Neural Network.\n\n![Layers](https://s3.amazonaws.com/video.udacity-data.com/topher/2018/May/5b1070e4_screen-shot-2018-05-31-at-2.59.36-pm/screen-shot-2018-05-31-at-2.59.36-pm.png)\n\n**Convolutional Layer:**\n\nTurns out that in order to understand Convolutional layer first we have to understand a concept called Filters. Expecially high-pass filters. \n\nTo detect changes in intensity in an image, we'll be using and creating specific image filters that look at groups of pixels and react to alternating patterns of dark/light pixels. These filters produce an output that shows edges of objects and differing textures. \n\nFor this we first contstruct a filter according to a size called window size, which is nothing but a matrix like this.\n\nHere in an example gif.\n\n![Example](https://media.giphy.com/media/jrzu0JxxZydz0valeu/giphy.gif)\n\nThese windows are usually of size 3x3 which helps CNNs identify the patterns in an image. These filters can be modified to get different filters as output. There may be several filters in a Convolutional layer.\n\nThe convolutional layer is produced by applying a series of many different image filters, also known as convolutional kernels, to an input image.\n\n![Example](https://s3.amazonaws.com/video.udacity-data.com/topher/2018/May/5b10723a_screen-shot-2018-05-31-at-3.06.07-pm/screen-shot-2018-05-31-at-3.06.07-pm.png)\n\nIn the example shown, 4 different filters produce 4 differently filtered output images. When we stack these images, we form a complete convolutional layer with a depth of 4!\n\n![Example](https://s3.amazonaws.com/video.udacity-data.com/topher/2018/May/5b10729b_screen-shot-2018-05-31-at-3.07.03-pm/screen-shot-2018-05-31-at-3.07.03-pm.png)\n\nThe depth of each consecutive convolutional layers may increase resulting in the networks capturing some incredibly complex features/patterns from the images. In fact this is very much similar to how our brains interpret images in an instant. \n\n**Pooling Layer:**\n\nPooling is used in Convolutional Neural Networks to make the detection of certain features somewhat invariant to scale and orientation changes. Another way of thinking about what pooling does is that it generalizes over lower level, more complex information.\n\nHow it is done is a window of fixed size of made to stroll over the image and inside the window a specific value according to a metric is calculated an a new tensor is formed. When we take the maximum element from the window it is known as Max-Pooling which is the most common pooling technique. There is also average pooling which you may see out on the wild. Here is an illustration which you might find helpful. There are also alternatives to Pooling, like Capsule Networks which is out of the score of this kernel. \n\nHere in an example gif of Max Pooling.\n\n![Example](https://media.giphy.com/media/U7PsR7cv9oIcB6eEAd/giphy.gif)\n\nWhen we combine pooling layers with convolutional layers, we reduce the dimension of the layers which helps in computation but more than that it picks the pixel values of higher significance. So our model so far looks something like this...\n\n![Example](https://i.ibb.co/V06mcY0/Intro-to-Deep-Learning-with-Py-Torch-Udacity-classroom-udacity-com.png)\n\n**Fully Connected Layers:**\n\nFully connected layers are no different from the ones you already know from MLPs. It is the final piece of the puzzle that makes it special and powerful. An even more refined form is passed on to FC layers to make the final prediction.\n\nHere comes our final model...\n\n![Example](http://cs231n.github.io/assets/cnn/convnet.jpeg)\n\n\nI know this may not be the best of definitions you would see for CNNs but the goal here is to make it atleast vaguely understandable. There is much more happening behind the scenes which I skipped. I encourage you to go out and explore on your own to find those awesome stuff. Hope it made sense to you. Let's move on.\n\n### **If you like this kernel or wish to fork it please give it an UPVOTE to show your appreciation.**\n\nImage Credits: Stanford CS231n, Udacity"},{"metadata":{},"cell_type":"markdown","source":"## Define the Network Architecture\n\nHere comes the important part of defining a CNN which will can done using the `torch.nn` module . First, you must define a Model class and fill in two functions `__init__` and `__forward__`. Now that you understand how a CNN works everything is pretty much self-explanatory. Read the docs for `nn.Conv2d` to know more about the parameters. \n\n* We define the convolutional layers first. Details can be mostly interpreted from the comment lines itself. But let me just explain one. Our image (RGB) has 3 channels so depth=3, that's why our first conv layer has 3 incoming channels, I've decided to have 16 filters for the first conv layer so out_channels=16 as it produces 16 different filters, the kernel size aka window size is set to 3 with padding=1 which makes up extra spaces on the edges of the image to help the 3x3 kernel slide over all the edge pixels of the image in case of size mismatch. \n\n* We then go on and define the pooling layers where a window size and stride of 2 is set. Exactly same as the example gif you saw previously. \n\n* The outputs are then connected to a FC layer.\n\n* Dropout is a regularization technique to avoid overfitting, which is also added. Everything is  put together in a forward propagation function later. \n\n* You might also want to use Batch Normalization but I decided not to because I achieved more than 99% accuracy without it.\n\n* The outputs here don't need to be 0 or 1, what Kaggle expects is the probabilities so we don't need an activation function like sigmoid on the output layer."},{"metadata":{"trusted":true},"cell_type":"code","source":"class CNN(nn.Module):\n    def __init__(self):\n        super(CNN, self).__init__()\n        # Convolutional Layer (sees 32x32x3 image tensor) \n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)\n        # Convolutional Layer (sees 16x16x16 image tensor)\n        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n        # Convolutional Layer (sees 8x8x32 image tensor)\n        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n        # Convolutional Layer (sees 4*4*64 image tensor)\n        self.conv4 = nn.Conv2d(64, 128, 3, padding=1)\n        # Maxpooling Layer\n        self.pool = nn.MaxPool2d(2, 2)\n        # Linear Fully-Connected Layer 1 (sees 2*2*128 image tensor)\n        self.fc1 = nn.Linear(128*2*2, 512)\n        # Linear FC Layer 2\n        self.fc2 = nn.Linear(512, 2)\n        # Set Dropout\n        self.dropout = nn.Dropout(0.2)\n        \n    def forward(self, x):\n        # add sequence of convolutional and max pooling layers\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = self.pool(F.relu(self.conv3(x)))\n        x = self.pool(F.relu(self.conv4(x)))\n        # flatten image input\n        x = x.view(-1, 128 * 2 * 2)\n        # add dropout layer\n        x = self.dropout(x)\n        # add 1st hidden layer, with relu activation function\n        x = F.relu(self.fc1(x))\n        # add dropout layer\n        x = self.dropout(x)\n        # add 2nd hidden layer, with relu activation function\n        x = self.fc2(x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check if CUDA is available\ntrain_on_gpu = torch.cuda.is_available()\n\nif not train_on_gpu:\n    print('CUDA is not available.  Training on CPU ...')\nelse:\n    print('CUDA is available!  Training on GPU ...')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a complete CNN\nmodel = CNN()\nprint(model)\n\n# Move model to GPU if available\nif train_on_gpu: model.cuda()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training our CNN\n\nNow the training steps are same as that of training an MLP which I explained in a previous kernel. The only difference here is that I'm saving the model everytime the validation loss decreases. Finally we'll get the best model params learned. This is one type of early stopping."},{"metadata":{"trusted":true},"cell_type":"code","source":"# specify loss function (categorical cross-entropy loss)\ncriterion = nn.CrossEntropyLoss()\n\n# specify optimizer\noptimizer = optim.Adamax(model.parameters(), lr=0.001)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# number of epochs to train the model\nn_epochs = 30\n\nvalid_loss_min = np.Inf # track change in validation loss\n\n# keeping track of losses as it happen\ntrain_losses = []\nvalid_losses = []\n\nfor epoch in range(1, n_epochs+1):\n\n    # keep track of training and validation loss\n    train_loss = 0.0\n    valid_loss = 0.0\n    \n    ###################\n    # train the model #\n    ###################\n    model.train()\n    for data, target in train_loader:\n        # move tensors to GPU if CUDA is available\n        if train_on_gpu:\n            data, target = data.cuda(), target.cuda()\n        # clear the gradients of all optimized variables\n        optimizer.zero_grad()\n        # forward pass: compute predicted outputs by passing inputs to the model\n        output = model(data)\n        # calculate the batch loss\n        loss = criterion(output, target)\n        # backward pass: compute gradient of the loss with respect to model parameters\n        loss.backward()\n        # perform a single optimization step (parameter update)\n        optimizer.step()\n        # update training loss\n        train_loss += loss.item()*data.size(0)\n        \n    ######################    \n    # validate the model #\n    ######################\n    model.eval()\n    for data, target in valid_loader:\n        # move tensors to GPU if CUDA is available\n        if train_on_gpu:\n            data, target = data.cuda(), target.cuda()\n        # forward pass: compute predicted outputs by passing inputs to the model\n        output = model(data)\n        # calculate the batch loss\n        loss = criterion(output, target)\n        # update average validation loss \n        valid_loss += loss.item()*data.size(0)\n    \n    # calculate average losses\n    train_loss = train_loss/len(train_loader.sampler)\n    valid_loss = valid_loss/len(valid_loader.sampler)\n    train_losses.append(train_loss)\n    valid_losses.append(valid_loss)\n        \n    # print training/validation statistics \n    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n        epoch, train_loss, valid_loss))\n    \n    # save model if validation loss has decreased\n    if valid_loss <= valid_loss_min:\n        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n        valid_loss_min,\n        valid_loss))\n        torch.save(model.state_dict(), 'best_model.pt')\n        valid_loss_min = valid_loss","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Performance Graph\nI told you how powerful CNNs are now look at that graph. We've achieved state of the art results as promised. "},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\nplt.plot(train_losses, label='Training loss')\nplt.plot(valid_losses, label='Validation loss')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend(frameon=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load Best parameters learned from training into our model to make predictions later\nmodel.load_state_dict(torch.load('best_model.pt'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Make Predictions on Test Set"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Turn off gradients\nmodel.eval()\n\npreds = []\nfor batch_i, (data, target) in enumerate(test_loader):\n    data, target = data.cuda(), target.cuda()\n    output = model(data)\n\n    pr = output[:,1].detach().cpu().numpy()\n    for i in pr:\n        preds.append(i)\n\n# Create Submission file        \nsample_sub['has_cactus'] = preds\nsample_sub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **If you like this kernel or wish to fork it please give it an UPVOTE to show your appreciation.**"},{"metadata":{},"cell_type":"markdown","source":"**Authored By:**\n\n[Abhinand](https://www.kaggle.com/abhinand05)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}