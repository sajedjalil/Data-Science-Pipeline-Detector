{"cells":[{"metadata":{},"cell_type":"markdown","source":"![](https://i.pinimg.com/originals/76/6b/2f/766b2f93981d547dff24d8388939213d.png)\n\nThis kernel will give you an overview of the evolution of Deep Convolution Neural Network architectures and also the tweak to solve **image classification problem**.\n\nWell we are so familiar with this name but have it ever occured to you that why not use a **Artificial Neural Network(ANN)**. Why to use CNN at the first place.?\n\nBecause if you would use Fully connected ANN then you would have to train a lot of parameters and network has to optimize all of these parameters, the training process could then become very time and storage intensive. In order to solve this computational problem, a different kind of network architecture is used, called **Convolutional Neural Network (CNN)**\n\nCNNs **introduce two new types of hidden layers** in which each neuron is only connected to a small subset of the neurons in the previous layer, to prevent the aforementioned problem.\n\n*  **Convolutional Layer** : What separates a convolutional layer from a fully-connected one is that each neuron is only connected to a small, local subset of the neurons in the previous layer, which is a square sized region across the height and width dimensions.\n\n    ![](https://i.pinimg.com/originals/83/13/cc/8313cc8171a0cd308fa0e9112aad2034.png)\n    \n    * Neurons of the **convolution operator** can recognize certain **local patterns of the previous layer’s output**.\n    * Since the patterns that are recognized should be independent of their position in the image, all neurons will be forced to recognize the same pattern by making all of them share one single set of parameters. This concept is referred to as **Parameter Sharing**.\n    * In order to now reconize multiple different features within one layer, it is required to have **several Filters**. \n    * **Hyperparameters of Convolution layer**.\n        * The size of the square is a hyperparameter named Receptive Field.\n        * How many convolutions are being conducted is defined by another hyperparameter, called Stride, which determines how big the gap between two scanned regions is.\n        * Padding : Padding is applied in some implementations to make the convolution result have a certain width and height, e.g. making the output have the same size as the input. Also it helps to cover the edges of images much better.\n        \n* **Pooling Layer**: The third kind of layer, which has the purpose of decreasing the complexity of CNNs, is the Pooling Layer. Similarly to the convolutional layer, neurons in the pooling layer are connected to a square sized region across the width and height dimensions of the previous layer. The main difference between convolution and pooling is that a pooling layer is not parametrized. Ex: Max Pooling, Average Pooling.\n\n![](https://i.pinimg.com/originals/70/fb/35/70fb355fce2fa9b9d276c719053bc0f3.png)\n\nSo now we have defined the details of the CNN network. Let's see how we solve image classification problem.\n\nIn any image classification problem we have a given image and we have to classify it any one of the classes. So the CNNs are basically stacked Convolutional + Pooling layers stack with Fully connected layers in the last layers.\n\n![](https://i.pinimg.com/originals/58/01/84/58018433ba0a666747099c4aff276426.png)"},{"metadata":{},"cell_type":"markdown","source":"Now, we have a little overview of what's going on now let's learn by implementing a CNN for a **binary classification problem**.\n\n> In this [competition](https://www.kaggle.com/c/aerial-cactus-identification), we have to create an algorithm that can identify a specific type of cactus in aerial imagery.\n"},{"metadata":{},"cell_type":"markdown","source":"**Load Libraries**"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom PIL import Image\nimport random\n\nimport seaborn as sns # for making plots with seaborn\ncolor = sns.color_palette()\n\nimport matplotlib.pyplot as plt\n\n# Networks\nfrom keras.preprocessing import image\nfrom keras.applications.resnet50 import ResNet50\nfrom keras.applications.vgg16 import VGG16\nfrom keras.applications.vgg19 import VGG19\nfrom keras.applications.inception_v3 import InceptionV3\nfrom keras.applications.xception import Xception\nfrom keras.applications.inception_resnet_v2 import InceptionResNetV2\nfrom keras.applications.mobilenet import MobileNet\nfrom keras.applications.densenet import DenseNet121, DenseNet169, DenseNet201\nfrom keras.applications.nasnet import NASNetLarge, NASNetMobile\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import GlobalAveragePooling2D\n\n# Layers\nfrom keras.layers import Dense, Activation, Flatten, Dropout\nfrom keras import backend as K\n\n# Other\nfrom keras import optimizers\nfrom keras import losses\nfrom keras.optimizers import SGD, Adam\nfrom keras.models import Sequential, Model\nfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler,EarlyStopping\nfrom keras.models import load_model\n\n# Utils\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport argparse\nimport random, glob\nimport os, sys, csv\nimport cv2\nimport time, datetime\nfrom sklearn.utils import class_weight\n\n# Files\nimport utils\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Set random seed for reprodue results\ndef seed_everything(seed=1234):\n    random.seed(seed)\n    np.random.seed(seed)\n    \nseed_everything()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Load Training Data**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/aerial-cactus-identification/train.csv\")\nprint(f'The train dataset have {train.shape[0]} rows and {train.shape[1]} columns')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['has_cactus']=train['has_cactus'].astype(str)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Counts of target value**"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"sns.countplot(train['has_cactus'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Note**: It's an imbalanced dataset. Later we will also learn how to handle imbalance in dataset.\n\nIt's always a good idea to keep all your hyperparameters and variables at the sampe place. It always come handy."},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_validation split\ntraining_data_percent = 0.85\nlen_df=len(train.id)\nTRAINING_SAMPLE=int(len_df*training_data_percent)\nVALIDATION_SAMPLE = int(len_df-TRAINING_SAMPLE)\n\nprint(f'The no. of training samples are {TRAINING_SAMPLE} and we are taking {training_data_percent * 100}% data as training\\n Validation samples: {VALIDATION_SAMPLE} ')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# FILE_PATH\nTRAIN_DIR = \"../input/aerial-cactus-identification/train/train/\"\nTEST_DIR = \"../input/aerial-cactus-identification/test/test\"\nWEIGHT_FILE = '../input/keras-pretrained-models/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n\n# GENERAL\nDROPOUT_RATE      = 0.4\nNB_CLASSES        = 2\n\n# LEARNING\nBATCH_SIZE        = 128\nNB_EPOCHS_BOTTLENECK = 5\nNB_EPOCHS_FINETUNING = 5\n\n# Global settings\nmodel = \"VGG16\"\nWIDTH, HEIGHT = 331,331\nFC_LAYERS = [1024]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Handle imbalance of the dataset by calculating class weight for each class and pass it while calling **model.fit**.\nClass with less number of samples are assighned higher weights."},{"metadata":{"trusted":true},"cell_type":"code","source":"class_weights = class_weight.compute_class_weight('balanced',\n                                                 np.unique(train['has_cactus']),\n                                                 train['has_cactus'])\n\nprint(f'The class weight of class0 i.e No cactus is {class_weights[0]} and \\nClass weight of class1 i.e Has cactus is {class_weights[1]}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Plot a batch of training data**"},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"# This function prepares a random batch from the dataset\ndef load_batch(dataset_df, batch_size = 25):\n    batch_df = dataset_df.loc[np.random.permutation(np.arange(0,\n                                                              len(dataset_df)))[:batch_size],:]\n    return batch_df\n\n# This function plots sample images in specified size and in defined grid\ndef plot_batch(images_df, grid_width, grid_height, im_scale_x, im_scale_y):\n    f, ax = plt.subplots(grid_width, grid_height)\n    f.set_size_inches(12, 12)\n\n    img_idx = 0\n    for i in range(0, grid_width):\n        for j in range(0, grid_height):\n            ax[i][j].axis('off')\n            ax[i][j].set_title(images_df.iloc[img_idx]['has_cactus'])\n            ax[i][j].imshow(Image.open(TRAIN_DIR + images_df.iloc[img_idx]['id']).resize\n                                             ((im_scale_x,im_scale_y)))\n            img_idx += 1\n            \n    plt.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=0, hspace=0.25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_df = load_batch(train, \n                    batch_size=36)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_batch(batch_df, grid_width=6, grid_height=6\n           ,im_scale_x=64, im_scale_y=64)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing and Augmenting the Datasets\n\nData preparation is almost always required when working with any data analysis, machine learning, neural network or deep learning models. It becomes even more important to augment data in the case of image recognition. \n\nKeras provides the [ImageDataGenerator](https://keras.io/preprocessing/image/) class that defines the configuration for image data preparation and augmentation. It defines the arguments of the ImageDataGenerator class as follows:\n\n**rotation_range** is a value in degrees (0-180), a range within which to randomly rotate pictures\n\n**width_shift and height_shift** are ranges (as a fraction of total width or height) within which to randomly translate pictures vertically or horizontally\n\n**rescale** is a value by which the data is multiplied before any other processing. The original images consist of RGB coefficients in the 0-255, but such values would be too high for the models to process (given a typical learning rate), values are targeted between 0 and 1 instead by scaling with a 1/255. factor.\n\n**shear_range** is for randomly applying shearing transformations\n\n**zoom_range** is for randomly zooming inside pictures\nhorizontal_flip is for randomly flipping half of the images horizontally, relevant when there are no assumptions of horizontal assymetric.\n\n**Iterating through the dataset**\n\nWhen training deep neural networks, we sometimes must load and preprocess the data while simultaneously training as the whole size of the dataset exceeds available RAM size, we can’t load it in advance and also to make models robust to things like translations, rotations we introduce randomness into the training process.\n\nDepending on the data at hand we have to use keras data generators or sometimes write our own custom image datagenerator.\n\nHere we will use keras data generator called **[flow_from_dataframe](https://keras.io/preprocessing/image/)**.\n\nTypical input image sizes are `224×224, 227×227, 256×256, and 299×299. VGG16, VGG19` accept `224×224` input images while Inception V3 and Xception require `299×299` pixel inputs."},{"metadata":{"trusted":true},"cell_type":"code","source":"datagen = ImageDataGenerator(\n        rotation_range=40,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        rescale=1./255,\n        shear_range=0.2,\n        zoom_range=0.2,\n        horizontal_flip=True,\n        fill_mode='nearest')\n\nval_datagen = ImageDataGenerator(rescale=1./255)\n\ntrain_generator=datagen.flow_from_dataframe(dataframe=train[:TRAINING_SAMPLE],directory=TRAIN_DIR,x_col='id',\n                                            y_col='has_cactus',class_mode='binary',batch_size=BATCH_SIZE,\n                                            target_size=(WIDTH,HEIGHT))\n\n\nvalidation_generator= val_datagen.flow_from_dataframe(dataframe=train[TRAINING_SAMPLE-1:len_df],directory=TRAIN_DIR,x_col='id',\n                                                y_col='has_cactus',class_mode='binary',batch_size=BATCH_SIZE,\n                                                target_size=(WIDTH,HEIGHT))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Transfer Learning\n\n**Transfer learning** allows us to deal with this scenario by leveraging the data of some related task or domain, known as the source task and source domain. We store the knowledge gained in solving the source task in the source domain and apply it to the target task and target domain.\n\n![](https://i.pinimg.com/originals/f8/cd/57/f8cd5798972afd87c469b2372ef70395.png)\n\n**VGGNET**\n\nVGGNet, scored second place in the ILSVRC and influenced the deep learning scene in an important way, as they showed that using a deeper architecture does generally lead to better results, which was not obvious at that time.\n\n![](https://raw.githubusercontent.com/theainerd/MLInterview/master/Deep%20Learning/images/Screenshot%20from%202018-09-30%2009-45-48.png)\n\n> Check refrences for more details on tranfer learning and more pretrained models\n"},{"metadata":{},"cell_type":"markdown","source":"**Create base model**"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# Prepare the model\nif model == \"VGG16\":\n    from keras.applications.vgg16 import preprocess_input\n    preprocessing_function = preprocess_input\n    base_model = VGG16(weights = WEIGHT_FILE, include_top=False, input_shape=(HEIGHT, WIDTH, 3))\nelif model == \"VGG19\":\n    from keras.applications.vgg19 import preprocess_input\n    preprocessing_function = preprocess_input\n    base_model = VGG19(weights = WEIGHT_FILE, include_top=False, input_shape=(HEIGHT, WIDTH, 3))\nelif model == \"ResNet50\":\n    from keras.applications.resnet50 import preprocess_input\n    preprocessing_function = preprocess_input\n    base_model = ResNet50(weights = WEIGHT_FILE, include_top=False, input_shape=(HEIGHT, WIDTH, 3))\nelif model == \"InceptionV3\":\n    from keras.applications.inception_v3 import preprocess_input\n    preprocessing_function = preprocess_input\n    base_model = InceptionV3(weights = WEIGHT_FILE, include_top=False, input_shape=(HEIGHT, WIDTH, 3))\nelif model == \"Xception\":\n    from keras.applications.xception import preprocess_input\n    preprocessing_function = preprocess_input\n    base_model = Xception(weights = WEIGHT_FILE, include_top=False, input_shape=(HEIGHT, WIDTH, 3))\nelif model == \"InceptionResNetV2\":\n    from keras.applications.inceptionresnetv2 import preprocess_input\n    preprocessing_function = preprocess_input\n    base_model = InceptionResNetV2(weights = WEIGHT_FILE, include_top=False, input_shape=(HEIGHT, WIDTH, 3))\nelif model == \"MobileNet\":\n    from keras.applications.mobilenet import preprocess_input\n    preprocessing_function = preprocess_input\n    base_model = MobileNet(weights = WEIGHT_FILE, include_top=False, input_shape=(HEIGHT, WIDTH, 3))\nelif model == \"DenseNet121\":\n    from keras.applications.densenet import preprocess_input\n    preprocessing_function = preprocess_input\n    base_model = DenseNet121(weights = WEIGHT_FILE, include_top=False, input_shape=(HEIGHT, WIDTH, 3))\nelif model == \"DenseNet169\":\n    from keras.applications.densenet import preprocess_input\n    preprocessing_function = preprocess_input\n    base_model = DenseNet169(weights = WEIGHT_FILE, include_top=False, input_shape=(HEIGHT, WIDTH, 3))\nelif model == \"DenseNet201\":\n    from keras.applications.densenet import preprocess_input\n    preprocessing_function = preprocess_input\n    base_model = DenseNet201(weights = WEIGHT_FILE, include_top=False, input_shape=(HEIGHT, WIDTH, 3))\nelif model == \"NASNetLarge\":\n    from keras.applications.nasnet import preprocess_input\n    preprocessing_function = preprocess_input\n    base_model = NASNetLarge(weights = WEIGHT_FILE, include_top=True, input_shape=(HEIGHT, WIDTH, 3))\nelif model == \"NASNetMobile\":\n    from keras.applications.nasnet import preprocess_input\n    preprocessing_function = preprocess_input\n    base_model = NASNetMobile(weights = WEIGHT_FILE, include_top=False, input_shape=(HEIGHT, WIDTH, 3))\nelse:\n    ValueError(\"The model you requested is not supported in Keras\")\n    \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Bottleneck Model\n\nThe top layer of the pre-trained model was removed and replaced with a new fully connected layer with a **Sigmoid classifier**. \n\n**GlobalAveragePooling2D** progressively reduces the spatial size and the amount of parameters and computation in the network as well as control overfitting. The [Dense](https://keras.io/layers/core/#dense) layer is the densely-connected Neural Network layer of size 1024 with the [Rectified Linear Unit](http://cs231n.github.io/neural-networks-1/) (relu) as the activator.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add on new FC layers with dropout for intializing the final fully connected layer\n\ndef build_bottleneck_model(base_model, dropout, fc_layers, num_classes):\n    # Freeze All Layers Except Bottleneck Layers for Fine-Tuning\n    for layer in base_model.layers:\n        layer.trainable = False\n\n    x = base_model.output\n    x = GlobalAveragePooling2D()(x)\n    \n    for fc in fc_layers:\n        x = Dense(fc, activation='relu')(x) # New FC layer, random init\n        x = Dropout(dropout)(x)\n\n    predictions = Dense(num_classes-1, activation='sigmoid')(x)\n    \n    bottleneck_model = Model(inputs=base_model.input, outputs=predictions)\n\n    return bottleneck_model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# FineTuning Model"},{"metadata":{},"cell_type":"markdown","source":"In the Fine Tuning phase some or none of the lower convolutional layers of the model were prevented from training depending upon the results from the Transfer Learning phase.\n\n**Step 1 - Set up fine tuning on pre-trained ImageNet vgg16 model**\n\nIn finetuning model, we would input the model weights learned from `bottleneck_model` using\n\n```python\nfrom keras.models import Model\nbase_model = Model.load_weights(file_path_of_the_weights_file)\n\n# Freeze Half of all Layers Except final Eight layers of VGG_NET Fine-Tuning\nfor layer in base_model.layers[:8]:\n    layer.trainable = False\nfor layer in base_model.layers[:8]:\n    layer.trainable = False\n    \n```\n**Step 2 - Compile the revised model using SGD optimizer with a learing rate of 0.0001 and a momentum of 0.9**\n\nkeep in mind that the **learning rate of optimizer low**. Rest it's all the same as bottleneck_model.\nUsually this works fine for me.\n\n```python\nfrom keras.optimizers import SGD\noptimizer=SGD(lr=0.0001, momentum=0.9)\n```\n\nFor fine tuning you have to freeze different number of layers based on the architecture you are using. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add on new FC layers with dropout for fine tuning\n\ndef build_finetuning_model(base_model, dropout, fc_layers, num_classes):\n\n    x = base_model.output\n    x = GlobalAveragePooling2D()(x)\n    \n    for fc in fc_layers:\n        x = Dense(fc, activation='relu')(x) # New FC layer, random init\n        x = Dropout(dropout)(x)\n\n    predictions = Dense(num_classes-1, activation='sigmoid')(x)\n    \n    finetuning_model = Model(inputs=base_model.input, outputs=predictions)\n\n    return finetuning_model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Building\n\n**Bottleneck Model**"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"bottleneck_model = build_bottleneck_model(base_model, DROPOUT_RATE, FC_LAYERS, NB_CLASSES)\nadam = Adam(lr=0.001)\nbottleneck_model.compile(adam, loss='binary_crossentropy', metrics=['accuracy'])\n    \n    \ndef lr_decay(epoch):\n    if epoch%3 == 0 and epoch!=0:\n        lr = K.get_value(model.optimizer.lr)\n        K.set_value(model.optimizer.lr, lr/2)\n        print(\"LR changed to {}\".format(lr/2))\n    return K.get_value(model.optimizer.lr)\n\nlearning_rate_schedule = LearningRateScheduler(lr_decay)\n\nearly_stopping = EarlyStopping(patience=2)\n\nfilepath= \"../working/\" + \"weightfile.h5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\ncallbacks_list = [checkpoint,early_stopping]\n\n\nhistory = bottleneck_model.fit_generator(train_generator, epochs=NB_EPOCHS_BOTTLENECK, workers=8, steps_per_epoch= TRAINING_SAMPLE // BATCH_SIZE, \nvalidation_data=validation_generator, validation_steps= VALIDATION_SAMPLE // BATCH_SIZE, class_weight=class_weights, shuffle=True, callbacks=callbacks_list)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cool, we achieved an accuracy of >92% and let's see fine tuning helps or not."},{"metadata":{},"cell_type":"markdown","source":"**Plot model accuracy and loss**"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Plot the training and validation loss + accuracy\ndef plot_training(history):\n    acc = history.history['acc']\n    val_acc = history.history['val_acc']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    epochs = range(len(acc))\n\n    plt.plot(epochs, acc, 'r.')\n    plt.plot(epochs, val_acc, 'r')\n    plt.title('Training and validation accuracy')\n\n    # plt.figure()\n    # plt.plot(epochs, loss, 'r.')\n    # plt.plot(epochs, val_loss, 'r-')\n    # plt.title('Training and validation loss')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_training(history)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**FineTuning Model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"weights_file_path = '../working/weightfile.h5'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"finetuning_model = build_finetuning_model(base_model, DROPOUT_RATE, FC_LAYERS, NB_CLASSES)\nfinetuning_model.load_weights(weights_file_path)\nfinetuning_model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='binary_crossentropy', metrics=['accuracy'])\n\n# Freeze Half of all Layers Except final Eight layers of VGG_NET Fine-Tuning\nfor layer in finetuning_model.layers[:8]:\n    layer.trainable = False\nfor layer in finetuning_model.layers[8:]:\n    layer.trainable = True\n    \n    \noptimizer=SGD(lr=0.001, momentum=0.9)    \ndef lr_decay(epoch):\n    if epoch%3 == 0 and epoch!=0:\n        lr = K.get_value(model.optimizer.lr)\n        K.set_value(model.optimizer.lr, lr/2)\n        print(\"LR changed to {}\".format(lr/2))\n    return K.get_value(model.optimizer.lr)\n\n# learning_rate_schedule = LearningRateScheduler(lr_decay)\n\nearly_stopping = EarlyStopping(patience=2)\n\nfilepath= \"../working/\" + \"_weightfile_finetuning.h5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\ncallbacks_list = [checkpoint,early_stopping]\n\n\nhistory_finetuning = bottleneck_model.fit_generator(train_generator, epochs=NB_EPOCHS_FINETUNING, workers=8, steps_per_epoch= TRAINING_SAMPLE // BATCH_SIZE, \nvalidation_data=validation_generator, validation_steps= VALIDATION_SAMPLE // BATCH_SIZE, class_weight=class_weights, shuffle=True, callbacks=callbacks_list)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Usually depending on the problem you will get an improvment of 2-5% of improvment from bottleneck model."},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_training(history_finetuning)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Some Key Ideas\n\n* Always run your model on small data and try to overfit it. That way you can save ton of time and experiment a lot.\n* If you are facing some problem don't give up. This guide [Troubleshooting Deep Neural Network](http://josh-tobin.com/assets/pdf/troubleshooting-deep-neural-networks-01-19.pdf) will surely come handy."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Refrences\n\n[A Non-Technical Survey on Deep Convolutional Neural Network Architectures](https://arxiv.org/pdf/1803.02129.pdf)\n\n[CS231n: Convolutional Neural Networks for Visual Recognition Spring 2018](http://cs231n.stanford.edu/2018/)\n\n[Keras Applications](https://keras.io/applications/) : Keras Applications are deep learning models that are made available alongside pre-trained weights. These models can be used for prediction, feature extraction, and fine-tuning.\n\n[Tutorial on Keras flow_from_dataframe](https://medium.com/@vijayabhaskar96/tutorial-on-keras-flow-from-dataframe-1fd4493d237c)\n\n**Some library worth exploring**\n\n[Albumentations](https://github.com/albu/albumentations): Fast image augmentation library\n\n[CBAM-keras](https://github.com/kobiso/CBAM-keras) Convolutional Block Attention Module to improve representation power by using attention mechanism: focusing on important features and supressing unnecessary ones.\n\n\n\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"<center><span style=\"color:red\">**If you liked this kernel do upvote and also share with people who could learn something new with this. Happy Learning **</span></centre> \n<img src=\"https://external-preview.redd.it/I3Z3MHAmKZ2dbH6TmHgEltNICo2xLScHVZnmprDUvc4.png?auto=webp&s=1498ffe4d22f902f2d47f05dd91ec135c9eb2061\" width=200,height=300>"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}