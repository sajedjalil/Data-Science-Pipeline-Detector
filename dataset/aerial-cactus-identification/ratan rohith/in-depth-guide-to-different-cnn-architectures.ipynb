{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Guide to different CNN Architectures using keras:\n## In this tutorial i am going to explain 5 different CNN architectures which are commonly used.\n## 1. VGG16/19\n## 2. GoogLeNet/Inception\n## 3. Xception\n## 4. Mobile net\n## 5. Resnet\n\n### Note: I am not going to train model on longer epochs my solely aim is to illustrate how different architectures works.If you want to get better accuracy and performance of the model please make deeper models and run for many epochs and tune the hyperparameters and also i commented some code blocks as they are taking longer time to run if you want to run them uncomment them."},{"metadata":{},"cell_type":"markdown","source":"# Importing libraries."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os,cv2\nfrom IPython.display import Image\nfrom keras.preprocessing import image\nfrom keras import optimizers\nimport numpy as np\nfrom keras import layers,models\nfrom keras.applications.imagenet_utils import preprocess_input\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom keras import regularizers\nfrom keras.models import Sequential, Model \nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras import applications\nprint(os.listdir(\"../input\"))\n\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Importing data "},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dir=\"../input/train/train\"\ntest_dir=\"../input/test/test\"\ntrain=pd.read_csv('../input/train.csv')\ntrain.has_cactus=train.has_cactus.astype(str)\ndf_test=pd.read_csv('../input/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10,6))\nsns.countplot(x = 'has_cactus',data = train)\nplt.xticks(rotation='vertical')\nplt.xlabel('Has cactus or not', fontsize=12)\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The number of images in test set is %d\"%(len(os.listdir('../input/test/test'))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sample image"},{"metadata":{"trusted":true},"cell_type":"code","source":"Image(os.path.join(\"../input/train/train\",train.iloc[0,0]),width=250,height=250)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"datagen=ImageDataGenerator(rescale=1./255)\nbatch_size=150","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_generator=datagen.flow_from_dataframe(dataframe=train[:15001],directory=train_dir,x_col='id',\n                                            y_col='has_cactus',class_mode='binary',batch_size=batch_size,\n                                            target_size=(150,150))\n\n\nvalidation_generator=datagen.flow_from_dataframe(dataframe=train[15000:],directory=train_dir,x_col='id',\n                                                y_col='has_cactus',class_mode='binary',batch_size=50,\n                                                target_size=(150,150))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Introduction to CNN's\nIn neural networks, Convolutional neural network (ConvNets or CNNs) is one of the main categories to do images recognition, images classifications. Objects detections, recognition faces etc., are some of the areas where CNNs are widely used.\nCNN image classifications takes an input image, process it and classify it under certain categories (Eg., Dog, Cat, Tiger, Lion). Computers sees an input image as array of pixels and it depends on the image resolution. Based on the image resolution, it will see h x w x d( h = Height, w = Width, d = Dimension ). Eg., An image of 6 x 6 x 3 array of matrix of RGB (3 refers to RGB values) and an image of 4 x 4 x 1 array of matrix of grayscale image. The below image shows the array of RGB matrix\n![Array of RGB Matrix](https://miro.medium.com/max/289/1*CBY94wikMUCZMB4-Xxs-pw.png)\nTechnically, deep learning CNN models to train and test, each input image will pass it through a series of convolution layers with filters (Kernals), Pooling, fully connected layers (FC) and apply Softmax function to classify an object with probabilistic values between 0 and 1. The below figure is a complete flow of CNN to process an input image and classifies the objects based on values.\n![](https://miro.medium.com/max/875/1*XbuW8WuRrAY5pC4t-9DZAQ.jpeg)\n## Covolution layer:\nConvolution is the first layer to extract features from an input image. Convolution preserves the relationship between pixels by learning image features using small squares of input data. It is a mathematical operation that takes two inputs such as image matrix and a filter or kernal\n![](https://miro.medium.com/max/576/1*kYSsNpy0b3fIonQya66VSQ.png)\nConsider a 5 x 5 whose image pixel values are 0, 1 and filter matrix 3 x 3 and Then the convolution of 5 x 5 image matrix multiplies with 3 x 3 filter matrix which is called “Feature Map” as output shown in below\n![](https://miro.medium.com/max/335/1*MrGSULUtkXc0Ou07QouV8A.gif)\nConvolution of an image with different filters can perform operations such as edge detection, blur and sharpen by applying filters.\n## Strides:\nStride is the number of pixels shifts over the input matrix. When the stride is 1 then we move the filters to 1 pixel at a time. When the stride is 2 then we move the filters to 2 pixels at a time and so on. The below figure shows convolution would work with a stride of 2.\n![](https://miro.medium.com/max/869/1*nGHLq1hx0gt02OK4l8WmRg.png)\n## Padding:\nSometimes filter does not perfectly fit the input image. We have two options:\n1. Pad the picture with zeros (zero-padding) so that it fits\n2. Drop the part of the image where the filter did not fit. This is called valid padding which keeps only valid part of the image.\n## Non Linearity:\nReLU stands for Rectified Linear Unit for a non-linear operation. The output is ƒ(x) = max(0,x).\n**Why ReLU is important** : ReLU’s purpose is to introduce non-linearity in our ConvNet. Since, the real world data would want our ConvNet to learn would be non-negative linear values.\n![](https://miro.medium.com/max/561/1*gcvuKm3nUePXwUOLXfLIMQ.png)\nThere are other non linear functions such as tanh or sigmoid can also be used instead of ReLU. Most of the data scientists uses ReLU since performance wise ReLU is better than other two.\n## Pooling Layer:\nPooling layers section would reduce the number of parameters when the images are too large. Spatial pooling also called subsampling or downsampling which reduces the dimensionality of each map but retains the important information. Spatial pooling can be of different types:\n* Max Pooling\n* Average Pooling\n* Sum Pooling\nMax pooling take the largest element from the rectified feature map. Taking the largest element could also take the average pooling. Sum of all elements in the feature map call as sum pooling.\n![](https://miro.medium.com/max/753/1*SmiydxM5lbTjoKWYPiuzWQ.png)\n## Fully Connected Layer:\nThe layer we call as FC layer, we flattened our matrix into vector and feed it into a fully connected layer like neural network.\n![](https://miro.medium.com/max/693/1*Mw6LKUG8AWQhG73H1caT8w.png)\nIn the above diagram, feature map matrix will be converted as vector (x1, x2, x3, …). With the fully connected layers, we combined these features together to create a model. Finally, we have an activation function such as softmax or sigmoid to classify the outputs as cat, dog, car, truck or what ever class you want etc.,\n## Complete Cnn Architecture:\n![](https://miro.medium.com/max/875/1*4GLv7_4BbKXnpc6BRb0Aew.png)"},{"metadata":{},"cell_type":"markdown","source":"# Building CNN model:"},{"metadata":{},"cell_type":"markdown","source":"We will build our model such that it contains 5 Conv2D + Maxpooling2D stages with relu activation function."},{"metadata":{},"cell_type":"markdown","source":"#### Credits and references : This basic cnn model is taken from this [kernel](https://www.kaggle.com/shahules/getting-started-with-cnn-and-vgg16)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Reference : https://www.kaggle.com/shahules/getting-started-with-cnn-and-vgg16\nmodel=models.Sequential()\nmodel.add(layers.Conv2D(32,(3,3),activation='relu',input_shape=(150,150,3)))\nmodel.add(layers.MaxPool2D((2,2)))\nmodel.add(layers.Conv2D(64,(3,3),activation='relu',input_shape=(150,150,3)))\nmodel.add(layers.MaxPool2D((2,2)))\nmodel.add(layers.Conv2D(128,(3,3),activation='relu',input_shape=(150,150,3)))\nmodel.add(layers.MaxPool2D((2,2)))\nmodel.add(layers.Conv2D(128,(3,3),activation='relu',input_shape=(150,150,3)))\nmodel.add(layers.MaxPool2D((2,2)))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(512,activation='relu'))\nmodel.add(layers.Dense(1,activation='sigmoid'))\n         ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Getting the summary of the model:"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Compiling our model:\n* Loss : Here we are using binary cross entropy as our loss function.The below is the formula for binary cross entropy.\n   **−∑iyilogy^i−∑i(1−yi)log(1−y^i)**.\n* Optimizer : We are using Adam as a optimiser.\n* metric : we are using accuracy as metric."},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss='binary_crossentropy',optimizer=optimizers.adam(),metrics=['acc'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fitting the model:\n## we are going to fit the model for 5 epochs."},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs=5\nhistory=model.fit_generator(train_generator,steps_per_epoch=100,epochs=5,validation_data=validation_generator,validation_steps=50)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluating our model:\n"},{"metadata":{},"cell_type":"markdown","source":"#### Number of epochs vs training and validation accuracy"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"acc=history.history['acc']  ##getting  accuracy of each epochs\nepochs_=range(0,epochs)    \nplt.plot(epochs_,acc,label='training accuracy')\nplt.xlabel('no of epochs')\nplt.ylabel('accuracy')\n\nacc_val=history.history['val_acc']  ##getting validation accuracy of each epochs\nplt.scatter(epochs_,acc_val,label=\"validation accuracy\")\nplt.title(\"no of epochs vs accuracy\")\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Number of epochs vs training and validation loss."},{"metadata":{"trusted":true},"cell_type":"code","source":"acc=history.history['loss']    ##getting  loss of each epochs\nepochs_=range(0,epochs)\nplt.plot(epochs_,acc,label='training loss')\nplt.xlabel('No of epochs')\nplt.ylabel('loss')\n\nacc_val=history.history['val_loss']  ## getting validation loss of each epochs\nplt.scatter(epochs_,acc_val,label=\"validation loss\")\nplt.title('no of epochs vs loss')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Getting started with brief introductions to diferent architectures:"},{"metadata":{},"cell_type":"markdown","source":"# VGG16:\nVGG16 is a convolutional neural network model proposed by K. Simonyan and A. Zisserman from the University of Oxford in the paper “Very Deep Convolutional Networks for Large-Scale Image Recognition”. The model achieves 92.7% top-5 test accuracy in ImageNet, which is a dataset of over 14 million images belonging to 1000 classes. It was one of the famous model submitted to ILSVRC-2014. It makes the improvement over AlexNet by replacing large kernel-sized filters (11 and 5 in the first and second convolutional layer, respectively) with multiple 3×3 kernel-sized filters one after another. VGG16 was trained for weeks and was using NVIDIA Titan Black GPU’s.\n## The Architecture:\n![](https://neurohive.io/wp-content/uploads/2018/11/vgg16-neural-network.jpg)\nThe input to cov1 layer is of fixed size 224 x 224 RGB image. The image is passed through a stack of convolutional (conv.) layers, where the filters were used with a very small receptive field: 3×3 (which is the smallest size to capture the notion of left/right, up/down, center). In one of the configurations, it also utilizes 1×1 convolution filters, which can be seen as a linear transformation of the input channels (followed by non-linearity). The convolution stride is fixed to 1 pixel; the spatial padding of conv. layer input is such that the spatial resolution is preserved after convolution, i.e. the padding is 1-pixel for 3×3 conv. layers. Spatial pooling is carried out by five max-pooling layers, which follow some of the conv.  layers (not all the conv. layers are followed by max-pooling). Max-pooling is performed over a 2×2 pixel window, with stride 2.\n\nThree Fully-Connected (FC) layers follow a stack of convolutional layers (which has a different depth in different architectures): the first two have 4096 channels each, the third performs 1000-way ILSVRC classification and thus contains 1000 channels (one for each class). The final layer is the soft-max layer. The configuration of the fully connected layers is the same in all networks.\n\nAll hidden layers are equipped with the rectification (ReLU) non-linearity. It is also noted that none of the networks (except for one) contain Local Response Normalisation (LRN), such normalization does not improve the performance on the ILSVRC dataset, but leads to increased memory consumption and computation time.\n## Configurations:\nAll configurations follow the generic design present in architecture and differ only in the depth: from 11 weight layers in the network A (8 conv. and 3 FC layers) to 19 weight layers in the network E (16 conv. and 3 FC layers). The width of conv. layers (the number of channels) is rather small, starting from 64 in the first layer and then increasing by a factor of 2 after each max-pooling layer, until it reaches 512.\n![](https://neurohive.io/wp-content/uploads/2018/11/Capture-564x570.jpg)\n## Drawbacks:\n* It is painfully slow to train.\n* The network architecture weights themselves are quite large.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = applications.VGG16(weights = \"imagenet\", include_top=False, input_shape = (150, 150, 3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D\nfor layer in model.layers[:5]:\n    layer.trainable = False\n\n#Adding custom Layers \nx = model.output\nx = Flatten()(x)\nx = Dense(1024, activation=\"relu\")(x)\nx = Dropout(0.5)(x)\nx = Dense(1024, activation=\"relu\")(x)\npredictions = Dense(1, activation=\"sigmoid\")(x)\n\n# creating the final model \nmodel_final =  Model(inputs=model.input, outputs=predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Note : As the kernels ram was limited i am not going to write advanced architectures i am trying to explain as simple as possible with small code snippet.You can develop on this code :)"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_final.compile(loss='binary_crossentropy',optimizer=optimizers.adam(),metrics=['acc'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history=model_final.fit_generator(train_generator,steps_per_epoch=100,epochs=5,validation_data=validation_generator,validation_steps=50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Number of epochs vs training and validation accuracy."},{"metadata":{"trusted":true},"cell_type":"code","source":"acc=history.history['acc']  ##getting  accuracy of each epochs\nepochs_=range(0,epochs)    \nplt.plot(epochs_,acc,label='training accuracy')\nplt.xlabel('no of epochs')\nplt.ylabel('accuracy')\n\nacc_val=history.history['val_acc']  ##getting validation accuracy of each epochs\nplt.scatter(epochs_,acc_val,label=\"validation accuracy\")\nplt.title(\"no of epochs vs accuracy\")\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Number of epochs vs training and validation loss."},{"metadata":{"trusted":true},"cell_type":"code","source":"acc=history.history['loss']    ##getting  loss of each epochs\nepochs_=range(0,epochs)\nplt.plot(epochs_,acc,label='training loss')\nplt.xlabel('No of epochs')\nplt.ylabel('loss')\n\nacc_val=history.history['val_loss']  ## getting validation loss of each epochs\nplt.scatter(epochs_,acc_val,label=\"validation loss\")\nplt.title('no of epochs vs loss')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# GoogLeNet/Inception:\nThe Inception network on the other hand, was complex (heavily engineered). It used a lot of tricks to push performance; both in terms of speed and accuracy. Its constant evolution lead to the creation of several versions of the network. The popular versions are as follows:\n1. Inception v1.\n2. Inception v2 and Inception v3.\n3. Inception v4 and Inception-ResNet.\nEach version is an iterative improvement over the previous one. Understanding the upgrades can help us to build custom classifiers that are optimized both in speed and accuracy. Also, depending on your data, a lower version may actually work better.\n#### Causes for the evolution of inception:\nSalient parts in the image can have extremely large variation in size. For instance, an image with a dog can be either of the following, as shown below. The area occupied by the dog is different in each image.\n![](https://cdn-images-1.medium.com/max/1000/1*aBdPBGAeta-_AM4aEyqeTQ.jpeg)\nBecause of this huge variation in the location of the information, choosing the right kernel size for the convolution operation becomes tough. A larger kernel is preferred for information that is distributed more globally, and a smaller kernel is preferred for information that is distributed more locally.\nVery deep networks are prone to overfitting. It also hard to pass gradient updates through the entire network.\nNaively stacking large convolution operations is computationally expensive.\n\n#### The solution:\nWhy not have filters with multiple sizes operate on the same level? The network essentially would get a bit “wider” rather than “deeper”. The authors designed the inception module to reflect the same.\nThe below image is the “naive” inception module. It performs convolution on an input, with 3 different sizes of filters (1x1, 3x3, 5x5). Additionally, max pooling is also performed. The outputs are concatenated and sent to the next inception module.\n![](https://cdn-images-1.medium.com/max/1000/1*DKjGRDd_lJeUfVlY50ojOA.png)\nAs stated before, deep neural networks are computationally expensive. To make it cheaper, the authors limit the number of input channels by adding an extra 1x1 convolution before the 3x3 and 5x5 convolutions. Though adding an extra operation may seem counterintuitive, 1x1 convolutions are far more cheaper than 5x5 convolutions, and the reduced number of input channels also help. Do note that however, the 1x1 convolution is introduced after the max pooling layer, rather than before.\n![](https://cdn-images-1.medium.com/max/1000/1*U_McJnp7Fnif-lw9iIC5Bw.png)\nUsing the dimension reduced inception module, a neural network architecture was built. This was popularly known as GoogLeNet (Inception v1). The architecture is shown below:\n![](https://cdn-images-1.medium.com/max/1000/1*uW81y16b-ptBDV8SIT1beQ.png)\nThis is the basic idea behind the evolution of inception net if you want to learn about other versions go to [here](https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202)"},{"metadata":{"trusted":true},"cell_type":"code","source":"inception_model = applications.inception_v3.InceptionV3(weights = \"imagenet\", include_top=False, input_shape = (150, 150, 3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inception_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for layer in inception_model.layers[:5]:\n    layer.trainable = False\n\n#Adding custom Layers \nx = inception_model.output\nx = Flatten()(x)\nx = Dense(512, activation=\"relu\")(x)\nx = Dropout(0.5)(x)\nx = Dense(1024, activation=\"relu\")(x)\nx = Dropout(0.5)(x)\npredictions = Dense(1, activation=\"sigmoid\")(x)\n\n# creating the final model \nmodel_final =  Model(inputs=inception_model.input, outputs=predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_final.compile(loss='binary_crossentropy',optimizer=optimizers.adam(),metrics=['acc'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history=model_final.fit_generator(train_generator,steps_per_epoch=100,epochs=5,validation_data=validation_generator,validation_steps=50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Epochs vs training and validation accuracy"},{"metadata":{"trusted":true},"cell_type":"code","source":"acc=history.history['acc']  ##getting  accuracy of each epochs\nepochs_=range(0,epochs)    \nplt.plot(epochs_,acc,label='training accuracy')\nplt.xlabel('no of epochs')\nplt.ylabel('accuracy')\n\nacc_val=history.history['val_acc']  ##getting validation accuracy of each epochs\nplt.scatter(epochs_,acc_val,label=\"validation accuracy\")\nplt.title(\"no of epochs vs accuracy\")\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Epochs vs training and validation losses"},{"metadata":{"trusted":true},"cell_type":"code","source":"acc=history.history['loss']    ##getting  loss of each epochs\nepochs_=range(0,epochs)\nplt.plot(epochs_,acc,label='training loss')\nplt.xlabel('No of epochs')\nplt.ylabel('loss')\n\nacc_val=history.history['val_loss']  ## getting validation loss of each epochs\nplt.scatter(epochs_,acc_val,label=\"validation loss\")\nplt.title('no of epochs vs loss')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Xception Net:\nIf we have to understand xception network we need to understand depthwise convolutions.These type of CNN’s are widely used because of the following two reasons –\n\n* They have lesser number of parameters to adjust as compared to the standard CNN’s, which reduces overfitting.\n* They are computationally cheaper because of fewer computations which makes them suitable for mobile vision applications.\nThis depth wise convolution process is broken down into 2 operations –\n1. Depth-wise convolutions\n2. Point-wise convolutions\n## DEPTH WISE CONVOLUTION:\nIn depth-wise operation, convolution is applied to a single channel at a time unlike standard CNN’s in which it is done for all the M channels. So here the filters/kernels will be of size Dk x Dk x 1. Given there are M channels in the input data, then M such filters are required. Output will be of size Dp x Dp x M.\n![](https://cdn-images-1.medium.com/max/1000/1*yG6z6ESzsRW-9q5F_neOsg.png)\nEach 5x5x1 kernel iterates 1 channel of the image (note: 1 channel, not all channels), getting the scalar products of every 25 pixel group, giving out a 8x8x1 image. Stacking these images together creates a 8x8x3 image.\n## POINT WISE CONVOLUTION:\nThe pointwise convolution is so named because it uses a 1x1 kernel, or a kernel that iterates through every single point. This kernel has a depth of however many channels the input image has; in our case, 3. Therefore, we iterate a 1x1x3 kernel through our 8x8x3 image, to get a 8x8x1 image.\n![](https://cdn-images-1.medium.com/max/1000/1*37sVdBZZ9VK50pcAklh8AQ.png)\nWe can create 256 1x1x3 kernels that output a 8x8x1 image each to get a final image of shape 8x8x256.\n![](https://cdn-images-1.medium.com/max/1000/1*Q7a20gyuunpJzXGnWayUDQ.png)\n## Complete architecture of depth wise separable convolution:\n![](https://cdn-images-1.medium.com/max/1000/1*VvBTMkVRus6bWOqrK1SlLQ.png)\n## Modified Depthwise Separable Convolution in Xception:\nThe google researchers modified depthwise separable convolution with pointwise convolution followed by depthwise convolution.below is the image. \n![](https://cdn-images-1.medium.com/max/1000/1*J8dborzVBRBupJfvR7YhuA.png)\n## Overall architecture of xception:\n![](https://cdn-images-1.medium.com/max/1000/1*hOcAEj9QzqgBXcwUzmEvSg.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# uncomment this to run\n#xception_model = applications.xception.Xception(weights = \"imagenet\", include_top=False, input_shape = (150, 150, 3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#uncomment this to run\n#xception_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#uncomment this to run\n'''for layer in xception_model.layers[:5]:\n    layer.trainable = False\n\n#Adding custom Layers \nx = xception_model.output\nx = Flatten()(x)\nx = Dense(1024,activation=\"relu\")(x)\nx = Dropout(0.5)(x)\nx = Dense(1024, activation=\"relu\")(x)\nx = Dropout(0.5)(x)\npredictions = Dense(1, activation=\"sigmoid\")(x)\n\n# creating the final model \nmodel_final =  Model(inputs=xception_model.input, outputs=predictions)'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model_final.compile(loss='binary_crossentropy',optimizer=optimizers.adam(),metrics=['acc'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Uncomment this to run the model\n#history=model_final.fit_generator(train_generator,steps_per_epoch=200,epochs=1,validation_data=validation_generator,validation_steps=150)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Mobile net:\nMobileNet is an architecture which is more suitable for mobile and embedded based vision applications where there is lack of compute power. This architecture was proposed by Google.Similarly like xception networks the mobile net uses depthwise separable covolutions.\n## Architecture:\n![](https://www.researchgate.net/publication/327134257/figure/fig2/AS:661949444530178@1534832454962/Original-architecture-of-MobileNet-as-shown-in-the-MobileNet-paper-The-architecture-is.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"mobile_model = applications.mobilenet.MobileNet(weights = \"imagenet\", include_top=False, input_shape = (150, 150, 3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for layer in mobile_model.layers[:5]:\n    layer.trainable = False\n\n#Adding custom Layers \nx = mobile_model.output\nx = Flatten()(x)\nx = Dense(512, activation=\"relu\")(x)\nx = Dropout(0.5)(x)\nx = Dense(1024, activation=\"relu\")(x)\nx = Dropout(0.5)(x)\npredictions = Dense(1, activation=\"sigmoid\")(x)\n\n# creating the final model \nmodel_final =  Model(inputs=mobile_model.input, outputs=predictions)\nmodel_final.compile(loss='binary_crossentropy',optimizer=optimizers.adam(),metrics=['acc'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history=model_final.fit_generator(train_generator,steps_per_epoch=100,epochs=5,validation_data=validation_generator,validation_steps=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc=history.history['acc']  ##getting  accuracy of each epochs\nepochs_=range(0,epochs)    \nplt.plot(epochs_,acc,label='training accuracy')\nplt.xlabel('no of epochs')\nplt.ylabel('accuracy')\n\nacc_val=history.history['val_acc']  ##getting validation accuracy of each epochs\nplt.scatter(epochs_,acc_val,label=\"validation accuracy\")\nplt.title(\"no of epochs vs accuracy\")\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc=history.history['loss']    ##getting  loss of each epochs\nepochs_=range(0,epochs)\nplt.plot(epochs_,acc,label='training loss')\nplt.xlabel('No of epochs')\nplt.ylabel('loss')\n\nacc_val=history.history['val_loss']  ## getting validation loss of each epochs\nplt.scatter(epochs_,acc_val,label=\"validation loss\")\nplt.title('no of epochs vs loss')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Resnet:\nBy increasing network depth does not work by simply stacking layers together. Deep networks are hard to train because of the notorious vanishing gradient problem — as the gradient is back-propagated to earlier layers, repeated multiplication may make the gradient infinitively small. As a result, as the network goes deeper, its performance gets saturated or even starts degrading rapidly.Before ResNet, there had been several ways to deal the vanishing gradient issue, for instance,adds an auxiliary loss in a middle layer as extra supervision, but none seemed to really tackle the problem once and for all.\nIn simple words, a residual module has two options, either it can perform a set of functions on the input, or it can skip this step altogether.\n\nThe core idea of ResNet is introducing a so-called “identity shortcut connection” that skips one or more layers, as shown in the following figure:\n![](https://cdn-images-1.medium.com/max/750/1*ByrVJspW-TefwlH7OLxNkg.png)\n## The architecture:\n![](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/08/08131926/temp12.png)"},{"metadata":{},"cell_type":"markdown","source":"## lets see resnet50"},{"metadata":{"trusted":true},"cell_type":"code","source":"resnet_model = applications.resnet50.ResNet50(weights = \"imagenet\", include_top=False, input_shape = (150, 150, 3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for layer in resnet_model.layers[:5]:\n    layer.trainable = False\n\n#Adding custom Layers \nx = resnet_model.output\nx = Flatten()(x)\nx = Dense(512, activation=\"relu\")(x)\nx = Dropout(0.5)(x)\nx = Dense(1024, activation=\"relu\")(x)\nx = Dropout(0.5)(x)\npredictions = Dense(1, activation=\"sigmoid\")(x)\n\n# creating the final model \nmodel_final =  Model(inputs=resnet_model.input, outputs=predictions)\nmodel_final.compile(loss='binary_crossentropy',optimizer=optimizers.adam(),metrics=['acc'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history=model_final.fit_generator(train_generator,steps_per_epoch=100,epochs=5,validation_data=validation_generator,validation_steps=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc=history.history['acc']  ##getting  accuracy of each epochs\nepochs_=range(0,epochs)    \nplt.plot(epochs_,acc,label='training accuracy')\nplt.xlabel('no of epochs')\nplt.ylabel('accuracy')\n\nacc_val=history.history['val_acc']  ##getting validation accuracy of each epochs\nplt.scatter(epochs_,acc_val,label=\"validation accuracy\")\nplt.title(\"no of epochs vs accuracy\")\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc=history.history['loss']    ##getting  loss of each epochs\nepochs_=range(0,epochs)\nplt.plot(epochs_,acc,label='training loss')\nplt.xlabel('No of epochs')\nplt.ylabel('loss')\n\nacc_val=history.history['val_loss']  ## getting validation loss of each epochs\nplt.scatter(epochs_,acc_val,label=\"validation loss\")\nplt.title('no of epochs vs loss')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## References and credits:\n1. For the Over view of resnets see this [blog](https://towardsdatascience.com/an-overview-of-resnet-and-its-variants-5281e2f56035) <br>\n2. Understanding depthwise separable convolutions and xception see this [post](https://towardsdatascience.com/review-xception-with-depthwise-separable-convolution-better-than-inception-v3-image-dc967dd42568)\n3. Understanding inception [networks](https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202)\n4. code references and snippets from [keras](https://keras.io/applications/#mobilenet)\n5. Different [architectures](https://www.analyticsvidhya.com/blog/2017/08/10-advanced-deep-learning-architectures-data-scientists/)\n6. Convolutional neural networks are fantastic for visual recognition [tasks](https://cv-tricks.com/cnn/understand-resnet-alexnet-vgg-inception/)"},{"metadata":{},"cell_type":"markdown","source":"### Thanks for reading,Please give suggestions(if any) in the comments. I will continue this tutorial series with more advacned architectures.Hope you enjoyed reading.Upvote if you like the kernel."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}