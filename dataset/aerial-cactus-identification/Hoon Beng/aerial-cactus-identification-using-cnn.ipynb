{"cells":[{"metadata":{},"cell_type":"markdown","source":"Started on 11 Jun 2019"},{"metadata":{},"cell_type":"markdown","source":"# Introduction"},{"metadata":{},"cell_type":"markdown","source":"#### Following from working on [Digit Recognizer][1] and [Fashion MNIST][2] using CNN, I decided to try my hand at this [Aerial Cactus Identification][3] problem. The approach is similar except that in this case, we are working with color images instead of gray-scale images in the other two problems.\n[1]: https://www.kaggle.com/rhodiumbeng/digit-recognizer-convolutional-neural-network\n[2]: https://www.kaggle.com/rhodiumbeng/fashion-mnist-convolutional-neural-network\n[3]: https://www.kaggle.com/c/aerial-cactus-identification"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Examine the data"},{"metadata":{},"cell_type":"markdown","source":"* The images (jpg) for the training data and test data can be found in the train and test folders. The filename of the jpg image files is also used as the unique 'id' in the csv files. \n* 'train_csv' contains the training data ('id' and label 'has_cactus') and 'sample_submission.csv' contains the test data 'id'. "},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# load data from csv files\ntrain_df = pd.read_csv('../input/train.csv')\ntest_df = pd.read_csv('../input/sample_submission.csv')\nprint(train_df.shape, test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['has_cactus'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Let's get a sense of the images labelled 'has_cactus' and those without cactus.\n* I use the image preprocessing functions available in Keras. Keras also has function to convert the jpg pixels into tensors for the purpose of applying neural network."},{"metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array\n\ntrain_path = '../input/train/train/'\ntest_path = '../input/test/test/'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Here are some images from the training data that are labelled as positive, i.e. has cactus:"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":false},"cell_type":"code","source":"# look at some of the pics from train_df with cactus\nhas_cactus = train_df[train_df['has_cactus']==1]\nplt.figure(figsize=(15,7))\nfor i in range(40):  \n    plt.subplot(4, 10, i+1)\n    plt.imshow(load_img(train_path+has_cactus.iloc[i]['id']))\n    plt.title(\"label=%d\" % has_cactus.iloc[i]['has_cactus'], y=1)\n    plt.axis('off')\nplt.subplots_adjust(wspace=0.3, hspace=-0.1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Here are some images from the training data that are labelled negative, i.e. no cactus:"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# look at some of the pics from train_df with no cactus\nno_cactus = train_df[train_df['has_cactus']==0]\nplt.figure(figsize=(15,7))\nfor i in range(40):  \n    plt.subplot(4, 10, i+1)\n    plt.imshow(load_img(train_path+no_cactus.iloc[i]['id']))\n    plt.title(\"label=%d\" % no_cactus.iloc[i]['has_cactus'], y=1)\n    plt.axis('off')\nplt.subplots_adjust(wspace=0.3, hspace=-0.1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Comparing the two sets of images, I see that it looks relatively easy for a person to label the images. Those images with cactus typically have line-like shapes in the images. So I expect that an accuracy of above 90% would be easily attained with a CNN."},{"metadata":{},"cell_type":"markdown","source":"# Prepare the data for use in CNN"},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"def prep_cnn_data(df, n_x, n_c, path):\n    \"\"\"\n    This function loads the image jpg data into tensors\n    \"\"\"\n    # initialize tensors\n    tensors = np.zeros((df.shape[0], n_x, n_x, n_c))\n    # load image as arrays into tensors\n    for i in range(df.shape[0]):\n        pic = load_img(path+df.iloc[i]['id'])\n        pic_array = img_to_array(pic)\n        tensors[i,:] = pic_array\n    # standardize the values by dividing by 255\n    tensors = tensors / 255.\n    return tensors","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prepare the train data for CNN\ntrain_pic_array = prep_cnn_data(train_df, 32, 3, path='../input/train/train/')\ntrain_Y = train_df['has_cactus'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prepare the test data for prediction later on\ntest_pic_array = prep_cnn_data(test_df, 32, 3, path='../input/test/test/')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_pic_array.shape, train_Y.shape)\nprint(test_pic_array.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create CNN Model"},{"metadata":{},"cell_type":"markdown","source":"* In the [Digit Recognizer][1] and [Fashion MNIST][2] problems, I found it useful to use Keras' ImageDataGenerator to augment the training data as the number of training examples were small. Here, I think data augmentation will come in handy as well as there are only 17,500 examples in the training set.\n[1]: https://www.kaggle.com/rhodiumbeng/digit-recognizer-convolutional-neural-network\n[2]: https://www.kaggle.com/rhodiumbeng/fashion-mnist-convolutional-neural-network"},{"metadata":{"trusted":true},"cell_type":"code","source":"# use Keras data generator to augment the training set\nfrom keras_preprocessing.image import ImageDataGenerator\ndata_augment = ImageDataGenerator(zoom_range=0.1, \n                                  width_shift_range=0.1, height_shift_range=0.1,\n                                  horizontal_flip=True, vertical_flip=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# build the CNN from keras\nfrom keras import models\nfrom keras import layers\n\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(32, kernel_size=3, padding='same', activation='relu', input_shape=(32, 32, 3)))\nmodel.add(layers.Conv2D(32, kernel_size=3, padding='valid', activation='relu'))\nmodel.add(layers.MaxPooling2D(pool_size=(2,2), strides=2))\nmodel.add(layers.Dropout(rate=0.4))\nmodel.add(layers.Conv2D(64, kernel_size=5, padding='same', activation='relu'))\nmodel.add(layers.Conv2D(64, kernel_size=5, padding='valid', activation='relu'))\nmodel.add(layers.MaxPooling2D(pool_size=(2,2), strides=2))\nmodel.add(layers.Dropout(rate=0.4))\nmodel.add(layers.Conv2D(128, kernel_size=3, padding='same', activation='relu'))\nmodel.add(layers.Conv2D(128, kernel_size=3, padding='valid', activation='relu'))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(512, activation='relu'))\nmodel.add(layers.Dropout(rate=0.4))\nmodel.add(layers.Dense(512, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', \n              metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Set up a dev set to check the performance of the CNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"# here I use 3500 examples from the training data\nX_dev = train_pic_array[:3500]\nrem_X_train = train_pic_array[3500:]\nprint(X_dev.shape, rem_X_train.shape)\n\nY_dev = train_Y[:3500]\nrem_Y_train = train_Y[3500:]\nprint(Y_dev.shape, rem_Y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Run the model on the train and validation data, and capture metrics history to visualise the performance of the model"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# Train and validate the model\nepochs = 150\nbatch_size = 1024\nhistory = model.fit_generator(data_augment.flow(rem_X_train, rem_Y_train, batch_size=batch_size), \n                              epochs=epochs, steps_per_epoch=rem_X_train.shape[0]//batch_size, \n                              validation_data=(X_dev, Y_dev))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# plot and visualise the training and validation losses\nloss = history.history['loss']\ndev_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\n\nfrom matplotlib import pyplot as plt\nplt.figure(figsize=(15,10))\nplt.plot(epochs, loss, 'bo', label='training loss')\nplt.plot(epochs, dev_loss, 'b', label='validation loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* After 100 epochs, the loss was small and the accuracy reached for the validation data is above 99%."},{"metadata":{},"cell_type":"markdown","source":"# Error Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"# do error analysis on the predictions for X_dev\npred_dev = model.predict(X_dev)\npred_dev = (pred_dev > 0.5).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# look at those that were classified wrongly in X_dev\nresult = pd.DataFrame(train_Y[:3500], columns=['Y_dev'])\nresult['Y_pred'] = pred_dev\nresult['correct'] = result['Y_dev'] - result['Y_pred']\nerrors = result[result['correct'] != 0]\nerror_list = errors.index\nprint('Number of errors is ', len(errors))\nprint('The indices are ', error_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot the image of the wrong in predictions for X_dev\nplt.figure(figsize=(15,8))\nfor i in range(len(error_list)):\n    plt.subplot(4, 10, i+1)\n    plt.imshow(load_img(train_path+train_df.iloc[error_list[i]]['id']))\n    plt.title(\"true={}\\npredict={}\".format(train_Y[error_list[i]], \n                                           pred_dev[error_list[i]]), y=1)\n    plt.axis('off')\nplt.subplots_adjust(wspace=0.3, hspace=-0.1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* These are those in the validation set that were classified incorrectly. Any obvious patterns? I can't tell."},{"metadata":{},"cell_type":"markdown","source":"# Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict on test set\npredictions = model.predict(test_pic_array)\nprint(predictions.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df['has_cactus'] = predictions\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# generate submission file in csv format\ntest_df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}