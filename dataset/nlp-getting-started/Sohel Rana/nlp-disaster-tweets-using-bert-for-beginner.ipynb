{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-21T20:06:18.757144Z","iopub.execute_input":"2021-05-21T20:06:18.75768Z","iopub.status.idle":"2021-05-21T20:06:18.766326Z","shell.execute_reply.started":"2021-05-21T20:06:18.757633Z","shell.execute_reply":"2021-05-21T20:06:18.765457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\nfrom collections import defaultdict, Counter\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nimport nltk\nfrom nltk.corpus import stopwords\n\nfrom wordcloud import WordCloud \nfrom nltk.tokenize import word_tokenize","metadata":{"execution":{"iopub.status.busy":"2021-05-21T20:06:18.810912Z","iopub.execute_input":"2021-05-21T20:06:18.811464Z","iopub.status.idle":"2021-05-21T20:06:18.819028Z","shell.execute_reply.started":"2021-05-21T20:06:18.811428Z","shell.execute_reply":"2021-05-21T20:06:18.81796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nltk.download('stopwords', quiet=True)\nstopwords = stopwords.words('english')\nsns.set(style=\"white\", font_scale=1.2)\nplt.rcParams[\"figure.figsize\"] = [10,8]\npd.set_option.display_max_columns = 0\npd.set_option.display_max_rows = 0","metadata":{"execution":{"iopub.status.busy":"2021-05-21T20:06:18.870592Z","iopub.execute_input":"2021-05-21T20:06:18.871174Z","iopub.status.idle":"2021-05-21T20:06:18.882869Z","shell.execute_reply.started":"2021-05-21T20:06:18.871138Z","shell.execute_reply":"2021-05-21T20:06:18.880581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\n\ntest = pd.read_csv(\"../input/nlp-getting-started/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-05-21T20:06:18.939298Z","iopub.execute_input":"2021-05-21T20:06:18.939607Z","iopub.status.idle":"2021-05-21T20:06:18.9781Z","shell.execute_reply.started":"2021-05-21T20:06:18.939577Z","shell.execute_reply":"2021-05-21T20:06:18.977304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-21T20:06:19.016Z","iopub.execute_input":"2021-05-21T20:06:19.016303Z","iopub.status.idle":"2021-05-21T20:06:19.02993Z","shell.execute_reply.started":"2021-05-21T20:06:19.016275Z","shell.execute_reply":"2021-05-21T20:06:19.029174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape, test.shape, test.shape[0]/train.shape[0]","metadata":{"execution":{"iopub.status.busy":"2021-05-21T20:06:19.058874Z","iopub.execute_input":"2021-05-21T20:06:19.059174Z","iopub.status.idle":"2021-05-21T20:06:19.065909Z","shell.execute_reply.started":"2021-05-21T20:06:19.059141Z","shell.execute_reply":"2021-05-21T20:06:19.065071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('There are {} rows and {} columns in train'.format(train.shape[0],train.shape[1]))\n\nprint('There are {} rows and {} columns in train'.format(test.shape[0],test.shape[1]))\n","metadata":{"execution":{"iopub.status.busy":"2021-05-21T20:06:19.104721Z","iopub.execute_input":"2021-05-21T20:06:19.104965Z","iopub.status.idle":"2021-05-21T20:06:19.110128Z","shell.execute_reply.started":"2021-05-21T20:06:19.104941Z","shell.execute_reply":"2021-05-21T20:06:19.109308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2021-05-21T20:06:19.147696Z","iopub.execute_input":"2021-05-21T20:06:19.147939Z","iopub.status.idle":"2021-05-21T20:06:19.161766Z","shell.execute_reply.started":"2021-05-21T20:06:19.147915Z","shell.execute_reply":"2021-05-21T20:06:19.160711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"null_counts = pd.DataFrame({\"Num_Null\": train.isnull().sum()})\n\nnull_counts[\"Pct_Null\"] = null_counts[\"Num_Null\"] / train.count() * 100\n\nnull_counts","metadata":{"execution":{"iopub.status.busy":"2021-05-21T20:06:19.216757Z","iopub.execute_input":"2021-05-21T20:06:19.216992Z","iopub.status.idle":"2021-05-21T20:06:19.235551Z","shell.execute_reply.started":"2021-05-21T20:06:19.216969Z","shell.execute_reply":"2021-05-21T20:06:19.234762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Part 1:  EDA Analysis ","metadata":{}},{"cell_type":"code","source":"keywords_vc = pd.DataFrame({\"Count\": train[\"keyword\"].value_counts()})\n\nsns.barplot(y=keywords_vc[0:30].index, x=keywords_vc[0:30][\"Count\"], orient='h')\n\nplt.title(\"Top 30 Keywords\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-21T20:06:19.279408Z","iopub.execute_input":"2021-05-21T20:06:19.279695Z","iopub.status.idle":"2021-05-21T20:06:19.641254Z","shell.execute_reply.started":"2021-05-21T20:06:19.279668Z","shell.execute_reply":"2021-05-21T20:06:19.640287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train[\"keyword\"].value_counts())","metadata":{"execution":{"iopub.status.busy":"2021-05-21T20:06:19.642824Z","iopub.execute_input":"2021-05-21T20:06:19.64318Z","iopub.status.idle":"2021-05-21T20:06:19.651886Z","shell.execute_reply.started":"2021-05-21T20:06:19.643143Z","shell.execute_reply":"2021-05-21T20:06:19.650974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"disaster_keywords = train.loc[train[\"target\"] == 1][\"keyword\"].value_counts()\n\nnondisaster_keywords = train.loc[train[\"target\"] == 0][\"keyword\"].value_counts()\n\n","metadata":{"execution":{"iopub.status.busy":"2021-05-21T20:06:19.653694Z","iopub.execute_input":"2021-05-21T20:06:19.654058Z","iopub.status.idle":"2021-05-21T20:06:19.66815Z","shell.execute_reply.started":"2021-05-21T20:06:19.654022Z","shell.execute_reply":"2021-05-21T20:06:19.667294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(1,2, figsize=(25,15))\n\nsns.barplot(y=disaster_keywords[0:30].index, x=disaster_keywords[0:30], orient='h', ax=ax[0], palette=\"Reds_d\")\n\nsns.barplot(y=nondisaster_keywords[0:30].index, x=nondisaster_keywords[0:30], orient='h', ax=ax[1], palette=\"Blues_d\")\n\nax[0].set_title(\"Top 30 Keywords - Disaster Tweets\")\nax[0].set_xlabel(\"Keyword Frequency\")\nax[1].set_title(\"Top 30 Keywords - Non-Disaster Tweets\")\nax[1].set_xlabel(\"Keyword Frequency\")\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-21T20:06:19.669898Z","iopub.execute_input":"2021-05-21T20:06:19.670362Z","iopub.status.idle":"2021-05-21T20:06:20.774326Z","shell.execute_reply.started":"2021-05-21T20:06:19.670324Z","shell.execute_reply":"2021-05-21T20:06:20.77327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"armageddon_tweets = train[(train[\"keyword\"].fillna(\"\").str.contains(\"armageddon\")) & (train[\"target\"] == 0)]\n\nprint(\"An example tweet:\\n\", armageddon_tweets.iloc[10, 3])\n\narmageddon_tweets.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-21T20:06:20.775762Z","iopub.execute_input":"2021-05-21T20:06:20.776104Z","iopub.status.idle":"2021-05-21T20:06:20.798604Z","shell.execute_reply.started":"2021-05-21T20:06:20.776069Z","shell.execute_reply":"2021-05-21T20:06:20.797475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def keyword_disaster_probabilities(x):\n    tweets_w_keyword = np.sum(train[\"keyword\"].fillna(\"\").str.contains(x))\n    tweets_w_keyword_disaster = np.sum(train[\"keyword\"].fillna(\"\").str.contains(x) & train[\"target\"] == 1)\n    return tweets_w_keyword_disaster / tweets_w_keyword\n\nkeywords_vc[\"Disaster_Probability\"] = keywords_vc.index.map(keyword_disaster_probabilities)\nkeywords_vc.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-21T20:06:20.800469Z","iopub.execute_input":"2021-05-21T20:06:20.800823Z","iopub.status.idle":"2021-05-21T20:06:22.999927Z","shell.execute_reply.started":"2021-05-21T20:06:20.800785Z","shell.execute_reply":"2021-05-21T20:06:22.999007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nkeywords_vc.sort_values(by=\"Disaster_Probability\", ascending=False).head(10)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T20:06:23.001588Z","iopub.execute_input":"2021-05-21T20:06:23.001959Z","iopub.status.idle":"2021-05-21T20:06:23.011616Z","shell.execute_reply.started":"2021-05-21T20:06:23.00192Z","shell.execute_reply":"2021-05-21T20:06:23.010819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"keywords_vc.sort_values(by=\"Disaster_Probability\").head(10)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T20:06:23.014983Z","iopub.execute_input":"2021-05-21T20:06:23.015659Z","iopub.status.idle":"2021-05-21T20:06:23.030882Z","shell.execute_reply.started":"2021-05-21T20:06:23.015615Z","shell.execute_reply":"2021-05-21T20:06:23.029929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"locations_vc = train[\"location\"].value_counts()\n\nsns.barplot(y=locations_vc[0:30].index, x=locations_vc[0:30], orient='h')\n\nplt.title(\"Top 30 Locations\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-21T20:06:23.03342Z","iopub.execute_input":"2021-05-21T20:06:23.033962Z","iopub.status.idle":"2021-05-21T20:06:23.403099Z","shell.execute_reply.started":"2021-05-21T20:06:23.033918Z","shell.execute_reply":"2021-05-21T20:06:23.402143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train[\"location\"].value_counts())","metadata":{"execution":{"iopub.status.busy":"2021-05-21T20:06:23.404433Z","iopub.execute_input":"2021-05-21T20:06:23.404787Z","iopub.status.idle":"2021-05-21T20:06:23.414292Z","shell.execute_reply.started":"2021-05-21T20:06:23.404751Z","shell.execute_reply":"2021-05-21T20:06:23.413361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"disaster_locations = train.loc[train[\"target\"] == 1][\"location\"].value_counts()\n\nnondisaster_locations = train.loc[train[\"target\"] == 0][\"location\"].value_counts()\n\n\n\nfig, ax = plt.subplots(1,2, figsize=(20,8))\nsns.barplot(y=disaster_locations[0:30].index, x=disaster_locations[0:30], orient='h', ax=ax[0], palette=\"Reds_d\")\nsns.barplot(y=nondisaster_locations[0:30].index, x=nondisaster_locations[0:30], orient='h', ax=ax[1], palette=\"Blues_d\")\n\nax[0].set_title(\"Top 30 Locations - Disaster Tweets\")\nax[0].set_xlabel(\"Keyword Frequency\")\nax[1].set_title(\"Top 30 Locations - Non-Disaster Tweets\")\nax[1].set_xlabel(\"Keyword Frequency\")\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-21T20:06:23.415582Z","iopub.execute_input":"2021-05-21T20:06:23.416137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[\"tweet_length\"] = train[\"text\"].apply(len)\n\nsns.distplot(train[\"tweet_length\"])\nplt.title(\"Histogram of Tweet Length\")\nplt.xlabel(\"Number of Characters\")\nplt.ylabel(\"Density\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"min(train[\"tweet_length\"]), max(train[\"tweet_length\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g = sns.FacetGrid(train, col=\"target\", height=5)\n\ng = g.map(sns.distplot, \"tweet_length\")\n\nplt.suptitle(\"Distribution Tweet Length\")\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def count_words(x):\n    return len(x.split())\n\ntrain[\"num_words\"] = train[\"text\"].apply(count_words)\n\nsns.distplot(train[\"num_words\"], bins=10)\nplt.title(\"Histogram of Number of Words per Tweet\")\nplt.xlabel(\"Number of Words\")\nplt.ylabel(\"Density\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g = sns.FacetGrid(train, col=\"target\", height=5)\n\ng = g.map(sns.distplot, \"num_words\")\n\nplt.suptitle(\"Distribution Number of Words\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def avg_word_length(x):\n    return np.sum([len(w) for w in x.split()]) / len(x.split())\n\ntrain[\"avg_word_length\"] = train[\"text\"].apply(avg_word_length)\nsns.distplot(train[\"avg_word_length\"])\nplt.title(\"Histogram of Average Word Length\")\nplt.xlabel(\"Average Word Length\")\nplt.ylabel(\"Density\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g = sns.FacetGrid(train, col=\"target\", height=5)\ng = g.map(sns.distplot, \"avg_word_length\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Common Stopwords","metadata":{}},{"cell_type":"code","source":"def create_corpus(target):\n    corpus = []\n\n    for w in train.loc[train[\"target\"] == target][\"text\"].str.split():\n        for i in w:\n            corpus.append(i)\n            \n    return corpus\n\ndef create_corpus_dict(target):\n    corpus = create_corpus(target)\n            \n    stop_dict = defaultdict(int)\n    for word in corpus:\n        if word in stopwords:\n            stop_dict[word] += 1\n    return sorted(stop_dict.items(), key=lambda x:x[1], reverse=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corpus_disaster_dict = create_corpus_dict(0)\ncorpus_non_disaster_dict = create_corpus_dict(1)\n\ndisaster_x, disaster_y = zip(*corpus_disaster_dict)\nnon_disaster_x, non_disaster_y = zip(*corpus_non_disaster_dict)\n\nfig, ax = plt.subplots(1,2, figsize=(20,8))\nsns.barplot(y=list(disaster_x)[0:30], x=list(disaster_y)[0:30], orient='h', palette=\"Reds_d\", ax=ax[0])\nsns.barplot(y=list(non_disaster_x)[0:30], x=list(non_disaster_y)[0:30], orient='h', palette=\"Blues_d\", ax=ax[1]) \n\nax[0].set_title(\"Top 30 Stop Words - Disaster Tweets\")\nax[0].set_xlabel(\"Stop Word Frequency\")\nax[1].set_title(\"Top 30 Stop Words - Non-Disaster Tweets\")\nax[1].set_xlabel(\"Stop Word Frequency\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Common non-stopwords","metadata":{}},{"cell_type":"code","source":"corpus_disaster, corpus_non_disaster = create_corpus(1), create_corpus(0)\ncounter_disaster, counter_non_disaster = Counter(corpus_disaster), Counter(corpus_non_disaster)\nx_disaster, y_disaster, x_non_disaster, y_non_disaster = [], [], [], []\n\ncounter = 0\nfor word, count in counter_disaster.most_common()[0:100]:\n    if (word not in stopwords and counter < 15):\n        counter += 1\n        x_disaster.append(word)\n        y_disaster.append(count)\n\ncounter = 0\nfor word, count in counter_non_disaster.most_common()[0:100]:\n    if (word not in stopwords and counter < 15):\n        counter += 1\n        x_non_disaster.append(word)\n        y_non_disaster.append(count)\n\nfig, ax = plt.subplots(1,2, figsize=(20,8))\nsns.barplot(x=y_disaster, y=x_disaster, orient='h', palette=\"Reds_d\", ax=ax[0])\nsns.barplot(x=y_non_disaster, y=x_non_disaster, orient='h', palette=\"Blues_d\", ax=ax[1])\nax[0].set_title(\"Top 15 Non-Stopwords - Disaster Tweets\")\nax[0].set_xlabel(\"Word Frequency\")\nax[1].set_title(\"Top 15 Non-Stopwords - Non-Disaster Tweets\")\nax[1].set_xlabel(\"Word Frequency\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Common Bigrams","metadata":{}},{"cell_type":"code","source":"def bigrams(target):\n    corpus = train[train[\"target\"] == target][\"text\"]\n    count_vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = count_vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in count_vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bigrams_disaster = bigrams(1)[:15]\nbigrams_non_disaster = bigrams(0)[:15]\n\nx_disaster, y_disaster = map(list, zip(*bigrams_disaster))\nx_non_disaster, y_non_disaster = map(list, zip(*bigrams_non_disaster))\n\nfig, ax = plt.subplots(1,2, figsize=(20,8))\nsns.barplot(x=y_disaster, y=x_disaster, orient='h', palette=\"Reds_d\", ax=ax[0])\nsns.barplot(x=y_non_disaster, y=x_non_disaster, orient='h', palette=\"Blues_d\", ax=ax[1])\n\nax[0].set_title(\"Top 15 Bigrams - Disaster Tweets\")\nax[0].set_xlabel(\"Word Frequency\")\nax[1].set_title(\"Top 15 Bigrams - Non-Disaster Tweets\")\nax[1].set_xlabel(\"Word Frequency\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Taret distribution","metadata":{}},{"cell_type":"code","source":"target_vc = train[\"target\"].value_counts(normalize=True)\nprint(\"Not Disaster: {:.2%}, Disaster: {:.2%}\".format(target_vc[0], target_vc[1]))\nsns.barplot(x=target_vc.index, y=target_vc)\nplt.title(\"Histogram of Disaster vs. Non-Disaster\")\nplt.xlabel(\"0 = Non-Disaster, 1 = Disaster\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.corpus import stopwords\n\n#function for removing pattern\ndef remove_pattern(input_txt, pattern):\n    r = re.findall(pattern, input_txt)\n    for i in r:\n        input_txt = re.sub(i, '', input_txt)\n    return input_txt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove '#' handle\ntrain['tweet'] = np.vectorize(remove_pattern)(train['text'], \"#[\\w]*\")\ntest['tweet'] = np.vectorize(remove_pattern)(test['text'], \"#[\\w]*\") \ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Delete everything except alphabet\ntrain['tweet'] = train['tweet'].str.replace(\"[^a-zA-Z#]\", \" \")\n\ntest['tweet'] = test['tweet'].str.replace(\"[^a-zA-Z#]\", \" \")\n\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Dropping words whose length is less than 3\ntrain['tweet'] = train['tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\ntest['tweet'] = test['tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\ntrain.head()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#convert all the words into lower case\ntrain['tweet'] = train['tweet'].str.lower()\n\ntest['tweet'] = test['tweet'].str.lower()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"set(stopwords.words('english'))\n\n# set of stop words\nstops = set(stopwords.words('english')) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tokens of words  \ntrain['tokenized_sents'] = train.apply(lambda row: nltk.word_tokenize(row['tweet']), axis=1)\n\ntest['tokenized_sents'] = test.apply(lambda row: nltk.word_tokenize(row['tweet']), axis=1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#function to remove stop words\ndef remove_stops(row):\n    my_list = row['tokenized_sents']\n    meaningful_words = [w for w in my_list if not w in stops]\n    return (meaningful_words)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#removing stop words\ntrain['clean_tweet'] = train.apply(remove_stops, axis=1)\n\ntest['clean_tweet'] = test.apply(remove_stops, axis=1)\n\ntrain.drop([\"tweet\",\"tokenized_sents\"], axis = 1, inplace = True)\ntest.drop([\"tweet\",\"tokenized_sents\"], axis = 1, inplace = True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#re-join the words after tokenization\ndef rejoin_words(row):\n    my_list = row['clean_tweet']\n    joined_words = ( \" \".join(my_list))\n    return joined_words\n\n\n\ntrain['clean_tweet'] = train.apply(rejoin_words, axis=1)\n\ntest['clean_tweet'] = test.apply(rejoin_words, axis=1)\n\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualization of all the words using word cloud","metadata":{}},{"cell_type":"code","source":"all_word = ' '.join([text for text in train['clean_tweet']])\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_word) \nplt.figure(figsize=(15, 12)) \nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off') \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualization of all the words which signify real disaster","metadata":{}},{"cell_type":"code","source":"normal_words =' '.join([text for text in train['clean_tweet'][train['target'] == 1]]) \n\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(normal_words) \n\nplt.figure(figsize=(15, 12)) \nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualization of all the words which signify unreal disaster\n# ","metadata":{}},{"cell_type":"code","source":"normal_words =' '.join([text for text in train['clean_tweet'][train['target'] == 0]]) \n\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(normal_words) \n\nplt.figure(figsize=(10, 7)) \nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Import more librarries","metadata":{}},{"cell_type":"code","source":"import gc\nimport time\nimport math\nimport random\nimport warnings","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nfrom datetime import date\nfrom transformers import *\nfrom sklearn.metrics import *\nfrom tqdm.notebook import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.data\nimport torch.nn.functional as F\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import string\nimport folium\nfrom colorama import Fore, Back, Style, init\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import scipy as sp\nimport networkx as nx\nfrom pandas import Timestamp\n\nfrom PIL import Image\nfrom IPython.display import SVG\nfrom keras.utils import model_to_dot\n\nimport requests\nfrom IPython.display import HTML","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\n\ntqdm.pandas()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\n\nimport transformers\nimport tensorflow as tf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import Callback\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Tf \nfrom tensorflow.keras.models import Model\nfrom kaggle_datasets import KaggleDatasets\nfrom tensorflow.keras.optimizers import Adam\nfrom tokenizers import BertWordPieceTokenizer\nfrom tensorflow.keras.layers import Dense, Input, Dropout, Embedding\nfrom tensorflow.keras.layers import LSTM, GRU, Conv1D, SpatialDropout1D\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import layers\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras import activations\nfrom tensorflow.keras import constraints\nfrom tensorflow.keras import initializers\nfrom tensorflow.keras import regularizers\n\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.optimizers import *\nfrom tensorflow.keras.activations import *\nfrom tensorflow.keras.constraints import *\nfrom tensorflow.keras.initializers import *\nfrom tensorflow.keras.regularizers import *\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import metrics\nfrom sklearn.utils import shuffle\nfrom gensim.models import Word2Vec\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer,HashingVectorizer\nfrom sklearn.model_selection import train_test_split\n\n#Sklearn models-ML classification\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#NLP \nfrom nltk.stem.wordnet import WordNetLemmatizer \nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize import TweetTokenizer  \n\nimport nltk\nfrom textblob import TextBlob\n\n\nfrom nltk.corpus import wordnet\nfrom nltk.corpus import stopwords\nfrom nltk import WordNetLemmatizer\nfrom nltk.stem import WordNetLemmatizer,PorterStemmer\nfrom wordcloud import WordCloud, STOPWORDS\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk.tokenize import sent_tokenize, word_tokenize\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stopword=set(STOPWORDS)\n\n\nlem = WordNetLemmatizer()\ntokenizer=TweetTokenizer()\n\nnp.random.seed(0)\nrandom_state = 42","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install GPUtil\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch import nn\nfrom transformers import AdamW, BertConfig, BertModel, BertTokenizer\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset, random_split\nfrom transformers import get_linear_schedule_with_warmup\nfrom sklearn.metrics import f1_score, accuracy_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from GPUtil import showUtilization as gpu_usage\nfrom numba import cuda\n\ndef free_gpu_cache():\n    print(\"Initial GPU Usage\")\n    gpu_usage()                             \n\n    torch.cuda.empty_cache()\n\n    cuda.select_device(0)\n    cuda.close()\n    cuda.select_device(0)\n\n    for obj in gc.get_objects():\n        if torch.is_tensor(obj):\n            del obj\n    gc.collect()\n    \n    print(\"GPU Usage after emptying the cache\")\n    gpu_usage()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2nd Part","metadata":{}},{"cell_type":"code","source":"from torch import nn\nfrom transformers import AdamW, BertConfig, BertModel, BertTokenizer\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset, random_split\nfrom transformers import get_linear_schedule_with_warmup\nfrom sklearn.metrics import f1_score, accuracy_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"../input/nlp-getting-started/train.csv\").loc[:,[\"text\",\"target\"]]\ntrain","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# select device\nif torch.cuda.is_available():        \n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device(\"cpu\")\n\ndevice","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data Preprocessing**","metadata":{}},{"cell_type":"code","source":"dupli_sum = train.duplicated().sum()\nif(dupli_sum>0):\n    print(dupli_sum, \" duplicates found\\nremoving...\")\n    train = train.loc[False==train.duplicated(), :]\nelse:\n    print(\"no duplicates found\")\ntrain","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = train[\"text\"].values\ny_train = train[\"target\"].values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"BERT depends on a special tokenizing format & vocabulary. Thus, we need to use its custom tokenizer.\n\nWe should take a look at the resulting number of tokens that we would get by using the tokenizer without padding/truncating the sequences. Based on that we can define our padding/truncating-strategy.","metadata":{}},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n\nlens = []\n\nfor text in X_train:\n    encoded_dict = tokenizer.encode_plus(text, add_special_tokens=True, return_tensors='pt')\n    lens.append(encoded_dict['input_ids'].size()[1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10,5))\n\nax.set_title(\"Frequency of Tokens in the whole Dataset\")\nax.set_xlabel(\"Token ID\")\nax.set_ylabel(\"Frequency\")\n\n\npd.Series(lens).value_counts().head(50).plot(kind=\"bar\");\n\nprint(\"text length mean: \", np.array(lens).mean())\nprint(\"text length median: \", np.median(lens))\nprint(\"text length standard deviation: \", np.array(lens).std())\nprint(\"suitable sequence length: \", np.array(lens).mean() + 2*np.array(lens).std())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Findings**:\n\nno huge difference between mean and median\ntruncate the texts to a tokenized length of  \n⌈\n  mean + 2  ⋅ std  ⌉\n  many tokens for more generalization  \n→\n  sequence_lengthsequence_length = 58","metadata":{}},{"cell_type":"code","source":"sequence_length = 58\n# X_train_tokens[i] := sequence of sequence_length many tokens that represent text_{i}\nX_train_tokens = []\n\nfor text in X_train:\n    encoded_dict = tokenizer.encode_plus(text,\n                                         add_special_tokens=True, # special tokens for BERT\n                                         max_length=sequence_length,\n                                         padding=\"max_length\",\n                                         return_tensors='pt', # pytorch tensor format\n                                         truncation=True)\n    X_train_tokens.append(encoded_dict['input_ids'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pytorch expects tensors\nX_train_tokens = torch.cat(X_train_tokens, dim=0) # concat into one tensor\n\ny_train = torch.tensor(y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tokenization of the first observation in the training set\n# zero-padding is added\nprint('Original:\\n', X_train[5])\nprint('Tokenization:\\n', X_train_tokens[5])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataloader\nWe will use Dataloaders to draw batches of data for our model individually.\n\nOne Dataloader is used to draw batches for training the model and the other Dataloader is used to draw data for validating its performance.\n\n\n* create a training set and a validation set\n* create a dataloader for each of these sets with a defined sampling policy and a batch size","metadata":{}},{"cell_type":"code","source":"batch_size = 32\n\n# split into training and validation data\ndataset = TensorDataset(X_train_tokens, y_train.float())\n\ntrain_size = int(0.80 * len(dataset))\nval_size = len(dataset) - train_size\n\ntrain_set, val_set = random_split(dataset, [train_size, val_size])\n\n\ntrain_dataloader = DataLoader(train_set, \n                              sampler=RandomSampler(train_set), \n                              batch_size=batch_size)\n\nvalidation_dataloader = DataLoader(val_set, \n                                   sampler=RandomSampler(val_set), \n                                   batch_size=batch_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's take a look at what we get from our Dataloaders:","metadata":{}},{"cell_type":"code","source":"for batch in train_dataloader:\n    print(\"what is drawn from our dataloader? \", type(batch))\n    \n    print(\"\\nfirst entry: \", batch[0].size(), type(batch[0]), batch[0].dtype)\n    print(\"\\nsecond entry: \", batch[1].size(), type(batch[1]), batch[1].dtype)\n    \n    break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can iteratively draw batches from the Dataloader\n\n* each Batch is stored in a list\n* the first entry of the list is a tensor of the dimension batch_size x features. The tokens are stored in it\n* the second entry of the list is a tensor of dimension batch_size. The class labels of each observation in the batch are stored in it","metadata":{}},{"cell_type":"markdown","source":"# Model Creation\nOur model consists of two main components. The first component is BERT, which creates a feature representation from given text sequences.\n\nThe second component is a classifier plugin which is used on top of the feature representation created by BERT.\n\nTo match our classifier to the feature representations, we have to investigate the latter:","metadata":{}},{"cell_type":"code","source":"bert = BertModel.from_pretrained(\"bert-base-uncased\")\nbert.to(device)\n\nfor batch in train_dataloader: \n    batch_features = batch[0].to(device)\n    bert_output = bert(input_ids=batch_features) \n    \n    print(\"bert output: \", type(bert_output), len(bert_output))\n    \n    print(\"first entry: \", type(bert_output[0]), bert_output[0].size())\n    \n    print(\"second entry: \", type(bert_output[1]), bert_output[1].size())\n    \n    break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# For each batch, BERT provides\n\n* an iterable having two entries\n* the first entry contains a tensor of size batch_size x sequence_length x 768, which stores the representation of each token in each sequence of the batch. As we can see, each single token is represented in a vector fo size 768.\n* the second entry contains a tensor of size batch_size x 768. It contains the pooled representation of the whole sequence per observation in our batch. This is what we want to use as an interface.","metadata":{}},{"cell_type":"code","source":"class BertClassifier(nn.Module):\n    def __init__(self):\n        super(BertClassifier, self).__init__()\n        \n        self.bert = BertModel.from_pretrained('bert-base-uncased') # returns pwerful representations of the microblogs\n        self.linear = nn.Linear(768, 1) # custom layer; input of the first custom layer has to match the dimensionality of the BERT-output; further custom layers are possible\n        self.sigmoid = nn.Sigmoid() # activation function applied to our custom layer to obtain probabilities\n    \n    def forward(self, tokens):\n        bert_output = self.bert(input_ids=tokens)\n        linear_output = self.linear(bert_output[1])\n        proba = self.sigmoid(linear_output)\n        return proba","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train the Model and evaluate it on the Validation Set\nnote: this is a greedy training loop that takes the model with the best possible validation score. A more robust approach would be to consider the difference between the training score and the validation score as well.\n\nEarly stopping w.r.t. the epochs would most likely not provide any improvements, since binary classification ontop of BERT usually overfits already after very few epochs (2 to 4).","metadata":{}},{"cell_type":"code","source":"def eval(y_batch, probas):\n    \n    preds_batch_np = np.round(probas.cpu().detach().numpy())\n    \n    y_batch_np = y_batch.cpu().detach().numpy()\n    \n    acc = accuracy_score(y_true=y_batch_np, y_pred=preds_batch_np)\n    \n    f1 = f1_score(y_true=y_batch_np, y_pred=preds_batch_np, average='weighted')\n    \n    return acc, f1\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef train(model, optimizer, scheduler, epochs, name):\n    history = []\n    best_f1 = 0\n    model.train()\n    \n    for epoch in range(epochs):\n        \n        # ===== train =====\n        print(\"=== Epoch: \", epoch+1, \" / \", epochs, \" ===\")\n        acc_total = 0\n        f1_total = 0\n        \n        for it, batch in enumerate(train_dataloader): \n            \n            x_batch, y_batch = [batch[0].to(device), batch[1].to(device)] # draw the batch\n            \n            probas = torch.flatten(model(tokens=x_batch))\n            \n            acc_f1_batch = eval(y_batch, probas)\n            \n            acc_total, f1_total = acc_total + acc_f1_batch[0], f1_total + acc_f1_batch[1]\n            \n            model.zero_grad() # reset the gradients\n            \n            loss_func = nn.BCELoss()\n            \n            batch_loss = loss_func(probas, y_batch)\n            \n            batch_loss.backward() # calculate gradient per (learnable) weight\n            \n            optimizer.step() # update (learnable) weights\n            \n            scheduler.step() # update learning rate\n            \n        acc_total = acc_total/len(train_dataloader) #len(train dataloader)=num_batches\n        \n        f1_total = f1_total/len(train_dataloader)\n        \n        print(\"accuracy: \", acc_total, \"\\nf1: \", f1_total)\n        \n        \n\n        # ===== validate =====\n        acc_val_total = 0\n        f1_val_total = 0\n        \n        for batch in validation_dataloader:\n            \n            x_batch, y_batch = [batch[0].to(device), batch[1].to(device)]\n            \n            with torch.no_grad(): # gradients don't have to be computed, because no update is performed\n                probas = torch.flatten(model(tokens=x_batch))\n            acc_f1_val_batch = eval(y_batch, probas)\n            \n            acc_val_total, f1_val_total = acc_val_total + acc_f1_val_batch[0], f1_val_total + acc_f1_val_batch[1]\n            \n        acc_val_total = acc_val_total/len(validation_dataloader)\n        f1_val_total = f1_val_total/len(validation_dataloader)\n        \n        print(\"validation accuracy: \", acc_val_total, \"\\nvalidation f1: \", f1_val_total, \"\\n\")\n        if(f1_val_total>best_f1): # save current mdoel if this epoch improved models validation performance \n            torch.save(model, name+\".pt\")\n            best_f1 = f1_val_total\n\n        history.append({\"acc\":acc_total, \"f1\":f1_total, \"acc_val\":acc_val_total, \"f1_val\":f1_val_total})\n    return [torch.load(name+\".pt\"), history]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 10\n\nbaseline_bert_clf = BertClassifier()\n\nbaseline_bert_clf = baseline_bert_clf.to(device)\n\nadam = AdamW(baseline_bert_clf.parameters(), lr=5e-5, eps=1e-8)\n\ntotal_steps = len(train_dataloader) * epochs\n\nsched = get_linear_schedule_with_warmup(adam, num_warmup_steps=0, num_training_steps=total_steps)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"baseline_bert_clf, history = train(model=baseline_bert_clf,\n                                   optimizer=adam,\n                                   scheduler=sched,\n                                   epochs=10,\n                                   name=\"baseline_bert_clf\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_df = pd.DataFrame(history)\n\nhistory_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# analogously to above:\nX_test = pd.read_csv(\"../input/nlp-getting-started/test.csv\")[\"text\"]\n\nX_test_tokens = []\n\nfor text in X_test:\n    \n    encoded_dict = tokenizer.encode_plus(text,\n                                         add_special_tokens=True,\n                                         max_length=sequence_length,\n                                         padding=\"max_length\",\n                                         return_tensors='pt',\n                                         truncation=True)\n    \n    X_test_tokens.append(encoded_dict['input_ids'])\n    \n    \n    \nX_test_tokens = torch.cat(X_test_tokens, dim=0)\n\n\n\ntest_set = TensorDataset(X_test_tokens)\n\ntest_dataloader = DataLoader(test_set, \n                             sampler=SequentialSampler(test_set), \n                             batch_size=batch_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots()\nax.set_title(\"f1 history\")\nax.set_ylabel(\"f1\")\nax.set_xlabel(\"epoch\")\nplt.xticks(ticks=np.arange(0,20), labels=np.arange(1,21))\nhistory_df.loc[:,['f1', 'f1_val']].plot(ax=ax)\nplt.savefig(\"f1.png\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predict on Test","metadata":{}},{"cell_type":"code","source":"# analogously to above:\nX_test = pd.read_csv(\"../input/nlp-getting-started/test.csv\")[\"text\"]\n\nX_test_tokens = []\nfor text in X_test:\n    encoded_dict = tokenizer.encode_plus(text,\n                                         add_special_tokens=True,\n                                         max_length=sequence_length,\n                                         padding=\"max_length\",\n                                         return_tensors='pt',\n                                         truncation=True)\n    X_test_tokens.append(encoded_dict['input_ids'])\nX_test_tokens = torch.cat(X_test_tokens, dim=0)\n\n\n\ntest_set = TensorDataset(X_test_tokens)\ntest_dataloader = DataLoader(test_set, \n                             sampler=SequentialSampler(test_set), \n                             batch_size=batch_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_preds = []\n\nfor batch in test_dataloader:\n    x_batch = batch[0].to(device)\n    with torch.no_grad():\n        probas = baseline_bert_clf(tokens=x_batch)\n    preds = np.round(probas.cpu().detach().numpy()).astype(int).flatten()\n    all_preds.extend(preds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"challenge_pred = pd.concat([pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")[\"id\"], pd.Series(all_preds)], axis=1)\nchallenge_pred.columns = ['id', 'target']\nchallenge_pred.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}