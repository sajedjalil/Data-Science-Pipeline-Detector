{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-18T12:16:04.640307Z","iopub.execute_input":"2021-09-18T12:16:04.64062Z","iopub.status.idle":"2021-09-18T12:16:04.733963Z","shell.execute_reply.started":"2021-09-18T12:16:04.640535Z","shell.execute_reply":"2021-09-18T12:16:04.733094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing all the dependencies","metadata":{}},{"cell_type":"code","source":"# !pip install tensorflow_text","metadata":{"execution":{"iopub.status.busy":"2021-09-18T12:16:04.735559Z","iopub.execute_input":"2021-09-18T12:16:04.735886Z","iopub.status.idle":"2021-09-18T12:16:04.739737Z","shell.execute_reply.started":"2021-09-18T12:16:04.73585Z","shell.execute_reply":"2021-09-18T12:16:04.739052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install sentencepiece \n# !pip install tensorflow_addons\n# !pip install gin","metadata":{"execution":{"iopub.status.busy":"2021-09-18T12:16:04.741028Z","iopub.execute_input":"2021-09-18T12:16:04.741553Z","iopub.status.idle":"2021-09-18T12:16:04.74787Z","shell.execute_reply.started":"2021-09-18T12:16:04.741518Z","shell.execute_reply":"2021-09-18T12:16:04.747262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\n!git clone --depth 1 -b v2.3.0 https://github.com/tensorflow/models.git\n\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport sys\nsys.path.append('models')\n!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\nfrom helper_functions import *\n\n# sys.path.append('models')\nfrom official.nlp.data import classifier_data_lib\nfrom official.nlp.bert import tokenization\n# from official.nlp import optimization\n\nimport re\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# DATA_URL = \"https://archive.org/download/fine-tune-bert-tensorflow-train.csv/train.csv.zip\"\n# df = pd.read_csv(DATA_URL, compression=\"zip\", low_memory=False)\n\nDATA_URL = \"https://raw.githubusercontent.com/gkv856/KaggleData/main/train.csv\"\nTEST_URL = \"https://raw.githubusercontent.com/gkv856/KaggleData/main/test.csv\"\n\ntrain = pd.read_csv(DATA_URL, low_memory=False)\ntest = pd.read_csv(TEST_URL, low_memory=False)\n\nprint(tf.version.VERSION)","metadata":{"execution":{"iopub.status.busy":"2021-09-18T12:16:04.749984Z","iopub.execute_input":"2021-09-18T12:16:04.750281Z","iopub.status.idle":"2021-09-18T12:16:16.473517Z","shell.execute_reply.started":"2021-09-18T12:16:04.750249Z","shell.execute_reply":"2021-09-18T12:16:16.472685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA - exploratory data analysis","metadata":{}},{"cell_type":"code","source":"train.shape, test.shape, test.shape[0]/train.shape[0]","metadata":{"execution":{"iopub.status.busy":"2021-09-18T12:16:16.474733Z","iopub.execute_input":"2021-09-18T12:16:16.475388Z","iopub.status.idle":"2021-09-18T12:16:16.483424Z","shell.execute_reply.started":"2021-09-18T12:16:16.475349Z","shell.execute_reply":"2021-09-18T12:16:16.482386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"There are '{}' rows and '{}' columns in training data\".format(train.shape[0],train.shape[1]))\nprint(\"There are '{}' rows and '{}' columns in test data\".format(test.shape[0],test.shape[1]))","metadata":{"execution":{"iopub.status.busy":"2021-09-18T12:16:16.485151Z","iopub.execute_input":"2021-09-18T12:16:16.485631Z","iopub.status.idle":"2021-09-18T12:16:16.49452Z","shell.execute_reply.started":"2021-09-18T12:16:16.485542Z","shell.execute_reply":"2021-09-18T12:16:16.493742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2021-09-18T12:16:16.496Z","iopub.execute_input":"2021-09-18T12:16:16.496493Z","iopub.status.idle":"2021-09-18T12:16:16.525338Z","shell.execute_reply.started":"2021-09-18T12:16:16.496461Z","shell.execute_reply":"2021-09-18T12:16:16.524529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"null_counts = pd.DataFrame({\"Num_Null\": train.isnull().sum()})\nnull_counts[\"Pct_Null\"] = null_counts[\"Num_Null\"] / train.count() * 100\nnull_counts","metadata":{"execution":{"iopub.status.busy":"2021-09-18T12:16:16.528254Z","iopub.execute_input":"2021-09-18T12:16:16.528434Z","iopub.status.idle":"2021-09-18T12:16:16.562684Z","shell.execute_reply.started":"2021-09-18T12:16:16.528412Z","shell.execute_reply":"2021-09-18T12:16:16.562002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"keywords_vc = pd.DataFrame({\"Count\": train[\"keyword\"].value_counts()})\nsns.barplot(y=keywords_vc[0:30].index, x=keywords_vc[0:30][\"Count\"], orient='h')\nplt.title(\"Top 30 Keywords\")\nplt.show()\nplt.figure(figsize=(40, 40))","metadata":{"execution":{"iopub.status.busy":"2021-09-18T12:16:16.565812Z","iopub.execute_input":"2021-09-18T12:16:16.566022Z","iopub.status.idle":"2021-09-18T12:16:16.998587Z","shell.execute_reply.started":"2021-09-18T12:16:16.566001Z","shell.execute_reply":"2021-09-18T12:16:16.997904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# total unique keywords\nlen(train[\"keyword\"].value_counts())","metadata":{"execution":{"iopub.status.busy":"2021-09-18T12:16:17.000189Z","iopub.execute_input":"2021-09-18T12:16:17.000467Z","iopub.status.idle":"2021-09-18T12:16:17.008958Z","shell.execute_reply.started":"2021-09-18T12:16:17.000434Z","shell.execute_reply":"2021-09-18T12:16:17.008092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"disaster_keywords = train.loc[train[\"target\"] == 1][\"keyword\"].value_counts()\nnondisaster_keywords = train.loc[train[\"target\"] == 0][\"keyword\"].value_counts()\n\nfig, ax = plt.subplots(1,2, figsize=(20,8))\nsns.barplot(y=disaster_keywords[0:30].index, x=disaster_keywords[0:30], orient='h', ax=ax[0], palette=\"Reds_d\")\nsns.barplot(y=nondisaster_keywords[0:30].index, x=nondisaster_keywords[0:30], orient='h', ax=ax[1], palette=\"Blues_d\")\nax[0].set_title(\"Top 30 Keywords - Disaster Tweets\")\nax[0].set_xlabel(\"Keyword Frequency\")\nax[1].set_title(\"Top 30 Keywords - Non-Disaster Tweets\")\nax[1].set_xlabel(\"Keyword Frequency\")\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-18T12:16:17.010642Z","iopub.execute_input":"2021-09-18T12:16:17.011092Z","iopub.status.idle":"2021-09-18T12:16:18.048573Z","shell.execute_reply.started":"2021-09-18T12:16:17.01091Z","shell.execute_reply":"2021-09-18T12:16:18.047873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mask = (train[\"keyword\"].fillna(\"\").str.contains(\"armageddon\")) & (train[\"target\"] == 0)\narmageddon_tweets = train[mask]\nprint(\"An example tweet from 10th row:\\n\", armageddon_tweets.iloc[10, 3])\narmageddon_tweets.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-18T12:16:18.0497Z","iopub.execute_input":"2021-09-18T12:16:18.050119Z","iopub.status.idle":"2021-09-18T12:16:18.072149Z","shell.execute_reply.started":"2021-09-18T12:16:18.050081Z","shell.execute_reply":"2021-09-18T12:16:18.071315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def keyword_disaster_probabilities(x):\n    tweets_w_keyword = np.sum(train[\"keyword\"].fillna(\"\").str.contains(x))\n    tweets_w_keyword_disaster = np.sum(train[\"keyword\"].fillna(\"\").str.contains(x) & train[\"target\"] == 1)\n    return tweets_w_keyword_disaster / tweets_w_keyword\n\nkeywords_vc[\"Disaster_Probability\"] = keywords_vc.index.map(keyword_disaster_probabilities)\nkeywords_vc.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-18T12:16:18.073416Z","iopub.execute_input":"2021-09-18T12:16:18.073835Z","iopub.status.idle":"2021-09-18T12:16:20.496973Z","shell.execute_reply.started":"2021-09-18T12:16:18.073792Z","shell.execute_reply":"2021-09-18T12:16:20.496151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# sorting the diaster probabilty\nk_vc = keywords_vc.sort_values(by=\"Disaster_Probability\", ascending=False)\nk_vc.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-09-18T12:16:20.498343Z","iopub.execute_input":"2021-09-18T12:16:20.498592Z","iopub.status.idle":"2021-09-18T12:16:20.510052Z","shell.execute_reply.started":"2021-09-18T12:16:20.49856Z","shell.execute_reply":"2021-09-18T12:16:20.509125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"k_vc.tail(10)","metadata":{"execution":{"iopub.status.busy":"2021-09-18T12:16:20.512037Z","iopub.execute_input":"2021-09-18T12:16:20.512418Z","iopub.status.idle":"2021-09-18T12:16:20.526999Z","shell.execute_reply.started":"2021-09-18T12:16:20.512381Z","shell.execute_reply":"2021-09-18T12:16:20.526392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"locations_vc = train[\"location\"].value_counts()\nsns.barplot(y=locations_vc[0:30].index, x=locations_vc[0:30], orient='h')\nplt.title(\"Top 30 Locations\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-18T12:16:20.528335Z","iopub.execute_input":"2021-09-18T12:16:20.528711Z","iopub.status.idle":"2021-09-18T12:16:20.91888Z","shell.execute_reply.started":"2021-09-18T12:16:20.528676Z","shell.execute_reply":"2021-09-18T12:16:20.918221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"disaster_locations = train.loc[train[\"target\"] == 1][\"location\"].value_counts()\nnondisaster_locations = train.loc[train[\"target\"] == 0][\"location\"].value_counts()\n\nfig, ax = plt.subplots(1,2, figsize=(20,8))\nsns.barplot(y=disaster_locations[0:30].index, x=disaster_locations[0:30], orient='h', ax=ax[0], palette=\"Reds_d\")\nsns.barplot(y=nondisaster_locations[0:30].index, x=nondisaster_locations[0:30], orient='h', ax=ax[1], palette=\"Blues_d\")\nax[0].set_title(\"Top 30 Locations - Disaster Tweets\")\nax[0].set_xlabel(\"Keyword Frequency\")\nax[1].set_title(\"Top 30 Locations - Non-Disaster Tweets\")\nax[1].set_xlabel(\"Keyword Frequency\")\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-18T12:16:20.920093Z","iopub.execute_input":"2021-09-18T12:16:20.920509Z","iopub.status.idle":"2021-09-18T12:16:21.921811Z","shell.execute_reply.started":"2021-09-18T12:16:20.920471Z","shell.execute_reply":"2021-09-18T12:16:21.921073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[\"tweet_length\"] = train[\"text\"].apply(len)\nsns.distplot(train[\"tweet_length\"])\nplt.title(\"Histogram of Tweet Length\")\nplt.xlabel(\"Number of Characters\")\nplt.ylabel(\"Density\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-18T12:16:21.923005Z","iopub.execute_input":"2021-09-18T12:16:21.923336Z","iopub.status.idle":"2021-09-18T12:16:22.215777Z","shell.execute_reply.started":"2021-09-18T12:16:21.923302Z","shell.execute_reply":"2021-09-18T12:16:22.21509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# min and max tweet length\nmin(train[\"tweet_length\"]), max(train[\"tweet_length\"])","metadata":{"execution":{"iopub.status.busy":"2021-09-18T12:16:22.21697Z","iopub.execute_input":"2021-09-18T12:16:22.217229Z","iopub.status.idle":"2021-09-18T12:16:22.226255Z","shell.execute_reply.started":"2021-09-18T12:16:22.217199Z","shell.execute_reply":"2021-09-18T12:16:22.225465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# histogram for each category of tweet\ng = sns.FacetGrid(train, col=\"target\", height=5)\ng = g.map(sns.histplot, \"tweet_length\")\nplt.suptitle(\"Distribution Tweet Length\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-18T12:16:22.22771Z","iopub.execute_input":"2021-09-18T12:16:22.227971Z","iopub.status.idle":"2021-09-18T12:16:22.722916Z","shell.execute_reply.started":"2021-09-18T12:16:22.22794Z","shell.execute_reply":"2021-09-18T12:16:22.722242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def count_words(x):\n    return len(x.split())\n\ntrain[\"num_words\"] = train[\"text\"].apply(count_words)\nsns.distplot(train[\"num_words\"], bins=10)\nplt.title(\"Histogram of Number of Words per Tweet\")\nplt.xlabel(\"Number of Words\")\nplt.ylabel(\"Density\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-18T12:16:22.724353Z","iopub.execute_input":"2021-09-18T12:16:22.724613Z","iopub.status.idle":"2021-09-18T12:16:22.981616Z","shell.execute_reply.started":"2021-09-18T12:16:22.72458Z","shell.execute_reply":"2021-09-18T12:16:22.980973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g = sns.FacetGrid(train, col=\"target\", height=5)\ng = g.map(sns.distplot, \"num_words\")\nplt.suptitle(\"Distribution Number of Words\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-18T12:16:22.983082Z","iopub.execute_input":"2021-09-18T12:16:22.983363Z","iopub.status.idle":"2021-09-18T12:16:23.71073Z","shell.execute_reply.started":"2021-09-18T12:16:22.98333Z","shell.execute_reply":"2021-09-18T12:16:23.71005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def avg_word_length(x):\n    return np.sum([len(w) for w in x.split()]) / len(x.split())\n\ntrain[\"avg_word_length\"] = train[\"text\"].apply(avg_word_length)\nsns.distplot(train[\"avg_word_length\"])\nplt.title(\"Histogram of Average Word Length\")\nplt.xlabel(\"Average Word Length\")\nplt.ylabel(\"Density\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-18T12:16:23.71438Z","iopub.execute_input":"2021-09-18T12:16:23.714572Z","iopub.status.idle":"2021-09-18T12:16:24.137445Z","shell.execute_reply.started":"2021-09-18T12:16:23.71455Z","shell.execute_reply":"2021-09-18T12:16:24.136768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train","metadata":{"execution":{"iopub.status.busy":"2021-09-18T12:16:24.138464Z","iopub.execute_input":"2021-09-18T12:16:24.138704Z","iopub.status.idle":"2021-09-18T12:16:24.156594Z","shell.execute_reply.started":"2021-09-18T12:16:24.138672Z","shell.execute_reply":"2021-09-18T12:16:24.155729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# our data is imbalanced and we will assume that in the real world/test data, \n# this will continued to be the case \ntrain[\"target\"].plot(kind=\"hist\")","metadata":{"execution":{"iopub.status.busy":"2021-09-18T12:16:24.158279Z","iopub.execute_input":"2021-09-18T12:16:24.158597Z","iopub.status.idle":"2021-09-18T12:16:24.367796Z","shell.execute_reply.started":"2021-09-18T12:16:24.158514Z","shell.execute_reply":"2021-09-18T12:16:24.367177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# later you can use the whole data for traiing and test\ntrain_df, valid_df = train_test_split(train, \n                                       random_state=42, \n                                      #  train_size=0.9, \n                                       test_size=.25,\n                                       stratify=train[\"target\"].values)\n\n# use the below structure for testing if the data is huge\n# train_df, remaining = train_test_split(train, \n#                                        random_state=42, \n#                                        train_size=0.0095, \n#                                        stratify=train[\"target\"].values)\n\n# valid_df, _ = train_test_split(remaining, \n#                               random_state=42, \n#                               train_size=0.0095, \n#                               stratify=remaining[\"target\"].values)\nlen(train_df), len(valid_df)","metadata":{"execution":{"iopub.status.busy":"2021-09-18T12:16:24.36902Z","iopub.execute_input":"2021-09-18T12:16:24.369275Z","iopub.status.idle":"2021-09-18T12:16:24.385611Z","shell.execute_reply.started":"2021-09-18T12:16:24.369244Z","shell.execute_reply":"2021-09-18T12:16:24.384867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating datasets\nwith tf.device('/cpu:0'):\n  train_data = tf.data.Dataset.from_tensor_slices((train_df[\"text\"].values,\n                                                   train_df[\"target\"].values))\n  \n  test_data = tf.data.Dataset.from_tensor_slices((valid_df[\"text\"].values,\n                                                   valid_df[\"target\"].values))\ntrain_data, test_data","metadata":{"execution":{"iopub.status.busy":"2021-09-18T12:16:24.386715Z","iopub.execute_input":"2021-09-18T12:16:24.387034Z","iopub.status.idle":"2021-09-18T12:16:26.050087Z","shell.execute_reply.started":"2021-09-18T12:16:24.386998Z","shell.execute_reply":"2021-09-18T12:16:26.04943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for text, label in train_data.take(1):\n  print(text)\n  print(label)","metadata":{"execution":{"iopub.status.busy":"2021-09-18T12:16:26.051366Z","iopub.execute_input":"2021-09-18T12:16:26.051696Z","iopub.status.idle":"2021-09-18T12:16:26.095865Z","shell.execute_reply.started":"2021-09-18T12:16:26.051658Z","shell.execute_reply":"2021-09-18T12:16:26.095219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nEach line of the dataset is composed of the review text and its label\n- Data preprocessing consists of transforming text to BERT input features:\ninput_word_ids, input_mask, segment_ids\n- In the process, tokenizing the text is done with the provided BERT model tokenizer\n\"\"\"\n\n# Label categories, right now our data has these categories\nlabel_list = [0, 1]\n\n# maximum length of (token) input sequences, or the words in a question\n# to save speed we should reset this\nmax_seq_length = 128\n\ntrain_batch_size = 32","metadata":{"execution":{"iopub.status.busy":"2021-09-18T12:16:26.097037Z","iopub.execute_input":"2021-09-18T12:16:26.097295Z","iopub.status.idle":"2021-09-18T12:16:26.103399Z","shell.execute_reply.started":"2021-09-18T12:16:26.097263Z","shell.execute_reply":"2021-09-18T12:16:26.102737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get BERT layer and tokenizer:\n# More details here: https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\nMODEL_URL = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\"\n# MODEL_URL = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\"\nbert_layer = hub.KerasLayer(MODEL_URL, trainable=True)\n\n\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","metadata":{"execution":{"iopub.status.busy":"2021-09-18T12:16:26.104841Z","iopub.execute_input":"2021-09-18T12:16:26.105188Z","iopub.status.idle":"2021-09-18T12:16:43.137471Z","shell.execute_reply.started":"2021-09-18T12:16:26.105138Z","shell.execute_reply":"2021-09-18T12:16:43.136689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Task 6: Tokenize and Preprocess Text for BERT","metadata":{}},{"cell_type":"code","source":"# This provides a function to convert row to input features and label\nfrom bs4 import BeautifulSoup\nimport re\nimport string\n\nabbreviations = {\n    \"$\" : \" dollar \",\n    \"â‚¬\" : \" euro \",\n    \"4ao\" : \"for adults only\",\n    \"a.m\" : \"before midday\",\n    \"a3\" : \"anytime anywhere anyplace\",\n    \"aamof\" : \"as a matter of fact\",\n    \"acct\" : \"account\",\n    \"adih\" : \"another day in hell\",\n    \"afaic\" : \"as far as i am concerned\",\n    \"afaict\" : \"as far as i can tell\",\n    \"afaik\" : \"as far as i know\",\n    \"afair\" : \"as far as i remember\",\n    \"afk\" : \"away from keyboard\",\n    \"app\" : \"application\",\n    \"approx\" : \"approximately\",\n    \"apps\" : \"applications\",\n    \"asap\" : \"as soon as possible\",\n    \"asl\" : \"age, sex, location\",\n    \"atk\" : \"at the keyboard\",\n    \"ave.\" : \"avenue\",\n    \"aymm\" : \"are you my mother\",\n    \"ayor\" : \"at your own risk\", \n    \"b&b\" : \"bed and breakfast\",\n    \"b+b\" : \"bed and breakfast\",\n    \"b.c\" : \"before christ\",\n    \"b2b\" : \"business to business\",\n    \"b2c\" : \"business to customer\",\n    \"b4\" : \"before\",\n    \"b4n\" : \"bye for now\",\n    \"b@u\" : \"back at you\",\n    \"bae\" : \"before anyone else\",\n    \"bak\" : \"back at keyboard\",\n    \"bbbg\" : \"bye bye be good\",\n    \"bbc\" : \"british broadcasting corporation\",\n    \"bbias\" : \"be back in a second\",\n    \"bbl\" : \"be back later\",\n    \"bbs\" : \"be back soon\",\n    \"be4\" : \"before\",\n    \"bfn\" : \"bye for now\",\n    \"blvd\" : \"boulevard\",\n    \"bout\" : \"about\",\n    \"brb\" : \"be right back\",\n    \"bros\" : \"brothers\",\n    \"brt\" : \"be right there\",\n    \"bsaaw\" : \"big smile and a wink\",\n    \"btw\" : \"by the way\",\n    \"bwl\" : \"bursting with laughter\",\n    \"c/o\" : \"care of\",\n    \"cet\" : \"central european time\",\n    \"cf\" : \"compare\",\n    \"cia\" : \"central intelligence agency\",\n    \"csl\" : \"can not stop laughing\",\n    \"cu\" : \"see you\",\n    \"cul8r\" : \"see you later\",\n    \"cv\" : \"curriculum vitae\",\n    \"cwot\" : \"complete waste of time\",\n    \"cya\" : \"see you\",\n    \"cyt\" : \"see you tomorrow\",\n    \"dae\" : \"does anyone else\",\n    \"dbmib\" : \"do not bother me i am busy\",\n    \"diy\" : \"do it yourself\",\n    \"dm\" : \"direct message\",\n    \"dwh\" : \"during work hours\",\n    \"e123\" : \"easy as one two three\",\n    \"eet\" : \"eastern european time\",\n    \"eg\" : \"example\",\n    \"embm\" : \"early morning business meeting\",\n    \"encl\" : \"enclosed\",\n    \"encl.\" : \"enclosed\",\n    \"etc\" : \"and so on\",\n    \"faq\" : \"frequently asked questions\",\n    \"fawc\" : \"for anyone who cares\",\n    \"fb\" : \"facebook\",\n    \"fc\" : \"fingers crossed\",\n    \"fig\" : \"figure\",\n    \"fimh\" : \"forever in my heart\", \n    \"ft.\" : \"feet\",\n    \"ft\" : \"featuring\",\n    \"ftl\" : \"for the loss\",\n    \"ftw\" : \"for the win\",\n    \"fwiw\" : \"for what it is worth\",\n    \"fyi\" : \"for your information\",\n    \"g9\" : \"genius\",\n    \"gahoy\" : \"get a hold of yourself\",\n    \"gal\" : \"get a life\",\n    \"gcse\" : \"general certificate of secondary education\",\n    \"gfn\" : \"gone for now\",\n    \"gg\" : \"good game\",\n    \"gl\" : \"good luck\",\n    \"glhf\" : \"good luck have fun\",\n    \"gmt\" : \"greenwich mean time\",\n    \"gmta\" : \"great minds think alike\",\n    \"gn\" : \"good night\",\n    \"g.o.a.t\" : \"greatest of all time\",\n    \"goat\" : \"greatest of all time\",\n    \"goi\" : \"get over it\",\n    \"gps\" : \"global positioning system\",\n    \"gr8\" : \"great\",\n    \"gratz\" : \"congratulations\",\n    \"gyal\" : \"girl\",\n    \"h&c\" : \"hot and cold\",\n    \"hp\" : \"horsepower\",\n    \"hr\" : \"hour\",\n    \"hrh\" : \"his royal highness\",\n    \"ht\" : \"height\",\n    \"ibrb\" : \"i will be right back\",\n    \"ic\" : \"i see\",\n    \"icq\" : \"i seek you\",\n    \"icymi\" : \"in case you missed it\",\n    \"idc\" : \"i do not care\",\n    \"idgadf\" : \"i do not give a damn fuck\",\n    \"idgaf\" : \"i do not give a fuck\",\n    \"idk\" : \"i do not know\",\n    \"ie\" : \"that is\",\n    \"i.e\" : \"that is\",\n    \"ifyp\" : \"i feel your pain\",\n    \"IG\" : \"instagram\",\n    \"iirc\" : \"if i remember correctly\",\n    \"ilu\" : \"i love you\",\n    \"ily\" : \"i love you\",\n    \"imho\" : \"in my humble opinion\",\n    \"imo\" : \"in my opinion\",\n    \"imu\" : \"i miss you\",\n    \"iow\" : \"in other words\",\n    \"irl\" : \"in real life\",\n    \"j4f\" : \"just for fun\",\n    \"jic\" : \"just in case\",\n    \"jk\" : \"just kidding\",\n    \"jsyk\" : \"just so you know\",\n    \"l8r\" : \"later\",\n    \"lb\" : \"pound\",\n    \"lbs\" : \"pounds\",\n    \"ldr\" : \"long distance relationship\",\n    \"lmao\" : \"laugh my ass off\",\n    \"lmfao\" : \"laugh my fucking ass off\",\n    \"lol\" : \"laughing out loud\",\n    \"ltd\" : \"limited\",\n    \"ltns\" : \"long time no see\",\n    \"m8\" : \"mate\",\n    \"mf\" : \"motherfucker\",\n    \"mfs\" : \"motherfuckers\",\n    \"mfw\" : \"my face when\",\n    \"mofo\" : \"motherfucker\",\n    \"mph\" : \"miles per hour\",\n    \"mr\" : \"mister\",\n    \"mrw\" : \"my reaction when\",\n    \"ms\" : \"miss\",\n    \"mte\" : \"my thoughts exactly\",\n    \"nagi\" : \"not a good idea\",\n    \"nbc\" : \"national broadcasting company\",\n    \"nbd\" : \"not big deal\",\n    \"nfs\" : \"not for sale\",\n    \"ngl\" : \"not going to lie\",\n    \"nhs\" : \"national health service\",\n    \"nrn\" : \"no reply necessary\",\n    \"nsfl\" : \"not safe for life\",\n    \"nsfw\" : \"not safe for work\",\n    \"nth\" : \"nice to have\",\n    \"nvr\" : \"never\",\n    \"nyc\" : \"new york city\",\n    \"oc\" : \"original content\",\n    \"og\" : \"original\",\n    \"ohp\" : \"overhead projector\",\n    \"oic\" : \"oh i see\",\n    \"omdb\" : \"over my dead body\",\n    \"omg\" : \"oh my god\",\n    \"omw\" : \"on my way\",\n    \"p.a\" : \"per annum\",\n    \"p.m\" : \"after midday\",\n    \"pm\" : \"prime minister\",\n    \"poc\" : \"people of color\",\n    \"pov\" : \"point of view\",\n    \"pp\" : \"pages\",\n    \"ppl\" : \"people\",\n    \"prw\" : \"parents are watching\",\n    \"ps\" : \"postscript\",\n    \"pt\" : \"point\",\n    \"ptb\" : \"please text back\",\n    \"pto\" : \"please turn over\",\n    \"qpsa\" : \"what happens\", #\"que pasa\",\n    \"ratchet\" : \"rude\",\n    \"rbtl\" : \"read between the lines\",\n    \"rlrt\" : \"real life retweet\", \n    \"rofl\" : \"rolling on the floor laughing\",\n    \"roflol\" : \"rolling on the floor laughing out loud\",\n    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n    \"rt\" : \"retweet\",\n    \"ruok\" : \"are you ok\",\n    \"sfw\" : \"safe for work\",\n    \"sk8\" : \"skate\",\n    \"smh\" : \"shake my head\",\n    \"sq\" : \"square\",\n    \"srsly\" : \"seriously\", \n    \"ssdd\" : \"same stuff different day\",\n    \"tbh\" : \"to be honest\",\n    \"tbs\" : \"tablespooful\",\n    \"tbsp\" : \"tablespooful\",\n    \"tfw\" : \"that feeling when\",\n    \"thks\" : \"thank you\",\n    \"tho\" : \"though\",\n    \"thx\" : \"thank you\",\n    \"tia\" : \"thanks in advance\",\n    \"til\" : \"today i learned\",\n    \"tl;dr\" : \"too long i did not read\",\n    \"tldr\" : \"too long i did not read\",\n    \"tmb\" : \"tweet me back\",\n    \"tntl\" : \"trying not to laugh\",\n    \"ttyl\" : \"talk to you later\",\n    \"u\" : \"you\",\n    \"u2\" : \"you too\",\n    \"u4e\" : \"yours for ever\",\n    \"utc\" : \"coordinated universal time\",\n    \"w/\" : \"with\",\n    \"w/o\" : \"without\",\n    \"w8\" : \"wait\",\n    \"wassup\" : \"what is up\",\n    \"wb\" : \"welcome back\",\n    \"wtf\" : \"what the fuck\",\n    \"wtg\" : \"way to go\",\n    \"wtpa\" : \"where the party at\",\n    \"wuf\" : \"where are you from\",\n    \"wuzup\" : \"what is up\",\n    \"wywh\" : \"wish you were here\",\n    \"yd\" : \"yard\",\n    \"ygtr\" : \"you got that right\",\n    \"ynk\" : \"you never know\",\n    \"zzz\" : \"sleeping bored and tired\"\n}\n# Change an abbreviation by its true meaning\ndef word_abbrev(word):\n    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word\n\n# Replace all abbreviations\ndef replace_abbrev(text):\n    string = \"\"\n    for word in text.split():\n        string += word_abbrev(word) + \" \"        \n    return string\n\n# Remove all emojis, replace by EMOJI\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'EMOJI', text)\n\n# Replace some others smileys with SADFACE\ndef transcription_sad(text):\n    eyes = \"[8:=;]\"\n    nose = \"['`\\-]\"\n    smiley = re.compile(r'[8:=;][\\'\\-]?[(\\\\/]')\n    return smiley.sub(r'SADFACE', text)\n\n# Replace some smileys with SMILE\ndef transcription_smile(text):\n    eyes = \"[8:=;]\"\n    nose = \"['`\\-]\"\n    smiley = re.compile(r'[8:=;][\\'\\-]?[)dDp]')\n    #smiley = re.compile(r'#{eyes}#{nose}[)d]+|[)d]+#{nose}#{eyes}/i')\n    return smiley.sub(r'SMILE', text)\n\n# Replace <3 with HEART\ndef transcription_heart(text):\n    heart = re.compile(r'<3')\n    return heart.sub(r'HEART', text)\n\n# Factorize elongated words, add ELONG\ndef remove_elongated_words(text):\n    rep = re.compile(r'\\b(\\S*?)([a-z])\\2{2,}\\b')\n    return rep.sub(r'\\1\\2 ELONG', text)\n\n# Factorize repeated punctuation, add REPEAT\ndef remove_repeat_punct(text):\n    rep = re.compile(r'([!?.]){2,}')\n    return rep.sub(r'\\1 REPEAT', text)\n\n\n# Remove all punctuations\ndef remove_all_punct(text):\n    table = str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\n# Remove punctuations\ndef remove_punct(text):\n    punctuations = '@#!?+&*[]-%.:/();$=><|{}^' + \"'`\" \n    for p in punctuations:\n        text = text.replace(p, f' {p} ')\n\n    text = text.replace('...', ' ... ')\n    if '...' not in text:\n        text = text.replace('..', ' ... ')   \n    return text\n\n# Remove all english stopwords\ndef remove_stopwords(text):\n    text = ' '.join([word for word in text.split() if word not in (stopwords)])\n    return text\n\ndef get_clean_text(text_str):\n  text_str = BeautifulSoup(text_str, \"lxml\").get_text()\n  text_str = re.sub(r\"@[A-Za-z0-9]+\", \" \", text_str)\n\n  # replacing URLs with just 'URL' text\n  text_str = re.sub(r\"https?://[[A-Za-z0-9./]+\", \"URL\", text_str)\n  \n  # remove html beacons\n  text_str = re.sub(r\"<.*?>\", \" \", text_str)\n\n  # replacing @someuser to 'USER' text\n  text_str = re.sub(r\"@\\S+\", \"USER\", text_str)\n\n  # replacing numbers to 'NUMBER' text\n  text_str = re.sub(r\"[-+]?[.\\d]*[\\d]+[:,.\\d]*\", \"NUMBER\", text_str)\n\n  # correcting the abbreviated words to real meaning\n  text_str = replace_abbrev(text_str)\n\n  # Remove emojis / smileys\n  text_str = remove_emoji(text_str)\n  text_str = transcription_sad(text_str)\n  text_str = transcription_smile(text_str)\n  text_str = transcription_heart(text_str)\n  \n  # # Remove repeated puntuations / words\n  text_str = remove_elongated_words(text_str)\n  text_str = remove_repeat_punct(text_str)\n\n  # text_str = remove_all_punct(text_str)\n  # text_str = remove_punct(text_str)\n  # text_str = remove_stopwords(text_str)\n\n\n  text_str = re.sub(r\"[^a-zA-Z.!?']\", \" \", text_str)\n  text_str = re.sub(r\" +\", \" \", text_str)\n  \n  \n\n\n  return text_str\n\ndef to_feature(text, label, label_list=label_list, max_seq_length=max_seq_length, tokenizer=tokenizer):\n  # guid is unique id for each example, we dont need textb as per our usecase\n  clean_text = get_clean_text(text.numpy())\n  # clean_text = text.numpy()\n\n  example = classifier_data_lib.InputExample(guid=None, text_a=clean_text, label=label.numpy())\n\n  feature = classifier_data_lib.convert_single_example(0, \n                                                       example, \n                                                       label_list, \n                                                       max_seq_length, \n                                                       tokenizer)\n  \n  return (feature.input_ids, feature.input_mask, feature.segment_ids, feature.label_id)","metadata":{"execution":{"iopub.status.busy":"2021-09-18T12:16:43.139384Z","iopub.execute_input":"2021-09-18T12:16:43.139639Z","iopub.status.idle":"2021-09-18T12:16:43.317626Z","shell.execute_reply.started":"2021-09-18T12:16:43.139608Z","shell.execute_reply":"2021-09-18T12:16:43.316904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# testing the feature function\n# ct= get_clean_text(\"@ENews Ben Affleck......I know there's a wife/kids and other girls but I can't help it. I've loved him since Armageddon #eonlinechat\")\n# ct = tf.constant(ct)\n# to_feature(ct, tf.constant(0))","metadata":{"execution":{"iopub.status.busy":"2021-09-18T12:16:43.319064Z","iopub.execute_input":"2021-09-18T12:16:43.319348Z","iopub.status.idle":"2021-09-18T12:16:43.434886Z","shell.execute_reply.started":"2021-09-18T12:16:43.319314Z","shell.execute_reply":"2021-09-18T12:16:43.434103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Task 7: Wrap a Python Function into a TensorFlow op for Eager Execution","metadata":{}},{"cell_type":"code","source":"def to_feature_map(text, label):\n  out = tf.py_function(to_feature, inp=[text, label], \n                       Tout=[tf.int32, tf.int32, tf.int32, tf.int32,])\n  iids, imask, segids, label = out[0], out[1], out[2], out[3]\n\n  iids.set_shape([max_seq_length])\n  imask.set_shape([max_seq_length])\n  segids.set_shape([max_seq_length])\n  label.set_shape([])\n\n  x = {\n        \"input_word_ids\": iids,\n        \"input_mask\": imask,\n        \"input_type_ids\": segids\n  }\n  return (x, label)","metadata":{"execution":{"iopub.status.busy":"2021-09-18T12:16:43.436305Z","iopub.execute_input":"2021-09-18T12:16:43.436638Z","iopub.status.idle":"2021-09-18T12:16:43.445434Z","shell.execute_reply.started":"2021-09-18T12:16:43.436602Z","shell.execute_reply":"2021-09-18T12:16:43.444591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#testing the data\ns = train_data.take(1)\nfor t, l in s:\n  # print(t)\n  inps = to_feature_map(t, l)[0]  \n  print(inps)","metadata":{"execution":{"iopub.status.busy":"2021-09-18T12:16:43.446771Z","iopub.execute_input":"2021-09-18T12:16:43.447094Z","iopub.status.idle":"2021-09-18T12:16:43.476543Z","shell.execute_reply.started":"2021-09-18T12:16:43.447061Z","shell.execute_reply":"2021-09-18T12:16:43.475877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with tf.device('/cpu:0'):\n  # train\n  train_data = (train_data\n                .map(to_feature_map, num_parallel_calls=tf.data.AUTOTUNE)\n                .shuffle(1000)\n                .batch(train_batch_size, drop_remainder=True)\n                .prefetch(tf.data.AUTOTUNE))\n\n  # valid\n  \n  test_data = (test_data\n                .map(to_feature_map, num_parallel_calls=tf.data.AUTOTUNE)\n                # .shuffle(1000) we dont need to shuffle the test data, just need them for predictions\n                .batch(train_batch_size, drop_remainder=True)\n                .prefetch(tf.data.AUTOTUNE))","metadata":{"execution":{"iopub.status.busy":"2021-09-18T12:16:43.477531Z","iopub.execute_input":"2021-09-18T12:16:43.478189Z","iopub.status.idle":"2021-09-18T12:16:43.584193Z","shell.execute_reply.started":"2021-09-18T12:16:43.478131Z","shell.execute_reply":"2021-09-18T12:16:43.583545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train data spec\ntrain_data.element_spec","metadata":{"execution":{"iopub.status.busy":"2021-09-18T12:16:43.585183Z","iopub.execute_input":"2021-09-18T12:16:43.585429Z","iopub.status.idle":"2021-09-18T12:16:43.591357Z","shell.execute_reply.started":"2021-09-18T12:16:43.585398Z","shell.execute_reply":"2021-09-18T12:16:43.590509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Add a Classification Head to the BERT Layer","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras import layers\n# Train model\nfrom datetime import datetime\nfrom tensorflow.keras import callbacks","metadata":{"execution":{"iopub.status.busy":"2021-09-18T12:16:43.593132Z","iopub.execute_input":"2021-09-18T12:16:43.593407Z","iopub.status.idle":"2021-09-18T12:16:43.599268Z","shell.execute_reply.started":"2021-09-18T12:16:43.593375Z","shell.execute_reply":"2021-09-18T12:16:43.598206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Building the model\ndef create_model(drop_rate=0.4, m_name=\"model\"):\n  input_word_ids = layers.Input(shape=(max_seq_length, ),\n                             dtype=tf.int32,\n                             name=\"input_word_ids\")\n  input_mask = layers.Input(shape=(max_seq_length, ),\n                             dtype=tf.int32,\n                             name=\"input_mask\")\n  input_type_ids = layers.Input(shape=(max_seq_length, ),\n                             dtype=tf.int32,\n                             name=\"input_type_ids\")\n  # pooled output is vector representation of the whole sentence\n  # sequesnced output is vector representation of each word\n  inp_as_dict = {\n      \"input_word_ids\": input_word_ids,\n      \"input_mask\": input_mask,\n      \"input_type_ids\": input_type_ids\n  }\n  inp_as_lst = [input_word_ids, input_mask, input_type_ids]\n\n  bert_outputs = bert_layer(inp_as_dict)\n  pooled_output = bert_outputs[\"pooled_output\"]      # [batch_size, 768].\n  sequence_output = bert_outputs[\"sequence_output\"]  # [batch_size, seq_length, 768].\n\n\n  # x = layers.Dense(512, activation = 'relu')(pooled_output)\n  # x = layers.Dropout(0.5)(x)\n  # x = layers.Dense(256, activation = 'relu')(x)\n  # x = layers.Dropout(0.2)(x)\n  # x = layers.Dense(64, activation = 'relu')(x)\n  x = layers.Dropout(drop_rate)(pooled_output)\n  outputs = layers.Dense(1, activation=\"sigmoid\", name=\"outputs\")(x)\n\n  model = tf.keras.Model(\n      #mapping the input dict values here\n      inputs = {\n            'input_word_ids': input_word_ids,\n            'input_mask': input_mask,\n            'input_type_ids': input_type_ids,\n      }, \n      outputs=outputs, name=m_name)\n  \n  return model","metadata":{"execution":{"iopub.status.busy":"2021-09-18T12:16:43.600895Z","iopub.execute_input":"2021-09-18T12:16:43.601226Z","iopub.status.idle":"2021-09-18T12:16:43.611898Z","shell.execute_reply.started":"2021-09-18T12:16:43.601117Z","shell.execute_reply":"2021-09-18T12:16:43.611247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compile_model(model, lr=2e-5):\n  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n                loss=tf.keras.losses.BinaryCrossentropy(),\n                metrics=[\"accuracy\"])\n\n  model_fig = tf.keras.utils.plot_model(model, show_shapes=True, dpi=72)\n  return model, model_fig","metadata":{"execution":{"iopub.status.busy":"2021-09-18T12:16:43.613354Z","iopub.execute_input":"2021-09-18T12:16:43.613667Z","iopub.status.idle":"2021-09-18T12:16:43.623982Z","shell.execute_reply.started":"2021-09-18T12:16:43.613634Z","shell.execute_reply":"2021-09-18T12:16:43.623145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = create_model(m_name=\"model0\")\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-09-18T12:16:43.625433Z","iopub.execute_input":"2021-09-18T12:16:43.625724Z","iopub.status.idle":"2021-09-18T12:16:44.561295Z","shell.execute_reply.started":"2021-09-18T12:16:43.625691Z","shell.execute_reply":"2021-09-18T12:16:44.56062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model, _ = compile_model(model, 1e-6)","metadata":{"execution":{"iopub.status.busy":"2021-09-18T12:16:44.562518Z","iopub.execute_input":"2021-09-18T12:16:44.56276Z","iopub.status.idle":"2021-09-18T12:16:45.405268Z","shell.execute_reply.started":"2021-09-18T12:16:44.562729Z","shell.execute_reply":"2021-09-18T12:16:45.404317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2021-09-18T12:16:45.407644Z","iopub.execute_input":"2021-09-18T12:16:45.407865Z","iopub.status.idle":"2021-09-18T12:16:46.112484Z","shell.execute_reply.started":"2021-09-18T12:16:45.407839Z","shell.execute_reply":"2021-09-18T12:16:46.111666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"now = datetime.now()\ncurr_time = now.strftime(\"%Y%m%d_%H%M%S\")\nfilepath = \"01Disaster_tweet/dense\" + curr_time  \nchk_pt = callbacks.ModelCheckpoint(filepath=filepath,\n                                    save_weights_only=True,\n                                    verbose=0)\n\nEPOCHS = 6\nhistory = model.fit(train_data,\n                    validation_data=test_data,\n                    epochs=EPOCHS,\n                    callbacks=[chk_pt])","metadata":{"execution":{"iopub.status.busy":"2021-09-18T12:16:46.114321Z","iopub.execute_input":"2021-09-18T12:16:46.114607Z","iopub.status.idle":"2021-09-18T12:26:54.318878Z","shell.execute_reply.started":"2021-09-18T12:16:46.114571Z","shell.execute_reply":"2021-09-18T12:26:54.316934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_graphs(history, metric):\n  plt.plot(history.history[metric])\n  plt.plot(history.history['val_'+metric], '')\n  plt.xlabel(\"Epochs\")\n  plt.ylabel(metric)\n  plt.legend([metric, 'val_'+metric])\n  plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-18T12:28:09.580434Z","iopub.execute_input":"2021-09-18T12:28:09.580719Z","iopub.status.idle":"2021-09-18T12:28:09.586957Z","shell.execute_reply.started":"2021-09-18T12:28:09.580689Z","shell.execute_reply":"2021-09-18T12:28:09.585958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_graphs(history, \"accuracy\")","metadata":{"execution":{"iopub.status.busy":"2021-09-18T12:28:24.985893Z","iopub.execute_input":"2021-09-18T12:28:24.986145Z","iopub.status.idle":"2021-09-18T12:28:25.19403Z","shell.execute_reply.started":"2021-09-18T12:28:24.986118Z","shell.execute_reply":"2021-09-18T12:28:25.193296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TEST_URL = \"https://raw.githubusercontent.com/gkv856/KaggleData/main/test.csv\"\ndf_test = pd.read_csv(TEST_URL)\ndf_test.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-18T12:28:33.774705Z","iopub.execute_input":"2021-09-18T12:28:33.774979Z","iopub.status.idle":"2021-09-18T12:28:34.097356Z","shell.execute_reply.started":"2021-09-18T12:28:33.774951Z","shell.execute_reply":"2021-09-18T12:28:34.096663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# num = 5\n# test_sample = df_test[\"text\"].head(num)\n# ids = df_test['id'].head(num)\n# test_sample\n\nnum = 5\ntest_sample = df_test[\"text\"]#.head(num)\nids = df_test['id']#.head(num)\ntest_sample","metadata":{"execution":{"iopub.status.busy":"2021-09-18T12:28:43.421606Z","iopub.execute_input":"2021-09-18T12:28:43.421869Z","iopub.status.idle":"2021-09-18T12:28:43.430543Z","shell.execute_reply.started":"2021-09-18T12:28:43.421841Z","shell.execute_reply":"2021-09-18T12:28:43.429726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# making preditions on the real test set\nreal_test_data = tf.data.Dataset.from_tensor_slices((test_sample, [0]*len(test_sample)))\nreal_test_data = (real_test_data.map(to_feature_map).batch(train_batch_size)\n                  .prefetch(tf.data.AUTOTUNE))\n\ny_preds = model.predict(real_test_data) \ny_preds = tf.round(y_preds)\ny_preds = tf.cast(y_preds, dtype=tf.int32)\n\nres = tf.squeeze(y_preds).numpy()\nmy_sub = pd.DataFrame({'id': ids,'target':res})\n\nmy_sub.head(20)","metadata":{"execution":{"iopub.status.busy":"2021-09-18T12:29:11.570242Z","iopub.execute_input":"2021-09-18T12:29:11.570753Z","iopub.status.idle":"2021-09-18T12:29:27.32286Z","shell.execute_reply.started":"2021-09-18T12:29:11.570718Z","shell.execute_reply":"2021-09-18T12:29:27.322193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_sub.to_csv(\"sub_clean_data_slow_lr.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-18T12:29:36.435095Z","iopub.execute_input":"2021-09-18T12:29:36.435776Z","iopub.status.idle":"2021-09-18T12:29:36.450282Z","shell.execute_reply.started":"2021-09-18T12:29:36.43574Z","shell.execute_reply":"2021-09-18T12:29:36.449608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nEPOCHS = 6\nhistory = model.fit(train_data,\n                    validation_data=test_data,\n                    epochs=EPOCHS,\n                    callbacks=[chk_pt])","metadata":{"execution":{"iopub.status.busy":"2021-09-18T12:30:07.173355Z","iopub.execute_input":"2021-09-18T12:30:07.17364Z","iopub.status.idle":"2021-09-18T12:40:02.002081Z","shell.execute_reply.started":"2021-09-18T12:30:07.173612Z","shell.execute_reply":"2021-09-18T12:40:02.001337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# making preditions on the real test set\nreal_test_data = tf.data.Dataset.from_tensor_slices((test_sample, [0]*len(test_sample)))\nreal_test_data = (real_test_data.map(to_feature_map).batch(train_batch_size)\n                  .prefetch(tf.data.AUTOTUNE))\n\ny_preds = model.predict(real_test_data) \ny_preds = tf.round(y_preds)\ny_preds = tf.cast(y_preds, dtype=tf.int32)\n\nres = tf.squeeze(y_preds).numpy()\nmy_sub = pd.DataFrame({'id': ids,'target':res})\n\nmy_sub.head(20)","metadata":{"execution":{"iopub.status.busy":"2021-09-18T12:40:59.047966Z","iopub.execute_input":"2021-09-18T12:40:59.048234Z","iopub.status.idle":"2021-09-18T12:41:14.300925Z","shell.execute_reply.started":"2021-09-18T12:40:59.048207Z","shell.execute_reply":"2021-09-18T12:41:14.300265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_sub.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-18T12:41:42.90081Z","iopub.execute_input":"2021-09-18T12:41:42.90156Z","iopub.status.idle":"2021-09-18T12:41:42.914077Z","shell.execute_reply.started":"2021-09-18T12:41:42.90152Z","shell.execute_reply":"2021-09-18T12:41:42.913295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}