{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# This notebook will help you to do:\n* Import training and test data\n* Preprocessing Text\n* Univariate Analysis\n* Bivariate Analysis\n* Run many ML algorithms using H2O\n* Compare all model performance in test dataset\n* Choosing the best model\n* Interpret model output with Shapley Value","metadata":{}},{"cell_type":"markdown","source":"# 1. Parameters","metadata":{}},{"cell_type":"code","source":"#Model ID\nModelId='NLP_Disaster_Tweets_FML_v1'\n\n#Setting the model target variable name\nvar_target = 'target'\n\n#process outputs such as MOJO model, images and performance of tested models\nOutputPath='/kaggle/working'\n\n#If you have a huge dataset, I should consider use a small sample for first execution\npct_sample_size = 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Import Libraries","metadata":{}},{"cell_type":"code","source":"import glob\nimport functools\nimport datetime as dt\nimport pandas as pd\nimport numpy as np\nimport h2o\nimport matplotlib.pyplot as plt\nimport shap\nfrom pandas_profiling import ProfileReport\nfrom collections import defaultdict\nfrom pandas_profiling.model.base import get_var_type\nimport seaborn as sns\nimport os\nimport random\nimport re","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Text Preprocessing","metadata":{}},{"cell_type":"code","source":"#Import bases with features for modeling\n#In this case we will use titanic dataset available below\ndataprep_df_full = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\n#The target variavle must be integer 0 or 1\n\ndataprep_df_full[var_target] = dataprep_df_full[var_target].astype(int)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### It is necessary to create a variable to indicate the records used in training and testing. In this case we will use the random variable, but you can use a date variable for exemple if you have a base with a reference date to fix the training base as an out of time validation.","metadata":{}},{"cell_type":"code","source":"random.seed(59354518745)\nfor i in range(len(dataprep_df_full)):\n    dataprep_df_full.loc[i, ('random')] = random.random()\ndataprep_df_full['dataset'] = ['train' if x <= 0.85 else 'test' for x in dataprep_df_full['random']]\ndataprep_df_full = dataprep_df_full.drop(columns=['random'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Work with a sample data if the pct_sample_size is less than 1\nif pct_sample_size == 1:\n    dataprep_df = dataprep_df_full\nelse:    \n    dataprep_df = dataprep_df_full.sample(frac=pct_sample_size, replace=False, random_state=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.1 Data Cleaning","metadata":{}},{"cell_type":"code","source":"def remove_pattern(input_txt,pattern):\n    r=re.findall(pattern, input_txt)\n    for i in r:\n        input_txt = re.sub(i,'',input_txt)\n    return input_txt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Removing Twitter handles (@user)\ndataprep_df['tidy_text'] = np.vectorize(remove_pattern)(dataprep_df['text'],'@[/w]*')\ndataprep_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Removing Punctuations, Numbers, and Special Characters\ndataprep_df['tidy_text'] = dataprep_df['tidy_text'].str.replace('[^a-zA-Z#]',' ')\ndataprep_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Removing Short Words\ndataprep_df['tidy_text'] = dataprep_df['tidy_text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))\ndataprep_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.2 Tokenizer and Lemma","metadata":{}},{"cell_type":"code","source":"#Tokenizing\ntokenized_text = dataprep_df['tidy_text'].apply(lambda x: x.split())\ntokenized_text.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Stemming\nfrom nltk.stem.porter import *\nstemmer = PorterStemmer()\ntokenized_text = tokenized_text.apply(lambda x: [stemmer.stem(i) for i in x])\ntokenized_text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range (len(tokenized_text)):\n    tokenized_text[i] = ' '.join(tokenized_text[i])\ndataprep_df['tidy_text'] = tokenized_text\ndataprep_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.3 TF-IDF features","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_vectorizer = TfidfVectorizer(max_df = 0.90, min_df= 2,\n                                  max_features=1000, stop_words='english')\ntfidf = tfidf_vectorizer.fit_transform(dataprep_df['tidy_text'])\ntfidf.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Set tfidf feature names\ndf_tfidf = pd.DataFrame(tfidf.todense())\ndf_tfidf.columns =[tfidf_vectorizer.get_feature_names()]\ndf_tfidf.columns =  df_tfidf.columns.get_level_values(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataprep_df = dataprep_df.merge(df_tfidf, left_index=True, right_index=True,suffixes=('', '_y'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Univariate Analysis","metadata":{}},{"cell_type":"markdown","source":"## 4.1 Pandas Profiling","metadata":{}},{"cell_type":"markdown","source":"##### For more details on the pandas profiling library see https://github.com/pandas-profiling/pandas-profiling\n","metadata":{}},{"cell_type":"code","source":"#Generate report\n#If the database has many records or columns, the report can take a long time\n#If this is the case, disable the explorative, samples, correlations, missing_diagrams, duplicates and interactions options by commenting out\nprofile = ProfileReport(dataprep_df, title=f\"Pandas Profiling Report{ModelId}\"\n                        ,explorative=True\n                        #,samples=None\n                        #,correlations=None\n                        #,missing_diagrams=None\n                        #,duplicates=None\n                        #,interactions=None\n                       )\n# profile.to_file(\"profile.html\")\n# display(profile)","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.2 Classify the types of variables\n#### list all columns to select the ones to be used","metadata":{}},{"cell_type":"code","source":"# Get all the types pandas_profiling offers\nlist_columns = dataprep_df.columns.drop(['dataset',var_target, 'id', 'text', 'tidy_text'])\nd = {col: get_var_type(dataprep_df[col])['type'].value for col in list_columns}\nfd = defaultdict(list)\nfor k, v in d.items():\n    fd[v].append(k)\n     \ncols_by_base_type = dict(fd)\n# Group the types pandas_profiling offers to match typical needs\ncat_num_cols = defaultdict(list)\nfor k, v in cols_by_base_type.items():\n    # Treat boolean and unique columns as categorical\n    k = 'CAT' if k in ['BOOL', 'UNIQUE'] else k\n    cat_num_cols[k].extend(v)\ndict(cat_num_cols)","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### From the variables listed above you can select which  one will be tested in the model and confirm if the correct type is numeric(NUM) or categorical (CAT). Paste the correct information below:","metadata":{}},{"cell_type":"code","source":"#It is necessary to define the types of variables (cageroric and numeric) to ensure that the type of data used in the modeling will be the most suitable.\n#For example, categorical variables need to be defined as a string because this prevents it from being treated as a numeric variable in H20 modeling\n#Another example is that the string variables will have a missing treatment by placing the missing category for all values found as 'null'\nCAT = ['keyword', 'location']\n#float\nNUM = cat_num_cols['NUM']\nselected_features = CAT + NUM","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Numeric features must be float type\nfor col_name in NUM:    \n    dataprep_df[col_name] = dataprep_df[col_name].astype(float)    \n\n#Categorical features must be string type and null values will be filled with \"missing\"\nfor col_name in CAT:        \n    dataprep_df[col_name] = dataprep_df[col_name].astype(str)\n    dataprep_df = dataprep_df.fillna(value={col_name: 'missing'})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Bivariate Analysis","metadata":{}},{"cell_type":"markdown","source":"## 5.1 Calculation of the Information Value for all variables defined as selected features","metadata":{}},{"cell_type":"code","source":"def calculate_IV(dataframe, coluna_feature, coluna_target, cat_goods = None, buckets=20):\n    '''\n    Function to calculate the IV.\n         Parameters\n         dataframe: DataFrame with the input and target variables.\n         column_feature (str): Name of the variable that contains the independent variable.\n         column_target (str): Name of the variable that contains the dependent variable or target.\n         cat_goods (str): Level of the target variable that should be considered \"GOOD\", if it is categorical.\n         buckets (int): Number of partitions to be created in numeric variables.\n    \n    Returns\n    stats (list):\n    List with:\n        [1] IV\n        [0] dataframe pandas with statistics table\n    '''\n    \n    # Initial definitions\n    df = dataframe.loc[:,(coluna_feature, coluna_target)]\n    tpVar = 'categorical'    \n    \n    #If the variable is numeric (float or int), it creates a category for discretization\n    if df[coluna_feature].dtype=='float64' or df[coluna_feature].dtype=='int64' or df[coluna_feature].dtype=='int32' or df[coluna_feature].dtype=='float32':\n        tpVar='numeric'\n        coluna_feature_bucket = coluna_feature + \"_bucket\"        \n        #create buckets using qcut\n        df[coluna_feature_bucket] = pd.qcut(df[coluna_feature], buckets, labels=False, duplicates='drop')\n        analyse_df = df.groupby(coluna_feature_bucket).agg({coluna_target: ['count', 'sum'], coluna_feature: ['min', 'max']})\n        analyse_df.columns = ['_'.join(tup).rstrip('_') for tup in analyse_df.columns.values]\n        analyse_df.rename(columns={(coluna_target+'_count'):'qty', (coluna_target+'_sum'):'qty_goods'}, inplace=True)\n        \n    #Categorical variables\n    if tpVar == 'categorical':        \n        analyse_df = df.groupby(coluna_feature).agg({coluna_target: ['count', 'sum']})\n        analyse_df.columns = ['_'.join(tup).rstrip('_') for tup in analyse_df.columns.values]\n        analyse_df.rename(columns={(coluna_target+'_count'):'qty', (coluna_target+'_sum'):'qty_goods'}, inplace=True)\n        \n    #IV Calculation\n    analyse_df.loc[:, 'qty_bads'] = analyse_df.loc[:,'qty'] - analyse_df.loc[:,'qty_goods']\n    analyse_df.loc[:, 'tot_goods'] = analyse_df.loc[:,'qty_goods'].sum()\n    analyse_df.loc[:, 'tot_bads'] = analyse_df.loc[:,'qty_bads'].sum()\n    analyse_df.loc[:, 'perc_goods'] = analyse_df.loc[:,'qty_goods'] / analyse_df.loc[:,'tot_goods']\n    analyse_df.loc[:, 'perc_bads'] = analyse_df.loc[:,'qty_bads'] / analyse_df.loc[:,'tot_bads']\n    analyse_df.loc[:, 'good_rate'] = analyse_df.loc[:,'qty_goods'] / analyse_df.loc[:,'qty']\n    analyse_df.loc[:, 'odds'] = analyse_df.loc[:,'perc_goods'] / analyse_df.loc[:,'perc_bads']\n    analyse_df.loc[:, 'ln_odds'] = np.log2(analyse_df['odds'])\n    analyse_df.loc[:, 'iv_cat'] = (analyse_df.loc[:,'perc_goods'] / analyse_df.loc[:,'perc_bads']) * analyse_df.loc[:, 'ln_odds']\n    \n    if tpVar == 'numeric':\n        analyse_df.reset_index(inplace=True)\n        tabela_pdf = analyse_df.loc[:, (coluna_feature_bucket, coluna_feature+\"_min\", coluna_feature+\"_max\", 'qty', 'good_rate', 'odds', 'iv_cat')]\n    else:\n        analyse_df.reset_index(inplace=True)\n        tabela_pdf = analyse_df.loc[:, (coluna_feature, 'qty', 'good_rate', 'odds', 'iv_cat')]\n        \n    df_iv = tabela_pdf.query('iv_cat != inf')['iv_cat'].sum()                       \n    resultado = [df_iv, tabela_pdf]\n    return resultado\n\ndef colunas_dataframe(dataframe):\n    lista_colunas = []\n    lista_colunas = [i for i in dataframe.columns if i in selected_features]\n    return lista_colunas\n\ndef table_iv(dataframe):\n    lista_colunas = colunas_dataframe(dataframe)\n    dict_resultados = {}\n    for col in lista_colunas:\n        print(\"{0:.0%}\".format((lista_colunas.index(col)+1) / (len(lista_colunas)+1)) + \":\" + col)              \n        dict_resultados[col] = calculate_IV(dataframe=dataframe, coluna_feature=col, coluna_target=var_target, buckets=10)\n    return dict_resultados","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start = dt.datetime.now()\n\nresult_data = table_iv(dataprep_df)\nresult_formated = pd.DataFrame.from_dict(data=result_data, orient='index').reset_index().rename(columns={'index': 'Variable', 0: 'IV'}).drop(columns=1)\nresult_formated_graph = result_formated.sort_values(by=['IV'], ascending=False)\n                                                                                                                 \n#Execution time\nstop = dt.datetime.now()\nexecution_time = stop-start\nprint(\"\\n\"+\"Execution time: \" + str (execution_time)+\"\\n\")","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(10,10))\nsns.barplot(y=\"Variable\", x=\"IV\", data=result_formated_graph.head(40), palette=\"Blues_r\").set_title(\"Information Value (IV)\")\nplt.axvline (x=0.02, linestyle=\"--\", color='r')","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.2 Charts with the good rate (% of target = 1) by categories or by value range for numeric variables (ranges created by decile)","metadata":{}},{"cell_type":"code","source":"# for i in selected_features:\n#     df_plot_tmp = result_data[i][1]\n#     df_plot_tmp['Distribution'] = df_plot_tmp.qty / df_plot_tmp.qty.sum()\n#     if i in CAT:\n# #         print('--------------------------------------- ' + str(i))\n#         df_plot_tmp = df_plot_tmp.sort_values(by=i)\n#         df_plot_t1 = df_plot_tmp.loc[:, (i,'Distribution')]\n#         df_plot_t1 = df_plot_t1.set_index(i)\n#         df_plot_t2 = df_plot_tmp.loc[:, (i, 'good_rate')]\n#         df_plot_t2 = df_plot_t2.set_index(i)\n#         df_plot_t1.Distribution.plot(ylim=[0,1], kind='bar', rot=90, figsize=(15,5), linewidth=2, fontsize=12, grid=True, legend=1, title=i)\n#         ax = df_plot_t2.good_rate.plot(secondary_y=True, kind=\"line\", rot=90, figsize=(15,5), linewidth=2, fontsize=12\\\n#                                        , marker=\"D\", ms=8, grid=True, color='r', legend=1)\n#         for p in range(len(df_plot_t2)):\n#             ax.annotate(str('{0:.1%}'.format(int(df_plot_t2.reset_index().iloc[p,1]*1000)/1000))\\\n#                             ,(df_plot_t2.reset_index().index.values[p]\\\n#                               ,df_plot_t2.reset_index().iloc[p,1]*1))\n#         plt.show()\n#         #display(ax)\n#     else:\n#         sort_var = str(i)+\"_max\"\n# #         print('--------------------------------------- ' + str(i))\n#         df_plot_tmp[sort_var] = df_plot_tmp[sort_var].astype(float)\n#         df_plot_tmp = df_plot_tmp.sort_values(by=sort_var)\n#         df_plot_tmp[i] = df_plot_tmp[sort_var].fillna(999999999.99).astype(float)\n#         df_plot_tmp[i] = (df_plot_tmp[i]*100).astype(int)/100\n#         df_plot_tmp[i] = df_plot_tmp[i].astype(str).replace(\"999999999.99\", \"missing\")\n#         df_plot_t1 = df_plot_tmp.loc[:, (i, 'Distribution')]\n#         df_plot_t1 = df_plot_t1.set_index(i)\n#         df_plot_t2 = df_plot_tmp.loc[:, (i, 'good_rate')]\n#         df_plot_t2 = df_plot_t2.set_index(i)\n#         df_plot_t1.Distribution.plot(ylim=[0,1], kind='bar', rot=90, figsize=(15,5), linewidth=2, fontsize=12, grid=True, legend=1, title=(i + \": ranges by decile\"))\n#         ax = df_plot_t2.good_rate.plot(secondary_y=True, kind=\"line\", rot=90, figsize=(15,5), linewidth=2, fontsize=12, marker=\"D\", ms=8, grid=True, color='r', legend=1)\n#         for p in range(len(df_plot_t2)):\n#             ax.annotate(str('{0:.1%}'.format(int(df_plot_t2.reset_index().iloc[p,1]*1000)/1000)), (df_plot_t2.reset_index().index.values[p], df_plot_t2.reset_index().iloc[p,1]*1))        \n#         plt.show()\n#         #display(ax)","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Modeling","metadata":{}},{"cell_type":"markdown","source":"## 6.1 Creating context and H2O and Importing data into the H2O context","metadata":{}},{"cell_type":"code","source":"# Number of threads, nthreads = -1, means use all cores on your machine\n# max_mem_size is the maximum memory (in GB) to allocate to H2O\nh2o.init(nthreads = -1, max_mem_size = 8)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Import TRAINING base to the H20 context\ndata_hdf = h2o.H2OFrame(dataprep_df.query('dataset == \"train\"'))\n\n# Conversion of Target variables and categorical features to factor (enum)\n#no H2O it is necessary that the categorical variables are transformed into a factor\ndata_hdf[var_target] = data_hdf[var_target].asfactor()\nfor col_name in CAT:\n    data_hdf[col_name] = data_hdf[col_name].asfactor()    \n    \n# Partition data into 90%, 10% chunks\n# Setting a seed will guarantee reproducibility\ntrain_hdf, valid_hdf = data_hdf.split_frame(ratios=[0.90], destination_frames=['train_hdf', 'valid_hdf'], seed=1)\n        \n#Notice that `split_frame()` uses approximate splitting not exact splitting (for efficiency), so these are not exactly 90%, 10% of the total rows.\nprint('Training: ' + str(train_hdf.nrow))\nprint('Validation: ' + str(valid_hdf.nrow))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Import TEST base to the H20 context\ntest_hdf = h2o.H2OFrame(dataprep_df.query('dataset == \"test\"'))\n\n# Conversion of Target variables and categorical features to factor (enum)\n#no H2O it is necessary that the categorical variables are transformed into a factor\ntest_hdf[var_target] = test_hdf[var_target].asfactor()\nfor col_name in CAT:\n    test_hdf[col_name] = test_hdf[col_name].asfactor()    \n    \nprint('Training: ' + str(test_hdf.nrow))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.2 Using H2O to performe many ML algorithms","metadata":{}},{"cell_type":"markdown","source":"## Logistic Regresion (GLM)","metadata":{}},{"cell_type":"code","source":"vModel = 'GLM_'\n\nstart = dt.datetime.now()\nfrom h2o.estimators.glm import H2OGeneralizedLinearEstimator\n\n#definir parámetros\nGLM = H2OGeneralizedLinearEstimator(family= 'binomial',\n                                    seed=1,\n                                    model_id='%s%s%s' % (vModel, ModelId, str(dt.datetime.now())[:19].replace('-',\"\").replace(':',\"\").replace(' ',\"_\")))\n\n#Executar Modelo\nGLM.train(x = selected_features,\n          y = var_target,\n          training_frame = train_hdf,\n          validation_frame = valid_hdf)\n\n#Execution time of the model\nstop = dt.datetime.now()\nexecution_time = stop-start\nprint(\"\\n\"+ \"Execution time: \" + str(execution_time) +\"\\n\")\nprint(GLM)","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## GBM - Gradient Boosting Machine","metadata":{"jupyter":{"outputs_hidden":true}}},{"cell_type":"code","source":"vModel='GBM_'\n\n#Execution time of the model\nstart = dt.datetime.now()\n\nfrom h2o.estimators.gbm import H2OGradientBoostingEstimator\nGBM = H2OGradientBoostingEstimator(model_id='%s%s%s' % (vModel, ModelId, str(dt.datetime.now())[:19].replace('-',\"\").replace(':',\"\").replace(' ',\"_\")),\n                                   ntrees=500,\n                                   score_tree_interval=5,     #used for early stopping\n                                   stopping_rounds=3,         #used for early stopping\n                                   stopping_metric='AUCPR',     #used for early stopping\n                                   stopping_tolerance=0.0005, #used for early stopping\n                                   seed=1)\n\n# The use of a validation_frame is recommended with using early stopping\nGBM.train(x=selected_features, y=var_target, training_frame=train_hdf, validation_frame=valid_hdf)\n\n#Execution time of the model\nstop = dt.datetime.now()\nexecution_time = stop-start\nprint(\"\\n\"+ \"Execution time: \" + str(execution_time) + \"\\n\")\nprint(GBM)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## GBM - Gradient Boosting Machine with Cross-Validation","metadata":{"jupyter":{"outputs_hidden":true}}},{"cell_type":"code","source":"vModel='GBM_cv_'\n\n#Execution time of the model\nstart = dt.datetime.now()\n\nfrom h2o.estimators.gbm import H2OGradientBoostingEstimator\nGBM_cv = H2OGradientBoostingEstimator(model_id='%s%s%s' % (vModel, ModelId, str(dt.datetime.now())[:19].replace('-',\"\").replace(':',\"\").replace(' ',\"_\"))\n                                   ,nfolds=5\n                                   ,seed=1)\n\n# The use of a validation_frame is recommended with using early stopping\nGBM_cv.train(x=selected_features, y=var_target, training_frame=train_hdf, validation_frame=valid_hdf)\n\n#Execution time of the model\nstop = dt.datetime.now()\nexecution_time = stop-start\nprint(\"\\n\"+ \"Execution time: \" + str(execution_time) + \"\\n\")\nprint(GBM_cv)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Random Forest","metadata":{}},{"cell_type":"code","source":"vModel='DRF_CV_'\n\n#Execution time of the model\nstart = dt.datetime.now()\n\nfrom h2o.estimators.random_forest import H2ORandomForestEstimator\n\nDRF = H2ORandomForestEstimator(seed=1\n                               ,nfolds=5\n                               ,model_id='%s%s%s' % (vModel, ModelId, str(dt.datetime.now())[:19].replace('-',\"\").replace(':',\"\").replace(' ',\"_\")))\n\n# The use of a validation_frame is recommended with using early stopping\nDRF.train(x=selected_features, y=var_target, training_frame=train_hdf, validation_frame=valid_hdf)\n\n#Execution time of the model\nstop = dt.datetime.now()\nexecution_time = stop-start\nprint(\"\\n\"+ \"Execution time: \" + str(execution_time) + \"\\n\")\nprint(DRF)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## H2OAutoML","metadata":{}},{"cell_type":"code","source":"vModel='AUTOML'\n\n#Execution time of the model\nstart = dt.datetime.now()\n\n#Set the maximum time in seconds for the H20 AutoML\nmax_runtime_secs=60*10\n\n#Define metrics to select the best model in AutoML\nsort_metric = 'AUCPR'\n\nfrom h2o.automl import H2OAutoML\nAUTOML = H2OAutoML(seed=1,\n                   include_algos = ['DRF', 'GLM', 'XGBoost', 'GBM', 'DeepLearning', 'StackedEnsemble'],\n                   max_runtime_secs = max_runtime_secs,\n                   stopping_metric = sort_metric,\n                   sort_metric = sort_metric)\nAUTOML.train(x=selected_features, y=var_target, training_frame = train_hdf, validation_frame = valid_hdf, leaderboard_frame=test_hdf)\n\n#View the AutoML Leaderboard\nlb = AUTOML.leaderboard\nprint(lb.head(rows=lb.nrows))\n\n#Execution time of the model\nstop = dt.datetime.now()\nexecution_time = stop-start\nprint(\"\\n\"+ \"Execution time: \" + str(execution_time) + \"\\n\")","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Choose the desired AutoML model\nbest_automl_position=0\nif len(AUTOML.leaderboard) > 0:\n    best_AutoML = h2o.get_model(AUTOML.leaderboard[best_automl_position, 0])\n    print(best_AutoML)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.3 Compare performance on the TEST dataset for all trained models","metadata":{}},{"cell_type":"code","source":"#Create empty model list\nlist_models = []\n\n#Define the list of all models that have been executed and should be compared\ntry:\n    list_models.append(GLM)\nexcept NameError:\n    GLM = None\ntry:\n    list_models.append(GBM)\nexcept NameError:\n    GBM = None\ntry:\n    list_models.append(GBM_cv)\nexcept NameError:\n    GBM_cv = None\ntry:\n    list_models.append(DRF)\nexcept NameError:\n    DRF = None\ntry:\n    list_models.append(best_AutoML)\nexcept NameError:\n    best_AutoML = None","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Compare performance on the TEST dataset for all trained models\nplt.rcParams.update({'font.size': 12})\nfig = plt.figure(figsize=(10, 10))\nfor i in list_models:\n    #Save all models in H20 format\n    h2o.save_model(model=i, path='%s/models/todos/' % OutputPath, force=True)\n    #calculate o KS in test dataset\n    h2o_predict = i.predict(test_hdf)\n    data = h2o_predict.cbind(test_hdf[var_target]).as_data_frame()\n    data['target0'] = 1 - data[var_target]\n    data['bucket'] = pd.qcut(data['p1'], 10, duplicates='drop', labels=False)\n    grouped = data.groupby('bucket', as_index=False)\n    kstable = pd.DataFrame()\n    kstable['min_prob'] = grouped.min()['p1']\n    kstable['max_prob'] = grouped.max()['p1']\n    kstable['events'] = grouped.sum()[var_target]\n    kstable['nonevents'] = grouped.sum()['target0']    \n    kstable = kstable.sort_values(by=\"min_prob\", ascending=False).reset_index(drop=True)\n    kstable['event_rate'] = (kstable.events / data[var_target].sum()).apply('{0:.2%}'.format)\n    kstable['nonevent_rate'] = (kstable.nonevents / data['target0'].sum()).apply('{0:.2%}'.format)\n    kstable['cum_eventrate'] = (kstable.events / data[var_target].sum()).cumsum()\n    kstable['cum_noneventrate'] = (kstable.nonevents / data['target0'].sum()).cumsum()\n    kstable['KS'] = np.round(kstable['cum_eventrate'] - kstable['cum_noneventrate'], 3)\n    ks = kstable['KS'].max()\n    \n    #Ascertain the performance of all models on the test base\n    performance = i.model_performance(test_hdf)\n    \n    #Salve metrics\n    f=open(\"%s/models/todos/performance_%s.csv\" % (OutputPath, i.model_id), 'w')\n    f.write(\n        str(i.model_id) + \";\"\n        + str(performance.accuracy()[0][0]) + \";\"\n        + str(performance.auc()) + ';'\n        + str(performance.aucpr()) + ';'\n        + str(performance.logloss()) + ';'\n        + str(ks) + ';'\n        + str(performance.F1()[0][0]))\n    f.write('\\n')\n    f.close()\n\n    #graph with the ROC curve of all models\n    fpr = performance.fprs\n    tpr = performance.tprs\n    plt.plot(fpr, tpr, lw=2, label=i.model_id.split(\"_\")[0]+\"_\"+i.model_id.split(\"_\")[1]+\"_\"+i.model_id.split(\"_\")[2])\n    plt.title(\"ROC Curve for Model (Test dataset)\")    \n    \n    if i.model_id==list_models[0].model_id:\n        df_plot = pd.DataFrame({'Model_id': i.model_id.split(\"_\")[0]+\"_\"+i.model_id.split(\"_\")[1]+\"_\"+i.model_id.split(\"_\")[2],\n                                    'AUROC': int(performance.auc()*100)/100,\n                                    'AUCPR': int(performance.aucpr()*100)/100,\n                                    'KS': int(ks*100)/100\n                                    }, index=[0])\n    else:\n        df_plot = df_plot.append(pd.DataFrame({'Model_id': i.model_id.split(\"_\")[0]+\"_\"+i.model_id.split(\"_\")[1]+\"_\"+i.model_id.split(\"_\")[2],\n                                    'AUROC': int(performance.auc()*100)/100,\n                                    'AUCPR': int(performance.aucpr()*100)/100,\n                                    'KS': int(ks*100)/100\n                                    }, index=[0]))\nplt.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--')\nplt.xlim([0.0, 1.05])\nplt.ylim([0.0, 1.05])\nplt.legend(loc=\"lower right\")\nplt.show()\nplt.close()\nax = df_plot.plot(kind='bar', x=\"Model_id\", title=\"AUROC, AUCPR e KS for Model (Test dataset)\", grid=True, figsize=(10,5), legend=1)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.005, p.get_height() * 1.005))\nplt.legend(loc=3, prop={'size': 10})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.4 Choose the best model among all tested","metadata":{}},{"cell_type":"code","source":"#Consider all models in the history ./models/todos/performance_*.csv. To disregard any old version, set erase_modelos = \"S\":\napagar_modelos = 'N'\nif apagar_modelos == 'S':\n    os.system('rm %s/models/todos/performance_*.csv' % OutputPath)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sort_metric_best_model='AUCPR'\n#importar todos os modelos testados e imprmie na tela os 10 melhores erdedando per AUC\nmodelos_testados = pd.concat(map(functools.partial(pd.read_csv, sep=';', header=None), glob.glob('%s/models/todos/performance_*.csv' % OutputPath)))\nmodelos_testados.columns = ('model_id', 'accuracy', 'AUC', 'AUCPR', 'logloss', 'KS', 'F1')\nmodelos_testados = modelos_testados.sort_values(by=sort_metric_best_model, ascending=False)\nmodelos_testados = modelos_testados.drop_duplicates(subset=[\"model_id\"])\nprint('MBest Models. Sorted by : ' + str(sort_metric_best_model))\nmodelos_testados.reset_index(0).head(30)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#If you want to choose a model other than the first one on the list. Choose the position number:\nposicao_melhor_modelo=0\n\nmelhor_modelo = h2o.load_model('%s/models/todos/%s' % (OutputPath, modelos_testados.iloc[posicao_melhor_modelo, 0]))\n(print(\"\\n\"+ \"BEST MODEL: \" + str(modelos_testados.iloc[posicao_melhor_modelo, 0]) + \"\\n\"))\n\nplt.rcParams.update({'font.size': 10})\ntry:\n    melhor_modelo.varimp_plot(50)\nexcept Exception as e:\n    print(\"Warning: This model doesn't have variable importances\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.4 Stepwise for Analysis of the importance of variables","metadata":{}},{"cell_type":"code","source":"#Listar todas as variáveis do modelo atual, ordenadas por variable importance\n#Para as variaveis definidas como fator (que possivelmente estão como dummys), remover a categoria do nome e deixar apenas o nome orifinal da variavel\n\n#List all variables in the current model, ordered by variable importance\n#For variables defined as a factor (which possibly are like dummys), remove the category from the name and leave only the orifinal name of the variable\ntry:\n    df_features_sorted = melhor_modelo.varimp(True).variable.str.split('.', expand=True).drop_duplicates(subset = 0)[0].reset_index(drop=True)\nexcept Exception as e:\n    #As the model with ensemble in H20 does not show the importance of variables, we will include variables with higher IV first using result_formatado graph of step 5.1\n    df_features_sorted = result_formated_graph.Variable.reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Define the number of variables to be increased with each new model. Try to put 10% or 20% of the total, as it can take a long time\nqt_var=100\nqt_total_var = len(df_features_sorted)\n\ndict_model_tmp={}\ndict_performance={}\n\nfor i in range(qt_var, qt_total_var+qt_var, qt_var):    \n    df_features_sorted[0:i].values.tolist()    \n    \n    #If no model chosen is not an ensemble of models. Then use the same model for training with increment of variables\n    melhor_modelo_tmp = melhor_modelo\n    if melhor_modelo_tmp.model_id.lower().find(\"ensemble\") == -1:\n        dict_model_tmp[i] = melhor_modelo_tmp\n        dict_model_tmp[i].train(x = df_features_sorted[0:i].values.tolist(),\n                                y = var_target,\n                                training_frame=train_hdf, \n                                validation_frame=valid_hdf)\n    ##If it is not possible, for the home of an ensemble of models, use GradientBoostingEstimator to make the assessment\n    else:\n        dict_model_tmp[i] = H2OGradientBoostingEstimator(seed=1, model_id=str('model_tmp_%s' % i))\n        dict_model_tmp[i].train(x = df_features_sorted[0:i].values.tolist(),\n                                y = var_target,\n                                training_frame=train_hdf, \n                                validation_frame=valid_hdf)       \n\n\n    perform_oot = dict_model_tmp[i].model_performance(test_hdf)\n    dict_performance_tmp = {}\n    dict_performance_tmp['AUC'] = {'qt_var': i, 'medida': 'AUC', 'Validation_Dataset': dict_model_tmp[i].auc(valid=True), 'Test_Dataset': perform_oot.auc()}\n    dict_performance_tmp['accuracy'] = {'qt_var': i, 'medida': 'accuracy', 'Validation_Dataset': dict_model_tmp[i].accuracy(valid=True)[0][0], 'Test_Dataset': perform_oot.accuracy()[0][0]}\n    dict_performance_tmp['AUCPR'] = {'qt_var': i, 'medida': 'AUCPR', 'Validation_Dataset': dict_model_tmp[i].aucpr(valid=True), 'Test_Dataset': perform_oot.aucpr()}\n    dict_performance_tmp['F1'] = {'qt_var': i, 'medida': 'F1', 'Validation_Dataset': dict_model_tmp[i].F1(valid=True)[0][0], 'Test_Dataset': perform_oot.F1()[0][0]}\n    dict_performance_tmp['logloss'] = {'qt_var': i, 'medida': 'logloss', 'Validation_Dataset': dict_model_tmp[i].logloss(valid=True), 'Test_Dataset': perform_oot.logloss()}\n    dict_performance[i] = pd.DataFrame(dict_performance_tmp).transpose()","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Plot graph comparing the increase in performance with the increase in variables\nfor i in dict_performance.keys():\n    if i == list(dict_performance.keys())[0]:\n        df_performance = dict_performance[i]\n    else:\n        df_performance = df_performance.append(dict_performance[i], ignore_index=True)\n\nlista_metricas_perf = df_performance['medida'].unique()\n\nfor i in range(len(lista_metricas_perf)):   \n    #selects only the metric to be analyzed\n    metrics_df_tmp = df_performance.query('medida == \"%s\"' % lista_metricas_perf[i])\n    metrics_df_tmp = metrics_df_tmp.set_index('qt_var')\n    del metrics_df_tmp['medida']\n    if lista_metricas_perf[i] == 'logloss':\n        max_oot = metrics_df_tmp[metrics_df_tmp['Test_Dataset'] == metrics_df_tmp.Test_Dataset.min()].index.values\n    else:\n        max_oot = metrics_df_tmp[metrics_df_tmp['Test_Dataset'] == metrics_df_tmp.Test_Dataset.max()].index.values\n        \n    if lista_metricas_perf[i] == sort_metric_best_model:\n        max_oot_filtro = max_oot[0]        \n    \n    ax=metrics_df_tmp.plot(figsize=(15,5), linewidth=2, fontsize=10, marker='D', ms=5,\\\n                            title='Best %s with %s Variables' % (lista_metricas_perf[i].upper(), str(max_oot[0])))\n    plt.xlabel('Variables Number')\n    plt.ylabel('%s' % lista_metricas_perf[i].upper())\n    plt.grid(axis='y')\n    plt.legend(loc=0, prop={'size': 12})\n    #display(ax)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Consider removing the following variables: '+ str(df_features_sorted[df_features_sorted.index > int(max_oot_filtro)].values.tolist()))","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.5 Exporting the best model to Deploy","metadata":{}},{"cell_type":"code","source":"#Save the H2O model in MOJO format and all the variables of the best model\nmelhor_modelo = h2o.load_model('%s/models/todos/%s' % (OutputPath, modelos_testados.iloc[posicao_melhor_modelo, 0]))\ncaminho_modelo_mojo = melhor_modelo.download_mojo('%s/models/melhores/' % OutputPath, get_genmodel_jar=True)\nprint(caminho_modelo_mojo)\ncaminho_modelo_h2o = h2o.save_model(model=melhor_modelo, path='%s/models/melhores/' % OutputPath, force=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    features_names= melhor_modelo.varimp(True)\n    features_names.to_csv('%s/models/melhores/features_names_%s.csv' % (OutputPath, melhor_modelo.model_id), sep=';')\nexcept Exception as e:\n    print(\"Warning: This model doesn't have variable importances\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. Calculate Shapley Values using SHAP KernelExplainer for H20 models","metadata":{}},{"cell_type":"markdown","source":"#### The SHAP library calculates the Shaley Value of each variable used in the model and shows the individual impact of each variable on the predicted value for each record. To better understand how the SHAP library works, see the link https://github.com/slundberg/shap","metadata":{}},{"cell_type":"code","source":"class H2oProbWrapper:\n    def __init__(self, h2o_model, feature_names):\n        self.h2o_model = h2o_model\n        self.feature_names = feature_names\n    def predict_binary_prob(self, X):\n        if isinstance(X, pd.Series):\n            X = X.values.reshape(1,-1) \n        self.dataframe = pd.DataFrame(X, columns=self.feature_names)\n        \n        global NUM\n        #Variaveis explicativas continuas\n        for col_name in NUM:    \n            self.dataframe[col_name] = self.dataframe[col_name].astype(float)\n            \n        global CAT\n        for col_name in CAT:    \n            self.dataframe[col_name] = self.dataframe[col_name].astype(str)\n            self.dataframe = self.dataframe.fillna(value={col_name: 'missing'})\n        \n        self.h2oframe = h2o.H2OFrame(self.dataframe)\n        for col_name in CAT:\n            self.h2oframe[col_name] = self.h2oframe[col_name].asfactor()\n        \n        self.predictions = self.h2o_model.predict(self.h2oframe).as_data_frame().values\n        return self.predictions.astype('float64') [:,-1]","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#The calculation of the Shapley Value for H20 models takes a while. So it will only be done for 20 records. Increase the sample to deepen your analysis\nshap_sample = dataprep_df.query('dataset == \"test\"').loc[:,(selected_features)].sample(n=20, replace=False, random_state=1)\nshap_sample = shap_sample.fillna(0)","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"h2o_wrapper = H2oProbWrapper(melhor_modelo, selected_features)\nh2o_explainer = shap.KernelExplainer(h2o_wrapper.predict_binary_prob, shap_sample)\nh2o_shap_values = h2o_explainer.shap_values(shap_sample, nsamples=\"auto\")","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Main SHAP Graphics","metadata":{}},{"cell_type":"code","source":"fig = shap.summary_plot(h2o_shap_values, shap_sample, plot_type=\"bar\", show=True)\ndisplay(fig)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(shap.summary_plot(h2o_shap_values, shap_sample, show=False))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#sort the features indexes by their importance in the model\n#(sum of SHAP value magnitudes over the validation dataset)\ntop_inds = np.argsort(-np.sum(np.abs(h2o_shap_values),0))\n\n#make SHAP plots of the three most important features\nfor i in range(9):\n    fig=shap.dependence_plot(top_inds[i], h2o_shap_values, shap_sample, show=False)\n#     display(fig)","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_shap_values = pd.DataFrame(h2o_shap_values)\ndf_shap_values['sum_shap'] = df_shap_values.sum(axis=1)","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Shap Force Plot","metadata":{}},{"cell_type":"code","source":"for i in df_shap_values.sort_values(by='sum_shap').iloc[0:3,:].index.values:\n    fig = shap.force_plot(h2o_explainer.expected_value, h2o_shap_values[i,:], shap_sample.iloc[i,:], matplotlib=True, show=True)\n    display(fig)","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Shap Waterfall Plot","metadata":{}},{"cell_type":"code","source":"for i in df_shap_values.sort_values(by='sum_shap').iloc[0:3,:].index.values:\n    fig = shap.plots._waterfall.waterfall_legacy(h2o_explainer.expected_value, h2o_shap_values[i,:], shap_sample.iloc[i,:].to_numpy(), selected_features, show=True)\n    display(fig)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 8. Predict test dataset using MOJO or H2O Model","metadata":{"jupyter":{"outputs_hidden":true}}},{"cell_type":"code","source":"#Import test dataset\nsubmission_df = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Text preprocessing\n\n#Removing Twitter handles (@user)\nsubmission_df['tidy_text'] = np.vectorize(remove_pattern)(submission_df['text'],'@[/w]*')\n\n#Removing Punctuations, Numbers, and Special Characters\nsubmission_df['tidy_text'] = submission_df['tidy_text'].str.replace('[^a-zA-Z#]',' ')\n\n#Removing Short Words\nsubmission_df['tidy_text'] = submission_df['tidy_text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))\n\n#Tokenizing\ntokenized_text = submission_df['tidy_text'].apply(lambda x: x.split())\n\n#Stemming\nfrom nltk.stem.porter import *\nstemmer = PorterStemmer()\ntokenized_text = tokenized_text.apply(lambda x: [stemmer.stem(i) for i in x])\n\nfor i in range (len(tokenized_text)):\n    tokenized_text[i] = ' '.join(tokenized_text[i])\nsubmission_df['tidy_text'] = tokenized_text\n\n#TF-IDF\ntfidf = tfidf_vectorizer.fit_transform(submission_df['tidy_text'])\n\n#Set tfidf feature names\ndf_tfidf = pd.DataFrame(tfidf.todense())\ndf_tfidf.columns =[tfidf_vectorizer.get_feature_names()]\ndf_tfidf.columns =  df_tfidf.columns.get_level_values(0)\n\nsubmission_df = submission_df.merge(df_tfidf, left_index=True, right_index=True,suffixes=('', '_y'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Numeric features must be float type\nfor col_name in NUM:\n    if col_name in submission_df.columns:\n        submission_df[col_name] = submission_df[col_name].astype(float)\n\n#Categorical features must be string type and null values will be filled with \"missing\"\nfor col_name in CAT:\n    if col_name in submission_df.columns:\n        submission_df[col_name] = submission_df[col_name].astype(str)\n        submission_df = submission_df.fillna(value={col_name: 'missing'})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Importar MOJO\ntry:\n    test_tmp = h2o.mojo_predict_pandas(submission_df, caminho_modelo_mojo)    \n    predict_df = submission_df.merge(test_tmp, left_index=True, right_index=True)\nexcept:    \n    submission_hdf = h2o.H2OFrame(submission_df)\n    for col_name in CAT:\n        submission_hdf[col_name] = submission_hdf[col_name].asfactor() \n    h2o_predict = melhor_modelo.predict(submission_hdf)\n    predict_df = h2o_predict.cbind(submission_hdf).as_data_frame()\n\npredict_df = predict_df.drop(columns=['target'])\npredict_df.rename(columns={'predict':'target'}, inplace=True)\npredict_df.loc[:, ('id', 'target')]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 9. Save final dataset with predictions","metadata":{"jupyter":{"outputs_hidden":true}}},{"cell_type":"code","source":"predict_df.loc[:, ('id', 'target')].to_csv('/kaggle/working/disaster_tweets_submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## FIM","metadata":{"jupyter":{"outputs_hidden":true}}}]}