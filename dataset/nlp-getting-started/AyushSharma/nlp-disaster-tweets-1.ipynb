{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import re\nimport tensorflow as tf\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nfrom sklearn.model_selection import KFold\n\nimport matplotlib.pyplot as plt\n\nprint(tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_pd = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest_pd = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_pd.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"no_of_rows, no_of_cols = train_pd.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By just doing a .head on train_pd we can see that location and keyword has NaN, that means we need to check how many rows have NaN for keyword and location to determine how good of a feature these two are. Let's do that below"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"keyword column analysis\")\nprint(\"-\" * 30)\nno_na_keywords, _ = train_pd[train_pd['keyword'].isna()].shape\nprint(f\"number of rows w/ value: NaN: {no_na_keywords} i.e {(no_na_keywords/no_of_rows)*100}%\")\nprint(f\"number of rows w/0 value: NaN: {no_of_rows - no_na_keywords} i.e {((no_of_rows-no_na_keywords)/no_of_rows)*100}%\")\n\nprint()\nprint('*' * 80)\nprint()\n\nprint(\"location column analysis\")\nprint(\"-\" * 30)\nno_na_location, _ = train_pd[train_pd['location'].isna()].shape\nprint(f\"number of rows w/ value: NaN: {no_na_location} i.e {(no_na_location/no_of_rows)*100}%\")\nprint(f\"number of rows w/0 value: NaN: {no_of_rows - no_na_location} i.e. {((no_of_rows-no_na_location)/no_of_rows)*100}%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With above analysis we can infer two things:\n1. it is okay to ignore the rows with keyword as NaN as number of such rows are really less i.e. < 1%\n2. location data is missing from 1/3rd of the data. Given this is location data, filling up the missing values might not make sense. We can still explore though from where the majority of the tweets are coming (let's say that place is locationX ) from but still assuming 33% of the data defaulting to locationX also IMO would be wrong.\n\nSo let's drop `location` as a feature for now and use only keywords and text data as features.\n\nNext steps:\n* drop the rows where the keywords are missing\n* clean text data\n* prepare a model using Embedding and bi-directional LSTM layers"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_pd = train_pd.drop(['location'], axis=1)\ntrain_pd = train_pd[train_pd['keyword'].notna()]\ntrain_pd = train_pd.reset_index(drop=True)\nprint(train_pd.head())\nprint(f\"Number of rows: {len(train_pd)}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test_pd.head())\nprint(f\"Number of rows: {len(test_pd)}\")\ntest_pd[test_pd['keyword'].isna()].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pd = test_pd.drop(['location'], axis=1)\ntest_pd = test_pd.fillna(value={'keyword': ''})\n\nprint(test_pd.head())\nprint(f\"Number of rows: {len(test_pd)}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eng_stopwords = set(stopwords.words('english'))\nstemmer = PorterStemmer()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Cleaning the data\n\nQuite some part of clean up being done in the function below has been taken from\nhttps://www.kaggle.com/gunesevitan/nlp-with-disaster-tweets-eda-full-cleaning/data, so would like to thank the notebook creator **gunesevitan**"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef parse_tweet(tweet):\n    tweet = tweet.lstrip().rstrip()\n    \n    # remove the links from the tweet (thanks to )\n    tweet = re.sub(\"http:\\S+\", \"\", tweet)\n    tweet = re.sub(\"https:\\S+\", \"\", tweet)\n    tweet = re.sub(r\"[,.;#?!&$:]+\\ *\", \" \", tweet)\n    tweet = tweet.replace(\"  \", \" \")\n    tweet = tweet.lower()\n    \n    tweet = tweet.replace('...', ' ... ').strip()\n    tweet = tweet.replace(\"'\", \" ' \").strip()        \n    \n    # Special characters\n    tweet = re.sub(r\"\\x89Û_\", \"\", tweet)\n    tweet = re.sub(r\"\\x89ÛÒ\", \"\", tweet)\n    tweet = re.sub(r\"\\x89ÛÓ\", \"\", tweet)\n    tweet = re.sub(r\"\\x89ÛÏWhen\", \"When\", tweet)\n    tweet = re.sub(r\"\\x89ÛÏ\", \"\", tweet)\n    tweet = re.sub(r\"China\\x89Ûªs\", \"China's\", tweet)\n    tweet = re.sub(r\"let\\x89Ûªs\", \"let's\", tweet)\n    tweet = re.sub(r\"\\x89Û÷\", \"\", tweet)\n    tweet = re.sub(r\"\\x89Ûª\", \"\", tweet)\n    tweet = re.sub(r\"\\x89Û\\x9d\", \"\", tweet)\n    tweet = re.sub(r\"å_\", \"\", tweet)\n    tweet = re.sub(r\"\\x89Û¢\", \"\", tweet)\n    tweet = re.sub(r\"\\x89Û¢åÊ\", \"\", tweet)\n    tweet = re.sub(r\"fromåÊwounds\", \"from wounds\", tweet)\n    tweet = re.sub(r\"åÊ\", \"\", tweet)\n    tweet = re.sub(r\"åÈ\", \"\", tweet)\n    tweet = re.sub(r\"JapÌ_n\", \"Japan\", tweet)    \n    tweet = re.sub(r\"Ì©\", \"e\", tweet)\n    tweet = re.sub(r\"å¨\", \"\", tweet)\n    tweet = re.sub(r\"SuruÌ¤\", \"Suruc\", tweet)\n    \n    # Contractions\n    tweet = re.sub(r\"he's\", \"he is\", tweet)\n    tweet = re.sub(r\"there's\", \"there is\", tweet)\n    tweet = re.sub(r\"We're\", \"We are\", tweet)\n    tweet = re.sub(r\"That's\", \"That is\", tweet)\n    tweet = re.sub(r\"won't\", \"will not\", tweet)\n    tweet = re.sub(r\"they're\", \"they are\", tweet)\n    tweet = re.sub(r\"Can't\", \"Cannot\", tweet)\n    tweet = re.sub(r\"wasn't\", \"was not\", tweet)\n    tweet = re.sub(r\"don\\x89Ûªt\", \"do not\", tweet)\n    tweet = re.sub(r\"aren't\", \"are not\", tweet)\n    tweet = re.sub(r\"isn't\", \"is not\", tweet)\n    tweet = re.sub(r\"What's\", \"What is\", tweet)\n    tweet = re.sub(r\"haven't\", \"have not\", tweet)\n    tweet = re.sub(r\"hasn't\", \"has not\", tweet)\n    tweet = re.sub(r\"There's\", \"There is\", tweet)\n    tweet = re.sub(r\"He's\", \"He is\", tweet)\n    tweet = re.sub(r\"It's\", \"It is\", tweet)\n    tweet = re.sub(r\"You're\", \"You are\", tweet)\n    tweet = re.sub(r\"I'M\", \"I am\", tweet)\n    tweet = re.sub(r\"shouldn't\", \"should not\", tweet)\n    tweet = re.sub(r\"wouldn't\", \"would not\", tweet)\n    tweet = re.sub(r\"i'm\", \"I am\", tweet)\n    tweet = re.sub(r\"I\\x89Ûªm\", \"I am\", tweet)\n    tweet = re.sub(r\"I'm\", \"I am\", tweet)\n    tweet = re.sub(r\"Isn't\", \"is not\", tweet)\n    tweet = re.sub(r\"Here's\", \"Here is\", tweet)\n    tweet = re.sub(r\"you've\", \"you have\", tweet)\n    tweet = re.sub(r\"you\\x89Ûªve\", \"you have\", tweet)\n    tweet = re.sub(r\"we're\", \"we are\", tweet)\n    tweet = re.sub(r\"what's\", \"what is\", tweet)\n    tweet = re.sub(r\"couldn't\", \"could not\", tweet)\n    tweet = re.sub(r\"we've\", \"we have\", tweet)\n    tweet = re.sub(r\"it\\x89Ûªs\", \"it is\", tweet)\n    tweet = re.sub(r\"doesn\\x89Ûªt\", \"does not\", tweet)\n    tweet = re.sub(r\"It\\x89Ûªs\", \"It is\", tweet)\n    tweet = re.sub(r\"Here\\x89Ûªs\", \"Here is\", tweet)\n    tweet = re.sub(r\"who's\", \"who is\", tweet)\n    tweet = re.sub(r\"I\\x89Ûªve\", \"I have\", tweet)\n    tweet = re.sub(r\"y'all\", \"you all\", tweet)\n    tweet = re.sub(r\"can\\x89Ûªt\", \"cannot\", tweet)\n    tweet = re.sub(r\"would've\", \"would have\", tweet)\n    tweet = re.sub(r\"it'll\", \"it will\", tweet)\n    tweet = re.sub(r\"we'll\", \"we will\", tweet)\n    tweet = re.sub(r\"wouldn\\x89Ûªt\", \"would not\", tweet)\n    tweet = re.sub(r\"We've\", \"We have\", tweet)\n    tweet = re.sub(r\"he'll\", \"he will\", tweet)\n    tweet = re.sub(r\"Y'all\", \"You all\", tweet)\n    tweet = re.sub(r\"Weren't\", \"Were not\", tweet)\n    tweet = re.sub(r\"Didn't\", \"Did not\", tweet)\n    tweet = re.sub(r\"they'll\", \"they will\", tweet)\n    tweet = re.sub(r\"they'd\", \"they would\", tweet)\n    tweet = re.sub(r\"DON'T\", \"DO NOT\", tweet)\n    tweet = re.sub(r\"That\\x89Ûªs\", \"That is\", tweet)\n    tweet = re.sub(r\"they've\", \"they have\", tweet)\n    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n    tweet = re.sub(r\"should've\", \"should have\", tweet)\n    tweet = re.sub(r\"You\\x89Ûªre\", \"You are\", tweet)\n    tweet = re.sub(r\"where's\", \"where is\", tweet)\n    tweet = re.sub(r\"Don\\x89Ûªt\", \"Do not\", tweet)\n    tweet = re.sub(r\"we'd\", \"we would\", tweet)\n    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n    tweet = re.sub(r\"weren't\", \"were not\", tweet)\n    tweet = re.sub(r\"They're\", \"They are\", tweet)\n    tweet = re.sub(r\"Can\\x89Ûªt\", \"Cannot\", tweet)\n    tweet = re.sub(r\"you\\x89Ûªll\", \"you will\", tweet)\n    tweet = re.sub(r\"I\\x89Ûªd\", \"I would\", tweet)\n    tweet = re.sub(r\"let's\", \"let us\", tweet)\n    \n    # Character entity references\n    tweet = re.sub(r\"&gt;\", \">\", tweet)\n    tweet = re.sub(r\"&lt;\", \"<\", tweet)\n    tweet = re.sub(r\"&amp;\", \"&\", tweet)\n        \n    # Typos, slang and informal abbreviations\n    tweet = re.sub(r\"w/e\", \"whatever\", tweet)\n    tweet = re.sub(r\"w/\", \"with\", tweet)\n    tweet = re.sub(r\"USAgov\", \"USA government\", tweet)\n    tweet = re.sub(r\"recentlu\", \"recently\", tweet)\n    tweet = re.sub(r\"Ph0tos\", \"Photos\", tweet)\n    tweet = re.sub(r\"exp0sed\", \"exposed\", tweet)\n    tweet = re.sub(r\"<3\", \"love\", tweet)\n    tweet = re.sub(r\"amageddon\", \"armageddon\", tweet)\n    tweet = re.sub(r\"Trfc\", \"Traffic\", tweet)\n    tweet = re.sub(r\"8/5/2015\", \"2015-08-05\", tweet)\n    tweet = re.sub(r\"chest/torso\", \"chest / torso\", tweet)\n    tweet = re.sub(r\"WindStorm\", \"Wind Storm\", tweet)\n    tweet = re.sub(r\"8/6/2015\", \"2015-08-06\", tweet)\n    tweet = re.sub(r\"10:38PM\", \"10:38 PM\", tweet)\n    tweet = re.sub(r\"10:30pm\", \"10:30 PM\", tweet)\n    \n    # Separating other punctuations\n    tweet = re.sub(r\"MH370:\", \"MH370 :\", tweet)\n    tweet = re.sub(r\"PM:\", \"Prime Minister :\", tweet)\n    tweet = re.sub(r\"Legionnaires:\", \"Legionnaires :\", tweet)\n    tweet = re.sub(r\"Latest:\", \"Latest :\", tweet)\n    tweet = re.sub(r\"Crash:\", \"Crash :\", tweet)\n    tweet = re.sub(r\"News:\", \"News :\", tweet)\n    tweet = re.sub(r\"derailment:\", \"derailment :\", tweet)\n    tweet = re.sub(r\"attack:\", \"attack :\", tweet)\n    tweet = re.sub(r\"Saipan:\", \"Saipan :\", tweet)\n    tweet = re.sub(r\"Photo:\", \"Photo :\", tweet)\n    tweet = re.sub(r\"Funtenna:\", \"Funtenna :\", tweet)\n    tweet = re.sub(r\"quiz:\", \"quiz :\", tweet)\n    tweet = re.sub(r\"VIDEO:\", \"VIDEO :\", tweet)\n    tweet = re.sub(r\"MP:\", \"MP :\", tweet)\n    tweet = re.sub(r\"UTC2015-08-05\", \"UTC 2015-08-05\", tweet)\n    tweet = re.sub(r\"California:\", \"California :\", tweet)\n    tweet = re.sub(r\"horror:\", \"horror :\", tweet)\n    tweet = re.sub(r\"Past:\", \"Past :\", tweet)\n    tweet = re.sub(r\"Time2015-08-06\", \"Time 2015-08-06\", tweet)\n    tweet = re.sub(r\"here:\", \"here :\", tweet)\n    tweet = re.sub(r\"fires.\", \"fires .\", tweet)\n    tweet = re.sub(r\"Forest:\", \"Forest :\", tweet)\n    tweet = re.sub(r\"Cramer:\", \"Cramer :\", tweet)\n    tweet = re.sub(r\"Chile:\", \"Chile :\", tweet)\n    tweet = re.sub(r\"link:\", \"link :\", tweet)\n    tweet = re.sub(r\"crash:\", \"crash :\", tweet)\n    tweet = re.sub(r\"Video:\", \"Video :\", tweet)\n    tweet = re.sub(r\"Bestnaijamade:\", \"bestnaijamade :\", tweet)\n    tweet = re.sub(r\"NWS:\", \"National Weather Service :\", tweet)\n    tweet = re.sub(r\".caught\", \". caught\", tweet)\n    tweet = re.sub(r\"Hobbit:\", \"Hobbit :\", tweet)\n    tweet = re.sub(r\"2015:\", \"2015 :\", tweet)\n    tweet = re.sub(r\"post:\", \"post :\", tweet)\n    tweet = re.sub(r\"BREAKING:\", \"BREAKING :\", tweet)\n    tweet = re.sub(r\"Island:\", \"Island :\", tweet)\n    tweet = re.sub(r\"Med:\", \"Med :\", tweet)\n    tweet = re.sub(r\"97/Georgia\", \"97 / Georgia\", tweet)\n    tweet = re.sub(r\"Here:\", \"Here :\", tweet)\n    tweet = re.sub(r\"horror;\", \"horror ;\", tweet)\n    tweet = re.sub(r\"people;\", \"people ;\", tweet)\n    tweet = re.sub(r\"refugees;\", \"refugees ;\", tweet)\n    tweet = re.sub(r\"Genocide;\", \"Genocide ;\", tweet)\n    tweet = re.sub(r\".POTUS\", \". POTUS\", tweet)\n    tweet = re.sub(r\"Collision-No\", \"Collision - No\", tweet)\n    tweet = re.sub(r\"Rear-\", \"Rear -\", tweet)\n    tweet = re.sub(r\"Broadway:\", \"Broadway :\", tweet)\n    tweet = re.sub(r\"Correction:\", \"Correction :\", tweet)\n    tweet = re.sub(r\"UPDATE:\", \"UPDATE :\", tweet)\n    tweet = re.sub(r\"Times:\", \"Times :\", tweet)\n    tweet = re.sub(r\"RT:\", \"RT :\", tweet)\n    tweet = re.sub(r\"Police:\", \"Police :\", tweet)\n    tweet = re.sub(r\"Training:\", \"Training :\", tweet)\n    tweet = re.sub(r\"Hawaii:\", \"Hawaii :\", tweet)\n    tweet = re.sub(r\"Selfies:\", \"Selfies :\", tweet)\n    tweet = re.sub(r\"Content:\", \"Content :\", tweet)\n    tweet = re.sub(r\"101:\", \"101 :\", tweet)\n    tweet = re.sub(r\"story:\", \"story :\", tweet)\n    tweet = re.sub(r\"injured:\", \"injured :\", tweet)\n    tweet = re.sub(r\"poll:\", \"poll :\", tweet)\n    tweet = re.sub(r\"Guide:\", \"Guide :\", tweet)\n    tweet = re.sub(r\"Update:\", \"Update :\", tweet)\n    tweet = re.sub(r\"alarm:\", \"alarm :\", tweet)\n    tweet = re.sub(r\"floods:\", \"floods :\", tweet)\n    tweet = re.sub(r\"Flood:\", \"Flood :\", tweet)\n    tweet = re.sub(r\"MH370;\", \"MH370 ;\", tweet)\n    tweet = re.sub(r\"life:\", \"life :\", tweet)\n    tweet = re.sub(r\"crush:\", \"crush :\", tweet)\n    tweet = re.sub(r\"now:\", \"now :\", tweet)\n    tweet = re.sub(r\"Vote:\", \"Vote :\", tweet)\n    tweet = re.sub(r\"Catastrophe.\", \"Catastrophe .\", tweet)\n    tweet = re.sub(r\"library:\", \"library :\", tweet)\n    tweet = re.sub(r\"Bush:\", \"Bush :\", tweet)\n    tweet = re.sub(r\";ACCIDENT\", \"; ACCIDENT\", tweet)\n    tweet = re.sub(r\"accident:\", \"accident :\", tweet)\n    tweet = re.sub(r\"Taiwan;\", \"Taiwan ;\", tweet)\n    tweet = re.sub(r\"Map:\", \"Map :\", tweet)\n    tweet = re.sub(r\"failure:\", \"failure :\", tweet)\n    tweet = re.sub(r\"150-Foot\", \"150 - Foot\", tweet)\n    tweet = re.sub(r\"failure:\", \"failure :\", tweet)\n    tweet = re.sub(r\"prefer:\", \"prefer :\", tweet)\n    tweet = re.sub(r\"CNN:\", \"CNN :\", tweet)\n    tweet = re.sub(r\"Oops:\", \"Oops :\", tweet)\n    tweet = re.sub(r\"Disco:\", \"Disco :\", tweet)\n    tweet = re.sub(r\"Disease:\", \"Disease :\", tweet)\n    tweet = re.sub(r\"Grows:\", \"Grows :\", tweet)\n    tweet = re.sub(r\"projected:\", \"projected :\", tweet)\n    tweet = re.sub(r\"Pakistan.\", \"Pakistan .\", tweet)\n    tweet = re.sub(r\"ministers:\", \"ministers :\", tweet)\n    tweet = re.sub(r\"Photos:\", \"Photos :\", tweet)\n    tweet = re.sub(r\"Disease:\", \"Disease :\", tweet)\n    tweet = re.sub(r\"pres:\", \"press :\", tweet)\n    tweet = re.sub(r\"winds.\", \"winds .\", tweet)\n    tweet = re.sub(r\"MPH.\", \"MPH .\", tweet)\n    tweet = re.sub(r\"PHOTOS:\", \"PHOTOS :\", tweet)\n    tweet = re.sub(r\"Time2015-08-05\", \"Time 2015-08-05\", tweet)\n    tweet = re.sub(r\"Denmark:\", \"Denmark :\", tweet)\n    tweet = re.sub(r\"Articles:\", \"Articles :\", tweet)\n    tweet = re.sub(r\"Crash:\", \"Crash :\", tweet)\n    tweet = re.sub(r\"casualties.:\", \"casualties .:\", tweet)\n    tweet = re.sub(r\"Afghanistan:\", \"Afghanistan :\", tweet)\n    tweet = re.sub(r\"Day:\", \"Day :\", tweet)\n    tweet = re.sub(r\"AVERTED:\", \"AVERTED :\", tweet)\n    tweet = re.sub(r\"sitting:\", \"sitting :\", tweet)\n    tweet = re.sub(r\"Multiplayer:\", \"Multiplayer :\", tweet)\n    tweet = re.sub(r\"Kaduna:\", \"Kaduna :\", tweet)\n    tweet = re.sub(r\"favorite:\", \"favorite :\", tweet)\n    tweet = re.sub(r\"home:\", \"home :\", tweet)\n    tweet = re.sub(r\"just:\", \"just :\", tweet)\n    tweet = re.sub(r\"Collision-1141\", \"Collision - 1141\", tweet)\n    tweet = re.sub(r\"County:\", \"County :\", tweet)\n    tweet = re.sub(r\"Duty:\", \"Duty :\", tweet)\n    tweet = re.sub(r\"page:\", \"page :\", tweet)\n    tweet = re.sub(r\"Attack:\", \"Attack :\", tweet)\n    tweet = re.sub(r\"Minecraft:\", \"Minecraft :\", tweet)\n    tweet = re.sub(r\"wounds;\", \"wounds ;\", tweet)\n    tweet = re.sub(r\"Shots:\", \"Shots :\", tweet)\n    tweet = re.sub(r\"shots:\", \"shots :\", tweet)\n    tweet = re.sub(r\"Gunfire:\", \"Gunfire :\", tweet)\n    tweet = re.sub(r\"hike:\", \"hike :\", tweet)\n    tweet = re.sub(r\"Email:\", \"Email :\", tweet)\n    tweet = re.sub(r\"System:\", \"System :\", tweet)\n    tweet = re.sub(r\"Radio:\", \"Radio :\", tweet)\n    tweet = re.sub(r\"King:\", \"King :\", tweet)\n    tweet = re.sub(r\"upheaval:\", \"upheaval :\", tweet)\n    tweet = re.sub(r\"tragedy;\", \"tragedy ;\", tweet)\n    tweet = re.sub(r\"HERE:\", \"HERE :\", tweet)\n    tweet = re.sub(r\"terrorism:\", \"terrorism :\", tweet)\n    tweet = re.sub(r\"police:\", \"police :\", tweet)\n    tweet = re.sub(r\"Mosque:\", \"Mosque :\", tweet)\n    tweet = re.sub(r\"Rightways:\", \"Rightways :\", tweet)\n    tweet = re.sub(r\"Brooklyn:\", \"Brooklyn :\", tweet)\n    tweet = re.sub(r\"Arrived:\", \"Arrived :\", tweet)\n    tweet = re.sub(r\"Home:\", \"Home :\", tweet)\n    tweet = re.sub(r\"Earth:\", \"Earth :\", tweet)\n    tweet = re.sub(r\"three:\", \"three :\", tweet)\n    \n    # Hashtags and usernames\n    tweet = re.sub(r\"IranDeal\", \"Iran Deal\", tweet)\n    tweet = re.sub(r\"ArianaGrande\", \"Ariana Grande\", tweet)\n    tweet = re.sub(r\"camilacabello97\", \"camila cabello\", tweet) \n    tweet = re.sub(r\"RondaRousey\", \"Ronda Rousey\", tweet)     \n    tweet = re.sub(r\"MTVHottest\", \"MTV Hottest\", tweet)\n    tweet = re.sub(r\"TrapMusic\", \"Trap Music\", tweet)\n    tweet = re.sub(r\"ProphetMuhammad\", \"Prophet Muhammad\", tweet)\n    tweet = re.sub(r\"PantherAttack\", \"Panther Attack\", tweet)\n    tweet = re.sub(r\"StrategicPatience\", \"Strategic Patience\", tweet)\n    tweet = re.sub(r\"socialnews\", \"social news\", tweet)\n    tweet = re.sub(r\"NASAHurricane\", \"NASA Hurricane\", tweet)\n    tweet = re.sub(r\"onlinecommunities\", \"online communities\", tweet)\n    tweet = re.sub(r\"humanconsumption\", \"human consumption\", tweet)\n    tweet = re.sub(r\"Typhoon-Devastated\", \"Typhoon Devastated\", tweet)\n    tweet = re.sub(r\"Meat-Loving\", \"Meat Loving\", tweet)\n    tweet = re.sub(r\"facialabuse\", \"facial abuse\", tweet)\n    tweet = re.sub(r\"LakeCounty\", \"Lake County\", tweet)\n    tweet = re.sub(r\"BeingAuthor\", \"Being Author\", tweet)\n    tweet = re.sub(r\"withheavenly\", \"with heavenly\", tweet)\n    tweet = re.sub(r\"thankU\", \"thank you\", tweet)\n    tweet = re.sub(r\"iTunesMusic\", \"iTunes Music\", tweet)\n    tweet = re.sub(r\"OffensiveContent\", \"Offensive Content\", tweet)\n    tweet = re.sub(r\"WorstSummerJob\", \"Worst Summer Job\", tweet)\n    tweet = re.sub(r\"HarryBeCareful\", \"Harry Be Careful\", tweet)\n    tweet = re.sub(r\"NASASolarSystem\", \"NASA Solar System\", tweet)\n    tweet = re.sub(r\"animalrescue\", \"animal rescue\", tweet)\n    tweet = re.sub(r\"KurtSchlichter\", \"Kurt Schlichter\", tweet)\n    tweet = re.sub(r\"aRmageddon\", \"armageddon\", tweet)\n    tweet = re.sub(r\"Throwingknifes\", \"Throwing knives\", tweet)\n    tweet = re.sub(r\"GodsLove\", \"God's Love\", tweet)\n    tweet = re.sub(r\"bookboost\", \"book boost\", tweet)\n    tweet = re.sub(r\"ibooklove\", \"I book love\", tweet)\n    tweet = re.sub(r\"NestleIndia\", \"Nestle India\", tweet)\n    tweet = re.sub(r\"realDonaldTrump\", \"Donald Trump\", tweet)\n    tweet = re.sub(r\"DavidVonderhaar\", \"David Vonderhaar\", tweet)\n    tweet = re.sub(r\"CecilTheLion\", \"Cecil The Lion\", tweet)\n    tweet = re.sub(r\"weathernetwork\", \"weather network\", tweet)\n    tweet = re.sub(r\"withBioterrorism&use\", \"with Bioterrorism & use\", tweet)\n    tweet = re.sub(r\"Hostage&2\", \"Hostage & 2\", tweet)\n    tweet = re.sub(r\"GOPDebate\", \"GOP Debate\", tweet)\n    tweet = re.sub(r\"RickPerry\", \"Rick Perry\", tweet)\n    tweet = re.sub(r\"frontpage\", \"front page\", tweet)\n    tweet = re.sub(r\"NewsInTweets\", \"News In Tweets\", tweet)\n    tweet = re.sub(r\"ViralSpell\", \"Viral Spell\", tweet)\n    tweet = re.sub(r\"til_now\", \"until now\", tweet)\n    tweet = re.sub(r\"volcanoinRussia\", \"volcano in Russia\", tweet)\n    tweet = re.sub(r\"ZippedNews\", \"Zipped News\", tweet)\n    tweet = re.sub(r\"MicheleBachman\", \"Michele Bachman\", tweet)\n    tweet = re.sub(r\"53inch\", \"53 inch\", tweet)\n    tweet = re.sub(r\"KerrickTrial\", \"Kerrick Trial\", tweet)\n    tweet = re.sub(r\"abstorm\", \"Alberta Storm\", tweet)\n    tweet = re.sub(r\"Beyhive\", \"Beyonce hive\", tweet)\n    tweet = re.sub(r\"IDFire\", \"Idaho Fire\", tweet)\n    tweet = re.sub(r\"DETECTADO\", \"Detected\", tweet)\n    tweet = re.sub(r\"RockyFire\", \"Rocky Fire\", tweet)\n    tweet = re.sub(r\"Listen/Buy\", \"Listen / Buy\", tweet)\n    tweet = re.sub(r\"NickCannon\", \"Nick Cannon\", tweet)\n    tweet = re.sub(r\"FaroeIslands\", \"Faroe Islands\", tweet)\n    tweet = re.sub(r\"yycstorm\", \"Calgary Storm\", tweet)\n    tweet = re.sub(r\"IDPs:\", \"Internally Displaced People :\", tweet)\n    tweet = re.sub(r\"ArtistsUnited\", \"Artists United\", tweet)\n    tweet = re.sub(r\"ClaytonBryant\", \"Clayton Bryant\", tweet)\n    tweet = re.sub(r\"jimmyfallon\", \"jimmy fallon\", tweet)\n\n    words = [x.lstrip().rstrip() for x in tweet.split()]\n    \n    # remove all the stop words\n    words = [x for x in words if x and x not in eng_stopwords]\n    \n    # check if tweet is not empty\n    if not words:\n        return \"\"\n    \n    # replace words with their stem words: https://www.nltk.org/howto/stem.html\n    # words = [stemmer.stem(x) for x in words]\n\n    # replace @usermentions with simply user\n    words = ['user' if x.startswith('@') else x for x in words]\n\n    tweet = \" \".join(words)\n    tweet = re.sub(r\"[,.;@#?!&$:]+\\ *\", \" \", tweet)\n    \n    # now we can return the valid tweet\n    return tweet","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So I have taken the approach to append the keyword simply in the beginning of the tweet to add the keyword info in the tweet too. It is just one approach I have tried here, still need to understand how to add the keyword info better in the tweet to be processed."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_pd['combined_filtered_data'] = train_pd['keyword'] + ' ' + train_pd['text']\ntrain_pd['combined_filtered_data'] = train_pd['combined_filtered_data'].apply(lambda x: parse_tweet(x))\ntrain_pd.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pd['combined_filtered_data'] = test_pd['keyword'] + ' ' + test_pd['text']\ntest_pd['combined_filtered_data'] = test_pd['combined_filtered_data'].apply(lambda x: parse_tweet(x))\ntest_pd.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We now fill all the NaN with empty string for `combined_filtered_data` column"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pd = test_pd.fillna(value={'combined_filtered_data': ''})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_len = max(map(len, train_pd['combined_filtered_data']))\nmax_len = max(max_len, max(map(len, test_pd['combined_filtered_data'])))\nprint(max_len)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tokenization of the tweets"},{"metadata":{},"cell_type":"markdown","source":"Let's define few costants first"},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_dim = 300\ntrunc_type='post'\npadding_type='post'\noov_tok = \"<OOV>\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we perform the text fitting on train and test filtered/cleaned tweet data to generate the tokens (word to index and index to word mapping) and later conver the text to sequences of indices.\n\n> NOTE: number of words in word_index (i.e. `len(tokenizer.word_index)`) is the vocabulary size of the corpus"},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer(oov_token=oov_tok)\ntokenizer.fit_on_texts(train_pd['combined_filtered_data'])\ntokenizer.fit_on_texts(test_pd['combined_filtered_data'])\n\nword_index = tokenizer.word_index\nvocab_size = len(word_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = tokenizer.texts_to_sequences(train_pd['combined_filtered_data'])\nX_padded = pad_sequences(X, maxlen=max_len, padding=padding_type, truncating=trunc_type)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can split the data here into train & validation chunk or later while fitting the model we can provide the validation_split percentage with `shuffle=True` to do the work for us. I have just kept the code for this but going with the latter approach"},{"metadata":{"trusted":true},"cell_type":"code","source":"split = int(1.0 * len(X_padded))\ntrainX, trainY = X_padded[:split], train_pd['target'][:split]\nvalX, valY = X_padded[split:], train_pd['target'][split:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testX = tokenizer.texts_to_sequences(test_pd['combined_filtered_data'])\ntestX = pad_sequences(testX, maxlen=max_len, padding=padding_type, truncating=trunc_type)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I am using the glove embeddings here for the first layer of the model which converts the text into tensor descrbing the word embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"embeddings_glove = np.load('../input/pickled-glove840b300d-for-10sec-loading/glove.840B.300d.pkl', allow_pickle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix = np.zeros((vocab_size + 1, embedding_dim))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_glove.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preparing the model\n\nWe are going to prepare the model now which comprises of an embedding layer + bi-directional LSTM + Dense layer with relu activation and finally another dense layer of size 1 given our aim is binary prediction.\n\n* Since, our aim is binary prediction, I am using `binary_crossentropy` as loss function.\n* In my validations, I could see `Adam` optimizer performing better, hence using the same.\n* Have kept the metrics to evaluate on as accuracy (need to understand more on what to select in different type of tasks)\n\nLet's compile the model!"},{"metadata":{},"cell_type":"markdown","source":"> Given the submissions are evaluated using F1 Score, we can also track the same during the training. Thus, below I have implemented my custom `F1Score` Metric."},{"metadata":{"trusted":true},"cell_type":"code","source":"class F1Score(tf.keras.metrics.Metric):\n    \n    def __init__(self, name='f1_score', **kwargs):\n        super(F1Score, self).__init__(name=name, **kwargs)\n        self.p = tf.keras.metrics.Precision()\n        self.r = tf.keras.metrics.Recall()\n\n    def update_state(self, *args, **kwargs):\n        self.p.update_state(*args, **kwargs)\n        self.r.update_state(*args, **kwargs)\n\n    def reset_states(self):\n        self.p.reset_states()\n        self.r.reset_states()\n\n    def result(self):\n        p_res, r_res = self.p.result(), self.r.result()\n        return (2 * p_res * r_res)/(p_res + r_res)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model():\n    tf.keras.backend.clear_session()\n    model = tf.keras.Sequential([\n        tf.keras.layers.Embedding(\n            vocab_size + 1, embedding_dim,\n            weights=[embedding_matrix], input_length=max_len,\n            trainable=False\n        ),\n        tf.keras.layers.Bidirectional(\n            tf.keras.layers.LSTM(300)\n        ),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.Dense(30, activation='relu'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.Dense(1, activation='sigmoid')\n    ])\n    model.compile(\n        loss='binary_crossentropy',\n        optimizer=tf.keras.optimizers.Adam(),\n        metrics=[F1Score()]\n    )\n    # print(model.summary())\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_metrics(history):\n    f1_score = history.history['f1_score']\n    loss = history.history['loss']\n    val_f1_score = history.history['val_f1_score']\n    val_loss = history.history['val_loss']\n\n    epochs = range(len(f1_score))\n\n    plt.plot(epochs, f1_score, 'b', label='Training f1_score')\n    plt.plot(epochs, val_f1_score, 'y', label='Validation f1_score')\n    plt.title('F1 Score')\n    plt.legend()\n\n    plt.figure()\n\n    plt.plot(epochs, loss, 'b', label='Training Loss')\n    plt.plot(epochs, val_loss, 'y', label='Validation Loss')\n    plt.title('loss')\n    plt.legend()\n\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Cross validation & Prediction\n\nWe are going to perform `5-Fold CV` and then take average of the results of models on test data per CV."},{"metadata":{"trusted":true},"cell_type":"code","source":"n_splits, seed = 5, 1479\nkf = KFold(n_splits=n_splits, random_state=seed, shuffle=True)\npredictions = []\n\nfold = 1\nfor train_index, val_index in kf.split(trainX):\n    cur_trainX, cur_trainY = trainX[train_index], trainY[train_index]\n    cur_valX, cur_valY = trainX[val_index], trainY[val_index]\n    \n    print(f\"Current fold: {fold}\")\n    model = create_model() \n    history = model.fit(cur_trainX, cur_trainY, epochs=15, validation_data=(cur_valX, cur_valY), batch_size=80, verbose=1)\n    plot_metrics(history)\n    cur_predictions = model.predict(testX)\n    print(\"-\" * 50)\n    \n    predictions.append(cur_predictions)\n    fold += 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission\n\nLet's take the average of the predictions and prepare the submission csv"},{"metadata":{"trusted":true},"cell_type":"code","source":"final_predictions = np.average(predictions, axis=0)\ntest_pd['target'] = final_predictions\ntest_pd['target'] = test_pd['target'].apply(lambda x: 1 if x >= 0.5 else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pd.to_csv('/kaggle/working/submission.csv', columns=['id', 'target'], index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}