{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"Columns\nid - a unique identifier for each tweet\ntext - the text of the tweet\nlocation - the location the tweet was sent from (may be blank)\nkeyword - a particular keyword from the tweet (may be blank)\ntarget - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)\"\"\"","metadata":{"id":"uU4CPDGd4A8R","outputId":"997b00ab-82a7-4aa8-b5bb-df64eba8de8f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Importing all libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\nfrom wordcloud import WordCloud\nimport missingno\n\n\n%matplotlib inline\nimport random\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB,CategoricalNB\nimport nltk\nnltk.download('stopwords')\nnltk.download('punkt')\nimport re\nfrom nltk.corpus import stopwords\nimport string\nfrom nltk.stem import WordNetLemmatizer\n\n\nfrom sklearn import preprocessing\nfrom sklearn.manifold import TSNE\nimport seaborn as sns\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn.metrics import log_loss\nfrom sklearn.linear_model import LogisticRegression\n\n\n\n\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.metrics import accuracy_score\nfrom time import time\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import model_selection","metadata":{"id":"oG-DjzxypNvl","outputId":"d7d78677-c9fb-45e1-c309-d5b4c781ab1b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\ntest= pd.read_csv(\"../input/nlp-getting-started/test.csv\")","metadata":{"id":"alJWXtNlpUQK","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#No of rows and columns\ntrain.shape","metadata":{"id":"iI4eyvrIpgsX","outputId":"81b20de3-41da-495a-af94-86f04ba1362a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Prints first 5 rows\ntrain.head(5)","metadata":{"id":"wCVIVWw-psMn","outputId":"c3eab98c-d550-49ee-a82a-0e22e89661b1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[\"location\"].value_counts()","metadata":{"id":"1B0HgvoB2uI0","outputId":"918a3b7f-40d9-4471-9307-ea8c53f493fa","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"id":"UmKoXJ9Vs_iS","outputId":"984cd16d-6095-4e2a-db8b-b2b7abf54c95","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.describe()","metadata":{"id":"b1cBoiGj0E0e","outputId":"2ccbd205-df4d-468b-e588-04f675e1f32c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#checking for null values\ntrain.isnull().sum()","metadata":{"id":"K0rnucTh0E3C","outputId":"fc0e4b65-4c5b-458a-a5b1-45c77991db3a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.isnull().sum()","metadata":{"id":"vE0j5uQv0dhy","outputId":"faf52673-2727-4dcc-9459-ced4a5223bcb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Remove redundant samples\ntrain=train.drop_duplicates(subset=['text', 'target'], keep='first')\ntrain.shape","metadata":{"id":"sD-9M6_IAxRX","outputId":"df69cb13-ad40-4499-9914-2890148546b9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"QC4LbND8Azhj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plot graph for missing values\n\nmissingno.matrix(train, figsize = (5,5))","metadata":{"id":"6Be8ImgG0kLq","outputId":"e13d0f5a-e8ee-4c46-e9e2-b414f1f2e81b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#location is having most number of null values","metadata":{"id":"N_ORqgdR1G-y","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.isna().sum().plot(kind=\"bar\")\nplt.title(\"no of null values in train data\")\nplt.show()","metadata":{"id":"ry-d0ye71m9r","outputId":"a2f1e8c8-1eb7-4e02-a78a-0dcf77e4f629","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Location with maximum null values\n#Keyword follows Location with null values","metadata":{"id":"KWv7Glhd1nnE","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"keywords_vc = pd.DataFrame({\"Count\": train[\"keyword\"].value_counts()})\nsns.barplot(y=keywords_vc[0:30].index, x=keywords_vc[0:30][\"Count\"], orient='h')\nplt.title(\"Top 30 Keywords\")\nplt.show()","metadata":{"id":"uORCs2w7XqEC","outputId":"64d947a7-cde0-4ae8-dc7f-c14dfe6b1e13","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"disaster_keywords = train.loc[train[\"target\"] == 1][\"keyword\"].value_counts()\nnondisaster_keywords = train.loc[train[\"target\"] == 0][\"keyword\"].value_counts()\n\nfig, ax = plt.subplots(1,2, figsize=(20,8))\nsns.barplot(y=disaster_keywords[0:30].index, x=disaster_keywords[0:30], orient='h', ax=ax[0], palette=\"Reds_d\")\nsns.barplot(y=nondisaster_keywords[0:30].index, x=nondisaster_keywords[0:30], orient='h', ax=ax[1], palette=\"Blues_d\")\nax[0].set_title(\"Top 30 Keywords - Disaster Tweets\")\nax[0].set_xlabel(\"Keyword Frequency\")\nax[1].set_title(\"Top 30 Keywords - Non-Disaster Tweets\")\nax[1].set_xlabel(\"Keyword Frequency\")\nplt.tight_layout()\nplt.show()","metadata":{"id":"SPXD-RmLXqHL","outputId":"4f8c566d-38f6-4e5c-d39f-7cf27e9a0cc8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# drop location and keyword column\ntrain = train.drop(['location','keyword'],axis=1)\ntest = test.drop(['location','keyword'],axis=1)","metadata":{"id":"Ldq-wraA1npk","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head(5)","metadata":{"id":"fyt1ps_d1nsJ","outputId":"a7f28723-f7f4-4b50-bfa7-0a42f9959db7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Let check how many real tweets and fake tweets\n\ntweetreal = len(train[train[\"target\"]==1])","metadata":{"id":"xNRQhCPW1nuX","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#percentage of real tweets\nRealTweetPercentage = tweetreal/train.shape[0]*100\nRealTweetPercentage","metadata":{"id":"BfR7Brg61nwZ","outputId":"a21cb2e6-ea4c-4a8a-80ce-5e79252df628","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Percentage of fake tweet\nFakeTweetPercentage = 100-RealTweetPercentage\nFakeTweetPercentage","metadata":{"id":"4-EWjWKY79oS","outputId":"32de3660-6054-4bd1-f0c0-cfaec7c0548c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plot target variables\nsns.countplot(x ='target', data= train)","metadata":{"id":"xmDFEURR8XQB","outputId":"f8fa958b-42f8-4eb3-ec88-b91d66fdf331","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Now lets understand the density of tweets in both test and train dataset.\n\nden_train = train['text'].str.len()\nden_test = test['text'].str.len()\n\nplt.hist(den_train, label = \"train_tweets\")\nplt.hist(den_test, label= \"text_tweets\")","metadata":{"id":"sIKXU7qX8sut","outputId":"50a57ccd-cf63-4a1d-d769-1b216ea817d9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#So here train data is having more tweets compared to the test data.","metadata":{"id":"t4vpJzux8s5u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Fetch wordcount for each abstract\ntrain['word_count'] = train['text'].apply(lambda x: len(str(x).split(\" \")))\ntrain[['text','word_count']].head(10)","metadata":{"id":"dWHe2er1ZGFz","outputId":"15a7466a-f318-43f5-c88c-1fcde5bc11da","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Descriptive statistics of word counts\ntrain.word_count.describe()\n\n#The average word count is about 15 words per abstract. The word count ranges from a minimum of 1 to a maximum of 54.","metadata":{"id":"Hx2XAqx0ZGIc","outputId":"9ae462af-21b6-4383-98b8-68e2c9a910fe","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"4bHKfXIlZGLL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Identify common words: Its the frequently used words as well as it could be potential data specific stop words.\nimport pandas as pd\n\nfreq = pd.Series(' '.join(train['text']).split()).value_counts()[:20]\nfreq","metadata":{"id":"5NBKJw9zZGNZ","outputId":"56c5708c-cded-4bd9-fdd9-c1ebfc2ae2df","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"7Luo3wBy8tSm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Identify uncommon words: Uncommon words in the train dataset\nfreq1 =  pd.Series(' '.join(train ['text']).split()).value_counts()[-20:]\nfreq1","metadata":{"id":"vl2ooLck1rn_","outputId":"15c66475-49c3-4168-ba7f-793e5fb4a159","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Disaster tweet\n\ndisaster_tweets = train[train['target'] ==1 ]['text']\nfor i in range(1,10):\n    print(disaster_tweets[i])","metadata":{"id":"-U4aiMa91rqJ","outputId":"74efbe7f-1ffc-434d-c33b-620a7c60523f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# non-disaster tweets\nnon_disaster_tweets = train[train['target'] !=1 ]['text']\nnon_disaster_tweets","metadata":{"id":"vMojnUsnqyrd","outputId":"0ad7d302-82bf-4ea7-90c8-55ecbbbc3aea","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#wordcloud\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=[20, 5])\nwordcloud1 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(disaster_tweets))\nax1.imshow(wordcloud1)\nax1.axis('off')\nax1.set_title('Disaster Tweets',fontsize=20);\n\n\nwordcloud2 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(non_disaster_tweets))\nax2.imshow(wordcloud2)\nax2.axis('off')\nax2.set_title('Non Disaster Tweets',fontsize=20);\n","metadata":{"id":"leckQrziqyuO","outputId":"7a779ae0-a6d5-4a8f-a538-b96475cefff1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head(5)","metadata":{"id":"eJ8VnMefTIaX","outputId":"bcf8c419-8efb-420d-b659-48288a415d0f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"uDkVDrs_TIj4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nnltk.download('stopwords')","metadata":{"id":"8Aa9n2qjqywy","outputId":"580e7651-5153-4f74-bc5b-7479d6c4cfb0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Cleaning","metadata":{"id":"3d5l1CPI-sIk"}},{"cell_type":"markdown","source":"Now lets do **Text-preprocessing**:\n1. Reduce sparsity\n2. Text clean-up\n3. Shrinkage the vocabulary to retain only the relevant words\n\nText preprocessing\n1: Noise Removal\n\n      a: Removing redundant text components\n\n      b: Punctuations, Tags, URL, stopwords\n\n2: Normalization\n\n      a: Stemming - Remove suffixes\n      b: Lemmatization- Works based on the roots of the word \n\n\n\n\n","metadata":{"id":"4wiBVn8-2GAm"}},{"cell_type":"code","source":"#Normalization is the method of handling multiple occurances of the same word\n#Stemming normalizes text by removing suffixes.\n#Lemmatisation is a more advanced technique which works based on the root of the word.\n","metadata":{"id":"SUNwcjq315wF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have 92 redundants sapmles in our dataset (7613-7521)=92\n\n---\n\n","metadata":{"id":"N5l0tr1q_Tx4"}},{"cell_type":"code","source":"#After removing all the redundant values lets check te counts\ntrain.target.value_counts()","metadata":{"id":"z0DiEua2_FHt","outputId":"9c0931b1-3727-4173-cfc9-3bb3b1750c1e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stopwords.words('english')","metadata":{"id":"v8MzwDZDBlkG","outputId":"7daae81d-5e0d-4192-e222-1d6287690fed","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#List of punctuations and we will remove them from our corpus\nimport string\nstring.punctuation","metadata":{"id":"F0WlnE9PBlms","outputId":"6d48c2f0-32f1-447c-85b6-e29213320442","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"WnvQsoBCBlpU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Cleaning data","metadata":{"id":"Ii2SxbklBlr2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_text(text):\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","metadata":{"id":"tOS2x3EGBluw","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['text'] = train['text'].apply(lambda x: clean_text(x))\ntest['text'] = test['text'].apply(lambda x: clean_text(x))","metadata":{"id":"LO6MMqQs_FOi","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['text'].head()","metadata":{"id":"08gg35fc1tdT","outputId":"f47cbb7c-25c3-42f3-8055-21d5ea2f94e8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweets = train[\"text\"]","metadata":{"id":"x_lVV32iCNz2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Tokenize\n\n\ntokenizer=nltk.tokenize.RegexpTokenizer(r'\\w+')\ntrain['text'] = train['text'].apply(lambda x:tokenizer.tokenize(x))\ntest['text'] = test['text'].apply(lambda x:tokenizer.tokenize(x))\ntrain['text'].head()","metadata":{"id":"4APLDnwv1tf8","outputId":"e12d20ea-eb48-4d91-c4ac-da2e5045219b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"uWMafuW31tiS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Removing stopwords\n\n\ndef remove_stopwords(text):\n    words = [w for w in text if w not in stopwords.words('english')]\n    return words \ntrain['text'] = train['text'].apply(lambda x : remove_stopwords(x))\ntest['text'] = test['text'].apply(lambda x : remove_stopwords(x))\ntest.head()","metadata":{"id":"hSRPm8KV16VO","outputId":"938f7a76-8f1e-49a6-9f83-50e933ef353b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"9yRw4ld82BxA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"disaster_tweets = train[train['target'] ==1 ]['text']\nfor i in range(1,10):\n    print(disaster_tweets[i])","metadata":{"id":"ZiEa8i_v2B0F","outputId":"3dec5d34-0a14-41f5-93ac-f6aec71f9d78","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# non-disaster tweets\nnon_disaster_tweets = train[train['target'] !=1 ]['text']\nnon_disaster_tweets.head()","metadata":{"id":"92jFoVrJ2B2e","outputId":"8fce1f41-0e30-4d5c-b5ae-0dfe700c8f99","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"5mG1L0qW16X7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lemmatization\n\nnltk.download()\nimport nltk\nnltk.download('averaged_perceptron_tagger')","metadata":{"id":"1oF7Qzzt24Hy","outputId":"4695887a-9a4a-4e4c-cbb6-a9c97aaef1ee","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nnltk.download('wordnet')\nnltk.download('averaged_perceptron_tagger')","metadata":{"id":"-6Me5AG94IA1","outputId":"6ec75a54-7ae4-4517-b6fb-43cc6f8aee68","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lemmatization\nlem = WordNetLemmatizer()\ndef lem_word(x):\n    return [lem.lemmatize(w) for w in x]\n\n    ","metadata":{"id":"PRQKZgEE24NY","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['text'] = train['text'].apply(lem_word)\ntest['text'] = test['text'].apply(lem_word)","metadata":{"id":"6DDUMJhk3uP7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def combine_text(list_of_text):\n    combined_text = ' '.join(list_of_text)\n    return combined_text\n\ntrain['text'] = train['text'].apply(lambda x : combine_text(x))\ntest['text'] = test['text'].apply(lambda x : combine_text(x))\ntrain['text']\ntrain.head()","metadata":{"id":"IQxdIXZL9nmv","outputId":"2b88eef5-6571-495f-b289-6c6a306380f8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.stem.porter import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nlem = WordNetLemmatizer()\nstem = PorterStemmer()\n\nnltk.download()","metadata":{"id":"cWm891pP7LFz","outputId":"32431560-84ca-40e1-be89-bad0eda56b3e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"brOIPDlDFvqY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count_vectorizer = CountVectorizer()\ntrain_vector = count_vectorizer.fit_transform(train['text'])\ntest_vector = count_vectorizer.transform(test['text'])\nprint(train_vector[0].todense())","metadata":{"id":"L_aG9fg_WNiV","outputId":"077b3c89-7b20-42ee-8008-1deb8e367ad7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#TF IDF\n\ntfidf = TfidfVectorizer(min_df = 2,max_df = 0.5,ngram_range = (1,2))\ntrain_tfidf = tfidf.fit_transform(train['text'])\ntest_tfidf = tfidf.transform(test['text'])\n\n","metadata":{"id":"eOy-83YXCw8p","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_tfidf","metadata":{"id":"a2seIIwB-VJh","outputId":"7e319fb2-3e74-41e2-e263-69b06fade843","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"l4F5noCx-W_U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"SOuXC7Pq-y7u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mnb = MultinomialNB(alpha = 2.0)\nscores_vector = model_selection.cross_val_score(mnb,train_vector,train['target'],cv = 10,scoring = 'f1')\nprint(\"score:\",scores_vector)\nscores_tfidf = model_selection.cross_val_score(mnb,train_tfidf,train['target'],cv = 10,scoring = 'f1')\nprint(\"score of tfidf:\",scores_tfidf)","metadata":{"id":"O_n247EY-y-t","outputId":"5ba02182-a4b9-41c7-e9f2-1660e7c733b5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mnb.get_params()","metadata":{"id":"htbrNBkK-zBR","outputId":"562cc283-a8d0-46d5-95f4-4bebee1c7a92","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Logistic Regression","metadata":{"id":"-plkSBM8-XCL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lg = LogisticRegression(C = 1.0)\nscores_vector = model_selection.cross_val_score(lg, train_vector, train[\"target\"], cv = 5, scoring = \"f1\")\nprint(\"score:\",scores_vector)\nscores_tfidf = model_selection.cross_val_score(lg, train_tfidf, train[\"target\"], cv = 5, scoring = \"f1\")\nprint(\"score of tfidf:\",scores_tfidf)","metadata":{"id":"JHcQo8my-XGm","outputId":"18b65583-d023-43eb-aacc-720d3d9f7e8c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lg.get_params()","metadata":{"id":"98wsyp7P-lsN","outputId":"57339bcd-0e0f-4129-8dcb-c59638d8565b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mnb.fit(train_tfidf, train[\"target\"])\ny_pred = mnb.predict(test_tfidf)","metadata":{"id":"sj3n-lvp-lu-","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred","metadata":{"id":"I7aV4TzI-lx0","outputId":"76bf4dea-2d30-46fd-83b5-d210da89228e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"PiiC2FEKdrBq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Submission","metadata":{"id":"qMqXzpEBdrix"}},{"cell_type":"code","source":"submission_file = pd.DataFrame({'Id':test['id'],'target':y_pred})","metadata":{"id":"FzB_BLog--ed","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_file.to_csv('submission_file.csv',index=False)","metadata":{"id":"GOt2fOOF--hd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_file = pd.read_csv('submission_file.csv')","metadata":{"id":"3RBZ3K9c--kV","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_file.head(10)","metadata":{"id":"R6xiM_Cp--nt","outputId":"9a297343-408c-43f0-cc11-91c614989ca3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"nUtQGCD9J7NN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Text preparation","metadata":{"id":"oUlgSbVDMyFZ"}},{"cell_type":"code","source":"\"\"\"Text in the corpus needs to be converted to a format that can be interpreted by the machine learning algorithms. There are 2 parts of this conversion â€” Tokenisation and Vectorisation.\"\"\"","metadata":{"id":"yFatxEr-J7P8","outputId":"c5d4918f-efbd-4f29-ff12-46a0f9474cf7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"Tokenisation is the process of converting the continuous text into a list of words. The list of words is then converted to a matrix of integers by the process of vectorisation. Vectorisation is also called feature extraction.\"\"\"","metadata":{"id":"RZjdlYx_J7Sn","outputId":"d0090d81-e0bd-4631-bdc7-d8e30b57c1b7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"For text preparation we use the bag of words model which ignores the sequence of the words and only considers word frequencies.\"\"\"","metadata":{"id":"J82g69H1NBOX","outputId":"66a9d14d-928d-43bb-a230-e49e5dac5595"},"execution_count":null,"outputs":[]}]}