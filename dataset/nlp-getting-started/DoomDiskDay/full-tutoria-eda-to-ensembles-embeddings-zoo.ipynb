{"cells":[{"metadata":{},"cell_type":"markdown","source":"![](https://breakingtech.it/wp-content/uploads/2018/04/twitter-moments-1.jpg)\n\n## What's in the notebook?\n- Full Exploratory Data Analysis (EDA)\n- Data Cleaning\n- Evaluation\n    - BL Models (majority model + tfidf & logreg)\n    - Gradient Boosting\n    - Simple RNN\n    - Glove Bi-LSTM\n    - BERT + sigmoid\n    - Ensemble (BERT + 10 shallow classifiers)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom collections import defaultdict\nfrom collections import  Counter\nplt.style.use('ggplot')\nstop=set(stopwords.words('english'))\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport nltk\nfrom sklearn.preprocessing import LabelEncoder\n## ADD STOPWORDS\nstop = set(list(stop) + [\"http\",\"https\", \"s\", \"nt\", \"m\"])\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def load_training(training_path=\"/kaggle/input/nlp-getting-started/train.csv\"):\n    df = pd.read_csv(training_path)\n    \n    print(df.head(10))\n    return df\n\ndf = load_training()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Analisys\nIn the following we're gonna see some data analysis on the corpus. \n\nSpecifically:\n- General dataset infos\n    - Number of samples\n    - Data Columns \n    - Class Label Distributiom\n- Text analysis\n    - Number of characters in tweets\n    - Number of words in a tweet\n    - Average word lenght in a tweet\n    - Word distribution\n    - Hashtag Analysis\n    - KW and Location Analysis"},{"metadata":{},"cell_type":"markdown","source":"## General dataset information\nHere we show the number of samples, the input data columns and the class label distributiom"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"-Number of samples: {}\".format(len(df)))\nprint(\"-Input data columns: {}\".format(df.columns))\nprint(\"-Class label distribution\")\nprint(\"--Number of positive samples: {}\".format(len(df.loc[df['target'] == 1])))\nprint(\"--Number of negative samples: {}\".format(len(df.loc[df['target'] == 0])))\nprint(\"--Plot of Y distributions\")\nx=df.target.value_counts()\nsns.barplot(x.index,x)\nplt.gca().set_ylabel('samples')\n\ndef plot_hist_classes(to_plot, _header):\n    fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\n    df_len = to_plot(1)\n    ax1.hist(df_len,color='red')\n    ax1.set_title('Negative Tweets [disasters]')\n    df_len = to_plot(0)\n    ax2.hist(df_len,color='green')\n    ax2.set_title('Positive Tweets [good posts]')\n    fig.suptitle(_header)\n    plt.show()\n    plt.close()\n    \n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Text analysis\nInsights on number of character and words in tweets, word lenght distribution, and word distribution."},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_word_distrib(target=1, field=\"text\"):\n    txt = df[df['target']==target][field].str.lower().str.replace(r'\\|', ' ').str.cat(sep=' ')\n    words = nltk.tokenize.word_tokenize(txt)\n    words_except_stop_dist = nltk.FreqDist(w for w in words if w not in stop) \n    \n    rslt = pd.DataFrame(words_except_stop_dist.most_common(top_N),\n                        columns=['Word', 'Frequency']).set_index('Word')\n    print(rslt)\n    matplotlib.style.use('ggplot')\n\n    rslt.plot.bar(rot=0)\n    \nprint(\"-Number of characters in tweets\")\ndef to_plot(_target):\n    return df[df['target']==_target]['text'].str.len()\n\n    \nplot_hist_classes(to_plot, _header='Characters Distribution in Tweets')\n\n\nprint(\"-Number of words in a tweet\")\ndef to_plot(_target):\n    return df[df['target']==_target]['text'].str.split().map(lambda x: len(x))\ndef how_to_plot(**kwargs):\n    ax1.hist(df_len,**kwargs)\nplot_hist_classes(to_plot, _header='Word Distribution in Tweet')\n\n\nprint(\"-Average word lenght in a tweet\")\nfig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\nword=df[df['target']==1]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='red')\nax1.set_title('Negative Tweets [disasters]')\nword=df[df['target']==0]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='green')\nax2.set_title('Positive Tweets [good posts]')\nfig.suptitle('Average word length in each tweet')\nplt.show()\n\nprint(\"-Word distribution\")\n\ntop_N = 10\n\nprint(\"-- Positive Class\")\n\nshow_word_distrib(target=1, field=\"text\")\n\nprint(\"-- Negative Class\")\nshow_word_distrib(target=0, field=\"text\")\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Hashtag analysis\nSmall analysis done on the hashtags, to check it's possible discriminator capability for this task."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"-Hashtag Analysis \")\ndef find_hashtags(tweet):\n    return \", \".join([match.group(0)[1:] for match in re.finditer(r\"#\\w+\", tweet)]) or None\n\ndef add_hashtags(df):\n    from sklearn.feature_extraction.text import CountVectorizer\n    \n    df['hashtag'] = df[\"text\"].apply(lambda x: find_hashtags(x))\n    df['hashtag'].fillna(value=\"no\", inplace=True)\n    \n    return df\n    \ntop_N = 20\n\ndf = add_hashtags(df)\n_l = len([v for v in df.hashtag.values if isinstance(v, str)])\nprint(\"-Number of tweets with hashtags: {}\".format(_l))\nprint(\"-- Hashtag distribution in positive samples \")\nshow_word_distrib(target=1, field=\"hashtag\")\n\nprint(\"-- Hashtag distribution in negative samples \")\nshow_word_distrib(target=0, field=\"hashtag\")\n\n\n#There is too much intersection between hashtag in positive and negative samples, meaning that an\n#hashtag approach will not work that well.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# KW and Location analysis\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Remove the encoded space character for keywords, since appears a lot of times and is junk\ndf['keyword'] = df['keyword'].map(lambda s: s.replace('%20', ' ') if isinstance(s, str) else s)\n\nun_KW  = {kw for kw in df['keyword' ].values if isinstance(kw, str)}\ntot_KW = len(df) - len(df[df[\"keyword\" ].isna()])\n\nun_LOC = {lc for lc in df['location'].values if isinstance(lc, str)}\ntot_LOC =  len(df) - len(df[df[\"location\"].isna()])\n\nprint(\"Unique KW: {}\".format(len(un_KW)))\nprint(\"Out of: {}\".format(tot_KW))\nprint(\"Samples with no KW: {}\".format(len(df[df['keyword'].isna()])))\n\n\nprint(\"Unique LOC: {}\".format(len(un_LOC)))\nprint(\"Out of: {}\".format(tot_LOC))\nprint(\"Samples with no Loc: {}\".format(len(df[df['location'].isna()])))\n\n\n#LOCATION IS TOO SPARSE TO BE USED.\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"disaster_keywords = [kw for kw in df.loc[df.target == 1].keyword]\nregular_keywords = [kw for kw in df.loc[df.target == 0].keyword]\n\ndisaster_keywords_counts = dict(pd.DataFrame(data={'x': disaster_keywords}).x.value_counts())\nregular_keywords_counts = dict(pd.DataFrame(data={'x': regular_keywords}).x.value_counts())\n\nall_keywords_counts =  dict(pd.DataFrame(data={'x': df.keyword.values}).x.value_counts())\n\n# we sort the keywords so the most frequents are on top and we print them with relative\n# occurrences in both classes of tweets:\n\nfor keyword, _ in sorted(all_keywords_counts.items(), key=lambda x: x[1], reverse=True)[:10]:\n    print(\"> KW: {}\".format(keyword))\n    print(\"-- # in negative tweets: {}\".format(disaster_keywords_counts.get(keyword, 0)))\n    print(\"-- # in positive tweets: {}\".format(regular_keywords_counts.get(keyword, 0)))\n    print('--------')\n\n\n#Many KWs in negative tweets are also present in positive ones, meaning that a KW approach is most likely to not work\n#The same result was given by the hashtag analysis.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Cleaning\nHere we are gonna clean the DF.\nSpecifically, we clean:\n- stopwords (Kept cause removing them cause drop of performances)\n- URL \n- HTML \n- emoji \n- punctuation"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef clean_df(df):\n    def remove_stopwords(text):\n        if text is not None:\n            tokens = [x for x in word_tokenize(text) if x not in stop]\n            return \" \".join(tokens)\n        else:\n            return None\n    \n    #TMP: TRY TO USE DEFAULT STRING FOR NONE. TODO: USE ROW[\"KEYWORDS\"]\n    #df['hashtag'] =df['hashtag'].apply(lambda x : \"NO\" if x is None else x)\n    \n    df[\"text\"] = df['text'].apply(lambda x : x.lower())\n    #df[\"hashtag\"] = df['hashtag'].apply(lambda x : x.lower())\n    \n    #df['text'] =df['text'].apply(lambda x : remove_stopwords(x))\n    #df['hashtag'] =df['hashtag'].apply(lambda x : remove_stopwords(x))\n    \n    \n    \n\n\n    def remove_URL(text):\n        url = re.compile(r'https?://\\S+|www\\.\\S+')\n        return url.sub(r'',text)\n\n    df['text']=df['text'].apply(lambda x : remove_URL(x))\n    def remove_html(text):\n        html=re.compile(r'<.*?>')\n        return html.sub(r'',text)\n\n    df['text']=df['text'].apply(lambda x : remove_html(x))\n    # Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\n    def remove_emoji(text):\n        emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n        return emoji_pattern.sub(r'', text)\n\n    df['text']=df['text'].apply(lambda x: remove_emoji(x))\n    def remove_punct(text):\n        table=str.maketrans('','',string.punctuation)\n        return text.translate(table)\n\n    df['text']=df['text'].apply(lambda x : remove_punct(x))\n    \n    df.text = df.text.replace('\\s+', ' ', regex=True)\n    return df\ndf = clean_df(df)\nprint(\"-- Word distrig Positive Class\")\n\nshow_word_distrib(target=1, field=\"text\")\n\nprint(\"-- Word distrib Negative Class\")\nshow_word_distrib(target=0, field=\"text\")\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Utils for models"},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_test(test_path=\"/kaggle/input/nlp-getting-started/test.csv\"):\n    \n    my_df = pd.read_csv(test_path)\n    \n    res_df = my_df[['id']]\n    my_df = my_df[['text']]\n    \n    add_hashtags(my_df)\n    my_df = clean_df(my_df)\n    print(\"Test DF: {}\".format(my_df.head(10)))\n    \n    return my_df, res_df\n\ndef dump_preds(res_df, preds, out=\"default\"):\n    res_df['target'] = None\n    \n    for i, p in  enumerate(preds):\n        res_df.ix[i, 'target'] = p\n    \n    res_df.to_csv(out, index = False)\n    \n\ndef split_data(df, _t=True):\n    X = df.text\n    if _t:\n        Y = df.target\n        le = LabelEncoder()\n        Y = le.fit_transform(Y)\n        Y = Y.reshape(-1,1)\n        return X, Y\n    else:\n        return X\n\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Baseline Models\nWe build a simple majority model and TFIDF + LogReg to check the problem hardness."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import model_selection\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\n\n\"\"\"\nBuild a baseline TFIDF + LOGREG based just on text\n\"\"\"\ndef build_tfidf_logreg(df):\n    my_df = df[['text','target']]\n    x_features = my_df.columns[0]\n    x_data = my_df[x_features]\n    Y = my_df[\"target\"]\n\n    x_train, x_validation, y_train, y_validation = model_selection.train_test_split(\n        x_data.values, Y.values, test_size=0.2, random_state=7)\n    \n    # configure TfidfVectorizer to accept tokenized data\n    # reference http://www.davidsbatista.net/blog/2018/02/28/TfidfVectorizer/\n    tfidf_vectorizer = TfidfVectorizer(\n        analyzer='word',\n        tokenizer=lambda x: x,\n        preprocessor=lambda x: x,\n        token_pattern=None)\n\n    lr = LogisticRegression()\n    tfidf_lr_pipe = Pipeline([('tfidf', tfidf_vectorizer), ('lr', lr)])\n    tfidf_lr_pipe.fit(x_train, y_train)\n    \n    return tfidf_lr_pipe\n\ndef test_tfidf_logreg(model, test_path=\"/kaggle/input/nlp-getting-started/test.csv\"):\n    \n    my_df, res_df = read_test(test_path=\"/kaggle/input/nlp-getting-started/test.csv\")\n    \n    #x_features = my_df.columns[0]\n    x_data = my_df[\"text\"].values\n\n    preds = model.predict(x_data)\n    \n    #dump_preds(res_df, preds, out=\"res_tfidf_logreg4_0.csv\")\n    \n    return res_df\n\n\n\"\"\"\nBuild a majority model\n\"\"\"\ndef test_majority_model(test_path=\"/kaggle/input/nlp-getting-started/test.csv\"):\n    \n    my_df = pd.read_csv(test_path)\n    \n    res = my_df[['id']]\n    res['target'] = 1\n    \n    res.to_csv(\"res_majority.csv\", index = False)\n    return res\n    \n\n#test_majority_model(test_path=\"/kaggle/input/nlp-getting-started/test.csv\")\n#0.42944\n\n#tfidf_log_reg = build_tfidf_logreg(df)\n#test_tfidf_logreg(tfidf_log_reg, test_path=\"/kaggle/input/nlp-getting-started/test.csv\")\n#0.63164\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Test Gradient Boosting\nHere we check a gradient boosting classifier, which is a bit less shallow model w.r.t logistic regression. In fact we gain a 3% w.r.t LogReg"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn import metrics\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\nX_train, y_train = split_data(df)\n\ntest_df, res_df = read_test(test_path=\"/kaggle/input/nlp-getting-started/test.csv\")\nX_test = split_data(test_df, _t=False)\n\ntext_clf = Pipeline([('vect', CountVectorizer()),\n                     ('tfidf', TfidfTransformer()),\n                     ('clf', GradientBoostingClassifier(n_estimators=100)),\n                     ])\n#text_clf.fit(X_train, y_train)\n#predicted = text_clf.predict(X_test)\n#dump_preds(res_df, predicted, out=\"submission.csv\")\n#0.66462","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Test RNN Model\nHere we test a simple LSTM model with Dropout. The experiment does not give better performances w.r.t gradient boosting."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.models import Model\nfrom keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\nfrom keras.optimizers import RMSprop\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence\nfrom keras.utils import to_categorical\nfrom keras.callbacks import EarlyStopping\n%matplotlib inline\n\n#Value tuned based on data analysis\nmax_words = 750\nmax_len = 160\n\ndef process_data(X, tok=None):\n    if tok is None:\n        tok = Tokenizer(num_words=max_words)\n        tok.fit_on_texts(X)\n    sequences = tok.texts_to_sequences(X)\n    sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n    return sequences_matrix, tok\n\ndef RNN():\n    inputs = Input(name='inputs',shape=[max_len])\n    layer = Embedding(max_words,50,input_length=max_len)(inputs)\n    layer = LSTM(64)(layer)\n    layer = Dense(256,name='FC1')(layer)\n    layer = Activation('relu')(layer)\n    layer = Dropout(0.5)(layer)\n    layer = Dense(1,name='out_layer')(layer)\n    layer = Activation('sigmoid')(layer)\n    model = Model(inputs=inputs,outputs=layer)\n    return model\n\ndef test_model(model, tok=None, test_path=\"/kaggle/input/nlp-getting-started/test.csv\", cut=0.5):\n    \n    my_df, res = read_test(test_path=\"/kaggle/input/nlp-getting-started/test.csv\")\n    \n    X = split_data(my_df, _t=False)\n    sequences_matrix, tok = process_data(X, tok=tok)\n    preds = model.predict(sequences_matrix)\n\n    normalized_preds = []\n    for p in preds:\n        if p >= cut:\n            normalized_preds.append(1)\n            \n        else:\n            normalized_preds.append(0)\n    #dump_preds(res, normalized_preds, out=\"res_rnn.csv\")\n    return res\n\ndef tune_cutoff(model, tok):\n    \n    x_tune = df['text'].values\n    x_target = df[['target']]\n    \n    \n    X, Y = split_data(df)\n    sequences_matrix, tok = process_data(X, tok=tok)\n    preds = model.predict(sequences_matrix)\n    \n    x_target[\"preds\"] = preds\n    \n    accumulator = 0\n    _0 = []\n    max_0 = 0\n    _1 = []\n    min_1 = 0\n    \n    for i, row in x_target.iterrows():\n        if row['target'] == 0:\n            _0.append(row['preds'])\n            if row['preds'] > max_0:\n                max_0 = row['preds']\n        else:\n            _1.append(row['preds'])\n            if row['preds'] < min_1:\n                min_1 = row['preds']\n\n    mean_0 = sum(_0)/len(_0)\n    mean_1 = sum(_1)/len(_1)\n    return max(mean_0, mean_1)/min(mean_0, mean_1)\n    \n\n#model = RNN()\n#model.summary()\n#model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])\n#X, Y = split_data(df)\n#sequences_matrix, tok = process_data(X, tok=None)\n\n#model.fit(sequences_matrix,Y,batch_size=128,epochs=10,validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.001)])\n\n    \n#cut = tune_cutoff(model, tok)\n\n#test_model(model, tok=tok, cut=cut)\n#0.57259\n#autocut: 0.57055","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Test Glove LSTM\nHere we check the usage of Glove embeddings. We first encode the sentence with average word Glove embedding and then we use a Bi-LSTM to classify the sample representation. \nWe reach a new best of 79.45, meaning that the embedding direction seem to be valuable. We are gonna test some other newer embeddings."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport re  #regular expression\nfrom bs4 import BeautifulSoup\nimport pandas as pd \nfrom sklearn import model_selection, preprocessing\nimport os\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Dense, Input, GlobalMaxPooling1D, Conv1D, MaxPooling1D, Embedding, Dropout, Bidirectional\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.initializers import Constant\nfrom tensorflow.keras import metrics\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n\n    \nvocab_size = 10000\nembedding_dim = 100\nmax_length = 50\ntrunc_type='post'\npadding_type='post'\noov_tok = \"<OOV>\"\nvalidation_split = 0.10\n\ndef load_glove_vectors():\n    print('Indexing word vectors.')\n    #Many thanks to rtatman for hosting the GloVe word embeddings dataset on Kaggle\n    #https://www.kaggle.com/rtatman/glove-global-vectors-for-word-representation\n    GLOVE_DIR = '/kaggle/input/glove6b100dtxt/'\n    embeddings_index = {}\n    print(list(os.walk(\"/kaggle/input\")))\n    with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')) as f:\n        for line in f:\n            word, coefs = line.split(maxsplit=1)\n            coefs = np.fromstring(coefs, 'f', sep=' ')\n            embeddings_index[word] = coefs\n    print('Found %s word vectors.' % len(embeddings_index))\n    return embeddings_index\n\ndef tokenize(df):\n    tokenizer = Tokenizer(num_words = vocab_size, oov_token = oov_tok)\n    tokenizer.fit_on_texts(df.text)\n    word_index = tokenizer.word_index\n    print('Found %s unique tokens.' % len(word_index))\n\n    training_sequences = tokenizer.texts_to_sequences(df.text)\n    training_padded = pad_sequences(training_sequences, maxlen = max_length, \n                                    padding = padding_type, truncating = trunc_type)\n    print('Shape of the data vector is', training_padded.shape, df.target.shape)\n    \n    return training_sequences, training_padded, word_index, tokenizer\n\ndef prepare_embedding_layer(word_index, embeddings_index):\n    print('Preparing the embedding matrix')\n    num_words = min(vocab_size, len(word_index) + 1)\n    embedding_matrix = np.zeros((num_words, embedding_dim))\n    for word, index in word_index.items():\n        if index >= vocab_size:\n            continue\n        embedding_vector = embeddings_index.get(word, np.zeros(embedding_dim, dtype='float32'))\n        \n        if embedding_vector is not None:\n            embedding_matrix[index] = embedding_vector\n\n    embedding_layer = Embedding(num_words, embedding_dim, \n                           embeddings_initializer = Constant(embedding_matrix), \n                           input_length = max_length, \n                           trainable = False)\n    \n    \n    return embedding_layer\n\ndef model(embedding_layer):\n    \n    METRICS = [\n          metrics.BinaryAccuracy(name='accuracy'),\n          metrics.Precision(name='precision'),\n          metrics.Recall(name='recall'),\n          metrics.AUC(name='auc')]\n\n    sequence_input = Input(shape = (max_length, ))\n    embedded_sequences = embedding_layer(sequence_input)\n    x = Bidirectional(tf.keras.layers.LSTM(64))(embedded_sequences)\n    x = Dropout(0.5)(x)\n    x = Dense(32, activation = 'relu')(x)\n    x = Dropout(0.5)(x)\n    output = Dense(1, activation = 'sigmoid')(x)\n    model =  Model(sequence_input, output)\n    model.compile(loss='binary_crossentropy',optimizer=tf.keras.optimizers.Adam(lr = .0002) ,metrics = METRICS)\n    return model\n\ndef test_model(model, test_df, tokenizer):\n    test_sequences = tokenizer.texts_to_sequences(test_df.text)\n    test_padded = pad_sequences(test_sequences, maxlen = max_length, \n                                    padding = padding_type, truncating = trunc_type)\n    predictions = model.predict(test_padded)\n    predictions = np.round(predictions).astype(int).flatten()\n    dump_preds(res, predictions, out=\"submission.csv\")\n\n\"\"\"\nembeddings_index = load_glove_vectors()\ntest_df, res = read_test(test_path=\"/kaggle/input/nlp-getting-started/test.csv\")\ntraining_sequences, training_padded, word_index,tokenizer = tokenize(df)\nembedding_layer =  prepare_embedding_layer(word_index, embeddings_index)\n\nX_train, X_valid, y_train, y_valid = model_selection.train_test_split(training_padded, \n                                                                          df.target, \n                                                                          test_size = validation_split, \n                                                                          random_state=1)\n\nmodel = model(embedding_layer)\n\nearly_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor='val_auc', \n    verbose=1,\n    patience=10,\n    mode='max',\n    restore_best_weights=True)\nhistory = model.fit(X_train, y_train, batch_size = 64, epochs = 30, \n                    callbacks = [early_stopping],\n                    validation_data = (X_valid, y_valid))\n\ntest_model(model, test_df, tokenizer)\n#0.7945\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# BERT TfHub\nUsing the BERT TfHub module, we build a DNN using BERT Embeddings and a simple Dense layer with sigmoid on the top. \nThis is the best scoring model so fa, reaching a best of 82.45%"},{"metadata":{"trusted":true},"cell_type":"code","source":"!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py\n\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.layers import Dense, Dropout, Bidirectional,  Conv1D, MaxPooling1D\nfrom tensorflow.keras.models import Model\nimport tensorflow_hub as hub\n\nimport tokenization\n\ndef bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n\ndef build_model(bert_layer, max_len=512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    \n    \"\"\"\n    x = Bidirectional(tf.keras.layers.LSTM(64))(sequence_output)\n    x = Dropout(0.1)(x)\n    x = Dense(32, activation = 'relu')(x)\n    x = Dropout(0.1)(x)\n    out = Dense(1, activation = 'sigmoid')(x)\n    #out = Dense(1, activation='sigmoid')(clf_output)\n    \"\"\"\n    kernel_size = 5\n    filters = 64\n    pool_size = 4\n    \n    x = Dropout(0.1)(sequence_output)\n    x = Conv1D(filters,\n                 kernel_size,\n                 padding='valid',\n                 activation='relu',\n                 strides=1)(x)\n    x = MaxPooling1D(pool_size=pool_size)(x)\n    x = Bidirectional(tf.keras.layers.LSTM(64))(x)\n    out = Dense(1, activation=\"sigmoid\")(x)\n\n    \n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model\n\ntest_df, res = read_test(test_path=\"/kaggle/input/nlp-getting-started/test.csv\")\n\nmodule_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\nbert_layer = hub.KerasLayer(module_url, trainable=True)\n\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n\ntrain_input = bert_encode(df.text.values, tokenizer, max_len=160)\ntest_input = bert_encode(test_df.text.values, tokenizer, max_len=160)\ntrain_labels = df.target.values\n\nmodel = build_model(bert_layer, max_len=160)\nmodel.summary()\n\n\nearly_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor='val_auc', \n    verbose=1,\n    patience=4,\n    mode='max',\n    restore_best_weights=True)\n\ntrain_history = model.fit(\n    train_input, train_labels,\n    validation_split=0.15,\n    epochs=20,\n    batch_size=16,\n    callbacks=[early_stopping]\n)\n\ntest_pred = model.predict(test_input)\n_t = []\nfor t in test_pred:\n    if t < 0.5:\n        _t.append(0)\n    else:\n        _t.append(1)\ndump_preds(res, _t, out=\"submission.csv\")\n#82.45\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XLNet / RoBERTa in 4 lines!\nHere the XLNet/ GPT-2 / RoBERTa evaluation using the huggingface simpletransformers."},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install simpletransformers\n!git clone --recursive https://github.com/NVIDIA/apex.git\n!pip install --upgrade --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" apex/. \nimport os\nimport shutil\n\nshutil.rmtree('apex')\nif os.path.exists(\"cache_dir\"):\n    shutil.rmtree('cache_dir')\n\nif os.path.exists(\"outputs\"):\n    shutil.rmtree('outputs')\n\nif os.path.exists(\"runs\"):\n    shutil.rmtree('runs')\n\nimport torch\nimport random \nimport numpy as np\n\nfrom simpletransformers.classification import ClassificationModel\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold, StratifiedKFold\nseed = 98\n\"\"\"\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\n\ntrain_data = df[['text', 'target']]\n\nprint(\"Building Model\")\ntrain_args = {\n    \"train_batch_size\": 16,\n    'eval_batch_size': 4,\n    \"num_train_epochs\": 10,\n    \"use_early_stopping\": True,\n    \"early_stopping_patience\": 3,\n    \"early_stopping_delta\": 0.005,\n    'max_seq_length': 200,  \n    'save_model_every_epoch': False,\n    'overwrite_output_dir': True,\n    'save_eval_checkpoints': False,\n    \"gradient_accumulation_steps\": 1,\n    \"overwrite_output_dir\": True\n}\nmodel = ClassificationModel(\"xlnet\", 'xlnet-base-cased', num_labels=2, args=train_args)\n#model = ClassificationModel(\"xlmroberta\", 'xlm-roberta-base', num_labels=2, args=train_args)\n\n\nprint(\"Training the model\")\nmodel.train_model(train_data)\n\n\ntest_df, res = read_test(test_path=\"/kaggle/input/nlp-getting-started/test.csv\")\n\nprint(\"Model evaluation\")\npredictions, raw_outputs = model.predict(test_df['text'])\n\ndump_preds(res, predictions, out=\"submission_xlnet.csv\")\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Test SuperLearner ensemble\nFinally we try a super-learner ensemble. Meaning that we use a set of models (also the previous BERT used) to classify the samples and then we use a meta-model to ingest these classification results (from all models) and classify the sample. \nIt is ensured that this ensemble cannot perform worse than the best in the ensemble, so we could enhance the BERT model capabilities.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# example of a super learner model for binary classification\n\n!pip install tamnun\nfrom tamnun.bert import BertClassifier, BertVectorizer\nfrom numpy import hstack\nfrom numpy import vstack\nfrom numpy import asarray\nfrom sklearn.datasets.samples_generator import make_blobs\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.base import TransformerMixin \nfrom tamnun.bert import BertClassifier, BertVectorizer\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import StratifiedKFold\nimport copy\n\nclass DenseTransformer(TransformerMixin):\n\n    def fit(self, X, y=None, **fit_params):\n        return self\n\n    def transform(self, X, y=None, **fit_params):\n        return X.todense()\n    \n# create a list of base-models\ndef get_models():\n    models = []\n    tfidf_vectorizer = TfidfVectorizer(\n        analyzer='word',\n        tokenizer=lambda x: x,\n        preprocessor=lambda x: x,\n        token_pattern=None\n    )\n    lr = LogisticRegression()\n    tfidf_lr_pipe = Pipeline([('tfidf', tfidf_vectorizer), ('lr', lr)])\n    dec_pipe = Pipeline([('tfidf', tfidf_vectorizer), ('dt', DecisionTreeClassifier())])\n    svc = Pipeline([('tfidf', tfidf_vectorizer), ('svc', SVC(gamma='scale', probability=True))])\n    gaus = Pipeline([('tfidf', tfidf_vectorizer),('to_dense', DenseTransformer()),  ('gaus', GaussianNB())])\n    kn = Pipeline([('tfidf', tfidf_vectorizer), ('kn', KNeighborsClassifier())])\n    ada = Pipeline([('tfidf', tfidf_vectorizer), ('ada',AdaBoostClassifier())])\n    bagging =  Pipeline([('tfidf', tfidf_vectorizer), ('bag',BaggingClassifier(n_estimators=10))])\n    ran_forest = Pipeline([('tfidf', tfidf_vectorizer), ('ran',RandomForestClassifier(n_estimators=10))])\n    extra_tree = Pipeline([('tfidf', tfidf_vectorizer), ('extr',ExtraTreesClassifier(n_estimators=10))])\n    gradient_boosting = Pipeline([('vect', CountVectorizer()),\n                     ('tfidf', TfidfTransformer()),\n                     ('clf', GradientBoostingClassifier(n_estimators=100)),\n                     ])\n    #bert =  Pipeline([('extr',BertVectorizer()), (\"extr2\",BertClassifier(num_of_classes=2))])\n    \n    module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\n    bert_layer = hub.KerasLayer(module_url, trainable=True)\n\n    vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n    do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n    tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n\n    model = build_model(bert_layer, max_len=160)\n    model.summary()\n    \n   # models.append(bert)\n    models.append(model)\n    models.append(dec_pipe)\n    models.append(tfidf_lr_pipe)\n    models.append(svc)\n    models.append(gaus)\n    models.append(kn)\n    models.append(ada)\n    models.append(bagging)\n    models.append(ran_forest)\n    models.append(extra_tree)\n    models.append(gradient_boosting)\n    \n    return models, tokenizer\n\n# collect out of fold predictions form k-fold cross validation\ndef get_out_of_fold_predictions(X, y, models, tokenizer):\n\tmeta_X, meta_y = list(), list()\n\t# define split of data\n\tkfold = StratifiedKFold(n_splits=5, shuffle=True)\n\t# enumerate splits\n\tfor train_ix, test_ix in kfold.split(X, y):\n\t\tfold_yhats = list()\n\t\ttrain_X, test_X = X[train_ix], X[test_ix]\n\t\ttrain_y, test_y = y[train_ix], y[test_ix]\n\t\tmeta_y.extend(test_y)\n\t\t# fit and make predictions with each sub-model\n\t\tfor i, model in enumerate(models):\n\t\t\tif i == 0:\n\t\t\t\ttrain_input = bert_encode(train_X, tokenizer, max_len=160)\n\t\t\t\tearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_auc', verbose=1, patience=10, mode='max', restore_best_weights=True)\n\t\t\t\tmodel.fit(train_input, train_y, validation_split=0.25, epochs=10,batch_size=16,  callbacks=[early_stopping])\n\t\t\t\t_test_X = bert_encode(test_X, tokenizer, max_len=160)\n\t\t\t\tyhat = model.predict(_test_X)\n\t\t\t\t_y = []\n\t\t\t\tfor __y in yhat:\n\t\t\t\t\tone_prob = __y[0]\n\t\t\t\t\tzero_prob = 1 - one_prob\n\t\t\t\t\t_y.append([zero_prob, one_prob])\n\t\t\t\tyhat = _y\n\t\t\telse:\n\t\t\t\tmodel.fit(train_X, train_y)\n\t\t\t\tyhat = model.predict_proba(test_X)\n\t\t\t# store columns\n\t\t\tfold_yhats.append(yhat)\n\t\t# store fold yhats as columns\n\t\tmeta_X.append(hstack(fold_yhats))\n\treturn vstack(meta_X), asarray(meta_y)\n\n# fit all base models on the training dataset\ndef fit_base_models(X, y, models, bert_tok):\n\tfor i,model in enumerate(models):\n\t\tif i == 0:\n\t\t\ttrain_input = bert_encode(X, bert_tok, max_len=160)\n\t\t\tearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_auc', verbose=1, patience=10, mode='max', restore_best_weights=True)\n\t\t\tmodel.fit(train_input, y, validation_split=0.10, epochs=10,batch_size=16,  callbacks=[early_stopping])\n\t\telse:  \n\t\t\tmodel.fit(X, y)\n\n# fit a meta model\ndef fit_meta_model(X, y):\n\tmodel = ExtraTreesClassifier(n_estimators=30)\n\tmodel.fit(X, y)\n\treturn model\n\n\n# make predictions with stacked model\ndef super_learner_predictions(X, models, meta_model, bert_tok):\n\tmeta_X = list()\n\tfor i, model in enumerate(models):\n\t\tif i == 0:\n\t\t\tx = bert_encode(X, bert_tok, max_len=160)\n\t\t\tyhat = model.predict(x)\n\t\t\t_y = []\n\t\t\tfor y in yhat:\n\t\t\t\tone_prob = y[0]\n\t\t\t\tzero_prob = 1 - one_prob\n\t\t\t\t_y.append([zero_prob, one_prob])\n\t\t\tyhat =_y\n\t\telse:\n\t\t\tyhat = model.predict_proba(X)\n\t\tmeta_X.append(yhat)\n\tmeta_X = hstack(meta_X)\n\t# predict\n\treturn meta_model.predict(meta_X)\n\n\n\"\"\"\ntest_df, res = read_test(test_path=\"/kaggle/input/nlp-getting-started/test.csv\")\nX, y = split_data(df)\nX_test = split_data(test_df, _t=False)\n\n# get models\nmodels, bert_tok = get_models()\n# get out of fold predictions\nmeta_X, meta_y = get_out_of_fold_predictions(X, y, models,bert_tok)\nprint('Meta ', meta_X.shape, meta_y.shape)\n# fit base models\nfit_base_models(X, y, models, bert_tok)\n# fit the meta model\nmeta_model = fit_meta_model(meta_X, meta_y)\n\n# evaluate meta model\nyhat = super_learner_predictions(X_test, models, meta_model, bert_tok)\n\nprint(\"YHat: {}\".format(yhat))\ndump_preds(res, yhat, out=\"submission_ensemble.csv\")\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Embedding Stacking!\nUnfortunately the GPU allowed in Kaggle is not enough to keep multiple embeddings, if Elmo is one of them. So this is just an example code, witouth an official score."},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install flair\nfrom flair.data import Corpus\nfrom flair.datasets import TREC_6\nfrom flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentRNNEmbeddings, StackedEmbeddings, BertEmbeddings, ELMoEmbeddings\nfrom flair.models import TextClassifier\nfrom flair.trainers import ModelTrainer\nfrom flair.data import Sentence\nimport pandas as pd\nfrom keras.layers import Input, Dense, GRU, Bidirectional, Flatten\nfrom keras.optimizers import Adam\nfrom keras.models import Model\nimport numpy as np\n\n\ndef generateTrainingData(dataset, batch_size, max_length, num_classes, emb_size,\n                        stacked_embedding):\n  \n  x_batch = []\n  y_batch = []\n  while True:\n    data = dataset.sample(frac=1)\n    for index, row in data.iterrows():\n \n        my_sent = row[\"text\"]\n        sentence = Sentence(my_sent)\n        stacked_embedding.embed(sentence)\n        \n        x = []\n        for token in sentence:\n          x.append(token.embedding.cpu().detach().numpy())\n          if len(x) == max_length:\n            break\n        \n        while len(x) < max_length:\n          x.append(np.zeros(emb_size))\n        \n        y = np.zeros(num_classes)\n        y[row[\"target\"]] = 1\n        \n        x_batch.append(x)            \n        y_batch.append(y)\n\n        if len(y_batch) == batch_size:\n          yield np.array(x_batch), np.array(y_batch)\n\n          x_batch = []\n          y_batch = []\n\ndef generatePredictionData(dataset, batch_size, max_length, num_classes, emb_size, stacked_embedding):\n  \n  x_batch = []\n  while True:\n    for text in dataset['text'].values:\n \n        my_sent = text\n        sentence = Sentence(my_sent)\n        stacked_embedding.embed(sentence)\n        \n        x = []\n        for token in sentence:\n          x.append(token.embedding.cpu().detach().numpy())\n          if len(x) == max_length:\n            break\n        \n        while len(x) < max_length:\n          x.append(np.zeros(emb_size))\n          \n        x_batch.append(x)            \n        if len(x_batch) == batch_size:\n          yield np.array(x_batch)\n\n          x_batch = []\n\ndef get_stacked_embeddings():\n  stacked_embedding = StackedEmbeddings([ELMoEmbeddings(), WordEmbeddings(\"en\")])\n  print(\"Stacked embedding size: {}\".format(stacked_embedding.embedding_length))\n  embedding_size = stacked_embedding.embedding_length\n  return stacked_embedding, embedding_size\n\ndef declare_model(batch_size, max_len, emb_size, gru_size, num_classes):\n \n  sample = Input(batch_shape=(batch_size, max_len, emb_size))\n  gru_out = Bidirectional(GRU(gru_size, return_sequences=True))(sample)\n  gru_out = Flatten()(gru_out)\n  predictions = Dense(num_classes, activation='sigmoid')(gru_out)\n\n  model = Model(inputs=sample, outputs=[predictions])\n  model.compile(optimizer=Adam(),loss='binary_crossentropy', metrics=[\"acc\"])\n  print(model.summary())\n\n  return model\n\n\"\"\"\nBATCH_SIZE = 256\nMAX_LEN = 150\nGRU_SIZE = 20\nNUM_CLASSES=2 \nEPOCHS = 1\n\nstacked_embedding, embedding_length = get_stacked_embeddings()\n\nm = declare_model(batch_size=BATCH_SIZE, max_len=MAX_LEN, emb_size=embedding_length, gru_size=GRU_SIZE, num_classes=NUM_CLASSES)\n\n\ngen = generateTrainingData(df, batch_size=BATCH_SIZE, max_length=MAX_LEN, num_classes=NUM_CLASSES, emb_size=embedding_length,\n  stacked_embedding= stacked_embedding)\nprint(gen)\nsteps_per_epoch = len(df)/BATCH_SIZE\nm.fit_generator(gen, steps_per_epoch=1, epochs=EPOCHS, workers=1)\n\ndf_test = df[:10]\ntest_gen = list(generatePredictionData(df_test,  batch_size=BATCH_SIZE, max_length=MAX_LEN, num_classes=NUM_CLASSES, \n        emb_size=embedding_length, stacked_embedding=stacked_embedding))\n\nprint(np.argmax(m.predict_generator(test_gen, steps=1), axis=1))\n\"\"\"\n\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}