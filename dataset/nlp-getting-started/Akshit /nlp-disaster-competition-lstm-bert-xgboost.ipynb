{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction:\n\n## 1. I am creating this notebook to help others who want to learn NLP.\n## 2. I will be using all the concepts that I know. Also I will try to give all the detailed explanation of the concepts "},{"metadata":{},"cell_type":"markdown","source":"# Data Dictionary:\n\n## 1.text - It represents the text of the tweet\n\n## 2.keyword - A \"Particular Word\" from that tweet (although this may be blank!) About Keyword: Keyword targeting allows you to connect with users based on words and phrases they've recently Tweeted or searched for on Twitter. This marketing capability allows you to reach your target audience when your business is most relevant to them.\n\n## 3.Location - Location of the Tweet\n\n## 4.Target - You are predicting whether a given tweet is about a real disaster or not. 1 rep Disaster Tweet and 0 Represents Not a Disaster\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nimport nltk\nimport missingno as msno\nfrom wordcloud import WordCloud, STOPWORDS\nimport string\nimport re\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.initializers import Constant\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n%matplotlib inline\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/nlp-getting-started/train.csv')\ntest = pd.read_csv('../input/nlp-getting-started/test.csv')\nsubmission = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.head())\nprint(train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test.head())\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking all Null values\nprint(train.isnull().sum())\nprint(test.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting the missing values\nmsno.bar(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Location and Keyword are showing the maximum null values\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking unique values\ntrain.location.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.keyword.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1.It looks like the locations were entered manually \n# 2.We cannot fill location with any of the location neither we can fill the keywords with some unique keyword, so I will be filling the null values by No keyword and No location."},{"metadata":{},"cell_type":"markdown","source":"# EDA + Data Preprocessing + Null value Imputations"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Target count\nplt.figure(figsize = (10, 8))\nuniques = train[\"target\"].value_counts()\nsns.barplot(x = uniques.index, y = uniques.values, data = uniques)\nplt.xlabel(\"Target Values\")\nplt.ylabel(\"Count Values\")\nsns.despine(left = True, bottom = True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plottin the most repetitive words in \"text\" column\nstopwords = set(STOPWORDS)\ndef word_cloud(data, title = None):\n    cloud = WordCloud(background_color = \"black\",\n      stopwords = stopwords,\n      max_words=200,\n      max_font_size=40, \n      scale=3,).generate(str(data))\n    fig = plt.figure(figsize= (15, 15))\n    plt.axis(\"off\")\n    if title: \n        fig.suptitle(title, fontsize=20)\n        fig.subplots_adjust(top=2.25)\n\n    plt.imshow(cloud)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Most repeated words in real disaster tweets,\n#making a word cloud\nword_cloud(train[train[\"target\"] == 1][\"text\"], \"Most repeated words in real disaster tweets in train data\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_cloud(test[\"text\"], \"Most repeated words in test['text']\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of keywords in real and fake tweets \nplt.figure(figsize = (10, 80), dpi = 100)\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=12)\nsns.countplot(y = \"keyword\", hue = \"target\", data = train)\nplt.legend(loc = 1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Null Value Imputation"},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"keyword\"].fillna(value = \"No_Keyword\", inplace = True)\ntrain[\"location\"].fillna(value = \"No_Location\", inplace = True)\ntest[\"keyword\"].fillna(value = \"No_Keyword\", inplace = True)\ntest[\"location\"].fillna(value = \"No_Location\", inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Getting the count of words"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Word count \n#splitting on the spaces availaible\ntrain['word_count'] = train[\"text\"].apply(lambda x: len(str(x).split(\" \")))#\nprint(train[['text','word_count']].head())\n# for test\ntest['word_count'] = train[\"text\"].apply(lambda x: len(str(x).split(\" \")))\ntest[['text','word_count']].head()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Getting the number of charachters"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Counting the number of charachters\n#This also includes spaces\ntrain['char_count'] = train['text'].str.len() \nprint(train[['text','char_count']].head())\n# For test dataset\ntest['char_count'] = train['text'].str.len() \ntest[['text','char_count']].head()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Calculating the average word length"},{"metadata":{"trusted":true},"cell_type":"code","source":"def avg_word(sentence):\n    words = sentence.split()\n    return (sum(len(word) for word in words)/len(words))\n\ntrain['avg_word'] = train['text'].apply(lambda x: avg_word(\"text\"))\nprint(train[['text','avg_word']].head())\n# Test dataset\ntest['avg_word'] = train['text'].apply(lambda x: avg_word(\"text\"))\ntest[['text','avg_word']].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Getting StopWords\n\n## Stop words are the common reoccuring words that we usually remove while making a NLP model but its better to extract as much features as possible for creating a model."},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntrain[\"num_stopwords\"] = train[\"text\"].apply(lambda x : len([word for word in str(x).lower().split()\\\n                                                             if word in stopwords]))\ntest[\"num_stopwords\"] = test[\"text\"].apply(lambda x : len([word for word in str(x).lower().split()\\\n                                                             if word in stopwords]))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Adding some extra features like count of unique words, Url, hashtags if there are any"},{"metadata":{"trusted":true},"cell_type":"code","source":"# unique_word_count\ntrain['unique_word_count'] = train['text'].apply(lambda x: len(set(str(x).split())))\ntest['unique_word_count'] = test['text'].apply(lambda x: len(set(str(x).split())))\n\n#Url counts\ntrain['url_count'] = train['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\ntest['url_count'] = test['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n\n#Hashtags count\ntrain['hashtag_count'] = train['text'].apply(lambda x: len([c for c in str(x) if c == '#']))\ntest['hashtag_count'] = test['text'].apply(lambda x: len([c for c in str(x) if c == '#']))\n\n#Mentiion word count\ntrain['mention_count'] = train['text'].apply(lambda x: len([c for c in str(x) if c == '@']))\ntest['mention_count'] = test['text'].apply(lambda x: len([c for c in str(x) if c == '@']))\n\n#numerical values in data \ntrain['numerics'] = train['text'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\ntest['numerics'] = test['text'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n\n#uppercase letters in data\ntrain['Upper'] = train['text'].apply(lambda x: len([x for x in x.split() if x.isupper()]))\ntest['Upper'] = test['text'].apply(lambda x: len([x for x in x.split() if x.isupper()]))\n\n#calculating punctuations\ntrain[\"num_punctuation\"] = train[\"text\"].apply(lambda x : len([p for p in x.split() if p in string.punctuation]))\ntest[\"num_punctuation\"] = test[\"text\"].apply(lambda x : len([p for p in x.split() if p in string.punctuation]))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking the dataset till now\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking the dataset till now\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Calculating Ngrams \n## :N-grams are the combination of multiple words used together. Ngrams with N=1 are called unigrams. Similarly, bigrams (N=2), trigrams (N=3) and so on can also be used.\n## Unigrams do not usually contain as much information as compared to bigrams and trigrams. The basic principle behind n-grams is that they capture the language structure, like what letter or word is likely to follow the given one. The longer the n-gram (the higher the n), the more context you have to work with.\n## Optimum length really depends on the application – if your n-grams are too short, you may fail to capture important differences."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Unigrams\nfrom collections import defaultdict\ntrain0 = train[train[\"target\"] == 0]\ntrain1 = train[train[\"target\"] == 1]\n\n## custom function for ngram generation ##\ndef generate_ngrams(text, n_gram=1):\n    \n    token = [token for token in text.lower().split() if token != \"\" if token not in stopwords]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [\" \".join(ngram) for ngram in ngrams]\n\n\nfreq_dict = defaultdict(int)\nfor sent in train0[\"text\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted0 = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted0.columns = [\"word\", \"wordcount\"]\n\n\nfreq_dict = defaultdict(int)\nfor sent in train1[\"text\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted1 = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted1.columns = [\"word\", \"wordcount\"]\n\nfig, axes = plt.subplots(1, 2, figsize = (12, 12))\nplt.tight_layout()\nsns.despine()\nfor i in range(2):\n    sns.barplot(x = \"wordcount\", y = \"word\", data = globals()[\"fd_sorted\" + str(i)].iloc[:50, :], ax = axes[i])\n    axes[i].set_xlabel('Count', fontsize=12)\n    axes[i].set_title(f\"Most repetitive words in {i} class\", fontsize=15)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Calculating Bigrams\n\ntrain0 = train[train[\"target\"] == 0]\ntrain1 = train[train[\"target\"] == 1]\n\n## custom function for ngram generation ##\ndef generate_ngrams(text, n_gram=2):\n    \n    token = [token for token in text.lower().split() if token != \"\" if token not in stopwords]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [\" \".join(ngram) for ngram in ngrams]\n\n\nfreq_dict = defaultdict(int)\nfor sent in train0[\"text\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted0 = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted0.columns = [\"word\", \"wordcount\"]\n\n\nfreq_dict = defaultdict(int)\nfor sent in train1[\"text\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted1 = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted1.columns = [\"word\", \"wordcount\"]\n\nfig, axes = plt.subplots(1, 2, figsize = (12, 12))\nplt.tight_layout()\nsns.despine()\nfor i in range(2):\n    sns.barplot(x = \"wordcount\", y = \"word\", data = globals()[\"fd_sorted\" + str(i)].iloc[:50, :], ax = axes[i])\n    axes[i].set_xlabel('Count', fontsize=12)\n    axes[i].set_title(f\"Most repetitive words in {i} class\", fontsize=15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Calculating trigrams\ntrain0 = train[train[\"target\"] == 0]\ntrain1 = train[train[\"target\"] == 1]\n\n## custom function for ngram generation ##\ndef generate_ngrams(text, n_gram=3):\n    \n    token = [token for token in text.lower().split() if token != \"\" if token not in stopwords]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [\" \".join(ngram) for ngram in ngrams]\n\n\nfreq_dict = defaultdict(int)\nfor sent in train0[\"text\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted0 = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted0.columns = [\"word\", \"wordcount\"]\n\n\nfreq_dict = defaultdict(int)\nfor sent in train1[\"text\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted1 = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted1.columns = [\"word\", \"wordcount\"]\n\nfig, axes = plt.subplots(1, 2, figsize = (12, 12))\nplt.tight_layout()\nsns.despine()\nfor i in range(2):\n    sns.barplot(x = \"wordcount\", y = \"word\", data = globals()[\"fd_sorted\" + str(i)].iloc[:50, :], ax = axes[i])\n    axes[i].set_xlabel('Count', fontsize=12)\n    axes[i].set_title(f\"Most repetitive words in {i} class\", fontsize=15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Glove Embeddings:\n\n## Like we have one hot encoding and label encoding in regular Ml models we have glove embeddings in NLP models. Its pre defined vector representations of different words."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = train.append(test, ignore_index = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Glove Embedding\nembedding_dict={}\nwith open('../input/glove42b300dtxt/glove.42B.300d.txt','r') as f:\n    for line in f:\n        values=line.split()\n        word = values[0]\n        vectors=np.asarray(values[1:],'float32')\n        embedding_dict[word]=vectors\nf.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(embedding_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_vocab(X):\n    \n    tweets = X.apply(lambda s: s.split()).values      \n    vocab = {}\n    \n    for tweet in tweets:\n        for word in tweet:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1                \n    return vocab\n\n\ndef check_embeddings_coverage(X, embeddings):\n    vocab = build_vocab(X)    \n    covered = {}\n    oov = {}    \n    n_covered = 0\n    n_oov = 0\n    \n    for word in vocab:\n        try:\n            covered[word] = embeddings[word]\n            n_covered += vocab[word]\n        except:\n            oov[word] = vocab[word]\n            n_oov += vocab[word]\n    return covered, oov, n_covered, n_oov","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"covered, oov, n_covered, n_oov = check_embeddings_coverage(df[\"text\"], embedding_dict)\nprint(f\"Number of words covered by Glove embeddings --> {n_covered}\")\nprint(f\"Number of words not covered by Glove embeddings --> {n_oov}\")\nprint(f\"Percentage of words covered by Glove embeddings --> {(n_covered/(n_covered + n_oov)) * 100}%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Text Cleaning\n\n## Tweets require a lot of cleaning process and its really difficult to clean all of the tweets one by one.\n## So in order to solve this problem I will deploy a general approach. I will define a function to clean most of the unwanted charachters like hashtags, emails, special charachters etc."},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"text\"] = df[\"text\"].apply(lambda x : x.lower())\ndf[\"keyword\"].fillna(\"keyword\", inplace = True)\ndf[\"text\"] = df[\"text\"] + \" \" + df[\"keyword\"]\ndf.drop([\"keyword\", \"location\"], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_all_words = \" \".join(df[\"text\"])\nnot_english = [word for word in list_all_words.split() if word.isalpha() == False]\n\ndef clean_data(data):\n    # Remove urls\n    data = re.sub(r'https?\\S+', '', data)\n    # Remove html tags\n    data = re.sub(r\"<.*?>\", \"\", data)\n    # Remove punctuations\n    t = [w for w in data if w not in string.punctuation]\n    data = \"\".join(t)\n    # Remove stopwords\n    t = [w for w in data.split() if w not in stopwords]\n    data = \" \".join(t)\n    # Removing numbers from text\n    data = re.sub(r\"\\d+\", \"\", data)\n\n    data = re.sub(r\"\\x89Û_\", \"\", data)\n    data = re.sub(r\"\\x89ÛÒ\", \"\", data)\n    data = re.sub(r\"\\x89ÛÓ\", \"\", data)\n    data = re.sub(r\"\\x89ÛÏWhen\", \"When\", data)\n    data = re.sub(r\"\\x89ÛÏ\", \"\", data)\n    data = re.sub(r\"China\\x89Ûªs\", \"China's\", data)\n    data = re.sub(r\"let\\x89Ûªs\", \"let's\", data)\n    data = re.sub(r\"\\x89Û÷\", \"\", data)\n    data = re.sub(r\"\\x89Ûª\", \"\", data)\n    data = re.sub(r\"\\x89Û\\x9d\", \"\", data)\n    data = re.sub(r\"å_\", \"\", data)\n    data = re.sub(r\"\\x89Û¢\", \"\", data)\n    data = re.sub(r\"\\x89Û¢åÊ\", \"\", data)\n    data = re.sub(r\"fromåÊwounds\", \"from wounds\", data)\n    data = re.sub(r\"åÊ\", \"\", data)\n    data = re.sub(r\"åÈ\", \"\", data)\n    data = re.sub(r\"JapÌ_n\", \"Japan\", data)    \n    data = re.sub(r\"Ì©\", \"e\", data)\n    data = re.sub(r\"å¨\", \"\", data)\n    data = re.sub(r\"SuruÌ¤\", \"Suruc\", data)\n    data = re.sub(r\"åÇ\", \"\", data)\n    data = re.sub(r\"å£3million\", \"3 million\", data)\n    data = re.sub(r\"åÀ\", \"\", data)\n    \n    # Remove words not alphabets\n    t = [w for w in data.split() if w not in not_english]\n    data = \" \".join(t)\n    \n    return data    \n\n\ndf[\"text\"] = df[\"text\"].apply(lambda x : clean_data(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking again after cleaning the data\ncovered, oov, n_covered, n_oov = check_embeddings_coverage(df[\"text\"], embedding_dict)\nprint(f\"Number of words covered by Glove embeddings --> {n_covered}\")\nprint(f\"Number of words not covered by Glove embeddings --> {n_oov}\")\nprint(f\"Percentage of words covered by Glove embeddings --> {(n_covered/(n_covered + n_oov)) * 100}%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 90% of the words have been converted into vectors"},{"metadata":{},"cell_type":"markdown","source":"# Bi-directional LSTM with Glove embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"embed_size = 300 # how big is each word vector\nmaxlen = 20 # max number of words in a comment to use\nmax_features = 20000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer(oov_token = \"<OOV>\", num_words = max_features)\ntokenizer.fit_on_texts(df[\"text\"])\nword_index = tokenizer.word_index\nsequences = tokenizer.texts_to_sequences(df[\"text\"])\npadded = pad_sequences(sequences, padding = \"post\", maxlen = maxlen)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x = padded[:7613, :]\ntest = padded[7613:, :]\ntrain_y = df[df[\"target\"].isnull() == False][\"target\"].apply(int).values.reshape(-1, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_words = min(max_features, len(word_index)) + 1\nembedding_dim = 300\n# first create a matrix of zeros, this is our embedding matrix\nembedding_matrix = np.zeros((num_words, embedding_dim))\n# for each word in out tokenizer lets try to find that work in our w2v model\nfor word, i in word_index.items():\n    if i > max_features:\n        continue\n    embedding_vector = covered.get(word)\n    if embedding_vector is not None:\n        # we found the word - add that words vector to the matrix\n        embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(num_words,\n                    embedding_dim,\n                    embeddings_initializer=Constant(embedding_matrix),\n                    input_length=maxlen,\n                    trainable=False),\n    tf.keras.layers.SpatialDropout1D(0.2),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n    tf.keras.layers.Dropout(0.10),\n    tf.keras.layers.Dense(units=32, activation=\"relu\"),\n    tf.keras.layers.Dense(units=8, activation=\"relu\"),\n    tf.keras.layers.Dense(units=1, activation=\"sigmoid\")\n    \n    \n])\n\nmodel.compile(loss = \"binary_crossentropy\", optimizer='adam', metrics = [\"accuracy\"])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 128\nnum_epochs = 20\n\nhistory = model.fit(train_x, train_y, batch_size = batch_size, epochs = num_epochs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make predictions\n# Preparing test data\ny_pred = model.predict(test)\n\nsubmission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\nsubmission['target'] = np.round(y_pred).astype('int')\nsubmission.to_csv('model_submission2.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XG Boost"},{"metadata":{"trusted":true},"cell_type":"code","source":"pad_df = pd.DataFrame(padded)\nnew_df = pd.concat([df, pad_df], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_df.drop(\"text\", inplace = True, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_new_df = new_df[new_df[\"target\"].isnull() == False]\ntest_new_df = new_df[new_df[\"target\"].isnull() == True]\ntest_new_df.drop(\"target\", inplace = True, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train_new_df.drop(\"target\", axis = 1).values\ny = train_new_df[\"target\"].apply(int).values.reshape(-1, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# It will take a while to run. I have already run this on my local host. So I am just writing the code here.\n\nparam_test = {\n    \"max_depth\":range(3,10,2),\n    \"min_child_weight\":range(1,6,2),\n    \"gamma\":[i/10.0 for i in range(0,5)]\n}\n\n\ngsearch = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=5,\n                                             min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n                                             objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27), \n                                             param_grid = param_test, n_jobs=4,iid=False, cv=5)\n\ngsearch.fit(X,y)\ngsearch.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb = XGBClassifier(\n     learning_rate =0.1,\n     n_estimators=140,\n     max_depth=5,\n     min_child_weight=1,\n     gamma=0,\n     subsample=0.8,\n     colsample_bytree=0.8,\n     objective= 'binary:logistic',\n     nthread=4,\n     scale_pos_weight=1,\n     seed=27)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}