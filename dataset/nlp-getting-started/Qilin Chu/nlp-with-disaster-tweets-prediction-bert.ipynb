{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Real or Not? NLP With Disaster Tweets"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport os\nimport re\nimport string\nimport warnings\nimport operator\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.util import ngrams\nfrom collections import defaultdict, Counter\nfrom tqdm import tqdm, tqdm_notebook\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import confusion_matrix\nfrom keras.preprocessing.text import Tokenizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setting some options for general use.\nwarnings.filterwarnings('ignore')\nstop_words = set(stopwords.words(\"english\"))\nstemmer_snowball = SnowballStemmer(\"english\")\nstemmer_porter = PorterStemmer()\nplt.style.use('ggplot')\nsns.set(font_scale=1.5)\npd.options.display.max_columns = 250\npd.options.display.max_rows = 250","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# parameters\nMax_length = 42\nDropout_num = 0  \nlearning_rate = 6e-6 \nvalid = 0.2\nepochs_num = 3\nbatch_size_num = 16\nids_error_corrected = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load CSV files containing training data\ntrain_path = \"/kaggle/input/nlp-getting-started/train.csv\"\ntest_path = \"/kaggle/input/nlp-getting-started/test.csv\"\ntrain_df = pd.read_csv(train_path, dtype={'id': np.int16, 'target': np.int8})\ntest_df = pd.read_csv(test_path, dtype={'id': np.int16})\nsubmission = pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')\n\n# Checking observation and feature numbers for train and test data.\nprint(f'train: {train_df.shape}')\nprint(f'test: {test_df.shape}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# target distribution\nfig, axes = plt.subplots(ncols=2, nrows=1, figsize=(12, 4), dpi=100)\nsns.countplot(train_df['target'], ax=axes[0])\naxes[1].pie(train_df['target'].value_counts(),\n            labels=['Not Disaster', 'Disaster'],\n            autopct='%1.2f%%',\n            shadow=True,\n            explode=(0.05, 0),\n            startangle=60)\nfig.suptitle('Distribution of the Tweets', fontsize=24)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# keyword & location\nmissing_cols = ['keyword', 'location']\nfig, axes = plt.subplots(ncols=2, figsize=(17, 4), dpi=100)\nsns.barplot(x=train_df[missing_cols].isnull().sum().index, y=train_df[missing_cols].isnull().sum().values, ax=axes[0])\nsns.barplot(x=test_df[missing_cols].isnull().sum().index, y=test_df[missing_cols].isnull().sum().values, ax=axes[1])\naxes[0].set_ylabel('Missing Value Count', size=15, labelpad=20)\naxes[0].tick_params(axis='x', labelsize=15)\naxes[0].tick_params(axis='y', labelsize=15)\naxes[1].tick_params(axis='x', labelsize=15)\naxes[1].tick_params(axis='y', labelsize=15)\naxes[0].set_title('Training Set', fontsize=13)\naxes[1].set_title('Test Set', fontsize=13)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dropping unwanted column\ntrain_df = train_df.drop(['location', 'keyword'], axis=1)\ntest_df = test_df.drop(['location', 'keyword'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing the Tweets\n1. URL\n2. HTML tags\n3. Emojis\n4. Special characters\n5. Non-ASCII characters\n6. Expand contractions\n7. Specific corrections\n8. Other context specific corrections\n9. Abbreviations\n10. punctuations"},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_url(tweet):\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url.sub(r'',tweet)\n\ndef remove_html(tweet):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',tweet)\n\ndef remove_emoji(tweet):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', tweet)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def special_characters(tweet):\n    \n    tweet = re.sub(r\"SuruÌ¤\", \"Suruc\", tweet)\n    tweet = re.sub(r\"JapÌ_n\", \"Japan\", tweet)  \n    tweet = re.sub(r\"\\x89ÛÏWhen\", \"When\", tweet)\n    tweet = re.sub(r\"å£3million\", \"3 million\", tweet)\n    tweet = re.sub(r\"fromåÊwounds\", \"from wounds\", tweet)\n    tweet = re.sub(r\"mÌ¼sica\", \"music\", tweet)\n    tweet = re.sub(r\"donå«t\", \"do not\", tweet)\n    tweet = re.sub(r\"didn`t\", \"did not\", tweet)\n    tweet = re.sub(r\"i\\x89Ûªm\", \"I am\", tweet)\n    tweet = re.sub(r\"I\\x89Ûªm\", \"I am\", tweet)\n    tweet = re.sub(r\"it\\x89Ûªs\", \"it is\", tweet)\n    tweet = re.sub(r\"It\\x89Ûªs\", \"It is\", tweet)\n    tweet = re.sub(r\"i\\x89Ûªd\", \"I would\", tweet)\n    tweet = re.sub(r\"I\\x89Ûªd\", \"I would\", tweet)\n    tweet = re.sub(r\"i\\x89Ûªve\", \"I have\", tweet)\n    tweet = re.sub(r\"I\\x89Ûªve\", \"I have\", tweet)\n    tweet = re.sub(r\"let\\x89Ûªs\", \"let us\", tweet)\n    tweet = re.sub(r\"don\\x89Ûªt\", \"do not\", tweet)\n    tweet = re.sub(r\"Don\\x89Ûªt\", \"Do not\", tweet)\n    tweet = re.sub(r\"can\\x89Ûªt\", \"cannot\", tweet)\n    tweet = re.sub(r\"Can\\x89Ûªt\", \"Cannot\", tweet)\n    tweet = re.sub(r\"that\\x89Ûªs\", \"that is\", tweet)\n    tweet = re.sub(r\"That\\x89Ûªs\", \"That is\", tweet)\n    tweet = re.sub(r\"here\\x89Ûªs\", \"here is\", tweet)\n    tweet = re.sub(r\"Here\\x89Ûªs\", \"Here is\", tweet)\n    tweet = re.sub(r\"you\\x89Ûªre\", \"you are\", tweet)\n    tweet = re.sub(r\"You\\x89Ûªre\", \"You are\", tweet)\n    tweet = re.sub(r\"you\\x89Ûªve\", \"you have\", tweet)\n    tweet = re.sub(r\"You\\x89Ûªve\", \"You have\", tweet)\n    tweet = re.sub(r\"you\\x89Ûªll\", \"you will\", tweet)\n    tweet = re.sub(r\"You\\x89Ûªll\", \"You will\", tweet)\n    tweet = re.sub(r\"China\\x89Ûªs\", \"China's\", tweet)\n    tweet = re.sub(r\"doesn\\x89Ûªt\", \"does not\", tweet)\n    tweet = re.sub(r\"wouldn\\x89Ûªt\", \"would not\", tweet)\n    tweet = re.sub(r\"\\x89Û_\", \"\", tweet)\n    tweet = re.sub(r\"\\x89Û¢\", \"\", tweet)\n    tweet = re.sub(r\"\\x89ÛÒ\", \"\", tweet)\n    tweet = re.sub(r\"\\x89ÛÓ\", \"\", tweet)\n    tweet = re.sub(r\"\\x89ÛÏ\", \"\", tweet)\n    tweet = re.sub(r\"\\x89Û÷\", \"\", tweet)\n    tweet = re.sub(r\"\\x89Ûª\", \"\", tweet)\n    tweet = re.sub(r\"\\x89Û¢åÊ\", \"\", tweet)\n    tweet = re.sub(r\"\\x89Û\\x9d\", \"\", tweet)\n    tweet = re.sub(r\"å_\", \"\", tweet)\n    tweet = re.sub(r\"å¨\", \"\", tweet)\n    tweet = re.sub(r\"åÀ\", \"\", tweet)\n    tweet = re.sub(r\"åÇ\", \"\", tweet)\n    tweet = re.sub(r\"åÊ\", \"\", tweet)\n    tweet = re.sub(r\"åÈ\", \"\", tweet)  \n    tweet = re.sub(r\"Ì©\", \"\", tweet)\n    \n    # Character entity references\n    tweet = re.sub(r\"&lt;\", \"<\", tweet)\n    tweet = re.sub(r\"&gt;\", \">\", tweet)\n    tweet = re.sub(r\"&amp;\", \"&\", tweet)\n    return tweet\n\n# Removes non-ASCII characters\ndef remove_nonASCII(tweet):\n    tweet = ''.join([x for x in tweet if x in string.printable])\n    return tweet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def expand_contractions(tweet):\n    \n    tweet = re.sub(r\"I'm\", \"I am\", tweet)\n    tweet = re.sub(r\"I'M\", \"I am\", tweet)\n    tweet = re.sub(r\"i'm\", \"I am\", tweet)\n    tweet = re.sub(r\"i'M\", \"I am\", tweet)\n    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n    tweet = re.sub(r\"I'd\", \"I would\", tweet)\n    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n    tweet = re.sub(r\"I'll\", \"I will\", tweet)\n    tweet = re.sub(r\"i've\", \"I have\", tweet)\n    tweet = re.sub(r\"I've\", \"I have\", tweet)\n    tweet = re.sub(r\"you're\", \"you are\", tweet)\n    tweet = re.sub(r\"You're\", \"You are\", tweet)\n    tweet = re.sub(r\"you'd\", \"you would\", tweet)\n    tweet = re.sub(r\"You'd\", \"You would\", tweet)\n    tweet = re.sub(r\"you've\", \"you have\", tweet)\n    tweet = re.sub(r\"You've\", \"You have\", tweet)\n    tweet = re.sub(r\"you'll\", \"you will\", tweet)\n    tweet = re.sub(r\"You'll\", \"You will\", tweet)  \n    tweet = re.sub(r\"y'know\", \"you know\", tweet)  \n    tweet = re.sub(r\"Y'know\", \"You know\", tweet)  \n    tweet = re.sub(r\"y'all\", \"you all\", tweet)\n    tweet = re.sub(r\"Y'all\", \"You all\", tweet)\n    tweet = re.sub(r\"we're\", \"we are\", tweet)\n    tweet = re.sub(r\"We're\", \"We are\", tweet)\n    tweet = re.sub(r\"we've\", \"we have\", tweet)\n    tweet = re.sub(r\"We've\", \"We have\", tweet) \n    tweet = re.sub(r\"we'd\", \"we would\", tweet)\n    tweet = re.sub(r\"We'd\", \"We would\", tweet)\n    tweet = re.sub(r\"WE'VE\", \"We have\", tweet)\n    tweet = re.sub(r\"we'll\", \"we will\", tweet)\n    tweet = re.sub(r\"We'll\", \"We will\", tweet)\n    tweet = re.sub(r\"they're\", \"they are\", tweet)\n    tweet = re.sub(r\"They're\", \"They are\", tweet)\n    tweet = re.sub(r\"they'd\", \"they would\", tweet)\n    tweet = re.sub(r\"They'd\", \"They would\", tweet)  \n    tweet = re.sub(r\"they've\", \"they have\", tweet)\n    tweet = re.sub(r\"They've\", \"They have\", tweet)\n    tweet = re.sub(r\"they'll\", \"they will\", tweet)\n    tweet = re.sub(r\"They'll\", \"They will\", tweet)\n    tweet = re.sub(r\"he's\", \"he is\", tweet)\n    tweet = re.sub(r\"He's\", \"He is\", tweet)\n    tweet = re.sub(r\"he'll\", \"he will\", tweet)\n    tweet = re.sub(r\"He'll\", \"He will\", tweet)\n    tweet = re.sub(r\"she's\", \"she is\", tweet)\n    tweet = re.sub(r\"She's\", \"She is\", tweet)\n    tweet = re.sub(r\"she'll\", \"she will\", tweet)\n    tweet = re.sub(r\"She'll\", \"She will\", tweet)\n    tweet = re.sub(r\"it's\", \"it is\", tweet)\n    tweet = re.sub(r\"It's\", \"It is\", tweet)\n    tweet = re.sub(r\"it'll\", \"it will\", tweet)\n    tweet = re.sub(r\"It'll\", \"It will\", tweet)\n    tweet = re.sub(r\"isn't\", \"is not\", tweet)\n    tweet = re.sub(r\"Isn't\", \"Is not\", tweet)\n    tweet = re.sub(r\"who's\", \"who is\", tweet)\n    tweet = re.sub(r\"Who's\", \"Who is\", tweet)\n    tweet = re.sub(r\"what's\", \"what is\", tweet)\n    tweet = re.sub(r\"What's\", \"What is\", tweet)\n    tweet = re.sub(r\"that's\", \"that is\", tweet)\n    tweet = re.sub(r\"That's\", \"That is\", tweet)\n    tweet = re.sub(r\"here's\", \"here is\", tweet)\n    tweet = re.sub(r\"Here's\", \"Here is\", tweet)\n    tweet = re.sub(r\"there's\", \"there is\", tweet)\n    tweet = re.sub(r\"There's\", \"There is\", tweet)\n    tweet = re.sub(r\"where's\", \"where is\", tweet)\n    tweet = re.sub(r\"Where's\", \"Where is\", tweet)  \n    tweet = re.sub(r\"wHeRE's\", \"where is\", tweet)  \n    tweet = re.sub(r\"how's\", \"how is\", tweet)  \n    tweet = re.sub(r\"How's\", \"How is\", tweet)  \n    tweet = re.sub(r\"how're\", \"how are\", tweet)  \n    tweet = re.sub(r\"How're\", \"How are\", tweet) \n    tweet = re.sub(r\"let's\", \"let us\", tweet)\n    tweet = re.sub(r\"Let's\", \"Let us\", tweet)\n    tweet = re.sub(r\"won't\", \"will not\", tweet)\n    tweet = re.sub(r\"wasn't\", \"was not\", tweet)\n    tweet = re.sub(r\"aren't\", \"are not\", tweet)\n    tweet = re.sub(r\"couldn't\", \"could not\", tweet)\n    tweet = re.sub(r\"shouldn't\", \"should not\", tweet)\n    tweet = re.sub(r\"haven't\", \"have not\", tweet)\n    tweet = re.sub(r\"Haven't\", \"Have not\", tweet)\n    tweet = re.sub(r\"hasn't\", \"has not\", tweet)\n    tweet = re.sub(r\"wouldn't\", \"would not\", tweet)\n    tweet = re.sub(r\"weren't\", \"were not\", tweet)\n    tweet = re.sub(r\"Weren't\", \"Were not\", tweet)\n    tweet = re.sub(r\"ain't\", \"am not\", tweet)\n    tweet = re.sub(r\"Ain't\", \"am not\", tweet)\n    tweet = re.sub(r\"don't\", \"do not\", tweet)\n    tweet = re.sub(r\"Don't\", \"do not\", tweet)\n    tweet = re.sub(r\"DON'T\", \"Do not\", tweet)\n    tweet = re.sub(r\"didn't\", \"did not\", tweet)\n    tweet = re.sub(r\"Didn't\", \"Did not\", tweet)\n    tweet = re.sub(r\"DIDN'T\", \"Did not\", tweet)\n    tweet = re.sub(r\"doesn't\", \"does not\", tweet)\n    tweet = re.sub(r\"can't\", \"cannot\", tweet)\n    tweet = re.sub(r\"Can't\", \"Cannot\", tweet)\n    tweet = re.sub(r\"Could've\", \"Could have\", tweet)\n    tweet = re.sub(r\"should've\", \"should have\", tweet)\n    tweet = re.sub(r\"would've\", \"would have\", tweet)\n    \n    return tweet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def specific_corrections(tweet):\n    \n    '''Typos, slang and informal abbreviations'''\n    \n    tweet = re.sub(r\"b/c\", \"because\", tweet)\n    tweet = re.sub(r\"w/e\", \"whatever\", tweet)\n    tweet = re.sub(r\"w/out\", \"without\", tweet)\n    tweet = re.sub(r\"w/o\", \"without\", tweet)\n    tweet = re.sub(r\"w/\", \"with \", tweet)   \n    tweet = re.sub(r\"<3\", \"love\", tweet)\n    tweet = re.sub(r\"c/o\", \"care of\", tweet)\n    tweet = re.sub(r\"p/u\", \"pick up\", tweet)\n    tweet = re.sub(r\"\\n\", \" \", tweet)\n   \n    # Typos\n    tweet = re.sub(r\"Trfc\", \"Traffic\", tweet)\n    tweet = re.sub(r\"recentlu\", \"recently\", tweet)\n    tweet = re.sub(r\"Ph0tos\", \"Photos\", tweet)\n    tweet = re.sub(r\"exp0sed\", \"exposed\", tweet)\n    tweet = re.sub(r\"amageddon\", \"armageddon\", tweet)\n    tweet = re.sub(r\"TRAUMATISED\", \"traumatized\", tweet)\n    tweet = re.sub(r\"Newss\", \"News\", tweet)\n    tweet = re.sub(r\"remedyyyy\", \"remedy\", tweet)\n    tweet = re.sub(r\"Bstrd\", \"bastard\", tweet)\n    tweet = re.sub(r\"bldy\", \"bloody\", tweet)\n    tweet = re.sub(r\"epicenterr\", \"epicenter\", tweet)\n    tweet = re.sub(r\"approachng\", \"approaching\", tweet)\n    tweet = re.sub(r\"evng\", \"evening\", tweet)\n    tweet = re.sub(r\"Sumthng\", \"something\", tweet)\n    tweet = re.sub(r\"kostumes\", \"costumes\", tweet)\n    tweet = re.sub(r\"glowng\", \"glowing\", tweet)\n    tweet = re.sub(r\"kindlng\", \"kindling\", tweet)\n    tweet = re.sub(r\"riggd\", \"rigged\", tweet)\n    tweet = re.sub(r\"HLPS\", \"helps\", tweet)\n    tweet = re.sub(r\"SNCTIONS\", \"sanctions\", tweet)\n    tweet = re.sub(r\"Politifiact\", \"PolitiFact\", tweet)\n    tweet = re.sub(r\"Kowing\", \"Knowing\", tweet)\n    tweet = re.sub(r\"wrld\", \"world\", tweet)   \n    tweet = re.sub(r\"shld\", \"should\", tweet)    \n    tweet = re.sub(r\"thruuu\", \"through\", tweet)\n    tweet = re.sub(r\"probaly\", \"probably\", tweet)\n    tweet = re.sub(r\"whatevs\", \"whatever\", tweet)\n    tweet = re.sub(r\"colomr\", \"colour\", tweet)\n    tweet = re.sub(r\"pileq\", \"pile\", tweet)\n    tweet = re.sub(r\"firefightr\", \"firefighter\", tweet)\n    tweet = re.sub(r\"LAIGHIGN\", \"laughing\", tweet)\n    tweet = re.sub(r\"EXCLUSIV\", \"Exclusive\", tweet) \n    tweet = re.sub(r\"belo-ooow\", \"below\", tweet)  \n    tweet = re.sub(r\"who-ooo-ole\", \"whole\", tweet)  \n    tweet = re.sub(r\"brother-n-law\", \"father-in-law\", tweet)  \n    tweet = re.sub(r\"referencereference\", \"reference\", tweet)\n    \n    # Hashtags and usernames\n    tweet = re.sub(r\"IranDeal\", \"Iran Deal\", tweet)\n    tweet = re.sub(r\"ProphetMuhammad\", \"Prophet Muhammad\", tweet)\n    tweet = re.sub(r\"StrategicPatience\", \"Strategic Patience\", tweet)\n    tweet = re.sub(r\"NASAHurricane\", \"NASA Hurricane\", tweet)\n    tweet = re.sub(r\"onlinecommunities\", \"online communities\", tweet)\n    tweet = re.sub(r\"LakeCounty\", \"Lake County\", tweet)\n    tweet = re.sub(r\"thankU\", \"thank you\", tweet)\n    tweet = re.sub(r\"iTunesMusic\", \"iTunes Music\", tweet)\n    tweet = re.sub(r\"OffensiveContent\", \"Offensive Content\", tweet)\n    tweet = re.sub(r\"WorstSummerJob\", \"Worst Summer Job\", tweet)\n    tweet = re.sub(r\"NASASolarSystem\", \"NASA Solar System\", tweet)\n    tweet = re.sub(r\"animalrescue\", \"animal rescue\", tweet)\n    tweet = re.sub(r\"Ptbo\", \"Peterborough\", tweet)\n    tweet = re.sub(r\"Throwingknifes\", \"Throwing knives\", tweet)\n    tweet = re.sub(r\"NestleIndia\", \"Nestle India\", tweet)\n    tweet = re.sub(r\"weathernetwork\", \"weather network\", tweet)\n    tweet = re.sub(r\"GOPDebate\", \"GOP Debate\", tweet)\n    tweet = re.sub(r\"volcanoinRussia\", \"volcano in Russia\", tweet)\n    tweet = re.sub(r\"53inch\", \"53 inch\", tweet)\n    tweet = re.sub(r\"FaroeIslands\", \"Faroe Islands\", tweet)\n    tweet = re.sub(r\"UTC2015\", \"UTC 2015\", tweet)\n    tweet = re.sub(r\"Time2015\", \"Time 2015\", tweet)\n    tweet = re.sub(r\"LivingSafely\", \"Living Safely\", tweet)\n    tweet = re.sub(r\"FIFA16\", \"Fifa 2016\", tweet)\n    tweet = re.sub(r\"bbcnews\", \"bbc news\", tweet)\n    tweet = re.sub(r\"UndergroundRailraod\", \"Underground Railraod\", tweet)\n    tweet = re.sub(r\"NoSurrender\", \"No Surrender\", tweet)\n    tweet = re.sub(r\"greatbritishbakeoff\", \"great british bake off\", tweet)\n    tweet = re.sub(r\"LondonFire\", \"London Fire\", tweet)\n    tweet = re.sub(r\"KOTAWeather\", \"KOTA Weather\", tweet)\n    tweet = re.sub(r\"LuchaUnderground\", \"Lucha Underground\", tweet)\n    tweet = re.sub(r\"KOIN6News\", \"KOIN 6 News\", tweet)\n    tweet = re.sub(r\"9NewsGoldCoast\", \"9 News Gold Coast\", tweet)\n    tweet = re.sub(r\"BlackLivesMatter\", \"Black Lives Matter\", tweet)\n    tweet = re.sub(r\"ENGvAUS\", \"England vs Australia\", tweet)\n    tweet = re.sub(r\"PlannedParenthood\", \"Planned Parenthood\", tweet)\n    tweet = re.sub(r\"calgaryweather\", \"Calgary Weather\", tweet)\n    tweet = re.sub(r\"renew911health\", \"renew 911 health\", tweet)\n    tweet = re.sub(r\"pdx911\", \"Portland Police\", tweet)\n    tweet = re.sub(r\"NJTurnpike\", \"New Jersey Turnpike\", tweet)\n    tweet = re.sub(r\"HannaPH\", \"Typhoon Hanna\", tweet)\n    tweet = re.sub(r\"cnnbrk\", \"CNN Breaking News\", tweet)\n    tweet = re.sub(r\"IndianNews\", \"Indian News\", tweet)\n    tweet = re.sub(r\"Daesh\", \"ISIS\", tweet)\n    tweet = re.sub(r\"FoxNew\", \"Fox News\", tweet)\n    tweet = re.sub(r\"RohnertParkDPS\", \"Rohnert Park DPS\", tweet)\n    tweet = re.sub(r\"FantasticFour\", \"Fantastic Four\", tweet)\n    tweet = re.sub(r\"BathAndNorthEastSomerset\", \"Bath and North East Somerset\", tweet)\n    tweet = re.sub(r\"residualincome\", \"residual income\", tweet)\n    tweet = re.sub(r\"YahooNewsDigest\", \"Yahoo News Digest\", tweet)\n    tweet = re.sub(r\"MalaysiaAirlines\", \"Malaysia Airlines\", tweet)\n    tweet = re.sub(r\"AmazonDeals\", \"Amazon Deals\", tweet)\n    tweet = re.sub(r\"EndConflict\", \"End Conflict\", tweet)\n    tweet = re.sub(r\"EndOccupation\", \"End Occupation\", tweet)\n    tweet = re.sub(r\"KindleCountdown\", \"Kindle Countdown\", tweet)\n    tweet = re.sub(r\"NoMoreHandouts\", \"No More Handouts\", tweet)\n    tweet = re.sub(r\"WindstormInsurer\", \"Windstorm Insurer\", tweet)\n    tweet = re.sub(r\"USAgov\", \"USA government\", tweet)\n    tweet = re.sub(r\"US govt\", \"USA government\", tweet)  \n    tweet = re.sub(r\"WAwildfire\", \"WA Wildfire\", tweet)\n    tweet = re.sub(r\"fingerrockfire\", \"Finger Rock Fire\", tweet)\n    tweet = re.sub(r\"newnewnew\", \"new new new\", tweet)\n    tweet = re.sub(r\"freshoutofthebox\", \"fresh out of the box\", tweet)\n    tweet = re.sub(r\"yycweather\", \"Calgary Weather\", tweet)\n    tweet = re.sub(r\"calgarysun\", \"Calgary Sun\", tweet)\n    tweet = re.sub(r\"shondarhimes\", \"Shonda Rhimes\", tweet)\n    tweet = re.sub(r\"SushmaSwaraj\", \"Sushma Swaraj\", tweet)\n    tweet = re.sub(r\"pray4japan\", \"Pray for Japan\", tweet)\n    tweet = re.sub(r\"hope4japan\", \"Hope for Japan\", tweet)\n    tweet = re.sub(r\"Illusionimagess\", \"Illusion images\", tweet)\n    tweet = re.sub(r\"ShallWeDance\", \"Shall We Dance\", tweet)\n    tweet = re.sub(r\"TCMParty\", \"TCM Party\", tweet)\n    tweet = re.sub(r\"marijuananews\", \"marijuana news\", tweet)\n    tweet = re.sub(r\"HeadlinesApp\", \"Headlines App\", tweet)\n    tweet = re.sub(r\"BBCNewsAsia\", \"BBC News Asia\", tweet)\n    tweet = re.sub(r\"BombEffects\", \"Bomb Effects\", tweet)\n    tweet = re.sub(r\"idkidk\", \"idk idk\", tweet)\n    tweet = re.sub(r\"BBCLive\", \"BBC Live\", tweet)\n    tweet = re.sub(r\"NaturalBirth\", \"Natural Birth\", tweet)\n    tweet = re.sub(r\"FusionFestival\", \"Fusion Festival\", tweet)\n    tweet = re.sub(r\"50Mixed\", \"50 Mixed\", tweet)\n    tweet = re.sub(r\"NoAgenda\", \"No Agenda\", tweet)\n    tweet = re.sub(r\"WhiteGenocide\", \"White Genocide\", tweet)\n    tweet = re.sub(r\"dirtylying\", \"dirty lying\", tweet)\n    tweet = re.sub(r\"SyrianRefugees\", \"Syrian Refugees\", tweet)\n    tweet = re.sub(r\"Auspol\", \"Australia Politics\", tweet)\n    tweet = re.sub(r\"WhiteTerrorism\", \"White Terrorism\", tweet)\n    tweet = re.sub(r\"truthfrequencyradio\", \"Truth Frequency Radio\", tweet)\n    tweet = re.sub(r\"ErasureIsNotEquality\", \"Erasure is not equality\", tweet)\n    tweet = re.sub(r\"toopainful\", \"too painful\", tweet)\n    tweet = re.sub(r\"melindahaunton\", \"Melinda Haunton\", tweet)\n    tweet = re.sub(r\"NoNukes\", \"No Nukes\", tweet)\n    tweet = re.sub(r\"curryspcworld\", \"Currys PC World\", tweet)\n    tweet = re.sub(r\"blackforestgateau\", \"black forest gateau\", tweet)\n    tweet = re.sub(r\"BBCOne\", \"BBC One\", tweet)\n    tweet = re.sub(r\"sebastianstanisaliveandwell\", \"Sebastian Stan is alive and well\", tweet)\n    tweet = re.sub(r\"concertphotography\", \"concert photography\", tweet)\n    tweet = re.sub(r\"TheaterTrial\", \"Theater Trial\", tweet)\n    tweet = re.sub(r\"TheBrooklynLife\", \"The Brooklyn Life\", tweet)\n    tweet = re.sub(r\"jokethey\", \"joke they\", tweet)\n    tweet = re.sub(r\"nflweek1picks\", \"NFL week 1 picks\", tweet)\n    tweet = re.sub(r\"nflnetwork\", \"NFL Network\", tweet)\n    tweet = re.sub(r\"NYDNSports\", \"NY Daily News Sports\", tweet)\n    tweet = re.sub(r\"crunchysensible\", \"crunchy sensible\", tweet)\n    tweet = re.sub(r\"RandomActsOfRomance\", \"Random acts of romance\", tweet)\n    tweet = re.sub(r\"MomentsAtHill\", \"Moments at hill\", tweet)\n    tweet = re.sub(r\"liveleakfun\", \"live leak fun\", tweet)\n    tweet = re.sub(r\"SahelNews\", \"Sahel News\", tweet)\n    tweet = re.sub(r\"abc7newsbayarea\", \"ABC 7 News Bay Area\", tweet)\n    tweet = re.sub(r\"CampLogistics\", \"Camp logistics\", tweet)\n    tweet = re.sub(r\"alaskapublic\", \"Alaska public\", tweet)\n    tweet = re.sub(r\"MarketResearch\", \"Market Research\", tweet)\n    tweet = re.sub(r\"AccuracyEsports\", \"Accuracy Esports\", tweet)\n    tweet = re.sub(r\"yychail\", \"Calgary hail\", tweet)\n    tweet = re.sub(r\"yyctraffic\", \"Calgary traffic\", tweet)\n    tweet = re.sub(r\"eliotschool\", \"eliot school\", tweet)\n    tweet = re.sub(r\"TheBrokenCity\", \"The Broken City\", tweet)\n    tweet = re.sub(r\"fieldworksmells\", \"field work smells\", tweet)\n    tweet = re.sub(r\"IranElection\", \"Iran Election\", tweet)\n    tweet = re.sub(r\"MyanmarFlood\", \"Myanmar Flood\", tweet)\n    tweet = re.sub(r\"abc7chicago\", \"ABC 7 Chicago\", tweet)\n    tweet = re.sub(r\"copolitics\", \"Colorado Politics\", tweet)\n    tweet = re.sub(r\"massiveflooding\", \"massive flooding\", tweet)\n    tweet = re.sub(r\"greektheatrela\", \"Greek Theatre Los Angeles\", tweet)\n    tweet = re.sub(r\"publicsafetyfirst\", \"public safety first\", tweet)\n    tweet = re.sub(r\"myhometown\", \"my hometown\", tweet)\n    tweet = re.sub(r\"tankerfire\", \"tanker fire\", tweet)\n    tweet = re.sub(r\"MEMORIALDAY\", \"memorial day\", tweet)\n    tweet = re.sub(r\"MEMORIAL_DAY\", \"memorial day\", tweet)\n    tweet = re.sub(r\"VirtualReality\", \"Virtual Reality\", tweet)\n    tweet = re.sub(r\"mortalkombatx\", \"Mortal Kombat X\", tweet)\n    tweet = re.sub(r\"mortalkombat\", \"Mortal Kombat\", tweet)\n    tweet = re.sub(r\"ToshikazuKatayama\", \"Toshikazu Katayama\", tweet)\n    tweet = re.sub(r\"ExtremeWeather\", \"Extreme Weather\", tweet)\n    tweet = re.sub(r\"WereNotGruberVoters\", \"We are not gruber voters\", tweet)\n    tweet = re.sub(r\"PhiladelphiaMuseu\", \"Philadelphia Museum\", tweet)\n    tweet = re.sub(r\"NorthIowa\", \"North Iowa\", tweet)\n    tweet = re.sub(r\"WillowFire\", \"Willow Fire\", tweet)\n    tweet = re.sub(r\"P_EOPLE\", \"PEOPLE\", tweet)\n    tweet = re.sub(r\"ThisIsAfrica\", \"This is Africa\", tweet)\n    tweet = re.sub(r\"viaYouTube\", \"via YouTube\", tweet)\n    \n    return tweet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_others(tweet):  \n    \n    tweet = re.sub(r\"2007he\", \"2007 he\", tweet)  \n    tweet = re.sub(r\"Hwy27\", \"Hwy 27\", tweet) \n    tweet = re.sub(r\"jokethey\", \"joke they\", tweet)  \n    tweet = re.sub(r\"40%money\", \"40% money\", tweet)  \n    tweet = re.sub(r\"hegot\", \"he got\", tweet)\n    tweet = re.sub(r\"wannabe\", \"wanna be\", tweet) \n    tweet = re.sub(r\"dadwho\", \"dad who\", tweet)  \n    tweet = re.sub(r\"fundwhen\", \"fund when\", tweet)\n    tweet = re.sub(r\"next chp\", \"next chapter\", tweet)\n    tweet = re.sub(r\"UR sons\", \"your sons\", tweet)  \n    tweet = re.sub(r\"Yr voice ws\", \"Your voice was\", tweet) \n    tweet = re.sub(r\"U're not\", \"You are not\", tweet)  \n    tweet = re.sub(r\"u'd win\", \"you had win\", tweet)  \n    tweet = re.sub(r\"Jus Kame\", \"Just came\", tweet)  \n    tweet = re.sub(r\"b4federal\", \"B-4, Federal\", tweet) \n    tweet = re.sub(r\"ppor child\", \"poor child\", tweet)  \n    tweet = re.sub(r\"stand ogt\", \"stand out\", tweet)\n    tweet = re.sub(r\"stand oup\", \"stand out\", tweet) \n    tweet = re.sub(r\"IS claims\", \"ISIS claims\", tweet)\n    tweet = re.sub(r\"2slow2report\", \"too slow to report\", tweet)\n    tweet = re.sub(r\"@ft\", \"@Financial Times\", tweet)\n    tweet = re.sub(r\"50ft\", \"50 ft\", tweet)\n    tweet = re.sub(r\"Ft ABH Shadow\", \"featuring ABH Shadow\", tweet)\n    tweet = re.sub(r\"Since1970the\", \"Since 1970 the\", tweet) \n    tweet = re.sub(r\"whats cracking cuz\", \"what is cracking cause\", tweet) \n    tweet = re.sub(r\"mentally ill\", \"mental illness\", tweet)\n    tweet = re.sub(r\"RIPRIPRIP\", \"RIP RIP RIP\", tweet)\n    tweet = re.sub(r\"RIPROSS\", \"RIP ROSS\", tweet)  \n    tweet = re.sub(r\"ABQ NM\", \"Albuquerque New Mexico\", tweet)\n    tweet = re.sub(r\"#BC\", \"#British Columbia\", tweet)\n    tweet = re.sub(r\"in BC\", \"in British Columbia\", tweet)\n    tweet = re.sub(r\"BC DROUGHT\", \"British Columbia Drought\", tweet)\n    tweet = re.sub(r\"in OK\", \"in Oklahoma\", tweet)\n    tweet = re.sub(r\"City OK\", \"City Oklahoma\", tweet)\n    tweet = re.sub(r\"Hinton OK\", \"Hinton Oklahoma\", tweet)\n    tweet = re.sub(r\"Guthrie OK\", \"Guthrie Oklahoma\", tweet)\n    tweet = re.sub(r\"Choctaw OK\", \"Choctaw Oklahoma\", tweet)\n    tweet = re.sub(r\"Oklahoma-OK\", \"Oklahoma City\", tweet)\n    tweet = re.sub(r\"Oklahoma [OK]\", \"Oklahoma City\", tweet)\n    tweet = re.sub(r\"JADE FL\", \"JADE Florida\", tweet) \n    tweet = re.sub(r\"Jacksonville FL\", \"Jacksonville Florida\", tweet)\n    tweet = re.sub(r\"Saint Petersburg FL\", \"Saint Petersburg Florida\", tweet)\n    tweet = re.sub(r\"Wahpeton ND\", \"Wahpeton, North Dakota\", tweet)\n    tweet = re.sub(r\"Northern Marians\", \"Northern Mariana Islands\", tweet)\n    tweet = re.sub(r\"Northern Ma\", \"Northern Mariana Islands\", tweet)\n    \n    # Abbreviation point\n    tweet = re.sub(r\"Dr\\.\", \"Doctor\", tweet)\n    tweet = re.sub(r\"f\\. M\\.O\\.P\\.\", \"featuring Mash Out Posse\", tweet)\n    tweet = re.sub(r\"M\\.O\\.P\\.\", \"Mash Out Posse\", tweet)\n    tweet = re.sub(r\"M\\.O\\.P\", \"Mash Out Posse\", tweet)\n    tweet = re.sub(r\"P\\.O\\.P\\.E\\.\", \"Pope\", tweet)\n    tweet = re.sub(r\"S\\.O\\.S\\.\", \"SOS\", tweet)\n    tweet = re.sub(r\"s\\.o\\.s\\.\", \"SOS\", tweet)  \n    tweet = re.sub(r\"Fire Co\\.\", \"Fire Company\", tweet)\n    tweet = re.sub(r\"Holt and Co\\.\", \"Holt and Company\", tweet)\n    tweet = re.sub(r\"roofing co\\.\", \"roofing company\", tweet)\n    tweet = re.sub(r\"Costa Co\\.\", \"Costa County\", tweet)\n    tweet = re.sub(r\"York Co\\.\", \"York County\", tweet)\n    tweet = re.sub(r\"Fairfax Co\\.\", \"Fairfax County\", tweet)\n    tweet = re.sub(r\"I\\.S\\.I\\.S\\.\", \"ISIS\", tweet)\n    tweet = re.sub(r\"U\\.N\\.\", \"United Nations\", tweet)\n    tweet = re.sub(r\"U\\.S\\.\", \"United States\", tweet)\n    tweet = re.sub(r\"U\\.S\", \"United States\", tweet)\n    tweet = re.sub(r\"U\\.s\\.\", \"United States\", tweet)\n    tweet = re.sub(r\"U\\.s\", \"United States\", tweet)\n    tweet = re.sub(r\"U-S\\.\", \"United States\", tweet)\n    tweet = re.sub(r\"U\\.S National\", \"United States National\", tweet)\n    tweet = re.sub(r\"LANCASTER N\\.H\\.\", \"Lancaster New Hampshire\", tweet)\n    tweet = re.sub(r\"Manchester N\\.H\\.\", \"Manchester New Hampshire\", tweet)\n   \n    # Normalization\n    tweet = re.sub(r\"\\:33333\", \"smile\", tweet)    # :33333\n    tweet = re.sub(r\"\\:\\)\\)\\)\\)\", \"smile\", tweet) # :))))\n    tweet = re.sub(r\"\\:\\)\\)\\)\", \"smile\", tweet) # :)))\n    tweet = re.sub(r\"\\:\\)\\)\", \"smile\", tweet)   # :))\n    tweet = re.sub(r\"\\:-\\)\",  \"smile\", tweet)   # :-)\n    tweet = re.sub(r\"\\;-\\)\",  \"smile\", tweet)   # ;-)\n    tweet = re.sub(r\"3\\-D\", \"smile\", tweet)  # 3-D\n    tweet = re.sub(r\"\\:O\", \"smile\", tweet)   # :O\n    tweet = re.sub(r\"\\:D\", \"smile\", tweet)   # :D\n    tweet = re.sub(r\"\\:P\", \"smile\", tweet)   # :P\n    tweet = re.sub(r\"\\:p\", \"smile\", tweet)   # :p\n    tweet = re.sub(r\"\\;\\)\", \"smile\", tweet)  # ;)\n    tweet = re.sub(r\"\\:\\)\", \"smile\", tweet)  # :)\n    tweet = re.sub(r\"\\=\\)\", \"smile\", tweet)  # =)\n    tweet = re.sub(r\"\\^\\^\", \"smile\", tweet)  # ^^\n    tweet = re.sub(r\"\\:-\\(\", \"sad\", tweet)   # :-(\n    tweet = re.sub(r\"\\:\\(\", \"sad\", tweet)    # :(\n    tweet = re.sub(r\"\\=\\(\", \"sad\", tweet)    # =(\n    tweet = re.sub(r\"\\-\\_\\_\\-\", \"\", tweet)   # -__-\n    tweet = re.sub(r\"\\.\\_\\.\", \"\", tweet)     # ._.\n    tweet = re.sub(r\"T\\_T\", \"\", tweet)       # T_T\n    \n    return tweet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normalization\nabbreviations = {\n    \n    \"i.e\":\"that is\", \"mofo\":\"mother fucker\", \"til\":\"till\",\n    \"ft.\":\"featuring\", \"mf\":\"mother fucker\", \"bout\":\"about\",\n    \"ft\":\"featuring\", \"mfs\":\"mother fucker\", \"nd\":\"and\", \n    \"feat.\":\"featuring\", \"ltd\":\"limited\", \"nvr\":\"never\",\n    \"feat\":\"featuring\", \"pls\":\"please\", \"ppl\":\"people\",\n    \"tbs\":\"tablespoons\", \"tho\":\"though\", \"fav\":\"favorite\",\n    \"bc\":\"because\", \"cuz\":\"because\", \"bcuz\":\"because\",\n    \"btwn\":\"between\", \"fwy\":\"Freeway\", \"hwy\":\"Highway\",\n    \"diff\":\"different\", \"appx\":\"approximately\", \n    \"im\":\"I am\", \"ive\":\"I have\", \"uve\":\"you have\", \n    \"youd\":\"you had\", \"hadnt\":\"had not\", \"isnt\":\"is not\",\n    \"dont\":\"do not\", \"didnt\":\"did not\", \"cant\":\"cannot\",\n    \"urself\":\"yourself\", \"wont\":\"would not\", \n    \"heres\":\"Here is\", \"lets\":\"Let us\", \"2day\":\"today\", \n    \"s2g\":\"swear to god\", \"be4\":\"before\", \"b4\":\"before\", \n    \"4the\":\"for the\", \"1st\":\"first\",\n   \n    # location\n    \"okwx\":\"Oklahoma Weather\", \"arwx\":\"Arkansas Weather\",    \n    \"gawx\":\"Georgia Weather\", \"cawx\":\"California Weather\",\n    \"tnwx\":\"Tennessee Weather\", \"azwx\":\"Arizona Weather\",  \n    \"alwx\":\"Alabama Weather\", \"scwx\":\"South Carolina Weather\",\n    \"isis\":\"Islamic State\", \"okc\":\"Oklahoma\",\"oun\":\"Oklahoma\",\n    \"isil\":\"Islamic State\", \"suruc\":\"Urfa\", \"pdx\":\"Portland\", \n    \"nm\":\"New Mexico\", \"newyork\":\"New York\", \"alska\":\"Alaska\",\n    \"nh\":\"New Hampshire\", \"nyc\":\"New York City\",\n    \"cnmi\":\"Northern Mariana Islands\", \"calif\":\"California\",\n    \"sarabia\":\"Saudi Arabia\", \"saudiarabia\":\"Saudi Arabia\", \n    \"mh370\":\"Malaysia Airlines Flight 370\", \n    \n    # units\n    \"12hr\":\"12 hr\",\"16yr\":\"16 year\", \"hrs\":\"hour\",\"hr\":\"hour\",\n    \"19yrs\":\"19 year\", \"yrs\":\"year\", \"min\":\"minute\", \n    \"20yrs\":\"20 year\", \"yr\":\"year\", \"mins\":\"minute\", \n    \n    # Typos\n    \"tren\":\"trend\", \"kno\":\"know\", \"swea\":\"swear\", \"stil\":\"still\",\n    \"fab\":\"fabulous\", \"srsly\":\"seriously\", \"epicente\":\"epicenter\", \n    \"jumpin\":\"jumping\", \"burnin\":\"burning\", \"throwin\":\"throwing\",\n    \"killin\":\"killing\", \"nothin\":\"nothing\", \"thinkin\":\"thinking\",  \n    \"tryin\":\"trying\", \"lookg\":\"looking\", \"fforecast\":\"Forecast\",\n    \"comin\":\"Coming\", \"newss\":\"news\", \"memez\":\"meme\", \"oli\":\"oil\",\n}\n\ndef convert_abbrev(word):\n    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word\n\ndef convert_abbrev_in_text(text):\n    tokens = word_tokenize(text)\n    tokens = [convert_abbrev(word) for word in tokens]\n    text = ' '.join(tokens)\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove unwanted words\ndef remove_non_alnum(tweet):\n    punctuation = re.compile('[^A-Za-z0-9]+')\n    return punctuation.sub(r' ',tweet)\n\n# Remove punctuations.\ndef remove_punct(text):\n    table = str.maketrans('', '', string.punctuation)\n    return text.translate(table)\n\n# Remove leading, trailing, and extra spaces\ndef remove_extra_spaces(text):\n    text = re.sub('\\s+', ' ', text).strip() \n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocessed_tweets_train = [] \nfor tweet in tqdm_notebook(train_df['text'].values):\n    tweet = remove_url(tweet)\n    tweet = remove_html(tweet)\n    tweet = remove_emoji(tweet)\n    tweet = special_characters(tweet)\n    tweet = remove_nonASCII(tweet)\n    tweet = expand_contractions(tweet)\n    tweet = specific_corrections(tweet)\n    tweet = remove_html(tweet)\n    tweet = clean_others(tweet)\n    tweet = convert_abbrev_in_text(tweet)\n    tweet = remove_punct(tweet)\n    tweet = remove_non_alnum(tweet)\n    tweet = remove_extra_spaces(tweet)\n    preprocessed_tweets_train.append(tweet.strip())\n    \ntrain_df['text'] = preprocessed_tweets_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocessed_tweets_test = []  \nfor tweet in tqdm_notebook(test_df['text'].values):\n    tweet = remove_url(tweet)\n    tweet = remove_html(tweet)\n    tweet = remove_emoji(tweet)\n    tweet = special_characters(tweet)\n    tweet = remove_nonASCII(tweet)\n    tweet = expand_contractions(tweet)\n    tweet = specific_corrections(tweet)\n    tweet = remove_html(tweet)\n    tweet = clean_others(tweet)\n    tweet = convert_abbrev_in_text(tweet)\n    tweet = remove_punct(tweet)\n    tweet = remove_non_alnum(tweet)\n    tweet = remove_extra_spaces(tweet)\n    preprocessed_tweets_test.append(tweet.strip())\n    \ntest_df['text'] = preprocessed_tweets_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Mislabeled Samples\n### Some of the tweets were found to be repeated and misclassified. Therefore, correctly classify them to improve the model performance"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_mislabeled = train_df.groupby(['text']).nunique().sort_values(by='target', ascending=False)\ndf_mislabeled = df_mislabeled[df_mislabeled['target'] > 1]['target']\ndf_mislabeled_all = df_mislabeled.index.tolist()\nprint(f'Number of repeated tweets(after preprocessing): {len(df_mislabeled_all)}')\ndf_mislabeled_all","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### These texts should be marked as 1\n* \"POTUS Strategic Patience is a strategy for Genocide refugees IDP Internally displaced people horror etc\"\n* \"CLEARED incident with injury I 495 inner loop Exit 31 MD 97 Georgia Ave Silver Spring\"\n* \"RT NotExplained The only known image of infamous hijacker D B Cooper\"\n* \"wowo 12000 Nigerian refugees repatriated from Cameroon\"\n* \"Bayelsa poll Tension in Bayelsa as Patience Jonathan plans to hijack APC PDP Plans by former First Lady and\"\n* \"hot C 130 specially modified to land in a stadium and rescue hostages in Iran in 1980 prebreak best\"\n* \"world FedEx no longer to transport bioterror germs in wake of anthrax lab mishaps\"\n* \"FedEx no longer to transport bioterror germs in wake of anthrax lab mishaps\"\n* \"FedEx no longer to transport bioterror germs in wake of anthrax lab mishaps via usatoday\"\n* \"Governor weighs parole for California school bus hijacker\"\n* \"Kosciusko police investigating pedestrian fatality hit by a train Thursday\"\n* \"A look at state actions a year after Ferguson s upheaval\"\n* \"Here is how media in Pakistan covered the capture of terrorist Mohammed Naved\"\n\n### The rest of the text should be marked as 0"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['target_new'] = train_df['target'].copy()  \n\ntarget_1_list = [      \n    \"POTUS Strategic Patience is a strategy for Genocide refugees IDP Internally displaced people horror etc\",\n    \"CLEARED incident with injury I 495 inner loop Exit 31 MD 97 Georgia Ave Silver Spring\",\n    \"RT NotExplained The only known image of infamous hijacker D B Cooper\",\n    \"wowo 12000 Nigerian refugees repatriated from Cameroon\", \n    \"Bayelsa poll Tension in Bayelsa as Patience Jonathan plans to hijack APC PDP Plans by former First Lady and\",\n    \"hot C 130 specially modified to land in a stadium and rescue hostages in Iran in 1980 prebreak best\",\n    \"world FedEx no longer to transport bioterror germs in wake of anthrax lab mishaps\",\n    \"FedEx no longer to transport bioterror germs in wake of anthrax lab mishaps\",\n    \"FedEx no longer to transport bioterror germs in wake of anthrax lab mishaps via usatoday\",\n    \"Governor weighs parole for California school bus hijacker\",\n    \"Kosciusko police investigating pedestrian fatality hit by a train Thursday\", \n    \"A look at state actions a year after Ferguson s upheaval\", \n    \"Here is how media in Pakistan covered the capture of terrorist Mohammed Naved\" ]\n\nfor mislabeled_sample in df_mislabeled_all:\n    if mislabeled_sample in target_1_list:\n        train_df.loc[train_df['text'] == mislabeled_sample, 'target_new'] = 1\n    else:\n        train_df.loc[train_df['text'] == mislabeled_sample, 'target_new'] = 0\n\nfilter_mislabel = (train_df['target'] != train_df['target_new'])\nprint(f'Number of relabeled: {len(train_df[filter_mislabel])}')\ntrain_df[filter_mislabel][:20]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Some other basic NLP techniques\n### We have to perform the following actions to get the purer text to find more repeated and misclassified tweets.\n1. Remove stopwords \n2. Stemming words"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove stopwords.\nstop_words = set(stopwords.words(\"english\"))\ndef remove_stopwords(tweet):\n    sentance = ' '.join(e.lower() for e in tweet.split() if e.lower() not in stop_words)\n    return sentance\n\n# Stemming words\nstemmer = SnowballStemmer(\"english\")\ndef stemming(text):    \n    text = [stemmer.stem(word) for word in text.split()]\n    return \" \".join(text)\n\n# Lemmatizing\nwn = nltk.WordNetLemmatizer()\ndef lemmatizing(text):    \n    text = [wn.lemmatize(word.lower()) for word in text.split()]\n    return \" \".join(text)\n\ntrain_df['text_pure'] = train_df['text'].apply(lambda x: remove_stopwords(x))\ntrain_df['text_pure'] = train_df['text_pure'].apply(lambda x: stemming(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Mislabeled Samples (Stemming)\n### Some of the tweets are repeated and misclassified."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_mislabeled = train_df.groupby(['text_pure']).nunique().sort_values(by='target_new', ascending=False)\ndf_mislabeled = df_mislabeled[df_mislabeled['target_new'] > 1]['target_new']\ndf_mislabeled_all = df_mislabeled.index.tolist()\nprint(f'Number of repeated tweets (after Stemming): {len(df_mislabeled_all)}')\ndf_mislabeled_all","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The texts \"look state action year ferguson upheav\" should be marked as 1\ntrain_df.loc[train_df['text_pure'] == \"look state action year ferguson upheav\", 'target_new'] = 1\ndf_mislabeled = train_df[train_df['text_pure'].isin(df_mislabeled_all)]\nfilter_mislabel = (df_mislabeled['target'] != df_mislabeled['target_new'])\nprint(f'Number of relabeled: {len(df_mislabeled[filter_mislabel])}')\ndf_mislabeled","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df.drop('text_pure', axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ids with target error\n## target 1 --> 0\n1. about \"bioterrorism\" : 882, 883, 886, 890, 893, 894, 896, 923, 926, 928\n2. about \"Fatality\" : 5223\n3. about \"Texas\" : 6965, 8939\n4. about \"insurance\", \"insurer\" : 2063, 2885, 4026, 7231, 8972, 9337, 10543, 10552\n5. about \"snowstorm\", \"windstorm\" : 8905, 8908, 8913, 8916, 8926, 8934, 8939, 10536\n6. about \"stock market crash\" : 8309, 8317, 8329, 8330\n7. about \"PantherAttack\" : 6731, 6745\n8. about \"Apollo Brown\" : 3802, 3837, 3842\n9. about \"false fire alarm\" : 4773, 4778, 4790\n10. about \"Ashes 2015\" : 1688, 1709\n11. about \"Reddit Will Now Quarantine\" : 7797\n12. about \"Hollywood movie about trapped miners\" : 9775"},{"metadata":{"trusted":true},"cell_type":"code","source":"ids_target1_error = [\n    328,443,513,791,794,882,883,886,890,893,894,896,923,926,928,1688,1709,2033,\n    2063,2619,2885,3097,3640,3802,3837,3842,3900,4026,4342,4530,4533,4575,4773,\n    4778,4790,5223,5781,6552,6554,6570,6701,6702,6729,6731,6745,6861,6945,6965,\n    7201,7226,7231,7264,7494,7797,8309,8317,8329,8330,8905,8908,8913,8916,8926,\n    8934,8939,8972,9337,9446,9775,9791,9808,10127,10543,10552 ]\n\nprint(f'Number of ids with target1 error: {len(ids_target1_error)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if ids_error_corrected:\n    train_df.at[train_df['id'].isin(ids_target1_error),'target_new'] = 0\ntrain_df[train_df['id'].isin(ids_target1_error)][:20]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## target 0 --> 1\n1. about \"FedEx\" : 832, 833, 836, 841, 851, 859, 860, 864, 868, 874, 878\n2. about \"Hazardous Weather Outlook\" : 5990, 6002\n3. about \"school bus hijacker\" : 6188, 6192, 6211"},{"metadata":{"trusted":true},"cell_type":"code","source":"ids_target0_error = [\n    832,833,836,841,851,859,860,864,868,874,878,903,5990,6002,6188,6192,6211]\n\nprint(f'Length of ids_error_target0: {len(ids_target0_error)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if ids_error_corrected:\n    train_df.at[train_df['id'].isin(ids_target0_error),'target_new'] = 1\ntrain_df[train_df['id'].isin(ids_target0_error)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualizing the Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Displaying target distribution.\nfig, axes = plt.subplots(ncols=2, nrows=1, figsize=(12, 4), dpi=100)\nsns.countplot(train_df['target_new'], ax=axes[0])\naxes[1].pie(train_df['target_new'].value_counts(),\n            labels=['Not Disaster', 'Disaster'],\n            autopct='%1.2f%%',\n            shadow=True,\n            explode=(0.05, 0),\n            startangle=60)\nfig.suptitle('Distribution of the Tweets', fontsize=24)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Comparing word counts Word Counts\ndef plot_word_number_histogram(textno, textyes):\n\n    fig, axes = plt.subplots(ncols=2, nrows=1, figsize=(13, 5), sharey=True)\n    sns.distplot(textno.str.split().map(lambda x: len(x)), ax=axes[0], color='#e74c3c')\n    sns.distplot(textyes.str.split().map(lambda x: len(x)), ax=axes[1], color='#e74c3c')\n    \n    axes[0].set_xlabel('Word Count')\n    axes[0].set_ylabel('Frequency')\n    axes[0].set_title('Non Disaster Tweets')\n    axes[1].set_xlabel('Word Count')\n    axes[1].set_title('Disaster Tweets')\n    \n    fig.suptitle('Words Per Tweet', fontsize=24, va='baseline')\n    fig.tight_layout()\n    \n# number of words per tweet\ntextno = train_df[train_df['target_new'] == 0]\ntextyes = train_df[train_df['target_new'] == 1]\nplot_word_number_histogram(textno['text'], textyes['text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Comparing average Word Length\ndef plot_word_len_histogram(textno, textyes):\n\n    fig, axes = plt.subplots(ncols=2, nrows=1, figsize=(13, 5), sharey=True)\n    sns.distplot(textno.str.split().apply(\n                 lambda x: [len(i) for i in x]).map(\n                 lambda x: np.mean(x)), ax=axes[0], color='#e74c3c')\n    sns.distplot(textyes.str.split().apply(\n                 lambda x: [len(i) for i in x]).map(\n                 lambda x: np.mean(x)), ax=axes[1], color='#e74c3c')\n    axes[0].set_xlabel('Word Length')\n    axes[0].set_ylabel('Frequency')\n    axes[0].set_title('Non Disaster Tweets')\n    axes[1].set_xlabel('Word Length')\n    axes[1].set_title('Disaster Tweets')\n    fig.suptitle('Mean Word Lengths', fontsize=24, va='baseline')\n    fig.tight_layout()\n    \nplot_word_len_histogram(textno['text'], textyes['text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Displaying most common words.\nfig, axes = plt.subplots(1, 2, figsize=(18, 8))\naxes = axes.flatten()\n\ntrain_df['text_lemma'] = train_df['text'].apply(lambda x: remove_stopwords(x))\ntrain_df['text_lemma'] = train_df['text_lemma'].apply(lambda x: lemmatizing(x))\n\nlis = [train_df[train_df['target_new'] == 0]['text_lemma'],\n       train_df[train_df['target_new'] == 1]['text_lemma']]\n\nfor i, j in zip(lis, axes):\n    new = i.str.split()\n    new = new.values.tolist()\n    corpus = [word for i in new for word in i]\n    counter = Counter(corpus)\n    most = counter.most_common()\n    x, y = [], []\n    for word, count in most[:30]:\n        if (word not in stop_words):\n            x.append(word)\n            y.append(count)\n    sns.barplot(x=y, y=x, palette='plasma', ax=j)\n    \naxes[0].set_title('Non Disaster Tweets')\naxes[1].set_title('Disaster Tweets')\naxes[0].set_xlabel('Count')\naxes[0].set_ylabel('Word')\naxes[1].set_xlabel('Count')\naxes[1].set_ylabel('Word')\n\nfig.suptitle('Most Common Unigrams', fontsize=24, va='baseline')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot most common ngrams\ndef ngrams(n, title):\n    \n    fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n    axes = axes.flatten()\n    for i, j in zip(lis, axes):\n        new = i.str.split()\n        new = new.values.tolist()\n        corpus = [word for i in new for word in i]\n\n        def _get_top_ngram(corpus, n=None):\n            #getting top ngrams\n            vec = CountVectorizer(ngram_range=(n, n), max_df=0.9,\n                                  stop_words='english').fit(corpus)\n            bag_of_words = vec.transform(corpus)\n            sum_words = bag_of_words.sum(axis=0)\n            words_freq = [(word, sum_words[0, idx])\n                          for word, idx in vec.vocabulary_.items()]\n            words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n            return words_freq[:15]\n\n        top_n_bigrams = _get_top_ngram(i, n)[:15]\n        x, y = map(list, zip(*top_n_bigrams))\n        sns.barplot(x=y, y=x, palette='plasma', ax=j)\n        axes[0].set_title('Non Disaster Tweets')\n        axes[1].set_title('Disaster Tweets')\n        axes[0].set_xlabel('Count')\n        axes[0].set_ylabel('Words')\n        axes[1].set_xlabel('Count')\n        axes[1].set_ylabel('Words')\n        fig.suptitle(title, fontsize=24, va='baseline')\n        plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Bigrams\nngrams(2, 'Most Common Bigrams')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Trigrams\nngrams(3, 'Most Common Trigrams')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# BERT using TFHub"},{"metadata":{"trusted":true},"cell_type":"code","source":"# We will use the official tokenization script created by the Google team\n!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py\nimport tokenization\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def bert_encode(texts, tokenizer, max_len=512): \n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(bert_layer, max_len=512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    \n    if Dropout_num == 0:\n        # Without Dropout\n        out = Dense(1, activation='sigmoid')(clf_output)\n    else:\n        # With Dropout(Dropout_num), Dropout_num > 0\n        x = Dropout(Dropout_num)(clf_output)\n        out = Dense(1, activation='sigmoid')(x)\n\n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    model.compile(Adam(lr=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Build and train BERT model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load BERT from the Tensorflow Hub\nmodule_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\nbert_layer = hub.KerasLayer(module_url, trainable=True)\n\n# Load tokenizer from the bert layer\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encode the text into tokens, masks, and segment flags  \ntrain_input = bert_encode(train_df['text'].values, tokenizer, max_len=Max_length)\ntest_input = bert_encode(test_df['text'].values, tokenizer, max_len=Max_length)\ntrain_labels = train_df['target_new'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build BERT model with my tuning\nmodel = build_model(bert_layer, max_len=Max_length)  \nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train BERT model with my tuning\ncheckpoint = ModelCheckpoint('model_BERT.h5', monitor='val_loss', save_best_only=True)\n\ntrain_history = model.fit(\n    train_input, train_labels,\n    validation_split = valid,\n    epochs = epochs_num, # recomended 3-5 epochs\n    callbacks=[checkpoint],\n    batch_size = batch_size_num\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction and Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_weights('model_BERT.h5')\n\n# for the testing data\ntest_pred = model.predict(test_input)\ntest_pred_int = test_pred.round().astype('int')\n\n# for the training data - for the Confusion Matrix\ntrain_pred = model.predict(train_input)\ntrain_pred_int = train_pred.round().astype('int')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Showing Confusion Matrix\ndef plot_cm(y_true, y_pred, title, figsize=(5,5)):\n    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n    cm_sum = np.sum(cm, axis=1, keepdims=True)\n    cm_perc = cm / cm_sum.astype(float) * 100\n    annot = np.empty_like(cm).astype(str)\n    nrows, ncols = cm.shape\n    for i in range(nrows):\n        for j in range(ncols):\n            c = cm[i, j]\n            p = cm_perc[i, j]\n            if i == j:\n                s = cm_sum[i]\n                annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n            elif c == 0: annot[i, j] = ''\n            else: annot[i, j] = '%.1f%%\\n%d' % (p, c)\n    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n    cm.index.name = 'Actual'\n    cm.columns.name = 'Predicted'\n    fig, ax = plt.subplots(figsize=figsize)\n    plt.title(title)\n    sns.heatmap(cm, cmap= \"YlGnBu\", annot=annot, fmt='', ax=ax)\n    \n# Confusion Matrix (Original target)\nplot_cm(train_df.target.values, train_pred_int, 'Confusion matrix(Original target)', figsize=(6,6))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion Matrix (target relabeled)\nplot_cm(train_df.target_new.values, train_pred_int, 'Confusion matrix(target relabeled)', figsize=(6,6))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('ggplot') \npred = pd.DataFrame(test_pred, columns=['preds'])\npred.plot.hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Submission by BERT\nsubmission['target'] = test_pred_int\nsubmission.to_csv(\"submission_final.csv\", index=False)\nsubmission.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prediction from user input"},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode(text, tokenizer, max_len=512):\n    \n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    text = tokenizer.tokenize(str(text))  \n    text = text[:max_len-2]\n    input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n    pad_len = max_len - len(input_sequence)\n        \n    tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n    tokens += [0] * pad_len\n    pad_masks = [1] * len(input_sequence) + [0] * pad_len\n    segment_ids = [0] * max_len\n        \n    all_tokens.append(tokens)\n    all_masks.append(pad_masks)\n    all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ResultDict={1:'Disaster', 0:'Non Disaster'}   \ndef predict_review(input_text):\n    input_seq = encode(input_text, tokenizer, max_len=Max_length)\n    predict_result = model.predict(input_seq)\n    i = predict_result[0][0].round().astype('int')\n    print('Input:', input_text) \n    pre_score = round(float(predict_result[0][0])*100, 4)\n    print(f'Output: {ResultDict[i]} ({pre_score}%)\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"userInput1 = '''Fire shuts down part of NJ Turnpike 96'''\nuserInput2 = '''600 passengers abandoned at LRT station during Tuesday's hailstorm # yyc # Calgary Storm # Alberta Storm'''\nuserInput3 = '''How did I know as soon as I walked out of class that Calgary would flood again today'''\nuserInput4 = '''for sixth year in a row premium costs for windstorm insurance to climb . this time by 5 percent'''\nuserInput5 = '''Truth... #News #BBC #CNN #Islam #Truth #god #ISIS #terrorism #Quran #Lies'''\nuserInput6 = '''Here is how media in Pakistan covered the capture of terrorist Mohammed Naved'''\nuserInput7 = '''Hellfire is surrounded by desires so be careful and donÛªt let your desires control you! #Afterlife'''\nuserInput8 = '''Who is bringing the tornadoes and floods. Who is bringing the climate change. #FARRAKHAN #QUOTE'''    \nuserInput9 = '''RT NotExplained: The only known image of infamous hijacker D.B. Cooper.'''  \nuserInput10 = '''Hollywood Movie About Trapped Miners Released in Chile'''\nuserInput11 = '''Texas Seeks Comment on Rules for Changes to Windstorm Insurer'''\nuserInput12 = '''TWIA board approves 5 percent rate hike : The TWIA Board of Directors...'''\nuserInput13 = '''Bayelsa poll : Tension in Bayelsa as Patience Jonathan plans to hijack APC PDP..'''\nuserInput14 = '''A look at state actions a year after Ferguson ' s upheaval'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_review(userInput1)\npredict_review(userInput2)\npredict_review(userInput3)\npredict_review(userInput4)\npredict_review(userInput5)\npredict_review(userInput6)\npredict_review(userInput7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_review(userInput8) \npredict_review(userInput9)  \npredict_review(userInput10) \npredict_review(userInput11) \npredict_review(userInput12) \npredict_review(userInput13) \npredict_review(userInput14) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Reference\n1. NLP - EDA, Bag of Words, TF IDF, GloVe, BERT\n    * kaggle.com/vbmokin/nlp-eda-bag-of-words-tf-idf-glove-bert\n2. NLP with Disaster Tweets - EDA, Cleaning and BERT\n    * kaggle.com/gunesevitan/nlp-with-disaster-tweets-eda-cleaning-and-bert\n3. Disaster NLP: Keras BERT using TFHub\n    * kaggle.com/xhlulu/disaster-nlp-keras-bert-using-tfhub\n4. Classifying Disaster Tweets (Top 30%)\n    * kaggle.com/apresswala52/classifying-disaster-tweets-top-30"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}