{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Dense, Input, GlobalAveragePooling1D, Dropout, SpatialDropout1D\nfrom tensorflow.keras.layers import LSTM, Embedding\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\n\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n\n\ndef plot_loss_evaluation(r):\n    plt.figure(figsize=(12, 8))\n\n    plt.subplot(2, 2, 1)\n    plt.plot(r.history['loss'], label='loss')\n    plt.plot(r.history['val_loss'], label='val_loss')\n    plt.legend()\n\n    plt.subplot(2, 2, 2)\n    plt.plot(r.history['accuracy'], label='accuracy')\n    plt.plot(r.history['val_accuracy'], label='val_acc')\n    plt.legend()\n    \n    plt.title('Training and Loss fuction evolution')\n    \ndef evaluate(model, X_train, X_test, y_train, y_test):\n    y_pred_train = np.round(model.predict(X_train))\n    y_pred_test = np.round(model.predict(X_test))\n    \n    print(\"=============Training Data===============\")\n    print(confusion_matrix(y_train, y_pred_train))\n    print(classification_report(y_train, y_pred_train))\n    print(f\"Accuracy score: {accuracy_score(y_train, y_pred_train) * 100:.2f}%\")\n    \n    print(\"=============Testing Data===============\")\n    print(confusion_matrix(y_test, y_pred_test))\n    print(classification_report(y_test, y_pred_test))\n    print(f\"Accuracy score: {accuracy_score(y_test, y_pred_test) * 100:.2f}%\")\n    \ndata = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\n\nprint('=============Splitting the data=============')\nX = data.text\ny = data.target\nprint(f'Data shape: {data.shape}')\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nprint(f'X_Train shape: {X_train.shape}, y_train shape: {y_train.shape}')\nprint(f'X_Test shape: {X_test.shape}, y_test shape: {y_test.shape}')\n\nprint('==============Convert Sentences to Sequences================')\nMAX_VOCAB_SIZE = 20000\ntokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, char_level=False)\ntokenizer.fit_on_texts(X_train)\nsequences_train = tokenizer.texts_to_sequences(X_train)\nsequences_test = tokenizer.texts_to_sequences(X_test)\n\n# pad sequence do that we get a NxT matrix\ndata_train = pad_sequences(sequences_train)\ndata_test = pad_sequences(sequences_test, maxlen=data_train.shape[1])\nprint(f\"Found {len(tokenizer.word_index)} unique tokens.\")\nprint(f\"Training Data shape: {data_train.shape}\")\nprint(f\"Testing Data shape: {data_test.shape}\")\n\nprint('===============Create The Model==========================')\n# We get to choose embedding dimensionality\nD = 100\n# Hidden state dimentionality\nM = 64\nV = len(tokenizer.word_index)\nT = data_train.shape[1]\n\n# model.add(embedding)\n# model.add(SpatialDropout1D(0.2))\n# model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n# model.add(Dense(1, activation='sigmoid'))\n\ni = Input(shape=(T,))\nx = Embedding(V + 1, D)(i)\nx = SpatialDropout1D(0.2)(x)\nx = LSTM(M, return_sequences=True, activation='relu')(x)\nx = GlobalAveragePooling1D()(x)\n# x = Dropout(0.2)(x)\nx = Dense(1, activation='sigmoid')(x)\n\nmodel = Model(i, x)\noptimizer = Adam(learning_rate=1e-5)\n# Compile and fit\nmodel.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\nprint('Training model...........')\nr = model.fit(data_train, y_train, epochs=15, \n              validation_data=(data_test, y_test), \n              batch_size=1)\n\nprint('================Model Evaluation=====================')\nevaluate(model, data_train, data_test, y_train, y_test)\nplot_loss_evaluation(r)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# 4. Making submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"# /kaggle/input/nlp-getting-started/test.csv\n# /kaggle/input/nlp-getting-started/sample_submission.csv\ntest = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\n\nprint('==============Convert Sentences to Sequences================')\nsequences_test = tokenizer.texts_to_sequences(test.text)\n\n# pad sequence do that we get a NxT matrix\ndata_test = pad_sequences(sequences_test, maxlen=data_train.shape[1])\nprint(f\"Found {len(tokenizer.word_index)} unique tokens.\")\nprint(f\"Testing Data shape: {data_test.shape}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_sub=pd.read_csv('../input/nlp-getting-started/sample_submission.csv')\ny_pre = model.predict(data_test)\ny_pre = np.round(y_pre).astype(int).reshape(3263)\nsub = pd.DataFrame({'id':sample_sub['id'].values.tolist(), 'target':y_pre})\nsub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}