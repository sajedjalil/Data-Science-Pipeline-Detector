{"cells":[{"metadata":{},"cell_type":"markdown","source":"Importing Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"markdown","source":"Importing Dataset"},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"import pandas as pd\nsample_submission = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")\ntest = pd.read_csv(\"../input/nlp-getting-started/test.csv\")\ntrain = pd.read_csv(\"../input/nlp-getting-started/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Steps\nThe problems seems to be of three steps:\n* Preprocessing of words\n* Word2Vec training on our dataset\n* Prediction using vector outputted by Word2Vec."},{"metadata":{},"cell_type":"markdown","source":"# Step 1\nText Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\nimport re\ndef clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['text'] = train['text'].apply(lambda x: clean_text(x))\ntest['text'] = test['text'].apply(lambda x: clean_text(x))\n\n# Let's take a look at the updated text\ntrain['text'].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 2 : Training Word2Vec on our dataset from scratch."},{"metadata":{},"cell_type":"markdown","source":"Well there are two methods of implementation for Word2Vec on a given dataset.\n* Either train from scratch.\n* Or use google pretrained word2vec model.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.models import Word2Vec, KeyedVectors\nimport nltk","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tokenizing the training and the test set\ntokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\ntrain['text'] = train['text'].apply(lambda x: tokenizer.tokenize(x))\ntest['text'] = test['text'].apply(lambda x: tokenizer.tokenize(x))\ntrain['text'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets remove the stopwords as it does not seems of any meaning","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords \ndef remove_stopwords(text):\n    \"\"\"\n    Removing stopwords belonging to english language\n    \n    \"\"\"\n    words = [w for w in text if w not in stopwords.words('english')]\n    return words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['text'] = train['text'].apply(lambda x : remove_stopwords(x))\ntest['text'] = test['text'].apply(lambda x : remove_stopwords(x))\ntrain.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"let us concatenate both test and train column to get a larger corpus containing larger no of words."},{"metadata":{"trusted":true},"cell_type":"code","source":"test['target'] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def combine_text(list_of_text):\n    '''Takes a list of text and combines them into one large chunk of text.'''\n    combined_text = ' '.join(list_of_text)\n    return combined_text\n\ntrain['text'] = train['text'].apply(lambda x : combine_text(x))\ntest['text'] = test['text'].apply(lambda x : combine_text(x))\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now we will make a corpus of words to start word2vec training.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.concat([train,test])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus = df['text'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Corpus_list = [nltk.word_tokenize(title) for title in corpus]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"Corpus_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Word2Vec(Corpus_list,min_count=1,size = 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.most_similar('death')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"We can also used google pretrained saved model on newspaper as no of trained words much larger corpus and would give more accurate results then this small dataset"},{"metadata":{},"cell_type":"markdown","source":"Importing the pretrained vectors using this link\n\nhttps://www.kaggle.com/umbertogriffo/googles-trained-word2vec-model-in-python"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pwd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = \"../input/googles-trained-word2vec-model-in-python/GoogleNews-vectors-negative300.bin\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gensim\nmodel = gensim.models.KeyedVectors.load_word2vec_format(path,binary=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets play around with this 300 sized vector space for all words."},{"metadata":{"trusted":true},"cell_type":"code","source":"w = model[\"hello\"]\nprint(len(w))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we can see their quite some diffrences between the pretrained vector on our dataset and that on google newspaper dataset."},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"print(w)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If you are very much intrested in this than follow this link\n\nhttps://code.google.com/archive/p/word2vec/"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Now to represent the word we must convert the whole word to something small value of single numerical.So right now covert the 300 vector word to a single value by simple averaging defined by the class below.\nAlso it seems that the pretrained vector is much better than the that trained on our small dataset.So we will carry on with this only."},{"metadata":{"trusted":true},"cell_type":"code","source":"class MeanEmbeddingVectorizer(object):\n\n    def __init__(self, word_model):\n        self.word_model = word_model\n        self.vector_size = word_model.wv.vector_size\n\n    def fit(self):  # comply with scikit-learn transformer requirement\n        return self\n\n    def transform(self, docs):  # comply with scikit-learn transformer requirement\n        doc_word_vector = self.word_average_list(docs)\n        return doc_word_vector\n\n    def word_average(self, sent):\n        \"\"\"\n        Compute average word vector for a single doc/sentence.\n\n\n        :param sent: list of sentence tokens\n        :return:\n            mean: float of averaging word vectors\n        \"\"\"\n        mean = []\n        for word in sent:\n            if word in self.word_model.wv.vocab:\n                mean.append(self.word_model.wv.get_vector(word))\n\n        if not mean:  # empty words\n            # If a text is empty, return a vector of zeros.\n            #logging.warning(\"cannot compute average owing to no vector for {}\".format(sent))\n            return np.zeros(self.vector_size)\n        else:\n            mean = np.array(mean).mean(axis=0)\n            return mean\n\n\n    def word_average_list(self, docs):\n        \"\"\"\n        Compute average word vector for multiple docs, where docs had been tokenized.\n\n        :param docs: list of sentence in list of separated tokens\n        :return:\n            array of average word vector in shape (len(docs),)\n        \"\"\"\n        return np.vstack([self.word_average(sent) for sent in docs])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In case you want to use different technique than averaging we can use tfidf using this technique."},{"metadata":{"trusted":true},"cell_type":"code","source":"class TfidfEmbeddingVectorizer(object):\n    def __init__(self, word2vec):\n        self.word2vec = word2vec\n        self.word2weight = None\n        self.dim = len(word2vec.itervalues().next())\n\n    def fit(self, X, y):\n        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n        tfidf.fit(X)\n        # if a word was never seen - it must be at least as infrequent\n        # as any of the known words - so the default idf is the max of \n        # known idf's\n        max_idf = max(tfidf.idf_)\n        self.word2weight = defaultdict(\n            lambda: max_idf,\n            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n\n        return self\n\n    def transform(self, X):\n        return np.array([\n                np.mean([self.word2vec[w] * self.word2weight[w]\n                         for w in words if w in self.word2vec] or\n                        [np.zeros(self.dim)], axis=0)\n                for words in X\n            ])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_vec_tr = MeanEmbeddingVectorizer(model)\ndoc_vec = mean_vec_tr.transform(Corpus_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Shape of word-mean doc2vec...')\ndisplay(doc_vec.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Corpus_train = train['text'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_corpus = [nltk.word_tokenize(title) for title in Corpus_train]\ndoc_vec_1 = mean_vec_tr.transform(train_corpus)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_corpus)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Shape of word-mean doc2vec...')\ndisplay(doc_vec_1.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Corpus_test = test['text'].values\ntest_corpus = [nltk.word_tokenize(title) for title in Corpus_test]\ndoc_vec_2 = mean_vec_tr.transform(test_corpus)\nprint('Shape of word-mean doc2vec...')\ndisplay(doc_vec_2.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = doc_vec_1\ny = train['target']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 3 : Training the model and predicting test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import model_selection\nclf = LogisticRegression(C=1.0)\nscores = model_selection.cross_val_score(clf,X,y, cv=5, scoring=\"f1\")\nscores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_vec_test = doc_vec_2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission_1 = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")\nsample_submission_1[\"target\"] = clf.predict(X_vec_test)\nsample_submission_1.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using Advance Algorithms","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import f1_score\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# xgb_model = XGBClassifier()\n\n# #brute force scan for all parameters, here are the tricks\n# #usually max_depth is 6,7,8\n# #learning rate is around 0.05, but small changes may make big diff\n# #tuning min_child_weight subsample colsample_bytree can have \n# #much fun of fighting against overfit \n# #n_estimators is how many round of boosting\n# #finally, ensemble xgboost with multiple seeds may reduce variance\n# parameters = {'nthread':[4], #when use hyperthread, xgboost may become slower\n#               'objective':['binary:logistic'],\n#               'learning_rate': [0.05,0.01,0.1], #so called `eta` value\n#               'max_depth': [6,7,8,10],\n#               'min_child_weight': [11],\n#               'silent': [1],\n#               'subsample': [0.8],\n#               'colsample_bytree': [0.7,0.6,.5],\n#               'n_estimators': [100,1000], #number of trees, change it to 1000 for better results\n#               'missing':[-999],\n#               'seed': [1337]}\n\n\n# clf = GridSearchCV(xgb_model, parameters, n_jobs=5,  \n#                    scoring='roc_auc',\n#                    verbose=2, refit=True)\n\n# clf.fit(X,y)\n\n# #trust your CV!\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(\"Best parameters set found on development set:\")\n# print()\n# print(clf.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = XGBClassifier(colsample_bytree=0.7, learning_rate= 0.05, max_depth= 8,\n                    min_child_weight=11, missing= -999, n_estimators= 1000,\n                    nthread= 4, objective='binary:logistic', seed=1337, silent=1, subsample=0.8)\nscores = model_selection.cross_val_score(clf,X,y, cv=5, scoring=\"f1\")\nscores\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission_1 = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")\nsample_submission_1[\"target\"] = clf.predict(X_vec_test)\nsample_submission_1.to_csv(\"submission_3.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}