{"cells":[{"metadata":{},"cell_type":"markdown","source":"# JAX for NLP\n\n## Installing JAX on GPU\n\nTo install JAX with GPU support, we first need to figure out which Python version and CUDA version are installed in our machine.\nTo do so, we just need to run the following two commands:"},{"metadata":{"trusted":true},"cell_type":"code","source":"!nvcc --version\n!python --version","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"According to the command's output, we have the following python and CUDA versions:\n- Python 3.6\n- CUDA 10.1\n\nLastly, to install JAX, we have to retrieve its wheels with GPU support from: `https://storage.googleapis.com/jax-releases/\\${CUDA_VER}/jaxlib-0.1.39-\\${PYTHON_VER}-none-linux_x86_64.whl`"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!pip install --upgrade \"https://storage.googleapis.com/jax-releases/cuda101/jaxlib-0.1.39-cp36-none-linux_x86_64.whl\"\n!pip install --upgrade jax ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are done here, now we can start with the cool part ðŸ˜Š"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import jax\nimport jax.numpy as np\n\nkey = jax.random.PRNGKey(0)\n\nprint('JAX is running on', jax.lib.xla_bridge.get_backend().platform)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Model explanation\n\nIn this notebook, we are going to implement a Natural Language Processing (NLP) model based on \"Word Embeddings\".   \n\n\"Word embeddings\" are a family of natural language processing techniques aiming at mapping semantic meaning into a geometric space. This is done by associating a numeric vector to every word in a dictionary, such that the distance (e.g. L2 distance or more commonly cosine distance) between any two vectors would capture part of the semantic relationship between the two associated words. The geometric space formed by these vectors is called an embedding space (FraÃ§ois Chollet, 2016).\n\nSince this competition is a *getting-started* one, we do not have enough data to build a set of robust embeddings, hence we are going to use embeddings that were pretrained on a huge text corpus, more precisely, we are going to use the Global Vectors for Word Representation [(GloVe)](https://nlp.stanford.edu/projects/glove/) embeddings.\n\nOn top of the GloVe embeddings, we are going to stack a set of 1D Convolutional layers and finally, a pair of linear or dense layers to do the classification. \n\nBut wait... Does JAX provide all of those features to work with NLP? For now, it doesn't. \n\nVarious python packages provide extra Deep Learning features over JAX, for example, the most popular packages nowadays are FLAX, Haiku, and STAX.\n\nIn this notebook we are not going to use any extra package apart from JAX, we are going to implement all the needed Layers from scratch.\n\n## Data preprocess\n\nBefore the implementation, we are going to take a quick look to the data and build our vocabulary."},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ../input/nlp-getting-started","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv('../input/nlp-getting-started/train.csv', encoding='utf-8')\ntest_df = pd.read_csv('../input/nlp-getting-started/test.csv', encoding='utf-8')\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we are going to clean the data applying these simple steps:\n\n1. Remove URLs\n2. Remove HTML Tags\n3. Remove Emojis\n4. Remove punctuation\n5. Lowercase all text"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport string\n\ndef clean_tweet(tweet: str) -> str:\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    tweet = url.sub(r'',tweet)\n    \n    html = re.compile(r'<.*?>')\n    tweet = html.sub(r'', tweet)\n    \n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    tweet = emoji_pattern.sub(r'', tweet)\n    \n    tweet = re.sub('([.,!?()#])', r' \\1 ', tweet)\n    tweet = re.sub('\\s{2,}', ' ', tweet)\n\n    return tweet.lower()\n\ndf['text'] = df['text'].apply(clean_tweet)\ntest_df['text'] = test_df['text'].apply(clean_tweet)\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are going to use Keras text preprocessing tools to create our vocabulary. This will help us create the vocabulary and also pad the sequences in an intuitive manner."},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nMAX_SEQUENCE_LENGTH = 64\n\ntokenizer = Tokenizer(num_words=None, filters='')\ntokenizer.fit_on_texts(df.text.tolist() + test_df.text.tolist())\n\ntrain_sequences = tokenizer.texts_to_sequences(df.text.tolist())\ntrain_sequences = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n\ntest_sequences = tokenizer.texts_to_sequences(test_df.text.tolist())\ntest_sequences = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n\nword2idx = tokenizer.word_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(word2idx.items())[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Layers implementation\n\nBefore starting the layers implementation, it is a good practice to define a common abstraction for all our modules. You can think of Keras using the superclass `Layer` and PyTorch using the `nn.Module` class.\n For simplicity our superclass will be a named tuple called `JaxModule`."},{"metadata":{"trusted":true},"cell_type":"code","source":"from typing import Any, Callable, Container, NamedTuple\n\nParameter = Container[np.ndarray]\nForwardFn = Callable[[Parameter, np.ndarray, Any], np.ndarray]\n\nclass JaxModule(NamedTuple):\n    # Dict or list contining the layer parameters\n    parameters: Parameter\n    \n    # How we operate with parameters to generate an output\n    forward_fn: ForwardFn\n    \n    def update(self, new_parameters) -> 'JaxModule':\n        # As tuples are immutable, we create a new jax module keeping the\n        # forward_fn but with new parameters\n        return JaxModule(new_parameters, self.forward_fn)\n    \n    def __call__(self, x: np.ndarray, **kwargs) -> np.ndarray:\n        # Automatically injects parameters\n        return self.forward_fn(self.parameters, x, **kwargs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To understand how we are going to use the `JaxModule` along the implementations, let's see how a simple linear layer would be implemented.\n\nAs you may know, a linear layer follows the below equation:\n\n$ f(x) = W Â· x + b $ where W and b are trainable parameters."},{"metadata":{"trusted":true},"cell_type":"code","source":"def linear(key: np.ndarray, \n           in_features: int, \n           out_features: int, \n           activation = lambda x: x) -> 'JaxModule':\n    x_init = jax.nn.initializers.xavier_normal()\n    u_init = jax.nn.initializers.uniform()\n\n    key, W_key, b_key = jax.random.split(key, 3)\n    W = x_init(W_key, shape=(in_features, out_features))\n    b = u_init(b_key, shape=(out_features,))\n    params = dict(W=W, bias=b)\n    \n    def forward_fn(params: Parameter, x: np.ndarray, **kwargs):\n        return np.dot(x, params['W']) + params['bias']\n    \n    return JaxModule(parameters=params, forward_fn=forward_fn)\n\ndef flatten(key: np.ndarray) -> 'JaxModule':\n    # Reshapes the data to have 2 dims [BATCH, FEATURES]\n    def forward_fn(params, x, **kwargs):\n        bs = x.shape[0]\n        return x.reshape(bs, -1)\n    return JaxModule({}, forward_fn)\n\n\nkey, subkey, linear_key = jax.random.split(key, 3)\n\n# Input vector of shape [BATCH, FEATURES]\nmock_in = jax.random.uniform(subkey, shape=(10, 32))\nlinear_layer = linear(linear_key, 32, 512)\n\nprint('Input shape:', mock_in.shape, \n      'Output shape after linear:', linear_layer(mock_in).shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Pretty easy, isn't it? Let's move to Convolutional 1D implementation. \n\nThis one is going to be a little bit more tricky. This is because we will get a bit deeper and use with a JAX primitive. A JAX primitive it is a function that directly wraps an XLA operation. \n\nSo, to develop the 1D Conv, we are going to need the primitive `conv_general_dilated`. We won't get into details on how to use this primitive here. If you are interested in it, I recommend you reading [this section](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#%F0%9F%94%AA-Convolutions) of JAX documentation."},{"metadata":{"trusted":true},"cell_type":"code","source":"def conv_1d(key: np.ndarray, \n            in_features: int, \n            out_features: int,\n            kernel_size: int,\n            strides: int = 1,\n            padding: str = 'SAME',\n            activation = lambda x: x) -> 'JaxModule':\n    \n    # [KERNEL_WIDTH, IN_FEATURES, OUT_FEATURES]\n    kernel_shape = (kernel_size, in_features, out_features)\n    # [BATCH, WIDTH, IN_FEATURES]\n    seq_shape = (None, None, in_features)\n    \n    # Declare convolutional specs\n    dn = jax.lax.conv_dimension_numbers(\n        seq_shape, kernel_shape, ('NWC', 'WIO', 'NWC'))\n    \n    key, k_key, b_key = jax.random.split(key, 3)\n    \n    kernel = jax.nn.initializers.glorot_normal()(k_key, shape=kernel_shape)\n    b = jax.nn.initializers.uniform()(b_key, shape=(out_features,))\n    params = dict(kernel=kernel, bias=b)\n    \n    def forward_fn(params: Parameter, x: np.ndarray, **kwargs):\n        return activation(jax.lax.conv_general_dilated(\n            x, params['kernel'], \n            (strides,), padding, \n            (1,), (1,), dimension_numbers=dn) + params['bias'])\n\n    return JaxModule(params, forward_fn)\n\n\nkey, subkey, conv_key = jax.random.split(key, 3)\n\n# Input vector of shape [BATCH, FEATURES]\nmock_in = jax.random.uniform(subkey, shape=(10, 128, 32))\nconv = conv_1d(conv_key, 32, 512, kernel_size=3, strides=2)\n\nprint('Input shape:', mock_in.shape, \n      'Output shape after 1D Convolution:', conv(mock_in).shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are almost done. We only have the Embedding layer implementation left. Believe me when I say that this implementation is the simplest that we are going to see today. \n\nActually, as we won't train embeddings from scratch, we don't really need a JaxModule initialized with random weights. What we are going to do, is just load the embeddings from a GloVe file and create a matrix where each row corresponds to one embedding of our vocabulary.\n\nSurisingly, NLP Stanford GloVe webpage has a set of embedding pretrained on Twitter text corpus available to download, which is going to be pretty handy for this competion as we are dealing with tweets classification."},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ../input","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as onp # Use onp to avoid overheat of moving each embedding to GPU\n\nembeddings_index = {}\nembedding_matrix = onp.zeros((len(word2idx) + 1, 50))\n\nf = open('../input/glove-twitter-27b-50d/glove.twitter.27B.50d.txt')\n\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = onp.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nfor word, i in word2idx.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n\nembeddings = jax.device_put(embedding_matrix)\nprint('Embedding for \"hello\" is:', embeddings[word2idx['hello']])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We create a helper function to index our embedding matrix with a sequence."},{"metadata":{"trusted":true},"cell_type":"code","source":"def embed_sequence(sequence: np.ndarray) -> np.ndarray:\n    return embeddings[sequence.astype('int32')]\n\nkey, subkey, embedding_key = jax.random.split(key, 3)\n\n# Input vector of shape [BATCH, FEATURES]\nmock_in = jax.random.randint(subkey, \n                             minval=0,\n                             maxval=512,\n                             shape=(10, 128))\n\nprint('Input shape:', mock_in.shape, \n      'Output shape after Embeddings:', embed_sequence(mock_in).shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Bfff... was a long journey, wasn't it? ðŸ˜© But that's all we need to do for our implementation, let's move to model training.\n\n## Training a model\n\nFirst of all we have to declare our model. To do so, we are going to use a similar API interface as the nn.Sequential defined in PyTorch or tf.keras.models.Sequential defined in Keras."},{"metadata":{"trusted":true},"cell_type":"code","source":"from typing import Sequence\nfrom functools import partial\n\n# Partially evaluated layers without the random key\nPartialLayer = Callable[[np.ndarray], JaxModule]\n\ndef sequential(key: np.ndarray, *modules: Sequence[PartialLayer]) -> JaxModule:\n    key, *subkeys = jax.random.split(key, len(modules) + 1)\n    model = [m(k) for k, m in zip(subkeys, modules)]\n    \n    def forward_fn(params, x, **kwargs):\n        for m, p in zip(model, params):\n            x = m.forward_fn(p, x, **kwargs)\n        return x\n    \n    return JaxModule([m.parameters for m in model], forward_fn)\n\nmock_model = sequential(\n    key,\n    partial(conv_1d, \n            in_features=50, out_features=256, \n            kernel_size=5, strides=2, activation=jax.nn.relu),\n    flatten,\n    partial(linear, \n            in_features=16384, out_features=128, \n            activation=jax.nn.relu),\n    partial(linear, \n            in_features=128, out_features=1, \n            activation=jax.nn.sigmoid))\n\nembedded_in = embed_sequence(mock_in)\n\nprint('Input shape:', mock_in.shape, \n      'Output shape after model:', mock_model(embedded_in).shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With JAX we can easily compute the model's weights gradients with respect an error function. Since we are dealing with a binary classification, we are going to implement the binary cross entropy loss (BCE loss). The BCE Loss follows the equation described below:\n\n$ bce = - (y * log(\\hat{y}) + (1 - y) * log(1 - \\hat{y})) $"},{"metadata":{"trusted":true},"cell_type":"code","source":"def bce(y_hat: np.ndarray, y: np.ndarray) -> float:\n    y_hat = y_hat.reshape(-1)\n    y = y.reshape(-1)\n    y_hat = np.clip(y_hat, 1e-6, 1 - 1e-6)\n    pt = np.where(y == 1, y_hat, 1 - y_hat)\n    loss = -np.log(pt)\n    return np.mean(loss)\n\n\ndef create_backward(model: JaxModule):\n    # Backward is just a function that receives the model parameters and combines them\n    # using the forward_fn. This will allow as to compute the partial derivate of the \n    # weights with respect to the loss\n    def backward(params: Sequence[Parameter], \n                 x: np.ndarray, y: np.ndarray) -> float:\n        y_hat = model.forward_fn(params, x)\n        return bce(y_hat, y)\n    return backward\n\n# Compile and differentiate the function\nbackward_fn = jax.jit(jax.value_and_grad(create_backward(mock_model)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Once we are capable of computing the gradients with respect the loss, we need an optimizer to update our parameters in the correct direction so we can minimize the loss. For simplicity, we implement a basic stocastic gradient descent (SGD). The SGD update rule is as follows:\n\n`new_param = old_param - learning_rate * grad`"},{"metadata":{"trusted":true},"cell_type":"code","source":"LEARNING_RATE = 1e-4\n\ndef optimizer_step(params: Sequence[Parameter], \n                   gradients: Sequence[Parameter]) -> Sequence[Parameter]:\n    def optim_single(param, grad):\n        for p in param:\n            param[p] = param[p] - LEARNING_RATE * grad[p]\n        return param\n\n    return [optim_single(p, g) for p, g in zip(params, gradients)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's put it all together and try a single update on our model and see if the parameters get updated."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Mock labels\ny_trues = np.array([0, 1, 1, 0, 0, 1, 0, 0, 1, 0])\n\nloss, grads = backward_fn(mock_model.parameters, embedded_in, y_trues)\nprint('Loss:', loss)\n\n# Update the parameters with the obtained gradients\nnew_params = optimizer_step(mock_model.parameters, grads)\nmodel = mock_model.update(new_params)\n\nloss, grads = backward_fn(mock_model.parameters, embedded_in, y_trues)\nprint('Loss after one step:', loss)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we are ready to do what we already know how to do:\n\n1. Split the data to compute the metrics on a validation set and ensure that we are not overfitting the dataset\n2. Create a training loop and update the model at each training step\n3. Create the submission file"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx = train_sequences\ny = df.target.values\n\nx_train, x_val, y_train, y_val = train_test_split(x, y, train_size=.9)\n\nx_train = jax.device_put(x_train)\nx_val = jax.device_put(x_val)\ny_train = jax.device_put(y_train)\ny_val = jax.device_put(y_val)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We define the final model. The one that should work... ðŸ˜¯\n\n> The aim of this kernel is not to provide the best solution, it is just to guide and encourage you to use JAX and see how flexible it is."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = sequential(\n    key,\n    partial(conv_1d, in_features=50, out_features=128,  kernel_size=7),\n    partial(conv_1d, in_features=128, out_features=128, kernel_size=7, strides=2, activation=jax.nn.relu),\n    \n    partial(conv_1d, in_features=128, out_features=256, kernel_size=5),\n    partial(conv_1d, in_features=256, out_features=256, kernel_size=5, strides=2, activation=jax.nn.relu),\n    \n    flatten,\n    partial(linear, in_features=4096, out_features=128, activation=jax.nn.relu),\n    partial(linear, in_features=128, out_features=1, activation=jax.nn.sigmoid))\n\nbackward_fn = jax.jit(jax.value_and_grad(create_backward(model)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We define the train and validation steps."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score, recall_score\n\nEPOCHS = 20\nBATCH_SIZE = 32\ntrain_steps = x_train.shape[0] // BATCH_SIZE\n\nfor epoch in range(EPOCHS):\n    \n    running_loss = 0.0\n    \n    for step in range(train_steps):        \n        key, subkey = jax.random.split(key)\n        batch_idx = jax.random.randint(subkey, \n                                       minval=0, \n                                       maxval=x_train.shape[0],\n                                       shape=(BATCH_SIZE,))\n        \n        x_batch = embed_sequence(x_train[batch_idx])\n        y_batch = y_train[batch_idx]\n        \n        loss, grads = backward_fn(model.parameters, x_batch, y_batch)\n        running_loss += loss\n        model = model.update(optimizer_step(model.parameters, grads))\n        \n    loss_mean = running_loss / float(step)\n    print(f'Epoch[{epoch}] loss: {loss_mean:.4f}')\n        \n    predictions = model(embed_sequence(x_val))\n    loss = bce(predictions, y_val)\n    predictions = predictions > .5\n    \n    print('-- Validation --')\n    print('Loss: {}'.format(loss))\n    print('Accuracy: {}'.format(accuracy_score(y_val, predictions)))\n    print('Recall: {}'.format(recall_score(y_val, predictions)))\n    print('F1-Score: {}'.format(f1_score(y_val, predictions)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, we create the submission file. ðŸŽ‡ðŸŽ†"},{"metadata":{"trusted":true},"cell_type":"code","source":"ids = test_df.id\ntargets = (model(embed_sequence(test_sequences)) > .4).reshape(-1)\npd.DataFrame(dict(id=ids, target=targets.astype('int32'))).to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}