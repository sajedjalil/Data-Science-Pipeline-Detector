{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# From shallow learning to 2020 SOTA(GPT-2, ROBERTA) ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Abstract","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In this notebook I explore a variety of Machine learning models ranging from good old shallow learning(Naive naives, TF-IDF, SVMs) to the state of the art in NLP(GPT2, ROBERTA) with the goal of finding the best possible model and preprocessing steps for the task of tweet classification posted on this Kaggle competition: https://www.kaggle.com/c/nlp-getting-started  \nAs a by-product of this experimentation we also obtain a clear comparison across a number of popular NLP algorithms.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Note**: Because Kaggle kernels can't run for more than 9 hours, I had to train the biggest models on my local computer and this notebook loads them from a checkpoint. Also for grid search the notebook loads the cached results from csv to save compute time.  \nAll the checkpoints and grid search results loaded in this notebook where generated using solely the code in this notebook.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Index","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"- [Exploratory analysis](#epa)\n- [Shallow Learning](#shallow_learning)\n- [Fast text](#fast_text)\n- [Text preprocessing](#text_preprop)\n- [BERT & ROBERTA](#bert_e2e)\n- [LSTMs](#Conclusions)\n- [GPT2](#gpt2)\n- [Conclusions](#Conclusions)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\n%load_ext autoreload\n%autoreload 2\n!pip install wordcloud\n!pip install transformers==3.0.2\n!pip install simpletransformers\n!pip install sklearn\n!pip install nltk\n!pip install unidecode\n!pip install normalise\n!pip install contractions\nimport os\nimport string\nimport re\nimport sys\nsys.path.insert(1, '/kaggle/input/pymodules4/') # link modules to be accessible from this Kaggle kernel\nos.system('python3 -m spacy download en')# it doesnt work when running directly on terminal\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.base import TransformerMixin, BaseEstimator\nimport spacy\nimport nltk\nimport unidecode\nfrom normalise import normalise\nimport contractions\nfrom nltk.corpus import stopwords\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport matplotlib.pyplot as plt\nimport dill\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.utils.validation import check_X_y, check_array, check_is_fitted\nfrom sklearn.utils.multiclass import unique_labels\nimport logging\nfrom simpletransformers.classification import ClassificationModel\nfrom spacy_text_classifier import SpacyClassifier\ntrain_df = pd.read_csv(\"/kaggle/input/data-baby2/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/data-baby2/test.csv\")\nmsk = np.random.rand(len(train_df)) < 0.3\ndev_df = train_df[msk]\ntrain_df = train_df[~msk]\ndev_df.reset_index(inplace=True)\ntrain_df.reset_index(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id=\"epa\">Exploratory Analysis</a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### A quick look at our data\n\nLet's look at our data... first, an example of what is NOT a disaster tweet.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[train_df[\"target\"] == 0][\"text\"].values[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[train_df[\"target\"] == 1][\"text\"].values[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_has_keyword_df = train_df[train_df.keyword.notnull()]\ntrain_has_location_df = train_df[train_df.location.notnull()]\ntrain_has_keyword_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text = \" \".join(keyword for keyword in train_has_keyword_df.keyword)\nprint (\"There are {} words in the combination of all keywords.\".format(len(text)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nax = sns.countplot(train_df['target'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## wordclouds","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def renderWordcloud(text):\n    # Create and generate a word cloud image:\n    wordcloud = WordCloud().generate(text)\n\n    # Display the generated image:\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis(\"off\")\n    F = plt.gcf()\n    Size = F.get_size_inches()\n    F.set_size_inches(Size[0]*2, Size[1]*2, forward=True) # Set forward to True to resize window along with plot in figure.\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Wordcloud of keywords","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"text = \" \".join(keyword for keyword in train_has_keyword_df.keyword)\nrenderWordcloud(text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Wordcloud of locations","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"text = \" \".join(loc for loc in train_has_location_df.location)\nrenderWordcloud(text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Unique words","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_has_keyword_df.keyword.unique()), len(train_has_location_df.location.unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Relationship between categorical vars & target","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### relationship between keyword and target","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"group_keyword_sum_target = train_has_keyword_df.groupby(\"keyword\").sum().sort_values(\"target\")\ngroup_keyword_len = train_has_keyword_df.groupby(\"keyword\").count()\ngroup_keyword_sum_target#['true/all'] = group_keyword_sum_target.target / group_keyword_len.target","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_rows', None)\ngroup_keyword_sum_target","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### correlation between keyword and Target","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### contigency table","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_target_1 = train_has_keyword_df['target']==1\ndf_target_1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(train_has_keyword_df['keyword'], [df_target_1], rownames=['keyword'], colnames=['target'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### relationship between location and target: percentage of true targets per location","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"group_location_sum_target = train_has_location_df.groupby(\"location\").sum().sort_values(\"target\")\ngroup_location_len = train_has_location_df.groupby(\"location\").count()\ngroup_location_sum_target['true/all'] = group_location_sum_target.target / group_location_len.target","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"group_location_sum_target.sort_values(by=['target'], ascending=False)[:50]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_rows', 20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### How are numbers formatted in tweets? Wordcloud of words from tweets that contain numbers","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"I'm interested in this cause to see wether numbers hold a correlation to words/meaning of the tweet","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\npattern = re.compile(\"[0-9]\")\nnumbers_df = train_df[train_df['text'].str.contains('[0-9]', regex= True, na=False)]\nnumber_texts = [keyword if pattern.search(keyword) else None for keyword in numbers_df.text]\ntext = \" \".join(number_texts)\nrenderWordcloud(text)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Models","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Building vectors\n\nThe theory behind this model is pretty simple: the words contained in each tweet are a good indicator of whether they're about a real disaster or not (this is not entirely correct, but it's a great place to start).\n\nWe'll use scikit-learn's `CountVectorizer` to count the words in each tweet and turn them into data our machine learning model can process.\n\nNote: a `vector` is, in this context, a set of numbers that a machine learning model can work with. We'll look at one in just a second.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"count_vectorizer = feature_extraction.text.CountVectorizer()\n\n## let's get counts for the first 5 tweets in the data\nexample_train_vectors = count_vectorizer.fit_transform(train_df[\"text\"][0:5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## we use .todense() here because these vectors are \"sparse\" (only non-zero elements are kept to save space)\nprint(example_train_vectors[0].todense().shape)\nprint(example_train_vectors[0].todense())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"example_train_vectors[4]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above tells us that:\n1. There are 54 unique words (or \"tokens\") in the first five tweets.\n2. The first tweet contains only some of those unique tokens - all of the non-zero counts above are the tokens that DO exist in the first tweet.\n\nNow let's create vectors for all of our tweets.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_vectors = count_vectorizer.fit_transform(train_df[\"text\"])\n## note that we're NOT using .fit_transform() here. Using just .transform() makes sure\n# that the tokens in the train vectors are the only ones mapped to the test vectors - \n# i.e. that the train and test vectors use the same set of tokens.\ndev_vectors = count_vectorizer.transform(dev_df[\"text\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Our model\n\nAs we mentioned above, we think the words contained in each tweet are a good indicator of whether they're about a real disaster or not. The presence of particular word (or set of words) in a tweet might link directly to whether or not that tweet is real.\n\nWhat we're assuming here is a _linear_ connection. So let's build a linear model and see!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## Our vectors are really big, so we want to push our model's weights\n## toward 0 without completely discounting different words - ridge regression \n## is a good way to do this.\nclf = linear_model.RidgeClassifier()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's test our model and see how well it does on the training data. For this we'll use `cross-validation` - where we train on a portion of the known data, then validate it with the rest. If we do this several times (with different portions) we can get a good idea for how a particular model or method performs.\n\nThe metric for this competition is F1, so let's use that here.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nclf.fit(train_vectors, train_df[\"target\"])\npredictions = clf.predict(dev_vectors);\nprint(classification_report(dev_df['target'], predictions))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The above scores aren't terrible! It looks like our assumption will score roughly 0.8 on the leaderboard. There are lots of ways to potentially improve on this (TFIDF, LSA, LSTM / RNNs, the list is long!) - give any of them a shot!\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Feature engineering","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### use keywords for prediction","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"count_vectorizer = feature_extraction.text.CountVectorizer()\ntrain_replaced_na_keyword = train_df.copy()\ntrain_replaced_na_keyword['keyword'] = train_df['keyword'].fillna(' ')\ntrain_vectors = count_vectorizer.fit_transform(train_replaced_na_keyword[\"keyword\"])\n\ndev_replaced_na_keyword = dev_df.copy()\ndev_replaced_na_keyword['keyword'] = dev_df['keyword'].fillna(' ')\ndev_vectors = count_vectorizer.transform(dev_replaced_na_keyword[\"keyword\"])\ntrain_replaced_na_keyword","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_vectors[1].todense()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = linear_model.RidgeClassifier()\nclf.fit(train_vectors, train_df[\"target\"])\npredictions = clf.predict(dev_vectors);\nprint(classification_report(dev_df['target'], predictions))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### use tweet text in combination with keyword","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"count_vectorizer = feature_extraction.text.CountVectorizer()\ntrain_textandkeyword_df = train_df.copy()\ntrain_textandkeyword_df['keyword'] = train_df['keyword'].fillna(' ')\ntrain_textandkeyword_df['textandkeyword'] = train_textandkeyword_df['text'] + \" // \" + train_textandkeyword_df['keyword']\ntrain_vectors = count_vectorizer.fit_transform(train_textandkeyword_df[\"textandkeyword\"])\n\n\n\ndev_textandkeyword_df = dev_df.copy()\ndev_textandkeyword_df['keyword'] = dev_df['keyword'].fillna(' ')\ndev_textandkeyword_df['textandkeyword'] = dev_textandkeyword_df['text'] + \" // \" + dev_textandkeyword_df['keyword']\ndev_vectors = count_vectorizer.transform(dev_textandkeyword_df[\"textandkeyword\"])\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = linear_model.RidgeClassifier()\nclf.fit(train_vectors, train_df[\"target\"])\npredictions = clf.predict(dev_vectors);\nprint(classification_report(dev_df['target'], predictions))\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Adding keyword to text reduces the score","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### use only text but only over tweets that have keywords","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = linear_model.RidgeClassifier()\ntrain_vectors = count_vectorizer.fit_transform(train_textandkeyword_df[\"text\"])\ndev_vectors = count_vectorizer.transform(dev_textandkeyword_df[\"text\"])\nclf.fit(train_vectors, train_df[\"target\"])\npredictions = clf.predict(dev_vectors);\nprint(classification_report(dev_df['target'], predictions))\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  Vector representations","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Use TF-IDF to highlight important words in the text","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Select top 10 words from every tweet using TF-IDF and feed them to a classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(stop_words='english')\nclf = linear_model.RidgeClassifier()\ntrain_vectors = vectorizer.fit_transform(train_df[\"text\"])\ndev_vectors = vectorizer.transform(dev_df[\"text\"])\nclf.fit(train_vectors, train_df[\"target\"])\npredictions = clf.predict(dev_vectors);\nprint(classification_report(dev_df['target'], predictions))\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"not removing stop words is better:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nvectorizer = TfidfVectorizer()\nclf = linear_model.RidgeClassifier()\ntrain_vectors = vectorizer.fit_transform(train_df[\"text\"])\ndev_vectors = vectorizer.transform(dev_df[\"text\"])\nclf.fit(train_vectors, train_df[\"target\"])\npredictions = clf.predict(dev_vectors);\nprint(classification_report(dev_df['target'], predictions))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <a id=\"best_shallow\">Best model so far: ridge classifier using TF-IDF for text encoding</a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = linear_model.RidgeClassifier()\npipe = Pipeline([('vectorizer', TfidfVectorizer()), ('predictor', clf)])\npipe.fit(train_df[\"text\"], train_df[\"target\"])\npredictions = pipe.predict(dev_df[\"text\"]);\nprint(classification_report(dev_df['target'], predictions))\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id=\"shallow_learning\">Evaluate multiple classifier models</a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator\nclass ClfSwitcher(BaseEstimator):\n    def __init__(self, estimator = linear_model.RidgeClassifier()):\n        \"\"\"\n        A Custom BaseEstimator that can switch between classifiers.\n        :param estimator: sklearn object - The classifier\n        \"\"\" \n        self.estimator = estimator\n\n\n    def fit(self, X, y=None, **kwargs):\n        self.estimator.fit(X, y)\n        return self\n\n\n    def predict(self, X, y=None):\n        return self.estimator.predict(X)\n\n\n    def predict_proba(self, X):\n        return self.estimator.predict_proba(X)\n\n\n    def score(self, X, y):\n        return self.estimator.score(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n\n\n\nclf = linear_model.RidgeClassifier()\npipeline = Pipeline([('tfidf', TfidfVectorizer(stop_words='english')), ('clf', ClfSwitcher())])\n\n\n\nparameters = [\n    {\n        'clf__estimator': [SVC()],\n    },\n    {\n        'clf__estimator': [SGDClassifier()],\n    },\n    {\n        'clf__estimator': [MultinomialNB()],\n    },\n    {\n        'clf__estimator': [linear_model.RidgeClassifier()],\n    },\n    {\n        'clf__estimator': [MLPClassifier(random_state=1, max_iter=200, early_stopping=True)],\n    },\n    {\n        'clf__estimator': [RandomForestClassifier()]\n    }\n]\n\n\nos.write(1, b\"Starting grid search of models\\n\")\ngscv = GridSearchCV(pipeline, parameters, cv=3, n_jobs=12, return_train_score=True, verbose=3, scoring='f1')\ngscv.fit(train_df[\"text\"], train_df[\"target\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame(gscv.cv_results_)\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"compare best two classifier: MultinomialNB & MLPClassifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nclf = MultinomialNB()\npipe = Pipeline([('vectorizer', TfidfVectorizer()), ('predictor', clf)])\npipe.fit(train_df[\"text\"], train_df[\"target\"])\npredictions = pipe.predict(dev_df[\"text\"]);\nprint(classification_report(dev_df['target'], predictions))\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nclf = MLPClassifier(random_state=1, max_iter=200, early_stopping=True)\npipe = Pipeline([('vectorizer', TfidfVectorizer()), ('predictor', clf)])\npipe.fit(train_df[\"text\"], train_df[\"target\"])\npredictions = pipe.predict(dev_df[\"text\"]);\nprint(classification_report(dev_df['target'], predictions))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"hyperparameter search for MultinomialNB","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline = Pipeline([('tfidf', TfidfVectorizer()), ('clf', MultinomialNB())])\n\n\n\nparameters = [\n    {\n        'clf__alpha': [1, 0, 0.5],\n        'clf__fit_prior': [True, False]\n    },\n]\n\nos.write(1, b\"Starting grid search of alpha & fit_prior\\n\")\ngscv = GridSearchCV(pipeline, parameters, cv=3, n_jobs=12, return_train_score=True, verbose=3, scoring='f1')\ngscv.fit(train_df[\"text\"], train_df[\"target\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame(gscv.cv_results_)\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = MultinomialNB(fit_prior=False)\npipe = Pipeline([('vectorizer', TfidfVectorizer()), ('predictor', clf)])\npipe.fit(train_df[\"text\"], train_df[\"target\"])\npredictions = pipe.predict(dev_df[\"text\"]);\nprint(classification_report(dev_df['target'], predictions))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# More advanced Vector representation techniques","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## <a id=\"fast_text\">Fast-text</a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n!pip install gensim\n\n\nfrom typing import Callable, List, Optional, Tuple\n\nimport pandas as pd\nfrom sklearn.base import TransformerMixin, BaseEstimator\nimport gensim\nfrom nltk import ngrams\nfrom gensim.models.keyedvectors import FastTextKeyedVectors\nimport random\n#api = gensim.downloader\n#api.BASE_DIR = \".\"\n#fastText_model = api.load(\"fasttext-wiki-news-subwords-300\")  \nfrom gensim.models import FastText\nfastText_model = FastText.load('/kaggle/input/english-wikipedia-articles-20170820-models/enwiki_2017_08_20_fasttext.model')\n\ndef randvec(w, n=50, lower=-1.0, upper=1.0):\n    \"\"\"Returns a random vector of length `n`. `w` is ignored.\"\"\"\n    return np.array([random.uniform(lower, upper) for i in range(n)])\n\ndef get_oov_fasttext(w):\n    twograms = ngrams(w, 2)\n    vectors = []\n    for gram in twograms:\n        word_2gram = gram[0] + gram[1]\n        if word_2gram in fastText_model:\n            vectors.append(fastText_model[word_2gram])\n    if(len(vectors) > 0):\n        return np.sum(vectors, axis=0)\n    else:\n        return randvec(w, n=300)\ndef fasttext_vec(w):    \n    \"\"\"Return `w`'s fastext representation if available, else return \n    a random vector.\"\"\"\n    if(w in fastText_model):\n        return fastText_model[w]\n    else:\n        return get_oov_fasttext(w)\n    \nclass FastTextTransformer(BaseEstimator, TransformerMixin):\n    def __init__( self, combine_strategy=\"concatenate\", max_sentence_length=30):\n        assert (combine_strategy==\"concatenate\" or combine_strategy=='mean')\n        self.combine_strategy = combine_strategy\n        self.max_sentence_length = max_sentence_length\n        self.empty_word_token = \"EOF\"\n        \n    def transform(self, text_list):\n        texts = text_list.tolist()\n        result = [];\n        for text in texts:\n            vectors = [];\n            words = text.split()\n            if(self.combine_strategy == 'concatenate'):\n                max_index = self.max_sentence_length\n            else:\n                max_index = len(words) \n            for index in range(max_index):\n                if(len(words) > index):\n                    word = words[index]\n                else:\n                    word = self.empty_word_token\n                vectors.append(fasttext_vec(word))\n            if(self.combine_strategy == 'concatenate'):\n                result.append(np.concatenate(vectors))\n            elif(self.combine_strategy == 'mean'):\n                result.append(np.mean(vectors, axis=0))\n        return result;\n\n    def fit(self, X, y=None):\n        \"\"\"No fitting necessary so we just return ourselves\"\"\"\n        return self","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.write(1, b\"Starting fasttext experiments\\n\")\nclf = SGDClassifier()\npipe = Pipeline([('vectorizer', FastTextTransformer(combine_strategy='concatenate')), ('predictor', clf)])\npipe.fit(train_df[\"text\"], train_df[\"target\"])\npredictions = pipe.predict(dev_df[\"text\"]);\nprint(classification_report(dev_df['target'], predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Use Mean function for combining word vectors","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = SGDClassifier()\npipe = Pipeline([('vectorizer', FastTextTransformer(combine_strategy='mean')), ('predictor', clf)])\npipe.fit(train_df[\"text\"], train_df[\"target\"])\npredictions = pipe.predict(dev_df[\"text\"]);\nprint(classification_report(dev_df['target'], predictions))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Compare same model using TF-IDF rather than fast-text for encoding","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = SGDClassifier()\npipe = Pipeline([('vectorizer', TfidfVectorizer()), ('predictor', clf)])\npipe.fit(train_df[\"text\"], train_df[\"target\"])\npredictions = pipe.predict(dev_df[\"text\"]);\nprint(classification_report(dev_df['target'], predictions))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Encoding text with TF-IDF vectorizer proves better than fast-text, which makes sense because TF-IDF helps our model pay attention to important words, while fast-text doesn't. This means that a model that has attention mechanism may deliver promising results, later in this notebook I'll experiment with one.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# <a id=\"text_preprop\">pre-processing techniques</a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# normalise has several nltk data dependencies. Install these by running the following python commands:\n\nimport nltk\nfor dependency in (\"brown\", \"names\", \"wordnet\", \"averaged_perceptron_tagger\", \"universal_tagset\"):\n    nltk.download(dependency)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp = spacy.load(\"en\", disable=['parser', 'tagger', 'ner'])\nnltk.download('stopwords')\nstops = stopwords.words(\"english\")\n\n\n\ndef remove_accented_chars(text):\n    \"\"\"remove accented characters from text, e.g. caf√©\"\"\"\n    text = unidecode.unidecode(text)\n    return text\n\ndef expand_contractions(text):\n    \"\"\"expand shortened words, e.g. don't to do not\"\"\"\n    return contractions.fix(text);\n\ndef remove_URL(text):\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url.sub(r'',str(text))\n\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\n\ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\n\ndef remove_punctuation(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\n#FIXME: for lemmatizing, normalizing and removing stop words we tokenize the text and then join all the tokens using Python String.join, when doing this info like some punctuation marks are dropped which reduces model's accuracy\n#If you find a way to over come this problem please let me know in the comments!\ndef text_normalizer(comment, lemmatize, lowercase, remove_stopwords, remove_accents, normalize_contractions, normalize_URL, normalize_emoji, normalize_html, normalize_punctuation):\n    if lowercase:\n        comment = comment.lower()\n    if(remove_accents):\n        comment = remove_accented_chars(comment)\n    if(normalize_contractions):\n        comment = expand_contractions(comment)\n    if(normalize_URL):\n        comment = remove_URL(comment)\n    if(normalize_emoji):\n        comment = remove_emoji(comment)   \n    if(normalize_html):\n        comment = remove_html(comment)   \n    if(normalize_punctuation):\n        comment = remove_punctuation(comment)   \n    if(remove_stopwords):\n        comment = nlp(comment)\n        words = [];\n        for token in comment:\n            if not remove_stopwords or (remove_stopwords and token.text not in stops):\n                    words.append(token.text)\n        comment = \" \".join(words)\n    if(lemmatize):\n        comment = nlp(comment)\n        comment = \" \".join(word.lemma_.strip() for word in comment)\n    return comment\n\n\nclass PrePropTextTransformer(BaseEstimator, TransformerMixin):\n    def __init__( self, lemmatize=False, lowercase=False, remove_stopwords=False, remove_accents=False, normalize_contractions=False, normalize_URL=False, normalize_emoji=False, normalize_html=False, normalize_punctuation=False):\n        self.lemmatize=lemmatize\n        self.lowercase=lowercase\n        self.remove_stopwords=remove_stopwords\n        self.remove_accents=remove_accents\n        self.normalize_contractions=normalize_contractions\n        self.normalize_URL=normalize_URL\n        self.normalize_emoji=normalize_emoji\n        self.normalize_html=normalize_html\n        self.normalize_punctuation=normalize_punctuation\n        \n    def transform(self, text_list):\n        texts = text_list.tolist()\n        result = [];\n        for text in texts:\n            result.append(text_normalizer(text, self.lemmatize, self.lowercase, self.remove_stopwords, self.remove_accents, self.normalize_contractions, self.normalize_URL, self.normalize_emoji, self.normalize_html, self.normalize_punctuation))\n        return pd.Series(result)\n\n    def fit(self, X, y=None):\n        \"\"\"No fitting necessary so we just return ourselves\"\"\"\n        return self\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### baseline:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nclf = MultinomialNB(fit_prior=False)\npipe = Pipeline([('vectorizer', TfidfVectorizer()), ('predictor', clf)])\npipe.fit(train_df[\"text\"], train_df[\"target\"])\npredictions = pipe.predict(dev_df[\"text\"]);\nprint(classification_report(dev_df['target'], predictions))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### use preprocess_text to try improve baseline","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nos.write(1, b\"Starting experimetns w text preprop\\n\")\nclf = MultinomialNB()\npipe = Pipeline([('preprop', PrePropTextTransformer()), ('vectorizer', TfidfVectorizer()), ('predictor', clf)])\n\npipe.fit(train_df[\"text\"], train_df[\"target\"])\npredictions = pipe.predict(dev_df[\"text\"]);\nprint(classification_report(dev_df['target'], predictions))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nclf = MultinomialNB(fit_prior=False)\npipe = Pipeline([('preprop', PrePropTextTransformer(lemmatize=True,\n                                                    lowercase=True,\n                                                    remove_stopwords=True,\n                                                    remove_accents=True, \n                                                    normalize_contractions=True,\n                                                    normalize_URL=True,\n                                                    normalize_emoji=True,\n                                                    normalize_html=True,\n                                                    normalize_punctuation=True\n                                                   )), ('vectorizer', TfidfVectorizer()), ('predictor', clf)])\n\npipe.fit(train_df[\"text\"], train_df[\"target\"])\npredictions = pipe.predict(dev_df[\"text\"]);\nprint(classification_report(dev_df['target'], predictions))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" %%time\npipeline = Pipeline([('preprop', PrePropTextTransformer()),\n                     ('vectorizer', TfidfVectorizer()),\n                     ('predictor', MultinomialNB(fit_prior=False))])\n\n\n\nparameters = [\n        {'preprop__lemmatize': [True, False]},\n        {'preprop__lowercase': [True, False]},\n        {'preprop__remove_stopwords': [True, False]},\n        {'preprop__remove_accents': [True, False]},\n        {'preprop__normalize_contractions': [True, False]},\n        {'preprop__normalize_URL': [True, False]},\n        {'preprop__normalize_emoji': [True, False]},\n        {'preprop__normalize_html': [True, False]},\n        {'preprop__normalize_punctuation': [True, False]}\n    ]\n\ntry:\n    grid_search_pd = pd.read_csv(\"/kaggle/input/precomputedgridsearches2/text_preprop_gs_results.csv\")\nexcept:\n    # FIXME: n_jobs has to be 1 or it crashes\n    gscv = GridSearchCV(pipeline, parameters, cv=3, n_jobs=1, return_train_score=True, verbose=3, scoring='f1')\n    gscv.fit(train_df[\"text\"], train_df[\"target\"])\n    grid_search_pd = pd.DataFrame(gscv.cv_results_);\n    grid_search_pd.to_csv(\"/kaggle/input/precomputedgridsearches/text_preprop_gs_results.csv\")\ngrid_search_pd\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## best result so far using text preprocessing:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nclf = MultinomialNB(fit_prior=False)\npipe = Pipeline([('preprop', PrePropTextTransformer(lemmatize=False,\n                                                    lowercase=False,\n                                                    remove_stopwords=False,\n                                                    remove_accents=True, \n                                                    normalize_contractions=False,\n                                                    normalize_URL=True,\n                                                    normalize_emoji=True,\n                                                    normalize_html=True,\n                                                    normalize_punctuation=False\n                                                   )), ('vectorizer', TfidfVectorizer()), ('predictor', clf)])\n\npipe.fit(train_df[\"text\"], train_df[\"target\"])\npredictions = pipe.predict(dev_df[\"text\"]);\nprint(classification_report(dev_df['target'], predictions))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Does using a tweet tokenizer improve it?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom nltk.tokenize import TweetTokenizer\nclf = MultinomialNB(fit_prior=False)\npipe = Pipeline([('preprop', PrePropTextTransformer(lemmatize=False,\n                                                    lowercase=False,\n                                                    remove_stopwords=False,\n                                                    remove_accents=True, \n                                                    normalize_contractions=False,\n                                                    normalize_URL=True,\n                                                    normalize_emoji=True,\n                                                    normalize_html=True,\n                                                    normalize_punctuation=False\n                                                   )), ('vectorizer', TfidfVectorizer(tokenizer=TweetTokenizer().tokenize)), ('predictor', clf)])\n\npipe.fit(train_df[\"text\"], train_df[\"target\"])\npredictions = pipe.predict(dev_df[\"text\"]);\nprint(classification_report(dev_df['target'], predictions))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# More advanced word embeddings","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### BERT","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from typing import Callable, List, Optional, Tuple\n\nimport pandas as pd\nfrom sklearn.base import TransformerMixin, BaseEstimator\nfrom spacy.util import minibatch\nimport torch\n!pip install transformers\nfrom transformers import BertModel, BertTokenizer\n\n\n\ndef mean_across_all_tokens(hidden_states):\n    return torch.mean(hidden_states[-1], dim=1)\n\ndef sum_all_tokens(hidden_states):\n    return torch.sum(hidden_states[-1], dim=1)\n\ndef concat_all_tokens(hidden_states):\n    batch_size, max_tokens, emb_dim = hidden_states[-1].shape\n    return torch.reshape(hidden_states[-1], (batch_size, max_tokens * emb_dim))\n\ndef CLS_token_embedding(hidden_states):\n    return hidden_states[-1][:, 0, :]\n\nclass BertTransformer(BaseEstimator, TransformerMixin):\n    def __init__(\n            self,\n            max_length: int = 60,\n            tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\"),\n            model = BertModel.from_pretrained(\"bert-base-uncased\", output_hidden_states=True),\n            embedding_func = mean_across_all_tokens,\n            combine_sentence_tokens=True\n    ):\n        self.tokenizer = tokenizer;\n        self.combine_sentence_tokens = combine_sentence_tokens;\n        self.embedding_func = embedding_func;\n        self.model = model\n        self.model.eval()\n        self.max_length = max_length\n\n    def _tokenize(self, text_list: List[str]) -> Tuple[torch.tensor, torch.tensor]:\n        # Tokenize the text with the provided tokenizer\n        input_ids = self.tokenizer.batch_encode_plus(text_list,\n                                                    add_special_tokens=True,\n                                                    max_length=self.max_length,\n                                                    pad_to_max_length=True\n                                                    )[\"input_ids\"]\n\n        return torch.LongTensor(input_ids)\n         \n\n    def _tokenize_and_predict(self, text_list: List[str]) -> torch.tensor:\n        input_ids_tensor = self._tokenize(text_list)\n        out = self.model(input_ids=input_ids_tensor)\n        hidden_states = out[2]\n        if(self.combine_sentence_tokens):\n            return self.embedding_func(hidden_states)\n        else:\n            return hidden_states[-1]\n    \n    def transform(self, text_list: List[str], batch_size=32):\n        if isinstance(text_list, pd.Series):\n            text_list = text_list.tolist()\n        batches = minibatch(text_list, size=batch_size)\n        predictions = []\n        for batch in batches:\n            with torch.no_grad():\n                 batch_predictions = self._tokenize_and_predict(batch)\n            predictions.append(batch_predictions)\n        return torch.cat(predictions, dim=0)\n\n    def fit(self, X, y=None):\n        \"\"\"No fitting necessary so we just return ourselves\"\"\"\n        return self","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bertTransformer = BertTransformer(combine_sentence_tokens=False)\nbertTransformer.transform([\"pablo\", \"I love Pablo\"]).shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nos.write(1, b\"Starting experiments with BERT\\n\")\nclf = linear_model.RidgeClassifier()\npipe = Pipeline([('vectorizer', BertTransformer()), ('predictor', clf)])\npipe.fit(train_df[\"text\"], train_df[\"target\"])\npredictions = pipe.predict(dev_df[\"text\"]);\nprint(classification_report(dev_df['target'], predictions))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"text preprocessing, BERT encoding, RidgeClassifier classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nclf = linear_model.RidgeClassifier()\npipe = Pipeline([('preprop', PrePropTextTransformer(lemmatize=False,\n                                                    lowercase=False,\n                                                    remove_stopwords=False,\n                                                    remove_accents=True, \n                                                    normalize_contractions=False,\n                                                    normalize_URL=True,\n                                                    normalize_emoji=True,\n                                                    normalize_html=True,\n                                                    normalize_punctuation=False\n                                                   )), ('vectorizer', BertTransformer()), ('predictor', clf)])\n\npipe.fit(train_df[\"text\"], train_df[\"target\"])\npredictions = pipe.predict(dev_df[\"text\"]);\nprint(classification_report(dev_df['target'], predictions))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='bert-NN'></a>\nBERT embeddings feeded to feed forward NN","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nclf = MLPClassifier(random_state=1, max_iter=200, early_stopping=True)\npipe = Pipeline([('preprop', PrePropTextTransformer(lemmatize=False,\n                                                    lowercase=False,\n                                                    remove_stopwords=False,\n                                                    remove_accents=True, \n                                                    normalize_contractions=False,\n                                                    normalize_URL=True,\n                                                    normalize_emoji=True,\n                                                    normalize_html=True,\n                                                    normalize_punctuation=False\n                                                   )), ('vectorizer', BertTransformer()), ('predictor', clf)])\n\npipe.fit(train_df[\"text\"], train_df[\"target\"])\npredictions = pipe.predict(dev_df[\"text\"]);\nprint(classification_report(dev_df['target'], predictions))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## hyper param search MLP classifier","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"reduce num of ephocs and set cv=2 to reduce training time","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nclf = MLPClassifier(random_state=1, max_iter=100, early_stopping=True)\npipe = Pipeline([('preprop', PrePropTextTransformer(lemmatize=False,\n                                                    lowercase=False,\n                                                    remove_stopwords=False,\n                                                    remove_accents=True, \n                                                    normalize_contractions=False,\n                                                    normalize_URL=True,\n                                                    normalize_emoji=True,\n                                                    normalize_html=True,\n                                                    normalize_punctuation=False\n                                                   )), ('vectorizer', BertTransformer()), ('clf', clf)])\n\nparameters = [\n        {'clf__hidden_layer_sizes': [(100, 100), (100,), (100, 50), (50, 50)]}\n    ]\n\ntry:\n    grid_search_pd = pd.read_csv(\"/kaggle/input/precomputedgridsearches2/mlp_clf_gs_results.csv\")\nexcept:\n    # FIXME: n_jobs has to be 1 or it crashes\n    gscv = GridSearchCV(pipe, parameters, cv=2, n_jobs=1, return_train_score=True, verbose=3, scoring='f1')\n    gscv.fit(train_df[\"text\"], train_df[\"target\"])\n    grid_search_pd = pd.DataFrame(gscv.cv_results_);\n    grid_search_pd.to_csv(\"/kaggle/input/precomputedgridsearches2/mlp_clf_gs_results.csv\")\ngrid_search_pd\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nclf = MLPClassifier(random_state=1, max_iter=100, early_stopping=True)\npipe = Pipeline([('preprop', PrePropTextTransformer(lemmatize=False,\n                                                    lowercase=False,\n                                                    remove_stopwords=False,\n                                                    remove_accents=True, \n                                                    normalize_contractions=False,\n                                                    normalize_URL=True,\n                                                    normalize_emoji=True,\n                                                    normalize_html=True,\n                                                    normalize_punctuation=False\n                                                   )), ('vectorizer', BertTransformer()), ('clf', clf)])\n\nparameters = [\n        {'clf__hidden_layer_sizes': [(100, 50, 50), (100,100, 50)]}\n    ]\n\ntry:\n    grid_search_pd = pd.read_csv(\"/kaggle/input/precomputedgridsearches2/mlp_clf2_gs_results.csv\")\nexcept:\n    # FIXME: n_jobs has to be 1 or it crashes\n    gscv = GridSearchCV(pipe, parameters, cv=2, n_jobs=1, return_train_score=True, verbose=3, scoring='f1')\n    gscv.fit(train_df[\"text\"], train_df[\"target\"])\n    grid_search_pd = pd.DataFrame(gscv.cv_results_);\n    grid_search_pd.to_csv(\"/kaggle/input/precomputedgridsearches2/mlp_clf2_gs_results.csv\")\ngrid_search_pd\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Increasing FFN network layers doesn't make a significant improvement","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### BERT compare different ways of generating sentence embeddings for classification: concatenate VS avg VS [CLS] token embedding","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"bertTransformer = BertTransformer()\nbertTransformer.transform([\"granola bars\"]).shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nclf = MLPClassifier(random_state=1, max_iter=100, early_stopping=True)\npipe = Pipeline([('preprop', PrePropTextTransformer(lemmatize=False,\n                                                    lowercase=False,\n                                                    remove_stopwords=False,\n                                                    remove_accents=True, \n                                                    normalize_contractions=False,\n                                                    normalize_URL=True,\n                                                    normalize_emoji=True,\n                                                    normalize_html=True,\n                                                    normalize_punctuation=False\n                                                   )), ('vectorizer', BertTransformer()), ('predictor', clf)])\n\n\nparameters = [\n        {'vectorizer__embedding_func': [sum_all_tokens, mean_across_all_tokens, concat_all_tokens, CLS_token_embedding]}\n    ]\n\ntry:\n    grid_search_pd = pd.read_csv(\"/kaggle/input/precomputedgridsearches2/emb_funcs_gs_results.csv\")\nexcept:\n    # FIXME: n_jobs has to be 1 or it crashes\n    gscv = GridSearchCV(pipe, parameters, cv=2, n_jobs=1, return_train_score=True, verbose=3, scoring='f1')\n    gscv.fit(train_df[\"text\"], train_df[\"target\"])\n    grid_search_pd = pd.DataFrame(gscv.cv_results_);\n    grid_search_pd.to_csv(\"/kaggle/input/precomputedgridsearches2/emb_funcs_gs_results.csv\")\ngrid_search_pd\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XGBoost model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time \nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.pipeline import Pipeline\nfrom transformers import RobertaTokenizer, RobertaModel\nfrom xgboost import XGBClassifier\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\nmodel = RobertaModel.from_pretrained('roberta-base', output_hidden_states=True)\nclf = XGBClassifier(n_estimators=300)\npipe = Pipeline([('preprop', PrePropTextTransformer(lemmatize=False,\n                                                    lowercase=False,\n                                                    remove_stopwords=False,\n                                                    remove_accents=True, \n                                                    normalize_contractions=False,\n                                                    normalize_URL=True,\n                                                    normalize_emoji=True,\n                                                    normalize_html=True,\n                                                    normalize_punctuation=False\n                                                   )), ('vectorizer', BertTransformer(model=model, tokenizer=tokenizer)), ('predictor', clf)])\n\npipe.fit(train_df[\"text\"], train_df[\"target\"])\npredictions = pipe.predict(dev_df[\"text\"]);\nprint(classification_report(dev_df['target'], predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Sequential models","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"So far we've combined the word embeddings in some way and trained feed-fordward, shadow models, let's see what happens when our word embeddings are evaluated using a sequential model!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### <a id=\"lstms\">LSTM classifiers</a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Vanilla LSTM","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nos.write(1, b\"Starting experiments with sequential models\\n\")\nbertTransformer = BertTransformer(combine_sentence_tokens=False)\nX_train = bertTransformer.transform(train_df[\"text\"])\nX_dev = bertTransformer.transform(dev_df[\"text\"])\ny_train = train_df[\"target\"].tolist()\ny_dev = dev_df[\"target\"].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom torch_rnn_classifier_attn import TorchRNNClassifier\ntorch_rnn = TorchRNNClassifier(\n        vocab=[],\n        use_embedding=False,\n        bidirectional=False,\n        hidden_dim=50,\n        max_iter=50,\n        eta=0.05) \n_ = torch_rnn.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom sklearn.metrics import classification_report\npredictions = torch_rnn.predict(X_dev)\nprint(classification_report(y_dev, predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"####  BI-LSTM classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom torch_rnn_classifier import TorchRNNClassifier\n\ntorch_bi_lstm = TorchRNNClassifier(\n        vocab=[],\n        use_embedding=False,\n        bidirectional=True,\n        hidden_dim=50,\n        max_iter=50,\n        eta=0.05) \n_ = torch_bi_lstm.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\npredictions = torch_bi_lstm.predict(X_dev)\nprint(classification_report(y_dev, predictions))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### BI-LSTM with attention","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Manning 2015 Global Attention ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom torch_rnn_classifier_attn import TorchRNNClassifier\ntorch_bilstm_attn_manning2015 = TorchRNNClassifier(\n        vocab=[],\n        use_embedding=False,\n        attention=\"GlobalAttnManning2015\",\n        bidirectional=True,\n        hidden_dim=50,\n        max_iter=30,\n        eta=0.05) \n_ = torch_bilstm_attn_manning2015.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\npredictions = torch_bilstm_attn_manning2015.predict(X_dev)\nprint(classification_report(y_dev, predictions))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### <a id=\"shou_peng_attn\">Shou, Peng, et al. 2016 Attention</a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntorch_bilstm_attn_ShouPeng2016 = TorchRNNClassifier(\n        vocab=[],\n        use_embedding=False,\n        attention=\"AttnShouPeng2016\",\n        bidirectional=True,\n        hidden_dim=50,\n        max_iter=30,\n        eta=0.05) \n_ = torch_bilstm_attn_ShouPeng2016.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\npredictions = torch_bilstm_attn_ShouPeng2016.predict(X_dev)\nprint(classification_report(y_dev, predictions))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id=\"bert_e2e\">BERT (finetuned)</a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Finetune BERT models for classification task","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"BERT model with an added single linear layer on top for classification. As we feed input data, the entire pre-trained BERT model and the additional untrained classification layer is trained on our specific task.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Even though we already tested a similar architecture in this notebook: [BERT embeddings feeded TO A SK-learn feed forward NN](#bert-NN), the below model has an advantage that makes it more promising: both the transformer used to generating the word embeddings and the classification layer weights are adjusted(https://arxiv.org/abs/2004.14448) ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class BertClassifierPredictor(BaseEstimator, ClassifierMixin):\n    def __init__(self, model=ClassificationModel('roberta', 'roberta-base', args={\"overwrite_output_dir\": True},\n                                                 use_cuda=False)):\n        self.model = model\n        logging.basicConfig(level=logging.INFO)\n        transformers_logger = logging.getLogger(\"transformers\")\n        transformers_logger.setLevel(logging.WARNING)\n\n\n    def fit(self, X, y):\n        X = X.tolist()\n        y = y.tolist()\n        self.classes_ = unique_labels(y)\n        self.X_ = X\n        self.y_ = y\n        d = {'text': X, 'labels': y}\n        df = pd.DataFrame(data=d)\n        self.model.train_model(df)\n        return self\n\n    def predict(self, X):\n        return self.model.predict(X.tolist())[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nos.write(1, b\"Starting experiments w BERT finetuning\\n\")\nmodel = ClassificationModel('bert', '/kaggle/input/simpletransformer-outputs/checkpoint-bert-epoch-1', args={\"overwrite_output_dir\": True}, use_cuda=False)\n\nclf = BertClassifierPredictor(model=model)\npipe = Pipeline([('preprop', PrePropTextTransformer(lemmatize=False,\n                                                    lowercase=False,\n                                                    remove_stopwords=False,\n                                                    remove_accents=True, \n                                                    normalize_contractions=False,\n                                                    normalize_URL=True,\n                                                    normalize_emoji=True,\n                                                    normalize_html=True,\n                                                    normalize_punctuation=False\n                                                   )), ('predictor', clf)])\n\n#model already trained, uncomment below line to continue trasining\n#pipe.fit(train_df[\"text\"], train_df[\"target\"])\npredictions = pipe.predict(dev_df[\"text\"]);\nprint(classification_report(dev_df['target'], predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <a id=\"winner\">Roberta(Finetuned)</a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom simpletransformers.classification import ClassificationModel\nimport logging\n\nlogging.basicConfig(level=logging.INFO)\ntransformers_logger = logging.getLogger(\"transformers\")\ntransformers_logger.setLevel(logging.WARNING)\n# Create a TransformerModel\nmodel = ClassificationModel('roberta', '/kaggle/input/simpletransformer-outputs/checkpoint-roberta-epoch-1', args={\"overwrite_output_dir\": True}, use_cuda=False)\n\nclf = BertClassifierPredictor(model=model)\npipe_roberta_finetuned = Pipeline([('preprop', PrePropTextTransformer(lemmatize=False,\n                                                    lowercase=False,\n                                                    remove_stopwords=False,\n                                                    remove_accents=True, \n                                                    normalize_contractions=False,\n                                                    normalize_URL=True,\n                                                    normalize_emoji=True,\n                                                    normalize_html=True,\n                                                    normalize_punctuation=False\n                                                   )), ('predictor', clf)])\n\n#model already trained, uncomment below line to continue trasining\n#pipe_roberta_finetuned.fit(train_df[\"text\"], train_df[\"target\"])\npredictions = pipe_roberta_finetuned.predict(dev_df['text'])\nprint(classification_report(dev_df['target'], predictions))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Roberta word vectors(not finetuned) + feed forward NN","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.pipeline import Pipeline\nfrom transformers import RobertaTokenizer, RobertaModel\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\nmodel = RobertaModel.from_pretrained('roberta-base', output_hidden_states=True)\nclf = MLPClassifier(random_state=1, max_iter=200, early_stopping=True)\npipe = Pipeline([('preprop', PrePropTextTransformer(lemmatize=False,\n                                                    lowercase=False,\n                                                    remove_stopwords=False,\n                                                    remove_accents=True, \n                                                    normalize_contractions=False,\n                                                    normalize_URL=True,\n                                                    normalize_emoji=True,\n                                                    normalize_html=True,\n                                                    normalize_punctuation=False\n                                                   )), ('vectorizer', BertTransformer(model=model, tokenizer=tokenizer)), ('predictor', clf)])\n\npipe.fit(train_df[\"text\"], train_df[\"target\"])\npredictions = pipe.predict(dev_df[\"text\"]);\nprint(classification_report(dev_df['target'], predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Roberta word vectors used in best model so far (biLSTM with attention)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\nmodel = RobertaModel.from_pretrained('roberta-base', output_hidden_states=True)\nbertTransformer = BertTransformer(model=model, tokenizer=tokenizer, combine_sentence_tokens=False)\nX_train = bertTransformer.transform(train_df[\"text\"])\nX_dev = bertTransformer.transform(dev_df[\"text\"])\ny_train = train_df[\"target\"].tolist()\ny_dev = dev_df[\"target\"].tolist()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom torch_rnn_classifier_attn import TorchRNNClassifier\ntorch_bilstm_attention_roberta_exp = TorchRNNClassifier(\n        vocab=[],\n        use_embedding=False,\n        attention=\"AttnShouPeng2016\",\n        bidirectional=True,\n        hidden_dim=50,\n        max_iter=30,\n        eta=0.05) \n_ = torch_bilstm_attention_roberta_exp.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\npredictions = torch_bilstm_attention_roberta_exp.predict(X_dev)\nprint(classification_report(y_dev, predictions))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Spacy classifier(Architecture=Enseble)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"From the doc(https://spacy.io/api/textcategorizer#architectures):  \nStacked ensemble of a bag-of-words model and a neural network model. The neural network uses a CNN with mean pooling and attention","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nos.write(1, b\"Starting experiments with spacyclassifier\\n\")\nspacy_classifier = SpacyClassifier(n_iter=50)\n\n\npipe = Pipeline([('preprop', PrePropTextTransformer(lemmatize=False,\n                                                    lowercase=False,\n                                                    remove_stopwords=False,\n                                                    remove_accents=True, \n                                                    normalize_contractions=False,\n                                                    normalize_URL=True,\n                                                    normalize_emoji=True,\n                                                    normalize_html=True,\n                                                    normalize_punctuation=False\n                                                   )), ('predictor', spacy_classifier)])\n\npipe.fit(train_df[\"text\"], train_df[\"target\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\npredictions = pipe.predict(dev_df[\"text\"]);\nprint(classification_report(dev_df['target'], predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id=\"gpt2\">GPT-2</a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Frozen(not finetuned during training) distill-GPT2 language model used to generate word embeddings + feed forward classification layer on top","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom GPT2_classifier import GPT2Classifier\ncheckpoint_path = \"/kaggle/input/models3/GPT2_CLS_CHECKPOINT_ephoc20.pth.tar\"\ngpt2_classifier = GPT2Classifier(max_iter=20, finetune_GPT2=False, batch_size=32, checkpoint_path=checkpoint_path,\n                 base_dir=\".\", classes=[0,1])\n\npipe = Pipeline([('preprop', PrePropTextTransformer(lemmatize=False,\n                                                    lowercase=False,\n                                                    remove_stopwords=False,\n                                                    remove_accents=True, \n                                                    normalize_contractions=False,\n                                                    normalize_URL=True,\n                                                    normalize_emoji=True,\n                                                    normalize_html=True,\n                                                    normalize_punctuation=False\n                                                   )), ('predictor', gpt2_classifier)])\n\npipe.fit(train_df[\"text\"], train_df[\"target\"])\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\npredictions = pipe.predict(dev_df[\"text\"]);\nprint(classification_report(dev_df['target'], predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> # <a id=\"Conclusions\">Conclusions</a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"From comparing all the models tested on this notebook I observe that the best performing model for the task of tweet classification is: [Roberta classifier(Finetuned)](#winner)\n\nOther interesting findings:\n- using TF-IDF vectors + shallow learning(eg: ridge classifier) yields results that are hard to beat by most deep learning models [Link](#best_shallow)\n- Adding attention to an LSTM classifier increases its accuracy noticeably, particularly when using shoug peng 2016 attention [Link](#shou_peng_attn)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Follow up work","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"- Train word embeddings and LSTMs w attention together, so word vectors get fine tuned, this will definitely yield better results than when we don't finetune the word vectors and only train the LSTM classifier\n- Can we place a NN on top of the LSTM and have it behave as an attention layer? if no why not?\n- Oversample the minority class\n- Use other types of attentions when working with RNNs Manning 2015 suggests a better type called local attention\n- Explore bigger versions of GPT2\n- Explore other ways of generating sentence embeddings(eg: add a CLS token and use its hidden state as sentence representation) using GPT2\n- Finetune GPT2 model, my model only trains the classificaiton layer, tried to also finetune GPT2 and even the current code is supposed to be able to finetune it, but for some reason doesn't work","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Submit best model predictions to Kaggle","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Train the final model on all available labelled data, the below df contains the examples used for training and dev sets during the notebooka","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Increased training ephocs from 1 to 2 for submission","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"entire_train_df = pd.concat([train_df, dev_df])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# retrain best model on  all available data\nmodel = ClassificationModel('roberta', '/kaggle/input/simpletransformer-outputs/checkpoint-roberta_submission-epoch-2', args={\"overwrite_output_dir\": True, \"num_train_epochs\": 2}, use_cuda=False)\n\nclf = BertClassifierPredictor(model=model)\npipe_roberta_finetuned_submit = Pipeline([('preprop', PrePropTextTransformer(lemmatize=False,\n                                                    lowercase=False,\n                                                    remove_stopwords=False,\n                                                    remove_accents=True, \n                                                    normalize_contractions=False,\n                                                    normalize_URL=True,\n                                                    normalize_emoji=True,\n                                                    normalize_html=True,\n                                                    normalize_punctuation=False\n                                                   )), ('predictor', clf)])\n#model already trained, uncomment below line to continue trasining\n#pipe_roberta_finetuned_submit.fit(entire_train_df[\"text\"], entire_train_df[\"target\"])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_vectors = test_df['text']\nsample_submission = pd.read_csv(\"/kaggle/input/data-baby2/sample_submission.csv\")\nsample_submission[\"target\"] = pipe_roberta_finetuned_submit.predict(test_vectors)\nsample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}