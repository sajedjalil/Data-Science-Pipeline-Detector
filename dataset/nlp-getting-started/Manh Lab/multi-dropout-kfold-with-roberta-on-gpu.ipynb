{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Multi-Sample Dropout\nDropout is an efficient regularization instrument for avoiding overfitting of deep neural networks. It works very simply randomly discarding a portion of neurons during training; as a result, a generalization occurs because in this way neurons depend no more on each other.\nIn this post, I try to reproduce the results presented in this paper; which introduced a technique called Multi-Sample Dropout. As declared by the author, its scopes are:\n* accelerate training and improve generalization over the original dropout;\n* reduce computational cost because most of the computation time is consumed in the layers below (often convolutional or recurrent) and the weights in the layers at the top are shared;\n* achieve lower error rates and losses.\n![](https://miro.medium.com/max/700/1*mdBXp3-D7G7mTcKDZHui8g.png)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## For Keras","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Dropout parameters change 0.1 - 0.5. You can customer this for betters model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_model(cfg):\n    model_input = tf.keras.Input(shape=(cfg['net_size'], cfg['net_size'], 3), name='imgIn')\n\n    dummy = tf.keras.layers.Lambda(lambda x:x)(model_input)\n    \n    outputs = []    \n    for i in range(cfg['net_count']):\n        constructor = getattr(efn, f'EfficientNetB{i}')\n        \n        x = constructor(include_top=False, weights='imagenet', \n                        input_shape=(cfg['net_size'], cfg['net_size'], 3), \n                        pooling='avg')(dummy)\n        dense = []\n        FC = tf.keras.layers.Dense(32, activation='relu')\n        for p in np.linspace(0.1,0.5, 5):\n            x_ = tf.keras.layers.Dropout(p)(x)\n            x_ = FC(x_)\n            x_ = tf.keras.layers.Dense(1, activation='sigmoid')(x_)\n            dense.append(x_)\n        x = tf.keras.layers.Average()(dense)\n        outputs.append(x)\n        \n    model = tf.keras.Model(model_input, outputs, name='aNetwork')\n    model.summary()\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## For pytorch","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class multilabel_dropout():\n    # Multisample Dropout: https://arxiv.org/abs/1905.09788\n    def __init__(self, HIGH_DROPOUT, HIDDEN_SIZE):\n        self.high_dropout = torch.nn.Dropout(config.HIGH_DROPOUT)\n        self.classifier = torch.nn.Linear(config.HIDDEN_SIZE * 2, 2)\n    def forward(self, out):\n        return torch.mean(torch.stack([\n            self.classifier(self.high_dropout(p))\n            for p in np.linspace(0.1,0.5, 5)\n        ], dim=0), dim=0)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}