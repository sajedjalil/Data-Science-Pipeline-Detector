{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Real or Not? NLP with Disaster Tweets\n\nIn this kernel you can find Basic EDA of Disaster Tweet dataset. Simple lemmatization using WordNetLemmatizer from nltk. Creation of dataset using torchtext. And LSTM implementation using PyTorch. Let's have some fun!\n\nWhat's in this kernel?\n* [Basic EDA](#EDA)\n* [Data Cleaning](#Data-Cleaning)\n* [PyTorch dataset](#PyTorch-dataset)\n* [PyTorch Model](#PyTorch-Model)\n* [PyTorch train](#PyTorch-train)\n* [Predictions](#Predictions)\n\n### Acknowledgments\n\n1. [Basic EDA,Cleaning and GloVe](https://www.kaggle.com/shahules/basic-eda-cleaning-and-glove#Exploratory-Data-Analysis-of-tweets) - For such a great EDA and cleaning\n2. [Text-Classification-Pytorch](https://github.com/prakashpandey9/Text-Classification-Pytorch) - For LSTM model"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Imports"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n\nimport nltk\nfrom nltk.corpus import stopwords\n# nltk.download('stopwords')\nfrom nltk.util import ngrams\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\n# nltk.download('punkt')\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\n\nimport re\nimport string\nimport os\nfrom collections import defaultdict\nfrom collections import Counter\n\nplt.style.use('ggplot')\nstop = set(stopwords.words('english'))\n\nimport gensim\nfrom tqdm.notebook import tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')\ntest = pd.read_csv('../input/nlp-getting-started/test.csv')\ntrain = pd.read_csv('../input/nlp-getting-started/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1 - real disaster\n<br>\n0 - no disaster"},{"metadata":{},"cell_type":"markdown","source":"### Most common words"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_corpus(target):\n    corpus = []\n    \n    for x in train.loc[train['target'] == target, 'text'].str.split():\n        for i in x:\n            corpus.append(i)\n            \n    return corpus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus = create_corpus(0)\n\ndic = defaultdict(int)\n\nfor word in corpus:\n    if word in stop:\n        dic[word] += 1\n        \ntop = sorted(dic.items(), key=lambda x: x[1], reverse=True)[:10]\n\nx, y = zip(*top)\nplt.title('With no disaster')\nplt.bar(x, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The main words are articles. It makes sense."},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus = create_corpus(1)\n\ndic = defaultdict(int)\n\nfor word in corpus:\n    if word in stop:\n        dic[word] += 1\n        \ntop = sorted(dic.items(), key=lambda x: x[1], reverse=True)[:10]\n\nx, y = zip(*top)\n\nplt.title('With disaster')\nplt.bar(x, y, color='green')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now <i>in</i> is almost on the first plays. During disaster we try to explain where is it."},{"metadata":{},"cell_type":"markdown","source":"### Punctuation"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 5))\ncorpus = create_corpus(0)\n\ndic = defaultdict(int)\n\nspecial = string.punctuation\nfor i in corpus:\n    if i in special:\n        dic[i] += 1\n        \nx, y = zip(*dic.items())\nplt.bar(x, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 5))\ncorpus = create_corpus(0)\n\ndic = defaultdict(int)\n\nspecial = string.punctuation\nfor i in corpus:\n    if i in special:\n        dic[i] += 1\n        \nx, y = zip(*dic.items())\nplt.bar(x, y, color='green')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Punctuation is almost the same. Maybe we can delete it."},{"metadata":{"trusted":true},"cell_type":"code","source":"counter = Counter(corpus)\nmost_common = counter.most_common()\n\nx = list()\ny = list()\n\nfor word, count in most_common[:40]:\n    if word not in stop:\n        x.append(word)\n        y.append(count)\n        \nsns.barplot(x=y, y=x, orient='h')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Bigrams"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_top_tweet_bigrams(corpus, n=10):\n    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    \n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0)\n    \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n    \n    return words_freq[:n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 5))\ntop_tweet_bigrams = get_top_tweet_bigrams(train['text'])[:10]\n\nx, y = map(list, zip(*top_tweet_bigrams))\n\nsns.barplot(x=y, y=x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Cleaning"},{"metadata":{},"cell_type":"markdown","source":"Let's concatenate <i>train</i> and <i>test</i> datasets."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.concat([train, test])\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can remove URLs, HTML tags and emojis. It is hard to get information from them."},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_URL(text):\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    \n    return url.sub('', text)\n\nexample = 'New competition launched: https://www.kaggle.com/c/nlp-getting-started'\n\nremove_URL(example)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text'] = df['text'].apply(lambda x: remove_URL(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"example = \"\"\"<div>\n<h1>Real or Fake</h1>\n<p>Kaggle </p>\n<a href=\"https://www.kaggle.com/c/nlp-getting-started\">getting started</a>\n</div>\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_html(text):\n    html = re.compile(r'<.*?>')\n    \n    return html.sub('', text)\n\nprint(remove_html(example))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text'] = df['text'].apply(lambda x: remove_html(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    \n    return emoji_pattern.sub(r'', text)\n\n\nremove_emoji(\"Omg another Earthquake ðŸ˜”ðŸ˜”\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text'] = df['text'].apply(lambda x: remove_emoji(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_punct(text):\n    table = str.maketrans('', '', string.punctuation)\n    \n    return text.translate(table)\n\nexample = \"I am #king\"\nprint(remove_punct(example))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text'] = df['text'].apply(lambda x: remove_punct(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Good thing to do is [lemmatizing](https://en.wikipedia.org/wiki/Lemmatisation). We can do it using [nltk](http://www.nltk.org/book/) library.\n\n> Lemmatization in linguistics is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form."},{"metadata":{"trusted":true},"cell_type":"code","source":"keywords = train.keyword.unique()[1:]\nkeywords = list(map(lambda x: x.replace('%20', ' '), keywords))\n\nwnl = WordNetLemmatizer()\n\ndef lemmatize_sentence(sentence):\n    sentence_words = sentence.split(' ')\n    new_sentence_words = list()\n    \n    for sentence_word in sentence_words:\n        sentence_word = sentence_word.replace('#', '')\n        new_sentence_word = wnl.lemmatize(sentence_word.lower(), wordnet.VERB)\n        new_sentence_words.append(new_sentence_word)\n        \n    new_sentence = ' '.join(new_sentence_words)\n    new_sentence = new_sentence.strip()\n    \n    return new_sentence","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text'] = df['text'].apply(lambda x: lemmatize_sentence(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## PyTorch"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom torch.nn import functional as F\nfrom torch.autograd import Variable\n\nfrom torchtext import data\nfrom torchtext import datasets\nfrom torchtext.vocab import Vectors, GloVe","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## PyTorch dataset"},{"metadata":{},"cell_type":"markdown","source":"If we want to use torchtext we should save <i>train</i>, <i>test</i> and </i>validation</i> datasets into separated files."},{"metadata":{"trusted":true},"cell_type":"code","source":"def prepare_csv(df_train, df_test, seed=27, val_ratio=0.3):\n    idx = np.arange(df_train.shape[0])\n    \n    np.random.seed(seed)\n    np.random.shuffle(idx)\n    \n    val_size = int(len(idx) * val_ratio)\n    \n    if not os.path.exists('cache'):\n        os.makedirs('cache')\n    \n    df_train.iloc[idx[val_size:], :][['id', 'target', 'text']].to_csv(\n        'cache/dataset_train.csv', index=False\n    )\n    \n    df_train.iloc[idx[:val_size], :][['id', 'target', 'text']].to_csv(\n        'cache/dataset_val.csv', index=False\n    )\n    \n    df_test[['id', 'text']].to_csv('cache/dataset_test.csv',\n                   index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Method for wrapping TabularDataset into iterator. So, we can iterate threw dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_iterator(dataset, batch_size, train=True,\n                 shuffle=True, repeat=False):\n    \n    device = torch.device('cuda:0' if torch.cuda.is_available()\n                          else 'cpu')\n    \n    dataset_iter = data.Iterator(\n        dataset, batch_size=batch_size, device=device,\n        train=train, shuffle=shuffle, repeat=repeat,\n        sort=False\n    )\n    \n    return dataset_iter","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For embeddings we are using [GloVe](https://github.com/maciejkula/glove-python):\n\n> Glove produces dense vector embeddings of words, where words that occur together are close in the resulting vector space.\n> <br><br> While this produces embeddings which are similar to word2vec (which has a great python implementation in gensim), the method is different: GloVe produces embeddings by factorizing the logarithm of the corpus word co-occurrence matrix."},{"metadata":{"trusted":true},"cell_type":"code","source":"import logging\nfrom copy import deepcopy\n\nLOGGER = logging.getLogger('tweets_dataset')\n\ndef get_dataset(fix_length=100, lower=False, vectors=None):\n    \n    if vectors is not None:\n        lower=True\n        \n    LOGGER.debug('Preparing CSV files...')\n    prepare_csv(train, test)\n    \n    TEXT = data.Field(sequential=True, \n#                       tokenize='spacy', \n                      lower=True, \n                      include_lengths=True, \n                      batch_first=True, \n                      fix_length=25)\n    LABEL = data.Field(use_vocab=True,\n                       sequential=False,\n                       dtype=torch.float16)\n    ID = data.Field(use_vocab=False,\n                    sequential=False,\n                    dtype=torch.float16)\n    \n    \n    LOGGER.debug('Reading train csv files...')\n    \n    train_temp, val_temp = data.TabularDataset.splits(\n        path='cache/', format='csv', skip_header=True,\n        train='dataset_train.csv', validation='dataset_val.csv',\n        fields=[\n            ('id', ID),\n            ('target', LABEL),\n            ('text', TEXT)\n        ]\n    )\n    \n    LOGGER.debug('Reading test csv file...')\n    \n    test_temp = data.TabularDataset(\n        path='cache/dataset_test.csv', format='csv',\n        skip_header=True,\n        fields=[\n            ('id', ID),\n            ('text', TEXT)\n        ]\n    )\n    \n    LOGGER.debug('Building vocabulary...')\n    \n    TEXT.build_vocab(\n        train_temp, val_temp, test_temp,\n        max_size=20000,\n        min_freq=10,\n        vectors=GloVe(name='6B', dim=300)  # We use it for getting vocabulary of words\n    )\n    LABEL.build_vocab(\n        train_temp\n    )\n    ID.build_vocab(\n        train_temp, val_temp, test_temp\n    )\n    \n    word_embeddings = TEXT.vocab.vectors\n    vocab_size = len(TEXT.vocab)\n    \n    train_iter = get_iterator(train_temp, batch_size=32, \n                              train=True, shuffle=True,\n                              repeat=False)\n    val_iter = get_iterator(val_temp, batch_size=32, \n                            train=True, shuffle=True,\n                            repeat=False)\n    test_iter = get_iterator(test_temp, batch_size=32, \n                             train=False, shuffle=False,\n                             repeat=False)\n    \n    \n    LOGGER.debug('Done preparing the datasets')\n    \n    return TEXT, vocab_size, word_embeddings, train_iter, val_iter, test_iter","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TEXT, vocab_size, word_embeddings, train_iter, val_iter, test_iter = get_dataset()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## PyTorch Model"},{"metadata":{},"cell_type":"markdown","source":"For this task we will try to use LSTM network."},{"metadata":{"trusted":true},"cell_type":"code","source":"class LSTMClassifier(torch.nn.Module):\n    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, weights):\n        super(LSTMClassifier, self).__init__()\n        \n        self.output_size = output_size\n        self.n_layers = n_layers\n        self.hidden_dim = hidden_dim\n        \n        self.word_embeddings = torch.nn.Embedding(vocab_size,\n                                                  embedding_dim)\n        self.word_embeddings.weight = torch.nn.Parameter(weights,\n                                                         requires_grad=False)\n        \n        self.dropout_1 = torch.nn.Dropout(0.3)\n        self.lstm = torch.nn.LSTM(embedding_dim,\n                                  hidden_dim,\n                                  n_layers,\n                                  dropout=0.3,\n                                  batch_first=True)\n        \n        self.dropout_2 = torch.nn.Dropout(0.3)\n        self.label_layer = torch.nn.Linear(hidden_dim, output_size)\n        \n        self.act = torch.nn.Sigmoid()\n        \n    def forward(self, x, hidden):\n        batch_size = x.size(0)\n        \n        x = self.word_embeddings(x)\n        \n        x = self.dropout_1(x)\n        \n        lstm_out, hidden = self.lstm(x, hidden)\n                \n        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n        \n        out = self.dropout_2(lstm_out)\n        out = self.label_layer(out)    \n        \n        out = out.view(batch_size, -1, self.output_size)\n        out = out[:, -1, :]\n\n        out = self.act(out)\n        \n        return out, hidden\n    \n    def init_hidden(self, batch_size):\n        weight = next(self.parameters()).data\n        \n        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n        \n        return hidden","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## PyTorch train"},{"metadata":{},"cell_type":"markdown","source":"Method for network training. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(model, train_iter, val_iter, optim, loss, num_epochs, batch_size=32):\n    h = model.init_hidden(batch_size)\n    \n    clip = 5\n    val_loss_min = np.Inf\n    \n    total_train_epoch_loss = list()\n    total_train_epoch_acc = list()\n        \n    total_val_epoch_loss = list()\n    total_val_epoch_acc = list()\n        \n    \n    device = torch.device('cuda:0' if torch.cuda.is_available()\n                           else 'cpu')\n    \n    for epoch in range(num_epochs):\n\n        model.train()\n        \n        train_epoch_loss = list()\n        train_epoch_acc = list()\n        \n        val_epoch_loss = list()\n        val_epoch_acc = list()\n        \n        for idx, batch in enumerate(tqdm(train_iter)):\n            h = tuple([e.data for e in h])\n\n            text = batch.text[0]\n            target = batch.target\n            target = target - 1\n            target = target.type(torch.LongTensor)\n\n            text = text.to(device)\n            target = target.to(device)\n\n            optim.zero_grad()\n            \n            if text.size()[0] is not batch_size:\n                continue\n            \n            prediction, h = model(text, h)\n                \n            loss_train = loss(prediction.squeeze(), target)\n            loss_train.backward()\n\n            num_corrects = (torch.max(prediction, 1)[1].\n                                view(target.size()).data == target.data).float().sum()\n\n            acc = 100.0 * num_corrects / len(batch)\n\n            train_epoch_loss.append(loss_train.item())\n            train_epoch_acc.append(acc.item())\n            \n            torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n            \n            optim.step()\n    \n        print(f'Train Epoch: {epoch}, Training Loss: {np.mean(train_epoch_loss):.4f}, Training Accuracy: {np.mean(train_epoch_acc): .2f}%')\n\n        model.eval()\n\n        with torch.no_grad():\n            for idx, batch in enumerate(tqdm(val_iter)):\n                val_h = tuple([e.data for e in h])\n\n                text = batch.text[0]\n                target = batch.target\n                target = target - 1\n                target = target.type(torch.LongTensor)\n                \n                text = text.to(device)\n                target = target.to(device)\n                \n                if text.size()[0] is not batch_size:\n                    continue\n\n                prediction, h = model(text, h)\n                loss_val = loss(prediction.squeeze(), target)\n\n                num_corrects = (torch.max(prediction, 1)[1].\n                                view(target.size()).data == target.data).float().sum()\n\n                acc = 100.0 * num_corrects / len(batch)\n\n                val_epoch_loss.append(loss_val.item())\n                val_epoch_acc.append(acc.item())\n                \n            print(f'Vadlidation Epoch: {epoch}, Training Loss: {np.mean(val_epoch_loss):.4f}, Training Accuracy: {np.mean(val_epoch_acc): .2f}%')\n                \n            if np.mean(val_epoch_loss) <= val_loss_min:\n#                 torch.save(model.state_dict(), 'state_dict.pth')\n                print('Validation loss decreased ({:.6f} --> {:.6f})'.\n                      format(val_loss_min, np.mean(val_epoch_loss)))\n                \n                val_loss_min = np.mean(val_epoch_loss)\n                \n        total_train_epoch_loss.append(np.mean(train_epoch_loss))\n        total_train_epoch_acc.append(np.mean(train_epoch_acc))\n    \n        total_val_epoch_loss.append(np.mean(val_epoch_loss))\n        total_val_epoch_acc.append(np.mean(val_epoch_acc))\n    \n    return (total_train_epoch_loss, total_train_epoch_acc,\n            total_val_epoch_loss, total_val_epoch_acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = 1e-4\nbatch_size = 32\noutput_size = 2\nhidden_size = 128\nembedding_length = 300\n\nmodel = LSTMClassifier(vocab_size=vocab_size, \n                       output_size=output_size, \n                       embedding_dim=embedding_length,\n                       hidden_dim=hidden_size,\n                       n_layers=2,\n                       weights=word_embeddings\n)\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available()\n                      else 'cpu')\n    \nmodel.to(device)\noptim = torch.optim.Adam(model.parameters(), lr=lr)\nloss = torch.nn.CrossEntropyLoss()\n    \ntrain_loss, train_acc, val_loss, val_acc = train_model(model=model,\n                                                       train_iter=train_iter,\n                                                       val_iter=val_iter,\n                                                       optim=optim,\n                                                       loss=loss,\n                                                       num_epochs=20,\n                                                       batch_size=batch_size)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nplt.title('Loss')\nsns.lineplot(range(len(train_loss)), train_loss, label='train')\nsns.lineplot(range(len(val_loss)), val_loss, label='test')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nplt.title('Accuracy')\nsns.lineplot(range(len(train_acc)), train_acc, label='train')\nsns.lineplot(range(len(val_acc)), val_acc, label='test')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"results_target = list()\n\nwith torch.no_grad():\n    for batch in tqdm(test_iter):\n        for text, idx in zip(batch.text[0], batch.id):\n            text = text.unsqueeze(0)\n            res, _ = model(text, hidden=None)\n\n            target = np.round(res.cpu().numpy())\n            \n            results_target.append(target[0][1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission['target'] = list(map(int, results_target))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}