{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Real or Not? NLP with Twitter\n\n**In this notebook, we will explore natural language processing (NLP) by analyzing 10,000 hand classified tweets. These tweets have been labeled as those about disasters, and those not. As an example, consider two hypothetical tweets:**\n\n> That concert last night was FIRE!  \n> There's a huge forest fire in Malibu right now!!!\n\n**Both of these tweets invoke the notion of 'fire' but only the second is about a real natural disaster. Can we train a model to distinguish between the two? Let's find out. If you have any questions or suggestions, please comment below. Enjoy:**","execution_count":null},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"#python basics\nfrom matplotlib import pyplot as plt\nimport math, os, re, time, random, string\nimport numpy as np, pandas as pd, seaborn as sns\n\n#this is just cool\nfrom tqdm import tqdm\n\n#visualization\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')   #for optimum aesthetics \nimport seaborn as sns\n\n#natural language processing\nfrom collections import defaultdict\nimport wordcloud\n\n#ignore warnings because they are annoying\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#for neural nets\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Before we start, it is good practice to seed everything at the beginning of a project for more reproducible results. We can do this now:**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed):\n    os.environ['PYTHONHASHSEED']=str(seed)\n    tf.random.set_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    \nseed_everything(34)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# I. EDA","execution_count":null},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/nlp-getting-started/train.csv')\ntest = pd.read_csv('../input/nlp-getting-started/test.csv')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We see that we have three features in this dataset: keyword, location, and text**\n\n**We will save the ID column now for our final submission and combine data sets into a single DataFrame. We can drop location because it is missing too many values to be useful. But keyword contains information that we can add to our text data after filling missing values:**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#save ID\ntest_id = test['id']\n\n#drop from train and test\ncolumns = {'id', 'location'}\ntrain = train.drop(columns = columns)\ntest = test.drop(columns = columns)\n\n#fill missing with unknown\ntrain['keyword'] = train['keyword'].fillna('unknown')\ntest['keyword'] = test['keyword'].fillna('unknown')\n\n#add keyword to tweets\ntrain['text'] = train['text'] + ' ' + train['keyword']\ntest['text'] = test['text'] + ' ' + test['keyword']\n\n#drop fkeyword rom train and test\ncolumns = {'keyword'}\ntrain = train.drop(columns = columns)\ntest = test.drop(columns = columns)\n\n#combine so we work smarter, not harder\ntotal = train.append(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('There are', len(train), 'rows in the train set')\nprint('There are', len(test), 'rows in the test set')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#set figure size\nfig, ax = plt.subplots(figsize = (10, 5))\n\n#create graphs\ngraph1 = sns.countplot(x = 'target', data = total)\n\n#give title and plot\nplt.title('Distribution of Classification Groups')\nplt.show(graph1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**So we have more tweets in class 0 (not a disaster tweet) than class 1 (disaster tweet)**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Adding basic features\n\n**We can easily add some new features to help us explore the attributes of tweets that are about disasters. If we think these new features are useful, we can add them as input for our neural network along with the text data**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#create column for the number of words in tweet\ntotal['word count'] = total['text'].apply(lambda x: len(x.split()))\n\n#split so we can use updated train set with new feature\ntrain = total[:len(train)]\n\n#define subplot to see graphs side by side\nfig, ax = plt.subplots(figsize = (10, 5))\n\n#create graphs\nsns.kdeplot(train['word count'][train['target'] == 0], shade = True, label = 'Not disaster tweet')\nsns.kdeplot(train['word count'][train['target'] == 1], shade = True, label = 'Disaster tweet')\n\n#set title and plot\nplt.title('Distribution of Tweet Word Count')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create column for the number of characters in a tweet\ntotal['character count'] = total['text'].apply(lambda x: len(x))\n\n#split so we can use updated train set with new feature\ntrain = total[:len(train)]\n\n#define subplot to see graphs side by side\nfig, ax = plt.subplots(figsize = (10, 5))\n\n#create graphs\nsns.kdeplot(train['character count'][train['target'] == 0], shade = True, label = 'Not disaster tweet')\nsns.kdeplot(train['character count'][train['target'] == 1], shade = True, label = 'Disaster tweet')\n\n#set title and plot\nplt.title('Distribution of Tweet Character Count')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**This is insightful as it tells us that very few disaster tweets are less than 50 characters and that the majority of them are more than 125 characters long. What else can we explore?**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#define function to find average word length \ndef average_word_length(x):\n    x = x.split()\n    return np.mean([len(i) for i in x])\n\n#broadcast to text column\ntotal['average word length'] = total['text'].apply(average_word_length)\n\n#split so we can use updated train set with new feature\ntrain = total[:len(train)]\n\n#define subplot to see graphs side by side\nfig, ax = plt.subplots(figsize = (10, 5))\n\n#create graphs\nsns.kdeplot(train['average word length'][train['target'] == 0], shade = True, label = 'Not disaster tweet')\nsns.kdeplot(train['average word length'][train['target'] == 1], shade = True, label = 'Disaster tweet')\n\n#set title\nplt.title('Distribution of Tweet Average Word Length')\n\n#splot graphs\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now that we have an idea of what new features to construct and how they might be useful, let's add the rest of them and visualize them:**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#add unique word count\ntotal['unique word count'] = total['text'].apply(lambda x: len(set(x.split())))\n\n#add stopword count\ntotal['stopword count'] = total['text'].apply(lambda x: len([i for i in x.lower().split() if i in wordcloud.STOPWORDS]))\n\n#add url count\n#total['url count'] = total['text'].apply(lambda x: len([i for i in x.lower().split() if 'http' in i or 'https' in i]))\n\n#add mention count\n#total['mention count'] = total['text'].apply(lambda x: len([i for i in str(x) if i == '@']))\n\n#add hashtag count\n#total['hashtag count'] = total['text'].apply(lambda x: len([i for i in str(x) if i == '#']))\n\n#add stopword ratio\ntotal['stopword ratio'] = total['stopword count'] / total['word count']\n\n#add punctuation count\ntotal['punctuation count'] = total['text'].apply(lambda x: len([i for i in str(x) if i in string.punctuation]))\n\n#split so we can use updated train set\ntrain = total[:len(train)]\n\ndisaster = train['target'] == 1\n\n#produce graphs to visualize newly added features\nfig, axes = plt.subplots(4, figsize=(20, 30))\n\ngraph1 = sns.kdeplot(train.loc[~disaster]['unique word count'], shade = True, label = 'Not Disaster', ax=axes[0])\ngraph1 = sns.kdeplot(train.loc[disaster]['unique word count'], shade = True, label = 'Disaster', ax=axes[0])\ngraph1.set_title('Distribution of Unique Word Count')\n\ngraph2 = sns.kdeplot(train.loc[~disaster]['stopword count'], shade = True, label = 'Not Disaster', ax=axes[1])\ngraph2 = sns.kdeplot(train.loc[disaster]['stopword count'], shade = True, label = 'Disaster', ax=axes[1])\ngraph2.set_title('Distribution of Stopword Word Count')\n\ngraph3 = sns.kdeplot(train.loc[~disaster]['punctuation count'], shade = True, label = 'Not Disaster', ax=axes[2], bw = 1)\ngraph3 = sns.kdeplot(train.loc[disaster]['punctuation count'], shade = True, label = 'Disaster', ax=axes[2], bw = 1)\ngraph3.set_title('Distribution of Punctuation Count')\n\ngraph4 = sns.kdeplot(train.loc[~disaster]['stopword ratio'], shade = True, label = 'Not Disaster', ax=axes[3], bw = .05)\ngraph4 = sns.kdeplot(train.loc[disaster]['stopword ratio'], shade = True, label = 'Disaster', ax=axes[3], bw = .05)\ngraph4.set_title('Distribution of Stopword Ratio')\n\nfig.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# II. Cleaning\n\n**Now that we have explored our data, we need to prepare it for machine learning. In general, to process text we need to apply the following procedure:** \n\n> raw text corpus -> processing text -> tokenized text -> corpus vocabulary -> text representation \n\n**We can do most of the hard work with Keras's Tokenize object, which automatically converts all words to lowercase and filters out punctuation**  \n\n**This tokenizer has many arguements that allow you to do most of the cleaning with one line of code, so we do not need to much processing ourselves. I have included some examples of how one would manually clean text for reference:**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#remove punctuation\ndef remove_punctuation(x):\n    return x.translate(str.maketrans('', '', string.punctuation))\n\n#remove stopwords\ndef remove_stopwords(x):\n    return ' '.join([i for i in x.split() if i not in wordcloud.STOPWORDS])\n\n#remove words less than 4 \ndef remove_less_than(x):\n    return ' '.join([i for i in x.split() if len(i) > 3])\n\n#remove words with non-alphabet characters\ndef remove_non_alphabet(x):\n    return ' '.join([i for i in x.split() if i.isalpha()])\n\ndef strip_all_entities(x):\n    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",x).split())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check strip_all_entities functionality\nstrip_all_entities('@shawn Titanic #tragedy could have been prevented Economic \\\n                   Times: Telegraph.co.ukTitanic tragedy could have been preve... http://bet.ly/tuN2wx')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Trick: we can even spell check words, although this is very computationally expensive:**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#install autocorrect\n!pip install autocorrect\nfrom autocorrect import Speller \n\n#create function to spell check strings\ndef spell_check(x):\n    spell = Speller(lang='en')\n    return \" \".join([spell(i) for i in x.split()])\n\n#showcase spellcheck \nmispelled = 'Pleaze spelcheck this sentince'\nspell_check(mispelled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PROCESS_TWEETS = False\nif PROCESS_TWEETS:\n    #apply all of above functions\n    total['text'] = total['text'].apply(lambda x: x.lower())\n    total['text'] = total['text'].apply(lambda x: re.sub(r'https?://\\S+|www\\.\\S+', '', x, flags = re.MULTILINE))\n    total['text'] = total['text'].apply(remove_punctuation)\n    total['text'] = total['text'].apply(remove_stopwords)\n    total['text'] = total['text'].apply(remove_less_than)\n    total['text'] = total['text'].apply(remove_non_alphabet)\n    total['text'] = total['text'].apply(spell_check)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We should expand all the contractions in our vocabulary to ensure that we do not lose meaning when we tokenize everything. The below is taken from:**\n\n> https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"contractions = { \n\"ain't\": \"am not / are not / is not / has not / have not\",\n\"aren't\": \"are not / am not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he had / he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he shall / he will\",\n\"he'll've\": \"he shall have / he will have\",\n\"he's\": \"he has / he is\",\n\"how'd\": \"how did\",\n\"how'd'y\": \"how do you\",\n\"how'll\": \"how will\",\n\"how's\": \"how has / how is / how does\",\n\"I'd\": \"I had / I would\",\n\"I'd've\": \"I would have\",\n\"I'll\": \"I shall / I will\",\n\"I'll've\": \"I shall have / I will have\",\n\"I'm\": \"I am\",\n\"I've\": \"I have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it had / it would\",\n\"it'd've\": \"it would have\",\n\"it'll\": \"it shall / it will\",\n\"it'll've\": \"it shall have / it will have\",\n\"it's\": \"it has / it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"mustn't've\": \"must not have\",\n\"needn't\": \"need not\",\n\"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\",\n\"oughtn't've\": \"ought not have\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"shan't've\": \"shall not have\",\n\"she'd\": \"she had / she would\",\n\"she'd've\": \"she would have\",\n\"she'll\": \"she shall / she will\",\n\"she'll've\": \"she shall have / she will have\",\n\"she's\": \"she has / she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"so've\": \"so have\",\n\"so's\": \"so as / so is\",\n\"that'd\": \"that would / that had\",\n\"that'd've\": \"that would have\",\n\"that's\": \"that has / that is\",\n\"there'd\": \"there had / there would\",\n\"there'd've\": \"there would have\",\n\"there's\": \"there has / there is\",\n\"they'd\": \"they had / they would\",\n\"they'd've\": \"they would have\",\n\"they'll\": \"they shall / they will\",\n\"they'll've\": \"they shall have / they will have\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"to've\": \"to have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we had / we would\",\n\"we'd've\": \"we would have\",\n\"we'll\": \"we will\",\n\"we'll've\": \"we will have\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what shall / what will\",\n\"what'll've\": \"what shall have / what will have\",\n\"what're\": \"what are\",\n\"what's\": \"what has / what is\",\n\"what've\": \"what have\",\n\"when's\": \"when has / when is\",\n\"when've\": \"when have\",\n\"where'd\": \"where did\",\n\"where's\": \"where has / where is\",\n\"where've\": \"where have\",\n\"who'll\": \"who shall / who will\",\n\"who'll've\": \"who shall have / who will have\",\n\"who's\": \"who has / who is\",\n\"who've\": \"who have\",\n\"why's\": \"why has / why is\",\n\"why've\": \"why have\",\n\"will've\": \"will have\",\n\"won't\": \"will not\",\n\"won't've\": \"will not have\",\n\"would've\": \"would have\",\n\"wouldn't\": \"would not\",\n\"wouldn't've\": \"would not have\",\n\"y'all\": \"you all\",\n\"y'all'd\": \"you all would\",\n\"y'all'd've\": \"you all would have\",\n\"y'all're\": \"you all are\",\n\"y'all've\": \"you all have\",\n\"you'd\": \"you had / you would\",\n\"you'd've\": \"you would have\",\n\"you'll\": \"you shall / you will\",\n\"you'll've\": \"you shall have / you will have\",\n\"you're\": \"you are\",\n\"you've\": \"you have\"\n}\n\ncontractions_re = re.compile('(%s)' % '|'.join(contractions.keys()))\ndef expand_contractions(s, contractions = contractions):\n    def replace(match):\n        return contractions[match.group(0)]\n    return contractions_re.sub(replace, s)\n\nexpand_contractions(\"can't stop won't stop\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#apply to whole text column\ntotal['text'] = total['text'].apply(expand_contractions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**If you have the patience, it is also useful to correct as much text as you possibly can just to feed our model the best data possible. We can go through and separate hashtags/usernames, correct slang, and even spell check to ensure that our data is as clean as possible**\n\n**Hint: try to come up with queries that allow you to view these hashtags, mispellings, etc. to expedite this process (or just steal the code below)**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":" def clean(tweet):\n\n    #correct some acronyms while we are at it\n    tweet = re.sub(r\"tnwx\", \"Tennessee Weather\", tweet)\n    tweet = re.sub(r\"azwx\", \"Arizona Weather\", tweet)  \n    tweet = re.sub(r\"alwx\", \"Alabama Weather\", tweet)\n    tweet = re.sub(r\"wordpressdotcom\", \"wordpress\", tweet)      \n    tweet = re.sub(r\"gawx\", \"Georgia Weather\", tweet)  \n    tweet = re.sub(r\"scwx\", \"South Carolina Weather\", tweet)  \n    tweet = re.sub(r\"cawx\", \"California Weather\", tweet)\n    tweet = re.sub(r\"usNWSgov\", \"United States National Weather Service\", tweet) \n    tweet = re.sub(r\"MH370\", \"Malaysia Airlines Flight 370\", tweet)\n    tweet = re.sub(r\"okwx\", \"Oklahoma City Weather\", tweet)\n    tweet = re.sub(r\"arwx\", \"Arkansas Weather\", tweet)  \n    tweet = re.sub(r\"lmao\", \"laughing my ass off\", tweet)  \n    tweet = re.sub(r\"amirite\", \"am I right\", tweet)\n    \n    #and some typos/abbreviations\n    tweet = re.sub(r\"w/e\", \"whatever\", tweet)\n    tweet = re.sub(r\"w/\", \"with\", tweet)\n    tweet = re.sub(r\"USAgov\", \"USA government\", tweet)\n    tweet = re.sub(r\"recentlu\", \"recently\", tweet)\n    tweet = re.sub(r\"Ph0tos\", \"Photos\", tweet)\n    tweet = re.sub(r\"exp0sed\", \"exposed\", tweet)\n    tweet = re.sub(r\"<3\", \"love\", tweet)\n    tweet = re.sub(r\"amageddon\", \"armageddon\", tweet)\n    tweet = re.sub(r\"Trfc\", \"Traffic\", tweet)\n    tweet = re.sub(r\"WindStorm\", \"Wind Storm\", tweet)\n    tweet = re.sub(r\"16yr\", \"16 year\", tweet)\n    tweet = re.sub(r\"TRAUMATISED\", \"traumatized\", tweet)\n    \n    #hashtags and usernames\n    tweet = re.sub(r\"IranDeal\", \"Iran Deal\", tweet)\n    tweet = re.sub(r\"ArianaGrande\", \"Ariana Grande\", tweet)\n    tweet = re.sub(r\"camilacabello97\", \"camila cabello\", tweet) \n    tweet = re.sub(r\"RondaRousey\", \"Ronda Rousey\", tweet)     \n    tweet = re.sub(r\"MTVHottest\", \"MTV Hottest\", tweet)\n    tweet = re.sub(r\"TrapMusic\", \"Trap Music\", tweet)\n    tweet = re.sub(r\"ProphetMuhammad\", \"Prophet Muhammad\", tweet)\n    tweet = re.sub(r\"PantherAttack\", \"Panther Attack\", tweet)\n    tweet = re.sub(r\"StrategicPatience\", \"Strategic Patience\", tweet)\n    tweet = re.sub(r\"socialnews\", \"social news\", tweet)\n    tweet = re.sub(r\"IDPs:\", \"Internally Displaced People :\", tweet)\n    tweet = re.sub(r\"ArtistsUnited\", \"Artists United\", tweet)\n    tweet = re.sub(r\"ClaytonBryant\", \"Clayton Bryant\", tweet)\n    tweet = re.sub(r\"jimmyfallon\", \"jimmy fallon\", tweet)\n    tweet = re.sub(r\"justinbieber\", \"justin bieber\", tweet)  \n    tweet = re.sub(r\"Time2015\", \"Time 2015\", tweet)\n    tweet = re.sub(r\"djicemoon\", \"dj icemoon\", tweet)\n    tweet = re.sub(r\"LivingSafely\", \"Living Safely\", tweet)\n    tweet = re.sub(r\"FIFA16\", \"Fifa 2016\", tweet)\n    tweet = re.sub(r\"thisiswhywecanthavenicethings\", \"this is why we cannot have nice things\", tweet)\n    tweet = re.sub(r\"bbcnews\", \"bbc news\", tweet)\n    tweet = re.sub(r\"UndergroundRailraod\", \"Underground Railraod\", tweet)\n    tweet = re.sub(r\"c4news\", \"c4 news\", tweet)\n    tweet = re.sub(r\"MUDSLIDE\", \"mudslide\", tweet)\n    tweet = re.sub(r\"NoSurrender\", \"No Surrender\", tweet)\n    tweet = re.sub(r\"NotExplained\", \"Not Explained\", tweet)\n    tweet = re.sub(r\"greatbritishbakeoff\", \"great british bake off\", tweet)\n    tweet = re.sub(r\"LondonFire\", \"London Fire\", tweet)\n    tweet = re.sub(r\"KOTAWeather\", \"KOTA Weather\", tweet)\n    tweet = re.sub(r\"LuchaUnderground\", \"Lucha Underground\", tweet)\n    tweet = re.sub(r\"KOIN6News\", \"KOIN 6 News\", tweet)\n    tweet = re.sub(r\"LiveOnK2\", \"Live On K2\", tweet)\n    tweet = re.sub(r\"9NewsGoldCoast\", \"9 News Gold Coast\", tweet)\n    tweet = re.sub(r\"nikeplus\", \"nike plus\", tweet)\n    tweet = re.sub(r\"david_cameron\", \"David Cameron\", tweet)\n    tweet = re.sub(r\"peterjukes\", \"Peter Jukes\", tweet)\n    tweet = re.sub(r\"MikeParrActor\", \"Michael Parr\", tweet)\n    tweet = re.sub(r\"4PlayThursdays\", \"Foreplay Thursdays\", tweet)\n    tweet = re.sub(r\"TGF2015\", \"Tontitown Grape Festival\", tweet)\n    tweet = re.sub(r\"realmandyrain\", \"Mandy Rain\", tweet)\n    tweet = re.sub(r\"GraysonDolan\", \"Grayson Dolan\", tweet)\n    tweet = re.sub(r\"ApolloBrown\", \"Apollo Brown\", tweet)\n    tweet = re.sub(r\"saddlebrooke\", \"Saddlebrooke\", tweet)\n    tweet = re.sub(r\"TontitownGrape\", \"Tontitown Grape\", tweet)\n    tweet = re.sub(r\"AbbsWinston\", \"Abbs Winston\", tweet)\n    tweet = re.sub(r\"ShaunKing\", \"Shaun King\", tweet)\n    tweet = re.sub(r\"MeekMill\", \"Meek Mill\", tweet)\n    tweet = re.sub(r\"TornadoGiveaway\", \"Tornado Giveaway\", tweet)\n    tweet = re.sub(r\"GRupdates\", \"GR updates\", tweet)\n    tweet = re.sub(r\"SouthDowns\", \"South Downs\", tweet)\n    tweet = re.sub(r\"braininjury\", \"brain injury\", tweet)\n    tweet = re.sub(r\"auspol\", \"Australian politics\", tweet)\n    tweet = re.sub(r\"PlannedParenthood\", \"Planned Parenthood\", tweet)\n    tweet = re.sub(r\"calgaryweather\", \"Calgary Weather\", tweet)\n    tweet = re.sub(r\"weallheartonedirection\", \"we all heart one direction\", tweet)\n    tweet = re.sub(r\"edsheeran\", \"Ed Sheeran\", tweet)\n    tweet = re.sub(r\"TrueHeroes\", \"True Heroes\", tweet)\n    tweet = re.sub(r\"ComplexMag\", \"Complex Magazine\", tweet)\n    tweet = re.sub(r\"TheAdvocateMag\", \"The Advocate Magazine\", tweet)\n    tweet = re.sub(r\"CityofCalgary\", \"City of Calgary\", tweet)\n    tweet = re.sub(r\"EbolaOutbreak\", \"Ebola Outbreak\", tweet)\n    tweet = re.sub(r\"SummerFate\", \"Summer Fate\", tweet)\n    tweet = re.sub(r\"RAmag\", \"Royal Academy Magazine\", tweet)\n    tweet = re.sub(r\"offers2go\", \"offers to go\", tweet)\n    tweet = re.sub(r\"ModiMinistry\", \"Modi Ministry\", tweet)\n    tweet = re.sub(r\"TAXIWAYS\", \"taxi ways\", tweet)\n    tweet = re.sub(r\"Calum5SOS\", \"Calum Hood\", tweet)\n    tweet = re.sub(r\"JamesMelville\", \"James Melville\", tweet)\n    tweet = re.sub(r\"JamaicaObserver\", \"Jamaica Observer\", tweet)\n    tweet = re.sub(r\"TweetLikeItsSeptember11th2001\", \"Tweet like it is september 11th 2001\", tweet)\n    tweet = re.sub(r\"cbplawyers\", \"cbp lawyers\", tweet)\n    tweet = re.sub(r\"fewmoretweets\", \"few more tweets\", tweet)\n    tweet = re.sub(r\"BlackLivesMatter\", \"Black Lives Matter\", tweet)\n    tweet = re.sub(r\"NASAHurricane\", \"NASA Hurricane\", tweet)\n    tweet = re.sub(r\"onlinecommunities\", \"online communities\", tweet)\n    tweet = re.sub(r\"humanconsumption\", \"human consumption\", tweet)\n    tweet = re.sub(r\"Typhoon-Devastated\", \"Typhoon Devastated\", tweet)\n    tweet = re.sub(r\"Meat-Loving\", \"Meat Loving\", tweet)\n    tweet = re.sub(r\"facialabuse\", \"facial abuse\", tweet)\n    tweet = re.sub(r\"LakeCounty\", \"Lake County\", tweet)\n    tweet = re.sub(r\"BeingAuthor\", \"Being Author\", tweet)\n    tweet = re.sub(r\"withheavenly\", \"with heavenly\", tweet)\n    tweet = re.sub(r\"thankU\", \"thank you\", tweet)\n    tweet = re.sub(r\"iTunesMusic\", \"iTunes Music\", tweet)\n    tweet = re.sub(r\"OffensiveContent\", \"Offensive Content\", tweet)\n    tweet = re.sub(r\"WorstSummerJob\", \"Worst Summer Job\", tweet)\n    tweet = re.sub(r\"HarryBeCareful\", \"Harry Be Careful\", tweet)\n    tweet = re.sub(r\"NASASolarSystem\", \"NASA Solar System\", tweet)\n    tweet = re.sub(r\"animalrescue\", \"animal rescue\", tweet)\n    tweet = re.sub(r\"KurtSchlichter\", \"Kurt Schlichter\", tweet)\n    tweet = re.sub(r\"Throwingknifes\", \"Throwing knives\", tweet)\n    tweet = re.sub(r\"GodsLove\", \"God's Love\", tweet)\n    tweet = re.sub(r\"bookboost\", \"book boost\", tweet)\n    tweet = re.sub(r\"ibooklove\", \"I book love\", tweet)\n    tweet = re.sub(r\"NestleIndia\", \"Nestle India\", tweet)\n    tweet = re.sub(r\"realDonaldTrump\", \"Donald Trump\", tweet)\n    tweet = re.sub(r\"DavidVonderhaar\", \"David Vonderhaar\", tweet)\n    tweet = re.sub(r\"CecilTheLion\", \"Cecil The Lion\", tweet)\n    tweet = re.sub(r\"weathernetwork\", \"weather network\", tweet)\n    tweet = re.sub(r\"GOPDebate\", \"GOP Debate\", tweet)\n    tweet = re.sub(r\"RickPerry\", \"Rick Perry\", tweet)\n    tweet = re.sub(r\"frontpage\", \"front page\", tweet)\n    tweet = re.sub(r\"NewsInTweets\", \"News In Tweets\", tweet)\n    tweet = re.sub(r\"ViralSpell\", \"Viral Spell\", tweet)\n    tweet = re.sub(r\"til_now\", \"until now\", tweet)\n    tweet = re.sub(r\"volcanoinRussia\", \"volcano in Russia\", tweet)\n    tweet = re.sub(r\"ZippedNews\", \"Zipped News\", tweet)\n    tweet = re.sub(r\"MicheleBachman\", \"Michele Bachman\", tweet)\n    tweet = re.sub(r\"53inch\", \"53 inch\", tweet)\n    tweet = re.sub(r\"KerrickTrial\", \"Kerrick Trial\", tweet)\n    tweet = re.sub(r\"abstorm\", \"Alberta Storm\", tweet)\n    tweet = re.sub(r\"Beyhive\", \"Beyonce hive\", tweet)\n    tweet = re.sub(r\"RockyFire\", \"Rocky Fire\", tweet)\n    tweet = re.sub(r\"Listen/Buy\", \"Listen / Buy\", tweet)\n    tweet = re.sub(r\"ArtistsUnited\", \"Artists United\", tweet)\n    tweet = re.sub(r\"ENGvAUS\", \"England vs Australia\", tweet)\n    tweet = re.sub(r\"ScottWalker\", \"Scott Walker\", tweet)\n\n    return tweet\n\ntotal['text'] = total['text'].apply(clean)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## N-Gram Analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets = [tweet for tweet in total['text']]\n\n#split data to update changes\ntrain = total[:len(train)]\ntest = total[len(train):]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_ngrams(text, n_gram=1):\n    token = [token for token in text.lower().split(' ') if token != '' if token not in wordcloud.STOPWORDS]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [' '.join(ngram) for ngram in ngrams]\n\n###################################################\n#### Unigrams\n###################################################\n\ndisaster_unigrams = defaultdict(int)\nfor word in total[train['target'] == 1]['text']:\n    for word in generate_ngrams(word, n_gram = 1):\n        disaster_unigrams[word] += 1\ndisaster_unigrams = pd.DataFrame(sorted(disaster_unigrams.items(), key=lambda x: x[1])[::-1])\n\nnondisaster_unigrams = defaultdict(int)\nfor word in total[train['target'] == 0]['text']:\n    for word in generate_ngrams(word, n_gram = 1):\n        nondisaster_unigrams[word] += 1\nnondisaster_unigrams = pd.DataFrame(sorted(nondisaster_unigrams.items(), key=lambda x: x[1])[::-1])\n\n###################################################\n#### Bigrams\n###################################################\n\ndisaster_bigrams = defaultdict(int)\nfor word in total[train['target'] == 1]['text']:\n    for word in generate_ngrams(word, n_gram = 2):\n        disaster_bigrams[word] += 1\ndisaster_bigrams = pd.DataFrame(sorted(disaster_bigrams.items(), key=lambda x: x[1])[::-1])\n\nnondisaster_bigrams = defaultdict(int)\nfor word in total[train['target'] == 0]['text']:\n    for word in generate_ngrams(word, n_gram = 2):\n        nondisaster_bigrams[word] += 1\nnondisaster_bigrams = pd.DataFrame(sorted(nondisaster_bigrams.items(), key=lambda x: x[1])[::-1])\n\n###################################################\n#### Trigrams\n###################################################\n\ndisaster_trigrams = defaultdict(int)\nfor word in total[train['target'] == 1]['text']:\n    for word in generate_ngrams(word, n_gram = 3):\n        disaster_trigrams[word] += 1\ndisaster_trigrams = pd.DataFrame(sorted(disaster_trigrams.items(), key=lambda x: x[1])[::-1])\n\nnondisaster_trigrams = defaultdict(int)\nfor word in total[train['target'] == 0]['text']:\n    for word in generate_ngrams(word, n_gram = 3):\n        nondisaster_trigrams[word] += 1\nnondisaster_trigrams = pd.DataFrame(sorted(nondisaster_trigrams.items(), key=lambda x: x[1])[::-1])\n\n###################################################\n#### 4-grams\n###################################################\n\ndisaster_4grams = defaultdict(int)\nfor word in total[train['target'] == 1]['text']:\n    for word in generate_ngrams(word, n_gram = 4):\n        disaster_4grams[word] += 1\ndisaster_4grams = pd.DataFrame(sorted(disaster_4grams.items(), key=lambda x: x[1])[::-1])\n\nnondisaster_4grams = defaultdict(int)\nfor word in total[train['target'] == 0]['text']:\n    for word in generate_ngrams(word, n_gram = 4):\n        nondisaster_4grams[word] += 1\nnondisaster_4grams = pd.DataFrame(sorted(nondisaster_4grams.items(), key=lambda x: x[1])[::-1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"N = 10\n\nfig, ax = plt.subplots(4, 2, figsize = (15, 15))\nplt.tight_layout()\n\nsns.barplot(y = disaster_unigrams[0].values[:N], x = disaster_unigrams[1].values[:N], ax = ax[0, 0], color='#171820')\nsns.barplot(y = nondisaster_unigrams[0].values[:N], x = nondisaster_unigrams[1].values[:N], ax = ax[0, 1], color = '#fdc029')\n\nsns.barplot(y = disaster_bigrams[0].values[:N], x = disaster_bigrams[1].values[:N], ax = ax[1, 0], color='#171820')\nsns.barplot(y = nondisaster_bigrams[0].values[:N], x = nondisaster_bigrams[1].values[:N], ax = ax[1, 1], color = '#fdc029')\n\nsns.barplot(y = disaster_trigrams[0].values[:N], x = disaster_trigrams[1].values[:N], ax = ax[2, 0], color='#171820')\nsns.barplot(y = nondisaster_trigrams[0].values[:N], x = nondisaster_trigrams[1].values[:N], ax = ax[2, 1], color = '#fdc029')\n\nsns.barplot(y = disaster_4grams[0].values[:N], x = disaster_4grams[1].values[:N], ax = ax[3, 0], color='#171820')\nsns.barplot(y = nondisaster_4grams[0].values[:N], x = nondisaster_4grams[1].values[:N], ax = ax[3, 1], color = '#fdc029')\n\nfor i in range(0, 4):\n    for j in range(0, 2):\n        ax[i, j].spines['right'].set_visible(False)\n        ax[i, j].spines['left'].set_visible(False)\n        ax[i, j].spines['top'].set_visible(False)\n        ax[i, j].set_xlabel('')\n        ax[i, j].set_ylabel('')\n        ax[i, j].tick_params(axis='x', labelsize=12)\n        ax[i, j].tick_params(axis='y', labelsize=12)\n    \n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# III. Base Model\n\n**In this section we will explore simple recurrent neural networks and word embeddings to see how well these 'shallow' networks perform for binary sentiment extraction. After experimenting with LSTMs, we will see how the infamous [BERT](https://arxiv.org/abs/1810.04805) transformer**\n\n## 1. Word Embeddings\n\n**The following reference takes you step by step through the process of using word embeddings with Keras. I highly recommend you read it if just beginning with embeddings: https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html**\n\n**One way to feed our model text data is to treat each word in our vocabulary as a separate feature and one hot encode them (Bag of Words encoding). This works and performs decently, but there is a major drawback to this approach**\n\n### Mathematical intuition:\n\n**We are interested in constructing a vector space to represent our words. Suppose our vocabulary is: 'cat', 'dog', 'plant', 'leaf', 'man', woman'. Then we can form a vector space via:**\n\n$$ cat = (1, 0, 0, 0, 0, 0) $$\n$$ dog = (0, 1, 0, 0, 0, 0) $$\n$$ plant = (0, 0, 1, 0, 0, 0) $$\n$$ leaf = (0, 0, 0, 1, 0, 0) $$\n$$ man = (0, 0, 0, 0, 1, 0) $$\n$$ woman = (0, 0, 0, 0, 0, 1) $$\n\n**But these vectors form an orthogonal basis, so when we take the dot product of them, we get 0:**\n\n$$ cat \\cdot dog = (1, 0, 0, 0, 0, 0) \\cdot (0, 1, 0, 0, 0, 0) = 0$$\n\n**This means that all of these vectors are as far as possible from each other in the vector space, i.e., they are not similar. But words like cat and dog are similar in meaning, so it would be great if our word encoddings could somehow capture this similarity**\n\n**So, instead of following the Bag of Word approach, we will import the pre-trained words from GloVe and use them to construct our word embeddings. But what is GloVe?**  \n\n> \"GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\"\n\n**For example, we can take the vector for King and subtract the vector for Man and the resulting vector is remarkably close to the vector for Queen. Using these word encodings as opposed to the Bag of Word encodings will substantially improve our classification accuracy**\n\n**Now, some of the GloVe embeddings actually have tokens for punctuation, so for completeness, I will show you how to include punctuation in your embeddings if you think it will improve the accuracy of your model:**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\n\n#find way to tokenize punctuation\nto_exclude = '*+-/()%\\n[\\\\]{|}^_`~\\t'\nto_tokenize = '!\"#$&?:;<=>@'\ntokenizer = Tokenizer(filters = to_exclude)\ntext = 'Why are you so f%#@ing angry all the time?!'\ntext = re.sub(r'(['+to_tokenize+'])', r' \\1 ', text)\ntokenizer.fit_on_texts([text])\n\n#view new text\nprint(tokenizer.word_index)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**That did the trick! We won't be using punctuation embedding for this notebook, but it is worth playing around to see if doing so improves your model's performance**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#broadcast to entire text column\n#total['text'] = total['text'].apply(lambda x: re.sub(r'(['+to_tokenize+'])', r' \\1 ', x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We can also include the `oov_token` parameter in the Keras Tokenizer object so that when the tokenizer finds words that our out of vocabulary in the test set, instead of skipping them, it includes them as a token of our choosing, we just need to ensure our token does not resemble any other words in our vocabulary. However, since we will ultimately embed our words as GloVe vectors, this step is useless. But if you were using your own embeddings, it could prove useful, so I will include it below:**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences\nfrom keras import Input\n\n#define tokenizer options\ntokenizer = Tokenizer()     \n#tokenizer = Tokenizer(oov_token = '<OOV>')           #if you wanted to tokenized OOV words\n#tokenizer = Tokenizer(filters = to_exclude)          #if you wanted to include punctuation\ntokenizer.fit_on_texts(tweets)\nsequences = tokenizer.texts_to_sequences(tweets)\n\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\n\ndata = pad_sequences(sequences)\nlabels = train['target']\nprint('Shape of data tensor:', data.shape)\nprint('Shape of label tensor:', labels.shape)\n\nnlp_train = data[:len(train)]\nlabels = labels\nnlp_test = data[len(train):]\n\nMAX_SEQUENCE_LENGTH = data.shape[1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Note: there are several variants of the GloVe vectors. Some are trained on different sources (Wikipedia vs. Twitter) and they all come in different dimensions. We will try both Wikipedia and Twitter based embeddings, typically using the higher dimensional representations, either 200D or 300D**\n\n**The main difference between the Wikipedia and Twitter GloVe vectors is that there are only 400,000 words in the former's vocabulary whereas there are over a million words in the latter's. For more information:**\n\n> https://nlp.stanford.edu/projects/glove/\n\n**Note: After tinkering around, I concluded that the Wikipedia GloVe embeddings worked best for my model. That being said, I am sure one could leverage the Twitter corpus to create a model that outperforms mine**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#get GloVe vector embeddings\nembeddings_index = {}\nwith open('../input/glove-global-vectors-for-word-representation/glove.6B.200d.txt','r') as f:\n    for line in tqdm(f):\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors in the GloVe library' % len(embeddings_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EMBEDDING_DIM = 200   #defined by size of GloVe word vector dimensions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#initialize embedding matrix with zeros\nembedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n\n#add glove word encodings to our library\nfor word, i in tqdm(word_index.items()):\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        \n        #words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector\n        \nprint(\"Our embedded matrix is of dimension\", embedding_matrix.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import neural network basic\nfrom keras.layers import Embedding, LSTM, Dense, SpatialDropout1D, Bidirectional, Dropout, Concatenate, LeakyReLU, GRU\nfrom keras import Input, Model, regularizers\nfrom keras.optimizers import Adam\nfrom keras.models import Sequential\nfrom keras.callbacks import EarlyStopping\n\nembedding = Embedding(len(word_index) + 1, EMBEDDING_DIM, weights = [embedding_matrix],\n                     input_length = MAX_SEQUENCE_LENGTH, trainable = False)  \n\n#we do not want embedding layer to train since it has been pretrained","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now, we could just feed our model our word embeddings, but we could also add the features we added during our initial exploration to improve performance**\n\n**This 'meta input' contains information about the tweets that is no longer available to our model because of how we processed and cleaned the text, such as number of the URLs in a tweet.**  \n\n**To include these features in our model, we pass our NLP data through our LSTM layer and then add the scaled meta input to it before the final layer:**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n\ndef scale(df, scaler):\n    return scaler.fit_transform(df.iloc[:, 2:])\n\n#and scal\nmeta_train = scale(train, StandardScaler())\nmeta_test = scale(test, StandardScaler())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Simple LSTM Model\n\n**We will now design a Long Short-Term Memory (LSTM) recurrent neural network to classify our tweets to see how it performs on the leaderboard**  \n\n**For those who want to learn more about Reccurent Neural Networks (RNN), the following video from MIT is an excellent resource:**  \n\n> https://www.youtube.com/watch?v=SEnXr6v2ifU\n\n**We will add various levels of dropouts and a regularizer to our LSTM layer to control overfitting (as LSTM models are prone to do). For more information, see the below articles:**\n\n> https://machinelearningmastery.com/how-to-reduce-overfitting-with-dropout-regularization-in-keras/\n> https://machinelearningmastery.com/use-dropout-lstm-networks-time-series-forecasting/  \n> https://machinelearningmastery.com/use-weight-regularization-lstm-networks-time-series-forecasting/","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#function to create lstm model\ndef create_lstm(spatial_dropout, dropout, recurrent_dropout, learning_rate, bidirectional = False):\n    #define activation\n    activation = LeakyReLU(alpha = 0.01)\n    \n    #define inputs\n    nlp_input = Input(shape = (MAX_SEQUENCE_LENGTH,), name = 'nlp_input')\n    meta_input_train = Input(shape = (7, ), name = 'meta_train')\n    emb = embedding(nlp_input)\n    emb = SpatialDropout1D(dropout)(emb)\n\n    #add LSTM layer\n    if bidirectional:\n        nlp_out = (Bidirectional(LSTM(100, dropout = dropout, recurrent_dropout = recurrent_dropout,\n                                 kernel_initializer = 'orthogonal')))(emb)\n    else:\n        nlp_out = (LSTM(100, dropout = dropout, recurrent_dropout = recurrent_dropout,\n                                 kernel_initializer = 'orthogonal'))(emb)        \n     \n    #add meta data    \n    x = Concatenate()([nlp_out, meta_input_train])\n    \n    #add output layer\n    x = Dropout(dropout)(x)\n    preds = Dense(1, activation='sigmoid', kernel_regularizer = regularizers.l2(1e-4))(x)\n    \n    #compile model\n    model = Model(inputs=[nlp_input , meta_input_train], outputs = preds)\n    optimizer = Adam(learning_rate = learning_rate)\n    model.compile(loss = 'binary_crossentropy', optimizer = optimizer, metrics = ['accuracy'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#define conveient training function to visualize learning curves\ndef plot_learning_curves(history): \n    fig, ax = plt.subplots(1, 2, figsize = (20, 10))\n\n    ax[0].plot(history.history['accuracy'])\n    ax[0].plot(history.history['val_accuracy'])\n\n    ax[1].plot(history.history['loss'])\n    ax[1].plot(history.history['val_loss'])\n\n    ax[0].legend(['train', 'validation'], loc = 'upper left')\n    ax[1].legend(['train', 'validation'], loc = 'upper left')\n\n    fig.suptitle(\"Model Accuracy\", fontsize=14)\n\n    ax[0].set_ylabel('Accuracy')\n    ax[0].set_xlabel('Epoch')\n    ax[1].set_ylabel('Loss')\n    ax[1].set_xlabel('Epoch')\n\n    return plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create our first model\nlstm = create_lstm(spatial_dropout = .2, dropout = .2, recurrent_dropout = .2,\n                     learning_rate = 3e-4, bidirectional = True)\nlstm.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fit model\nhistory1 = lstm.fit([nlp_train, meta_train], labels, validation_split = .2,\n                       epochs = 5, batch_size = 21, verbose = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#view model 1 learning curves\nplot_learning_curves(history1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. A Note on  Overfitting\n\n**Dropout layers can help reduce overfitting, but there is no obvious way to add them and where to add them, so we will just have to experiment a bit**\n\n**For the curious reader, the article below is very informative:**\n\n> https://arxiv.org/pdf/1512.05287.pdf\n\n**Hint: you can also implement a callback that automatically halts model training when your performance decreases. This can also serve as a layman's grid search for epochs, seeing as you will be able to see at which epoch your model is optimized. I have already determined the optimal epoch range by trial and error, but I would recommend including a call back during your first pass of model training. Below is an easy way to include it:**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#define early stopping callback\ncallback = EarlyStopping(monitor = 'val_loss', patience = 4)\n\n#include it in your models training\n#history = model.fit(train, labels, validation_split = .2, epochs = 100, callbacks = [callback])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**You can monitor by different metrics, the most important being val_accuracy and val_loss. To prevent overfitting, I recommend selecting val_loss as your monitoring parameter. The patience arguement is just the number of epochs before stopping once the monitoring parameter stops improving**\n\n**Note: if you use a smaller batch size (or a larger learning rate), the val_loss will be noisier (zig-zag more) so you need to set a larger patience arguement to ensure your model does not prematurely end it's training**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# IV. More Complex Model(s)\n\n**In this section we will see if adding additional layers improves performance. We also changed the model's optimizer to LeakyRelu which allows the model to retain some information about the negative values that propogate through it. It is more computationally expensive and its performance is not consistent, so in practice it is not used much, but let's experiment with it here:**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 1. Adding Hidden Layers\n\n**We will now add a fully connected hidden layer after our LSTM layer. We will use ReLu activation for this layer, so it is best to use orthogonal initialization. See below links for more on LSTMs and optimal initializers:**\n\n> https://arxiv.org/pdf/1702.00071.pdf\n> https://smerity.com/articles/2016/orthogonal_init.html","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#function to create lstm model\ndef create_lstm_2(spatial_dropout, dropout, recurrent_dropout, learning_rate, bidirectional = False):\n    #define activation\n    activation = LeakyReLU(alpha = 0.01)\n    \n    #define inputs\n    nlp_input = Input(shape = (MAX_SEQUENCE_LENGTH,), name = 'nlp_input')\n    meta_input_train = Input(shape = (7, ), name = 'meta_train')\n    emb = embedding(nlp_input)\n    emb = SpatialDropout1D(dropout)(emb)\n\n    #add LSTM layer\n    if bidirectional:\n        nlp_out = (Bidirectional(LSTM(100, dropout = dropout, recurrent_dropout = recurrent_dropout,\n                                kernel_initializer = 'orthogonal')))(emb)\n    else:\n        nlp_out = (LSTM(100, dropout = dropout, recurrent_dropout = recurrent_dropout,\n                      kernel_initializer = 'orthogonal'))(emb)\n     \n    #add meta data    \n    x = Concatenate()([nlp_out, meta_input_train])\n    \n    #add second hidden layer\n    x = Dropout(dropout)(x)\n    x = (Dense(100, activation = activation, kernel_regularizer = regularizers.l2(1e-4),\n              kernel_initializer = 'he_normal'))(x)\n    \n    #add output layer\n    x = Dropout(dropout)(x)\n    preds = Dense(1, activation='sigmoid', kernel_regularizer = regularizers.l2(1e-4))(x)\n    \n    #compile model\n    model = Model(inputs=[nlp_input , meta_input_train], outputs = preds)\n    optimizer = Adam(learning_rate = learning_rate)\n    model.compile(loss = 'binary_crossentropy', optimizer = optimizer, metrics = ['accuracy'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#define new model\nlstm_2 = create_lstm_2(spatial_dropout = .4, dropout = .4, recurrent_dropout = .4,\n                       learning_rate = 3e-4, bidirectional = True)\n\nlstm_2.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fit model\nhistory2 = lstm_2.fit([nlp_train, meta_train], labels, validation_split = .2,\n                       epochs = 30, batch_size = 21, verbose = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_learning_curves(history2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create submission for complex lstm model\nsubmission_lstm = pd.DataFrame()\nsubmission_lstm['id'] = test_id\nsubmission_lstm['prob'] = lstm_2.predict([nlp_test, meta_test])\nsubmission_lstm['target'] = submission_lstm['prob'].apply(lambda x: 0 if x < .5 else 1)\nsubmission_lstm.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Dual LSTM Layers\n\n**Now we will take the output of one LSTM layer and feed it directly into another LSTM layer. This is easy to do, the only change being we need to set `return_sequences` = True on the first layer:**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#function to create dual lstm model\ndef create_dual_lstm(spatial_dropout, dropout, recurrent_dropout, learning_rate, bidirectional = False):\n    #define activation\n    activation = LeakyReLU(alpha = 0.01)\n    \n    #define inputs\n    nlp_input = Input(shape = (MAX_SEQUENCE_LENGTH,), name = 'nlp_input')\n    meta_input_train = Input(shape = (7, ), name = 'meta_train')\n    emb = embedding(nlp_input)\n    emb = SpatialDropout1D(dropout)(emb)\n\n    #add dual LSTM layers\n    if bidirectional:\n        nlp_out = (Bidirectional(LSTM(100, dropout = dropout, recurrent_dropout = recurrent_dropout,\n                                 kernel_initializer = 'orthogonal', return_sequences = True)))(emb)\n        nlp_out = SpatialDropout1D(dropout)(nlp_out)\n        nlp_out = (Bidirectional(LSTM(100, dropout = dropout, recurrent_dropout = recurrent_dropout,\n                                 kernel_initializer = 'orthogonal')))(emb)\n    else:\n        nlp_out = (LSTM(100, dropout = dropout, recurrent_dropout = recurrent_dropout,\n                                 kernel_initializer = 'orthogonal', return_sequences = True))(emb)\n        nlp_out = SpatialDropout1D(dropout)(nlp_out)\n        nlp_out = (LSTM(100, dropout = dropout, recurrent_dropout = recurrent_dropout,\n                                 kernel_initializer = 'orthogonal'))(emb)\n     \n     \n    #add meta data    \n    x = Concatenate()([nlp_out, meta_input_train])\n    \n    #add second hidden layer\n    #x = Dropout(dropout)(x)\n    #x = (Dense(100, activation = activation, kernel_regularizer = regularizers.l2(1e-4),\n              #kernel_initializer = 'he_normal'))(x)\n    \n    #add output layer\n    x = Dropout(dropout)(x)\n    preds = Dense(1, activation='sigmoid', kernel_regularizer = regularizers.l2(1e-4))(x)\n    \n    #compile model\n    model = Model(inputs=[nlp_input , meta_input_train], outputs = preds)\n    optimizer = Adam(learning_rate = learning_rate)\n    model.compile(loss = 'binary_crossentropy', optimizer = optimizer, metrics = ['accuracy'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#define new model\ndual_lstm = create_dual_lstm(spatial_dropout = .4, dropout = .4, recurrent_dropout = .4,\n                       learning_rate = 3e-4, bidirectional = True)\n\ndual_lstm.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history3 = dual_lstm.fit([nlp_train, meta_train], labels, validation_split = .2,\n         epochs = 25, batch_size = 21, verbose = 1) #callbacks = [callback]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create submission for complex lstm model\nsubmission_lstm2 = pd.DataFrame()\nsubmission_lstm2['id'] = test_id\nsubmission_lstm2['prob'] = dual_lstm.predict([nlp_test, meta_test])\nsubmission_lstm2['target'] = submission_lstm2['prob'].apply(lambda x: 0 if x < .5 else 1)\nsubmission_lstm2.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_learning_curves(history3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. BERT\n\n**And now we will test the infamous BERT to see how it performs compared to RNNs. We don't actually need to clean our tweets at all, we can just feed them to BERT (after encoding).**\n\n**HuggingFace Transformers makes it unbelievable easy to use transformers. In fact, you don't even need to specify the transformer or tokenizer: its architecture can be guessed from the name or path of the pretrained model you specify in the `from_pretrained` method. To read more about AutoModels/Tokenizers, see [this](https://huggingface.co/transformers/model_doc/auto.html)**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#choose batch size\nBATCH_SIZE = 32\n\n#how many epochs?\nEPOCHS = 2\n\n#use meta data?\nUSE_META = True\n\n#add dense layer?\nADD_DENSE = False\nDENSE_DIM = 64\n\n#add dropout?\nADD_DROPOUT = False\nDROPOUT = .2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install --quiet transformers\n#import model and Tokenizer\nfrom transformers import TFAutoModel, AutoTokenizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#BERT\nTOKENIZER = AutoTokenizer.from_pretrained(\"bert-large-uncased\")\nenc = TOKENIZER.encode(\"Encode me!\")\ndec = TOKENIZER.decode(enc)\nprint(\"Encode: \" + str(enc))\nprint(\"Decode: \" + str(dec))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def bert_encode(data,maximum_len) :\n    input_ids = []\n    attention_masks = []\n  \n\n    for i in range(len(data.text)):\n        encoded = TOKENIZER.encode_plus(data.text[i],\n                                        add_special_tokens=True,\n                                        max_length=maximum_len,\n                                        pad_to_max_length=True,\n                                        return_attention_mask=True)\n      \n        input_ids.append(encoded['input_ids'])\n        attention_masks.append(encoded['attention_mask'])\n        \n    return np.array(input_ids),np.array(attention_masks)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(model_layer, learning_rate, use_meta = USE_META, add_dense = ADD_DENSE,\n               dense_dim = DENSE_DIM, add_dropout = ADD_DROPOUT, dropout = DROPOUT):\n    \n    #define inputs\n    input_ids = tf.keras.Input(shape=(60,),dtype='int32')\n    attention_masks = tf.keras.Input(shape=(60,),dtype='int32')\n    meta_input = tf.keras.Input(shape = (meta_train.shape[1], ))\n    \n    #insert BERT layer\n    transformer_layer = model_layer([input_ids,attention_masks])\n    \n    #choose only last hidden-state\n    output = transformer_layer[1]\n    \n    #add meta data\n    if use_meta:\n        output = tf.keras.layers.Concatenate()([output, meta_input])\n    \n    #add dense relu layer\n    if add_dense:\n        print(\"Training with additional dense layer...\")\n        output = tf.keras.layers.Dense(dense_dim,activation='relu')(output)\n    \n    #add dropout\n    if add_dropout:\n        print(\"Training with dropout...\")\n        output = tf.keras.layers.Dropout(dropout)(output)\n    \n    #add final node for binary classification\n    output = tf.keras.layers.Dense(1,activation='sigmoid')(output)\n    \n    #assemble and compile\n    if use_meta:\n        print(\"Training with meta-data...\")\n        model = tf.keras.models.Model(inputs = [input_ids,attention_masks, meta_input],outputs = output)\n    \n    else:\n        print(\"Training without meta-data...\")\n        model = tf.keras.models.Model(inputs = [input_ids,attention_masks],outputs = output)\n\n    model.compile(tf.keras.optimizers.Adam(lr=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#reimport train and test since we won't be doing any text cleaning\ntrain = pd.read_csv('../input/nlp-getting-started/train.csv')\ntest = pd.read_csv('../input/nlp-getting-started/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get BERT layer\nbert_large = TFAutoModel.from_pretrained('bert-large-uncased')\n\n#get BERT tokenizer\nTOKENIZER = AutoTokenizer.from_pretrained(\"bert-large-uncased\")\n\n#get our inputs\ntrain_input_ids,train_attention_masks = bert_encode(train,60)\ntest_input_ids,test_attention_masks = bert_encode(test,60)\n\n#debugging step\nprint('Train length:', len(train_input_ids))\nprint('Test length:', len(test_input_ids))\n\n#and build and view parameters\nBERT_large = build_model(bert_large, learning_rate = 1e-5)\nBERT_large.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint = tf.keras.callbacks.ModelCheckpoint('large_model.h5', monitor='val_loss', save_best_only = True, save_weights_only = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train BERT\nhistory_bert = BERT_large.fit([train_input_ids,train_attention_masks, meta_train], train.target,\n                         validation_split = .2, epochs = EPOCHS, callbacks = [checkpoint], batch_size = BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#load model with best losses\nBERT_large.load_weights('large_model.h5')\n\npreds_bert = BERT_large.predict([test_input_ids,test_attention_masks,meta_test])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#save as dataframe\nsubmission_bert = pd.DataFrame()\nsubmission_bert['id'] = test_id\nsubmission_bert['prob'] = preds_bert\nsubmission_bert['target'] = np.round(submission_bert['prob']).astype(int)\nsubmission_bert.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# V. Submission\n\n**Now we are finally ready to submit our predictions:**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#and last but not least, submit\nsubmission_bert = submission_bert[['id', 'target']]\nsubmission_bert.to_csv('submission_bert.csv', index = False)\nprint('Blended submission has been saved to disk')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}