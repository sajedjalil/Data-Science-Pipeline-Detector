{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <div align = \"center\"><h1>NLP with Disaster Tweets</h3></div>\n<img src = \"https://www.thestatesman.com/wp-content/uploads/2022/05/54a547740a8b1bf4b5960fcbfba825e4-1.jpg\" style = \"width: 100%\">\n\n# Introduction\n<div class = \"alert alert-info\">\n<b>Natural Language Processing (NLP)</b> enables computers to understand natural languages as humans do. Whether the language is spoken or written, natural language processing uses artificial intelligence to take real-world inputs, process it, and make sense of it in a way a computer can understand. In this kernel we'll explore classification of tweets as disaster or non-disaster using <b> Hugging Face ü§ó Transformers</b> and <b>BERT</b>. Founded in 2016, Hugging Face ü§ó evolved from a developer of natural language processing (NLP) technology into an open-source library and community platform where popular NLP models such as <b>BERT, GPT-2, T5 and DistilBERT</b> are available.    \n</div>","metadata":{}},{"cell_type":"code","source":"!pip install text-hammer","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-02T08:13:30.449179Z","iopub.execute_input":"2022-06-02T08:13:30.449463Z","iopub.status.idle":"2022-06-02T08:13:37.838145Z","shell.execute_reply.started":"2022-06-02T08:13:30.449423Z","shell.execute_reply":"2022-06-02T08:13:37.837332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\nfrom wordcloud import WordCloud\n\nfrom geopy.geocoders import Nominatim\nfrom geopy.extra.rate_limiter import RateLimiter\nimport folium \nfrom folium import plugins \n\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom collections import Counter\nfrom collections import defaultdict\nimport text_hammer as th\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, Input, Model\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.utils import plot_model\n\nimport transformers\nfrom transformers import AutoTokenizer, DataCollatorWithPadding\nfrom transformers import AutoModelForSequenceClassification, TrainingArguments\nfrom transformers import TFBertModel, Trainer\nfrom tokenizers import Tokenizer\nfrom transformers import BertTokenizer\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-06-02T08:15:26.350764Z","iopub.execute_input":"2022-06-02T08:15:26.35103Z","iopub.status.idle":"2022-06-02T08:15:27.02745Z","shell.execute_reply.started":"2022-06-02T08:15:26.350999Z","shell.execute_reply":"2022-06-02T08:15:27.026545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Color Scheme\nSetting the color scheme for the notebook","metadata":{}},{"cell_type":"code","source":"custom_colors = ['#000000', '#E31E33', '#4A53E1', '#F5AD02', '#94D5EA', '#F6F8F7']\ncustom_palette = sns.set_palette(sns.color_palette(custom_colors))\nsns.palplot(sns.color_palette(custom_colors), size = 1)\nplt.tick_params(axis = 'both', labelsize = 0, length = 0)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-02T08:15:29.273121Z","iopub.execute_input":"2022-06-02T08:15:29.273931Z","iopub.status.idle":"2022-06-02T08:15:29.372975Z","shell.execute_reply.started":"2022-06-02T08:15:29.273893Z","shell.execute_reply":"2022-06-02T08:15:29.37211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Check For GPU","metadata":{}},{"cell_type":"code","source":"def kaggle_gpu():\n    device_name = tf.test.gpu_device_name()\n\n    if 'GPU' not in device_name:\n        print('GPU device not found')\n    else:\n        print('Found GPU at: {}'.format(device_name))\n    \n    if  tf.config.list_physical_devices('GPU'):\n        print('GPU availabe')\n    else:\n        print('GPU not available')\n\nkaggle_gpu()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-02T08:15:29.754457Z","iopub.execute_input":"2022-06-02T08:15:29.75516Z","iopub.status.idle":"2022-06-02T08:15:37.886793Z","shell.execute_reply.started":"2022-06-02T08:15:29.755116Z","shell.execute_reply":"2022-06-02T08:15:37.885992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking at the input files","metadata":{}},{"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2022-06-02T08:15:37.888898Z","iopub.execute_input":"2022-06-02T08:15:37.889517Z","iopub.status.idle":"2022-06-02T08:15:37.897899Z","shell.execute_reply.started":"2022-06-02T08:15:37.889477Z","shell.execute_reply":"2022-06-02T08:15:37.896978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Reading the dataframe","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest_df = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\ndf","metadata":{"execution":{"iopub.status.busy":"2022-06-02T08:15:37.899601Z","iopub.execute_input":"2022-06-02T08:15:37.900282Z","iopub.status.idle":"2022-06-02T08:15:37.97319Z","shell.execute_reply.started":"2022-06-02T08:15:37.900245Z","shell.execute_reply":"2022-06-02T08:15:37.972395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df.isna().sum())\nprint('----------------------------')\nprint('Total Missing Values: ', df.isna().sum().sum())\nprint('----------------------------')","metadata":{"execution":{"iopub.status.busy":"2022-06-02T08:15:37.97511Z","iopub.execute_input":"2022-06-02T08:15:37.975352Z","iopub.status.idle":"2022-06-02T08:15:37.99185Z","shell.execute_reply.started":"2022-06-02T08:15:37.975318Z","shell.execute_reply":"2022-06-02T08:15:37.991045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Most of the missing values in our dataframe are from the `location` and `keyword` columns","metadata":{}},{"cell_type":"markdown","source":"# EDA (Exploratory Data Analysis)","metadata":{}},{"cell_type":"markdown","source":"Visualizing the missing data in the form of a chart","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (15, 10))\nsns.heatmap(df.isna(), yticklabels = False, cbar = False, cmap = 'afmhot')\nplt.title(\"Visualizing the Missing Data\", fontsize = 20)\nplt.xticks(rotation = 35, fontsize = 15)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-02T08:15:37.993326Z","iopub.execute_input":"2022-06-02T08:15:37.993572Z","iopub.status.idle":"2022-06-02T08:15:38.198737Z","shell.execute_reply.started":"2022-06-02T08:15:37.993539Z","shell.execute_reply":"2022-06-02T08:15:38.198043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Bar graph representation of the missing values","metadata":{}},{"cell_type":"code","source":"msno.bar(df, color = (0, 0, 0), sort = \"ascending\", figsize = (15, 10))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-02T08:15:38.200185Z","iopub.execute_input":"2022-06-02T08:15:38.200726Z","iopub.status.idle":"2022-06-02T08:15:38.75741Z","shell.execute_reply.started":"2022-06-02T08:15:38.200685Z","shell.execute_reply":"2022-06-02T08:15:38.756498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets take a look at the class distribution of our dataset","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (15, 12))\nax = plt.axes()\nax.set_facecolor('black')\nax = sns.countplot(x = 'target', data = df, palette = [custom_colors[2], custom_colors[1]], edgecolor = 'white', linewidth = 1.2)\nplt.title('Disaster Count', fontsize = 25)\nplt.xlabel('Disaster', fontsize = 20)\nplt.ylabel('Count', fontsize = 20)\nax.xaxis.set_tick_params(labelsize = 15)\nax.yaxis.set_tick_params(labelsize = 15)\nbbox_args = dict(boxstyle = 'round', fc = '0.9')\nfor p in ax.patches:\n        ax.annotate('{:.0f} = {:.2f}%'.format(p.get_height(), (p.get_height() / len(df['target'])) * 100), (p.get_x() + 0.25, p.get_height() + 60), \n                   color = 'black',\n                   bbox = bbox_args,\n                   fontsize = 15)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-02T08:15:38.759083Z","iopub.execute_input":"2022-06-02T08:15:38.759356Z","iopub.status.idle":"2022-06-02T08:15:38.965806Z","shell.execute_reply.started":"2022-06-02T08:15:38.759319Z","shell.execute_reply":"2022-06-02T08:15:38.965134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is a class imbalance in the dataset, with 4342 non-disaster tweets and 3271 disaster tweets.\n\nLet's take a look at where most of the tweets in our dataset come from:","metadata":{}},{"cell_type":"code","source":"df['location'].value_counts()[:10]","metadata":{"execution":{"iopub.status.busy":"2022-06-02T08:15:38.966876Z","iopub.execute_input":"2022-06-02T08:15:38.967276Z","iopub.status.idle":"2022-06-02T08:15:38.979417Z","shell.execute_reply.started":"2022-06-02T08:15:38.967239Z","shell.execute_reply":"2022-06-02T08:15:38.978562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Bar chart representation of the locations from where the highest number of tweets originate","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (15, 13))\nax = plt.axes()\nax.set_facecolor('black')\nax = ((df.location.value_counts())[:10]).plot(kind = 'bar', color = custom_colors[2], linewidth = 2, edgecolor = 'white')\nplt.title('Location Count', fontsize = 30)\nplt.xlabel('Location', fontsize = 25)\nplt.ylabel('Count', fontsize = 25)\nax.xaxis.set_tick_params(labelsize = 15, rotation = 30)\nax.yaxis.set_tick_params(labelsize = 15)\nbbox_args = dict(boxstyle = 'round', fc = '0.9')\nfor p in ax.patches:\n        ax.annotate('{:.0f}'.format(p.get_height()), (p.get_x() + 0.15, p.get_height() + 2),\n                   bbox = bbox_args,\n                   color = custom_colors[2],\n                   fontsize = 15)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T08:15:39.083184Z","iopub.execute_input":"2022-06-02T08:15:39.083388Z","iopub.status.idle":"2022-06-02T08:15:39.401028Z","shell.execute_reply.started":"2022-06-02T08:15:39.083364Z","shell.execute_reply":"2022-06-02T08:15:39.400241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Visualizing the top 10 locations from where most of the tweets come from","metadata":{}},{"cell_type":"code","source":"new_df = pd.DataFrame()\nnew_df['location'] = ((df['location'].value_counts())[:10]).index\nnew_df['count'] = ((df['location'].value_counts())[:10]).values\ngeolocator = Nominatim(user_agent = 'Rahil')\ngeocode = RateLimiter(geolocator.geocode, min_delay_seconds = 0.5)\nlat = {}\nlong = {}\nfor i in new_df['location']:\n    location = geocode(i)\n    lat[i] = location.latitude\n    long[i] = location.longitude\nnew_df['latitude'] = new_df['location'].map(lat)\nnew_df['longitude'] = new_df['location'].map(long)\nmap = folium.Map(location = [10.0, 10.0], tiles = 'CartoDB dark_matter', zoom_start = 1.5)\nmarkers = []\ntitle = '''<h1 align = \"center\" style = \"font-size: 35px\"><b>Top 10 Tweet Locations</b></h1>'''\nfor i, r in new_df.iterrows():\n    loss = r['count']\n    if r['count'] > 0:\n        counts = r['count'] * 0.4\n        folium.CircleMarker([float(r['latitude']), float(r['longitude'])], radius = float(counts), color = custom_colors[1], fill = True).add_to(map)\nmap.get_root().html.add_child(folium.Element(title))\nmap","metadata":{"execution":{"iopub.status.busy":"2022-06-02T08:15:39.420544Z","iopub.execute_input":"2022-06-02T08:15:39.420735Z","iopub.status.idle":"2022-06-02T08:15:44.318701Z","shell.execute_reply.started":"2022-06-02T08:15:39.420711Z","shell.execute_reply":"2022-06-02T08:15:44.317888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"non_list_stopwords = stopwords.words('english')\nstopwords = list(stopwords.words('english'))\nstopwords[:10]","metadata":{"execution":{"iopub.status.busy":"2022-06-02T08:15:44.320395Z","iopub.execute_input":"2022-06-02T08:15:44.320725Z","iopub.status.idle":"2022-06-02T08:15:44.333553Z","shell.execute_reply.started":"2022-06-02T08:15:44.320687Z","shell.execute_reply":"2022-06-02T08:15:44.332552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"non_disaster_tweets_length = (df[df['target'] == 0])['text'].str.len()\ndisaster_tweets_length = (df[df['target'] == 1])['text'].str.len()\nprint(non_disaster_tweets_length)\nprint(disaster_tweets_length)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T08:15:44.334873Z","iopub.execute_input":"2022-06-02T08:15:44.335195Z","iopub.status.idle":"2022-06-02T08:15:44.35378Z","shell.execute_reply.started":"2022-06-02T08:15:44.335157Z","shell.execute_reply":"2022-06-02T08:15:44.352996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(1, 2, figsize = (30, 15))\nfig.suptitle('Tweet Character Length', fontsize = 45)\n\naxes[0].set_facecolor('black')\naxes[0].hist(non_disaster_tweets_length, color = custom_colors[1], edgecolor = 'white', linewidth = 4)\naxes[0].set_title('Non-Disaster Tweets', fontsize = 40)\naxes[0].set_xlabel('Character Length', fontsize = 35)\naxes[0].set_ylabel('Frequency', fontsize = 35)\naxes[0].xaxis.set_tick_params(labelsize = 30)\naxes[0].yaxis.set_tick_params(labelsize = 30)\n\naxes[1].set_facecolor('black')\naxes[1].hist(disaster_tweets_length, color = custom_colors[2], edgecolor = 'white', linewidth = 4)\naxes[1].set_title('Disaster Tweets', fontsize = 40)\naxes[1].set_xlabel('Character Length', fontsize = 35)\naxes[1].set_ylabel('Frequency', fontsize = 35)\naxes[1].xaxis.set_tick_params(labelsize = 30)\naxes[1].yaxis.set_tick_params(labelsize = 30)\n\nplt.subplots_adjust(wspace = 0.25, hspace = 0.1)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-02T08:16:10.235587Z","iopub.execute_input":"2022-06-02T08:16:10.235865Z","iopub.status.idle":"2022-06-02T08:16:10.763518Z","shell.execute_reply.started":"2022-06-02T08:16:10.235832Z","shell.execute_reply":"2022-06-02T08:16:10.761665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class tweet_functions:\n    \n    '''Getting the count of different attributes of our tweets'''\n    \n    def __init__(self, column):\n        self.column = column\n        \n    def count_characters(self):\n        return((self.column).apply(lambda word: len(str(word))))\n    \n    def count_words(self):\n        return((self.column).apply(lambda word: len(str(word).split())))\n    \n    def count_urls(self):\n        return((self.column).apply(lambda word: len([url for url in str(word).lower().split() if 'http' in word or 'https' in word])))\n    \n    def count_hashtags(self):\n        return((self.column).apply(lambda word: len([hashtag for hashtag in str(word) if '#' in hashtag])))\n    \n    def count_tags(self):\n        return((self.column).apply(lambda word: len([tag for tag in str(word) if '@' in tag])))\n    \n    def count_stopwords(self):\n        return((self.column).apply(lambda word: len([word for word in str(word).lower().split() if word in stopwords])))","metadata":{"execution":{"iopub.status.busy":"2022-06-02T08:16:10.764805Z","iopub.execute_input":"2022-06-02T08:16:10.765039Z","iopub.status.idle":"2022-06-02T08:16:10.775597Z","shell.execute_reply.started":"2022-06-02T08:16:10.764996Z","shell.execute_reply":"2022-06-02T08:16:10.77495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(nrows = 3, ncols = 2, figsize = (30, 30))\n\naxes[0][0].set_facecolor('black')\nsns.distplot(tweet_functions((df[df['target'] == 0])['text']).count_characters(), ax = axes[0][0], color = custom_colors[3], label = 'Non-Disaster Tweets', kde_kws = dict(linewidth = 3.5))\nsns.distplot(tweet_functions((df[df['target'] == 1])['text']).count_characters(), ax = axes[0][0], color = custom_colors[4], label = 'Disaster Tweets', kde_kws = dict(linewidth = 3.5))\naxes[0][0].set_title('Character Count', fontsize = 45)\naxes[0][0].set_xlabel('Characters', fontsize = 40)\naxes[0][0].set_ylabel('Density', fontsize = 40)\naxes[0][0].xaxis.set_tick_params(labelsize = 30)\naxes[0][0].yaxis.set_tick_params(labelsize = 30)\naxes[0][0].legend(facecolor = 'black', labelcolor = 'white', prop = {'size': 25}).get_frame().set_linewidth(2.5)\n\naxes[0][1].set_facecolor('black')\nsns.distplot(tweet_functions((df[df['target'] == 0])['text']).count_words(), ax = axes[0][1], color = custom_colors[3], label = 'Non-Disaster Tweets', kde_kws = dict(linewidth = 3.5))\nsns.distplot(tweet_functions((df[df['target'] == 1])['text']).count_words(), ax = axes[0][1], color = custom_colors[4], label = 'Disaster Tweets', kde_kws = dict(linewidth = 3.5))\naxes[0][1].set_title('Word Count', fontsize = 45)\naxes[0][1].set_xlabel('Words', fontsize = 40)\naxes[0][1].set_ylabel('Density', fontsize = 40)\naxes[0][1].xaxis.set_tick_params(labelsize = 30)\naxes[0][1].yaxis.set_tick_params(labelsize = 30)\naxes[0][1].legend(facecolor = 'black', labelcolor = 'white', prop = {'size': 25}).get_frame().set_linewidth(2.5)\n\naxes[1][0].set_facecolor('black')\nsns.distplot(tweet_functions((df[df['target'] == 0])['text']).count_urls(), ax = axes[1][0], color = custom_colors[3], label = 'Non-Disaster Tweets', kde_kws = dict(linewidth = 3.5))\nsns.distplot(tweet_functions((df[df['target'] == 1])['text']).count_urls(), ax = axes[1][0], color = custom_colors[4], label = 'Disaster Tweets', kde_kws = dict(linewidth = 3.5))\naxes[1][0].set_title('URL Count', fontsize = 45)\naxes[1][0].set_xlabel('URLs', fontsize = 40)\naxes[1][0].set_ylabel('Density', fontsize = 40)\naxes[1][0].xaxis.set_tick_params(labelsize = 30)\naxes[1][0].yaxis.set_tick_params(labelsize = 30)\naxes[1][0].legend(facecolor = 'black', labelcolor = 'white', prop = {'size': 25}).get_frame().set_linewidth(2.5)\n\naxes[1][1].set_facecolor('black')\nsns.distplot(tweet_functions((df[df['target'] == 0])['text']).count_hashtags(), ax = axes[1][1], color = custom_colors[3], label = 'Non-Disaster Tweets', kde_kws = dict(linewidth = 3.5))\nsns.distplot(tweet_functions((df[df['target'] == 1])['text']).count_hashtags(), ax = axes[1][1], color = custom_colors[4], label = 'Disaster Tweets', kde_kws = dict(linewidth = 3.5))\naxes[1][1].set_title('Hashtag Count', fontsize = 45)\naxes[1][1].set_xlabel('Hashtags', fontsize = 40)\naxes[1][1].set_ylabel('Density', fontsize = 40)\naxes[1][1].xaxis.set_tick_params(labelsize = 30)\naxes[1][1].yaxis.set_tick_params(labelsize = 30)\naxes[1][1].legend(facecolor = 'black', labelcolor = 'white', prop = {'size': 25}).get_frame().set_linewidth(2.5)\n\naxes[2][0].set_facecolor('black')\nsns.distplot(tweet_functions((df[df['target'] == 0])['text']).count_tags(), ax = axes[2][0], color = custom_colors[3], label = 'Non-Disaster Tweets', kde_kws = dict(linewidth = 3.5))\nsns.distplot(tweet_functions((df[df['target'] == 1])['text']).count_tags(), ax = axes[2][0], color = custom_colors[4], label = 'Disaster Tweets', kde_kws = dict(linewidth = 3.5))\naxes[2][0].set_title('Mention Count', fontsize = 45)\naxes[2][0].set_xlabel('Mentions', fontsize = 40)\naxes[2][0].set_ylabel('Density', fontsize = 40)\naxes[2][0].xaxis.set_tick_params(labelsize = 30)\naxes[2][0].yaxis.set_tick_params(labelsize = 30)\naxes[2][0].legend(facecolor = 'black', labelcolor = 'white', prop = {'size': 25}).get_frame().set_linewidth(2.5)\n\naxes[2][1].set_facecolor('black')\nsns.distplot(tweet_functions((df[df['target'] == 0])['text']).count_stopwords(), ax = axes[2][1], color = custom_colors[3], label = 'Non-Disaster Tweets', kde_kws = dict(linewidth = 3.5))\nsns.distplot(tweet_functions((df[df['target'] == 1])['text']).count_stopwords(), ax = axes[2][1], color = custom_colors[4], label = 'Disaster Tweets', kde_kws = dict(linewidth = 3.5))\naxes[2][1].set_title('Stopword Count', fontsize = 45)\naxes[2][1].set_xlabel('Stopwords', fontsize = 40)\naxes[2][1].set_ylabel('Density', fontsize = 40)\naxes[2][1].xaxis.set_tick_params(labelsize = 30)\naxes[2][1].yaxis.set_tick_params(labelsize = 30)\naxes[2][1].legend(facecolor = 'black', labelcolor = 'white', prop = {'size': 25}).get_frame().set_linewidth(2.5)\n\nplt.subplots_adjust(hspace = 0.5)\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-02T08:16:10.840561Z","iopub.execute_input":"2022-06-02T08:16:10.840848Z","iopub.status.idle":"2022-06-02T08:16:13.89545Z","shell.execute_reply.started":"2022-06-02T08:16:10.840819Z","shell.execute_reply":"2022-06-02T08:16:13.892016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing the Tweets","metadata":{}},{"cell_type":"code","source":"def remove_urls(text):\n    urls = re.compile(r'https?://\\S+|www\\.\\S+')\n    return urls.sub(r'', text)\n\ndef remove_HTML(text):\n    html = re.compile('<.*?>')\n    return html.sub(r'', text)\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile('['\n                           u'\\U0001F600-\\U0001F64F'\n                           u'\\U0001F300-\\U0001F5FF'\n                           u'\\U0001F680-\\U0001F6FF'\n                           u'\\U0001F1E0-\\U0001F1FF'\n                           u'\\U00002702-\\U000027B0'\n                           u'\\U000024C2-\\U0001F251'\n                           ']+', flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\ndef remove_emoticons(text):\n    emoticons = {\n    u\":‚Äë\\)\":\"Happy face or smiley\",\n    u\":\\)\":\"Happy face or smiley\",\n    u\":-\\]\":\"Happy face or smiley\",\n    u\":\\]\":\"Happy face or smiley\",\n    u\":-3\":\"Happy face smiley\",\n    u\":3\":\"Happy face smiley\",\n    u\":->\":\"Happy face smiley\",\n    u\":>\":\"Happy face smiley\",\n    u\"8-\\)\":\"Happy face smiley\",\n    u\":o\\)\":\"Happy face smiley\",\n    u\":-\\}\":\"Happy face smiley\",\n    u\":\\}\":\"Happy face smiley\",\n    u\":-\\)\":\"Happy face smiley\",\n    u\":c\\)\":\"Happy face smiley\",\n    u\":\\^\\)\":\"Happy face smiley\",\n    u\"=\\]\":\"Happy face smiley\",\n    u\"=\\)\":\"Happy face smiley\",\n    u\":‚ÄëD\":\"Laughing, big grin or laugh with glasses\",\n    u\":D\":\"Laughing, big grin or laugh with glasses\",\n    u\"8‚ÄëD\":\"Laughing, big grin or laugh with glasses\",\n    u\"8D\":\"Laughing, big grin or laugh with glasses\",\n    u\"X‚ÄëD\":\"Laughing, big grin or laugh with glasses\",\n    u\"XD\":\"Laughing, big grin or laugh with glasses\",\n    u\"=D\":\"Laughing, big grin or laugh with glasses\",\n    u\"=3\":\"Laughing, big grin or laugh with glasses\",\n    u\"B\\^D\":\"Laughing, big grin or laugh with glasses\",\n    u\":-\\)\\)\":\"Very happy\",\n    u\":‚Äë\\(\":\"Frown, sad, andry or pouting\",\n    u\":-\\(\":\"Frown, sad, andry or pouting\",\n    u\":\\(\":\"Frown, sad, andry or pouting\",\n    u\":‚Äëc\":\"Frown, sad, andry or pouting\",\n    u\":c\":\"Frown, sad, andry or pouting\",\n    u\":‚Äë<\":\"Frown, sad, andry or pouting\",\n    u\":<\":\"Frown, sad, andry or pouting\",\n    u\":‚Äë\\[\":\"Frown, sad, andry or pouting\",\n    u\":\\[\":\"Frown, sad, andry or pouting\",\n    u\":-\\|\\|\":\"Frown, sad, andry or pouting\",\n    u\">:\\[\":\"Frown, sad, andry or pouting\",\n    u\":\\{\":\"Frown, sad, andry or pouting\",\n    u\":@\":\"Frown, sad, andry or pouting\",\n    u\">:\\(\":\"Frown, sad, andry or pouting\",\n    u\":'‚Äë\\(\":\"Crying\",\n    u\":'\\(\":\"Crying\",\n    u\":'‚Äë\\)\":\"Tears of happiness\",\n    u\":'\\)\":\"Tears of happiness\",\n    u\"D‚Äë':\":\"Horror\",\n    u\"D:<\":\"Disgust\",\n    u\"D:\":\"Sadness\",\n    u\"D8\":\"Great dismay\",\n    u\"D;\":\"Great dismay\",\n    u\"D=\":\"Great dismay\",\n    u\"DX\":\"Great dismay\",\n    u\":‚ÄëO\":\"Surprise\",\n    u\":O\":\"Surprise\",\n    u\":‚Äëo\":\"Surprise\",\n    u\":o\":\"Surprise\",\n    u\":-0\":\"Shock\",\n    u\"8‚Äë0\":\"Yawn\",\n    u\">:O\":\"Yawn\",\n    u\":-\\*\":\"Kiss\",\n    u\":\\*\":\"Kiss\",\n    u\":X\":\"Kiss\",\n    u\";‚Äë\\)\":\"Wink or smirk\",\n    u\";\\)\":\"Wink or smirk\",\n    u\"\\*-\\)\":\"Wink or smirk\",\n    u\"\\*\\)\":\"Wink or smirk\",\n    u\";‚Äë\\]\":\"Wink or smirk\",\n    u\";\\]\":\"Wink or smirk\",\n    u\";\\^\\)\":\"Wink or smirk\",\n    u\":‚Äë,\":\"Wink or smirk\",\n    u\";D\":\"Wink or smirk\",\n    u\":‚ÄëP\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"X‚ÄëP\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"XP\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":‚Äë√û\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":√û\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":b\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"d:\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"=p\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\">:P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":‚Äë/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":-[.]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\">:[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\">:/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\"=/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\"=[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":L\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\"=L\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":S\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":‚Äë\\|\":\"Straight face\",\n    u\":\\|\":\"Straight face\",\n    u\":$\":\"Embarrassed or blushing\",\n    u\":‚Äëx\":\"Sealed lips or wearing braces or tongue-tied\",\n    u\":x\":\"Sealed lips or wearing braces or tongue-tied\",\n    u\":‚Äë#\":\"Sealed lips or wearing braces or tongue-tied\",\n    u\":#\":\"Sealed lips or wearing braces or tongue-tied\",\n    u\":‚Äë&\":\"Sealed lips or wearing braces or tongue-tied\",\n    u\":&\":\"Sealed lips or wearing braces or tongue-tied\",\n    u\"O:‚Äë\\)\":\"Angel, saint or innocent\",\n    u\"O:\\)\":\"Angel, saint or innocent\",\n    u\"0:‚Äë3\":\"Angel, saint or innocent\",\n    u\"0:3\":\"Angel, saint or innocent\",\n    u\"0:‚Äë\\)\":\"Angel, saint or innocent\",\n    u\"0:\\)\":\"Angel, saint or innocent\",\n    u\":‚Äëb\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"0;\\^\\)\":\"Angel, saint or innocent\",\n    u\">:‚Äë\\)\":\"Evil or devilish\",\n    u\">:\\)\":\"Evil or devilish\",\n    u\"\\}:‚Äë\\)\":\"Evil or devilish\",\n    u\"\\}:\\)\":\"Evil or devilish\",\n    u\"3:‚Äë\\)\":\"Evil or devilish\",\n    u\"3:\\)\":\"Evil or devilish\",\n    u\">;\\)\":\"Evil or devilish\",\n    u\"\\|;‚Äë\\)\":\"Cool\",\n    u\"\\|‚ÄëO\":\"Bored\",\n    u\":‚ÄëJ\":\"Tongue-in-cheek\",\n    u\"#‚Äë\\)\":\"Party all night\",\n    u\"%‚Äë\\)\":\"Drunk or confused\",\n    u\"%\\)\":\"Drunk or confused\",\n    u\":-###..\":\"Being sick\",\n    u\":###..\":\"Being sick\",\n    u\"<:‚Äë\\|\":\"Dump\",\n    u\"\\(>_<\\)\":\"Troubled\",\n    u\"\\(>_<\\)>\":\"Troubled\",\n    u\"\\(';'\\)\":\"Baby\",\n    u\"\\(\\^\\^>``\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n    u\"\\(\\^_\\^;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n    u\"\\(-_-;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n    u\"\\(~_~;\\) \\(„Éª\\.„Éª;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n    u\"\\(-_-\\)zzz\":\"Sleeping\",\n    u\"\\(\\^_-\\)\":\"Wink\",\n    u\"\\(\\(\\+_\\+\\)\\)\":\"Confused\",\n    u\"\\(\\+o\\+\\)\":\"Confused\",\n    u\"\\(o\\|o\\)\":\"Ultraman\",\n    u\"\\^_\\^\":\"Joyful\",\n    u\"\\(\\^_\\^\\)/\":\"Joyful\",\n    u\"\\(\\^O\\^\\)Ôºè\":\"Joyful\",\n    u\"\\(\\^o\\^\\)Ôºè\":\"Joyful\",\n    u\"\\(__\\)\":\"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"_\\(\\._\\.\\)_\":\"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"<\\(_ _\\)>\":\"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"<m\\(__\\)m>\":\"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"m\\(__\\)m\":\"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"m\\(_ _\\)m\":\"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"\\('_'\\)\":\"Sad or Crying\",\n    u\"\\(/_;\\)\":\"Sad or Crying\",\n    u\"\\(T_T\\) \\(;_;\\)\":\"Sad or Crying\",\n    u\"\\(;_;\":\"Sad of Crying\",\n    u\"\\(;_:\\)\":\"Sad or Crying\",\n    u\"\\(;O;\\)\":\"Sad or Crying\",\n    u\"\\(:_;\\)\":\"Sad or Crying\",\n    u\"\\(ToT\\)\":\"Sad or Crying\",\n    u\";_;\":\"Sad or Crying\",\n    u\";-;\":\"Sad or Crying\",\n    u\";n;\":\"Sad or Crying\",\n    u\";;\":\"Sad or Crying\",\n    u\"Q\\.Q\":\"Sad or Crying\",\n    u\"T\\.T\":\"Sad or Crying\",\n    u\"QQ\":\"Sad or Crying\",\n    u\"Q_Q\":\"Sad or Crying\",\n    u\"\\(-\\.-\\)\":\"Shame\",\n    u\"\\(-_-\\)\":\"Shame\",\n    u\"\\(‰∏Ä‰∏Ä\\)\":\"Shame\",\n    u\"\\(Ôºõ‰∏Ä_‰∏Ä\\)\":\"Shame\",\n    u\"\\(=_=\\)\":\"Tired\",\n    u\"\\(=\\^\\¬∑\\^=\\)\":\"cat\",\n    u\"\\(=\\^\\¬∑\\¬∑\\^=\\)\":\"cat\",\n    u\"=_\\^=\t\":\"cat\",\n    u\"\\(\\.\\.\\)\":\"Looking down\",\n    u\"\\(\\._\\.\\)\":\"Looking down\",\n    u\"\\^m\\^\":\"Giggling with hand covering mouth\",\n    u\"\\(\\„Éª\\„Éª?\":\"Confusion\",\n    u\"\\(?_?\\)\":\"Confusion\",\n    u\">\\^_\\^<\":\"Normal Laugh\",\n    u\"<\\^!\\^>\":\"Normal Laugh\",\n    u\"\\^/\\^\":\"Normal Laugh\",\n    u\"\\Ôºà\\*\\^_\\^\\*Ôºâ\" :\"Normal Laugh\",\n    u\"\\(\\^<\\^\\) \\(\\^\\.\\^\\)\":\"Normal Laugh\",\n    u\"\\(^\\^\\)\":\"Normal Laugh\",\n    u\"\\(\\^\\.\\^\\)\":\"Normal Laugh\",\n    u\"\\(\\^_\\^\\.\\)\":\"Normal Laugh\",\n    u\"\\(\\^_\\^\\)\":\"Normal Laugh\",\n    u\"\\(\\^\\^\\)\":\"Normal Laugh\",\n    u\"\\(\\^J\\^\\)\":\"Normal Laugh\",\n    u\"\\(\\*\\^\\.\\^\\*\\)\":\"Normal Laugh\",\n    u\"\\(\\^‚Äî\\^\\Ôºâ\":\"Normal Laugh\",\n    u\"\\(#\\^\\.\\^#\\)\":\"Normal Laugh\",\n    u\"\\Ôºà\\^‚Äî\\^\\Ôºâ\":\"Waving\",\n    u\"\\(;_;\\)/~~~\":\"Waving\",\n    u\"\\(\\^\\.\\^\\)/~~~\":\"Waving\",\n    u\"\\(-_-\\)/~~~ \\($\\¬∑\\¬∑\\)/~~~\":\"Waving\",\n    u\"\\(T_T\\)/~~~\":\"Waving\",\n    u\"\\(ToT\\)/~~~\":\"Waving\",\n    u\"\\(\\*\\^0\\^\\*\\)\":\"Excited\",\n    u\"\\(\\*_\\*\\)\":\"Amazed\",\n    u\"\\(\\*_\\*;\":\"Amazed\",\n    u\"\\(\\+_\\+\\) \\(@_@\\)\":\"Amazed\",\n    u\"\\(\\*\\^\\^\\)v\":\"Laughing,Cheerful\",\n    u\"\\(\\^_\\^\\)v\":\"Laughing,Cheerful\",\n    u\"\\(\\(d[-_-]b\\)\\)\":\"Headphones,Listening to music\",\n    u'\\(-\"-\\)':\"Worried\",\n    u\"\\(„Éº„Éº;\\)\":\"Worried\",\n    u\"\\(\\^0_0\\^\\)\":\"Eyeglasses\",\n    u\"\\(\\ÔºæÔΩñ\\Ôºæ\\)\":\"Happy\",\n    u\"\\(\\ÔºæÔΩï\\Ôºæ\\)\":\"Happy\",\n    u\"\\(\\^\\)o\\(\\^\\)\":\"Happy\",\n    u\"\\(\\^O\\^\\)\":\"Happy\",\n    u\"\\(\\^o\\^\\)\":\"Happy\",\n    u\"\\)\\^o\\^\\(\":\"Happy\",\n    u\":O o_O\":\"Surprised\",\n    u\"o_0\":\"Surprised\",\n    u\"o\\.O\":\"Surpised\",\n    u\"\\(o\\.o\\)\":\"Surprised\",\n    u\"oO\":\"Surprised\",\n    u\"\\(\\*Ôø£mÔø£\\)\":\"Dissatisfied\",\n    u\"\\(‚ÄòA`\\)\":\"Snubbed or Deflated\"\n    }\n    emoticon_pattern = re.compile(u'(' + u'|'.join(k for k in emoticons) + u')')\n    return emoticon_pattern.sub(r'', text)\n\ndef remove_mentions(text):\n    mentions = re.compile('@[A-Za-z0-9_]+')\n    return mentions.sub(r'', text)\n\ndef word_lemmatizer(text):\n    lemmatizer = WordNetLemmatizer()\n    return ' '.join([lemmatizer.lemmatize(word) for word in text.split()])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-02T08:16:13.897754Z","iopub.execute_input":"2022-06-02T08:16:13.898252Z","iopub.status.idle":"2022-06-02T08:16:13.93527Z","shell.execute_reply.started":"2022-06-02T08:16:13.898214Z","shell.execute_reply":"2022-06-02T08:16:13.934552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text'] = df['text'].str.lower() # convert to lowercase\ndf['text'] = df['text'].apply(lambda text: remove_urls(text)) # remove URLs\ndf['text'] = df['text'].apply(lambda text: remove_HTML(text)) # remove HTML tags\ndf['text'] = df['text'].str.translate(str.maketrans('', '', string.punctuation)) # remove punctuations\ndf['text'] = df['text'].apply(lambda text: ' '.join([word for word in str(text).split() if word not in stopwords])) # remove stopwords\ndf['text'] = df['text'].apply(lambda text: remove_emoji(text)) # remove emojis\ndf['text'] = df['text'].apply(lambda text: remove_emoticons(text)) # remove emoticons\ndf['text'] = df['text'].apply(lambda text: remove_mentions(text)) # remove mentions\ndf['text'] = df['text'].apply(lambda text: word_lemmatizer(text)) # lemmatize words\ndf['text'] = df['text'].apply(lambda text: th.cont_exp(text)) # convert i'm to i am, you're to you are, etc\nprint(df['text'])\n\ntest_df['text'] = test_df['text'].str.lower() # convert to lowercase\ntest_df['text'] = test_df['text'].apply(lambda text: remove_urls(text)) # remove URLs\ntest_df['text'] = test_df['text'].apply(lambda text: remove_HTML(text)) # remove HTML tags\ntest_df['text'] = test_df['text'].str.translate(str.maketrans('', '', string.punctuation)) # remove punctuations\ntest_df['text'] = test_df['text'].apply(lambda text: ' '.join([word for word in str(text).split() if word not in stopwords])) # remove stopwords\ntest_df['text'] = test_df['text'].apply(lambda text: remove_emoji(text)) # remove emojis\ntest_df['text'] = test_df['text'].apply(lambda text: remove_emoticons(text)) # remove emoticons\ntest_df['text'] = test_df['text'].apply(lambda text: remove_mentions(text)) # remove mentions\ntest_df['text'] = test_df['text'].apply(lambda text: word_lemmatizer(text)) # lemmatize words\ntest_df['text'] = test_df['text'].apply(lambda text: th.cont_exp(text)) # convert i'm to i am, you're to you are, etc\nprint(test_df['text'])","metadata":{"execution":{"iopub.status.busy":"2022-06-02T08:16:13.936623Z","iopub.execute_input":"2022-06-02T08:16:13.936878Z","iopub.status.idle":"2022-06-02T08:16:29.95755Z","shell.execute_reply.started":"2022-06-02T08:16:13.936842Z","shell.execute_reply":"2022-06-02T08:16:29.956748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"counter = Counter()\nfor text in df['text'].values:\n    for word in text.split():\n        counter[word] += 1\ncounter.most_common(10)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T08:16:29.95982Z","iopub.execute_input":"2022-06-02T08:16:29.960116Z","iopub.status.idle":"2022-06-02T08:16:30.011855Z","shell.execute_reply.started":"2022-06-02T08:16:29.960065Z","shell.execute_reply":"2022-06-02T08:16:30.011099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = dict(sorted(counter.items(), key = lambda x: x[1] ,reverse = True)[:10])\nwords = list(data.keys())\nfrequency = list(data.values())\n\nfig, ax = plt.subplots(nrows = 1, ncols = 1, figsize = (15, 15))\nax.set_facecolor('black')\nax = sns.barplot(x = frequency, y = words, color = '#8699A7', edgecolor = 'white', linewidth = 2)\nplt.title('Word Frequency', fontsize = 35)\nplt.xlabel('Frequency', fontsize = 30)\nplt.ylabel('Words', fontsize = 30)\nplt.xticks(size = 20)\nplt.yticks(size = 20)\nbbox_args = dict(boxstyle = 'round', fc = '0.9')\nfor p in ax.patches:\n    width = p.get_width()\n    plt.text(9.5 + p.get_width(), p.get_y() + 0.5 * p.get_height(), '{:1.0f}'.format(width), \n             ha = 'center', \n             va = 'center', \n             color = 'black', \n             bbox = bbox_args, \n             fontsize = 15)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-02T08:16:30.013088Z","iopub.execute_input":"2022-06-02T08:16:30.013334Z","iopub.status.idle":"2022-06-02T08:16:30.403683Z","shell.execute_reply.started":"2022-06-02T08:16:30.013299Z","shell.execute_reply":"2022-06-02T08:16:30.402959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating Ngrams\nIn extremely simple terms an `ngram` is a sequence of `n` words. Lets take an example: `This is a sentence`. \n\n<img src = \"https://images.deepai.org/glossary-terms/867de904ba9b46869af29cead3194b6c/8ARA1.png\">\n\nBased on the value of `n` we can generate different `ngrams` as follows:\n> - **N = 1 (Unigrams)**: `This`, `is`, `a`, `sentence`\n> - **N = 2 (Bigrams)**: `This is`, `is a`, `a sentence`\n> - **N = 3 (Trigrams)**: `This is a`, `is a sentence`\n\nNgrams find their applications in auto completion of sentences, speech recognition, machine translation and predictive text inputs.","metadata":{}},{"cell_type":"code","source":"def generate_ngrams(text, n_gram = 0):\n    token = [token for token in text.lower().split(' ') if token != '' if token not in non_list_stopwords]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [' '.join(ngram) for ngram in ngrams]\n\ndef generate_ngram_dictionaries(n_gram):\n    non_disaster_ngrams = defaultdict(int)\n    disaster_ngrams = defaultdict(int)\n\n    for tweet in df[df['target'] == 0]['text']:\n        for word in generate_ngrams(tweet, n_gram = n_gram):\n            non_disaster_ngrams[word] += 1\n\n    for tweet in df[df['target'] == 1]['text']:\n        for word in generate_ngrams(tweet, n_gram = n_gram):\n            disaster_ngrams[word] += 1\n\n    non_disaster_ngram_data = dict(sorted(non_disaster_ngrams.items(), key = lambda x: x[1], reverse = True)[:10])\n    non_disaster_ngram_words = list(non_disaster_ngram_data.keys())\n    non_disaster_ngram_frequency = list(non_disaster_ngram_data.values())\n\n    disaster_ngram_data = dict(sorted(disaster_ngrams.items(), key = lambda x: x[1], reverse = True)[:10])\n    disaster_ngram_words = list(disaster_ngram_data.keys())\n    disaster_ngram_frequency = list(disaster_ngram_data.values())\n    \n    return non_disaster_ngram_data, non_disaster_ngram_words, non_disaster_ngram_frequency, disaster_ngram_data, disaster_ngram_words, disaster_ngram_frequency\n\ndef create_ngram_graphs(non_disaster_ngram_words, non_disaster_ngram_frequency, disaster_ngram_words, disaster_ngram_frequency, n_gram):\n    fig, axes = plt.subplots(nrows = 1, ncols = 2, figsize = (30, 20))\n\n    axes[0].set_facecolor('black')\n    sns.barplot(x = non_disaster_ngram_frequency, y = non_disaster_ngram_words, ax = axes[0], color = '#0057B1', edgecolor = 'white', linewidth = 2)\n    if(n_gram == 1):\n        axes[0].set_title('Non-Disaster Unigrams', fontsize = 45)\n    if(n_gram == 2):\n        axes[0].set_title('Non-Disaster Bigrams', fontsize = 45)\n    if(n_gram == 3):\n        axes[0].set_title('Non-Disaster Trigrams', fontsize = 45)\n    axes[0].set_xlabel('Count', fontsize = 40)\n    axes[0].set_ylabel('Words', fontsize = 40)\n    if(n_gram == 1):\n        axes[0].xaxis.set_tick_params(labelsize = 30)\n        axes[0].yaxis.set_tick_params(labelsize = 30)\n    elif(n_gram == 2):\n        axes[0].xaxis.set_tick_params(labelsize = 20)\n        axes[0].yaxis.set_tick_params(labelsize = 20)\n    else:\n        axes[0].xaxis.set_tick_params(labelsize = 18)\n        axes[0].yaxis.set_tick_params(labelsize = 18)\n    for p in axes[0].patches:\n        width = p.get_width()\n        if(n_gram == 1 or n_gram == 2):\n            axes[0].text(0.75 + p.get_width(), p.get_y() + 0.5 * p.get_height(), '{:1.0f}'.format(width), \n                     ha = 'center', \n                     va = 'center', \n                     color = 'blue', \n                     bbox = bbox_args, \n                     fontsize = 25)\n        if(n_gram == 3):\n            axes[0].text(0.6 + p.get_width(), p.get_y() + 0.5 * p.get_height(), '{:1.0f}'.format(width), \n                     ha = 'center', \n                     va = 'center', \n                     color = 'blue', \n                     bbox = bbox_args, \n                     fontsize = 22)\n\n    axes[1].set_facecolor('black')\n    sns.barplot(x = disaster_ngram_frequency, y = disaster_ngram_words, ax = axes[1], palette = [custom_colors[1]], edgecolor = 'white', linewidth = 2)\n    if(n_gram == 1):\n        axes[1].set_title('Disaster Unigrams', fontsize = 45)\n    if(n_gram == 2):\n        axes[1].set_title('Disaster Bigrams', fontsize = 45)\n    if(n_gram == 3):\n        axes[1].set_title('Disaster Trigrams', fontsize = 45)\n    axes[1].set_xlabel('Count', fontsize = 40)\n    axes[1].set_ylabel('Words', fontsize = 40)\n    if(n_gram == 1):\n        axes[1].xaxis.set_tick_params(labelsize = 30)\n        axes[1].yaxis.set_tick_params(labelsize = 30)\n    elif(n_gram == 2):\n        axes[1].xaxis.set_tick_params(labelsize = 20)\n        axes[1].yaxis.set_tick_params(labelsize = 20)\n    else:\n        axes[1].xaxis.set_tick_params(labelsize = 18)\n        axes[1].yaxis.set_tick_params(labelsize = 18)\n    for p in axes[1].patches:\n        width = p.get_width()\n        if(n_gram == 1 or n_gram == 2):\n            axes[1].text(0.8 + p.get_width(), p.get_y() + 0.5 * p.get_height(), '{:1.0f}'.format(width), \n                     ha = 'center', \n                     va = 'center', \n                     color = 'red', \n                     bbox = bbox_args, \n                     fontsize = 25)\n        if(n_gram == 3):\n            axes[1].text(0.6 + p.get_width(), p.get_y() + 0.5 * p.get_height(), '{:1.0f}'.format(width), \n                     ha = 'center', \n                     va = 'center', \n                     color = 'red', \n                     bbox = bbox_args, \n                     fontsize = 18)\n    if(n_gram == 1 or n_gram == 2):\n        plt.subplots_adjust(wspace = 0.4)\n    if(n_gram == 3):\n        plt.subplots_adjust(wspace = 0.9)\n        \ndef final_output_ngram_graphs(ngram):\n    _, non_disaster_ngram_words, non_disaster_ngram_frequency, _, disaster_ngram_words, disaster_ngram_frequency = generate_ngram_dictionaries(ngram)\n    create_ngram_graphs(non_disaster_ngram_words, non_disaster_ngram_frequency, disaster_ngram_words, disaster_ngram_frequency, ngram)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-02T08:16:30.40538Z","iopub.execute_input":"2022-06-02T08:16:30.406088Z","iopub.status.idle":"2022-06-02T08:16:30.441282Z","shell.execute_reply.started":"2022-06-02T08:16:30.406036Z","shell.execute_reply":"2022-06-02T08:16:30.440384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Unigrams","metadata":{}},{"cell_type":"code","source":"final_output_ngram_graphs(1)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T08:16:30.442682Z","iopub.execute_input":"2022-06-02T08:16:30.442976Z","iopub.status.idle":"2022-06-02T08:16:31.376532Z","shell.execute_reply.started":"2022-06-02T08:16:30.442935Z","shell.execute_reply":"2022-06-02T08:16:31.375892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Bigrams","metadata":{}},{"cell_type":"code","source":"final_output_ngram_graphs(2)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T08:16:31.377943Z","iopub.execute_input":"2022-06-02T08:16:31.378446Z","iopub.status.idle":"2022-06-02T08:16:32.340527Z","shell.execute_reply.started":"2022-06-02T08:16:31.378408Z","shell.execute_reply":"2022-06-02T08:16:32.339866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Trigrams","metadata":{}},{"cell_type":"code","source":"final_output_ngram_graphs(3)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T08:16:32.341769Z","iopub.execute_input":"2022-06-02T08:16:32.342163Z","iopub.status.idle":"2022-06-02T08:16:33.412545Z","shell.execute_reply.started":"2022-06-02T08:16:32.342123Z","shell.execute_reply":"2022-06-02T08:16:33.411088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Packed Bubble Chart\nThe packed bubbled chart, also known as a bubble chart, is a means to show relational value without regards to axes. The bubbles are packed in as tightly as possible to make efficient use of space.","metadata":{}},{"cell_type":"code","source":"def dictionaries_for_packed_bubble_chart(ngram):\n    _, non_disaster_ngram_words_list, non_disaster_ngram_frequency_list, _, disaster_ngram_words_list, disaster_ngram_frequency_list = generate_ngram_dictionaries(ngram)\n    packed_bubble_chart_dict = {\n        'non_disaster_ngrams': non_disaster_ngram_words_list,\n        'non_disaster_ngrams_frequency': non_disaster_ngram_frequency_list,\n        'disaster_ngrams': disaster_ngram_words_list,\n        'disaster_ngrams_frequency': disaster_ngram_frequency_list,\n        'colors': ['#5A69AF', '#579E65', '#F9C784', '#FC944A', '#F24C00', '#00B825', '#FC944A', '#EF4026', '#F9C784', '#FC944A']\n    }\n    return packed_bubble_chart_dict","metadata":{"execution":{"iopub.status.busy":"2022-06-02T08:16:33.803753Z","iopub.execute_input":"2022-06-02T08:16:33.804196Z","iopub.status.idle":"2022-06-02T08:16:33.809481Z","shell.execute_reply.started":"2022-06-02T08:16:33.804157Z","shell.execute_reply":"2022-06-02T08:16:33.808651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BubbleChart:\n    \n    def __init__(self, area, bubble_spacing=0):\n\n        area = np.asarray(area)\n        r = np.sqrt(area / np.pi)\n\n        self.bubble_spacing = bubble_spacing\n        self.bubbles = np.ones((len(area), 4))\n        self.bubbles[:, 2] = r\n        self.bubbles[:, 3] = area\n        self.maxstep = 2 * self.bubbles[:, 2].max() + self.bubble_spacing\n        self.step_dist = self.maxstep / 2\n\n        length = np.ceil(np.sqrt(len(self.bubbles)))\n        grid = np.arange(length) * self.maxstep\n        gx, gy = np.meshgrid(grid, grid)\n        self.bubbles[:, 0] = gx.flatten()[:len(self.bubbles)]\n        self.bubbles[:, 1] = gy.flatten()[:len(self.bubbles)]\n\n        self.com = self.center_of_mass()\n\n    def center_of_mass(self):\n        return np.average(self.bubbles[:, :2], axis = 0, weights = self.bubbles[:, 3])\n\n    def center_distance(self, bubble, bubbles):\n        return np.hypot(bubble[0] - bubbles[:, 0], bubble[1] - bubbles[:, 1])\n\n    def outline_distance(self, bubble, bubbles):\n        center_distance = self.center_distance(bubble, bubbles)\n        return center_distance - bubble[2] - bubbles[:, 2] - self.bubble_spacing\n\n    def check_collisions(self, bubble, bubbles):\n        distance = self.outline_distance(bubble, bubbles)\n        return len(distance[distance < 0])\n\n    def collides_with(self, bubble, bubbles):\n        distance = self.outline_distance(bubble, bubbles)\n        idx_min = np.argmin(distance)\n        return idx_min if type(idx_min) == np.ndarray else [idx_min]\n\n    def collapse(self, n_iterations=50):\n\n        for _i in range(n_iterations):\n            moves = 0\n            for i in range(len(self.bubbles)):\n                rest_bub = np.delete(self.bubbles, i, 0)\n\n                dir_vec = self.com - self.bubbles[i, :2]\n\n                dir_vec = dir_vec / np.sqrt(dir_vec.dot(dir_vec))\n\n                new_point = self.bubbles[i, :2] + dir_vec * self.step_dist\n                new_bubble = np.append(new_point, self.bubbles[i, 2:4])\n\n                if not self.check_collisions(new_bubble, rest_bub):\n                    self.bubbles[i, :] = new_bubble\n                    self.com = self.center_of_mass()\n                    moves += 1\n                    \n                else:\n                    for colliding in self.collides_with(new_bubble, rest_bub):\n\n                        dir_vec = rest_bub[colliding, :2] - self.bubbles[i, :2]\n                        dir_vec = dir_vec / np.sqrt(dir_vec.dot(dir_vec))\n\n                        orth = np.array([dir_vec[1], - dir_vec[0]])\n\n                        new_point1 = (self.bubbles[i, :2] + orth * self.step_dist)\n                        new_point2 = (self.bubbles[i, :2] - orth * self.step_dist)\n                        \n                        dist1 = self.center_distance(self.com, np.array([new_point1]))\n                        dist2 = self.center_distance(self.com, np.array([new_point2]))\n                        \n                        new_point = new_point1 if dist1 < dist2 else new_point2\n                        new_bubble = np.append(new_point, self.bubbles[i, 2:4])\n                        \n                        if not self.check_collisions(new_bubble, rest_bub):\n                            self.bubbles[i, :] = new_bubble\n                            self.com = self.center_of_mass()\n\n            if moves / len(self.bubbles) < 0.1:\n                self.step_dist = self.step_dist / 2\n\n    def plot(self, ax, labels, colors):\n\n        for i in range(len(self.bubbles)):\n            circ = plt.Circle(self.bubbles[i, :2], self.bubbles[i, 2], color=colors[i])\n            ax.add_patch(circ)\n            ax.text(*self.bubbles[i, :2], labels[i], horizontalalignment = 'center', verticalalignment = 'center')\n\n\ndef final_packed_bubble_chart(ngram, displayer):\n    \n    packed_bubble_chart_dict = dictionaries_for_packed_bubble_chart(ngram)\n    \n    if(displayer == 'Non Disaster'):\n        bubble_chart = BubbleChart(area = packed_bubble_chart_dict['non_disaster_ngrams_frequency'], bubble_spacing = 0.001)\n        bubble_chart.collapse()\n        fig, ax = plt.subplots(subplot_kw = dict(aspect = 'equal'), figsize = (30, 30))\n        ax.set_facecolor('black')\n        bubble_chart.plot(ax, packed_bubble_chart_dict['non_disaster_ngrams'], colors = packed_bubble_chart_dict['colors'])\n        \n    if(displayer == 'Disaster'):\n        bubble_chart = BubbleChart(area = packed_bubble_chart_dict['disaster_ngrams_frequency'], bubble_spacing = 0.001)\n        bubble_chart.collapse()\n        fig, ax = plt.subplots(subplot_kw = dict(aspect = 'equal'), figsize = (30, 30))\n        ax.set_facecolor('black')\n        bubble_chart.plot(ax, packed_bubble_chart_dict['disaster_ngrams'], colors = packed_bubble_chart_dict['colors'])\n        \n    ax.set_title('Packed Bubble Chart for ' + displayer + ' Ngrams = ' + str(ngram), fontsize = 30)\n    ax.relim()\n    ax.autoscale_view()\n    plt.xticks([])\n    plt.yticks([])\n    if(ngram == 1):\n        plt.rcParams.update({'font.size': 30})\n    if(ngram == 2):\n        plt.rcParams.update({'font.size': 25})\n    if(ngram == 3):\n        plt.rcParams.update({'font.size': 18})\n    plt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-02T08:16:34.193782Z","iopub.execute_input":"2022-06-02T08:16:34.194206Z","iopub.status.idle":"2022-06-02T08:16:34.224354Z","shell.execute_reply.started":"2022-06-02T08:16:34.194168Z","shell.execute_reply":"2022-06-02T08:16:34.223565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_packed_bubble_chart(1, 'Non Disaster')","metadata":{"execution":{"iopub.status.busy":"2022-06-02T08:16:34.708218Z","iopub.execute_input":"2022-06-02T08:16:34.708484Z","iopub.status.idle":"2022-06-02T08:16:35.275465Z","shell.execute_reply.started":"2022-06-02T08:16:34.708447Z","shell.execute_reply":"2022-06-02T08:16:35.271956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_packed_bubble_chart(2, 'Disaster')","metadata":{"execution":{"iopub.status.busy":"2022-06-02T08:16:35.277084Z","iopub.execute_input":"2022-06-02T08:16:35.277779Z","iopub.status.idle":"2022-06-02T08:16:35.867547Z","shell.execute_reply.started":"2022-06-02T08:16:35.277738Z","shell.execute_reply":"2022-06-02T08:16:35.863466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_packed_bubble_chart(3, 'Disaster')","metadata":{"execution":{"iopub.status.busy":"2022-06-02T08:16:35.869136Z","iopub.execute_input":"2022-06-02T08:16:35.869855Z","iopub.status.idle":"2022-06-02T08:16:36.463566Z","shell.execute_reply.started":"2022-06-02T08:16:35.869818Z","shell.execute_reply":"2022-06-02T08:16:36.462945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Wordcloud of Tweets\nWord clouds (also known as text clouds or tag clouds) work in a simple way: the more a specific word appears in a source of textual data (such as a speech, blog post, or database), the bigger and bolder it appears in the word cloud. A word cloud is a collection, or cluster, of words depicted in different sizes. The bigger and bolder the word appears, the more often it‚Äôs mentioned within a given text and the more important it is.","metadata":{}},{"cell_type":"code","source":"wordcloud = WordCloud(width = 1400, height = 600, background_color = 'black').generate(''.join(text for text in df['text']))\nplt.figure(figsize = (20, 10))\nplt.title('Wordcloud Visualization of Tweets', fontsize = 30)\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-02T08:16:36.465265Z","iopub.execute_input":"2022-06-02T08:16:36.465748Z","iopub.status.idle":"2022-06-02T08:16:39.228625Z","shell.execute_reply.started":"2022-06-02T08:16:36.465709Z","shell.execute_reply":"2022-06-02T08:16:39.227997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2022-06-02T08:16:39.230127Z","iopub.execute_input":"2022-06-02T08:16:39.230474Z","iopub.status.idle":"2022-06-02T08:16:39.246858Z","shell.execute_reply.started":"2022-06-02T08:16:39.230443Z","shell.execute_reply":"2022-06-02T08:16:39.246309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.drop(columns = ['id', 'keyword', 'location'])","metadata":{"execution":{"iopub.status.busy":"2022-06-02T08:16:39.248108Z","iopub.execute_input":"2022-06-02T08:16:39.24861Z","iopub.status.idle":"2022-06-02T08:16:39.253374Z","shell.execute_reply.started":"2022-06-02T08:16:39.248574Z","shell.execute_reply":"2022-06-02T08:16:39.252793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2022-06-02T08:16:39.255204Z","iopub.execute_input":"2022-06-02T08:16:39.255738Z","iopub.status.idle":"2022-06-02T08:16:39.270461Z","shell.execute_reply.started":"2022-06-02T08:16:39.255704Z","shell.execute_reply":"2022-06-02T08:16:39.269651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('bert-large-uncased')\nbert = TFBertModel.from_pretrained('bert-large-uncased')","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-02T08:16:39.271741Z","iopub.execute_input":"2022-06-02T08:16:39.272208Z","iopub.status.idle":"2022-06-02T08:17:55.857178Z","shell.execute_reply.started":"2022-06-02T08:16:39.272171Z","shell.execute_reply":"2022-06-02T08:17:55.856422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = tokenizer(\n    text = df['text'].tolist(),\n    add_special_tokens = True,\n    max_length = 36, \n    truncation = True,\n    padding = True, \n    return_tensors = 'tf',\n    return_attention_mask = True,\n    verbose = True)\n\nX_test = tokenizer(\n    text = test_df['text'].tolist(),\n    add_special_tokens = True,\n    max_length = 36, \n    truncation = True,\n    padding = True, \n    return_tensors = 'tf',\n    return_attention_mask = True,\n    verbose = True)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T08:17:55.858594Z","iopub.execute_input":"2022-06-02T08:17:55.859054Z","iopub.status.idle":"2022-06-02T08:17:56.544009Z","shell.execute_reply.started":"2022-06-02T08:17:55.859011Z","shell.execute_reply":"2022-06-02T08:17:56.543271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train)\nprint(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T08:17:56.54532Z","iopub.execute_input":"2022-06-02T08:17:56.545584Z","iopub.status.idle":"2022-06-02T08:17:56.555375Z","shell.execute_reply.started":"2022-06-02T08:17:56.545548Z","shell.execute_reply":"2022-06-02T08:17:56.554596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train['input_ids'].shape)\nprint(X_train['attention_mask'].shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T08:17:56.557916Z","iopub.execute_input":"2022-06-02T08:17:56.558252Z","iopub.status.idle":"2022-06-02T08:17:56.564999Z","shell.execute_reply.started":"2022-06-02T08:17:56.558214Z","shell.execute_reply":"2022-06-02T08:17:56.564162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = df['target'].values\ny_train","metadata":{"execution":{"iopub.status.busy":"2022-06-02T08:17:56.566871Z","iopub.execute_input":"2022-06-02T08:17:56.567363Z","iopub.status.idle":"2022-06-02T08:17:56.575349Z","shell.execute_reply.started":"2022-06-02T08:17:56.567328Z","shell.execute_reply":"2022-06-02T08:17:56.574427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating the Model","metadata":{}},{"cell_type":"code","source":"input_ids = Input(shape=(36,), dtype=tf.int32, name = 'input_ids')\nattention_mask = Input(shape=(36,), dtype=tf.int32, name = 'attention_mask')\n\nembeddings = bert(input_ids = input_ids, attention_mask = attention_mask)[0]\nlayer = layers.Dropout(0.2)(embeddings)\nlayer = layers.Dense(1024, activation = 'relu')(layer)\nlayer = layers.Dense(32, activation = 'relu')(layer)\nlayer = layers.Flatten()(layer)\ny = layers.Dense(1, activation = 'sigmoid')(layer)\n    \nmodel = keras.Model(inputs = [input_ids, attention_mask], outputs = y)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T08:17:56.577066Z","iopub.execute_input":"2022-06-02T08:17:56.577682Z","iopub.status.idle":"2022-06-02T08:18:04.658797Z","shell.execute_reply.started":"2022-06-02T08:17:56.577644Z","shell.execute_reply":"2022-06-02T08:18:04.658064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-06-02T08:18:04.660022Z","iopub.execute_input":"2022-06-02T08:18:04.660275Z","iopub.status.idle":"2022-06-02T08:18:04.703312Z","shell.execute_reply.started":"2022-06-02T08:18:04.660242Z","shell.execute_reply":"2022-06-02T08:18:04.702524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(\n    optimizer = keras.optimizers.Adam(learning_rate = 6e-6, epsilon = 1e-8, decay = 0.01, clipnorm = 1.0),\n    loss = BinaryCrossentropy(from_logits = True), \n    metrics = ['accuracy']\n)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T08:18:04.704614Z","iopub.execute_input":"2022-06-02T08:18:04.704968Z","iopub.status.idle":"2022-06-02T08:18:04.732767Z","shell.execute_reply.started":"2022-06-02T08:18:04.704927Z","shell.execute_reply":"2022-06-02T08:18:04.732025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model(model, show_shapes = True)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T08:18:36.929471Z","iopub.execute_input":"2022-06-02T08:18:36.929755Z","iopub.status.idle":"2022-06-02T08:18:38.12885Z","shell.execute_reply.started":"2022-06-02T08:18:36.929723Z","shell.execute_reply":"2022-06-02T08:18:38.128038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training the Model","metadata":{}},{"cell_type":"code","source":"classifier = model.fit(\n    x = {'input_ids': X_train['input_ids'],\n         'attention_mask': X_train['attention_mask']\n        },\n    y = y_train,\n    validation_split = 0.05,\n    epochs = 5,\n    batch_size = 32\n)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-02T08:19:34.759828Z","iopub.execute_input":"2022-06-02T08:19:34.760173Z","iopub.status.idle":"2022-06-02T08:29:23.355921Z","shell.execute_reply.started":"2022-06-02T08:19:34.760137Z","shell.execute_reply":"2022-06-02T08:29:23.355137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Performance","metadata":{}},{"cell_type":"code","source":"def model_performance_graphs():\n    \n    fig, axes = plt.subplots(1, 2, figsize = (15, 8))\n\n    axes[0].plot(classifier.epoch, classifier.history['accuracy'], label = 'acc')\n    axes[0].plot(classifier.epoch, classifier.history['val_accuracy'], label = 'val_acc')\n    axes[0].set_title('Accuracy vs Epochs', fontsize = 20)\n    axes[0].set_xlabel('Epochs', fontsize = 15)\n    axes[0].set_ylabel('Accuracy', fontsize = 15)\n    axes[0].legend()\n\n    axes[1].plot(classifier.epoch, classifier.history['loss'], label = 'loss')\n    axes[1].plot(classifier.epoch, classifier.history['val_loss'], label=\"val_loss\")\n    axes[1].set_title(\"Loss Curve\",fontsize=18)\n    axes[1].set_xlabel(\"Epochs\",fontsize=15)\n    axes[1].set_ylabel(\"Loss\",fontsize=15)\n    axes[1].legend()\n\n    plt.show()\n    \nmodel_performance_graphs()","metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"execution":{"iopub.status.busy":"2022-06-02T08:29:23.358652Z","iopub.execute_input":"2022-06-02T08:29:23.359052Z","iopub.status.idle":"2022-06-02T08:29:23.699671Z","shell.execute_reply.started":"2022-06-02T08:29:23.35901Z","shell.execute_reply":"2022-06-02T08:29:23.69893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Making the Predictions","metadata":{}},{"cell_type":"code","source":"test_df = test_df[['id', 'text']]\npred = model.predict({'input_ids': X_test['input_ids'],\n                      'attention_mask': X_test['attention_mask']})\nprint(pred)\npred = tf.squeeze(tf.round((pred)))\nprint(np.array(pred))","metadata":{"execution":{"iopub.status.busy":"2022-06-02T08:29:23.701002Z","iopub.execute_input":"2022-06-02T08:29:23.701424Z","iopub.status.idle":"2022-06-02T08:29:47.416583Z","shell.execute_reply.started":"2022-06-02T08:29:23.701383Z","shell.execute_reply":"2022-06-02T08:29:47.415778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating the Submission File","metadata":{}},{"cell_type":"code","source":"test_df['target'] = pred\ntest_df['target'] = test_df['target'].astype(int)\ntest_df = test_df[['id', 'target']]\ntest_df.to_csv('submission.csv', index = False)\ntest_df","metadata":{"execution":{"iopub.status.busy":"2022-06-02T08:29:47.418316Z","iopub.execute_input":"2022-06-02T08:29:47.418914Z","iopub.status.idle":"2022-06-02T08:29:47.442465Z","shell.execute_reply.started":"2022-06-02T08:29:47.418874Z","shell.execute_reply":"2022-06-02T08:29:47.441789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# References\n> - https://www.kaggle.com/code/gunesevitan/nlp-with-disaster-tweets-eda-cleaning-and-bert#2.-Meta-Features\n> - https://www.kaggle.com/code/shahules/basic-eda-cleaning-and-glove\n> - https://www.kaggle.com/code/sudalairajkumar/getting-started-with-text-preprocessing/notebook\n> - https://www.kaggle.com/code/aishwarya2210/prediction-of-tweets-using-bert-model\n> - https://www.kaggle.com/code/xhlulu/disaster-nlp-keras-bert-using-tfhub\n> - https://www.kaggle.com/code/ratan123/start-from-here-disaster-tweets-eda-basic-model#4.-Exploring-location-column","metadata":{"execution":{"iopub.status.busy":"2022-04-25T02:12:28.561622Z","iopub.execute_input":"2022-04-25T02:12:28.562335Z","iopub.status.idle":"2022-04-25T02:12:28.569901Z","shell.execute_reply.started":"2022-04-25T02:12:28.562279Z","shell.execute_reply":"2022-04-25T02:12:28.568876Z"}}},{"cell_type":"markdown","source":"<div class=\"alert alert-warning\" role=\"alert\">üöß Work in Progress üöß</div>","metadata":{}}]}