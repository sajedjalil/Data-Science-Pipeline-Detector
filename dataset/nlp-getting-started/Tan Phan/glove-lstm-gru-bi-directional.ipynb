{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-24T22:57:40.547905Z","iopub.execute_input":"2021-10-24T22:57:40.548216Z","iopub.status.idle":"2021-10-24T22:57:40.574283Z","shell.execute_reply.started":"2021-10-24T22:57:40.548141Z","shell.execute_reply":"2021-10-24T22:57:40.573538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import spacy\nimport pandas as pd\nimport numpy as np\nimport re\nimport string\nimport timeit\nimport tensorflow as tf\nimport tensorflow_hub as hub\n# Cleaning Data\nfrom keras.preprocessing.text import Tokenizer\nfrom nltk import corpus, SnowballStemmer, word_tokenize\n# !pip install pyspellchecker\n# from spellchecker import SpellChecker\nfrom tqdm import tqdm\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Preprocessing and Modelling\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Embedding, SimpleRNN, Dense, Bidirectional, GRU, LSTM, SpatialDropout1D\nfrom tensorflow.compat.v1.keras.layers import CuDNNLSTM\nfrom keras.models import Sequential\n","metadata":{"execution":{"iopub.status.busy":"2021-10-24T23:07:25.289277Z","iopub.execute_input":"2021-10-24T23:07:25.289674Z","iopub.status.idle":"2021-10-24T23:07:25.297327Z","shell.execute_reply.started":"2021-10-24T23:07:25.289639Z","shell.execute_reply":"2021-10-24T23:07:25.296356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=0></a>\n## <p style=\"background-color:lightblue; font-family:newtimeroman; font-size:120%; text-align:left; border-radius: 15px 50px;\">Table of Content</p>\n\n* [1. Loading Data 游눑](#1)\n* [2. EDA 游늵](#2)\n* [3. Data Preprocessing](#3)\n    * [3.1 Remove corpus](#3.1)\n    * [3.2 Remove Stemming](#3.2)\n* [4. Vectorization](#4)\n    * [4.1 Common Vectorizer Usage](#4.1)\n    * [4.2 If-Idf Term Weightings](#4.2)\n* [5. Model](#5)\n    * [5.1. GloVe for Vectorization](#5.1)\n        * [5.1.1 Preparing the data](#5.1.1)\n        * [5.1.2 The First recurrent Baseline with 'SimpleRNN' layer](#5.1.2)\n        * [5.1.3 'Bidirectional' and 'LSTM' layer](#5.1.3)\n        * [5.1.4 'Bidirectional' and GRU layer](#5.1.4)\n        * [5.1.5 SpatialDropout1D + Bidirectional + LSTM layer](#5.1.5) \n        * [5.1.6 Bidirectional + double LSTM layer](#5.1.6)\n* [Make a Submission](#10)","metadata":{}},{"cell_type":"markdown","source":"<a id='1'></a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">1. Loading Data 游눑</p>\n\nJust load the dataset and global variables for colors and so on.\n\n[Content](#0)","metadata":{}},{"cell_type":"code","source":"train_full = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest_full = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\ndf = pd.concat([train_full, test_full])\n\ntrain_len = train_full.shape[0]\n\nprint('Training Set Shape = {}'.format(train_full.shape))\nprint('Training Set Memory Usage = {:.2f}MB'.format(train_full.memory_usage().sum()/2**20))\n\nprint('Test Set Shape = {}'.format(test_full.shape))\nprint('Test Set Memory Usage = {:.2f}MB'.format(test_full.memory_usage().sum()/2**20))","metadata":{"execution":{"iopub.status.busy":"2021-10-24T22:57:56.583236Z","iopub.execute_input":"2021-10-24T22:57:56.583838Z","iopub.status.idle":"2021-10-24T22:57:56.680099Z","shell.execute_reply.started":"2021-10-24T22:57:56.583797Z","shell.execute_reply":"2021-10-24T22:57:56.679126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check Unbalancing Data\ntrain_full.groupby('target').describe()","metadata":{"execution":{"iopub.status.busy":"2021-10-24T22:57:56.682805Z","iopub.execute_input":"2021-10-24T22:57:56.683059Z","iopub.status.idle":"2021-10-24T22:57:56.728866Z","shell.execute_reply.started":"2021-10-24T22:57:56.683034Z","shell.execute_reply":"2021-10-24T22:57:56.727841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using to plot model that be trained\ndef plot_model(history):\n    acc = history['acc'] if 'acc' in history.keys() else history['accuracy']\n    val_acc = history['val_acc'] if 'val_acc' in history.keys() else history['val_accuracy']\n    loss = history['loss'] \n    val_loss = history['val_loss'] \n    epochs = range(len(acc))\n\n    plt.plot(epochs, acc, 'bo', label='Training acc')\n    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n    plt.title('Training and validation accuracy')\n    plt.legend()\n\n    plt.figure()\n\n    plt.plot(epochs, loss, 'bo', label='Training loss')\n    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.legend()\n\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-24T22:57:56.730282Z","iopub.execute_input":"2021-10-24T22:57:56.730647Z","iopub.status.idle":"2021-10-24T22:57:56.737945Z","shell.execute_reply.started":"2021-10-24T22:57:56.73061Z","shell.execute_reply":"2021-10-24T22:57:56.736999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='2'></a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">2. EDA 游늵</p>\n\nNow we are going to take a look about the target distribution and the messages length.\n\n[Content](#0)","metadata":{}},{"cell_type":"markdown","source":"# Class Distribution\n","metadata":{}},{"cell_type":"code","source":"# Counting the number of meaning tweet (1 is real disaster, 0 is no disaster)\nx = train_full.target.value_counts()\n# Display it\nsns.barplot(x.index, x)\nplt.gca().set_ylabel('Number of tweets')","metadata":{"execution":{"iopub.status.busy":"2021-10-24T22:57:56.739466Z","iopub.execute_input":"2021-10-24T22:57:56.740048Z","iopub.status.idle":"2021-10-24T22:57:56.880779Z","shell.execute_reply.started":"2021-10-24T22:57:56.740012Z","shell.execute_reply":"2021-10-24T22:57:56.879952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Number of characters in tweets","metadata":{}},{"cell_type":"code","source":"# Define the default histogram chart\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(25,10))\nfig.suptitle(\"Characters in Tweets\", color=\"Blue\", size=30)\n# Disaster Chart\ndisaster_len = train_full[train_full.target==1].text.str.len()\nax1.hist(disaster_len, color='red')\nax1.set_title(\"Disaster Tweets\")\nax1.set_xlabel(\"Tweet Character Length\")\nax1.set_ylabel(\"Quantity\")\n# No Disaster Chart\nnodisaster_len = train_full[train_full.target==0].text.str.len()\nax2.hist(nodisaster_len, color='blue')\nax2.set_title(\"No Disaster Tweets\")\nax2.set_xlabel(\"Tweet Character Length\")\nax2.set_ylabel(\"Quantity\")\n# Display charts\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-24T22:57:56.883307Z","iopub.execute_input":"2021-10-24T22:57:56.883671Z","iopub.status.idle":"2021-10-24T22:57:57.270561Z","shell.execute_reply.started":"2021-10-24T22:57:56.883633Z","shell.execute_reply":"2021-10-24T22:57:57.269671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Range from 120 to 140 characters is the most common in tweet.","metadata":{}},{"cell_type":"markdown","source":"# Number of Words in a tweet","metadata":{}},{"cell_type":"code","source":"# Define the defaults histogram\nfig, (ax1,ax2) = plt.subplots(1,2, figsize=(25,10))\nfig.suptitle(\"Words in Tweet\", size=30, color=\"Blue\")\n# Disaster chart\nDisasterWord_len=train_full[train_full.target==1].text.str.split().map(lambda x: len(x))\nax1.hist(DisasterWord_len, color='Red')\nax1.set_title(\"Disaster Tweets\")\nax1.set_xlabel(\"Tweet Word Length\")\nax1.set_ylabel(\"Quantity\")\n# No Disaster chart\nNoDisasterWord_len = train_full[train_full.target==0].text.str.split().map(lambda x: len(x))\nax2.hist(NoDisasterWord_len, color='Green')\nax2.set_title(\"No Disaster Tweets\")\nax2.set_xlabel(\"Tweet Word Length\")\nax2.set_ylabel(\"Quantity\")\n# Display Histogram\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-24T22:57:57.272537Z","iopub.execute_input":"2021-10-24T22:57:57.273063Z","iopub.status.idle":"2021-10-24T22:57:57.771654Z","shell.execute_reply.started":"2021-10-24T22:57:57.273022Z","shell.execute_reply":"2021-10-24T22:57:57.770731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the default plots\nfig, (ax1, ax2) = plt.subplots(1,2,figsize=(25,10))\nfig.suptitle(\"Average Word Length in a Tweet\", color=\"blue\", size=30)\n# Disaster Plot\nDisasterAvgWord_len = train_full[train_full.target==1].text.str.split().apply(lambda x: [len(i) for i in x])\nsns.distplot(DisasterAvgWord_len.map(lambda x: np.mean(x)), ax=ax1, color='red')\nax1.set_title(\"Disaster Tweets\")\n# Distribution Plots\nNoDisasterAvgWord_len= train_full[train_full.target==0].text.str.split().apply(lambda x: [len(i) for i in x])\nsns.distplot(NoDisasterAvgWord_len.map(lambda x: np.mean(x)), ax=ax2, color=\"green\")\nax2.set_title(\"No Disaster Tweets\")\n# Display Plots\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-24T22:57:57.773447Z","iopub.execute_input":"2021-10-24T22:57:57.773841Z","iopub.status.idle":"2021-10-24T22:57:58.817156Z","shell.execute_reply.started":"2021-10-24T22:57:57.773799Z","shell.execute_reply":"2021-10-24T22:57:58.816322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='3'></a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">3. Data Pre-processing </p>\n\nNow we are going to engineering the data to make it easier for the model to clasiffy.\n\nThis section is very important to reduce the dimensions of the problem.\n\n\n[Content](#0)","metadata":{}},{"cell_type":"markdown","source":"<a id=3.1 ></a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:left; border-radius: 15px 50px;\">3.1 Cleaning the corpus</p>\n\n\n[Content](#0)","metadata":{}},{"cell_type":"code","source":"emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n# Collect all clean-corpus technics into a function\ndef clean_text(tweet):\n    \n    tweet = str(tweet).lower() # Make text lowercase\n    tweet = re.sub('\\[.*?\\]', '', tweet)\n    tweet = re.sub('https?://\\S+|www\\.\\S+', '', tweet)\n    tweet = re.sub('<.*?>+', '', tweet)\n    tweet = re.sub('[%s]' % re.escape(string.punctuation), '', tweet) # Remove Punctuation\n    tweet = re.sub('\\n', '', tweet)\n    tweet = re.sub('\\w*\\d\\w*', '', tweet)\n    \n    # Remove Emoji\n    tweet = emoji_pattern.sub(r'', tweet)\n    # Special characters\n    tweet = re.sub(r\"\\x89칕_\", \"\", tweet)\n    tweet = re.sub(r\"\\x89칕뇦", \"\", tweet)\n    tweet = re.sub(r\"\\x89칕칍\", \"\", tweet)\n    tweet = re.sub(r\"\\x89칕칊When\", \"When\", tweet)\n    tweet = re.sub(r\"\\x89칕칊\", \"\", tweet)\n    tweet = re.sub(r\"China\\x89칕춹s\", \"China's\", tweet)\n    tweet = re.sub(r\"let\\x89칕춹s\", \"let's\", tweet)\n    tweet = re.sub(r\"\\x89칕칭\", \"\", tweet)\n    tweet = re.sub(r\"\\x89칕춹\", \"\", tweet)\n    tweet = re.sub(r\"\\x89칕\\x9d\", \"\", tweet)\n    tweet = re.sub(r\"친_\", \"\", tweet)\n    tweet = re.sub(r\"\\x89칕춱\", \"\", tweet)\n    tweet = re.sub(r\"\\x89칕춱친칅\", \"\", tweet)\n    tweet = re.sub(r\"from친칅wounds\", \"from wounds\", tweet)\n    tweet = re.sub(r\"친칅\", \"\", tweet)\n    tweet = re.sub(r\"친칃\", \"\", tweet)\n    tweet = re.sub(r\"Jap칇_n\", \"Japan\", tweet)    \n    tweet = re.sub(r\"칇춸\", \"e\", tweet)\n    tweet = re.sub(r\"친춷\", \"\", tweet)\n    tweet = re.sub(r\"Suru칇춳\", \"Suruc\", tweet)\n    tweet = re.sub(r\"친칂\", \"\", tweet)\n    tweet = re.sub(r\"친춲3million\", \"3 million\", tweet)\n    tweet = re.sub(r\"친\", \"\", tweet)\n    \n    # Contractions\n    tweet = re.sub(r\"he's\", \"he is\", tweet)\n    tweet = re.sub(r\"there's\", \"there is\", tweet)\n    tweet = re.sub(r\"We're\", \"We are\", tweet)\n    tweet = re.sub(r\"That's\", \"That is\", tweet)\n    tweet = re.sub(r\"won't\", \"will not\", tweet)\n    tweet = re.sub(r\"they're\", \"they are\", tweet)\n    tweet = re.sub(r\"Can't\", \"Cannot\", tweet)\n    tweet = re.sub(r\"wasn't\", \"was not\", tweet)\n    tweet = re.sub(r\"don\\x89칕춹t\", \"do not\", tweet)\n    tweet = re.sub(r\"aren't\", \"are not\", tweet)\n    tweet = re.sub(r\"isn't\", \"is not\", tweet)\n    tweet = re.sub(r\"What's\", \"What is\", tweet)\n    tweet = re.sub(r\"haven't\", \"have not\", tweet)\n    tweet = re.sub(r\"hasn't\", \"has not\", tweet)\n    tweet = re.sub(r\"There's\", \"There is\", tweet)\n    tweet = re.sub(r\"He's\", \"He is\", tweet)\n    tweet = re.sub(r\"It's\", \"It is\", tweet)\n    tweet = re.sub(r\"You're\", \"You are\", tweet)\n    tweet = re.sub(r\"I'M\", \"I am\", tweet)\n    tweet = re.sub(r\"shouldn't\", \"should not\", tweet)\n    tweet = re.sub(r\"wouldn't\", \"would not\", tweet)\n    tweet = re.sub(r\"i'm\", \"I am\", tweet)\n    tweet = re.sub(r\"I\\x89칕춹m\", \"I am\", tweet)\n    tweet = re.sub(r\"I'm\", \"I am\", tweet)\n    tweet = re.sub(r\"Isn't\", \"is not\", tweet)\n    tweet = re.sub(r\"Here's\", \"Here is\", tweet)\n    tweet = re.sub(r\"you've\", \"you have\", tweet)\n    tweet = re.sub(r\"you\\x89칕춹ve\", \"you have\", tweet)\n    tweet = re.sub(r\"we're\", \"we are\", tweet)\n    tweet = re.sub(r\"what's\", \"what is\", tweet)\n    tweet = re.sub(r\"couldn't\", \"could not\", tweet)\n    tweet = re.sub(r\"we've\", \"we have\", tweet)\n    tweet = re.sub(r\"it\\x89칕춹s\", \"it is\", tweet)\n    tweet = re.sub(r\"doesn\\x89칕춹t\", \"does not\", tweet)\n    tweet = re.sub(r\"It\\x89칕춹s\", \"It is\", tweet)\n    tweet = re.sub(r\"Here\\x89칕춹s\", \"Here is\", tweet)\n    tweet = re.sub(r\"who's\", \"who is\", tweet)\n    tweet = re.sub(r\"I\\x89칕춹ve\", \"I have\", tweet)\n    tweet = re.sub(r\"y'all\", \"you all\", tweet)\n    tweet = re.sub(r\"can\\x89칕춹t\", \"cannot\", tweet)\n    tweet = re.sub(r\"would've\", \"would have\", tweet)\n    tweet = re.sub(r\"it'll\", \"it will\", tweet)\n    tweet = re.sub(r\"we'll\", \"we will\", tweet)\n    tweet = re.sub(r\"wouldn\\x89칕춹t\", \"would not\", tweet)\n    tweet = re.sub(r\"We've\", \"We have\", tweet)\n    tweet = re.sub(r\"he'll\", \"he will\", tweet)\n    tweet = re.sub(r\"Y'all\", \"You all\", tweet)\n    tweet = re.sub(r\"Weren't\", \"Were not\", tweet)\n    tweet = re.sub(r\"Didn't\", \"Did not\", tweet)\n    tweet = re.sub(r\"they'll\", \"they will\", tweet)\n    tweet = re.sub(r\"they'd\", \"they would\", tweet)\n    tweet = re.sub(r\"DON'T\", \"DO NOT\", tweet)\n    tweet = re.sub(r\"That\\x89칕춹s\", \"That is\", tweet)\n    tweet = re.sub(r\"they've\", \"they have\", tweet)\n    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n    tweet = re.sub(r\"should've\", \"should have\", tweet)\n    tweet = re.sub(r\"You\\x89칕춹re\", \"You are\", tweet)\n    tweet = re.sub(r\"where's\", \"where is\", tweet)\n    tweet = re.sub(r\"Don\\x89칕춹t\", \"Do not\", tweet)\n    tweet = re.sub(r\"we'd\", \"we would\", tweet)\n    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n    tweet = re.sub(r\"weren't\", \"were not\", tweet)\n    tweet = re.sub(r\"They're\", \"They are\", tweet)\n    tweet = re.sub(r\"Can\\x89칕춹t\", \"Cannot\", tweet)\n    tweet = re.sub(r\"you\\x89칕춹ll\", \"you will\", tweet)\n    tweet = re.sub(r\"I\\x89칕춹d\", \"I would\", tweet)\n    tweet = re.sub(r\"let's\", \"let us\", tweet)\n    tweet = re.sub(r\"it's\", \"it is\", tweet)\n    tweet = re.sub(r\"can't\", \"cannot\", tweet)\n    tweet = re.sub(r\"don't\", \"do not\", tweet)\n    tweet = re.sub(r\"you're\", \"you are\", tweet)\n    tweet = re.sub(r\"i've\", \"I have\", tweet)\n    tweet = re.sub(r\"that's\", \"that is\", tweet)\n    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n    tweet = re.sub(r\"doesn't\", \"does not\", tweet)\n    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n    tweet = re.sub(r\"didn't\", \"did not\", tweet)\n    tweet = re.sub(r\"ain't\", \"am not\", tweet)\n    tweet = re.sub(r\"you'll\", \"you will\", tweet)\n    tweet = re.sub(r\"I've\", \"I have\", tweet)\n    tweet = re.sub(r\"Don't\", \"do not\", tweet)\n    tweet = re.sub(r\"I'll\", \"I will\", tweet)\n    tweet = re.sub(r\"I'd\", \"I would\", tweet)\n    tweet = re.sub(r\"Let's\", \"Let us\", tweet)\n    tweet = re.sub(r\"you'd\", \"You would\", tweet)\n    tweet = re.sub(r\"It's\", \"It is\", tweet)\n    tweet = re.sub(r\"Ain't\", \"am not\", tweet)\n    tweet = re.sub(r\"Haven't\", \"Have not\", tweet)\n    tweet = re.sub(r\"Could've\", \"Could have\", tweet)\n    tweet = re.sub(r\"youve\", \"you have\", tweet)  \n    tweet = re.sub(r\"don친춺t\", \"do not\", tweet)   \n    \n    # Character entity references\n    tweet = re.sub(r\"&gt;\", \">\", tweet)\n    tweet = re.sub(r\"&lt;\", \"<\", tweet)\n    tweet = re.sub(r\"&amp;\", \"&\", tweet)\n    \n    # Typos, slang and informal abbreviations\n    tweet = re.sub(r\"w/e\", \"whatever\", tweet)\n    tweet = re.sub(r\"w/\", \"with\", tweet)\n    tweet = re.sub(r\"USAgov\", \"USA government\", tweet)\n    tweet = re.sub(r\"recentlu\", \"recently\", tweet)\n    tweet = re.sub(r\"Ph0tos\", \"Photos\", tweet)\n    tweet = re.sub(r\"amirite\", \"am I right\", tweet)\n    tweet = re.sub(r\"exp0sed\", \"exposed\", tweet)\n    tweet = re.sub(r\"<3\", \"love\", tweet)\n    tweet = re.sub(r\"amageddon\", \"armageddon\", tweet)\n    tweet = re.sub(r\"Trfc\", \"Traffic\", tweet)\n    tweet = re.sub(r\"8/5/2015\", \"2015-08-05\", tweet)\n    tweet = re.sub(r\"WindStorm\", \"Wind Storm\", tweet)\n    tweet = re.sub(r\"8/6/2015\", \"2015-08-06\", tweet)\n    tweet = re.sub(r\"10:38PM\", \"10:38 PM\", tweet)\n    tweet = re.sub(r\"10:30pm\", \"10:30 PM\", tweet)\n    tweet = re.sub(r\"16yr\", \"16 year\", tweet)\n    tweet = re.sub(r\"lmao\", \"laughing my ass off\", tweet)   \n    tweet = re.sub(r\"TRAUMATISED\", \"traumatized\", tweet)\n    \n    # Hashtags and usernames\n    tweet = re.sub(r\"IranDeal\", \"Iran Deal\", tweet)\n    tweet = re.sub(r\"ArianaGrande\", \"Ariana Grande\", tweet)\n    tweet = re.sub(r\"camilacabello97\", \"camila cabello\", tweet) \n    tweet = re.sub(r\"RondaRousey\", \"Ronda Rousey\", tweet)     \n    tweet = re.sub(r\"MTVHottest\", \"MTV Hottest\", tweet)\n    tweet = re.sub(r\"TrapMusic\", \"Trap Music\", tweet)\n    tweet = re.sub(r\"ProphetMuhammad\", \"Prophet Muhammad\", tweet)\n    tweet = re.sub(r\"PantherAttack\", \"Panther Attack\", tweet)\n    tweet = re.sub(r\"StrategicPatience\", \"Strategic Patience\", tweet)\n    tweet = re.sub(r\"socialnews\", \"social news\", tweet)\n    tweet = re.sub(r\"NASAHurricane\", \"NASA Hurricane\", tweet)\n    tweet = re.sub(r\"onlinecommunities\", \"online communities\", tweet)\n    tweet = re.sub(r\"humanconsumption\", \"human consumption\", tweet)\n    tweet = re.sub(r\"Typhoon-Devastated\", \"Typhoon Devastated\", tweet)\n    tweet = re.sub(r\"Meat-Loving\", \"Meat Loving\", tweet)\n    tweet = re.sub(r\"facialabuse\", \"facial abuse\", tweet)\n    tweet = re.sub(r\"LakeCounty\", \"Lake County\", tweet)\n    tweet = re.sub(r\"BeingAuthor\", \"Being Author\", tweet)\n    tweet = re.sub(r\"withheavenly\", \"with heavenly\", tweet)\n    tweet = re.sub(r\"thankU\", \"thank you\", tweet)\n    tweet = re.sub(r\"iTunesMusic\", \"iTunes Music\", tweet)\n    tweet = re.sub(r\"OffensiveContent\", \"Offensive Content\", tweet)\n    tweet = re.sub(r\"WorstSummerJob\", \"Worst Summer Job\", tweet)\n    tweet = re.sub(r\"HarryBeCareful\", \"Harry Be Careful\", tweet)\n    tweet = re.sub(r\"NASASolarSystem\", \"NASA Solar System\", tweet)\n    tweet = re.sub(r\"animalrescue\", \"animal rescue\", tweet)\n    tweet = re.sub(r\"KurtSchlichter\", \"Kurt Schlichter\", tweet)\n    tweet = re.sub(r\"aRmageddon\", \"armageddon\", tweet)\n    tweet = re.sub(r\"Throwingknifes\", \"Throwing knives\", tweet)\n    tweet = re.sub(r\"GodsLove\", \"God's Love\", tweet)\n    tweet = re.sub(r\"bookboost\", \"book boost\", tweet)\n    tweet = re.sub(r\"ibooklove\", \"I book love\", tweet)\n    tweet = re.sub(r\"NestleIndia\", \"Nestle India\", tweet)\n    tweet = re.sub(r\"realDonaldTrump\", \"Donald Trump\", tweet)\n    tweet = re.sub(r\"DavidVonderhaar\", \"David Vonderhaar\", tweet)\n    tweet = re.sub(r\"CecilTheLion\", \"Cecil The Lion\", tweet)\n    tweet = re.sub(r\"weathernetwork\", \"weather network\", tweet)\n    tweet = re.sub(r\"withBioterrorism&use\", \"with Bioterrorism & use\", tweet)\n    tweet = re.sub(r\"Hostage&2\", \"Hostage & 2\", tweet)\n    tweet = re.sub(r\"GOPDebate\", \"GOP Debate\", tweet)\n    tweet = re.sub(r\"RickPerry\", \"Rick Perry\", tweet)\n    tweet = re.sub(r\"frontpage\", \"front page\", tweet)\n    tweet = re.sub(r\"NewsInTweets\", \"News In Tweets\", tweet)\n    tweet = re.sub(r\"ViralSpell\", \"Viral Spell\", tweet)\n    tweet = re.sub(r\"til_now\", \"until now\", tweet)\n    tweet = re.sub(r\"volcanoinRussia\", \"volcano in Russia\", tweet)\n    tweet = re.sub(r\"ZippedNews\", \"Zipped News\", tweet)\n    tweet = re.sub(r\"MicheleBachman\", \"Michele Bachman\", tweet)\n    tweet = re.sub(r\"53inch\", \"53 inch\", tweet)\n    tweet = re.sub(r\"KerrickTrial\", \"Kerrick Trial\", tweet)\n    tweet = re.sub(r\"abstorm\", \"Alberta Storm\", tweet)\n    tweet = re.sub(r\"Beyhive\", \"Beyonce hive\", tweet)\n    tweet = re.sub(r\"IDFire\", \"Idaho Fire\", tweet)\n    tweet = re.sub(r\"DETECTADO\", \"Detected\", tweet)\n    tweet = re.sub(r\"RockyFire\", \"Rocky Fire\", tweet)\n    tweet = re.sub(r\"Listen/Buy\", \"Listen / Buy\", tweet)\n    tweet = re.sub(r\"NickCannon\", \"Nick Cannon\", tweet)\n    tweet = re.sub(r\"FaroeIslands\", \"Faroe Islands\", tweet)\n    tweet = re.sub(r\"yycstorm\", \"Calgary Storm\", tweet)\n    tweet = re.sub(r\"IDPs:\", \"Internally Displaced People :\", tweet)\n    tweet = re.sub(r\"ArtistsUnited\", \"Artists United\", tweet)\n    tweet = re.sub(r\"ClaytonBryant\", \"Clayton Bryant\", tweet)\n    tweet = re.sub(r\"jimmyfallon\", \"jimmy fallon\", tweet)\n    tweet = re.sub(r\"justinbieber\", \"justin bieber\", tweet)  \n    tweet = re.sub(r\"UTC2015\", \"UTC 2015\", tweet)\n    tweet = re.sub(r\"Time2015\", \"Time 2015\", tweet)\n    tweet = re.sub(r\"djicemoon\", \"dj icemoon\", tweet)\n    tweet = re.sub(r\"LivingSafely\", \"Living Safely\", tweet)\n    tweet = re.sub(r\"FIFA16\", \"Fifa 2016\", tweet)\n    tweet = re.sub(r\"thisiswhywecanthavenicethings\", \"this is why we cannot have nice things\", tweet)\n    tweet = re.sub(r\"bbcnews\", \"bbc news\", tweet)\n    tweet = re.sub(r\"UndergroundRailraod\", \"Underground Railraod\", tweet)\n    tweet = re.sub(r\"c4news\", \"c4 news\", tweet)\n    tweet = re.sub(r\"OBLITERATION\", \"obliteration\", tweet)\n    tweet = re.sub(r\"MUDSLIDE\", \"mudslide\", tweet)\n    tweet = re.sub(r\"NoSurrender\", \"No Surrender\", tweet)\n    tweet = re.sub(r\"NotExplained\", \"Not Explained\", tweet)\n    tweet = re.sub(r\"greatbritishbakeoff\", \"great british bake off\", tweet)\n    tweet = re.sub(r\"LondonFire\", \"London Fire\", tweet)\n    tweet = re.sub(r\"KOTAWeather\", \"KOTA Weather\", tweet)\n    tweet = re.sub(r\"LuchaUnderground\", \"Lucha Underground\", tweet)\n    tweet = re.sub(r\"KOIN6News\", \"KOIN 6 News\", tweet)\n    tweet = re.sub(r\"LiveOnK2\", \"Live On K2\", tweet)\n    tweet = re.sub(r\"9NewsGoldCoast\", \"9 News Gold Coast\", tweet)\n    tweet = re.sub(r\"nikeplus\", \"nike plus\", tweet)\n    tweet = re.sub(r\"david_cameron\", \"David Cameron\", tweet)\n    tweet = re.sub(r\"peterjukes\", \"Peter Jukes\", tweet)\n    tweet = re.sub(r\"JamesMelville\", \"James Melville\", tweet)\n    tweet = re.sub(r\"megynkelly\", \"Megyn Kelly\", tweet)\n    tweet = re.sub(r\"cnewslive\", \"C News Live\", tweet)\n    tweet = re.sub(r\"JamaicaObserver\", \"Jamaica Observer\", tweet)\n    tweet = re.sub(r\"TweetLikeItsSeptember11th2001\", \"Tweet like it is september 11th 2001\", tweet)\n    tweet = re.sub(r\"cbplawyers\", \"cbp lawyers\", tweet)\n    tweet = re.sub(r\"fewmoretweets\", \"few more tweets\", tweet)\n    tweet = re.sub(r\"BlackLivesMatter\", \"Black Lives Matter\", tweet)\n    tweet = re.sub(r\"cjoyner\", \"Chris Joyner\", tweet)\n    tweet = re.sub(r\"ENGvAUS\", \"England vs Australia\", tweet)\n    tweet = re.sub(r\"ScottWalker\", \"Scott Walker\", tweet)\n    tweet = re.sub(r\"MikeParrActor\", \"Michael Parr\", tweet)\n    tweet = re.sub(r\"4PlayThursdays\", \"Foreplay Thursdays\", tweet)\n    tweet = re.sub(r\"TGF2015\", \"Tontitown Grape Festival\", tweet)\n    tweet = re.sub(r\"realmandyrain\", \"Mandy Rain\", tweet)\n    tweet = re.sub(r\"GraysonDolan\", \"Grayson Dolan\", tweet)\n    tweet = re.sub(r\"ApolloBrown\", \"Apollo Brown\", tweet)\n    tweet = re.sub(r\"saddlebrooke\", \"Saddlebrooke\", tweet)\n    tweet = re.sub(r\"TontitownGrape\", \"Tontitown Grape\", tweet)\n    tweet = re.sub(r\"AbbsWinston\", \"Abbs Winston\", tweet)\n    tweet = re.sub(r\"ShaunKing\", \"Shaun King\", tweet)\n    tweet = re.sub(r\"MeekMill\", \"Meek Mill\", tweet)\n    tweet = re.sub(r\"TornadoGiveaway\", \"Tornado Giveaway\", tweet)\n    tweet = re.sub(r\"GRupdates\", \"GR updates\", tweet)\n    tweet = re.sub(r\"SouthDowns\", \"South Downs\", tweet)\n    tweet = re.sub(r\"braininjury\", \"brain injury\", tweet)\n    tweet = re.sub(r\"auspol\", \"Australian politics\", tweet)\n    tweet = re.sub(r\"PlannedParenthood\", \"Planned Parenthood\", tweet)\n    tweet = re.sub(r\"calgaryweather\", \"Calgary Weather\", tweet)\n    tweet = re.sub(r\"weallheartonedirection\", \"we all heart one direction\", tweet)\n    tweet = re.sub(r\"edsheeran\", \"Ed Sheeran\", tweet)\n    tweet = re.sub(r\"TrueHeroes\", \"True Heroes\", tweet)\n    tweet = re.sub(r\"S3XLEAK\", \"sex leak\", tweet)\n    tweet = re.sub(r\"ComplexMag\", \"Complex Magazine\", tweet)\n    tweet = re.sub(r\"TheAdvocateMag\", \"The Advocate Magazine\", tweet)\n    tweet = re.sub(r\"CityofCalgary\", \"City of Calgary\", tweet)\n    tweet = re.sub(r\"EbolaOutbreak\", \"Ebola Outbreak\", tweet)\n    tweet = re.sub(r\"SummerFate\", \"Summer Fate\", tweet)\n    tweet = re.sub(r\"RAmag\", \"Royal Academy Magazine\", tweet)\n    tweet = re.sub(r\"offers2go\", \"offers to go\", tweet)\n    tweet = re.sub(r\"foodscare\", \"food scare\", tweet)\n    tweet = re.sub(r\"MNPDNashville\", \"Metropolitan Nashville Police Department\", tweet)\n    tweet = re.sub(r\"TfLBusAlerts\", \"TfL Bus Alerts\", tweet)\n    tweet = re.sub(r\"GamerGate\", \"Gamer Gate\", tweet)\n    tweet = re.sub(r\"IHHen\", \"Humanitarian Relief\", tweet)\n    tweet = re.sub(r\"spinningbot\", \"spinning bot\", tweet)\n    tweet = re.sub(r\"ModiMinistry\", \"Modi Ministry\", tweet)\n    tweet = re.sub(r\"TAXIWAYS\", \"taxi ways\", tweet)\n    tweet = re.sub(r\"Calum5SOS\", \"Calum Hood\", tweet)\n    tweet = re.sub(r\"po_st\", \"po.st\", tweet)\n    tweet = re.sub(r\"scoopit\", \"scoop.it\", tweet)\n    tweet = re.sub(r\"UltimaLucha\", \"Ultima Lucha\", tweet)\n    tweet = re.sub(r\"JonathanFerrell\", \"Jonathan Ferrell\", tweet)\n    tweet = re.sub(r\"aria_ahrary\", \"Aria Ahrary\", tweet)\n    tweet = re.sub(r\"rapidcity\", \"Rapid City\", tweet)\n    tweet = re.sub(r\"OutBid\", \"outbid\", tweet)\n    tweet = re.sub(r\"lavenderpoetrycafe\", \"lavender poetry cafe\", tweet)\n    tweet = re.sub(r\"EudryLantiqua\", \"Eudry Lantiqua\", tweet)\n    tweet = re.sub(r\"15PM\", \"15 PM\", tweet)\n    tweet = re.sub(r\"OriginalFunko\", \"Funko\", tweet)\n    tweet = re.sub(r\"rightwaystan\", \"Richard Tan\", tweet)\n    tweet = re.sub(r\"CindyNoonan\", \"Cindy Noonan\", tweet)\n    tweet = re.sub(r\"RT_America\", \"RT America\", tweet)\n    tweet = re.sub(r\"narendramodi\", \"Narendra Modi\", tweet)\n    tweet = re.sub(r\"BakeOffFriends\", \"Bake Off Friends\", tweet)\n    tweet = re.sub(r\"TeamHendrick\", \"Hendrick Motorsports\", tweet)\n    tweet = re.sub(r\"alexbelloli\", \"Alex Belloli\", tweet)\n    tweet = re.sub(r\"itsjustinstuart\", \"Justin Stuart\", tweet)\n    tweet = re.sub(r\"gunsense\", \"gun sense\", tweet)\n    tweet = re.sub(r\"DebateQuestionsWeWantToHear\", \"debate questions we want to hear\", tweet)\n    tweet = re.sub(r\"RoyalCarribean\", \"Royal Carribean\", tweet)\n    tweet = re.sub(r\"samanthaturne19\", \"Samantha Turner\", tweet)\n    tweet = re.sub(r\"JonVoyage\", \"Jon Stewart\", tweet)\n    tweet = re.sub(r\"renew911health\", \"renew 911 health\", tweet)\n    tweet = re.sub(r\"SuryaRay\", \"Surya Ray\", tweet)\n    tweet = re.sub(r\"pattonoswalt\", \"Patton Oswalt\", tweet)\n    tweet = re.sub(r\"minhazmerchant\", \"Minhaz Merchant\", tweet)\n    tweet = re.sub(r\"TLVFaces\", \"Israel Diaspora Coalition\", tweet)\n    tweet = re.sub(r\"pmarca\", \"Marc Andreessen\", tweet)\n    tweet = re.sub(r\"pdx911\", \"Portland Police\", tweet)\n    tweet = re.sub(r\"jamaicaplain\", \"Jamaica Plain\", tweet)\n    tweet = re.sub(r\"Japton\", \"Arkansas\", tweet)\n    tweet = re.sub(r\"RouteComplex\", \"Route Complex\", tweet)\n    tweet = re.sub(r\"INSubcontinent\", \"Indian Subcontinent\", tweet)\n    tweet = re.sub(r\"NJTurnpike\", \"New Jersey Turnpike\", tweet)\n    tweet = re.sub(r\"Politifiact\", \"PolitiFact\", tweet)\n    tweet = re.sub(r\"Hiroshima70\", \"Hiroshima\", tweet)\n    tweet = re.sub(r\"GMMBC\", \"Greater Mt Moriah Baptist Church\", tweet)\n    tweet = re.sub(r\"versethe\", \"verse the\", tweet)\n    tweet = re.sub(r\"TubeStrike\", \"Tube Strike\", tweet)\n    tweet = re.sub(r\"MissionHills\", \"Mission Hills\", tweet)\n    tweet = re.sub(r\"ProtectDenaliWolves\", \"Protect Denali Wolves\", tweet)\n    tweet = re.sub(r\"NANKANA\", \"Nankana\", tweet)\n    tweet = re.sub(r\"SAHIB\", \"Sahib\", tweet)\n    tweet = re.sub(r\"PAKPATTAN\", \"Pakpattan\", tweet)\n    tweet = re.sub(r\"Newz_Sacramento\", \"News Sacramento\", tweet)\n    tweet = re.sub(r\"gofundme\", \"go fund me\", tweet)\n    tweet = re.sub(r\"pmharper\", \"Stephen Harper\", tweet)\n    tweet = re.sub(r\"IvanBerroa\", \"Ivan Berroa\", tweet)\n    tweet = re.sub(r\"LosDelSonido\", \"Los Del Sonido\", tweet)\n    tweet = re.sub(r\"bancodeseries\", \"banco de series\", tweet)\n    tweet = re.sub(r\"timkaine\", \"Tim Kaine\", tweet)\n    tweet = re.sub(r\"IdentityTheft\", \"Identity Theft\", tweet)\n    tweet = re.sub(r\"AllLivesMatter\", \"All Lives Matter\", tweet)\n    tweet = re.sub(r\"mishacollins\", \"Misha Collins\", tweet)\n    tweet = re.sub(r\"BillNeelyNBC\", \"Bill Neely\", tweet)\n    tweet = re.sub(r\"BeClearOnCancer\", \"be clear on cancer\", tweet)\n    tweet = re.sub(r\"Kowing\", \"Knowing\", tweet)\n    tweet = re.sub(r\"ScreamQueens\", \"Scream Queens\", tweet)\n    tweet = re.sub(r\"AskCharley\", \"Ask Charley\", tweet)\n    tweet = re.sub(r\"BlizzHeroes\", \"Heroes of the Storm\", tweet)\n    tweet = re.sub(r\"BradleyBrad47\", \"Bradley Brad\", tweet)\n    tweet = re.sub(r\"HannaPH\", \"Typhoon Hanna\", tweet)\n    tweet = re.sub(r\"meinlcymbals\", \"MEINL Cymbals\", tweet)\n    tweet = re.sub(r\"Ptbo\", \"Peterborough\", tweet)\n    tweet = re.sub(r\"cnnbrk\", \"CNN Breaking News\", tweet)\n    tweet = re.sub(r\"IndianNews\", \"Indian News\", tweet)\n    tweet = re.sub(r\"savebees\", \"save bees\", tweet)\n    tweet = re.sub(r\"GreenHarvard\", \"Green Harvard\", tweet)\n    tweet = re.sub(r\"StandwithPP\", \"Stand with planned parenthood\", tweet)\n    tweet = re.sub(r\"hermancranston\", \"Herman Cranston\", tweet)\n    tweet = re.sub(r\"WMUR9\", \"WMUR-TV\", tweet)\n    tweet = re.sub(r\"RockBottomRadFM\", \"Rock Bottom Radio\", tweet)\n    tweet = re.sub(r\"ameenshaikh3\", \"Ameen Shaikh\", tweet)\n    tweet = re.sub(r\"ProSyn\", \"Project Syndicate\", tweet)\n    tweet = re.sub(r\"Daesh\", \"ISIS\", tweet)\n    tweet = re.sub(r\"s2g\", \"swear to god\", tweet)\n    tweet = re.sub(r\"listenlive\", \"listen live\", tweet)\n    tweet = re.sub(r\"CDCgov\", \"Centers for Disease Control and Prevention\", tweet)\n    tweet = re.sub(r\"FoxNew\", \"Fox News\", tweet)\n    tweet = re.sub(r\"CBSBigBrother\", \"Big Brother\", tweet)\n    tweet = re.sub(r\"JulieDiCaro\", \"Julie DiCaro\", tweet)\n    tweet = re.sub(r\"theadvocatemag\", \"The Advocate Magazine\", tweet)\n    tweet = re.sub(r\"RohnertParkDPS\", \"Rohnert Park Police Department\", tweet)\n    tweet = re.sub(r\"THISIZBWRIGHT\", \"Bonnie Wright\", tweet)\n    tweet = re.sub(r\"Popularmmos\", \"Popular MMOs\", tweet)\n    tweet = re.sub(r\"WildHorses\", \"Wild Horses\", tweet)\n    tweet = re.sub(r\"FantasticFour\", \"Fantastic Four\", tweet)\n    tweet = re.sub(r\"HORNDALE\", \"Horndale\", tweet)\n    tweet = re.sub(r\"PINER\", \"Piner\", tweet)\n    tweet = re.sub(r\"BathAndNorthEastSomerset\", \"Bath and North East Somerset\", tweet)\n    tweet = re.sub(r\"thatswhatfriendsarefor\", \"that is what friends are for\", tweet)\n    tweet = re.sub(r\"residualincome\", \"residual income\", tweet)\n    tweet = re.sub(r\"YahooNewsDigest\", \"Yahoo News Digest\", tweet)\n    tweet = re.sub(r\"MalaysiaAirlines\", \"Malaysia Airlines\", tweet)\n    tweet = re.sub(r\"AmazonDeals\", \"Amazon Deals\", tweet)\n    tweet = re.sub(r\"MissCharleyWebb\", \"Charley Webb\", tweet)\n    tweet = re.sub(r\"shoalstraffic\", \"shoals traffic\", tweet)\n    tweet = re.sub(r\"GeorgeFoster72\", \"George Foster\", tweet)\n    tweet = re.sub(r\"pop2015\", \"pop 2015\", tweet)\n    tweet = re.sub(r\"_PokemonCards_\", \"Pokemon Cards\", tweet)\n    tweet = re.sub(r\"DianneG\", \"Dianne Gallagher\", tweet)\n    tweet = re.sub(r\"KashmirConflict\", \"Kashmir Conflict\", tweet)\n    tweet = re.sub(r\"BritishBakeOff\", \"British Bake Off\", tweet)\n    tweet = re.sub(r\"FreeKashmir\", \"Free Kashmir\", tweet)\n    tweet = re.sub(r\"mattmosley\", \"Matt Mosley\", tweet)\n    tweet = re.sub(r\"BishopFred\", \"Bishop Fred\", tweet)\n    tweet = re.sub(r\"EndConflict\", \"End Conflict\", tweet)\n    tweet = re.sub(r\"EndOccupation\", \"End Occupation\", tweet)\n    tweet = re.sub(r\"UNHEALED\", \"unhealed\", tweet)\n    tweet = re.sub(r\"CharlesDagnall\", \"Charles Dagnall\", tweet)\n    tweet = re.sub(r\"Latestnews\", \"Latest news\", tweet)\n    tweet = re.sub(r\"KindleCountdown\", \"Kindle Countdown\", tweet)\n    tweet = re.sub(r\"NoMoreHandouts\", \"No More Handouts\", tweet)\n    tweet = re.sub(r\"datingtips\", \"dating tips\", tweet)\n    tweet = re.sub(r\"charlesadler\", \"Charles Adler\", tweet)\n    tweet = re.sub(r\"twia\", \"Texas Windstorm Insurance Association\", tweet)\n    tweet = re.sub(r\"txlege\", \"Texas Legislature\", tweet)\n    tweet = re.sub(r\"WindstormInsurer\", \"Windstorm Insurer\", tweet)\n    tweet = re.sub(r\"Newss\", \"News\", tweet)\n    tweet = re.sub(r\"hempoil\", \"hemp oil\", tweet)\n    tweet = re.sub(r\"CommoditiesAre\", \"Commodities are\", tweet)\n    tweet = re.sub(r\"tubestrike\", \"tube strike\", tweet)\n    tweet = re.sub(r\"JoeNBC\", \"Joe Scarborough\", tweet)\n    tweet = re.sub(r\"LiteraryCakes\", \"Literary Cakes\", tweet)\n    tweet = re.sub(r\"TI5\", \"The International 5\", tweet)\n    tweet = re.sub(r\"thehill\", \"the hill\", tweet)\n    tweet = re.sub(r\"3others\", \"3 others\", tweet)\n    tweet = re.sub(r\"stighefootball\", \"Sam Tighe\", tweet)\n    tweet = re.sub(r\"whatstheimportantvideo\", \"what is the important video\", tweet)\n    tweet = re.sub(r\"ClaudioMeloni\", \"Claudio Meloni\", tweet)\n    tweet = re.sub(r\"DukeSkywalker\", \"Duke Skywalker\", tweet)\n    tweet = re.sub(r\"carsonmwr\", \"Fort Carson\", tweet)\n    tweet = re.sub(r\"offdishduty\", \"off dish duty\", tweet)\n    tweet = re.sub(r\"andword\", \"and word\", tweet)\n    tweet = re.sub(r\"rhodeisland\", \"Rhode Island\", tweet)\n    tweet = re.sub(r\"easternoregon\", \"Eastern Oregon\", tweet)\n    tweet = re.sub(r\"WAwildfire\", \"Washington Wildfire\", tweet)\n    tweet = re.sub(r\"fingerrockfire\", \"Finger Rock Fire\", tweet)\n    tweet = re.sub(r\"57am\", \"57 am\", tweet)\n    tweet = re.sub(r\"fingerrockfire\", \"Finger Rock Fire\", tweet)\n    tweet = re.sub(r\"JacobHoggard\", \"Jacob Hoggard\", tweet)\n    tweet = re.sub(r\"newnewnew\", \"new new new\", tweet)\n    tweet = re.sub(r\"under50\", \"under 50\", tweet)\n    tweet = re.sub(r\"getitbeforeitsgone\", \"get it before it is gone\", tweet)\n    tweet = re.sub(r\"freshoutofthebox\", \"fresh out of the box\", tweet)\n    tweet = re.sub(r\"amwriting\", \"am writing\", tweet)\n    tweet = re.sub(r\"Bokoharm\", \"Boko Haram\", tweet)\n    tweet = re.sub(r\"Nowlike\", \"Now like\", tweet)\n    tweet = re.sub(r\"seasonfrom\", \"season from\", tweet)\n    tweet = re.sub(r\"epicente\", \"epicenter\", tweet)\n    tweet = re.sub(r\"epicenterr\", \"epicenter\", tweet)\n    tweet = re.sub(r\"sicklife\", \"sick life\", tweet)\n    tweet = re.sub(r\"yycweather\", \"Calgary Weather\", tweet)\n    tweet = re.sub(r\"calgarysun\", \"Calgary Sun\", tweet)\n    tweet = re.sub(r\"approachng\", \"approaching\", tweet)\n    tweet = re.sub(r\"evng\", \"evening\", tweet)\n    tweet = re.sub(r\"Sumthng\", \"something\", tweet)\n    tweet = re.sub(r\"EllenPompeo\", \"Ellen Pompeo\", tweet)\n    tweet = re.sub(r\"shondarhimes\", \"Shonda Rhimes\", tweet)\n    tweet = re.sub(r\"ABCNetwork\", \"ABC Network\", tweet)\n    tweet = re.sub(r\"SushmaSwaraj\", \"Sushma Swaraj\", tweet)\n    tweet = re.sub(r\"pray4japan\", \"Pray for Japan\", tweet)\n    tweet = re.sub(r\"hope4japan\", \"Hope for Japan\", tweet)\n    tweet = re.sub(r\"Illusionimagess\", \"Illusion images\", tweet)\n    tweet = re.sub(r\"SummerUnderTheStars\", \"Summer Under The Stars\", tweet)\n    tweet = re.sub(r\"ShallWeDance\", \"Shall We Dance\", tweet)\n    tweet = re.sub(r\"TCMParty\", \"TCM Party\", tweet)\n    tweet = re.sub(r\"marijuananews\", \"marijuana news\", tweet)\n    tweet = re.sub(r\"onbeingwithKristaTippett\", \"on being with Krista Tippett\", tweet)\n    tweet = re.sub(r\"Beingtweets\", \"Being tweets\", tweet)\n    tweet = re.sub(r\"newauthors\", \"new authors\", tweet)\n    tweet = re.sub(r\"remedyyyy\", \"remedy\", tweet)\n    tweet = re.sub(r\"44PM\", \"44 PM\", tweet)\n    tweet = re.sub(r\"HeadlinesApp\", \"Headlines App\", tweet)\n    tweet = re.sub(r\"40PM\", \"40 PM\", tweet)\n    tweet = re.sub(r\"myswc\", \"Severe Weather Center\", tweet)\n    tweet = re.sub(r\"ithats\", \"that is\", tweet)\n    tweet = re.sub(r\"icouldsitinthismomentforever\", \"I could sit in this moment forever\", tweet)\n    tweet = re.sub(r\"FatLoss\", \"Fat Loss\", tweet)\n    tweet = re.sub(r\"02PM\", \"02 PM\", tweet)\n    tweet = re.sub(r\"MetroFmTalk\", \"Metro Fm Talk\", tweet)\n    tweet = re.sub(r\"Bstrd\", \"bastard\", tweet)\n    tweet = re.sub(r\"bldy\", \"bloody\", tweet)\n    tweet = re.sub(r\"MetrofmTalk\", \"Metro Fm Talk\", tweet)\n    tweet = re.sub(r\"terrorismturn\", \"terrorism turn\", tweet)\n    tweet = re.sub(r\"BBCNewsAsia\", \"BBC News Asia\", tweet)\n    tweet = re.sub(r\"BehindTheScenes\", \"Behind The Scenes\", tweet)\n    tweet = re.sub(r\"GeorgeTakei\", \"George Takei\", tweet)\n    tweet = re.sub(r\"WomensWeeklyMag\", \"Womens Weekly Magazine\", tweet)\n    tweet = re.sub(r\"SurvivorsGuidetoEarth\", \"Survivors Guide to Earth\", tweet)\n    tweet = re.sub(r\"incubusband\", \"incubus band\", tweet)\n    tweet = re.sub(r\"Babypicturethis\", \"Baby picture this\", tweet)\n    tweet = re.sub(r\"BombEffects\", \"Bomb Effects\", tweet)\n    tweet = re.sub(r\"win10\", \"Windows 10\", tweet)\n    tweet = re.sub(r\"idkidk\", \"I do not know I do not know\", tweet)\n    tweet = re.sub(r\"TheWalkingDead\", \"The Walking Dead\", tweet)\n    tweet = re.sub(r\"amyschumer\", \"Amy Schumer\", tweet)\n    tweet = re.sub(r\"crewlist\", \"crew list\", tweet)\n    tweet = re.sub(r\"Erdogans\", \"Erdogan\", tweet)\n    tweet = re.sub(r\"BBCLive\", \"BBC Live\", tweet)\n    tweet = re.sub(r\"TonyAbbottMHR\", \"Tony Abbott\", tweet)\n    tweet = re.sub(r\"paulmyerscough\", \"Paul Myerscough\", tweet)\n    tweet = re.sub(r\"georgegallagher\", \"George Gallagher\", tweet)\n    tweet = re.sub(r\"JimmieJohnson\", \"Jimmie Johnson\", tweet)\n    tweet = re.sub(r\"pctool\", \"pc tool\", tweet)\n    tweet = re.sub(r\"DoingHashtagsRight\", \"Doing Hashtags Right\", tweet)\n    tweet = re.sub(r\"ThrowbackThursday\", \"Throwback Thursday\", tweet)\n    tweet = re.sub(r\"SnowBackSunday\", \"Snowback Sunday\", tweet)\n    tweet = re.sub(r\"LakeEffect\", \"Lake Effect\", tweet)\n    tweet = re.sub(r\"RTphotographyUK\", \"Richard Thomas Photography UK\", tweet)\n    tweet = re.sub(r\"BigBang_CBS\", \"Big Bang CBS\", tweet)\n    tweet = re.sub(r\"writerslife\", \"writers life\", tweet)\n    tweet = re.sub(r\"NaturalBirth\", \"Natural Birth\", tweet)\n    tweet = re.sub(r\"UnusualWords\", \"Unusual Words\", tweet)\n    tweet = re.sub(r\"wizkhalifa\", \"Wiz Khalifa\", tweet)\n    tweet = re.sub(r\"acreativedc\", \"a creative DC\", tweet)\n    tweet = re.sub(r\"vscodc\", \"vsco DC\", tweet)\n    tweet = re.sub(r\"VSCOcam\", \"vsco camera\", tweet)\n    tweet = re.sub(r\"TheBEACHDC\", \"The beach DC\", tweet)\n    tweet = re.sub(r\"buildingmuseum\", \"building museum\", tweet)\n    tweet = re.sub(r\"WorldOil\", \"World Oil\", tweet)\n    tweet = re.sub(r\"redwedding\", \"red wedding\", tweet)\n    tweet = re.sub(r\"AmazingRaceCanada\", \"Amazing Race Canada\", tweet)\n    tweet = re.sub(r\"WakeUpAmerica\", \"Wake Up America\", tweet)\n    tweet = re.sub(r\"\\\\Allahuakbar\\\\\", \"Allahu Akbar\", tweet)\n    tweet = re.sub(r\"bleased\", \"blessed\", tweet)\n    tweet = re.sub(r\"nigeriantribune\", \"Nigerian Tribune\", tweet)\n    tweet = re.sub(r\"HIDEO_KOJIMA_EN\", \"Hideo Kojima\", tweet)\n    tweet = re.sub(r\"FusionFestival\", \"Fusion Festival\", tweet)\n    tweet = re.sub(r\"50Mixed\", \"50 Mixed\", tweet)\n    tweet = re.sub(r\"NoAgenda\", \"No Agenda\", tweet)\n    tweet = re.sub(r\"WhiteGenocide\", \"White Genocide\", tweet)\n    tweet = re.sub(r\"dirtylying\", \"dirty lying\", tweet)\n    tweet = re.sub(r\"SyrianRefugees\", \"Syrian Refugees\", tweet)\n    tweet = re.sub(r\"changetheworld\", \"change the world\", tweet)\n    tweet = re.sub(r\"Ebolacase\", \"Ebola case\", tweet)\n    tweet = re.sub(r\"mcgtech\", \"mcg technologies\", tweet)\n    tweet = re.sub(r\"withweapons\", \"with weapons\", tweet)\n    tweet = re.sub(r\"advancedwarfare\", \"advanced warfare\", tweet)\n    tweet = re.sub(r\"letsFootball\", \"let us Football\", tweet)\n    tweet = re.sub(r\"LateNiteMix\", \"late night mix\", tweet)\n    tweet = re.sub(r\"PhilCollinsFeed\", \"Phil Collins\", tweet)\n    tweet = re.sub(r\"RudyHavenstein\", \"Rudy Havenstein\", tweet)\n    tweet = re.sub(r\"22PM\", \"22 PM\", tweet)\n    tweet = re.sub(r\"54am\", \"54 AM\", tweet)\n    tweet = re.sub(r\"38am\", \"38 AM\", tweet)\n    tweet = re.sub(r\"OldFolkExplainStuff\", \"Old Folk Explain Stuff\", tweet)\n    tweet = re.sub(r\"BlacklivesMatter\", \"Black Lives Matter\", tweet)\n    tweet = re.sub(r\"InsaneLimits\", \"Insane Limits\", tweet)\n    tweet = re.sub(r\"youcantsitwithus\", \"you cannot sit with us\", tweet)\n    tweet = re.sub(r\"2k15\", \"2015\", tweet)\n    tweet = re.sub(r\"TheIran\", \"Iran\", tweet)\n    tweet = re.sub(r\"JimmyFallon\", \"Jimmy Fallon\", tweet)\n    tweet = re.sub(r\"AlbertBrooks\", \"Albert Brooks\", tweet)\n    tweet = re.sub(r\"defense_news\", \"defense news\", tweet)\n    tweet = re.sub(r\"nuclearrcSA\", \"Nuclear Risk Control Self Assessment\", tweet)\n    tweet = re.sub(r\"Auspol\", \"Australia Politics\", tweet)\n    tweet = re.sub(r\"NuclearPower\", \"Nuclear Power\", tweet)\n    tweet = re.sub(r\"WhiteTerrorism\", \"White Terrorism\", tweet)\n    tweet = re.sub(r\"truthfrequencyradio\", \"Truth Frequency Radio\", tweet)\n    tweet = re.sub(r\"ErasureIsNotEquality\", \"Erasure is not equality\", tweet)\n    tweet = re.sub(r\"ProBonoNews\", \"Pro Bono News\", tweet)\n    tweet = re.sub(r\"JakartaPost\", \"Jakarta Post\", tweet)\n    tweet = re.sub(r\"toopainful\", \"too painful\", tweet)\n    tweet = re.sub(r\"melindahaunton\", \"Melinda Haunton\", tweet)\n    tweet = re.sub(r\"NoNukes\", \"No Nukes\", tweet)\n    tweet = re.sub(r\"curryspcworld\", \"Currys PC World\", tweet)\n    tweet = re.sub(r\"ineedcake\", \"I need cake\", tweet)\n    tweet = re.sub(r\"blackforestgateau\", \"black forest gateau\", tweet)\n    tweet = re.sub(r\"BBCOne\", \"BBC One\", tweet)\n    tweet = re.sub(r\"AlexxPage\", \"Alex Page\", tweet)\n    tweet = re.sub(r\"jonathanserrie\", \"Jonathan Serrie\", tweet)\n    tweet = re.sub(r\"SocialJerkBlog\", \"Social Jerk Blog\", tweet)\n    tweet = re.sub(r\"ChelseaVPeretti\", \"Chelsea Peretti\", tweet)\n    tweet = re.sub(r\"irongiant\", \"iron giant\", tweet)\n    tweet = re.sub(r\"RonFunches\", \"Ron Funches\", tweet)\n    tweet = re.sub(r\"TimCook\", \"Tim Cook\", tweet)\n    tweet = re.sub(r\"sebastianstanisaliveandwell\", \"Sebastian Stan is alive and well\", tweet)\n    tweet = re.sub(r\"Madsummer\", \"Mad summer\", tweet)\n    tweet = re.sub(r\"NowYouKnow\", \"Now you know\", tweet)\n    tweet = re.sub(r\"concertphotography\", \"concert photography\", tweet)\n    tweet = re.sub(r\"TomLandry\", \"Tom Landry\", tweet)\n    tweet = re.sub(r\"showgirldayoff\", \"show girl day off\", tweet)\n    tweet = re.sub(r\"Yougslavia\", \"Yugoslavia\", tweet)\n    tweet = re.sub(r\"QuantumDataInformatics\", \"Quantum Data Informatics\", tweet)\n    tweet = re.sub(r\"FromTheDesk\", \"From The Desk\", tweet)\n    tweet = re.sub(r\"TheaterTrial\", \"Theater Trial\", tweet)\n    tweet = re.sub(r\"CatoInstitute\", \"Cato Institute\", tweet)\n    tweet = re.sub(r\"EmekaGift\", \"Emeka Gift\", tweet)\n    tweet = re.sub(r\"LetsBe_Rational\", \"Let us be rational\", tweet)\n    tweet = re.sub(r\"Cynicalreality\", \"Cynical reality\", tweet)\n    tweet = re.sub(r\"FredOlsenCruise\", \"Fred Olsen Cruise\", tweet)\n    tweet = re.sub(r\"NotSorry\", \"not sorry\", tweet)\n    tweet = re.sub(r\"UseYourWords\", \"use your words\", tweet)\n    tweet = re.sub(r\"WordoftheDay\", \"word of the day\", tweet)\n    tweet = re.sub(r\"Dictionarycom\", \"Dictionary.com\", tweet)\n    tweet = re.sub(r\"TheBrooklynLife\", \"The Brooklyn Life\", tweet)\n    tweet = re.sub(r\"jokethey\", \"joke they\", tweet)\n    tweet = re.sub(r\"nflweek1picks\", \"NFL week 1 picks\", tweet)\n    tweet = re.sub(r\"uiseful\", \"useful\", tweet)\n    tweet = re.sub(r\"JusticeDotOrg\", \"The American Association for Justice\", tweet)\n    tweet = re.sub(r\"autoaccidents\", \"auto accidents\", tweet)\n    tweet = re.sub(r\"SteveGursten\", \"Steve Gursten\", tweet)\n    tweet = re.sub(r\"MichiganAutoLaw\", \"Michigan Auto Law\", tweet)\n    tweet = re.sub(r\"birdgang\", \"bird gang\", tweet)\n    tweet = re.sub(r\"nflnetwork\", \"NFL Network\", tweet)\n    tweet = re.sub(r\"NYDNSports\", \"NY Daily News Sports\", tweet)\n    tweet = re.sub(r\"RVacchianoNYDN\", \"Ralph Vacchiano NY Daily News\", tweet)\n    tweet = re.sub(r\"EdmontonEsks\", \"Edmonton Eskimos\", tweet)\n    tweet = re.sub(r\"david_brelsford\", \"David Brelsford\", tweet)\n    tweet = re.sub(r\"TOI_India\", \"The Times of India\", tweet)\n    tweet = re.sub(r\"hegot\", \"he got\", tweet)\n    tweet = re.sub(r\"SkinsOn9\", \"Skins on 9\", tweet)\n    tweet = re.sub(r\"sothathappened\", \"so that happened\", tweet)\n    tweet = re.sub(r\"LCOutOfDoors\", \"LC Out Of Doors\", tweet)\n    tweet = re.sub(r\"NationFirst\", \"Nation First\", tweet)\n    tweet = re.sub(r\"IndiaToday\", \"India Today\", tweet)\n    tweet = re.sub(r\"HLPS\", \"helps\", tweet)\n    tweet = re.sub(r\"HOSTAGESTHROSW\", \"hostages throw\", tweet)\n    tweet = re.sub(r\"SNCTIONS\", \"sanctions\", tweet)\n    tweet = re.sub(r\"BidTime\", \"Bid Time\", tweet)\n    tweet = re.sub(r\"crunchysensible\", \"crunchy sensible\", tweet)\n    tweet = re.sub(r\"RandomActsOfRomance\", \"Random acts of romance\", tweet)\n    tweet = re.sub(r\"MomentsAtHill\", \"Moments at hill\", tweet)\n    tweet = re.sub(r\"eatshit\", \"eat shit\", tweet)\n    tweet = re.sub(r\"liveleakfun\", \"live leak fun\", tweet)\n    tweet = re.sub(r\"SahelNews\", \"Sahel News\", tweet)\n    tweet = re.sub(r\"abc7newsbayarea\", \"ABC 7 News Bay Area\", tweet)\n    tweet = re.sub(r\"facilitiesmanagement\", \"facilities management\", tweet)\n    tweet = re.sub(r\"facilitydude\", \"facility dude\", tweet)\n    tweet = re.sub(r\"CampLogistics\", \"Camp logistics\", tweet)\n    tweet = re.sub(r\"alaskapublic\", \"Alaska public\", tweet)\n    tweet = re.sub(r\"MarketResearch\", \"Market Research\", tweet)\n    tweet = re.sub(r\"AccuracyEsports\", \"Accuracy Esports\", tweet)\n    tweet = re.sub(r\"TheBodyShopAust\", \"The Body Shop Australia\", tweet)\n    tweet = re.sub(r\"yychail\", \"Calgary hail\", tweet)\n    tweet = re.sub(r\"yyctraffic\", \"Calgary traffic\", tweet)\n    tweet = re.sub(r\"eliotschool\", \"eliot school\", tweet)\n    tweet = re.sub(r\"TheBrokenCity\", \"The Broken City\", tweet)\n    tweet = re.sub(r\"OldsFireDept\", \"Olds Fire Department\", tweet)\n    tweet = re.sub(r\"RiverComplex\", \"River Complex\", tweet)\n    tweet = re.sub(r\"fieldworksmells\", \"field work smells\", tweet)\n    tweet = re.sub(r\"IranElection\", \"Iran Election\", tweet)\n    tweet = re.sub(r\"glowng\", \"glowing\", tweet)\n    tweet = re.sub(r\"kindlng\", \"kindling\", tweet)\n    tweet = re.sub(r\"riggd\", \"rigged\", tweet)\n    tweet = re.sub(r\"slownewsday\", \"slow news day\", tweet)\n    tweet = re.sub(r\"MyanmarFlood\", \"Myanmar Flood\", tweet)\n    tweet = re.sub(r\"abc7chicago\", \"ABC 7 Chicago\", tweet)\n    tweet = re.sub(r\"copolitics\", \"Colorado Politics\", tweet)\n    tweet = re.sub(r\"AdilGhumro\", \"Adil Ghumro\", tweet)\n    tweet = re.sub(r\"netbots\", \"net bots\", tweet)\n    tweet = re.sub(r\"byebyeroad\", \"bye bye road\", tweet)\n    tweet = re.sub(r\"massiveflooding\", \"massive flooding\", tweet)\n    tweet = re.sub(r\"EndofUS\", \"End of United States\", tweet)\n    tweet = re.sub(r\"35PM\", \"35 PM\", tweet)\n    tweet = re.sub(r\"greektheatrela\", \"Greek Theatre Los Angeles\", tweet)\n    tweet = re.sub(r\"76mins\", \"76 minutes\", tweet)\n    tweet = re.sub(r\"publicsafetyfirst\", \"public safety first\", tweet)\n    tweet = re.sub(r\"livesmatter\", \"lives matter\", tweet)\n    tweet = re.sub(r\"myhometown\", \"my hometown\", tweet)\n    tweet = re.sub(r\"tankerfire\", \"tanker fire\", tweet)\n    tweet = re.sub(r\"MEMORIALDAY\", \"memorial day\", tweet)\n    tweet = re.sub(r\"MEMORIAL_DAY\", \"memorial day\", tweet)\n    tweet = re.sub(r\"instaxbooty\", \"instagram booty\", tweet)\n    tweet = re.sub(r\"Jerusalem_Post\", \"Jerusalem Post\", tweet)\n    tweet = re.sub(r\"WayneRooney_INA\", \"Wayne Rooney\", tweet)\n    tweet = re.sub(r\"VirtualReality\", \"Virtual Reality\", tweet)\n    tweet = re.sub(r\"OculusRift\", \"Oculus Rift\", tweet)\n    tweet = re.sub(r\"OwenJones84\", \"Owen Jones\", tweet)\n    tweet = re.sub(r\"jeremycorbyn\", \"Jeremy Corbyn\", tweet)\n    tweet = re.sub(r\"paulrogers002\", \"Paul Rogers\", tweet)\n    tweet = re.sub(r\"mortalkombatx\", \"Mortal Kombat X\", tweet)\n    tweet = re.sub(r\"mortalkombat\", \"Mortal Kombat\", tweet)\n    tweet = re.sub(r\"FilipeCoelho92\", \"Filipe Coelho\", tweet)\n    tweet = re.sub(r\"OnlyQuakeNews\", \"Only Quake News\", tweet)\n    tweet = re.sub(r\"kostumes\", \"costumes\", tweet)\n    tweet = re.sub(r\"YEEESSSS\", \"yes\", tweet)\n    tweet = re.sub(r\"ToshikazuKatayama\", \"Toshikazu Katayama\", tweet)\n    tweet = re.sub(r\"IntlDevelopment\", \"Intl Development\", tweet)\n    tweet = re.sub(r\"ExtremeWeather\", \"Extreme Weather\", tweet)\n    tweet = re.sub(r\"WereNotGruberVoters\", \"We are not gruber voters\", tweet)\n    tweet = re.sub(r\"NewsThousands\", \"News Thousands\", tweet)\n    tweet = re.sub(r\"EdmundAdamus\", \"Edmund Adamus\", tweet)\n    tweet = re.sub(r\"EyewitnessWV\", \"Eye witness WV\", tweet)\n    tweet = re.sub(r\"PhiladelphiaMuseu\", \"Philadelphia Museum\", tweet)\n    tweet = re.sub(r\"DublinComicCon\", \"Dublin Comic Con\", tweet)\n    tweet = re.sub(r\"NicholasBrendon\", \"Nicholas Brendon\", tweet)\n    tweet = re.sub(r\"Alltheway80s\", \"All the way 80s\", tweet)\n    tweet = re.sub(r\"FromTheField\", \"From the field\", tweet)\n    tweet = re.sub(r\"NorthIowa\", \"North Iowa\", tweet)\n    tweet = re.sub(r\"WillowFire\", \"Willow Fire\", tweet)\n    tweet = re.sub(r\"MadRiverComplex\", \"Mad River Complex\", tweet)\n    tweet = re.sub(r\"feelingmanly\", \"feeling manly\", tweet)\n    tweet = re.sub(r\"stillnotoverit\", \"still not over it\", tweet)\n    tweet = re.sub(r\"FortitudeValley\", \"Fortitude Valley\", tweet)\n    tweet = re.sub(r\"CoastpowerlineTramTr\", \"Coast powerline\", tweet)\n    tweet = re.sub(r\"ServicesGold\", \"Services Gold\", tweet)\n    tweet = re.sub(r\"NewsbrokenEmergency\", \"News broken emergency\", tweet)\n    tweet = re.sub(r\"Evaucation\", \"evacuation\", tweet)\n    tweet = re.sub(r\"leaveevacuateexitbe\", \"leave evacuate exit be\", tweet)\n    tweet = re.sub(r\"P_EOPLE\", \"PEOPLE\", tweet)\n    tweet = re.sub(r\"Tubestrike\", \"tube strike\", tweet)\n    tweet = re.sub(r\"CLASS_SICK\", \"CLASS SICK\", tweet)\n    tweet = re.sub(r\"localplumber\", \"local plumber\", tweet)\n    tweet = re.sub(r\"awesomejobsiri\", \"awesome job siri\", tweet)\n    tweet = re.sub(r\"PayForItHow\", \"Pay for it how\", tweet)\n    tweet = re.sub(r\"ThisIsAfrica\", \"This is Africa\", tweet)\n    tweet = re.sub(r\"crimeairnetwork\", \"crime air network\", tweet)\n    tweet = re.sub(r\"KimAcheson\", \"Kim Acheson\", tweet)\n    tweet = re.sub(r\"cityofcalgary\", \"City of Calgary\", tweet)\n    tweet = re.sub(r\"prosyndicate\", \"pro syndicate\", tweet)\n    tweet = re.sub(r\"660NEWS\", \"660 NEWS\", tweet)\n    tweet = re.sub(r\"BusInsMagazine\", \"Business Insurance Magazine\", tweet)\n    tweet = re.sub(r\"wfocus\", \"focus\", tweet)\n    tweet = re.sub(r\"ShastaDam\", \"Shasta Dam\", tweet)\n    tweet = re.sub(r\"go2MarkFranco\", \"Mark Franco\", tweet)\n    tweet = re.sub(r\"StephGHinojosa\", \"Steph Hinojosa\", tweet)\n    tweet = re.sub(r\"Nashgrier\", \"Nash Grier\", tweet)\n    tweet = re.sub(r\"NashNewVideo\", \"Nash new video\", tweet)\n    tweet = re.sub(r\"IWouldntGetElectedBecause\", \"I would not get elected because\", tweet)\n    tweet = re.sub(r\"SHGames\", \"Sledgehammer Games\", tweet)\n    tweet = re.sub(r\"bedhair\", \"bed hair\", tweet)\n    tweet = re.sub(r\"JoelHeyman\", \"Joel Heyman\", tweet)\n    tweet = re.sub(r\"viaYouTube\", \"via YouTube\", tweet)\n    \n    # Urls\n    tweet = re.sub(r\"https?:\\/\\/t.co\\/[A-Za-z0-9]+\", \"\", tweet)\n    \n    # Words with punctuations and special characters\n    punctuations = '@#!?+&*[]-%.:/();$=><|{}^' + \"'`\"\n    for p in punctuations:\n        tweet = tweet.replace(p, f' {p} ')\n        \n    # ... and ..\n    tweet = tweet.replace('...', ' ... ')\n    if '...' not in tweet:\n        tweet = tweet.replace('..', ' ... ')      \n        \n    # Acronyms\n    tweet = re.sub(r\"MH370\", \"Malaysia Airlines Flight 370\", tweet)\n    tweet = re.sub(r\"m칇췊sica\", \"music\", tweet)\n    tweet = re.sub(r\"okwx\", \"Oklahoma City Weather\", tweet)\n    tweet = re.sub(r\"arwx\", \"Arkansas Weather\", tweet)    \n    tweet = re.sub(r\"gawx\", \"Georgia Weather\", tweet)  \n    tweet = re.sub(r\"scwx\", \"South Carolina Weather\", tweet)  \n    tweet = re.sub(r\"cawx\", \"California Weather\", tweet)\n    tweet = re.sub(r\"tnwx\", \"Tennessee Weather\", tweet)\n    tweet = re.sub(r\"azwx\", \"Arizona Weather\", tweet)  \n    tweet = re.sub(r\"alwx\", \"Alabama Weather\", tweet)\n    tweet = re.sub(r\"wordpressdotcom\", \"wordpress\", tweet)    \n    tweet = re.sub(r\"usNWSgov\", \"United States National Weather Service\", tweet)\n    tweet = re.sub(r\"Suruc\", \"Sanliurfa\", tweet)   \n    \n    # Grouping same words without embeddings\n    tweet = re.sub(r\"Bestnaijamade\", \"bestnaijamade\", tweet)\n    tweet = re.sub(r\"SOUDELOR\", \"Soudelor\", tweet)\n    \n    return tweet\ndf.text = df.text.apply(lambda x: clean_text(x))","metadata":{"execution":{"iopub.status.busy":"2021-10-24T22:57:58.819681Z","iopub.execute_input":"2021-10-24T22:57:58.820191Z","iopub.status.idle":"2021-10-24T23:04:38.277442Z","shell.execute_reply.started":"2021-10-24T22:57:58.820151Z","shell.execute_reply":"2021-10-24T23:04:38.276557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Remove Stopwords","metadata":{}},{"cell_type":"code","source":"stop_words = corpus.stopwords.words('english')\ndef remove_stopwords(text):\n    return ' '.join(w for w in text.split(' ') if w not in stop_words)\ndf.text = df.text.apply(remove_stopwords)","metadata":{"execution":{"iopub.status.busy":"2021-10-24T23:17:27.764225Z","iopub.execute_input":"2021-10-24T23:17:27.764552Z","iopub.status.idle":"2021-10-24T23:17:28.053334Z","shell.execute_reply.started":"2021-10-24T23:17:27.764521Z","shell.execute_reply":"2021-10-24T23:17:28.052444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=3.2 ></a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:left; border-radius: 15px 50px;\">3.2 Stemming</p>\n\n\n[Content](#0)","metadata":{}},{"cell_type":"code","source":"stemmer = SnowballStemmer(\"english\")\n\ndef stemming_text(text):\n    return ' '.join(stemmer.stem(w) for w in text.split(' '))\n\ndf.text = df.text.apply(stemming_text)","metadata":{"execution":{"iopub.status.busy":"2021-10-24T23:04:38.637291Z","iopub.execute_input":"2021-10-24T23:04:38.637616Z","iopub.status.idle":"2021-10-24T23:04:40.155913Z","shell.execute_reply.started":"2021-10-24T23:04:38.637582Z","shell.execute_reply":"2021-10-24T23:04:40.155043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=3.3 ></a>\n   ## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:left; border-radius: 20px 50px;\">3.3 Spelling Checker</p>\n   \n\n[Content](#0)","metadata":{}},{"cell_type":"code","source":"# spell = SpellChecker()\n# def correct_spelling(text):    \n#     misspelled_words = spell.unknown(text.split())\n#     corrected_text = [spell.correction(w) if w in misspelled_words else w for w in text.split()] \n#     return \" \".join(corrected_text)\n# t1 = timeit.timeit()\n# # df.text = df.text.apply(correct_spelling)\n# t2 = timeit.timeit()\n# print(t2-t1)","metadata":{"execution":{"iopub.status.busy":"2021-10-24T23:04:40.157265Z","iopub.execute_input":"2021-10-24T23:04:40.157616Z","iopub.status.idle":"2021-10-24T23:04:40.164392Z","shell.execute_reply.started":"2021-10-24T23:04:40.157579Z","shell.execute_reply":"2021-10-24T23:04:40.163505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Spend so much time to apply Spelling Checker, I will not use it in this data.","metadata":{}},{"cell_type":"markdown","source":"<a id=4 ></a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:left; border-radius: 20px 50px;\">4. Vectorization</p>\n\nThree steps using the Bag-of-words (BOW) model:\n1. Term frequency : count occurrences of word in sentence\n2. Inverse document frequency: \n3. L2 Norm\nReference : https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction\n\n[Content](#0)","metadata":{}},{"cell_type":"markdown","source":"<a id=4.1 ></a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:left; border-radius: 20px 50px;\">4.1 Common Vectorizer Usage</p>\nReference: https://scikit-learn.org/stable/modules/feature_extraction.html#common-vectorizer-usage\n\n[Content](#0)","metadata":{}},{"cell_type":"code","source":"# Instantiate the Vectorizer\nvect = CountVectorizer(stop_words='english', ngram_range=(1,2), min_df=0, max_df=0.9, max_features=100)\ndf_dtm = vect.fit_transform(df)\ndf_dtm.toarray()[0]","metadata":{"execution":{"iopub.status.busy":"2021-10-24T23:04:40.165998Z","iopub.execute_input":"2021-10-24T23:04:40.166433Z","iopub.status.idle":"2021-10-24T23:04:40.177193Z","shell.execute_reply.started":"2021-10-24T23:04:40.166393Z","shell.execute_reply":"2021-10-24T23:04:40.176413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=4.2 ></a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:left; border-radius: 20px 50px;\">4.2 TF-IDF</p>\nReference: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer\n\n[Content](#0)","metadata":{}},{"cell_type":"code","source":"tfidf_vect = TfidfVectorizer(stop_words='english', ngram_range=(1,2), min_df=0, max_df=0.98, max_features=100)\ndf_ifidf= tfidf_vect.fit_transform(df)\ndf_ifidf.toarray()[0]","metadata":{"execution":{"iopub.status.busy":"2021-10-24T23:04:40.179871Z","iopub.execute_input":"2021-10-24T23:04:40.180602Z","iopub.status.idle":"2021-10-24T23:04:40.194501Z","shell.execute_reply.started":"2021-10-24T23:04:40.180558Z","shell.execute_reply":"2021-10-24T23:04:40.193676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=5 ></a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:left; border-radius: 20px 50px;\">5. Model</p>\n\n[Content](#0)","metadata":{}},{"cell_type":"markdown","source":"<a id=5.1 ></a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:left; border-radius: 20px 50px;\">5.1. GloVe for Vectorization</p>\n\nGloVe (*Global Vectors for Word Representation*) is:\n\n    * An unsupervised learning algorithm for obtaining vector representations for words.\n\n    * Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\n    \n    * Non-contextual Word Embedding\n    \n    * Un-directional Model\n\n[Content](#0)","metadata":{}},{"cell_type":"code","source":"# Create Corpus from Text DataFrame\ncorpus=[]\nfor tweet in tqdm(df['text']):\n    words = [word.lower() for word in word_tokenize(tweet) if (word.isalpha()==1 & (word not in stop_words))]\n    corpus.append(words)","metadata":{"execution":{"iopub.status.busy":"2021-10-24T23:17:42.673633Z","iopub.execute_input":"2021-10-24T23:17:42.673976Z","iopub.status.idle":"2021-10-24T23:17:44.553548Z","shell.execute_reply.started":"2021-10-24T23:17:42.673946Z","shell.execute_reply":"2021-10-24T23:17:44.552397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(corpus)\nword_index =  tokenizer.word_index\nvocal_length = len(word_index) + 1\nvocal_length","metadata":{"execution":{"iopub.status.busy":"2021-10-24T23:17:47.206905Z","iopub.execute_input":"2021-10-24T23:17:47.207271Z","iopub.status.idle":"2021-10-24T23:17:47.311614Z","shell.execute_reply.started":"2021-10-24T23:17:47.207238Z","shell.execute_reply":"2021-10-24T23:17:47.310613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create Embedding Dictionary from GloVe Data\nembedding_dict = {}\nwith open('/kaggle/input/glove-global-vectors-for-word-representation/glove.6B.100d.txt', 'r') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        vectors = np.asarray(values[1:], 'float32')\n        embedding_dict[word] = vectors\nf.close()\nlen(embedding_dict)","metadata":{"execution":{"iopub.status.busy":"2021-10-24T23:18:07.209621Z","iopub.execute_input":"2021-10-24T23:18:07.210188Z","iopub.status.idle":"2021-10-24T23:18:24.461446Z","shell.execute_reply.started":"2021-10-24T23:18:07.210136Z","shell.execute_reply":"2021-10-24T23:18:24.460691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preparing the GloVe word-embeddings matrix\nEMBEDDING_DIM = 100\n\nembedding_matrix = np.zeros((vocal_length, EMBEDDING_DIM))\n\nfor word, i in tqdm(word_index.items()):\n    embedding_vector = embedding_dict.get(word)\n    if embedding_vector is not None:\n        # word not found will be all-zeros.\n        embedding_matrix[i] = embedding_vector","metadata":{"execution":{"iopub.status.busy":"2021-10-24T23:18:48.893447Z","iopub.execute_input":"2021-10-24T23:18:48.893854Z","iopub.status.idle":"2021-10-24T23:18:48.942235Z","shell.execute_reply.started":"2021-10-24T23:18:48.893817Z","shell.execute_reply":"2021-10-24T23:18:48.941225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=5.1.1 ></a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:100%; text-align:left; border-radius: 20px 50px;\">5.1.1 Preparing the data</p>\n\n[Content](#0)","metadata":{}},{"cell_type":"code","source":"MAX_SEQUENCE_LENGTH = 40\n\npadded_sentences = pad_sequences(tokenizer.texts_to_sequences(corpus), maxlen=MAX_SEQUENCE_LENGTH, truncating='post', padding=\"post\")\n\ntrain = padded_sentences[:train_len]\ntarget = train_full['target'].values\ntest = padded_sentences[train_len:]","metadata":{"execution":{"iopub.status.busy":"2021-10-24T23:18:56.644856Z","iopub.execute_input":"2021-10-24T23:18:56.645194Z","iopub.status.idle":"2021-10-24T23:18:56.780918Z","shell.execute_reply.started":"2021-10-24T23:18:56.645163Z","shell.execute_reply":"2021-10-24T23:18:56.779999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=5.1.2 ></a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:100%; text-align:left; border-radius: 20px 50px;\">5.1.2 The First recurrent Baseline with 'SimpleRNN' layer</p>\n\n[Content](#0)","metadata":{}},{"cell_type":"code","source":"def modelRNN(embed_dim):\n    model = Sequential()\n    model.add(Embedding(vocal_length, \n                        EMBEDDING_DIM,\n                        weights=[embedding_matrix],\n                        input_length=MAX_SEQUENCE_LENGTH,\n                        trainable=False))\n    model.add(SimpleRNN(embed_dim))\n    model.add(Dense(1, activation='sigmoid'))\n\n    model.compile(optimizer='rmsprop', \n                  loss='binary_crossentropy',\n                  metrics=['acc'])\n\n    history = model.fit(train, target,\n                         epochs=10,\n                         batch_size=128,\n                         validation_split=0.2)\n    return model, history","metadata":{"execution":{"iopub.status.busy":"2021-10-24T23:04:42.51766Z","iopub.status.idle":"2021-10-24T23:04:42.518423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# mod_RNN, his_RNN = modelRNN(64)\n# plot_model(his_RNN.history)","metadata":{"execution":{"iopub.status.busy":"2021-10-24T23:04:42.519629Z","iopub.status.idle":"2021-10-24T23:04:42.520427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In general, the Validation accuracy climbed slowly. Model is needed improvement.  ","metadata":{}},{"cell_type":"markdown","source":"<img src=\"https://i.ibb.co/6wdWCPm/RNN.png\" alt=\"RNN\" border=\"0\">","metadata":{}},{"cell_type":"markdown","source":"<a id=5.1.3 ></a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:100%; text-align:left; border-radius: 20px 50px;\">5.1.3 'Bidirectional' and 'LSTM' layer</p>\n\n[Content](#0)","metadata":{}},{"cell_type":"code","source":"def modelLSTM(embed_dim):\n    model = Sequential()\n    model.add(Embedding(vocal_length, \n                        EMBEDDING_DIM, \n                        weights=[embedding_matrix], \n                        input_length=MAX_SEQUENCE_LENGTH, \n                        trainable=False))\n    model.add(Bidirectional(LSTM(embed_dim,\n                                dropout=0.2,\n                                recurrent_dropout=0.2)))\n    model.add(Dense(1, activation='sigmoid'))\n\n    model.compile(optimizer='rmsprop', \n                  loss='binary_crossentropy',\n                  metrics=['acc'])\n\n    history = model.fit(train, target,\n                         epochs=8,\n                         batch_size=128,\n                         validation_split=0.2)\n    return model, history","metadata":{"execution":{"iopub.status.busy":"2021-10-24T23:04:42.52162Z","iopub.status.idle":"2021-10-24T23:04:42.522445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# mod_LSTM, his_LSTM = modelLSTM(64)\n# plot_model(his_LSTM.history)","metadata":{"execution":{"iopub.status.busy":"2021-10-24T23:04:42.523637Z","iopub.status.idle":"2021-10-24T23:04:42.52439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<img src=\"https://i.ibb.co/Cv80Nbj/LSTM.png\" alt=\"LSTM\" border=\"0\">","metadata":{}},{"cell_type":"markdown","source":"<a id=5.1.4 ></a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:100%; text-align:left; border-radius: 20px 50px;\">5.1.4 'Bidirectional' and GRU layer</p>\n\n[Content](#0)","metadata":{}},{"cell_type":"code","source":"def modelGRU(embed_dim):\n    model = Sequential()\n    model.add(Embedding(vocal_length,\n                       EMBEDDING_DIM,\n                       weights=[embedding_matrix],\n                       input_length=MAX_SEQUENCE_LENGTH,\n                       trainable=False))\n    model.add(Bidirectional(GRU(embed_dim,\n                               dropout=0.3,\n                               recurrent_dropout=0.4)))\n    model.add(Dense(1, activation='sigmoid'))\n\n    model.compile(optimizer='rmsprop', \n                  loss='binary_crossentropy',\n                  metrics=['acc'])\n\n    history = model.fit(train, target,\n                         epochs=18,\n                         batch_size=128,\n                         validation_split=0.2)\n    return model, history","metadata":{"execution":{"iopub.status.busy":"2021-10-24T23:19:13.813288Z","iopub.execute_input":"2021-10-24T23:19:13.813625Z","iopub.status.idle":"2021-10-24T23:19:13.820714Z","shell.execute_reply.started":"2021-10-24T23:19:13.813591Z","shell.execute_reply":"2021-10-24T23:19:13.819615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_GRU, history_GRU = modelGRU(64)\nplot_model(history_GRU.history)","metadata":{"execution":{"iopub.status.busy":"2021-10-24T23:19:21.710211Z","iopub.execute_input":"2021-10-24T23:19:21.710554Z","iopub.status.idle":"2021-10-24T23:23:55.103331Z","shell.execute_reply.started":"2021-10-24T23:19:21.710523Z","shell.execute_reply":"2021-10-24T23:23:55.102478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Result: Model is better with increasing dropout hyperparameter. I am make confused with maximum dropout that can be chosen. \nIn practical, dropout = 0.3 is best.","metadata":{}},{"cell_type":"markdown","source":"<a id=5.1.5 ></a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:100%; text-align:left; border-radius: 20px 50px;\">5.1.5 SpatialDropout1D + Bidirectional + LSTM layer</p>\n\n[Content](#0)","metadata":{}},{"cell_type":"code","source":"def modelLSTM_dropout(embed_dim):\n    model = Sequential()\n    model.add(Embedding(vocal_length,\n                       EMBEDDING_DIM,\n                       weights=[embedding_matrix],\n                       input_length=MAX_SEQUENCE_LENGTH,\n                       trainable=False))\n    model.add(SpatialDropout1D(0.2))\n    model.add(Bidirectional(LSTM(embed_dim,\n                               dropout=0.2,\n                               recurrent_dropout=0.2)))\n    model.add(Dense(1, activation='sigmoid'))\n\n    model.compile(optimizer='rmsprop', \n                  loss='binary_crossentropy',\n                  metrics=['acc'])\n\n    history = model.fit(train, target,\n                         epochs=10,\n                         batch_size=128,\n                         validation_split=0.2)\n    return model, history","metadata":{"execution":{"iopub.status.busy":"2021-10-24T23:04:42.529515Z","iopub.status.idle":"2021-10-24T23:04:42.530282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model_LSTM_dropout, history_LSTM_dropout = modelLSTM_dropout(64)\n# plot_model(history_LSTM_dropout.history)","metadata":{"execution":{"iopub.status.busy":"2021-10-24T23:04:42.531456Z","iopub.status.idle":"2021-10-24T23:04:42.532231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<img src=\"https://i.ibb.co/0rCjB7T/LSTMDropout.png\" alt=\"LSTMDropout\" border=\"0\">","metadata":{}},{"cell_type":"markdown","source":"Result : Adding Dropout working with Bidirectorial LSTM make worse result. Model is easier to overfit and sensitive ","metadata":{}},{"cell_type":"markdown","source":"<a id=5.1.6 ></a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:100%; text-align:left; border-radius: 20px 50px;\">5.1.6 Bidirectional + double LSTM layer</p>\n\n[Content](#0)","metadata":{}},{"cell_type":"code","source":"def model_doubleLSTM(embed_dim):\n    model = Sequential()\n    model.add(Embedding(vocal_length,\n                       EMBEDDING_DIM,\n                       weights=[embedding_matrix],\n                       input_length=MAX_SEQUENCE_LENGTH,\n                       trainable=False))\n    \n    model.add(Bidirectional(GRU(embed_dim,\n                            dropout=0.2,\n                            recurrent_dropout=0.2,\n                               return_sequences=True)))\n    model.add(Bidirectional(GRU(embed_dim,\n                           dropout=0.2,\n                           recurrent_dropout=0.2)))\n    model.add(Dense(1, activation='sigmoid'))\n\n    model.compile(optimizer='rmsprop', \n                  loss='binary_crossentropy',\n                  metrics=['acc'])\n\n    history = model.fit(train, target,\n                         epochs=10,\n                         batch_size=128,\n                         validation_split=0.2)\n    return model, history","metadata":{"execution":{"iopub.status.busy":"2021-10-24T23:04:42.533504Z","iopub.status.idle":"2021-10-24T23:04:42.53432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model_doubleLSTM, history_doubleLSTM = model_doubleLSTM(64)\n# plot_model(history_doubleLSTM.history)","metadata":{"execution":{"iopub.status.busy":"2021-10-24T23:04:42.535515Z","iopub.status.idle":"2021-10-24T23:04:42.536304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<img src=\"https://i.ibb.co/RT7wV2V/double-LSTM.png\" alt=\"double-LSTM\" border=\"0\">","metadata":{}},{"cell_type":"markdown","source":"Result: It is a bad idea","metadata":{}},{"cell_type":"markdown","source":"<a id=10 ></a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:left; border-radius: 20px 50px;\">Make a Submission</p>\n\n[Content](#0)","metadata":{}},{"cell_type":"code","source":"def submission_glove(model, test):\n    \"\"\"For GloVe\"\"\"\n    sample_sub = pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')\n    y_preds = model.predict(test).round().astype(int).reshape(3263) \n    sub=pd.DataFrame({'id':sample_sub['id'].values.tolist(),'target':y_preds})\n    sub.to_csv('submission.csv',index=False)\n","metadata":{"execution":{"iopub.status.busy":"2021-10-24T23:32:13.386554Z","iopub.execute_input":"2021-10-24T23:32:13.386902Z","iopub.status.idle":"2021-10-24T23:32:13.392407Z","shell.execute_reply.started":"2021-10-24T23:32:13.386869Z","shell.execute_reply":"2021-10-24T23:32:13.391234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_glove(model_GRU, test)","metadata":{"execution":{"iopub.status.busy":"2021-10-24T23:32:38.744117Z","iopub.execute_input":"2021-10-24T23:32:38.744451Z","iopub.status.idle":"2021-10-24T23:32:41.041435Z","shell.execute_reply.started":"2021-10-24T23:32:38.744419Z","shell.execute_reply":"2021-10-24T23:32:41.040534Z"},"trusted":true},"execution_count":null,"outputs":[]}]}