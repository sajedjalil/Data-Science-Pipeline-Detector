{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction"},{"metadata":{},"cell_type":"markdown","source":"### This notebook aims to give an **up-to-date list of slangs and abbreviations** used in today modern tweets and chat texts.\n\nAs I was going through interesting notebooks which explain very well text preprocessing, I notice at each abbreviation conversion part, people use the slang translator text from GitHub (source: http://https://github.com/rishabhverma17/sms_slang_translator/blob/master/slang.txt) which is great but not up to date since it was created in 2017 and slangs evolve very quickly.\n\nSo, this notebook completes the list with recent most used slangs in tweets.\n\nAs words conversion is an important part in text preprocessing, this notebook may come handy for cleaner texts.\n\n\nDisclaimer: Some terms in this list and text in the dataset may be considered profane, vulgar, or offensive."},{"metadata":{"_uuid":"f48710de-1245-4f65-ae0d-6837a0c196e9","_cell_guid":"dad0b6a1-e8f3-4c87-8e20-d0ba947b8994","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport re\nfrom bs4 import BeautifulSoup\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\n# Input data files are available in the \"../input/\" directory.\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Get train & test data\npd.set_option('display.max_colwidth', -1)\n#pd.set_option('display.max_rows', None)\ntrain = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\ndef callback(operation_future):\n    result = operation_future.result()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Slangs"},{"metadata":{},"cell_type":"markdown","source":"To form this list, I used multiple sources ([The Urban Dictionary](https://www.urbandictionary.com/), [The Most Used Internet Abbreviations for Texting and Tweeting](https://preply.com/en/blog/2018/05/04/the-most-used-internet-abbreviations-for-texting-and-tweeting/), [SMS Slang Translator](https://github.com/rishabhverma17/sms_slang_translator/blob/master/slang.txt), [English abbreviations](https://www.englisch-hilfen.de/en/words/abbreviations.htm)) and complete it also with some terms.\n\nThis list is mainly focus on slangs and abbreviations used in tweets and texting messages, so depending on the text data you want to preprocess (tweets, text documents, formal texts...) and the context, you may need to add, remove or modify some terms.\n\nI tried to be as exhaustive as possible, that's why you can see some terms multiple times in the dict because they can be spelled differently.\n\nAlso I choose to handle all strings as lower case to avoid dealing with different case spelling. But you can modify it as you please (lower, UPPER, LoPpeR &#x1F921;) "},{"metadata":{"trusted":true},"cell_type":"code","source":"abbreviations = {\n    \"$\" : \" dollar \",\n    \"â‚¬\" : \" euro \",\n    \"4ao\" : \"for adults only\",\n    \"a.m\" : \"before midday\",\n    \"a3\" : \"anytime anywhere anyplace\",\n    \"aamof\" : \"as a matter of fact\",\n    \"acct\" : \"account\",\n    \"adih\" : \"another day in hell\",\n    \"afaic\" : \"as far as i am concerned\",\n    \"afaict\" : \"as far as i can tell\",\n    \"afaik\" : \"as far as i know\",\n    \"afair\" : \"as far as i remember\",\n    \"afk\" : \"away from keyboard\",\n    \"app\" : \"application\",\n    \"approx\" : \"approximately\",\n    \"apps\" : \"applications\",\n    \"asap\" : \"as soon as possible\",\n    \"asl\" : \"age, sex, location\",\n    \"atk\" : \"at the keyboard\",\n    \"ave.\" : \"avenue\",\n    \"aymm\" : \"are you my mother\",\n    \"ayor\" : \"at your own risk\", \n    \"b&b\" : \"bed and breakfast\",\n    \"b+b\" : \"bed and breakfast\",\n    \"b.c\" : \"before christ\",\n    \"b2b\" : \"business to business\",\n    \"b2c\" : \"business to customer\",\n    \"b4\" : \"before\",\n    \"b4n\" : \"bye for now\",\n    \"b@u\" : \"back at you\",\n    \"bae\" : \"before anyone else\",\n    \"bak\" : \"back at keyboard\",\n    \"bbbg\" : \"bye bye be good\",\n    \"bbc\" : \"british broadcasting corporation\",\n    \"bbias\" : \"be back in a second\",\n    \"bbl\" : \"be back later\",\n    \"bbs\" : \"be back soon\",\n    \"be4\" : \"before\",\n    \"bfn\" : \"bye for now\",\n    \"blvd\" : \"boulevard\",\n    \"bout\" : \"about\",\n    \"brb\" : \"be right back\",\n    \"bros\" : \"brothers\",\n    \"brt\" : \"be right there\",\n    \"bsaaw\" : \"big smile and a wink\",\n    \"btw\" : \"by the way\",\n    \"bwl\" : \"bursting with laughter\",\n    \"c/o\" : \"care of\",\n    \"cet\" : \"central european time\",\n    \"cf\" : \"compare\",\n    \"cia\" : \"central intelligence agency\",\n    \"csl\" : \"can not stop laughing\",\n    \"cu\" : \"see you\",\n    \"cul8r\" : \"see you later\",\n    \"cv\" : \"curriculum vitae\",\n    \"cwot\" : \"complete waste of time\",\n    \"cya\" : \"see you\",\n    \"cyt\" : \"see you tomorrow\",\n    \"dae\" : \"does anyone else\",\n    \"dbmib\" : \"do not bother me i am busy\",\n    \"diy\" : \"do it yourself\",\n    \"dm\" : \"direct message\",\n    \"dwh\" : \"during work hours\",\n    \"e123\" : \"easy as one two three\",\n    \"eet\" : \"eastern european time\",\n    \"eg\" : \"example\",\n    \"embm\" : \"early morning business meeting\",\n    \"encl\" : \"enclosed\",\n    \"encl.\" : \"enclosed\",\n    \"etc\" : \"and so on\",\n    \"faq\" : \"frequently asked questions\",\n    \"fawc\" : \"for anyone who cares\",\n    \"fb\" : \"facebook\",\n    \"fc\" : \"fingers crossed\",\n    \"fig\" : \"figure\",\n    \"fimh\" : \"forever in my heart\", \n    \"ft.\" : \"feet\",\n    \"ft\" : \"featuring\",\n    \"ftl\" : \"for the loss\",\n    \"ftw\" : \"for the win\",\n    \"fwiw\" : \"for what it is worth\",\n    \"fyi\" : \"for your information\",\n    \"g9\" : \"genius\",\n    \"gahoy\" : \"get a hold of yourself\",\n    \"gal\" : \"get a life\",\n    \"gcse\" : \"general certificate of secondary education\",\n    \"gfn\" : \"gone for now\",\n    \"gg\" : \"good game\",\n    \"gl\" : \"good luck\",\n    \"glhf\" : \"good luck have fun\",\n    \"gmt\" : \"greenwich mean time\",\n    \"gmta\" : \"great minds think alike\",\n    \"gn\" : \"good night\",\n    \"g.o.a.t\" : \"greatest of all time\",\n    \"goat\" : \"greatest of all time\",\n    \"goi\" : \"get over it\",\n    \"gps\" : \"global positioning system\",\n    \"gr8\" : \"great\",\n    \"gratz\" : \"congratulations\",\n    \"gyal\" : \"girl\",\n    \"h&c\" : \"hot and cold\",\n    \"hp\" : \"horsepower\",\n    \"hr\" : \"hour\",\n    \"hrh\" : \"his royal highness\",\n    \"ht\" : \"height\",\n    \"ibrb\" : \"i will be right back\",\n    \"ic\" : \"i see\",\n    \"icq\" : \"i seek you\",\n    \"icymi\" : \"in case you missed it\",\n    \"idc\" : \"i do not care\",\n    \"idgadf\" : \"i do not give a damn fuck\",\n    \"idgaf\" : \"i do not give a fuck\",\n    \"idk\" : \"i do not know\",\n    \"ie\" : \"that is\",\n    \"i.e\" : \"that is\",\n    \"ifyp\" : \"i feel your pain\",\n    \"IG\" : \"instagram\",\n    \"iirc\" : \"if i remember correctly\",\n    \"ilu\" : \"i love you\",\n    \"ily\" : \"i love you\",\n    \"imho\" : \"in my humble opinion\",\n    \"imo\" : \"in my opinion\",\n    \"imu\" : \"i miss you\",\n    \"iow\" : \"in other words\",\n    \"irl\" : \"in real life\",\n    \"j4f\" : \"just for fun\",\n    \"jic\" : \"just in case\",\n    \"jk\" : \"just kidding\",\n    \"jsyk\" : \"just so you know\",\n    \"l8r\" : \"later\",\n    \"lb\" : \"pound\",\n    \"lbs\" : \"pounds\",\n    \"ldr\" : \"long distance relationship\",\n    \"lmao\" : \"laugh my ass off\",\n    \"lmfao\" : \"laugh my fucking ass off\",\n    \"lol\" : \"laughing out loud\",\n    \"ltd\" : \"limited\",\n    \"ltns\" : \"long time no see\",\n    \"m8\" : \"mate\",\n    \"mf\" : \"motherfucker\",\n    \"mfs\" : \"motherfuckers\",\n    \"mfw\" : \"my face when\",\n    \"mofo\" : \"motherfucker\",\n    \"mph\" : \"miles per hour\",\n    \"mr\" : \"mister\",\n    \"mrw\" : \"my reaction when\",\n    \"ms\" : \"miss\",\n    \"mte\" : \"my thoughts exactly\",\n    \"nagi\" : \"not a good idea\",\n    \"nbc\" : \"national broadcasting company\",\n    \"nbd\" : \"not big deal\",\n    \"nfs\" : \"not for sale\",\n    \"ngl\" : \"not going to lie\",\n    \"nhs\" : \"national health service\",\n    \"nrn\" : \"no reply necessary\",\n    \"nsfl\" : \"not safe for life\",\n    \"nsfw\" : \"not safe for work\",\n    \"nth\" : \"nice to have\",\n    \"nvr\" : \"never\",\n    \"nyc\" : \"new york city\",\n    \"oc\" : \"original content\",\n    \"og\" : \"original\",\n    \"ohp\" : \"overhead projector\",\n    \"oic\" : \"oh i see\",\n    \"omdb\" : \"over my dead body\",\n    \"omg\" : \"oh my god\",\n    \"omw\" : \"on my way\",\n    \"p.a\" : \"per annum\",\n    \"p.m\" : \"after midday\",\n    \"pm\" : \"prime minister\",\n    \"poc\" : \"people of color\",\n    \"pov\" : \"point of view\",\n    \"pp\" : \"pages\",\n    \"ppl\" : \"people\",\n    \"prw\" : \"parents are watching\",\n    \"ps\" : \"postscript\",\n    \"pt\" : \"point\",\n    \"ptb\" : \"please text back\",\n    \"pto\" : \"please turn over\",\n    \"qpsa\" : \"what happens\", #\"que pasa\",\n    \"ratchet\" : \"rude\",\n    \"rbtl\" : \"read between the lines\",\n    \"rlrt\" : \"real life retweet\", \n    \"rofl\" : \"rolling on the floor laughing\",\n    \"roflol\" : \"rolling on the floor laughing out loud\",\n    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n    \"rt\" : \"retweet\",\n    \"ruok\" : \"are you ok\",\n    \"sfw\" : \"safe for work\",\n    \"sk8\" : \"skate\",\n    \"smh\" : \"shake my head\",\n    \"sq\" : \"square\",\n    \"srsly\" : \"seriously\", \n    \"ssdd\" : \"same stuff different day\",\n    \"tbh\" : \"to be honest\",\n    \"tbs\" : \"tablespooful\",\n    \"tbsp\" : \"tablespooful\",\n    \"tfw\" : \"that feeling when\",\n    \"thks\" : \"thank you\",\n    \"tho\" : \"though\",\n    \"thx\" : \"thank you\",\n    \"tia\" : \"thanks in advance\",\n    \"til\" : \"today i learned\",\n    \"tl;dr\" : \"too long i did not read\",\n    \"tldr\" : \"too long i did not read\",\n    \"tmb\" : \"tweet me back\",\n    \"tntl\" : \"trying not to laugh\",\n    \"ttyl\" : \"talk to you later\",\n    \"u\" : \"you\",\n    \"u2\" : \"you too\",\n    \"u4e\" : \"yours for ever\",\n    \"utc\" : \"coordinated universal time\",\n    \"w/\" : \"with\",\n    \"w/o\" : \"without\",\n    \"w8\" : \"wait\",\n    \"wassup\" : \"what is up\",\n    \"wb\" : \"welcome back\",\n    \"wtf\" : \"what the fuck\",\n    \"wtg\" : \"way to go\",\n    \"wtpa\" : \"where the party at\",\n    \"wuf\" : \"where are you from\",\n    \"wuzup\" : \"what is up\",\n    \"wywh\" : \"wish you were here\",\n    \"yd\" : \"yard\",\n    \"ygtr\" : \"you got that right\",\n    \"ynk\" : \"you never know\",\n    \"zzz\" : \"sleeping bored and tired\"\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_abbrev(word):\n    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"stopwords_list = stopwords.words('english')\n\ndef clean_text(text):\n    # Remove URLs\n    text = re.sub(r\"http\\S+\", '', text)\n    text = re.sub(r\"www\\S+\", '', text)\n    text = re.sub(r\"pic.twitter.com\\S+\", '', text)\n\n    # Remove XML tags\n    text = BeautifulSoup(text, \"lxml\").text\n    return text\n\ndef get_abbrev(text):    \n    tokens = word_tokenize(text)\n    tokens = [word for word in tokens if word.lower() in abbreviations.keys()]\n    text = ' '.join(tokens)\n    return text\n    \n\ndef convert_abbrev_in_text(text):\n    tokens = word_tokenize(text)\n    tokens=[convert_abbrev(word) for word in tokens]\n    text = ' '.join(tokens)\n    return text\n\ntrain['clean_text'] = train['text'].apply(lambda x: clean_text(x))\ntrain['converted_text'] = train['clean_text'].apply(lambda x: convert_abbrev_in_text(x))\ntrain['abbreviations_in_text'] = train['clean_text'].apply(lambda x: get_abbrev(x))\n\ntest['clean_text'] = test['text'].apply(lambda x: clean_text(x))\ntest['converted_text'] = test['clean_text'].apply(lambda x: convert_abbrev_in_text(x))\ntest['abbreviations_in_text'] = test['clean_text'].apply(lambda x: get_abbrev(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We can explore a bit the dataset and see which abbreviations (or slangs) are present in the data (train and test) and which are not."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df = pd.concat([train[train['abbreviations_in_text']!= \"\"], test[test['abbreviations_in_text']!= \"\"]], sort=False)[['id', 'text', 'converted_text', 'abbreviations_in_text']] ;\nprint(df.shape)\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"not_in_data = [] ; in_data = []\nfor i in abbreviations.keys():\n    if train[train['abbreviations_in_text'].str.contains(i, na=False, case=False)].empty and test[test['text'].str.contains(i, na=False, case=False)].empty :\n        not_in_data.append(i)\n    else :\n        in_data.append(i)\nprint(\"Abbreviations in data : {}\\n{}\\n\\nAbbreviations not in data : {}\\n{}\\n\".format(len(in_data), in_data, len(not_in_data), not_in_data))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# Abbreviations frequency in train and test data\nabbrv_in_train = train.abbreviations_in_text.str.get_dummies(sep=' ').sum().sort_values(ascending=False)\nabbrv_in_test = test.abbreviations_in_text.str.get_dummies(sep=' ').sum().sort_values(ascending=False)\nabbrv_in_data = pd.concat([abbrv_in_train, abbrv_in_test], axis=1, sort=False, keys=['train', 'test']).fillna(0)\nabbrv_in_data","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Plotting data\nx = np.arange(30)\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(20,14))\nax1.bar(x, abbrv_in_train[:30], color=\"blue\", label='train')\nax2.bar(x, abbrv_in_test[:30], color=\"orange\", label='test')\n    \n# Add some text for labels, title and custom x-axis tick labels, etc.\nax1.set_xticks(x) ; ax2.set_xticks(x)\nax1.set_xticklabels(abbrv_in_train.index, fontsize=14)\nax2.set_xticklabels(abbrv_in_test.index, fontsize=14)\nax1.legend() ; ax2.legend()\nfig.text(0.5, -0.02, 'Abbreviations and/or Slangs', ha='center', fontsize=16)\nfig.text(-0.01, 0.5, 'Frenquencies', va='center', rotation='vertical', fontsize=16)\nfig.suptitle('30 most common Abbreviations in train and test data', fontsize=20)\n\nfor i in x:\n    ax1.annotate(abbrv_in_train[i], xy=(i, abbrv_in_train[i]), textcoords=\"offset points\", rotation=0, xytext=(0, 3), ha='center', fontsize=14)\n    ax2.annotate(abbrv_in_test[i], xy=(i, abbrv_in_test[i]), textcoords=\"offset points\", rotation=0, xytext=(0, 3), ha='center', fontsize=14)\n\nplt.tight_layout(pad=3.0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hope you find this notebook useful, if so please UPVOTE &#x1F44D;\n\nAlso, don't hesitate if you have any suggestions to improve this notebook. They are more than welcome!!\n\nIf you do use the notebook, please credit me &#x1F60A; [Up-to-date list of Slangs for Text Preprocessing by @nmaguette](https://www.kaggle.com/nmaguette/up-to-date-slangs-conversion-for-text-processing/)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}