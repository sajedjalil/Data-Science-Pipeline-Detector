{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **0: Library and function import**","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport re\n\n# Remove the useless url tag \ndef remove_url(raw_str):\n    clean_str = re.sub(r'http\\S+', '', raw_str)\n    return clean_str","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **1: Data Loading**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")\ntrain_df = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\ntest_df = pd.read_csv(\"../input/nlp-getting-started/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Randomization\nstate = 1\ntrain_df = train_df.sample(frac=1,random_state=state)\ntest_df = test_df.sample(frac=1,random_state=state)\ntrain_df.reset_index(inplace=True, drop=True) \ntest_df.reset_index(inplace=True, drop=True) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ** 2: Data Preprocessing**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Credit: Gunes Evitan\n#https://www.kaggle.com/gunesevitan/nlp-with-disaster-tweets-eda-full-cleaning#4.-Embeddings-&-Text-Cleaning\ndef clean(tweet):\n    \n    # Punctuations at the start or end of words    \n    #for punctuation in \"#@!?()[]*%\":\n    #    tweet = tweet.replace(punctuation, f' {punctuation} ').strip()\n        \n    #tweet = tweet.replace('...', ' ... ').strip()\n    #tweet = tweet.replace(\"'\", \" ' \").strip()        \n    \n    # Special characters\n    tweet = re.sub(r\"\\x89Û_\", \"\", tweet)\n    tweet = re.sub(r\"\\x89ÛÒ\", \"\", tweet)\n    tweet = re.sub(r\"\\x89ÛÓ\", \"\", tweet)\n    tweet = re.sub(r\"\\x89ÛÏWhen\", \"When\", tweet)\n    tweet = re.sub(r\"\\x89ÛÏ\", \"\", tweet)\n    tweet = re.sub(r\"China\\x89Ûªs\", \"China's\", tweet)\n    tweet = re.sub(r\"let\\x89Ûªs\", \"let's\", tweet)\n    tweet = re.sub(r\"\\x89Û÷\", \"\", tweet)\n    tweet = re.sub(r\"\\x89Ûª\", \"\", tweet)\n    tweet = re.sub(r\"\\x89Û\\x9d\", \"\", tweet)\n    tweet = re.sub(r\"å_\", \"\", tweet)\n    tweet = re.sub(r\"\\x89Û¢\", \"\", tweet)\n    tweet = re.sub(r\"\\x89Û¢åÊ\", \"\", tweet)\n    tweet = re.sub(r\"fromåÊwounds\", \"from wounds\", tweet)\n    tweet = re.sub(r\"åÊ\", \"\", tweet)\n    tweet = re.sub(r\"åÈ\", \"\", tweet)\n    tweet = re.sub(r\"JapÌ_n\", \"Japan\", tweet)    \n    tweet = re.sub(r\"Ì©\", \"e\", tweet)\n    tweet = re.sub(r\"å¨\", \"\", tweet)\n    tweet = re.sub(r\"SuruÌ¤\", \"Suruc\", tweet)\n    \n    # Contractions\n    tweet = re.sub(r\"he's\", \"he is\", tweet)\n    tweet = re.sub(r\"there's\", \"there is\", tweet)\n    tweet = re.sub(r\"We're\", \"We are\", tweet)\n    tweet = re.sub(r\"That's\", \"That is\", tweet)\n    tweet = re.sub(r\"won't\", \"will not\", tweet)\n    tweet = re.sub(r\"they're\", \"they are\", tweet)\n    tweet = re.sub(r\"Can't\", \"Cannot\", tweet)\n    tweet = re.sub(r\"wasn't\", \"was not\", tweet)\n    tweet = re.sub(r\"don\\x89Ûªt\", \"do not\", tweet)\n    tweet = re.sub(r\"aren't\", \"are not\", tweet)\n    tweet = re.sub(r\"isn't\", \"is not\", tweet)\n    tweet = re.sub(r\"What's\", \"What is\", tweet)\n    tweet = re.sub(r\"haven't\", \"have not\", tweet)\n    tweet = re.sub(r\"hasn't\", \"has not\", tweet)\n    tweet = re.sub(r\"There's\", \"There is\", tweet)\n    tweet = re.sub(r\"He's\", \"He is\", tweet)\n    tweet = re.sub(r\"It's\", \"It is\", tweet)\n    tweet = re.sub(r\"You're\", \"You are\", tweet)\n    tweet = re.sub(r\"I'M\", \"I am\", tweet)\n    tweet = re.sub(r\"shouldn't\", \"should not\", tweet)\n    tweet = re.sub(r\"wouldn't\", \"would not\", tweet)\n    tweet = re.sub(r\"i'm\", \"I am\", tweet)\n    tweet = re.sub(r\"I\\x89Ûªm\", \"I am\", tweet)\n    tweet = re.sub(r\"I'm\", \"I am\", tweet)\n    tweet = re.sub(r\"Isn't\", \"is not\", tweet)\n    tweet = re.sub(r\"Here's\", \"Here is\", tweet)\n    tweet = re.sub(r\"you've\", \"you have\", tweet)\n    tweet = re.sub(r\"you\\x89Ûªve\", \"you have\", tweet)\n    tweet = re.sub(r\"we're\", \"we are\", tweet)\n    tweet = re.sub(r\"what's\", \"what is\", tweet)\n    tweet = re.sub(r\"couldn't\", \"could not\", tweet)\n    tweet = re.sub(r\"we've\", \"we have\", tweet)\n    tweet = re.sub(r\"it\\x89Ûªs\", \"it is\", tweet)\n    tweet = re.sub(r\"doesn\\x89Ûªt\", \"does not\", tweet)\n    tweet = re.sub(r\"It\\x89Ûªs\", \"It is\", tweet)\n    tweet = re.sub(r\"Here\\x89Ûªs\", \"Here is\", tweet)\n    tweet = re.sub(r\"who's\", \"who is\", tweet)\n    tweet = re.sub(r\"I\\x89Ûªve\", \"I have\", tweet)\n    tweet = re.sub(r\"y'all\", \"you all\", tweet)\n    tweet = re.sub(r\"can\\x89Ûªt\", \"cannot\", tweet)\n    tweet = re.sub(r\"would've\", \"would have\", tweet)\n    tweet = re.sub(r\"it'll\", \"it will\", tweet)\n    tweet = re.sub(r\"we'll\", \"we will\", tweet)\n    tweet = re.sub(r\"wouldn\\x89Ûªt\", \"would not\", tweet)\n    tweet = re.sub(r\"We've\", \"We have\", tweet)\n    tweet = re.sub(r\"he'll\", \"he will\", tweet)\n    tweet = re.sub(r\"Y'all\", \"You all\", tweet)\n    tweet = re.sub(r\"Weren't\", \"Were not\", tweet)\n    tweet = re.sub(r\"Didn't\", \"Did not\", tweet)\n    tweet = re.sub(r\"they'll\", \"they will\", tweet)\n    tweet = re.sub(r\"they'd\", \"they would\", tweet)\n    tweet = re.sub(r\"DON'T\", \"DO NOT\", tweet)\n    tweet = re.sub(r\"That\\x89Ûªs\", \"That is\", tweet)\n    tweet = re.sub(r\"they've\", \"they have\", tweet)\n    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n    tweet = re.sub(r\"should've\", \"should have\", tweet)\n    tweet = re.sub(r\"You\\x89Ûªre\", \"You are\", tweet)\n    tweet = re.sub(r\"where's\", \"where is\", tweet)\n    tweet = re.sub(r\"Don\\x89Ûªt\", \"Do not\", tweet)\n    tweet = re.sub(r\"we'd\", \"we would\", tweet)\n    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n    tweet = re.sub(r\"weren't\", \"were not\", tweet)\n    tweet = re.sub(r\"They're\", \"They are\", tweet)\n    tweet = re.sub(r\"Can\\x89Ûªt\", \"Cannot\", tweet)\n    tweet = re.sub(r\"you\\x89Ûªll\", \"you will\", tweet)\n    tweet = re.sub(r\"I\\x89Ûªd\", \"I would\", tweet)\n    tweet = re.sub(r\"let's\", \"let us\", tweet)\n    \n    # Character entity references\n    tweet = re.sub(r\"&gt;\", \">\", tweet)\n    tweet = re.sub(r\"&lt;\", \"<\", tweet)\n    tweet = re.sub(r\"&amp;\", \"&\", tweet)\n        \n    # Typos, slang and informal abbreviations\n    tweet = re.sub(r\"w/e\", \"whatever\", tweet)\n    tweet = re.sub(r\"w/\", \"with\", tweet)\n    tweet = re.sub(r\"USAgov\", \"USA government\", tweet)\n    tweet = re.sub(r\"recentlu\", \"recently\", tweet)\n    tweet = re.sub(r\"Ph0tos\", \"Photos\", tweet)\n    tweet = re.sub(r\"exp0sed\", \"exposed\", tweet)\n    tweet = re.sub(r\"<3\", \"love\", tweet)\n    tweet = re.sub(r\"amageddon\", \"armageddon\", tweet)\n    tweet = re.sub(r\"Trfc\", \"Traffic\", tweet)\n    tweet = re.sub(r\"8/5/2015\", \"2015-08-05\", tweet)\n    tweet = re.sub(r\"chest/torso\", \"chest / torso\", tweet)\n    tweet = re.sub(r\"WindStorm\", \"Wind Storm\", tweet)\n    tweet = re.sub(r\"8/6/2015\", \"2015-08-06\", tweet)\n    tweet = re.sub(r\"10:38PM\", \"10:38 PM\", tweet)\n    tweet = re.sub(r\"10:30pm\", \"10:30 PM\", tweet)\n    \n    # Separating other punctuations\n    tweet = re.sub(r\"MH370:\", \"MH370 :\", tweet)\n    tweet = re.sub(r\"PM:\", \"Prime Minister :\", tweet)\n    tweet = re.sub(r\"Legionnaires:\", \"Legionnaires :\", tweet)\n    tweet = re.sub(r\"Latest:\", \"Latest :\", tweet)\n    tweet = re.sub(r\"Crash:\", \"Crash :\", tweet)\n    tweet = re.sub(r\"News:\", \"News :\", tweet)\n    tweet = re.sub(r\"derailment:\", \"derailment :\", tweet)\n    tweet = re.sub(r\"attack:\", \"attack :\", tweet)\n    tweet = re.sub(r\"Saipan:\", \"Saipan :\", tweet)\n    tweet = re.sub(r\"Photo:\", \"Photo :\", tweet)\n    tweet = re.sub(r\"Funtenna:\", \"Funtenna :\", tweet)\n    tweet = re.sub(r\"quiz:\", \"quiz :\", tweet)\n    tweet = re.sub(r\"VIDEO:\", \"VIDEO :\", tweet)\n    tweet = re.sub(r\"MP:\", \"MP :\", tweet)\n    tweet = re.sub(r\"UTC2015-08-05\", \"UTC 2015-08-05\", tweet)\n    tweet = re.sub(r\"California:\", \"California :\", tweet)\n    tweet = re.sub(r\"horror:\", \"horror :\", tweet)\n    tweet = re.sub(r\"Past:\", \"Past :\", tweet)\n    tweet = re.sub(r\"Time2015-08-06\", \"Time 2015-08-06\", tweet)\n    tweet = re.sub(r\"here:\", \"here :\", tweet)\n    tweet = re.sub(r\"fires.\", \"fires .\", tweet)\n    tweet = re.sub(r\"Forest:\", \"Forest :\", tweet)\n    tweet = re.sub(r\"Cramer:\", \"Cramer :\", tweet)\n    tweet = re.sub(r\"Chile:\", \"Chile :\", tweet)\n    tweet = re.sub(r\"link:\", \"link :\", tweet)\n    tweet = re.sub(r\"crash:\", \"crash :\", tweet)\n    tweet = re.sub(r\"Video:\", \"Video :\", tweet)\n    tweet = re.sub(r\"Bestnaijamade:\", \"bestnaijamade :\", tweet)\n    tweet = re.sub(r\"NWS:\", \"National Weather Service :\", tweet)\n    tweet = re.sub(r\".caught\", \". caught\", tweet)\n    tweet = re.sub(r\"Hobbit:\", \"Hobbit :\", tweet)\n    tweet = re.sub(r\"2015:\", \"2015 :\", tweet)\n    tweet = re.sub(r\"post:\", \"post :\", tweet)\n    tweet = re.sub(r\"BREAKING:\", \"BREAKING :\", tweet)\n    tweet = re.sub(r\"Island:\", \"Island :\", tweet)\n    tweet = re.sub(r\"Med:\", \"Med :\", tweet)\n    tweet = re.sub(r\"97/Georgia\", \"97 / Georgia\", tweet)\n    tweet = re.sub(r\"Here:\", \"Here :\", tweet)\n    tweet = re.sub(r\"horror;\", \"horror ;\", tweet)\n    tweet = re.sub(r\"people;\", \"people ;\", tweet)\n    tweet = re.sub(r\"refugees;\", \"refugees ;\", tweet)\n    tweet = re.sub(r\"Genocide;\", \"Genocide ;\", tweet)\n    tweet = re.sub(r\".POTUS\", \". POTUS\", tweet)\n    tweet = re.sub(r\"Collision-No\", \"Collision - No\", tweet)\n    tweet = re.sub(r\"Rear-\", \"Rear -\", tweet)\n    tweet = re.sub(r\"Broadway:\", \"Broadway :\", tweet)\n    tweet = re.sub(r\"Correction:\", \"Correction :\", tweet)\n    tweet = re.sub(r\"UPDATE:\", \"UPDATE :\", tweet)\n    tweet = re.sub(r\"Times:\", \"Times :\", tweet)\n    tweet = re.sub(r\"RT:\", \"RT :\", tweet)\n    tweet = re.sub(r\"Police:\", \"Police :\", tweet)\n    tweet = re.sub(r\"Training:\", \"Training :\", tweet)\n    tweet = re.sub(r\"Hawaii:\", \"Hawaii :\", tweet)\n    tweet = re.sub(r\"Selfies:\", \"Selfies :\", tweet)\n    tweet = re.sub(r\"Content:\", \"Content :\", tweet)\n    tweet = re.sub(r\"101:\", \"101 :\", tweet)\n    tweet = re.sub(r\"story:\", \"story :\", tweet)\n    tweet = re.sub(r\"injured:\", \"injured :\", tweet)\n    tweet = re.sub(r\"poll:\", \"poll :\", tweet)\n    tweet = re.sub(r\"Guide:\", \"Guide :\", tweet)\n    tweet = re.sub(r\"Update:\", \"Update :\", tweet)\n    tweet = re.sub(r\"alarm:\", \"alarm :\", tweet)\n    tweet = re.sub(r\"floods:\", \"floods :\", tweet)\n    tweet = re.sub(r\"Flood:\", \"Flood :\", tweet)\n    tweet = re.sub(r\"MH370;\", \"MH370 ;\", tweet)\n    tweet = re.sub(r\"life:\", \"life :\", tweet)\n    tweet = re.sub(r\"crush:\", \"crush :\", tweet)\n    tweet = re.sub(r\"now:\", \"now :\", tweet)\n    tweet = re.sub(r\"Vote:\", \"Vote :\", tweet)\n    tweet = re.sub(r\"Catastrophe.\", \"Catastrophe .\", tweet)\n    tweet = re.sub(r\"library:\", \"library :\", tweet)\n    tweet = re.sub(r\"Bush:\", \"Bush :\", tweet)\n    tweet = re.sub(r\";ACCIDENT\", \"; ACCIDENT\", tweet)\n    tweet = re.sub(r\"accident:\", \"accident :\", tweet)\n    tweet = re.sub(r\"Taiwan;\", \"Taiwan ;\", tweet)\n    tweet = re.sub(r\"Map:\", \"Map :\", tweet)\n    tweet = re.sub(r\"failure:\", \"failure :\", tweet)\n    tweet = re.sub(r\"150-Foot\", \"150 - Foot\", tweet)\n    tweet = re.sub(r\"failure:\", \"failure :\", tweet)\n    tweet = re.sub(r\"prefer:\", \"prefer :\", tweet)\n    tweet = re.sub(r\"CNN:\", \"CNN :\", tweet)\n    tweet = re.sub(r\"Oops:\", \"Oops :\", tweet)\n    tweet = re.sub(r\"Disco:\", \"Disco :\", tweet)\n    tweet = re.sub(r\"Disease:\", \"Disease :\", tweet)\n    tweet = re.sub(r\"Grows:\", \"Grows :\", tweet)\n    tweet = re.sub(r\"projected:\", \"projected :\", tweet)\n    tweet = re.sub(r\"Pakistan.\", \"Pakistan .\", tweet)\n    tweet = re.sub(r\"ministers:\", \"ministers :\", tweet)\n    tweet = re.sub(r\"Photos:\", \"Photos :\", tweet)\n    tweet = re.sub(r\"Disease:\", \"Disease :\", tweet)\n    tweet = re.sub(r\"pres:\", \"press :\", tweet)\n    tweet = re.sub(r\"winds.\", \"winds .\", tweet)\n    tweet = re.sub(r\"MPH.\", \"MPH .\", tweet)\n    tweet = re.sub(r\"PHOTOS:\", \"PHOTOS :\", tweet)\n    tweet = re.sub(r\"Time2015-08-05\", \"Time 2015-08-05\", tweet)\n    tweet = re.sub(r\"Denmark:\", \"Denmark :\", tweet)\n    tweet = re.sub(r\"Articles:\", \"Articles :\", tweet)\n    tweet = re.sub(r\"Crash:\", \"Crash :\", tweet)\n    tweet = re.sub(r\"casualties.:\", \"casualties .:\", tweet)\n    tweet = re.sub(r\"Afghanistan:\", \"Afghanistan :\", tweet)\n    tweet = re.sub(r\"Day:\", \"Day :\", tweet)\n    tweet = re.sub(r\"AVERTED:\", \"AVERTED :\", tweet)\n    tweet = re.sub(r\"sitting:\", \"sitting :\", tweet)\n    tweet = re.sub(r\"Multiplayer:\", \"Multiplayer :\", tweet)\n    tweet = re.sub(r\"Kaduna:\", \"Kaduna :\", tweet)\n    tweet = re.sub(r\"favorite:\", \"favorite :\", tweet)\n    tweet = re.sub(r\"home:\", \"home :\", tweet)\n    tweet = re.sub(r\"just:\", \"just :\", tweet)\n    tweet = re.sub(r\"Collision-1141\", \"Collision - 1141\", tweet)\n    tweet = re.sub(r\"County:\", \"County :\", tweet)\n    tweet = re.sub(r\"Duty:\", \"Duty :\", tweet)\n    tweet = re.sub(r\"page:\", \"page :\", tweet)\n    tweet = re.sub(r\"Attack:\", \"Attack :\", tweet)\n    tweet = re.sub(r\"Minecraft:\", \"Minecraft :\", tweet)\n    tweet = re.sub(r\"wounds;\", \"wounds ;\", tweet)\n    tweet = re.sub(r\"Shots:\", \"Shots :\", tweet)\n    tweet = re.sub(r\"shots:\", \"shots :\", tweet)\n    tweet = re.sub(r\"Gunfire:\", \"Gunfire :\", tweet)\n    tweet = re.sub(r\"hike:\", \"hike :\", tweet)\n    tweet = re.sub(r\"Email:\", \"Email :\", tweet)\n    tweet = re.sub(r\"System:\", \"System :\", tweet)\n    tweet = re.sub(r\"Radio:\", \"Radio :\", tweet)\n    tweet = re.sub(r\"King:\", \"King :\", tweet)\n    tweet = re.sub(r\"upheaval:\", \"upheaval :\", tweet)\n    tweet = re.sub(r\"tragedy;\", \"tragedy ;\", tweet)\n    tweet = re.sub(r\"HERE:\", \"HERE :\", tweet)\n    tweet = re.sub(r\"terrorism:\", \"terrorism :\", tweet)\n    tweet = re.sub(r\"police:\", \"police :\", tweet)\n    tweet = re.sub(r\"Mosque:\", \"Mosque :\", tweet)\n    tweet = re.sub(r\"Rightways:\", \"Rightways :\", tweet)\n    tweet = re.sub(r\"Brooklyn:\", \"Brooklyn :\", tweet)\n    tweet = re.sub(r\"Arrived:\", \"Arrived :\", tweet)\n    tweet = re.sub(r\"Home:\", \"Home :\", tweet)\n    tweet = re.sub(r\"Earth:\", \"Earth :\", tweet)\n    tweet = re.sub(r\"three:\", \"three :\", tweet)\n    \n    # Hashtags and usernames\n    tweet = re.sub(r\"IranDeal\", \"Iran Deal\", tweet)\n    tweet = re.sub(r\"ArianaGrande\", \"Ariana Grande\", tweet)\n    tweet = re.sub(r\"camilacabello97\", \"camila cabello\", tweet) \n    tweet = re.sub(r\"RondaRousey\", \"Ronda Rousey\", tweet)     \n    tweet = re.sub(r\"MTVHottest\", \"MTV Hottest\", tweet)\n    tweet = re.sub(r\"TrapMusic\", \"Trap Music\", tweet)\n    tweet = re.sub(r\"ProphetMuhammad\", \"Prophet Muhammad\", tweet)\n    tweet = re.sub(r\"PantherAttack\", \"Panther Attack\", tweet)\n    tweet = re.sub(r\"StrategicPatience\", \"Strategic Patience\", tweet)\n    tweet = re.sub(r\"socialnews\", \"social news\", tweet)\n    tweet = re.sub(r\"NASAHurricane\", \"NASA Hurricane\", tweet)\n    tweet = re.sub(r\"onlinecommunities\", \"online communities\", tweet)\n    tweet = re.sub(r\"humanconsumption\", \"human consumption\", tweet)\n    tweet = re.sub(r\"Typhoon-Devastated\", \"Typhoon Devastated\", tweet)\n    tweet = re.sub(r\"Meat-Loving\", \"Meat Loving\", tweet)\n    tweet = re.sub(r\"facialabuse\", \"facial abuse\", tweet)\n    tweet = re.sub(r\"LakeCounty\", \"Lake County\", tweet)\n    tweet = re.sub(r\"BeingAuthor\", \"Being Author\", tweet)\n    tweet = re.sub(r\"withheavenly\", \"with heavenly\", tweet)\n    tweet = re.sub(r\"thankU\", \"thank you\", tweet)\n    tweet = re.sub(r\"iTunesMusic\", \"iTunes Music\", tweet)\n    tweet = re.sub(r\"OffensiveContent\", \"Offensive Content\", tweet)\n    tweet = re.sub(r\"WorstSummerJob\", \"Worst Summer Job\", tweet)\n    tweet = re.sub(r\"HarryBeCareful\", \"Harry Be Careful\", tweet)\n    tweet = re.sub(r\"NASASolarSystem\", \"NASA Solar System\", tweet)\n    tweet = re.sub(r\"animalrescue\", \"animal rescue\", tweet)\n    tweet = re.sub(r\"KurtSchlichter\", \"Kurt Schlichter\", tweet)\n    tweet = re.sub(r\"aRmageddon\", \"armageddon\", tweet)\n    tweet = re.sub(r\"Throwingknifes\", \"Throwing knives\", tweet)\n    tweet = re.sub(r\"GodsLove\", \"God's Love\", tweet)\n    tweet = re.sub(r\"bookboost\", \"book boost\", tweet)\n    tweet = re.sub(r\"ibooklove\", \"I book love\", tweet)\n    tweet = re.sub(r\"NestleIndia\", \"Nestle India\", tweet)\n    tweet = re.sub(r\"realDonaldTrump\", \"Donald Trump\", tweet)\n    tweet = re.sub(r\"DavidVonderhaar\", \"David Vonderhaar\", tweet)\n    tweet = re.sub(r\"CecilTheLion\", \"Cecil The Lion\", tweet)\n    tweet = re.sub(r\"weathernetwork\", \"weather network\", tweet)\n    tweet = re.sub(r\"withBioterrorism&use\", \"with Bioterrorism & use\", tweet)\n    tweet = re.sub(r\"Hostage&2\", \"Hostage & 2\", tweet)\n    tweet = re.sub(r\"GOPDebate\", \"GOP Debate\", tweet)\n    tweet = re.sub(r\"RickPerry\", \"Rick Perry\", tweet)\n    tweet = re.sub(r\"frontpage\", \"front page\", tweet)\n    tweet = re.sub(r\"NewsInTweets\", \"News In Tweets\", tweet)\n    tweet = re.sub(r\"ViralSpell\", \"Viral Spell\", tweet)\n    tweet = re.sub(r\"til_now\", \"until now\", tweet)\n    tweet = re.sub(r\"volcanoinRussia\", \"volcano in Russia\", tweet)\n    tweet = re.sub(r\"ZippedNews\", \"Zipped News\", tweet)\n    tweet = re.sub(r\"MicheleBachman\", \"Michele Bachman\", tweet)\n    tweet = re.sub(r\"53inch\", \"53 inch\", tweet)\n    tweet = re.sub(r\"KerrickTrial\", \"Kerrick Trial\", tweet)\n    tweet = re.sub(r\"abstorm\", \"Alberta Storm\", tweet)\n    tweet = re.sub(r\"Beyhive\", \"Beyonce hive\", tweet)\n    tweet = re.sub(r\"IDFire\", \"Idaho Fire\", tweet)\n    tweet = re.sub(r\"DETECTADO\", \"Detected\", tweet)\n    tweet = re.sub(r\"RockyFire\", \"Rocky Fire\", tweet)\n    tweet = re.sub(r\"Listen/Buy\", \"Listen / Buy\", tweet)\n    tweet = re.sub(r\"NickCannon\", \"Nick Cannon\", tweet)\n    tweet = re.sub(r\"FaroeIslands\", \"Faroe Islands\", tweet)\n    tweet = re.sub(r\"yycstorm\", \"Calgary Storm\", tweet)\n    tweet = re.sub(r\"IDPs:\", \"Internally Displaced People :\", tweet)\n    tweet = re.sub(r\"ArtistsUnited\", \"Artists United\", tweet)\n    tweet = re.sub(r\"ClaytonBryant\", \"Clayton Bryant\", tweet)\n    tweet = re.sub(r\"jimmyfallon\", \"jimmy fallon\", tweet)\n    \n    return tweet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import STOPWORDS\n\nstopwords = [\"they've\", 'k', 'whom', \"he'll\", 'could', 'itself', 'hence',  \"when's\", 'where', 'through',\n             'was', 'its', 'into', 'however', \"she'd\", \"we'd\", 'they', 'below', 'again', \"she'll\", \"he's\",\n             'did', 'my', 'are', 'our', \"where's\", 'above', 'ever', 'yourself', \"i'd\", 'just', 'we', \"i'll\",\n             'it', 'from', \"we'll\", 'that', 'he', 'cannot', \"you're\", 'her', 'this', \"why's\", 'once',\n             'am', 'ourselves', 'out', 'get', 'would', 'up', \"it's\", 'same', 'these',  \"you've\", 'such', 'between',\n             'himself', 'also', \"that's\", 'r', 'www', 'all', \"what's\", 'if', 'http', 'herself',   'after', 'had',\n             'has', 'your', 'while', 'other', 'their', 'shall', 'more', 'off', 'as', 'hers', 'with',   'over',\n             'by',  'there', \"we're\", \"they'll\", 'any', 'to',  'no',  'about',  'both', \"he'd\", 'only', 'here', \n             'than', 'what', 'been', 'does', \"we've\", 'theirs', 'being', 'ought', \"they'd\", 'few', 'you', 'under', \n             'since',  'can', 'them', 'at', 'else', 'each', 'ours', 'therefore', 'most', 'before', 'then', 'his', 'me',\n             'a', 'further', \"how's\", 'during', 'of', 'like', 'on',  'themselves', 'why', 'those', 'in', 'too', 'she',\n             'because', \"i've\", \"let's\", 'how', 'very', \"you'd\", 'own', 'but', \"she's\", 'i', 'yourselves', 'down', 'should',\n             'and', 'do', 'or', 'were', 'some', 'an', \"who's\", 'otherwise', 'be', 'him', 'myself', 'have', 'which', \"there's\",\n             \"i'm\", 'when', 'doing', \"you'll\", 'com', 'for', 'who', 'yours', 'until', 'the', 'is', 'so', \"they're\", \"here's\", \n             'nor', 'having', 'will', 'may', 'one', 'now']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocessor2(text): \n    text = text.replace('%20',' ')\n    text = text.lower()\n    text = text.replace(\"n't\",\"nt\")\n    text = re.sub(r\"\\s\\w\\s\", \"\", text)\n    text = re.sub(r\"\\s\\d\\s\", \"\", text)\n    #emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',text)\n    #text = (re.sub('[^a-zA-Z0-9_]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', ''))\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a= \"i don't like you 2 a r!\"\na=preprocessor2(a)\nprint(a)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['text'] = train_df['text'].apply(lambda x:remove_url(x))\ntrain_df['keyword_cleaned'] = train_df['keyword'].copy().apply(lambda x : clean(str(x))).apply(lambda x : preprocessor2(x)).apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))\ntrain_df['location_cleaned'] = train_df['location'].copy().apply(lambda x : clean(str(x))).apply(lambda x : preprocessor2(x)).apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))\ntrain_df['text_cleaned'] = train_df['text'].copy().apply(lambda x : clean(str(x))).apply(lambda x : preprocessor2(x)).apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df['text'] = test_df['text'].apply(lambda x:remove_url(x))\ntest_df['keyword_cleaned'] = test_df['keyword'].copy().apply(lambda x : clean(str(x))).apply(lambda x : preprocessor2(x)).apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))\ntest_df['location_cleaned'] = test_df['location'].copy().apply(lambda x : clean(str(x))).apply(lambda x : preprocessor2(x)).apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))\ntest_df['text_cleaned'] = test_df['text'].copy().apply(lambda x : clean(str(x))).apply(lambda x : preprocessor2(x)).apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Understanding of the training dataset - basic","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Target count\nfig, ax = plt.subplots(figsize = (8,5))\npd.value_counts(train_df['target']).plot(kind=\"bar\")\nax.set_title('Target Count')\nax.set_ylabel('Frequency')\nax.grid(True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Top 20 Locations with most target occurance\nloc_pos = train_df[(train_df.location_cleaned != 'nan') & (train_df.location_cleaned != '') & (train_df['target'] == 1)]['location_cleaned'].value_counts()\nloc_neg = train_df[(train_df.location_cleaned != 'nan') & (train_df.location_cleaned != '') & (train_df['target'] == 0)]['location_cleaned'].value_counts()\n\nloc_pos_dict = loc_pos[:20].to_dict()\nloc_neg_dict = loc_neg[:20].to_dict()\n\nnames0 = list(loc_neg_dict.keys())\nvalues0 = list(loc_neg_dict.values())\nnames1 = list(loc_pos_dict.keys())\nvalues1 = list(loc_pos_dict.values())\n\n#Graph\nfig, (ax1, ax2) = plt.subplots(figsize = (20,5), nrows=1, ncols=2)\n\nax1.bar(range(len(loc_pos_dict)),values1,tick_label=names1)\nax1.set_xticklabels(names1, rotation=\"vertical\")\nax1.set_ylim(0, 100)\nax1.grid(True)\nax1.set_title('Location with most Pos target')\nax1.set_ylabel('Frequency')\n\nax2.bar(range(len(loc_neg_dict)),values0,tick_label=names0)\nax2.set_xticklabels(names0, rotation=\"vertical\")\nax2.set_ylim(0, 100)\nax2.grid(True)\nax2.set_title('Location with most Neg target')\nax2.set_ylabel('Frequency')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#As we can see some same meaning words using different abbreviation, so that we try to make a function to align these words\ndef preprocessor3(text):\n    text = re.sub(r'^washington d c ', \"washington dc\", text)\n    text = re.sub(r'^washington +[\\w]*', \"washington dc\", text)\n    text = re.sub(r'^new york +[\\w]*', \"new york\", text)\n    text = re.sub(r'^nyc$', \"new york\", text)\n    text = re.sub(r'^chicago +[\\w]*', \"chicago\", text)\n    text = re.sub(r'^california +[\\w]*', \"california\", text)\n    text = re.sub(r'^los angeles +[\\w]*', \"los angeles\", text)\n    text = re.sub(r'^san francisco +[\\w]*', \"san francisco\", text)\n    text = re.sub(r'^london +[\\w]*', \"london\", text)\n    text = re.sub(r'^usa$', \"united states\", text)\n    text = re.sub(r'^us$', \"united states\", text)\n    text = re.sub(r'^uk$', \"united kingdom\", text)\n    \n    return text\n\ndef preprocessor4(text):\n    abb = ['ak', 'al', 'az', 'ar', 'ca', 'co',\n           'ct', 'de', 'dc', 'fl', 'ga', 'hi',\n           'id', 'il', 'in', 'ia', 'ks', 'ky',\n           'la', 'me', 'mt', 'ne', 'nv', 'nh',\n           'nj', 'nm', 'ny', 'nc', 'nd', 'oh',\n           'ok', 'or', 'md', 'ma', 'mi', 'mn',\n           'ms', 'mo', 'pa', 'ri', 'sc', 'sd',\n           'tn', 'tx', 'ut', 'vt', 'va', 'wa',\n           'wv', 'wi', 'wy']\n    \n    for i in abb:\n        text = re.sub(r'^{0}$'.format(i), '', text)\n        \n    return text ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['location_cleaned'] = train_df['location_cleaned'].copy().apply(lambda x : preprocessor3(x)).apply(lambda x : preprocessor4(x))\ntrain_df['text_cleaned'] = train_df['text_cleaned'].copy().apply(lambda x : preprocessor3(x)).apply(lambda x : preprocessor4(x))\n\ntest_df['location_cleaned'] = test_df['location_cleaned'].copy().apply(lambda x : preprocessor3(x)).apply(lambda x : preprocessor4(x))\ntest_df['text_cleaned'] = test_df['text_cleaned'].copy().apply(lambda x : preprocessor3(x)).apply(lambda x : preprocessor4(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Understanding of the training dataset - word cloud visualization","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"loc_pos_dict = loc_pos[:].to_dict()\nloc_neg_dict = loc_neg[:].to_dict()\n\nloc_list = list(loc_pos_dict.keys()) + list(loc_neg_dict.keys())\nunique_loc = []\nlower_loc = []\nfor x in loc_list:\n    if x not in unique_loc:\n        unique_loc.append(x)\n        \nfor x in unique_loc:\n    lower_loc.append(x.lower().replace(',','').replace('.',''))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud,ImageColorGenerator\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n#stopwords = list(STOPWORDS)+['will','may','one','now','nan','don'] #+ lower_loc\n\nclass wc_base2:\n    def __init__(self, data):\n        self.temp = data.apply(lambda x: ' '.join([word for word in x.split()]))\n        self.text = \" \".join(word for word in data)\n        self.wordlist = []\n        \n    def plot_wc(self, mask=None, max_words=200, figure_size=(20,10), title=None, stopwords=stopwords):\n\n        print (\"There are {} words in the combination of all review.\".format(len(self.text)))\n\n        wordcloud = WordCloud(background_color='black',\n                        stopwords=stopwords,\n                        max_words = max_words,\n                        collocations=False,\n                        random_state = 10,\n                        width = 800,\n                        height =400)\n\n        wordcloud.generate(self.text)\n         \n        self.wordlist = list(wordcloud.words_.keys())\n\n        plt.figure(figsize=figure_size)\n        plt.imshow(wordcloud)\n        plt.title(title)\n        plt.axis(\"off\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(stopwords)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wc = wc_base2(train_df[train_df.target == 0].text_cleaned)\nwc.plot_wc(title=\"Word Cloud of tweets with Negative target\")\nwc_s = set(wc.wordlist)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wc2 = wc_base2(train_df[train_df.target == 1].text_cleaned)\nwc2.plot_wc(title=\"Word Cloud of tweets with Positive target\")\nwc2_s = set(wc2.wordlist)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['text_cleaned'] = train_df['text_cleaned'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))\ntest_df['text_cleaned'] = test_df['text_cleaned'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.text_cleaned.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.0 Data Augmentation -- EDA (Random  Swap - RS) and EDA (Random  Deletion - RD) - for training set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['index_no'] = train_df.index\ntrain_df['sent_w_index'] = train_df['text_cleaned'] + ' ' + train_df['index_no'].astype('str')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.sent_w_index[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def random_swap(text):\n    text_list = text.split()\n    seed = int(text_list[-1])\n\n    text_list=text_list[:-1]\n    text_length = len(text_list)\n   \n    np.random.seed(seed)\n    try:\n        a = np.random.randint(0, text_length,size=2)\n    except:\n        return\n    #print(a)\n\n    temp_a = text_list[a[0]]\n    temp_b = text_list[a[1]]\n    \n    text_list[a[0]] = temp_b\n    text_list[a[1]] = temp_a\n    \n    redo = ' '.join([str(i) for i in text_list])\n   \n    return redo    \n\ndef random_del(text):\n    text_list = text.split()\n    seed = int(text_list[-1])\n\n    text_list=text_list[:-1]\n    text_length = len(text_list)\n   \n    np.random.seed(seed)\n    try:\n        a = np.random.randint(0, text_length,size=1)\n    except:\n        return\n    \n    text_list.pop((a[0]))\n    \n    redo = ' '.join([str(i) for i in text_list])\n      \n    return redo    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['da_text_cleaned'] = train_df['sent_w_index'].apply(lambda x:random_swap(x))\ntrain_df['da_text_cleaned2'] = train_df['sent_w_index'].apply(lambda x:random_del(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.drop(['index_no','sent_w_index'], axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#temp_df1 = list(zip(train_df.target,train_df.keyword_cleaned,train_df.location_cleaned,train_df.da_text_cleaned))\n#temp_df2 = list(zip(train_df.target,train_df.keyword_cleaned,train_df.location_cleaned,train_df.da_text_cleaned2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#x = pd.DataFrame(temp_df1, columns =['target','keyword_cleaned','location_cleaned','text_cleaned'])\n#y = pd.DataFrame(temp_df2, columns =['target','keyword_cleaned','location_cleaned','text_cleaned'])\n\n#train_df.drop(['da_text_cleaned','da_text_cleaned2'], axis=1,inplace=True)\n\n#z = pd.concat([train_df,x,y], axis=0, join='outer', ignore_index=False, keys=None, sort = False)\n#z = z[['id','keyword_cleaned','location_cleaned','text_cleaned','target']].copy()\n#z.reset_index(inplace=True, drop=True) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_df = z \ntest_df = test_df[['id','keyword_cleaned','location_cleaned','text_cleaned']].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Randomization\nstate = 1\ntrain_df = train_df.sample(frac=1,random_state=state)\ntrain_df.reset_index(inplace=True, drop=True) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['text_cleaned'] = train_df['text_cleaned'].apply(lambda x : str(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.1 Build a Simple deep learning model  ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence\n\ntop_word = 35000\n\ntext_lengths = [len(x.split()) for x in (train_df.text_cleaned)]\n#text_lengths = [x for x in text_lengths if x < 50]\nplt.hist(text_lengths, bins=25)\nplt.title('Histogram of # of Words in Texts')\n\ntok = Tokenizer(num_words=top_word)\ntok.fit_on_texts((train_df['text_cleaned']+train_df['keyword_cleaned']+train_df['location_cleaned']))\n\nmax_words = max(text_lengths) + 1\nmax_words_ky = max([len(x.split()) for x in (train_df.keyword_cleaned)]) + 1\nmax_words_lc = max([len(x.split()) for x in (train_df.location_cleaned)]) + 1\nprint(\"top_word: \", str(top_word))\nprint(\"max_words: \", str(max_words))\nprint(\"max_words_ky: \", str(max_words_ky))\nprint(\"max_words_lc: \", str(max_words_lc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Training set\n\nX_train_tx = tok.texts_to_sequences(train_df['text_cleaned'])\nX_train_ky = tok.texts_to_sequences(train_df['keyword_cleaned'])\nX_train_lc = tok.texts_to_sequences(train_df['location_cleaned'])\n\nX_test_tx = tok.texts_to_sequences(test_df['text_cleaned'])\nX_test_ky = tok.texts_to_sequences(test_df['keyword_cleaned'])\nX_test_lc = tok.texts_to_sequences(test_df['location_cleaned'])\n\n\nY_train = train_df['target']\n\nprint('Found %s unique tokens.' % len(tok.word_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.utils import to_categorical\n# One-hot category\nY_train = to_categorical(Y_train)\nprint(\"Y_train.shape: \", Y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_tx = sequence.pad_sequences(X_train_tx, maxlen=max_words)\nX_train_ky = sequence.pad_sequences(X_train_ky, maxlen=max_words_ky)\nX_train_lc = sequence.pad_sequences(X_train_lc, maxlen=max_words_lc)\n\nX_test_tx = sequence.pad_sequences(X_test_tx, maxlen=max_words)\nX_test_ky = sequence.pad_sequences(X_test_ky, maxlen=max_words_ky)\nX_test_lc = sequence.pad_sequences(X_test_lc, maxlen=max_words_lc)\n\nprint(\"X_train_tx.shape: \", X_train_tx.shape)\nprint(\"X_train_ky.shape: \", X_train_ky.shape)\nprint(\"X_train_lc.shape: \", X_train_lc.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from numpy import array\nfrom numpy import asarray\nfrom numpy import zeros\n\nembeddings_dictionary = dict()\nglove_file = open('/kaggle/input/glove6b/glove.6B.300d.txt', encoding=\"utf8\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for line in glove_file:\n    records = line.split()\n    word = records[0]\n    vector_dimensions = asarray(records[1:], dtype='float32')\n    \n    embeddings_dictionary [word] = vector_dimensions\n\nglove_file.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_dim = 300\nembedding_matrix = zeros((top_word, embedding_dim))\nfor word, index in tok.word_index.items():\n    embedding_vector = embeddings_dictionary.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[index] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.models import Model\nfrom keras.layers import Dense, Dropout, Embedding, LSTM, Input, Conv2D, MaxPooling2D,Conv1D,MaxPooling1D\nfrom keras.layers import Bidirectional,  Reshape, Flatten, GRU\nfrom keras.layers.merge import concatenate","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Standard Model - action in keywords, locations and texts","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"input1 = Input(shape=(max_words,))\nembedding_layer1 = Embedding(top_word, embedding_dim, weights=[embedding_matrix], input_length=max_words, trainable=False)(input1)\ndropout1 = Dropout(0.2)(embedding_layer1)\nlstm1_1 = LSTM(128,return_sequences = True)(dropout1)\nlstm1_2 = LSTM(128,return_sequences = True)(lstm1_1)\nlstm1_2a = LSTM(128,return_sequences = True)(lstm1_2)\nlstm1_3 = LSTM(128)(lstm1_2a)\n\ninput2 = Input(shape=(max_words_ky,))\nembedding_layer2 = Embedding(top_word, embedding_dim, weights=[embedding_matrix], input_length=max_words_ky, trainable=False)(input2)\ndropout2 = Dropout(0.2)(embedding_layer2)\nlstm2_1 = LSTM(64,return_sequences = True)(dropout2)\nlstm2_2 = LSTM(64,return_sequences = True)(lstm2_1)\nlstm2_3 = LSTM(64)(lstm2_2)\n\ninput3 = Input(shape=(max_words_lc,))\nembedding_layer3 = Embedding(top_word, embedding_dim, weights=[embedding_matrix], input_length=max_words_lc, trainable=False)(input3)\ndropout3 = Dropout(0.2)(embedding_layer3)\nlstm3_1 = LSTM(32,return_sequences = True)(dropout3)\nlstm3_2 = LSTM(32,return_sequences = True)(lstm3_1)\nlstm3_3 = LSTM(32)(lstm3_2)\n\nmerge = concatenate([lstm1_3, lstm2_3,lstm3_3])\n\ndropout = Dropout(0.8)(merge)\ndense1 = Dense(256, activation='relu')(dropout)\ndense2 = Dense(128, activation='relu')(dense1)\noutput = Dense(2, activation='softmax')(dense2)\nmodel1 = Model(inputs=[input1,input2,input3], outputs=output)\nmodel1.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Improved Model -LSTM+CNN - action in keywords, locations and texts","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"input1 = Input(shape=(max_words,))\nembedding_layer1 = Embedding(top_word, embedding_dim, weights=[embedding_matrix], input_length=max_words, trainable=False)(input1)\nlstm1_1 = Bidirectional(LSTM(100, return_sequences=True,dropout = 0.2))(embedding_layer1)\nlstm1_1a = Bidirectional(LSTM(100, return_sequences=True,dropout = 0.2))(lstm1_1)\nlstm1_1b = Bidirectional(LSTM(100, return_sequences=True,dropout = 0.2))(lstm1_1a)\nres = Reshape((-1, X_train_tx.shape[1], 100))(lstm1_1b)\nconv1 = Conv2D(100, (3,3), padding='same',activation=\"relu\")(res)\npool1 = MaxPooling2D(pool_size=(2,2))(conv1)\nflat1 = Flatten()(pool1)\n\ninput2 = Input(shape=(max_words_ky,))\nembedding_layer2 = Embedding(top_word, embedding_dim, weights=[embedding_matrix], input_length=max_words_ky, trainable=False)(input2)\nlstm2_1 = Bidirectional(LSTM(100, return_sequences=True,dropout = 0.2))(embedding_layer2)\nlstm2_1a = Bidirectional(LSTM(100, return_sequences=True,dropout = 0.2))(lstm2_1)\nlstm2_1b = Bidirectional(LSTM(100, return_sequences=True,dropout = 0.2))(lstm2_1a)\nres2 = Reshape((-1, X_train_ky.shape[1], 100))(lstm2_1b)\nconv2 = Conv2D(100, (3,3), padding='same',activation=\"relu\")(res2)\npool2 = MaxPooling2D(pool_size=(2,2))(conv2)\nflat2 = Flatten()(pool2)\n\ninput3 = Input(shape=(max_words_lc,))\nembedding_layer3 = Embedding(top_word, embedding_dim, weights=[embedding_matrix], input_length=max_words_lc, trainable=False)(input3)\nlstm3_1 = Bidirectional(LSTM(100, return_sequences=True,dropout = 0.2))(embedding_layer3)\nlstm3_1a = Bidirectional(LSTM(100, return_sequences=True,dropout = 0.2))(lstm3_1)\nlstm3_1b = Bidirectional(LSTM(100, return_sequences=True,dropout = 0.2))(lstm3_1a)\nres3 = Reshape((-1, X_train_lc.shape[1], 100))(lstm3_1b)\nconv3 = Conv2D(100, (3,3), padding='same',activation=\"relu\")(res3)\npool3 = MaxPooling2D(pool_size=(2,2))(conv3)\nflat3 = Flatten()(pool3)\n\nmerge = concatenate([flat1, flat2, flat3])\n\ndropout = Dropout(0.4)(merge)\ndense1 = Dense(256, activation='relu')(dropout)\ndense2 = Dense(128, activation='relu')(dense1)\noutput = Dense(2, activation='softmax')(dense2)\nmodel2 = Model(inputs=[input1,input2,input3], outputs=output)\nmodel2.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"input1 = Input(shape=(max_words,))\ninput2 = Input(shape=(max_words_ky,))\ninput3 = Input(shape=(max_words_lc,))\n\nmerge = concatenate([input1, input2, input3])\n\nembedding_layer1 = Embedding(top_word, embedding_dim, weights=[embedding_matrix], input_length=42, trainable=False)(merge)\nlstm1_1 = Bidirectional(LSTM(128, return_sequences=True,dropout = 0.2))(embedding_layer1)\nlstm1_1a = Bidirectional(LSTM(128, return_sequences=True,dropout = 0.2))(lstm1_1)\n#lstm1_1b = Bidirectional(LSTM(128, return_sequences=True,dropout = 0.2))(lstm1_1a)\nlstm1_1b = Bidirectional(LSTM(128, return_sequences=True,dropout = 0.2))(lstm1_1a)\n\n#res2 = Reshape((-1, 40, 256))(lstm1_1b)\nconv2 = Conv1D(64, 3, padding='same',activation=\"relu\")(lstm1_1b)\npool2 = MaxPooling1D(pool_size=2)(conv2)\nconv3 = Conv1D(64, 3, padding='same',activation=\"relu\")(pool2)\npool3 = MaxPooling1D(pool_size=2)(conv3)\nflat2 = Flatten()(pool3)\n\ndense1 = Dense(256, activation='relu')(flat2)\ndropout = Dropout(0.8)(dense1)\ndense2 = Dense(128, activation='relu')(dropout)\noutput = Dense(2, activation='softmax')(dense2)\nmodel3 = Model(inputs=[input1,input2,input3], outputs=output)\nmodel3.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.optimizers import Adam","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer1 = Adam(lr = .0001, beta_1 = .9, beta_2 = .999, epsilon = 1e-10, decay = .0, amsgrad = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer2 = Adam(lr = .0001, beta_1 = .9, beta_2 = .999, epsilon = 1e-10, decay = .0, amsgrad = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1.compile(loss=\"binary_crossentropy\", optimizer=optimizer1,\n              metrics=[\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model2.compile(loss=\"binary_crossentropy\", optimizer=optimizer2,\n              metrics=[\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.callbacks import EarlyStopping, ReduceLROnPlateau\nes = EarlyStopping(monitor='val_loss', mode='min',verbose=1, patience = 4)\nlearning_rate_reduction = ReduceLROnPlateau(monitor = 'val_accuracy', patience = 2, verbose = 1, \n                                           factor = 0.5, min_lr = 1e-8, cooldown=1)\n#history = model.fit([X_train_tx,X_train_ky], Y_train, validation_split=0.2, epochs=30, batch_size=64, verbose=2, callbacks=[es])\nhistory = model2.fit([X_train_tx,X_train_ky,X_train_lc], Y_train, validation_split=0.2, epochs=20, batch_size=16, verbose=2, callbacks=[es, learning_rate_reduction])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history2 = model1.fit([X_train_tx,X_train_ky,X_train_lc], Y_train, validation_split=0.2, epochs=20, batch_size=16, verbose=2, callbacks=[es, learning_rate_reduction])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def result_eva (loss,val_loss,acc,val_acc):\n    import matplotlib.pyplot as plt\n    %matplotlib inline\n    \n    epochs = range(1,len(loss)+1)\n    plt.plot(epochs, loss,'b-o', label ='Training Loss')\n    plt.plot(epochs, val_loss,'r-o', label ='Validation Loss')\n    plt.title(\"Training and Validation Loss\")\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.show()\n    \n    epochs = range(1, len(acc)+1)\n    plt.plot(epochs, acc, \"b-o\", label=\"Training Acc\")\n    plt.plot(epochs, val_acc, \"r-o\", label=\"Validation Acc\")\n    plt.title(\"Training and Validation Accuracy\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_eva(history.history['loss'], history.history['val_loss'], history.history['accuracy'], history.history['val_accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_eva(history2.history['loss'], history2.history['val_loss'], history2.history['accuracy'], history2.history['val_accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model2.save('nlp_disaster.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1.save('nlp_disaster2.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import load_model\n\nmodel = Model()\nmodel = load_model('nlp_disaster.h5')\n#model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",metrics=[\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Y_pred = model.predict([X_test_tx,X_test_ky], batch_size=64, verbose=2)\nY_pred = model.predict([X_test_tx,X_test_ky,X_test_lc], batch_size=16, verbose=2)\nY_pred = np.argmax(Y_pred,axis=1)\n\npred_df = pd.DataFrame(Y_pred, columns=['target'])\nresult = pd.concat([test_df,pred_df], axis=1, join='outer', ignore_index=False, keys=None, sort = False)\nresult = result[['id','target']]\nprint(Y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result.to_csv('sample_submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_pred = model1.predict([X_test_tx,X_test_ky,X_test_lc], batch_size=16, verbose=2)\nY_pred = np.argmax(Y_pred,axis=1)\n\npred_df = pd.DataFrame(Y_pred, columns=['target'])\nresult = pd.concat([test_df,pred_df], axis=1, join='outer', ignore_index=False, keys=None, sort = False)\nresult = result[['id','target']]\nprint(Y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result.to_csv('sample_submission2.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}