{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re\nfrom nltk.stem import PorterStemmer\nps = PorterStemmer()\nfrom nltk.corpus import stopwords\nstopwords = set(stopwords.words('english'))\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn import model_selection, naive_bayes, svm\nimport nltk\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# reading train and test files\ntrain = pd.read_csv('../input/nlp-getting-started/train.csv', dtype={'id': np.int16, 'target': np.int8})\ntest = pd.read_csv('../input/nlp-getting-started/test.csv', dtype={'id': np.int16})\nprint ('Done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# basic info about data like shape ,type of variable, null values in different columns\nprint('shape of train dataset is {} columns and {} rows'.format(train.shape[0],train.shape[1]))\nprint('shape of test dataset is {} columns and {} rows'.format(test.shape[0],test.shape[1]))\nprint ('#####################################################################################')\nprint('columns name in train dataset')\nprint(train.columns.tolist())\nprint ('#####################################################################################')\nprint('target column is {}'.format(set(train.columns).difference(set(test.columns))))\nprint ('#####################################################################################')\nprint('variable type of different columns in train dataset')\nprint(train.dtypes)\nprint ('#####################################################################################')\nprint('total null values in train dataset')\nprint(train.isnull().sum().sum())\nprint ('#####################################################################################')\nprint('null values across different columns of train dataset')\nprint(train.isnull().sum())\nprint ('#####################################################################################')\nprint('number of unique values across different columns of train dataset')\nprint(train.nunique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analysis of columns with null values\nprint('% of missing values in columns with null values:')\nfor col in train.columns:\n    pct = []\n    if train[col].isnull().sum() > 0:\n        pct = ((train[col].isnull().sum())*100/len(train)).round(2)\n        #print (pct)\n        print ('percentage of missing value in column \"{}\" is {}%'.format(col,pct))\n\nprint(\"##############################################################################\")\nprint('count values of different items in column with null values')      \nprint(\"##############################################################################\")\nprint('column:keyword')\nprint(train['keyword'].value_counts()[:10])\nprint(\"##############################################################################\")\nprint('column:location')\nprint(train['location'].value_counts()[:10])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#'looking top 5 rows of train dataset to get more insights'\n\nprint('looking top 5 rows of train dataset to get more insights')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##### basic visualisation across different columns of train dataset \n# first we will try to check whether train dataset is balanced or unbalanced dataset\nprint('Distribution of target column')\n\ntgt_count = train['target'].value_counts()\nprint(tgt_count)\nsns.barplot(tgt_count.index, tgt_count)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above barplot signify train dataset to be bit unbalanced as it contain more number of target = 0 (4342) as compared to\ntarget =1 (3271)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# relationship between target and keywords\n# credit for this plot : https://www.kaggle.com/gunesevitan/nlp-with-disaster-tweets-eda-cleaning-and-bert\ntrain['target_mean'] = train.groupby('keyword')['target'].transform('mean')\n\nfig = plt.figure(figsize=(8, 72), dpi=100)\n\nsns.countplot(y=train.sort_values(by='target_mean', ascending=False)['keyword'],\n              hue=train.sort_values(by='target_mean', ascending=False)['target'])\n\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=12)\nplt.legend(loc=1)\nplt.title('Target Distribution in Keywords')\n\nplt.show()\n\ntrain.drop(columns=['target_mean'], inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above plot signifies importance of keyword in telling whether target will be 0 or 1. For some keyword like derailment or debris, target is always 1. And for keyword like aftershock it is always 0.\n\nIt will be important to impute missing values in 'keyword' column"},{"metadata":{"trusted":true},"cell_type":"code","source":"# cleaning text column of train dataset\nimport re\n\ndef decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase\n\ndef cleanhtml (sentence):\n    cleantext = re.sub(r'http\\S+',r'',sentence)\n    return cleantext\n\ndef cleanpunc(sentence):\n    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n    cleaned = re.sub(r'[.|,|)|(|)|\\|/]',r' ',cleaned)      \n    return cleaned\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\nstr1=' '\nfinal_string=[]\ns=''\n\nfor sent in train['text']:\n    filter_sent = []\n    sent = decontracted(sent)\n    sent1 = remove_emoji(sent)\n    rem_html = cleanhtml(sent1)\n    rem_punc = cleanpunc (rem_html)\n    for w in rem_punc.split():\n        if ((w.isalpha())):\n            if (w.lower() not in stopwords):\n#               s=(ps.stem(w.lower())).encode('utf8')\n                s=(w.lower()).encode('utf8')\n                filter_sent.append(s)\n            else:\n                continue\n        else:\n            continue\n    str1 = b\" \".join(filter_sent)\n    final_string.append(str1)\n    \n# attaching column new_col (cleaned text ) to dataframe\ntrain['clean_text'] = np.array(final_string)\ntrain['clean_text'].head(10)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cleaning text column of train dataset\nimport re\n\ndef decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase\n\ndef cleanhtml (sentence):\n    cleantext = re.sub(r'http\\S+',r'',sentence)\n    return cleantext\n\ndef cleanpunc(sentence):\n    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n    cleaned = re.sub(r'[.|,|)|(|)|\\|/]',r' ',cleaned)      \n    return cleaned\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\nstr1=' '\nfinal_string=[]\ns=''\n\nfor sent in test['text']:\n    filter_sent = []\n    sent = decontracted(sent)\n    sent1 = remove_emoji(sent)\n    rem_html = cleanhtml(sent1)\n    rem_punc = cleanpunc (rem_html)\n    for w in rem_punc.split():\n        if ((w.isalpha())):\n            if (w.lower() not in stopwords):\n#               s=(ps.stem(w.lower())).encode('utf8')\n                s=(w.lower()).encode('utf8')\n                filter_sent.append(s)\n            else:\n                continue\n        else:\n            continue\n    str1 = b\" \".join(filter_sent)\n    final_string.append(str1)\n    \n# attaching column new_col (cleaned text ) to dataframe\ntest['clean_text'] = np.array(final_string)\ntest['clean_text'].head(10)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### EDA ########\ndef length(text):    \n    '''a function which returns the length of text'''\n    return len(text)\n\ntrain['tweet_len'] = train['text'].apply(length)\ntest['tweet_len'] = test['text'].apply(length)\n\nplt.figure(figsize=(16,8))\nplt.subplot(1,2,1)\nax = sns.violinplot(x=\"target\", y=\"tweet_len\", data=train)\nplt.subplot(1,2,2)\nsns.distplot(train[train['target']==0]['tweet_len'][0:],label='Not Real')\nsns.distplot(train[train['target']==1]['tweet_len'][0:],label='Real')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"slight variation in # of charcter for tweets with target = 1 and target = 0\nThanks to https://www.kaggle.com/vbmokin/nlp-eda-bag-of-words-tf-idf-glove-bert"},{"metadata":{"trusted":true},"cell_type":"code","source":"#### EDA ########\n\ntrain['word_count'] = train.clean_text.apply(lambda x : len(x.split()))\ntest['word_count'] = test.clean_text.apply(lambda x : len(x.split()))\n\n#train[['word_count','clean_text']].head()\nplt.figure(figsize=(16,8))\nplt.subplot(1,2,1)\nax2 = sns.violinplot(x=\"target\", y='word_count', data=train)\nplt.subplot(1,2,2)\nsns.distplot(train[train['target']==0]['word_count'][0:],label='Not Real')\nsns.distplot(train[train['target']==1]['word_count'][0:],label='Real')\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not much difference in distribution of number of words in tweets with target=1 and target=0"},{"metadata":{"trusted":true},"cell_type":"code","source":"##### mean word length #########################################\ntrain['avg_char_count'] = train.clean_text.apply(lambda x : np.mean([len(w) for w in x.split()]))\ntrain['avg_char_count'].fillna(0,inplace=True)\n\ntest['avg_char_count'] = test.clean_text.apply(lambda x : np.mean([len(w) for w in x.split()]))\ntest['avg_char_count'].fillna(0,inplace=True)\n#train['avg_char_count'].isnull().sum()\n#train['avg_char_count'].head(10)\nplt.figure(figsize=(16,8))\nplt.subplot(1,2,1)\nax2 = sns.violinplot(x=\"target\", y='avg_char_count', data=train)\nplt.subplot(1,2,2)\nsns.distplot(train[train['target']==0]['avg_char_count'][0:],label='Not Real')\nsns.distplot(train[train['target']==1]['avg_char_count'][0:],label='Real')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not much difference in distribution of average words length in tweets with target=1 and target=0"},{"metadata":{"trusted":true},"cell_type":"code","source":"########### eda ##############\n## no. of stopwords\ntrain['num_stopwords'] = train.text.apply(lambda x :len([w for w in x.lower().split() if w in stopwords]))\ntest['num_stopwords'] = test.text.apply(lambda x :len([w for w in x.lower().split() if w in stopwords]))\n\nplt.figure(figsize=(16,8))\nplt.subplot(1,2,1)\nax2 = sns.violinplot(x=\"target\", y='num_stopwords', data=train)\nplt.subplot(1,2,2)\nsns.distplot(train[train['target']==0]['num_stopwords'][0:],label='Not Real')\nsns.distplot(train[train['target']==1]['num_stopwords'][0:],label='Real')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##### url_count in the text\ntrain['url_count'] = train.text.apply(lambda x : len([w for w in str(x).lower().split() if w in 'http' or w in 'https']))\ntest['url_count'] = test.text.apply(lambda x : len([w for w in str(x).lower().split() if w in 'http' or w in 'https']))\n\n#dtrain['url_count'].head(10)\n#dtrain['url_count'].value_counts()\n\nx = train[['target','url_count']].groupby('url_count')['target'].value_counts()\nx","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"train['url_count'] have no predictive power"},{"metadata":{},"cell_type":"markdown","source":"text with target=0 tends to have larger number of stopwords. But distribution shows that this feature may not \nhave contribute much in classification "},{"metadata":{"trusted":true},"cell_type":"code","source":"######## hastag count #############################\ntrain['hash_count'] = train.text.apply(lambda x : len([c for c in x if c in ('#')]))\ntest['hash_count'] = test.text.apply(lambda x : len([c for c in x if c in ('#')]))\n\ntrain['hash_count'].head()\nmax(train['hash_count'])\n\nx = train[['target','hash_count']].groupby('hash_count')['target'].value_counts()\nx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### retweet_count ############\n#import re\n\nretweet_dict = {}\nfor i in range(0,len(train)):\n    txt = train['text'][i]\n    txt1 = cleanhtml(txt)\n    txt2 = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",txt1).split())\n    retweet_dict[txt] = txt2.lower()\n\nf_retweet_dict = {} \nfor key1 in retweet_dict:\n    if key1 not in f_retweet_dict:\n        val = retweet_dict[key1]\n        count = 0\n        for key2 in retweet_dict:\n            if val == retweet_dict[key2]:\n                 count = count+1\n        f_retweet_dict[key1]= count\nprint(\"###################### print first 10 element of dictionary####################\")\nprint({k: f_retweet_dict[k] for k in list(f_retweet_dict)[:10] })\ntrain['repeat_tweet_count'] = train['text'].map(f_retweet_dict)\n#train[['same_tweet_count','text']].head(20)\n#train['same_tweet_count'].isnull().sum()\n\ny = train[['target','repeat_tweet_count']].groupby('repeat_tweet_count')['target'].value_counts()\ny","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hastag count seem to have less predictive power as for each # of hash_count, target 0 and target 1 have similar value count"},{"metadata":{"trusted":true},"cell_type":"code","source":"# credit goes to : https://www.kaggle.com/saipkb86/disaster-tweets-logistic-naive\n# Referenec : https://www.analyticsvidhya.com/blog/2018/07/hands-on-sentiment-analysis-dataset-python/\n\ndef hashtag_extract(x):\n    hashtags = []\n    # Loop over the words in the tweet\n    for i in x:\n        ht = re.findall(r\"#(\\w+)\", i)\n        hashtags.append(ht)\n\n    return hashtags\n\nHT_regular = hashtag_extract(train['text'][train['target'] == 0])\n\n# extracting hashtags from racist/sexist tweets\nHT_disaster = hashtag_extract(train['text'][train['target'] == 1])\n\n# unnesting list\nHT_regular = sum(HT_regular,[])\nHT_disaster = sum(HT_disaster,[])\n\nfig,axes = plt.subplots(2,1,figsize=(18,10))\n\na = nltk.FreqDist(HT_regular)\nd = pd.DataFrame({'Hashtag': list(a.keys()),\n                  'Count': list(a.values())})\n# selecting top 10 most frequent hashtags     \nd = d.nlargest(columns=\"Count\", n = 10) \nplt.figure(figsize=(16,5))\nsns.barplot(data=d, x= \"Hashtag\", y = \"Count\",ax=axes[0]).set_title('Normal Tweets')\n\n\na = nltk.FreqDist(HT_disaster)\nd = pd.DataFrame({'Hashtag': list(a.keys()),\n                  'Count': list(a.values())})\n# selecting top 10 most frequent hashtags     \nd = d.nlargest(columns=\"Count\", n = 10) \nplt.figure(figsize=(16,5))\nsns.barplot(data=d, x= \"Hashtag\", y = \"Count\",ax=axes[1]).set_title('Disaster Tweets')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### retweet_count ############ for test dataset\nretweet_dict_test = {}\nfor i in range(0,len(test)):\n    txt = test['text'][i]\n    txt1 = cleanhtml(txt)\n    txt2 = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",txt1).split())\n    retweet_dict_test[txt] = txt2.lower()\n\nf_retweet_dict_test = {} \nfor key1 in retweet_dict_test:\n    if key1 not in f_retweet_dict_test:\n        val = retweet_dict_test[key1]\n        count = 0\n        for key2 in retweet_dict_test:\n            if val == retweet_dict_test[key2]:\n                 count = count+1\n        f_retweet_dict_test[key1]= count\nprint(\"###################### print first 10 element of dictionary####################\")\nprint({k: f_retweet_dict_test[k] for k in list(f_retweet_dict_test)[:10] })\ntest['repeat_tweet_count'] = test['text'].map(f_retweet_dict_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above table signify that as number of retweet increases, probability of respective target being 1 increases\n\nThis feature seem to have good predictive power..."},{"metadata":{"trusted":true},"cell_type":"code","source":"### adding a column 'imputed_kw' to give information which all train['keyword'] indexes have null values\nids_train = train[train['keyword'].isnull()].index.tolist()\ntrain['imputed_kw'] = 0\nfor i in ids_train:\n    train['imputed_kw'].iloc[i]=1\n    \nids_test = test[test['keyword'].isnull()].index.tolist()\ntest['imputed_kw'] = 0\nfor i in ids_test:\n    test['imputed_kw'].iloc[i]=1\n#train[['imputed_kw','keyword']].head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# imputation of missing values in 'keyword' column\n# kw_dict contain key = 'keyword' and value = cleaned and stemmed keyword.. for eg. key='airplane%20accident' , value = airplan accid\nkw_dict = {}\n\nfor i in range(0,len(train)):\n    kw = train['keyword'].iloc[i]\n    if kw is not np.nan:\n        kw_nonum = re.sub(r'[0-9]+',' ',kw) # removing any digits 0-9\n        kw_nopunc = re.sub(r'[%]','',kw_nonum)# removing punctuation '%'\n        if kw not in kw_dict:       # if keyword is present in kw_dict, then stop\n            if len(kw_nopunc.split()) > 1:  # if cleaned 'keyword' contains more than 1 word\n                split_kw = kw_nopunc.split()# spli the leyword and put it in 'split_kw'\n                sdf = []\n                s=''\n                for w in split_kw:          # w is word in split_kw\n                    if w not in stopwords:\n                        w1 = ps.stem(w)     # stemming of word\n                        sdf.append(w1.lower())   # appending stemmed word in sdf\n                s =' '.join(sdf)         # after stemming , joining all words in sdf\n                kw_dict[kw] = s          # putting s in kw_dict with original keyword as key\n            else :\n                w2 = ps.stem(kw)\n                kw_dict[kw] = w2.lower()# if cleaned 'keyword' contains has only 1 word\n\nfirst5pairs = {k: kw_dict[k] for k in list(kw_dict)[:5]}\nprint(\"############ First five key-value pairs of kw_dict ##################\")\nprint(first5pairs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c = train[['keyword','target']].groupby(['keyword'])['target'].value_counts()\ncpct = c / c.groupby(level=0).sum()\n\n\npct_dict = {}\nfor kw in kw_dict:\n    if (kw,1) in cpct.index:\n        pct_dict[kw] = cpct[(kw,1)]\n    else:\n        pct_dict[kw] = 0\n        \npct_dict5 = {k:pct_dict[k] for k in list(pct_dict)[:5]}\nprint(pct_dict5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sorting keyword in dictionary by respective value and then reversing the order, \n#so list having element lined up according the probailitues of keyword signifying target='1'\n\na = list(reversed(sorted(pct_dict,key=pct_dict.get)))\nmy_list = [str(l) for l in a]\n#my_list\n\n#kw_dict['wild%20fires']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ids contain index value for which train['keyword'] is missing\n# ids_sorted is not needed , but it is list ids sorted by index values\n# final_kw_ids  contain key = index and value = 'Keyword' which is searched by using train['text'] data by matching with different keywords\nids = train[train['keyword'].isnull()].index.tolist()\nids_sorted = sorted(ids)\nfinal_kw_ids = {}\nfor x in my_list:\n    #kw1 = kw\n    pr_kw = kw_dict[x]\n    for i in ids_sorted:\n            txt = train['clean_text'][i].decode('utf_8')\n            if len(pr_kw.split()) > 1:\n                pr_kw_list = pr_kw.split()\n                if all(w in txt for w in pr_kw_list):\n                    final_kw_ids[i] = x\n\nfor key in final_kw_ids:\n    ids_sorted.remove(key)\n    \nfor x in my_list:\n        pr_kw = kw_dict[x]\n        for i in ids_sorted:\n            txt = train['clean_text'][i].decode('utf_8')\n            if len(pr_kw.split()) == 1:\n                if pr_kw in txt :\n                    final_kw_ids[i] = x\n\n\n                \nfinal_kw_ids5 = {k:final_kw_ids[k] for k in list(final_kw_ids)[:5]}\nprint(final_kw_ids5)                \n\n# ids_test contain index value for which train['keyword'] is missing\n# ids_test_sorted is not needed , but it is list ids sorted by index values\n# final_kw_ids_test  contain key = index and value = 'Keyword' which is searched by using test['text'] data by matching with different keywords\nids_test = test[test['keyword'].isnull()].index.tolist()\nids_test_sorted = sorted(ids_test)\nfinal_kw_ids_test = {}\nfor x in my_list:\n    #kw1 = kw\n    pr_kw = kw_dict[x]\n    for i in ids_test_sorted:\n            txt = test['clean_text'][i].decode('utf_8')\n            if len(pr_kw.split()) > 1:\n                pr_kw_list = pr_kw.split()\n                if all(w in txt for w in pr_kw_list):\n                    final_kw_ids_test[i] = x\n\nfor key in final_kw_ids_test:\n    ids_test_sorted.remove(key)\n    \nfor x in my_list:\n        pr_kw = kw_dict[x]\n        for i in ids_test_sorted:\n            txt = test['clean_text'][i].decode('utf_8')\n            if len(pr_kw.split()) == 1:\n                if pr_kw in txt :\n                    final_kw_ids_test[i] = x\n\n\n                \nfinal_kw_ids_test5 = {k:final_kw_ids_test[k] for k in list(final_kw_ids_test)[:5]}\nprint(final_kw_ids_test5)\n            \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##### imputing value from final_kw_ids into train data :\n\n#for key in final_kw_ids:\n#   kw = final_kw_ids[key]\n#   train['keyword'].iloc[key] = kw\n\n\ntrain['keyword'].update(pd.Series(final_kw_ids))\ntest['keyword'].update(pd.Series(final_kw_ids_test))\n\n#train.isnull().sum()\ntest.isnull().sum()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##### checking the train['keyword'], for which we are not able to find 'keywords'\nnull_ids = train[train['keyword'].isnull()].index.tolist()\ntrain['text'][null_ids]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##### checking the train['keyword'], for which we are not able to find 'keywords'\nnull_ids = test[test['keyword'].isnull()].index.tolist()\ntest['text'][null_ids]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above 'train['text']' shows that these tweet does not indicate any disaster like situation.\nso these tweets can be labeled 'nodisaster'"},{"metadata":{"trusted":true},"cell_type":"code","source":"# tweets can be labeled 'nodisaster'\ntrain['keyword'].fillna('nodisaster',inplace=True)\ntrain['keyword'].value_counts()\n# checking top 20 train['text'] and train['keyword'] combination\n#train[['text','keyword']].head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tweets can be labeled 'nodisaster'\ntest['keyword'].fillna('nodisaster',inplace=True)\ntest['keyword'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pct_dict['nodisaster'] = 0\n\ntrain['kw_dist_pct'] = train['keyword'].map(pct_dict)\ntest['kw_dist_pct'] = test['keyword'].map(pct_dict)\n#test['kw_dist_pct'].isnull().sum()\n#train['kw_dist_pct'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# since there are lot of null values in train['location'].. it may take lot of effort to impute. \n# it would be rather easier to merge location with text \n# first we should save info about which all indexes of column 'location' is null\n### adding a column 'imputed_kw' to give information which all train['keyword'] indexes have null values\nids = train[train['location'].isnull()].index.tolist()\ntrain['imputed_loc'] = 1\nfor i in ids:\n    train['imputed_loc'].iloc[i]=0\n\n#train[['imputed_loc','location']].head(20)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# since there are lot of null values in train['location'].. it may take lot of effort to impute. \n# it would be rather easier to merge location with text \n# first we should save info about which all indexes of column 'location' is null\n### adding a column 'imputed_kw' to give information which all train['keyword'] indexes have null values\nids = test[test['location'].isnull()].index.tolist()\ntest['imputed_loc'] = 1\nfor i in ids:\n    test['imputed_loc'].iloc[i]=0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# joining text and location column into column \"full_text\"\n\ntrain[\"full_text\"] = train[\"text\"].fillna('').map(str) + \" \" + train[\"location\"].fillna('').map(str) #+train[\"keyword\"]\ntest[\"full_text\"] = test[\"text\"].fillna('').map(str) + \" \" + test[\"location\"].fillna('').map(str) #+ test[\"keyword\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cleaning the column full text\n\nimport re\n\ndef decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase\n\ndef cleanhtml (sentence):\n    cleantext = re.sub(r'http\\S+',r'',sentence)\n    return cleantext\n\ndef cleanpunc(sentence):\n    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n    cleaned = re.sub(r'[.|,|)|(|)|\\|/]',r' ',cleaned)      \n    return cleaned\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\nstr1=' '\nfinal_string=[]\ns=''\n\nfor sent in train['full_text']:\n    filter_sent = []\n    sent = decontracted(sent)\n    sent1 = remove_emoji(sent)\n    rem_html = cleanhtml(sent1)\n    rem_punc = cleanpunc (rem_html)\n    for w in rem_punc.split():\n        if ((w.isalpha()) & (len(w)>2)):\n            if (w.lower() not in stopwords):\n                s=(ps.stem(w.lower())).encode('utf8')\n                #s=(w.lower()).encode('utf8')\n                filter_sent.append(s)\n            else:\n                continue\n        else:\n            continue\n    str1 = b\" \".join(filter_sent)\n    final_string.append(str1)\n    \n# attaching column new_col (cleaned text ) to dataframe\ntrain['clean_fulltext'] = np.array(final_string)\ntrain['clean_fulltext'].head(10)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cleaning the column full text\n\nimport re\n\ndef decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase\n\ndef cleanhtml (sentence):\n    cleantext = re.sub(r'http\\S+',r'',sentence)\n    return cleantext\n\ndef cleanpunc(sentence):\n    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n    cleaned = re.sub(r'[.|,|)|(|)|\\|/]',r' ',cleaned)      \n    return cleaned\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\nstr1=' '\nfinal_string=[]\ns=''\n\nfor sent in test['full_text']:\n    filter_sent = []\n    sent = decontracted(sent)\n    sent1 = remove_emoji(sent)\n    rem_html = cleanhtml(sent1)\n    rem_punc = cleanpunc (rem_html)\n    for w in rem_punc.split():\n        if ((w.isalpha()) & (len(w)>2)):\n            if (w.lower() not in stopwords):\n                s=(ps.stem(w.lower())).encode('utf8')\n                #s=(w.lower()).encode('utf8')\n                filter_sent.append(s)\n            else:\n                continue\n        else:\n            continue\n    str1 = b\" \".join(filter_sent)\n    final_string.append(str1)\n    \n# attaching column new_col (cleaned text ) to dataframe\ntest['clean_fulltext'] = np.array(final_string)\ntest['clean_fulltext'].head(10)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"############## wordcloud #####################\n# we need to create corpus of word from train['text']. we will create seperate corpus of word for text with target=1 and target=0\nids_1 = train[train['target']==1].index.tolist()\nall_words1=[]\n\nfor i in ids_1:\n    txt = train['clean_fulltext'][i].decode('utf_8')\n    for w in txt.split():\n        all_words1.append(w)\ntext_1 = ' '.join(all_words1)    \n    \n    \nwordcloud1 = WordCloud(width=800, height=400).generate(text_1)\n#wordcloud2.generate_from_frequencies\nplt.figure( figsize=(20,10) )\n\nplt.imshow(wordcloud1)\nplt.axis(\"off\")\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"############## wordcloud #####################\n# we need to create corpus of word from train['text']. we will create seperate corpus of word for text with target=1 and target=0\nids_0 = train[train['target']==0].index.tolist()\nall_words0=[]\n\nfor i in ids_0:\n    txt = train['clean_fulltext'][i].decode('utf_8')\n    for w in txt.split():\n        all_words0.append(w)\ntext_0 = ' '.join(all_words0)    \n    \n    \nwordcloud0 = WordCloud(width=800, height=400).generate(text_0)\n#wordcloud2.generate_from_frequencies\nplt.figure( figsize=(20,10) )\nplt.imshow(wordcloud0)\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"word 'new' which is stemmed form of word 'news' in text columns occur more times in tweets with target=0. This inference can be used in cming with feature which tells which tweet text have news or not. tf-idf will make it into feature by itself."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.columns)\nprint(test.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nfrom sklearn.preprocessing import normalize\nimport scipy as sp\n\n# Create vectorizer for function to use\nvectorizer = TfidfVectorizer(ngram_range=(1,3),min_df=2,max_df=0.5,max_features=1000)\ny = train[\"target\"].values \n\nX = sp.sparse.hstack((vectorizer.fit_transform(train.clean_fulltext),sc.fit_transform(train[['tweet_len','num_stopwords','hash_count','repeat_tweet_count', 'imputed_kw', 'kw_dist_pct',\n       'imputed_loc']].values)),format='csr')\n\nX_columns=vectorizer.get_feature_names()+train[['tweet_len','num_stopwords','hash_count','repeat_tweet_count', 'imputed_kw', 'kw_dist_pct',\n       'imputed_loc']].columns.tolist()\nprint(X.shape)\ntest_sp = sp.sparse.hstack((vectorizer.transform(test.clean_fulltext),sc.transform(test[['tweet_len','num_stopwords','hash_count','repeat_tweet_count', 'imputed_kw', 'kw_dist_pct',\n       'imputed_loc']].values)),format='csr')\ntest_columns=vectorizer.get_feature_names()+test[['tweet_len','num_stopwords','hash_count','repeat_tweet_count', 'imputed_kw', 'kw_dist_pct',\n       'imputed_loc']].columns.tolist()\nprint(test_sp.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn\nfrom sklearn import linear_model\nfrom sklearn.model_selection import learning_curve, GridSearchCV\n\n# Create logistic regression object\nlogistic = linear_model.LogisticRegression(max_iter=500)\n# Create a list of all of the different penalty values that you want to test and save them to a variable called 'penalty'\npenalty = ['l2']\n# Create a list of all of the different C values that you want to test and save them to a variable called 'C'\nC = [0.0001, 0.001, 0.01, 1,10, 100]\n# Now that you have two lists each holding the different values that you want test, use the dict() function to combine them into a dictionary. \n# Save your new dictionary to the variable 'hyperparameters'\nhyperparameters = dict(C=C, penalty=penalty)\n# Fit your model using gridsearch\nclf = GridSearchCV(logistic, hyperparameters, cv=5, verbose=1,scoring='f1')\nbest_model = clf.fit(X, y)\n#Print all the Parameters that gave the best results:\nprint('Best Parameters',clf.best_params_)\n# You can also print the best penalty and C value individually from best_model.best_estimator_.get_params()\nprint('Best Penalty:', best_model.best_estimator_.get_params()['penalty'])\nprint('Best C:', best_model.best_estimator_.get_params()['C'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\n\nkfold = model_selection.KFold(n_splits=5)\nmodel = LogisticRegression(penalty='l2',dual=False,max_iter=1000,C=1)\nmodel.fit(X,y)\nresults = model_selection.cross_val_score(model, X, y, cv=kfold)\nprint(\"Accuracy: Final mean:%.3f%%, Final standard deviation:(%.3f%%)\" % (results.mean()*100.0, results.std()*100.0))\nprint('Accuracies from each of the 5 folds using kfold:',results)\nprint(\"Variance of kfold accuracies:\",results.var())\nsub = model.predict(test_sp)\ntest['target'] = sub\n\nFinal_submission= test[['id','target']]\nFinal_submission.to_csv('submission1.csv',index=False)\nfeature_importance = abs(model.coef_[0])\nfeature_importance = 100.0 * (feature_importance / feature_importance.max())\nsorted_idx = np.argsort(feature_importance)\npos = np.arange(sorted_idx.shape[0]) + .5\nfeatfig = plt.figure(figsize=(8,64), dpi=100)\nfeatax = featfig.add_subplot(1, 1, 1)\nfeatax.barh(pos, feature_importance[sorted_idx], align='center')\nfeatax.set_yticks(pos)\nfeatax.set_yticklabels(np.array(X_columns)[sorted_idx], fontsize=8)\nfeatax.set_xlabel('Relative Feature Importance')\n\nplt.tight_layout()   \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Classifier - Algorithm - SVM\n# fit the training dataset on the classifier\nSVM = svm.SVC(C=1.71, kernel='linear', degree=3, gamma='auto')\nkfold = model_selection.KFold(n_splits=5)\nSVM.fit(X,y)\nresults = model_selection.cross_val_score(SVM, X, y, cv=kfold)\nprint(\"Accuracy: Final mean:%.3f%%, Final standard deviation:(%.3f%%)\" % (results.mean()*100.0, results.std()*100.0))\nprint('Accuracies from each of the 5 folds using kfold:',results)\nprint(\"Variance of kfold accuracies:\",results.var())\n# predict the labels on validation dataset\npredictions_SVM = SVM.predict(test_sp)\ntest['target'] = predictions_SVM\nFinal_submission2= test[['id','target']]\nFinal_submission2.to_csv('submission2.csv',index=False)\n# Use accuracy_score function to get the accuracy\n#print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, Test_Y)*100)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}