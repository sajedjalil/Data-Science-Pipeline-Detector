{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# *This particular challenge is perfect for data scientists looking to get started with Natural Language Processing*.","metadata":{}},{"cell_type":"markdown","source":"<img src='https://i.morioh.com/94c2283427.png' width='600'>","metadata":{}},{"cell_type":"markdown","source":"<div class='alert alert-info'>\n    <h2><center>AIM: To Predict whether a given tweet is about a real disaster or not. If so, predict as 1, else as 0.</center></h2>\n    </div>\n","metadata":{}},{"cell_type":"markdown","source":"<div class='alert alert-info'>\n    <h2><center>What is Natural Language Processing?</center></h2>\n\n<h3>Natural Language Processing is the technology used to aid computers to understand the human‚Äôs natural language.</h3><br>\n    <h3>It‚Äôs not an easy task teaching machines to understand how we communicate.</h3>\n\n<h3>Natural Language Processing, is a branch of artificial intelligence that deals with the interaction between computers and humans using the natural language.\nThe ultimate objective of NLP is to read, decipher, understand, and make sense of the human languages in a manner that is valuable.\n<h3>Most NLP techniques rely on machine learning to derive meaning from human languages</h3>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div class='alert alert-warning'>\n    <h3><center>Each sample in the train and test set has the following information:</center></h3>\n\n    \n1. The text of a tweet\n2. A keyword from that tweet (although this may be blank!)\n3. The location the tweet was sent from (may also be blank)\n    </div>","metadata":{}},{"cell_type":"markdown","source":"<div class='alert alert-warning'>\n    \n<h3><center>I am going to try to keep it simple as much as possible! We will be following 4 steps to obtain the desired predictions</center></h3>\n\nSTEP 1: INPUT (Obtain the input files - train and test)<br>\nSTEP 2: EDA (To visualize the class distributions of the target variable)<br>\nSTEP 3: FEATURE ENGINEERING (Extarcting the features from the given text using TfidfVectorizer)<br>\nSTEP 4: MODEL BUILDING & MAKING PREDICTIONS (Using logistic regression to start with)<br>\nSTEP 5: OUTPUT<br>","metadata":{}},{"cell_type":"markdown","source":"<div class='alert alert-info'>\n    <h3><center>STEP 1</center></h3>\nSo let's start with STEP 1 - Obtaining the inputs\n    </div>","metadata":{}},{"cell_type":"markdown","source":"Let's import the necessary packages!!","metadata":{}},{"cell_type":"code","source":"\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O\nimport missingno as mno\nfrom sklearn.feature_extraction.text import TfidfVectorizer # For extracting the features from the tweet text\nfrom sklearn.linear_model import LogisticRegression #To build a logistic model\nfrom sklearn.metrics import accuracy_score  #To obtain the evaluation metrics\nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-23T19:35:07.156303Z","iopub.execute_input":"2021-06-23T19:35:07.156617Z","iopub.status.idle":"2021-06-23T19:35:08.255343Z","shell.execute_reply.started":"2021-06-23T19:35:07.156588Z","shell.execute_reply":"2021-06-23T19:35:08.254239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Now since we have downloaded the necessary packages, lets import the data** ","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0"}},{"cell_type":"code","source":"train=pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest=pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-06-23T19:35:08.257146Z","iopub.execute_input":"2021-06-23T19:35:08.257539Z","iopub.status.idle":"2021-06-23T19:35:08.326667Z","shell.execute_reply.started":"2021-06-23T19:35:08.257499Z","shell.execute_reply":"2021-06-23T19:35:08.325808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class='alert alert-info'>\n    <h2><center>STEP 2 : EDA ON THE CLASS DISTRIBUTION OF THE TARGET VARIABLE</center></h2>\n    </div>","metadata":{}},{"cell_type":"code","source":"train['location'].isna().sum(),train.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-23T19:35:08.32984Z","iopub.execute_input":"2021-06-23T19:35:08.330391Z","iopub.status.idle":"2021-06-23T19:35:08.342422Z","shell.execute_reply.started":"2021-06-23T19:35:08.330359Z","shell.execute_reply":"2021-06-23T19:35:08.341464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\noccurences=train['target'].value_counts().reset_index().rename(columns={'index':'Class','target':'Number of Occurences'})\nsns.barplot(x=occurences['Class'],y=occurences['Number of Occurences'])\n","metadata":{"execution":{"iopub.status.busy":"2021-06-23T19:35:08.344472Z","iopub.execute_input":"2021-06-23T19:35:08.344837Z","iopub.status.idle":"2021-06-23T19:35:08.497106Z","shell.execute_reply.started":"2021-06-23T19:35:08.3448Z","shell.execute_reply":"2021-06-23T19:35:08.496233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"occurences['Percentage(%)']=(occurences['Number of Occurences']/occurences['Number of Occurences'].sum())*100\noccurences.set_index('Class')","metadata":{"execution":{"iopub.status.busy":"2021-06-23T19:35:08.499794Z","iopub.execute_input":"2021-06-23T19:35:08.500226Z","iopub.status.idle":"2021-06-23T19:35:08.529955Z","shell.execute_reply.started":"2021-06-23T19:35:08.500177Z","shell.execute_reply":"2021-06-23T19:35:08.529048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The number of class 0 records are more in number compared to the class 1 records! But the differences is quite less!**","metadata":{}},{"cell_type":"markdown","source":"\n<div class='alert alert-info'>\n    <h2><center>STEP 3: EXTRACTING THE FEATURES FROM THE TWEET TEXT</center></h2>\n    </div>","metadata":{}},{"cell_type":"code","source":"traindata = list(np.array(train.iloc[:,3])) #Extracting the text feature alone from the train data\ntestdata = list(np.array(test.iloc[:,3]))#Extracting the text feature alone from the test data\ny = np.array(train.iloc[:,4]).astype(int)#Extracting the target varaible from the train data\n\nX_all = traindata + testdata #combining both the test and train data\nlentrain = len(traindata)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T19:35:08.531883Z","iopub.execute_input":"2021-06-23T19:35:08.532356Z","iopub.status.idle":"2021-06-23T19:35:08.53883Z","shell.execute_reply.started":"2021-06-23T19:35:08.532323Z","shell.execute_reply":"2021-06-23T19:35:08.538173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Implementing TFIDF to extract the features from the text\ntfidf = TfidfVectorizer(min_df=3,  max_features=None, strip_accents='unicode',  \n        analyzer='word',token_pattern=r'\\w{1,}',ngram_range=(1, 2), use_idf=1,smooth_idf=1,sublinear_tf=1)\n\nprint(\"Implementing TFIDF to both the test and train data\")\ntfidf.fit(X_all)\nprint(\"Transforming the data\")\nX_all = tfidf.transform(X_all)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T19:35:08.539787Z","iopub.execute_input":"2021-06-23T19:35:08.540176Z","iopub.status.idle":"2021-06-23T19:35:09.445219Z","shell.execute_reply.started":"2021-06-23T19:35:08.540137Z","shell.execute_reply":"2021-06-23T19:35:09.444391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Voila! The TFIDF features are now ready and we can proceed with step 4** ","metadata":{}},{"cell_type":"markdown","source":"<div class='alert alert-info'>\n    <h2><center>STEP 4: MODEL BUILDING AND PREDICTION</center></h2>\n    </div> \n","metadata":{}},{"cell_type":"code","source":"X = X_all[:lentrain] # Seperating the train data from the entire data\nX_test = X_all[lentrain:] # Seperating the test data from the entire data\n\nlog = LogisticRegression(penalty='l2',dual=False, tol=0.0001, \n                             C=1, fit_intercept=True, intercept_scaling=1.0, \n                             class_weight=None, random_state=None) #initialising the logistic regression function with the respective parameters\n\nprint(\"Training on the train data\")\nlog.fit(X,y)\n\n#Evaluating with the train data's target variable to obatin the training accuracy!\ny_pred_X=log.predict(X)\nprint('Training accuracy is {}'.format(accuracy_score(y, y_pred_X)))\n\npredictions = log.predict(X_test) #Prediciting the target for the test data\n\npredictions","metadata":{"execution":{"iopub.status.busy":"2021-06-23T19:35:09.446531Z","iopub.execute_input":"2021-06-23T19:35:09.446904Z","iopub.status.idle":"2021-06-23T19:35:09.591932Z","shell.execute_reply.started":"2021-06-23T19:35:09.446865Z","shell.execute_reply":"2021-06-23T19:35:09.591009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class='alert alert-info'>\n    <h2><center>STEP 5: GENERATING THE OUTPUT FILE </center></h2>\n    </div> \n\n","metadata":{}},{"cell_type":"code","source":"test","metadata":{"execution":{"iopub.status.busy":"2021-06-23T19:35:09.5932Z","iopub.execute_input":"2021-06-23T19:35:09.593471Z","iopub.status.idle":"2021-06-23T19:35:09.607363Z","shell.execute_reply.started":"2021-06-23T19:35:09.593445Z","shell.execute_reply":"2021-06-23T19:35:09.606634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_ids=test['id']\nsubmission = pd.DataFrame(predictions,index=test_ids,columns=['target'])\nsubmission.to_csv('submission_nlprnot.csv')\nprint(\"submission file created..\")","metadata":{"execution":{"iopub.status.busy":"2021-06-23T19:35:09.608577Z","iopub.execute_input":"2021-06-23T19:35:09.608833Z","iopub.status.idle":"2021-06-23T19:35:10.003669Z","shell.execute_reply.started":"2021-06-23T19:35:09.608806Z","shell.execute_reply":"2021-06-23T19:35:10.002841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class='alert alert-warning'>\n    <h3><center>In order to improve the score, you could use different classifier with different sets of parameter tuning(Grid search or CVsearch)</center></h3>\n    </div>","metadata":{}},{"cell_type":"markdown","source":"## Here are some other notebooks and datasets for you to explore more on twitter sentiment analysisüëç\n\n### Notebooks:\n- [pfizer tweets EDA abd text analysis](https://www.kaggle.com/kaushiksuresh147/pfizer-tweets-eda-and-text-analysis)\n- [IPL 20-2021 Twitter analysis & EDA](https://www.kaggle.com/kaushiksuresh147/ipl-20-2021-twitter-analysis-eda)\n- [How to extract tweets from twitter using twitter API in python](https://www.kaggle.com/kaushiksuresh147/twitter-data-extraction-for-ipl2020)\n- [Covid Vaccine EDA](https://www.kaggle.com/kaushiksuresh147/covid-vaccine-eda)</p>\n\n\n### Datasets\n- [Bitcoin Tweets](https://www.kaggle.com/kaushiksuresh147/bitcoin-tweets)\n- [IPL 2020 & 2021 Tweets](https://www.kaggle.com/kaushiksuresh147/ipl2020-tweets)\n- [Covid Vaccine Tweets](https://www.kaggle.com/kaushiksuresh147/covidvaccine-tweets)\n- [The Social Dilemma Tweets - Text Classification](https://www.kaggle.com/kaushiksuresh147/the-social-dilemma-tweets)\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}