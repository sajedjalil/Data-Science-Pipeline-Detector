{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# [Real or Not? NLP with Disaster Tweets](https://www.kaggle.com/c/nlp-getting-started)","metadata":{}},{"cell_type":"markdown","source":"### I plan to research the possibilities and compare different models with different parameters of Simple Transformers models to solve the issue \"Real or Not? NLP with Disaster Tweets\"","metadata":{}},{"cell_type":"markdown","source":"I plan to study of **each of Simple Transformers models** with different parameters without K-fold since cross-validation complicates the analysis of the model:\n\n* visualize **embeddings** (I use the function from my notebook in this competition with about 500 forks: [NLP - EDA, Bag of Words, TF IDF, GloVe, BERT](https://www.kaggle.com/vbmokin/nlp-eda-bag-of-words-tf-idf-glove-bert));\n* study **outliers** (forecast errors) (I use the functions of my notebook with more than 700 forks: [TSE2020 - RoBERTa (CNN) - Outlier Analysis, 3chr](https://www.kaggle.com/vbmokin/tse2020-roberta-cnn-outlier-analysis-3chr));\n* build a **confusion matrix** (from the same notebook [NLP - EDA, Bag of Words, TF IDF, GloVe, BERT](https://www.kaggle.com/vbmokin/nlp-eda-bag-of-words-tf-idf-glove-bert));\n\nI commit the result of each prediction and save (parameters and LB) it in a section with successful commits.\n\nThis notebook use my public dataset with cleaning data for this competition [NLP with Disaster Tweets - cleaning data](https://www.kaggle.com/vbmokin/nlp-with-disaster-tweets-cleaning-data):\n* `train_data_cleaning.csv`\n* `test_data_cleaning.csv`\n\nto speed up and increase the accuracy of calculations.\n\nSee all models in list of **transformers** library: https://huggingface.co/transformers/pretrained_models.html","metadata":{}},{"cell_type":"markdown","source":"# Acknowledgements\n\nThis kernel uses such good notebooks and resources: \n* libraries [transformers](https://huggingface.co/transformers) and [simpletransformers](https://github.com/ThilinaRajapakse/simpletransformers)\n* dataset [NLP with Disaster Tweets - cleaning data](https://www.kaggle.com/vbmokin/nlp-with-disaster-tweets-cleaning-data)\n* notebook [SimpleTransformers + Hyperparam Tuning + k-fold CV](https://www.kaggle.com/szelee/simpletransformers-hyperparam-tuning-k-fold-cv)\n* notebook [NLP with DT cleaning: Simple Transformers predict](https://www.kaggle.com/vbmokin/nlp-with-dt-cleaning-simple-transformers-predict)\n* notebook [NLP with Disaster Tweets - EDA and Cleaning data](https://www.kaggle.com/vbmokin/nlp-with-disaster-tweets-eda-and-cleaning-data)\n* notebook [NLP - EDA, Bag of Words, TF IDF, GloVe, BERT](https://www.kaggle.com/vbmokin/nlp-eda-bag-of-words-tf-idf-glove-bert)\n* notebook [TSE2020 - RoBERTa (CNN) - Outlier Analysis, 3chr](https://www.kaggle.com/vbmokin/tse2020-roberta-cnn-outlier-analysis-3chr)","metadata":{}},{"cell_type":"markdown","source":"<a class=\"anchor\" id=\"0.1\"></a>\n## Table of Contents\n\n1. [All commits](#1)\n    - [Commit now](#1.1)\n    - [Successful and most interesting commits without KFolds](#1.2)\n        - [DistilBERT](#1.2.1)\n            - [distilbert-base-uncased](#1.2.1.1)\n            - [distilbert-base-cased](#1.2.1.2)\n        - [RoBERTa](#1.2.2)\n            - [distilroberta-base](#1.2.2.1)\n            - [roberta-base](#1.2.2.2)\n        - [ALBERT](#1.2.3)\n            - [albert-base-v1](#1.2.3.1)\n            - [albert-xlarge-v2](#1.2.3.2)\n        - [BERT](#1.2.4)\n            - [bert-base-uncased](#1.2.4.1)\n            - [bert-base-cased](#1.2.4.2)\n            - [bert-base-multilingual-cased](#1.2.4.3)            \n    - [Successful commits with KFolds](#1.3)\n        - [DistilBERT](#1.3.1)\n            - [distilbert-base-uncased](#1.3.1.1)            \n1. [Import libraries](#2)\n1. [Download data](#3)\n1. [EDA](#4)\n1. [Model training and prediction](#5)\n    - [Without KFold](#5.1)\n    - [With KFold](#5.2)\n1. [Submission](#6)\n1. [Visualization of model outputs for all training data](#7)\n1. [Outlier Analysis](#8)\n    - [Word Cloud visualization](#8.1)\n    - [Punctuation marks repetition analysis](#8.2)\n1. [Showing Confusion Matrices](#9)\n1. [Resume](#10)","metadata":{}},{"cell_type":"markdown","source":"## 1. All commits<a class=\"anchor\" id=\"1\"></a>\n\n[Back to Table of Contents](#0.1)","metadata":{}},{"cell_type":"markdown","source":"* DATA1 - dataset 1 (commits 1-23, 43,...) - original dataset of the competition \n* DATA2 - dataset 2 (commits 24-42, 44,...) - cleaned dataset from [NLP with Disaster Tweets - cleaning data](https://www.kaggle.com/vbmokin/nlp-with-disaster-tweets-cleaning-data)","metadata":{}},{"cell_type":"markdown","source":"## 1.1. Commit now <a class=\"anchor\" id=\"1.1\"></a>\n\n[Back to Table of Contents](#0.1)","metadata":{}},{"cell_type":"code","source":"model_type = 'distilbert'\nmodel_name = 'distilbert-base-uncased'\nwith_kfold = False\nweight = [0.43, 0.57]\ndataset = 'DATA2'  # or 'DATA1'\nn_splits = 1   # if with_kfold then must be n_splits > 1\nseed = 42\nmodel_args =  {'fp16': False,\n               'train_batch_size': 4,\n               'gradient_accumulation_steps': 2,\n               'do_lower_case': True,\n               'learning_rate': 1e-05,\n               'overwrite_output_dir': True,\n               'manual_seed': seed,\n               'num_train_epochs': 2}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.2. Successful and most interesting commits without KFolds <a class=\"anchor\" id=\"1.2\"></a>\n\n[Back to Table of Contents](#0.1)","metadata":{}},{"cell_type":"markdown","source":"## 1.2.1. DistilBERT <a class=\"anchor\" id=\"1.2.1\"></a>\n\n[Back to Table of Contents](#0.1)","metadata":{}},{"cell_type":"markdown","source":"## 1.2.1.1. distilbert-base-uncased <a class=\"anchor\" id=\"1.2.1.1\"></a>\n12-layer, 768-hidden, 12-heads, 110M parameters. Trained on lower-cased English text.\n\n[Back to Table of Contents](#0.1)","metadata":{}},{"cell_type":"markdown","source":"* DATA2 - Commit 67 (LB = 0.84278): lr = 1e-05, num_epochs = 2, seed = 100, acc = 0.887, num_outliers = 864(26.5%), weight = [0.44, 0.56]\n* DATA2 - Commit 62 (LB = 0.84186): lr = 1e-05, num_epochs = 2, seed = 100, acc = 0.886, num_outliers = 865(26.5%), weight = [0.45, 0.55]\n* DATA2 - Commit 56 (LB = 0.84155): lr = 1e-05, num_epochs = 2, seed = 100, acc = 0.887, num_outliers = 857(26.3%), weight = [0.4, 0.6]\n* DATA1 - Commit 6 (LB = 0.84125): lr = 1e-05, num_epochs = 2, acc = 0.888, num_outliers = 852(26.1%)\n* DATA2 - Commit 55 (LB = 0.84125): lr = 9e-06, num_epochs = 2, seed = 100, acc = 0.884, num_outliers = 884(27.1%), weight = [0.4, 0.6]\n* DATA2 - Commit 32 (LB = 0.84125): lr = 9e-06, num_epochs = 2, seed = 100, acc = 0.883, num_outliers = 893(27.4%)\n* DATA2 - Commit 49 (LB = 0.84033): lr = 1e-05, num_epochs = 2, seed = 100, acc = 0.886, num_outliers = 871(26.7%)\n* DATA2 - Commit 25 (LB = 0.84002): lr = 1e-05, seed = 100, acc = 0.885, num_outliers = 873(26.8%)\n* DATA2 - Commit 54 (LB = 0.83971): lr = 9e-06, num_epochs = 2, seed = 100, acc = 0.882, num_outliers = 895(27.4%), weigh = [0.6, 0.4]\n* DATA2 - Commit 57 (LB = 0.83971): lr = 1e-05, num_epochs = 2, seed = 100, acc = 0.887, num_outliers = 862(26.4%), weight = [0.3, 0.7]\n* DATA2 - Commit 58 (LB = 0.83879): lr = 1.5e-05, num_epochs = 2, seed = 100, acc = 0.9, num_outliers = 763(23.4%), weight = [0.4, 0.6]\n* DATA2 - Commit 45 (LB = 0.83849): lr = 1e-05, num_epochs = 2, seed = 42, acc = 0.887, num_outliers = 857(26.3%)\n* DATA1 - Commit 9 (LB = 0.83726): lr = 2e-05, acc = 0.905, num_outliers = 720(22.1%)\n* DATA2 - Commit 29 (LB = 0.83604): lr = 3e-05, acc = 0.866, num_outliers = 1018(31.2%)\n* DATA1 - Commit 5 (LB = 0.83389): lr = 1e-05, seed = 100, acc = 0.915, num_outliers = 649(19.9%)\n* DATA2 - Commit 30 (LB = 0.83174): lr = 4e-05, num_epochs = 2, seed = 100, acc = 0.917, num_outliers = 632(19.4%)","metadata":{}},{"cell_type":"markdown","source":"## 1.2.1.2. distilbert-base-cased <a class=\"anchor\" id=\"1.2.1.2\"></a>\n6-layer, 768-hidden, 12-heads, 65M parameters. The DistilBERT model distilled from the BERT model bert-base-cased checkpoint\n\n[Back to Table of Contents](#0.1)","metadata":{}},{"cell_type":"markdown","source":"* DATA2 - Commit 28 (LB = 0.82194): lr = 1e-05, num_epochs = 2, seed = 100, acc = 0.883, num_outliers = 887(27.2%)","metadata":{}},{"cell_type":"markdown","source":"## 1.2.2. RoBERTa <a class=\"anchor\" id=\"1.2.2\"></a>\n\n[Back to Table of Contents](#0.1)","metadata":{}},{"cell_type":"markdown","source":"### 1.2.2.1. distilroberta-base <a class=\"anchor\" id=\"1.2.2.1\"></a>\n6-layer, 768-hidden, 12-heads, 82M parameters. The DistilRoBERTa model distilled from the RoBERTa model roberta-base checkpoint.\n\n[Back to Table of Contents](#0.1)","metadata":{}},{"cell_type":"markdown","source":"* DATA1 - Commit 22 (LB = 0.83818): lr = 1e-05, num_epochs = 2, seed = 100, acc = 0.878, num_outliers = 930(28.5%)\n* DATA1 - Commit 21 (LB = 0.83358): lr = 2e-05, num_epochs = 2, seed = 1, acc = 0.892, num_outliers = 819(25.1%)","metadata":{}},{"cell_type":"markdown","source":"### 1.2.2.2. roberta-base <a class=\"anchor\" id=\"1.2.2.2\"></a>\n12-layer, 768-hidden, 12-heads, 125M parameters. RoBERTa using the BERT-base architecture\n\n[Back to Table of Contents](#0.1)","metadata":{}},{"cell_type":"markdown","source":"* DATA2 - Commit 51 (LB = 0.83512): lr = 1e-05, num_epochs = 2, seed = 100, acc = 0.885, num_outliers = 873(26.8%)\n* DATA1 - Commit 50 (LB = 0.83021): lr = 1e-05, num_epochs = 2, seed = 100, acc = 0.886, num_outliers = 868(26.6%)\n* DATA1 - Commit 52 (LB = 0.82899): lr = 9e-06, num_epochs = 2, seed = 100, acc = 0.887, num_outliers = 862(26.4%)","metadata":{}},{"cell_type":"markdown","source":"## 1.2.3. ALBERT <a class=\"anchor\" id=\"1.2.3\"></a>\n\n[Back to Table of Contents](#0.1)","metadata":{}},{"cell_type":"markdown","source":"### 1.2.3.1. albert-base-v1 <a class=\"anchor\" id=\"1.2.3.1\"></a>\n12 repeating layers, 128 embedding, 768-hidden, 12-heads, 11M parameters. ALBERT base model\n\n[Back to Table of Contents](#0.1)","metadata":{}},{"cell_type":"markdown","source":"* DATA2 - Commit 27 (LB = 0.83328): lr = 1e-05, num_epochs = 2, seed = 100, acc = 0.868, num_outliers = 1002(30.7%)","metadata":{}},{"cell_type":"markdown","source":"### 1.2.3.2. albert-xlarge-v2 <a class=\"anchor\" id=\"1.2.3.2\"></a>\n24 repeating layers, 128 embedding, 2048-hidden, 16-heads, 58M parameters. ALBERT xlarge model with no dropout, additional training data and longer training\n\n[Back to Table of Contents](#0.1)","metadata":{}},{"cell_type":"markdown","source":"* DATA2 - Commit 26: lr = 1e-05, num_epochs = 2, seed = 100, acc = 0.572, num_outliers = 3259(99.9%)\n\nI don't send this submission file. It's not a good solution.","metadata":{}},{"cell_type":"markdown","source":"## 1.2.4. BERT <a class=\"anchor\" id=\"1.2.4\"></a>\n\n[Back to Table of Contents](#0.1)","metadata":{}},{"cell_type":"markdown","source":"\"**bert-large-uncased**\" get error (see unsuccessful commits 3, 4, 42): \"OSError: [Errno 28] No space left on device\"","metadata":{}},{"cell_type":"markdown","source":"### 1.2.4.1. bert-base-uncased <a class=\"anchor\" id=\"1.2.4.1\"></a>\n12-layer, 768-hidden, 12-heads, 110M parameters. Trained on lower-cased English text.\n\n[Back to Table of Contents](#0.1)","metadata":{}},{"cell_type":"markdown","source":"* DATA2 - Commit 36 (LB = 0.84063): lr = 9e-06, num_epochs = 2, seed = 100, acc = 0.898, num_outliers = 780(23.9%)\n* DATA2 - Commit 34 (LB = 0.84033): lr = 1e-05, num_epochs = 2, seed = 100, acc = 0.9, num_outliers = 763(23.4%)\n* DATA2 - Commit 37 (LB = 0.83818): lr = 2e-05, num_epochs = 2, seed = 100, acc = 0.921, num_outliers = 601(18.4%)\n* DATA2 - Commit 38 (LB = 0.83634): lr = 8e-06, num_epochs = 2, seed = 100, acc = 0.893, num_outliers = 812(24.9%)\n* DATA1 - Commit 44 (LB = 0.83512): lr = 9e-06, num_epochs = 2, seed = 100, acc = 0.898, num_outliers = 775(23.8%)\n* DATA1 - Commit 43 (LB = 0.83144): lr = 9e-06, num_epochs = 1, seed = 100, acc = 0.869, num_outliers = 999(30.6%)","metadata":{}},{"cell_type":"markdown","source":"### 1.2.4.2. bert-base-cased <a class=\"anchor\" id=\"1.2.4.2\"></a>\n12-layer, 768-hidden, 12-heads, 110M parameters. Trained on cased English text.\n\n[Back to Table of Contents](#0.1)","metadata":{}},{"cell_type":"markdown","source":"* DATA2 - Commit 39 (LB = 0.83573): lr = 9e-06, num_epochs = 2, seed = 100, acc = 0.896, num_outliers = 794(24.3%)","metadata":{}},{"cell_type":"markdown","source":"### 1.2.4.3. bert-base-multilingual-cased <a class=\"anchor\" id=\"1.2.4.3\"></a>\n12-layer, 768-hidden, 12-heads, 110M parameters. Trained on cased text in the top 104 languages with the largest Wikipedias.\n\n[Back to Table of Contents](#0.1)","metadata":{}},{"cell_type":"markdown","source":"* DATA1 - Commit 60 (LB = 0.82807): lr = 1e-05, num_epochs = 2, seed = 100, acc = 0.885, num_outliers = 878(26.9%), weight = [0.5, 0.5]","metadata":{}},{"cell_type":"markdown","source":"## 1.3. Successful commits with KFolds <a class=\"anchor\" id=\"1.3\"></a>\n\n[Back to Table of Contents](#0.1)","metadata":{}},{"cell_type":"markdown","source":"## 1.3.1. DistilBERT <a class=\"anchor\" id=\"1.3.1\"></a>\n\n[Back to Table of Contents](#0.1)","metadata":{}},{"cell_type":"markdown","source":"## 1.3.1.1. distilbert-base-uncased <a class=\"anchor\" id=\"1.3.1.1\"></a>\n12-layer, 768-hidden, 12-heads, 110M parameters. Trained on lower-cased English text.\n\n[Back to Table of Contents](#0.1)","metadata":{}},{"cell_type":"markdown","source":"* DATA2 - Commit 33 (LB = 0.83910): lr = 5e-05, n_splits = 5, num_epochs = 3, seed = 1, acc = 0.828, num_outliers = 1313(40.2%)\n* DATA2 - Commit 59 (LB = 0.83879): lr = 1e-05, n_splits = 10, num_epochs = 2, seed = 100, acc = 0.84, num_outliers = 1217(37.3%), weight = [0.4, 0.6]\n* DATA1 - Commit 13 (LB = 0.83450): lr = 9e-06, n_splits = 5, num_epochs = 2, seed = 100, acc = 0.837, num_outliers = 1240(38.0%)","metadata":{}},{"cell_type":"markdown","source":"## 2. Import libraries<a class=\"anchor\" id=\"2\"></a>\n\n[Back to Table of Contents](#0.1)","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade transformers\n!pip install simpletransformers","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os, re, string\nimport random\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport seaborn as sns\n\nfrom wordcloud import WordCloud, STOPWORDS\nfrom collections import Counter\n\nimport numpy as np\nimport pandas as pd\nimport sklearn\nfrom sklearn.decomposition import PCA, TruncatedSVD\n\nimport torch\n\nfrom simpletransformers.classification import ClassificationModel\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split, KFold\n\nimport warnings\nwarnings.simplefilter('ignore')\n\npd.set_option('max_rows', 100)\npd.set_option('max_colwidth', 2000)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Download data<a class=\"anchor\" id=\"3\"></a>\n\n[Back to Table of Contents](#0.1)","metadata":{}},{"cell_type":"markdown","source":"### See my posts about this issue \"[Cleaning dataset for this competition](https://www.kaggle.com/c/nlp-getting-started/discussion/166426)\"","metadata":{}},{"cell_type":"markdown","source":"Thanks to [NLP with Disaster Tweets - EDA and Cleaning data](https://www.kaggle.com/vbmokin/nlp-with-disaster-tweets-eda-and-cleaning-data)","metadata":{}},{"cell_type":"code","source":"if dataset == 'DATA1':\n    # Original dataset of the competition\n    train_data = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')[['text', 'target']]\n    test_data = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')[['text']]\n    \nelif (dataset == 'DATA2') or (dataset == 'DATA2b'):\n    # Cleaned dataset from https://www.kaggle.com/vbmokin/nlp-with-disaster-tweets-cleaning-data\n    train_data = pd.read_csv('../input/nlp-with-disaster-tweets-cleaning-data/train_data_cleaning.csv')[['text', 'target']]\n    test_data = pd.read_csv('../input/nlp-with-disaster-tweets-cleaning-data/test_data_cleaning.csv')[['text']]\n\n# Original dataset of the competition\nsample_submission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['target'].hist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Weights which I offer for 0 and 1:\", weight)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data['text']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. EDA<a class=\"anchor\" id=\"4\"></a>\n\n[Back to Table of Contents](#0.1)","metadata":{}},{"cell_type":"markdown","source":"### See my posts about this issue \"[Punctuation marks repetition in incorrectly classified text](https://www.kaggle.com/c/nlp-getting-started/discussion/166248)\"","metadata":{}},{"cell_type":"code","source":"def subtext_repeation_in_df(df, col, subtext, num):\n    # Calc statistics as table for character repetition (1...num times) from subtext list in the df[col]\n    \n    text = \"\".join(df[col])\n    result = pd.DataFrame(columns = ['subtext', 'count'])\n    i = 0\n    if (len(df) > 0) and (len(subtext) > 0):\n        for c in subtext:\n            for j in range(num):\n                cs = c*(j+1)\n                result.loc[i,'count'] = text.count(cs)\n                if c == ' ':\n                    cs = cs.replace(' ','<space>')\n                result.loc[i,'subtext'] = cs                \n                i += 1\n    print('Number of all data is', len(df))\n    result = result[result['count'] > 0].reset_index(drop=True)\n    display(result.sort_values(by='subtext'))\n    \n    print('Text examples')\n    problem_examples = pd.DataFrame(columns = ['problem_examples'])\n    problem_examples['problem_examples'] = ''\n    for i in range(len(result)):\n        problem_examples.loc[i,'problem_examples'] = df[df[col].str.find(result.loc[i,'subtext'])>-1].reset_index(drop=True).loc[0, col]\n    problem_examples = problem_examples.drop_duplicates()\n    display(problem_examples)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Analysis of punctuation marks repetition in training data\nprint('Statistics for punctuation marks repetition in training data')\nsubtext_repeation_in_df(train_data, 'text', list(string.punctuation), 10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Analysis of punctuation marks repetition in test data\nprint('Statistics for punctuation marks repetition in test data')\nsubtext_repeation_in_df(test_data, 'text', list(string.punctuation), 10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. Model training and prediction<a class=\"anchor\" id=\"5\"></a>\n\n[Back to Table of Contents](#0.1)","metadata":{}},{"cell_type":"markdown","source":"## 5.1. Without KFold<a class=\"anchor\" id=\"5.1\"></a>\n\n[Back to Table of Contents](#0.1)","metadata":{}},{"cell_type":"code","source":"# Model training without KFold\nif not with_kfold:\n    model = ClassificationModel(model_type, model_name, args=model_args, weight=weight) \n    model.train_model(train_data)\n    result, model_outputs, wrong_predictions = model.eval_model(train_data, acc=sklearn.metrics.accuracy_score)\n    y_preds, _, = model.predict(test_data['text'])\n    pred_train, _ = model.predict(train_data['text'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not with_kfold:\n    acc = result['acc']\n    print('acc =',acc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.2. With KFold<a class=\"anchor\" id=\"5.2\"></a>\n\n[Back to Table of Contents](#0.1)","metadata":{}},{"cell_type":"code","source":"# Model training with KFold\nif with_kfold:\n    kf = KFold(n_splits=n_splits, random_state=seed, shuffle=True)\n\n    results = []\n    wrong_predictions = []\n    y_preds = np.zeros(test_data.shape[0])\n    pred_train = np.zeros(train_data.shape[0])\n    \n    first_fold = True\n    for train_index, val_index in kf.split(train_data):\n        train_df = train_data.iloc[train_index]\n        val_df = train_data.iloc[val_index]\n\n        # Model training\n        model = ClassificationModel(model_type, model_name, args=model_args)\n        model.train_model(train_df)\n\n        # Validation data prediction\n        result, model_outputs_fold, wrong_predictions_fold = model.eval_model(val_df, acc=sklearn.metrics.accuracy_score)\n        pred_train[val_index], _ = model.predict(val_df['text'].reset_index(drop=True))\n        \n        # Save fold results\n        if first_fold:\n            model_outputs = model_outputs_fold\n            first_fold = False\n        else: model_outputs = np.vstack((model_outputs,model_outputs_fold))\n        \n        wrong_predictions += wrong_predictions_fold\n        results.append(result['acc'])\n\n        # Test data prediction\n        y_pred, _ = model.predict(test_data['text'])\n        y_preds += y_pred / n_splits","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Thanks to https://www.kaggle.com/szelee/simpletransformers-hyperparam-tuning-k-fold-cv\n# CV accuracy result output\nif with_kfold:\n    for i, result in enumerate(results, 1):\n        print(f\"Fold-{i}: {result}\")\n    \n    acc = np.mean(results)\n\n    print(f\"{n_splits}-fold CV accuracy result: Mean: {acc} Standard deviation:{np.std(results)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6. Submission<a class=\"anchor\" id=\"6\"></a>\n\n[Back to Table of Contents](#0.1)","metadata":{}},{"cell_type":"code","source":"y_preds[:] = y_preds[:]>=0.5\ny_preds = y_preds.astype(int)\nnp.mean(y_preds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data prediction and submission\nsample_submission[\"target\"] = y_preds\nsample_submission.to_csv(\"submission.csv\", index=False)\ny_preds[:20]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7. Visualization of model outputs for all training data <a class=\"anchor\" id=\"7\"></a>\n\n[Back to Table of Contents](#0.1)","metadata":{}},{"cell_type":"code","source":"# Visualization of model outputs for each rows of training data\ndef plot_data_lavel(data, labels):\n    colors = ['orange','blue']\n    plt.scatter(data[:,0], data[:,1], s=8, alpha=.8, c=labels, cmap=matplotlib.colors.ListedColormap(colors))\n    orange_patch = mpatches.Patch(color='orange', label='Not')\n    blue_patch = mpatches.Patch(color='blue', label='Real')\n    plt.legend(handles=[orange_patch, blue_patch], prop={'size': 30})\n\nfig = plt.figure(figsize=(16, 16))          \nplot_data_lavel(model_outputs, train_data['target'].values)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 8. Outlier Analysis<a class=\"anchor\" id=\"8\"></a>\n\n[Back to Table of Contents](#0.1)","metadata":{}},{"cell_type":"markdown","source":"This chapter uses notebook [TSE2020 - RoBERTa (CNN) - Outlier Analysis, 3chr](https://www.kaggle.com/vbmokin/tse2020-roberta-cnn-outlier-analysis-3chr)","metadata":{}},{"cell_type":"markdown","source":"## 8.1. Word Cloud visualization <a class=\"anchor\" id=\"8.1\"></a>\n\n[Back to Table of Contents](#0.1)","metadata":{}},{"cell_type":"code","source":"stop_words = list(STOPWORDS) + list('0123456789') + ['rt', 'amp', 'us', 'will', 'via', 'dont', 'cant', 'u', 'work', 'im',\n                               'got', 'back', 'first', 'one', 'two', 'know', 'going', 'time', 'go', 'may', 'youtube', 'say', 'day', 'love', \n                               'still', 'see', 'watch', 'make', 'think', 'even', 'right', 'left', 'take', 'want', 'http', 'https', 'co']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_word_cloud(x, col, num_common_words, stop_words):\n    # Building the WordCloud for the num_common_words most common data in x[col] without words from list stop_words\n    \n    corpus = \" \".join(x[col].str.lower())\n    corpus = corpus.translate(str.maketrans('', '', string.punctuation))\n    corpus_without_stopwords = [word for word in corpus.split() if word not in stop_words]\n    common_words = Counter(corpus_without_stopwords).most_common(num_common_words)\n    \n    plt.figure(figsize=(12,8))\n    word_cloud = WordCloud(stopwords = stop_words,\n                           background_color='black',\n                           max_font_size = 80\n                           ).generate(\" \".join(corpus_without_stopwords))\n    plt.imshow(word_cloud)\n    plt.axis('off')\n    plt.show()\n    return common_words","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training data visualization as WordCloud\nprint('Word Cloud for training data without stopwords and apostrophes')\nplot_word_cloud(train_data, 'text', 50, stop_words)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test data visualization as WordCloud\nprint('Word Cloud for test data without stopwords and apostrophes')\nplot_word_cloud(test_data, 'text', 50, stop_words)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Form DataFrame with outliers\noutliers = pd.DataFrame(columns = ['text', 'label'])\nfor i in range(len(wrong_predictions)):\n    outliers.loc[i, 'text'] = wrong_predictions[i].text_a\n    outliers.loc[i, 'label'] = wrong_predictions[i].label","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outliers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Outliers visualization as WordCloud\nprint('Word Cloud for outliers without stopwords and apostrophes in the training data predictions')\noutliers_top50 = plot_word_cloud(outliers, 'text', 50, stop_words)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outliers_top50","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 8.2. Analysis of punctuation marks repetition in text <a class=\"anchor\" id=\"8.2\"></a>\n\n[Back to Table of Contents](#0.1)","metadata":{}},{"cell_type":"code","source":"# Analysis of punctuation marks repetition in outliers\nprint('Statistics for punctuation marks repetition in outliers')\nsubtext_repeation_in_df(outliers, 'text', list(string.punctuation), 10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 9. Showing Confusion Matrices<a class=\"anchor\" id=\"9\"></a>\n\n[Back to Table of Contents](#0.1)","metadata":{}},{"cell_type":"code","source":"# Thanks to https://www.kaggle.com/marcovasquez/basic-nlp-with-tensorflow-and-wordcloud and https://www.kaggle.com/vbmokin/nlp-eda-bag-of-words-tf-idf-glove-bert\n# Showing Confusion Matrix\ndef plot_cm(y_true, y_pred, title, figsize=(5,5)):\n    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n    cm_sum = np.sum(cm, axis=1, keepdims=True)\n    cm_perc = cm / cm_sum.astype(float) * 100\n    annot = np.empty_like(cm).astype(str)\n    nrows, ncols = cm.shape\n    for i in range(nrows):\n        for j in range(ncols):\n            c = cm[i, j]\n            p = cm_perc[i, j]\n            if i == j:\n                s = cm_sum[i]\n                annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n            elif c == 0:\n                annot[i, j] = ''\n            else:\n                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n    cm.index.name = 'Actual'\n    cm.columns.name = 'Predicted'\n    fig, ax = plt.subplots(figsize=figsize)\n    plt.title(title)\n    sns.heatmap(cm, cmap= \"YlGnBu\", annot=annot, fmt='', ax=ax)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Showing Confusion Matrix for ST Bert model\nplot_cm(pred_train, train_data['target'].values, 'Confusion matrix for ST Bert model', figsize=(7,7))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 10. Resume<a class=\"anchor\" id=\"10\"></a>\n\n[Back to Table of Contents](#0.1)","metadata":{}},{"cell_type":"code","source":"num_outliers_per_cent = round(len(outliers)*100/len(test_data), 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc_round = round(acc,3)\nprint('acc =', acc, '=', acc_round)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if n_splits == 1:\n    n_splits_res = \"\"\nelse: n_splits_res = f\"n_splits = {n_splits}, \"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Model - {model_type}, {model_name}\")\nprint(f\"* {dataset} - Commit __ (LB = 0._____): lr = {model_args['learning_rate']}, {n_splits_res}num_epochs = {model_args['num_train_epochs']}, seed = {seed}, acc = {acc_round}, num_outliers = {len(outliers)}({num_outliers_per_cent}%), weight = {weight}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I hope you find this kernel useful and enjoyable.","metadata":{}},{"cell_type":"markdown","source":"Your comments and feedback are most welcome.","metadata":{}},{"cell_type":"markdown","source":"[Go to Top](#0)","metadata":{}}]}