{"cells":[{"metadata":{},"cell_type":"markdown","source":"# EDA and preparing data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Please give a upvote","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom collections import defaultdict\nfrom collections import  Counter\nplt.style.use('ggplot')\nstop=set(stopwords.words('english'))\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport plotly.io as pio\npio.templates.default = \"plotly_dark\"\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loading data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/nlp-getting-started/train.csv')\ntest  = pd.read_csv('../input/nlp-getting-started/test.csv')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data frequency","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Train data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('There are {} rows and {} columns in the train data'.format(train.shape[0],train.shape[1]))\nprint('There are {} rows and {} columns in the test data'.format(test.shape[0],test.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Data distribution","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Around 43% of data is real tweets data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Number of real disaster tweets {} , {} %'.format(train[train.target==1].shape[0],train[train.target==1].shape[0]/train.shape[0] *100))\nprint('Number of fake disaster tweets {} , {} %'.format(train[train.target==0].shape[0],train[train.target==0].shape[0]/train.shape[0] *100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory data analysis","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Number of characters in tweets","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"tweet_len_disaster     =train[train['target']==1]['text'].str.len()\ntweet_len_non_disaster =train[train['target']==0]['text'].str.len()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig = make_subplots(rows=1, cols=2, subplot_titles=('Disaster',\"Non-disaster\"))\n\ntrace0= go.Histogram(\n    \n    x=tweet_len_disaster,\n    name=\"Disaster\",\n    opacity=0.75\n)\n\ntrace1= go.Histogram(\n    \n    x=tweet_len_non_disaster,\n    name=\"Non-Disaster\",\n    opacity=0.75\n)\n\nfig.append_trace(trace0,1,1)\nfig.append_trace(trace1,1,2)\n\nfig.update_layout(template=\"plotly_dark\",title_text='<b>Distribution of length of characters in tweets</b>',font=dict(family=\"Arial,Balto,Courier new,Droid sans\",color='white'))\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Number od words in tweets","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"tweet_len_1=train[train['target']==1]['text'].str.split().map(lambda x: len(x))\ntweet_len_0=train[train['target']==0]['text'].str.split().map(lambda x: len(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig = make_subplots(rows=1, cols=2, subplot_titles=('Disaster',\"Non-disaster\"))\n\ntrace0= go.Histogram(\n    \n    x=tweet_len_1,\n    name=\"Disaster\",\n    opacity=0.75\n)\n\ntrace1= go.Histogram(\n    \n    x=tweet_len_0,\n    name=\"Non-Disaster\",\n    opacity=0.75\n)\n\nfig.append_trace(trace0,1,1)\nfig.append_trace(trace1,1,2)\n\nfig.update_layout(template=\"plotly_dark\",title_text='<b>Distribution of length of words in tweets</b>',font=dict(family=\"Arial,Balto,Courier new,Droid sans\",color='white'))\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Average world length","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"word_1=train[train['target']==1]['text'].str.split().apply(lambda x : [len(i) for i in x])\nword_0=train[train['target']==0]['text'].str.split().apply(lambda x : [len(i) for i in x])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_1=word_1.map(lambda x: np.mean(x))\nword_0=word_0.map(lambda x: np.mean(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hist_data = [word_1,word_0]\ngroup_labels = ['disaster','non-disaster']\nfig = ff.create_distplot(hist_data, group_labels, bin_size=.2)\nfig.update_layout(title_text='Average word length in a tweet',width=900,height=450)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"def create_corpus(target):\n    corpus=[]\n    \n    for x in train[train['target']==target]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Common stop words in tweets","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"corpus_0=create_corpus(0)\n\ndic_0=defaultdict(int)\nfor word in corpus_0:\n    if word in stop:\n        dic_0[word]+=1\n\ntop_0=sorted(dic_0.items(), key=lambda x:x[1],reverse=True)[:10] \n    \n\n\nx_0,y_0=zip(*top_0)\n\n\ncorpus_1=create_corpus(1)\n\ndic_1=defaultdict(int)\nfor word in corpus_1:\n    if word in stop:\n        dic_1[word]+=1\n        \ntop_1=sorted(dic_1.items(), key=lambda x:x[1],reverse=True)[:10] \n\nx_1,y_1=zip(*top_1)\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = go.Figure(data=[\n    go.Bar(name='non-disaster', x=x_0, y=y_0),\n    go.Bar(name='diaster', x=x_1, y=y_1)\n])\nfig.update_layout(title_text='common stop words')\nfig.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Punctuations","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import string\nspecial = string.punctuation\n\n\n\ncorpus0=create_corpus(0)\n\ndic0=defaultdict(int)\n\nfor i in (corpus0):\n    if i in special:\n        dic0[i]+=1\n        \n\ncorpus1=create_corpus(1)\n\ndic1=defaultdict(int)\nfor i in (corpus1):\n    if i in special:\n        dic1[i]+=1\n      \n\n    \nx0,y0=zip(*dic0.items())\n\nx1,y1=zip(*dic1.items())    \n        \n\n    \n    \nfig = go.Figure(data=[\n    go.Bar(name='non-disaster', x=x0, y=y0),\n    go.Bar(name='diaster', x=x1, y=y1)\n])\nfig.update_layout(title_text='Punctuations')\nfig.show()\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Common words","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"counter0=Counter(corpus0)\nmost0=counter0.most_common()\nx0=[]\ny0=[]\nfor word,count in most0[:100]:\n    if (word not in stop) :\n        x0.append(word)\n        y0.append(count)\n\ncounter1=Counter(corpus1)\nmost1=counter1.most_common()\nx1=[]\ny1=[]\nfor word,count in most1[:100]:\n    if (word not in stop) :\n        x1.append(word)\n        y1.append(count)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure(data=[\n    go.Bar(name='non-disaster', x=x0, y=y0),\n    go.Bar(name='diaster', x=x1, y=y1)\n])\nfig.update_layout(title_text='common  words')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ngram analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_top_tweet_bigrams(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_tweet_bigrams=get_top_tweet_bigrams(train[train.target==0]['text'])[:10]\nx0,y0=map(list,zip(*top_tweet_bigrams))\ntop_tweet_bigrams=get_top_tweet_bigrams(train[train.target==1]['text'])[:10]\nx1,y1=map(list,zip(*top_tweet_bigrams))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure(data=[\n    go.Bar(name='non-disaster', x=x0, y=y0),\n    go.Bar(name='diaster', x=x1, y=y1)\n])\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data cleaning","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Removing urls","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_URL(text):\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url.sub(r'',text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"remove_URL(\"New competition launched :https://www.kaggle.com/c/nlp-getting-started\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['text']=train['text'].apply(lambda x : remove_URL(x))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Removing html tags","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"example = \"\"\"<div>\n<h1>Real or Fake</h1>\n<p>Kaggle </p>\n<a href=\"https://www.kaggle.com/c/nlp-getting-started\">getting started</a>\n</div>\"\"\"\nprint(remove_html(example))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['text']=train['text'].apply(lambda x : remove_html(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Removing Emojis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"remove_emoji(\"Omg another Earthquake ðŸ˜”ðŸ˜”\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['text']=train['text'].apply(lambda x: remove_emoji(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Removing punctuations","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nexample=\"I am a #king\"\nprint(remove_punct(example))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['text']=train['text'].apply(lambda x : remove_punct(x))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}