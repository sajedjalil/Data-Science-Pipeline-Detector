{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Enginnering"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train=pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest=pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train[['text','target']]\ntest = test[['id','text']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import PorterStemmer\nimport nltk\nfrom nltk.corpus import stopwords\nstopwords = set(stopwords.words(\"english\"))\nimport re","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Remove stopwords (or, are, is etc) from data "},{"metadata":{"trusted":true},"cell_type":"code","source":"train['text'] = train['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))\ntest['text'] = test['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus_train = train['text']\ncorpus_test = test['text']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def replace(text):\n    text = text.str.replace(r'^.+@[^\\.].*\\.[a-z]{2,}$',\" \")  # remove emailaddress\n    text = text.str.replace(r'\\W+',\" \")     # remove symbols\n    text = text.str.replace(r' ',\" \")       # remove punctuations\n    text = text.str.replace('\\d+',\" \")      # remove numbers\n    text = text.str.lower()                 # remove capital letters as they does not make any effect\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus_train = replace(corpus_train)\ncorpus_test = replace(corpus_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nnltk.download('wordnet')\nfrom textblob import Word","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Remove rare words from text that are not been used oftenly"},{"metadata":{"trusted":true},"cell_type":"code","source":"freq = pd.Series(' '.join(corpus_train).split()).value_counts()[-19500:]\ncorpus_train = corpus_train.apply(lambda x: \" \".join(x for x in x.split() if x not in freq))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"freq.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"freq = pd.Series(' '.join(corpus_test).split()).value_counts()[-10000:]\ncorpus_test = corpus_test.apply(lambda x: \" \".join(x for x in x.split() if x not in freq))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualise most occuring words from training corpus"},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud \nimport matplotlib.pyplot as plt\ndef wordcloud(text):\n    wordcloud = WordCloud(\n        background_color='white',\n        max_words=500,\n        max_font_size=30, \n        scale=3,\n        random_state=5\n    ).generate(str(corpus_train))\n    fig = plt.figure(figsize=(15, 12))\n    plt.axis('off')\n    plt.imshow(wordcloud)\n    plt.show()\n    \nwordcloud(corpus_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\ntarget = train['target']\nsns.countplot(target)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Unlike humans, machine cannot understand raw text. Hence need to convert text into corresponding numerical form.<br>\nTfidfvectorizer count each word occurence from document "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nTfidf_vect = TfidfVectorizer(max_features = 7000)\nTfidf_vect.fit(corpus_train)\nX_train = Tfidf_vect.transform(corpus_train)\nX_test = Tfidf_vect.transform(corpus_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Hyperparameter Tuning \nParameters are conditions or settings that are to be defined in models. These changing of parameters according to the need is called *Hyperparameter Tuning*. \nTechnically parameters passed within algorithm are not best parameters for every dataset. \nHence to choose the best parameters hyperparameter tuning is done\nHyperparameter tuning are of two types *Grid SearchCV* and *Random SearchCV.*\n \n*Grid Search* is the approach where every parameter is selected from grid list specified and tried on the model and the best one can be interpreted. We will use Grid Search approach in this problem.<br>\nWhere in *Random Search*, search the parameter randomly sepcified randomly choosed according to the specifity of model with in range. \n![](https://i.stack.imgur.com/cIDuR.png)\nAbove diagram states that Grid Search explores at every fix distinct places while Random Search has no such fix trials"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\nparameters = { \n    'gamma': [0.001, 0.01, 0.1, 0.4, 0.6, 0.7, 'auto'], # for complex decision boundary (mainly used for rbf kerel)\n    \n    'kernel': ['rbf','linear'], # used for different type of data\n                                # linear - when data is easy to classify \n                                # rbf - when data is too complex\n    \n    'C': [0.001, 0.01, 0.1, 1, 1.5, 2, 3, 10], # inverse weight on regularization parameter \n                                               # (how finely to classify, decreasing will prevent overfititing and vice versa)\n}\nmodel = GridSearchCV(SVC(), parameters, cv=10, n_jobs=-1).fit(X_train, target)\nmodel.cv_results_['params'][model.best_index_]\ny_val_pred = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above hyperparameter tuning is time consuming so putting the results directly we get,"},{"metadata":{},"cell_type":"markdown","source":"Here we will use **SVC (Support Vector Classifier)** <br>\nSVC aims to fit data that is provided with returning best fit hyperplane that divides <br>\nthe data between classes while prediction helps to sort which class features belongs to."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nSVM = SVC(C=1.0, kernel='linear', gamma='auto')\nSVM.fit(X_train,target)\nSVM_predictions = SVM.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"file_submission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\nfile_submission.target = SVM_predictions\nfile_submission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}