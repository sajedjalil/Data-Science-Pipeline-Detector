{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Main objectives\n\n1. Performing exploratory data analysis of tweets contained in the dataset.\n2. Data cleaning of text data.\n3. Tokenizing interesting parts of tweets and prepare them for machine learning algorithm. \n4. Using different algorithms trying to achieve best results in public leaderboard. "},{"metadata":{},"cell_type":"markdown","source":"# Import libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd \nimport numpy as np \n\n#Plots\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Map\nfrom geopy.geocoders import Nominatim\nfrom geopy.extra.rate_limiter import RateLimiter\nimport folium \nfrom folium import plugins \n\n#Worldcloud\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\n#Regex\nimport re\n\n#String\nimport string\n\n#Sklearn\nfrom sklearn.svm import LinearSVC\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, confusion_matrix\n\n#Tensorflow\nimport tensorflow as tf\n\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\n\n#Spacy\nimport spacy\nfrom spacy import displacy\nnlp=spacy.load('en_core_web_sm')\n\nsns.set_style('whitegrid')\n%matplotlib inline\n\n\nfrom nltk.tokenize import word_tokenize\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train=pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest=pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Quick overview"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Train dataset: {} rows arranged in {} columns.\".format(train.shape[0],train.shape[1]))\nprint(\"Test dataset: {} rows arranged in {} columns.\".format(test.shape[0],test.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Sneek peek of datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Columns description:\n\n* id - identifier for each tweet\n* keyword - a particular keyword from the tweet (contain NaN)\n* location - the location the tweet was sent from (contain NaN)\n* text - the text of the tweet\n* target - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)"},{"metadata":{},"cell_type":"markdown","source":"# Missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_columns = ['keyword', 'location']\n\n\nfig = plt.figure(figsize=(14,6))\n\nax1=fig.add_subplot(121)\nsns.barplot(x=train[missing_columns].isnull().sum().index, y=train[missing_columns].isnull().sum().values,palette='mako',ax=ax1)\nax1.set_title('Missing values in train set')\n\nax2=fig.add_subplot(122)\nsns.barplot(x=test[missing_columns].isnull().sum().index, y=test[missing_columns].isnull().sum().values,palette='mako',ax=ax2)\nax2.set_title('Missing values in test set')\n\nfig.suptitle('Missing values in dataset')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see in both train and test sets there is a lot of missing values for two columns - 'keyword' and 'location'. For further building of machine learning models, missing values will be dropped."},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"### Preview of our target variable - \"target\"."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Extract number of target values.\nvalues=train.target.value_counts()\nplt.figure(figsize=(7,6))\nsns.barplot(x=values.index,y=values,palette=['blue','red'])\nplt.ylabel('Samples')\nplt.xlabel('0:Not disaster | 1:Disaster')\nplt.title('Distribution of target values',fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"disaster = train.target.value_counts()[1]/len(train.target)\nnot_disaster = train.target.value_counts()[0]/len(train.target)\npercentage = {'Disaster tweets %':[disaster], 'Non disaster tweets %':[not_disaster]}\npercentage_data = pd.DataFrame(percentage)\npercentage_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = train.groupby('target').size()\n\ndata.plot(kind='pie', subplots=True, figsize=(10, 8), autopct = \"%.2f%%\", colors=['blue','red'])\nplt.title(\"Pie chart of different types of disasters\",fontsize=16)\nplt.ylabel(\"\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In considered population of tweets there is more non disaster tweets."},{"metadata":{},"cell_type":"markdown","source":"### Comparasion of text length in Real and Fake disaster Tweets\nAdd column to hold particular tweet length"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.length = train.text.apply(len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(14,6))\nax1 = fig.add_subplot(121)\nsns.boxplot(x=train.target[train.target==0],y=train.length, ax=ax1,color='blue')\ndescribe = train.length[train.target==0].describe().to_frame().round(2)\n\nax2 = fig.add_subplot(122)\nax2.axis('off')\nfont_size = 16\nbbox = [0, 0, 1, 1]\ntable = ax2.table(cellText = describe.values, rowLabels = describe.index, bbox=bbox, colLabels=describe.columns)\ntable.set_fontsize(font_size)\nfig.suptitle('Distribution of text length for non disaster tweets.', fontsize=16)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(14,6))\nax1 = fig.add_subplot(121)\nsns.boxplot(x=train.target[train.target==1],y=train.length, ax=ax1,color='red')\ndescribe = train.length[train.target==1].describe().to_frame().round(2)\n\nax2 = fig.add_subplot(122)\nax2.axis('off')\nfont_size = 16\nbbox = [0, 0, 1, 1]\ntable = ax2.table(cellText = describe.values, rowLabels = describe.index, bbox=bbox, colLabels=describe.columns)\ntable.set_fontsize(font_size)\nfig.suptitle('Distribution of text length for disaster tweets.', fontsize=16)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Tweet keywords analysis\n\nUnique words in test and train data."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Unique keywords in train data: {}'.format(len(train.keyword.unique())))\nprint('Unique keywords in test data: {}'.format(len(test.keyword.unique())))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(14,6))\n\nax1 = fig.add_subplot(111)\ntrain_keywords=sns.barplot(x=train.keyword.value_counts()[:25].index,y=train.keyword.value_counts()[:25][:],palette='icefire',ax=ax1)\ntrain_keywords.set_xticklabels(train_keywords.get_xticklabels(),rotation=90)\ntrain_keywords.set_ylabel('Keyword frequency')\nplt.title('Top 25 common keywords in train data',fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,6))\ncommon_keyword=sns.barplot(x=train.location.value_counts()[:25].index,y=train.location.value_counts()[:25][:],palette='icefire')\ncommon_keyword.set_xticklabels(common_keyword.get_xticklabels(),rotation=90)\ncommon_keyword.set_ylabel('Location frequency',fontsize=12)\nplt.title('Top 25 common location of tweets for train data',fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(14,6))\n\nax1 = fig.add_subplot(111)\ntest_keywords=sns.barplot(x=test.keyword.value_counts()[:25].index,y=test.keyword.value_counts()[:25][:],palette='icefire',ax=ax1)\ntest_keywords.set_xticklabels(test_keywords.get_xticklabels(),rotation=90)\ntest_keywords.set_ylabel('Keyword frequency')\nplt.title('Top 25 common keywords in test data',fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on this plot we can assume that all disaster tweets(fake or real) contains some words related to disasters."},{"metadata":{},"cell_type":"markdown","source":"### Analysis of the number of words used in tweets"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['number_of_words'] = train.text.apply(lambda x: len((str(x).split())))\ntrain['number_of_unique_words'] = train.text.apply(lambda x: len(set(str(x).split())))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(ncols=2,figsize=(14,7))\n\nword_count_1 = sns.distplot(train.number_of_words[train.target==1],color='red',ax=ax[0])\nword_count_0 = sns.distplot(train.number_of_words[train.target==0],color='blue',ax=ax[0])\nunique_count_1 = sns.distplot(train.number_of_unique_words[train.target==1],color='red',ax=ax[1])\nunique_count_0 = sns.distplot(train.number_of_unique_words[train.target==0],color='blue',ax=ax[1])\nword_count_1.set_title('Number of words used to disaster vs. non disaster tweets')\nunique_count_1.set_title('Number of unique words used to disaster vs. non disaster tweets')\nplt.suptitle('Analysis of number of words used in tweets',fontsize=16)\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From these distplots we can formulate a conclusion that the content of disaster tweets contains more words and at the same time more unique words."},{"metadata":{},"cell_type":"markdown","source":"### Most frequent tweet locations"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Unique keywords in train data: {}'.format(len(train.location.unique())))\nprint('Unique keywords in test data: {}'.format(len(test.location.unique())))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,6))\ncommon_keyword=sns.barplot(x=test.location.value_counts()[:25].index,y=test.location.value_counts()[:25][:],palette='icefire')\ncommon_keyword.set_xticklabels(common_keyword.get_xticklabels(),rotation=90)\ncommon_keyword.set_ylabel('Location frequency',fontsize=12)\nplt.title('Top 25 common location of tweets for test data',fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualize top 25 locations of disaster tweets on world map"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = train.location.value_counts()[:25,]\ndata = pd.DataFrame(data)\ndata = data.reset_index()\ndata.columns = ['location', 'counts'] \ngeolocator = Nominatim(user_agent=\"Location Map\")\ngeocode = RateLimiter(geolocator.geocode, min_delay_seconds=1)\n\ndict_latitude = {}\ndict_longitude = {}\nfor i in data.location.values:\n    print(i)\n    location = geocode(i)\n    dict_latitude[i] = location.latitude\n    dict_longitude[i] = location.longitude\ndata['latitude'] = data.location.map(dict_latitude)\ndata['longitude'] = data.location.map(dict_longitude)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"location_map = folium.Map(location=[7.0, 7.0], zoom_start=2)\nmarkers = []\nfor i, row in data.iterrows():\n    loss = row['counts']\n    if row['counts'] > 0:\n        count = row['counts']*0.4\n    folium.CircleMarker([float(row['latitude']), float(row['longitude'])], radius=float(count), color='red', fill=True).add_to(location_map)\nlocation_map","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Missing values\n\nThanks to: https://www.kaggle.com/gunesevitan/nlp-with-disaster-tweets-eda-cleaning-and-bert"},{"metadata":{},"cell_type":"markdown","source":"### Removing URLS"},{"metadata":{"trusted":true},"cell_type":"code","source":"example=\"AMAZING NOTEBOOK(shameless self promotion :D): https://www.kaggle.com/michawilkosz/simple-way-to-top-26-blended-regression-model\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_URL(text):\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url.sub(r'',text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(remove_URL(example))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['text'] = train['text'].apply(lambda x : remove_URL(x))\ntest['text'] = test['text'].apply(lambda x : remove_URL(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Removing HTML tags"},{"metadata":{"trusted":true},"cell_type":"code","source":"example = \"\"\"<div>\n<h1>House Prices Notebook</h1>\n<p>Simple way to top 26 blended regression model</p>\n<a href=\"https://www.kaggle.com/michawilkosz/simple-way-to-top-26-blended-regression-model</a>\n</div>\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(remove_html(example))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['text']=train['text'].apply(lambda x : remove_html(x))\ntest['text']=test['text'].apply(lambda x : remove_html(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Removing Emojis"},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(remove_emoji(\"Oh no! Another pandemic ðŸ˜”ðŸ˜”\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['text']=train['text'].apply(lambda x : remove_emoji(x))\ntest['text']=test['text'].apply(lambda x : remove_emoji(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Removing punctuations"},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\nexample=\"I am a #king\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(remove_punct(example))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And of course columns with NaN values:"},{"metadata":{"trusted":true},"cell_type":"code","source":"test.dropna(how='any',inplace=True,axis=1)\ntrain.dropna(how='any',inplace=True,axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Cool looking and useful wordclouds\nThanks to: https://www.datacamp.com/community/tutorials/wordcloud-python"},{"metadata":{"trusted":true},"cell_type":"code","source":"def exctract_text(data, target):\n    extracted=[]\n    \n    for x in data[data['target']==target]['text'].str.split():\n        for i in x:\n            extracted.append(i)\n    return extracted","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"extracted_text_1 = exctract_text(train,1)\nextracted_text_0 = exctract_text(train,0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,6))\nword_cloud = WordCloud(background_color=\"white\",max_font_size=60).generate(\" \".join(extracted_text_1[:50]))\nplt.imshow(word_cloud,interpolation='bilinear')\nplt.axis('off')\nplt.title('Most common words in disaster tweets.',fontsize=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,6))\nword_cloud = WordCloud(background_color=\"white\",max_font_size=60).generate(\" \".join(extracted_text_0[:50]))\nplt.imshow(word_cloud,interpolation='bilinear')\nplt.axis('off')\nplt.title('Most common words in non disaster tweets.',fontsize=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on sampled wordclouds for both disaster and non disaster tweets. It is clear that non disaster tweets contains more words used in colloquial speech(e.g. love, car, fruits, summer)."},{"metadata":{},"cell_type":"markdown","source":"# Tokenizer from spacy"},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom spacy.lang.en import English\n\nmarks = string.punctuation\nmarks = list(marks)\nmarks.append(\"...\")\nmarks.append(\"....\")\n\n\n\nnlp = spacy.load('en')\nstop_words = list(STOP_WORDS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenizer(sentence):\n    doc = nlp(sentence)\n    clean_tokens = []\n    for token in doc:\n        if token.lemma_ != '-PRON-':\n            token = token.lemma_.lower().strip()\n        else:\n            token = token.lower_\n        if token not in stop_words and token not in marks:\n            clean_tokens.append(token)\n    return clean_tokens","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bow_vector = CountVectorizer(tokenizer = tokenizer, ngram_range=(1,1))\ntfidf_vector = TfidfVectorizer(tokenizer = tokenizer)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train test split "},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train['text']\ny = train['target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Logistic Regression Classifier\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression()\n\n#Pipeline\nlr_pipe = Pipeline([('vectorizer', bow_vector),\n                 ('classifier', classifier)])\n\n# model generation\nlr_pipe.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting with a test dataset\nlr_pred = lr_pipe.predict(X_test)\n\n# Model Accuracy\nprint(\"Logistic Regression Accuracy:\",accuracy_score(y_test, lr_pred))\nprint(\"Logistic Regression Precision:\",precision_score(y_test, lr_pred))\nprint(\"Logistic Regression Recall:\",recall_score(y_test, lr_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# SVM"},{"metadata":{"trusted":true},"cell_type":"code","source":"svc = LinearSVC()\nsvc_pipe = Pipeline([('tfidf', tfidf_vector), ('clf', svc)])\nsvc_pipe.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svc_pred = svc_pipe.predict(X_test)\n\n# Model Accuracy\nprint(\"SVC Accuracy:\",accuracy_score(y_test, svc_pred))\nprint(\"SVC Precision:\",precision_score(y_test, svc_pred))\nprint(\"SVC Regression Recall:\",recall_score(y_test, svc_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LSTM"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train['text']\ny = train['target']\nX = X.values\ny = y.values\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(X)  \nvocab_size = len(tokenizer.word_index) + 1 \n\nsequences = tokenizer.texts_to_sequences(X) \n\nmax_length = 0 \nfor i in X:\n    if len(i) > max_length:\n        max_length = len(i)\n        \npadded = pad_sequences(sequences, maxlen=max_length, padding='post')\n\nembeddings_dictionary = dict()\nembedding_dim = 100\nglove_file = open('../input/glove6b100dtxt/glove.6B.100d.txt')\n\nfor line in glove_file:\n    records = line.split()\n    word = records[0]\n    vector_dimensions = np.asarray(records[1:], dtype='float32')\n    embeddings_dictionary [word] = vector_dimensions\n    \nglove_file.close()\n\nembeddings_matrix = np.zeros((vocab_size, embedding_dim))\nfor word, index in tokenizer.word_index.items():\n    embedding_vector = embeddings_dictionary.get(word)\n    if embedding_vector is not None:\n        embeddings_matrix[index] = embedding_vector\n\nX_train, X_test, y_train, y_test = train_test_split(padded, y, test_size=0.2)\n\nX_train = np.array(X_train)\nX_test = np.array(X_test)\ny_train = np.array(y_train)\ny_test = np.array(y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length, weights=[embeddings_matrix], trainable=False),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Conv1D(64, 5, activation='relu'),\n    tf.keras.layers.MaxPooling1D(pool_size=4),\n    tf.keras.layers.LSTM(64),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()\n\nnum_epochs = 50\n\nhistory = model.fit(X_train, y_train, epochs=num_epochs, validation_data=(X_test, y_test), verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lstm_pred = model.predict_classes(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"LSTM Accuracy:\",accuracy_score(y_test, lstm_pred))\nprint(\"LSTM Precision:\",precision_score(y_test, lstm_pred))\nprint(\"LSTM Recall:\",recall_score(y_test, lstm_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# BERT\nSimple try of this powerful pre-trained model. Works really slowly.\n\nThanks for awesome tutorial: https://www.kaggle.com/pavansanagapati/knowledge-graph-nlp-tutorial-bert-spacy-nltk"},{"metadata":{"trusted":true},"cell_type":"code","source":"!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\n\nimport tokenization\n\nmax_len=512","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(bert_layer, max_len=512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(clf_output)\n    \n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\nbert_layer = hub.KerasLayer(module_url, trainable=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\nsubmission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_input = bert_encode(train.text.values, tokenizer, max_len=160)\ntest_input = bert_encode(test.text.values, tokenizer, max_len=160)\ntrain_labels = train.target.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = build_model(bert_layer, max_len=160)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint = ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True)\n\ntrain_history = model.fit(\n    train_input, train_labels,\n    validation_split=0.2,\n    epochs=3,\n    callbacks=[checkpoint],\n    batch_size=16\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_weights('model.h5')\ntest_pred = model.predict(test_input)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission['target'] = test_pred.round().astype(int)\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}