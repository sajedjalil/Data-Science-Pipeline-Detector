{"cells":[{"metadata":{},"cell_type":"markdown","source":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n  <h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\">&nbsp;Summary:</h1>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#1\" role=\"tab\" aria-controls=\"profile\">1. Introduction<span class=\"badge badge-primary badge-pill\">1</span></a>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#2\" role=\"tab\" aria-controls=\"messages\">2. EDA<span class=\"badge badge-primary badge-pill\">2</span></a>\n   <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#3\" role=\"tab\" aria-controls=\"messages\">3. Models<span class=\"badge badge-primary badge-pill\">3</span></a>\n     <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#4\" role=\"tab\" aria-controls=\"messages\">4. Evaluate Models<span class=\"badge badge-primary badge-pill\">4</span></a>\n</div>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"!pip3 install mglearn","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ndf_test = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1\"></a> <br>\n<font size=\"+3\" color=\"black\"><b>1 - Introduction</b></font><br><a id=\"1\"></a>\n<br> \n\n* first I will observe the distribution between the real and fake twitters\n* And let's see how the usage ratio is between real and fake twitter, in an attempt to identify patterns\n","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import re\ndef count_twitters_user(df):\n    twitter_username_re = re.compile(r'@([A-Za-z0-9_]+)')\n    count = 0\n    list_ = []\n    for text in df['text']:\n        users_in_twitter = re.findall(twitter_username_re, text)\n        for user in users_in_twitter:\n            list_.append(user)\n        count += len(users_in_twitter)\n    return len(set(list_)), set(list_)\n\n\ndef count_twitters_hashtags(df):\n    twitter_hashtag_re = re.compile(r'#([A-Za-z0-9_]+)')\n    count = 0\n    list_ = []\n    for text in df['text']:\n        hashtags = re.findall(twitter_hashtag_re, text)\n        for tags in hashtags:\n            list_.append(tags)\n        count += len(hashtags)\n    return len(set(list_)), set(list_)\n\nreal = df_train[df_train['target']==1]\nfake = df_train[df_train['target']==0]\n\ncount_real, users_real = count_twitters_user(real)\ncount_fake, users_fake = count_twitters_user(fake)\n\n\ncount_tags_real, tags_real = count_twitters_hashtags(real)\ncount_tags_fake, tags_fake = count_twitters_hashtags(fake)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"2\"></a> <br>\n<font size=\"+3\" color=\"black\"><b>3 - EDA</b></font><br><a id=\"2\"></a>\n<br> ","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import plotly.express as px\nimport plotly.graph_objects as go\nfig = go.Figure()\nfig.add_trace(go.Pie(\n    values=list(df_train['target'].value_counts()),\n    labels=['Real', 'Fake'],\n   # marker_color='lightsalmon'\n))\nfig.update_layout({\n        'plot_bgcolor': 'rgba(0, 0, 0, 0)',\n        'paper_bgcolor': 'rgba(0, 0, 0, 0)',\n        'title': 'Distribution',\n})\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* the data is relatively balanced.\n* observe the values and presence of # among twitter in an attempt to observe some pattern for identification and users\n","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig = go.Figure()\nfig.add_trace(go.Bar(\n    y=[count_tags_real, count_tags_fake],\n    x=['Real', 'Fake'],\n    marker_color='lightsalmon'\n))\nfig.update_layout({\n        'plot_bgcolor': 'rgba(0, 0, 0, 0)',\n        'paper_bgcolor': 'rgba(0, 0, 0, 0)',\n        'title': 'unique hashtags mentions in twitters',\n})\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig = go.Figure()\nfig.add_trace(go.Bar(\n    y=[count_real, count_fake],\n    x=['Real', 'Fake'],\n    marker_color='lightsalmon'\n))\nfig.update_layout({\n        'plot_bgcolor': 'rgba(0, 0, 0, 0)',\n        'paper_bgcolor': 'rgba(0, 0, 0, 0)',\n        'title': 'unique \"@users\" mentions in twitters',\n})\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"users_only_fake = list(users_fake - users_real)\nprint('there are', len(users_only_fake), 'mentionate only fake twitter news')\nusers_only_fake = list(tags_fake - tags_real)\nprint('there are', len(users_only_fake), 'mentionate only fake twitter news')\nusers_only_fake = list(users_real - users_fake)\nprint('there are', len(users_only_fake), 'mentionate only fake twitter real')\nusers_only_fake = list(tags_real - tags_fake)\nprint('there are', len(users_only_fake), 'mentionate only fake twitter real')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Based on the information above the hashtags and users bring information regarding real and fake twitters, so in preprocessing the data is interested in maintaining the presence of these elements.\n","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\ndef get_top_n_words(corpus, n=None, stop=None):\n    vec = CountVectorizer(stop_words=stop).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in     vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from plotly.subplots import make_subplots\n\nfig = make_subplots(\n    rows=2, cols=2,\n    specs=[[{}, {}],\n           [{\"colspan\": 2}, None]],\n    subplot_titles=(\"Real twitters\",\"Fake twitters\", \"All twitters\"))\n\ndata = get_top_n_words(df_train['text'], 25)\nnew_list_words = [ seq[0] for seq in data ]\nnew_list_values = [ seq[1] for seq in data ]\n\ndata_real = get_top_n_words(real['text'], 25)\nnew_list_words_real = [ seq[0] for seq in data_real ]\nnew_list_values_real = [ seq[1] for seq in data_real ]\n\ndata_fake = get_top_n_words(fake['text'], 25)\nnew_list_words_fake = [ seq[0] for seq in data_fake ]\nnew_list_values_fake = [ seq[1] for seq in data_fake ]\n\n\nfig.add_trace(go.Bar(x=new_list_words_real, y=new_list_values_real),\n                 row=1, col=1)\n\nfig.add_trace(go.Bar(x=new_list_words_fake, y=new_list_values_fake),\n                 row=1, col=2)\nfig.add_trace(go.Bar(x=new_list_words, y=new_list_values),\n                 row=2, col=1)\n\nfig.update_layout(showlegend=False, title_text=\"Specs with Subplot Title\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The frequent words are similar in fake and real twitters.\n\n","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":false},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation as LDA\nimport mglearn\nvec = CountVectorizer()\n\nX = vec.fit_transform(df_train['text'])\n\ncomponents = 6\n\nlda=LDA(n_components=components, n_jobs=-1, random_state=42)\nlda_dtf=lda.fit_transform(X)\nsorting=np.argsort(lda.components_)[:,::-1]\nfeatures=np.array(vec.get_feature_names())\nmglearn.tools.print_topics(topics=range(components), feature_names=features,sorting=sorting, topics_per_chunk=components, n_words=15)\n\ntopic = []\nfor n in range(lda_dtf.shape[0]):\n    topic_most_pr = lda_dtf[n].argmax()\n    topic.append(topic_most_pr)\ndf_train['topic'] = topic","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* even with topic modeling there are only stopwords, they will be removed for better visualization of relevant words from the data\n","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import nltk\nstopwords = nltk.corpus.stopwords.words('english')\nstopwords = stopwords + [\"http\", \"https\", \"co\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"real = df_train[df_train['target']==1]\nfake = df_train[df_train['target']==0]\nfig = make_subplots(\n    rows=2, cols=2,\n    specs=[[{}, {}],\n           [{\"colspan\": 2}, None]],\n    subplot_titles=(\"Real twitters\",\"Fake twitters\", \"All twitters\"))\n\ndata = get_top_n_words(df_train['text'], 25, stopwords)\nnew_list_words = [ seq[0] for seq in data ]\nnew_list_values = [ seq[1] for seq in data ]\n\ndata_real = get_top_n_words(real['text'], 25, stopwords)\nnew_list_words_real = [ seq[0] for seq in data_real ]\nnew_list_values_real = [ seq[1] for seq in data_real ]\n\ndata_fake = get_top_n_words(fake['text'], 25, stopwords)\nnew_list_words_fake = [ seq[0] for seq in data_fake ]\nnew_list_values_fake = [ seq[1] for seq in data_fake ]\n\n\nfig.add_trace(go.Bar(x=new_list_words_real, y=new_list_values_real),\n                 row=1, col=1)\n\nfig.add_trace(go.Bar(x=new_list_words_fake, y=new_list_values_fake),\n                 row=1, col=2)\nfig.add_trace(go.Bar(x=new_list_words, y=new_list_values),\n                 row=2, col=1)\n\nfig.update_layout(showlegend=False, title_text=\"Specs with Subplot Title\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* the most frequent words seem to be just stopwords and \"http\", \"https\" and \"co\", so let's remove all those words and see which are the most relevant words using chi2","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_selection import chi2\n\nvectorizer = CountVectorizer(stop_words = stopwords)\n\nX = vectorizer.fit_transform(df_train['text'])\n\nchi2score = chi2(X,df_train['target'])[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"wscores = dict(zip(vectorizer.get_feature_names(), chi2score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"dict_ = {k: v for k, v in sorted(wscores.items(), key=lambda item: item[1], reverse=True)}\nkeys = list(dict_.keys())\nvalues = list(dict_.values())\nfig = px.bar(x=list(keys[0:50]), y=list(values[0:50]))\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* with the removal of stopwords words appearing related to disasters.\n* let's start modeling, performing two experiments, one using the stopwords removal and the other keeping the original data.\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3\"></a> <br>\n<font size=\"+3\" color=\"black\"><b>3 - Model</b></font><br><a id=\"3\"></a>\n<br> \n\n*  So now i will test the model classification ignoring the use of stopwords and the words http, https and co, training some models using hyper parameterization to get better results and in the end use stack to combine the result in the perspective of having a better result\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## XGBClassifier","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import xgboost\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\n\nvec = TfidfVectorizer(stop_words=stopwords)\nX= vec.fit_transform(df_train['text'])\ny = df_train['target']\nxgboost_params = {'n_estimators' :[25,50,100, None],\n                   'learning_rate': [0.0001, 0.001, 0.01, 0.1],\n                  'gamma':[0.5, 0.1, 1, 10],\n                  'max_depth':[5, 10, 15, None]}\n\nxgb = xgboost.XGBClassifier(random_state=42)\nclf_xgb = GridSearchCV(xgb, xgboost_params, cv=5,n_jobs= 4, verbose = 1)\nclf_xgb.fit(X, y)\nprint(clf_xgb.best_estimator_)\nprint(clf_xgb.best_score_)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LGBMClassifier","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import lightgbm as lgb\nlightgbm_params ={'learning_rate':[0.0001, 0.001, 0.003, 0.01, 0.1],\n                  'n_estimators':[10,20, 50, 100, None],\n                 'max_depth':[4, 6, 10, 15, 20, 50, None]}\ngbm = lgb.LGBMClassifier(random_state = 42)\nclf_gbm = GridSearchCV(gbm, lightgbm_params, cv=5,n_jobs= 4, verbose = 1)\nclf_gbm.fit(X, y)\nprint(clf_gbm.best_estimator_)\nprint(clf_gbm.best_score_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LinearSVC","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from sklearn.svm import LinearSVC\n\nsvr_params = {'C':[0.0001, 0.001,0.01, 0.1, 1 , 10, 100]}\nsvr = LinearSVC(random_state=42)\nclf_svr = GridSearchCV(svr, svr_params, cv=5, n_jobs=4, verbose=1)\nclf_svr.fit(X, y)\nprint(clf_svr.best_estimator_)\nprint(clf_svr.best_score_)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## RandomForestClassifier","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nlightran_params ={'n_estimators':[10,20, 50, 100, None],\n                 'max_depth':[4, 6, 10, 15, 20, 50, None]}\nrandom = RandomForestClassifier(random_state = 42)\nclf_random = GridSearchCV(random, lightran_params, cv=5,n_jobs= 4, verbose = 1)\nclf_random.fit(X, y)\nprint(clf_random.best_estimator_)\nprint(clf_random.best_score_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LogisticRegression","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlf_params ={'C':[0.0001, 0.001,0.01, 0.1, 1 , 10, 100],\n           'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\n\nlf = LogisticRegression(random_state=42)\nclf_lf = GridSearchCV(lf, lf_params, cv=5,n_jobs= 4, verbose = 1)\nclf_lf.fit(X, y)\n\nprint(clf_lf.best_estimator_)\nprint(clf_lf.best_score_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ExtraTreesClassifier","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesClassifier\nlightree_params ={'n_estimators':[10,20, 50, 100, None],\n                  'max_depth':[4, 6, 10, 15, 20, 50, None]}\n\ntree = ExtraTreesClassifier(random_state=42, n_jobs=4)\nclf_tree = GridSearchCV(tree, lightree_params, cv=5, n_jobs= 4, verbose = 1)\nclf_tree.fit(X, y)\nprint(clf_tree.best_estimator_)\nprint(clf_tree.best_score_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XGBClassifier","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import xgboost\nfrom sklearn.model_selection import GridSearchCV\n\nvec = TfidfVectorizer()\nX= vec.fit_transform(df_train['text'])\ny = df_train['target']\nxgboost_params = {'n_estimators' :[25,50,100, None],\n                   'learning_rate': [0.0001, 0.001, 0.01, 0.1],\n                  'gamma':[0.5, 0.1, 1, 10],\n                  'max_depth':[5, 10, 15, None]}\n\nxgb = xgboost.XGBClassifier(random_state=42, n_jobs=4)\nclf_xgb = GridSearchCV(xgb, xgboost_params, cv=5,n_jobs= 4, verbose = 1)\nclf_xgb.fit(X, y)\nprint(clf_xgb.best_estimator_)\nprint(clf_xgb.best_score_)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LGBMClassifier","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import lightgbm as lgb\nlightgbm_params ={'learning_rate':[0.0001, 0.001, 0.003, 0.01, 0.1],\n                  'n_estimators':[10,20, 50, 100, None],\n                 'max_depth':[4, 6, 10, 15, 20, 50, None]}\ngbm = lgb.LGBMClassifier(random_state = 42, n_jobs=4)\nclf_gbm = GridSearchCV(gbm, lightgbm_params, cv=5,n_jobs= 4, verbose = 1)\nclf_gbm.fit(X, y)\nprint(clf_gbm.best_estimator_)\nprint(clf_gbm.best_score_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LinearSVC","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from sklearn.svm import LinearSVC\n\nsvr_params = {'C':[0.0001, 0.001,0.01, 0.1, 1 , 10, 100]}\nsvr = LinearSVC(random_state=42)\nclf_svr = GridSearchCV(svr, svr_params, cv=5, n_jobs=4, verbose=1)\nclf_svr.fit(X, y)\nprint(clf_svr.best_estimator_)\nprint(clf_svr.best_score_)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## RandomForestClassifier","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nlightran_params ={'n_estimators':[10,20, 50, 100, None],\n                 'max_depth':[4, 6, 10, 15, 20, 50, None]}\nrandom = RandomForestClassifier(random_state = 42, n_jobs=42)\nclf_random = GridSearchCV(random, lightran_params, cv=5,n_jobs= 4, verbose = 1)\nclf_random.fit(X, y)\nprint(clf_random.best_estimator_)\nprint(clf_random.best_score_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ExtraTreesClassifier","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesClassifier\nlightree_params ={'n_estimators':[10,20, 50, 100, None],\n                  'max_depth':[4, 6, 10, 15, 20, 50, None]}\n\ntree = ExtraTreesClassifier(random_state=42, n_jobs=42)\nclf_tree = GridSearchCV(tree, lightree_params, cv=5, n_jobs= 4, verbose = 1)\nclf_tree.fit(X, y)\nprint(clf_tree.best_estimator_)\nprint(clf_tree.best_score_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LogisticRegression","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlf_params ={'C':[0.0001, 0.001,0.01, 0.1, 1 , 10, 100],\n           'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\n\nlf = LogisticRegression(random_state=42)\nclf_lf = GridSearchCV(lf, lf_params, cv=5,n_jobs= 4, verbose = 1)\nclf_lf.fit(X, y)\n\nprint(clf_lf.best_estimator_)\nprint(clf_lf.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from sklearn.ensemble import StackingClassifier\nestimators = [\n    ('svc', LinearSVC(C=0.1, random_state=42)),\n    ('extra', ExtraTreesClassifier(random_state=42, n_jobs=4)),\n    ('random',RandomForestClassifier(random_state=42, n_jobs=4)),\n    ('lgb', lgb.LGBMClassifier(max_depth=50, n_estimators=50, random_state=42)),\n    ('xgb', xgboost.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=1, gamma=10, gpu_id=-1,\n              importance_type='gain', interaction_constraints='',\n              learning_rate=0.1, max_delta_step=0, max_depth=5,\n              min_child_weight=1,\n              n_estimators=50, n_jobs=0, num_parallel_tree=1, random_state=42,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n              tree_method='exact', validate_parameters=1, verbosity=None))\n]\nclf_stack = StackingClassifier(\n    estimators=estimators, final_estimator=LogisticRegression(C=1, random_state=42, solver='newton-cg')\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from sklearn.ensemble import StackingClassifier\nestimators = [\n    ('lr',LogisticRegression(C=1, random_state=42, solver='newton-cg')),\n    ('extra', ExtraTreesClassifier(random_state=42, n_jobs=4)),\n    ('random',RandomForestClassifier(random_state=42, n_jobs=4)),\n    ('lgb', lgb.LGBMClassifier(max_depth=50, n_estimators=50, random_state=42, n_jobs=4)),\n]\nclf_stack1 = StackingClassifier(\n    estimators=estimators, final_estimator=LinearSVC(C=0.1, random_state=42)\n)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from sklearn.ensemble import StackingClassifier\nestimators = [\n    ('lr',LogisticRegression(C=1, random_state=42, solver='newton-cg')),\n    ('extra', ExtraTreesClassifier(random_state=42, n_jobs=4)),\n    ('random',RandomForestClassifier(random_state=42, n_jobs=4)),\n    ('lgb', lgb.LGBMClassifier(max_depth=50, n_estimators=50, random_state=42, n_jobs=4)),\n    ('xgb', xgboost.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=1, gamma=10, gpu_id=-1,\n              importance_type='gain', interaction_constraints='',\n              learning_rate=0.1, max_delta_step=0, max_depth=5,\n              min_child_weight=1,\n              n_estimators=50, n_jobs=4, num_parallel_tree=1, random_state=42,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n              tree_method='exact', validate_parameters=1, verbosity=None))\n]\nclf_stack2 = StackingClassifier(\n    estimators=estimators, final_estimator=LinearSVC(C=0.1, random_state=42)\n)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from sklearn.ensemble import StackingClassifier\nestimators = [\n    ('lr',LogisticRegression(C=1, random_state=42, solver='newton-cg')),\n    ('extra', ExtraTreesClassifier(random_state=42, n_jobs=4)),\n    ('lgb', lgb.LGBMClassifier(max_depth=50, n_estimators=50, random_state=42, n_jobs=4)),\n    ('xgb', xgboost.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=1, gamma=10, gpu_id=-1,\n              importance_type='gain', interaction_constraints='',\n              learning_rate=0.1, max_delta_step=0, max_depth=5,\n              min_child_weight=1,\n              n_estimators=50, n_jobs=4, num_parallel_tree=1, random_state=42,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n              tree_method='exact', validate_parameters=1, verbosity=None)),\n    ('svc',LinearSVC(C=0.1, random_state=42))\n]\nclf_stack3 = StackingClassifier(\n    estimators=estimators, final_estimator=RandomForestClassifier(random_state=42, n_jobs=4)\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from tqdm import tqdm\nscores = {}\n\nlg = LogisticRegression(C=1, random_state=42, solver='newton-cg')\nlist_ = []\nfor a in tqdm(range(2,9, 2)):\n    score = cross_val_score(lg, X, df_train['target'], cv=a)\n    list_.append(score.mean())\nscores['Logistic Regression'] = list_\n\nlinear = LinearSVC(C=0.1, random_state=42)\nlist_ = []\nfor a in tqdm(range(2,9, 2)):\n    score = cross_val_score(linear, X, df_train['target'], cv=a)\n    list_.append(score.mean())\n\nscores['Linear SVC'] = list_\n\nrandom = RandomForestClassifier(random_state=42, n_jobs=4)\nlist_ = []\nfor a in tqdm(range(2,9, 2)):\n    score = cross_val_score(random, X, df_train['target'], cv=a)\n    list_.append(score.mean())\n\nscores['Random Forest'] = list_\n\nextr = ExtraTreesClassifier(random_state=42,  n_jobs=4)\nlist_ = []\nfor a in tqdm(range(2,9, 2)):\n    score = cross_val_score(extr, X, df_train['target'], cv=a)\n    list_.append(score.mean())\n\nscores['Extra Tree'] = list_\n    \nlgbm = lgb.LGBMClassifier(max_depth=50, n_estimators=50, random_state=42,  n_jobs=4)\nlist_ = []\nfor a in tqdm(range(2,9, 2)):\n    score = cross_val_score(lgbm, X, df_train['target'], cv=a)\n    list_.append(score.mean())\n\nscores['LGBM'] = list_","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"xgb = xgboost.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=1, gamma=10, gpu_id=-1,\n              importance_type='gain', interaction_constraints='',\n              learning_rate=0.1, max_delta_step=0, max_depth=5,\n              min_child_weight=1,\n              n_estimators=50, n_jobs=4, num_parallel_tree=1, random_state=42,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n              tree_method='exact', validate_parameters=1, verbosity=None)\nlist_ = []\nfor a in tqdm(range(2,9, 2)):\n    score = cross_val_score(xgb, X, df_train['target'], cv=a)\n    list_.append(score.mean())\n\nscores['XGBoost'] = list_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"list_ = []\nfor a in tqdm(range(2,9, 2)):\n    score = cross_val_score(clf_stack, X, df_train['target'], cv=a)\n    list_.append(score.mean())\n\nscores['Stack1'] = list_\n\nlist_ = []\nfor a in tqdm(range(2,9, 2)):\n    score = cross_val_score(clf_stack1, X, df_train['target'], cv=a)\n    list_.append(score.mean())\n\nscores['Stack2'] = list_\n    \nlist_ = []\nfor a in tqdm(range(2,9, 2)):\n    score = cross_val_score(clf_stack2, X, df_train['target'], cv=a)\n    list_.append(score.mean())\n\nscores['Stack3'] = list_\n\nlist_ = []\nfor a in tqdm(range(2,9, 2)):\n    score = cross_val_score(clf_stack3, X, df_train['target'], cv=a)\n    list_.append(score.mean())\n\nscores['Stack4'] = list_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4\"></a> <br>\n<font size=\"+3\" color=\"black\"><b>4 - Evaluate Models</b></font><br><a id=\"4\"></a>\n<br> ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* So know try to identify best model","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import plotly.graph_objects as go\ndef plot_scores(scores):\n    fig = go.Figure()\n    for key, values in zip(scores.keys(), scores.values()):\n        fig.add_trace(go.Scatter(y=values, x=[2,4,6,8],\n                        mode='lines',\n                        name='scores '+str(key)))\n    fig.update_layout({\n        'plot_bgcolor': 'rgba(0, 0, 0, 0)',\n        'paper_bgcolor': 'rgba(0, 0, 0, 0)',\n        'title': 'Results CV',\n    })\n    fig.show()\nplot_scores(scores)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport plotly.express as px\n\nstds = []\nfor value in scores.values():\n    std = np.std(np.array(value), axis=0)\n    stds.append(std)\ndf_to_scatter = pd.DataFrame([])\ndf_to_scatter['scores'] = stds\ndf_to_scatter['models'] = scores.keys()\n\nfig = px.scatter(df_to_scatter, x=\"models\", y=\"scores\", color=\"models\",\n                 size='scores')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* the choose model was stack1 because when CV is 2 is the model with best result, and between stack has low stds in results","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = vec.transform(df_test['text'])\nsub = pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')\nmodel = clf_stack1.fit(X, df_train['target'])\npredict = clf_stack1.predict(X_test)\nsub['target'] = predict\nsub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}