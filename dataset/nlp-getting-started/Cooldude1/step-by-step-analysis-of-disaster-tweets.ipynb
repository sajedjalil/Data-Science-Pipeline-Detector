{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# About the Notebook\n\n<h2> This is a step by step guide on Natural Language Processing with Disaster Tweets. </h2>\n\n<h3>The aim of this challenge is to build a text classifier to predict disaster and non-disastrous tweets in text.</h3>\n\n<h3>This notebook is divided into following 4 sections:</h3>\n\n* **Exploring the Data** : In this section, we will explore the data using various visualization plots to gain an insight on our data.\n* **Removing the Garbage**: In this section, we will clean the data and remove the noise in the data.\n* **Getting Ready**: In this section, we will transform the data in a suitable format for prediction.\n* **The Final Stage**: In this section, we will implement various ML/DL models to classify textual data.","metadata":{}},{"cell_type":"code","source":"# Importing Libraries\n\n# Used for Data Manipulation and Analysis\nimport pandas as pd\nimport numpy as np\n\n# Used for Data Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-bright')\n\n# Used for NLP\nimport nltk\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom wordcloud import WordCloud\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Used for ML/Deep Learning Algorithms\nfrom tensorflow.keras.layers import Embedding,Dropout\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom tensorflow.keras.layers import LSTM,Bidirectional,GRU,MaxPooling1D,Conv1D\nfrom tensorflow.keras.layers import Dense\nfrom sklearn.metrics import classification_report,accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\n\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Chapter 1: Exploring the Data","metadata":{}},{"cell_type":"markdown","source":"<h4>Below cell loads the training and testing data into variables train and test resp. using pandas.</h4>","metadata":{}},{"cell_type":"code","source":"# Reading data from csv\ntrain = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\ntest  = pd.read_csv(\"../input/nlp-getting-started/test.csv\")\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4> Let's explore the columns in the dataset </h4>\n\n*  id      : A unique identifier for each tweet.   \n*  keyword : A particular keyword from the tweet (may be blank).\n*  location: The location the tweet was sent from (may be blank).\n*  text    : The text of the tweet.\n*  target  : This denotes whether a tweet is about a real disaster (1) or not (0). ","metadata":{}},{"cell_type":"code","source":"# Displaying rows and columns in dataset\n\nprint(\"There are {} rows and {} columns in training data\".format(train.shape[0],train.shape[1]))\nprint(\"There are {} rows and {} columns in training data\".format(test.shape[0],test.shape[1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4> Let's explore the Target Variable </h4>","metadata":{}},{"cell_type":"code","source":"# Visualizing the target classes\nplt.figure(figsize=(8,5))\nplt.title(\"Count of Target Classes\")\nsns.countplot(y=train[\"target\"],linewidth=2,\n                   edgecolor='black')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4>Inference:-</h4>\n\n*From the Countplot, it can be observed that the total number of samples in Target Class 1 is around 3200 while in Target Class 0, it is about 4500. Also, the classes seems to be in balanced state.*","metadata":{}},{"cell_type":"markdown","source":"<h4>Let's start by analysing total number of characters in text.</h4>","metadata":{}},{"cell_type":"code","source":"fig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,5))\nchar_len_dis = train[train['target']==1]['text'].str.len()\nax1.hist(char_len_dis,color='red',edgecolor='black', linewidth=1.2)\nax1.set_title('Disaster Tweets')\nchar_len_ndis = train[train['target']==0]['text'].str.len()\nax2.hist(char_len_ndis,color='blue',edgecolor='black', linewidth=1.2)\nax2.set_title('Non-Disaster Tweets')\nplt.suptitle(\"Length of Characters in text\")\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4>Inference:-</h4>\n\n*From the above histograms, it can be observed that the characters count for disaster and non-disaster tweets are in the range of (120,140).*","metadata":{}},{"cell_type":"code","source":"# Analysing number of words in text.\n\nfig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,5))\nchar_len_dis = train[train['target']==1]['text'].str.split().map(lambda x: len(x))\nax1.hist(char_len_dis,color='red',edgecolor='black', linewidth=1.2)\nax1.set_title('Disaster Tweets')\nchar_len_ndis = train[train['target']==0]['text'].str.split().map(lambda x: len(x))\nax2.hist(char_len_ndis,color='blue',edgecolor='black', linewidth=1.2)\nax2.set_title('Non-Disaster Tweets')\nplt.suptitle(\"Length of words in text\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4>Inference:-</h4>\n\n\n*From the above histograms, it can be observed that the words count for disaster and non-disaster tweets are in the range of (15-20).*","metadata":{}},{"cell_type":"code","source":"# Analysing average word length in text.\n\nfig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,5))\nchar_len_dis = train[train['target']==1]['text'].str.split().apply(lambda x: [len(i) for i in x])\nsns.distplot(char_len_dis.map(lambda x: np.mean(x)),ax=ax1,color='darkblue')\nax1.set_title('Disaster Tweets')\nchar_len_ndis = train[train['target']==0]['text'].str.split().apply(lambda x: [len(i) for i in x])\nsns.distplot(char_len_ndis.map(lambda x: np.mean(x)),ax=ax2,color='green')\nax2.set_title('Non-Disaster Tweets')\nplt.suptitle(\"Average Word Length in text\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4>Inference:-</h4>\n\n\n*From the above distributions, it can be observed that the average word count for disaster tweets are found to be in the range(7-7.5) while for non-disaster tweets are in the range of (4.5-5).*","metadata":{}},{"cell_type":"markdown","source":"<h3>Let's explore the data further in depth:-</h3>\n\n<h4>Below sections perform following analysis:</h4>\n\n* Stop Words Analysis\n* Punctuations Analysis\n* Analysis of Missing words.","metadata":{}},{"cell_type":"code","source":"# Creating sample corpus for further analysis.\ndef create_corpus(target):\n    corpus = []\n    for x in train[train['target']==target]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Analysing top stop words in text.\nfrom collections import defaultdict\n\ndef analyze_stopwords(data,func,target):\n    values_list = []\n    for labels in range(0,len(target)):\n        dic = defaultdict(int)\n        corpus = func(target[labels])\n        for word in corpus:\n            dic[word]+=1\n        top = sorted(dic.items(),key = lambda x: x[1],reverse=True)[:10]\n        x_items,y_values = zip(*top)\n        values_list.append(x_items)\n        values_list.append(y_values)\n    \n    fig,(ax1,ax2) = plt.subplots(1,2,figsize=(15,5))\n    ax1.barh(values_list[0],values_list[1],color=\"lightblue\",edgecolor='black', linewidth=1.2)\n    ax1.set_title(\"Non-Disaster Tweets\")\n    \n    ax2.barh(values_list[2],values_list[3],color=\"lightgreen\",edgecolor='black', linewidth=1.2)\n    ax2.set_title(\"Disaster Tweets\")\n            \n    plt.suptitle(\"Top Stop words in text\")\n    plt.show()\n\nanalyze_stopwords(train,create_corpus,[0,1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4>Inference:-</h4>\n\n*The above Bar Charts displays the top 10 stop words in tweets. From the bar chart, it is observed that <strong>the most frequently</strong> occuring stopword in both disaster/non-disaster tweets is <strong>\"the\"(1000+)</strong> while <strong>the least occuring </strong>for non-disaster is <strong>\"for\"(400)</strong> and for disaster tweets is <strong>\"is\"(300)</strong>.*","metadata":{}},{"cell_type":"code","source":"# Anaysing Punctuations\nimport string\n\ndef analyze_punctuation(data,func,target):\n    values_list = []\n    special = string.punctuation\n    for labels in range(0,len(target)):\n        dic = defaultdict(int)\n        corpus = func(target[labels])\n        for i in corpus:\n            if i in special:\n                dic[i]+=1\n        x_items,y_values = zip(*dic.items())\n        values_list.append(x_items)\n        values_list.append(y_values)\n    \n    fig,(ax1,ax2) = plt.subplots(1,2,figsize=(15,5))\n    ax1.bar(values_list[0],values_list[1],color=\"lightblue\",edgecolor='black', linewidth=1.2)\n    ax1.set_title(\"Non-Disaster Tweets\")\n    \n    ax2.bar(values_list[2],values_list[3],color=\"lightgreen\",edgecolor='black', linewidth=1.2)\n    ax2.set_title(\"Disaster Tweets\")\n            \n    plt.suptitle(\"Punctuations in text\")\n    plt.show()\n\nanalyze_punctuation(train,create_corpus,[0,1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4>Inference:-</h4>\n\n*The above Bar Charts displays the top 10 punctuations in tweets. From the bar chart, it is observed that <strong>the most </strong> occuring punctuation in both disaster/non-disaster tweets is <strong>\"-\"(350+)</strong> while <strong>the least occuring </strong>for non-disaster are <strong>\"%\",\"/:\",\"$\",\"_\"</strong> and for disaster tweets is <strong>\"=>\", \")\"</strong>.*","metadata":{}},{"cell_type":"code","source":"# Checking Null values\nmissing_train = train.isnull().sum()  \nmissing_test = test.isnull().sum()  \nfig, (ax1,ax2) = plt.subplots(1,2,figsize=(15,5))\nmissing_train = missing_train[missing_train>0].sort_values()\nax1.pie(missing_train,autopct='%1.1f%%',startangle=30,explode=[0.9,0],labels=[\"keyword\",\"location\"],colors=['yellow','cyan'])\nax1.set_title(\"Null values present in Train Dataset\")\n\nmissing_test = missing_test[missing_test>0].sort_values()\nax2.pie(missing_test,autopct='%1.1f%%',startangle=30,explode=[0.9,0],labels=[\"keyword\",\"location\"],colors=['yellow','#66ff00'])\nax2.set_title(\"Null values present in Test Dataset\")\nplt.suptitle(\"Distribution of Null Values in Dataset\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4>Inference:-</h4>\n\n*The above pictorial representation displays the missing values in dataset. From the distribution, it is observed that columns <strong>Keyword and Location</strong> contains  missing values. For training data, the % of missing values  are <strong>97.6 for \"location\" and 24 for \"keywords\"</strong> while for testing data, it is  <strong>97.7% for \"location\" and 23% for keywords</strong>. Also, the column having <strong>maximum missing values is: location</strong> while <strong>Keywords column has the minimum</strong> count of missing values for both sets of data.*","metadata":{}},{"cell_type":"markdown","source":"<h4>Let's  analyze the keywords column:-</h4>\n\n","metadata":{}},{"cell_type":"code","source":"# Analysing Top 20  disastrous KeyWords in text .\nplt.figure(figsize=(10,7))\ntrain[train['target']==1]['keyword'].value_counts()[:20].plot(kind='barh', fontsize=12,title='Top 20 Disastrous Keywords in Text', color='#0096FF',edgecolor='black', linewidth=1.2)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4>Inference:-</h4>\n\n*The above Bar Chart represents the top 20 disastrous keywords in text. From the bar chart, it is observed that <strong>the most </strong> occuring keywords are <strong>derailment,wreckage,outbreak (35+)</strong> while <strong>the least occuring </strong>are <strong>sandstorm and evacuation (28)</strong>.*","metadata":{}},{"cell_type":"markdown","source":"<h4>Let's analyze Locations column:-</h4>","metadata":{}},{"cell_type":"code","source":"# Analysing Top 20 disastrous Locations in text.\nplt.figure(figsize=(10,7))\ntrain[train[\"target\"]==1][\"location\"].value_counts()[:20].plot(kind='barh',fontsize=12, title='Top 20 Disastrous Locations in Text', color='#66ff00',edgecolor='black', linewidth=1.2)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4>Inference:-</h4>\n\n*The above Bar Chart displays the top 20 Locations in tweets. From the chart, it is observed that <strong>the most </strong> occuring/referred location is <strong>USA/United States (68)</strong> while <strong>the least occuring </strong> are <strong>Los Angeles and California (5)</strong>.*","metadata":{}},{"cell_type":"markdown","source":"# Chapter 2: Removing the Garbage\n\n<h4> Let's Clean the Data. Following operations are carried out on text column for performing Data Cleaning </h4>\n\n* Removal of URL's.\n* Removal of HTMl tags.\n* Removal of Emoji's.\n* Filtering out miscellaneous text.\n* Lowering the text.\n* Performing Stemming (in case of bag of words(bow) and tf-idf) and lemmatization for (LSTM).\n* Discarding words of length < 2.\n\n\nNote: The Stemming process is performed for bow and tf-idf because there is no need of meaningful words while lemmatization is performed for LSTM because we do require meaningful words (discussed in subsequent sections).\n","metadata":{}},{"cell_type":"code","source":"# Seperating independent and dependent features\nX = train.drop(columns=[\"target\"],axis=1)\ny = train[\"target\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Perfoming data cleaning\n\nmessages_train = X.copy()\nmessages_test  = test.copy()\n\nps = PorterStemmer()\nwl = WordNetLemmatizer()\ndef preprocess_data(data):\n    '''\n    Input: Data to be cleaned.\n    Output: Cleaned Data.\n    \n    '''\n    review =re.sub(r'https?://\\S+|www\\.\\S+|http?://\\S+',' ',data) #removal of url\n    review =re.sub(r'<.*?>',' ',review) #removal of html tags\n    review = re.sub(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # removal of emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\",' ',review)\n    review = re.sub('[^a-zA-Z]',' ',review) # filtering out miscellaneous text.\n    review = review.lower() # Lowering all the words in text\n    review = review.split()\n    review = [ps.stem(words) for words in review if words not in stopwords.words('english')] #Stemming\n    review = [i for i in review if len(i)>2] # Removal of words with length<2\n    review = ' '.join(review)\n    return review\n\ntrain[\"Cleaned_text\"] = train[\"text\"].apply(preprocess_data)\ntest[\"Cleaned_text\"] = test[\"text\"].apply(preprocess_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4> Let's take a look at Cleaned Data </h4>","metadata":{}},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4> Let's analyze common words after cleaning of text using Word Cloud</h4>","metadata":{}},{"cell_type":"code","source":"# Analysing common words using WordCloud \n\nwc = WordCloud(background_color='black')\nwc.generate(' '.join(train['Cleaned_text']))\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4>Inference:-</h4>\n\n*The above displays top common words post the cleaning of text in training data. The most occuring words are: <strong>amp, new, one, people, time etc. </strong>.*","metadata":{}},{"cell_type":"markdown","source":"<h4>Let's further analyse top 50 words of disaster/non-disastrous in training data:-</h4>\n","metadata":{}},{"cell_type":"code","source":"# Analysing top 50 words in training data\n\ndisaster_tweet = train[train.target==1][\"Cleaned_text\"]\nnon_disaster_tweet = train[train.target==0][\"Cleaned_text\"]\n\ncolor = ['Paired','Accent']\nsplitedData = [disaster_tweet,non_disaster_tweet]\ntitle = [\"Disaster Tweets\", \"Non-Disaster Tweets\"]\nfor item in range(2):\n    plt.figure(figsize=(20,8))\n    plt.title(title[item],fontsize=12)\n    pd.Series(' '.join([i for i in splitedData[item]]).split()).value_counts().head(50).plot(kind='bar',fontsize=10,colormap=color[item],edgecolor='black', linewidth=1.2)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4>Inference:-</h4>\n\n*The above Bar Charts displays the top 50 Words post cleaning in text. From the chart, it is observed that <strong>the most </strong> occuring word for disaster tweets is: <strong> fire (250+)</strong> and for non-disaster is :<strong> like (250+) </strong>while <strong>the least occuring </strong> are <strong> collapse, atomic (80)</strong> for disaster and <strong> feel, really etc. (80) </strong> for non-disastrous.*","metadata":{}},{"cell_type":"markdown","source":"<strong> From above chart, it appears that our cleaned text still contains some unnecessary words (such as: like, amp, get, would etc.) that aren't relevant and can confuse our model, resulting in false prediction. Now, we will further remove some confusing words from text based on above charts.</strong>","metadata":{}},{"cell_type":"code","source":"common_words = ['via','like','build','get','would','one','two','feel','lol','fuck','take','way','may','first','latest'\n                'want','make','back','see','know','let','look','come','got','still','say','think','great','pleas','amp']\n\ndef text_cleaning(data):\n    return ' '.join(i for i in data.split() if i not in common_words)\n\ntrain[\"Cleaned_text\"] = train[\"Cleaned_text\"].apply(text_cleaning)\ntest[\"Cleaned_text\"] = test[\"Cleaned_text\"].apply(text_cleaning)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<strong> Let's check one example whether there are any change occured or not</strong>","metadata":{}},{"cell_type":"code","source":"train.head(1) # Much more cleaner","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Chapter 3: Getting Ready!!!! \n\n<h4> Let's transform the data in numerical format that is suitable for prediction by models. \n     We will use following techniques to transform the data.</h4>\n\n* BOW.\n* TF-IDF.\n* Word Embbedding using Glove Vectors.","metadata":{}},{"cell_type":"markdown","source":"<h4> Let's first start by analysing top N-grams using Bag Of Words.</h4>\n","metadata":{}},{"cell_type":"code","source":"# Creating function for analysing top n grams\n\ndef top_ngrams(data,n,grams):\n    '''\n    Input:- Data: Input Data\n            n   : Number of top n-words\n            grams:Type of N-grams. 1-> Unigram  2-> Bigram  3->Trigram\n            \n    Output: Word Frequency of top  n words\n    \n    '''\n    if grams == 1:\n        count_vec = CountVectorizer(ngram_range=(1,1)).fit(data)\n        bow = count_vec.transform(data)\n        add_words = bow.sum(axis=0)\n        word_freq = [(word, add_words[0, idx]) for word, idx in count_vec.vocabulary_.items()]\n        word_freq = sorted(word_freq, key = lambda x: x[1], reverse=True)\n    elif grams == 2:\n        count_vec = CountVectorizer(ngram_range=(2,2)).fit(data)\n        bow = count_vec.transform(data)\n        add_words = bow.sum(axis=0)\n        word_freq = [(word,add_words[0,idx]) for word,idx in count_vec.vocabulary_.items()]\n        word_freq = sorted(word_freq, key = lambda x: x[1], reverse=True)\n    elif grams == 3:\n        count_vec = CountVectorizer(ngram_range=(3,3)).fit(data)\n        bow = count_vec.transform(data)\n        add_words = bow.sum(axis=0)\n        word_freq = [(word,add_words[0,idx]) for word,idx in count_vec.vocabulary_.items()]\n        word_freq = sorted(word_freq, key = lambda x: x[1], reverse=True)\n\n    return word_freq[:n]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4> Plotting top 20 N-grams </h4>\n","metadata":{}},{"cell_type":"code","source":"common_words_uni = top_ngrams(train[\"Cleaned_text\"],20,1)\ncommon_words_bi = top_ngrams(train[\"Cleaned_text\"],20,2)\ncommon_words_tri = top_ngrams(train[\"Cleaned_text\"],20,3)\ncommon_words_uni_df = pd.DataFrame(common_words_uni,columns=['word','freq'])\ncommon_words_bi_df = pd.DataFrame(common_words_bi,columns=['word','freq'])\ncommon_words_tri_df = pd.DataFrame(common_words_tri,columns=['word','freq'])\nfig,(ax1,ax2,ax3) = plt.subplots(3,1,figsize=(15,20))\nax1.bar(common_words_uni_df[\"word\"],common_words_uni_df[\"freq\"],color=\"#FFCCCB\",edgecolor='black', linewidth=1.2)\nax1.set_title(\"Top 20 Unigrams in Text.\")\nax1.set_xlabel(\"Words\")\nax1.set_ylabel(\"Frequency\")\nax1.set_xticklabels(rotation=90,labels=common_words_uni_df[\"word\"],fontsize=10)    \n\nax2.bar(common_words_bi_df[\"word\"],common_words_bi_df[\"freq\"],color=\"lightblue\",edgecolor='black', linewidth=1.2)\nax2.set_title(\"Top 20 Bigrams in Text.\")\nax2.set_xlabel(\"Words\")\nax2.set_ylabel(\"Frequency\")\nax2.set_xticklabels(rotation=90,labels=common_words_bi_df[\"word\"],fontsize=10)    \n\nax3.bar(common_words_tri_df[\"word\"],common_words_tri_df[\"freq\"] ,color=\"lightgreen\",edgecolor='black', linewidth=1.2)\nax3.set_title(\"Top 20 Trigrams in Text.\")\nax3.set_xlabel(\"Words\")\nax3.set_ylabel(\"Frequency\")\nax3.set_xticklabels(rotation=90,labels=common_words_tri_df[\"word\"],fontsize=10) \nplt.suptitle(\"Visualization of Top 20 Unigrams, Bigrams and Trigrams\",fontsize=\"15\")\nplt.tight_layout(pad=1.85)\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4>Inference:-</h4>\n\n*The above Bar Charts displays the top 20 N-Grams. From the chart, it is observed that <strong>the most </strong> occuring unigram in text is: <strong> fire (350+)</strong>, for bigram, it is :<strong> suicide bomber (60) </strong> and for trigram, it is <strong> liked youtube video </strong> while <strong> the least </strong> is <strong> burning (100) </strong> for unigram, it is <strong> home-razed (29)</strong> for bigram and it is <strong>wreckage conclusively confirmed (24)</strong> for trigrams.*","metadata":{}},{"cell_type":"markdown","source":"**Let's perform data encoding using bow and tf-idf.** \n\nThe function accepts cleaned training and testing data, boolean flags for selecting type of encoding as input and outputs cleaned text.","metadata":{}},{"cell_type":"code","source":"# Creating functions for using BOW,TF-IDF \n\ndef encoding(train_data,test_data,bow,tf_idf):\n    '''\n    Input : Data to be encoded and choice of encoding.\n    Output : Desired Encoding.\n    \n    '''\n    if bow==True: \n        cv = CountVectorizer(ngram_range=(1, 1))\n        cv_df_train = cv.fit_transform(train_data).toarray()\n        train_df = pd.DataFrame(cv_df_train,columns=cv.get_feature_names())\n        cv_df_test = cv.transform(test_data).toarray()\n        test_df = pd.DataFrame(cv_df_test,columns=cv.get_feature_names())\n        \n    elif tf_idf==True:\n        \n        tfidf = TfidfVectorizer(\n            ngram_range=(1, 1), use_idf=1,smooth_idf=1,sublinear_tf=1)    \n        tf_df_train = tfidf.fit_transform(train_data).toarray()\n        train_df = pd.DataFrame(tf_df_train,columns=tfidf.get_feature_names())\n        tf_df_test = tfidf.transform(test_data).toarray()\n        test_df = pd.DataFrame(tf_df_test,columns=tfidf.get_feature_names())\n        \n    return train_df,test_df\n\n\nx_final,x_test_final = encoding(train[\"Cleaned_text\"],test[\"Cleaned_text\"],bow=True,tf_idf=False)\ny_final = np.array(y) # Converting y to array","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**In the above code, we have selected n-grams to be (1,1) as starting point, but feel free to experiment with other options.**","metadata":{}},{"cell_type":"code","source":"# Checking dimensions of training and testing data\nx_final.shape,y_final.shape,x_test_final.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4>Using Word Embedding using Glove Vectors for Encoding:-</h4>","metadata":{}},{"cell_type":"code","source":"#Converting to list\ntext = train[\"Cleaned_text\"].tolist()\ntext_test = test[\"Cleaned_text\"].tolist()\ntext[:3] # Analysing first 3 sentence in train data.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's initialize a Tokenizer to read all the words in the text.**","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\ntoken = Tokenizer()\ntoken.fit_on_texts(text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Finding the vocab size\nvocab_size = len(token.word_index)+1\nprint(\"The vocabulary size is : {}\".format(vocab_size))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Positions of tokens arranged by tokenizer in vovabulary.\n\n#print(token.index_word)  #==> O/p: 1:'like',2:'fire' etc.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Encoding tokens in words to numerical formats\nencoded_text = token.texts_to_sequences(text)\nencoded_text_test = token.texts_to_sequences(text_test)\nencoded_text[:2]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Considering 120 words\nmax_length = 120 # Considering top 120 tokens.\nX = pad_sequences(encoded_text,maxlen=max_length,padding='post') # This is done to make the sequence of same length.\nX_test = pad_sequences(encoded_text_test,maxlen=max_length,padding='post')\nX","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using Glove Vector representations:\n# you -0.11076 0.30786 -0.5198 0.035138 0.10368 -0.052505...... -0.35471 0.2331 -0.0067546 -0.18892 0.27837 -0.38501 -0.11408 0.28191 -0.30946 -0.21878 -0.059105 0.47604 0.05661\n\n# The first word is key and rest is their vector reprr.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n#declaring dict to store all the words as keys in the dictionary and their vector representations as values\nglove_vectors = dict()\n\n# Now, we will convert the words in glove vectors into key value pairs. We have used glove representation of 200D. \n\n\nfile = open('../input/glove6b200d/glove.6B.200d.txt',encoding='utf-8')\n\nfor line in file:\n    values = line.split()  # contains list of keys and their vectors\n    word = values[0] # contains words\n    vectors = np.asarray(values[1:]) # storing vectors\n    glove_vectors[word] = vectors #storing the vector representation of the respective word in the dictionary\nfile.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking length of glove vectors\nprint(\"The maximum size of global vectors is : {}\".format(len(glove_vectors)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking dimensions of Glove Vectors.\nglove_vectors.get('you').shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now we are creating a matrix for the tokens which we are having in our dataset and then storing their vector representation values in the matrix if it matches with glove_vectors words else print the misspelled words or words which are not present.\n\nword_vector_matrix = np.zeros((vocab_size,200))  # size of the word matrix\nfor word,index in token.word_index.items():\n    vector = glove_vectors.get(word)\n    if vector is not None:\n        word_vector_matrix[index] = vector\n    #else:\n        #print(word)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"The Size of Word Matrix is :{}\".format(word_vector_matrix.shape))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Chapter 4: The Final Stage \n\n<h4> Let's create some Models !!! </h4>\n","metadata":{}},{"cell_type":"code","source":"# Dividing the data into training, validation and testing\nfrom sklearn.model_selection import train_test_split\n# for bow and tf-idf\n#x_train, x_test, y_train, y_test = train_test_split(x_final, y_final, test_size=0.1, random_state=42, stratify = y_final)\n#X_train, x_valid, Y_train, y_valid = train_test_split(x_train, y_train, test_size=0.1, random_state=42, stratify = y_train)\n#x_test_final = x_test_final\n\n#  for Word Embeddings\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, stratify = y)\nX_train, x_valid, Y_train, y_valid = train_test_split(x_train, y_train, test_size=0.1, random_state=42, stratify = y_train)\n#x_test_final = x_test_final","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**1. Logistic Regression**","metadata":{}},{"cell_type":"code","source":"model_1 = LogisticRegression(C=1.0)\nmodel_1.fit(X_train,Y_train)\npred_1 = model_1.predict(x_test)\ncr1    = classification_report(y_test,pred_1)\nprint(cr1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**2. Naive Bayes**","metadata":{}},{"cell_type":"code","source":"model_2 = MultinomialNB(alpha=0.7)\nmodel_2.fit(X_train,Y_train)\npred_2 = model_2.predict(x_test)\ncr2    = classification_report(y_test,pred_2)\nprint(cr2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**3. Decision Trees**","metadata":{}},{"cell_type":"code","source":"model_3 = DecisionTreeClassifier()\nmodel_3.fit(X_train,Y_train)\npred_3 = model_3.predict(x_test)\ncr3    = classification_report(y_test,pred_3)\nprint(cr3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**4. Random Forest (Untuned)**","metadata":{}},{"cell_type":"code","source":"model_4 = RandomForestClassifier()\nmodel_4.fit(X_train,Y_train)\npred_4 = model_4.predict(x_test)\ncr4    = classification_report(y_test,pred_4)\nprint(cr4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**5. XGBOOST (Untuned)**","metadata":{}},{"cell_type":"code","source":"model_5 = XGBClassifier()\nmodel_5.fit(X_train,Y_train)\npred_5 = model_5.predict(x_test)\ncr5    = classification_report(y_test,pred_5)\nprint(cr5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**6. CatBoost (Untuned)**","metadata":{}},{"cell_type":"code","source":"model_6 = CatBoostClassifier(iterations=100)\nmodel_6.fit(X_train,Y_train)\npred_6 = model_6.predict(x_test)\ncr6    = classification_report(y_test,pred_6)\nprint(cr6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**7. Passive Agressive Classifier**","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import PassiveAggressiveClassifier\nmodel_7 = PassiveAggressiveClassifier()\nmodel_7.fit(X_train,Y_train)\npred_7 = model_7.predict(x_test)\ncr7    = classification_report(y_test,pred_7)\nprint(cr7)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**8. Voting Ensemble**","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\n\nestimators = []\nestimators.append(('LR', \n                  LogisticRegression(C=1.0)))\nestimators.append(('NB', MultinomialNB(alpha=0.7)))\nestimators.append(('XBG', XGBClassifier()))\n\n\nvc = VotingClassifier(estimators=estimators,voting='soft')\nvc.fit(X_train,Y_train)\npred_vc = vc.predict(x_test)\ncr_vc    = classification_report(y_test,pred_vc)\nprint(cr_vc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Finding the max. length of sentences \n\n'''\ncurr_length = 0\nprev_length = 0\n\ndef calc_len(data,curr_length,prev_length):\n    \n    \n    Input: Data for which length is to be calculated.\n    Output: Calculated length.\n    \n    \n    result = 0\n    for i in range(0,len(data)):\n        curr_length = len(data[i])\n        if curr_length > prev_length:\n            prev_length = curr_length\n\n        else:\n            i+=1\n\n        curr_length = 0\n        \n    result = prev_length\n    return result\n\ntrain_length = calc_len(one_hot_train,curr_length,prev_length)\ntest_length  = calc_len(one_hot_test,curr_length,prev_length)\nprint(\"The length of training data: {} and testing data {}.\".format(train_length,test_length))\n'''","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Correcting spellings\n#!pip install pyspellchecker\n'''\nfrom spellchecker import SpellChecker\nfrom tqdm import tqdm\nspell = SpellChecker()\ndef correct_spell(text):\n    corrected_text = []\n    mispelled_words = spell.unknown(text.split())\n    for word in tqdm(text.split()):\n        if word in mispelled_words:\n            corrected_text.append(spell.correction(text))\n        else:\n            corrected_text.append(word)\n    return \" \".join(corrected_text)\n\ntrain[\"Cleaned_text\"] = train[\"Cleaned_text\"].apply(lambda x: correct_spell(x))\n'''","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**9. Long Short Tem Memory (LSTM)**","metadata":{}},{"cell_type":"code","source":"from keras.optimizers import Adam,SGD\nfrom tensorflow.keras import regularizers\n\nembedding_feature_vector = 200 # Since we used glove vector embedding of dim 200.\nmodel = Sequential()\nmodel.add(Embedding(vocab_size,embedding_feature_vector,input_length=max_length,weights = [word_vector_matrix], trainable = False))\nmodel.add(Dropout(0.35))\nmodel.add(LSTM(200))\nmodel.add(Dropout(0.35))\nmodel.add(Dense(32,activation='relu'))\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy',optimizer=Adam(lr=1e-4),metrics=['accuracy'])\nprint(model.summary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import *\nn_epoch = 30\n\nearly_stop = EarlyStopping(monitor='val_loss', patience=5, verbose=1, \n                           mode='min', restore_best_weights=True)\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, \n                              verbose=1, mode='min')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training the model\nhistory = model.fit(X_train,Y_train,validation_data=(x_valid,y_valid),callbacks=[reduce_lr,early_stop],epochs=n_epoch,batch_size= 64)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = model.predict_classes(x_test)\ncr = classification_report(y_test,predictions)\nprint(cr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#weights_82 = model.save_weights('weights_82_81(82).h5')\n#model_82 = model.to_json()","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Verifying the results.\nfrom tensorflow.keras.models import model_from_json\nmodel = model_from_json(model_82)\nmodel.load_weights('./weights_82_81(82).h5')\nmodel.compile(loss='binary_crossentropy',optimizer=Adam(lr=1e-4),metrics=['accuracy'])\nprint(model.summary())\nmodel.evaluate(x_test, y_test, batch_size=64, verbose=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4>Now, Let's Compare the performance of various ML/DL Models</h4>","metadata":{}},{"cell_type":"code","source":"acc_1 = 0.78084\nacc_2 = 0.78740\nacc_3 = 0.76378\nacc_4 = 0.78609\nacc_5 = 0.80709\nacc_6 = 0.79659\nacc_7 = 0.75590\nacc_8 = 0.80052\nacc_9 = 0.82021\nresults = pd.DataFrame([[\"Logistic Regression\",acc_1],[\"Naive Bayes\",acc_2],[\"Decision Trees\",acc_3],\n                       [\"Random Forest\",acc_4],[\"XGBoost\",acc_5],[\"CatBoost\",acc_6],\n                       [\"Passive Aggressor\",acc_7],[\"Voting Ensemble(LR+NB+XGB)\",acc_8],\n                       [\"LSTM\",acc_9]],columns = [\"Models\",\"Accuracy Score\"]).sort_values(by='Accuracy Score',ascending=False)\nresults.style.background_gradient(cmap='Blues')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4> Conclusion </h4>\n\nFrom the above table, it appears that <strong>LSTM model </strong>performs the best by achieving an overall accuracy of about <strong>82.02% </strong> while <strong>Passive Aggresive Model </strong> performs less accurately by outputing an accuracy score of about <strong>75.59% </strong>. \n\n<h5> Since, the LSTM Model shows robust performance, therefore it is selected as the final prediction model. </h4>","metadata":{}},{"cell_type":"code","source":"# Making Predictions on test data\npredictions_test = pd.DataFrame(model.predict_classes(X_test))\ntest_id = pd.DataFrame(test[\"id\"])\nsubmission = pd.concat([test_id,predictions_test],axis=1)\nsubmission.columns = [\"id\",\"target\"]\nsubmission.to_csv(\"Submission.csv\",index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## If you like my work/notebook, please upvote it!!!!","metadata":{}}]}