{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Tensorflow Keras Tutorial - subwords and Bidirectional LSTM (Part 6)\n\n**What is Keras?** Keras is a wrapper that allows you to implement Deep Neural Network without getting into intrinsic details of the Network. It can use Tensorflow or Theano as backend. This tutorial series will cover Keras from beginner to intermediate level.\n\nYOU CAN CHECK OUT REST OF THE TUTORIALS OF THIS SERIES.\n\n[PART 1](https://www.kaggle.com/akashkr/tf-keras-tutorial-neural-network-part-1)<br>\n[PART 2](https://www.kaggle.com/akashkr/tf-keras-tutorial-cnn-part-2)<br>\n[PART 3](https://www.kaggle.com/akashkr/tf-keras-tutorial-binary-classification-part-3)<br>\n[PART 4](https://www.kaggle.com/akashkr/tf-keras-tutorial-pretrained-models-part-4)<br>\n\n<font color=red>IF YOU HAVEN'T GONE THROUGH THE PREVIOUS PART OF THIS TUTORIAL, IT'S RECOMMENDED FOR YOU TO GO THROUGH THAT FIRST.</font><br>\n[PART 5](https://www.kaggle.com/akashkr/tf-keras-tutorial-basics-of-nlp-part-5)\n\nIn the previous notebooks we worked on image data. Now we are going to see text data. The common places where NLP is applied is Document Classification, Sentiment Analysis, Chat-bots etc.\n\nIn this tutorial we are going to see,\n* Text Preprocessing\n* Modelling\n    - LSTM classic\n    - LSTM Bidirectional\n    - Convolutional\n    - GRU\n    - Glove","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom plotly.offline import init_notebook_mode, iplot, plot\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom sklearn.model_selection import train_test_split\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport tensorflow as tf\nfrom tqdm import tqdm\nimport pandas as pd\nimport numpy as np\nimport pickle\nimport re\n\ninit_notebook_mode(connected=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/nlp-getting-started/train.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Overview of Dataset\n\n### Data Format\n\nEach sample in the train and test set has the following information:\n\n* The text of a tweet\n* A keyword from that tweet (although this may be blank!)\n* The location the tweet was sent from (may also be blank)\n\n### Target\n\n**You are predicting whether a given tweet is about a real disaster or not**. If so, predict a 1. If not, predict a 0.\n\n### Columns\n\nid - a unique identifier for each tweet\ntext - the text of the tweet\nlocation - the location the tweet was sent from (may be blank)\nkeyword - a particular keyword from the tweet (may be blank)\ntarget - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)\n\n> NOTE: **We will be using just the text and target features of the data**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Shape of data: {train_df.shape}')\n# Find the number of missing values\nprint(train_df.info())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting the data into 2/3 as train and 1/3 as test\nX_train, X_test, y_train, y_test = train_test_split(train_df['text'], train_df['target'], test_size=0.33, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size = 10000\nembedding_dim = 16\nmax_length = 50\ntrunc_type='post'\noov_tok = \"<OOV>\"\n\n# Tokenization\ntokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(X_train)\n\nword_index = tokenizer.word_index\nsequences = tokenizer.texts_to_sequences(X_train)\ntesting_sequences = tokenizer.texts_to_sequences(X_test)\n\n# Padding\npadded = pad_sequences(sequences, maxlen=max_length, truncating=trunc_type)\ntesting_padded = pad_sequences(testing_sequences, maxlen=max_length)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Train padded shape: {padded.shape}')\nprint(f'Test padded shape: {testing_padded.shape}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modelling\n### LSTM - Long Short Term Memory networks\n\nLSTM resolve the long term dependency issues of a network.\n\nLets take a sentence - `We live in Patna, which has a hot and humid climate`<br>\nIn this sentence \"hot\" and \"humid\" describe \"Patna\". But with our previous encoding, we don't take into account the relation of word before and after certain word. LSTM solves this issue by feeding the output of word at place `t` to the input of word at place `t+1`.\n\nThere are main types of LSTM. We are going to cover a few\n\n#### LSTM Classic\nIn this LSTM, the output of `t` is fed as input of `t+1` node. Here is a digram of **2 layer LSTM**.\n![](https://adventuresinmachinelearning.com/wp-content/uploads/2018/02/Keras-LSTM-tutorial-architecture.png)\n\n#### Bidirectional LSTM\nIn this LSTM, the output of `t` is fed as input of `t-1` and `t+1` node.\n![](https://www.i2tutorials.com/wp-content/uploads/2019/05/Deep-Dive-into-Bidirectional-LSTM-i2tutorials.jpg)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### LSTM Classic","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model with simple LSTM\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, 64),\n    tf.keras.layers.LSTM(64),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()\n\nnum_epochs = 10\nhistory_lstm = model.fit(padded, y_train, epochs=num_epochs, validation_data=(testing_padded, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Bidirectional LSTM","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Two bidirectional LSTM layers\n# Note that if you want to connect one LSTM to another, you have to pass return_sequences=True\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, 64, input_length=max_length),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()\n\nnum_epochs = 10\nhistory_bi = model.fit(padded, y_train, epochs=num_epochs, validation_data=(testing_padded, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Convolutional Neural Network","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convolutional layer \nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, 64, input_length=max_length),\n    tf.keras.layers.Conv1D(128, 5, activation='relu'),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()\n\nnum_epochs = 10\nhistory_conv = model.fit(padded, y_train, epochs=num_epochs, validation_data=(testing_padded, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### GRU\nA gated recurrent unit (GRU) is a gating mechanism in recurrent neural networks (RNN) similar to a long short-term memory (LSTM) unit but without an output gate. GRUâ€™s try to solve the vanishing gradient problem that can come with standard recurrent neural networks.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using Gated Recurrent Unit\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim),\n    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(32)),\n    tf.keras.layers.Dense(6, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()\n\nnum_epochs = 10\nhistory_gru = model.fit(padded, y_train, epochs=num_epochs, validation_data=(testing_padded, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Glove\n\nGloVe stands for global vectors for word representation. For more about Glove - [Read this](https://medium.com/analytics-vidhya/word-vectorization-using-glove-76919685ee0b).\n\nWe will be following 3 steps to generate model using glove vector.\n* Load Glove model which has 2.2M word embedding, each embedding of length 300\n* Get embedding of those words which are in our corpus and construct a embedding matrix of size - **Number of word in data x 300**\n* Assign the weight to our first layer and make them non-trainable","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load Glove model\nglove_pickle = '../input/pickled-glove840b300d-for-10sec-loading/glove.840B.300d.pkl'\n\nwith open(glove_pickle,'rb') as f:\n    embeddings_index = pickle.load(f)\n    \nprint(f'Number of words in Glove: {len(embeddings_index)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create an embedding matrix for the words we have in the dataset\nembedding_matrix = np.zeros((len(word_index) + 1, 300))\nfor word, i in tqdm(word_index.items()):\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n        \nprint(f'Shape of Embedding: {embedding_matrix.shape}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# A simple LSTM with glove embeddings and one dense layer\nmodel = tf.keras.Sequential()\nmodel.add(\n    tf.keras.layers.Embedding(\n        len(word_index) + 1, 300, weights=[embedding_matrix],\n        input_length=max_length, trainable=False\n    )\n)\nmodel.add(tf.keras.layers.LSTM(100, dropout=0.3, recurrent_dropout=0.3))\nmodel.add(tf.keras.layers.Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n    \nmodel.summary()\n\nnum_epochs = 10\nhistory_glove = model.fit(padded, y_train, epochs=num_epochs, validation_data=(testing_padded, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualization","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.graph_objects as go\nfig = make_subplots(\n    rows=2, cols=2,\n    subplot_titles=(\"Bi LSTM\", \"Convolution\", \"GRU\", \"Glove\"),\n    shared_yaxes=True,\n    shared_xaxes=True,\n    vertical_spacing=0.1,\n    horizontal_spacing=0.03)\n\nepochs = list(range(1, len(history_lstm.history['accuracy'])+1))\n\nfig.add_trace(\n    go.Scatter(x=epochs, y=history_bi.history['accuracy'], name='accuracy'),\n    row=1, col=1\n)\nfig.add_trace(\n    go.Scatter(x=epochs, y=history_bi.history['val_accuracy'], name='val_accuracy'),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=epochs, y=history_conv.history['accuracy'], name='accuracy'),\n    row=1, col=2\n)\nfig.add_trace(\n    go.Scatter(x=epochs, y=history_conv.history['val_accuracy'], name='val_accuracy'),\n    row=1, col=2\n)\n\nfig.add_trace(\n    go.Scatter(x=epochs, y=history_gru.history['accuracy'], name='accuracy'),\n    row=2, col=1\n)\nfig.add_trace(\n    go.Scatter(x=epochs, y=history_gru.history['val_accuracy'], name='val_accuracy'),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=epochs, y=history_glove.history['accuracy'], name='accuracy'),\n    row=2, col=2\n)\nfig.add_trace(\n    go.Scatter(x=epochs, y=history_glove.history['val_accuracy'], name='val_accuracy'),\n    row=2, col=2\n)\n\nfig.update_layout(title='Accuracy', showlegend=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = make_subplots(\n    rows=2, cols=2,\n    subplot_titles=(\"Bi LSTM\", \"Convolution\", \"GRU\", \"Glove\"),\n    shared_yaxes=True,\n    shared_xaxes=True,\n    vertical_spacing=0.1,\n    horizontal_spacing=0.03)\n\nepochs = list(range(1, len(history_lstm.history['loss'])+1))\n\nfig.add_trace(\n    go.Scatter(x=epochs, y=history_bi.history['loss'], name='loss'),\n    row=1, col=1\n)\nfig.add_trace(\n    go.Scatter(x=epochs, y=history_bi.history['val_loss'], name='val_loss'),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=epochs, y=history_conv.history['loss'], name='loss'),\n    row=1, col=2\n)\nfig.add_trace(\n    go.Scatter(x=epochs, y=history_conv.history['val_loss'], name='val_loss'),\n    row=1, col=2\n)\n\nfig.add_trace(\n    go.Scatter(x=epochs, y=history_gru.history['loss'], name='loss'),\n    row=2, col=1\n)\nfig.add_trace(\n    go.Scatter(x=epochs, y=history_gru.history['val_loss'], name='val_loss'),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=epochs, y=history_glove.history['loss'], name='loss'),\n    row=2, col=2\n)\nfig.add_trace(\n    go.Scatter(x=epochs, y=history_glove.history['val_loss'], name='val_loss'),\n    row=2, col=2\n)\n\nfig.update_layout(title='Loss', showlegend=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So that was all about some basic modelling for text classification. In the Next tutorial we'll be looking at text generation! Sounds Interesting? Here it is\n> ## PART 7: [Text Generation](https://www.kaggle.com/akashkr/tf-keras-tutorial-rnn-text-generation-part-7)","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}