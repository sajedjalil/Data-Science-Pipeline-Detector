{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings \nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing the libraries \n\nimport string\nimport re\nimport numpy as np \nimport pandas as pd\nimport nltk\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport emoji\nimport os\nfrom collections import defaultdict \n\nfrom tqdm import tqdm\nimport statistics \nfrom statistics import mode, mean\n\nfrom mlxtend.evaluate import confusion_matrix\nfrom mlxtend.classifier import StackingClassifier\n\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, KFold, GridSearchCV\nfrom sklearn.metrics import accuracy_score, auc, roc_auc_score, f1_score, recall_score, log_loss, roc_curve, auc\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.impute import KNNImputer\nfrom sklearn_pandas import CategoricalImputer\nimport xgboost as xgb\n\nfrom mlxtend.plotting import plot_confusion_matrix\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom nltk import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import VotingClassifier\n\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import LSTM, GRU\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\nfrom keras.utils import np_utils\n\n\nfrom keras.preprocessing import sequence, text\nfrom keras.callbacks import EarlyStopping\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets load our training and test datasets \n\nstop_words = stopwords.words(\"english\")\n\ntrain = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\n\ntest = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\nsample = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# increasing the limit of number of rows to be displayed\npd.set_option('display.max_rows', 10000)\npd.set_option('display.max_colwidth', -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a class where I will do my preprocessing and apply simple machine learning models\n\nword_net_lemmatizer = WordNetLemmatizer()\nw_token = nltk.tokenize.WhitespaceTokenizer()\n\n#creating a class\nclass Ensembler(object):\n    def __init__(self, model_dict, num_folds, optimize = accuracy_score, vectorizer_type = \"TfidfVectorizer\"):\n        \n        \"\"\"\n        :param model_dict: dictionary consisting of different models\n        :param num_folds : the number of folds \n        :param optimize  : the function to optimize, ex. accuracy score, classification report\n        :vectorizer_type : type of vectorizer to use (CountVectorizer or TfidfVectorizer or Word Embedding)\n        \"\"\"\n        \n        #Initializing\n        self.model_dict = model_dict\n        self.num_folds = num_folds\n        self.optimize = optimize\n        self.vectorizer_type = vectorizer_type\n        \n        \n        self.training_data = None\n        self.training_target = None\n        self.xtrain = None\n        self.test_data = None\n        self.lbl_enc = None\n        self.train_pred_dict = None\n        self.test_pred_dict = None\n        self.num_classes = None\n       \n        \n    # Creating a simple function to clean the tweets text (like removing emojis, links, punctuations and also doing spell check) \n    def clean_text(self, text):\n        text = str(text)\n        text = re.sub(r'http\\S+', '', text)\n        text = re.sub(r'<.*?>', '', text)\n        #removing emojis and unwanted text\n        text = emoji.get_emoji_regexp().sub(r\"\", text)\n        text = text.translate(str.maketrans(\"\",\"\", string.punctuation))\n        text = text.strip(\" \")\n        text = text.lower()\n        \n        #replacing incorrect words with correct words (can also use spellcheck library)\n        text = re.sub(\"rockyfire\", 'rocky fire', text)\n        text = re.sub('cafire', 'california fire', text)\n        text = re.sub('goooooooaaaaaal', 'goal', text)\n        text = re.sub('bbcmtd','bbc midlands', text)\n        text = re.sub('africanbaze',\"african blaze\", text)\n        text = re.sub('newsnigeria', \"news nigeria\", text)\n        text = re.sub('\\x89ûó', \"uo\", text)\n        text = re.sub('superintende', \"superintendent\", text)\n        text = re.sub('southridgelife', \"south ridge life\", text)\n        text = re.sub('carolinaåêablaze', \"carolina blaze\", text)\n        text = re.sub('elbestia',\"el bestia\", text)\n        text = re.sub('\\x89û', \"u\", text)\n        text = re.sub('news24680', \"news 24680\", text)\n        text = re.sub('nxwestmidlands', \"west midlands\", text)\n        text = re.sub('nashvilletraffic', \"nashville traffic\", text)\n        text = re.sub('personalinjury', \"personal injury\", text)\n        text = re.sub('otleyhour', 'otley hour', text)\n        text = re.sub('caraccidentlawyer', 'car accident lawyer', text)\n        text = re.sub('teeû', 'teen', text)\n        text = re.sub('bigrigradio', 'big radio', text)\n        text = re.sub('sleepjunkies', 'sleep junky', text)\n        text = re.sub('hwymagellan', 'highway magellan', text)\n        text = re.sub('080615', ' ', text)\n        text = re.sub('110358', ' ', text)\n        text = re.sub('accidentwho', 'accident who', text)\n        text = re.sub('truckcrash', 'truck crash', text)\n        text = re.sub('crashgt', 'crash gate', text)\n        text = re.sub('i540', 'I 540', text)\n        text = re.sub('fyi', 'for your information', text)\n        text = re.sub('cadfyi', 'cad for your information', text)\n        text = re.sub(' n ', ' and ', text)\n        text = re.sub('confû', 'conf', text)\n        text = re.sub('nh3a', 'national highway 3a', text)\n        text = re.sub('damagewpd1600', 'damage wpd 1600', text)\n        text = re.sub('nearfatal', 'near fatal', text)\n        text = re.sub('southaccident', 'south accident', text)\n        text = re.sub('rdconsider', 'consider', text)\n        text = re.sub('your4state', 'your for state', text)\n        text = re.sub('measuresarrestpastornganga', 'measures arrest pastor nganga', text)\n        text = re.sub('aftershockdelo', 'after shock delo', text)\n        text = re.sub('trapmusic', 'trap music', text)\n        text = re.sub('icesû', 'ices', text)\n        text = re.sub('growingupspoiled', 'growing up spoiled', text)\n        text = re.sub('kjfordays', 'kj for days', text)\n        text = re.sub('wisdomwed', 'wisdom wed', text)\n        text = re.sub('ã¢', 'ac', text)\n        text = re.sub('fullã¢', 'full ac', text)\n        text = re.sub('esquireattire', 'esquire attire', text)\n        text = re.sub('wdyouth', 'wd youth', text)\n        text = re.sub('onfireanders', 'on fire anders', text)\n        text = re.sub('aftershockorg', 'after shock org', text)\n        text = re.sub('watchthevideo', 'watch the video', text)\n        text = re.sub('wednesdayû', 'Wednesday', text)\n        text = re.sub('freakyû', 'freaky', text)\n        text = re.sub('airplaneåê29072015', 'airplane', text)\n        text = re.sub('canûªt', 'can not', text)\n        text = re.sub('randomthought', 'random thought', text)\n        text = re.sub('rejectdcartoons', 'rejected cartoons', text)\n        text = re.sub('û÷minimum', 'minimum', text)\n        text = re.sub('wageûª', 'wage', text)\n        text = re.sub('andû', 'and', text)\n        text = re.sub('celticindeed', 'celtic indeed', text)\n        text = re.sub('viralspell', 'viral spell', text)\n        text = re.sub('suregod', 'sure god', text)\n        text = re.sub('breakfastone', 'break fast tone', text)\n        text = re.sub('uptotheminute', 'upto the minute', text)\n        text = re.sub('stormbeard', 'storm beard', text)\n        text = re.sub('steellord', 'steel lord', text)\n        text = re.sub('fantasticfourfant4sticwhatever', 'fantastic four whatever', text)\n        text = re.sub('starmade', 'star made', text)\n        text = re.sub('signatureschange', 'signatures change', text)\n        text = re.sub('petitiontake', 'petition take', text)\n        text = re.sub('dieplease', 'die please', text)\n        text = re.sub('warmbodies', 'warm bodies', text)\n        text = re.sub('geekapocalypse', 'geek apocalypse', text)\n        text = re.sub('doublecups', 'double cups', text)\n        text = re.sub('wifekids', 'wife kids', text)\n        text = re.sub('whitewalkers', 'white walkers', text)\n        text = re.sub('historicchurch', 'historic church', text)\n        text = re.sub('newsintweets', 'news in tweets', text)\n        text = re.sub('griefûª', 'grief', text)\n        text = re.sub('countynews', 'county news', text)\n        text = re.sub('û÷politics', 'politics', text)\n        text = re.sub('chicagoarea', 'chicago area', text)\n        text = re.sub('theadvocatemag', 'the advocate mag', text)\n        text = re.sub('arsonû', 'arson', text)\n        text = re.sub('cloudygoldrush', 'cloudy gold rush', text)\n        text = re.sub('uniteblue', 'unite blue', text)\n        text = re.sub('tonightûªs', 'tonights', text)\n        text = re.sub('arsonistmusic', 'arsonist music', text)\n        text = re.sub('slimebeast', 'slime beast', text)\n        text = re.sub('bestcomedyvine', 'best comedy vine', text)\n        text = re.sub('localarsonist', 'local arsonist', text)\n        text = re.sub('nativehuman', 'native human', text)\n        text = re.sub('myreligion', 'my religion', text)\n        text = re.sub('ûïhatchet', 'hatchet', text)\n        text = re.sub('controlû', 'control', text)\n        text = re.sub('4suspected', 'four suspected', text)\n        text = re.sub('acebreakingnews', 'ace breaking news', text)\n        text = re.sub('attackshare', 'attack share', text)\n        text = re.sub('obamadhs', 'obama dhs', text)\n        text = re.sub('blazerfan', 'blazer fan', text)\n        text = re.sub('benothing', 'be nothing', text)\n        text = re.sub('daytonarea', 'dayton area', text)\n        text = re.sub('messeymetoo', 'messy me too', text)\n        text = re.sub('robotcoingame', 'robot coin game', text)\n        text = re.sub('freebitcoin', 'free bitcoin', text)\n        text = re.sub('sportsroadhouse', 'sports road house', text)\n        text = re.sub('weddinghour', 'wedding hour', text)\n        text = re.sub('ghostoftheav', 'ghost of the av', text)\n        text = re.sub('fuelgas', 'fuel gas', text)\n        text = re.sub('û÷avalancheûª', 'avlanche', text)\n        text = re.sub('coloradoavalanche', 'colorado avlanche', text)\n        text = re.sub('mildmannered', 'mild mannered', text)\n        text = re.sub('neur0sis', 'neurosis', text)\n        text = re.sub('cbsbigbrother', 'cbs big brother', text)\n        text = re.sub('sexydragonmagic', 'sexy dragon magic', text)\n        text = re.sub('detroitpls', 'detroit please', text)\n        text = re.sub('postbattle', 'post battle', text)\n        text = re.sub('httû', ' ', text)\n        text = re.sub('foxnewû', 'foxnews', text)\n        text = re.sub('bioterû', 'bio terrorism', text)\n        text = re.sub('infectiousdiseases', 'infectious diseases', text)\n        text = re.sub('thelonevirologi', 'the lone virology', text)\n        text = re.sub('clergyforced', 'clergy forced', text)\n        text = re.sub('bioterrorismap', 'bio terrorism', text)\n        text = re.sub('digitalhealth', 'digital health', text)\n        text = re.sub('bioterrorismim', 'bio terrorism', text)\n        text = re.sub('hostageamp2', 'hostage and 2', text)\n        text = re.sub('wbioterrorismampuse', 'bioterrorism puse', text)\n        text = re.sub('harvardu', 'harvard university', text)\n        text = re.sub('irandeal', 'iran deal', text)\n        text = re.sub('cdcgov', 'cdc government', text)\n        text = re.sub('raisinfingers', 'raising fingers', text)\n        text = re.sub('skywars', 'sky wars', text)\n        text = re.sub('agochicago', 'ago chicago', text)\n        text = re.sub('thisispublichealth', 'this is public health', text)\n        text = re.sub('sothwest', 'south west', text)\n        text = re.sub('weekold', 'week old', text)\n        text = re.sub('fireû', 'fire', text)\n        text = re.sub('artisteoftheweekfact', 'artist of the week fact', text)\n        text = re.sub('clubbanger', 'club banger', text)\n        text = re.sub('listenlive', 'listen live', text)\n        text = re.sub('weatherstay', 'weather stay', text)\n        text = re.sub('transcendblazing', 'transcend blazing', text)\n        text = re.sub('stoponesounds', 'stop one sounds', text)\n        text = re.sub('stickynyc', 'sticky new york city', text)\n        text = re.sub('95roots', '95 roots', text)\n        text = re.sub('blazingben', 'blazing ben', text)\n        text = re.sub('shouout', 'shout out', text)\n        text = re.sub('s3xleak', 'sex leak', text)\n        text = re.sub('ph0tos', 'photos', text)\n        text = re.sub('exp0sed', 'exposed', text)\n        text = re.sub('notû', 'not', text)\n        text = re.sub('blazingelwoods', 'blazing el woods', text)\n        text = re.sub('funkylilshack', 'funky little shack', text)\n        text = re.sub('wellgrounded', 'well grounded', text)\n        text = re.sub('sodamntrue', 'so damn true', text)\n        text = re.sub('hopeinhearts', 'hope in hearts', text)\n        text = re.sub('onlyftf', 'only for this Friday', text)\n        text = re.sub('robsimss', 'rob sims', text)\n        text = re.sub('cantmisskid', 'can not miss kid', text)\n        text = re.sub('yahooschwab', 'yahoo schwab', text)\n        text = re.sub('fiascothat', 'fiasco that', text)\n        text = re.sub('harperanetflixshow', 'harper net flix show', text)\n        text = re.sub('stopharper', 'stop harper', text)\n        text = re.sub('graywardens', 'gray wardens', text)\n        text = re.sub('realhotcullen', 'real hot cullen', text)\n        text = re.sub('developmentû', 'development', text)\n        text = re.sub('iclowns', 'I clowns', text)\n        text = re.sub('2iclown', 'two I clown', text)\n        text = re.sub('revolutionblight', 'revolution light', text)\n        text = re.sub('healthweekly1', 'health weekly', text)\n        text = re.sub('amateurnester', 'amateur nester', text)\n        text = re.sub('parksboardfacts', 'parks board facts', text)\n        text = re.sub('stevenontwatter', 'steven on twitter', text)\n        text = re.sub('pussyxdestroyer', 'pussy destroyer', text)\n        text = re.sub('radioriffrocks', 'radio riff rocks', text)\n        text = re.sub('tweet4taiji', 'tweet for taiji', text)\n        text = re.sub('blizzardfans', 'blizzard fans', text)\n        text = re.sub('blizzardgamin', 'blizzard gaming', text)\n        text = re.sub('fairx818x', 'fair', text)\n        text = re.sub('playoverwatch', 'play over watch', text)\n        text = re.sub('blizzardcs', 'blizzards', text)\n        text = re.sub('blizzarddraco', 'blizzard draco', text)\n        text = re.sub('lonewolffur', 'lone wolf fur', text)\n        text = re.sub('bubblycuteone', 'bubbly cute one', text)\n        text = re.sub('nailreal', 'nail real', text)\n        text = re.sub('bookanother', 'book another', text)\n        text = re.sub('chamberedblood', 'chambered blood', text)\n        text = re.sub('speakingfromexperience', 'speaking from experience', text)\n        text = re.sub('decisionsondecisions', 'decisions on decisions', text)\n        text = re.sub('dangerousbeans', 'dangerous beans', text)\n        text = re.sub('5sos', '5 sos', text)\n        text = re.sub('everwhe', 'everywhere', text)\n        text = re.sub('bloodû', 'blood', text)\n        text = re.sub('butterlondon', 'butter london', text)\n        text = re.sub('bbloggers', 'bloggers', text)\n        text = re.sub('resigninshame', 'resign in shame', text)\n        text = re.sub('kingûªs', 'kings', text)\n        text = re.sub('û÷the', 'the', text)\n        text = re.sub('towerûª', 'tower', text)\n        text = re.sub('thedarktower', 'the dark tower', text)\n        text = re.sub('bdisgusting', ' disgusting', text)\n        text = re.sub('wwwbigbaldhead', 'big bald head', text)\n        text = re.sub('jessienojoke', 'jessie no joke', text)\n        text = re.sub('chxrmingprince', 'charming prince', text)\n        text = re.sub('indiansfor', 'indians for', text)\n        text = re.sub('bloodymonday', 'bloody Monday', text)\n        text = re.sub('tvshowtime', 'tv showtime', text)\n        text = re.sub('machinegunkelly', 'machine gun kelly', text)\n        text = re.sub('thisdayinhistory', 'this day in history', text)\n        text = re.sub('taylorswift13', 'taylor swift 13', text)\n        text = re.sub('musicadvisory', 'music advisory', text)\n        text = re.sub('weûªre', 'we are', text)\n        text = re.sub('weûªve', 'we have', text)\n        text = re.sub('theboyofmasks', 'the boy of masks', text)\n        text = re.sub('gentlementhe', 'gentle men the', text)\n        text = re.sub('kalinandmyles', 'kalin and myles', text)\n        text = re.sub('kalinwhite', 'kalin white', text)\n        text = re.sub('givebackkalinwhiteaccount', 'give back kalin white account', text)\n        text = re.sub('princessduck', 'princess duck', text)\n        text = re.sub('dogûªs', 'dogs', text)\n        text = re.sub('hopefulbatgirl', 'hopeful bat girl', text)\n        text = re.sub('piperwearsthepants', 'piper wears the pants', text)\n        text = re.sub('questergirl', 'quester girl', text)\n        text = re.sub('readû', 'read', text)\n        text = re.sub('bagû', 'bag', text)\n        text = re.sub('deliciousvomit', 'delicious vomit', text)\n        text = re.sub('today4got', 'today forgot', text)\n        text = re.sub('ovofest', 'ovo fest', text)\n        text = re.sub('2k15', '2015', text)\n        text = re.sub('officialrealrap', 'official real rap', text)\n        text = re.sub('im2ad', 'I am too ad', text)\n        text = re.sub('bodybagging', 'body bagging', text)\n        text = re.sub('womengirls', 'women girls', text)\n        text = re.sub('boomerangtime', 'boomerang time', text)\n        text = re.sub('û÷institute', 'institute', text)\n        text = re.sub('peaceûª', 'peace', text)\n        text = re.sub('moving2k15', 'moving 2015', text)\n        text = re.sub('expertwhiner', 'expert whiner', text)\n        text = re.sub('shopûªs', 'shop', text)\n        text = re.sub('cutekitten', 'cute kitten', text)\n        text = re.sub('catsofinstagram', 'cats of instagram', text)\n        text = re.sub('summerinsweden', 'summer in sweden', text)\n        text = re.sub('ûïparties', 'parties', text)\n        text = re.sub('drivingû', 'driving', text)\n        text = re.sub('youûªll', 'you all', text)\n        text = re.sub('û÷body', 'body', text)\n        text = re.sub('bagsûª', 'bags', text)\n        text = re.sub('whatcanthedo', 'what can the do', text)\n        text = re.sub('70year', '70 years', text)\n        text = re.sub('hatchetwielding', 'hatchet wielding', text)\n        text = re.sub('invadedbombed', 'invaded bombed', text)\n        text = re.sub('bombedout', 'bombed out', text)\n        text = re.sub('elephantintheroom', 'elephant in the room', text)\n        text = re.sub('abombed', 'a bombed', text)\n        text = re.sub('tblack', 'black', text)\n        text = re.sub('bellybombed', 'belly bombed', text)\n        text = re.sub('teamstream', 'team stream', text)\n        text = re.sub('beyondgps', 'beyond gps', text)\n        text = re.sub('cityamp3others', 'city and three others', text)\n        text = re.sub('scheduleû', 'schedule', text)\n        text = re.sub('moscowghost', 'moscow ghost', text)\n        text = re.sub('banthebomb', 'ban the bomb', text)\n        text = re.sub('setting4success', 'setting for success', text)\n        text = re.sub('pearlharbor', 'pearl harbor', text)\n        text = re.sub('push2left', 'push to left', text)\n        text = re.sub('snapharmony', 'snap harmony', text)\n        text = re.sub('australiaûªs', 'australia', text)\n        text = re.sub('huffpostarts', 'huffpost arts', text)\n        text = re.sub('jewishpress', 'jewish press', text)\n        text = re.sub('bloopandablast', 'bloop and a blast', text)\n        text = re.sub('traintragedy', 'train tragedy', text)\n        text = re.sub('slingnews', 'sling news', text)\n        text = re.sub('urgentthere', 'urgent there', text)\n        text = re.sub('blacklivesmatter', 'black lives matter', text)\n        text = re.sub('fewmoretweets', 'few more tweets', text)\n        text = re.sub('9newsmornings', '9news mornings', text)\n        text = re.sub('strikesstrikes', 'strikes', text)\n        text = re.sub('doctorfluxx', 'doctor flux', text)\n        text = re.sub('thestrain', 'the strain', text)\n        text = re.sub('newyorkû', 'new york', text)\n        text = re.sub('tweetlikeitsseptember11th2001', 'tweet like it is september 11th 2011', text)\n        text = re.sub('222pm', '2:22 pm', text)\n        text = re.sub('ppsellsbabyparts', 'pp sells baby parts', text)\n        text = re.sub('bbcintroducing', 'bbc introducing', text)\n        text = re.sub('giantgiantsound', 'giant giant sound', text)\n        text = re.sub('threealarm', 'three alarm', text)\n        text = re.sub('3alarm', 'three alarm', text)\n        text = re.sub('wildlionx3', 'wild lion', text)\n        text = re.sub('cubstalk', 'cub stalk', text)\n        text = re.sub('letsfootball', 'lets football', text)\n        text = re.sub('totteham', 'tottenham', text)\n        text = re.sub('thatûªs', 'that is', text)\n        text = re.sub('burnfat', 'burn fat', text)\n        text = re.sub('rvaping101', 'vaping 101', text)\n        text = re.sub('lightningcaused', 'lightening caused', text)\n        text = re.sub('mountainsû', 'mountains', text)\n        text = re.sub('sniiiiiiff', 'sniff', text)\n        text = re.sub('youûªre', 'you are', text)\n        text = re.sub('foxnewsvideo', 'foxnews video', text)\n        text = re.sub('forestservice', 'forest service', text)\n        text = re.sub('buildingsûówe', 'buildings we', text)\n        text = re.sub('progress4ohio', 'progress for ohio', text)\n        text = re.sub('slashandburn', 'slash and burn', text)\n        text = re.sub('jamaicaobserver', 'jamaica observer', text)\n        text = re.sub('cnewslive', 'cnews live', text)\n        text = re.sub('appreciativeinquiry', 'appreciative inquiry', text)\n        text = re.sub('standwithpp', 'stand with pp', text)\n        text = re.sub('scaryeven', 'scary even', text)\n        text = re.sub('attackclose', 'attack close', text)               \n        \n        return text\n\n    # Creating another function to lemmatize the text \n    def lemmatizer(self, text):\n        lis = []\n        words = text.split()\n        for word in words: \n            if(word not in stop_words):\n                lis.append(word_net_lemmatizer.lemmatize(word, pos = 'v'))\n            else:\n                continue        \n        return lis\n\n    # Creating function to create embedding_index which we will use for word embeddings\n    def word_embedding_index(self):\n        # loading the glove vectors in a dictionary\n        embeddings_index = {}\n        f = open('/kaggle/input/glove42b300dtxt/glove.42B.300d.txt', encoding = 'utf-8')\n        print(\"Starting Embedding\")\n        for line in (f):\n            values = line.split()\n            word = values[0]\n            coefs = np.asarray(values[1:], dtype = 'float32')\n            embeddings_index[word] = coefs\n        f.close()\n        print(\"Embedding Done\")\n        return embeddings_index\n        \n    \n    \n    # This function will create a normalized vector for the whole sentence\n    def sen2vect(self, s, embedding_index):\n        embeddings_index = embedding_index \n        words= str(s)       #typecasting to string\n        words = word_tokenize(words)    #creating a word_token from the sentence/tweets that we have\n        words = [w for w in words if not w in stop_words]    #removing the stopwords from tweet\n        words = [w for w in words if w.isalpha()]            #only keeping the words\n    \n        M = []\n        \n        #using for loop to store all the coefficients in a list\n        for w in words:\n            try:\n                M.append(embeddings_index[w])               #using embedding_index creating a token from that sentence\n            except:\n                continue\n        \n        M = np.array(M)\n        v = M.sum(axis = 0)\n        #this is used to normalize the vector\n        if(type(v) !=np.ndarray):\n            return np.zeros(300)\n        return v/np.sqrt((v ** 2).sum())\n    \n    \n    # Function definition for plotting the confusion matrix \n    def conf_matrix(self, data, Labels = ['No-Disaster', 'Disaster']):\n        matrix_data = data\n        count = 1\n        # using for loop to access the data (its a dictionary)\n        for name, data in matrix_data.items():\n        \n            # setting the figure size\n            plt.figure(figsize=(8,12), facecolor='white')\n        \n            # assigning the number of subplots and with each count increment the axis\n            ax = plt.subplot(2, 1,count)\n        \n            # using seaborn heatmap to plot confusion matrix\n            sns.heatmap(data, xticklabels= Labels, yticklabels= Labels, annot = True, ax = ax, fmt= 'g')\n        \n            # setting the title name\n            ax.set_title(\"Confusion Matrix of {}\".format(name))\n        \n            plt.xlabel(\"Predicted Class\") #setting the x label\n            plt.ylabel(\"True Class\")   # setting the y label\n            plt.show()\n            count = count +1   # incrementor to increase the value of axis with each loop \n\n\n    #Here creating a function for preprocessing, all the preprocessing steps are done here (cleaning text, lemmatizing, word _embedding)\n    def preprocess(self, training_data, test_data):\n        \"\"\"\n        :param training_data: training_data in tabular format\n        :param test_data    : test_data in tabular format\n        :return             : return preprocessed data for modelling\n        \"\"\"\n        #initializing variables\n        self.training_data = training_data\n        self.test_data = test_data\n        self.vectorizer_type = self.vectorizer_type\n        \n        #Here I am using a clean_text() function to remove html strings and emojis\n        self.training_data['text'] = self.training_data[\"text\"].apply(lambda x: self.clean_text(x))\n        self.test_data['text'] = self.test_data['text'].apply(lambda x: self.clean_text(x))\n            \n        # Lets use lemmatize function to lemmatize the text\n        self.training_data['text'] = self.training_data[\"text\"].apply(lambda x: \" \".join(self.lemmatizer(x)))\n        self.test_data['text'] = self.test_data['text'].apply(lambda x: \" \".join(self.lemmatizer(x)))\n            \n        self.training_data['text']\n        # Here I am also dropping all the columns except for the text column\n        self.training_data = self.training_data.drop(['id', 'keyword','location'],axis = 1)\n        self.test_data = self.test_data.drop(['id', 'keyword', 'location'], axis = 1)\n        \n        if(self.vectorizer_type == \"TfidfVectorizer\"):\n            #lets use tfidf vectorizer to convert categorical columns to numerical columns\n            self.tfv = TfidfVectorizer(min_df = 3, max_features = 300, strip_accents=\"unicode\", analyzer=\"word\", token_pattern=r'\\w{1,}', ngram_range= (1,3), use_idf= 1, smooth_idf = 1, sublinear_tf = 1, stop_words='english')\n            \n            #fitting the tfidf on the training data\n            self.tfv.fit(self.training_data['text'])\n            \n            #transforming both train and test data using tfidf vectorizer\n            train_transformed = self.tfv.transform(self.training_data['text'])\n            test_transformed = self.tfv.transform(self.test_data['text'])\n                \n        elif(self.vectorizer_type == \"CountVectorizer\"):\n            #lets use tfidf vectorizer to convert categorical columns to numerical columns\n            self.ctv =CountVectorizer(max_features = 300,analyzer= 'word', token_pattern=r'\\w{1,}',ngram_range= (1,3), stop_words='english')\n            \n            #fitting the count vectorizer on the training data\n            self.ctv.fit(self.training_data['text'])\n            \n            #transforming both train and test data using countVectorizer\n            train_transformed = self.ctv.transform(self.training_data['text'])\n            test_transformed = self.ctv.transform(self.test_data['text'])\n                \n        elif(self.vectorizer_type == \"WordEmbedding\"):\n            #Calling my word_embedding_index function that I initialized above to create embedding index\n            embeddings_index = self.word_embedding_index()\n            print(\"Transforming the dataset\")\n            \n            #Now, using my embedding_index, transforming both train and test data using sen2vect function.\n            train_transformed = [self.sen2vect(x, embeddings_index) for x in (self.training_data['text'])]\n            test_transformed = [self.sen2vect(x, embeddings_index) for x in (self.test_data['text'])]\n            \n            #converting both the train and test transformed to numpy array\n            train_transformed = np.array(train_transformed)\n            test_transformed = np.array(test_transformed)\n            print(\"Dataset Transformed. Next step Modelling\")\n        \n        #returning \n        return self.training_data, self.test_data, train_transformed, test_transformed\n    \n    \n    #Creating a fit_predict_train_valid function, here i am splitting the train data into train and valid and applying simple\n    #machine learning models\n    def fit_predict_train_valid(self,train_transformed, test_transformed, model_dict, vectorizer_type = \"TfidfVectorizer\"):\n        \n        #initalizing \n        self.train_transformed =train_transformed\n        self.test_transformed  = test_transformed\n        self.model_dict = model_dict    \n         \n        #using stratifiedKFold to split the train data into train and valid\n        skf = StratifiedKFold(n_splits = self.num_folds, random_state = 0, shuffle =True)\n        \n        #Initializng some required lists and dictionaries\n        count = 0   #this will keep in check the number of folds\n        single_model_score = {}    #storing all the accuracy scores in a dictionary\n        stack_model_score = []    #a list to store all the scores of stack model\n        model_list = []            #creating a list of models that we are using\n        mean_dict = {}           #creating a mean_dict to store the mean of accuracy scores of all the models\n        \n        #creating a for loop to split our data into train and valid\n        for train_index, valid_index in skf.split(train_transformed, self.training_data['target']):\n            count = count+1\n            print(\"Training for Fold: {}\".format(count))\n            \n            #splitting into xtrain, xvalid, ytrain, yvalid\n            xtrain, xvalid = train_transformed[train_index], train_transformed[valid_index]\n            ytrain, yvalid = self.training_data['target'][train_index], self.training_data['target'][valid_index]\n            \n            #using for loop to run all the models inside the model_dict dictionary\n            for name, model in model_dict.items():\n                \n                #initializing the model\n                model = model\n                #fitting the model\n                model.fit(xtrain, ytrain)\n                \n                #predicting the xvalid from our model\n                y_pred = model.predict(xvalid)\n                \n                #calculating the accuracy scores\n                score = accuracy_score(yvalid, y_pred)\n                \n                #using if else function to store the accuracy scores in the dictionary. Storing at as a list of values\n                if(name not in single_model_score):\n                    single_model_score[name] = [score]\n                else: \n                    single_model_score[name].append(score)\n                    \n                #appening all the models in model_list\n                model_list.append(model)\n                \n                #printing the accuracy score of each model in every fold\n                print(\"\\tAccuracy score of {}:{:.3f}\".format(name, score))\n            \n            # here i am using stacking classifier, an ensemble technique to see if we can improve the accuracy \n            stack_classifier = StackingClassifier(classifiers = model_list, meta_classifier= xgb.XGBClassifier(n_estimators =  400, learning_rate =0.1, max_depth=5, min_child_weight=1, subsample=0.8, colsample_bytree=0.8, objective= 'binary:logistic',nthread=4, scale_pos_weight=1))\n            \n            #fitting the stack_classifier on xtrain and ytrain\n            stack_classifier.fit(xtrain, ytrain)    \n            \n            #predicting the results of xvalid using stack_classifier\n            final_pred = stack_classifier.predict(xvalid)\n            \n            #calculating the accuracy score of our stack_Classifier\n            stack_score = accuracy_score(yvalid, final_pred)\n            \n            #using stack_model_Score list to store the accuracy scores in different folds\n            stack_model_score.append(stack_score)\n            \n            #printing the accuracy score of stack classifier with each fold\n            print(\"\\tAccuracy score by Stacking all classifier is: {:.3f}\".format(stack_score))\n        \n     \n        #using for loop to print the mean_accuracy scores of all the models in all the folds\n        for name, score in single_model_score.items():\n            print(\"Mean Accuracy score of {} is: {:.3f}\".format(name, np.mean(score)))\n            #storing all the mean accuracies in a dictionary\n            mean_dict[name] = np.mean(score)\n        print(\"Mean Accuracy score of Stacking of classifier is:{:.3f} \".format(mean(stack_model_score)))\n        \n        #returning mean_dict and model_list which we will use in our next function\n        return mean_dict, model_dict\n    \n    \n    #this function will use the best model from our previous function and train the whole training dataset and then will\n    #do a prediction on the unseen test data and will create a submission file for kaggle\n    def final_train_predict(self, train_transformed, ytrain_target, test_transformed, model_dict, score_dict, submission):\n        \n        #initializing\n        self.train_transformed = train_transformed\n        self.test_transformed = test_transformed\n        self.model_dict = model_dict\n        \n        sample = submission\n        mean_score_dict = score_dict\n        y_true = ytrain_target\n        \n        #initializing the lists that are required to plot the roc curve\n        model_list = []\n        auc_list = []\n        fpr_list = []\n        tpr_list = []\n        \n        #from the mean_dict we are getting the model from which we got the maximum accuracies in our validation data\n        modelName_max_accuracy = max(mean_score_dict, key = mean_score_dict.get)\n        \n        #using for loop to go through all the models in our model_dict dictionary\n        for name, model in self.model_dict.items():\n            \n            #appending the model in a list which we will use for stacking classifier\n            model_list.append(model)\n            \n            #using if statement to only use the model from which we got the maximum accuracy\n            if(modelName_max_accuracy in name):\n                #initializing the model\n                model = model\n                #fitting the whole train data in our model\n                model.fit(train_transformed, y_true)\n                \n                #predicting the train data and test data using our model\n                ytrain_pred = model.predict(train_transformed)\n                ytest_pred = model.predict(test_transformed)\n                \n                #creating a confusion matrix from our predicitions. Since, test data is unseen we will use our training data for that\n                confusionMatrix = confusion_matrix(y_true, ytrain_pred)\n                \n                #calculating fpr, tpr and auc scores\n                fpr, tpr, threshold = roc_curve(y_true, ytrain_pred)\n                auc1 = auc(fpr, tpr)\n                \n                #storing all the scores in a list which we will use later for plotting the roc curve\n                fpr_list.append(fpr)\n                tpr_list.append(tpr)\n                auc_list.append(auc1)\n                \n                #printing the accuracy score of our model on train data\n                print(\"Accuracy score of {} is:{:.3f}\".format(modelName_max_accuracy, accuracy_score(y_true, ytrain_pred)))\n                \n                #creating a csv submission file\n                sample['target'] = ytest_pred\n                sample.to_csv(modelName_max_accuracy+'_'+self.vectorizer_type+'_submission.csv', index = False)\n            \n            #else statement to continue if the model didn't preform good in validation data\n            else: \n                continue\n            \n            \n            #Lets try the whole training dataset on stacking classifier\n            stack_classifier = StackingClassifier(classifiers = model_list, meta_classifier= xgb.XGBClassifier(n_estimators =  400, learning_rate =0.1, max_depth=5, min_child_weight=1, subsample=0.8, colsample_bytree=0.8, objective= 'binary:logistic',nthread=4, scale_pos_weight=1))\n            \n            #fitting the training data on stacking classifier\n            stack_classifier.fit(train_transformed, y_true)\n            \n            #predicting the train and test data results using stacking classifier\n            stack_ytrain_pred = stack_classifier.predict(train_transformed)    \n            stack_ytest_pred = stack_classifier.predict(test_transformed)\n            \n            #creating a confusion matrix from our prediction\n            confusionMat = confusion_matrix(y_true, stack_ytrain_pred)\n            \n            #calculating the fpr, tpr and auc scores from the results of stacking classifier\n            fpr_stack, tpr_stack, threshold_stack = roc_curve(y_true, stack_ytrain_pred)\n            auc_stack = auc(fpr_stack, tpr_stack)\n            \n            #storing all the results in a list\n            fpr_list.append(fpr_stack)\n            tpr_list.append(tpr_stack)\n            auc_list.append(auc_stack)\n            \n            #printing the accuracy of my stack_Classifier on training data\n            print(\"Accuracy score of {} is:{:.3f}\".format(\"StackingClassifier\", accuracy_score(y_true, stack_ytrain_pred)))\n            \n            #creating a csv submission file for stack of classifiers\n            sample['target'] = stack_ytest_pred\n            sample.to_csv(\"Stack_\"+self.vectorizer_type+\"_submission.csv\", index = False)\n            \n            #plotting the confusion matrix using the con_matrix function initialized before the preprocess steps\n            matrix_data = {modelName_max_accuracy: confusionMatrix, \"StackingClassifier\": confusionMat}\n            self.conf_matrix(matrix_data)\n            \n            #returning required lists\n            return fpr_list, tpr_list, auc_list\n        \n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Implementing Count Vectorizer","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Here I am using the above class to implement countVectorizer\nvect = \"CountVectorizer\"\n\n#initializing the model dictionary that we will use for modeling\nmodel_dict = {\"Logistic Regression\": LogisticRegression(C = 1.0, max_iter = 10000), \n              \"Random Forest Classifier\": RandomForestClassifier(n_estimators = 600),\n              \"XGBoost Classifier\": xgb.XGBClassifier(n_estimators =  400, learning_rate =0.1, subsample=0.8, colsample_bytree=0.8, objective= 'binary:logistic',\n                                 nthread=4, scale_pos_weight=1)}\n\n#initializing an object for class\nensCount = Ensembler(model_dict = model_dict, num_folds = 5, optimize=accuracy_score, vectorizer_type =vect)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#preprocessing using original train and test data. Return values from this function are, preprocessed training set, test set and \n#transformed train set and test set\n\ntraining, testing, train_tran_count, test_tran_count =  ensCount.preprocess(train, test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#splitting the data and training on xtrain, ytrain and predicting on xvalid, yvalid. using the transformed data for modeling\n#Return values from this function are mean_accuracy_Score dictionary and list of models\n\nmean_dict, model_list = ensCount.fit_predict_train_valid(train_tran_count, test_tran_count, model_dict = model_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#modeling on the whole training data and predicting on the unseen test data. Also, creating submission file.\n#Return values from this function are list of fpr, tpr and auc scores\n\nfpr_count_list, tpr_count_list, auc_count_list = ensCount.final_train_predict(train_tran_count, training['target'], test_tran_count, model_list,\n                                                                  mean_dict, sample)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Implementing Tfidf Vectorizer.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Here I am implementing Tfidf Vectorizer\n\nvect = \"TfidfVectorizer\"\n\n#initializing model dictionary. these are the models which we will use in our class for training and prediciton\nmodel_dict = {\"Logistic Regression\": LogisticRegression(C = 1.0, max_iter = 10000), \n              \"Random Forest Classifier\": RandomForestClassifier(n_estimators = 600),\n              \"XGBoost Classifier\": xgb.XGBClassifier(n_estimators =  400, learning_rate =0.1, subsample=0.8, colsample_bytree=0.8, objective= 'binary:logistic',\n                                 nthread=4, scale_pos_weight=1)}\n\n#initializing a class object \nensTfidf = Ensembler(model_dict = model_dict, num_folds = 5, optimize=accuracy_score, vectorizer_type =vect)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#this is a preprocessing step. using original train and test data to preprocess.\n\ntraining, testing, train_tran_tfidf, test_tran_tfidf =  ensTfidf.preprocess(train, test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#using the fit_predict_train_valid function from class, splitting the data into train and validation and modelling on those\n\nmean_dict, model_list = ensTfidf.fit_predict_train_valid(train_tran_tfidf, test_tran_tfidf, model_dict = model_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#here, I am training on the whole train transformed data and predicting on the unseen data. \n\nfpr_tfidf_list, tpr_tfidf_list, auc_tfidf_list = ensTfidf.final_train_predict(train_tran_tfidf, training['target'], test_tran_tfidf, model_list,\n                                                                  mean_dict, sample)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Implementing Word Embedding","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here I am implementing the glove word embedding from stanford\n\nvect = \"WordEmbedding\"\n\n#againg initializing the model_dictionary that we will use in our class\nmodel_dict = {\"Logistic Regression\": LogisticRegression(C = 1.0, max_iter = 10000), \n              \"Random Forest Classifier\": RandomForestClassifier(n_estimators = 600),\n              \"XGBoost Classifier\": xgb.XGBClassifier(n_estimators =  400, learning_rate =0.1, subsample=0.8, colsample_bytree=0.8, objective= 'binary:logistic',\n                                 nthread=4, scale_pos_weight=1)}\n\n#creating an object for our ensembler class\nensWord = Ensembler(model_dict = model_dict, num_folds = 5, optimize=accuracy_score, vectorizer_type =vect)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#preprocessing the data on our train and test set.\n\ntraining, testing, train_tran, test_tran =  ensWord.preprocess(train, test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#here i am splitting the data and creating predictive models on train and valid sets.\n\nmean_dict, model_list = ensWord.fit_predict_train_valid(train_tran, test_tran, model_dict = model_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#here, i am using the whole training dataset for training and predicting on the test data. fpr, tpr and auc are the returned values\n\nfpr_word_list, tpr_word_list, auc_word_list = ensWord.final_train_predict(train_tran, training['target'], test_tran, model_list,\n                                                                  mean_dict, sample)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Implementing Deep Learning Models with Keras","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# function for creating a confusion matrix outside the class\n\ndef confusionMatrix(data, Labels = ['No-Disaster', 'Disaster']):\n            \n    # setting the figure size\n    plt.figure(figsize=(4,4), facecolor='white')\n        \n    # using seaborn heatmap to plot confusion matrix\n    sns.heatmap(data, xticklabels= Labels, yticklabels= Labels, annot = True, fmt= 'g')\n    \n    # setting the title name\n    plt.title(\"Confusion Matrix\")\n    plt.xlabel(\"Predicted Class\") #setting the x label\n    plt.ylabel(\"True Class\")   # setting the y label\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#converting the transformed train and test that we are getting from above class into numpy arrays\ntrain_tran1 = np.array(train_tran)\ntest_tran1 = np.array(test_tran)\n\n#doing one hot encoding on target values in training set\ny_train_enc = np_utils.to_categorical(training['target'])\ntesting['target'] = \"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_tran1.shape)\nprint(test_tran1.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating a 3 layer simple nn \n\n#initializing \nmodel = Sequential()\n\n#creating a dense layer with 300 input dimensions and giving 400 as output\nmodel.add(Dense(400, input_dim = 300, activation = \"relu\"))\n#adding dropout, this actually improved the accuracy a little bit\nmodel.add(Dropout(0.60))\n#this will normalize the input values\nmodel.add(BatchNormalization())\n\n#adding another layer\nmodel.add(Dense(400, activation ='relu'))\n#again adding a droput and batchnormalization\nmodel.add(Dropout(0.8))\nmodel.add(BatchNormalization())\n\n#dding the output layer with softmax activation\nmodel.add(Dense(2))\nmodel.add(Activation('softmax'))\n\n\n# compile the model\nmodel.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#this will monitor the validity accuracy and will do an earlystop if accuracy doesnt increase much\nearlystop = EarlyStopping(monitor = 'val_accuracy', min_delta = 0, patience = 3, \n                        verbose = 0, mode = 'auto')\n\n#fitting the model on our transformed train data\nmodel.fit(train_tran1, y=y_train_enc,validation_split=0.3, batch_size = 128, epochs = 30, verbose=1, callbacks = [earlystop])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# list all data in history\nprint(model.history.history.keys())\n# summarize history for accuracy\nplt.plot(model.history.history['accuracy'])\nplt.plot(model.history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(model.history.history['loss'])\nplt.plot(model.history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict probabilities for test set\ny_pred_proba = model.predict(test_tran1, verbose=0)\n\n# predicting classes for test set\ny_pred = model.predict_classes(test_tran1, verbose=0)\n\n#predicting classes for our train dataset\nytrain_pred = model.predict_classes(train_tran1)\n\n#confusion matrix for our simple keras model on training set\ncon_mat = confusion_matrix(training['target'], ytrain_pred)\n\n#creating a submission file for our model \nsample1 =pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')\nsample1['target'] = y_pred\nsample1.to_csv(\"SimpleKerasModel.csv\", index = False)\n\n#also creating a classification report for this model\nprint(\"Classification report for training: \")\nprint(classification_report(training['target'], ytrain_pred))\nprint(\"\\n Confusion Matrix for training is: \")\nconfusionMatrix(con_mat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets use keras tokenizer this time\ntoken1 = text.Tokenizer(num_words = None)\nmax_len = 70 \n\n#fitting the tokenizer on training dataset\ntoken1.fit_on_texts(list(training['text']))\n\n#converting the text to sequence of integers on both training and test data\ntrain_seq = token1.texts_to_sequences(training['text'])\ntest_seq = token1.texts_to_sequences(testing['text'])\n\n# zero pad the sequences \ntrain_pad = sequence.pad_sequences(train_seq, maxlen = max_len)\ntest_pad = sequence.pad_sequences(test_seq, maxlen = max_len)\n\n#creating a word index\nword_index1 = token1.word_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating a glove word embedding index(dictionary)\nembeddings_index = {}\nf = open('/kaggle/input/glove42b300dtxt/glove.42B.300d.txt', encoding = 'utf-8')\nfor line in (f):\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype = 'float32')\n    embeddings_index[word] = coefs\n    \nf.close()\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create an embedding matrix for the words we have in the dataset\nembedding_matrix = np.zeros((len(word_index1)+1, 300))\n\n#taking the word from the word index that we created using keras tokenizer\nfor word, i in tqdm(word_index1.items()):\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        #storing it in a dictionary with a value of embedding index of that word\n        embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a LSTM model with glove embeddings\nmodel = Sequential()\n\n#adding an embedding layer with an input dimension of 300 and with wights as the coefficients of golve word index\nmodel.add(Embedding(len(word_index1)+1, 300, weights = [embedding_matrix],\n        input_length = max_len, trainable = False))\n\n#here using spatial dropout. Removing specifing part of element from all channels\nmodel.add(SpatialDropout1D(0.3))\n#adding LSTM layer\nmodel.add(LSTM(300, dropout = 0.3, recurrent_dropout = 0.3))\n\n#adding another Dense layer \nmodel.add(Dense(1024, activation = 'relu'))\nmodel.add(Dropout(0.8))\n#adding another dense layer\nmodel.add(Dense(1024, activation = 'relu'))\nmodel.add(Dropout(0.8))\n\n#and then the output layer\nmodel.add(Dense(2))\nmodel.add(Activation('softmax'))\nmodel.compile(loss= 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n\n#this will monitor the validity accuracy and will do an earlystop if accuracy doesnt increase much\nearlystop = EarlyStopping(monitor = 'val_accuracy', min_delta = 0, patience = 3, \n                        verbose = 0, mode = 'auto')\n\n# Fitting the model with early stopping callback\nmodel.fit(train_pad, y = y_train_enc, batch_size = 32, epochs = 100,\n    verbose = 1, validation_split = 0.2, callbacks = [earlystop])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.history.history.keys()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# list all data in history\nprint(model.history.history.keys())\n# summarize history for accuracy\nplt.plot(model.history.history['accuracy'])\nplt.plot(model.history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(model.history.history['loss'])\nplt.plot(model.history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#predicting probabilities for test set\ny_pred_proba = model.predict(test_pad, verbose=0)\n\n# predicting classes for test set\ny_pred = model.predict_classes(test_pad, verbose=0)\n\n#creating a submission file\nsample1['target'] = y_pred\nsample1.to_csv(\"Lstm_keras.csv\", index = False)\n\n#predicting the classes of training set\nytrain_pred = model.predict_classes(train_pad)\n\n#confusion matrix for train set\ncon_mat = confusion_matrix(training['target'], ytrain_pred)\n\n#also printing classification report for this model\nprint(\"Classification report for training: \")\nprint(classification_report(training['target'], ytrain_pred))\nprint(\"\\n Confusion Matrix for training is: \")\nconfusionMatrix(con_mat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#calculating the fpr, tpr and auc for plotting the roc curve\nfpr_keras, tpr_keras, thresholds_keras = roc_curve(training['target'], ytrain_pred)\nauc_keras = auc(fpr_keras, tpr_keras)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Implementation of BERT","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing a tokenization file from github. actually writing it on my local machine.\n\nimport urllib.request\nurl = 'https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py'\nfilename = 'myfile.py'\nurllib.request.urlretrieve(url, filename)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing the require libraries\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\nimport myfile","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating a bert encoding function \ndef bert_encode(texts, tokenizer, max_len=512):\n    #creating a list for tokens, masks and segments of text\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    #for loop to go through the text\n    for text in texts:\n        text = tokenizer.tokenize(text)   #this will tokanize the text \n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"] #adding cls and sep token to the beginning and end to the text\n        pad_len = max_len - len(input_sequence)       # deciding the padding lenghth\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)   #creating tokens\n        tokens += [0] * pad_len                           #adding a padding to match the length of tokens\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len      #creating a padding for masked tokens\n        segment_ids = [0] * max_len                          \n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#creating a function to build a model\ndef build_model(bert_layer, max_len=512):\n    #taking the input, mask and segment and creating ids\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n    \n    #creating a sequence of output and then building a model\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(clf_output)\n    \n    #initializing and compiling \n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    model.compile(Adam(lr=2e-6), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#downloading bert layer\nmodule_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\nbert_layer = hub.KerasLayer(module_url, trainable=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating a vocab file from bert layer\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = myfile.FullTokenizer(vocab_file, do_lower_case)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testing['text'].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#encoding the text using bert encode function\ntrain_input = bert_encode(training.text.values, tokenizer, max_len=160)\n#encoding the test data using bert encode function\ntest_input = bert_encode(testing.text.values, tokenizer, max_len=160)\n#assigning target labels with target values\ntrain_labels = train.target.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_input)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#building a model\nmodel = build_model(bert_layer, max_len = 160)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#training model with bert layer\ntrain_history = model.fit(\n    train_input, train_labels,\n    validation_split=0.2,\n    epochs=3,\n    batch_size=16\n)\n\nmodel.save('model.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test predicition on unseen data\ntest_pred = model.predict(test_input)\n\n#creating a submission file\nsubmission=pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')\nsubmission['target'] = test_pred.round().astype(int)\nsubmission.to_csv('submission_bert.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#predicting the classes of training set\nytrain_pred = model.predict(train_input)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ytrain_pred = [1 if(i>0.5) else 0 for i in ytrain_pred]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#confusion matrix for train set\ncon_mat = confusion_matrix(train_labels, np.array(ytrain_pred))\n\n#also printing classification report for this model\nprint(\"Accuracy of training: {}\".format(accuracy_score(train_labels, ytrain_pred)))\nprint(\"Classification report for training: \")\nprint(classification_report(train_labels, ytrain_pred))\nprint(\"\\n Confusion Matrix for training is: \")\nconfusionMatrix(con_mat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#calculating the fpr, tpr and auc for plotting the roc curve\nfpr_bert, tpr_bert, thresholds_bert = roc_curve(train_labels, ytrain_pred)\nauc_bert = auc(fpr_bert, tpr_bert)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"auc_count_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr_keras, tpr_keras, label='Keras (area = {:.3f})'.format(auc_keras))\nplt.plot(fpr_count_list[0], tpr_count_list[0], label='CountVectorizer (area = {:.3f})'.format(auc_count_list[0]))\nplt.plot(fpr_tfidf_list[0], tpr_tfidf_list[0], label='TfidfVectorizer (area = {:.3f})'.format(auc_tfidf_list[0]))\nplt.plot(fpr_word_list[0], tpr_word_list[0], label='WordEmbedding (area = {:.3f})'.format(auc_word_list[0]))\nplt.plot(fpr_word_list[1], tpr_word_list[1], label='StackedClassifier (area = {:.3f})'.format(auc_word_list[1]))\nplt.plot(fpr_bert, tpr_bert, label='Bert Algorithm (area = {:.3f})'.format(auc_bert))\n#plt.plot(fpr_rf, tpr_rf, label='RF (area = {:.3f})'.format(auc_rf))\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('ROC curve')\nplt.legend(loc='best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}