{"cells":[{"metadata":{},"cell_type":"markdown","source":"My methodology is as follows:\n 1. Clean the text using my [Tweet Cleaner](https://www.kaggle.com/jdparsons/tweet-cleaner) notebook\n 2. Send the clean text to GPT-2 using my [GPT-2: fake real disasters](https://www.kaggle.com/jdparsons/gpt-2-fake-real-disasters-data-augmentation) notebook. This generates similar tweets with the same label, which doubles the size of my training data.\n 3. In this notebook:\n  * I, um, use USE (Universal Sentence Encoder) to convert the tweets into 512 dimensional vectors\n  * Perform a grid search on many Light GBM parameters to find the best ones\n  * Use K-Fold Cross Validation to train multiple LGB models on different random splits of the data. The prediction function averages the votes into a final answer.\n\nMy best score was 0.82413 with the previously mentioned preprocessing notebooks and the following Light GBM parameters:\n * K-fold DV=5\n * 'feature_fraction': 0.9, 'lambda_l1': 1.0, 'lambda_l2': 0.001, 'learning_rate': 0.05, 'max_depth': -1, 'n_estimators': 128, 'num_leaves': 64,\n\nMy next experiment will be to send my GPT-2 augmented data to Google's AutoML and see how it scores compared to this hand-crafted approach. I'll publish that notebook and update this description when I have the results. **If you have any ideas for improving this notebook, please let me know in the comments!**\n\nCode linted via http://pep8online.com/ and https://yapf.now.sh/ to follow the Google python style guide (mostly)."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport sys\nimport torch\nimport random\nimport numpy as np\nimport pandas as pd\nimport time\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import auc, accuracy_score, roc_auc_score, roc_curve\nfrom sklearn.model_selection import GridSearchCV, KFold\nimport tensorflow_hub as hub\nimport matplotlib.pyplot as plt\n\n# set pandas preview to use full width of browser to see more of the column data\npd.set_option('display.max_columns', None)\npd.set_option('display.expand_frame_repr', False)\npd.set_option('max_colwidth', -1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_files = False\n\nif show_files is True:\n    # helper method to quickly see the file paths of your imported data\n    for dirname, _, filenames in os.walk('/kaggle/input'):\n        for filename in filenames:\n            print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# uncomment these lines to try different types of preprocessed data\n\n#train_file_path = '../input/nlp-getting-started/train.csv'\n#train_file_path = '../input/tweet-cleaner/train_df_clean.csv'\ntrain_file_path = '../input/gpt-2-fake-real-disasters-data-augmentation/train_df_combined.csv'\n\n#test_file_path = '../input/nlp-getting-started/test.csv'\ntest_file_path = '../input/tweet-cleaner/test_df_clean.csv'\n\ntrain_full = pd.read_csv(train_file_path)\ntrain = train_full[['text']]\ntarget = train_full[['target']]\ntest = pd.read_csv(test_file_path)\ntest = test[['id', 'text']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"# from https://www.kaggle.com/denychaen/tweets-simple-baseline-tfembed-lgb\nembed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/3\")\nX_train_embeddings = embed(train['text'].values)\nX_test_embeddings = embed(test['text'].values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Grid Search"},{"metadata":{"trusted":true},"cell_type":"code","source":"do_grid_search = False\n\nif do_grid_search is True:\n    X_train, X_test, y_train, y_test = train_test_split(\n        np.array(X_train_embeddings['outputs']),\n        target,\n        test_size=0.2,\n        random_state=20)\n\n    estimator = lgb.LGBMClassifier()\n\n    # Try your own ranges here, you may find better ones than me!\n    param_grid = {\n        'n_estimators': [64, 200],\n        'num_leaves': [31, 64],\n        'learning_rate': [0.05, 0.1],\n        'feature_fraction': [0.8, 1.0],\n        'max_depth': [-1],\n        'lambda_l1': [0, 1],\n        'lambda_l2': [0, 1],\n    }\n\n    gridsearch = GridSearchCV(\n        estimator, param_grid, refit=True, verbose=0, cv=5)\n\n    gridsearch.fit(\n        X_train,\n        y_train,\n        eval_set=[(X_test, y_test)],\n        eval_metric=['auc', 'binary_logloss'],\n        early_stopping_rounds=10,\n        verbose=0)\n\n    print('Best parameters found by grid search are:', gridsearch.best_params_)\n\n    Y_pred = gridsearch.best_estimator_.predict(X_test)\n\n    print(metrics.classification_report(y_test, Y_pred, digits=3),)\n    print(metrics.confusion_matrix(y_test, Y_pred))\n\n    # if we just guessed the most common class (0) for every prediction\n    print('The null acccuracy is:',\n          max(y_test['target'].mean(), 1 - y_test['target'].mean()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train final model"},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    'objective': 'binary',\n    'feature_fraction': 0.9,\n    'lambda_l1': 1.0,\n    'lambda_l2': 0.001,\n    'learning_rate': 0.05,\n    'max_depth': -1,\n    'n_estimators': 128,\n    'num_leaves': 64,\n    'metric': 'binary_logloss',\n    'random_seed': 42\n}\n\nkf = KFold(n_splits=5, random_state=42)\nmodels = []\n\nX_train = pd.DataFrame(data=np.array(X_train_embeddings['outputs']))\ny_train = target\n\nfor train_index, test_index in kf.split(X_train):\n    train_features = X_train.loc[train_index]\n    train_target = y_train.loc[train_index]\n\n    test_features = X_train.loc[test_index]\n    test_target = y_train.loc[test_index]\n\n    d_training = lgb.Dataset(\n        train_features, label=train_target, free_raw_data=False)\n    d_test = lgb.Dataset(test_features, label=test_target, free_raw_data=False)\n\n    evals_result = {}  # to record eval results for plotting\n\n    model = lgb.train(\n        params,\n        train_set=d_training,\n        num_boost_round=200,\n        valid_sets=[d_training, d_test],\n        verbose_eval=50,\n        early_stopping_rounds=5,\n        evals_result=evals_result)\n\n    # https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/\n    print('Plotting metrics recorded during training...')\n    ax = lgb.plot_metric(evals_result, metric='binary_logloss')\n    plt.show()\n\n    models.append(model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction method"},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://www.kaggle.com/rohanrao/ashrae-half-and-half\n# averages the votes of each model, then returns the majority vote\ndef get_predictions(data):\n\n    results = []\n    for model in models:\n        if results == []:\n            results = model.predict(\n                data, num_iteration=model.best_iteration) / len(models)\n        else:\n            results += model.predict(\n                data, num_iteration=model.best_iteration) / len(models)\n\n    # if the average value is less than .5, then the majority vote was 0, otherwise it was 1\n    results = [int(round(x)) for x in results]\n\n    return results","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Generate submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = get_predictions(X_test_embeddings['outputs'])\n\nssub = pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')\n\nssub[\"target\"] = preds\nssub.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}