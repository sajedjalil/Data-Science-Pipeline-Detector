{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Importing required Libraries.  \n*í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ê°€ì ¸ì˜¤ê¸°*","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom collections import defaultdict\nfrom collections import  Counter\nplt.style.use('ggplot')\nstop=set(stopwords.words('english'))\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam\n\n","metadata":{"execution":{"iopub.status.busy":"2022-05-28T02:18:09.719474Z","iopub.execute_input":"2022-05-28T02:18:09.719772Z","iopub.status.idle":"2022-05-28T02:18:09.730044Z","shell.execute_reply.started":"2022-05-28T02:18:09.719719Z","shell.execute_reply":"2022-05-28T02:18:09.728965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n#os.listdir('../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt')","metadata":{"execution":{"iopub.status.busy":"2022-05-28T02:18:09.759254Z","iopub.execute_input":"2022-05-28T02:18:09.75948Z","iopub.status.idle":"2022-05-28T02:18:09.763054Z","shell.execute_reply.started":"2022-05-28T02:18:09.759437Z","shell.execute_reply":"2022-05-28T02:18:09.762148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading the data and getting basic idea  \n*ë°ì´í„° ë¡œë“œ ë° ê¸°ë³¸ ì•„ì´ë””ì–´ ì–»ê¸°*","metadata":{}},{"cell_type":"code","source":"tweet= pd.read_csv('../input/nlp-getting-started/train.csv')\ntest=pd.read_csv('../input/nlp-getting-started/test.csv')\ntweet.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-05-28T02:18:09.799Z","iopub.execute_input":"2022-05-28T02:18:09.799219Z","iopub.status.idle":"2022-05-28T02:18:09.840398Z","shell.execute_reply.started":"2022-05-28T02:18:09.799177Z","shell.execute_reply":"2022-05-28T02:18:09.839814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('There are {} rows and {} columns in train'.format(tweet.shape[0],tweet.shape[1]))\nprint('There are {} rows and {} columns in train'.format(test.shape[0],test.shape[1]))","metadata":{"execution":{"iopub.status.busy":"2022-05-28T02:18:09.848911Z","iopub.execute_input":"2022-05-28T02:18:09.84913Z","iopub.status.idle":"2022-05-28T02:18:09.854595Z","shell.execute_reply.started":"2022-05-28T02:18:09.849089Z","shell.execute_reply":"2022-05-28T02:18:09.853792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Columns  \n`id` - a unique identifier for each tweet  \n`text` - the text of the tweet    \n`location` - the location the tweet was sent from (may be blank)  \n`keyword` - a particular keyword from the tweet (may be blank)  \n`target` - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)","metadata":{}},{"cell_type":"markdown","source":"## Class distribution  \n*í´ë˜ìŠ¤ ë¶„í¬*","metadata":{}},{"cell_type":"markdown","source":"Before we begin with anything else,let's check the class distribution.There are only two classes 0 and 1.  \n*ë‹¤ë¥¸ ì‘ì—…ì„ ì‹œì‘í•˜ê¸° ì „ì— í´ë˜ìŠ¤ ë¶„í¬ë¥¼ í™•ì¸í•˜ê² ìŠµë‹ˆë‹¤. í´ë˜ìŠ¤ 0ê³¼ 1ë§Œ ë‘ ê°œ ìˆìŠµë‹ˆë‹¤.*","metadata":{}},{"cell_type":"code","source":"x=tweet.target.value_counts()\nsns.barplot(x.index,x)\nplt.gca().set_ylabel('samples')","metadata":{"execution":{"iopub.status.busy":"2022-05-28T02:18:09.904433Z","iopub.execute_input":"2022-05-28T02:18:09.904667Z","iopub.status.idle":"2022-05-28T02:18:10.100644Z","shell.execute_reply.started":"2022-05-28T02:18:09.904623Z","shell.execute_reply":"2022-05-28T02:18:10.099609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are more tweets with class 0 ( No disaster) than class 1 ( disaster tweets).  \n*í´ë˜ìŠ¤ 1(ì¬í•´ íŠ¸ìœ—)ë³´ë‹¤ í´ë˜ìŠ¤ 0(ì¬í•´ ì—†ìŒ)ì˜ íŠ¸ìœ—ì´ ë” ë§ìŠµë‹ˆë‹¤.*","metadata":{}},{"cell_type":"markdown","source":"## Exploratory Data Analysis of tweets  \n*íŠ¸ìœ—ì˜ íƒìƒ‰ì  ë°ì´í„° ë¶„ì„*","metadata":{}},{"cell_type":"markdown","source":"First,we will do very basic analysis,that is character level,word level and sentence level analysis.  \n*ë¨¼ì € ë¬¸ì ìˆ˜ì¤€, ë‹¨ì–´ ìˆ˜ì¤€ ë° ë¬¸ì¥ ìˆ˜ì¤€ ë¶„ì„ì¸ ë§¤ìš° ê¸°ë³¸ì ì¸ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.*","metadata":{}},{"cell_type":"markdown","source":"### Number of characters in tweets  \n*íŠ¸ìœ—ì˜ ë¬¸ì ìˆ˜*","metadata":{}},{"cell_type":"code","source":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len=tweet[tweet['target']==1]['text'].str.len()\nax1.hist(tweet_len,color='red')\nax1.set_title('disaster tweets')\ntweet_len=tweet[tweet['target']==0]['text'].str.len()\nax2.hist(tweet_len,color='green')\nax2.set_title('Not disaster tweets')\nfig.suptitle('Characters in tweets')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-05-28T02:18:10.106034Z","iopub.execute_input":"2022-05-28T02:18:10.106472Z","iopub.status.idle":"2022-05-28T02:18:10.573599Z","shell.execute_reply.started":"2022-05-28T02:18:10.106308Z","shell.execute_reply":"2022-05-28T02:18:10.571485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The distribution of both seems to be almost same.120 t0 140 characters in a tweet are the most common among both.  \n*ë‘˜ì˜ ë¶„í¬ëŠ” ê±°ì˜ ê°™ì€ ê²ƒ ê°™ìŠµë‹ˆë‹¤. 120 ~ 140 íŠ¸ìœ—ì˜ ë¬¸ìê°€ ë‘˜ ì¤‘ ê°€ì¥ ì¼ë°˜ì ì…ë‹ˆë‹¤.*","metadata":{}},{"cell_type":"markdown","source":"### Number of words in a tweet  \n*íŠ¸ìœ—ì˜ ë‹¨ì–´ ìˆ˜*","metadata":{}},{"cell_type":"code","source":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len=tweet[tweet['target']==1]['text'].str.split().map(lambda x: len(x))\nax1.hist(tweet_len,color='red')\nax1.set_title('disaster tweets')\ntweet_len=tweet[tweet['target']==0]['text'].str.split().map(lambda x: len(x))\nax2.hist(tweet_len,color='green')\nax2.set_title('Not disaster tweets')\nfig.suptitle('Words in a tweet')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-05-28T02:18:10.578259Z","iopub.execute_input":"2022-05-28T02:18:10.580583Z","iopub.status.idle":"2022-05-28T02:18:11.057557Z","shell.execute_reply.started":"2022-05-28T02:18:10.580515Z","shell.execute_reply":"2022-05-28T02:18:11.05653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###  Average word length in a tweet  \n\n*íŠ¸ìœ—ì˜ í‰ê·  ë‹¨ì–´ ê¸¸ì´*","metadata":{}},{"cell_type":"code","source":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\nword=tweet[tweet['target']==1]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='red')\nax1.set_title('disaster')\nword=tweet[tweet['target']==0]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='green')\nax2.set_title('Not disaster')\nfig.suptitle('Average word length in each tweet')","metadata":{"execution":{"iopub.status.busy":"2022-05-28T02:18:11.0623Z","iopub.execute_input":"2022-05-28T02:18:11.062894Z","iopub.status.idle":"2022-05-28T02:18:12.303655Z","shell.execute_reply.started":"2022-05-28T02:18:11.062799Z","shell.execute_reply":"2022-05-28T02:18:12.302615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_corpus(target):\n    corpus=[]\n    \n    for x in tweet[tweet['target']==target]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus","metadata":{"execution":{"iopub.status.busy":"2022-05-28T02:18:12.31195Z","iopub.execute_input":"2022-05-28T02:18:12.312711Z","iopub.status.idle":"2022-05-28T02:18:12.321744Z","shell.execute_reply.started":"2022-05-28T02:18:12.312372Z","shell.execute_reply":"2022-05-28T02:18:12.320658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Common stopwords in tweets  \n*íŠ¸ìœ—ì˜ ì¼ë°˜ì ì¸ ë¶ˆìš©ì–´*","metadata":{}},{"cell_type":"markdown","source":"First we  will analyze tweets with class 0.  \n*ë¨¼ì € í´ë˜ìŠ¤ 0ì˜ íŠ¸ìœ—ì„ ë¶„ì„í•©ë‹ˆë‹¤.*","metadata":{}},{"cell_type":"code","source":"corpus=create_corpus(0)\n\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n        \ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] \n","metadata":{"execution":{"iopub.status.busy":"2022-05-28T02:18:12.326861Z","iopub.execute_input":"2022-05-28T02:18:12.327466Z","iopub.status.idle":"2022-05-28T02:18:12.377114Z","shell.execute_reply.started":"2022-05-28T02:18:12.327253Z","shell.execute_reply":"2022-05-28T02:18:12.376314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x,y=zip(*top)\nplt.bar(x,y)","metadata":{"execution":{"iopub.status.busy":"2022-05-28T02:18:12.380311Z","iopub.execute_input":"2022-05-28T02:18:12.380547Z","iopub.status.idle":"2022-05-28T02:18:12.700532Z","shell.execute_reply.started":"2022-05-28T02:18:12.380503Z","shell.execute_reply":"2022-05-28T02:18:12.698751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now,we will analyze tweets with class 1.  \n*ì´ì œ í´ë˜ìŠ¤ 1ë¡œ íŠ¸ìœ—ì„ ë¶„ì„í•˜ê² ìŠµë‹ˆë‹¤.*","metadata":{}},{"cell_type":"code","source":"corpus=create_corpus(1)\n\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n\ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] \n    \n\n\nx,y=zip(*top)\nplt.bar(x,y)","metadata":{"execution":{"iopub.status.busy":"2022-05-28T02:18:12.702187Z","iopub.execute_input":"2022-05-28T02:18:12.702738Z","iopub.status.idle":"2022-05-28T02:18:13.049531Z","shell.execute_reply.started":"2022-05-28T02:18:12.702547Z","shell.execute_reply":"2022-05-28T02:18:13.048522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In both of them,\"the\" dominates which is followed by \"a\" in class 0 and \"in\" in class 1.  \n*ë‘˜ ë‹¤ \"the\"ê°€ ì§€ë°°ì ì´ë©° í´ë˜ìŠ¤ 0ì—ì„œëŠ” \"a\"ê°€, í´ë˜ìŠ¤ 1ì—ì„œëŠ” \"in\"ì´ ë’¤ë”°ë¦…ë‹ˆë‹¤.*","metadata":{}},{"cell_type":"markdown","source":"### Analyzing punctuations.\n*êµ¬ë‘ì  ë¶„ì„.*","metadata":{}},{"cell_type":"markdown","source":"First let's check tweets indicating real disaster.  \n*ë¨¼ì € ì‹¤ì œ ì¬ë‚œì„ ë‚˜íƒ€ë‚´ëŠ” íŠ¸ìœ—ì„ í™•ì¸í•©ì‹œë‹¤.*","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,5))\ncorpus=create_corpus(1)\n\ndic=defaultdict(int)\nimport string\nspecial = string.punctuation\nfor i in (corpus):\n    if i in special:\n        dic[i]+=1\n        \nx,y=zip(*dic.items())\nplt.bar(x,y)","metadata":{"execution":{"iopub.status.busy":"2022-05-28T02:18:13.055037Z","iopub.execute_input":"2022-05-28T02:18:13.055606Z","iopub.status.idle":"2022-05-28T02:18:13.655107Z","shell.execute_reply.started":"2022-05-28T02:18:13.055434Z","shell.execute_reply":"2022-05-28T02:18:13.654242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now,we will move on to class 0.  \n*ì´ì œ í´ë˜ìŠ¤ 0ìœ¼ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.*","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,5))\ncorpus=create_corpus(0)\n\ndic=defaultdict(int)\nimport string\nspecial = string.punctuation\nfor i in (corpus):\n    if i in special:\n        dic[i]+=1\n        \nx,y=zip(*dic.items())\nplt.bar(x,y,color='green')","metadata":{"execution":{"iopub.status.busy":"2022-05-28T02:18:13.656448Z","iopub.execute_input":"2022-05-28T02:18:13.656872Z","iopub.status.idle":"2022-05-28T02:18:14.110796Z","shell.execute_reply.started":"2022-05-28T02:18:13.656728Z","shell.execute_reply":"2022-05-28T02:18:14.109305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Common words  \n*í”í•œ ë‹¨ì–´*","metadata":{}},{"cell_type":"code","source":"\ncounter=Counter(corpus)\nmost=counter.most_common()\nx=[]\ny=[]\nfor word,count in most[:40]:\n    if (word not in stop) :\n        x.append(word)\n        y.append(count)","metadata":{"execution":{"iopub.status.busy":"2022-05-28T02:18:14.112211Z","iopub.execute_input":"2022-05-28T02:18:14.112503Z","iopub.status.idle":"2022-05-28T02:18:14.153058Z","shell.execute_reply.started":"2022-05-28T02:18:14.112452Z","shell.execute_reply":"2022-05-28T02:18:14.152123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.barplot(x=y,y=x)","metadata":{"execution":{"iopub.status.busy":"2022-05-28T02:18:14.15669Z","iopub.execute_input":"2022-05-28T02:18:14.156968Z","iopub.status.idle":"2022-05-28T02:18:14.426722Z","shell.execute_reply.started":"2022-05-28T02:18:14.156921Z","shell.execute_reply":"2022-05-28T02:18:14.425902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lot of cleaning needed.  \në§ì€ ì²­ì†Œê°€ í•„ìš”í•©ë‹ˆë‹¤.","metadata":{}},{"cell_type":"markdown","source":"### Ngram analysis  \n*Ngram ë¶„ì„*","metadata":{}},{"cell_type":"markdown","source":"we will do a bigram (n=2) analysis over the tweets.Let's check the most common bigrams in tweets.  \n*ìš°ë¦¬ëŠ” íŠ¸ìœ—ì— ëŒ€í•´ bigram(n=2) ë¶„ì„ì„ ìˆ˜í–‰í•  ê²ƒì…ë‹ˆë‹¤. íŠ¸ìœ—ì—ì„œ ê°€ì¥ ì¼ë°˜ì ì¸ bigramì„ í™•ì¸í•©ì‹œë‹¤.*","metadata":{}},{"cell_type":"code","source":"def get_top_tweet_bigrams(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","metadata":{"execution":{"iopub.status.busy":"2022-05-28T02:18:14.428018Z","iopub.execute_input":"2022-05-28T02:18:14.42846Z","iopub.status.idle":"2022-05-28T02:18:14.436753Z","shell.execute_reply.started":"2022-05-28T02:18:14.428406Z","shell.execute_reply":"2022-05-28T02:18:14.435878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,5))\ntop_tweet_bigrams=get_top_tweet_bigrams(tweet['text'])[:10]\nx,y=map(list,zip(*top_tweet_bigrams))\nsns.barplot(x=y,y=x)","metadata":{"execution":{"iopub.status.busy":"2022-05-28T02:18:14.438198Z","iopub.execute_input":"2022-05-28T02:18:14.438739Z","iopub.status.idle":"2022-05-28T02:18:15.531168Z","shell.execute_reply.started":"2022-05-28T02:18:14.438689Z","shell.execute_reply":"2022-05-28T02:18:15.530171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will need lot of cleaning here..  \n*ì—¬ê¸° ì²­ì†Œê°€ ë§ì´ í•„ìš”í•©ë‹ˆë‹¤..*","metadata":{}},{"cell_type":"markdown","source":"## Data Cleaning  \n*ë°ì´í„° ì •ë¦¬*\n\nAs we know,twitter tweets always have to be cleaned before we go onto modelling.So we will do some basic cleaning such as spelling correction,removing punctuations,removing html tags and emojis etc.\n\nì•Œë‹¤ì‹œí”¼, íŠ¸ìœ„í„° íŠ¸ìœ—ì€ ëª¨ë¸ë§ì„ ì‹œì‘í•˜ê¸° ì „ì— í•­ìƒ ì •ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ë§ì¶¤ë²• ìˆ˜ì •, êµ¬ë‘ì  ì œê±°, html íƒœê·¸ ë° ì´ëª¨í‹°ì½˜ ì œê±° ë“±ê³¼ ê°™ì€ ê¸°ë³¸ì ì¸ ì •ë¦¬ ì‘ì—…ì„ ìˆ˜í–‰í•˜ê² ìŠµë‹ˆë‹¤.","metadata":{}},{"cell_type":"code","source":"df=pd.concat([tweet,test])\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-28T02:18:15.5358Z","iopub.execute_input":"2022-05-28T02:18:15.536242Z","iopub.status.idle":"2022-05-28T02:18:15.56035Z","shell.execute_reply.started":"2022-05-28T02:18:15.536084Z","shell.execute_reply":"2022-05-28T02:18:15.559188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Removing urls  \n*URL ì œê±°*","metadata":{}},{"cell_type":"code","source":"example=\"New competition launched :https://www.kaggle.com/c/nlp-getting-started\"","metadata":{"execution":{"iopub.status.busy":"2022-05-28T02:18:15.565486Z","iopub.execute_input":"2022-05-28T02:18:15.568405Z","iopub.status.idle":"2022-05-28T02:18:15.574033Z","shell.execute_reply.started":"2022-05-28T02:18:15.565745Z","shell.execute_reply":"2022-05-28T02:18:15.572038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_URL(text):\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\nremove_URL(example)","metadata":{"execution":{"iopub.status.busy":"2022-05-28T02:18:15.575706Z","iopub.execute_input":"2022-05-28T02:18:15.576386Z","iopub.status.idle":"2022-05-28T02:18:15.591482Z","shell.execute_reply.started":"2022-05-28T02:18:15.576093Z","shell.execute_reply":"2022-05-28T02:18:15.590295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text']=df['text'].apply(lambda x : remove_URL(x))","metadata":{"execution":{"iopub.status.busy":"2022-05-28T02:18:15.593691Z","iopub.execute_input":"2022-05-28T02:18:15.594337Z","iopub.status.idle":"2022-05-28T02:18:15.650666Z","shell.execute_reply.started":"2022-05-28T02:18:15.593974Z","shell.execute_reply":"2022-05-28T02:18:15.650045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Removing HTML tags  \n\n*HTML íƒœê·¸ ì œê±°*","metadata":{}},{"cell_type":"code","source":"example = \"\"\"<div>\n<h1>Real or Fake</h1>\n<p>Kaggle </p>\n<a href=\"https://www.kaggle.com/c/nlp-getting-started\">getting started</a>\n</div>\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-05-28T02:18:15.652163Z","iopub.execute_input":"2022-05-28T02:18:15.652646Z","iopub.status.idle":"2022-05-28T02:18:15.65671Z","shell.execute_reply.started":"2022-05-28T02:18:15.652537Z","shell.execute_reply":"2022-05-28T02:18:15.655778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\nprint(remove_html(example))","metadata":{"execution":{"iopub.status.busy":"2022-05-28T02:18:15.658609Z","iopub.execute_input":"2022-05-28T02:18:15.659119Z","iopub.status.idle":"2022-05-28T02:18:15.666237Z","shell.execute_reply.started":"2022-05-28T02:18:15.658875Z","shell.execute_reply":"2022-05-28T02:18:15.665242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text']=df['text'].apply(lambda x : remove_html(x))","metadata":{"execution":{"iopub.status.busy":"2022-05-28T02:18:15.667962Z","iopub.execute_input":"2022-05-28T02:18:15.668416Z","iopub.status.idle":"2022-05-28T02:18:15.694904Z","shell.execute_reply.started":"2022-05-28T02:18:15.668221Z","shell.execute_reply":"2022-05-28T02:18:15.694342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Romoving Emojis  \n\n*ì´ëª¨í‹°ì½˜ ì œê±°*","metadata":{}},{"cell_type":"code","source":"# Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\nremove_emoji(\"Omg another Earthquake ğŸ˜”ğŸ˜”\")","metadata":{"execution":{"iopub.status.busy":"2022-05-28T02:18:15.696458Z","iopub.execute_input":"2022-05-28T02:18:15.696855Z","iopub.status.idle":"2022-05-28T02:18:15.704023Z","shell.execute_reply.started":"2022-05-28T02:18:15.696692Z","shell.execute_reply":"2022-05-28T02:18:15.702905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text']=df['text'].apply(lambda x: remove_emoji(x))\n","metadata":{"execution":{"iopub.status.busy":"2022-05-28T02:18:15.706002Z","iopub.execute_input":"2022-05-28T02:18:15.706512Z","iopub.status.idle":"2022-05-28T02:18:15.790588Z","shell.execute_reply.started":"2022-05-28T02:18:15.706266Z","shell.execute_reply":"2022-05-28T02:18:15.789926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Removing punctuations  \n*êµ¬ë‘ì  ì œê±°*","metadata":{}},{"cell_type":"code","source":"def remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\nexample=\"I am a #king\"\nprint(remove_punct(example))","metadata":{"execution":{"iopub.status.busy":"2022-05-28T02:18:15.792681Z","iopub.execute_input":"2022-05-28T02:18:15.793173Z","iopub.status.idle":"2022-05-28T02:18:15.799159Z","shell.execute_reply.started":"2022-05-28T02:18:15.793126Z","shell.execute_reply":"2022-05-28T02:18:15.798212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text']=df['text'].apply(lambda x : remove_punct(x))","metadata":{"execution":{"iopub.status.busy":"2022-05-28T02:18:15.800902Z","iopub.execute_input":"2022-05-28T02:18:15.801434Z","iopub.status.idle":"2022-05-28T02:18:15.865616Z","shell.execute_reply.started":"2022-05-28T02:18:15.801135Z","shell.execute_reply":"2022-05-28T02:18:15.864941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Spelling Correction  \n*ë§ì¶¤ë²• ìˆ˜ì •*\n","metadata":{}},{"cell_type":"markdown","source":"Even if I'm not good at spelling I can correct it with python, I will use `pyspellcheker` to do that.  \n*ì² ìë¥¼ ì˜ ëª»ì¨ë„ pythonìœ¼ë¡œ ê³ ì¹  ìˆ˜ ìˆì–´ìš”, 'pyspellcheker'ë¥¼ ì‚¬ìš©í•´ì„œ ìˆ˜ì •í•˜ê² ìŠµë‹ˆë‹¤*","metadata":{}},{"cell_type":"markdown","source":"### `pyspellcheker`  \nPure Python Spell Checking based on Peter Norvigâ€™s blog post on setting up a simple spell checking algorithm.  \n*ê°„ë‹¨í•œ ë§ì¶¤ë²• ê²€ì‚¬ ì•Œê³ ë¦¬ì¦˜ ì„¤ì •ì— ëŒ€í•œ Peter Norvigì˜ ë¸”ë¡œê·¸ ê²Œì‹œë¬¼ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” ìˆœìˆ˜ Python ë§ì¶¤ë²• ê²€ì‚¬ì…ë‹ˆë‹¤.*\n\nIt uses a Levenshtein Distance algorithm to find permutations within an edit distance of 2 from the original word. It then compares all permutations (insertions, deletions, replacements, and transpositions) to known words in a word frequency list. Those words that are found more often in the frequency list are more likely the correct results.\n\n*Levenshtein Distance ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ì›ë˜ ë‹¨ì–´ì—ì„œ 2 í¸ì§‘ ê±°ë¦¬ ë‚´ì—ì„œ ìˆœì—´ì„ ì°¾ìŠµë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ ëª¨ë“  ìˆœì—´(ì‚½ì…, ì‚­ì œ, ëŒ€ì²´ ë° ì „ì¹˜)ì„ ë‹¨ì–´ ë¹ˆë„ ëª©ë¡ì˜ ì•Œë ¤ì§„ ë‹¨ì–´ì™€ ë¹„êµí•©ë‹ˆë‹¤. ë¹ˆë„ ëª©ë¡ì—ì„œ ë” ìì£¼ ë°œê²¬ë˜ëŠ” ë‹¨ì–´ê°€ ì˜¬ë°”ë¥¸ ê²°ê³¼ì¼ ê°€ëŠ¥ì„±ì´ ë” í½ë‹ˆë‹¤.*  \n\n*(https://pypi.org/project/pyspellchecker/)*","metadata":{}},{"cell_type":"code","source":"!pip install pyspellchecker","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-28T02:18:15.867038Z","iopub.execute_input":"2022-05-28T02:18:15.867577Z","iopub.status.idle":"2022-05-28T02:18:20.805814Z","shell.execute_reply.started":"2022-05-28T02:18:15.867283Z","shell.execute_reply":"2022-05-28T02:18:20.805017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from spellchecker import SpellChecker\n\nspell = SpellChecker()\ndef correct_spellings(text):\n    corrected_text = []\n    misspelled_words = spell.unknown(text.split())\n    for word in text.split():\n        if word in misspelled_words:\n            corrected_text.append(spell.correction(word))\n        else:\n            corrected_text.append(word)\n    return \" \".join(corrected_text)\n        \ntext = \"corect me plese\"\ncorrect_spellings(text)","metadata":{"execution":{"iopub.status.busy":"2022-05-28T02:18:20.809116Z","iopub.execute_input":"2022-05-28T02:18:20.809397Z","iopub.status.idle":"2022-05-28T02:18:20.95507Z","shell.execute_reply.started":"2022-05-28T02:18:20.809341Z","shell.execute_reply":"2022-05-28T02:18:20.953959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df['text']=df['text'].apply(lambda x : correct_spellings(x)#)","metadata":{"execution":{"iopub.status.busy":"2022-05-28T02:18:20.956509Z","iopub.execute_input":"2022-05-28T02:18:20.956945Z","iopub.status.idle":"2022-05-28T02:18:20.960885Z","shell.execute_reply.started":"2022-05-28T02:18:20.956756Z","shell.execute_reply":"2022-05-28T02:18:20.959937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## GloVe for Vectorization  \n\n*ë²¡í„°í™”ë¥¼ ìœ„í•œ GloV*","metadata":{}},{"cell_type":"markdown","source":"### `GloVe` : Global Vectors for Word Representation  \nGloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.  \n*GloVeëŠ” ë‹¨ì–´ì— ëŒ€í•œ ë²¡í„° í‘œí˜„ì„ ì–»ê¸° ìœ„í•œ ë¹„ì§€ë„ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤. í›ˆë ¨ì€ ë§ë­‰ì¹˜ì—ì„œ ì§‘ê³„ëœ ì „ì—­ ë‹¨ì–´ ë‹¨ì–´ ë™ì‹œ ë°œìƒ í†µê³„ì— ëŒ€í•´ ìˆ˜í–‰ë˜ë©° ê²°ê³¼ í‘œí˜„ì€ ë‹¨ì–´ ë²¡í„° ê³µê°„ì˜ í¥ë¯¸ë¡œìš´ ì„ í˜• í•˜ìœ„ êµ¬ì¡°ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.*  \n*(https://nlp.stanford.edu/pubs/glove.pdf)*","metadata":{}},{"cell_type":"markdown","source":"Here we will use GloVe pretrained corpus model to represent our words.It is available in 3 varieties :50D ,100D and 200 Dimentional.We will try 100 D here.  \n*ì—¬ê¸°ì—ì„œëŠ” GloVe ì‚¬ì „ í›ˆë ¨ëœ ë§ë­‰ì¹˜ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë‹¨ì–´ë¥¼ í‘œí˜„í•©ë‹ˆë‹¤. 50D, 100D ë° 200 Dimentionalì˜ 3ê°€ì§€ ì¢…ë¥˜ê°€ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” 100Dë¥¼ ì‹œë„í•˜ê² ìŠµë‹ˆë‹¤.*","metadata":{}},{"cell_type":"code","source":"\ndef create_corpus(df):\n    corpus=[]\n    for tweet in tqdm(df['text']):\n        words=[word.lower() for word in word_tokenize(tweet) if((word.isalpha()==1) & (word not in stop))]\n        corpus.append(words)\n    return corpus\n        \n        ","metadata":{"execution":{"iopub.status.busy":"2022-05-28T02:18:20.962493Z","iopub.execute_input":"2022-05-28T02:18:20.963019Z","iopub.status.idle":"2022-05-28T02:18:20.972137Z","shell.execute_reply.started":"2022-05-28T02:18:20.962926Z","shell.execute_reply":"2022-05-28T02:18:20.971018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corpus=create_corpus(df)","metadata":{"execution":{"iopub.status.busy":"2022-05-28T02:18:20.973653Z","iopub.execute_input":"2022-05-28T02:18:20.974258Z","iopub.status.idle":"2022-05-28T02:18:23.486944Z","shell.execute_reply.started":"2022-05-28T02:18:20.974136Z","shell.execute_reply":"2022-05-28T02:18:23.486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_dict={}\nwith open('../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt','r') as f:\n    for line in f:\n        values=line.split()\n        word=values[0]\n        vectors=np.asarray(values[1:],'float32')\n        embedding_dict[word]=vectors\nf.close()","metadata":{"execution":{"iopub.status.busy":"2022-05-28T02:18:23.488214Z","iopub.execute_input":"2022-05-28T02:18:23.488517Z","iopub.status.idle":"2022-05-28T02:18:41.376663Z","shell.execute_reply.started":"2022-05-28T02:18:23.488471Z","shell.execute_reply":"2022-05-28T02:18:41.375929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_LEN=50\ntokenizer_obj=Tokenizer()\ntokenizer_obj.fit_on_texts(corpus)\nsequences=tokenizer_obj.texts_to_sequences(corpus)\n\ntweet_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')","metadata":{"execution":{"iopub.status.busy":"2022-05-28T02:18:41.381689Z","iopub.execute_input":"2022-05-28T02:18:41.381943Z","iopub.status.idle":"2022-05-28T02:18:41.654363Z","shell.execute_reply.started":"2022-05-28T02:18:41.381898Z","shell.execute_reply":"2022-05-28T02:18:41.653652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_index=tokenizer_obj.word_index\nprint('Number of unique words:',len(word_index))","metadata":{"execution":{"iopub.status.busy":"2022-05-28T02:18:41.656941Z","iopub.execute_input":"2022-05-28T02:18:41.657392Z","iopub.status.idle":"2022-05-28T02:18:41.663836Z","shell.execute_reply.started":"2022-05-28T02:18:41.65734Z","shell.execute_reply":"2022-05-28T02:18:41.663166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_words=len(word_index)+1\nembedding_matrix=np.zeros((num_words,100))\n\nfor word,i in tqdm(word_index.items()):\n    if i > num_words:\n        continue\n    \n    emb_vec=embedding_dict.get(word)\n    if emb_vec is not None:\n        embedding_matrix[i]=emb_vec\n            ","metadata":{"execution":{"iopub.status.busy":"2022-05-28T02:18:41.665469Z","iopub.execute_input":"2022-05-28T02:18:41.666383Z","iopub.status.idle":"2022-05-28T02:18:41.736617Z","shell.execute_reply.started":"2022-05-28T02:18:41.666342Z","shell.execute_reply":"2022-05-28T02:18:41.735815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Baseline Model  \n*ê¸°ì¤€ ëª¨ë¸*","metadata":{}},{"cell_type":"code","source":"model=Sequential()\n\nembedding=Embedding(num_words,100,embeddings_initializer=Constant(embedding_matrix),\n                   input_length=MAX_LEN,trainable=False)\n\nmodel.add(embedding)\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation='sigmoid'))\n\n\noptimzer=Adam(learning_rate=1e-5)\n\nmodel.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])\n\n","metadata":{"execution":{"iopub.status.busy":"2022-05-28T02:18:41.737962Z","iopub.execute_input":"2022-05-28T02:18:41.738393Z","iopub.status.idle":"2022-05-28T02:18:42.03689Z","shell.execute_reply.started":"2022-05-28T02:18:41.73821Z","shell.execute_reply":"2022-05-28T02:18:42.036148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-05-28T02:18:42.038223Z","iopub.execute_input":"2022-05-28T02:18:42.038536Z","iopub.status.idle":"2022-05-28T02:18:42.046532Z","shell.execute_reply.started":"2022-05-28T02:18:42.03849Z","shell.execute_reply":"2022-05-28T02:18:42.045549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train=tweet_pad[:tweet.shape[0]]\ntest=tweet_pad[tweet.shape[0]:]","metadata":{"execution":{"iopub.status.busy":"2022-05-28T02:18:42.047968Z","iopub.execute_input":"2022-05-28T02:18:42.048456Z","iopub.status.idle":"2022-05-28T02:18:42.055165Z","shell.execute_reply.started":"2022-05-28T02:18:42.048402Z","shell.execute_reply":"2022-05-28T02:18:42.054144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train,X_test,y_train,y_test=train_test_split(train,tweet['target'].values,test_size=0.15)\nprint('Shape of train',X_train.shape)\nprint(\"Shape of Validation \",X_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-28T02:18:42.056695Z","iopub.execute_input":"2022-05-28T02:18:42.057272Z","iopub.status.idle":"2022-05-28T02:18:42.067881Z","shell.execute_reply.started":"2022-05-28T02:18:42.057223Z","shell.execute_reply":"2022-05-28T02:18:42.067179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}