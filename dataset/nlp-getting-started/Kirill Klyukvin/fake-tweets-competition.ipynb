{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Hallo my beautiful Kagglers!\n\nWelcome back to my notebooks. \nAfter getting into the top 13% at the Digit Recognizing competition, I would like to improve my NLP skills.  \nAs usual, I start to learn the topic from scratch. From basic sklearn models to neural networks like BERT.  \n\nIf you have any comments or suggestions, please, don't hesitate to put them in the comments section below!\n\nHere we go!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"*NB*  \n\nI rerun this notebook often, so I've just turn some rows of code into *#this form*.  \nIf you want, please, remove the 'sharp' characters and run the whole notebook.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Libs and data import","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nfrom numpy import savetxt \nimport pandas as pd \nimport re\nimport gc\nimport random\nimport os\nimport tensorflow as tf\n\nimport torch\nimport transformers\nimport spacy\nfrom spacy.lemmatizer import Lemmatizer\nfrom spacy.lookups import Lookups\nfrom spacy.lang.en.stop_words import STOP_WORDS\nimport codecs\nfrom gensim.models import Word2Vec\n\nimport nltk\nfrom nltk.corpus import stopwords as nltk_stopwords\nfrom gensim.models import Word2Vec\nfrom tqdm import notebook\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\nwarnings.simplefilter('ignore')\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\nfrom sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, classification_report, precision_recall_curve\nfrom sklearn.metrics import plot_confusion_matrix, make_scorer\nfrom sklearn.model_selection import train_test_split, cross_validate, cross_val_score, GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV, KFold, StratifiedShuffleSplit\nfrom sklearn.neighbors import DistanceMetric\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, OrdinalEncoder, LabelEncoder, Binarizer, OneHotEncoder\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import SGDClassifier, LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC, NuSVC\nfrom sklearn.gaussian_process.kernels import RBF\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier, Pool, cv\nimport lightgbm as lgb  \n\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow import keras\nfrom tensorflow.keras.optimizers import SGD, Adam\nfrom tensorflow.keras.layers import Dense, Input, Dropout, GlobalAveragePooling1D\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, Callback","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.cuda.is_available()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\nsamp_sub = pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Set the seed","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"RND_ST = 2202","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dive into the data. \n\n# Data analysys","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"OMG, we have missing values!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['keyword'].unique()[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['keyword'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"FATALITY. FLAWLESS VICTORY.  \nSorry. \n\nSeems like we can use keywords for prediction. First, I would like to make a classification with the tweets text only.  \n\nLet's see, what topic consisnts more fake tweets. \n\nHow many different topics do we have?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train['keyword'].unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Is that real, 222 unique value without any duplicates?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.groupby('keyword')['keyword'].count().head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At least, *bioterror/bioterrorism*, *annihilated/annihilation* and *blaze/blazing* mean the same. What if we apply lemmatization for this column? How much can we decrease the number of unique values?  \nWe also need to remove '%20' symbols.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def space_code_removing(df):\n    \n    for i in range(df.shape[0]):\n        df.loc[i, 'keyword'] = re.sub(r'%20', ' ', str(df.loc[i, 'keyword']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"space_code_removing(train)\nspace_code_removing(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['keyword'].unique()[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def lemmatize(df):\n    \n    lemmatizer = spacy.load('en_core_web_sm')\n    \n    for i in range(df.shape[0]):\n        lemma = lemmatizer(str(df.loc[i, 'keyword']))\n        df.loc[i, 'keyword_lemma'] = \" \".join([token.lemma_ for token in lemma])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To increase iteration time, I will create a table grouped by keywords. Then I will lemmatize 222 rows instead of ~7600.  \nThis dataframe will consist of the numbers of tweets of each topic. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"topics = (train.groupby('keyword')['target']\n        .agg(['count','sum'])\n        .reset_index()\n        .sort_values(by='count', ascending=False))\n        \ntopics['fake'] = topics['count'] - topics['sum']\n        \ntopics.rename(columns={'count':'total', 'sum':'true'}, inplace=True)\n\ntopics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lemmatize(topics)\n\nlen(topics['keyword_lemma'].unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seems like we didn't remove all repeated topics, but reduce the number of unique values from 222 to 186.  ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"topics.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Bags have been transformed into Bag. Good job!  \n\nDrop the keyword columns, regroup the topics and add percentage of Real and Fake tweets.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"topics = topics.drop('keyword', axis=1)\n\ntopics = topics.groupby('keyword_lemma')[['total','true','fake']].sum().reset_index()\n\ntopics['true_prcntg'] = (topics['true'] * 100 / topics['total']).round(2)\ntopics['fake_prcntg'] = (100 - topics['true_prcntg'])\n\ntopics","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we have a nice and descriptive table.   \nLet's find the top 10 true and fake topics.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"real_topics = topics[['keyword_lemma','true_prcntg','total']].sort_values(by='true_prcntg', ascending=False).head(10)\nreal_topics","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"People never lie about the wreckage, derailment and debris. Also, they tell the truth about outbreaks, typhoons and oil spills.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fake_topics = topics[['keyword_lemma','fake_prcntg','total']].sort_values(by='fake_prcntg', ascending=False).head(10)\nfake_topics","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Aftershocks, ruins and body bags are the most fake topics in Twitter.  \n\nAdd some visualisation to our chart.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_style('whitegrid')\n\nfig, axes = plt.subplots(1, 2, figsize=(17,5))\n\nsns.barplot(x='true_prcntg', y='keyword_lemma', data=real_topics, color='royalblue', ax=axes[0])\nsns.barplot(x='fake_prcntg', y='keyword_lemma', data=fake_topics, color='salmon', ax=axes[1])\n\naxes[1].set_ylabel('')\naxes[0].set_xlabel('Real news percentage')\naxes[1].set_xlabel('Fake news percentage')\n\nplt.suptitle('Top 10 Real and Fake news topics in train dataset', size=18);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Which topics are the most controversional?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cntrv_topics = topics[['keyword_lemma','true_prcntg']].query('48 <= true_prcntg <= 52').sort_values(by='true_prcntg', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cntrv_topics","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For somebody, storms and open wounds are the real disasters. Other people don't afraid it. \n\nWhat about balance between fake and real news?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = pd.DataFrame(train['target'].value_counts())\nname = pd.Series(['real', 'fake'], name='name')\ntemp = temp.join(name)\ntemp\n\nfig, ax = plt.subplots(figsize=(6,6))\nax.vlines(x=temp.name, ymin=0, ymax=temp['target'], color='dimgrey', alpha=0.85, linewidth=2)\nax.scatter(x=temp.name, y=temp['target'], s=75, color='firebrick', alpha=0.85)\n\nfor row in temp.itertuples():\n    ax.text(row.Index, row.target+100, s=row.target, \n            horizontalalignment= 'center', verticalalignment='bottom', fontsize=10)\n\nax.set_title('Fake and real tweets in train dataset', size=15, y=(1.02))\nax.set_ylabel('Tweets')\nax.set_ylim(0, 5000)\n\n#plt.tight_layout\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not great, not terrible. First evaluation we will try to make with the currant statement.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Cleaning and lemmatization\n\nOk, let's start text processing.\n\n*13.08.20 upd - more symbols and word were replaced.*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(50):\n    print(train.loc[i, 'text'])\n    print()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are special symbols (#), Capital Letters, dots and commas. Time to remove them all.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def text_processing(df):\n    \n    text = df['text'].values\n    \n    documents = []\n\n    lemmatizer = spacy.load('en_core_web_sm')\n    \n    df_new = df.copy()\n\n    for sen in range(0, len(text)):\n        # remove special symbols\n        document = re.sub(r'\\W', ' ', str(text[sen]))\n    \n        # remove individual symbols\n        document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n    \n        # remove individual symbols from the start of the tweet\n        document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n    \n        # replace few spaces to a single one\n        document = re.sub(r'\\s+', ' ', document, flags=re.I)\n    \n        # remove 'b'\n        document = re.sub(r'^b\\s+', '', document)\n    \n        # convert all letters to a lower case\n        document = document.lower()\n    \n        # spacy lemmarization\n        lemma = lemmatizer(str(document))\n        document = \" \".join([token.lemma_ for token in lemma])\n        \n        # remove spacy pronouns lemmas\n        #document = re.sub(r'-PRON-', '', document)\n        \n        df_new.loc[sen, 'text_lemm'] = document\n        \n    return df_new","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def text_processing_02(df):\n    \n    text = df['text'].values\n    \n    df_new = df.copy()\n\n    for sen in range(0, len(text)):\n      \n        ## removing part\n        \n        # remove hyperlinks\n        document = re.sub(r'http\\S+', '', str(text[sen]))\n        # remove hashtags\n        document = re.sub(r'#\\S+', ' ', document)\n        # remove special symbols\n        document = re.sub(r'\\W', ' ', document)\n        # remove individual symbols\n        document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n        # remove individual symbols from the start of the tweet\n        document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n        # replace few spaces to a single one\n        document = re.sub(r'\\s+', ' ', document, flags=re.I)\n        # remove 'b'\n        document = re.sub(r'^b\\s+', '', document)\n        #remove 没贸\n        document = re.sub(r'没贸', '', document)\n        \n        # convert all letters to a lower case\n        document = document.lower()\n        \n        ## replacing part\n        document = re.sub(r'hwy', 'highway', document)\n        document = re.sub(r'nsfw', 'not safe for work', document)\n        \n        \n        df_new.loc[sen, 'text_lemm'] = document\n        \n    return df_new","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking the function on three samples.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = train.iloc[46:49].reset_index()\n\ntemp_prep = text_processing_02(temp)\n\ntemp_prep","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Nice, it works.  \nLemmatize tweets for test and train datasets.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntrain_lemm = text_processing_02(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(50):\n    print(train_lemm.loc[i, 'text_lemm'])\n    print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntest_lemm = text_processing_02(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Drop all necessary columns. Split datasets into features and target.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_lemm = train_lemm.drop(['id','keyword','location','text'], axis=1)\ntest_lemm = test_lemm.drop(['id','keyword','location','text'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train_lemm['text_lemm']\ny_train = train_lemm['target']\n\nX_test = test_lemm['text_lemm']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Text vectorization \n\nWe will use tf-idf vectorization for the first attempt.  \nAnd keep only unigrams (default ngram_range parameter).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#tfidfconverter = TfidfVectorizer(max_features=1000, \n                                 #min_df=3, max_df=0.5, \n                                 #ngram_range=(1,1),\n                                 #stop_words=STOP_WORDS)\n\n#tfidfconverter.fit(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X_train_tf = tfidfconverter.transform(X_train)\n#X_test_tf = tfidfconverter.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Models are ready for learning!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# First attempt of model selection","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Start with simple sklearn models.   \nAs usual create a function for a GridSearch and Cross validation score first. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def grid_search(model, params, features, target):\n    \n    search = GridSearchCV(model, params, verbose=1, cv=3, scoring='f1', n_jobs=-1)\n    search.fit(features, target)\n    \n    print(search.best_score_)\n    print(search.best_params_)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cross_val(model, feat, target):\n    \n    cvs = cross_val_score(model, feat, target, cv=5, scoring='f1').mean()\n    \n    return(cvs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SGD Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sgd = SGDClassifier(random_state=RND_ST)\n\nsgd_params = dict(alpha=[1e-03, 1e-04, 1e-05, 1e-06],\n                  penalty=['l1','l2'], \n                  tol=[1e-03, 1e-04, 1e-05])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#grid_search(sgd, sgd_params, X_train_tf, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sgd = SGDClassifier(alpha=0.0001, penalty='l2', tol=0.001,  random_state=RND_ST)#","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#cross_val(sgd, X_train_tf, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not so good. Anyway, let's make the first submission.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"*This approach gave me 0.79313 point on the leaderboard*","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### LinearSVM","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"svm = LinearSVC(random_state = RND_ST)\n\nsvm_params = dict(C=[0.01,0.1,1,10,100],\n                  max_iter=[100,250,500,1000,2000])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#grid_search(svm, svm_params, X_train_tf, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#cross_val(svm, X_train_tf, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svm = LinearSVC(C=1, max_iter=1000, random_state=RND_ST)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*0.78884 - submission score. Worse than a SGD*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# BERT text preprocessing","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Try to apply DistillBERT. \n\nThis neural model is a light version of BERT. It requires less time and CPU resources.  \n\nStart with making train and test datasets for our model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_bert = train[['target','text']]\ntest_bert = test[['text']]\n\ny_train = train['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Downloading model and tokenizer\n\nmodel_class, tokenizer_class, pretrained_weights = (\n    transformers.DistilBertModel, transformers.DistilBertTokenizer, 'distilbert-base-uncased')\n\ntokenizer = tokenizer_class.from_pretrained(pretrained_weights)\nmodel = model_class.from_pretrained(pretrained_weights)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's tokenize the tweets.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#tokenized_train = train_bert['text'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n#tokenized_test = test_bert['text'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n\ntokenized_train = X_train.apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\ntokenized_test = X_test.apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Make attention mask, embbeddings and updated features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def bert_features(tokenized):\n\n    max_len = 0\n    for i in tokenized.values:\n        if len(i) > max_len:\n            max_len = len(i)\n\n    padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n    \n    attention_mask = np.where(padded != 0, 1, 0)\n    \n    batch_size = 1\n    embeddings = []\n    for i in notebook.tqdm(range(padded.shape[0] // batch_size)):\n            batch = torch.LongTensor(padded[batch_size*i:batch_size*(i+1)]) \n            attention_mask_batch = torch.LongTensor(attention_mask[batch_size*i:batch_size*(i+1)])\n        \n            with torch.no_grad():\n                batch_embeddings = model(batch, attention_mask=attention_mask_batch)\n        \n            embeddings.append(batch_embeddings[0][:,0,:].numpy())\n    \n    features = np.concatenate(embeddings)\n    \n    return(features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_bert = bert_features(tokenized_train) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_bert = bert_features(tokenized_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del tokenized_train, tokenized_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from numpy import savetxt \n#savetxt('/kaggle/working/X_train_bert.csv', X_train_bert, delimiter=',')\n#savetxt('/kaggle/working/X_test_bert.csv', X_test_bert, delimiter=',')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*I have preprocessed the text and just load it to increase re-iteration time.*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_bert = np.loadtxt('/kaggle/input/distilbert-preprocessed/X_train_bert.csv', delimiter=',')\nX_test_bert = np.loadtxt('/kaggle/input/distilbert-preprocessed/X_test_bert.csv', delimiter=',')\n\ny_train = train['target']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### BERT preprocessed model selection","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Start with simple logistic regression.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression(random_state=RND_ST)\n\nlr_params = {'C': np.linspace(0.0001, 100, 20),\n             'max_iter':[50,100,200,500]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_search(lr, lr_params, X_train_bert, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"0.7613775148606625  \n{'C': 15.789557894736841, 'max_iter': 50}","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_final = LogisticRegression(C=15.789557894736841, max_iter=50, random_state=RND_ST)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Best score on the test dataset  \n0.80815*","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Continue with CatBoost by Yandex.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We will make a special cross validation function for catboost classifier.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def cat_classifier(features, target):\n    \n    data = Pool(data = features, \n            label = target)\n    \n    scores = cv(data,\n            cbc_params,\n            fold_count=3, \n            plot=\"False\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cbc_params = dict(loss_function='Logloss',\n                    iterations=300,\n                    learning_rate=0.07,\n                    depth=4,\n                    subsample=0.7,\n                    verbose=100, \n                    random_state=RND_ST)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_classifier(X_train_bert, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### learn: 0.1747454\ttest: 0.4294865\tbest: 0.4257702 (218)\ttotal: 1m 39s\tremaining: 0us\n\n### learn: 0.2388240\ttest: 0.4301056\tbest: 0.4288780 (350)\ttotal: 1m 36s\tremaining: 0us\n\n### learn: 0.2695160\ttest: 0.4249459\tbest: 0.4249238 (295)\ttotal: 1m 26s\tremaining: 0us","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cbc = CatBoostClassifier(loss_function='Logloss',\n                    iterations=400,\n                    learning_rate=0.09,\n                    depth=4,\n                    subsample=0.8,\n                    verbose=100, \n                    random_state=RND_ST)\n\n#cross_val(cbc, X_train_bert, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*This approach gave me the highest position (best score = 0.81550) on the leadearbord.*","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### lightGBM","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_lgb = lgb.Dataset(X_train_bert, label=y_train, free_raw_data=False)\n\nlgb_param = {'num_leaves': 70, \n         'objective':'binary',\n         'min_data_in_leaf':23,\n         'max_depth':4,\n         'learning_rate':0.1,\n         'num_iterations':96,\n         'max_bin':3000,\n         'verbosity':0,\n         #'min_split_gain':90,\n         'random_state':RND_ST\n        }\n\n#NUM_ROUNDS = 500","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_history = lgb.cv(params=lgb_param, \n                     train_set=train_lgb, \n                     metrics='cross_entropy', \n                     early_stopping_rounds=5)\n\nlen(lgb_history['cross_entropy-mean'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_history['cross_entropy-mean'][-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### best 0.4277043669978033","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## NN is the future \n\nToday I will use keras for build a simple NN. I'll use torch a bit later.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\n\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam, SGD, RMSprop\n\nfrom keras.callbacks import EarlyStopping","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_hist(history):\n\n    plt.plot(history.history['acc'])\n    plt.plot(history.history['val_acc'])\n    plt.title('model accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.grid()\n    plt.show()\n\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.grid()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = Adam(lr=0.0001)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = SGD(lr=0.0001)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_bert.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    del model\n    print('refined')\nexcept:\n    print('next')\n\nmodel = Sequential()\n\nmodel.add(Dense(50, input_dim=768, activation='relu', kernel_initializer='lecun_uniform'))\nmodel.add(Dense(50, activation='relu', kernel_initializer='lecun_uniform'))\n\nmodel.add(Dense(1, activation='sigmoid', kernel_initializer='lecun_uniform'))\n\nmodel.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['acc'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_train_bert, y_train, epochs=1500, validation_split=0.1, batch_size=300, verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_hist(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submisson","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def submission(model, train, target, test):\n    \n    model.fit(train, target)\n    \n    pred = model.predict(test)\n    \n    submission = samp_sub.copy()\n    submission['target'] = pred\n    \n    submission.to_csv('/kaggle/working/cbcbert_15.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission(cbc, X_train_bert, y_train, X_test_bert)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Make a special function for LightGBM.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def submission_lgb(params, data, test_features):\n    \n    lgbm = lgb.train(params, data)\n    pred = lgbm.predict(test_features).round().astype('int')\n    \n    submission = samp_sub.copy()\n    submission['target'] = pred\n    \n    submission.to_csv('/kaggle/working/lgbm_02.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_lgb(lgb_param, train_lgb, X_test_bert)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_lgb","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Make a special function for RNN.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def nn_pred(model, X_test):\n\n    prediction_nn = model.predict(X_test).round().astype('int')\n    submission = samp_sub.copy()\n    submission['target'] = prediction_nn\n    \n    submission.to_csv('/kaggle/working/nn_003.csv', index=False)\n\nnn_pred(model, X_test_bert)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Scoreboard \n\n\n0.81550 / Rank 394 / cbc with bert preprocessing  \n0.81121 / Rank 426 / svm_bert with distill bert preprocessing    \n0.80478 / Rank 498 / sgd_bert with distill bert preprocessing  \n0.79313 / Rank 870 / sgd model  ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Keep moving**","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}