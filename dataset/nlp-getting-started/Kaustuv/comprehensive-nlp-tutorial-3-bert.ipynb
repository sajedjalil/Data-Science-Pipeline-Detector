{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This is part 3 of *Comprehensive NLP Tutorial* series. In [Part-1](https://www.kaggle.com/kksienc/comprehensive-nlp-tutorial-1-ml-perspective) we performed text processing using ML techniques and in [Part-2](https://www.kaggle.com/kksienc/comprehensive-nlp-tutorial-2-dl-perspective) we used Word Embedding and Deep Learning algorithms. In this part we will look into state-of-the-art 'BERT' technique.","metadata":{}},{"cell_type":"markdown","source":"<a class=\"kk\" id=\"0.1\"></a>\n## Contents\n\n1. [BERT Introduction](#1)\n1. [Salient BERT Features](#2)\n1. [BERT Implementation](#3)\n    1. [BERT Pretrained Layer](#3.1)\n    1. [BERT Encoding](#3.2)\n        1. [Encoding Sample Example](#3.2.1)\n        1. [Encoding The Dataset](#3.2.2)\n    1. [BERT Modeling](#3.3)\n        \n ","metadata":{}},{"cell_type":"markdown","source":"# 1. BERT  Introduction  <a class=\"kk\" id=\"1\"></a>\n[Back to Contents](#0.1)\n\n[BERT](https://github.com/google-research/bert) stands for <B> Bidirectional Encoder Representations from Transformers </B>. It was created and published in 2018 by Jacob Devlin and his colleagues from Google. BERT [paper](https://arxiv.org/pdf/1810.04805.pdf) depicts that a directionally trained language model can have a deeper sense of language context and flow than single-direction language models. Since creation BERT is a go to model for NLP and has inspired many NLP architecture such as RoBERTa, XLNet etc.\n\n","metadata":{}},{"cell_type":"code","source":"from IPython.display import Image\nImage(\"/kaggle/input/images/bert.png\",  width=350)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" Lets understand BERT by breaking BERT abbreviation,\n\n- <B>Bidirectional</B> :  BERT takes whole text passage as input and reads passage in both direction to understand the meaning of each word.\n\n\n- <B>Transformers</B> :  BERT is based on a Deep Transformer network. Transformer network is a type of network that can process efficiently long texts by using attention. An attention is a mechanism to learn contextual relations between words (or sub-words) in a text. \n\n\n- <B>Encoder Representation</B> : Originally, Transformer includes two separate mechanisms — an encoder that reads the text input and a decoder that produces a prediction for the task, since BERT’s goal is to generate a language model only the encoder mechanism is necessary hence, 'encoder representation'\n ","metadata":{}},{"cell_type":"markdown","source":"<B> How BERT performs Bidirectional training? </B>\n\nBERT uses following two prediction models simultaneously with the goal of minimizing the combined loss function of the two strategies\n\n1. <B>Masked Language Model </B>:  Before feeding word sequences into BERT, 15% of the words in each sequence are replaced with a [MASK] token. The model then attempts to predict the original value of the masked words, based on the context provided by the other, non-masked, words in the sequence.\n\n\n2. <B>Next Sentence Prediction </B> :  The model receives pairs of sentences as input and learns to predict if the second sentence in the pair is the subsequent sentence in the original document. During training, 50% of the inputs are a pair in which the second sentence is the subsequent sentence in the original document, while in the other 50% a random sentence from the corpus is chosen as the second sentence. The assumption is that the random sentence will be disconnected from the first sentence.\n","metadata":{}},{"cell_type":"markdown","source":"# 2. Salient BERT Features  <a class=\"kk\" id=\"2\"></a>\n[Back to Contents](#0.1)\n\n\n1. <B>Dynamic Word Embedding </B>:Unlike previous [Part-2](https://www.kaggle.com/kksienc/comprehensive-nlp-tutorial-2-dl-perspective) word embedding techniques (Word2Vec etc), where each word had a static vector representation, BERT produces word representations that are not fixed but are dynamically informed by the words around them.   \n2. <B>Pre trained Model</B> : BERT uses transfer learning i.e. it fetches knowledge of pretrained model and then fine-tune it. BERT is pre-trained on a large corpus of unlabelled text including the entire Wikipedia and Book Corpus. The pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models. BERT is clearly not designed to train any dataset from scratch.\n3. <B>Word Segmentation </B>:BERT does not flag OOV and unknown word like 'OOV' and 'UNK'. It decomposes these into meaningful sub-word and characters tokens and then generates embeddings.\n\n4. <B> Multilanguage Support </B>: BERT had been adopted by Google-Search for over 70 languages.\n\n\n ","metadata":{}},{"cell_type":"markdown","source":"# 3.  BERT  Implementation  <a class=\"kk\" id=\"3\"></a>\n[Back to Contents](#0.1)\n\nWe will use Goolge TensorFlow [library](https://pypi.org/project/bert-for-tf2/) for our BERT modeling. It uses Keras backend.\n ","metadata":{}},{"cell_type":"markdown","source":"## 3.1 BERT Pretrained Layer  <a class=\"kk\" id=\"3.1\"></a>\n\nFirst we will fetch our pretrained BERT layer and load the tokenizer.\n","metadata":{}},{"cell_type":"code","source":"# install tesorflow bert package\n!pip install bert-for-tf2\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow.keras import layers\nimport bert\n\n#Loding pretrained bert layer\nBertTokenizer = bert.bert_tokenization.FullTokenizer\nbert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n                            trainable=False)\n\n\n# Loading tokenizer from the bert layer\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = BertTokenizer(vocab_file, do_lower_case)\n","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.2 BERT  Encoding  <a class=\"kk\" id=\"3.2\"></a>\n\n\n-  Each sentence is first tokenized into tokens \n-  A [CLS] token is inserted at the beginning of the first sentence and a [SEP] token is inserted at the end of each sentence.\n-  Tokens that comply with the fixed vocabulary are fetched and assigned with following 3 properties.\n    1. Token IDs -  to assign Unique token-id from BERT’s tokenizer.\n    2. Padding ID (Mask-Id) - to indicate which elements in the sequence are tokens and which are padding elements.\n    3. Segment IDs - to distinguish different sentences.   \n\nLets encode a sample text \n ","metadata":{}},{"cell_type":"markdown","source":"### 3.2.1 BERT Text Encoding : Sample Example  <a class=\"kk\" id=\"3.2.1\"></a>\n ","metadata":{}},{"cell_type":"code","source":"text = 'Encoding will be clear with this example'\n# tokenize\ntokens_list = tokenizer.tokenize(text)\nprint('Text after tokenization')\nprint(tokens_list)\n\n# initilize dimension\nmax_len =25\ntext = tokens_list[:max_len-2]\ninput_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\nprint(\"After adding  flasges -[CLS] and [SEP]: \")\nprint(input_sequence)\n\n\ntokens = tokenizer.convert_tokens_to_ids(input_sequence )\nprint(\"tokens to id \")\nprint(tokens)\n\npad_len = max_len -len(input_sequence)\ntokens += [0] * pad_len\nprint(\"tokens: \")\nprint(tokens)\n\nprint(pad_len)\npad_masks = [1] * len(input_sequence) + [0] * pad_len\nprint(\"Pad Masking: \")\nprint(pad_masks)\n\nsegment_ids = [0] * max_len\nprint(\"Segment Ids: \")\nprint(segment_ids)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2.2 Encoding Dataset  <a class=\"kk\" id=\"3.2.2\"></a>\n \nNow lets fetch, clean and encode our dataset","metadata":{}},{"cell_type":"code","source":"# fetch & cleaning  datsset\n!pip install pyspellchecker\nimport pandas as pd\nfrom nltk.corpus import stopwords \nfrom nltk.corpus import wordnet\nfrom spellchecker import SpellChecker\nfrom nltk.stem import WordNetLemmatizer \nimport nltk \nimport re\n\ntrain_df = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\ntest_df = pd.read_csv(\"../input/nlp-getting-started/test.csv\")\n\ndef convert_to_antonym(sentence):\n    words = nltk.word_tokenize(sentence)\n    new_words = []\n    temp_word = ''\n    for word in words:\n        antonyms = []\n        if word == 'not':\n            temp_word = 'not_'\n        elif temp_word == 'not_':\n            for syn in wordnet.synsets(word):\n                for s in syn.lemmas():\n                    for a in s.antonyms():\n                        antonyms.append(a.name())\n            if len(antonyms) >= 1:\n                word = antonyms[0]\n            else:\n                word = temp_word + word # when antonym is not found, it will\n                                    # remain not_happy\n            \n            temp_word = ''\n        if word != 'not':\n            new_words.append(word)\n    return ' '.join(new_words)\n\n\ndef correct_spellings(text):\n    spell = SpellChecker()\n    corrected_words = []\n    misspelled_words = spell.unknown(text.split())\n    for word in text.split():\n        if word in misspelled_words:\n            corrected_words.append(spell.correction(word))\n        else:\n            corrected_words.append(word)\n    return \" \".join(corrected_words)\n        \n\ndef clean_text(text):\n    \"\"\"\n        text: a string\n        \n        return: modified initial string\n  \"\"\"\n    text = text.lower() # lowercase text\n    text= re.sub(r'[^\\w\\s#]',' ',text) #Removing every thing other than space, word and hash\n    text  = re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text )\n    text= re.sub(r'[0-9]',' ',text)\n    #text = correct_spellings(text)\n    text = convert_to_antonym(text)\n    text = re.sub(' +', ' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text    \n    return text\n\n\ntrain_df['text'] = train_df['text'].apply(clean_text)\ntest_df['text'] = test_df['text'].apply(clean_text)\n\n","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function to encode the text into tokens, masks, and segment flags\nimport numpy as np\ndef bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n\nMAX_LEN = 64\n\n# encode train set \ntrain_input = bert_encode(train_df.text.values, tokenizer, max_len=MAX_LEN)\n# encode  test set \ntest_input = bert_encode(test_df.text.values, tokenizer, max_len= MAX_LEN )\ntrain_labels = train_df.target.values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lets see encoded train set \ntrain_input","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Input has 3 array first for token id, second for mask-id and third for segment-id ","metadata":{}},{"cell_type":"markdown","source":"## 3.3 BERT Modeling  <a class=\"kk\" id=\"3.3\"></a>\n\nLets build & train a basic BERT model.","metadata":{}},{"cell_type":"code","source":"# first define input for token, mask and segment id  \nfrom tensorflow.keras.layers import  Input\ninput_word_ids = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"input_word_ids\")\ninput_mask = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"input_mask\")\nsegment_ids = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"segment_ids\")\n\n#  output  \nfrom tensorflow.keras.layers import Dense\npooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])  \nclf_output = sequence_output[:, 0, :]\nout = Dense(1, activation='sigmoid')(clf_output)   \n\n# intilize model\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nmodel = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\nmodel.compile(Adam(lr=2e-6), loss='binary_crossentropy', metrics=['accuracy'])\nmodel.summary()\n\n# train\ntrain_history = model.fit(\n    train_input, train_labels,\n    validation_split=0.2,\n    epochs=2,\n    batch_size=32\n)\n\nmodel.save('model.h5')\n","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Output Prediction","metadata":{}},{"cell_type":"code","source":"test_pred = model.predict(test_input)\npreds = test_pred.round().astype(int)\npreds","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Thanks for reading. If found it useful, do <font color='purple'><B> Upvote <B/></font> \n","metadata":{}}]}