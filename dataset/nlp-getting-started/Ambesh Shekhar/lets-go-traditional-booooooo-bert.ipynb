{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Bi-LSTM, CNNRNN, RNNCNN","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"This notebook consists of network model made using Tensorflow and traditional recurrent nn algorithms. <br>\nThe data it is trained on is from [Real or Not? NLP with Disaster Tweets](https://www.kaggle.com/c/nlp-getting-started) and along with that I have used GloVe for embedding matrix<br>\nI have also used a method to make learning faster by using Cyclic learning rate method. The model use _*triangular*_ policy to deal with the training.\nIf you find this kernel helpful enough then please  **Upvote**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Libraries","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Embedding, SpatialDropout1D, Dense, LSTM, \\\nBidirectional, Lambda, Conv1D, MaxPooling1D, GRU,GlobalMaxPooling1D,GlobalAveragePooling1D, concatenate\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import *\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom tensorflow.keras import Sequential","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cyclic Learning Rate {class}","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"class CyclicLR(Callback):\n    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n                 gamma=1., scale_fn=None, scale_mode='cycle'):\n        super(CyclicLR, self).__init__()\n\n        self.base_lr = base_lr\n        self.max_lr = max_lr\n        self.step_size = step_size\n        self.mode = mode\n        self.gamma = gamma\n        if scale_fn == None:\n            if self.mode == 'triangular':\n                self.scale_fn = lambda x: 1.\n                self.scale_mode = 'cycle'\n            elif self.mode == 'triangular2':\n                self.scale_fn = lambda x: 1/(2.**(x-1))\n                self.scale_mode = 'cycle'\n            elif self.mode == 'exp_range':\n                self.scale_fn = lambda x: gamma**(x)\n                self.scale_mode = 'iterations'\n        else:\n            self.scale_fn = scale_fn\n            self.scale_mode = scale_mode\n        self.clr_iterations = 0.\n        self.trn_iterations = 0.\n        self.history = {}\n\n        self._reset()\n\n    def _reset(self, new_base_lr=None, new_max_lr=None,\n               new_step_size=None):\n        \"\"\"Resets cycle iterations.\n        Optional boundary/step size adjustment.\n        \"\"\"\n        if new_base_lr != None:\n            self.base_lr = new_base_lr\n        if new_max_lr != None:\n            self.max_lr = new_max_lr\n        if new_step_size != None:\n            self.step_size = new_step_size\n        self.clr_iterations = 0.\n        \n    def clr(self):\n        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n        if self.scale_mode == 'cycle':\n            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n        else:\n            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n        \n    def on_train_begin(self, logs={}):\n        logs = logs or {}\n\n        if self.clr_iterations == 0:\n            K.set_value(self.model.optimizer.lr, self.base_lr)\n        else:\n            K.set_value(self.model.optimizer.lr, self.clr())        \n            \n    def on_batch_end(self, epoch, logs=None):\n        \n        logs = logs or {}\n        self.trn_iterations += 1\n        self.clr_iterations += 1\n\n        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n        self.history.setdefault('iterations', []).append(self.trn_iterations)\n\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n        \n        K.set_value(self.model.optimizer.lr, self.clr())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Processing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/nlp-getting-started/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_SEQUENCE_LENGTH = 60\nMAX_NB_WORDS = 30000\nEMBEDDING_DIM = 300\ntokenizer = Tokenizer(num_words=MAX_NB_WORDS)\ntokenizer.fit_on_texts(data['text'].values)\nsequences = tokenizer.texts_to_sequences(data['text'].values)\n\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\n\npad_text = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading the embeddings","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"embeddings_index = {}\nf = open('../input/glove840b300dtxt/glove.840B.300d.txt','r',encoding='utf-8')\nfor line in f:\n    values = line.split(' ')\n    word = values[0]\n    coefs = np.asarray([float(val) for val in values[1:]])\n    embeddings_index[word] = coefs\nf.close()\nprint('\\nFound %s word vectors.' % len(embeddings_index))\nembedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\nprint('Found %s word vectors.' % len(embedding_matrix))\ndel embedding_vector,embeddings_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test, y_train, y_test = train_test_split(pad_text,data['target'].values,\n                                                   test_size=0.33,shuffle=True,random_state=124, stratify=data['target'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# BiLSTM Network\nAs you could see I have applied a Global max pooling on the output(*hidden states*) from the Bidirection-LSTM. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"input_text = Input(shape=(60,),dtype='int64')\n\nembedding_layer = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1],\n                            weights=[embedding_matrix],\n                            trainable=False, mask_zero=True)(input_text)\ntext_embed = SpatialDropout1D(0.4)(embedding_layer)\n\nhidden_states = Bidirectional(LSTM(units=300, return_sequences=True))(text_embed)\nglobal_max_pooling = Lambda(lambda x: K.max(x, axis=1))  # GlobalMaxPooling1D didn't support masking\nsentence_embed = global_max_pooling(hidden_states)\n\ndense_layer = Dense(256, activation='relu')(sentence_embed)\noutput = Dense(1, activation='sigmoid')(dense_layer)\n\nBiLSTM = Model(input_text, output)\nBiLSTM.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=Adam(6e-6))\n\nlr_callback = CyclicLR()\nhistory = BiLSTM.fit(X_train,y_train, batch_size=5, epochs=3,\n                       validation_data=(X_test,y_test), callbacks=[lr_callback])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"val_preds = BiLSTM.predict(X_test)\nval_preds = np.round(val_preds).astype(int)\nprint(classification_report(y_test,val_preds,target_names = ['Not Relevant', 'Relevant']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## CNNRNN\nI have used CNN to work on finding relevant data in the sequence, whereas for recurrent operations I have used Bidirection GRU, which is again max pooled.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"input_text = Input(shape=(60,),dtype='int64')\n\nembedding_layer = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1],\n                            weights=[embedding_matrix],\n                            trainable=False, mask_zero=True)(input_text)\ntext_embed = SpatialDropout1D(0.4)(embedding_layer)\nconv_layer = Conv1D(300, kernel_size=3, padding=\"valid\", activation='relu')(text_embed)\nconv_max_pool = MaxPooling1D(pool_size=2)(conv_layer)\n\ngru_layer = Bidirectional(GRU(300, return_sequences=True))(conv_max_pool)\nsentence_embed = GlobalMaxPooling1D()(gru_layer)\n\ndense_layer = Dense(256, activation='relu')(sentence_embed)\noutput = Dense(1, activation='sigmoid')(dense_layer)\n\nCNNRNN = Model(input_text, output)\nCNNRNN.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=Adam(6e-6))\n\nlr_callback = CyclicLR()\nhistory = CNNRNN.fit(X_train,y_train, batch_size=5, epochs=3,\n                       validation_data=(X_test,y_test), callbacks=[lr_callback])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"val_preds = CNNRNN.predict(X_test)\nval_preds = np.round(val_preds).astype(int)\nprint(classification_report(y_test,val_preds,target_names = ['Not Relevant', 'Relevant']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## RNNCNN\nThis network uses output from 1D convolution and applies average pooling and Max pooling and concatenates the output from both pooling into single.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"input_text = Input(shape=(60,),dtype='int64')\n\nembedding_layer = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1],\n                            weights=[embedding_matrix],\n                            trainable=False, mask_zero=True)(input_text)\ntext_embed = SpatialDropout1D(0.4)(embedding_layer)\ngru_layer = Bidirectional(GRU(300, return_sequences=True))(text_embed)\n\nconv_layer = Conv1D(64, kernel_size=2, padding=\"valid\", kernel_initializer=\"he_uniform\")(gru_layer)\n\navg_pool = GlobalAveragePooling1D()(conv_layer)\nmax_pool = GlobalMaxPooling1D()(conv_layer)\nsentence_embed = concatenate([avg_pool, max_pool])\n\ndense_layer = Dense(256, activation='relu')(sentence_embed)\noutput = Dense(1, activation='sigmoid')(dense_layer)\n\nRNNCNN = Model(input_text, output)\n\nRNNCNN.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=Adam(6e-6))\n\nlr_callback = CyclicLR()\nhistory = RNNCNN.fit(X_train,y_train, batch_size=5, epochs=3,\n                       validation_data=(X_test,y_test), callbacks=[lr_callback])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_preds = RNNCNN.predict(X_test)\nval_preds = np.round(val_preds).astype(int)\nprint(classification_report(y_test,val_preds,target_names = ['Not Relevant', 'Relevant']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Let stack the output to a linear regressor","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train,_, y_train, _ = train_test_split(data['text'].values,data['target'].values,\n                                                   test_size=0.2,shuffle=True,random_state=124, stratify=data['target'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_SEQUENCE_LENGTH = 60\nMAX_NB_WORDS = 30000\nEMBEDDING_DIM = 300\n# tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n# tokenizer.fit_on_texts(data['text'].values)\nsequences = tokenizer.texts_to_sequences(train)\n\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\n\npad_text = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bilstm=BiLSTM.predict(pad_text)\ncr = CNNRNN.predict(pad_text)\nrc = RNNCNN.predict(pad_text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I just can't stop myself from using pandas, they are really fantastic","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = pd.DataFrame({\"BiLSTM\":bilstm.flatten(),\"CR\":cr.flatten(),\"RC\":rc.flatten(),\"target\":y_train})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Yeah this network is simple as f***, but still scores 80%.....\nclf = Sequential([\n    Dense(3,activation = 'relu'),\n    Dense(1,activation= 'sigmoid')\n])\nclf.compile(loss = 'binary_crossentropy',optimizer = Adam(3e-5),metrics = ['acc'])","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"history =clf.fit(prediction.loc[:,['BiLSTM','CR','RC']].values,prediction.iloc[:,-1],validation_split= 0.1,batch_size = 5,epochs = 32)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# चलो भविष्यवाणी करते हैं","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(\"../input/nlp-getting-started/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_SEQUENCE_LENGTH = 60\nMAX_NB_WORDS = 30000\nEMBEDDING_DIM = 300\n# tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n# tokenizer.fit_on_texts(data['text'].values)\nsequences = tokenizer.texts_to_sequences(test.text.values)\n\n# word_index = tokenizer.word_index\n# print('Found %s unique tokens.' % len(word_index))\n\npad_text = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bilstm_test = BiLSTM.predict(pad_text).flatten()\ncr_test = CNNRNN.predict(pad_text).flatten()\nrc_test = RNNCNN.predict(pad_text).flatten()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_predictions = clf.predict(np.stack([bilstm_test,cr_test,rc_test],axis=-1)).flatten()\ntest_predictions = np.round(test_predictions).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")\nsubmission['target'] = test_predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv(\"/kaggle/working/submission.csv\",index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}