{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this tutorial I will focus on these 3 different strategies : \n1. 1- Bag-of-words( with Tf-Idf) : used with simple machine learning algorithm \n1. 2-Word Embedding (with Word2vec) : used with deep learning neural network \n1. 3- Bert : used with transfer learning from attention-based transformers. \n\n\nNLP : it is about programming computers to process and analyze large amounts of natural language data. "},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#First of all, I need to import the following libraries:\n## for data\nimport json\nimport pandas as pd\nimport numpy as np\n## for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n## for processing\nimport re\nimport nltk\n## for bag-of-words\nfrom sklearn import feature_extraction, model_selection, naive_bayes, pipeline, manifold, preprocessing\n## for explainer\nfrom lime import lime_text\n## for word embedding\nimport gensim\nimport gensim.downloader as gensim_api\n## for deep learning\nfrom tensorflow.keras import models, layers, preprocessing as kprocessing\nfrom tensorflow.keras import backend as K\n## for bert language model\nimport transformers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtf = pd.read_csv(\"../input/nlp-getting-started/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"the univariate distribution of the target : the labels frequency with a bar plot. "},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots()\nfig.suptitle(\"target\", fontsize=12)\ndtf[\"target\"].reset_index().groupby(\"target\").count().sort_values(by= \"index\").plot(kind=\"barh\", legend=False, \n        ax=ax).grid(axis='x')\nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"the dataset is balanced "},{"metadata":{},"cell_type":"markdown","source":"Before explaining and building the models, I am going to give an example of preprocessing by cleaning text, removing stop words, and applying lemmatization. I will write a function and apply it to the whole data set."},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nPreprocess a string.\n:parameter\n    :param text: string - name of column containing text\n    :param lst_stopwords: list - list of stopwords to remove\n    :param flg_stemm: bool - whether stemming is to be applied\n    :param flg_lemm: bool - whether lemmitisation is to be applied\n:return\n    cleaned text\n'''\ndef utils_preprocess_text(text, flg_stemm=False, flg_lemm=True, lst_stopwords=None):\n    ## clean (convert to lowercase and remove punctuations and   \n    #characters and then strip\n    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n            \n    ## Tokenize (convert from string to list)\n    lst_text = text.split()\n    ## remove Stopwords\n    if lst_stopwords is not None:\n        lst_text = [word for word in lst_text if word not in \n                    lst_stopwords]\n                \n    ## Stemming (remove -ing, -ly, ...)\n    if flg_stemm == True:\n        ps = nltk.stem.porter.PorterStemmer()\n        lst_text = [ps.stem(word) for word in lst_text]\n                \n    ## Lemmatisation (convert the word into root word)\n    if flg_lemm == True:\n        lem = nltk.stem.wordnet.WordNetLemmatizer()\n        lst_text = [lem.lemmatize(word) for word in lst_text]\n            \n    ## back to string from list\n    text = \" \".join(lst_text)\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"lst_stopwords = nltk.corpus.stopwords.words(\"english\")\nlst_stopwords","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So let's apply the function and store the result in a new column named 'text_clean' "},{"metadata":{"trusted":true},"cell_type":"code","source":"dtf[\"text_clean\"] = dtf[\"text\"].apply(lambda x: \n          utils_preprocess_text(x, flg_stemm=False, flg_lemm=True, \n          lst_stopwords=lst_stopwords))\ndtf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## split dataset\ndtf_train, dtf_test = model_selection.train_test_split(dtf, test_size=0.3)\n## get target\ny_train = dtf_train[\"target\"].values\ny_test = dtf_test[\"target\"].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Bag-of-Words******\n\nSo Bag of words works like this  : it builds a vocabulary from a corpus of documents and then counts how many times the words appear in each document. \nso each word becomes a feature and a  document is represented by a vector with the same lenghth of the vocabulary. \n*the Feature matrix shape = Number of documents * length of vocabulary.*\n\nAs you can image , this approach causes  a huge sparse matrix ; a significant dimensionality problem : the more documents you have the larger is the vocabulary. That's why the bag of words model is usually preceded by an important preprocessing (word claning , stop words removals , stemming/lemmatization) aimed to reduce the dimensionnality problem. "},{"metadata":{},"cell_type":"markdown","source":"I skipped the part of TFIDF ! I will expalain it later "},{"metadata":{},"cell_type":"markdown","source":"So after splitting the data we will focus now on ***Feature Engineering***, which is the process of creating features by extracting information from the data. I am going to use the Tf-Idf vectorizer with a limit of 10,000 words (so the length of my vocabulary will be 10k), capturing unigrams (i.e. ‚Äúnew‚Äù and ‚Äúyork‚Äù) and bigrams (i.e. ‚Äúnew york‚Äù) #if we use trigrams it will be for example \"new york city\". I will provide the code for the classic count vectorizer as well:"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Count (classic BoW)\nvectorizer = feature_extraction.text.CountVectorizer(max_features=10000, ngram_range=(1,2))\n\n## Tf-Idf (advanced variant of BoW)\nvectorizer = feature_extraction.text.TfidfVectorizer(max_features=10000, ngram_range=(1,2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus = dtf_train[\"text_clean\"]\nvectorizer.fit(corpus)\nX_train = vectorizer.transform(corpus)\ndic_vocabulary = vectorizer.vocabulary_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to know the position of a certain word, we can look it up in the vocabulary:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"word = \"forest\"\ndic_vocabulary[word]\n#If the word exists in the vocabulary, \n#this command prints a number N, \n#meaning that the Nth feature of the matrix is that word.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**reduce the matrix dimensionality****\nin order to reduce the dimensionality of our matrix ! [Feature matrix shape: Number of documents x Length of vocabulary ] we can carry out some Feature Selection, the process of selecting a subset of relevant variables. I will proceed as follows:\n* treat each category as binary (for example, the ‚ÄúTech‚Äù category is 1 for the Tech news and 0 for the others);\n1. perform a Chi-Square test to determine whether a feature and the (binary) target are independent;\n1. keep only the features with a certain p-value from the Chi-Square test."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import feature_selection \ny = dtf_train[\"target\"]\nX_names = vectorizer.get_feature_names()\np_value_limit = 0.95\ndtf_features = pd.DataFrame()\nfor cat in np.unique(y):\n    chi2, p = feature_selection.chi2(X_train, y==cat)\n    dtf_features = dtf_features.append(pd.DataFrame(\n                   {\"feature\":X_names, \"score\":1-p, \"y\":cat}))\n    dtf_features = dtf_features.sort_values([\"y\",\"score\"], \n                    ascending=[True,False])\n    dtf_features = dtf_features[dtf_features[\"score\"]>p_value_limit]\nX_names = dtf_features[\"feature\"].unique().tolist()\nlen(X_names)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for cat in np.unique(y):\n    print(\"# {}:\".format(cat))\n    print(\"  . selected features:\",\n         len(dtf_features[dtf_features[\"y\"]==cat]))\n    print(\"  . top features:\", \",\".join(\ndtf_features[dtf_features[\"y\"]==cat][\"feature\"].values[:10]))\n    print(\" \")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#training the model ! \nNaive Bayes algorithm: a probabilistic classifier that makes use of Bayes‚Äô Theorem, a rule that uses probability to make predictions based on prior knowledge of conditions that might be related. "},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier = naive_bayes.MultinomialNB()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I‚Äôm going to train this classifier on the feature matrix and then test it on the transformed test set. To that end, I need to build a scikit-learn pipeline: a sequential application of a list of transformations and a final estimator. Putting the Tf-Idf vectorizer and the Naive Bayes classifier in a pipeline allows us to transform and predict test data in just one step."},{"metadata":{"trusted":true},"cell_type":"code","source":"## pipeline\nmodel = pipeline.Pipeline([(\"vectorizer\", vectorizer),  \n                           (\"classifier\", classifier)])\n## train classifier\nmodel[\"classifier\"].fit(X_train, y_train)\n## test\nX_test = dtf_test[\"text_clean\"].values\npredicted = model.predict(X_test)\npredicted_prob = model.predict_proba(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can now evaluate the performance of the Bag-of-Words model, I will use the following metrics:****"},{"metadata":{},"cell_type":"markdown","source":"**Accuracy** is one metric for evaluating classification models. Informally, accuracy is the fraction of predictions our model got right. Formally, accuracy has the following definition:\nAccuracy in Machine Learning\nThe accuracy of a machine learning classification algorithm is one way to measure how often the algorithm classifies a data point correctly. Accuracy is the number of correctly predicted data points out of all the data points. More formally, it is defined as the number of true positives and true negatives divided by the number of true positives, true negatives, false positives, and false negatives. A true positive or true negative is a data point that the algorithm correctly classified as true or false, respectively. A false positive or false negative, on the other hand, is a data point that the algorithm incorrectly classified. For example, if the algorithm classified a false data point as true, it would be a false positive. Often, accuracy is used along with precision and recall, which are other metrics that use various ratios of true/false positives/negatives. Together, these metrics provide a detailed look at how the algorithm is classifying data points. \n\nExample\nConsider a classification algorithm that decides whether an email is spam or not. The algorithm is trained, and we want to see how well it performs on a set of ten emails it has never seen before. Of the ten emails, six are not spam and four are spam. The algorithm classifies three of the messages as spam, of which two are actually spam, and one is not spam. In the table, the true positives (the emails that are correctly identified as spam) are colored in green, the true negatives (the emails that are correctly identified as not spam) are colored in blue, the false positives (the not spam emails that are incorrectly classified as spam) are colored in red, and the false negatives (the spam emails that are incorrectly identified as not spam) are colored in orange. There are two true positives, five true negatives, two false negatives, and one false positive. Using the formula for accuracy, we get: \n\nThis algorithm has 70% accuracy classifying emails as spam or not. \n![](https://images.deepai.org/django-summernote/2019-05-09/ad392084-735b-432a-bdf0-b4b56a455de3.jpg)\n"},{"metadata":{},"cell_type":"markdown","source":"What is a **Confusion Matrix**?\nThe million dollar question ‚Äì what, after all, is a confusion matrix?\n\nA **Confusion matrix** is an N x N matrix used for evaluating the performance of a classification model, where N is the number of target classes. The matrix compares the actual target values with those predicted by the machine learning model. This gives us a holistic view of how well our classification model is performing and what kinds of errors it is making.\n\nFor a binary classification problem, we would have a 2 x 2 matrix as shown below with 4 values:\n![](https://cdn.analyticsvidhya.com/wp-content/uploads/2020/04/Basic-Confusion-matrix.png) \n"},{"metadata":{},"cell_type":"markdown","source":"**AUC - ROC** curve is a performance measurement for classification problem at various thresholds settings. ROC is a probability curve and AUC represents degree or measure of separability. It tells how much model is capable of distinguishing between classes. Higher the AUC, better the model is at predicting 0s as 0s and 1s as 1s. By analogy, Higher the AUC, better the model is at distinguishing between patients with disease and no disease.\nThe ROC curve is plotted with TPR against the FPR where TPR is on y-axis and FPR is on the x-axis.\n![](https://miro.medium.com/max/361/1*pk05QGzoWhCgRiiFbz-oKQ.png) "},{"metadata":{},"cell_type":"markdown","source":"Precision is how close measure values are to each other, basically how many decimal places are at the end of a given measurement.  Precision does matter.  Accuracy is how close a measure value is to the true value.  Accuracy matters too, but it‚Äôs best when measurements are both precise and accurate.\n\nFailure to understand the tension between precision and accuracy can have profound negative effects on how one processes data, and the final outcome of geospatial analysis.\n\n![](https://i0.wp.com/wp.stolaf.edu/it/files/2017/06/precsionvsaccuracy_crashcourse.png?resize=579%2C600&ssl=1) "},{"metadata":{},"cell_type":"markdown","source":"Recall is calculated as the ratio of the number of true positives divided by the sum of the true positives and the false negatives. Recall is the same as sensitivity.\n\nRecall = True Positives / (True Positives + False Negatives)\n1\nRecall = True Positives / (True Positives + False Negatives)"},{"metadata":{},"cell_type":"markdown","source":"****** SO the ACCURACY IS 0.793 **\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\naccuracy = metrics.accuracy_score(y_test, predicted)\naccuracy ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nprint(classification_report(y_test, predicted))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Generating the confusion matrix\nBut let‚Äôs take a look at generating that confusion matrix now. As we discussed, it‚Äôs part of the evaluation step, and we use it to visualize its predictive and generalization power on the test set.\n\nRecall that we compare the predictions generated during evaluation with the ground truth available for those inputs.\n\nThe plot_confusion_matrix call takes care of this for us, and we simply have to provide it the classifier (clf), the test set (X_test and y_test), a color map and whether to normalize the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generate confusion matrix\nfrom sklearn.metrics import plot_confusion_matrix\nfrom mlxtend.plotting import plot_decision_regions\nmatrix = plot_confusion_matrix(model, X_test, y_test,\n                                 cmap=plt.cm.Blues,\n                                 normalize='true')\nplt.title('Confusion matrix for our classifier')\nplt.show(matrix)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Word Embedding"},{"metadata":{},"cell_type":"markdown","source":"Word Embedding is the collective name for feature learning techniques where words from the vocabulary are mapped to vectors of real numbers. These vectors are calculated from the probability distribution for each word appearing before or after another. To put it another way, words of the same context usually appear together in the corpus, so they will be close in the vector space as well. For instance, let‚Äôs take the 3 sentences from the previous example:\n\n"},{"metadata":{},"cell_type":"markdown","source":"Let us now define Word Embeddings formally. A Word Embedding format generally tries to map a word using a dictionary to a vector. Let us break this sentence down into finer details to have a clear view.\n\nTake a look at this example ‚Äì sentence=‚Äù Word Embeddings are Word converted into numbers ‚Äù\n\nA word in this sentence may be ‚ÄúEmbeddings‚Äù or ‚Äúnumbers ‚Äù etc.\n\nA vector representation of a word may be a one-hot encoded vector where 1 stands for the position where the word exists and 0 everywhere else. The vector representation of ‚Äúnumbers‚Äù in this format according to the above dictionary is [0,0,0,0,0,1] and of converted is[0,0,0,1,0,0].\n\nThis isüëÜüëÜüëÜ just a very simple method to represent a word in the vector form. "},{"metadata":{},"cell_type":"markdown","source":"et us look at different types of Word Embeddings or Word Vectors and their advantages and disadvantages over the rest : Word2Vec produces a vector space, typically of several hundred dimensions, with each unique word in the corpus such that words that share common contexts in the corpus are located close to one another in the space. That can be done using 2 different approaches: starting from a single word to predict its context (Skip-gram) or starting from the context to predict a word (Continuous Bag-of-Words). "},{"metadata":{},"cell_type":"markdown","source":"Credits to : \n1. 1- https://www.analyticsvidhya.com/blog/2020/04/confusion-matrix-machine-learning/ \n1. 2- https://deepai.org/researchers \n1. 3- https://wp.stolaf.edu/it/gis-precision-accuracy/ \n1. 4- https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5 \n\n1. 5 https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/\n1. 6 https://www.machinecurve.com/index.php/2020/05/05/how-to-create-a-confusion-matrix-with-scikit-learn/"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}