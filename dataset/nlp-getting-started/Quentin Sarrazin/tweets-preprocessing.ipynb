{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom nltk.corpus import stopwords # import English stopwords\nstopwords = set(stopwords.words('english'))\n\nimport re #used for Regular Expression \nimport string\nfrom collections import defaultdict\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading data\ntrain_data = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest_data = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Mislabelled tweets\nids_with_target_error = [328,443,513,2619,3640,3900,4342,5781,6552,6554,6570,6701,6702,6729,6861,7226]\n\n# Modification of the target\ntrain_data.at[train_data['id'].isin(ids_with_target_error),'target'] = 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1 - Cleaning functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove all emojis, replace by EMOJI\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'EMOJI', text)\n\ntest = \"Oh non ðŸ˜” je suis triste ...\"\nprint(remove_emoji(test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove @ and mention, replace by USER\ndef remove_mention(text):\n    at=re.compile(r'@\\S+')\n    return at.sub(r'USER',text)\n\ntest = \"Je mentionne @me ok\"\nprint(remove_mention(test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove HTML beacon\ndef remove_HTML(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\ntest = \"\"\"<div>\n<h1>Real or Fake</h1>\n<p>Kaggle </p>\n<a href=\"https://www.kaggle.com/c/nlp-getting-started\">getting started</a>\n</div>\"\"\"\nprint(remove_HTML(test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove numbers, replace it by NUMBER\ndef remove_number(text):\n    num = re.compile(r'[-+]?[.\\d]*[\\d]+[:,.\\d]*')\n    return num.sub(r'NUMBER', text)\n\nprint(remove_number(\"13.5\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove all URLs, replace by URL\ndef remove_URL(text):\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url.sub(r'URL',text)\n\ntest = \"Coucou, venez sur https://www.youtube.com/\"\nprint(remove_URL(test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Factorize repeated punctuation, add REPEAT\ndef remove_repeat_punct(text):\n    rep = re.compile(r'([!?.]){2,}')\n    return rep.sub(r'\\1 REPEAT', text)\n\nprint(remove_repeat_punct(\"!!!!\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Factorize elongated words, add ELONG\ndef remove_elongated_words(text):\n    rep = re.compile(r'\\b(\\S*?)([a-z])\\2{2,}\\b')\n    return rep.sub(r'\\1\\2 ELONG', text)\n\nprint(remove_elongated_words(\"yessss\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove words in capslock, add ALLCAPS\ndef remove_allcaps(text):\n    caps = re.compile(r'([^a-z0-9()<>\\'`\\-]){2,}')\n    return caps.sub(r'ALLCAPS', text)\n\nprint(remove_allcaps(\"AAAAHH\")) # Can't find how to keep the word in downcase ...","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replace some smileys with SMILE\ndef transcription_smile(text):\n    eyes = \"[8:=;]\"\n    nose = \"['`\\-]\"\n    smiley = re.compile(r'[8:=;][\\'\\-]?[)dDp]')\n    #smiley = re.compile(r'#{eyes}#{nose}[)d]+|[)d]+#{nose}#{eyes}/i')\n    return smiley.sub(r'SMILE', text)\n\nprint(transcription_smile(\":-)\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replace some others smileys with SADFACE\ndef transcription_sad(text):\n    eyes = \"[8:=;]\"\n    nose = \"['`\\-]\"\n    smiley = re.compile(r'[8:=;][\\'\\-]?[(\\\\/]')\n    return smiley.sub(r'SADFACE', text)\n\nprint(transcription_sad(\":/ :(\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replace <3 with HEART\ndef transcription_heart(text):\n    heart = re.compile(r'<3')\n    return heart.sub(r'HEART', text)\n\nprint(transcription_heart(\"<3\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove all english stopwords\ndef remove_stopwords(text):\n    text = ' '.join([word for word in text.split() if word not in (stopwords)])\n    return text\n\ntest = \"i am an happy man \"\nprint(remove_stopwords(test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove all punctuations\ndef remove_all_punct(text):\n    table = str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\ntest = \"Ceci est, un message ! avec de la . ponctuation \" \" random \"\nprint(remove_all_punct(test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove punctuations\ndef remove_punct(text):\n    punctuations = '@#!?+&*[]-%.:/();$=><|{}^' + \"'`\" \n    for p in punctuations:\n        text = text.replace(p, f' {p} ')\n\n    text = text.replace('...', ' ... ')\n    if '...' not in text:\n        text = text.replace('..', ' ... ')   \n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove non printable characters\ndef remove_not_ASCII(text):\n    text = ''.join([word for word in text if word in string.printable])\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Some english abbreviations\nabbreviations = {\n    \"$\" : \" dollar \",\n    \"â‚¬\" : \" euro \",\n    \"4ao\" : \"for adults only\",\n    \"a.m\" : \"before midday\",\n    \"a3\" : \"anytime anywhere anyplace\",\n    \"aamof\" : \"as a matter of fact\",\n    \"acct\" : \"account\",\n    \"adih\" : \"another day in hell\",\n    \"afaic\" : \"as far as i am concerned\",\n    \"afaict\" : \"as far as i can tell\",\n    \"afaik\" : \"as far as i know\",\n    \"afair\" : \"as far as i remember\",\n    \"afk\" : \"away from keyboard\",\n    \"app\" : \"application\",\n    \"approx\" : \"approximately\",\n    \"apps\" : \"applications\",\n    \"asap\" : \"as soon as possible\",\n    \"asl\" : \"age, sex, location\",\n    \"atk\" : \"at the keyboard\",\n    \"ave.\" : \"avenue\",\n    \"aymm\" : \"are you my mother\",\n    \"ayor\" : \"at your own risk\", \n    \"b&b\" : \"bed and breakfast\",\n    \"b+b\" : \"bed and breakfast\",\n    \"b.c\" : \"before christ\",\n    \"b2b\" : \"business to business\",\n    \"b2c\" : \"business to customer\",\n    \"b4\" : \"before\",\n    \"b4n\" : \"bye for now\",\n    \"b@u\" : \"back at you\",\n    \"bae\" : \"before anyone else\",\n    \"bak\" : \"back at keyboard\",\n    \"bbbg\" : \"bye bye be good\",\n    \"bbc\" : \"british broadcasting corporation\",\n    \"bbias\" : \"be back in a second\",\n    \"bbl\" : \"be back later\",\n    \"bbs\" : \"be back soon\",\n    \"be4\" : \"before\",\n    \"bfn\" : \"bye for now\",\n    \"blvd\" : \"boulevard\",\n    \"bout\" : \"about\",\n    \"brb\" : \"be right back\",\n    \"bros\" : \"brothers\",\n    \"brt\" : \"be right there\",\n    \"bsaaw\" : \"big smile and a wink\",\n    \"btw\" : \"by the way\",\n    \"bwl\" : \"bursting with laughter\",\n    \"c/o\" : \"care of\",\n    \"cet\" : \"central european time\",\n    \"cf\" : \"compare\",\n    \"cia\" : \"central intelligence agency\",\n    \"csl\" : \"can not stop laughing\",\n    \"cu\" : \"see you\",\n    \"cul8r\" : \"see you later\",\n    \"cv\" : \"curriculum vitae\",\n    \"cwot\" : \"complete waste of time\",\n    \"cya\" : \"see you\",\n    \"cyt\" : \"see you tomorrow\",\n    \"dae\" : \"does anyone else\",\n    \"dbmib\" : \"do not bother me i am busy\",\n    \"diy\" : \"do it yourself\",\n    \"dm\" : \"direct message\",\n    \"dwh\" : \"during work hours\",\n    \"e123\" : \"easy as one two three\",\n    \"eet\" : \"eastern european time\",\n    \"eg\" : \"example\",\n    \"embm\" : \"early morning business meeting\",\n    \"encl\" : \"enclosed\",\n    \"encl.\" : \"enclosed\",\n    \"etc\" : \"and so on\",\n    \"faq\" : \"frequently asked questions\",\n    \"fawc\" : \"for anyone who cares\",\n    \"fb\" : \"facebook\",\n    \"fc\" : \"fingers crossed\",\n    \"fig\" : \"figure\",\n    \"fimh\" : \"forever in my heart\", \n    \"ft.\" : \"feet\",\n    \"ft\" : \"featuring\",\n    \"ftl\" : \"for the loss\",\n    \"ftw\" : \"for the win\",\n    \"fwiw\" : \"for what it is worth\",\n    \"fyi\" : \"for your information\",\n    \"g9\" : \"genius\",\n    \"gahoy\" : \"get a hold of yourself\",\n    \"gal\" : \"get a life\",\n    \"gcse\" : \"general certificate of secondary education\",\n    \"gfn\" : \"gone for now\",\n    \"gg\" : \"good game\",\n    \"gl\" : \"good luck\",\n    \"glhf\" : \"good luck have fun\",\n    \"gmt\" : \"greenwich mean time\",\n    \"gmta\" : \"great minds think alike\",\n    \"gn\" : \"good night\",\n    \"g.o.a.t\" : \"greatest of all time\",\n    \"goat\" : \"greatest of all time\",\n    \"goi\" : \"get over it\",\n    \"gps\" : \"global positioning system\",\n    \"gr8\" : \"great\",\n    \"gratz\" : \"congratulations\",\n    \"gyal\" : \"girl\",\n    \"h&c\" : \"hot and cold\",\n    \"hp\" : \"horsepower\",\n    \"hr\" : \"hour\",\n    \"hrh\" : \"his royal highness\",\n    \"ht\" : \"height\",\n    \"ibrb\" : \"i will be right back\",\n    \"ic\" : \"i see\",\n    \"icq\" : \"i seek you\",\n    \"icymi\" : \"in case you missed it\",\n    \"idc\" : \"i do not care\",\n    \"idgadf\" : \"i do not give a damn fuck\",\n    \"idgaf\" : \"i do not give a fuck\",\n    \"idk\" : \"i do not know\",\n    \"ie\" : \"that is\",\n    \"i.e\" : \"that is\",\n    \"ifyp\" : \"i feel your pain\",\n    \"IG\" : \"instagram\",\n    \"iirc\" : \"if i remember correctly\",\n    \"ilu\" : \"i love you\",\n    \"ily\" : \"i love you\",\n    \"imho\" : \"in my humble opinion\",\n    \"imo\" : \"in my opinion\",\n    \"imu\" : \"i miss you\",\n    \"iow\" : \"in other words\",\n    \"irl\" : \"in real life\",\n    \"j4f\" : \"just for fun\",\n    \"jic\" : \"just in case\",\n    \"jk\" : \"just kidding\",\n    \"jsyk\" : \"just so you know\",\n    \"l8r\" : \"later\",\n    \"la\" : \"los angeles\",\n    \"lb\" : \"pound\",\n    \"lbs\" : \"pounds\",\n    \"ldr\" : \"long distance relationship\",\n    \"lmao\" : \"laugh my ass off\",\n    \"lmfao\" : \"laugh my fucking ass off\",\n    \"lol\" : \"laughing out loud\",\n    \"ltd\" : \"limited\",\n    \"ltns\" : \"long time no see\",\n    \"m8\" : \"mate\",\n    \"mf\" : \"motherfucker\",\n    \"mfs\" : \"motherfuckers\",\n    \"mfw\" : \"my face when\",\n    \"mofo\" : \"motherfucker\",\n    \"mph\" : \"miles per hour\",\n    \"mr\" : \"mister\",\n    \"mrw\" : \"my reaction when\",\n    \"ms\" : \"miss\",\n    \"mte\" : \"my thoughts exactly\",\n    \"nagi\" : \"not a good idea\",\n    \"nbc\" : \"national broadcasting company\",\n    \"nbd\" : \"not big deal\",\n    \"nfs\" : \"not for sale\",\n    \"ngl\" : \"not going to lie\",\n    \"nhs\" : \"national health service\",\n    \"nrn\" : \"no reply necessary\",\n    \"nsfl\" : \"not safe for life\",\n    \"nsfw\" : \"not safe for work\",\n    \"nth\" : \"nice to have\",\n    \"nvr\" : \"never\",\n    \"nyc\" : \"new york city\",\n    \"oc\" : \"original content\",\n    \"og\" : \"original\",\n    \"ohp\" : \"overhead projector\",\n    \"oic\" : \"oh i see\",\n    \"omdb\" : \"over my dead body\",\n    \"omg\" : \"oh my god\",\n    \"omw\" : \"on my way\",\n    \"p.a\" : \"per annum\",\n    \"p.m\" : \"after midday\",\n    \"pm\" : \"prime minister\",\n    \"poc\" : \"people of color\",\n    \"pov\" : \"point of view\",\n    \"pp\" : \"pages\",\n    \"ppl\" : \"people\",\n    \"prw\" : \"parents are watching\",\n    \"ps\" : \"postscript\",\n    \"pt\" : \"point\",\n    \"ptb\" : \"please text back\",\n    \"pto\" : \"please turn over\",\n    \"qpsa\" : \"what happens\", #\"que pasa\",\n    \"ratchet\" : \"rude\",\n    \"rbtl\" : \"read between the lines\",\n    \"rlrt\" : \"real life retweet\", \n    \"rofl\" : \"rolling on the floor laughing\",\n    \"roflol\" : \"rolling on the floor laughing out loud\",\n    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n    \"rt\" : \"retweet\",\n    \"ruok\" : \"are you ok\",\n    \"sfw\" : \"safe for work\",\n    \"sk8\" : \"skate\",\n    \"smh\" : \"shake my head\",\n    \"sq\" : \"square\",\n    \"srsly\" : \"seriously\", \n    \"ssdd\" : \"same stuff different day\",\n    \"tbh\" : \"to be honest\",\n    \"tbs\" : \"tablespooful\",\n    \"tbsp\" : \"tablespooful\",\n    \"tfw\" : \"that feeling when\",\n    \"thks\" : \"thank you\",\n    \"tho\" : \"though\",\n    \"thx\" : \"thank you\",\n    \"tia\" : \"thanks in advance\",\n    \"til\" : \"today i learned\",\n    \"tl;dr\" : \"too long i did not read\",\n    \"tldr\" : \"too long i did not read\",\n    \"tmb\" : \"tweet me back\",\n    \"tntl\" : \"trying not to laugh\",\n    \"ttyl\" : \"talk to you later\",\n    \"u\" : \"you\",\n    \"u2\" : \"you too\",\n    \"u4e\" : \"yours for ever\",\n    \"utc\" : \"coordinated universal time\",\n    \"w/\" : \"with\",\n    \"w/o\" : \"without\",\n    \"w8\" : \"wait\",\n    \"wassup\" : \"what is up\",\n    \"wb\" : \"welcome back\",\n    \"wtf\" : \"what the fuck\",\n    \"wtg\" : \"way to go\",\n    \"wtpa\" : \"where the party at\",\n    \"wuf\" : \"where are you from\",\n    \"wuzup\" : \"what is up\",\n    \"wywh\" : \"wish you were here\",\n    \"yd\" : \"yard\",\n    \"ygtr\" : \"you got that right\",\n    \"ynk\" : \"you never know\",\n    \"zzz\" : \"sleeping bored and tired\"\n}\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Change an abbreviation by its true meaning\ndef word_abbrev(word):\n    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word\n\n# Replace all abbreviations\ndef replace_abbrev(text):\n    string = \"\"\n    for word in text.split():\n        string += word_abbrev(word) + \" \"        \n    return string\n\nprint(replace_abbrev(\"wtf are you doing here mf ?\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Clean text\ndef clean_tweet(text):\n    \n    # Remove non text\n    text = remove_URL(text)\n    text = remove_HTML(text)\n    text = remove_not_ASCII(text)\n    \n    # Lower text, replace abbreviations\n    text = text.lower()\n    text = replace_abbrev(text)  \n    text = remove_mention(text)\n    text = remove_number(text)\n    \n    # Remove emojis / smileys\n    text = remove_emoji(text)\n    text = transcription_sad(text)\n    text = transcription_smile(text)\n    text = transcription_heart(text)\n    \n    # Remove repeated puntuations / words\n    text = remove_elongated_words(text)\n    text = remove_repeat_punct(text)\n\n    #text = remove_all_punct(text)\n    #text = remove_punct(text)\n    #text = remove_stopwords(text)\n\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Clean text and add a new feature\ntrain_data[\"clean_text\"] = train_data[\"text\"].apply(clean_tweet)\ntest_data[\"clean_text\"] = test_data[\"text\"].apply(clean_tweet)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Examples\ntrain_data.head(100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2 - Create corpus / Remove useless stopwords"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create corpus from data with target 'target'\ndef create_corpus(data, target):\n    corpus=[]\n    for tweet in data[data['target']==target]['clean_text'].str.split():\n        for word in tweet:\n            corpus.append(word)\n    return corpus\n\n# Create dictionary from a corpus (keys = words, values = number of each word)\ndef create_dic(corpus):\n    dic = defaultdict(int)\n    for word in corpus:\n        dic[word] += 1\n    return dic\n\n# Merge two dictionary and return one with values = [x,y]\ndef merge_dic(dic0, dic1):\n    result = {}\n    for word0 in dic0:\n        result[word0]=[0,0]\n    for word1 in dic1:\n        result[word1]=[0,0]\n    for word0 in dic0:\n        result[word0][0] += dic0[word0]\n    for word1 in dic1:\n        result[word1][1] += dic1[word1]\n    return result\n\n# Create corpuses\nnot_disaster_corpus = create_corpus(train_data, 0)\ndisaster_corpus = create_corpus(train_data, 1)\n\n# Create dictionaries\nnot_disaster_dic = create_dic(not_disaster_corpus)\ndisaster_dic = create_dic(disaster_corpus)\n\n# Merge dictionaries\nmerge_dic = merge_dic(not_disaster_dic, disaster_dic)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sort word with the estimated probability of appearance in a disaster or not disaster tweet\n# We only keep words with a significative appearence in one of the categories\nmost_probably_not_disaster = (sorted(merge_dic.items(), key=lambda x: x[1][0]/x[1][1] if (x[1][0]+x[1][1]>20 and x[1][1]!=0) else 1, reverse = True))[:1000]\nmost_probably_disaster = (sorted(merge_dic.items(), key=lambda x: x[1][0]/x[1][1] if (x[1][0]+x[1][1]>20 and x[1][1]!=0) else 1, reverse = False))[:1000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ratio of number of disaster tweets by non disaster tweets\nratio_disaster = len(train_data[train_data['target']==1])/len(train_data[train_data['target']==0])\n\n# We keep words for each categories\n# We apply arctan on the ratio to compute a float between 0 and 1\n# We keep words with a score under or ahead the treshold depending on what kind of word we want\ncertainly_disaster_words = [w[0] for w in most_probably_disaster if (w[1][1] != 0 and 2/np.pi*np.arctan(w[1][0]/w[1][1]*ratio_disaster)<0.4)]\ncertainly_not_disaster_words = [w[0] for w in most_probably_not_disaster if (w[1][1] != 0 and 2/np.pi*np.arctan(w[1][0]/w[1][1]*ratio_disaster)>0.6)]\n\n# Set of usefull words\nusefull_words = []\nusefull_words.append(certainly_disaster_words)\nusefull_words.append(certainly_not_disaster_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(certainly_disaster_words[:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(certainly_not_disaster_words[:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove only stopwords not in usefull words\ndef remove_useless_stopwords(text):\n    text = ' '.join([word for word in text.split() if (word not in (stopwords)) or (word in usefull_words)])\n    return text\n\ntest = \"i'm a our you debris\"\nprint(remove_useless_stopwords(test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove useless stopwords from clean text\ntrain_data[\"clean_text\"] = train_data[\"clean_text\"].apply(remove_useless_stopwords)\ntest_data[\"clean_text\"] = test_data[\"clean_text\"].apply(remove_useless_stopwords)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head(10)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}