{"cells":[{"metadata":{},"cell_type":"markdown","source":"##  Stacking Different Feature Models\n\nIn this notebook, I am training on two different feature spaces:\n\n1. Combined TFIDF for analyzer=word analyzer=char. Most of the code is taken from the notebook of @nicapotato [tf-idf-xgboost](https://www.kaggle.com/nicapotato/tf-idf-xgboost)\n2. TensorFlow Universal Sentence Encoder. The features are modelled on basis of [nlp-disaster-tweets-1](https://www.kaggle.com/akazuko/nlp-disaster-tweets-1)\n\nBoth the features are trained separately on XGBoost"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.preprocessing import OneHotEncoder\n\nfrom scipy.sparse import hstack\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier # <3\nfrom sklearn.model_selection import train_test_split\nimport gc\nimport re\nfrom nltk.corpus import stopwords\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv').fillna(' ')#.sample(1000)\ntest = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv').fillna(' ')#.sample(1000)\n\nkeep_train_index=train['text'].drop_duplicates().index\ntrain=train.iloc[keep_train_index,:]\ntrain_text = train[\"text\"]\n\n\ntest_text=test['text']\nall_text = pd.concat([train_text, test_text])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Text PreProcessing\nThe text is pre-processed to :\n1. Remove URLs\n2. Twitter Handles\n3. Spit on punctuations\n4. Remove stopwords\n5. Remove numbers"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Contractions\n#Reference: https://www.kaggle.com/gunesevitan/nlp-with-disaster-tweets-eda-cleaning-and-bert\ndef replace_contractions(tweet):\n    tweet = re.sub(r\"he's\", \"he is\", tweet)\n    tweet = re.sub(r\"there's\", \"there is\", tweet)\n    tweet = re.sub(r\"We're\", \"We are\", tweet)\n    tweet = re.sub(r\"That's\", \"That is\", tweet)\n    tweet = re.sub(r\"won't\", \"will not\", tweet)\n    tweet = re.sub(r\"they're\", \"they are\", tweet)\n    tweet = re.sub(r\"Can't\", \"Cannot\", tweet)\n    tweet = re.sub(r\"wasn't\", \"was not\", tweet)\n    tweet = re.sub(r\"don\\x89Ã›Âªt\", \"do not\", tweet)\n    tweet = re.sub(r\"aren't\", \"are not\", tweet)\n    tweet = re.sub(r\"isn't\", \"is not\", tweet)\n    tweet = re.sub(r\"What's\", \"What is\", tweet)\n    tweet = re.sub(r\"haven't\", \"have not\", tweet)\n    tweet = re.sub(r\"hasn't\", \"has not\", tweet)\n    tweet = re.sub(r\"There's\", \"There is\", tweet)\n    tweet = re.sub(r\"He's\", \"He is\", tweet)\n    tweet = re.sub(r\"It's\", \"It is\", tweet)\n    tweet = re.sub(r\"You're\", \"You are\", tweet)\n    tweet = re.sub(r\"I'M\", \"I am\", tweet)\n    tweet = re.sub(r\"shouldn't\", \"should not\", tweet)\n    tweet = re.sub(r\"wouldn't\", \"would not\", tweet)\n    tweet = re.sub(r\"i'm\", \"I am\", tweet)\n    tweet = re.sub(r\"I\\x89Ã›Âªm\", \"I am\", tweet)\n    tweet = re.sub(r\"I'm\", \"I am\", tweet)\n    tweet = re.sub(r\"Isn't\", \"is not\", tweet)\n    tweet = re.sub(r\"Here's\", \"Here is\", tweet)\n    tweet = re.sub(r\"you've\", \"you have\", tweet)\n    tweet = re.sub(r\"you\\x89Ã›Âªve\", \"you have\", tweet)\n    tweet = re.sub(r\"we're\", \"we are\", tweet)\n    tweet = re.sub(r\"what's\", \"what is\", tweet)\n    tweet = re.sub(r\"couldn't\", \"could not\", tweet)\n    tweet = re.sub(r\"we've\", \"we have\", tweet)\n    tweet = re.sub(r\"it\\x89Ã›Âªs\", \"it is\", tweet)\n    tweet = re.sub(r\"doesn\\x89Ã›Âªt\", \"does not\", tweet)\n    tweet = re.sub(r\"It\\x89Ã›Âªs\", \"It is\", tweet)\n    tweet = re.sub(r\"Here\\x89Ã›Âªs\", \"Here is\", tweet)\n    tweet = re.sub(r\"who's\", \"who is\", tweet)\n    tweet = re.sub(r\"I\\x89Ã›Âªve\", \"I have\", tweet)\n    tweet = re.sub(r\"y'all\", \"you all\", tweet)\n    tweet = re.sub(r\"can\\x89Ã›Âªt\", \"cannot\", tweet)\n    tweet = re.sub(r\"would've\", \"would have\", tweet)\n    tweet = re.sub(r\"it'll\", \"it will\", tweet)\n    tweet = re.sub(r\"we'll\", \"we will\", tweet)\n    tweet = re.sub(r\"wouldn\\x89Ã›Âªt\", \"would not\", tweet)\n    tweet = re.sub(r\"We've\", \"We have\", tweet)\n    tweet = re.sub(r\"he'll\", \"he will\", tweet)\n    tweet = re.sub(r\"Y'all\", \"You all\", tweet)\n    tweet = re.sub(r\"Weren't\", \"Were not\", tweet)\n    tweet = re.sub(r\"Didn't\", \"Did not\", tweet)\n    tweet = re.sub(r\"they'll\", \"they will\", tweet)\n    tweet = re.sub(r\"they'd\", \"they would\", tweet)\n    tweet = re.sub(r\"DON'T\", \"DO NOT\", tweet)\n    tweet = re.sub(r\"That\\x89Ã›Âªs\", \"That is\", tweet)\n    tweet = re.sub(r\"they've\", \"they have\", tweet)\n    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n    tweet = re.sub(r\"should've\", \"should have\", tweet)\n    tweet = re.sub(r\"You\\x89Ã›Âªre\", \"You are\", tweet)\n    tweet = re.sub(r\"where's\", \"where is\", tweet)\n    tweet = re.sub(r\"Don\\x89Ã›Âªt\", \"Do not\", tweet)\n    tweet = re.sub(r\"we'd\", \"we would\", tweet)\n    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n    tweet = re.sub(r\"weren't\", \"were not\", tweet)\n    tweet = re.sub(r\"They're\", \"They are\", tweet)\n    tweet = re.sub(r\"Can\\x89Ã›Âªt\", \"Cannot\", tweet)\n    tweet = re.sub(r\"you\\x89Ã›Âªll\", \"you will\", tweet)\n    tweet = re.sub(r\"I\\x89Ã›Âªd\", \"I would\", tweet)\n    tweet = re.sub(r\"let's\", \"let us\", tweet)\n    tweet = re.sub(r\"it's\", \"it is\", tweet)\n    tweet = re.sub(r\"can't\", \"cannot\", tweet)\n    tweet = re.sub(r\"don't\", \"do not\", tweet)\n    tweet = re.sub(r\"you're\", \"you are\", tweet)\n    tweet = re.sub(r\"i've\", \"I have\", tweet)\n    tweet = re.sub(r\"that's\", \"that is\", tweet)\n    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n    tweet = re.sub(r\"doesn't\", \"does not\", tweet)\n    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n    tweet = re.sub(r\"didn't\", \"did not\", tweet)\n    tweet = re.sub(r\"ain't\", \"am not\", tweet)\n    tweet = re.sub(r\"you'll\", \"you will\", tweet)\n    tweet = re.sub(r\"I've\", \"I have\", tweet)\n    tweet = re.sub(r\"Don't\", \"do not\", tweet)\n    tweet = re.sub(r\"I'll\", \"I will\", tweet)\n    tweet = re.sub(r\"I'd\", \"I would\", tweet)\n    tweet = re.sub(r\"Let's\", \"Let us\", tweet)\n    tweet = re.sub(r\"you'd\", \"You would\", tweet)\n    tweet = re.sub(r\"It's\", \"It is\", tweet)\n    tweet = re.sub(r\"Ain't\", \"am not\", tweet)\n    tweet = re.sub(r\"Haven't\", \"Have not\", tweet)\n    tweet = re.sub(r\"Could've\", \"Could have\", tweet)\n    tweet = re.sub(r\"youve\", \"you have\", tweet)  \n    tweet = re.sub(r\"donÃ¥Â«t\", \"do not\", tweet) \n    return tweet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"custom_stop_list = []\n#stop_files=['slang.txt']\n#for stopfile in stop_files:\n#    with open(\"../data/\"+stopfile) as f:\n#        for line in f:\n#            custom_stop_list.extend(line.split())\n\nstopword_set = stopwords.words('english')+custom_stop_list+['url']\n\n# Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\n#https://www.kaggle.com/shahules/basic-eda-cleaning-and-glove\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\n\nremove_emoji(\"Omg another Earthquake ðŸ˜”ðŸ˜”\")\ndef preProcess(iter):\n    #https://www.kaggle.com/shahules/basic-eda-cleaning-and-glove\n    \n           \n        \n        # remove extra space\n        regex_ws=re.compile(\"\\s+\")\n        ret=regex_ws.sub(\" \",iter)\n        \n        ret=ret.replace(\"&amp;\",\"&\").replace(\"&lt;\",\"<\").replace(\"&gt;\",\">\")\n        \n        \n        #Replace slang words\n        #for key in abbreviations.keys():\n        #    ret=ret.replace(key,abbreviations[key])\n        \n        #Replace URL\n        regexp=\"(https?:\\/\\/(?:www\\.|(?!www)|(?:xmlns\\.))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})\"\n        ret=re.sub(regexp,\"url\",ret)\n        \n        #replace @addresses\n        regexp='@[A-z0-9_]+'\n        ret=re.sub(regexp,\"@twitterhandle\",ret)\n        \n        ret=remove_emoji(ret)\n        #ret=replace_contractions(ret)\n        #Split on punctuations\n        ret1=re.split(\"[,_, \\<>!\\?\\.:\\n\\\"=*/]+\",ret)\n        \n        #Remove Stopwords\n        ret2=[word for word in ret1 if word not in stopword_set]\n        ret2=\" \".join(ret2)\n        \n        #Remove  numbers\n        ret2=re.sub(r\"(\\s\\d+)\",\" \",ret2)\n                \n        #STEM TEXT\n        #ret3=stem_text(strip_punctuation(ret2))\n    \n        return ret2\nte=preProcess(\"I'll make the day a fantastic 1!!\")\nprint(te)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## TF-IDF\nThe ``word-vectorizer`` extracts features based on combined data of Test and Train.\nThe ``char-vectorizer`` extracts features on character n-gram range of length 2 to 6. This helps to take into account word subsets. For example: word-vectorizer will count fire and fires as separate features. However, character n-gram will count the subset \"fire\" in \"fires\". This will increase the weightage of the word.\nI found that character vectorizers resulted in more accurate models than stemming or spell-correction."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"TFIDF\")\n#Reference: https://www.kaggle.com/nicapotato/tf-idf-xgboost\nword_vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='word',\n    token_pattern=r'\\w{1,}',\n    stop_words='english',\n    ngram_range=(1, 1),\n    norm='l2',\n    min_df=0,\n    smooth_idf=False,\n    preprocessor=preProcess,\n    max_features=15000)\nword_vectorizer.fit(all_text)\ntrain_word_features = word_vectorizer.transform(train_text)\ntest_word_features = word_vectorizer.transform(test_text)\n\n\nchar_vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='char',\n    stop_words='english',\n    ngram_range=(2, 6),\n    norm='l2',\n    min_df=0,\n    smooth_idf=False,preprocessor=preProcess,\n    max_features=30000)\nchar_vectorizer.fit(all_text)\ntrain_char_features = char_vectorizer.transform(train_text)\ntest_char_features = char_vectorizer.transform(test_text)\n\nenc = OneHotEncoder(handle_unknown='ignore')\ntrain_keyword_features = enc.fit_transform(train[['keyword']].to_numpy().reshape(-1,1))\ntest_keyword_features = enc.transform(test[['keyword']].to_numpy().reshape(-1,1))\n\n\ntrain_features = hstack([train_char_features, train_word_features]).tocsr()\ndel train_char_features,train_word_features\ntest_features = hstack([test_char_features, test_word_features]).tocsr()\ndel test_char_features,test_word_features\n\nprint(train_features.shape)\nprint(test_features.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## F1 Score\n\n``f1_metric`` returns __1-f1_score__. This is done because XGBoost tries to minimize the metric.\nUsing the feval parameter in XGBoost, this can be defined as the custom metric."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import  metrics\ndef f1_metric(preds,dtrain):\n    ytrue=dtrain.get_label()\n    return 'f1_score', 1-metrics.f1_score(preds.round().astype(np.int), ytrue, average='macro')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model\nXGBClassifier is used as the common model.\nStratified K-fold is used for splitting the data into train and test. The results over all folds are averaged to get the best prediction.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndef train_model(train_features, train_target,test_features,n_estimator):\n    d_test = xgb.DMatrix(test_features)\n    #del test_features\n    gc.collect()\n    \n    print(\"Modeling\")\n    cv_scores = []\n    xgb_preds = []\n    submission = pd.DataFrame.from_dict({'id': test['id']})\n   \n    \n    xgb_params = {'eta': 0.1, \n                  'max_depth': 7, \n                  'subsample': 0.7,\n                  'colsample_bytree': 0.75, \n                  'n_estimators':n_estimator,\n                  'objective': 'binary:logistic', \n                  'metric': 'f1_score', \n                  'eval_metric':'error',\n                  'gamma':0,\n                  'seed': 314159265,\n                  'verbose':20,\n                  'min_child_weight':1\n                  \n                 }\n    \n    cv=StratifiedKFold(n_splits=4)\n\n    for i,(train_index, valid_index) in enumerate(cv.split(train_features, train_target)):\n        X_train, X_valid=train_features[train_index],train_features[valid_index]\n        y_train, y_valid = train_target.iloc[train_index],train_target.iloc[valid_index]\n        \n                \n        d_train = xgb.DMatrix(X_train, y_train)\n        d_valid = xgb.DMatrix(X_valid, y_valid)\n\n        watchlist = [(d_valid, 'valid')]\n        model = xgb.train(xgb_params, d_train, 200, watchlist, verbose_eval=False,early_stopping_rounds=100, feval=f1_metric)\n        print(model.attributes())\n        print(model.attributes()['best_msg'])\n        cv_scores.append(float(model.attributes()['best_score']))\n        submission[i] = model.predict(d_test)#.round().astype(np.int)\n        #del X_train, X_valid, y_train, y_valid\n        gc.collect()\n    print('Total CV score is {}'.format(np.mean(cv_scores)))\n    '''d_train = xgb.DMatrix(train_features, train_target)\n\n    model = xgb.cv(xgb_params, d_train, 300,  verbose_eval=False,nfold=5,stratified=True,\n                          early_stopping_rounds=150, feval=f1_metric)'''\n    print(model.attributes())\n    print(model.attributes()['best_msg'])\n    cv_scores.append(float(model.attributes()['best_score']))\n    submission = model.predict(d_test)#.round().astype(np.int)\n    \n    return submission\n    #return model\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_target = train['target']\nprint(train_target.shape)\ntrain_target.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsubmission_tfidf=train_model(train_features, train_target,test_features,n_estimator=97)\n#res=train_model(train_features, train_target,test_features,n_estimator=4)\n#print(res)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## TensorFlow Universal Sentence Encoder\n\nTFIDF is based only on frequency of words in a tweet.\nSentence embedding gives a semantic meaning to the tweet and represent it in a vector form.\nTo make the train_model function reusable, the tensor in embedding is converted to a CSR matrix.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#!pip3 install --quiet tensorflow-hub\n#Reference: https://www.kaggle.com/akazuko/nlp-disaster-tweets-1\nimport tensorflow_hub as hub\nfrom scipy import sparse\nembed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\ntrain_text=train_text.apply(remove_emoji)\ntest_text=test_text.apply(remove_emoji)\nX_train_embeddings = embed(train_text)\nX_test_embeddings = embed(test_text)\nX_feat=sparse.csr_matrix(X_train_embeddings.numpy())\nX_test=sparse.csr_matrix(X_test_embeddings.numpy())\nX_feat.shape\nsubmission_hub=train_model(X_feat,train_target,X_test,n_estimator=100)#\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(submission_hub)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#submission_hub=np.mean(submission_hub.iloc[:,1:],axis=0)\n#submission_tfidf=np.mean(submission_tfidf.iloc[:,1:],axis=0)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Evaluation\n\n**Note** The f1-score above is __1-f1__. \nThe Universal Sentence Encoder features give better f1_score than TFIDF.\nHowever, combining the two models gives a better score."},{"metadata":{"trusted":true},"cell_type":"code","source":"submission=(0.35*submission_tfidf+0.65*submission_hub).round().astype(np.int)\nfinal_submit = pd.DataFrame.from_dict({'id': test['id']})\nfinal_submit['target']=submission\n\nprint(final_submit)\nfinal_submit.to_csv('submission.csv', index=False)\nprint(final_submit['target'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Further Enhancements:\n1. Hyperparameter Tuning\n2. Stacking with different classification models"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}