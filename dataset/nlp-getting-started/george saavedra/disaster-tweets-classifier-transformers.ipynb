{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# NLP Disaster Tweets Classifier with Transformers\n\nWelcome folks!, in the current project I will implement on this dataset what I have done previously in another project called \"Best Sentiment Classifier Transformers\" in which I showed you in detail how to implement four types of well-known transformer models making use of the transformers HuggingFace library and Keras API.\n\nThe main task corresponds to a binary text classification on Disaster Tweets Competition and the dataset contains 7.613 instances for training, whereas the testing set contains 3263 from which we have to classify as \"Disaster\" or \"non-Disaster\".\n\nBy the way I really encourage you to see my project \"Best NLP Disaster Tweets Classifier!\" in which I perform an exhaustive and spotless explanation of corpus processing and NLU, this is because in the current project we will mainly focus on implementing state of the art transformer models and comparing their performance.\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport matplotlib\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nimport seaborn as sns\nsns.set(style='whitegrid')\n\nfrom wordcloud import WordCloud\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA, TruncatedSVD\nfrom sklearn.metrics import classification_report,confusion_matrix\n\nfrom collections import defaultdict\nfrom collections import Counter\n\nimport re\nimport gensim\nimport string\n\nfrom tqdm import tqdm\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, LSTM,Dense, SpatialDropout1D, Dropout\nfrom keras.initializers import Constant\n\nimport tensorflow as tf\n\nimport warnings\nwarnings.simplefilter('ignore')","metadata":{"id":"me7d-FZ00aen","execution":{"iopub.status.busy":"2022-01-12T22:29:43.887993Z","iopub.execute_input":"2022-01-12T22:29:43.88836Z","iopub.status.idle":"2022-01-12T22:29:50.66068Z","shell.execute_reply.started":"2022-01-12T22:29:43.888274Z","shell.execute_reply":"2022-01-12T22:29:50.65992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's start by reading the 3 csv files containing training, testing and sample submission, then run a couple of functions to know a little bit more about our data. ","metadata":{"id":"DJQrZBqcaVt3"}},{"cell_type":"code","source":"df=pd.read_csv('../input/nlp-getting-started/train.csv')\ndf_test=pd.read_csv('../input/nlp-getting-started/test.csv')\nsample_submission=pd.read_csv('../input/nlp-getting-started/sample_submission.csv')","metadata":{"id":"dkqzDqP1ZHHe","execution":{"iopub.status.busy":"2022-01-12T22:29:52.034538Z","iopub.execute_input":"2022-01-12T22:29:52.035031Z","iopub.status.idle":"2022-01-12T22:29:52.181329Z","shell.execute_reply.started":"2022-01-12T22:29:52.034987Z","shell.execute_reply":"2022-01-12T22:29:52.180741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape, df_test.shape","metadata":{"id":"lsk0k5j8ITlq","outputId":"6eba054b-d26d-4efc-bad7-4fc3b0547d0f","execution":{"iopub.status.busy":"2022-01-12T22:29:53.249022Z","iopub.execute_input":"2022-01-12T22:29:53.249787Z","iopub.status.idle":"2022-01-12T22:29:53.258201Z","shell.execute_reply.started":"2022-01-12T22:29:53.249729Z","shell.execute_reply":"2022-01-12T22:29:53.257325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"id":"6QC6JG9kP8Ea","outputId":"06507bd0-be23-4b63-c378-8f43265b76d3","execution":{"iopub.status.busy":"2022-01-12T22:29:56.0958Z","iopub.execute_input":"2022-01-12T22:29:56.096108Z","iopub.status.idle":"2022-01-12T22:29:56.121659Z","shell.execute_reply.started":"2022-01-12T22:29:56.096073Z","shell.execute_reply":"2022-01-12T22:29:56.120904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.loc[:,['text','target']]","metadata":{"id":"AWdKml9vZNbv","outputId":"b8a0005c-185c-4f21-a7a8-d23270208736","execution":{"iopub.status.busy":"2022-01-12T22:29:59.127778Z","iopub.execute_input":"2022-01-12T22:29:59.128065Z","iopub.status.idle":"2022-01-12T22:29:59.143402Z","shell.execute_reply.started":"2022-01-12T22:29:59.128036Z","shell.execute_reply":"2022-01-12T22:29:59.142762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"id":"Ri873plxINjO","outputId":"2ab3cdac-7b1c-4d0c-bca5-807a4bd7f121","execution":{"iopub.status.busy":"2022-01-12T22:30:02.124883Z","iopub.execute_input":"2022-01-12T22:30:02.125191Z","iopub.status.idle":"2022-01-12T22:30:02.147367Z","shell.execute_reply.started":"2022-01-12T22:30:02.125157Z","shell.execute_reply":"2022-01-12T22:30:02.146699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"id":"cUad5rZuaNip","outputId":"332888e5-cabc-499c-d561-19c0b2354813","execution":{"iopub.status.busy":"2022-01-12T22:30:02.414413Z","iopub.execute_input":"2022-01-12T22:30:02.41529Z","iopub.status.idle":"2022-01-12T22:30:02.425181Z","shell.execute_reply.started":"2022-01-12T22:30:02.41525Z","shell.execute_reply":"2022-01-12T22:30:02.424584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.target.value_counts()","metadata":{"id":"qk1t7FatZgnM","outputId":"a8d7f4a3-4b48-45e5-8bc0-63cab5357ac9","execution":{"iopub.status.busy":"2022-01-12T22:30:09.612701Z","iopub.execute_input":"2022-01-12T22:30:09.612988Z","iopub.status.idle":"2022-01-12T22:30:09.619717Z","shell.execute_reply.started":"2022-01-12T22:30:09.612955Z","shell.execute_reply":"2022-01-12T22:30:09.619093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see above the distribution of the classes is slightly unbalanced, this is why we should expect to have sidetrack in the prediction towards class 0.","metadata":{"id":"d4MiMBrCeOFS"}},{"cell_type":"code","source":"df2=df.copy(deep=True)\npie1=pd.DataFrame(df2['target'].replace(1,'disaster').replace(0,'non-disaster').value_counts())\npie1.reset_index(inplace=True)\npie1.plot(kind='pie', title='Pie chart of Disaster/Non-disaster tweets',y = 'target', \n          autopct='%1.1f%%', shadow=False, labels=pie1['index'], legend = False, fontsize=14, figsize=(12,12))","metadata":{"id":"_YyDWkNtZggo","outputId":"49324690-20b9-4495-9d37-eecc85f31c75","execution":{"iopub.status.busy":"2022-01-12T22:30:12.697397Z","iopub.execute_input":"2022-01-12T22:30:12.697803Z","iopub.status.idle":"2022-01-12T22:30:12.951368Z","shell.execute_reply.started":"2022-01-12T22:30:12.697771Z","shell.execute_reply":"2022-01-12T22:30:12.950476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Time now to find out the number of words in headline tweets, in order to understand a bit better we will plot histograms for both classes:\n","metadata":{"id":"L6X_98lWbz-R"}},{"cell_type":"code","source":"f, (ax1, ax2,) = plt.subplots(1,2,figsize=(25,8))\n\nax1.hist(df[df['target'] == 0]['text'].str.split().map(lambda x: len(x)), bins=29, color='b')\nax1.set_title('Non-disaster tweets')\n\nax2.hist(df[df['target'] == 1]['text'].str.split().map(lambda x: len(x)), bins=29, color='r')\nax2.set_title('Disaster tweets')\n\nf.suptitle('Histogram number of words in tweets')","metadata":{"id":"p6m01h4pbkDc","outputId":"25262dc4-782d-486b-dd67-601f63638290","execution":{"iopub.status.busy":"2022-01-12T22:30:24.222744Z","iopub.execute_input":"2022-01-12T22:30:24.223066Z","iopub.status.idle":"2022-01-12T22:30:24.824113Z","shell.execute_reply.started":"2022-01-12T22:30:24.22303Z","shell.execute_reply":"2022-01-12T22:30:24.82318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In both plots we can see the distributions are Gaussian-like shapes with similar frequencies, it seems that the longest tweet in the entire dataset corresponds to a Non-disaster and is around 31 words, now let's obtain the longest one by using the max() function:","metadata":{}},{"cell_type":"code","source":"df['text'].str.split().map(lambda x: len(x)).max()","metadata":{"execution":{"iopub.status.busy":"2022-01-12T22:30:41.018889Z","iopub.execute_input":"2022-01-12T22:30:41.019721Z","iopub.status.idle":"2022-01-12T22:30:41.054563Z","shell.execute_reply.started":"2022-01-12T22:30:41.019665Z","shell.execute_reply":"2022-01-12T22:30:41.05382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Effectively was 31 words, this means if we would Tokenize by word the max_length should be 31, however as transformers consider sub-words tokenization such number could be increased depending on the words being used which can increase such length to 40 or even more, thus we have to take that into account when modeling as it could cause our model to take significatively a long time to train, therefore we have to find a trade-off between training time and performance.","metadata":{}},{"cell_type":"code","source":"dfff=pd.DataFrame(df['text'].str.split().map(lambda x: len(x))>=10)\nprint('Number of sentences which contain more than 10 words: ', dfff.loc[dfff['text']==True].shape[0])\nprint(' ')\ndfff=pd.DataFrame(df['text'].str.split().map(lambda x: len(x))>=15)\nprint('Number of sentences which contain more than 15 words: ', dfff.loc[dfff['text']==True].shape[0])\nprint(' ')\ndfff=pd.DataFrame(df['text'].str.split().map(lambda x: len(x))>=20)\nprint('Number of sentences which contain more than 20 words: ', dfff.loc[dfff['text']==True].shape[0])\nprint(' ')\ndfff=pd.DataFrame(df['text'].str.split().map(lambda x: len(x))>=25)\nprint('Number of sentences which contain more than 25 words: ', dfff.loc[dfff['text']==True].shape[0])\nprint(' ')\ndfff=pd.DataFrame(df['text'].str.split().map(lambda x: len(x))==31)\nprint('Number of sentences which contain 31 words: ', dfff.loc[dfff['text']==True].shape[0])\nprint(' ')","metadata":{"execution":{"iopub.status.busy":"2022-01-12T22:32:01.21267Z","iopub.execute_input":"2022-01-12T22:32:01.212943Z","iopub.status.idle":"2022-01-12T22:32:01.356435Z","shell.execute_reply.started":"2022-01-12T22:32:01.212914Z","shell.execute_reply":"2022-01-12T22:32:01.354776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Above we see 350 tweets contain more than 25 words and only 3 tweets are of 31 words, now we have to consider that our dataset is \"small sized\" (7.613 instances) and the following cleaning process will get rid of a big portion useless part of the sentences we can be sure after tokenization process the sequences created will not be much longer than 31 words. Below we can see the three headline tweets containing 31 words, observe there are misspelled words, emojis, acronyms and some of them can be decomposed into sub-words:","metadata":{}},{"cell_type":"code","source":"print(df.loc[954,'text'])\nprint(' ')\nprint(df.loc[4432,'text'])\nprint(' ')\nprint(df.loc[5005,'text'])","metadata":{"execution":{"iopub.status.busy":"2022-01-12T22:45:18.47257Z","iopub.execute_input":"2022-01-12T22:45:18.473165Z","iopub.status.idle":"2022-01-12T22:45:18.478822Z","shell.execute_reply.started":"2022-01-12T22:45:18.473124Z","shell.execute_reply":"2022-01-12T22:45:18.478251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cleaning:\n\nThe tweets contained in the dataset are almost raw, this means we have to get rid of all 'impurities' such as tags, symbols, punctuations, emojis, etc. These does not add significant information to the prediction moreover makes our sentences more subjective. This process comprehend 6 key steps which will make our sentences partially-suit to be used in training of the model.","metadata":{"id":"ONu0rf7QeiMi"}},{"cell_type":"markdown","source":"### Removing URLs: \nSome tweets either disaster or non-disaster include links 'URLs' which correspond to videos or other webpages containing key information about the subject they are trying to communicate, as we want to clean the sentences we must get rid of them. The function which applies such step will be caled remove_URL:","metadata":{"id":"2mGy_NNpeu5n"}},{"cell_type":"code","source":"example=\"New competition launched :https://www.kaggle.com/c/nlp-getting-started\"","metadata":{"id":"nCOiWapIV6zE","execution":{"iopub.status.busy":"2022-01-12T22:50:43.632857Z","iopub.execute_input":"2022-01-12T22:50:43.633141Z","iopub.status.idle":"2022-01-12T22:50:43.637732Z","shell.execute_reply.started":"2022-01-12T22:50:43.63311Z","shell.execute_reply":"2022-01-12T22:50:43.636667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_URL(text):\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\nremove_URL(example)","metadata":{"id":"YE5BJObeWn-Q","outputId":"91dafa34-2074-4843-f693-731da3f3cd69","execution":{"iopub.status.busy":"2022-01-12T22:50:44.417662Z","iopub.execute_input":"2022-01-12T22:50:44.418244Z","iopub.status.idle":"2022-01-12T22:50:44.425487Z","shell.execute_reply.started":"2022-01-12T22:50:44.418204Z","shell.execute_reply":"2022-01-12T22:50:44.424539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text']=df['text'].apply(lambda x : remove_URL(x))","metadata":{"id":"gy7tu5aNWn2J","execution":{"iopub.status.busy":"2022-01-12T22:50:45.966739Z","iopub.execute_input":"2022-01-12T22:50:45.967034Z","iopub.status.idle":"2022-01-12T22:50:46.007117Z","shell.execute_reply.started":"2022-01-12T22:50:45.966988Z","shell.execute_reply":"2022-01-12T22:50:46.006487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['text']=df_test['text'].apply(lambda x : remove_URL(x))","metadata":{"id":"BFbPHF0GQ-2c","execution":{"iopub.status.busy":"2022-01-12T22:50:46.230318Z","iopub.execute_input":"2022-01-12T22:50:46.230923Z","iopub.status.idle":"2022-01-12T22:50:46.251884Z","shell.execute_reply.started":"2022-01-12T22:50:46.230882Z","shell.execute_reply":"2022-01-12T22:50:46.250886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Removing HTML tags:\n\nWe have to consider that some tweets were obtained using web scrapping, using this method the components of a publication are companied by special tags identifying them. As such tags are unuseful we must get rid of them to gather only the text. The function which applies such step will be called remove_html:","metadata":{"id":"k2mC_ZsWXLFN"}},{"cell_type":"code","source":"example = \"\"\"<div>\n<h1>Real or Fake</h1>\n<p>Kaggle </p>\n<a href=\"https://www.kaggle.com/c/nlp-getting-started\">getting started</a>\n</div>\"\"\"","metadata":{"id":"zBmhbF-UXG8h","execution":{"iopub.status.busy":"2022-01-12T22:50:53.759429Z","iopub.execute_input":"2022-01-12T22:50:53.759703Z","iopub.status.idle":"2022-01-12T22:50:53.764165Z","shell.execute_reply.started":"2022-01-12T22:50:53.759672Z","shell.execute_reply":"2022-01-12T22:50:53.763302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n    \nprint(remove_html(example))","metadata":{"id":"UrHXZVtTWnsg","outputId":"1c7fbb1a-45bb-4b04-8757-ece160cf6e20","execution":{"iopub.status.busy":"2022-01-12T22:50:54.132374Z","iopub.execute_input":"2022-01-12T22:50:54.132825Z","iopub.status.idle":"2022-01-12T22:50:54.137778Z","shell.execute_reply.started":"2022-01-12T22:50:54.132774Z","shell.execute_reply":"2022-01-12T22:50:54.137091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text']=df['text'].apply(lambda x : remove_html(x))","metadata":{"id":"RH8zNMOkXR1i","execution":{"iopub.status.busy":"2022-01-12T22:50:55.822469Z","iopub.execute_input":"2022-01-12T22:50:55.822912Z","iopub.status.idle":"2022-01-12T22:50:55.841853Z","shell.execute_reply.started":"2022-01-12T22:50:55.82286Z","shell.execute_reply":"2022-01-12T22:50:55.841056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['text']=df_test['text'].apply(lambda x : remove_html(x))","metadata":{"id":"CGIsAvb3RHIO","execution":{"iopub.status.busy":"2022-01-12T22:50:56.069723Z","iopub.execute_input":"2022-01-12T22:50:56.069981Z","iopub.status.idle":"2022-01-12T22:50:56.080811Z","shell.execute_reply.started":"2022-01-12T22:50:56.069953Z","shell.execute_reply":"2022-01-12T22:50:56.079888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Removing Emojis:\n\nEmojis are an efficient way to show the feeling of the publishers in the message, we could translate the meaning of them to words and help to improve the scope of the message. These could be useful or confuse the algorithm when finding the same feeling for disaster and non-disaster tweets, because of this we prefer to get rid of them, the function which applies such step will be called remove_emoji:","metadata":{"id":"NRTrav5YXl4C"}},{"cell_type":"code","source":"# Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\nremove_emoji(\"Omg another Earthquake üòîüòî\")","metadata":{"id":"qAANjLFmXRsD","outputId":"4f2227b5-68c9-4b66-bd93-b82534725595","execution":{"iopub.status.busy":"2022-01-12T22:51:13.949663Z","iopub.execute_input":"2022-01-12T22:51:13.950548Z","iopub.status.idle":"2022-01-12T22:51:13.96129Z","shell.execute_reply.started":"2022-01-12T22:51:13.950482Z","shell.execute_reply":"2022-01-12T22:51:13.960564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text']=df['text'].apply(lambda x: remove_emoji(x))","metadata":{"id":"hN_8t-cdXot2","execution":{"iopub.status.busy":"2022-01-12T22:51:14.279575Z","iopub.execute_input":"2022-01-12T22:51:14.279866Z","iopub.status.idle":"2022-01-12T22:51:14.346808Z","shell.execute_reply.started":"2022-01-12T22:51:14.279831Z","shell.execute_reply":"2022-01-12T22:51:14.346128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['text']=df_test['text'].apply(lambda x: remove_emoji(x))","metadata":{"id":"E_WF2ZSfROUx","execution":{"iopub.status.busy":"2022-01-12T22:51:14.630057Z","iopub.execute_input":"2022-01-12T22:51:14.630442Z","iopub.status.idle":"2022-01-12T22:51:14.661271Z","shell.execute_reply.started":"2022-01-12T22:51:14.630411Z","shell.execute_reply":"2022-01-12T22:51:14.660469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Contractions and acronyms:\nPeople world-wide make use of acronyms to speed-up the publishing of a tweet, some of them can be miswritten and others can be decomposed creating words that make sense, this process is exhaustive and requires investing a long time searching the meaning of each one, the function which replaces the contractions and acronyms by the words they stand for will be called cleaner: \n","metadata":{"id":"c7qg__RV73AQ"}},{"cell_type":"code","source":"def cleaner(tweet):\n  # Acronyms and miswritten words\n  tweet = re.sub(r\"Typhoon-Devastated\", \"typhoon devastated\", tweet)\n  tweet = re.sub(r\"TyphoonDevastated\", \"typhoon devastated\", tweet)\n  tweet = re.sub(r\"typhoondevastated\", \"typhoon devastated\", tweet)\n  tweet = re.sub(r\"MH370\", \"Malaysia Airlines Flight\", tweet)\n  tweet = re.sub(r\"MH\", \"Malaysia Airlines Flight\", tweet)\n  tweet = re.sub(r\"mh370\", \"Malaysia Airlines Flight\", tweet)\n  tweet = re.sub(r\"year-old\", \"years old\", tweet)\n  tweet = re.sub(r\"yearold\", \"years old\", tweet)\n  tweet = re.sub(r\"yr old\", \"years old\", tweet)\n  tweet = re.sub(r\"PKK\", \"Kurdistan Workers Party\", tweet)\n  tweet = re.sub(r\"MP\", \"madhya pradesh\", tweet)\n  tweet = re.sub(r\"rly\", \"railway\", tweet)\n  tweet = re.sub(r\"CDT\", \"Central Daylight Time\", tweet)\n  tweet = re.sub(r\"sensorsenso\", \"sensor senso\", tweet)\n  tweet = re.sub(r\"pm\", \"\", tweet)\n  tweet = re.sub(r\"PM\", \"\", tweet)\n  tweet = re.sub(r\"nan\", \" \", tweet)\n  tweet = re.sub(r\"terrorismturn\", \"terrorism turn\", tweet)\n  tweet = re.sub(r\"epicente\", \"epicenter\", tweet)\n  tweet = re.sub(r\"epicenterr\", \"epicenter\", tweet)\n  tweet = re.sub(r\"WAwildfire\", \"Washington Wildfire\", tweet)\n  tweet = re.sub(r\"prebreak\", \"pre break\", tweet)\n  tweet = re.sub(r\"nowplaying\", \"now playing\", tweet)\n  tweet = re.sub(r\"RT\", \"retweet\", tweet)\n  tweet = re.sub(r\"EbolaOutbreak\", \"Ebola Outbreak\", tweet)\n  tweet = re.sub(r\"LondonFire\", \"London Fire\", tweet)\n  tweet = re.sub(r\"IDFire\", \"Idaho Fire\", tweet)\n  tweet = re.sub(r\"withBioterrorism&use\", \"with Bioterrorism & use\", tweet)\n  tweet = re.sub(r\"NASAHurricane\", \"NASA Hurricane\", tweet)\n  tweet = re.sub(r\"withweapons\", \"with weapons\", tweet)\n  tweet = re.sub(r\"NuclearPower\", \"Nuclear Power\", tweet)\n  tweet = re.sub(r\"WhiteTerrorism\", \"White Terrorism\", tweet)\n  tweet = re.sub(r\"MyanmarFlood\", \"Myanmar Flood\", tweet)\n  tweet = re.sub(r\"ExtremeWeather\", \"Extreme Weather\", tweet)\n\n  # Special characters\n  tweet = re.sub(r\"%20\", \" \", tweet)\n  tweet = re.sub(r\"%\", \" \", tweet)\n  tweet = re.sub(r\"@\", \" \", tweet)\n  tweet = re.sub(r\"#\", \" \", tweet)\n  tweet = re.sub(r\"'\", \" \", tweet)\n  tweet = re.sub(r\"\\x89√ª_\", \" \", tweet)\n  tweet = re.sub(r\"\\x89√ª√≤\", \" \", tweet)\n  tweet = re.sub(r\"16yr\", \"16 year\", tweet)\n  tweet = re.sub(r\"re\\x89√ª_\", \" \", tweet)\n  tweet = re.sub(r\"\\x89√ª\", \" \", tweet)\n  tweet = re.sub(r\"\\x89√õ\", \" \", tweet)\n  tweet = re.sub(r\"re\\x89√õ\", \"re \", tweet)\n  tweet = re.sub(r\"re\\x89√ª\", \"re \", tweet)\n  tweet = re.sub(r\"\\x89√ª¬™\", \"'\", tweet)\n  tweet = re.sub(r\"\\x89√ª\", \" \", tweet)\n  tweet = re.sub(r\"\\x89√ª√≤\", \" \", tweet)\n  tweet = re.sub(r\"\\x89√õ_\", \"\", tweet)\n  tweet = re.sub(r\"\\x89√õ√í\", \"\", tweet)\n  tweet = re.sub(r\"\\x89√õ√ì\", \"\", tweet)\n  tweet = re.sub(r\"\\x89√õ√èWhen\", \"When\", tweet)\n  tweet = re.sub(r\"\\x89√õ√è\", \"\", tweet)\n  tweet = re.sub(r\"China\\x89√õ¬™s\", \"China's\", tweet)\n  tweet = re.sub(r\"let\\x89√õ¬™s\", \"let's\", tweet)\n  tweet = re.sub(r\"\\x89√õ√∑\", \"\", tweet)\n  tweet = re.sub(r\"\\x89√õ¬™\", \"\", tweet)\n  tweet = re.sub(r\"\\x89√õ\\x9d\", \"\", tweet)\n  tweet = re.sub(r\"√•_\", \"\", tweet)\n  tweet = re.sub(r\"\\x89√õ¬¢\", \"\", tweet)\n  tweet = re.sub(r\"\\x89√õ¬¢√•√ä\", \"\", tweet)\n  tweet = re.sub(r\"from√•√äwounds\", \"from wounds\", tweet)\n  tweet = re.sub(r\"√•√ä\", \"\", tweet)\n  tweet = re.sub(r\"√•√à\", \"\", tweet)\n  tweet = re.sub(r\"Jap√å_n\", \"Japan\", tweet)    \n  tweet = re.sub(r\"√å¬©\", \"e\", tweet)\n  tweet = re.sub(r\"√•¬®\", \"\", tweet)\n  tweet = re.sub(r\"Suru√å¬§\", \"Suruc\", tweet)\n  tweet = re.sub(r\"√•√á\", \"\", tweet)\n  tweet = re.sub(r\"√•¬£3million\", \"3 million\", tweet)\n  tweet = re.sub(r\"√•√Ä\", \"\", tweet)\n\n  # Contractions\n  tweet = re.sub(r\"he's\", \"he is\", tweet)\n  tweet = re.sub(r\"there's\", \"there is\", tweet)\n  tweet = re.sub(r\"We're\", \"We are\", tweet)\n  tweet = re.sub(r\"That's\", \"That is\", tweet)\n  tweet = re.sub(r\"won't\", \"will not\", tweet)\n  tweet = re.sub(r\"they're\", \"they are\", tweet)\n  tweet = re.sub(r\"Can't\", \"Cannot\", tweet)\n  tweet = re.sub(r\"wasn't\", \"was not\", tweet)\n  tweet = re.sub(r\"don\\x89√õ¬™t\", \"do not\", tweet)\n  tweet = re.sub(r\"aren't\", \"are not\", tweet)\n  tweet = re.sub(r\"isn't\", \"is not\", tweet)\n  tweet = re.sub(r\"What's\", \"What is\", tweet)\n  tweet = re.sub(r\"haven't\", \"have not\", tweet)\n  tweet = re.sub(r\"hasn't\", \"has not\", tweet)\n  tweet = re.sub(r\"There's\", \"There is\", tweet)\n  tweet = re.sub(r\"He's\", \"He is\", tweet)\n  tweet = re.sub(r\"It's\", \"It is\", tweet)\n  tweet = re.sub(r\"You're\", \"You are\", tweet)\n  tweet = re.sub(r\"I'M\", \"I am\", tweet)\n  tweet = re.sub(r\"Im\", \"I am\", tweet)\n  tweet = re.sub(r\"shouldn't\", \"should not\", tweet)\n  tweet = re.sub(r\"wouldn't\", \"would not\", tweet)\n  tweet = re.sub(r\"i'm\", \"I am\", tweet)\n  tweet = re.sub(r\"I\\x89√õ¬™m\", \"I am\", tweet)\n  tweet = re.sub(r\"I'm\", \"I am\", tweet)\n  tweet = re.sub(r\"Isn't\", \"is not\", tweet)\n  tweet = re.sub(r\"Here's\", \"Here is\", tweet)\n  tweet = re.sub(r\"you've\", \"you have\", tweet)\n  tweet = re.sub(r\"you\\x89√õ¬™ve\", \"you have\", tweet)\n  tweet = re.sub(r\"we're\", \"we are\", tweet)\n  tweet = re.sub(r\"what's\", \"what is\", tweet)\n  tweet = re.sub(r\"couldn't\", \"could not\", tweet)\n  tweet = re.sub(r\"we've\", \"we have\", tweet)\n  tweet = re.sub(r\"it\\x89√õ¬™s\", \"it is\", tweet)\n  tweet = re.sub(r\"doesn\\x89√õ¬™t\", \"does not\", tweet)\n  tweet = re.sub(r\"It\\x89√õ¬™s\", \"It is\", tweet)\n  tweet = re.sub(r\"Here\\x89√õ¬™s\", \"Here is\", tweet)\n  tweet = re.sub(r\"who's\", \"who is\", tweet)\n  tweet = re.sub(r\"I\\x89√õ¬™ve\", \"I have\", tweet)\n  tweet = re.sub(r\"y'all\", \"you all\", tweet)\n  tweet = re.sub(r\"can\\x89√õ¬™t\", \"cannot\", tweet)\n  tweet = re.sub(r\"would've\", \"would have\", tweet)\n  tweet = re.sub(r\"it'll\", \"it will\", tweet)\n  tweet = re.sub(r\"we'll\", \"we will\", tweet)\n  tweet = re.sub(r\"wouldn\\x89√õ¬™t\", \"would not\", tweet)\n  tweet = re.sub(r\"We've\", \"We have\", tweet)\n  tweet = re.sub(r\"he'll\", \"he will\", tweet)\n  tweet = re.sub(r\"Y'all\", \"You all\", tweet)\n  tweet = re.sub(r\"Weren't\", \"Were not\", tweet)\n  tweet = re.sub(r\"Didn't\", \"Did not\", tweet)\n  tweet = re.sub(r\"they'll\", \"they will\", tweet)\n  tweet = re.sub(r\"they'd\", \"they would\", tweet)\n  tweet = re.sub(r\"DON'T\", \"DO NOT\", tweet)\n  tweet = re.sub(r\"That\\x89√õ¬™s\", \"That is\", tweet)\n  tweet = re.sub(r\"they've\", \"they have\", tweet)\n  tweet = re.sub(r\"i'd\", \"I would\", tweet)\n  tweet = re.sub(r\"should've\", \"should have\", tweet)\n  tweet = re.sub(r\"You\\x89√õ¬™re\", \"You are\", tweet)\n  tweet = re.sub(r\"where's\", \"where is\", tweet)\n  tweet = re.sub(r\"Don\\x89√õ¬™t\", \"Do not\", tweet)\n  tweet = re.sub(r\"we'd\", \"we would\", tweet)\n  tweet = re.sub(r\"i'll\", \"I will\", tweet)\n  tweet = re.sub(r\"weren't\", \"were not\", tweet)\n  tweet = re.sub(r\"They're\", \"They are\", tweet)\n  tweet = re.sub(r\"Can\\x89√õ¬™t\", \"Cannot\", tweet)\n  tweet = re.sub(r\"you\\x89√õ¬™ll\", \"you will\", tweet)\n  tweet = re.sub(r\"I\\x89√õ¬™d\", \"I would\", tweet)\n  tweet = re.sub(r\"let's\", \"let us\", tweet)\n  tweet = re.sub(r\"it's\", \"it is\", tweet)\n  tweet = re.sub(r\"can't\", \"can not\", tweet)\n  tweet = re.sub(r\"cant\", \"can not\", tweet)\n  tweet = re.sub(r\"don't\", \"do not\", tweet)\n  tweet = re.sub(r\"dont\", \"do not\", tweet)\n  tweet = re.sub(r\"you're\", \"you are\", tweet)\n  tweet = re.sub(r\"i've\", \"I have\", tweet)\n  tweet = re.sub(r\"that's\", \"that is\", tweet)\n  tweet = re.sub(r\"i'll\", \"I will\", tweet)\n  tweet = re.sub(r\"doesn't\", \"does not\", tweet)\n  tweet = re.sub(r\"i'd\", \"I would\", tweet)\n  tweet = re.sub(r\"didn't\", \"did not\", tweet)\n  tweet = re.sub(r\"ain't\", \"am not\", tweet)\n  tweet = re.sub(r\"you'll\", \"you will\", tweet)\n  tweet = re.sub(r\"I've\", \"I have\", tweet)\n  tweet = re.sub(r\"Don't\", \"do not\", tweet)\n  tweet = re.sub(r\"I'll\", \"I will\", tweet)\n  tweet = re.sub(r\"I'd\", \"I would\", tweet)\n  tweet = re.sub(r\"Let's\", \"Let us\", tweet)\n  tweet = re.sub(r\"you'd\", \"You would\", tweet)\n  tweet = re.sub(r\"It's\", \"It is\", tweet)\n  tweet = re.sub(r\"Ain't\", \"am not\", tweet)\n  tweet = re.sub(r\"Haven't\", \"Have not\", tweet)\n  tweet = re.sub(r\"Could've\", \"Could have\", tweet)\n  tweet = re.sub(r\"youve\", \"you have\", tweet)  \n  tweet = re.sub(r\"don√•¬´t\", \"do not\", tweet)\n\n  return tweet","metadata":{"id":"HRNcaawI8Og8","execution":{"iopub.status.busy":"2022-01-12T22:51:41.451958Z","iopub.execute_input":"2022-01-12T22:51:41.452791Z","iopub.status.idle":"2022-01-12T22:51:41.507739Z","shell.execute_reply.started":"2022-01-12T22:51:41.452743Z","shell.execute_reply":"2022-01-12T22:51:41.506762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text'] = df['text'].apply(lambda s : cleaner(s))","metadata":{"id":"bs2qxpTd8OhF","execution":{"iopub.status.busy":"2022-01-12T22:51:41.954396Z","iopub.execute_input":"2022-01-12T22:51:41.954682Z","iopub.status.idle":"2022-01-12T22:51:43.587186Z","shell.execute_reply.started":"2022-01-12T22:51:41.95465Z","shell.execute_reply":"2022-01-12T22:51:43.586272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['text'] = df_test['text'].apply(lambda s : cleaner(s))","metadata":{"id":"KFvdBDlMRnYC","execution":{"iopub.status.busy":"2022-01-12T22:51:43.589031Z","iopub.execute_input":"2022-01-12T22:51:43.589346Z","iopub.status.idle":"2022-01-12T22:51:44.274298Z","shell.execute_reply.started":"2022-01-12T22:51:43.589303Z","shell.execute_reply":"2022-01-12T22:51:44.273548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Removing punctuations:\n\nIn this step the there are only a few tweets cleaned that still contain symbols and punctuations, as they don't add key information to the message we will get rid of them, the function which applies such step will be called remove_punct:\n","metadata":{"id":"Xj7pOUPCX1Yj"}},{"cell_type":"code","source":"def remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\nexample=\"I am a #king\"\nprint(remove_punct(example))","metadata":{"id":"CUdCO-IsXokm","outputId":"cb53eded-df1f-4034-92f0-e8c2b2133955","execution":{"iopub.status.busy":"2022-01-12T22:51:48.259238Z","iopub.execute_input":"2022-01-12T22:51:48.259496Z","iopub.status.idle":"2022-01-12T22:51:48.264718Z","shell.execute_reply.started":"2022-01-12T22:51:48.259467Z","shell.execute_reply":"2022-01-12T22:51:48.263734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text']=df['text'].apply(lambda x : remove_punct(x))","metadata":{"id":"BSA_wMvRXob3","execution":{"iopub.status.busy":"2022-01-12T22:51:48.838954Z","iopub.execute_input":"2022-01-12T22:51:48.839262Z","iopub.status.idle":"2022-01-12T22:51:48.886954Z","shell.execute_reply.started":"2022-01-12T22:51:48.839229Z","shell.execute_reply":"2022-01-12T22:51:48.886245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['text']=df_test['text'].apply(lambda x : remove_punct(x))","metadata":{"id":"rZZN0h-4RT0R","execution":{"iopub.status.busy":"2022-01-12T22:51:49.274575Z","iopub.execute_input":"2022-01-12T22:51:49.274865Z","iopub.status.idle":"2022-01-12T22:51:49.299712Z","shell.execute_reply.started":"2022-01-12T22:51:49.274832Z","shell.execute_reply":"2022-01-12T22:51:49.298976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Removing multiple spaces:\n\nNow, some sentences cleaned have different types of extra whitespaces, obviusly they don't add anything to the corpus and we will get rid of them with the following lines:","metadata":{"id":"fQ2LvmsDyKop"}},{"cell_type":"code","source":"df['text']=df['text'].str.replace('   ', ' ')\ndf['text']=df['text'].str.replace('     ', ' ')\ndf['text']=df['text'].str.replace('\\xa0 \\xa0 \\xa0', ' ')\ndf['text']=df['text'].str.replace('  ', ' ')\ndf['text']=df['text'].str.replace('‚Äî', ' ')\ndf['text']=df['text'].str.replace('‚Äì', ' ')","metadata":{"id":"AcR_SG4ux60h","execution":{"iopub.status.busy":"2022-01-12T22:53:40.495363Z","iopub.execute_input":"2022-01-12T22:53:40.495845Z","iopub.status.idle":"2022-01-12T22:53:40.555837Z","shell.execute_reply.started":"2022-01-12T22:53:40.4958Z","shell.execute_reply":"2022-01-12T22:53:40.554847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['text']=df_test['text'].str.replace('   ', ' ')\ndf_test['text']=df_test['text'].str.replace('     ', ' ')\ndf_test['text']=df_test['text'].str.replace('\\xa0 \\xa0 \\xa0', ' ')\ndf_test['text']=df_test['text'].str.replace('  ', ' ')\ndf_test['text']=df_test['text'].str.replace('‚Äî', ' ')\ndf_test['text']=df_test['text'].str.replace('‚Äì', ' ')","metadata":{"id":"0OT2HBHtRtq7","execution":{"iopub.status.busy":"2022-01-12T22:53:40.907426Z","iopub.execute_input":"2022-01-12T22:53:40.907745Z","iopub.status.idle":"2022-01-12T22:53:40.939222Z","shell.execute_reply.started":"2022-01-12T22:53:40.907711Z","shell.execute_reply":"2022-01-12T22:53:40.938494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modeling\n\nIn other project published I showed several models to tackle this problem, however they are based on Bag-of-words embedding method and as we know currently they are a bit obsolete because do not apply an attention mechanism which has proven to be very powerful and makes the model understand the meaning of sentences. This is why in the current step we will build, train and compare the following \"attention based\" algorithms:\n\n- BERT (Bidirectional Encoder Representation from Transformers)\n- RoBERTa (Robustly Optimized BERT Pre-training Approach)\n- DistilBERT (Distilled BERT)\n- XLNet (Generalized Auto-Regressive model)\n\nEach one of the mentioned have its pros and cons, the most preferred and widely used model is the BERT for being the middle term in performance, whereas RoBERTa and XLNet are known for their better error metrics and DistilBERT for its faster training. We will consider all of these characteristics and choose the best one for our dataset.\n\nWe will start by installing the transformers library and importing the functions needed.","metadata":{"id":"91csd0qnOM9F"}},{"cell_type":"code","source":"!pip install transformers","metadata":{"id":"eO7yKYAYXEdw","outputId":"0520daa6-4659-48fd-ba44-e41f78fc2606","execution":{"iopub.status.busy":"2022-01-12T21:58:16.919171Z","iopub.execute_input":"2022-01-12T21:58:16.919422Z","iopub.status.idle":"2022-01-12T21:58:26.133103Z","shell.execute_reply.started":"2022-01-12T21:58:16.919387Z","shell.execute_reply":"2022-01-12T21:58:26.132209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then what we need from tensorflow.keras:","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import Input, Dropout, Dense\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.initializers import TruncatedNormal\nfrom tensorflow.keras.losses import CategoricalCrossentropy\nfrom tensorflow.keras.metrics import CategoricalAccuracy\nfrom tensorflow.keras.utils import to_categorical\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split","metadata":{"id":"0WxByEPhN0A9","execution":{"iopub.status.busy":"2022-01-12T21:58:26.135328Z","iopub.execute_input":"2022-01-12T21:58:26.135637Z","iopub.status.idle":"2022-01-12T21:58:26.469583Z","shell.execute_reply.started":"2022-01-12T21:58:26.135603Z","shell.execute_reply":"2022-01-12T21:58:26.468818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we have to gather from the dataset only the two columns useful for training (text and target), then let's create a new column corresponding to our label as categorical which will be useful later.","metadata":{"id":"x5E0gesFavck"}},{"cell_type":"code","source":"# Select required columns\ndata = df[['text', 'target']]\n\n# Set your model output as categorical and save in new label col\ndata['target_label'] = pd.Categorical(data['target'])\n\n# Transform your output to numeric\ndata['target'] = data['target_label'].cat.codes","metadata":{"id":"3bz0aa9zNz3M","execution":{"iopub.status.busy":"2022-01-12T22:01:23.983216Z","iopub.execute_input":"2022-01-12T22:01:23.983496Z","iopub.status.idle":"2022-01-12T22:01:23.995431Z","shell.execute_reply.started":"2022-01-12T22:01:23.983465Z","shell.execute_reply":"2022-01-12T22:01:23.994479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# BERT\n\nAs first step we have to import the Model, Config and Tokenizer corresponding to Bert in order to build properly the model.","metadata":{"id":"2FpEoKeH1bW8"}},{"cell_type":"code","source":"from transformers import TFBertModel,  BertConfig, BertTokenizerFast","metadata":{"id":"sKz3PSFc1alE","execution":{"iopub.status.busy":"2022-01-12T22:01:30.026147Z","iopub.execute_input":"2022-01-12T22:01:30.026918Z","iopub.status.idle":"2022-01-12T22:01:31.743037Z","shell.execute_reply.started":"2022-01-12T22:01:30.026868Z","shell.execute_reply":"2022-01-12T22:01:31.742292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model we will use is 'bert_base_uncased' and the max_length chosen is 45 in order to cover even the longest possible sequence, also because the number of instances in the dataset is relatively small the training will not take too much time.","metadata":{"id":"l06aRxCfb_gg"}},{"cell_type":"code","source":"### --------- Setup BERT ---------- ###\n\n# Name of the BERT model to use\nmodel_name = 'bert-base-uncased'\n\n# Max length of tokens\nmax_length = 45\n\n# Load transformers config and set output_hidden_states to False\nconfig = BertConfig.from_pretrained(model_name)\nconfig.output_hidden_states = False\n\n# Load BERT tokenizer\ntokenizer = BertTokenizerFast.from_pretrained(pretrained_model_name_or_path = model_name, config = config)\n\n# Load the Transformers BERT model\ntransformer_bert_model = TFBertModel.from_pretrained(model_name, config = config)","metadata":{"id":"XT7W2ZCRNztg","outputId":"5a133c6b-8ca1-4457-c583-66c788eca6d0","execution":{"iopub.status.busy":"2022-01-12T22:02:24.708681Z","iopub.execute_input":"2022-01-12T22:02:24.709271Z","iopub.status.idle":"2022-01-12T22:02:51.651581Z","shell.execute_reply.started":"2022-01-12T22:02:24.709232Z","shell.execute_reply":"2022-01-12T22:02:51.650694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that our model has been loaded we can start the processes of building and  tuning according to our dataset and task using the functional API of keras.\n\nAs we see below the input layer must consider the max_length of sequences and then this is fed to the bert model, a dropout layer to reduce overfitting (0.1) and finally a dense layer with number of neurons equal to number of classes in our label (2).","metadata":{"id":"1u_e_aLkcylz"}},{"cell_type":"code","source":"# Load the MainLayer\nbert = transformer_bert_model.layers[0]\n\n# Build your model input\ninput_ids = Input(shape=(max_length,), name='input_ids', dtype='int32')\ninputs = {'input_ids': input_ids}\n\n# Load the Transformers BERT model as a layer in a Keras model\nbert_model = bert(inputs)[1]\ndropout = Dropout(config.hidden_dropout_prob, name='pooled_output')\npooled_output = dropout(bert_model, training=False)\n\n# Then build your model output\ntargets = Dense(units=len(data.target_label.value_counts()), kernel_initializer=TruncatedNormal(stddev=config.initializer_range), name='target')(pooled_output)\noutputs = {'target': targets}\n\n# And combine it all in a model object\nmodel = Model(inputs=inputs, outputs=outputs, name='BERT_Binary_Classifier')\n\n# Take a look at the model\nmodel.summary()","metadata":{"id":"t6JmujgbNzf5","outputId":"85678e63-60dc-4c00-904e-9341c64b6a7c","execution":{"iopub.status.busy":"2022-01-12T22:02:51.654602Z","iopub.execute_input":"2022-01-12T22:02:51.654798Z","iopub.status.idle":"2022-01-12T22:02:57.927394Z","shell.execute_reply.started":"2022-01-12T22:02:51.654774Z","shell.execute_reply":"2022-01-12T22:02:57.92669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The next cell considers the model training and we have to set the optimizer, the loss function as categorical crossentropy and accuracy as metric, as final step we take these as features of the model.compile function.","metadata":{"id":"Wvms3ZLLgV8E"}},{"cell_type":"code","source":"### ------- Train the model ------- ###\n\nfrom tensorflow.keras.optimizers import RMSprop,Adam,SGD,Adadelta\n\noptimizer = Adam(learning_rate=6e-05,epsilon=1e-08,decay=0.01,clipnorm=1.0)\n\n# Set loss and metrics\nloss = {'target': CategoricalCrossentropy(from_logits = True)}\n\n# Compile the model\nmodel.compile(optimizer = optimizer, loss = loss, metrics = ['accuracy'])\n\n# Ready output data for the model\ny_target = to_categorical(data['target'])\n\n# Tokenize the input (takes some time)\nx_train = tokenizer(\n            text=data['text'].to_list(),\n            add_special_tokens=True,\n            max_length=max_length,\n            truncation=True,\n            padding=True, \n            return_tensors='tf',\n            return_token_type_ids = False,\n            return_attention_mask = True,\n            verbose = True)\n\n# Fit the model\nhistory = model.fit(\n    x={'input_ids': x_train['input_ids']},\n    y={'target': y_target},\n    validation_split=0.25,\n    batch_size=64,\n    epochs=1,\n    verbose=1)","metadata":{"id":"ZlNwyL0RPje3","outputId":"48a49835-a8fa-44ea-f8b5-95a99e973157","execution":{"iopub.status.busy":"2022-01-12T22:04:36.298616Z","iopub.execute_input":"2022-01-12T22:04:36.29922Z","iopub.status.idle":"2022-01-12T22:05:33.11669Z","shell.execute_reply.started":"2022-01-12T22:04:36.299185Z","shell.execute_reply":"2022-01-12T22:05:33.115943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model took 56 seconds and had following characteristics: train/test accuracy of 75.1%/83.5%, val_size=25%, Adam and 1 epoch, which makes sense and we expect such performance for these complex models, almost no disadvantages as it trained so fast because of the small dataset.","metadata":{"id":"O2Gb1W9Qts0o"}},{"cell_type":"markdown","source":"## Inference","metadata":{"id":"X-RFqOfZGaTG"}},{"cell_type":"markdown","source":"Finally the following cells are to compute the label predicted of the test (out-of-bag) instances.","metadata":{"id":"zjMx1oZ4hGV9"}},{"cell_type":"code","source":"x_test = tokenizer(\n          text=df_test['text'].to_list(),\n          add_special_tokens=True,\n          max_length=max_length,\n          truncation=True,\n          padding=True, \n          return_tensors='tf',\n          return_token_type_ids = False,\n          return_attention_mask = True,\n          verbose = True)","metadata":{"id":"GPOnqa6HKxvG","execution":{"iopub.status.busy":"2022-01-12T22:05:50.657208Z","iopub.execute_input":"2022-01-12T22:05:50.657473Z","iopub.status.idle":"2022-01-12T22:05:50.897371Z","shell.execute_reply.started":"2022-01-12T22:05:50.657445Z","shell.execute_reply":"2022-01-12T22:05:50.896645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_predicted = model.predict(\n    x={'input_ids': x_test['input_ids']},\n)","metadata":{"id":"bEgdTcEXPjSB","execution":{"iopub.status.busy":"2022-01-12T22:05:54.712488Z","iopub.execute_input":"2022-01-12T22:05:54.71308Z","iopub.status.idle":"2022-01-12T22:06:04.855481Z","shell.execute_reply.started":"2022-01-12T22:05:54.713041Z","shell.execute_reply":"2022-01-12T22:06:04.854706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"label_predicted contains a key which is 'target' same name as our actual label, if we show the array contained it corresponds to a matrix predictions for each instance where the highest in each row is the class predicted, therefore we have to apply argmax, firstly let us see such matrix predicted:","metadata":{"id":"77F_0-zRvK_y"}},{"cell_type":"code","source":"label_predicted['target']","metadata":{"id":"WKU1leT4YNgy","outputId":"cf2e3991-6245-432b-a15e-c7e4d9c6ab9f","execution":{"iopub.status.busy":"2022-01-12T22:06:04.85939Z","iopub.execute_input":"2022-01-12T22:06:04.859607Z","iopub.status.idle":"2022-01-12T22:06:04.869134Z","shell.execute_reply.started":"2022-01-12T22:06:04.859582Z","shell.execute_reply":"2022-01-12T22:06:04.867839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_pred_max=[np.argmax(i) for i in label_predicted['target']]","metadata":{"id":"Jof0eQtKn03c","execution":{"iopub.status.busy":"2022-01-12T22:06:14.878926Z","iopub.execute_input":"2022-01-12T22:06:14.87918Z","iopub.status.idle":"2022-01-12T22:06:14.895991Z","shell.execute_reply.started":"2022-01-12T22:06:14.879152Z","shell.execute_reply":"2022-01-12T22:06:14.895263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_pred_max[:10]","metadata":{"id":"dEgXfOtYLKyf","outputId":"bee037d7-077f-4cf5-b86a-c7c92b782399","execution":{"iopub.status.busy":"2022-01-12T22:06:15.223017Z","iopub.execute_input":"2022-01-12T22:06:15.223273Z","iopub.status.idle":"2022-01-12T22:06:15.228618Z","shell.execute_reply.started":"2022-01-12T22:06:15.223244Z","shell.execute_reply":"2022-01-12T22:06:15.227934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will build the next 3 models the same way as the previous one, notice there are some lines which includes extra functions proper for the model:","metadata":{"id":"XTByuGy5HSfy"}},{"cell_type":"markdown","source":"# RoBERTa","metadata":{"id":"tEeVaR0T3k4v"}},{"cell_type":"code","source":"from transformers import RobertaTokenizer, TFRobertaModel, RobertaConfig ","metadata":{"id":"4SW5OLfJ3k4z","execution":{"iopub.status.busy":"2022-01-12T22:07:40.807117Z","iopub.execute_input":"2022-01-12T22:07:40.807432Z","iopub.status.idle":"2022-01-12T22:07:40.853711Z","shell.execute_reply.started":"2022-01-12T22:07:40.807404Z","shell.execute_reply":"2022-01-12T22:07:40.853004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### --------- Setup Roberta ---------- ###\n\nmodel_name = 'roberta-base'\n\n# Max length of tokens\nmax_length = 45\n\n# Load transformers config and set output_hidden_states to False\nconfig = RobertaConfig.from_pretrained(model_name)\nconfig.output_hidden_states = False\n\n# Load Roberta tokenizer\ntokenizer = RobertaTokenizer.from_pretrained(pretrained_model_name_or_path = model_name, config = config)\n\n# Load the Roberta model\ntransformer_roberta_model = TFRobertaModel.from_pretrained(model_name, config = config)","metadata":{"outputId":"e423a93b-f3db-49ae-d7de-08634f775686","id":"3xfOae3K3PK6","execution":{"iopub.status.busy":"2022-01-12T22:07:46.298786Z","iopub.execute_input":"2022-01-12T22:07:46.299335Z","iopub.status.idle":"2022-01-12T22:08:11.689046Z","shell.execute_reply.started":"2022-01-12T22:07:46.299297Z","shell.execute_reply":"2022-01-12T22:08:11.688376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### ------- Build the model ------- ###\n\n# Load the MainLayer\nroberta = transformer_roberta_model.layers[0]\n\n# Build your model input\ninput_ids = Input(shape=(max_length,), name='input_ids', dtype='int32')\ninputs = {'input_ids': input_ids}\n\n# Load the Transformers RoBERTa model as a layer in a Keras model\nroberta_model = roberta(inputs)[1]\ndropout = Dropout(config.hidden_dropout_prob, name='pooled_output')\npooled_output = dropout(roberta_model, training=False)\n\n# Then build your model output\ntargets = Dense(units=len(data.target_label.value_counts()), kernel_initializer=TruncatedNormal(stddev=config.initializer_range), name='target')(pooled_output)\noutputs = {'target': targets}\n\n# And combine it all in a model object\nmodel2 = Model(inputs=inputs, outputs=outputs, name='RoBERTa_Binary_Classifier')\n\n# Take a look at the model\nmodel2.summary()","metadata":{"id":"jWyAgPfB4r5t","outputId":"f3b9f751-bf2a-48df-c900-238d19beea85","execution":{"iopub.status.busy":"2022-01-12T22:08:11.69074Z","iopub.execute_input":"2022-01-12T22:08:11.69099Z","iopub.status.idle":"2022-01-12T22:08:15.746941Z","shell.execute_reply.started":"2022-01-12T22:08:11.690956Z","shell.execute_reply":"2022-01-12T22:08:15.746108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### ------- Train the model ------- ###\n\noptimizer = Adam(learning_rate=6e-05,epsilon=1e-08,decay=0.01,clipnorm=1.0)\n\n# Set loss and metrics\nloss = {'target': CategoricalCrossentropy(from_logits = True)}\n\n# Compile the model\nmodel2.compile(optimizer = optimizer, loss = loss, metrics = ['accuracy'])\n\n# Ready output data for the model\ny_target = to_categorical(data['target'])\n\n# Tokenize the input (takes some time)\nx_train = tokenizer(\n            text=data['text'].to_list(),\n            add_special_tokens=True,\n            max_length=max_length,\n            truncation=True,\n            padding=True, \n            return_tensors='tf',\n            return_token_type_ids = False,\n            return_attention_mask = True,\n            verbose = True)\n\n# Fit the model\nhistory = model2.fit(\n    x={'input_ids': x_train['input_ids']},\n    y={'target': y_target},\n    validation_split=0.25,\n    batch_size=64,\n    epochs=3,\n    verbose=1)","metadata":{"outputId":"3b76beaa-f9f9-4c40-9ddd-166b19c18fa8","id":"C6uU6Cy43k5D","execution":{"iopub.status.busy":"2022-01-12T22:08:22.145363Z","iopub.execute_input":"2022-01-12T22:08:22.146103Z","iopub.status.idle":"2022-01-12T22:10:32.61979Z","shell.execute_reply.started":"2022-01-12T22:08:22.146068Z","shell.execute_reply":"2022-01-12T22:10:32.619085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model took 2 minutes 8 seconds and had following characteristics: train/test accuracy of 85.0%/82.7%, val_size=25%, Adam and 3 epochs.","metadata":{"id":"w27gjtf_3k5G"}},{"cell_type":"markdown","source":"## Inference","metadata":{"id":"dh2fWpX5HvN2"}},{"cell_type":"code","source":"x_test = tokenizer(\n          text=df_test['text'].to_list(),\n          add_special_tokens=True,\n          max_length=max_length,\n          truncation=True,\n          padding=True, \n          return_tensors='tf',\n          return_token_type_ids = False,\n          return_attention_mask = True,\n          verbose = True)","metadata":{"id":"yrkmA0r03k5I","execution":{"iopub.status.busy":"2022-01-12T22:10:43.305286Z","iopub.execute_input":"2022-01-12T22:10:43.305757Z","iopub.status.idle":"2022-01-12T22:10:44.368793Z","shell.execute_reply.started":"2022-01-12T22:10:43.305719Z","shell.execute_reply":"2022-01-12T22:10:44.368004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_predicted = model2.predict(\n    x={'input_ids': x_test['input_ids']},\n)","metadata":{"id":"CwayzWdV3k5L","execution":{"iopub.status.busy":"2022-01-12T22:10:44.370576Z","iopub.execute_input":"2022-01-12T22:10:44.370894Z","iopub.status.idle":"2022-01-12T22:10:54.438818Z","shell.execute_reply.started":"2022-01-12T22:10:44.370856Z","shell.execute_reply":"2022-01-12T22:10:54.437931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_predicted['target']","metadata":{"outputId":"1631c2b3-14f9-4a46-b848-90ac64ba01a5","id":"HWCiRohB3k5N","execution":{"iopub.status.busy":"2022-01-12T22:10:54.440327Z","iopub.execute_input":"2022-01-12T22:10:54.440597Z","iopub.status.idle":"2022-01-12T22:10:54.449167Z","shell.execute_reply.started":"2022-01-12T22:10:54.44056Z","shell.execute_reply":"2022-01-12T22:10:54.448245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_pred_max=[np.argmax(i) for i in label_predicted['target']]","metadata":{"id":"81cAFj0F3k5O","execution":{"iopub.status.busy":"2022-01-12T22:10:57.541182Z","iopub.execute_input":"2022-01-12T22:10:57.54188Z","iopub.status.idle":"2022-01-12T22:10:57.558685Z","shell.execute_reply.started":"2022-01-12T22:10:57.541841Z","shell.execute_reply":"2022-01-12T22:10:57.557964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_pred_max[:10]","metadata":{"id":"yxdkMKCF3k5P","outputId":"22ff2b30-34e1-40ae-edf7-d7f24d109235","execution":{"iopub.status.busy":"2022-01-12T22:10:57.975438Z","iopub.execute_input":"2022-01-12T22:10:57.975882Z","iopub.status.idle":"2022-01-12T22:10:57.984642Z","shell.execute_reply.started":"2022-01-12T22:10:57.975832Z","shell.execute_reply":"2022-01-12T22:10:57.983453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DistilBERT","metadata":{"id":"eTVWFjOcCY8t"}},{"cell_type":"code","source":"from transformers import DistilBertTokenizer, TFDistilBertModel, DistilBertConfig ","metadata":{"id":"6mRmY3QSChdA","execution":{"iopub.status.busy":"2022-01-12T22:12:09.697967Z","iopub.execute_input":"2022-01-12T22:12:09.698228Z","iopub.status.idle":"2022-01-12T22:12:09.708967Z","shell.execute_reply.started":"2022-01-12T22:12:09.6982Z","shell.execute_reply":"2022-01-12T22:12:09.708235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### --------- Setup DistilBERT ---------- ###\n\nmodel_name = 'distilbert-base-uncased'\n\n# Max length of tokens\nmax_length = 45\n\n# Load transformers config and set output_hidden_states to False\nconfig = DistilBertConfig.from_pretrained(model_name)\nconfig.output_hidden_states = False\n\n# Load Distilbert tokenizer\ntokenizer = DistilBertTokenizer.from_pretrained(pretrained_model_name_or_path = model_name, config = config)\n\n# Load the Distilbert model\ntransformer_distilbert_model = TFDistilBertModel.from_pretrained(model_name, config = config)","metadata":{"id":"MrTQFDqrChdD","outputId":"76e469f7-cd60-480b-a466-d47d627ac61f","execution":{"iopub.status.busy":"2022-01-12T22:12:10.134359Z","iopub.execute_input":"2022-01-12T22:12:10.134959Z","iopub.status.idle":"2022-01-12T22:12:25.227483Z","shell.execute_reply.started":"2022-01-12T22:12:10.134923Z","shell.execute_reply":"2022-01-12T22:12:25.226839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"DistilBERT does not consider a pooling layer in the default model which converts the output (None,45,768) to (None,768), this is why we will select the first and third dimension of the 'layer 0' so as to have such output shape required, the next layers are the same as before:","metadata":{}},{"cell_type":"code","source":"# Load the MainLayer\ndistilbert = transformer_distilbert_model.layers[0]\n\n# Build your model input\ninput_ids = Input(shape=(max_length,), name='input_ids', dtype='int32')\ninputs = {'input_ids': input_ids}\n\n# Load the Transformers DistilBERT model as a layer in a Keras model\ndistilbert_model = distilbert(inputs)[0][:,0,:]\ndropout = Dropout(0.1, name='pooled_output')\npooled_output = dropout(distilbert_model, training=False)\n\n# Then build your model output\ntargets = Dense(units=len(data.target_label.value_counts()), kernel_initializer=TruncatedNormal(stddev=config.initializer_range), name='target')(pooled_output)\noutputs = {'target': targets}\n\n# And combine it all in a model object\nmodel3 = Model(inputs=inputs, outputs=outputs, name='DistilBERT_Binary_Classifier')\n\n# Take a look at the model\nmodel3.summary()","metadata":{"outputId":"72180d59-a27d-4a6e-f79e-c331c8c4b776","id":"qntTp9w4ChdG","execution":{"iopub.status.busy":"2022-01-12T22:12:25.229926Z","iopub.execute_input":"2022-01-12T22:12:25.230333Z","iopub.status.idle":"2022-01-12T22:12:27.548258Z","shell.execute_reply.started":"2022-01-12T22:12:25.230291Z","shell.execute_reply":"2022-01-12T22:12:27.547569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### ------- Train the model ------- ###\n\n# Set an optimizer\noptimizer = Adam(learning_rate=6e-05,epsilon=1e-08,decay=0.01,clipnorm=1.0)\n\n# Set loss and metrics\nloss = {'target': CategoricalCrossentropy(from_logits = True)}\n\n# Compile the model\nmodel3.compile(optimizer = optimizer, loss = loss, metrics = ['accuracy'])\n\n# Ready output data for the model\ny_target = to_categorical(data['target'])\n\n# Tokenize the input (takes some time)\nx_train = tokenizer(\n    text=data['text'].to_list(),\n    add_special_tokens=True,\n    max_length=max_length,\n    truncation=True,\n    padding=True, \n    return_tensors='tf',\n    return_token_type_ids = False,\n    return_attention_mask = True,\n    verbose = True)\n\n# Fit the model\nhistory = model3.fit(\n    x={'input_ids': x_train['input_ids']},\n    y={'target': y_target},\n    validation_split=0.25,\n    batch_size=64,\n    epochs=1,\n    verbose=1)","metadata":{"id":"sKXLBXBQChdK","outputId":"ef6cb4df-785a-4d82-e237-847871a6286b","execution":{"iopub.status.busy":"2022-01-12T22:12:27.549535Z","iopub.execute_input":"2022-01-12T22:12:27.549819Z","iopub.status.idle":"2022-01-12T22:13:00.988004Z","shell.execute_reply.started":"2022-01-12T22:12:27.549782Z","shell.execute_reply":"2022-01-12T22:13:00.987223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model took 28 seconds and had following characteristics: train/test accuracy of 79.4%/82.7%, val_size=25%, Adam and 1 epoch.","metadata":{"id":"mZmCFt-V-EpQ"}},{"cell_type":"markdown","source":"## Inference","metadata":{"id":"5uLm9PqwIq9O"}},{"cell_type":"code","source":"x_test = tokenizer(\n    text=df_test['text'].to_list(),\n    add_special_tokens=True,\n    max_length=max_length,\n    truncation=True,\n    padding=True, \n    return_tensors='tf',\n    return_token_type_ids = False,\n    return_attention_mask = True,\n    verbose = True)","metadata":{"id":"w1yvk2e5-Il2","execution":{"iopub.status.busy":"2022-01-12T22:13:10.581846Z","iopub.execute_input":"2022-01-12T22:13:10.582105Z","iopub.status.idle":"2022-01-12T22:13:12.421354Z","shell.execute_reply.started":"2022-01-12T22:13:10.582076Z","shell.execute_reply":"2022-01-12T22:13:12.420629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_predicted = model3.predict(\n    x={'input_ids': x_test['input_ids']},\n)","metadata":{"id":"ouV3FLwq-Il4","execution":{"iopub.status.busy":"2022-01-12T22:13:12.424708Z","iopub.execute_input":"2022-01-12T22:13:12.424915Z","iopub.status.idle":"2022-01-12T22:13:17.294604Z","shell.execute_reply.started":"2022-01-12T22:13:12.42489Z","shell.execute_reply":"2022-01-12T22:13:17.293846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_predicted['target']","metadata":{"outputId":"3de7ef1a-735d-4c39-c13d-f28af1dc92a2","id":"drV0GXoF-Il6","execution":{"iopub.status.busy":"2022-01-12T22:13:17.298295Z","iopub.execute_input":"2022-01-12T22:13:17.298862Z","iopub.status.idle":"2022-01-12T22:13:17.30519Z","shell.execute_reply.started":"2022-01-12T22:13:17.29882Z","shell.execute_reply":"2022-01-12T22:13:17.304359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_pred_max=[np.argmax(i) for i in label_predicted['target']]","metadata":{"id":"8-R7Q5eW-Il7","execution":{"iopub.status.busy":"2022-01-12T22:13:17.307144Z","iopub.execute_input":"2022-01-12T22:13:17.307769Z","iopub.status.idle":"2022-01-12T22:13:17.327172Z","shell.execute_reply.started":"2022-01-12T22:13:17.307725Z","shell.execute_reply":"2022-01-12T22:13:17.326548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_pred_max[:10]","metadata":{"id":"IMvRQL3q-Il8","outputId":"57f55d9f-2343-4ba6-d2b1-8e8c7ce2cff4","execution":{"iopub.status.busy":"2022-01-12T22:13:17.328363Z","iopub.execute_input":"2022-01-12T22:13:17.328684Z","iopub.status.idle":"2022-01-12T22:13:17.334018Z","shell.execute_reply.started":"2022-01-12T22:13:17.328648Z","shell.execute_reply":"2022-01-12T22:13:17.333284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# XLNet\n\nThe tokenizer corresponding to XLNet requires an extra library called sentencepiece which we have to install and import as follows:","metadata":{"id":"hm8E0E9fF-jl"}},{"cell_type":"code","source":"!pip install sentencepiece ","metadata":{"id":"2tMcpkT8WKvb","outputId":"0878dbee-6c6c-4525-f7d5-21e6f37009a6","execution":{"iopub.status.busy":"2022-01-12T22:14:02.781649Z","iopub.execute_input":"2022-01-12T22:14:02.782438Z","iopub.status.idle":"2022-01-12T22:14:10.355773Z","shell.execute_reply.started":"2022-01-12T22:14:02.782389Z","shell.execute_reply":"2022-01-12T22:14:10.354852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import XLNetTokenizer, TFXLNetModel, XLNetConfig\nimport sentencepiece","metadata":{"id":"oCgB1inEF-jo","execution":{"iopub.status.busy":"2022-01-12T22:14:10.359236Z","iopub.execute_input":"2022-01-12T22:14:10.359532Z","iopub.status.idle":"2022-01-12T22:14:10.379777Z","shell.execute_reply.started":"2022-01-12T22:14:10.359488Z","shell.execute_reply":"2022-01-12T22:14:10.379005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### --------- Setup XLNet ---------- ###\n\nmodel_name = 'xlnet-base-cased'\n\n# Max length of tokens\nmax_length = 45\n\n# Load transformers config and set output_hidden_states to False\nconfig = XLNetConfig.from_pretrained(model_name)\nconfig.output_hidden_states = False\n\n# Load XLNet tokenizer\ntokenizer = XLNetTokenizer.from_pretrained(pretrained_model_name_or_path = model_name, config = config)\n\n# Load the XLNet model\ntransformer_xlnet_model = TFXLNetModel.from_pretrained(model_name, config = config)","metadata":{"id":"J6iR6nuZF-jq","outputId":"9253aced-dd4d-49ba-9d6e-20b959497d3d","execution":{"iopub.status.busy":"2022-01-12T22:14:18.053714Z","iopub.execute_input":"2022-01-12T22:14:18.054002Z","iopub.status.idle":"2022-01-12T22:14:40.744286Z","shell.execute_reply.started":"2022-01-12T22:14:18.05397Z","shell.execute_reply":"2022-01-12T22:14:40.743566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Something similar to DistilBERT happens to the current model, because we have to convert the output shape of the default model first layer to the appropriate (None, 768), in this case we will use tf.squeeze function as can be seen below:","metadata":{}},{"cell_type":"code","source":"### ------- Build the model ------- ###\n\n# Load the MainLayer\nxlnet = transformer_xlnet_model.layers[0]\n\n# Build your model input\ninput_ids = Input(shape=(max_length,), name='input_ids', dtype='int32')\ninputs = {'input_ids': input_ids}\n\n# Load the Transformers XLNet model as a layer in a Keras model\nxlnet_model = xlnet(inputs)[0]\nxlnet_model = tf.squeeze(xlnet_model[:, -1:, :], axis=1)\ndropout = Dropout(0.1, name='pooled_output')\npooled_output = dropout(xlnet_model, training=False)\n\n# Then build your model output\ntargets = Dense(units=len(data.target_label.value_counts()), kernel_initializer=TruncatedNormal(stddev=config.initializer_range), name='target')(pooled_output)\noutputs = {'target': targets}\n\n# And combine it all in a model object\nmodel4 = Model(inputs=inputs, outputs=outputs, name='XLNet_Binary_Classifier')\n\n# Take a look at the model\nmodel4.summary()","metadata":{"id":"j4pitfLF_kOk","outputId":"7c05d354-2f2b-4170-a31f-7b8ad9ccdbb8","execution":{"iopub.status.busy":"2022-01-12T22:14:44.276356Z","iopub.execute_input":"2022-01-12T22:14:44.276914Z","iopub.status.idle":"2022-01-12T22:14:49.692957Z","shell.execute_reply.started":"2022-01-12T22:14:44.276868Z","shell.execute_reply":"2022-01-12T22:14:49.692189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### ------- Train the model ------- ###\n\noptimizer = Adam(learning_rate=6e-05,epsilon=1e-08,decay=0.01,clipnorm=1.0)\n\n# Set loss and metrics\nloss = {'target': CategoricalCrossentropy(from_logits = True)}\n\n# Compile the model\nmodel4.compile(optimizer = optimizer, loss = loss, metrics = ['accuracy'])\n\n# Ready output data for the model\ny_target = to_categorical(data['target'])\n\n# Tokenize the input (takes some time)\nx_train = tokenizer(\n            text=data['text'].to_list(),\n            add_special_tokens=True,\n            max_length=max_length,\n            truncation=True,\n            padding=True, \n            return_tensors='tf',\n            return_token_type_ids = False,\n            return_attention_mask = True,\n            verbose = True)\n\n# Fit the model\nhistory = model4.fit(\n    x={'input_ids': x_train['input_ids']},\n    y={'target': y_target},\n    validation_split=0.25,\n    batch_size=64,\n    epochs=1,\n    verbose=1)","metadata":{"outputId":"abfe6e13-6b59-4d34-ea4f-826034de7a81","id":"1PuDnCXvAPZr","execution":{"iopub.status.busy":"2022-01-12T22:14:53.898132Z","iopub.execute_input":"2022-01-12T22:14:53.898987Z","iopub.status.idle":"2022-01-12T22:16:34.522326Z","shell.execute_reply.started":"2022-01-12T22:14:53.89894Z","shell.execute_reply":"2022-01-12T22:16:34.521579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model took 59 seconds and had following characteristics: train/test accuracy of 76.0%/82.3%, val_size=25%, Adam and 1 epoch.","metadata":{"id":"Bs3CU_SXAPZw"}},{"cell_type":"markdown","source":"## Inference","metadata":{"id":"7YhZmqoeI4sp"}},{"cell_type":"code","source":"x_test = tokenizer(\n    text=df_test['text'].to_list(),\n    add_special_tokens=True,\n    max_length=max_length,\n    truncation=True,\n    padding=True, \n    return_tensors='tf',\n    return_token_type_ids = False,\n    return_attention_mask = True,\n    verbose = True)","metadata":{"id":"sXlfjlOiAzxu","execution":{"iopub.status.busy":"2022-01-12T22:16:34.524847Z","iopub.execute_input":"2022-01-12T22:16:34.525102Z","iopub.status.idle":"2022-01-12T22:16:35.16397Z","shell.execute_reply.started":"2022-01-12T22:16:34.525069Z","shell.execute_reply":"2022-01-12T22:16:35.163134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_predicted = model4.predict(\n    x={'input_ids': x_test['input_ids']},\n)","metadata":{"id":"mBeJVeT7Azxw","execution":{"iopub.status.busy":"2022-01-12T22:16:35.165285Z","iopub.execute_input":"2022-01-12T22:16:35.16558Z","iopub.status.idle":"2022-01-12T22:16:45.163937Z","shell.execute_reply.started":"2022-01-12T22:16:35.165542Z","shell.execute_reply":"2022-01-12T22:16:45.163156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_predicted['target']","metadata":{"outputId":"2c86f4ed-d091-46c3-d4fe-3d51c76e8b0c","id":"hb9UY-wdAzxz","execution":{"iopub.status.busy":"2022-01-12T22:16:45.16606Z","iopub.execute_input":"2022-01-12T22:16:45.166339Z","iopub.status.idle":"2022-01-12T22:16:45.175178Z","shell.execute_reply.started":"2022-01-12T22:16:45.166302Z","shell.execute_reply":"2022-01-12T22:16:45.174464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_pred_max=[np.argmax(i) for i in label_predicted['target']]","metadata":{"id":"2tW4p3WkAzx0","execution":{"iopub.status.busy":"2022-01-12T22:16:45.176951Z","iopub.execute_input":"2022-01-12T22:16:45.17731Z","iopub.status.idle":"2022-01-12T22:16:45.195287Z","shell.execute_reply.started":"2022-01-12T22:16:45.177207Z","shell.execute_reply":"2022-01-12T22:16:45.194679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_pred_max[:10]","metadata":{"id":"PHTh34PAAzx1","outputId":"b63ddaae-01eb-4115-e1b0-4696289bcdd3","execution":{"iopub.status.busy":"2022-01-12T22:16:45.196451Z","iopub.execute_input":"2022-01-12T22:16:45.196923Z","iopub.status.idle":"2022-01-12T22:16:45.202829Z","shell.execute_reply.started":"2022-01-12T22:16:45.196879Z","shell.execute_reply":"2022-01-12T22:16:45.20193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Submission with pre-trained model:","metadata":{"id":"Xs-Mk0KKAzx2"}},{"cell_type":"code","source":"sample_submission['target'] = label_pred_max\nsample_submission.head(10)","metadata":{"outputId":"7fc19a34-e032-4d44-b87a-a7db3316a27a","id":"4A8gXtcqAzx3","execution":{"iopub.status.busy":"2022-01-12T22:17:06.05284Z","iopub.execute_input":"2022-01-12T22:17:06.053132Z","iopub.status.idle":"2022-01-12T22:17:06.064713Z","shell.execute_reply.started":"2022-01-12T22:17:06.053102Z","shell.execute_reply":"2022-01-12T22:17:06.063929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission.to_csv(\"submission.csv\", index=False, header=True)","metadata":{"id":"DOd1QK33Azx8","execution":{"iopub.status.busy":"2022-01-12T22:17:06.770124Z","iopub.execute_input":"2022-01-12T22:17:06.770381Z","iopub.status.idle":"2022-01-12T22:17:06.783326Z","shell.execute_reply.started":"2022-01-12T22:17:06.770352Z","shell.execute_reply":"2022-01-12T22:17:06.78249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Discussion\n\nIn general the performance of the four models was similar, also based on the experience from the previous project we can endorse the idea that BERT is the middle term of trade-off between accuracy and training time, whereas DistilBERT was the fastest by far, but having a lower accuracy than the previous as is explained by HuggingFace it achieves 95% accuracy of BERT, finally RoBERTa and XLNet were the slowest models but not the highest accuracies as it corresponds to BERT (validation acc).\n\nI have submitted the predition of the testing set for all models and the best one was BERT reaching 83.12% of accuracy and the lowest was DistilBERT reaching 81.98%. We can say there is a slight difference but as we are dealing with a small sized dataset such gap becomes bigger and more important. As in the previous project the main reason of the misclassifications is because the label is unbalanced (slightly though), but it makes our prediction to sidetrack, therefore we should have to find a proper method to solve such problem either oversampling or undersampling.\n\nAnother possible reason of the limited accuracy no matter which model we use is the vocabulary as the cleaning process didn't cover all misspelled words, idioms and weird acronyms widely used (as people normally communicate informally) makes our model understand poorly some sentences. Obviously if we try to take down such problem the cleaning process would take a long time to do because we have to look at each sentence and define all posible confusing words.\n\nAlso I have to inform that I have trained each model for more epochs but the unique improvement was in training accuracy whereas validation stayed the same or even decreased, I would really encourage you to try with more epochs and compare their performance having previously done a wider cleaning process so as to be sure it will work better.","metadata":{}},{"cell_type":"markdown","source":"I would like to know any feedback in order to increase the performance of the models or tell me if you found a different one even better!\n\nIf you liked this notebook I would appreciate so much your upvote if you want to see more projects/tutorials like this one. I encourage you to see my projects portfolio, am sure you will love it.\n\nThank you!","metadata":{}},{"cell_type":"code","source":"","metadata":{"id":"NztzBVXWdGMM"},"execution_count":null,"outputs":[]}]}