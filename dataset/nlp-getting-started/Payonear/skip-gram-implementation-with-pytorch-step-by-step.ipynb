{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# <center> Diving into Word2Vec basics. Skip-Gram implementation with Pytorch. <center>\n##### <center> By Artur Kolishenko (@payonear)"},{"metadata":{},"cell_type":"markdown","source":"### Structure\n1. [Import libraries](#Import-libraries)\n3. [Skip-Gram example with PyTorch](#Skip-Gram-example-with-PyTorch)\n6. [Summary](#Summary)"},{"metadata":{},"cell_type":"markdown","source":"Word Embeddings are extremely important for NLP tasks. Ready-to-use solutions like Glove, FastText etc. are very useful and relatively efficient, so why wasting time on Skip-Gram, which works empirically worse and need much time to fit? The goal of this notebook is to manually implement this model for deeper understanding of Word2Vec mechanism to prepare myself and probably readers for diving deeper into more complex solutions. In the notebook I'll use PyTorch, but if you're not familiar with it, no problem, some comments for understanding the code are provided. Nevertheless, it's assumed the one is familiar with basic Neural Nets concepts. Besides, I'll not discuss here why we need Word2Vec as there are much useful info on this topic throughout internet."},{"metadata":{},"cell_type":"markdown","source":"### Import libraries"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import torch\ntorch.manual_seed(10)\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nimport pandas as pd\nimport numpy as np\nfrom sklearn import decomposition\nfrom pathlib import Path\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nplt.rcParams['figure.figsize'] = (10,8)\nimport nltk\n#Import stopwords\nfrom nltk.corpus import stopwords","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Skip-Gram example with PyTorch"},{"metadata":{},"cell_type":"markdown","source":"Consider we have a simplified corpus of words like below."},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus = [\n    'drink milk',\n    'drink cold water',\n    'drink cold cola',\n    'drink juice',\n    'drink cola',\n    'eat bacon',\n    'eat mango',\n    'eat cherry',\n    'eat apple',\n    'juice with sugar',\n    'cola with sugar',\n    'mango is fruit',\n    'apple is fruit',\n    'cherry is fruit',\n    'Berlin is Germany',\n    'Boston is USA',\n    'Mercedes from Germany',\n    'Mercedes is a car',\n    'Ford from USA',\n    'Ford is a car'\n]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Skip-Gram model tries to predict context given a word. So as input it expect word and as output words which often appears with the inputed one. Below I implement some suportive functions."},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_vocabulary(corpus):\n    '''Creates a dictionary with all unique words in corpus with id'''\n    vocabulary = {}\n    i = 0\n    for s in corpus:\n        for w in s.split():\n            if w not in vocabulary:\n                vocabulary[w] = i\n                i+=1\n    return vocabulary\n\ndef prepare_set(corpus, n_gram = 1):\n    '''Creates a dataset with Input column and Outputs columns for neighboring words. \n       The number of neighbors = n_gram*2'''\n    columns = ['Input'] + [f'Output{i+1}' for i in range(n_gram*2)]\n    result = pd.DataFrame(columns = columns)\n    for sentence in corpus:\n        for i,w in enumerate(sentence.split()):\n            inp = [w]\n            out = []\n            for n in range(1,n_gram+1):\n                # look back\n                if (i-n)>=0:\n                    out.append(sentence.split()[i-n])\n                else:\n                    out.append('<padding>')\n                \n                # look forward\n                if (i+n)<len(sentence.split()):\n                    out.append(sentence.split()[i+n])\n                else:\n                    out.append('<padding>')\n            row = pd.DataFrame([inp+out], columns = columns)\n            result = result.append(row, ignore_index = True)\n    return result\n\ndef prepare_set_ravel(corpus, n_gram = 1):\n    '''Creates a dataset with Input column and Output column for neighboring words. \n       The number of neighbors = n_gram*2'''\n    columns = ['Input', 'Output']\n    result = pd.DataFrame(columns = columns)\n    for sentence in corpus:\n        for i,w in enumerate(sentence.split()):\n            inp = w\n            for n in range(1,n_gram+1):\n                # look back\n                if (i-n)>=0:\n                    out = sentence.split()[i-n]\n                    row = pd.DataFrame([[inp,out]], columns = columns)\n                    result = result.append(row, ignore_index = True)\n                \n                # look forward\n                if (i+n)<len(sentence.split()):\n                    out = sentence.split()[i+n]\n                    row = pd.DataFrame([[inp,out]], columns = columns)\n                    result = result.append(row, ignore_index = True)\n    return result","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A bit of preprocessing."},{"metadata":{"trusted":true},"cell_type":"code","source":"stop_words = set(stopwords.words('english'))\n\ndef preprocess(corpus):\n    result = []\n    for i in corpus:\n        out = nltk.word_tokenize(i)\n        out = [x.lower() for x in out]\n        out = [x for x in out if x not in stop_words]\n        result.append(\" \". join(out))\n    return result\n\ncorpus = preprocess(corpus)\ncorpus","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we are creating a vocabulary which gives id for each word appearing in corpus. "},{"metadata":{"trusted":true},"cell_type":"code","source":"vocabulary = create_vocabulary(corpus)\nvocabulary","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below we can observe the logic. We are taking two neighbors from each side of center word. We can see many padding tokens, that is because maximal length of our sentences is 3, which is why each word will have at least two neighbors being padding. It's done just for presentation purposes. Below you can see some plots, which will help to understand the logic better."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_emb = prepare_set(corpus, n_gram = 2)\ntrain_emb.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![caption](https://miro.medium.com/max/335/1*uYiqfNrUIzkdMrmkBWGMPw.png)"},{"metadata":{},"cell_type":"markdown","source":"Let's put it in form needed for training."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_emb = prepare_set_ravel(corpus, n_gram = 2)\ntrain_emb.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, replace words with their indexes."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_emb.Input = train_emb.Input.map(vocabulary)\ntrain_emb.Output = train_emb.Output.map(vocabulary)\ntrain_emb.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ok, now we have some input and output, what does it mean? Time to build a model. The architecture of Skip-Gram model is pretty simple. We have One-Hot encoded vector as an input and One-Hot encoded vector as an output. Which means input and output layers are vectors of length which equals the length of vocabulary, where almost all elements are zeros except one which let us identify which word from vocabulary is encoded. In between of Input and Output there is a hidden layer of length we choose. The length of hidden layer predefines the dimension of embedding vectors. The most interesting elements of this NN are weights between hidden layer and two other. Multiplicatin of One-Hot encoded vector with matrix of weights will activate just one corresponding row of this weights matrix. In other words one step of Neural Net will optimize only one row which corresponds to one particular word from vocabulary. This allow to use vectors from weights matrix as vector representations of words afterwards. "},{"metadata":{},"cell_type":"markdown","source":"![caption](https://www.researchgate.net/publication/322905432/figure/fig1/AS:614314310373461@1523475353979/The-architecture-of-Skip-gram-model-20.png)"},{"metadata":{},"cell_type":"markdown","source":"It's time to define the Loss function. In Skip-Gram we want to predict context by given word. So we want to maximize the following equation:\n\n\\begin{align} \n& max \\prod_{center} \\prod_{context} P(context | center; \\theta) \n\\end{align}\n<br />\n<br />\nWe want to maximize it, because we're interested in maximizng of $P(context|center)$ for each `context` `center` pair. But neural nets don't like to maximize, but rather minimize. So equation transforms to:\n\n\\begin{align} \n& min -\\prod_{center} \\prod_{context} P(context | center; \\theta) \n\\end{align}\n<br />\n<br />\nAdding logrithm before the equation helps to use it's useful property, concretely:\n\n\\begin{align} \n& min -\\prod_{center} \\prod_{context} log\\;P(context | center; \\theta)\n\\end{align}\n<br />\n<br />\n\\begin{align} \n& log(a * b) = log(a) + log(b) \n\\end{align}\n<br />\n<br />\n\\begin{align} \n& min -\\sum_{center} \\sum_{context} log\\;P(context | center; \\theta) \n\\end{align}\n<br />\n<br />\nIt's left to define $P(context|center; \\theta)$, here Softmax function is used:\n\n\\begin{align} \n& P(context|center) = \\frac{exp(u^T_{context}v_{center})}{\\sum_{\\omega\\in vocab} exp(u^T_{\\omega} v_{center})} \n\\end{align}\n<br />\n<br />\nwhere $u^T_{context}v_{center}$ is a scalar product of vectors $u$ and $v$ (`context`, `center` respectively). Summarizing, the cost or loss function looks like this:\n\n\\begin{align} \n& min -\\sum_{center} \\sum_{context} log\\;\\frac{exp(u^T_{context}v_{center})}{\\sum_{\\omega\\in vocab} exp(u^T_{\\omega} v_{center})} \n\\end{align}\n<br />\n<br />\nThanks to PyTorch developers it contains CrossEntropyLoss funtion which is exactly the funtion above. [See details in PyTorch documentation](https://pytorch.org/docs/stable/nn.html)."},{"metadata":{},"cell_type":"markdown","source":"Let's write some supportive functions."},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size = len(vocabulary)\n\ndef get_input_tensor(tensor):\n    '''Transform 1D tensor of word indexes to one-hot encoded 2D tensor'''\n    size = [*tensor.shape][0]\n    inp = torch.zeros(size, vocab_size).scatter_(1, tensor.unsqueeze(1), 1.)\n    return Variable(inp).float()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We want our words to be represented by vectors consisting of 5 elements. So, `embedding_dims` equals 5. If you want to perform calculations on GPU, just replace `cpu` with GPU id, for example `cuda:1`."},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_dims = 5\ndevice = torch.device('cpu')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, weight initialization. Look above on the architecture of NN. W1 matrix of size **vocab_size $\\times $ embedding_dims**, W2 of shape **embedding_dims $\\times $ vocab_size**. Pay attention we put requires_grad as True, because we want NN to compute gradients for those weights matices for their optimization. Function torch.randn randomly initialize weights. But very important to initialize weights correctly. What does it mean? Weights should be initialized to small random numbers. If you are not careful enough with this step, model can generate unexpected and not useful results. For this `uniform` function is used here, it limits bounds of weights to $(-0.5/$embedding_dims, $0.5/$embedding_dims)."},{"metadata":{"trusted":true},"cell_type":"code","source":"initrange = 0.5 / embedding_dims\nW1 = Variable(torch.randn(vocab_size, embedding_dims, device=device).uniform_(-initrange, initrange).float(), requires_grad=True) # shape V*H\nW2 = Variable(torch.randn(embedding_dims, vocab_size, device=device).uniform_(-initrange, initrange).float(), requires_grad=True) #shape H*V\nprint(f'W1 shape is: {W1.shape}, W2 shape is: {W2.shape}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Initalization of model hyperparams."},{"metadata":{"trusted":true},"cell_type":"code","source":"num_epochs = 2000\nlearning_rate = 2e-1\nlr_decay = 0.99\nloss_hist = []","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here I use DataLoader from PyTorch, it loads data by batches. That's very useful tool, if you are not familiar with it I encourage you to have a look to avoid unnecessary coding. My dataset is very small, so I don't need DataLoader here, it's added just for fun, that's why batch_size equals the dataset's number of rows. I simply use the whole dataset for one iteration, here one iteration == one epoch. Pay attention, that `get_input_tensor` function defined above is used only for input layer, that's because CrossEntropyLoss expect true outputs as vector in long format, provided by DataLoader."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfor epo in range(num_epochs):\n    for x,y in zip(DataLoader(train_emb.Input.values, batch_size=train_emb.shape[0]), DataLoader(train_emb.Output.values, batch_size=train_emb.shape[0])):\n        \n        # one-hot encode input tensor\n        input_tensor = get_input_tensor(x) #shape N*V\n     \n        # simple NN architecture\n        h = input_tensor.mm(W1) # shape 1*H\n        y_pred = h.mm(W2) # shape 1*V\n        \n        # define loss func\n        loss_f = torch.nn.CrossEntropyLoss() # see details: https://pytorch.org/docs/stable/nn.html\n        \n        #compute loss\n        loss = loss_f(y_pred, y)\n        \n        # bakpropagation step\n        loss.backward()\n        \n        # Update weights using gradient descent. For this step we just want to mutate\n        # the values of w1 and w2 in-place; we don't want to build up a computational\n        # graph for the update steps, so we use the torch.no_grad() context manager\n        # to prevent PyTorch from building a computational graph for the updates\n        with torch.no_grad():\n            # SGD optimization is implemented in PyTorch, but it's very easy to implement manually providing better understanding of process\n            W1 -= learning_rate*W1.grad.data\n            W2 -= learning_rate*W2.grad.data\n            # zero gradients for next step\n            W1.grad.data.zero_()\n            W1.grad.data.zero_()\n    if epo%10 == 0:\n        learning_rate *= lr_decay\n    loss_hist.append(loss)\n    if epo%50 == 0:\n        print(f'Epoch {epo}, loss = {loss}')\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's have a look at our embeddings. W2 is transposed to get shape [vocab_size, embedding_dims]. Weights are tensors, so we should convert it to numpy."},{"metadata":{"trusted":true},"cell_type":"code","source":"W1 = W1.detach().numpy()\nW2 = W2.T.detach().numpy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using decomposition, PCA for example, we can visualize our vectors in 2D vector space."},{"metadata":{"trusted":true},"cell_type":"code","source":"svd = decomposition.TruncatedSVD(n_components=2)\nW1_dec = svd.fit_transform(W1)\nx = W1_dec[:,0]\ny = W1_dec[:,1]\nplot = sns.scatterplot(x, y)\n\nfor i in range(0,W1_dec.shape[0]):\n     plot.text(x[i], y[i]+2e-2, list(vocabulary.keys())[i], horizontalalignment='center', size='small', color='black', weight='semibold');\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"W2_dec = svd.fit_transform(W2)\nx1 = W2_dec[:,0]\ny1 = W2_dec[:,1]\nplot1 = sns.scatterplot(x1, y1)\nfor i in range(0,W2_dec.shape[0]):\n     plot1.text(x1[i], y1[i]+1, list(vocabulary.keys())[i], horizontalalignment='center', size='small', color='black', weight='semibold');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That's interesting, we can use W1 or W2 as embeddings, or maybe both. Look at those plots above. In case of W1 we see, that this embedding learnt that Mercedes and Berlin are somehow similar. So as Ford and Boston. That's because they co-occur often with Germany and USA respectively. Food objects highly concentrated, so as beverages. Cola and juice are near sugar. W2 is interesting as well. Ford-USA and Merceds-Germany vectors seems quite similar. So as Boston-USA and Berlin-Germany. Again, fruits are highly conentrated,as they co-occur just with fruit and eat tokens. That's why eat and fruit are near as well."},{"metadata":{},"cell_type":"markdown","source":"[This](https://ronxin.github.io/wevi/) is fantastic interactive tool, which helps to understand how these to models work for those who love good visualization. I suggest you to have a look."},{"metadata":{},"cell_type":"markdown","source":"### Summary\n\nSkip-Gram model is a good Word2Vec mechanism which sometimes track semantic similarity and context of words. But why is that not the best solution in case of this competition. First and most obvious, too few data. That's simply not enough to train good Word2Vec model. Second, cross entropy error has the unfortunate property that distributions with long tails are often modeled poorly with too much weight given to the unlikely events. Third, evaluating the normalization factor of the softmax for each term is costly and training model even on our competition tweets' corpus takes much time. Fourth, Skip-Gram is sensitive to noise and poorly vectorize rare words from corpus. Nevertheless, understanding the mechanism of Skip-Gram is important for further diving to more complex Word2Vec models. Hope thin notebook is useful. If so, please upvote."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}