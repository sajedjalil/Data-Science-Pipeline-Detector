{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"## Basic notebook showing the steps to use the Real or not? NLP with disaster tweets from Kaggle\n\nIn this notebook we will cover the introductory steps required to completed the competition and submit your results. \n\n\nSteps to be covered: \n\n* Import libraries\n* Import the data set and do some basic EDA\n    * Visualise keywords\n* Impliment text preprocessing\n    * StopWords\n    * Stemming\n    * Lemmatization\n    * Bag of words\n* Create a Bag of words model\n* Train a Naive Bayes Model \n* Submit results","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Import the libraries required:\n\nYou may need to install some of them such as the nltk and regex which you can using the following commands\nFor NLTK:  `conda install -c anaconda nltk` \nFor Regex: `conda install -c conda-forge regex`\n\nAlso make sure you have the `SciKit-Learn`, `Matpplotlib`, `Pandas`, `Numpy` & `Seaborn` libraries installed.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.naive_bayes import MultinomialNB","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Import the dataset\n\nUsing `pd.read_csv()` we will import the training dataset that was downloaded and look and the first five rows using the `.head()` function.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"trainDf = pd.read_csv('../input/nlp-getting-started/train.csv')\ntrainDf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here we use the .info() funtion to show some basic information about the dataset such as the datatypes, value counts & column counts.\n\ntrainDf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainDf['target'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(trainDf['target'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The percentages of each target is:\\n',(trainDf['target'].value_counts()/trainDf.shape[0])*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainDf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So above we can see some basic information regarding the dataset starting with an overview of the data information and then moving onto showing the `value_counts` for each of the target values and displaying this on a plot using `seaborn`. We then moved onto to calculating the percentages for each target value and finally checked for any missing values. Moving forward we shall begin by filling the missing values...","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Missing Data\n\nHere we are going to fill the missing the data in the keyword & location columns. \n\nFor `keywords` we shall fill with: no_keyword\n\nFor `location` we shall fill with: no_location\n\nBoth operations will use the `.fillna()` function.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"trainDf['keyword'].fillna('no_keyword', inplace=True)\ntrainDf['location'].fillna('no_location', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using inplace=True applies the operation permanently.\n\nCheck the missing values again:\n\n***NOTE: Above we used `.isnull()` whereas below we used `.isna()` both do same operation.***","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"trainDf.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plotting the keywords & Locations\n\nHere we are going to look at the different keywords in a visual format using a plot.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"trainDf['keyword'].value_counts().plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see above trying to plot all the keywords on a plot does not work so we will use a selecion instead.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"trainDf['keyword'].value_counts()[:10].plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Much better we can now see a the top 10 keywords from the data.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Location plot\n\nNow we shall use the same method to plot the locations","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"trainDf['location'].value_counts()[:10].plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not the easiest to visualise. Seeing as we know the `no_location` values is going to be the most we wont plot this one. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"trainDf['location'].value_counts()[1:11].plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That's alot better. We can now see the top 10 locations, excluding `no_location`","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Text preprocessing\n\nThis is quite a large topic on it's own so first let's define some key terms:\n\n* Stopwords: \n    > A stop word is a commonly used word (such as “the”, “a”, “an”, “in”) that a search engine has been programmed to ignore, both when indexing entries for searching and when retrieving them as the result of a search query.\n\n* Stemming: \n    > Stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma.A stemming algorithm reduces the words “chocolates”, “chocolatey”, “choco” to the root word, “chocolate” and “retrieval”, “retrieved”, “retrieves” reduce to the stem “retrieve”.\n\n* Lemmatization\n    > Lemmatization is the process of grouping together the different inflected forms of a word so they can be analysed as a single item. Lemmatization is similar to stemming but it brings context to the words. So it links words with similar meaning to one word. Difference between stemming and lemmatization is that lemmatization gives proper meaningful dictionary words.\n\n* Bag of words\n    > The bag-of-words model is a simplifying representation used in natural language processing and information retrieval (IR). In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Importing stop words from NLTK","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"nltk.download('stopwords')\nprint(stopwords.words('english')[:10])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Tf-IDF(Term Frequency and Inverse Document Frequency)\n\n\nTF-IDF (term frequency-inverse document frequency) is a statistical measure that evaluates how relevant a word is to a document in a collection of documents. This is done by multiplying two metrics: how many times a word appears in a document, and the inverse document frequency of the word across a set of documents.\n\n* Term Frequency (TF): is a scoring of the frequency of the word in the current document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. The term frequency is often divided by the document length to normalize.\n\n     `TF=Number of times term t appers in documnet/total no of terms in document.`\n* Inverse Document Frequency (IDF): is a scoring of how rare the word is across documents. IDF is a measure of how rare a term is. Rarer the term, more is the IDF score.\n\n     `IDF=log(total no of documents/no of documents with term t in it)`","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"What we are doing next to is loop through the dataset and:\n* Create an empty list called `sents` to store the sentences\n* loop through the dataset and:\n    * replace all non-alphabeticals with spaces\n    * convert all words to lower case\n    * initiate the `PorterStemmer`\n    * select all the words apart from the stopwords \n    * applying stemming using the `PorterStemmer.stem` function\n* join all the tweets and append to sents ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sents = []\nfor i in range(0,trainDf.shape[0]):\n  tweets = re.sub('[^a-zA-Z]', ' ',trainDf['text'][i])\n  tweets = tweets.lower()\n  tweets = tweets.split()\n  ps = PorterStemmer()\n  all_stopwords = stopwords.words('english')\n  tweets = [ps.stem(word) for word in tweets if not word in set(all_stopwords)]\n  tweets = ' '.join(tweets)\n  sents.append(tweets)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sents[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = CountVectorizer(max_features = 1000)\nX = cv.fit_transform(sents).toarray()\nX[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y=trainDf.iloc[:,-1].values\nX_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier = MultinomialNB()\nclassifier.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction=classifier.predict(X_test)\ncm = confusion_matrix(Y_test,prediction)\nprint(cm)\naccuracy_score(Y_test,prediction)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('../input/nlp-getting-started/test.csv')\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['keyword'].fillna('no_keyword', inplace=True)\ntest['location'].fillna('no_location', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentsT = []\nfor i in range(0,test.shape[0]):\n  tweets = re.sub('[^a-zA-Z]', ' ',test['text'][i])\n  tweets = tweets.lower()\n  tweets = tweets.split()\n  ps = PorterStemmer()\n  all_stopwords = stopwords.words('english')\n  tweets = [ps.stem(word) for word in tweets if not word in set(all_stopwords)]\n  tweets = ' '.join(tweets)\n  sentsT.append(tweets)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentsT[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv1 = CountVectorizer(max_features = 1000)\nx_test = cv1.fit_transform(sentsT).toarray()\nx_test[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_test=test.iloc[:,-1].values\nprint(Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions=classifier.predict(x_test)\nprint(predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')\nsample_submission['target'] = predictions\nsample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.to_csv('sumbission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}