{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# BERT\n\nBERT stands for Bidirectional Encoder Representations from Transformers. BERT is a “deeply bidirectional” model. Bidirectional means that BERT learns information from both the left and the right side of a token’s context during the training phase.\n\nThe bidirectionality of a model is important for truly understanding the meaning of a language. Let’s see an example to illustrate this. There are two sentences in this example and both of them involve the word “bank”:\n![BERT captures both the left and right context](https://cdn.analyticsvidhya.com/wp-content/uploads/2019/09/sent_context.png)\n\nIf we try to predict the nature of the word “bank” by only taking either the left or the right context, then we will be making an error in at least one of the two given examples.\n\nOne way to deal with this is to consider both the left and the right context before making a prediction. That’s exactly what BERT does! Traditionally, we had language models either trained to predict the next word in a sentence (right-to-left context used in GPT) or language models that were trained on a left-to-right context or a shallow concatenation of these two (ElMo). This made the models susceptible to errors due to loss in information.\n\n![](https://cdn.analyticsvidhya.com/wp-content/uploads/2019/09/bert-vs-openai-.jpg)\n\nIt’s evident from the above image: BERT is bi-directional, GPT is unidirectional (information flows only from left-to-right), and ELMO is shallowly bidirectional.\n\nBERT is pre-trained on two NLP tasks:\n\n* Masked Language Modeling\n*  Next Sentence Prediction\n\n(This excerpt is taken from here:\nhttps://www.analyticsvidhya.com/blog/2019/09/demystifying-bert-groundbreaking-nlp-framework/\nFor detailed explanation please visit the link.)","metadata":{}},{"cell_type":"markdown","source":"The model structure implemented in this notebook is similar to this:\n\n![](https://cdn.analyticsvidhya.com/wp-content/uploads/2019/09/bert_pipeline2.png)","metadata":{}},{"cell_type":"code","source":"import re\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport transformers\nimport tensorflow.keras as keras \nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\npd.set_option('display.max_columns', None) \npd.set_option('display.max_rows', 20)  \npd.set_option('display.max_colwidth', -1)  ","metadata":{"execution":{"iopub.status.busy":"2021-08-29T07:19:03.800007Z","iopub.execute_input":"2021-08-29T07:19:03.800444Z","iopub.status.idle":"2021-08-29T07:19:11.712075Z","shell.execute_reply.started":"2021-08-29T07:19:03.800356Z","shell.execute_reply":"2021-08-29T07:19:11.710004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The Dataset\n### 1-real disaster\n### 0-not a disaster","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/nlp-getting-started/train.csv')\nprint(len(df))\nprint(df.columns)\n\ndf","metadata":{"execution":{"iopub.status.busy":"2021-08-29T07:19:11.713685Z","iopub.execute_input":"2021-08-29T07:19:11.71398Z","iopub.status.idle":"2021-08-29T07:19:11.792579Z","shell.execute_reply.started":"2021-08-29T07:19:11.713953Z","shell.execute_reply":"2021-08-29T07:19:11.791532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x = 'target', data = df)\nplt.xlabel('Classes')\nplt.ylabel('Count')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-29T07:19:11.794531Z","iopub.execute_input":"2021-08-29T07:19:11.794948Z","iopub.status.idle":"2021-08-29T07:19:11.927809Z","shell.execute_reply.started":"2021-08-29T07:19:11.794907Z","shell.execute_reply":"2021-08-29T07:19:11.927069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Text Cleaning and Preprocessing","metadata":{}},{"cell_type":"code","source":"def clean_text(text):\n    \n    # Remove http/https links\n    text = re.sub(r'http\\S+', '', text)  \n    \n    # Remove mentions\n    text = re.sub(r\"(?:\\@)\\w+\", '', text)\n    \n    # Remove any characters that is not and alphabet or number\n    text = re.sub(r'[^a-zA-Z0-9\\'.,?$&\\s]', '', text)  \n    \n    # Lower case all the alphabets\n    text = text.lower()\n    \n    return text\n\n# Let's view some random tweets with their cleaned versions\nfor i in range(10):\n    index = np.random.randint(low=0, high=len(df))\n    print('Raw text:', df['text'][index])\n    print('Cleaned text:', clean_text(df['text'][index]))\n    print('Label: ', df['target'][index], '\\n')","metadata":{"execution":{"iopub.status.busy":"2021-08-29T07:28:00.339616Z","iopub.execute_input":"2021-08-29T07:28:00.339965Z","iopub.status.idle":"2021-08-29T07:28:00.353458Z","shell.execute_reply.started":"2021-08-29T07:28:00.339938Z","shell.execute_reply":"2021-08-29T07:28:00.352932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenizing the Inputs","metadata":{}},{"cell_type":"code","source":"# Tokenizing the sentences \ndef convert_to_features(data, tokenizer, max_len=None):\n    data = data.replace('\\n', '')\n    \n    # Return a dictionary containing 'input_ids', 'attention_mask' & 'token_type_ids' each of shape (1, max_len)\n    if max_len is not None:\n        tokenized = tokenizer.encode_plus(\n            data, \n            padding ='max_length',\n            max_length=max_len, \n            truncation=True,\n            return_tensors='np',\n            return_attention_mask=True,\n            return_token_type_ids=True,\n        )\n        \n    else:\n        tokenized = tokenizer.encode_plus(\n            data,  \n            return_tensors='np',\n            return_attention_mask=True,\n            return_token_type_ids=True,\n        )\n    return tokenized\n\n# Create dataset for data with labels\ndef create_inputs_with_targets(x, y, tokenizer, max_len=128):\n    \n    dataset_dict = {\n        \"input_ids\": [],\n        \"attention_mask\": [],\n        'labels': []\n    }\n    \n    for sentence, label in tqdm(zip(x,y)):\n        cleaned_sentence = clean_text(sentence)\n        temp = convert_to_features(cleaned_sentence, tokenizer, max_len=max_len)\n        dataset_dict[\"input_ids\"].append(temp[\"input_ids\"][0])\n        dataset_dict[\"attention_mask\"].append(temp[\"attention_mask\"][0])\n        dataset_dict[\"labels\"].append(label)\n\n    x = [\n        np.array(dataset_dict[\"input_ids\"]),\n        np.array(dataset_dict[\"attention_mask\"]),\n    ]\n    \n    y = np.array(dataset_dict['labels'])\n    \n    return x, y\n\n# Create dataset for data without labels\ndef create_inputs_without_targets(x, tokenizer, max_len=128):\n    \n    dataset_dict = {\n        \"input_ids\": [],\n        \"attention_mask\": [],\n    }\n    \n    for sentence in tqdm(x):\n        cleaned_sentence = clean_text(sentence)\n        temp = convert_to_features(cleaned_sentence, tokenizer, max_len=max_len)\n        dataset_dict[\"input_ids\"].append(temp[\"input_ids\"][0])\n        dataset_dict[\"attention_mask\"].append(temp[\"attention_mask\"][0])\n\n    x = [\n        np.array(dataset_dict[\"input_ids\"]),\n        np.array(dataset_dict[\"attention_mask\"]),\n    ]\n    \n    return x","metadata":{"execution":{"iopub.status.busy":"2021-08-29T07:28:06.682352Z","iopub.execute_input":"2021-08-29T07:28:06.682689Z","iopub.status.idle":"2021-08-29T07:28:06.695563Z","shell.execute_reply.started":"2021-08-29T07:28:06.68266Z","shell.execute_reply":"2021-08-29T07:28:06.694555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# I will be using the BERT base model here\nbase_model = 'bert-base-uncased'\n\nbert_tokenizer = transformers.BertTokenizer.from_pretrained(base_model)\nmax_len = 80","metadata":{"execution":{"iopub.status.busy":"2021-08-29T07:28:10.186062Z","iopub.execute_input":"2021-08-29T07:28:10.186414Z","iopub.status.idle":"2021-08-29T07:28:12.121632Z","shell.execute_reply.started":"2021-08-29T07:28:10.186387Z","shell.execute_reply":"2021-08-29T07:28:12.120428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Splitting the dataframe into 80-20% training and validation dataframes\nvalidation_data_indices = df.sample(frac=0.2).index\nvalidation_df = df.loc[validation_data_indices, :].reset_index(drop=True)\ntrain_df = df.drop(validation_data_indices, axis=0).reset_index(drop=True)\ntest_df = pd.read_csv('../input/nlp-getting-started/test.csv')\n\nx_train, y_train = create_inputs_with_targets(list(train_df['text']), list(train_df['target']), tokenizer=bert_tokenizer, max_len=max_len)\nx_val, y_val = create_inputs_with_targets(list(validation_df['text']), list(validation_df['target']), tokenizer=bert_tokenizer, max_len=max_len)\nx_test = create_inputs_without_targets(list(test_df['text']), tokenizer=bert_tokenizer, max_len=max_len)\n\nprint('Training dataframe size: ', len(train_df))\nprint('Validation dataframe size: ', len(validation_df))\nprint('Test dataframe size: ', len(test_df))","metadata":{"execution":{"iopub.status.busy":"2021-08-29T07:28:13.970589Z","iopub.execute_input":"2021-08-29T07:28:13.970935Z","iopub.status.idle":"2021-08-29T07:28:22.471974Z","shell.execute_reply.started":"2021-08-29T07:28:13.970906Z","shell.execute_reply":"2021-08-29T07:28:22.470967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building The Model","metadata":{}},{"cell_type":"code","source":"def create_model(model_name, max_len=128):\n    \n    seed = 500\n    my_init = tf.keras.initializers.glorot_uniform(seed)\n    max_len = max_len\n    \n    # BERT encoder\n    encoder = transformers.TFAutoModel.from_pretrained(model_name)\n    \n    # UnFreeze the base model weights\n    encoder.trainable = True\n\n    # Define input shapes \n    input_ids = keras.layers.Input(shape=(max_len,), dtype=tf.int32)\n    attention_mask = keras.layers.Input(shape=(max_len,), dtype=tf.int32)\n     \n    sequence_output = encoder(input_ids, attention_mask=attention_mask)['last_hidden_state']\n    \n    # Add trainable layers on top of frozen layers to adapt the pretrained features on the new data.\n    bi_lstm = tf.keras.layers.Bidirectional(keras.layers.LSTM(64, return_sequences=True))(sequence_output)\n    \n    # Applying hybrid pooling approach \n    avg_pool = tf.keras.layers.GlobalAveragePooling1D()(bi_lstm)\n    max_pool = tf.keras.layers.GlobalMaxPooling1D()(bi_lstm)\n    \n    concat = tf.keras.layers.concatenate([avg_pool, max_pool])\n    dropout = tf.keras.layers.Dropout(0.3)(concat)\n    output = tf.keras.layers.Dense(1, activation=\"sigmoid\")(dropout)\n    \n    model = tf.keras.models.Model(\n        inputs=[input_ids, attention_mask], outputs=[output]\n    )\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2021-08-29T07:28:25.996324Z","iopub.execute_input":"2021-08-29T07:28:25.996668Z","iopub.status.idle":"2021-08-29T07:28:26.006593Z","shell.execute_reply.started":"2021-08-29T07:28:25.996632Z","shell.execute_reply":"2021-08-29T07:28:26.005589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Initializing and Training the Model on TPU ","metadata":{}},{"cell_type":"code","source":"epochs = 20\nlr = 2e-4\n\n# Initialize the model on tpu\nuse_tpu = True\nif use_tpu:\n    # Create distribution strategy\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n    # Create model\n    with strategy.scope():\n        model = create_model(base_model, max_len=max_len)\n        \n        optimizer = keras.optimizers.Adam(learning_rate=lr)\n    \n        model.compile(optimizer=optimizer,\n                      loss = keras.losses.BinaryCrossentropy(), \n                      metrics= [keras.metrics.BinaryAccuracy()])\n        \nelse:\n    model = create_model()\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-08-29T07:28:28.691849Z","iopub.execute_input":"2021-08-29T07:28:28.692194Z","iopub.status.idle":"2021-08-29T07:29:09.229152Z","shell.execute_reply.started":"2021-08-29T07:28:28.692165Z","shell.execute_reply":"2021-08-29T07:29:09.228093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model\nmy_callbacks = [keras.callbacks.EarlyStopping(monitor='val_binary_accuracy', patience=2, mode='max', restore_best_weights=True)]\n\nhist = model.fit(x_train, \n                 y_train,\n                 validation_data = (x_val, y_val),\n                 epochs= epochs, \n                 batch_size= 128,\n                 callbacks = my_callbacks,\n                 verbose= 1)","metadata":{"execution":{"iopub.status.busy":"2021-08-29T07:29:09.230456Z","iopub.execute_input":"2021-08-29T07:29:09.230805Z","iopub.status.idle":"2021-08-29T07:31:42.231452Z","shell.execute_reply.started":"2021-08-29T07:29:09.230745Z","shell.execute_reply":"2021-08-29T07:31:42.226336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"predictions = model.predict(x_test)\n\nids = list(test_df['id'])\ntarget = [round(i[0]) for i in predictions]\n\nsub = pd.DataFrame({'id':ids, 'target':target}, index=None)\nsub.to_csv('submission.csv', index=False)\nsub","metadata":{"execution":{"iopub.status.busy":"2021-08-03T10:47:53.745579Z","iopub.execute_input":"2021-08-03T10:47:53.745907Z","iopub.status.idle":"2021-08-03T10:48:03.032227Z","shell.execute_reply.started":"2021-08-03T10:47:53.745879Z","shell.execute_reply":"2021-08-03T10:48:03.031469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Do not submit this\n\n### Here is the perfect submission file. This is only put here in case you want to test your model's performance.","metadata":{}},{"cell_type":"code","source":"# !git clone https://github.com/mitramir55/Kaggle_NLP_competition.git\n# perfect = pd.read_csv('Kaggle_NLP_competition/perfect_submission.csv')\n# cheat = list(perfect['target'])","metadata":{},"execution_count":null,"outputs":[]}]}