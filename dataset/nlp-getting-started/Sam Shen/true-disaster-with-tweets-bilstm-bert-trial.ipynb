{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Kernel Explanation\nThis Kernel constructed the model with two different kinds of methods\n\n1. BiLSTM (pytorch + GloVe)\n2. Bert (keras + TfHub)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"This is the first competition that I participated in. I hope the kernel will provide some intuitive to novice for building their deep learning model.\n\n**Please note you have to run the method 2 first, otherwise there will be a source allocation issue on GPU**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Package Import","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\n!pip install sacremoses\nimport sacremoses\nimport tqdm\nimport re\nimport string\n!pip install sklearn\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Preprocessing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Text Cleaning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\ndef remove_URL(text):\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url.sub('', text)\ndef remove_html(text):\n    html = re.compile(r'<.*?>')\n    return html.sub('', text)\ndef remove_punct(text):\n    table = str.maketrans('', '', string.punctuation)\n    return text.translate(table)\ndef text_cleaning(x):\n    return remove_punct(remove_html(remove_URL(remove_emoji(x))))\ntrain['text'] = train['text'].apply(lambda x: text_cleaning(x))\ntest['text'] = test['text'].apply(lambda x: text_cleaning(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Forming training dataset, validation dataset, testing dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test = test.text.values\nx_train, x_val, y_train, y_val = train_test_split(\n    train.text.values, train.target.values, test_size=0.3, random_state=101\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"shape of test set:\", x_test.shape)\nprint(\"shape of train set:\", x_train.shape)\nprint(\"shape of val set:\", x_val.shape)\nprint(\"true disaster rate in the train set:\", round(sum(y_train)/len(y_train), 2))\nprint(\"true disaster rate in the val set:\", round(sum(y_val)/len(y_val), 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"number = 101\nprint(\"sample text:\", x_train[number])\nprint(\"sample ans:\", \"true disaster\" if y_train[number] else \"not a disaster\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Method 1: GloVe + BiLSTM with Pytorch\nThe code below was referred from the NYU [Nature Language Understanding](https://cims.nyu.edu/~sbowman/teaching.shtml) Course taught by [Sam Bowman](https://cims.nyu.edu/~sbowman/index.shtml)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### GloVe:\n\nGloVe is an unsupervised learning algorithm for obtaining vector representations for words. More explanations have been attached below:\n* [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove/)\n* [NLP — Word Embedding & GloVe](https://medium.com/@jonathan_hui/nlp-word-embedding-glove-5e7f523999f6)\n* [GloVe详解 - 范永勇](www.fanyeong.com/2018/02/19/glove-in-detail/) (If you can understand Mandarin, this is a good source to learn from)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_glove(path, embedding_dim):\n    with open(path) as f:\n        pad_token, unk_token = \"<PAD>\", \"<UNK>\"\n        token_li = [pad_token, unk_token] # note that index 1 is <UNK>, which means unknown word\n        embedding_ls = [np.zeros(embedding_dim), np.random.rand(embedding_dim)]\n        for line in f:\n            token, raw_embedding = line.split(maxsplit=1)\n            token_li.append(token)\n            embedding_ls.append(np.array([float(i) for i in raw_embedding.split()]))\n    return token_li, np.array(embedding_ls)\npath, embedding_dim = \"../input/glove6b300d-50k/glove.6B.300d__50k.txt\", 300\nvocab, embeddings = load_glove(path, embedding_dim)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[Tokenize](https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html):\n\nsentence -> tokenize\n\n\"Coronavirus: WHO advises to wear masks in public areas\" -> ['Coronavirus', ':', 'WHO', 'advises', 'to', 'wear', 'masks', 'in', 'public', 'areas']","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def featurize(data, labels, tokenizer, vocal, max_seq_len = 128):\n    voc_to_idx = {word:idx for idx, word in enumerate(vocab)}\n    text_data = [[voc_to_idx.get(token, 1) for token in tokenizer.tokenize(text.lower())] for text in tqdm.tqdm_notebook(data)]\n    label_data = labels\n    return text_data, label_data\ntokenizer = sacremoses.MosesTokenizer()\ntrain_idc, train_lab = featurize(x_train, y_train, tokenizer, vocab)\nval_idc, val_lab = featurize(x_val, y_val, tokenizer, vocab)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"\\nTrain text first 5 examples:\\n\", train_idc[:5])\nprint(\"\\nTrain label first 5 examples:\\n\", train_lab[:5])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Pytorch: \n\nIf this is the first time you use the pytorch, highly recommend that go and check out the [DEEP LEARNING WITH PYTORCH: A 60 MINUTE BLITZ](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html) first before scroll down below","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom torch.utils.data import dataloader, Dataset\n\nclass data_loader(Dataset):\n    def __init__(self, data_li, target_li, max_sen_len=128):\n        self.data_li = data_li\n        self.target_li = target_li\n        self.max_sen_len = max_sen_len\n        assert (len(self.data_li) == len(self.target_li))\n    def __len__(self):\n        return len(self.data_li)\n    def __getitem__(self, key, max_sen_len=None):\n        if not max_sen_len: max_sen_len = self.max_sen_len\n        token_idx = self.data_li[key][:max_sen_len]\n        label = self.target_li[key]\n        return [token_idx, label]\n    def collate_func(self, batch):\n        data_list = [] # store padded sequences\n        label_list = []\n        max_batch_seq_len = min(len(max(batch, key=lambda x: len(x[0]))[0]), 128)\n        for row in batch:\n            sen_len = len(row[0])\n            data_list.append(row[0]+[0]*(max_batch_seq_len-sen_len) if sen_len < max_batch_seq_len else row[0][:max_batch_seq_len])\n            label_list.append(row[1])\n        return [torch.tensor(np.array(data_list)), torch.tensor(np.array(label_list))]\nbatch_size, max_sen_len = 64, 60\ntrain_dataset = data_loader(train_idc, train_lab, max_sen_len)\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, collate_fn=train_dataset.collate_func, shuffle=True)\nval_dataset = data_loader(val_idc, val_lab, train_dataset.max_sen_len)\nval_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=batch_size, collate_fn=train_dataset.collate_func, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check data loader:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_batch, labels = next(iter(train_loader))\nprint(\"data_batch\\n\", \"shape: \", data_batch.shape, \"\\n\", \"sample:\", data_batch)\nprint(\"labels\\n\", \"shape: \", labels.shape, \"\\n\", \"sample:\", labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### BiLSTM model\n\nBiLSTM is the bidirectional LSTM model. It can capture the both front and back information per vocabulary for a sentence\n\nMore for BiLSTM:\n* [Bi-LSTM](https://medium.com/@raghavaggarwal0089/bi-lstm-bc3d68da8bd0)\n* [The Bidirectional Language Model](https://medium.com/@plusepsilon/the-bidirectional-language-model-1f3961d1fb27)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nclass LSTMClassifier(nn.Module):\n    def __init__(self, embeddings, hidden_size, num_layers, num_class, bidirectional, dropout_prob=0.3):\n        super().__init__()\n        self.embedding_layer = self.load_pretrain_embeddings(embeddings)\n        self.embeddings_dim = embeddings.shape[1]\n        self.bilstm = nn.LSTM(input_size=embeddings.shape[1], hidden_size=hidden_size, num_layers=num_layers, dropout=dropout_prob, bidirectional=bidirectional)\n        self.dropout = nn.Dropout(p=dropout_prob)\n        self.non_linear = nn.ReLU()\n        self.clf = nn.Linear(2*hidden_size, num_classes) # classifier layer: 2 is due to bidirectional\n        self.maxpool2 = nn.MaxPool2d(kernel_size=1)\n    def load_pretrain_embeddings(self, embeddings):\n        embedding_layer = nn.Embedding(embeddings.shape[0], embeddings.shape[1], padding_idx=0)\n        embedding_layer.weight.data = torch.Tensor(embeddings).float()\n        return embedding_layer\n    def forward(self, inputs):\n        X = self.embedding_layer(inputs)\n        bilstm_out, (h_n, c_n) = self.bilstm(X)\n        out = torch.max(input=bilstm_out, dim=1)\n        out = self.non_linear(out.values)\n        logits = self.clf(out)\n        return logits","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Define the evaluation function","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate(model, dataloader, device):\n    model.eval()\n    with torch.no_grad():\n        all_preds, all_labels = [], []\n        for batch_text, batch_label in dataloader:\n            y_preds = model(batch_text.to(device))\n            all_preds.append(y_preds.detach().cpu().numpy())\n            all_labels.append(batch_label.numpy())\n    preds, labels = np.concatenate(np.array(all_preds), axis = 0), np.concatenate(np.array(all_labels), axis = 0)\n    return (preds.argmax(-1)==labels).mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tuning hyperparameters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"hidden_size = 32\nnum_layers = 1\nnum_classes = 2\nbidirectional = True\ntorch.manual_seed(1234)\ndevice = torch.device('cpu')\n# device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device('cpu')\n# we will use cpu and leave the GPU resource for method 2\nprint(\"device: \", device)\nmodel1 = LSTMClassifier(embeddings, hidden_size, num_layers, num_classes, bidirectional)\nmodel1.to(device)\ncriterion = nn.CrossEntropyLoss()\nlearning_rate = 0.005\noptimizer = optim.Adam(model1.parameters(), lr=learning_rate)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"model structure:\\n\", model1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Start training now and validate at the same time with tolerances","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_loss_history = []\nval_acc_history = []\nbest_val_accuracy = 0\ntolerance = 0\nearly_stop_patience = 2\nnum_epochs = 10\n  \nfor epoch in tqdm.tqdm_notebook(range(num_epochs)):\n    model1.train()\n    for i, (batch_data, batch_label) in enumerate(train_loader):\n        y_preds = model1(batch_data.to(device))\n        loss = criterion(y_preds, batch_label.to(device)) # note that the prediction value need to be infront of the true value\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        train_loss_history.append(loss.item())\n    val_acc = evaluate(model=model1, dataloader=val_loader, device=device)\n    val_acc_history.append(val_acc)\n    torch.save(model1, \"pytorch_bilstm_best.pt\")    \n    print(\"epoch: {}; val_accuracy: {}\".format(epoch, val_acc))\n    if val_acc > best_val_accuracy: best_val_accuracy = val_acc\n    else: tolerance += 1\n    if tolerance > early_stop_patience: break   \nprint(\"Best validation accuracy is: \", best_val_accuracy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluate the test set\n\nConstruct test data loader","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_idc, test_lab = featurize(x_test, [0]*len(x_test), tokenizer, vocab) # test label is fake data\ntest_dataset = data_loader(test_idc, test_lab, max_sen_len)\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, collate_fn=test_dataset.collate_func, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Test evaluation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model1.eval()\nwith torch.no_grad():\n    all_preds = []\n    for batch_text, _ in test_loader:\n        preds = model1(batch_text.to(device))\n        all_preds.append(preds.detach().cpu().numpy())\n    all_preds = np.concatenate(np.array(all_preds), axis = 0)\n\npred_res1 = all_preds.argmax(-1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Method 2: Bert sample on TfHub with Keras\n\nThe following code has referred the web page below:\n* [bert_en_uncased_L-24_H-1024_A-16](https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/2)\n* [Disaster NLP: Keras BERT using TFHub](https://www.kaggle.com/xhlulu/disaster-nlp-keras-bert-using-tfhub)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"If this is your first time to use keras, highly recommend [TensorFlow in Practice Specialization](https://www.coursera.org/specializations/tensorflow-in-practice) in the Coursera. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"If you want to know more about the transformer and the bert, I had listed two videos below. These are the best tutorial I know so far.\n* [Transformer Neural Networks - EXPLAINED! (Attention is all you need)](https://www.youtube.com/watch?v=TQQlZhbC5ps&t=2s)\n* [BERT Neural Network - EXPLAINED!](https://www.youtube.com/watch?v=xI0HHN5XKDo)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\n# gpus = tf.config.experimental.list_physical_devices('GPU')\n# tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])\n\n!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py\nimport tokenization","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\", trainable=True)\n\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check the tokenize result below:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"text = \"Coronavirus: WHO advises to wear masks in public areas\"\ntokenize_data = tokenizer.tokenize(text)\nprint(\"Text after tokenization:\\n\", tokenize_data, \"\\n\")\n\nmax_len = 20\ntext = tokenize_data[:max_len-2]\ninput_seq = [\"[CLS]\"] + text + [\"[SEP]\"]\nprint(\"After adding [CLS] and [SEP]:\\n\", input_seq, \"\\n\")\n\npad_len = max_len-len(input_seq)\ntokens = tokenizer.convert_tokens_to_ids(input_seq)+[0]*pad_len\nprint(\"After converting Tokens to Id and adding the pad:\\n\", tokens, \"\\n\")\n\npad_masks = [1]*len(input_seq) + [0]*pad_len\nprint(\"Pad Masking:\\n\", pad_masks, \"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Preprocess with adding [CLS], [SEP], padding, and tokenize","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def pre_process(context_data, tokenizer, max_len=128):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    for text in context_data:\n        input_seq = [\"[CLS]\"]+ tokenizer.tokenize(text)[:max_len-2] + [\"[SEP]\"] # 2 for [CLS] and [SEP]\n        pad_len = max_len-len(input_seq)\n        tokens = tokenizer.convert_tokens_to_ids(input_seq) + [0]*pad_len\n        pad_masks = [1]*len(input_seq) + [0]*pad_len\n        segment_ids = [0]*max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_len = 128\ntrain_input = pre_process(train.text.values, tokenizer, max_len)\ntrain_label = train.target.values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Model construction","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\ninput_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\nsegment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\npooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n# Fine Tuning\nclf_output = sequence_output[:, 0, :]\nout = Dense(1, activation='sigmoid')(clf_output)\n\nmodel2 = Model(\n    inputs=[input_word_ids, input_mask, segment_ids],\n    outputs=out\n)\nmodel2.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\nmodel2.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint = ModelCheckpoint('keras_bert_model.h5', monitor='val_loss', save_best_only=True)\n\ntrain_history = model.fit(\n    train_input, train_label,\n    validation_split=0.3,\n    epochs=3,\n    callbacks=[checkpoint],\n    batch_size=16\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Load model with test evaluation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model2.load_weights('keras_bert_model.h5')\ntest_input = pre_process(test.text.values, tokenizer, max_len)\npred_res2 = model2.predict(test_input).round().astype(int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Sumbit the result","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_res = pred_res2 # pred_res1 or pred_res2\n\npred_res = np.array(sorted(np.array([list(test[\"id\"])]+[list(pred_res)]).T, key=lambda x: x[0]))\n\nsubmission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\nassert(list(submission.id)==list(pred_res[:,0])) # check the number of test data\nsubmission[\"target\"] = pred_res[:,1]\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}