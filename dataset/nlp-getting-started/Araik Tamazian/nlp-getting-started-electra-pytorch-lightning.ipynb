{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport re\nimport os\nimport string\nfrom matplotlib import pyplot as plt\nfrom tqdm.auto import tqdm\nfrom transformers import ElectraTokenizer, ElectraForSequenceClassification,AdamW,get_linear_schedule_with_warmup\nimport torch\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom sklearn.model_selection import StratifiedKFold","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    ROOT_DIR = '../input/nlp-getting-started'\n    BATCH_SIZE = 32\n    ELECTRA_MODEL = 'google/electra-base-discriminator'\n    EPOCHS = 30\n    DEVICE = 'cuda'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess(text):\n    text=text.lower()\n    # remove hyperlinks\n    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text)\n    text = re.sub(r'http?:\\/\\/.*[\\r\\n]*', '', text)\n    #Replace &amp, &lt, &gt with &,<,> respectively\n    text=text.replace(r'&amp;?',r'and')\n    text=text.replace(r'&lt;',r'<')\n    text=text.replace(r'&gt;',r'>') \n    #remove mentions\n    text = re.sub(r\"(?:\\@)\\w+\", '', text)\n    #remove non ascii chars\n    text=text.encode(\"ascii\",errors=\"ignore\").decode()\n    #remove some puncts (except . ! ?)\n    text=re.sub(r'[:\"#$%&\\*+,-/:;<=>@\\\\^_`{|}~]+','',text)\n    text=re.sub(r'[!]+','!',text)\n    text=re.sub(r'[?]+','?',text)\n    text=re.sub(r'[.]+','.',text)\n    text=re.sub(r\"'\",\"\",text)\n    text=re.sub(r\"\\(\",\"\",text)\n    text=re.sub(r\"\\)\",\"\",text)\n\n    text=\" \".join(text.split())\n    return text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LitDataNLP(pl.LightningDataModule):\n    def __init__(self, fold, tokenizer, data_dir:str = './', batch_size: int = 32):\n        super().__init__()\n        self.fold = fold\n        self.tokenizer = tokenizer\n        self.data_dir = data_dir\n        self.batch_size = batch_size\n        \n    def setup(self, stage=None):\n        train_df = pd.read_csv(os.path.join(self.data_dir, 'train.csv'))\n        #train_df['text'] = train_df['text'].apply(preprocess)\n        train_df = train_df[train_df[\"text\"]!='']\n        train_df = train_df[[\"text\",\"target\"]]\n        texts = train_df.text.values\n        labels = train_df.target.values\n        indices=self.tokenizer.batch_encode_plus(texts,max_length=64,add_special_tokens=True, \n                                            return_attention_mask=True,pad_to_max_length=True,\n                                            truncation=True)\n\n        input_ids=np.array(indices[\"input_ids\"])\n        attention_masks=np.array(indices[\"attention_mask\"])\n        skf = StratifiedKFold(5, shuffle=True, random_state=42)\n        #train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n        #                                                    random_state=42, test_size=0.2)\n        #train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n        #                                             random_state=42, test_size=0.2)\n        for fold, (tr_idx, val_idx) in enumerate(skf.split(input_ids, labels)):\n            train_inputs = input_ids[tr_idx]\n            train_labels = labels[tr_idx]\n            validation_inputs = input_ids[val_idx]\n            validation_labels = labels[val_idx]\n            if fold == self.fold:\n                break\n                \n        for fold, (tr_idx, val_idx) in enumerate(skf.split(attention_masks, labels)):\n            train_masks = attention_masks[tr_idx]\n            validation_masks = attention_masks[val_idx]\n            if fold == self.fold:\n                break\n                \n        self.train_inputs = torch.tensor(train_inputs)\n        self.validation_inputs = torch.tensor(validation_inputs)\n        self.train_labels = torch.tensor(train_labels, dtype=torch.long)\n        self.validation_labels = torch.tensor(validation_labels, dtype=torch.long)\n        self.train_masks = torch.tensor(train_masks, dtype=torch.long)\n        self.validation_masks = torch.tensor(validation_masks, dtype=torch.long)\n        \n    def train_dataloader(self):\n        train_data = TensorDataset(self.train_inputs, self.train_masks, self.train_labels)\n        train_sampler = RandomSampler(train_data)\n        return DataLoader(train_data, sampler=train_sampler, batch_size=self.batch_size)\n    \n    def val_dataloader(self):\n        validation_data = TensorDataset(self.validation_inputs, self.validation_masks, self.validation_labels)\n        validation_sampler = SequentialSampler(validation_data)\n        return DataLoader(validation_data, sampler=validation_sampler, batch_size=self.batch_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LitNLPModel(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.model = ElectraForSequenceClassification.from_pretrained(CFG.ELECTRA_MODEL, num_labels=2)\n        self.f1_score = pl.metrics.F1(num_classes=2)\n        \n    def forward(self, b_input_ids, b_input_mask, b_labels):\n        output = self.model(b_input_ids, \n                        token_type_ids=None, \n                        attention_mask=b_input_mask,\n                        labels=b_labels)\n        return output\n    \n    def training_step(self, batch, batch_idx):\n        b_input_ids = batch[0]\n        b_input_mask = batch[1]\n        b_labels = batch[2]\n        z = self(b_input_ids, b_input_mask, b_labels)\n        loss = z[0]\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        b_input_ids = batch[0]\n        b_input_mask = batch[1]\n        b_labels = batch[2]\n        z = self(b_input_ids, b_input_mask, b_labels)\n        val_loss = z[0]\n        logits = z[1]\n        #logits = logits.detach().cpu().numpy()\n        #label_ids = b_labels.to('cpu').numpy()\n        self.log('val_loss', val_loss, prog_bar=True)\n        self.log('val_f1_score', self.f1_score(logits, b_labels), prog_bar=True)\n        return val_loss\n    \n    def configure_optimizers(self):\n        optimizer = AdamW(model.parameters(), lr=6e-6)\n        scheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps=0, \n                                            num_training_steps=189*CFG.EPOCHS)\n        return [optimizer], [scheduler]\n    \n    def flat_accuracy(self, preds, labels):\n        pred_flat = np.argmax(preds, axis=1).flatten()\n        labels_flat = labels.flatten()\n        return np.sum(pred_flat == labels_flat) / len(labels_flat)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = ElectraTokenizer.from_pretrained(CFG.ELECTRA_MODEL)\nfor fold in range(5):\n    dm = LitDataNLP(fold=fold, tokenizer=tokenizer, data_dir=CFG.ROOT_DIR, batch_size=CFG.BATCH_SIZE)\n    chk_callback = ModelCheckpoint(\n        monitor='val_f1_score',\n        filename='model_best',\n        save_top_k=1,\n        mode='max',\n    )\n    es_callback = EarlyStopping(\n       monitor='val_f1_score',\n       min_delta=0.001,\n       patience=5,\n       verbose=False,\n       mode='max'\n    )\n    model = LitNLPModel()\n\n    trainer = pl.Trainer(\n        gpus=1,\n        max_epochs=CFG.EPOCHS,\n        callbacks=[chk_callback, es_callback]\n    )\n\n    trainer.fit(model, dm)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission","metadata":{}},{"cell_type":"code","source":"def run_inference(data_dir, model, device, batch_size:int = 32):\n    test_df = pd.read_csv(os.path.join(data_dir, 'test.csv'))\n    #test_df['text'] = test_df['text'].apply(preprocess)\n    comments = test_df.text.values\n\n    indices = tokenizer.batch_encode_plus(comments, max_length=128, add_special_tokens=True, \n                                           return_attention_mask=True, pad_to_max_length=True,\n                                           truncation=True)\n    input_ids = indices[\"input_ids\"]\n    attention_masks = indices[\"attention_mask\"]\n\n    test_inputs = torch.tensor(input_ids)\n    test_masks = torch.tensor(attention_masks)\n\n    # Create the DataLoader.\n    test_data = TensorDataset(test_inputs, test_masks)\n    test_sampler = SequentialSampler(test_data)\n    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n\n    print('Predicting labels...')\n    \n    preds = []\n    for fold in range(5):\n        model.load_state_dict(torch.load(f'./lightning_logs/version_{fold}/checkpoints/model_best.ckpt')['state_dict'])\n        model.eval()\n        model.to(device)\n\n        # Tracking variables \n        predictions = []\n\n        # Predict \n        for batch in tqdm(test_dataloader, total=len(test_dataloader)):\n            batch = tuple(t.to(device) for t in batch)\n            b_input_ids, b_input_mask = batch\n\n            with torch.no_grad():\n                outputs = model(b_input_ids, b_input_mask, None)\n\n            logits = outputs[0]\n\n            logits = logits.detach().cpu().numpy()\n\n            # Store predictions and true labels\n            predictions.append(logits)\n\n        flat_predictions = [item for sublist in predictions for item in sublist]\n        flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n        preds.append(flat_predictions)\n    return np.round(np.mean(preds, axis=0), 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = run_inference(CFG.ROOT_DIR, model, CFG.DEVICE, batch_size=CFG.BATCH_SIZE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_sub = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')\nsub = pd.DataFrame({'id':sample_sub['id'].values.tolist(),'target':preds.astype(int)})\nsub.to_csv('submission.csv',index=False)\nsub","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}