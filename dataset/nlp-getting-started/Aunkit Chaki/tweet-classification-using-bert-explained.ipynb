{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction","metadata":{}},{"cell_type":"markdown","source":"Hi everyone, in this notebook we will implement BERT transformer to classify disaster tweets. I will be explaining almost every code cell. \n\nFor understanding this notebook you need to have basic understanding of BERT architecture and a general idea of how neural networks works in the domain of NLP. This notebook will show how you can implement bert using [transformer](http://https://huggingface.co/transformers/) library from [hugging face](http://https://huggingface.co/).\n\nHere are some resources to understand how bert works.\n1. Blogs : https://jalammar.github.io/illustrated-bert/\n2. Video : https://www.youtube.com/watch?v=knTc-NQSjKA&ab_channel=stanfordonline","metadata":{}},{"cell_type":"markdown","source":"# Importing libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np    \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport re\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#The transformer library comes preinstalled in kaggle notebooks.\nfrom transformers import AutoTokenizer, TFBertModel\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.initializers import TruncatedNormal\nfrom tensorflow.keras.losses import CategoricalCrossentropy,BinaryCrossentropy\nfrom tensorflow.keras.metrics import CategoricalAccuracy,BinaryAccuracy\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.utils import plot_model\n\n#tf.config.experimental.list_physical_devices('GPU')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-05T07:03:27.850443Z","iopub.execute_input":"2021-12-05T07:03:27.851394Z","iopub.status.idle":"2021-12-05T07:03:33.104718Z","shell.execute_reply.started":"2021-12-05T07:03:27.851242Z","shell.execute_reply":"2021-12-05T07:03:33.102013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading the data ","metadata":{}},{"cell_type":"markdown","source":"We have seen the data beforehand and seen that the data consists of a column called \"keywords\" but as most of the entries were empty in the column so we dropped it. Using the usecols attribute of pandas we import only the specific columns from the dataset.","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/nlp-getting-started/train.csv', usecols=['id','text','target'])\ntest = pd.read_csv('../input/nlp-getting-started/test.csv', usecols=['id','text'])\nsample = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-12-05T07:03:33.105794Z","iopub.status.idle":"2021-12-05T07:03:33.10692Z","shell.execute_reply.started":"2021-12-05T07:03:33.106656Z","shell.execute_reply":"2021-12-05T07:03:33.106684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here sample shows up the format of data to be uploaded on kaggle comptetions.\n\nLets look at our data.","metadata":{}},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-05T07:03:33.108773Z","iopub.status.idle":"2021-12-05T07:03:33.109391Z","shell.execute_reply.started":"2021-12-05T07:03:33.10915Z","shell.execute_reply":"2021-12-05T07:03:33.109175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-05T07:03:33.11062Z","iopub.status.idle":"2021-12-05T07:03:33.111239Z","shell.execute_reply.started":"2021-12-05T07:03:33.111007Z","shell.execute_reply":"2021-12-05T07:03:33.111031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-05T07:03:33.112405Z","iopub.status.idle":"2021-12-05T07:03:33.113041Z","shell.execute_reply.started":"2021-12-05T07:03:33.112808Z","shell.execute_reply":"2021-12-05T07:03:33.112833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"x = train.target.value_counts()\nsns.barplot(x.index, x)\nplt.gca().set_ylabel('samples')","metadata":{"execution":{"iopub.status.busy":"2021-12-05T07:03:33.114253Z","iopub.status.idle":"2021-12-05T07:03:33.114891Z","shell.execute_reply.started":"2021-12-05T07:03:33.114657Z","shell.execute_reply":"2021-12-05T07:03:33.114681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see a bit of imbalancement is present but it is decent enough. Hence, we don't need to apply any sort of standard ML techniques to balance our dataset.\n\nIn the below code cell we are taking all the examples in our training set whose **target** value is 1, we are converting it into a string and then applying the [**len()**](http://www.w3schools.com/python/ref_func_len.asp) python function to get the length of every characters in a single example. Then we do the same thing with **target** value 0. We are storing the outputs and plotting them.","metadata":{}},{"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,6))\ntrain_len1 = train[train['target'] == 1]['text'].str.len()\nax1.hist(train_len1, color='red')\nax1.set_title(\"Disaster Tweet\")\ntrain_len2 = train[train['target'] == 0]['text'].str.len()\nax2.hist(train_len2, color='green')\nax2.set_title(\"Non disaster Tweet\")\nfig.suptitle(\"Characters in tweet\")\nplt.show()\n\nprint(train_len1.shape, train_len2.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-05T07:03:33.11607Z","iopub.status.idle":"2021-12-05T07:03:33.116723Z","shell.execute_reply.started":"2021-12-05T07:03:33.116459Z","shell.execute_reply":"2021-12-05T07:03:33.116483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The below code cell is very much similar to the above one except the fact that we are splitting the text using the [**split()**](http://www.w3schools.com/python/ref_string_split.asp#:~:text=%20Python%20String%20split%20%28%29%20Method%20%201,...%20%204%20More%20Examples.%20%20More%20) function and then using [**lambda**](http://www.w3schools.com/python/python_lambda.asp) to get the length of the input string. We apply the lambda function using the [**map**](https://www.w3schools.com/python/ref_func_map.asp) function inbuilt in python. ","metadata":{}},{"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,6))\ntrain_len1 = train[train['target']==1]['text'].str.split().map(lambda x: len(x))\nax1.hist(train_len1, color='red')\nax1.set_title(\"Disaster Tweets\")\n\ntrain_len2 = train[train['target']==0]['text'].str.split().map(lambda x: len(x))\nax2.hist(train_len1, color='green')\nax2.set_title(\"Non Disaster Tweets\")\nfig.suptitle(\"Words in tweet\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-05T07:03:33.117918Z","iopub.status.idle":"2021-12-05T07:03:33.118521Z","shell.execute_reply.started":"2021-12-05T07:03:33.118287Z","shell.execute_reply":"2021-12-05T07:03:33.118311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The below graph shows us how many words are there per input text on an average. We split the string which returns a list of all the words in our text, then we take the length of it and apply the mean function, which will help us to get the average of the number of words per text. The red and green line in the graph is shown because we set kde=True in our code. KDE stands for kernel density estimation. You can read more about it from [here](http://deepai.org/machine-learning-glossary-and-terms/kernel-density-estimation).","metadata":{}},{"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\ntrain_len1 = train[train['target'] == 1]['text'].str.split().apply(lambda x: len(x))\nsns.histplot(train_len1.map(lambda x: np.mean(x)), ax=ax1, color='red', kde=True)\nax1.set_title('Disaster')\n\ntrain_len2 = train[train['target'] == 0]['text'].str.split().apply(lambda x: len(x))\nsns.histplot(train_len1.map(lambda x: np.mean(x)), ax=ax2, color='green', kde=True)\nax2.set_title('Non Disaster')","metadata":{"execution":{"iopub.status.busy":"2021-12-05T07:03:33.119726Z","iopub.status.idle":"2021-12-05T07:03:33.120342Z","shell.execute_reply.started":"2021-12-05T07:03:33.120112Z","shell.execute_reply":"2021-12-05T07:03:33.120137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Text Cleaning\n\nText cleaning is a very important step in NLP. Although BERT been a very advanced architecture it really doesn't require text cleaning but we will be doing it for better practice but however we won't clean the data while testing it. The below testing will be done for input data only. \n\nThe below text cleaning processes is taken from, I chose this notebook as it changes special detailed things which are specific to this dataset : https://www.kaggle.com/gunesevitan/nlp-with-disaster-tweets-eda-cleaning-and-bert#4.-Embeddings-and-Text-Cleaning","metadata":{}},{"cell_type":"code","source":"%%time\n\ndef clean(tweet): \n            \n    # These will change special characters\n    tweet = re.sub(r\"\\x89Û_\", \"\", tweet)\n    tweet = re.sub(r\"\\x89ÛÒ\", \"\", tweet)\n    tweet = re.sub(r\"\\x89ÛÓ\", \"\", tweet)\n    tweet = re.sub(r\"\\x89ÛÏWhen\", \"When\", tweet)\n    tweet = re.sub(r\"\\x89ÛÏ\", \"\", tweet)\n    tweet = re.sub(r\"China\\x89Ûªs\", \"China's\", tweet)\n    tweet = re.sub(r\"let\\x89Ûªs\", \"let's\", tweet)\n    tweet = re.sub(r\"\\x89Û÷\", \"\", tweet)\n    tweet = re.sub(r\"\\x89Ûª\", \"\", tweet)\n    tweet = re.sub(r\"\\x89Û\\x9d\", \"\", tweet)\n    tweet = re.sub(r\"å_\", \"\", tweet)\n    tweet = re.sub(r\"\\x89Û¢\", \"\", tweet)\n    tweet = re.sub(r\"\\x89Û¢åÊ\", \"\", tweet)\n    tweet = re.sub(r\"fromåÊwounds\", \"from wounds\", tweet)\n    tweet = re.sub(r\"åÊ\", \"\", tweet)\n    tweet = re.sub(r\"åÈ\", \"\", tweet)\n    tweet = re.sub(r\"JapÌ_n\", \"Japan\", tweet)    \n    tweet = re.sub(r\"Ì©\", \"e\", tweet)\n    tweet = re.sub(r\"å¨\", \"\", tweet)\n    tweet = re.sub(r\"SuruÌ¤\", \"Suruc\", tweet)\n    tweet = re.sub(r\"åÇ\", \"\", tweet)\n    tweet = re.sub(r\"å£3million\", \"3 million\", tweet)\n    tweet = re.sub(r\"åÀ\", \"\", tweet)\n    \n    # Abbreviations\n    tweet = re.sub(r\"he's\", \"he is\", tweet)\n    tweet = re.sub(r\"there's\", \"there is\", tweet)\n    tweet = re.sub(r\"We're\", \"We are\", tweet)\n    tweet = re.sub(r\"That's\", \"That is\", tweet)\n    tweet = re.sub(r\"won't\", \"will not\", tweet)\n    tweet = re.sub(r\"they're\", \"they are\", tweet)\n    tweet = re.sub(r\"Can't\", \"Cannot\", tweet)\n    tweet = re.sub(r\"wasn't\", \"was not\", tweet)\n    tweet = re.sub(r\"don\\x89Ûªt\", \"do not\", tweet)\n    tweet = re.sub(r\"aren't\", \"are not\", tweet)\n    tweet = re.sub(r\"isn't\", \"is not\", tweet)\n    tweet = re.sub(r\"What's\", \"What is\", tweet)\n    tweet = re.sub(r\"haven't\", \"have not\", tweet)\n    tweet = re.sub(r\"hasn't\", \"has not\", tweet)\n    tweet = re.sub(r\"There's\", \"There is\", tweet)\n    tweet = re.sub(r\"He's\", \"He is\", tweet)\n    tweet = re.sub(r\"It's\", \"It is\", tweet)\n    tweet = re.sub(r\"You're\", \"You are\", tweet)\n    tweet = re.sub(r\"I'M\", \"I am\", tweet)\n    tweet = re.sub(r\"shouldn't\", \"should not\", tweet)\n    tweet = re.sub(r\"wouldn't\", \"would not\", tweet)\n    tweet = re.sub(r\"i'm\", \"I am\", tweet)\n    tweet = re.sub(r\"I\\x89Ûªm\", \"I am\", tweet)\n    tweet = re.sub(r\"I'm\", \"I am\", tweet)\n    tweet = re.sub(r\"Isn't\", \"is not\", tweet)\n    tweet = re.sub(r\"Here's\", \"Here is\", tweet)\n    tweet = re.sub(r\"you've\", \"you have\", tweet)\n    tweet = re.sub(r\"you\\x89Ûªve\", \"you have\", tweet)\n    tweet = re.sub(r\"we're\", \"we are\", tweet)\n    tweet = re.sub(r\"what's\", \"what is\", tweet)\n    tweet = re.sub(r\"couldn't\", \"could not\", tweet)\n    tweet = re.sub(r\"we've\", \"we have\", tweet)\n    tweet = re.sub(r\"it\\x89Ûªs\", \"it is\", tweet)\n    tweet = re.sub(r\"doesn\\x89Ûªt\", \"does not\", tweet)\n    tweet = re.sub(r\"It\\x89Ûªs\", \"It is\", tweet)\n    tweet = re.sub(r\"Here\\x89Ûªs\", \"Here is\", tweet)\n    tweet = re.sub(r\"who's\", \"who is\", tweet)\n    tweet = re.sub(r\"I\\x89Ûªve\", \"I have\", tweet)\n    tweet = re.sub(r\"y'all\", \"you all\", tweet)\n    tweet = re.sub(r\"can\\x89Ûªt\", \"cannot\", tweet)\n    tweet = re.sub(r\"would've\", \"would have\", tweet)\n    tweet = re.sub(r\"it'll\", \"it will\", tweet)\n    tweet = re.sub(r\"we'll\", \"we will\", tweet)\n    tweet = re.sub(r\"wouldn\\x89Ûªt\", \"would not\", tweet)\n    tweet = re.sub(r\"We've\", \"We have\", tweet)\n    tweet = re.sub(r\"he'll\", \"he will\", tweet)\n    tweet = re.sub(r\"Y'all\", \"You all\", tweet)\n    tweet = re.sub(r\"Weren't\", \"Were not\", tweet)\n    tweet = re.sub(r\"Didn't\", \"Did not\", tweet)\n    tweet = re.sub(r\"they'll\", \"they will\", tweet)\n    tweet = re.sub(r\"they'd\", \"they would\", tweet)\n    tweet = re.sub(r\"DON'T\", \"DO NOT\", tweet)\n    tweet = re.sub(r\"That\\x89Ûªs\", \"That is\", tweet)\n    tweet = re.sub(r\"they've\", \"they have\", tweet)\n    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n    tweet = re.sub(r\"should've\", \"should have\", tweet)\n    tweet = re.sub(r\"You\\x89Ûªre\", \"You are\", tweet)\n    tweet = re.sub(r\"where's\", \"where is\", tweet)\n    tweet = re.sub(r\"Don\\x89Ûªt\", \"Do not\", tweet)\n    tweet = re.sub(r\"we'd\", \"we would\", tweet)\n    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n    tweet = re.sub(r\"weren't\", \"were not\", tweet)\n    tweet = re.sub(r\"They're\", \"They are\", tweet)\n    tweet = re.sub(r\"Can\\x89Ûªt\", \"Cannot\", tweet)\n    tweet = re.sub(r\"you\\x89Ûªll\", \"you will\", tweet)\n    tweet = re.sub(r\"I\\x89Ûªd\", \"I would\", tweet)\n    tweet = re.sub(r\"let's\", \"let us\", tweet)\n    tweet = re.sub(r\"it's\", \"it is\", tweet)\n    tweet = re.sub(r\"can't\", \"cannot\", tweet)\n    tweet = re.sub(r\"don't\", \"do not\", tweet)\n    tweet = re.sub(r\"you're\", \"you are\", tweet)\n    tweet = re.sub(r\"i've\", \"I have\", tweet)\n    tweet = re.sub(r\"that's\", \"that is\", tweet)\n    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n    tweet = re.sub(r\"doesn't\", \"does not\", tweet)\n    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n    tweet = re.sub(r\"didn't\", \"did not\", tweet)\n    tweet = re.sub(r\"ain't\", \"am not\", tweet)\n    tweet = re.sub(r\"you'll\", \"you will\", tweet)\n    tweet = re.sub(r\"I've\", \"I have\", tweet)\n    tweet = re.sub(r\"Don't\", \"do not\", tweet)\n    tweet = re.sub(r\"I'll\", \"I will\", tweet)\n    tweet = re.sub(r\"I'd\", \"I would\", tweet)\n    tweet = re.sub(r\"Let's\", \"Let us\", tweet)\n    tweet = re.sub(r\"you'd\", \"You would\", tweet)\n    tweet = re.sub(r\"It's\", \"It is\", tweet)\n    tweet = re.sub(r\"Ain't\", \"am not\", tweet)\n    tweet = re.sub(r\"Haven't\", \"Have not\", tweet)\n    tweet = re.sub(r\"Could've\", \"Could have\", tweet)\n    tweet = re.sub(r\"youve\", \"you have\", tweet)  \n    tweet = re.sub(r\"donå«t\", \"do not\", tweet)   \n            \n    # Character entity references\n    tweet = re.sub(r\"&gt;\", \">\", tweet)\n    tweet = re.sub(r\"&lt;\", \"<\", tweet)\n    tweet = re.sub(r\"&amp;\", \"&\", tweet)\n    \n    # These are for typos, slang and informal abbreviations\n    tweet = re.sub(r\"w/e\", \"whatever\", tweet)\n    tweet = re.sub(r\"w/\", \"with\", tweet)\n    tweet = re.sub(r\"USAgov\", \"USA government\", tweet)\n    tweet = re.sub(r\"recentlu\", \"recently\", tweet)\n    tweet = re.sub(r\"Ph0tos\", \"Photos\", tweet)\n    tweet = re.sub(r\"amirite\", \"am I right\", tweet)\n    tweet = re.sub(r\"exp0sed\", \"exposed\", tweet)\n    tweet = re.sub(r\"<3\", \"love\", tweet)\n    tweet = re.sub(r\"amageddon\", \"armageddon\", tweet)\n    tweet = re.sub(r\"Trfc\", \"Traffic\", tweet)\n    tweet = re.sub(r\"8/5/2015\", \"2015-08-05\", tweet)\n    tweet = re.sub(r\"WindStorm\", \"Wind Storm\", tweet)\n    tweet = re.sub(r\"8/6/2015\", \"2015-08-06\", tweet)\n    tweet = re.sub(r\"10:38PM\", \"10:38 PM\", tweet)\n    tweet = re.sub(r\"10:30pm\", \"10:30 PM\", tweet)\n    tweet = re.sub(r\"16yr\", \"16 year\", tweet)\n    tweet = re.sub(r\"lmao\", \"laughing my ass off\", tweet)   \n    tweet = re.sub(r\"TRAUMATISED\", \"traumatized\", tweet)\n    \n    # This is to handle hashtags and usernames\n    tweet = re.sub(r\"IranDeal\", \"Iran Deal\", tweet)\n    tweet = re.sub(r\"ArianaGrande\", \"Ariana Grande\", tweet)\n    tweet = re.sub(r\"camilacabello97\", \"camila cabello\", tweet) \n    tweet = re.sub(r\"RondaRousey\", \"Ronda Rousey\", tweet)     \n    tweet = re.sub(r\"MTVHottest\", \"MTV Hottest\", tweet)\n    tweet = re.sub(r\"TrapMusic\", \"Trap Music\", tweet)\n    tweet = re.sub(r\"ProphetMuhammad\", \"Prophet Muhammad\", tweet)\n    tweet = re.sub(r\"PantherAttack\", \"Panther Attack\", tweet)\n    tweet = re.sub(r\"StrategicPatience\", \"Strategic Patience\", tweet)\n    tweet = re.sub(r\"socialnews\", \"social news\", tweet)\n    tweet = re.sub(r\"NASAHurricane\", \"NASA Hurricane\", tweet)\n    tweet = re.sub(r\"onlinecommunities\", \"online communities\", tweet)\n    tweet = re.sub(r\"humanconsumption\", \"human consumption\", tweet)\n    tweet = re.sub(r\"Typhoon-Devastated\", \"Typhoon Devastated\", tweet)\n    tweet = re.sub(r\"Meat-Loving\", \"Meat Loving\", tweet)\n    tweet = re.sub(r\"facialabuse\", \"facial abuse\", tweet)\n    tweet = re.sub(r\"LakeCounty\", \"Lake County\", tweet)\n    tweet = re.sub(r\"BeingAuthor\", \"Being Author\", tweet)\n    tweet = re.sub(r\"withheavenly\", \"with heavenly\", tweet)\n    tweet = re.sub(r\"thankU\", \"thank you\", tweet)\n    tweet = re.sub(r\"iTunesMusic\", \"iTunes Music\", tweet)\n    tweet = re.sub(r\"OffensiveContent\", \"Offensive Content\", tweet)\n    tweet = re.sub(r\"WorstSummerJob\", \"Worst Summer Job\", tweet)\n    tweet = re.sub(r\"HarryBeCareful\", \"Harry Be Careful\", tweet)\n    tweet = re.sub(r\"NASASolarSystem\", \"NASA Solar System\", tweet)\n    tweet = re.sub(r\"animalrescue\", \"animal rescue\", tweet)\n    tweet = re.sub(r\"KurtSchlichter\", \"Kurt Schlichter\", tweet)\n    tweet = re.sub(r\"aRmageddon\", \"armageddon\", tweet)\n    tweet = re.sub(r\"Throwingknifes\", \"Throwing knives\", tweet)\n    tweet = re.sub(r\"GodsLove\", \"God's Love\", tweet)\n    tweet = re.sub(r\"bookboost\", \"book boost\", tweet)\n    tweet = re.sub(r\"ibooklove\", \"I book love\", tweet)\n    tweet = re.sub(r\"NestleIndia\", \"Nestle India\", tweet)\n    tweet = re.sub(r\"realDonaldTrump\", \"Donald Trump\", tweet)\n    tweet = re.sub(r\"DavidVonderhaar\", \"David Vonderhaar\", tweet)\n    tweet = re.sub(r\"CecilTheLion\", \"Cecil The Lion\", tweet)\n    tweet = re.sub(r\"weathernetwork\", \"weather network\", tweet)\n    tweet = re.sub(r\"withBioterrorism&use\", \"with Bioterrorism & use\", tweet)\n    tweet = re.sub(r\"Hostage&2\", \"Hostage & 2\", tweet)\n    tweet = re.sub(r\"GOPDebate\", \"GOP Debate\", tweet)\n    tweet = re.sub(r\"RickPerry\", \"Rick Perry\", tweet)\n    tweet = re.sub(r\"frontpage\", \"front page\", tweet)\n    tweet = re.sub(r\"NewsInTweets\", \"News In Tweets\", tweet)\n    tweet = re.sub(r\"ViralSpell\", \"Viral Spell\", tweet)\n    tweet = re.sub(r\"til_now\", \"until now\", tweet)\n    tweet = re.sub(r\"volcanoinRussia\", \"volcano in Russia\", tweet)\n    tweet = re.sub(r\"ZippedNews\", \"Zipped News\", tweet)\n    tweet = re.sub(r\"MicheleBachman\", \"Michele Bachman\", tweet)\n    tweet = re.sub(r\"53inch\", \"53 inch\", tweet)\n    tweet = re.sub(r\"KerrickTrial\", \"Kerrick Trial\", tweet)\n    tweet = re.sub(r\"abstorm\", \"Alberta Storm\", tweet)\n    tweet = re.sub(r\"Beyhive\", \"Beyonce hive\", tweet)\n    tweet = re.sub(r\"IDFire\", \"Idaho Fire\", tweet)\n    tweet = re.sub(r\"DETECTADO\", \"Detected\", tweet)\n    tweet = re.sub(r\"RockyFire\", \"Rocky Fire\", tweet)\n    tweet = re.sub(r\"Listen/Buy\", \"Listen / Buy\", tweet)\n    tweet = re.sub(r\"NickCannon\", \"Nick Cannon\", tweet)\n    tweet = re.sub(r\"FaroeIslands\", \"Faroe Islands\", tweet)\n    tweet = re.sub(r\"yycstorm\", \"Calgary Storm\", tweet)\n    tweet = re.sub(r\"IDPs:\", \"Internally Displaced People :\", tweet)\n    tweet = re.sub(r\"ArtistsUnited\", \"Artists United\", tweet)\n    tweet = re.sub(r\"ClaytonBryant\", \"Clayton Bryant\", tweet)\n    tweet = re.sub(r\"jimmyfallon\", \"jimmy fallon\", tweet)\n    tweet = re.sub(r\"justinbieber\", \"justin bieber\", tweet)  \n    tweet = re.sub(r\"UTC2015\", \"UTC 2015\", tweet)\n    tweet = re.sub(r\"Time2015\", \"Time 2015\", tweet)\n    tweet = re.sub(r\"djicemoon\", \"dj icemoon\", tweet)\n    tweet = re.sub(r\"LivingSafely\", \"Living Safely\", tweet)\n    tweet = re.sub(r\"FIFA16\", \"Fifa 2016\", tweet)\n    tweet = re.sub(r\"thisiswhywecanthavenicethings\", \"this is why we cannot have nice things\", tweet)\n    tweet = re.sub(r\"bbcnews\", \"bbc news\", tweet)\n    tweet = re.sub(r\"UndergroundRailraod\", \"Underground Railraod\", tweet)\n    tweet = re.sub(r\"c4news\", \"c4 news\", tweet)\n    tweet = re.sub(r\"OBLITERATION\", \"obliteration\", tweet)\n    tweet = re.sub(r\"MUDSLIDE\", \"mudslide\", tweet)\n    tweet = re.sub(r\"NoSurrender\", \"No Surrender\", tweet)\n    tweet = re.sub(r\"NotExplained\", \"Not Explained\", tweet)\n    tweet = re.sub(r\"greatbritishbakeoff\", \"great british bake off\", tweet)\n    tweet = re.sub(r\"LondonFire\", \"London Fire\", tweet)\n    tweet = re.sub(r\"KOTAWeather\", \"KOTA Weather\", tweet)\n    tweet = re.sub(r\"LuchaUnderground\", \"Lucha Underground\", tweet)\n    tweet = re.sub(r\"KOIN6News\", \"KOIN 6 News\", tweet)\n    tweet = re.sub(r\"LiveOnK2\", \"Live On K2\", tweet)\n    tweet = re.sub(r\"9NewsGoldCoast\", \"9 News Gold Coast\", tweet)\n    tweet = re.sub(r\"nikeplus\", \"nike plus\", tweet)\n    tweet = re.sub(r\"david_cameron\", \"David Cameron\", tweet)\n    tweet = re.sub(r\"peterjukes\", \"Peter Jukes\", tweet)\n    tweet = re.sub(r\"JamesMelville\", \"James Melville\", tweet)\n    tweet = re.sub(r\"megynkelly\", \"Megyn Kelly\", tweet)\n    tweet = re.sub(r\"cnewslive\", \"C News Live\", tweet)\n    tweet = re.sub(r\"JamaicaObserver\", \"Jamaica Observer\", tweet)\n    tweet = re.sub(r\"TweetLikeItsSeptember11th2001\", \"Tweet like it is september 11th 2001\", tweet)\n    tweet = re.sub(r\"cbplawyers\", \"cbp lawyers\", tweet)\n    tweet = re.sub(r\"fewmoretweets\", \"few more tweets\", tweet)\n    tweet = re.sub(r\"BlackLivesMatter\", \"Black Lives Matter\", tweet)\n    tweet = re.sub(r\"cjoyner\", \"Chris Joyner\", tweet)\n    tweet = re.sub(r\"ENGvAUS\", \"England vs Australia\", tweet)\n    tweet = re.sub(r\"ScottWalker\", \"Scott Walker\", tweet)\n    tweet = re.sub(r\"MikeParrActor\", \"Michael Parr\", tweet)\n    tweet = re.sub(r\"4PlayThursdays\", \"Foreplay Thursdays\", tweet)\n    tweet = re.sub(r\"TGF2015\", \"Tontitown Grape Festival\", tweet)\n    tweet = re.sub(r\"realmandyrain\", \"Mandy Rain\", tweet)\n    tweet = re.sub(r\"GraysonDolan\", \"Grayson Dolan\", tweet)\n    tweet = re.sub(r\"ApolloBrown\", \"Apollo Brown\", tweet)\n    tweet = re.sub(r\"saddlebrooke\", \"Saddlebrooke\", tweet)\n    tweet = re.sub(r\"TontitownGrape\", \"Tontitown Grape\", tweet)\n    tweet = re.sub(r\"AbbsWinston\", \"Abbs Winston\", tweet)\n    tweet = re.sub(r\"ShaunKing\", \"Shaun King\", tweet)\n    tweet = re.sub(r\"MeekMill\", \"Meek Mill\", tweet)\n    tweet = re.sub(r\"TornadoGiveaway\", \"Tornado Giveaway\", tweet)\n    tweet = re.sub(r\"GRupdates\", \"GR updates\", tweet)\n    tweet = re.sub(r\"SouthDowns\", \"South Downs\", tweet)\n    tweet = re.sub(r\"braininjury\", \"brain injury\", tweet)\n    tweet = re.sub(r\"auspol\", \"Australian politics\", tweet)\n    tweet = re.sub(r\"PlannedParenthood\", \"Planned Parenthood\", tweet)\n    tweet = re.sub(r\"calgaryweather\", \"Calgary Weather\", tweet)\n    tweet = re.sub(r\"weallheartonedirection\", \"we all heart one direction\", tweet)\n    tweet = re.sub(r\"edsheeran\", \"Ed Sheeran\", tweet)\n    tweet = re.sub(r\"TrueHeroes\", \"True Heroes\", tweet)\n    tweet = re.sub(r\"S3XLEAK\", \"sex leak\", tweet)\n    tweet = re.sub(r\"ComplexMag\", \"Complex Magazine\", tweet)\n    tweet = re.sub(r\"TheAdvocateMag\", \"The Advocate Magazine\", tweet)\n    tweet = re.sub(r\"CityofCalgary\", \"City of Calgary\", tweet)\n    tweet = re.sub(r\"EbolaOutbreak\", \"Ebola Outbreak\", tweet)\n    tweet = re.sub(r\"SummerFate\", \"Summer Fate\", tweet)\n    tweet = re.sub(r\"RAmag\", \"Royal Academy Magazine\", tweet)\n    tweet = re.sub(r\"offers2go\", \"offers to go\", tweet)\n    tweet = re.sub(r\"foodscare\", \"food scare\", tweet)\n    tweet = re.sub(r\"MNPDNashville\", \"Metropolitan Nashville Police Department\", tweet)\n    tweet = re.sub(r\"TfLBusAlerts\", \"TfL Bus Alerts\", tweet)\n    tweet = re.sub(r\"GamerGate\", \"Gamer Gate\", tweet)\n    tweet = re.sub(r\"IHHen\", \"Humanitarian Relief\", tweet)\n    tweet = re.sub(r\"spinningbot\", \"spinning bot\", tweet)\n    tweet = re.sub(r\"ModiMinistry\", \"Modi Ministry\", tweet)\n    tweet = re.sub(r\"TAXIWAYS\", \"taxi ways\", tweet)\n    tweet = re.sub(r\"Calum5SOS\", \"Calum Hood\", tweet)\n    tweet = re.sub(r\"po_st\", \"po.st\", tweet)\n    tweet = re.sub(r\"scoopit\", \"scoop.it\", tweet)\n    tweet = re.sub(r\"UltimaLucha\", \"Ultima Lucha\", tweet)\n    tweet = re.sub(r\"JonathanFerrell\", \"Jonathan Ferrell\", tweet)\n    tweet = re.sub(r\"aria_ahrary\", \"Aria Ahrary\", tweet)\n    tweet = re.sub(r\"rapidcity\", \"Rapid City\", tweet)\n    tweet = re.sub(r\"OutBid\", \"outbid\", tweet)\n    tweet = re.sub(r\"lavenderpoetrycafe\", \"lavender poetry cafe\", tweet)\n    tweet = re.sub(r\"EudryLantiqua\", \"Eudry Lantiqua\", tweet)\n    tweet = re.sub(r\"15PM\", \"15 PM\", tweet)\n    tweet = re.sub(r\"OriginalFunko\", \"Funko\", tweet)\n    tweet = re.sub(r\"rightwaystan\", \"Richard Tan\", tweet)\n    tweet = re.sub(r\"CindyNoonan\", \"Cindy Noonan\", tweet)\n    tweet = re.sub(r\"RT_America\", \"RT America\", tweet)\n    tweet = re.sub(r\"narendramodi\", \"Narendra Modi\", tweet)\n    tweet = re.sub(r\"BakeOffFriends\", \"Bake Off Friends\", tweet)\n    tweet = re.sub(r\"TeamHendrick\", \"Hendrick Motorsports\", tweet)\n    tweet = re.sub(r\"alexbelloli\", \"Alex Belloli\", tweet)\n    tweet = re.sub(r\"itsjustinstuart\", \"Justin Stuart\", tweet)\n    tweet = re.sub(r\"gunsense\", \"gun sense\", tweet)\n    tweet = re.sub(r\"DebateQuestionsWeWantToHear\", \"debate questions we want to hear\", tweet)\n    tweet = re.sub(r\"RoyalCarribean\", \"Royal Carribean\", tweet)\n    tweet = re.sub(r\"samanthaturne19\", \"Samantha Turner\", tweet)\n    tweet = re.sub(r\"JonVoyage\", \"Jon Stewart\", tweet)\n    tweet = re.sub(r\"renew911health\", \"renew 911 health\", tweet)\n    tweet = re.sub(r\"SuryaRay\", \"Surya Ray\", tweet)\n    tweet = re.sub(r\"pattonoswalt\", \"Patton Oswalt\", tweet)\n    tweet = re.sub(r\"minhazmerchant\", \"Minhaz Merchant\", tweet)\n    tweet = re.sub(r\"TLVFaces\", \"Israel Diaspora Coalition\", tweet)\n    tweet = re.sub(r\"pmarca\", \"Marc Andreessen\", tweet)\n    tweet = re.sub(r\"pdx911\", \"Portland Police\", tweet)\n    tweet = re.sub(r\"jamaicaplain\", \"Jamaica Plain\", tweet)\n    tweet = re.sub(r\"Japton\", \"Arkansas\", tweet)\n    tweet = re.sub(r\"RouteComplex\", \"Route Complex\", tweet)\n    tweet = re.sub(r\"INSubcontinent\", \"Indian Subcontinent\", tweet)\n    tweet = re.sub(r\"NJTurnpike\", \"New Jersey Turnpike\", tweet)\n    tweet = re.sub(r\"Politifiact\", \"PolitiFact\", tweet)\n    tweet = re.sub(r\"Hiroshima70\", \"Hiroshima\", tweet)\n    tweet = re.sub(r\"GMMBC\", \"Greater Mt Moriah Baptist Church\", tweet)\n    tweet = re.sub(r\"versethe\", \"verse the\", tweet)\n    tweet = re.sub(r\"TubeStrike\", \"Tube Strike\", tweet)\n    tweet = re.sub(r\"MissionHills\", \"Mission Hills\", tweet)\n    tweet = re.sub(r\"ProtectDenaliWolves\", \"Protect Denali Wolves\", tweet)\n    tweet = re.sub(r\"NANKANA\", \"Nankana\", tweet)\n    tweet = re.sub(r\"SAHIB\", \"Sahib\", tweet)\n    tweet = re.sub(r\"PAKPATTAN\", \"Pakpattan\", tweet)\n    tweet = re.sub(r\"Newz_Sacramento\", \"News Sacramento\", tweet)\n    tweet = re.sub(r\"gofundme\", \"go fund me\", tweet)\n    tweet = re.sub(r\"pmharper\", \"Stephen Harper\", tweet)\n    tweet = re.sub(r\"IvanBerroa\", \"Ivan Berroa\", tweet)\n    tweet = re.sub(r\"LosDelSonido\", \"Los Del Sonido\", tweet)\n    tweet = re.sub(r\"bancodeseries\", \"banco de series\", tweet)\n    tweet = re.sub(r\"timkaine\", \"Tim Kaine\", tweet)\n    tweet = re.sub(r\"IdentityTheft\", \"Identity Theft\", tweet)\n    tweet = re.sub(r\"AllLivesMatter\", \"All Lives Matter\", tweet)\n    tweet = re.sub(r\"mishacollins\", \"Misha Collins\", tweet)\n    tweet = re.sub(r\"BillNeelyNBC\", \"Bill Neely\", tweet)\n    tweet = re.sub(r\"BeClearOnCancer\", \"be clear on cancer\", tweet)\n    tweet = re.sub(r\"Kowing\", \"Knowing\", tweet)\n    tweet = re.sub(r\"ScreamQueens\", \"Scream Queens\", tweet)\n    tweet = re.sub(r\"AskCharley\", \"Ask Charley\", tweet)\n    tweet = re.sub(r\"BlizzHeroes\", \"Heroes of the Storm\", tweet)\n    tweet = re.sub(r\"BradleyBrad47\", \"Bradley Brad\", tweet)\n    tweet = re.sub(r\"HannaPH\", \"Typhoon Hanna\", tweet)\n    tweet = re.sub(r\"meinlcymbals\", \"MEINL Cymbals\", tweet)\n    tweet = re.sub(r\"Ptbo\", \"Peterborough\", tweet)\n    tweet = re.sub(r\"cnnbrk\", \"CNN Breaking News\", tweet)\n    tweet = re.sub(r\"IndianNews\", \"Indian News\", tweet)\n    tweet = re.sub(r\"savebees\", \"save bees\", tweet)\n    tweet = re.sub(r\"GreenHarvard\", \"Green Harvard\", tweet)\n    tweet = re.sub(r\"StandwithPP\", \"Stand with planned parenthood\", tweet)\n    tweet = re.sub(r\"hermancranston\", \"Herman Cranston\", tweet)\n    tweet = re.sub(r\"WMUR9\", \"WMUR-TV\", tweet)\n    tweet = re.sub(r\"RockBottomRadFM\", \"Rock Bottom Radio\", tweet)\n    tweet = re.sub(r\"ameenshaikh3\", \"Ameen Shaikh\", tweet)\n    tweet = re.sub(r\"ProSyn\", \"Project Syndicate\", tweet)\n    tweet = re.sub(r\"Daesh\", \"ISIS\", tweet)\n    tweet = re.sub(r\"s2g\", \"swear to god\", tweet)\n    tweet = re.sub(r\"listenlive\", \"listen live\", tweet)\n    tweet = re.sub(r\"CDCgov\", \"Centers for Disease Control and Prevention\", tweet)\n    tweet = re.sub(r\"FoxNew\", \"Fox News\", tweet)\n    tweet = re.sub(r\"CBSBigBrother\", \"Big Brother\", tweet)\n    tweet = re.sub(r\"JulieDiCaro\", \"Julie DiCaro\", tweet)\n    tweet = re.sub(r\"theadvocatemag\", \"The Advocate Magazine\", tweet)\n    tweet = re.sub(r\"RohnertParkDPS\", \"Rohnert Park Police Department\", tweet)\n    tweet = re.sub(r\"THISIZBWRIGHT\", \"Bonnie Wright\", tweet)\n    tweet = re.sub(r\"Popularmmos\", \"Popular MMOs\", tweet)\n    tweet = re.sub(r\"WildHorses\", \"Wild Horses\", tweet)\n    tweet = re.sub(r\"FantasticFour\", \"Fantastic Four\", tweet)\n    tweet = re.sub(r\"HORNDALE\", \"Horndale\", tweet)\n    tweet = re.sub(r\"PINER\", \"Piner\", tweet)\n    tweet = re.sub(r\"BathAndNorthEastSomerset\", \"Bath and North East Somerset\", tweet)\n    tweet = re.sub(r\"thatswhatfriendsarefor\", \"that is what friends are for\", tweet)\n    tweet = re.sub(r\"residualincome\", \"residual income\", tweet)\n    tweet = re.sub(r\"YahooNewsDigest\", \"Yahoo News Digest\", tweet)\n    tweet = re.sub(r\"MalaysiaAirlines\", \"Malaysia Airlines\", tweet)\n    tweet = re.sub(r\"AmazonDeals\", \"Amazon Deals\", tweet)\n    tweet = re.sub(r\"MissCharleyWebb\", \"Charley Webb\", tweet)\n    tweet = re.sub(r\"shoalstraffic\", \"shoals traffic\", tweet)\n    tweet = re.sub(r\"GeorgeFoster72\", \"George Foster\", tweet)\n    tweet = re.sub(r\"pop2015\", \"pop 2015\", tweet)\n    tweet = re.sub(r\"_PokemonCards_\", \"Pokemon Cards\", tweet)\n    tweet = re.sub(r\"DianneG\", \"Dianne Gallagher\", tweet)\n    tweet = re.sub(r\"KashmirConflict\", \"Kashmir Conflict\", tweet)\n    tweet = re.sub(r\"BritishBakeOff\", \"British Bake Off\", tweet)\n    tweet = re.sub(r\"FreeKashmir\", \"Free Kashmir\", tweet)\n    tweet = re.sub(r\"mattmosley\", \"Matt Mosley\", tweet)\n    tweet = re.sub(r\"BishopFred\", \"Bishop Fred\", tweet)\n    tweet = re.sub(r\"EndConflict\", \"End Conflict\", tweet)\n    tweet = re.sub(r\"EndOccupation\", \"End Occupation\", tweet)\n    tweet = re.sub(r\"UNHEALED\", \"unhealed\", tweet)\n    tweet = re.sub(r\"CharlesDagnall\", \"Charles Dagnall\", tweet)\n    tweet = re.sub(r\"Latestnews\", \"Latest news\", tweet)\n    tweet = re.sub(r\"KindleCountdown\", \"Kindle Countdown\", tweet)\n    tweet = re.sub(r\"NoMoreHandouts\", \"No More Handouts\", tweet)\n    tweet = re.sub(r\"datingtips\", \"dating tips\", tweet)\n    tweet = re.sub(r\"charlesadler\", \"Charles Adler\", tweet)\n    tweet = re.sub(r\"twia\", \"Texas Windstorm Insurance Association\", tweet)\n    tweet = re.sub(r\"txlege\", \"Texas Legislature\", tweet)\n    tweet = re.sub(r\"WindstormInsurer\", \"Windstorm Insurer\", tweet)\n    tweet = re.sub(r\"Newss\", \"News\", tweet)\n    tweet = re.sub(r\"hempoil\", \"hemp oil\", tweet)\n    tweet = re.sub(r\"CommoditiesAre\", \"Commodities are\", tweet)\n    tweet = re.sub(r\"tubestrike\", \"tube strike\", tweet)\n    tweet = re.sub(r\"JoeNBC\", \"Joe Scarborough\", tweet)\n    tweet = re.sub(r\"LiteraryCakes\", \"Literary Cakes\", tweet)\n    tweet = re.sub(r\"TI5\", \"The International 5\", tweet)\n    tweet = re.sub(r\"thehill\", \"the hill\", tweet)\n    tweet = re.sub(r\"3others\", \"3 others\", tweet)\n    tweet = re.sub(r\"stighefootball\", \"Sam Tighe\", tweet)\n    tweet = re.sub(r\"whatstheimportantvideo\", \"what is the important video\", tweet)\n    tweet = re.sub(r\"ClaudioMeloni\", \"Claudio Meloni\", tweet)\n    tweet = re.sub(r\"DukeSkywalker\", \"Duke Skywalker\", tweet)\n    tweet = re.sub(r\"carsonmwr\", \"Fort Carson\", tweet)\n    tweet = re.sub(r\"offdishduty\", \"off dish duty\", tweet)\n    tweet = re.sub(r\"andword\", \"and word\", tweet)\n    tweet = re.sub(r\"rhodeisland\", \"Rhode Island\", tweet)\n    tweet = re.sub(r\"easternoregon\", \"Eastern Oregon\", tweet)\n    tweet = re.sub(r\"WAwildfire\", \"Washington Wildfire\", tweet)\n    tweet = re.sub(r\"fingerrockfire\", \"Finger Rock Fire\", tweet)\n    tweet = re.sub(r\"57am\", \"57 am\", tweet)\n    tweet = re.sub(r\"fingerrockfire\", \"Finger Rock Fire\", tweet)\n    tweet = re.sub(r\"JacobHoggard\", \"Jacob Hoggard\", tweet)\n    tweet = re.sub(r\"newnewnew\", \"new new new\", tweet)\n    tweet = re.sub(r\"under50\", \"under 50\", tweet)\n    tweet = re.sub(r\"getitbeforeitsgone\", \"get it before it is gone\", tweet)\n    tweet = re.sub(r\"freshoutofthebox\", \"fresh out of the box\", tweet)\n    tweet = re.sub(r\"amwriting\", \"am writing\", tweet)\n    tweet = re.sub(r\"Bokoharm\", \"Boko Haram\", tweet)\n    tweet = re.sub(r\"Nowlike\", \"Now like\", tweet)\n    tweet = re.sub(r\"seasonfrom\", \"season from\", tweet)\n    tweet = re.sub(r\"epicente\", \"epicenter\", tweet)\n    tweet = re.sub(r\"epicenterr\", \"epicenter\", tweet)\n    tweet = re.sub(r\"sicklife\", \"sick life\", tweet)\n    tweet = re.sub(r\"yycweather\", \"Calgary Weather\", tweet)\n    tweet = re.sub(r\"calgarysun\", \"Calgary Sun\", tweet)\n    tweet = re.sub(r\"approachng\", \"approaching\", tweet)\n    tweet = re.sub(r\"evng\", \"evening\", tweet)\n    tweet = re.sub(r\"Sumthng\", \"something\", tweet)\n    tweet = re.sub(r\"EllenPompeo\", \"Ellen Pompeo\", tweet)\n    tweet = re.sub(r\"shondarhimes\", \"Shonda Rhimes\", tweet)\n    tweet = re.sub(r\"ABCNetwork\", \"ABC Network\", tweet)\n    tweet = re.sub(r\"SushmaSwaraj\", \"Sushma Swaraj\", tweet)\n    tweet = re.sub(r\"pray4japan\", \"Pray for Japan\", tweet)\n    tweet = re.sub(r\"hope4japan\", \"Hope for Japan\", tweet)\n    tweet = re.sub(r\"Illusionimagess\", \"Illusion images\", tweet)\n    tweet = re.sub(r\"SummerUnderTheStars\", \"Summer Under The Stars\", tweet)\n    tweet = re.sub(r\"ShallWeDance\", \"Shall We Dance\", tweet)\n    tweet = re.sub(r\"TCMParty\", \"TCM Party\", tweet)\n    tweet = re.sub(r\"marijuananews\", \"marijuana news\", tweet)\n    tweet = re.sub(r\"onbeingwithKristaTippett\", \"on being with Krista Tippett\", tweet)\n    tweet = re.sub(r\"Beingtweets\", \"Being tweets\", tweet)\n    tweet = re.sub(r\"newauthors\", \"new authors\", tweet)\n    tweet = re.sub(r\"remedyyyy\", \"remedy\", tweet)\n    tweet = re.sub(r\"44PM\", \"44 PM\", tweet)\n    tweet = re.sub(r\"HeadlinesApp\", \"Headlines App\", tweet)\n    tweet = re.sub(r\"40PM\", \"40 PM\", tweet)\n    tweet = re.sub(r\"myswc\", \"Severe Weather Center\", tweet)\n    tweet = re.sub(r\"ithats\", \"that is\", tweet)\n    tweet = re.sub(r\"icouldsitinthismomentforever\", \"I could sit in this moment forever\", tweet)\n    tweet = re.sub(r\"FatLoss\", \"Fat Loss\", tweet)\n    tweet = re.sub(r\"02PM\", \"02 PM\", tweet)\n    tweet = re.sub(r\"MetroFmTalk\", \"Metro Fm Talk\", tweet)\n    tweet = re.sub(r\"Bstrd\", \"bastard\", tweet)\n    tweet = re.sub(r\"bldy\", \"bloody\", tweet)\n    tweet = re.sub(r\"MetrofmTalk\", \"Metro Fm Talk\", tweet)\n    tweet = re.sub(r\"terrorismturn\", \"terrorism turn\", tweet)\n    tweet = re.sub(r\"BBCNewsAsia\", \"BBC News Asia\", tweet)\n    tweet = re.sub(r\"BehindTheScenes\", \"Behind The Scenes\", tweet)\n    tweet = re.sub(r\"GeorgeTakei\", \"George Takei\", tweet)\n    tweet = re.sub(r\"WomensWeeklyMag\", \"Womens Weekly Magazine\", tweet)\n    tweet = re.sub(r\"SurvivorsGuidetoEarth\", \"Survivors Guide to Earth\", tweet)\n    tweet = re.sub(r\"incubusband\", \"incubus band\", tweet)\n    tweet = re.sub(r\"Babypicturethis\", \"Baby picture this\", tweet)\n    tweet = re.sub(r\"BombEffects\", \"Bomb Effects\", tweet)\n    tweet = re.sub(r\"win10\", \"Windows 10\", tweet)\n    tweet = re.sub(r\"idkidk\", \"I do not know I do not know\", tweet)\n    tweet = re.sub(r\"TheWalkingDead\", \"The Walking Dead\", tweet)\n    tweet = re.sub(r\"amyschumer\", \"Amy Schumer\", tweet)\n    tweet = re.sub(r\"crewlist\", \"crew list\", tweet)\n    tweet = re.sub(r\"Erdogans\", \"Erdogan\", tweet)\n    tweet = re.sub(r\"BBCLive\", \"BBC Live\", tweet)\n    tweet = re.sub(r\"TonyAbbottMHR\", \"Tony Abbott\", tweet)\n    tweet = re.sub(r\"paulmyerscough\", \"Paul Myerscough\", tweet)\n    tweet = re.sub(r\"georgegallagher\", \"George Gallagher\", tweet)\n    tweet = re.sub(r\"JimmieJohnson\", \"Jimmie Johnson\", tweet)\n    tweet = re.sub(r\"pctool\", \"pc tool\", tweet)\n    tweet = re.sub(r\"DoingHashtagsRight\", \"Doing Hashtags Right\", tweet)\n    tweet = re.sub(r\"ThrowbackThursday\", \"Throwback Thursday\", tweet)\n    tweet = re.sub(r\"SnowBackSunday\", \"Snowback Sunday\", tweet)\n    tweet = re.sub(r\"LakeEffect\", \"Lake Effect\", tweet)\n    tweet = re.sub(r\"RTphotographyUK\", \"Richard Thomas Photography UK\", tweet)\n    tweet = re.sub(r\"BigBang_CBS\", \"Big Bang CBS\", tweet)\n    tweet = re.sub(r\"writerslife\", \"writers life\", tweet)\n    tweet = re.sub(r\"NaturalBirth\", \"Natural Birth\", tweet)\n    tweet = re.sub(r\"UnusualWords\", \"Unusual Words\", tweet)\n    tweet = re.sub(r\"wizkhalifa\", \"Wiz Khalifa\", tweet)\n    tweet = re.sub(r\"acreativedc\", \"a creative DC\", tweet)\n    tweet = re.sub(r\"vscodc\", \"vsco DC\", tweet)\n    tweet = re.sub(r\"VSCOcam\", \"vsco camera\", tweet)\n    tweet = re.sub(r\"TheBEACHDC\", \"The beach DC\", tweet)\n    tweet = re.sub(r\"buildingmuseum\", \"building museum\", tweet)\n    tweet = re.sub(r\"WorldOil\", \"World Oil\", tweet)\n    tweet = re.sub(r\"redwedding\", \"red wedding\", tweet)\n    tweet = re.sub(r\"AmazingRaceCanada\", \"Amazing Race Canada\", tweet)\n    tweet = re.sub(r\"WakeUpAmerica\", \"Wake Up America\", tweet)\n    tweet = re.sub(r\"\\\\Allahuakbar\\\\\", \"Allahu Akbar\", tweet)\n    tweet = re.sub(r\"bleased\", \"blessed\", tweet)\n    tweet = re.sub(r\"nigeriantribune\", \"Nigerian Tribune\", tweet)\n    tweet = re.sub(r\"HIDEO_KOJIMA_EN\", \"Hideo Kojima\", tweet)\n    tweet = re.sub(r\"FusionFestival\", \"Fusion Festival\", tweet)\n    tweet = re.sub(r\"50Mixed\", \"50 Mixed\", tweet)\n    tweet = re.sub(r\"NoAgenda\", \"No Agenda\", tweet)\n    tweet = re.sub(r\"WhiteGenocide\", \"White Genocide\", tweet)\n    tweet = re.sub(r\"dirtylying\", \"dirty lying\", tweet)\n    tweet = re.sub(r\"SyrianRefugees\", \"Syrian Refugees\", tweet)\n    tweet = re.sub(r\"changetheworld\", \"change the world\", tweet)\n    tweet = re.sub(r\"Ebolacase\", \"Ebola case\", tweet)\n    tweet = re.sub(r\"mcgtech\", \"mcg technologies\", tweet)\n    tweet = re.sub(r\"withweapons\", \"with weapons\", tweet)\n    tweet = re.sub(r\"advancedwarfare\", \"advanced warfare\", tweet)\n    tweet = re.sub(r\"letsFootball\", \"let us Football\", tweet)\n    tweet = re.sub(r\"LateNiteMix\", \"late night mix\", tweet)\n    tweet = re.sub(r\"PhilCollinsFeed\", \"Phil Collins\", tweet)\n    tweet = re.sub(r\"RudyHavenstein\", \"Rudy Havenstein\", tweet)\n    tweet = re.sub(r\"22PM\", \"22 PM\", tweet)\n    tweet = re.sub(r\"54am\", \"54 AM\", tweet)\n    tweet = re.sub(r\"38am\", \"38 AM\", tweet)\n    tweet = re.sub(r\"OldFolkExplainStuff\", \"Old Folk Explain Stuff\", tweet)\n    tweet = re.sub(r\"BlacklivesMatter\", \"Black Lives Matter\", tweet)\n    tweet = re.sub(r\"InsaneLimits\", \"Insane Limits\", tweet)\n    tweet = re.sub(r\"youcantsitwithus\", \"you cannot sit with us\", tweet)\n    tweet = re.sub(r\"2k15\", \"2015\", tweet)\n    tweet = re.sub(r\"TheIran\", \"Iran\", tweet)\n    tweet = re.sub(r\"JimmyFallon\", \"Jimmy Fallon\", tweet)\n    tweet = re.sub(r\"AlbertBrooks\", \"Albert Brooks\", tweet)\n    tweet = re.sub(r\"defense_news\", \"defense news\", tweet)\n    tweet = re.sub(r\"nuclearrcSA\", \"Nuclear Risk Control Self Assessment\", tweet)\n    tweet = re.sub(r\"Auspol\", \"Australia Politics\", tweet)\n    tweet = re.sub(r\"NuclearPower\", \"Nuclear Power\", tweet)\n    tweet = re.sub(r\"WhiteTerrorism\", \"White Terrorism\", tweet)\n    tweet = re.sub(r\"truthfrequencyradio\", \"Truth Frequency Radio\", tweet)\n    tweet = re.sub(r\"ErasureIsNotEquality\", \"Erasure is not equality\", tweet)\n    tweet = re.sub(r\"ProBonoNews\", \"Pro Bono News\", tweet)\n    tweet = re.sub(r\"JakartaPost\", \"Jakarta Post\", tweet)\n    tweet = re.sub(r\"toopainful\", \"too painful\", tweet)\n    tweet = re.sub(r\"melindahaunton\", \"Melinda Haunton\", tweet)\n    tweet = re.sub(r\"NoNukes\", \"No Nukes\", tweet)\n    tweet = re.sub(r\"curryspcworld\", \"Currys PC World\", tweet)\n    tweet = re.sub(r\"ineedcake\", \"I need cake\", tweet)\n    tweet = re.sub(r\"blackforestgateau\", \"black forest gateau\", tweet)\n    tweet = re.sub(r\"BBCOne\", \"BBC One\", tweet)\n    tweet = re.sub(r\"AlexxPage\", \"Alex Page\", tweet)\n    tweet = re.sub(r\"jonathanserrie\", \"Jonathan Serrie\", tweet)\n    tweet = re.sub(r\"SocialJerkBlog\", \"Social Jerk Blog\", tweet)\n    tweet = re.sub(r\"ChelseaVPeretti\", \"Chelsea Peretti\", tweet)\n    tweet = re.sub(r\"irongiant\", \"iron giant\", tweet)\n    tweet = re.sub(r\"RonFunches\", \"Ron Funches\", tweet)\n    tweet = re.sub(r\"TimCook\", \"Tim Cook\", tweet)\n    tweet = re.sub(r\"sebastianstanisaliveandwell\", \"Sebastian Stan is alive and well\", tweet)\n    tweet = re.sub(r\"Madsummer\", \"Mad summer\", tweet)\n    tweet = re.sub(r\"NowYouKnow\", \"Now you know\", tweet)\n    tweet = re.sub(r\"concertphotography\", \"concert photography\", tweet)\n    tweet = re.sub(r\"TomLandry\", \"Tom Landry\", tweet)\n    tweet = re.sub(r\"showgirldayoff\", \"show girl day off\", tweet)\n    tweet = re.sub(r\"Yougslavia\", \"Yugoslavia\", tweet)\n    tweet = re.sub(r\"QuantumDataInformatics\", \"Quantum Data Informatics\", tweet)\n    tweet = re.sub(r\"FromTheDesk\", \"From The Desk\", tweet)\n    tweet = re.sub(r\"TheaterTrial\", \"Theater Trial\", tweet)\n    tweet = re.sub(r\"CatoInstitute\", \"Cato Institute\", tweet)\n    tweet = re.sub(r\"EmekaGift\", \"Emeka Gift\", tweet)\n    tweet = re.sub(r\"LetsBe_Rational\", \"Let us be rational\", tweet)\n    tweet = re.sub(r\"Cynicalreality\", \"Cynical reality\", tweet)\n    tweet = re.sub(r\"FredOlsenCruise\", \"Fred Olsen Cruise\", tweet)\n    tweet = re.sub(r\"NotSorry\", \"not sorry\", tweet)\n    tweet = re.sub(r\"UseYourWords\", \"use your words\", tweet)\n    tweet = re.sub(r\"WordoftheDay\", \"word of the day\", tweet)\n    tweet = re.sub(r\"Dictionarycom\", \"Dictionary.com\", tweet)\n    tweet = re.sub(r\"TheBrooklynLife\", \"The Brooklyn Life\", tweet)\n    tweet = re.sub(r\"jokethey\", \"joke they\", tweet)\n    tweet = re.sub(r\"nflweek1picks\", \"NFL week 1 picks\", tweet)\n    tweet = re.sub(r\"uiseful\", \"useful\", tweet)\n    tweet = re.sub(r\"JusticeDotOrg\", \"The American Association for Justice\", tweet)\n    tweet = re.sub(r\"autoaccidents\", \"auto accidents\", tweet)\n    tweet = re.sub(r\"SteveGursten\", \"Steve Gursten\", tweet)\n    tweet = re.sub(r\"MichiganAutoLaw\", \"Michigan Auto Law\", tweet)\n    tweet = re.sub(r\"birdgang\", \"bird gang\", tweet)\n    tweet = re.sub(r\"nflnetwork\", \"NFL Network\", tweet)\n    tweet = re.sub(r\"NYDNSports\", \"NY Daily News Sports\", tweet)\n    tweet = re.sub(r\"RVacchianoNYDN\", \"Ralph Vacchiano NY Daily News\", tweet)\n    tweet = re.sub(r\"EdmontonEsks\", \"Edmonton Eskimos\", tweet)\n    tweet = re.sub(r\"david_brelsford\", \"David Brelsford\", tweet)\n    tweet = re.sub(r\"TOI_India\", \"The Times of India\", tweet)\n    tweet = re.sub(r\"hegot\", \"he got\", tweet)\n    tweet = re.sub(r\"SkinsOn9\", \"Skins on 9\", tweet)\n    tweet = re.sub(r\"sothathappened\", \"so that happened\", tweet)\n    tweet = re.sub(r\"LCOutOfDoors\", \"LC Out Of Doors\", tweet)\n    tweet = re.sub(r\"NationFirst\", \"Nation First\", tweet)\n    tweet = re.sub(r\"IndiaToday\", \"India Today\", tweet)\n    tweet = re.sub(r\"HLPS\", \"helps\", tweet)\n    tweet = re.sub(r\"HOSTAGESTHROSW\", \"hostages throw\", tweet)\n    tweet = re.sub(r\"SNCTIONS\", \"sanctions\", tweet)\n    tweet = re.sub(r\"BidTime\", \"Bid Time\", tweet)\n    tweet = re.sub(r\"crunchysensible\", \"crunchy sensible\", tweet)\n    tweet = re.sub(r\"RandomActsOfRomance\", \"Random acts of romance\", tweet)\n    tweet = re.sub(r\"MomentsAtHill\", \"Moments at hill\", tweet)\n    tweet = re.sub(r\"eatshit\", \"eat shit\", tweet)\n    tweet = re.sub(r\"liveleakfun\", \"live leak fun\", tweet)\n    tweet = re.sub(r\"SahelNews\", \"Sahel News\", tweet)\n    tweet = re.sub(r\"abc7newsbayarea\", \"ABC 7 News Bay Area\", tweet)\n    tweet = re.sub(r\"facilitiesmanagement\", \"facilities management\", tweet)\n    tweet = re.sub(r\"facilitydude\", \"facility dude\", tweet)\n    tweet = re.sub(r\"CampLogistics\", \"Camp logistics\", tweet)\n    tweet = re.sub(r\"alaskapublic\", \"Alaska public\", tweet)\n    tweet = re.sub(r\"MarketResearch\", \"Market Research\", tweet)\n    tweet = re.sub(r\"AccuracyEsports\", \"Accuracy Esports\", tweet)\n    tweet = re.sub(r\"TheBodyShopAust\", \"The Body Shop Australia\", tweet)\n    tweet = re.sub(r\"yychail\", \"Calgary hail\", tweet)\n    tweet = re.sub(r\"yyctraffic\", \"Calgary traffic\", tweet)\n    tweet = re.sub(r\"eliotschool\", \"eliot school\", tweet)\n    tweet = re.sub(r\"TheBrokenCity\", \"The Broken City\", tweet)\n    tweet = re.sub(r\"OldsFireDept\", \"Olds Fire Department\", tweet)\n    tweet = re.sub(r\"RiverComplex\", \"River Complex\", tweet)\n    tweet = re.sub(r\"fieldworksmells\", \"field work smells\", tweet)\n    tweet = re.sub(r\"IranElection\", \"Iran Election\", tweet)\n    tweet = re.sub(r\"glowng\", \"glowing\", tweet)\n    tweet = re.sub(r\"kindlng\", \"kindling\", tweet)\n    tweet = re.sub(r\"riggd\", \"rigged\", tweet)\n    tweet = re.sub(r\"slownewsday\", \"slow news day\", tweet)\n    tweet = re.sub(r\"MyanmarFlood\", \"Myanmar Flood\", tweet)\n    tweet = re.sub(r\"abc7chicago\", \"ABC 7 Chicago\", tweet)\n    tweet = re.sub(r\"copolitics\", \"Colorado Politics\", tweet)\n    tweet = re.sub(r\"AdilGhumro\", \"Adil Ghumro\", tweet)\n    tweet = re.sub(r\"netbots\", \"net bots\", tweet)\n    tweet = re.sub(r\"byebyeroad\", \"bye bye road\", tweet)\n    tweet = re.sub(r\"massiveflooding\", \"massive flooding\", tweet)\n    tweet = re.sub(r\"EndofUS\", \"End of United States\", tweet)\n    tweet = re.sub(r\"35PM\", \"35 PM\", tweet)\n    tweet = re.sub(r\"greektheatrela\", \"Greek Theatre Los Angeles\", tweet)\n    tweet = re.sub(r\"76mins\", \"76 minutes\", tweet)\n    tweet = re.sub(r\"publicsafetyfirst\", \"public safety first\", tweet)\n    tweet = re.sub(r\"livesmatter\", \"lives matter\", tweet)\n    tweet = re.sub(r\"myhometown\", \"my hometown\", tweet)\n    tweet = re.sub(r\"tankerfire\", \"tanker fire\", tweet)\n    tweet = re.sub(r\"MEMORIALDAY\", \"memorial day\", tweet)\n    tweet = re.sub(r\"MEMORIAL_DAY\", \"memorial day\", tweet)\n    tweet = re.sub(r\"instaxbooty\", \"instagram booty\", tweet)\n    tweet = re.sub(r\"Jerusalem_Post\", \"Jerusalem Post\", tweet)\n    tweet = re.sub(r\"WayneRooney_INA\", \"Wayne Rooney\", tweet)\n    tweet = re.sub(r\"VirtualReality\", \"Virtual Reality\", tweet)\n    tweet = re.sub(r\"OculusRift\", \"Oculus Rift\", tweet)\n    tweet = re.sub(r\"OwenJones84\", \"Owen Jones\", tweet)\n    tweet = re.sub(r\"jeremycorbyn\", \"Jeremy Corbyn\", tweet)\n    tweet = re.sub(r\"paulrogers002\", \"Paul Rogers\", tweet)\n    tweet = re.sub(r\"mortalkombatx\", \"Mortal Kombat X\", tweet)\n    tweet = re.sub(r\"mortalkombat\", \"Mortal Kombat\", tweet)\n    tweet = re.sub(r\"FilipeCoelho92\", \"Filipe Coelho\", tweet)\n    tweet = re.sub(r\"OnlyQuakeNews\", \"Only Quake News\", tweet)\n    tweet = re.sub(r\"kostumes\", \"costumes\", tweet)\n    tweet = re.sub(r\"YEEESSSS\", \"yes\", tweet)\n    tweet = re.sub(r\"ToshikazuKatayama\", \"Toshikazu Katayama\", tweet)\n    tweet = re.sub(r\"IntlDevelopment\", \"Intl Development\", tweet)\n    tweet = re.sub(r\"ExtremeWeather\", \"Extreme Weather\", tweet)\n    tweet = re.sub(r\"WereNotGruberVoters\", \"We are not gruber voters\", tweet)\n    tweet = re.sub(r\"NewsThousands\", \"News Thousands\", tweet)\n    tweet = re.sub(r\"EdmundAdamus\", \"Edmund Adamus\", tweet)\n    tweet = re.sub(r\"EyewitnessWV\", \"Eye witness WV\", tweet)\n    tweet = re.sub(r\"PhiladelphiaMuseu\", \"Philadelphia Museum\", tweet)\n    tweet = re.sub(r\"DublinComicCon\", \"Dublin Comic Con\", tweet)\n    tweet = re.sub(r\"NicholasBrendon\", \"Nicholas Brendon\", tweet)\n    tweet = re.sub(r\"Alltheway80s\", \"All the way 80s\", tweet)\n    tweet = re.sub(r\"FromTheField\", \"From the field\", tweet)\n    tweet = re.sub(r\"NorthIowa\", \"North Iowa\", tweet)\n    tweet = re.sub(r\"WillowFire\", \"Willow Fire\", tweet)\n    tweet = re.sub(r\"MadRiverComplex\", \"Mad River Complex\", tweet)\n    tweet = re.sub(r\"feelingmanly\", \"feeling manly\", tweet)\n    tweet = re.sub(r\"stillnotoverit\", \"still not over it\", tweet)\n    tweet = re.sub(r\"FortitudeValley\", \"Fortitude Valley\", tweet)\n    tweet = re.sub(r\"CoastpowerlineTramTr\", \"Coast powerline\", tweet)\n    tweet = re.sub(r\"ServicesGold\", \"Services Gold\", tweet)\n    tweet = re.sub(r\"NewsbrokenEmergency\", \"News broken emergency\", tweet)\n    tweet = re.sub(r\"Evaucation\", \"evacuation\", tweet)\n    tweet = re.sub(r\"leaveevacuateexitbe\", \"leave evacuate exit be\", tweet)\n    tweet = re.sub(r\"P_EOPLE\", \"PEOPLE\", tweet)\n    tweet = re.sub(r\"Tubestrike\", \"tube strike\", tweet)\n    tweet = re.sub(r\"CLASS_SICK\", \"CLASS SICK\", tweet)\n    tweet = re.sub(r\"localplumber\", \"local plumber\", tweet)\n    tweet = re.sub(r\"awesomejobsiri\", \"awesome job siri\", tweet)\n    tweet = re.sub(r\"PayForItHow\", \"Pay for it how\", tweet)\n    tweet = re.sub(r\"ThisIsAfrica\", \"This is Africa\", tweet)\n    tweet = re.sub(r\"crimeairnetwork\", \"crime air network\", tweet)\n    tweet = re.sub(r\"KimAcheson\", \"Kim Acheson\", tweet)\n    tweet = re.sub(r\"cityofcalgary\", \"City of Calgary\", tweet)\n    tweet = re.sub(r\"prosyndicate\", \"pro syndicate\", tweet)\n    tweet = re.sub(r\"660NEWS\", \"660 NEWS\", tweet)\n    tweet = re.sub(r\"BusInsMagazine\", \"Business Insurance Magazine\", tweet)\n    tweet = re.sub(r\"wfocus\", \"focus\", tweet)\n    tweet = re.sub(r\"ShastaDam\", \"Shasta Dam\", tweet)\n    tweet = re.sub(r\"go2MarkFranco\", \"Mark Franco\", tweet)\n    tweet = re.sub(r\"StephGHinojosa\", \"Steph Hinojosa\", tweet)\n    tweet = re.sub(r\"Nashgrier\", \"Nash Grier\", tweet)\n    tweet = re.sub(r\"NashNewVideo\", \"Nash new video\", tweet)\n    tweet = re.sub(r\"IWouldntGetElectedBecause\", \"I would not get elected because\", tweet)\n    tweet = re.sub(r\"SHGames\", \"Sledgehammer Games\", tweet)\n    tweet = re.sub(r\"bedhair\", \"bed hair\", tweet)\n    tweet = re.sub(r\"JoelHeyman\", \"Joel Heyman\", tweet)\n    tweet = re.sub(r\"viaYouTube\", \"via YouTube\", tweet)\n           \n    # links and urls\n    tweet = re.sub(r\"https?:\\/\\/t.co\\/[A-Za-z0-9]+\", \"\", tweet)\n        \n    # punctuation\n    punctuations = '@#!?+&*[]-%.:/();$=><|{}^' + \"'`\"\n    for p in punctuations:\n        tweet = tweet.replace(p, f' {p} ')\n        \n    \n    tweet = tweet.replace('...', ' ... ')\n    if '...' not in tweet:\n        tweet = tweet.replace('..', ' ... ')      \n        \n    # Acronyms\n    tweet = re.sub(r\"MH370\", \"Malaysia Airlines Flight 370\", tweet)\n    tweet = re.sub(r\"mÌ¼sica\", \"music\", tweet)\n    tweet = re.sub(r\"okwx\", \"Oklahoma City Weather\", tweet)\n    tweet = re.sub(r\"arwx\", \"Arkansas Weather\", tweet)    \n    tweet = re.sub(r\"gawx\", \"Georgia Weather\", tweet)  \n    tweet = re.sub(r\"scwx\", \"South Carolina Weather\", tweet)  \n    tweet = re.sub(r\"cawx\", \"California Weather\", tweet)\n    tweet = re.sub(r\"tnwx\", \"Tennessee Weather\", tweet)\n    tweet = re.sub(r\"azwx\", \"Arizona Weather\", tweet)  \n    tweet = re.sub(r\"alwx\", \"Alabama Weather\", tweet)\n    tweet = re.sub(r\"wordpressdotcom\", \"wordpress\", tweet)    \n    tweet = re.sub(r\"usNWSgov\", \"United States National Weather Service\", tweet)\n    tweet = re.sub(r\"Suruc\", \"Sanliurfa\", tweet)   \n    \n    # Grouping same words without embeddings\n    tweet = re.sub(r\"Bestnaijamade\", \"bestnaijamade\", tweet)\n    tweet = re.sub(r\"SOUDELOR\", \"Soudelor\", tweet)\n    \n    return tweet","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-12-05T07:03:33.122831Z","iopub.status.idle":"2021-12-05T07:03:33.123445Z","shell.execute_reply.started":"2021-12-05T07:03:33.123213Z","shell.execute_reply":"2021-12-05T07:03:33.123237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['text'] = train['text'].apply(lambda s : clean(s))","metadata":{"execution":{"iopub.status.busy":"2021-12-05T07:03:33.124645Z","iopub.status.idle":"2021-12-05T07:03:33.125331Z","shell.execute_reply.started":"2021-12-05T07:03:33.125084Z","shell.execute_reply":"2021-12-05T07:03:33.12511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[train.target == 0]","metadata":{"execution":{"iopub.status.busy":"2021-12-05T07:03:33.126575Z","iopub.status.idle":"2021-12-05T07:03:33.127217Z","shell.execute_reply.started":"2021-12-05T07:03:33.126983Z","shell.execute_reply":"2021-12-05T07:03:33.127008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the above dataframe 'target'=0 indicates normal tweets with no information about disaster or anything in them. And similarly 'target'=1 indicates disaster tweets.\n\nIt is always a good practice to have a copy of your dataset in case you mess things up in future. ","metadata":{}},{"cell_type":"code","source":"train_cleaned_df = train.copy()","metadata":{"execution":{"iopub.status.busy":"2021-12-05T07:03:33.128392Z","iopub.status.idle":"2021-12-05T07:03:33.129023Z","shell.execute_reply.started":"2021-12-05T07:03:33.128788Z","shell.execute_reply":"2021-12-05T07:03:33.128812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading Pretrained BERT Model\n\nWe have already imported the BERT model and it's respective tokenizer. \n\nThe bert models can be found from here : https://huggingface.co/models\nWe are using the 'bert-large-uncased' model here. But more models can be found in the above links.\n\n'bert-large-uncased' is the exact bert model we are going to use. We use AutoTokenizer and it will create a generic tokenizer class that will be instantiated as one of the tokenizer classes of the hugging face library when created with the AutoTokenizer.from_pretrained() class method. We can pass the BERT model of our choice through it to get the respective tokenizers.","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('bert-large-uncased')\nbert = TFBertModel.from_pretrained('bert-large-uncased')","metadata":{"execution":{"iopub.status.busy":"2021-12-05T07:03:33.130216Z","iopub.status.idle":"2021-12-05T07:03:33.130863Z","shell.execute_reply.started":"2021-12-05T07:03:33.13063Z","shell.execute_reply":"2021-12-05T07:03:33.130655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer('Shine on you crazy diamond.')","metadata":{"execution":{"iopub.status.busy":"2021-12-05T07:03:33.132031Z","iopub.status.idle":"2021-12-05T07:03:33.132668Z","shell.execute_reply.started":"2021-12-05T07:03:33.132407Z","shell.execute_reply":"2021-12-05T07:03:33.132431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Converting inputs according to BERT input\n\nThe for loop will return every entry at the 'text' column of the **train** dataset. We are splitting the text into words and storing the length of the sequence of words in an array. The outside **max** function will return the maximum element from the list which is basically the text with highest number of words in the dataset.","metadata":{}},{"cell_type":"code","source":"print(\"max len of tweets\",max([len(x.split()) for x in train.text]))","metadata":{"execution":{"iopub.status.busy":"2021-12-05T07:03:33.133862Z","iopub.status.idle":"2021-12-05T07:03:33.134489Z","shell.execute_reply.started":"2021-12-05T07:03:33.134255Z","shell.execute_reply":"2021-12-05T07:03:33.134279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the following piece of code we are converting the input dataset in form of tokenizer, **add_special_tokens** is set to true because we want to replace unknown words to special tokens, we set the max length to 73 and we use truncation and padding. We don't need token type ids but we need attention mask to get the output in form of BERT inputs. This tokenizer will output a **input_ids** and **attention_masks** which we will use to the BERT model as inputs. The **input_ids** are simply the ids of the numeric representations of the texts. **Attention_mask** is useful when we add padding to the input tokens. The attention mask tells us which input_ids correspond to padding. Padding is added because we want all the input sentences to be of the same length (at least for a batch) so that we are able to form tensor objects properly.","metadata":{}},{"cell_type":"code","source":"x_train = tokenizer(\n    text=train.text.tolist(),\n    add_special_tokens=True,\n    max_length=73,\n    truncation=True,\n    padding=True,\n    return_tensors='tf',\n    return_token_type_ids = False,\n    return_attention_mask = True,\n    verbose = True)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-05T07:03:33.135687Z","iopub.status.idle":"2021-12-05T07:03:33.136313Z","shell.execute_reply.started":"2021-12-05T07:03:33.136078Z","shell.execute_reply":"2021-12-05T07:03:33.136104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train['input_ids'].shape","metadata":{"execution":{"iopub.status.busy":"2021-12-05T07:03:33.137485Z","iopub.status.idle":"2021-12-05T07:03:33.138121Z","shell.execute_reply.started":"2021-12-05T07:03:33.137887Z","shell.execute_reply":"2021-12-05T07:03:33.13791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train['attention_mask'].shape","metadata":{"execution":{"iopub.status.busy":"2021-12-05T07:03:33.1393Z","iopub.status.idle":"2021-12-05T07:03:33.139948Z","shell.execute_reply.started":"2021-12-05T07:03:33.139714Z","shell.execute_reply":"2021-12-05T07:03:33.139739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are saving the y_train values in a list.","metadata":{}},{"cell_type":"code","source":"y_train = train.target.values\nprint(len(y_train))","metadata":{"execution":{"iopub.status.busy":"2021-12-05T07:03:33.141123Z","iopub.status.idle":"2021-12-05T07:03:33.141762Z","shell.execute_reply.started":"2021-12-05T07:03:33.1415Z","shell.execute_reply":"2021-12-05T07:03:33.141523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.target.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-12-05T07:03:33.142925Z","iopub.status.idle":"2021-12-05T07:03:33.143543Z","shell.execute_reply.started":"2021-12-05T07:03:33.143308Z","shell.execute_reply":"2021-12-05T07:03:33.143333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building the model ","metadata":{}},{"cell_type":"markdown","source":"In the followin piece of code, we set two input layers for the model one for input ids and other for the attention mask. We pass them to the bert model. The bert model provides two output. **sequence_output** and **pooled_output**, we will be using the pooled output as it represents the actual features from the model. SO this is the second out so we use [1].\nThen we use a very simple ANN with two layers with 120 neuros and 32 neurons, respectively. We use relu activation function in the output with some dropouts. We finally use a single neuron in the output as this is a binary classification problem of prediction a given tweet is a disaster one or not. (0 or 1)\n\nWe set the second layer as **\"trainable\"** because we want the BERT model to be trainable.","metadata":{}},{"cell_type":"code","source":"max_len = 73\n\ninput_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\ninput_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\n\nembeddings = bert(input_ids,attention_mask = input_mask)[1] \n\nout = tf.keras.layers.Dropout(0.1)(embeddings)\nout = Dense(128, activation='relu')(out)\nout = tf.keras.layers.Dropout(0.1)(out)\nout = Dense(32,activation = 'relu')(out)\n\ny = Dense(1,activation = 'sigmoid')(out)\n    \nmodel = tf.keras.Model(inputs=[input_ids, input_mask], outputs=y)\nmodel.layers[2].trainable = True","metadata":{"execution":{"iopub.status.busy":"2021-12-05T07:03:33.144759Z","iopub.status.idle":"2021-12-05T07:03:33.145385Z","shell.execute_reply.started":"2021-12-05T07:03:33.14515Z","shell.execute_reply":"2021-12-05T07:03:33.145175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-12-05T07:03:33.146538Z","iopub.status.idle":"2021-12-05T07:03:33.147171Z","shell.execute_reply.started":"2021-12-05T07:03:33.146938Z","shell.execute_reply":"2021-12-05T07:03:33.146962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We use [Adam](http://https://keras.io/api/optimizers/adam/) optimizer to trackback our losses.Then we use [BinaryCrossentropy](http://https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/loss-functions/binary-crossentropy) to calculate our losses. And use the **accuracy** metric, and then we finally complie our model.\n\nIf you are not aware how Adam optimizer or BinaryCrossEntropy loss works then  please refer to these to these amazing resources.\n\n**Adam**\n1. Blog : https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/\n2. Video : https://www.youtube.com/watch?v=JXQT_vxqwIs&ab_channel=DeepLearningAI\n\n**BinaryCrossEntropy**\n1. Blog : https://www.bing.com/search?q=BinaryCrossEntropy&cvid=986370ad4e7e4caa92d9aba3a0958783&aqs=edge.0.69i59l2j0l7.1874j0j4&FORM=ANAB01&PC=ACTS\n2. Video : https://www.youtube.com/watch?v=Md4b67HvmRo&ab_channel=DigitalSreeni","metadata":{}},{"cell_type":"markdown","source":"# Training phase","metadata":{}},{"cell_type":"code","source":"optimizer = Adam(\n    learning_rate=5e-05, # this learning rate is taken from huggingface website \n    epsilon=1e-08,\n    decay=0.01,\n    clipnorm=1.0)\n\n# Set loss and metrics\nloss = BinaryCrossentropy(from_logits = True)\nmetric = BinaryAccuracy('accuracy'),\n# Compile the model\nmodel.compile(\n    optimizer = optimizer,\n    loss = loss, \n    metrics = metric)","metadata":{"execution":{"iopub.status.busy":"2021-12-05T07:03:33.148351Z","iopub.status.idle":"2021-12-05T07:03:33.149004Z","shell.execute_reply.started":"2021-12-05T07:03:33.148772Z","shell.execute_reply":"2021-12-05T07:03:33.148796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model(model, show_shapes = True)","metadata":{"execution":{"iopub.status.busy":"2021-12-05T07:03:33.150191Z","iopub.status.idle":"2021-12-05T07:03:33.150815Z","shell.execute_reply.started":"2021-12-05T07:03:33.150571Z","shell.execute_reply":"2021-12-05T07:03:33.150609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now it's time for us to train our model. We pass the two inputs as key-value paid. We set the batchsize to be 32 and train the model for 12 epochs. We are not performing validation split here as we don't have a huge dataset so it would be better to use the whole training data to train the model instead of splitting it. We will be using the test dataset to test our outputs for our given problem set. ","metadata":{}},{"cell_type":"code","source":"train_history = model.fit(\n    x ={'input_ids':x_train['input_ids'],'attention_mask':x_train['attention_mask']} ,\n    y = y_train, epochs=12, batch_size=32\n)","metadata":{"execution":{"iopub.status.busy":"2021-12-05T07:03:33.151968Z","iopub.status.idle":"2021-12-05T07:03:33.15261Z","shell.execute_reply.started":"2021-12-05T07:03:33.152352Z","shell.execute_reply":"2021-12-05T07:03:33.152375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing Phase","metadata":{}},{"cell_type":"code","source":"test.head(8)","metadata":{"execution":{"iopub.status.busy":"2021-11-05T07:38:26.231891Z","iopub.execute_input":"2021-11-05T07:38:26.232089Z","iopub.status.idle":"2021-11-05T07:38:26.242118Z","shell.execute_reply.started":"2021-11-05T07:38:26.232066Z","shell.execute_reply":"2021-11-05T07:38:26.241408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is observed that without cleaning our test dataset the model provides better output.","metadata":{}},{"cell_type":"markdown","source":"We use the same tokenizer as we used in training data, then we pass it to the model and ask it to predict each entries in the test dataset.","metadata":{}},{"cell_type":"code","source":"x_test = tokenizer(\n    text=test.text.tolist(),\n    add_special_tokens=True,\n    max_length=73,\n    truncation=True,\n    padding=True, \n    return_tensors='tf',\n    return_token_type_ids = False,\n    return_attention_mask = True,\n    verbose = True)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-05T07:38:26.243501Z","iopub.execute_input":"2021-11-05T07:38:26.244062Z","iopub.status.idle":"2021-11-05T07:38:26.559465Z","shell.execute_reply.started":"2021-11-05T07:38:26.244003Z","shell.execute_reply":"2021-11-05T07:38:26.558668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(x_test[2])","metadata":{"execution":{"iopub.status.busy":"2021-11-05T07:38:26.561739Z","iopub.execute_input":"2021-11-05T07:38:26.561954Z","iopub.status.idle":"2021-11-05T07:38:26.567363Z","shell.execute_reply.started":"2021-11-05T07:38:26.56193Z","shell.execute_reply":"2021-11-05T07:38:26.566674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted = model.predict({'input_ids':x_test['input_ids'],'attention_mask':x_test['attention_mask']})","metadata":{"execution":{"iopub.status.busy":"2021-11-05T07:38:26.568676Z","iopub.execute_input":"2021-11-05T07:38:26.569084Z","iopub.status.idle":"2021-11-05T07:39:04.477777Z","shell.execute_reply.started":"2021-11-05T07:38:26.569047Z","shell.execute_reply":"2021-11-05T07:39:04.477011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If a value in **predicted** (which is a python list of probabilities for each text example) is greater than 0.5 then np.where changes it to 1, or else if it's less than 0.5 it changes it to 0.\n\nhttps://numpy.org/doc/stable/reference/generated/numpy.where.html","metadata":{}},{"cell_type":"code","source":"y_predicted = np.where(predicted>0.5,1,0)","metadata":{"execution":{"iopub.status.busy":"2021-11-05T07:39:04.481286Z","iopub.execute_input":"2021-11-05T07:39:04.4818Z","iopub.status.idle":"2021-11-05T07:39:04.487966Z","shell.execute_reply.started":"2021-11-05T07:39:04.481768Z","shell.execute_reply":"2021-11-05T07:39:04.487213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_predictedd = y_predicted.reshape((1,3263))[0]","metadata":{"execution":{"iopub.status.busy":"2021-11-05T07:39:04.488928Z","iopub.execute_input":"2021-11-05T07:39:04.489122Z","iopub.status.idle":"2021-11-05T07:39:04.499325Z","shell.execute_reply.started":"2021-11-05T07:39:04.489092Z","shell.execute_reply":"2021-11-05T07:39:04.498474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample['id'] = test.id\nsample['target'] = y_predictedd","metadata":{"execution":{"iopub.status.busy":"2021-11-05T07:39:04.500899Z","iopub.execute_input":"2021-11-05T07:39:04.501528Z","iopub.status.idle":"2021-11-05T07:39:04.51163Z","shell.execute_reply.started":"2021-11-05T07:39:04.501492Z","shell.execute_reply":"2021-11-05T07:39:04.510959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample.to_csv('submission_a.csv',index = False)","metadata":{"execution":{"iopub.status.busy":"2021-11-05T07:39:04.514498Z","iopub.execute_input":"2021-11-05T07:39:04.514734Z","iopub.status.idle":"2021-11-05T07:39:04.530291Z","shell.execute_reply.started":"2021-11-05T07:39:04.51471Z","shell.execute_reply":"2021-11-05T07:39:04.529604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}