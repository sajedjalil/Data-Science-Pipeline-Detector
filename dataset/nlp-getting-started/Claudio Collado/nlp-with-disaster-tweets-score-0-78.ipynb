{"cells":[{"metadata":{},"cell_type":"markdown","source":"# NLP with Disaster Tweets"},{"metadata":{},"cell_type":"markdown","source":"# Limpieza de datos / Vectorizacion (BOW / TFIDF) / Ridge Regresion Classifier\n\n## Score 0.78"},{"metadata":{},"cell_type":"markdown","source":"# 1. Librerias\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Paquetes Clasicos\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Paquetes para Preprocesamiento\nimport re\nimport string\nimport nltk\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')\nfrom nltk import word_tokenize          \nfrom nltk.stem import WordNetLemmatizer \nfrom nltk.corpus import stopwords \nimport unicodedata\n\n#Vectorizacion\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n#Pipeline\nfrom sklearn.pipeline import Pipeline\n\n#Tuning de Parametros\nfrom sklearn.model_selection import GridSearchCV\n\n#Clasificadores\nfrom sklearn.linear_model import RidgeClassifier # Classifier using Ridge regression.\nfrom sklearn.linear_model import LogisticRegression # Classifier using Logistic Regression.\nfrom sklearn.linear_model import Perceptron # Classifier using Perceptron.\nfrom sklearn.naive_bayes import MultinomialNB # Classifier using Naive Bayes classifier multinomial\nfrom sklearn.svm import LinearSVC #Classifier using Linear Support Vector\nfrom sklearn.svm import SVC #C-Support Vector\nfrom sklearn.ensemble import GradientBoostingClassifier #Classifier using Gradient Boosting classifier ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Datos"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dataset de entranamiento\ndf_train = pd.read_csv(\"../input/nlp-getting-started/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dataset de test\ndf_test = pd.read_csv(\"../input/nlp-getting-started/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.1 Dataset 'train'"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Observo la constitucion general del Dataset de entrenamiento\n\ndf_train.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dimensiones del dataset\n\ndf_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Cantidad de tweets catalogados como Verdaderos(1) y Falsos(0)\n\ndf_train['target'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Tratamiento de NaN**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Reemplazo los NaN por el string 'None'\n\ndf_train.location.fillna('None', inplace=True)\ndf_train.keyword.fillna('None', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Observo como queda el dataframe\n\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Tweets Repetidos**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Cuento la cantidad de 'text' que se encuentran repetidos\n\nduplicados_train = df_train['text'].duplicated(keep=False)\nduplicados_train.sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Elimino las filas donde 'text' esta repetido\n\ndup_train = df_train[['text','target']][duplicados_train]\ndup_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Verifico que el tamaño del dataset disminuyo la cantidad de repetidos\n\ndf_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A simple vista se observa que alguno de los tweets repetidos tienen como target diferentes valores, lo cual en principio es confuso. Se decide eliminar todos los tweets repetidos"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Elimino las filas donde 'text' esta repetido\n\ndf_train = df_train.drop_duplicates('text',keep=False)\n\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Verifico que el tamaño del dataset disminuyo la cantidad de repetidos\n\ndf_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.2 Dataset 'test'"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Observo la constitucion general del Dataset de test\n\ndf_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dimensiones del dataset\n\ndf_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Tratamiento de NaN**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Reemplazo los NaN por el string 'None'\n\ndf_test.location.fillna('None', inplace=True)\ndf_test.keyword.fillna('None', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Observo como queda el dataframe\n\ndf_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Tweets Repetidos**"},{"metadata":{"trusted":true},"cell_type":"code","source":"duplicados_test = df_test['text'].duplicated(keep=False)\nduplicados_test.sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dup_test = df_test[['text']][duplicados_test]\ndup_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"En este caso no es necesario eliminar los tweets. Seria conveniente verificar que los mismos fueron catalogados de igual forma al finalizar el analisis"},{"metadata":{},"cell_type":"markdown","source":"# 3. Preprocesamiento"},{"metadata":{},"cell_type":"markdown","source":"Se definen funciones especificas para realizar el preprocesamiento de los datos:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transformar el texto a minuscula - OK\n\ndef minuscula(texto):\n    return texto.lower()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Remover URL - OK\n\ndef remover_url(texto):\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url.sub(r'', texto)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Remover Usuarios (''@usuario') - OK\n#Si aparecen usuarios combinados ('@usuario__usuario') queda '_usuario' y la funcion remover_no_alfabeto(texto) lo elimina\n\ndef remover_usuario(texto):\n    text = re.sub(r\"\\@[A-Za-z0-9]+\", \"\", texto)\n    return texto","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remover Emoji - OK\n\ndef remover_emoji(texto):\n    emoji_patrones = re.compile(\n        '['\n        u'\\U0001F600-\\U0001F64F' \n        u'\\U0001F300-\\U0001F5FF' \n        u'\\U0001F680-\\U0001F6FF' \n        u'\\U0001F1E0-\\U0001F1FF' \n        u'\\U00002702-\\U000027B0'\n        u'\\U000024C2-\\U0001F251'\n        ']+',\n        flags=re.UNICODE)\n    return emoji_patrones.sub(r'', texto)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Expandir las Abreviaturas - OK\n\nabreviaturas = {\n    \"$\" : \" dollar \",\n    \"€\" : \" euro \",\n    \"4ao\" : \"for adults only\",\n    \"a.m\" : \"before midday\",\n    \"a3\" : \"anytime anywhere anyplace\",\n    \"aamof\" : \"as a matter of fact\",\n    \"acct\" : \"account\",\n    \"adih\" : \"another day in hell\",\n    \"afaic\" : \"as far as i am concerned\",\n    \"afaict\" : \"as far as i can tell\",\n    \"afaik\" : \"as far as i know\",\n    \"afair\" : \"as far as i remember\",\n    \"afk\" : \"away from keyboard\",\n    \"app\" : \"application\",\n    \"approx\" : \"approximately\",\n    \"apps\" : \"applications\",\n    \"asap\" : \"as soon as possible\",\n    \"asl\" : \"age, sex, location\",\n    \"atk\" : \"at the keyboard\",\n    \"ave.\" : \"avenue\",\n    \"aymm\" : \"are you my mother\",\n    \"ayor\" : \"at your own risk\", \n    \"b&b\" : \"bed and breakfast\",\n    \"b+b\" : \"bed and breakfast\",\n    \"b.c\" : \"before christ\",\n    \"b2b\" : \"business to business\",\n    \"b2c\" : \"business to customer\",\n    \"b4\" : \"before\",\n    \"b4n\" : \"bye for now\",\n    \"b@u\" : \"back at you\",\n    \"bae\" : \"before anyone else\",\n    \"bak\" : \"back at keyboard\",\n    \"bbbg\" : \"bye bye be good\",\n    \"bbc\" : \"british broadcasting corporation\",\n    \"bbias\" : \"be back in a second\",\n    \"bbl\" : \"be back later\",\n    \"bbs\" : \"be back soon\",\n    \"be4\" : \"before\",\n    \"bfn\" : \"bye for now\",\n    \"blvd\" : \"boulevard\",\n    \"bout\" : \"about\",\n    \"brb\" : \"be right back\",\n    \"bros\" : \"brothers\",\n    \"brt\" : \"be right there\",\n    \"bsaaw\" : \"big smile and a wink\",\n    \"btw\" : \"by the way\",\n    \"bwl\" : \"bursting with laughter\",\n    \"c/o\" : \"care of\",\n    \"cet\" : \"central european time\",\n    \"cf\" : \"compare\",\n    \"cia\" : \"central intelligence agency\",\n    \"csl\" : \"can not stop laughing\",\n    \"cu\" : \"see you\",\n    \"cul8r\" : \"see you later\",\n    \"cv\" : \"curriculum vitae\",\n    \"cwot\" : \"complete waste of time\",\n    \"cya\" : \"see you\",\n    \"cyt\" : \"see you tomorrow\",\n    \"dae\" : \"does anyone else\",\n    \"dbmib\" : \"do not bother me i am busy\",\n    \"diy\" : \"do it yourself\",\n    \"dm\" : \"direct message\",\n    \"dwh\" : \"during work hours\",\n    \"e123\" : \"easy as one two three\",\n    \"eet\" : \"eastern european time\",\n    \"eg\" : \"example\",\n    \"embm\" : \"early morning business meeting\",\n    \"encl\" : \"enclosed\",\n    \"encl.\" : \"enclosed\",\n    \"etc\" : \"and so on\",\n    \"faq\" : \"frequently asked questions\",\n    \"fawc\" : \"for anyone who cares\",\n    \"fb\" : \"facebook\",\n    \"fc\" : \"fingers crossed\",\n    \"fig\" : \"figure\",\n    \"fimh\" : \"forever in my heart\", \n    \"ft.\" : \"feet\",\n    \"ft\" : \"featuring\",\n    \"ftl\" : \"for the loss\",\n    \"ftw\" : \"for the win\",\n    \"fwiw\" : \"for what it is worth\",\n    \"fyi\" : \"for your information\",\n    \"g9\" : \"genius\",\n    \"gahoy\" : \"get a hold of yourself\",\n    \"gal\" : \"get a life\",\n    \"gcse\" : \"general certificate of secondary education\",\n    \"gfn\" : \"gone for now\",\n    \"gg\" : \"good game\",\n    \"gl\" : \"good luck\",\n    \"glhf\" : \"good luck have fun\",\n    \"gmt\" : \"greenwich mean time\",\n    \"gmta\" : \"great minds think alike\",\n    \"gn\" : \"good night\",\n    \"g.o.a.t\" : \"greatest of all time\",\n    \"goat\" : \"greatest of all time\",\n    \"goi\" : \"get over it\",\n    \"gps\" : \"global positioning system\",\n    \"gr8\" : \"great\",\n    \"gratz\" : \"congratulations\",\n    \"gyal\" : \"girl\",\n    \"h&c\" : \"hot and cold\",\n    \"hp\" : \"horsepower\",\n    \"hr\" : \"hour\",\n    \"hrh\" : \"his royal highness\",\n    \"ht\" : \"height\",\n    \"ibrb\" : \"i will be right back\",\n    \"ic\" : \"i see\",\n    \"icq\" : \"i seek you\",\n    \"icymi\" : \"in case you missed it\",\n    \"idc\" : \"i do not care\",\n    \"idgadf\" : \"i do not give a damn fuck\",\n    \"idgaf\" : \"i do not give a fuck\",\n    \"idk\" : \"i do not know\",\n    \"ie\" : \"that is\",\n    \"i.e\" : \"that is\",\n    \"ifyp\" : \"i feel your pain\",\n    \"IG\" : \"instagram\",\n    \"iirc\" : \"if i remember correctly\",\n    \"ilu\" : \"i love you\",\n    \"ily\" : \"i love you\",\n    \"imho\" : \"in my humble opinion\",\n    \"imo\" : \"in my opinion\",\n    \"imu\" : \"i miss you\",\n    \"iow\" : \"in other words\",\n    \"irl\" : \"in real life\",\n    \"j4f\" : \"just for fun\",\n    \"jic\" : \"just in case\",\n    \"jk\" : \"just kidding\",\n    \"jsyk\" : \"just so you know\",\n    \"l8r\" : \"later\",\n    \"lb\" : \"pound\",\n    \"lbs\" : \"pounds\",\n    \"ldr\" : \"long distance relationship\",\n    \"lmao\" : \"laugh my ass off\",\n    \"lmfao\" : \"laugh my fucking ass off\",\n    \"lol\" : \"laughing out loud\",\n    \"ltd\" : \"limited\",\n    \"ltns\" : \"long time no see\",\n    \"m8\" : \"mate\",\n    \"mf\" : \"motherfucker\",\n    \"mfs\" : \"motherfuckers\",\n    \"mfw\" : \"my face when\",\n    \"mofo\" : \"motherfucker\",\n    \"mph\" : \"miles per hour\",\n    \"mr\" : \"mister\",\n    \"mrw\" : \"my reaction when\",\n    \"ms\" : \"miss\",\n    \"mte\" : \"my thoughts exactly\",\n    \"nagi\" : \"not a good idea\",\n    \"nbc\" : \"national broadcasting company\",\n    \"nbd\" : \"not big deal\",\n    \"nfs\" : \"not for sale\",\n    \"ngl\" : \"not going to lie\",\n    \"nhs\" : \"national health service\",\n    \"nrn\" : \"no reply necessary\",\n    \"nsfl\" : \"not safe for life\",\n    \"nsfw\" : \"not safe for work\",\n    \"nth\" : \"nice to have\",\n    \"nvr\" : \"never\",\n    \"nyc\" : \"new york city\",\n    \"oc\" : \"original content\",\n    \"og\" : \"original\",\n    \"ohp\" : \"overhead projector\",\n    \"oic\" : \"oh i see\",\n    \"omdb\" : \"over my dead body\",\n    \"omg\" : \"oh my god\",\n    \"omw\" : \"on my way\",\n    \"p.a\" : \"per annum\",\n    \"p.m\" : \"after midday\",\n    \"pm\" : \"prime minister\",\n    \"poc\" : \"people of color\",\n    \"pov\" : \"point of view\",\n    \"pp\" : \"pages\",\n    \"ppl\" : \"people\",\n    \"prw\" : \"parents are watching\",\n    \"ps\" : \"postscript\",\n    \"pt\" : \"point\",\n    \"ptb\" : \"please text back\",\n    \"pto\" : \"please turn over\",\n    \"qpsa\" : \"what happens\", \n    \"ratchet\" : \"rude\",\n    \"rbtl\" : \"read between the lines\",\n    \"rlrt\" : \"real life retweet\", \n    \"rofl\" : \"rolling on the floor laughing\",\n    \"roflol\" : \"rolling on the floor laughing out loud\",\n    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n    \"rt\" : \"retweet\",\n    \"ruok\" : \"are you ok\",\n    \"sfw\" : \"safe for work\",\n    \"sk8\" : \"skate\",\n    \"smh\" : \"shake my head\",\n    \"sq\" : \"square\",\n    \"srsly\" : \"seriously\", \n    \"ssdd\" : \"same stuff different day\",\n    \"tbh\" : \"to be honest\",\n    \"tbs\" : \"tablespooful\",\n    \"tbsp\" : \"tablespooful\",\n    \"tfw\" : \"that feeling when\",\n    \"thks\" : \"thank you\",\n    \"tho\" : \"though\",\n    \"thx\" : \"thank you\",\n    \"tia\" : \"thanks in advance\",\n    \"til\" : \"today i learned\",\n    \"tl;dr\" : \"too long i did not read\",\n    \"tldr\" : \"too long i did not read\",\n    \"tmb\" : \"tweet me back\",\n    \"tntl\" : \"trying not to laugh\",\n    \"ttyl\" : \"talk to you later\",\n    \"u\" : \"you\",\n    \"u2\" : \"you too\",\n    \"u4e\" : \"yours for ever\",\n    \"utc\" : \"coordinated universal time\",\n    \"w/\" : \"with\",\n    \"w/o\" : \"without\",\n    \"w8\" : \"wait\",\n    \"wassup\" : \"what is up\",\n    \"wb\" : \"welcome back\",\n    \"wtf\" : \"what the fuck\",\n    \"wtg\" : \"way to go\",\n    \"wtpa\" : \"where the party at\",\n    \"wuf\" : \"where are you from\",\n    \"wuzup\" : \"what is up\",\n    \"wywh\" : \"wish you were here\",\n    \"yd\" : \"yard\",\n    \"ygtr\" : \"you got that right\",\n    \"ynk\" : \"you never know\",\n    \"zzz\" : \"sleeping bored and tired\"\n}\n\ndef expandir_abreviatura(texto,mapping = abreviaturas):\n    texto = ' '.join([mapping[t] if t in mapping else t for t in texto.split(\" \")])\n    return texto","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Expandir las Contracciones - OK\n\ncontracciones_mapeo = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \n                       \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \n                       \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \n                       \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \n                       \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\n                       \"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \n                       \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \n                       \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\",\n                       \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \n                       \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \n                       \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \n                       \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \n                       \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \n                       \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \n                       \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \n                       \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \n                       \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\n\ndef expandir_contraccion(texto,mapping = contracciones_mapeo):\n    specials =[\"’\", \"‘\", \"´\", \"`\"]\n    for s in specials:\n        texto = texto.replace(s,\"'\")\n    \n    texto = ' '.join([mapping[t] if t in mapping else t for t in texto.split(\" \")])\n    return texto","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remover los tags HTML - OK\n\ndef remover_tag_html(texto):\n    html = re.compile(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n    return re.sub(html, '', texto)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Remover acentos - OK\n\ndef remover_acento(texto):\n    texto = unicodedata.normalize('NFKD', texto).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n    return texto","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Remover Puntos - OK\n\ndef remover_punto(texto):\n    import string\n    texto = ''.join([c for c in texto if c not in string.punctuation])\n    return texto","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Remover Numeros - OK\n\ndef remover_numero(texto):\n    texto = ''.join([i for i in texto if not i.isdigit()])\n    return texto","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Remover espacios en Blanco (extras/tabs) - OK\n\ndef remover_espacio_extra(texto):\n    import re\n    pattern = r'^\\s*|\\s\\s*'\n    return re.sub(pattern, ' ', texto).strip()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Remueve todo lo que se sea del alfabeto - OK\n\ndef remover_no_alfabeto(texto):\n    return ' '.join([i for i in texto.split() if i.isalpha() == True])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Remover Stop-Word - OK\n\ndef remover_stop_word(texto):\n    return \" \".join ([word for word in word_tokenize(texto) if not word in stopwords.words('english')])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lematizar - OK\n\ndef lematizar(texto):\n    lemma = WordNetLemmatizer()\n    return \" \".join([lemma.lemmatize(word) for word in word_tokenize(texto)])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Una vez definidias todas las funciones que se utilizaran para realizar la limpieza se define una funcion de ejecuta todas estas a una columna del dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocesar_df(df, col_name, clean_col_name):\n    df[clean_col_name] = df[col_name].apply(lambda x: minuscula (x))\\\n                                    .apply(lambda x: remover_url(x))\\\n                                    .apply(lambda x: remover_usuario(x))\\\n                                    .apply(lambda x: remover_emoji(x))\\\n                                    .apply(lambda x: expandir_abreviatura(x))\\\n                                    .apply(lambda x: expandir_contraccion(x))\\\n                                    .apply(lambda x: remover_tag_html(x))\\\n                                    .apply(lambda x: remover_acento(x))\\\n                                    .apply(lambda x: remover_punto(x))\\\n                                    .apply(lambda x: remover_numero(x))\\\n                                    .apply(lambda x: remover_espacio_extra(x))\\\n                                    .apply(lambda x: remover_no_alfabeto(x))\\\n                                    .apply(lambda x: remover_stop_word(x))\\\n                                    .apply(lambda x: lematizar(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.1 Preprocesamiento del Dataset 'train' "},{"metadata":{},"cell_type":"markdown","source":"Realizo el preprocesamiento de la comlumna **'text'** del dataset **'train'**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Llamo a la funcion que realiza el preprocesamiento\n\npreprocesar_df(df_train,'text', 'texto_preprocesado')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Observo las 5 primeras filas\n\ndf_train.head(30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dimensiones del dataset\n\ndf_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Defino los vectores que se utilizaran para el entrenamiento de los modelos"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train_original = df_train['text']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train_preprocesado = df_train['texto_preprocesado']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = df_train['target']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.2 Preprocesamiento del Dataset 'test' "},{"metadata":{},"cell_type":"markdown","source":"Realizo el preprocesamiento de la comlumna **'text'** del dataset **'test'**"},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocesar_df(df_test,'text', 'texto_preprocesado')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Observo las 5 primeras filas\n\ndf_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Defino los vectores que se utilizaran para el entrenamiento de los modelos"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test_original = df_test['text']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test_preprocesado = df_test['texto_preprocesado']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Hiperparametros de Vectorizadores\n\nSe definen los Hiperparametros que se utilizaran en GridSearchCV para realizar el tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Parametros para el vectorizador BOW - CountVectorizer()\n\nparametros_bow = {'vectorizador__strip_accents':'unicode',\n                  'vectorizador__ngram_range': [(1,1),(2,2),(1,2)],\n                  'vectorizador__max_df':[10,20,30],\n                  'vectorizador__binary':[False,True]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Parametros para el vectorizador TFIDF - TfidfVectorizer()\n\nparametros_tfidf = {'vectorizador__strip_accents':'unicode',\n                  'vectorizador__ngram_range': [(1,1),(2,2),(1,2)],\n                  'vectorizador__max_df':[10,20,30],\n                  'vectorizador__binary':[False,True],\n                  'vectorizador__use_idf':[True,False]}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Tuning de Parametros (GridSearch) y Resultados"},{"metadata":{},"cell_type":"markdown","source":"## 5.1 Ridge Regresion"},{"metadata":{},"cell_type":"markdown","source":"### 5.1.1 BOW"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Pipeline\n\npipeline_bow_RR = Pipeline([('vectorizador', CountVectorizer()),('clf', RidgeClassifier())])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Datos sin Preprocesar**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dataset sin preprocesamiento\n\n#Hiperparametros del vectorizador y del clasificador\nhiperparametros_1 = {'vectorizador__strip_accents':['unicode'],\n                     'vectorizador__ngram_range': [(1,1),(2,2),(1,2)],\n                     'vectorizador__min_df':[10,20,30],\n                     'vectorizador__binary':[False,True],\n                     'clf__alpha': [0.001,0.01,0.1,0.5,1,1.5,2,2.5,3,3.5,4,4.5,5],\n                     'clf__normalize':[False,True],\n                     'clf__random_state':[42]}\n\n#Tuning de Hiperparametros\nclf_RR_bow_original = GridSearchCV(pipeline_bow_RR, hiperparametros_1,cv=5, n_jobs=-1,verbose=2)\nclf_RR_bow_original.fit(x_train_original, y_train)\n\n\n#Muestro los resultados\n\nprint(\"Mejor Score: \", clf_RR_bow_original.best_score_)\n\nprint(\"Mejores Parametros: \", clf_RR_bow_original.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Generacion del SUBMIT"},{"metadata":{},"cell_type":"markdown","source":"Leo el archivo .csv modelo que tenemos se utilizar para realizar el submit a Kaggle"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creo una nueva columna con los valores que predice el modelo"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission['target'] = clf_RR_bow_original.predict(x_test_original)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Guardo el archivo .csv para realizar el submit a Kaggle"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv(\"original_bow_hiper3_ridge_regression.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Datos Preprocesados**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dataset Preprocesado\n\n#Hiperparametros del vectorizador y del clasificador\nhiperparametros_2 = {'vectorizador__strip_accents':['unicode'],\n                     'vectorizador__ngram_range': [(1,1),(2,2),(1,2)],\n                     'vectorizador__min_df':[10,20,30],\n                     'vectorizador__binary':[False,True],\n                     'clf__alpha': [0.001,0.01,0.1,0.5,1,1.5,2,2.5,3,3.5,4,4.5,5],\n                     'clf__normalize':[False,True],\n                     'clf__random_state':[42]}\n\n#Tuning de Hiperparametros\nclf_RR_bow_preprocesado = GridSearchCV(pipeline_bow_RR, hiperparametros_2,cv=5, n_jobs=-1,verbose=2)\nclf_RR_bow_preprocesado.fit(x_train_preprocesado, y_train)\n\n\n#Muestro los resultados\n\nprint(\"Mejor Score: \", clf_RR_bow_preprocesado.best_score_)\n\nprint(\"Mejores Parametros: \", clf_RR_bow_preprocesado.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Generacion del SUBMIT"},{"metadata":{},"cell_type":"markdown","source":"Leo el archivo .csv modelo que tenemos se utilizar para realizar el submit a Kaggle"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creo una nueva columna con los valores que predice el modelo"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission['target'] = clf_RR_bow_preprocesado.predict(x_test_preprocesado)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Guardo el archivo .csv para realizar el submit a Kaggle"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv(\"preprocesado_bow_hiper3_ridge_regression.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.1.2 TFIDF "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Pipeline\n\npipeline_tfidf_RR = Pipeline([('vectorizador', TfidfVectorizer()),('clf', RidgeClassifier())])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Datos sin Preprocesar**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dataset sin preprocesamiento\n\n#Hiperparametros del vectorizador y del clasificador\nhiperparametros_3 = {'vectorizador__strip_accents':['unicode'],\n                     'vectorizador__ngram_range': [(1,1),(2,2),(1,2)],\n                     'vectorizador__max_df':[10,20,30],\n                     'vectorizador__binary':[False,True],\n                     'vectorizador__use_idf':[True,False],\n                     'clf__alpha': [0.001,0.01,0.1,0.5,1,1.5,2,2.5,3,3.5,4,4.5,5],\n                     'clf__normalize':[False,True],\n                     'clf__random_state':[42]}\n\n#Tuning de Hiperparametros\nclf_RR_tfidf_original = GridSearchCV(pipeline_tfidf_RR, hiperparametros_3,cv=5, n_jobs=-1,verbose=2)\nclf_RR_tfidf_original.fit(x_train_original, y_train)\n\n\n#Muestro los resultados\n\nprint(\"Mejor Score: \", clf_RR_tfidf_original.best_score_)\n\nprint(\"Mejores Parametros: \", clf_RR_tfidf_original.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Generacion del SUBMIT"},{"metadata":{},"cell_type":"markdown","source":"Leo el archivo .csv modelo que tenemos se utilizar para realizar el submit a Kaggle"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creo una nueva columna con los valores que predice el modelo"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission['target'] = clf_RR_tfidf_original.predict(x_test_original)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Guardo el archivo .csv para realizar el submit a Kaggle"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv(\"original_tfidf_hiper3_ridge_regression.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Datos Preprocesados**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dataset Preprocesado\n\n#Hiperparametros del vectorizador y del clasificador\nhiperparametros_4 = {'vectorizador__strip_accents':['unicode'],\n                     'vectorizador__ngram_range': [(1,1),(2,2),(1,2)],\n                     'vectorizador__max_df':[10,20,30],\n                     'vectorizador__binary':[False,True],\n                     'vectorizador__use_idf':[True,False],\n                     'clf__alpha': [0.001,0.01,0.1,0.5,1,1.5,2,2.5,3,3.5,4,4.5,5],\n                     'clf__normalize':[False,True],\n                     'clf__random_state':[42]}\n\n#Tuning de Hiperparametros\nclf_RR_tfidf_preprocesado = GridSearchCV(pipeline_tfidf_RR, hiperparametros_4,cv=5, n_jobs=-1,verbose=2)\nclf_RR_tfidf_preprocesado.fit(x_train_preprocesado, y_train)\n\n\n#Muestro los resultados\n\nprint(\"Mejor Score: \", clf_RR_tfidf_preprocesado.best_score_)\n\nprint(\"Mejores Parametros: \", clf_RR_tfidf_preprocesado.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Generacion del SUBMIT"},{"metadata":{},"cell_type":"markdown","source":"Leo el archivo .csv modelo que tenemos se utilizar para realizar el submit a Kaggle"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creo una nueva columna con los valores que predice el modelo"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission['target'] = clf_RR_tfidf_preprocesado.predict(x_test_preprocesado)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Guardo el archivo .csv para realizar el submit a Kaggle"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv(\"preprocesado_tfidf_hiper3_ridge_regression.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}