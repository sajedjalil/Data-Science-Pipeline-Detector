{"cells":[{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# Suppress warnings \nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport numpy as np\nimport nltk\nfrom nltk.corpus import stopwords\nimport re\nimport string\n\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer,HashingVectorizer\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div align='center'><font size=\"6\" color=\"#F39C12\">Getting started with NLP-Feature Vectors</font></div>\n\n<hr>\n\n\n<p style='text-align:justify'><b>Key Objectives:</b>This notebook comes as a second part to the <b>[Getting started with NLP Notebooks](https://www.kaggle.com/parulpandey/getting-started-with-nlp-a-general-intro)</b>.In this notebook we shall study the various ways of vectorizing text data.Vectorization converts text data into feature vectors.</p>\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Importing the dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/nlp-getting-started/train.csv')\ntest = pd.read_csv('../input/nlp-getting-started/test.csv')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Text Vectorization Methods\n\nThere are many methods to vctorize text, but in this notebook I shall discuss few of them:","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 1.Countvectorizer\n\nThe [Scikit-Learn's CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) provides a simple way to both tokenize a collection of text documents and build a vocabulary of known words, but also to encode new documents using that vocabulary.\n\n![](https://imgur.com/xxErhnB.png)\n\nWe take a dataset and convert it into a corpus. Then we create a vocabulary of all the unique words in the corpus. Using this vocabulary, we can then  create a feature vector of the count of the words. Let's see this through a simple example. Let's say we have a corpus containing two sentences as follows","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences = ['The weather is sunny', 'The weather is partly sunny and partly cloudy.']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\nvectorizer = CountVectorizer()\nvectorizer.fit(sentences)\nvectorizer.vocabulary_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting all the sentences to arrays\nvectorizer.transform(sentences).toarray()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By default, a scikit learn Count vectorizer can perform the following opertions over a text corpus:\n\n- Encoding via utf-8\n- converts text to lowercase\n- Tokenizes text using word level tokenization\n\nCountVectorizer has a number of parameters. Let's look at some of them :","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 1.1 Stopword \n\nSometimes, some extremely common words which would appear to be of little value in helping select documents matching a user need are excluded from the vocabulary entirely. These words are called stop words. If `stop_word` parameter is specified with a list of stopwords, they will be removed from the vocabulary. Here I'll use the stopwords from NLTK but we can also specify custom stopwords too.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"stopwords = stopwords.words('english')\n\ncount_vectorizer = CountVectorizer(stop_words = stopwords)\ncount_vectorizer.fit(train['text'])\n\ntrain_vectors = count_vectorizer.transform(train['text'])\ntest_vectors = count_vectorizer.transform(test['text'])\n\ntrain_vectors.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"See how the columns have reduced from 21637 to 21498. This is because some of the stopwords were removed.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 1.2 MIN_DF and MAX_DF parameter\n\n`MIN_DF` lets you ignore those terms that appear rarely in a corpus. In other words, if `MIN_df`is 2, it  means that a word has to occur at least two documents to be considered useful.\n\n`MAX_DF` on the other hand, ignores terms that have a document frequency strictly higher than the given threshold.These will be words which appear a lot of documents.\n\nThis means we can eliminate those words that are either rare or appear too frequently in a corpus. \n\nWhen mentioned in absolute values i.e 1,2, etc, the value means if the word appears in 1 or 2 documents. However, when given in float, eg 30%, it means it appears in 30% of the documents.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"count_vectorizer = CountVectorizer(stop_words = stopwords, min_df=2 ,max_df=0.8)\ncount_vectorizer.fit(train['text'])\n\ntrain_vectors = count_vectorizer.transform(train['text'])\ntest_vectors = count_vectorizer.transform(test['text'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.3.Custom Preprocesser\n\nWe can also preprocess the text by passing it as an argument to countvectorizer. The following options are avialable:\n\n- strip_accents - This removes any accents from the text during the preprocessing step.\n- lowercase -  which is default set as true but can be set to False if lowercasing isnot desired\n- preprocessor - we can create our custom preprocessor and set this argument to that.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a custom preprocessor that lowercases, removes special characters, removes hyperlinks and punctuation\n\ndef custom_preprocessor(text):\n    '''\n    Make text lowercase, remove text in square brackets,remove links,remove special characters\n    and remove words containing numbers.\n    '''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub(\"\\\\W\",\" \",text) # remove special chars\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    \n    return text\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count_vectorizer = CountVectorizer(list(train['text']),preprocessor=custom_preprocessor)\n\ntrain_vectors = count_vectorizer.fit_transform(list(train['text']))\ntest_vectors = count_vectorizer.transform(list(test['text']))\n ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.4. N-Grams and analyzer parameter\n\nThis paramneter specifies the upper and lower limit for the range of words/characters to be extracted from text. The following n-grams range stand for:\n(1,1) - unigrams  eg 'United'\n(1,2) - unigrams and bigrams eg - 'United', 'United States'\n(2, 2)- only bigrams etc eg 'United States)\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# World level unigrams and bigrams\n\ncount_vectorizer = CountVectorizer(list(train['text']),preprocessor=custom_preprocessor,ngram_range=(1,2))\n\ntrain_vectors = count_vectorizer.fit_transform(list(train['text']))\ntest_vectors = count_vectorizer.transform(list(test['text']))\n\nlist(count_vectorizer.vocabulary_)[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# character level bigrams\n\n\ncount_vectorizer = CountVectorizer(list(train['text']),preprocessor=custom_preprocessor,ngram_range=(2,2),\n                                  analyzer='char_wb')\n\ntrain_vectors = count_vectorizer.fit_transform(list(train['text']))\ntest_vectors = count_vectorizer.transform(list(test['text']))\n\nprint(list(count_vectorizer.vocabulary_)[:20])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating a Baseline Model using Countvectorizer","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\ncount_vectorizer = CountVectorizer(token_pattern=r'\\w{1,}',\n                   ngram_range=(1, 2), stop_words = stopwords,preprocessor=custom_preprocessor)\ncount_vectorizer .fit(train['text'])\n\ntrain_vectors = count_vectorizer.transform(train['text'])\ntest_vectors = count_vectorizer.transform(test['text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression(C=1.0)\nscores = model_selection.cross_val_score(clf, train_vectors, train[\"target\"], cv=5, scoring=\"f1\")\nscores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting a simple Logistic Regression on Counts\nclf.fit(train_vectors, train[\"target\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Submission\nsample_submission = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")\nsample_submission[\"target\"] = clf.predict(test_vectors)\nsample_submission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This gets me a score of 0.80777 on the Public LB, which isn't bad with simple Logistic Regression model.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 2.TF-IDF Vectorizer\n\n![](https://imgur.com/J5lS7kX.png)\n\nIn the CountVectorizer, we use the counts of the words, in TFIDF we take the relative importance of that term in the entire corpus. TFIDF is composed of two words: TF and IDF. \n**TF** stands for the normalized  term frequency. Term Frequency is a scoring of the frequency of the word in the current document.`TF = (Number of times term t appears in a document)/(Number of terms in the document)`\n\n**IDF** or Inverse Document Frequency: is a scoring of how rare the word is across documents. `IDF = 1+log(N/n)`, where N is the number of documents and n is the number of documents a term t has appeared in.TF-IDF weight is often used in information retrieval and text mining. This weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus\n\n*Because the ratio of the id f log function is greater or equal to 1, the TF–IDF score is\nalways greater than or equal to zero. We interpret the score to mean that the closer the\nTF–IDF score of a term is to 1, the more informative that term is to that document.\nThe closer the score is to zero, the less informative that term is.*\nfrom : [Applied Text Analysis with Python](https://www.amazon.in/Applied-Text-Analysis-Python-Language-Aware/dp/9352137434/ref=asc_df_9352137434/?tag=googleshopdes-21&linkCode=df0&hvadid=396988721232&hvpos=1o1&hvnetw=g&hvrand=11704105753328600061&hvpone=&hvptwo=&hvqmt=&hvdev=c&hvdvcmdl=&hvlocint=&hvlocphy=9062140&hvtargid=pla-838697427991&psc=1&ext_vrnc=hi)\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"TFIDF can be generated at word, character or even N gram level. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# word level\ntfidf = TfidfVectorizer(analyzer='word',token_pattern=r'\\w{1,}',max_features=5000)\ntrain_tfidf = tfidf.fit_transform(train['text'])\ntest_tfidf = tfidf.transform(test[\"text\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ngram level\ntfidf = TfidfVectorizer(analyzer='word',ngram_range=(2,3),token_pattern=r'\\w{1,}',max_features=5000)\ntrain_tfidf = tfidf.fit_transform(train['text'])\ntest_tfidf = tfidf.transform(test[\"text\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# characters level\ntfidf = TfidfVectorizer(analyzer='char',ngram_range=(2,3),token_pattern=r'\\w{1,}',max_features=5000)\ntrain_tfidf = tfidf.fit_transform(train['text'])\ntest_tfidf = tfidf.transform(test[\"text\"])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating a Baseline Model using TFIDF","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf_vectorizer = TfidfVectorizer( min_df=3,  max_features=None,analyzer='word',token_pattern=r'\\w{1,}',\n            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n            stop_words = stopwords)\n\ntrain_tfidf = tfidf.fit_transform(train['text'])\ntest_tfidf = tfidf.transform(test[\"text\"])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression(C=1.0)\nscores = model_selection.cross_val_score(clf, train_tfidf, train[\"target\"], cv=5, scoring=\"f1\")\nscores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting a simple Logistic Regression on TFIDF\nclf.fit(train_tfidf, train[\"target\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.Hashing Vectorizer\n\n![](https://imgur.com/e3GRaHn.png)\n\nHashing Vectorizer is yet another technique for vectorizing a collection of text documents. So why do we need yet another technique when we already have so many already. Well, the reason is that,both CountVectorizer and TF-IDF result in storing the entire vocabulary dictionary in memory i.e the number of unique tokens.This could be challenging in scenarios when the token vocabulary becomes very large, to the order of millions.\n\nHashing Vectorizer is based on the Hash Function which are  are fundamental to computer science.A Hash Function fundamentally maps data of arbitrary sizes to data of a fixed size.Here is a great article which explains this concept very well and is a great read: [Introducing One of the Best Hacks in Machine Learning: the Hashing Trick](https://medium.com/value-stream-design/introducing-one-of-the-best-hacks-in-machine-learning-the-hashing-trick-bf6a9c8af18f). Let's see how can we implement it in scikit learn:\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"hash_vectorizer = HashingVectorizer(n_features=10000,norm=None,alternate_sign=False)\nhash_vectorizer.fit(train['text'])\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_vectors = hash_vectorizer.transform(train['text'])\ntest_vectors = hash_vectorizer.transform(test['text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_vectors[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression(C=1.0)\nscores = model_selection.cross_val_score(clf, train_vectors, train[\"target\"], cv=5, scoring=\"f1\")\nscores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting a simple Logistic Regression on TFIDF\nclf.fit(train_vectors, train[\"target\"])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}