{"cells":[{"metadata":{},"cell_type":"markdown","source":"Inspiration from [this notebook](https://www.kaggle.com/shahules/basic-eda-cleaning-and-glove) "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom collections import defaultdict\nfrom collections import  Counter\nplt.style.use('ggplot')\nstop=set(stopwords.words('english'))\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,Dropout\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"input_dir = \"/kaggle/input/nlp-getting-started\"\ntrain = pd.read_csv(os.path.join(input_dir, 'train.csv'))\ntest = pd.read_csv(os.path.join(input_dir, 'test.csv'))\ndf=pd.concat([train,test])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Collect some key metrics:\n\nLets compute the following metrics for the training dataset, to get a global view of the dataset\n\n* Number of samples: Total number of examples \n* Number of samples per class: Number of samples per class (rean disaster /non real disaster).\n* Number of words per sample: Median or mean number of words in one sample.\n* Frequency distribution of words: Distribution showing the frequency (number of occurrences) of each word in the dataset.\n* Distribution of sample length: Distribution showing the number of words per sample in the dataset.\n\n((( source https://developers.google.com/machine-learning/guides/text-classification/step-2)))"},{"metadata":{"trusted":true},"cell_type":"code","source":"key_metrics= {'samples' : len(train),\n             'samples_per_class' : train['target'].value_counts().median(),\n             'median_of_samples_lengths': np.median(train['text'].str.split().map(lambda x: len(x))),\n             }\nkey_metrics = pd.DataFrame.from_dict(key_metrics, orient='index').reset_index()\nkey_metrics.columns = ['metric', 'value']\nkey_metrics","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some plots are always more comprehensive than numbers, so lets visulize and discover some useful insights\n### Target class distribution with a bar plot"},{"metadata":{"trusted":true},"cell_type":"code","source":"green = '#52BE80'\nred = '#EC7063'\nsns.countplot(train['target'], palette=[green, red])\n# PS : here: https://htmlcolorcodes.com/fr/tableau-de-couleur/ => to a cheerful color table where you can get an infinity of color codes  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"blue = \"#5DADE2\"\ndef get_sentences_len_histogram(fig_size = (10,6), _class=None, color=blue):\n    \"\"\"make histogram of lengths of the tweets\n    * _class : consider all the dataset\n    - _class=0 only non disaster tweets\n    - _class=1 only disaster tweets\"\"\"\n    f, ax = plt.subplots(figsize=fig_size)\n    if str(_class)=='None':\n        tweets_len=train['text'].str.len()\n        ax.set_title('tweets length')\n        ax.hist(tweets_len,color=color)\n    else: \n        tweets_len=train[train['target']==_class]['text'].str.len()\n        ax.set_title(f'{_class} disaster tweets length ')\n        ax.hist(tweets_len,color=color)\n\n    return ax\n\ndef get_words_count_histogram(fig_size = (10,6), _class=None, color=blue):\n    \"\"\"make histogram of lengths of the tweets\n    * _class : consider all the dataset\n    - _class=0 only non disaster tweets\n    - _class=1 only disaster tweets\"\"\"\n    f, ax = plt.subplots(figsize=fig_size)\n    if str(_class)=='None':\n        tweets_len=train['text'].str.split().map(lambda x: len(x))\n        ax.set_title('words counts by tweet')\n        ax.hist(tweets_len,color=color)\n    else: \n        tweets_len=train[train['target']==_class]['text'].str.split().map(lambda x: len(x))\n        ax.set_title(f'words counts by  \"{_class}\" disaster tweets ')\n        ax.hist(tweets_len,color=color)\n\n    return ax","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Histogram of text sentences lengths: \n#### a. in all the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"get_sentences_len_histogram()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# two subplots\nax1 = get_sentences_len_histogram(_class=0, color=green)\nax1 = get_sentences_len_histogram(_class=1, color=red)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Distribution of samples length is mainly concentrated between 120 to 140 characters"},{"metadata":{},"cell_type":"markdown","source":"### Histogram of number of words for each tweet: \n#### a. in all the dataset\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"get_words_count_histogram()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# two subplots\nax1 = get_words_count_histogram(_class=0, color=green)\nax1 = get_words_count_histogram(_class=1, color=red)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Text cleaning:\n\nwe can apply some basic data cleaning that are recurrent in tweeters such as removing punctuation, html tags urls and emojis, spelling correction,.."},{"metadata":{"trusted":true},"cell_type":"code","source":"from string import digits \n\ndef tweets_cleaning(x, remove_emojis=True, remove_stop_words=True):\n    \"\"\"Apply function to a \"\"\"\n    x = x.lower().strip()\n    # romove urls\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    x = url.sub(r'',x)\n    # remove html tags\n    html = re.compile(r'<.*?>')\n    x = html.sub(r'',x)\n    # remove punctuation\n    operator = str.maketrans('','',string.punctuation) #????\n    x = x.translate(operator)\n    # remove digits\n    remove_digits = str.maketrans('', '', digits) \n    x = ' '.join([i.translate(remove_digits) for i in x.split()])\n    \n    if remove_emojis:\n        x = x.encode('ascii', 'ignore').decode('utf8').strip()\n    if remove_stop_words:\n        x = ' '.join([word for word in x.split(' ') if word not in stop])\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['cleaned_tweets'] = df['text'].apply(tweets_cleaning)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = df[~df['target'].isna()]\ntrain['target'] = train['target'].astype(int)\ntest = df[df['target'].isna()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### train validation test split:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets keep a part a validation subset that will be used by our model\nX_train, X_val, y_train, y_val = train_test_split(train, train['target'], test_size=0.2, random_state=42)\nprint('Shape of train',X_train.shape)\nprint(\"Shape of Validation \",X_val.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Text pre-processing\n\n1. TokenizationÂ \nIt consists in dividing the texts into words or smaller sub-texts, allowing us to determine the \"vocabulary\" of the dataset (set of unique tokens present in the data). Usually we use word-level representation. For our exemple we will use NLTK Tokenizer()\n2. Word indexing:\nConstruct a vocablary_index mapper based on word frequency: the index would be inversely proportional to the word occurrence frequency in the overall dataset. the most frequent world would have index=1.. And every single word would get a unique\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.python.keras.preprocessing import sequence\nfrom tensorflow.python.keras.preprocessing import text\n\n# Vectorization parameters\n# Limit on the number of features. We use the top 20K features.\nTOP_K = 20000\n\n# Limit on the length of text sequences. \n# Sequences longer than this will be truncated.\n# and less than it will be padded\nMAX_SEQUENCE_LENGTH = 50\n\nclass CustomTokenizer:\n    def __init__(self, train_texts):\n        self.train_texts = train_texts\n        self.tokenizer = Tokenizer(num_words=TOP_K)\n        \n#    @property\n#    def max_len(self):\n        # Get max sequence length.\n#        max_length = len(max(self.train_texts , key=len))\n#        return min(max_length, MAX_SEQUENCE_LENGTH)\n        \n    def train_tokenize(self):\n        # Get max sequence length.\n        max_length = len(max(self.train_texts , key=len))\n        self.max_length = min(max_length, MAX_SEQUENCE_LENGTH)\n    \n        # Create vocabulary with training texts.\n        self.tokenizer.fit_on_texts(self.train_texts)\n        \n    def vectorize_input(self, tweets):\n        # Vectorize training and validation texts.\n        \n        tweets = self.tokenizer.texts_to_sequences(tweets)\n        # Fix sequence length to max value. Sequences shorter than the length are\n        # padded in the beginning and sequences longer are truncated\n        # at the beginning.\n        tweets = sequence.pad_sequences(tweets, maxlen=self.max_length, truncating='post',padding='post')\n        return tweets\n    \ntokenizer = CustomTokenizer(train_texts = X_train['cleaned_tweets'])\n# fit o the train\ntokenizer.train_tokenize()\ntokenized_train = tokenizer.vectorize_input(X_train['cleaned_tweets'])\ntokenized_val = tokenizer.vectorize_input(X_val['cleaned_tweets'])\ntokenized_test = tokenizer.vectorize_input(test['cleaned_tweets'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Construct an embedding MatrixÂ ðŸ§±\nFirst of we will download Glove pre-trained embedding from the official site:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tqdm\nimport requests\nimport zipfile\nURL = \"http://nlp.stanford.edu/data/glove.42B.300d.zip\"\n\n\n\ndef fetch_data(url=URL, target_file='glove.zip', delete_zip=False):\n    #if the dataset already exists exit\n    if os.path.isfile(target_file):\n        print(\"datasets already downloded :) \")\n        return\n\n    #download (large) zip file\n    #for large https request on stream mode to avoid out of memory issues\n    #see : http://masnun.com/2016/09/18/python-using-the-requests-module-to-download-large-files-efficiently.html\n    print(\"**************************\")\n    print(\"  Downloading zip file\")\n    print(\"  >_<  Please wait >_< \")\n    print(\"**************************\")\n    response = requests.get(url, stream=True)\n    #read chunk by chunk\n    handle = open(target_file, \"wb\")\n    for chunk in tqdm.tqdm(response.iter_content(chunk_size=512)):\n        if chunk:  \n            handle.write(chunk)\n    handle.close()  \n    print(\"  Download completed ;) :\") \n    #extract zip_file\n    zf = zipfile.ZipFile(target_file)\n    print(\"1. Extracting {} file\".format(target_file))\n    zf.extractall()\n    if delete_zip:\n        print(\"2. Deleting {} file\".format(dataset_name+\".zip\"))\n        os.remove(path=zip_file)\n        \nfetch_data()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then we will create an embedding matrix we will map each word index to its corresponding embedding vector:"},{"metadata":{"trusted":true},"cell_type":"code","source":"glove_file = \"glove.42B.300d.txt\"\n\n\nEMBEDDING_VECTOR_LENGTH = 50 # <=200\ndef construct_embedding_matrix(glove_file, word_index):\n    embedding_dict = {}\n    with open(glove_file,'r') as f:\n        for line in f:\n            values=line.split()\n            # get the word\n            word=values[0]\n            if word in word_index.keys():\n                # get the vector\n                vector = np.asarray(values[1:], 'float32')\n                embedding_dict[word] = vector\n    ###  oov words (out of vacabulary words) will be mapped to 0 vectors\n\n    num_words=len(word_index)+1\n    #initialize it to 0\n    embedding_matrix=np.zeros((num_words, EMBEDDING_VECTOR_LENGTH))\n\n    for word,i in tqdm.tqdm(word_index.items()):\n        if i < num_words:\n            vect=embedding_dict.get(word, [])\n            if len(vect)>0:\n                embedding_matrix[i] = vect[:EMBEDDING_VECTOR_LENGTH]\n    return embedding_matrix\n\nembedding_matrix =  construct_embedding_matrix(glove_file, tokenizer.tokenizer.word_index)\nprint(embedding_matrix.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In addition to the accuracy, we will add the precision, recall and F1-score as backend metrics to binary Keras Classifier model: we have to calculate them manually, because these metrics are not supported by keras since 2.0 version."},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model\n\nfrom keras import backend as K\n\ndef recall_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives / (possible_positives + K.epsilon())\n    return recall\n\ndef precision_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    return precision\n\ndef f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)/(precision+recall+K.epsilon()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets create the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"model=Sequential()\noptimzer=Adam(clipvalue=0.5)\n\nembedding=Embedding(len(tokenizer.tokenizer.word_index)+1, EMBEDDING_VECTOR_LENGTH, embeddings_initializer=Constant(embedding_matrix),\n                   input_length=MAX_SEQUENCE_LENGTH, trainable=False)\n\nmodel.add(embedding)\nmodel.add(Dropout(0.2))\n#model.add(Dense(30, activation='relu'))#, kernel_constraint=maxnorm(3)))\nmodel.add(LSTM(64, dropout=0.2, recurrent_dropout=0.5))\nmodel.add(Dense(1, activation='sigmoid'))\n# compile the model\nmodel.compile(optimizer=optimzer, loss='binary_crossentropy', metrics=['acc',f1_m,precision_m, recall_m])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### fit the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"history=model.fit(tokenized_train,y_train, \n                  batch_size=32, epochs=20, \n                  validation_data=(tokenized_val,y_val), \n                  verbose=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n### Model evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nloss, accuracy, f1_score, precision, recall = model.evaluate(tokenized_val, y_val, verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'F1 score : {\"%.3f\"%f1_score}')\nprint(f'precision : {\"%.3f\"%precision}')\nprint(f'Recall : {\"%.3f\"%recall}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Make new predictions:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# generate scores\ntest['scores'] = model.predict(tokenized_test)\n# generate deisions\ntest['predction'] = np.round(test['scores']).astype(int)\ntest = test.drop('target', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}