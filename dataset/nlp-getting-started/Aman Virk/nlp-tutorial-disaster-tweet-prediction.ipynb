{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-05T10:57:47.804728Z","iopub.execute_input":"2021-07-05T10:57:47.805037Z","iopub.status.idle":"2021-07-05T10:57:47.816539Z","shell.execute_reply.started":"2021-07-05T10:57:47.804968Z","shell.execute_reply":"2021-07-05T10:57:47.815325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Introduction to Natural Language Processing","metadata":{"execution":{"iopub.status.busy":"2021-07-05T10:06:14.154671Z","iopub.execute_input":"2021-07-05T10:06:14.155119Z","iopub.status.idle":"2021-07-05T10:06:14.159922Z","shell.execute_reply.started":"2021-07-05T10:06:14.155085Z","shell.execute_reply":"2021-07-05T10:06:14.158502Z"}}},{"cell_type":"markdown","source":"### Python packages used","metadata":{}},{"cell_type":"code","source":"## Packages for data processing\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n## Packages for basic text processing\nimport re\nimport string \n\n## Import Logistic Regression function for model training from linear model module of sklearn package\nfrom sklearn.linear_model import LogisticRegression\n\n## Import functions for evaluation from  model selection module of sklearn package\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\n## Import functions for model selection from  model selection module of sklearn package\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\n\n## Import functions for text vectorization from feature extraction module of sklearn package\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n## Import function from NLTK package for Text tokenization and Normalization\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\n\n## Import matplotlib and wordcloud for Visualization\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\n\n## Import IFrame for display videos\nfrom IPython.display import IFrame\n\n## Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2021-07-05T10:57:47.81983Z","iopub.execute_input":"2021-07-05T10:57:47.820175Z","iopub.status.idle":"2021-07-05T10:57:48.341996Z","shell.execute_reply.started":"2021-07-05T10:57:47.820143Z","shell.execute_reply":"2021-07-05T10:57:48.341027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Natural Language Processing\n\n#### What is Natural Language?\n\n    What does it mean, Natural Language is which we humans use to communicate our messages,views and expressions.\n\n#### Is there any non Natural Language?\n\n    We can say that all programming language low or high level are non Natural Language because there structure is well defined and they are built for systems to communicate within themselves or for humans to express the logic for computers.\n\n#### Which one is complicated for systems to process?\n\n    Because programming language are designed for systems, even they might be difficult for humans to learn but for systems they are well strucuted and not complicated to process but human languages are not designed or developed to have this in mind. Human or Natural language needs to process and transalated into the language or representation which systems can understand. Natural language is result of human evalution and resulted from need to communicate the messages over the time, And humans just not use the words but there are many other aspects like expressions, tone of voice and how we structure the same words in different sentences to convey different meanings. For that reason it makes Natural Language a lot difficult to process and interpret underlying messages for systems which might be simple and straightforward for humans in most of the cases. Other than that humans do not have single language for communication globally and that makes it even more complicated because there is no single underlying structure.\n\n    Natural Language Processing (NLP) is a field which deals with processing of Natural Languages for systems so that interpretation of these languages can be used in many other applications. There are further sub fields of NLP which are Natural Language Understanding (NLU) and Natural Language Generation (NLG).\n\n    Natural Language Processing is sub field of Artificial Intelligence and deals the part of AI where there is need to process Natural Language to make intelligent agents.\n    \n    Natural Language Processing is not a single field but it combines multiple fields like Linguistics, Computer Science and Machine Learning\n    \n#### Applications of Natural Language Processing\n\n1. Machine Translation\n2. Question Answering\n3. Chatbots\n4. Summarizatin\n5. Information Extraction\n\nWatch the followng video from Dan Jurafsky for introduction to Natural Language Processing:","metadata":{}},{"cell_type":"code","source":"IFrame(\"https://www.youtube.com/embed/oWsMIW-5xUc?controls=0\",width=600,height=400)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T10:57:48.343923Z","iopub.execute_input":"2021-07-05T10:57:48.344335Z","iopub.status.idle":"2021-07-05T10:57:48.35305Z","shell.execute_reply.started":"2021-07-05T10:57:48.34429Z","shell.execute_reply":"2021-07-05T10:57:48.352127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predicting Disaster Tweets\n\nFor this tutorial we will use dataset from Kaggle playground competition \"Natural Language Processing with Disaster Tweets\". Objective of this problem is to predicts which Tweets are about real disasters and which one’s aren’t. Dataset contains tweets that were hand classified as disaster or non disaster. For more information you can visit following link:\n\nhttps://www.kaggle.com/c/nlp-getting-started\n\nWe will use Supervised Machine Learning technique Logistic Regression for building prediction model. To train Logistic model we will use linear models package from Sklearn machine learning python package, for documentation look into the below documentation for logistic regression model:\n    \nhttps://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n  \nTo learn more about Logistic Regression watch the following video from Andrew Ng's Machine Learning course:","metadata":{}},{"cell_type":"code","source":"IFrame('https://www.youtube.com/embed/-la3q9d7AKQ?controls=0',width=600,height=400)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T10:57:48.354153Z","iopub.execute_input":"2021-07-05T10:57:48.354446Z","iopub.status.idle":"2021-07-05T10:57:48.370455Z","shell.execute_reply.started":"2021-07-05T10:57:48.354415Z","shell.execute_reply":"2021-07-05T10:57:48.369525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. Load data and basic data exploration","metadata":{}},{"cell_type":"code","source":"tweets_dataset = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-07-05T10:57:48.371597Z","iopub.execute_input":"2021-07-05T10:57:48.371899Z","iopub.status.idle":"2021-07-05T10:57:48.406752Z","shell.execute_reply.started":"2021-07-05T10:57:48.371869Z","shell.execute_reply":"2021-07-05T10:57:48.405697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.1 Load Data","metadata":{}},{"cell_type":"markdown","source":"#### 1.1.1 Quick look at tweets dataset\n\nLoad data using pandas package from csv file and explore\n\n- How many rows and columns are there in dataset?\n- Quick look at first few rows of dataset","metadata":{}},{"cell_type":"code","source":"print(\"Tweets dataset contains %d rows and %d columns.\" % tweets_dataset.shape)\ntweets_dataset.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-05T10:57:48.408186Z","iopub.execute_input":"2021-07-05T10:57:48.408497Z","iopub.status.idle":"2021-07-05T10:57:48.423138Z","shell.execute_reply.started":"2021-07-05T10:57:48.408463Z","shell.execute_reply":"2021-07-05T10:57:48.421984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1.1.2 Keep only text and target columns for further processing\n\nFor this tutorial, we need text column and target labels, so will discard any other columns from dataset","metadata":{}},{"cell_type":"code","source":"tweets_dataset=tweets_dataset[['text','target']]","metadata":{"execution":{"iopub.status.busy":"2021-07-05T10:57:48.42501Z","iopub.execute_input":"2021-07-05T10:57:48.425561Z","iopub.status.idle":"2021-07-05T10:57:48.435651Z","shell.execute_reply.started":"2021-07-05T10:57:48.425511Z","shell.execute_reply":"2021-07-05T10:57:48.43466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1.1.3 Look the distribution of target variable\n\nIt is important to look at the distribution of target variable, because metrics used later for evaluation might be misleading if target value has imbalanced distribution of True and False labels","metadata":{}},{"cell_type":"code","source":"tweets_dataset.target.value_counts().sort_values().plot(kind = 'barh')","metadata":{"execution":{"iopub.status.busy":"2021-07-05T10:57:48.439429Z","iopub.execute_input":"2021-07-05T10:57:48.439833Z","iopub.status.idle":"2021-07-05T10:57:48.674071Z","shell.execute_reply.started":"2021-07-05T10:57:48.439794Z","shell.execute_reply":"2021-07-05T10:57:48.673162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Target value 1 means, the text is labeled as disaster tweet","metadata":{}},{"cell_type":"code","source":"print(\"%.1f tweets are labeled as disaster tweets in data\" % (\n    (tweets_dataset.target[tweets_dataset.target==1].count()/tweets_dataset.target.count())*100))","metadata":{"execution":{"iopub.status.busy":"2021-07-05T10:57:48.676317Z","iopub.execute_input":"2021-07-05T10:57:48.676659Z","iopub.status.idle":"2021-07-05T10:57:48.68994Z","shell.execute_reply.started":"2021-07-05T10:57:48.676626Z","shell.execute_reply":"2021-07-05T10:57:48.688935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.2 Split dataset into train and test datasets\n\nFor predicting disaster tweets using NLP problem, we will first split the data into two groups one for building or training the model and other set to evaluate the prediction accuracy on unseen data. For more about splitting the data into training and test please go through below link\n\nhttps://developers.google.com/machine-learning/crash-course/training-and-test-sets/splitting-data\n\n#### 1.2.1 Split the dataset into 70:30 ratio. 70% for training the model and 30% for validation","metadata":{}},{"cell_type":"code","source":"train, test = train_test_split(tweets_dataset,test_size=0.3,random_state=1)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T10:57:48.691332Z","iopub.execute_input":"2021-07-05T10:57:48.691666Z","iopub.status.idle":"2021-07-05T10:57:48.707459Z","shell.execute_reply.started":"2021-07-05T10:57:48.691635Z","shell.execute_reply":"2021-07-05T10:57:48.706453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.2 Explore the distribution of train and test datasets","metadata":{}},{"cell_type":"code","source":"print(\"Train dataset contains %d rows and %d columns.\" % train.shape)\nprint(\"Test dataset contains %d rows and %d columns.\" % test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T10:57:48.708954Z","iopub.execute_input":"2021-07-05T10:57:48.709652Z","iopub.status.idle":"2021-07-05T10:57:48.71732Z","shell.execute_reply.started":"2021-07-05T10:57:48.709605Z","shell.execute_reply":"2021-07-05T10:57:48.716421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Text Vectorization","metadata":{}},{"cell_type":"markdown","source":"To process the text data first we need to convert text data into numerical representation for systems to learn further from data. Vectorization is the process of converting a word ito a vector of numbers that contains the information in the word. \n\n- Simplest approach for vectorzing is to use counts of words.\n- Other more sophisticated apprach is TF-IDF (Term Frequency-Inverse Document Frequency)\n    \nTo learn more about these approaches go through following links:\n    \n1. https://www.analyticsvidhya.com/blog/2020/02/quick-introduction-bag-of-words-bow-tf-idf/\n2. https://www.mygreatlearning.com/blog/bag-of-words/\n    \nWatch the following video from Dan Jurafsky on basic text processing and word tokenization:","metadata":{}},{"cell_type":"code","source":"IFrame(\"https://www.youtube.com/embed/pEwBjcYdcKw?controls=0\",width=600,height=400)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T10:57:48.718728Z","iopub.execute_input":"2021-07-05T10:57:48.719256Z","iopub.status.idle":"2021-07-05T10:57:48.734206Z","shell.execute_reply.started":"2021-07-05T10:57:48.719222Z","shell.execute_reply":"2021-07-05T10:57:48.733137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.1 Use Bag-of-words\n\nFirst approach we will use for text vectorization is Bag of words. We will use CountVectorizer() from feature_extraction module in sklearn package. For documentation go through following link:\n\nhttps://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html","metadata":{}},{"cell_type":"code","source":"count_vectorizer = CountVectorizer()\ntrain_vectors = count_vectorizer.fit_transform(train[\"text\"])","metadata":{"execution":{"iopub.status.busy":"2021-07-05T10:57:48.73595Z","iopub.execute_input":"2021-07-05T10:57:48.736681Z","iopub.status.idle":"2021-07-05T10:57:48.863763Z","shell.execute_reply.started":"2021-07-05T10:57:48.736631Z","shell.execute_reply":"2021-07-05T10:57:48.862966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.2 Apply above learned vector transformation to test dataset\n\nIn our test data there might be some words which were not available in our vocabulary from train dataset. So we will learn the vocabulary using text from training dataset and will apply the same transformation to test data set.","metadata":{}},{"cell_type":"code","source":"test_vectors = count_vectorizer.transform(test[\"text\"])","metadata":{"execution":{"iopub.status.busy":"2021-07-05T10:57:48.864756Z","iopub.execute_input":"2021-07-05T10:57:48.865143Z","iopub.status.idle":"2021-07-05T10:57:48.906106Z","shell.execute_reply.started":"2021-07-05T10:57:48.865111Z","shell.execute_reply":"2021-07-05T10:57:48.905337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Train a model","metadata":{}},{"cell_type":"markdown","source":"Next step is to train our model using training vector to learn the relationship between words in text and labels assigned for each tweet.     For traning the model we will use 5 fold cross validation. To learn about cross validation watch the following video from Josh Starmer","metadata":{}},{"cell_type":"code","source":"IFrame(\"https://www.youtube.com/embed/fSytzGwwBVw\",width=600,height=400)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T10:57:48.907097Z","iopub.execute_input":"2021-07-05T10:57:48.907485Z","iopub.status.idle":"2021-07-05T10:57:48.911873Z","shell.execute_reply.started":"2021-07-05T10:57:48.907454Z","shell.execute_reply":"2021-07-05T10:57:48.911221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will use the F1 metric for training and selecting our model using cross_val_score() function from model_selection module of sklearn package. For more info follow the link below:\n\nhttps://scikit-learn.org/stable/modules/cross_validation.html\n\nThere are several metrices which can be used while validating the model and testing the results of our predictions:\n\n1. Confusion Matrix\n2. Accuracy\n2. Precision\n3. Recall\n4. F1 Score\n\nFor learning about these metrics go through below given links:\n\n- https://medium.com/analytics-vidhya/confusion-matrix-accuracy-precision-recall-f1-score-ade299cf63cd\n- https://www.analyticsvidhya.com/blog/2020/04/confusion-matrix-machine-learning/","metadata":{}},{"cell_type":"code","source":"IFrame(\"https://www.youtube.com/embed/jrAyRCa7aY8\",width=600,height=400)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T10:57:48.912891Z","iopub.execute_input":"2021-07-05T10:57:48.913275Z","iopub.status.idle":"2021-07-05T10:57:48.926196Z","shell.execute_reply.started":"2021-07-05T10:57:48.913247Z","shell.execute_reply":"2021-07-05T10:57:48.925181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.1 Train and fit model on training dataset","metadata":{}},{"cell_type":"markdown","source":"First we need to initializer Logistic Regrssion model","metadata":{"execution":{"iopub.status.busy":"2021-07-05T10:28:21.775151Z","iopub.execute_input":"2021-07-05T10:28:21.775591Z","iopub.status.idle":"2021-07-05T10:28:21.781282Z","shell.execute_reply.started":"2021-07-05T10:28:21.775557Z","shell.execute_reply":"2021-07-05T10:28:21.780054Z"}}},{"cell_type":"code","source":"clf = LogisticRegression()","metadata":{"execution":{"iopub.status.busy":"2021-07-05T10:57:48.927525Z","iopub.execute_input":"2021-07-05T10:57:48.928008Z","iopub.status.idle":"2021-07-05T10:57:48.93425Z","shell.execute_reply.started":"2021-07-05T10:57:48.92797Z","shell.execute_reply":"2021-07-05T10:57:48.933545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then perform 5 fold cross validation for model selection. After model selection we need to use this model on training dataset to learn the relationshipt between text data and target labels","metadata":{}},{"cell_type":"code","source":"scores = cross_val_score(clf, train_vectors, train[\"target\"], cv=5, scoring=\"f1\")\nfor k, score in zip(range(len(scores)),scores):\n    print(\"F1 Score for fold %d is %.2f \" % (k+1,score))","metadata":{"execution":{"iopub.status.busy":"2021-07-05T10:57:48.937462Z","iopub.execute_input":"2021-07-05T10:57:48.937765Z","iopub.status.idle":"2021-07-05T10:57:52.776462Z","shell.execute_reply.started":"2021-07-05T10:57:48.937736Z","shell.execute_reply":"2021-07-05T10:57:52.77542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf.fit(train_vectors, train[\"target\"])","metadata":{"execution":{"iopub.status.busy":"2021-07-05T10:57:52.78124Z","iopub.execute_input":"2021-07-05T10:57:52.783736Z","iopub.status.idle":"2021-07-05T10:57:53.421349Z","shell.execute_reply.started":"2021-07-05T10:57:52.783675Z","shell.execute_reply":"2021-07-05T10:57:53.420149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Prediction","metadata":{}},{"cell_type":"markdown","source":"### 4.1 Predict and explore how well our model predicted the unseen test data\n\nNow as our model has learnt some patterns using training data, now we can use it predict the labels on our remaining test dataset. Predictions can be made using predict function of Logistic Regression model.","metadata":{}},{"cell_type":"code","source":"test[\"pred\"] = clf.predict(test_vectors)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T10:57:53.422894Z","iopub.execute_input":"2021-07-05T10:57:53.423298Z","iopub.status.idle":"2021-07-05T10:57:53.435086Z","shell.execute_reply.started":"2021-07-05T10:57:53.423254Z","shell.execute_reply":"2021-07-05T10:57:53.433526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.2 Let's check how well our model performed on test dataset\n\nAfter predictions we need to evaluate how well our model performed on unseen data. We will look into accuracy metric and confusion matrix.","metadata":{}},{"cell_type":"code","source":"print(\"We got %.1f%% accuracy on our test dataset\" % (float(accuracy_score(test.target, test.pred))*100))","metadata":{"execution":{"iopub.status.busy":"2021-07-05T10:57:53.436854Z","iopub.execute_input":"2021-07-05T10:57:53.437691Z","iopub.status.idle":"2021-07-05T10:57:53.452046Z","shell.execute_reply.started":"2021-07-05T10:57:53.437643Z","shell.execute_reply":"2021-07-05T10:57:53.450317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion_matrix(test.target, test.pred)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T10:57:53.45419Z","iopub.execute_input":"2021-07-05T10:57:53.454621Z","iopub.status.idle":"2021-07-05T10:57:53.481129Z","shell.execute_reply.started":"2021-07-05T10:57:53.454573Z","shell.execute_reply":"2021-07-05T10:57:53.479949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test[(test.target==1) & (test.pred==1)].shape","metadata":{"execution":{"iopub.status.busy":"2021-07-05T10:57:53.482586Z","iopub.execute_input":"2021-07-05T10:57:53.482975Z","iopub.status.idle":"2021-07-05T10:57:53.502953Z","shell.execute_reply.started":"2021-07-05T10:57:53.482931Z","shell.execute_reply":"2021-07-05T10:57:53.501841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tn, fp, fn, tp = confusion_matrix(test.target, test.pred).ravel()\ntot = confusion_matrix(test.target, test.pred).sum()\n\nprint(\"True Negative Rate: %.1f%%\" % ((tn/tot)*100))\nprint(\"False Positive Rate: %.1f%%\" % ((fp/tot)*100))\nprint(\"False Negative Rate: %.1f%%\" % ((fn/tot)*100))\nprint(\"True Positive Rate: %.1f%%\" % ((tp/tot)*100))","metadata":{"execution":{"iopub.status.busy":"2021-07-05T10:57:53.504458Z","iopub.execute_input":"2021-07-05T10:57:53.504846Z","iopub.status.idle":"2021-07-05T10:57:53.530424Z","shell.execute_reply.started":"2021-07-05T10:57:53.504805Z","shell.execute_reply":"2021-07-05T10:57:53.52965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Summary and Next Steps\n\nThis was basic model which used simple vectorization technique and logistic regression model which provided 79.6% accuracy on unseen data. This model was built on raw text without any cleaning and processing, and there might be a lot of noise in text data. So it is very important to normalize the text data","metadata":{}},{"cell_type":"markdown","source":"## 5. Text Normalization\n\nText normalization is a pre-processing step which aims at improving the quality of the text and making it suitable to process. Below are the some basic steps for text normalization.\n \n1. Case normalization: applies to the languages that use uppercase and lowercase letters. All letters are converted to the same case. Application of this step might depend on case to case basis. In some of the cases, case type of words might contain some useful information.\n2. Puncuation removal: To remove the puncations in the text. In most cases, this should give a good quality.\n3. Stop Word removal: Stop word removal involves removing common words such as articles (the, an) and conjuncations (and, but) among others\n    - one way is to consider the frequency of occurances of words in a corpus, set of documents or text.\n    - Most frequent terms can then be selected as stop words to remove. But there can be cases where frequent terms might also have significant value e.g. word economy in financial documents might be a frequent word but still captures useful information. \n    - Stop word removal helps in reducing the size of vocabulary.\n4. Stemming and Lemmatization: This means reducing the word to it's root form\n     - helps in reducing the vocabulary.\n     - stemming and Lemmatization differs in their apprach and sophistication but serve the same objective.\n     - Stemming is a a simple approach that chops off the affixes of words. Stemming is aimed at reducing vocabulary and understanding of morphological processes.\n     - Lemmatization approaches this task in a more sophisticated manner, using vocabularies and morphological analysis of words. Using morphological information a words root can be returned.\n     \n     \nReference: Advanced Natural Language Processing with TensorFlow 2, Ashish Bansal\nhttps://www.amazon.com/Advanced-Natural-Language-Processing-TensorFlow/dp/1800200935","metadata":{}},{"cell_type":"code","source":"IFrame(\"https://www.youtube.com/embed/3_0rQjEgEy8?controls=0\",width=600,height=400)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T10:57:53.533789Z","iopub.execute_input":"2021-07-05T10:57:53.535472Z","iopub.status.idle":"2021-07-05T10:57:53.543265Z","shell.execute_reply.started":"2021-07-05T10:57:53.535424Z","shell.execute_reply":"2021-07-05T10:57:53.542292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.1 Text Cleaning and Normalization\n\nWe will perform some text cleaning and text normalization. Below are some of the operations we are going to perform on our text data:\n\n1. Lowercase all words in text\n2. Remove newline characters if any in text\n3. Remove punctuations\n4. Remove url's and links from text\n5. Remove tags from text\n6. Remove multiple spaces from text\n7. Remove special characters\n8. Remove stop words from text\n9. Apply stemming to normalize the text\n\nWill use word tokenizer from NLTK package for tokenization and Porter Stemmer from NLTK for stemming.\n\nNLTK is a python package for Natural Language Processing. It is considered as Swiss Knife of NLP, it might not be good for specific task but provides a lot of functionality for many general tasks in NLP processing. Below is link for Natural Language Processing using NLTK and python if you want to learn about the package in more details:\n\nhttp://www.nltk.org/book/\n\nFor text cleaning we will used regular expression and re package from python. Regular expression is very important basic tool for text processing. Below are some links to the videos from Dan Jurafsky and Christopher Manning to gain more understanding about regular expressions","metadata":{}},{"cell_type":"code","source":"IFrame(\"https://www.youtube.com/embed/EyzTQ0OKeNw\",width=600,height=400)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T10:57:53.547271Z","iopub.execute_input":"2021-07-05T10:57:53.547785Z","iopub.status.idle":"2021-07-05T10:57:53.555726Z","shell.execute_reply.started":"2021-07-05T10:57:53.547747Z","shell.execute_reply":"2021-07-05T10:57:53.554723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IFrame(\"https://www.youtube.com/embed/sUNEGBuRWzU\",width=600,height=400)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T10:57:53.557274Z","iopub.execute_input":"2021-07-05T10:57:53.55794Z","iopub.status.idle":"2021-07-05T10:57:53.567673Z","shell.execute_reply.started":"2021-07-05T10:57:53.557895Z","shell.execute_reply":"2021-07-05T10:57:53.566592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.2 Part of Speech Tagging\n\nOther important step in Text tokenization and normalization is Part-of-speech tagging. Objective of the part of speech tagging is to take a piece of text and tag each word with a Part-of-speech (POS) identifier:\n\nExample of some POS tags:\n\n\n\n\n| Tag | Meaning | English Examples\n| --- | --- | --- |\n|ADJ | adjective | new, good, high, special, big, local\n|ADP | adposition | on, of, at, with, by, into, under\n|ADV | adverb | really, already, still, early, now\n|CONJ | conjunction | and, or, but, if, while, although\n|DET | determiner, article | the, a, some, most, every, no, which\n|NOUN | noun | year, home, costs, time, Africa\n|NUM | numeral | twenty-four, fourth, 1991, 14:24\n|PRT | particle | at, on, out, over per, that, up, with\n|PRON | pronoun | he, their, her, its, my, I, us\n|VERB | verb | is, say, told, given, playing, would\n\nFor more detailed understanding for word tokenizer and POS taggin using NLTK package refer to below link:\n\nhttp://www.nltk.org/book/ch05.html\n\nWatch the below video from Christopher Manning about Part-of-speech tagging. Though we are not going to use this for our tweet prediction but it is good to have understanding of part of speech tagging because it can be useful in other tasks.","metadata":{}},{"cell_type":"code","source":"IFrame(\"https://www.youtube.com/embed/tJBvmkNsoN8\",width=600,height=400)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T10:57:53.56927Z","iopub.execute_input":"2021-07-05T10:57:53.569634Z","iopub.status.idle":"2021-07-05T10:57:53.578392Z","shell.execute_reply.started":"2021-07-05T10:57:53.5696Z","shell.execute_reply":"2021-07-05T10:57:53.577688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stop_words = stopwords.words('english')\nstemmer = PorterStemmer()\ndef lower_text(text):\n    \"\"\"\n        function to convert text into lowercase\n        input: text\n        output: cleaned text\n    \"\"\"\n    text = text.lower() # lowering\n    return text\n    \ndef remove_newline(text):\n    \"\"\"\n        function to remove new line characters in text\n        input: text\n        output: cleaned text\n    \"\"\"\n    text = re.sub(r'\\n',' ', text)\n    return text\n\ndef remove_punctuations(text):\n    \"\"\"\n        function to remove punctuations from text\n        input: text\n        output: cleaned text\n    \"\"\"    \n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    return text\n\n\ndef remove_links(text):\n    \"\"\"\n        function to links and urls from text\n        input: text\n        output: cleaned text\n    \"\"\"    \n    text = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', text, flags=re.MULTILINE)\n    \n    return text\n\ndef remove_tags(text):\n    \"\"\"\n        function to remove references and hashtags from text\n        input: text\n        output: cleaned text\n    \"\"\"    \n    text = re.sub(\"(@[A-Za-z0-9]+)|(#[A-Za-z0-9]+)|(\\w+:\\/\\/\\S+)\",\"\",text)\n    return text\n    \ndef remove_multiplespaces(text):\n    \"\"\"\n        function to remove multiple spaces from text\n        input: text\n        output: cleaned text\n    \"\"\"    \n    text = re.sub(r'\\s+', ' ', text, flags=re.I)\n    return text\n\ndef remove_specialchars(text):\n    \"\"\"\n        function to remove special characters from text\n        input: text\n        output: cleaned text\n    \"\"\"    \n    text = re.sub(r'\\W', ' ', text)\n    return text\n\ndef remove_stopwords(text):\n    \"\"\"\n        function to tokenize the words using nltk word tokenizer and remove the stop words using nltk package's english stop words\n        input: text\n        output: cleaned text\n    \"\"\"    \n    text = ' '.join([word for word in word_tokenize(text) if word not in stop_words])\n    return text\n\ndef word_stemming(text):\n    \"\"\"\n        function to perform stemming using porter stemmer from nltk package\n        input: text\n        output: cleaned text\n    \"\"\"        \n    text=' '.join([stemmer.stem(word) for word in word_tokenize(text)])\n    return text","metadata":{"execution":{"iopub.status.busy":"2021-07-05T10:57:53.579717Z","iopub.execute_input":"2021-07-05T10:57:53.579999Z","iopub.status.idle":"2021-07-05T10:57:53.591202Z","shell.execute_reply.started":"2021-07-05T10:57:53.579955Z","shell.execute_reply":"2021-07-05T10:57:53.590452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.1.1 Below code will use above functions for text cleaning and normalization\n\n#### 5.1.1.1 Apply the processing on Train dataset","metadata":{}},{"cell_type":"code","source":"# Covert text to lowercase\ntrain.text=train.text.apply(lambda text: lower_text(text))\n\n# Remove newlines\ntrain.text=train.text.apply(lambda text: remove_newline(text))\n\n# Remove punctuations\ntrain.text=train.text.apply(lambda text: remove_punctuations(text))\n\n# Remove links\ntrain.text=train.text.apply(lambda text: remove_links(text))\n\n# Remove tags\ntrain.text=train.text.apply(lambda text: remove_tags(text))\n\n# Remove multiple spaces\ntrain.text=train.text.apply(lambda text: remove_multiplespaces(text))\n\n# Remove special characters\ntrain.text=train.text.apply(lambda text: remove_specialchars(text))\n\n# Remove stopwords\ntrain.text=train.text.apply(lambda text: remove_stopwords(text))\n\n# Apply Stemming\ntrain.text=train.text.apply(lambda text: word_stemming(text))","metadata":{"execution":{"iopub.status.busy":"2021-07-05T10:57:53.592541Z","iopub.execute_input":"2021-07-05T10:57:53.592839Z","iopub.status.idle":"2021-07-05T10:57:56.318342Z","shell.execute_reply.started":"2021-07-05T10:57:53.592809Z","shell.execute_reply":"2021-07-05T10:57:56.317318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 5.1.1.2 Apply the processing on Test dataset","metadata":{}},{"cell_type":"code","source":"# Covert text to lowercase\ntest.text=test.text.apply(lambda text: lower_text(text))\n\n# Remove newlines\ntest.text=test.text.apply(lambda text: remove_newline(text))\n\n# Remove punctuations\ntest.text=test.text.apply(lambda text: remove_punctuations(text))\n\n# Remove links\ntest.text=test.text.apply(lambda text: remove_links(text))\n\n# Remove tags\ntest.text=test.text.apply(lambda text: remove_tags(text))\n\n# Remove multiple spaces\ntest.text=test.text.apply(lambda text: remove_multiplespaces(text))\n\n# Remove special characters\ntest.text=test.text.apply(lambda text: remove_specialchars(text))\n\n# Remove stopwords\ntest.text=test.text.apply(lambda text: remove_stopwords(text))\n\n# Apply Stemming\ntest.text=test.text.apply(lambda text: word_stemming(text))","metadata":{"execution":{"iopub.status.busy":"2021-07-05T10:57:56.319724Z","iopub.execute_input":"2021-07-05T10:57:56.320182Z","iopub.status.idle":"2021-07-05T10:57:57.473835Z","shell.execute_reply.started":"2021-07-05T10:57:56.320136Z","shell.execute_reply":"2021-07-05T10:57:57.472848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Text Visualization\n\nThere is a one basic visualization named as Word Clouds. This visualization provides information about frequently used words in our text corpus. wordcloud package provides word cloud visualizations in python. For more learning about wordcloud refer to the following link\n\nhttps://www.analyticsvidhya.com/blog/2020/10/word-cloud-or-tag-cloud-in-python/","metadata":{}},{"cell_type":"code","source":"wc_disaster = WordCloud(\n        width=800, height=600,\n        background_color='white',\n        stopwords=STOPWORDS\n    ).generate(' '.join(train[train.target==1]['text']))\n\nwc_nondisaster = WordCloud(\n        width=800, height=600,\n        background_color='white',\n        stopwords=STOPWORDS\n    ).generate(' '.join(train[train.target==0]['text']))\n\nfig, (ax1, ax2) = plt.subplots(1, 2,figsize=(20, 12))\n\nax1.imshow(wc_disaster)\nax1.set_title(\"Word cloud of disaster tweets\", fontsize=20)\nax1.axis(\"off\")\n\nax2.imshow(wc_nondisaster)\nax2.set_title(\"Word cloud of non disaster tweets\", fontsize=20)\nax2.axis(\"off\")\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-05T10:57:57.475013Z","iopub.execute_input":"2021-07-05T10:57:57.475309Z","iopub.status.idle":"2021-07-05T10:58:00.653991Z","shell.execute_reply.started":"2021-07-05T10:57:57.475279Z","shell.execute_reply":"2021-07-05T10:58:00.652862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7. Train the model with new training data and predict using new test data\n\n### 7.1 Text vectorizaton","metadata":{}},{"cell_type":"code","source":"count_vectorizer = CountVectorizer()\ntrain_vectors = count_vectorizer.fit_transform(train[\"text\"])\ntest_vectors = count_vectorizer.transform(test[\"text\"])","metadata":{"execution":{"iopub.status.busy":"2021-07-05T10:58:00.655567Z","iopub.execute_input":"2021-07-05T10:58:00.655996Z","iopub.status.idle":"2021-07-05T10:58:00.775166Z","shell.execute_reply.started":"2021-07-05T10:58:00.655952Z","shell.execute_reply":"2021-07-05T10:58:00.774318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7.2 Train the model","metadata":{}},{"cell_type":"code","source":"clf = LogisticRegression()\n\nscores = cross_val_score(clf, train_vectors, train[\"target\"], cv=5, scoring=\"f1\")\nfor k, score in zip(range(len(scores)),scores):\n    print(\"F1 Score for fold %d is %.2f \" % (k+1,score))\n    \nclf.fit(train_vectors, train[\"target\"])","metadata":{"execution":{"iopub.status.busy":"2021-07-05T10:58:00.776143Z","iopub.execute_input":"2021-07-05T10:58:00.776556Z","iopub.status.idle":"2021-07-05T10:58:02.851724Z","shell.execute_reply.started":"2021-07-05T10:58:00.776527Z","shell.execute_reply":"2021-07-05T10:58:02.850294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7.3 Make predictions and evaluate the model","metadata":{}},{"cell_type":"code","source":"test[\"pred\"] = clf.predict(test_vectors)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T10:58:02.857282Z","iopub.execute_input":"2021-07-05T10:58:02.8613Z","iopub.status.idle":"2021-07-05T10:58:02.873007Z","shell.execute_reply.started":"2021-07-05T10:58:02.861234Z","shell.execute_reply":"2021-07-05T10:58:02.871659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"We got %.1f%% accuracy on our test dataset\" % (float(accuracy_score(test.target, test.pred))*100))","metadata":{"execution":{"iopub.status.busy":"2021-07-05T10:58:02.878809Z","iopub.execute_input":"2021-07-05T10:58:02.882843Z","iopub.status.idle":"2021-07-05T10:58:02.900201Z","shell.execute_reply.started":"2021-07-05T10:58:02.882775Z","shell.execute_reply":"2021-07-05T10:58:02.898689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tn, fp, fn, tp = confusion_matrix(test.target, test.pred).ravel()\ntot = confusion_matrix(test.target, test.pred).sum()\n\nprint(\"True Negative Rate: %.1f%%\" % ((tn/tot)*100))\nprint(\"False Positive Rate: %.1f%%\" % ((fp/tot)*100))\nprint(\"False Negative Rate: %.1f%%\" % ((fn/tot)*100))\nprint(\"True Positive Rate: %.1f%%\" % ((tp/tot)*100))","metadata":{"execution":{"iopub.status.busy":"2021-07-05T10:58:02.906033Z","iopub.execute_input":"2021-07-05T10:58:02.910412Z","iopub.status.idle":"2021-07-05T10:58:02.946061Z","shell.execute_reply.started":"2021-07-05T10:58:02.910326Z","shell.execute_reply":"2021-07-05T10:58:02.944988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Summary and Next Steps\n\nWe used the simple text vectorization with text cleaning and normalization which resulted in very little improvement in accuracy from 79.6% to 79.8% that is not a huge difference. But objective here was to learn about the text normalizaton and pre-processing but single approach can not provide a significant difference in results. This depends on case to case basis and combination of different approaches based on task and data.","metadata":{}},{"cell_type":"markdown","source":"## 8. Train the model using TF-IDF vectorization and see if that can help in some improvement ","metadata":{"execution":{"iopub.status.busy":"2021-07-05T00:52:40.917128Z","iopub.execute_input":"2021-07-05T00:52:40.917569Z","iopub.status.idle":"2021-07-05T00:52:40.922583Z","shell.execute_reply.started":"2021-07-05T00:52:40.917533Z","shell.execute_reply":"2021-07-05T00:52:40.921137Z"}}},{"cell_type":"markdown","source":"### 8.1 TF-IDF Vectorization\n\nFor last model we used Bag-of-words, now we will use TF-IDF vectorization approach. For documentation abut TF-IDF from sklearn package follow the below link\n\nhttps://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html","metadata":{}},{"cell_type":"code","source":"tfidf_vectorizer = TfidfVectorizer()\ntrain_vectors = tfidf_vectorizer.fit_transform(train[\"text\"])\ntest_vectors = tfidf_vectorizer.transform(test[\"text\"])","metadata":{"execution":{"iopub.status.busy":"2021-07-05T10:58:02.951253Z","iopub.execute_input":"2021-07-05T10:58:02.951941Z","iopub.status.idle":"2021-07-05T10:58:03.073292Z","shell.execute_reply.started":"2021-07-05T10:58:02.951902Z","shell.execute_reply":"2021-07-05T10:58:03.072424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 8.2 Train the model","metadata":{"execution":{"iopub.status.busy":"2021-07-05T01:03:31.572966Z","iopub.execute_input":"2021-07-05T01:03:31.573395Z","iopub.status.idle":"2021-07-05T01:03:31.577657Z","shell.execute_reply.started":"2021-07-05T01:03:31.573357Z","shell.execute_reply":"2021-07-05T01:03:31.576683Z"}}},{"cell_type":"code","source":"clf = LogisticRegression()\n\nscores = cross_val_score(clf, train_vectors, train[\"target\"], cv=5, scoring=\"f1\")\nfor k, score in zip(range(len(scores)),scores):\n    print(\"F1 Score for fold %d is %.2f \" % (k+1,score))\n    \nclf.fit(train_vectors, train[\"target\"])","metadata":{"execution":{"iopub.status.busy":"2021-07-05T10:58:03.074856Z","iopub.execute_input":"2021-07-05T10:58:03.075436Z","iopub.status.idle":"2021-07-05T10:58:04.6443Z","shell.execute_reply.started":"2021-07-05T10:58:03.075378Z","shell.execute_reply":"2021-07-05T10:58:04.643276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 8.3 Make predictions and evaluate the model","metadata":{}},{"cell_type":"code","source":"test[\"pred\"] = clf.predict(test_vectors)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T10:58:04.649348Z","iopub.execute_input":"2021-07-05T10:58:04.651986Z","iopub.status.idle":"2021-07-05T10:58:04.663576Z","shell.execute_reply.started":"2021-07-05T10:58:04.65192Z","shell.execute_reply":"2021-07-05T10:58:04.661939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"We got %.1f%% accuracy on our test dataset\" % (float(accuracy_score(test.target, test.pred))*100))","metadata":{"execution":{"iopub.status.busy":"2021-07-05T10:58:04.670266Z","iopub.execute_input":"2021-07-05T10:58:04.674719Z","iopub.status.idle":"2021-07-05T10:58:04.69508Z","shell.execute_reply.started":"2021-07-05T10:58:04.674642Z","shell.execute_reply":"2021-07-05T10:58:04.693118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tn, fp, fn, tp = confusion_matrix(test.target, test.pred).ravel()\ntot = confusion_matrix(test.target, test.pred).sum()\n\nprint(\"True Negative Rate: %.1f%%\" % ((tn/tot)*100))\nprint(\"False Positive Rate: %.1f%%\" % ((fp/tot)*100))\nprint(\"False Negative Rate: %.1f%%\" % ((fn/tot)*100))\nprint(\"True Positive Rate: %.1f%%\" % ((tp/tot)*100))","metadata":{"execution":{"iopub.status.busy":"2021-07-05T10:58:04.702126Z","iopub.execute_input":"2021-07-05T10:58:04.706557Z","iopub.status.idle":"2021-07-05T10:58:04.74003Z","shell.execute_reply.started":"2021-07-05T10:58:04.706475Z","shell.execute_reply":"2021-07-05T10:58:04.738978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tutorial Summary and Next Steps\n\nIn this tutorial notebook we tried following approaches:\n    \n1. Approach 1: Logistic Model with bag of words approach which resulted in 79.6% accuracy on test data.\n2. Approach 2: Logistic Model using bag of words using cleaned and normalized data which resulted in 79.8% accuracy on test data.\n3. Approach 3: Logistic Model using TF-IDF vectorization using cleaned and normalized data which resulted in 80.1% accuracy on test data.\n   \nThis was introduction to Natural Language Processing where we used text data for perdiction and it was just a start. For more accuracy we might need to perform some different text normalization, use of different model instead of Logistic Regression or different metric to evaluate our model.  ","metadata":{}},{"cell_type":"markdown","source":"Landscape of Natural language processing has changed a lot in recent years with state of art research and development in this field with use availability of huge computation power and advancment in Neaural Network Models specially Recurrent Neural Networks.","metadata":{}}]}