{"cells":[{"metadata":{},"cell_type":"markdown","source":"# NLP Disasters Prediction\n## Benchmarking BERT Variations\n\n![NLP](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcTWL5PJwJNJ8t8FJzL3b0qvXwfpC1jbDVG3Q59ue1xgN87mxfK7)\n\nThe purpose of this notebook is to benchmark BERT variations, checking the validation and submission scores for each one of them and try ensemble the predictions in order to get a better classifier.\n\n* **1.** We start by running the models: BERT, ROBERTA, XLNET and ALBERT using the combination of Hugging Face + KTrain wrapper. We can notice how the KTrain tool can reduce significantly the necessary lines of code to create a model.\n* **2.** Then, we evaluate the ROC curves and metrics for each one of these models.\n* **3.** Finally, we try to stack the models by taking averages and comparing different scores with the pure estimators\n* **4.** Submit the models and evaluate the submission metrics\n* **5.** Take conclusions\n\n**Note:** In my previous [NLP Disasters Predictions notebook](https://www.kaggle.com/guidant/disasternlp-benchmarking-tfhub-bert-variations), I tried to improve the BERT model by adding new features and autocorrecting the input texts. These tries were not succcessful but, as we can see here, we will improve the BERT result this time. I am writting a new notebook because the idea and the libraries that I will use are both, different from the ones of the previous try."},{"metadata":{},"cell_type":"markdown","source":"# Part 0. Before starting: Install ktrain and import libraries\n\nWe start by installing the ktrain library and by importing everything that we will use. Notice that, in this version of the docker, it's necessary to update the scikit learn before installing the ktrain tool."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%capture\n!pip install -U scikit-learn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%capture\n!pip install ktrain","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nimport ktrain\nimport plotnine\nimport seaborn as sns\n\nfrom ktrain import text\nfrom transformers import *\nfrom plotnine import *\nfrom plotnine.options import figure_size\n\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.metrics import roc_curve, auc, accuracy_score, f1_score\n\nfrom pylab import rcParams","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Importing the inputs and taking a look:"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"input_dir = '/kaggle/input/nlp-getting-started/'\ncheckpoint_dir = '/kaggle/input/checkpoint-nlp-disaster/'\noutput_dir = '/kaggle/working/'\n\ndf_fit = pd.read_csv(input_dir + 'train.csv')\ndf_sub = pd.read_csv(input_dir + 'test.csv')\n\ndf_fit.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ok then, we are ready to start!\n\n# 1. Training Different BERT Variations\n\n![SSS](https://faculty.elgin.edu/dkernler/statistics/ch01/images/strata-sample.gif)\n\nWe start by getting a stratified sample for training and testing. In our case, stratification means: getting different sets with the same percentage of zeroes and ones in the target columns (i.e: taking samples with the a similar variation and distribution of different characteristics):"},{"metadata":{"trusted":true},"cell_type":"code","source":"sss = StratifiedShuffleSplit(n_splits=1, random_state=42)\ntrain_index, valid_index = next(sss.split(df_fit, df_fit['target']))\n\ndf_trn = df_fit.iloc[train_index, :]\ndf_tst = df_fit.iloc[valid_index, :]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we have a for loop to get the training, testing and submission results for each model in order to generate our benchmarks. The models_list is the list of the models that will be trained and if it has no elements, we load the results of the previous execution. It's not possible to run all the models in a loop like that because of memory limitations but I ran each model in different sections, saving all the results:"},{"metadata":{"trusted":true},"cell_type":"code","source":"benchmark_dict = dict()\n\n# model_list = ['bert-base-uncased', 'xlnet-large-cased', 'albert-base-v2', 'roberta-base']\nmodel_list = []\n\nif len(model_list) > 0:\n\n    nrow_trn, nrow_tst, nrow_sub = df_trn.shape[0], df_tst.shape[0], df_sub.shape[0]\n    dict_benchmark = {'Model': [], 'Type': [], 'Id': [], 'Prediction': [], 'Target': []}\n    dict_submission = {'Model': [], 'Id': [], 'Proba': []}\n\n    for curr_model in model_list:\n    \n        curr_benchmark = dict()\n\n        t = text.Transformer(curr_model, maxlen=140, classes=[0, 1])\n        trn = t.preprocess_train(df_trn['text'].tolist(), df_trn['target'].tolist())\n        val = t.preprocess_train(df_tst['text'].tolist(), df_tst['target'].tolist())\n\n        model = t.get_classifier()\n        learner = ktrain.get_learner(model, train_data=trn, val_data=val, batch_size=16)\n        learner.fit_onecycle(2e-6, 3)\n        predictor = ktrain.get_predictor(model, t)\n    \n        curr_predict_trn = predictor.predict_proba(df_trn['text'].tolist())[:, 1].tolist()\n        curr_predict_tst = predictor.predict_proba(df_tst['text'].tolist())[:, 1].tolist()\n        curr_predict_sub = predictor.predict_proba(df_sub['text'].tolist())[:, 1].tolist()\n    \n        dict_benchmark['Model'] += (nrow_trn + nrow_tst) * [curr_model]\n        dict_benchmark['Type'] += (nrow_trn * ['Train']) + (nrow_tst * ['Test'])\n        dict_benchmark['Id'] += df_trn['id'].tolist() + df_tst['id'].tolist()\n        dict_benchmark['Prediction'] += curr_predict_trn + curr_predict_tst\n        dict_benchmark['Target'] += df_trn['target'].tolist() + df_tst['target'].tolist()\n    \n        dict_submission['Model'] += nrow_sub * [curr_model]\n        dict_submission['Id'] += df_sub['id'].tolist()\n        dict_submission['Proba'] += curr_predict_sub\n    \n    df_benchmark, df_submission = pd.DataFrame(dict_benchmark), pd.DataFrame(dict_submission)\n    df_benchmark.to_csv(output_dir + 'benchmark.csv'), df_submission.to_csv('submission.csv')\n    \nelse:\n    \n    benchmark_list, submission_list = [], []\n    for model_name in ['albert', 'bert', 'roberta', 'xlnet']:\n        benchmark_list.append(pd.read_csv(checkpoint_dir + 'benchmark_' + model_name + '.csv'))\n        submission_list.append(pd.read_csv(checkpoint_dir + 'submission_' + model_name + '.csv'))\n        \n    df_benchmark = pd.concat(benchmark_list, ignore_index=True)\n    df_submission = pd.concat(submission_list, ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, here we have the dataset with all the predictions and targets:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_benchmark = df_benchmark.loc[:, ['Model', 'Type', 'Id', 'Prediction', 'Target']]\ndf_benchmark.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Evaluating the BERT Variations\n\n## 2.1. Comparing ROC Curves\n\nLet's start by comparing the ROC curves of each model:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_df_roc(df_in, models_list=None):\n    df_roc = pd.DataFrame({'Model': [], 'FalsePositiveRate': [], 'TruePositiveRate': []})\n    if models_list is None:\n        models_list = set(df_in['Model'].tolist())\n    for curr_model in models_list:\n        fpr_list, tpr_list, _ = roc_curve(df_in.loc[df_in['Model'] == curr_model, :]['Target'].tolist(), \n                                          df_in.loc[df_in['Model'] == curr_model, :]['Prediction'].tolist())\n        model_list = [curr_model] * len(fpr_list)\n    \n        df2append = pd.DataFrame({\n            'Model': model_list,\n            'FalsePositiveRate': fpr_list,\n            'TruePositiveRate': tpr_list\n        })\n        df_roc = pd.concat([df_roc, df2append], ignore_index=True)\n    return df_roc\n\ndf_roc = get_df_roc(df_benchmark)\ndf_roc.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using the facets layer to compare all the curves:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_roc_curves(df_in, models_list=None, size_x=10, size_y=7):\n    plotnine.options.figure_size = (size_x, size_y)\n    return ggplot(get_df_roc(df_in, models_list), aes(x='FalsePositiveRate', y='TruePositiveRate', color='Model', group='Model')) +\\\n        geom_line() + facet_wrap('~ Model', nrow=2) + theme(legend_position='none') +\\\n        theme(text=element_text(size=14)) + ggtitle('Comparing ROC curves:')\n    \nplot_roc_curves(df_benchmark)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the ROBERTA and BERT models seems to have a better behaviour. \n\n## 2.2. Comparing Scores\n\nWe should confirm it by taking a look at different types of scores. We will check the **AUC (Area Under ROC Curve)**, the **Accuracy** and the **F1-Score**:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_benchmark_scores(df_in, models_list=set(df_benchmark['Model'].tolist())):\n\n    dict_metrics = dict(Model=[], Metric=[], Value=[])\n\n    for curr_model in models_list:\n        for metric in ['Accuracy', 'AUC', 'F1-Score']:\n        \n            filter_cond = df_in['Model'] == curr_model\n            filter_cond &= df_in['Type'] == 'Test'\n            \n            sorted_df = df_in.loc[filter_cond, ['Id', 'Prediction', 'Target']].sort_values(by='Id')\n            predictions = sorted_df['Prediction'].tolist()\n            targets = sorted_df['Target'].tolist()\n        \n            if metric in ['Accuracy', 'F1-Score']:\n                predictions = [1 if X >= 0.5 else 0 for X in predictions]\n            \n            if metric == 'Accuracy':\n                score = accuracy_score(targets, predictions)\n            elif metric == 'F1-Score':\n                score = accuracy_score(targets, predictions)\n            elif metric == 'AUC':\n                fpr, tpr, _ = roc_curve(targets, predictions)\n                score = auc(fpr, tpr)\n            \n            dict_metrics['Model'].append(curr_model)\n            dict_metrics['Metric'].append(metric)\n            dict_metrics['Value'].append(score)\n        \n    return pd.DataFrame(dict_metrics)\n\ndf_metrics = get_benchmark_scores(df_benchmark)\ndf_metrics.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting the table results in the form of a comparative grid, where the models represent the columns and the rows represent the scores:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_benchmark_scores(df_in, models_list=set(df_benchmark['Model'].tolist())):\n    \n    return ggplot(get_benchmark_scores(df_in, models_list), aes(x='Model', y='Value', fill='Model')) +\\\n        geom_hline(aes(yintercept='Value'), linetype='dashed') +\\\n        geom_point(size=5) + facet_grid('Metric~Model',scales='free_x') +\\\n        ggtitle('Comparing Scores for Different Models')\n\nplot_benchmark_scores(df_benchmark)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again, we can see that the best results are obtained for the BERT and the ROBERTA model. **BUT** the Roberta Model seems to be better than the original BERT! Let's start to plan our model stacking.\n\n# 3. Stacking the Models\n\n## 3.1. Comparing Correlations\n\nComparing the correlations between the outputs of each pair of models over the validation set:"},{"metadata":{"trusted":true},"cell_type":"code","source":"dict_corr = dict(Model1=[], Model2=[], Correlation=[])\nmodels_list = set(df_benchmark['Model'].tolist())\n\nfor model1 in models_list:\n    for model2 in models_list:\n        \n        dict_corr['Model1'].append(model1)\n        dict_corr['Model2'].append(model2)\n        \n        filter_cond1 = df_benchmark['Model'] == model1\n        filter_cond1 &= df_benchmark['Type'] == 'Test'\n        \n        filter_cond2 = df_benchmark['Model'] == model2\n        filter_cond2 &= df_benchmark['Type'] == 'Test'\n        \n        pred1 = df_benchmark.loc[filter_cond1]['Prediction'].tolist()\n        pred2 = df_benchmark.loc[filter_cond2]['Prediction'].tolist()\n        \n        dict_corr['Correlation'].append(np.corrcoef(np.array(pred1), np.array(pred2))[0, 1])\n        \ndict_model_bias = {\n    'albert-base-v2': 'ALBERT',\n    'bert-base-uncased': 'BERT',\n    'roberta-base': 'ROBERTA',\n    'xlnet-large-cased': 'XLNET'\n}\n        \ndf_corr = pd.DataFrame(dict_corr)\ndf_corr['Model1'] = df_corr['Model1'].apply(lambda X: dict_model_bias[X])\ndf_corr['Model2'] = df_corr['Model2'].apply(lambda X: dict_model_bias[X])\n\ndf_corr.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_corr['X'] = 0\ndf_corr['Y'] = 0\n\nggplot(df_corr, aes(x='X', y='Y', fill='Correlation')) +\\\n    geom_point(aes(size='Correlation')) + ggtitle('Correlation Among Models') + facet_grid('Model1~Model2') +\\\n    theme(text=element_text(size=10), axis_text=element_blank()) + xlab('Model 1') + ylab('Model 2') +\\\n    scale_size_continuous([10, 20])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there is a strong correlation between the best $2$ models, which makes sense: both of them has really high scores and consequently both models will generate similar predictions. Two stacking possibilities will be explored here:\n\n1. Taking a simple average between the BERT and the ROBERTA model (the best ones) and\n2. Taking a complete average, along all estimators\n\n## 3.2. Evaluating Ensemble Models\n\nSo, let's compare the $2$ ensemble models proposed in the last section with the best pure models (BERT and Roberta). We will use the functions that we already defined in the previous sections:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_df_ensemble(df_in, name_new_model):\n    \n    df_ensemble = df_in.groupby('Id').mean()\n    df_ensemble['Id'] = df_ensemble.index\n\n    train_ids = list(set(df_in[df_in['Type'] == 'Train']['Id'].tolist()))\n    test_ids = list(set(df_in[df_in['Type'] == 'Test']['Id'].tolist()))\n\n    df_ensemble['Id'] = df_ensemble.index\n    df_ensemble['Model'] = name_new_model\n    df_ensemble['Type'] = 'NA'\n    df_ensemble.loc[df_ensemble['Id'].isin(train_ids), 'Type'] = 'Train'\n    df_ensemble.loc[df_ensemble['Id'].isin(test_ids), 'Type'] = 'Test'\n    df_ensemble = df_ensemble.loc[:, ['Model', 'Type', 'Id', 'Prediction', 'Target']]\n    \n    return df_ensemble\n\ndf_ensemble_all = get_df_ensemble(df_benchmark, 'ensemble_all')\nbest_models = ['bert-base-uncased', 'roberta-base']\ndf_ensemble_best = get_df_ensemble(df_benchmark.loc[df_benchmark['Model'].isin(best_models), :], 'ensemble_best')\n\ndf_benchmark_improved = pd.concat([df_benchmark, df_ensemble_all, df_ensemble_best], ignore_index=True)\nplot_benchmark_scores(df_benchmark_improved, models_list=best_models + ['ensemble_best', 'ensemble_all'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, ensembling all the models generates results that are almost equal to a ROBERTA Neural Network. Let's check the ROC Curves...we could explose all the models but I want to not show many useless graphics with the price of losing interpretability."},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_roc_curves(df_benchmark_improved, models_list=['bert-base-uncased', 'roberta-base', 'ensemble_all', 'ensemble_best'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Preparing / Evaluating Submissions\n\nLet's submit $4$ different tries:\n\n1. The BERT model\n2. The ROBERTA model\n3. The average of the outputs of all used models and\n4. The average of the outputs of the BERT and ROBERTA estimators\n\nThen, I will show and compare the submission results for each one of them."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_submission = df_submission.loc[:, ['Model', 'Id', 'Proba']]\ndf_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_submission_ensemble_all = df_submission.groupby('Id').mean()\ndf_submission_ensemble_all['Id'] = df_submission_ensemble_all.index\ndf_submission_ensemble_all['Model'] = 'ensemble_all'\ndf_submission_ensemble_all = df_submission_ensemble_all.loc[:, ['Model', 'Id', 'Proba']]\n\ndf_submission_ensemble_best = df_submission.loc[df_submission['Model'].isin(best_models), :].groupby('Id').mean()\ndf_submission_ensemble_best['Id'] = df_submission_ensemble_best.index\ndf_submission_ensemble_best['Model'] = 'ensemble_best'\ndf_submission_ensemble_best = df_submission_ensemble_best.loc[:, ['Model', 'Id', 'Proba']]\n\ndf_submission_enhanced = pd.concat([df_submission, df_submission_ensemble_all, df_submission_ensemble_best], ignore_index=True)\ndf_submission_enhanced.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def export_model(model_name):\n    df_export = df_submission_enhanced.loc[df_submission_enhanced['Model'] == model_name, :]\n    df_export.loc[:, 'Target'] = df_export['Proba'].apply(lambda X: 1 if X >= 0.5 else 0)\n    df_export = df_export.loc[:, ['Id', 'Target']]\n    df_export.rename({'Id': 'id', 'Target': 'target'}, inplace=True)\n    df_export.to_csv(output_dir + model_name + '.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Exporting the different submissions:"},{"metadata":{"trusted":true},"cell_type":"code","source":"export_model('ensemble_all')\nexport_model('ensemble_best')\nexport_model('bert-base-uncased')\nexport_model('roberta-base')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And here I noted the submission results for each try. Again, we can see that the ROBERTA and the Ensemble Model involving all the estimators generate the same final scores. For that reason, in our benchmark, I will consider that the best model is the average of all the possibilities since, in the real life, stacked models tend to generate less cases of overfitting:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_submission_results = pd.DataFrame({\n    'Model': ['Roberta', 'Bert', 'Ensemble All', 'Ensemble Best'],\n    'Value': [0.83026, 0.82310, 0.83026, 0.82822],\n    'X': [0] * 4\n})\n\nggplot(df_submission_results, aes(x='X', y='Value', fill='Model')) + geom_hline(aes(yintercept='Value'), linetype='dashed') +\\\n    geom_point(stat='identity', color='black', size=10) + facet_grid('~Model') +\\\n    xlab('Model') + ggtitle('Benchmarking Submissions') + ylab('F1 Kaggle Final Score') +\\\n    theme(legend_position='none', axis_text_x=element_blank(), text=element_text(size=12))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Conclusions\n\n* The BERT Mmodel and the ROBERTA model are the best single estimators BUT\n* Taking the average of all the Neural Networks is better than taking the average just between the $2$ models\n* It happens because the BERT and the ROBERTA models are strongly correlated AND\n* We can also conclude that our champions are: ROBERTA or FULL AVERAGE MODEL\n* Then: we get the FULL AVERAGE MODEL because, in real life, it will be more robust, being harder to get overfitted"},{"metadata":{},"cell_type":"markdown","source":"# Versions Log\n\n* **V1 to V9**: Tests\n* **V10**: First version\n* **V11, V12**: Some corrections + Addition of Ensemble Models ROC curves"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}