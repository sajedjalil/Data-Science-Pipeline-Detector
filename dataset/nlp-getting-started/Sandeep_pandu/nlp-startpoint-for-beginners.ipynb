{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Basics to start with NLP:\n--\nThis competition is a great start to beginners in NLP. So I hope following are the basic things to start and then build on it in later stages.\n1. Cleaning\n2. Extracting Metafeatures\n3. Extracting text features\n4. Building Baseline Model"},{"metadata":{},"cell_type":"markdown","source":"Cleaning:\n--\nIn NLP, cleaning the text huge role before you start building any model. If you have cleaner data, you can use word embedding like Word2vec, Glove, Fasttext. I prefer, beginner should start from basic text data preprocessing.\n1. A helper function is provided to make lower case and convert some casual words to formal way and remove other than text like symbols, punctuations, numbers.\n\nNote: \nThe idea is to convert the input text into same casing format so that 'one1', 'ONE' and 'One' are treated the same way. This is more helpful for text featurization techniques like frequency, tfidf as it helps to combine the same words together thereby reducing the duplication and get correct counts / tfidf values. Remember this depends on the task, if you are working Parts of Speech tag(POS tag) then these Upper case plays a huge role.\n\n2. Removal of stopwords,This is to remove common words like of, are, is which does not add information.\n\nNote:  In cases like POS tagging, we should not remove them as provide very valuable information about the POS.\n\n3. Removal of frequent words\n\n4. Removal of rare words\n\n5. Lemmatization, to convert the words into correct root form. Ex. 'running' to 'run'. (Usually stemming can also be done to do that but it may give the root which may not be correct word. Ex. Some words looses the end letter which is actually not intended). So use lemmatization\n\nNote:\nHere lemmatization, if you just perform the lemmatization with correct parts of speech like verb, noun,.. Some words can be bring down to its root by only based on the its verb or noun.. This is very important.\n"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport re\nimport nltk\nimport spacy\nimport string\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# A helper function to clean the data, you can lines based on the dataset. Note that this dataset\n# is lot cleaner than usual NLP problems. In usual NLP, you need to keenly look at the data and then clean.\n# Remember, the uncleaned text is very crucial for feature engineering(Metafeatures). So keep them in one column\ndef clean_text(text):\n    import re\n    text = text.lower()\n    text = re.sub(r\"i'm\", \"i am\", text)\n    text = re.sub(r\"you'll\", \"you will\", text)\n    text = re.sub(r\"i'll\", \"i will\", text)\n    text = re.sub(r\"she'll\", \"she will\", text)\n    text = re.sub(r\"he'll\", \"he will\", text)\n    text = re.sub(r\"he's\", \"he is\", text)\n    text = re.sub(r\"she's\", \"she is\", text)\n    text = re.sub(r\"that's\", \"that is\", text)\n    text = re.sub(r\"what's\", \"what is\", text)\n    text = re.sub(r\"where's\", \"where is\", text)\n    text = re.sub(r\"there's\", \"there is\", text)\n    text = re.sub(r\"here's\", \"here is\", text)\n    text = re.sub(r\"who's\", \"who is\", text)\n    text = re.sub(r\"how's\", \"how is\", text)\n    text = re.sub(r\"\\'ll\", \" will\", text)\n    text = re.sub(r\"\\'ve\", \" have\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    text = re.sub(r\"\\'d\", \" would\", text)\n    text = re.sub(r\"can't\", \"cannot\", text)\n    text = re.sub(r\"won't\", \"will not\", text)\n    text = re.sub(r\"don't\", \"do not\", text)\n    text = re.sub(r\"shouldn't\", \"should not\", text)\n    text = re.sub(r\"n't\", \" not\", text)\n    text = re.sub(r\"[^a-z]\", \" \", text) # This removes anything other than lower case letters(very imp)\n    text = re.sub(r\"   \", \" \", text) # Remove any extra spaces\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['clean_text'] = train_df['text'].apply(clean_text)\ntest_df['clean_text'] = test_df['text'].apply(clean_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removal of punctuations(we had already done this in the clean text, but this way also faster to compute)\nPUNCT_TO_REMOVE = string.punctuation\ndef remove_punctuation(text):\n    \"\"\"custom function to remove the punctuation\"\"\"\n    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n\ntrain_df[\"clean_text\"] = train_df[\"clean_text\"].apply(lambda text: remove_punctuation(text))\ntest_df[\"clean_text\"] = test_df[\"clean_text\"].apply(lambda text: remove_punctuation(text))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removal of stopwords\nfrom nltk.corpus import stopwords\nSTOPWORDS = set(stopwords.words('english'))\ndef remove_stopwords(text):\n    \"\"\"custom function to remove the stopwords\"\"\"\n    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n\ntrain_df[\"clean_text\"] = train_df[\"clean_text\"].apply(lambda text: remove_stopwords(text))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df[\"clean_text\"] = test_df[\"clean_text\"].apply(lambda text: remove_stopwords(text))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removal of frequent words(here it seems some important words like fire, so I am removing only top 10.)\n# Note: Always print yourself the frequent words and then decide it on how many to remove.\ncnt = Counter()\nfor text in train_df[\"clean_text\"].values:\n    for word in text.split():\n        cnt[word] += 1\nFREQWORDS = set([w for (w, wc) in cnt.most_common(5)])\ndef remove_freqwords(text):\n    \"\"\"custom function to remove the frequent words\"\"\"\n    return \" \".join([word for word in str(text).split() if word not in FREQWORDS])\n\ntrain_df[\"clean_text\"] = train_df[\"clean_text\"].apply(lambda text: remove_freqwords(text))\n\n\n# Removal of frequent words on test data\n\ncnt = Counter()\nfor text in test_df[\"clean_text\"].values:\n    for word in text.split():\n        cnt[word] += 1\ncnt.most_common(5)\nFREQWORDS = set([w for (w, wc) in cnt.most_common(5)])\ntest_df[\"clean_text\"] = test_df[\"clean_text\"].apply(lambda text: remove_freqwords(text))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removal of rare words (this is also important). But I have not tried any efficient way of implementing it.\n# Usually people remove last 10 or 20 words. But I usually do remove all the words with frequency == 1.\n# If any one better idea for this, please help me. Sharing is caring:)\n\n# # Removing rarewords which has frequency one\n# freq = pd.Series(' '.join(train_df['clean_text']).split()).value_counts()\n# rare_words = freq[freq <= 1]\n# rare_words = list(rare_words.index)\n\n# # Takes too much\n# for i in range(len(train_df)):\n#     print(i)\n#     tokens = train_df['clean_text'][i].split()\n#     output = []\n#     for token in tokens:\n#         if token not in rare_words:\n#             output += [token]\n#     train_df['clean_text'][i] = output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lemmatizing the words using WordNet\nfrom nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\nwordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\ndef lemmatize_words(text):\n    pos_tagged_text = nltk.pos_tag(text.split())\n    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n\ntrain_df[\"clean_text\"] = train_df[\"clean_text\"].apply(lambda text: lemmatize_words(text))\ntest_df[\"clean_text\"] = test_df[\"clean_text\"].apply(lambda text: lemmatize_words(text))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seems Data is cleaned. Please store this in a .csv before you contiue. This way,you can keep clean code and easy to reuse. I prefer to create a template seperately for NLP data cleaning."},{"metadata":{},"cell_type":"markdown","source":"Extracting Metafeatures:\n--\nIn most of the NLP tasks, some feature engineering techniques are common which can improve your model performance to a great extent(even for this dataset). I have intially built baseline without these and scored around 77% and then it increased to around 80%.(this is close to AUTO ML Bench mark).\n1. Average length of the words\n2. Number of stopwords\n3. Length of the sentence. We can extend this list to may, this depends on your creativity. Usually I prefer to spend some time to look at data for each class and then start creating them. Following are few common features."},{"metadata":{"trusted":true},"cell_type":"code","source":"## Number of words in the text ##\ntrain_df[\"num_words\"] = train_df[\"text\"].apply(lambda x: len(str(x).split()))\ntest_df[\"num_words\"] = test_df[\"text\"].apply(lambda x: len(str(x).split()))\n\n## Number of unique words in the text ##\ntrain_df[\"num_unique_words\"] = train_df[\"text\"].apply(lambda x: len(set(str(x).split())))\ntest_df[\"num_unique_words\"] = test_df[\"text\"].apply(lambda x: len(set(str(x).split())))\n\n## Number of characters in the text ##\ntrain_df[\"num_chars\"] = train_df[\"text\"].apply(lambda x: len(str(x)))\ntest_df[\"num_chars\"] = test_df[\"text\"].apply(lambda x: len(str(x)))\n\n## Number of stopwords in the text ##\n\neng_stopwords = set(stopwords.words(\"english\"))\ntrain_df[\"num_stopwords\"]=train_df[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\ntest_df[\"num_stopwords\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n\n## Number of punctuations in the text ##\ntrain_df[\"num_punctuations\"] =train_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\ntest_df[\"num_punctuations\"] =test_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n\n## Number of title case words in the text ##\ntrain_df[\"num_words_upper\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\ntest_df[\"num_words_upper\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n\n## Number of title case words in the text ##\ntrain_df[\"num_words_title\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\ntest_df[\"num_words_title\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n\n## Average length of the words in the text ##\ntrain_df[\"mean_word_len\"] = train_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\ntest_df[\"mean_word_len\"] = test_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n\ntrain_df['hastags'] = train_df['text'].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))\ntest_df['hastags'] = test_df['text'].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.violinplot(x='target', y='mean_word_len', data=train_df)\nplt.xlabel('Event occurred or not', fontsize=12)\nplt.ylabel('Mean word length in text', fontsize=12)\nplt.title(\"Number of words by each class\", fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It can be seen that, for event = 0 less mean word length than event = 1. This is good, it may help our classifier. Similarly check the other features as well."},{"metadata":{},"cell_type":"markdown","source":"Base Model:\n--"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn import ensemble, metrics, model_selection, naive_bayes\nimport xgboost as xgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_to_drop = ['id', 'text','keyword','location','clean_text']\ntrain_X = train_df.drop(cols_to_drop+['target'], axis=1)\ntrain_y = train_df['target']\ntest_X = test_df.drop(cols_to_drop, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note: I suggest you to just find a tree based model like XGB, Random forest on the extracted features to get the feature importance. Observe them. I am leaving it you. It will be great if you do for yourself. This helps you to understand the data and can know what is important and what not."},{"metadata":{},"cell_type":"markdown","source":"Text Based Features:\n--"},{"metadata":{"trusted":true},"cell_type":"code","source":"### Fit transform the tfidf vectorizer ###\ntfidf_vec = TfidfVectorizer(stop_words='english', ngram_range=(1,2))\nfull_tfidf = tfidf_vec.fit_transform(train_df['clean_text'].values.tolist() + test_df['clean_text'].values.tolist())\ntrain_tfidf = tfidf_vec.transform(train_df['clean_text'].values.tolist())\ntest_tfidf = tfidf_vec.transform(test_df['clean_text'].values.tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def runMNB(train_X, train_y, test_X, test_y, test_X2):\n    model = naive_bayes.MultinomialNB()\n    #model = linear_model.RidgeClassifier()\n    model.fit(train_X, train_y)\n    pred_test_y = model.predict(test_X)\n    pred_test_y2 = model.predict(test_X2)\n    return pred_test_y, pred_test_y2, model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_scores = []\npred_full_test = 0\npred_train = np.zeros([train_df.shape[0]])\nkf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\nfor dev_index, val_index in kf.split(train_X):\n    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n    dev_y, val_y = train_y[dev_index], train_y[val_index]\n    pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n    pred_full_test = pred_full_test + pred_test_y\n    pred_train[val_index] = pred_val_y\n    cv_scores.append(metrics.f1_score(val_y,pred_val_y))\nprint(cv_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add the SVD on tfidf to add some more information(You can also try NMF)\nn_comp = 20\nsvd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\nsvd_obj.fit(full_tfidf)\ntrain_svd = pd.DataFrame(svd_obj.transform(train_tfidf))\ntest_svd = pd.DataFrame(svd_obj.transform(test_tfidf))\n    \ntrain_svd.columns = ['svd_word_'+str(i) for i in range(n_comp)]\ntest_svd.columns = ['svd_word_'+str(i) for i in range(n_comp)]\ntrain_df = pd.concat([train_df, train_svd], axis=1)\ntest_df = pd.concat([test_df, test_svd], axis=1)\ndel full_tfidf, train_tfidf, test_tfidf, train_svd, test_svd","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Char TFIDF(different n_gram range):\n--\nTry tfidf at different character levels and add this to the final dataset. This is may add some additional information. "},{"metadata":{"trusted":true},"cell_type":"code","source":"### Fit transform the tfidf vectorizer ###\ntfidf_vec = TfidfVectorizer(ngram_range=(1,7), analyzer='char')\nfull_tfidf = tfidf_vec.fit_transform(train_df['clean_text'].values.tolist() + test_df['clean_text'].values.tolist())\ntrain_tfidf = tfidf_vec.transform(train_df['clean_text'].values.tolist())\ntest_tfidf = tfidf_vec.transform(test_df['clean_text'].values.tolist())\n\ncv_scores = []\npred_full_test = 0\npred_train = np.zeros([train_df.shape[0]])\nkf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\nfor dev_index, val_index in kf.split(train_X):\n    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n    dev_y, val_y = train_y[dev_index], train_y[val_index]\n    pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n    pred_full_test = pred_full_test + pred_test_y\n    pred_train[val_index] = pred_val_y\n    cv_scores.append(metrics.f1_score(val_y,pred_val_y))\nprint(cv_scores)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# SVD on character tfidf\nn_comp = 20\nsvd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\nsvd_obj.fit(full_tfidf)\ntrain_svd = pd.DataFrame(svd_obj.transform(train_tfidf))\ntest_svd = pd.DataFrame(svd_obj.transform(test_tfidf))\n    \ntrain_svd.columns = ['svd_char_'+str(i) for i in range(n_comp)]\ntest_svd.columns = ['svd_char_'+str(i) for i in range(n_comp)]\ntrain_df = pd.concat([train_df, train_svd], axis=1)\ntest_df = pd.concat([test_df, test_svd], axis=1)\ndel full_tfidf, train_tfidf, test_tfidf, train_svd, test_svd","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Baseline with all the above data:\n--"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_to_drop = ['id', 'text','keyword','location','clean_text']\ntrain_X = train_df.drop(cols_to_drop+['target'], axis=1)\ntrain_y = train_df['target']\ntest_X = test_df.drop(cols_to_drop, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn import linear_model\nX_train,X_val,y_train,y_val  = train_test_split(train_X,train_y,test_size = 0.2)\nmodel = linear_model.RidgeClassifier()\nmodel.fit(train_X, train_y)\npred_test_y = model.predict(test_X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = pd.read_csv(\"sample_submission.csv\")\nsample_submission[\"target\"] = pred_test_y\nsample_submission.to_csv(\"submission_baseline.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This gave me score close to 80% in the leaderboard. Great a way to start and long road ahead."},{"metadata":{},"cell_type":"markdown","source":"Next step:\n--\n1. Try different models like two boosting algorithms like XGB, LGBM.. , one tree based like RandomForest, linear classifier like Naive Bayes, Ridge. Note these will be helpful at the end of the competitions. This helps you doing ensembling which may wins you alot of competitions. But Don't waste initially to hyper-parameter tuning and ensembling. Do at the end of the competitions.\n2. Word vectors, as a beginner I strongly suggest to read on word embeddings. They are lot important for building stronger models like neural networks. \n3. Apply Neural networks like RNN especially LSTM and GRU(This is very basic in NN and can be done in minutes using keras.Sorry I wont be able to provide code. I do not have enough time)\n4. Read about attention, transformer and these are currently trending in applications. "},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}