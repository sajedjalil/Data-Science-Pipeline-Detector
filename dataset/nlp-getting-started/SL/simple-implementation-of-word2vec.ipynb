{"cells":[{"metadata":{},"cell_type":"markdown","source":"<font size =5>**About this Kernel ** </font>\n\nWriting this Kernel as a sequel to my intial notebook published [here](https://www.kaggle.com/slatawa/tfidf-implementation-to-get-80-accuracy). If you are new to NLP I suggest you check that out first- in this notebook I have demonstrated basic concepts of NLP and achieved around 80% accuracy with basic cleaning and TFIDF.\n\nComing to this notebook , I wanted to explore word embeddings using Word2Vec and then apply it for classifcation. In my search over last few days I did not come across any notebook which could demonstrate a simple implementaion of word2vec , so decided to write one which can be used as starting point for any one trying to Dabble with Embeddings. Without further wait let's start with the dataset , another article which I found very usefull if you want to have a quick [read](https://towardsdatascience.com/an-implementation-guide-to-word2vec-using-numpy-and-google-sheets-13445eebd281 )\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport warnings \n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nwarnings.filterwarnings(\"ignore\")\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. Load datasets"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df_train= pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ndf_test=pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2. Let's take a look through the data"},{"metadata":{"trusted":true},"cell_type":"code","source":" print(df_train.head(5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_train.info())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3. Clean the data\n\nAs first step in cleaning - let us replace some commonly occuring shorthands "},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(text):\n    import re\n    text = text.lower()\n    text = re.sub(r\"i'm\", \"i am\", text)\n    text = re.sub(r\"you'll\", \"you will\", text)\n    text = re.sub(r\"i'll\", \"i will\", text)\n    text = re.sub(r\"she'll\", \"she will\", text)\n    text = re.sub(r\"he'll\", \"he will\", text)\n    text = re.sub(r\"he's\", \"he is\", text)\n    text = re.sub(r\"she's\", \"she is\", text)\n    text = re.sub(r\"that's\", \"that is\", text)\n    text = re.sub(r\"what's\", \"what is\", text)\n    text = re.sub(r\"where's\", \"where is\", text)\n    text = re.sub(r\"there's\", \"there is\", text)\n    text = re.sub(r\"here's\", \"here is\", text)\n    text = re.sub(r\"who's\", \"who is\", text)\n    text = re.sub(r\"how's\", \"how is\", text)\n    text = re.sub(r\"\\'ll\", \" will\", text)\n    text = re.sub(r\"\\'ve\", \" have\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    text = re.sub(r\"\\'d\", \" would\", text)\n    text = re.sub(r\"can't\", \"cannot\", text)\n    text = re.sub(r\"won't\", \"will not\", text)\n    text = re.sub(r\"don't\", \"do not\", text)\n    text = re.sub(r\"shouldn't\", \"should not\", text)\n    text = re.sub(r\"n't\", \" not\", text)\n    text = re.sub(r\"   \", \" \", text) # Remove any extra spaces\n    return text\n\n\ndf_train['clean_text'] = df_train['text'].apply(clean_text)\ndf_test['clean_text'] = df_test['text'].apply(clean_text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the next step we are going to do some further massaging which would make Job of Prediction Algorithm easy\n\n* Let us remove any characters other then alphabets\n* Convert all dictionary to lower case - for consistency \n* Lemmatize - More details on Stemming and Lemmatization [here](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def massage_text(text):\n    import re\n    from nltk.corpus import stopwords\n    ## remove anything other then characters and put everything in lowercase\n    tweet = re.sub(\"[^a-zA-Z]\", ' ', text)\n    tweet = tweet.lower()\n    tweet = tweet.split()\n\n    from nltk.stem import WordNetLemmatizer\n    lem = WordNetLemmatizer()\n    tweet = [lem.lemmatize(word) for word in tweet\n             if word not in set(stopwords.words('english'))]\n    tweet = ' '.join(tweet)\n    return tweet\n    print('--here goes nothing')\n    print(text)\n    print(tweet)\n\ndf_train['clean_text'] = df_train['text'].apply(massage_text)\ndf_test['clean_text'] = df_test['text'].apply(massage_text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a look at the data now "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.iloc[0:10][['text','clean_text']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's tokenize the clean text column now "},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.tokenize import word_tokenize\n\ndf_train['tokens']=df_train['clean_text'].apply(lambda x: word_tokenize(x))\ndf_test['tokens'] = df_test['clean_text'].apply(lambda x: word_tokenize(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4. Apply Word2Vector to get word embeddings (convert your words into vectors) - Till now what we have done is basic standard cleaning now let's start with the actual Embeddings and then training. Word2Vector needs a way to map words to Vectors for this either we can use pretrained models from Glove, for this task  have trained the mappings from the tweets text itself. Wanted to try score while using a pretrained model like Glove but now at the moment , would look to add that in the future versions. If you are interested to explore that more , check [this](http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/) "},{"metadata":{},"cell_type":"markdown","source":"4.1 Below we create a list corpus which we would be using to train word2vec mappings "},{"metadata":{"trusted":true},"cell_type":"code","source":"import gensim\ndef fn_pre_process_data(doc):\n    for rec in doc:\n        yield gensim.utils.simple_preprocess(rec)\n\ncorpus = list(fn_pre_process_data(df_train['clean_text']))\ncorpus += list(fn_pre_process_data(df_test['clean_text']))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's inititate the embedding model , we will come back to the passed arguments later"},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.models import Word2Vec\n\nprint('initiated ...')\nwv_model = Word2Vec(corpus,size=150,window=3,min_count=2)\nwv_model.train(corpus,total_examples=len(corpus),epochs=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have the embedding mnodel ready, let's convert the train and text tokens now \n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_word_embeddings(token_list,vector,k=150):\n    if len(token_list) < 1:\n        return np.zeros(k)\n    else:\n        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in token_list] \n    \n    sum = np.sum(vectorized,axis=0)\n    ## return the average\n    return sum/len(vectorized)        \ndef get_embeddings(tokens,vector):\n        embeddings = tokens.apply(lambda x: get_word_embeddings(x, wv_model))\n        return list(embeddings)\n\ntrain_embeddings = get_embeddings(df_train['tokens'],wv_model)\ntest_embeddings = get_embeddings(df_test['tokens'],wv_model)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"5 Start applying models - Now that we have word embeddings ready let's start applying Learning Models to it ."},{"metadata":{},"cell_type":"markdown","source":"We can start by applying Logisitic Regression to get a baseline , we would be using Gridsearch to get the best combination of parameters for LR"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\n\nlr_model = LogisticRegression()\ngrid_values ={'penalty':['l1','l2'],'C':[0.0001,0.001,0.01,0.1,1,10]}\ngrid_search_model = GridSearchCV(lr_model,param_grid=grid_values,cv=3)\ngrid_search_model.fit(train_embeddings,df_train['target'])\nprint(grid_search_model.best_estimator_)\nprint(grid_search_model.best_score_)\nprint(grid_search_model.best_params_)\n\npredict_lr = grid_search_model.predict(test_embeddings)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_df = pd.DataFrame()\npredict_df['id'] = df_test['id']\npredict_df['target'] = predict_lr\npredict_df.to_csv('sample_submission_100.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"5.2 Let's try SVM "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\n\nsvc_model = SVC()\ngrid_values ={'C':[0.0001,0.001,0.01,0.1,1,10]}\n\ngrid_search_model = GridSearchCV(svc_model,param_grid=grid_values,cv=3)\ngrid_search_model.fit(train_embeddings,df_train['target'])\nprint(grid_search_model.best_estimator_)\nprint(grid_search_model.best_score_)\nprint(grid_search_model.best_params_)\n\npredict_svc = grid_search_model.predict(test_embeddings)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"5.3 Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nrf_model = RandomForestClassifier()\ngrid_values ={'n_estimators':[10,50,100,150,200,300,400]}\n\ngrid_search_model = GridSearchCV(rf_model,param_grid=grid_values,cv=3)\ngrid_search_model.fit(train_embeddings,df_train['target'])\nprint(grid_search_model.best_estimator_)\nprint(grid_search_model.best_score_)\nprint(grid_search_model.best_params_)\n\npredict_rf = grid_search_model.predict(test_embeddings)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size =5> **Final Thoughts ** </font>\n\n\nI had (imagined) word2vec giving better accuracy scores then what I achieved with TFIDF [here](https://www.kaggle.com/slatawa/tfidf-implementation-to-get-80-accuracy) but I scored a mere 71.xx with the LR submissions,a bit of tweaking might get it back on track, but this should give you a good starting point in case you want to play with word-embeddings. \n\n**Pleae UPvote if you found this usefull and do leave comments if you think something can be improved here or you have any question regarding the notebook/code.**"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}