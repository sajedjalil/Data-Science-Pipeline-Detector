{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n## **0. ã¯ã˜ã‚ã«**\n\næ—¥æœ¬èªã®å‡¦ç†ã‚‚ã¾ã˜ãˆãªãŒã‚‰ã€ã‚³ãƒ³ãƒšãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã„ã¤ã¤è‡ªåˆ†ç”¨ã«NLPã‚’å°‘ã—ãšã¤ã¾ã¨ã‚ã¦ã„ãã¾ã™ã€‚<br>\nã‚¼ãƒ­ã‹ã‚‰ä½œã‚‹Deep Learningâ€•è‡ªç„¶è¨€èªå‡¦ç†ç·¨ã§ç†è§£ã—ãŸã“ã¨ã‚‚ã¾ã¨ã‚ã¦ã„ãã¾ã™ã€‚<br>\nRNNã‹ã‚‰ã¾ã¨ã‚ã¦ã„ãã¾ã™ãŒã€å°‘ã—ãšã¤æ›¸ãè¶³ã—ã¦ã„ãã¾ã™ã€‚<br>","metadata":{}},{"cell_type":"markdown","source":"#### 0.1ã€€æœ€æ–°ç‰ˆã®æ—¥æœ¬èªå¯¾å¿œtransformer ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n\nç¾åœ¨ã®transformersã¯mecabã§ã¯ãªãfugashiã‚’ä½¿ã£ã¦ã„ã¾ã™ã€‚ï¼ˆfugashiã¯MeCabã®ãƒ©ãƒƒãƒ‘ãƒ¼ï¼‰<br>\nä»¥ä¸‹ã‚’å®Ÿè¡Œã™ã‚‹ã ã‘ã§ã€fugashiã‚‚è¾æ›¸ã‚‚å…¨éƒ¨å…¥ã‚Šã¾ã™ã€‚<br>\næ˜”ã¯fugashiã‚’åˆ¥é€”installã—ã¦ã„ãŸã€‚<br>\n\nhttps://qiita.com/m__k/items/863013dbe847dc613844","metadata":{}},{"cell_type":"code","source":"!pip install transformers[ja]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nto\nimport gensim.downloader\nimport transformers\nfrom wordcloud import WordCloud\n\ntrain = pd.read_csv('../input/nlp-getting-started/train.csv',dtype={'id': np.int16, 'target': np.int8})","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-03T12:50:54.937986Z","iopub.execute_input":"2021-11-03T12:50:54.938603Z","iopub.status.idle":"2021-11-03T12:50:54.969843Z","shell.execute_reply.started":"2021-11-03T12:50:54.938562Z","shell.execute_reply":"2021-11-03T12:50:54.969075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Table of Contents:<br>\n1. [refernce](https://www.kaggle.com/tfukuda675/lnp-beginner#1.-reference)<br>\n2. [Unique process of NLP](https://www.kaggle.com/tfukuda675/lnp-beginner#2.-Unique-process-of-NLP)<br>\n 2.1 Tokenization\n 2.2 Padding\n3. [RNN](https://www.kaggle.com/tfukuda675/lnp-beginner#3.-RNN)<br>\n 3.1 simple code<br>\n4. LSTM<br>\n 4.1 simple code<br>\n5. Transformer<br>\n 5.1 base line<br>","metadata":{}},{"cell_type":"markdown","source":"## **1. reference**\n\n#### This kernel includes codes and ideas from kernels below.\n* [NLP with Disaster Tweets - EDA, Cleaning and BERT](https://www.kaggle.com/tfukuda675/nlp-with-disaster-tweets-eda-cleaning-and-bert/edit/run/78390965) by [gunesevitan](https://www.kaggle.com/gunesevitan)\n* [n-Depth Guide ğŸ“™ to Google's BERT](https://www.kaggle.com/ratan123/in-depth-guide-to-google-s-bert) by [ratan rohith]\n(https://www.kaggle.com/ratan123)\n* [Introduction to Japanese spaCy/GINZA [æ—¥æœ¬èª/Eng]](https://www.kaggle.com/marutama/introduction-to-japanese-spacy-ginza-eng) by [NIWASHI](https://www.kaggle.com/marutama)\n* <a href=\"https://jp.mathworks.com/content/dam/mathworks/mathworks-dot-com/company/events/webinar-cta/2514077_JP_2018-09-07_Deep_Learning_LSTM_1.pdf#page=9\">Structure of RNN</a>\n* [æ·±å±¤å­¦ç¿’ï¼ã‚¼ãƒ­ã‹ã‚‰ä½œã‚‹Deep Learningï¼’ã€€ç¬¬ï¼•ç« ãƒ¡ãƒ¢](https://qiita.com/jun40vn/items/35f6f0d26f9e58f01e4e)\n* [æ·±å±¤å­¦ç¿’ï¼ã‚¼ãƒ­ã‹ã‚‰ä½œã‚‹Deep Learningï¼’ã€€ç¬¬ï¼–ç« ãƒ¡ãƒ¢](https://qiita.com/jun40vn/items/e690dfe80faa6512049f)\n* [è‡ªç„¶è¨€èªå‡¦ç†ã®å¿…é ˆçŸ¥è­˜ Transformer ã‚’å¾¹åº•è§£èª¬ï¼](https://deepsquare.jp/2020/07/transformer/)\n* [LSTMãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ¦‚è¦](https://qiita.com/KojiOhki/items/89cd7b69a8a6239d67ca)\n* [colahæ°ã®è§£èª¬ Understanding LSTM](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n* [ç”»åƒç”¨Transformerã‚’åˆ©ç”¨ã—ã¦è¡›æ˜Ÿç”»åƒã®åˆ†é¡æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã™ã‚‹](https://sorabatake.jp/20454/) @ç©ºç•‘\n* [PyTorchã§è‡ªç„¶è¨€èªå‡¦ç†ã§ã‚ˆãä½¿ç”¨ã•ã‚Œã‚‹Transformerã‚’å°‘ãªã„ã‚³ãƒ¼ãƒ‰ã§ä½¿ç”¨ã—ã¦ã¿ã‚‹](https://www.yurui-deep-learning.com/2021/01/07/pytorch-transformer/)\n* [LSTMã«ã‚ˆã‚‹ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®äºˆæ¸¬ã¨åˆ†é¡](https://jp.mathworks.com/content/dam/mathworks/mathworks-dot-com/company/events/webinar-cta/2514077_JP_2018-09-07_Deep_Learning_LSTM_1.pdf) @mathworks\n* [RNNã®å•é¡Œç‚¹](https://www.anarchive-beta.com/entry/2021/01/07/180000)\n* [ä½œã£ã¦ç†è§£ã™ã‚‹ Transformer / Attention](https://qiita.com/halhorn/items/c91497522be27bde17ce)\n* [AIç•Œã‚’å¸­å·»ã™ã‚‹ã€ŒTransformerã€ã‚’ã‚†ã£ãã‚Šè§£èª¬](https://zenn.dev/attentionplease/articles/1a01887b783494)\n* [è«–æ–‡è§£èª¬ Attention Is All You Need (Transformer)](https://deeplearning.hatenablog.com/entry/transformer)\n* [è‡ªç„¶è¨€èªå‡¦ç†ã®å·¨ç£ã€ŒTransformerã€ã®Self-Attention Layerç´¹ä»‹](https://medium.com/lsc-psd/%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86%E3%81%AE%E5%B7%A8%E7%8D%A3-transformer-%E3%81%AEself-attention-layer%E7%B4%B9%E4%BB%8B-a04dc999efc5)\n* [Transformers Explained Visually (Part 3): Multi-head Attention, deep dive](https://towardsdatascience.com/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853)\n* [ã¯ã˜ã‚ã¦ã®è‡ªç„¶è¨€èªå‡¦ç†](https://www.ogis-ri.co.jp/otc/hiroba/technical/similar-document-search/)\n\n<br><br>\n#### å„Layerã®èª¬æ˜\n* [positional encoding](https://qiita.com/halhorn/items/c91497522be27bde17ce#positional-encoding)\n* [Feed Forward Net](https://www.hellocybernetics.tech/entry/2016/05/22/014656)\n  éš ã‚Œå±¤1å±¤ã®å…¨çµåˆNN\n* [Batch Normalize](https://qiita.com/t-tkd3a/items/14950dbf55f7a3095600#%E5%85%A8%E7%B5%90%E5%90%88nn-%E3%81%AE-batch-normalization)\n* [Layer Normalization](https://data-analytics.fun/2020/07/16/understanding-layer-normalization/)\n* [gMLP](https://deepsquare.jp/2021/05/gmlp/)\n\n<br><br>\n#### ã‚ˆãå¿˜ã‚Œã‚‹å˜èªã¾ã¨ã‚\n* [ãƒãƒ«ãƒ ](https://manabitimes.jp/math/1269)\n* [ã‚¢ãƒãƒ€ãƒ¼ãƒ«ç©](https://python.atelierkobato.com/hadamard/)<br>\n<img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/0866ce505a5888cd4db8e6ba2b710fe8cd5a6578\" width=\"500\">\n* [ã‚¢ãƒ•ã‚£ãƒ³å¤‰æ›](https://qiita.com/koshian2/items/c133e2e10c261b8646bf)<br>\nå…¨çµåˆå±¤ã«ã‚ˆã‚‹å¤‰æ›ã®äº‹ã€‚å¹¾ä½•å­¦ã®åˆ†é‡ã§ã¯ã‚¢ãƒ•ã‚£ãƒ³å¤‰æ›ã«ç›¸å½“ã™ã‚‹ã®ã§ã€ã“ã“ã§ã¯ã‚¢ãƒ•ã‚£ãƒ³å¤‰æ›ã¨èª­ã‚“ã§ã„ã‚‹ã€‚<br>\n* tanhã¨sigmoidã«ã¤ã„ã¦<br>\ntanhã¯ã€ä½•ã‹ã—ã‚‰ã®æƒ…å ±ã®å‡ºåŠ›ã«ã¤ã„ã¦ã„ã‚‹ã€‚æƒ…å ±ã«å¯¾ã™ã‚‹å¼·å¼±ã¨è§£é‡ˆã§ãã‚‹ã€‚<br>\nsigmoidã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚’ã©ã‚Œã ã‘é€šã™ã‹ã¨ã„ã†å‰²åˆã€‚<br>\n* [Batch Normalization](https://yaakublog.com/batch-normalization)<br>\n* [è©•ä¾¡é–¢æ•°ã¾ã¨ã‚](https://amateur-engineer-blog.com/machine-learning-metrics/)<br>\n","metadata":{}},{"cell_type":"markdown","source":"## **2. Unique process of NLP**\n\nRNN, LSTM, transformer cannot treat texts directory.<br>\nAt first, we need to run tokernize. This process transform words to ids.<br>\nNext, run padding process. this process add special id to align the length to the longest sentence.<br>\nThose process use language model, BERT etc,.<br>\n<br>\næ–‡å­—åˆ—ã‚’ãã®ã¾ã¾å‡¦ç†ã§ããªã„ç‚ºã€ã¾ãšãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã‚’è¡Œã„ã¾ã™ã€‚<br>\nãã®å¾Œã€å„ã‚»ãƒ³ãƒ†ãƒ³ã‚¹ã®é•·ã•ã‚’æƒãˆã‚‹ç‚ºã€paddingã‚’è¡Œã„ã¾ã™ã€‚<br>\nã“ã‚Œã‚‰ã®ãƒ—ãƒ­ã‚»ã‚¹ã§ã¯ã€BERTã§çŸ¥ã‚‰ã‚Œã‚‹ã‚ˆã†ãªãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã„ã¾ã™ã€‚<br>\nå½“ç„¶ãƒ¢ãƒ‡ãƒ«ã®å½±éŸ¿ã‚‚å¤§ãã„ã§ã™ã€‚<br>\nãƒ¢ãƒ‡ãƒ«ã«ã¤ã„ã¦ã¯åˆ¥é€”ã¾ã¨ã‚ã¾ã™ã€‚<br>\n\n#### 2.1 Tokenization\næ–‡å­—åˆ—ã‚’ãã®ã¾ã¾å­¦ç¿’ã™ã‚‹äº‹ã¯ã§ãã¾ã›ã‚“ã€‚<br>\nå­¦ç¿’ã§ãã‚‹ã‚ˆã†ã«ã€æ•°å€¤ã«ç½®ãæ›ãˆã‚‹å‡¦ç†ãŒå¿…è¦ã§ã™ã€‚ãã‚Œã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–(Tokenize)ã¨è¨€ã„ã¾ã™ã€‚<br>\nä»¥ä¸‹ã®ç”»åƒã®ä¸Š2ã¤ãŒãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã«ç›¸å½“ã—ã¾ã™ã€‚<br>\n<img src=\"https://camo.qiitausercontent.com/51b2ff47f2bb952f38a63b02d056ebfc7dea097d/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f36323435302f35323261393063362d613732352d333731362d346161382d6637626563396430396262622e706e67\" width=600>\n<br>\n\n#### 2.2 Tokenization result of 1st text\n\nShow BERT tokenizer result with transformer bert model.<br>\nThe 'input_ids' result is tokenization text.<br>\n<br>\næ—¥æœ¬èªã‚’æ‰±ã†å ´åˆã¯ã€æ—¥æœ¬èªã®tokenizerã‚’åˆ©ç”¨ã™ã‚‹ã€‚<br>","metadata":{}},{"cell_type":"code","source":"tokenizer = transformers.AutoTokenizer.from_pretrained('bert-base-cased')\n##== whwn you want to treat Japanese character,\n# !pip install transformers[ja]\n#tokenizer = transformers.AutoTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\ntokenizer(train.text[0])","metadata":{"execution":{"iopub.status.busy":"2021-11-02T09:57:52.423605Z","iopub.execute_input":"2021-11-02T09:57:52.423904Z","iopub.status.idle":"2021-11-02T09:57:58.32201Z","shell.execute_reply.started":"2021-11-02T09:57:52.423865Z","shell.execute_reply":"2021-11-02T09:57:58.321204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.2 Padding\nå­¦ç¿’æ™‚ã«ã¯å…¥åŠ›ã¯å›ºå®šé•·ãŒæœŸå¾…ã•ã‚Œã¦ã„ã¾ã™ã€‚<br>\nãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã—ãŸã‚‚ã®ã®é•·ã•ã‚’æƒãˆã‚‹ãŸã‚ã€paddingã§\"0\"ãªã©ã‚’è¿½åŠ ã—ã¾ã™ã€‚<br>\n<br>\n\npaddingå‰ã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿\n```\nraw_inputs = [\n    [711, 632, 71],\n    [73, 8, 3215, 55, 927],\n    [83, 91, 1, 645, 1253, 927],\n]\n```\npaddingå¾Œ\n```\n## Result\n[[ 711  632   71    0    0    0]\n [  73    8 3215   55  927    0]\n [  83   91    1  645 1253  927]]\n```\n<br>","metadata":{}},{"cell_type":"markdown","source":"## **3. RNN** \n\n#### 3.1 RNN one layer\n\nUnderstand rnn structure via python code.<br>\n\n#### å¼ã®èª¬æ˜\n##### **forwarã«ã¤ã„ã¦**<br>\n<img src=\"https://camo.qiitausercontent.com/d0046c189cf724199009651c48433d0097df591a/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3230393730352f34303634653037662d623962622d303432392d616339322d6338366233313837616164622e706e67\" width=600>\n<br><br>\n\n##### **backwardã«ã¤ã„ã¦**<br>\n\n<img src=\"https://camo.qiitausercontent.com/8942bf9b069aece2d1c19a676a089414643a02c8/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3230393730352f31396133383030642d336236342d393133652d396165342d3731616664653534356138652e706e67\" width=600>\n\n<br><br>\n#### **ã‚³ãƒ¼ãƒ‰ã®èª¬æ˜**\nthree dots ã«ã¤ã„ã¦ã¯[ã“ã¡ã‚‰](https://note.nkmk.me/python-numpy-ellipsis/)<br>","metadata":{}},{"cell_type":"code","source":"class RNN:\n    def __init__(self, Wx, Wh, b):\n        self.params = [Wx, Wh, b]\n        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n        self.cache = None\n    \n    def forward(self, x, h_prev):\n        Wx, Wh, b = self.params\n        t = np.dot(h_prev, Wh) + np.dot(x, Wx) + b\n        h_next = np.tanh(t)\n        \n        self.cache = (x, h_prev, h_next)\n        return h_next\n    \n    def backward(self, dh_next):\n        Wx, Wh, d = self.params\n        x, h_prev, h_next = self.cache\n        \n        dt = dh_next * (1 - h_next ** 2)\n        db = np.sum(dt, axis=0)\n        dWh = np.dot(h_prev.T, dt)\n        dh_prev = np.dot(x.T, dt)\n        dx = np.dot(dt, Wx.T)\n        \n        self.grads[0][...] = dWx  ## three dots\n        self.grads[1][...] = dWh\n        self.grads[2][...] = db\n        \n        return dx, dh_prev","metadata":{"execution":{"iopub.status.busy":"2021-11-02T09:57:58.323297Z","iopub.execute_input":"2021-11-02T09:57:58.323758Z","iopub.status.idle":"2021-11-02T09:57:58.334794Z","shell.execute_reply.started":"2021-11-02T09:57:58.323715Z","shell.execute_reply":"2021-11-02T09:57:58.334009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.ï¼’ Time RNN one layer\n\nRNNã‚’æ¨ªã«Tå€‹ä¸¦ã¹ãŸã‚‚ã®ã€‚<br>\nRNNã§ä¸€ã¤ã®ã‚»ãƒ³ãƒ†ãƒ³ã‚¹é•·ã‚’å–ã‚Šæ‰±ã„ã€æ¨ªã«Tå€‹ä¸¦ã¹ã‚‹äº‹ã§æ™‚é–“æ–¹å‘ã®è§£åƒåº¦ã‚’ä½œã‚Šå‡ºã—ã¦ã„ã‚‹ã€‚<br>\n\n#### å¼ã®èª¬æ˜\n<br>\n\n##### **forwardã«ã¤ã„ã¦**\n<br>\n<img src=\"https://camo.qiitausercontent.com/3d6925879a4d36d8cd7562a9adf75d76f43f281e/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3230393730352f65363761373333612d623134662d373232362d646262382d3061393135396663396434312e706e67\" width=\"600\">\n<br><br>\n\n##### **backwordã«ã¤ã„ã¦**\n<br>\n<img src=\"https://camo.qiitausercontent.com/78da7c43ce5218b6eb574f29e3ffb574f977b92e/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3230393730352f39383436336136652d313534392d356264322d373530312d6635353066346538303932302e706e67\" width=\"400\">\n<br><br>\n","metadata":{}},{"cell_type":"code","source":"class TimeRNN:\n    def __init__(self, Wx, Wh, b, stateful=False):\n        self.params = [Wx, Wh, b]\n        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n        self.layers = None\n\n        self.h, self.dh = None, None\n        self.stateful = stateful\n\n    def forward(self, xs):\n        Wx, Wh, b = self.params\n        N, T, D = xs.shape\n        D, H = Wx.shape\n\n        self.layers = []\n        hs = np.empty((N, T, H), dtype='f')\n\n        if not self.stateful or self.h is None:\n            self.h = np.zeros((N, H), dtype='f')\n\n        for t in range(T):\n            layer = RNN(*self.params)  ## <= RNNã‚’Tå€‹ç”Ÿæˆã—ã¦ã„ã‚‹ã€‚\n            self.h = layer.forward(xs[:, t, :], self.h)\n            hs[:, t, :] = self.h\n            self.layers.append(layer)\n\n        return hs\n    \n    def backward(self, dhs):\n        Wx, Wh, b = self.params\n        N, T, H = dhs.shape\n        D, H = Wx.shape\n\n        dxs = np.empty((N, T, D), dtype='f')\n        dh = 0\n        grads = [0, 0, 0]\n        for t in reversed(range(T)):\n            layer = self.layers[t]\n            dx, dh = layer.backward(dhs[:, t, :] + dh)\n            dxs[:, t, :] = dx\n\n            for i, grad in enumerate(layer.grads):\n                grads[i] += grad\n\n        for i, grad in enumerate(grads):\n            self.grads[i][...] = grad\n        self.dh = dh\n\n        return dxs\n\n    def set_state(self, h):\n        self.h = h\n\n    def reset_state(self):\n        self.h = None","metadata":{"execution":{"iopub.status.busy":"2021-11-02T09:57:58.336378Z","iopub.execute_input":"2021-11-02T09:57:58.336764Z","iopub.status.idle":"2021-11-02T09:57:58.351507Z","shell.execute_reply.started":"2021-11-02T09:57:58.336725Z","shell.execute_reply":"2021-11-02T09:57:58.350728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **3.3 RNNLM**\n\nTimeRNNã€€layerã«åŠ ãˆã€TimeAffineã€€layerã€ TimeEmbedding layerã€TimeSoftmaxWithLossã€€layerã‚’çµ„ã¿åˆã‚ã›ã¦<br>\nRNNLMã‚’ä½œã‚‹ã€‚\n\n\n##### **Structure of RNNLM**\n\n<img src =\"https://camo.qiitausercontent.com/ac8303dfae114898ac9d1f752e0b9378ba975784/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3230393730352f61346461333038372d623736642d353636322d386539322d3364306131333062643737342e706e67\" width=\"500\">\n<br><br>\n\n##### **Time Embedding**\n\n<img src=\"https://camo.qiitausercontent.com/32654fed87f53f2b82b773d50c36e94a97f9bec0/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3230393730352f31363033633765302d653532392d346237632d653936312d3039396561653661663433632e706e67\" width=\"500\">\n<br>\nTime Embeddingãƒ¬ã‚¤ãƒ¤ã¯ã€xsã‹ã‚‰1åˆ—ã¥ã¤ãƒ‡ãƒ¼ã‚¿ã‚’åˆ‡ã‚Šå‡ºã—Embeddingãƒ¬ã‚¤ãƒ¤ã«å…¥åŠ›ã—ã¦ã€<br>\nãã®å‡ºåŠ›ã‚’out(N, T, D)ã«æºœã‚è¾¼ã‚€ã€ã¨ã„ã†å‹•ä½œã‚’forãƒ«ãƒ¼ãƒ—ã§Tå›ç¹°ã‚Šè¿”ã™ã¨ã„ã†ã‚‚ã®ã§ã™ã€‚\n<br><br>\n\n##### **Time Affine Layer**\n\n<img src=\"https://camo.qiitausercontent.com/40771ddfdce111906aa55c50a000af060e146803/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3230393730352f37393361623630622d336263332d653063332d663166342d3064316436653038333730352e706e67\" width=\"500\">\n<br>\nTime Affineãƒ¬ã‚¤ãƒ¤ã¯ã€Affineãƒ¬ã‚¤ãƒ¤ã®å…¥å‡ºåŠ›ã«ã€æ™‚é–“è»¸æ–¹å‘ã®Tã«å¯¾å¿œå‡ºæ¥ã‚‹ã‚ˆã†ã«ãã‚Œãã‚Œreshapeã‚’ä»˜ã‘åŠ ãˆãŸã‚‚ã®ã§ã™ã€‚<br>\n<br><br>\n\n##### **Time SoftMax WithLoss**\n<img src=\"https://camo.qiitausercontent.com/e2d2604218ab475a245c7bae0a160e7f97416108/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3230393730352f63616563366363642d373362332d663733612d653230352d3737363838326639346230652e706e67\" width=\"400\">\n<br>\nTime Softmax with Loss ãƒ¬ã‚¤ãƒ¤ãƒ¼ ã¯ã€xt,ttxt,ttã®Sotmax with Loss ã‚’Tå€‹åˆç®—ã—ã¦Tã§å‰²ã‚‹ãƒ¬ã‚¤ãƒ¤ãƒ¼ã§ã™ã€‚<br>\n\n#### **ã‚³ãƒ¼ãƒ‰ã®èª¬æ˜**\nthree dots ã«ã¤ã„ã¦ã¯[ã“ã¡ã‚‰](https://note.nkmk.me/python-numpy-ellipsis/)<br>","metadata":{}},{"cell_type":"code","source":"class SimpleRnnlm:\n    def __init__(self, vocab_size, wordvec_size, hidden_size):\n        V, D, H = vocab_size, wordvec_size, hidden_size\n        rn = np.random.randn\n\n        # é‡ã¿ã®åˆæœŸåŒ–\n        embed_W = (rn(V, D) / 100).astype('f')\n        rnn_Wx = (rn(D, H) / np.sqrt(D)).astype('f')\n        rnn_Wh = (rn(H, H) / np.sqrt(H)).astype('f')\n        rnn_b = np.zeros(H).astype('f')\n        affine_W = (rn(H, V) / np.sqrt(H)).astype('f')\n        affine_b = np.zeros(V).astype('f')\n\n        # ãƒ¬ã‚¤ãƒ¤ã®ç”Ÿæˆ\n        self.layers = [\n            TimeEmbedding(embed_W),\n            TimeRNN(rnn_Wx, rnn_Wh, rnn_b, stateful=True),\n            TimeAffine(affine_W, affine_b)\n        ]\n        self.loss_layer = TimeSoftmaxWithLoss()\n        self.rnn_layer = self.layers[1]\n\n        # ã™ã¹ã¦ã®é‡ã¿ã¨å‹¾é…ã‚’ãƒªã‚¹ãƒˆã«ã¾ã¨ã‚ã‚‹\n        self.params, self.grads = [], []\n        for layer in self.layers:\n            self.params += layer.params\n            self.grads += layer.grads\n\n    def forward(self, xs, ts):\n        for layer in self.layers:\n            xs = layer.forward(xs)\n        loss = self.loss_layer.forward(xs, ts)\n        return loss\n\n    def backward(self, dout=1):\n        dout = self.loss_layer.backward(dout)\n        for layer in reversed(self.layers):\n            dout = layer.backward(dout)\n        return dout\n\n    def reset_state(self):\n        self.rnn_layer.reset_state()\n        \nclass TimeEmbedding:\n    def __init__(self, W):\n        self.params = [W]\n        self.grads = [np.zeros_like(W)]\n        self.layers = None\n        self.W = W\n\n    def forward(self, xs):\n        N, T = xs.shape\n        V, D = self.W.shape\n\n        out = np.empty((N, T, D), dtype='f')\n        self.layers = []\n\n        for t in range(T):\n            layer = Embedding(self.W)\n            out[:, t, :] = layer.forward(xs[:, t])\n            self.layers.append(layer)\n\n        return out\n\n    def backward(self, dout):\n        N, T, D = dout.shape\n\n        grad = 0\n        for t in range(T):\n            layer = self.layers[t]\n            layer.backward(dout[:, t, :])\n            grad += layer.grads[0]\n\n        self.grads[0][...] = grad\n        return None\n\nclass TimeAffine:\n    def __init__(self, W, b):\n        self.params = [W, b]\n        self.grads = [np.zeros_like(W), np.zeros_like(b)]\n        self.x = None\n\n    def forward(self, x):\n        N, T, D = x.shape\n        W, b = self.params\n\n        rx = x.reshape(N*T, -1)\n        out = np.dot(rx, W) + b\n        self.x = x\n        return out.reshape(N, T, -1)\n\n    def backward(self, dout):\n        x = self.x\n        N, T, D = x.shape\n        W, b = self.params\n\n        dout = dout.reshape(N*T, -1)\n        rx = x.reshape(N*T, -1)\n\n        db = np.sum(dout, axis=0)\n        dW = np.dot(rx.T, dout)\n        dx = np.dot(dout, W.T)\n        dx = dx.reshape(*x.shape)\n\n        self.grads[0][...] = dW\n        self.grads[1][...] = db\n\n        return dx\n    \nclass TimeSoftmaxWithLoss:\n    def __init__(self):\n        self.params, self.grads = [], []\n        self.cache = None\n        self.ignore_label = -1\n\n    def forward(self, xs, ts):\n        N, T, V = xs.shape\n\n        if ts.ndim == 3:  # æ•™å¸«ãƒ©ãƒ™ãƒ«ãŒone-hotãƒ™ã‚¯ãƒˆãƒ«ã®å ´åˆ\n            ts = ts.argmax(axis=2)\n\n        mask = (ts != self.ignore_label)\n\n        # ãƒãƒƒãƒåˆ†ã¨æ™‚ç³»åˆ—åˆ†ã‚’ã¾ã¨ã‚ã‚‹ï¼ˆreshapeï¼‰\n        xs = xs.reshape(N * T, V)\n        ts = ts.reshape(N * T)\n        mask = mask.reshape(N * T)\n\n        ys = softmax(xs)\n        ls = np.log(ys[np.arange(N * T), ts])\n        ls *= mask  # ignore_labelã«è©²å½“ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã¯æå¤±ã‚’0ã«ã™ã‚‹\n        loss = -np.sum(ls)\n        loss /= mask.sum()\n\n        self.cache = (ts, ys, mask, (N, T, V))\n        return loss\n\n    def backward(self, dout=1):\n        ts, ys, mask, (N, T, V) = self.cache\n\n        dx = ys\n        dx[np.arange(N * T), ts] -= 1\n        dx *= dout\n        dx /= mask.sum()\n        dx *= mask[:, np.newaxis]  # ignore_labelã«è©²å½“ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã¯å‹¾é…ã‚’0ã«ã™ã‚‹\n\n        dx = dx.reshape((N, T, V))\n\n        return dx","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **4. LSTM**\n\n1ã¤ã®LSTMã®ä¸­ã«ã€forget agteã¨ã€input gateã¨stateã‚’è¨˜æ†¶ã™ã‚‹cellã¨ã€outputã‚²ãƒ¼ãƒˆãŒ<br>\nå…¥åŠ›å´ã‹ã‚‰ä¸¦åˆ—ã«ã€ãã‚Œãã‚Œã®outputãŒç›´åˆ—ã«ã¤ãªãŒã£ã¦ã„ã‚‹ã€‚<br>\nã“ã®æ§‹é€ ãŒã‚ã‹ã‚Œã°ã€ãã‚Œãã‚Œã®ãƒ‘ãƒ¼ãƒ„ã¯ç°¡å˜ã€‚<br>\n\n#### 4.1.1 Confirm \"gradient vanishing problem\"\n\n<br>\n\n#### 4.1.2 Confirm \"gradient exploding problem\"\n\n<br>\n\n#### 4.1.3 diff RNN and LSTM\n\n* RNN<br>\n<img src=\"https://camo.qiitausercontent.com/b643944d722e601f9e3d4b7856cd096895f5ce1f/687474703a2f2f636f6c61682e6769746875622e696f2f706f7374732f323031352d30382d556e6465727374616e64696e672d4c53544d732f696d672f4c53544d332d53696d706c65524e4e2e706e67\" width=500>\n<br><br>\n* LSTM<br>\n<img src=\"https://camo.qiitausercontent.com/a12fe62032a6633b05b8ef7c2512d1e54c9c4afd/687474703a2f2f636f6c61682e6769746875622e696f2f706f7374732f323031352d30382d556e6465727374616e64696e672d4c53544d732f696d672f4c53544d332d636861696e2e706e67\" width=500><br><br>\n\n<br>\n\n#### ** 4.1.4 LSTM Core Idea\nThe key to LSTMs is the cell state, the horizontal line running through the top of the diagram.<br>\n<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-C-line.png\" width=500><br><br>\n\n#### **ã‚³ãƒ¼ãƒ‰ã®èª¬æ˜**\n\nDx4Hã§ã€4ãŒã¤ã„ã¦ã„ã‚‹ç†ç”±ã¯ã€fã¨gã¨iã¨oã®Wã‚’æ¨ªã«ã¤ãªã’ã¦ä¸€æ°—ã«æ‰±ã£ã¦ã„ã‚‹ç‚ºã€‚\n","metadata":{}},{"cell_type":"markdown","source":"#### ï¼”.2 LSTM one layer\n\nUnderstand LSTM structure via python code.<br>\n\n#### å¼ã®èª¬æ˜\n##### **forwarã«ã¤ã„ã¦** <br>\n<img src=\"https://camo.qiitausercontent.com/b9387ed37d1f247f8f679184f8dd2efd1e7a709a/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3230393730352f35306164343264652d643433642d333162392d333761652d3637666462313032393939612e706e67\" width=600>\n<br><br>\n\n##### **backwardã«ã¤ã„ã¦** <br>\n<img src=\"https://camo.qiitausercontent.com/bd2fad9749a2dd322a1dd9f8dcfbcf3ee209107a/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3230393730352f62366631386438372d613436662d643762302d303435382d3733656163373434663465662e706e67\" width=600>\n<br><br>\n","metadata":{}},{"cell_type":"code","source":"class LSTM:\n    def __init__(self, Wx, Wh, b):\n\n        self.params = [Wx, Wh, b]\n        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n        self.cache = None\n\n    def forward(self, x, h_prev, c_prev):\n        Wx, Wh, b = self.params\n        N, H = h_prev.shape\n\n        A = np.dot(x, Wx) + np.dot(h_prev, Wh) + b\n\n        f = A[:, :H]\n        g = A[:, H:2*H]\n        i = A[:, 2*H:3*H]\n        o = A[:, 3*H:]\n\n        f = sigmoid(f)\n        g = np.tanh(g)\n        i = sigmoid(i)\n        o = sigmoid(o)\n\n        c_next = f * c_prev + g * i\n        h_next = o * np.tanh(c_next)\n\n        self.cache = (x, h_prev, c_prev, i, f, g, o, c_next)\n        return h_next, c_next\n\n    def backward(self, dh_next, dc_next):\n        Wx, Wh, b = self.params\n        x, h_prev, c_prev, i, f, g, o, c_next = self.cache\n\n        tanh_c_next = np.tanh(c_next)\n\n        ds = dc_next + (dh_next * o) * (1 - tanh_c_next ** 2)\n\n        dc_prev = ds * f\n\n        di = ds * g\n        df = ds * c_prev\n        do = dh_next * tanh_c_next\n        dg = ds * i\n\n        di *= i * (1 - i)\n        df *= f * (1 - f)\n        do *= o * (1 - o)\n        dg *= (1 - g ** 2)\n\n        dA = np.hstack((df, dg, di, do))\n\n        dWh = np.dot(h_prev.T, dA)\n        dWx = np.dot(x.T, dA)\n        db = dA.sum(axis=0)\n\n        self.grads[0][...] = dWx\n        self.grads[1][...] = dWh\n        self.grads[2][...] = db\n\n        dx = np.dot(dA, Wx.T)\n        dh_prev = np.dot(dA, Wh.T)\n\n        return dx, dh_prev, dc_prev","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **5. Transformer**\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **6. Visualization**\n\n#### **6.1 word cloud**","metadata":{}},{"cell_type":"code","source":"words = [ w for t in train[\"text\"].str.split().tolist() for w in t]","metadata":{"execution":{"iopub.status.busy":"2021-11-03T12:53:52.597063Z","iopub.execute_input":"2021-11-03T12:53:52.597866Z","iopub.status.idle":"2021-11-03T12:53:52.636973Z","shell.execute_reply.started":"2021-11-03T12:53:52.597819Z","shell.execute_reply":"2021-11-03T12:53:52.635924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,8))\nword_cloud = WordCloud(\n                          background_color='black',\n                          max_font_size = 80\n                         ).generate(\" \".join(words[:200]))\nplt.imshow(word_cloud)\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-03T12:54:57.997779Z","iopub.execute_input":"2021-11-03T12:54:57.99808Z","iopub.status.idle":"2021-11-03T12:54:58.480008Z","shell.execute_reply.started":"2021-11-03T12:54:57.998046Z","shell.execute_reply":"2021-11-03T12:54:58.479218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}