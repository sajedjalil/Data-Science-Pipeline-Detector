{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport transformers\nfrom transformers import *\nfrom sklearn import metrics\nfrom sklearn.model_selection import KFold\n\n\nprint('Transformers version: ', transformers.__version__)\nprint('Tensorflow version: ', tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Import Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dir = '/kaggle/input/nlp-getting-started/'\ntrain_df = pd.read_csv(data_dir+'train.csv')\ntest_df = pd.read_csv(data_dir+'test.csv')\ntrain_df = train_df.sample(n=len(train_df), random_state=42)\nsample_submission = pd.read_csv(data_dir+'sample_submission.csv')\nprint(train_df['target'].value_counts())\ntrain_df.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Prep Functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.tokenize.treebank import TreebankWordTokenizer\ntree_tokenizer = TreebankWordTokenizer()\ndef get_tree_tokens(x):\n    x = tree_tokenizer.tokenize(x)\n    x = ' '.join(x)\n    return x\ntrain_df.text = train_df.text.apply(get_tree_tokens)\ntest_df.text = test_df.text.apply(get_tree_tokens)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from: https://www.kaggle.com/utsavnandi/roberta-using-huggingface-tf-implementation\ndef to_tokens(input_text, tokenizer):\n    output = tokenizer.encode_plus(input_text, max_length=90, pad_to_max_length=True)\n    return output\n\ndef select_field(features, field):\n    return [feature[field] for feature in features]\n\nimport re\ndef clean_tweet(tweet):\n    # Removing the @\n    #tweet = re.sub(r\"@[A-Za-z0-9]+\", ' ', tweet)\n    # Removing the URL links\n    #tweet = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', tweet)\n    # Keeping only letters\n    #tweet = re.sub(r\"[^a-zA-Z.!?']\", ' ', tweet)\n    # Removing additional whitespaces\n    tweet = re.sub(r\" +\", ' ', tweet)\n    return tweet\n\ndef preprocess_data(tokenizer, train_df, test_df):\n    train_text = train_df['text'].apply(clean_tweet)\n    test_text = test_df['text'].apply(clean_tweet)\n    train_encoded = train_text.apply(lambda x: to_tokens(x, tokenizer))\n    test_encoded = test_text.apply(lambda x: to_tokens(x, tokenizer))\n\n    #create attention masks\n    input_ids_train = np.array(select_field(train_encoded, 'input_ids'))\n    attention_masks_train = np.array(select_field(train_encoded, 'attention_mask'))\n\n    input_ids_test = np.array(select_field(test_encoded, 'input_ids'))\n    attention_masks_test = np.array(select_field(test_encoded, 'attention_mask'))\n\n    # concatonate masks\n    train_X = [input_ids_train, attention_masks_train]\n    test_X = [input_ids_test, attention_masks_test]\n    #OHE target\n    train_y = tf.keras.utils.to_categorical(train_df['target'].values.reshape(-1, 1))\n\n    return train_X, train_y, test_X","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Function to load models"},{"metadata":{"trusted":true},"cell_type":"code","source":"# code from https://github.com/huggingface/transformers\n# Transformers has a unified API\n# for 10 transformer architectures and 30 pretrained weights.\n#          Model          | Tokenizer          | Pretrained weights shortcut\ndef load_pretrained_model(model_class='bert', model_name='bert-base-cased', task='binary', learning_rate=3e-5, epsilon=1e-8, lower_case=False):\n  MODEL_CLASSES = {\n    \"bert\": (BertConfig, TFBertForSequenceClassification, BertTokenizer),\n    \"xlnet\": (XLNetConfig, TFXLNetForSequenceClassification, XLNetTokenizer),\n    \"xlm\": (XLMConfig, TFXLMForSequenceClassification, XLMTokenizer),\n    \"roberta\": (RobertaConfig, TFRobertaForSequenceClassification, RobertaTokenizer),\n    \"distilbert\": (DistilBertConfig, TFDistilBertForSequenceClassification, DistilBertTokenizer),\n    \"albert\": (AlbertConfig, TFAlbertForSequenceClassification, AlbertTokenizer),\n    #\"xlmroberta\": (XLMRobertaConfig, XLMRobertaForSequenceClassification, XLMRobertaTokenizer), No tensorflow version yet\n  }\n  model_metrics = [\n        tf.keras.metrics.TruePositives(name='tp'),\n        tf.keras.metrics.FalsePositives(name='fp'),\n        tf.keras.metrics.TrueNegatives(name='tn'),\n        tf.keras.metrics.FalseNegatives(name='fn'), \n        tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n        tf.keras.metrics.Precision(name='precision'),\n        tf.keras.metrics.Recall(name='recall'),\n        tf.keras.metrics.AUC(name='auc'),\n  ]\n\n  \n  config_class, model_class, tokenizer_class = MODEL_CLASSES[model_class]\n\n  config = config_class.from_pretrained(model_name, num_labels=2, finetuning_task=task)\n\n\n  model = model_class.from_pretrained(model_name)\n  optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=epsilon, clipnorm=1.0)\n  loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n  metric = tf.keras.metrics.BinaryAccuracy('accuracy')\n  model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n  #model.summary()\n\n  tokenizer = tokenizer_class.from_pretrained(model_name, lower_case = lower_case)\n\n  return config, model, tokenizer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# load model, process data for model\n_, _, tokenizer = load_pretrained_model(model_class='roberta', model_name='roberta-base', learning_rate=2e-5, lower_case=False)\ntrain_X, train_y, test_X = preprocess_data(tokenizer=tokenizer, train_df=train_df, test_df=test_df)\n\n\nkf = KFold(n_splits=6)\ntest_preds = []\ni = 0\nfor train_idx, test_idx in kf.split(train_X[0]):\n    i+=1\n    if i not in [1, 5]: #only do 2 folds to save time\n        continue\n    train_split_X = [train_X[i][train_idx] for i in range(len(train_X))]\n    test_split_X = [train_X[i][test_idx] for i in range(len(train_X))]\n\n    train_split_y = train_y[train_idx]\n    test_split_y = train_y[test_idx]\n    #create class weights to account for inbalance\n    positive = train_df.iloc[train_idx, :].target.value_counts()[0]\n    negative = train_df.iloc[train_idx, :].target.value_counts()[1]\n    pos_weight = positive / (positive + negative)\n    neg_weight = negative / (positive + negative)\n\n    class_weight = [{0:pos_weight, 1:neg_weight}, {0:neg_weight, 1:pos_weight}]\n\n    K.clear_session()\n    config, model, tokenizer = load_pretrained_model(model_class='roberta', model_name='roberta-base', learning_rate=2e-5, lower_case=False)\n\n    # fit, test model\n    model.fit(train_split_X, train_split_y, batch_size=64, epochs=3, class_weight=class_weight, validation_data=(test_split_X, test_split_y))\n\n    val_preds = model.predict(test_split_X, batch_size=32, verbose=1)\n    val_preds = np.argmax(val_preds, axis=1).flatten()\n    print(metrics.accuracy_score(train_df.iloc[test_idx, :].target.values, val_preds))\n\n    preds1 = model.predict(test_X, batch_size=32, verbose=1)\n    test_preds.append(preds1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Output Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds2 = np.average(test_preds, axis=0)\ntest_preds3 = np.argmax(test_preds2, axis=1).flatten()\nsample_submission['target'] = test_preds3\nsample_submission['target'].value_counts()\nsample_submission.to_csv('new_submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}