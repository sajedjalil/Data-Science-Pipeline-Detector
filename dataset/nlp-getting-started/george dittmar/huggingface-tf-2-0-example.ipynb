{"cells":[{"metadata":{},"cell_type":"markdown","source":"In this notebook we will look at using Huggingface with TF 2.0 to do a straight forward attempt at text classification. This is a somewhat barebones notebook to just show what is required to train the BERT model. Comments are appreciated on how I can improve the information here or any issues you noticed with my process much appreciated! :)","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First we need to install the transformers library from huggingface https://huggingface.co with the command below. Make sure the internet is turned on in the kernel if the notebook does not already have transformers in stalled.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!pip install transformers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For this model we are going to use the DistilBert model and tokenizers because it is lighter on resources but I also included the imports for standard Bert models. You can swap these out at the beginnning and the rest of the code will work.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import TFBertForSequenceClassification, BertTokenizer, TFDistilBertForSequenceClassification, DistilBertTokenizer, glue_convert_examples_to_features\nimport tensorflow as tf\nimport pandas as pd\n\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n#model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get the data and read into pandas frames from the csv\ntrain = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Now that we have the data loaded lets takea  quick look at the training set. We can drop all columns besides the text and target column for now.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now lets create a new df just using the text and target columns. We will then drop any empty rows just incase. Then from that I create a seperate train_x df and a numpy array called train_y for the labels.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_text_df = train[['text', 'target']]\ntrain_text_df = train_text_df.dropna()\ntrain_X = train_text_df['text']\ntrain_y = train_text_df['target'].to_numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Now that we have our training set setup with the labels needed we can move onto tokenizing the input data. Since we are using the BERT tokenizer from huggingface we can easily use the batch_encode_plus function to pass the dataframe and it will apply the tokenization to all the data. All we are going to set for this is pad_to_max_length to True and we want to return tensors for TF. This will return the data in an object that TF can easily work with.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x = tokenizer.batch_encode_plus(train_X, pad_to_max_length=True, return_tensors=\"tf\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets look at the tf object now. As you can see we get a dictionary back with \"input_ids\" and \"attention_mask\". For our purposes we do not need the attention mask so we will only be using the input_ids.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now all we need to do is setup the model like you would any other TF keras model. Just setup the optmizer, loss, and metrix then compile the model and you can fit the pretrained model on the training set.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\nloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nbce = tf.keras.losses.BinaryCrossentropy()\nmetric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n\nmodel.compile(optimizer=optimizer, loss=loss, metrics=[metric])\nmodel.fit(x=train_x['input_ids'], y=train_y, epochs=15, batch_size=128, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_x = test['text'].to_numpy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we have our test set setup we need to send that input through the tokenizer setup the same as our training set. Once that is tokenized we can send it to the predict function in the model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_x = tokenizer.batch_encode_plus(test_x, pad_to_max_length=True, return_tensors=\"tf\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model.predict(test_x['input_ids'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since we are doing binary classification there will only be two label values that are returned per instance so we can then just take the argmax of each result to get our final label outut of either zero or one.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_label = [ np.argmax(x) for x in predictions[0]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({'id': test['id'], 'target': predictions_label})\nsubmission['target'] = submission['target'].astype('int')\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}