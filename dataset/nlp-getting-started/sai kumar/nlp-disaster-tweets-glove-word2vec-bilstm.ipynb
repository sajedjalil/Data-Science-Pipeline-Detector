{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"***After my previous notebook on Bow and Tf-idf models this notebook takes you through word embeddings and different methods to create word embeddings and using those embeddings with a Sequential model using bidirectional LSTM layer which classifies our text achieving a submission score of >0.8 .***","metadata":{}},{"cell_type":"markdown","source":"# Text processing","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train=pd.read_csv('../input/nlp-getting-started/train.csv')\ntest=pd.read_csv('../input/nlp-getting-started/test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can check out my previous notebook [here](https://www.kaggle.com/urstrulysai/bow-tf-idf-models-with-basic-lr-0-80-score) for detailed text pre processing and basic NLP models before going into word embeddings and LSTMs, GRUs, RNNs.","metadata":{}},{"cell_type":"code","source":"import nltk\nnltk.download('punkt')\nnltk.download('stopwords')\nimport re\n!pip install contractions\nimport contractions\nfrom nltk.stem import SnowballStemmer\nfrom nltk.stem import WordNetLemmatizer\nnltk.download('wordnet')\n!pip install pyspellchecker\nfrom spellchecker import SpellChecker","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stop_words=nltk.corpus.stopwords.words('english')\ni=0\n#sc=SpellChecker()\n#data=pd.concat([train,test])\nwnl=WordNetLemmatizer()\nstemmer=SnowballStemmer('english')\nfor doc in train.text:\n    doc=re.sub(r'https?://\\S+|www\\.\\S+','',doc)\n    doc=re.sub(r'<.*?>','',doc)\n    doc=re.sub(r'[^a-zA-Z\\s]','',doc,re.I|re.A)\n    #doc=' '.join([stemmer.stem(i) for i in doc.lower().split()])\n    doc=' '.join([wnl.lemmatize(i) for i in doc.lower().split()])\n    #doc=' '.join([sc.correction(i) for i in doc.split()])\n    doc=contractions.fix(doc)\n    tokens=nltk.word_tokenize(doc)\n    filtered=[token for token in tokens if token not in stop_words]\n    doc=' '.join(filtered)\n    train.text[i]=doc\n    i+=1\ni=0\nfor doc in test.text:\n    doc=re.sub(r'https?://\\S+|www\\.\\S+','',doc)\n    doc=re.sub(r'<.*?>','',doc)\n    doc=re.sub(r'[^a-zA-Z\\s]','',doc,re.I|re.A)\n    #doc=' '.join([stemmer.stem(i) for i in doc.lower().split()])\n    doc=' '.join([wnl.lemmatize(i) for i in doc.lower().split()])\n    #doc=' '.join([sc.correction(i) for i in doc.split()])\n    doc=contractions.fix(doc)\n    tokens=nltk.word_tokenize(doc)\n    filtered=[token for token in tokens if token not in stop_words]\n    doc=' '.join(filtered)\n    test.text[i]=doc\n    i+=1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Word embedding models","metadata":{}},{"cell_type":"markdown","source":"spaCy provides 96,200,300-dimensional word embeddings for several languages, which have been learned from large corpora. In other words, each word in the model's vocabulary is represented by a list of 96/200/300 floating point numbers – a vector – and these vectors are embedded into a 96/200/300-dimensional space.\n1. en_core_web_sm: Here en is for english and sm is for small i.e 96 dimensional word embeddings\n2. en_core_web_md: md is for medium i.e 200 dimensional word embeddings\n3. en_core_web_lg: lg is for large i.e 300 dimensional word embeddings","metadata":{}},{"cell_type":"code","source":"import spacy\nnlp=spacy.load('en_core_web_sm')\nvecs=np.array([nlp(token).vector for token in train['text']])\ntrain_df=pd.DataFrame(vecs)\ntest_df=pd.DataFrame(np.array([nlp(token).vector for token in test['text']]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can use these word embeddings and fit any classification model to get predictions. If not spacy we have glove embeddings uploaded in kaggle as datasets which can be loaded in directly and be used for predctions with any classification model. GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space. To get sentence level embeddings we can directly average all the word embeddings of words in that particular sentence so that we can use these for predictions by a classifier model.","metadata":{}},{"cell_type":"markdown","source":"1. There are other type of embedding models too such as Word2Vec and FastText. Word2Vec has again two types skipgram model which predicts context words based on target word and CBoW(continous bag of words) model which predicts target words based on context words. \n2. Word2Vec model ignored morphological structure of each word and considers a word as single entity. FastText model considers each word as a bag of character n grams and then takes average of embedding of these n grams. Rare words get good representation using FastText model.","metadata":{}},{"cell_type":"code","source":"# Robust Word2Vec with gensim\nfrom gensim.models import word2vec\n\nword2vec_model = word2vec.Word2Vec([nltk.word_tokenize(doc) for doc in train.text], #tokenized_corpus\n                                 vector_size = 15, # feature size\n                                 window = 20, # context window\n                                 min_count = 1, # word count\n                                 sg = 1, # 1 for skipgram, cbow otherwise\n                                 sample = 1e-3, # downsample settling for frequent words\n                                 \n                                )\n#word2vec_model.wv['word'] # returns you the embedding for word\n#word2vec_model.wx # returns dictionary of words mapped to their respective embeddings","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fasttext model\n\nfrom gensim.models import FastText\n\nFastTextModel = FastText([nltk.word_tokenize(doc) for doc in train.text], #tokenized_corpus\n                                 vector_size = 15, # feature size\n                                 window = 20, # context window\n                                 min_count = 1, # word count\n                                 sg = 1, # 1 for skipgram, cbow otherwise\n                                 sample = 1e-3, # downsample settling for frequent words\n                                 )\n#FastTextModel.wv.similarity(w1 = 'word1', w2 = 'word2') # returns how similar two words are\n#FastTextModel.wv.doesnt_match(array_of_words) # returns the odd one out the array of words","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Below I have loaded in the dataset uploaded by @anindya2906 . In this dataset we have 50,100,200 and also 300 dimensional glove pre trained word embeddings.\nOr else we can find the embeddings at the official website [here](https://nlp.stanford.edu/projects/glove/)","metadata":{}},{"cell_type":"code","source":"# loading in each glove word embedding into a dictionary\ndict1={}\nwith open('../input/glove6b/glove.6B.200d.txt','r') as f:\n    for line in f:\n        values=line.split()\n        word=values[0]\n        vectors=np.asarray(values[1:],'float32')\n        dict1[word]=vectors\nf.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tok=tf.keras.preprocessing.text.Tokenizer()\n\n# This class allows to vectorize a text corpus, by turning each text into either a sequence of \n# integers (each integer being the index of a token in a dictionary) or into a vector \n# where the coefficient for each token could be binary, based on word count, based on tf-idf...\n\ntok.fit_on_texts([nltk.word_tokenize(doc) for doc in train.text])\nseq_train=tok.texts_to_sequences([nltk.word_tokenize(doc) for doc in train.text])\nseq_test=tok.texts_to_sequences([nltk.word_tokenize(doc) for doc in test.text])\npad_train=tf.keras.preprocessing.sequence.pad_sequences(seq_train,maxlen=25,padding='post',truncating='post')\npad_test=tf.keras.preprocessing.sequence.pad_sequences(seq_test,maxlen=25,padding='post',truncating='post')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1. If you want to know more about [tensorflow's tokenizer](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer). \n2. pad_sequences is used to convert each sentence to the same length specified either by padding 0s pre sentence or post sentence till max length is achieved or by truncating if sentence is of greater length than max length.","metadata":{}},{"cell_type":"code","source":"# embedding matrix is of shape (total words + 1 , dimension of embedding)\n# tok.word_index gives us list of words and the len(that list) gives total number of words in all the sentences.\n\nemb_matrix=np.zeros((len(tok.word_index)+1,200))\nfor word,i in tok.word_index.items():\n    if dict1.get(word) is not None:\n        emb_matrix[i]=dict1.get(word)\n\n# This embedding matrix will be used as weights in Embedding layer later.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Building","metadata":{}},{"cell_type":"markdown","source":"Do go through the below tensorflow tutorials if you want to get a full idea on how RNNs, GRUs, LSTMs and Bidirectional layers work in the background. You will also get an idea of how to use back to back LSTM layers by calling return_sequence=True and use of return_state and much more.\n1. [Text classification with an RNN](https://www.tensorflow.org/tutorials/text/text_classification_rnn)\n2. [Recurrent Neural Networks (RNN) with Keras](https://www.tensorflow.org/guide/keras/rnn)\n3. [Masking and padding with Keras](https://www.tensorflow.org/guide/keras/masking_and_padding)\n4. You can find a research paper [here](https://arxiv.org/abs/1512.05287) if you want to know difference between dropout and recurrent dropout in RNNs.","metadata":{}},{"cell_type":"code","source":"model=tf.keras.Sequential([\n    tf.keras.layers.Embedding(len(tok.word_index)+1,200,weights=[emb_matrix],input_length=25,mask_zero=True,trainable=False),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(100, dropout=0.2,recurrent_dropout=0.2,return_sequences=True)),\n    tf.keras.layers.GlobalMaxPooling1D(), # use if return sequences is set to true\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(64,activation='relu'),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(32,activation='relu'),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(1,activation='sigmoid')\n])\n\n# we can try out different hidden layers including GRUs, RNNs etc whichever gives the best result.\nprint(model.summary())\nmodel.compile(loss=tf.keras.losses.BinaryCrossentropy(), # as this is a binary classification problem\n              optimizer=tf.keras.optimizers.Adam(3e-4),\n              metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint=tf.keras.callbacks.ModelCheckpoint('model.h5',monitor='val_loss',save_best_only=True)\nreduce_lr=tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',factor=0.2,patience=2,min_lr=1e-5)\nes=tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=3,restore_best_weights=True)\n\nhistory=model.fit(pad_train,train.target,batch_size=8,epochs=20,validation_split=0.2,callbacks=[checkpoint,es,reduce_lr])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we can use different methos to increase the f1 score. Using StratifiedKFold definitely gives better results, Data Augmentation can also be tried and many such more techniques. We can vary the hyperparams such as batch size, learning rate and see how the model performs.","metadata":{}},{"cell_type":"code","source":"model.load_weights('./model.h5')\npred=model.predict(pad_test) # else directly use model.predict_classes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(np.where(pred>0.5,1,0)).value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame({\n    'id':test.id,\n    'target':np.where(pred>0.5,1,0)[:,0] # you can also vary the threshold 0.5 and see if you get better scores\n}).to_csv('submission.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Do upvote if you find this helpful in any way. It will help me stay motivated to share many such notebooks and resources. Cheers!!**","metadata":{}}]}