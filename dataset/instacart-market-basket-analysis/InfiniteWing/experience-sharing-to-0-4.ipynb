{"metadata":{"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"},"language_info":{"mimetype":"text/x-python","nbconvert_exporter":"python","codemirror_mode":{"name":"ipython","version":3},"name":"python","version":"3.6.1","pygments_lexer":"ipython3","file_extension":".py"}},"nbformat":4,"cells":[{"cell_type":"markdown","metadata":{"_uuid":"ca388020e05a0223f44d2f655868c10b774bbe03","_cell_guid":"999f35ed-8234-4371-8f7b-995d27f7d230"},"source":"# **Experience sharing to 0.4**\n### *InfiniteWing*\n### *2017-08-15*\n\nHello, this notebook will demonstrate my solution and some experiences. Though my method can't beat [sh1ng's baseline](https://www.kaggle.com/c/instacart-market-basket-analysis/discussion/37697), but it should be helpful to someone.\n\n( I ensemble my solution with sh1ng's baseline to reach 0.439 on private LB.  )\n\n###Overview\n\nI got different gain from each part of work, you can have a quick look on them.\n\n0. 鯤's base XGBoost model, 0.380\n1. Faron's F1-opitimize, gain ~0.015\n2. Add aisle and department features, gain ~0.005\n3. Use Single None model, gain ~0.0007"},{"cell_type":"markdown","metadata":{"_uuid":"80154fa0e72510c1ad30835deba98a274d123515","_cell_guid":"450786e6-f9d8-45e6-a9cc-4d350ac22360"},"source":"### Data overview\n\nThese work have been done by many awesome kernel/discussion contributors. You can take a look on them:\n\n1. [Exploratory Analysis - Instacart](https://www.kaggle.com/philippsp/exploratory-analysis-instacart), by Philipp Spachtholz\n2. [Simple Exploration Notebook - Instacart](https://www.kaggle.com/sudalairajkumar/simple-exploration-notebook-instacart), by SRK\n\nThe relationships between each input datas(by Jordan Tremoureux, from [this post](https://www.kaggle.com/c/instacart-market-basket-analysis/discussion/33128) ):\n\n![data relationships](https://s5.postimg.org/or0m15w7r/instacart_Files.png) \n\n\n"},{"cell_type":"markdown","metadata":{"_uuid":"8f630df3ec76d2c440d4e0947fe432a81ba3b6fa","_cell_guid":"b47aa5e6-7e52-4a36-bef7-0e1dc9624aa9"},"source":"### Public kernels\n1. [light GBM benchmark 0.3692](https://www.kaggle.com/paulantoine/light-gbm-benchmark-0-3692), by paulantoine\n2. [Instacart XGBoost Starter - LB 0.3791](https://www.kaggle.com/fabienvs/instacart-xgboost-starter-lb-0-3791), by Fabienvs\n3. [LB 0.3805009, Python Edition](https://www.kaggle.com/nickycan/lb-0-3805009-python-edition), by 鲲\n\n\nYou should read and try to know these public kernels. I take the 鲲's kernel as my base model because it has better baseline score and it's written in python.\n"},{"cell_type":"markdown","metadata":{"_uuid":"529091d835b7292e2648043bfbe7e688a88cab56","_cell_guid":"33a1d146-b5e6-4b0a-900f-ebf035defc83"},"source":"### F1-opitimize\nThe F1-opitimize concept is discuss detailly in [this post](https://www.kaggle.com/c/instacart-market-basket-analysis/discussion/37016). Here's two papers about F1-opitimize:\n\n1. [Optimizing F-measure: A Tale of Two Approaches](https://arxiv.org/abs/1206.4625)\n2. [Thresholding Classifiers to Maximize F1 Score](https://arxiv.org/abs/1402.1892)\n\n\n\nAnd there's public kernels which already implements F1-opitimize:\n\n1. [F1-Score Expectation Maximization in O(n²)](https://www.kaggle.com/mmueller/f1-score-expectation-maximization-in-o-n), by Faron\n2. [Approximate caclulation of EF1 (need O(N) )](https://www.kaggle.com/kruegger/approximate-caclulation-of-ef1-need-o-n), by Kruegger\n\n\n\nThe concept of F1-opitimize is, take the input probability as ground-truth probability. Use mathtical method to calculate the best item length to get the best F1 score. ( It's my understanding, if I make misunderstand, please correct me. )\n\nYou can take a look on the output image of Faron's kernel:\n\n![Faron's kernel](https://www.kaggle.io/svf/1377158/01d11f4e9ed2f693bf57afd844473fc8/expected_f1.png)\n\nIt means when your predict probability are: [0.45, 0.35, 0.31, 0.29, 0.27, 0.25, 0.22, 0.20, 0.17, 0.15, 0.10, 0.05, 0.02], you should pick the top 7 items."},{"cell_type":"markdown","metadata":{"_uuid":"c639ea85b4c4aee7556889600e66644498d7be1d","_cell_guid":"02592ad4-fa37-4140-b6ab-c0a10bf9eee4"},"source":"### Merge F1-opitimize to public kernel\n    # Continue 鯤's kernel\n    X_test.loc[:,'reordered']=(bst.predict(d_test)).astype(float)\n    preds=X_test['reordered'].values\n    order_ids=X_test['order_id'].values\n    product_ids=X_test['product_id'].values\n    order2preds={}\n\n    for i in range(len(preds)):\n        order_id=order_ids[i]\n        product_id=product_ids[i]\n        pred=preds[i]\n        if(order_id not in order2preds):\n            order2preds[order_id]={}\n        order2preds[order_id][product_id]=pred\n    final_preds=[]\n    final_order_ids=list(order2preds.keys())\n    print(\"Start F1 opitimize\")\n    for order_id in tqdm(order2preds):\n        product2preds=order2preds[order_id]\n        product2preds=sorted(product2preds.items(), key=lambda x:x[1],reverse=True)\n        probabilities=[v[1] for v in product2preds]\n        products=[str(v[0]) for v in product2preds]\n        # Use Faron's F1-opitimize\n        opt=F1Optimizer.maximize_expectation(probabilities)\n        best_k=opt[0]\n        pred=products[:best_k]\n        if(opt[1]):\n            pred.append('None')\n\n        final_preds.append(' '.join(pred))\n    submit=pd.DataFrame({'order_id': final_order_ids, 'products': final_preds})\n    submit=submit.sort_values('order_id')\n    submit.to_csv('sub.csv', index=False)"},{"cell_type":"markdown","metadata":{"_uuid":"5c4cf7bceae1729320ff14ae647b8680b424dbb3","_cell_guid":"c083aa48-94c1-4c69-ba82-0c89766f5f90"},"source":"### Create more features\n\nFor myself, I add several aisle and department features. It gives me around ~0.005 gain to reach 0.4 score on LB. \n\n[create_departments_aisles_features.py](https://github.com/InfiniteWing/Kaggle/blob/master/Instacart%20Market%20Basket%20Analysis/create_departments_aisles_features.py)\n\nAfter you save the new features to file, you can simply load it by pandas. And then merge the new features with original features. Such like:\n\n    new_feature_df = pd.read_csv(\"departments_aisles_features.csv\")\n    data=data.merge(new_feature_df,on=['user_id', 'product_id'], how='left')\n\n"},{"cell_type":"markdown","metadata":{"_uuid":"5a596f327d438825629658c8cb18ba866eb4eeb1","_cell_guid":"ac64682b-df1c-4345-bd92-f84b92e57530"},"source":"### Handling None\nI use simple none prediction at first. If an customer's none order rate >=0.25, then I will append an 'None' to prediction. It helps me to gain ~0.006 at beginning. After Faron reveal his F1-opitimize code, I find it will gain more if I use the None probability which calculated by F1-opitimize. However, at last day before competition end, I start to build a single None model which will handle None's probability. It gives me ~0.0007 gain on local cv and LB. Here's how I did it, first we need to build some features for None model:\n\ncreate_none_features_train.py\n"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_execution_state":"idle","_uuid":"34dbab0bb4b466f6f529f65008f479602d658645","_cell_guid":"7f9be804-4e5f-4aa3-b102-d3dc17670a90","collapsed":true},"source":"from tqdm import tqdm\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport statistics\norders={}\nproducts={}\nusers={}\nproducts_aisles={}\nproducts_departments={}\norders_isnone={}\norders_dows={}\norders_hours={}\norders_days_since_prior_order={}\ntrain_days_since_prior_order={}\nprint(\"Start loading products.csv\")\nproducts_df=pd.read_csv('../input/products.csv', encoding = 'UTF-8')\nfor index, row in tqdm(products_df.iterrows()):\n    product_id=str(int(row['product_id']))\n    aisles_id=str(int(row['aisle_id']))\n    departments_id=str(int(row['department_id']))\n    products_aisles[product_id]=aisles_id\n    products_departments[product_id]=departments_id\n    \n\nprint(\"Start loading order_products__prior.csv\")\n#read prior orders\nfr = open(\"../input/order_products__prior.csv\", 'r', encoding = 'UTF-8')\nfr.readline()# skip header\nlines=fr.readlines()\nfor i,line in tqdm(enumerate(lines)):\n    datas=line.replace(\"\\n\",\"\").split(\",\")\n    order_id=(datas[0])\n    product_id=(datas[1])\n    reorderer=int(datas[3])\n    if(order_id not in orders):\n        orders[order_id]={}\n    orders[order_id][product_id]=reorderer\n\nprint(\"Start loading order_products__train.csv\")\n#read train orders\nfr = open(\"../input/order_products__train.csv\", 'r', encoding = 'UTF-8')\nfr.readline()# skip header\nlines=fr.readlines()\nfor i,line in tqdm(enumerate(lines)):\n    datas=line.replace(\"\\n\",\"\").split(\",\")\n    order_id=(datas[0])\n    product_id=(datas[1])\n    reorderer=int(datas[3])\n    if(order_id not in orders):\n        orders[order_id]={}\n    orders[order_id][product_id]=reorderer\n\nfor order_id in orders:\n    reorder_list=list(orders[order_id].values())\n    if(sum(reorder_list)==0):\n        orders_isnone[order_id]=1\n    else:\n        orders_isnone[order_id]=0\n    \nprint(\"Start loading orders.csv\")\n#read orders\nfr = open(\"../input/orders.csv\", 'r', encoding = 'UTF-8')\n\nfr.readline()# skip header\nlines=fr.readlines()\nfor i,line in tqdm(enumerate(lines)):\n    datas=line.replace(\"\\n\",\"\").split(\",\")\n    order_id=(datas[0])\n    user_id=(datas[1])\n    eval_set=(datas[2])\n    order_number=(datas[3])\n    order_dow=int(datas[4])\n    order_hours=int(datas[5])\n    orders_dows[order_id]=order_dow\n    orders_hours[order_id]=order_hours\n    if(user_id not in users):\n        users[user_id]={}\n    if(eval_set==\"prior\"):\n        try:\n            days_since_prior_order=int(float(datas[6]))\n            if(user_id not in orders_days_since_prior_order):\n                orders_days_since_prior_order[user_id]=[]\n            orders_days_since_prior_order[user_id].append(days_since_prior_order)\n        except:\n            pass\n        users[user_id][order_number]=order_id\n    elif(eval_set==\"train\"):\n        users[user_id][\"train\"]=order_id\n        days_since_prior_order=int(float(datas[6]))\n        train_days_since_prior_order[order_id]=days_since_prior_order\n    elif(eval_set==\"test\"):\n        users[user_id][\"test\"]=order_id\n    elif(eval_set==\"valid\"):\n        users[user_id][\"valid\"]=order_id\n\n\nprint(\"Start creating features\")\n\nuser_buytime_mean={}\nuser_buydow_mean={}\nuser_products={}\nuser_departments={}\nuser_aisles={}\nuser_none_order_rate={}\nuser_total_products={}\nuser_order_len={}\nuser_overall_reorder={}\n\nfor user_id in tqdm(users):\n    if('train' not in list(users[user_id].keys())):\n        continue\n    user_buytime_mean[user_id]=[]\n    user_buydow_mean[user_id]=[]\n    user_products[user_id]=[]\n    user_departments[user_id]=[]\n    user_aisles[user_id]=[]\n    user_none_order_rate[user_id]=[]\n    user_order_len[user_id]=[]\n    user_total_products[user_id]=0\n    user_overall_reorder[user_id]=[]\n    for i,(order_number, order_id) in enumerate(users[user_id].items()):\n        if(order_number in [\"test\",\"train\",\"valid\"]):\n            continue\n        user_buytime_mean[user_id].append(orders_hours[order_id])\n        user_buydow_mean[user_id].append(orders_dows[order_id])\n        if(int(order_number)!=1):\n            user_none_order_rate[user_id].append(orders_isnone[order_id])\n        user_order_len[user_id].append(len(orders[order_id]))\n        for product_id in orders[order_id]:\n            user_total_products[user_id]+=1\n            if(product_id not in user_products[user_id]):\n                user_products[user_id].append(product_id)\n            \n            aisles_id=products_aisles[product_id]\n            departments_id=products_departments[product_id]\n            \n            if(departments_id not in user_departments[user_id]):\n                user_departments[user_id].append(departments_id)\n            if(aisles_id not in user_aisles[user_id]):\n                user_aisles[user_id].append(aisles_id)\n            user_overall_reorder[user_id].append(orders[order_id][product_id])\n\noutcsv = open(\"none_train_datas.csv\", 'w', encoding = 'UTF-8')\n\n\ncols=[]\ncols.append(\"user_id\")\ncols.append(\"order_id\")\n\ncols.append(\"total_product\")\ncols.append(\"total_order\")\ncols.append(\"total_distinct_product\")\ncols.append(\"totoal_dep\")\ncols.append(\"totoal_aisle\")\n\ncols.append(\"order_dow\")\ncols.append(\"order_hour\")\ncols.append(\"days_since_prior_order\")\n\ncols.append(\"overall_reorder_rate\")\ncols.append(\"none_order_rate\")\n\ncols.append(\"mean_order_dow\")\ncols.append(\"mean_order_hour\")\ncols.append(\"mean_days_since_prior_order\")\ncols.append(\"mean_basket_size\")\n\ncols.append(\"is_none\")\n\noutcsv.writelines(','.join(cols)+\"\\n\")\noutcsv.flush()\n\nprint(\"Start saving features to csv\")\nfor user_id in tqdm(users):\n    if('train' not in list(users[user_id].keys())):\n        continue\n    \n    order_id=users[user_id][\"train\"]\n    \n    features=[]\n    features.append(user_id)\n    features.append(order_id)\n    \n    features.append(user_total_products[user_id])\n    features.append(len(users[user_id])-1)\n    features.append(len(user_products[user_id]))\n    features.append(len(user_departments[user_id]))\n    features.append(len(user_aisles[user_id]))\n    \n    features.append(orders_dows[order_id])\n    features.append(orders_hours[order_id])\n    features.append(train_days_since_prior_order[order_id])\n    \n    overall_reorder_rate=sum(user_overall_reorder[user_id])/len(user_overall_reorder[user_id])\n    none_order_rate=sum(user_none_order_rate[user_id])/len(user_none_order_rate[user_id])\n    mean_order_dow=sum(user_buydow_mean[user_id])/len(user_buydow_mean[user_id])\n    mean_order_hour=sum(user_buytime_mean[user_id])/len(user_buytime_mean[user_id])\n    mean_days_since_prior_order=sum(orders_days_since_prior_order[user_id])/len(orders_days_since_prior_order[user_id])\n    mean_basket_size=sum(user_order_len[user_id])/len(user_order_len[user_id])\n    is_none=orders_isnone[order_id]\n    \n    features.append(overall_reorder_rate)\n    features.append(none_order_rate)\n    features.append(mean_order_dow)\n    features.append(mean_order_hour)\n    features.append(mean_days_since_prior_order)\n    features.append(mean_basket_size)\n    features.append(is_none)\n    \n    features_str=[]\n    for feature in features:\n        if(isinstance(feature, float)):\n            features_str.append(str(round(feature,5)))\n        else:\n            features_str.append(str(feature))\n            \n    \n    outcsv.writelines(','.join(features_str)+\"\\n\")\n    outcsv.flush()\n        "},{"cell_type":"markdown","metadata":{"_uuid":"48b1e3cf944180c4c57b7c9e1385a5418afb73e7","_cell_guid":"645cc0ea-b581-46cc-adc7-024d7e7a723c"},"source":"create_none_features_test.py"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_execution_state":"idle","_uuid":"13d2fca5aabb4a6b68bc29c3aa8f65b88f3beb3d","_cell_guid":"9e88cc8c-b368-40d2-acca-30bdd032766e","collapsed":true},"source":"from tqdm import tqdm\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport statistics\norders={}\nproducts={}\nusers={}\nproducts_aisles={}\nproducts_departments={}\norders_isnone={}\norders_dows={}\norders_hours={}\norders_days_since_prior_order={}\ntrain_days_since_prior_order={}\nprint(\"Start loading products.csv\")\nproducts_df=pd.read_csv('../input/products.csv', encoding = 'UTF-8')\nfor index, row in tqdm(products_df.iterrows()):\n    product_id=str(int(row['product_id']))\n    aisles_id=str(int(row['aisle_id']))\n    departments_id=str(int(row['department_id']))\n    products_aisles[product_id]=aisles_id\n    products_departments[product_id]=departments_id\n    \n\nprint(\"Start loading order_products__prior.csv\")\n#read prior orders\nfr = open(\"../input/order_products__prior.csv\", 'r', encoding = 'UTF-8')\nfr.readline()# skip header\nlines=fr.readlines()\nfor i,line in tqdm(enumerate(lines)):\n    datas=line.replace(\"\\n\",\"\").split(\",\")\n    order_id=(datas[0])\n    product_id=(datas[1])\n    reorderer=int(datas[3])\n    if(order_id not in orders):\n        orders[order_id]={}\n    orders[order_id][product_id]=reorderer\n\nprint(\"Start loading order_products__train.csv\")\n#read train orders\nfr = open(\"../input/order_products__train.csv\", 'r', encoding = 'UTF-8')\nfr.readline()# skip header\nlines=fr.readlines()\nfor i,line in tqdm(enumerate(lines)):\n    datas=line.replace(\"\\n\",\"\").split(\",\")\n    order_id=(datas[0])\n    product_id=(datas[1])\n    reorderer=int(datas[3])\n    if(order_id not in orders):\n        orders[order_id]={}\n    orders[order_id][product_id]=reorderer\n\nfor order_id in orders:\n    reorder_list=list(orders[order_id].values())\n    if(sum(reorder_list)==0):\n        orders_isnone[order_id]=1\n    else:\n        orders_isnone[order_id]=0\n    \nprint(\"Start loading orders.csv\")\n#read orders\nfr = open(\"../input/orders.csv\", 'r', encoding = 'UTF-8')\n\nfr.readline()# skip header\nlines=fr.readlines()\nfor i,line in tqdm(enumerate(lines)):\n    datas=line.replace(\"\\n\",\"\").split(\",\")\n    order_id=(datas[0])\n    user_id=(datas[1])\n    eval_set=(datas[2])\n    order_number=(datas[3])\n    order_dow=int(datas[4])\n    order_hours=int(datas[5])\n    orders_dows[order_id]=order_dow\n    orders_hours[order_id]=order_hours\n    if(user_id not in users):\n        users[user_id]={}\n    if(eval_set==\"prior\"):\n        try:\n            days_since_prior_order=int(float(datas[6]))\n            if(user_id not in orders_days_since_prior_order):\n                orders_days_since_prior_order[user_id]=[]\n            orders_days_since_prior_order[user_id].append(days_since_prior_order)\n        except:\n            pass\n        users[user_id][order_number]=order_id\n    elif(eval_set==\"train\"):\n        users[user_id][\"train\"]=order_id\n    elif(eval_set==\"test\"):\n        users[user_id][\"test\"]=order_id\n        days_since_prior_order=int(float(datas[6]))\n        train_days_since_prior_order[order_id]=days_since_prior_order\n    elif(eval_set==\"valid\"):\n        users[user_id][\"valid\"]=order_id\n\n\nprint(\"Start creating features\")\n\nuser_buytime_mean={}\nuser_buydow_mean={}\nuser_products={}\nuser_departments={}\nuser_aisles={}\nuser_none_order_rate={}\nuser_total_products={}\nuser_order_len={}\nuser_overall_reorder={}\n\nfor user_id in tqdm(users):\n    if('test' not in list(users[user_id].keys())):\n        continue\n    user_buytime_mean[user_id]=[]\n    user_buydow_mean[user_id]=[]\n    user_products[user_id]=[]\n    user_departments[user_id]=[]\n    user_aisles[user_id]=[]\n    user_none_order_rate[user_id]=[]\n    user_order_len[user_id]=[]\n    user_total_products[user_id]=0\n    user_overall_reorder[user_id]=[]\n    for i,(order_number, order_id) in enumerate(users[user_id].items()):\n        if(order_number in [\"test\",\"train\",\"valid\"]):\n            continue\n        user_buytime_mean[user_id].append(orders_hours[order_id])\n        user_buydow_mean[user_id].append(orders_dows[order_id])\n        if(int(order_number)!=1):\n            user_none_order_rate[user_id].append(orders_isnone[order_id])\n        user_order_len[user_id].append(len(orders[order_id]))\n        for product_id in orders[order_id]:\n            user_total_products[user_id]+=1\n            if(product_id not in user_products[user_id]):\n                user_products[user_id].append(product_id)\n            \n            aisles_id=products_aisles[product_id]\n            departments_id=products_departments[product_id]\n            \n            if(departments_id not in user_departments[user_id]):\n                user_departments[user_id].append(departments_id)\n            if(aisles_id not in user_aisles[user_id]):\n                user_aisles[user_id].append(aisles_id)\n            user_overall_reorder[user_id].append(orders[order_id][product_id])\n\noutcsv = open(\"none_test_datas.csv\", 'w', encoding = 'UTF-8')\n\n\ncols=[]\ncols.append(\"user_id\")\ncols.append(\"order_id\")\n\ncols.append(\"total_product\")\ncols.append(\"total_order\")\ncols.append(\"total_distinct_product\")\ncols.append(\"totoal_dep\")\ncols.append(\"totoal_aisle\")\n\ncols.append(\"order_dow\")\ncols.append(\"order_hour\")\ncols.append(\"days_since_prior_order\")\n\ncols.append(\"overall_reorder_rate\")\ncols.append(\"none_order_rate\")\n\ncols.append(\"mean_order_dow\")\ncols.append(\"mean_order_hour\")\ncols.append(\"mean_days_since_prior_order\")\ncols.append(\"mean_basket_size\")\n\n#cols.append(\"is_none\")\n\noutcsv.writelines(','.join(cols)+\"\\n\")\noutcsv.flush()\n\nprint(\"Start saving features to csv\")\nfor user_id in tqdm(users):\n    if('test' not in list(users[user_id].keys())):\n        continue\n    \n    order_id=users[user_id][\"test\"]\n    \n    features=[]\n    features.append(user_id)\n    features.append(order_id)\n    \n    features.append(user_total_products[user_id])\n    features.append(len(users[user_id])-1)\n    features.append(len(user_products[user_id]))\n    features.append(len(user_departments[user_id]))\n    features.append(len(user_aisles[user_id]))\n    \n    features.append(orders_dows[order_id])\n    features.append(orders_hours[order_id])\n    features.append(train_days_since_prior_order[order_id])\n    \n    overall_reorder_rate=sum(user_overall_reorder[user_id])/len(user_overall_reorder[user_id])\n    none_order_rate=sum(user_none_order_rate[user_id])/len(user_none_order_rate[user_id])\n    mean_order_dow=sum(user_buydow_mean[user_id])/len(user_buydow_mean[user_id])\n    mean_order_hour=sum(user_buytime_mean[user_id])/len(user_buytime_mean[user_id])\n    mean_days_since_prior_order=sum(orders_days_since_prior_order[user_id])/len(orders_days_since_prior_order[user_id])\n    mean_basket_size=sum(user_order_len[user_id])/len(user_order_len[user_id])\n    #is_none=orders_isnone[order_id]\n    \n    features.append(overall_reorder_rate)\n    features.append(none_order_rate)\n    features.append(mean_order_dow)\n    features.append(mean_order_hour)\n    features.append(mean_days_since_prior_order)\n    features.append(mean_basket_size)\n    #features.append(is_none)\n    \n    features_str=[]\n    for feature in features:\n        if(isinstance(feature, float)):\n            features_str.append(str(round(feature,5)))\n        else:\n            features_str.append(str(feature))\n            \n    \n    outcsv.writelines(','.join(features_str)+\"\\n\")\n    outcsv.flush()\n        "},{"cell_type":"markdown","metadata":{"_uuid":"a2f671fcd109e211f8b2bec3cec7fd138286d10e","_cell_guid":"8ffc793e-84af-4712-82f0-321bb655fd5d"},"source":"XGBoost model for None"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_execution_state":"idle","_uuid":"e128ee9c7c758ffcc17026ebd8a8796425b5ed6f","_cell_guid":"f14a8817-d10a-48ba-8a03-cbbb4788c062","collapsed":true},"source":"import numpy as np \nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\n# read datasets\ntrain = pd.read_csv('none_train_datas.csv')\ntest=pd.read_csv('none_test_datas.csv')\norder_id=test['order_id']\n\ny_train = train[\"is_none\"]\ny_mean = np.mean(y_train)\n       \nprint('Shape train: {}\\n Shape test: {}'.format(train.shape,test.shape))\n\nimport xgboost as xgb\n\nxgb_params = {\n    'eta': 0.005,\n    'max_depth': 6,\n    'subsample': 0.8,\n    'objective': 'reg:linear',\n    'eval_metric': 'logloss',\n    'base_score': y_mean,\n    'silent': 1\n}\n\ndtrain = xgb.DMatrix(train.drop(['user_id','order_id','is_none'], axis=1), y_train)\ndtest = xgb.DMatrix(test.drop(['user_id','order_id'], axis=1))\n'''\ncv_result = xgb.cv(xgb_params, \n                   dtrain, \n                   nfold=10,\n                   num_boost_round=1500, # increase to have better results (~700)\n                   early_stopping_rounds=50,\n                   verbose_eval=50, \n                   show_stdv=False\n                  )\n'''\n#num_boost_rounds = len(cv_result)\n#print(num_boost_rounds)\nnum_boost_rounds=800\nmodel = xgb.train(dict(xgb_params, silent=1), dtrain, num_boost_round=num_boost_rounds)\npreds=model.predict(dtest)\n\nout = pd.DataFrame({'order_id': order_id, 'none_pred': preds})\nout.to_csv('test_none_pred.csv', index=False)"},{"cell_type":"markdown","metadata":{"_uuid":"359e31622de74b950713d4b7ad0f079dfd9a98e2","_cell_guid":"3cba7189-99e1-4a02-b319-4d8c7e295bff"},"source":"### Futher more\nMany people had share their solusion on forum, you can learn from them:\n\n1. [3rd-Place Solution Overview](https://www.kaggle.com/c/instacart-market-basket-analysis/discussion/38097), by sjv\n2. [4-th Place Tips](https://www.kaggle.com/c/instacart-market-basket-analysis/discussion/38102), by GeorgeGui\n3. [9th place Approach](https://www.kaggle.com/c/instacart-market-basket-analysis/discussion/38100), by KazAnova\n4. [SQL feature engineering + XGBoost (.4026 private LB, 1/2 of #100 solution)](https://www.kaggle.com/c/instacart-market-basket-analysis/discussion/38105), by happycube\n5. [Top-30 Silver Hints](https://www.kaggle.com/c/instacart-market-basket-analysis/discussion/38122), by Fred Navruzov\n6. [6th place solution overview](https://www.kaggle.com/c/instacart-market-basket-analysis/discussion/38112), by Akulov Yaroslav\n7. [#11 Solution](https://www.kaggle.com/c/instacart-market-basket-analysis/discussion/38126), by zr\n8. [12th solution](https://www.kaggle.com/c/instacart-market-basket-analysis/discussion/38110), by plantsgo\n9. [2nd Place Solution](https://www.kaggle.com/c/instacart-market-basket-analysis/discussion/38143), by ONODERA"},{"cell_type":"markdown","metadata":{"_uuid":"089f40b5cd00615150824554e309774a0ec7cd5b","_cell_guid":"6c8a6f07-7525-42f2-b691-4bc3d9893fb4"},"source":"Thanks for reading, I will be happy if this kernel helps someone. See you next competition!"}],"nbformat_minor":1}