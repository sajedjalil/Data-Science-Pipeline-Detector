{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Top-$N$ Random Forest Model\n\nIn this notebook we construct a random forest classifier to compute probabilities of ultimate purchase for all previously purchased user-product pairs. Next, we select top probability scores to provide recommendations using top-$N$ variants for different use cases."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\n\npd.options.display.latex.repr=True","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Load Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"store = pd.HDFStore('../input/instacart-feature-engineering/io.h5', 'r')\nstore.open()\nstore.keys()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dsets = ['train', 'test', 'kaggle']\n\nX = dict.fromkeys(dsets)\ny = dict.fromkeys(dsets)\n\nfor ds in dsets:\n    X[ds] = store['/X/' + str(ds)]\n    y[ds] = store['/y/' + str(ds)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"store.close()\nstore.is_open","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest Classifier\nInitialize and fit the random forest classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import necessary modules\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nimport datetime\nfrom scipy.stats import randint\n\nfrom sklearn import metrics\nfrom sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize and fit classifier\nrfc = RandomForestClassifier(n_estimators=600,\n                             max_features='sqrt',\n                             min_impurity_decrease=3e-7,\n                             min_samples_leaf=24,\n                             n_jobs=-1,\n                             random_state=20190603,\n                             oob_score=True,\n                             warm_start=True)\n\nprint(datetime.datetime.now())\n\nrfc.fit(X['train'], y['train'].values.ravel())\n\nprint(datetime.datetime.now())\n\noob_error = 1 - rfc.oob_score_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a look at the node sizes of the trees."},{"metadata":{"trusted":true},"cell_type":"code","source":"# tree_.node_count estimators\n\nnode_counts = [rfc.estimators_[i].tree_.node_count for i in range(len(rfc.estimators_))]\n\nplt.hist(node_counts)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,12))\nsns.barplot(data=(pd.DataFrame(data=rfc.feature_importances_,\n           index=X['train'].columns)\n                  .reset_index()\n                  .sort_values(by=0,ascending=False)\n                  .rename(columns = {'index': 'feature',\n                                     0: 'feature_importance'}\n                         )\n                 ),\n            x='feature_importance',\n            y='feature'\n           )\nplt.title('RandomForestClassifier feature_importances_')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The `feature_importances_` attribute shows that one of the most fruitful avenues to pursue to improve classifier performance is to focus on manually creating additional user-product features which capture more complex user-product interactions."},{"metadata":{},"cell_type":"markdown","source":"## Random Forest Classifier Metrics\n\nThe data has an imbalance on the order of $\\text{Skew} = 10$. Since any given basket typically has far fewer items than the total previous products a user has purchased, there are many more actual negatives than actual positives. The ease of finding negatives inflates the AUC score; the precision-recall curve shows the classifier has predictive power only for larger classification probability threshold values. [Reference](https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a dictionary of metrics to compute multiple scores\n\nfrom imblearn.metrics import geometric_mean_score\n\nmetrics_dict = {}\n\nmetrics_dict['auc_roc'] = {'fcn' : metrics.roc_auc_score,\n                        'name': 'AUC-ROC',\n                        'thr' : False}\n\nmetrics_dict['auc_pr'] = {'fcn' : metrics.average_precision_score,\n                        'name': 'AUC-PR',\n                        'thr' : False}\n\nmetrics_dict['log_loss'] = {'fcn' : metrics.log_loss,\n                        'name': 'Log Loss',\n                        'thr' : False}\n\nmetrics_dict['prec'] = {'fcn' : metrics.precision_score,\n                        'name': 'Precision',\n                        'thr' : True}\n\nmetrics_dict['rec'] = {'fcn' : metrics.recall_score,\n                        'name': 'Recall',\n                        'thr' : True}\n\nmetrics_dict['f1'] = {'fcn' : metrics.f1_score,\n                        'name': 'F1 Score',\n                        'thr' : True}\n\nmetrics_dict['bal_acc'] = {'fcn' : metrics.balanced_accuracy_score,\n                        'name': 'Balanced Accuracy',\n                        'thr' : True}\n\nmetrics_dict['g_mean'] = {'fcn' : geometric_mean_score,\n                        'name': 'Geometric Mean',\n                        'thr' : True}\n\nmetrics_dict['kappa'] = {'fcn' : metrics.cohen_kappa_score,\n                        'name': 'Cohen\\'s Kappa',\n                        'thr' : True}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# oob scores\ny_score = rfc.oob_decision_function_\n\n# predictions\ny_predict_binary = dict.fromkeys(dsets)\ny_predict_proba = dict.fromkeys(dsets)\ny_predict_proba_df = dict.fromkeys(dsets)\n\nfor ds in dsets:\n    # binary predictions\n    y_predict_binary[ds] = rfc.predict(X[ds])\n\n    # probability of True\n    y_predict_proba[ds] = rfc.predict_proba(X[ds])[:, 1]\n\n    # True probabilities as Series\n    y_predict_proba_df[ds] = pd.Series(data=y_predict_proba[ds],\n                                         index=X[ds].index)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ROC Curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr = dict.fromkeys(dsets)\ntpr = dict.fromkeys(dsets)\nroc_auc = dict.fromkeys(dsets)\n\nfor ds in dsets[:2]:\n    fpr[ds], tpr[ds], _ = metrics.roc_curve(y[ds], y_predict_proba[ds])\n    roc_auc[ds] = metrics.roc_auc_score(y[ds], y_predict_proba[ds])\n\nplt.figure(figsize=(14,10))\nlw = 2\nplt.plot(fpr['train'], tpr['train'], color='blue',\n         lw=lw, label='train (AUC = %0.2f)' % roc_auc['train'])\nplt.plot(fpr['test'], tpr['test'], color='darkorange',\n         lw=lw, label='test (AUC = %0.2f)' % roc_auc['test'])\nplt.plot([0, 1], [0, 1], color='black', lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Precision-Recall Curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"# adapted from sklearn docs\nfrom sklearn.utils.fixes import signature\n\nprecision, recall, _ = metrics.precision_recall_curve(y['test'], y_predict_proba['test'])\n\naverage_precision = metrics.average_precision_score(y['test'], y_predict_proba['test'])\n\n# # Iso-F1\n# for i in np.linspace(0.2, 0.9, 7, endpoint=False):\n#     plt.plot(xs, xs * i / (2 * xs - i), color='navy')\n\nplt.figure(figsize=(14,10))\n\n# In matplotlib < 1.5, plt.fill_between does not have a 'step' argument\nstep_kwargs = ({'step': 'post'}\n               if 'step' in signature(plt.fill_between).parameters\n               else {})\nplt.step(recall, precision, color='b', alpha=0.2,\n         where='post')\nplt.fill_between(recall, precision, alpha=0.2, color='r', **step_kwargs)\n\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.0])\nplt.xlim([0.0, 1.0])\nplt.title('2-class Precision-Recall curve: AUC-PR={0:0.2f}'.format(\n          average_precision))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Top-$N_\\text{threshold}$ Variants\n\nIf we choose the $N_\\text{threshold}$ user-product pairs $(u,p)$ with the greatest $P((u,p))$, then this top-$N$ variant is a reparamaterization of the classification threshold, $p_0$, via\n$$N_\\text{threshold} =  \\left\\{ (u,p) \\ | \\ P((u,p)) > p_0 \\right\\}.$$\n\nAn advantage of this variant is that it recommends the items users are, in aggregate, most likely to purchase. Therefore, we can, overall, make the 'best' recommendations. A disadvantage is that there is a lot variation in the number of recommended products.\n\nA few potentially principled choices for $N$ follow:"},{"metadata":{},"cell_type":"markdown","source":"### $N_{0.5}$\n\nA direct interpretation of `y_predict_proba` in terms of probabilities would make a threshold of $p_0 = 0.5$ the most principled choice. On the other hand, choices made in the model definition and implementation could make this interpretation dubious. Nonetheless, we can define $$N_{0.5} = \\left\\{ (u,p) \\ | \\ P((u,p)) > 0.5 \\right\\}$$ to take a look at scores."},{"metadata":{},"cell_type":"markdown","source":"### $N_\\text{skew}$\n\nIt may instead be most principled to choose $N$ such that the skew of predictions equals the `train` skew (negative classes divided by positive classes). Let $y_\\oplus$ be the count of positive classes (`train`) and $y_\\ominus$ be the count of negative classes (`train`). We compute `skew_train` as $\\mathrm{skew} = y_\\ominus / y_\\oplus$. For $s \\in \\mathrm{DSets}$, let $n_s = |I_s|$, the length of the multiindex of user-product pairs. Define\n$$N_{\\text{skew}_s} := n_s * P(y_\\oplus) = \\frac{n_s}{1+\\mathrm{skew}},$$\nwhere $$P(y_\\oplus) = \\frac{y_\\oplus}{y_\\oplus + y_\\ominus}$$\nis the ratio of positive classes in `train` to the length of `train`."},{"metadata":{"trusted":true},"cell_type":"code","source":"skew_train = ((y['train'].count() - y['train'].sum())/y['train'].sum())\n\nprint('The skew of the training data set is skew_train = %.2f.' % skew_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### $N_\\text{basket}$\n\nA couple other values we may consider to be principled are\n$$ N_\\text{basket} = \\sum_{u \\in U} \\overline{ b(u) }$$\n$$ N_\\text{basket reorder} = \\sum_{u \\in U} \\overline{ r(u) }$$\nwhere $\\overline{b(u)}$ is the mean basket size for user $u$ and $\\overline{ r(u)}$ is the mean reordered items per basket for user $u$."},{"metadata":{},"cell_type":"markdown","source":"### Prediction Vectors"},{"metadata":{"trusted":true},"cell_type":"code","source":"variant_group = 'N_threshold'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute N\n\nN_05 = dict.fromkeys(dsets)\nN_skew = dict.fromkeys(dsets)\nN_basket = dict.fromkeys(dsets)\nN_basket_reorder = dict.fromkeys(dsets)\n\nfor ds in dsets:\n    N_05[ds] = y_predict_binary[ds].sum()\n    \n    N_skew[ds] = int(len(y_predict_proba_df[ds]) / (1 + skew_train))    \n    \n    N_basket[ds] = int(X[ds].U_order_size_mean\n                        .groupby('user_id')\n                        .max()\n                        .astype('float64')\n                        .sum())\n    \n    N_basket_reorder[ds] = int(X[ds].U_reorder_size_mean\n                        .groupby('user_id')\n                        .max()\n                        .astype('float64')\n                        .sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute binary prediction vectors\n\ny_topN_05 = dict.fromkeys(dsets)\ny_topN_skew = dict.fromkeys(dsets)\ny_topN_basket = dict.fromkeys(dsets)\ny_topN_basket_reorder = dict.fromkeys(dsets)\n\nfor ds in dsets:\n    y_topN_05[ds] = pd.Series(data=y_predict_binary[ds],\n                              index=X[ds].index)\n    \n    y_topN_skew[ds] = pd.Series(data=True,\n                         index=y_predict_proba_df[ds]\n                         .nlargest(N_skew[ds])\n                         .index).reindex_like(X[ds]).fillna(False)  \n        \n    y_topN_basket[ds] = pd.Series(data=True,\n                         index=y_predict_proba_df[ds]\n                         .nlargest(N_basket[ds])\n                         .index).reindex_like(X[ds]).fillna(False)\n\n    y_topN_basket_reorder[ds] = pd.Series(data=True,\n                         index=y_predict_proba_df[ds]\n                         .nlargest(N_basket_reorder[ds])\n                         .index).reindex_like(X[ds]).fillna(False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# proba thresholds (test)\n\np_05 = dict.fromkeys(dsets)\np_skew = dict.fromkeys(dsets)\np_basket = dict.fromkeys(dsets)\np_basket_reorder = dict.fromkeys(dsets)\n\nfor ds in dsets:\n    p_05[ds] = 0.5\n    p_skew[ds] = float(y_predict_proba_df[ds]\n                       .nlargest(N_skew[ds])\n                       .tail(1))\n    p_basket[ds] = float(y_predict_proba_df[ds]\n                         .nlargest(N_basket[ds])\n                         .tail(1))\n    p_basket_reorder[ds] = float(y_predict_proba_df[ds]\n                                 .nlargest(N_basket_reorder[ds])\n                                 .tail(1))    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a dictionary of the predictions of various variants\n\nfrom collections import defaultdict\n\nvariants_dict = defaultdict(dict)\n\nvariants_dict[variant_group]['N_05'] = {\n    'N' : N_05,\n    'y' : y_topN_05,\n    'p' : p_05\n}\n\nvariants_dict[variant_group]['N_skew'] = {\n    'N' : N_skew,\n    'y' : y_topN_skew,\n    'p' : p_skew\n}\n\nvariants_dict[variant_group]['N_basket'] = {\n    'N' : N_basket,\n    'y' : y_topN_basket,\n    'p' : p_basket\n}\n\nvariants_dict[variant_group]['N_basket_reorder'] = {\n    'N' : N_basket_reorder,\n    'y' : y_topN_basket_reorder,\n    'p' : p_basket_reorder\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Inspect $N$ and $p$ values"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.concat(\n    {\n        ds: pd.DataFrame({\n            variant: {\n                col: variants_dict[variant_group][variant][col][ds]\n                for col in variants_dict[variant_group][variant].keys()\n            }\n            for variant in variants_dict[variant_group].keys()\n        }).transpose()[['N', 'p']].assign(\n            N_frac=lambda x: x['N'] / len(y[ds])).sort_index(axis=1)\n        for ds in dsets[:2]\n    },\n    axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Inspect Basket Sizes ('train'):"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.concat([\n    y_topN_05['train'].groupby('user_id').sum()\n    .astype('int').rename('top_N_05_pred'),\n    y_topN_skew['train'].groupby('user_id').sum()\n    .astype('int').rename('top_N_skew_pred'),\n    y_topN_basket['train'].groupby('user_id').sum()\n    .astype('int').rename('top_N_basket_pred'),\n    y_topN_basket_reorder['train'].groupby('user_id').sum()\n    .astype('int').rename('top_N_basket_reorder_pred'),\n    X['train'].U_order_size_mean.groupby('user_id').max(),\n    X['train'].U_reorder_size_mean.groupby('user_id').max(),\n],\naxis=1).head(15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is a good deal of variation in basket size prediction and sometimes a considerable mismatch between predictions and mean basket sizes. This may be a reason for the threshold variant to be of less practical relevance from a product standpoint."},{"metadata":{},"cell_type":"markdown","source":"### Scores"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make a dataframe with one row of threshold metrics to score y_pred from\n# a dictionary of metrics of the form metrics_dict\n\ndef scores_dataframe(y_true_dict, y_pred_dict, metrics_dict, col_name=0):\n    \n    cols = []\n    scores = []\n\n    for ds in dsets[:2]:\n        for key, metric in metrics_dict.items():\n            if metric['thr'] == True:\n                cols.append((ds, metric['name']))\n                scores.append(metric['fcn'](y_true_dict[ds].values.ravel(),\n                                            y_pred_dict[ds].values.ravel()))\n\n    return pd.DataFrame(data=[scores],\n                        columns=pd.MultiIndex.from_tuples(cols, names=['dset', 'metric']),\n                        index=[col_name])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,8))\nplt.title('Scores for ' + variant_group + ' variants')\nsns.heatmap(\npd.concat(\n    [scores_dataframe(y, val['y'], metrics_dict, key)['test']\n    for key, val in variants_dict[variant_group].items()]),\n    annot=True,\n    fmt='.3f',\n    cmap=\"OrRd\"\n)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note the heatmap colors are only comparable *within columns*.\n\nSince $N_\\text{basket}$ is greater than $N_\\text{basket reorder}$, the latter choice has greater precision, while $N_\\text{skew}$ balances precision and recall. We may not necessarily care about high precision scores. Avoiding false negatives is not an issue if we want to be more liberal in recommendations."},{"metadata":{},"cell_type":"markdown","source":"### Confusion Matrices"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion matrix in a standard format\n\ndef make_confusion_df(y_test, y_pred):\n    cm = metrics.confusion_matrix(\n        y_test, y_pred\n    )\n\n    return (pd.DataFrame(data=cm)\n            .iloc[::-1,::-1]\n            .rename_axis('Predicted Label')\n            .rename_axis('True Label', axis=1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make a normed confusion matrix (recall version)\n# for variant in the keys of variant_group\n# and 'test' set and y_pred_dict of form variants_dict\n\ndef make_norm_confusion_df(y_test, y_pred_dict, variant_group, variant):\n    cm = metrics.confusion_matrix(\n        y_test,\n        y_pred_dict[variant_group][variant]['y']['test']\n    )\n    \n    long_form_matrix_df = (pd.DataFrame(\n        data=(cm / (cm.sum(axis=1)[:, np.newaxis])))\n            .iloc[::-1,::-1]\n            .rename_axis('Predicted Label')\n            .rename_axis('True Label', axis=1)\n    .reset_index()\n    .melt(id_vars=['Predicted Label'])\n    .assign(variant=variant))\n    \n    cols = long_form_matrix_df.columns.tolist()\n    cols = cols[-1:] + cols[:-1]\n    long_form_matrix_df = long_form_matrix_df[cols]\n    \n    return long_form_matrix_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Concatenate long-form confusion matrices in single df\n\ndef combine_norm_confusion_df(y_test, y_pred_dict, variant_group):\n    return pd.concat(\n    [make_norm_confusion_df(y_test, y_pred_dict, variant_group, variant)\n    for variant in variants_dict[variant_group].keys()]\n    )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[SO on normalized confusion](https://stackoverflow.com/a/20934655)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot function for FacetGrid use:\n# From long format df (kw-packed), pivot and plot heatmap\n\ndef draw_heatmap(index, columns, values, **kwargs):\n    data = kwargs.pop('data')\n    d = data.pivot(index=index,\n                   columns=columns,\n                   values=values).iloc[::-1,::-1]\n    sns.heatmap(d, **kwargs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create long ('tidy') DataFrame of confusion matrices\n\nconfusion_matrices_long = combine_norm_confusion_df(y['test'], variants_dict, variant_group)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot normalized confusion heatmaps on FacetGrid\n\nfg = sns.FacetGrid(confusion_matrices_long,\n                   col='variant',\n                   col_wrap=2,\n                   height=5)\n\nfg.map_dataframe(draw_heatmap,\n                 index='Predicted Label', \n                 columns='True Label',\n                 values='value',\n                 square=True,\n                 annot=True,\n                 fmt='0.3f',\n                 cmap='Blues')\n\nfg.set_axis_labels('True Label', 'Predicted Label')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Top-$N(u)$ Variant\nIf we want to avoid the high variation of basket sizes in the top-$N_\\text{total}$ variant, we can define ultimate basket sizes by user. Let\n$$N(u) = \\left\\lceil \\overline{b(u)} \\right\\rceil$$\n$$N_\\text{reorder}(u) = \\left\\lceil \\overline{r(u)} \\right\\rceil$$\nfor all $u \\in U$, that is, the ceiling of the mean (re)orders per basket for user $u$. Using the ceiling rather than round or floor offers the advantage of recommending at least one item to users in a straight-forward way. The mean reorders per basket is biased even as a predictor of ultimate *re*orders since user's initial orders have zero reorders. On the other hand, since we are predicting fewer positives, the precision of $N_\\text{reorder}(u)$ is better.\n\nThese kinds of variants, or those which offer control of ultimate basket size while offering more flexibility in that control, may be useful for product applications like auto-populating user carts with items we most expect users to reorder. For such an application we would prefer models with higher precision."},{"metadata":{"trusted":true},"cell_type":"code","source":"variant_group = 'N_u'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute N_u\n\nN_u = dict.fromkeys(dsets)\nN_u_reorder = dict.fromkeys(dsets)\n\nfor ds in dsets:\n    N_u[ds] = (X[ds].U_order_size_mean\n                        .groupby('user_id')\n                        .max()\n                        .apply(np.ceil)\n                        .astype('uint8'))\n\n    N_u_reorder[ds] = (X[ds].U_reorder_size_mean\n                        .groupby('user_id')\n                        .max()\n                        .apply(np.ceil)\n                        .astype('uint8'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This [top-$N(u)$ `groupby()` reference](https://stackoverflow.com/questions/37482733/selecting-top-n-elements-from-each-group-in-pandas-groupby) was helpful."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute binary prediction vectors\n\ny_top_N_u = dict.fromkeys(dsets)\ny_top_N_u_reorder = dict.fromkeys(dsets)\n\nfor ds in dsets:\n    y_top_N_u[ds] = (pd.Series(data=True,\n                          index=y_predict_proba_df[ds].groupby('user_id')\n                          .apply(lambda gp: gp.nlargest(N_u[ds].loc[gp.name]))\n                          .reset_index(level=1, drop=True).index)\n                 .reindex_like(X[ds])\n                 .fillna(False))\n    y_top_N_u_reorder[ds] = (pd.Series(data=True,\n                      index=y_predict_proba_df[ds].groupby('user_id')\n                      .apply(lambda gp: gp.nlargest(N_u_reorder[ds].loc[gp.name]))\n                      .reset_index(level=1, drop=True).index)\n             .reindex_like(X[ds])\n             .fillna(False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Note this is not a threshold variant so there are no 'p' keys\n\nvariants_dict[variant_group]['N_u'] = {\n    'N' : N_u,\n    'y' : y_top_N_u\n}\n\nvariants_dict[variant_group]['N_u_reorder'] = {\n    'N' : N_u_reorder,\n    'y' : y_top_N_u_reorder\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Inspect Basket Sizes\n\nNote this variant group is defined by *setting* basket sizes to user mean basket sizes:"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.concat([y_top_N_u['train'].groupby('user_id').sum().rename('top_N_u_pred'),\n           y_top_N_u_reorder['train'].groupby('user_id').sum().rename('top_N_reorder_u_pred'),\n           X['train'].U_order_size_mean.groupby('user_id').max(),\n           X['train'].U_reorder_size_mean.groupby('user_id').max()\n          ],\n          axis=1).head(15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Of course now there is no difference between mean basket sizes and predictions. Although we have not (yet) created a variant which offers a compromise between the high variation in differences of mean basket size and prediction of `N_threshold` and the (near) zero variation in differences of mean basket size and prediction of `N_u`, there are many such variants which would be straight-forward to construct."},{"metadata":{},"cell_type":"markdown","source":"### Scores"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,4))\nplt.title('Scores for ' + variant_group + ' variants')\nsns.heatmap(\npd.concat(\n    [scores_dataframe(y, val['y'], metrics_dict, key)['test']\n    for key, val in variants_dict[variant_group].items()]),\n    annot=True,\n    fmt='.3f',\n    cmap=\"OrRd\"\n)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Confusion Matrices"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create long ('tidy') DataFrame of confusion matrices\n\nconfusion_matrices_long = combine_norm_confusion_df(y['test'], variants_dict, variant_group)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot normalized confusion heatmaps on FacetGrid\n\nfg = sns.FacetGrid(confusion_matrices_long,\n                   col='variant',\n                   col_wrap=2,\n                   height=5)\n\nfg.map_dataframe(draw_heatmap,\n                 index='Predicted Label', \n                 columns='True Label',\n                 values='value',\n                 square=True,\n                 annot=True,\n                 fmt='0.3f',\n                 cmap='Blues')\n\nfg.set_axis_labels('True Label', 'Predicted Label')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Top-$N$\n\nWhile the top-$N_u$ model gives better predictions, a top-$N$ model with a fixed $N$ for all users may be useful, for example, in displaying previously purchased product recommendations on a web page of fixed size. Below are scores for $N \\in \\left\\{4, 8, 12, 16, 20, 24\\right\\}$. An application like this one would prefer higher recall models."},{"metadata":{"trusted":true},"cell_type":"code","source":"variant_group = 'top_N'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define Ns\n\nNs = list(range(4,25,4))\n\ny_top = []\nfor N in Ns:\n    y_top.append(dict.fromkeys(dsets))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute top-N predictions and add to variants_dict\n\nfor N in Ns:\n    for ds in dsets:\n        y_top[Ns.index(N)][ds] = (pd.Series(data=True,\n                          index=y_predict_proba_df[ds].groupby('user_id')\n                          .nlargest(N)\n                          .reset_index(level=1, drop=True).index)\n                 .reindex_like(X[ds])\n                 .fillna(False))\n        \n        variants_dict[variant_group][N] = {\n            'N' : N,\n            'y' : y_top[Ns.index(N)]\n        }","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Scores"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,12))\nplt.title('Scores for ' + variant_group + ' variants')\nsns.heatmap(\npd.concat(\n    [scores_dataframe(y, val['y'], metrics_dict, key)['test']\n    for key, val in variants_dict[variant_group].items()]),\n    annot=True,\n    fmt='.3f',\n    cmap=\"OrRd\"\n)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Confusion Matrices"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create long ('tidy') DataFrame of confusion matrices\n\nconfusion_matrices_long = combine_norm_confusion_df(y['test'], variants_dict, variant_group)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot normalized confusion heatmaps on FacetGrid\n\nfg = sns.FacetGrid(confusion_matrices_long,\n                   col='variant',\n                   col_wrap=3,\n                   height=4)\n\nfg.map_dataframe(draw_heatmap,\n                 index='Predicted Label', \n                 columns='True Label',\n                 values='value',\n                 square=True,\n                 annot=True,\n                 fmt='0.3f',\n                 cmap='Blues')\n\nfg.set_axis_labels('True Label', 'Predicted Label')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Kaggle Output (sanity check)"},{"metadata":{},"cell_type":"markdown","source":"Additional threshold choices for Kaggle submission:"},{"metadata":{"trusted":true},"cell_type":"code","source":"variant_group = 'p_kaggle'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute kaggle threshold predictions and add to variants_dict\n\np_0s = list(np.round(np.linspace(0.192, 0.198, num=4), 3))\n\ny_kaggle = []\n\nfor p_0 in p_0s:\n    for ds in dsets:\n        y_kaggle.append(dict.fromkeys(dsets))\n        y_kaggle[p_0s.index(p_0)][ds] = pd.Series((y_predict_proba_df[ds] >= p_0))\n        \n        variants_dict[variant_group][p_0] = {\n            'y' : y_kaggle[p_0s.index(p_0)]\n        }","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Scores"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,10))\nplt.title('Scores for ' + variant_group + ' variants')\nsns.heatmap(\npd.concat(\n    [scores_dataframe(y, val['y'], metrics_dict, key)['test']\n    for key, val in variants_dict[variant_group].items()])\n    .sort_index(),\n    annot=True,\n    fmt='.3f',\n    cmap=\"OrRd\"\n)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Confusion Matrices"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create long ('tidy') DataFrame of confusion matrices\n\nconfusion_matrices_long = combine_norm_confusion_df(y['test'], variants_dict, variant_group)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot normalized confusion heatmaps on FacetGrid\n\nfg = sns.FacetGrid(confusion_matrices_long,\n                   col='variant',\n                   col_wrap=2,\n                   height=5)\n\nfg.map_dataframe(draw_heatmap,\n                 index='Predicted Label', \n                 columns='True Label',\n                 values='value',\n                 square=True,\n                 annot=True,\n                 fmt='0.3f',\n                 cmap='Blues')\n\nfg.set_axis_labels('True Label', 'Predicted Label')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Output all predictions to .csv\n\nfor var_gp_name, var_gp in variants_dict.items():\n    for var_name, var in var_gp.items():\n        for ds in dsets:\n            pd.DataFrame(data=var['y'][ds],\n                         index=X[ds].index\n                        ).to_csv(str(var_gp_name) + '-' + str(var_name) + '-' + str(ds) + '.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"latex_envs":{"LaTeX_envs_menu_present":true,"autoclose":true,"autocomplete":true,"bibliofile":"biblio.bib","cite_by":"apalike","current_citInitial":1,"eqLabelWithNumbers":true,"eqNumInitial":1,"hotkeys":{"equation":"ctrl-meta-e"},"labels_anchors":false,"latex_user_defs":false,"report_style_numbering":false,"user_envs_cfg":false},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":true}},"nbformat":4,"nbformat_minor":1}