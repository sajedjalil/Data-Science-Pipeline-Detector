{"cells":[{"metadata":{},"cell_type":"markdown","source":"(all code in this example are from the notebook : https://www.kaggle.com/abhinand05/bert-for-humans-tutorial-baseline/notebook by [abhinand05](https://www.kaggle.com/abhinand05))","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Problematic\n\nThe code below present a machine learning model for text understanding. We use a fine-tuning mecanism of BERT model to get a response to a generic question about an article.\nIt's an extractive approach that extract the response from the article using a start and end position. two responses are possible : a large and small one.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Code Implementation on TSF 2.0","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Qute from the original notebook : \n\n> In this notebook, we'll be using the Bert baseline for Tensorflow to create predictions for the Natural Questions test set. Note that this uses a model that has already been pre-trained - we're only doing inference here. A GPU is required, and this should take between 1-2 hours to run.\n> \n> The original script can be found [here](https://github.com/google-research/language/blob/master/language/question_answering/bert_joint/run_nq.py).\n> The supporting modules were drawn from the [official Tensorflow model repository](https://github.com/tensorflow/models/tree/master/official). The bert-joint-baseline data is described [here](https://github.com/google-research/language/tree/master/language/question_answering/bert_joint).\n\n\nThe most code below is from the original notefook (link on thte top), However I a review it, add comment, fixe issues and make it more readebale.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nImport all needed Python Package\n\"\"\"\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\n\n# this is translation of BERT in TensorFlow 2.0, made by dimitre oliveira (https://www.kaggle.com/dimitreoliveira)\nimport tf2_0_baseline_w_bert_translated_to_tf2_0 as tf2baseline # Oliviera's script\n\nimport bert_modeling as modeling\nimport bert_optimization as optimization\nimport bert_tokenization as tokenization\n\nimport json\nimport absl\nimport sys\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def del_all_flags(FLAGS):\n    flags_dict = FLAGS._flags()\n    keys_list = [keys for keys in flags_dict]\n    for keys in keys_list:\n        FLAGS.__delattr__(keys)\n\n\"\"\"\nDelete All TF Flags that are already defined\n\"\"\"        \ndel_all_flags(absl.flags.FLAGS)\n\n\n\"\"\"\nDefine TF flags, that represent variables used by our system and model\n\nNote :  All thoses flags can be changed as desired during the code\n\"\"\"\n\nflags = absl.flags\n\nflags.DEFINE_string(\n    \"bert_config_file\", \"/kaggle/input/bertjointbaseline/bert_config.json\",\n    \"The config json file corresponding to the pre-trained BERT model. \"\n    \"This specifies the model architecture.\")\n\nflags.DEFINE_string(\"vocab_file\", \"/kaggle/input/bertjointbaseline/vocab-nq.txt\",\n                    \"The vocabulary file that the BERT model was trained on.\")\n\nflags.DEFINE_string(\n    \"output_dir\", \"outdir\",\n    \"The output directory where the model checkpoints will be written.\")\n\nflags.DEFINE_string(\"train_precomputed_file\", None,\n                    \"Precomputed tf records for training.\")\n\nflags.DEFINE_integer(\"train_num_precomputed\", None,\n                     \"Number of precomputed tf records for training.\")\n\nflags.DEFINE_string(\n    \"output_prediction_file\", \"predictions.json\",\n    \"Where to print predictions in NQ prediction format, to be passed to\"\n    \"natural_questions.nq_eval.\")\n\nflags.DEFINE_string(\n    \"init_checkpoint\", \"/kaggle/input/bertjointbaseline/bert_joint.ckpt\",\n    \"Initial checkpoint (usually from a pre-trained BERT model).\")\n\nflags.DEFINE_bool(\n    \"do_lower_case\", True,\n    \"Whether to lower case the input text. Should be True for uncased \"\n    \"models and False for cased models.\")\n\nflags.DEFINE_integer(\n    \"max_seq_length\", 384,\n    \"The maximum total input sequence length after WordPiece tokenization. \"\n    \"Sequences longer than this will be truncated, and sequences shorter \"\n    \"than this will be padded.\")\n\nflags.DEFINE_integer(\n    \"doc_stride\", 128,\n    \"When splitting up a long document into chunks, how much stride to \"\n    \"take between chunks.\")\n\nflags.DEFINE_integer(\n    \"max_query_length\", 64,\n    \"The maximum number of tokens for the question. Questions longer than \"\n    \"this will be truncated to this length.\")\n\nflags.DEFINE_bool(\"do_train\", False, \"Whether to run training.\")\n\nflags.DEFINE_bool(\"do_predict\", True, \"Whether to run eval on the dev set.\")\n\nflags.DEFINE_integer(\"train_batch_size\", 32, \"Total batch size for training.\")\n\nflags.DEFINE_integer(\"predict_batch_size\", 8,\n                     \"Total batch size for predictions.\")\n\nflags.DEFINE_float(\"learning_rate\", 5e-5, \"The initial learning rate for Adam.\")\n\nflags.DEFINE_float(\"num_train_epochs\", 3.0,\n                   \"Total number of training epochs to perform.\")\n\nflags.DEFINE_float(\n    \"warmup_proportion\", 0.1,\n    \"Proportion of training to perform linear learning rate warmup for. \"\n    \"E.g., 0.1 = 10% of training.\")\n\nflags.DEFINE_integer(\"save_checkpoints_steps\", 1000,\n                     \"How often to save the model checkpoint.\")\n\nflags.DEFINE_integer(\"iterations_per_loop\", 1000,\n                     \"How many steps to make in each estimator call.\")\n\nflags.DEFINE_integer(\n    \"n_best_size\", 20,\n    \"The total number of n-best predictions to generate in the \"\n    \"nbest_predictions.json output file.\")\n\nflags.DEFINE_integer(\n    \"verbosity\", 1, \"How verbose our error messages should be\")\n\nflags.DEFINE_integer(\n    \"max_answer_length\", 30,\n    \"The maximum length of an answer that can be generated. This is needed \"\n    \"because the start and end predictions are not conditioned on one another.\")\n\nflags.DEFINE_float(\n    \"include_unknowns\", -1.0,\n    \"If positive, probability of including answers of type `UNKNOWN`.\")\n\nflags.DEFINE_bool(\"use_tpu\", False, \"Whether to use TPU or GPU/CPU.\")\nflags.DEFINE_bool(\"use_one_hot_embeddings\", False, \"Whether to use use_one_hot_embeddings\")\n\nabsl.flags.DEFINE_string(\n    \"gcp_project\", None,\n    \"[Optional] Project name for the Cloud TPU-enabled project. If not \"\n    \"specified, we will attempt to automatically detect the GCE project from \"\n    \"metadata.\")\n\nflags.DEFINE_bool(\n    \"verbose_logging\", False,\n    \"If true, all of the warnings related to data processing will be printed. \"\n    \"A number of warnings are expected for a normal NQ evaluation.\")\n\nflags.DEFINE_boolean(\n    \"skip_nested_contexts\", True,\n    \"Completely ignore context that are not top level nodes in the page.\")\n\nflags.DEFINE_integer(\"task_id\", 0,\n                     \"Train and dev shard to read from and write to.\")\n\nflags.DEFINE_integer(\"max_contexts\", 48,\n                     \"Maximum number of contexts to output for an example.\")\n\nflags.DEFINE_integer(\n    \"max_position\", 50,\n    \"Maximum context position for which to generate special tokens.\")\n\n\n## Special flags - do not change\n\nflags.DEFINE_string(\n    \"predict_file\", \"/kaggle/input/tensorflow2-question-answering/simplified-nq-test.jsonl\",\n    \"NQ json for predictions. E.g., dev-v1.1.jsonl.gz or test-v1.1.jsonl.gz\")\nflags.DEFINE_boolean(\"logtostderr\", True, \"Logs to stderr\")\nflags.DEFINE_boolean(\"undefok\", True, \"it's okay to be undefined\")\nflags.DEFINE_string('f', '', 'kernel')\nflags.DEFINE_string('HistoryManager.hist_file', '', 'kernel')\n\nFLAGS = flags.FLAGS\nFLAGS(sys.argv) # Parse the flags","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Next we will implement our fine-tuning of BERT to resolve our initial issue**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n1. Set up BERT Model\n\"\"\"\n\n# get configuration of BERT from the file \"bert_config.json\"\nbert_config = modeling.BertConfig.from_json_file(FLAGS.bert_config_file)\n\ntf2baseline.validate_flags_or_throw(bert_config)\n\ntf.io.gfile.makedirs(FLAGS.output_dir)\n\nrun_config = tf.estimator.RunConfig(\n    model_dir=FLAGS.output_dir,\n    save_checkpoints_steps=FLAGS.save_checkpoints_steps)\n\n# get a tokenizer, based on BERT Vocabs file\ntokenizer = tokenization.FullTokenizer(\n    vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)\n\nnum_train_steps = None\nnum_warmup_steps = None\n\n# initialise BERT MODEL (Question : How the model is uploaded to model_fn)\nmodel_fn = tf2baseline.model_fn_builder(\n    bert_config=bert_config,\n    init_checkpoint=FLAGS.init_checkpoint,\n    learning_rate=FLAGS.learning_rate,\n    num_train_steps=num_train_steps,\n    num_warmup_steps=num_warmup_steps,\n    use_tpu=FLAGS.use_tpu,\n    use_one_hot_embeddings=FLAGS.use_one_hot_embeddings)\n\n# initialise estimator\nestimator = tf.estimator.Estimator(\n    model_fn=model_fn,\n    config=run_config,\n    params={'batch_size':FLAGS.train_batch_size})\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n2. Read in the test set\n\"\"\"\n\nprint(\"FLAGS.predict_file : \", FLAGS.predict_file)\n\neval_examples = tf2baseline.read_nq_examples(\n      input_file=FLAGS.predict_file, is_training=False) \n\n\nrandom_examples = eval_examples[np.random.randint(len(eval_examples) -1)]\n\nprint('Random Example / id', random_examples.example_id)\nprint('Random Example / qas_id', random_examples.qas_id)\nprint('Random Example / questions', random_examples.questions)\nprint('Random Example / doc_tokens', random_examples.doc_tokens[:20])\nprint('Random Example / doc_tokens_map', random_examples.doc_tokens_map[:20])\nprint('Random Example / answer', random_examples.answer)\nprint('Random Example / start_position', random_examples.start_position)\nprint('Random Example / end_position', random_examples.end_position)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n3. Run it past the pre-built Bert model to create embeddings\n\"\"\"\n\ndef append_feature(feature):\n    eval_features.append(feature)\n    eval_writer.process_feature(feature)\n    \neval_writer = tf2baseline.FeatureWriter(\n      filename=os.path.join(FLAGS.output_dir, \"eval.tf_record\"),\n      is_training=False)\n\neval_features = []\n\nnum_spans_to_ids = tf2baseline.convert_examples_to_features(\n      examples=eval_examples,\n      tokenizer=tokenizer,\n      is_training=False,\n      output_fn=append_feature)\n\neval_writer.close()\n\nrandom_features = eval_features[np.random.randint(len(eval_features) -1)]\n\nprint('Random Eval Feature / unique_id', random_features.unique_id)\nprint('Random Eval Feature / example_index', random_features.example_index)\nprint('Random Eval Feature / doc_span_index', random_features.doc_span_index)\nprint('Random Eval Feature / tokens', random_features.tokens[:10])\nprint('Random Eval Feature / token_to_orig_map', list(random_features.token_to_orig_map)[:10] )\nprint('Random Eval Feature / token_is_max_context', list(random_features.token_is_max_context)[:10])\nprint('Random Eval Feature / input_ids', random_features.input_ids[:10])\nprint('Random Eval Feature / input_mask', random_features.input_mask[:10])\nprint('Random Eval Feature / segment_ids', random_features.segment_ids[:10])\nprint('Random Eval Feature / start_position', random_features.start_position)\nprint('Random Eval Feature / end_position', random_features.end_position)\nprint('Random Eval Feature / answer_text', random_features.answer_text)\nprint('Random Eval Feature / answer_type', random_features.answer_type)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n4. Use embeddings to fine-tuning BERT Model\n\"\"\"\n\neval_filename = eval_writer.filename\n\nprint(\"***** Running predictions *****\")\nprint(f\"  Num orig examples = %d\" % len(eval_examples))\nprint(f\"  Num split examples = %d\" % len(eval_features))\nprint(f\"  Batch size = %d\" % FLAGS.predict_batch_size)\n\n\n#for spans, ids in num_spans_to_ids.items():\n#    print(f\"  Num split into %d = %d\" % (spans, len(ids)))\n\npredict_input_fn = tf2baseline.input_fn_builder(\n      input_file=eval_filename,\n      seq_length=FLAGS.max_seq_length,\n      is_training=False,\n      drop_remainder=False)\n\nall_results = []\n\nfor result in estimator.predict( predict_input_fn, yield_single_examples=True):\n    if len(all_results) % 1000 == 0:\n      print(\"Processing example: %d\" % (len(all_results)))\n\n    unique_id = int(result[\"unique_ids\"])\n    start_logits = [float(x) for x in result[\"start_logits\"].flat]\n    end_logits = [float(x) for x in result[\"end_logits\"].flat]\n    answer_type_logits = [float(x) for x in result[\"answer_type_logits\"].flat]\n\n    all_results.append(\n        tf2baseline.RawResult(\n            unique_id=unique_id,\n            start_logits=start_logits,\n            end_logits=end_logits,\n            answer_type_logits=answer_type_logits))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\n5. Make predictions\n'''\n\ncandidates_dict = tf2baseline.read_candidates(FLAGS.predict_file)\n\nraw_dataset = tf.data.TFRecordDataset(eval_filename)\n\neval_features = []\nfor raw_record in raw_dataset:\n    eval_features.append(tf.train.Example.FromString(raw_record.numpy()))\n    \n\nnq_pred_dict = tf2baseline.compute_pred_dict(candidates_dict, eval_features,\n                                   [r._asdict() for r in all_results])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\n5. Display random predictions\n'''\n\ndef create_short_answer(entry):\n    # if entry[\"short_answers_score\"] < 1.5:\n    #     return \"\"\n    \n    answer = []    \n    for short_answer in entry[\"short_answers\"]:\n        if short_answer[\"start_token\"] > -1:\n            answer.append(str(short_answer[\"start_token\"]) + \":\" + str(short_answer[\"end_token\"]))\n    if entry[\"yes_no_answer\"] != \"NONE\":\n        answer.append(entry[\"yes_no_answer\"])\n    return \" \".join(answer)\n\ndef create_long_answer(entry):\n   # if entry[\"long_answer_score\"] < 1.5:\n   # return \"\"\n\n    answer = []\n    if entry[\"long_answer\"][\"start_token\"] > -1:\n        answer.append(str(entry[\"long_answer\"][\"start_token\"]) + \":\" + str(entry[\"long_answer\"][\"end_token\"]))\n    return \" \".join(answer)\n\npredict_values = list(nq_pred_dict.values())\nrandom_index = np.random.randint(len(predict_values))\n\nstart_short_answer, end_short_answer = map(int, create_short_answer(predict_values[random_index]).split(':'))\nstart_long_answer, end_long_answer = map(int, create_long_answer(predict_values[random_index]).split(':'))\n\nprediction = predict_values[random_index]\n\nfor ex in eval_examples:\n    if( ex.example_id == int(prediction['example_id']) ) :\n        print('Question : ', ex.questions)\n        print( '*' * 50)\n        print('Short answer : ', ' '.join(ex.doc_tokens[start_short_answer:end_short_answer]))\n        print('Long answer : ', ' '.join(ex.doc_tokens[start_long_answer:end_long_answer]))\n        print( '*' * 50)\n        print('Document : ', ' '.join(ex.doc_tokens))\n        break\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=4 color='#57467B'>If you find this kernel useful ! Please Give it an UPVOTE  :) </font>","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}