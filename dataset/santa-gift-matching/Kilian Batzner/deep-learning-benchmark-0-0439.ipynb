{"cells":[{"metadata":{"_uuid":"d818c77b3fd8e38d9d62b5f2e625df873cbf552a","_cell_guid":"c9740270-8fa5-47c3-ad1d-3b3d954fd472"},"source":"## Summary\n\nI tried out a neural network for classification because Machine Learning. With the large number of samples and 1000 samples per class, the task seems to be well-suited for Deep Learning. As expected, the model achieved significant results. There is still some room for improvement, so maybe combining the model with XGBoost can help there.\n\n## Approach\n\nI used the subm.csv output file from ZFTurbo's [Greedy children baseline [0.8168]](https://www.kaggle.com/zfturbo/greedy-children-baseline-0-8168) kernel. As features, I used the children's wishlists and their id resulting in 11 features.","cell_type":"markdown"},{"source":"import pandas as pd\nfrom keras.layers import Dense\nfrom keras.models import Sequential\n\nINPUT_PATH = '../input/'\n\nchild_wishes = pd.read_csv(INPUT_PATH + 'santa-gift-matching/child_wishlist.csv', header=None)\ntargets = pd.read_csv(INPUT_PATH + 'greedy-children-baseline-0-8168/subm.csv')['GiftId']\nchild_wishes['target'] = targets","metadata":{"_uuid":"4c88961c8f25de81332c8687a6469d1a06b2e3a7","_cell_guid":"2e928003-ba22-4bdb-a0f0-0aea04babc63"},"execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_kg_hide-output":false,"_uuid":"a986ccb71701370fbdc6df5eec64cdba91539e49","_cell_guid":"cb9a9f93-68b3-4df4-b616-54d4e09f7067"},"source":"We only need 10% of the available data to predict the rest because Artificial Intelligence is very powerful. So, now we make sure that 10% of the labels of each class appear in the training and validation set.","cell_type":"markdown"},{"source":"# For each of the 1000 gifts, put 80 samples are in the training data and 20 in \n# the validation data\ntrain_data = pd.DataFrame()\nvalid_data = pd.DataFrame()\nfor gift_id in range(1000):\n    train_split = child_wishes.loc[child_wishes['target'] == gift_id].iloc[:80]\n    valid_split = child_wishes.loc[child_wishes['target'] == gift_id].iloc[80:100]\n    train_data = train_data.append(train_split)\n    valid_data = valid_data.append(valid_split)\n\n# Shuffle the training data\ntrain_data = train_data.sample(frac=1)","metadata":{"_uuid":"b6b4e30c158814079e393ef52c78beaace35a515","_cell_guid":"cd2a45e5-1463-450e-8cad-557330638a4c"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"# Assign the inputs and targets \ny_train = pd.get_dummies(train_data['target']).values\nX_train = train_data.drop('target', axis=1).values\ny_valid = pd.get_dummies(valid_data['target']).values\nX_valid = valid_data.drop('target', axis=1).values\n\nprint('Shapes: X_train: %s, y_train: %s, X_valid: %s, y_valid: %s' % \n      (X_train.shape, y_train.shape, X_valid.shape, y_valid.shape))","metadata":{"_uuid":"6e0f3d5d28bc0b852792445f9655ebc30f2dd776","_cell_guid":"4aae7239-2fe2-4831-8eff-151fc5db2a57"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"# Train the model!\nmodel = Sequential()\nmodel.add(Dense(200, activation='relu', input_shape=(11,)))\nmodel.add(Dense(200, activation='relu'))\nmodel.add(Dense(1000, activation='softmax'))\nmodel.compile(optimizer='rmsprop',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\nmodel.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=2)","metadata":{"_uuid":"d169dd746cbbad6de0893bf4b9a2ec055337cb1b","_cell_guid":"af7910de-76f7-42b4-bc78-28cb986a75fe"},"execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_uuid":"9b94c6925705e7f6e7d91d4de026ef1b723ffa1b","_cell_guid":"952c387f-8ca3-4134-8ece-27a96a51b276"},"source":"Looks good! We see significant progress being made during training. Now, let's unleash the beast!","cell_type":"markdown"},{"source":"# Predict the whole dataset\nX_all = child_wishes.drop('target', axis=1).values\npredicted_probs = model.predict_proba(X_all)","metadata":{"collapsed":true,"_uuid":"4cc43c82e8f58c64de300eba15d4b7f9809c2e48","_cell_guid":"827d7e83-a5df-45c2-9c22-f015921b28dc"},"execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_uuid":"56687d543cff48ef3bdcae272cb8373ef4b1bc6a","_cell_guid":"7e932e46-4e77-4525-bfb9-2affaacf541b"},"source":"The problem here is, that there is no guarantee that the model's predictions conform to the 1000 kids per gift constraint and the twin constraint. To keep things simple, we now assign the gifts greedily to the kids based on the probabilites predicted by the model. Deep Learning may yield further improvement at this step.","cell_type":"markdown"},{"source":"# Assign gifts greedily until there are no gifts left\nchild_to_gift = {}\n# Keep track of how many gifts there are left to give\ngift_counts = dict((gift_id, 1000) for gift_id in range(1000))\navailable_gifts = gift_counts.keys()\n\nfor i, gift_probs in enumerate(predicted_probs):\n    child_id = child_wishes.iloc[i, 0]\n\n    # Ignore children (twins) that already have a gift\n    if child_id in child_to_gift:\n        continue\n\n    candidate_gifts = available_gifts\n    # If this is a twin we need the gift two times\n    if child_id < 4000:\n        candidate_gifts = [g for g in available_gifts if gift_counts[g] >= 2]\n\n    # Get the candidate gift with the highest probability\n    gift_id = max(candidate_gifts, key=lambda gift_id: gift_probs[gift_id])\n\n    child_to_gift[child_id] = gift_id\n    gift_counts[gift_id] -= 1\n\n    # If this is a twin, assign the gift to his other sibling as well\n    if child_id < 4000:\n        sibling_id = child_id + 1 if child_id % 2 == 0 else child_id - 1\n        child_to_gift[sibling_id] = gift_id\n        gift_counts[gift_id] -= 1\n\n    # Recalculate the available gifts\n    available_gifts = [g for g in available_gifts if gift_counts[g] > 0]\n    \npred = sorted(child_to_gift.items(), key= lambda t: t[0])","metadata":{"collapsed":true,"_uuid":"6fbd630c70d2866c035f02494dc726573a5478a1","_cell_guid":"deca777e-8b63-4831-a9ce-134dfc31849a"},"execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{},"source":"Done! We have our final predictions. Let's find out by how much this model outperforms the others.","cell_type":"markdown"},{"source":"import numpy as np\nfrom collections import Counter\n\nn_children = 1000000  # n children to give\nn_gift_type = 1000  # n types of gifts available\nn_gift_quantity = 1000  # each type of gifts are limited to this quantity\nn_gift_pref = 10  # number of gifts a child ranks\nn_child_pref = 1000  # number of children a gift ranks\ntwins = int(0.004 * n_children)  # 0.4% of all population, rounded to the closest even number\nratio_gift_happiness = 2\nratio_child_happiness = 2\n\n\ndef avg_normalized_happiness(pred, child_pref, gift_pref):\n    # check if number of each gift exceeds n_gift_quantity\n    gift_counts = Counter(elem[1] for elem in pred)\n    for count in gift_counts.values():\n        assert count <= n_gift_quantity\n\n    # check if twins have the same gift\n    for t1 in range(0, twins, 2):\n        twin1 = pred[t1]\n        twin2 = pred[t1 + 1]\n        assert twin1[1] == twin2[1]\n\n    max_child_happiness = n_gift_pref * ratio_child_happiness\n    max_gift_happiness = n_child_pref * ratio_gift_happiness\n    total_child_happiness = 0\n    total_gift_happiness = np.zeros(n_gift_type)\n\n    for row in pred:\n        child_id = row[0]\n        gift_id = row[1]\n\n        # check if child_id and gift_id exist\n        assert child_id < n_children\n        assert gift_id < n_gift_type\n        assert child_id >= 0\n        assert gift_id >= 0\n        child_happiness = (n_gift_pref - np.where(gift_pref[child_id] == gift_id)[0]) * ratio_child_happiness\n        if not child_happiness:\n            child_happiness = -1\n\n        gift_happiness = (n_child_pref - np.where(child_pref[gift_id] == child_id)[0]) * ratio_gift_happiness\n        if not gift_happiness:\n            gift_happiness = -1\n\n        total_child_happiness += child_happiness\n        total_gift_happiness[gift_id] += gift_happiness\n\n    # print(max_child_happiness, max_gift_happiness\n    print('normalized child happiness=',\n          float(total_child_happiness) / (float(n_children) * float(max_child_happiness)), \\\n          ', normalized gift happiness', np.mean(total_gift_happiness) / float(max_gift_happiness * n_gift_quantity))\n    return float(total_child_happiness) / (float(n_children) * float(max_child_happiness)) + np.mean(\n        total_gift_happiness) / float(max_gift_happiness * n_gift_quantity)","metadata":{"collapsed":true},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"gift_pref = pd.read_csv(INPUT_PATH + 'santa-gift-matching/child_wishlist.csv', header=None).drop(0, 1).values\nchild_pref = pd.read_csv(INPUT_PATH + 'santa-gift-matching/gift_goodkids.csv', header=None).drop(0, 1).values\nscore = avg_normalized_happiness(pred, child_pref, gift_pref)","metadata":{},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"print(score)","metadata":{},"execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_uuid":"226d89daa99fdee6698bc661322f2a9d2cb68201","_cell_guid":"9ff5662d-118e-4094-8503-f111ce7186dc"},"source":"## Conclusion\n\nWow! A simple first try with Deep Learning already achieved a whopping -4% Average Normalized Happiness. There is still room for improvement. For example, the 11 features could be preprocessed by a Convolutional Neural Network.. Also, the greedy matching at the end might be replaced with a Blockchain mechanism. But all in all, this path looks very promising. I am curious as to what further enhancements of this approach will yield.","cell_type":"markdown"}],"nbformat_minor":1,"metadata":{"language_info":{"mimetype":"text/x-python","pygments_lexer":"ipython3","name":"python","nbconvert_exporter":"python","codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","version":"3.6.3"},"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"}},"nbformat":4}