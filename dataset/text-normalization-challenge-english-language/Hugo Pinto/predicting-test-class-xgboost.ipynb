{"nbformat_minor":1,"nbformat":4,"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.1","nbconvert_exporter":"python","pygments_lexer":"ipython3","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"mimetype":"text/x-python"}},"cells":[{"source":"# Labeling Test set - Machine Learning approach\n\nIn order to label the test set, this kernel uses XGBoost to predit its 'class'.\n\nThe entire process is done using a sample of the data. Feel free to modify and improve it!","cell_type":"markdown","metadata":{"_cell_guid":"8d797194-37f5-470e-a57d-0fb34360ff08","_uuid":"18473364c9386a68167ebb92f055ead7fe21a12b"}},{"source":"### Libraries","cell_type":"markdown","metadata":{"_cell_guid":"29395dae-faaf-4515-bd0f-7f3cdfc38419","_uuid":"f88f613cba47afb551f46c2f74d75aba1b5d8108"}},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"ce414e3e-6e72-4361-97b8-2c589f3afc91","_uuid":"0a8ce979d9a7d9eaa94380dcce05fd58b52b3542"},"source":"import pandas as pd\nimport numpy as np\nseed = 42\nnp.random.seed(seed)\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.datasets import dump_svmlight_file,load_svmlight_file\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.model_selection import train_test_split"},{"source":"### Methods","cell_type":"markdown","metadata":{"_cell_guid":"47d48e94-5a3b-427a-b16e-e4c54cfec620","_uuid":"13de2f602e73a33db2220d4ef7f74b397ee7f57b"}},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"a37471ea-6668-4471-b922-ab4778e4fa39","_uuid":"c40fbae8904299ba8f211d3866004c7dc275da4a"},"source":"def instantiate_xgb(rounds=100, lr = 0.1, depth=3, sub=1, col=1, seed=seed) :\n    clf = XGBClassifier( learning_rate=           lr,\n                         n_estimators=           rounds,\n                         max_depth=              depth,\n                         subsample=              sub,\n                         colsample_bytree=       col,\n                         objective=              'multi:softmax',\n                         seed=                   seed)\n                     \n    return clf\n\ndef get_count_vectorizer(key, train, test, components=25, iters=25):\n    print ('Count Vectorizer: {}'.format(key))\n    count = CountVectorizer(analyzer=u'char', ngram_range=(1, 3)).fit(train[key].apply(str))\n    X_train = count.transform(train[key].apply(str))\n    X_test = count.transform(test[key].apply(str))\n    \n    print ('TruncatedSVD: {}'.format(key))\n    svd = TruncatedSVD(n_components=components, n_iter=iters, random_state=seed\n                                     ).fit(X_train)\n    \n    X_train = svd.transform(X_train)\n    X_test = svd.transform(X_test)\n    \n    print ('Shapes: {}\\t{}'.format(X_train.shape, X_test.shape))\n    return X_train, X_test\n\ndef get_tfidf(key, train, test, components=25, iters=25):\n    print ('TFIDF: {}'.format(key))\n    tfidf = TfidfVectorizer(\n        min_df=5, strip_accents='unicode', lowercase =True,\n        analyzer='word', token_pattern=r'\\w+', ngram_range=(1, 3), use_idf=True, \n        smooth_idf=True, sublinear_tf=True, stop_words = 'english'\n        ).fit(train[key].apply(str))\n\n    X_train = tfidf.transform(train[key].apply(str))\n    X_test = tfidf.transform(test[key].apply(str))\n    \n    print ('TruncatedSVD: {}'.format(key))\n    svd = TruncatedSVD(n_components=25, n_iter=25, random_state=seed).fit(X_train)\n    X_train = svd.transform(X_train)\n    X_test = svd.transform(X_test)\n    \n    print ('Shapes: {}\\t{}'.format(X_train.shape, X_test.shape))\n    return X_train, X_test"},{"source":"### Reading and processing data","cell_type":"markdown","metadata":{"_cell_guid":"d3e4b214-109a-4dec-a261-183365b18884","_uuid":"d898c5bee010dd7555f170624cd1b9224ebd4d6c"}},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"7b9c7b9a-35d8-4be8-85e3-379e8cd8a2ef","_uuid":"9a587c75306e877f89c0f1a5e7f3a4595a8efff9"},"source":"x_train = pd.read_csv('../input/en_train.csv', nrows=100000)\nx_test = pd.read_csv('../input/en_test.csv')"},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"07d7f2ac-c65d-49cf-b686-ccbf7af3cd1a","_uuid":"5cd49b4e793f0d27e381229b8289f1134eca6999"},"source":"y_train = pd.factorize(x_train['class'])\nx_train = x_train.drop(['class'], axis=1)\nkey = 'before'"},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"ac75c9c2-80fc-4746-b5cf-e53200033ae1","_uuid":"7d04a006dcdd1615734b5182368b562dcdc1b72e"},"source":"train_cnt, test_cnt = get_count_vectorizer(key, x_train, x_test)"},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"541f8fe3-ac97-4c83-b543-3ce92d18cb74","_uuid":"2ff5b077dd404f14807ea30bd07190818c489b15"},"source":"train_tfidf, test_tfidf = get_tfidf(key, x_train, x_test)"},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"0e817592-db6a-44d3-a9e3-7242e6c046fe","_uuid":"1c018b60aa846b35d163ea7ef3def325a206161f"},"source":"x_train[key+'_len'] = x_train[key].map(lambda x: len(str(x)))\nx_test[key+'_len'] = x_test[key].map(lambda x: len(str(x)))\n\nx_train['sentence_length'] = x_train.groupby(['sentence_id'])['sentence_id'].transform(np.size)\nx_test['sentence_length'] = x_test.groupby(['sentence_id'])['sentence_id'].transform(np.size)"},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"2e0507bd-b67f-4b5d-be21-946bfd07a6f7","_uuid":"c52877ae603fbf7e643c98b2348a826cf12466da"},"source":"x_train = np.concatenate((x_train.drop(['before', 'after','sentence_id'],axis=1), \n                          train_cnt,\n                          train_tfidf), axis=1)\nx_test = np.concatenate((x_test.drop(['before', 'sentence_id'], axis=1),\n                         test_cnt,\n                         test_tfidf), axis=1)\n\nprint ('Shapes: {}\\t{}'.format(x_train.shape, x_test.shape))\ndel train_cnt, train_tfidf, test_cnt, test_tfidf"},{"source":"### Start training","cell_type":"markdown","metadata":{"_cell_guid":"9e6ddd6f-9f2f-43c7-b632-cdf506a78ce0","_uuid":"da1767ab8d7916e3b41909ce4a42769199a49235"}},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"scrolled":true,"_cell_guid":"751d6591-5dde-4fd4-8d4b-a1049450cb1c","_uuid":"d2a9f373df6cea87fb950c38bfee08de83b5ed47"},"source":"X_train, X_valid, Y_train, Y_valid = train_test_split(\n    x_train, y_train[0], test_size=0.3, random_state=seed)\n\nclf = instantiate_xgb(rounds=100, seed=seed)\n\nclf.fit(X_train, Y_train, \n        eval_set=[(X_valid, Y_valid)],\n        eval_metric='merror',\n        early_stopping_rounds=30,\n        verbose=10)"},{"source":"### Predicting and writing to csv","cell_type":"markdown","metadata":{"_cell_guid":"75074877-8cfd-4146-a195-67febd74ef55","_uuid":"6d0b275563c92ee6a3f53fbd99feffb49d0bff08"}},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"c4cdf808-bc6f-4fe3-b4a1-7faac897f2b5","_uuid":"c21c9d18403b7d2ce0f2c8b5b460c24340d23536"},"source":"preds = clf.predict(x_test)\ny = pd.Series(preds).apply(lambda x: y_train[1][x])\ny.to_csv('./test_class_preds.csv', index=False)"},{"source":"## Visualizing predictions on validation set","cell_type":"markdown","metadata":{"_cell_guid":"e6360e74-2cd6-4e75-ba19-dcfdd5340b5d","_uuid":"9ecbc9ea06cd83fe3bfeeefca9b18af63396ace0"}},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"f61cc4e7-4366-42e4-8be5-c6c9d3778816","_uuid":"5fceba0f1e46172150b7e195e09356f87208ae8d"},"source":"df = pd.DataFrame()\ndf['valid_class'] = pd.Series(Y_valid).apply(lambda x: y_train[1][x])\ndf['valid_preds'] = pd.Series(clf.predict(X_valid)).apply(lambda x: y_train[1][x])\n\nprint(df.head(30))"},{"source":"XGBoost showed itself a very good option predicting the 'class' attribute.\n\nFuther feature engineering still can be done and I plan to improve this kernel in the next versions.\n\n#### To be  continued...","cell_type":"markdown","metadata":{"_cell_guid":"f7d8f510-72a1-40d7-bd1c-05f2410df497","_uuid":"4774abb4feef00b6868cb14fbd4c64a5d33f7bbe"}}]}