{"cells":[{"source":"**Introduction**\n\nI previously built a XGboost to label data without context. Thought the results are good, but it's not satisfying.\n\nAnd later I built a shallow RNN(due to the limitations of hardwares), and I found the RNN to be so disappoint.\n\nTherefore, I built a new XGboost model to use the context to label data.\n\nAnd most importantly, unlike the previous notebook, **this one is trained with all data, including PLAIN VERBATIM LETTERS etc.**\n\nThe major difference in the XGboost without context and with context is their dataset preparation.\n\nXGboost with context(we will use XGBwithC for short) need the previous words(regardless of whether it comes from the same sentence or not). So we define a padding and 2 special symbols to let XGBoost understand, what's is the boundary of words and what are spaces that can be safely ignored.\n\n**Results**\n\nBoth trained on 960,000 samples(words, not sentences) from en_train.csv and both have exactly the same parameters for training.\n\nAfter 60 epochs, here are the total different results:\n\n*XGBoost without context:\n\nvalid-merror:**0.005521**\ttrain-merror:**0.004168***\n\n*XGBoost with context:\n\nvalid-merror:**0.003729**\ttrain-merror:**0.000811***\n\n**Analysis**\n\nI noticed astonishingly, XGBoost with context performs so well that it outperforms XGboost without context by a great margin. And what's more, **XGboost with context nearly perfectly fits to the training dataset it has seen.** I believe, if built larger and fed more data, it might be able to challenge deep RNN.\n\n**Notebook explaintion**\n\nDue to the time limit on notebook, I choose 320,000 samples instead of 960,000 to save time.\n\nYou can view outputs in this notebook.\n\n**Final Words**\n\nI have really limited time and I lack powerful computers(I only have my faithful alienware 15 r2 by my side)\n\nTherefore I won't be participating in this competition any more.\n\nAnd **I hope this script can help you in your research**.","metadata":{"_uuid":"6eb071bde88ed08cb1074b68ec11f1fdb8be3e9d","collapsed":true,"_cell_guid":"b4d0b03a-adea-4e73-ab2b-a97667efb4a7"},"cell_type":"markdown"},{"source":"import pandas as pd\nimport numpy as np\nimport os\nimport pickle\nimport gc\nimport xgboost as xgb\nimport numpy as np\nimport re\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nmax_num_features = 10\npad_size = 1\nboundary_letter = -1\nspace_letter = 0\nmax_data_size = 320000\n\nout_path = r'.'\ndf = pd.read_csv(r'../input/en_train.csv')\n\nx_data = []\ny_data =  pd.factorize(df['class'])\nlabels = y_data[1]\ny_data = y_data[0]\ngc.collect()\nfor x in df['before'].values:\n    x_row = np.ones(max_num_features, dtype=int) * space_letter\n    for xi, i in zip(list(str(x)), np.arange(max_num_features)):\n        x_row[i] = ord(xi)\n    x_data.append(x_row)\n\ndef context_window_transform(data, pad_size):\n    pre = np.zeros(max_num_features)\n    pre = [pre for x in np.arange(pad_size)]\n    data = pre + data + pre\n    neo_data = []\n    for i in np.arange(len(data) - pad_size * 2):\n        row = []\n        for x in data[i : i + pad_size * 2 + 1]:\n            row.append([boundary_letter])\n            row.append(x)\n        row.append([boundary_letter])\n        neo_data.append([int(x) for y in row for x in y])\n    return neo_data\n\nx_data = x_data[:max_data_size]\ny_data = y_data[:max_data_size]\nx_data = np.array(context_window_transform(x_data, pad_size))\ngc.collect()\nx_data = np.array(x_data)\ny_data = np.array(y_data)\n\nprint('Total number of samples:', len(x_data))\nprint('Use: ', max_data_size)\n#x_data = np.array(x_data)\n#y_data = np.array(y_data)\n\nprint('x_data sample:')\nprint(x_data[0])\nprint('y_data sample:')\nprint(y_data[0])\nprint('labels:')\nprint(labels)","execution_count":null,"cell_type":"code","metadata":{"_uuid":"507c9117084d895edf7823bc21e512a299000220","_cell_guid":"996fd5b5-2565-4d26-a7c2-49935e2a908e"},"outputs":[]},{"source":"x_train = x_data\ny_train = y_data\ngc.collect()\n\nx_train, x_valid, y_train, y_valid= train_test_split(x_train, y_train,\n                                                      test_size=0.1, random_state=2017)\ngc.collect()\nnum_class = len(labels)\ndtrain = xgb.DMatrix(x_train, label=y_train)\ndvalid = xgb.DMatrix(x_valid, label=y_valid)\nwatchlist = [(dvalid, 'valid'), (dtrain, 'train')]\n\nparam = {'objective':'multi:softmax',\n         'eta':'0.3', 'max_depth':10,\n         'silent':1, 'nthread':-1,\n         'num_class':num_class,\n         'eval_metric':'merror'}\nmodel = xgb.train(param, dtrain, 50, watchlist, early_stopping_rounds=20,\n                  verbose_eval=10)\ngc.collect()\n\npred = model.predict(dvalid)\npred = [labels[int(x)] for x in pred]\ny_valid = [labels[x] for x in y_valid]\nx_valid = [ [ chr(x) for x in y[2 + max_num_features: 2 + max_num_features * 2]] for y in x_valid]\nx_valid = [''.join(x) for x in x_valid]\nx_valid = [re.sub('a+$', '', x) for x in x_valid]\n\ngc.collect()\n\ndf_pred = pd.DataFrame(columns=['data', 'predict', 'target'])\ndf_pred['data'] = x_valid\ndf_pred['predict'] = pred\ndf_pred['target'] = y_valid\ndf_pred.to_csv(os.path.join(out_path, 'pred.csv'))\n\ndf_erros = df_pred.loc[df_pred['predict'] != df_pred['target']]\ndf_erros.to_csv(os.path.join(out_path, 'errors.csv'), index=False)\n\nmodel.save_model(os.path.join(out_path, 'xgb_model'))","execution_count":null,"cell_type":"code","metadata":{"_uuid":"3565df1fd23bbd6d11adee7660c6837318d4a07d","_cell_guid":"ecef095f-4641-4dd3-8d07-71eaed4bd95f"},"outputs":[]}],"metadata":{"language_info":{"mimetype":"text/x-python","codemirror_mode":{"version":3,"name":"ipython"},"file_extension":".py","nbconvert_exporter":"python","name":"python","pygments_lexer":"ipython3","version":"3.6.1"},"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":1}