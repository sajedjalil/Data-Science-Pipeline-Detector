{"cells":[{"metadata":{"_cell_guid":"1fd48515-f553-4c69-983e-41f339f5c43a","_uuid":"043c49e23daeeffe6dcbacdf3743b4c23738a9c8"},"cell_type":"markdown","source":"## Preparation\n\nFirst we load some libraries..."},{"metadata":{"collapsed":true,"_cell_guid":"c3ce94e7-6ca7-4f2b-93a2-3b281a08cbf4","_uuid":"82f4a7060abb5994af249e47e6a40413ef9c39eb","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom hyperopt import hp, tpe\nfrom hyperopt.fmin import fmin\n\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import make_scorer\n\nimport xgboost as xgb\n\nimport lightgbm as lgbm","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"fc4b69c0-0185-4212-ac89-c6802209f5ca","_uuid":"f073a695175270a54031fcf27a2c59a12e92430e"},"cell_type":"markdown","source":"Next we load the data ; note i'm only loading training data because i'm just tuning parameters and comparing models  in this notebook. To speed up tuning, you can also load part of the data (add  nrows=50000), note this will give different results though, so you should re-tune with full data."},{"metadata":{"collapsed":true,"_cell_guid":"61051316-a385-4339-9c17-a648c3553fe6","_uuid":"c9aad7bf25e68c79fc99629ab7e793adadb73f09","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/train.csv')\n\nX = df.drop(['id', 'target'], axis=1)\nY = df['target']","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5bd0142a-c345-4673-b454-49805b6e0a2e","_uuid":"aff78187abed5987c2cc988e6b614143cf5fae26"},"cell_type":"markdown","source":"Next up we define the gini evaluation functions. This is not a standard metric so we need to code it  ourself (actually i borrowed the code for this)."},{"metadata":{"collapsed":true,"_cell_guid":"745440c1-e5de-4ae3-af4e-b0178beb7fe7","_uuid":"678ebcca843e5322c13a24e9950ac43c4882caf3","trusted":true},"cell_type":"code","source":"def gini(truth, predictions):\n    g = np.asarray(np.c_[truth, predictions, np.arange(len(truth)) ], dtype=np.float)\n    g = g[np.lexsort((g[:,2], -1*g[:,1]))]\n    gs = g[:,0].cumsum().sum() / g[:,0].sum()\n    gs -= (len(truth) + 1) / 2.\n    return gs / len(truth)\n\ndef gini_xgb(predictions, truth):\n    truth = truth.get_label()\n    return 'gini', -1.0 * gini(truth, predictions) / gini(truth, truth)\n\ndef gini_lgb(truth, predictions):\n    score = gini(truth, predictions) / gini(truth, truth)\n    return 'gini', score, True\n\ndef gini_sklearn(truth, predictions):\n    return gini(truth, predictions) / gini(truth, truth)\n\ngini_scorer = make_scorer(gini_sklearn, greater_is_better=True, needs_proba=True)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"ea7b7006-11e9-4f09-9d3c-10cdda2e51dc","_uuid":"28807b2beda9dab296fecec9388fc3984d9b30ba"},"cell_type":"markdown","source":"## Tuning Random Forest\n\nIn this part we'll tune the random forest classsifier, using hyperopt library. The hyperopt library has a similar purpose as gridsearch, but instead of doing an exhaustive search of the parameter space it evaluates a few well-chosen data points and then extrapolates the optimal solution based on modeling. In practice that means it often needs much fewer iterations to find a good solution.\n\nRandom forests work by averaging predictions from many decision trees - the idea is that by averaging many trees the mistakes of each tree are ironed out. Each decision tree can be somewhat overfitted, by averaging them the final result should be good.\n\nThe important parameters to tune are:\n* Number of trees in the forest (n_estimators)\n* Tree complexity (max_depth)"},{"metadata":{"_cell_guid":"5fd52781-cf9a-4299-8d35-93800e90a36d","scrolled":true,"_uuid":"958d1109036581199cea00cd75cd7451a6f48f7d","trusted":true,"collapsed":true},"cell_type":"code","source":"def objective(params):\n    params = {'n_estimators': int(params['n_estimators']), 'max_depth': int(params['max_depth'])}\n    clf = RandomForestClassifier(n_jobs=4, class_weight='balanced', **params)\n    score = cross_val_score(clf, X, Y, scoring=gini_scorer, cv=StratifiedKFold()).mean()\n    print(\"Gini {:.3f} params {}\".format(score, params))\n    return score\n\nspace = {\n    'n_estimators': hp.quniform('n_estimators', 25, 500, 25),\n    'max_depth': hp.quniform('max_depth', 1, 10, 1)\n}\n\nbest = fmin(fn=objective,\n            space=space,\n            algo=tpe.suggest,\n            max_evals=10)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5e25f7d2-7198-4fdf-bcd1-7624080b6c47","_uuid":"eaf4a969f73a9e1eb75610e20ba351f3b6f61ae2"},"cell_type":"markdown","source":"So we can see hyperopt exploring parameter space above - let's see what hyperopt estimates as the optimal parameters?"},{"metadata":{"_cell_guid":"4b09072a-7f1f-4c4c-9e57-c82791f411de","_uuid":"3e6ee0091f52a115449cf415dd56640a0ba69bd8","trusted":true,"collapsed":true},"cell_type":"code","source":"print(\"Hyperopt estimated optimum {}\".format(best))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c93a6feb-224d-4821-869f-98eedbe4dfbe","_uuid":"fe8009471b78ce5d87439742d522457059ba53b9"},"cell_type":"markdown","source":"## Tuning XGBoost\n\nSimilar to tuning above, now we will tune xgboost parameters using hyperopt!\n\nXGBoost is also an based on an ensemble of decision trees, but different from random forest. The trees are not averaged, but added. The decision trees are trained to correct residuals from the previous trees. The idea is that many small decision trees are trained, each adding a bit of info to improve overall predictions.\n\nI will follow this guide for tuning [Tuning XGBoost](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/)\n\nI'm initially fixing the number of trees to 250 and learning rate to 0.05 (determined that with a quick experiment) - then we can find good values for the other parameters. Later we can re-iterate this.\n\nThe most important parameters are:\n* Number of trees (n_estimators)\n* Learning rate - later trees have less influence (learning_rate)\n* Tree complexity (max_depth)\n* Gamma - Make individual trees conservative, reduce overfitting \n* Column sample per tree - reduce overfitting"},{"metadata":{"_cell_guid":"4783f3d1-a56b-4976-8d96-c262bfd374e6","_uuid":"bc0200f6305d06711aa799cafb72d02b5ab7556e","trusted":true,"collapsed":true},"cell_type":"code","source":"def objective(params):\n    params = {\n        'max_depth': int(params['max_depth']),\n        'gamma': \"{:.3f}\".format(params['gamma']),\n        'colsample_bytree': '{:.3f}'.format(params['colsample_bytree']),\n    }\n    \n    clf = xgb.XGBClassifier(\n        n_estimators=250,\n        learning_rate=0.05,\n        n_jobs=4,\n        **params\n    )\n    \n    score = cross_val_score(clf, X, Y, scoring=gini_scorer, cv=StratifiedKFold()).mean()\n    print(\"Gini {:.3f} params {}\".format(score, params))\n    return score\n\nspace = {\n    'max_depth': hp.quniform('max_depth', 2, 8, 1),\n    'colsample_bytree': hp.uniform('colsample_bytree', 0.3, 1.0),\n    'gamma': hp.uniform('gamma', 0.0, 0.5),\n}\n\nbest = fmin(fn=objective,\n            space=space,\n            algo=tpe.suggest,\n            max_evals=10)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"fe837fb9-6c2f-4cff-ba91-23eaf8bdb820","_uuid":"8a9a647364960d8a4cd5e10b3843b1180a879a43"},"cell_type":"markdown","source":"So hyperopt hass explored the parameter space again - let's have a look at the estimated optimum!"},{"metadata":{"_cell_guid":"69ef7782-df28-45a6-b4fb-75644d5ba3d4","_uuid":"48d303ddab1c99b73875c62ab9ad8802fc0ea408","trusted":true,"collapsed":true},"cell_type":"code","source":"print(\"Hyperopt estimated optimum {}\".format(best))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1bcd08e6-0c4c-421d-be0e-2e681bea523a","_uuid":"42e76878fbef93445a7494ca1c028f55b9a1c69e"},"cell_type":"markdown","source":"## Tuning LightGBM\n\nLightGBM is very similar to xgboost, it is also uses a gradient boosted tree approach. So the explanation above mostly holds also.\n\nThe important parameters to tune are:\n* Number of estimators\n* Tree complexity - in lightgbm that is controlled by number of leaves (num_leaves)\n* Learning rate\n* Feature fraction\n\nWe will fix number of estimators to 500 and learning rate to 0.01 (chosen experimentally) and tune the remaining parameters with hyperopt. Then later we could revisit for better results! "},{"metadata":{"_cell_guid":"eb929207-06c1-4cb3-8f79-52bf26127b8e","_uuid":"1a7a3c0bc793a18d20574c2bb7cd6a6c08bece3b","trusted":true,"collapsed":true},"cell_type":"code","source":"def objective(params):\n    params = {\n        'num_leaves': int(params['num_leaves']),\n        'colsample_bytree': '{:.3f}'.format(params['colsample_bytree']),\n    }\n    \n    clf = lgbm.LGBMClassifier(\n        n_estimators=500,\n        learning_rate=0.01,\n        **params\n    )\n    \n    score = cross_val_score(clf, X, Y, scoring=gini_scorer, cv=StratifiedKFold()).mean()\n    print(\"Gini {:.3f} params {}\".format(score, params))\n    return score\n\nspace = {\n    'num_leaves': hp.quniform('num_leaves', 8, 128, 2),\n    'colsample_bytree': hp.uniform('colsample_bytree', 0.3, 1.0),\n}\n\nbest = fmin(fn=objective,\n            space=space,\n            algo=tpe.suggest,\n            max_evals=10)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bfe7697b-f3e3-4044-a3a9-17d4e5a929ac","_uuid":"cd9c989d476ed7d5755e2447d453645211f3af75"},"cell_type":"markdown","source":"Now let's see what hyperopt estimates as the optimum?"},{"metadata":{"_cell_guid":"d524e6f2-df80-4436-b8d3-c9b2703b7707","_uuid":"6823e55c1cfaf3f29169622eb606b8f08c67bb9b","trusted":true,"collapsed":true},"cell_type":"code","source":"print(\"Hyperopt estimated optimum {}\".format(best))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"96821023-60d6-4058-a0a1-b1b8afdfc9b9","_uuid":"8d3fb4591976783c8dc71c31273c9e913608b8a0"},"cell_type":"markdown","source":"## Intepretation of tuning\n\nIt's interesting to compare the parameters that hyperopt found for random forest and XGBoost. Random forest ended up with 375 trees of depth 7, where XGBoost has 250 of depth 5. This fits the theory that random forest averages many complex (independantly trained) trees to get good results, where xgboost & lightgbm (boosting) add up many simple trees (trained on residuals)."},{"metadata":{"_cell_guid":"a54bc925-565d-4149-a5d5-6a0d68c526b7","_uuid":"7928486fcc1ec21967c768456b9629cc97938657"},"cell_type":"markdown","source":"## Comparing the models\n\nNow let's see how the models perform - if hyperopt has determined a sensible set of parameters for us..."},{"metadata":{"_cell_guid":"599d5009-eb55-46ca-b346-c5e8741c4d35","_uuid":"2bf2e277717bffec3c0b50204064ba2b3ee71c31","trusted":true,"collapsed":true},"cell_type":"code","source":"rf_model = RandomForestClassifier(\n    n_jobs=4,\n    class_weight='balanced',\n    n_estimators=325,\n    max_depth=5\n)\nparams={'n_estimators': list(range(40,61, 1))}\n\nxgb_model = xgb.XGBClassifier(\n    n_estimators=250,\n    learning_rate=0.05,\n    n_jobs=4,\n    max_depth=2,\n    colsample_bytree=0.7,\n    gamma=0.15\n)\n\nlgbm_model = lgbm.LGBMClassifier(\n    n_estimators=500,\n    learning_rate=0.01,\n    num_leaves=16,\n    colsample_bytree=0.7\n)\n\nmodels = [\n    ('Random Forest', rf_model),\n    ('XGBoost', xgb_model),\n    ('LightGBM', lgbm_model),\n]\n\nfor label, model in models:\n    scores = cross_val_score(model, X, Y, cv=StratifiedKFold(), scoring=gini_scorer)\n    print(\"Gini coefficient: %0.4f (+/- %0.4f) [%s]\" % (scores.mean(), scores.std(), label))","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}