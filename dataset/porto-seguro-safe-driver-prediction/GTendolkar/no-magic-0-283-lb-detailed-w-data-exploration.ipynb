{"nbformat_minor":1,"cells":[{"source":"Hello,  \nThis is my first post on Kaggle. I am participating in this challenge as part of a course project on optimisation at Ohio State University. This kernel gets a 0.283 on LB. Any suggestions on how to improve further would be much appreciated. (Note: The public kernels claiming 0.284 did not give me any improvement over the score achieved using this kernel)\n\nTL;DR\nBasic idea - 3 significantly different LightGBM trees - 4x upsampling - bayesian encoding of categorical features - dropping calculated features - 10 fold CV  \nJump to last section to directly view the kernel\n\nThis code is based on [Oliver's kernel](https://www.kaggle.com/ogrellier/xgb-classifier-upsampling-lb-0-283/code) and many public contributors. Thanks to them","metadata":{"_cell_guid":"07b74db2-e3cc-4507-a0d8-541d6fb535d7","_uuid":"1031fddcb5b6654859fa2f0072a2a1f424e87ec9"},"cell_type":"markdown"},{"source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nmpl.rcParams['agg.path.chunksize'] = 10000 # Jupyter notebook backend restricts number of points in plot\nimport pandas as pd\nimport scipy as scp\nimport csv\nimport seaborn as sns\n\ntrain_master = pd.read_csv('../input/train.csv')\ntest_master = pd.read_csv('../input/test.csv')\ntrain_master.describe()","metadata":{"collapsed":true,"_cell_guid":"9b38bcf7-3bff-4220-98ae-9fd884e8628a","_uuid":"e0f1b00f6c3887c3f37687d60a22ac163bce8853","scrolled":false},"cell_type":"code","outputs":[],"execution_count":null},{"source":"# Visual Data Exploration\n\nThere are 3 types of variables - Binary, Categorical and Continuous. Lets start with target variable by visualizing their distribution.","metadata":{"_cell_guid":"4d0e67a9-f718-4f09-ac3d-abc749b4fe12","_uuid":"e0a2e20a40ee2f89fcd36d99f5f65b51f963727b"},"cell_type":"markdown"},{"source":"binary_columns = [s for s in list(train_master.columns.values) if '_bin' in s]\ncategorical_columns = [s for s in list(train_master.columns.values) if '_cat' in s]\nnon_continuous_feature_subs = ['_cat', '_bin', 'target', 'id']\ncontinuous_columns = [s for s in list(train_master.columns.values) \n                      if all(x not in s for x in non_continuous_feature_subs)]\ntarget_column = 'target'\n\nind_columns = [s for s in list(train_master.columns.values) if '_ind' in s]\ncar_columns = [s for s in list(train_master.columns.values) if '_car' in s]\ncalc_columns = [s for s in list(train_master.columns.values) if '_calc' in s]\nreg_columns = [s for s in list(train_master.columns.values) if '_reg' in s]","metadata":{"collapsed":true,"_cell_guid":"abffdbf4-c724-49ed-9827-dee29ab3c122","_uuid":"6a3d4d70b66db7dc2e1d1f99207dd12aa106bf23"},"cell_type":"code","outputs":[],"execution_count":null},{"source":"## Target Variable\n\nLets check the distribution of target classes","metadata":{"_cell_guid":"3226b615-908d-4928-9f01-b193c2bf30a9","_uuid":"709eafd2831ea94e5ffcd9ada51a7a027747281f"},"cell_type":"markdown"},{"source":"from plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\n\ninit_notebook_mode()\n\nlabels = ['1','0']\nvalues = [(train_master[target_column]==1).sum(),(train_master[target_column]==0).sum()]\ncolors = ['#FEBFB3', '#E1396C']\n\ntrace = go.Pie(labels=labels, values=values,\n               hoverinfo='label+percent', textinfo='value', \n               textfont=dict(size=20),\n               marker=dict(colors=colors, \n                           line=dict(color='#000000', width=2)))\n\niplot([trace])","metadata":{"collapsed":true,"_cell_guid":"bc954b0d-7c05-469b-b248-396d8fa0f016","_uuid":"7725e57674ea5a29d444ebf68388983e12b5e895","scrolled":true},"cell_type":"code","outputs":[],"execution_count":null},{"source":"We see that the target is unevenly distributed such that the number of insurance claims are << non-claims. To overcome this problem, we will either upsample the data by duplicating rows with positive target values or downsample data by deleting rows with negative target values. Do note that upsampling has to be done DURING cross validation instead of BEFORE. Check out the upsampling section in ensemble and CV notebook to see why. Downsampling leads to loss of information and therefore we will use the upsampling technique.","metadata":{"_cell_guid":"693b7397-d211-4edc-89a4-2a4cdb7c6e08","_uuid":"d32f62d8c139e13004032bb70613f075bb165431"},"cell_type":"markdown"},{"source":"# Binary Features\n\nLets check the distribution of 1s and 0s in binary features","metadata":{"_cell_guid":"cf6034ac-6bbf-41d6-90eb-843d319a3f9e","_uuid":"579a4d55577221b6fa7d431995c85e6edfbfcf9f"},"cell_type":"markdown"},{"source":"zero_list = []\none_list = []\nfor col in binary_columns:\n    zero_list.append((train_master[col]==0).sum())\n    one_list.append((train_master[col]==1).sum())\n\ntrace1 = go.Bar(\n    x=binary_columns,\n    y=zero_list ,\n    name='0s count'\n)\ntrace2 = go.Bar(\n    x=binary_columns,\n    y=one_list,\n    name='1s count'\n)\n\ndata = [trace1, trace2]\nlayout = go.Layout(\n    barmode='stack',\n    title='Count of 1s and 0s in binary variables'\n)\n\nfig = go.Figure(data=data, layout=layout)\niplot(fig, filename='stacked-bar')","metadata":{"collapsed":true,"_cell_guid":"1efa22c8-a9b6-43b5-a963-fd09e56b6ae2","_uuid":"513a4c1fa10e9c083822643b79cd9861e8b31599"},"cell_type":"code","outputs":[],"execution_count":null},{"source":"We see that variables ```ps_ind_10_bin```, ```ps_ind_11_bin```, ```ps_ind_12_bin``` and ```ps_ind_13_bin``` have almost all 0s and therefore may not be of much use in prediction. A feature selection step in pipeline will determine whether to include them or not. It is important to note that feature selection should be performed DURING cross validation and not before. In short, feature selection should not use the data from validation set in CV. For more information check out the upsampling section in ensemble and CV notebook.\n\nStill, to get a feel for data, lets check for similarity between features. To check similarity between 2 binary variables, we will XOR each's row element and count the percentage of 0s and 1s","metadata":{"_cell_guid":"53220ebe-1a9b-4f48-8b84-1e18dfb7b31c","_uuid":"60bb3677298196021cc206e4998436359d635ce2"},"cell_type":"markdown"},{"source":"binary_corr_data = []\nr = 0\nfor i in binary_columns:\n    binary_corr_data.append([])\n    for j in binary_columns:\n        s = sum(train_master[i]^train_master[j])/float(len(train_master[i]))\n        binary_corr_data[r].append(s)\n    r+=1","metadata":{"collapsed":true,"_cell_guid":"1f4a4271-ba21-462f-a59e-bb243f4610bb","_uuid":"772f6b72a966ad64df917d52a3f4ff3d1b440394"},"cell_type":"code","outputs":[],"execution_count":null},{"source":"trace = go.Heatmap(z=binary_corr_data, x=binary_columns, y=binary_columns, colorscale='Greys')\ndata=[trace]\niplot(data)","metadata":{"collapsed":true,"_cell_guid":"5d362d9e-9acc-4cd5-be58-27376ebf0a94","_uuid":"eeded3eb9701ddb05965d92a977c02bda762d022","scrolled":true},"cell_type":"code","outputs":[],"execution_count":null},{"source":"The heatmap gives us some insights into the most important variables. For example, lightly colored columns are most uncorrelated fromall other variables. These are features like - ```ps_ind_06_bin```, ```ps_ind_16_bin```, ```ps_calc_16_bin```, ```ps_calc_17_bin```, ```ps_calc_19_bin```\n\nIn the same way, lets check similarity of each feature with the target variable to visualise each features prediction power.","metadata":{"_cell_guid":"95d50a29-ef7c-4893-ac5f-667fe272e7e1","_uuid":"9079c5b54d499ee7af3bea447ce189f9102602e1"},"cell_type":"markdown"},{"source":"binary_target_corr_data = []\nfor i in binary_columns:\n    s = sum(train_master[i]^train_master[target_column])/float(len(train_master[i]))\n    binary_target_corr_data.append(s)","metadata":{"collapsed":true,"_cell_guid":"a257f50e-50e1-479e-af1f-77f06920d7ae","_uuid":"7551aeb35dd9dae85c3d3486fb3f9e11722f6073"},"cell_type":"code","outputs":[],"execution_count":null},{"source":"binary_target_corr_chart = [go.Bar(\n    x=binary_columns,\n    y=binary_target_corr_data\n)]\niplot(binary_target_corr_chart)","metadata":{"collapsed":true,"_cell_guid":"368820dc-1ff4-4ab9-9c34-6301b4d398fb","_uuid":"ccd7b2e143883435bac523c29dac0421a790bfba"},"cell_type":"code","outputs":[],"execution_count":null},{"source":"We again observe that features -  ```ps_ind_06_bin```, ```ps_ind_16_bin```, ```ps_calc_16_bin```, ```ps_calc_17_bin```, ```ps_calc_19_bin``` are most insightful\n\nWe will perform a more formal feature selection during the cross validation stage.","metadata":{"_cell_guid":"910f8e16-2e0a-4516-9978-fa803fd10973","_uuid":"d06f46e2922dda03f9abd003d2b67ce885e47209"},"cell_type":"markdown"},{"source":"# Continuous Features\n\nFirst lets check if there are missing values in the data set (For binary variables it was specified that it had no missing values).","metadata":{"_cell_guid":"dcc4efe3-3f17-447a-aebb-f731c8efe609","_uuid":"401c192d6e7152106d6c3a16bd8668757833087e"},"cell_type":"markdown"},{"source":"value_list = []\nmissing_list = []\nfor col in continuous_columns:\n    value_list.append((train_master[col]!=-1).sum())\n    missing_list.append((train_master[col]==-1).sum())\n\ntrace1 = go.Bar(\n    x=continuous_columns,\n    y=value_list ,\n    name='Actual Values'\n)\ntrace2 = go.Bar(\n    x=continuous_columns,\n    y=missing_list,\n    name='Missing Values'\n)\n\ndata = [trace1, trace2]\nlayout = go.Layout(\n    barmode='stack',\n    title='Count of missing values in continuous variables'\n)\n\nfig = go.Figure(data=data, layout=layout)\niplot(fig, filename='stacked-bar')","metadata":{"collapsed":true,"_cell_guid":"5ec7e476-95f1-40aa-a591-f06bec585858","_uuid":"5db0f1b27d90281ac51bf31fa74890293c7bd584"},"cell_type":"code","outputs":[],"execution_count":null},{"source":"We see that only ```ps_reg_03``` and ```ps_car_14``` have significant number of missing values. Apart from that, ```ps_car_11``` has 5 and ```ps_car_12``` has 1 missing value. Lets evaluate the chi squared test between each of continuous variables and the target variable","metadata":{"_cell_guid":"7c49a337-3092-400c-870b-de7cd9227e90","_uuid":"f2581c27571a7f588b28bb9a2750e9046b5d5dcc"},"cell_type":"markdown"},{"source":"from sklearn.feature_selection import chi2, mutual_info_classif\n\nminfo_target_to_continuous_features = mutual_info_classif(\n    train_master[continuous_columns],train_master[target_column])\n\nminfo_target_to_continuous_chart = [go.Bar(\n    x=continuous_columns,\n    y=minfo_target_to_continuous_features\n)]\niplot(minfo_target_to_continuous_chart)","metadata":{"collapsed":true,"_cell_guid":"a83f3ea1-1947-4b33-8c90-a6728c1ee8dd","_uuid":"4e6de18b6458f58999110fa322d04c92a701e2d4"},"cell_type":"code","outputs":[],"execution_count":null},{"source":"It seems like ```ps_reg_03``` and ```ps_car_14``` are fairly independant of the target variable. Again, a more formal feature selection will be performed during cross validation\n\nLets evaluate the pearson correlation between between each of continuous variables to see if two features are highly correlated and therefore present redundant information.","metadata":{"_cell_guid":"1c8744b0-d10a-4e89-b60a-2e15e4c76a66","_uuid":"3bb9f4dee41a5cd84b0723d9b2bf8d4cf5471a16"},"cell_type":"markdown"},{"source":"continuous_corr_data = train_master[continuous_columns].corr(method='pearson').as_matrix()\n\ntrace = go.Heatmap(z=continuous_corr_data, x=continuous_columns, \n                   y=continuous_columns, colorscale='Greys')\ndata=[trace]\niplot(data)","metadata":{"collapsed":true,"_cell_guid":"28555b42-5715-4477-afdf-318dda232272","_uuid":"b825100891d8676f42878d23df0c20b2a3de66e2"},"cell_type":"code","outputs":[],"execution_count":null},{"source":"It seems like ```ps_reg_03``` and ```ps_reg_01``` maybe linearly related. Same for ```ps_car_12``` and ```ps_car_13```","metadata":{"_cell_guid":"802ee27e-8348-440f-a10e-24dca1af1277","_uuid":"4d8c3a8d1387eb0c13355678e532a5c08c5c9c85"},"cell_type":"markdown"},{"source":"# Categorical Features\n\nCategorical features are a tricky business in classification, especially while using trees. Since tree algorithms use binary trees, they need to find an appropriate split. But categorical variables have no inherent order in them. Various techniques are used to overcome this problem. One such technique is encoding of categorical variables such that the encoded variables have an order. But what encoding scheme to follow?\n\nA 2001 [paper](https://www.researchgate.net/publication/220520258_A_Preprocessing_Scheme_for_High-Cardinality_Categorical_Attributes_in_Classification_and_Prediction_Problems) by Daniele Micci-Barreca illustrates one approach.","metadata":{"_cell_guid":"9e9553d7-3072-4783-b88f-161f5f100bb9","_uuid":"cc025337d29a2c86ec1ae15f1b27d2b7761a239c"},"cell_type":"markdown"},{"source":"# Evaluation Criteria for Predictions\n\nAccording to competition details, the challenge uses normalized gini coefficient to evaluate the predictions. This is the same function we should be using for our cross validation step (Gini is linearly related to AUC-ROC so we will use AUC for evaluation). Lets define the function.","metadata":{"_cell_guid":"8f47da99-c894-4165-a607-63a56424ee0f","_uuid":"fce6672b9a300ade951fd04aa50c62403acd494f"},"cell_type":"markdown"},{"source":"def gini(y, pred):\n    fpr, tpr, thr = metrics.roc_curve(y, pred, pos_label=1)\n    g = 2 * metrics.auc(fpr, tpr) -1\n    return g","metadata":{"collapsed":true,"_cell_guid":"d93969af-c4fb-420e-b84c-1d3c5d9c780b","_uuid":"ca71b128aa4a196a66ed81a22a6eacb058f2128c"},"cell_type":"code","outputs":[],"execution_count":null},{"source":"# Cross Validation\n\nTo evaluate performance of our model and to ensure that our it generalizes well over new data, we do a 10 fold cross-validation. Folds will be stratified to ensure equal proportions of target variable in each. In each fold \n1. We upsample the positive target data from training fold\n2. Choose best features using training fold\n3. Train the model over training fold and evaluate it over the hold out validation fold.","metadata":{"_cell_guid":"61633303-5955-4aee-80de-8a07862d8a4c","_uuid":"496b0f3ba65858d330048f32ec688d4c59b5605a"},"cell_type":"markdown"},{"source":"from sklearn.model_selection import StratifiedKFold\n\nn_splits = 10\nfolds = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=7)","metadata":{"collapsed":true,"_cell_guid":"4723bf6d-58ac-4868-b51c-851227402497","_uuid":"83c29704217edc94c706d70511dc70796531461d"},"cell_type":"code","outputs":[],"execution_count":null},{"source":"# Single Model\n\nWe will start with XgBoost. To evaluate optimal parameters for XgBoost would require a search like grid search. Parameter tuning is needed to avoid overfitting and there are two basic ways to do that -   \n1. By controlling complexity of tree (Regularization)\n2. By adding randomness via sub-sampling data and columns\n\nSuppose we want to find the optimal values from - \n\n```max_depth``` = {3, 4, 5} (Regularization)  \n```gamma``` = {1, 5, 9} (Regularization)  \n```colsample_bytree``` = {0.7, 0.8, 0.9}  \n```subsample``` = {0.7, 0.8, 0.9}  \n```learning_rate``` = {0.1, 0.05, 0.005}\n\nA grid search over these parameters means evaluating model at 243 parameter combinations. And with 10-fold CV, it means that you train the model 2430 times. Training 1 model on one core of an c4.4xlarge EC2 instance takes anywhere between 10 to 15 minutes depending on the depth of the tree and learning rates. With 16 cores running parallely will take approximately 38 hours. c4.4xlarge charges ~ \\$0.8/hr which leads to a total cost of $30 and two days of time. And this is just to evaluate XgBoost over a small set of parameter range.\n\nIf you want to do it yourself, I have listed the code in appendix notebook with details of setting up an EC2 instance with jupyter notebook interface.\n\nCombining a short grid search costing me $10 and prior experience, I decided to use the following parameters - \n\n```max_depth``` = 4  \n```gamma``` = 9  \n```colsample_bytree``` = 0.8  \n```subsample``` = 0.8  \n```learning_rate``` = 0.05\n\nThe objective function is (gives the probabilities)  \n```objective``` = ```\"binary:logistic\"```\n\nThe other option which gave equally good performance was evaluation of pair-wise ranks  \n```objective``` = ```\"rank:pairwise\"```\n\nParameters for cross validation -   \n```num_rounds``` = 1000 with early stopping window of 10 epochs (And use all trees to predict)  \n```folds``` = 10\n\n(For brevity, I have excluded the code but the it gave a LB of 0.281)","metadata":{"_cell_guid":"ac01524e-e066-4152-b852-734c6dc6bfff","_uuid":"f558acc458836ecf258fb4b206f23bf3ffb5d01f"},"cell_type":"markdown"},{"source":"# Average of 3 boostes trees with LightGBM\n\n(Note: LightGBM proved to be extremely fast compared to XGBoost)\n\nTrees have this property that when we change the training dataset, we may end up with drastically different trees. Averaging over all such different trees should better generalise our CV score over the test data. Look at it as a forest of gradient boosted trees.\n\nEach tree is differentiated by its complexity and randomness. We will generate 3 trees as follows -\n1. A deeper tree (```max_depth``` = 5) that randomly chooses only a small (30%) subset of the available features and 0.7 bagging fraction\n2. An average tree (```max_depth``` = 4) that randomly chooses 90% of features and 0.9 bagging fraction\n2. An shallow tree (```max_depth``` = 3) that chooses all features and all rows\n\nAt each fold, we evaluate the arithmatic mean of predictions of the 3 trees on the validation set. The overall score over training data is then taken as the average score over all folds.\n\nThis model leads to 0.287 CV and 0.283 on LB (Seems like a drastic overfit!)\n\n# Parameter Selection\n\nThe main parameter to control complexity of tree will be ```max_depth``` instead of ```num_leaves```. Since LightGBM grows tree leaf wise, for same number of leaves, LightGBM will give a much deeper tree than depth wise. ```max_depth``` is a much more intuitive limit on how deep the tree is going to grow.\n\nWe will use ```feature_fraction``` and ```bagging_fraction``` and ```bagging_freq``` to control randomness.\n\nWe will again experiment with LightGBM's internal handling of categorical features and our encoding scheme defined previously. For this model, we use our previous encoding scheme.\n\nLets first define the encoding scheme.\n","metadata":{"_cell_guid":"88f75a8a-88f3-44b5-ba06-2bad56eb935a","_uuid":"e627966c68074be2b833aa585d041ceb9d0c5639"},"cell_type":"markdown"},{"source":"import numpy as np\nfrom sklearn import metrics\n\ndef encode_cat_features(train_df, test_df, cat_cols, target_col_name, smoothing=1):\n    prior = train_df[target_col_name].mean()\n    probs_dict = {}\n    for c in cat_cols:\n        probs = train_df.groupby(c, as_index=False)[target_col_name].mean()\n        probs['counts'] = train_df.groupby(c, as_index=False)[target_col_name].count()[[target_col_name]]\n        probs['smoothing'] = 1 / (1 + np.exp(-(probs['counts'] - 1) / smoothing))\n        probs['enc'] = prior * (1 - probs['smoothing']) + probs['target'] * probs['smoothing']\n        probs_dict[c] = probs[[c,'enc']]\n    return probs_dict","metadata":{"collapsed":true,"_cell_guid":"85081958-bd60-4f5b-bb60-c259169e682f","_uuid":"51e135c9bb24bc36fd2ebf265e7db31fa85f4ba8"},"cell_type":"code","outputs":[],"execution_count":null},{"source":"Lets start building the model. Uncomment the code (commented because it exceeds time limit)","metadata":{"_cell_guid":"93fdd19e-e29d-4da0-a95b-8bd900b55f50","_uuid":"d4ed16e2118fbd6f36fe848a995af1b62a83309b"},"cell_type":"markdown"},{"source":"'''\nimport lightgbm as lgb\nimport numpy as np\nimport pandas as pd\n\nnp.random.seed(3)\nmodel_scores = {}\n\n# Drop binary columns with almost all zeros. \n# Why now? Just follow along for now. We have a lot of experimentation to be done\ntrain = train_master.drop(['ps_ind_10_bin', 'ps_ind_11_bin', 'ps_ind_13_bin'],axis=1)\ntest = test_master.drop(['ps_ind_10_bin', 'ps_ind_11_bin', 'ps_ind_13_bin'],axis=1)\n\n# Drop calculated features\n# But WHY??? \n# Because we are assuming that tree can generate any complicated function \n# of base features and calculated features add no more information\n# Is this assumption valid? Results will tell\ncalc_columns = [s for s in list(train_master.columns.values) if '_calc' in s]\ntrain = train.drop(calc_columns, axis=1)  \ntest = test.drop(calc_columns, axis=1)\n\n# Get categorical columns for encoding later\ncategorical_columns = [s for s in list(train_master.columns.values) if '_cat' in s]\ntarget_column = 'target'\n\n# Replace missing values with NaN\ntrain = train.replace(-1, np.nan)\ntest = test.replace(-1, np.nan)\n\n# Initialize DS to store validation fold predictions\ny_val_fold = np.empty(len(train))\n\n# Initialize DS to store test predictions with aggregate model and individual models\ny_test = np.zeros(len(test))\ny_test_model_1 = np.zeros(len(test))\ny_test_model_2 = np.zeros(len(test))\ny_test_model_3 = np.zeros(len(test))\n\nfor fold_number, (train_ids, val_ids) in enumerate(\n    folds.split(train.drop(['id',target_column], axis=1), \n                train[target_column])):\n    \n    X = train.iloc[train_ids]\n    X_val = train.iloc[val_ids]\n    X_test = test\n    \n    # Encode categorical variables using training fold\n    encoding_dict = encode_cat_features(X, X_val, categorical_columns, target_column)\n    \n    for c, encoding in encoding_dict.items():\n        X = pd.merge(X, encoding[[c,'enc']], how='left', on=c, sort=False,suffixes=('', '_'+c))\n        X = X.drop(c, axis = 1)\n        X = X.rename(columns = {'enc':'enc_'+c})\n        \n        X_test = pd.merge(X_test, encoding[[c,'enc']], how='left', on=c, sort=False,suffixes=('', '_'+c))\n        X_test = X_test.drop(c, axis = 1)\n        X_test = X_test.rename(columns = {'enc':'enc_'+c})\n        \n        X_val = pd.merge(X_val, encoding[[c,'enc']], how='left', on=c, sort=False,suffixes=('', '_'+c))\n        X_val = X_val.drop(c, axis = 1)\n        X_val = X_val.rename(columns = {'enc':'enc_'+c})\n        \n    # Seperate target column and remove id column from all\n    y = X[target_column]\n    X = X.drop(['id',target_column], axis=1)\n    X_test = X_test.drop('id', axis=1)\n    y_val = X_val[target_column]\n    X_val = X_val.drop(['id',target_column], axis=1)\n    \n    # Upsample data in training folds\n    ids_to_duplicate = pd.Series(y == 1)\n    X = pd.concat([X, X.loc[ids_to_duplicate]], axis=0)\n    y = pd.concat([y, y.loc[ids_to_duplicate]], axis=0)\n    # Again Upsample (total increase becomes 4 times)\n    X = pd.concat([X, X.loc[ids_to_duplicate]], axis=0)\n    y = pd.concat([y, y.loc[ids_to_duplicate]], axis=0)\n    \n    # Shuffle after concatenating duplicate rows\n    # We cannot use inbuilt shuffles since both dataframes have to be shuffled in sync\n    shuffled_ids = np.arange(len(X))\n    np.random.shuffle(shuffled_ids)\n    X = X.iloc[shuffled_ids]\n    y = y.iloc[shuffled_ids]\n    \n    # Feature Selection goes here\n    # TODO\n    \n    # Define parameters of GBM as explained before for 3 trees\n    params_1 = {\n        'task': 'train',\n        'boosting_type': 'gbdt',\n        'objective': 'binary',\n        'metric': 'auc',\n        'max_depth': 3,\n        'learning_rate': 0.05,\n        'feature_fraction': 1,\n        'bagging_fraction': 1,\n        'bagging_freq': 10,\n        'verbose': 0\n    }\n    params_2 = {\n        'task': 'train',\n        'boosting_type': 'gbdt',\n        'objective': 'binary',\n        'metric': 'auc',\n        'max_depth': 4,\n        'learning_rate': 0.05,\n        'feature_fraction': 0.9,\n        'bagging_fraction': 0.9,\n        'bagging_freq': 2,\n        'verbose': 0\n    }\n    params_3 = {\n        'task': 'train',\n        'boosting_type': 'gbdt',\n        'objective': 'binary',\n        'metric': 'auc',\n        'max_depth': 5,\n        'learning_rate': 0.05,\n        'feature_fraction': 0.3,\n        'bagging_fraction': 0.7,\n        'bagging_freq': 10,\n        'verbose': 0\n    }\n    \n    # Create appropriate format for training and evaluation data\n    lgb_train = lgb.Dataset(X, y)\n    lgb_eval = lgb.Dataset(X_val, y_val, reference=lgb_train)\n    \n    # Create the 3 classifiers with 1000 rounds and a window of 100 for early stopping\n    clf_1 = lgb.train(params_1,lgb_train, num_boost_round=1000,\n                      valid_sets=lgb_eval, early_stopping_rounds=100, verbose_eval=50)\n    clf_2 = lgb.train(params_2,lgb_train, num_boost_round=1000,\n                      valid_sets=lgb_eval, early_stopping_rounds=100, verbose_eval=50)\n    clf_3 = lgb.train(params_3,lgb_train, num_boost_round=1000,\n                      valid_sets=lgb_eval, early_stopping_rounds=100, verbose_eval=50)\n    \n    # Predict raw scores for validation ids\n    # At each fold, 1/10th of the training data get scores\n    y_val_fold[val_ids] = (clf_1.predict(X_val, raw_score=True)+\n                           clf_2.predict(X_val, raw_score=True)+\n                           clf_3.predict(X_val, raw_score=True)) / 3\n\n    # Predict and average over folds, raw scores for test data\n    y_test += (clf_1.predict(X_test, raw_score=True)+\n               clf_2.predict(X_test, raw_score=True)+\n               clf_3.predict(X_test, raw_score=True)) / (3*n_splits)\n    y_test_model_1 += clf_1.predict(X_test, raw_score=True) / n_splits\n    y_test_model_2 += clf_2.predict(X_test, raw_score=True) / n_splits\n    y_test_model_3 += clf_3.predict(X_test, raw_score=True) / n_splits\n    \n    # Display fold predictions\n    # Gini requires only order and therefore raw scores need not be scaled\n    print(\"Fold %2d : %.9f\" % (fold_number + 1, gini(y_val, y_val_fold[val_ids])))\n    \n# Display aggregate predictions\n# Gini requires only order and therefore raw scores need not be scaled\nprint(\"Average score over all folds: %.9f\" % gini(train_master[target_column], y_val_fold))\n'''","metadata":{"collapsed":true,"_cell_guid":"e3d9ecd6-1933-4465-b02d-82735784b75e","_uuid":"7de0ffddadadbd0f29276cc12e5051fd55816195"},"cell_type":"code","outputs":[],"execution_count":null},{"source":"Scale scores and save predictions for submission","metadata":{"_cell_guid":"cff83ff9-2cce-4581-beb2-090d03b7d360","_uuid":"51217b1c1b344d298b1884ea9a6c58179289d575"},"cell_type":"markdown"},{"source":"'''\ntemp = y_test\n# Scale the raw scores to range [0.0, 1.0]\ntemp = np.add(temp,abs(min(temp)))/max(np.add(temp,abs(min(temp))))\n\ndf = pd.DataFrame(columns=['id', 'target'])\ndf['id']=test_master['id']\ndf['target']=temp\ndf.to_csv('benchmark__0_283.csv', index=False, float_format=\"%.9f\")\ndf.shape\n'''","metadata":{"collapsed":true,"_cell_guid":"e1d21353-bde7-4585-87f6-01b0a9696571","_uuid":"dd595c4755b905ad349a6024733ef3673c441e01"},"cell_type":"code","outputs":[],"execution_count":null},{"source":"I have purposefully kept aside all extra analysis and messy experiments. I have tried most of the tricks mentioned in the discussions - using HM instead of AM or Logistic regression for ensembling, recursive feature selection, custom features like sum of binary variables, even downgrading LightGBM version, etc, but none has helped me improve the score. I guess I just have to experiment adding more classifiers and averaging them appropriately.\n\nSome current known drawbacks - \n1. Encoding is based on training set and there are some categories in hold out that do not occur in training. This results in NaN values on the join\n2. Literally no feature engineering\n3. Overfitting\n\nPlease do let me know your opinions and suggestions and thanks to public contributors for the invaluable help!\n\nP.S. I know this is off topic but I will be shameless and ask - I am looking for internships for summer 2018. Any help in that direction will also be appreciated :)","metadata":{"_cell_guid":"08a88d64-a620-4974-b29a-9aab4c6acda0","_uuid":"0a10207ad795b7655e757147bd7ef460dba6703c"},"cell_type":"markdown"}],"metadata":{"language_info":{"nbconvert_exporter":"python","mimetype":"text/x-python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"name":"python","file_extension":".py","version":"3.6.3"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat":4}