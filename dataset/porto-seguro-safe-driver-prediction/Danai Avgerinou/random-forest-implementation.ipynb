{"metadata":{"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"},"language_info":{"name":"python","mimetype":"text/x-python","pygments_lexer":"ipython3","file_extension":".py","version":"3.6.4","nbconvert_exporter":"python","codemirror_mode":{"name":"ipython","version":3}}},"cells":[{"source":"# Random Forest Implementation","cell_type":"markdown","metadata":{"_cell_guid":"177237ef-a78c-44fa-9df9-f02d7f969dfa","_uuid":"10472660d267454d8860c8a568a746a7f1a1392f"}},{"source":"The following Kernel is an implementation of Random Forest, using sklearn. The score that it gives for this competition is low (0.270), however it is a good example for those who are interested in learning a little bit more about how to use sklearn to create random forests. ","cell_type":"markdown","metadata":{"_cell_guid":"fdc6cea6-7d17-47eb-b6ba-f2261b4b78d3","_uuid":"ba1d57b03512d38e078c37532b86f0d05c579f23"}},{"source":"import pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import export_graphviz\nfrom sklearn import metrics\nfrom sklearn.tree import _tree\nimport graphviz \n\npd.options.display.max_rows = 20 # don't display many rows","cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"74045d60-ab59-4596-97ce-ba6a0b41a9c1","collapsed":true,"_uuid":"f6fd19baf41dec6359c39aaffed56e9d438af25a"}},{"source":"train_data = pd.read_csv('../input/testtraincsv/train.csv')\ntest = pd.read_csv('../input/testtraincsv/test.csv')","cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"4c2260d0-2c00-4eb9-97cc-66d895991d79","collapsed":true,"_uuid":"17a0c4aaed33e60f1459cb5bd3c65c2d26282e47"}},{"source":"First, lets take a look at the data and the different feautures.","cell_type":"markdown","metadata":{"_cell_guid":"af5d15ca-7bf5-4a55-ba63-0e3bc2e29c82","_uuid":"746878b0225bdac5c72b3883f0a908701d4e2c9b"}},{"source":"train_data.describe().transpose()","cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"393f56f7-f806-4e0c-b3e9-ec27d1b3f645","collapsed":true,"_uuid":"3bd175d5a7266ac92d50d05fdaa237c65056db5a"}},{"source":"Next step is to separate the data matrix and response.","cell_type":"markdown","metadata":{"_cell_guid":"e98a4a7c-0a41-43d6-9105-be91c5116a53","_uuid":"aed3bc7757be989bcec381937d611a258db4cfe9"}},{"source":"X, y = train_data.drop('target', axis=1), train_data['target']","cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"d639f99c-ef94-4feb-9c79-b9ab18f4e25c","collapsed":true,"_uuid":"89cda5d51721cd05074577b39aa39c6e32c0414f"}},{"source":"Now we are ready to apply Random Forest.","cell_type":"markdown","metadata":{"_cell_guid":"02b17bbc-3cc3-4aec-bffa-ae64d276d991","_uuid":"49c6a340ac92f26387c19455305cbc5e44f0e5ee"}},{"source":"regr = RandomForestRegressor()\nregr.fit(X, y)","cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"6f826f93-4265-4da6-8e37-700e4bdb5135","collapsed":true,"_uuid":"9c88f5d0ea08ea988f0ed9058dc73476a9da0acf"}},{"source":"To evaluate our model, we are calculating the $R^2$","cell_type":"markdown","metadata":{"_cell_guid":"14b779b5-de62-48dd-b01e-5b7b11c44364","_uuid":"961ee62424c0b2cab08f338c9bc9ef8dfe206701"}},{"source":"print(f\"R^2 : {regr.score(X, y)}\")","cell_type":"code","execution_count":null,"outputs":[],"metadata":{"scrolled":true,"_cell_guid":"50c4fdb6-27f1-427c-ba6b-4cb03aa6b545","collapsed":true,"_uuid":"1326f39caec4053582ea68f26e0dc271b672d208"}},{"source":"Splitting the X and y into train and test and calculate the $R^2$ for each one. ","cell_type":"markdown","metadata":{"_cell_guid":"18eb7f45-13ce-4d99-a221-7231082cf4df","_uuid":"d9417244255216a58a8d3f4e34cd5f40c42cc08c"}},{"source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\nregr = RandomForestRegressor()\nregr.fit(X_train, y_train)\nprint(f\"R^2 of train : {regr.score(X_train, y_train)}\")\nprint(f\"R^2 of test : {regr.score(X_test, y_test)}\")","cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"6a424f78-ca05-4360-840e-b7ce22581e21","collapsed":true,"_uuid":"942afe402385d9f42e38b812ed3284de404c7ac0"}},{"source":"The difference between the $R^2$ of train and test imply overfitting.","cell_type":"markdown","metadata":{"_cell_guid":"7ffb9bb3-01ab-4e7f-ac71-de48030e700e","_uuid":"112e03fa04cb4d1569e56f2b52d9f29f4c024648"}},{"source":"Now let's increase the number of splits of the Random Forest to improve the $R^2$","cell_type":"markdown","metadata":{"_cell_guid":"7f0b3f4c-e7b9-4f92-b5d4-551eae398711","_uuid":"4e36a2387576dc7745e7538cc229f799ed5943bc"}},{"source":"regr = RandomForestRegressor(n_estimators=40)\nregr.fit(X_train, y_train)\nprint(f\"R^2 of train: {regr.score(X_train, y_train)}\")\nprint(f\"R^2 of test: {regr.score(X_test, y_test)}\")","cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"ab59cb90-a62e-42a1-b036-df2ed1e58328","collapsed":true,"_uuid":"edcea7fc93565ad66481d87dfb7055b55ea5c965"}},{"source":"regr = RandomForestRegressor(n_estimators=100)\nregr.fit(X_train, y_train)\nprint(f\"R-squared of train: {regr.score(X_train, y_train)}\")\nprint(f\"R-squared of test: {regr.score(X_test, y_test)}\")","cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"70da6fbf-5025-4f4e-a2b4-2f1a41afc494","collapsed":true,"_uuid":"2e37ed41a5cc3ef84eff199d118033c4ea22e705"}},{"source":"As expected, the bigger the number of estimators the $R^2$ are getting closer","cell_type":"markdown","metadata":{"_cell_guid":"bc125de9-843f-49aa-8bae-6b88349d867d","_uuid":"2f4b450d8b79ede68602b91ab4645be52f654375"}},{"source":"from sklearn.ensemble import GradientBoostingRegressor\ngbrt=GradientBoostingRegressor(n_estimators=100)\ngbrt.fit(X, y)\ny_pred=gbrt.predict(test)","cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"8555cd4a-a371-4093-9751-e2f9c58dbc82","collapsed":true,"_uuid":"e060fa8311ddac3b5aaca9c452dc817d617a70b6"}},{"source":"y_pred","cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"cfb32bb3-3c5f-4e09-a1e9-89e2d1307d8d","collapsed":true,"_uuid":"8caee034d9093634aa4e0682c04731a11e70ed31"}},{"source":"index = test['id']\ndf = pd.DataFrame(y_pred, index=index)\ndf.columns = ['target']","cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"b2ae54a6-8af1-4551-8be7-4a9822946def","collapsed":true,"_uuid":"ce742f28392b3ff6429d2fd14c6a5e6dce69486d"}},{"source":"df","cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"034901fa-e325-4eef-b8fb-0a3ef3531075","collapsed":true,"_uuid":"51a250c6cda1b84a9fa8b8f5b3da77d7951396e8"}},{"source":"df.to_csv(\"submit_me.csv\")","cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"431e0134-7f15-4919-9606-9cc33f2dede9","collapsed":true,"_uuid":"7e6c53c67067e1ba74919cd002593525ea1c7517"}},{"source":"As someone can understand from above, the $R^2$ of Train and Test do not converge, which is our goal here, to be sure we do not overfit.\nPotential improvements can be feauture engineering, to find the feautures that influence our predictions the most and then apply Random Forests again.","cell_type":"markdown","metadata":{"_cell_guid":"f142161d-8db7-4ebb-8fd3-d43cf20ea055","_uuid":"f88d86fb42e62e7eb77f1e6d35d7bd1ece34f934"}}],"nbformat_minor":1,"nbformat":4}