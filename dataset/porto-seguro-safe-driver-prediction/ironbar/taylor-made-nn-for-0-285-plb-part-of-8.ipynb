{"cells":[{"cell_type":"markdown","source":"# Taylor made keras model\nI have uploaded the notebook I used for building the best NN model of our ensemble. It is not cleaned but maybe could be helpful.","metadata":{"nbpresent":{"id":"0257e2ee-fc30-4538-b1ae-e35d63005945"}}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os \nimport glob\nimport seaborn as sns\nfrom termcolor import colored\nimport keras\nfrom sklearn.model_selection import KFold\nfrom tqdm import tqdm\nimport sys\nimport hyperopt\nfrom hyperopt import fmin, tpe, hp\nimport time\nimport cPickle\nimport scipy.stats as ss\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Input\nfrom keras import regularizers\nfrom keras.models import Model\nfrom keras.layers import Concatenate\n\nfrom porto.definitions import TRAIN_SET_PATH, TEST_SET_PATH, DATA_DIR\nfrom porto.dataset import DatasetCleaner, DatasetBuilder\nfrom porto.metrics import gini_normalized\n\n%matplotlib inline","execution_count":null,"outputs":[],"metadata":{"nbpresent":{"id":"4519d3ce-1785-48bd-9c91-c85c6e9f0c64"}}},{"cell_type":"markdown","source":"## Gather the data","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv(TRAIN_SET_PATH)\ndf_test = pd.read_csv(TEST_SET_PATH)","execution_count":null,"outputs":[],"metadata":{"collapsed":true,"nbpresent":{"id":"aea4f3e7-a6b3-43c0-bfed-b76c5b300497"},"scrolled":true}},{"cell_type":"markdown","source":"Clean the missing values","metadata":{}},{"cell_type":"code","source":"dataset_cleaner = DatasetCleaner(min_category_samples=50)\ndataset_cleaner.fit(df_train)\ndataset_cleaner.transform(df_train)\ndataset_cleaner.transform(df_test)","execution_count":null,"outputs":[],"metadata":{"collapsed":true}},{"cell_type":"markdown","source":"Get rid of calc features.","metadata":{}},{"cell_type":"code","source":"unwanted = df_train.columns[df_train.columns.str.startswith('ps_calc_')]\ndf_train.drop(unwanted, axis=1, inplace=True)\ndf_test.drop(unwanted, axis=1, inplace=True)\nprint df_train.shape","execution_count":null,"outputs":[],"metadata":{}},{"cell_type":"markdown","source":"### One hot encoding\nApply one hot encoding to categorical features.","metadata":{}},{"cell_type":"code","source":"categorical_columns = df_train.columns[df_train.columns.str.endswith('_cat')]\ndf_train = pd.concat([pd.get_dummies(df_train, columns=categorical_columns), df_train[categorical_columns]], axis=1)\ndf_test = pd.concat([pd.get_dummies(df_test, columns=categorical_columns), df_test[categorical_columns]], axis=1)\nprint df_train.shape, df_test.shape","execution_count":null,"outputs":[],"metadata":{}},{"cell_type":"code","source":"df_train[df_train.columns[df_train.columns.str.startswith(categorical_columns[7])]].head(15)","execution_count":null,"outputs":[],"metadata":{}},{"cell_type":"markdown","source":"### Custom binary encoding\nApply custom binary encoding to discrete numeric features.","metadata":{}},{"cell_type":"code","source":"numerical_discrete_features = ['ps_ind_01', 'ps_ind_03', 'ps_ind_14', 'ps_ind_15', \n                               'ps_reg_01', 'ps_reg_02',\n                               'ps_car_15', 'ps_car_11']","execution_count":null,"outputs":[],"metadata":{"collapsed":true}},{"cell_type":"code","source":"def apply_custom_binary_encoding(column):\n    column_values = df_train[column]\n    unique_values = sorted(df_train[column].unique())\n    for i, value in enumerate(unique_values[0:-1]):\n        new_column_name = '%s_cbe%02d' % (column, i)\n        df_train[new_column_name] = (df_train[column] > value).astype(np.int)\n        df_test[new_column_name] = (df_test[column] > value).astype(np.int)","execution_count":null,"outputs":[],"metadata":{"collapsed":true}},{"cell_type":"code","source":"for column in numerical_discrete_features:\n    apply_custom_binary_encoding(column)\nprint df_train.shape, df_test.shape","execution_count":null,"outputs":[],"metadata":{}},{"cell_type":"code","source":"df_train[df_train.columns[df_train.columns.str.startswith('ps_ind_01')]].head(15)","execution_count":null,"outputs":[],"metadata":{}},{"cell_type":"markdown","source":"### Target encoding","metadata":{}},{"cell_type":"code","source":"def add_noise(series, noise_level):\n    return series * (1 + noise_level * np.random.randn(len(series)))\n\n\ndef target_encode(trn_series=None,\n                  tst_series=None,\n                  target=None,\n                  min_samples_leaf=1,\n                  smoothing=1,\n                  noise_level=0):\n    \"\"\"\n    Smoothing is computed like in the following paper by Daniele Micci-Barreca\n    https://kaggle2.blob.core.windows.net/forum-message-attachments/225952/7441/high%20cardinality%20categoricals.pdf\n    trn_series : training categorical feature as a pd.Series\n    tst_series : test categorical feature as a pd.Series\n    target : target data as a pd.Series\n    min_samples_leaf (int) : minimum samples to take category average into account\n    smoothing (int) : smoothing effect to balance categorical average vs prior\n    \"\"\"\n    assert len(trn_series) == len(target)\n    assert trn_series.name == tst_series.name\n    temp = pd.concat([trn_series, target], axis=1)\n    # Compute target mean\n    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n    # Compute smoothing\n    smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) / smoothing))\n    # Apply average function to all target data\n    prior = target.mean()\n    # The bigger the count the less full_avg is taken into account\n    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n    # Apply averages to trn and tst series\n    ft_trn_series = pd.merge(\n        trn_series.to_frame(trn_series.name),\n        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n        on=trn_series.name,\n        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n    # pd.merge does not keep the index so restore it\n    ft_trn_series.index = trn_series.index\n    ft_tst_series = pd.merge(\n        tst_series.to_frame(tst_series.name),\n        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n        on=tst_series.name,\n        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n    # pd.merge does not keep the index so restore it\n    ft_tst_series.index = tst_series.index\n    return add_noise(ft_trn_series, noise_level), add_noise(ft_tst_series, noise_level)","execution_count":null,"outputs":[],"metadata":{"collapsed":true}},{"cell_type":"code","source":"target_encoding_columns = list(numerical_discrete_features) + list(categorical_columns)\nfor f in target_encoding_columns:\n    df_train[f + \"_tef\"], df_test[f + \"_tef\"] = target_encode(trn_series=df_train[f],\n                                         tst_series=df_test[f],\n                                         target=df_train['target'],\n                                         min_samples_leaf=200,\n                                         smoothing=10,\n                                         noise_level=0)\nprint df_train.shape, df_test.shape","execution_count":null,"outputs":[],"metadata":{}},{"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[],"metadata":{}},{"cell_type":"markdown","source":"### Normalize numerical columns","metadata":{}},{"cell_type":"code","source":"numerical_columns = [column for column in df_train.columns if not 'bin' in column and not 'cat' in column]\nnumerical_columns = [column for column in numerical_columns if not 'cbe' in column and not 'tef' in column]\nnumerical_columns = [column for column in numerical_columns if column.startswith('ps_')]\ntfe_columns = [column for column in df_train.columns if 'tef' in column]\n\nnormalize_columns = numerical_columns + tfe_columns","execution_count":null,"outputs":[],"metadata":{"collapsed":true}},{"cell_type":"code","source":"for column in normalize_columns:\n    mean_value = df_train[column].mean()\n    std_value = df_train[column].std()\n    df_train[column] = (df_train[column] - mean_value)/std_value\n    df_test[column] = (df_test[column] - mean_value)/std_value\n    \n    max_value = np.maximum(df_train[column].max(), -df_train[column].min())\n    MAX_VALUE_ALLOWED = 5.\n    if max_value > MAX_VALUE_ALLOWED:\n        scale = MAX_VALUE_ALLOWED/max_value\n        df_train[column] *= scale\n        df_test[column] *= scale","execution_count":null,"outputs":[],"metadata":{"collapsed":true}},{"cell_type":"markdown","source":"### Organize the columns for feeding the network","metadata":{}},{"cell_type":"markdown","source":"Remove categorical columns because they have been encoded and we don't need them any more.","metadata":{}},{"cell_type":"code","source":"df_train.drop(categorical_columns, axis=1, inplace=True)\ndf_test.drop(categorical_columns, axis=1, inplace=True)\nprint df_train.shape, df_test.shape","execution_count":null,"outputs":[],"metadata":{}},{"cell_type":"markdown","source":"Now we have to divide the features by individual, region and car.\nWe also have to divide them between categorical and non-categorical. In the categorical columns I will encode the custom binary encoding features. In the non-categorical columns I will include the target encode of categorical columns ","metadata":{}},{"cell_type":"code","source":"df_train.columns","execution_count":null,"outputs":[],"metadata":{}},{"cell_type":"code","source":"column_dict = {}\nfor key in ['car', 'ind', 'reg']:\n    column_dict[key] = [column for column in df_train.columns if key in column]\n    column_dict['%s_categorical' % key] = [column for column in column_dict[key] if '_cbe' in column or 'cat_' in column]\n    column_dict['%s_categorical' % key] = [column for column in column_dict['%s_categorical' % key] if 'tef' not in column]\n    column_dict[key] = [column for column in column_dict[key] if column not in column_dict['%s_categorical' % key]]\nfor key in column_dict:\n    print key, len(column_dict[key])","execution_count":null,"outputs":[],"metadata":{}},{"cell_type":"code","source":"for key in column_dict:\n    print key\n    print column_dict[key]\n    print","execution_count":null,"outputs":[],"metadata":{"scrolled":true}},{"cell_type":"markdown","source":"### Prepare the data for keras","metadata":{}},{"cell_type":"code","source":"x = {key: df_train[column_dict[key]].values for key in column_dict}","execution_count":null,"outputs":[],"metadata":{"collapsed":true}},{"cell_type":"code","source":"x_test = {key: df_test[column_dict[key]].values for key in column_dict}","execution_count":null,"outputs":[],"metadata":{"collapsed":true}},{"cell_type":"code","source":"y = df_train.target.values\nids = df_train.id.values","execution_count":null,"outputs":[],"metadata":{"collapsed":true}},{"cell_type":"code","source":"column_dict[key]","execution_count":null,"outputs":[],"metadata":{}},{"cell_type":"markdown","source":"### Load the test prediction of the best model","metadata":{}},{"cell_type":"code","source":"def load_save_dict(filename):\n    with open(filename, 'r') as f:\n        save_dict = cPickle.load(f)\n    return save_dict\nkeras_save_dict = load_save_dict('/media/guillermo/Data/Kaggle/Porto_Safe_Driver/experiments/keras_log_20_5folds/2017_11_05_07_51_47.pkl')\nbest_test_pred= keras_save_dict['test_pred'][:, 0]","execution_count":null,"outputs":[],"metadata":{"collapsed":true}},{"cell_type":"code","source":"keras_save_dict = load_save_dict('/media/guillermo/Data/Kaggle/Porto_Safe_Driver/experiments/keras_log_20_5folds/2017_11_05_07_51_47.pkl')","execution_count":null,"outputs":[],"metadata":{"collapsed":true}},{"cell_type":"code","source":"best_test_pred= keras_save_dict['test_pred'][:, 0]","execution_count":null,"outputs":[],"metadata":{"collapsed":true}},{"cell_type":"code","source":"val_pred = keras_save_dict['val_pred'][:, 0]\nsampling_probabilities = np.abs(df_train.target.values - val_pred)","execution_count":null,"outputs":[],"metadata":{"collapsed":true}},{"cell_type":"markdown","source":"## Function for getting the score","metadata":{}},{"cell_type":"code","source":"def save_log(filepath, params, time_stamp, gini_val_list, \n             gini_train_list, best_epoch_list, new_gini_val_list, \n             gini_test, optimizer_score):\n    if not os.path.exists(filepath):\n        with open(filepath, 'w') as f:\n            f.write('\\t'.join(['timestamp', 'new_gini_val_score', 'gini_test', 'gini_val_mean', \n                               'gini_train_mean', 'best_epoch',\n                               'gini_val_std', 'gini_train_std','params']) + '\\n')\n    with open(filepath, 'a') as f:\n        text = time_stamp + '\\t'\n        text += '%.4f\\t%.4f\\t%.4f\\t' % (new_gini_val_list[-1], gini_test, optimizer_score)\n        text += '%.4f\\t%.4f\\t' % (np.mean(gini_val_list), np.mean(gini_train_list))\n        text += '%.1f\\t' % np.mean(best_epoch_list)\n        text += '%.4f\\t%.4f\\t%s\\n' % (np.std(gini_val_list), np.std(gini_train_list), params)\n        f.write(text)","execution_count":null,"outputs":[],"metadata":{"collapsed":true}},{"cell_type":"code","source":"def get_keras_model(encoding_conf, layers, dropout_rates, l1=0, l2=0, encoding_activation='relu'):\n    # Create the encoding\n    encoding_list = []\n    input_list = []\n    for key in ['reg', 'car', 'ind']:\n        categorical_key = '%s_categorical' % key\n        input_layer = Input(shape=(x[categorical_key].shape[1],), name=categorical_key)\n        input_list.append(input_layer)\n        encoding = Dense(int(encoding_conf[categorical_key]), \n                         activation=encoding_activation, name='%s_encoding' % categorical_key, \n                        kernel_regularizer=regularizers.l1_l2(l1, l2))(input_layer)\n        input_layer = Input(shape=(x[key].shape[1],), name=key)\n        input_list.append(input_layer)\n        encoding_input = Concatenate(axis=1)([input_layer, encoding])\n        encoding = Dense(int(encoding_conf[key]), activation=encoding_activation, name='%s_encoding' % key,\n                        kernel_regularizer=regularizers.l1_l2(l1, l2))(encoding_input)\n        encoding_list.append(encoding)\n        \n    encoding = Concatenate(axis=1)(encoding_list)\n    \n    first_layer = True\n    for n_units, drop in zip(layers, dropout_rates):\n        if first_layer:\n            output = Dense(n_units, activation='relu')(encoding)\n            first_layer = False\n        else:\n            output = Dense(n_units, activation='relu')(output)\n        if drop > 0:\n            output = Dropout(drop)(output)\n    # Add the final layer\n    output = Dense(1, activation='sigmoid', name='output')(output)\n    \n    model = Model(inputs=input_list, outputs=output)\n    model.compile(loss='binary_crossentropy',  optimizer='RMSprop')\n    return model","execution_count":null,"outputs":[],"metadata":{"collapsed":true}},{"cell_type":"code","source":"def get_upsampled_index(y, train_index, n_upsampling):\n    positive_index = train_index[y[train_index] == 1]\n    upsampled_index = train_index.tolist() + positive_index.tolist()*(n_upsampling)\n    np.random.shuffle(upsampled_index)\n    return upsampled_index","execution_count":null,"outputs":[],"metadata":{"collapsed":true}},{"cell_type":"code","source":"def get_noisy_target(y, prob):\n    if prob == 0:\n        return y\n    noise = np.random.binomial(1, prob, y.shape)\n    noisy_target = noise + y\n    noisy_target[noisy_target == 2] = 0\n    return noisy_target","execution_count":null,"outputs":[],"metadata":{"collapsed":true}},{"cell_type":"code","source":"def plot_train_evolution(val_score):\n    plt.figure(figsize=(12, 6))\n    plt.plot(val_score, label='val')\n    plt.plot(val_score, 'ro')\n    plt.ylabel('validation score')\n    plt.xlabel('Number of epochs')\n    plt.ylim(ymin=np.max(val_score) - 0.01)\n    plt.show()","execution_count":null,"outputs":[],"metadata":{"collapsed":true}},{"cell_type":"code","source":"def get_score(params):\n    # Get the parameters for the model\n    model_layers = [int(params['n_units_per_layer'])]*int(params['n_layers'])\n    model_dropout_rates = [params['dropout_rate']]*int(params['n_layers'])    \n    \n    time_stamp = time.strftime(\"%Y_%m_%d_%H_%M_%S\")\n    gini_train_list, gini_val_list, best_epoch_list  = [], [], []\n    test_pred_list, val_pred_list, val_id_list = [], [], []\n    new_gini_val_list = []\n    \n    random_state = -1\n    sys.stdout.flush()\n    for i in tqdm(range(params['max_experiments'])):\n        random_state += 1\n        \n        fold_val_ids = []\n        unsorted_val_preds = []\n        \n        for n_fold in range(params['n_folds']):\n            kf = KFold(n_splits=params['n_folds'], shuffle=True, random_state=random_state)\n            partitions = [_ for _ in kf.split(y)]\n            train_index, val_index = partitions[n_fold]\n\n            x_train = {key:x[key][train_index] for key in x}\n            x_val = {key:x[key][val_index] for key in x}\n            upsampled_probs = y[train_index]*params['n_upsampling'] + 1\n            train_sample_probs = sampling_probabilities[train_index]\n            train_sample_probs *= upsampled_probs\n            train_sample_probs /= np.sum(train_sample_probs)\n            #print 'train_sample_probs: ', train_sample_probs[0:20]\n\n            model = get_keras_model(params['encoding_conf'], encoding_activation=params['encoding_activation'],\n                                    layers=model_layers, dropout_rates=model_dropout_rates,\n                                    l1=params['l1'], l2=params['l2'])\n\n            model_gini_train_list = []\n            model_gini_val_list = []\n            best_weights = None\n            for epoch in range(params['max_epochs']):\n                for _ in range(params['val_period']):\n                    epoch_index = np.random.choice(train_index, size=params['epoch_size'],\n                                                   p=train_sample_probs, replace=False)\n                    x_train_epoch = {key:x[key][epoch_index] for key in x}\n                    model.fit(x=x_train_epoch, y=y[epoch_index], epochs=1, \n                              batch_size=params['batch_size'], verbose=False)\n\n                preds_val = model.predict(x=x_val, batch_size=params['batch_size'])\n                gini_val = gini_normalized(y[val_index], preds_val)\n                model_gini_val_list.append(gini_val)\n\n                best_epoch = np.argmax(model_gini_val_list)\n                if best_epoch == epoch:\n                    best_weights = model.get_weights()\n                if epoch - best_epoch >= params['patience']:\n                    break\n\n            best_epoch = np.argmax(model_gini_val_list)\n            best_epoch_list.append(best_epoch)\n            gini_val_list.append(model_gini_val_list[best_epoch])\n\n            model.set_weights(best_weights)\n            preds_test = preds_val = model.predict(x=x_test, batch_size=params['batch_size'])\n            test_pred_list.append(preds_test)\n\n            preds_train = model.predict(x=x_train, batch_size=params['batch_size'])\n            gini_train = gini_normalized(y[train_index], preds_train)\n            gini_train_list.append(gini_train)\n\n            preds_val = model.predict(x=x_val, batch_size=params['batch_size'])\n            unsorted_val_preds.append(preds_val)\n            fold_val_ids.append(ids[val_index])\n\n            if params['verbose']:\n                print colored('Gini val: %.4f\\tGini train: %.4f' % (gini_val_list[-1], gini_train_list[-1]), 'blue')\n                plot_train_evolution(model_gini_val_list)\n        # Sort the validation predictions\n        fold_val_ids = np.concatenate(fold_val_ids)\n        unsorted_val_preds = np.concatenate(unsorted_val_preds)\n        sorted_index = np.argsort(fold_val_ids)\n        sorted_val_preds = unsorted_val_preds[sorted_index]\n        val_pred_list.append(sorted_val_preds)\n        # Get the gini validation score\n        new_gini_val = gini_normalized(y, np.mean(val_pred_list, axis=0))\n        new_gini_val_list.append(new_gini_val)\n        # Get test score\n        test_pred_mean = np.mean(test_pred_list, axis=0)\n        gini_test = gini_normalized(best_test_pred, test_pred_mean)\n        \n        if params['verbose']:\n            text = 'Gini val: %.4f\\tGini test: %.4f' % (new_gini_val, gini_test)\n            print colored(text, 'blue')\n     \n    gini_train_score = np.mean(gini_train_list)\n    gini_val_score = np.mean(gini_val_list)\n    gini_val = new_gini_val_list[-1]\n    \n    \n    print time_stamp\n    print colored('params: %s' % params, 'green')\n    print colored('Gini val score: %.4f' % gini_val, 'green')\n    print colored('Gini test score: %.4f' % gini_test, 'green')\n    \n    optimizer_score = gini_test - 2.9949206966767021*gini_val - 0.12420528931875374\n    \n    save_log(params['log_file'], params, time_stamp, gini_val_list, \n             gini_train_list, best_epoch_list, new_gini_val_list,\n             gini_test, optimizer_score)\n    save_dict = {\n        'gini_test': gini_test,\n        'test_pred': test_pred_mean,\n        'val_pred': np.mean(val_pred_list, axis=0),\n        'new_gini_val_list': new_gini_val_list,\n        'gini_train_list': gini_train_list,\n        'gini_val_list': gini_val_list,\n        'params': params,\n        'time_stamp': time_stamp,\n        'best_epoch_list': best_epoch_list,\n    }\n    dirname = os.path.splitext(os.path.basename(params['log_file']))[0]\n    pickle_path = os.path.join(DATA_DIR, 'experiments', dirname, '%s.pkl' % time_stamp)\n    if not os.path.exists(os.path.dirname(pickle_path)):\n        os.mkdir(os.path.dirname(pickle_path))\n    with open(pickle_path, 'w') as f:\n        cPickle.dump(save_dict, f)\n        \n    \n    \n    return optimizer_score","execution_count":null,"outputs":[],"metadata":{"collapsed":true}},{"cell_type":"markdown","source":"## Speed up","metadata":{}},{"cell_type":"markdown","source":"I think there are too much evaluations, let's divide them by 4 and 2","metadata":{}},{"cell_type":"code","source":"sampling_probabilities[:] = 1","execution_count":null,"outputs":[],"metadata":{"collapsed":true}},{"cell_type":"code","source":"raise","execution_count":null,"outputs":[],"metadata":{}},{"cell_type":"code","source":"params = {\n    'encoding_conf': {\n        'reg_categorical': 3.0, \n        'car_categorical': 1, \n        'ind_categorical': 50.0, \n        'ind': 70.0, \n        'car': 35.0, \n        'reg': 34.0}, \n    'n_layers': 1, \n    'n_units_per_layer': 90.0, \n    'dropout_rate': 0.5, \n    'encoding_activation': 'tanh', \n    \n    'l2': 0.0001, \n    'l1': 1e-05, \n    \n    'batch_size': 2048,\n    'val_period': 4,\n    'epoch_size': 100000, \n    'patience': 12, \n    'n_upsampling': 25, \n    'n_folds': 5, \n    'max_epochs': 1000, \n    'max_experiments': 10, \n    'verbose': True, \n    'log_file': '../logs/keras_v31_5folds.csv'}\nget_score(params)","execution_count":null,"outputs":[],"metadata":{"scrolled":true}},{"cell_type":"code","source":"params = {\n    'encoding_conf': {\n        'reg_categorical': 3.0, \n        'car_categorical': 1, \n        'ind_categorical': 50.0, \n        'ind': 70.0, \n        'car': 35.0, \n        'reg': 34.0}, \n    'n_layers': 1, \n    'n_units_per_layer': 90.0, \n    'dropout_rate': 0.5, \n    'encoding_activation': 'tanh', \n    \n    'l2': 0.0001, \n    'l1': 1e-05, \n    \n    'batch_size': 2048,\n    'val_period': 8,\n    'epoch_size': 50000, \n    'patience': 12, \n    'n_upsampling': 25, \n    'n_folds': 5, \n    'max_epochs': 1000, \n    'max_experiments': 10, \n    'verbose': True, \n    'log_file': '../logs/keras_v31_5folds.csv'}\nget_score(params)","execution_count":null,"outputs":[],"metadata":{}},{"cell_type":"code","source":"params = {\n    'encoding_conf': {\n        'reg_categorical': 3.0, \n        'car_categorical': 1, \n        'ind_categorical': 50.0, \n        'ind': 70.0, \n        'car': 35.0, \n        'reg': 34.0}, \n    'n_layers': 1, \n    'n_units_per_layer': 90.0, \n    'dropout_rate': 0.5, \n    'encoding_activation': 'tanh', \n    \n    'l2': 0.0001, \n    'l1': 1e-05, \n    \n    'batch_size': 2048,\n    'val_period': 16,\n    'epoch_size': 25000, \n    'patience': 12, \n    'n_upsampling': 25, \n    'n_folds': 5, \n    'max_epochs': 1000, \n    'max_experiments': 10, \n    'verbose': True, \n    'log_file': '../logs/keras_v31_5folds.csv'}\nget_score(params)","execution_count":null,"outputs":[],"metadata":{}},{"cell_type":"code","source":"","execution_count":null,"outputs":[],"metadata":{"collapsed":true}}],"nbformat":4,"nbformat_minor":1,"metadata":{"language_info":{"file_extension":".py","mimetype":"text/x-python","codemirror_mode":{"version":2,"name":"ipython"},"name":"python","version":"2.7.13","pygments_lexer":"ipython2","nbconvert_exporter":"python"},"nbpresent":{"themes":{},"slides":{"1ee3170a-b315-438d-8b07-773dc39d544d":{"id":"1ee3170a-b315-438d-8b07-773dc39d544d","regions":{},"prev":null}}},"kernelspec":{"language":"python","display_name":"Python 2","name":"python2"}}}