{"metadata":{"language_info":{"pygments_lexer":"ipython3","file_extension":".py","mimetype":"text/x-python","version":"3.6.3","nbconvert_exporter":"python","name":"python","codemirror_mode":{"version":3,"name":"ipython"}},"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"}},"nbformat":4,"nbformat_minor":1,"cells":[{"cell_type":"markdown","source":"\nBased on work done on this notebook, https://www.kaggle.com/sudosudoohio/stratified-kfold-xgboost-eda-tutorial-0-281.\nFilled in missing values with Median and  the public score increased from 0.281 to 0.285 after training with new data. \nAlso, custom weights are given to the labels to tackle imbalanced labels in the data. However, performance hasn't improved.\n\nHope you will find the notebook helpful! .\n","metadata":{"_cell_guid":"49635227-fe34-4a12-845e-12fd7ed9b3bf","_uuid":"fffbdd4f2428560a066c240064a3c60c6a46edbe"}},{"cell_type":"code","execution_count":null,"source":"## Importing Requried Libraries\nimport numpy as np\nimport subprocess\nimport pandas as pd\n\nfrom IPython.display import Image\n\nfrom collections import Counter\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.model_selection import StratifiedKFold\nimport xgboost as xgb\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.metrics import log_loss, accuracy_score\n\n# classifiers\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# reproducibility\nseed = 123","outputs":[],"metadata":{"_cell_guid":"1899d740-4d31-4e2b-90d1-f5f5f2e88056","_uuid":"896e8658c6a2145b6654a4130e72a6a89b491d96","collapsed":true}},{"cell_type":"code","execution_count":null,"source":"#### LOADING DATA ####\n### TRAIN DATA\ntrain_data = pd.read_csv(\"../input/train.csv\", na_values='-1')\ncor_data = train_data.copy()\n                        \n## Filling the missing data NAN with median of the column\ntrain_data_nato_median = pd.DataFrame()\nfor column in train_data.columns:\n    train_data_nato_median[column] = train_data[column].fillna(train_data[column].median())\n\ntrain_data = train_data_nato_median.copy()\n\n### TEST DATA\ntest_data = pd.read_csv(\"../input/test.csv\", na_values='-1')\n## Filling the missing data NAN with mean of the column\ntest_data_nato_median = pd.DataFrame()\nfor column in test_data.columns:\n    test_data_nato_median[column] = test_data[column].fillna(test_data[column].median())\n    \ntest_data = test_data_nato_median.copy()\ntest_data_id = test_data.pop('id')","outputs":[],"metadata":{"_cell_guid":"708ddfef-023b-490e-ae61-e467e283802a","_uuid":"67f598fd71cd27c05a31b57cf9e7697d1482c767","collapsed":true}},{"cell_type":"code","execution_count":null,"source":"## Identifying Categorical data\ncolumn_names = train_data.columns\ncategorical_column = column_names[column_names.str[10] == 'c']\n\n## Changing categorical columns to category data type\ndef int_to_categorical(data):\n    \"\"\" \n    changing columns to catgorical data type\n    \"\"\"\n    for column in categorical_column:\n        data[column] =  data[column].astype('category')","outputs":[],"metadata":{"_cell_guid":"d1f96e8b-fb3c-40b1-8092-12b5ec9c93ea","_uuid":"e4a9310d991ee8ecb21360c4654d1864a5425897","collapsed":true}},{"cell_type":"code","execution_count":null,"source":"## Creating list of train and test data and converting columns of interest to categorical type\ndatas = [train_data,test_data]\n\nfor data in datas:\n    int_to_categorical(data)\n\n#print(test_data.dtypes)","outputs":[],"metadata":{"_cell_guid":"6743fa84-191d-4608-ae93-900305d22801","_uuid":"61ba13b4285b66805e134ea8392680a1629b7480","collapsed":true}},{"cell_type":"code","execution_count":null,"source":"## Decribing categorical variables\n# def decribe_Categorical(x):\n#     \"\"\" \n#     Function to decribe Categorical data\n#     \"\"\"\n#     from IPython.display import display, HTML\n#     display(HTML(x[x.columns[x.dtypes ==\"category\"]].describe().to_html))\n\n# decribe_Categorical(train_data)","outputs":[],"metadata":{"_cell_guid":"3134726c-67bb-465e-8231-918577724631","_uuid":"8fae15b916487e22cb01d4ea0a9012cc2d5e09ac","collapsed":true}},{"cell_type":"code","execution_count":null,"source":"### FUNCTION TO CREATE DUMMIES COLUMNS FOR CATEGORICAL VARIABLES\ndef creating_dummies(data):\n    \"\"\"creating dummies columns categorical varibles\n    \"\"\"\n    for column in categorical_column:\n        dummies = pd.get_dummies(data[column],prefix=column)\n        data = pd.concat([data,dummies],axis =1)\n        ## dropping the original columns ##\n        data.drop([column],axis=1,inplace= True)","outputs":[],"metadata":{"_cell_guid":"6f0bdecf-8461-4736-88b8-ae41c6aea95c","_uuid":"763aef1c801f3f20b7365bf13ac6410925377956","collapsed":true}},{"cell_type":"code","execution_count":null,"source":"### CREATING DUMMIES FOR CATEGORICAL VARIABLES  \nfor column in categorical_column:\n        dummies = pd.get_dummies(train_data[column],prefix=column)\n        train_data = pd.concat([train_data,dummies],axis =1)\n        train_data.drop([column],axis=1,inplace= True)\n\n\nfor column in categorical_column:\n        dummies = pd.get_dummies(test_data[column],prefix=column)\n        test_data = pd.concat([test_data,dummies],axis =1)\n        test_data.drop([column],axis=1,inplace= True)\n\nprint(train_data.shape)\nprint(test_data.shape)","outputs":[],"metadata":{"_cell_guid":"e3d43674-aff5-4b79-88dd-d91279fc02b1","_uuid":"368485254ab1ac1140282022ca6dccb2cd4b5008","collapsed":true}},{"cell_type":"code","execution_count":null,"source":"#Define covariates in X and dependent variable in y\nfeatures = train_data.iloc[:,2:] ## FEATURE DATA\ntargets= train_data.target ### LABEL DATA\n\n### CHECKING DIMENSIONS\nprint(features.shape)\nprint(targets.shape)","outputs":[],"metadata":{"_cell_guid":"949e5932-c260-4ad2-a6de-aec29f28f5e4","_uuid":"b86c7493a3549c27d4b10d4a00cbd2efd112daf0","collapsed":true}},{"cell_type":"code","execution_count":null,"source":"ax = sns.countplot(x = targets ,palette=\"Set2\")\nsns.set(font_scale=1.5)\nax.set_xlabel(' ')\nax.set_ylabel(' ')\nfig = plt.gcf()\nfig.set_size_inches(10,5)\nax.set_ylim(top=700000)\nfor p in ax.patches:\n    ax.annotate('{:.2f}%'.format(100*p.get_height()/len(targets)), (p.get_x()+ 0.3, p.get_height()+10000))\n\nplt.title('Distribution of 595212 Targets')\nplt.xlabel('Initiation of Auto Insurance Claim Next Year')\nplt.ylabel('Frequency [%]')\nplt.show()","outputs":[],"metadata":{"_cell_guid":"7df178da-7c3a-45e9-865e-6abb6c3bcc44","_uuid":"3966583665e454cebacf83df86c1a672fa383d12","collapsed":true}},{"cell_type":"code","execution_count":null,"source":"sns.set(style=\"white\")\n\n\n# Compute the correlation matrix\ncorr = cor_data.corr()\n\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n\nplt.show()","outputs":[],"metadata":{"_cell_guid":"2b32c1b6-ea9e-4c26-b7bf-95459927e99c","_uuid":"dd467e34ba8e6d864d647305c6fb21537f18b565","collapsed":true}},{"cell_type":"code","execution_count":null,"source":"# Define the gini metric - from https://www.kaggle.com/c/ClaimPredictionChallenge/discussion/703#5897\ndef gini(actual, pred, cmpcol = 0, sortcol = 1):\n    assert( len(actual) == len(pred) )\n    all = np.asarray(np.c_[ actual, pred, np.arange(len(actual)) ], dtype=np.float)\n    all = all[ np.lexsort((all[:,2], -1*all[:,1])) ]\n    totalLosses = all[:,0].sum()\n    giniSum = all[:,0].cumsum().sum() / totalLosses\n    \n    giniSum -= (len(actual) + 1) / 2.\n    return giniSum / len(actual)\n \ndef gini_normalized(a, p):\n    return gini(a, p) / gini(a, a)\n\ndef gini_xgb(preds, dtrain):\n    labels = dtrain.get_label()\n    gini_score = gini_normalized(labels, preds)\n    return 'gini', gini_score","outputs":[],"metadata":{"_cell_guid":"123d5459-dbe8-4df4-b0a5-4ad94f036fa9","_uuid":"0c5c0462ff9983bdcf1c89c884c0fc9442e4e56b","collapsed":true}},{"cell_type":"code","execution_count":null,"source":"unwanted = features.columns[features.columns.str.startswith('ps_calc_')]\nprint(unwanted)","outputs":[],"metadata":{"_cell_guid":"fbfa83d5-2f86-4365-b4a7-378889ed4b17","_uuid":"a77fa700d92f39ac0f023b0b2c7d66c6b4f59daa","collapsed":true}},{"cell_type":"code","execution_count":null,"source":"train = features.drop(unwanted, axis=1)  \ntest = test_data.drop(unwanted, axis=1)  \n\nprint(train.shape)\nprint(test.shape)","outputs":[],"metadata":{"_cell_guid":"5d643323-6d06-4749-84c2-ca274798c2e2","_uuid":"1db3335c21c84f9a553528e3808b2ba5d8d97bca","collapsed":true}},{"cell_type":"code","execution_count":null,"source":"## Spliting train data \nkfold = 2 ## I used 5 Kfolds. In interest of computational time using 2\nskf = StratifiedKFold(n_splits=kfold, random_state=42)","outputs":[],"metadata":{"_cell_guid":"57912ae9-09de-4377-b051-255bb6a041ba","_uuid":"3c2ece821dc84ba4ad6fa2a9a1766d1dc34e1318","collapsed":true}},{"cell_type":"code","execution_count":null,"source":"## Specifiying parameters\nparams = {\n    'min_child_weight': 10.0,\n    'objective': 'binary:logistic',\n    'max_depth': 7,\n    'max_delta_step': 1.8,\n    'colsample_bytree': 0.4,\n    'subsample': 0.8,\n    'eta': 0.025,\n    'gamma': 0.65,\n    'num_boost_round' : 700\n    }","outputs":[],"metadata":{"_cell_guid":"fc99b487-5f10-4ea3-b5cf-1e089958156d","_uuid":"39cd5d7e4980878ebb0f44c4bc3c3066ad9e0ce4","collapsed":true}},{"cell_type":"code","execution_count":null,"source":"# Converting pandas series to numpy array to be fed to XGBoost package\nX= train.values\ny = targets.values\n\ntype(X),type(y)","outputs":[],"metadata":{"_cell_guid":"c086e7e0-b1fb-402f-9139-0c2f5453d613","_uuid":"8a33fb3638a0fcc0f426226a39f44d6766f7fcfa","collapsed":true}},{"cell_type":"code","execution_count":null,"source":"# Submission data frame\nsub = pd.DataFrame()\nsub['id'] = test_data_id\nsub['target'] = np.zeros_like(test_data_id)\nsub.shape","outputs":[],"metadata":{"_cell_guid":"18f4994b-7519-4e86-a0c2-5472e952dfa7","_kg_hide-output":false,"_uuid":"17f733b2d78191c0aec31f70630bc02cfd6425bd","collapsed":true,"_kg_hide-input":false}},{"cell_type":"markdown","source":"We are providing custom weights to the labels in the data. Here I used two ways to do it,\n1. Manually assigning weights to the label as by numpy array 'weights' in the code\n2. using 'scale_pos_weights' parameter","metadata":{"_cell_guid":"0adb0108-b6f0-4a8d-91c2-90bd0eed79f1","_uuid":"9bd061364e5f5abf9677ad28988b1be175b64cde"}},{"cell_type":"code","execution_count":null,"source":"## Model fitting\n# THis is where the magic happens\n\nfor i, (train_index, test_index) in enumerate(skf.split(X, y)):\n    print('[Fold %d/%d]' % (i + 1, kfold))\n    X_train, X_valid = X[train_index], X[test_index]\n    y_train, y_valid = y[train_index], y[test_index]\n    ### Custom weights: To deal with imbalanced label\n    #weights = np.zeros(len(y_train))\n    #weights[y_train == 0] = 1\n    #weights[y_train == 1] = 9            \n    #d_train = xgb.DMatrix(X_train, label = y_train, weight = weights)\n    \n    \n    d_train = xgb.DMatrix(X_train, label = y_train)\n    d_valid = xgb.DMatrix(X_valid, y_valid)\n    d_test = xgb.DMatrix(test.values)\n    watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n    \n    ### USing Scale_pos_weights parameter\n    ## In this case we are splitting giving weights to the label according \n    # to their relative ratio's in the data\n    \n   # train_labels = d_train.get_label()\n   # ratio = float(np.sum(train_labels == 0))/ np.sum(train_labels == 1)\n   # params['scale_pos_weight'] = ratio\n\n    # Train the model! We pass in a max of 1,600 rounds (with early stopping after 70)\n    # and the custom metric (maximize=True tells xgb that higher metric is better)\n    mdl = xgb.train(params, d_train, 1600, watchlist, early_stopping_rounds=70, \n                    feval=gini_xgb, maximize=True, verbose_eval=100)\n\n    print('[Fold %d/%d Prediciton:]' % (i + 1, kfold))\n    # Predict on our test data\n    p_test = mdl.predict(d_test, ntree_limit=mdl.best_ntree_limit)\n    sub['target'] += p_test/kfold","outputs":[],"metadata":{"_cell_guid":"ab801f2a-6051-46b9-a810-ffe2a7f355b9","_uuid":"8bc351192e885240e800dc1535cfc91948479c68","collapsed":true}},{"cell_type":"markdown","source":"*OUTPUT*\nAdding each layer of topic of the other:\n1. Base XGBoost: 0.281\n2. XGBoost with missing values filled with Median(equal class weights): 0.285\n3. XGbosot with custom weight: 0.282\n4. XGboost with scale_pos_weights parameter: 0.280","metadata":{"_cell_guid":"3cf1a555-3c28-4e7c-9ac0-4efac6920bd0","_uuid":"be516852c13a25037e8b7198f41d487cca20c846"}},{"cell_type":"code","execution_count":null,"source":"#sub.to_csv(\"ModifiedXGBOOST.csv\",index =  False)","outputs":[],"metadata":{"_cell_guid":"ed0df5da-fbe5-4a47-93a4-f20ad0a494f1","_uuid":"6023489fd32873f6fd82930bd7c7e8c875544af6","collapsed":true}},{"cell_type":"markdown","source":"**FUTURE WORK:**\n1. Planning on using Gridsearch \n2. Dealing with Imbalanced data such as this,\n    For class imbalance, we have many methods that could deal with the problem. There is no one method that could essentially solve the issue every single time. Here are few methods that we could use to deal with Class imbalance.\n\n1. Weighted loss functions: Giving higher weights to minority class(in this case, class 1). For example, Logistic regression model has inbuilt parameter 'class_weights' which can used to assign weights to the class labels as per choice.\n2. Random Undersampling: As the name suggests, randomly undersample examples from majority class\n3. NEARMISS-1 : Retain points from majority class whose mean distance to the K nearest points in S is lowest\n4. NEARMISS-2: Keep points from majority class whose mean distance to the K farthest points in minority class is lowest\n5. NEARMISS-3: Select K nearest neighbours In majority class for every point in minority class\n6. Condensed Nearest Neighbour(CNN)\n7. Edited Nearest Neighbour(ENN)\n8. Repeated Edited Nearest Neighbour\n9. TOMEK LINK REMOVAL\n10. Random Oversampling\n11. Synthetic Minority Oversampling Technique(SMOTE)\n12. SMOTE + Tomek Link Removal\n13. SMOTE + ENN\n14. EASYENSEMBLE\n15. BALANCECASCADE and many more\n\nReference: \n1. https://www.youtube.com/watch?v=-Z1PaqYKC1w\n2. https://www.youtube.com/watch?v=32tXCJP0HYc&list=PLZnYQQzkMilqTC12LmnN4WpQexB9raKQG&index=13","metadata":{"_cell_guid":"366a1d9e-8459-4f2e-9e14-f9f848482e98","_uuid":"34eed91f65557ef407bd266b6988fc3a4ad60392"}}]}