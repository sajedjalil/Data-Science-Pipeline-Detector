{"cells":[{"outputs":[],"execution_count":null,"source":"# Some concepts borrowed from other kernels (will give credit later)\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport warnings\nfrom collections import Counter\nfrom sklearn.feature_selection import mutual_info_classif\nwarnings.filterwarnings('ignore')","metadata":{},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"train = pd.read_csv(\"train.csv\")\n#train.head()","metadata":{"collapsed":true},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"train_copy = train\ntrain_copy = train_copy.replace(-1, np.NaN)","metadata":{"collapsed":true},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"import missingno as msno\n# Nullity or missing values by columns\nmsno.matrix(df=train_copy.iloc[:,2:39], figsize=(20, 14), color=(0.42, 0.1, 0.05))\n\n#ps_reg_03, ps_car_03_cat and ps_car_05_cat has many missing values, hence we need to be carefull while doing NA. For the time being we will just do na, but later try different things","metadata":{},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"data = [go.Bar(\n            x = train[\"target\"].value_counts().index.values,\n            y = train[\"target\"].value_counts().values,\n            text='Distribution of target variable'\n    )]\n\nlayout = go.Layout(\n    title='Target variable distribution'\n)\n\nfig = go.Figure(data=data, layout=layout)\n\npy.iplot(fig, filename='basic-bar')\n\n#Skewed target, Hence F1 score is more important than accuracy","metadata":{},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"#Remove colums if contains all NULL (none so here)\ntrain = train.dropna(axis=1, how='all')","metadata":{"collapsed":true},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"cols = train.columns.tolist()\nprint cols","metadata":{},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"target = train.target\ntrain.drop('target', axis=1, inplace=True)\ntrain.drop('id', axis=1, inplace=True)\ntrain.dtypes\n\n#float64 are continuous variable , int64 are either binary or categorical","metadata":{},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"cols = train.columns.tolist()\nprint len(cols)\nprint(train.skew())\n# May require transformaion of the skewed variables","metadata":{},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"train.describe()\n# Different stastical figures","metadata":{},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"from sklearn import preprocessing\ndef draw_histograms(df, variables, n_rows, n_cols):\n    fig=plt.figure(figsize=(60,80))\n    for i, var_name in enumerate(variables):\n        ax=fig.add_subplot(n_rows,n_cols,i+1)\n        df[var_name].hist(bins=10,ax=ax)\n        ax.set_title(var_name)\n    fig.tight_layout()  # Improves appearance a bit.\n    plt.show()\n\nle = preprocessing.LabelEncoder()\nfor x in range(len(cols)):\n    typ = train[cols[x]].dtype\n    if typ == 'int64':\n        train[cols[x]] = train[cols[x]].fillna(value=0)\n    elif typ == 'float64':\n        train[cols[x]] = train[cols[x]].fillna(value=0.0)\n    elif typ == 'object':\n        pass\n        train[cols[x]] = train[cols[x]].fillna(value=0)\ndraw_histograms(train, train.columns, 8, 8)\n\n#looking at the histogram of the input variable, some type of normalization of feature scaling may be required for some variables","metadata":{},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"train.plot.box(return_type='axes', figsize=(90,70))\n#Box plot of all varibales https://www.wellbeingatschool.org.nz/information-sheet/understanding-and-interpreting-box-plots","metadata":{},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"Counter(train.dtypes.values)","metadata":{},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"train_float = train.select_dtypes(include=['float64'])\ntrain_int = train.select_dtypes(include=['int64'])","metadata":{"collapsed":true},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"colormap = plt.cm.inferno\nplt.figure(figsize=(16,12))\nplt.title('Pearson correlation of continuous features', y=1.05, size=15)\nsns.heatmap(train_float.corr(),linewidths=0.1,vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=True)\n\n#pearson correlation of continuous varibales shows some strong correlation between variables, we may have to drop some correlated varibales or have to transform them to new variables","metadata":{},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"cat_features = [a for a in train.columns if a.endswith('cat')]\nprint cat_features","metadata":{},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"import scipy \nfrom scipy.stats import spearmanr\nfrom pylab import rcParams","metadata":{"collapsed":true},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"train_cat = train[cat_features]\ntrain_cat.head()","metadata":{},"cell_type":"code"},{"source":"","metadata":{},"cell_type":"raw"},{"outputs":[],"execution_count":null,"source":"bin_features = [a for a in train.columns if a.endswith('bin')]\nprint bin_features","metadata":{},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"bin_train = train[bin_features]\nbin_train.head()","metadata":{},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"#rcParams['figure.figsize'] = 5, 4\n#sns.set_style(\"whitegrid\")","metadata":{"collapsed":true},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"#sns.pairplot(train_cat)","metadata":{"collapsed":true},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"from scipy.stats import chi2_contingency\nfor x in range(len(cat_features)):\n    for y in range((x+1), len(cat_features)):\n        col1 = train_cat[cat_features[x]]\n        col2 = train_cat[cat_features[y]]\n        table = pd.crosstab(col1,col2)\n        chi2, p, dof, expected = chi2_contingency(table.values)\n        print cat_features[x], cat_features[y], ':'\n        print 'Chi-square statistics: %0.3f p_value: %0.3f' % (chi2, p)\n        \n#chisquare test show heavy correlation between categorical vribales, not sure about it","metadata":{},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"#from scipy.stats import chi2_contingency\nfor x in range(len(bin_features)):\n    for y in range((x+1), len(bin_features)):\n        col1 = bin_train[bin_features[x]]\n        col2 = bin_train[bin_features[y]]\n        table = pd.crosstab(col1,col2)\n        chi2, p, dof, expected = chi2_contingency(table.values)\n        print bin_features[x], bin_features[y], ':'\n        print 'Chi-square statistics: %0.3f p_value: %0.3f' % (chi2, p)\n        \n#chisquare test of binary variables","metadata":{},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"#sns.pairplot(bin_train)","metadata":{"collapsed":true},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"from sklearn.feature_selection import VarianceThreshold\ntrain_bin_copy = bin_train\nsel = VarianceThreshold(threshold=(.8 * (1 - .8)))\nsel.fit_transform(train_bin_copy)\n\nprint train_bin_copy.columns\nprint bin_train.columns","metadata":{},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"from sklearn.feature_selection import VarianceThreshold\ntrain_cat_copy = train_cat\nsel = VarianceThreshold(threshold=(.8 * (1 - .8)))\nsel.fit_transform(train_cat_copy)\n\nprint train_cat_copy.columns\nprint train_cat.columns","metadata":{},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\nprint bin_train.head()\nX_new = SelectKBest(chi2, k=5).fit_transform(bin_train, target)\nX_new.shape\n\n#SelectKbest does univariate feature selection , can ue chi2, ANOVA etc. http://scikit-learn.org/stable/modules/feature_selection.html\n# K is number of most important features","metadata":{},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"print X_new","metadata":{},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"#Tree based feature selection, I would first do the hyperparameter tuning usig GBM and then use SelectFromModel to choose the best features\n#  have used the the link to do hyperparameter tuning\n\nfrom sklearn.ensemble import GradientBoostingClassifier  #GBM algorithm\nfrom sklearn import cross_validation, metrics   #Additional scklearn functions\nfrom sklearn.grid_search import GridSearchCV   #Perforing grid search\n\n","metadata":{"collapsed":true},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"#I ran following gridsearch method to come up with best parameters, hence commenting them\n\n#param_test1 = {'n_estimators':range(20,81,10)}\n#gsearch1 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, min_samples_split=500,min_samples_leaf=50,max_depth=8,max_features='sqrt',subsample=0.8,random_state=10), \n#param_grid = param_test1, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n#gsearch1.fit(train, target)","metadata":{"collapsed":true},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"#gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_","metadata":{"collapsed":true},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"#param_test2 = {'max_depth':range(5,16,2), 'min_samples_split':range(200,1001,200)}\n#gsearch2 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, n_estimators=60, max_features='sqrt', subsample=0.8, random_state=10), \n#param_grid = param_test2, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n#gsearch2.fit(train,target)\n#gsearch2.grid_scores_, gsearch2.best_params_, gsearch2.best_score_","metadata":{"collapsed":true},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"#param_test3 = {'min_samples_leaf':range(30,71,10)}\n#gsearch3 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, n_estimators=60,max_depth=7,min_samples_split=600,max_features='sqrt', subsample=0.8, random_state=10), \n#param_grid = param_test3, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n#gsearch3.fit(train,target)\n#gsearch3.grid_scores_, gsearch3.best_params_, gsearch3.best_score_","metadata":{"collapsed":true},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"#param_test4 = {'max_features':range(30,46,2)}\n#gsearch4 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, n_estimators=60,max_depth=7, min_samples_split=600, min_samples_leaf=50, subsample=0.8, random_state=10),\n#param_grid = param_test4, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n#gsearch4.fit(train,target)\n#gsearch4.grid_scores_, gsearch4.best_params_, gsearch4.best_score_","metadata":{"collapsed":true},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"#param_test5 = {'subsample':[0.6,0.7,0.75,0.8,0.85,0.9]}\n#gsearch5 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, n_estimators=60,max_depth=7,min_samples_split=600, min_samples_leaf=50, random_state=10,max_features=31),\n#param_grid = param_test5, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n#gsearch5.fit(train,target)\n#gsearch5.grid_scores_, gsearch5.best_params_, gsearch5.best_score_","metadata":{"collapsed":true},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"import xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn import cross_validation, metrics   #Additional scklearn functions\nfrom sklearn.grid_search import GridSearchCV   #Perforing grid search","metadata":{"collapsed":true},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"#param_test1 = {\n# 'max_depth':range(3,10,2),\n# 'min_child_weight':range(1,6,2)\n#}\n#gsearch1 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=5,\n# min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n# objective= 'binary:logistic', nthread=50, scale_pos_weight=1, seed=27), \n# param_grid = param_test1, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n#gsearch1.fit(train,target)\n#gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_","metadata":{"collapsed":true},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"#param_test2 = {\n# 'max_depth':[5,6,7],\n# 'min_child_weight':[1,3,5]\n#}\n#gsearch2 = GridSearchCV(estimator = XGBClassifier( learning_rate=0.1, n_estimators=140, max_depth=5,\n# min_child_weight=2, gamma=0, subsample=0.8, colsample_bytree=0.8,\n# objective= 'binary:logistic', nthread=70, scale_pos_weight=1,seed=27), \n# param_grid = param_test2, scoring='roc_auc',n_jobs=1,iid=False, cv=5)\n#gsearch2.fit(train,target)\n#gsearch2.grid_scores_, gsearch2.best_params_, gsearch2.best_score_","metadata":{"collapsed":true},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"#param_test2b = {\n# 'min_child_weight':[6,8,10,12]\n#}\n#gsearch2b = GridSearchCV(estimator = XGBClassifier( learning_rate=0.1, n_estimators=140, max_depth=5,\n# min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n# objective= 'binary:logistic', nthread=20, scale_pos_weight=1,seed=27), \n# param_grid = param_test2b, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n#gsearch2b.fit(train,target)","metadata":{"collapsed":true},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"#gsearch2b.grid_scores_, gsearch2b.best_params_, gsearch2b.best_score_","metadata":{"collapsed":true},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"#param_test3 = {\n# 'gamma':[i/10.0 for i in range(0,5)]\n#}\n#gsearch3 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=5,\n# min_child_weight=6, gamma=0, subsample=0.8, colsample_bytree=0.8,\n# objective= 'binary:logistic', nthread=20, scale_pos_weight=1,seed=27), \n# param_grid = param_test3, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n#gsearch3.fit(train,target)\n#gsearch3.grid_scores_, gsearch3.best_params_, gsearch3.best_score_","metadata":{"collapsed":true},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"#param_test4 = {\n# 'subsample':[i/10.0 for i in range(6,10)],\n# 'colsample_bytree':[i/10.0 for i in range(6,10)]\n#}\n#gsearch4 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=177, max_depth=5,\n# min_child_weight=6, gamma=0.1, subsample=0.8, colsample_bytree=0.8,\n# objective= 'binary:logistic', nthread=20, scale_pos_weight=1,seed=27), \n# param_grid = param_test4, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n#gsearch4.fit(train,target)\n#gsearch4.grid_scores_, gsearch4.best_params_, gsearch4.best_score_","metadata":{"collapsed":true},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"#param_test5 = {\n# 'subsample':[i/100.0 for i in range(75,90,5)],\n# 'colsample_bytree':[i/100.0 for i in range(75,90,5)]\n#}\n#gsearch5 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=177, max_depth=5,\n# min_child_weight=6, gamma=0.1, subsample=0.8, colsample_bytree=0.8,\n# objective= 'binary:logistic', nthread=20, scale_pos_weight=1,seed=27), \n# param_grid = param_test5, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n#gsearch5.fit(train,target)","metadata":{"collapsed":true},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"#gsearch5.grid_scores_, gsearch5.best_params_, gsearch5.best_score_","metadata":{"collapsed":true},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"#param_test7 = {\n# 'reg_alpha':[0, 0.001, 0.005, 0.01, 0.05]\n#}\n#gsearch7 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=177, max_depth=5,\n# min_child_weight=6, gamma=0.1, subsample=0.8, colsample_bytree=0.8,\n# objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n# param_grid = param_test7, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n#gsearch7.fit(train,target)\n#gsearch7.grid_scores_, gsearch7.best_params_, gsearch7.best_score_","metadata":{"collapsed":true},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"from matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 12, 4\n\ndef modelfit(alg, dtrain, targer,useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n    \n    if useTrainCV:\n        xgb_param = alg.get_xgb_params()\n        xgtrain = xgb.DMatrix(dtrain.values, label=target.values)\n        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n            metrics='auc', early_stopping_rounds=early_stopping_rounds)\n        alg.set_params(n_estimators=cvresult.shape[0])\n    \n    #Fit the algorithm on the data\n    alg.fit(dtrain, target,eval_metric='auc')\n        \n    #Predict training set:\n    dtrain_predictions = alg.predict(dtrain)\n    dtrain_predprob = alg.predict_proba(dtrain)[:,1]\n        \n    #Print model report:\n    print \"\\nModel Report\"\n    print \"Accuracy : %.4g\" % metrics.accuracy_score(target.values, dtrain_predictions)\n    print \"AUC Score (Train): %f\" % metrics.roc_auc_score(target, dtrain_predprob)\n                    \n    feat_imp = pd.Series(alg.feature_importances_, dtrain.columns).sort_values(ascending=False)\n    feat_imp.plot(kind='bar', title='Feature Importances')\n    plt.ylabel('Feature Importance Score')\n\nxgb4 = XGBClassifier(\n learning_rate =0.01,\n n_estimators=5000,\n max_depth=5,\n min_child_weight=6,\n gamma=0,\n subsample=0.8,\n colsample_bytree=0.8,\n reg_alpha=0,\n objective= 'binary:logistic',\n nthread=80,\n scale_pos_weight=1,\n seed=27)\nmodelfit(xgb4, train, target)","metadata":{},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"from sklearn.feature_selection import SelectFromModel\n\nclf = XGBClassifier(\n learning_rate =0.01,\n n_estimators=5000,\n max_depth=5,\n min_child_weight=6,\n gamma=0,\n subsample=0.8,\n colsample_bytree=0.8,\n reg_alpha=0,\n objective= 'binary:logistic',\n nthread=80,\n scale_pos_weight=1,\n seed=27)\nclf = clf.fit(train, target)\nclf.feature_importances_  \n\nmodel = SelectFromModel(clf, prefit=True)\nX_new = model.transform(train)\nX_new.shape \n\n#This is choosing 23 features out of 50 features","metadata":{},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"from matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 12, 4\n\n\ndef modelfit(alg, dtrain, target, performCV=True, printFeatureImportance=True, cv_folds=5):\n    #Fit the algorithm on the data\n    alg.fit(dtrain, target)\n        \n    #Predict training set:\n    dtrain_predictions = alg.predict(dtrain)\n    dtrain_predprob = alg.predict_proba(dtrain)[:,1]\n    \n    #Perform cross-validation:\n    if performCV:\n        cv_score = cross_validation.cross_val_score(alg, dtrain, target, cv=cv_folds, scoring='roc_auc')\n    \n    #Print model report:\n    print \"\\nModel Report\"\n    print \"Accuracy : %.4g\" % metrics.accuracy_score(target.values, dtrain_predictions)\n    print \"AUC Score (Train): %f\" % metrics.roc_auc_score(target, dtrain_predprob)\n    \n    if performCV:\n        print \"CV Score : Mean - %.7g | Std - %.7g | Min - %.7g | Max - %.7g\" % (np.mean(cv_score),np.std(cv_score),np.min(cv_score),np.max(cv_score))\n        \n    #Print Feature Importance:\n    if printFeatureImportance:\n        feat_imp = pd.Series(alg.feature_importances_, dtrain.columns).sort_values(ascending=False)\n        feat_imp.plot(kind='bar', title='Feature Importances')\n        plt.ylabel('Feature Importance Score')\n        \n\ngbm_tuned_2 = GradientBoostingClassifier(learning_rate=0.01, n_estimators=600,max_depth=7, min_samples_split=600,min_samples_leaf=50, subsample=0.80, random_state=10, max_features=31)\nmodelfit(gbm_tuned_2, train, target)","metadata":{},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"from sklearn.feature_selection import SelectFromModel\n\nclf = GradientBoostingClassifier(learning_rate=0.01, n_estimators=600,max_depth=7, min_samples_split=600,min_samples_leaf=50, subsample=0.80, random_state=10, max_features=31)\nclf = clf.fit(train, target)\nclf.feature_importances_  \n\nmodel = SelectFromModel(clf, prefit=True)\nX_new = model.transform(train)\nX_new.shape ","metadata":{"collapsed":true},"cell_type":"code"},{"outputs":[],"execution_count":null,"source":"","metadata":{"collapsed":true},"cell_type":"code"}],"nbformat_minor":1,"nbformat":4,"metadata":{"kernelspec":{"name":"python2","display_name":"Python 2","language":"python"},"language_info":{"name":"python","file_extension":".py","codemirror_mode":{"name":"ipython","version":2},"pygments_lexer":"ipython2","mimetype":"text/x-python","nbconvert_exporter":"python","version":"2.7.12"}}}