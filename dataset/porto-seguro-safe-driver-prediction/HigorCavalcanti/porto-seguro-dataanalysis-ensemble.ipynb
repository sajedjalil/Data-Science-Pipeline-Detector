{"cells":[{"metadata":{"_cell_guid":"5aa21b7b-6288-4970-b95f-af76030794b7","_uuid":"69020ca293aa56504d47c26ecdd86796da9ceb06"},"source":"# Summary\n1. Introduction\n    1. Importing Modules\n    1. Defining Score Metric\n1. Feature Selection\n    1. Data Analysis\n    1. Data Manipulation\n1. Classification\n1. Submission\n1. Results\n1. The Team","cell_type":"markdown"},{"metadata":{"_cell_guid":"07608deb-e5c9-4652-85cf-688b31423371","_uuid":"02327fc3581ab98149854769b68b465e75e04e97"},"source":"# 1. Introduction\n\nThis is a complete notebook on how to develop a working solution for [Porto Seguro](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction) competition. It is based on self highlights, as well as highlights from other kernels. The goal is to provide an beginner friendly Kernel with exploratory and visual considerations about why the good kernels do what they do on the data.","cell_type":"markdown"},{"metadata":{"_cell_guid":"7652f966-8950-4ffe-90b5-fdb2be260fd6","_uuid":"5f18b846d7d2504b493f9eac87e440dc12836094"},"source":"## 1.A. Importing Modules","cell_type":"markdown"},{"metadata":{"collapsed":true,"_cell_guid":"a8665e9a-1488-4f15-ba0c-ca55a30cd13a","_uuid":"0a6d65608e54b23724015d8e775a6d5855ebf2c2"},"execution_count":null,"outputs":[],"source":"# data mining\nimport numpy as np\nimport pandas as pd\n\n# data visualization\nimport seaborn as sns\nimport missingno as msno\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# machine learning\nfrom lightgbm import LGBMClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_val_score\n\nfrom subprocess import check_output \nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))","cell_type":"code"},{"metadata":{"_cell_guid":"5b8894dd-fff6-4e48-a01e-67d5c2749cf5","_uuid":"6b4743256bd3825d466930fb22652193268e90be"},"source":"## 1.B. Defining Score Metric\nThis competition will be using the Normalized Gini Coeficient to calculate our predictions score. A better understanding of this metric can be obtained [here](https://www.kaggle.com/batzner/gini-coefficient-an-intuitive-explanation).","cell_type":"markdown"},{"metadata":{"collapsed":true,"_cell_guid":"40bcc83b-f261-4bce-ae4f-7823f62b824e","_uuid":"27f07943df1b4d8230f928604fc42a99da88b000"},"execution_count":null,"outputs":[],"source":"def gini(actual, pred):\n    assert (len(actual) == len(pred))\n    all = np.asarray(np.c_[actual, pred, np.arange(len(actual))], dtype=np.float)\n    all = all[np.lexsort((all[:, 2], -1 * all[:, 1]))]\n    totalLosses = all[:, 0].sum()\n    giniSum = all[:, 0].cumsum().sum() / totalLosses\n\n    giniSum -= (len(actual) + 1) / 2.\n    return giniSum / len(actual)\n\n\ndef gini_normalized(actual, pred):\n    return gini(actual, pred) / gini(actual, actual)\n\ndef gini_xgb(preds, dtrain):\n    labels = dtrain.get_label()\n    gini_score = gini_normalized(labels, preds)\n    return 'gini', gini_score","cell_type":"code"},{"metadata":{"_cell_guid":"15a982b1-c87c-465f-968b-573ca0288f9e","_uuid":"4db81867a9ada907e123fb9b3d6fb495993af3fb"},"source":"# 2. Feature Selection","cell_type":"markdown"},{"metadata":{"_cell_guid":"361bc350-4045-422a-99ac-6c032ef52235","_uuid":"f2c5899b0edc9cfc5bc071ecf34bc654812e006e"},"source":"## 2.A. Data Analysis\nA summary of the data analysis discoveries shows us that the dataset is highly unbalanced, there are many missing value and some columns have no correlation with target and should be dropped.","cell_type":"markdown"},{"metadata":{"_cell_guid":"db38b047-6d01-4087-a97f-9fadd28906cb","_uuid":"8c29036523f31daf3f58f49bdef37425d4586b73"},"source":"### Importing Data","cell_type":"markdown"},{"metadata":{"collapsed":true,"_cell_guid":"4e1d6d5b-0310-495b-9b12-19917801b1be","_uuid":"04873de5ab3c260a3a2a351273f2411bfed05a35"},"execution_count":null,"outputs":[],"source":"df = pd.read_csv('../input/train.csv', na_values='-1')\ntest_df = pd.read_csv('../input/test.csv', na_values='-1')\ndf.head()","cell_type":"code"},{"metadata":{"_cell_guid":"5d5f1554-72e2-407e-9c69-eec72e1d91f4","_uuid":"08a24b21bdf30c4ed370be866ee458480d036d0d"},"source":"### Class\nThe data is splited in two classes. Unluckily, the huge majority of the samples belongs to the same class. Actually, only 3.64% belong to the other, characterizing the dataset as highly unbalanced. The bias is expected to predict 0 all times. Techniques to deal with unbalanced training set should be used.\n","cell_type":"markdown"},{"metadata":{"collapsed":true,"_cell_guid":"4993549c-349b-44d2-80ff-9f385404b29b","_uuid":"9ba5d5d94104f981974cf665249a4b338a8f77b0","scrolled":false},"execution_count":null,"outputs":[],"source":"entries = df.shape[0]\nplot = sns.countplot(x='target', data=df)\nfor p in plot.patches:\n    plot.annotate('{:.2f}%'.format(100*p.get_height()/entries), (p.get_x()+ 0.3, p.get_height()+10000))","cell_type":"code"},{"metadata":{"_cell_guid":"a35af9f3-9a7e-4000-b5cd-d728d839fb29","_uuid":"9b8b53a44ba03a8b9b8a7017ddc4e6f085f16f6b"},"source":"### Null values\nWe then search the dataset for missing entries in the samples. A quick analysis shows that many columns have multiple missing values, requiring our attention. We should proceed then either filling this values with reasonably values, or deleting the feature from our dataset. Feature analysis is then required.","cell_type":"markdown"},{"metadata":{"collapsed":true,"_cell_guid":"a5d0174c-3e61-4661-b4c2-7a2134b12aac","_uuid":"d6f98f835570513673c7cf0b8509de7f95759ec5"},"execution_count":null,"outputs":[],"source":"msno.matrix(df=df.iloc[:, :], figsize=(20, 14), color=(0.8, 0.5, 0.2))   ","cell_type":"code"},{"metadata":{"collapsed":true,"_cell_guid":"3a95fbfe-d20e-4c2e-812b-1f78cebd2cba","_uuid":"c6346cb683520b680441e9a597e44dcf229c7842"},"execution_count":null,"outputs":[],"source":"print('Column \\t\\t Number of Null')\nfor column in df.columns:\n    print('{}:\\t {} ({:.2f}%)'.format(column,len(df[column][np.isnan(df[column])]), 100*len(df[column][np.isnan(df[column])])/entries))","cell_type":"code"},{"metadata":{"_cell_guid":"3108fc0e-48bc-475c-b63c-3212c9a107bc","_uuid":"fb9df497cc092dfcd04c2ef9740f4e2ff3dd4fc7"},"source":"### Correlation Matrix\n\nThe correlation between pairs of features shows that there is no correlation at all between *ps_calc_etc* features and the *target* or any other features. So dropping them would prevent the curse of dimensionality.","cell_type":"markdown"},{"metadata":{"collapsed":true,"_cell_guid":"05967c4b-7b84-4348-bbb8-7644e73167b2","_uuid":"e2538bf82816a196eafe587f21f0a3edc72ac297"},"execution_count":null,"outputs":[],"source":"corr = df.corr()\nf, ax = plt.subplots(figsize=(11, 9))\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(corr, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n\nplt.show()","cell_type":"code"},{"metadata":{"_cell_guid":"14c25fc5-21e4-42f0-82e7-e291b6975a5a","_uuid":"7c4561823207f5a80fcdc55ddd93f3cde1fedcd7"},"source":"## 2.B. Data Manipulation\nBased on the data analysis step, we are going to remove the *ps_calc_etc* features, and convert categorial features to dummy columns.","cell_type":"markdown"},{"metadata":{"_cell_guid":"9e1a4340-73bb-4733-a223-0fa4ed55e062","_uuid":"bba8e27588e7f20df3762fffd0931d95be1aa9af"},"source":"### Removing Features\nSince *ps_calc_etc* features aren't related to *target*, removing them can prevent random junk to affect our model, and improving training and classifications times.","cell_type":"markdown"},{"metadata":{"collapsed":true,"_cell_guid":"18a9a1b5-fc5a-49bb-bec3-7b9e4b87d0c4","_uuid":"666675e522ffda0b03aab6e3287ba633a9475a72","_kg_hide-output":true},"execution_count":null,"outputs":[],"source":"unwanted = df.columns[df.columns.str.startswith('ps_calc_')]\ndf = df.drop(unwanted, axis=1)\ntest_df = test_df.drop(unwanted, axis=1)\ndf.head()","cell_type":"code"},{"metadata":{"_cell_guid":"d207fb45-5093-432b-9300-5d0657fbebb5","_uuid":"a99032c10be6c3b5f8e681856b108562dc37f827"},"source":"### Changing Categorical Features to Dummy Values\nWhy do we need to convert categorical features to dummy values? Let's call your variable X and assume it takes on values \"1\", \"2\", \"3\", \"4\" or \"5\". If you feed X into the model as numbers, the model will estimate only a single parameter, which is the effect on the target variable of increasing X by 1 unit. So if you hold everything else constant and increase X from 1 to 2, that affects the target variable the same way as increasing it from 2 to 3 or from 4 to 5.\n\nIf instead you model X as categorical, you will estimate 4 parameters: the effect of increasing X from 1 to 2, from 2 to 3, and so on. These values could all be different. And that's what we really want to extract from categorical variables from the very beggining: the weight of each category.","cell_type":"markdown"},{"metadata":{"collapsed":true,"_cell_guid":"9c109eba-e7a8-4a15-8516-32e6fa9952f3","_uuid":"2ae6fe4c8d83993500c203252835b1af48a714e5"},"execution_count":null,"outputs":[],"source":"cat_columns = [a for a in df.columns if a.endswith('cat')]\n\nfor col in cat_columns:\n\tdummy = pd.get_dummies(pd.Series(df[col]))\n\tdf = pd.concat([df,dummy],axis=1)\n\tdf = df.drop([col],axis=1)\n    \nfor col in cat_columns:\n\tdummy = pd.get_dummies(pd.Series(test_df[col]))\n\ttest_df = pd.concat([test_df,dummy],axis=1)\n\ttest_df = test_df.drop([col],axis=1)\n    \ndf.head()\n","cell_type":"code"},{"metadata":{"_cell_guid":"7884c731-5bf2-40fd-9580-5ac8ec12b5ac","_uuid":"43a7aad47bc22e668c921d0e03486c1f03fd5f59"},"source":"# 3. Classification\nSince we have a unbalanced training set, we are going to use Classifier Ensemble methods to predict a better output, united with a StratifieldKFold strategy to train each base model with multiple balanced training sets.","cell_type":"markdown"},{"metadata":{"_cell_guid":"0ad34feb-e7e8-4793-8007-ddaa455644ca","_uuid":"8af9cccd2d6b5c436c6dca0bb3b47278b4d9f06b"},"source":"### Ensemble Class Creation","cell_type":"markdown"},{"metadata":{"collapsed":true,"_cell_guid":"2c568091-13ce-45f2-a8c7-0f477ee4685c","_uuid":"906948d6e1012e64eacb69f4782cc9af203540ba"},"execution_count":null,"outputs":[],"source":"class Ensemble(object):\n    def __init__(self, kfold, stacker, models):\n        self.kfold = kfold\n        self.stacker = stacker\n        self.models = models\n\n    def fit_predict(self, x, y, test):\n        x = np.array(x)\n        y = np.array(y)\n        t = np.array(test)\n        \n        train = np.zeros((x.shape[0], len(self.models)))\n        test = np.zeros((t.shape[0], len(self.models)))\n        \n        skf = list(StratifiedKFold(n_splits=self.kfold, shuffle=True, random_state=2016).split(x, y))\n        \n        for i, model in enumerate(self.models):\n\n            test_i = np.zeros((t.shape[0], self.kfold))\n\n            for j, (train_idx, test_idx) in enumerate(skf):\n                x_train = x[train_idx]\n                y_train = y[train_idx]\n                x_valid = x[test_idx]\n                y_valid = y[test_idx]\n\n                print (\"Fit %s fold %d\" % (str(model).split('(')[0], j+1))\n                \n                model.fit(x_train, y_train)\n                y_train_pred = model.predict_proba(x_train)[:,1]\n                y_pred = model.predict_proba(x_valid)[:,1]   \n                \n                print(\"[Train] Gini score: %.6lf\" % gini_normalized(y_train, y_train_pred))\n                print(\"[Test] Gini score: %.6lf\\n\" % gini_normalized(y_valid, y_pred))\n\n                train[test_idx, i] = y_pred\n                test_i[:, j] = model.predict_proba(t)[:,1]\n            test[:, i] = test_i.mean(axis=1)\n\n        self.stacker.fit(train, y)\n        valid = self.stacker.predict_proba(train)[:,1]\n        res = self.stacker.predict_proba(test)[:,1]\n        print(\"Staker Gini Score: %.6lf\" % gini_normalized(valid, y))\n        return res","cell_type":"code"},{"metadata":{"_cell_guid":"f0f906a8-1bdc-452f-b076-7412dc60170a","_uuid":"16804b0c66d4d2d16e61e21f370461678c1273d6"},"source":"### Preparing Data for Training/Predict","cell_type":"markdown"},{"metadata":{"collapsed":true,"_cell_guid":"972568ac-b06c-4372-9e24-cf920d48f955","_uuid":"aaef7b52679c52837e5b829ffe4162bce05002e0"},"execution_count":null,"outputs":[],"source":"x = df.drop(['id', 'target'], axis=1)\ny = df['target'].values\ntest_id = test_df['id']\ntest_df = test_df.drop('id', axis=1)","cell_type":"code"},{"metadata":{"_cell_guid":"4ea1f9ef-29d8-4a25-b7ff-659a564bd650","_uuid":"f778025e8896192dc866005177fdfae1e2154397"},"source":"### Defining Base Models for Ensemble\nThose are some dummy parameters for creating models. Real parameters are in grey. In order to run with the real parameters, you need to run the code locally on your machine, because Kaggle Kernels timeout after 1 hour training.","cell_type":"markdown"},{"metadata":{"collapsed":true,"_cell_guid":"02a0ea8d-ca03-48b4-b5cd-bc2feb7de554","_uuid":"5a98be4822e54da61de9c2724d3f823d667d31be"},"execution_count":null,"outputs":[],"source":"lgb_params = {\n    'learning_rate': 0.02,\n    'n_estimators': 1, # use 650 for real model\n    'max_bin': 10,\n    'subsample': 0.8,\n    'subsample_freq': 10,\n    'colsample_bytree': 0.8,\n    'min_child_samples': 500,\n    'random_state': 99\n}\n\nlgb_model = LGBMClassifier(**lgb_params)\n\nlgb2_params = {\n    'learning_rate': 0.02,\n    'n_estimators': 1, #use 1090 for real model\n    'colsample_bytree': 0.3,\n    'subsample': 0.7,\n    'subsample_freq': 2,\n    'num_leaves': 16,\n    'random_state': 99\n}\n\nlgb_model2 = LGBMClassifier(**lgb2_params)\n\nlgb3_params = {\n    'n_estimators': 1, #use 1100 for real model\n    'max_depth': 4,\n    'learning_rate': 0.02,\n    'random_state': 99\n}\n\nlgb_model3 = LGBMClassifier(**lgb3_params)\n\nlog_model = LogisticRegression()","cell_type":"code"},{"metadata":{"_cell_guid":"64857a61-048c-44c7-94ca-38ff9d887525","_uuid":"2452b47821fe80e90b92f6638a0b3e0f37752453"},"source":"### Fit/Prediction\nGini score is calculated for each model of the Ensemble, as well as for the final combined classifier.","cell_type":"markdown"},{"metadata":{"collapsed":true,"_cell_guid":"b227d20f-1f59-4819-8317-73ca37df7219","_uuid":"1dfac60f16cb0460ec58fbf731074cc9eab63c27"},"execution_count":null,"outputs":[],"source":"stack = Ensemble(kfold=3,\n        stacker = log_model,\n        models = (lgb_model, lgb_model2, lgb_model3))        \n        \ny_pred = stack.fit_predict(x, y, test_df)","cell_type":"code"},{"metadata":{"_cell_guid":"4ec9dfc7-9e9e-4490-baf5-2d2531458052","_uuid":"63af425b4d888af38ce0f8954d72837efa05ce0a"},"source":"# 4. Submission\nThis output is just a dummy. In orther to get the real output, you must use the right parameters for the models and run the code locally on your machine.","cell_type":"markdown"},{"metadata":{"collapsed":true,"_cell_guid":"392c7fe1-2f74-4502-9a7c-2a84b3fe0739","_uuid":"f070d17bb04b8e29bb6245bddce99fc9f939e081"},"execution_count":null,"outputs":[],"source":"sub = pd.DataFrame()\nsub['id'] = test_id\nsub['target'] = y_pred\nsub.to_csv('output.csv', index=False)","cell_type":"code"},{"metadata":{},"source":"# 5. Results\n\nFor this approach, we managed to score 0.28960 at the leaderboards. The top submission scored 0.29698. That's pretty close, but, still, we are at the 1260/5169 position on the leaderboard. That's because in this competition the tiniest improvement would make you jump hundreds of positions up. Those positions were last updated at December 5th, 2017.","cell_type":"markdown"},{"metadata":{},"source":"# The Team\n\nOur team's name on the leaderboard is \"Discípulos de Cleber\". We are three students of the Federal University of Pernambuco:\n* Higor Cavalcanti\n* Lavínia Francesca\n* João Vasconcelos","cell_type":"markdown"}],"metadata":{"language_info":{"mimetype":"text/x-python","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.3"},"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"}},"nbformat_minor":1,"nbformat":4}