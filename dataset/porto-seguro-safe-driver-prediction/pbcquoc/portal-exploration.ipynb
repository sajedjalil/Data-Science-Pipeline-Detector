{"cells":[{"source":"In this notebook, Let us try and explore the data given for Portal Seguro competition. Before we dive deep into the data, Let us know a little more about the competition.  \n**Portal Seguro**: The company offers car insurance, residential, health, life, business, consortium also offers auto and homeowners, pension, savings bonds and other financial services.  \n**Objective**: The task of predicting the probability that a driver will initiate an insurance claim in the next year.\n","cell_type":"markdown","metadata":{"_cell_guid":"f1f4c9fc-c8f7-4dcd-add4-07731345d049","_uuid":"05b0d611315ad47f43fa97d6672624530c0a71d4"}},{"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.decomposition import PCA, KernelPCA\nimport xgboost as xgb\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.manifold import TSNE\n\ncolor = sns.color_palette()\n%matplotlib inline\npd.options.mode.chained_assignment = None\npd.options.display.max_columns = 999","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"8eca0ffe-7664-4d1f-a645-6bd4ce78ef4a","collapsed":true,"_uuid":"17ec78ad60c520ba873c4e007c3a0e0788c7b814"}},{"source":"train_df = pd.read_csv(\"../input/train.csv\")","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"a2dff769-951b-477d-aec7-a97c34a84ed2","collapsed":true,"_uuid":"0180fee4319022c0ed7a9a147344b4ff14e6c6e7"}},{"source":"print(\"train dataset shape: \", train_df.shape)\ntrain_df.head()","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"9126b57a-d65b-4ef3-966d-65d78096951b","_uuid":"d5a8ad256f06b5dc3f84050ff91f48727b0a1cf1"}},{"source":"train_df['ps_car_15'].min()","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"28783725-6b48-44f6-9aec-c5187de7cb00","_uuid":"e3db592f2756a856e8f215b22da1686485784080"}},{"source":"## Data Quality Checks\nFirst let us check whether there are any missing values in the train dataset","cell_type":"markdown","metadata":{"_cell_guid":"1e3a3a68-2670-4243-8b70-5f2431a9ef53","_uuid":"757a04f3ecb3cec3610a6ea891c70a999bc1ca01"}},{"source":"train_df.isnull().values.any()","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"beb982cc-69c3-481e-93e6-e02c4ad9776b","collapsed":true,"_uuid":"23925f5cc988e97e474d9145267645b68b2c8d36"}},{"source":"The null values check return **false** but it doesn't really mean that ... as described *\"Values of -1 indicate that the feature was missing from the observation\"*\n\nHere let us count how many -1 in each column","cell_type":"markdown","metadata":{"_cell_guid":"dfba037d-ff61-472e-abf2-8eaef6f6efc3","_uuid":"24b0df174f980bef3690082258832b5e9d036491"}},{"source":"missing_df = np.sum(train_df==-1, axis=0)\nmissing_df.sort_values(ascending=False, inplace=True)\n\nplt.figure(figsize=(10, 20))\nsns.barplot(x=missing_df.values, y=missing_df.index)\nplt.title(\"Number of missing values in each column\")\nplt.xlabel(\"Count of missing values\")\nplt.show()","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"9cca6d0f-a9b8-48d8-8f7a-0716948a4e7a","collapsed":true,"_uuid":"a76b311b1592e31198bad5793ee966c3b7e77d80"}},{"source":"We can observe that there are 7 columns out of 59 total columns that actually contained null values. ","cell_type":"markdown","metadata":{"_cell_guid":"bb62e44e-6176-4463-aa9f-49ca90b8808a","_uuid":"279bc675508fe37e79289db7503dd75bc73973f8"}},{"source":"Let us check target variable distribution","cell_type":"markdown","metadata":{"_cell_guid":"ee8b6664-8589-42bd-8319-7623b6b39003","_uuid":"a9f0a96027c784597c162810d6fc7a01714accb5"}},{"source":"sns.countplot(x=\"target\", data=train_df)\nplt.show()","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"bd97b9fa-7d5a-400f-b4e5-f2b30eb496dc","collapsed":true,"_uuid":"372638728b0f7c9d1a963ed64cc54d8d50a3576f"}},{"source":"It is clear that target variable is imbalance so very small amount of policy hoder was filed","cell_type":"markdown","metadata":{"_cell_guid":"22fd5075-193e-469e-8937-26b6f1e12fc3","_uuid":"c4cbec7315caefe8104987dbaeac483f409f261e"}},{"source":"### Bin variable distribution","cell_type":"markdown","metadata":{"_cell_guid":"4c98d424-f8d0-44d1-8c0b-bd6653d5ee5f","_uuid":"0f70d481e8e40d75af21b3ad7ff02b1d43b27bfd"}},{"source":"bin_vars = []\nfor col in train_df.columns:\n    if col.endswith(\"bin\"):\n        bin_var = train_df.groupby(col).size()  \n        bin_vars.append(bin_var)\n        \nbin_df = pd.concat(bin_vars, axis=0, keys=[s.index.name for s in bin_vars]).unstack()\n\n_ = bin_df.plot(kind='bar', stacked=True, grid=False, figsize=(10, 8))\n","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"8b5405f6-68be-4855-b4a0-817bf59b7f45","collapsed":true,"_uuid":"ef177dc1a323b4a67e94f14e7ba7c0668c517ff5"}},{"source":"There are 4 features: **ps_ind_10_bin, ps_ind_11_bin, ps_ind_12_bin, ps_ind_13_bin** which are almost zero so we should consider remove it from training dataset","cell_type":"markdown","metadata":{"_cell_guid":"ca6072fb-575e-4954-8ef5-d962d6c4dc26","_uuid":"764a591bd395985412a20051838e29e0de9b7136"}},{"source":"### Category variable distribution","cell_type":"markdown","metadata":{"_cell_guid":"8debc61d-8106-44d9-b717-661fdefea5e8","_uuid":"5b335d1ae95b433473eee54c4d4f7afa49866945"}},{"source":"bin_vars = []\nfor col in train_df.columns:\n    if col.endswith(\"cat\"):\n        bin_var = train_df.groupby(col).size()  \n        bin_vars.append(bin_var)\n        \nbin_df = pd.concat(bin_vars, axis=0, keys=[s.index.name for s in bin_vars]).unstack()\n\n_ = bin_df.plot(kind='bar', stacked=True, grid=False, figsize=(10, 8), legend=False)","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"76262d98-40f8-487d-988e-a0958b9868bf","collapsed":true,"_uuid":"594e717e741c05db33c242869e90a44680ca1894"}},{"source":"**ps_car_10_cat** is completely dominated by 1 while **ps_car_11_cat** have too much catetogies. Both variables should be removed ","cell_type":"markdown","metadata":{"_cell_guid":"c24a2937-5198-4820-8eab-56c8ee92bd1d","_uuid":"2aa88fc1c23ca6af13cd6cdbc3b8cd6f0bd57d23"}},{"source":"Let us check how correlation between these variables","cell_type":"markdown","metadata":{"_cell_guid":"07b7c966-54aa-46ab-b3ce-08dfbf038622","_uuid":"8abdd26050f92f2dec911af6a5e0e438a08815d8"}},{"source":"corr = train_df.corr()\n\nplt.figure(figsize=(20,15))\nsns.heatmap(corr)\nplt.show()","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"17a8b8cb-fc10-47d5-9bab-a5969045e352","collapsed":true,"_uuid":"b4dbc1ccf9173f2961e769fe24e5928e9ea51c78"}},{"source":"We can observe that manny colums don't have  linear correlation with others, that mean each of these columns contains some independent information. So if we use PCA this columns will be remained  \nThere are **ps_calc_01, ps_calc_02,..., ps_calc_03, ps_calc_15_bin, ps_calc_16_bin, ..., ps_calc_20_bin** which are almost zero linear dependence with **target**","cell_type":"markdown","metadata":{"_cell_guid":"d970d266-d40e-4b62-825b-9fdb34860c80","_uuid":"ff16f550d9e72ff6fc280e06a654b4421f470537"}},{"source":"### Dimension reduction","cell_type":"markdown","metadata":{"_cell_guid":"b4176c5e-f137-4950-a298-3e6e8f6ecf59","collapsed":true,"_uuid":"e26735051889379830f2546db9288fb6c11c8196"}},{"source":"Let us plot how data distrubute on 2d plane use **PCA**","cell_type":"markdown","metadata":{"_cell_guid":"2a4a3af9-9780-4d7b-8077-4e9e28da03db","_uuid":"c6ed5250a93daa9809cb6dd62a0f88938dcfc827"}},{"source":"ignore_columns = ['target', 'id', 'ps_ind_10_bin', 'ps_ind_11_bin', 'ps_ind_12_bin', 'ps_ind_13_bin'] + ['ps_calc_{:02d}'.format(i) for i in range(1, 15)] + ['ps_calc_{:02d}_bin'.format(i) for i in range(15, 21)]\ntrain_columns = [col for col in train_df.columns if col not in ignore_columns]","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"3012783c-ae7f-4e2b-9393-e9ce3dda1a18","collapsed":true,"_uuid":"a3ee8ef807f4cd50b5943cbd11e2ce53cf1be9bb"}},{"source":"\nX = train_df[train_columns].values\ntarget = train_df.target\nprint(\"Training data shape: \", X.shape)\n\npca = PCA(n_components=2)\nreduced_dim = pca.fit_transform(X)\nreduced_dim = reduced_dim[np.random.randint(0, len(reduced_dim), size=10000)]\n                                            \nreduced_df = pd.DataFrame(data=reduced_dim, columns=['x', 'y'])\nreduced_df['target'] = target\n\nplt.figure(figsize=(20, 8))\nsns.jointplot(x='x', y='y', data=reduced_df, size=7, color=\"g\")\nplt.show()","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"cd5e82ff-4119-407a-b0ba-75f91659d59a","collapsed":true,"_uuid":"a5b5648da02cabb545397c89d1d0031bbd409baa"}},{"source":"We can observe that there are many points that have x in range **[-41.99, -31.573]** and heavily overlap","cell_type":"markdown","metadata":{"_cell_guid":"90293dfe-c167-4e4d-bbd3-11894638c59f","_uuid":"bef86c6ca5fe5fdfdb9027f5d79acc38b1d714f0"}},{"source":"plt.figure(figsize=(20,15))\nsns.lmplot(x='x', y='y', hue='target', data=reduced_df, size=7, fit_reg=False)\nplt.show()","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"bfe1180f-41ad-4c4a-89bd-bb01589e7bf9","collapsed":true,"_uuid":"93ec083c18f4e27e0b5fad9eeb5ef6fa7e4f5849"}},{"source":"Define gini score","cell_type":"markdown","metadata":{"_cell_guid":"8066fe6b-6361-4334-a32f-4467cdd44a3a","_uuid":"66daf4e8c85ff166502cb2ecb766e98cbda70459"}},{"source":"def gini(actual, pred, cmpcol = 0, sortcol = 1):\n    assert( len(actual) == len(pred) )\n    all = np.asarray(np.c_[ actual, pred, np.arange(len(actual)) ], dtype=np.float)\n    all = all[ np.lexsort((all[:,2], -1*all[:,1])) ]\n    totalLosses = all[:,0].sum()\n    giniSum = all[:,0].cumsum().sum() / totalLosses\n    \n    giniSum -= (len(actual) + 1) / 2.\n    return giniSum / len(actual)\n \ndef gini_normalized(a, p):\n    return gini(a, p) / gini(a, a)\n\ndef gini_xgb(preds, dtrain):\n    labels = dtrain.get_label()\n    gini_score = gini_normalized(labels, preds)\n    return 'gini', gini_score","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"74a1124d-377d-4395-9d86-0b25182f2750","collapsed":true,"_uuid":"4b4f5badc53afd61db633d5a55f54949ef90daa7"}},{"source":"Build the first model using **XGBoost**","cell_type":"markdown","metadata":{"_cell_guid":"5eff1100-d2d8-4b10-a999-31d82d697752","_uuid":"aad635c3c17f88f5abb736c2ec81884a9c11a997"}},{"source":"\n\nX = train_df[train_columns]\ny = train_df.target\nx_train = X[:-100000]\ny_train = y[:-100000]\nx_val = X[-100000:]\ny_val = y[-100000:]\n\ndtrain = xgb.DMatrix(x_train, y_train)\ndval = xgb.DMatrix(x_val, y_val)\nwatchlist = [(dtrain, 'train'), (dval, 'valid')]\n\nxgb_params = {\n        'eta': 0.037,\n        'max_depth': 5,\n        'subsample': 0.80,\n        'objective': 'binary:logistic',\n        'eval_metric': 'mae',\n        'lambda': 0.8,   \n        'alpha': 0.4, \n        'base_score': 0.0364,\n        'silent': 1\n    }\n\nnum_boost_rounds = 250\nmodel = xgb.train(dict(xgb_params, silent=1), dtrain, evals=watchlist, feval=gini_xgb, num_boost_round=num_boost_rounds, verbose_eval=20)\n","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"07c6b9db-fb9b-4f09-8c28-852d2b6f17da","collapsed":true,"_uuid":"8daf58e24213d12fc91cfdcedae7a95c2b6767eb"}},{"source":"fig, ax = plt.subplots(figsize=(12,18))\nxgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\nplt.show()","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"03bee379-f304-4cbb-8722-5e0cc7bdd235","collapsed":true,"_uuid":"8cc2dfaea80d16203cdf75bc5d7a0d043b8b4845"}},{"source":"Let us find where predictions is wrong","cell_type":"markdown","metadata":{"_cell_guid":"d2957a7c-5ab4-4a3c-8200-f0d7d8263dfd","_uuid":"fc9b706f313e9ec5a91b6adaff0d04a21ed01238"}},{"source":"predict = model.predict(dval)\nidx = np.abs(y_val - predict).nlargest(1400).index.values\ny_val[idx].value_counts()\n#predict1 = predict > 0.1\n#confusion_matrix(y_val, predict1)","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"be49caf2-61fa-4354-b1f0-f3c6f99b8f9a","collapsed":true,"_uuid":"18750004a6953522c50c5714528963a081e4006e"}},{"source":"All samples with label 1 has very large residual error. It imply that we should focus more on it","cell_type":"markdown","metadata":{"_cell_guid":"79eb5ccb-632c-4f30-9bd5-5986bd77664d","_uuid":"26c5f8635e7aa48a4d95bc40634c54db95662996"}},{"source":"try to balance training set","cell_type":"markdown","metadata":{"_cell_guid":"b27c10bc-74e3-47b2-a0ec-58775569ec69","_uuid":"3d40b417c478f4f6daf0560503baf6ffacb3d713"}},{"source":"#cat_columns = [col for col in train_df.columns if col.endswith('cat') and (col!='ps_car_11_cat')]\n#train_df = pd.get_dummies(train_df, columns=cat_columns, prefix=cat_columns)\n\nignore_columns = ['target', 'id', 'ps_ind_10_bin', 'ps_ind_11_bin', 'ps_ind_12_bin', 'ps_ind_13_bin'] + ['ps_calc_{:02d}'.format(i) for i in range(1, 15)] + ['ps_calc_{:02d}_bin'.format(i) for i in range(15, 21)]\ntrain_columns = [col for col in train_df.columns if col not in ignore_columns]\n#X = train_df[train_columns]\n#y = train_df.target\n\n#x_train = X[:-100000]\n#y_train = y[:-100000]\npositive = train_df[train_df.target==1].head(20000)\nnegative = train_df[train_df.target==0].head(50000)\ntrain_df = pd.concat([positive, negative], axis=0)\n# Performing one hot encoding\n\n\ntrain_df = train_df.sample(frac=1.0)\nX = train_df[train_columns]\ny = train_df.target\n\nprint(positive.shape)\n\nx_train = X[:-5000]\ny_train = y[:-5000]\nx_val = X[-5000:]\ny_val = y[-5000:]\n\ndtrain = xgb.DMatrix(x_train, y_train)\ndval = xgb.DMatrix(x_val, y_val)\nwatchlist = [(dtrain, 'train'), (dval, 'valid')]\n\nxgb_params = {\n        'eta': 0.037,\n        'max_depth': 5,\n        'subsample': 0.80,\n        'objective': 'reg:logistic',\n        'eval_metric': 'auc',\n        'lambda': 0.8,   \n        'alpha': 0.4, \n        'base_score': 0.01,\n        'silent': 1\n    }\n\nnum_boost_rounds = 250\nmodel = xgb.train(dict(xgb_params, silent=1), dtrain, evals=watchlist, feval=gini_xgb, num_boost_round=num_boost_rounds, verbose_eval=20)\n","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"ba98ef17-feff-4384-9286-aff63704173b","collapsed":true,"_uuid":"8ba48b4085b19d24bf8370c1c106dda0353e904a"}},{"source":"predict = model.predict(dval)\nidx = np.abs(y_val - predict).nlargest(1400).index.values\ny_val[idx].value_counts()\npredict1 = predict > 0.\nconfusion_matrix(y_val, predict1)","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"70eb75fb-0142-4fb0-87a7-72ad99619b1d","collapsed":true,"_uuid":"331de6bd6533ca024d161f0bd25d04b4dea3b1b0"}},{"source":"we can observe even if we try to balnace our dataset but all wrong preditions still the same as before, it hint that features are not good enough to seperate between two labels. ","cell_type":"markdown","metadata":{"_cell_guid":"fa57833b-fd27-4c1d-98e1-e0bcf7bd4d70","_uuid":"3b68ba595ba39e55e2d48fd11e42aee558813f50"}},{"source":"ignore_columns = ['target', 'id', 'ps_ind_10_bin', 'ps_ind_11_bin', 'ps_ind_12_bin', 'ps_ind_13_bin'] + ['ps_calc_{:02d}'.format(i) for i in range(1, 15)] + ['ps_calc_{:02d}_bin'.format(i) for i in range(15, 21)] #+ [col for col in train_df.columns if col.endswith('cat')]\ncat_columns = [col for col in train_df.columns if col.endswith('cat')]\ntrain_df = pd.get_dummies(train_df, columns=cat_columns, prefix=cat_columns)\n\ntrain_columns = [col for col in train_df.columns if col not in ignore_columns]\n\n#for col in train_df.columns:\n#    if col.endswith('cat'):\n#        count = train_df[col].value_counts()\n#        train_df[col] = train_df.replace({col:count})\n\n\n#log_columns = ['ps_car_12','ps_car_13','ps_car_14','ps_car_15','ps_calc_01','ps_calc_02','ps_calc_03']\n#log_columns = [col for col in train_df.columns if 'reg' in col]    \n#for col in log_columns:\n#    train_df[col] = np.square(train_df[col] +0.00001)\n    \nX = train_df[train_columns].values[:10000]\ntarget = train_df.target[:10000]\nprint(\"Training data shape: \", X.shape)\n\n#pca = KernelPCA(n_components=2, kernel='linear')\n#reduced_dim = pca.fit_transform(X)\nreduced_dim = TSNE(n_components=2).fit_transform(X)\nreduced_dim = reduced_dim[np.random.randint(0, len(reduced_dim), size=10000)]\n                                            \nreduced_df = pd.DataFrame(data=reduced_dim, columns=['x', 'y'])\nreduced_df['target'] = target\n\nplt.figure(figsize=(20, 8))\nsns.jointplot(x='x', y='y', data=reduced_df, size=7, color=\"g\")\nplt.show()\n\nplt.figure(figsize=(20,15))\nsns.lmplot(x='x', y='y', hue='target', data=reduced_df, size=7, fit_reg=False)\nplt.show()","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"29199385-bb49-4f27-962d-dde7620c1d39","collapsed":true,"_uuid":"a06043c82f6cf66390af043c47d53ffd843ce4ab"}},{"source":"","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"1935ca9d-62e2-4112-81c6-98fa2aa5ef27","collapsed":true,"_uuid":"82ebc9fa351458908f23b75f530ed142830d43cf"}},{"source":"","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"3dd47c60-6c53-49ac-ae75-420e79f47402","collapsed":true,"_uuid":"f3023e34ed7df8cee957ec7f49c590121d4166fb"}}],"nbformat":4,"nbformat_minor":1,"metadata":{"language_info":{"codemirror_mode":{"version":3,"name":"ipython"},"name":"python","mimetype":"text/x-python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3","file_extension":".py"},"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"}}}