{"cells":[{"metadata":{"_cell_guid":"45ba73d4-6c4c-40bb-9390-7dfc956c555d","_uuid":"dbd332f83c89108c4e641218f5c4e7b9cd325b80","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"MAX_ROUNDS = 650\nOPTIMIZE_ROUNDS = False\nLEARNING_RATE = 0.05"},{"metadata":{"_cell_guid":"7e199c98-16b0-45e7-a6d1-bdd9325c2631","_uuid":"b277fe426336d65ac71f0e6ac96c7ee16d02074c"},"cell_type":"markdown","source":"I recommend initially setting <code>MAX_ROUNDS</code> fairly high and using <code>OPTIMIZE_ROUNDS</code> to get an idea of the appropriate number of rounds (which, in my judgment, should be close to the maximum value of the optimized <code>tree_count</code> among all folds, maybe even a bit higher if your model is adequately regularized...or maybe not:  it's also a good idea to uncomment <code>verbose=True</code> sometimes and look at the pattern of validation scores for each fold to see what values might work well for all folds).  For a submission run I would turn off <code>OPTIMIZE_ROUNDS</code> and set <code>MAX_ROUNDS</code> to the appropraite number of total rounds.  The problem with \"early stopping\" by choosing the best round for each fold is that it overfits to the validation data.   It's therefore liable not to produce the optimal model for predicting test data, and if it's used to produce validation data for stacking/ensembling with other models, it would cause this one to have too much weight in the ensemble.\n\nAlso, CatBoost is notoriously slow.  If you want to run it in Kaggle's environment, you need to set a fairly high learning rate so that it will get to a decent fit reasonably quickly.  If you want to use it to do well in this competition, you need to set a lower learning rate and run it for a long time (higher <code>MAX_ROUNDS</code>).  That means you either need to run it overnight or run it on a fast computer with multiple cores (or both)."},{"metadata":{"_cell_guid":"b7258128-55f9-4543-8611-5e0a6661837b","_uuid":"72171ee53e170096d37a18eef84682fa348ae5c4","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"import numpy as np\nimport pandas as pd\nfrom catboost import CatBoostClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom numba import jit"},{"metadata":{"_cell_guid":"3d16f16e-12cc-4b41-b7bd-fa05ce44770c","_uuid":"154b078a7e86c0a5a328118a61d28e2581bb3b0a","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"# Compute gini\n\n# from CPMP's kernel https://www.kaggle.com/cpmpml/extremely-fast-gini-computation\n@jit\ndef eval_gini(y_true, y_prob):\n    y_true = np.asarray(y_true)\n    y_true = y_true[np.argsort(y_prob)]\n    ntrue = 0\n    gini = 0\n    delta = 0\n    n = len(y_true)\n    for i in range(n-1, -1, -1):\n        y_i = y_true[i]\n        ntrue += y_i\n        gini += y_i * delta\n        delta += 1 - y_i\n    gini = 1 - 2 * gini / (ntrue * (n - ntrue))\n    return gini"},{"metadata":{"_cell_guid":"52b50086-b405-4598-b11c-97887cdcce8e","_uuid":"07a5a5782894611e9006ae1b399b0b8fb8a0f06b","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"# Read data\ntrain_df = pd.read_csv('../input/train.csv', na_values=\"-1\") # .iloc[0:200,:]\ntest_df = pd.read_csv('../input/test.csv', na_values=\"-1\")"},{"metadata":{"_cell_guid":"1b36eb15-ee01-43a3-8766-27650f98158d","_uuid":"6255e3c12616b0279cef5c1bdec97751bb72d8b8","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"# Process data\nid_test = test_df['id'].values\nid_train = train_df['id'].values\n\ntrain_df = train_df.fillna(999)\ntest_df = test_df.fillna(999)\n\ncol_to_drop = train_df.columns[train_df.columns.str.startswith('ps_calc_')]\ntrain_df = train_df.drop(col_to_drop, axis=1)  \ntest_df = test_df.drop(col_to_drop, axis=1)  \n\nfor c in train_df.select_dtypes(include=['float64']).columns:\n    train_df[c]=train_df[c].astype(np.float32)\n    test_df[c]=test_df[c].astype(np.float32)\nfor c in train_df.select_dtypes(include=['int64']).columns[2:]:\n    train_df[c]=train_df[c].astype(np.int8)\n    test_df[c]=test_df[c].astype(np.int8)\n    \ny = train_df['target']\nX = train_df.drop(['target', 'id'], axis=1)\ny_valid_pred = 0*y\nX_test = test_df.drop(['id'], axis=1)\ny_test_pred = 0"},{"metadata":{"_cell_guid":"7c6e4823-4e8c-4408-b961-576d469e9241","_uuid":"6aa7ada2193c2e4b8a63eebda925cee5023b45b0","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"# Set up folds\nK = 5\nkf = KFold(n_splits = K, random_state = 1, shuffle = True)"},{"metadata":{"_cell_guid":"5d8108f3-e9e8-45d6-93b5-740eb7b4b10b","_uuid":"581c3f15f294378a0e2ac3305e9e3d375f664b21","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"# Set up classifier\nmodel = CatBoostClassifier(\n    learning_rate=LEARNING_RATE, \n    depth=6, \n    l2_leaf_reg = 14, \n    iterations = MAX_ROUNDS,\n#    verbose = True,\n    loss_function='Logloss'\n)"},{"metadata":{"_cell_guid":"c4e48347-920f-4ba7-8b37-cfbaab4c3c00","scrolled":true,"_uuid":"2b9ed96c98b705d3e4bf2a3d60323dfab4332674","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"# Run CV\n\nfor i, (train_index, test_index) in enumerate(kf.split(train_df)):\n    \n    # Create data for this fold\n    y_train, y_valid = y.iloc[train_index], y.iloc[test_index]\n    X_train, X_valid = X.iloc[train_index,:], X.iloc[test_index,:]\n    print( \"\\nFold \", i)\n    \n    # Run model for this fold\n    if OPTIMIZE_ROUNDS:\n        fit_model = model.fit( X_train, y_train, \n                               eval_set=[X_valid, y_valid],\n                               use_best_model=True\n                             )\n        print( \"  N trees = \", model.tree_count_ )\n    else:\n        fit_model = model.fit( X_train, y_train )\n        \n    # Generate validation predictions for this fold\n    pred = fit_model.predict_proba(X_valid)[:,1]\n    print( \"  Gini = \", eval_gini(y_valid, pred) )\n    y_valid_pred.iloc[test_index] = pred\n    \n    # Accumulate test set predictions\n    y_test_pred += fit_model.predict_proba(X_test)[:,1]\n    \ny_test_pred /= K  # Average test set predictions\n\nprint( \"\\nGini for full training set:\" )\neval_gini(y, y_valid_pred)"},{"metadata":{"_cell_guid":"0e3dfd76-c566-4b8d-a460-b56e964d0772","_uuid":"e61bf4e22c1c29c8358caeecb6e67d6658f2005d","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"# Save validation predictions for stacking/ensembling\nval = pd.DataFrame()\nval['id'] = id_train\nval['target'] = y_valid_pred.values\nval.to_csv('cat_valid.csv', float_format='%.6f', index=False)"},{"metadata":{"_cell_guid":"f4cbef2c-e52b-4afb-b8ef-904ee9b5f9d5","_uuid":"380fc8053d00cd8bb2796bfd2b59d10cbc4ce7e1","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"# Create submission file\nsub = pd.DataFrame()\nsub['id'] = id_test\nsub['target'] = y_test_pred\nsub.to_csv('cat_submit.csv', float_format='%.6f', index=False)"},{"metadata":{"_cell_guid":"10f560d4-ca82-480d-a923-642bedcb3822","_uuid":"f72a82b52898ce7478777be8ad42e2c91b238a77"},"cell_type":"markdown","source":"CV scores certainly seem to be correlated with public LB scores, but the correlation is not nearly as strong as one might hope. The difference so far ranges from .0019 to .0032 for this script without <code>OPTIMIZE_ROUNDS</code> and can be higher when <code>OPTIMIZE_ROUNDS</code> is set (which is to be expected, since that parameter causes the fit to select on each fold for the best CV performance without generally achieving a comparable improvement in LB performance).  Versions 17 through 20 (substantively identical) have slightly better public LB performance than earlier versions, when I sort my submissions by score.  (If one wants to be optimistic about the less significant digits of the LB score, maybe a difference of .0019 is really .0024, and a difference of .0032 is really .0027, so maybe the CV is working better than it first appears.)"}],"metadata":{"language_info":{"nbconvert_exporter":"python","pygments_lexer":"ipython3","mimetype":"text/x-python","name":"python","version":"3.6.3","codemirror_mode":{"version":3,"name":"ipython"},"file_extension":".py"},"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"}},"nbformat":4,"nbformat_minor":1}