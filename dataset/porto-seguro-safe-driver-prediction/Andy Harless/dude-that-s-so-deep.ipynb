{"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"},"language_info":{"file_extension":".py","name":"python","codemirror_mode":{"version":3,"name":"ipython"},"version":"3.6.3","mimetype":"text/x-python","nbconvert_exporter":"python","pygments_lexer":"ipython3"}},"nbformat":4,"nbformat_minor":1,"cells":[{"metadata":{"_uuid":"12f808bbb94d2ff51d4b8d531a32f21e179ae4cc","_cell_guid":"845e32ab-4600-4db5-84c6-1847f14aa48d"},"source":"This kernel is originally copied from [Tilii](https://www.kaggle.com/tilii7/keras-averaging-runs-gini-early-stopping), and it retains his original framework and most of the comments, but there are significant changes in the network itself.  The results of the demo verison are awful, but the full run on my computer at home produces decent results.  (At least I think it does: when I wrote this, it hadn't finished running, but based on fold performance relative to earlier versions, it seems to be heading for about 0.275 CV gini.)","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"32a79662d683ff157e2dd45d549940acec844253","_cell_guid":"fd43921a-c00d-4ebd-a6fc-1ed771baeda5","collapsed":true},"source":"DEMO_MODE = True   # Warning: takes 2 days on my GTX1080 with DEMO_MODE=False\n                   #   (But you can also adjust the folds and runs parameters directly)","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"7a90bcae1134c958f391542f4d67df93ed020d98","_cell_guid":"be30f312-b2fb-4e5b-a0b8-9ddbbb190d2e","collapsed":true},"source":"import numpy as np\nnp.random.seed()\nimport pandas as pd\nfrom datetime import datetime\nfrom sklearn.metrics import log_loss, roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom keras.models import load_model\nfrom keras.models import Sequential, Model\nfrom keras.layers import Input, Dense, Dropout, Activation\nfrom keras.layers.advanced_activations import PReLU\nfrom keras.layers.noise import GaussianDropout\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger, Callback\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.optimizers import Adam\nfrom keras import regularizers","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"bc2f0776a6622aa29b628ef37a660f340337f0cc","_cell_guid":"cd63fa76-0b6d-4b1b-bd9c-7dcaba0ccd40"},"source":"This callback is very important. It calculates roc_auc and gini values so they can be monitored during the run. Also, it creates a log of those parameters so that they can be used for early stopping. A tip of the hat to **[Roberto](https://www.kaggle.com/rspadim)** and **[this kernel](https://www.kaggle.com/rspadim/gini-keras-callback-earlystopping-validation)** for helping me figure out the latter.\n\n*Note that this callback in combination with early stopping doesn't print well if you are using verbose=1 (moving arrow) during fitting. I recommend that you use verbose=2 in fitting.*","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"ca16921d40817307195c9567405e4aa7c3765097","_cell_guid":"8d9c3461-8f27-4754-a204-a4cf82a62450","collapsed":true},"source":"class roc_auc_callback(Callback):\n    def __init__(self,training_data,validation_data):\n        self.x = training_data[0]\n        self.y = training_data[1]\n        self.x_val = validation_data[0]\n        self.y_val = validation_data[1]\n\n    def on_train_begin(self, logs={}):\n        return\n\n    def on_train_end(self, logs={}):\n        return\n\n    def on_epoch_begin(self, epoch, logs={}):\n        return\n\n    def on_epoch_end(self, epoch, logs={}):\n        y_pred = self.model.predict_proba(self.x, verbose=0)\n        roc = roc_auc_score(self.y, y_pred)\n        logs['roc_auc'] = roc_auc_score(self.y, y_pred)\n        logs['norm_gini'] = ( roc_auc_score(self.y, y_pred) * 2 ) - 1\n\n        y_pred_val = self.model.predict_proba(self.x_val, verbose=0)\n        roc_val = roc_auc_score(self.y_val, y_pred_val)\n        logs['roc_auc_val'] = roc_auc_score(self.y_val, y_pred_val)\n        logs['norm_gini_val'] = ( roc_auc_score(self.y_val, y_pred_val) * 2 ) - 1\n\n        print('\\rroc_auc: %s - roc_auc_val: %s - norm_gini: %s - norm_gini_val: %s' % (str(round(roc,5)),str(round(roc_val,5)),str(round((roc*2-1),5)),str(round((roc_val*2-1),5))), end=10*' '+'\\n')\n        return\n\n    def on_batch_begin(self, batch, logs={}):\n        return\n\n    def on_batch_end(self, batch, logs={}):\n        return\n","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"18a2e97141c98c5ea3dfa45c35e75e0257ddf20e","_cell_guid":"b4000c8c-8794-45e1-9ce5-ef84f9dc13a7"},"source":"Housekeeping utilities.","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"89ab39c0da583cf503966fa4edccfec9f25c2e50","_cell_guid":"69074e99-541c-4e93-b4d8-97f7a6407465","collapsed":true},"source":"def timer(start_time=None):\n    if not start_time:\n        start_time = datetime.now()\n        return start_time\n    elif start_time:\n        thour, temp_sec = divmod(\n            (datetime.now() - start_time).total_seconds(), 3600)\n        tmin, tsec = divmod(temp_sec, 60)\n        print('\\n Time taken: %i hours %i minutes and %s seconds.' %\n              (thour, tmin, round(tsec, 2)))\n\ndef scale_data(X, scaler=None):\n    if not scaler:\n        scaler = MinMaxScaler(feature_range=(-1, 1))\n        scaler.fit(X)\n    X = scaler.transform(X)\n    return X, scaler","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"ea7bfad9bf8617aa9182ad7f6cd3ea9e6fb11276","_cell_guid":"2f346986-75c0-4bed-b4db-e7ccc99f88f8"},"source":"I never seem to be able to write a generic routine for data loading where one would just plug in file names and everything else would be done automatically. Still trying.","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"217687c584b7af8619705f3cf23e480bcf029e58","_cell_guid":"8665bec9-6b3f-437a-836c-c0ee252338b8","collapsed":true},"source":"# train and test data path\nDATA_TRAIN_PATH = '../input/train.csv'\nDATA_TEST_PATH = '../input/test.csv'\n\ndef load_data(path_train=DATA_TRAIN_PATH, path_test=DATA_TEST_PATH):\n    train_loader = pd.read_csv(path_train, dtype={'target': np.int8, 'id': np.int32})\n    train = train_loader.drop(['target', 'id'], axis=1)\n    train_labels = train_loader['target'].values\n    train_ids = train_loader['id'].values\n    print('\\n Shape of raw train data:', train.shape)\n\n    test_loader = pd.read_csv(path_test, dtype={'id': np.int32})\n    test = test_loader.drop(['id'], axis=1)\n    test_ids = test_loader['id'].values\n    print(' Shape of raw test data:', test.shape)\n\n    return train, train_labels, test, train_ids, test_ids","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"1e112501fd4387f197b98b72f27155981b585178","_cell_guid":"1c0173f8-8f85-468d-97e0-6fa55c9cf640"},"source":"You can ignore most of the parameters below other than the top two. Obviously, more folds means longer running time, but I can tell you from experience that 10 folds with Keras will usually do better than 4. The number of \"runs\" should be in the 3-5 range. At a minimum, I suggest 5 folds and 3 independent runs per fold (which will eventually get averaged).  This is because of stochastic nature of neural networks, so one run per fold may or may not produce the best possible result.\n\n**If you can afford it, 10 folds and 5 runs per fold would be my recommendation. Be warned that it may take a day or two, even if you have a GPU.**","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"bfb737fe439f350215b3cde25389b5505c6cb244","_cell_guid":"fee66b38-1654-4a53-94e9-1e96356fecd9","collapsed":true},"source":"if DEMO_MODE:\n    folds = 2\n    runs = 1\n    STARTING_LR = 2e-3\n    LR_DECAY = 3e-5\n    PATIENCE = 5\n    MAX_EPOCHS = 20\nelse:\n    folds = 10\n    runs = 5\n    STARTING_LR = 4e-3\n    LR_DECAY = 1e-4\n    PATIENCE = 12\n    MAX_EPOCHS = 5000","outputs":[],"cell_type":"code"},{"execution_count":null,"metadata":{"_uuid":"e34bced4579cd9b755c33c13a419f29f0ce8354f","_cell_guid":"0609e539-110a-4968-bab8-6156e3ebb904","collapsed":true},"source":"cv_LL = 0\ncv_AUC = 0\ncv_gini = 0\nfpred = []\navpred = []\navreal = []\navids = []","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"49a0ff4069fb3e38e1441d907869e3e2457fbb57","_cell_guid":"988ac526-b38d-46ee-8636-c23128470b0e"},"source":"Loading data. Converting \"categorical\" variables, even though in this dataset they are actually numeric.","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"fea20a516b7076d491b9aa4bfe2153243e0974c8","_cell_guid":"b6cc7153-1c92-47d2-a24f-9b41668a0a90","collapsed":true},"source":"# Load data set and target values\ntrain, target, test, tr_ids, te_ids = load_data()\nn_train = train.shape[0]\ntrain_test = pd.concat((train, test)).reset_index(drop=True)\ncol_to_drop = train.columns[train.columns.str.endswith('_cat')]\ncol_to_dummify = train.columns[train.columns.str.endswith('_cat')].astype(str).tolist()\n\nfor col in col_to_dummify:\n    dummy = pd.get_dummies(train_test[col].astype('category'))\n    columns = dummy.columns.astype(str).tolist()\n    columns = [col + '_' + w for w in columns]\n    dummy.columns = columns\n    train_test = pd.concat((train_test, dummy), axis=1)\n\ntrain_test.drop(col_to_dummify, axis=1, inplace=True)\ntrain_test_scaled, scaler = scale_data(train_test)\ntrain = train_test_scaled[:n_train, :]\ntest = train_test_scaled[n_train:, :]\nprint('\\n Shape of processed train data:', train.shape)\nprint(' Shape of processed test data:', test.shape)","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"7de3cc91cc55f33ce70ce3acbda04fdb9c618ecc","_cell_guid":"94b32825-7e74-4f2f-8609-66eae854113f"},"source":"The two parameters below are worth playing with. Larger patience gives the network a better chance to find solutions when it gets close to the local/global minimum. It also means longer training times. Batch size is one of those parameters that can always be optimized for any given dataset. If you have a GPU, larger batch sizes translate to faster training, but that may or may not be better for the quality of training.","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"37113e0d59829e615f475e1ac9316c4d2097534d","_cell_guid":"631f1bcc-b309-449f-9595-aee7200f0210","collapsed":true},"source":"patience = PATIENCE\nbatchsize = 128","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"f9c1cbc23ef29cc5573390df6896640d8a1da536","_cell_guid":"6b757c87-bdc0-4fd6-a620-9a1361aa7e0b"},"source":"There are lots of comments within the code below. I think the callback section is particularly import.","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"e205d78f5cd47452a1a4fba004f9169534356a9f","_cell_guid":"8da63d81-1961-4b91-b934-9dc7fe05d8ea","collapsed":true,"scrolled":true},"source":"# Let's split the data into folds. I always use the same random number for reproducibility, \n# and suggest that you do the same (you certainly don't have to use 1001).\n\nskf = StratifiedKFold(n_splits=folds, random_state=1001)\nstarttime = timer(None)\nfor i, (train_index, test_index) in enumerate(skf.split(train, target)):\n    start_time = timer(None)\n    X_train, X_val = train[train_index], train[test_index]\n    y_train, y_val = target[train_index], target[test_index]\n    train_ids, val_ids = tr_ids[train_index], tr_ids[test_index]\n    \n# This is where we define and compile the model. These parameters are not optimal, as they were chosen \n# to get a notebook to complete in 60 minutes. Other than leaving BatchNormalization and last sigmoid \n# activation alone, virtually everything else can be optimized: number of neurons, types of initializers, \n# activation functions, dropout values. The same goes for the optimizer at the end.\n\n#########\n# Never move this model definition to the beginning of the file or anywhere else outside of this loop. \n# The model needs to be initialized anew every time you run a different fold. If not, it will continue \n# the training from a previous model, and that is not what you want.\n#########\n\n    # This definition must be within the for loop or else it will continue training previous model\n    def baseline_model():\n\n        model = Sequential()\n        model.add(\n            Dense(200, input_dim=X_train.shape[1], kernel_initializer='glorot_normal',\n                kernel_regularizer=regularizers.l2(1e-4)))\n        model.add(PReLU())\n        model.add(BatchNormalization())\n        model.add(GaussianDropout(0.5))\n        \n        model.add(Dense(120, kernel_initializer='glorot_normal',\n                        kernel_regularizer=regularizers.l2(5e-5)))\n        model.add(PReLU())\n        model.add(BatchNormalization())\n        model.add(GaussianDropout(0.4))\n        \n        model.add(Dense(80, kernel_initializer='glorot_normal',\n                       kernel_regularizer=regularizers.l2(2e-5)))\n        model.add(PReLU())\n        model.add(BatchNormalization())\n        model.add(GaussianDropout(0.35))\n        \n        model.add(Dense(40, kernel_initializer='glorot_normal',\n                  kernel_regularizer=regularizers.l2(1e-5)))\n        model.add(PReLU())\n        model.add(BatchNormalization())\n        model.add(GaussianDropout(0.3))\n        \n        model.add(Dense(15, kernel_initializer='glorot_normal',\n                 kernel_regularizer=regularizers.l2(5e-6)))\n        model.add(PReLU())\n        model.add(BatchNormalization())\n        model.add(GaussianDropout(0.2))\n        \n        model.add(Dense(1, activation='sigmoid'))\n\n        # Compile model\n        model.compile( optimizer=Adam(lr=STARTING_LR, decay=LR_DECAY),\n                       metrics = ['accuracy'], loss='binary_crossentropy' )\n\n        return model\n\n# This is where we repeat the runs for each fold. If you choose runs=1 above, it will run a \n# regular N-fold procedure.\n\n#########\n# It is important to leave the call to random seed here, so each run starts with a different seed.\n#########\n\n    for run in range(runs):\n        print('\\n Fold %d - Run %d\\n' % ((i + 1), (run + 1)))\n        np.random.seed()\n\n# Lots to unpack here.\n\n# The first callback prints out roc_auc and gini values at the end of each epoch. It must be listed \n# before the EarlyStopping callback, which monitors gini values saved in the previous callback. Make \n# sure to set the mode to \"max\" because the default value (\"auto\") will not handle gini properly \n# (it will act as if the model is not improving even when roc/gini go up).\n\n# CSVLogger creates a record of all iterations. Not really needed but it doesn't hurt to have it.\n\n# ModelCheckpoint saves a model each time gini improves. Its mode also must be set to \"max\" for reasons \n# explained above.\n\n        callbacks = [\n            roc_auc_callback(training_data=(X_train, y_train),validation_data=(X_val, y_val)),  # call this before EarlyStopping\n            EarlyStopping(monitor='norm_gini_val', patience=patience, mode='max', verbose=1),\n            CSVLogger('keras-kfold-run-01-v1-epochs.log', separator=',', append=False),\n            ModelCheckpoint(\n                    'keras-kfold-run-01-v1-fold-' + str('%02d' % (i + 1)) + '-run-' + str('%02d' % (run + 1)) + '.check',\n                    monitor='norm_gini_val', mode='max', # mode must be set to max or Keras will be confused\n                    save_best_only=True,\n                    verbose=1)\n        ]\n\n# The classifier is defined here. Epochs should be be set to a very large number (not 3 like below) which \n# will never be reached anyway because of early stopping. I usually put 5000 there. Because why not.\n\n        nnet = KerasClassifier(\n            build_fn=baseline_model,\n# Epoch needs to be set to a very large number ; early stopping will prevent it from reaching\n            epochs=MAX_EPOCHS,\n            batch_size=batchsize,\n            validation_data=(X_val, y_val),\n            verbose=2,\n            shuffle=True,\n            callbacks=callbacks)\n\n        fit = nnet.fit(X_train, y_train)\n        \n# We want the best saved model - not the last one where the training stopped. So we delete the old \n# model instance and load the model from the last saved checkpoint. Next we predict values both for \n# validation and test data, and create a summary of parameters for each run.\n\n        del nnet\n        nnet = load_model('keras-kfold-run-01-v1-fold-' + str('%02d' % (i + 1)) + '-run-' + str('%02d' % (run + 1)) + '.check')\n        scores_val_run = nnet.predict_proba(X_val, verbose=0)\n        LL_run = log_loss(y_val, scores_val_run)\n        print('\\n Fold %d Run %d Log-loss: %.5f' % ((i + 1), (run + 1), LL_run))\n        AUC_run = roc_auc_score(y_val, scores_val_run)\n        print(' Fold %d Run %d AUC: %.5f' % ((i + 1), (run + 1), AUC_run))\n        print(' Fold %d Run %d normalized gini: %.5f' % ((i + 1), (run + 1), AUC_run*2-1))\n        y_pred_run = nnet.predict_proba(test, verbose=0)\n        if run > 0:\n            scores_val = scores_val + scores_val_run\n            y_pred = y_pred + y_pred_run\n        else:\n            scores_val = scores_val_run\n            y_pred = y_pred_run\n            \n# We average all runs from the same fold and provide a parameter summary for each fold. Unless something \n# is wrong, the numbers printed here should be better than any of the individual runs.\n\n    scores_val = scores_val / runs\n    y_pred = y_pred / runs\n    LL = log_loss(y_val, scores_val)\n    print('\\n Fold %d Log-loss: %.5f' % ((i + 1), LL))\n    AUC = roc_auc_score(y_val, scores_val)\n    print(' Fold %d AUC: %.5f' % ((i + 1), AUC))\n    print(' Fold %d normalized gini: %.5f' % ((i + 1), AUC*2-1))\n    timer(start_time)\n    \n# We add up predictions on the test data for each fold. Create out-of-fold predictions for validation data.\n\n    if i > 0:\n        fpred = pred + y_pred\n        avreal = np.concatenate((avreal, y_val), axis=0)\n        avpred = np.concatenate((avpred, scores_val), axis=0)\n        avids = np.concatenate((avids, val_ids), axis=0)\n    else:\n        fpred = y_pred\n        avreal = y_val\n        avpred = scores_val\n        avids = val_ids\n    pred = fpred\n    cv_LL = cv_LL + LL\n    cv_AUC = cv_AUC + AUC\n    cv_gini = cv_gini + (AUC*2-1)\n","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"b0fc6c40b88ab16828fb041bb64510a6dab44d27","_cell_guid":"a14d6cf9-abe2-46d0-9d7d-c3d07f555748"},"source":"Here we average all the predictions and provide the final summary.","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"ca37252600dc011cd9bc354fc702efd76e0324f2","_cell_guid":"ffee9fa9-5ff7-41df-8702-e8bf0c386eb6","collapsed":true},"source":"LL_oof = log_loss(avreal, avpred)\nprint('\\n Average Log-loss: %.5f' % (cv_LL/folds))\nprint(' Out-of-fold Log-loss: %.5f' % LL_oof)\nAUC_oof = roc_auc_score(avreal, avpred)\nprint('\\n Average AUC: %.5f' % (cv_AUC/folds))\nprint(' Out-of-fold AUC: %.5f' % AUC_oof)\nprint('\\n Average normalized gini: %.5f' % (cv_gini/folds))\nprint(' Out-of-fold normalized gini: %.5f' % (AUC_oof*2-1))\nscore = str(round((AUC_oof*2-1), 5))\ntimer(starttime)\nmpred = pred / folds","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"61295f2ab3eabbe49441bd7e1b47736f3c342d6d","_cell_guid":"fd1bd64e-fdea-4b1b-88ca-d469b7013bed"},"source":"Save the file with out-of-fold predictions. For easier book-keeping, file names have the out-of-fold gini score and are are tagged by date and time.","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"3e20f234c25aaa7302f8a9ed7b752cdaed415b9a","_cell_guid":"e9914308-7d3b-4eef-8ea0-65532d1b4c21","collapsed":true},"source":"print('#\\n Writing results')\nnow = datetime.now()\noof_result = pd.DataFrame(avreal, columns=['target'])\noof_result['prediction'] = avpred\noof_result['id'] = avids\noof_result.sort_values('id', ascending=True, inplace=True)\noof_result = oof_result.set_index('id')\nsub_file = 'train_kfold-keras-run-01-v1-oof_' + str(score) + '_' + str(now.strftime('%Y-%m-%d-%H-%M')) + '.csv'\nprint('\\n Writing out-of-fold file:  %s' % sub_file)\noof_result.to_csv(sub_file, index=True, index_label='id')","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"f4d9e51f7ea16afbb5c18886c3fddd48bbd55674","_cell_guid":"72a76239-2c3e-4fd1-b280-3862755106be"},"source":"Save the final prediction. This is the one to submit.","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"61a7063ce1fb44e4a79a1a52ae12db2600000df5","_cell_guid":"30446b64-aec5-45da-bf09-91fac68ae6eb","collapsed":true},"source":"result = pd.DataFrame(mpred, columns=['target'])\nresult['id'] = te_ids\nresult = result.set_index('id')\nprint('\\n First 10 lines of your k-fold average prediction:\\n')\nprint(result.head(10))\nsub_file = 'submission_kfold-average-keras-run-01-v1_' + str(score) + '_' + str(now.strftime('%Y-%m-%d-%H-%M')) + '.csv'\nprint('\\n Writing submission:  %s' % sub_file)\nresult.to_csv(sub_file, index=True, index_label='id')","outputs":[],"cell_type":"code"}]}