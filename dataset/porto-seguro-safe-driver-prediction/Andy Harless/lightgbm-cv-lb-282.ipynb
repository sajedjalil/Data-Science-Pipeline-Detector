{"nbformat_minor":1,"cells":[{"cell_type":"markdown","metadata":{"_uuid":"ef25ae18c01b92601c3222ad9d23a86957e95782","_cell_guid":"12b97784-1dee-48c4-b831-56c696c88f78"},"source":"Based on the1owl's [kernel](https://www.kaggle.com/the1owl/forza-baseline-lightgbm-example)"},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"dbd332f83c89108c4e641218f5c4e7b9cd325b80","_cell_guid":"45ba73d4-6c4c-40bb-9390-7dfc956c555d","collapsed":true},"outputs":[],"source":"MAX_ROUNDS = 1200\nOPTIMIZE_ROUNDS = False\nLEARNING_RATE = 0.024"},{"cell_type":"markdown","metadata":{"_uuid":"b277fe426336d65ac71f0e6ac96c7ee16d02074c","_cell_guid":"7e199c98-16b0-45e7-a6d1-bdd9325c2631"},"source":"I recommend initially setting <code>MAX_ROUNDS</code> fairly high and using <code>OPTIMIZE_ROUNDS</code> to get an idea of the appropriate number of rounds (which, in my judgment, should be close to the maximum value of <code>best_iteration</code> among all folds, maybe even a bit higher if your model is adequately regularized...or alternatively, you can look at the detailed output from the boosting rounds and choose a value that seems like it would work OK for all folds).  Then I would turn off <code>OPTIMIZE_ROUNDS</code> and set <code>MAX_ROUNDS</code> to the appropraite number of total rounds.  The problem with \"early stopping\" by choosing the best round for each fold is that it overfits to the validation data.    It's therefore liable not to produce the optimal model for predicting test data, and if it's used to produce validation predictions for stacking/ensembling with other models, it would cause this one to have too much weight in the ensemble."},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"72171ee53e170096d37a18eef84682fa348ae5c4","_cell_guid":"b7258128-55f9-4543-8611-5e0a6661837b","collapsed":true},"outputs":[],"source":"import numpy as np\nimport pandas as pd\nfrom catboost import CatBoostClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom numba import jit\nfrom sklearn import *\nimport lightgbm as lgb\nfrom multiprocessing import *"},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"154b078a7e86c0a5a328118a61d28e2581bb3b0a","_cell_guid":"3d16f16e-12cc-4b41-b7bd-fa05ce44770c","collapsed":true},"outputs":[],"source":"# Compute gini\n\n# from CPMP's kernel https://www.kaggle.com/cpmpml/extremely-fast-gini-computation\n@jit\ndef eval_gini(y_true, y_prob):\n    y_true = np.asarray(y_true)\n    y_true = y_true[np.argsort(y_prob)]\n    ntrue = 0\n    gini = 0\n    delta = 0\n    n = len(y_true)\n    for i in range(n-1, -1, -1):\n        y_i = y_true[i]\n        ntrue += y_i\n        gini += y_i * delta\n        delta += 1 - y_i\n    gini = 1 - 2 * gini / (ntrue * (n - ntrue))\n    return gini"},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"affb627af275ad7aa0c5c85dd826ee140370eb3e","_cell_guid":"7f8f84f9-aa46-43d6-8d78-619ef9a7dcce","collapsed":true},"outputs":[],"source":"def transform_df(df):\n    df = pd.DataFrame(df)\n    dcol = [c for c in df.columns if c not in ['id','target']]\n    df['ps_car_13_x_ps_reg_03'] = df['ps_car_13'] * df['ps_reg_03']\n    df['negative_one_vals'] = np.sum((df[dcol]==-1).values, axis=1)\n    for c in dcol:\n        if '_bin' not in c: #standard arithmetic\n            df[c+str('_median_range')] = (df[c].values > d_median[c]).astype(np.int)\n            df[c+str('_mean_range')] = (df[c].values > d_mean[c]).astype(np.int)\n    for c in one_hot:\n        if len(one_hot[c])>2 and len(one_hot[c]) < 7:\n            for val in one_hot[c]:\n                df[c+'_oh_' + str(val)] = (df[c].values == val).astype(np.int)\n    return df\n\ndef multi_transform(df):\n    p = Pool(cpu_count())\n    df = p.map(transform_df, np.array_split(df, cpu_count()))\n    df = pd.concat(df, axis=0, ignore_index=True).reset_index(drop=True)\n    p.close(); p.join()\n    return df\n\ndef gini_lgb(preds, dtrain):\n    y = list(dtrain.get_label())\n    score = eval_gini(y, preds) / eval_gini(y, y)\n    return 'gini', score, True\n\n"},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"07a5a5782894611e9006ae1b399b0b8fb8a0f06b","_cell_guid":"52b50086-b405-4598-b11c-97887cdcce8e","collapsed":true},"outputs":[],"source":"# Read data\ntrain_df = pd.read_csv('../input/train.csv') # .iloc[0:200,:]\ntest_df = pd.read_csv('../input/test.csv')"},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"6255e3c12616b0279cef5c1bdec97751bb72d8b8","_cell_guid":"1b36eb15-ee01-43a3-8766-27650f98158d","collapsed":true},"outputs":[],"source":"# Process data\ncol = [c for c in train_df.columns if c not in ['id','target']]\ncol = [c for c in col if not c.startswith('ps_calc_')]\n\nid_test = test_df['id'].values\nid_train = train_df['id'].values\n\ny = train_df['target']\nX = train_df[col]\ny_valid_pred = 0*y\nX_test = test_df.drop(['id'], axis=1)\ny_test_pred = 0"},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"6aa7ada2193c2e4b8a63eebda925cee5023b45b0","_cell_guid":"7c6e4823-4e8c-4408-b961-576d469e9241","collapsed":true},"outputs":[],"source":"# Set up folds\nK = 5\nkf = KFold(n_splits = K, random_state = 1, shuffle = True)"},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"581c3f15f294378a0e2ac3305e9e3d375f664b21","_cell_guid":"5d8108f3-e9e8-45d6-93b5-740eb7b4b10b","collapsed":true},"outputs":[],"source":"# Set up classifier\nparams = {\n    'learning_rate': LEARNING_RATE, \n    'max_depth': 4, \n    'lambda_l1': 16.7,\n    'boosting': 'gbdt', \n    'objective': 'binary', \n    'metric': 'auc',\n    'feature_fraction': .7,\n    'is_training_metric': False, \n    'seed': 99\n}"},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"2b9ed96c98b705d3e4bf2a3d60323dfab4332674","scrolled":false,"_cell_guid":"c4e48347-920f-4ba7-8b37-cfbaab4c3c00","collapsed":true},"outputs":[],"source":"# Run CV\n\nfor i, (train_index, test_index) in enumerate(kf.split(train_df)):\n    \n    # Create data for this fold\n    y_train, y_valid = y.iloc[train_index].copy(), y.iloc[test_index].copy()\n    X_train, X_valid = X.iloc[train_index,:].copy(), X.iloc[test_index,:].copy()\n    test = test_df.copy()[col]\n    print( \"\\nFold \", i)\n\n    # Transform data for this fold\n    one_hot = {c: list(X_train[c].unique()) for c in X_train.columns}\n    X_train = X_train.replace(-1, np.NaN)  # Get rid of -1 while computing summary stats\n    d_median = X_train.median(axis=0)\n    d_mean = X_train.mean(axis=0)\n    X_train = X_train.fillna(-1)  # Restore -1 for missing values\n\n    X_train = multi_transform(X_train)\n    X_valid = multi_transform(X_valid)\n    test = multi_transform(test)\n\n    # Run model for this fold\n    if OPTIMIZE_ROUNDS:\n        fit_model = lgb.train( \n                               params, \n                               lgb.Dataset(X_train, label=y_train), \n                               MAX_ROUNDS, \n                               lgb.Dataset(X_valid, label=y_valid), \n                               verbose_eval=50, \n                               feval=gini_lgb, \n                               early_stopping_rounds=200 \n                             )\n        print( \" Best iteration = \", fit_model.best_iteration )\n        pred = fit_model.predict(X_valid, num_iteration=fit_model.best_iteration)\n        test_pred = fit_model.predict(test[col], num_iteration=fit_model.best_iteration)\n    else:\n        fit_model = lgb.train( \n                               params, \n                               lgb.Dataset(X_train, label=y_train), \n                               MAX_ROUNDS, \n                               verbose_eval=50 \n                             )\n        pred = fit_model.predict(X_valid)\n        test_pred = fit_model.predict(test)\n\n    # Save validation predictions for this fold\n    print( \"  Gini = \", eval_gini(y_valid, pred) )\n    y_valid_pred.iloc[test_index] = (np.exp(pred) - 1.0).clip(0,1)\n    \n    # Accumulate test set predictions\n    y_test_pred += (np.exp(test_pred) - 1.0).clip(0,1)\n    \ny_test_pred /= K  # Average test set predictions\n\nprint( \"\\nGini for full training set:\" )\neval_gini(y, y_valid_pred)"},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"e61bf4e22c1c29c8358caeecb6e67d6658f2005d","_cell_guid":"0e3dfd76-c566-4b8d-a460-b56e964d0772","collapsed":true},"outputs":[],"source":"# Save validation predictions for stacking/ensembling\nval = pd.DataFrame()\nval['id'] = id_train\nval['target'] = y_valid_pred.values\nval.to_csv('lgb_valid.csv', float_format='%.6f', index=False)"},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"380fc8053d00cd8bb2796bfd2b59d10cbc4ce7e1","_cell_guid":"f4cbef2c-e52b-4afb-b8ef-904ee9b5f9d5","collapsed":true},"outputs":[],"source":"# Create submission file\nsub = pd.DataFrame()\nsub['id'] = id_test\nsub['target'] = y_test_pred\nsub.to_csv('lgb_submit.csv', float_format='%.6f', index=False)"},{"cell_type":"markdown","metadata":{"_uuid":"18b6d7a7385825ed07623911b64ca30008983e13","_cell_guid":"aa79af82-15f2-4d12-aef9-4048034742ce"},"source":"version 8:  Resubmitting identical run because version 7 seems to have become invisible<br>\nversions 9,10 (substantively identical): Set <code>lambda_l1=16.7</code>, sorted LB score improved but still reported as .282<br>\nversion 11: With <code>OPTIMIZE_ROUNDS</code>, negate gini score so LightGBM will minimize. <code>is_higher_better</code> not working.<br>\nversion 12: Un-negate gini score and set <code>is_higher_better</code> again. Maybe I misunderstood.<br>\nversion 14: Set <code>MAX_ROUNDS=1400</code> for prediction run.<br>\nversions 15-21: Set <code>feature_fraction</code>. LB score goes up but still reported as .282"},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"539c2db7c30cecbb9f37dc006b1b81fedd349b87","_cell_guid":"ca9594fa-57ce-4393-9b1c-3383310e83a3","collapsed":true},"outputs":[],"source":""}],"nbformat":4,"metadata":{"language_info":{"nbconvert_exporter":"python","version":"3.6.3","name":"python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","file_extension":".py","mimetype":"text/x-python"},"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"}}}