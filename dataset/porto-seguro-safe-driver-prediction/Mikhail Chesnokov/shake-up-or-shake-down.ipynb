{"nbformat":4,"cells":[{"source":"# Shake-up or Shake-down?\n\nEverybody is talking about shake-up at this competition ([here](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/43144), [here](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/43315) ,[here](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/43547), [here](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/43336)). Here is my 2 cents. Let's try to estimate shake-up numericaly somehow. This notebook based on [nice exploration](https://www.kaggle.com/vpaslay/is-your-small-gini-significant) of how many samples should be guessed additionally to get an improvement of 0.001 of gini score and a [discussion](https://www.kaggle.com/vpaslay/is-your-small-gini-significant#244525) below it. Another [interesting kernel](https://www.kaggle.com/alexfir/expected-gini-standard-error) on this topic estimated the standard error of simple model depending on test size.\n\nThe main question I want to explore here is:\n- How much can be the **difference between public and private test score**?\n\nWe will use simple and naive method to estimate aforementioned difference depending on the public score. We do not have labels for test dataset, but we have train labels, so let's assume that our train set can represent test set. We will use OOF predictions of train set, split them randomly with a same proportion as public and private leaderboard split (private is **70%** of all test). (OOF predictions were taken from [notebook v 38](https://www.kaggle.com/aharless/xgboost-cv-lb-284), feature \"New kernel with this data\" didn't work as I expected and I couldn't read the data =( so downloaded and uploaded the validation predictions).\n\n## Assumptions\nIt should be noticed that here we assume several things:\n- Train and test datasets have similar class balances;\n- Difference of sample sizes of train and test can be ignored;\n- Generaly: OOF predictions of train set can represent test set.","metadata":{"_cell_guid":"74a0b7d8-b324-4b61-92e4-950670b3ef57","_uuid":"5067b4a4d6863e7eb0305b641c3eaf47f6addc9d"},"cell_type":"markdown"},{"source":"## Load data\n\nLet's load OOF predictions and train target (with ID field) and define gini calculating function.","metadata":{"_cell_guid":"23e716df-126a-4a47-bd6a-eb4b7c26ff81","_uuid":"1b43e016d2766d41e43db10406e4e877b79acfb0"},"cell_type":"markdown"},{"source":"import numpy as np \nimport pandas as pd \nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-poster')\n\n# load the data\noof_preds = pd.read_csv('../input/xgb-valid-preds-public/xgb_valid.csv')\ny = pd.read_csv('../input/porto-seguro-safe-driver-prediction/train.csv', \n                usecols = ['id', 'target'])\n\nprint('Shape of OOF preds: \\t', oof_preds.shape)\nprint('Shape of train target:\\t', y.shape)","metadata":{"_kg_hide-output":false,"_cell_guid":"5648258a-0eb5-4456-b742-e3b919ce1cb5","_kg_hide-input":true,"_uuid":"87da39ffb7e8d38cb58b6f40557c2311ce42025f"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"# gini calculation from https://www.kaggle.com/tezdhar/faster-gini-calculation\ndef ginic(actual, pred):\n    actual = np.asarray(actual) #In case, someone passes Series or list\n    n = len(actual)\n    a_s = actual[np.argsort(pred)]\n    a_c = a_s.cumsum()\n    giniSum = a_c.sum() / a_s.sum() - (n + 1) / 2.0\n    return giniSum / n\n \ndef gini_normalizedc(a, p):\n    if p.ndim == 2:#Required for sklearn wrapper\n        p = p[:,1] #If proba array contains proba for both 0 and 1 classes, just pick class 1\n    return ginic(a, p) / ginic(a, a)","metadata":{"_cell_guid":"a52c15c2-808e-49da-8c90-473c4c8c5ed4","_kg_hide-input":true,"_uuid":"1e476f0ebd1d5147b706d37cf28441bf879ddf54","collapsed":true},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"## Single split\n\nHere we make one split of OOF predictions using *train_test_split* from *sklearn* (with fixed seed). As mentioned above proportion of test size is 70% from all test - so we will use he same share. ","metadata":{"_cell_guid":"9393c537-ed1a-4028-a9fb-af83e9675768","_uuid":"3f7e32c73e022ae4bd5edf62f077ccc94f1c1707"},"cell_type":"markdown"},{"source":"PROPORTION_PRIVATE = 0.70\ny_preds_public, y_preds_private, y_public, y_private = train_test_split(oof_preds.target.values, \n                                                                        y.target.values, \n                                                                        test_size=PROPORTION_PRIVATE, \n                                                                        random_state=42)\n\nprint('Proportion of private:\\t',PROPORTION_PRIVATE)\nprint('Public score:\\t', round(gini_normalizedc(y_public, y_preds_public), 6))\nprint('Private score:\\t', round(gini_normalizedc(y_private, y_preds_private), 6))","metadata":{"_kg_hide-output":false,"_cell_guid":"3cd2dae7-41f9-4ce5-a78d-e6dfbdc33f0c","_kg_hide-input":true,"_uuid":"2193f4825c6f056e86781808021b00d96f6e2f85"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"So, we splited OOF predictions somehow and got 0.275 gini score on small part (public) and 0.290 on big part (private). That was a lucky split=) Let's do it many times to collect statistics over scores.","metadata":{"_cell_guid":"b37c7593-c8e7-4b4b-b3da-ef3ee830b6fd","_uuid":"36dfae53aab4442eb68bd22f35f977317fd90e3a"},"cell_type":"markdown"},{"source":"## 10k splits\n\nHere we will do the public-private split 10 000 times with different random seeds and collect gini scores from every split. (take some time - about 20 min)","metadata":{"_cell_guid":"55a55966-ae82-40bc-960e-9f4d10a1f9e3","_uuid":"843fbe84f175382d4e070874ebe2408589b1ba1a"},"cell_type":"markdown"},{"source":"%%time\ngini_public = []\ngini_private = []\n# do the split 10k times\nfor rs in range(10000):\n    y_preds_public, y_preds_private, y_public, y_private = train_test_split(oof_preds.target.values, \n                                                                            y.target.values, \n                                                                            test_size=PROPORTION_PRIVATE, \n                                                                            random_state=rs)\n    gini_public.append(gini_normalizedc(y_public, y_preds_public))\n    gini_private.append(gini_normalizedc(y_private, y_preds_private))\n\n# save results to numpy arrays\ngini_public_arr = np.array(gini_public)\ngini_private_arr = np.array(gini_private)","metadata":{"_kg_hide-output":false,"_cell_guid":"b1c6fc95-fd6f-4f3f-a433-626b78b0807d","_kg_hide-input":true,"_uuid":"556eb4d27473e091f2acad4376d3b464851d2ff7"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"Let's plot a histogram of public-private difference of scores:","metadata":{"_cell_guid":"4ef1c0ad-c855-4708-8d98-532392ccc9c6","_uuid":"cf397112a1cf3c1fa3788332aaad62ee578f53e1"},"cell_type":"markdown"},{"source":"# 10000 random_states\nplt.figure(figsize=(10,6))\nplt.hist(gini_public_arr - gini_private_arr, bins=50)\nplt.title('(Public - Private) scores')\nplt.xlabel('Gini score difference')\nplt.show()","metadata":{"_cell_guid":"0581778b-de95-4a29-ad13-aa25b9016893","_kg_hide-input":true,"_uuid":"1a2ea76c71f69b5f5689f3df740a63d3fd53e49f"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"Looks much the same as in [aforementioned kernel](https://www.kaggle.com/vpaslay/is-your-small-gini-significant): we have deviation mostly between -0.02 and 0.02, it's realy huge range that leads to depression=(.\n\nBut wait! Here we use the OOF predictions of model which score on leaderboard **we know** (it's 0.284). So let's naively assume that this score represent our public-private split score on train and find in our array of public ginis (computed above) those splits, which score 0.284. Let's plot the public-private difference only for them:","metadata":{"_cell_guid":"1cb69165-6952-4058-b900-d4b7d3779e7a","_uuid":"1c55a581ac80fbdf068348b76a049528446cfabc"},"cell_type":"markdown"},{"source":"#find indexies where public score was .284\nmy_indexies = np.where((gini_public_arr >= 0.284) &(gini_public_arr < 0.285))[0]\n\nplt.figure(figsize=(10,6))\nplt.hist(gini_public_arr[my_indexies] - gini_private_arr[my_indexies], bins=50)\nplt.title('(Public - Private) scores, where public = .284')\nplt.xlabel('Gini score difference')\nplt.show()","metadata":{"_cell_guid":"89e2763b-4843-4dc1-9f17-236680c69344","_kg_hide-input":true,"_uuid":"973a6326bbcc8e5193f4c61996f54a03cc6a6503"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"Hm... absolutely different picture: we have not so wide, uniform range between -0.003 and -0.0016, and most importantly that private score is a higher than public (all differences < 0).\n\nFor comparison let's look at differences in splits, which scores 0.286 on public part:","metadata":{"_cell_guid":"84e86f7d-6552-4d7a-b43c-bc5920c12c0a","_uuid":"3c2a6a18c604c2c23060b890ce350fe51d9db662"},"cell_type":"markdown"},{"source":"#find indexies where public score was .286\nmy_indexies = np.where((gini_public_arr >= 0.286) &(gini_public_arr < 0.287))[0]\n\nplt.figure(figsize=(10,6))\nplt.hist(gini_public_arr[my_indexies] - gini_private_arr[my_indexies], bins=50)\nplt.title('(Public - Private) scores, where public = .286')\nplt.xlabel('Gini score difference')\nplt.show()","metadata":{"_cell_guid":"2718821e-a9ed-4a62-a529-15ee300a90ed","_kg_hide-input":true,"_uuid":"2e9a522440180416361c3a992225ffbc65318a32"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"On the plot above again we have uniform distribution in range between -0.0020 and 0.0012. Lets compare it with range of public between 0.284 and 0.287 (not including 0.287)","metadata":{"_cell_guid":"e49d6757-db91-4c67-b901-0d27ed41edef","_uuid":"5cf20660c31c61737bc5bb38a6087d9f7abc5fdf"},"cell_type":"markdown"},{"source":"#find indexies where public score was .284-.287\nmy_indexies = np.where((gini_public_arr >= 0.284) &(gini_public_arr < 0.287))[0]\n\nplt.figure(figsize=(10,6))\nplt.hist(gini_public_arr[my_indexies] - gini_private_arr[my_indexies], bins=50)\nplt.title('(Public - Private) scores, where public between .284 and .287')\nplt.xlabel('Gini score difference')\nplt.show()","metadata":{"_cell_guid":"e5e3173c-ae9c-4d56-aa3a-ce9a0fffc86f","_kg_hide-input":true,"_uuid":"6a7daf4381ecb6aa45cb6578cc3acc070011248a"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"## Summary\n\nSo if we take into consideration **all assumptions mentioned above** and assume that our model (which OOF we used here) quiet stable and scores 0.284 on public we can expect private score between 0.285 and 0.287 (from -0.001 to -0.003), which is literally speaking \"shake UP\", not \"shake DOWN\" of scores. \n\nSo that is quite interesting conclusion and what needed to be mentioned that this method is truely naive (and maybe, misleading) and used several assumptions, which can be violated in real train-test setting.\n\nHope this notebook will help you guys. If you have any comments or remarks feel free to write them below.","metadata":{"_cell_guid":"0719f6a5-acef-4652-a656-d7f628ef61ec","_uuid":"c4bbbe009c52675e188d3e808a713e64e620349d"},"cell_type":"markdown"}],"nbformat_minor":1,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","nbconvert_exporter":"python","version":"3.6.3","name":"python","file_extension":".py","pygments_lexer":"ipython3","codemirror_mode":{"version":3,"name":"ipython"}}}}