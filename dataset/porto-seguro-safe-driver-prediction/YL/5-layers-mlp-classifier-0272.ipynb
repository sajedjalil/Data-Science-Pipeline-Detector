{"nbformat":4,"cells":[{"outputs":[],"metadata":{"collapsed":true,"_uuid":"cf57e98aa619eb82a530700313f13ff45d422e65","_cell_guid":"b687c75d-144d-478a-b400-28b8a09290b9"},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n# setup\n# -------------------------------------------------------------\n# system\nimport os\n# -------------------------------------------------------------\n# fundamental modules\nimport operator\nimport pandas as pd\nimport numpy as np\nfrom collections import namedtuple\nBatch = namedtuple('Batch', ['data'])\n\n# ds tools\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score, auc, precision_recall_curve, roc_curve, average_precision_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.utils import shuffle\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import MinMaxScaler\n\n# models\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\nimport sklearn.linear_model as lm\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nimport mxnet as mx\n\n# for gini\nfrom numba import jit\n\n# -------------------------------------------------------------\nimport gc\nimport logging\nimport sys\nroot_logger = logging.getLogger()\nstdout_handler = logging.StreamHandler(sys.stdout)\nroot_logger.addHandler(stdout_handler)\nroot_logger.setLevel(logging.DEBUG)\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\n#from subprocess import check_output\n#print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n# Any results you write to the current directory are saved as output.\n\n# =================================================================================\n# custom objective function (similar to auc)\ndef gini(y, pred):\n    g = np.asarray(np.c_[y, pred, np.arange(len(y)) ], dtype=np.float)\n    g = g[np.lexsort((g[:,2], -1*g[:,1]))]\n    gs = g[:,0].cumsum().sum() / g[:,0].sum()\n    gs -= (len(y) + 1) / 2.\n    return gs / len(y)\n\ndef gini_nn(y, pred):\n    return gini(y, pred) / gini(y, y)\n\n# -------------------------------------------------------------\n# for sklearn api\n@jit\ndef eval_gini(y_true, y_prob):\n    \"\"\"\n    Original author CPMP : https://www.kaggle.com/cpmpml\n    In kernel : https://www.kaggle.com/cpmpml/extremely-fast-gini-computation\n    \"\"\"\n    y_true = np.asarray(y_true)\n    y_true = y_true[np.argsort(y_prob)]\n    ntrue = 0\n    gini = 0\n    delta = 0\n    n = len(y_true)\n    for i in range(n-1, -1, -1):\n        y_i = y_true[i]\n        ntrue += y_i\n        gini += y_i * delta\n        delta += 1 - y_i\n    gini = 1 - 2 * gini / (ntrue * (n - ntrue))\n    return gini\n\ndef gini_xgb_sklearn_api(preds, dtrain):\n    labels = dtrain.get_label()\n    gini_score = eval_gini(labels, preds)\n    return [('gini', gini_score)]\n","execution_count":1},{"outputs":[],"metadata":{"collapsed":true,"_uuid":"6695c59350b1d4563dd4316c13f969023f953ea1","_cell_guid":"856de70d-1d7e-4bba-8645-ba17b562fbd8"},"cell_type":"code","source":"# read in files\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\n# basic understanding of the dataset\nprint (\"Dimension of train data {}\".format(train.shape))\nprint (\"Dimension of test data {}\".format(test.shape))","execution_count":2},{"cell_type":"raw","source":"====================================================================================\nWork with features\ncredit to:\nHeads or Tails's Steering Wheel of Fortune - Porto Seguro EDA notebook\nCam Nugent's deep neural network: insurance claims (~0.268)","metadata":{"_uuid":"72170f02d8472c92c57938954a4354cba1ea6e52","_cell_guid":"e729daae-b40e-442f-9c3c-3da55329027d"}},{"outputs":[],"metadata":{"collapsed":true,"_uuid":"a030275f1114dc879071d4bcc6bfcbc6babaa99e","_cell_guid":"b9827ae7-dcd9-4136-a0e9-662121c9f802"},"cell_type":"code","source":"# list columns that have missing values\nfor col in train.columns:\n    null_sum = (train[col] == -1).sum()\n    if null_sum > 0:\n        print('feature %s has %i missing entries' %(col, null_sum))","execution_count":3},{"outputs":[],"metadata":{"collapsed":true,"_uuid":"7a8e06adc8aed0343383f760a07d371311113d0e","_cell_guid":"2f526b16-aaab-4fbc-8ae9-7fbbc89bdf53"},"cell_type":"code","source":"# olivier's feature selection and \nfeat_olivier_peatle = [\n    \"ps_car_13\",    # xgb #1  lgb #1\n    \"ps_reg_03\",    # xgb #2  lgb #2\n    \"ps_ind_03\",    # xgb #3  lgb #3\n    \"ps_car_14\",    # xgb #4  lgb #5\n    \"ps_ind_15\",    # xgb #5  lgb #4\n    \"ps_reg_02\",    # xgb #6  lgb #8\n    \"ps_ind_05_cat\",# xgb #7  lgb #6\n    \"ps_ind_01\",    # xgb #8  lgb #9\n    \"ps_car_11_cat\",# xgb #9  lgb #10\n    \"ps_car_01_cat\",# xgb #10 lgb #11\n    \"ps_reg_01\",    # xgb #11 lgb #7\n    \"ps_car_12\",    # xgb #12 lgb #15\n    \"ps_car_15\",    # xgb #13 lgb #12\n    \"ps_car_06_cat\",# xgb #14 lgb #13\n    \"ps_car_09_cat\",# xgb #15 lgb #19\n    \"ps_ind_02_cat\",# xgb #16 lgb #18\n    \n    \"ps_car_07_cat\",# xgb #18 lgb #16\n    \"ps_car_11\",    # xgb #19 lgb #20\n    \"ps_car_03_cat\",# xgb #20 lgb #17\n    \"ps_ind_04_cat\",# xgb #21 lgb #24\n    \"ps_car_04_cat\",# xgb #22 lgb #25\n\n    \n    'ps_car_05_cat',# xgb #27 lgb #27\n    \n    \"ps_car_02_cat\",# xgb #29 lgb #29\n    \"ps_car_08_cat\",# xgb #30 lgb #30\n    \n    \n    'ps_car_10_cat',# xgb #33 lgb #34\n    \n    \n\n    'ps_ind_10_bin', 'ps_ind_11_bin', 'ps_ind_12_bin', 'ps_ind_13_bin', # these three sums to ps_ind_14, so ps_ind_14 removed\n    'ps_ind_06_bin', 'ps_ind_07_bin', 'ps_ind_08_bin', 'ps_ind_09_bin', # these four sums to 1\n    'ps_ind_16_bin', 'ps_ind_17_bin', 'ps_ind_18_bin', #If one of ps_ind_16_bin, ps_ind_17_bin and ps_ind_18_bin is 1, then the others are 0, but they can all be 0\n]","execution_count":4},{"outputs":[],"metadata":{"collapsed":true,"_uuid":"77b02a338c9cc9f10364743dc27bae65b10b4791","_cell_guid":"437c8ba7-dd33-42c7-86cc-c4e1d7ce8c9a"},"cell_type":"code","source":"# data preparation for training\n\n# drop id and target column\ntrain_y = train['target']\ntrain_x = train.drop(['id', 'target'], axis = 1)\n\n# prepare testing dataset\ntest_x = test.drop(['id'], axis = 1)#test[feat_olivier_peatle]","execution_count":5},{"outputs":[],"metadata":{"collapsed":true,"_uuid":"2ae766197cd59f450e9883863328a50a309d576b","_cell_guid":"5fee528e-d5e5-4887-9c14-94ee31e720a7"},"cell_type":"code","source":"merged_dat = pd.concat([train_x, test_x], axis = 0)\nprint(merged_dat.shape)","execution_count":6},{"outputs":[],"metadata":{"collapsed":true,"_uuid":"4e2497d0e4b70372c77132246abf699741c91406","_cell_guid":"0d30e296-b3e0-4b61-a1ad-bea0b0cd7207"},"cell_type":"code","source":"# add feature \"https://www.kaggle.com/headsortails/steering-wheel-of-fortune-porto-seguro-eda\"\n# Number of NAs per ID\nmerged_dat['nona_calc_'] = (merged_dat == -1).sum(axis = 1)\n# Sum of binary features\nclmn_fltr = [col for col in merged_dat.columns if 'ind' in col and 'bin' in col]\nmerged_dat['sumbin_calc_'] = merged_dat[clmn_fltr].sum(axis = 1)\n# Difference measure for binary features\nfltr_df = merged_dat[clmn_fltr]\ndiff_df = fltr_df.apply(lambda x: x - fltr_df.iloc[0, :], axis = 1).fillna(0)\nmerged_dat['diffbin_calc_'] = diff_df.sum(axis = 1)","execution_count":7},{"outputs":[],"metadata":{"collapsed":true,"_uuid":"567d8b2b25b0f6f2181f366da567a4602f9fd6db","_cell_guid":"8f636c50-6fa6-4a70-9f6b-601ddbeecd42"},"cell_type":"code","source":"merged_dat = merged_dat[feat_olivier_peatle + ['nona_calc_', 'sumbin_calc_', 'diffbin_calc_']]","execution_count":8},{"outputs":[],"metadata":{"collapsed":true,"_uuid":"667b6476b210b9af5f236211d1f6c7b44f2558d0","_cell_guid":"ca627216-8a24-4260-ab05-6c14bb3bc256"},"cell_type":"code","source":"#change data to float32\nfor c, dtype in zip(merged_dat.columns, merged_dat.dtypes): \n    if dtype == np.float64:     \n        merged_dat[c] = merged_dat[c].astype(np.float32)","execution_count":9},{"outputs":[],"metadata":{"collapsed":true,"_uuid":"2f9b2e76c9eb5009aaf669df8fe19f069b6bc48c","_cell_guid":"867980a3-ed2e-431d-9e8d-cecc7c2494e2"},"cell_type":"code","source":"#one hot encode the categoricals\ncat_features = [col for col in merged_dat.columns if col.endswith('cat')]\n# count ind (no cat nor bin) column as categoricals\ncat_features = cat_features + \\\n               [col for col in merged_dat.columns if 'ind' in col and 'cat' not in col and 'bin' not in col]\nfor column in cat_features:\n    temp = pd.get_dummies(pd.Series(merged_dat[column]))\n    merged_dat = pd.concat([merged_dat, temp],axis=1)\n    merged_dat.drop([column], axis=1, inplace = True)","execution_count":10},{"outputs":[],"metadata":{"collapsed":true,"_uuid":"35b3f0069794dab570f368e0510cba25460f3f55","_cell_guid":"b49cc59a-87f0-4140-ad8a-98f06b05fbf4"},"cell_type":"code","source":"merged_dat.replace(-1, 0, inplace = True)","execution_count":11},{"outputs":[],"metadata":{"collapsed":true,"_uuid":"38ebcc71e49dce50d5b0dfa995578f1fb9127321","_cell_guid":"ad1733e2-e351-4c5f-a9bb-e9d2d6d0c81e"},"cell_type":"code","source":"# normailise the scale of the numericals\nscaler = MinMaxScaler()\nmerged_dat = scaler.fit_transform(merged_dat)","execution_count":12},{"outputs":[],"metadata":{"collapsed":true,"_uuid":"91a9ba57e84f9c2a68d57fdef4d9f8484e557544","_cell_guid":"c94a7348-3ff1-4996-911d-9d444f6298ba"},"cell_type":"code","source":"train_X = merged_dat[:train_x.shape[0]]\ntest_X = merged_dat[train_x.shape[0]:]","execution_count":13},{"cell_type":"markdown","source":"====================================================================================\nMxnet MLP","metadata":{"_uuid":"72170f02d8472c92c57938954a4354cba1ea6e52","_cell_guid":"e729daae-b40e-442f-9c3c-3da55329027d"}},{"cell_type":"markdown","source":"**Please run this notebook locally as it has to save model checkpoints and the best epoch is selected manually for prediction**","metadata":{"_uuid":"3881cd0e906a0e3dba68fbe25d320399a485acaa","_cell_guid":"3e93f752-d28b-4f95-8df5-6c1fe6bf1b5d"}},{"outputs":[],"metadata":{"collapsed":true,"_uuid":"96f2811bf638befe5caa9c09421210926900ad4d","_cell_guid":"bc9cd9b0-36a7-4231-a414-785bacd5090a"},"cell_type":"code","source":"# ------------------------------------------------\ngc.collect()\n\n# process data\nX = train_X\ny = train_y.values\n\n# k fold training\nk = 5\nskf = StratifiedKFold(n_splits = k, random_state = 0, shuffle  = True)\n\n# ------------------------------------------------\n# setup neural mlp model\ndef get_mlp(n_in):\n    \"\"\"\n    multi-layer perceptron\n    \"\"\"\n    n_out = 1\n    data = mx.symbol.Variable('data')\n    fc1  = mx.symbol.FullyConnected(data = data, name='fc1', num_hidden = n_in)\n    act1 = mx.symbol.Activation(data = fc1, name = 'relu1', act_type = \"relu\")\n    fc2  = mx.symbol.FullyConnected(data = act1, name = 'fc2', num_hidden = 3 * int((n_in - n_out) / 4 // 1 + n_out))\n    act2 = mx.symbol.Activation(data = fc2, name = 'relu2', act_type = \"relu\")\n    fc3  = mx.symbol.FullyConnected(data = act2, name = 'fc3', num_hidden = 2 * int((n_in - n_out) / 4 // 1 + n_out))\n    act3 = mx.symbol.Activation(data = fc3, name = 'relu3', act_type = \"relu\")\n    fc4  = mx.symbol.FullyConnected(data = act3, name = 'fc4', num_hidden = int((n_in - n_out) / 4 // 1 + n_out))\n    act4 = mx.symbol.Activation(data = fc4, name = 'relu4', act_type = \"relu\")\n    fc5  = mx.symbol.FullyConnected(data = act4, name = 'fc5', num_hidden = n_out)\n    mlp  = mx.symbol.LogisticRegressionOutput(data = fc5, name = 'softmax')\n    return mlp\n\noptimizer_params = {'learning_rate': 0.1, \n                    'momentum' : 0.9,\n                    'wd' : 0.0001, \n                    'lr_scheduler': mx.lr_scheduler.FactorScheduler(step = 1000, factor = 0.5)}\n\ngini_metric = mx.metric.create(gini_nn)\n\nprint('---training using multi layer perception---')\nfor i, (train_index, valid_index) in enumerate(skf.split(X, y)):\n    print('=====================================================')\n    print('kfold learning: {}  of  {} : '.format(i + 1, k))\n    \n    # process data\n    X_train, X_valid = X[train_index], X[valid_index]\n    y_train, y_valid = y[train_index], y[valid_index]\n    \n    # apply upsampling using olivier's approach: XGB classifier, upsampling LB 0.283\n    upsampling = False\n    if upsampling:\n        X_train = np.append(X_train, X_train[np.where(y_train == 1)[0], :], axis = 0)\n        y_train = np.append(y_train, y_train[np.where(y_train == 1)[0]], axis = 0)\n        X_train, y_train = shuffle(X_train, y_train)\n    \n    # mxnet ndarray\n    mx_train = mx.io.NDArrayIter(X_train, y_train, batch_size = 50000)\n    mx_valid = mx.io.NDArrayIter(X_valid, y_valid, batch_size = 50000)\n    #---------------------------------\n    model_mlp = mx.mod.Module(symbol = get_mlp(X.shape[1]), context = mx.gpu())\n    model_mlp.fit(mx_train,\n                  num_epoch          = 400,\n                  optimizer          = 'sgd',\n                  eval_data          = mx_valid,\n                  eval_metric        = gini_metric,\n                  optimizer_params   = optimizer_params,\n                  initializer        = mx.init.Xavier(rnd_type='gaussian', factor_type=\"in\", magnitude=2),\n                  epoch_end_callback = mx.callback.do_checkpoint('fold_' + str(i), 10),\n                 )","execution_count":16},{"outputs":[],"metadata":{"collapsed":true,"_uuid":"9dc6788534bd7d4ff17bca3c9e5264f396a6523b","_cell_guid":"8471f424-985f-46e0-8994-fc7263287366"},"cell_type":"code","source":"# collate results\nfold_pred_collator = pd.DataFrame()\n\n# setup prediction\nbest_epoch = [170, 150, 120, 150, 150]\nfor i in range(0, 5):\n    print('generating prediction using fold # %i model.' %i)\n    sym, arg_params, aux_params = mx.model.load_checkpoint('fold_' + str(i), best_epoch[i])\n    mdl_pred = mx.mod.Module(symbol = sym, context = mx.gpu(), label_names = None)\n    mdl_pred.bind(for_training = False, data_shapes = [('data', test_X.shape)], label_shapes = None)\n    mdl_pred.set_params(arg_params, aux_params, allow_missing=True)\n    mdl_pred.forward(Batch([mx.nd.array(test_X)]))\n    fold_pred_collator['fold_' + str(i)] = mdl_pred.get_outputs()[0].asnumpy().squeeze()\n    ","execution_count":35},{"outputs":[],"metadata":{"collapsed":true,"_uuid":"1e1989a093f4c51337eba0205e5a68eb256cb554","_cell_guid":"369a4068-cf87-436b-acf7-dffd0d85f7e8"},"cell_type":"code","source":"sbmtn_result_df = test['id'].to_frame()\nsbmtn_result_df['target'] = fold_pred_collator.mean(axis = 1)","execution_count":41},{"outputs":[],"metadata":{"collapsed":true,"_uuid":"7a499111fd2b79b4da7a46402c7279539aca39ac","_cell_guid":"2edabc48-b86d-4bf0-9c77-89fead56dbb9"},"cell_type":"code","source":"sbmtn_result_df.to_csv('nn_submission.csv', index = False)","execution_count":42}],"nbformat_minor":1,"metadata":{"language_info":{"pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"mimetype":"text/x-python","file_extension":".py","nbconvert_exporter":"python","name":"python","version":"3.6.3"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}}}