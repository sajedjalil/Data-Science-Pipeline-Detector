{"cells":[{"metadata":{"_uuid":"61e6abb3027184b728050e2bf6b51925db8abdc6","_cell_guid":"3637caa0-ccaf-440d-93a4-d4554501be74"},"outputs":[],"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running thiimport numpy as np \nimport pandas as pd\nfrom pandas import Series, DataFrame \nfrom tensorflow.python.framework import ops\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport math\nimport sklearn.metrics as skm\n%matplotlib inline\nfrom sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\nplt.rcParams['figure.figsize'] = (7.0, 4.0) # set default size of plots\nplt.rcParams['image.interpolation'] = 'nearest'\nplt.rcParams['image.cmap'] = 'gray'\nfrom sklearn.linear_model import LogisticRegression\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":1},{"metadata":{"_uuid":"fd79ccaf2d9b40caea038a3ade80321e3064fa78","_cell_guid":"bd32dd47-bf47-4199-9989-9f90590d7c03"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"c64a9555323f185d67fc90b703dacb235ff2aabb","_cell_guid":"36703564-d313-4cb3-a356-3ab21b300793"},"cell_type":"markdown","source":"Since the data iteslf doesn't tell us about the features.  We will use Autoencoders to build features of importance and use them using logistic regression. We will see the jump in accuracy by using the Autoencoder generated features. Autoencoder will pass the features of X(currently 198) to a neual netwok layer of nodes= 100, thus shinking features by 100 and also those features would contain the core features of training dataset."},{"metadata":{"_uuid":"33710f4ae2f921b64348fbb6670a12828d7a34ec","collapsed":true,"_cell_guid":"34044484-cd76-4442-8cae-aff953533250"},"outputs":[],"cell_type":"code","source":"#Load the train and test csv files.\ntrain = pd.read_csv('../input/testcsv/train.csv')\ntest = pd.read_csv('../input/testcsv/test.csv')","execution_count":3},{"metadata":{"_uuid":"5d8634a9a466506a3614058868836345930fae41","_cell_guid":"fee38698-0da1-4777-98ea-bb05c856adbc"},"outputs":[],"cell_type":"code","source":"# Preprocessing Step taken by Forca\nid_test = test['id'].values\ntarget_train = train['target'].values\ntrain = train.drop(['target','id'], axis = 1)\ntest = test.drop(['id'], axis = 1)\ncol_to_drop = train.columns[train.columns.str.startswith('ps_calc_')]\ntrain = train.drop(col_to_drop, axis=1)\ntest = test.drop(col_to_drop, axis=1)  \ntrain = train.replace(-1, np.nan)\ntest = test.replace(-1, np.nan)\ncat_features = [a for a in train.columns if a.endswith('cat')]\nfor column in cat_features:\n    temp = pd.get_dummies(pd.Series(train[column]))\n    train = pd.concat([train,temp],axis=1)\n    train = train.drop([column],axis=1)\nfor column in cat_features:\n    temp = pd.get_dummies(pd.Series(test[column]))\n    test = pd.concat([test,temp],axis=1)\n    test = test.drop([column],axis=1)\ntest=test.fillna(method='pad')\ntrain=train.fillna(method='pad')\nprint(train.values.shape, test.values.shape)\nprint(target_train.shape)","execution_count":4},{"metadata":{"_uuid":"5c9db3f245921fb78da8869e1258cf870f5e367c","_cell_guid":"6bb93a28-cbd8-41c4-9c34-257c91f387a0"},"outputs":[],"cell_type":"code","source":"#Function to evaluate the Gini Coefficients\ndef eval_gini(y_true, y_prob):\n    y_true = np.asarray(y_true)\n    y_true = y_true[np.argsort(y_prob)]\n    ntrue = 0\n    gini = 0\n    delta = 0\n    n = len(y_true)\n    for i in range(n-1, -1, -1):\n        y_i = y_true[i]\n        ntrue += y_i\n        gini += y_i * delta\n        delta += 1 - y_i\n    gini = 1 - 2 * gini / (ntrue * (n - ntrue))\n    return gini\n\n\n#Applying vanila logistic regression.\nfrom sklearn.cross_validation import train_test_split\nx_train ,x_test, y_train, y_test = train_test_split(train, target_train,test_size=0.3)\n\nlog_model=LogisticRegression()\nlog_model.fit(x_train,y_train)\npred_log = log_model.predict_proba(x_test)[:,1]\nprint( \"Gini log  Test= \", eval_gini(y_test, pred_log))","execution_count":8},{"metadata":{"_uuid":"4f2781cc2f0d0b68e964634dc5706d3f5005b851","_cell_guid":"2ed23b69-b7dd-4ad4-803b-34a7c681a767"},"cell_type":"markdown","source":"We can see that vanila logistic regression doesnt perform well on the dataset. For this we need to find the features. autoencoder has the ability to provide the non linear core principle dimension of the data. Lets build the autoencoder. Most of the parts are learned from Andrew Ng's deep learning specialization."},{"metadata":{"_uuid":"c0639cb9f17e9e3ca8db40343daf513f10f545db","collapsed":true,"_cell_guid":"d5afd898-69de-4aa3-a0c7-e718bb4416ac"},"outputs":[],"cell_type":"code","source":"#Mini Batches Generation for TensorFlow. Lots of code ahead, don't worry, its just helper functions.\ndef random_mini_batches(X, Y, mini_batch_size = 1024, seed = 0):\n    np.random.seed(seed)            \n    m = X.shape[1]                  \n    mini_batches = []\n    \n    permutation = list(np.random.permutation(m))\n    shuffled_X = X[:, permutation]\n    shuffled_Y = Y[:, permutation]\n    \n    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n    for k in range(0, num_complete_minibatches):\n        mini_batch_X = shuffled_X[:, k*mini_batch_size : mini_batch_size*(k+1)]\n        mini_batch_Y = shuffled_Y[:,mini_batch_size*k : mini_batch_size*(k+1)]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    if m % mini_batch_size != 0:\n        mini_batch_X = shuffled_X[:, mini_batch_size*num_complete_minibatches : m]\n        mini_batch_Y = shuffled_Y[:, mini_batch_size*num_complete_minibatches : m]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    return mini_batches","execution_count":9},{"metadata":{"_uuid":"2209bfce429bc62895a45901f414929bbe914125","collapsed":true,"_cell_guid":"836e2dd6-aa21-498f-8bc9-dad8a2637809"},"outputs":[],"cell_type":"code","source":"# creating placeholders for TensorFlow\ndef create_placeholders(n_x, n_y):\n    X = tf.placeholder(dtype=\"float\", shape=(n_x, None), name='X')\n    Y = tf.placeholder(dtype=\"float\", shape=(n_y, None), name='Y')\n    return X, Y","execution_count":10},{"metadata":{"_uuid":"f3de9014bce2b6dfbb7bfa5c729ca2f04db644d3","collapsed":true,"_cell_guid":"f9b0470e-2181-4dae-b0b7-4b2025229b42"},"outputs":[],"cell_type":"code","source":"# initialize_parameters\ntf.reset_default_graph()\ndef initialize_parameters(f1=198, f2=100, f3=50):\n    tf.set_random_seed(1)  \n    W1 = tf.get_variable(\"W1\", [f2,f1], initializer = tf.contrib.layers.xavier_initializer(seed=1))\n    b1 = tf.get_variable('b1', [f2,1], initializer= tf.zeros_initializer())\n    W2 = tf.get_variable(\"W2\", [f3,f2], initializer = tf.contrib.layers.xavier_initializer(seed=1))\n    b2 = tf.get_variable('b2', [f3,1], initializer= tf.zeros_initializer())\n    W3 = tf.get_variable('W3', [f2,f3], initializer=tf.contrib.layers.xavier_initializer(seed=1))\n    b3 = tf.get_variable('b3', [f2,1], initializer= tf.zeros_initializer())\n    W4 = tf.get_variable('W4', [f1,f2], initializer=tf.contrib.layers.xavier_initializer(seed=1))\n    b4 = tf.get_variable('b4', [f1,1], initializer= tf.zeros_initializer())\n    parameters = {\"W1\": W1,\n                  \"b1\": b1,\n                  \"W2\": W2,\n                  \"b2\": b2,\n                  \"W4\": W4,\n                  \"b4\": b4,\n                  \"W3\": W3,\n                  \"b3\": b3}\n    \n    return parameters","execution_count":11},{"metadata":{"_uuid":"f62767b5ff9c13409fa2ec93caa7972ba885e282","collapsed":true,"_cell_guid":"d24f70dc-abed-4d03-ae22-e68c011f7d7d"},"outputs":[],"cell_type":"code","source":"#Forward Prop steps for tensorflow\ndef forward_propagation(X, parameters):\n    W1 = parameters['W1']\n    b1 = parameters['b1']\n    W2 = parameters['W2']\n    b2 = parameters['b2']\n    W3 = parameters['W3']\n    b3 = parameters['b3']\n    W4 = parameters['W4']\n    b4 = parameters['b4']\n    \n    Z1 = tf.add(tf.matmul(W1, X), b1)                                  # Z1 = np.dot(W1, X) + b1\n    A1 = tf.nn.tanh(Z1)\n    Z2 = tf.add(tf.matmul(W2, A1), b2)                                  # Z1 = np.dot(W1, X) + b1\n    A2 = tf.nn.relu(Z2)# A1 = relu(Z1)                                              # A2 = relu(Z2)\n    Z3 = tf.add(tf.matmul(W3, A2), b3)                           # Z3 = np.dot(W3,Z2) + b3\n    A3=  tf.nn.tanh(Z3)\n    Z4 = tf.add(tf.matmul(W4, A3), b4)                           # Z3 = np.dot(W3,Z2) + b3\n    A4=  tf.nn.relu(Z4)\n    return A4, A2\n","execution_count":12},{"metadata":{"_uuid":"043428805edd537a75cdebf41004135ea7c62e09","collapsed":true,"_cell_guid":"bbcc0cbe-5253-4a3c-8754-7a76d6b8e0ea"},"outputs":[],"cell_type":"code","source":"#Cost computation for tensorflow\ndef compute_cost(A4, Y, parameters):\n    m = Y.shape[1]\n    W1 = parameters['W1']\n    W2 = parameters['W2']\n    W3 = parameters['W3']\n    W4 = parameters['W4']\n    #Cost is the the difference of output with input.\n    cost = tf.reduce_mean(tf.pow(Y - A4, 2))\n    return cost","execution_count":13},{"metadata":{"_uuid":"87e29b41d77d586e3f15b7a1e158445973ffd7e3","collapsed":true,"_cell_guid":"0a81beb6-53dd-400c-b3f0-23f9deb21807"},"outputs":[],"cell_type":"code","source":"#Final AE Model\ndef model(X_train, Y_train, f_2, f_3, learning_rate,\n          num_epochs , minibatch_size , print_cost):\n    ops.reset_default_graph()                         \n    tf.set_random_seed(1)                             \n    seed = 3                                          \n    (n_x, m) = X_train.shape \n    n_y = Y_train.shape[0]                            \n    costs = []                                        \n    X, Y = create_placeholders(n_x, n_y)\n    parameters = initialize_parameters(f2=f_2,f3=f_3)\n    A4, A2 = forward_propagation(X, parameters)\n    cost = compute_cost(A4, Y, parameters)\n    #Tensorflow optimizer\n    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n    init = tf.global_variables_initializer()\n    with tf.Session() as sess:\n        sess.run(init) \n        # Do the training loop\n        for epoch in range(num_epochs):\n            epoch_cost = 0.                      \n            num_minibatches = int(m / minibatch_size)\n            seed = seed + 1\n            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n            for minibatch in minibatches:\n                # Select a minibatch\n                (minibatch_X, minibatch_Y) = minibatch\n                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n                epoch_cost += minibatch_cost / num_minibatches\n\n            # Print the cost every epoch\n            if print_cost == True:\n                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n            if print_cost == True:\n                costs.append(epoch_cost)\n                \n        plt.plot(np.squeeze(costs))\n        plt.ylabel('cost')\n        plt.xlabel('iterations (per tens)')\n        plt.title(\"Learning rate =\" + str(learning_rate))\n        plt.show()\n        parameters = sess.run(parameters)\n        return parameters, A2","execution_count":14},{"metadata":{"_uuid":"e63782f4060c374159c4e3daf3d58dca69cb89a5","collapsed":true,"_cell_guid":"d6216be5-3eec-407f-8145-02dbb95da08d"},"outputs":[],"cell_type":"code","source":"train=train.as_matrix()\ntest=test.as_matrix()","execution_count":15},{"metadata":{"_uuid":"7b1141fc36f6a564893cad25d2550f508255ab94","_cell_guid":"f8881d2d-6c3b-4793-8e0c-6f017c35a568"},"outputs":[],"cell_type":"code","source":"#Run the MODEL\nparameters, _ = model(train.T,train.T,160,100,minibatch_size=512,num_epochs=10, learning_rate=0.001, print_cost=True)","execution_count":16},{"metadata":{"_uuid":"b59f99108f864c486c9974ad94e498296b1f625e","_cell_guid":"bdab0529-2454-4d9e-822e-02e64a7255ac"},"cell_type":"markdown","source":"The cost here is the reconstruction cost of y using reconstructed using this NN.\nWe can see the cost converging when we train the Autoencoder to X with X itself. The middle layer which has the size of 130 nodes. So the final feature count would be 130. We call them A2 featuers and these features will be a input to Logistic regression model. And we will see improved accuracy."},{"metadata":{"_uuid":"176df102261fc36a5f6c4f16230eeac7ae05ba77","collapsed":true,"_cell_guid":"5c535d4a-b40d-4556-bce8-bd1e5ee12e4a"},"outputs":[],"cell_type":"code","source":"#Helper functions to calculate the features using parameters\n\ndef forward_propagationout(X, parameters):\n    # retrieve parameters\n    W1 = parameters['W1']\n    b1 = parameters['b1']\n    W2 = parameters['W2']\n    b2 = parameters['b2']\n    W3 = parameters['W3']\n    b3 = parameters['b3']\n    W4 = parameters['W4']\n    b4 = parameters['b4']\n    \n    Z1 = np.dot(W1, X) + b1                                  # Z1 = np.dot(W1, X) + b1\n    A1 = np.tanh(Z1)\n    Z2 = np.dot(W2, A1) + b2                                   # Z1 = np.dot(W1, X) + b1\n    A2 = relu(Z2)                                             # A2 = relu(Z2)\n    Z3 = np.dot(W3, A2) + b3                           # Z3 = np.dot(W3,Z2) + b3\n    A3=  np.tanh(Z3)\n    Z4 = np.dot(W4, A3) + b4                            # Z3 = np.dot(W3,Z2) + b3\n    A4=  relu(Z4)\n    return A2\ndef relu(x):\n    s = np.maximum(0,x)\n    return s\n\n","execution_count":17},{"metadata":{"_uuid":"e2ff35c4f687305c462399f5174fd07e1a3e942e","_cell_guid":"20b0e697-6025-4bb7-bb7e-5980be1f64d8"},"outputs":[],"cell_type":"code","source":"#A2 featues from the middle layer of AE\nA2=forward_propagationout(train.T.astype('float32'), parameters)\nx_train ,x_test, y_train, y_test = train_test_split(A2.T, target_train,test_size=0.3)\nlog_model=LogisticRegression()\nlog_model.fit(x_train,y_train)\npred_log = log_model.predict_proba(x_test)[:,1]\nprint( \"Gini log  Test= \", eval_gini(y_test, pred_log))","execution_count":18},{"metadata":{"_uuid":"95681e4c9f62d3634d009e8cae3d9432607b0034","collapsed":true,"_cell_guid":"10a959f5-8c86-417b-8d83-933296eae1cc"},"cell_type":"markdown","source":"I hope you must have got the significance of Autoencoders by now, \nWe have just increased the **Test Gini index from 0.255 to 0.2678.** . This is a drastic improvement.\nThis happened because of better features created by AE.\nThough this increase is not going to help you score better rank. But its difficult to create featues when no information is given and specifically in this case only Autoencoders are here to help you. You can reduce the amount of code using Keras.\nYou can now stack this Logistic regession model to any other model to get better rank.  If needs, I can provide the parameters file to create those features and do the regression.\nThanks for reading."}],"nbformat":4,"metadata":{"language_info":{"pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.3","file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat_minor":1}