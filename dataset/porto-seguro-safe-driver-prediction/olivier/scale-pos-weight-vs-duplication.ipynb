{"nbformat_minor":1,"cells":[{"cell_type":"markdown","metadata":{"_uuid":"adf773f9b1c57bd6f2a7d483206a4cd5c1c1758d","_cell_guid":"da6aa26a-7972-488f-ac14-646ffdb70c62"},"source":"The purpose of this notebook is to check the effect of both scale_pos_weight and duplication on a LightGBM classifier."},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"037573c3943c5c71f471ee7a2cc9faa2583ea142","_cell_guid":"a924553d-bcf2-429a-8f9d-1f61fecf0da9","collapsed":true},"outputs":[],"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import StratifiedKFold\nimport gc\nfrom numba import jit\nfrom sklearn.preprocessing import LabelEncoder\nimport time \nfrom sklearn.metrics import confusion_matrix, f1_score\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n%matplotlib inline\nimport matplotlib.gridspec as gridspec\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_curve\nimport itertools\nimport math\nnp.set_printoptions(precision=3)\n"},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"9a4d445da3a2b2e5e479ab8ead5e5c8ca00ef7d8","_cell_guid":"76893dbf-a616-44d4-b988-adab98425eff","collapsed":true},"outputs":[],"source":"@jit  # for more info please visit https://numba.pydata.org/\ndef eval_gini(y_true, y_prob):\n    \"\"\"\n    Original author CMPM \n    https://www.kaggle.com/cpmpml/extremely-fast-gini-computation\n    \"\"\"\n    y_true = np.asarray(y_true)\n    y_true = y_true[np.argsort(y_prob)]\n    ntrue = 0\n    gini = 0\n    delta = 0\n    n = len(y_true)\n    for i in range(n-1, -1, -1):\n        y_i = y_true[i]\n        ntrue += y_i\n        gini += y_i * delta\n        delta += 1 - y_i\n    gini = 1 - 2 * gini / (ntrue * (n - ntrue))\n    return gini\n\n# This is taken from sklearn examples and is available at\n# http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n"},{"cell_type":"markdown","metadata":{"_uuid":"03c05e2f6ae6aaef9fe0adfbb2033b7d56493001","_cell_guid":"51a49415-dcff-41d5-bd89-2da96e433f1b"},"source":"### Get the data and reduce the features \nFeature selection is done according to kernel \nhttps://www.kaggle.com/ogrellier/noise-analysis-of-porto-seguro-s-features \n"},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"401c253e8901e414057f1da1db5ff43ea55bd5b5","_cell_guid":"db8286e3-f398-4e39-9155-17e539d00153","collapsed":true},"outputs":[],"source":"trn_df = pd.read_csv(\"../input/train.csv\", index_col=0)\ntarget = trn_df[\"target\"]\ndel trn_df[\"target\"]\n\ntrain_features = [\n    \"ps_car_13\",  #            : 1571.65 / shadow  609.23\n\t\"ps_reg_03\",  #            : 1408.42 / shadow  511.15\n\t\"ps_ind_05_cat\",  #        : 1387.87 / shadow   84.72\n\t\"ps_ind_03\",  #            : 1219.47 / shadow  230.55\n\t\"ps_ind_15\",  #            :  922.18 / shadow  242.00\n\t\"ps_reg_02\",  #            :  920.65 / shadow  267.50\n\t\"ps_car_14\",  #            :  798.48 / shadow  549.58\n\t\"ps_car_12\",  #            :  731.93 / shadow  293.62\n\t\"ps_car_01_cat\",  #        :  698.07 / shadow  178.72\n\t\"ps_car_07_cat\",  #        :  694.53 / shadow   36.35\n\t\"ps_ind_17_bin\",  #        :  620.77 / shadow   23.15\n\t\"ps_car_03_cat\",  #        :  611.73 / shadow   50.67\n\t\"ps_reg_01\",  #            :  598.60 / shadow  178.57\n\t\"ps_car_15\",  #            :  593.35 / shadow  226.43\n\t\"ps_ind_01\",  #            :  547.32 / shadow  154.58\n\t\"ps_ind_16_bin\",  #        :  475.37 / shadow   34.17\n\t\"ps_ind_07_bin\",  #        :  435.28 / shadow   28.92\n\t\"ps_car_06_cat\",  #        :  398.02 / shadow  212.43\n\t\"ps_car_04_cat\",  #        :  376.87 / shadow   76.98\n\t\"ps_ind_06_bin\",  #        :  370.97 / shadow   36.13\n\t\"ps_car_09_cat\",  #        :  214.12 / shadow   81.38\n\t\"ps_car_02_cat\",  #        :  203.03 / shadow   26.67\n\t\"ps_ind_02_cat\",  #        :  189.47 / shadow   65.68\n\t\"ps_car_11\",  #            :  173.28 / shadow   76.45\n\t\"ps_car_05_cat\",  #        :  172.75 / shadow   62.92\n\t\"ps_calc_09\",  #           :  169.13 / shadow  129.72\n\t\"ps_calc_05\",  #           :  148.83 / shadow  120.68\n\t\"ps_ind_08_bin\",  #        :  140.73 / shadow   27.63\n\t\"ps_car_08_cat\",  #        :  120.87 / shadow   28.82\n\t\"ps_ind_09_bin\",  #        :  113.92 / shadow   27.05\n\t\"ps_ind_04_cat\",  #        :  107.27 / shadow   37.43\n\t\"ps_ind_18_bin\",  #        :   77.42 / shadow   25.97\n\t\"ps_ind_12_bin\",  #        :   39.67 / shadow   15.52\n\t\"ps_ind_14\",  #            :   37.37 / shadow   16.65\n]\n    \ntrn_df = trn_df[train_features]"},{"cell_type":"markdown","metadata":{"_uuid":"d5ff6720cee65bd1eb5e4189433a1c29099086ee","_cell_guid":"5c407b57-95eb-4010-bddc-e83547fcfa8a"},"source":"### Let's have a look at the evolution of predictions with scale_pose_weight"},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"ac4b44873f00fe25c295c75947fd41bd8b019309","_cell_guid":"2c0faf80-6ac5-49a6-af7c-7f6d213d0846","collapsed":true},"outputs":[],"source":"scale_pos_weights = range(1, 20, 2)\noof_proba = np.empty((len(trn_df), len(scale_pos_weights)))\noof_label = np.empty((len(trn_df), len(scale_pos_weights)))\nfor i_w, scale_pos_weight in enumerate(scale_pos_weights):\n    n_splits = 2 # That's enough to get a feel on what's going on\n    n_estimators = 100\n    folds = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=14) \n\n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(target, target)):\n        trn_dat, trn_tgt = trn_df.iloc[trn_idx], target.iloc[trn_idx]\n        val_dat, val_tgt = trn_df.iloc[val_idx], target.iloc[val_idx]\n\n        clf = LGBMClassifier(n_estimators=n_estimators,\n                             max_depth=-1,\n                             num_leaves=25,\n                             learning_rate=.1, \n                             subsample=.8, \n                             colsample_bytree=.8,\n                             min_split_gain=1,\n                             reg_alpha=0,\n                             reg_lambda=0,\n                             scale_pos_weight=scale_pos_weight, # <= We do not overweight positive samples\n                             n_jobs=2)\n\n        clf.fit(trn_dat, trn_tgt, \n                eval_set=[(trn_dat, trn_tgt), (val_dat, val_tgt)],\n                eval_metric=\"auc\",\n                early_stopping_rounds=None,\n                verbose=False)\n\n        oof_proba[val_idx, i_w] = clf.predict_proba(val_dat)[:, 1]\n        oof_label[val_idx, i_w] = clf.predict(val_dat)\n        \n    print(\"Full OOF score : %.6f for scale_pos_weight = %2d\" \n          % (eval_gini(target, oof_proba[:, i_w]), scale_pos_weight))"},{"cell_type":"markdown","metadata":{"_uuid":"f11adf537b8683dd446ca169dce96751e956eb19","_cell_guid":"3b75729c-2d32-48ac-8e3a-cb23d0b27f6b"},"source":"### Now let's look at the evolution of f1_scores against scale_pos_weight\nWe can see that probabilities are more spread into [0, 1] space when scale_pos_weight increase"},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"34fed3b8a3e213ac7302287387b5b3dd5762e11c","_cell_guid":"72e09f2a-8294-4712-8d21-472717bc1bfe","collapsed":true},"outputs":[],"source":"fig, ax = plt.subplots(figsize=(15, 10))\nplt.rc('legend', fontsize=18) \nplt.rc('axes', labelsize=18)\nplt.rc('axes', titlesize=24)\nfor i_w, scale_pos_weight in enumerate(scale_pos_weights):\n    # Get False positives, true positives and the list of thresholds used to compute them\n    fpr, tpr, thresholds = roc_curve(target, oof_proba[:, i_w])\n    # Compute recall, precision and f1_score\n    recall = tpr\n    precision = tpr / (fpr + tpr + 1e-5)\n    f1_scores = 2 * precision * recall / (precision + recall + 1e-5)\n    # Finally plot the f1_scores against thresholds\n    plt.plot(thresholds[-30000:], f1_scores[-30000:], \n             label=\"scale_pos_weight=%2d\" % scale_pos_weight)\nplt.title(\"F1 scores against threshold for different scale_pos_weight\")\nplt.ylabel(\"F1 score\")\nplt.xlabel(\"Probability thresholds\")\nplt.legend(loc=\"lower left\")\nax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n\n"},{"cell_type":"markdown","metadata":{"_uuid":"b4396821e4bb6d39f612ee8311f691ffa4cb8410","_cell_guid":"4fa40a79-86f3-4bc6-9d14-c7bd7b4ecad7"},"source":"### Confusion matrices\nTo see the way probabilities spread more when scale_pos_weight increase let's display the evolution of the confusion matrices "},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"185f6258ef88724f4a40063cdcc19277c97c7b29","_cell_guid":"4be31532-81c7-4300-aaf6-1d35eff032bc","collapsed":true},"outputs":[],"source":"fig = plt.figure(figsize=(20, 15))\nplt.rc('legend', fontsize=10) \nplt.rc('axes', labelsize=10)\nplt.rc('axes', titlesize=10)\ngs = gridspec.GridSpec(int(len(scale_pos_weights) / 2), 2)\nfor i_w, weight in enumerate(scale_pos_weights):\n    ax = plt.subplot(gs[int(i_w / 2), i_w % 2])\n    class_names = [\"safe\", \"unsafe\"]\n    cnf_matrix = confusion_matrix(target, oof_label[:, i_w])\n    plot_confusion_matrix(cnf_matrix, classes=class_names, \n                          normalize=True,\n                          title='Matrix for scale_pos_weight = %2d, Gini %.6f' \n                          % (weight, eval_gini(target, oof_proba[:, i_w])))\nplt.tight_layout()"},{"cell_type":"markdown","metadata":{"_uuid":"06087f1d313bda68c0b5fe2c13366a1e0a2e479f","_cell_guid":"305cdf44-34a6-438a-aab6-10630d37b268"},"source":"As you can see we have more and more true positives while false negative increase as well. \n"},{"cell_type":"markdown","metadata":{"_uuid":"f4c0d2b17519058e01903f7299707bb52dcb4bfc","_kg_hide-input":false,"_cell_guid":"fdf00c9f-69ae-4c8c-955d-6420ce57c6e2","collapsed":true},"source":"### What about duplication ?"},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"263807436fd025bd40760b694a5b5bd53b0e41f1","_cell_guid":"dbc2ad3e-95f4-4e92-bd6f-5da826bb3d81"},"outputs":[],"source":"dupes = np.arange(0.5, 3.1, .5)\noof_proba = np.empty((len(trn_df), len(dupes)))\noof_label = np.empty((len(trn_df), len(dupes)))\nfor i_w, dupe in enumerate(dupes):\n    n_splits = 2 # That's enough to get a feel on what's going on\n    n_estimators = 100\n    folds = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=14) \n\n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(target, target)):\n        trn_dat, trn_tgt = trn_df.iloc[trn_idx], target.iloc[trn_idx]\n        val_dat, val_tgt = trn_df.iloc[val_idx], target.iloc[val_idx]\n\n        clf = LGBMClassifier(n_estimators=n_estimators,\n                             max_depth=-1,\n                             num_leaves=25,\n                             learning_rate=.1, \n                             subsample=.8, \n                             colsample_bytree=.8,\n                             min_split_gain=1,\n                             reg_alpha=0,\n                             reg_lambda=0,\n                             scale_pos_weight=1,\n                             min_child_weight=1,\n                             n_jobs=2)\n        # Duplicate positives on the training part\n        pos = pd.Series(trn_tgt == 1)\n        pos_dat = trn_dat.loc[pos]\n        pos_tgt = trn_tgt.loc[pos]\n        pos_idx = np.arange(len(pos_tgt))\n        if dupe <= 1.0: \n            # Add positive examples\n            np.random.shuffle(pos_idx)\n            trn_dat = pd.concat([trn_dat, pos_dat.iloc[pos_idx[:int(len(pos) * dupe)]]], axis=0)\n            trn_tgt = pd.concat([trn_tgt, pos_tgt.iloc[pos_idx[:int(len(pos) * dupe)]]], axis=0)\n        else:\n            dupint = math.floor(dupe)\n            remain = dupe - dupint\n            for i in range(dupint):\n                trn_dat = pd.concat([trn_dat, pos_dat], axis=0)\n                trn_tgt = pd.concat([trn_tgt, pos_tgt], axis=0)\n            np.random.shuffle(pos_idx)\n            trn_dat = pd.concat([trn_dat, pos_dat.iloc[pos_idx[:int(len(pos) * dupe)]]], axis=0)\n            trn_tgt = pd.concat([trn_tgt, pos_tgt.iloc[pos_idx[:int(len(pos) * dupe)]]], axis=0)\n        print(len(trn_dat), len(pos_dat))\n        # Shuffle data\n        idx = np.arange(len(trn_dat))\n        np.random.shuffle(idx)\n        trn_dat = trn_dat.iloc[idx]\n        trn_tgt = trn_tgt.iloc[idx]\n        \n        clf.fit(trn_dat, trn_tgt, \n                eval_set=[(trn_dat, trn_tgt), (val_dat, val_tgt)],\n                eval_metric=\"auc\",\n                early_stopping_rounds=None,\n                verbose=False)\n\n        oof_proba[val_idx, i_w] = clf.predict_proba(val_dat)[:, 1]\n        oof_label[val_idx, i_w] = clf.predict(val_dat)\n\n    print(\"Full OOF score : %.6f for duplication %.1f\" \n          % (eval_gini(target, oof_proba[:, i_w]), dupe))"},{"cell_type":"markdown","metadata":{},"source":"### Let's check how confusion matrices are affected\nPlease remember labels are computed using a .5 threshold"},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"87e9b481b0fac1f7909265de52c11ecc2ed0c008","_kg_hide-input":false,"_cell_guid":"a72e47ef-2a65-4f60-a4de-de665606a212"},"outputs":[],"source":"fig = plt.figure(figsize=(20, 15))\nplt.rc('legend', fontsize=10) \nplt.rc('axes', labelsize=10)\nplt.rc('axes', titlesize=10)\ngs = gridspec.GridSpec(math.ceil(len(dupes) / 2), 2)\nfor i_w, weight in enumerate(dupes):\n    ax = plt.subplot(gs[int(i_w / 2), i_w % 2])\n    class_names = [\"safe\", \"unsafe\"]\n    cnf_matrix = confusion_matrix(target, oof_label[:, i_w])\n    plot_confusion_matrix(cnf_matrix, classes=class_names, \n                          normalize=False,\n                          title='Matrix for duplication = %.1f, Gini %.6f' \n                          % (weight, eval_gini(target, oof_proba[:, i_w])))\nplt.tight_layout()"},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"d9b7a45cceb1e9dbaf1f2b85e9ddd02aabb4a4c2","_cell_guid":"4bcfc850-caeb-4dac-ba0a-1f4bd0fdfcce"},"outputs":[],"source":"fig, ax = plt.subplots(figsize=(15, 10))\nplt.rc('legend', fontsize=18) \nplt.rc('axes', labelsize=18)\nplt.rc('axes', titlesize=24)\nfor i_w, dupe in enumerate(dupes):\n    # Get False positives, true positives and the list of thresholds used to compute them\n    fpr, tpr, thresholds = roc_curve(target, oof_proba[:, i_w])\n    # Compute recall, precision and f1_score\n    recall = tpr\n    precision = tpr / (fpr + tpr + 1e-5)\n    f1_scores = 2 * precision * recall / (precision + recall + 1e-5)\n    # Finally plot the f1_scores against thresholds\n    plt.plot(thresholds[-30000:], f1_scores[-30000:], \n             label=\"duplication rate x %.1f\" % dupe)\nplt.title(\"F1 scores against threshold for different duplication rate\")\nplt.ylabel(\"F1 score\")\nplt.xlabel(\"Probability thresholds\")\nplt.legend(loc=\"lower left\")\nax.legend(loc='center left', bbox_to_anchor=(1, 0.5))"},{"cell_type":"markdown","metadata":{},"source":"I must say this is a strange plot with lots of overlap. I'm really puzzled by this...\n\nAt least we can see that you really need a strong duplication rate to spread probabilities the way scale_pos_weight does. As far as I'm concerned I will probably go the scale_pos_weight way. If you plan to merge predictions you will have to make sure they span approximately the same way in the [0, 1] space, all the more if you use geometric or harmonic mean...  "},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true},"outputs":[],"source":""}],"nbformat":4,"metadata":{"language_info":{"nbconvert_exporter":"python","version":"3.6.3","name":"python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","file_extension":".py","mimetype":"text/x-python"},"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"}}}