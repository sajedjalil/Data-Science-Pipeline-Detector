{"nbformat_minor":1,"cells":[{"cell_type":"markdown","metadata":{"_uuid":"44fbb83d4c6019b6a12139e8ddd5f6b67e79ee8e","_cell_guid":"9c21c679-c70d-45fe-a57f-5036f6f9e702"},"source":"As part of the feature selection work started by @Tilii, I just came across an interesting paper proposing to select features using target permuations instead of feature permutations. This has the advantage of keeping relationships between features while modifying the outcome.\nHere is [the paper.](http://academic.oup.com/bioinformatics/article/26/10/1340/193348/Permutation-importance-a-corrected-feature)\n\nTilii's kernel is here : https://www.kaggle.com/tilii7/boruta-feature-elimination\n\n"},{"execution_count":null,"cell_type":"code","metadata":{},"outputs":[],"source":"import pandas as pd\nimport numpy as np\nfrom sklearn.metrics import roc_auc_score, log_loss\nfrom sklearn.model_selection import StratifiedKFold\nfrom lightgbm import LGBMClassifier\nfrom numba import jit"},{"cell_type":"markdown","metadata":{},"source":"### Define Gini metric"},{"execution_count":null,"cell_type":"code","metadata":{},"outputs":[],"source":"@jit  # for more info please visit https://numba.pydata.org/\ndef eval_gini(y_true, y_prob):\n    \"\"\"\n    Original author CMPM \n    https://www.kaggle.com/cpmpml/extremely-fast-gini-computation\n    \"\"\"\n    y_true = np.asarray(y_true)\n    y_true = y_true[np.argsort(y_prob)]\n    ntrue = 0\n    gini = 0\n    delta = 0\n    n = len(y_true)\n    for i in range(n-1, -1, -1):\n        y_i = y_true[i]\n        ntrue += y_i\n        gini += y_i * delta\n        delta += 1 - y_i\n    gini = 1 - 2 * gini / (ntrue * (n - ntrue))\n    return gini"},{"cell_type":"markdown","metadata":{},"source":"### Read train data"},{"execution_count":null,"cell_type":"code","metadata":{},"outputs":[],"source":"trn_df = pd.read_csv(\"../input/train.csv\", index_col=0)\ntarget = trn_df.target\ndel trn_df[\"target\"]"},{"cell_type":"markdown","metadata":{},"source":"### Create a LightGBM RandomForest Classifier"},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true},"outputs":[],"source":"clf = LGBMClassifier(boosting_type=\"rf\",\n                     num_leaves=1024,\n                     max_depth=6,\n                     n_estimators=500, \n                     subsample=.632,\n                     colsample_bytree=.5,\n                     n_jobs=2)"},{"execution_count":null,"cell_type":"code","metadata":{},"outputs":[],"source":"n_splits = 2\nn_runs = 5\nimp_df = np.zeros((len(trn_df.columns), n_splits * n_runs))\nnp.random.seed(9385610)\nidx = np.arange(len(target))\nfor run in range(n_runs):\n    # Shuffle target\n    np.random.shuffle(idx)\n    perm_target = target.iloc[idx]\n    # Create a new split\n    folds = StratifiedKFold(n_splits, True, None)\n    oof = np.empty(len(trn_df))\n    \n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(perm_target, perm_target)):\n        trn_dat, trn_tgt = trn_df.iloc[trn_idx], perm_target.iloc[trn_idx]\n        val_dat, val_tgt = trn_df.iloc[val_idx], perm_target.iloc[val_idx]\n        # Train classifier\n        clf.fit(trn_dat, trn_tgt)\n        # Keep feature importances for this fold and run\n        imp_df[:, n_splits * run + fold_] = clf.feature_importances_\n        # Update OOF for gini score display\n        oof[val_idx] = clf.predict_proba(val_dat)[:, 1]\n        \n    print(\"Run %2d OOF score : %.6f\" % (run, eval_gini(perm_target, oof)))\n    "},{"cell_type":"markdown","metadata":{},"source":"### Run a benchmark test"},{"execution_count":null,"cell_type":"code","metadata":{},"outputs":[],"source":"bench_imp_df = np.zeros((len(trn_df.columns), n_splits * n_runs))\nfor run in range(n_runs):\n    # Shuffle target AND dataset\n    np.random.shuffle(idx)\n    perm_target = target.iloc[idx]\n    perm_data = trn_df.iloc[idx]\n    \n    # Create a new split\n    folds = StratifiedKFold(n_splits, True, None)\n    oof = np.empty(len(trn_df))\n    \n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(perm_target, perm_target)):\n        trn_dat, trn_tgt = perm_data.iloc[trn_idx], perm_target.iloc[trn_idx]\n        val_dat, val_tgt = perm_data.iloc[val_idx], perm_target.iloc[val_idx]\n        # Train classifier\n        clf.fit(trn_dat, trn_tgt)\n        # Keep feature importances for this fold and run\n        bench_imp_df[:, n_splits * run + fold_] = clf.feature_importances_\n        # Update OOF for gini score display\n        oof[val_idx] = clf.predict_proba(val_dat)[:, 1]\n        \n    print(\"Run %2d OOF score : %.6f\" % (run, eval_gini(perm_target, oof)))\n"},{"cell_type":"markdown","metadata":{},"source":"Hopefully scores are better when features AND datasets are shuffled together !!!\nBut What about feature importances"},{"execution_count":null,"cell_type":"code","metadata":{},"outputs":[],"source":"bench_mean = bench_imp_df.mean(axis=1)\nperm_mean = imp_df.mean(axis=1)\n\nvalues = []\nfor i, f in enumerate(trn_df.columns):\n    values.append((f, bench_mean[i], perm_mean[i], bench_mean[i] / perm_mean[i]))\n\nprint(\"%-20s | benchmark | permutation | Ratio\" % \"Feature\")\nvalues = sorted(values, key=lambda x: x[3])\nfor f, b, p, r in values[::-1]:\n    print(\"%-20s |   %7.1f |     %7.1f |   %7.1f\" \n          % (f, b, p, r))"},{"cell_type":"markdown","metadata":{},"source":"Again *calc* features look pretty bad."},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true},"outputs":[],"source":""}],"nbformat":4,"metadata":{"language_info":{"nbconvert_exporter":"python","version":"3.6.3","name":"python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","file_extension":".py","mimetype":"text/x-python"},"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"}}}