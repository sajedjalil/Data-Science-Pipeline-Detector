{"nbformat":4,"nbformat_minor":1,"cells":[{"cell_type":"code","outputs":[],"execution_count":null,"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_auc_score\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import StratifiedKFold\n\nimport xgboost as xgb\nimport lightgbm as lgb\nimport time\n\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_colwidth', 500)\npd.set_option('display.max_rows', 1000)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","metadata":{"_uuid":"d0673b9bd61398c6fe3d238710fd4ad725db7df9","_cell_guid":"288e711f-b3ff-422e-a480-f4d7d15ddf03","collapsed":true}},{"cell_type":"markdown","source":"","metadata":{"_uuid":"7a07b0af0cd81ebd82ce93251add858c886fcfc3","_cell_guid":"93ac38f6-d126-4038-a74b-55ebe737931a"}},{"cell_type":"markdown","source":"## Note to others\n\nThis is the second of a series of notebooks that I am working on for educational purpose, to demonstrate some run-of-the mill techniques we use on Kaggle for a student audience.\n\nSo when most folks here are competiting to overfit the LB, we are doing some small effort to fill our students with handy knowledge and the virtue of solid CV. \n\nAnyhow, since our students are also competing as teams in this competition, we decided to use the kernel facility as a way to share knowledge. Nevertheless, all comments are welcome, and let's all enjoy the last days of this competition!","metadata":{"_uuid":"bc57a5473245411ee23bd3beeb9c32411f81e93f","_cell_guid":"4784499f-69bb-45fc-8010-03f94f975712"}},{"cell_type":"markdown","source":"# Porto Seguo - End-to-end Ensemble\n\nIn this competition we are tasked with making predictive models that can predict if a given driver will make insurance claim. In a [\"previous kernel\"](https://www.kaggle.com/yifanxie/porto-seguro-tutorial-simple-e2e-pipeline) we have breifly explored the data, did some useful categorical feature encoding, and presented a simple model building pipeline.\n\nIn this kernel, we are going to progress a bit more on the model building aspect. Firstly, we will introduce the technique to generate out-out-fold train and test predictions for several models, we will then use these out-of-fold predictions to be  ensemble model. \n\nStrickly speaking, the ensemble method we use here is referred as **Stacked generalization** as very well ilustrated already by the following blog/articles:\n* [*Kaggle Ensemble Guide*](https://mlwave.com/kaggle-ensembling-guide/) by [Triskelion](https://www.kaggle.com/triskelion)\n* [*Stacking Made Easy: An Introduction to StackNet*](http://blog.kaggle.com/2017/06/15/stacking-made-easy-an-introduction-to-stacknet-by-competitions-grandmaster-marios-michailidis-kazanova/) by Competitions Grandmaster [Marios Michailidis (KazAnova)](https://www.kaggle.com/kazanova)\n\nSo without further ado, let's get technical.\n","metadata":{"_uuid":"53f6798ebda6c7faaf74344fa88d58dbb7c13010","_cell_guid":"1b9e3253-3add-432f-87b7-793f0ba9292f"}},{"cell_type":"code","outputs":[],"execution_count":null,"source":"train=pd.read_csv('../input/train.csv')\ntest=pd.read_csv('../input/test.csv')\nsample_submission=pd.read_csv('../input/sample_submission.csv')","metadata":{"_uuid":"7f843356d8325e4f99ebe2faae68410155f589c0","_cell_guid":"663d970a-8cd7-475a-ad65-7f687b9bbf8e","collapsed":true}},{"cell_type":"markdown","source":"\n# 1. Categorical feature encoding and feature reduction\nThe following part is a strict copy & paste from the first kernel, so for detailed explaination please check it out there.\n\n## 1.1 Frequency Encoding","metadata":{"_uuid":"ca741f0f9081fe15672b08ae826f08dd00cf0665","_cell_guid":"f06a2004-a599-4780-9277-f43eac745a63"}},{"cell_type":"code","outputs":[],"execution_count":null,"source":"# This function late in a list of features 'cols' from train and test dataset, \n# and performing frequency encoding. \ndef freq_encoding(cols, train_df, test_df):\n    # we are going to store our new dataset in these two resulting datasets\n    result_train_df=pd.DataFrame()\n    result_test_df=pd.DataFrame()\n    \n    # loop through each feature column to do this\n    for col in cols:\n        \n        # capture the frequency of a feature in the training set in the form of a dataframe\n        col_freq=col+'_freq'\n        freq=train_df[col].value_counts()\n        freq=pd.DataFrame(freq)\n        freq.reset_index(inplace=True)\n        freq.columns=[[col,col_freq]]\n\n        # merge ths 'freq' datafarme with the train data\n        temp_train_df=pd.merge(train_df[[col]], freq, how='left', on=col)\n        temp_train_df.drop([col], axis=1, inplace=True)\n\n        # merge this 'freq' dataframe with the test data\n        temp_test_df=pd.merge(test_df[[col]], freq, how='left', on=col)\n        temp_test_df.drop([col], axis=1, inplace=True)\n\n        # if certain levels in the test dataset is not observed in the train dataset, \n        # we assign frequency of zero to them\n        temp_test_df.fillna(0, inplace=True)\n        temp_test_df[col_freq]=temp_test_df[col_freq].astype(np.int32)\n\n        if result_train_df.shape[0]==0:\n            result_train_df=temp_train_df\n            result_test_df=temp_test_df\n        else:\n            result_train_df=pd.concat([result_train_df, temp_train_df],axis=1)\n            result_test_df=pd.concat([result_test_df, temp_test_df],axis=1)\n    \n    return result_train_df, result_test_df","metadata":{"_uuid":"28ec82dcfb1888ebda974dc842dee72a0b2e8fb6","_cell_guid":"41904a7e-9bcb-45ee-b9b3-2215e87b789d","collapsed":true}},{"cell_type":"markdown","source":"let's run the frequency encoding function","metadata":{"_uuid":"4c14712fbcab2d1f7b5e60a754b41a0979e5f42f","_cell_guid":"40a02505-48bb-41d8-b448-0730d3fa1c1f"}},{"cell_type":"code","outputs":[],"execution_count":null,"source":"cat_cols=['ps_ind_02_cat','ps_car_04_cat', 'ps_car_09_cat',\n          'ps_ind_05_cat', 'ps_car_01_cat', 'ps_car_11_cat']\n\n# generate dataframe for frequency features for the train and test dataset\ntrain_freq, test_freq=freq_encoding(cat_cols, train, test)\n\n# merge them into the original train and test dataset\ntrain=pd.concat([train, train_freq], axis=1)\ntest=pd.concat([test,test_freq], axis=1)","metadata":{"_uuid":"1d1c5b0639871b06ddc529aa889d269aea28518b","_cell_guid":"16abd778-25fe-465c-9c86-61a00cbe2f92","collapsed":true}},{"cell_type":"markdown","source":"## 1.2 Binary Encoding","metadata":{"_uuid":"10df2ee5ce0a0d47c6f6caa8805bc497cad5e39c","_cell_guid":"b7777ede-01b4-48c4-9d83-00b3ef16ccdd"}},{"cell_type":"code","outputs":[],"execution_count":null,"source":"# perform binary encoding for categorical variable\n# this function take in a pair of train and test data set, and the feature that need to be encode.\n# it returns the two dataset with input feature encoded in binary representation\n# this function assumpt that the feature to be encoded is already been encoded in a numeric manner \n# ranging from 0 to n-1 (n = number of levels in the feature). \n\ndef binary_encoding(train_df, test_df, feat):\n    # calculate the highest numerical value used for numeric encoding\n    train_feat_max = train_df[feat].max()\n    test_feat_max = test_df[feat].max()\n    if train_feat_max > test_feat_max:\n        feat_max = train_feat_max\n    else:\n        feat_max = test_feat_max\n        \n    # use the value of feat_max+1 to represent missing value\n    train_df.loc[train_df[feat] == -1, feat] = feat_max + 1\n    test_df.loc[test_df[feat] == -1, feat] = feat_max + 1\n    \n    # create a union set of all possible values of the feature\n    union_val = np.union1d(train_df[feat].unique(), test_df[feat].unique())\n\n    # extract the highest value from from the feature in decimal format.\n    max_dec = union_val.max()\n    \n    # work out how the ammount of digtis required to be represent max_dev in binary representation\n    max_bin_len = len(\"{0:b}\".format(max_dec))\n    index = np.arange(len(union_val))\n    columns = list([feat])\n    \n    # create a binary encoding feature dataframe to capture all the levels for the feature\n    bin_df = pd.DataFrame(index=index, columns=columns)\n    bin_df[feat] = union_val\n    \n    # capture the binary representation for each level of the feature \n    feat_bin = bin_df[feat].apply(lambda x: \"{0:b}\".format(x).zfill(max_bin_len))\n    \n    # split the binary representation into different bit of digits \n    splitted = feat_bin.apply(lambda x: pd.Series(list(x)).astype(np.uint8))\n    splitted.columns = [feat + '_bin_' + str(x) for x in splitted.columns]\n    bin_df = bin_df.join(splitted)\n    \n    # merge the binary feature encoding dataframe with the train and test dataset - Done! \n    train_df = pd.merge(train_df, bin_df, how='left', on=[feat])\n    test_df = pd.merge(test_df, bin_df, how='left', on=[feat])\n    return train_df, test_df\n","metadata":{"_uuid":"e6a2780f14d6913486d46126ecfbde20c77bcfc0","_cell_guid":"f1a04729-193a-4baf-8353-d4f4dba38a84","collapsed":true}},{"cell_type":"markdown","source":"let's run the binary encoding function","metadata":{"_uuid":"8604fb2c717bed8ae225ff5008f281b4ea9009c8","_cell_guid":"79ee91b1-d759-457d-b8d7-cb96d81552c7"}},{"cell_type":"code","outputs":[],"execution_count":null,"source":"cat_cols=['ps_ind_02_cat','ps_car_04_cat', 'ps_car_09_cat',\n          'ps_ind_05_cat', 'ps_car_01_cat']\n\ntrain, test=binary_encoding(train, test, 'ps_ind_02_cat')\ntrain, test=binary_encoding(train, test, 'ps_car_04_cat')\ntrain, test=binary_encoding(train, test, 'ps_car_09_cat')\ntrain, test=binary_encoding(train, test, 'ps_ind_05_cat')\ntrain, test=binary_encoding(train, test, 'ps_car_01_cat')","metadata":{"_uuid":"d99067cf879e1eea6c32ca86d1db96ef323275bf","_cell_guid":"841e7b7e-9b02-4857-b99f-477c40514099","collapsed":true}},{"cell_type":"markdown","source":"optionally, you can also choose to drop the original categorical features. Shoud you do it? I say trust your CV :)\nlet's do this here jut for demonstration purpose","metadata":{"_uuid":"30512ce286b81605dc8db605d195f5ae6a4f306f","_cell_guid":"4685cab7-f959-4b87-ae17-f9641834116e"}},{"cell_type":"code","outputs":[],"execution_count":null,"source":"col_to_drop = train.columns[train.columns.str.startswith('ps_calc_')]\ntrain.drop(col_to_drop, axis=1, inplace=True)  \ntest.drop(col_to_drop, axis=1, inplace=True)  ","metadata":{"_uuid":"f89ee9756b9d0a37de012a0bfd5d1ffe13f04b60","_cell_guid":"69d26f77-c492-470e-8c55-f85225552718","collapsed":true}},{"cell_type":"markdown","source":"## 1.3 Feature Reduction\nLet's now drop all the features with the wording \"cal\" - \"Cal, you are FIRED\" ^ ^","metadata":{"_uuid":"d0e441792dcaa67828b049f308baed76a8ae05e2","_cell_guid":"80c9e6d1-e2b9-48a0-b1ae-0ce18ff7440d"}},{"cell_type":"code","outputs":[],"execution_count":null,"source":"col_to_drop = train.columns[train.columns.str.startswith('ps_calc_')]\ntrain.drop(col_to_drop, axis=1, inplace=True)  \ntest.drop(col_to_drop, axis=1, inplace=True)","metadata":{"_uuid":"8e95a6e5311373285133a5294f5b19a66ab5fd21","_cell_guid":"56598067-fa88-4646-9da7-804f6612766b","collapsed":true}},{"cell_type":"markdown","source":"Right, after the above data manipulation, we can now take a brief look at our dataset.","metadata":{"_uuid":"ba8cabf0a496a6f08159c2f5c7008c8260d150fb","_cell_guid":"071c6421-3d3c-44ca-9a26-a11d697b63a9"}},{"cell_type":"code","outputs":[],"execution_count":null,"source":"train.head(5)","metadata":{"_uuid":"c9af17715af8e9bd7211c3cf756635511c6a0ef2","_cell_guid":"1160a785-c819-4a36-ad5c-5fbc51410b84","collapsed":true}},{"cell_type":"markdown","source":"# 2. K-fold CV with Out-of-Fold Prediction\n\n\n*Note: for demonstration purpose, I have dump down the parameters of the models to make them run faster, so please take time to find a good cominbation of parameter when you send them out to battle for real!*\n\n## 2.1 OOF utility functions\nFirstly, let's write this handy function to convert AUC score into Gini Normalised Coeficient","metadata":{"_uuid":"f21971667aba5e18cbf1a11f78236026e7f8b697","_cell_guid":"ccc3c205-d2fa-4fae-91f3-bc655880068b"}},{"cell_type":"code","outputs":[],"execution_count":null,"source":"def auc_to_gini_norm(auc_score):\n    return 2*auc_score-1","metadata":{"_uuid":"a0eed4fe1d170f71de94cbb8fec9f13f66ed6c43","_cell_guid":"f2919865-f875-4f06-925c-7f09b1afc2c7","collapsed":true}},{"cell_type":"markdown","source":"### 2.1.1 Sklearn K-fold & OOF function\nNext up next provide a K-fold function that generate out-of-fold predictions for train and test data.","metadata":{"_uuid":"9a768496d0e67cb9ea0f4042f430bfb3aee58dc4","_cell_guid":"72abf4bc-7617-440a-9440-d0225b2b2671"}},{"cell_type":"code","outputs":[],"execution_count":null,"source":"def cross_validate_sklearn(clf, x_train, y_train , x_test, kf,scale=False, verbose=True):\n    start_time=time.time()\n    \n    # initialise the size of out-of-fold train an test prediction\n    train_pred = np.zeros((x_train.shape[0]))\n    test_pred = np.zeros((x_test.shape[0]))\n\n    # use the kfold object to generate the required folds\n    for i, (train_index, test_index) in enumerate(kf.split(x_train, y_train)):\n        # generate training folds and validation fold\n        x_train_kf, x_val_kf = x_train.loc[train_index, :], x_train.loc[test_index, :]\n        y_train_kf, y_val_kf = y_train[train_index], y_train[test_index]\n\n        # perform scaling if required i.e. for linear algorithms\n        if scale:\n            scaler = StandardScaler().fit(x_train_kf.values)\n            x_train_kf_values = scaler.transform(x_train_kf.values)\n            x_val_kf_values = scaler.transform(x_val_kf.values)\n            x_test_values = scaler.transform(x_test.values)\n        else:\n            x_train_kf_values = x_train_kf.values\n            x_val_kf_values = x_val_kf.values\n            x_test_values = x_test.values\n        \n        # fit the input classifier and perform prediction.\n        clf.fit(x_train_kf_values, y_train_kf.values)\n        val_pred=clf.predict_proba(x_val_kf_values)[:,1]\n        train_pred[test_index] += val_pred\n\n        y_test_preds = clf.predict_proba(x_test_values)[:,1]\n        test_pred += y_test_preds\n\n        fold_auc = roc_auc_score(y_val_kf.values, val_pred)\n        fold_gini_norm = auc_to_gini_norm(fold_auc)\n\n        if verbose:\n            print('fold cv {} AUC score is {:.6f}, Gini_Norm score is {:.6f}'.format(i, fold_auc, fold_gini_norm))\n\n    test_pred /= kf.n_splits\n\n    cv_auc = roc_auc_score(y_train, train_pred)\n    cv_gini_norm = auc_to_gini_norm(cv_auc)\n    cv_score = [cv_auc, cv_gini_norm]\n    if verbose:\n        print('cv AUC score is {:.6f}, Gini_Norm score is {:.6f}'.format(cv_auc, cv_gini_norm))\n        end_time = time.time()\n        print(\"it takes %.3f seconds to perform cross validation\" % (end_time - start_time))\n    return cv_score, train_pred,test_pred\n","metadata":{"_uuid":"8269f0290fbc3cd08073d32bc448188ab66d68d0","_cell_guid":"b842a528-e657-446b-a9bd-be48cf5cf6f0","collapsed":true}},{"cell_type":"markdown","source":"### 2.1.2 Xgboost K-fold & OOF function\nIn this part, we are going to use the native interface of XGB and LGB, so the following functions are tailor for this. For sure it would be easiler just to call the respective sklearn api, but the native interfaces provide some nice additional capability. For instance, the 'hist' option to use fast histogram in XGB is only available via the native interface as far as I know. \n\nAlso, we need to provide the following function to convert probability into rank for these two OOF function. The needs to use normalised rank instead of predicted probabilities will become appearent later in this notebook :) ","metadata":{"_uuid":"5912b3af83ab01617b57f709b671018ac24830e7","_cell_guid":"d77d32a5-7fe7-4b1e-af57-35ed65bc840a"}},{"cell_type":"code","outputs":[],"execution_count":null,"source":"def probability_to_rank(prediction, scaler=1):\n    pred_df=pd.DataFrame(columns=['probability'])\n    pred_df['probability']=prediction\n    pred_df['rank']=pred_df['probability'].rank()/len(prediction)*scaler\n    return pred_df['rank'].values","metadata":{"_uuid":"a5ea0645e15eda4bd878ef753daa39d408020ca8","_cell_guid":"d5321609-aed2-467f-ab17-7c054cdf00fc","collapsed":true}},{"cell_type":"markdown","source":"The following is the k-fold function for XGB to generate OOF predictions, this function is very much similar to its sklearn counter part. The difference is that we need to use the XGB interface to facilitate the classifer, also we provide an option cover probability into rank.","metadata":{"_uuid":"198fc4ade5533041c9a5405cb9d606856546a7a9","_cell_guid":"becfc331-0087-47f3-aeee-b454bf6703ce"}},{"cell_type":"code","outputs":[],"execution_count":null,"source":"def cross_validate_xgb(params, x_train, y_train, x_test, kf, cat_cols=[], verbose=True, \n                       verbose_eval=50, num_boost_round=4000, use_rank=True):\n    start_time=time.time()\n\n    train_pred = np.zeros((x_train.shape[0]))\n    test_pred = np.zeros((x_test.shape[0]))\n\n    # use the k-fold object to enumerate indexes for each training and validation fold\n    for i, (train_index, val_index) in enumerate(kf.split(x_train, y_train)): # folds 1, 2 ,3 ,4, 5\n        # example: training from 1,2,3,4; validation from 5\n        x_train_kf, x_val_kf = x_train.loc[train_index, :], x_train.loc[val_index, :]\n        y_train_kf, y_val_kf = y_train[train_index], y_train[val_index]\n        x_test_kf=x_test.copy()\n\n        d_train_kf = xgb.DMatrix(x_train_kf, label=y_train_kf)\n        d_val_kf = xgb.DMatrix(x_val_kf, label=y_val_kf)\n        d_test = xgb.DMatrix(x_test_kf)\n\n        bst = xgb.train(params, d_train_kf, num_boost_round=num_boost_round,\n                        evals=[(d_train_kf, 'train'), (d_val_kf, 'val')], verbose_eval=verbose_eval,\n                        early_stopping_rounds=50)\n\n        val_pred = bst.predict(d_val_kf, ntree_limit=bst.best_ntree_limit)\n        if use_rank:\n            train_pred[val_index] += probability_to_rank(val_pred)\n            test_pred+=probability_to_rank(bst.predict(d_test))\n        else:\n            train_pred[val_index] += val_pred\n            test_pred+=bst.predict(d_test)\n\n        fold_auc = roc_auc_score(y_val_kf.values, val_pred)\n        fold_gini_norm = auc_to_gini_norm(fold_auc)\n\n        if verbose:\n            print('fold cv {} AUC score is {:.6f}, Gini_Norm score is {:.6f}'.format(i, fold_auc, \n                                                                                     fold_gini_norm))\n\n    test_pred /= kf.n_splits\n\n    cv_auc = roc_auc_score(y_train, train_pred)\n    cv_gini_norm = auc_to_gini_norm(cv_auc)\n    cv_score = [cv_auc, cv_gini_norm]\n    if verbose:\n        print('cv AUC score is {:.6f}, Gini_Norm score is {:.6f}'.format(cv_auc, cv_gini_norm))\n        end_time = time.time()\n        print(\"it takes %.3f seconds to perform cross validation\" % (end_time - start_time))\n\n        return cv_score, train_pred,test_pred\n","metadata":{"_uuid":"099501f1edf2c162fdaac17eb4eeff2bbe604cf1","_cell_guid":"2f79a43f-8b4c-4269-8009-d36835185171","collapsed":true}},{"cell_type":"markdown","source":"### 2.1.3 LigthGBM K-fold & OOF function\nThe same function for LGB, this one is almost identifical to the one for XGB, apart from code that call the LightGBM interface","metadata":{"_uuid":"be3998a3bc1a33eb0735b5e3bf748ddc15a0278c","_cell_guid":"d88e531c-c8f6-426d-a112-3f560ef94726"}},{"cell_type":"code","outputs":[],"execution_count":null,"source":"def cross_validate_lgb(params, x_train, y_train, x_test, kf, cat_cols=[],\n                       verbose=True, verbose_eval=50, use_cat=True, use_rank=True):\n    start_time = time.time()\n    train_pred = np.zeros((x_train.shape[0]))\n    test_pred = np.zeros((x_test.shape[0]))\n\n    if len(cat_cols)==0: use_cat=False\n\n    # use the k-fold object to enumerate indexes for each training and validation fold\n    for i, (train_index, val_index) in enumerate(kf.split(x_train, y_train)): # folds 1, 2 ,3 ,4, 5\n        # example: training from 1,2,3,4; validation from 5\n        x_train_kf, x_val_kf = x_train.loc[train_index, :], x_train.loc[val_index, :]\n        y_train_kf, y_val_kf = y_train[train_index], y_train[val_index]\n\n        if use_cat:\n            lgb_train = lgb.Dataset(x_train_kf, y_train_kf, categorical_feature=cat_cols)\n            lgb_val = lgb.Dataset(x_val_kf, y_val_kf, reference=lgb_train, categorical_feature=cat_cols)\n        else:\n            lgb_train = lgb.Dataset(x_train_kf, y_train_kf)\n            lgb_val = lgb.Dataset(x_val_kf, y_val_kf, reference=lgb_train)\n\n        gbm = lgb.train(params,\n                        lgb_train,\n                        num_boost_round=4000,\n                        valid_sets=lgb_val,\n                        early_stopping_rounds=30,\n                        verbose_eval=verbose_eval)\n\n        val_pred = gbm.predict(x_val_kf)\n\n        if use_rank:\n            train_pred[val_index] += probability_to_rank(val_pred)\n            test_pred += probability_to_rank(gbm.predict(x_test))\n            # test_pred += gbm.predict(x_test)\n        else:\n            train_pred[val_index] += val_pred\n            test_pred += gbm.predict(x_test)\n\n        # test_pred += gbm.predict(x_test)\n        fold_auc = roc_auc_score(y_val_kf.values, val_pred)\n        fold_gini_norm = auc_to_gini_norm(fold_auc)\n        if verbose:\n            print('fold cv {} AUC score is {:.6f}, Gini_Norm score is {:.6f}'.format(i, fold_auc, fold_gini_norm))\n\n    test_pred /= kf.n_splits\n\n    cv_auc = roc_auc_score(y_train, train_pred)\n    cv_gini_norm = auc_to_gini_norm(cv_auc)\n    cv_score = [cv_auc, cv_gini_norm]\n    if verbose:\n        print('cv AUC score is {:.6f}, Gini_Norm score is {:.6f}'.format(cv_auc, cv_gini_norm))\n        end_time = time.time()\n        print(\"it takes %.3f seconds to perform cross validation\" % (end_time - start_time))\n    return cv_score, train_pred,test_pred\n","metadata":{"_uuid":"68d5fc9f577bf7fae4c9e43d930226bcbc0a801d","_cell_guid":"660c8bfd-b9a5-4229-acfc-a13b5bf9a312","collapsed":true}},{"cell_type":"markdown","source":"# 3. Generate level 1 OOF predictions\nAlmost there to actually generate some level OOF output! last things to do is the prepare our train and test data for our dear machine learning algorithms, and create the StratifiedKFold object","metadata":{"_uuid":"29a83d72b6aef7935fc4d5267cec1429d2bec084","_cell_guid":"d1104350-688d-4432-9892-e7fc70c38d09"}},{"cell_type":"code","outputs":[],"execution_count":null,"source":"drop_cols=['id','target']\ny_train=train['target']\nx_train=train.drop(drop_cols, axis=1)\nx_test=test.drop(['id'], axis=1)","metadata":{"_uuid":"9d710b263b6b573addb8274368f6f901a87a4c5e","_cell_guid":"bae61a0d-f7d1-44b7-9a34-0eeb3a76643e","collapsed":true}},{"cell_type":"markdown","source":"Here, I would like remind you that for stacking, you SHALL use consistent fold distribution at ALL level for ALL your model. The technical reaons for this had been discussed at length in this [forum thread](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/43467)  by our right honourable fellow competitors","metadata":{"_uuid":"5aca7bfdbc70feca18c8a23effb2292bcb0afeaf","_cell_guid":"081c5554-6a18-44ee-9c9f-591b5443a4fd"}},{"cell_type":"code","outputs":[],"execution_count":null,"source":"kf=StratifiedKFold(n_splits=5, shuffle=True, random_state=2017)","metadata":{"_uuid":"d0c8b2aff87ddd31fadba8897176960e4d504141","_cell_guid":"08e8bede-febb-46e3-806a-5b109e7b9e12","collapsed":true}},{"cell_type":"markdown","source":"Right! next generate some level 1 model output...","metadata":{"_uuid":"a89ea82c9c03df8736099c302eb06b5b4066c67b","_cell_guid":"84102ee4-e398-440c-9d2a-c4d11dfa7dba"}},{"cell_type":"markdown","source":"## 3.1 Random Forest\nLet's use the old good random forest algorithm","metadata":{"_uuid":"448a39761296de5394eabfa06eb02d3f21a21ac8","_cell_guid":"ddb8214f-5f76-45fb-88e9-53081fc3f0b9"}},{"cell_type":"code","outputs":[],"execution_count":null,"source":"rf=RandomForestClassifier(n_estimators=200, n_jobs=6, min_samples_split=5, max_depth=7,\n                          criterion='gini', random_state=0)\n\noutcomes =cross_validate_sklearn(rf, x_train, y_train ,x_test, kf, scale=False, verbose=True)\n\nrf_cv=outcomes[0]\nrf_train_pred=outcomes[1]\nrf_test_pred=outcomes[2]\n\nrf_train_pred_df=pd.DataFrame(columns=['prediction_probability'], data=rf_train_pred)\nrf_test_pred_df=pd.DataFrame(columns=['prediction_probability'], data=rf_test_pred)\n","metadata":{"_uuid":"d9d210d8fbb0b8570c74351f9761f3cfbaa2a8a8","_cell_guid":"bcf041f4-ed95-4c41-9d38-ce32cd099f93","collapsed":true}},{"cell_type":"code","outputs":[],"execution_count":null,"source":"","metadata":{"_uuid":"1a56f2a3392f33a243820eb5f1ca86614032cd68","_cell_guid":"e6cb945c-1600-4132-9b9f-cb866cbdb897","collapsed":true}},{"cell_type":"markdown","source":"## 3.2 Extra Tree\nWe love tree! We love more trees, and therefore let's have extra tree :)","metadata":{"_uuid":"0456f0d04285becf31878b71c496b723a60f1904","_cell_guid":"815ebf15-39c6-4dd0-b68f-7cb23c497b4a"}},{"cell_type":"code","outputs":[],"execution_count":null,"source":"et=RandomForestClassifier(n_estimators=100, n_jobs=6, min_samples_split=5, max_depth=5,\n                          criterion='gini', random_state=0)\n\noutcomes =cross_validate_sklearn(et, x_train, y_train ,x_test, kf, scale=False, verbose=True)\n\net_cv=outcomes[0]\net_train_pred=outcomes[1]\net_test_pred=outcomes[2]\n\net_train_pred_df=pd.DataFrame(columns=['prediction_probability'], data=et_train_pred)\net_test_pred_df=pd.DataFrame(columns=['prediction_probability'], data=et_test_pred)","metadata":{"_uuid":"17a987622a889bf904088bc2465072e9307c37b0","_cell_guid":"4065f021-c477-4de9-9853-17522818fec9","collapsed":true}},{"cell_type":"markdown","source":"## 3.3 Logistic Regression\nLet's now throw in our favourite linear friend - Logistic Regression","metadata":{"_uuid":"9329faa24da83c423e2b19969b254847e8e3944a","_cell_guid":"8f18eeb8-f974-4451-a843-8befa3dbea9d"}},{"cell_type":"code","outputs":[],"execution_count":null,"source":"logit=LogisticRegression(random_state=0, C=0.5)\n\noutcomes = cross_validate_sklearn(logit, x_train, y_train ,x_test, kf, scale=True, verbose=True)\n\nlogit_cv=outcomes[0]\nlogit_train_pred=outcomes[1]\nlogit_test_pred=outcomes[2]\n\nlogit_train_pred_df=pd.DataFrame(columns=['prediction_probability'], data=logit_train_pred)\nlogit_test_pred_df=pd.DataFrame(columns=['prediction_probability'], data=logit_test_pred)","metadata":{"_uuid":"5ce672ad798acbbdfdb15ba06e89fe16ddca0672","_cell_guid":"ecd502b5-7e08-4447-93bd-27acee67e73b","collapsed":true}},{"cell_type":"markdown","source":"## 3.4 BernoulliNB\nA little bit of diversity from Naive Bayes never heard, this one of those algorithms that normally don't generate sigle output that rival XGB/LGB, but nevertheless help to improve the overal stacking performance due the diversity it bring to the party","metadata":{"_uuid":"b39561df8d66c702496a6837ff1a58a4d36b8774","_cell_guid":"9bf6a4bb-78de-4e32-8aab-fec9bf9f1aa5"}},{"cell_type":"code","outputs":[],"execution_count":null,"source":"nb=BernoulliNB()\n\noutcomes =cross_validate_sklearn(nb, x_train, y_train ,x_test, kf, scale=True, verbose=True)\n\nnb_cv=outcomes[0]\nnb_train_pred=outcomes[1]\nnb_test_pred=outcomes[2]\n\nnb_train_pred_df=pd.DataFrame(columns=['prediction_probability'], data=nb_train_pred)\nnb_test_pred_df=pd.DataFrame(columns=['prediction_probability'], data=nb_test_pred)","metadata":{"_uuid":"9d2174a4cf1e03f5f4c0664c14dce5ac7e04932d","_cell_guid":"4395e5e4-1264-40f3-af2a-ed3496162bee","collapsed":true}},{"cell_type":"markdown","source":"## 3.5 XGB\nNow this is our go-to GBM Bazooka:","metadata":{"_uuid":"8dda4f8fea6f5484c457b136d5681e75b1cbf5d2","_cell_guid":"524e0dab-60c7-449d-8d73-51265f0d02f9"}},{"cell_type":"code","outputs":[],"execution_count":null,"source":"xgb_params = {\n    \"booster\"  :  \"gbtree\", \n    \"objective\"         :  \"binary:logistic\",\n    \"tree_method\": \"hist\",\n    \"eval_metric\": \"auc\",\n    \"eta\": 0.1,\n    \"max_depth\": 5,\n    \"min_child_weight\": 10,\n    \"gamma\": 0.70,\n    \"subsample\": 0.76,\n    \"colsample_bytree\": 0.95,\n    \"nthread\": 6,\n    \"seed\": 0,\n    'silent': 1\n}\n\noutcomes=cross_validate_xgb(xgb_params, x_train, y_train, x_test, kf, use_rank=False, verbose_eval=False)\n\nxgb_cv=outcomes[0]\nxgb_train_pred=outcomes[1]\nxgb_test_pred=outcomes[2]\n\nxgb_train_pred_df=pd.DataFrame(columns=['prediction_probability'], data=xgb_train_pred)\nxgb_test_pred_df=pd.DataFrame(columns=['prediction_probability'], data=xgb_test_pred)","metadata":{"_uuid":"937922adeb5fbf7811be2fdde2979d79a9b7e4b3","_cell_guid":"cfe1eb87-99cf-40c1-974f-f96101573cd4","collapsed":true}},{"cell_type":"markdown","source":"## 3.6 LightGBM\nThere is a crack in everything, that's how the light gets in :) ","metadata":{"_uuid":"eb19b04c2289ea9b2b23280d1938aacc1eff2fda","_cell_guid":"75ee3c6a-5ad3-4c18-8658-a42c194e884e"}},{"cell_type":"code","outputs":[],"execution_count":null,"source":"lgb_params = {\n    'task': 'train',\n    'boosting_type': 'dart',\n    'objective': 'binary',\n    'metric': {'auc'},\n    'num_leaves': 22,\n    'min_sum_hessian_in_leaf': 20,\n    'max_depth': 5,\n    'learning_rate': 0.1,  # 0.618580\n    'num_threads': 6,\n    'feature_fraction': 0.6894,\n    'bagging_fraction': 0.4218,\n    'max_drop': 5,\n    'drop_rate': 0.0123,\n    'min_data_in_leaf': 10,\n    'bagging_freq': 1,\n    'lambda_l1': 1,\n    'lambda_l2': 0.01,\n    'verbose': 1\n}\n\n\ncat_cols=['ps_ind_02_cat','ps_car_04_cat', 'ps_car_09_cat','ps_ind_05_cat', 'ps_car_01_cat']\noutcomes=cross_validate_lgb(lgb_params,x_train, y_train ,x_test,kf, cat_cols, use_cat=True, \n                            verbose_eval=False, use_rank=False)\n\nlgb_cv=outcomes[0]\nlgb_train_pred=outcomes[1]\nlgb_test_pred=outcomes[2]\n\nlgb_train_pred_df=pd.DataFrame(columns=['prediction_probability'], data=lgb_train_pred)\nlgb_test_pred_df=pd.DataFrame(columns=['prediction_probability'], data=lgb_test_pred)","metadata":{"_uuid":"63881261ab97d934403ac3760400d7c511b0247d","_cell_guid":"b742c028-171e-4a63-95d5-4c1689b5a8a3","collapsed":true}},{"cell_type":"markdown","source":"We now have our level 1 friends ready, lets proceed and send them into the stacking party!","metadata":{"_uuid":"2882f053de9e46731f9cb54f1dcd2ba50a1a16c5","_cell_guid":"873a0a34-6b8d-4c10-b4d0-e56f167fe4bf"}},{"cell_type":"markdown","source":"# 4. Level 2 ensemble","metadata":{"_uuid":"6eae249c862c8c70b8dd1eab00a7161c7afe9b21","_cell_guid":"53506626-7455-487c-9004-ec1fef5f9950"}},{"cell_type":"markdown","source":"## 4.1 Generate L1 output dataframe\nLet's group ouf level 1 OOF predictions output together to genenerate the input for level 2 stacking","metadata":{"_uuid":"f2f75805fa76689520a8962d7e30e43da0dbd3e1","_cell_guid":"b2a72388-1033-45b4-8ee5-765e7c3ea62d"}},{"cell_type":"code","outputs":[],"execution_count":null,"source":"columns=['rf','et','logit','nb','xgb','lgb']\ntrain_pred_df_list=[rf_train_pred_df, et_train_pred_df, logit_train_pred_df, nb_train_pred_df,\n                    xgb_train_pred_df, lgb_train_pred_df]\n\ntest_pred_df_list=[rf_test_pred_df, et_test_pred_df, logit_test_pred_df, nb_test_pred_df,\n                    xgb_test_pred_df, lgb_test_pred_df]\n\nlv1_train_df=pd.DataFrame(columns=columns)\nlv1_test_df=pd.DataFrame(columns=columns)\n\nfor i in range(0,len(columns)):\n    lv1_train_df[columns[i]]=train_pred_df_list[i]['prediction_probability']\n    lv1_test_df[columns[i]]=test_pred_df_list[i]['prediction_probability']\n\n","metadata":{"_uuid":"58fdabc3681ff22f66b8d28788d758421206ae51","_cell_guid":"37c8de30-8b9a-4e6a-98ba-28aba0ab140c","collapsed":true}},{"cell_type":"markdown","source":"## 4.2 Level 2 XGB\nBack to XGB for level 2! everything shall be the same, paint old easy mdoel building, right? well..","metadata":{"_uuid":"18338e86f1ac79a492626c246d220350b8e72cc2","_cell_guid":"d7fda480-9b13-49ad-b8bb-47d71e7a70a0"}},{"cell_type":"code","outputs":[],"execution_count":null,"source":"xgb_lv2_outcomes=cross_validate_xgb(xgb_params, lv1_train_df, y_train, lv1_test_df, kf, \n                                          verbose=True, verbose_eval=False, use_rank=False)\n\nxgb_lv2_cv=xgb_lv2_outcomes[0]\nxgb_lv2_train_pred=xgb_lv2_outcomes[1]\nxgb_lv2_test_pred=xgb_lv2_outcomes[2]","metadata":{"_uuid":"93a7284549332ac014d58fcaf11ce4bd7ecb2b30","_cell_guid":"a2571a87-805c-40b6-b443-11bbe01491b3","collapsed":true}},{"cell_type":"markdown","source":"So what just happened there?  Our CV score for each training fold is pretty descent, but our overall training CV score just fell through the crack!  Well, it turns out since we are using AUC/Gini as metric which is ranking dependent, and it turns out that if you apply xgb and lgb at level 2 stacking, the ranking get messed up when each fold's prediction scores are put together.  And this goes back to why we implemented that function to convert probability into ranks earlier.\n\nNow, let's use the *use_rank* option, and see what happens:","metadata":{"_uuid":"284cc22e8e855eefbe2cee1f1eef91d3196411d5","_cell_guid":"0e10f37d-04c2-4c4f-b101-7e5c24f898ba"}},{"cell_type":"code","outputs":[],"execution_count":null,"source":"xgb_lv2_outcomes=cross_validate_xgb(xgb_params, lv1_train_df, y_train, lv1_test_df, kf, \n                                          verbose=True, verbose_eval=False, use_rank=True)\n\nxgb_lv2_cv=xgb_lv2_outcomes[0]\nxgb_lv2_train_pred=xgb_lv2_outcomes[1]\nxgb_lv2_test_pred=xgb_lv2_outcomes[2]","metadata":{"_uuid":"a917e434256ffb0d7f902b2bbfdeeaccc3027b48","_cell_guid":"55d4e233-d879-41d2-ad08-b9ed8930bda4","collapsed":true}},{"cell_type":"markdown","source":"Much better, the OOF score for train prediction looks great! and you can see the score here is better already than any of the level 1 OOF train score. The best score in level 1 comes from XGB with 0.282 region, and we are now on 0.284","metadata":{"_uuid":"b3d261ef2a978d3e5146628bdf9147383b3e18a0","_cell_guid":"7ad65b10-2aa5-4000-80ce-28cd0a983ead"}},{"cell_type":"markdown","source":" ## 4.3 Level 2 LightGBM\nSame story for LightGBM at level 2, we need to use the *use_rank* option:","metadata":{"_uuid":"d1ab11ac7fff586b60057b0ab4f69b923994643d","_cell_guid":"472eba65-4179-4815-93e0-47ff4ae6cd6e"}},{"cell_type":"code","outputs":[],"execution_count":null,"source":"lgb_lv2_outcomes=cross_validate_lgb(lgb_params,lv1_train_df, y_train ,lv1_test_df,kf, [], use_cat=False, \n                                    verbose_eval=False, use_rank=True)\n\nlgb_lv2_cv=xgb_lv2_outcomes[0]\nlgb_lv2_train_pred=lgb_lv2_outcomes[1]\nlgb_lv2_test_pred=lgb_lv2_outcomes[2]","metadata":{"_uuid":"175f084b769733d7e77898be13f1e2832025ed14","_cell_guid":"71e744c1-7401-4860-aa1d-ea53d2d5fe9d","collapsed":true}},{"cell_type":"markdown","source":"No surprise here :)","metadata":{"_uuid":"bba2b9bde24b635421bc8763a8991302fd206b35","_cell_guid":"f3d2b4f5-c10b-46c5-9de2-03720822c8f6"}},{"cell_type":"markdown","source":"## 4.3 Level 2 Random Forest\nNow let's try a few more algorithms on level 2, and let's revisit random forest again.","metadata":{"_uuid":"903cc2e466be61ba82652b377fdeb6091642d2a4","_cell_guid":"02362546-a5ea-4fdd-a494-cfae659d2753"}},{"cell_type":"code","outputs":[],"execution_count":null,"source":"rf_lv2=RandomForestClassifier(n_estimators=200, n_jobs=6, min_samples_split=5, max_depth=7,\n                          criterion='gini', random_state=0)\nrf_lv2_outcomes = cross_validate_sklearn(rf_lv2, lv1_train_df, y_train ,lv1_test_df, kf, \n                                            scale=True, verbose=True)\nrf_lv2_cv=rf_lv2_outcomes[0]\nrf_lv2_train_pred=rf_lv2_outcomes[1]\nrf_lv2_test_pred=rf_lv2_outcomes[2]","metadata":{"_uuid":"4ea07ca9bc4d354c318e878862528324398222b1","_cell_guid":"93ba02c4-799d-499b-bc80-327f84198fe3","collapsed":true}},{"cell_type":"markdown","source":"## 4.4 Level 2 Logistic Regression\nLogistic Regression, take 2","metadata":{"_uuid":"7f9f35db4c34140cf2f55d066b4f54ab0208ef32","_cell_guid":"7fbfcdd7-1a5e-4189-94ba-f66c7349156d"}},{"cell_type":"code","outputs":[],"execution_count":null,"source":"logit_lv2=LogisticRegression(random_state=0, C=0.5)\nlogit_lv2_outcomes = cross_validate_sklearn(logit_lv2, lv1_train_df, y_train ,lv1_test_df, kf, \n                                            scale=True, verbose=True)\nlogit_lv2_cv=logit_lv2_outcomes[0]\nlogit_lv2_train_pred=logit_lv2_outcomes[1]\nlogit_lv2_test_pred=logit_lv2_outcomes[2]","metadata":{"_uuid":"63f07ed58d6303bbc80e6656478200ded8910507","_cell_guid":"d2bef002-a7f1-4474-b29c-e3e462cd99c9","collapsed":true}},{"cell_type":"markdown","source":"Hopefully by now you can see that on level 2, models like random forest and logistic regression are now producing very competivie results thanks to the meta-features from the level 1 OOF output.\n\nWe are having fun! and why stop in level 2? let's bring on level 3 :)","metadata":{"_uuid":"525aa3cdc796f92589e9c3992fd4d35e1773d4cf","_cell_guid":"dd417c2d-89a9-4e95-9606-86ea6a626008"}},{"cell_type":"markdown","source":"# 5. Level 3 ensemble\nOn level 3, we follow simlar workflow as level 2. First we put the OOF output from level 2 together, and then send them to our chosen algorithms.\n\n## 5.1 Generate L2 output dataframe","metadata":{"_uuid":"b45cae4fbebe5034730ada219dafdb13397455bc","_cell_guid":"19098161-0329-496a-b634-05a31f114376"}},{"cell_type":"code","outputs":[],"execution_count":null,"source":"lv2_columns=['rf_lf2', 'logit_lv2', 'xgb_lv2','lgb_lv2']\ntrain_lv2_pred_list=[rf_lv2_train_pred, logit_lv2_train_pred, xgb_lv2_train_pred, lgb_lv2_train_pred]\n\ntest_lv2_pred_list=[rf_lv2_test_pred, logit_lv2_test_pred, xgb_lv2_test_pred, lgb_lv2_test_pred]\n\nlv2_train=pd.DataFrame(columns=lv2_columns)\nlv2_test=pd.DataFrame(columns=lv2_columns)\n\nfor i in range(0,len(lv2_columns)):\n    lv2_train[lv2_columns[i]]=train_lv2_pred_list[i]\n    lv2_test[lv2_columns[i]]=test_lv2_pred_list[i]","metadata":{"_uuid":"05687284bf3603330e718b181de02661b5bc7f88","_cell_guid":"d3f1a9e1-a19c-4fd8-8e84-51d01ca917d2","collapsed":true}},{"cell_type":"markdown","source":"## 5.2 Level 3 XGB \nOn this level, let's just stay with our trusted weapon XGB","metadata":{"_uuid":"7845485550ae94ba0d696481c00afd00fe4089cc","_cell_guid":"cd4e5284-f9eb-437f-ba05-4724b431a456"}},{"cell_type":"code","outputs":[],"execution_count":null,"source":"xgb_lv3_params = {\n    \"booster\"  :  \"gbtree\", \n    \"objective\"         :  \"binary:logistic\",\n    \"tree_method\": \"hist\",\n    \"eval_metric\": \"auc\",\n    \"eta\": 0.1,\n    \"max_depth\": 2,\n    \"min_child_weight\": 10,\n    \"gamma\": 0.70,\n    \"subsample\": 0.76,\n    \"colsample_bytree\": 0.95,\n    \"nthread\": 6,\n    \"seed\": 0,\n    'silent': 1\n}\n\n\n\nxgb_lv3_outcomes=cross_validate_xgb(xgb_lv3_params, lv2_train, y_train, lv2_test, kf, \n                                          verbose=True, verbose_eval=False, use_rank=True)\n\nxgb_lv3_cv=xgb_lv3_outcomes[0]\nxgb_lv3_train_pred=xgb_lv3_outcomes[1]\nxgb_lv3_test_pred=xgb_lv3_outcomes[2]","metadata":{"_uuid":"c3efb88090bda634c06a5216df312770191acc1f","_cell_guid":"f9102031-44a1-4bda-95ba-a2d98cfc3a0f","collapsed":true}},{"cell_type":"markdown","source":"This is slightly better than the XGB ouput at level 2, but not by much, as we are now seeing diminsing return as the level improve. Let's try tp pair this with something linear.","metadata":{"_uuid":"07d4a195c73b0671fb95520317c48060b294656d","_cell_guid":"085da573-181e-42ba-bc0d-3776553945a3"}},{"cell_type":"markdown","source":"## 5.3 Level 3 Logistic Regression\nand of course that something linear is going to be Logistic Regression","metadata":{"_uuid":"39f8348e5c883b9a587fc98ba131af0f12101e10","_cell_guid":"1dad51db-30a2-4b8f-ba1c-0ad9e5b86fd8"}},{"cell_type":"code","outputs":[],"execution_count":null,"source":"logit_lv3=LogisticRegression(random_state=0, C=0.5)\nlogit_lv3_outcomes = cross_validate_sklearn(logit_lv3, lv2_train, y_train ,lv2_test, kf, \n                                            scale=True, verbose=True)\nlogit_lv3_cv=logit_lv3_outcomes[0]\nlogit_lv3_train_pred=logit_lv3_outcomes[1]\nlogit_lv3_test_pred=logit_lv3_outcomes[2]","metadata":{"_uuid":"360e43b3b87ee793b3c27e60d174134a365d402c","_cell_guid":"f1c910d7-8f6e-458f-b861-304ac288fe62","collapsed":true}},{"cell_type":"markdown","source":"At this level, we don't see that much different between XGB and Logistic Regression anymore.","metadata":{"_uuid":"a516769a3a8404cf615ea1cf320575c22572cf7b","_cell_guid":"280720bf-0b4c-4bfd-95d8-00186323ab06"}},{"cell_type":"markdown","source":"## 5.4 Average L3 outputs & Submission Generation","metadata":{"_uuid":"934040c62cd513b09bd0af3689a1c0285861a051","_cell_guid":"c333fd59-6bf5-4350-a76f-bfcba67f67a3"}},{"cell_type":"markdown","source":"We can always still do a simple weight average, to bring the two together and see if there any extra juice to be squeezed","metadata":{"_uuid":"ff9a2ce74ad2885a27c22afaf799f034825288f6","_cell_guid":"2492c644-af71-4351-97dc-0d6885f66dbe"}},{"cell_type":"code","outputs":[],"execution_count":null,"source":"weight_avg=logit_lv3_train_pred*0.5+ xgb_lv3_train_pred*0.5\nprint(auc_to_gini_norm(roc_auc_score(y_train, weight_avg)))","metadata":{"_uuid":"2bd20f0d24d9932e80fe80301d1b09cf96f3fe3e","_cell_guid":"4893f0ae-4e9c-4f64-b957-8b3047cf245c","collapsed":true}},{"cell_type":"markdown","source":"Well, for training score, we manage to arravie at 0.28443.\nWe can now try to apply the same weight distribution to generate our submission.","metadata":{"_uuid":"3dc6cf4250515a287495432336795d94d7b4ce92","_cell_guid":"9215e016-6174-4e09-9d30-f8b57cfa1c3d"}},{"cell_type":"code","outputs":[],"execution_count":null,"source":"submission=sample_submission.copy()\nsubmission['target']=logit_lv3_test_pred*0.5+ xgb_lv3_test_pred*0.5\nfilename='stacking_demonstration.csv.gz'\nsubmission.to_csv(filename,compression='gzip', index=False)","metadata":{"_uuid":"3821ea872e98e93c69fe6519d8d5deedc6868090","_cell_guid":"c589c9cb-9f49-44af-b97a-65a4697ae3e3","collapsed":true}},{"cell_type":"markdown","source":"# 6 After Thought","metadata":{"_uuid":"30f28bb0811a8dfbebdc29cdd1795f57d2ccbf34","_cell_guid":"92ca905f-da5f-4b10-ad2b-22b151034a2e"}},{"cell_type":"markdown","source":"So I hope this three-level stacking guide is useful to demonstrate how you can capture more information from the training data, and hopefully this can generate to better test prediction. Well I use the word \"hopefully\" here as we all know the dataset in this competition is pretty noisy.   and in truth I am not sure if stacking beyong 2 level would bring much benefit, but then we will learn from the highflyers who survive the shake up!\n\nMy personal likely strategy to approach stacking is probably:\n* go with a 2-level approach, and weight average on level 2\n* applying the same stacking routine to several different random seed. \n* weigh average the above\n\nI seriously think robust CV is the key for this competition, and always be suspicious of all things shared on kernel. Alternatively, perhaps with 1 day to go, someone will share a leak or a 0.291 script to send us into a frenzy? One can always hope...\n\nEnjoy every bit of these last days - May all the insured drivers never have to claim! :)\n","metadata":{"_uuid":"439cefee307a58912106d186025fd5272e06edd3","_cell_guid":"d8afa058-4b3c-4bbd-82b8-ac574b0469a6"}}],"metadata":{"language_info":{"nbconvert_exporter":"python","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.3","name":"python","mimetype":"text/x-python","pygments_lexer":"ipython3"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}}}