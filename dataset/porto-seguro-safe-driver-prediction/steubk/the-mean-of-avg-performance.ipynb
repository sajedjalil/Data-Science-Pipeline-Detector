{"nbformat_minor":1,"metadata":{"language_info":{"mimetype":"text/x-python","nbconvert_exporter":"python","name":"python","file_extension":".py","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.3"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"source":"A naive simulation of how  size of test set can affect the models average performance.\n\nI stole the starting idea from the amazing kernel [Is Your Small Gini Significant?](https://www.kaggle.com/vpaslay/is-your-small-gini-significant) of [Victor Paslay](Victor Paslay)\n\nIt seems that the bigger is the test set the more the average of models over perform single models\nAs usual any criticism, suggestions and hints are very welcomed. \n\n\n\n\n","cell_type":"markdown","metadata":{}},{"source":"import numpy as np\nfrom sklearn.metrics import roc_auc_score\n\nclass Sim:\n    def __init__(self, length, seed, name):\n        self.name = name\n        self.LENGTH = length\n        self.PRIV_PUB_CUT = int(self.LENGTH * 0.3)\n        np.random.seed(seed)\n        self.PERFECT_SUB = np.random.rand(self.LENGTH)\n        #Assumption\n        #Imbalance of positive and negative classes in the test set is the same as in the training set\n        #see https://www.kaggle.com/vpaslay/is-your-small-gini-significant\n        self.TARGET = (self.PERFECT_SUB > 0.963552).astype(dtype=int)\n\n    def gini(self,y_target, y_score):\n        return 2 * roc_auc_score(y_target, y_score) - 1\n\n    def gini_private(self,y_score):\n        return self.gini(self.TARGET[self.PRIV_PUB_CUT:], y_score[self.PRIV_PUB_CUT:])\n\n    def gini_public(self, y_score):\n        return self.gini(self.TARGET[:self.PRIV_PUB_CUT], y_score[:self.PRIV_PUB_CUT])\n\n    def evaluate_sub(self,sub):\n        return self.gini (self.TARGET, sub ), self.gini_public ( sub ), self.gini_private ( sub )\n\n\n    def evaluate_subs (self, subs):\n        samples= subs.shape[1] \n        results = np.zeros((samples,3))\n\n        for i in range ( samples ):\n            sub = subs[:,i]\n            results[ i, : ] = np.array( self.evaluate_sub(sub) )     \n\n        return results\n\n    def create_random_sub (self, naive_target):\n\n        random_sub =  np.random.rand(self.LENGTH) \n        _t = ( np.random.rand(self.LENGTH) >  naive_target ).astype(dtype=int)\n\n\n        return self.PERFECT_SUB + _t*(random_sub-self.PERFECT_SUB)\n\n\n    def create_semi_random_subs (self, naive_target, noise=0.02, samples=5):\n        #the naive assumption\n        _t = ( np.random.rand(self.LENGTH) >  naive_target   ).astype(dtype=int)\n\n        subs = np.zeros((self.LENGTH,samples))\n\n        for i in range (samples):\n\n            _n = np.maximum(_t,( np.random.rand(self.LENGTH) > 1.0 - noise ).astype(dtype=int))\n\n            random_sub =  np.random.rand(self.LENGTH) \n\n\n            random_sub = self.PERFECT_SUB + _n*(random_sub - self.PERFECT_SUB)\n\n            subs [:, i] =  random_sub\n\n        return subs\n\n    \nTESTSET_LENGTH = 595212\n    \nsim_testset = Sim(TESTSET_LENGTH, seed=2017, name= \"Testset\")\nsim_half_testset = Sim(int(TESTSET_LENGTH/2), seed=2017, name = \"Half a testset\")\nsim_doubled_testset = Sim(2*TESTSET_LENGTH, seed=2017, name=\"Doubled testset\")\n\nsimulations = [ sim_half_testset,sim_testset, sim_doubled_testset]\n\nfor sim in simulations:\n    \n    print(sim.name)\n    print(\"\\tgini for perfect score: {:f}\".format(sim.gini ( sim.TARGET, sim.PERFECT_SUB)) )\n\n    m = sim.evaluate_sub (sim.create_random_sub(0.28))\n    print(\"\\trandom sub\")\n    print(\"\\tgini : {:f} {:f} {:f}\".format ( m[0], m[1], m[2] ))\n\n    print(\"\\t10 semi random subs\")\n\n    subs = sim.create_semi_random_subs (0.284,  noise=0.05, samples=10)\n    avg_subs = np.mean(subs,axis=1)\n\n    m = sim.evaluate_sub (avg_subs)\n    print(\"\\tavg gini: {:f} {:f} {:f}\".format ( m[0], m[1], m[2] ))\n    mean=np.mean(sim.evaluate_subs(subs),axis=1)\n    print (\"\\t    gini: {:f} {:f} {:f}\".format( mean[0], mean[1], mean[2]))\n","cell_type":"code","metadata":{"_uuid":"7ec813c52c64408e40a85caf08ebc0650944ae79","_cell_guid":"89443289-b883-4d9c-81bc-f03e745bc384"},"execution_count":null,"outputs":[]},{"source":"import matplotlib.pyplot as plt\n%matplotlib inline  \n\n\nl=100\nnaive_target=0.28\nnoise=0.01\nsamples=5\n\n\nfor sim in simulations:\n    \n\n    avg_sub_res = np.zeros((l,3))\n    single_sub_res = np.zeros((l*samples,3))\n    avg_subs = np.zeros((sim.LENGTH,l))\n\n    for i in range(l):\n        semi_random_subs = sim.create_semi_random_subs (naive_target, noise=noise, samples=samples)\n\n        for j in range(samples):\n            single_sub_res[5*i+j,:] = sim.evaluate_sub (semi_random_subs[:,j])\n\n        avg_subs [:, i] = np.mean(semi_random_subs,axis=1)\n        avg_sub_res [i,:] = sim.evaluate_sub (avg_subs[:,i])\n\n\n    plt.figure(figsize=(5,5))\n    plt.title(sim.name)\n\n\n    plt.scatter(single_sub_res[:,1], single_sub_res[:,2], marker='o', color='r',alpha=0.7,label='single sub')\n    plt.scatter(avg_sub_res[:,1], avg_sub_res[:,2], marker='x', color='b',alpha=0.7,label='5 subs avg')\n\n\n    plt.ylabel('Private LB')\n    plt.xlabel('Public LB')\n    plt.legend(loc='lower right')\n    plt.show()\n\n    print(\"single sub mean\")\n    print (np.mean(single_sub_res))\n\n    print(\"5 subs mean\")\n    print (np.mean(avg_sub_res))\n","cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"source":"","cell_type":"code","metadata":{"collapsed":true},"execution_count":null,"outputs":[]}],"nbformat":4}