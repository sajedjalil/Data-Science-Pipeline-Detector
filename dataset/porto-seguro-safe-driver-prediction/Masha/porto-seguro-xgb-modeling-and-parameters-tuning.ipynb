{"cells":[{"outputs":[],"source":"%load_ext autoreload\n%autoreload 2\n\nimport matplotlib.pylab as plt\n%matplotlib inline\nfrom matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 12, 4\n%matplotlib inline\n\nimport pandas as pd\n\nimport numpy as np\n\nimport sklearn\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import recall_score\nfrom sklearn import metrics\n\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier\n\nfrom IPython.display import display\n\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","metadata":{"_cell_guid":"34f4e4e4-6737-4777-bd91-3e4115f6e072","_uuid":"a93c21205bea554b6d322b9124549199851113a5"},"execution_count":null,"cell_type":"code"},{"source":"**GINI calculations**","metadata":{},"cell_type":"markdown"},{"outputs":[],"source":"def gini(actual, pred, cmpcol = 0, sortcol = 1):\n    assert( len(actual) == len(pred) )\n    all = np.asarray(np.c_[ actual, pred, np.arange(len(actual)) ], dtype=np.float)\n    all = all[ np.lexsort((all[:,2], -1*all[:,1])) ]\n    totalLosses = all[:,0].sum()\n    giniSum = all[:,0].cumsum().sum() / totalLosses\n    \n    giniSum -= (len(actual) + 1) / 2.\n    return giniSum / len(actual)\n\ndef gini_normalized(a, p):\n    return gini(a, p) / gini(a, a)\n\ndef gini_xgb(preds, dtrain):\n    labels = dtrain.get_label()\n    gini_score = gini_normalized(labels, preds)\n    return [('gini', gini_score)]","metadata":{"collapsed":true},"execution_count":null,"cell_type":"code"},{"outputs":[],"source":"PATH = \"../input/train.csv\"\ndata_raw= pd.read_csv(f'{PATH}', low_memory=False)","metadata":{"collapsed":true},"execution_count":null,"cell_type":"code"},{"outputs":[],"source":"def display_all(df):\n    with pd.option_context(\"display.max_rows\", 1000): \n        with pd.option_context(\"display.max_columns\", 1000): \n            display(df)\ndisplay_all(data_raw.head(5))","metadata":{},"execution_count":null,"cell_type":"code"},{"outputs":[],"source":"# Describe the data set\ndisplay_all(data_raw.describe(include='all'))","metadata":{},"execution_count":null,"cell_type":"code"},{"source":"**Important observations:**\n- Target variable (*target*) has its mean 3.65% meaning that only 3.65% of data is classified as the target. Hence, the dataset is unbalanced","metadata":{},"cell_type":"markdown"},{"outputs":[],"source":"# Distribution of target variable\nimport matplotlib.pyplot as plt\nplt.hist(data_raw['target'])\nplt.show()\n\nprint('Percentage of claims filed :' , str(np.sum(data_raw['target'])/data_raw.shape[0]*100), '%')","metadata":{},"execution_count":null,"cell_type":"code"},{"source":"But before jumping into balancing the data let's **look at the NA's, which in this dataset are denoted as -1**.","metadata":{},"cell_type":"markdown"},{"outputs":[],"source":"nas = np.sum(data_raw == -1)/len(data_raw) *100\nprint(\"The percentage of missing values is\")\nprint (nas[nas>0].sort_values(ascending = False))","metadata":{},"execution_count":null,"cell_type":"code"},{"source":"Now, for categorical variables we will create dummy variables (aka **one-hot encoding**)","metadata":{},"cell_type":"markdown"},{"outputs":[],"source":"# make a copy of the initial dataset\ndata_clean = data_raw.copy()\n#data_clean.columns\ncat_cols = [c for c in data_clean.columns if c.endswith('cat')]\nfor column in cat_cols:\n    temp=pd.get_dummies(data_clean[column], prefix=column, prefix_sep='_')\n    data_clean=pd.concat([data_clean,temp],axis=1)\n    data_clean=data_clean.drop([column],axis=1)\n\nprint('data_clean shape is:',data_clean.shape)","metadata":{},"execution_count":null,"cell_type":"code"},{"outputs":[],"source":"# Impute missing values with medians\n\nnum_cols = ['ps_reg_03','ps_car_14', 'ps_car_11', 'ps_car_12' ]\n\nfor n in num_cols:\n    dummy_name = str(n) + 'NA'\n    data_clean[dummy_name] = (data_clean[n]==-1).astype(int)\n    med = data_clean[data_clean[n]!=-1][n].median()\n    data_clean.loc[data_clean[n]==-1,n] = med\n    \n\n    ","metadata":{"collapsed":true},"execution_count":null,"cell_type":"code"},{"outputs":[],"source":"#Make transformation to ps_car_13, as suggested here: https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/41489\ndata_clean['ps_car_13_trans'] = round(data_clean['ps_car_13']* data_clean['ps_car_13']* 90000,2)","metadata":{"collapsed":true},"execution_count":null,"cell_type":"code"},{"source":"**Undersampling**:\nLet's take 25% of abundant data (target = 0) and stack together with the rare data (target = 1)","metadata":{},"cell_type":"markdown"},{"outputs":[],"source":"sub_df_0= data_clean[(data_clean['target']==0)]\nsub_df_1= data_clean[(data_clean['target']==1)]\nsub_df_1.shape","metadata":{},"execution_count":null,"cell_type":"code"},{"outputs":[],"source":"sub_df = sub_df_0.sample(frac = 0.25, random_state = 42)\ndata_sub = pd.concat([sub_df_1,sub_df])","metadata":{"collapsed":true},"execution_count":null,"cell_type":"code"},{"source":"## XGBoost model","metadata":{},"cell_type":"markdown"},{"outputs":[],"source":"# First split the data into training and validation (test) sets\ntraining_features, test_features, \\\ntraining_target, test_target, = train_test_split(data_sub.drop(['id','target'], axis=1),\n                                               data_sub['target'],\n                                               test_size = .2,\n                                               random_state=12)\n\n# Now further split the training test into training and validation to \nx_train, x_val, y_train, y_val = train_test_split(training_features, training_target,\n                                                  test_size = .2,\n                                                  random_state=12)","metadata":{"collapsed":true},"execution_count":null,"cell_type":"code"},{"source":"We first run an XGB trainng model with some baseline parameters and then proceed to tuning the key parameters via a series of loops. \n\nTo grasp an idea of what parameters to tune and in what order, look here:\n- https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n- https://www.slideshare.net/odsc/owen-zhangopen-sourcetoolsanddscompetitions1?next_slideshow=1","metadata":{},"cell_type":"markdown"},{"source":"Since it takes time (10-30) minutes to run loops, I'll comment out the code and hardcode the results below each code cell with a loop. To actually run the code, please uncomment it.","metadata":{},"cell_type":"markdown"},{"outputs":[],"source":"xgb_params = {'eta': 0.02, \n              'max_depth': 6, \n              'subsample': 1.0, \n              'colsample_bytree': 0.3,\n              'min_child_weight': 1,\n              'objective': 'binary:logistic', \n              'eval_metric': 'auc', \n              'seed': 99, \n              'silent': True}\nd_train = xgb.DMatrix(x_train, y_train)\nd_valid = xgb.DMatrix(x_val,y_val)\nd_test = xgb.DMatrix(test_features)\n\nwatchlist = [(d_train, 'train'), (d_valid, 'valid')]\n#model = xgb.train(xgb_params, d_train, 1000,  watchlist, feval=gini_xgb, maximize=True, verbose_eval=100, early_stopping_rounds=200)\n#print(model.best_score, model.best_iteration, model.best_ntree_limit)","metadata":{"collapsed":true},"execution_count":null,"cell_type":"code"},{"source":"[0]\ttrain-gini:0.196601\tvalid-gini:0.19429\n\nMultiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n\nWill train until valid-gini hasn't improved in 200 rounds.\n\n[100]\ttrain-gini:0.321987\tvalid-gini:0.278464\n\n[200]\ttrain-gini:0.359511\tvalid-gini:0.285658\n\n[300]\ttrain-gini:0.392516\tvalid-gini:0.289204\n\n[400]\ttrain-gini:0.419991\tvalid-gini:0.289849\n\n[500]\ttrain-gini:0.443408\tvalid-gini:0.291014\n\n[600]\ttrain-gini:0.458003\tvalid-gini:0.290918\n\n[700]\ttrain-gini:0.473734\tvalid-gini:0.290754\n\n[800]\ttrain-gini:0.487845\tvalid-gini:0.290586\n\nStopping. Best iteration:\n[\n648]\ttrain-gini:0.465592\tvalid-gini:0.291336\n\n0.291336 648 649","metadata":{},"cell_type":"markdown"},{"outputs":[],"source":"#results = {'best_score':[],'best_iter':[],'best_ntree_limit':[]}","metadata":{"_kg_hide-output":true,"_kg_hide-input":false,"collapsed":true},"execution_count":null,"cell_type":"code"},{"source":"train-gini:0.465592\tvalid-gini:0.291336\nBest_ntree_limit = 649\n\nNow, let's tune the **learning rate (eta)**","metadata":{},"cell_type":"markdown"},{"outputs":[],"source":"results = {'eta':[],'best_score':[],'best_ntree_limit':[]}\nfor e in [0.01, 0.02, 0.03,0.05,0.1,0.2]:\n    xgb_params = {'eta': e, \n                  'max_depth': 6, \n                  'subsample': 1.0, \n                  'colsample_bytree': 0.3,\n                  'min_child_weight': 1,\n                  'objective': 'binary:logistic', \n                  'seed': 99, \n                  'silent': True}\n\n    watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n   # m = xgb.train(xgb_params, d_train, 1000,  watchlist, feval=gini_xgb, maximize=True, verbose_eval=100, early_stopping_rounds=200)\n    #results['best_score'].append(m.best_score)\n    #results['best_ntree_limit'].append(m.best_ntree_limit)\n    #results['eta'].append(e)\n    \n#print('eta:',results['eta'],'best_score:',results['best_score'],'best_ntree_limit:', results['best_ntree_limit'])","metadata":{"collapsed":true},"execution_count":null,"cell_type":"code"},{"source":"[0]\ttrain-gini:0.196601\tvalid-gini:0.19429\nMultiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n\nWill train until valid-gini hasn't improved in 200 rounds.\n[100]\ttrain-gini:0.306974\tvalid-gini:0.276946\n[200]\ttrain-gini:0.324915\tvalid-gini:0.279907\n[300]\ttrain-gini:0.341885\tvalid-gini:0.284432\n[400]\ttrain-gini:0.360752\tvalid-gini:0.286026\n[500]\ttrain-gini:0.379185\tvalid-gini:0.2878\n[600]\ttrain-gini:0.394244\tvalid-gini:0.288596\n[700]\ttrain-gini:0.408409\tvalid-gini:0.289171\n[800]\ttrain-gini:0.420627\tvalid-gini:0.289688\n[900]\ttrain-gini:0.433574\tvalid-gini:0.289878\n[0]\ttrain-gini:0.196601\tvalid-gini:0.19429\nMultiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n\nWill train until valid-gini hasn't improved in 200 rounds.\n[100]\ttrain-gini:0.321987\tvalid-gini:0.278464\n[200]\ttrain-gini:0.359511\tvalid-gini:0.285658\n[300]\ttrain-gini:0.392516\tvalid-gini:0.289204\n[400]\ttrain-gini:0.419991\tvalid-gini:0.289849\n[500]\ttrain-gini:0.443408\tvalid-gini:0.291014\n[600]\ttrain-gini:0.458003\tvalid-gini:0.290918\n[700]\ttrain-gini:0.473734\tvalid-gini:0.290754\n[800]\ttrain-gini:0.487845\tvalid-gini:0.290586\nStopping. Best iteration:\n[648]\ttrain-gini:0.465592\tvalid-gini:0.291336\n\n[0]\ttrain-gini:0.196601\tvalid-gini:0.19429\nMultiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n\nWill train until valid-gini hasn't improved in 200 rounds.\n[100]\ttrain-gini:0.337376\tvalid-gini:0.283497\n[200]\ttrain-gini:0.390756\tvalid-gini:0.289544\n[300]\ttrain-gini:0.429966\tvalid-gini:0.292974\n[400]\ttrain-gini:0.456514\tvalid-gini:0.292849\n[500]\ttrain-gini:0.481765\tvalid-gini:0.291735\nStopping. Best iteration:\n[340]\ttrain-gini:0.44194\tvalid-gini:0.293288\n\n[0]\ttrain-gini:0.196601\tvalid-gini:0.19429\nMultiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n\nWill train until valid-gini hasn't improved in 200 rounds.\n[100]\ttrain-gini:0.37337\tvalid-gini:0.288152\n[200]\ttrain-gini:0.440843\tvalid-gini:0.289715\n[300]\ttrain-gini:0.48632\tvalid-gini:0.287219\n[400]\ttrain-gini:0.520738\tvalid-gini:0.286422\nStopping. Best iteration:\n[201]\ttrain-gini:0.441803\tvalid-gini:0.289792\n\n[0]\ttrain-gini:0.196601\tvalid-gini:0.19429\nMultiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n\nWill train until valid-gini hasn't improved in 200 rounds.\n[100]\ttrain-gini:0.431092\tvalid-gini:0.28708\n[200]\ttrain-gini:0.516016\tvalid-gini:0.28225\n[300]\ttrain-gini:0.579317\tvalid-gini:0.275065\nStopping. Best iteration:\n[102]\ttrain-gini:0.433424\tvalid-gini:0.287461\n\n[0]\ttrain-gini:0.196601\tvalid-gini:0.19429\nMultiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n\nWill train until valid-gini hasn't improved in 200 rounds.\n[100]\ttrain-gini:0.512916\tvalid-gini:0.273241\n[200]\ttrain-gini:0.638731\tvalid-gini:0.261126\nStopping. Best iteration:\n[32]\ttrain-gini:0.389099\tvalid-gini:0.282492\n\neta: [0.01, 0.02, 0.03, 0.05, 0.1, 0.2] best_score: [0.290367, 0.291336, 0.293288, 0.289792, 0.287461, 0.282492] best_ntree_limit: [988, 649, 341, 202, 103, 33]","metadata":{},"cell_type":"markdown"},{"source":"We see that **$\\eta = 0.03$** gives better score of 0.293288 and at such learning rate **n_trees** = 341\n\nWe can now tune ```max_depth``` parameter","metadata":{},"cell_type":"markdown"},{"outputs":[],"source":"results = {'max_depth':[],'best_score':[],'best_ntree_limit':[]}\nfor md in range(3,9,1):\n    xgb_params = {'eta': 0.03, \n                  'max_depth': md, \n                  'subsample': 1.0, \n                  'colsample_bytree': 0.3,\n                  'min_child_weight': 1,\n                  'objective': 'binary:logistic', \n                  'seed': 99, \n                  'silent': True}\n\n    watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n    #m = xgb.train(xgb_params, d_train, 1000,  watchlist, feval=gini_xgb, maximize=True, verbose_eval=50, early_stopping_rounds=200)\n    #results['best_score'].append(m.best_score)\n    #results['best_ntree_limit'].append(m.best_ntree_limit)\n    #results['max_depth'].append(md)\n    \n#print('max_depth:',results['max_depth'],'best_score:',results['best_score'],'best_ntree_limit:', results['best_ntree_limit'])","metadata":{"collapsed":true},"execution_count":null,"cell_type":"code"},{"source":"[0]\ttrain-gini:0.152895\tvalid-gini:0.148037\nMultiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n\nWill train until valid-gini hasn't improved in 200 rounds.\n[50]\ttrain-gini:0.254714\tvalid-gini:0.261111\n[100]\ttrain-gini:0.264271\tvalid-gini:0.268258\n[150]\ttrain-gini:0.275104\tvalid-gini:0.274721\n[200]\ttrain-gini:0.283029\tvalid-gini:0.278809\n[250]\ttrain-gini:0.290808\tvalid-gini:0.282106\n[300]\ttrain-gini:0.296478\tvalid-gini:0.284243\n[350]\ttrain-gini:0.301258\tvalid-gini:0.285575\n[400]\ttrain-gini:0.305827\tvalid-gini:0.286782\n[450]\ttrain-gini:0.310241\tvalid-gini:0.287885\n[500]\ttrain-gini:0.313672\tvalid-gini:0.288415\n[550]\ttrain-gini:0.31689\tvalid-gini:0.288889\n[600]\ttrain-gini:0.320219\tvalid-gini:0.289408\n[650]\ttrain-gini:0.322747\tvalid-gini:0.289832\n[700]\ttrain-gini:0.325932\tvalid-gini:0.290298\n[750]\ttrain-gini:0.328984\tvalid-gini:0.290569\n[800]\ttrain-gini:0.331656\tvalid-gini:0.29101\n[850]\ttrain-gini:0.334356\tvalid-gini:0.291343\n[900]\ttrain-gini:0.336958\tvalid-gini:0.291606\n[950]\ttrain-gini:0.339361\tvalid-gini:0.291371\n[0]\ttrain-gini:0.171693\tvalid-gini:0.166814\nMultiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n\nWill train until valid-gini hasn't improved in 200 rounds.\n[50]\ttrain-gini:0.269709\tvalid-gini:0.270024\n[100]\ttrain-gini:0.280589\tvalid-gini:0.276234\n[150]\ttrain-gini:0.294328\tvalid-gini:0.280918\n[200]\ttrain-gini:0.306189\tvalid-gini:0.283884\n[250]\ttrain-gini:0.316871\tvalid-gini:0.286329\n[300]\ttrain-gini:0.325312\tvalid-gini:0.287213\n[350]\ttrain-gini:0.333742\tvalid-gini:0.288468\n[400]\ttrain-gini:0.340944\tvalid-gini:0.289095\n[450]\ttrain-gini:0.347357\tvalid-gini:0.288774\n[500]\ttrain-gini:0.353035\tvalid-gini:0.289154\n[550]\ttrain-gini:0.358306\tvalid-gini:0.289405\n[600]\ttrain-gini:0.363562\tvalid-gini:0.289747\n[650]\ttrain-gini:0.368137\tvalid-gini:0.289983\n[700]\ttrain-gini:0.373225\tvalid-gini:0.290168\n[750]\ttrain-gini:0.377744\tvalid-gini:0.290104\n[800]\ttrain-gini:0.382511\tvalid-gini:0.289967\n[850]\ttrain-gini:0.38713\tvalid-gini:0.28973\nStopping. Best iteration:\n[691]\ttrain-gini:0.372335\tvalid-gini:0.290245\n\n[0]\ttrain-gini:0.186317\tvalid-gini:0.186526\nMultiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n\nWill train until valid-gini hasn't improved in 200 rounds.\n[50]\ttrain-gini:0.287977\tvalid-gini:0.274809\n[100]\ttrain-gini:0.304387\tvalid-gini:0.280157\n[150]\ttrain-gini:0.323393\tvalid-gini:0.284439\n[200]\ttrain-gini:0.340842\tvalid-gini:0.287747\n[250]\ttrain-gini:0.356246\tvalid-gini:0.290444\n[300]\ttrain-gini:0.369436\tvalid-gini:0.2913\n[350]\ttrain-gini:0.381322\tvalid-gini:0.291347\n[400]\ttrain-gini:0.390526\tvalid-gini:0.291576\n[450]\ttrain-gini:0.398911\tvalid-gini:0.291635\n[500]\ttrain-gini:0.406795\tvalid-gini:0.291134\n[550]\ttrain-gini:0.414509\tvalid-gini:0.290781\n[600]\ttrain-gini:0.422203\tvalid-gini:0.290599\nStopping. Best iteration:\n[435]\ttrain-gini:0.396286\tvalid-gini:0.291927\n\n[0]\ttrain-gini:0.196601\tvalid-gini:0.19429\nMultiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n\nWill train until valid-gini hasn't improved in 200 rounds.\n[50]\ttrain-gini:0.311761\tvalid-gini:0.278182\n[100]\ttrain-gini:0.337376\tvalid-gini:0.283497\n[150]\ttrain-gini:0.365534\tvalid-gini:0.287479\n[200]\ttrain-gini:0.390756\tvalid-gini:0.289544\n[250]\ttrain-gini:0.412683\tvalid-gini:0.292106\n[300]\ttrain-gini:0.429966\tvalid-gini:0.292974\n[350]\ttrain-gini:0.444596\tvalid-gini:0.293222\n[400]\ttrain-gini:0.456514\tvalid-gini:0.292849\n[450]\ttrain-gini:0.469462\tvalid-gini:0.292862\n[500]\ttrain-gini:0.481765\tvalid-gini:0.291735\nStopping. Best iteration:\n[340]\ttrain-gini:0.44194\tvalid-gini:0.293288\n\n[0]\ttrain-gini:0.208237\tvalid-gini:0.200162\nMultiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n\nWill train until valid-gini hasn't improved in 200 rounds.\n[50]\ttrain-gini:0.346494\tvalid-gini:0.279076\n[100]\ttrain-gini:0.384323\tvalid-gini:0.284499\n[150]\ttrain-gini:0.424827\tvalid-gini:0.28727\n[200]\ttrain-gini:0.460108\tvalid-gini:0.288491\n[250]\ttrain-gini:0.490432\tvalid-gini:0.288809\n[300]\ttrain-gini:0.511943\tvalid-gini:0.288354\n[350]\ttrain-gini:0.527088\tvalid-gini:0.288391\n[400]\ttrain-gini:0.543902\tvalid-gini:0.288251\n[450]\ttrain-gini:0.561581\tvalid-gini:0.288526\nStopping. Best iteration:\n[266]\ttrain-gini:0.496139\tvalid-gini:0.289732\n\n[0]\ttrain-gini:0.218514\tvalid-gini:0.196553\nMultiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n\nWill train until valid-gini hasn't improved in 200 rounds.\n[50]\ttrain-gini:0.395258\tvalid-gini:0.279268\n[100]\ttrain-gini:0.448654\tvalid-gini:0.285241\n[150]\ttrain-gini:0.50154\tvalid-gini:0.286381\n[200]\ttrain-gini:0.547344\tvalid-gini:0.285872\n[250]\ttrain-gini:0.580952\tvalid-gini:0.286312\n[300]\ttrain-gini:0.605303\tvalid-gini:0.286118\nStopping. Best iteration:\n[134]\ttrain-gini:0.487134\tvalid-gini:0.287129\n\nmax_depth: [3, 4, 5, 6, 7, 8] best_score: [0.291721, 0.290245, 0.291927, 0.293288, 0.289732, 0.287129] best_ntree_limit: [911, 692, 436, 341, 267, 135]","metadata":{},"cell_type":"markdown"},{"source":"We can see that **```max_depth``` = 6** is the best choice, valid-gini:0.293288, n_trees_341\nNow, let's tweak ```min_child_weight``` parameter\n","metadata":{},"cell_type":"markdown"},{"outputs":[],"source":"results = {'min_child_w':[],'best_score':[],'best_ntree_limit':[]}\nfor mcw in range(1,10,1):\n    xgb_params = {'eta': 0.03, \n                  'max_depth': 6, \n                  'subsample': 1.0, \n                  'colsample_bytree': 0.3,\n                  'min_child_weight': mcw,\n                  'objective': 'binary:logistic', \n                  'seed': 99, \n                  'silent': True}\n\n    watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n    #m = xgb.train(xgb_params, d_train, 1000,  watchlist, feval=gini_xgb, maximize=True, verbose_eval=200, early_stopping_rounds=200)\n    #results['best_score'].append(m.best_score)\n    #results['best_ntree_limit'].append(m.best_ntree_limit)\n    #results['min_child_w'].append(mcw)\n    \n#print('min_child_w:',results['min_child_w'],'best_score:',results['best_score'],'best_ntree_limit:', results['best_ntree_limit'])","metadata":{"collapsed":true},"execution_count":null,"cell_type":"code"},{"source":"[0]\ttrain-gini:0.196601\tvalid-gini:0.19429\nMultiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n\nWill train until valid-gini hasn't improved in 200 rounds.\n[200]\ttrain-gini:0.390756\tvalid-gini:0.289544\n[400]\ttrain-gini:0.456514\tvalid-gini:0.292849\nStopping. Best iteration:\n[340]\ttrain-gini:0.44194\tvalid-gini:0.293288\n\n[0]\ttrain-gini:0.197209\tvalid-gini:0.19521\nMultiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n\nWill train until valid-gini hasn't improved in 200 rounds.\n[200]\ttrain-gini:0.389033\tvalid-gini:0.290442\n[400]\ttrain-gini:0.455562\tvalid-gini:0.293512\nStopping. Best iteration:\n[391]\ttrain-gini:0.452666\tvalid-gini:0.293653\n\n[0]\ttrain-gini:0.197141\tvalid-gini:0.194548\nMultiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n\nWill train until valid-gini hasn't improved in 200 rounds.\n[200]\ttrain-gini:0.38573\tvalid-gini:0.288662\n[400]\ttrain-gini:0.448931\tvalid-gini:0.28902\nStopping. Best iteration:\n[354]\ttrain-gini:0.438576\tvalid-gini:0.289972\n\n[0]\ttrain-gini:0.197227\tvalid-gini:0.194243\nMultiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n\nWill train until valid-gini hasn't improved in 200 rounds.\n[200]\ttrain-gini:0.384145\tvalid-gini:0.289345\n[400]\ttrain-gini:0.445069\tvalid-gini:0.29142\nStopping. Best iteration:\n[321]\ttrain-gini:0.426088\tvalid-gini:0.29215\n\n[0]\ttrain-gini:0.197234\tvalid-gini:0.194579\nMultiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n\nWill train until valid-gini hasn't improved in 200 rounds.\n[200]\ttrain-gini:0.381508\tvalid-gini:0.290414\n[400]\ttrain-gini:0.442692\tvalid-gini:0.292785\nStopping. Best iteration:\n[354]\ttrain-gini:0.430505\tvalid-gini:0.29324\n\n[0]\ttrain-gini:0.197748\tvalid-gini:0.194716\nMultiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n\nWill train until valid-gini hasn't improved in 200 rounds.\n[200]\ttrain-gini:0.378905\tvalid-gini:0.289523\n[400]\ttrain-gini:0.436356\tvalid-gini:0.292219\nStopping. Best iteration:\n[388]\ttrain-gini:0.433689\tvalid-gini:0.29251\n\n[0]\ttrain-gini:0.198659\tvalid-gini:0.199186\nMultiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n\nWill train until valid-gini hasn't improved in 200 rounds.\n[200]\ttrain-gini:0.377616\tvalid-gini:0.289681\n[400]\ttrain-gini:0.432548\tvalid-gini:0.292334\n[600]\ttrain-gini:0.476844\tvalid-gini:0.289811\nStopping. Best iteration:\n[429]\ttrain-gini:0.438909\tvalid-gini:0.292701\n\n[0]\ttrain-gini:0.19869\tvalid-gini:0.199082\nMultiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n\nWill train until valid-gini hasn't improved in 200 rounds.\n[200]\ttrain-gini:0.375943\tvalid-gini:0.289621\n[400]\ttrain-gini:0.43302\tvalid-gini:0.291149\n[600]\ttrain-gini:0.474083\tvalid-gini:0.289815\nStopping. Best iteration:\n[406]\ttrain-gini:0.434396\tvalid-gini:0.291263\n\n[0]\ttrain-gini:0.198665\tvalid-gini:0.199127\nMultiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n\nWill train until valid-gini hasn't improved in 200 rounds.\n[200]\ttrain-gini:0.375206\tvalid-gini:0.28903\n[400]\ttrain-gini:0.431248\tvalid-gini:0.290529\nStopping. Best iteration:\n[261]\ttrain-gini:0.395649\tvalid-gini:0.291537\n\nmin_child_w: [1, 2, 3, 4, 5, 6, 7, 8, 9] best_score: [0.293288, 0.293653, 0.289972, 0.29215, 0.29324, 0.29251, 0.292701, 0.291263, 0.291537] best_ntree_limit: [341, 392, 355, 322, 355, 389, 430, 407, 262]","metadata":{},"cell_type":"markdown"},{"source":"So, ```min_child_weight``` = 2 is the best, valid-gini:0.293653, n_tree = 392\nFinally, we can tweak ```colsample_bytree``` parameter\n","metadata":{},"cell_type":"markdown"},{"outputs":[],"source":"results = {'colsample_bytree':[],'best_score':[],'best_ntree_limit':[]}\nfor cst in [0.3,0.4,0.5]:\n    xgb_params = {'eta': 0.03, \n                  'max_depth': 6, \n                  'subsample': 1.0, \n                  'colsample_bytree': cst,\n                  'min_child_weight': 2,\n                  'objective': 'binary:logistic', \n                  'seed': 99, \n                  'silent': True}\n\n    #watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n    #m = xgb.train(xgb_params, d_train, 1000,  watchlist, feval=gini_xgb, maximize=True, verbose_eval=200, early_stopping_rounds=200)\n    #results['best_score'].append(m.best_score)\n    #results['best_ntree_limit'].append(m.best_ntree_limit)\n    #results['colsample_bytree'].append(cst)\n    \n#print('colsample_bytree:',results['colsample_bytree'],'best_score:',results['best_score'],'best_ntree_limit:', results['best_ntree_limit'])","metadata":{"collapsed":true},"execution_count":null,"cell_type":"code"},{"source":"[0]\ttrain-gini:0.197209\tvalid-gini:0.19521\nMultiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n\nWill train until valid-gini hasn't improved in 200 rounds.\n[200]\ttrain-gini:0.389033\tvalid-gini:0.290442\n[400]\ttrain-gini:0.455562\tvalid-gini:0.293512\nStopping. Best iteration:\n[391]\ttrain-gini:0.452666\tvalid-gini:0.293653\n\n[0]\ttrain-gini:0.204759\tvalid-gini:0.189515\nMultiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n\nWill train until valid-gini hasn't improved in 200 rounds.\n[200]\ttrain-gini:0.393239\tvalid-gini:0.288466\n[400]\ttrain-gini:0.459999\tvalid-gini:0.289528\nStopping. Best iteration:\n[248]\ttrain-gini:0.413525\tvalid-gini:0.290517\n\n[0]\ttrain-gini:0.21643\tvalid-gini:0.209827\nMultiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n\nWill train until valid-gini hasn't improved in 200 rounds.\n[200]\ttrain-gini:0.399235\tvalid-gini:0.289937\n[400]\ttrain-gini:0.463524\tvalid-gini:0.290695\nStopping. Best iteration:\n[259]\ttrain-gini:0.4212\tvalid-gini:0.291762\n\ncolsample_bytree: [0.3, 0.4, 0.5] best_score: [0.293653, 0.290517, 0.291762] best_ntree_limit: [392, 249, 260]","metadata":{},"cell_type":"markdown"},{"source":"So, ```colsample_bytree``` = 0.3 is the best option with valid-gini = 0.293653 and n_tree = 392","metadata":{},"cell_type":"markdown"},{"source":"Now let's train the model on the full data set.","metadata":{},"cell_type":"markdown"},{"outputs":[],"source":"training_features, test_features, \\\ntraining_target, test_target, = train_test_split(data_clean.drop(['id','target'], axis = 1),\n                                               data_clean['target'],\n                                               test_size = .2,\n                                               random_state=12)\n\n# Now further split the training test into training and validation to \nx_train, x_val, y_train, y_val = train_test_split(training_features, training_target,\n                                                  test_size = .2,\n                                                  random_state=12)","metadata":{"collapsed":true},"execution_count":null,"cell_type":"code"},{"outputs":[],"source":"#Final model\nxgb_params = {'eta': 0.03, \n                  'max_depth': 6, \n                  'subsample': 1.0, \n                  'colsample_bytree': 0.3,\n                  'min_child_weight': 2,\n                  'objective': 'binary:logistic', \n                  'seed': 99, \n                  'silent': True}\nd_train = xgb.DMatrix(x_train, y_train)\nd_valid = xgb.DMatrix(x_val,y_val)\n\n#watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n#model = xgb.train(xgb_params, d_train, 392,  watchlist, feval=gini_xgb, maximize=True, verbose_eval=200, early_stopping_rounds=200)","metadata":{"collapsed":true},"execution_count":null,"cell_type":"code"},{"source":"Now let's see what features are important","metadata":{},"cell_type":"markdown"},{"outputs":[],"source":"#Feature importance\n#feat_imp = pd.Series(model.get_fscore()).sort_values(ascending=False)\n#feat_imp.plot(kind='bar', title='Feature Importances')\n#feat_imp[:60]","metadata":{"collapsed":true},"execution_count":null,"cell_type":"code"},{"source":"Looks like features with the score under 100 look equally unimportant. \nThus, let's only keep those features that have a feature importance score >= 100.","metadata":{},"cell_type":"markdown"},{"outputs":[],"source":"#to_keep = feat_imp[feat_imp>=100].index\n#df = data_clean[to_keep]\n#x_train = df\n#y_train = data_clean['target']","metadata":{"collapsed":true},"execution_count":null,"cell_type":"code"},{"outputs":[],"source":"xgb_params = {'eta': 0.03, \n                  'max_depth': 6, \n                  'subsample': 1.0, \n                  'colsample_bytree': 0.3,\n                  'min_child_weight': 2,\n                  'objective': 'binary:logistic', \n                  'seed': 99, \n                  'silent': True}\n#xgb.DMatrix(x_train[predictors].values, label=y_train.values)\n#d_train = xgb.DMatrix(x_train, y_train)\n#d_valid = xgb.DMatrix(x_val,y_val)\n\n#watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n#model = xgb.train(xgb_params, d_train, 392, feval=gini_xgb, maximize=True, verbose_eval=False)","metadata":{"collapsed":true},"execution_count":null,"cell_type":"code"},{"source":"## Prepare Test Set","metadata":{},"cell_type":"markdown"},{"outputs":[],"source":"#Download and transform test set\ntest = pd.read_csv('../input/test.csv', low_memory=False)","metadata":{"collapsed":true},"execution_count":null,"cell_type":"code"},{"outputs":[],"source":"test.head(5)","metadata":{},"execution_count":null,"cell_type":"code"},{"outputs":[],"source":"nas = np.sum(test == -1)/len(test) *100\nprint(\"The percentage of missing values is\")\nprint (nas[nas>0].sort_values(ascending = False))","metadata":{},"execution_count":null,"cell_type":"code"},{"outputs":[],"source":"#Transformations\ntest_clean = test.copy()\n\ncat_cols = [c for c in test_clean.columns if c.endswith('cat')]\n\n# Creating dummies for missing values in categorical features\nfor column in cat_cols:\n    temp=pd.get_dummies(test_clean[column], prefix=column, prefix_sep='_')\n    test_clean=pd.concat([test_clean,temp],axis=1)\n    test_clean=test_clean.drop([column],axis=1)\n\nprint('test_clean shape is:',test_clean.shape)\n\n    \n# Impute missing values with medians\n\nnum_cols = ['ps_reg_03','ps_car_14', 'ps_car_11']\n\nfor n in num_cols:\n    dummy_name = str(n) + 'NA'\n    test_clean[dummy_name] = (test_clean[n]==-1).astype(int)\n    med = test_clean[test_clean[n]!=-1][n].median()\n    test_clean.loc[test_clean[n]==-1,n] = med\n    print(n,np.sum(data_clean[n] == -1)/len(data_clean) *100)\n","metadata":{},"execution_count":null,"cell_type":"code"},{"source":"### Make predictions","metadata":{},"cell_type":"markdown"},{"outputs":[],"source":"#x_test = test_clean[to_keep]\n#dtest = xgb.DMatrix(x_test)\n#xgb_pred = model.predict(dtest)\n\n#id_test = test_clean['id'].values\n#output = pd.DataFrame({'id': id_test, 'target': xgb_pred})\n","metadata":{"collapsed":true},"execution_count":null,"cell_type":"code"},{"outputs":[],"source":"","metadata":{"collapsed":true},"execution_count":null,"cell_type":"code"}],"nbformat":4,"metadata":{"language_info":{"nbconvert_exporter":"python","version":"3.6.3","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python","pygments_lexer":"ipython3"},"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"}},"nbformat_minor":1}