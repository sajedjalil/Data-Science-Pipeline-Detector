{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.1","nbconvert_exporter":"python","pygments_lexer":"ipython3","mimetype":"text/x-python","file_extension":".py","codemirror_mode":{"name":"ipython","version":3}}},"cells":[{"metadata":{"_cell_guid":"0c87cab5-4879-4fee-a48d-afdce13edf90","_uuid":"e1efb3fac70706ff317444ebdc7d7c4e98b23a0d"},"source":"# Safe Driver Prediction Explotory Data Analysis\n\n`Kueip- Sept 2017`\n\n---\n## Outline:\n-   ** Intoduction** ([completed]())\n    - Packages Loading\n    - Check Memory Usage\n-  ** Multii-Variables Analysis**  ([non-complete]())\n-  ** Bi-Variables Analysis**  ([non-complete]())\n        - Feature Values Distribution\n-  ** Target Value Analysis** ([completed]())\n-  ** Missing Values Analysis** ([completed]())\n    - Matrix\n    - HeatMap\n    - Bar\n\n-  ** Feature Important** ([non-complete]())\n        - Decision Tree\n        - RandomForest\n        - XGB\n        - LGB","cell_type":"markdown"},{"metadata":{"_cell_guid":"6f1fd4f8-dc7b-4951-9261-bd1ca8000a4b","_uuid":"879e34e35f14a76b8691d6b541a43dfcc2282e43"},"source":"","cell_type":"markdown"},{"metadata":{"_cell_guid":"340fc40a-da02-4a21-9467-32e92365d6aa","_uuid":"05046c6cca457d3f88d223cd72f33218b765dd9d"},"source":"# Introduction\nPorto Seguro, one of Brazil’s largest auto and homeowner insurance companies, completely agrees. Inaccuracies in car insurance company’s claim predictions raise the cost of insurance for good drivers and reduce the price for bad ones.","cell_type":"markdown"},{"metadata":{"_cell_guid":"5c26f9b0-8e85-4610-9873-cf06e6b8e2e7","_uuid":"ebc6e09fabb5c68e806b4b1457eb2ef20a6d321a"},"source":"### Packages Loading","cell_type":"markdown"},{"execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"b84eac91-4c52-430c-8dd5-c3ef4326a86c","_uuid":"fe991bf2237c7c9058e2293e707eb9c9a8c8155b","_kg_hide-input":true},"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport matplotlib\n%matplotlib inline\nimport seaborn as sns # visualization\nfrom subprocess import check_output\nimport missingno as msno\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import BaggingClassifier, RandomForestClassifier \n\nimport xgboost as xgb # Gradeint Boosting\nfrom xgboost import XGBClassifier # Gradeint Boosting\nimport lightgbm as lgb # Gradeint Boosting\nimport gc\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# Any results you write to the current directory are saved as output.","cell_type":"code","outputs":[]},{"execution_count":null,"metadata":{"_cell_guid":"7814780a-1d28-4648-80f3-18f5b1ee9611","_uuid":"97af5403e5e75c5b8735a72fc744e7c7af3385d0","_kg_hide-input":true},"source":"train = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")\nprint(\"Train shape : \", train.shape)\nprint(\"Test shape : \", test.shape)","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"c5183e70-469b-4148-b353-52ee0590c3b0","_uuid":"3ecb0ab00f831f00eb959e80de479355ae3f60ac"},"source":"- No. of rows are large with 58 columns. \n\nFrom VC dimension theory, we dont worry about overfitting too much, if we could cover the function set, choose the proper number of features.\n","cell_type":"markdown"},{"execution_count":null,"metadata":{"_cell_guid":"78b66aec-7eab-48aa-8d18-ccfdd259cb3d","_uuid":"2b0d517f560f9dee276ce51790948ebb7ae84bd6","_kg_hide-input":true},"source":"train.head()","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"fcec6130-8453-48cb-85b2-f836961b7530","_uuid":"ac8084920f5ff6b13b4f273f3558e3208754f2f5"},"source":"### Check Memory Usage","cell_type":"markdown"},{"execution_count":null,"metadata":{"_cell_guid":"66cfb59b-f8c5-43c3-b1be-6017c9cb57ed","_uuid":"8adebbf6d6d663c98a4454dae53f26827a3bc8ed","_kg_hide-input":true},"source":"train.info(verbose=False),test.info(verbose=False)","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"f90f61ce-ec97-4659-9307-02ec60fbc4ad","_uuid":"6828288dc95e45ec87b88cbb5d3045ea3bdd2946"},"source":"### Convert Type","cell_type":"markdown"},{"execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"e1a64d00-c742-43aa-a15e-19bc4b819bdc","_uuid":"b3772cc9cc75e6cf164a31020973ee6e8f6cffb1","_kg_hide-input":true},"source":"for c, dtype in zip(train.columns, train.dtypes):\n    if dtype == np.float64:\n        train[c] = train[c].astype(np.float32) \n    elif dtype == np.int64:\n        train[c] = train[c].astype(np.int32) \ngc.collect()\nfor c, dtype in zip(test.columns, test.dtypes):\n    if dtype == np.float64:\n        test[c] = test[c].astype(np.float32) \n    elif dtype == np.int64:\n        test[c] = test[c].astype(np.int32) ","cell_type":"code","outputs":[]},{"metadata":{},"source":"## Multi-Variable Analysis","cell_type":"markdown"},{"execution_count":null,"metadata":{"collapsed":true,"_kg_hide-input":true},"source":"from collections import Counter\ncount = Counter()\nunique_values_dict = {}\nfor col in train.columns:\n    unique_values_dict[col] = np.sort(train[col].unique())\n    count[col] = len(np.sort(train[col].unique()))   ","cell_type":"code","outputs":[]},{"execution_count":null,"metadata":{"_kg_hide-input":true},"source":"cat_cols = [ col for col , val in count.items() if(val==10)]\nplt.figure(figsize=(20,70))\nfor i in range(len(cat_cols)):\n    c = cat_cols[i]\n    \n    means = train.groupby(c).target.mean()\n    stds = train.groupby(c).target.std()#.fillna(0)\n    means_astds = train.groupby(c).target.mean() + train.groupby(c).target.std()\n    means_sstds = train.groupby(c).target.mean() - train.groupby(c).target.std()\n    \n    ddd = pd.concat([means, stds, means_astds, means_sstds], axis=1); \n    ddd.columns = ['means', 'stds', 'means + stds', 'means - stds']\n    ddd.sort_values('means', inplace=True)\n    \n    plt.subplot(len(cat_cols), 2, 2*i+1)\n    ax = sns.countplot(train[c], order=ddd.index.values)\n    plt.xticks(rotation=90)\n    for p in ax.patches:\n        x=p.get_bbox().get_points()[:,0]\n        y=p.get_bbox().get_points()[1,1]\n        ax.annotate('{:.0f}'.format(y), (x.mean(), y), ha='center', va='bottom')\n    \n    plt.subplot(len(cat_cols ), 2, 2*i+2)\n    plt.fill_between(range(len(train[c].unique())), \n                     ddd.means.values - ddd.stds.values,\n                     ddd.means.values + ddd.stds.values,\n                     alpha=0.3\n                    )\n    plt.xticks(range(len(train[c].unique())), ddd.index.values, rotation=90,fontsize=18)\n    plt.plot(ddd.means.values, color='b', marker='.', linestyle='dashed', linewidth=0.7)\n    plt.plot(ddd['means + stds'].values, color='g', linestyle='dashed', linewidth=0.7)\n    plt.plot(ddd['means - stds'].values, color='r', linestyle='dashed', linewidth=0.7)\n    plt.xlabel(c + ': Means, STDs and +- STDs',fontsize=18)\n    #plt.ylim(80, 270)\nplt.show()","cell_type":"code","outputs":[]},{"execution_count":null,"metadata":{"_kg_hide-input":true},"source":"import plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\na=[column for column in train]\ntrace = go.Heatmap(z=train.corr().values,\n                   x=a,\n                   y=a)\ndata=[trace]\npy.iplot(data, filename='backorders heatmap')","cell_type":"code","outputs":[]},{"metadata":{},"source":"## Binary Variables:\n","cell_type":"markdown"},{"execution_count":null,"metadata":{"_kg_hide-input":true},"source":"cat_cols = [ col for col , val in count.items() if(val==2)]\nplt.figure(figsize=(25,100))\nfor i in range(len(cat_cols)):\n    c = cat_cols[i]\n    \n    means = train.groupby(c).target.mean()\n    stds = train.groupby(c).target.std()#.fillna(0)\n    means_astds = train.groupby(c).target.mean() + train.groupby(c).target.std()\n    means_sstds = train.groupby(c).target.mean() - train.groupby(c).target.std()\n    \n    ddd = pd.concat([means, stds, means_astds, means_sstds], axis=1); \n    ddd.columns = ['means', 'stds', 'means + stds', 'means - stds']\n    ddd.sort_values('means', inplace=True)\n    \n    plt.subplot(len(cat_cols), 2, 2*i+1)\n    ax = sns.countplot(train[c], order=ddd.index.values)\n    plt.xticks(rotation=90)\n    for p in ax.patches:\n        x=p.get_bbox().get_points()[:,0]\n        y=p.get_bbox().get_points()[1,1]\n        ax.annotate('{:.0f}'.format(y), (x.mean(), y), ha='center', va='bottom')\n    \n    plt.subplot(len(cat_cols ), 2, 2*i+2)\n    plt.fill_between(range(len(train[c].unique())), \n                     ddd.means.values - ddd.stds.values,\n                     ddd.means.values + ddd.stds.values,\n                     alpha=0.3\n                    )\n    plt.xticks(range(len(train[c].unique())), ddd.index.values, rotation=90,fontsize=18)\n    plt.yticks(fontsize=18)\n    plt.plot(ddd.means.values, color='b', marker='.', linestyle='dashed', linewidth=0.7)\n    plt.plot(ddd['means + stds'].values, color='g', linestyle='dashed', linewidth=0.7)\n    plt.plot(ddd['means - stds'].values, color='r', linestyle='dashed', linewidth=0.7)\n    plt.xlabel(c + ': Means, STDs and +- STDs',fontsize=16)\n    #plt.ylim(80, 270)\nplt.show()","cell_type":"code","outputs":[]},{"metadata":{},"source":"- **ps_ind_10_bin**\n- **ps_ind_11_bin**\n- **ps_ind_12_bin**\n- **ps_ind_13_bin** \n\nhave obvious imbalanced distribution.","cell_type":"markdown"},{"metadata":{"_cell_guid":"0930e350-acdc-473e-8e26-80d42da52f18","_uuid":"d15f3cce659c8c9fb260c628d615d98a71770ce8"},"source":"## Target Variable Analysis","cell_type":"markdown"},{"execution_count":null,"metadata":{"_cell_guid":"21aad605-ea42-4fa3-8566-c66273760e9e","_uuid":"662f07d35023e38312329ee65ad9dcfabd41c211","_kg_hide-input":true},"source":"labels = '1', '0'\nsizes = [train[train.target==1].shape[0],train[train.target==0].shape[0]]\ncolors = ['gold', 'lightskyblue']\nexplode = (0.1, 0)  # explode 1st slice\n# Plot\nplt.pie(sizes, explode=explode, labels=labels, colors=colors,\n        autopct='%1.2f%%', shadow=True, startangle=140)\n\nplt.axis('equal')\nplt.show()","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"59f7cf18-7179-46db-85ec-27b91083a2c2","_uuid":"7d4e9f6bbf56e3e007864ab60944c1a564bde9b1","_kg_hide-input":true},"source":"- imblalanced data\n- Scikit-Learn provide [StratifiedShuffleSplit](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html#sklearn.model_selection.StratifiedShuffleSplit) API by preserving the percentage of samples for each class.","cell_type":"markdown"},{"metadata":{"_cell_guid":"d72a1c09-3839-4d64-bcaa-7112c5fabcbe","_uuid":"b898d432403276f33a4ded5c2e380e601fa4a089"},"source":"### A Review on Imbalanced Learning Methods\n\nImbalanced classification is a supervised learning problem where one class outnumbers other class by a large proportion. This problem is faced more frequently in binary classification problems than multi-level classification problems. The reasons which leads to reduction in accuracy of ML algorithms on imbalanced data sets:\n    1. ML algorithms struggle with accuracy because of the unequal distribution in dependent variable.\n    2. This causes the performance of existing classifiers to get biased towards majority class.\n    3. The algorithms are accuracy driven i.e. they aim to minimize the overall error to which the minority class contributes very little.\n    4. ML algorithms assume that the data set has balanced class distributions.\n    5. They also assume that errors obtained from different classes have same cost","cell_type":"markdown"},{"metadata":{"_cell_guid":"8ea75f0d-51ab-42d9-ba57-e165cca78e37","_uuid":"99df4d771b21b50604e010b5834d21cadd7b4385"},"source":"### How to use imbalanced data to cheat your boss ? Let's conduct an experiment!","cell_type":"markdown"},{"execution_count":null,"metadata":{"_cell_guid":"f514df6a-624d-43f1-a711-78f9117e0b06","_uuid":"e1ad178dcab8f84d356cbc10ed39a3324b2a4ad7","_kg_hide-input":false},"source":"from sklearn.model_selection import StratifiedShuffleSplit\nX = train.drop(['id','target'], axis=1).values\ny = train.target.values\nsss = StratifiedShuffleSplit(n_splits=10, test_size=0.3, random_state=0)\nfor train_index, test_index in sss.split(X, y):\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    break\n    \nfrom sklearn.dummy import DummyClassifier\n# Negative class (0) is most frequent\ndummy_majority = DummyClassifier(strategy = 'most_frequent').fit(X_train, y_train)\n# Therefore the dummy 'most_frequent' classifier always predicts class 0\ndummy_majority.score(X_test, y_test)","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"9a1213f5-bbe6-4c03-bd86-7b762555d31c","_uuid":"4feba81a90595f0950b5c8c796cf9a040d83b0cd"},"source":"\nHey Boss, I design a bullshit classifier with accuracy 96.35%.\n\n.\n\n.\n\n\nNow, you should know why **Normalized Gini** is the metric in this case, instead of **accuracy**. \n(If  we just used a majority class to assign values to all records, we will still be having a high accuracy.)\n\nOne Specific example, if this bullshiter recognize a terrorist isnt a terroist, it will become a disaster.","cell_type":"markdown"},{"metadata":{"_cell_guid":"37f14b57-b78b-41b6-90d2-7586c0ad6885","_uuid":"f592b4c99ec9bf5cef0bbd153d1377bd136955aa"},"source":"## Missing Values Analysis\n- Thanks **Pedro Schoen** for pointing missing are encoded as **-1**\n- Let's encode **-1** as `np.nan`","cell_type":"markdown"},{"execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"2b19bfdc-cd97-47cd-bada-658ec373132d","_uuid":"a4bc90dd571f97ee3924cde257b591a63d2ae237"},"source":"for col in np.intersect1d(train.columns,test.columns):\n    train.loc[train[col]==-1,col] = np.nan\n    test.loc[test[col]==-1,col] = np.nan","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"68ca9e0c-ffd2-42f0-932d-e0186caec653","_uuid":"67f2c5e32cd150749fb75dec3568819d602e30b2"},"source":"## Train Set Missing Values","cell_type":"markdown"},{"execution_count":null,"metadata":{"_cell_guid":"0c686bbc-16db-4978-8556-ef8aa803a2fc","_uuid":"7b5178389b7839ff3a28ef4d2a384bb2dfc38485","_kg_hide-input":true},"source":"missing_df = train.isnull().sum(axis=0).reset_index()\nmissing_df.columns = ['column_name', 'missing_count']\nmissing_df['ratio'] = round(missing_df['missing_count'] / train.shape[0],4)\nmissing_df[missing_df['ratio']>0][['column_name', 'missing_count','ratio']].sort_values(by='ratio',ascending=False)","cell_type":"code","outputs":[]},{"execution_count":null,"metadata":{"_cell_guid":"4fb765a0-d495-4e00-b9c6-2d5fec05fdb3","_uuid":"f036353f874ab74b9b759933194e1f0cec492626","_kg_hide-input":true},"source":"def missingno_matrix(df):\n    missingValueColumns = df.columns[df.isnull().any()].tolist()\n    msno.matrix(df[missingValueColumns],width_ratios=(10,1),\\\n            figsize=(20,8),color=(0,0, 0),fontsize=12,sparkline=True,labels=True)\n    plt.show()\nmissingno_matrix(train)","cell_type":"code","outputs":[]},{"execution_count":null,"metadata":{"_cell_guid":"ce1df280-c7c3-4971-a360-e7f956623f19","_uuid":"6dbbf91dcc265e3303a2f8c593b863a94e7f80da","_kg_hide-input":true},"source":"def missingno_heatmap(df):\n    missingValueColumns = df.columns[df.isnull().any()].tolist()\n    msno.heatmap(df[missingValueColumns],figsize=(20,20))\n    plt.show()\nmissingno_heatmap(train)","cell_type":"code","outputs":[]},{"execution_count":null,"metadata":{"_cell_guid":"4fdb88a0-ce6c-4b95-8531-fc46cc7d5484","_uuid":"34c2d2bb2bd5891ec397f39ca2c7784ad63afa11","_kg_hide-input":true},"source":"def missing_bar(df):\n\n    missingValueColumns = df.columns[df.isnull().any()].tolist()\n    msno.bar(df[missingValueColumns],figsize=(20,8),color=\"#34495e\",fontsize=12,labels=True)\n    plt.show()\nmissing_bar(train)","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"f3656b4a-0d08-446f-810d-36f29b933a57","_uuid":"c2ee2a92e3a468b0c7bdc7714619514e53ac8f00"},"source":"## Test Set Missing Values","cell_type":"markdown"},{"execution_count":null,"metadata":{"_cell_guid":"46b79a5f-9368-4302-a1be-6c4cb9dde4b0","_uuid":"b0eb83cda124947b8d8cc62743efd14d3f4200ea","_kg_hide-input":true},"source":"missing_df = test.isnull().sum(axis=0).reset_index()\nmissing_df.columns = ['column_name', 'missing_count']\nmissing_df['ratio'] = round(missing_df['missing_count'] / test.shape[0],4)\nmissing_df[missing_df['ratio']>0][['column_name', 'missing_count','ratio']].sort_values(by='ratio',ascending=False)","cell_type":"code","outputs":[]},{"execution_count":null,"metadata":{"_cell_guid":"e86686ec-b0dc-4f05-bd32-855b79e33bc2","_uuid":"2a13117e1370fdf04b2c33e07a5a9d15fb1a3ead"},"source":"missingno_matrix(test)","cell_type":"code","outputs":[]},{"execution_count":null,"metadata":{"_cell_guid":"95a08c3b-d6d7-4823-9a5d-19b694d3c7ea","_uuid":"4cbd929f34f40018d33d1129c6cfa393056f05b6"},"source":"missingno_heatmap(test)","cell_type":"code","outputs":[]},{"execution_count":null,"metadata":{"_cell_guid":"2db271e9-43be-4b35-811f-ecb7f78cd300","_uuid":"c20e23581e540f5bef5d6fd8901c840f88a6f6c6"},"source":"missing_bar(test)","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"b1a7d469-1fbe-4484-9267-c39dd95af71c","_uuid":"a22ffc2dd4b7cb3b3647a7ba7252f798757017aa"},"source":"- Good News. Both dataset get the same NaN ratio/distribution","cell_type":"markdown"},{"metadata":{"_cell_guid":"b65682ec-6fc3-4fc8-ab62-4d7e596e6fcb","_uuid":"0c373ad876a1eddb6393a4bbb0d1814c3030734f"},"source":"## Feature Importance","cell_type":"markdown"},{"metadata":{"_cell_guid":"57e64a35-5d5a-427e-aa78-08cc856811f4","_uuid":"54bceb8a14ddf503ad8dd3499ce940b203c7d7c5"},"source":"- **Decision Tree Classifier**\n\nA decision tree is a flowchart-like structure in which each internal node represents a \"test\" on an attribute (e.g. whether a coin flip comes up heads or tails), each branch represents the outcome of the test, and each leaf node represents a class label (decision taken after computing all attributes). The paths from root to leaf represent classification rules.","cell_type":"markdown"},{"execution_count":null,"metadata":{"_cell_guid":"6b6d7ed7-da7f-44f2-893a-602fe3969bcc","_uuid":"1bbc6fdd6154f4dacb777a060aec53d6b7065388","_kg_hide-input":true},"source":"matplotlib.style.use('fivethirtyeight')\nmatplotlib.rcParams['figure.figsize'] = (12,6)\nmodel = DecisionTreeClassifier(max_depth=6 ,random_state=87)\nmodel.fit(X_train, y_train)\nfeat_names = train.drop(['id','target'],axis=1).columns\n## plot the importances ##\nimportances = model.feature_importances_\n\nindices = np.argsort(importances)[::-1]\nplt.figure(figsize=(12,6))\nplt.title(\"Feature importances by DecisionTreeClassifier\")\nplt.bar(range(len(indices)), importances[indices], color='lightblue',  align=\"center\")\nplt.step(range(len(indices)), np.cumsum(importances[indices]), where='mid', label='Cumulative')\nplt.xticks(range(len(indices)), feat_names[indices], rotation='vertical',fontsize=14)\nplt.xlim([-1, len(indices)])\nplt.show()","cell_type":"code","outputs":[]},{"execution_count":null,"metadata":{"_cell_guid":"81041894-9e01-4ca1-beed-3a8f6ecdf4aa","_uuid":"440d02c85abf13fcc419d971c0f046836ceead2e","_kg_hide-input":true},"source":"from sklearn.tree import export_graphviz\nimport graphviz\ntreegraph = export_graphviz(model, out_file=None, \n                         feature_names=train.drop(['id','target'],axis=1).columns,  \n                         filled=True, rounded=True,  \n                         special_characters=True)  \ngraph = graphviz.Source(treegraph)  \ngraph","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"897a3f10-b87c-40a2-bfdb-ce3488e63b4b","_uuid":"53963883ce0bfd3915d99aacfd1f8bf9e27ed6cd"},"source":"- **RandomForest Classifier**\n\nA random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and use averaging to improve the predictive accuracy and control over-fitting.","cell_type":"markdown"},{"execution_count":null,"metadata":{"_cell_guid":"0eb5cbba-92b4-4c35-baa0-89c3ada4f600","_uuid":"acae970b3f0b87fd53d2a964b187e008fe4ae880","_kg_hide-input":true},"source":"model = RandomForestClassifier(max_depth=8)\nmodel.fit(X_train, y_train)\nfeat_names = train.drop(['id','target'],axis=1).columns\n## plot the importances ##\nimportances = model.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)\nindices = np.argsort(importances)[::-1]\n\nplt.figure(figsize=(12,6))\nplt.title(\"Feature importances by Random Forest\")\nplt.bar(range(len(indices)), importances[indices], color='lightblue', yerr=std[indices], align=\"center\")\nplt.step(range(len(indices)), np.cumsum(importances[indices]), where='mid', label='Cumulative')\nplt.xticks(range(len(indices)), feat_names[indices], rotation='vertical',fontsize=14)\nplt.xlim([-1, len(indices)])\nplt.show()","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"6dd36016-7311-4dd8-97b9-2ba88fe1ec8b","_uuid":"5c3e8f7316577a806a67d1d7dd4caf129dd7f88c"},"source":"- **XGB Classifier**\n\nXGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework","cell_type":"markdown"},{"execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"4d768caa-5dd7-41f2-9df7-364546cb1385","_uuid":"2914411ac5b4f76c83b2c3bbfac4ea6eeef90714","_kg_hide-input":true},"source":"model = XGBClassifier(eta = 0.01, max_depth = 8, subsample = 0.8, colsample_bytree= 0.8)\nmodel.fit(X_train, y_train)\nimportances = model.feature_importances_\nindices = np.argsort(importances)[::-1]\nplt.figure(figsize=(12,6))\nplt.title(\"Feature importances by XGB\") # Thanks Oscar Takeshita's kindly remind\nplt.bar(range(len(indices)), importances[indices], color='lightblue', align=\"center\")\nplt.step(range(len(indices)), np.cumsum(importances[indices]), where='mid', label='Cumulative')\nplt.xticks(range(len(indices)), feat_names[indices], rotation='vertical',fontsize=14)\nplt.xlim([-1, len(indices)])\nplt.show()","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"ddf25b5c-0188-4e8d-9408-70b63a5a66ee","_uuid":"1991cddd99e3be7d225d586ff6ca34198d6700a7"},"source":"- Create Tree digraph by using \n\n`xgb.to_graphviz`","cell_type":"markdown"},{"execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"b67ab82a-69cf-4f62-94be-b7d9e9dfdde1","_uuid":"eff6d0dfbe5d6d72dceb8502999752580dc0271d","_kg_hide-input":true},"source":"xgb.to_graphviz(model, fmap='', rankdir='UT', num_trees=6,\n                yes_color='#0000FF', no_color='#FF0000')","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"56078f62-2373-4a7f-809b-3d46318b8afd","_uuid":"6bcb30cc1999bb898ca1125bf4a3cf81af8278e2"},"source":"- **LightGBM Classifier**\n\nLightGBM is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient with the following advantages","cell_type":"markdown"},{"execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"f303b6e9-6bd9-4324-a961-347bc4384b09","_uuid":"24abc8159e2f59f18505a66622cbc21c287da509","_kg_hide-input":true},"source":"lgb_params = {}\nlgb_params['objective'] = 'binary'\nlgb_params['sub_feature'] = 0.80 \nlgb_params['max_depth'] = 7\nlgb_params['feature_fraction'] = 0.7\nlgb_params['bagging_fraction'] = 0.7\nlgb_params['bagging_freq'] = 10\nlgb_params['learning_rate'] = 0.01\n\nlgb_train = lgb.Dataset(X_train, y_train)\nlightgbm = lgb.train(lgb_params, lgb_train, feature_name=[ i for i in feat_names])","cell_type":"code","outputs":[]},{"execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"74d0a9dc-7269-49da-bff3-e5ee10cf37c6","_uuid":"40b09ec29a1cecfafc8cccd2a11d0d6eaf91b0d7","_kg_hide-input":true},"source":"plt.figure(figsize=(12,6))\nlgb.plot_importance(lightgbm,max_num_features=30)\nplt.title(\"Feature importances by LightGBM\")\nplt.show()","cell_type":"code","outputs":[]},{"execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"a21f7dcc-082c-4189-81b5-c387b2139c86","_uuid":"b8a7c3385f1eaa1063d87c947b6a5a376b682c28","_kg_hide-input":true},"source":"ax = lgb.plot_tree(lightgbm, tree_index=83, figsize=(20, 8), show_info=['split_gain'])\nplt.show()","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"46a6d860-4f99-4f24-a6fe-c5b71fa6d497","_uuid":"6e6d627a9762373b16e7704542e6dd171104ce72"},"source":"# Acknowledgement:\n1. Pedro Schoen\n\n\n## Stay Tuned\n","cell_type":"markdown"}],"nbformat_minor":1,"nbformat":4}