{"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"},"language_info":{"nbconvert_exporter":"python","name":"python","version":"3.6.3","file_extension":".py","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3"}},"nbformat_minor":1,"cells":[{"cell_type":"code","metadata":{"_uuid":"3903b539642b92fe50955591d41cb0bedee7165c","_cell_guid":"90363b4f-3d45-49d6-970f-c4ca98dd89ec","collapsed":true},"outputs":[],"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport time\n\n#This keeps the \"middle\" columns from being omitted when wide dataframes are being displayed\npd.options.display.max_columns = None\n\ntrain_df = pd.read_csv('../input/train.csv')\ntest_df = pd.read_csv('../input/test.csv')\n\npotential_features = ['ps_ind_06_bin',\n                      'ps_ind_07_bin',\n                      'ps_ind_08_bin',\n                      'ps_ind_16_bin',\n                      'ps_ind_17_bin',\n                      'ps_car_08_cat',\n                      'ps_ind_04_cat',\n                      'ps_car_03_cat',\n                      'ps_car_11_cat',\n                      'ps_car_09_cat',\n                      'ps_car_06_cat',\n                      'ps_ind_05_cat',\n                      'ps_car_05_cat',\n                      'ps_car_04_cat',\n                      'ps_car_01_cat',\n                      'ps_car_02_cat',\n                      'ps_ind_02_cat',\n                      'ps_car_07_cat',\n                      'ps_car_13',\n                      'ps_car_12',\n                      'ps_reg_02',\n                      'ps_reg_03',\n                      'ps_car_15',\n                      'ps_reg_01',\n                      'ps_ind_15',\n                      'ps_ind_01',\n                      'ps_car_14',\n                      'ps_ind_03',\n                      'ps_ind_14']\n\ntrain_df = train_df[['target'] + potential_features]\n\n#save test id's for later\ntest_ids = test_df['id']\ntest_df = test_df[potential_features]","execution_count":1},{"cell_type":"code","metadata":{"_uuid":"f939b1a31069ce2709b8746d77e09d2a5058f354","_cell_guid":"5bd3c82c-95f2-4b7e-8934-b988f04c07bf","collapsed":true},"outputs":[],"source":"# Compute gini\n# from CPMP's kernel https://www.kaggle.com/cpmpml/extremely-fast-gini-computation\nfrom numba import jit\n@jit\ndef eval_gini(y_true, y_prob):\n    y_true = np.asarray(y_true)\n    y_true = y_true[np.argsort(y_prob)]\n    ntrue = 0\n    gini = 0\n    delta = 0\n    n = len(y_true)\n    for i in range(n-1, -1, -1):\n        y_i = y_true[i]\n        ntrue += y_i\n        gini += y_i * delta\n        delta += 1 - y_i\n    gini = 1 - 2 * gini / (ntrue * (n - ntrue))\n    return gini","execution_count":2},{"cell_type":"code","metadata":{"_uuid":"8be9e69947c3afdede00a8e1952550bd43ca4beb","_cell_guid":"85ca7226-0e33-4bc0-8b3e-53b19bb02509"},"outputs":[],"source":"#Balancing: Oversample positive class...add randomly sampled positive class by a factor of over_factor\nover_factor = 0.6\nrow_count = round(len(train_df[train_df['target'] == 1]) * over_factor)\nnew_rows = train_df[train_df['target'] == 1].sample(row_count, replace=True, random_state=1)\n\ntrain_df = train_df.append(new_rows, ignore_index=True)\ntrain_df['target'].value_counts(1)","execution_count":3},{"cell_type":"code","metadata":{"_uuid":"96954e0be0a8eabe369f0573273785136cf45565","_cell_guid":"c74a2069-6fa9-4f13-9986-572adfba548c"},"outputs":[],"source":"#Find columns with missing data\ntrain_miss = []\ntest_miss = []\nprint(\"train columns with missing data: \")\nfor col in train_df.columns:\n    if (train_df[col].min() == -1) and ('cat' not in col):\n        train_miss.append(col)\n        print (col)\n\nprint(\"\\ntest columns with missing data: \")\nfor col in test_df.columns:\n    if (test_df[col].min() == -1) and ('cat' not in col):\n        test_miss.append(col)\n        print (col)","execution_count":4},{"cell_type":"code","metadata":{"_uuid":"e122ffa4dae9b72c32a7c953bf6a028b304d8d4a","_cell_guid":"115c9845-34c0-48b6-ad55-657d90138b80"},"outputs":[],"source":"for col in train_miss:\n    print (\"Train: \", col, \": \", len(train_df[train_df[col] == -1])) #count number of missing data points\n    \nfor col in test_miss:\n    print (\"Test: \", col, \": \", len(test_df[test_df[col] == -1])) #count number of missing data points","execution_count":5},{"cell_type":"code","metadata":{"_uuid":"d1bc80254a292add418d40d9a07db3ed37eb2b04","_cell_guid":"43de601b-83e0-4183-9b84-514b63336663","collapsed":true},"outputs":[],"source":"#Missing Value Handling\n\n#For categorical features, I plan to treat missing values as another category, so nothing needs to be done\n#The other features with remaining missing values are 'ps_car_12', 'ps_reg_03', 'ps_car_14'\n#For ps_car_12, I'll simply replace the missing values with the mean of the values present\n\ntrain_df.loc[train_df['ps_car_12'] == -1, 'ps_car_12'] = train_df[train_df['ps_car_12'] != -1]['ps_car_12'].mean()","execution_count":6},{"cell_type":"code","metadata":{"_uuid":"63eaf23b0661d3d5861b14f98f03fe2079856b72","_cell_guid":"75eb96e9-3c7e-426e-9e82-f2dfe4a73e88","collapsed":true},"outputs":[],"source":"#For the other two missing features, I'll use linear regression using the most correlated features\nfrom sklearn import linear_model\nfeature_mod = linear_model.LinearRegression()\n\nfrom xgboost import XGBRegressor\nfeature_xgb = XGBRegressor(n_estimators=200)\n\n# Fill missing features for ps_car_14, using ps_car_12 and ps_car 13\ncorr_list = ['ps_car_12', 'ps_car_13']\nfeature_mod.fit(train_df[train_df['ps_car_14'] != -1][corr_list], train_df[train_df['ps_car_14'] != -1]['ps_car_14'])\ntrain_df.loc[train_df['ps_car_14'] == -1, 'ps_car_14'] = feature_mod.predict(train_df[train_df['ps_car_14'] == -1][corr_list])\ntest_df.loc[test_df['ps_car_14'] == -1, 'ps_car_14'] = feature_mod.predict(test_df[test_df['ps_car_14'] == -1][corr_list])\n\n# Fill missing features for ps_reg_03, using ps_reg_02, ps_car_12, ps_car 13, and ps_ind_01\n# Should consider trying other models to impute ps_reg_03 if that feature proves to be important\ncorr_list = ['ps_reg_02', 'ps_car_13', 'ps_car_12', 'ps_ind_01']\nfeature_xgb.fit(train_df[train_df['ps_reg_03'] != -1][corr_list], train_df[train_df['ps_reg_03'] != -1]['ps_reg_03'])\ntrain_df.loc[train_df['ps_reg_03'] == -1, 'ps_reg_03'] = feature_xgb.predict(train_df[train_df['ps_reg_03'] == -1][corr_list])\ntest_df.loc[test_df['ps_reg_03'] == -1, 'ps_reg_03'] = feature_xgb.predict(test_df[test_df['ps_reg_03'] == -1][corr_list])","execution_count":7},{"cell_type":"code","metadata":{"_uuid":"55e73338f00451d742dadef5c3eb3844d7ced690","_cell_guid":"0260716b-8076-4f3d-aa8c-9016a096d718","collapsed":true},"outputs":[],"source":"#Encoding\n#I'll use one-hot encoding for all categorical variables except ps_car_11_cat (cardinality is too high)\ncat_cols = [x for x in potential_features if '_cat' in x ]\ncat_cols = list(set(cat_cols) - set(['ps_car_11_cat']))\n\ntrain_df = pd.get_dummies(data=train_df, columns=cat_cols, drop_first=True)\ntest_df = pd.get_dummies(data=test_df, columns=cat_cols, drop_first=True)","execution_count":8},{"cell_type":"code","metadata":{"_uuid":"5aca98cea5a9194bdac12d7d5f25caad46690f7a","_cell_guid":"571f2ccd-a814-4527-accf-7dcfb31899c8","collapsed":true},"outputs":[],"source":"#Next, I'll use binary encoding for ps_car_11_cat\n#The following creates binary format with 0 padding for number of columns required\ncolumns_needed = max(train_df['ps_car_11_cat'].max().item().bit_length(), \n                    test_df['ps_car_11_cat'].max().item().bit_length())\nformat_string = '0>'+ str(columns_needed) + 'b'\n\n#The rest of this cool trick was inspired from here: https://stackoverflow.com/questions/46775546\n#First do train_df\nbin_cols = train_df['ps_car_11_cat'].apply(lambda x: format(x, format_string)).str.extractall('(\\d)').unstack().astype(np.int8).add_prefix('ps_car_11_cat_b')\n\nbin_cols.columns = bin_cols.columns.droplevel()\ntrain_df = pd.concat([train_df, bin_cols], axis=1)\ntrain_df.drop('ps_car_11_cat', inplace=True, axis=1)\n\n#Now do test_df\nbin_cols = test_df['ps_car_11_cat'].apply(lambda x: format(x, format_string)).str.extractall('(\\d)').unstack().astype(np.int8).add_prefix('ps_car_11_cat_b')\n\nbin_cols.columns = bin_cols.columns.droplevel()\ntest_df = pd.concat([test_df, bin_cols], axis=1)\ntest_df.drop('ps_car_11_cat', inplace=True, axis=1)","execution_count":9},{"cell_type":"code","metadata":{"_uuid":"be59bd416e280a762a2589bfbbca55a0ea3b67fc","_cell_guid":"ccb4499d-b9cf-4903-bfb5-f4a336ca92c1","collapsed":true},"outputs":[],"source":"# Set up classifier\nfrom xgboost import XGBClassifier\n\n#MAX_ROUNDS = 400 #original\nMAX_ROUNDS = 400\n#LEARNING_RATE = 0.07 #original\nLEARNING_RATE = 0.07\n\nmodel = XGBClassifier(    \n                        n_estimators=MAX_ROUNDS,\n                        max_depth=4,\n                        objective=\"binary:logistic\",\n                        learning_rate=LEARNING_RATE, \n                        subsample=.8,\n                        min_child_weight=6,\n                        colsample_bytree=.8,\n                        gamma=10,\n                        reg_alpha=8,\n                        reg_lambda=1.3\n                     )","execution_count":10},{"cell_type":"code","metadata":{"_uuid":"cbafb23ef87393b622adac3590914b8e44973743","_cell_guid":"4aa4ad99-2926-45a0-b622-abdd728110fc"},"outputs":[],"source":"# based on https://www.kaggle.com/aharless/xgboost-cv-lb-284 with minor modifications\n\nfrom sklearn.model_selection import KFold\n\nK = 5\nkf = KFold(n_splits = K, random_state = 1, shuffle = True)\n\ny_valid_preds = 0 * train_df['target']\ny_test_preds = 0\n\nfor i, (train_index, valid_index) in enumerate(kf.split(train_df)):\n    print(\"\\nStarting fold {}\".format(i+1))\n    start = time.time()\n    \n    y_train = train_df['target'].loc[train_index]\n    X_train = train_df.drop('target', axis=1).loc[train_index]\n    y_valid = train_df['target'].loc[valid_index]\n    X_valid = train_df.drop('target', axis=1).loc[valid_index]\n      \n    fit_model = model.fit(X_train, y_train, verbose=True)\n    \n    preds = fit_model.predict_proba(X_valid)[:, 1]\n    gini = eval_gini(y_valid, preds)\n    y_valid_preds.loc[valid_index] = preds\n\n    y_test_preds += fit_model.predict_proba(test_df)[:, 1]\n    \n    print(\"Fold {} Gini score: {}\".format(i, gini))\n    print(\"Completed fold {} in {:.2f} minutes\\n\".format(i+1, (time.time() - start)/60))\n\ny_test_preds /= K\n\nprint( \"\\nGini for full training set:\" )\neval_gini(train_df['target'], y_valid_preds)","execution_count":11},{"cell_type":"code","metadata":{"_uuid":"64b1b735ea181621a9fcd5699e95e45101b38444","_cell_guid":"6c663e59-0707-4620-80a9-655165c6a287"},"outputs":[],"source":"from xgboost import plot_importance\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# plot feature importance\nfig, ax = plt.subplots(figsize=(14, 18))\nplot_importance(fit_model, ax=ax)","execution_count":12},{"cell_type":"code","metadata":{"_uuid":"093f2610bc6bd5562a29e27b4970b3d16e5ffd8f","_cell_guid":"c1a849ba-57b9-44e9-b787-67ece805592b","collapsed":true},"outputs":[],"source":"# Create submission file\nsub = pd.DataFrame()\nsub['id'] = test_ids\nsub['target'] = y_test_preds\nsub.to_csv('xgb_submit_3.csv', float_format='%.6f', index=False)","execution_count":13},{"cell_type":"markdown","metadata":{"_uuid":"e67f54f37c3b98c1765243a30028031743ad0b20","_cell_guid":"ef66ef33-2b12-4255-b2aa-f8d358379027"},"source":"**Version 1:** \nCV Gini score: 0.304170535958173   LB score: 0.279 (position #2458)\n* All potential features included\n* Over_factor = 0.6\n* Binary encoding for ps_car_11_cat\n* Linear regression to complete nulls for ps_reg_03\n\n**Version 2:** \nCV Gini score: 0.29678440600440603   LB score: 0.277 (position # n/a)\n* All potential features included\n* Over_factor = 0.6\n* Binary encoding for ps_car_11_cat\n* Linear regression to complete nulls for ps_reg_03\n* MAX_ROUNDS = 200, reg_alpha=9, reg_lambda=1.7\n\n**Version 3:** \nCV Gini score: 0.30403963632991526   LB score: 0.280 (position #2460)\n* All potential features included\n* Over_factor = 0.6\n* Binary encoding for ps_car_11_cat\n* XGBRegression to complete nulls for ps_reg_03\n\n"},{"cell_type":"markdown","metadata":{"_uuid":"dc02ec139f0214f6a30eb7fb1c6b7aa3af2273a0","_cell_guid":"a20107ce-603d-4037-9c90-007c5e559fbe"},"source":"**Variables:**\n* Features to include\n* Level of over-sampling of positive class\n* Null handling technique (particularly for ps_reg_03)\n* Presence/absence of scaling\n* Category encoding method (particuarly for ps_car_11_cat)\n* XGBoost parameters"}],"nbformat":4}