{"nbformat_minor":1,"metadata":{"language_info":{"version":"3.6.3","codemirror_mode":{"version":3,"name":"ipython"},"name":"python","nbconvert_exporter":"python","file_extension":".py","pygments_lexer":"ipython3","mimetype":"text/x-python"},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"}},"cells":[{"metadata":{"_uuid":"de843714198868d3dfb0813172d8a7a9eb53df08","collapsed":true,"_cell_guid":"96148e5f-c46e-4167-b534-418c2c26ce00"},"source":"# Porto Seguro’s Safe Driver Prediction\n\n### Predicting if a driver will file an insurance claim next year\n\n![Porto Seguro Image](https://www.inbenta.com/wp-content/uploads/2016/11/7266.jpg)","cell_type":"markdown"},{"metadata":{"_uuid":"8687cb1afb22682727ef244cea1f7f9b3d30ebcc","_cell_guid":"2d82b7e1-f984-49ac-8d71-c9c2943e70f8"},"source":"## Introduction\n\nNothing ruins the thrill of buying a brand new car more quickly than seeing your new insurance bill. The sting’s even more painful when you know you’re a good driver. It doesn’t seem fair that you have to pay so much if you’ve been cautious on the road for years.\n\nPorto Seguro, one of Brazil’s largest auto and homeowner insurance companies, completely agrees. Inaccuracies in car insurance company’s claim predictions raise the cost of insurance for good drivers and reduce the price for bad ones.\n\nIn this competition, we’re challenged to build a model that predicts the probability that a driver will initiate an auto insurance claim in the next year. While Porto Seguro has used machine learning for the past 20 years, they’re looking to Kaggle’s machine learning community to explore new, more powerful methods. A more accurate prediction will allow them to further tailor their prices, and hopefully make auto insurance coverage more accessible to more drivers.\n\n## Approach\n\nUsing data visualization techniques with the help of useful libraries such as [Matplotlib](https://matplotlib.org) and [Seaborn](http://seaborn.pydata.org), we are able to identify relationships between various features in the given dataset.\n\nFollowing the identification of such relationships, we impute missing data values. For categorical features, we simply create a new category to account for the missing data (i.e. NA category). For numeric features, we can opt to use the median of the distribution to impute missing data.\n\nAfter the data is cleaned and processed, we identify features which are informative of the target label. Following which, we conduct feature engineering on our existing pool of features to create new informative features. \n\nLastly, we fit an [Extreme Gradient Boosting](http://xgboost.readthedocs.io/en/latest/model.html) (otherwise known as the XGB model) Model to our data. Using cross-validation via the Stratified KFolds method, we select the best model (best number of trees) to predict for the given testing set.\n\n## Evaluation\n\nWe will use the Normalized Gini Coefficient as our evaluation metric, similar to the evaluation criteria set by Porto Seguro. For a more comprehensive understanding of what exactly the Normalized Gini Coefficient is, please visit this [kernel](https://www.kaggle.com/batzner/gini-coefficient-an-intuitive-explanation).\n\n## Afternote\n\nAfter submission, it turns out that our XGB model achieved a score of 0.279, which places us at the top 48 percentile of the competition. While not spectacular, I'm just glad that I learnt much more about the specifics behind the Extreme Gradient Boosting model (and its implementation in Python), and have a better idea of how the Normalized Gini Coefficient works now.\n\nAlso, in the event that you found this kernel useful, please take a look at some other kernels which I have referenced in my analysis (they were really useful in helping me understand how):\n\n* [HyungsukKang's Stratified KFold+XGBoost+EDA Tutorial(0.281)](https://www.kaggle.com/sudosudoohio/stratified-kfold-xgboost-eda-tutorial-0-281)\n* [Rudolph's Porto: xgb+lgb kfold LB 0.282](https://www.kaggle.com/rshally/porto-xgb-lgb-kfold-lb-0-282)\n* [Olivier's XGB classifier, upsampling LB 0.283](https://www.kaggle.com/ogrellier/xgb-classifier-upsampling-lb-0-283)","cell_type":"markdown"},{"metadata":{"_uuid":"3d8db12723c5812869076c04577ce4fdb70c1b7b","_cell_guid":"0677ea4a-52fd-4aa7-9130-2eaf3c3d4694"},"source":"## Table of Contents\n\n1. [Importing key libraries and reading dataframes](#Importing-key-libraries-and-reading-dataframes)\n2. [Exploratory Data Analysis](#Exploratory-Data-Analysis)\n3. [Feature Selection](#Feature-Selection)\n    1. [Binary and Numeric Features](#Binary-and-Numeric-Features)\n    2. [Categorical Features](#Categorical-Features)\n    3. [Subsetting the dataframe](#Subsetting-the-dataframe)\n4. [Missing Data Imputation](#Missing-Data-Imputation)\n5. [Feature Importances](#Feature-Importances)\n6. [Feature Engineering](#Feature-Engineering)\n    1. [Polynomial Features](#Polynomial-Features)\n7. [Model Fitting](#Model-Fitting)","cell_type":"markdown"},{"metadata":{"_uuid":"3095127e48294b276942a3d18463c88f4ea3c693","_cell_guid":"cfa51cf6-afa0-42b1-b757-736fbde13bea"},"source":"### Importing key libraries and reading dataframes","cell_type":"markdown"},{"metadata":{"_uuid":"aaa7149eb636cbe14a4cbdfebe4e4212cf66cc36","collapsed":true,"_cell_guid":"20d7a27c-1ffc-4fee-a3c6-f660589dd777"},"source":"%matplotlib inline\nimport pandas as pd # Dataframe manipulation\nimport numpy as np \nimport matplotlib.pyplot as plt # Base plotting\nimport seaborn as sns # Sophisticated plotting (?)\nimport warnings\n# Ignore all warnings - users beware\nwarnings.filterwarnings(\"ignore\")","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"0e2e26f90dbc0390bff8a9c999874ebcfd4f8dc0","collapsed":true,"_cell_guid":"33b004db-05bd-4f60-b62d-6e96c50243b1"},"source":"# Read dataframe into Python\ndf_train = pd.read_csv('../input/train.csv')\ndf_test = pd.read_csv('../input/test.csv')","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"0d3abf39f034a8db35dd8620bccfac3e5bcb89f5","_cell_guid":"56289c22-344b-41a2-af17-7aa76ab67832"},"source":"### Exploratory Data Analysis\n\nNow that we have loaded the dataframe in Python, let's combine the training and testing dataset. We can split them later after we have conducted feature transformation, selection and scaling. \n\nThen, we  take a quick look at the first 5 rows of the data, along with its dimensions.","cell_type":"markdown"},{"metadata":{"_uuid":"46043f2f7af0d382820e2aa724433927e54f3fb4","collapsed":true,"_cell_guid":"6f481abf-a873-4c60-9747-ca8b2c7d569b"},"source":"# Combine the training and test dataset\ndf = pd.concat([df_train, df_test])","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"0fe5c439b4c32547ccd791a971d182cad77d1b05","collapsed":true,"_cell_guid":"8e584b69-1831-4e14-91d1-875de600277f"},"source":"df.set_index('id', inplace = True)\ndf.head(5)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"3f92c9958153648fbd220bd2fe350f0a437b7b52","collapsed":true,"_cell_guid":"040e4a25-3bfd-460d-9a70-79c988e02d26"},"source":"# print dimensions of dataframes\nprint(df.shape)\nprint(df_train.shape)\nprint(df_test.shape)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"2fcdb6798562db4e2409205edda8d77a4cba96f0","_cell_guid":"11dba63c-19ee-4daa-8ced-1e2af12d0389"},"source":"Let's call on the `describe` function in Pandas to understand the dataframe better.","cell_type":"markdown"},{"metadata":{"_uuid":"6b7b07c508bdb36db0ed36f004d450d504d83aa6","collapsed":true,"_cell_guid":"d616fa94-dfbc-4481-a700-d9df211ce0f7"},"source":"df.describe()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"f4168b2d2a9ba9f9334827121095306ae450a8d7","_cell_guid":"b750acbf-34de-4766-87ec-f21149f80557"},"source":"From the summary of the dataset, we note that there are some features which ends with the word 'bin', while other words might end with the letter 'cat'. Also, we note that there are negative values in the dataset.\n\nA quick look at the Kaggle page seems to suggest that features that end with the word 'bin' are binary features, while features which end with the word 'cat' are categorical features. \n\nWe do note that from the [Kaggle page](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/data) there are missing values (as indicated by a '-1') in the dataset. However, nothing seems to be too alarming from this exercise. Let's check for missing values in the dataframe.","cell_type":"markdown"},{"metadata":{"_uuid":"778a62a317e82d4674cd6440c9e4dcced9085fdd","collapsed":true,"_cell_guid":"e1022070-82e5-447a-ae8a-e62be4867097"},"source":"(pd.DataFrame(np.sum(df.apply(lambda x: x == -1))\n              /len(df))[0][pd.DataFrame(np.sum(df.apply(lambda x: x == -1))/len(df))[0] != 0])","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"a13abdcd913d92d8b003bfdf81c1333dc5d79747","_cell_guid":"9967537f-06c5-47aa-8a01-7ae7b5a83f78"},"source":"We note that 3 features, `ps_car_03_cat`, `ps_car_05_cat` and `ps_reg_03` contains a significant amount of missing values (>15% of values are missing from the dataframe).","cell_type":"markdown"},{"metadata":{"_uuid":"3792089979d18a23cc4415c56a4d211cac58e68d","_cell_guid":"879ac9da-a3c4-40c7-967d-bb25956ddfc9"},"source":"While we can assume that Porto Seguro has mapped all missing values to take on the value -1, let's check whether there are any remaining missing values.","cell_type":"markdown"},{"metadata":{"_uuid":"19bb3c6e0aa909dc0d826a03767f20ac1ea8f50e","collapsed":true,"_cell_guid":"7c7c5ff5-61d9-4338-8675-37d2d194aed7"},"source":"np.sum(pd.isnull(df))","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"7013cd13ecdbcd0a66722a7f89ef91f25a37df91","_cell_guid":"2b789e5a-76ca-4cb0-8aa6-63169d118dae"},"source":"It turns out that there is none.","cell_type":"markdown"},{"metadata":{"_uuid":"2e2a8a4ec8bd5097b471c8fc73e60f7baf4f13ee","_cell_guid":"be7ff9a1-8877-4e70-83d8-27b75f5a2e7c"},"source":"Before we proceed to remove these features, let's take a look at the correlation between our features and the target label. It wouldn't be wise to remove features which are really informative of the target label.\n\nTo do this, we separate the features into categorical features, and binary + numeric features.","cell_type":"markdown"},{"metadata":{"_uuid":"7d89f53ca48c106104de2788f7f3c2fb947c61d9","collapsed":true,"_cell_guid":"d9c50340-84e4-45d3-9f60-4c54ae4fd491"},"source":"categorical_features = df.columns[df.columns.str.endswith('cat')].tolist()\nbinary_features = df.columns[df.columns.str.endswith('bin')].tolist()\nnumeric_features = [feature for feature in df.columns.tolist()\n                    if feature not in categorical_features and feature not in binary_features]","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"15f85b2019b18cda2632dbefd975497caacbb86a","collapsed":true,"_cell_guid":"19da669f-8cb5-41ab-9326-e7d3f7667072"},"source":"binary_numeric = binary_features + numeric_features","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"af93ca5ddcb5581b2be4e0316dc13cc838d2533b","_cell_guid":"2db81d69-1000-4358-a05c-78f90bc8341b"},"source":"Are there any categorical features which were supposed to be classified as binary features? We can use the `set` function to find the unique values that the feature can take on.","cell_type":"markdown"},{"metadata":{"_uuid":"515dd9c77b3f45a8074b4ac64962df590eb40469","collapsed":true,"_cell_guid":"be55af1a-cded-4df8-90b9-d203c50abd5c"},"source":"df[categorical_features].apply(set)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"6145fba142c91a21ba61928f7a0dfb54521f7c55","_cell_guid":"f04ef341-67f5-4e29-b6f9-e2d02cc99be6"},"source":"It appears that 6 features should (could) be classified as binary features.","cell_type":"markdown"},{"metadata":{"_uuid":"cbd07489ff6961f9e2c3236803bedaac61a9d20a","collapsed":true,"_cell_guid":"7cd59b81-de92-4619-b11c-673030228395"},"source":"for feature in ['ps_car_02_cat', 'ps_car_03_cat', 'ps_car_05_cat', \n                'ps_car_07_cat', 'ps_car_08_cat', 'ps_ind_04_cat']:\n    binary_numeric.append(feature)\n    binary_features.append(feature)\n    categorical_features.remove(feature)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"05ed07045d80c2c036f6277d00ae6a07b83da9a2","collapsed":true,"_cell_guid":"4467bcf7-4fe5-4305-961b-91b5b5da0138"},"source":"categorical_features","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"eafebb58f92e8b1ed763ab6d6386c0d60ffbc6f8","_cell_guid":"ecdab558-759b-4564-b874-e27d27fbd0d5"},"source":"For now, let's take a look at the correlation matrix across different features, regardless of whether they are numeric, binary or categorical features.","cell_type":"markdown"},{"metadata":{"_uuid":"76c70bc342aeba09ebaca220940aec4d787c67eb","collapsed":true,"_cell_guid":"c0b535a7-4df9-422f-951f-5589cc1a8e8b"},"source":"df[df == -1] = np.nan","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"c7f0fa007e4158d8c358febd151ce7772e73d6ee","collapsed":true,"_cell_guid":"b210f35a-0047-4904-8577-abb93e3d2e4f"},"source":"sns.set_style('white')\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\nplt.figure(figsize=(20,15))\n\nsns.heatmap(df[binary_numeric].corr(), vmin = -1, vmax = 1, cmap=cmap)\n\nplt.show()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"b272c47926064d69bdfc4b774123dbd6d6660320","_cell_guid":"1e50dd4c-1f8c-4f38-98f1-6f99f27a750e"},"source":"From this heatmap, we note that only a handful of features are informative of the target label. In particular, we note that the `ps_calc_` features are not correlated with any other features. Let's take a closer look at the correlation between our features and the target label.","cell_type":"markdown"},{"metadata":{"_uuid":"74505dbe88078ed27431795416a3586323457f1a","collapsed":true,"_cell_guid":"05d38b98-21d1-464d-ad06-f6434af3cc91"},"source":"plt.figure(figsize=(20, 15))\n(df.corr()\n     .target\n     .drop('target')\n     .sort_values(ascending=False)\n     .plot\n     .barh())","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"2a1260797197d957b004e6e21c2c666933f5dd25","_cell_guid":"895bd412-e3f3-4dc9-96f6-df41a35ecc2b"},"source":"Before we begin to plot the numeric and binary features, let's see what unique values that the categorical features can take on.","cell_type":"markdown"},{"metadata":{"_uuid":"4d2b2eea510aa0de2a2c96af1d049a6b038d9db5","_cell_guid":"3936a232-c39e-45d7-9eff-da02e6393c19"},"source":"From the horizontal bar plots, it appears that many of the features have correlation which are close to 0 with the target label. Let's take a look at the distributions of the features.","cell_type":"markdown"},{"metadata":{"_uuid":"5284737d75462ca305c852653a2fb2fdb89a0f5a","collapsed":true,"_cell_guid":"bd0ae773-81b2-44b6-9fa2-b2da046301f7"},"source":"print('No. of numeric features: %d' % len(numeric_features))\nprint('No. of binary features: %d' % len(binary_features))","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"a2e133c5a43e1a358224675a5187cb117a7d360d","collapsed":true,"_cell_guid":"92278163-fdfd-493c-a31a-5dd63e733b32"},"source":"plt.figure(figsize=(20,20))\nfor idx, num_feat in enumerate(numeric_features):\n    plt.subplot(5, 6, idx+1)\n    sns.distplot(df[num_feat].dropna(), kde = False, norm_hist=True)\n\nplt.show()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"70febc9393bb30958ef91c564c6a1159dff3708b","collapsed":true,"_cell_guid":"96a50355-ce54-41d9-9e6b-8be9c933ba27"},"source":"plt.figure(figsize=(20,20))\nfor idx, bin_feat in enumerate(binary_features):\n    plt.subplot(6, 4, idx+1)\n    sns.distplot(df[bin_feat].dropna(), kde = False, norm_hist=True)\n\nplt.show()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"6a40c4dfb845385703f88a25838855d632e78428","_cell_guid":"dea726f8-b4c1-43a3-af7d-9198fac26e33"},"source":"Let's take a look at our categorical features now.","cell_type":"markdown"},{"metadata":{"_uuid":"83618875b62acaceaf7d4ec3004763cd3271196e","collapsed":true,"_cell_guid":"95f36d0b-2aaa-47f9-a552-fe9001788ada"},"source":"len(categorical_features)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"018dc2cc549a1adf6f3d0ebb875fcd462d198ebf","_cell_guid":"de359227-33e9-4b16-8454-255a5d586cae"},"source":"Of the categorical features, what are their distributions?","cell_type":"markdown"},{"metadata":{"_uuid":"0eb4885b91e5e0034d4dd6aa45da7ca20fa37fb7","collapsed":true,"_cell_guid":"d136f479-520f-49cf-9a68-c861f7cd83cc"},"source":"plt.figure(figsize=(20,15))\n\nfor idx, cat_feat in enumerate(categorical_features):\n    plt.subplot(4, 2, idx+1)\n    sns.distplot(df[cat_feat].dropna(), kde=False, norm_hist=True)\n    \nplt.show()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"e42658843377f2f3bdc1bd0e6f90e37ea9573ca2","collapsed":true,"_cell_guid":"69494b1c-07c9-4659-a3c7-458ba1dcbc85"},"source":"plt.figure(figsize=(20,15))\n\nfor idx, cat_feat in enumerate(categorical_features):\n    plt.subplot(4, 2, idx+1)\n    sns.pointplot(x=cat_feat, y='target', data=df.iloc[:df_train.shape[0]])\n    \nplt.show()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"fc64ecadf89151e898c3a7776dadea755b039636","_cell_guid":"2da3ef74-defd-4f4f-bbfd-e7841be80a3d"},"source":"From the categorical features, we note the categorical features might be indicative of the target label. \n\nUpon closer inspection, we find that for dense feature values, the probability of survival is low. Let's investigate this phenomenon further.","cell_type":"markdown"},{"metadata":{"_uuid":"2a480da8406f74f25405db525bcbd8a985bb2c95","collapsed":true,"_cell_guid":"dab0cf68-d251-4175-9833-70454ba72e0e"},"source":"fig, axs = plt.subplots(8, 1, figsize=(20, 25))\n\nfor ax, cat_feat in zip(axs, categorical_features):\n    ax2 = ax.twinx()\n    sns.distplot(df[cat_feat].dropna(), kde=False, norm_hist=True, ax = ax)\n    sns.pointplot(x=cat_feat, y='target', data=df.iloc[:df_train.shape[0]], ax=ax2)\n    \nplt.show()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"c2870b541770098befea23e4a156edd922c6ec70","_cell_guid":"5a987c0f-a574-4588-976f-d4714e58296e"},"source":"### Feature Selection\n\nFrom what we have previously seen, we can now proceed to extract features which are more informative of the target label. For example, we note that the feature, `ps_car_01_cat` and `ps_cat_06_cat` are pretty informative.","cell_type":"markdown"},{"metadata":{"_uuid":"5b17f93e9589b3b6d3f8b88664b274a8f1455210","_cell_guid":"1d52f88d-fe3d-4b06-8f50-f81a9b6a74ab"},"source":"#### Binary and Numeric Features\n\nUsing the correlation matrix (in the form of a heatmap) done previously, we impose an artificial correlation threshold (with the target lavbel) of 0.005 to select key binary and numeric features from our dataset.","cell_type":"markdown"},{"metadata":{"_uuid":"f7d7b1357e3a1450c3065c1f571ea4824fb7f989","collapsed":true,"_cell_guid":"6f182677-2da5-44eb-8270-8378bef2ee1a"},"source":"df[df == -1] = np.nan\n\n# Binary and Numeric Features\n\nno_of_features = sum(df[binary_numeric].corr()\n                     .target\n                     .abs()\n                     .drop('target')\n                     .sort_values(ascending=False) > 0.005)\nno_of_features","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"e2886aac9e57f677cb8e1088fe4a1b77508e091b","collapsed":true,"_cell_guid":"ae034239-b3b5-4393-9847-3cef83b7bb51"},"source":"bin_num_features = (df[binary_numeric].corr()\n                    .target\n                    .abs()\n                    .drop('target')\n                    .sort_values(ascending = False))[:no_of_features].index.tolist()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"66c3ad2731b1a57c8cece1f03de354df4bfbfdc2","_cell_guid":"bb4f899e-5ead-4d5b-84e7-b3439bba868a"},"source":"#### Categorical Features\n\nLet's select the key categorical features later, when we plot our feature importances.","cell_type":"markdown"},{"metadata":{"_uuid":"49ccca60f081df9cb941f4e243e5d8aae8af519c","collapsed":true,"_cell_guid":"4383fddb-de96-4064-82b0-53b346176530"},"source":"cat_features = [feature for feature in df.columns.tolist() \n                if (feature not in bin_num_features) and (feature.endswith('cat'))]","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"4f93040ee98eddfd7e567cb301fdca3be87e4b5d","_cell_guid":"8ed369a9-e10a-49c5-b494-755fbebb3a7d"},"source":"#### Subsetting the dataframe\n\nUsing the features selected through the correlation threshold, let's create our new dataframe.\n\nFollowing which, we call on our heatmap again to understand the correlation across our numeric and binary features better.","cell_type":"markdown"},{"metadata":{"_uuid":"f08be5617a576f54db21512063aa564c9ce0209c","collapsed":true,"_cell_guid":"fe431efc-dad8-4ae6-87d3-fd136e0e24c7"},"source":"df_fs1 = df[bin_num_features + cat_features]\n\ndf_fs1['target'] = df.target\nbin_num_feat = [column for column in df_fs1.columns \n                if column not in cat_features]","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"d9aa51f1636b6599e1787814cd33560c5c5568ae","collapsed":true,"_cell_guid":"15c10011-e2ac-46ba-9046-b4fdccbc5787"},"source":"sns.set_style('white')\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\nplt.figure(figsize=(20, 20))\nsns.heatmap(df_fs1[bin_num_feat].iloc[:df_train.shape[0]].corr(), vmin = -1, vmax = 1, \n            annot = True, cmap = cmap)\nplt.plot()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"67fff6fb48a914bfdece8b1a02dda43709f9aed9","_cell_guid":"6c637a8b-399f-4c69-9049-4d678f2631ec"},"source":"After selecting our key features, we note that some of them are correlated with one another. Why might this be a problem?\n\n[Multicollinearity](https://en.wikipedia.org/wiki/Multicollinearity) occurs when one predictor is highly correlated with another predictor. Consequences of multicollinearity are imprecise predictors, standard errors of predictors tend to be higher.\n\nTo test whether multicollinearity is an issue in our case, we can turn to the [Variance Inflation Factor](https://en.wikipedia.org/wiki/Variance_inflation_factor). Simply put, it uses a rule of thumb that $R^2$ > 0.9, where $R$ is the correlation between 2 features. Using that formula, $R$ is approximated to be 0.95.\n\nTo be stricter, we impose a correlation threshold of 0.9 (slightly stricter than the 0.95 rule of thumb) in our selection of independent features. Using this threshold, we remove the feature `ps_ind_14`, as it has a correlation of 0.89 with the another feature, `ps_ind_12_bin`. Also, we note that the feature has a lower correlation with the target label compared to the other feature, `ps_ind_12_bin`.","cell_type":"markdown"},{"metadata":{"_uuid":"f681e2c17b6ece8176929e16fda60a9e91d0a02b","collapsed":true,"_cell_guid":"1a6179b7-28d1-4717-8259-aecc6c724824"},"source":"del df_fs1['ps_ind_14']","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"3f6df6d1ea542f8ac7adceb3f8e8eacb76a37a5a","_cell_guid":"464ef4f7-68dd-4be1-89a1-99e2390d6b34"},"source":"Before we convert the features to dummies, we first impute missing data.","cell_type":"markdown"},{"metadata":{"_uuid":"3899752355f3e179303993382e7297cae02307ce","_cell_guid":"6beaa9c8-49ac-450b-bfa1-5d34160dacea"},"source":"### Missing Data Imputation\n\nLet's proceed to impute our missing data. \n\nFirst, we begin by finding whether there are any NA values which requires us to impute.","cell_type":"markdown"},{"metadata":{"_uuid":"e5f1841a9b001338faf6753df3eaa690803cf594","collapsed":true,"_cell_guid":"0a23e1ee-87d1-4883-b5a9-f23741414ddc"},"source":"np.sum(df_fs1.isnull())","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"6aa1bbb87aeda10843357f5abd32eae70979caf0","_cell_guid":"b9cf0579-6efc-41f2-b848-be8cd5d8a836"},"source":"Let's remove features where missing values account for at least 20% of the data.","cell_type":"markdown"},{"metadata":{"_uuid":"59b35d09372ccece8d2bdb41c2d21c435e5eee18","collapsed":true,"_cell_guid":"e4dcdcd7-0add-441e-b93b-95bc7be9ecd4"},"source":"[feat for feat in df_fs1.columns.tolist() \n if np.sum(pd.isnull(df_fs1[feat])) > (df_fs1.shape[0])*0.20]","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"4aa40bc52c8549c4a414e98e054634d7a5797112","_cell_guid":"b4b11c2c-f93e-490d-b12c-12fbf00862f5"},"source":"Using this simple rule of thumb, we find that the features `ps_car_03_cat` and `ps_car_05_cat` fulfills this criteria.","cell_type":"markdown"},{"metadata":{"_uuid":"46dd69bb1fcbace05f30d14caedc227fd817d2be","collapsed":true,"_cell_guid":"9286c9aa-4315-4293-88ea-4e230c24e228"},"source":"del df_fs1['ps_car_03_cat']\ndel df_fs1['ps_car_05_cat']","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"91989ea83a55d4a82ef1b0bac1bfb568cfa280fc","_cell_guid":"01a75ac6-15ac-4533-babb-b532a40346ff"},"source":"For categorical features which have missing values, we can circumvent this issue by creating a new category for it.","cell_type":"markdown"},{"metadata":{"_uuid":"e2cfca0102c99b332e0cf664d32883231799d550","collapsed":true,"_cell_guid":"d5b29645-6da2-4c6e-90c5-3b31015dd7e1"},"source":"[feat for feat in df_fs1.columns.tolist() \n if (feat.endswith('cat'))  and ((np.sum(pd.isnull(df_fs1[feat]))) > 0)]","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"e740de7ce103104ae3974486a7621b2a41de3ca1","collapsed":true,"_cell_guid":"d32a83bb-96e5-47b8-9530-2719a23e8de4"},"source":"df_fs1.ps_car_02_cat.fillna('-1', inplace = True)\ndf_fs1.ps_car_07_cat.fillna('-1', inplace = True)\ndf_fs1.ps_ind_04_cat.fillna('-1', inplace = True)\ndf_fs1.ps_car_01_cat.fillna('-1', inplace = True)\ndf_fs1.ps_car_09_cat.fillna('-1', inplace = True)\ndf_fs1.ps_ind_02_cat.fillna('-1', inplace = True)\ndf_fs1.ps_ind_05_cat.fillna('-1', inplace = True)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"bff14300e49b0de272d8197d077c6c29986c1060","_cell_guid":"324b1f61-7e6f-4215-b287-efbb510a9899"},"source":"What other columns require us to fill in missing values?","cell_type":"markdown"},{"metadata":{"_uuid":"087468aed8027e88bc39c2fb2d958757dbc29232","collapsed":true,"_cell_guid":"e0fbb22b-cbae-4293-916c-8157d0aa5bf7"},"source":"[feat for feat in df_fs1.columns.tolist() \n if np.sum(pd.isnull(df_fs1[feat])) > 0]","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"ca5fc4b7ec9db5243829f92821be1f4c249caef0","_cell_guid":"6826f5bd-5d37-40e4-b368-46069db3adb6"},"source":"For these features, let's use the median of these features to impute the missing values.","cell_type":"markdown"},{"metadata":{"_uuid":"63b104b5aa21d748e7494c60637d1f466fa944a3","collapsed":true,"_cell_guid":"db02e2e0-f2e4-4fe4-ba61-788ad54babd6"},"source":"df_fs1['ps_car_12'].fillna(df_fs1['ps_car_12'].median(), inplace = True)\ndf_fs1['ps_reg_03'].fillna(df_fs1['ps_reg_03'].median(), inplace = True)\ndf_fs1['ps_car_14'].fillna(df_fs1['ps_car_14'].median(), inplace = True)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"3841134ace6cbed183a84b485c5a5735ca33f9ac","_cell_guid":"187ccefb-aa02-418a-b25d-3f5109c13874"},"source":"Let's check whether there are any more missing values.","cell_type":"markdown"},{"metadata":{"_uuid":"12305ea718a23bd2809370cb9d8f633d680d02cb","collapsed":true,"_cell_guid":"f2a388b3-9e31-4228-b718-6513a8d902f2"},"source":"np.sum(df_fs1.isnull())","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"eea9dac91776b9001c89912052078b71ff21a408","_cell_guid":"2922b687-b9ab-4a21-891c-9c1e93a55021"},"source":"There are no more missing values in our dataset!","cell_type":"markdown"},{"metadata":{"_uuid":"eeb78bc50745258adb25804b05846e293701a5ef","_cell_guid":"ab6523f5-9620-4812-b6c6-4a4953028ce5"},"source":"### Feature Importances\n\nLet's test out how significant our features are in predicting the target label, using the `feature_importances_` method from the RandomForestClassifier class. Following which, we can plot the relative importance of the features using a horizontal barplot.\n\nThe code to generate the `feature_importances_` plot was taken from the [Yhat Blog](http://blog.yhat.com/tutorials/5-Feature-Engineering.html).","cell_type":"markdown"},{"metadata":{"_uuid":"7a353898e0d4a0751332314b6cd8bcc16eb46c8b","_cell_guid":"86b474a9-ba2c-4ff4-8956-d7e60ccabb9b"},"source":"Let's proceed to select our categorical features, using a RandomForestClassifier.","cell_type":"markdown"},{"metadata":{"_uuid":"6ab3f1d1ed2b2b1ce0642e006f75a98890b30035","collapsed":true,"_cell_guid":"ba0ee76c-1a7d-43e3-aaf8-7fa7dcf1c7f3"},"source":"features = np.array([feature for feature in df_fs1.columns.tolist() \n                     if feature != 'target'])","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"67a17528ed44872400bf85e74a27ff8260047648","collapsed":true,"_cell_guid":"86dd7b15-919d-4a65-86a7-9f24ce2dbac1"},"source":"random_state = 1212","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"be58808a9ad1a50aa38056e65515e5249e32c7ae","collapsed":true,"_cell_guid":"4ecf3472-e8b0-4410-b380-999975338770"},"source":"idx = df_fs1[df_fs1.target.notnull()].index.tolist()\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier(50, random_state=random_state)\nclf.fit(df_fs1[features].loc[idx], df_fs1.target.loc[idx])","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"f5a71b67a886e911a6465f29e2de2a1ce3720438","collapsed":true,"_cell_guid":"9749c084-1d8c-4d5a-a3c7-9a68d14de257"},"source":"importances = clf.feature_importances_\nsorted_idx = np.argsort(importances)\n\nplt.figure(figsize=(15, 10))\n\npadding = np.arange(len(features)) + 0.5\nplt.barh(padding, importances[sorted_idx], align='center')\nplt.yticks(padding, features[sorted_idx])\nplt.xlabel(\"Relative Importance\")\nplt.title(\"Variable Importance\")\n\nplt.show()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"5b92bd74911e949fbdc1fef736a17ca6d2cdf0d0","_cell_guid":"7bdaf1db-779c-4863-b106-3b476ef5ecd4"},"source":"From our existing feature set, it appears that most of the categorical and binary features are not really informative in predicting the target label. Nontheless, let's keep these features for now, and see if there is a need to remove them later.","cell_type":"markdown"},{"metadata":{"_uuid":"26ab7711adb37489ca19c71e22f90a47f88dfc02","collapsed":true,"_cell_guid":"73f1acce-c748-4707-a663-cc42db661acf"},"source":"combined = df_fs1[features]\ncombined['target'] = df_train.set_index('id').target","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"5f3e7ffab63412254a8da705eec5344188718214","_cell_guid":"d48715d8-b4ac-4ded-9d68-341fadaad602"},"source":"### Feature Engineering\n\nLet's take a look at our **key** features more closely, and see whether we are able to create new features from our existing set.","cell_type":"markdown"},{"metadata":{"_uuid":"2fd4b8351410ef65f671991567fe65bb8fabe727","collapsed":true,"_cell_guid":"de2e25b1-e586-48b3-b23f-17bfd9089335"},"source":"plt.figure(figsize=(20,20))\n\nplt.subplot(221)\nsns.distplot(combined[combined.target == 0].ps_car_13.dropna(),\n             bins = np.linspace(0, 4, 41), kde = False, norm_hist = True, color = 'red')\nsns.distplot(combined[combined.target == 1].ps_car_13.dropna(),\n             bins = np.linspace(0, 4, 41), kde = False, norm_hist = True, color = 'blue')\nplt.title('ps_car_13 Distribution')\n\nplt.subplot(222)\nsns.distplot(combined[combined.target == 0].ps_reg_03,\n             bins = np.linspace(0, 2, 11), kde = False, norm_hist = True, color = 'red')\nsns.distplot(combined[combined.target == 1].ps_reg_03,\n             bins = np.linspace(0, 2, 11), kde = False, norm_hist = True, color = 'blue')\nplt.title('ps_reg_03 Distribution')\n\nplt.subplot(223)\nsns.distplot(combined[combined.target == 0].ps_car_14,\n             bins = np.linspace(0.2, 0.6, 10), kde = False, norm_hist = True, color = 'red')\nsns.distplot(combined[combined.target == 1].ps_car_14, \n             bins = np.linspace(0.2, 0.6, 10), kde = False, norm_hist = True, color = 'blue')\nplt.title('ps_car_14 Distribution')\n\nplt.subplot(224)\nsns.distplot(combined[combined.target == 0].ps_ind_15.dropna(),\n             bins = np.linspace(0, 15, 16), kde = False, norm_hist = True, color = 'red')\nsns.distplot(combined[combined.target == 1].ps_ind_15.dropna(),\n             bins = np.linspace(0, 15, 16), kde = False, norm_hist = True, color = 'blue')\nplt.title('ps_ind_15 Distribution')","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"0c3a96a22deb9ea89f65978e956e92c545793cb2","collapsed":true,"_cell_guid":"02c22344-924e-4bb7-8d2a-ea1ea8d65b83"},"source":"plt.figure(figsize=(20,15))\n\nplt.subplot(221)\nsns.distplot(combined[combined.target == 0].ps_ind_03.dropna(),\n             bins = range(0, 8, 1), kde = False, norm_hist = True, color = 'red')\nsns.distplot(combined[combined.target == 1].ps_ind_03.dropna(),\n             bins = range(0, 8, 1), kde = False, norm_hist = True, color = 'blue')\nplt.title('ps_ind_03 Distribution')\n\nplt.subplot(222)\nsns.distplot(combined[combined.target == 0].ps_reg_02.dropna(),\n             bins = np.linspace(0, 2, 11), kde = False, norm_hist = True, color = 'red')\nsns.distplot(combined[combined.target == 1].ps_reg_02.dropna(),\n             bins = np.linspace(0, 2, 11), kde = False, norm_hist = True, color = 'blue')\nplt.title('ps_reg_02 Distribution')\n\nplt.subplot(223)\nsns.distplot(combined[combined.target == 0].ps_car_11_cat.dropna(), \n             bins = range(0, 110, 5), kde = False, norm_hist = True, color = 'red')\nsns.distplot(combined[combined.target == 1].ps_car_11_cat.dropna(), \n             bins = range(0, 110, 5), kde = False, norm_hist = True, color = 'blue')\nplt.title('ps_car_11_cat Distribution')\n\nplt.subplot(224)\nsns.distplot(combined[combined.target == 0].ps_ind_01.dropna(),\n             bins = range(0, 8, 1), kde = False, norm_hist = True, color = 'red')\nsns.distplot(combined[combined.target == 1].ps_ind_01.dropna(),\n             bins = range(0, 8, 1), kde = False, norm_hist = True, color = 'blue')\nplt.title('ps_ind_01 Distribution')","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"3e8edf6206ba8452d3587606f0c338bbb45bbccf","collapsed":true,"_cell_guid":"8b9193b4-7a04-486e-ae7a-e218f56445be"},"source":"plt.figure(figsize=(20,15))\n\nplt.subplot(221)\nsns.distplot(combined[combined.target == 0].ps_car_15.dropna(), \n             kde = False, norm_hist = True, color = 'red')\nsns.distplot(combined[combined.target == 1].ps_car_15.dropna(), \n             kde = False, norm_hist = True, color = 'blue')\nplt.title('ps_car_15 Distribution')\n\nplt.subplot(222)\nsns.distplot(combined[combined.target == 0].ps_reg_01.dropna().astype('float'),\n             bins = range(0, 11, 1), kde = False, norm_hist = True, color = 'red')\nsns.distplot(combined[combined.target == 1].ps_reg_01.dropna().astype('float'),\n             bins = range(0, 11, 1), kde = False, norm_hist = True, color = 'blue')\nplt.title('ps_reg_01 Distribution')\n\nplt.subplot(223)\nsns.distplot(combined[combined.target == 0].ps_car_01_cat.dropna().astype('float'), \n             bins = range(-1, 11, 1), kde = False, norm_hist = True, color = 'red')\nsns.distplot(combined[combined.target == 1].ps_car_01_cat.dropna().astype('float'), \n             bins = range(-1, 11, 1), kde = False, norm_hist = True, color = 'blue')\nplt.title('ps_car_01_cat Distribution')\n\nplt.subplot(224)\nsns.distplot(combined[combined.target == 0].ps_car_06_cat.dropna(), \n             bins = range(0, 17, 1), kde = False, norm_hist = True, color = 'red')\nsns.distplot(combined[combined.target == 1].ps_car_06_cat.dropna(), \n             bins = range(0, 17, 1), kde = False, norm_hist = True, color = 'blue')\nplt.title('ps_car_06_cat Distribution')","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"2f8b60e254278b3f83d45fcf995f3fefc0573c57","_cell_guid":"dba77544-ff8e-46e0-8583-9a5bb26208ed"},"source":"There doesn't appear to be any good features we can extract from our existing pool.\n\nLet's take a look at the correlation across features.","cell_type":"markdown"},{"metadata":{"_uuid":"657c5c325e63f14117771d99d77644e4fdeb4f3c","collapsed":true,"_cell_guid":"704a1709-7a83-4ad6-8ab0-68ac69c8aea1"},"source":"combined['target'] = df_train.set_index('id').target\n\nplt.figure(figsize=(20, 15))\nsns.heatmap(combined.corr(), annot = True, cmap = cmap)\nplt.show()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"bd44b66ebd410c3e737b7e36c1044b2d9d01328e","_cell_guid":"385e192f-4ff2-4bc1-907f-0db8c4ec6333"},"source":"#### Polynomial Features\n\nCan interaction terms help to improve the fit of our model? We will focus on the interaction terms of the top 10 features from our previous analysis, to minimise the computational complexity.","cell_type":"markdown"},{"metadata":{"_uuid":"63726cdfb7b3bb272c328084d23897a0e428ba3d","collapsed":true,"_cell_guid":"c392f80b-34d1-45ef-8311-cee98ee7391f"},"source":"ind_var = [feature for feature in combined.columns[sorted_idx][-10:] \n           if feature != 'target']\nind_var.reverse()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"ea143f6eac532daffc08da1a27a258e97fc1789d","collapsed":true,"_cell_guid":"3c1e890d-131e-409f-9077-d2e0b1a4fe18"},"source":"from sklearn.preprocessing import PolynomialFeatures\n\ntrain = combined[pd.notnull(combined.target)][ind_var].reset_index(drop=True)\n\npoly = PolynomialFeatures(interaction_only = True, include_bias = False)\n\ntrain_interaction = pd.DataFrame(poly.fit_transform(train))\ntrain_interaction['target'] = df_train.target","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"577ba6a0f11be22d730edae17157e7e4a1a351e9","_cell_guid":"f3d5225c-9050-467c-9419-af1804845a74"},"source":"What is the performance/variance explained of the interaction features? Let's take a quick look.","cell_type":"markdown"},{"metadata":{"_uuid":"dc0117ad359f7e61cb573371f78bae10169d5fe6","collapsed":true,"_cell_guid":"ca75df16-8802-4480-b7bd-749b5d8a973f"},"source":"features = np.array([feature for feature in train_interaction.columns.tolist()\n                     if feature != 'target'])\n\nclf = RandomForestClassifier(50, random_state = random_state)\nclf.fit(train_interaction.iloc[:df_train.shape[0]][features], \n        train_interaction.iloc[:df_train.shape[0]]['target'])","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"66b27a7a34f5932ea1f2031b3ce27c2a2ad1d792","collapsed":true,"_cell_guid":"c17215ea-3026-4877-a2be-3feb3fc85271"},"source":"importances = clf.feature_importances_\nsorted_idx = np.argsort(importances)\n\nplt.figure(figsize=(15, 10))\n\nplt.figure(figsize=(20, 20))\npadding = np.arange(len(features)) + 0.5\nplt.barh(padding, importances[sorted_idx], align='center')\nplt.yticks(padding, features[sorted_idx])\nplt.xlabel(\"Relative Importance\")\nplt.title(\"Variable Importance\")\n\nplt.show()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"5327dbbaa70feea9f1d2425a3e821a8399a0f7b3","_cell_guid":"3c97f386-5a5e-465d-aded-705653bdd254"},"source":"Looking at the `feature_importances_plot`, we note that feature10 is a strong indicator for the target label. What is feature10?","cell_type":"markdown"},{"metadata":{"_uuid":"b0d79cc498c3f98d8e5c7299ae6aece9f5f2af81","collapsed":true,"_cell_guid":"4f8fc306-6aa2-45e1-aae7-0d8d032ab836"},"source":"[feat for feat in ind_var]","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"4cbb9ad923f2efe8db8ed7288bbadc88ca0ad75a","_cell_guid":"5aa515d2-22e1-4da6-a18f-247b4ca64f74"},"source":"Using the list above, it turns out `feature10` is the interaction term between `ps_car_13` and `ps_reg_03`. Let's include feature10 in our dataframe!","cell_type":"markdown"},{"metadata":{"_uuid":"7f3d841ca89e9cebbe2a8a4326b4c1963e18a071","collapsed":true,"_cell_guid":"84e97f66-8420-4a45-a7d8-b69ad5d05047"},"source":"combined['feature10'] = combined['ps_car_13'] * combined['ps_reg_03']","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"6929cbed484636064a9e4de74a21a4b8f7df1044","_cell_guid":"e59f0235-ed9a-4420-bee0-ee5f86b9b44c"},"source":"To make sure that these features are informative, let's take a look at the correlation of these features with our target label.","cell_type":"markdown"},{"metadata":{"_uuid":"53dd875f8826b137f160b913f269f17c3e2f5af5","collapsed":true,"_cell_guid":"7b4852c3-59a3-4af8-a24c-efdd1500db0f"},"source":"combined['target'] = df_train.set_index('id').target\n\nplt.figure(figsize=(20, 20))\nsns.heatmap(combined.corr(), annot = True)\nplt.show()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"9f110c160680a753a4a0c6c4d629629da32068ab","_cell_guid":"8f375a5c-6986-41c5-8b5b-2211a1e4fe99"},"source":"From the heatmap, it appears that `feature10` isn't strongly correlated with all other features. \n\nNote: It has a correlation of 0.82 with the feature `ps_reg_03`, but that isn't **really** alarming. Let's keep it.","cell_type":"markdown"},{"metadata":{"_uuid":"ce56de3dac8c865f5d01c905f66d2d632999cf18","_cell_guid":"e708d3cb-183a-44e6-9894-82cbf8d09325"},"source":"After the removal of these features, let's see the relative importance of each feature in our dataset!","cell_type":"markdown"},{"metadata":{"_uuid":"ff17d3988723135d8c95954782de1e300242c12c","collapsed":true,"_cell_guid":"7c157992-2b61-40e2-82c1-6380cb1ec8ed"},"source":"features = np.array([feature for feature in combined.columns.tolist()\n                     if feature != 'target'])\n\nclf = RandomForestClassifier(50, random_state = random_state)\nclf.fit(combined[pd.notnull(combined.target)][features], \n        combined[pd.notnull(combined.target)].target)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"fea615ae8f83ae453ea4fd393fc3b681310418aa","collapsed":true,"_cell_guid":"6aef90a7-c552-4463-80f0-3dc764ec5691"},"source":"importances = clf.feature_importances_\nsorted_idx = np.argsort(importances)\n\nplt.figure(figsize=(15, 10))\n\nplt.figure(figsize=(20, 20))\npadding = np.arange(len(features)) + 0.5\nplt.barh(padding, importances[sorted_idx], align='center')\nplt.yticks(padding, features[sorted_idx])\nplt.xlabel(\"Relative Importance\")\nplt.title(\"Variable Importance\")\n\nplt.show()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"02ba2dac90226fc95771d63d9a7590d545cba2a3","_cell_guid":"b4683897-15c0-4f50-a81a-dc05a13807d8"},"source":"From the `feature_importances_` plot, it appears that our new features are doing really well! Let's keep these features as they have high relative importance and low correlation with other features.\n\nLet's remove the target label, and split our dataset into training and testing dataset.","cell_type":"markdown"},{"metadata":{"_uuid":"a61086dd2b34b44c7d8c131ce24ec59eae3ac348","collapsed":true,"_cell_guid":"63faf820-3b63-42af-9f39-43febcb04ce5"},"source":"del combined['target']","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"b05951ffa9a8f435e4aafefdd61f730194d02511","collapsed":true,"_cell_guid":"1dcad5b7-bb5e-4779-be06-69b286eab77d"},"source":"X_train = combined.reset_index(drop = True).iloc[:df_train.shape[0], ]\nX_test = combined.reset_index(drop = True).iloc[df_train.shape[0]:, ]","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"9ea6e7c959405d0f50c86bbfdfe205c01f35b4c4","_cell_guid":"cdbc766e-3c9a-4852-a3d7-e2c25a7318c1"},"source":"### Model Fitting\n\nLet's fit an [Extreme Gradient Boosting](http://xgboost.readthedocs.io/en/latest/) model to predict for the probability of insurance claim.\n\nLet's define our evaluation metric and cost function first. This was taken off [Rudolph's iPython Notebook](https://www.kaggle.com/rshally/porto-xgb-lgb-kfold-lb-0-282/notebook).","cell_type":"markdown"},{"metadata":{"_uuid":"c6edf206186e84710b11fba196803027d2f520d6","collapsed":true,"_cell_guid":"220bc5b8-c89d-4e62-92a4-5683d89a3c36"},"source":"def gini(actual, pred, cmpcol = 0, sortcol = 1):\n    assert( len(actual) == len(pred) )\n    all = np.asarray(np.c_[ actual, pred, np.arange(len(actual)) ], dtype=np.float)\n    all = all[ np.lexsort((all[:,2], -1*all[:,1])) ]\n    totalLosses = all[:,0].sum()\n    giniSum = all[:,0].cumsum().sum() / totalLosses\n    \n    giniSum -= (len(actual) + 1) / 2.\n    return giniSum / len(actual)\n \ndef gini_normalized(a, p):\n    return gini(a, p) / gini(a, a)\n\ndef gini_xgb(preds, dtrain):\n    labels = dtrain.get_label()\n    gini_score = gini_normalized(labels, preds)\n    return 'gini', gini_score","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"9b43bdf17af2f215fe24cfcea456bd0cb3625d96","_cell_guid":"67e2ab32-9c8a-4673-91a2-41faaade66ba"},"source":"Defining our features, X, and target labels, y.","cell_type":"markdown"},{"metadata":{"_uuid":"673e2f199f71b0de4dad00800a6007c59b91a9e7","collapsed":true,"_cell_guid":"168facb2-6e7e-49d4-a5a6-242befad7840"},"source":"features = X_train.columns.tolist\n\nX = X_train.values; test = X_test.values\n\ny = df_train.set_index('id').target.values","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"de6fba8cb9e31b618d4abfcf6ae3bf687a5f5f5f","collapsed":true,"_cell_guid":"4de5a7c3-c4de-4db7-bce4-d91e16a40975"},"source":"params = {\n    'objective': 'binary:logistic',\n    'min_child_weight': 12.0,\n    'max_depth': 5,\n    'colsample_bytree': 0.5,\n    'subsample': 0.8,\n    'eta': 0.025,\n    'gamma': 0.8,\n    'max_delta_step': 1.5\n}","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"01ffbc9fdd8d50c710595bd065552922eefc8766","collapsed":true,"_cell_guid":"eaa8e027-a957-408e-b968-09ad6ba7eb60"},"source":"import xgboost as xgb\nfrom sklearn.model_selection import StratifiedKFold\n\nsubmission = pd.DataFrame()\nsubmission['id'] = df_test['id'].values\nsubmission['target'] = 0\n\nnrounds=1000\nfolds = 5\nskf = StratifiedKFold(n_splits=folds, random_state=random_state)\n\nfor i, (train_index, test_index) in enumerate(skf.split(X, y)):\n    print('XGB KFold: %d: ' % int(i+1))\n    \n    X_subtrain, X_subtest = X[train_index], X[test_index]\n    y_train, y_valid = y[train_index], y[test_index]\n    \n    d_subtrain = xgb.DMatrix(X_subtrain, y_train) \n    d_subtest = xgb.DMatrix(X_subtest, y_valid) \n    d_test = xgb.DMatrix(test)\n    \n    watchlist = [(d_subtrain, 'subtrain'), (d_subtest, 'subtest')]\n    \n    mdl = xgb.train(params, d_subtrain, nrounds, watchlist, early_stopping_rounds=80, \n                    feval=gini_xgb, maximize=True, verbose_eval=50)\n    \n    # Predict test set based on the best_ntree_limit\n    p_test = mdl.predict(d_test, ntree_limit=mdl.best_ntree_limit)\n    \n    # Take the average of the prediction via 5 folds to predict for the test set\n    submission['target'] += p_test/folds","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"33c9f15cda56ff7574a2a3a638d8678de56f08b6","_cell_guid":"5ee9061a-76a6-4714-a6f0-81df37703d7f"},"source":"Looking at the cross-validation scores, it appears that we are performing pretty well across all Stratified Folds (save for the 5th one).\n\nAfter training our model, it's time to submit. Let's see how well we performed.","cell_type":"markdown"},{"metadata":{"_uuid":"26bfb5654da2a138b72d0c06c26fcd1cb2022298","collapsed":true,"_cell_guid":"48125223-dd3c-4ad8-bf35-28cce1d4f72f"},"source":"submission.to_csv('submission.csv', index=False)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"2eddce1a9335ea9783a4e7cca00de430dfaaa59b","_cell_guid":"c10f34b9-f7b0-4e34-afbd-09a9431bf553"},"source":"### Conclusion\n\nOur model achieved a Normalized Gini Coefficient score of 0.279, which places us at the top 48 percentile of the Kaggle competition!","cell_type":"markdown"}],"nbformat":4}