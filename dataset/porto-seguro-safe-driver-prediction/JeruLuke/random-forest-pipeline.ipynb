{"cells":[{"metadata":{"_uuid":"8a58dba16746f81e1a955f55aa2f3ae777423d3f","_cell_guid":"65e40b62-9f77-4f12-a1d4-3987cfc503c0"},"cell_type":"markdown","source":"In this edited version of my kernel, I have included some new features and some others are under progress. Some have been influenced from [THIS KERNEL](https://www.kaggle.com/sudosudoohio/stratified-kfold-xgboost-eda-tutorial-0-281).\n\n# What lies ahead of you?\n\n*  **Data Exploration**\n    * Analyzing Datatypes\n    * Analyzing Missing Values\n    * Visualizing missing values\n    * Memory Usage Analysis\n    \n*  **Data Analysis** (visualizing each and every type of feature in the data set)\n    * Splitting columns based on types\n    * Binary Features\n    * Categorical Features\n    * Continuous/Ordinal Features\n    * Correlation (**ps_calc** have an outrageous attitude!!!)\n* **Feature Engineering** \n    * New Binary features\n    * New Continuous/Ordinal features (*in progress*)\n*  **Modeling**\n    * Gradient Boosting\n    * XGBoost"},{"metadata":{"_uuid":"a56251bda185c7171eba71d63e25283616809d4a","_cell_guid":"bfa0a3f1-2ef3-4cc1-8765-90dcebe10e09"},"cell_type":"markdown","source":"# Importing Libraries and Loading Data"},{"metadata":{"_uuid":"23743af38f0ec256b8c02169a62340606707c134","collapsed":true,"_cell_guid":"d61160ed-4ab6-4fee-9769-95ed0079280f"},"cell_type":"code","execution_count":null,"outputs":[],"source":"import numpy as np # linear algebra\nimport seaborn as sns\nimport missingno as msno\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt\nfrom subprocess import check_output\nfrom sklearn import *\nimport xgboost as xgb\nfrom multiprocessing import *\nfrom ggplot import *\n\ndf_train = pd.read_csv('../input/train.csv')\ndf_test = pd.read_csv('../input/test.csv')\ndf_sample = pd.read_csv('../input/sample_submission.csv')\n# Any results you write to the current directory are saved as output."},{"metadata":{"_uuid":"6ae2e0b6c56cfb4ee5ec84c602444cd6c85e2fae","collapsed":true,"_cell_guid":"79213fd9-8be7-458d-91e3-61d10673a28a"},"cell_type":"code","execution_count":null,"outputs":[],"source":"df_train.shape"},{"metadata":{"_uuid":"0ebe47c489a6672491194ec0db2b31af53559217","collapsed":true,"_cell_guid":"853fab44-f65e-40f9-b2ce-8900a6414cc5"},"cell_type":"code","execution_count":null,"outputs":[],"source":"print(len(df_train.columns))\n#new_cont_ord_cols = [c for c in df_train.columns if not c.startswith('ps_calc_')]\nnew_cont_ord_cols = [c for c in df_train.columns if not c.endswith('bin')]\nno_bin_cat_cols = [c for c in new_cont_ord_cols if not c.endswith('cat')][2:]"},{"metadata":{"_uuid":"9ff8a646222ce9f2fcad53a1189fb14c70925172","collapsed":true,"_cell_guid":"6aaa924e-5c50-45db-96bd-c691b56cf8ae"},"cell_type":"code","execution_count":null,"outputs":[],"source":"''' \ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\ncol = [c for c in train.columns if c not in ['id','target']]\nprint(len(col))\ncol = [c for c in col if not c.startswith('ps_calc_')]\nprint(len(col))\n\ntrain = train.replace(-1, np.NaN)\nd_median = train.median(axis=0)\nd_mean = train.mean(axis=0)\ntrain = train.fillna(-1)\none_hot = {c: list(train[c].unique()) for c in train.columns if c not in ['id','target']}\n\n'''"},{"metadata":{"_uuid":"f658449fe7fba95295adef06a04191ed1a8e4a18","collapsed":true,"_cell_guid":"be37c1ac-0a1a-4e9c-8fa3-5cd6ed69dd6a"},"cell_type":"code","execution_count":null,"outputs":[],"source":"'''\ndef transform_df(df):\n    df = pd.DataFrame(df)\n    dcol = [c for c in df.columns if c not in ['id','target']]\n    df['ps_car_13_x_ps_reg_03'] = df['ps_car_13'] * df['ps_reg_03']\n    df['negative_one_vals'] = np.sum((df[dcol]==-1).values, axis=1)\n    for c in dcol:\n        if '_bin' not in c: #standard arithmetic\n            df[c+str('_median_range')] = (df[c].values > d_median[c]).astype(np.int)\n            df[c+str('_mean_range')] = (df[c].values > d_mean[c]).astype(np.int)\n            #df[c+str('_sq')] = np.power(df[c].values,2).astype(np.float32)\n            #df[c+str('_sqr')] = np.square(df[c].values).astype(np.float32)\n            #df[c+str('_log')] = np.log(np.abs(df[c].values) + 1)\n            #df[c+str('_exp')] = np.exp(df[c].values) - 1\n    for c in one_hot:\n        if len(one_hot[c])>2 and len(one_hot[c]) < 7:\n            for val in one_hot[c]:\n                df[c+'_oh_' + str(val)] = (df[c].values == val).astype(np.int)\n    return df\n\ndef multi_transform(df):\n    print('Init Shape: ', df.shape)\n    p = Pool(cpu_count())\n    df = p.map(transform_df, np.array_split(df, cpu_count()))\n    df = pd.concat(df, axis=0, ignore_index=True).reset_index(drop=True)\n    p.close(); p.join()\n    print('After Shape: ', df.shape)\n    return df\n\ndef gini(y, pred):\n    fpr, tpr, thr = metrics.roc_curve(y, pred, pos_label=1)\n    g = 2 * metrics.auc(fpr, tpr) -1\n    return g\n\ndef gini_xgb(pred, y):\n    y = y.get_label()\n    return 'gini', gini(y, pred)\n\nparams = {'eta': 0.02, 'max_depth': 4, 'subsample': 0.9, 'colsample_bytree': 0.9, 'objective': 'binary:logistic', 'eval_metric': 'auc', 'seed': 99, 'silent': True}\nx1, x2, y1, y2 = model_selection.train_test_split(train, train['target'], test_size=0.25, random_state=99)\n\nx1 = multi_transform(x1)\nx2 = multi_transform(x2)\ntest = multi_transform(test)\n\ncol = [c for c in x1.columns if c not in ['id','target']]\ncol = [c for c in col if not c.startswith('ps_calc_')]\nprint(x1.values.shape, x2.values.shape)\n\n#remove duplicates just in case\ntdups = multi_transform(train)\ndups = tdups[tdups.duplicated(subset=col, keep=False)]\n\nx1 = x1[~(x1['id'].isin(dups['id'].values))]\nx2 = x2[~(x2['id'].isin(dups['id'].values))]\nprint(x1.values.shape, x2.values.shape)\n\ny1 = x1['target']\ny2 = x2['target']\nx1 = x1[col]\nx2 = x2[col]\n\nwatchlist = [(xgb.DMatrix(x1, y1), 'train'), (xgb.DMatrix(x2, y2), 'valid')]\nmodel = xgb.train(params, xgb.DMatrix(x1, y1), 5000,  watchlist, feval=gini_xgb, maximize=True, verbose_eval=50, early_stopping_rounds=200)\ntest['target'] = model.predict(xgb.DMatrix(test[col]), ntree_limit=model.best_ntree_limit+45)\ntest['target'] = (np.exp(test['target'].values) - 1.0).clip(0,1)\n\nsub = pd.DataFrame()\nsub['id'] = test['id']\nsub['target'] = test['target']\nsub.to_csv('xgb1.csv', index=False)\n\n#test[['id','target']].to_csv('xgb_submission.csv', index=False, float_format='%.5f')\n'''"},{"metadata":{"_uuid":"99b89f8ed83433947005e3c7cc7e36545599328d","_cell_guid":"6854b2c5-4f52-4bbe-8bb5-470b709f370c"},"cell_type":"markdown","source":"# Data Exploration\n\nFirst things first, let us explore what we have!"},{"metadata":{"_uuid":"c1518fd7b4ef43ee34be4d3faa8bdc9b854e3a0d","collapsed":true,"_cell_guid":"4d6e5017-015f-4d74-b457-db69d7d19ea1"},"cell_type":"code","execution_count":null,"outputs":[],"source":"df_train.head()"},{"metadata":{"_uuid":"2dcebc1e8edbfeda80c4158ab75a116e8331a911","_cell_guid":"9a620fd4-441d-49c3-8724-17ce7e18ca3f"},"cell_type":"markdown","source":"Saving the **target** variable separately and dropping it from the training set."},{"metadata":{"_uuid":"671195790116b5317030db01b9616c2d9a356d1c","collapsed":true,"_cell_guid":"05091036-b20e-470a-99c6-e75c4096f7bc"},"cell_type":"code","execution_count":null,"outputs":[],"source":"target = df_train['target']\n#df_train = df_train.drop('target', 1)"},{"metadata":{"_uuid":"6dd9111c3366668b3d40104d78e4a248fad8a488","_cell_guid":"72a9741b-7170-4db5-a6cd-4c89ad290d0c"},"cell_type":"markdown","source":"## Analyzing Datatypes\n\nWe only have two datatypes in our dataset: **int** and **float**."},{"metadata":{"_uuid":"8f5cc2d12dedc01105c271530bf764885833f23f","collapsed":true,"_cell_guid":"20cd566e-ae52-4c2a-bdbf-71039fe406ed"},"cell_type":"code","execution_count":null,"outputs":[],"source":"print(df_train.dtypes.unique())\nprint(df_train.dtypes.nunique())\n\nprint(df_test.dtypes.unique())\nprint(df_test.dtypes.nunique())"},{"metadata":{"_uuid":"b179ec1687d97fefe4147ab87ac300fff1a80197","collapsed":true,"_cell_guid":"1f1f17c5-962b-42b3-9436-9f425c5714a7"},"cell_type":"code","execution_count":null,"outputs":[],"source":"pp = pd.value_counts(df_train.dtypes)\npp.plot.bar()\nplt.show()"},{"metadata":{"_uuid":"847774dfe1aa5f6b915653de8059e361541690b6","_cell_guid":"2adf413c-f9d5-49f6-be8a-ffbe72486cc3"},"cell_type":"markdown","source":"## Analyzing Missing Values"},{"metadata":{"_uuid":"7de41aabb54441fb75f2a0171a04b98c7f3ebad0","collapsed":true,"_cell_guid":"1549d650-816b-443a-aa07-529b359eca0a"},"cell_type":"code","execution_count":null,"outputs":[],"source":"print (df_train.isnull().values.any())\nprint (df_test.isnull().values.any())"},{"metadata":{"_uuid":"2da9a0a3f03aa5aa5b7a6f894e286e07a59c499f","_cell_guid":"a36c890d-859e-4c37-a0e9-5b9bf2338eb5"},"cell_type":"markdown","source":"However, as mentioned by someone in the comments, \"This isn't true!\" The missing values have been replaced by -1.\n\nWe will replace them using np.nan and see how it is distributed.\n\n"},{"metadata":{"_uuid":"e8569604229ef89357bdca8f7a59d4faf80e8c62","collapsed":true,"_cell_guid":"764bfa14-b9bf-4312-a826-02312334076f"},"cell_type":"code","execution_count":null,"outputs":[],"source":"#df_train.replace(-1, np.nan)\n#df_test.replace(-1, np.nan)\ndf_train[(df_train == -1)] = np.nan\ndf_test[(df_test == -1)] = np.nan\n\nprint('done') "},{"metadata":{"_uuid":"8095b98db72ea138ad565a6be52f30bcc0b81131","_cell_guid":"bac4c14e-c91d-4f77-8913-d4e8926434d6"},"cell_type":"markdown","source":"Checking for missing values again"},{"metadata":{"_uuid":"4f58cf31aa682a46df44e3b5339e4461e112ed05","collapsed":true,"_cell_guid":"fe6ca5a8-1f48-4d62-a10a-371a3fe08cb5"},"cell_type":"code","execution_count":null,"outputs":[],"source":"print (df_train.isnull().values.any())\nprint (df_test.isnull().values.any())   "},{"metadata":{"_uuid":"3a0a6025937e1489ec18c994e2309df177c0f5c3","_cell_guid":"5b8c0b43-5207-4d91-a0fd-3d6f90677797"},"cell_type":"markdown","source":"Printing list of columns with missing values in both the train and test dataframe:"},{"metadata":{"_uuid":"084a8dbf2d29ac5ed8be5761d5dcb5428cd66acb","collapsed":true,"_cell_guid":"69ea56de-7e9d-4b31-90ab-db7308813bc6"},"cell_type":"code","execution_count":null,"outputs":[],"source":"cols_missing_val_train = df_train.columns[df_train.isnull().any()].tolist()\nprint(cols_missing_val_train)\nprint('\\n')\n\ncols_missing_val_test = df_test.columns[df_test.isnull().any()].tolist()\nprint(cols_missing_val_test)"},{"metadata":{"_uuid":"e34029659734dd2eae8581733921b2c74398cfb3","_cell_guid":"fceddfef-6cfc-4ad6-aa4e-107430b314ca"},"cell_type":"markdown","source":"We see that the train dataframe has an extra column with missing values (**ps_car_12**).\n\n## Visualizing missing values"},{"metadata":{"_uuid":"65a0532b38ac77f4ee1fcfd39633b66e3d0f8450","collapsed":true,"_cell_guid":"2e8d84ce-8f28-4e9b-a4ae-bdead8f174a0"},"cell_type":"code","execution_count":null,"outputs":[],"source":"#--- Train dataframe ---\nmsno.bar(df_train[cols_missing_val_train],figsize=(20,8),color=\"#19455e\",fontsize=18,labels=True,)"},{"metadata":{"_uuid":"f86d616eb207258e327ab5a5af92de060d905439","collapsed":true,"_cell_guid":"bda5e206-0667-43db-9ea8-bc2188786e9e"},"cell_type":"code","execution_count":null,"outputs":[],"source":"#--- Test dataframe ---\nmsno.bar(df_test[cols_missing_val_test],figsize=(20,8),color=\"#50085e\",fontsize=18,labels=True,)"},{"metadata":{"_uuid":"34ee946f25678c096eab7af5652faa0ea3b7f471","_cell_guid":"957d13b0-0b4e-4845-a5da-5478bc2de640"},"cell_type":"markdown","source":"We can see that the missing values a proportional in both the test and train dataframes."},{"metadata":{"_uuid":"706f1f49217a75220d1a5ba3cde828293b195757","collapsed":true,"_cell_guid":"30454b9e-9073-4cc4-a75d-57d47469e457"},"cell_type":"code","execution_count":null,"outputs":[],"source":"#--- Train dataframe ---\nmsno.matrix(df_train[cols_missing_val_train],width_ratios=(10,1),\\\n            figsize=(20,8),color=(0.2,0.2,0.2),fontsize=18,sparkline=True,labels=True)"},{"metadata":{"_uuid":"246f4861e54c5ad3e2f1b7fe53c3b5ce1d7b492e","collapsed":true,"_cell_guid":"d6f21c58-9353-4f24-9d87-1f2ae178e62f"},"cell_type":"code","execution_count":null,"outputs":[],"source":"#--- Test dataframe ---\nmsno.matrix(df_test[cols_missing_val_test],width_ratios=(10,1),\\\n            figsize=(20,8),color=(0.2,0.2,0.2),fontsize=18,sparkline=True,labels=True)"},{"metadata":{"_uuid":"a9dcf5864909adcf50bb576d23320990de9c4f08","_cell_guid":"ed6f6b70-e6fe-4f83-bf48-39d523f438f9"},"cell_type":"markdown","source":"We see a similar resemblance of proportional missing values in the train and test dataframes!"},{"metadata":{"_uuid":"4b0cf425a9bcf1920c69a1fb30c365077be7be1c","_cell_guid":"cc0a0691-f8bf-40b5-8eb8-dd9139bda988"},"cell_type":"markdown","source":"Replacing the missing values to -1."},{"metadata":{"_uuid":"9bd6e3b38aa061fc3ac25ba94baa25da3d17e064","collapsed":true,"_cell_guid":"ac0e94f3-19ac-4a4d-8958-62b83851d9ad"},"cell_type":"code","execution_count":null,"outputs":[],"source":"df_train.replace(np.nan, -1, inplace=True)\ndf_test.replace(np.nan, -1, inplace=True)"},{"metadata":{"_uuid":"ce7d79a652ffe0b377a3007151882f02bd10f61c","_cell_guid":"048e11ec-6269-46a6-96f4-1bd52a852b55"},"cell_type":"markdown","source":"## Memory Usage"},{"metadata":{"_uuid":"134235fdd1ee002ca52553e2ff00cd5c3bd025e7","collapsed":true,"_cell_guid":"c8370db6-90d8-43a5-9ff3-71c15514112a"},"cell_type":"code","execution_count":null,"outputs":[],"source":"#--- memory consumed by train dataframe ---\nmem = df_train.memory_usage(index=True).sum()\nprint(\"Memory consumed by training set  :   {} MB\" .format(mem/ 1024**2))\nprint('\\n')\n#--- memory consumed by test dataframe ---\nmem = df_test.memory_usage(index=True).sum()\nprint(\"Memory consumed by test set      :   {} MB\" .format(mem/ 1024**2))"},{"metadata":{"_uuid":"cd3a51aac56c31c45656e57cf6b7410521b4552e","_cell_guid":"4967ee31-9786-482e-8a5d-7cb971b77079"},"cell_type":"markdown","source":"By altering the datatypes we can reduce memory usage:"},{"metadata":{"_uuid":"d70b7771266e71347dd329398025bb2e0a579aa5","collapsed":true,"_cell_guid":"b0a4959f-971c-4a60-9254-f9439435a578"},"cell_type":"code","execution_count":null,"outputs":[],"source":"def change_datatype(df):\n    float_cols = list(df.select_dtypes(include=['int']).columns)\n    for col in float_cols:\n        if ((np.max(df[col]) <= 127) and(np.min(df[col] >= -128))):\n            df[col] = df[col].astype(np.int8)\n        elif ((np.max(df[col]) <= 32767) and(np.min(df[col] >= -32768))):\n            df[col] = df[col].astype(np.int16)\n        elif ((np.max(df[col]) <= 2147483647) and(np.min(df[col] >= -2147483648))):\n            df[col] = df[col].astype(np.int32)\n        else:\n            df[col] = df[col].astype(np.int64)\n\nchange_datatype(df_train)\nchange_datatype(df_test) "},{"metadata":{"_uuid":"76c8387f532ac92e00127203de2c7087b3e32bd8","collapsed":true,"_cell_guid":"27cd988f-fece-40bf-991a-8ee80fa573dd"},"cell_type":"code","execution_count":null,"outputs":[],"source":"#--- Converting columns from 'float64' to 'float32' ---\ndef change_datatype_float(df):\n    float_cols = list(df.select_dtypes(include=['float']).columns)\n    for col in float_cols:\n        df[col] = df[col].astype(np.float32)\n        \nchange_datatype_float(df_train)\nchange_datatype_float(df_test)"},{"metadata":{"_uuid":"b3fa512371b42483a315abcf7258c0a2f3bd5cec","_cell_guid":"47827cf2-3562-41d4-8483-9da10534a5be"},"cell_type":"markdown","source":"Let us check the memory consumed again:"},{"metadata":{"_uuid":"e582f29d035799ddc9d4013e544eef5db287180b","collapsed":true,"_cell_guid":"7d1b2ef8-459b-458e-b30c-a0ad6188bf91"},"cell_type":"code","execution_count":null,"outputs":[],"source":"#--- memory consumed by train dataframe ---\nmem = df_train.memory_usage(index=True).sum()\nprint(\"Memory consumed by training set  :   {} MB\" .format(mem/ 1024**2))\nprint('\\n') \n#--- memory consumed by test dataframe ---\nmem = df_test.memory_usage(index=True).sum()\nprint(\"Memory consumed by test set      :   {} MB\" .format(mem/ 1024**2))"},{"metadata":{"_uuid":"f12e8f6f7985dac071cb11c64b3b05f09852e020","_cell_guid":"8b41a6f2-dbc8-4d50-b8a3-4005aee7c669"},"cell_type":"markdown","source":"That is memory consumption reduced by **greater than 50%** !!!"},{"metadata":{"_uuid":"e5047aadbad6e54d4e91a8384b5e045e9ccb18a6","collapsed":true,"_cell_guid":"2d7a9ee5-9716-4b01-afe0-f5e363bbc243"},"cell_type":"code","execution_count":null,"outputs":[],"source":"print(len(df_test.columns))\nprint(len(df_train.columns))\n#print(len(target.columns))"},{"metadata":{"_uuid":"b78863353b1eb3c35fe409f47fd9ffc8109ac717","_cell_guid":"290da136-b20b-485f-8b87-baf3322d3b94"},"cell_type":"markdown","source":"# Quick Modeling (without any analysis)"},{"metadata":{"_uuid":"6ce0a0acd790d5157b21941ab9c97eb6a9600dac","_cell_guid":"43071b6e-0efd-40c3-ba55-ae03eccdd0f3"},"cell_type":"markdown","source":"Quick check to make sure the columns are the same in both `train` and `test` data."},{"metadata":{"_uuid":"f967aa56829f962ffe4f4b1cfab03f32cdf0ca59","collapsed":true,"_cell_guid":"a64cb294-6a9b-4f47-9c0e-424fc993876f"},"cell_type":"code","execution_count":null,"outputs":[],"source":"len(set(df_test.columns) and set(df_train.columns))"},{"metadata":{"_uuid":"3c098d2560e89e932e97fd28d333f975fef8576c","_cell_guid":"1323f62f-40f8-41c0-8971-2f786e7a9ba3"},"cell_type":"markdown","source":"## Random Forest"},{"metadata":{"_uuid":"fbc9436bf1521c4befe855c0114a2c9aa27e0b92","collapsed":true,"_cell_guid":"92e55304-c4ee-4bc5-9a96-9589f5073b20"},"cell_type":"code","execution_count":null,"outputs":[],"source":"df_train = df_train.replace(-1, np.NaN)\nd_median = df_train.median(axis=0)\nd_mean = df_train.mean(axis=0)\ndf_train = df_train.fillna(-1)\n\ndcol = [c for c in df_train.columns if c not in ['id','target']]\ndf_train['ps_car_13_x_ps_reg_03'] = df_train['ps_car_13'] * df_train['ps_reg_03']\n#df['negative_one_vals'] = np.sum((df[dcol]==-1).values, axis=1)\nfor c in dcol:\n        if '_bin' not in c: #standard arithmetic\n            df_train[c+str('_median_range')] = (df_train[c].values > d_median[c]).astype(np.int)\n            df_train[c+str('_mean_range')] = (df_train[c].values > d_mean[c]).astype(np.int)\n            df_train[c+str('_sq')] = np.power(df_train[c].values,2).astype(np.float32)\n            #df[c+str('_sqr')] = np.square(df[c].values).astype(np.float32)\n            df_train[c+str('_log')] = np.log(np.abs(df_train[c].values) + 1)\n            df_train[c+str('_exp')] = np.exp(df_train[c].values) - 1"},{"metadata":{"_uuid":"5f79b4464fd0c0511e2c404aefdc0c3d4de637e3","collapsed":true,"_cell_guid":"673b8eb0-f458-452c-b10e-f6c6908d79b7"},"cell_type":"code","execution_count":null,"outputs":[],"source":"change_datatype(df_train)"},{"metadata":{"_uuid":"dc9b8500679d498300a1d8a02f9b7a738cc61da2","collapsed":true,"_cell_guid":"9688c31d-5736-4385-967a-4f41c593f9c9"},"cell_type":"code","execution_count":null,"outputs":[],"source":"df_train.head()"},{"metadata":{"_uuid":"e1b79a01d9fe6b286bc49eaa18701c4602ed4243","collapsed":true,"_cell_guid":"1a8c54f6-f39a-4d05-b9b1-16869c537520"},"cell_type":"code","execution_count":null,"outputs":[],"source":"from sklearn.model_selection import train_test_split\n\nfeatures= [c for c in df_train.columns.values if c  not in ['id', 'target']]\n#numeric_features= [c for c in df.columns.values if c  not in ['id','text','author','processed']]\n#target = 'author'\n\nX_train, X_test, y_train, y_test = train_test_split(df_train[features], df_train['target'], test_size=0.33, random_state=42)\nX_train.head()"},{"metadata":{"_uuid":"046f73de0614a5d7a89a37da35d3793632f60c69","collapsed":true,"_cell_guid":"dccec6f7-f368-48b4-ab98-cc391144ae7d"},"cell_type":"code","execution_count":null,"outputs":[],"source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import GradientBoostingClassifier\n\npipeline = Pipeline([\n    #('features',feats),\n    ('classifier', RandomForestClassifier(random_state = 42))\n    #('classifier', GradientBoostingClassifier(random_state = 42))\n])\n\npipeline.fit(X_train, y_train)\n\npreds = pipeline.predict(X_test)\nnp.mean(preds == y_test)"},{"metadata":{"_uuid":"068ff9652fb90394185ebd74721ccde18bb75dc5","collapsed":true,"_cell_guid":"73d744a1-dac6-4699-9ba8-549eff4423b5"},"cell_type":"code","execution_count":null,"outputs":[],"source":"pipeline.get_params().keys()"},{"metadata":{"_uuid":"92c4599b77b8b55afa27acd820508e84e2385c95","collapsed":true,"_cell_guid":"505b63f0-a5b3-490f-8829-b852efbe713b"},"cell_type":"code","execution_count":null,"outputs":[],"source":"from sklearn.model_selection import GridSearchCV\n\nhyperparameters = { #'features__text__tfidf__max_df': [0.9, 0.95],\n                    #'features__text__tfidf__ngram_range': [(1,1), (1,2)],\n                    #'classifier__learning_rate': [0.1, 0.2],\n                    'classifier__n_estimators': [20, 30, 50],\n                    'classifier__max_depth': [2, 4],\n                    'classifier__min_samples_leaf': [2, 4]\n                  }\nclf = GridSearchCV(pipeline, hyperparameters, cv = 3)\n \n# Fit and tune model\nclf.fit(X_train, y_train)"},{"metadata":{"_uuid":"3cc439fdeb4e50983d7b619412907919d9684802","collapsed":true,"_cell_guid":"11da6112-db42-4620-a509-d5bfad61e5ef"},"cell_type":"code","execution_count":null,"outputs":[],"source":"clf.best_params_"},{"metadata":{"_uuid":"cea970f6c1d485d3ac3013f1dedf421d0c664ff5","collapsed":true,"_cell_guid":"40f0b8f2-67ee-464d-bf34-95ac8ec9a1a4"},"cell_type":"code","execution_count":null,"outputs":[],"source":"#refitting on entire training data using best settings\nclf.refit\n\npreds = clf.predict(X_test)\nprobs = clf.predict_proba(X_test)\n\nnp.mean(preds == y_test)"},{"metadata":{"_uuid":"9f20c96a6c76903201fefa42b20354a8de7e5cce","collapsed":true,"_cell_guid":"b8694bcf-1e55-44c0-a791-294cd80bb3c9"},"cell_type":"code","execution_count":null,"outputs":[],"source":"df_test = df_test.replace(-1, np.NaN)\ndt_median = df_test.median(axis=0)\ndt_mean = df_test.mean(axis=0)\ndf_test = df_test.fillna(-1)\n\ndtcol = [c for c in df_test.columns if c not in ['id']]\ndf_test['ps_car_13_x_ps_reg_03'] = df_test['ps_car_13'] * df_test['ps_reg_03']\n#df['negative_one_vals'] = np.sum((df[dcol]==-1).values, axis=1)\nfor c in dtcol:\n        if '_bin' not in c: #standard arithmetic\n            df_test[c+str('_median_range')] = (df_test[c].values > dt_median[c]).astype(np.int)\n            df_test[c+str('_mean_range')] = (df_test[c].values > dt_mean[c]).astype(np.int)\n            df_test[c+str('_sq')] = np.power(df_test[c].values,2).astype(np.float32)\n            #df[c+str('_sqr')] = np.square(df[c].values).astype(np.float32)\n            df_test[c+str('_log')] = np.log(np.abs(df_test[c].values) + 1)\n            df_test[c+str('_exp')] = np.exp(df_test[c].values) - 1"},{"metadata":{"_uuid":"a202783552b2b3e6b500e495218c1ff796146ca1","collapsed":true,"_cell_guid":"e576741c-93bc-4642-a309-319291a1e35f"},"cell_type":"code","execution_count":null,"outputs":[],"source":"change_datatype(df_test)"},{"metadata":{"_uuid":"81f129440860e66ad5cd500750e9dd99bdd4af1f","collapsed":true,"_cell_guid":"c33b2448-dd5f-4251-87ee-11481e98fc3e"},"cell_type":"code","execution_count":null,"outputs":[],"source":"submission = pd.read_csv('../input/test.csv')\n\n#preprocessing\n#test_features= [c for c in submission.columns.values if c  not in ['id']]\ntest_features= [c for c in df_test.columns.values if c  not in ['id']]\n#submission = processing(submission)\npredictions = clf.predict_proba(df_test[test_features])\n\npreds = pd.DataFrame(data = predictions, columns = clf.best_estimator_.named_steps['classifier'].classes_)\n\n#generating a submission file\nresult = pd.concat([submission[['id']], preds], axis=1)\nresult = result.drop(0, axis=1)\nresult.columns = ['id', 'target']\nresult.head()\n\nresult.to_csv('random_forest.csv', index=False)"},{"metadata":{"_uuid":"d4ccd84d1a26887fc9617c3f8ccb41d0b0db54ee","collapsed":true,"_cell_guid":"a4c01040-fd8f-44cc-9593-d4e4be004978"},"cell_type":"code","execution_count":null,"outputs":[],"source":""},{"metadata":{"_uuid":"a9f13317aa950bbef8ab9d214f8aae2a8eb26eca","collapsed":true,"_cell_guid":"5fa9e661-7932-42b6-bd79-0d02b80047ab"},"cell_type":"code","execution_count":null,"outputs":[],"source":"'''  \n\nfrom sklearn.cross_validation import train_test_split\nimport xgboost as xgb\n\nX_train = df_train.drop(['id'],axis = 1)\nX_id_train = df_train['id'].values\nY_train = target.values\n\nX_test = df_test.drop(['id'], axis=1)\nX_id_test = df_test['id'].values\n\nx_train, x_valid, y_train, y_valid = train_test_split(X_train, Y_train, test_size = 0.4, random_state = 1000)\nprint('Train samples: {} Validation samples: {}'.format(len(x_train), len(x_valid)))\n\nd_train = xgb.DMatrix(x_train, y_train)\nd_valid = xgb.DMatrix(x_valid, y_valid)\nd_test = xgb.DMatrix(X_test)\n\nparams = {}\nparams['min_child_weight'] = 10.0\nparams['objective'] = 'binary:logistic'\nparams['eta'] = 0.02\nparams['silent'] = True\nparams['max_depth'] = 9\nparams['subsample'] = 0.9\nparams['colsample_bytree'] = 0.9\n\n# Define the gini metric - from https://www.kaggle.com/c/ClaimPredictionChallenge/discussion/703#5897\ndef gini(actual, pred, cmpcol = 0, sortcol = 1):\n    assert( len(actual) == len(pred) )\n    all = np.asarray(np.c_[ actual, pred, np.arange(len(actual)) ], dtype=np.float)\n    all = all[ np.lexsort((all[:,2], -1*all[:,1])) ]\n    totalLosses = all[:,0].sum()\n    giniSum = all[:,0].cumsum().sum() / totalLosses\n    \n    giniSum -= (len(actual) + 1) / 2.\n    return giniSum / len(actual)\n \ndef gini_normalized(a, p):\n    return gini(a, p) / gini(a, a)\n\n# Create an XGBoost-compatible metric from Gini\n\ndef gini_xgb(preds, dtrain):\n    labels = dtrain.get_label()\n    gini_score = gini_normalized(labels, preds)\n    return [('gini', gini_score)]\n\nwatchlist = [(d_train, 'train'), (d_valid, 'valid')]\n\nmodel = xgb.train(params, d_train, 100, watchlist, early_stopping_rounds=100, feval=gini_xgb, maximize=True, verbose_eval=10)\n\nxgb.plot_importance(model)\nfig, ax = plt.subplots(figsize=(12,18))\nplt.show()\n\np_test = model.predict(d_test)\n\n#--- Submission file ---\n\nsub = pd.DataFrame()\nsub['id'] = X_id_test\nsub['target'] = p_test\nsub.to_csv('xgb.csv', index=False)\n\n\nimportance = model.get_fscore(fmap='xgb.fmap')\nimportance = sorted(importance.items(), key=operator.itemgetter(1))\n\ndf = pd.DataFrame(importance, columns=['feature', 'fscore'])\n\nplt.figure()\ndf.plot()\ndf.plot(kind='barh', x='feature', y='fscore', legend=False, figsize=(10, 25))\nplt.gcf().savefig('features_importance.png')\n\n''' "},{"metadata":{"_uuid":"e2455b1cf9c1d913d528a60439b382d5dd8c8911","_cell_guid":"a024635e-676d-4839-a158-2825806cf2ed"},"cell_type":"markdown","source":"# Data Analysis\n## Splitting columns based on types\nAccording to the data given to us:\n* features that belong to similar groupings are tagged as such in the feature names (e.g., **ind**, **reg**, **car**, **calc**). \n* feature names include the postfix **bin** to indicate binary features and **cat** to indicate categorical features.\n* feature names without **boon** or **cat** are grouped as** continuous/ordinal** features."},{"metadata":{"_uuid":"5471b6d52e3f61b5d3993ef2e7cd29f52feffac4","collapsed":true,"_cell_guid":"8b1478e4-fb5f-4b3f-9dbc-5d74b4d53b83"},"cell_type":"code","execution_count":null,"outputs":[],"source":"#-- List of all columns --\ntrain_cols = df_train.columns.tolist()\n\n#--- binary and categorical features list ---\nbin_cols = []\ncat_cols = []\n\n#--- continous/ordinal features list ---\ncont_ord_cols = []\n\n#--- different feature groupings ---\nind_cols = []\nreg_cols = []\ncar_cols = []\ncalc_cols = []\n\nfor col in train_cols:\n    if (('ps' in str(col)) & ('bin' not in str(col)) & ('cat' not in str(col))):\n        cont_ord_cols.append(col)\n    \nfor col in train_cols:\n    if ('bin' in str(col)):\n        bin_cols.append(col)\n    if ('cat' in str(col)):\n        cat_cols.append(col)\n        \n    if ('ind' in str(col)):\n        ind_cols.append(col)\n    if ('reg' in str(col)):\n        reg_cols.append(col)\n    if ('car' in str(col)):\n        car_cols.append(col)\n    if ('calc' in str(col)):\n        calc_cols.append(col)\n        "},{"metadata":{"_uuid":"44b50c2bc028d949e32ab10ff8487b788a1f2112","_cell_guid":"a9d338e0-1bce-4b2f-9f45-daf90040a750"},"cell_type":"markdown","source":"Columns present in `cont_ord_cols` list have a collection of different types of columns.\n\nSo we can divide them into **continuous** and **ordinal** variables based on their data types."},{"metadata":{"_uuid":"1b9189777a8926e08c5a4b1e2363a8bf419f85b5","collapsed":true,"_cell_guid":"7df83856-8d3b-4751-88db-75e12977e4e1"},"cell_type":"code","execution_count":null,"outputs":[],"source":"float_cols = []\nint_cols = []\nfor col in cont_ord_cols:\n    if (df_train[col].dtype == np.float32):\n          float_cols.append(col)        #--- continuous variables ---\n    elif ((df_train[col].dtype == np.int8) or (df_train[col].dtype == np.int16)):\n          int_cols.append(col)          #--- ordinal variables ---"},{"metadata":{"_uuid":"85ca0dca1cb03f41508d7c56ab88d51ca0a1841a","_cell_guid":"2b532554-01cc-4268-9309-e35111ca1547"},"cell_type":"markdown","source":"The following snippet is confirmation that all the variables are **ordinal** beacuse they have more than 2 unique values."},{"metadata":{"_uuid":"8e0bc790307b1d9a6bc46e802989860accd8238e","collapsed":true,"_cell_guid":"126c4688-828a-4b65-bd2f-60549f644496"},"cell_type":"code","execution_count":null,"outputs":[],"source":"for col in int_cols:\n    print (df_train[col].nunique())"},{"metadata":{"_uuid":"c04f78ed6c918b916a6948ffb626a14ec55c79af","_cell_guid":"c92587ce-4620-4ab9-9100-934c85e53995"},"cell_type":"markdown","source":"Exploring each of the above extracted grouped features individually:\n\n## Binary features:\n\nBinary features whose single attribute is less than 10% will be collected in a separate list"},{"metadata":{"_uuid":"5d4e1db46403c770c0a1284943de8222f5590675","collapsed":true,"_cell_guid":"bb1849ff-286e-4845-b5c8-253c43c475b0"},"cell_type":"code","execution_count":null,"outputs":[],"source":"cols_to_delete = []\nth = 0.1\nfor col in range(0, len(bin_cols)):\n    print (bin_cols[col])\n    print (df_train[bin_cols[col]].unique())\n    pp = pd.value_counts(df_train[bin_cols[col]])\n    \n    for i in range(0, len(pp)):\n        if((pp[i]/float(len(df_train))) <= th):\n            cols_to_delete.append(bin_cols[col])\n            \n    pp.plot.bar()\n    plt.show()"},{"metadata":{"_uuid":"f282ef3d4610f1588bf79ee23e824912e9db5c03","collapsed":true,"_cell_guid":"2d63b04d-1bd7-479d-8916-cd5144714cd1"},"cell_type":"code","execution_count":null,"outputs":[],"source":"print(cols_to_delete)"},{"metadata":{"_uuid":"4291edbc13125f8190834df48d677b453cc1b676","_cell_guid":"f88be7c8-5d9d-4c30-9f3e-db5a10847eac"},"cell_type":"markdown","source":"The above mentioned columns have highly skewed values hence can be dropped from both the training and test set."},{"metadata":{"_uuid":"0b11ed516b4eaffc30a4dc5f7cc7dd43deab97b9","collapsed":true,"_cell_guid":"0d932784-4b72-47e9-9251-8fc7794a6b9c"},"cell_type":"code","execution_count":null,"outputs":[],"source":" \nfor col in cols_to_delete:\n    df_train.drop([col], axis=1, inplace=True)\n    df_test.drop([col], axis=1, inplace=True)\n    "},{"metadata":{"_uuid":"cc92dee9e0e1fbbb08102bf9a057f00b8b8be524","_cell_guid":"d0d2bad2-73c4-40ea-b034-b7b6e22a0a58"},"cell_type":"markdown","source":"## Categorical Features\n\nExploring the categorical variables:"},{"metadata":{"_uuid":"27de8d32ef546283cd6617b593b2efc55f25848d","collapsed":true,"_cell_guid":"d8af278e-e134-4fc1-ae26-4984136b1726"},"cell_type":"code","execution_count":null,"outputs":[],"source":"for col in range(0, len(cat_cols)):\n    print (cat_cols[col])\n    print (df_train[cat_cols[col]].unique())\n    pp = pd.value_counts(df_train[cat_cols[col]])      \n    pp.plot.bar()\n    plt.show()"},{"metadata":{"_uuid":"d8032acbd75ac68a1c466d40da66236349916d73","_cell_guid":"9888ab86-5b9d-444f-a542-2451f5186514"},"cell_type":"markdown","source":"From the graphs, only **ps_car_10_cat** is highly skewed hence can be removed from training and test set."},{"metadata":{"_uuid":"09cf720d0c5c72fd5294e45a947867d0990389f5","collapsed":true,"_cell_guid":"feca851c-421f-46ce-8f85-ae744ec0e973"},"cell_type":"code","execution_count":null,"outputs":[],"source":"'''\n\ncat_cols_to_delete = [ 'ps_car_10_cat']\n\nfor col in cat_cols_to_delete:\n    df_train.drop([col], axis=1, inplace=True)\n    df_test.drop([col], axis=1, inplace=True) \n\n''' "},{"metadata":{"_uuid":"326d49d900f6c5060506560b0ee21ef9499aab6d","_cell_guid":"ed41cfc2-21a3-4129-9e48-af692ebc923c"},"cell_type":"markdown","source":"## Continuous/Ordinal Features\n\nFeatures having different prefixes such as **ind**, **reg**, **car** and **calc**; excluding binary and categorical features."},{"metadata":{"_uuid":"8966a72e5c1f20546e4a192730dd4bf76e48e04d","collapsed":true,"_cell_guid":"e6875b9a-9655-4150-b266-a15f8aa8ded7"},"cell_type":"code","execution_count":null,"outputs":[],"source":"ind_cols_no_bin_cat = []\nreg_cols_no_bin_cat = []\ncar_cols_no_bin_cat = []\ncalc_cols_no_bin_cat = []\n\nfor col in train_cols:\n    if (('ind' in str(col)) and ('bin' not in str(col)) and ('cat' not in str(col))):\n        ind_cols_no_bin_cat.append(col)\n    if (('reg' in str(col)) and ('bin' not in str(col)) and ('cat' not in str(col))):\n        reg_cols_no_bin_cat.append(col)\n    if (('car' in str(col)) and ('bin' not in str(col)) and ('cat' not in str(col))):\n        car_cols_no_bin_cat.append(col)\n    if (('calc' in str(col)) and ('bin' not in str(col)) and ('cat' not in str(col))):\n        calc_cols_no_bin_cat.append(col)"},{"metadata":{"_uuid":"4c56748d03e33aed945dada1eddec2a9c3de5cec","_cell_guid":"e8619816-c4f0-4b95-8bf5-0d69d3867ae8"},"cell_type":"markdown","source":"### Visualizing **ind** features\n\n(Uncomment the following snippets of code to visualzie the various grouped features. They take a long time to load hence I have commented them out)"},{"metadata":{"_uuid":"bee57f45702121a55678c96050b4e6f314595c72","collapsed":true,"_cell_guid":"cf945bf3-5067-4d3d-9f5f-247b635fed20"},"cell_type":"code","execution_count":null,"outputs":[],"source":"'''\nwhat_col = ind_cols_no_bin_cat\nfor col in range(0, len(what_col)):\n    print (what_col[col])\n    print (df_train[what_col[col]].unique())\n    pp = pd.value_counts(df_train[what_col[col]])      \n    pp.plot.bar()\n    plt.show()\n'''   "},{"metadata":{"_uuid":"dc33ccbd3642e1e7787e5a4395c213c5d3adc184","_cell_guid":"7b6e9d2a-03e8-4ed7-8e10-42371ac7b6d3"},"cell_type":"markdown","source":"Column **ps_ind_14** is heavily skewed hence can be removed."},{"metadata":{"_uuid":"fd0babb91e2f2ee0d6f499f8b72c907e821491d5","_cell_guid":"5cebbef0-19c1-4ee6-90cc-e00a912772cb"},"cell_type":"markdown","source":"### Visualizing **reg** features"},{"metadata":{"_uuid":"c9beb6a44fda8863d90ae9c043f1e496ccded8b9","collapsed":true,"_cell_guid":"e984ce2d-8f23-4f42-95fa-bebfb463bad1"},"cell_type":"code","execution_count":null,"outputs":[],"source":"'''\nwhat_col = reg_cols_no_bin_cat\nfor col in range(0, len(what_col)):\n    print (what_col[col])\n    print (df_train[what_col[col]].unique())\n    pp = pd.value_counts(df_train[what_col[col]])      \n    pp.plot.bar()\n    plt.show()\n '''  "},{"metadata":{"_uuid":"82f32b22717ef34c74c2b397c75bc15b37a5c6c1","_cell_guid":"990841c0-bb9d-405c-930a-bd8665399536"},"cell_type":"markdown","source":"Column **ps_reg_03** does not seem to show anything at all, hence can be removed.\n\n### Visualizing **car** features"},{"metadata":{"_uuid":"cd0057cc7442aea8a7f4d2a936f92210b0b7a2c2","collapsed":true,"_cell_guid":"5066a831-8953-480d-96e5-59f4c1585e5e"},"cell_type":"code","execution_count":null,"outputs":[],"source":"''' \nwhat_col = car_cols_no_bin_cat\nfor col in range(0, len(what_col)):\n    print (what_col[col])\n    print (df_train[what_col[col]].unique())\n    pp = pd.value_counts(df_train[what_col[col]])      \n    pp.plot.bar()\n    plt.show()\n'''"},{"metadata":{"_uuid":"386ee75ec13f28a8c37173311c2b88b1ce86ab86","_cell_guid":"b29d6c0d-0cb6-4000-bc90-e221155d586b"},"cell_type":"markdown","source":"### Visualizing **calc** features"},{"metadata":{"_uuid":"d7ec732d37b939391fc50c17a1c17ea7c015ae48","collapsed":true,"_cell_guid":"4a1b4306-f51c-48b3-af9d-608ca954d13b"},"cell_type":"code","execution_count":null,"outputs":[],"source":"''' \nwhat_col = calc_cols_no_bin_cat\nfor col in range(0, len(what_col)):\n    print (what_col[col])\n    print (df_train[what_col[col]].unique())\n    pp = pd.value_counts(df_train[what_col[col]])      \n    pp.plot.bar()\n    plt.show()\n'''"},{"metadata":{"_uuid":"f0c604138d2b59cc5ed77fe655001a0fee9e7d3a","_cell_guid":"b11cbbee-d910-4a36-9729-518f9e267188"},"cell_type":"markdown","source":"Colukmns belonging to type ***calc***: \n* **ps_calc_01**,\n* **ps_calc_02**,\n* **ps_calc_03** \n\nhave a uniform distribution, which do not offer anything significant. Hence these can also be removed."},{"metadata":{"_uuid":"31052acea6cb8b059135ce79a3a47f10ee3d281d","collapsed":true,"_cell_guid":"50bca12a-ab93-49ae-b8ee-65c366301889"},"cell_type":"code","execution_count":null,"outputs":[],"source":"''' other_cols_to_delete = ['ps_ind_14', 'ps_calc_01', 'ps_calc_02', 'ps_calc_03', 'ps_reg_03']\n\nfor col in other_cols_to_delete:\n    df_train.drop([col], axis=1, inplace=True)\n    df_test.drop([col], axis=1, inplace=True)''' "},{"metadata":{"_uuid":"4ee560528f553939434ab93d40b2ea6ae44da9b2","_cell_guid":"4c5c3c6f-ea43-4f10-a7db-664d7ac81de7"},"cell_type":"markdown","source":"# Feature Engineering\n\n### NOTE: ALWAYS REMEMBER TO INCLUDE SAME SET OF FEATURES FOR THE TEST DATA ALSO!!"},{"metadata":{"_uuid":"ee6e40e819cc441af705c12afc9d84b7d99e8e36","_cell_guid":"0d998f90-860d-4770-b0dc-b4ecaf0cd7bc"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"b64b317fbb20fb7f815d6a535594aeeb8356abf5","collapsed":true,"_cell_guid":"89d139b0-fa35-4a6f-872f-3b63f631bcfc"},"cell_type":"code","execution_count":null,"outputs":[],"source":"'''        \nfor col1 in int_cols:\n    for col2 in float_cols:\n        l_mean = \n        df_train[col1 + '_' + col2] = \n''' "},{"metadata":{"_uuid":"13d0eb1346eec09166c4b3c5ca34c35df14a3427","_cell_guid":"c3041fa6-d91d-43be-93a8-f0e4ec070cf5"},"cell_type":"markdown","source":"## New Binary Features\n\nHere I have included logical AND, OR and XOR operation between every binary feature."},{"metadata":{"_uuid":"d0f0057b65680c1e005952ef19f9e48b6be79d50","collapsed":true,"_cell_guid":"322abe86-6610-4232-bd98-5b71a66c712a"},"cell_type":"code","execution_count":null,"outputs":[],"source":"train_cols = df_train.columns\nbin_cols = df_train.columns[df_train.columns.str.endswith('bin')]\n''' \nfor i in [\"X1\",\"X2\"]:\n    for j in [\"X2\",\"X3\"]:\n        if i != j:\n            col_name = i + j\n            k[col_name + '_OR'] = k[i]|k[j] \n            k[col_name + '_AND'] = k[i]&k[j] \n            k[col_name + '_XOR'] = k[i]^k[j] \n           \ndef second_order(df, c_names):\n    names_col=[]\n    pp=0\n    for i in c_names[:c_names.size-1]:\n        for j in c_names[pp:c_names.size]:\n            if i != j:\n                col_name = i + str('_') + j\n                df[col_name + '_OR'] = df[i]|df[j] \n                df[col_name + '_AND'] = df[i]&df[j] \n                df[col_name + '_XOR'] = df[i]^df[j]\n            \n                #col_name = ii + str('_and_') + jj\n                #names_col.append(col_name)\n                #df[col_name] = df[ii]&df[jj]\n        pp+=1\n    return df, names_col   \n\ndf_train, train_new_cols = second_order(df_train, bin_cols)\ndf_test, test_new_cols = second_order(df_test, bin_cols)\n\nprint(len(df_train.columns))\nprint(len(df_test.columns))\n'''"},{"metadata":{"_uuid":"e1b1bd0e3a6e830eb260963fa5daaeb59aa223d7","_cell_guid":"29751663-4e79-45fa-83f5-8743f4d54769"},"cell_type":"markdown","source":"## New Continuous/Ordinal Features (*in progress*)"},{"metadata":{"_uuid":"01896e3d531b4d83ee684c25cb9fe421e863f61b","collapsed":true,"_cell_guid":"c1f044c7-0f66-4b08-bae0-3109fca0ec5f"},"cell_type":"code","execution_count":null,"outputs":[],"source":"''' \nprint(len(df_train.columns))\n#new_cont_ord_cols = [c for c in df_train.columns if not c.startswith('ps_calc_')]\n#new_cont_ord_cols = [c for c in df_train.columns if not c.endswith('bin') ]\nfor col in no_bin_cat_cols:\n    #df_train[col + str('_greater_median')] = (df_train[col].values > df_train[col].median()).astype(np.int)\n    #df_train[col + str('_greater_mean')] = (df_train[col].values > df_train[col].mean()).astype(np.int)\n    df_train[col + str('_sq')] = np.power(df_train[col].values,2).astype(np.float32)\n    df_train[col + str('_sqr')] = np.square(df_train[col].values).astype(np.float32)\n    df_train[col + str('_log')] = np.log(np.abs(df_train[col].values) + 1)\n    #df_train[col + str('_exp')] = np.exp(df_train[col].values) - 1\n    \n#new_cont_ord_test_cols = [c for c in df_test.columns if not c.startswith('ps_calc_')]\nfor col in no_bin_cat_cols:\n    #df_test[col + str('_greater_median')] = (df_test[col].values > df_test[col].median()).astype(np.int)\n    #df_test[col + str('_greater_mean')] = (df_test[col].values > df_test[col].mean()).astype(np.int)\n    df_test[col + str('_sq')] = np.power(df_test[col].values,2).astype(np.float32)\n    df_test[col + str('_sqr')] = np.square(df_test[col].values).astype(np.float32)\n    df_test[col + str('_log')] = np.log(np.abs(df_test[col].values) + 1)\n    #df_test[col + str('_exp')] = np.exp(df_test[col].values) - 1    \n'''    "},{"metadata":{"_uuid":"d4dddb7c90ae67700b519967849aa775cb1c45c7","_cell_guid":"be43ab4a-82d2-4964-9850-839e000a3835"},"cell_type":"markdown","source":"## New Second Order Continuous/Ordinal Features (*based on Gradient Boosting feature importance*)"},{"metadata":{"_uuid":"91e7adb833bcdc4e4d7d4783b24d41f23694e273","collapsed":true,"_cell_guid":"61519a29-46de-40e7-bc7c-a75a7cd0ef5b"},"cell_type":"code","execution_count":null,"outputs":[],"source":"'''\nnew_col =['ps_car_12', 'ps_car_14', 'ps_car_15', 'ps_car_13', 'ps_reg_03', 'ps_ind_03', 'ps_ind_15', 'ps_reg_02', 'ps_reg_01', 'ps_calc_02', 'ps_calc_11', 'ps_calc_10']\n\n\ndef new_second_order(df, c_names):\n    names_col=[]\n    pp=0\n    for i in c_names[:len(c_names)-1]:\n        for j in c_names[pp:len(c_names)]:\n            if i != j:\n                col_name = i + str('_*_') + j\n                df[col_name] = df[i] * df[j] \n                \n            \n                #col_name = ii + str('_and_') + jj\n                #names_col.append(col_name)\n                #df[col_name] = df[ii]&df[jj]\n        pp+=1\n    return df, names_col   \n\ndf_train, train_new_cols = new_second_order(df_train, new_col)\ndf_test, test_new_cols = new_second_order(df_test, new_col)\n'''"},{"metadata":{"_uuid":"62a780379f95b546b53107d62ba5dc8e3a8c4ce6","collapsed":true,"_cell_guid":"64b6d144-7b7e-4747-ac5d-f60361cb7c8e"},"cell_type":"code","execution_count":null,"outputs":[],"source":"print(len(df_train.columns))\nprint(len(df_test.columns))"},{"metadata":{"_uuid":"6b80e22d2e3439f0cb7f936974e0856b0a3fb8ae","_cell_guid":"cef918e0-e684-4c0b-8773-e539e6388bb8"},"cell_type":"markdown","source":"## Correlation"},{"metadata":{"_uuid":"d0e8de9e8644d237108d575c4116f451faaef9e9","collapsed":true,"_cell_guid":"f5acedb2-b89f-459e-b6e5-d42622a11ea3"},"cell_type":"code","execution_count":null,"outputs":[],"source":"''' \nsns.set(style=\"white\")\ncorr = df_train.corr()\nf, ax = plt.subplots(figsize=(18, 15))\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(corr, cmap=cmap, vmax=.3, center=0,square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.show()\n'''"},{"metadata":{"_uuid":"1df399ebfd1d03ebd73840e43205602eae81e671","_cell_guid":"628abcae-4489-4b78-82fc-8db61e391988"},"cell_type":"markdown","source":"Outrageous! Not even a single **calc** feature seems to have any interest in indulging themselves with anything!! It is better to remove them all!!"},{"metadata":{"_uuid":"cc73aea7b54c691c2d1515b3ec073d2c907dbe7d","collapsed":true,"_cell_guid":"6504b8ea-ae88-4280-b82b-fd5ae7c8526b"},"cell_type":"code","execution_count":null,"outputs":[],"source":"'''\nremoved_calc_cols = []\nfor col in df_train.columns:\n    if ('calc' in str(col)):\n        removed_calc_cols.append(col)\n    \n#unwanted = train.columns[train.columns.str.startswith('ps_calc_')]\n\ndf_train = df_train.drop(removed_calc_cols, axis=1)  \ndf_test = df_test.drop(removed_calc_cols, axis=1)  \n''' "},{"metadata":{"_uuid":"fe00c4dd1f497fd168420852e51619fcbbbd4de0","collapsed":true,"_cell_guid":"6eff7b53-c5cd-4fb4-8ef8-e2b6a77df691"},"cell_type":"code","execution_count":null,"outputs":[],"source":"df_train.replace(np.nan, -1, inplace=True)\ndf_test.replace(np.nan, -1, inplace=True)\nprint('Done')"},{"metadata":{"_uuid":"73984fe2974a0cd9d2c06b869f47c430815d3dcc","collapsed":true,"_cell_guid":"012682c6-487b-42d3-9856-e4f30a187f2d"},"cell_type":"markdown","source":"# Modeling\n## Gradient Boosting"},{"metadata":{"_uuid":"20036b57ad7317470fccadf3d25e6014cdacd7d5","collapsed":true,"_cell_guid":"d9a8ece6-5ebf-4850-a55f-1f1310d21d39"},"cell_type":"code","execution_count":null,"outputs":[],"source":"''' \nX_train = df_train.drop(['id'],axis = 1)\nX_id_train = df_train['id'].values\nY_train = target.values\n\nX_test = df_test.drop(['id'], axis=1)\nX_id_test = df_test['id'].values\n'''"},{"metadata":{"_uuid":"ca8bca20047bdc809301fae8ba4d0b79833e72bb","collapsed":true,"_cell_guid":"43fd4d84-f0fb-4101-81d1-a276a1ecfbf0"},"cell_type":"code","execution_count":null,"outputs":[],"source":"''' \nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nGBR = GradientBoostingRegressor(n_estimators = 100, learning_rate = 0.02, max_depth=7, random_state = 0, loss='ls')\n#GBR = GradientBoostingClassifier(learning_rate = 0.02, n_estimators = 500, max_depth = 9, min_samples_split = 2, min_samples_leaf = 2, max_features = 10, random_state=123)\n    \nGBR.fit(X_train, Y_train)\n\nprint (GBR)\n'''"},{"metadata":{"_uuid":"43a5a94d7d93155ac95b686e3f01c0036db28da3","collapsed":true,"_cell_guid":"34181575-bbc1-4580-8557-1e70dba998eb"},"cell_type":"code","execution_count":null,"outputs":[],"source":"#--- List of important features for Gradient Boosting Regressor ---\n''' \nfeatures_list = X_train.columns.values\nfeature_importance = GBR.feature_importances_\nsorted_idx = np.argsort(feature_importance)\n\nprint(sorted_idx)\n''' "},{"metadata":{"_uuid":"35af0b205e76fe833376e23c89a51817e1218208","collapsed":true,"_cell_guid":"2e725180-9b13-4538-b301-038d9703302a"},"cell_type":"code","execution_count":null,"outputs":[],"source":"''' \nplt.figure(figsize=(15, 15))\nplt.barh(range(len(sorted_idx)), feature_importance[sorted_idx], align='center')\nplt.yticks(range(len(sorted_idx)), features_list[sorted_idx])\nplt.xlabel('Importance')\nplt.title('Feature importances')\nplt.draw()\nplt.show()\n''' "},{"metadata":{"_uuid":"84757621c0bcca610e842b06e78d495d1c8dcb06","collapsed":true,"_cell_guid":"de77d964-9b10-4acc-a7dc-399644cdfd22"},"cell_type":"code","execution_count":null,"outputs":[],"source":"#--- Predicting Gradient boost result for test data ---\n# y_GBR = GBR.predict(X_test)"},{"metadata":{"_uuid":"02d6f0ccc104c0e87ca3c06d35c6b1abe9c9d1eb","collapsed":true,"_cell_guid":"708f6a6f-fd53-42c1-bf2d-763c0701ebc4"},"cell_type":"code","execution_count":null,"outputs":[],"source":"''' \nfinal = pd.DataFrame()\nfinal['id'] = X_id_test\nfinal['target'] = y_GBR\nfinal.to_csv('Gradient_Boost_1.csv', index=False)\nprint('DONE!!')\n'''"},{"metadata":{"_uuid":"263f3bc5c3edd453f54907e68df2fef624b7d737","_cell_guid":"d5c41bc7-9563-4c62-b52f-bbe5f468d02e"},"cell_type":"markdown","source":"## XGBoost"},{"metadata":{"_uuid":"64e860b8d7937ce309b5ad31a00f69a888bb9fc6","collapsed":true,"_cell_guid":"e3a23733-74c0-4e13-b7c0-43d38afc2383"},"cell_type":"code","execution_count":null,"outputs":[],"source":"import xgboost as xgb"},{"metadata":{"_uuid":"b6893a21f2ff117a4bbfb9bdb9597161d8e8bf18","collapsed":true,"_cell_guid":"08582fdb-1c70-42a4-993f-ab2593b3eea7"},"cell_type":"code","execution_count":null,"outputs":[],"source":"def gini(actual, pred, cmpcol = 0, sortcol = 1):\n    assert( len(actual) == len(pred) )\n    all = np.asarray(np.c_[ actual, pred, np.arange(len(actual)) ], dtype=np.float)\n    all = all[ np.lexsort((all[:,2], -1*all[:,1])) ]\n    totalLosses = all[:,0].sum()\n    giniSum = all[:,0].cumsum().sum() / totalLosses\n    \n    giniSum -= (len(actual) + 1) / 2.\n    return giniSum / len(actual)\n \ndef gini_normalized(a, p):\n    return gini(a, p) / gini(a, a)\n\ndef gini_xgb(preds, dtrain):\n    labels = dtrain.get_label()\n    gini_score = gini_normalized(labels, preds)\n    return 'gini', gini_score"},{"metadata":{"_uuid":"e5d0b7d00a3c6d3c2983ad50c29a695331b857d0","collapsed":true,"_cell_guid":"ec199e9d-9ed7-4a92-ac4b-3aed87c56263"},"cell_type":"code","execution_count":null,"outputs":[],"source":"'''\nfrom sklearn.model_selection import StratifiedKFold\n\nkfold = 5\nskf = StratifiedKFold(n_splits=kfold, random_state=42)\n'''"},{"metadata":{"_uuid":"ac3e195d0bd032268ff8640a3b5d7768137a5669","collapsed":true,"_cell_guid":"f28897d3-e8f4-4528-934f-36a516fceb08"},"cell_type":"code","execution_count":null,"outputs":[],"source":"params = {\n    'min_child_weight': 10.0,\n    'objective': 'binary:logistic',\n    'max_depth': 7,\n    'max_delta_step': 1.8,\n    'colsample_bytree': 0.4,\n    'subsample': 0.8,\n    'eta': 0.025,\n    'gamma': 0.65,\n    'num_boost_round' : 700\n    }"},{"metadata":{"_uuid":"95a1e30e3226746c093ad0913dee2e4ed6c4d233","collapsed":true,"_cell_guid":"93073909-596a-4832-a8e3-5fd6d2b38423"},"cell_type":"code","execution_count":null,"outputs":[],"source":"'''\nfor i, (train_index, test_index) in enumerate(skf.split(X_train, Y_train)):\n    print('[Fold %d/%d]' % (i + 1, kfold))\n    X_train, X_valid = X_train[train_index], X_train[test_index]\n    y_train, y_valid = Y_train[train_index], Y_train[test_index]\n    # Convert our data into XGBoost format\n    d_train = xgb.DMatrix(X_train, y_train)\n    d_valid = xgb.DMatrix(X_valid, y_valid)\n    d_test = xgb.DMatrix(X_test.values)\n    watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n\n    # Train the model! We pass in a max of 1,600 rounds (with early stopping after 70)\n    # and the custom metric (maximize=True tells xgb that higher metric is better)\n    mdl = xgb.train(params, d_train, 1600, watchlist, early_stopping_rounds=70, feval=gini_xgb, maximize=True, verbose_eval=100)\n\n    print('[Fold %d/%d Prediciton:]' % (i + 1, kfold))\n    # Predict on our test data\n    p_test = mdl.predict(d_test)\n    sub['target'] += p_test/kfold\n'''    "},{"metadata":{"_uuid":"edc4580ae21c42cf117b4d10849dd201ec00eacc","_cell_guid":"011b7ea5-4093-48f1-8354-05e29c3e139c"},"cell_type":"markdown","source":"## Random Forest"},{"metadata":{"_uuid":"d806c29880e0c4a925b52d5b793b2486cdf75b70","collapsed":true,"_cell_guid":"1764612d-ac0c-4193-b656-d3f10cd02c03"},"cell_type":"code","execution_count":null,"outputs":[],"source":"''' \nfrom sklearn.ensemble import RandomForestClassifier  \n\nRF = RandomForestClassifier(n_estimators=100, max_depth=8, criterion='entropy', min_samples_split=10, max_features=120, n_jobs=-1, random_state=123, verbose=1, class_weight = \"balanced\")\nRF.fit(X_train, Y_train)\n\nprint(RF)\n\n#--- List of important features ---\n\nfeatures_list = X_train.columns.values\nfeature_importance = RF.feature_importances_\nsorted_idx = np.argsort(feature_importance)\n\nprint(sorted_idx)\n\nplt.figure(figsize=(15, 15))\nplt.barh(range(len(sorted_idx)), feature_importance[sorted_idx], align='center')\nplt.yticks(range(len(sorted_idx)), features_list[sorted_idx])\nplt.xlabel('Importance')\nplt.title('Feature importances')\nplt.draw()\nplt.show()\n\n \nY_pred = RF.predict(X_test)\n\nfinal = pd.DataFrame()\nfinal['id'] = X_id_test\nfinal['target'] = Y_pred\nfinal.to_csv('RF.csv', index=False)\nprint('DONE!!')\n\n'''"},{"metadata":{"_uuid":"6fa07c28d97f5e5318f741aaf62264ef86d19b53","collapsed":true,"_cell_guid":"f5c7f77d-a103-46bd-a882-eba967a19f8c"},"cell_type":"code","execution_count":null,"outputs":[],"source":"#-- Adaboost ---\n'''\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\n\nAda_R = AdaBoostRegressor(DecisionTreeRegressor(max_depth=7), n_estimators = 400, random_state = 99)\n\nAda_R.fit(X_train, Y_train)\n\nprint (Ada_R)\n\nfeatures_list = X_train.columns.values\nfeature_importance = Ada_R.feature_importances_\nsorted_idx = np.argsort(feature_importance)\n\nprint(sorted_idx)\n\nplt.figure(figsize=(15, 15))\nplt.barh(range(len(sorted_idx)), feature_importance[sorted_idx], align='center')\nplt.yticks(range(len(sorted_idx)), features_list[sorted_idx])\nplt.xlabel('Importance')\nplt.title('Feature importances')\nplt.draw()\nplt.show()\n\n#--- Predicting Ada boost result for test data ---\ny_Ada = Ada_R.predict(X_test)\n\n\nfinal = pd.DataFrame()\nfinal['id'] = X_id_test\nfinal['target'] = y_Ada\nfinal.to_csv('Ada_Boost_1.csv', index=False)\nprint('DONE!!')\n''' "},{"metadata":{"_uuid":"d17ebbfa606c7db59e9562373b78b04cbd0ee35a","collapsed":true,"_cell_guid":"7ea31b75-c6c1-4854-be44-6a497e020341"},"cell_type":"code","execution_count":null,"outputs":[],"source":"'''  \n\nfrom sklearn.cross_validation import train_test_split\nimport xgboost as xgb\n\nX_train = df_train.drop(['id'],axis = 1)\nX_id_train = df_train['id'].values\nY_train = target.values\n\nX_test = df_test.drop(['id'], axis=1)\nX_id_test = df_test['id'].values\n\nx_train, x_valid, y_train, y_valid = train_test_split(X_train, Y_train, test_size=0.2, random_state=4242)\nprint('Train samples: {} Validation samples: {}'.format(len(x_train), len(x_valid)))\n\nd_train = xgb.DMatrix(x_train, y_train)\nd_valid = xgb.DMatrix(x_valid, y_valid)\nd_test = xgb.DMatrix(X_test)\n\nparams = {}\nparams['min_child_weight'] = 10.0\nparams['objective'] = 'binary:logistic'\nparams['eta'] = 0.02\nparams['silent'] = True\nparams['max_depth'] = 9\nparams['subsample'] = 0.9\nparams['colsample_bytree'] = 0.9\n\n# Define the gini metric - from https://www.kaggle.com/c/ClaimPredictionChallenge/discussion/703#5897\ndef gini(actual, pred, cmpcol = 0, sortcol = 1):\n    assert( len(actual) == len(pred) )\n    all = np.asarray(np.c_[ actual, pred, np.arange(len(actual)) ], dtype=np.float)\n    all = all[ np.lexsort((all[:,2], -1*all[:,1])) ]\n    totalLosses = all[:,0].sum()\n    giniSum = all[:,0].cumsum().sum() / totalLosses\n    \n    giniSum -= (len(actual) + 1) / 2.\n    return giniSum / len(actual)\n \ndef gini_normalized(a, p):\n    return gini(a, p) / gini(a, a)\n\n# Create an XGBoost-compatible metric from Gini\n\ndef gini_xgb(preds, dtrain):\n    labels = dtrain.get_label()\n    gini_score = gini_normalized(labels, preds)\n    return [('gini', gini_score)]\n\nwatchlist = [(d_train, 'train'), (d_valid, 'valid')]\n\nmodel = xgb.train(params, d_train, 100, watchlist, early_stopping_rounds=100, feval=gini_xgb, maximize=True, verbose_eval=10)\n\nxgb.plot_importance(model)\nfig, ax = plt.subplots(figsize=(12,18))\nplt.show()\n\np_test = model.predict(d_test)\n\n#--- Submission file ---\n\nsub = pd.DataFrame()\nsub['id'] = X_id_test\nsub['target'] = p_test\nsub.to_csv('xgb2.csv', index=False)\n\n\nimportance = model.get_fscore(fmap='xgb.fmap')\nimportance = sorted(importance.items(), key=operator.itemgetter(1))\n\ndf = pd.DataFrame(importance, columns=['feature', 'fscore'])\n\nplt.figure()\ndf.plot()\ndf.plot(kind='barh', x='feature', y='fscore', legend=False, figsize=(10, 25))\nplt.gcf().savefig('features_importance.png')\n''' "},{"metadata":{"_uuid":"2600a5e8c0cf4304f33d4df8a351eb5686448983","collapsed":true,"_cell_guid":"42b05673-ff63-4245-9fde-a44b99568bf7"},"cell_type":"markdown","source":"### Can you think of more features? Let me know in the comments!\n\n# STAY TUNED FOR MORE UPDATES !!!"}],"metadata":{"language_info":{"version":"3.6.3","nbconvert_exporter":"python","codemirror_mode":{"version":3,"name":"ipython"},"pygments_lexer":"ipython3","file_extension":".py","name":"python","mimetype":"text/x-python"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat_minor":1,"nbformat":4}