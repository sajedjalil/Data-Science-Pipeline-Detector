{"cells":[{"metadata":{"_cell_guid":"4a2ced75-0b94-498b-aa08-7a17252b2ece","_uuid":"310dd5f5368f5d94c6548809dc5bd777fd843bc7"},"source":"# Tips for Using Scikit-Learn for Evaluation","cell_type":"markdown"},{"metadata":{"_cell_guid":"2fb2298a-1ad9-4c8f-bdda-66eff55af8d3","_uuid":"3eab7fdab169a8ac1ffaf6d7455f56f063492738"},"source":"I ran into a few things that tripped me up when working with the evaluation metric in Scikit-Learn.\n\nThere are a few examples out there of how to calculate Gini, but I found that basing it off of the `roc_auc_score` within Scikit-Learn seemed cleaner than building on by hand or using some of the other ones that I've seen out there. \n\nOne of the first things you'd like to see from an algorithm and a dataset is evidence that it's learning. The function `learning_curve` is useful for that and you can pass it a custom scoring function. \n\nHowever, I ran into some unexpected results at first.","cell_type":"markdown"},{"metadata":{"scrolled":true,"_cell_guid":"75cf13f1-1130-49a0-943a-3fe0ffaafc98","_uuid":"07e8cc99f4f953ffc7cff2e1640689551a1e4623","collapsed":true},"source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.metrics import make_scorer, roc_auc_score\nfrom sklearn.linear_model import LogisticRegression","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"822fee9a-70c1-44a1-9991-bc0fa934a25b","_uuid":"c0489117d20bf223a886bd90843095494f72dff7","collapsed":true},"source":"train_data = pd.read_csv('../input/train.csv', na_values='-1')\ntrain_data.fillna(value=train_data.median(), inplace=True)","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"cf02532a-dd0c-41ae-ab71-40dcaa7d0b5b","_uuid":"02957568442454bd4e837a54d2f1caaa5d68f27a"},"source":"**NB**: This kernel doesn't showcase any feature engineering, just some simple interpolation to ensure that a predictive model will run.","cell_type":"markdown"},{"metadata":{"_cell_guid":"6b050c95-f58a-4a63-9054-57ee79e7536b","_uuid":"f62b3e3aab064e828d71ea2f4e864dc292673221","collapsed":true},"source":"X_train, y_train = train_data.iloc[:, 2:], train_data.iloc[:, 1]\n\ndel train_data","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"ced813bb-fcf0-4a50-9f5a-193c8cfc62f1","_uuid":"5b7f7d8edaf51210d5e5b8f1f6e21f9c36d9e5d2"},"source":"Given the `roc_auc_score` function built into Scikit-Learn, we can easily compute the Gini coefficient.","cell_type":"markdown"},{"metadata":{"_cell_guid":"103b20c2-7b3a-41e7-a108-f4f0aaae7464","_uuid":"cb16e7ce9010485b5335e6945f706d95510512c0","collapsed":true},"source":"def gini_normalized(y_actual, y_pred):\n    \"\"\"Simple normalized Gini based on Scikit-Learn's roc_auc_score\"\"\"\n    gini = lambda a, p: 2 * roc_auc_score(a, p) - 1\n    return gini(y_actual, y_pred) / gini(y_actual, y_actual)","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"11a222eb-1503-40af-a3d4-3932a86d99ab","_uuid":"23d597c2bc181db571d3e4db7a814dab884f4a4d","collapsed":true},"source":"lr = LogisticRegression()\n\ntrain_sizes, train_scores, test_scores = learning_curve(\n    estimator=lr,\n    X=X_train,\n    y=y_train,\n    train_sizes=np.linspace(0.05, 1, 6),\n    cv=5,\n    scoring=make_scorer(gini_normalized)\n)","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"8cd88494-fbc0-4db7-ba19-94b83ff8a73a","_uuid":"4f3aa15fbfb1717a959513968eed0f9442717837","scrolled":true},"source":"train_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\nplt.plot(train_sizes, train_mean, \n         color='blue', marker='o', \n         markersize=5, \n         label='training gini')\nplt.fill_between(train_sizes, \n                 train_mean + train_std,\n                 train_mean - train_std, \n                 alpha=0.15, color='blue')\nplt.plot(train_sizes, test_mean, \n         color='green', linestyle='--', \n         marker='s', markersize=5, \n         label='validation gini')\nplt.fill_between(train_sizes, \n                 test_mean + test_std,\n                 test_mean - test_std, \n                 alpha=0.15, color='green')\nplt.grid()\nplt.xlabel('Number of training samples')\nplt.ylabel('Normalized Gini')\nplt.legend(loc='lower right')\nplt.ylim([-0.25, 0.25])\nplt.show()","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"726640a3-8dc4-4dc5-8640-53c4e8eb5e8e","_uuid":"169561a86fa201c7d349ec4d4061624a00afb1d6"},"source":"The graph above looks really odd, as both the training and validation normalized Gini values are near zero for all of the dataset sizes. Without digging into it, I would have thought that there was no learning going on at all.","cell_type":"markdown"},{"metadata":{"_cell_guid":"ee98f11a-8a62-4254-a332-47a72ab42b78","_uuid":"063a63b1fbaef2e2ed1aae1da04c64440d76f264"},"source":"It turned out that the culprit was how Scikit-Learn scored the hold-out set. By default, it predicts using the `predict` method on the model rather than the `predict_proba` method. The output from `predict` on a classification problem is the class labels while the output from `predict_proba` is the probabilities for the class labels. For computing the Gini value on the results, the output of `predict_proba` is more appropriate. ","cell_type":"markdown"},{"metadata":{"_cell_guid":"c2e1d0e7-f9a0-4399-8030-1476fa42b7dc","_uuid":"2ec7657d0b111bdd44a6074dec4e2607f567bb45"},"source":"To ensure this happens, we modify the `gini_normalized` function to allow that.","cell_type":"markdown"},{"metadata":{"_cell_guid":"35389cb6-a41e-4b27-bad0-a294ba1d8d45","_uuid":"c84d7d0f80ffe191ed5beca181eb3e9a75a2889a","collapsed":true},"source":"def gini_normalized(y_actual, y_pred):\n    \"\"\"Simple normalized Gini based on Scikit-Learn's roc_auc_score\"\"\"\n    \n    # If the predictions y_pred are binary class probabilities\n    if y_pred.ndim == 2:\n        if y_pred.shape[1] == 2:\n            y_pred = y_pred[:, 1]\n    gini = lambda a, p: 2 * roc_auc_score(a, p) - 1\n    return gini(y_actual, y_pred) / gini(y_actual, y_actual)","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"349ea8e2-b45c-4d9b-b562-fc343bf460bf","_uuid":"a7f1bcba28e80f84125da8060f45506d27d11a16","collapsed":true},"source":"lr = LogisticRegression()\n\ntrain_sizes, train_scores, test_scores = learning_curve(\n    estimator=lr,\n    X=X_train,\n    y=y_train,\n    train_sizes=np.linspace(0.05, 1, 6),\n    cv=5,\n    scoring=make_scorer(gini_normalized, needs_proba=True)\n)","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"610b1720-8352-443f-8227-5ccc2fd9c4ed","_uuid":"e85621f767cd014029bfd9e4d452699d23270c56"},"source":"train_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\nplt.plot(train_sizes, train_mean, \n         color='blue', marker='o', \n         markersize=5, \n         label='training gini')\nplt.fill_between(train_sizes, \n                 train_mean + train_std,\n                 train_mean - train_std, \n                 alpha=0.15, color='blue')\nplt.plot(train_sizes, test_mean, \n         color='green', linestyle='--', \n         marker='s', markersize=5, \n         label='validation gini')\nplt.fill_between(train_sizes, \n                 test_mean + test_std,\n                 test_mean - test_std, \n                 alpha=0.15, color='green')\nplt.grid()\nplt.xlabel('Number of training samples')\nplt.ylabel('Normalized Gini')\nplt.legend(loc='lower right')\nplt.ylim([0.2, 0.3])\nplt.show()","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"655c2f25-3845-44c0-9f7b-189eeefdd986","_uuid":"559343ff56cebf381d3ffae7012d878c23a296df"},"source":"Once we've adjusted the `gini_normalized` and plotted the results of model fitting at different samples sizes of the training dataset, we see that indeed learning is happening correctly. \n\nWe also see that our evaluation metric seems correct.","cell_type":"markdown"},{"metadata":{"_cell_guid":"eeb9d9ef-69a7-4f76-a0a2-50f7491c98d7","_uuid":"796102ea49ab681a96fdec7ddf984c5265bfea03"},"source":"It might seem simple to veteran competitors, but I think it serves as another lesson learned to ensure that you understand the evaluation metric and that you can reproduce some of the baseline results.","cell_type":"markdown"}],"nbformat":4,"nbformat_minor":1,"metadata":{"language_info":{"mimetype":"text/x-python","nbconvert_exporter":"python","codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","pygments_lexer":"ipython3","name":"python","version":"3.6.3"},"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"}}}