{"cells":[{"source":"The idea of this Notebook is reorder the categorical variables to create a smaller decision tree, smaler ~= min(model.tree_.max_depth) ≃ getMetric()\n\nIt execute a random permutation, or a sequential permutatition (from itertools). I have no idea if this can optimize the classifiers or not, just an start point to reorder categorical variables based on some metric\nhttps://github.com/rspadim/CategoricalReorders\n\nchanged to multithread version ( 16 cores =] )","metadata":{"_uuid":"e333703c4c31cf9e90d6db37457068198b2a6c1e","_cell_guid":"7dfdfbdb-d389-4c21-ba76-89b15e6e4cbf"},"cell_type":"markdown"},{"source":"Read data","metadata":{"_uuid":"c65a3b0cb7c02913453e9bab9f157a369664660c","_cell_guid":"c206b3ba-3b96-4ee8-8e1d-bac72cf59721"},"cell_type":"markdown"},{"source":"import pandas as pd\nprint(\"reading files...\")\ntrain  =pd.read_csv(\"../input/train.csv\")\npredict=pd.read_csv(\"../input/test.csv\")\ncat_cols = [col for col in train.columns if '_cat' in col]\nprint(\"done :)\")","outputs":[],"execution_count":null,"metadata":{"_uuid":"a91a60132385d6bd2226f49744311d7c74e728dc","_cell_guid":"182415be-a332-4363-81d0-aca3c5a92c7b"},"cell_type":"code"},{"source":"Reorder function - it return the series and a dictionary to replace the predict dataset, maybe we can do better with a class?","metadata":{"_uuid":"e45257e9d015c96ce0d71d049ae91e2d363831b9","_cell_guid":"58c2b487-9b67-469f-8b33-a7aebfb7948e"},"cell_type":"markdown"},{"source":"import time\nimport numpy as np\nfrom math import factorial\nfrom itertools import permutations\nfrom sklearn.tree import DecisionTreeClassifier,DecisionTreeRegressor\nfrom sklearn.metrics import roc_auc_score,log_loss,mean_absolute_error,mean_squared_error,r2_score\nfrom xgboost import XGBClassifier\ndef getModel(classifier=True,tree_seed=19870425):\n    ## tests with xgb\n    #return XGBClassifier(max_depth=10000,\n    #                     learning_rate=0.1,\n    #                     n_estimators=10000, \n    #                     silent=True, \n    #                     objective='binary:logistic', \n    #                     booster='gbtree', \n    #                     n_jobs=1, \n    #                     nthread=None, \n    #                     gamma=0, \n    #                     min_child_weight=1, \n    #                     max_delta_step=0, \n    #                     subsample=1, \n    #                     colsample_bytree=1, \n    #                     colsample_bylevel=1, \n    #                     reg_alpha=0, \n    #                     reg_lambda=1, \n    #                     scale_pos_weight=1, \n    #                     base_score=0.5, \n    #                     random_state=tree_seed, \n    #                     seed=tree_seed, \n    #                     missing=None)\n    \n    \n    if(classifier):\n        return DecisionTreeClassifier(max_depth=None,presort=True,criterion='entropy',class_weight='balanced',random_state=tree_seed)\n    return DecisionTreeRegressor(max_depth=None,presort=True,random_state=tree_seed)\n\ndef getMetric(model):\n    ## tests with xgb\n    #print(type(model))\n    #print(vars(model))\n    #print(model._Booster.get_dump())\n    #__die\n    #if(type(model)==DecisionTreeClassifier):\n    #    return model.tree_.max_depth\n    #return 0\n    return model.tree_.max_depth\n\n# small black magic\ndef reorderCategorical(df,feature_col,target_col,classifier=True,\n                                 max_iterations=721,verbose=False,random_permutation=None,\n                                 tree_seed=19870425,random_seed=19870425):\n    #time it\n    start     = time.time()\n    values    =df[feature_col].sort_values().unique() #nd array, since df[col] is a series\n    len_values=len(values)\n\n    #min dictionary (l<=>l)\n    optimized=False\n    default_dict={l:l for l in values}\n    min_dict    ={l:l for l in values}\n    if(len_values<3):\n        if(verbose):\n            print(feature_col,': uniques=',len_values,', values=',values)\n            print('\\t\\tLESS THAN 3 UNIQUE VALUES, Time spent (seconds):',time.time() - start)\n        return df[feature_col],min_dict\n    \n    #Current Values\n    model=getModel(classifier,tree_seed)\n    model.fit(df[feature_col].values.reshape(-1,1),df[target_col])\n    min_depth_count=getMetric(model)\n    if(verbose):\n        print(feature_col,': uniques=',len_values,', depth=',min_depth_count,', values=',values)\n        if(classifier):\n            print('\\t\\tROC_AUC/LogLoss: ',\n                      roc_auc_score(df[target_col],model.predict_proba(df[feature_col].values.reshape(-1,1))[:,1] ),'/',\n                      log_loss(     df[target_col],model.predict_proba(df[feature_col].values.reshape(-1,1))[:,1]))\n        else:\n            print('\\t\\tMAE/MSE/R²: ',\n                      mean_absolute_error(df[target_col],model.predict(df[feature_col].values.reshape(-1,1))[:,1] ),'/',\n                      mean_squared_error( df[target_col],model.predict(df[feature_col].values.reshape(-1,1))[:,1]),'/',\n                      r2_score(           df[target_col],model.predict(df[feature_col].values.reshape(-1,1))[:,1]))\n    if(min_depth_count==1):\n        if(verbose):\n            print('\\t\\tDEPTH=1, Time spent (seconds):',time.time() - start)\n        return df[feature_col],min_dict\n    \n    #Naive order by count\n    if(classifier):\n        first_try=df[df[target_col]==0].groupby(feature_col)[feature_col].count().sort_values(ascending=True)\n    else:\n        #maybe a median/mean order? for example, target_col>mean(target) ?\n        first_try=df.groupby(feature_col)[feature_col].count().sort_values(ascending=True)\n    l,values_dict=0,{}\n    for i in first_try.index:\n        values_dict[values[l]]=i\n        l+=1\n    \n    model=getModel(classifier,tree_seed)\n    model.fit(df[feature_col].replace(values_dict).values.reshape(-1,1),df[target_col])\n    # better than l<=>l ?\n    if(min_depth_count>getMetric(model)):\n        optimized=True\n        if(verbose):\n            print('\\tNaive order by count: from ',min_depth_count,' to ',getMetric(model),', dict:',min_dict)\n            if(classifier):\n                print('\\t\\tROC_AUC/LogLoss: ',\n                          roc_auc_score(df[target_col],model.predict_proba(df[feature_col].replace(values_dict).values.reshape(-1,1))[:,1] ),'/',\n                          log_loss(     df[target_col],model.predict_proba(df[feature_col].replace(values_dict).values.reshape(-1,1))[:,1]))\n            else:\n                print('\\t\\tMAE/MSE/R²: ',\n                          mean_absolute_error(df[target_col],model.predict(df[feature_col].replace(values_dict).values.reshape(-1,1))[:,1] ),'/',\n                          mean_squared_error( df[target_col],model.predict(df[feature_col].replace(values_dict).values.reshape(-1,1))[:,1]),'/',\n                          r2_score(           df[target_col],model.predict(df[feature_col].replace(values_dict).values.reshape(-1,1))[:,1]))\n        min_depth_count,min_dict=getMetric(model),values_dict\n        if(min_depth_count==1):\n            if(verbose):\n                print('\\t\\tDEPTH=1, Time spent (seconds):',time.time() - start)\n            return df[feature_col].replace(values_dict),values_dict\n    elif(verbose):\n        print('\\t\\t=[ No optimization using naive order by Count')\n    \n    # Search Space:\n    # maybe random_permutatition isn't the best method... \n    #     if len(permutations)~=factorial(len_values) < max_iterations, we can use permutatition (real brute force)\n    if(random_permutation==None):\n        random_permutation=False\n        if(factorial(len_values)>max_iterations):\n            random_permutation=True\n            if(verbose):\n                print('\\t\\tToo big search space, using RANDOM SAMPLING')\n        elif(verbose):\n            print('\\t\\tmax_iterations (',max_iterations,') >Factorial(length) (',factorial(len_values),'), USING PERMUTATION')\n    \n    # TODO: maybe we can do better with GA ?!\n    if(random_permutation):\n        # random permutation ( good lucky =] )\n        np.random.seed(random_seed)\n        space=range(max_iterations)\n    else:\n        # default itertools permutation\n        space=permutations(values)\n\n    count=0\n    for perm in space:\n        if(count>max_iterations):\n            break\n        # random permutation\n        if(random_permutation):\n            perm=np.random.permutation(values)\n        \n        values_dict={values[i]:perm[i] for i in range(0,len_values)}\n        model=getModel(classifier,tree_seed)\n        model.fit(df[feature_col].replace(values_dict).values.reshape(-1,1),df[target_col])\n        if(min_depth_count>getMetric(model)):\n            optimized=True\n            if(verbose):\n                print('\\t',count,'/',max_iterations,'NEW!!! from',min_depth_count,' to ',getMetric(model),' dict:',values_dict)\n                if(classifier):\n                    print('\\t\\tROC_AUC/LogLoss: ',\n                              roc_auc_score(df[target_col],model.predict_proba(df[feature_col].replace(values_dict).values.reshape(-1,1))[:,1] ),'/',\n                              log_loss(     df[target_col],model.predict_proba(df[feature_col].replace(values_dict).values.reshape(-1,1))[:,1]))\n                else:\n                    print('\\t\\tMAE/MSE/R²: ',\n                              mean_absolute_error(df[target_col],model.predict(df[feature_col].replace(values_dict).values.reshape(-1,1))[:,1] ),'/',\n                              mean_squared_error( df[target_col],model.predict(df[feature_col].replace(values_dict).values.reshape(-1,1))[:,1]),'/',\n                              r2_score(           df[target_col],model.predict(df[feature_col].replace(values_dict).values.reshape(-1,1))[:,1]))\n            min_depth_count,min_dict=getMetric(model),values_dict\n            if(min_depth_count==1):\n                print('\\t\\tDEPTH=1')\n                break\n        count+=1\n    if(verbose):\n        print('\\t\\tTime spent (seconds):',time.time() - start)\n    if(not optimized):\n        return df[feature_col],default_dict\n    return df[feature_col].replace(values_dict),values_dict\n","outputs":[],"execution_count":null,"metadata":{"collapsed":true,"_uuid":"c5f62300c9cd996e3687e0218155e25e24ac9289","_cell_guid":"86452d0c-290e-4ef2-a9c0-d4025ad9ceea"},"cell_type":"code"},{"source":"Let's work! BRUTE FORCE IT!","metadata":{"_uuid":"d89e3e7ccf4fb31e38eba56c4cd00380fbf967ed","_cell_guid":"f0f6e3ad-1cca-43ce-b00f-d372905688b2"},"cell_type":"markdown"},{"source":"\n#MULTI THREAD VERSION:\nimport psutil \nimport threading\n\nlock = threading.Lock()\ndef threaded_function(args):\n    global train,predict,lock\n    #print('cat_cols:',len(args))\n    for i in args:\n        reordered,values_dict=reorderCategorical(train,i,'target',verbose=True)\n        with lock:\n            train[  i+'_reordered']=reordered\n            predict[i+'_reordered']=predict[i].replace(values_dict)\n        print(i)\n\nif __name__ == \"__main__\":\n    # 4 threads\n    print(\"Dream machine: :P, 128GB, 16cores\")\n    print('cores: ',psutil.cpu_count(),' threads:',psutil.cpu_count(logical=False),\n         'freq: ',psutil.cpu_freq())\n    print('memory: ',psutil.virtual_memory())\n    print('swap: ',psutil.swap_memory())\n\n    cores=16\n    lencat =len(cat_cols)\n    lencatdiv=lencat//cores\n    start_end=[]\n    for i in range(0,cores):\n        if(i==cores-1):\n            start_end.append([i*lencatdiv+1,lencat]) # last one\n        else:\n            start_end.append([i*lencatdiv+1,lencatdiv*(i+1)])\n    print('start/end: ',len(start_end))\n    threads,l=[],0\n    \n    for i in start_end:\n        #print(\"cols: \",i[0],i[1],' - ',cat_cols[i[0] : i[1]])\n        #print(\"thread: \",l)\n        threads.append( threading.Thread(target = threaded_function, args = (cat_cols[i[0] : i[1]],) ) )\n        threads[l].start()\n        l+=1\n    l=0\n    for i in start_end:\n        threads[l].join()\n        l+=1\n    print(\"thread finished...exiting\")\n","outputs":[],"execution_count":null,"metadata":{"scrolled":true,"_uuid":"0fad6268511318b6f2c1bfb36684a5422e3386a9","_cell_guid":"b150a41a-8151-4fca-a512-4ea1ac367d47"},"cell_type":"code"},{"source":"## SINGLE THREAD\n#for i in cat_cols:\n#    train[i+'_reordered'],values_dict=reorderCategorical(train,i,'target',verbose=True,max_iterations=5)\n#    predict[i+'_reordered']=predict[i].replace(values_dict)\n#print('Nice job! =]')","outputs":[],"execution_count":null,"metadata":{"collapsed":true,"scrolled":true,"_uuid":"b1cffc8a187092f477de756616cc65297fd0e752","_cell_guid":"793f85fd-f38b-46c6-9537-8ae9ef9aa32b"},"cell_type":"code"},{"source":"THANKS KAGGLE COMPUTERS!","metadata":{"_uuid":"18cbd2fed12c095c09272d9c2c1471a1e781c6e5","_cell_guid":"dd1268bd-6a44-4707-b217-8285c18eb259"},"cell_type":"markdown"},{"source":"train.to_csv(  'Reordered-train.csv',index=False)\npredict.to_csv('Reordered-test.csv',index=False)","outputs":[],"execution_count":null,"metadata":{"collapsed":true,"_uuid":"08668c62c0a9bb0e60546e87e1d5263eb188f7a5","_cell_guid":"a2f0fdfa-86b1-4261-99cb-9aa3cb1693d5"},"cell_type":"code"},{"source":"","outputs":[],"execution_count":null,"metadata":{"collapsed":true,"_uuid":"96d1f883bd352ad741467784212d3258373305e2","_cell_guid":"21459b9e-d3cb-43cd-aa35-53810b9b0c57"},"cell_type":"code"}],"nbformat_minor":1,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","nbconvert_exporter":"python","codemirror_mode":{"version":3,"name":"ipython"},"mimetype":"text/x-python","pygments_lexer":"ipython3","version":"3.6.3","file_extension":".py"}},"nbformat":4}