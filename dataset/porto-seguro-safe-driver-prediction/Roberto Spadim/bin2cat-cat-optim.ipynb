{"cells":[{"source":"Read data","cell_type":"markdown","metadata":{"_uuid":"c65a3b0cb7c02913453e9bab9f157a369664660c","_cell_guid":"c206b3ba-3b96-4ee8-8e1d-bac72cf59721"}},{"outputs":[],"execution_count":null,"source":"import pandas as pd\nprint(\"reading files...\")\ntrain  =pd.read_csv(\"../input/train.csv\")\npredict=pd.read_csv(\"../input/test.csv\")\nbin_cols = [col for col in train.columns if '_bin' in col]\nprint(\"done :)\")","cell_type":"code","metadata":{"_uuid":"a91a60132385d6bd2226f49744311d7c74e728dc","collapsed":true,"_cell_guid":"182415be-a332-4363-81d0-aca3c5a92c7b"}},{"source":"convert bins to categorical","cell_type":"markdown","metadata":{"_uuid":"910d94a3e44959989e9aad5fe3ab0e47adf91eeb","_cell_guid":"84a3abfb-b727-4cab-9d25-8d6078a83691"}},{"outputs":[],"execution_count":null,"source":"# from https://www.kaggle.com/rspadim/convert-binary-to-categorical/notebook\n\nimport warnings\nimport numpy as np\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nclass BinToCat(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n    def fit(self, X, y=None, **kwargs):\n        cols=X.columns\n        if(len(cols)>64):\n            warnings.warn(\"Caution, more than 64 bin columns, 2**64 can overflow int64\")\n        for i in cols:\n            unique_vals=X[i].unique()\n            if(len(unique_vals)>2):\n                raise Exception(\"Column \"+i+\" have more than 2 values, is it binary? values: \"+str(unique_vals))\n            if not (0 in unique_vals and 1 in unique_vals):\n                raise Exception(\"Column \"+i+\" have values different from 0/1, is it binary? values: \"+str(unique_vals))\n        self.scale=np.array([1<<i for i in range(np.shape(X)[1])])\n        \n    def transform(self, X):\n        return np.sum(self.scale*X,axis=1)\n        ","cell_type":"code","metadata":{"_uuid":"5738284a130de686d50b03125af31917f82a714d","collapsed":true,"_cell_guid":"81157029-2e51-4e46-b42f-4378608ec6f9"}},{"outputs":[],"execution_count":null,"source":"a=BinToCat()\na.fit(train[bin_cols])\ntrain['bins']  =a.transform(train[bin_cols])\npredict['bins']=a.transform(predict[bin_cols])\n","cell_type":"code","metadata":{"_uuid":"06435e3b3c49036641f8a7307e5a43bdc64988d4","collapsed":true,"_cell_guid":"dc53bdd5-8f5f-439d-aea6-d2e383961970"}},{"source":"Reorder function - it return the series and a dictionary to replace the predict dataset","cell_type":"markdown","metadata":{"_uuid":"e45257e9d015c96ce0d71d049ae91e2d363831b9","_cell_guid":"58c2b487-9b67-469f-8b33-a7aebfb7948e"}},{"outputs":[],"execution_count":null,"source":"# from https://www.kaggle.com/rspadim/categorical-optimization-tree-and-logistic\nimport time\nimport numpy as np\nfrom math import factorial\nfrom itertools import permutations\nfrom sklearn.tree import DecisionTreeClassifier,DecisionTreeRegressor\nfrom sklearn.metrics import roc_auc_score,log_loss,mean_absolute_error,mean_squared_error,r2_score\nfrom xgboost import XGBClassifier\n\n\ndef getModel(classifier=True,tree_seed=19870425):\n    ## tests with xgb\n    #return XGBClassifier(max_depth=10000,\n    #                     learning_rate=0.1,\n    #                     n_estimators=10000, \n    #                     silent=True, \n    #                     objective='binary:logistic', \n    #                     booster='gbtree', \n    #                     n_jobs=1, \n    #                     nthread=None, \n    #                     gamma=0, \n    #                     min_child_weight=1, \n    #                     max_delta_step=0, \n    #                     subsample=1, \n    #                     colsample_bytree=1, \n    #                     colsample_bylevel=1, \n    #                     reg_alpha=0, \n    #                     reg_lambda=1, \n    #                     scale_pos_weight=1, \n    #                     base_score=0.5, \n    #                     random_state=tree_seed, \n    #                     seed=tree_seed, \n    #                     missing=None)\n    \n    \n    if(classifier):\n        return DecisionTreeClassifier(max_depth=None,presort=True,criterion='entropy',class_weight='balanced',random_state=tree_seed)\n    return DecisionTreeRegressor(max_depth=None,presort=True,random_state=tree_seed)\n\ndef getMetric(model):\n    ## tests with xgb\n    #print(type(model))\n    #print(vars(model))\n    #print(model._Booster.get_dump())\n    #__die\n    #if(type(model)==DecisionTreeClassifier):\n    #    return model.tree_.max_depth\n    #return 0\n    return model.tree_.max_depth\n\n# small black magic\ndef reorderCategorical(df,feature_col,target_col,classifier=True,\n                                 max_iterations=721,verbose=False,random_permutation=None,\n                                 tree_seed=19870425,random_seed=19870425):\n    #time it\n    start     = time.time()\n    values    =df[feature_col].sort_values().unique() #nd array, since df[col] is a series\n    len_values=len(values)\n\n    #min dictionary (l<=>l)\n    optimized=False\n    default_dict={l:l for l in values}\n    min_dict    ={l:l for l in values}\n    if(len_values<3):\n        if(verbose):\n            print(feature_col,': uniques=',len_values,', values=',values)\n            print('\\t\\tLESS THAN 3 UNIQUE VALUES, Time spent (seconds):',time.time() - start)\n        return df[feature_col],min_dict\n    \n    #Current Values\n    model=getModel(classifier,tree_seed)\n    model.fit(df[feature_col].values.reshape(-1,1),df[target_col])\n    min_depth_count=getMetric(model)\n    if(verbose):\n        print(feature_col,': uniques=',len_values,', depth=',min_depth_count,', values=',values)\n        if(classifier):\n            print('\\t\\tROC_AUC/LogLoss: ',\n                      roc_auc_score(df[target_col],model.predict_proba(df[feature_col].values.reshape(-1,1))[:,1] ),'/',\n                      log_loss(     df[target_col],model.predict_proba(df[feature_col].values.reshape(-1,1))[:,1]))\n        else:\n            print('\\t\\tMAE/MSE/R²: ',\n                      mean_absolute_error(df[target_col],model.predict(df[feature_col].values.reshape(-1,1))[:,1] ),'/',\n                      mean_squared_error( df[target_col],model.predict(df[feature_col].values.reshape(-1,1))[:,1]),'/',\n                      r2_score(           df[target_col],model.predict(df[feature_col].values.reshape(-1,1))[:,1]))\n    if(min_depth_count==1):\n        if(verbose):\n            print('\\t\\tDEPTH=1, Time spent (seconds):',time.time() - start)\n        return df[feature_col],min_dict\n    \n    #Naive order by count\n    if(classifier):\n        first_try=df[df[target_col]==0].groupby(feature_col)[feature_col].count().sort_values(ascending=True)\n    else:\n        #maybe a median/mean order? for example, target_col>mean(target) ?\n        first_try=df.groupby(feature_col)[feature_col].count().sort_values(ascending=True)\n    l,values_dict=0,{}\n    for i in first_try.index:\n        values_dict[values[l]]=i\n        l+=1\n    \n    model=getModel(classifier,tree_seed)\n    model.fit(df[feature_col].replace(values_dict).values.reshape(-1,1),df[target_col])\n    # better than l<=>l ?\n    if(min_depth_count>getMetric(model)):\n        optimized=True\n        if(verbose):\n            print('\\tNaive order by count: from ',min_depth_count,' to ',getMetric(model),', dict:',min_dict)\n            if(classifier):\n                print('\\t\\tROC_AUC/LogLoss: ',\n                          roc_auc_score(df[target_col],model.predict_proba(df[feature_col].replace(values_dict).values.reshape(-1,1))[:,1] ),'/',\n                          log_loss(     df[target_col],model.predict_proba(df[feature_col].replace(values_dict).values.reshape(-1,1))[:,1]))\n            else:\n                print('\\t\\tMAE/MSE/R²: ',\n                          mean_absolute_error(df[target_col],model.predict(df[feature_col].replace(values_dict).values.reshape(-1,1))[:,1] ),'/',\n                          mean_squared_error( df[target_col],model.predict(df[feature_col].replace(values_dict).values.reshape(-1,1))[:,1]),'/',\n                          r2_score(           df[target_col],model.predict(df[feature_col].replace(values_dict).values.reshape(-1,1))[:,1]))\n        min_depth_count,min_dict=getMetric(model),values_dict\n        if(min_depth_count==1):\n            if(verbose):\n                print('\\t\\tDEPTH=1, Time spent (seconds):',time.time() - start)\n            return df[feature_col].replace(values_dict),values_dict\n    elif(verbose):\n        print('\\t\\t=[ No optimization using naive order by Count')\n    \n    # Search Space:\n    # maybe random_permutatition isn't the best method... \n    #     if len(permutations)~=factorial(len_values) < max_iterations, we can use permutatition (real brute force)\n    if(random_permutation==None):\n        random_permutation=False\n        if(factorial(len_values)>max_iterations):\n            random_permutation=True\n            if(verbose):\n                print('\\t\\tToo big search space, using RANDOM SAMPLING')\n        elif(verbose):\n            print('\\t\\tmax_iterations (',max_iterations,') >Factorial(length) (',factorial(len_values),'), USING PERMUTATION')\n    \n    # TODO: maybe we can do better with GA ?!\n    if(random_permutation):\n        # random permutation ( good lucky =] )\n        np.random.seed(random_seed)\n        space=range(max_iterations)\n    else:\n        # default itertools permutation\n        space=permutations(values)\n\n    count=0\n    for perm in space:\n        if(count>max_iterations):\n            break\n        # random permutation\n        if(random_permutation):\n            perm=np.random.permutation(values)\n        \n        values_dict={values[i]:perm[i] for i in range(0,len_values)}\n        model=getModel(classifier,tree_seed)\n        model.fit(df[feature_col].replace(values_dict).values.reshape(-1,1),df[target_col])\n        if(min_depth_count>getMetric(model)):\n            optimized=True\n            if(verbose):\n                print('\\t',count,'/',max_iterations,'NEW!!! from',min_depth_count,' to ',getMetric(model),' dict:',values_dict)\n                if(classifier):\n                    print('\\t\\tROC_AUC/LogLoss: ',\n                              roc_auc_score(df[target_col],model.predict_proba(df[feature_col].replace(values_dict).values.reshape(-1,1))[:,1] ),'/',\n                              log_loss(     df[target_col],model.predict_proba(df[feature_col].replace(values_dict).values.reshape(-1,1))[:,1]))\n                else:\n                    print('\\t\\tMAE/MSE/R²: ',\n                              mean_absolute_error(df[target_col],model.predict(df[feature_col].replace(values_dict).values.reshape(-1,1))[:,1] ),'/',\n                              mean_squared_error( df[target_col],model.predict(df[feature_col].replace(values_dict).values.reshape(-1,1))[:,1]),'/',\n                              r2_score(           df[target_col],model.predict(df[feature_col].replace(values_dict).values.reshape(-1,1))[:,1]))\n            min_depth_count,min_dict=getMetric(model),values_dict\n            if(min_depth_count==1):\n                print('\\t\\tDEPTH=1')\n                break\n        count+=1\n    if(verbose):\n        print('\\t\\tTime spent (seconds):',time.time() - start)\n    if(not optimized):\n        return df[feature_col],default_dict\n    return df[feature_col].replace(values_dict),values_dict\n","cell_type":"code","metadata":{"_uuid":"c5f62300c9cd996e3687e0218155e25e24ac9289","collapsed":true,"_cell_guid":"86452d0c-290e-4ef2-a9c0-d4025ad9ceea"}},{"source":"Let's work! BRUTE FORCE IT!","cell_type":"markdown","metadata":{"_uuid":"d89e3e7ccf4fb31e38eba56c4cd00380fbf967ed","_cell_guid":"f0f6e3ad-1cca-43ce-b00f-d372905688b2"}},{"outputs":[],"execution_count":null,"source":"# SINGLE THREAD\n_,values_dict=reorderCategorical(train,'bins','target',verbose=True,max_iterations=100)\ntrain['bins_reordered']  =train['bins'].replace(values_dict)\npredict['bins_reordered']=predict['bins'].replace(values_dict)\nprint('Nice job! =]')","cell_type":"code","metadata":{"_uuid":"b1cffc8a187092f477de756616cc65297fd0e752","collapsed":true,"scrolled":true,"_cell_guid":"793f85fd-f38b-46c6-9537-8ae9ef9aa32b"}},{"source":"THANKS KAGGLE COMPUTERS!","cell_type":"markdown","metadata":{"_uuid":"18cbd2fed12c095c09272d9c2c1471a1e781c6e5","_cell_guid":"dd1268bd-6a44-4707-b217-8285c18eb259"}},{"outputs":[],"execution_count":null,"source":"train.to_csv(  'train.bin2cat-reordered.csv',index=False)\npredict.to_csv('test.bin2cat-reordered.csv',index=False)","cell_type":"code","metadata":{"_uuid":"08668c62c0a9bb0e60546e87e1d5263eb188f7a5","collapsed":true,"_cell_guid":"a2f0fdfa-86b1-4261-99cb-9aa3cb1693d5"}},{"outputs":[],"execution_count":null,"source":"from sklearn import tree\nfrom graphviz import Source\nimport matplotlib.pyplot as plt","cell_type":"code","metadata":{"_uuid":"96d1f883bd352ad741467784212d3258373305e2","collapsed":true,"_cell_guid":"21459b9e-d3cb-43cd-aa35-53810b9b0c57"}},{"outputs":[],"execution_count":null,"source":"#output tree using binaries:)\nmodel=DecisionTreeClassifier(criterion='gini',class_weight='balanced',max_depth=None)\n# dataset 1\nmodel.fit(train[bin_cols],train['target'])\ny_hat1=model.predict_proba(train[bin_cols])[:,1]\nloss1 =log_loss(train['target'],y_hat1)\nprint('    depth:  ',model.tree_.max_depth)\nprint('    logloss:',loss1)","cell_type":"code","metadata":{"collapsed":true}},{"outputs":[],"execution_count":null,"source":"#plot tree :)\nSource( tree.export_graphviz(model, out_file=None))","cell_type":"code","metadata":{"_uuid":"8476c258ae02b064188882aaed081ac4734fe331","collapsed":true,"_cell_guid":"34625ea6-e52a-4729-9506-2e17e412de2e"}},{"outputs":[],"execution_count":null,"source":"model.fit(train['bins_reordered'].reshape(-1,1),train['target'])\ny_hat1=model.predict_proba(train['bins_reordered'].reshape(-1,1))[:,1]\nloss1 =log_loss(train['target'],y_hat1)\nprint('    depth:  ',model.tree_.max_depth)\nprint('    logloss:',loss1)","cell_type":"code","metadata":{"collapsed":true}},{"outputs":[],"execution_count":null,"source":"#plot tree :)\nSource( tree.export_graphviz(model, out_file=None))","cell_type":"code","metadata":{"collapsed":true}}],"nbformat":4,"metadata":{"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"},"language_info":{"name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","file_extension":".py","mimetype":"text/x-python","version":"3.6.3","codemirror_mode":{"name":"ipython","version":3}}},"nbformat_minor":1}