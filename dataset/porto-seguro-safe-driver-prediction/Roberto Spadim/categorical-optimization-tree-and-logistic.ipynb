{"nbformat":4,"cells":[{"cell_type":"markdown","metadata":{"_uuid":"eb03d8e044458b5bdd864ee7a9d41db68cf00c96","_cell_guid":"044726cd-d497-41b0-b8d7-0d3aa12b27b6"},"source":"Motivation:\n\nfrom https://www.kaggle.com/rspadim/categorical-variables-feature-tree-optimization/ comment by Nooh:\n \n>https://www.kaggle.com/rspadim/categorical-variables-feature-tree-optimization/comments#233465\n>\"Can you guide a little as what exactly this technique does? Its kinda new to me\"\n\nI will write what I was trying to do, and maybe give some idea to logistic regression optimization (but not sure if this is ok)\n\n---\n\nConsider you have two small dataset:\n\n    X=[1,2,3,4,5]\n    Y=[0,0,1,0,0]\n\nThe problem is: **create a decision tree to reproduce the X->Y relation, consider that tree can only use >= operator, and have max_depth=infinity.**\n\nThe most probable tree is:\n\n    if(x>=3): #first depth\n        if(x>=4): #second depth\n            y=0\n        else:     #second depth\n            y=1\n    else:     #first depth\n        y=0\n\nthe problem now is: **create the decision tree with max_depth=1, and classification error=0 (overfit it)**. \n\nanswer: **it's not possible without reordering X dataset, all tree will have an classification error**. ok... all trees:\n\n    if(x>=0):\n        y=0\n    else:\n        y=1\n\n    if(x>=1):\n        y=0\n    else:\n        y=1\n\nok, i will stop here, you will never overfit this dataset with only one depth\n"},{"cell_type":"markdown","metadata":{"_uuid":"60f16ad21ca5cf262dc1255806bbd38b8739c772","_cell_guid":"0f02c303-0372-40b5-9c36-75ca406f2ddd"},"source":"in first overfitted tree (depth=2) you have a global if (x>=3), and a second depth with if (x>=4), the only way to do it with only one if is:\n\nchange 'x=3' with 'x=1' or 'x=5'. my algorithm is not optimal (it don't converge fast and don't give the best answer allways), first I do a naive algorithm reordering by X features count (maybe by Y is better, but I did a naive algorithm, =) you can optimize it ) and if it didn't converge to 1 depth, run a permutation of all unique values of X, if X unique values length is too big (more than 6 values ≃ 721 permutations if I'm not wrong) I use a random sampling (with a limit of tries) to try new orders (good luck!!!). \n\nat each new order of X values you create a new tree, if tree depth is smaller (in this case depth=1) than initial tree (in this case depth=2) save the reorder dictionary.\n\n\n---\n\n\"naive\" algorithm part:\n\n    #group by X, count X values, sorting by count(X) asc\n    first_try=df.groupby(feature_col)[feature_col].count().sort_values(ascending=True)\n    l,values_dict=0,{}\n    for i in first_try.index:\n        values_dict[values[l]]=i\n        l+=1\n    model=getModel(classifier,tree_seed)\n    model.fit(df[feature_col].replace(values_dict).values.reshape(-1,1),df[target_col])\n\nwhat this will do?\n\n    X[1 -> 1, 2 -> 1, 3->1, 4-> 1, 5->1]\n\nno optimization here, probably tree depth = 2\n\nwhat we could do? permutation since it's a small dataset: [1,2,3,4,5], [1,2,3,5,4], [1,2,5,3,4] ... but we can do better, let's go to first best answer:\n\n    group by Y:\n    Y[0->{1,2,4,5},1->{3}]\n\nnow, reorder X values\n0 values will be:\n\n    1->0, 2->1, 4-> 2, 5-> 3\n\n1 values will be:\n\n    3->4\n\nnew dataset is:\n\n    X=[0,1,2,3,4]\n    Y=[0,0,0,0,1]\n\ncreate the new tree:\n\n    if(X>=4):\n        y=1\n    else:\n        y=0\n\n(i will talk more about it at end of this notebook, it's  just an idea about optimization with unbalanced data)"},{"cell_type":"markdown","metadata":{"_uuid":"9d7a0dd544a8e9d75c0d5015cfe0a07b14b1d500","_cell_guid":"fc77eb84-fab1-4aa3-8b37-d0b9638c3504"},"source":"---\n\nit's a model complexity minimization algorithm, \"minimize the (tree depth) subject to reordering categorical features\", now we have only 1 if(x>=4)\n\nnow logistic regression...\n\nthe problem is... how this make betters linear models? I'm not sure about my answer, BUT, from our friend @Eric Vos, \"this optimize linear models\", why? ordered data is easier to linear models fit =), when linear values are easy to separe with big weights. \n\nchanging X values the new parameters of model Y=logit(weights), can be very small for Y=0 (X<=3) and very big to Y=1 (X>4), but you will see that X=4 is \"not good\", X=100000000 is \"better\", from logistic regression (wikipedia, but just to explain what happens in logistic classifier parameters):\n\nln(pi / (1-pi) = B0 + B1*x1 + ....\n\npi = 1/(1+exp(- B0 + B1*x1 + ...)\n\ny = 1 / (1+exp(-f(X))\n\ndiff(y) = Y(1-y) * df/dX"},{"cell_type":"markdown","metadata":{"_uuid":"1c6f504896039ce079b62d7b282b46d8b6e02844","_cell_guid":"3d671bbc-d75c-490a-a15a-7619550148fb"},"source":"# Let's code =)"},{"source":"import numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import log_loss\nfrom graphviz import Source\nimport matplotlib.pyplot as plt","cell_type":"code","outputs":[],"metadata":{"collapsed":true,"_uuid":"07d6a2d75bc35cdf3c5be5b721432e3f4d1de8a8","_cell_guid":"7d612434-f3a0-47ed-88a0-2b2497d5193e"},"execution_count":null},{"cell_type":"markdown","metadata":{"_uuid":"4b74241e25cd6310be1a18a6beec285db1cb2e4f","_cell_guid":"7c6df2ca-11bd-41b4-8f37-7423b73c3a25"},"source":"# DataSets:"},{"cell_type":"markdown","metadata":{"_uuid":"0ccffe0cca2dac788cbfc6430386719a3ba5bd9a","_cell_guid":"dc3ee0dc-ef59-44d7-9b89-7d530dbb367e"},"source":"I will use 5 datasets to show some things...\n\n1) is normal unsorted data\n\n2) sorted X, X=4 Y=1, all others Y=0\n\n3) one hot encode with only 1 Y=1\n\n4) one hot encode with 2 Y=1\n\n5) same from dataset 2 but with X=5 replaced by X=10000000"},{"source":"# FIRST DATA SET\nX1=np.array([[1],[2],[3],[4],[5]])\ny1=np.array([[0],[0],[1],[0],[0]]) # with bad y=1 at x=3, we will get depth=2\n\n# REORDERED\nX2=np.array([[0],[1],[2],[3],[4]])\ny2=np.array([[0],[0],[0],[0],[1]]) # nice x -> y, we will get depth =1\n\n# TEST ONE HOT ENCODE\nX3=np.array([[1,0,0,0,0],[0,1,0,0,0],[0,0,1,0,0],[0,0,0,0,1],[0,0,0,0,1]])\ny3=np.array([[0],[0],[1],[0],[0]])\n\n# TEST ONE HOT ENCODE (x1=1 and x3=1)\nX4=np.array([[1,0,0,0,0],[0,1,0,0,0],[0,0,1,0,0],[0,0,0,0,1],[0,0,0,0,1]])\ny4=np.array([[1],[0],[1],[0],[0]])\n\n# TEST LINEAR MODELS:\nX5=np.array([[0],[1],[2],[3],[10000000]])  #nice X values, it's good to linear models\ny5=np.array([[0],[0],[0],[0],[1]]) # nice x->y","cell_type":"code","outputs":[],"metadata":{"collapsed":true,"_uuid":"9f8e280d2c7351c662086863e83e15b43a8a4b83","_cell_guid":"c16759c9-cb0d-4ca9-a3a7-bf789a5910b9"},"execution_count":null},{"cell_type":"markdown","metadata":{"_uuid":"3c0cdcc0f6e8272cf0c8b85f51a9dfec03962820","_cell_guid":"5f6e4bfc-e4a3-485b-81de-23d5dc8124f3"},"source":"# First, let's see decision trees:\n"},{"source":"#http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n#    I will set some parameters, the main parameter is class_weight\n#\n#    criterion\n#       The function to measure the quality of a split. \n#       Supported criteria are “gini” for the Gini impurity and \n#       “entropy” for the information gain. <- i like this, but let's use gini...\n#\n#    class_weight\n#        The “balanced” mode uses the values of y to automatically adjust weights \n#        inversely proportional to class frequencies in the input data as \n#\n#        weight= n_samples / (n_classes * np.bincount(y))\n#        \n#    max_depth\n#        The maximum depth of the tree. If None, then nodes are expanded until all \n#        leaves are pure or until all leaves contain less than min_samples_split samples.\n\nmodel=DecisionTreeClassifier(criterion='gini',class_weight='balanced',max_depth=None)","cell_type":"code","outputs":[],"metadata":{"collapsed":true,"_uuid":"f46ff8564c86cd67615c4e1991416cb09deec4ea","_cell_guid":"ff285f57-965c-4d4e-a0c9-0c44c19ceeac"},"execution_count":null},{"cell_type":"markdown","metadata":{"_uuid":"bf81d45b3a06ad24f11222f27ea10d3f106a1f1d","_cell_guid":"31cb9565-b3b0-4909-b43a-22136fdf784d"},"source":"# Dataset 1 - default values"},{"source":"# dataset 1\nmodel.fit(X1,y1)\ny_hat1=model.predict_proba(X1)[:,1]\nloss1 =log_loss(y1,y_hat1)\nprint(\"dataset1: \")\nprint('    depth:  ',model.tree_.max_depth)\nprint('    proba:  ',y_hat1)\nprint('    logloss:',loss1)\n#plot tree :)\nSource( tree.export_graphviz(model, out_file=None))","cell_type":"code","outputs":[],"metadata":{"collapsed":true,"_uuid":"f3e29e92014e306f9c59f71fc47222b6ca259285","_cell_guid":"b92aa342-d4ea-4ceb-a6aa-6f84d8c24b77"},"execution_count":null},{"cell_type":"markdown","metadata":{"_uuid":"4ee57c04cf1fa102e109897ce7f8ec9d6bfd2e63","_cell_guid":"884e4a64-c068-414a-9c99-18ad86d22fcb"},"source":"Here an important note...\n\ncheck samples: \n\n    samples=5\n    samples=2, samples=3 -> X values [1,2] , [3,4,5]\n    samples=1, samples=2 -> X values [3]   , [4,5]\n\nthat's how tree see X values [1,2,3,4,5] / Y [0,0,1,0,0], it cut Y=1 in 3 leafs (box without arrows ≃ final values) with 2 depths"},{"cell_type":"markdown","metadata":{"_uuid":"8e27fbf0be79d34e1d565d19e6623f8d96a0cfac","_cell_guid":"9ec85872-d558-40ea-8c1b-0599b154a059"},"source":"# Dataset 2 - ordered X categories"},{"source":"# dataset 2\nmodel.fit(X2,y2)\ny_hat2=model.predict_proba(X2)[:,1]\nloss2 =log_loss(y2,y_hat2)\nprint(\"dataset2: \")\nprint('    depth:  ',model.tree_.max_depth)\nprint('    proba:  ',y_hat1)\nprint('    logloss:',loss1)\n#plot tree :)\nSource( tree.export_graphviz(model, out_file=None))","cell_type":"code","outputs":[],"metadata":{"collapsed":true,"_uuid":"bf8b4bb36a4eb82d2306afefba02c7d4ca0f3043","_cell_guid":"a758ab3e-7984-4718-a654-666e6ea4cbd2"},"execution_count":null},{"cell_type":"markdown","metadata":{"_uuid":"5c3466431858c8c45ddc1256a3ae4dedca90aa54","_cell_guid":"174af679-6ad6-4a5a-bf97-39b76b25ede3"},"source":"check samples: \n\n    samples=5\n    samples=4, samples=1 -> X values [0,1,2,3] , [4]\n\nthat's how tree see X values [0,1,2,3,4], only one depth, 2 leafs"},{"cell_type":"markdown","metadata":{"_uuid":"7adac21ed687bacc71c63a9b1a9f1c191985f0a8","_cell_guid":"934bfcdd-5d78-46fd-9b6e-ca26f54f1d72"},"source":"# Dataset 5 (big X value)"},{"source":"# dataset 5\nmodel.fit(X5,y5)\ny_hat5=model.predict_proba(X5)[:,1]\nloss5 =log_loss(y5,y_hat5)\nprint(\"dataset5: \")\nprint('    depth:  ',model.tree_.max_depth)\nprint('    proba:  ',y_hat5)\nprint('    logloss:',loss5)\n#plot tree :)\nSource( tree.export_graphviz(model, out_file=None))","cell_type":"code","outputs":[],"metadata":{"collapsed":true,"_uuid":"34f23dab62e8aae5008708ff8968b68327dcb02d","_cell_guid":"ae81908a-fb53-413c-a901-870b919ba0de"},"execution_count":null},{"cell_type":"markdown","metadata":{"_uuid":"eecb742412e1f543540fa10df66b1d2b25f9a77d","_cell_guid":"e5708e24-6226-4024-8f59-34d319c9d601"},"source":"same here, but X<= 50000000000000000000000000000000000000000000000 :P\n\nFrom: https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/41161#231655\n>CPMP\n>xgboost is almost insensitive to monotonic feature transformations. What may really change xgboost behavior is a change in the order of values for a given feature.\n\nthat's why @CPMP told it, values transformation in one feature column without mixing others features isn't good to trees, BUT, reordering is, it's right just remember that SIN() COS() functions can reorder it too, be carefull with \"Non Monotonic\" math functions =]"},{"cell_type":"markdown","metadata":{"_uuid":"3fa39bca102178ca033e3aeb90d10c70be790c5c","_cell_guid":"d4000a80-1d88-4c59-9284-c93fdb30020a"},"source":"# Decision tree with one hot encode"},{"cell_type":"markdown","metadata":{"_uuid":"0fda0068b4646e9e37590730449e61d8e6c877ef","_cell_guid":"793ff44e-e24e-448a-a7c5-b2ceae4923e6"},"source":"# Dataset 3 - one Y=1"},{"source":"# dataset 3\nprint('X=',X3)\nprint('Y=',y3)\nmodel.fit(X3,y3)\ny_hat3=model.predict_proba(X3)[:,1]\nloss3 =log_loss(y3,y_hat3)\nprint(\"dataset3: \")\nprint('    depth:  ',model.tree_.max_depth)\nprint('    proba:  ',y_hat3)\nprint('    logloss:',loss3)\n#plot tree :)\nSource( tree.export_graphviz(model, out_file=None))\n","cell_type":"code","outputs":[],"metadata":{"collapsed":true,"_uuid":"c9dcd0f7664503a9caaf3cfc660562985781438a","_cell_guid":"6f70a394-bfc8-43cc-996d-ea42f292afd8"},"execution_count":null},{"cell_type":"markdown","metadata":{"_uuid":"46310e07b2b970d7d98e525a2d19b283a7bf9249","_cell_guid":"d8c2ad2c-8a03-4d4f-982f-2e7efb9d2444"},"source":"# Dataset 4 - two Y=1"},{"source":"# dataset 4\nprint('X=',X4)\nprint('Y=',y4)\nmodel.fit(X4,y4)\ny_hat4=model.predict_proba(X4)[:,1]\nloss4 =log_loss(y4,y_hat4)\nprint(\"dataset3: \")\nprint('    depth:  ',model.tree_.max_depth)\nprint('    proba:  ',y_hat3)\nprint('    logloss:',loss3)\n#plot tree :)\nSource( tree.export_graphviz(model, out_file=None))\n","cell_type":"code","outputs":[],"metadata":{"collapsed":true,"_uuid":"b1500d0a3fa88929c303863aac33ac2c4a6dc0da","_cell_guid":"e1bd5ee1-e190-4ec9-828a-16e4d2047ff9"},"execution_count":null},{"cell_type":"markdown","metadata":{"_uuid":"91a85ca4911a2040724c3dcfd70edb2899f80d4a","_cell_guid":"08f89407-98c7-4708-a1a7-56b9146872a7"},"source":"Check that one hot encode, give depth=2 leafs=3, when we could use max depth =1, leafs=2, with sorted categorical variables\n\nThat's why you should be carefull with OHE and decision trees!"},{"cell_type":"markdown","metadata":{"_uuid":"f0c5d3edad12951140c42b16cdd6480fe91a8c5a","_cell_guid":"a76b0a44-8932-4777-a2b2-64df2710182d"},"source":"let's see if decision tree fit Y values well? it must!"},{"source":"#Y values\nplt.title('Dataset 1')\nplt.plot(y1,label='Y_true')\nplt.plot(y_hat1,label='y_hat1 - log loss:'+str(loss1))\nplt.legend()\nplt.show()\n\nplt.title('Dataset 2')\nplt.plot(y2,label='Y_true')\nplt.plot(y_hat2,label='y_hat2 - log loss:'+str(loss2))\nplt.legend()\nplt.show()\n\nplt.title('Dataset 3')\nplt.plot(y3,label='Y_true')\nplt.plot(y_hat3,label='y_hat3 - log loss:'+str(loss3))\nplt.legend()\nplt.show()\n\nplt.title('Dataset 4')\nplt.plot(y4,label='Y_true')\nplt.plot(y_hat4,label='y_hat4 - log loss:'+str(loss4))\nplt.legend()\nplt.show()\n\nplt.title('Dataset 5')\nplt.plot(y5,label='Y_true')\nplt.plot(y_hat5,label='y_hat5 - log loss:'+str(loss5))\nplt.legend()\nplt.show()","cell_type":"code","outputs":[],"metadata":{"collapsed":true,"_uuid":"16ef61dd2e03fe0f25ec2a2f4f55da249de4689e","_cell_guid":"ffdd19ca-e523-47b7-95f0-709fd38cdfae"},"execution_count":null},{"cell_type":"markdown","metadata":{"_uuid":"bd17d301e11e4eb5bd1e674f928087bc78fa7c1b","_cell_guid":"737cb3d8-7061-4b0b-a41d-663b058ccb0a"},"source":"Now, let's see what logistic regression do, I never tryed it before, let's try"},{"source":"#http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\nmodel=LogisticRegression()\n\n# try changing penalty and C hyperparameters :)","cell_type":"code","outputs":[],"metadata":{"collapsed":true,"_uuid":"d7216d1f4ee34a61ffe53cc1cec77eff60156613","_cell_guid":"bb847ae7-0183-41bd-86da-830176307817"},"execution_count":null},{"source":"model.fit(X1,y1.ravel())\ny_hat1=model.predict_proba(X1)[:,1]\nloss1 =log_loss(y1,model.predict_proba(X1)[:,1])\nprint(\"model1: \")\nprint('    coefs:  ',model.coef_)\nprint('    y_true: ',y1.ravel())\nprint('    proba:  ',y_hat1)\nprint('    logloss:',loss1)\n","cell_type":"code","outputs":[],"metadata":{"collapsed":true,"_uuid":"4c50f68e630dafefe12ae82743b9ef3221a67365","_cell_guid":"fc47fe92-918d-4020-aa00-dec29f0c6866"},"execution_count":null},{"source":"model.fit(X2,y2.ravel())\ny_hat2=model.predict_proba(X2)[:,1]\nloss2 =log_loss(y2,model.predict_proba(X2)[:,1])\nprint(\"model2: \")\nprint('    coefs:  ',model.coef_)\nprint('    y_true: ',y2.ravel())\nprint('    proba:  ',y_hat2)\nprint('    logloss:',loss2)\n\n","cell_type":"code","outputs":[],"metadata":{"collapsed":true,"_uuid":"df72aaa650903aea8f332b36dd742f0906f29611","_cell_guid":"e760a05d-97a3-497a-9762-433566109209"},"execution_count":null},{"source":"model.fit(X3,y3.ravel())\ny_hat3=model.predict_proba(X3)[:,1]\nloss3 =log_loss(y3,model.predict_proba(X3)[:,1])\nprint(\"model3: \")\nprint('    coefs:  ',model.coef_)\nprint('    y_true: ',y3.ravel())\nprint('    proba:  ',y_hat3)\nprint('    logloss:',loss3)\n\n","cell_type":"code","outputs":[],"metadata":{"collapsed":true,"_uuid":"1b7efc2e52201adc2372d8ff77f28928481462f2","_cell_guid":"0dba2c40-fc80-42c0-a988-9797c9a159f1"},"execution_count":null},{"source":"model.fit(X4,y4.ravel())\ny_hat4=model.predict_proba(X4)[:,1]\nloss4 =log_loss(y4,model.predict_proba(X4)[:,1])\nprint(\"model4: \")\nprint('    coefs:  ',model.coef_)\nprint('    y_true: ',y4.ravel())\nprint('    proba:  ',y_hat4)\nprint('    logloss:',loss4)\n","cell_type":"code","outputs":[],"metadata":{"collapsed":true,"_uuid":"47a41fc8d9366334a012d2517beaedef7e96331e","_cell_guid":"d50be9de-6866-4bd8-94bd-5170f69ce554"},"execution_count":null},{"source":"model.fit(X5,y5)\ny_hat5=model.predict_proba(X5)[:,1]\nloss5 =log_loss(y5,model.predict_proba(X5)[:,1])\nprint(\"model5: \")\nprint('    coefs:  ',model.coef_)\nprint('    y_true: ',y5.ravel())\nprint('    proba:  ',y_hat5)\nprint('    logloss:',loss5)\n\n","cell_type":"code","outputs":[],"metadata":{"collapsed":true,"_uuid":"b25777e88e362c38eea32d7b0b49acfa6424a422","_cell_guid":"04d4f747-09dc-496e-b140-19b83fffc165"},"execution_count":null},{"cell_type":"markdown","metadata":{"_uuid":"3f8db4a43bf4c7e6e815eb75d888b22a1276461a","_cell_guid":"e397d5c9-b7fd-4c0d-96a0-36f9753858ee"},"source":"Let's see some plots...\n\n# Dataset 1, 3 have same Y, but 3 have One Hot Encode (nice to logistic regression)"},{"source":"plt.title('Dataset 1,3')\nplt.plot(y1,label='Y_true')\nplt.plot(y_hat1,label='y_hat1 - log loss:'+str(loss1))\nplt.plot(y_hat3,label='y_hat3 - log loss:'+str(loss3))\nplt.legend()\nplt.show()\n","cell_type":"code","outputs":[],"metadata":{"collapsed":true,"_uuid":"50baaf1fe30afc2d6dd211111624b7953e32784d","_cell_guid":"685c87cf-72c2-44be-9510-87cc1d68cf12"},"execution_count":null},{"cell_type":"markdown","metadata":{"_uuid":"47ca7fdda654a5cc7cd776d7c5419c6e82f6b7bb","_cell_guid":"b3591650-17f7-4bc1-ae51-4acf796a3693"},"source":"# Dataset 2,5 have same Y too\n\nbut check how a BIG X value helps a lot!"},{"source":"print('X2=',X2.ravel(),'Y2=',y2.ravel())\nprint('X5=',X5.ravel(),'Y5=',y5.ravel())\nplt.title('Dataset 2,5')\nplt.plot(y2,label='Y_true')\nplt.plot(y_hat2,label='y_hat2 - log loss:'+str(loss2))\nplt.plot(y_hat5,label='y_hat5 - log loss:'+str(loss5))\nplt.legend()\nplt.show()\n","cell_type":"code","outputs":[],"metadata":{"collapsed":true,"scrolled":false,"_uuid":"5b8d7f19e6d35ea6b7d907f7b4e22493d7b4748c","_cell_guid":"3f7a49ea-84ae-4145-888d-a4037e885b74"},"execution_count":null},{"cell_type":"markdown","metadata":{"_uuid":"58d990f7312918e2c8dfc0b20cfecd3a6afe665f","_cell_guid":"14901ab1-46f6-4f81-9e31-9c44830ac2bb"},"source":"In this case, only sorting features isn't 100%, but we can use small X values to Y=0, and big to Y=1\n\nfor example...  \n\nwith a 1000 unique values X dataset, and from x=1 to x=500 we have y=0, and to x=10000 to x=10500 we have Y=1\n\nlet's try..."},{"source":"X1=list(range(0,1000))\ny1=[0]*500 +[1]*500\n\nX2_10000=list(range(10000,10500))\nX2_0    =list(range(0,500))\nX2=X2_0+X2_10000\ny2=y1\n\nprint('X1=',X1)\nprint('y1=',y1)\nprint('\\n\\n-------------------------\\n\\n')\nprint('X2=',X2)\nprint('y2=',y2)","cell_type":"code","outputs":[],"metadata":{"collapsed":true,"_uuid":"27e1915eac335d6ad72671787d1bba523854840e","_cell_guid":"a8137ec2-e0ac-4025-b477-29254f88870a"},"execution_count":null},{"cell_type":"markdown","metadata":{"_uuid":"216829eee0af79e6a6d5ee9261866dda448691e3","_cell_guid":"8d94fc66-3011-4471-aa08-fab8fd3b9077"},"source":"Let's fit it with logistic regression"},{"source":"X1_v=np.vstack(X1)\nmodel.fit(X1_v,y1)\ny_hat1=model.predict(X1_v)\nloss1 =log_loss(y1,y_hat1)\nprint(\"model1: \")\nprint('    coefs:  ',model.coef_)\nprint('    y_true: ',y1)\nprint('    proba:  ',y_hat1)\nprint('    logloss:',loss1)\n\nX2_v=np.vstack(X2)\nmodel.fit(X2_v,y2)\ny_hat2=model.predict(X2_v)\nloss2 =log_loss(y2,y_hat2)\nprint(\"model2: \")\nprint('    coefs:  ',model.coef_)\nprint('    y_true: ',y2)\nprint('    proba:  ',y_hat2)\nprint('    logloss:',loss2)\n","cell_type":"code","outputs":[],"metadata":{"collapsed":true,"_uuid":"5b709d5e811ad289f1bf268b7868d9145903caa1","_cell_guid":"1b869989-5bff-498c-81ed-730c37f1b30a"},"execution_count":null},{"source":"plt.title('Dataset with X small and X big')\nplt.plot(y1,label='Y_true')\nplt.plot(y_hat1,label='y_hat1 - X small - log loss:'+str(loss1),alpha=.8)\nplt.plot(y_hat2,label='y_hat2 - X big - log loss:'+str(loss2),alpha=.2)\nplt.legend()\nplt.show()\n","cell_type":"code","outputs":[],"metadata":{"collapsed":true,"_uuid":"307d9a7386b5bc28f6319787efd12e76f1c8a43e","_cell_guid":"9a4fae85-9dc7-4023-8cbe-df768bc5efa3"},"execution_count":null},{"cell_type":"markdown","metadata":{"_uuid":"cb29d6760588704cfe3e67bbc4f3d7f639cb76ef","_cell_guid":"0b77f06a-0735-4f33-820c-58cbee0cd272"},"source":"Check that y_hat2 have better log loss!\n\nI think that's all =) good luck!"},{"cell_type":"markdown","metadata":{"collapsed":true,"_uuid":"3f0e62736fd33d1a7151c21c1a8a59222e8822b1","_cell_guid":"291673b8-8f11-4629-a312-f5ea1a5ae9cf"},"source":"---\nlet's go back to unbalanced X-Y and optimization idea, just see what happen when try to implement some categorical optimization\n\n\n\ni will implement this new idea soon :), but must check problems with unbalanced data, example: X=3 Y->20% = 0, 80% = 1\n\n    x=[3,3,3,3,3,3,3,4]\n    y=[0,1,0,1,0,1,0,1]\n\nwhat's the best x values? maybe here an global optimization is better, must use try-and-error feedback... but some naive ideas:\n\n    x=3 => y=0 p()=3/7, y=1 p()=4/7\n    x=4 => y=1 p()=1\n\nmaybe:\n\n    y=1\n\nbut using sklearn we check this :\n\n    if(x>=4):\n        y=1\n    else:\n        if(x>=3): (y=0 p()=3/7, y=1 p()=4/7, more probability to y=1 than y=0)\n            y=3/7 (i was thinking this could be 1, but not... this should be 4/7 or 3/7, sklearn used 3/7... why? i don't know, but this reduce log loss!)\n        else:\n            we don't have more data... reduce this if \n\nreduce everything to\n\n    if(x>=4):\n        y=1\n    else:\n       y=3/7"},{"source":"model=DecisionTreeClassifier(criterion='gini',class_weight='balanced',max_depth=None)\nx=np.vstack([3,3,3,3,3,3,3,4])\ny=[0,1,0,1,0,1,0,1]\n\n# dataset 3\nprint('X=',x.ravel())\nprint('Y=',y)\nmodel.fit(x,y)\ny_hat_both=model.predict_proba(x)\ny_hat_both2=y_hat_both.copy()\ny_hat_both2[:,0],y_hat_both2[:,1]=y_hat_both[:,1],y_hat_both[:,0] # inverse\ny_hat_both2[7][0]=0\ny_hat_both2[7][1]=1\n\ny_hat    =y_hat_both[:,1]\ny_hat_4_7=y_hat_both2[:,1]\nloss =log_loss(y,y_hat)\nloss1=log_loss(y,y_hat_4_7)\nprint(\"X-Y: \")\nprint('    depth:  ',model.tree_.max_depth)\nprint('    y_true: ',y)\nprint('    proba:  ',y_hat)\nprint('    logloss:',loss)\n\nprint(\"\\nWhat happen if use 4/7 instead of 3/7? let's see log loss only, but each metric give something different\")\nprint('    proba:  ',y_hat_4_7)\nprint('    logloss:',loss1)\n\n#plot tree :)\nSource( tree.export_graphviz(model, out_file=None))\n","cell_type":"code","outputs":[],"metadata":{"collapsed":true,"_uuid":"6c5862d57ab840e662b29cddfde01942d6a79ee0","_cell_guid":"88d30d41-5631-4ba6-a485-718406275c30"},"execution_count":null},{"source":"# plot output\nplt.title('Unbalanced')\nplt.plot(y,label='Y_true')\nplt.plot(y_hat,label='y_hat - log loss:'+str(loss))\nplt.plot(y_hat_4_7,label='y_hat using 4/7 - log loss:'+str(loss1))\nplt.legend()\nplt.show()\n","cell_type":"code","outputs":[],"metadata":{"collapsed":true,"_uuid":"62bcf1f4600d151c715f88080b5715fe2509161b","_cell_guid":"f51b32e0-71b4-4334-b9d5-c2e488a57f54"},"execution_count":null},{"cell_type":"markdown","metadata":{"collapsed":true,"_uuid":"0af041011cb1f856e9b4b91881bea3f0f9066b4f","_cell_guid":"dccc918e-f45f-444d-bd02-23b79ef2267c"},"source":"Let's use this last tree, to check rock auc with different probabilities :)"},{"source":"from sklearn.metrics import roc_auc_score\nprint('roc=',roc_auc_score(y,y_hat)    ,', gini=',roc_auc_score(y,y_hat)*2-1)\nprint('roc=',roc_auc_score(y,y_hat_4_7),', gini=',roc_auc_score(y,y_hat_4_7)*2-1)\n\nimport scikitplot as skplt\n\nskplt.metrics.plot_roc_curve(y, y_hat_both)\nplt.show()\nskplt.metrics.plot_roc_curve(y, y_hat_both2)\nplt.show()","cell_type":"code","outputs":[],"metadata":{"collapsed":true,"_uuid":"158b02951cc9e024c83cfa08019ccb755ed2bb37","_cell_guid":"f6369c94-6a94-45f7-a900-0a166cd9b370"},"execution_count":null},{"source":"","cell_type":"code","outputs":[],"metadata":{"collapsed":true,"_uuid":"99c137b5a267fa0169e2563a6a66e390eebeb42f","_cell_guid":"94916d51-03ca-4ea0-9969-42db49c285b8"},"execution_count":null}],"metadata":{"language_info":{"codemirror_mode":{"version":3,"name":"ipython"},"nbconvert_exporter":"python","version":"3.6.3","pygments_lexer":"ipython3","file_extension":".py","mimetype":"text/x-python","name":"python"},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"}},"nbformat_minor":1}