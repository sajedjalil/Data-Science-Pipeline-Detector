{"metadata":{"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"version":3,"name":"ipython"},"file_extension":".py","pygments_lexer":"ipython3","name":"python","mimetype":"text/x-python","version":"3.6.3","nbconvert_exporter":"python"}},"nbformat":4,"cells":[{"metadata":{"_uuid":"f4d2751317380840e5a1cef3b36dd400d257f123","_cell_guid":"27de7b34-4b57-4fd9-a9c5-d966f8cff7a9"},"cell_type":"markdown","source":"### GOAL\n\nThe goal of this notebook is to work ouor way through getting better predictive results by iterating. We will apply a ML and data-driven feature engineering approach since all of the feature don't say much about themselves as they are.\n\nTuning our algorithm and ensembling or stacking different ones will be the last thing we will do.\n\n"},{"metadata":{"_uuid":"dacba37a9bd873ac462108e70309803871d0074e","collapsed":true,"_cell_guid":"6bf2560a-10d9-4d16-a23c-d9262169088c"},"cell_type":"code","source":"%matplotlib inline\n\nimport pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt\nimport matplotlib\nfrom sklearn.model_selection import train_test_split, StratifiedShuffleSplit\nimport xgboost as xgb\n\nfrom IPython.display import display\nfrom collections import defaultdict","outputs":[],"execution_count":null},{"metadata":{"_uuid":"eb7b027f98a19ab0a5b10d0dbe0df2778281aa83","_cell_guid":"1d6abd26-39db-4e52-a0c9-0ecb60b7bf51"},"cell_type":"code","source":"dtrain = pd.read_csv('../input/train-and-test-csv/train.csv')\ndtest = pd.read_csv('../input/train-and-test-csv/test.csv')","outputs":[],"execution_count":null},{"metadata":{"_uuid":"f6bea63cd16df4279f1fa5c2bb54ccc1635342b3","collapsed":true,"_cell_guid":"1803e861-d170-4b1d-8ee2-eb9420713fe7"},"cell_type":"code","source":"def display_all(data):\n    with pd.option_context('display.max_rows', 1000):\n        with pd.option_context('display.max_columns', 1000):\n            return display(data)","outputs":[],"execution_count":null},{"metadata":{"_uuid":"897be662abc331b27314edf74a6b99c31ec985c9","_cell_guid":"c9f9301c-f997-45cb-bdf0-02cffd13e534"},"cell_type":"markdown","source":"### What do we now about data to start with ?\n\n*\"In the train and test data, features that belong to similar groupings are tagged as such in the feature names **(e.g., ind, reg, car, calc)**. **In addition, feature names include the postfix bin to indicate binary features and cat to indicate categorical features.** Features without these designations are either continuous or ordinal. Values of -1 indicate that the feature was missing from the observation. The target columns signifies whether or not a claim was filed for that policy holder.\"*"},{"metadata":{"_uuid":"3dabb3ec426403c22ffe3cb1ec7f53f01506d74c","_cell_guid":"b2dbde57-4d38-4f0e-8ac0-19d09626a707"},"cell_type":"markdown","source":"## Write something about target...\n\nWe see that there is almost 600K training and 900K test data observations, exactly 1.5x. As a good practice we mostly want to have a  balanced sample size to build models on and assess since they might generalize better with a similar sample size. \n\nSince we will be iterating quickly we want to work with a sample of data while applying our data driven feature engineering approach. For this purpose a similar ratio as train and test for subsampling can be taken.\n\n**Huge Problem Imbalance:** As seen below our dependent variable highly imbalanced ~3.7%. However this is very likely to happen in insurance claim context. Better risk allocating insurance companies generally have this type of ratio whereas a number up to ~10% can be seen through different companies in the sector. This been said having this low ratio is making the problem even harder!"},{"metadata":{"_uuid":"ec1b83a60b4331f8c63bdecb235bf98478774ad8","_cell_guid":"05887e5e-e5bd-4bbd-84b3-ebf72c57a438"},"cell_type":"code","source":"print(dtrain.shape)\nprint(dtest.shape)","outputs":[],"execution_count":null},{"metadata":{"_uuid":"244936de1fc84ddccb8dc8a6fa08e033706351e1","_cell_guid":"698ac7fd-1016-4322-a6d5-c5d8ea46bcc3"},"cell_type":"code","source":"#Target\nprint(dtrain.target.value_counts(normalize=True))","outputs":[],"execution_count":null},{"metadata":{"_uuid":"139170e565edcdf83f09f8353b092c06d45c738a","_cell_guid":"c843df69-1889-47e9-838d-1c8ccbe64c51"},"cell_type":"markdown","source":"### Get a feel of the data\n\nAs we look at both training and test samples we see that most of the data is already processed and most of the columns are categorical, but still there are some numerical ones too. \n\nFirst we will make some bad assumptions since we know NOTHING yet about the data and iterate our way through talking with the models and try to come up with better predictors while testing them."},{"metadata":{"_uuid":"7364fa64e0d4961df133189669b4ec7fa1ab4479","_cell_guid":"faf3eac5-d9b3-4ad2-9162-0bb013616bb1"},"cell_type":"code","source":"display_all(dtrain.head())","outputs":[],"execution_count":null},{"metadata":{"_uuid":"55ef331aa535f690205372bd4a41d2e2f3a815a8","_cell_guid":"15794ce1-4dd0-4b89-a58a-a14a0d66723a"},"cell_type":"code","source":"display_all(dtest.head())","outputs":[],"execution_count":null},{"metadata":{"_uuid":"53d6d59525bac898597b4a4ce558b4c0eafc581e","collapsed":true,"_cell_guid":"e4bc5aa1-11ce-4da2-9e06-52790742e396"},"cell_type":"code","source":"dtrain.replace(to_replace=-1, value=np.nan, inplace=True)\ndtest.replace(to_replace=-1, value=np.nan, inplace=True)","outputs":[],"execution_count":null},{"metadata":{"_uuid":"b8edb2b29ffc172becf880e307f0d054c06fc59c","collapsed":true,"_cell_guid":"befde657-571e-427f-8b7f-9620e9fc216b"},"cell_type":"code","source":"pred_columns = dtrain.columns[2:].values","outputs":[],"execution_count":null},{"metadata":{"_uuid":"47276bb936f7855c067049475ca68cfb12ed509a","_cell_guid":"5b578404-b137-4fa8-83c1-077ad8eaf37d"},"cell_type":"markdown","source":"### Let's output some summaries..\n\nThis will be helpful to see each variables in both train and test. That way we can look at to summaries to come up with new ideas. For example what to do NAs?"},{"metadata":{"_uuid":"80adf751c8c80907e2da049ea2e54e52ed08fa17","collapsed":true,"_cell_guid":"0b15ea0a-6f40-4ca4-abea-8736e2b1fac6"},"cell_type":"code","source":"### Let's do the column dtype conversions\npred_columns = dtrain.columns[2:]\nbin_cols = [c for c in pred_columns if 'bin' in c]\ncat_cols = [c for c in pred_columns if 'cat' in c]\nnum_cols= [c for c in pred_columns if c not in bin_cols and c not in cat_cols]","outputs":[],"execution_count":null},{"metadata":{"_uuid":"03e95e48168bac428b54796f09d1810548f1b230","_cell_guid":"6be41f53-f26e-4f43-9393-1506413a1490"},"cell_type":"markdown","source":"### Show summaries for categorical and numerical features\n\n\nIn categorical case, data in general show consistency between training and test. Which is good for us since data seems to be coming from the same generator.\n\nWe can also see that cardinalities for both train test data are the same, so we don't need to worry about missing category levels.\n\nWe can also see that similar group variables are have strong correlated features such as \n\n- Corr between ps_reg_02 and ps_reg_03: 0.7427425174286711\n- Corr between ps_car_12 and ps_car_13: 0.6705204199424284\n- ..."},{"metadata":{"_uuid":"dfe7594ee4a22873f6558024695b3a44ffe6d806","scrolled":true,"_cell_guid":"70d168e0-b933-4560-8282-3fc21db8e2fa"},"cell_type":"code","source":"for c in pred_columns:\n    if 'cat' in c or 'bin' in c:\n        print(f'Column: {c.upper()}')\n        print('Train Summary')\n        print(f'Cardinality {len(dtrain[c].unique())}')\n        print(dtrain[c].value_counts(dropna=False))\n        print('Test Summary')\n        print(f'Cardinality {len(dtest[c].unique())}')\n        print(dtest[c].value_counts(dropna=False))\n        print()\n        print()","outputs":[],"execution_count":null},{"metadata":{"_uuid":"be5487668be61c8b105f5a2e1145fd830f6b010e","collapsed":true,"_cell_guid":"b34cf7ec-4b40-454b-8bd1-4195187bffaf"},"cell_type":"code","source":"# For fast iteration we will be using a subsample of data\ndef subsample(data, ratio=0.5):\n    subsample, _ = train_test_split(data, test_size =ratio, stratify=dtrain.target)\n    return subsample","outputs":[],"execution_count":null},{"metadata":{"_uuid":"43bd1ccf78d828cc7f4e2adde8be795a036322ca","collapsed":true,"_cell_guid":"3256c949-b64e-4399-b449-05c47c205260"},"cell_type":"code","source":"subsample = subsample(dtrain)","outputs":[],"execution_count":null},{"metadata":{"_uuid":"c17c31d3789a3315fc11d25d6d90d1c2499d4f25","scrolled":true,"_cell_guid":"9621d536-4f4f-4822-b8be-9996e388fae1"},"cell_type":"code","source":"# If feature is not binary include in correlation matrix\ncorr_cols = [c for c in num_cols if len(dtrain[c].unique()) > 2]\nplt.imshow((subsample[corr_cols].corr()), cmap='hot', interpolation='nearest')\nplt.show()","outputs":[],"execution_count":null},{"metadata":{"_uuid":"f58df626800a6e3ed60bfa3c5b97185a1a1059b9","_cell_guid":"f153ece7-07a9-4eac-b74a-18c509ac23de"},"cell_type":"code","source":"corr_mat = np.array(subsample[corr_cols].corr())\n\ni_ix = np.where((corr_mat > 0.4) | (corr_mat < -0.4))[0]\nj_ix = np.where((corr_mat > 0.4) | (corr_mat < -0.4))[1]\nfor i, j in zip(i_ix, j_ix):\n    if i != j:\n        print(f'Corr between {corr_cols[i]} and {corr_cols[j]}: {corr_mat[i, j]}')","outputs":[],"execution_count":null},{"metadata":{"_uuid":"f2474b9c430fadfd6769ecbe862b362b893146e2","_cell_guid":"209d35c6-554e-40f6-afe3-295a4a73c484"},"cell_type":"markdown","source":"### ITERATIONS\n\nIn this part we will engineer features and run randomforest on a validation set and test it on our competition metric and try to if it adds some value to our model.\n\nRandom forest are fast and have good interpretations."},{"metadata":{"_uuid":"8a79b26410407980ae47f3276e72683801f4ae8e","_cell_guid":"2560ce6f-4781-49d2-a47e-5b8d21db43f9"},"cell_type":"markdown","source":"### GINI SCORE\n\nDefine competition metric"},{"metadata":{"_uuid":"b845146c564b7189c80e32868f1e678f6b538581","collapsed":true,"_cell_guid":"cb681e08-8d94-4674-83c2-f329d17fcde7"},"cell_type":"code","source":"def gini(actual, pred, cmpcol = 0, sortcol = 1):\n    assert( len(actual) == len(pred) )\n    all = np.asarray(np.c_[ actual, pred, np.arange(len(actual)) ], dtype=np.float)\n    all = all[ np.lexsort((all[:,2], -1*all[:,1])) ]\n    totalLosses = all[:,0].sum()\n    giniSum = all[:,0].cumsum().sum() / totalLosses\n\n    giniSum -= (len(actual) + 1) / 2.\n    return giniSum / len(actual)\n\ndef gini_normalized(a, p):\n    return gini(a, p) / gini(a, a)","outputs":[],"execution_count":null},{"metadata":{"_uuid":"4b0819dc398462ffb55652e3de9a528ddf356355","_cell_guid":"8ad97548-7e29-41a4-bbe4-04f8ae2ffa6c"},"cell_type":"markdown","source":"### CV FOLD ON SUBSAMPLE\n\nHaving a good cv fold and using it for validation is important in order to see whether are engineered features or new methods pay off **(random state)**\n\nSince we don't care about tuning parameters and since this is just the beginning we will use default xgboost. Also we want it to be shallow to get fast results.\n\nThis is our motivation:\n\n[thanks to @Winks here](\"https://datascience.stackexchange.com/questions/10640/how-to-perform-feature-engineering-on-unknown-features\")\n\n[also this video](\"https://www.youtube.com/watch?v=bL4b1sGnILU&t=891s\")\n\n\"First, run your boosting algorithm using only stumps, 1-level decision trees. Stumps are very weak, but Boosting makes it a reasonnable model. This will act as your baseline. Depending on the library you are using, you should be ale to display pretty easily which are the most used features, and you should plot them against the response (or do an histogram if the response is categorical) to identify some pattern. This might give you an intuition on what would be a good single feature transformation.\n\nNext, run the Boosting algorithm with 2-level decision trees. This model is a lot more complex than the previous one; if two variables taken together have more power than taken individually, this model should outperform your previous one (again, not in term of training error, but on validation error!). Based on this, you should be able to extract which variable are often used together, and this should lead you to potential multi-feature transformations.\"\n\nFor each iterations we will be using the feedback function and score summary below, as well as other outputs from xgboost."},{"metadata":{"_uuid":"d4252eeb116c9ce52f21d9360cc9a5c08d86a191","collapsed":true,"_cell_guid":"e550acdb-25ff-4f23-b35e-5a7a5dfbfdbb"},"cell_type":"code","source":"shuffleSplit = StratifiedShuffleSplit(n_splits=5, test_size=0.5, random_state= 10)","outputs":[],"execution_count":null},{"metadata":{"_uuid":"5ad291b8aa8f9d02ecd693440880785d076b531d","collapsed":true,"_cell_guid":"91449bcd-1d3e-4e7f-8123-1691223b9a08"},"cell_type":"code","source":"subsample.reset_index(drop=True, inplace=True)","outputs":[],"execution_count":null},{"metadata":{"_uuid":"f29287e07d5a5b1f6a21de487a0d9a4a8d0b60f4","collapsed":true,"_cell_guid":"ebbac501-316c-4267-8bd1-2f40c9b81ab2"},"cell_type":"code","source":"def xgb_feedback(params, nrounds, prc_subsample):\n    val_scores = []\n    for train_ix, val_ix in shuffleSplit.split(prc_subsample, prc_subsample.target):\n        X_train, y_train = prc_subsample.loc[train_ix].drop(['id', 'target'], axis=1), prc_subsample.target.loc[train_ix]\n        X_val, y_val = prc_subsample.loc[val_ix].drop(['id', 'target'], axis=1), prc_subsample.target.loc[val_ix]\n\n        #create dmatrix\n        dtrain = xgb.DMatrix(data=X_train, label=y_train, missing= np.nan)\n        dval = xgb.DMatrix(data=X_val, label=y_val, missing= np.nan)\n\n        #train\n        model = xgb.train(params, dtrain, num_boost_round=nrounds)\n        preds = model.predict(dval)\n        score = gini_normalized(y_val, preds)\n        val_scores.append(score)\n    return val_scores, model","outputs":[],"execution_count":null},{"metadata":{"_uuid":"2291433b69c1545efb6b6d196b5464d6d9613745","collapsed":true,"_cell_guid":"f8e41ef8-1d75-4acd-9ac7-601ea87f7d16"},"cell_type":"code","source":"def score_summary(scores):return f'Mean: {np.mean(scores)} Std: {np.std(scores)}'","outputs":[],"execution_count":null},{"metadata":{"_uuid":"3e9c0a2aee5cd684aae0b46a956e24e15316b24d","collapsed":true,"_cell_guid":"c059d123-404f-4285-af24-944a680238c8"},"cell_type":"code","source":"iter_performances = defaultdict()\ndef add_to_iter(scores, name):\n    iter_performances[name]= scores","outputs":[],"execution_count":null},{"metadata":{"_uuid":"d91a42a5d3c41f57bafbe1ed6d07327ef6295b93","collapsed":true,"_cell_guid":"f35f0f35-22d9-4043-9fe6-5663481d4547"},"cell_type":"code","source":"def plot_model(model):\n    matplotlib.rcParams['figure.figsize'] = [10, 7]\n    ax = xgb.plot_importance(model)","outputs":[],"execution_count":null},{"metadata":{"_uuid":"d7409e04b22cffa045763440c36e1d8c336cca8f","_cell_guid":"44139a11-a023-4c15-bcec-275775081fce"},"cell_type":"markdown","source":"### 1) BASELINE - NO FEATURE ENGINEERING\n\nHere we use data as it is. Including missing values as -1. Remember we can always call a new subsample but this might shadow our benchmarking and it wouldn't be unbiased so I will use the same subsample through this notebook but my processed subsamples will have the name prc_subsample."},{"metadata":{"_uuid":"e542be88f170a1741aa082852520fda0dcdae523","collapsed":true,"_cell_guid":"42b05855-712f-417f-a1ca-ec102f303215"},"cell_type":"code","source":"#first process is just to put those NA -1s back\ndef prc1(data):\n    data = data.fillna(-1)\n    return data","outputs":[],"execution_count":null},{"metadata":{"_uuid":"aa45baf5dfca453496628e34167ea598f8df63b9","_cell_guid":"c63f24e3-9121-48a6-8f8a-d84ac812b8a0"},"cell_type":"code","source":"prc_subsample = prc1(subsample)\n\nparams = {\"objective\":\"binary:logistic\", \"max_depth\":1}\nval_scores, model = xgb_feedback(nrounds=100, params=params, prc_subsample=prc_subsample)\n\ninit_benchmark = score_summary(val_scores)\n\n# So this will be our initial benchmark\nprint(init_benchmark)\n\nadd_to_iter(init_benchmark, \"benchmark\")","outputs":[],"execution_count":null},{"metadata":{"_uuid":"98f8e0764446374a6f7fdf4d965cf54fea2a7e57","_cell_guid":"6bec6c79-6665-44e8-ab5e-fca384e5faf9"},"cell_type":"markdown","source":"**The Beauty of this approach that it takes seconds to run a model and see the feature importances. ps_car_13 is stated to be a very effective feature by most of the competitors out there and here it is fast and accurate. [see it here](\"https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/41489\")**\n\nSo let's take a look at these top features..."},{"metadata":{"_uuid":"074ca8281be759b639bfe53614a5dc763e8a991f","scrolled":false,"_cell_guid":"80f21211-6216-4a42-88e2-9fb799777650"},"cell_type":"code","source":"matplotlib.rcParams['figure.figsize'] = [10, 7]\nax = xgb.plot_importance(model)","outputs":[],"execution_count":null},{"metadata":{"_uuid":"9cfe0b5233448b1d141ccc3f9aed454b7a9f81f5","_cell_guid":"7290108f-b3c2-4685-9cf9-3cbd4fa1a530"},"cell_type":"markdown","source":"### 2) ITER 1: PS_CAR_13"},{"metadata":{"_uuid":"03d7439794f25c0a1e0e8aa519cb2a7ee2b4f8c9","_cell_guid":"9b318d10-a2e8-437c-a7e4-e10c8f4176ce"},"cell_type":"markdown","source":"This distribution skewed to the right let's try log transform on this. Note for transformation on columns we will use {transform_name}_features and for other features we will use {fi}_features"},{"metadata":{"_uuid":"9f5a17c24cc4238430b261e6a3feeb971dced802","_cell_guid":"5788db0b-5619-40fa-b34d-c6b44f180645"},"cell_type":"code","source":"subsample.ps_car_13.hist(bins=30)","outputs":[],"execution_count":null},{"metadata":{"_uuid":"4753f561b2412754acdd00ea272b7cdc99b38667","_cell_guid":"0aeb1c19-4739-4ed9-88f4-f6a6f4f4e670"},"cell_type":"code","source":"np.log(subsample.ps_car_13).hist(bins=30)","outputs":[],"execution_count":null},{"metadata":{"_uuid":"0acd286fb188c60365b963995cacffef1952d6fd","_cell_guid":"ba41dd33-8c7a-4e9e-8aaf-4daa600f9077"},"cell_type":"code","source":"round(subsample.ps_car_13**2 * 48400).hist(bins =100)","outputs":[],"execution_count":null},{"metadata":{"_uuid":"31ba816014a043c96025915aec5efe8699dbe667","collapsed":true,"_cell_guid":"41c31047-c4a0-426b-beca-99acd5ba2a1b"},"cell_type":"code","source":"def prc2(data):\n    data = data.copy()\n    #log transform ps_car_13\n    data['log_ps_car_13'] = np.log(data[\"ps_car_13\"])\n    #create ps_car_13 feature from the kernel link\n    #thanks to @raddar\n    data['f1_ps_car_13'] = round(subsample[\"ps_car_13\"]**2 * 48400)\n    #also log this\n    data['log_f1_ps_car_13'] = np.log(round(subsample[\"ps_car_13\"]**2 * 48400))\n    return data","outputs":[],"execution_count":null},{"metadata":{"_uuid":"1ee9bebd88d7c2f0d4a8e66af9f92fc5e6c6c093","_cell_guid":"0dc25fdb-9264-48cd-ba4f-10874ddaf31e"},"cell_type":"code","source":"prc_subsample = prc2(subsample)\n\nparams = {\"objective\":\"binary:logistic\", \"max_depth\":1}\nval_scores, model = xgb_feedback(nrounds=100, params=params, prc_subsample=prc_subsample)\n\niter1_benchmark = score_summary(val_scores)\n\n# So this will be our initial benchmark\nprint(iter1_benchmark)\n\nadd_to_iter(iter1_benchmark, \"iter1\")","outputs":[],"execution_count":null},{"metadata":{"_uuid":"b7212a96c779516de802ad3e17c2a4f629c2f6d0","_cell_guid":"dcf84022-5dea-4944-ada5-a30c3968e175"},"cell_type":"markdown","source":"We see a little bit of improvement but not much, but std of val score is still the same which is a good indicator. Our val score on average improved by 0.25%. But this improvement maybe due to np.nan so let's try again with also NAs as -1. In fact improvement was due to NA introduction. So NAs must be important....Let's create a column with number of NAs"},{"metadata":{"_uuid":"ce3f96fbde3f076aafacb9461753316d3bc4d968","_cell_guid":"7cc542b4-bdcb-499b-9a62-8b71b49061c6"},"cell_type":"code","source":"prc_subsample = prc2(prc1(subsample))\n\nparams = {\"objective\":\"binary:logistic\", \"max_depth\":1}\nval_scores, model = xgb_feedback(nrounds=100, params=params, prc_subsample=prc_subsample)\n\niter1_nas_benchmark = score_summary(val_scores)\n\n# So this will be our initial benchmark\nprint(iter1_nas_benchmark)\n\nadd_to_iter(iter1_nas_benchmark, \"iter1_na_as_neg1\")","outputs":[],"execution_count":null},{"metadata":{"_uuid":"74b2b4ca3f2d19946efbbc10f1ba38d52a8aec8c","_cell_guid":"bda9680e-b2d5-44a9-950b-0e631137707a"},"cell_type":"code","source":"iter_performances","outputs":[],"execution_count":null},{"metadata":{"_uuid":"c9a3d74c3ad77019bc857ad56a84e020b208841e","_cell_guid":"8f92eb4c-445b-4efb-a5c9-b1f394f93333"},"cell_type":"markdown","source":"### 3) ITER 2 COLUMN: NUMBER OF NAs"},{"metadata":{"_uuid":"d1302ad3fb9aa3ad036b2ba0a901bbcd90c7ef4d","collapsed":true,"_cell_guid":"6cefa7d1-0ed7-48bc-98f2-d2588e74e1ef"},"cell_type":"code","source":"def prc3(data):\n    data= data.copy()\n    data[\"number_of_nan\"] = data.isnull().sum(axis=1)\n    return data","outputs":[],"execution_count":null},{"metadata":{"_uuid":"e8fd6ee387381acb55891fb078ed485f1e42ca1a","_cell_guid":"7e57ce82-6f70-4a22-aed2-d3d68a6b11df"},"cell_type":"code","source":"prc_subsample = prc3(subsample)\n\nparams = {\"objective\":\"binary:logistic\", \"max_depth\":1}\nval_scores, model = xgb_feedback(nrounds=100, params=params, prc_subsample=prc_subsample)\n\niter2 = score_summary(val_scores)\n\n# So this will be our initial benchmark\nprint(iter2)\n\nadd_to_iter(iter2, \"iter2\")","outputs":[],"execution_count":null},{"metadata":{"_uuid":"84407e0198cf3e59a38de61c14b59dc558343b10","_cell_guid":"4f2506e8-97a7-4821-b1c2-5b504111d1d1"},"cell_type":"code","source":"iter_performances","outputs":[],"execution_count":null},{"metadata":{"_uuid":"c4da700ef578492f048ed77afb89b28265497b70","_cell_guid":"d29aac3d-46f0-456e-bfab-478dea597181"},"cell_type":"code","source":"plot_model(model)","outputs":[],"execution_count":null},{"metadata":{"_uuid":"1e2cf028090288f27f3a415a2513dcc2a09b1c58","_cell_guid":"cf03247c-28c7-4e75-9193-83644e1e51c8"},"cell_type":"markdown","source":"### 4) ITER 3 : PS_CAR_15 + PS_REG_03"},{"metadata":{"_uuid":"f66155c73fb92ca66cc800bef99f7da3e90a04fb","_cell_guid":"3a809af9-e125-4f17-95a2-ec03dca66537"},"cell_type":"code","source":"# thanks to Pascal Nagel's kernel\ndef recon(reg):\n    if np.isnan(reg):\n        return reg\n    else:\n        integer = int(np.round((40*reg)**2)) # gives 2060 for our example\n        for f in range(28):\n            if (integer - f) % 27 == 0:\n                F = f\n        M = (integer - F)//27\n        return F, M\n# Using the above example to test\nps_reg_03_example = 1.1321312468057179\nprint(\"Federative Unit (F): \", recon(ps_reg_03_example)[0])\nprint(\"Municipality (M): \", recon(ps_reg_03_example)[1])","outputs":[],"execution_count":null},{"metadata":{"_uuid":"1287b1a7fe0b9013f3035d1315dc9bf311ce1f73","collapsed":true,"_cell_guid":"378b8d09-6768-41fc-b4a7-0b731c8d088c"},"cell_type":"code","source":"def prc4(data):\n    data = data.copy()\n    data[\"f1_ps_car_15\"] = 1 / np.exp(data[\"ps_car_15\"])\n    data[\"f2_ps_car_15\"] = (data[\"ps_car_15\"])**2 \n    data['ps_reg_F'] = data['ps_reg_03'].apply(lambda x: recon(x) if np.isnan(x) else recon(x)[0])\n    data['ps_reg_M'] = data['ps_reg_03'].apply(lambda x: recon(x) if np.isnan(x) else recon(x)[1])\n    return data","outputs":[],"execution_count":null},{"metadata":{"_uuid":"7f90cd16441848c813f452565d720f185f852c75","_cell_guid":"97d0c212-9183-4473-be97-3b519a2105e3"},"cell_type":"code","source":"prc_subsample = prc4(subsample)\n\nparams = {\"objective\":\"binary:logistic\", \"max_depth\":1}\nval_scores, model = xgb_feedback(nrounds=100, params=params, prc_subsample=prc_subsample)\n\niter3 = score_summary(val_scores)\n\n# So this will be our initial benchmark\nprint(iter3)\n\nadd_to_iter(iter3, \"iter3\")","outputs":[],"execution_count":null},{"metadata":{"_uuid":"5ad537e9ca94d878da4de2ced9f26c03da0fafbb","_cell_guid":"1b0cbcd4-fee0-455a-8081-52c6661873bc"},"cell_type":"code","source":"iter_performances","outputs":[],"execution_count":null},{"metadata":{"_uuid":"66d9f396e56c674602fa7659a06fe211d3fea9a7","_cell_guid":"e45eb61a-4a40-4278-bd5f-f5c17ef61748"},"cell_type":"code","source":"plot_model(model)","outputs":[],"execution_count":null},{"metadata":{"_uuid":"3d18be8ac2e5ee68e9a8de2e2f99ffad67407e64","_cell_guid":"aeca176e-559d-4df0-b510-7add4324c698"},"cell_type":"markdown","source":"### 5) ITER  4 - [\"ps_ind_06_bin\",\"ps_ind_07_bin\", \"ps_ind_08_bin\", \"ps_ind_09_bin\"] OHE\n\nWe can see that there is only 1 observation of 1 per row...\n\nWe see that these single preprocesses and feature engineering have relative lifts on validation sore"},{"metadata":{"_uuid":"0b017fe00f987b52629d193f03b609eb133ebf2d","_cell_guid":"46e8ec7f-4867-40d2-b47d-47971bcd9158"},"cell_type":"code","source":"sum(subsample[[\"ps_ind_06_bin\",\"ps_ind_07_bin\", \"ps_ind_08_bin\", \"ps_ind_09_bin\"]].sum(axis =1) > 2)","outputs":[],"execution_count":null},{"metadata":{"_uuid":"11ffd5556f67b2fbb56cf89baff899dbaea895f4","collapsed":true,"_cell_guid":"0fbebe90-3c11-486b-91ff-f2b37719fe82"},"cell_type":"code","source":"def prc5(data):\n    data = data.copy()\n    arr = np.array(data[[\"ps_ind_06_bin\",\"ps_ind_07_bin\", \"ps_ind_08_bin\", \"ps_ind_09_bin\"]])\n    data[\"ps_ind_bin_6789\"] = arr.dot(np.array([6, 7, 8, 9]))\n    return data","outputs":[],"execution_count":null},{"metadata":{"_uuid":"d5dd1255de1311d77a0a3b5cfe376d6c7964e6dd","_cell_guid":"d8e329b2-deca-4390-8a44-bafb0619e67d"},"cell_type":"code","source":"prc_subsample = prc5(subsample)\n\nparams = {\"objective\":\"binary:logistic\", \"max_depth\":1}\nval_scores, model = xgb_feedback(nrounds=100, params=params, prc_subsample=prc_subsample)\n\niter4 = score_summary(val_scores)\n\n# So this will be our initial benchmark\nprint(iter4)\n\nadd_to_iter(iter4, \"iter4\")","outputs":[],"execution_count":null},{"metadata":{"_uuid":"dd7241fc09c76c2b94df473df0f9ccbe8404a3ab","_cell_guid":"bb101711-8467-48ef-92d9-300dee987a1c"},"cell_type":"code","source":"iter_performances","outputs":[],"execution_count":null},{"metadata":{"_uuid":"aba8178bce176d26e722bd1e96fc865c5e214f0f","_cell_guid":"99cebd48-fb49-4217-9127-be9ffffc69c0"},"cell_type":"code","source":"plot_model(model)","outputs":[],"execution_count":null},{"metadata":{"_uuid":"0e5ce837af1744cd3f648259001d51e5c6eb7327","_cell_guid":"5f68a1a0-aec6-48d1-8222-213eb49c05db"},"cell_type":"markdown","source":"### MID-WAY PIPE\n\nLet's pipe all of our processes so far except for NAs being = -1."},{"metadata":{"_uuid":"20d86aa11e0c349d5c502d60373a0b707979b7b0","collapsed":true,"_cell_guid":"5be078a7-e651-4022-91e6-12e541997024"},"cell_type":"code","source":"prc_subsample = prc5(prc4(prc3(prc2(subsample))))","outputs":[],"execution_count":null},{"metadata":{"_uuid":"e452291c25dee9e583aa2853607a1e0e71d0eec7","_cell_guid":"1677bc17-a667-463e-a583-5e6bd1081f78"},"cell_type":"code","source":"params = {\"objective\":\"binary:logistic\", \"max_depth\":1}\nval_scores, model = xgb_feedback(nrounds=100, params=params, prc_subsample=prc_subsample)\n\nmid_score = score_summary(val_scores)\n\n# So this will be our initial benchmark\nprint(mid_score)\n\nadd_to_iter(mid_score, \"mid_way_score\")","outputs":[],"execution_count":null},{"metadata":{"_uuid":"1505a1da28609717e65ddda99434e5c7c4162a33","_cell_guid":"c0b54993-85ff-4715-b7f8-f933aab584e4"},"cell_type":"code","source":"iter_performances","outputs":[],"execution_count":null},{"metadata":{"_uuid":"7761c4ed878e18cb2da272c41b16662cbffc76a1","_cell_guid":"aacc9984-c216-48c8-b9da-b115ea8a4f14"},"cell_type":"code","source":"plot_model(model)","outputs":[],"execution_count":null},{"metadata":{"_uuid":"8a8b4ae941e990daef21bf43b5805a35d18a85c3","collapsed":true,"_cell_guid":"690c1d65-d6de-45bf-9c75-5d6a6262c06f"},"cell_type":"code","source":"def midway_prc(data):return prc5(prc4(prc3(prc2(data))))","outputs":[],"execution_count":null},{"metadata":{"_uuid":"8df064348ebd2e453d20df11c218edb333238e81","_cell_guid":"dfe28b42-a3d8-48e2-ab50-4991f64499ff"},"cell_type":"markdown","source":"### CONTINUE HERE\n\n- Do different types of encoding on important features\n- Check interactions by 2-split subsample trees\n- Check Ordinality\n- Check OHE effect"},{"metadata":{"_uuid":"02e5f0f94e7989962e0d4790f1e970afd260fa50","_cell_guid":"89de24c4-cb15-4f41-a5b5-aa5292548ee9"},"cell_type":"markdown","source":"### SUBMISSION \n\nLet's make a submission to see how well we are doing so far with all the help of others...\n\nWe will not tune our parameters superbly since there is way much more things to do...\n\nSo far we explored stuff on subsample but it's not coincidence that we get better results on whole data since more data means better results. No proper tuning with a fast and dirty approach gives ~0.69 in LB"},{"metadata":{"_uuid":"23fde8de4e892cdac945432ac0a8ec419825ee42","collapsed":true,"_cell_guid":"67b94c10-b7bb-4f00-97db-f0ffa03fdaee"},"cell_type":"raw","source":"data = midway_prc(dtrain)"},{"metadata":{"_uuid":"e80673cd41cdddf7271c3fe628b924f0261c36b7","_cell_guid":"60ea62a2-b0e4-4d0c-a854-985f0647b4aa"},"cell_type":"raw","source":"params = {\"objective\":\"binary:logistic\", \"max_depth\":m, \"eta\":0.1}\nval_scores, model = xgb_feedback(nrounds=95, params=params, prc_subsample=data)\nprint(f'param {m}, avg val {np.mean(val_scores)}')"},{"metadata":{"_uuid":"ce79489da20daf982026a34759b219b4d0b49f2c","collapsed":true,"_cell_guid":"747fa4d7-b12d-4914-b7da-5cbbad8df51b"},"cell_type":"raw","source":"data = midway_prc(dtrain)\ntest = midway_prc(dtest)"},{"metadata":{"_uuid":"46ec495e8c56188a14fad72a0f13c3f65a6485d4","collapsed":true,"_cell_guid":"8ef0a4ca-aa04-4da7-a0a2-ae42e0bd4849"},"cell_type":"raw","source":"params = {\"objective\":\"binary:logistic\", \"max_depth\":3, \"eta\":0.1, \"num_rounds\":95}"},{"metadata":{"_uuid":"7da37c113ba43ddd5919da4d49a45ac86d2f1ca6","collapsed":true,"_cell_guid":"41beaaa1-aee1-439f-b74f-df7613eb22c5"},"cell_type":"raw","source":"def xgb_submission(params, train, test):\n    X_train, y_train = train.drop(['id', 'target'], axis=1), train.target\n    X_test = test.drop(['id'], axis=1)\n\n    #create dmatrix\n    dtrain = xgb.DMatrix(data=X_train, label=y_train, missing= np.nan)\n    dtest = xgb.DMatrix(data=X_test, missing= np.nan)\n\n    #train\n    model = xgb.train(params, dtrain, num_boost_round=95)\n    preds = model.predict(dtest)\n    return model, preds"},{"metadata":{"_uuid":"45117ac7d339381e40c9677bc5cf9ce4330f830d","_cell_guid":"a691190e-c387-433e-8363-f515a789af70"},"cell_type":"raw","source":"params"},{"metadata":{"_uuid":"c9ec5568fd3d07bebdc065dd5377d830503ac745","_cell_guid":"b3ecd22f-f7ba-4b78-b8e3-26a13516bd50"},"cell_type":"raw","source":"model, preds = xgb_submission(params, data, test)"},{"metadata":{"_uuid":"6c168e77b084d8b1b892950981b1277d759c9d75","_cell_guid":"93e26a59-28de-47e1-8a25-d64a92300355"},"cell_type":"raw","source":"plot_model(model)"}],"nbformat_minor":1}