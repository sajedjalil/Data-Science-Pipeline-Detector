{"cells":[{"metadata":{"_cell_guid":"02a9d983-f0db-4b01-935c-20ab3e693aa0","_uuid":"9715cd3ad2a5f0845131bb300787861a16c3100a"},"cell_type":"markdown","source":"**Non-Linear vs Linear Models**\n\nThe aim of this competition was to predict if a driver will file an insurance claim next year.  This kernel summarizes the solutions of the best available kernels based on non-linear and linear methods. It compares the prediction accuracy of the most successful machine learning predictors gradient boosting and neural nets with Generalized Linear Models (GLM). Finally it is shown how to install the required software on a windows PC. \n\n\nA) Top models: 2nd place simplified solution (links)\n\nB) GLM: Logistic Regression\n\nC) Run locally: Installation on a windows PC\n"},{"metadata":{"_cell_guid":"685c5f0b-841b-4682-9614-b78d20436096","_uuid":"af25bab4a8a02f7b0ba72103ec9f2c16888a1d8d"},"cell_type":"markdown","source":"**A)   Top Models: 2nd place simplified solution (links)**\n\nXiaozhou Wang (\"Little Boat\") has generously posted the 2nd place solution of his team: \nhttps://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/44558\n\nThey presented a very interesting simplified version of their python code on kaggle and github: \nhttps://github.com/xiaozhouwang/kaggle-porto-seguro/tree/master/code\n\nOverview on simplified solution:\n![Simplified Solution](https://i.imgur.com/AznmQA9.jpg)\n\nTheir best models are based on gradient boosting and neural nets:\n* nn_model290.py to get a nn model that scores 0.290X\n* gbm_model291.py to get a gbm model that scores 0.291X\n* The blend of this two models would have been good enough for second place\n\nCurrently the posted scripts \"2nd place solution NN model\" and \"2nd Place Lightgbm Solution\" (v1) don't run as kernels.  Despite that a slightly modified version of the lightgbm solution can be studied here:\nhttps://www.kaggle.com/floser/study-2nd-place-lightgbm-solution\n\nThe following modifications have been made:\n* Some minor syntax adaptions to Python 3\n* Some print statements to show what's going on\n* GBM with much less rounds and samples to get a (worse) solution in a few minutes\n\nThe nn model is beyond the current resources of kernels. Therefore, C) shows how to install the necessary software to run the scripts locally on a Windows PC."},{"metadata":{"collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"},"cell_type":"markdown","source":"**B) Logistic Regression**\n\nBased on Sudhir Kumar's kernel \"Simple logistic model - PORTO\", Nov. 2017 \nhttps://www.kaggle.com/sudhirnl7/simple-logistic-model-porto\n\nSteps:\n1.\tRead data and explore (very briefly)\n2.\tClean and modify data\n3.\tPrepare features for modeling\n4.\tRegularized logistic regression\n"},{"metadata":{"collapsed":true,"_cell_guid":"0d899e2e-56f5-4861-bf38-60ac65b3b7e1","_uuid":"80bb295d586b3bad7de8b043288a3a732bcbee93","trusted":false},"cell_type":"code","source":"# Import libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, roc_auc_score ,roc_curve,auc\nfrom sklearn.model_selection import StratifiedKFold,GridSearchCV\nseed =123\n% matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"73848f97-b6eb-4414-a8e2-9fb59953bcdb","_uuid":"e4f4210544007902b09999e8f98950ff3b2f5aba"},"cell_type":"markdown","source":"1 - Read data and explore"},{"metadata":{"collapsed":true,"_cell_guid":"ce927de8-ed4d-4410-b82c-b93476fb3cb5","_uuid":"ba966508a5d5f7065f8b94db48d774b60ee686cd","trusted":false},"cell_type":"code","source":"# Read data sets, set missings, show dimensions\npath = '../input/'\ntrain = pd.read_csv(path+'train.csv',na_values=-1)\ntest = pd.read_csv(path+'test.csv',na_values=-1)\nprint('Number of rows and columns:',train.shape)\nprint('Number of rows and columns:',test.shape)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"f0f0728f-8b90-43ab-90e8-684b5100d17b","_uuid":"4b76d4c2f168267b07793e1a2c879351c96d90d0","trusted":false},"cell_type":"code","source":"# Let's inspect the first data lines visually\ntrain.head(9).T","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"49eee6e6-1d87-46e1-bb18-4faaf46840be","_uuid":"f40a764e09ee6c9843fadcc71d2a7474dbd11c29","trusted":false},"cell_type":"code","source":"# Explore the target variable\nplt.figure(figsize=(10,3))\nsns.countplot(train['target'],palette='rainbow')\nplt.xlabel('Target')\ntrain['target'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"03ddba4b-b556-4ebc-9e5f-9bb46d6370d4","_uuid":"b518375c4a516e3293e5a0c81b85e3727242b75b","trusted":false},"cell_type":"code","source":"# Plot correlations\ncor = train.corr()\nplt.figure(figsize=(12,9))\nsns.heatmap(cor,cmap='RdBu')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4b41e96e-17e7-494d-934c-d9a7aa9e4f7b","_uuid":"447a86d013bc464d98ae29789b864feb5c078421"},"cell_type":"markdown","source":"The ps_calc*-features are perfectly uncorrelated. As stated in other kernels/discussions they seem to be useless for prediction and will be excluded."},{"metadata":{"_cell_guid":"2c69d2c0-ea23-4768-946c-55dfeb2ff82e","_uuid":"7e821500836708856a795bd04703eb7e8c8df1a5"},"cell_type":"markdown","source":"2 - Clean and modify data"},{"metadata":{"collapsed":true,"_cell_guid":"96db8545-081a-4407-a3b9-a40c1392764a","_uuid":"49fa59c70610d6911e2899c9131ad3294aaabeca","trusted":false},"cell_type":"code","source":"# Drop ps_calc* variables (see above)\nps_cal = train.columns[train.columns.str.startswith('ps_calc')] \ntrain = train.drop(ps_cal,axis =1)\ntest = test.drop(ps_cal,axis=1)\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"ae229751-3706-4333-80b0-8cd26ec09c87","_uuid":"6ff46a708ba952c8576e1e2888b19595ff9bc90b","trusted":false},"cell_type":"code","source":"# Plot missing values of train and test data \nk= pd.DataFrame()\nk['train']= train.isnull().sum()\nk['test'] = test.isnull().sum()\nfig,ax = plt.subplots(figsize=(16,5))\nk.plot(kind='bar',ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"936a72b7-028b-4256-bee9-e2a11b8efa0d","_uuid":"65fc1f6259b4f27bbb75b55110f79229894fd0be","trusted":false},"cell_type":"code","source":"# Replace missing with mode\ndef missing_value(df):\n    col = df.columns\n    for i in col:\n        if df[i].isnull().sum()>0:\n            df[i].fillna(df[i].mode()[0],inplace=True)\n\nmissing_value(train)\nmissing_value(test)\n\n# Count categories \ndef basic_details(df):\n    b = pd.DataFrame()\n    #b['Missing value'] = df.isnull().sum()\n    b['N unique value'] = df.nunique()\n    b['dtype'] = df.dtypes\n    return b\nbasic_details(train)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"collapsed":true,"_cell_guid":"8f70833f-6f55-4a0d-974c-ca7177a65e1d","_uuid":"fa195ec8f7e148ec3b5abcb7cd7690c26bddb4b8","trusted":false},"cell_type":"code","source":"# change data type to category (if not more than 104 categories, see ps_car_11_cat*egory)\ndef category_type(df):\n    col = df.columns\n    for i in col:\n        if df[i].nunique()<=104:\n            df[i] = df[i].astype('category')\ncategory_type(train)\ncategory_type(test)\n#basic_details(train)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"06419d6f-8ed3-4bc4-8cb3-cdc013ced85c","_uuid":"e8eb9ee57e9ac89d24e5fbad8259494a0dcab002","trusted":false},"cell_type":"code","source":"# generate some variables lists\ncat_col = [col for col in train.columns if '_cat' in col]\n#print(cat_col)\nbin_col = [col for col in train.columns if 'bin' in col]\n#print(bin_col)\ntot_cat_col = list(train.select_dtypes(include=['category']).columns)\n#print(tot_cat_col)\nother_cat_col = [c for c in tot_cat_col if c not in cat_col+ bin_col]\nother_cat_col","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b0947b1e-fd18-404f-b032-81ee715b19fd","_uuid":"d2ce1645eaf9f3eadf5e451af15bbc0755a0097b"},"cell_type":"markdown","source":"3 - Prepare features for modeling"},{"metadata":{"collapsed":true,"_cell_guid":"4832ff52-2b4a-40b5-82ea-52f4426d19fb","_uuid":"585f2f3c81c2e8a5dbb890331988d42198bfa54c","trusted":false},"cell_type":"code","source":"# Calculate Median and mean for categorical data\ndef transform_df(df):\n    df = pd.DataFrame(df)\n    dcol= [c for c in train.columns if train[c].nunique()>2]\n    dcol.remove('id')   \n    d_median = df[dcol].median(axis=0)\n    d_mean = df[dcol].mean(axis=0)\n    q1 = df[dcol].apply(np.float32).quantile(0.25)\n    q2 = df[dcol].apply(np.float32).quantile(0.5)\n    q3 = df[dcol].apply(np.float32).quantile(0.75)\n    \n    #Add mean and median column to data set having more then 2 categories\n    for c in dcol:\n        df[c+str('_median_range')] = (df[c].astype(np.float32).values > d_median[c]).astype(np.int8)\n        df[c+str('_mean_range')] = (df[c].astype(np.float32).values > d_mean[c]).astype(np.int8)\n        df[c+str('_q1')] = (df[c].astype(np.float32).values < q1[c]).astype(np.int8)\n        df[c+str('_q2')] = (df[c].astype(np.float32).values < q2[c]).astype(np.int8)\n        df[c+str('_q3')] = (df[c].astype(np.float32).values > q3[c]).astype(np.int8)\n    return df\n\ntrain = transform_df(train)\ntest = transform_df(test)\nbasic_details(train)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"collapsed":true,"_cell_guid":"5d1b6958-2b9b-41c3-98e0-4ca25a2638e1","_uuid":"ebc7dd8687e29301d4d0c7cad60a9824fb791c24","trusted":false},"cell_type":"code","source":"# Correlation plot of the numeric or \"many level\" features (for ideas about their meaning have a look in discussions)\nnum_col = ['ps_reg_03','ps_car_12','ps_car_13','ps_car_14']\ncor = train[num_col].corr()\nplt.figure(figsize=(10,4))\nsns.heatmap(cor,annot=True)\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"collapsed":true,"_cell_guid":"c589c067-dd99-475e-bb43-0bca9219df8e","_uuid":"a1bd52c3b74f19434d042ea07cee478955cd37b1","trusted":false},"cell_type":"code","source":"# Determine outliers in dataset\ndef outlier(df,columns):\n    for i in columns:\n        quartile_1,quartile_3 = np.percentile(df[i],[25,75])\n        quartile_f,quartile_l = np.percentile(df[i],[1,99])\n        IQR = quartile_3-quartile_1\n        lower_bound = quartile_1 - (1.5*IQR)\n        upper_bound = quartile_3 + (1.5*IQR)\n        print(i,lower_bound,upper_bound,quartile_f,quartile_l)\n                \n        df[i].loc[df[i] < lower_bound] = quartile_f\n        df[i].loc[df[i] > upper_bound] = quartile_l\n        \noutlier(train,num_col)\noutlier(test,num_col) ","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"4717c6e7-e784-4548-9e1d-2131ed12802c","_uuid":"f21830368991897bd8f344c8ba2f038ef871374a","trusted":false},"cell_type":"code","source":"# One Hot Encoding (generate dummy variables for categories)\ndef OHE(df1,df2,column):\n    cat_col = column\n    #cat_col = df.select_dtypes(include =['category']).columns\n    len_df1 = df1.shape[0]\n    \n    df = pd.concat([df1,df2],ignore_index=True)\n    c2,c3 = [],{}\n    \n    print('Categorical feature',len(column))\n    for c in cat_col:\n        if df[c].nunique()>2 :\n            c2.append(c)\n            c3[c] = 'ohe_'+c\n    \n    df = pd.get_dummies(df, prefix=c3, columns=c2,drop_first=True)\n\n    df1 = df.loc[:len_df1-1]\n    df2 = df.loc[len_df1:]\n    print('Train',df1.shape)\n    print('Test',df2.shape)\n    return df1,df2\n\ntrain1,test1 = OHE(train,test,tot_cat_col)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"c4e65396-f42c-4cb6-88e8-08c8574bc452","_uuid":"f93e65a6969773236a334a57d801785b0c871265","trusted":false},"cell_type":"code","source":"# Split data set\nX = train1.drop(['target','id'],axis=1)\ny = train1['target'].astype('category')\nx_test = test1.drop(['target','id'],axis=1)\ndel train1,test1","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b17cf982-47e6-4878-b757-7f5eec3a87f4","_uuid":"71509b4a78fe99083241cfb1ff8ee9e93c26d320"},"cell_type":"markdown","source":"4 - Regularized logistic regression"},{"metadata":{"collapsed":true,"_cell_guid":"dd299b20-c4a3-4988-b180-ecae906b9b49","_uuid":"363a233c1a54519726ca57ad88ee8075b797b556","trusted":false},"cell_type":"code","source":"# Logistic regression model (L2-penalty), 5-fold CV, C determined by grid-search\nkf = StratifiedKFold(n_splits=5,random_state=seed,shuffle=True)\npred_test_full=0\ncv_score=[]\ni=1\nfor train_index,test_index in kf.split(X,y):    \n    print('\\n{} of kfold {}'.format(i,kf.n_splits))\n    xtr,xvl = X.loc[train_index],X.loc[test_index]\n    ytr,yvl = y[train_index],y[test_index]\n    \n    lr = LogisticRegression(class_weight='balanced',C=0.003)\n    lr.fit(xtr, ytr)\n    pred_test = lr.predict_proba(xvl)[:,1]\n    score = roc_auc_score(yvl,pred_test)\n    print('roc_auc_score',score)\n    cv_score.append(score)\n    pred_test_full += lr.predict_proba(x_test)[:,1]\n    i+=1","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"ca0b8032-bc84-494a-baad-2401c4c67696","_uuid":"7129b7534d791dc88a204b62591c6a51fff85c96","trusted":false},"cell_type":"code","source":"# Model performance\nprint('Confusion matrix\\n',confusion_matrix(yvl,lr.predict(xvl)))\nprint('Cv',cv_score,'\\nMean cv Score',np.mean(cv_score))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"16085bfb-1b1b-4a8e-8eb2-606525a0fa48","_uuid":"a2483e6baebd0dc68573432da6b3f9065b116d70","trusted":false},"cell_type":"code","source":"# Area Under ROC-Curve (Normalized gini = 2*AUC-1)\nproba = lr.predict_proba(xvl)[:,1]\nfpr,tpr, threshold = roc_curve(yvl,proba)\nauc_val = auc(fpr,tpr)\n\nplt.figure(figsize=(14,8))\nplt.title('Area Under ROC-Curve')\nplt.plot(fpr,tpr,'b',label = 'AUC = %0.2f' % auc_val)\nplt.legend(loc='lower right')\nplt.plot([0,1],[0,1],'r--')\nplt.ylabel('True positive rate')\nplt.xlabel('False positive rate')","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"664de51d-9cbf-458b-9db5-de4f85d74b78","_uuid":"8632728815ffa597ee8b245ebb41841a8b804b74","trusted":false},"cell_type":"code","source":"# Make prediction and create submission file\ny_pred = pred_test_full/5\nsubmit = pd.DataFrame({'id':test['id'],'target':y_pred})\nsubmit.to_csv('lr_porto.csv',index=False) ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0fe53f24-d69b-4eb6-a239-3c685a54889b","_uuid":"b90d0796397984ac5eae2f1d5bab7151170c2ac5"},"cell_type":"markdown","source":"This scores 0.275 on private Leader Board (~ place 3500) and is quite good for a linear model. "},{"metadata":{"_cell_guid":"79ad866b-5eae-49b3-9d1e-77f769031fa2","_uuid":"7e8fee4c67e964e60fbd870745b81f1e8e9b2da2"},"cell_type":"markdown","source":"================================================================="},{"metadata":{"_cell_guid":"647b2e0f-f460-4ce9-8e1f-714fb6d8e4de","_uuid":"c5327323b09fa805378481710e1ca1bf0e2a0fcc"},"cell_type":"markdown","source":"**C) Run locally: Installation on a windows PC**\n\nTo run this codes locally on a windows PC with Python and Jupyter Notebook you just need to follow these instructions:\n\n1. Download Anaconda 5.1 for Windows 64bit, \nhttps://www.anaconda.com/download/ (0.5 GB)\n\n2. Open “Anaconda Prompt” and execute (remove :"},{"metadata":{"collapsed":true,"_cell_guid":"faf7f8ae-afea-4c88-a923-e54c607acf92","_uuid":"f06b10cf5e83a43ac0d02e6ab70ad305d44c65a4","trusted":false},"cell_type":"code","source":"#conda install jupyter\n#conda install scipy\n#pip install sklearn\n#pip install pandas\n#pip install pandas-datareader\n#pip install matplotlib\n#pip install pillow\n#pip install requests\n#pip install h5py\n#pip install tensorflow==1.4.0\n#pip install keras==2.1.2","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7ee30-f008-4c18-bbc5-c16bee54c867","_uuid":"3c93a929129346d17ef0df8e65e392e85fd16b13"},"cell_type":"markdown","source":"Additionally: Gradient Boosting (very useful, see kaggle.com)"},{"metadata":{"collapsed":true,"_cell_guid":"c5f139ad-580f-41e7-81f2-864203939898","_uuid":"be5a519a49a320df9a4cef4b446c41ad12204470","trusted":false},"cell_type":"code","source":"#conda install –c rdonnelly py-xgboost\n#conda install –c conda-forge lightgbm","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d250bb30-dd63-44cf-847f-8ad237cf2025","_uuid":"f4f1fbef8f3d0a5e3fd01b69cd0170853a6d1c22"},"cell_type":"markdown","source":"3. Test: Open jupyter notebook and execute:"},{"metadata":{"_cell_guid":"4f2f5f47-cf9b-4739-9810-1c0df6335b96","_uuid":"41381167bb2292333d2aa6bd616a5c3d5fda746e"},"cell_type":"markdown","source":"\t#import keras\n\t#Import tensorflow\n\t#import xgboost\n\t#import lightgbm"},{"metadata":{"_uuid":"3673f789fb129cd2e973022b2be8071f1ff5bf64"},"cell_type":"markdown","source":"This did work perfectly in March 2018. In case of issues please check for newer versions."},{"metadata":{"_cell_guid":"671841e4-f03f-4ec7-8154-8e9bb0b48714","_uuid":"ca3b52397575e718799fcc45326dcf198c22c9a1"},"cell_type":"markdown","source":"\n\nSource and recommendation:\n\ni) Jeff Heaton’s Video “Installing TensorFlow, Keras, and Python in Windows”, Nov., 2017.  \nhttps://www.youtube.com/watch?v=z0qhKP2liHs\n\nii) and his course about Deep Learning:\n\nhttps://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class1_intro_python.ipynb .   At the end you will find the installation script."}],"metadata":{"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}