{"cells":[{"metadata":{"_uuid":"7d8c6ba1fb81f0dbf465ca35dca41295db19bf03","_cell_guid":"5e03764a-08c4-4168-8b09-436447840765"},"cell_type":"markdown","source":"**Study 2nd-place lightgbm solution**\n\nThis Kernel ist based on the code posted by Xiaozhou Wang presenting the 2nd place solution of his team:  \nhttps://www.kaggle.com/xiaozhouwang/2nd-place-lightgbm-solution\n\nYou can find more information about the solution here: https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/44558\n\nCurrently the posted script \"2nd place solution NN model\" (version 1) does not run. Below following modifications were made:\n- Some minor syntax adaptions to Python 3\n- Some print statements to show what's going on\n- GBM with much less rounds and samples to get a (worse) solution in a few minutes  \n"},{"metadata":{"collapsed":true,"_uuid":"88890bebc393b7ac92e11ad7dc6404a1ff6d7eaa","_cell_guid":"9c71b36c-f6b3-4d6f-93b4-b908c6f28fa2","trusted":false},"cell_type":"code","source":"import lightgbm as lgbm\nfrom scipy import sparse as ssp\nfrom sklearn.model_selection import StratifiedKFold\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\n","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"f2444223d44c7ff7c89be70b9c7e3c829f225fd8","_cell_guid":"83b1c1ba-dd5f-4b85-a03f-762c45a932cc","trusted":false},"cell_type":"code","source":"def Gini(y_true, y_pred):\n    # check and get number of samples\n    assert y_true.shape == y_pred.shape\n    n_samples = y_true.shape[0]\n\n    # sort rows on prediction column\n    # (from largest to smallest)\n    arr = np.array([y_true, y_pred]).transpose()\n    true_order = arr[arr[:, 0].argsort()][::-1, 0]\n    pred_order = arr[arr[:, 1].argsort()][::-1, 0]\n\n    # get Lorenz curves\n    L_true = np.cumsum(true_order) * 1. / np.sum(true_order)\n    L_pred = np.cumsum(pred_order) * 1. / np.sum(pred_order)\n    L_ones = np.linspace(1 / n_samples, 1, n_samples)\n\n    # get Gini coefficients (area between curves)\n    G_true = np.sum(L_ones - L_true)\n    G_pred = np.sum(L_ones - L_pred)\n\n    # normalize to true Gini coefficient\n    return G_pred * 1. / G_true\n\ncv_only = True\nsave_cv = True\nfull_train = False\n\ndef evalerror(preds, dtrain):\n    labels = dtrain.get_label()\n    return 'gini', Gini(labels, preds), True","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"4ac5f31ce172e66f1a1cbaf2da102706f2ca9262","_cell_guid":"789e9567-1494-40ac-b929-725136a19e4e","trusted":false},"cell_type":"code","source":"path = \"../input/\"\ntrain = pd.read_csv(path+'train.csv')\ntrain_label = train['target']\ntrain_id = train['id']\ntest = pd.read_csv(path+'test.csv')\ntest_id = test['id']\ntrain.head(10).T #f","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5590007e7073cb3582512fb9819f02e41e6ae786","_cell_guid":"37dd29bd-9058-4575-b963-b88f43c6e353"},"cell_type":"markdown","source":"Feature Engineering:"},{"metadata":{"collapsed":true,"_uuid":"b45fa78f01373a5f64c5c11da8e5faf6f2c90cae","_cell_guid":"2e49f694-e6ba-4cd2-834e-e5ab1f8b555d","trusted":false},"cell_type":"code","source":"NFOLDS = 5\n\nkfold = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=218)\ny = train['target'].values\ndrop_feature = [\n    'id',\n    'target'\n]\n\nX = train.drop(drop_feature,axis=1)\nfeature_names = X.columns.tolist()\ncat_features = [c for c in feature_names if ('cat' in c and 'count' not in c)]\nnum_features = [c for c in feature_names if ('cat' not in c and 'calc' not in c)]\n\ntrain['missing'] = (train==-1).sum(axis=1).astype(float)\ntest['missing'] = (test==-1).sum(axis=1).astype(float)\nnum_features.append('missing')\n\nfor c in cat_features:\n    le = LabelEncoder()\n    le.fit(train[c])\n    train[c] = le.transform(train[c])\n    test[c] = le.transform(test[c])\n\nenc = OneHotEncoder()\nenc.fit(train[cat_features])\nX_cat = enc.transform(train[cat_features])\nX_t_cat = enc.transform(test[cat_features])\n\nind_features = [c for c in feature_names if 'ind' in c]\n\ncount=0\nfor c in ind_features:\n    if count==0:\n        train['new_ind'] = train[c].astype(str)+'_'\n        test['new_ind'] = test[c].astype(str)+'_'\n        count+=1\n    else:\n        train['new_ind'] += train[c].astype(str)+'_'\n        test['new_ind'] += test[c].astype(str)+'_'\n\ncat_count_features = []\n\nfor c in cat_features+['new_ind']:\n    d = pd.concat([train[c],test[c]]).value_counts().to_dict()\n    train['%s_count'%c] = train[c].apply(lambda x:d.get(x,0))\n    test['%s_count'%c] = test[c].apply(lambda x:d.get(x,0))\n    cat_count_features.append('%s_count'%c)\n\ntrain_list = [train[num_features+cat_count_features].values,X_cat,]\ntest_list = [test[num_features+cat_count_features].values,X_t_cat,]\n\nX = ssp.hstack(train_list).tocsr()\nX_test = ssp.hstack(test_list).tocsr()\n\ntrain.head(3).T #f","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3af0fd3778d49b227c86b02d08386372fbd7d430","_cell_guid":"0c531230-8fda-4c49-9f51-9121d24ff58d"},"cell_type":"markdown","source":"Training (lightgbm) and predictions:"},{"metadata":{"collapsed":true,"_uuid":"c840cce6d7df56e78d794f56ba11e956a65b3b0e","_cell_guid":"4a637d4a-8f29-4bdb-af15-c88577ebd4d0","trusted":false},"cell_type":"code","source":"#f mod: Py2 to Py3, some print(), xrange() -> range()\n# Runs hours with original parameters. Try getting it down to 10 min (locally)\n# Original parameters: num_boost_round = 10000, for s in range(16):, verbose_eval=100\n\nimport time             #f\ntic = time.time()       #f start time\n\nlearning_rate = 0.05 #f\nnum_leaves = 15\nmin_data_in_leaf = 2000\nfeature_fraction = 0.6\nnum_boost_round  = 1000   #f\n\nparams = {\"objective\": \"binary\",\n          \"boosting_type\": \"gbdt\",\n          \"learning_rate\": learning_rate,\n          \"num_leaves\": num_leaves,\n          \"max_bin\": 256,\n          \"feature_fraction\": feature_fraction,\n          \"verbosity\": 0,\n          \"drop_rate\": 0.1,\n          \"is_unbalance\": False,\n          \"max_drop\": 50,\n          \"min_child_samples\": 10,\n          \"min_child_weight\": 150,\n          \"min_split_gain\": 0,\n          \"subsample\": 0.9\n          }\n\nx_score = []\n\nfinal_cv_train = np.zeros(len(train_label))\nfinal_cv_pred = np.zeros(len(test_id))\n\n\n\nfor s in range(4 ):     #f\n    cv_train = np.zeros(len(train_label))\n    cv_pred = np.zeros(len(test_id))\n    params['seed'] = s\n\n    print(\"S = \",s)     #\n \n    if cv_only:\n        kf = kfold.split(X, train_label)\n        best_trees = []\n        fold_scores = []\n        for i, (train_fold, validate) in enumerate(kf):\n            X_train, X_validate, label_train, label_validate = \\\n                X[train_fold, :], X[validate, :], train_label[train_fold], train_label[validate]\n            dtrain = lgbm.Dataset(X_train, label_train)\n            dvalid = lgbm.Dataset(X_validate, label_validate, reference=dtrain)\n            bst = lgbm.train(params, dtrain, num_boost_round, valid_sets=dvalid, feval=evalerror, verbose_eval=50 ,\n                            early_stopping_rounds=100)\n            best_trees.append(bst.best_iteration)\n            cv_pred += bst.predict(X_test, num_iteration=bst.best_iteration)\n            cv_train[validate] += bst.predict(X_validate)\n\n            score = Gini(label_validate, cv_train[validate])\n            print(score)\n            fold_scores.append(score)\n\n        cv_pred /= NFOLDS\n        final_cv_train += cv_train\n        final_cv_pred += cv_pred\n\n        print(\"cv score:\")\n        print(Gini(train_label, cv_train))\n        print(\"current score:\", Gini(train_label, final_cv_train / (s + 1.)), s+1)\n        print(fold_scores)\n        print(best_trees, np.mean(best_trees))\n        x_score.append(Gini(train_label, cv_train))\nprint(\" x_score = \",x_score)                         #f \nprint(\"time in seconds: \", time.time() - tic)        #f\n\n#pd.DataFrame({'id': test_id, 'target': final_cv_pred / 16.}).to_csv('lgbm3_pred_avg.csv', index=False)\n#pd.DataFrame({'id': train_id, 'target': final_cv_train / 16.}).to_csv('lgbm3_cv_avg.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","mimetype":"text/x-python","name":"python","file_extension":".py","codemirror_mode":{"name":"ipython","version":3}},"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"}},"nbformat":4,"nbformat_minor":4}