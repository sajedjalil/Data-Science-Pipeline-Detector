{"nbformat_minor":1,"nbformat":4,"cells":[{"cell_type":"markdown","source":"1.  Introduction\n    1. My Goal\n    2. Reference\n2.  Data Understanding\n    1. Data check\n    2. Find Null data\n    3. Feature analysis\n    4. Predict null data based on statistical method\n    5. One hot encoding\n3. Prediction with xgboost\n    1. Using stratified shuffle split\n4. Things to do","metadata":{"_cell_guid":"e1b3f784-66a9-47d7-bbb1-b05dd5d1eff8","_uuid":"2e4092479b0e8b39ec9b94b1d25831ab923cfefd"}},{"cell_type":"markdown","source":"# 1. Introduction\n- In this competition, we need to predict insurance claim. \n- This competition contains classification problem.\n- Metric is gini-coefficient.","metadata":{"_cell_guid":"6d109c03-c64a-42f3-be8a-9937589b4f09","_uuid":"d2f74865126b6ac4f817e168da840d6847f19704"}},{"cell_type":"markdown","source":"### 1.1 My Goal\n- In this competition, I'm going to write my first kernel.\n- And I wish my kernel will help someone who is beginner like me!\n- If I can, I want to get a high rank.","metadata":{"_cell_guid":"0073e775-4d7d-4b72-9a38-12f3dcc9e2f3","_uuid":"ad6ee016c1e6ab7e43d95915fbea925930b068a0"}},{"cell_type":"markdown","source":"## 1.1.1  Reference\n- I used a few feature extraction ideas from [Keui Shen Nong's work](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/kernels?sortBy=votes&group=everyone&pageSize=20&language=all&outputType=all&competitionId=7082&startRow=0). His work is nice!!\n- [Anokas's work](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/kernels?sortBy=votes&group=everyone&pageSize=20&language=all&outputType=all&competitionId=7082&startRow=0) made me to start this competition! He is fast!","metadata":{"_cell_guid":"c1176149-ca14-4c8a-853a-f2829932cdb6","_uuid":"c9ad5d6b02586729f8bd4ceb20b213e71f30ef5f"}},{"cell_type":"code","outputs":[],"source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib\n%matplotlib inline\n\nimport seaborn as sns # visualization\nimport missingno as msno\n\nimport xgboost as xgb # Gradient Boosting\nimport warnings\nsns.set(style='white', context='notebook', palette='deep')\n\n# warnings.filterwarnings(\"ignore\")\n# Any results you write to the current directory are saved as output","execution_count":null,"metadata":{"_cell_guid":"c31205bd-8e5f-40b2-b2c1-d9b005727ba6","collapsed":true,"_uuid":"294daef1d80fb982bfe18517becefb22f19319ca"}},{"cell_type":"markdown","source":"# 2. Data understanding","metadata":{"_cell_guid":"2e769d14-58fb-4750-af0f-dd211ae3ab14","_uuid":"f565564b41d1dd847448353cdd63b81f32106208"}},{"cell_type":"code","outputs":[],"source":"np.random.seed(1989)\ntrain = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")\nprint(\"Train shape : \", train.shape)\nprint(\"Test shape : \", test.shape )","execution_count":null,"metadata":{"_cell_guid":"5284e3ef-3e37-48be-9eb0-f0bb50fd80a6","collapsed":true,"_uuid":"701f6b376051d3537407e6386a3871e9d9ebf627"}},{"cell_type":"code","outputs":[],"source":"train.head()","execution_count":null,"metadata":{"_cell_guid":"65713ea7-0c8e-43da-bd1c-fd0ef6345014","collapsed":true,"_uuid":"8c3a11c01ccd7502ca308afa2f8684a560050d65"}},{"cell_type":"code","outputs":[],"source":"print(train.info())","execution_count":null,"metadata":{"_cell_guid":"29b1e1e0-a7c0-4ab7-9a30-c839633c740e","_kg_hide-output":true,"collapsed":true,"_uuid":"269784136524338962e5444bb1d15f057ea16387"}},{"cell_type":"code","outputs":[],"source":"print(test.info())","execution_count":null,"metadata":{"_cell_guid":"d3099e51-fc55-43f1-98bc-c600387e6f6f","_kg_hide-output":true,"collapsed":true,"_uuid":"410c7e0fd087ae6e81d5247c939cf4abe6e95ca7"}},{"cell_type":"markdown","source":"### My thought for data understanding \nAs you can see, the number of feature is 57. But, the name of each feature don't say what the feature exactly means. So we really need to stand on the computational and statistical method.","metadata":{"_cell_guid":"6cfdd6aa-62d1-4571-8e27-817cad5dba03","_uuid":"e21f07f1b075f2a1001df077a588dc509012ff9c"}},{"cell_type":"code","outputs":[],"source":"targets = train['target'].values","execution_count":null,"metadata":{"_cell_guid":"190f6998-521a-493a-9420-c850700d754c","collapsed":true,"_uuid":"bbbf740e89e040dae4def684d64f216a2f199880"}},{"cell_type":"code","outputs":[],"source":"sns.set(style=\"darkgrid\")\nax = sns.countplot(x = targets)\nfor p in ax.patches:\n    ax.annotate('{:.2f}%'.format(100*p.get_height()/len(targets)), (p.get_x()+ 0.3, p.get_height()+10000))\nplt.title('Distribution of Target', fontsize=20)\nplt.xlabel('Claim', fontsize=20)\nplt.ylabel('Frequency [%]', fontsize=20)\nax.set_ylim(top=700000)","execution_count":null,"metadata":{"_cell_guid":"9a1b8f3a-4a46-4181-bcff-e8bb45f77a4a","collapsed":true,"_uuid":"567e8e5412c30cb42cecfe94ff1b5a47164b5135"}},{"cell_type":"markdown","source":"As you can see, the class 1 of claim is very smaller than 0. So, If we simply split the train and test during training,\nthis ununiform distribution could cause biased model. This must be treated cautiously and could be solved by more reasonable sampling! In this kernel, I will use staratified Kfold method. I reffered to [HyungsukKang work!\n](https://www.kaggle.com/sudosudoohio/stratified-kfold-xgboost-with-analysis-0-281)","metadata":{"_cell_guid":"ee71f4be-3aa7-4375-8b89-43dbf211f5c3","_uuid":"8187b3d4fc703fb66b9d7b28c5d7a130e634c2ea"}},{"cell_type":"markdown","source":"## 2.1 Data check\n- This coding style was from beluga's work. I respect him.\n- In this section, we check whether tha data between train and test are unique or overlapped.\n- and see if there are some missing values.","metadata":{"_cell_guid":"3665ab68-09cc-405a-9fce-f3a106d0849b","_uuid":"15c5fff1c7670c9c193f1bc26de82b0b0f93c84b"}},{"cell_type":"code","outputs":[],"source":"print('Id is unique.') if train.id.nunique() == train.shape[0] else print('Oh no')\nprint('Train and test sets are distinct.') if len(np.intersect1d(train.id.values, test.id.values)) == 0 else print('Oh no')\nprint('We do not need to worry about missing values.') if train.count().min() == train.shape[0] else print('Oh no')","execution_count":null,"metadata":{"_cell_guid":"b461e638-100a-4c78-adb3-946311d646e1","collapsed":true,"_uuid":"69fdfe11bc0d3baf7e93fe65c7bea3088227fe0f"}},{"cell_type":"markdown","source":"In 'Data description', we can see below sentences.\n> Values of -1 indicate that the feature was missing from the observation.\n\nSo, we need to find null value by finding '-1' value.","metadata":{"_cell_guid":"2ecd624d-73de-461c-aaa3-aa4bfc11b733","_uuid":"47fa28f3379640abbd9bc62060a4af137fe64558"}},{"cell_type":"markdown","source":"## 2.2 Find Null data\nWe need to find some features containing null data.<br>\nreference: [Anisotropic's work](https://www.kaggle.com/arthurtok/interactive-porto-insights-a-plot-ly-tutorial)","metadata":{"_cell_guid":"b4f2029d-bc0d-4ea6-b475-b932c35073c2","_uuid":"3bdda4f462d221c462edef215ac464b840849a34"}},{"cell_type":"code","outputs":[],"source":"import missingno as msno\n\ntrain_null = train\ntrain_null = train_null.replace(-1, np.NaN)\n\nmsno.matrix(df=train_null.iloc[:, :], figsize=(20, 14), color=(0.8, 0.5, 0.2))   ","execution_count":null,"metadata":{"_cell_guid":"402ea329-40c4-47d5-8b5f-369a3f08423e","collapsed":true,"_uuid":"c539a90cd3000bea3c801157e9b3e10be935f30c"}},{"cell_type":"code","outputs":[],"source":"test_null = test\ntest_null = test_null.replace(-1, np.NaN)\n\nmsno.matrix(df=test_null.iloc[:, :], figsize=(20, 14), color=(0.8, 0.5, 0.2))   ","execution_count":null,"metadata":{"_cell_guid":"1ff6ea41-c9e9-4e21-a34f-16a46f248431","scrolled":true,"collapsed":true,"_uuid":"b29183a900a8575cbc1343ce4bf6cfc379fed265"}},{"cell_type":"code","outputs":[],"source":"# Extract columns with null data\ntrain_null = train_null.loc[:, train_null.isnull().any()]\ntest_null = test_null.loc[:, test_null.isnull().any()]\n\nprint(train_null.columns)\nprint(test_null.columns)","execution_count":null,"metadata":{"_cell_guid":"13b15e5e-2589-40ac-983c-a5bd25d12bfd","collapsed":true,"_uuid":"21d37a071f33eb920859fa566439819501f5338e"}},{"cell_type":"code","outputs":[],"source":"print('Columns \\t Number of NaN')\nfor column in train_null.columns:\n    print('{}:\\t {}'.format(column,len(train_null[column][np.isnan(train_null[column])])))","execution_count":null,"metadata":{"_cell_guid":"e9130dfb-db5e-45d6-9410-8b73b30d31b0","scrolled":true,"collapsed":true,"_uuid":"e3340588ce0191a51808aed2731bde7ccb372fec"}},{"cell_type":"markdown","source":"As you can see, Many NaN will bring the error on my prediction. So, before leanring and prediction, NaN values must be replaced with some reasonable values. \nTo do that, analysis on feature it self is needed.","metadata":{"_cell_guid":"eb203716-c1a6-4a7c-a3b3-79d38121a3ad","_uuid":"814858182cf3d51d436552ff75855ff93af07de8"}},{"cell_type":"markdown","source":"## 2.3 Feature analysis\nAs said in data description, there are many type of features denoted by using postfix such as bin, cat. So, we need to divide them into each parts, and analyze individually.","metadata":{"_cell_guid":"85dc8bf4-4dbd-45d1-ad59-7112151ee555","_uuid":"e50d2d0e10ec3ea59f0491e94f9c9522211b90af"}},{"cell_type":"code","outputs":[],"source":"# divides all features in to 'bin', 'cat' and 'etc' group.\n\nfeature_list = list(train.columns)\ndef groupFeatures(features):\n    features_bin = []\n    features_cat = []\n    features_etc = []\n    for feature in features:\n        if 'bin' in feature:\n            features_bin.append(feature)\n        elif 'cat' in feature:\n            features_cat.append(feature)\n        elif 'id' in feature or 'target' in feature:\n            continue\n        else:\n            features_etc.append(feature)\n    return features_bin, features_cat, features_etc\n\nfeature_list_bin, feature_list_cat, feature_list_etc = groupFeatures(feature_list)\nprint(\"# of binary feature : \", len(feature_list_bin))\nprint(\"# of categorical feature : \", len(feature_list_cat))\nprint(\"# of other feature : \", len(feature_list_etc))","execution_count":null,"metadata":{"_cell_guid":"9af12608-38ae-4868-9cf6-25089ae492ad","collapsed":true,"_uuid":"6182423a43a202b82631380d38ff90a9b6de3b6b"}},{"cell_type":"markdown","source":"To see the characteristics of each features, I plotted the histogram for each feature with their head values. This analysis helps us to do the preprocessing of data well.","metadata":{"_cell_guid":"192121e1-5388-4a12-aedc-0d47f7c26233","_uuid":"8317f56147a2350302e2a6b03453b97bf90ae1b3"}},{"cell_type":"code","outputs":[],"source":"def TrainTestHistogram(train, test, feature):\n    fig, axes = plt.subplots(len(feature), 2, figsize=(10, 40))\n    fig.tight_layout()\n\n    left  = 0  # the left side of the subplots of the figure\n    right = 0.9    # the right side of the subplots of the figure\n    bottom = 0.1   # the bottom of the subplots of the figure\n    top = 0.9      # the top of the subplots of the figure\n    wspace = 0.3   # the amount of width reserved for blank space between subplots\n    hspace = 0.7   # the amount of height reserved for white space between subplot\n\n    plt.subplots_adjust(left=left, bottom=bottom, right=right, \n                        top=top, wspace=wspace, hspace=hspace)\n    count = 0\n    for i, ax in enumerate(axes.ravel()):\n        if i % 2 == 0:\n            title = 'Train: ' + feature[count]\n            ax.hist(train[feature[count]], bins=30, normed=False)\n            ax.set_title(title)\n        else:\n            title = 'Test: ' + feature[count]\n            ax.hist(test[feature[count]], bins=30, normed=False)\n            ax.set_title(title)\n            count = count + 1","execution_count":null,"metadata":{"_cell_guid":"7015c6c1-e501-4f0c-85de-bfc379270ed5","collapsed":true,"_uuid":"169ffa23cdad45520a29b3ce0c68bdacfb5cd38b"}},{"cell_type":"code","outputs":[],"source":"# For bin features\nTrainTestHistogram(train, test, feature_list_bin)","execution_count":null,"metadata":{"_cell_guid":"523587ab-5a67-4ddd-92a4-27a53574b1e9","collapsed":true,"_uuid":"c37dd025986f57a34cfc433ef4a523ee735e0e43"}},{"cell_type":"code","outputs":[],"source":"# For cat features\nTrainTestHistogram(train, test, feature_list_cat)","execution_count":null,"metadata":{"_cell_guid":"17c6efe8-409e-491c-a8a8-88f2ab86085e","scrolled":false,"collapsed":true,"_uuid":"f0d2769d522032aa717dca0568f6d35eae0647f5"}},{"cell_type":"code","outputs":[],"source":"# For etc features\nTrainTestHistogram(train, test, feature_list_etc)","execution_count":null,"metadata":{"_cell_guid":"a3237b6f-6f08-4671-aec8-f6108fd6434c","scrolled":false,"collapsed":true,"_uuid":"805b998a1d4ff766355b97d45426eba2e336a6bb"}},{"cell_type":"markdown","source":"With above histogram grid plot, we observed some information.\n1. The train and test data have similar distribition in case of binary and categorical data. (Q. Are there other insights from this plots?)\n2. Etc feature also have large similarity between train and test set. But, there are somewhat difference between train and test in case of etc.\n\n### I'm beginner of statistics and kaggle. So If you have an idea from these plots, please advise me!! Thanks!","metadata":{"_cell_guid":"645741a8-aa79-4e0f-adc7-b3fe6928e9b9","_uuid":"f568375f16378115e1df3a7fd592c343ecc373fe"}},{"cell_type":"markdown","source":"Let's see the etc feature more deelply","metadata":{"_cell_guid":"9261af9f-4c53-48b5-b888-f9d673f4ddff","_uuid":"ac44941a70315e2091f52c33c973aa9a418632de"}},{"cell_type":"code","outputs":[],"source":"left  = 0  # the left side of the subplots of the figure\nright = 0.9    # the right side of the subplots of the figure\nbottom = 0.1   # the bottom of the subplots of the figure\ntop = 0.9      # the top of the subplots of the figure\nwspace = 0.3   # the amount of width reserved for blank space between subplots\nhspace = 0.7   # the amount of height reserved for white space between subplot\n\nfig, axes = plt.subplots(13, 2, figsize=(10, 40))\nplt.subplots_adjust(left=left, bottom=bottom, right=right, \n                    top=top, wspace=wspace, hspace=hspace)\n\nfor i, ax in enumerate(axes.ravel()):\n    title = 'Train: ' + feature_list_etc[i]\n    ax.hist(train[feature_list_etc[i]], bins=20, normed=True)\n    ax.set_title(title)\n    ax.text(0, 1.2, train[feature_list_etc[i]].head(), horizontalalignment='left',\n            verticalalignment='top', style='italic',\n       bbox={'facecolor':'red', 'alpha':0.2, 'pad':10}, transform=ax.transAxes)","execution_count":null,"metadata":{"_cell_guid":"94cb6619-ab46-4a9d-9231-a0a2bfc322ba","collapsed":true,"_uuid":"b6e33f2f55100c15b72e1efcd5e110f2b30199cd"}},{"cell_type":"markdown","source":"Data description said that feature without 'bin' and 'cat' is either ordinal or continuous.\nSo, simply, these features could be grouped in ordinal and continous group.\n\n### I'm going to analyze this part more deeply\n---\nOrdinal group\n- ps_ind_01\n- ps_ind_03\n- ps_ind_14\n- ps_ind_15\n- ps_reg_01\n- ps_reg_02\n- ps_car_11\n- ps_calc_01\n- ps_calc_02\n- ps_calc_03\n- ps_calc_04\n- ps_calc_05\n- ps_calc_06\n- ps_calc_07\n- ps_calc_08\n- ps_calc_09\n- ps_calc_10\n- ps_calc_11\n- ps_calc_12\n- ps_calc_13\n- ps_calc_14\n---\nContinuous group\n- ps_reg_03\n- ps_car_12\n- ps_car_13\n- ps_car_14\n- ps_car_15\n","metadata":{"_cell_guid":"09da3d5a-88a7-48d6-8eed-2b5d9282c427","_uuid":"61ecd4fd3d85b553386d7d17184852effe491c6b"}},{"cell_type":"markdown","source":"## 2.4 Predict null data based on statistical method\nFor now, I will change the null data with values obtained from some statistics. I will focus on this part later to get a good result.","metadata":{"_cell_guid":"a8a9834d-345d-4282-8eb4-fa621a8ceda6","_uuid":"cb80ec60c1df111ec2e259841f3f89f6317a8ba9"}},{"cell_type":"code","outputs":[],"source":"# For ordinal group\netc_ordianal_features = ['ps_ind_01', 'ps_ind_03', 'ps_ind_14', 'ps_ind_15', 'ps_reg_01',\n                    'ps_reg_02', 'ps_car_11', 'ps_calc_01', 'ps_calc_02', 'ps_calc_03',\n                    'ps_calc_04', 'ps_calc_05', 'ps_calc_06', 'ps_calc_07', 'ps_calc_08',\n                    'ps_calc_09', 'ps_calc_10', 'ps_calc_11', 'ps_calc_12', 'ps_calc_13',\n                    'ps_calc_14']\n\netc_continuous_features = ['ps_reg_03', 'ps_car_12', 'ps_car_13', 'ps_car_14', 'ps_car_15']\n\ntrain_null_columns = train_null.columns\ntest_null_columns = test_null.columns","execution_count":null,"metadata":{"_cell_guid":"cc7bb993-5b51-4f8d-b0b3-8d6848d865cc","collapsed":true,"_uuid":"fb70450618e34532f9970939d97c946c68956c83"}},{"cell_type":"code","outputs":[],"source":"# For train\nfor feature in train_null_columns:\n    if 'cat' in feature or 'bin' in feature:\n        # For categorical and binary features with postfix, substitue null values with the most frequent value to avoid float number.\n        train_null[feature].fillna(train_null[feature].value_counts().idxmax(), inplace=True)\n    elif feature in etc_continuous_features:\n        train_null[feature].fillna(train_null[feature].median(), inplace=True)\n    elif feature in etc_ordianal_features:\n        # For categorical and binary features which was assumed, substitue null values with the most frequent value to avoid float number.\n        train_null[feature].fillna(train_null[feature].value_counts().idxmax(), inplace=True)\n    else:\n        print(feature)","execution_count":null,"metadata":{"_cell_guid":"b9bd9373-3252-4798-8b31-a7db4b9ef261","collapsed":true,"_uuid":"a1dd1d908b75afd57a9723cdfbad2275049c500d"}},{"cell_type":"code","outputs":[],"source":"# For test\nfor feature in test_null_columns:\n    if 'cat' in feature or 'bin' in feature:\n        # For categorical and binary features with postfix, substitue null values with the most frequent value to avoid float number.\n        test_null[feature].fillna(test_null[feature].value_counts().idxmax(), inplace=True)\n    elif feature in etc_continuous_features:\n        test_null[feature].fillna(test_null[feature].median(), inplace=True)\n    elif feature in etc_ordianal_features:\n        # For categorical and binary features which was assumed, substitue null values with the most frequent value to avoid float number.\n        test_null[feature].fillna(test_null[feature].value_counts().idxmax(), inplace=True)\n    else:\n        print(feature)","execution_count":null,"metadata":{"_cell_guid":"9b0d6138-c291-4546-94a7-a63d9aeff5aa","collapsed":true,"_uuid":"deee92baa86d9c138fe83dd71fe6333dd31d7c2c"}},{"cell_type":"code","outputs":[],"source":"for feature in train_null_columns:\n    train[feature] = train_null[feature]\n    \nfor feature in test_null_columns:\n    test[feature] = test_null[feature]","execution_count":null,"metadata":{"_cell_guid":"8d8fbf85-c548-4df9-bd1c-5817123c4cfb","collapsed":true,"_uuid":"dd6c1a0f786c041deaf24602c4aeb993d925f9e5"}},{"cell_type":"markdown","source":"Until now, we change all NaN values with some values obtained from simple statistics.","metadata":{"_cell_guid":"73b8ede3-0f0f-4e81-94bc-4ea3643c65d8","_uuid":"bcebe157708e329d09568a6dddbe27f85a4bc27c"}},{"cell_type":"code","outputs":[],"source":"msno.matrix(df=train.iloc[:, :], figsize=(20, 14), color=(0.8, 0.5, 0.2))   ","execution_count":null,"metadata":{"_cell_guid":"8e3eebdc-e778-4da3-9f9b-c649b7b57aa2","collapsed":true,"_uuid":"61d8484218f34139627bd95b102e658bc525e9f4"}},{"cell_type":"code","outputs":[],"source":"msno.matrix(df=test.iloc[:, :], figsize=(20, 14), color=(0.8, 0.5, 0.2))","execution_count":null,"metadata":{"_cell_guid":"58f811cd-83b8-402a-ae8b-2134ba5966aa","scrolled":true,"collapsed":true,"_uuid":"30e81797a63b44fa5521e415d6e083520a0a7fe0"}},{"cell_type":"markdown","source":"As you can see, there is no NaN values in both train and test set.","metadata":{"_cell_guid":"299e1d0f-c51e-46fa-bf70-c8d741172065","_uuid":"f96a4a4300bfe42b427d98e7630ffe1ee9581172"}},{"cell_type":"markdown","source":"## 2.5 Onehot encoding for categorical data\n\nFor categorical data, I used pd.get_dummy() function to do onehot encoding!","metadata":{"_cell_guid":"6cef6717-758e-4ab7-93a2-b1130018ac16","_uuid":"4a44c568901805ff0e1d747f5b48c28a443b8318"}},{"cell_type":"code","outputs":[],"source":"def oneHotEncode_dataframe(df, features):\n    for feature in features:\n        temp_onehot_encoded = pd.get_dummies(df[feature])\n        column_names = [\"{}_{}\".format(feature, x) for x in temp_onehot_encoded.columns]\n        temp_onehot_encoded.columns = column_names\n        df = df.drop(feature, axis=1)\n        df = pd.concat([df, temp_onehot_encoded], axis=1)\n    return df","execution_count":null,"metadata":{"_cell_guid":"ae8220ea-1144-40ed-904a-c7f2d303e050","collapsed":true,"_uuid":"aac5c6c8c1a239ed6eef461adb86a5a400196c26"}},{"cell_type":"code","outputs":[],"source":"train = oneHotEncode_dataframe(train, feature_list_cat)\ntest = oneHotEncode_dataframe(test, feature_list_cat)","execution_count":null,"metadata":{"_cell_guid":"85a92b01-aab8-4a2d-95ef-c5822ad9d557","collapsed":true,"_uuid":"ee9f61e76b2e63576544ce8edbc645346f6dcaea"}},{"cell_type":"markdown","source":"# 3. Prediction with xgboost\nLet's do the ML and prediction with xgboost!!","metadata":{"_cell_guid":"03349bec-116a-45e6-b4db-16e5bdc3b71e","_uuid":"cc616c0a87ab8c5461e38a1df9f0c571640aa6a2"}},{"cell_type":"code","outputs":[],"source":"# Define the gini metric - from https://www.kaggle.com/c/ClaimPredictionChallenge/discussion/703#5897\ndef gini(actual, pred, cmpcol = 0, sortcol = 1):\n    assert( len(actual) == len(pred) )\n    all = np.asarray(np.c_[ actual, pred, np.arange(len(actual)) ], dtype=np.float)\n    all = all[ np.lexsort((all[:,2], -1*all[:,1])) ]\n    totalLosses = all[:,0].sum()\n    giniSum = all[:,0].cumsum().sum() / totalLosses\n    \n    giniSum -= (len(actual) + 1) / 2.\n    return giniSum / len(actual)\n \ndef gini_normalized(a, p):\n    return gini(a, p) / gini(a, a)\n\ndef gini_xgb(preds, dtrain):\n    labels = dtrain.get_label()\n    gini_score = gini_normalized(labels, preds)\n    return 'gini', gini_score","execution_count":null,"metadata":{"_cell_guid":"796f8a1c-32bb-41f5-98a8-795e2207c440","collapsed":true,"_uuid":"a0acf23b1702fc97f012a14d5c78a5371c6a5465"}},{"cell_type":"markdown","source":"## 3.1 Using stratified shuffle split\nStratified shuffle split gives the index set of splited data in train/test sets.\nThe frequency of each class in each fold have a same ratio with the ration of each class in all dataset.\nDetail is [here.](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html#sklearn.model_selection.StratifiedShuffleSplit)<br>\nApplying stratified shuffle split, we can avoid imbalanced sampling problem in case of train and test data split.\nProcess is explaned below.\n\n1. Choose some classifier. In this case, I chose xgboost.\n2. Set the parameter \n-> I think the optimization of parameters would be precede to obtain more good result.\n3.  With classifier, do the staratified Shuffle Split.\n4. Train at each shuffle split and predict\n5. Get a averarage prediction!","metadata":{"_cell_guid":"3ab4332d-bb74-44ad-b299-61cf163ec0ce","_uuid":"59aa187f855f78cc1710c84505b2fdf1f16d5478"}},{"cell_type":"code","outputs":[],"source":"n_split = 3\nSSS = StratifiedShuffleSplit(n_splits=3, test_size=0.5, random_state=1989)","execution_count":null,"metadata":{"_cell_guid":"e0832527-41ad-48b6-8def-a86995193a86","collapsed":true,"_uuid":"1c6115429f5bec5725e42436a7bd2b866fcd66d9"}},{"cell_type":"code","outputs":[],"source":"# Parameter optimization is needed!\nparams = {\n    'min_child_weight': 10.0,\n    'max_depth': 7,\n    'max_delta_step': 1.8,\n    'colsample_bytree': 0.4,\n    'subsample': 0.8,\n    'eta': 0.025,\n    'gamma': 0.65,\n    'num_boost_round' : 700\n}","execution_count":null,"metadata":{"_cell_guid":"937209ff-b2cc-462b-8abf-1fd7d5054758","collapsed":true,"_uuid":"9b5284c8eaca17b8ffe0ec7b458781bb52b15dc9"}},{"cell_type":"code","outputs":[],"source":"X = train.drop(['id', 'target'], axis=1).values\ny = train.target.values\ntest_id = test.id.values\ntest = test.drop('id', axis=1)","execution_count":null,"metadata":{"_cell_guid":"4edbcd13-f0dc-4d52-9b6d-2f2840c332f0","collapsed":true,"_uuid":"19468000a0ab6ce9f086c423f710f9c2b0475b6b"}},{"cell_type":"code","outputs":[],"source":"sub = pd.DataFrame()\nsub['id'] = test_id\nsub['target'] = np.zeros_like(test_id)","execution_count":null,"metadata":{"_cell_guid":"dcf8fcca-a1ef-484a-a132-c0d7c9d603e1","collapsed":true,"_uuid":"01ac52004b0ab57f78e7f95a975f89bf61358581"}},{"cell_type":"code","outputs":[],"source":"SSS.get_n_splits(X, y)","execution_count":null,"metadata":{"_cell_guid":"ddefcb37-4468-4137-82e9-74b02b28efdd","collapsed":true,"_uuid":"bf9e5589c71fb41fce02d3b2bc229d8bb5dbd7c3"}},{"cell_type":"code","outputs":[],"source":"print(SSS)","execution_count":null,"metadata":{"_cell_guid":"44897646-a9aa-4fcd-a3a7-2b814bd7f796","collapsed":true,"_uuid":"4b6b575a43f9f4319d47f9f47f37f0f9ab6c501e"}},{"cell_type":"code","outputs":[],"source":"for train_index, test_index in SSS.split(X, y):\n    print(\"TRAIN: \", train_index, \"TEST: \", test_index)","execution_count":null,"metadata":{"_cell_guid":"afc95a4a-4964-4fa5-8f60-8d631dd1225b","collapsed":true,"_uuid":"1543f01335009dd70a72ab6dd854259824920ace"}},{"cell_type":"markdown","source":"Stratified shuffle split returns the index set of splited train and test.\nUsing this, we can train and predict.","metadata":{"_cell_guid":"302156b7-9f30-427b-bbf1-a394c9e77148","_uuid":"2050f2f17fb17e57452316370dbfde6120849d31"}},{"cell_type":"code","outputs":[],"source":"for i, (train_index, test_index) in enumerate(SSS.split(X, y)):\n    print('------# {} of {} shuffle split------'.format(i + 1, n_split))\n    X_train, X_valid = X[train_index], X[test_index]\n    y_train, y_valid = y[train_index], y[test_index]\n    \n    # Convert splited data into XGBoost format\n    d_train = xgb.DMatrix(X_train, y_train)\n    d_valid = xgb.DMatrix(X_valid, y_valid)\n    d_test = xgb.DMatrix(test.values)\n    watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n\n    # Train the model! \n    model = xgb.train(params, d_train, 2000, watchlist, \n                      early_stopping_rounds=100, feval=gini_xgb, maximize=True, verbose_eval=100)\n\n    print('------# {} of {} prediction------'.format(i + 1, n_split))\n    # Predict on our test data\n    p_test = model.predict(d_test)\n    sub['target'] = sub['target'] + p_test/n_split","execution_count":null,"metadata":{"_cell_guid":"56d503fa-8fb7-4cfb-be01-235fc4ae7a7f","collapsed":true,"_uuid":"eeab7b0e548fcaa81a5f88ab67e199a45cd83597"}},{"cell_type":"code","outputs":[],"source":"# sub.to_csv('stratifiedShuffleSplit_xgboost.csv', index=False)","execution_count":null,"metadata":{"_cell_guid":"31bc17fb-22e0-4e1f-8af9-bed1185601f3","collapsed":true,"_uuid":"1f43ebbb5cb35f15675a7244ebb9c4a0ca272f86"}},{"cell_type":"markdown","source":"# 4. Things to do\n1. PCA analysis\n2. one-hot encoding <br> \n3. Find more deep insight for each features\n4. Apply other Algorithms and other technique such as ensemble, etc.","metadata":{"_cell_guid":"c2398983-99c5-40ee-bb69-7519a50b51d5","_uuid":"dbb24119a1043cc909588431fcfc883a1dc18d5b"}},{"cell_type":"markdown","source":"# PS\nMaking kernel is very helpful to study kaggle and data analysis. <br>\nI want to keep improving this kernel. If you want to comment or ask me, feel free. Thanks!","metadata":{"_cell_guid":"5b513ca7-3251-4486-af7f-8a16e00ac6ec","_uuid":"f9701c568037de8050e199aacf1cbd5f72bb9287"}}],"metadata":{"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"},"language_info":{"mimetype":"text/x-python","pygments_lexer":"ipython3","file_extension":".py","codemirror_mode":{"version":3,"name":"ipython"},"nbconvert_exporter":"python","version":"3.6.3","name":"python"}}}