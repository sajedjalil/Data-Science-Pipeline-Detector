{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"version":"3.6.3","pygments_lexer":"ipython3","codemirror_mode":{"version":3,"name":"ipython"},"mimetype":"text/x-python","name":"python","file_extension":".py","nbconvert_exporter":"python"}},"nbformat_minor":1,"nbformat":4,"cells":[{"metadata":{"_uuid":"e695f8aebaeb8d06ef117ec8c2250e06ce903e98","_cell_guid":"43245047-852b-4562-8633-94b1932c3891"},"cell_type":"markdown","source":"# Construction of a deep neural network for predicting insurance claim probabilities\n\nIn this script we will build an deep neural network (3 layers) to predict the probabilities of customers filing insurance claims. I am doing the preprocessing in pandas and scaling the data using scikit learn's StandardScaler function.\n\nImports for the functions we use:"},{"metadata":{"_uuid":"bce6bbc5c9fd81f78268f5e614e98ddac7b97bc5","_cell_guid":"fb9001ad-8783-4cf5-a290-3f09de10d1c7","collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.preprocessing import StandardScaler"},{"metadata":{"_uuid":"eb2145ceee123e21b49db3fdee9e5dd0f4c4dde6","_cell_guid":"93b12094-3ce9-40b7-9ce9-ac1837fea214"},"cell_type":"markdown","source":"Next we read in the data."},{"metadata":{"_uuid":"88fdc5fc67d758709c962bda5ae84bf73a1bc98b","_cell_guid":"526c140d-b2eb-4b6f-8c62-6a091ca96b86","collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"test_dat = pd.read_csv('../input/test.csv')\ntrain_dat = pd.read_csv('../input/train.csv')\nsubmission = pd.read_csv('../input/sample_submission.csv')"},{"metadata":{"_uuid":"bbbe7f94838b7c298592275f443e57364d2f9739","_cell_guid":"ea8337b9-4eba-409f-add7-a0d56b17d1e5"},"cell_type":"markdown","source":"## Cleaning the data\n\nWe split the y values off to a separate object, and drop the ids and targets from the train and test dataframes. Since we will be manipulating the data, we need to merge the train and test dataframes so that we are making the same changes to each dataset (otherwise predictions would be impossible)."},{"metadata":{"_uuid":"77292331b789bb38afc8307c0def84dd32c7884a","_cell_guid":"70e81350-fe84-42aa-9f85-e8b3bd558391","collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"train_y = train_dat['target']\ntrain_x = train_dat.drop(['target', 'id'], axis = 1)\ntest_dat = test_dat.drop(['id'], axis = 1)\n\nmerged_dat = pd.concat([train_x, test_dat],axis=0)"},{"metadata":{"_uuid":"7349f0fb72780d130fdcf091597b1cd37c9f47bc","_cell_guid":"97a3792b-28b8-440a-b5a9-8c3e8d3475c4"},"cell_type":"markdown","source":"Below we make several changes to the data prior to training the neural network. First the inputs are changed to float32 (as float64 is not compatabile with all of tensorflow's models). Second we one-hot encode the categorical variables. Thirdly, we standardize the scale of the numerical variables."},{"metadata":{"_uuid":"777fbe6a23700d6d7173a4d71c0592ed64696379","_cell_guid":"83a8759a-6c9e-4a74-8535-3ed4ef673e6d","collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"#change data to float32\nfor c, dtype in zip(merged_dat.columns, merged_dat.dtypes): \n    if dtype == np.float64:     \n        merged_dat[c] = merged_dat[c].astype(np.float32)\n\n#one hot encode the categoricals\ncat_features = [col for col in merged_dat.columns if col.endswith('cat')]\nfor column in cat_features:\n    temp=pd.get_dummies(pd.Series(merged_dat[column]))\n    merged_dat=pd.concat([merged_dat,temp],axis=1)\n    merged_dat=merged_dat.drop([column],axis=1)\n\n#standardize the scale of the numericals\nnumeric_features = [col for col in merged_dat.columns if '_calc_' in  str(col)]\nnumeric_features = [col for col in numeric_features if '_bin' not in str(col)]\n\nscaler = StandardScaler()\nscaled_numerics = scaler.fit_transform(merged_dat[numeric_features])\nscaled_num_df = pd.DataFrame(scaled_numerics, columns =numeric_features )"},{"metadata":{"_uuid":"cedff756ca9e8dfbed874d2c6195556111fdcf84","_cell_guid":"5c9c246d-23db-46d4-a6da-abdebf7c8ae2"},"cell_type":"markdown","source":"With the data munged, we can now split it back into train and test variables"},{"metadata":{"_uuid":"156828b5f5579c1cbefc10aaf22e17d4fdc420fb","_cell_guid":"86b2cab7-4550-4826-815d-b7f9ce000137","collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"merged_dat = merged_dat.drop(numeric_features, axis=1)\n\nmerged_dat = np.concatenate((merged_dat.values,scaled_num_df), axis = 1)\n\ntrain_x = merged_dat[:train_x.shape[0]]\ntest_dat = merged_dat[train_x.shape[0]:]"},{"metadata":{"_uuid":"f6eb7d56c1f90f24f818090757f9aaaf25e917d5","_cell_guid":"cc082007-4658-4741-9b71-3c723911c57d"},"cell_type":"markdown","source":"## Training the neural network\n\nBelow we ste up the neural network in tensorflow, and fit the training data."},{"metadata":{"_uuid":"96092ed42831032aada01ce7d6aa9fe3d94d620b","_cell_guid":"1d30a908-54af-4242-b318-16de157bed96","collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"config = tf.contrib.learn.RunConfig(tf_random_seed=42)\n\nfeature_cols = tf.contrib.learn.infer_real_valued_columns_from_input(train_x)"},{"metadata":{"_uuid":"757b98b9fc68f349ac16f895db9c6f97bfabef4d","_cell_guid":"193834a4-2dce-42ba-95f9-943ef5b3c61b"},"cell_type":"markdown","source":"We then create the DNN classifier. DNN == deep neural network\n\nhidden units means the number of units per layer, of the neural network abd all layers fully connected.\n'[64,32]' would be 64 nodes in first layer and 32 in second."},{"metadata":{"_uuid":"45936c5923c95ba32bc8b2222e833534aec986e1","_cell_guid":"c06efc22-3018-45f2-b161-a91aabaad725","collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"dnn_clf = tf.contrib.learn.DNNClassifier(hidden_units=[150,150,150], n_classes=2,\n                                         feature_columns=feature_cols, config=config)"},{"metadata":{"_uuid":"ff5029d1b10c7446180031171e3016766af088ce","_cell_guid":"17697cd5-b6e6-476b-866e-ffd76c438b4c"},"cell_type":"markdown","source":"Fitting the model is just like with SciKit learn, the steps variable indicates the number of training iterations and the batch_size is the number of samples used to train the network on each step."},{"metadata":{"_uuid":"88c8479dfda3190045c5c35aed697f34d6b12770","_cell_guid":"cd21a472-ebef-40d6-b93a-21ba5033d3db","collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"dnn_clf.fit(train_x, train_y, batch_size=50, steps=40000)"},{"metadata":{"_uuid":"42dc9bd5e58dddd66e6569ce6197aaed63ba52f9","_cell_guid":"a4e019c4-6c5d-4bbf-8960-502c86b21438"},"cell_type":"markdown","source":"## Predicting probabilities with the neural network\nWe then predict the probabilities using predict proba. A generator object is produced, so I wrap it in a list function so that we can move to list form (and easily put the data into a pandas df). "},{"metadata":{"_uuid":"372c326f352c93b9c70a4473a15e5c4b37709054","_cell_guid":"cdb095dc-329d-4859-a599-23a089630faf","collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"dnn_y_pred = dnn_clf.predict_proba(test_dat)\n\ndnn_out = list(dnn_y_pred)"},{"metadata":{"_uuid":"0bf9a35a40d0ae2dd2a3bf742c42afa18b59be9b","_cell_guid":"70611de2-ee52-46c2-9eee-9e81e135dea5"},"cell_type":"markdown","source":"Note the predicted probabilities are provided in pairs of (P[0], P[1]). We want the probability of a claim (P[1]) so I use a list comprehension below to grab only the second member of the array when I add the data to the output dataframe."},{"metadata":{"_uuid":"afbc5e8239d29fa9a59179b91681902a6f04b439","_cell_guid":"22c525e4-0040-4035-9b71-59f56fb60f00","collapsed":true},"outputs":[],"execution_count":null,"cell_type":"code","source":"dnn_output = submission\ndnn_output['target'] = [x[1] for x in dnn_out]\n\ndnn_output.to_csv('dnn_predictions.csv', index=False, float_format='%.4f')"}]}