{"cells":[{"cell_type":"markdown","source":"Make sure to install the superb [__Bayesian Optimization__](https://github.com/fmfn/BayesianOptimization) library.","metadata":{}},{"source":"# This line is needed for python 2.7 ; probably not for python 3\nfrom __future__ import print_function\n\nimport numpy as np\nimport pandas as pd\nimport gc\nimport warnings\n\nfrom bayes_opt import BayesianOptimization\n\nfrom sklearn.cross_validation import cross_val_score, StratifiedKFold, StratifiedShuffleSplit\nfrom sklearn.metrics import log_loss, matthews_corrcoef, roc_auc_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport xgboost as xgb\nimport contextlib","cell_type":"code","metadata":{"_uuid":"d1eab8b46d36b46fecb475ef17a9806d98d8d770","_cell_guid":"242f8c7e-4e80-4b1c-ae28-07b49a55145b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This will be used to capture stderr and stdout without having anything print on screen.\n\n**It turns out that Kaggle does not have \"cStringIO\", so I will comment out this portion.**","metadata":{}},{"source":"#@contextlib.contextmanager\n#def capture():\n#    import sys\n#    from cStringIO import StringIO\n#    olderr, oldout = sys.stderr, sys.stdout\n#    try:\n#        out=[StringIO(), StringIO()]\n#        sys.stderr,sys.stdout = out\n#        yield out\n#    finally:\n#        sys.stderr,sys.stdout = olderr,oldout\n#        out[0] = out[0].getvalue().splitlines()\n#        out[1] = out[1].getvalue().splitlines()","cell_type":"code","metadata":{"collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Scaling is really not needed for XGBoost, but I leave it here in case if you do the optimization using ML approaches that need it.","metadata":{}},{"source":"def scale_data(X, scaler=None):\n    if not scaler:\n        scaler = MinMaxScaler(feature_range=(-1, 1))\n        scaler.fit(X)\n    X = scaler.transform(X)\n    return X, scaler","cell_type":"code","metadata":{"collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Loading files.","metadata":{}},{"source":"DATA_TRAIN_PATH = '../input/train.csv'\nDATA_TEST_PATH = '../input/test.csv'\n\ndef load_data(path_train=DATA_TRAIN_PATH, path_test=DATA_TEST_PATH):\n    train_loader = pd.read_csv(path_train, dtype={'target': np.int8, 'id': np.int32})\n    train = train_loader.drop(['target', 'id'], axis=1)\n    train_labels = train_loader['target'].values\n    train_ids = train_loader['id'].values\n    print('\\n Shape of raw train data:', train.shape)\n\n    test_loader = pd.read_csv(path_test, dtype={'id': np.int32})\n    test = test_loader.drop(['id'], axis=1)\n    test_ids = test_loader['id'].values\n    print(' Shape of raw test data:', test.shape)\n\n    return train, train_labels, test, train_ids, test_ids","cell_type":"code","metadata":{"collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Define cross-validation variables that are used for parameter search. Each parameter has its own line, so it is easy to comment something out if you wish. Keep in mind that in such a case you must comment out the matching lines in optimization and explore sections below.\n\n*Note that the learning rate (\"eta\") is set to 0.1 below. That is certainly not optimal, but it will make the search go faster. You will probably want to experiment with values in 0.01-0.05 range, but beware that it will significantly slow down the process because more iterations will be required to get to early stopping. Doing 10-fold instead of 5-fold cross-validation will also result in a small gain, but will double the search time.*\n\nXGBoost outputs lots of interesting info, but it is not very helpful and clutters the screen when doing grid search. So we will run XGboost CV with verbose turned on, but will capture stderr in result[0] and stdout in result[1]. We will extract the relevant info from these variables later, and will print the record of each CV run into a log file.\n\nAUC will be optimized here. We can go with separately defined gini scorer and use **feval=gini** below but I don't think it makes any difference because AUC and gini are directly correlated.\n\n**Commenting out the capture so there will be no record of xgb.cv in log file.**","metadata":{}},{"source":"# Comment out any parameter you don't want to test\ndef XGB_CV(\n          max_depth,\n          gamma,\n          min_child_weight,\n          max_delta_step,\n          subsample,\n          colsample_bytree\n         ):\n\n    global AUCbest\n    global ITERbest\n\n#\n# Define all XGboost parameters\n#\n\n    paramt = {\n              'booster' : 'gbtree',\n              'max_depth' : int(max_depth),\n              'gamma' : gamma,\n              'eta' : 0.1,\n              'objective' : 'binary:logistic',\n              'nthread' : 4,\n              'silent' : True,\n              'eval_metric': 'auc',\n              'subsample' : max(min(subsample, 1), 0),\n              'colsample_bytree' : max(min(colsample_bytree, 1), 0),\n              'min_child_weight' : min_child_weight,\n              'max_delta_step' : int(max_delta_step),\n              'seed' : 1001\n              }\n\n    folds = 5\n    cv_score = 0\n\n    print(\"\\n Search parameters (%d-fold validation):\\n %s\" % (folds, paramt), file=log_file )\n    log_file.flush()\n\n    xgbc = xgb.cv(\n                    paramt,\n                    dtrain,\n                    num_boost_round = 20000,\n                    stratified = True,\n                    nfold = folds,\n#                    verbose_eval = 10,\n                    early_stopping_rounds = 100,\n                    metrics = 'auc',\n                    show_stdv = True\n               )\n\n# This line would have been on top of this section\n#    with capture() as result:\n\n# After xgb.cv is done, this section puts its output into log file. Train and validation scores \n# are also extracted in this section. Note the \"diff\" part in the printout below, which is the \n# difference between the two scores. Large diff values may indicate that a particular set of \n# parameters is overfitting, especially if you check the CV portion of it in the log file and find \n# out that train scores were improving much faster than validation scores.\n\n#    print('', file=log_file)\n#    for line in result[1]:\n#        print(line, file=log_file)\n#    log_file.flush()\n\n    val_score = xgbc['test-auc-mean'].iloc[-1]\n    train_score = xgbc['train-auc-mean'].iloc[-1]\n    print(' Stopped after %d iterations with train-auc = %f val-auc = %f ( diff = %f ) train-gini = %f val-gini = %f' % ( len(xgbc), train_score, val_score, (train_score - val_score), (train_score*2-1),\n(val_score*2-1)) )\n    if ( val_score > AUCbest ):\n        AUCbest = val_score\n        ITERbest = len(xgbc)\n\n    return (val_score*2) - 1","cell_type":"code","metadata":{"collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The \"real\" code starts here.","metadata":{}},{"source":"# Define the log file. If you repeat this run, new output will be added to it\nlog_file = open('Porto-AUC-5fold-XGB-run-01-v1-full.log', 'a')\nAUCbest = -1.\nITERbest = 0\n\n# Load data set and target values\ntrain, target, test, tr_ids, te_ids = load_data()\nn_train = train.shape[0]\ntrain_test = pd.concat((train, test)).reset_index(drop=True)\ncol_to_drop = train.columns[train.columns.str.endswith('_cat')]\ncol_to_dummify = train.columns[train.columns.str.endswith('_cat')].astype(str).tolist()\n\nfor col in col_to_dummify:\n    dummy = pd.get_dummies(train_test[col].astype('category'))\n    columns = dummy.columns.astype(str).tolist()\n    columns = [col + '_' + w for w in columns]\n    dummy.columns = columns\n    train_test = pd.concat((train_test, dummy), axis=1)\n\ntrain_test.drop(col_to_dummify, axis=1, inplace=True)\ntrain_test_scaled, scaler = scale_data(train_test)\ntrain = train_test_scaled[:n_train, :]\ntest = train_test_scaled[n_train:, :]\nprint('\\n Shape of processed train data:', train.shape)\nprint(' Shape of processed test data:', test.shape)\n\n# We really didn't need to load the test data in the first place unless you are planning to make\n# a prediction at the end of this run.\n# del test\n# gc.collect()","cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I am doing a stratified split and using only 25% of the data. Obviously, this is done to make sure that this notebook can run to completion on Kaggle. In a production version, you should uncomment the first line in the section below, and comment out or delete everything else.","metadata":{}},{"source":"# dtrain = xgb.DMatrix(train, label = target)\n\nsss = StratifiedShuffleSplit(target, random_state=1001, test_size=0.75)\nfor train_index, test_index in sss:\n    break\nX_train, y_train = train[train_index], target[train_index]\ndel train, target\ngc.collect()\ndtrain = xgb.DMatrix(X_train, label = y_train)\n","cell_type":"code","metadata":{"collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"These are the parameters and their ranges that will be used during optimization. They must match the parameters that are passed above to the XGB_CV function. If you commented out any of them above, you should do the same here. Note that these are pretty wide ranges for most parameters.","metadata":{}},{"source":"XGB_BO = BayesianOptimization(XGB_CV, {\n                                     'max_depth': (2, 12),\n                                     'gamma': (0.001, 10.0),\n                                     'min_child_weight': (0, 20),\n                                     'max_delta_step': (0, 10),\n                                     'subsample': (0.4, 1.0),\n                                     'colsample_bytree' :(0.4, 1.0)\n                                    })","cell_type":"code","metadata":{"collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This portion of the code is not necessary. You can simply specify that 10-20 random parameter combinations (**init_points** below) be used. However, I like to try couple of high- and low-end values for each parameter as a starting point, and after that fewer random points are needed. Note that a number of options must be the same for each parameter, and they are applied vertically.","metadata":{}},{"source":"XGB_BO.explore({\n              'max_depth':            [3, 8, 3, 8, 8, 3, 8, 3],\n              'gamma':                [0.5, 8, 0.2, 9, 0.5, 8, 0.2, 9],\n              'min_child_weight':     [0.2, 0.2, 0.2, 0.2, 12, 12, 12, 12],\n              'max_delta_step':       [1, 2, 2, 1, 2, 1, 1, 2],\n              'subsample':            [0.6, 0.8, 0.6, 0.8, 0.6, 0.8, 0.6, 0.8],\n              'colsample_bytree':     [0.6, 0.8, 0.6, 0.8, 0.6, 0.8, 0.6, 0.8],\n              })","cell_type":"code","metadata":{"collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In my version of sklearn there are many warning thrown out by the GP portion of this code. This is set to prevent them from showing on screen.\n\nIf you have a special relationship with your computer and want to know everything it is saying back, you'd probably want to remove the two \"warnings\" lines and slide the XGB_BO line all the way left.\n\nI am doing only 2 initial points, which along with 8 exploratory points above makes it 10 \"random\" parameter combinations. I'd say that 15-20 is usually adequate. For n_iter 25-50 is usually enough.\n\nThere are several commented out maximize lines that could be worth exploring. The exact combination of parameters determines **[exploitation vs. exploration](https://github.com/fmfn/BayesianOptimization/blob/master/examples/exploitation%20vs%20exploration.ipynb)**. It is tough to know which would work better without actually trying, though in my hands exploitation with \"expected improvement\" usually works the best. That's what the XGB_BO.maximize line below is specifying.","metadata":{}},{"source":"print('-'*130)\nprint('-'*130, file=log_file)\nlog_file.flush()\n\nwith warnings.catch_warnings():\n    warnings.filterwarnings('ignore')\n    XGB_BO.maximize(init_points=2, n_iter=5, acq='ei', xi=0.0)\n\n# XGB_BO.maximize(init_points=10, n_iter=50, acq='ei', xi=0.0)\n# XGB_BO.maximize(init_points=10, n_iter=50, acq='ei', xi=0.01)\n# XGB_BO.maximize(init_points=10, n_iter=50, acq='ucb', kappa=10)\n# XGB_BO.maximize(init_points=10, n_iter=50, acq='ucb', kappa=1)","cell_type":"code","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This portions gives the summary and creates a CSV file with results.","metadata":{}},{"source":"print('-'*130)\nprint('Final Results')\nprint('Maximum XGBOOST value: %f' % XGB_BO.res['max']['max_val'])\nprint('Best XGBOOST parameters: ', XGB_BO.res['max']['max_params'])\nprint('-'*130, file=log_file)\nprint('Final Result:', file=log_file)\nprint('Maximum XGBOOST value: %f' % XGB_BO.res['max']['max_val'], file=log_file)\nprint('Best XGBOOST parameters: ', XGB_BO.res['max']['max_params'], file=log_file)\nlog_file.flush()\nlog_file.close()\n\nhistory_df = pd.DataFrame(XGB_BO.res['all']['params'])\nhistory_df2 = pd.DataFrame(XGB_BO.res['all']['values'])\nhistory_df = pd.concat((history_df, history_df2), axis=1)\nhistory_df.rename(columns = { 0 : 'gini'}, inplace=True)\nhistory_df['AUC'] = ( history_df['gini'] + 1 ) / 2\nhistory_df.to_csv('Porto-AUC-5fold-XGB-run-01-v1-grid.csv')","cell_type":"code","metadata":{"collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Good luck! Let me know how it works.","metadata":{}}],"nbformat":4,"nbformat_minor":1,"metadata":{"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"},"language_info":{"mimetype":"text/x-python","name":"python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","file_extension":".py","version":"3.6.3","nbconvert_exporter":"python"}}}