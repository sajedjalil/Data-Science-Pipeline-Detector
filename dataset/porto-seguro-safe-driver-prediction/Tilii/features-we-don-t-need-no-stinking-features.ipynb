{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"file_extension":".py","mimetype":"text/x-python","pygments_lexer":"ipython3","name":"python","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.3","nbconvert_exporter":"python"}},"nbformat_minor":1,"nbformat":4,"cells":[{"metadata":{"_cell_guid":"2a347db6-b6ae-443f-ab26-5a98fe6e29f7","_uuid":"6ff52a2937a3fc9aae74700af978afd45d23f306"},"source":"There has been an ongoing discussion about the feature usefulness (or lack thereof) - see [__here__](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/41487). We also have many feature importance plots to choose from that have been provided by other Kagglers. A tried and true approach to this is [__recursive feature elimination__](https://en.wikipedia.org/wiki/Feature_selection), where we remove N features (1 <= N < total features) at a time and see how it affects our predictions. If the score goes up we toss those features, or keep them if the score gets worse. Here I use sklearn's [__recursive feature elimination wrapped with cross-validation__](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html) because in my experience it provides a very unbiased estimate of feature importance. Random Forest classifier is used primarily for speed, but you can substitute in there any tree-based method that provides information about feature importance either through a coef attribute or through a feature_importances attribute. \n\nSorry about the click-baiting title - the inspiration was [__this famous line__](https://www.youtube.com/watch?v=XT8hE7_8BCY) that just came to me as I was thinking about feature elimination.","cell_type":"markdown"},{"metadata":{"_cell_guid":"865a0d4b-2b56-459d-aeed-49acbbfd2c99","collapsed":true,"_uuid":"07d9343742caa492d8fff3acaa0d078eeb09b43d"},"source":"__author__ = 'Tilii: https://kaggle.com/tilii7'\n\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_auc_score\n\ndef timer(start_time=None):\n    if not start_time:\n        start_time = datetime.now()\n        return start_time\n    elif start_time:\n        tmin, tsec = divmod((datetime.now() - start_time).total_seconds(), 60)\n        print('\\n Time taken: %i minutes and %s seconds.' % (tmin, round(tsec, 2)))\n\ntrain = pd.read_csv('../input/train.csv', dtype={'id': np.int32, 'target': np.int8})\nX = train.drop(['id', 'target'], axis=1).values\ny = train['target'].values\ntest = pd.read_csv('../input/test.csv', dtype={'id': np.int32})\nX_test = test.drop(['id'], axis=1).values\n\nall_features = [x for x in train.drop(['id', 'target'], axis=1).columns]","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"0327b561-7381-4f79-abec-0ab3a1a64204","_uuid":"73b351369b0ea8944821f57f5f907a80b544697a"},"source":"Here we define Random Forest classifier and RFECV parameters. To test the features properly, it is probably a good idea to change n_estimators to 200 and max_depth=20 (or remove max_depth). It will take longer, on the order of 2 hours, if you choose to do so.\n\nYet another important parameter is **step**, which specifies how many features are removed at a time. Setting it to 2-5 usually works well, but set it to 1 if you want to be thorough.\n\nNote that I am specifying n_jobs=4 because Kaggle provides 4 CPUs per job. You may wish to set that to -1 so that all CPUs on your system are used. Also, the whole countdown will go 5 times because we are doing 5-fold cross-validation.","cell_type":"markdown"},{"metadata":{"_cell_guid":"4245ced6-a637-4903-a086-7a71dd69e1c2","collapsed":true,"_uuid":"48648ba686a8ff80698993048691f667d9dcb8f7"},"source":"folds = 5\nstep = 2\n\nrfc = RandomForestClassifier(n_estimators=100, max_features='sqrt', max_depth=10, n_jobs=4)\n\nrfecv = RFECV(\n              estimator=rfc,\n              step=step,\n              cv=StratifiedKFold(\n                                 n_splits=folds,\n                                 shuffle=False,\n                                 random_state=1001).split(X,y),\n              scoring='roc_auc',\n              n_jobs=1,\n              verbose=2)","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"186fe1bb-25a2-4487-bd27-169761d7a89d","_uuid":"d424556c8150801a60ef56ebfeea38733df99f44"},"source":"We estimate the feature importance and time the whole process.","cell_type":"markdown"},{"metadata":{"_cell_guid":"c12279b7-a664-41d1-ba34-bca868f9bed6","collapsed":true,"_uuid":"16178992b0c526626fe960d4021cefd318e4c5fe"},"source":"starttime = timer(None)\nstart_time = timer(None)\nrfecv.fit(X, y)\ntimer(start_time)","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"ba7b2eb7-23db-4ef1-9873-c243bcba2608","_uuid":"6b8b96261671a6bea8d7f5fdb94fb605cbee1c64"},"source":"Let's summarize the output.","cell_type":"markdown"},{"metadata":{"_cell_guid":"37c554b4-eb51-4555-8d30-c3e2a51b6b30","collapsed":true,"_uuid":"a3d4c70f27e8d457b66c1e716c0d39f83204be68"},"source":"print('\\n Optimal number of features: %d' % rfecv.n_features_)\nsel_features = [f for f, s in zip(all_features, rfecv.support_) if s]\nprint('\\n The selected features are {}:'.format(sel_features))","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"e4af9fa6-cb3b-42ce-a952-06c059656f1b","_uuid":"73133184dd479f6884207f86bc86dc425b6bbdea"},"source":"Plot number of features vs. CV scores.","cell_type":"markdown"},{"metadata":{"_cell_guid":"0bc57c13-37a7-412b-a2c7-bd42989ecc37","collapsed":true,"_uuid":"73aba053a4b43188dc46a2f3f53a6082f19c74e8"},"source":"plt.figure(figsize=(12, 9))\nplt.xlabel('Number of features tested x 2')\nplt.ylabel('Cross-validation score (AUC)')\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\nplt.savefig('Porto-RFECV-01.png', dpi=150)\nplt.show()","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"d473901c-8ca6-4c5d-9b0a-97df35ccd0a7","_uuid":"d463321a26b18b4277cb53882db87508a7b5cea0"},"source":"Save sorted feature rankings.","cell_type":"markdown"},{"metadata":{"_cell_guid":"e6d821dc-f799-4fb8-a2fc-f1e81d0e59fb","collapsed":true,"_uuid":"c7122eaaa29e73b098c35658519a6c64ed9b5059"},"source":"ranking = pd.DataFrame({'Features': all_features})\nranking['Rank'] = np.asarray(rfecv.ranking_)\nranking.sort_values('Rank', inplace=True)\nranking.to_csv('Porto-RFECV-ranking-01.csv', index=False)","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"0c04d25d-fe9a-4d94-8e4d-1333e1e91df0","_uuid":"42d90aae82fc7574423684018de0998e8c092a9d"},"source":"Make a prediction. This is only a proof-of-principle as the prediction will likely be poor until more optimal parameters are used above.","cell_type":"markdown"},{"metadata":{"_cell_guid":"92fc4377-5006-4ab3-8fcb-4cb113e695c5","collapsed":true,"_uuid":"134f960272cc6469b9a207ec92e866173d52258b"},"source":"score = round((np.max(rfecv.grid_scores_) * 2 - 1), 5)\ntest['target'] = rfecv.predict_proba(X_test)[:,1]\ntest = test[['id', 'target']]\nnow = datetime.now()\nsub_file = 'submission_5fold-RFECV-RandomForest-01_' + str(score) + '_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\nprint(\"\\n Writing submission file: %s\" % sub_file)\ntest.to_csv(sub_file, index=False)\ntimer(starttime)","execution_count":null,"cell_type":"code","outputs":[]}]}