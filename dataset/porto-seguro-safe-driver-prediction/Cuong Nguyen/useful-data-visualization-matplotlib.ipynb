{"cells":[{"outputs":[],"metadata":{"_cell_guid":"74c43420-a733-43d8-80c1-a178e12fb0c8","_uuid":"1f70666ea8d1f4149e585bcc2ad7eb85ba4f6529","collapsed":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport gc\nimport xgboost as xgb","execution_count":2},{"outputs":[],"metadata":{"_cell_guid":"e9ea19a5-3c5f-43ac-8739-0a3b90536eb6","_uuid":"b56b0d5cad317feafa21f92ea70929231cd4d538"},"cell_type":"code","source":"def visualize():\n    # config\n    train_path = '../input/train.csv'\n\n    # load data\n    print('\\nloading data ... ')\n    train_df = pd.read_csv(train_path)\n    train_df.drop(['id'], axis = 1, inplace = True)\n    print('train shape: ', train_df.shape)\n    print('feature types: ', Counter(train_df.dtypes.values))\n    print('features: ')\n    for feature in train_df:\n        print(' ', feature)\n\n    # check NA ratio\n    # replace -1 by NA because -1 in data indicate that the feature was missing \n    print('\\nchecking NA ratio ... ')\n    train_df_copied = train_df\n    train_df_copied = train_df_copied.replace(-1, np.NaN) # Values of -1 indicate that the feature was missing from the observation\"\n\n    na_ratio = (train_df_copied.isnull().sum() / len(train_df_copied)).sort_values(ascending=False)\n    print('NA ratio: ')\n    print(na_ratio)\n\n    del train_df_copied\n    gc.collect()\n    \n    # show the target feature\n    print('\\nshowing the target feature ... ')\n    zero_count = (train_df['target']==0).sum()\n    one_count = (train_df['target']==1).sum()\n    plt.bar(np.arange(2), [zero_count, one_count])\n    plt.show()\n    \n    print('target 0: ', zero_count)\n    print('target 1: ', one_count)\n    \n    # show feature's distribution\n    print('\\ndislaying distribution of features ... ')\n    for feature in train_df:\n        plt.figure(figsize=(8,6))\n        plt.scatter(range(train_df.shape[0]), np.sort(train_df[feature].values))\n        plt.xlabel('index', fontsize=12)\n        plt.ylabel(feature, fontsize=12)\n        plt.show() \n        \n    # compute features's correlation\n    print('\\ncomputing correlation of the features and showing the most positive and negative correlated features ... ')\n    f, ax = plt.subplots(figsize = (15, 15))\n    plt.title('correlation of continuous features')\n    sns.heatmap(train_df.corr(), ax = ax)\n    plt.show()\n    \n    corr_values = train_df.corr().unstack().sort_values(ascending=False)\n    print(type(corr_values))\n    for pair, value in corr_values.iteritems():\n        if abs(value) > 0.3 and abs(value) < 1.0:\n            print(pair, value)\n    \n    # binary features inpection\n    print('\\n inspecting binary features ... ')\n    bin_cols = [col for col in train_df.columns if '_bin' in col]\n    zero_list = []\n    one_list = []\n    for col in bin_cols:\n        zero_list.append((train_df[col]==0).sum())\n        one_list.append((train_df[col]==1).sum())\n        \n    plt.figure(figsize = (10, 10))\n    p1 = plt.bar(np.arange(len(bin_cols)), zero_list, width = 0.5)\n    p2 = plt.bar(np.arange(len(bin_cols)), one_list, bottom = zero_list, width = 0.5)\n    plt.xticks(np.arange(len(bin_cols)), bin_cols, rotation = 90)\n    plt.legend((p1[0], p2[0]), ('zero count', 'one count'))\n    plt.show()\n    \n    # compute feature importance\n    print('\\n computing feature importance ... ')\n    xgb_params = {\n        'eta': 0.05,\n        'max_depth': 8,\n        'subsample': 0.7,\n        'colsample_bytree': 0.7,\n        'objective': 'reg:linear',\n        'silent': 1,\n        'seed' : 0\n    }\n    \n    train_y = train_df['target'].values\n    train_x = train_df.drop(['target'], axis=1)\n\n    d_train = xgb.DMatrix(train_x, train_y, feature_names=train_x.columns.values)\n    model = xgb.train(dict(xgb_params, silent=0), d_train, num_boost_round = 100)\n    \n    importance = model.get_fscore()\n    features_df = pd.DataFrame()\n    features_df['feature'] = importance.keys()\n    features_df['fscore'] = importance.values()\n    features_df['fscore'] = features_df['fscore'] / features_df['fscore'].sum()\n    features_df.sort_values(by = ['fscore'], ascending = True, inplace = True)\n    \n    plt.figure()\n    features_df.plot(kind = 'barh', x = 'feature', y='fscore', legend = False, figsize = (10, 10))\n    plt.title('XGBoost Feature Importance')\n    plt.xlabel('fscore')\n    plt.ylabel('features')\n    plt.show()\n\n    print(features_df)\n    \n    # release\n    del train_df\n    gc.collect()\n    \nif __name__ == \"__main__\":\n    visualize()\n    print('\\n\\n\\nThe end.')","execution_count":3}],"nbformat_minor":1,"nbformat":4,"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"version":3,"name":"ipython"},"nbconvert_exporter":"python","version":"3.6.3","file_extension":".py","pygments_lexer":"ipython3","mimetype":"text/x-python","name":"python"}}}