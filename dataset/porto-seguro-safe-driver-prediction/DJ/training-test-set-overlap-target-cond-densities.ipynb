{"nbformat":4,"nbformat_minor":1,"metadata":{"language_info":{"nbconvert_exporter":"python","pygments_lexer":"ipython3","mimetype":"text/x-python","name":"python","codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","version":"3.6.1"},"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"}},"cells":[{"source":"In this kernel, I'll explore my curiosities regarding the Porto Seguro dataset. I'll answer a few questions:\n\n1. **How many unique values are there per columns? How does this change between the training and test sets?**\n2. **For categorical columns, what is the normalized histograms conditional on the target?**\n3. **For regression columns, what is the density conditional on the target?**\n\nLet's get started.","metadata":{"_uuid":"5cdaf2813490ef21e82a673997aacec1a93c1473","_cell_guid":"89684e91-1a2e-4bf9-9755-63132191f541"},"cell_type":"markdown"},{"execution_count":null,"outputs":[],"metadata":{"_uuid":"0d92fa8911862f04cd0e7006e6c9977dcb31e071","collapsed":true,"_cell_guid":"cc871894-6362-4f16-b283-06972bfccb0a"},"cell_type":"code","source":"%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')"},{"source":"Now read in our data:","metadata":{"_uuid":"a3e2ed9eacc8e58d514e49e54b67f6b52e651850","_cell_guid":"e8d793de-2501-44fd-a7cb-30c334d6127e"},"cell_type":"markdown"},{"execution_count":null,"outputs":[],"metadata":{"_uuid":"475d194566e6cdf6a4992f33ffa71b3c91c2faf4","collapsed":true,"_cell_guid":"3a813cff-c699-4849-b644-fca8c35f916d"},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')"},{"source":"## How many unique values are there per columns? How does this change between the training and test sets?","metadata":{"_uuid":"4d1f943b64a416621bc756e6d3fef227dcff6d64","_cell_guid":"7ba232a2-c27e-412f-929e-d9dd4a2d3f3b"},"cell_type":"markdown"},{"execution_count":null,"outputs":[],"metadata":{"_uuid":"6e4e7be63d516f015a388f11d538508bb0d31e4f","collapsed":true,"_cell_guid":"718714ee-821d-4f2f-8037-766bf7504a68"},"cell_type":"code","source":"unique_counts = pd.DataFrame(train.nunique(),columns=['train_nunique'])\nunique_counts = unique_counts.join(pd.DataFrame(test.nunique(),columns=['test_nunique']))"},{"source":"This will tell us how many unique counts are in each column, but I'm actually concerned with their difference. For a column, how often do values show up in the test set and not in the training set? What about visa versa? Let's answer this. ","metadata":{"_uuid":"5730354c7726c036c2a7557b2ba535e42bc936a5","_cell_guid":"1529860d-d914-4e1f-9022-0cdfdcadce3f"},"cell_type":"markdown"},{"execution_count":null,"outputs":[],"metadata":{"_uuid":"05c5a518cd7928af6fab26ac3079131cf420204b","_cell_guid":"c8de7c68-202b-446a-b0c4-d94c1319ec10"},"cell_type":"code","source":"unique_counts['count_in_train_not_test'] = np.nan\nunique_counts['count_in_test_not_train'] = np.nan\ncols = set(train.columns.values) - {'target'}\nfor c in cols:\n    train_set_c = set(train[c])\n    test_set_c = set(test[c])\n    unique_counts.loc[c, 'count_in_train_not_test'] = len(train_set_c - test_set_c)\n    unique_counts.loc[c, 'count_in_test_not_train'] = len(test_set_c - train_set_c)\nunique_counts['interesting'] = unique_counts['count_in_train_not_test'] != 0\nunique_counts['interesting'] = unique_counts['interesting'] | unique_counts['count_in_test_not_train'] != 0\nunique_counts[unique_counts['interesting']]"},{"source":"The good news is there are no categorical values that show up in one and not the other.\n\nBut these regression variables are strange. If these continuous variables had infinite precision, we would expect zero overlap. That's not the case - there is a ton of overlap! Which is good news - these make our test and training set more similar.\n\nBut what about the differences? If the differences are due to precision, but they span the same range, then these differences are no cause for concern. But if they arise because the test and train set have very different ranges for these variables, then that's a big issue!\n\nSo let's explore.","metadata":{"_uuid":"ed4e20d732c4c81b9e9598362a8fe6b61b15a717","_cell_guid":"2dbf1a8f-6037-44b5-93dc-12866df7b173"},"cell_type":"markdown"},{"execution_count":null,"outputs":[],"metadata":{"_uuid":"e70a4051bc50a71df3226a5ae86d2611b3c2c1b1","_cell_guid":"d8970336-c098-469d-9dc4-9e3a6e9727d9"},"cell_type":"code","source":"interesting_cols = list(unique_counts[unique_counts['interesting']].index[2:].values)\ntrain['which'] = 'training'\ntest['which'] = 'testing'\ncols_pick = interesting_cols + ['which','id']\nboth = train[cols_pick].append(test[cols_pick])\nboth = pd.melt(both,id_vars=['id','which'], value_vars=interesting_cols)\nplt.figure(figsize=(20,10))\nax = sns.boxplot(x=\"variable\", y=\"value\", hue=\"which\", data=both.sample(frac=.05), palette=\"Set3\")"},{"source":"OK - so they are virtually the same. Nothing to worry about here..","metadata":{"_uuid":"f2a49684d6dffb07ab0028ec95ac27ab0059ea60","_cell_guid":"9f3e1597-1aca-43ce-a196-df85a8d1acce"},"cell_type":"markdown"},{"source":"## For categorical columns, what is the normalized histograms conditional on the target?","metadata":{"_uuid":"853f9421536532382b0557d97f984bfccf5d674d","_cell_guid":"6cc7cb58-270c-4cdc-8e77-12b4a911b726"},"cell_type":"markdown"},{"execution_count":null,"outputs":[],"metadata":{},"cell_type":"code","source":"col_groups = {'categorical': ['ps_ind_08_bin','ps_calc_20_bin','ps_ind_12_bin','ps_ind_13_bin',\n                 'ps_ind_10_bin','ps_calc_18_bin','ps_ind_09_bin','ps_calc_17_bin',\n                 'ps_calc_15_bin','ps_ind_16_bin','ps_calc_19_bin','ps_ind_17_bin',\n                 'ps_ind_18_bin','ps_ind_07_bin','ps_ind_11_bin','ps_ind_06_bin',\n                 'ps_calc_16_bin','ps_car_06_cat','ps_car_01_cat','ps_car_04_cat',\n                 'ps_car_09_cat','ps_car_11_cat','ps_ind_05_cat','ps_ind_04_cat',\n                 'ps_car_08_cat','ps_car_05_cat','ps_ind_02_cat','ps_car_03_cat',\n                 'ps_car_07_cat','ps_car_02_cat','ps_car_10_cat'],\n 'regression': ['ps_calc_11','ps_ind_14','ps_calc_05','ps_car_11','ps_calc_03',\n                'ps_car_12','ps_reg_01','ps_ind_03','ps_calc_01','ps_ind_15',\n                'ps_calc_14','ps_reg_03','ps_car_13','ps_calc_13','ps_car_14',\n                'ps_calc_04','ps_calc_10','ps_reg_02','ps_calc_06','ps_calc_08',\n                'ps_calc_02','ps_calc_12','ps_car_15','ps_calc_07','ps_ind_01',\n                'ps_calc_09']}\n\ndef cat_hist_cond(cat_col):\n    #This plots the conditional (on the target) histogram for a given categorical column name\n    train_temp = train[[cat_col] + ['target']]\n    targ_counts = train_temp.groupby('target').count()\n    counts = train_temp.groupby([cat_col] + ['target']).size().reset_index(name='counts')\n    for i in range(2):\n        counts.loc[counts['target']==i,'counts'] = counts.loc[counts['target']==i,'counts']/targ_counts.loc[i].values[0]\n    if cat_col == 'ps_car_11_cat':\n        plt.figure(figsize=(24,6))\n    else:\n        plt.figure(figsize=(14,6))\n    ax = sns.barplot(x=cat_col,y='counts',hue='target',data=counts)\n    plt.title('Normalized histogram of ' + cat_col + ' for each target outcome')\n    plt.show()\n\ndef reg_dens_cond(reg_col,width):\n    #This plots the conditional (on the target) density for a given regression column name\n    train_temp = train[[reg_col] + ['target']]\n    #We standardize so we can use the same KDE width for both distributions.\n    train_temp.loc[:,reg_col] -= np.mean(train_temp.loc[:,reg_col])\n    train_temp.loc[:,reg_col] /= np.std(train_temp.loc[:,reg_col])\n    plt.figure(figsize=(14,6))\n    sns.kdeplot(train_temp.loc[train_temp['target']==0,reg_col],bw=width,label=\"targ = 0\")\n    sns.kdeplot(train_temp.loc[train_temp['target']==1,reg_col],bw=width,label=\"targ = 1\")\n    plt.title('Density of ' + reg_col + ' for each target outcome')\n    plt.show()\n\nfor cc in col_groups['categorical']:\n    cat_hist_cond(cc)\nfor cc in col_groups['regression']:\n    reg_dens_cond(cc,.2)"},{"source":"From this, I'm walking away with the following observations:\n\n1. The binary columns ps_ind_16, ps_ind_17, ps_ind_07 and ps_ind_06 have the largest difference in distribution depending on whether we condition on the target. Maybe we should cross them?\n2. For categorical columns, the following values change considerably depending on the target:\n    - [0, 1, 11, 15] for ps_car_06_cat\n    - [7, 11] for ps_car_01_cat\n    - [0] for ps_car_04_cat\n    - [104] for ps_car_11_cat\n3. For regression columns:\n    - ps_reg_01, ps_car_13, ps_reg_02, ps_ind_01 and ps_reg_03 get skewed right for target = 1\n    - ps_ind_15 get's skewed left for target = 1\n    \nI probably should have done statistical tests on the histograms.. to be continued..\n    ","metadata":{},"cell_type":"markdown"}]}