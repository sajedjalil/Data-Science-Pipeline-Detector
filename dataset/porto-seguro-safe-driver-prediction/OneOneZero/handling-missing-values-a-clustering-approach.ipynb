{"cells":[{"metadata":{"_cell_guid":"4a697c33-4596-47a1-9ffd-7e99b180c724","_uuid":"cb634975920667213fcfcd5916aea4b760063139"},"cell_type":"markdown","source":"I wanted to show you an idea I had about how to handle missing values. We'll use the porto seguro training data to see, how the idea works."},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"5c19625b-9f85-4811-8b12-a09d26189717","_uuid":"786441f3e5d6f680c858145ca2d4654c6910b3b3"},"source":"import pandas as pd\nfrom sklearn.cluster import MiniBatchKMeans\n\nX = pd.read_csv(\"../input/train.csv\", na_values = -1)\nX.drop([\"id\", \"target\"], axis = 1, inplace = True)\n\nna_count = X.isnull().sum()\nna_columns = list(na_count[na_count>0].index.values)\n\nprint(\"columns with missing values:\")\nprint(na_columns)\n\nna_count.plot(kind = \"bar\")\n"},{"metadata":{"_cell_guid":"c1a99a4b-682b-41bb-b8d4-728f8c811061","_uuid":"1e6651f327702df2c01e0dbcf31cd4a3d26be587"},"cell_type":"markdown","source":"As you can see, there are some features with a lot of missing values. So how do we handle them? Normally, I would replace nominal values with the median of the not-missing values and categorical/binary features with the most common value of the not-missing values. Below I try something more..."},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_cell_guid":"aa2e7bc7-f9e8-4b85-892d-793823b98970","_uuid":"a28d6fb195e724f2b6f492dc6a0d24d10b3f168f"},"source":"#create df only with columns with no missing values\nX_no_missing = X.drop(na_columns, axis = 1)\n \n#one hot encoding of categorical features\ncat_columns_no_missing = list(filter(lambda x: x.endswith(\"cat\"),\n                                     X_no_missing.columns.values))\nX_no_missing_oh = pd.get_dummies(X_no_missing, columns = cat_columns_no_missing)   "},{"metadata":{"_cell_guid":"097f37ed-e71e-43c5-b495-a289c5c0369b","_uuid":"62ede25e4bcaf2db7a237602ebd3d3bfb2120f8a"},"cell_type":"markdown","source":"So I drop all columns that contain missing values and then I use KMeans on the remaining columns to cluster the samples."},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"36ef098b-acb0-442c-96f7-f2775de2c41e","_uuid":"a725d65a9fd2d10df53b899da5ad3eaae4aa19d7"},"source":"#train kmeans\nkmeans = MiniBatchKMeans(n_clusters = 15, random_state = 0, batch_size = 2000)\nkmeans.fit(X_no_missing_oh)\nprint(\"Clustersize: \\n\")\nprint(pd.Series(kmeans.labels_).value_counts())\n\n#store cluster labels in df\nX[\"cluster\"] = kmeans.labels_"},{"metadata":{"_cell_guid":"ddc0c719-a720-409c-a4ce-59bca052a535","_uuid":"246a343777d544ecd38668284a6d442bc4ce154d"},"cell_type":"markdown","source":"We see that all clusters have approximately the same size. As a next step we loop over all columns containing missing values. For each column we drop the missing values and use the rest to calculate a replacement value. This would be the most common label for categorical/binary features and the median for nominal features. "},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"fa3e7e0d-fbcd-4634-9488-97d152474834","_uuid":"b4e1b8232dc99634df5a507fe13c20d5bc1e9455"},"source":"#for columns with missing values, drop missing values and find median or most common value - per cluster\nValues_replace_missing = pd.DataFrame()\n\nfor i in na_columns:\n    clean_df = X[[\"cluster\", i]].dropna()\n    if i.endswith(\"cat\"):\n        Values_replace_missing[i] = clean_df.groupby([\"cluster\"]).agg(lambda x:x.value_counts().index.values[0])\n    else:\n        Values_replace_missing[i] = clean_df.groupby([\"cluster\"]).median() \n\nprint(Values_replace_missing)"},{"metadata":{},"cell_type":"markdown","source":"As you can see, different clusters have different replacement values. This is especially prominent for \"ps_car_05_cat\". Now we have to replace the missing values with the ones we calculated above. "},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"40bec42e-31b0-44a5-afc0-f6ec85dd6eea","_uuid":"3d8467cd1f141bf87a9715f73eb83f5ee8567bf3"},"source":"    #replace missing values with median or most common value in the same cluster\n    for cl, cat in ((x, y) for x in range(15) for y in na_columns):\n        X.loc[(X[\"cluster\"] == cl) & pd.isnull(X[cat]), cat] = Values_replace_missing.loc[cl, cat]\n    \n    #print remaining missing values (should be zero)\n    print(\"\\n remaining missing values: \" + str(X.isnull().sum().sum()))"},{"metadata":{"collapsed":true,"_cell_guid":"9dede6d6-309d-465b-9f6f-0c1583a1d441","_uuid":"135ec22982c57106038bcc7f2f733a958ff9969d"},"cell_type":"markdown","source":"I have not tested the impact of this approach on the prediction quality but maybe this is interesting for you, too. So what do you think: does this approach make sense?"}],"metadata":{"language_info":{"file_extension":".py","mimetype":"text/x-python","name":"python","version":"3.6.3","nbconvert_exporter":"python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3"},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"}},"nbformat_minor":1,"nbformat":4}