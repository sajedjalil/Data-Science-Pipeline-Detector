{"metadata":{"language_info":{"version":"3.6.3","codemirror_mode":{"version":3,"name":"ipython"},"name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","file_extension":".py","mimetype":"text/x-python"},"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"}},"nbformat":4,"cells":[{"source":"This kernel provides examples of Python implementation of tree-based algorithms. The Porto Seguro's dataset is a good case study to begin with, as it is a binary classification problem (0/1), both the number of observations and features are not too large nor too small (training is quite fast) and we have to deal with missing values and imbalanced target variable.\n\nWe start by building a single decision tree, where maximum depth is chosen via cross validation (either via cross_val_score or GridSearchCV). This technique does not lead to good out-of-sample performance. We then try Random Forest algorithm to increase robustness, but only a small improvement was achieved. Stronger Gini score is found using gradient boosting, such as XGBoost and Light GBM. While XGBoost allows us to post our highest score (0.276) on the leaderboard, Light GBM provides very similar performance on the test set with much less training time than XGBoost.\n\nSome parts of the code have been sourced from multiple kernels of this competition.","metadata":{"_cell_guid":"2c26a6fe-2713-4ec0-83db-5d49c2590949","_uuid":"7bb0241d2a00847857a6488eec2760d456c4d843"},"cell_type":"markdown"},{"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","metadata":{"_cell_guid":"0ad7e78a-3bd5-43ba-8257-beb50b0b2539","_uuid":"f3c0bf7d83ea55e4c457077248b5fb660b01cc5c"},"cell_type":"code","execution_count":4},{"outputs":[],"source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport graphviz\n\nimport gc\n\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nimport lightgbm as lgb\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import make_scorer","metadata":{"_cell_guid":"1f996537-37aa-4769-9623-95b37373d2c5","collapsed":true,"_uuid":"1e313603f23e24221fda3b8a103c45342ffb03c1"},"cell_type":"code","execution_count":2},{"outputs":[],"source":"rawdata_train = pd.read_csv(\"../input/train.csv\", sep = ',',na_values = -1)\nrawdata_test = pd.read_csv(\"../input/test.csv\", sep = ',',na_values = -1)","metadata":{"_cell_guid":"7d4e2782-28f1-46b0-a1ac-07c4e4d97ead","collapsed":true,"_uuid":"c011b51243673b2c7e1649c8ddc82c0f20a22c76"},"cell_type":"code","execution_count":5},{"source":"**Fix missing values**\n\nFirst, we visualize which features have this issue. Those with too many missing values are dropped. The others have their missing entries replaced by the mode value","metadata":{"_cell_guid":"8c8c926f-b9de-4fdf-b0f7-2d202a24e0f5","_uuid":"144ec8bfbc17be8f50418d5120d6922c5e15eef1"},"cell_type":"markdown"},{"outputs":[],"source":"def describe_missing_values(df):\n    na_percent = {}\n    N = df.shape[0]\n    for column in df:\n        na_percent[column] = df[column].isnull().sum() * 100 / N\n\n    na_percent = dict(filter(lambda x: x[1] != 0, na_percent.items()))\n    plt.bar(range(len(na_percent)), na_percent.values())\n    plt.ylabel('Percent')\n    plt.xticks(range(len(na_percent)), na_percent.keys(), rotation='vertical')\n    plt.show()","metadata":{"_cell_guid":"6f4f0e45-b86d-47cb-b4ff-a137c67ff8c1","collapsed":true,"_uuid":"a09323d43e8392bbbb681e5ac9124f8263c54fee"},"cell_type":"code","execution_count":7},{"outputs":[],"source":"print(\"Missing values for train set\")\ndescribe_missing_values(rawdata_train)\nprint(\"Missing values for test set\")\ndescribe_missing_values(rawdata_test)","metadata":{"_cell_guid":"eba3c2e8-40bb-48c1-bd25-0066a6c6d0f3","_uuid":"8c543176774e310eb93d928611f7e7a98f83605c"},"cell_type":"code","execution_count":8},{"outputs":[],"source":"X = rawdata_train.drop({'target','id','ps_car_03_cat','ps_car_05_cat'},axis=1)\nY = rawdata_train['target']\nX_test = rawdata_test.drop({'id','ps_car_03_cat','ps_car_05_cat'},axis=1)\n\ncat_cols = [col for col in X.columns if 'cat' in col]\nbin_cols = [col for col in X.columns if 'bin' in col]\ncon_cols = [col for col in X.columns if col not in bin_cols + cat_cols]\n\nfor col in cat_cols:\n    X[col].fillna(value=X[col].mode()[0], inplace=True)\n    X_test[col].fillna(value=X_test[col].mode()[0], inplace=True)\n    \nfor col in bin_cols:\n    X[col].fillna(value=X[col].mode()[0], inplace=True)\n    X_test[col].fillna(value=X_test[col].mode()[0], inplace=True)\n    \nfor col in con_cols:\n    X[col].fillna(value=X[col].mean(), inplace=True)\n    X_test[col].fillna(value=X_test[col].mean(), inplace=True)","metadata":{"_cell_guid":"aeb9980d-b1a5-4228-ba4f-2c7b01859218","collapsed":true,"_uuid":"67d022c269e45ec8d3b90e9c5e831c8c7aaba90f"},"cell_type":"code","execution_count":10},{"source":"**Defining Gini scoring metric**\n\nAs the submissions are evaluated using the Normalized Gini Coefficient, we create a scoring function, which will be fed into the algorithms.","metadata":{"_cell_guid":"e3848068-9c4e-4240-9e80-d6c46d62adb0","_uuid":"53eb159438002e3247bc81fb4c761f7e7b0acc1c"},"cell_type":"markdown"},{"outputs":[],"source":"def gini(actual, pred):\n    assert (len(actual) == len(pred))\n    all = np.asarray(np.c_[actual, pred, np.arange(len(actual))], dtype=np.float)\n    all = all[np.lexsort((all[:, 2], -1 * all[:, 1]))]\n    totalLosses = all[:, 0].sum()\n    giniSum = all[:, 0].cumsum().sum() / totalLosses\n\n    giniSum -= (len(actual) + 1) / 2.\n    return giniSum / len(actual)\n\n\ndef gini_normalized_score(actual, pred):\n    return gini(actual, pred) / gini(actual, actual)\n\nscore_gini = make_scorer(gini_normalized_score, greater_is_better=True, needs_threshold = True)","metadata":{"_cell_guid":"b00fbd06-769d-4b56-aecc-dbedebd09978","collapsed":true,"_uuid":"63bc50765ea61c8b3006e2883c8f747cfbc933d3"},"cell_type":"code","execution_count":12},{"source":"**Single Classification Tree**\n\nWe choose the tree depth via cross validation. One can use either cross_val_score (it requires to manually code a loop over the depth possibilities) or GridSearchCV do all the work for you. In both cases, we see that strongest gini score is achieved with 8 as max depth.","metadata":{"_cell_guid":"3104e467-98d4-4094-9cbd-859559fa6a6b","_uuid":"c9f6d0281096312ae805cda8df46b9145200e4c4"},"cell_type":"markdown"},{"outputs":[],"source":"depth_gini = []\nfor i in range(3,15):\n    clf = tree.DecisionTreeClassifier(max_depth=i)\n    # Perform 5-fold cross validation\n    scores_gini = cross_val_score(clf, X, Y, cv=5, scoring = score_gini)\n    depth_gini.append((i,scores_gini.mean()))\nplt.plot(*zip(*depth_gini))\nplt.xlabel('tree depth')\nplt.ylabel('cv gini score')\nplt.show()","metadata":{"_cell_guid":"6064e2e0-1227-4677-9a6d-c7cea9b2af39","_uuid":"802ff4986415d850f1eb422dee365848a4b5aa6e"},"cell_type":"code","execution_count":19},{"outputs":[],"source":"parameters = {'max_depth': np.arange(3,15)}\nclf = GridSearchCV(estimator = tree.DecisionTreeClassifier(), param_grid = parameters, scoring = score_gini, cv = 5)\nclf.fit(X, Y)\nprint(\"Best parameters set found on development set:\")\nprint()\nprint(clf.best_estimator_)\nprint()\nprint(\"Grid scores on development set:\")\nprint()\nfor params, mean_score, scores in clf.grid_scores_:\n    print(\"%0.3f (+/-%0.03f) for %r\"\n            % (mean_score, scores.std() / 2, params))\nprint()","metadata":{"_cell_guid":"60465a1a-0af6-4c25-9532-3237f7babc80","_uuid":"08d92a310c8ff4113a24311003f955bb9e5fd3b3"},"cell_type":"code","execution_count":20},{"outputs":[],"source":"clf = tree.DecisionTreeClassifier(max_depth=8)\nclf = clf.fit(X,Y)\nY_pred_clf = clf.predict(X)\nY_pred_proba_clf = clf.predict_proba(X)\nY_pred_clf = clf.predict(X)","metadata":{"_cell_guid":"04134285-3939-4760-85ae-5a9d3d388d1e","collapsed":true,"_uuid":"41153709468abb76bf37350df1c75065fe4a43c6"},"cell_type":"code","execution_count":24},{"source":"Graphviz allows to visualize the tree.","metadata":{"_cell_guid":"c57cb936-9b17-4bae-a63c-22d47d8db532","_uuid":"0493f4851a741c84726d537d02eaa5d4346ebf50"},"cell_type":"markdown"},{"outputs":[],"source":"dot_data = tree.export_graphviz(clf,out_file=None)\ngraph = graphviz.Source(dot_data)\ngraph","metadata":{"_cell_guid":"81838dc0-7bef-4cea-a26d-002b920349ee","_uuid":"1abe6ee8909cebd3225c1b680f483433f12d8284"},"cell_type":"code","execution_count":25},{"source":"**Random Forest**\n\nWe use the same approach to choose max depth of a Random Forest Classifier.","metadata":{"_cell_guid":"3b154613-649d-4330-93e2-d72eefd0e038","_uuid":"ac07711355fb9302cb15a71b276e2e99dd7e8dc3"},"cell_type":"markdown"},{"outputs":[],"source":"depth_gini = []\nfor i in range(3,15):\n    rf = RandomForestClassifier(max_depth=i)\n    # Perform 5-fold cross validation\n    scores_gini = cross_val_score(rf, X, Y, cv=5, scoring = score_gini)\n    depth_gini.append((i,scores_gini.mean()))\nplt.plot(*zip(*depth_gini))\nplt.xlabel('tree depth')\nplt.ylabel('cv gini score')\nplt.show()","metadata":{"_cell_guid":"5686a11c-404c-4a7e-a0cf-b0128d42002e","_uuid":"fbe4d5e23215b42ca41bacee56eb30e51b0a44ce"},"cell_type":"code","execution_count":26},{"outputs":[],"source":"rf = RandomForestClassifier(max_depth=8)\nrf = rf.fit(X,Y)\nY_pred_rf = rf.predict(X)\nY_pred_proba_rf = rf.predict_proba(X)\nY_pred_rf = rf.predict(X)","metadata":{"_cell_guid":"5511dba0-6488-4c3a-a064-8d3b05bf741b","collapsed":true,"_uuid":"19161a6a0a6a5acadc99cfa2aa1c89f1bdea9f95"},"cell_type":"code","execution_count":28},{"source":"It may be useful to plot feature importance resulting from the training of Random Forest.","metadata":{"_cell_guid":"7bdeb980-18ef-4be5-b111-d1d4d9ad9618","_uuid":"c2edb4f929d3033fd3342358859ac79b7c639cf3"},"cell_type":"markdown"},{"outputs":[],"source":"importances = rf.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in rf.estimators_],\n             axis=0)\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in range(X.shape[1]):\n    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n\n# Plot the feature importances of the forest\nplt.figure()\nplt.title(\"Feature importances\")\nplt.bar(range(X.shape[1]), importances[indices],\n       color=\"r\", yerr=std[indices], align=\"center\")\nplt.xticks(range(X.shape[1]), indices)\nplt.xlim([-1, X.shape[1]])\nplt.show()","metadata":{"_cell_guid":"923cbe38-97cb-455f-9c7b-cc2c5542839a","_uuid":"0eaf79b10b2e004027a9f59d261dc26666223a43"},"cell_type":"code","execution_count":29},{"source":"**Comparison of Single Classification Tree and Random Forest**\n\nAn easy method to compare performance of these two models is to plot the ROC curve, and calculate the AUC. We find that Random Forest achieves slightly higher AUC than a single decision tree.","metadata":{"_cell_guid":"1aca1f59-b398-4787-bb0d-8a8ca7a117a2","_uuid":"cc591808c271d3b1c8508c849eccfd6f13e83ad6"},"cell_type":"markdown"},{"outputs":[],"source":"confusion_matrix(Y,Y_pred_clf)","metadata":{"_cell_guid":"40f5fe6b-9968-419a-b1f4-c03f0b90ef1d","_uuid":"cf9b022488f7e231b12a159a437e430be9df7cbe"},"cell_type":"code","execution_count":31},{"outputs":[],"source":"fpr_clf, tpr_clf, thresholds_clf = roc_curve(Y,Y_pred_proba_clf[:,1],pos_label = 1)\nfpr_rf, tpr_rf, thresholds_rf = roc_curve(Y,Y_pred_proba_rf[:,1],pos_label = 1)\nplt.plot(fpr_clf,tpr_clf)\nplt.plot(fpr_rf,tpr_rf)\nplt.xlabel('false positive rate')\nplt.ylabel('true positive rate')\nplt.show()\nauc_clf = np.trapz(tpr_clf,fpr_clf)\nauc_rf = np.trapz(tpr_rf,fpr_rf)\nprint(auc_clf)\nprint(auc_rf)","metadata":{"_cell_guid":"f19252e2-cd86-4d88-84ff-71ef95f85e2d","_uuid":"eee6020d0c5dec12cd8bbe1350bd3139569ea7c8"},"cell_type":"code","execution_count":33},{"source":"**XGBoost**\n\nSingle Classification Tree and Random Forest delivered poor prediction performance so far. We now try XGBoost and Light GBM. We increase the gini score from 0.24 with previous algorithms to 0.275. XGBoost takes longer to train.","metadata":{"_cell_guid":"4936540d-b6a3-4e67-abaa-d2176d25846d","_uuid":"ca265bd65162cc8717c0fe1244ce56f30f379a78"},"cell_type":"markdown"},{"outputs":[],"source":"X = rawdata_train.drop({'target','id'},axis=1)\nY = rawdata_train['target']\nX_test = rawdata_test","metadata":{"_cell_guid":"1a18a0d2-eb4f-4342-b3c7-dd4d4943a001","collapsed":true,"_uuid":"1304a59b1eabec63987415bb2e9a6a2f796f7c82"},"cell_type":"code","execution_count":34},{"outputs":[],"source":"# Create an XGBoost-compatible metric from Gini\ndef gini_xgb(preds, dtrain):\n    labels = dtrain.get_label()\n    gini_score = gini_normalized_score(labels, preds)\n    return [('gini', gini_score)]","metadata":{"_cell_guid":"05678495-7546-4d5f-808e-bd5e07bbc576","collapsed":true,"_uuid":"a8dc593a496b378eec10825da5c5b931cbe0e837"},"cell_type":"code","execution_count":35},{"outputs":[],"source":"params = {'eta': 0.2,\n          'max_depth': 4,\n          'objective': 'binary:logistic',\n          'eval_metric': 'auc',\n          'silent': True}\n\nfeatures = X.columns\nsubmission = X_test['id'].to_frame()\nsubmission['target']=0\n\nkfold = 3\nskf = StratifiedKFold(n_splits=kfold)\nfor i, (train_index, test_index) in enumerate(skf.split(X, Y)):\n    print(' xgb kfold: {}  of  {} : '.format(i+1, kfold))\n    X_train, X_valid = X.loc[train_index], X.loc[test_index]\n    Y_train, Y_valid = Y.loc[train_index], Y.loc[test_index]\n    d_train = xgb.DMatrix(X_train, Y_train) \n    d_valid = xgb.DMatrix(X_valid, Y_valid) \n    watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n    xgb_model = xgb.train(params, d_train, 1000, watchlist, early_stopping_rounds=100, \n                        feval=gini_xgb, maximize=True, verbose_eval=100)\n    submission['target'] += xgb_model.predict(xgb.DMatrix(X_test[features]), \n                        ntree_limit=xgb_model.best_ntree_limit+50) / (kfold)\ngc.collect()\nsubmission.head(2)","metadata":{"_cell_guid":"a6d17499-fea1-4bfe-9250-641d6ad4d4f6","collapsed":true,"_uuid":"9d6f3e728960eae69368c9ce0b1e8121ebf5be89"},"cell_type":"code","execution_count":null},{"source":"GridSearchCV also works with XGBoost.","metadata":{"_cell_guid":"8ec9164d-8d7f-46b4-a921-34937ca388eb","_uuid":"f9c414a08eb06d3dd60846a391cd572e54562266"},"cell_type":"markdown"},{"outputs":[],"source":"parameters = {'max_depth': np.arange(3,7),\n            'learning_rate': [0.2],\n             'n_estimators': [20,100]}\n\nclf = GridSearchCV(estimator = xgb.XGBClassifier(silent=True), param_grid = parameters, scoring = score_gini, cv = 3, verbose = 10, n_jobs = -1)\nclf.fit(X, Y)","metadata":{"_cell_guid":"d907429a-f4af-4431-94f0-ff5af7036187","collapsed":true,"_uuid":"272dabda2c9ab43ebef7f0a147570ce6b817f838"},"cell_type":"code","execution_count":null},{"source":"**Light GBM**","metadata":{"_cell_guid":"2103ef3c-b3b1-4438-aef1-5582a57d3a65","_uuid":"9748f756e0e17b8b293f4da785920b244fd640bf"},"cell_type":"markdown"},{"outputs":[],"source":"def gini_lgb(preds, dtrain):\n    labels = dtrain.get_label()\n    gini_score = gini_normalized_score(labels, preds)\n    return [('gini', gini_score, True)]","metadata":{"_cell_guid":"a88ab4f5-4a88-47ae-8426-a324ccbf9014","collapsed":true,"_uuid":"fe4154015817ed292535b7709e278b419e3d3d50"},"cell_type":"code","execution_count":36},{"outputs":[],"source":"params = {'learning_rate' : 0.2, 'max_depth':6, 'max_bin':10,  'objective': 'binary', \n        'metric': 'auc'}\n\nfeatures = X.columns\nsubmission = X_test['id'].to_frame()\nsubmission['target']=0\n\nkfold = 5\nskf = StratifiedKFold(n_splits=kfold)\nfor i, (train_index, test_index) in enumerate(skf.split(X, Y)):\n    print(' lgb kfold: {}  of  {} : '.format(i+1, kfold))\n    X_train, X_valid = X.loc[train_index], X.loc[test_index]\n    Y_train, Y_valid = Y.loc[train_index], Y.loc[test_index]\n    lgb_model = lgb.train(params, lgb.Dataset(X_train, label=Y_train), 400, \n                  lgb.Dataset(X_valid, label=Y_valid), verbose_eval=100, \n                  feval=gini_lgb, early_stopping_rounds=50)\n    submission['target'] += lgb_model.predict(X_test[features], \n                        num_iteration=lgb_model.best_iteration) / (kfold)\ngc.collect()\nsubmission.head(2)","metadata":{"_cell_guid":"0857a9b5-012a-4709-9206-08caccae314f","_uuid":"4da1239ebf0a53084be94aa7aa6f04e06dd43815"},"cell_type":"code","execution_count":38},{"source":"**Submit predictions**","metadata":{"_cell_guid":"83b223b6-0bd2-4c7f-ac47-1ef6a7f0db9d","_uuid":"9b22f27b46ff27fd7da140ef39815a25456eeac4"},"cell_type":"markdown"},{"outputs":[],"source":"submission.to_csv(\"./submission.csv\", index=False, float_format='%.5f')","metadata":{"_cell_guid":"a96743da-891f-4e8d-b6c8-89f2803f7532","collapsed":true,"_uuid":"6ef7bbdabb1eeaef6cd058bc70fc218962222710"},"cell_type":"code","execution_count":39}],"nbformat_minor":1}