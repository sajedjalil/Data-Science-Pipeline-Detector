{"cells":[{"metadata":{"_cell_guid":"907772f8-e7b0-4b30-b144-cc58259c9f10","_uuid":"ca9caf3a492d2078ef878a7306614f8d9b84f4dd"},"source":"Also found on my Github profile Mizioand,\nhttps://github.com/MizioAnd/PortoSeguroInsur/blob/master/porto_seguro_insur.py","cell_type":"markdown"},{"outputs":[],"execution_count":null,"metadata":{"_cell_guid":"8bf94415-ceb5-4709-9315-2e699625d640","_uuid":"ce928bbdb584c99cf8b985f8f4fa4e91c9eeee47"},"source":"# porto_seguro_insur.py\n#  Assumes python vers. 3.6\n# __author__ = 'mizio'\n\nimport csv as csv\nimport numpy as np\nimport pandas as pd\nimport pylab as plt\nfrom fancyimpute import MICE\nimport random\nfrom sklearn.model_selection import cross_val_score\nimport datetime\nimport tensorflow as tf\n\n\nclass PortoSeguroInsur:\n    def __init__(self):\n        self.df = PortoSeguroInsur.df\n        self.df_test = PortoSeguroInsur.df_test\n        self.df_submission = PortoSeguroInsur.df_submission\n        self.timestamp = datetime.datetime.now().strftime('%Y%m%d_%Hh%Mm%Ss')\n\n\n    # Load data into Pandas DataFrame\n    # For .read_csv, always use header=0 when you know row 0 is the header row\n    df = pd.read_csv('../input/train.csv', header=0)\n    df_test = pd.read_csv('../input/test.csv', header=0)\n    df_submission = pd.read_csv('../input/sample_submission.csv', header=0)\n\n    @staticmethod\n    def features_with_null_logical(df, axis=1):\n        row_length = len(df._get_axis(0))\n        # Axis to count non null values in. aggregate_axis=0 implies counting for every feature\n        aggregate_axis = 1 - axis\n        features_non_null_series = df.count(axis=aggregate_axis)\n        # Whenever count() differs from row_length it implies a null value exists in feature column and a False in mask\n        mask = row_length == features_non_null_series\n        return mask\n\n    def missing_values_in_dataframe(self, df):\n        mask = self.features_with_null_logical(df)\n        print(df[mask[mask == 0].index.values].isnull().sum())\n        print('\\n')\n\n    @staticmethod\n    def extract_numerical_features(df):\n        df = df.copy()\n        df = df.copy()\n        non_numerical_feature_names = df.columns[np.where(PortoSeguroInsur.numerical_feature_logical_incl_hidden_num(\n            df) == 0)]\n        return non_numerical_feature_names\n\n    @staticmethod\n    def extract_non_numerical_features(df):\n        df = df.copy()\n        non_numerical_feature_names = df.columns[np.where(PortoSeguroInsur.numerical_feature_logical_incl_hidden_num(\n            df))]\n        return non_numerical_feature_names\n\n    @staticmethod\n    def numerical_feature_logical_incl_hidden_num(df):\n        logical_of_non_numeric_features = np.zeros(df.columns.shape[0], dtype=int)\n        for ite in np.arange(0, df.columns.shape[0]):\n            try:\n                str(df[df.columns[ite]][0]) + df[df.columns[ite]][0]\n                logical_of_non_numeric_features[ite] = True\n            except TypeError:\n                print('Oops')\n        return logical_of_non_numeric_features\n\n    def clean_data(self, df, is_train_data=1):\n        df = df.copy()\n        if df.isnull().sum().sum() > 0:\n            if is_train_data:\n                df = df.dropna()\n            else:\n                df = df.dropna(1)\n        return df\n\n    def reformat_data(self, labels, num_labels):\n        # Map labels/target value to one-hot-encoded frame. None is same as implying newaxis() just replicating array\n        # if num_labels > 2:\n        labels = (np.arange(num_labels) == labels[:, None]).astype(np.float64)\n        return labels\n\n    def accuracy(self, predictions, labels):\n        # Sum the number of cases where the predictions are correct and divide by the number of predictions\n        number_of_correct_predictions = np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n        return 100*number_of_correct_predictions/predictions.shape[0]\n\n    def linear_model(self, input_vector, weight_matrix, bias_vector):\n        # f(x) = Wx + b\n        # W is the weight matrix with elements w_ij\n        # x is the input vector\n        # b is the bias vector\n        # In the machine learning literature f(x) is called an activation\n        return tf.matmul(input_vector, weight_matrix) + bias_vector\n\n    def activation_out(self, logit):\n        return self.activation(logit, switch_var=0)\n\n    def activation_hidden(self, logit):\n        return self.activation(logit, switch_var=0)\n\n    def activation(self, logit, switch_var=0):\n        # Also called the activation function\n        if switch_var == 0:\n            # Logistic sigmoid function.\n            # sigma(a) = 1/(1+exp(-a))\n            return tf.nn.sigmoid(logit)\n        elif switch_var == 1:\n            # Using Rectifier as activation function. Rectified linear unit (ReLU). Compared to sigmoid or other\n            # activation functions it allows for faster and effective training of neural architectures.\n            # f(x) = max(x,0)\n            return tf.nn.relu(logit)\n        else:\n            # Softmax function.\n            # S(y_i) = e^y_i/(Sum_j e^y_j)\n            return tf.nn.softmax(logit)\n\n\ndef main():\n    porto_seguro_insur = PortoSeguroInsur()\n    df = porto_seguro_insur.df.copy()\n    df_test = porto_seguro_insur.df_test.copy()\n    df_submission = porto_seguro_insur.df_submission.copy()\n\n    df = df.replace(-1, np.NaN)\n    df_test = df_test.replace(-1, np.NaN)\n\n    print(df.shape)\n    print(df_test.shape)\n    # Clean data for NaN\n    df = porto_seguro_insur.clean_data(df)\n    df_test = porto_seguro_insur.clean_data(df_test, is_train_data=0)\n    print('df_test.shape: %s' % str(df_test.shape))  # (892816, 46)\n    # df_test = porto_seguro_insur.clean_data(df_test, is_train_data=0)\n    id_df_test = df_test['id']  # Submission column\n    print(\"After dropping NaN\")\n    print(df.shape)\n    print(df_test.shape)\n\n    is_explore_data = 1\n    if is_explore_data:\n        # Overview of train data\n        print('\\n TRAINING DATA:----------------------------------------------- \\n')\n        print(df.head(3))\n        print('\\n')\n        print(df.info())\n        print('\\n')\n        print(df.describe())\n        print('\\n')\n        print(df.dtypes)\n        print(df.get_dtype_counts())\n\n        # missing_values\n        print('All df set missing values')\n        porto_seguro_insur.missing_values_in_dataframe(df)\n\n        print('Uniques')\n        uniques_in_id = np.unique(df.id.values).shape[0]\n        print(uniques_in_id)\n        print('uniques_in_id == df.shape[0]')\n        print(uniques_in_id == df.shape[0])\n\n        # Overview of sample_submission format\n        print('\\n sample_submission \\n')\n        print(df_submission.head(3))\n        print('\\n')\n        print(df_submission.info())\n        print('\\n')\n\n    is_prepare_data = 1\n    if is_prepare_data:\n        df_test_num_features = porto_seguro_insur.extract_numerical_features(df_test)\n        df_y = df.loc[:, ['target']]\n        df = df.loc[:, df_test_num_features]\n\n    is_prediction = 1\n    if is_prediction:\n        # Subset the data to make it run faster\n        # (595212, 59)\n        # (892816, 58)\n        # After dropping NaN\n        # (124931, 59)\n        # (186567, 58)\n        subset_size = 10000\n\n        num_labels = np.unique(df_y.loc[:subset_size, 'target'].values).shape[0]\n        num_columns = df[(df.columns[(df.columns != 'target') & (df.columns != 'id')])].shape[1]\n        # Reformat datasets\n        x_train = df.loc[:subset_size, (df.columns[(df.columns != 'target') & (df.columns != 'id')])].values\n        y_train = df_y.loc[:subset_size, 'target'].values\n        # We only need to one-hot-encode our labels since otherwise they will not match the dimension of the\n        # logits in our later computation.\n        y_train = porto_seguro_insur.reformat_data(y_train, num_labels=num_labels)\n\n        # Todo: we need testdata with labels to benchmark test results.\n        # Hack. Use subset of training data (not used in training model) as test data, since it has a label/target value\n        # In case there are duplicates in training data it may imply that test results are too good, when using\n        # a subset of training data for test.\n        # Todo: do cross-validation instead of only one subset testing as below in x_val\n        # Validation data is a subset of training data.\n        x_val = df.loc[subset_size:2*subset_size, (df.columns[(df.columns != 'target') & (df.columns != 'id')])].values\n        y_val = df_y.loc[subset_size:2*subset_size, 'target'].values\n        y_val = porto_seguro_insur.reformat_data(y_val, num_labels=num_labels)\n        # Test data.\n        x_test = df_test.loc[:, (df_test.columns[(df_test.columns != 'id')])].values\n\n        # Todo: we need validation data with labels to perform crossvalidation while training and get a better result.\n\n        # Tensorflow uses a dataflow graph to represent your computations in terms of dependencies.\n        graph = tf.Graph()\n        with graph.as_default():\n            # Load training and test data into constants that are attached to the graph\n            tf_train = tf.constant(x_train)\n            tf_train_labels = tf.constant(y_train)\n            tf_val = tf.constant(x_val)\n            tf_test = tf.constant(x_test)\n\n            # As in a neural network the goal is to compute the cross-entropy D(S(w,x), L)\n            # x, input training data\n            # w_ij, are elements of the weight matrix\n            # L, labels or target values of the training data (classification problem)\n            # S(), is softmax function\n            # Do the Multinomial Logistic Classification\n            # step 1.\n            # Compute y from the linear model y = WX + b, where b is the bias and W is randomly chosen on\n            # Gaussian distribution and bias is set to zero. The result is called the logit.\n            # step 2.\n            # Compute the softmax function S(Y) which gives distribution\n            # step 3.\n            # Compute cross-entropy D(S, L) = - Sum_i L_i*log(S_i)\n            # step 4.\n            # Compute loss L = 1/N * D(S, L)\n            # step 5.\n            # Use gradient-descent to find minimum of loss wrt. w and b by minimizing L(w,b).\n            # Update your weight and bias until minimum of loss function is reached\n            # w_i -> w_i - alpha*delta_w L\n            # b -> b - alpha*delta_b L\n            # OBS. step 5 is faster optimized if you have transformed the data to have zero mean and equal variance\n            # mu(x_i) = 0\n            # sigma(x_i) = sigma(x_j)\n            # This transformation makes it a well conditioned problem.\n\n            # Make a 2-layer Neural network (count number of layers of adaptive weights) with num_columns nodes\n            # in hidden layer.\n            # Initialize weights on truncated normal distribution. Initialize biases to zero.\n            # For every input vector corresponding to one sample we have D features s.t.\n            # a_j = Sum_i^D w^(1)_ji x_i + w^(1)_j0 , where index j is the number of nodes in the first hidden layer\n            # and it runs j=1,...,M\n            # Vectorizing makes the notation more compact\n            #     | --- x_1 --- |\n            #     | --- x_2 --- |\n            # X = | --- ..  --- |\n            #     | --- x_N --- |\n            # where each x is now a sample vector of dimension (1 x D) and where N is the number of samples.\n            # Similarly, define a tiling of N weight matrices w,\n            #     | --- w --- |\n            #     | --- w --- |\n            # W = | --- ..--- |\n            #     | --- w --- |\n            # where each w is now a matrix of dimension (M x D)\n            # We now form the tensor product between W and X but we need to transpose X as x.T to get (M x D).(D x 1)\n            # multiplication,\n            #       |  w.(x_1.T) |\n            #       |  w.(x_2.T) |\n            # W.X = |  ..        |\n            #       |  w.(x_N.T) |\n            # with W.X having dimensions (M*N x 1).\n            # Additionally, define a tiling of N bias vectors b that each are of dimension (M x 1),\n            #     |  b  |\n            #     |  b  |\n            # B = |  .. |\n            #     |  b  |\n            # with B having dimensions (M*N x 1).\n            # Finally, the activation is a (M*N x 1) vector given as A = W.X + B.\n            # Next, this is passed to an activation function like a simoid and then inserted in second layer of the NN.\n            # Let Z = sigmoid(A)\n            # Let C be the activation of the second layer,\n            # C = W^(2).Z + B^(2)\n            # where W^(2) is the tiling N second layer weight matrices w^(2) each with dimension (K x M). K is the\n            # number of outputs in the classification. The dimension of C is (K x N).\n            # Lastly, apply the sigmoid function to get the predictions\n            # P = sigmoid( C )\n            # which has dimensions (K x N) and is as expected an output vector (K x 1) for every N samples in our\n            # dataset. The output (K x 1)-vector is in a one-hot-encoded form.\n\t\t\t\n\t\t\t# Choose number of nodes > than number of features.\n            M_nodes = x_train.shape[1]\n            weights_1_layer = tf.Variable(tf.truncated_normal([num_columns, M_nodes], dtype=np.float64))\n            biases_1_layer = tf.Variable(tf.zeros([M_nodes], dtype=np.float64))\n            weights_2_layer = tf.Variable(tf.truncated_normal([M_nodes, num_labels], dtype=np.float64))\n            biases_2_layer = tf.Variable(tf.zeros([num_labels], dtype=np.float64))\n\n            # Logits and loss function.\n            logits_hidden_1_layer = porto_seguro_insur.linear_model(tf_train, weights_1_layer, biases_1_layer)\n            # Output unit activations of first layer\n            a_1_layer = porto_seguro_insur.activation_hidden(logits_hidden_1_layer)\n            logits_2_layer = porto_seguro_insur.linear_model(a_1_layer, weights_2_layer, biases_2_layer)\n            switch_var = 0\n            if switch_var == 1:\n                loss_function = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels,\n                                                                                       logits=logits_2_layer))\n            else:\n                loss_function = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf_train_labels,\n                                                                                       logits=logits_2_layer))\n\n            # Find minimum of loss function using gradient-descent.\n            optimized_weights_and_bias = tf.train.GradientDescentOptimizer(0.5).minimize(loss=loss_function)\n\n            # Accuracy variables using the initial values for weights and bias of our linear model.\n            train_prediction = porto_seguro_insur.activation_out(logits_2_layer)\n            # Applying optimized weights and bias to validation data\n            logits_hidden_1_layer_val = porto_seguro_insur.linear_model(tf_val, weights_1_layer, biases_1_layer)\n            a_1_layer_val = porto_seguro_insur.activation_hidden(logits_hidden_1_layer_val)\n            logits_2_layer_val = porto_seguro_insur.linear_model(a_1_layer_val, weights_2_layer, biases_2_layer)\n            val_prediction = porto_seguro_insur.activation_out(logits_2_layer_val)\n\n            # Applying optimized weights and bias to test data\n            logits_hidden_1_layer_test = porto_seguro_insur.linear_model(tf_test, weights_1_layer, biases_1_layer)\n            a_1_layer_test = porto_seguro_insur.activation_hidden(logits_hidden_1_layer_test)\n            logits_2_layer_test = porto_seguro_insur.linear_model(a_1_layer_test, weights_2_layer, biases_2_layer)\n            test_prediction = porto_seguro_insur.activation_out(logits_2_layer_test)\n\n        number_of_iterations = 900\n        # Creating a tensorflow session to effeciently run same computation multiple times using definitions in defined\n        # dataflow graph.\n        with tf.Session(graph=graph) as session:\n            # Ensure that variables are initialized as done in our graph defining the dataflow.\n            tf.global_variables_initializer().run()\n            for ite in range(number_of_iterations):\n                # Compute loss and predictions\n                loss, predictions = session.run([optimized_weights_and_bias, loss_function, train_prediction])[1:3]\n                if ite % 100 == 0:\n                    print('Loss at iteration %d: %f' % (ite, loss))\n                    print('Training accuracy: %.1f%%' % porto_seguro_insur.accuracy(predictions, y_train))\n            print('Test accuracy: %.1f%%' % porto_seguro_insur.accuracy(val_prediction.eval(), y_val))\n            output = test_prediction.eval()\n\n    is_make_prediction = 1\n    if is_make_prediction:\n        ''' Submission '''\n        # Submission requires a csv file with id and target columns.\n        submission = pd.DataFrame({'id': id_df_test, 'target': np.argmax(output, 1)})\n        submission.to_csv(''.join(['submission_porto_seguro_insur_', porto_seguro_insur.timestamp, '.csv']), index=False)\n        print(porto_seguro_insur.timestamp)\n\nif __name__ == '__main__':\n    main()","cell_type":"code"},{"outputs":[],"execution_count":null,"metadata":{"_cell_guid":"e84dfb99-a8b5-4af1-b5bf-e885fca30d5a","_uuid":"6b62e146e8e1af3e7eb2b4a44d8dec706f8b8335","collapsed":true},"source":"","cell_type":"code"}],"nbformat":4,"nbformat_minor":1,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python","pygments_lexer":"ipython3","name":"python","file_extension":".py","version":"3.6.3","mimetype":"text/x-python"}}}