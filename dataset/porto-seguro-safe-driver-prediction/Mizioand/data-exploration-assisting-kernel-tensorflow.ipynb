{"nbformat_minor":1,"nbformat":4,"cells":[{"source":"Data Exploration assisting kernel \"TensorFlow with 2-layer Neural Network\",\nhttps://github.com/MizioAnd/PortoSeguroInsur/blob/master/porto_seguro_insur.py\n\nThis notebook is also found on my Github profile Mizioand,\nhttps://github.com/MizioAnd/PortoSeguroInsur/blob/master/porto_seguro_notebook.ipynb","metadata":{"_uuid":"cac489a7df551c9ee6c1f371b45bb4032b247564","_cell_guid":"20937555-4b1c-4aeb-8769-df21d0541878"},"cell_type":"markdown"},{"outputs":[],"source":"# porto_seguro_insur.py\n#  Assumes python vers. 3.6\n# __author__ = 'mizio'\n\nimport csv as csv\nimport numpy as np\nimport pandas as pd\nimport pylab as plt\nfrom fancyimpute import MICE\nimport random\nfrom sklearn.model_selection import cross_val_score\nimport datetime\nimport seaborn as sns\nimport tensorflow as tf","execution_count":null,"metadata":{"_uuid":"387757df74ea205a932f7f576b9acfcc51023a03","collapsed":true,"_cell_guid":"1990775f-04f2-4875-8b63-d8c6ba2b3040"},"cell_type":"code"},{"outputs":[],"source":"class PortoSeguroInsur:\n    def __init__(self):\n        self.df = PortoSeguroInsur.df\n        self.df_test = PortoSeguroInsur.df_test\n        self.df_submission = PortoSeguroInsur.df_submission\n        self.timestamp = datetime.datetime.now().strftime('%Y%m%d_%Hh%Mm%Ss')\n\n\n    # Load data into Pandas DataFrame\n    # For .read_csv, always use header=0 when you know row 0 is the header row\n    df = pd.read_csv('../input/train.csv', header=0)\n    df_test = pd.read_csv('../input/test.csv', header=0)\n    df_submission = pd.read_csv('../input/sample_submission.csv', header=0)\n\n    @staticmethod\n    def features_with_null_logical(df, axis=1):\n        row_length = len(df._get_axis(0))\n        # Axis to count non null values in. aggregate_axis=0 implies counting for every feature\n        aggregate_axis = 1 - axis\n        features_non_null_series = df.count(axis=aggregate_axis)\n        # Whenever count() differs from row_length it implies a null value exists in feature column and a False in mask\n        mask = row_length == features_non_null_series\n        return mask\n\n    def missing_values_in_dataframe(self, df):\n        mask = self.features_with_null_logical(df)\n        print(df[mask[mask == 0].index.values].isnull().sum())\n        print('\\n')\n\n    @staticmethod\n    def extract_numerical_features(df):\n        df = df.copy()\n        df = df.copy()\n        non_numerical_feature_names = df.columns[np.where(PortoSeguroInsur.numerical_feature_logical_incl_hidden_num(\n            df) == 0)]\n        return non_numerical_feature_names\n\n    @staticmethod\n    def extract_non_numerical_features(df):\n        df = df.copy()\n        non_numerical_feature_names = df.columns[np.where(PortoSeguroInsur.numerical_feature_logical_incl_hidden_num(\n            df))]\n        return non_numerical_feature_names\n\n    @staticmethod\n    def numerical_feature_logical_incl_hidden_num(df):\n        logical_of_non_numeric_features = np.zeros(df.columns.shape[0], dtype=int)\n        for ite in np.arange(0, df.columns.shape[0]):\n            try:\n                str(df[df.columns[ite]][0]) + df[df.columns[ite]][0]\n                logical_of_non_numeric_features[ite] = True\n            except TypeError:\n                hej = 'Oops'\n        return logical_of_non_numeric_features\n\n    def clean_data(self, df, is_train_data=1):\n        df = df.copy()\n        if df.isnull().sum().sum() > 0:\n            if is_train_data:\n                df = df.dropna()\n            else:\n                df = df.dropna(1)\n        return df\n\n    def reformat_data(self, labels, num_labels):\n        # Map labels/target value to one-hot-encoded frame. None is same as implying newaxis() just replicating array\n        # if num_labels > 2:\n        labels = (np.arange(num_labels) == labels[:, None]).astype(np.float64)\n        return labels\n\n    def accuracy(self, predictions, labels):\n        # Sum the number of cases where the predictions are correct and divide by the number of predictions\n        number_of_correct_predictions = np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n        return 100*number_of_correct_predictions/predictions.shape[0]\n\n    def linear_model(self, input_vector, weight_matrix, bias_vector):\n        # f(x) = Wx + b\n        # W is the weight matrix with elements w_ij\n        # x is the input vector\n        # b is the bias vector\n        # In the machine learning literature f(x) is called an activation\n        return tf.matmul(input_vector, weight_matrix) + bias_vector\n\n    def activation_out(self, logit):\n        return self.activation(logit, switch_var=0)\n\n    def activation_hidden(self, logit):\n        return self.activation(logit, switch_var=0)\n\n    def activation(self, logit, switch_var=0):\n        # Also called the activation function\n        if switch_var == 0:\n            # Logistic sigmoid function.\n            # sigma(a) = 1/(1+exp(-a))\n            return tf.nn.sigmoid(logit)\n        elif switch_var == 1:\n            # Using Rectifier as activation function. Rectified linear unit (ReLU). Compared to sigmoid or other\n            # activation functions it allows for faster and effective training of neural architectures.\n            # f(x) = max(x,0)\n            return tf.nn.relu(logit)\n        else:\n            # Softmax function.\n            # S(y_i) = e^y_i/(Sum_j e^y_j)\n            return tf.nn.softmax(logit)\n\n    def missing_values_in_dataframe(self, df):\n        mask = self.features_with_null_logical(df)\n        print(df[mask[mask == 0].index.values].isnull().sum())\n        print('\\n')\n        \n    @staticmethod\n    def extract_numerical_features(df):\n        df = df.copy()\n        # Identify numerical columns which are of type object\n        numerical_features = pd.Series(data=False, index=df.columns, dtype=bool)\n\n        for feature in df.columns:\n            if any(tuple(df[feature].apply(lambda x: type(x)) == int)) or \\\n                            any(tuple(df[feature].apply(lambda x: type(x)) == float)) & \\\n                            (not any(tuple(df[feature].apply(lambda x: type(x)) == str))):\n                numerical_features[feature] = 1\n        return numerical_features[numerical_features == 1].index\n","execution_count":null,"metadata":{"_uuid":"c9e9578822303c2cd85fb38255c6886f7f5d3bfc","collapsed":true,"_cell_guid":"9fb3c38d-9013-4933-a811-ce7590a9f186"},"cell_type":"code"},{"outputs":[],"source":"porto_seguro_insur = PortoSeguroInsur()\ndf = porto_seguro_insur.df.copy()\ndf_test = porto_seguro_insur.df_test.copy()\ndf_submission = porto_seguro_insur.df_submission.copy()\n\ndf = df.replace(-1, np.NaN)\ndf_test = df_test.replace(-1, np.NaN)\n\nprint('All df set missing values')\nporto_seguro_insur.missing_values_in_dataframe(df)\n\n# Train Data: numeric feature columns with none or nan in test data\nprint('\\nColumns in train data with none/nan values:')\nprint('\\nTraining set numerical features\\' missing values')\ndf_numerical_features = porto_seguro_insur.extract_numerical_features(df)\nprint('\\nNumber of numerical features df: %s' % df_numerical_features.shape[0])\nporto_seguro_insur.missing_values_in_dataframe(df[df_numerical_features])\n\n# Test Data: Print numeric feature columns with none/nan in test data\nprint('\\nColumns in test data with none/nan values:')\nprint('\\nTest set numerical features\\' missing values')\ndf_test_numerical_features = porto_seguro_insur.extract_numerical_features(df_test)\nprint('\\nNumber of numerical features df_test: %s' % df_test_numerical_features.shape[0])\nporto_seguro_insur.missing_values_in_dataframe(df_test[df_test_numerical_features])\n\nprint(df.shape)\nprint(df_test.shape)\n# Clean data for NaN\ndf = porto_seguro_insur.clean_data(df)\ndf_test = porto_seguro_insur.clean_data(df_test, is_train_data=0)\nprint('df_test.shape: %s' % str(df_test.shape))  # (892816, 46)\n# df_test = porto_seguro_insur.clean_data(df_test, is_train_data=0)\nid_df_test = df_test['id']  # Submission column\nprint(\"After dropping NaN\")\nprint(df.shape)\nprint(df_test.shape)","execution_count":null,"metadata":{"_uuid":"5bced9121386187c087d82fe8b6052af0b9ff951","_cell_guid":"b1702deb-ee35-4a7c-8bad-380c2a0e8a03"},"cell_type":"code"},{"outputs":[],"source":"is_explore_data = 1\nif is_explore_data:\n    # Overview of train data\n    print('\\n TRAINING DATA:----------------------------------------------- \\n')\n    print(df.head(3))\n    print('\\n')\n    print(df.info())\n    print('\\n')\n    print(df.describe())\n    print('\\n')\n    print(df.dtypes)\n    print(df.get_dtype_counts())\n\n    # missing_values\n    print('All df set missing values')\n    porto_seguro_insur.missing_values_in_dataframe(df)\n\n    print('Uniques')\n    uniques_in_id = np.unique(df.id.values).shape[0]\n    print(uniques_in_id)\n    print('uniques_in_id == df.shape[0]')\n    print(uniques_in_id == df.shape[0])\n\n    # Overview of sample_submission format\n    print('\\n sample_submission \\n')\n    print(df_submission.head(3))\n    print('\\n')\n    print(df_submission.info())\n    print('\\n')","execution_count":null,"metadata":{"_uuid":"d4d08055862307fa76d3f5943c39283e003e4d76","_cell_guid":"1158fec2-b30f-41b4-83b2-198763ecbba2"},"cell_type":"code"},{"outputs":[],"source":"# Categorical plot with seaborn\nis_categorical_plot = 1\nif is_categorical_plot:\n    # sns.countplot(y='MSZoning', hue='MSSubClass', data=df, palette='Greens_d')\n    # plt.show()\n    # sns.stripplot(x='SalePrice', y='MSZoning', data=df, jitter=True, hue='LandContour')\n    # plt.show()\n    # sns.boxplot(x='SalePrice', y='MSZoning', data=df, hue='MSSubClass')\n    # plt.show()\n\n    # Heatmap of feature correlations\n    print('\\nCorrelations in training data')\n    plt.figure(figsize=(10, 8))\n    correlations_train = porto_seguro_insur.df.corr()\n    sns.heatmap(correlations_train, vmax=0.8, square=True)\n    plt.show()\n    \n    # Heatmap of feature correlations\n    print('\\nCorrelations in test data')\n    plt.figure(figsize=(10, 8))\n    correlations_test = porto_seguro_insur.df_test.corr()\n    sns.heatmap(correlations_test, vmax=0.8, square=True)\n    plt.show()","execution_count":null,"metadata":{"_uuid":"5bc5557d36fb270c25ee1051dbaf2e54cf5c9602","_cell_guid":"896b33d7-cae8-479d-9433-abe3667b4a4a"},"cell_type":"code"},{"outputs":[],"source":"# Zoom of heatmap with coefficients\nplt.figure(figsize=(20, 12))\ntop_features = 10\ncolumns = correlations_train.nlargest(top_features, 'target')['target'].index\ncorrelation_coeff = np.corrcoef(porto_seguro_insur.df[columns].values.T)\nsns.set(font_scale=1.20)\ncoeff_heatmap = sns.heatmap(correlation_coeff, annot=True, cmap='YlGn', cbar=True, \n                            square=True, fmt='.2f', annot_kws={'size': 10}, \n                            yticklabels=columns.values, xticklabels=columns.values)\nplt.show()","execution_count":null,"metadata":{"_uuid":"1e280bd87a3dfe4e3322f1e3c61168d75d6dbe58","_cell_guid":"6a76d554-772c-4975-add3-39787af7fa50"},"cell_type":"code"},{"source":"Note that correlations with target are low. The best features are ps_car_13 and ps_car_12.\nCheck if there are categorical features that need to be one-hot-encoded.\nExample note that in features we have,\nps_ind_02_cat\nand\nps_ind_06_bin\nwhere 'cat' and 'bin' may be abbreviations for categorical and binary feature values.","metadata":{"_uuid":"a13e1bbdb8133ead3021ec02994914f0274ad3f3","_cell_guid":"7902bc3f-59a6-454c-ad29-61bb0d24cde9"},"cell_type":"markdown"},{"outputs":[],"source":"# Check output space for each feature. Expect 58 uniques i.e. one for every feature.\nser_with_uniques = pd.Series()\nfor ite in df.columns:\n    ser_with_uniques[ite] = df[ite].unique().shape[0]\nprint(ser_with_uniques)","execution_count":null,"metadata":{"_uuid":"461024eb0e8c0d17c0fd1daa5f57659c7895c696","_cell_guid":"540c1379-fd5d-47e7-bd04-5f8f62f9aa60"},"cell_type":"code"},{"outputs":[],"source":"# Check if two-value features are binaries\nindices_of_two_value_feats = ser_with_uniques == 2\nprint(indices_of_two_value_feats)","execution_count":null,"metadata":{},"cell_type":"code"},{"outputs":[],"source":"feats_with_two_value = ser_with_uniques[indices_of_two_value_feats]\nprint(feats_with_two_value.axes[0])\nprint(type(feats_with_two_value.axes))","execution_count":null,"metadata":{},"cell_type":"code"},{"outputs":[],"source":"ser_with_max_of_uniques = pd.Series()\nfor ite in feats_with_two_value.axes[0]:\n    ser_with_max_of_uniques[ite] = df[ite].unique()\nprint(ser_with_max_of_uniques)","execution_count":null,"metadata":{},"cell_type":"code"},{"source":"Hence the two-value features are binaries.","metadata":{},"cell_type":"markdown"},{"outputs":[],"source":"","execution_count":null,"metadata":{"collapsed":true},"cell_type":"code"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"nbconvert_exporter":"python","file_extension":".py","name":"python","mimetype":"text/x-python","codemirror_mode":{"version":3,"name":"ipython"},"pygments_lexer":"ipython3","version":"3.6.3"}}}