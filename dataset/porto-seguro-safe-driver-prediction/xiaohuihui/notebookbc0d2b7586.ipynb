{"metadata":{"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"},"language_info":{"name":"python","file_extension":".py","nbconvert_exporter":"python","mimetype":"text/x-python","version":"3.6.3","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3}}},"nbformat":4,"nbformat_minor":1,"cells":[{"execution_count":null,"source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport warnings\nfrom collections import Counter\nfrom sklearn.feature_selection import mutual_info_classif\nfrom sklearn.externals import joblib\nwarnings.filterwarnings('ignore')","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"ab35bed0-6a03-42e2-9c01-fd5f9a0fea67","_uuid":"d730821b143b165cd00109f50bc717075943aa60"}},{"execution_count":null,"source":"train = pd.read_csv('../input/train.csv')\ntrain.head()","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"fcfa2dd6-b2d0-4cc8-8131-78a791fef035","_uuid":"260a2cacb954913fd95d31d375bb23d306b1aaa7"}},{"execution_count":null,"source":"round(0)","cell_type":"code","outputs":[],"metadata":{"_uuid":"c0f1843f966d33d02b8809c6558777fdb2551c8b","_cell_guid":"4fa09871-ee72-4bef-a4b3-1e60f411ef31"}},{"execution_count":null,"source":"test = pd.read_csv('../input/test.csv')\ntest.tail()","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"045beb2c-09a5-4118-970f-9cc6d823ce0d","_uuid":"b491a5e445d0ab4c57ed76e6ce2c3663ee8b0ec3"}},{"execution_count":null,"source":"print(train.shape, test.shape)","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"ed7c0f25-852e-49f2-897e-f9b5b7702f70","_uuid":"2261adec335cadbe8abaa8d5ba98d00bf68f036a"}},{"metadata":{"_cell_guid":"f05a6b73-87da-4524-8606-960c500d5349","_uuid":"d688d466d2f0b37f4a5653ab3f9ccc8d7becd3fd"},"source":"## Construction of metadata","cell_type":"markdown"},{"execution_count":null,"source":"data = []\nfor col in train.columns:\n    # Defining the role\n    if col == 'target' or col == 'id':\n        role = col\n    else:\n        role = 'input'\n         \n    # Defining the level\n    if 'bin' in col or col == 'target':\n        level = 'binary'\n    elif 'cat' in col or col == 'id':\n        level = 'nominal'\n    elif train[col].dtype == float:\n        level = 'interval'\n    elif train[col].dtype == int:\n        level = 'ordinal'\n        \n    # Initialize keep to True for all variables except for id\n    keep = True\n    if col == 'id':\n        keep = False\n    \n    # Defining the data type \n    dtype = train[col].dtype\n    \n    # Creating a Dict that contains all the metadata for the variable\n    col_dict = {'col_name': col, 'role': role, 'level': level, 'keep': keep, 'dtype': dtype}\n    data.append(col_dict)\n    \nmeta = pd.DataFrame(data, columns=['col_name', 'role', 'level', 'keep', 'dtype'])\nmeta.set_index('col_name', inplace=True)","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"e64a1c5f-8aae-44ca-97cd-733808ec0cbf","_uuid":"3412a9847f57233da9bacdbc89cd89d21ab9b8b6","collapsed":true}},{"execution_count":null,"source":"bin_cols = meta[(meta.level=='binary') & meta.keep].index\ntrain[bin_cols].describe()","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"0228a322-55e4-455c-a658-cbdcd712f4cc","_uuid":"5ac0580f79a56117859dab7847ddbfadb2690d59","collapsed":true}},{"metadata":{"_cell_guid":"76804373-8d11-4add-8c68-49645be9fdae","_uuid":"490feacc1dc052e9157570e02b798cfd8261e8c2"},"source":"## Balance training data distribution","cell_type":"markdown"},{"execution_count":null,"source":"# from sklearn.utils import shuffle\n# desired_apriori=0.10\n\n# # Get the indices per target value\n# idx_0 = train[train.target == 0].index\n# idx_1 = train[train.target == 1].index\n\n# # Get original number of records per target value\n# nb_0 = len(train.loc[idx_0])\n# nb_1 = len(train.loc[idx_1])\n\n# undersampling_rate = ((1 - desired_apriori) * nb_1) / (nb_0 * desired_apriori)\n# undersampling_nb_0 = int(nb_0 * undersampling_rate)\n# print('Number of training samples with target == 0 after undersampling', undersampling_nb_0)\n\n# undersampled_idx = shuffle(idx_0, random_state=77, n_samples=undersampling_nb_0)\n# idx_list = list(undersampled_idx) + list(idx_1)\n# balanced_train = train.loc[idx_list].reset_index(drop=True)","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"3527485f-bac7-464b-abac-3e55ee241ebe","_uuid":"66362c23b2112d14bc806a34a90ec8b4d7786e80","collapsed":true}},{"metadata":{"_cell_guid":"54739a03-45d1-4d32-bc48-68163543aaa7","_uuid":"e81d6f87eb3b1d6174cb050f2e15943da01d441d"},"source":"## 1. Data quality checks","cell_type":"markdown"},{"metadata":{"_cell_guid":"b8905d9b-6300-4553-bb53-f6b0a2f401aa","_uuid":"6f16d1891d9bcc92216e04b75a247f9774a67802"},"source":"### Null or missing values check","cell_type":"markdown"},{"execution_count":null,"source":"train.isnull().any().any()","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"66e7b8e6-eec4-4a26-ada7-6761824774c8","_uuid":"648468a6fcd4b0359bd49228eb760854777f8c0c","collapsed":true}},{"execution_count":null,"source":"ms_cols = []\ntrain_copy = train\ntrain_copy = train_copy.replace(-1, np.NaN)\nfor col in train_copy.columns:\n    ms_nb = train_copy[col].isnull().sum()\n    if ms_nb > 0:\n        ms_cols.append(col)\n        print('Column {} has {} records ({:.2%}) with missing values'.format(col, ms_nb, ms_nb/train_copy.shape[0]))","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"68497924-128b-4ceb-9104-bf8ba5e3a710","_uuid":"d3898e6b49c8fc67c98dc53f2447800dd6d0f44d","collapsed":true}},{"execution_count":null,"source":"# import missingno as msno\n# msno.matrix(df=train_copy.iloc[:, 2:40], figsize=(20, 14),\n#             color=(0.42, 0.1, 0.05))","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"1b54d19b-92a9-4043-a159-94edb81f9c98","_uuid":"b029c5781a3d5ef318dba6aaef31d2750e78b440","collapsed":true}},{"metadata":{"_cell_guid":"792bd630-91bb-48a7-9b67-8f425df202a8","_uuid":"2806ab716abf40b2fdb2c91c79b452cc143cab00"},"source":"### Dropout columns with too many missing values and imputing","cell_type":"markdown"},{"execution_count":null,"source":"from sklearn.preprocessing import Imputer\n\ndrop_cols = ['ps_car_03_cat', 'ps_car_05_cat']\nreal_train = train.drop(drop_cols, axis=1)\nreal_test = test.drop(drop_cols, axis=1)\nmeta.loc[drop_cols, 'keep'] = False\n\nmean_imp = Imputer(missing_values=-1, strategy='mean', axis=0)\nmode_imp = Imputer(missing_values=-1, strategy='most_frequent', axis=0)\n\n# Imputing training data\nreal_train['ps_reg_03'] = mean_imp.fit_transform(X=real_train[['ps_reg_03']]).ravel()\nreal_train['ps_car_11'] = mode_imp.fit_transform(X=real_train[['ps_car_11']]).ravel()\nreal_train['ps_car_12'] = mode_imp.fit_transform(X=real_train[['ps_car_12']]).ravel()\nreal_train['ps_car_14'] = mean_imp.fit_transform(X=real_train[['ps_car_14']]).ravel()\n\n# Imputing test data\nreal_test['ps_reg_03'] = mean_imp.fit_transform(X=real_test[['ps_reg_03']]).ravel()\nreal_test['ps_car_11'] = mode_imp.fit_transform(X=real_test[['ps_car_11']]).ravel()\nreal_test['ps_car_12'] = mode_imp.fit_transform(X=real_test[['ps_car_12']]).ravel()\nreal_test['ps_car_14'] = mean_imp.fit_transform(X=real_test[['ps_car_14']]).ravel()","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"39e5272e-b5a7-42c9-89e8-8cc8599cd5e0","_uuid":"7955264e845e63cad7742ddfb9b9588d2ad7b4ed","collapsed":true}},{"metadata":{"_cell_guid":"3071c022-232f-44e3-80de-2e16f046ea30","_uuid":"786914b55e6410d50229340522a9f50d57af2465"},"source":"We didn't impute the missing values in the categorical columns, instead, we kept it as a seperate category. As later on we can see that customers with a missing value in these variables appear to have a much higher possibility to file an insurance claim (a good takeaway for the future data preproccessing method)","cell_type":"markdown"},{"execution_count":null,"source":"# Replace missing values with 999\n# real_train1 = real_train.replace(-1, 999)\n# real_test1 = real_test.replace(-1, 999)","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"87d99eb5-847d-41ff-907c-0e39cd113a86","_uuid":"123e2f24afb7de87ef63a86bf208b35517c7e9b9","collapsed":true}},{"metadata":{"_cell_guid":"dcd2601b-4634-4168-9819-2429b99a9783","_uuid":"b4a2f2a221039e30d3377cce888f77a5a82ce90e"},"source":"## Check the cardinality of categorical columns","cell_type":"markdown"},{"execution_count":null,"source":"cat_cols = meta[(meta.level=='nominal') & meta.keep].index\n\nfor col in cat_cols:\n    distinct_values = real_train[col].value_counts().shape[0]\n    print('Categorical column {} has {} distinct values'.format(col, distinct_values))","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"45cc4f21-8dfc-49dc-ba1d-153325e7e816","_uuid":"5bdcadf5a913fb29d1e5d7c71c37f74971641dde","collapsed":true}},{"metadata":{"_cell_guid":"9668648c-f966-4318-8580-21f3edcc67cc","_uuid":"5bf86161ec4c905f814038f5717f46d7f78eb8f5"},"source":"## Handling column \"ps_car_11_cat\" specifically as it has too many cardinality","cell_type":"markdown"},{"execution_count":null,"source":"# Script by https://www.kaggle.com/ogrellier\n# Code: https://www.kaggle.com/ogrellier/python-target-encoding-for-categorical-features\ndef add_noise(series, noise_level):\n    return series * (1 + noise_level * np.random.randn(len(series)))\n\ndef target_encode(trn_series=None, \n                  tst_series=None, \n                  target=None, \n                  min_samples_leaf=1, \n                  smoothing=1,\n                  noise_level=0):\n    \"\"\"\n    Smoothing is computed like in the following paper by Daniele Micci-Barreca\n    https://kaggle2.blob.core.windows.net/forum-message-attachments/225952/7441/high%20cardinality%20categoricals.pdf\n    trn_series : training categorical feature as a pd.Series\n    tst_series : test categorical feature as a pd.Series\n    target : target data as a pd.Series\n    min_samples_leaf (int) : minimum samples to take category average into account\n    smoothing (int) : smoothing effect to balance categorical average vs prior  \n    \"\"\" \n    assert len(trn_series) == len(target)\n    assert trn_series.name == tst_series.name\n    temp = pd.concat([trn_series, target], axis=1)\n    # Compute target mean \n    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n    # Compute smoothing\n    smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) / smoothing))\n    # Apply average function to all target data\n    prior = target.mean()\n    # The bigger the count the less full_avg is taken into account\n    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n    # Apply averages to trn and tst series\n    ft_trn_series = pd.merge(\n        trn_series.to_frame(trn_series.name),\n        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n        on=trn_series.name,\n        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n    # pd.merge does not keep the index so restore it\n    ft_trn_series.index = trn_series.index \n    ft_tst_series = pd.merge(\n        tst_series.to_frame(tst_series.name),\n        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n        on=tst_series.name,\n        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n    # pd.merge does not keep the index so restore it\n    ft_tst_series.index = tst_series.index\n    return add_noise(ft_trn_series, noise_level), add_noise(ft_tst_series, noise_level)","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"617b7167-2efc-4f07-9643-b0e213f0ded4","_uuid":"12583fb629f3e5491a9a936391fcf22985562a6a","collapsed":true}},{"execution_count":null,"source":"train_encoded, test_encoded = target_encode(train[\"ps_car_11_cat\"], \n                             test[\"ps_car_11_cat\"], \n                             target=train.target, \n                             min_samples_leaf=100,\n                             smoothing=10,\n                             noise_level=0.01)\n    \nreal_train['ps_car_11_cat_te'] = train_encoded\nreal_train.drop('ps_car_11_cat', axis=1, inplace=True)\nmeta.loc['ps_car_11_cat','keep'] = False  # Updating the meta\nreal_test['ps_car_11_cat_te'] = test_encoded\nreal_test.drop('ps_car_11_cat', axis=1, inplace=True)","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"fe6b5881-199f-4843-a4e5-bd6f4685de32","_uuid":"82dc8a18e2fd710c34acd01bd80c5039c4a3bcdc","collapsed":true}},{"metadata":{"_cell_guid":"4bb19cad-7124-4860-9a71-cf48d654c017","_uuid":"50f5adf5562e6468d26d2ea783b8fdf389bb3f7a"},"source":"## Dropout calculated columns ","cell_type":"markdown"},{"execution_count":null,"source":"# calc_cols = [col for col in train.columns if 'calc' in col]\n\n# ###=========== A data exploration of calc features from armamut ==============###\n# # Script: https://www.kaggle.com/armamut/ps-calc-15-bin-ps-calc-20-bin\n\n# # Columns -> binary decoded.\n\n# tmp  =real_train['ps_calc_15_bin'] * 32 + real_train['ps_calc_16_bin'] * 16 + real_train['ps_calc_17_bin'] * 8\n# tmp += real_train['ps_calc_18_bin'] * 4 + real_train['ps_calc_19_bin'] * 2 + real_train['ps_calc_20_bin'] * 1\n\n# tmp2 = [5, 22, 9, 32, 13, 38, 20, 47, 2, 19, 8, 30, 10, 35, 17, 45, 1,\n#         15, 4, 24, 7, 29, 14, 40, 0, 12, 3, 21, 6, 26, 11, 36, 27, 52,\n#         37, 57, 42, 60, 51, 63, 23, 49, 34, 56, 39, 59, 48, 62, 18, 46,\n#         28, 53, 33, 55, 44, 61, 16, 43, 25, 50, 31, 54, 41, 58]\n# tmp2 = pd.Series(tmp2)\n\n# real_train['ps_calc_15_16_17_18_19_20'] = tmp.map(tmp2)\n# real_test['ps_calc_15_16_17_18_19_20'] = tmp.map(tmp2)\n# # You may now drop the others peacefully.\n# # real_train_nocalc = real_train.drop(['ps_calc_15_bin', 'ps_calc_16_bin', 'ps_calc_17_bin',\n# #               'ps_calc_18_bin', 'ps_calc_19_bin', 'ps_calc_20_bin'], axis=1, inplace=False)\n# ###============================================================================###\n\n# real_train_nocalc = real_train.drop(calc_cols, axis=1)\n# real_test_nocalc = real_test.drop(calc_cols, axis=1)\n# real_train_nocalc.shape","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"25e4573e-d866-4444-8012-fcbe0e8b4c78","_uuid":"a18617fbe21bdaa8751adaa77bb93ca675353d2c","collapsed":true}},{"execution_count":null,"source":"calc_cols = [col for col in train.columns if 'calc' in col]\n\nreal_train_nocalc = real_train.drop(calc_cols, axis=1)\nreal_test_nocalc = real_test.drop(calc_cols, axis=1)\nreal_train_nocalc.shape","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"9ea02492-b18a-4493-a016-fd30095f5d1c","_uuid":"d2c7d114b267814f27e4b21d2b092d0a4b1cc17e","collapsed":true}},{"metadata":{"_cell_guid":"edbb0044-a8f0-411d-90f4-9a8b0903a415","_uuid":"ce1c0328625140ebc7064484c188ce20c36c212a"},"source":"### Target variable inspection","cell_type":"markdown"},{"execution_count":null,"source":"def plotTargetDistribution(dataset):\n    data = [go.Bar(\n                x = dataset['target'].value_counts().index.values,\n                y = dataset['target'].value_counts().values,\n                text = 'Distribution of target variable')]\n    layout = go.Layout(\n                title = 'Distribution of target variable')\n    fig = go.Figure(data=data, layout=layout)\n    py.iplot(fig, filename='basic-bar')\nplotTargetDistribution(real_train)","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"ddd1a52a-5634-4610-8559-80d6bbb4cb5a","_uuid":"3b7ec2e5bf5b37ebbdad834ed0fb5acd1a634881","collapsed":true}},{"metadata":{"_cell_guid":"f8d49c51-4891-4166-98b8-5f41e0c807d8","_uuid":"9e1f2686bceabbdc9d39d82a10eb87b9dd638d68"},"source":"### Typedata check","cell_type":"markdown"},{"execution_count":null,"source":"# Counter(real_train.dtypes.values)\n# train_float = real_train.select_dtypes(include=['float64'])\n# train_int = real_train.select_dtypes(include=['int64'])","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"db7c6450-2bf9-411b-8a0d-975c87aa1754","_uuid":"c3beade78af75d2786efafb672ac8cb74634b2c6","collapsed":true}},{"metadata":{"_cell_guid":"7d1a3a53-0031-495b-9df1-363425d6c7af","_uuid":"d97bb78ea21d06a4b98ae870658fb2316a122d5f"},"source":"## 2. Correlation plots","cell_type":"markdown"},{"metadata":{"_cell_guid":"188e0563-11f4-47e6-b8be-362cbdfc1e95","_uuid":"5fb2ebb3460cc8e39d4b8920ab6b2c4994acd921"},"source":"### Correlation of float features","cell_type":"markdown"},{"execution_count":null,"source":"float_int_cols = meta[((meta.level == 'interval') | (meta.level == 'ordinal')) & meta.keep].index\nfloat_int_train = real_train[float_int_cols]\n\ncolormap = plt.cm.cubehelix_r\nplt.figure(figsize=(16,12))\nplt.title('Pearson correlation of continuous features', y=1.05, size=15)\nsns.heatmap(float_int_train.corr(),linewidths=0.1,vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=True)","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"bd9fdebe-72d8-421d-91b8-a67f21d4a8e1","_uuid":"fb4820c9fd1f831cd16e9f57803a69f79ae4e537","collapsed":true}},{"metadata":{"_cell_guid":"0f156d1f-4a94-4a5b-b36e-cf8d643857a0","_uuid":"2fb8a8d4af4b04e0770b14103ba3bb4467bc253d"},"source":"Column pairs with high correlation:\n1. ps_reg_01 and ps_reg_02: 0.47\n2. ps_reg_02 and ps_reg_03: 0.7\n3. ps_car_12 and ps_car_13: 0.67\n4. ps_car_12 and ps_car_14: 0.58\n5. ps_car_13 and ps_car_14: 0.44\n6. ps_car_13 and ps_car_15: 0.52\n\nMaybe we need to do some domensionality reduction by using PCA...","cell_type":"markdown"},{"metadata":{"_cell_guid":"46ee0444-63eb-45e4-a88f-9cf4a034488a","_uuid":"2464587d6836e88b5ff5de220f17fd5682a8a83a"},"source":"### Dropout float features with high correlation to another column","cell_type":"markdown"},{"metadata":{"_cell_guid":"585182bb-0ac4-4686-8087-7c58c2a10ae0","_uuid":"45c0ccf75164e33a05d5c20a34c00c197f6a41e4"},"source":"Within the two groups of highest correlation features, there is one column \"ps_car_13\" shared by both, so we will throw that column directly as to preserve as much information","cell_type":"markdown"},{"execution_count":null,"source":"# realTrain = realTrain.drop('ps_car_13', axis=1)","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"85a6f0d4-1bf0-4b5d-9fd0-d5ede1b90a1a","_uuid":"6067d338f9b64671ba86a438e5d77529146c45e4","collapsed":true}},{"metadata":{"_cell_guid":"98d83d12-2b3e-462f-a309-9fc83fab92e0","_uuid":"e909d848621c745c0a364f20291a3f1c74abd9cc"},"source":"## 3. Binary features inspection","cell_type":"markdown"},{"execution_count":null,"source":"bin_cols = meta[(meta.level == 'binary') & (meta.role != 'target') & meta.keep].index\nones_list = []\nzeros_list = []\nfor col in bin_cols:\n    zeros_nb = (real_train[col] == 0).sum()\n    ones_nb = real_train.shape[0] - zeros_nb\n    ones_list.append(ones_nb)\n    zeros_list.append(zeros_nb)\n    print('Binary column {} has {} records ({:.2%}) with value zero'.format(col, zeros_nb, zeros_nb/real_train.shape[0]))","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"855f74f7-0b1e-4737-93d8-8713a8ecd2ab","_uuid":"d7b0976b37b1ef747cab080caee7b42b36b4b9e8","collapsed":true}},{"execution_count":null,"source":"trace0 = go.Bar(x=bin_cols, y=zeros_list, name='Zeros count')\ntrace1 = go.Bar(x=bin_cols, y=ones_list, name='Ones count')\n\ndata = [trace0, trace1]\nlayout = go.Layout(barmode='stack', title='Count of zeros and ones')\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='stacked-bar')","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"34c0f02b-c549-4c45-84a1-8204d5fab534","_uuid":"4ccef2064afaf22381a0e3e734e99e60353eb67a","scrolled":false,"collapsed":true}},{"metadata":{"_cell_guid":"d07bd035-e7c6-4179-be85-96ec714d4ea4","_uuid":"e6b67b3f32dbda58595eca4484c3976b77d08f74"},"source":"### Dropout binary features dominated by zeros","cell_type":"markdown"},{"execution_count":null,"source":"imbalanced_cols = ['ps_ind_10_bin', 'ps_ind_11_bin', 'ps_ind_13_bin']\nreal_train = real_train.drop(imbalanced_cols, axis=1)\nreal_test = real_test.drop(imbalanced_cols, axis=1)\n\nreal_train_nocalc = real_train_nocalc.drop(imbalanced_cols, axis=1)\nreal_test_nocalc = real_test_nocalc.drop(imbalanced_cols, axis=1)","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"5809bd01-883d-463f-9c29-d0d408ec6fc8","_uuid":"eb412b18489591937cc66aea2c83129ff0c339eb","collapsed":true}},{"execution_count":null,"source":"# plotTargetDistribution(train_data)\n# plotTargetDistribution(val_data)","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"29fe315f-eb58-4a99-b76c-aaf3e9c23a9c","_uuid":"3a70474ccf5c0376b1272fcbf86b88e747ef7dbc","collapsed":true}},{"metadata":{"_cell_guid":"ad8b7253-20e6-4db2-be8d-8e1d9d4e7510","_uuid":"d9199fc562b2efc86aeaf33f025fc53b62458d76"},"source":"## Feature engineering","cell_type":"markdown"},{"metadata":{"_cell_guid":"b528732a-12e8-490d-b349-e0bae3f1f98f","_uuid":"4ff7d0c8608e486538d37301228e43a9ea0192d8"},"source":"### Create dummy variables","cell_type":"markdown"},{"execution_count":null,"source":"cat_cols = meta[(meta.level=='nominal') & meta.keep].index\n# cat_cols = cat_cols.drop('ps_car_11_cat')\n\nprint('Nb of columns in train data before dummification: {}'.format(real_train.shape[1]))\nreal_train = pd.get_dummies(data=real_train, columns=cat_cols, drop_first=True)\nprint('Nb of columns in train data before dummification: {}'.format(real_train.shape[1]))\n\nprint('Nb of columns in test data before dummification: {}'.format(real_test.shape[1]))\nreal_test = pd.get_dummies(data=real_test, columns=cat_cols, drop_first=True)\nprint('Nb of columns in test data after dummification: {}'.format(real_test.shape[1]))\n\nreal_train_nocalc = pd.get_dummies(data=real_train_nocalc, columns=cat_cols, drop_first=True)\nreal_test_nocalc = pd.get_dummies(data=real_test_nocalc, columns=cat_cols, drop_first=True)","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"1342afd7-9099-4092-8199-a34eba2220ce","_uuid":"700124d1ece349d4c2a2df7a930715b835ed8009","collapsed":true}},{"metadata":{"_cell_guid":"f949412d-3109-4829-a7e4-02e477e5a5d5","_uuid":"3a6c2063aaae14d7a78889859b4c0badb454d5a0"},"source":"### Create interaction variables","cell_type":"markdown"},{"execution_count":null,"source":"from sklearn.preprocessing import PolynomialFeatures\nfloat_cols = meta[(meta.level=='interval') & meta.keep].index\n\npoly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)\ninteractions = pd.DataFrame(data=poly.fit_transform(real_train[float_cols]), columns=poly.get_feature_names(float_cols))\ninteractions.drop(float_cols, axis=1, inplace=True)\ninteracted_train = pd.concat(objs=[real_train, interactions], axis=1)\ninteracted_test = pd.concat(objs=[real_test, interactions], axis=1)","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"23777eab-f010-40e5-b0ab-29cbfe724332","_uuid":"0b8eb8780c7f42d8a835579bd13979a95339d108","collapsed":true}},{"metadata":{"_cell_guid":"bc8c7c8d-10ae-4512-9324-300431c7b663","_uuid":"4bbf8d290483c00a100e0282ff6eeccc7f806e4a"},"source":"## 4. Learning models and predictions","cell_type":"markdown"},{"execution_count":null,"source":"from sklearn.model_selection import train_test_split\ntrain_data, val_data = train_test_split(real_train, train_size=0.9, random_state=77)","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"7fe5aa5c-ec18-4690-8149-365e4bcb7f56","_uuid":"00fcf7c2d97ae8e4bec61d74ea57587f59760a9b","collapsed":true}},{"execution_count":null,"source":"# _, compressed_train = train_test_split(real_train_nocalc, train_size=0.95, random_state=777)\n# train_data, val_data = train_test_split(compressed_train, train_size=0.8, random_state=777)","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"ccde133e-a6a3-4207-8bd4-bb72c7d472e9","_uuid":"bc06e520d9ba3744364ae22af1ec6f7014959c68","collapsed":true}},{"metadata":{"_cell_guid":"cc8b72be-09ef-4503-9a07-b96a2d09371e","_uuid":"9bd392323e3c00a0d8614be8b8100d155eef7bef","collapsed":true},"source":"### Feature importance via random forest","cell_type":"markdown"},{"execution_count":null,"source":"# from sklearn.ensemble import RandomForestClassifier\n# rf = RandomForestClassifier(n_estimators=300, min_samples_leaf=4,\n#                             n_jobs=-1, random_state=77, max_features=1, class_weight={0:1, 1:700})\n# rf.fit(X=train_data.drop(['id', 'target'], axis=1), y=train_data['target'])\n# features = train_data.drop(['id', 'target'], axis=1).columns.values","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"5e11967f-0ebb-4c2b-b969-9d1850e75ec0","_uuid":"2e7a368f1536b0f3c6c7fc0c48d2afca89a13eff","collapsed":true}},{"execution_count":null,"source":"# predVal = rf.predict(X=val_data.drop(['id', 'target'], axis=1))","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"9e46647a-40fb-49ae-a5f2-3ea844c9c65d","_uuid":"4183b20077236e730b42fb3144345964cdd1c625","collapsed":true}},{"metadata":{"_cell_guid":"e6e89b42-f735-4d13-bbd8-1422d65b3854","_uuid":"4cd0c7e76bf863337cba42aae5588e671cdac6c6"},"source":"### Visualisation of features importances","cell_type":"markdown"},{"execution_count":null,"source":"# x, y = (list(x) for x in zip(*sorted(zip(rf.feature_importances_, features), reverse=False)))\n# trace = go.Bar(x=x, y=y, marker=dict(color=x, colorscale='Viridis'), \n#                name='Random Forest feature importance', orientation='h')\n# layout = dict(title='Barplot of reature importance', width=900, height=2000, \n#              yaxis=dict(showgrid=False, showline=False, showticklabels=True))\n# fig = go.Figure(data=[trace])\n# fig['layout'].update(layout)\n# py.iplot(figure_or_data=fig, filename='Barplots')","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"38ecfca8-2ab8-488a-b6fc-99cebd29969b","_uuid":"ea87f2916558a06235b297209f30535a2e903964","collapsed":true}},{"metadata":{"_cell_guid":"c6be4b65-f043-4ee8-aa16-ae2e0c3ad4ed","_uuid":"876ae56ffdcaf7942495833b672c1a32b63d19f2"},"source":"### Select features with a given threshold feature importance with SelectFromModel method","cell_type":"markdown"},{"execution_count":null,"source":"# from sklearn.feature_selection import SelectFromModel\n# sfm = SelectFromModel(estimator=rf, threshold=0.001, prefit=True)\n# sfm.transform(X=train_data.drop(['id', 'target'], axis=1))\n\n# train_data = train_data.iloc[:, sfm.get_support(indices=True)]\n# val_data = val_data.iloc[:, sfm.get_support(indices=True)]","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"10c3ebc6-4f70-4e31-b424-104bf369580d","_uuid":"ebb84bc85eee0bc24189297051ad46bcc8fbc343","collapsed":true}},{"metadata":{"_cell_guid":"ecbb7826-46f8-4c24-98fb-0f1c4477e6fd","_uuid":"49effd71e3e85cb1546f01c0de02f7984c760e2f"},"source":"### Training a neural network","cell_type":"markdown"},{"execution_count":null,"source":"# from sklearn.neural_network import MLPClassifier\n# nnet = MLPClassifier(hidden_layer_sizes=(7, 7, 7), max_iter=250, batch_size=700, \n#                      random_state=777, verbose=True, tol=1e-7)\n# nnet.fit(X=train_data.drop(['id', 'target'], axis=1), y=train_data[['target']])","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"799e8d4a-5070-44fd-a773-fd4e7886c4a6","_uuid":"7a94e126fc76830c746252563e55df9ac7254cc7","collapsed":true}},{"metadata":{"_cell_guid":"17d87afc-9699-453f-a0e4-b0c95f23e718","_uuid":"548d0c8fda95b34551353943bbaa6f9aa7fb59d0"},"source":"### Introduce imblearn package to hopefully resolve skewed data problem","cell_type":"markdown"},{"execution_count":null,"source":"# from imblearn.ensemble import BalanceCascade\n# bc = BalanceCascade(random_state=7)\n# X_resampled, y_resampled = bc.fit_sample(realTrain.drop(['id', 'target'], axis=1), \n#                                          realTrain[['target']])","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"315eba3e-74de-43b8-9413-f140f56b317d","_uuid":"136aa1ff7d5f52a9343affb3ed52c34f27b399c9","collapsed":true}},{"metadata":{"_cell_guid":"593193e5-4ab3-4c55-a138-8d7e7fd522a4","_uuid":"6b0cdf38a1f07b5f3897672c94d60c1dbe8831ad"},"source":"## Compute gini coefficient","cell_type":"markdown"},{"execution_count":null,"source":"# Define the gini metric - from https://www.kaggle.com/c/ClaimPredictionChallenge/discussion/703#5897\ndef gini(actual, pred, cmpcol = 0, sortcol = 1):\n    assert( len(actual) == len(pred) )\n    all = np.asarray(np.c_[ actual, pred, np.arange(len(actual)) ], dtype=np.float)\n    all = all[ np.lexsort((all[:,2], -1*all[:,1])) ]\n    totalLosses = all[:,0].sum()\n    giniSum = all[:,0].cumsum().sum() / totalLosses\n    \n    giniSum -= (len(actual) + 1) / 2.\n    return giniSum / len(actual)\n \ndef gini_normalized(a, p):\n    return gini(a, p) / gini(a, a)\n\ndef gini_xgb(preds, d_train):\n    targets = d_train.get_label()\n    gini_score = gini_normalized(targets, preds)\n    return [('gini', gini_score)]\n    ","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"a56c9f34-c714-42b0-80f7-e1d833e1263c","_uuid":"6f9c8c7af09ce879cf11ec804914624782dcca5d","collapsed":true}},{"metadata":{"_cell_guid":"64d90e12-6e92-474e-ad8a-62c4aad3b6a2","_uuid":"779adfc6cb7bf9c0d14bca7e4c418bf34160bed1","collapsed":true},"source":"\n## XGBoost","cell_type":"markdown"},{"execution_count":null,"source":"import xgboost as xgb\nd_train = xgb.DMatrix(real_train.drop(['id', 'target'], axis=1), real_train[['target']])\n# d_val = xgb.DMatrix(val_data.drop(['id', 'target'], axis=1), val_data[['target']])\nd_test = xgb.DMatrix(real_test.drop(['id'], axis=1))\n\nn_splits = 5\nn_estimators = 7\nfolds = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=77) \n\nparams = {\n        'objective': 'binary:logistic', \n        'eta': 0.015,\n        'eval_metric': 'auc', \n        'max_depth': 6, \n        'min_child_weight': 10,\n        'gamma': 1, \n        'reg_lambda': 0.3, \n        'reg_alpha': 0.07, \n        'subsample': 0.8,\n        'colsample_bytree': 0.8,\n        'silent': False, \n        }\n\n# watchlist = [(d_train, 'train'), (d_val, 'valid')]\n\nxgbCV = xgboost.cv(params=params, dtrain=d_train, stratified=True, num_boost_round=10000, feval=gini_xgb,\n                   early_stopping_rounds=100, maximize=True, verbose_eval=5, nfold=3)","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"e45ff0d0-3f16-4984-869f-0406130db0cc","_uuid":"8490ca7244881ea60b91a5b8af907d38359e5009","collapsed":true}},{"execution_count":null,"source":"from xgboost import XGBClassifier\nfrom sklearn.model_selection import StratifiedKFold\nimport gc\nfrom numba import jit\nfrom sklearn.preprocessing import LabelEncoder\nimport time \n\n@jit\ndef eval_gini(y_true, y_prob):\n    \"\"\"\n    Original author CPMP : https://www.kaggle.com/cpmpml\n    In kernel : https://www.kaggle.com/cpmpml/extremely-fast-gini-computation\n    \"\"\"\n    y_true = np.asarray(y_true)\n    y_true = y_true[np.argsort(y_prob)]\n    ntrue = 0\n    gini = 0\n    delta = 0\n    n = len(y_true)\n    for i in range(n-1, -1, -1):\n        y_i = y_true[i]\n        ntrue += y_i\n        gini += y_i * delta\n        delta += 1 - y_i\n    gini = 1 - 2 * gini / (ntrue * (n - ntrue))\n    return gini\n\ndef gini_xgb(preds, dtrain):\n    labels = dtrain.get_label()\n    gini_score = eval_gini(labels, preds)\n    return [('gini', gini_score)]\n\ntrn_df = real_train\nsub_df = real_test\n\ntarget = trn_df.target\ndel trn_df[\"target\"]\n\nn_splits = 5\nn_estimators = 7\nfolds = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=77) \nimp_df = np.zeros((len(trn_df.columns), n_splits))\nxgb_evals = np.zeros((n_estimators, n_splits))\noof = np.empty(len(trn_df))\nsub_preds = np.zeros(len(sub_df))\nincrease = True\nnp.random.seed(0)\n\nprint('Start fitting: ')\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(target, target)):\n    trn_dat, trn_tgt = trn_df.iloc[trn_idx], target.iloc[trn_idx]\n    val_dat, val_tgt = trn_df.iloc[val_idx], target.iloc[val_idx]\n\n    clf = XGBClassifier(n_estimators=n_estimators,\n                        max_depth=4,\n                        objective=\"binary:logistic\",\n                        learning_rate=0.1, \n                        subsample=.8, \n                        colsample_bytree=.8,\n                        gamma=1,\n                        reg_alpha=0,\n                        reg_lambda=1,\n                      #  min_child_weight=10, \n                        nthread=2)\n    # Upsample during cross validation to avoid having the same samples\n    # in both train and validation sets\n    # Validation set is not up-sampled to monitor overfitting\n    if increase:\n        # Get positive examples\n        pos = pd.Series(trn_tgt == 1)\n        # Add positive examples\n        trn_dat = pd.concat([trn_dat, trn_dat.loc[pos]], axis=0)\n        trn_tgt = pd.concat([trn_tgt, trn_tgt.loc[pos]], axis=0)\n        # Shuffle data\n        idx = np.arange(len(trn_dat))\n        np.random.shuffle(idx)\n        trn_dat = trn_dat.iloc[idx]\n        trn_tgt = trn_tgt.iloc[idx]\n    \n    print('{}th fold'.format(fold_))\n    clf.fit(trn_dat, trn_tgt, \n            eval_set=[(trn_dat, trn_tgt), (val_dat, val_tgt)],\n            eval_metric=gini_xgb,\n            early_stopping_rounds=None,\n            verbose=False)\n            \n    imp_df[:, fold_] = clf.feature_importances_\n    oof[val_idx] = clf.predict_proba(val_dat)[:, 1]\n    \n    # Find best round for validation set\n    xgb_evals[:, fold_] = clf.evals_result_[\"validation_1\"][\"gini\"]\n    best_round = np.argsort(xgb_evals[:, fold_])[::-1][0]\n    \n    # Display results\n    print(\"Fold %2d : %.6f @%4d / best score is %.6f @%4d\" \n          % (fold_ + 1, \n             eval_gini(val_tgt, oof[val_idx]),\n             n_estimators,\n             xgb_evals[best_round, fold_],\n             best_round))\n             \n    # Update submission\n    sub_preds += clf.predict_proba(sub_df)[:, 1] / n_splits \n          \nprint(\"Full OOF score : %.6f\" % eval_gini(target, oof))\n\n# Compute mean score and std\nmean_eval = np.mean(xgb_evals, axis=1)\nstd_eval = np.std(xgb_evals, axis=1)\nbest_round = np.argsort(mean_eval)[::-1][0]\n\nprint(\"Best mean score : %.6f + %.6f @%4d\"\n      % (mean_eval[best_round], std_eval[best_round], best_round))\n    \nimportances = imp_df.mean(axis=1)\nfor i, imp in enumerate(importances):\n    print(\"%-20s : %10.4f\" % (trn_df.columns[i], imp))\n    \nsub_df[\"target\"] = sub_preds\n\nsub_df[[\"target\"]].to_csv(\"submission_20fold.csv\", index=True, float_format=\"%.9f\")","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"55e1f14a-5c61-49b1-8100-bc4e38fabb62","_uuid":"eb30249859f28708bf849b648d3338adb6ac5318","collapsed":true}},{"execution_count":null,"source":"# from sklearn.model_selection import GridSearchCV, StratifiedKFold\n# from xgboost import XGBClassifier\n\n# X_train = real_train.drop(['id', 'target'], axis=1)\n# y_train = real_train['target'].values\n# # A parameter grid for XGBoost\n# params = {\n#         'objective': ['binary:logistic'], \n#         'min_child_weight': [10],\n#         'gamma': [1], \n#         'reg_lambda': [0.3], \n#         'subsample': [0.8],\n#         'colsample_bytree': [0.8],\n#         'max_depth': [6], \n#         'learning_rate': [0.015],\n#         'n_estimators': [700],\n#         'silent': [True], \n#        # 'nthread': [1], \n#         }\n# folds = 4\n\n# xgbc = XGBClassifier()\n# # xgbc.get_params().keys() # get the name of all parameters\n# skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=77)\n# grid_search = GridSearchCV(param_grid=params, estimator=xgbc, verbose=45, \n#                            cv=skf.split(X_train, y_train), n_jobs=4, scoring='roc_auc')","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"c756eb91-e4a4-4b75-a38f-9fa151681938","_uuid":"66051b49155b2e58711228d5cd678b7d26bd9548","collapsed":true}},{"execution_count":null,"source":"# grid_search.fit(X_train, y_train)\n# print('Best estimator: ', grid_search.best_estimator_)\n# print('Best score: ', grid_search.best_score_ * 2 - 1)\n# print('Best parameters: ', grid_search.best_params_)","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"8214ac71-d1fc-4da6-a6f5-471fb66d6db2","_uuid":"e0b4e5cccd9ee1197441c1c6b421814be7f35c93","scrolled":true,"collapsed":true}},{"execution_count":null,"source":"import xgboost as xgb\nd_train = xgb.DMatrix(train_data.drop(['id', 'target'], axis=1), train_data[['target']])\nd_val = xgb.DMatrix(val_data.drop(['id', 'target'], axis=1), val_data[['target']])\nd_test = xgb.DMatrix(real_test.drop(['id'], axis=1))\n\n# xgboost parameters\n# params = {}\n# params['objective'] = 'binary:logistic'\n# params['eta'] = 0.04\n# # params['max_delta_step'] = 8\n# params['eval_metric'] ='auc'\n# params['silent'] = True\n# params['max_depth'] = 6\n# params['subsample'] = 0.9\n# params['colsample_bytree'] = 0.9\n\nparams = {\n        'objective': 'binary:logistic', \n        'eta': 0.015,\n        'eval_metric': 'auc', \n        'max_depth': 6, \n        'min_child_weight': 10,\n        'gamma': 1, \n        'reg_lambda': 0.3, \n        'reg_alpha': 0.07, \n        'subsample': 0.8,\n        'colsample_bytree': 0.8,\n        'silent': True, \n        'n_estimators': 400,\n        }\n\nwatchlist = [(d_train, 'train'), (d_val, 'valid')]\n\nmdl_xgb = xgb.train(params, d_train, num_boost_round=10000, evals=watchlist, early_stopping_rounds=100, \n                    feval=gini_xgb, maximize=True, verbose_eval=10)\n\n# Save model to disk as soon as learning process finishes in case the kernel dies\nfilename = 'xgb_model.joblib.pkl'\n_ = joblib.dump(mdl_xgb, filename, compress=9)","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"3aa566b7-7cfd-4f57-8e80-1212c50ea66c","_uuid":"1ddfe19061818a57030b0ab528ece50f39e08e67","scrolled":true,"collapsed":true}},{"execution_count":null,"source":"","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"885f20c1-7abe-4ddf-97a7-1d03513f8783","_uuid":"8f77dc09dcac4e6b71a2f8d62193acdf60945683","scrolled":true,"collapsed":true}},{"execution_count":null,"source":"# Prediction on test set\ntest_pred = mdl_xgb.predict(data=d_test)","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"d6f7272c-72f4-4fd2-84f0-afa729cc780d","_uuid":"3b6aa932462cea3cd582d8521fe3ba9856aa4782","scrolled":true,"collapsed":true}},{"execution_count":null,"source":"# val_pred = mdl_xgb.predict(data=d_val)\n# gini_xgb(d_train=d_val, preds=val_pred)","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"9e45321b-29f1-4659-af5f-8ba839e20eaf","_uuid":"4bd9c76974bfd8063751d293ca92fffe7699f298","collapsed":true}},{"execution_count":null,"source":"# Save prediction results to csv\nsubmissions = pd.DataFrame()\nsubmissions['id'] = real_test.iloc[:, 0]\nsubmissions['target'] = test_pred\nsubmissions.to_csv('xgb_model.csv', index=False)","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"b8f56ee8-d7da-4bce-92a5-223c0f5a8aec","_uuid":"a84049942fed93715f07050f77c6fd0280875a4d","collapsed":true}},{"metadata":{"_cell_guid":"e7183a99-4688-4eca-bf66-adea22379c33","_uuid":"76134c9b21d120f4aad4b7068c9c7973da687934"},"source":"### Evaluation the model on validation data","cell_type":"markdown"},{"execution_count":null,"source":"# from sklearn.metrics import confusion_matrix\n# predVal = nnet.predict(X=train_data.drop(['id', 'target'], axis=1))\n# conf_mat = confusion_matrix(y_pred=predVal, y_true=train_data.target).transpose()\n# print(conf_mat)\n# precision = conf_mat[0, 0] / (conf_mat[0, 0] + conf_mat[0, 1])\n# recall = conf_mat[0, 0] / (conf_mat[0, 0] + conf_mat[1, 0])\n# f1 = 2 * precision * recall / (precision + recall)\n# f1","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"bf5dbaa8-6851-4265-a7ac-95831b22f59b","_uuid":"d986d0d6f30a5d8fb9cc37a0a5cb2c4919e07963","collapsed":true}},{"metadata":{"_cell_guid":"3276cae8-998e-4781-b883-4d5251b01ad2","_uuid":"5ab40ea59a9b0df69ec50597914517a11e0ce9c5","collapsed":true},"source":"### Predictions on test data","cell_type":"markdown"},{"execution_count":null,"source":"","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"4a7869b3-64cf-4512-a26c-b4f57cc3422b","_uuid":"52dcf3d6fe99851c4bf97212bc6a76c46d60a870","collapsed":true}}]}