{"metadata":{"language_info":{"file_extension":".py","codemirror_mode":{"version":3,"name":"ipython"},"nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3","mimetype":"text/x-python","name":"python"},"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"}},"nbformat":4,"cells":[{"source":"**You can find final submission in the input data.** \n","metadata":{"_cell_guid":"748d7f4d-f1b1-4bc7-bf2c-0f4f5c443686","_uuid":"04a142c0e29d07f336a5a5fc0fd25d3e8f2dce1b"},"cell_type":"markdown"},{"source":"Hello everyone. In this kernel I want to share one of our solutions. \n\nHere will be fragments of the code with changed model parametrs because of forgotten values, but all submission files will be provided. ","metadata":{"_cell_guid":"302bd0c4-37e5-4099-a011-a661e772b536","_uuid":"ddbaa04d0af865ed7f52ea84569b3fa6b2cd3f7e","_kg_hide-input":true},"cell_type":"markdown"},{"source":"We took 3 different XGBoost models:\n- 2 models with 5Kfolds, but with different parametrs (eta, max_depth)\n- 1 model with 4Kfolds\n","metadata":{"_cell_guid":"8f4a1a49-6148-44a9-bc1d-3f44f4e0c820","_uuid":"e163e1985dc896b2a4283de937c966c82ced8a74"},"cell_type":"markdown"},{"source":"Also, we made feature binarization and scaling by my class:","metadata":{"_cell_guid":"807beaff-9483-4f47-a02d-6d458331771b","_uuid":"4953a8776fd809719df45b74eff9afa0520a17db"},"cell_type":"markdown"},{"execution_count":null,"source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler, LabelBinarizer\nimport xgboost as xgb\nclass FeatureBinarizatorAndScaler:\n    \"\"\" This class needed for scaling and binarization features\n    \"\"\"\n    NUMERICAL_FEATURES = list()\n    CATEGORICAL_FEATURES = list()\n    BIN_FEATURES = list()\n    binarizers = dict()\n    scalers = dict()\n\n    def __init__(self, numerical=list(), categorical=list(), binfeatures = list(), binarizers=dict(), scalers=dict()):\n        self.NUMERICAL_FEATURES = numerical\n        self.CATEGORICAL_FEATURES = categorical\n        self.BIN_FEATURES = binfeatures\n        self.binarizers = binarizers\n        self.scalers = scalers\n\n    def fit(self, train_set):\n        for feature in train_set.columns:\n\n            if feature.split('_')[-1] == 'cat':\n                self.CATEGORICAL_FEATURES.append(feature)\n            elif feature.split('_')[-1] != 'bin':\n                self.NUMERICAL_FEATURES.append(feature)\n\n            else:\n                self.BIN_FEATURES.append(feature)\n        for feature in self.NUMERICAL_FEATURES:\n            scaler = StandardScaler()\n            self.scalers[feature] = scaler.fit(np.float64(train_set[feature]).reshape((len(train_set[feature]), 1)))\n        for feature in self.CATEGORICAL_FEATURES:\n            binarizer = LabelBinarizer()\n            self.binarizers[feature] = binarizer.fit(train_set[feature])\n\n\n    def transform(self, data):\n        binarizedAndScaledFeatures = np.empty((0, 0))\n        for feature in self.NUMERICAL_FEATURES:\n            if feature == self.NUMERICAL_FEATURES[0]:\n                binarizedAndScaledFeatures = self.scalers[feature].transform(np.float64(data[feature]).reshape(\n                    (len(data[feature]), 1)))\n            else:\n                binarizedAndScaledFeatures = np.concatenate((\n                    binarizedAndScaledFeatures,\n                    self.scalers[feature].transform(np.float64(data[feature]).reshape((len(data[feature]),\n                                                                                       1)))), axis=1)\n        for feature in self.CATEGORICAL_FEATURES:\n\n            binarizedAndScaledFeatures = np.concatenate((binarizedAndScaledFeatures,\n                                                         self.binarizers[feature].transform(data[feature])), axis=1)\n\n        for feature in self.BIN_FEATURES:\n            binarizedAndScaledFeatures = np.concatenate((binarizedAndScaledFeatures, np.array(data[feature]).reshape((\n                len(data[feature]), 1))), axis=1)\n        print(binarizedAndScaledFeatures.shape)\n        return binarizedAndScaledFeatures","metadata":{"_cell_guid":"cd34eb71-d013-4e3e-a399-522ab0577fec","_uuid":"958dfd51abb4671c66dabdc61988ce58f126d6ee","collapsed":true},"outputs":[],"cell_type":"code"},{"source":"Gini coefficient:","metadata":{"_cell_guid":"4f48a756-3e0d-45f7-96cc-f4a118c28a2a","_uuid":"7abae331092757be9df1f3fef5d7ded9dabdf0d2"},"cell_type":"markdown"},{"execution_count":null,"source":"def gini(actual, pred, cmpcol=0, sortcol=1):\n    assert (len(actual) == len(pred))\n    all = np.asarray(np.c_[actual, pred, np.arange(len(actual))], dtype=np.float)\n    all = all[np.lexsort((all[:, 2], -1 * all[:, 1]))]\n    totalLosses = all[:, 0].sum()\n    giniSum = all[:, 0].cumsum().sum() / totalLosses\n\n    giniSum -= (len(actual) + 1) / 2.\n    return giniSum / len(actual)\n\n\ndef gini_normalized(a, p):\n    return gini(a, p) / gini(a, a)\n\n\ndef gini_xgb(preds, dtrain):\n    labels = dtrain.get_label()\n    gini_score = gini_normalized(labels, preds)\n    return [('gini', gini_score)]\n","metadata":{"_cell_guid":"c3324459-2b8e-40d7-93cf-07c8d436eff4","_uuid":"d9bbba07c9912ffa50bac99c1734821c8712b788","collapsed":true},"outputs":[],"cell_type":"code"},{"source":"Let's create the submission file. All models will be after:","metadata":{"_cell_guid":"63ba53b0-f071-4dda-88f4-374da829c88d","_uuid":"ae9b6c7a12e92001de991491b1a8a0b6c0f7a692"},"cell_type":"markdown"},{"execution_count":null,"source":"first = pd.read_csv('../input/results/5folds(v. 1).csv')\nsecond  = pd.read_csv('../input/results/5folds(v. 2).csv')\nthird =  pd.read_csv('../input/results/4folds.csv')\nfourth = pd.read_csv('../input/results/LightGBM.csv')\nfifth = pd.read_csv('../input/results/NN.csv')\n\na = pd.DataFrame()\na['id'] = first['id']\na['target'] = (first['target']+second['target']+third['target']+fourth['target']+fifth['target'])/5\na.to_csv('submission.csv')","metadata":{"_cell_guid":"1485ef66-c612-4c37-8b51-bd968347edd1","_uuid":"09f73dc763281feb762d034b5db275e8192d750e","collapsed":true},"outputs":[],"cell_type":"code"},{"source":"Here is our preprocessing approach:","metadata":{"_cell_guid":"b76927d7-cf8f-4d4f-8c58-d4872c6f7f4c","_uuid":"f17e021b725d5f46d8ced3d5439861d5aa33526e"},"cell_type":"markdown"},{"execution_count":null,"source":"def preproc(X_train):\n    # Adding new features and deleting features with low importance\n    multreg = X_train['ps_reg_01'] * X_train['ps_reg_03'] * X_train['ps_reg_02']\n    ps_car_reg = X_train['ps_car_13'] * X_train['ps_reg_03'] * X_train['ps_car_13']\n    X_train = X_train.drop(['ps_calc_01', 'ps_calc_02', 'ps_calc_03', 'ps_calc_04', 'ps_calc_05', 'ps_calc_06',\n                            'ps_calc_07', 'ps_calc_08', 'ps_calc_09', 'ps_calc_10', 'ps_calc_11', 'ps_calc_12',\n                            'ps_calc_13', 'ps_calc_14', 'ps_calc_15_bin', 'ps_calc_16_bin', 'ps_calc_17_bin',\n                            'ps_calc_18_bin', 'ps_calc_19_bin', 'ps_calc_20_bin', 'ps_car_10_cat', 'ps_ind_10_bin',\n                            'ps_ind_13_bin', 'ps_ind_12_bin'], axis=1)\n    X_train['mult'] = multreg\n    X_train['ps_car'] = ps_car_reg\n    X_train['ps_ind'] = X_train['ps_ind_03'] * X_train['ps_ind_15']\n    return X_train\n","metadata":{"_cell_guid":"58640db0-5f60-48a4-b5e6-5ca71196985b","_uuid":"ab6895cfc80078fa46eaff13343b7828258b1b68","collapsed":true},"outputs":[],"cell_type":"code"},{"source":"And main XGBoost body:\n","metadata":{"_cell_guid":"5c568415-cac6-4b7c-b725-3bb3c21a09f2","_uuid":"8a5bafb824d5cf029a7c42abc4c5477a9d0f2f66"},"cell_type":"markdown"},{"execution_count":null,"source":"X_train = pd.read_csv('../input/porto-seguro-safe-driver-prediction/train.csv')\ny_train = X_train['target']\nX_train = X_train.drop(['id', 'target'], axis=1)\nX_test = pd.read_csv('../input/porto-seguro-safe-driver-prediction/test.csv')\nX_test = X_test.drop(['id'], axis=1)\nX_train = preproc(X_train)\nX_test = preproc(X_test)\n\nbinarizerandscaler = FeatureBinarizatorAndScaler()\nbinarizerandscaler.fit(X_train)\nX_train = binarizerandscaler.transform(X_train)\nX_test = binarizerandscaler.transform(X_test)\n\n# Kinetic features https://www.kaggle.com/alexandrudaia/kinetic-and-transforms-0-482-up-the-board\nkinetic_train = []\nfor i in range(4):\n    kinetic_train = pd.read_csv('../input/kinetic-features/'+str(i+1)+'k.csv').iloc[:, 1:2]\n    X_train = np.concatenate((X_train, np.array(kinetic_train).reshape((len(kinetic_train), 1))), axis=1)\n    kinetic_test = pd.read_csv('../input/kinetic-features/'+str(i+1)+'kt.csv').iloc[:, 1:2]\n    X_test = np.concatenate((X_test, np.array(kinetic_test).reshape((len(kinetic_test), 1))), axis=1)\n\ni = 0\nK = 5\nkf = KFold(n_splits=K, random_state=42, shuffle=True)\n# 5 Cross Validation\nresults = []\nfor train_index, test_index in kf.split(X_train):\n    train_X, valid_X = X_train[train_index], X_train[test_index]\n    train_y, valid_y = y_train[train_index], y_train[test_index]\n    weights = np.zeros(len(y_train))\n    weights[y_train == 0] = 1\n    weights[y_train == 1] = 1\n    print(weights, np.mean(weights))\n    watchlist = [(xgb.DMatrix(train_X, train_y, weight=weights), 'train'), (xgb.DMatrix(valid_X, valid_y), 'valid')]\n    # Setting parameters for XGBoost model\n    params = {'eta': 0.03, 'max_depth': 4, 'objective': 'binary:logistic', 'seed': 42, 'silent': True}\n    model = xgb.train(params, xgb.DMatrix(train_X, train_y, weight=weights), 1500, watchlist,  maximize=True, verbose_eval=5,\n                        feval=gini_xgb, early_stopping_rounds=100)\n    resy = pd.DataFrame(model.predict(xgb.DMatrix(X_test)))\n    i += 1\n    # Saving results for all CV models\n    results.append(resy)\n    # resy.to_csv(str(i)+'fold.csv')\n\n# Creating the submission file\nsubmission = pd.DataFrame((results[0]+results[1]+results[2]+results[3]+results[4])/5)\n#submission.to_csv('sumbission.csv')","metadata":{"_cell_guid":"0ce2555a-9263-4571-b7ad-ce4702debc0c","_uuid":"cdb0c393b7ef2169e395d5b2d7fead27bfe6629e","collapsed":true},"outputs":[],"cell_type":"code"},{"source":"","metadata":{"_cell_guid":"3053e599-8577-4476-be46-d4c18fd3b0cf","_uuid":"7dfc3c9756c826d25d4cf4990f7e588dbfc9b757"},"cell_type":"markdown"},{"source":"I've took this LightGBM https://www.kaggle.com/the1owl/forza-baseline-lightgbm-example but I also change weights. Many thanks to the1owl.","metadata":{"_cell_guid":"a64a9076-62da-4eeb-8703-521c037eae6b","_uuid":"1491416aae4349c9cafc67cf422ffd9b8e79ad6d","collapsed":true},"cell_type":"markdown"},{"source":"We took about 20 NN models and calculated average.\nHere is one of the examples:","metadata":{"_cell_guid":"ff35f58f-d99c-4b42-9713-df5088cac5c5","_uuid":"eea0469dee2fd9d23223cc06da7fc264b566c742"},"cell_type":"markdown"},{"execution_count":null,"source":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.optimizers import SGD\nfrom keras.initializers import random_uniform\n\nimport pandas as pd\n\nX_train = pd.read_csv('../input/porto-seguro-safe-driver-prediction/train.csv')\ny_train = X_train['target']\nX_test = pd.read_csv('../input/porto-seguro-safe-driver-prediction/test.csv')\nX_test = X_test.drop(['id'], axis=1)\nX_train = X_train.drop(['id', 'target'], axis = 1)\ny_train1 = abs(-1+y_train)\ny_train = pd.concat([y_train, y_train1], axis=1)\nbinarizerandscaler = FeatureBinarizatorAndScaler()\nbinarizerandscaler.fit(X_train)\nX_train = binarizerandscaler.transform(X_train)\nX_test = binarizerandscaler.transform(X_test)\ny_train = y_train.as_matrix()\n\n\n#hyperparameters\ninput_dimension = 226\nlearning_rate = 0.0025\nmomentum = 0.85\nhidden_initializer = random_uniform(seed=SEED)\ndropout_rate = 0.2\n\n\n# create model\nmodel = Sequential()\nmodel.add(Dense(128, input_dim=input_dimension, kernel_initializer=hidden_initializer, activation='relu'))\nmodel.add(Dropout(dropout_rate))\nmodel.add(Dense(64, kernel_initializer=hidden_initializer, activation='relu'))\nmodel.add(Dense(2, kernel_initializer=hidden_initializer, activation='softmax'))\n\nsgd = SGD(lr=learning_rate, momentum=momentum)\nmodel.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['acc'])\nmodel.fit(X_train, y_train, epochs=130, batch_size=128)\npredictions = model.predict_proba(X_test)\n\nans = pd.DataFrame(predictions)\nans = ans[0]\n#ans.to_csv('./ans.csv', index=False)\n\n\n    #save the model\nmodel_json = model.to_json()\n#with open(\"./ans.json\", \"w\") as json_file:\n#    json_file.write(model_json)\n\n #   model.save_weights(\"./ans.h5\")\n","metadata":{"_cell_guid":"c2120a75-203d-4b52-ba83-f6d4cb367acb","_uuid":"f46fe561148ae557387be23423e08c4a7b0c622b","collapsed":true},"outputs":[],"cell_type":"code"},{"source":"","metadata":{"_cell_guid":"bd512109-1105-4520-84b4-3c498ef0d9b8","_uuid":"6c96a86f7f155628024329ed7c0956f92d42b798"},"cell_type":"markdown"}],"nbformat_minor":1}