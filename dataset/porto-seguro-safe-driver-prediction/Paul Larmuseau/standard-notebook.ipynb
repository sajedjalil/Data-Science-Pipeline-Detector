{"nbformat_minor":1,"nbformat":4,"metadata":{"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","name":"python","codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","version":"3.6.1"},"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"}},"cells":[{"outputs":[],"metadata":{"scrolled":true,"collapsed":true,"_cell_guid":"2d929b28-96a4-435f-b3ad-f1cba55a4b40","_uuid":"dc77766e5fe76a18b9231545af4c94031a317a82"},"cell_type":"code","execution_count":null,"source":"import seaborn as sns\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n# read data into dataset variable\ntrain = pd.read_csv(\"../input/train.csv\") #[:20000]\ntest = pd.read_csv(\"../input/test.csv\") #[:10000]\n#train=train.append(test)\ntrain.describe().T\n"},{"outputs":[],"metadata":{"scrolled":false,"collapsed":true,"_cell_guid":"712e741e-eb19-4a00-9b1c-af590e80bd57","_uuid":"1a6ca37710a82df873c2425bbcd66e60c0878bc5"},"cell_type":"code","execution_count":null,"source":"from sklearn.linear_model import OrthogonalMatchingPursuit,RANSACRegressor,LogisticRegression,ElasticNetCV,HuberRegressor, Ridge, Lasso,LassoCV,Lars,BayesianRidge,SGDClassifier,LogisticRegressionCV,RidgeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import MinMaxScaler,PolynomialFeatures\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nparam_grid = {'C': [0.1,1, 10, 100, 1000], 'gamma': [1,0.1,0.01,0.001,0.0001], 'kernel': ['rbf']}\n\n\nX = train.drop('target',axis=1).fillna(0) \n\ndef rmsle(y_predicted, y_real):\n    return np.sqrt(np.mean(np.power(np.log1p(y_predicted)-np.log1p(y_real), 2)))\ndef procenterror(y_predicted, y_real):\n     return np.round( np.mean(np.abs(y_predicted-y_real) )/ np.mean(y_real) *100 ,1)\n\n    \n\n\nY=train['target'].fillna(0)\nscaler = MinMaxScaler()\nscaler.fit(X)\nX=scaler.transform(X)\n#poly = PolynomialFeatures(2)\n#X=poly.fit_transform(X)\n\n#print(X)\n\nnames = [\n         #'ElasticNet',\n         #'SVC',\n         #'kSVC',\n         #'KNN',\n         'DecisionTree',\n         'RandomForestClassifier',\n         #'GridSearchCV',\n         #'HuberRegressor',\n         #'Ridge',\n         #'Lasso',\n         #'LassoCV',\n         #'Lars',\n         #'BayesianRidge',\n         #'SGDClassifier',\n         #'RidgeClassifier',\n         #'LogisticRegression',\n         #'OrthogonalMatchingPursuit',\n         #'RANSACRegressor',\n         ]\n\nclassifiers = [\n    #ElasticNetCV(cv=10, random_state=0),\n    #SVC(),\n    #SVC(kernel = 'rbf', random_state = 0),\n    #KNeighborsClassifier(n_neighbors = 1),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(n_estimators = 200),\n    #GridSearchCV(SVC(),param_grid, refit = True, verbose = 1),\n    #HuberRegressor(fit_intercept=True, alpha=0.0, max_iter=100,epsilon=2.95),\n    #Ridge(fit_intercept=True, alpha=0.0, random_state=0, normalize=True),\n    #Lasso(alpha=0.05),\n    #LassoCV(),\n    #Lars(n_nonzero_coefs=10),\n    #BayesianRidge(),\n    #SGDClassifier(),\n    #RidgeClassifier(),\n    #LogisticRegression(),\n    #OrthogonalMatchingPursuit(),\n    #RANSACRegressor(),\n]\ncorrection= [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n\ntemp=zip(names,classifiers,correction)\n#print(temp)\n\nfor name, clf,correct in temp:\n    regr=clf.fit(X,Y)\n    #print( name,'% errors', abs(regr.predict(X)+correct-Y).sum()/(Y.sum())*100)\n    print(name,'%error',procenterror(regr.predict(X),Y),'rmsle',rmsle(regr.predict(X),Y))\n    from sklearn.metrics import classification_report, confusion_matrix, accuracy_score,f1_score, precision_score, recall_score\n\n    # Confusion Matrix\n    print(name,'Confusion Matrix')\n    print(confusion_matrix(Y, np.round(regr.predict(X) ) ) )\n    print('--'*40)\n\n    # Classification Report\n    print('Classification Report')\n    print(classification_report(Y,np.round( regr.predict(X) ) ))\n\n    # Accuracy\n    print('--'*40)\n    logreg_accuracy = round(accuracy_score(Y, np.round( regr.predict(X) ) ) * 100,2)\n    print('Accuracy', logreg_accuracy,'%')\n    \n    # Create a submission file\n    sub = pd.DataFrame()\n    sub['id'] = test['id']\n    sub['target'] = regr.predict(test)\n    sub.to_csv(name, index=False)\n\n    print(sub.head())\n\n    "}]}