{"cells":[{"source":"**Introduction**\nThis jupyter notebook intends to employ a few visualization techniques for high-dimensional data such as PCA and TSNE to see if we can find any separation between target 0 and 1. Before that, some basic data cleanings are performed.\n\nContents include:\n- deal with nan\n- feature selection\n- correlation matrix\n- outlier deletion & normalization\n- feature importance analysis\n- dimensionality reduction and visualization","cell_type":"markdown","metadata":{"_cell_guid":"1e4429e3-4a19-47c1-a2db-2c6ec65fa42e","_uuid":"282a5ef09305ea1693d7dd1ddaf70a405aa64b9f"}},{"source":"\nLoading data and checking the first sevaral rows","cell_type":"markdown","metadata":{"_cell_guid":"7f424fa0-778d-428d-b686-1fff2494e6db","_uuid":"2b16e6c9bca6da670cb40e2386ce594f75d95473"}},{"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.\n\n\n# import libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# import training data\ntrain = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")\n# submit_exp = pd.read_csv(\"../input/sample_submission.csv\")\n# print(submit_exp.head())\n\n# train and test\nid_train = train[\"id\"]\ny_train = train[\"target\"]\nX_train = train.drop([\"id\",\"target\"], axis=1)\n\nid_test = test[\"id\"]\nX_test = test.drop([\"id\"], axis=1)\n\nprint(X_train.head())\n# print(X_test.head())","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"f7628b16-67a5-4231-ae2b-0f7159a643ff","_uuid":"1da703ebfe8876813e68bfd0041950b441af4eea"}},{"source":"Dealing with nans using \"missingno\" as a visualization method","cell_type":"markdown","metadata":{"_cell_guid":"24892d54-3279-449e-a4d9-aff6c91c0d3a","_uuid":"f301248254cfdeb509beb1d95f8aa0ff315c7358"}},{"source":"# -1 means nan in this case...so put nan back\nX_train = X_train.replace(-1, np.NaN)\nX_test = X_test.replace(-1, np.NaN)\n\n# concatenate train and test to deal with nan together\nXmat = pd.concat([X_train, X_test])\n\n# visualize the number of nans in each column\n# (shamelessly adapted from:\n#https://www.kaggle.com/arthurtok/interactive-porto-insights-a-plot-ly-tutorial)\nimport missingno as msno\n\nmsno.matrix(df=X_train.iloc[:,:39], figsize=(20,14), color=(0.5,0,0))","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"820bfdbf-996a-481b-a7c1-d38c8f38fbcb","_uuid":"0ca46577ef8849c84ade09849cd56127144427f2"}},{"source":"For some columns having around 50% of nans, the presence of nan itself may be meaningful.\n","cell_type":"markdown","metadata":{"_cell_guid":"98ee4e21-cd27-4a58-a123-60afb053123e","_uuid":"d8ae258692622ff109239cef1308a0e7e4142894"}},{"source":"# Columns with many nans itself may be meaningful\ndef nan2bi(x):\n    if np.isnan(x):\n        return 1\n    else:\n        return 0\n\nXmat = pd.concat([X_train, X_test])\ncols = [\"ps_reg_03\",\"ps_car_03_cat\",\"ps_car_05_cat\"]\nfor c in cols:\n    Xmat[c + \"_isnan\"] = Xmat[c].apply(nan2bi)\n    \n# For other columns replace nan with median\nXmat = Xmat.fillna(Xmat.median())\n\n# remove other columns with nan, if any\nXmat = Xmat.dropna(axis=1)\nprint(Xmat.shape)","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"aecfd8aa-6b3f-4307-a350-ac48b3334e3e","_uuid":"790c2d99f5e6e686aad2445e98bbb80ab4c81b08"}},{"source":"There are binary predictors...let's see if they are skewed or evenly distributed.\n","cell_type":"markdown","metadata":{"_cell_guid":"944c5050-a258-4105-bcb0-9eba62f741e5","_uuid":"598bbf3155759dd746c85787be231377adec0905"}},{"source":"# some of binary variables can be skewed\nbin_col = [col for col in Xmat.columns if '_bin' in col]\ncounts = []\nfor col in bin_col:\n    counts.append(100*(Xmat[col]==1).sum()/Xmat.shape[0])\n\nax = sns.barplot(x=counts, y=bin_col, orient='h')\nax.set(xlabel=\"% of 1 in a column\")\nplt.show()\n","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"2e808813-2064-4e7d-8c1b-b7e188ae3914","scrolled":true,"_uuid":"93a50c2f6665616e34f8a87fdd722c4e9725f14e"}},{"source":"Let's remove some columns with very skewed data (**\"ps_ind_10_bin\",\"ps_ind_11_bin\"**, **\"ps_ind_12_bin\"**and **\"ps_ind_13_bin\"**).","cell_type":"markdown","metadata":{"_cell_guid":"8ae6c41f-ee38-43c9-9893-a78b51bc94f7","_uuid":"379c935547a312ddb49e9335217c7d7091945cb1"}},{"source":"# upon visual inspection, some columns with skewed data are removed\nXmat = Xmat.drop([\"ps_ind_10_bin\",\"ps_ind_11_bin\",\"ps_ind_12_bin\",\"ps_ind_13_bin\"], axis=1)\nprint(Xmat.shape)","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"95e6c56e-399b-4641-a540-14aa39f1e5e5","_uuid":"344511116dc863fd5459e1df978d8112150d6148"}},{"source":"As a custom, let's see the correlation matrix between predictors.","cell_type":"markdown","metadata":{"_cell_guid":"edd32c17-163b-4471-a6e8-6361e7a5a6e5","_uuid":"5c81c2233952fe81d29049d3db9f0010034f39e8"}},{"source":"# check correlation matrix\nsns.set(style=\"white\")\n\n# Compute the correlation matrix (let's put y_train back this time)\nXcorrmat = Xmat.iloc[:X_train.shape[0],:]\nXcorrmat['target'] = y_train\ncorr = Xcorrmat.corr()\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(20,12))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n\nplt.show()","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"efd383fd-2f44-4696-ba84-0dba9127ab58","_uuid":"09d6bd5de840130c92f0047a60412e1ca7d585d6"}},{"source":"We can drop \"_calc\", as they do not show any dependency on other predictors.\nAlso, let's just normalize data using z-scoring.","cell_type":"markdown","metadata":{"_cell_guid":"fe951dcd-dd55-4edd-a6d8-a79119b369e2","_uuid":"0fb31a3321173acda4a4866e8eb79f85790db99b"}},{"source":"# drop all the \"_calc\"\ncalc_col = [col for col in Xmat.columns if '_calc' in col]\nXmat = Xmat.drop(calc_col, axis=1)\n\n# zscoring as a means of normalization\nX_train = Xmat.iloc[:X_train.shape[0],:]\nX_test = Xmat.iloc[X_train.shape[0]:,:]\nX_train = (X_train - X_train.mean())/X_train.std()\nX_test = (X_test - X_test.mean())/X_test.std()\n\n# vizualize\nf, ax = plt.subplots(figsize=(11, 9))\nsns.heatmap(X_train, cmap=cmap)\nplt.show()\n","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"ab38ccbc-84ca-4808-9fbe-ea5bbafbd861","scrolled":true,"_uuid":"2ced328b74d6b16ad6bd45917fbfc218a4d279b2"}},{"source":"Apparently there are some outliers in 'ps_ind_14', and 'ps_car_10_cat'. Let's remove these columns.","cell_type":"markdown","metadata":{"_cell_guid":"a9c339b6-1789-48ba-b015-367284221f0b","_uuid":"f636bcadc69df344cecd8ffd40579adfbe49a43e"}},{"source":"# outlier deletion\nX_train = X_train.drop(['ps_ind_14','ps_car_10_cat'], axis=1)\nX_test = X_test.drop(['ps_ind_14','ps_car_10_cat'], axis=1)\n\nX_train = (X_train - X_train.mean())/X_train.std()\nX_test = (X_test - X_test.mean())/X_test.std()\n\n# vizualize\nf, ax = plt.subplots(figsize=(11, 9))\nsns.heatmap(X_train, cmap=cmap)\nplt.show()\n","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"878d033b-7764-4a67-863e-0e763bb577cb","_uuid":"62290181041d1fe1614936504169d2a4f872b56b"}},{"source":"Let's use random forest classifier to let us know the importance of features.","cell_type":"markdown","metadata":{"_cell_guid":"7e5eb3f7-21e0-49ed-810b-d21d8882ee96","_uuid":"cd3dfeb0448f57676d297cdf05ef1a1aeaf18e97"}},{"source":"# feature importance using random forest\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=100, max_features='sqrt')\nrf.fit(X_train, y_train)\n\nprint('Training done using Random Forest')\n\nranking = np.argsort(-rf.feature_importances_)\nf, ax = plt.subplots(figsize=(11, 9))\nsns.barplot(x=rf.feature_importances_[ranking], y=X_train.columns.values[ranking], orient='h')\nax.set_xlabel(\"feature importance\")\nplt.show()\n","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"f7b4c92d-840f-4ce7-8fb6-cc68966c1e2b","_uuid":"dc9bcf652015b35ccc7463b3d8e5a23b59f41fcb"}},{"source":"So the top 3 important features are **'ps_car_13', 'ps_reg_03',  **and** 'ps_car_14'**.","cell_type":"markdown","metadata":{"_cell_guid":"c35da309-aabf-495a-aed4-2dadc9eab1c1","_uuid":"c32219b7779cad397e1d62262db8362c7db8cb1e"}},{"source":"To visualize the data split by the target value in a comprehensive way, let's split the training data based on the target value. ","cell_type":"markdown","metadata":{"_cell_guid":"15454ce1-8ef9-4629-acb6-d53b0cb02af0","_uuid":"068855c39ad4ad0c2e05468e47f9aba7e80047db"}},{"source":"# dimensioanlity reduction and visualization\nXdr = X_train\nXdr['target'] = y_train\nXdr1 = Xdr.loc[y_train==1, :]\nXdr0 = Xdr.loc[y_train==0, :]\n\nprint('rows for target 1: ' + str(Xdr1.shape[0]))\nprint('rows for target 0: ' + str(Xdr0.shape[0]))","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"d49540cc-ab77-47a2-bc4f-e81938f8eb1b","_uuid":"4b5daf1cdd4252a656d91ca0cc4b3a4ec8c7951e"}},{"source":"Target values are skewed...we need to deal with it.","cell_type":"markdown","metadata":{"_cell_guid":"978e6f75-9851-4267-91d4-acc1077b3cc2","_uuid":"87cc2cb518448607032275cee24cc402d14ebb8d"}},{"source":"# random sampling from X_train0, as the target value distribution is skewed\n# use N = 20,000 samples for now\nN = 20000\nnp.random.seed(20171021)\nXdr0 = Xdr0.iloc[np.random.choice(Xdr0.shape[0], N), :]\nXdr1 = Xdr1.iloc[np.random.choice(Xdr1.shape[0], N), :]\n\nXdr = pd.concat([Xdr0, Xdr1])","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"98c397d6-26d5-48e2-ab1b-13ca2dec1f62","_uuid":"48b2cb801e66db6be27de898d3b882051e3127c1"}},{"source":"**Pairplot**\nLet's use the first 7 important features to see if they show any separation between target 0 and 1.","cell_type":"markdown","metadata":{"_cell_guid":"e8fb18da-36f2-4880-a6b0-d3a88cc1738c","_uuid":"ef7f74ac6fb02799e356bc2203abac28f5140d42"}},{"source":"# pairplot\nXpair =pd.concat([Xdr.iloc[:,ranking[:7]], Xdr['target']], axis=1)\n\nax = sns.pairplot(Xpair, hue='target')\nplt.show()","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"a7ff8069-5e48-43f4-921b-6d99fff5ce3c","scrolled":false,"_uuid":"fc2fa0d1fb1b3c2f7c96a81828d518676b607580"}},{"source":"**PCA**","cell_type":"markdown","metadata":{"_cell_guid":"2fbc6b2a-c6f8-4f03-82ea-1f85afcf945c","_uuid":"5e5d42c09162a98f21a37804592355ac81cf97ab"}},{"source":"Xdr = Xdr.drop(['target'], axis=1)\n\n# PCA\nfrom sklearn.decomposition import PCA\n\npcamat = PCA(n_components=2).fit_transform(Xdr)\n\nplt.figure()\nplt.scatter(pcamat[:Xdr0.shape[0],0],pcamat[:Xdr0.shape[0],1], c='b', label='targ 0', alpha=0.3)\nplt.scatter(pcamat[Xdr0.shape[0]:,0],pcamat[Xdr0.shape[0]:,1],c='r', label='targ 1', alpha=0.3)\nplt.legend()\nplt.title('PC space')\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.tight_layout()\nplt.show()\nprint(\"PCA done\")\n","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"de3fe797-b409-4545-9e50-33de5a21ca98","_uuid":"175b48ada8a79b764f9d365baaabb4e58a61ae54"}},{"source":"**TSNE** ","cell_type":"markdown","metadata":{"_cell_guid":"9cedb06d-2049-4dca-b296-a7a0de0de777","_uuid":"b515c09cc1cc61ef050e08bf97be60714d35ecb7"}},{"source":"# TSNE \nfrom sklearn.manifold import TSNE\n\ntsnemat = TSNE(n_components=2, random_state=0).fit_transform(Xdr)\n\nplt.figure()\nplt.scatter(tsnemat[:Xdr0.shape[0],0],tsnemat[:Xdr0.shape[0],1], c='b', label='targ 0', alpha=0.3)\nplt.scatter(tsnemat[Xdr0.shape[0]:,0],tsnemat[Xdr0.shape[0]:,1],c='r', label='targ 1', alpha=0.3)\nplt.legend()\nplt.title('TSNE space')\nplt.xlabel('dim 1')\nplt.ylabel('dim 2')\nplt.tight_layout()\nplt.show()\nprint(\"TSNE done\")\n","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"21d0a3f8-3a37-436f-a036-63b5c534f9ac","_uuid":"22367360ae9f2108511b3abf2540cebf196c1060"}},{"source":"Unfortunately we see no separation between target 0 and 1 in either pairplot, PC space, or TSNE space. This competition is apparently a very hard one;( \n\nStill, like other Kagglers, using **XGBoost **as a classification algorithm seems to be a way to achieve high score. ","cell_type":"markdown","metadata":{"_cell_guid":"ef8fe84c-fc08-4234-9b93-315c3e022658","_uuid":"7e9f2c1e51fe4d854ca93da0c3196dcf9ec925be"}},{"source":"","cell_type":"markdown","metadata":{"_cell_guid":"edf34b12-1e1e-4b20-b91f-9d81f91db1cf","_uuid":"0d564526b6fbcc45a9324f5808a16f6a8cdb0825"}},{"source":"","cell_type":"markdown","metadata":{"_cell_guid":"a8da415a-4fb6-467c-92ad-c738126bcb5a","_uuid":"1e0daeaefa273e019cd9dba9828f5fb16336d1e2"}},{"source":"","cell_type":"markdown","metadata":{"_cell_guid":"f438c4f9-1df2-4e0c-9ca9-9451a3e953eb","_uuid":"28ecbe136d282a46296b92650acae10ce0f71b44"}},{"source":"","cell_type":"markdown","metadata":{"_cell_guid":"b2276fab-16f8-4c9b-a7d2-7694c93abdb8","_uuid":"547a0e48ac9a13a10cfe76a58c42785f208b845d"}},{"source":"","cell_type":"markdown","metadata":{"_cell_guid":"4d4c5bc3-f1f2-4caa-9a8a-811108981373","_uuid":"075b848c24701d1968c1f4acb1519de0c0091528"}}],"nbformat":4,"nbformat_minor":1,"metadata":{"language_info":{"codemirror_mode":{"version":3,"name":"ipython"},"name":"python","mimetype":"text/x-python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3","file_extension":".py"},"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"}}}