{"metadata":{"language_info":{"codemirror_mode":{"version":3,"name":"ipython"},"file_extension":".py","version":"3.6.3","nbconvert_exporter":"python","name":"python","pygments_lexer":"ipython3","mimetype":"text/x-python"},"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"}},"cells":[{"metadata":{"_cell_guid":"c73d9e8c-028c-443d-8d3b-c4cc2b92257a","_uuid":"f2517a98c4b0b920265fca01b8f24556b2122d70"},"cell_type":"markdown","source":"# Overview\n\n\n## The problem\n\nThe Porto Seguro data set is heavily unbalanced, with positive examples around 3% of the total data set. This makes it hard for conventional neural network loss functions to work with. We can use class weights to correct this, but then we will tend to overfit to the positive examples. That might be ok if we, say, had only 3% pictures of dogs in a sample of pictures of cats and dogs, but the problem is that a driver doesn't *always* claim in the same way that a dog is *always* a dog; the dataset is inherently noisy and probablistic by it's nature. This is where a ranking based metric like Gini or AUC can be useful.\n\nHowever, this is a problem for neural nets, as AUC (and hence Gini which is (2AUC-1) is not differentiable. Using conventional gradient descent metrics such as binary cross entropy will lead us to just classify everything as in class zero, which will give around 97% accuracy, with a good chance of the relative predictions within a class - on which AUC depends - being very noisy as we haven't trained our network to optimise them.\n\n## The solution\n\nThis notebook demonstrates a custom loss function for neural nets, that provides a differentiable approximation to AUC. AUC, in turn, has a linear relationship with Gini, hence this is very useful when we want to train a network to maximise AUC.\n\nWe set up 2 identical NNs and run them for a few epochs, to show how this approach improves convergence on AUC compared to binary crossentropy.\n\nI've used this to get a network that has a local CV AUC around 0.642, which corresponds to Gini of 0.284. The performance on the LB test set is considerably worse (around 0.270) in the one case I tested - mainly I've used them as inputs to blends.\n\nThis is hacked together from various bits of my local code, and hasn't been thoroughly tested, so let please me know of any bugs etc.\n\nI would have coded as a script, but I need to use the Theano backend as the AUC function uses Theano specific code. If anyone knows how to make Kaggle Kernels use the Theano backend for script, let me know, and I'll post it as a script.\n\nThis is my pretty much my first kernel so feedback welcome.\n\n**Imports and constants**\n\nFirst things first...note that we need the Theano backend. Converting to Tensorflow is on my to-do list."},{"outputs":[],"metadata":{"_cell_guid":"4d2257b7-b397-4709-8483-34f32e2ba805","collapsed":true,"_uuid":"08f2a9a1f9347b4a3694e00d066919eb8775cd87"},"cell_type":"code","execution_count":null,"source":"import numpy as np\nimport pandas as pd\n\n%env KERAS_BACKEND=theano\n\nfrom keras.models import Sequential\nfrom keras.layers import Dropout\nfrom keras.layers.normalization import BatchNormalization\nfrom keras import regularizers\nfrom keras.layers import Dense\nfrom keras.optimizers import Adam\nfrom keras.utils import custom_object_scope\nfrom keras import callbacks\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn import preprocessing\n\nimport theano\n\n# train and test data path\nDATA_TRAIN_PATH = '../input/train.csv'\nDATA_TEST_PATH = '../input/test.csv'\n\nfeaturesToDrop = [\n    'ps_calc_10',\n    'ps_calc_01',\n    'ps_calc_02',\n    'ps_calc_03',\n    'ps_calc_13',\n    'ps_calc_08',\n    'ps_calc_07',\n    'ps_calc_12',\n    'ps_calc_04',\n    'ps_calc_17_bin',\n    'ps_car_10_cat',\n    'ps_car_11_cat',\n    'ps_calc_14',\n    'ps_calc_11',\n    'ps_calc_06',\n    'ps_calc_16_bin',\n    'ps_calc_19_bin',\n    'ps_calc_20_bin',\n    'ps_calc_15_bin',\n    'ps_ind_11_bin',\n    'ps_ind_10_bin'\n]\n\n"},{"metadata":{"_cell_guid":"77f3ad9c-8fbf-454c-acce-15079d124fcb","_uuid":"19902b39fbc3b64bb654e62de44ee7dc0c449672"},"cell_type":"markdown","source":"**The secret sauce**\n\nThis is where the magic happens - the soft_AUC function. This \n- Takes the predictions\n- Splits them into groups according to whether the true values are one/zero\n- Takes each pair of predictions from the one/zero groups, and subtracts the zeroes from the ones.\n- Takes the mean of the sigmoid of the result\n\nIf AUC is perfect, an (actual) one in the CV data will always have a higher pred than a zero in the CV data. Each time the prediction is wrong, and a one has a lower pred than a zero, the output loss is increased. Hence this is an suitable loss function to substitute for genuine AUC in that it decreases as AUC decreases and vice versa.\n\nLike AUC, we only care about relative ordering of predictions between the classes. We don't care about the absolute values of the predictions. This means that your final output values from your NN will also only care about ordering, and hence you use them to blend you will need to use ranking or similar to blend.\n\nIt's important to note that you need a large enough batch size, as if you have no examples of one class you'll get no data, and ideally you want several positive cases in each batch. I used a batch size of 4096, which with a 3% approx positive class rate, gives around 100 positives per batch - enough to give a useful result but not so many as to make calculations take forever. I did try calculating on the whole training batch, but convergence was not as good.\n\nSome useful references:\n * http://www.ipipan.waw.pl/~sj/pdf/PKDD07web.pdf\n * https://github.com/Lasagne/Lasagne/issues/767 - my code based heavily on this code.\n * http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.2.3727&rep=rep1&type=pdf"},{"outputs":[],"metadata":{"_cell_guid":"97449bba-3635-4c35-822b-3c60e5bbec8a","collapsed":true,"_uuid":"541421e60545bf5cda3c61cabc3b61a531b26a98"},"cell_type":"code","execution_count":null,"source":"\n# An analogue to AUC which takes the differences between each pair of true/false predictions\n# and takes the average sigmoid of the differences to get a differentiable loss function.\n# Based on code and ideas from https://github.com/Lasagne/Lasagne/issues/767\ndef soft_AUC_theano(y_true, y_pred):\n    # Extract 1s\n    pos_pred_vr = y_pred[y_true.nonzero()]\n    # Extract zeroes\n    neg_pred_vr = y_pred[theano.tensor.eq(y_true, 0).nonzero()]\n    # Broadcast the subtraction to give a matrix of differences  between pairs of observations.\n    pred_diffs_vr = pos_pred_vr.dimshuffle(0, 'x') - neg_pred_vr.dimshuffle('x', 0)\n    # Get signmoid of each pair.\n    stats = theano.tensor.nnet.sigmoid(pred_diffs_vr * 2)\n    # Take average and reverse sign\n    return 1-theano.tensor.mean(stats) # as we want to minimise, and get this to zero\n\n"},{"metadata":{"_cell_guid":"cea17e84-57b3-4f12-b214-934e8339cd24","_uuid":"f78c0afa84626757da63146ddd3c9e6efa356955"},"cell_type":"markdown","source":"**Callback**\n\nNow, we define a callback to print out our SKLearn AUC, and add this to the logs so we can use it for early stopping. See https://keras.io/callbacks/\n\nIn my own version I also use this to save down best scores in csv files where I store metadata for each network I run. This makes it much easier to look at trends of hyperparameter performance with overnight runs."},{"outputs":[],"metadata":{"_cell_guid":"55522cf5-abf3-43c1-b394-b7aaa87aaa05","collapsed":true,"_uuid":"fb27cbdeb24fcc9f875a77a2b04a278f797b7be9"},"cell_type":"code","execution_count":null,"source":"\n# This callback records the SKLearn calculated AUC each round, for use by early stopping\n# It also has slots where you can save down metadata or the model at useful points -\n# for Kaggle kernel purposes I've commented these out\nclass AUC_SKlearn_callback(callbacks.Callback):\n    def __init__(self, X_train, y_train, useCv = True):\n        super(AUC_SKlearn_callback, self).__init__()\n        self.bestAucCv = 0\n        self.bestAucTrain = 0\n        self.cvLosses = []\n        self.bestCvLoss = 1,\n        self.X_train = X_train\n        self.y_train = y_train\n        self.useCv = useCv\n\n    def on_train_begin(self, logs={}):\n        return\n\n    def on_train_end(self, logs={}):\n        return\n\n    def on_epoch_begin(self, epoch, logs={}):\n        return\n\n    def on_epoch_end(self, epoch, logs={}):\n        train_pred = self.model.predict(np.array(self.X_train))\n        aucTrain = roc_auc_score(self.y_train, train_pred)\n        print(\"SKLearn Train AUC score: \" + str(aucTrain))\n\n        if (self.bestAucTrain < aucTrain):\n            self.bestAucTrain = aucTrain\n            print (\"Best SKlearn AUC training score so far\")\n            #**TODO: Add your own logging/saving/record keeping code here\n\n        if (self.useCv) :\n            cv_pred = self.model.predict(self.validation_data[0])\n            aucCv = roc_auc_score(self.validation_data[1], cv_pred)\n            print (\"SKLearn CV AUC score: \" +  str(aucCv))\n\n            if (self.bestAucCv < aucCv) :\n                # Great! New best *actual* CV AUC found (as opposed to the proxy AUC surface we are descending)\n                print(\"Best SKLearn genuine AUC so far so saving model\")\n                self.bestAucCv = aucCv\n\n                # **TODO: Add your own logging/model saving/record keeping code here.\n                self.model.save(\"best_auc_model.h5\", overwrite=True)\n\n            vl = logs.get('val_loss')\n            if (self.bestCvLoss < vl) :\n                print(\"Best val loss on SoftAUC so far\")\n                #**TODO -  Add your own logging/saving/record keeping code here.\n        return\n\n    def on_batch_begin(self, batch, logs={}):\n        return\n\n    def on_batch_end(self, batch, logs={}):\n        # logs include loss, and optionally acc( if accuracy monitoring is enabled).\n        return\n"},{"metadata":{"_cell_guid":"389cb28a-bd78-4d41-a65d-3da14634c978","_uuid":"ff96fbea52d3b482f456c264b99f7c760ac6179c"},"cell_type":"markdown","source":"**Model creation and training functions**\n\nStandard model creation and training code, note that we reference the custom loss function and the callback here."},{"outputs":[],"metadata":{"_cell_guid":"33900d41-17fc-4b28-b579-fd1234abcb12","collapsed":true,"_uuid":"754b0a54e8c71d95b93342b2832692d29dd960a2"},"cell_type":"code","execution_count":null,"source":"# Create the model.\ndef create_model_AUC(input_dim, first_layer_size, second_layer_size, third_layer_size, lr, l2reg, dropout):\n    return create_model(input_dim, first_layer_size, second_layer_size, third_layer_size, lr, l2reg, dropout, \"AUC\")\n\ndef create_model_bce(input_dim, first_layer_size, second_layer_size, third_layer_size, lr, l2reg, dropout):\n    return create_model(input_dim, first_layer_size, second_layer_size, third_layer_size, lr, l2reg, dropout, \"crossentropy\")\n\n\ndef create_model(input_dim, first_layer_size, second_layer_size, third_layer_size, lr, l2reg, dropout, mode=\"AUC\") :\n    print(\"Creating model with input dim \", input_dim)\n    # likely to need tuning!\n    reg = regularizers.l2(l2reg)\n\n    model = Sequential()\n\n    model.add(Dense(units=first_layer_size, kernel_initializer='lecun_normal', kernel_regularizer=reg, activation='relu', input_dim=input_dim))\n    model.add(BatchNormalization())\n    model.add(Dropout(dropout))\n\n    model.add(Dense(units=second_layer_size, kernel_initializer='lecun_normal', activation='relu', kernel_regularizer=reg))\n    model.add(BatchNormalization(axis=1))\n    model.add(Dropout(dropout))\n\n    model.add(Dense(units=third_layer_size, kernel_initializer='lecun_normal', activation='relu', kernel_regularizer=reg))\n    model.add(BatchNormalization())\n    model.add(Dropout(dropout))\n\n    model.add(Dense(1, kernel_initializer='lecun_normal', activation='sigmoid'))\n\n    # classifier.compile(loss='mean_absolute_error', optimizer='rmsprop', metrics=['mae', 'accuracy'])\n    opt = Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n    if (mode == \"AUC\"):\n        model.compile(loss=soft_AUC_theano, metrics=[soft_AUC_theano], optimizer=opt)  # not sure whether to use metrics here?\n    else:\n        model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=opt)  # not sure whether to use metrics here?\n    return model\n\n\ndef train_model( X_train, y_train, model, valSplit=0.15, epochs = 5, batch_size = 4096):\n\n    callbacksList = [AUC_SKlearn_callback(X_train, y_train, useCv = (valSplit > 0))]\n    if (valSplit > 0) :\n        early_stopping = callbacks.EarlyStopping(monitor='val_loss', min_delta=0.00001, patience=5,\n                                                       verbose=0, mode='min')\n        callbacksList.append( early_stopping )\n    return model.fit(x=np.array(X_train), y=np.array(y_train),\n                        callbacks=callbacksList, validation_split=valSplit,\n                        verbose=2, batch_size=batch_size, epochs=epochs)\n\n"},{"metadata":{"_cell_guid":"a7853a08-9376-46bd-ace5-fd0e51d8209f","_uuid":"6bb0d11018ea9ee0298af4224eb4b005d09c2183"},"cell_type":"markdown","source":"**Data preparation**\n\nWe remove some of the noisier features, there was an excellent Kernel which I will try to find and cite that the list of columns to remove was taken from. This makes a big difference to effectiveness."},{"outputs":[],"metadata":{"_cell_guid":"8cbbceb4-f70e-4f1f-ac10-41bc004e7703","collapsed":true,"_uuid":"1584194e1cd69f9cc105ffc568f187a810dc4fee"},"cell_type":"code","execution_count":null,"source":"\n\ndef scale_features(df_for_range, df_to_scale, columnsToScale) :\n    # Scale columnsToScale in df_to_scale\n    columnsOut = list(map( (lambda x: x + \"_scaled\"), columnsToScale))\n    for c, co in zip(columnsToScale, columnsOut) :\n        scaler = preprocessing.MinMaxScaler(feature_range=(-1,1))\n        print(\"scaling \", c ,\" to \",co)\n        vals = df_for_range[c].values.reshape(-1, 1)\n        scaler.fit(vals )\n        df_to_scale[co]=scaler.transform(df_to_scale[c].values.reshape(-1,1))\n\n    df_to_scale.drop (columnsToScale, axis=1, inplace = True)\n\n    return df_to_scale\n\n\ndef one_hot (df, cols):\n    # One hot cols requested, drop original cols, return df\n    df = pd.concat([df, pd.get_dummies(df[cols], columns=cols)], axis=1)\n    df.drop(cols, axis=1, inplace = True)\n    return df\n\ndef get_data() :\n    X_train = pd.read_csv(DATA_TRAIN_PATH, index_col = \"id\")\n    X_test = pd.read_csv(DATA_TEST_PATH, index_col = \"id\")\n\n    y_train = pd.DataFrame(index = X_train.index)\n    y_train['target'] = X_train.loc[:,'target']\n    X_train.drop ('target', axis=1, inplace = True)\n    X_train.drop (featuresToDrop, axis=1, inplace = True)\n    X_test.drop (featuresToDrop,axis=1, inplace = True)\n\n    # car_11 is really a cat col\n    X_train.rename(columns={'ps_car_11': 'ps_car_11a_cat'}, inplace=True)\n    X_test.rename(columns={'ps_car_11': 'ps_car_11a_cat'}, inplace=True)\n\n    cat_cols = [elem for elem in list(X_train.columns) if \"cat\" in elem]\n    bin_cols = [elem for elem in list(X_train.columns) if \"bin\" in elem]\n    other_cols = [elem for elem in list(X_train.columns) if elem not in bin_cols and elem not in cat_cols]\n\n    # Scale numeric features in region of -1,1 using training set as the scaling range\n    X_test = scale_features(X_train, X_test, columnsToScale=other_cols)\n    X_train = scale_features(X_train, X_train, columnsToScale=other_cols)\n\n    X_train = one_hot(X_train, cat_cols)\n    X_test = one_hot(X_test, cat_cols)\n\n\n    return X_train, X_test, y_train\n"},{"metadata":{"_cell_guid":"bb57a2df-e1ce-4a7b-897a-3d80d7be16b2","_uuid":"b2bc8931a2ab197b71be5bffa698820811f6c0a0"},"cell_type":"markdown","source":"**Put it all together**\n\nWe set this up to run 2 comparable networks over a few epochs, to demonstrate that convergence is superior. I only run for 5 epochs to give a flavour of the comparison and avoid timing out the kernel.\n\nI found each epoch took around 3-5 minutes on my GT 1070 based windows system, and decent results were obtained after about 30 epochs, so around 2 hours training time. I've not given away the best hyperparameters I've found; I'll leave that as an exercise for the reader.\n\nThe submission generation code is untested, and you would want to run the network for longer and tune the hyperparameters a bit anyway before using this.\n"},{"outputs":[],"metadata":{"_cell_guid":"1f4d94c2-75c9-468b-9388-edc11f0a2585","collapsed":true,"_uuid":"6961420279b611e5b12098afd8c31b17130d61f6"},"cell_type":"code","execution_count":null,"source":"\n\ndef makeOutputFile(pred_fun, test, subsFile) :\n    df_out = pd.DataFrame(index=test.index)\n    y_pred = pred_fun( test )\n    df_out['target'] = y_pred\n    df_out.to_csv(subsFile, index_label=\"id\")\n\ndef main() :\n    X_train, X_test, y_train = get_data()\n    model = create_model( input_dim=X_train.shape[1],\n                          first_layer_size=300,\n                          second_layer_size=200,\n                          third_layer_size=200,\n                          lr=0.0001,\n                          l2reg = 0.1,\n                          dropout = 0.2,\n                          mode=\"AUC\")\n\n    train_model(X_train, y_train, model)\n\n    with custom_object_scope({'soft_AUC_theano': soft_AUC_theano}):\n        pred_fun = lambda x: model.predict(np.array(x))\n        makeOutputFile(pred_fun, X_test, \"auc.csv\")\n\n    model = create_model_bce( input_dim=X_train.shape[1],\n                          first_layer_size=300,\n                          second_layer_size=200,\n                          third_layer_size=200,\n                          lr=0.0001,\n                          l2reg = 0.1,\n                          dropout = 0.2)\n\n    train_model(X_train, y_train, model)\n\n    pred_fun = lambda x: model.predict(np.array(x))\n    makeOutputFile(pred_fun, X_test, \"no_auc.csv\")\n\nmain()"},{"metadata":{"_cell_guid":"0dde9387-ba46-4b20-b8a0-2620935d05d3","_uuid":"406aa1a4d99ef0fd6b75ab3903910b63e1c85931"},"cell_type":"markdown","source":"**Conclusions**\n\nWe can see that the AUC (and hence Gini) rises much quicker using the custom loss function. Please upvote if you like this kernal or find it useful."}],"nbformat_minor":1,"nbformat":4}