{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import os\nimport math\nimport datetime\nimport re\nfrom string import punctuation\n\nfrom kaggle.competitions import nflrush\nimport tqdm\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nimport lightgbm as lgb\nimport random\n\nfrom sklearn.preprocessing import StandardScaler\n\nsns.set_style('whitegrid')\nsns.set_context('talk')\n\npd.set_option(\"display.max_columns\", 100)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"env = nflrush.make_env()\ntrain = pd.read_csv('../input/nfl-big-data-bowl-2020/train.csv', dtype={'WindSpeed': 'object'})\n\n# Original dataframe:\n#train = pd.read_csv('train.csv', dtype={'WindSpeed': 'object'})\nlen(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## First split into player-specific data and play-specific data"},{"metadata":{"trusted":true},"cell_type":"code","source":"def split_play_and_player_cols(df,predicting=False):\n    df['IsRusher'] = df['NflId'] == df['NflIdRusher']\n    \n    play_ids = df[\"PlayId\"].unique()\n    #play_ids_filter = np.random.choice(play_ids,int(len(play_ids)*0.01),replace=False)\n    #df = df.loc[df.PlayId.isin(play_ids_filter)]\n    \n    df['PlayId'] = df['PlayId'].astype(str)\n    \n    # We must assume here that the first 22 rows correspond to the same player:\n    player_cols = [\n        'PlayId', # This is the link between them\n        'Season',\n        'Team',\n        'X',\n        'Y',\n        'S',\n        'A',\n        'Dis',\n        'Dir',\n        'NflId',\n        'IsRusher',\n    ]\n\n    df_players = df[player_cols]\n    \n    play_cols = [\n        'PlayId',\n        'Season',\n        'PossessionTeam',\n        'HomeTeamAbbr',\n        'VisitorTeamAbbr',\n        'PlayDirection', \n        'FieldPosition',\n        'YardLine',\n    ]\n    if not predicting:\n        play_cols.append('Yards')\n        \n    df_play = df[play_cols].copy()\n\n    ## Fillna in FieldPosition attribute\n    #df['FieldPosition'] = df.groupby(['PlayId'], sort=False)['FieldPosition'].apply(lambda x: x.ffill().bfill())\n    \n    # Get first \n    df_play = df_play.groupby('PlayId').first().reset_index()\n\n    #print('rows/plays in df: ', len(df_play))\n    assert df_play.PlayId.nunique() == df.PlayId.nunique(), \"Play/player split failed?\"  # Boom\n    \n    return df_play, df_players","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"play_ids = train[\"PlayId\"].unique()\n\ndf_play, df_players = split_play_and_player_cols(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Team Abbr"},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_team_abbr(df):\n\n    #These are only problems:\n    map_abbr = {'ARI': 'ARZ', 'BAL': 'BLT', 'CLE': 'CLV', 'HOU': 'HST'}\n    for abb in df['PossessionTeam'].unique():\n        map_abbr[abb] = abb\n\n    df['PossessionTeam'] = df['PossessionTeam'].map(map_abbr)\n    df['HomeTeamAbbr'] = df['HomeTeamAbbr'].map(map_abbr)\n    df['VisitorTeamAbbr'] = df['VisitorTeamAbbr'].map(map_abbr)\n\n    df['HomePossession'] = df['PossessionTeam'] == df['HomeTeamAbbr']\n    \n    return\n\nprocess_team_abbr(df_play)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### PlayDirection"},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_play_direction(df):\n    df['IsPlayLeftToRight'] = df['PlayDirection'].apply(lambda val: True if val.strip() == 'right' else False)\n    return\nprocess_play_direction(df_play)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Yards Til End Zone"},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_yard_til_end_zone(df):\n    def convert_to_yardline100(row):\n        return (100 - row['YardLine']) if (row['PossessionTeam'] == row['FieldPosition']) else row['YardLine']\n    df['Yardline100'] = df.apply(convert_to_yardline100, axis=1)\n    return\n\nprocess_yard_til_end_zone(df_play)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create Tracking Data Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_players = df_players.merge(\n    df_play[['PlayId', 'PossessionTeam', 'HomeTeamAbbr', 'PlayDirection', 'Yardline100']], \n    how='left', on='PlayId')\n\ndf_players.loc[df_players.Season == 2017, 'S'] = 10*df_players.loc[df_players.Season == 2017,'Dis']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def standarize_direction(df):\n    # adjusted the data to always be from left to right\n    df['HomePossesion'] = df['PossessionTeam'] == df['HomeTeamAbbr']\n\n    df['Dir_rad'] = np.mod(90 - df.Dir, 360) * math.pi/180.0\n\n    df['ToLeft'] = df.PlayDirection == \"left\"\n    df['TeamOnOffense'] = \"home\"\n    df.loc[df.PossessionTeam != df.HomeTeamAbbr, 'TeamOnOffense'] = \"away\"\n    df['IsOnOffense'] = df.Team == df.TeamOnOffense # Is player on offense?\n    df['X_std'] = df.X\n    df.loc[df.ToLeft, 'X_std'] = 120 - df.loc[df.ToLeft, 'X']\n    df['Y_std'] = df.Y\n    df.loc[df.ToLeft, 'Y_std'] = 160/3 - df.loc[df.ToLeft, 'Y']\n    df['Dir_std'] = df.Dir_rad\n    df.loc[df.ToLeft, 'Dir_std'] = np.mod(np.pi + df.loc[df.ToLeft, 'Dir_rad'], 2*np.pi)\n   \n    #Replace Null in Dir_rad\n    df.loc[(df.IsOnOffense) & df['Dir_std'].isna(),'Dir_std'] = 0.0\n    df.loc[~(df.IsOnOffense) & df['Dir_std'].isna(),'Dir_std'] = np.pi\n\nstandarize_direction(df_players)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def data_augmentation(df, sample_ids):\n    df_sample = df.loc[df.PlayId.isin(sample_ids)].copy()\n    df_sample['Y_std'] = 160/3  - df_sample['Y_std']\n    df_sample['Dir_std'] = df_sample['Dir_std'].apply(lambda x: 2*np.pi - x)\n    df_sample['PlayId'] = df_sample['PlayId'].apply(lambda x: x+'_aug')\n    return df_sample\n\ndef process_tracking_data(df):\n    # More feature engineering for all:\n    df['Sx'] = df['S']*df['Dir_std'].apply(math.cos)\n    df['Sy'] = df['S']*df['Dir_std'].apply(math.sin)\n    \n    # ball carrier position\n    rushers = df[df['IsRusher']].copy()\n    rushers.set_index('PlayId', inplace=True, drop=True)\n    playId_rusher_map = rushers[['X_std', 'Y_std', 'Sx', 'Sy']].to_dict(orient='index')\n    rusher_x = df['PlayId'].apply(lambda val: playId_rusher_map[val]['X_std'])\n    rusher_y = df['PlayId'].apply(lambda val: playId_rusher_map[val]['Y_std'])\n    rusher_Sx = df['PlayId'].apply(lambda val: playId_rusher_map[val]['Sx'])\n    rusher_Sy = df['PlayId'].apply(lambda val: playId_rusher_map[val]['Sy'])\n    \n    # Calculate differences between the rusher and the players:\n    df['player_minus_rusher_x'] = rusher_x - df['X_std']\n    df['player_minus_rusher_y'] = rusher_y - df['Y_std']\n\n    # Velocity parallel to direction of rusher:\n    df['player_minus_rusher_Sx'] = rusher_Sx - df['Sx']\n    df['player_minus_rusher_Sy'] = rusher_Sy - df['Sy']\n\n    return\n\nsample_ids = np.random.choice(df_play.PlayId.unique(), int(0.5*len(df_play.PlayId.unique())))\n#sample_ids = df_play.PlayId.unique()\n\ndf_players_aug = data_augmentation(df_players, sample_ids)\ndf_players = pd.concat([df_players, df_players_aug])\ndf_players.reset_index()\n\ndf_play_aug = df_play.loc[df_play.PlayId.isin(sample_ids)].copy()\ndf_play_aug['PlayId'] = df_play_aug['PlayId'].apply(lambda x: x+'_aug')\ndf_play = pd.concat([df_play, df_play_aug])\ndf_play.reset_index()\n\n# This is necessary to maintain the order when in the next cell we use groupby\ndf_players.sort_values(by=['PlayId'],inplace=True)\ndf_play.sort_values(by=['PlayId'],inplace=True)\n\nprocess_tracking_data(df_players)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tracking_level_features = [\n    'PlayId',\n    'IsOnOffense',\n    'X_std',\n    'Y_std',\n    'Sx',\n    'Sy',\n    'player_minus_rusher_x',\n    'player_minus_rusher_y',\n    'player_minus_rusher_Sx',\n    'player_minus_rusher_Sy',\n    'IsRusher'\n]\n\ndf_all_feats = df_players[tracking_level_features]\n\nprint('Any null values: ', df_all_feats.isnull().sum().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ngrouped = df_all_feats.groupby('PlayId')\ntrain_x = np.zeros([len(grouped.size()),11,10,10])\ni = 0\nplay_ids = df_play.PlayId.values\nfor name, group in grouped:\n    if name!=play_ids[i]:\n        print(\"Error\")\n\n    [[rusher_x, rusher_y, rusher_Sx, rusher_Sy]] = group.loc[group.IsRusher==1,['X_std', 'Y_std','Sx','Sy']].values\n\n    offense_ids = group[group.IsOnOffense & ~group.IsRusher].index\n    defense_ids = group[~group.IsOnOffense].index\n\n    for j, defense_id in enumerate(defense_ids):\n        [def_x, def_y, def_Sx, def_Sy] = group.loc[defense_id,['X_std', 'Y_std','Sx','Sy']].values\n        [def_rusher_x, def_rusher_y] = group.loc[defense_id,['player_minus_rusher_x', 'player_minus_rusher_y']].values\n        [def_rusher_Sx, def_rusher_Sy] =  group.loc[defense_id,['player_minus_rusher_Sx', 'player_minus_rusher_Sy']].values\n        \n        train_x[i,j,:,:4] = group.loc[offense_ids,['Sx','Sy','X_std', 'Y_std']].values - np.array([def_Sx, def_Sy, def_x,def_y])\n        train_x[i,j,:,-6:] = [def_rusher_Sx, def_rusher_Sy, def_rusher_x, def_rusher_y, def_Sx, def_Sy]\n    \n    i+=1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Transform train_y to one hot encoded vector\n\nThen we train it with logloss function and directly predict pdf (then transform to cdf)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transform Y into indexed-classes:\ntrain_y = df_play[['PlayId', 'Yards']].copy()\n\ntrain_y['YardIndex'] = train_y['Yards'].apply(lambda val: val + 99)\n\nmin_idx_y = 71\nmax_idx_y = 150\n\ntrain_y['YardIndexClipped'] = train_y['YardIndex'].apply(\n    lambda val: min_idx_y if val < min_idx_y else max_idx_y if val > max_idx_y else val)\n\nprint('max yardIndex: ', train_y.YardIndex.max())\nprint('max yardIndexClipped: ', train_y.YardIndexClipped.max())\nprint('min yardIndex: ', train_y.YardIndex.min())\nprint('min yardIndexClipped: ', train_y.YardIndexClipped.min())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_season = df_play[['PlayId', 'Season']].copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train ConvNet\n\n\nBelow class Metric based entirely on: https://www.kaggle.com/kingychiu/keras-nn-starter-crps-early-stopping\n\nBelow early stopping entirely based on: https://www.kaggle.com/c/nfl-big-data-bowl-2020/discussion/112868#latest-656533\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import Model\n\nfrom tensorflow.keras.layers import (\n    Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, AvgPool1D, AvgPool2D, Reshape,\n    Input, Activation, BatchNormalization, Dense, Add, Lambda, Dropout, LayerNormalization)\n\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.callbacks import Callback, EarlyStopping\n\nimport tensorflow as tf \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\n\ndef crps(y_true, y_pred):\n    loss = K.mean(K.sum((K.cumsum(y_pred, axis = 1) - K.cumsum(y_true, axis=1))**2, axis=1))/199\n    return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"min_idx_y = 71\nmax_idx_y = 150\nnum_classes_y = max_idx_y - min_idx_y + 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_conv_net(num_classes_y):\n    #_, x, y, z = train_x.shape\n    inputdense_players = Input(shape=(11,10,10), name = \"playersfeatures_input\")\n    \n    X = Conv2D(128, kernel_size=(1,1), strides=(1,1), activation='relu')(inputdense_players)\n    X = Conv2D(160, kernel_size=(1,1), strides=(1,1), activation='relu')(X)\n    X = Conv2D(128, kernel_size=(1,1), strides=(1,1), activation='relu')(X)\n    \n    # The second block of convolutions learns the necessary information per defense player before the aggregation.\n    # For this reason the pool_size should be (1, 10). If you want to learn per off player the pool_size must be \n    # (11, 1)\n    Xmax = MaxPooling2D(pool_size=(1,10))(X)\n    Xmax = Lambda(lambda x1 : x1*0.3)(Xmax)\n\n    Xavg = AvgPool2D(pool_size=(1,10))(X)\n    Xavg = Lambda(lambda x1 : x1*0.7)(Xavg)\n\n    X = Add()([Xmax, Xavg])\n    X = Lambda(lambda y : K.squeeze(y,2))(X)\n    X = BatchNormalization()(X)\n    \n    X = Conv1D(160, kernel_size=1, strides=1, activation='relu')(X)\n    X = BatchNormalization()(X)\n    X = Conv1D(96, kernel_size=1, strides=1, activation='relu')(X)\n    X = BatchNormalization()(X)\n    X = Conv1D(96, kernel_size=1, strides=1, activation='relu')(X)\n    X = BatchNormalization()(X)\n    \n    Xmax = MaxPooling1D(pool_size=11)(X)\n    Xmax = Lambda(lambda x1 : x1*0.3)(Xmax)\n\n    Xavg = AvgPool1D(pool_size=11)(X)\n    Xavg = Lambda(lambda x1 : x1*0.7)(Xavg)\n\n    X = Add()([Xmax, Xavg])\n    X = Lambda(lambda y : K.squeeze(y,1))(X)\n    \n    X = Dense(96, activation=\"relu\")(X)\n    X = BatchNormalization()(X)\n\n    X = Dense(256, activation=\"relu\")(X)\n    X = LayerNormalization()(X)\n    X = Dropout(0.3)(X)\n\n    outsoft = Dense(num_classes_y, activation='softmax', name = \"output\")(X)\n\n    model = Model(inputs = [inputdense_players], outputs = outsoft)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Metric(Callback):\n    def __init__(self, model, callbacks, data):\n        super().__init__()\n        self.model = model\n        self.callbacks = callbacks\n        self.data = data\n\n    def on_train_begin(self, logs=None):\n        for callback in self.callbacks:\n            callback.on_train_begin(logs)\n\n    def on_train_end(self, logs=None):\n        for callback in self.callbacks:\n            callback.on_train_end(logs)\n\n    def on_epoch_end(self, batch, logs=None):\n        X_valid, y_valid = self.data[0], self.data[1]\n\n        y_pred = self.model.predict(X_valid)\n        y_true = np.clip(np.cumsum(y_valid, axis=1), 0, 1)\n        y_pred = np.clip(np.cumsum(y_pred, axis=1), 0, 1)\n        val_s = ((y_true - y_pred) ** 2).sum(axis=1).sum(axis=0) / (199 * X_valid.shape[0])\n        logs['val_CRPS'] = val_s\n        \n        for callback in self.callbacks:\n            callback.on_epoch_end(batch, logs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nmodels = []\nkf = KFold(n_splits=6, shuffle=True)\nscore = []\n\nfor i, (tdx, vdx) in enumerate(kf.split(train_x, train_y)):\n    print(f'Fold : {i}')\n    X_train, X_val = train_x[tdx], train_x[vdx],\n    y_train, y_val = train_y.iloc[tdx]['YardIndexClipped'].values, train_y.iloc[vdx]['YardIndexClipped'].values\n    season_val = df_season.iloc[vdx]['Season'].values\n\n    y_train_values = np.zeros((len(y_train), num_classes_y), np.int32)\n    for irow, row in enumerate(y_train):\n        y_train_values[(irow, row - min_idx_y)] = 1\n        \n    y_val_values = np.zeros((len(y_val), num_classes_y), np.int32)\n    for irow, row in enumerate(y_val - min_idx_y):\n        y_val_values[(irow, row)] = 1\n\n    val_idx = np.where(season_val!=2017)\n    \n    X_val = X_val[val_idx]\n    y_val_values = y_val_values[val_idx]\n\n    y_train_values = y_train_values.astype('float32')\n    y_val_values = y_val_values.astype('float32')\n    \n    model = get_conv_net(num_classes_y)\n\n    es = EarlyStopping(monitor='val_CRPS',\n                        mode='min',\n                        restore_best_weights=True,\n                        verbose=0,\n                        patience=10)\n    \n    es.set_model(model)\n    metric = Metric(model, [es], [X_val, y_val_values])\n\n    lr_i = 1e-3\n    lr_f = 5e-4\n    n_epochs = 30 \n\n    decay = (1-lr_f/lr_i)/((lr_f/lr_i)* n_epochs - 1)  #Time-based decay formula\n    alpha = (lr_i*(1+decay))\n    \n    opt = Adam(learning_rate=1e-3)\n    model.compile(loss=crps,\n                  optimizer=opt)\n    \n    model.fit(X_train,\n              y_train_values, \n              epochs=n_epochs,\n              batch_size=64,\n              verbose=0,\n              callbacks=[metric],\n              validation_data=(X_val, y_val_values))\n\n    val_crps_score = min(model.history.history['val_CRPS'])\n    print(\"Val loss: {}\".format(val_crps_score))\n    \n    score.append(val_crps_score)\n\n    models.append(model)\n    \nprint(np.mean(score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.mean(score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_cdf_prediction_model(predict_x, n_classes=None, model=None, min_idx=None, max_idx=None, yardline100=None):\n    '''\n    predict_x - array-like of shape [nsamples, n_features]\n    min_idx - minimum index considered in training for target var\n    max_idx - maximum index considered in training for target var\n    '''\n    #now = time()\n    prediction = model.predict(predict_x)\n    \n    # Convert data to array of pdfs indexed by training example\n    predict_pdfs = np.zeros((len(predict_x), n_classes))\n\n    predict_pdfs[:, min_idx:max_idx+1] = prediction\n    \n    # can't predict probability of gaining more yards than end zone,\n    # so instead: drop and re-normalize?\n    max_target_cls_idx = yardline100 + 99\n    for idx, predict_row in enumerate(predict_pdfs):\n        max_idx = max_target_cls_idx[idx]\n        #predict_pdfs[idx, max_idx] = np.sum(predict_row[max_idx:])\n        predict_pdfs[idx, max_idx+1:] = 0.0\n        # Now renormalize:\n        predict_pdfs[idx, :] = predict_pdfs[idx, :]/predict_pdfs[idx, :].sum()\n    \n    # convert to cdfs:\n    predict_cdfs = np.cumsum(predict_pdfs, axis=1)\n    return predict_cdfs, predict_pdfs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submission:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def split_play_and_player_cols(df,predicting=False):\n    df['IsRusher'] = df['NflId'] == df['NflIdRusher']\n    \n    play_ids = df[\"PlayId\"].unique()\n    \n    # We must assume here that the first 22 rows correspond to the same player:\n    player_cols = [\n        'PlayId', # This is the link between them\n        'Season',\n        'Team',\n        'X',\n        'Y',\n        'S',\n        'A',\n        'Dis',\n        'Dir',\n        'NflId',\n        'IsRusher',\n    ]\n\n    df_players = df[player_cols]\n    \n    play_cols = [\n        'PlayId',\n        'Season',\n        'PossessionTeam',\n        'HomeTeamAbbr',\n        'VisitorTeamAbbr',\n        'PlayDirection', \n        'FieldPosition',\n        'YardLine',\n    ]\n    if not predicting:\n        play_cols.append('Yards')\n        \n    df_play = df[play_cols].copy()\n\n    ## Fillna in FieldPosition attribute\n    #df['FieldPosition'] = df.groupby(['PlayId'], sort=False)['FieldPosition'].apply(lambda x: x.ffill().bfill())\n    \n    # Get first \n    df_play = df_play.groupby('PlayId').first().reset_index()\n\n    #print('rows/plays in df: ', len(df_play))\n    assert df_play.PlayId.nunique() == df.PlayId.nunique(), \"Play/player split failed?\"  # Boom\n    \n    return df_play, df_players","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_features_one_play(df_test):\n    df_play, df_players_split = split_play_and_player_cols(df_test, predicting=True)\n    \n    process_team_abbr(df_play)\n    process_play_direction(df_play)\n    process_yard_til_end_zone(df_play)\n    \n    df_players = df_players_split.merge(\n        df_play[['PlayId','PossessionTeam','HomeTeamAbbr','PlayDirection', 'Yardline100']], \n        how='left', on='PlayId')\n    \n    standarize_direction(df_players)\n    process_tracking_data(df_players)\n    df_all_feats = df_players[tracking_level_features]\n\n  \n    train_x = np.zeros([1,11,10,10])\n    [[rusher_x, rusher_y, rusher_Sx, rusher_Sy]] = df_all_feats.loc[df_all_feats.IsRusher==1,['X_std', 'Y_std','Sx','Sy']].values\n\n    offense_ids = df_all_feats[df_all_feats.IsOnOffense & ~df_all_feats.IsRusher].index\n    defense_ids = df_all_feats[~df_all_feats.IsOnOffense].index\n\n    for j, defense_id in enumerate(defense_ids):\n        [def_x, def_y, def_Sx, def_Sy] = df_all_feats.loc[defense_id,['X_std', 'Y_std','Sx','Sy']].values\n        [def_rusher_x, def_rusher_y] = df_all_feats.loc[defense_id,['player_minus_rusher_x', 'player_minus_rusher_y']].values\n        [def_rusher_Sx, def_rusher_Sy] =  df_all_feats.loc[defense_id,['player_minus_rusher_Sx', 'player_minus_rusher_Sy']].values\n\n        train_x[0,j,:,:4] = df_all_feats.loc[offense_ids,['Sx','Sy','X_std', 'Y_std']].values - np.array([def_Sx, def_Sy, def_x,def_y])\n        train_x[0,j,:,-6:] = [def_rusher_Sx, def_rusher_Sy, def_rusher_x, def_rusher_y, def_Sx, def_Sy]\n\n\n    yardline100 = df_play['Yardline100'].values\n    return [train_x, yardline100]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"iter_test = env.iter_test()\nfor iplay, (test_df, sample_prediction_df) in enumerate(iter_test):\n    if iplay % 10 == 0:\n        print('Processing test play {}...'.format(iplay))\n    [inference_row, yardline100] = extract_features_one_play(test_df)\n    \n    cdfs_arr = []\n    for model in models:\n        cdfs_, pdfs_ = get_cdf_prediction_model(\n            inference_row, \n            n_classes=199, \n            model=model, \n            min_idx=min_idx_y, \n            max_idx=max_idx_y,\n            yardline100=yardline100)\n        cdfs_arr.append(cdfs_)\n    cdfs = np.mean(cdfs_arr, axis=0)\n\n    # To avoid rounding error:\n    cdf_val = cdfs[0].clip(0.0,1.0)\n    \n    for icol, col in enumerate(sample_prediction_df.columns):\n        sample_prediction_df.loc[:, col] = cdf_val[icol]\n    env.predict(sample_prediction_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env.write_submission_file()\nprint([filename for filename in os.listdir('/kaggle/working') if '.csv' in filename])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"nbformat":4,"nbformat_minor":4}