{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom kaggle.competitions import nflrush\nfrom string import punctuation\nfrom tqdm import tqdm\nimport gc, re\nimport pickle\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold\nimport lightgbm as lgb\nfrom catboost import Pool, CatBoostRegressor\nfrom sklearn.preprocessing import StandardScaler\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"%%time\ntrain_df = pd.read_csv('/kaggle/input/nfl-big-data-bowl-2020/train.csv', low_memory=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (f'Shape of training dataset: {train_df.shape}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.Turf.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So each PlayId has data of all 22 players and there are 23171 plays given"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Function to reduce the memory usage\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print(f'Mem. usage decreased to {end_mem} Mb ({100 * (start_mem - end_mem) / start_mem}% reduction)')\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_player_specific_cols(col_names):\n    cols, total_players = [], 22\n    for col in col_names:\n        for player in range(total_players):\n            cols.append(f'{col}_player{player}')\n    return cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def mean_without_overflow_fast(col):\n    col /= len(col)\n    return col.mean() * len(col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode_cyclic_feature(df, col, max_vals):\n    df[col + '_sin'] = np.sin(2 * np.pi * df[col]/max_vals)\n    df[col + '_cos'] = np.cos(2 * np.pi * df[col]/max_vals)\n    del df[col]\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_timestamp(df, timestamp_col):\n    df[f'{timestamp_col}Hour'] = np.uint8(df[timestamp_col].dt.hour)\n    df[f'{timestamp_col}Minute'] = np.uint8(df[timestamp_col].dt.minute)\n    df[f'{timestamp_col}Second'] = np.uint8(df[timestamp_col].dt.second)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_player_specific_cols(col_names):\n    cols, total_players = [], 22\n    for col in col_names:\n        for player in range(total_players):\n            cols.append(f'{col}_player{player}')\n    return cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def height_to_inches(player_height):\n    return int(player_height.split('-')[0]) * 12 + int(player_height.split('-')[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def bdate_to_age(bdate):\n    now = pd.to_datetime('now')\n    return (now.year - bdate.dt.year) - ((now.month - bdate.dt.month) < 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_grouping_dict(df, key):\n    dicts = []\n    for _, row in df.iterrows():\n        dicts.append(dict([(pos.split()[1], pos.split()[0]) for (pos) in row[key].split(',')]))\n    return dicts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def groupby_playid(df, is_training=True):\n    \n    total_players = 22\n    non_player_features = ['GameId', 'PlayId', 'Season', 'YardLine', 'Quarter', 'GameClock',\n       'PossessionTeam', 'Down', 'Distance', 'FieldPosition',\n       'HomeScoreBeforePlay', 'VisitorScoreBeforePlay',\n       'OffenseFormation', 'OffensePersonnel', 'DefendersInTheBox',\n       'DefensePersonnel', 'PlayDirection', 'TimeHandoff', 'TimeSnap',\n       'Yards', 'HomeTeamAbbr', 'VisitorTeamAbbr', 'Week', 'Stadium',\n       'Location', 'StadiumType', 'Turf', 'GameWeather', 'Temperature',\n       'Humidity', 'WindSpeed', 'WindDirection', 'NflId']\n    \n    if not is_training:\n        non_player_features.remove('Yards')\n    \n    player_features = ['Team', 'X', 'Y', 'S', 'A', 'Dis', 'Orientation', 'Dir',\n       'DisplayName', 'JerseyNumber', 'PlayerHeight', 'PlayerWeight',\n       'PlayerBirthDate', 'PlayerCollegeName', 'Position', 'NflIdRusher']\n    \n    playids_groups = df.groupby('PlayId').size().keys()\n    \n    player_features_columns = []\n    for feature in player_features:\n        for player in range(total_players):\n            player_features_columns.append(f'{feature}_player{player}')\n    \n    # first assign non_player features which are common for a single game playid\n    final_df = pd.DataFrame()\n    final_df[non_player_features] = df.groupby('PlayId')[non_player_features].first().reset_index(drop=True)\n    final_df = final_df.reindex(final_df.columns.tolist() + player_features_columns, axis=1)\n    temp_cols = []\n    if is_training:\n        for group in tqdm(playids_groups, position=0, leave=True):\n            temp_cols.append(df[df['PlayId'] == group][player_features].melt()['value'])\n    else:\n        for group in playids_groups:\n            temp_cols.append(df[df['PlayId'] == group][player_features].melt()['value'])\n    final_df[player_features_columns] = pd.DataFrame(temp_cols).values\n    \n    return final_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def feature_engineering(df, is_training=True, label_encoders={}):\n    \n    if is_training:\n        label_encoders['NflId'] = LabelEncoder()\n        label_encoders['NflId'].fit(df['NflId'])\n    try:\n        df['NflId'] = label_encoders['NflId'].transform(df['NflId'])\n    except:\n        df['NflId'] = np.nan\n       \n    team_dict = {\n        'away': 0,\n        'home': 1\n    }\n    df['Team'] = df['Team'].map(team_dict)\n    season_dict = {\n        2017: 0,\n        2018: 1\n    }\n    df['Season'] = df['Season'].map(season_dict)\n    df = groupby_playid(df, is_training)\n    \n    df = df.drop(['Season', 'Temperature', 'Humidity'], axis = 1)\n    \n    if is_training:\n        df = df.apply(lambda group: group.interpolate(limit_direction='both'))\n    \n    df['WindDirection'] = df['WindDirection'].fillna(method='backfill')\n    df['WindSpeed'] = df['WindSpeed'].fillna(method='backfill')\n    df['GameWeather'] = df['GameWeather'].fillna(method='backfill')\n    df['StadiumType'] = df['StadiumType'].fillna(method='backfill')\n    df['FieldPosition'] = df['FieldPosition'].fillna(method='backfill')\n    df['OffenseFormation'] = df['OffenseFormation'].fillna(method='backfill')\n    \n    df['GameClock'] = pd.to_datetime(df['GameClock'])\n    df['TimeHandoff'] = pd.to_datetime(df['TimeHandoff'])\n    df['TimeSnap'] = pd.to_datetime(df['TimeSnap'])\n    \n    df = extract_timestamp(df, 'GameClock')\n    df = extract_timestamp(df, 'TimeHandoff')\n    df = extract_timestamp(df, 'TimeSnap')\n    df = df.drop(['GameClock', 'TimeHandoff', 'TimeSnap'], axis=1)\n    \n    df = encode_cyclic_feature(df, 'GameClockHour', 24)\n    df = encode_cyclic_feature(df, 'GameClockMinute', 60)\n    df = encode_cyclic_feature(df, 'GameClockSecond', 60)\n    \n    df = encode_cyclic_feature(df, 'TimeHandoffHour', 24)\n    df = encode_cyclic_feature(df, 'TimeHandoffMinute', 60)\n    df = encode_cyclic_feature(df, 'TimeHandoffSecond', 60)\n    \n    df = encode_cyclic_feature(df, 'TimeSnapHour', 24)\n    df = encode_cyclic_feature(df, 'TimeSnapMinute', 60)\n    df = encode_cyclic_feature(df, 'TimeSnapSecond', 60)\n    \n    def transform_game_weather(x):\n        x = str(x).lower()\n        if 'indoor' in x:\n            return  'indoor'\n        elif 'cloud' in x or 'coudy' in x or 'clouidy' in x:\n            return 'cloudy'\n        elif 'rain' in x or 'shower' in x:\n            return 'rain'\n        elif 'sunny' in x:\n            return 'sunny'\n        elif 'clear' in x:\n            return 'clear'\n        elif 'cold' in x or 'cool' in x:\n            return 'cool'\n        elif 'snow' in x:\n            return 'snow'\n        return x\n    \n    df['GameWeather'] = df['GameWeather'].apply(lambda row: transform_game_weather(row))\n    \n    categorical_features = ['PossessionTeam', 'FieldPosition', 'OffenseFormation', 'PlayDirection', 'HomeTeamAbbr', \n                        'VisitorTeamAbbr', 'NflId','Stadium', 'Location', 'GameWeather'] + get_player_specific_cols(['Position', 'PlayerCollegeName', 'NflIdRusher'])\n    \n    for col in get_player_specific_cols(['PlayerHeight']):\n        df[col] = df[col].apply(lambda x: height_to_inches(x))\n    \n    for col in get_player_specific_cols(['PlayerBirthDate']):\n        df[col] = pd.to_datetime(df[col])\n        df[col] = bdate_to_age(df[col])\n    \n    for cat in categorical_features:\n        if is_training:\n            label_encoders[cat] = LabelEncoder()\n            label_encoders[cat].fit(df[cat])\n        try:\n            df[cat] = label_encoders[cat].transform(df[cat])\n        except Exception as e:\n            df[cat] = np.nan # Put NaN in case when any unseen label is found in testing dataset.\n            \n        \n#     offense_groups = ['QB', 'RB', 'OL', 'FB', 'WR', 'TE']\n#     defense_groups = ['DL', 'LB', 'CB', 'S']\n    \n#     offense_dicts = get_grouping_dict(df, 'OffensePersonnel')\n#     defense_dicts = get_grouping_dict(df, 'DefensePersonnel')\n    \n#     offense_grps_df = pd.DataFrame(offense_dicts).rename(columns={'OL': 'OL_offense', 'DL': 'DL_offense', 'LB': 'LB_offense', 'DB': 'DB_offense'}).fillna(0).astype(int)\n#     defense_grps_df = pd.DataFrame(defense_dicts).rename(columns={'OL': 'OL_defense', 'DL': 'DL_defense', 'LB': 'LB_defense', 'DB': 'DB_defense'}).fillna(0).astype(int)\n    \n#     df = pd.concat([df, offense_grps_df, defense_grps_df], axis=1)\n    df = df.drop(['OffensePersonnel', 'DefensePersonnel'], axis=1)\n    \n    try:\n        df['NflIdRusher'] = label_encoders['NflId'].transform(df['NflIdRusher'])\n    except:\n        df['NflIdRusher'] = np.nan\n        \n    wind_directions = ['N', 'E', 'S', 'W', 'NE', 'SE', 'SW', 'NW', 'NNE', 'ENE', 'ESE', 'SSE', 'SSW', 'WSW', 'WNW', 'NNW']  # https://www.quora.com/What-is-the-definition-of-SSW-wind-direction\n    \n    df.loc[df['WindSpeed'].isin(wind_directions), 'WindSpeed'] = np.nan\n    df.loc[~df['WindDirection'].isin(wind_directions), 'WindDirection'] = np.nan\n    \n    df['WindDirection'] = df['WindDirection'].fillna(method='backfill')\n    df['WindSpeed'] = df['WindSpeed'].fillna(method='backfill')\n    \n    if is_training:\n        label_encoders['WindDirection'] = LabelEncoder()\n        label_encoders['WindDirection'].fit(df['WindDirection'])\n    try:\n        df['WindDirection'] = label_encoders['WindDirection'].transform(df['WindDirection'])\n    except Exception as e:\n        df['WindDirection'] = np.nan\n    \n    def transform_windspeed(speed):\n        speed = str(speed)\n        if 'MPH' in speed or 'mph' in speed or 'MPh' in speed:\n            speed = speed.replace('MPH', '').strip()\n            speed = speed.replace('MPH', '').strip()\n            speed = speed.replace('MPh', '').strip()\n        if '-' in speed:\n            return (float(speed.split('-')[0]) + float(speed.split('-')[1]))/2\n        try:\n            return float(speed)\n        except:\n            return 10 # https://sciencing.com/average-daily-wind-speed-24011.html\n        \n    df['WindSpeed'] = df['WindSpeed'].apply(lambda speed: transform_windspeed(speed))\n    \n    beaufort = [(0, 0, 0.3), (1, 0.3, 1.6), (2, 1.6, 3.4), (3, 3.4, 5.5), (4, 5.5, 8), \n                (5, 8, 10.8), (6, 10.8, 13.9), (7, 13.9, 17.2), (8, 17.2, 20.8), \n                (9, 20.8, 24.5), (10, 24.5, 28.5), (11, 28.5, 33), (12, 33, 200)]\n\n    for item in beaufort:\n        df.loc[(df['WindSpeed']>=item[1]) & (df['WindSpeed']<item[2]), 'beaufort_scale'] = item[0]\n    \n    df['DefendersInTheBox_vs_Distance'] = df['DefendersInTheBox'] / df['Distance']\n    df['Field_eq_Possession'] = df['FieldPosition'] == df['PossessionTeam']\n    \n    # Add BMI as a feature: formula for BMI: kg/m^2\n    total_players = 22\n    \n    def get_bmi(height, weight):\n        return weight / (height ** 2) * 755\n    \n    def is_rusher(x, y):\n        return x == y\n    \n    for player in range(total_players):\n        df[f'BMI_player{player}'] = np.vectorize(get_bmi)(df[f'PlayerHeight_player{player}'], df[f'PlayerWeight_player{player}'])\n        df[f'is_rusher_player{player}'] = np.vectorize(is_rusher)(df['NflId'], df[f'NflIdRusher_player{player}'])\n\n    # Cleaning the Turf to Natural and artificial\n    # from https://www.kaggle.com/c/nfl-big-data-bowl-2020/discussion/112681#latest-649087\n    Turf = {'Field Turf':'Artificial', 'A-Turf Titan':'Artificial', 'Grass':'Natural', 'UBU Sports Speed S5-M':'Artificial', \n            'Artificial':'Artificial', 'DD GrassMaster':'Artificial', 'Natural Grass':'Natural', \n            'UBU Speed Series-S5-M':'Artificial', 'FieldTurf':'Artificial', 'FieldTurf 360':'Artificial', 'Natural grass':'Natural', 'grass':'Natural', \n            'Natural':'Natural', 'Artifical':'Artificial', 'FieldTurf360':'Artificial', 'Naturall Grass':'Natural', 'Field turf':'Artificial', \n            'SISGrass':'Artificial', 'Twenty-Four/Seven Turf':'Artificial', 'natural grass':'Natural'} \n\n    df['Turf'] = df['Turf'].map(Turf)\n    df['Turf'] = df['Turf'] == 'Natural'\n    \n    def clean_StadiumType(txt):\n        if pd.isna(txt):\n            return np.nan\n        txt = txt.lower()\n        txt = ''.join([c for c in txt if c not in punctuation])\n        txt = re.sub(' +', ' ', txt)\n        txt = txt.strip()\n        txt = txt.replace('outside', 'outdoor')\n        txt = txt.replace('outdor', 'outdoor')\n        txt = txt.replace('outddors', 'outdoor')\n        txt = txt.replace('outdoors', 'outdoor')\n        txt = txt.replace('oudoor', 'outdoor')\n        txt = txt.replace('indoors', 'indoor')\n        txt = txt.replace('ourdoor', 'outdoor')\n        txt = txt.replace('retractable', 'rtr.')\n        return txt\n        \n    df['StadiumType'] = df['StadiumType'].apply(clean_StadiumType)\n    \n    def transform_StadiumType(txt):\n        if pd.isna(txt):\n            return np.nan\n        if 'outdoor' in txt or 'open' in txt:\n            return 1\n        if 'indoor' in txt or 'closed' in txt:\n            return 0\n\n        return np.nan\n    \n    df['StadiumType'] = df['StadiumType'].apply(transform_StadiumType)\n    \n    # from https://www.kaggle.com/c/nfl-big-data-bowl-2020/discussion/112173#latest-647309\n#     df['JerseyNumberGrouped'] = df['JerseyNumber'] // 10\n    \n    if is_training:\n        return df, label_encoders\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntrain_df, label_encoders = feature_engineering(train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = reduce_mem_usage(train_df)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"non_feature_cols = ['GameId', 'PlayId'] + get_player_specific_cols(['DisplayName', 'JerseyNumber'])\ntarget_col = ['Yards']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_train = train_df[target_col]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train_df.drop(non_feature_cols+target_col, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\nscaler.fit(Y_train.values.reshape(-1, 1))\nY_train = scaler.transform(Y_train.values.reshape(-1, 1)).flatten()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now our data is training ready. Let's train a model!"},{"metadata":{"trusted":true},"cell_type":"code","source":"# seed = 666\n# n_folds = 10\n# models, y_valid_pred = [], np.zeros(len(X_train))\n# lgb_params={\n#     'learning_rate': 0.01,\n#     'objective': 'regression',\n#     'n_estimators': 1000,\n#     'num_leaves': 20,\n#     'metric': 'rmse',\n#     'bagging_fraction': 0.7,\n#     'feature_fraction': 0.7\n# }\n\n# kf = KFold(n_splits = n_folds, shuffle=False, random_state=seed)\n\n# for train_idx, val_idx in kf.split(X_train, Y_train):\n#     x_train, y_train = X_train.iloc[train_idx, :], Y_train[train_idx]\n#     x_val, y_val = X_train.iloc[val_idx, :], Y_train[val_idx]\n    \n#     training_data = lgb.Dataset(x_train, label=y_train)\n#     val_data = lgb.Dataset(x_val, label=y_val)\n    \n#     regressor = lgb.LGBMRegressor(**lgb_params)\n#     regressor.fit(x_train, y_train, eval_set=[(x_val, y_val)], early_stopping_rounds=100, verbose=100)\n    \n#     y_valid_pred[val_idx] += regressor.predict(x_val, num_iteration=regressor.best_iteration_)\n#     models.append(regressor)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed = 666\nn_folds = 12\nmodels, y_valid_pred = [], np.zeros(len(X_train))\n\nkf = KFold(n_splits = n_folds, shuffle=False, random_state=seed)\n\nfor train_idx, val_idx in kf.split(X_train, Y_train):\n    x_train, y_train = X_train.iloc[train_idx, :], Y_train[train_idx]\n    x_val, y_val = X_train.iloc[val_idx, :], Y_train[val_idx]\n\n    model = CatBoostRegressor(loss_function=\"RMSE\",\n                               eval_metric=\"RMSE\",\n                               task_type=\"CPU\",\n                               learning_rate=0.02,\n                               iterations=2000,\n                               l2_leaf_reg=5,\n                               random_seed=42,\n                               od_type=\"Iter\",\n                               depth=6,\n                               early_stopping_rounds=150,\n                               border_count=32\n                              )\n\n    train_data = Pool(x_train, y_train)\n    valid_data = Pool(x_val, y_val)\n\n    regressor = model.fit(train_data,\n                        eval_set=valid_data,\n                        use_best_model=True,\n                        verbose=100)\n    \n    y_valid_pred[val_idx] += regressor.predict(x_val)\n    models.append(regressor)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reference: https://www.kaggle.com/hukuda222/nfl-simple-model-using-lightgbm\ny_pred = np.zeros((len(X_train),199))\ny_ans = np.zeros((len(X_train),199))\n\nfor i,p in enumerate(np.round(scaler.inverse_transform(y_valid_pred))):\n    p+=99\n    for j in range(199):\n        if j>=p+10:\n            y_pred[i][j]=1.0\n        elif j>=p-10:\n            y_pred[i][j]=(j+10-p)*0.05\n\nfor i,p in enumerate(scaler.inverse_transform(Y_train)):\n    p+=99\n    for j in range(199):\n        if j>=p:\n            y_ans[i][j]=1.0\n\nprint(\"validation score:\",np.sum(np.power(y_pred-y_ans,2))/(199*((len(X_train)))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('models.pickle', 'wb') as handle:\n    pickle.dump(models, handle, protocol=pickle.HIGHEST_PROTOCOL)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('label_encoders.pickle', 'wb') as handle:\n    pickle.dump(label_encoders, handle, protocol=pickle.HIGHEST_PROTOCOL)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import FileLink, FileLinks\nFileLink('models.pickle')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"FileLink('label_encoders.pickle')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('models.pickle', 'rb') as handle:\n    models = pickle.load(handle)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('label_encoders.pickle', 'rb') as handle:\n    label_encoders = pickle.load(handle)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env = nflrush.make_env()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"non_feature_cols = ['GameId', 'PlayId'] + get_player_specific_cols(['DisplayName', 'JerseyNumber'])\nfor (test_df, sample_prediction_df) in tqdm(env.iter_test(), position=0, leave=True):\n    test_df = feature_engineering(test_df, False, label_encoders)\n    test_df = test_df.drop(non_feature_cols, axis=1)\n    y_pred = np.zeros(199)        \n    y_pred_p = np.sum(np.round(scaler.inverse_transform([model.predict(test_df)[0] for model in models])))/n_folds\n    y_pred_p += 99\n    for j in range(199):\n        if j>=y_pred_p+10:\n            y_pred[j]=1.0\n        elif j>=y_pred_p-10:\n            y_pred[j]=(j+10-y_pred_p)*0.05\n    env.predict(pd.DataFrame(data=[y_pred],columns=sample_prediction_df.columns))\nenv.write_submission_file()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}