{"cells":[{"metadata":{},"cell_type":"markdown","source":"# NFL Big Data Bowl\n\nIn the competition we are asked to predict how many yards a NFL player will gain after receiving a handoff.\n\nThere is no real EDA for this kernel. I wanted to make a kernel that demonstrates this modeling process from the classification perspective. If you are interested in good EDA kernels and pairing it with this kernel there is a great kernel by Rob Mulla here https://www.kaggle.com/robikscube/big-data-bowl-comprehensive-eda-with-pandas and also another good kernel by SRK here https://www.kaggle.com/sudalairajkumar/simple-exploration-notebook-nfl."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport gc\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom tqdm import tqdm\nfrom multiprocessing import Pool\nfrom kaggle.competitions import nflrush\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\npd.set_option('display.max_rows', 500)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Features get distance relative to the rushing player for each player on the field starting with home -> away\n\ntrain_df = pd.read_csv('/kaggle/input/nfl-big-data-bowl-2020/train.csv', low_memory=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Functions to generate features\n\nLike a lot of other kernels already I will do some simple feature generation. In the data for each `PlayId` there is 22 rows. Each row represents a single player. Overall, there are 11 players per team on the field during a play. I pivot the data so there is one row for each `PlayId`. I only generated one new features `Force` which is the players weight in ibs. and the acceleration in $\\dfrac{yards}{second ^ 2}$. Not the \"correct\" definition of force but I'm using that here.\n\nI also just convert categorical variables to an integer label. From what I have seen so far it will be important to have a nice function or class that can handle all of the feature building because you can only get access to the testing data in slices."},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_category_labels(x):\n    unique_categories = x.unique().tolist() + ['unknown']  # For this competition allow for this in test data\n    label = range(len(unique_categories))\n    mapping = dict(list(zip(unique_categories, label)))\n    \n    return x.map(mapping)\n\ndef build_features(df, college_names_map=None, display_names_map=None, position_names_map=None):\n    \n    # Add new features\n    df['Force'] = df['PlayerWeight'] * df['A']  # F = ma (mass is players weight in ibs and acceleration is measure in yards/sec ** 2)\n    \n    # Have relative position so do not need the cumcount\n    df = df.sort_values(['GameId', 'PlayId']).reset_index(drop=True)\n    df['row_number'] = df.groupby(['GameId', 'PlayId']).cumcount() + 1\n    \n    # Split player height\n    height = df['PlayerHeight'].str.split('-', n = 1, expand = True)\n    df['PlayerHeightFeet'] = height[0].astype(int)\n    df['PlayerHeightInches'] = height[1].astype(int)\n\n    # Split player birthdate\n    birth_date = df['PlayerBirthDate'].str.split('/', n = 2, expand = True)\n    df['PlayerDOBMonth'] = birth_date[0].astype(int)\n    df['PlayerDOBDay'] = birth_date[1].astype(int)\n    df['PlayerDOBYear'] = birth_date[2].astype(int)\n\n    if college_names_map is None:\n        college_names = df['PlayerCollegeName'].unique().tolist() + ['unknown']\n        college_names_map = dict(list(zip(college_names, range(len(college_names)))))\n    df['PlayerCollegeName'] = df['PlayerCollegeName'].map(college_names_map)\n\n    if display_names_map is None:\n        display_names = df['DisplayName'].unique().tolist() + ['unknown']\n        display_names_map = dict(list(zip(display_names, range(len(display_names)))))\n    df['DisplayName'] = df['DisplayName'].map(display_names_map)\n\n    if position_names_map is None:\n        position_names = df['Position'].unique().tolist() + ['unknown']\n        position_names_map = dict(list(zip(position_names, range(len(position_names)))))\n    df['Position'] = df['Position'].map(position_names_map)\n    \n    # Unstack individual player features\n    values = ['X', 'Y', 'S', 'A', 'Dis', 'Orientation', 'Dir', 'NflId', 'DisplayName', 'JerseyNumber', 'PlayerHeightFeet', 'PlayerHeightInches',\n              'PlayerWeight', 'PlayerDOBMonth', 'PlayerDOBDay', 'PlayerDOBYear', 'PlayerCollegeName', 'Position', 'Force']\n\n    group_df = df.pivot_table(index=['GameId', 'PlayId'], columns=['row_number'], values=values).reset_index()\n\n    columns = []\n    for c in group_df.columns:\n        col_name = str(c[0]) + str(c[1])\n        columns.append(col_name)\n\n    group_df.columns = columns\n    \n    # Get play id specific features\n    if 'Yards' in df.columns:\n        play_features = ['Team', 'Season', 'YardLine', 'Quarter', 'GameClock', 'Down', 'Distance', 'PossessionTeam', 'FieldPosition',\n                         'HomeScoreBeforePlay', 'VisitorScoreBeforePlay', 'NflIdRusher', 'OffenseFormation', 'OffensePersonnel',\n                         'DefendersInTheBox', 'DefensePersonnel', 'PlayDirection', 'TimeHandoff', 'TimeSnap', 'HomeTeamAbbr',\n                         'VisitorTeamAbbr', 'Week', 'Stadium', 'Location', 'StadiumType', 'Turf', 'GameWeather', 'Temperature',\n                         'Humidity', 'WindSpeed', 'WindDirection', 'Yards']\n    else:\n        play_features = ['Team', 'Season', 'YardLine', 'Quarter', 'GameClock', 'Down', 'Distance', 'PossessionTeam', 'FieldPosition',\n                         'HomeScoreBeforePlay', 'VisitorScoreBeforePlay', 'NflIdRusher', 'OffenseFormation', 'OffensePersonnel',\n                         'DefendersInTheBox', 'DefensePersonnel', 'PlayDirection', 'TimeHandoff', 'TimeSnap', 'HomeTeamAbbr',\n                         'VisitorTeamAbbr', 'Week', 'Stadium', 'Location', 'StadiumType', 'Turf', 'GameWeather', 'Temperature',\n                         'Humidity', 'WindSpeed', 'WindDirection']\n\n    play_id_feature_df = df.groupby(['PlayId'])[play_features].first()\n    simple_df = pd.merge(group_df, play_id_feature_df, on=['PlayId'])\n    \n    # Convert categorical variables\n    categorical_variables = ['Team', 'PossessionTeam', 'FieldPosition', 'OffenseFormation', 'OffensePersonnel',\n                             'DefensePersonnel', 'PlayDirection', 'HomeTeamAbbr', 'VisitorTeamAbbr', 'Stadium', 'Location',\n                             'StadiumType', 'Turf', 'GameWeather', 'WindDirection']\n    \n    for column in categorical_variables:\n        simple_df[column] = generate_category_labels(simple_df[column])\n        \n    return simple_df, college_names_map, display_names_map, position_names_map","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build features and label maps for training\n# Label maps well be applied to test data\nsimple_df, college_names_map, display_names_map, position_names_map = build_features(train_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building Classes\n\nBy examining the data we know two things:\n\n1. There is not a yardage gain for all of the -99 to 99 yards that the metric needs to be computed\n2. There will be some class imbalance\n\nThis means that we will need to be creative in the way that we model this problem as a multiclass classification problem. Below is a plot of the `Yards` which is what we wnat to predict. In the NFL positive yardage between 1 and 10 is very common and even negative yardage between -5 and -1 is also fairly common. For the purposes of this kernel I will build classes for -3 to 11 yards and a class for all yards less than -3 and a class for all yards greater than 11. \n\nThis is somewhat aribitrary and based off my knowledge of the game. You can experiment with this to see what works.\n\nBelow the plot is a function to create the classes and targets. It is by no means optimized and can certainly use some work but it works all the way through to the end of the kernel."},{"metadata":{"trusted":true},"cell_type":"code","source":"simple_df['Yards'].value_counts().plot(kind='bar', figsize=(20, 10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Building the targets\n# Want to do as a multiclass classification problem\n\ntarget = simple_df['Yards']\nactuals = simple_df['Yards']\n\nstart = -3\nend = 11\n\ndef create_target_bins(x, start=-3, end=11):\n    if x in set(range(start, end + 1)):\n        return f'Yards{x}'\n    elif x < start:\n        return f'Yards_less_than_{start}'\n    elif x > end:\n        return f'Yards_greater_than_{end}'\n\ndef create_class_names(target, start=-3, end=11):\n\n    bin0 = [f'Yards_less_than_{start}']\n    bins = [f'Yards{i}' for i in range(start, end + 1)]\n    bin_last = [f'Yards_greater_than_{end}']\n\n    classes = bin0 + bins + bin_last\n    target_classes = target.apply(create_target_bins)\n    return classes, target_classes\n\nclasses, target_classes = create_class_names(target, start=start, end=end)\n\nmap_ = dict(list(zip(classes, range(len(classes)))))\ninverse_map_ = dict(list(zip(range(len(classes)), classes)))\n\ntarget = target_classes.map(map_)\ntarget.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"objectives = pd.DataFrame(list(zip(target, actuals)), columns=['target', 'Yards'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Generate Train/Test set\n\nLike any other machine learning problem we want to be able to evaluate our model performance. Here I use a simple train/test split by calling one iteration of `KFold`. Theoretically, what happend in the past will should not have any impact on the next play in the NFL so currently I choose `KFold`. In the future I will be trying a time series validation strategy as well.\n\nAlso for now I am dropping a few columns that I just did not have the time to deal with properly yet."},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can do two types of validation KFold (because a previous play should not affect the current play or time series where we hold out each teams most recent game)\n# This is what we will be predicting for the LB.\n\ngroup_id = simple_df['PlayId']\ntrain_inds, test_inds = next(KFold(n_splits=10, random_state = 2019).split(simple_df))\n\nX_train = simple_df.iloc[train_inds]\nX_test = simple_df.iloc[test_inds]\n\ny_train = objectives.iloc[train_inds]['target']\ny_test = objectives.iloc[test_inds]['target']\n\ny_test_actual = objectives.iloc[test_inds]['Yards'].values\n\ntest_ids = X_test['PlayId'].values\nX_train.drop(['PlayId', 'GameId', 'Yards', 'GameClock', 'TimeSnap', 'TimeHandoff', 'WindSpeed'], axis=1, inplace=True)\nX_test.drop(['PlayId', 'GameId', 'Yards', 'GameClock', 'TimeSnap', 'TimeHandoff', 'WindSpeed'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For this competition I want to add focal loss as the loss metric with the imbalanced multiclass classification I think this well help\n# If we want to mimic counts then from the test counts should be computed for each fold as well as aggregate metrics\n\nparams = {\n          \"objective\" : \"multiclass\",\n          \"num_class\" : len(classes),\n          \"num_leaves\" : 2 ** 8,\n          \"max_depth\": 5,\n          \"learning_rate\" : 0.01,\n          \"bagging_fraction\" : 0.9,  # subsample\n          \"feature_fraction\" : 0.9,  # colsample_bytree\n          \"bagging_freq\" : 2,        # subsample_freq\n          \"verbosity\" : -1\n}\n\nlgb_train, lgb_valid = lgb.Dataset(X_train, y_train.values), lgb.Dataset(X_test, y_test.values)\nmodel = lgb.train(params, lgb_train, 1000, valid_sets=[lgb_train, lgb_valid], verbose_eval=5, early_stopping_rounds=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb.plot_importance(model, max_num_features=25)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building the predictions\n\nThis is the part that took me some time to think about. What the metric is asking for is a cumulative probability distribution $P(Y \\leq n)$. If we model this as a multiclass classification we have the probabilities we just have to take the cumulative sum of each row. The interesting thing here for me is what to do with the binned classes?\n\nMy idea was to take the probability for the binned classes and divide that probability by the number of steps between -99 and -3 for the class where yards $\\le -3$ and the number of steps between 11 and 99 for the class where yards $\\ge 11$.\n\nIn the function I also clip the probabilities because I got an error that the probabilities were not in $(0, 1]$ even though they were. I think it is just float precision computations."},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_cdf_predictions(X, model, classes, start=-3, end=11):\n    predictions = model.predict(X, num_iteration=model.best_iteration)\n    predictions = pd.DataFrame(predictions)\n    \n    cdf_predictions = np.zeros((predictions.shape[0], 199))\n    actual_classes = [f'Yards{i}' for i in range(-99, 100)]\n    cdf_predictions = pd.DataFrame(cdf_predictions, columns=actual_classes)\n    predictions.columns = classes\n\n    begin_columns = [f'Yards{i}' for i in range(-99, start)]\n    cdf_predictions.loc[:, begin_columns] = np.tile(predictions.iloc[:, 0].values, (len(begin_columns), 1)).T / len(begin_columns)\n\n    columns = [f'Yards{i}' for i in range(start, end + 1)]\n    cdf_predictions.loc[:, columns] = predictions.loc[:, columns].values\n\n    end_columns = [f'Yards{i}' for i in range(end + 1, 100)]\n    cdf_predictions.loc[:, end_columns] = np.tile(predictions.iloc[:, -1].values, (len(end_columns), 1)).T / len(end_columns)\n    \n    predictions = pd.DataFrame(cdf_predictions).cumsum(axis=1)\n    return predictions.clip(0.0, 1.0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = build_cdf_predictions(X_test, model, classes, start, end)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Get the test score\n\nFinally, we want to get the test score from our model. Here it is `0.1704`."},{"metadata":{"trusted":true},"cell_type":"code","source":"def crps_score(y_true, y_pred):\n    \n    N = len(y_true)\n    n = np.asarray([i for i in range(-99, 100)])\n    n = np.tile(n, (len(y_true), 1))\n    y_true = np.tile(y_test.values, (n.shape[1], 1)).T\n    \n    heavyside_calculation = ((n - y_true) >= 0).astype(int)\n    score = ((y_pred - heavyside_calculation) ** 2).sum(axis=1)\n    return score.sum() / (199 * N)\n\ncrps_score(y_test_actual, predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Making predictions\n\nIn order to make predictions we have to call the special python module `nflrush` for this competition. There seem to be a lot of rules around this API. For example, you can only call it once in a notebook session. If you want to be able to call it again you have to restart your notebook session.\n\nAlso, you can only get the next iteration of the test data AFTER you have made a prediction on the current `PlayId`. We have to iterate though and build our features on each `PlayId` frame and then make the prediction. Once, we iterate through each test_df frame we can submit the predictions from `env.predict` and when it is completely finished we make an output submission with `env.write_submission`.\n\nAgain, this kernel is still in testing phase and the speed of the prediction process is not optimzed in any way."},{"metadata":{"trusted":true},"cell_type":"code","source":"env = nflrush.make_env()\niter_test = env.iter_test()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build test data frame and examine\ntest_df = []\nfor test_data, _ in tqdm(iter_test):\n    test_data, _, _, _ = build_features(test_data, college_names_map, display_names_map, position_names_map)\n    test_data.drop(['PlayId', 'GameId', 'GameClock', 'TimeSnap', 'TimeHandoff', 'WindSpeed'], axis=1, inplace=True)\n    predictions = build_cdf_predictions(test_data, model, classes, start, end)\n    print(predictions)\n    env.predict(predictions)\nenv.write_submission_file()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}