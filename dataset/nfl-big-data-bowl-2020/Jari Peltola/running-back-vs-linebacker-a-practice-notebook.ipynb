{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"%%html\n<style type=\"text/css\">\n\ndiv.h2 {\n\n    background-color: #159957;\n    background-image: linear-gradient(120deg, #155799, #159957);\n    text-align: left;\n    color: white;              \n    padding:9px;\n    padding-right: 100px; \n    font-size: 20px; \n    max-width: 1500px; \n    margin: auto; \n    margin-top: 40px; \n\n}\n\n                                                                         \nbody {\n\n  font-size: 12px;\n\n}    \n                                     \n\ndiv.h3 {\n\n    color: #159957; \n    font-size: 18px; \n    margin-top: 20px; \n    margin-bottom:4px;\n\n}\n                                      \n\ndiv.h4 {\n\n    color: #159957;\n    font-size: 15px; \n    margin-top: 20px; \n    margin-bottom: 8px;\n\n}\n\n   \nspan.note {\n\n    font-size: 5; \n    color: gray; \n    font-style: italic;\n\n}\n\n  \nhr {\n\n    display: block; \n    color: gray\n    height: 1px; \n    border: 0; \n    border-top: 1px solid;\n\n}\n                                 \n\nhr.light {\n\n    display: block;\n    color: lightgray\n    height: 1px; \n    border: 0; \n    border-top: 1px solid;\n\n}   \n\n   \n                                      \n                                      \n                        \n                                      \n                                      \n                                      \n                                      \n                                                          \n\ntable.dataframe th \n\n{\n\n    border: 1px darkgray solid;\n    color: black;\n    align: left;\n    background-color: white;\n\n}\n\n    \n\n                                      \n\ntable.dataframe td \n                                      \n{\n\n    border: 1px darkgray solid;\n    color: black;\n    background-color: white;\n    font-size: 12px;\n    text-align: center;\n\n} \n                                   \n\ntable.rules th \n\n{\n\n    border: 1px darkgray solid;\n    color: black;\n    background-color: white;\n    font-size: 11px;\n    align: left;\n\n}\n                                   \n\ntable.rules td \n\n{\n\n    border: 1px darkgray solid;\n    color: black;\n    background-color: white;\n    font-size: 13px;\n    text-align: center;\n\n} \n\n   \ntable.rules tr.best\n\n{\n\n    color: green;\n\n}    \n\n    \n.output { \n\n    align-items: left; \n\n}\n\n        \n.output_png {\n\n    display: table-cell;\n\n    text-align: left;\n\n    margin:auto;\n\n}                                          \n\n                                \n</style>","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Disclaimer"},{"metadata":{},"cell_type":"markdown","source":"***All seasoned data analysts should ignore this notebook since they will neither learn nor miss anything relevant by further reading. However if you are a football fan or only a few weeks in to data science just like me, this notebook can serve as a sort of peer publication.***\n\n*I am a published author of suspense and young adult novels as well as political scientist, not a coder or data analyst. I recently joined Kaggle after I noticed a competition by NFL. As an avid follower of the league, it was interesting to analyze possible reasons for player lower leg injuries.*\n\n*This practice notebook is based on dataset included in the NFL Big Data Bowl 2020, in which a model was created to predict ball carrier runs. I have studied data analysis for roughly a month and have therefore no skills for making a high-end custom ML model. This notebook includes no model-building and is itself part of my personal learning process more than anything else.* \n\n*Finally, since I am only a true data science rookie, this notebook is prone to mistakes, and its methods and results should not be applied as such in other contexts. It is also a fact that this is the second Jupyter Notebook I've done in my life.*\n<hr>"},{"metadata":{},"cell_type":"markdown","source":"<div class=\"h2\"><i>Running Back vs. Linebacker: A Practice Notebook</i></div>"},{"metadata":{},"cell_type":"markdown","source":"**TABLE OF CONTENTS**\n1. Introduction <br>\n2. Data Preprocessing <br>\n2.1 *Euclidean Distance* <br>\n2.2 *Acceleration*<br>\n2.3 *Speed* <br>\n3. Analysis <br>\n4. (How To Become) ML Model<br>\n5. Conclusion\n"},{"metadata":{},"cell_type":"markdown","source":"### 1. Introduction "},{"metadata":{},"cell_type":"markdown","source":"Football is a situational game divided in individual plays. An average game in 2019 NFL season consisted of 124 plays, each one with their specific play call i.e. plan to execute that particular play.\n\nRunning plays are the backbone of football. They are used to get the 'dirty yards' needed for first down i.e. four new attempts to gain ten yards on the field. In **NFL Big Data Bowl 2020** a challenge was set to create a model for predicting how many yards the ball carrier - most often running back - will gain in a play.\n\nThis notebook will take a different approcach - I have no skills for building a custom learning model. Using dataset included in the original contest, it was possible to decipher that the average number of yards per run play was 4.18 yards. This was the basis for this notebook and its research question.\n\nFootball is also a game of matchups. The offensive and defensive lines face each other dozens of times in a game, and the same goes for running backs and defensive backs. On defense linerbackers form the secondary line behind the big guys, and it is the job of agile linebackers to try preventing running back from gaining yards.\n\nIn this notebook, the matchups between running backs and linebackers are the focal point. In the end I decided to ask the following research question:\n\n<div class=\"h4\"><i>Are there factors linked either to running backs or linebackers that contribute to longer run plays than the average 4.18 yards?</i></div>\n<br>\n<hr>"},{"metadata":{},"cell_type":"markdown","source":"### 2. Data Preprocessing"},{"metadata":{},"cell_type":"markdown","source":"The dataset included in the **NFL Data Bowl 2020** includes lot of information irrelevant for the research question. Therefore a part of the dataset can be dropped right from the start."},{"metadata":{"trusted":true},"cell_type":"code","source":"# original css stylesheet: Kaggle, member: TexasTom\n\n#import modules\nimport pandas as pd\nimport numpy as np\n\n#load dataset\ndf = pd.read_csv(\"../input/nfl-big-data-bowl-2020/train.csv\", low_memory = False)\n\n# set maximum number of columns in diplay\npd.set_option('display.max_columns', 36)\n\n#drop possible NaN values\ndf.dropna(inplace = True)\n\n# switch Position value HB (half back i.e. running back) to RB\n# group different linebacker positions (ILB, MLB, OLB, LB) under same label LB \ndf[\"Position\"]= df[\"Position\"].replace(\"HB\", \"RB\")\ndf[\"Position\"]= df[\"Position\"].replace(\"ILB\", \"LB\")\ndf[\"Position\"]= df[\"Position\"].replace(\"MLB\", \"LB\")\ndf[\"Position\"]= df[\"Position\"].replace(\"OLB\", \"LB\")\n\n# select and drop original columns relevant to task at hand\ncols = ['Orientation', 'Dir', 'Dis', 'DisplayName', 'JerseyNumber', 'Season', 'Team', 'PossessionTeam', 'FieldPosition', 'HomeScoreBeforePlay', 'VisitorScoreBeforePlay', \n       'PlayDirection', 'OffenseFormation', 'PlayerBirthDate', 'PlayerCollegeName', 'TimeHandoff', 'HomeTeamAbbr', 'VisitorTeamAbbr', 'Week', 'Stadium', 'Location', 'StadiumType', 'Turf',\n       'GameWeather', 'Temperature', 'Humidity', 'WindSpeed', 'WindDirection']\ndf = df.drop(cols, axis=1)\n\n# select only rows with RB or LB as Position value\ndf = df[df['Position'].isin(['RB', 'LB']) ]\n\n# create a new column storing as string whether the player is RB or LB\ndf.loc[:,'RbLb'] = df['Position']\n\n# arrange dataframe by column value, in this case PlayId \n# method courtesy of StackOverFlow, member: yogitha jaya reddy gari\ndf = df.sort_values(['PlayId'],ascending=False).groupby('PlayId',as_index = False).apply(lambda x: x.reset_index(drop = True))\ndf.reset_index().drop(['level_0','level_1'],axis = 1)\n\ndf.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Glancing at the data, it seems that not all linebackers are always labeled as such in specific plays. One reason for this is that there are an increasing number of 'flex players' in the NFL, meaning they are able to line up in several positions. Some of them even take the field both in offensive and defensive plays. This factor may be considered as providing some 'data noise' in further steps of this analysis.\n\nNext some new columns will be created for new values based on the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"# get numeric values for Position column\ndf = pd.get_dummies(df, columns=['Position'])\n\n# create new columns based on existing data\ndf['Is3Wr'] = df['OffensePersonnel'].str.contains('3 WR')\ndf['Is3Wr'] = df['Is3Wr'].map({True: 1, False: 0})\n\ndf['Is3Lb'] = df['DefensePersonnel'].str.contains('3 LB')\ndf['Is3Lb'] = df['Is3Lb'].map({True: 1, False: 0})\n\ndf['Is4Lb'] = df['DefensePersonnel'].str.contains('4 LB')\ndf['Is4Lb'] = df['Is4Lb'].map({True: 1, False: 0})\n\ndf['Is4Lb'] = df['DefensePersonnel'].str.contains('4 LB')\ndf['Is4Lb'] = df['Is4Lb'].map({True: 1, False: 0})\n\ndf.loc[:,'X_lb'] = df['X']\ndf.loc[:,'Y_lb'] = df['Y']\ndf.loc[:,'X_rb'] = df['X']\ndf.loc[:,'Y_rb'] = df['Y']\n\n# create separate x/y coordinate values for running backs and linebackers\n# these values are taken from original dataset X and Y columns\ndf['X_lb'] = df['Position_LB'].apply(lambda x: None if x==1 else 0)\ndf['X_lb'] = df['X_lb'].fillna(df['X'])\n\ndf['Y_lb'] = df['Position_LB'].apply(lambda x: None if x==1 else 0)\ndf['Y_lb'] = df['Y_lb'].fillna(df['Y'])\n\ndf['X_rb'] = df['Position_LB'].apply(lambda x: None if x==0 else 0)\ndf['X_rb'] = df['X_rb'].fillna(df['X'])\n\ndf['Y_rb'] = df['Position_LB'].apply(lambda x: None if x==0 else 0)\ndf['Y_rb'] = df['Y_rb'].fillna(df['Y'])\n\n# replace 0 values with NaN\ndf.X_lb = df.X_lb.replace(0, np.nan)\ndf.Y_lb = df.Y_lb.replace(0, np.nan)\ndf.X_rb = df.X_rb.replace(0, np.nan)\ndf.Y_rb = df.Y_rb.replace(0, np.nan)\n\n# sort dataframe index and create multi index consisting of Play and Players\ndf.sort_index(inplace = True) \ndf.index.names = ['Play','Players']\n\n# df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The multi index column Play describes the number of individual plays in the dataset. The Players column refers to players on field in each individual play. For example, the first two lines have to players from the same Play etc. All in all the count of personnel in Players column ranges from 2 to more. \n\nIf one takes a look at the first play in the dataset, the DefensePersonnel column indicates that there were three linebackers included in the play. However only one linebacker is listed under that specific PlayId. This tells us that **not all linebackers on the field in individual plays have been included in the dataset, which as such is a major deficiency.**\n<hr>\nContinuing this notebook, next the NaN values in the coordinate columns will be replaced by the average values of player position on field. As such this would make no sense, but the average values are calculated by play, as described in Play index column. As all players by rule are in fixed positions when the ball is snapped, the average value is based on player positions in that particular play. \n\nThe purpose of all this is to create rows where for example a linebacker has his own position coordinates, and the running back, on the field in that same play, his own position coordinates. In running back rows the average position of linebackers in that play is applied, if there are more than one linebacker in the dataset in that particular play. \n\nTo accomplish this, some temporary columns are first created below."},{"metadata":{"trusted":true},"cell_type":"code","source":"# the average positional X coordinates by Play\na1 = df.groupby('Play')['X_lb'].mean()\nb1 = df.groupby('Play')['X_rb'].mean()\n\n# subttract RB average X coordinate values from LB average X values\nc1 = (b1 - a1)\n\n# make sure these values are absolute i.e. positive\nc1 = np.absolute(c1)\n# calculate average\nc1 = c1.mean()\n\n# create new temporary column xtr1\n# this value is the existing running back X position minus the calculated average \ndf['xtr1'] = df['X_rb'] - c1\n\n# on rows where there are no running back X coordinate values, use values stored in the new column\ndf['X_lb'] = df['X_lb'].fillna(df.xtr1)\n\n## df.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# repeat the process above on Y coordinates for running backs\na2 = df.groupby('Play')['Y_lb'].mean()\nb2 = df.groupby('Play')['Y_rb'].mean()\n\nc2 = (b2 - a2)\nc2 = np.absolute(c2)\nc2 = c2.mean()\n\ndf['xtr2'] = df['Y_rb'] - c2\ndf['Y_lb'] = df['Y_lb'].fillna(df.xtr2)\n\n# df.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next the remaining NaN values are filled by using one line of code based on Play index. This should in theory fill all remaining NaN cells in coordinates columns. This however does not happen, meaning the process above must be repeated on the two remaining coordinates columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"# fill NaN values based on specific multi index. Original code: StackOverFlow, user: piRSquared\ndf = df.groupby(level='Play').bfill()\n\n# repeat filling missing values with average values\na3 = df.groupby('Play')['X_rb'].mean()\nb3 = df.groupby('Play')['X_lb'].mean()\n\nc3 = (b3 - a3)\nc3 = np.absolute(c3)\nc3 = c3.mean()\n\ndf['xtr3'] = df['X_lb'] + c3\ndf['X_rb'] = df['X_rb'].fillna(df.xtr3)\n\n\n# repeat filling missing values with average values\na4 = df.groupby('Play')['Y_rb'].mean()\nb4 = df.groupby('Play')['Y_lb'].mean()\n\nc4 = (b4 - a4)\nc4 = np.absolute(c4)\nc4 = c4.mean()\n\ndf['xtr4'] = df['Y_lb'] + c4\ndf['Y_rb'] = df['Y_rb'].fillna(df.xtr4)\n\n# drop unnecessary coordinate columns\nxtr_cols = ['X', 'Y', 'xtr1', 'xtr2', 'xtr3', 'xtr4']\ndf = df.drop(xtr_cols, axis=1)\n\n# df.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<hr>"},{"metadata":{},"cell_type":"markdown","source":"#### 2.1 Euclidean Distance"},{"metadata":{},"cell_type":"markdown","source":"Next the positional coordinates are used to calculate Euclidean distance between running back and linebacker per each row in the dataset. As noted before, in some cases this value is based on the positional average of several linebackers in particular play. As the defensive players are however lined up by rule due time of snap, the variance of these average values and actual positions is not a major issue compared to the fact that many linebackers' positional data is missing from the original dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"# import module\nimport math \n\n# x and y coordinates to lists\na = df['X_lb'].values.tolist()\nb = df['Y_lb'].values.tolist()\nc = df['X_rb'].values.tolist()\nd = df['Y_rb'].values.tolist()      \n\n# empty list for Euclidean distance\nMyList = []\n\n# function to calculate Euclidean distance for LB and RB x,y  values in lists\ndef distance(x1, y1, x2, y2): \n                    result = [math.sqrt(math.pow(x2 - x1, 2) + math.pow(y2 - y1, 2) * 1.0) for (x1, y1, x2, y2) in zip(a,b,c,d)] \n                    MyList.append(result)\n            \n# execute function on list values            \ndistance (a,b,c,d)\n\n# flatten results list so that it fits the dataframe\nMyList = np.array(MyList).flatten()\n\n# round MyList to two digits to fit the dataframe format\nMyList = np.round(MyList, 2)\n\n#create new column 'euc' for Euclidean distance\ndf['euc'] = np.array(MyList)\n\n# df.head(25)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One thing instantly clear is the similarity of distance values. This is caused by the rules of football, according to which both offense and defense must be lined up in a certain way when ball is snapped. This means that the same positional setup in the line of scrimmage is more or less repeated as such dozens of times in a game.\n<hr>"},{"metadata":{},"cell_type":"markdown","source":"#### 2.2 Acceleration"},{"metadata":{},"cell_type":"markdown","source":"Next separate acceleration columns based on player position are created."},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a new column acc_lb for acceleration by LB position\ndf.loc[:,'acc_lb'] = df['A']\n\n# insert the acceleration value from column A, otherwise 0\ndf['acc_lb'] = df['Position_LB'].apply(lambda x: None if x==1 else 0)\ndf['acc_lb'] = df['acc_lb'].fillna(df['A'])\n\n# replace 0 values in column with NaN\ndf.acc_lb = df.acc_lb.replace(0, np.nan)\n\n# fill NaN values based on multi index Play. Original code: StackOverFlow, user: piRSquared\ndf = df.groupby(level='Play').bfill()\n\n\n# there are still NaN values left in acc_lb column\n# next the NaN values are replaced with the average LB acceleration\n\n\n# create average acceleration for LB\nacc_1 = df['acc_lb'].mean()\n\n# round acc_1 to two digits\nacc_1 = np.round(acc_1, 2)\n\n# replace acc_lb NaN values with average LB acceleration (acc_1) \ndf['acc_lb'] = df['acc_lb'].fillna(acc_1)\n\n\n# next the process above is repeated on RB position\n\n\n# create a new column acc_rb for acceleration by RB position\ndf.loc[:,'acc_rb'] = df['A']\n\n# insert the acceleration value from column A, otherwise 0\ndf['acc_rb'] = df['Position_LB'].apply(lambda x: None if x==0 else 0)\ndf['acc_rb'] = df['acc_rb'].fillna(df['A'])\n\n# replace 0 values in column with NaN\ndf.acc_rb = df.acc_rb.replace(0, np.nan)\n\n# fill NaN values based on multi index Play\ndf = df.groupby(level='Play').bfill()\n\n# for remaining NaN values, create average acceleration for RB\nacc_2 = df['acc_rb'].mean()\n\n# round acc_2 to two digits\nacc_2 = np.round(acc_2, 2)\n\n# replace acc_rb NaN values with average RB acceleration (acc_2) \ndf['acc_rb'] = df['acc_rb'].fillna(acc_2)\n\n\n# drop original acceleration column A\nacc_col = ['A']\ndf = df.drop(acc_col, axis=1)\n\n\n# df.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A key aspect in running back vs. linebackers matchup is their acceleration in a particular play. The faster the running back accelerates, the more yards he can be expected to gain before being caught by defensive secondary line i.e. linebackers. Conversely, linebackers are hypothetically more likely to catch the ball carrier if their acceleration is high.\n\nNext a value is created comparing the running back and linebackers acceleration in particular plays as described in Play column. The value is calculated simply dividing the running back acceleration value by linebacker acceleration.\n\nFor example, if running back acceleration is 2.53 compared to linebacker acceleration 1.26 in particular play, the result is 2.53 / 1.26 = 2.01. Thus the new value is higher if the running back acceleration has advantage acceleration-wise in a particular play. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# RB and LB acceleration values to two lists\nac1 = df['acc_lb'].values.tolist()\nac2 = df['acc_rb'].values.tolist()\n\n# empty list for relative acceleration\nRelAcc = []\n\n# function to calculate relative acceleration using two lists of values\ndef relative_acc(x1, x2): \n                    result =  [(x2 / x1) for (x1, x2) in zip(ac1,ac2)] \n                    RelAcc.append(result)   \n        \n# execute function on list values            \nrelative_acc (ac1,ac2)\n\n# flatten results list so that it fits the dataframe\nRelAcc = np.array(RelAcc).flatten()\n\n# round RelAcc to two digits to fit the dataframe format\nRelAcc = np.round(RelAcc, 2)\n\n#create new column 'RelAcc' for relative acceleration value\ndf['RelAcc'] = np.array(RelAcc)\n        \n# df.head(20)        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<hr>"},{"metadata":{},"cell_type":"markdown","source":"#### 2.3 Speed"},{"metadata":{},"cell_type":"markdown","source":"Finally, the speed value (S) is treated in a similar manner. First, two separate columns for running back and linebacker speed are created. After those values are in place, a new value RelSpd is created, dividing the running back speed by linebacker speed."},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a new column spd_lb for speed by LB position\ndf.loc[:,'spd_lb'] = df['S']\n\n# insert the speed value from column S, otherwise 0\ndf['spd_lb'] = df['Position_LB'].apply(lambda x: None if x==1 else 0)\ndf['spd_lb'] = df['spd_lb'].fillna(df['S'])\n\n# replace 0 values in column with NaN\ndf.spd_lb = df.spd_lb.replace(0, np.nan)\n\n# fill NaN values based on multi index Play. Original code: StackOverFlow, user: piRSquared\ndf = df.groupby(level='Play').bfill()\n\n# create average speed for LB\nspd_1 = df['spd_lb'].mean()\n\n# round spd_1 to two digits\nspd_1 = np.round(spd_1, 2)\n\n# replace spd_lb NaN values with average LB speed (spd_1) \ndf['spd_lb'] = df['spd_lb'].fillna(spd_1)\n\n\n# the process above is repeated on RB position\n\n\n# create a new column spd_rb for speed by RB position\ndf.loc[:,'spd_rb'] = df['S']\n\n# insert the speed value from column S, otherwise 0\ndf['spd_rb'] = df['Position_LB'].apply(lambda x: None if x==0 else 0)\ndf['spd_rb'] = df['spd_rb'].fillna(df['S'])\n\n# replace 0 values in column with NaN\ndf.spd_rb = df.spd_rb.replace(0, np.nan)\n\n# fill NaN values based on multi index Play\ndf = df.groupby(level='Play').bfill()\n\n# for remaining NaN values, create average speed for RB\nspd_2 = df['spd_rb'].mean()\n\n# round spd_2 to two digits\nspd_2 = np.round(spd_2, 2)\n\n# replace spd_rb NaN values with average RB speed (spd_2) \ndf['spd_rb'] = df['spd_rb'].fillna(spd_2)\n\n\n\n# RB and LB speed values to two lists\nsp1 = df['spd_lb'].values.tolist()\nsp2 = df['spd_rb'].values.tolist()\n\n# empty list for relative speed\nRelSpd = []\n\n# function to calculate relative speed using two lists of values\ndef relative_spd (x1, x2): \n                    result =  [(x2 / x1) for (x1, x2) in zip(sp1,sp2)] \n                    RelSpd.append(result)   \n        \n# execute function on list values            \nrelative_spd (sp1,sp2)\n\n# flatten results list so that it fits the dataframe\nRelSpd = np.array(RelSpd).flatten()\n\n# round RelSpd to two digits to fit the dataframe format\nRelSpd = np.round(RelSpd, 2)\n\n#create new column RelSpd for relative speed value\ndf['RelSpd'] = np.array(RelSpd)\n\n# create a new column PlayYards with Yards column values\n# this is not necessary but it easily relocates the column\ndf.loc[:,'PlayYards'] = df['Yards']\n\n\n# drop original speed column S and Yards \ndrp_cols = ['S', 'Yards']\ndf = df.drop(drp_cols, axis=1)\n\n# df.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<hr>"},{"metadata":{},"cell_type":"markdown","source":"### 3. Analysis"},{"metadata":{},"cell_type":"markdown","source":"The starting point of this analysis was the very observation that the average number of yards gained in a play by ball carrier (i.e. running back) in the 2020 Data Bowl dataset was 4.18 yards. Based on this, I formulated the following research question: is it possible to predict which run plays go longer than the average value? Or, rather, what features in the dataset are emphasized in plays reaching more than 4.18 yards?\n\nBelow a new column Yds4_18 is created. The value of the column is 0 if the run is equal or less than 4.18 yards, and 1 of the run play reaches over the average length."},{"metadata":{"trusted":true},"cell_type":"code","source":"# the average yards gained in a play in the dataset is 4.18\n# create a new column Yds4_18\ndf.loc[:,'Yds4_18'] = df['PlayYards']\n\n# set new column value 0 if PlayYards are equal or less than 4.18, 1 if more\nf = lambda x: 0 if x <= 4.18 else 1\ndf['Yds4_18'] = df['Yds4_18'].map(f)\n\n#df.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The overall plot below shows that there are - as could be assumed - over two times more data on linebackers compared to running backs. After all, most of the time there is only one running back in a play. However, the dataset would even more imbalanced if all linebacker data were included in the 2020 Data Bowl dataset, as noted before. \n\nNot having that data available necessarily affects also the outcome of this analysis. Of course this only a practice notebook, so this issue will be sidelined from now on."},{"metadata":{"trusted":true},"cell_type":"code","source":"# import modules\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# set plot size and font\nsns.set(rc={'figure.figsize':(9.7,8.27)})\nsns.set(font='sans-serif', palette='colorblind')\n\n# set plot parameters\nplot = sns.countplot(x = 'RbLb',\n              data = df,\n              order = df['RbLb'].value_counts().index)\n\n# set plot title etc.\nplot.axes.set_title('Total count of dataset rows divided by position',fontsize=24)\nplot.set_xlabel(\"Position\",fontsize=18)\nplot.set_ylabel(\"Total count of rows\",fontsize=18)\nplot.tick_params(labelsize=14)\n\n# show plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sending three wide receivers in a play increases the relative chance for a 4.18+ yard run. The reason for this is ovbious: the defense is expecting - and thus preparing for - a pass play instead of a run."},{"metadata":{"trusted":true},"cell_type":"code","source":"# set plot size and font\nsns.set(rc={'figure.figsize':(9.7,8.27)})\nsns.set(font='sans-serif', palette='colorblind')\n\n# set plot parameters\nplot = sns.countplot(x = 'Yds4_18',\n              data = df,\n              hue = 'Is3Wr',\n              order = df['Yds4_18'].value_counts().index)\n\n# set plot title etc.\nplot.axes.set_title('4.18 yards threshold divided by 3WR on field',fontsize=24)\nplot.set_xlabel(\"0 = 4.18 yards or less, 1 = more\",fontsize=18)\nplot.set_ylabel(\"Total count of rows\",fontsize=18)\nplot.tick_params(labelsize=14)\nplot.legend (loc=1, fontsize = 16, fancybox=True, framealpha=1, shadow=True, borderpad=1, title = '0 = not 3WR, 1 = 3WR')\n\n# show plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The lineplot below takes the running back acceleration as x value and compares it to yards gained by the ball carried in a play. The hue factor is 4.18+ yard plays, and when outliers are excluded, it looks like the increasing acceleration does not really have clear significance on yards gained as an individual feature.\n\nThe acc_rb.describe function (not in the code) reveals that the running back acceleration average (yards/second) is 2.59 with standard deviation of 0.79. Although there is a slight increase in 4.18+ yard rushes in range of 2.5 - 4.0 yds/second acceleration, it does not really stand out from overall data.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# set plot parameters\nsns.set(font='sans-serif', palette='colorblind', font_scale=1.5) \nsns.lineplot(y='PlayYards', x='acc_rb', data=df, hue='Yds4_18', legend = 'full')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The linebackers acceleration average in the dataset is 1.75 yds/second with standard deviation of 0.93. In pass rush linebacker acceleration is a key factor, because the players must by the rules stay in the lineup before the snap: a flying start results as offside and and a five-yard penalty.\n\nAgain, as an individual feature linebacker acceleration does not seem to affect neither the number of yards gained nor the likelihood for 4.18+ yards run play. The high values between 4 and 6 yards/second seem like outliers, when we think about the mean value of linebacker acceleration (1.75 yds/s).\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# set plot parameters\nsns.set(font='sans-serif', palette='colorblind', font_scale=1.5) \nsns.lineplot(y='PlayYards', x='acc_lb', data=df, hue='Yds4_18', legend = 'full')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The RelAcc column value was created by dividing running back acceleration with linebacker acceleration in each play (and applying the average linebacker acceleration when necessary).\n\nThe RelAcc.describe function reveals that the RelAcc column average is 2.32 with large-ish standard deviation of 4.44. The third percentile (75 percent of all datapoints) is 2.38.\n\nThe RelAcc column includes very high values as outliers, with the maximum value being 268.5. This is presented as scatterplot below:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# define plot\nsns.scatterplot(x = \"RelAcc\", y = \"PlayYards\", data = df, color = 'lime')\n\n# set plot title etc.\nplt.xlabel('Relative acceleration')\nplt.ylabel('PlayYards')\nplt.title('Relative acceleration and yards gained in play')\n\n# show plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When the 2.32 average of values is added to standard deviation 4.44, we get 6.76. In the following plot only RelAcc values below 6.77 are included to give a closer view on the case."},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a new column RelAcc6_177\ndf.loc[:,'RelAcc6_77'] = df['RelAcc']\n\n# set new column value 0 if RelAcc is equal or less than 6.77, 1 if more\nf = lambda x: 0 if x <= 6.77 else 1\ndf['RelAcc6_77'] = df['RelAcc6_77'].map(f)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The count below shows that of 66954 total datapoints, only 2659 (3.9 percent) have a value above 6.77. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# create variable acc_count with count of different values in column RelAll6_77\nacc_count = df['RelAcc6_77'].value_counts()\n\n# print variable\nprint (acc_count)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The same dichotomy is be shown as a plot below. The blue color represents 96.1 percent of all datapoints (with value below 6.77)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# set plot parameters\nsns.set(font='sans-serif', palette='colorblind', font_scale=1.5) \nsns.lineplot(y='PlayYards', x='RelAcc', data=df, hue='RelAcc6_77', legend = 'full')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on information above, next the highest 3.9 percent of RelAcc values are dropped from the dataset, and a new column RelAcc_2 is created for the remaining values. In the new column, the dropped values (those above 6.77) are replaced with the average value of RelAcc column (2.32)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# store relacc_mean\nrelacc_mean = df['RelAcc'].mean()\n\n# round relacc_mean to two digits\nrelacc_mean = np.round(relacc_mean, 2)\n\n# create new column RelAcc_2 where value is 0 if RelAcc is greater or equal than 6.77\ndf['RelAcc_2'] = df['RelAcc'].apply(lambda x: None if x <= 6.77 else 0)\n\n# get other column values from RelAcc\ndf['RelAcc_2'] = df['RelAcc_2'].fillna(df['RelAcc']) \n\n# replace 0 values with Nan\ndf.RelAcc_2 = df.RelAcc_2.replace(0, np.nan)\n\n# replace Nan values with average RelAcc value stored in relacc_mean\ndf['RelAcc_2'] = df['RelAcc_2'].fillna(relacc_mean)\n\n# drop unnecessary columns for relative acceleration\nrelacc_cols = ['RelAcc', 'RelAcc6_77']\ndf = df.drop(relacc_cols, axis=1)\n\n#df.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the histogram below, the now-overrepresented average relative acceleration values are well shown in an otherwise relatively even divide. In percentages, we are however talking about 5.5 percent share what would otherwise be about 2 percent. Compared to the earlier outliers in the same data, this sounds acceptable. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# import module\nimport plotly.express as px\n\n# set plot parameters\nfig = px.histogram(df, x=\"RelAcc_2\", nbins = 100, histnorm = 'percent')\nfig.data[0].marker.color = \"orange\"\nfig.data[0].marker.line.width = 2\nfig.data[0].marker.line.color = \"black\"\n\n# set plot title\nfig.update_layout(\n    title={\n        'text': \"Relative acceleration datapoints divided by percentage\",\n        'y':0.95,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'})\n\n# show plot\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At first glance, the increased relative acceleration seems to slightly correlate with 4.18+ yard run plays. However - just like the position group acceleration values - it is more of a complementary factor in larger setting than a decisive factor by itself."},{"metadata":{"trusted":true},"cell_type":"code","source":"# set plot parameters\nsns.set(font='sans-serif', palette='colorblind', font_scale=1.5) \nsns.lineplot(y='PlayYards', x='RelAcc_2', data=df, hue='Yds4_18')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below is a plot showing the number of 4.18+ yard rush plays compared to relative acceleration of 2.32 or more.\n\nAs one can see, in both categories 4.18+ yard run plays form roughly a quarter of all datapoints included. This means that increasing the value of relative acceleration does not significantly increase the likelihood of a 4.18+ yard run by ball carrier.\n\nThus a random guess before a run play in a football game (let's assume we know it will be a run play) is right three out of four times, if the guess predicts the run to be less than 4.18 yards. Conversely, a guess predicting a longer run would in average be correct one in four cases.\n\nIf we could create machine learning model able to predict 4.18+ run plays with better accuracy than 25 percent as well as shorter runs with a 75+ percent accuracy, it could in this context be considered as a relative success compared to a mere coin toss.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a new column RelAcc2_32\ndf.loc[:,'RelAcc2_32'] = df['RelAcc_2']\n\n# set new column value 0 if RelAcc_2 is equal or less than 2.32, 1 if more\nf = lambda x: 0 if x <= 2.32 else 1\ndf['RelAcc2_32'] = df['RelAcc2_32'].map(f)\n\n# define plot size, color etc.\nsns.set(rc={'figure.figsize':(9.7,7.27)})\nsns.set(font='sans-serif', palette='colorblind')\n\n# set plot parameters\nplot = sns.countplot(x = 'Yds4_18',\n              data = df,\n              hue = 'RelAcc2_32',\n              order = df['Yds4_18'].value_counts().index)\n\n# set plot title etc.\nplot.axes.set_title('4.18 yards threshold divided by relative acceleration (2.32)',fontsize=18)\nplot.set_xlabel(\"0 = 4.18 yards run or less, 1 = more\",fontsize=18)\nplot.set_ylabel(\"Total count of rows\",fontsize=18)\nplot.tick_params(labelsize=14)\nplot.legend (loc=1, fontsize = 16, fancybox=True, framealpha=1, shadow=True, borderpad=1, title = '0 = 2.32 or less, 1 = more')\n\n# show plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The value first created for further analysis was euc, which describes the Euclidean distance between running back and linebacker (or linebackers). As noted, at the beginning of the play both teams are by rules lined up in the line of scrimmage.\n\nAs the histogram below shows, the two peaks - running back and the average position of a linebacker in line of scrimmage - together form some two thirds of all datapoints. It can be safely assumed that the shorter distance peak consists of running back vs. middle linebacker distances, and the other one describes the distance between running back and outer linebackers on both sides of the defensive line closer to sidelines.\n\nWe can also make the assumption that the shorter of the two peaks - with wider range of values - on the bottom describes the linebacker positions, since in the dataset there are 1-4 linebackers on the field depending on the play. The running back is without exception lined next to quarterback for ball handoff, whereas linebackers are spread out in wider formation.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import module\nimport plotly.express as px\n\n# set plot parameters\nfig = px.histogram(df, x=\"euc\", nbins = 100, histnorm = 'percent')\nfig.data[0].marker.color = \"orange\"\nfig.data[0].marker.line.width = 2\nfig.data[0].marker.line.color = \"black\"\n\n# set plot title\nfig.update_layout(\n    title={\n        'text': \"Euclidean distance datapoints divided by percentage\",\n        'y':0.95,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'})\n\n# show plot\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As the plot shows, there may be some outliers included in the Euclidean distance values. According to euc.describe function, of 69613 total datapoint the mean value is 6.46 with standard deviation of 1.77. The third percentile value is 6.88, and the maximum value is 48.6.\n\nHowever, the histogram above shows no clusters larger than 0.2 percent after the euc value reaches 12.\n\nBased on this, a new column euc_12 is created splitting the euc values in two groups, one consisting of values 12.0 and smaller, leaving the other one for values larger than 12.0.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a new column euc_12\ndf.loc[:,'euc_12'] = df['euc']\n\n# set new column value 0 if euc is equal or less than 12.0, 1 if more\nf = lambda x: 0 if x <= 12.0 else 1\ndf['euc_12'] = df['euc_12'].map(f)\n\n# print out the number of 0 and 1 values in the new column\neuc_count = df['euc_12'].value_counts()\nprint (euc_count)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<hr>\nThe printout shows that only about 0.01 of all euc datapoints in the dataset have a value of 12 or larger.\n\nBased on this, next euc values larger than 12 will be removed, and they will be replaced with the average euc value. This will be done by creating a new column euc_2.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# store euc_mean\neuc_mean = df['euc'].mean()\n\n# round euc_mean to two digits\neuc_mean = np.round(euc_mean, 2)\n\n# create new column euc_2 where value is 0 if euc is greater or equal than 12.0\ndf['euc_2'] = df['euc'].apply(lambda x: None if x <= 12.0 else 0)\n\n# get other column values from euc\ndf['euc_2'] = df['euc_2'].fillna(df['euc']) \n\n# replace 0 values with Nan\ndf.euc_2 = df.euc_2.replace(0, np.nan)\n\n# replace Nan values with average euc value stored in euc_mean\ndf['euc_2'] = df['euc_2'].fillna(euc_mean)\n\n# drop previous columns for Euclidean distance as well as RelAcc2_32\neuc_cols = ['euc', 'euc_12', 'RelAcc2_32']\ndf = df.drop(euc_cols, axis=1)\n\n# df.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now the two clusters in the distance values show even more clearly:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# set histogram parameters\nfig = px.histogram(df, x=\"euc_2\", nbins = 100, histnorm = 'percent')\nfig.data[0].marker.color = \"orange\"\nfig.data[0].marker.line.width = 2\nfig.data[0].marker.line.color = \"black\"\n\n# set plot title\nfig.update_layout(\n    title={\n        'text': \"euc_2 column datapoints divided by percentage\",\n        'y':0.95,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'})\n\n# show histogram\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As a rule of thumb, the farther a linebacker is from ball carrier (running back), the less he has in average a chance of reaching the running back before 4.18 yard threshold.\n\nIt is good to remember though that - depending on play call - this is not always even the task. ***Playcalling is an essential part of football, and excluding it from dataset inevitably limits the scope and relevance of any play analysis.***\n\nOne of the most talked-about plays in 2020 Superbowl was the '3rd & 15 play' in the fourth quarter. The Kansas City Chiefs quarterback Patrick Mahomes faced third down with 15 yards to gain, and they were losing the game. The Chiefs offensive formation was shotgun, which usually leads to a quick pass to wide receiver along with a wish that the receiver can somehow gain the necessary yards after the catch. Shotgun formation is also risky, because it leaves the quarterback without protection if something goes wrong.\n\nInstead of regular shotgun, Mahomes had called 'wasp play', which in Chiefs playbook meant one of the receivers going deep for a long pass. Mahomes backed up eleven steps (normally 5-7 is the maximum) and threw the ball for 55 yards. In the end Chiefs scored a touchdown and turned the game around for a Superbowl win.\n\nAlso, a mere decription of personnel formation does not tell everything about the play. For example a running back cannot - or shouldn't - run against defensive blitz: 3-5 defensive players trying to get to the quarterback. Although biltz most often happen with less defenders in the box, this is not always the case. The important point is however that a blitz play may occur with the same defensive personnel as some other playcall - it cannot be deducted from personnel list.\n<hr>\nThe plot below shows that likelihood for 4.18+ yard run stays relatively same regardless of Euclidean distance larger than average. Using the earlier coin toss example, there is a slightly larger than 50-50 chance for a 4.18+ yard run when the distance reaches the average value 6.46. However the same goes also to runs below 4.18 yards, so the Euclidean distance itself does not explain anything.\n\nMore likely, the classic run play rule is still valid also in the age of analytics: the run play should always be aimed at the linebacker considered weakest - either speed, strengh, agility or all of them comive lineup on field in a play. More likely, the classic run play rule is still valid also in the age of analytics: the run play should always be aimed at the linebacker considered weakest - either by speed, strength, acceleration, agility, experience or all of them combined."},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a new column euc6_46\ndf.loc[:,'euc6_46'] = df['euc_2']\n\n# set new column value 0 if euc is equal or less than 6.46, 1 if more\nf = lambda x: 0 if x <= 6.46 else 1\ndf['euc6_46'] = df['euc6_46'].map(f)\n\n# set plot size, color etc.\nsns.set(rc={'figure.figsize':(9.7,7.27)})\nsns.set(font='sans-serif', palette='colorblind')\n\n# set plot parameters\nplot = sns.countplot(x = 'Yds4_18',\n              data = df,\n              hue = 'euc6_46',\n              order = df['Yds4_18'].value_counts().index)\n\n# set plot title etc.\nplot.axes.set_title('4.18 yards threshold divided by Euclidean distance (6.46)',fontsize=18)\nplot.set_xlabel(\"0 = 4.18 yards run or less, 1 = more\",fontsize=18)\nplot.set_ylabel(\"Total count of rows\",fontsize=18)\nplot.tick_params(labelsize=14)\nplot.legend (loc=1, fontsize = 16, fancybox=True, framealpha=1, shadow=True, borderpad=1, title = '0 = 6.46 or less, 1 = more')\n\n# show plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The linebacker speed has an average values of 2.59 (yds/second) with standard deviation of 1.21. The third percentile is 3.31 and the maximum 7.77, meaning the linebacker speed dataset does not include any significant outliers.\n\nAs for running backs, the average value is 4.17 with standard deviation of 0.89. The third percentile is 4.48, and the maximum is 8.5. This means both speed data columns are useful concerning the integrity of dataset.\n\nThe RelSpd value was created dividing running back speed with linerbacker speed (or average linebacker speed in a play). The aim for this was to see if higher relative running back speed affects the probability for a 4.18+ yard run.\n\nThe average RelSpd value is 2.32 with standard deviation of 4.64. The third percentile value is 2.35, and the maximum is 417, meaning there are significant outliers.\n\nThe next plot inspect those outliers more closely.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# set histogram paramters\nfig = px.histogram(df, x=\"RelSpd\", nbins = 200, histnorm = 'percent')\nfig.data[0].marker.color = \"orange\"\nfig.data[0].marker.line.width = 2\nfig.data[0].marker.line.color = \"black\"\n\n# set plot title\nfig.update_layout(\n    title={\n        'text': \"Relative speed datapoints divided by percentage\",\n        'y':0.95,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'})\n\n# show histogram\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As one can see, the 12.0 markdown has a very small percentage of all datapoints, so we exclude all values above 12 using the method familiar from acceleration and distance."},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a new column spd_12\ndf.loc[:,'RelSpd_12'] = df['RelSpd']\n\n# set new column value 0 if RelSpd is equal or less than 12.0, 1 if more\nf = lambda x: 0 if x <= 12.0 else 1\ndf['RelSpd_12'] = df['RelSpd_12'].map(f)\n\n# print out the number of 0 and 1 values in the new column\nRelSpd_count = df['RelSpd_12'].value_counts()\nprint (RelSpd_count)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<hr>\nOf almost 94000 values, the exclusion will remove only 990."},{"metadata":{"trusted":true},"cell_type":"code","source":"# store RelSpd_mean\nRelSpd_mean = df['RelSpd'].mean()\n\n# round euc_mean to two digits\nRelSpd_mean = np.round(RelSpd_mean, 2)\n\n# create new column RelSpd_2 where value is 0 if RelSpd is greater or equal than 12\ndf['RelSpd_2'] = df['RelSpd'].apply(lambda x: None if x <= 12 else 0)\n\n# get other column values from RelSpd\ndf['RelSpd_2'] = df['RelSpd_2'].fillna(df['RelSpd']) \n\n# replace 0 values with Nan\ndf.RelSpd_2 = df.RelSpd_2.replace(0, np.nan)\n\n# replace Nan values with average RelSpd value stored in relacc_mean\ndf['RelSpd_2'] = df['RelSpd_2'].fillna(RelSpd_mean)\n\n# drop previous columns for RelSpd\nrelspd_cols = ['RelSpd', 'RelSpd_12']\ndf = df.drop(relspd_cols, axis=1)\n\n# df.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.RelSpd_2.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<hr>\nNext a new column RelSpd2_06 is created, with values less than relative speed average from above printout (2.06) marked as 0 and higher values as 1."},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a new column RelSpd2_06\ndf.loc[:,'RelSpd2_06'] = df['RelSpd_2']\n\n# set new column value 0 if RelSpd is equal or less than RelSpd_mean_2 (2.06), 1 if more\nf = lambda x: 0 if x <= 2.06 else 1\ndf['RelSpd2_06'] = df['RelSpd2_06'].map(f)\n\n# df.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As the plot below shows, when the relative speed of running back vs. linebacker or linebackers increase, there is a better chance for the ball carrier to reach 4.18+ yards. However, on the level of probability, we are still talking about a coin toss since any reasonable margin of error would cover both outcomes."},{"metadata":{"trusted":true},"cell_type":"code","source":"# set plot size, color etc.\nsns.set(rc={'figure.figsize':(9.7,7.27)})\nsns.set(font='sans-serif', palette='colorblind')\n\n# set plot parameters\nplot = sns.countplot(x = 'Yds4_18',\n                data = df,\n                hue = 'RelSpd2_06',\n                order = df['Yds4_18'].value_counts().index)\n\n# set plot title etc.\nplot.axes.set_title('4.18 yards threshold divided by relative speed (2.06)',fontsize=18)\nplot.set_xlabel(\"0 = 4.18 yards run or less, 1 = more\",fontsize=18)\nplot.set_ylabel(\"Total count of rows\",fontsize=18)\nplot.tick_params(labelsize=14)\nplot.legend (loc=1, fontsize = 16, fancybox=True, framealpha=1, shadow=True, borderpad=1, title = '0 = 2.06 or less, 1 = more')\n\n# show plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As argued earlier, football is a situational game especially when it comes to running plays. For example, if the ball carrier is near his own goal line, there is a risk for him fumbling the ball, which would probably result as a defensive touchdown. Conversely, in the same setting linebackers have the majority of the field behind them, meaning there is a risk of them being too aggressive and leaving the field open for pass or an easy long run.\n\nBelow is a histogram of the X position of running backs in the dataset. There are two visible clusters in the histogram, but in fact they both describe the same thing. This is explained in the plot after the histogram.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# set histogram parameters\nfig = px.histogram(df, x=\"X_rb\", nbins = 100, histnorm = 'percent')\nfig.data[0].marker.color = \"orange\"\nfig.data[0].marker.line.width = 2\nfig.data[0].marker.line.color = \"black\"\n\n# set plot title\nfig.update_layout(\n    title={\n        'text': \"Running back X position datapoints divided by percentage\",\n        'y':0.95,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'})\n\n# show histogram\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The YardLine column in the dataset describes where on the field the play started from. One must keep in mind here that football field consists of two halves each having the same yardlines from 0 to 49, 0 being the endzone goal line. The emphasis on 25-yard line is explained by the rules of football: that is where the plays start from after a touchdown or touchback. This is also why the 25-yard line showed up twice in earlier histogram."},{"metadata":{"trusted":true},"cell_type":"code","source":"# set histogram parameters\nfig = px.histogram(df, x=\"YardLine\", nbins = 100, histnorm = 'percent')\nfig.data[0].marker.color = \"orange\"\nfig.data[0].marker.line.width = 2\nfig.data[0].marker.line.color = \"black\"\n\n# set plot title\nfig.update_layout(\n    title={\n        'text': \"YardLine column datapoints divided by percentage\",\n        'y':0.95,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'})\n\n# show histogram\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is customary for teams to use run plays on first and second down. This is because gaining a first down requires advancing the ball for 10 yards, and getting halfway by running the ball makes the following pass plays much more likely to succeed.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# set histogram parameters\nfig = px.histogram(df, x=\"Down\", nbins = 8, histnorm = 'percent')\nfig.data[0].marker.color = \"orange\"\nfig.data[0].marker.line.width = 4\nfig.data[0].marker.line.color = \"black\"\n\n# set plot title\nfig.update_layout(\n    title={\n        'text': \"Down column datapoints divided by percentage\",\n        'y':0.95,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'})\n\n# set x ticks\nfig.update_xaxes(nticks = 4)\n\n# show histogram\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The likelihood for successful 4.18+ yard run does not correlate with down, as the plot below shows."},{"metadata":{"trusted":true},"cell_type":"code","source":"# set plot size, color etc.\nsns.set(rc={'figure.figsize':(9.7,7.27)})\nsns.set(font='sans-serif', palette='colorblind')\n\n# set plot parameters\nplot = sns.countplot(x = 'Yds4_18',\n                data = df,\n                hue = 'Down',\n                order = df['Yds4_18'].value_counts().index)\n\n# set plot title etc.\nplot.axes.set_title('4.18 yards threshold divided by down (1-4)',fontsize=24)\nplot.set_xlabel(\"0 = 4.18 yards run or less, 1 = more\",fontsize=18)\nplot.set_ylabel(\"Total count of rows\",fontsize=18)\nplot.tick_params(labelsize=14)\nplot.legend (loc=1, fontsize = 16, fancybox=True, framealpha=1, shadow=True, borderpad=1, title = 'Down (1-4)')\n\n# show plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div class=\"h3\"><i>Concluding the job, no significant features correlating with 4.18+ yard runs were found in the analysis.\n<br>\n<br>\nIt is now time to introduce a chimpanzee with dart.\n</i></div>\n<hr>"},{"metadata":{},"cell_type":"markdown","source":"### 4. (How To Become) ML Model"},{"metadata":{},"cell_type":"markdown","source":"***Referring to the disclaimer in the beginning, I am not a coder or data scientist. Therefore I have no skills for building high-end machine learning custom models.***\n\nIf one takes a look at the NFL Big Data Bowl 2020 results, the winning models were excellent in predicting how many yards the ball carrier will reach in a play - I could never come up with anything similar. All contecnt in this notebook is practice only, and reading through is useful only if you are an absolute beginner just like me.\n\nThis is where the chimpanzee with dart stems from. A couple of years ago in Russia, Lusha the chimpanzee took on some of the top bankers in the country and beat 94 percent of them in making lucrative investment decisions. Some serious questions were raised afterwards about the wunderkinds of Russian financial sector with their ridiculous bonuses. Lusha made the high-paid professionals look like clowns, which was an accomplishment in itself considering that it was Lusha who actually worked in circus for a living.\n\nKeeping this in mind, it is not useful for me trying to copy-paste existing ML model gurus in this notebook. Rather, I will start from the other end.\n\n**Next I will imagine myself as an ML model, which in the beginning knows nothing about what it should actually do.**\n\nI start by using 4.18+ yard runs as the deciding factor in the dataset:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# set plot size, color etc.\nsns.set(rc={'figure.figsize':(9.7,7.27)})\nsns.set(font='sans-serif', palette='colorblind')\n\n# set plot parameters\nplot = sns.countplot(x = 'Yds4_18',\n              data = df,\n              order = df['Yds4_18'].value_counts().index)\n\n# set plot title and x/y labels\nplot.axes.set_title('Total count of dataset rows divided by 4.18+ run plays',fontsize=18)\nplot.set_xlabel(\"0 = 4.18 yards or less, 1 = more\",fontsize=18)\nplot.set_ylabel(\"Total count of rows\",fontsize=18)\nplot.tick_params(labelsize=14)\n\n#show the plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The same as printout:"},{"metadata":{"trusted":true},"cell_type":"code","source":"run4_18_count = df['Yds4_18'].value_counts()\nprint (run4_18_count)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<hr>\n67 percent of all datapoints in the dataset are from runs that did not reach the 4.18 yard benchmark, which is the average value of PlayYards column. Thus, for a model trying to predict longer runs, only one third of the dataset is from that perspective relevant.\n\nAs an aspiring machine learning model, I would still start from the larger portion of dataset. Next I will treat runs with less than 0 yards as an anomaly, and anything longer than four yards as temporarily irrelevant. After all, in a bitcoin toss I would already have a 66-percent chance of predicting a run to result in less than four yards.\n\nBelow is the PlayYards column plotted by yards:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# set histogram parameters\nfig = px.histogram(df, x=\"PlayYards\", nbins = 100, histnorm = 'percent')\nfig.data[0].marker.color = \"orange\"\nfig.data[0].marker.line.width = 2\nfig.data[0].marker.line.color = \"black\"\n\n# set plot title\nfig.update_layout(\n    title={\n        'text': \"PlayYards column datapoints divided by percentage\",\n        'y':0.95,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'})\n\n# show histogram\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If the short run plays are inspected more closely, we can see that the number of datapoints starts to come down already after two yards:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create new dataframe including only PlayYard values between 1-4\ndf_2 = df[(df['PlayYards']>= 0 ) & (df['PlayYards']<= 4)]\n\n# plot historgram with yards between 1-4\nfig = px.histogram(df_2, x=\"PlayYards\", nbins = 16, histnorm = 'percent')\nfig.data[0].marker.color = \"orange\"\nfig.data[0].marker.line.width = 4\nfig.data[0].marker.line.color = \"black\"\n\n# set plot title\nfig.update_layout(\n    title={\n        'text': \"PlayYards column datapoints divided by yards 0-4\",\n        'y':0.95,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'})\n\n# set x ticks\nfig.update_xaxes(nticks=5)\n\n# show ploe\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Altogether, there's a data-journalistic minor scoop above.\n\n**Of 0-4 yard runs, about 60 percent of plays in football will not go longer than two yards. This means that the running back never confronts linebackers in those plays.**\n\nIf I were an aspiring machine learning model, this would be more good news. First I could get two thirds of predictions right only by predicting runs to result in less than four yards, and now selecting two yards or less will give me 6 cases out of 10 correct in subcategory 0-4 yards. Also, two yards would be the best individual guess with 23 percent of all datapoint in 0-4 yards selection.\n\nNow let's do the same with yards between 5-10:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create new dataframe including only PlayYard values between 5-10\ndf_3 = df[(df['PlayYards']>= 5 ) & (df['PlayYards']<= 10)]\n\n# plot histogram with yards between 5-10\nfig = px.histogram(df_3, x=\"PlayYards\", nbins = 16, histnorm = 'percent')\nfig.data[0].marker.color = \"orange\"\nfig.data[0].marker.line.width = 4\nfig.data[0].marker.line.color = \"black\"\n\n# set plot title\nfig.update_layout(\n    title={\n        'text': \"PlayYards column datapoints divided by yards 5-10\",\n        'y':0.95,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'})\n\n# show plot\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div class=\"h4\"><i>\nIn longer run plays, 5-7 yard runs form two thirds of all datapoints just like 0-2 yard runs in the other subcategory. My strategy as an aspiring ML model would be to go for 5-7 if I think the run will be more than average, and 0-2 if I think the run will not reach the 4.18 yard average threshold.\n<br>    \n<br>\nDivided between positive yardage 0-99, that's only six datapoints out of 100, meaning hitting those six datapoints correctly would give me a decent chance for a baseline case.\n <hr>\n</i></div>\n\nHowever, the original Big Data Bowl contest was not about long runs: it was predicting the overall yardage in any run play. Everything so far has indicated that predicting 4.18+ yard runs with a ML model and dataset in use is - frankly - not possible.\n\nIn fact, an orangutang with dart would probably fare better.\n\nThus the expectations are low, and this is not a problem. I don't know much of anything about machine learning, and I am only doing this for the practice. Were I hired to do this analysis with some relevant results expected, at this point I would run for the hills for sure.\n\nEarlier I contended that if we could create \"machine learning model able to predict 4.18+ yard run plays with better accuracy than 25 percent as well as shorter runs with a 75+ percent accuracy, it could in this context be considered as a relative success.\"\n\nI now get back to being an aspiring ML model myself and start inspecting which values in the dataset would be most useful in beating Lusha the chimpanzee in prediction game.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import modules \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\n# show dataframe column names\ndf.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<hr>\nWe already know that there is no 'golden column' in dataset able to provide us the road to 4.18+ yard runs. Therefore I will start by taking a wide selection of different values and see what the algorithm thinks about them. If you just opened this notebook by chance, there are more about these values in the Analysis section."},{"metadata":{"trusted":true},"cell_type":"code","source":"# select the desired features as X\nX = df[['Is3Wr', 'Is3Lb', 'Is4Lb', 'acc_lb', 'acc_rb', 'spd_lb', \n               'spd_rb','RelAcc_2', 'euc_2','RelSpd_2']]\n\n# select the labels as y (in this case 4.18+ yard runs)\ny = df['Yds4_18']\n\n# perform train-test split, train data 80%, no random state\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state = 0)\n\n# print train-test dataset sizes\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<hr>\nThe printout shows that I - an aspiring ML model - now have ten columns and 75826 datapoints to train myself with, and 18957 datapoints are left to evaluate my performance compared to the aforementioned chimpanzee with dart."},{"metadata":{"trusted":true},"cell_type":"code","source":"# import module\nfrom sklearn.preprocessing import StandardScaler\n\n# scale the feature data\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# create and fit the Logistic Regression model\nmodel = LogisticRegression(solver='lbfgs', class_weight='balanced')\nmodel.fit(X_train, y_train)\n\n# print the scores\nprint (model.score(X_train, y_train))\nprint (model.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<hr>\nSo far I - an aspiring ML model - know what I already knew after analysing the data.\n\nI am practically no better than a coin toss since I haven't really learned anything new in my modeling career yet."},{"metadata":{"trusted":true},"cell_type":"code","source":"# print the coefficients\nprint(model.coef_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<hr>\nLooking at the coefficients (how much a specific feature affects the labels, in thir case 4.18+ runs), this is no wonder. \n\nOur columns simply are not very helpful in predicting 4.18+ yard runs. Of the ten feature columns used, number five in the list (running back acceleration) looks like the most significant factor if the Logistic Regression algorithm is asked."},{"metadata":{"trusted":true},"cell_type":"code","source":"# print each feature with its respective coefficient value\nprint(list(zip(['Is3Wr', 'Is3Lb', 'Is4Lb', 'acc_lb', 'acc_rb', 'spd_lb', \n               'spd_rb','RelAcc_2', 'euc_2','RelSpd_2'],model.coef_[0])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<hr>\nDouble-checking the coefficients by feature (i.e. column) name confirms this, and so does the confusion matrix below:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import module\nfrom sklearn.metrics import confusion_matrix\n\n# set model prediction\ny_pred = model.predict(X_test)\n\n# print prediction\nconfusion_matrix = confusion_matrix(y_test, y_pred)\nprint(confusion_matrix)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" <hr>\n As the printout shows, there are 7410 correct and 5296 incorrect predictions for short runs and 3565 correct predictions compared to 2686 incorrect ones for longer 4.18+ yard runs.\n \n The classification report below returns similar figures, with a f1-score 0.65 (1 is the best score, 0 is the worst) for short runs but only 0.47 for longer ones:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import module\nfrom sklearn.metrics import classification_report\n\n# prin report\nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<hr>\nAs shown below, building a random forest classifier out of same dataset does not improve anything - rather vice versa.\n\nTo put it bluntly, we simply have nothing relevant in our dataset to predict 4.18+ yard runs."},{"metadata":{"trusted":true},"cell_type":"code","source":"# import module\nfrom sklearn.ensemble import RandomForestClassifier\n\n# select the desired features as X\nX = df[['Is3Wr', 'Is3Lb', 'Is4Lb', 'acc_lb', 'acc_rb', 'spd_lb', \n               'spd_rb','RelAcc_2', 'euc_2','RelSpd_2']]\n\n# select the labels as y (in this case 4.18+ yard runs)\ny = df['Yds4_18']\n\n# scale the feature data\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# define and fit Random Forest Classifier model\nclassifier = RandomForestClassifier(random_state=0, max_depth = 8, n_estimators = 100)\nclassifier.fit(X_train, y_train)\n\n# set model prediction\ny_pred = classifier.predict(X_test)\n\n# print report\nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<hr>\nOne other way of evaluating the dataset within a model is a ROC curve, ROC being an abbreviation of Receiver Operating Characteristic. \n\nIf the model works well, the True Positive Rate of ROC rises steeply upwards and eventually turns toward False Positive rate so that the area under the curve covers as much area as possible.\n\nBy just throwing our dataset in, one can see below that the ROC curve (yellow line) covers only sligthly more area than a 50-50 coin toss (the straight line in the middle) or the now-infamous Lusha.\n\nThis basically means there is no predictive model available using out dataset.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import module\nfrom sklearn.metrics import roc_curve\n\n# define function for ROC surve\ndef plot_roc_curve(fper, tper):  \n    plt.plot(fper, tper, color='orange', label='ROC')\n    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curve')\n    plt.legend()\n    plt.show()\n\n# set predictions        \nprobs = classifier.predict_proba(X_test)  \nprobs = probs[:, 1]  \nfper, tper, thresholds = roc_curve(y_test, probs) \n\n# plot ROC curve\nplot_roc_curve(fper, tper)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Even as a mere aspiring ML model, I start to get the hang of what's going on: not much.\n\nThe Gradient Booster algorithm can do a lot, but even it cannot make an irrelevant dataset relevant. And we do have next to nothing to boost.\n\nJust for fun, let's first run the learning rate code below to see what would be the best learning rate value for Gradient Booster.\n\nLearning rates less than 1.0 make less corrections on the process: it is common to have small values in the range of 0.1 to 0.3, as well as values less than 0.1."},{"metadata":{"trusted":true},"cell_type":"code","source":"# import modules\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\n# select the desired features as X\nX = df[['Is3Wr', 'Is3Lb', 'Is4Lb', 'acc_lb', 'acc_rb', 'spd_lb', \n               'spd_rb','RelAcc_2', 'euc_2','RelSpd_2']]\n\n# select the labels as y (in this case 4.18+ yard runs)\ny = df['Yds4_18']\n\n# scale the feature data\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# perform train-test split, train data 80%, no random state\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state = 0)\n\n# define list of learning rates to test\nlr_list = [0.05, 0.075, 0.1, 0.25, 0.5, 1, 2]\n\n# check which learning rate is the best and print the result\nfor learning_rate in lr_list:\n    gb_clf = GradientBoostingClassifier(n_estimators= 32, learning_rate=learning_rate, max_features=2, max_depth=2, random_state=0)\n    gb_clf.fit(X_train, y_train)   \n    print(\"Learning rate: \", learning_rate)\n    print(\"Accuracy score (training): {0:.3f}\".format(gb_clf.score(X_train, y_train)))\n    print(\"Accuracy score (test): {0:.3f}\".format(gb_clf.score(X_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<hr>\nLearning rate 0.25 looks like the top result for test dataset (this may vary depending on running the code).\n\nI know that the score is really low, but as as an aspiring ML model I will go for that:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import modules\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\n# select the desired features as X\nX = df[['Is3Wr', 'Is3Lb', 'Is4Lb', 'acc_lb', 'acc_rb', 'spd_lb', \n               'spd_rb','RelAcc_2', 'euc_2','RelSpd_2']]\n\n# select the labels as y (in this case 4.18+ yard runs)\ny = df['Yds4_18']\n\n# scale the feature data\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# perform train-test split, train data 80%, no random state\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state = 0)\n\n# fit the Gradient Booster model and set predictions\ngb_clf2 = GradientBoostingClassifier(n_estimators=32, learning_rate=0.25, max_features=2, max_depth=3, random_state=0)\ngb_clf2.fit(X_train, y_train)\npredictions = gb_clf2.predict(X_test)\n\n#print results\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, predictions))\n\nprint(\"Classification Report\")\nprint(classification_report(y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<hr>\nAs the confusion matrix shows, the Gradient Booster classifier got right 12592 short runs compared to 114 wrong predictions. This is a success rate of 99.1 percent, which would be excellent without the long, 4.18+ yard runs. In that category, 114 predictions were correct compared to 6137 incorrect ones, meaning in this category Lusha's dart predictions would reign victorious forever.\n\nThen again, this practice notebook is all about trial and error, not winning a ML modeling contest. Let's just go on and run the code below:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import modules\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom math import sqrt\n\n# select the desired features as X\nX = df[['Is3Wr', 'Is3Lb', 'Is4Lb', 'acc_lb', 'acc_rb', 'spd_lb', \n               'spd_rb','RelAcc_2', 'euc_2','RelSpd_2']]\n\n# select the labels as y (in this case 4.18+ yard runs)\ny = df['Yds4_18']\n\n# scale the feature data\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# define and fit the K nearest neighbors model\nmodel = KNeighborsRegressor(n_neighbors=9)\nmodel.fit(X_train, y_train)\n\n# calculate the errors for our training data\nmse = mean_squared_error(y_train, model.predict(X_train))\nmae = mean_absolute_error(y_train, model.predict(X_train))\n\n# print results\nprint(\"mean squared error = \",mse,\" & mean absolute error = \",mae,\" & root mean squared error = \", sqrt(mse))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<hr>\nThe most clear error metric above is the mean absolute error. In our dataset, the average prediction was 0.18 away from real datapoint.\n\nAgain, 0 is the best value compared to 1, so at least in theory I - an aspiring ML model - am not doing that bad were there not those unpredictable longer run plays.\n\nWe can calculate the same on the test data:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate the errors for our training data\ntest_mse = mean_squared_error(y_test, model.predict(X_test))\ntest_mae = mean_absolute_error(y_test, model.predict(X_test))\n\n# print results\nprint(\"mean squared error = \",test_mse,\" & mean absolute error = \",test_mae,\" & root mean squared error = \", sqrt(test_mse))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<hr>\n"},{"metadata":{},"cell_type":"markdown","source":"The fact that those error rates are larger in test set indicates that there is overfitting going on. In this case this is not an issue since I am an aspiring ML model, not a ML catwalk professional.\n\nThe reason for overfitting is most likely in choices made before. As our features (i.e. columns) used in evaluation don't correlate with our labels (i.e. whether the run will reach 4.18 yards), the model will therefore go for a more likely event (short run play) in its predictions.\n\nKnowing what we know, it is unlikely also that the K Neighbors classifier alogrithm would be some kind of game saver here. After all, it is based on surrounding values and their effect on predictability, and so far nothing we have tried has not predicted 4.18+ yard runs."},{"metadata":{"trusted":true},"cell_type":"code","source":"# import module\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# select the desired features as X\nX = df[['Is3Wr', 'Is3Lb', 'Is4Lb', 'acc_lb', 'acc_rb', 'spd_lb', \n               'spd_rb','RelAcc_2', 'euc_2','RelSpd_2']]\n\n# select the labels as y (in this case 4.18+ yard runs)\ny = df['Yds4_18']\n\n# scale the feature data\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# define and fit K nearest neighbor classifier model\nclassifier = KNeighborsClassifier(n_neighbors=7)\nclassifier.fit(X_train, y_train)\n\n# set predictions\ny_pred = classifier.predict(X_test)\nconfusion_matrix = confusion_matrix(y_test, y_pred)\n\n# print results\nprint(confusion_matrix)\nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<hr>\nThe K Neighbors classifier got correct 83 percent of predictions on short runs, whereas the success rate on longer runs was considerably lower (less than 23 percent).\n\nThe key value in K Neighbors algorithm is K, which describes the number of neighboring datapoints used in classification. Just think about drawing a cloed area around a datapoint an expanding it gradually to fit more datapoints i.e. nearest neighbors.\n\nK=1 would mean that only the nearest neighbor is taken into account. With this approach, someone living in a white house next to 1600 Pennsylvania Avenue could also be classified as the President of the United States based on his neighbor's house color if the model was asked. \n\nOf course this would be incorrect or 'overfitting', as they say. Then again, too large a K value results in underfitting. Just imagine drawing a circle around half a million people celebrating Superbowl win in downtown Kansas and trying to predict if those people have consumed whiskey. Undoubtedly we would get a positive response, but the response would be the same also on vodka, beer and horse tranquilizers. The latter doesn't make any claims about Kansas City Chiefs fans - there were actual horses in the victory parade.\n\nA well-accepted rule of thumb is to use K values 5 and 7 as base case, but it is also possible to get K value by calculating it:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create an empty list for error values\nerror = []\n\n# function to calculate K value\nfor i in range(1, 40):\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train, y_train)\n    pred_i = knn.predict(X_test)\n    error.append(np.mean(pred_i != y_test))\n\n# create plot for visualizing the results    \nplt.figure(figsize=(12, 6))\nplt.plot(range(1, 40), error, color='red', linestyle='dashed', marker='o',\n         markerfacecolor='blue', markersize=10)\n\n# set plot title etc.\nplt.title('Error Rate K Value')\nplt.xlabel('K Value')\nplt.ylabel('Mean Error')\n\n# show plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What we usually get is a line coming down to near-zero when the K value is somewhere between 5-10. However this is not the case in our dataset.\n\nThen again, the plot above describes our problem perfectly. Because predicting longer runs is not possible within the dataset, it raises the error rate so that there are no meaningful K values available. \n\nFor example, the error rate 0.38 for K=5 is ridiculously high. As mentioned, the error rate should be close to zero at that point. Then again, K values near zero in the graph include 30 nearest neighbors, meaning the model would underfit in a serious manner.\n\nThe code below shows the same problem from another perspective.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import modules\nimport eli5\nfrom pdpbox import pdp, get_dataset, info_plots\nfrom eli5.sklearn import PermutationImportance\nimport joblib\n\n\n# select the desired features as X\nX = df[['Is3Wr', 'Is3Lb', 'Is4Lb', 'acc_lb', 'acc_rb', 'spd_lb', \n               'spd_rb','RelAcc_2', 'euc_2','RelSpd_2']]\n\n# select the labels as y (in this case 4.18+ yard runs)\ny = df['Yds4_18']\n\n# scale the feature data\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# perform train-test split, train data 80%, no random state\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state = 0)\n\n# define Random Forest Classifier model\nmodel = RandomForestClassifier(n_estimators=32, random_state=0).fit(X_train, y_train)\n\n# fit permutation importance and show the results \nperm = PermutationImportance(model, random_state=1).fit(X_test, y_test)\neli5.show_weights(perm, feature_names = X_test.columns.tolist())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The only dataset columns (i.e. features) with significance (i.e. weight) in 4.18+ yard runs are running back acceleration and three wide receivers on the field, and even those two aren't that significant. This does not mean that the listed features have no meaning at all. What they tell us is that removing any of them does not affect the overall outcome of our model in any significant way, because their relative importance is so similar.\n\n**Thus, as an aspiring ML model, I don't learn anything about 4.18+ yard runs because the dataset cannot teach me anything.**\n\nThe graph below picks the most significant factor (running back acceleration) from the list above. As one can see, values from 2.5 yards/second up to 4 yds/s make longer runs more likely.\n\n*Then again, what kind of a football coach didn't already know that?*"},{"metadata":{"trusted":true},"cell_type":"code","source":"# define column used in graph\nfeature_name = 'acc_rb'\n\n# Create the data that we will plot\nmy_pdp = pdp.pdp_isolate(model=model, dataset=X_train, model_features=X_train.columns, feature=feature_name)\n\n# set the plot\npdp.pdp_plot(my_pdp, feature_name)\n\n# show plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<hr>"},{"metadata":{},"cell_type":"markdown","source":"### 5. Conclusion"},{"metadata":{},"cell_type":"markdown","source":"Concluding this notebook, sometimes the data at hand cannot offer answers to every single question. In this case it was clear from the beginning that long, 4.18+ yard runs most likely cannot be predicted with the dataset used. However in the beginning this was a mere coffee table hypothesis - now we have proved it.\n\nAnother thing to remember are the fundamental choices defined in the beginning. These results show no correlation between running back and linebackers, when 4.18+ runs are concerned. This does not mean that other factors in dataset could not provide us more information. In fact, we actually have been just given a new research question:\n\n<div class=\"h4\"><i>\nIf 4.18+ yard runs in football are not about the matchup between running backs and linebackers, then what are they all about?\n</i></div>\n<br>\nEarlier analysis showed that many run plays are so short the ball carrier never confronts linebackers in the first place. As a hypothesis, it could be that *the fate of longer runs lies on the matchup between offensive and defensive lines.* The ball carrier can break free only if his lead blocker - an offensive lineman - paves the road for him. Conversely, if a defensive lineman wins his own matchup against the opposing offensive lineman, the linebacker is then free to reach for the running back: at least this would be my starting point for further analysis.\n\nAs an aspiring ML model, I did not learn how to predict long runs in football. However, as someone only starting to learn the ropes of data analysis, I did discover new things that may prove to be useful in the future.\n\n***I am looking forward to facing Lusha's dart also in future data analysis matchups.***\n<hr>"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":4}