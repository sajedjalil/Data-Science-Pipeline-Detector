{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport os\nimport sys\nimport numpy as np\nimport pandas as pd\nimport datetime\nimport random\nimport time as tm\nimport gc\n\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport seaborn as sns\n\nimport tensorflow as tf\nimport keras\nfrom keras.utils import plot_model\nfrom keras import regularizers\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Dropout, Flatten, Embedding, Concatenate, Reshape, GlobalAveragePooling1D, Activation, BatchNormalization, Add\nfrom keras.initializers import glorot_normal\nfrom keras import backend as K\nfrom keras.callbacks import Callback, ModelCheckpoint\n\nimport category_encoders as ce\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, Imputer\n\nimport tqdm\nfrom tqdm import tqdm_notebook\n\nfrom kaggle.competitions import nflrush\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\npd.options.display.max_columns = 1000\npd.options.display.max_rows = 1000\npd.options.display.max_colwidth = -1\n\ndef seed_everything(seed=0):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\nRANDOM_STATE = 313\nseed_everything(seed=RANDOM_STATE)\n\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: \n        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = tm.time()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# choose mode\nprod = 1\ncommit = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if prod == 1:\n    train = pd.read_csv('../input/nfl-big-data-bowl-2020/train.csv', low_memory=False)\nelse:\n    train = pd.read_csv('../input/nfl-big-data-bowl-2020/train.csv', low_memory=False)\n    train = train[pd.to_datetime(train['TimeHandoff'], infer_datetime_format=True, utc=True) >= pd.to_datetime('2018-01-01', infer_datetime_format=True, utc=True)]\n    train = train.iloc[0:22*100]\n\nprint(train.shape)\ntrain = reduce_mem_usage(train, verbose=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Helper Functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def handle_windspeed(speed):\n    if pd.isna(speed) is True:\n        return speed\n    else:\n        speed_lower = speed.lower().strip()\n        try:\n            candidate_speed = int(speed_lower)\n            return candidate_speed\n        except Exception:\n            ss = speed.split(\" \")\n            sd = speed.split(\"-\")\n            if ss[-1] == \"mph\":\n                return int(ss[0])\n            elif ss[-1].isnumeric() is True:\n                return int(float(ss[0])*0.80 + float(ss[-1])*0.20)\n            elif sd[-1].isnumeric() is True:\n                return int(float(sd[0])/2 + float(sd[-1])/2) \n            elif speed.isalpha() is True:\n                return np.nan\n            elif \"mph\" in speed and len(speed) <= 5:\n                return int(speed.replace(\"mph\", \"\"))\n            \ndef handle_offence_personnel(x):\n    map_ = {'DB' : 0, 'DL' : 0, 'LB' : 0, 'OL' : 0, 'QB' : 0, 'RB' : 0, 'TE' : 0, 'WR' : 0}\n    for formation in x.strip().split(\",\"):\n        item = formation.strip().split(\" \")\n        map_.update({item[1]: int(item[0])})\n    return map_\n\ndef handle_defence_personnel(x):\n    map_ = {'OL': 0, 'LB': 0, 'DL': 0, 'DB': 0}\n    for formation in x.strip().split(\",\"):\n        item = formation.strip().split(\" \")\n        map_.update({item[1]: int(item[0])})\n    return map_\n\ndef feature_set(orig_features=None, new_features=None, operation=\"add\"):\n    if operation == \"add\":\n        return list(set(orig_features + new_features))\n    else:\n        return list(set(set(orig_features) - set(new_features)))\n\ndef handle_height(x):\n    split_ = x.split('-')\n    foot = int(split_[0])\n    inch = int(split_[-1])\n    return foot * 12 + inch\n\ndef handle_game_clock(x):\n    split_ = x.split(\":\")\n    minute = int(split_[0])\n    second = int(split_[1])\n    return np.around((minute * 60.0 + second)/60.0)\n\ndef tag_to_left(x):\n    if x == \"left\":\n        return True\n    else:\n        return False\n\ndef fix_team_abbr(x):\n    if x == \"ARI\":\n        return \"ARZ\"\n    elif x == \"BAL\":\n        return \"BLT\"\n    elif x == \"CLE\":\n        return \"CLV\"\n    elif x == \"HOU\":\n        return \"HST\"\n    else:\n        return x\n    \ndef tag_team_on_offence(row):\n    if row['PossessionTeam'] == row['HomeTeamAbbr']:\n        return 'home'\n    else:\n        return 'away'\n\ndef tag_yards_from_own_goal(row):\n    if row['FieldPosition'] == row['PossessionTeam']:\n        return row['YardLine']\n    else:\n        return 50 + (50 - row['YardLine'])\n\ndef tag_X_std(row):\n    if row['ToLeft'] is True:\n        return (120 - row['X']) - 10\n    else:\n        return row['X'] - 10\n\ndef tag_Y_std(row):\n    if row['ToLeft'] is True:\n        return 160/3 - row['Y']\n    else:\n        return row['Y']\n\ndef tag_dir_std_1(row):\n    if row[\"ToLeft\"] is True and row['Dir'] < 90:\n        return row['Dir'] + 360\n    else:\n        return row['Dir']\n\ndef tag_dir_to_right(row):\n    if row[\"ToLeft\"] is False and row[\"Dir\"] > 270:\n        return row[\"Dir\"] - 360\n    else:\n        return row[\"Dir_std_1\"]\n\ndef tag_dir_std_2(row):\n    if row[\"ToLeft\"] is True:\n        return row[\"Dir_std_1\"] - 180\n    else:\n        return row[\"Dir_std_1\"]\n    \ndef tag_orien_std_1(row):\n    if row[\"ToLeft\"] is True and row['Orientation'] < 90:\n        return row['Orientation'] + 360\n    else:\n        return row['Orientation']\n\ndef tag_orien_to_right(row):\n    if row[\"ToLeft\"] is False and row[\"Orientation\"] > 270:\n        return row[\"Orientation\"] - 360\n    else:\n        return row[\"Orien_std_1\"]\n\ndef tag_orien_std_2(row):\n    if row[\"ToLeft\"] is True:\n        return row[\"Orien_std_1\"] - 180\n    else:\n        return row[\"Orien_std_1\"]\n    \ndef generate_y(x):\n    step = np.zeros(199).astype(np.int8)\n    step[99 + x:] = 1\n    return step\n\ndef encode_OP_DP(df, op_dp_columns, column, prefix, mode_=\"train\"):\n    temp = df[column].iloc[np.arange(0, len(df), 22)].apply(lambda x: pd.Series(handle_offence_personnel(x)))\n    temp.columns = [prefix + c for c in temp.columns]\n    if mode_ == \"train\":\n        op_dp_columns += list(temp.columns)\n    temp['PlayId'] = df['PlayId'].iloc[np.arange(0, len(df), 22)]\n    df = df.merge(temp, on = \"PlayId\")\n    return df, op_dp_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compound_1_f(play):\n    ball_carrier = play.loc[play['IsBallCarrier'] ==1]\n    closeness_seq = np.sqrt(np.power(play.loc[ play['IsOnOffense'] == 0, 'X_std'] - ball_carrier['X_std'].item(), 2) + np.power(play.loc[ play['IsOnOffense'] == 0, 'Y_std'] - ball_carrier['Y_std'].item(), 2))\n    sorted_closeness_seq = closeness_seq.sort_values()\n    closest_opponent_1_id = sorted_closeness_seq[0:1].index.item()\n    closest_opponent_2_id = sorted_closeness_seq[1:2].index.item()\n    closest_opponent_1 = play.loc[closest_opponent_1_id]\n    closest_opponent_2 = play.loc[closest_opponent_2_id]\n        \n    dy_1 = np.abs(closest_opponent_1[\"Y_std\"].item() - ball_carrier[\"Y_std\"].item())\n    dy_2 = np.abs(closest_opponent_2[\"Y_std\"].item() - ball_carrier[\"Y_std\"].item())\n    ave_dy = (dy_1 + dy_2)/2\n    dx_1 = np.abs(closest_opponent_1[\"X_std\"].item() - ball_carrier[\"X_std\"].item())\n    dx_2 = np.abs(closest_opponent_2[\"X_std\"].item() - ball_carrier[\"X_std\"].item())\n    ave_dx = (dx_1 + dx_2)/2\n    angle_1 = np.arctan(dy_1/dx_1)\n    angle_2 = np.arctan(dy_2/dx_2) \n    delta_angle = np.abs(angle_1 - angle_2)\n    dist_opp_1 = np.sqrt(np.power(dy_1 ,2) + np.power(dx_1 ,2)) \n    dist_opp_2 = np.sqrt(np.power(dy_2 ,2) + np.power(dx_2 ,2))\n    dist_opp_ave = np.sqrt(np.power(ave_dy ,2) + np.power(ave_dx ,2))\n    area = 0.5 * np.sin(delta_angle) * dist_opp_1 * dist_opp_2\n    \n    is_bc_dir_reverse = 1 if ball_carrier[\"Dir_std_2\"].item() < 0 or ball_carrier[\"Dir_std_2\"].item() > 180 else 0\n    is_bc_or_reverse  = 1 if ball_carrier[\"Orien_std_2\"].item() < 0 or ball_carrier[\"Orien_std_2\"].item() > 180 else 0\n    diff_bc_dir_or = ball_carrier[\"Dir_std_2\"].item() - ball_carrier[\"Orien_std_2\"].item()\n\n    return area, dist_opp_1, dist_opp_2, dist_opp_ave, max(area, dist_opp_1, dist_opp_2, dist_opp_ave), min(area, dist_opp_1, dist_opp_2, dist_opp_ave), \\\n                is_bc_dir_reverse, is_bc_or_reverse, diff_bc_dir_or\n\ndef compound_2_f(play):\n    offensive = play.loc[play['IsOnOffense'] == 1]\n    defensive = play.loc[play['IsOnOffense'] == 0]\n    \n    mean_X_offensive = offensive['X_std'].mean()\n    mean_X_defensive = defensive['X_std'].mean()\n    \n    mean_Y_offensive = offensive['Y_std'].mean()\n    mean_Y_defensive = defensive['Y_std'].mean()\n    \n    mean_S_x_offensive = offensive['S_x'].mean()\n    mean_S_x_defensive = defensive['S_x'].mean()\n        \n    mean_S_y_offensive = offensive['S_y'].mean()\n    mean_S_y_defensive = defensive['S_y'].mean()\n    \n    mean_A_x_offensive = offensive['A_x'].mean()\n    mean_A_x_defensive = defensive['A_x'].mean()\n    \n    mean_A_y_offensive = offensive['A_y'].mean()\n    mean_A_y_defensive = defensive['A_y'].mean()\n    \n    dx = np.abs(mean_X_offensive - mean_X_defensive)\n    dy = np.abs(mean_Y_offensive - mean_Y_defensive)\n    dist_dx_dy = np.sqrt(np.power(dx ,2) + np.power(dy ,2)) \n    \n    return  mean_X_offensive, mean_X_defensive,     \\\n            mean_Y_offensive, mean_Y_defensive,     \\\n            mean_S_x_offensive, mean_S_x_defensive, \\\n            mean_S_y_offensive, mean_S_y_defensive, \\\n            mean_A_x_offensive, mean_A_x_defensive, \\\n            mean_A_y_offensive, mean_A_y_defensive, \\\n            dx, dy, dist_dx_dy\n\ndef handle_compound(df, column_1, column_2):\n    temp = df.groupby(['PlayId']).apply(lambda x: compound_1_f(x)).reset_index()\n    temp.columns= [\"PlayId\", column_1]\n    df = df.merge(temp, how=\"left\", on='PlayId')\n    del temp\n    gc.collect()\n    df['trig_area'] = df[column_1].apply(lambda x: x[0])\n    df['dist_opp_1'] = df[column_1].apply(lambda x: x[1])\n    df['dist_opp_2'] = df[column_1].apply(lambda x: x[2])\n    df['dist_opp_ave'] = df[column_1].apply(lambda x: x[3])\n    df['max_compound_1'] = df[column_1].apply(lambda x: x[4])\n    df['min_compound_1'] = df[column_1].apply(lambda x: x[5])\n\n    df['is_bc_dir_reverse'] = df[column_1].apply(lambda x: x[6])\n    df['is_bc_or_reverse'] = df[column_1].apply(lambda x: x[7])\n    df['diff_bc_dir_or'] = df[column_1].apply(lambda x: x[8])\n    \n    df.drop(column_1, axis=1, inplace=True)\n\n    temp = df.groupby(['PlayId']).apply(lambda x: compound_2_f(x)).reset_index()\n    temp.columns= [\"PlayId\", column_2]\n    df = df.merge(temp, how=\"left\", on='PlayId')\n    del temp\n    gc.collect()\n\n    df['mean_X_offensive'] = df[column_2].apply(lambda x: x[0])\n    df['mean_X_defensive'] = df[column_2].apply(lambda x: x[1])\n\n    df['mean_Y_offensive'] = df[column_2].apply(lambda x: x[2])\n    df['mean_Y_defensive'] = df[column_2].apply(lambda x: x[3])\n\n    df['mean_S_x_offensive'] = df[column_2].apply(lambda x: x[4])\n    df['mean_S_x_defensive'] = df[column_2].apply(lambda x: x[5])\n\n    df['mean_S_y_offensive'] = df[column_2].apply(lambda x: x[6])\n    df['mean_S_y_defensive'] = df[column_2].apply(lambda x: x[7])\n\n    df['mean_A_x_offensive'] = df[column_2].apply(lambda x: x[8])\n    df['mean_A_x_defensive'] = df[column_2].apply(lambda x: x[9])\n\n    df['mean_A_y_offensive'] = df[column_2].apply(lambda x: x[10])\n    df['mean_A_y_defensive'] = df[column_2].apply(lambda x: x[11])\n\n    df['field_dx'] = df[column_2].apply(lambda x: x[12])\n    df['field_dy'] = df[column_2].apply(lambda x: x[13])\n    df['field_dist'] = df[column_2].apply(lambda x: x[14])\n    df.drop(column_2, axis=1, inplace=True)\n    return df\n\ndef impute(df, fill_map, mode_=\"train\"):\n    if mode_ == \"train\":\n        for col in df.select_dtypes(include=\"number\").columns:\n            if col not in ['GameId', 'PlayId', 'NflId', 'NflIdRusher']:\n                fill_value = df[col].median()\n                fill_map.update({col: fill_value})\n                df[col].fillna(fill_value, inplace=True)\n\n        for col in df.select_dtypes(include=\"object\").columns:\n            fill_value = df[col].mode()[0]\n            fill_map.update({col: fill_value})\n            df[col].fillna(fill_value, inplace=True)\n    else:\n        df.fillna(fill_map, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"def major_preprocess(df, mode=\"train\"):\n    global poor_fill_rate, fill_map, op_dp_columns, play_cat_classes, player_cat_classes, le_play_cat, le_player_cat, ss, ss_compound\n\n    df.drop(poor_fill_rate, inplace=True, axis=1)    \n    impute(df, fill_map, mode_=mode)\n    \n    df, op_dp_columns = encode_OP_DP(df, op_dp_columns, 'OffensePersonnel', \"OP_\", mode_=mode)\n    df, op_dp_columns = encode_OP_DP(df, op_dp_columns, 'DefensePersonnel', \"DP_\", mode_=mode)\n    df.drop(['DefensePersonnel', 'OffensePersonnel'], axis=1, inplace=True)\n    \n    df['SnapHandoffDiff'] = ((pd.to_datetime(df['TimeHandoff'], infer_datetime_format=True, utc=True) - \\\n                     pd.to_datetime(df['TimeSnap'], infer_datetime_format=True, utc=True))).dt.seconds.astype(np.int8)\n\n    df['WindSpeed'] = df['WindSpeed'].astype(str).apply(handle_windspeed)\n\n    df['Turf'] = df['Turf'].map(Turf)\n\n    df['DiffHomeVisitor'] = df['HomeScoreBeforePlay'] - df['VisitorScoreBeforePlay']\n\n    df['PlayerHeight'] = df['PlayerHeight'].apply(lambda x: handle_height(x)).astype(np.int8)\n    \n    df['GameClock'] = 15 - df['GameClock'].apply(lambda x: handle_game_clock(x)).astype(np.int8)\n\n    df['HowTired'] = df['GameClock'] + (df['Quarter'] - 1) * 15\n\n    df['Age'] = ((pd.to_datetime(df['TimeHandoff'], infer_datetime_format=True, utc=True) - \\\n                     pd.to_datetime(df['PlayerBirthDate'], infer_datetime_format=True, utc=True)).dt.days/360).astype(np.int8)\n\n    df['ToLeft'] = df['PlayDirection'].apply(lambda x: tag_to_left(x)) # not a feature\n    \n    df['VisitorTeamAbbr'] = df['VisitorTeamAbbr'].apply(lambda x: fix_team_abbr(x))\n    \n    df['HomeTeamAbbr'] = df['HomeTeamAbbr'].apply(lambda x: fix_team_abbr(x))\n    \n    df['IsBallCarrier'] = (df['NflId'] == df['NflIdRusher'])\n\n    df['TeamOnOffense'] = df[['PossessionTeam', 'HomeTeamAbbr']].apply(func=tag_team_on_offence, axis=1, result_type='reduce') # not a feature\n    \n    df['IsOnOffense'] = (df['Team'] == df['TeamOnOffense'])\n\n    df['YardsFromOwnGoal'] = df.apply(func=tag_yards_from_own_goal, axis=1, result_type='reduce')\n\n    df['X_std'] = df.apply(func=tag_X_std, axis=1, result_type='reduce')\n    \n    df['Y_std'] = df.apply(func=tag_Y_std, axis=1, result_type='reduce')\n    \n    df['inv_X'] = 1/df['X_std']\n    \n    df['inv_X'] = df['inv_X'].replace([np.inf, -np.inf], [0, 0])\n    \n    df['inv_Y'] = 1/df['Y_std']\n    \n    df['inv_Y'] = df['inv_Y'].replace([np.inf, -np.inf], [0, 0])\n    \n    df['euc_dist'] = np.sqrt(np.power(df[\"X_std\"], 2) + np.power(df[\"Y_std\"], 2))\n\n    df[\"Dir_std_1\"] = df.apply(func=tag_dir_std_1, axis=1, result_type='reduce')\n    \n    df[\"Dir_std_1\"] = df.apply(func=tag_dir_to_right, axis=1, result_type='reduce')\n    \n    df[\"Dir_std_2\"] = df.apply(func=tag_dir_std_2, axis=1, result_type='reduce')\n\n    df[\"Orien_std_1\"] = df.apply(func=tag_orien_std_1, axis=1, result_type='reduce')\n    \n    df[\"Orien_std_1\"] = df.apply(func=tag_orien_to_right, axis=1, result_type='reduce')\n    \n    df[\"Orien_std_2\"] = df.apply(func=tag_orien_std_2, axis=1, result_type='reduce')\n\n    df['is_dir_reverse'] = df['Dir_std_2'].apply(lambda x: 1 if x < 0 or x > 180 else 0)\n    \n    df['is_or_reverse'] = df['Orien_std_2'].apply(lambda x: 1 if x < 0 or x > 180 else 0)\n    \n    df['diff_dir_or'] = df['Dir_std_2'] - df['Orien_std_2']\n\n    df['IsBallCarrier'] = df['IsBallCarrier'].apply(lambda x: 1 if x is True else 0)\n    \n    df['IsOnOffense'] = df['IsOnOffense'].apply(lambda x: 1 if x is True else 0)\n\n    dir_sin = df[\"Dir_std_2\"].apply(lambda x: np.sin(x * np.pi/180.0))\n    \n    dir_cos = df[\"Dir_std_2\"].apply(lambda x: np.cos(x * np.pi/180.0))\n\n    df['S_y'] = dir_sin * df['S']\n    \n    df['S_x'] = dir_cos * df['S']\n\n    df['A_y'] = dir_sin * df['A']\n    \n    df['A_x'] = dir_cos * df['A']\n    \n    df.fillna(-999, inplace=True)\n    \n    df.sort_values(['GameId', 'PlayId', 'IsOnOffense', 'IsBallCarrier', 'X_std'], ascending=[True, True, True, False, True], inplace=True)\n    \n    df = handle_compound(df, 'Compound_1', 'Compound_2')\n    \n    df[total_cat] = df[total_cat].astype(str)\n\n    for col in total_cat:\n        df[col] = col + \"_\" + df[col]\n\n    if mode == \"train\":\n        for col in play_cat:\n            play_cat_classes.append(df[col].unique().tolist())\n\n        for col in player_cat:\n            player_cat_classes.append(df[col].unique().tolist())\n\n        play_cat_classes = [item for l in play_cat_classes for item in l] + ['unknown_play_cat']\n        player_cat_classes = [item for l in player_cat_classes for item in l] + ['unknown_player_cat']\n\n        le_play_cat.fit(play_cat_classes)\n        le_player_cat.fit(player_cat_classes)\n\n        for col in play_cat:\n            df[col] = le_play_cat.transform(df[col])\n\n        for col in player_cat:\n            df[col] = le_player_cat.transform(df[col])\n    else:\n        for col in play_cat:\n            df[col] = df[col].apply(lambda x: le_play_cat.transform([x])[0] if x in le_play_cat.classes_ else le_play_cat.transform([\"unknown_play_cat\"])[0])\n\n        for col in player_cat:\n            df[col] = df[col].apply(lambda x: le_player_cat.transform([x])[0] if x in le_player_cat.classes_ else le_player_cat.transform([\"unknown_player_cat\"])[0])\n    \n    if mode == \"train\":\n        df[total_numeric] = ss.fit_transform(df[total_numeric])\n        df[compound] = ss_compound.fit_transform(df[compound])\n    else:\n        df[total_numeric] = ss.transform(df[total_numeric])\n        df[compound] = ss_compound.transform(df[compound])\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Input Data Parcels & Target"},{"metadata":{"trusted":true},"cell_type":"code","source":"def data_parcels(df, mode=\"train\"):\n    df_play_cat = df[play_cat]\n    df_player_cat = df[player_cat]\n    df_play_numeric = df[play_numeric]\n    df_player_numeric = df[player_numeric]\n    df_compound = df[compound]\n    \n    df_play_cat = df_play_cat.iloc[[i for i in range(0, len(df_play_cat), 22)]].reset_index(drop=True).values\n    df_play_numeric = df_play_numeric.iloc[[i for i in range(0, len(df_play_numeric), 22)]].reset_index(drop=True).values\n    df_player_cat = np.stack([df_player_cat.iloc[[i for i in range(j, len(df_player_cat), 22)]].reset_index(drop=True).values for j in range(22)]).transpose(1, 0, 2)\n    df_player_numeric = np.stack([df_player_numeric.iloc[[i for i in range(j, len(df_player_numeric), 22)]].reset_index(drop=True).values for j in range(22)]).transpose(1, 0, 2)\n    df_compound = df_compound.iloc[[i for i in range(0, len(df_compound), 22)]].reset_index(drop=True).values\n    \n    if mode == \"train\":\n        df_y = df[\"Yards\"]\n        df_y = df_y.iloc[[i for i in range(0, len(df_y), 22)]].reset_index(drop=True)\n        df_y = np.vstack(df_y.apply(lambda x: generate_y(x)).values)\n    else:\n        df_y = np.zeros(1)\n        \n    if mode == \"train\":\n        print([df_play_cat.shape, df_play_numeric.shape, df_player_cat.shape, df_player_numeric.shape, df_compound.shape, df_y.shape])\n\n    return df_play_cat, df_play_numeric, df_player_cat, df_player_numeric, df_compound, df_y","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Main"},{"metadata":{"trusted":true},"cell_type":"code","source":"poor_fill_rate = ['WindDirection', 'Temperature', 'GameWeather', 'StadiumType']\nfill_map = dict()\nop_dp_columns = list()\nplay_cat_classes = list()\nplayer_cat_classes = list()\nle_play_cat = LabelEncoder()\nle_player_cat = LabelEncoder()\nss = StandardScaler()\nss_compound = StandardScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Turf = {'Field Turf':0, 'A-Turf Titan':0, 'Grass':1, 'UBU Sports Speed S5-M':0, \n        'Artificial':0, 'DD GrassMaster':0, 'Natural Grass':1, 'UBU Speed Series-S5-M':0, \n        'FieldTurf':0, 'FieldTurf 360':0, 'Natural grass':1, 'grass':1, 'Natural':1, \n        'Artifical':0, 'FieldTurf360':0, 'Naturall Grass':1, 'Field turf':0, 'SISGrass':0, \n        'Twenty-Four/Seven Turf':0, 'natural grass':1}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"play_cat = ['Team', 'Season', 'PossessionTeam', 'FieldPosition', 'OffenseFormation', 'PlayDirection', 'HomeTeamAbbr', \n        'VisitorTeamAbbr', 'Week', 'Stadium', 'StadiumType', 'Location',  'GameWeather', 'WindDirection']\n\nplay_cat = feature_set(play_cat, poor_fill_rate, \"sub\")\n\nplay_numeric = ['GameClock', 'YardLine', 'Quarter', 'Down', 'Distance', 'HomeScoreBeforePlay', 'VisitorScoreBeforePlay', \n                'DefendersInTheBox', 'Temperature', 'Humidity', 'WindSpeed', 'Turf', 'SnapHandoffDiff', 'DiffHomeVisitor', \n                'IsOnOffense', 'YardsFromOwnGoal'] + op_dp_columns\n\nplay_numeric = feature_set(play_numeric, poor_fill_rate, \"sub\")\n\nplayer_cat = ['PlayerCollegeName', 'Position']\n\nplayer_numeric = ['S', 'A', 'Dis', 'PlayerHeight', 'PlayerWeight', 'S_y', 'S_x', 'A_y', 'A_x', 'Age', 'HowTired', \n                  'IsBallCarrier', 'X_std', 'Y_std', \"Dir_std_2\", \"Orien_std_2\", \"is_dir_reverse\", \"is_or_reverse\", \n                  \"diff_dir_or\", 'euc_dist', 'inv_X', 'inv_Y']\n\nids = ['GameId', 'PlayId', 'NflId', 'NflIdRusher', 'DisplayName', 'JerseyNumber']\n\ntime = ['TimeHandoff', 'TimeSnap', 'PlayerBirthDate']\n\ncompound_1 = ['trig_area', 'dist_opp_1', 'dist_opp_2', 'dist_opp_ave', 'max_compound_1', 'min_compound_1', \n              'is_bc_dir_reverse', 'is_bc_or_reverse', 'diff_bc_dir_or']\n\ncompound_2 = ['mean_X_offensive', 'mean_X_defensive', 'mean_Y_offensive', 'mean_Y_defensive', 'mean_S_x_offensive', \n              'mean_S_x_defensive', 'mean_S_y_offensive', 'mean_S_y_defensive', 'mean_A_x_offensive', 'mean_A_x_defensive', \n              'mean_A_y_offensive', 'mean_A_y_defensive', 'field_dx', 'field_dy', 'field_dist']\n\ntotal_cat = play_cat + player_cat\n\ntotal_numeric = play_numeric + player_numeric\n\ncompound = compound_1 + compound_2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.system(\"echo 'Preprocessing Start'\")\ntrain = major_preprocess(train, mode=\"train\")\ntrain_play_cat, train_play_numeric, train_player_cat, train_player_numeric, train_compound, train_y = data_parcels(train, mode=\"train\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"def crps(y_true, y_pred):\n    loss = K.mean((K.clip(K.cumsum(y_pred, axis=1), 0, 1) - y_true)**2)\n    return loss\n\nclass BaseLogger(Callback):\n    def __init__(self):\n        super(BaseLogger, self).__init__()\n    \n    def on_train_begin(self, logs={}):\n        pass\n    \n    def on_epoch_begin(self, epoch, logs={}):\n        pass\n    \n    def on_epoch_end(self, epoch, logs={}):\n        improve_in_train_crps = np.nan\n        improve_in_val_crps = np.nan\n        \n        try:\n            improve_in_train_crps = self.model.history.history['crps'][-2] - self.model.history.history['crps'][-1]\n            improve_in_val_crps = self.model.history.history['val_crps'][-2] - self.model.history.history['val_crps'][-1]\n        except Exception:\n            improve_in_train_crps = np.nan\n            improve_in_val_crps = np.nan\n        \n        print(f\"Model: -- train_crps: {logs['crps']:.6f}, -- val_crps: {logs['val_crps']:.6f}\")\n        print(f\"Improve: -- train_crps: {improve_in_train_crps:.6f}, -- val_crps: {improve_in_val_crps:.6f}\")\n    \n    def on_train_end(self, logs={}):\n        pass\n\ndef get_top_models(models, num=2):\n    val_scores = list()\n    for model in models:\n        val_score = np.mean(model.history.history['val_crps'][-3:])\n        val_scores.append(val_score)\n    \n    sorted_models = sorted(list(zip(val_scores, models)), key=lambda x: x[0], reverse=False)[0:num]\n    model_chosen = [m[1] for m in sorted_models]\n    return model_chosen\n\ndef print_scores(models):\n    train_crps = 0\n    val_crps = 0\n\n    for m in models:\n        train_crps += np.mean(m.history.history['crps'][-3:])\n        val_crps += np.mean(m.history.history['val_crps'][-3:])\n\n    mean_train_crps = train_crps/len(models)\n    mean_val_crps = val_crps/len(models)\n\n    print(\"train crps: {0}\".format(mean_train_crps))\n    print(\"val crps: {0}\".format(mean_val_crps))\n    \n    try:\n        os.system(\"echo 'train: {0}'\".format(mean_train_crps))\n        os.system(\"echo 'val: {0}'\".format(mean_val_crps))\n    except Exception:\n        pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.system(\"echo 'Modeling Start'\")\n\nKFolds = 5\nkf = KFold(n_splits=KFolds, random_state=RANDOM_STATE)\n\nmodels = list()\n\nfor train_idx, val_idx in kf.split(train_y):\n    X_train_play_cat, X_val_play_cat = train_play_cat[train_idx], train_play_cat[val_idx]\n    X_train_player_cat, X_val_player_cat = train_player_cat[train_idx], train_player_cat[val_idx]\n    X_train_play_numeric, X_val_play_numeric = train_play_numeric[train_idx], train_play_numeric[val_idx]\n\n    X_train_compound, X_val_compound = train_compound[train_idx], train_compound[val_idx]\n    X_train_player_numeric, X_val_player_numeric = train_player_numeric[train_idx], train_player_numeric[val_idx]\n    y_train, y_val = train_y[train_idx], train_y[val_idx]\n    \n    input_play_cat = Input(shape=(X_train_play_cat.shape[1],), name= \"input_play_cat\")\n    input_play_numeric = Input(shape=(X_train_play_numeric.shape[1],), name= \"input_play_numeric\")\n    input_compound = Input(shape=(X_train_compound.shape[1],), name= \"input_compound\")\n\n    input_player_cat = Input(shape=(X_train_player_cat.shape[1], X_train_player_cat.shape[2]), name= \"input_player_cat\")\n    input_player_numeric = Input(shape=(X_train_player_numeric.shape[1], X_train_player_numeric.shape[2]), name= \"input_player_numeric\")\n    # ^ injection point for GlobalAveragePooling input_player_numeric (2D)\n    rsh_player_numeric = Reshape((input_player_numeric.shape[1] * input_player_numeric.shape[2],), name= \"reshape_player_numeric\")(input_player_numeric)\n\n    embedding_play_cat = Embedding(len(le_play_cat.classes_) + 1, 8, embeddings_regularizer=regularizers.l2(1e-4), name=\"embedding_play_cat\")\n    embedding_player_cat = Embedding(len(le_player_cat.classes_) + 1, 12, embeddings_regularizer=regularizers.l2(1e-4), name=\"embedding_player_cat\")\n    \n    emb_play_cat = embedding_play_cat(input_play_cat)\n    \n    emb_player_cat = embedding_player_cat(input_player_cat)\n    emb_player_cat = Reshape((int(emb_player_cat.shape[1]), int(emb_player_cat.shape[2]) * int(emb_player_cat.shape[3])), name= \"reshape_emb_player_cat\")(emb_player_cat)\n    # ^ injection point for GlobalAveragePooling emb_player_cat (2D)\n\n    flat_emb_play_cat = Flatten(name= \"flat_emb_play_cat\")(emb_play_cat)\n    flat_emb_player_cat = Flatten(name= \"flat_rsh_emb_player_cat\")(emb_player_cat)\n    \n    # GlobalAveragePooling\n    concat_pooling_feat = Concatenate(name = \"player_features_GA\")([input_player_numeric, emb_player_cat])\n    global_averages = list()\n    pooling_units = int(64)\n    for d in range(3):\n        concat_pooling_feat = Dense(pooling_units, activation=None, kernel_initializer=glorot_normal(seed=RANDOM_STATE), name=f\"dense_GA_{d}\")(concat_pooling_feat)\n        global_averages.append(GlobalAveragePooling1D(name=f\"GA_{d}\")(concat_pooling_feat))\n        concat_pooling_feat = BatchNormalization(name=f\"batchN_GA_{d}\")(concat_pooling_feat)\n        concat_pooling_feat = Activation('relu')(concat_pooling_feat)\n        pooling_units =int(pooling_units/2)\n        \n    encoded_features = Concatenate(name = \"encoded_GA_features\")(global_averages)\n    encoded_features = BatchNormalization(name=\"batchN_GA_concat\")(encoded_features)\n    encoded_features = Activation('relu')(encoded_features)\n    encoded_features = Dropout(0.2)(encoded_features)\n    \n    residual_input = Concatenate(name = \"all_features\")([flat_emb_play_cat, flat_emb_player_cat, input_play_numeric, input_compound, rsh_player_numeric, encoded_features])\n    residual_input_skip = residual_input\n    \n    # Residual Network\n    for r in range(3):\n        # Full-Block\n        hidden_layer_fb = Dense(256, activation=None, kernel_initializer=glorot_normal(seed=RANDOM_STATE), name=f\"resnet_fb_{r}\")(residual_input)\n        hidden_layer_fb = BatchNormalization(name=f\"batchN_resnet_{r}\")(hidden_layer_fb)\n        hidden_layer_fb = Activation('relu')(hidden_layer_fb)\n        hidden_layer_fb = Dropout(0.2)(hidden_layer_fb)\n\n        # Block_1_of_2\n        hidden_layer_b_1_2 = Dense(128, activation=None, kernel_initializer=glorot_normal(seed=RANDOM_STATE), name=f\"resnet_b_{r}_1_2\")(hidden_layer_fb)\n        hidden_layer_b_1_2 = BatchNormalization()(hidden_layer_b_1_2)\n\n        # Skip Connection\n        hidden_layer_skip = Dense(128, activation=None, kernel_initializer=glorot_normal(seed=RANDOM_STATE), name=f\"skip_connection_{r}\")(residual_input_skip)\n        hidden_layer_skip = BatchNormalization(name=f\"batchN_skip_{r}\")(hidden_layer_skip)\n\n        # Block_2_of_2\n        hidden_layer_b_2_2 = Add()([hidden_layer_b_1_2, hidden_layer_skip])\n        hidden_layer_b_2_2 = Activation('relu')(hidden_layer_b_2_2)\n        hidden_layer_b_2_2 = Dropout(0.1)(hidden_layer_b_2_2)\n\n        residual_input = hidden_layer_b_2_2\n        residual_input_skip = hidden_layer_b_2_2\n\n    # output layer\n    out_ = Dense(199, activation=\"softmax\", name = \"softmax\")(hidden_layer_b_2_2)\n\n    model = Model(inputs=[input_play_cat, input_play_numeric, input_compound, input_player_cat, input_player_numeric], outputs=[out_])\n    model.compile(loss=[crps], optimizer=keras.optimizers.Adam(learning_rate=0.001), metrics=[crps])\n\n    earlyStop = keras.callbacks.callbacks.EarlyStopping(monitor='val_crps', patience=20, verbose=1, mode='min', restore_best_weights=True)\n    reduceLR = keras.callbacks.callbacks.ReduceLROnPlateau(monitor='val_crps', factor=0.7, patience=10, verbose=1, min_lr=0.00001, mode='min')\n    modelCheck = ModelCheckpoint('best_model.h5', monitor='val_crps', mode='min', save_best_only=True, verbose=1, save_weights_only=True)\n    \n    x_train = [X_train_play_cat, X_train_play_numeric, X_train_compound, X_train_player_cat, X_train_player_numeric]\n    y_train = y_train\n    \n    x_val = [X_val_play_cat, X_val_play_numeric, X_val_compound, X_val_player_cat, X_val_player_numeric]\n    y_val = y_val\n    \n    verbose = 1\n    if commit == 1:\n        verbose = 0\n        \n    model.fit(\n        x_train,\n        y_train,\n        batch_size=len(y_train),\n        epochs=400,\n        verbose=verbose,\n        validation_data=(x_val, y_val),\n        callbacks=[BaseLogger(), earlyStop, reduceLR, modelCheck],\n        use_multiprocessing=True\n    )\n    \n    model.load_weights(\"best_model.h5\")\n    \n    models.append(model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Scores, Model Summary and Visuals"},{"metadata":{"trusted":true},"cell_type":"code","source":"models = get_top_models(models, num=2)\nprint_scores(models)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if commit == 0:\n    print(models[0].summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_model(models[0], to_file='model.png')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction & Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"os.system(\"echo 'Prediction Start'\")\nenv = nflrush.make_env()\n\nfor (test_df, sample_prediction_df) in env.iter_test():\n    test_df = major_preprocess(test_df, mode=\"test\")\n    test_df_play_cat, test_df_play_numeric, test_df_player_cat, test_df_player_numeric, test_df_compound, dummy = data_parcels(test_df, mode=\"test\")\n\n    y_pred = 0\n    for m in models:\n        pred = m.predict([test_df_play_cat, test_df_play_numeric, test_df_compound, test_df_player_cat, test_df_player_numeric])\n        pred =  np.cumsum(pred)\n        y_pred += pred\n    y_pred /= len(models)\n    y_pred = np.clip(y_pred, 0, 1)\n    \n    env.predict(pd.DataFrame(data=[y_pred], columns=sample_prediction_df.columns))\n    \nenv.write_submission_file()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"finish = tm.time()\nprint(f\"Build took: {(finish - start)/60.0} minutes ... good bye!\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"nbformat":4,"nbformat_minor":1}