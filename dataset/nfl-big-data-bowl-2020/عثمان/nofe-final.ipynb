{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"RUN_KAGGLE = True\n\n''' PRODUCTION SETTINGS\n10SPLITS-KFOLD\n\n'''\n\nn_runs = 4 # n-1 will be kept.\nn_splits = 10 # Use 10 for final sub!!!!!!!\n\n# Cols we use to bound predictions:\nbounding_cols = ['rusherX','rusherSX','YardLine']\n\nMIN_CLIP = -9\nMAX_CLIP = 70\nNUM_PLAYERS_TRIM = 2 # how many distant players to trim off the field\n\nDO_LGBM = False\n\nN_OUTPUTS = MAX_CLIP - MIN_CLIP + 1\nN_OUTPUTS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import logging\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nimport sklearn.metrics as mtr\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom sklearn.utils import class_weight\nfrom keras.callbacks import Callback, EarlyStopping, ModelCheckpoint, LambdaCallback\nfrom keras.models import Model\nfrom keras.optimizers import Nadam\nfrom keras import losses\nfrom keras.layers import Input, Dense, Concatenate, Reshape, Dropout, merge, Add, BatchNormalization, Lambda, GaussianNoise, Layer\nfrom keras.layers.embeddings import Embedding\nfrom keras.models import load_model\nfrom tqdm import tqdm_notebook as tqdm\nfrom sklearn.model_selection import KFold,GroupKFold\nimport warnings, math, numba\nimport random as rn\nfrom time import time\n\nimport tensorflow as tf\nimport keras.backend as K\n\nfrom tensorflow.python.keras.optimizer_v2.optimizer_v2 import OptimizerV2\nfrom tensorflow.python import ops, math_ops, state_ops, control_flow_ops\n\nfrom scipy.spatial import Voronoi, ConvexHull, voronoi_plot_2d\n\nimport os, gc\nwarnings.filterwarnings(\"ignore\")\n\nimport seaborn as sns\nimport matplotlib.patches as patches\nimport matplotlib.pyplot as plt\n\n# from kaggle.competitions import nflrush\n# env = nflrush.make_env()\n# iter_test = env.iter_test()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if RUN_KAGGLE:\n    train = pd.read_csv('../input/nfl-big-data-bowl-2020/train.csv', parse_dates=['TimeHandoff','TimeSnap','PlayerBirthDate'], dtype={'WindSpeed': 'object'})\nelse:\n    train = pd.read_csv(\"../train.csv\",  low_memory=False, parse_dates=['TimeHandoff','TimeSnap','PlayerBirthDate'], dtype={'WindSpeed': 'object'})\n    test = pd.read_csv(\"../df_test.csv\", low_memory=False, parse_dates=['TimeHandoff','TimeSnap','PlayerBirthDate'], dtype={'WindSpeed': 'object'})","execution_count":null,"outputs":[]},{"metadata":{"heading_collapsed":true},"cell_type":"markdown","source":"# Optimizer"},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"class RAdam(OptimizerV2):\n    \"\"\"RAdam optimizer.\n    According to the paper\n    [On The Variance Of The Adaptive Learning Rate And Beyond](https://arxiv.org/pdf/1908.03265v1.pdf).\n    \"\"\"\n\n    def __init__(self,\n                 learning_rate=0.001,\n                 beta_1=0.9,\n                 beta_2=0.999,\n                 epsilon=1e-7,\n                 weight_decay=0.,\n                 amsgrad=False,\n                 total_steps=0,\n                 warmup_proportion=0.1,\n                 min_lr=0.,\n                 name='RAdam',\n                 **kwargs):\n        r\"\"\"Construct a new Adam optimizer.\n        Args:\n            learning_rate: A Tensor or a floating point value.    The learning rate.\n            beta_1: A float value or a constant float tensor. The exponential decay\n                rate for the 1st moment estimates.\n            beta_2: A float value or a constant float tensor. The exponential decay\n                rate for the 2nd moment estimates.\n            epsilon: A small constant for numerical stability. This epsilon is\n                \"epsilon hat\" in the Kingma and Ba paper (in the formula just before\n                Section 2.1), not the epsilon in Algorithm 1 of the paper.\n            weight_decay: A floating point value. Weight decay for each param.\n            amsgrad: boolean. Whether to apply AMSGrad variant of this algorithm from\n                the paper \"On the Convergence of Adam and beyond\".\n            total_steps: An integer. Total number of training steps.\n                Enable warmup by setting a positive value.\n            warmup_proportion: A floating point value. The proportion of increasing steps.\n            min_lr: A floating point value. Minimum learning rate after warmup.\n            name: Optional name for the operations created when applying gradients.\n                Defaults to \"Adam\".    @compatibility(eager) When eager execution is\n                enabled, `learning_rate`, `beta_1`, `beta_2`, and `epsilon` can each be\n                a callable that takes no arguments and returns the actual value to use.\n                This can be useful for changing these values across different\n                invocations of optimizer functions. @end_compatibility\n            **kwargs: keyword arguments. Allowed to be {`clipnorm`, `clipvalue`, `lr`,\n                `decay`}. `clipnorm` is clip gradients by norm; `clipvalue` is clip\n                gradients by value, `decay` is included for backward compatibility to\n                allow time inverse decay of learning rate. `lr` is included for backward\n                compatibility, recommended to use `learning_rate` instead.\n        \"\"\"\n\n        super(RAdam, self).__init__(name, **kwargs)\n        self._set_hyper('learning_rate', kwargs.get('lr', learning_rate))\n        self._set_hyper('beta_1', beta_1)\n        self._set_hyper('beta_2', beta_2)\n        self._set_hyper('decay', self._initial_decay)\n        self._set_hyper('weight_decay', weight_decay)\n        self._set_hyper('total_steps', float(total_steps))\n        self._set_hyper('warmup_proportion', warmup_proportion)\n        self._set_hyper('min_lr', min_lr)\n        self.epsilon = epsilon or K.epsilon()\n        self.amsgrad = amsgrad\n        self._initial_weight_decay = weight_decay\n        self._initial_total_steps = total_steps\n\n    def _create_slots(self, var_list):\n        for var in var_list:\n            self.add_slot(var, 'm')\n        for var in var_list:\n            self.add_slot(var, 'v')\n        if self.amsgrad:\n            for var in var_list:\n                self.add_slot(var, 'vhat')\n\n    def set_weights(self, weights):\n        params = self.weights\n        num_vars = int((len(params) - 1) / 2)\n        if len(weights) == 3 * num_vars + 1:\n            weights = weights[:len(params)]\n        super(RAdam, self).set_weights(weights)\n\n    def _resource_apply_dense(self, grad, var):\n        var_dtype = var.dtype.base_dtype\n        lr_t = self._decayed_lr(var_dtype)\n        m = self.get_slot(var, 'm')\n        v = self.get_slot(var, 'v')\n        beta_1_t = self._get_hyper('beta_1', var_dtype)\n        beta_2_t = self._get_hyper('beta_2', var_dtype)\n        epsilon_t = ops.convert_to_tensor(self.epsilon, var_dtype)\n        local_step = math_ops.cast(self.iterations + 1, var_dtype)\n        beta_1_power = math_ops.pow(beta_1_t, local_step)\n        beta_2_power = math_ops.pow(beta_2_t, local_step)\n\n        if self._initial_total_steps > 0:\n            total_steps = self._get_hyper('total_steps', var_dtype)\n            warmup_steps = total_steps * self._get_hyper('warmup_proportion', var_dtype)\n            min_lr = self._get_hyper('min_lr', var_dtype)\n            decay_steps = K.maximum(total_steps - warmup_steps, 1)\n            decay_rate = (min_lr - lr_t) / decay_steps\n            lr_t = tf.where(\n                local_step <= warmup_steps,\n                lr_t * (local_step / warmup_steps),\n                lr_t + decay_rate * K.minimum(local_step - warmup_steps, decay_steps),\n            )\n\n        sma_inf = 2.0 / (1.0 - beta_2_t) - 1.0\n        sma_t = sma_inf - 2.0 * local_step * beta_2_power / (1.0 - beta_2_power)\n\n        m_t = state_ops.assign(m,\n                               beta_1_t * m + (1.0 - beta_1_t) * grad,\n                               use_locking=self._use_locking)\n        m_corr_t = m_t / (1.0 - beta_1_power)\n\n        v_t = state_ops.assign(v,\n                               beta_2_t * v + (1.0 - beta_2_t) * math_ops.square(grad),\n                               use_locking=self._use_locking)\n        if self.amsgrad:\n            vhat = self.get_slot(var, 'vhat')\n            vhat_t = state_ops.assign(vhat,\n                                      math_ops.maximum(vhat, v_t),\n                                      use_locking=self._use_locking)\n            v_corr_t = math_ops.sqrt(vhat_t / (1.0 - beta_2_power))\n        else:\n            vhat_t = None\n            v_corr_t = math_ops.sqrt(v_t / (1.0 - beta_2_power))\n\n        r_t = math_ops.sqrt((sma_t - 4.0) / (sma_inf - 4.0) *\n                            (sma_t - 2.0) / (sma_inf - 2.0) *\n                            sma_inf / sma_t)\n\n        var_t = tf.where(sma_t >= 5.0, r_t * m_corr_t / (v_corr_t + epsilon_t), m_corr_t)\n\n        if self._initial_weight_decay > 0.0:\n            var_t += self._get_hyper('weight_decay', var_dtype) * var\n\n        var_update = state_ops.assign_sub(var,\n                                          lr_t * var_t,\n                                          use_locking=self._use_locking)\n\n        updates = [var_update, m_t, v_t]\n        if self.amsgrad:\n            updates.append(vhat_t)\n        return control_flow_ops.group(*updates)\n\n    def _resource_apply_sparse(self, grad, var, indices):\n        var_dtype = var.dtype.base_dtype\n        lr_t = self._decayed_lr(var_dtype)\n        beta_1_t = self._get_hyper('beta_1', var_dtype)\n        beta_2_t = self._get_hyper('beta_2', var_dtype)\n        epsilon_t = ops.convert_to_tensor(self.epsilon, var_dtype)\n        local_step = math_ops.cast(self.iterations + 1, var_dtype)\n        beta_1_power = math_ops.pow(beta_1_t, local_step)\n        beta_2_power = math_ops.pow(beta_2_t, local_step)\n\n        if self._initial_total_steps > 0:\n            total_steps = self._get_hyper('total_steps', var_dtype)\n            warmup_steps = total_steps * self._get_hyper('warmup_proportion', var_dtype)\n            min_lr = self._get_hyper('min_lr', var_dtype)\n            decay_steps = K.maximum(total_steps - warmup_steps, 1)\n            decay_rate = (min_lr - lr_t) / decay_steps\n            lr_t = tf.where(\n                local_step <= warmup_steps,\n                lr_t * (local_step / warmup_steps),\n                lr_t + decay_rate * K.minimum(local_step - warmup_steps, decay_steps),\n            )\n\n        sma_inf = 2.0 / (1.0 - beta_2_t) - 1.0\n        sma_t = sma_inf - 2.0 * local_step * beta_2_power / (1.0 - beta_2_power)\n\n        m = self.get_slot(var, 'm')\n        m_scaled_g_values = grad * (1 - beta_1_t)\n        m_t = state_ops.assign(m, m * beta_1_t, use_locking=self._use_locking)\n        with ops.control_dependencies([m_t]):\n            m_t = self._resource_scatter_add(m, indices, m_scaled_g_values)\n        m_corr_t = m_t / (1.0 - beta_1_power)\n\n        v = self.get_slot(var, 'v')\n        v_scaled_g_values = (grad * grad) * (1 - beta_2_t)\n        v_t = state_ops.assign(v, v * beta_2_t, use_locking=self._use_locking)\n        with ops.control_dependencies([v_t]):\n            v_t = self._resource_scatter_add(v, indices, v_scaled_g_values)\n\n        if self.amsgrad:\n            vhat = self.get_slot(var, 'vhat')\n            vhat_t = state_ops.assign(vhat,\n                                      math_ops.maximum(vhat, v_t),\n                                      use_locking=self._use_locking)\n            v_corr_t = math_ops.sqrt(vhat_t / (1.0 - beta_2_power))\n        else:\n            vhat_t = None\n            v_corr_t = math_ops.sqrt(v_t / (1.0 - beta_2_power))\n\n        r_t = math_ops.sqrt((sma_t - 4.0) / (sma_inf - 4.0) *\n                            (sma_t - 2.0) / (sma_inf - 2.0) *\n                            sma_inf / sma_t)\n\n        var_t = tf.where(sma_t >= 5.0, r_t * m_corr_t / (v_corr_t + epsilon_t), m_corr_t)\n\n        if self._initial_weight_decay > 0.0:\n            var_t += self._get_hyper('weight_decay', var_dtype) * var\n\n        var_update = self._resource_scatter_add(var, indices, tf.gather(-lr_t * var_t, indices))\n\n        updates = [var_update, m_t, v_t]\n        if self.amsgrad:\n            updates.append(vhat_t)\n        return control_flow_ops.group(*updates)\n\n    def get_config(self):\n        config = super(RAdam, self).get_config()\n        config.update({\n            'learning_rate': self._serialize_hyperparameter('learning_rate'),\n            'beta_1': self._serialize_hyperparameter('beta_1'),\n            'beta_2': self._serialize_hyperparameter('beta_2'),\n            'decay': self._serialize_hyperparameter('decay'),\n            'weight_decay': self._serialize_hyperparameter('weight_decay'),\n            'epsilon': self.epsilon,\n            'amsgrad': self.amsgrad,\n            'total_steps': self._serialize_hyperparameter('total_steps'),\n            'warmup_proportion': self._serialize_hyperparameter('warmup_proportion'),\n            'min_lr': self._serialize_hyperparameter('min_lr'),\n        })\n        return config","execution_count":null,"outputs":[]},{"metadata":{"heading_collapsed":true},"cell_type":"markdown","source":"# Eval Funcs"},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"def build_pidf(pis):\n    if len(pis)==0:\n        return None\n    \n    keys = list(pis[0].keys())\n    pidf = pd.DataFrame({\n        'cols':keys,\n        'means':[np.nanmean([pi[key] for pi in pis]) for key in keys],\n        'medians':[np.nanmedian([pi[key] for pi in pis]) for key in keys],\n        'stds':[np.nanstd([pi[key] for pi in pis]) for key in keys]\n    })\n    pidf['std_mean_ratio'] = np.round(pidf.stds / pidf.means * 100, 2)\n    pidf.sort_values(['means', 'std_mean_ratio'], inplace=True, ascending=False)\n    return pidf","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"def plot_feature_importance(perms, cutoff=None):\n    pidf_cvs = pd.DataFrame({\n        'cols': sum([list(fold.keys()) for fold in perms], []),\n        'vals': sum([list(fold.values()) for fold in perms], [])\n    }).sort_values('vals', ascending=False)\n    \n    if cutoff is not None:\n        cutoff_df = pidf_cvs.groupby('cols').vals.mean().sort_values().reset_index()\n        pidf_cvs = pidf_cvs[pidf_cvs.cols.isin(cutoff_df.iloc[:-cutoff].cols)]\n        cutoff_df.rename(columns={'vals':'sortorder'}, inplace=True)\n        pidf_cvs = pidf_cvs.merge(cutoff_df, how='left', on='cols')\n        pidf_cvs.sort_values('sortorder', ascending=False)\n        \n    fig, ax = plt.subplots(figsize=(8, 8))\n    #pidf_cvs.plot.barh(ax=ax)\n    #fig.show()\n    sns.barplot(y=\"cols\", x=\"vals\", data=pidf_cvs)","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"@numba.jit(numba.f8[:](numba.f8[:]))\ndef norm(x):\n    return x / x.sum(axis=1).reshape(-1,1)\n    \n@numba.jit(numba.f8[:](numba.f8[:]))\ndef cumsum(x):\n    return np.clip(np.cumsum(x, axis=1), 0, 1)\n\n@numba.jit(numba.f8[:](numba.f8[:], numba.f8[:]))\ndef crps(y_pred, y_true):\n    return ((y_true - y_pred) ** 2).sum(axis=1).sum(axis=0) / (y_true.shape[1] * y_true.shape[0]) \n\n@numba.jit\ndef euclidean_distance(x1,y1,x2,y2):\n    x_diff = x1-x2\n    y_diff = y1-y2\n    return math.sqrt(x_diff*x_diff + y_diff*y_diff)\n    \n@numba.jit\ndef euclidean_flat(p1,p2):\n    x_diff = p2[:,0]-p1[:,0]\n    y_diff = p2[:,1]-p1[:,1]\n    return np.sqrt(x_diff*x_diff + y_diff*y_diff)","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"# TODO: Note - looks like this one is returning something different from the one in show_voroni!!!\ndef calcvs(play):\n    xy = play[['X', 'Y']].values\n    rusher = play.IsRusher.values\n    vor = Voronoi(xy)\n\n    # PROBLEMATIC - doesnt seem like we're getting the right index always.\n    rx_idx = np.argmax(rusher)\n    region = vor.regions[vor.point_region[rx_idx]]\n    rx_poly = np.array([vor.vertices[i] for i in region])\n\n    try:\n        ch = ConvexHull(rx_poly)\n        return ch.area\n    except:\n        return np.nan\n        \ndef vfeats(df):\n    # We use this to cut the area behind the rusher and so we can select his area\n    # Since he's usually the guy on the far left:\n    rusher = df[df.IsRusher] #it's already a .copy()\n    rusher.IsRusher = False\n    rusher.X -= 1; df = df.append(rusher.copy(), sort=False)\n    rusher.X += 99; df = df.append(rusher.copy(), sort=False)\n    rusher.X -= 99\n    rusher.Y -= 15; df = df.append(rusher.copy(), sort=False)\n    rusher.Y += 30; df = df.append(rusher.copy(), sort=False)\n\n    results = df.groupby('PlayId').apply(calcvs).reset_index()\n    return results\n\ndef PlayerInfluence(grp):\n    ball_coords = grp.ball_coords.values[0]\n    offp_coords = grp.offp_coords.values[0]\n    defp_coords = grp.defp_coords.values[0]\n    \n    MAXV = 16\n    MAXV_log1p = np.log1p(MAXV)\n    \n    results = []\n    for x in range(int(ball_coords[0]+1), int(ball_coords[0]+1+MAXV)):\n        point = ball_coords.copy()\n        point[0] = x\n        point = np.repeat(point.reshape(1,-1), defp_coords.shape[0], axis=0)\n        \n        # Calculate distance from each offender/defender to the points on front of the ball/runner (+x)\n        offp_point_dist = euclidean_flat(offp_coords, point).clip(0,MAXV)\n        defp_point_dist = euclidean_flat(defp_coords, point).clip(0,MAXV)\n        \n        # Compute influence\n        offp_point_inf = MAXV_log1p - np.log1p(offp_point_dist)\n        defp_point_inf = MAXV_log1p - np.log1p(defp_point_dist)\n\n        results.append(\n            offp_point_inf.sum() - defp_point_inf.sum()\n        )\n        \n    return results\n\ndef NearestEnemy(subset):\n    mycoords = subset.XY\n    defp_coords = subset.defp_coords\n\n    mycoords = np.repeat(mycoords.reshape(1,-1), defp_coords.shape[0], axis=0)\n    return euclidean_flat(defp_coords, mycoords).min()\n    ","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"# author : nlgn\n# Link : https://www.kaggle.com/kingychiu/keras-nn-starter-crps-early-stopping\nclass Metric(Callback):\n    def __init__(self, model, callbacks, data):\n        super().__init__()\n        self.model = model\n        self.callbacks = callbacks\n        self.data = data\n        self.epochnum = 0\n\n    def on_train_begin(self, logs=None):\n        self.epochnum = 0\n        for callback in self.callbacks:\n            callback.on_train_begin(logs)\n\n    def on_train_end(self, logs=None):\n        for callback in self.callbacks:\n            callback.on_train_end(logs)\n\n    def set_original_data(self, orig_train, orig_valid):\n        global bounding_cols\n        self.orig_train = orig_train[bounding_cols].copy()\n        self.orig_valid = orig_valid[bounding_cols].copy()\n        \n    def bound_prediction(self, y_pred, non_scaled_data, ACCEPTABLE_DIST_FROM_ARGMAX=30):\n        \"\"\"\n        Zero out values we cannot get to\n\n        Look for the Yard Line Value corresponding to the position we believe\n        we will not go behind\n        \"\"\"\n        \n        global N_OUTPUTS\n        \n        # Zero out any y_preds less than this value\n        indices = np.repeat(np.arange(N_OUTPUTS).reshape(1,N_OUTPUTS), y_pred.shape[0], axis=0)\n\n        # NOTE: `rusher_dist_scrimmage = YardLine - rusher_X`\n        \n        # We cannot lose more than YardLine yards. Furthermore, we can't lose more\n        # than 3 yards less than where the rusher is currently positioned.\n        rusherX = non_scaled_data.rusherX.astype(int).values\n        YardLine = non_scaled_data.YardLine.values\n        Slippage = (non_scaled_data.rusherSX.values > 0).astype(int)\n\n        # We can lose up to our rusher position yards and 3 extra\n        # However if rusher is currently moving in the +x direction\n        # Then that 3 extra turns into 2\n        # We also bound to the stadium (e.g. 1 yard line)\n        MaxLossYardLine = np.maximum(0, rusherX - 3 + Slippage)\n\n        # Convert MaxLossYardLine to frame of reference of the actual YardLine\n        MaxLossYards = (MaxLossYardLine - YardLine - MIN_CLIP).reshape(-1,1)\n        y_pred[indices < MaxLossYards] = 0\n        \n\n        # We cannot gain more yards than 100-YardLine, e.g. dist_to_td\n        MaxGainYards = (100 - YardLine - MIN_CLIP).reshape(-1,1)\n        y_pred[indices > MaxGainYards] = 0\n        \n        # TODO: Revisit:\n        # Any value > ACCEPTABLE_DIST_FROM_ARGMAX units from our argmax is zero'd\n        # maxes = np.argmax(y_pred, axis=1)\n        #y_pred[indices > (maxes+ACCEPTABLE_DIST_FROM_ARGMAX).reshape(-1,1)] = 0\n        #y_pred[indices < (maxes-ACCEPTABLE_DIST_FROM_ARGMAX).reshape(-1,1)] = 0\n        # NOTE: We don't have to use amax..... we can use mu, which is one of our predicted values...\n        \n        # TODO: Expderiment doing this @ convert stage\n        return norm(y_pred)\n        #return y_pred#norm(y_pred)\n        \n    def convert_to_y199_cumsum(self, y_pred):\n        output = np.zeros((y_pred.shape[0], 199))\n        output[:,99+MIN_CLIP:99+MAX_CLIP+1] = y_pred\n        \n        return cumsum(output)\n\n    def on_epoch_end(self, batch, logs=None):\n        self.epochnum += 1\n        X_train, y_train = self.data[0][0], self.data[0][1]['ent']\n        y_pred = self.model.predict(X_train, batch_size=1024)[0]\n        y_pred = self.bound_prediction(y_pred, self.orig_train)\n        y_pred = self.convert_to_y199_cumsum(y_pred)\n        y_train = self.convert_to_y199_cumsum(y_train)\n        tr_s = crps(y_pred, y_train)\n        tr_s = np.round(tr_s, 6)\n        logs['tr_CRPS'] = tr_s\n\n        \n        X_valid, y_valid = self.data[1][0], self.data[1][1]['ent']\n        y_pred = self.model.predict(X_valid, batch_size=1024)[0]\n        y_pred = self.bound_prediction(y_pred, self.orig_valid)\n        y_pred = self.convert_to_y199_cumsum(y_pred)\n        y_valid = self.convert_to_y199_cumsum(y_valid)\n        val_s = crps(y_pred, y_valid)\n        val_s = np.round(val_s, 6)\n        logs['val_CRPS'] = val_s\n        \n        print(f'{self.epochnum}\\ttCRPS: {tr_s}\\tvCRPS: {val_s}')\n\n        for callback in self.callbacks:\n            callback.on_epoch_end(batch, logs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"def fast_preprocess(df, isTrain=False):\n    global MIN_CLIP,MAX_CLIP\n    seconds_in_year = 60*60*24*365.25\n\n    t = time()\n    \n    #####################################################################################\n    # Clean Data:\n    \n    dirs = {'ARI':'ARZ', 'BAL':'BLT', 'CLE':'CLV', 'HOU':'HST'}\n    for bad,good in dirs.items():\n        df.loc[df.VisitorTeamAbbr==bad, 'VisitorTeamAbbr'] = good\n        df.loc[df.HomeTeamAbbr==bad, 'HomeTeamAbbr'] = good\n        \n    df['IsToLeft'] = df.PlayDirection == 'left'\n    df['IsRusher'] = df.NflId == df.NflIdRusher\n    \n    df['TeamOnOffense'] = \"home\"\n    df.loc[df.PossessionTeam != df.HomeTeamAbbr, 'TeamOnOffense'] = 'away'\n    df['IsOnOffense'] = df.Team == df.TeamOnOffense\n    del df['TeamOnOffense']\n    \n    # Used in some downstream calcs\n    df.FieldPosition.replace({np.nan:''}, inplace=True)\n    mask = df.FieldPosition == df.PossessionTeam\n    df['YardLine_std'] = 110 - df.YardLine\n    df.loc[mask, 'YardLine_std'] = 10 + df.loc[mask,'YardLine']\n    df.YardLine = df.YardLine_std - 10\n    del df['YardLine_std']\n    \n    df['X_std'] = df.X\n    df['Y_std'] = df.Y\n    df.loc[df.IsToLeft, 'X_std'] = 120 - df.loc[df.IsToLeft, 'X']\n    df.loc[df.IsToLeft, 'Y_std'] = 160/3 - df.loc[df.IsToLeft, 'Y'] \n    df.X = df.X_std - 10\n    df.Y = df.Y_std\n    del df['X_std'], df['Y_std']\n\n    # ROTATE THIS 90 to the right\n    df.loc[df.IsToLeft, 'Dir'] = np.mod(180 + df[df.IsToLeft].Dir, 360)\n    df.Dir = np.mod(df.Dir/180*math.pi + 3*math.pi/2, math.pi*2)\n\n    # TODO: Either do this or keep season as a feature:\n    # https://www.kaggle.com/c/nfl-big-data-bowl-2020/discussion/113277#latest-662092\n    df.loc[df.Season==2019,'Season'] = 2018\n    mask = df.Season == 2017\n    df.loc[mask, 'Orientation'] = np.mod(math.pi/2 + df.loc[mask].Orientation, math.pi*2)\n    \n    # Correct 2017 Distributions for A and S separately...\n    rusher2017_A = [2.54, 0.8112192479993628, 1.0391018448524394, 2.7]\n    rusher2017_S = [3.84, 1.074601007663326, 1.1184611205650106, 4.54]\n    no_rusher2017_A = [1.43, 0.8586214845778048, 1.034906128189814, 1.56]\n    no_rusher2017_S = [2.24, 1.258396412289438, 1.412108400649438, 2.54]\n\n    df.loc[mask & df.IsRusher, 'S'] = (df.S[mask & df.IsRusher] - rusher2017_S[0]) / rusher2017_S[1] * rusher2017_S[2] + rusher2017_S[3]\n    df.loc[mask & df.IsRusher, 'A'] = (df.A[mask & df.IsRusher] - rusher2017_A[0]) / rusher2017_A[1] * rusher2017_A[2] + rusher2017_A[3]    \n    df.loc[mask & ~df.IsRusher, 'S'] = (df.S[mask & ~df.IsRusher] - no_rusher2017_S[0]) / no_rusher2017_S[1] * no_rusher2017_S[2] + no_rusher2017_S[3]\n    df.loc[mask & ~df.IsRusher, 'A'] = (df.A[mask & ~df.IsRusher] - no_rusher2017_A[0]) / no_rusher2017_A[1] * no_rusher2017_A[2] + no_rusher2017_A[3]\n    \n    df['dist_next_point_time'] = np.square(df.S) + 2 * df.A * df.Dis\n    df.dist_next_point_time = np.log(10+df.dist_next_point_time)\n    \n    \n    #####################################################################################\n    # Rusher Features:\n    \n    features = [\n        'GameId','PlayId','X','Y','Dir',\n        'YardLine','Season','S','A','Dis',\n        'dist_next_point_time',\n        \n    ]\n    if isTrain: features += ['Yards']\n    rushers = df[df.IsRusher][features].copy()\n    \n    # Per dset, these should be stable. I'm not worried:\n    rushers.Dir.replace({np.nan: rushers.Dir.mean()}, inplace=True)\n    rushers.A.replace({np.nan: rushers.A.mean()}, inplace=True)\n    rushers.S.replace({np.nan: rushers.S.mean()}, inplace=True)\n    rushers.Dis.replace({np.nan: rushers.Dis.mean()}, inplace=True)\n    \n    rushers.rename(columns={\n        'X':'rusherX',\n        'Y':'rusherY',\n    }, inplace=True)\n    \n    rushers['rusherSX'] = rushers.S * np.cos(rushers.Dir)\n    rushers['rusherSY'] = rushers.S * np.sin(rushers.Dir)\n    rushers['dist_yardline'] = rushers.YardLine - rushers.rusherX\n    rushers.A = np.log1p(rushers.A)\n    rushers.Dis = np.log1p(rushers.Dis) \n\n    if isTrain: \n        rushers['Orig_Yards_NOTRAIN'] = rushers.Yards.copy()\n        rushers.Yards = rushers.Yards.clip(MIN_CLIP,MAX_CLIP)\n    \n    \n    # Verify rusher feats...\n    rushercols = [\n        # Very Important\n        'A', 'S', 'rusherSX', 'dist_yardline', 'Season',\n\n        # Testing\n        # ?\n        \n        # Hurts Me\n        #'rusherX','Dis','Dir',\n        \n\n        # Removing seems to hurt marginally\n        'dist_next_point_time', 'YardLine', 'rusherY',\n    ]\n    \n    #####################################################################################\n    # Player features\n    \n    \n    # Influence (Distance) Features:\n    df['XY'] = df[['X','Y']].apply(lambda x: x.tolist(), axis=1)\n    \n    offp_coords = df[df.IsOnOffense==True][['PlayId','XY']].groupby('PlayId').agg(list).reset_index()\n    defp_coords = df[df.IsOnOffense==False][['PlayId','XY']].groupby('PlayId').agg(list).reset_index()\n    ball_coords = df[df.IsRusher==True][['PlayId','XY','A']].groupby(['PlayId','A']).agg(list).reset_index()\n\n    offp_coords.columns = ['PlayId', 'offp_coords']\n    defp_coords.columns = ['PlayId', 'defp_coords']\n    ball_coords.columns = ['PlayId', 'A', 'ball_coords']\n\n    ball_coords.ball_coords = ball_coords.ball_coords.apply(lambda x: np.array(x[0]))\n    defp_coords.defp_coords = defp_coords.defp_coords.apply(np.array)\n    offp_coords.offp_coords = offp_coords.offp_coords.apply(np.array)\n\n    ball_coords = ball_coords.merge(defp_coords, how='left', on='PlayId')\n    ball_coords = ball_coords.merge(offp_coords, how='left', on='PlayId')\n\n    InfluenceRusherX = ball_coords.groupby('PlayId').apply(PlayerInfluence).reset_index()\n    InfluenceRusherX.columns = ['PlayId', 'InfluenceRusherX']\n    InfluenceRusherX['InfluenceRusherX_flip'] = InfluenceRusherX.InfluenceRusherX.apply(lambda x: np.argmax(-np.sign(x)))\n    rushers = rushers.merge(InfluenceRusherX[['PlayId','InfluenceRusherX_flip']], how='left', on='PlayId')\n    \n    del ball_coords, InfluenceRusherX; gc.collect()\n    rushercols.append('InfluenceRusherX_flip')\n    \n    \n    ### TODO:\n    rushers = rushers.merge(\n        defp_coords[['PlayId','defp_coords']],\n        how='left', on='PlayId'\n    )\n    rushers['XY'] = rushers[['rusherX','rusherY']].apply(lambda x: x.tolist(), axis=1)\n    rushers.XY = rushers.XY.apply(np.array)\n    rushers['distEnemyNearest'] = rushers[['XY','defp_coords']].apply(NearestEnemy, axis=1)\n    rushercols.append('distEnemyNearest')\n    \n    del offp_coords, defp_coords; gc.collect()\n    \n    ##########################################################################################\n    \n    # Split into offensive / defensive\n    player_dist = df[~df.IsRusher][[\n        'PlayId','NflId',\n        'X','Y','S','A',\n        'Dir','Dis','PlayerWeight',\n        'IsOnOffense','Position',\n    ]]\n    player_dist.A = np.log1p(player_dist.A)\n    player_dist.Dis = np.log1p(player_dist.Dis)\n    player_dist['SX'] = player_dist.S * np.cos(player_dist.Dir)\n    player_dist['AX'] = player_dist.A * np.cos(player_dist.Dir)\n    player_dist['WX'] = player_dist.PlayerWeight * np.cos(player_dist.Dir)\n    \n    player_dist = player_dist.merge(rushers[['PlayId','rusherX','rusherY']], on='PlayId', how='inner')\n    player_dist['rusherDist'] = player_dist[['X','Y','rusherX','rusherY']].apply(lambda x: euclidean_distance(x[0],x[1],x[2],x[3]), axis=1)\n    \n\n    # Sorting by distance to rusher\n    player_dist.sort_values('rusherDist', inplace=True)\n    defensivePlayers = player_dist[player_dist.IsOnOffense==False].copy()\n    offensivePlayers = player_dist[player_dist.IsOnOffense==True].copy()\n\n    ##########################################################\n    # Additional rusher columns\n\n    # Count how many defenders are on the side of the field the rusher is moving towards\n    # relative to the rusher:\n    defense = defensivePlayers.groupby('PlayId').Y.agg(list).reset_index()\n    defense.columns = ['PlayId', 'defenseY']\n\n    rushers = rushers.merge(defense, on='PlayId', how='left')\n    rushers.defenseY = rushers[['defenseY','rusherY','rusherSY']].apply(lambda row: sum([1 for dy in row.defenseY if np.sign(row.rusherSY)==np.sign(dy - row.rusherY)]), axis=1)\n    rushercols.append('defenseY')\n    \n    \n    ##########################################################################################\n    # Voroni features of Rusher VS Offense\n    vf = vfeats(df[df.IsRusher | ~df.IsOnOffense][['PlayId','IsRusher','X','Y']].copy())\n    vf.columns = ['PlayId','vArea']\n    rushers = rushers.merge(vf, how='left',on='PlayId')\n    rushercols.append('vArea')\n    ##########################################################################################\n    \n    \n    \n    # TODO: Any feature that isn't being used, remove to speed up compute\n    # Doesnt help: Dir, Y, X, Dis, PlayerWeight\n    # Helps: rusherDist\n    # On the fence: 'A'\n    basecols = ['S','rusherDist']\n    \n    # Per dset, these should be stable. I'm not worried:\n    defensivePlayers.sort_values('rusherDist', inplace=True)\n    if 'Dir' in defensivePlayers: defensivePlayers.Dir.replace({np.nan: defensivePlayers.Dir.mean()}, inplace=True)\n    if 'Dis' in defensivePlayers: defensivePlayers.Dis.replace({np.nan: defensivePlayers.Dis.mean()}, inplace=True)\n    defensivePlayers.A.replace({np.nan: defensivePlayers.A.mean()}, inplace=True)\n    defensivePlayers.S.replace({np.nan: defensivePlayers.S.mean()}, inplace=True)\n    defensivePlayers.Position = defensivePlayers.Position.isin('OLB,MLB,LB,DE,DT,DL,DB'.split(',')).astype(np.float32)\n    defensivePlayers['feats'] = defensivePlayers[basecols].apply(lambda x: x.tolist(), axis=1)\n    \n    offensivePlayers.sort_values('rusherDist', inplace=True)\n    if 'Dir' in offensivePlayers: offensivePlayers.Dir.replace({np.nan: offensivePlayers.Dir.mean()}, inplace=True)\n    if 'Dis' in offensivePlayers: offensivePlayers.Dis.replace({np.nan: offensivePlayers.Dis.mean()}, inplace=True)\n    offensivePlayers.A.replace({np.nan: offensivePlayers.A.mean()}, inplace=True)\n    offensivePlayers.S.replace({np.nan: offensivePlayers.S.mean()}, inplace=True)\n    offensivePlayers.Position = offensivePlayers.Position.isin('TE,T,G,C,OT,OG'.split(',')).astype(np.float32)\n    offensivePlayers['feats'] = offensivePlayers[basecols].apply(lambda x: x.tolist(), axis=1)\n    \n    \n    # Different dir handling for offensive vs defensive players—once we decide too include it\n    #if 'Dir' in offensivePlayers: offensivePlayers.Dir = np.mod(offensivePlayers.Dir+np.pi, np.pi*2)\n    \n    # Flat Lists:\n    off_pfeats = offensivePlayers[['PlayId','feats']].groupby('PlayId').agg(lambda x: sum(x, [])[:-len(basecols)*NUM_PLAYERS_TRIM] ).reset_index() # 10\n    def_pfeats = defensivePlayers[['PlayId','feats']].groupby('PlayId').agg(lambda x: sum(x, [])[:-len(basecols)*NUM_PLAYERS_TRIM] ).reset_index() # 11\n\n    off_cols = [f'off{player_}_{col_}' for player_ in range(10-NUM_PLAYERS_TRIM) for col_ in basecols]\n    def_cols = [f'def{player_}_{col_}' for player_ in range(11-NUM_PLAYERS_TRIM) for col_ in basecols]\n    off_pfeats[off_cols] = pd.DataFrame(off_pfeats.feats.tolist(), columns=off_cols)\n    def_pfeats[def_cols] = pd.DataFrame(def_pfeats.feats.tolist(), columns=def_cols)\n    del off_pfeats['feats'], def_pfeats['feats']\n\n    \n    \n    \n    \n    ##########################################################################################\n    # Finally (we do these afterward just out of fear of re-ordering; plus we need to alter offPlayers:\n    # For the rest of the analysis, we dont count the QB or any other offensive player behind the RB\n    offensivePlayers = offensivePlayers[offensivePlayers.X>offensivePlayers.rusherX]\n    \n    SX1 = offensivePlayers.groupby('PlayId').SX.mean().sort_index().reset_index(); SX1.columns=['PlayId','SX1']\n    SX2 = defensivePlayers.groupby('PlayId').SX.mean().sort_index().reset_index(); SX2.columns=['PlayId','SX2']\n    SX1 = SX1.merge(SX2, how='left', on='PlayId'); SX1.SX1 += SX1.SX2; del SX1['SX2']; SX1.columns=['PlayId','Mean_SX']\n    rushers = rushers.merge(SX1, how='left', on='PlayId')\n    \n    AX1 = offensivePlayers.groupby('PlayId').AX.mean().sort_index().reset_index(); AX1.columns=['PlayId','AX1']\n    AX2 = defensivePlayers.groupby('PlayId').AX.mean().sort_index().reset_index(); AX2.columns=['PlayId','AX2']\n    AX1 = AX1.merge(AX2, how='left', on='PlayId'); AX1.AX1 += AX1.AX2; del AX1['AX2']; AX1.columns=['PlayId','Mean_AX']\n    rushers = rushers.merge(AX1, how='left', on='PlayId')\n\n    WX1 = offensivePlayers.groupby('PlayId').WX.mean().sort_index().reset_index(); WX1.columns=['PlayId','WX1']\n    WX2 = defensivePlayers.groupby('PlayId').WX.mean().sort_index().reset_index(); WX2.columns=['PlayId','WX2']\n    WX1 = WX1.merge(WX2, how='left', on='PlayId'); WX1.WX1 += WX1.WX2; del WX1['WX2']; WX1.columns=['PlayId','Mean_WX']\n    rushers = rushers.merge(WX1, how='left', on='PlayId')\n    \n    del AX1, AX2, SX1, SX2, WX1, WX2\n    rushercols += ['Mean_SX', 'Mean_AX', 'Mean_WX']\n    \n    ##########################################################################################\n    \n    \n    rushers.sort_values('PlayId', inplace=True)\n    off_pfeats.sort_values('PlayId', inplace=True)\n    def_pfeats.sort_values('PlayId', inplace=True)\n    rushers.reset_index(drop=True, inplace=True)\n    off_pfeats.reset_index(drop=True, inplace=True)\n    def_pfeats.reset_index(drop=True, inplace=True)\n    pfeats = pd.concat([\n        rushers,\n        off_pfeats[[c for c in off_pfeats.columns if c != 'PlayId']],\n        def_pfeats[[c for c in def_pfeats.columns if c != 'PlayId']],\n    ], axis=1)\n    del off_pfeats, def_pfeats, rushers; gc.collect()\n    \n    \n    # Downcast everything to float32\n    for col, dt in zip(pfeats.columns, pfeats.dtypes):\n        if col not in ['GameId', 'PlayId', 'Yards', 'Orig_Yards_NOTRAIN'] and dt=='int64':\n            pfeats[col] = pfeats[col].astype(np.float32)\n\n        if dt!='float64': continue\n        pfeats[col] = pfeats[col].astype(np.float32)\n    \n    print(time() - t)\n    return pfeats, basecols, rushercols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def prep_data(data, basecols, rushercols):\n    # Currently sorted by dist to rusher...\n    filtered_cols = []\n    train_cols = []\n    out = {}\n    \n    for player_ in range(10-NUM_PLAYERS_TRIM):\n        cols = [f'off{player_}_{col_}' for col_ in basecols]\n        out[f'off{player_}'] = data[cols]\n        filtered_cols = sum([filtered_cols, cols], [])\n\n    for player_ in range(11-NUM_PLAYERS_TRIM):\n        cols = [f'def{player_}_{col_}' for col_ in basecols]\n        out[f'def{player_}'] = data[cols]\n        filtered_cols = sum([filtered_cols, cols], [])\n        \n    # Add rusher specific data\n    out['rusher'] = data[rushercols]\n    filtered_cols = sum([filtered_cols, rushercols], [])\n    \n    train_cols = list(out.keys())\n    \n    # Add non-train columns here\n    for col in data.columns:\n        if col in filtered_cols: continue\n        out[col] = data[col]\n\n    \n    return out, train_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()\nwhat, basecols, rushercols = fast_preprocess(train.copy(), isTrain=True)\nwhat.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"z = what.isna().sum()\nz[z>0]","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# # Investigate:\n# for col in what.columns:\n#     #if 'dist_next_point_time' not in col: continue\n#     plt.title(col)\n#     plt.hist(what[col], 100)\n#     plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()\nX = what.copy()\n\n# transformed for mse, we should also standardize....\nymu = np.log1p(X.Yards.values - MIN_CLIP) # we zero it for sparse xent\n\nyent = np.zeros((X.Yards.shape[0], N_OUTPUTS))\nfor idx, target in enumerate(list(X.Yards.astype(int))):\n    yent[idx, target-MIN_CLIP] = 1\n    \nyent_NOCLIP_CSUM = np.zeros((X.Orig_Yards_NOTRAIN.shape[0], 199))\nfor idx, target in enumerate(list(X.Orig_Yards_NOTRAIN.astype(int))):\n    yent_NOCLIP_CSUM[idx, target+99] = 1\nyent_NOCLIP_CSUM = np.clip(np.cumsum(yent_NOCLIP_CSUM, axis=1), 0, 1)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"keys = X.Yards.unique()-MIN_CLIP\nvalues = class_weight.compute_class_weight('balanced', keys, X.Yards.values-MIN_CLIP)\ncweights = dict(zip(keys, values))\ncweights\n# cweights = None","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Original model, 16,16,32 -> 1024 -> 64\ndef BuildModel(basecols, rushercols):\n    global N_OUTPUTS\n    \n    n_basecols = len(basecols)\n    n_rusher_cols = len(rushercols)\n    \n    # Inputs\n    inputs = [ Input(shape=(n_rusher_cols,), name='rusher') ]\n    for i in range(10-NUM_PLAYERS_TRIM): inputs.append(Input(shape=(n_basecols,), name=f'off{i}'))\n    for i in range(11-NUM_PLAYERS_TRIM): inputs.append(Input(shape=(n_basecols,), name=f'def{i}'))\n    \n    # Reused Layers\n    off_dense = Dense( 4, activation='elu', name='off_dense') # 8\n    def_dense = Dense(16, activation='elu', name='def_dense') # 16?\n    run_dense = Dense(128, activation='elu', name='run_dense')\n    \n    \n    # Flow\n    x = [GaussianNoise(0.0025)(inp) for inp in inputs]\n    x[0] = run_dense(x[0])\n    for i in range(1,11-NUM_PLAYERS_TRIM):      x[i] = off_dense(x[i])\n    for i in range(11-NUM_PLAYERS_TRIM,len(x)): x[i] = def_dense(x[i])\n\n    x = Concatenate(name='flow')(x)\n    x = Dropout(0.25)(x)\n    x = Dense(256, activation='elu')(x)\n    x = Dense(64, activation='elu')(x)\n    x = BatchNormalization()(x)\n    \n    # mu in raw space [0,N_OUTPUTS)\n    # We don't have to add/sub Min_Clip because we're already in raw space starting from 0\n    # So we just apply the log1p transform\n    mu_raw = Dense(1, activation='relu', name='mu_raw')(x)\n    mu = Lambda(lambda x: K.log(1 + K.reshape(x[:,0],(-1,1))), name='mu')(mu_raw)\n    \n    ent = Dense(N_OUTPUTS, name='ent', activation='sigmoid')(\n        Concatenate()([\n            x, mu_raw\n        ])\n    )\n    reg_pass = Lambda(lambda x: x, name='reg_pass')(ent) # passthrough for different loss function...\n    \n    model = Model(\n        inputs,\n        [ent,reg_pass,mu]\n    )\n    \n    model.compile(\n        #optimizer=Nadam(learning_rate=0.0025, beta_1=0.9, beta_2=0.999),\n        optimizer=RAdam(weight_decay=0.0003),\n        loss={\n            'ent': 'categorical_crossentropy', # binary_crossentropy\n            'reg_pass': losses.mae,\n            'mu': losses.mae,\n        },\n        loss_weights={\n            'ent': 1,\n            'reg_pass': 1,\n            'mu': 1,\n        }\n    )\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Does It Blend!?"},{"metadata":{"trusted":true},"cell_type":"code","source":"kX, train_cols = prep_data(X, basecols, rushercols)\n\nglobal_scalars = {\n    tcolgrp: StandardScaler().fit(kX[tcolgrp])\n    for tcolgrp in train_cols\n}\n\nrun_scalars = StandardScaler().fit(kX['rusher'])\n\ndef_scalars = StandardScaler().fit(\n    pd.concat([\n        kX[colgrp].rename(columns={oc:basecols[i] for i,oc in enumerate(kX[colgrp].columns)})\n        for colgrp in train_cols\n        if 'def' in colgrp\n    ], axis=0)\n)\n\noff_scalars = StandardScaler().fit(\n    pd.concat([\n        kX[colgrp].rename(columns={oc:basecols[i] for i,oc in enumerate(kX[colgrp].columns)})\n        for colgrp in train_cols\n        if 'off' in colgrp\n    ], axis=0)\n)\n\ndel kX; gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"t = time()\nK.clear_session()\n\nif RUN_KAGGLE:\n    kf = KFold(n_splits=n_splits, random_state=42, shuffle=True)\nelse:\n    kf = GroupKFold(n_splits=n_splits)\n    \noof = np.zeros((X.Yards.shape[0], N_OUTPUTS))\noof_mu = np.zeros((X.Yards.shape[0]))\nreport = []\nuse_models = []\nperms = []\n\nfor fold_, (tdx, vdx) in enumerate(kf.split(X, yent, X.GameId)):\n    X_train, X_val = X.loc[tdx].copy(), X.loc[vdx].copy()\n    X_train_bound, X_val_bound = X_train[bounding_cols].copy(), X_val[bounding_cols].copy()\n    y_train_ent, y_train_mu = yent[tdx], ymu[tdx]\n    y_val_ent, y_val_mu = yent[vdx], ymu[vdx]\n    \n    kX_train, train_cols = prep_data(X_train, basecols, rushercols)\n    kX_val, _ = prep_data(X_val, basecols, rushercols)\n    \n    # Global Scaling\n    for tcolgrp in train_cols:\n        kX_train[tcolgrp] = global_scalars[tcolgrp].transform(kX_train[tcolgrp])\n        kX_val[tcolgrp] = global_scalars[tcolgrp].transform(kX_val[tcolgrp])\n    \n    y_train = {'ent':y_train_ent, 'reg_pass':y_train_ent, 'mu':y_train_mu}\n    y_val   = {'ent':y_val_ent,   'reg_pass':y_train_ent, 'mu':y_val_mu  }\n\n    # For Drop Importance:\n    y_val_crps = None\n    y_val_crps_orig = yent_NOCLIP_CSUM[vdx]\n    \n    # I say blend the best 3 of 4 runs...\n    oof_mu_runs = np.zeros((n_runs, X_val.shape[0]))\n    oof_runs = np.zeros((n_runs, X_val.shape[0], N_OUTPUTS))\n    oof_run_scores = []\n    for run_ in range(n_runs):\n        print(f'Fold: {fold_}, Run: {run_}')\n        model = BuildModel(basecols, rushercols)\n        \n        if fold_==0 and run_==0: model.summary()\n        es = EarlyStopping(\n            monitor='val_CRPS',\n            mode='min',\n            restore_best_weights=True, \n            verbose=2, \n            patience=20 if RUN_KAGGLE == False else 40\n        )\n\n        es.set_model(model)\n        metric = Metric(model, [es], [(kX_train,y_train), (kX_val,y_val)])\n        metric.set_original_data(X_train_bound, X_val_bound)\n        hist = model.fit(kX_train, y_train, class_weight=cweights, callbacks=[metric], epochs=400, batch_size=1024//2, verbose=False)\n        model.save(f'fold_{fold_}_{run_}.h5')\n        score_ = min(hist.history['val_CRPS'])\n        oof_run_scores.append(score_)\n        \n        # Stash OOF:\n        preds, _, preds_mu = model.predict(kX_val, batch_size=1024)\n        preds = metric.bound_prediction(preds, X_val_bound)\n        oof_runs[run_] = preds.copy()\n        oof_mu_runs[run_] = preds_mu.flatten().copy()\n        preds = metric.convert_to_y199_cumsum(preds)\n        \n        \n        if y_val_crps is None:\n            y_val_crps = metric.convert_to_y199_cumsum(y_val_ent)\n        \n        orig_score_ = crps(preds, y_val_crps_orig)\n        print(f'RunScore: {score_}, (Original): {orig_score_}')\n\n\n    # Top-k, by minimizing score\n    worst_run = np.argmax(oof_run_scores)\n    bad_model = f'fold_{fold_}_{worst_run}.h5'\n    use_models += [f'fold_{fold_}_{run_}.h5' for run_ in range(n_runs) if run_ != worst_run]\n    \n    if DO_LGBM:\n        # Add in a LGM model...\n        pass\n    \n    # Average the good runs\n    oof[vdx] = np.delete(oof_runs, worst_run, axis=0).mean(axis=0)\n    oof_mu[vdx] = np.delete(oof_mu_runs, worst_run, axis=0).mean(axis=0)\n    \n    # Transform to competition space, only for evaluation\n    preds = metric.convert_to_y199_cumsum(oof[vdx])\n    score_ = crps(preds, y_val_crps)\n    orig_score_ = crps(preds, y_val_crps_orig)\n    print(oof_run_scores, np.array(oof_run_scores).std(), 'STD') # todo: - make these orig score\n    report.append(f'Fold: {fold_}, TopK Mean Score: {score_}, (Original): {orig_score_}')\n    print(report[-1], end='\\n\\n')\n    print('—'*100)\n    \noof = metric.convert_to_y199_cumsum(oof)\noof_yent = metric.convert_to_y199_cumsum(yent)\nscore_ = crps(oof, oof_yent)\norig_score_ = crps(oof, yent_NOCLIP_CSUM)\nreport.append(f'Final Model Score: {score_}, (Original): {orig_score_}')\nreport.append(str(int(time() - t)) + ' Seconds to Train')\nreport","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"BEST CURRENT: distEnemyNearest\n['Fold: 0, TopK Mean Score: 0.012057172782096007, (Original): 0.012097108043781937\n 'Fold: 1, TopK Mean Score: 0.012509004743729533, (Original): 0.012524873738220215\n 'Fold: 2, TopK Mean Score: 0.012575775957873013, (Original): 0.012615985655969915',\n 'Fold: 3, TopK Mean Score: 0.012725437670387542, (Original): 0.01277326495291635',\n 'Fold: 4, TopK Mean Score: 0.011948991950306628, (Original): 0.01200008018400733',\n 'Final Model Score: 0.012363085703402736, (Original): 0.01240862868282777',\n '745 Seconds to Train']\n\n \nRunScore: 0.012144, (Original): 0.01220009377787243\nRunScore: 0.012191, (Original): 0.012247465752106385\nRunScore: 0.01214, (Original): 0.012196549494609235\nRunScore: 0.01218, (Original): 0.01223627213573673\n\nRunScore: 0.012582, (Original): 0.012614249898394662\nRunScore: 0.012578, (Original): 0.012610696820387535\nRunScore: 0.012594, (Original): 0.012626605014312883\nRunScore: 0.012629, (Original): 0.012661611179314641\n\nRunScore: 0.012646, (Original): 0.012686080854032241\nRunScore: 0.012744, (Original): 0.012784458497814616\nRunScore: 0.012661, (Original): 0.012701354661734455\nRunScore: 0.012627, (Original): 0.012667627788678372\n\nRunScore: 0.012804, (Original): 0.012851986168484986\nRunScore: 0.012781, (Original): 0.012829043376371873\nRunScore: 0.012779, (Original): 0.012827240090873835\nRunScore: 0.012807, (Original): 0.012854773150758787\n\nRunScore: 0.012079, (Original): 0.012130025993067096\nRunScore: 0.012017, (Original): 0.012068392028822521\nRunScore: 0.012037, (Original): 0.012088363623920625\nRunScore: 0.012023, (Original): 0.012074008350774705 \n            \nnext test: remove an extra enemy or friend"},{"metadata":{},"cell_type":"raw","source":"    \nxFinal2:\n['Fold: 0, TopK Mean Score: 0.012009871044478622, (Original): 0.012066053913496555',\n 'Fold: 1, TopK Mean Score: 0.012442996862533829, (Original): 0.012475417027876672',\n 'Fold: 2, TopK Mean Score: 0.012496540170002185, (Original): 0.012536749867859113',\n 'Fold: 3, TopK Mean Score: 0.012649277671575488, (Original): 0.012697104953570739',\n 'Fold: 4, TopK Mean Score: 0.01185753040058065, (Original): 0.011908618633024995',\n 'Final Model Score: 0.01229108890864962, (Original): 0.012336631887320858']\n     "},{"metadata":{},"cell_type":"markdown","source":"- [good] Remove furtherest two players\n- [good] Add rusher FE from other notebook\n- [bad] Cleaned \"Position\", e.g. OFF: {QB, WR, OL}\n- [notime] Randomize order of players (ties into standardization strategy)\n- [notime] Drop Importance\n- [rejected] def_basecols, off_basecols :: use whatever helps, drop whatever hurts -- already minimized\n- [reied] Add additional base features for defenders and offenders (dist to nearest enemy)\n\n- [work needed] Neural architecture tweaks\n- Handling Season Feature: base off leaked yards in 2019\n\n### Finally\n\n- Submit LGB Blend of other model....\n- Submit this model..."},{"metadata":{},"cell_type":"markdown","source":"# Inference"},{"metadata":{"trusted":true},"cell_type":"code","source":"from kaggle.competitions import nflrush\nenv = nflrush.make_env()\niter_test = env.iter_test()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = []\nfor model_path in use_models:\n    models.append(load_model(model_path))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for (test_df, sample_prediction_df) in iter_test:\n    test_df1, _, _ = fast_preprocess(test_df.copy(), isTrain=False)\n    kX_sub_bound = test_df1[bounding_cols].copy()\n    kX_sub, _ = prep_data(test_df1, basecols, rushercols)\n\n    # Global Scaling\n    for tcolgrp in train_cols:\n        kX_sub[tcolgrp] = global_scalars[tcolgrp].transform(kX_sub[tcolgrp])\n\n    y_pred = np.mean(\n        [\n            metric.bound_prediction(\n                model.predict(kX_sub, batch_size=1024)[0],\n                kX_sub_bound\n            )\n            for model in models    \n        ],\n        axis=0\n    )\n    y_pred = metric.convert_to_y199_cumsum(y_pred)\n    preds_df = pd.DataFrame(data=[y_pred.flatten()], columns=sample_prediction_df.columns)\n    env.predict(preds_df)\n    \nenv.write_submission_file()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"}},"nbformat":4,"nbformat_minor":1}