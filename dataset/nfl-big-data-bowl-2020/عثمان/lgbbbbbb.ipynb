{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# new submission??\n# - how much free space exists on the side (Y) of the field the risher is more on. this can be dist to the nearest defender from the y-side\n# - [building] LIGHTGBM\n# - [todo] divisor=1, timing...\n# - [todo] stack ridge,lasso,survival model linear models?\n\n# - AUG: sample features from columns where the yards match to create pseudo samples\n# - FE: rusher_dist_td or other rusher_X type inputs....\n# - FE: rusherA; typically moving close to max speed in intended direction\n\n\n# FINAL SUB\n# n_splits = 10\n# 2019 as new season or as same season...\n# KFold looks like it won\n# divisor=1 is worse but trains faster, maybe use divisor==3?\n\nRUN_KAGGLE = True\nDO_LGBM = True\n\nDIVISOR = 2\nn_runs = 4 # n-1 will be kept.\nn_splits = 10 # Use 10 for final sub!!!!!!!\n\n\n# Cols we use to bound predictions:\nbounding_cols = ['rusher_X','rusher_SX','YardLine']\n\n# Derived by: X.Yards.value_counts().sort_index()[0:50]\nMIN_CLIP = -9\nMAX_CLIP = 70\n\nN_OUTPUTS = MAX_CLIP - MIN_CLIP + 1\nN_OUTPUTS","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Init"},{"metadata":{"trusted":true},"cell_type":"code","source":"import logging\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nimport sklearn.metrics as mtr\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom sklearn.utils import class_weight\nfrom keras.callbacks import Callback, EarlyStopping, ModelCheckpoint, LambdaCallback\nfrom keras.models import Model\nfrom keras.optimizers import Nadam\nfrom keras import losses\nfrom keras.layers import Input, Dense, Concatenate, Reshape, Dropout, merge, Add, BatchNormalization, Lambda, GaussianNoise, Layer\nfrom keras.layers.embeddings import Embedding\nfrom keras.models import load_model\nfrom tqdm import tqdm_notebook as tqdm\nfrom sklearn.model_selection import KFold,GroupKFold\nimport warnings, math, numba\nimport random as rn\nfrom time import time\n\nimport tensorflow as tf\nimport keras.backend as K\n\nfrom tensorflow.python.keras.optimizer_v2.optimizer_v2 import OptimizerV2\nfrom tensorflow.python import ops, math_ops, state_ops, control_flow_ops\n\nfrom scipy.spatial import Voronoi, ConvexHull, voronoi_plot_2d\n\nimport os, gc\nwarnings.filterwarnings(\"ignore\")\n\nimport seaborn as sns\nimport matplotlib.patches as patches\nimport matplotlib.pyplot as plt\n\n# from kaggle.competitions import nflrush\n# env = nflrush.make_env()\n# iter_test = env.iter_test()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if RUN_KAGGLE:\n    train = pd.read_csv('../input/nfl-big-data-bowl-2020/train.csv', parse_dates=['TimeHandoff','TimeSnap','PlayerBirthDate'], dtype={'WindSpeed': 'object'})\nelse:\n    train = pd.read_csv(\"../train.csv\",  low_memory=False, parse_dates=['TimeHandoff','TimeSnap','PlayerBirthDate'], dtype={'WindSpeed': 'object'})\n    test = pd.read_csv(\"../df_test.csv\", low_memory=False, parse_dates=['TimeHandoff','TimeSnap','PlayerBirthDate'], dtype={'WindSpeed': 'object'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_rounds = 1000\nparams = {\n#     'objective':'multiclass',\n#     'num_class': N_OUTPUTS,\n#     'learning_rate': 0.01,\n#     \"boosting\": \"gbdt\",\n#     \"metric\": \"multi_logloss\",\n#     \"verbosity\": -1,\n#     \"seed\":1234\n    \n    \n    'num_leaves': 50, #Original 50\n    'min_data_in_leaf': 30, #Original 30\n    'objective':'multiclass',\n    'num_class': N_OUTPUTS,\n    'max_depth': -1,\n    'learning_rate': 0.01,\n    \"min_child_samples\": 20,\n    \"boosting\": \"gbdt\",\n    \"feature_fraction\": 0.7, #0.9\n    \"bagging_freq\": 1,\n    \"bagging_fraction\": 0.9,\n    \"bagging_seed\": 11,\n    \n    \"lambda_l1\": 0.1,\n    \"verbosity\": -1,\n    \"seed\":1234,\n    \"metric\": \"multi_logloss\",\n}\n\ncat_feats = [\n    'Season','InfluenceRusherX_flip','def_triCon',\n    'oline_num_def_inbox','defense_Y','Congestion',\n]\n\n# reg_cols\nlgb_cols = [\n    'Season', 'rusher_A', 'rusher_SX', 'avg_def9_dist_to_rusher', 'rusher_dist_scrimmage',\n    'rusher_S', 'avg_off5_dist_to_rusher', 'min_off5_dist_to_rusher',\n    'dist_next_point_time', 'min_def9_dist_to_rusher', 'std_off5_dist_to_rusher',\n    'defense_scrimmage_Y_std', 'std_def9_dist_to_rusher', 'Dis', 'closest_def_S',\n    'closest_def_A', 'defense_Y', 'InfluenceRusherX_flip', 'Congestion', 'def_triCon',\n    'vArea', 'avg_def9_A', 'avg_def9_S', 'std_def9_S', 'min_def9_S',\n    'RusherMaxObservedS', 'Mean_SX','Mean_AX','Mean_WX',\n    'oline_length', 'oline_bline_area', 'oline_num_def_inbox', 'oline_avg_ol_dist',\n\n#     # For diversity:\n#     'def9_dist_to_rusher_0',\n#     'def9_dist_to_rusher_1', 'def9_dist_to_rusher_2',\n#     'def9_dist_to_rusher_3', 'def9_dist_to_rusher_4',\n#     'def9_dist_to_rusher_5', 'def9_dist_to_rusher_6',\n#     'def9_dist_to_rusher_7', 'def9_dist_to_rusher_8',\n]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Optimizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"class RAdam(OptimizerV2):\n    \"\"\"RAdam optimizer.\n    According to the paper\n    [On The Variance Of The Adaptive Learning Rate And Beyond](https://arxiv.org/pdf/1908.03265v1.pdf).\n    \"\"\"\n\n    def __init__(self,\n                 learning_rate=0.001,\n                 beta_1=0.9,\n                 beta_2=0.999,\n                 epsilon=1e-7,\n                 weight_decay=0.,\n                 amsgrad=False,\n                 total_steps=0,\n                 warmup_proportion=0.1,\n                 min_lr=0.,\n                 name='RAdam',\n                 **kwargs):\n        r\"\"\"Construct a new Adam optimizer.\n        Args:\n            learning_rate: A Tensor or a floating point value.    The learning rate.\n            beta_1: A float value or a constant float tensor. The exponential decay\n                rate for the 1st moment estimates.\n            beta_2: A float value or a constant float tensor. The exponential decay\n                rate for the 2nd moment estimates.\n            epsilon: A small constant for numerical stability. This epsilon is\n                \"epsilon hat\" in the Kingma and Ba paper (in the formula just before\n                Section 2.1), not the epsilon in Algorithm 1 of the paper.\n            weight_decay: A floating point value. Weight decay for each param.\n            amsgrad: boolean. Whether to apply AMSGrad variant of this algorithm from\n                the paper \"On the Convergence of Adam and beyond\".\n            total_steps: An integer. Total number of training steps.\n                Enable warmup by setting a positive value.\n            warmup_proportion: A floating point value. The proportion of increasing steps.\n            min_lr: A floating point value. Minimum learning rate after warmup.\n            name: Optional name for the operations created when applying gradients.\n                Defaults to \"Adam\".    @compatibility(eager) When eager execution is\n                enabled, `learning_rate`, `beta_1`, `beta_2`, and `epsilon` can each be\n                a callable that takes no arguments and returns the actual value to use.\n                This can be useful for changing these values across different\n                invocations of optimizer functions. @end_compatibility\n            **kwargs: keyword arguments. Allowed to be {`clipnorm`, `clipvalue`, `lr`,\n                `decay`}. `clipnorm` is clip gradients by norm; `clipvalue` is clip\n                gradients by value, `decay` is included for backward compatibility to\n                allow time inverse decay of learning rate. `lr` is included for backward\n                compatibility, recommended to use `learning_rate` instead.\n        \"\"\"\n\n        super(RAdam, self).__init__(name, **kwargs)\n        self._set_hyper('learning_rate', kwargs.get('lr', learning_rate))\n        self._set_hyper('beta_1', beta_1)\n        self._set_hyper('beta_2', beta_2)\n        self._set_hyper('decay', self._initial_decay)\n        self._set_hyper('weight_decay', weight_decay)\n        self._set_hyper('total_steps', float(total_steps))\n        self._set_hyper('warmup_proportion', warmup_proportion)\n        self._set_hyper('min_lr', min_lr)\n        self.epsilon = epsilon or K.epsilon()\n        self.amsgrad = amsgrad\n        self._initial_weight_decay = weight_decay\n        self._initial_total_steps = total_steps\n\n    def _create_slots(self, var_list):\n        for var in var_list:\n            self.add_slot(var, 'm')\n        for var in var_list:\n            self.add_slot(var, 'v')\n        if self.amsgrad:\n            for var in var_list:\n                self.add_slot(var, 'vhat')\n\n    def set_weights(self, weights):\n        params = self.weights\n        num_vars = int((len(params) - 1) / 2)\n        if len(weights) == 3 * num_vars + 1:\n            weights = weights[:len(params)]\n        super(RAdam, self).set_weights(weights)\n\n    def _resource_apply_dense(self, grad, var):\n        var_dtype = var.dtype.base_dtype\n        lr_t = self._decayed_lr(var_dtype)\n        m = self.get_slot(var, 'm')\n        v = self.get_slot(var, 'v')\n        beta_1_t = self._get_hyper('beta_1', var_dtype)\n        beta_2_t = self._get_hyper('beta_2', var_dtype)\n        epsilon_t = ops.convert_to_tensor(self.epsilon, var_dtype)\n        local_step = math_ops.cast(self.iterations + 1, var_dtype)\n        beta_1_power = math_ops.pow(beta_1_t, local_step)\n        beta_2_power = math_ops.pow(beta_2_t, local_step)\n\n        if self._initial_total_steps > 0:\n            total_steps = self._get_hyper('total_steps', var_dtype)\n            warmup_steps = total_steps * self._get_hyper('warmup_proportion', var_dtype)\n            min_lr = self._get_hyper('min_lr', var_dtype)\n            decay_steps = K.maximum(total_steps - warmup_steps, 1)\n            decay_rate = (min_lr - lr_t) / decay_steps\n            lr_t = tf.where(\n                local_step <= warmup_steps,\n                lr_t * (local_step / warmup_steps),\n                lr_t + decay_rate * K.minimum(local_step - warmup_steps, decay_steps),\n            )\n\n        sma_inf = 2.0 / (1.0 - beta_2_t) - 1.0\n        sma_t = sma_inf - 2.0 * local_step * beta_2_power / (1.0 - beta_2_power)\n\n        m_t = state_ops.assign(m,\n                               beta_1_t * m + (1.0 - beta_1_t) * grad,\n                               use_locking=self._use_locking)\n        m_corr_t = m_t / (1.0 - beta_1_power)\n\n        v_t = state_ops.assign(v,\n                               beta_2_t * v + (1.0 - beta_2_t) * math_ops.square(grad),\n                               use_locking=self._use_locking)\n        if self.amsgrad:\n            vhat = self.get_slot(var, 'vhat')\n            vhat_t = state_ops.assign(vhat,\n                                      math_ops.maximum(vhat, v_t),\n                                      use_locking=self._use_locking)\n            v_corr_t = math_ops.sqrt(vhat_t / (1.0 - beta_2_power))\n        else:\n            vhat_t = None\n            v_corr_t = math_ops.sqrt(v_t / (1.0 - beta_2_power))\n\n        r_t = math_ops.sqrt((sma_t - 4.0) / (sma_inf - 4.0) *\n                            (sma_t - 2.0) / (sma_inf - 2.0) *\n                            sma_inf / sma_t)\n\n        var_t = tf.where(sma_t >= 5.0, r_t * m_corr_t / (v_corr_t + epsilon_t), m_corr_t)\n\n        if self._initial_weight_decay > 0.0:\n            var_t += self._get_hyper('weight_decay', var_dtype) * var\n\n        var_update = state_ops.assign_sub(var,\n                                          lr_t * var_t,\n                                          use_locking=self._use_locking)\n\n        updates = [var_update, m_t, v_t]\n        if self.amsgrad:\n            updates.append(vhat_t)\n        return control_flow_ops.group(*updates)\n\n    def _resource_apply_sparse(self, grad, var, indices):\n        var_dtype = var.dtype.base_dtype\n        lr_t = self._decayed_lr(var_dtype)\n        beta_1_t = self._get_hyper('beta_1', var_dtype)\n        beta_2_t = self._get_hyper('beta_2', var_dtype)\n        epsilon_t = ops.convert_to_tensor(self.epsilon, var_dtype)\n        local_step = math_ops.cast(self.iterations + 1, var_dtype)\n        beta_1_power = math_ops.pow(beta_1_t, local_step)\n        beta_2_power = math_ops.pow(beta_2_t, local_step)\n\n        if self._initial_total_steps > 0:\n            total_steps = self._get_hyper('total_steps', var_dtype)\n            warmup_steps = total_steps * self._get_hyper('warmup_proportion', var_dtype)\n            min_lr = self._get_hyper('min_lr', var_dtype)\n            decay_steps = K.maximum(total_steps - warmup_steps, 1)\n            decay_rate = (min_lr - lr_t) / decay_steps\n            lr_t = tf.where(\n                local_step <= warmup_steps,\n                lr_t * (local_step / warmup_steps),\n                lr_t + decay_rate * K.minimum(local_step - warmup_steps, decay_steps),\n            )\n\n        sma_inf = 2.0 / (1.0 - beta_2_t) - 1.0\n        sma_t = sma_inf - 2.0 * local_step * beta_2_power / (1.0 - beta_2_power)\n\n        m = self.get_slot(var, 'm')\n        m_scaled_g_values = grad * (1 - beta_1_t)\n        m_t = state_ops.assign(m, m * beta_1_t, use_locking=self._use_locking)\n        with ops.control_dependencies([m_t]):\n            m_t = self._resource_scatter_add(m, indices, m_scaled_g_values)\n        m_corr_t = m_t / (1.0 - beta_1_power)\n\n        v = self.get_slot(var, 'v')\n        v_scaled_g_values = (grad * grad) * (1 - beta_2_t)\n        v_t = state_ops.assign(v, v * beta_2_t, use_locking=self._use_locking)\n        with ops.control_dependencies([v_t]):\n            v_t = self._resource_scatter_add(v, indices, v_scaled_g_values)\n\n        if self.amsgrad:\n            vhat = self.get_slot(var, 'vhat')\n            vhat_t = state_ops.assign(vhat,\n                                      math_ops.maximum(vhat, v_t),\n                                      use_locking=self._use_locking)\n            v_corr_t = math_ops.sqrt(vhat_t / (1.0 - beta_2_power))\n        else:\n            vhat_t = None\n            v_corr_t = math_ops.sqrt(v_t / (1.0 - beta_2_power))\n\n        r_t = math_ops.sqrt((sma_t - 4.0) / (sma_inf - 4.0) *\n                            (sma_t - 2.0) / (sma_inf - 2.0) *\n                            sma_inf / sma_t)\n\n        var_t = tf.where(sma_t >= 5.0, r_t * m_corr_t / (v_corr_t + epsilon_t), m_corr_t)\n\n        if self._initial_weight_decay > 0.0:\n            var_t += self._get_hyper('weight_decay', var_dtype) * var\n\n        var_update = self._resource_scatter_add(var, indices, tf.gather(-lr_t * var_t, indices))\n\n        updates = [var_update, m_t, v_t]\n        if self.amsgrad:\n            updates.append(vhat_t)\n        return control_flow_ops.group(*updates)\n\n    def get_config(self):\n        config = super(RAdam, self).get_config()\n        config.update({\n            'learning_rate': self._serialize_hyperparameter('learning_rate'),\n            'beta_1': self._serialize_hyperparameter('beta_1'),\n            'beta_2': self._serialize_hyperparameter('beta_2'),\n            'decay': self._serialize_hyperparameter('decay'),\n            'weight_decay': self._serialize_hyperparameter('weight_decay'),\n            'epsilon': self.epsilon,\n            'amsgrad': self.amsgrad,\n            'total_steps': self._serialize_hyperparameter('total_steps'),\n            'warmup_proportion': self._serialize_hyperparameter('warmup_proportion'),\n            'min_lr': self._serialize_hyperparameter('min_lr'),\n        })\n        return config","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Metrics"},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_pidf(pis):\n    if len(pis)==0:\n        return None\n    \n    keys = list(pis[0].keys())\n    pidf = pd.DataFrame({\n        'cols':keys,\n        'means':[np.nanmean([pi[key] for pi in pis]) for key in keys],\n        'medians':[np.nanmedian([pi[key] for pi in pis]) for key in keys],\n        'stds':[np.nanstd([pi[key] for pi in pis]) for key in keys]\n    })\n    pidf['std_mean_ratio'] = np.round(pidf.stds / pidf.means * 100, 2)\n    pidf.sort_values(['means', 'std_mean_ratio'], inplace=True, ascending=False)\n    return pidf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_feature_importance(perms, cutoff=None):\n    pidf_cvs = pd.DataFrame({\n        'cols': sum([list(fold.keys()) for fold in perms], []),\n        'vals': sum([list(fold.values()) for fold in perms], [])\n    }).sort_values('vals', ascending=False)\n    \n    if cutoff is not None:\n        cutoff_df = pidf_cvs.groupby('cols').vals.mean().sort_values().reset_index()\n        pidf_cvs = pidf_cvs[pidf_cvs.cols.isin(cutoff_df.iloc[:-cutoff].cols)]\n        cutoff_df.rename(columns={'vals':'sortorder'}, inplace=True)\n        pidf_cvs = pidf_cvs.merge(cutoff_df, how='left', on='cols')\n        pidf_cvs.sort_values('sortorder', ascending=False)\n        \n    fig, ax = plt.subplots(figsize=(8, 8))\n    #pidf_cvs.plot.barh(ax=ax)\n    #fig.show()\n    sns.barplot(y=\"cols\", x=\"vals\", data=pidf_cvs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# evaluation metric\n\n@numba.jit(numba.f8[:](numba.f8[:]))\ndef norm(x):\n    return x / x.sum(axis=1).reshape(-1,1)\n    \n@numba.jit(numba.f8[:](numba.f8[:]))\ndef cumsum(x):\n    return np.clip(np.cumsum(x, axis=1), 0, 1)\n\n@numba.jit(numba.f8[:](numba.f8[:], numba.f8[:]))\ndef crps(y_pred, y_true):\n    return ((y_true - y_pred) ** 2).sum(axis=1).sum(axis=0) / (y_true.shape[1] * y_true.shape[0]) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# author : nlgn\n# Link : https://www.kaggle.com/kingychiu/keras-nn-starter-crps-early-stopping\nclass Metric(Callback):\n    def __init__(self, model, callbacks, data):\n        super().__init__()\n        self.model = model\n        self.callbacks = callbacks\n        self.data = data\n        self.epochnum = 0\n\n    def on_train_begin(self, logs=None):\n        self.epochnum = 0\n        for callback in self.callbacks:\n            callback.on_train_begin(logs)\n\n    def on_train_end(self, logs=None):\n        for callback in self.callbacks:\n            callback.on_train_end(logs)\n\n    def set_original_data(self, orig_train, orig_valid):\n        global bounding_cols\n        self.orig_train = orig_train[bounding_cols].copy()\n        self.orig_valid = orig_valid[bounding_cols].copy()\n        \n    def bound_prediction(self, y_pred, non_scaled_data, ACCEPTABLE_DIST_FROM_ARGMAX=30):\n        \"\"\"\n        Zero out values we cannot get to\n\n        Look for the Yard Line Value corresponding to the position we believe\n        we will not go behind\n        \"\"\"\n        \n        global N_OUTPUTS\n        \n        # Zero out any y_preds less than this value\n        indices = np.repeat(np.arange(N_OUTPUTS).reshape(1,N_OUTPUTS), y_pred.shape[0], axis=0)\n\n        # NOTE: `rusher_dist_scrimmage = YardLine - rusher_X`\n        \n        # We cannot lose more than YardLine yards. Furthermore, we can't lose more\n        # than 3 yards less than where the rusher is currently positioned.\n        rusher_X = non_scaled_data.rusher_X.astype(int).values\n        YardLine = non_scaled_data.YardLine.values\n        Slippage = (non_scaled_data.rusher_SX.values > 0).astype(int)\n\n        # We can lose up to our rusher position yards and 3 extra\n        # However if rusher is currently moving in the +x direction\n        # Then that 3 extra turns into 2\n        # We also bound to the stadium (e.g. 1 yard line)\n        MaxLossYardLine = np.maximum(0, rusher_X - 3 + Slippage)\n\n        # Convert MaxLossYardLine to frame of reference of the actual YardLine\n        MaxLossYards = (MaxLossYardLine - YardLine - MIN_CLIP).reshape(-1,1)\n        y_pred[indices < MaxLossYards] = 0\n        \n\n        # We cannot gain more yards than 100-YardLine, e.g. dist_to_td\n        MaxGainYards = (100 - YardLine - MIN_CLIP).reshape(-1,1)\n        y_pred[indices > MaxGainYards] = 0\n        \n        # TODO: Revisit:\n        # Any value > ACCEPTABLE_DIST_FROM_ARGMAX units from our argmax is zero'd\n        # maxes = np.argmax(y_pred, axis=1)\n        #y_pred[indices > (maxes+ACCEPTABLE_DIST_FROM_ARGMAX).reshape(-1,1)] = 0\n        #y_pred[indices < (maxes-ACCEPTABLE_DIST_FROM_ARGMAX).reshape(-1,1)] = 0\n        # NOTE: We don't have to use amax..... we can use mu, which is one of our predicted values...\n        \n        # TODO: Expderiment doing this @ convert stage\n        return norm(y_pred)\n        #return y_pred#norm(y_pred)\n        \n    def convert_to_y199_cumsum(self, y_pred):\n        output = np.zeros((y_pred.shape[0], 199))\n        output[:,99+MIN_CLIP:99+MAX_CLIP+1] = y_pred\n        \n        #csum = np.clip(np.cumsum(output, axis=1), 0, 1)\n        csum = cumsum(output)\n        \n        # This is our best guess for the value (better than argmax)\n        # We might have to +1\n        #cross90 = np.argmax(csum>0.95, axis=1)\n#         cross10 = np.argmax(csum>0.05, axis=1)\n        \n#         # All eggs in one basket:\n#         mask = np.repeat(np.arange(199).reshape(1,-1), y_pred.shape[0], axis=0)\n#         output[mask < cross50.reshape(-1,1) - 4] = 0\n#         output[mask > cross50.reshape(-1,1) + 8] = 0\n#         csum = cumsum(output)\n        \n#         mask = np.repeat(np.arange(199).reshape(1,-1), y_pred.shape[0], axis=0)\n#         output[mask < cross10.reshape(-1,1)] /= 2\n        #output[mask > cross90.reshape(-1,1)] /= 2\n        \n#         csum = cumsum(output)\n        # Alternatively try it again, looks like our indices were wrong!!!\n        \n        return csum\n        \n            \n        '''\n        We've discovered that if we look at where we cross the 50% boundary, it seems that\n        value is typically 1 unit shy of the actual real answer, on average.\n        \n        How to leverage this?\n        - Increase the amount predicted at this 50% crossover?\n        - shift the crossover?\n        - shift the value after the crossover?\n        - other?\n            err_ent = np.argmax(oof>0.5, axis=1) - 99\n            current_err = (X.Yards.values - err_ent).sum()\n            patch_err = (X.Yards.values - (err_ent+1)).sum()\n\n            current_err, patch_err\n        '''\n        return csum\n    def on_epoch_end(self, batch, logs=None):\n        self.epochnum += 1\n        X_train, y_train = self.data[0][0], self.data[0][1]['ent']\n        y_pred = self.model.predict(X_train, batch_size=1024)[0]\n        y_pred = self.bound_prediction(y_pred, self.orig_train)\n        y_pred = self.convert_to_y199_cumsum(y_pred)\n        y_train = self.convert_to_y199_cumsum(y_train)\n        tr_s = crps(y_pred, y_train)\n        tr_s = np.round(tr_s, 6)\n        logs['tr_CRPS'] = tr_s\n\n        \n        X_valid, y_valid = self.data[1][0], self.data[1][1]['ent']\n        y_pred = self.model.predict(X_valid, batch_size=1024)[0]\n        y_pred = self.bound_prediction(y_pred, self.orig_valid)\n        y_pred = self.convert_to_y199_cumsum(y_pred)\n        y_valid = self.convert_to_y199_cumsum(y_valid)\n        val_s = crps(y_pred, y_valid)\n        val_s = np.round(val_s, 6)\n        logs['val_CRPS'] = val_s\n        \n        print(f'{self.epochnum}\\ttCRPS: {tr_s}\\tvCRPS: {val_s}')\n\n        for callback in self.callbacks:\n            callback.on_epoch_end(batch, logs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# TODO: Note - looks like this one is returning something different from the one in show_voroni!!!\ndef calcvs(play):\n    xy = play[['X', 'Y']].values\n    rusher = play.IsRusher.values\n    vor = Voronoi(xy)\n\n    # PROBLEMATIC - doesnt seem like we're getting the right index always.\n    rx_idx = np.argmax(rusher)\n    region = vor.regions[vor.point_region[rx_idx]]\n    rx_poly = np.array([vor.vertices[i] for i in region])\n\n    try:\n        ch = ConvexHull(rx_poly)\n        return ch.area\n    except:\n        return np.nan\n        \ndef vfeats(df):\n    # We use this to cut the area behind the rusher and so we can select his area\n    # Since he's usually the guy on the far left:\n    rusher = df[df.IsRusher] #it's already a .copy()\n    rusher.IsRusher = False\n    rusher.X -= 1; df = df.append(rusher.copy(), sort=False)\n    rusher.X += 99; df = df.append(rusher.copy(), sort=False)\n    rusher.X -= 99\n    rusher.Y -= 15; df = df.append(rusher.copy(), sort=False)\n    rusher.Y += 30; df = df.append(rusher.copy(), sort=False)\n\n    results = df.groupby('PlayId').apply(calcvs).reset_index()\n    return results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@numba.jit\ndef euclidean_distance(x1,y1,x2,y2):\n    x_diff = x1-x2\n    y_diff = y1-y2\n    return math.sqrt(x_diff*x_diff + y_diff*y_diff)\n    \n@numba.jit\ndef quad(a,b,c):\n    # Return solutions for quadratic\n    sol = None\n    if abs(a) < 1e-6:\n        if abs(b) < 1e-6:\n            sol = (0,0) if abs(c) < 1e-6 else None\n        else:\n            sol = (-c/b, -c/b)\n    else:\n        disc = b*b - 4*a*c\n        if disc >= 0:\n            disc = np.sqrt(disc)\n            a = 2*a\n            sol = ((-b-disc)/a, (-b+disc)/a)\n    return sol\n\n@numba.jit\ndef intercept(def_x, def_y, def_v, run_x, run_y, run_vx, run_vy, direct=False, get_intercept=False):\n    if direct:\n        # Don't extrapolate\n        dir_x = run_x - def_x\n        dir_y = run_y - def_y\n        \n        # Normalize\n        dir_len = np.sqrt(dir_x*dir_x + dir_y*dir_y)\n        return dir_x/dir_len, dir_y/dir_len\n\n    # Courtesy https://stackoverflow.com/questions/2248876/2d-game-fire-at-a-moving-target-by-predicting-intersection-of-projectile-and-u\n    # Calculate the x,y direction the defender needs to be moving at to intercept the player\n    # If no real solution is possible, then just return x,y values\n    # that move us directly to runners current pos\n    tx = run_x - def_x\n    ty = run_y - def_y\n    tvx = run_vx\n    tvy = run_vy\n    \n    #Get quadratic equation components\n    a = tvx*tvx + tvy*tvy - def_v*def_v\n    b = 2 * (tvx * tx + tvy * ty);\n    c = tx*tx + ty*ty\n\n    ts = quad(a, b, c)\n\n    # Find smallest positive solution\n    sol = None\n    if ts is not None:\n        t0, t1 = ts\n        t = min(t0, t1)\n        if t < 0:\n            t = max(t0, t1)\n        if t > 0:\n            sol = {\n                'x': run_x + run_vx*t,\n                'y': run_y + run_vy*t,\n            }\n\n    if get_intercept:\n        if sol is None:\n            return MAX_CLIP\n        return sol['x']\n            \n    # Sol is the intercept point...\n    # Now, calculate desired x,y accel directions accordingly....\n    if sol is not None:\n        dir_x = sol['x'] - def_x\n        dir_y = sol['y'] - def_y\n    else:\n        dir_x = run_x - def_x\n        dir_y = run_y - def_y\n        \n    # Normalize\n    dir_len = np.sqrt(dir_x*dir_x + dir_y*dir_y)\n    return dir_x/dir_len, dir_y/dir_len","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@numba.jit\ndef euclidean_flat(p1,p2):\n    x_diff = p2[:,0]-p1[:,0]\n    y_diff = p2[:,1]-p1[:,1]\n    return np.sqrt(x_diff*x_diff + y_diff*y_diff)\n\ndef PlayerInfluence(grp):\n    ball_coords = grp.ball_coords.values[0]\n    offp_coords = grp.offp_coords.values[0]\n    defp_coords = grp.defp_coords.values[0]\n    \n    MAXV = 16\n    MAXV_log1p = np.log1p(MAXV)\n    \n    results = []\n    for x in range(int(ball_coords[0]+1), int(ball_coords[0]+1+MAXV)):\n        point = ball_coords.copy()\n        point[0] = x\n        point = np.repeat(point.reshape(1,-1), defp_coords.shape[0], axis=0)\n        \n        # Calculate distance from each offender/defender to the points on front of the ball/runner (+x)\n        offp_point_dist = euclidean_flat(offp_coords, point).clip(0,MAXV)\n        defp_point_dist = euclidean_flat(defp_coords, point).clip(0,MAXV)\n        \n        # Compute influence\n        offp_point_inf = MAXV_log1p - np.log1p(offp_point_dist)\n        defp_point_inf = MAXV_log1p - np.log1p(defp_point_dist)\n\n        results.append(\n            offp_point_inf.sum() - defp_point_inf.sum()\n        )\n        \n    return results\n\n@numba.jit\ndef PointInTriangle(p, p0, p1, p2):\n    s = p0[1] * p2[0] - p0[0] * p2[1] + (p2[1] - p0[1]) * p[0] + (p0[0] - p2[0]) * p[1]\n    t = p0[0] * p1[1] - p0[1] * p1[0] + (p0[1] - p1[1]) * p[0] + (p1[0] - p0[0]) * p[1]\n\n    if (s < 0) != (t < 0): return False\n\n    A = -p1[1] * p2[0] + p0[1] * (p2[0] - p1[0]) + p0[0] * (p1[1] - p2[1]) + p1[0] * p2[1]\n\n    return s <= 0 and s + t >= A if A < 0 else s >= 0 and s + t <= A\n\n@numba.jit\ndef PointInTriangleFlat(p, p0x, p0y, p1x, p1y, p2x, p2y):\n    return PointInTriangle(p, [p0x,p0y], [p1x,p1y], [p2x,p2y])\n\n# @numba.jit\ndef build_oline_features(grp):\n    IDX_X = 0\n    IDX_Y = 1\n    IDX_ROLE = 2\n    IDX_OFFENSE = 3\n\n    grp = grp[['X','Y','role','IsOnOffense']].values\n    oliners = grp[grp[:,IDX_ROLE]==1]\n    defense = grp[grp[:,IDX_OFFENSE]==0, :2] # only interested in X,Y\n    TE = grp[grp[:,IDX_ROLE]==2]\n    BC = grp[grp[:,IDX_ROLE]==3][0]\n    BCx = BC[IDX_X]\n\n    oA = oliners[0]\n    oB = oliners[-1]\n\n    if TE.shape[0] == 0:\n        # Extend both sides:\n        temp = oB.copy(); temp[IDX_Y] += 2; grp = np.concatenate([grp,temp.reshape(1,-1)], axis=0)\n        temp = oA.copy(); temp[IDX_Y] -= 2; grp = np.concatenate([temp.reshape(1,-1), grp], axis=0)\n\n    else:\n        TE = TE[0]\n        d = euclidean_distance(TE[0], TE[1], oA[0], oA[1])\n        if d<4:\n            # TE is on the low side, extend the high side\n            TE[IDX_ROLE] = 1\n            temp = oB.copy(); temp[IDX_Y] += 2;\n            grp = np.concatenate([TE.reshape(1,-1),grp,temp.reshape(1,-1)], axis=0)\n\n        else:\n            d = euclidean_distance(TE[0], TE[1], oB[0], oB[1])\n\n            if d<4:\n                # TE is on the hi side, extend the low side\n                TE[IDX_ROLE] = 1\n                temp = oA.copy(); temp[IDX_Y] -= 2;\n                grp = np.concatenate([temp.reshape(1,-1),grp,TE.reshape(1,-1)], axis=0)\n\n    # Update:\n    oliners = grp[grp[:,IDX_ROLE]==1]\n\n    oline_length = np.ptp(oliners[:, IDX_Y])\n    oline_bline_area, oline_bline_num_def, oline_num_def_inbox = 0,0,0\n    oline_bline_num_def_engaged = 0\n    oline_num_def_safety = (\n        (defense[:, IDX_X] >= BCx+10) &\n        (defense[:, IDX_X] <= BCx+20) &\n        (defense[:, IDX_Y] >= oliners[0, IDX_Y]) &\n        (defense[:, IDX_Y] <= oliners[oliners.shape[0]-1, IDX_Y])\n    ).sum()\n\n    # show_play_std(play_id=pid, train=pre, displayit=False)\n\n    oline_avg_ol_dist = []\n    for idx, olA in enumerate(oliners):\n        if idx == oliners.shape[0] - 2: break\n        olB = oliners[idx+1]\n        oline_avg_ol_dist.append(euclidean_distance(olA[0], olA[1], olB[0], olB[1]))\n    oline_avg_ol_dist = np.array(oline_avg_ol_dist).mean()\n\n    # These are sorted ascending\n    for idx in range(oliners.shape[0]-1):\n        man1 = oliners[idx]\n        man2 = oliners[idx+1]\n\n        man1x, man1y = man1[IDX_X], man1[IDX_Y]\n        man2x, man2y = man2[IDX_X], man2[IDX_Y]\n\n        # Area has two components; the square piece, and the triangle piece\n        if man2x<man1x:\n            MIN_MAN_X = man2x\n            MAX_MAN_X = man1x\n            min_man = man2\n            max_man = man1\n        else:\n            MIN_MAN_X = man1x\n            MAX_MAN_X = man2x\n            min_man = man1\n            max_man = man2\n\n        h = man2y - man1y\n        oline_bline_area += h * max(0, MIN_MAN_X - BCx) # square piece\n        oline_bline_area += h * abs(man2x - man1x) / 2  # triangle piece\n\n        square_piece = defense[\n            (defense[:, IDX_X] >= BCx) &\n            (defense[:, IDX_X] <= MIN_MAN_X) &\n            (defense[:, IDX_Y] >= man1y) &\n            (defense[:, IDX_Y] <= man2y)\n        ]\n        oline_bline_num_def += square_piece.shape[0] # Square piece\n\n        for defplayer in defense:\n            # Triangle piece\n            oline_bline_num_def += int(PointInTriangleFlat(\n                BC,\n                MIN_MAN_X, min_man[IDX_Y],\n                MIN_MAN_X, max_man[IDX_Y],\n                MAX_MAN_X, max_man[IDX_Y]\n            ))\n\n        oline_num_def_inbox += (\n            # Only Square piece\n            (defense[:, IDX_X] >= BCx) &\n            (defense[:, IDX_X] <= BCx + 10) &\n            (defense[:, IDX_Y] >= man1y) &\n            (defense[:, IDX_Y] <= man2y)\n        ).sum()\n\n        # For engaged part, we only check the square portion\n        # For the triangle portion, we hope dist<4 to the closer guy\n        for defplayer in square_piece:\n            d = euclidean_distance(\n                defplayer[0], defplayer[1],\n                man1x, man1y\n            )\n            if d < 1.5:\n                oline_bline_num_def_engaged += 1\n                continue\n\n            d = euclidean_distance(\n                defplayer[0], defplayer[1],\n                man2x, man2y\n            )\n            if d < 1.5:\n                oline_bline_num_def_engaged += 1\n\n    return [\n        oline_length,\n        oline_bline_area,\n        oline_bline_num_def,\n        oline_num_def_inbox,\n        oline_bline_num_def_engaged,\n        oline_num_def_safety,\n        oline_avg_ol_dist\n    ]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fast Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"def fast_preprocess(df, means={}, isTrain=False, no_fe=False):\n    global MIN_CLIP,MAX_CLIP\n    seconds_in_year = 60*60*24*365.25\n\n    t = time()\n    \n    dirs = {'ARI':'ARZ', 'BAL':'BLT', 'CLE':'CLV', 'HOU':'HST'}\n    for bad,good in dirs.items():\n        df.loc[df.VisitorTeamAbbr==bad, 'VisitorTeamAbbr'] = good\n        df.loc[df.HomeTeamAbbr==bad, 'HomeTeamAbbr'] = good\n        \n    df['IsToLeft'] = df.PlayDirection == 'left'\n    df['IsRusher'] = df.NflId == df.NflIdRusher\n    \n    df['TeamOnOffense'] = \"home\"\n    df.loc[df.PossessionTeam != df.HomeTeamAbbr, 'TeamOnOffense'] = 'away'\n    df['IsOnOffense'] = df.Team == df.TeamOnOffense\n    del df['TeamOnOffense']\n    \n    # Used in some downstream calcs\n    df.FieldPosition.replace({np.nan:''}, inplace=True)\n    mask = df.FieldPosition == df.PossessionTeam\n    df['YardLine_std'] = 110 - df.YardLine\n    df.loc[mask, 'YardLine_std'] = 10 + df.loc[mask,'YardLine']\n    df.YardLine = df.YardLine_std - 10\n    del df['YardLine_std']\n    \n    df['X_std'] = df.X\n    df['Y_std'] = df.Y\n    df.loc[df.IsToLeft, 'X_std'] = 120 - df.loc[df.IsToLeft, 'X']\n    df.loc[df.IsToLeft, 'Y_std'] = 160/3 - df.loc[df.IsToLeft, 'Y'] \n    df.X = df.X_std - 10\n    df.Y = df.Y_std\n    del df['X_std'], df['Y_std']\n\n    # ROTATE THIS 90 to the right\n    df.loc[df.IsToLeft, 'Dir'] = np.mod(180 + df[df.IsToLeft].Dir, 360)\n    df.Dir = np.mod(df.Dir/180*math.pi + 3*math.pi/2, math.pi*2)\n\n    # Either do this or keep season as a feature:\n    # https://www.kaggle.com/c/nfl-big-data-bowl-2020/discussion/113277#latest-662092\n    df.loc[df.Season==2019,'Season'] = 2018\n    mask = df.Season == 2017\n    df.loc[mask, 'Orientation'] = np.mod(math.pi/2 + df.loc[mask].Orientation, math.pi*2)\n    \n    # Correct 2017 Distributions for A and S separately...\n    rusher2017_A = [2.54, 0.8112192479993628, 1.0391018448524394, 2.7]\n    rusher2017_S = [3.84, 1.074601007663326, 1.1184611205650106, 4.54]\n    no_rusher2017_A = [1.43, 0.8586214845778048, 1.034906128189814, 1.56]\n    no_rusher2017_S = [2.24, 1.258396412289438, 1.412108400649438, 2.54]\n\n    df.loc[mask & df.IsRusher, 'S'] = (df.S[mask & df.IsRusher] - rusher2017_S[0]) / rusher2017_S[1] * rusher2017_S[2] + rusher2017_S[3]\n    df.loc[mask & df.IsRusher, 'A'] = (df.A[mask & df.IsRusher] - rusher2017_A[0]) / rusher2017_A[1] * rusher2017_A[2] + rusher2017_A[3]    \n    df.loc[mask & ~df.IsRusher, 'S'] = (df.S[mask & ~df.IsRusher] - no_rusher2017_S[0]) / no_rusher2017_S[1] * no_rusher2017_S[2] + no_rusher2017_S[3]\n    df.loc[mask & ~df.IsRusher, 'A'] = (df.A[mask & ~df.IsRusher] - no_rusher2017_A[0]) / no_rusher2017_A[1] * no_rusher2017_A[2] + no_rusher2017_A[3]\n    \n    if no_fe:\n        return df\n    \n    # Rusher Features:\n    features = [\n        'GameId','PlayId','NflIdRusher','X','Y','Dir',\n        'YardLine','Season','S','A','Dis',\n    ]\n    if isTrain: features += ['Yards']\n    rushers = df[df.IsRusher][features].copy()\n    \n    ##########################################################################################\n    # Voroni features of Rusher VS Offense\n    vf = vfeats(df[df.IsRusher | ~df.IsOnOffense][['PlayId','IsRusher','X','Y']].copy())\n    vf.columns = ['PlayId','vArea']\n    rushers = rushers.merge(vf, how='left',on='PlayId')\n    if isTrain: print(time()-t, 'Done Veroni'); t=time()\n \n    ##########################################################################################\n    \n    # Influence (Distance) Features:\n    df['XY'] = df[['X','Y']].apply(lambda x: x.tolist(), axis=1)\n    \n    offp_coords = df[df.IsOnOffense==True][['PlayId','XY']].groupby('PlayId').agg(list).reset_index()\n    defp_coords = df[df.IsOnOffense==False][['PlayId','XY']].groupby('PlayId').agg(list).reset_index()\n    ball_coords = df[df.IsRusher==True][['PlayId','XY','A']].groupby(['PlayId','A']).agg(list).reset_index()\n\n    offp_coords.columns = ['PlayId', 'offp_coords']\n    defp_coords.columns = ['PlayId', 'defp_coords']\n    ball_coords.columns = ['PlayId', 'A', 'ball_coords']\n\n    ball_coords.ball_coords = ball_coords.ball_coords.apply(lambda x: np.array(x[0]))\n    defp_coords.defp_coords = defp_coords.defp_coords.apply(np.array)\n    offp_coords.offp_coords = offp_coords.offp_coords.apply(np.array)\n\n    ball_coords = ball_coords.merge(defp_coords, how='left', on='PlayId')\n    ball_coords = ball_coords.merge(offp_coords, how='left', on='PlayId')\n\n    InfluenceRusherX = ball_coords.groupby('PlayId').apply(PlayerInfluence).reset_index()\n    InfluenceRusherX.columns = ['PlayId', 'InfluenceRusherX']\n    InfluenceRusherX['InfluenceRusherX_flip'] = InfluenceRusherX.InfluenceRusherX.apply(lambda x: np.argmax(-np.sign(x)))\n    rushers = rushers.merge(InfluenceRusherX[['PlayId','InfluenceRusherX_flip']], how='left', on='PlayId')\n\n    def triangleCongestion(grp):\n        p0 = grp.ball_coords.values[0]\n\n        A = 10 - 5 * (min(8, grp.A.values[0]) / 8)\n        offp_coords = grp.offp_coords.values[0]\n        defp_coords = grp.defp_coords.values[0]\n\n        p1, p2 = p0.copy(), p0.copy()\n        p1[0] += 20; p1[1] -= A\n        p2[0] += 20; p2[1] += A\n        \n        def_conjestion = 0\n            \n        for p in defp_coords:\n            if not PointInTriangle(p, p0, p1, p2): continue\n            def_conjestion += 1\n            \n        return def_conjestion\n        \n    triCon = ball_coords.groupby('PlayId').apply(triangleCongestion).reset_index()\n    triCon.columns = ['PlayId', 'def_triCon']\n    rushers = rushers.merge(triCon[['PlayId','def_triCon']], how='left', on='PlayId')\n    \n    del offp_coords, defp_coords, ball_coords, df['XY']\n    if isTrain: print(time()-t, 'Done Influence'); t=time()\n    #########################################################################################\n\n    \n    # TODO: Consider this one...\n    rushers['rusher_dist_td'] = 99 - rushers.YardLine # We use YardLine so we can update yards...\n    rushers['rusher_dist_scrimmage'] = rushers.YardLine - rushers.X\n    rushers['rusher_moving_back'] = rushers.Dir.between(np.pi/2, 3*np.pi/2).astype(np.int)\n    rushers = rushers.rename(columns={\n        'X':'rusher_X',\n        'Y':'rusher_Y',\n    })\n    features = [        \n        'GameId','PlayId','NflIdRusher','rusher_X','rusher_Y','Dir',\n        'S','A','Dis','YardLine','Season',\n        'rusher_dist_scrimmage','rusher_moving_back','rusher_dist_td',\n        'vArea','InfluenceRusherX_flip','def_triCon',\n    ]\n    \n    if isTrain:\n        features += ['Yards','TYards','Orig_Yards_NOTRAIN',]\n        rushers['TYards'] = (\n            # How far I'm going to move from my current position:\n            rushers.Yards + rushers.rusher_dist_scrimmage.astype(int)\n        ).clip(MIN_CLIP, MAX_CLIP)\n        rushers['Orig_Yards_NOTRAIN'] = rushers.Yards.copy()\n        rushers.Yards = rushers.Yards.clip(MIN_CLIP,MAX_CLIP)\n        \n    rushers = rushers[features]\n    \n    ##########################################################################################\n    # OLine Features: https://www.kaggle.com/sherkt1/final-hole-metric-features-v5-0\n    '''\n        oline_length, y length of the offensive line\n        oline_bline_area, area of the line under the curve\n        oline_bline_num_def, number of defensive players who've penetrated the curve\n        oline_num_def_inbox, number of defensive players within the box (rusher.x+10)\n        oline_bline_num_def_engaged, number of defensive players who've penetrated the curve and are within 1.5 yards of OLman\n        oline_num_def_safety, number of defensers in backfield rusher.x+[10,20]\n        oline_avg_ol_dist, mean distance between consecutive OLmen\n    '''\n    if isTrain: print(time()-t, 'Preparing OLine'); t=time()\n    players = df[[\n        # select oline features here\n        'PlayId','IsOnOffense','X','Y','Position','IsRusher'\n    ]].copy()\n    players['role'] = players.Position.isin('T,G,C,OT,OG'.split(',')).astype(np.uint8)\n    players.loc[players.Position=='TE','role'] = 2\n    players.loc[players.IsRusher,'role'] = 3\n    del players['Position'], players['IsRusher'] # role = 1 for OLine, 2 for TE, 0 for other\n    players.IsOnOffense = players.IsOnOffense.astype(np.uint8)\n    players.sort_values(['IsOnOffense','Y'], inplace=True)\n    oline_feats = players.groupby('PlayId').apply(build_oline_features).reset_index()\n    del players\n    oline_feats.columns = ['PlayId', 'olinef']\n\n    # Merge relevant features\n    oline_feats['oline_length'] = oline_feats.olinef.apply(lambda x: x[0])\n    oline_feats['oline_bline_area'] = oline_feats.olinef.apply(lambda x: x[1])\n    oline_feats['oline_bline_num_def'] = oline_feats.olinef.apply(lambda x: x[2])\n    oline_feats['oline_num_def_inbox'] = oline_feats.olinef.apply(lambda x: x[3])\n    oline_feats['oline_bline_num_def_engaged'] = oline_feats.olinef.apply(lambda x: x[4])\n    oline_feats['oline_num_def_safety'] = oline_feats.olinef.apply(lambda x: x[5])\n    oline_feats['oline_avg_ol_dist'] = oline_feats.olinef.apply(lambda x: x[6])\n    rushers = rushers.merge(\n        oline_feats[[\n            'PlayId', 'oline_length',\n            'oline_bline_area', 'oline_bline_num_def', 'oline_num_def_inbox',\n            'oline_bline_num_def_engaged', 'oline_num_def_safety', 'oline_avg_ol_dist',\n        ]],\n        how='left', on='PlayId'\n    )\n    if isTrain: print(time()-t, 'Done OLine'); t=time()\n    ##########################################################################################\n    \n    # Max Observed rusher speed, truncated at median\n    if 'RusherMaxObservedS' not in means:\n        medianS = rushers.S.median()\n        RusherMaxObservedS = rushers.groupby('NflIdRusher').S.median().reset_index()\n        RusherMaxObservedS = {NflIdRusher:S for NflIdRusher, S in RusherMaxObservedS.values}\n        means['RusherMaxObservedS'] = RusherMaxObservedS\n        means['RusherMaxObservedS_Median'] = medianS\n    rushers['RusherMaxObservedS'] = rushers.NflIdRusher.map(means['RusherMaxObservedS']).fillna(means['RusherMaxObservedS_Median'])\n    rushers.loc[rushers.RusherMaxObservedS<means['RusherMaxObservedS_Median'], 'RusherMaxObservedS'] = means['RusherMaxObservedS_Median']\n    \n    \n    rushers['dist_next_point_time'] = np.square(rushers.S) + 2 * rushers.A * rushers.Dis\n    rushers['rusher_SX'] = rushers.S * np.cos(rushers.Dir)\n    rushers['rusher_SY'] = rushers.S * np.sin(rushers.Dir)\n    rushers.rename(columns={'S':'rusher_S','A':'rusher_A'}, inplace=True)\n    if isTrain: print(time()-t, 'Advanced Rusher'); t=time()\n    \n    \n    # Player distances relative to the RB's\n    player_dist = df[~df.IsRusher][['PlayId','NflId','X','Y','S','A','IsOnOffense','PlayerWeight','Dir']]\n    player_dist['SX'] = player_dist.S * np.cos(player_dist.Dir)\n    player_dist['AX'] = player_dist.A * np.cos(player_dist.Dir)\n    player_dist['WX'] = player_dist.PlayerWeight * np.cos(player_dist.Dir)\n    player_dist = player_dist.merge(rushers, on='PlayId', how='inner')\n    player_dist['dist_to_rusher'] = player_dist[['X','Y','rusher_X','rusher_Y']].apply(lambda x: euclidean_distance(x[0],x[1],x[2],x[3]), axis=1)\n    \n    defensivePlayers = player_dist[player_dist.IsOnOffense==False]\n    offensivePlayers = player_dist[player_dist.IsOnOffense==True]\n    \n    # For the rest of the analysis, we dont count the QB or any other offensive player behind the RB\n    offensivePlayers = offensivePlayers[offensivePlayers.X>offensivePlayers.rusher_X]\n    \n    SX1 = offensivePlayers.groupby('PlayId').SX.mean().sort_index().reset_index(); SX1.columns=['PlayId','SX1']\n    SX2 = defensivePlayers.groupby('PlayId').SX.mean().sort_index().reset_index(); SX2.columns=['PlayId','SX2']\n    SX1 = SX1.merge(SX2, how='left', on='PlayId'); SX1.SX1 += SX1.SX2; del SX1['SX2']; SX1.columns=['PlayId','Mean_SX']\n    rushers = rushers.merge(SX1, how='left', on='PlayId')\n    \n    AX1 = offensivePlayers.groupby('PlayId').AX.mean().sort_index().reset_index(); AX1.columns=['PlayId','AX1']\n    AX2 = defensivePlayers.groupby('PlayId').AX.mean().sort_index().reset_index(); AX2.columns=['PlayId','AX2']\n    AX1 = AX1.merge(AX2, how='left', on='PlayId'); AX1.AX1 += AX1.AX2; del AX1['AX2']; AX1.columns=['PlayId','Mean_AX']\n    rushers = rushers.merge(AX1, how='left', on='PlayId')\n\n    WX1 = offensivePlayers.groupby('PlayId').WX.mean().sort_index().reset_index(); WX1.columns=['PlayId','WX1']\n    WX2 = defensivePlayers.groupby('PlayId').WX.mean().sort_index().reset_index(); WX2.columns=['PlayId','WX2']\n    WX1 = WX1.merge(WX2, how='left', on='PlayId'); WX1.WX1 += WX1.WX2; del WX1['WX2']; WX1.columns=['PlayId','Mean_WX']\n    rushers = rushers.merge(WX1, how='left', on='PlayId')\n    \n    del AX1, AX2, SX1, SX2, WX1, WX2\n    ###############################################################\n    \n    # Count how many defenders are on the side of the field the rusher is moving towards\n    # relative to the rusher:\n    defense = defensivePlayers.groupby('PlayId').Y.agg(list).reset_index()\n    defense.columns = ['PlayId', 'defense_Y']\n\n    rushers = rushers.merge(defense, on='PlayId', how='left')\n    rushers.defense_Y = rushers[['defense_Y','rusher_Y','rusher_SY']].apply(lambda row: sum([1 for dy in row.defense_Y if np.sign(row.rusher_SY)==np.sign(dy - row.rusher_Y)]), axis=1)\n    \n    if isTrain: print(time()-t, 'Done OffDef'); t=time()\n    ###############################################################\n    \n    if isTrain: print(time()-t, 'Done IX'); t=time()\n\n    closest_def = defensivePlayers[['PlayId','dist_to_rusher','S','A','X']].sort_values(['PlayId','dist_to_rusher'])\n\n    def cdefagg_S(row):\n        return row.sort_values('dist_to_rusher').S.values[:3].mean()\n    def cdefagg_A(row):\n        return row.sort_values('dist_to_rusher').A.values[:3].mean()\n    def cdefagg_X(row):\n        return row.sort_values('dist_to_rusher').X.values[:3].mean()\n    \n    cdef = defensivePlayers[['PlayId','dist_to_rusher','S']].groupby('PlayId').apply(cdefagg_S).reset_index()\n    cdef.columns = ['PlayId', 'closest_def_S']\n    rushers = rushers.merge(cdef, how='left', on='PlayId')\n    \n    cdef = defensivePlayers[['PlayId','dist_to_rusher','A']].groupby('PlayId').apply(cdefagg_A).reset_index()\n    cdef.columns = ['PlayId', 'closest_def_A']\n    rushers = rushers.merge(cdef, how='left', on='PlayId')\n    \n    # Stats on the location and spread of the defense along the scrimmage line\n    grp = defensivePlayers.groupby('PlayId').Y.agg(['mean','std']).reset_index()\n    grp.columns = ['PlayId','defense_scrimmage_Y_mean','defense_scrimmage_Y_std']\n    rushers = rushers.merge(grp, how='left', on='PlayId')\n    rushers['rusher_defense_scrimmage_Y_dist'] = rushers.rusher_Y - rushers.defense_scrimmage_Y_mean\n    if isTrain: print(time()-t, 'Done closest def'); t=time()\n\n\n    def offense_mid6(x):\n        # For offensive players, we don't care about the QB and players very far from us\n        # So we sort by distance, skip the closest guy (QB) then take the next 5 closest team mates\n        # We already filter to include only ppl with X>rusherX\n        subset = np.sort(x)[:8]\n        return [subset.min(), subset.mean(), subset.max(), subset.std()]\n\n    \n    grp = player_dist[player_dist.IsOnOffense==True].groupby('PlayId')\n    offensive_dist = grp.dist_to_rusher.agg(offense_mid6).reset_index()\n    offensive_dist['min_off5_dist_to_rusher'] = offensive_dist.dist_to_rusher.apply(lambda x: x[0])\n    offensive_dist['avg_off5_dist_to_rusher'] = offensive_dist.dist_to_rusher.apply(lambda x: x[1])\n    offensive_dist['max_off5_dist_to_rusher'] = offensive_dist.dist_to_rusher.apply(lambda x: x[2])\n    offensive_dist['std_off5_dist_to_rusher'] = offensive_dist.dist_to_rusher.apply(lambda x: x[3])\n    del offensive_dist['dist_to_rusher']\n    rushers = rushers.merge(offensive_dist, on='PlayId', how='inner')\n    del offensive_dist\n\n    \n    def defense_closest9(x):\n        # For defensive players, we don't care about the Defensive Backs.\n        # Just the ppl closest to the Rusher\n        subset = np.sort(x)[:9]\n        subset = x\n        return [subset.min(), subset.mean(), subset.max(), subset.std()]\n    \n    grp = player_dist[player_dist.IsOnOffense==False].groupby('PlayId')\n    defensive_dist = grp.dist_to_rusher.agg(defense_closest9).reset_index()\n    defensive_dist['min_def9_dist_to_rusher'] = defensive_dist.dist_to_rusher.apply(lambda x: x[0])\n    defensive_dist['avg_def9_dist_to_rusher'] = defensive_dist.dist_to_rusher.apply(lambda x: x[1])\n    defensive_dist['max_def9_dist_to_rusher'] = defensive_dist.dist_to_rusher.apply(lambda x: x[2])\n    defensive_dist['std_def9_dist_to_rusher'] = defensive_dist.dist_to_rusher.apply(lambda x: x[3])\n    \n    def odist(x):\n        return np.sort(x).tolist()\n    temp = grp.dist_to_rusher.agg(odist).reset_index()\n    for i in range(9):\n        defensive_dist[f'def9_dist_to_rusher_{i}'] = temp.dist_to_rusher.apply(lambda x: x[i])\n    \n    del defensive_dist['dist_to_rusher']\n    rushers = rushers.merge(defensive_dist, on='PlayId', how='inner')\n    del defensive_dist    \n    \n    if isTrain: print(time()-t, 'dist_to_rusher'); t=time()\n    \n    ######    ######    ######    ######    ######    ######    ######    ######    ######    ######    ######\n    #TODO\n    # - fastest accelerating defenders acceleration\n    # \t- dist to this person\n    def defense_fA(row):\n        # For defensive players, we don't care about the Defensive Backs.\n        # Just the ppl closest to the Rusher\n        subset = row.sort_values('A', ascending=False).iloc[0]\n        return [subset.A, subset.dist_to_rusher]\n    defensive_fA = grp[['dist_to_rusher','A']].apply(defense_fA).reset_index()\n    defensive_fA.rename(columns={0:'temp'}, inplace=True)\n    defensive_fA['defensive_fA_maxA'] = defensive_fA.temp.apply(lambda x: x[0])\n    defensive_fA['defensive_fA_dist'] = defensive_fA.temp.apply(lambda x: x[1])\n    del defensive_fA['temp']\n    rushers = rushers.merge(defensive_fA, on='PlayId', how='inner')\n    del defensive_fA\n    if isTrain: print(time()-t, 'Done A'); t=time()\n    \n    # - fastest speeding defenders speed\n    # - dist to this person\n    def defense_fS(row):\n        # For defensive players, we don't care about the Defensive Backs.\n        # Just the ppl closest to the Rusher\n        subset = row.sort_values('S', ascending=False).iloc[0]\n        return [subset.S, subset.dist_to_rusher]\n    defensive_fS = grp[['dist_to_rusher','S']].apply(defense_fS).reset_index()\n    defensive_fS.rename(columns={0:'temp'}, inplace=True)\n    defensive_fS['defensive_fS_maxS'] = defensive_fS.temp.apply(lambda x: x[0])\n    defensive_fS['defensive_fS_dist'] = defensive_fS.temp.apply(lambda x: x[1])\n    del defensive_fS['temp']\n    rushers = rushers.merge(defensive_fS, on='PlayId', how='inner')\n    del defensive_fS\n    if isTrain: print(time()-t, 'Done B'); t=time()\n    \n    \n    # - closest 9 defenders stats on A, and stats on S\n    def defense_closest9_A(row):\n        # For defensive players, we don't care about the Defensive Backs.\n        # Just the ppl closest to the Rusher\n        subset = row.sort_values('dist_to_rusher').A.iloc[:9]\n        return [subset.min(), subset.mean(), subset.max(), subset.std()]\n    defensive_A = grp[['dist_to_rusher','A']].apply(defense_closest9_A).reset_index()\n    defensive_A.rename(columns={0:'temp'}, inplace=True)\n    defensive_A['min_def9_A'] = defensive_A.temp.apply(lambda x: x[0])\n    defensive_A['avg_def9_A'] = defensive_A.temp.apply(lambda x: x[1])\n    defensive_A['max_def9_A'] = defensive_A.temp.apply(lambda x: x[2])\n    defensive_A['std_def9_A'] = defensive_A.temp.apply(lambda x: x[3])\n    del defensive_A['temp']\n    rushers = rushers.merge(defensive_A, on='PlayId', how='inner')\n    del defensive_A\n    if isTrain: print(time()-t, 'Done C'); t=time()\n    \n    def defense_closest9_S(row):\n        # For defensive players, we don't care about the Defensive Backs.\n        # Just the ppl closest to the Rusher\n        subset = row.sort_values('dist_to_rusher').S.iloc[:9]\n        return [subset.min(), subset.mean(), subset.max(), subset.std()]\n    defensive_S = grp[['dist_to_rusher','S']].apply(defense_closest9_S).reset_index()\n    defensive_S.rename(columns={0:'temp'}, inplace=True)\n    defensive_S['min_def9_S'] = defensive_S.temp.apply(lambda x: x[0])\n    defensive_S['avg_def9_S'] = defensive_S.temp.apply(lambda x: x[1])\n    defensive_S['max_def9_S'] = defensive_S.temp.apply(lambda x: x[2])\n    defensive_S['std_def9_S'] = defensive_S.temp.apply(lambda x: x[3])\n    del defensive_S['temp']\n    rushers = rushers.merge(defensive_S, on='PlayId', how='inner')\n    del defensive_S\n    if isTrain: print(time()-t, 'Done D'); t=time()\n    \n    # Log transforms\n    for col in 'dist_next_point_time,rusher_dist_from_center_Y,min_off5_dist_to_rusher,max_def9_dist_to_rusher,std_def9_dist_to_rusher,defense_centroid_to_rusher_dist,offense_centroid_to_rusher_dist,min_def9_S,avg_def9_S,closest_def_A,closest_def_S,closest_def_S,vArea,std_def9_S,ix_std_dist,RusherMaxObservedS'.split(','):\n        if col in rushers.columns:\n            rushers[col] = np.log1p(rushers[col])\n\n            \n    ###################\n    def inrange(grp):\n        return grp.Y.between(grp.rusher_Y-5, grp.rusher_Y+5).sum()\n    \n    temp = player_dist.groupby('PlayId').apply(inrange).reset_index()\n    temp.columns = ['PlayId', 'Congestion']\n    rushers = rushers.merge(temp, how='left', on='PlayId')\n    \n    \n    # TODO: \n    # Log transforms on ix_mean_dist,ix_median_dist,ix_max_dist\n    return rushers, means","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # IMPORTNAT EDA TO CORRECT A/S VALUES FOR 2017:\n# col='A' # do for A,S, and do for IsRusher and !isRusher...\n\n# rushers = train[train.NflId!=train.NflIdRusher].copy()\n# std2017 = rushers.loc[rushers.Season==2017,col].std()\n# std2018 = rushers.loc[rushers.Season==2018,col].std()\n# mean2017 = rushers.loc[rushers.Season==2017,col].mean()\n# mean2018 = rushers.loc[rushers.Season==2018,col].mean()\n# median2017 = rushers.loc[rushers.Season==2017,col].median()\n# median2018 = rushers.loc[rushers.Season==2018,col].median()\n\n# std20189 = rushers.loc[rushers.Season==2018,col].append(test[test.NflId!=test.NflIdRusher][col]).std()\n# median20189 = rushers.loc[rushers.Season==2018,col].append(test[test.NflId!=test.NflIdRusher][col]).median()\n# mean20189 = rushers.loc[rushers.Season==2018,col].append(test[test.NflId!=test.NflIdRusher][col]).mean()\n\n# rushers.loc[rushers.Season==2017,col] = (rushers.loc[rushers.Season==2017,col] - median2017) / std2017 * std20189 + median20189\n\n\n# plt.figure(figsize=(8,5))\n# plt.title(col)\n\n# sns.distplot(rushers[(rushers.Season==2017)][col], label=\"2017\")\n# sns.distplot(rushers[(rushers.Season==2018)][col], label=\"2018\")\n# sns.distplot(test[test.NflId!=test.NflIdRusher][col], label=\"2019\")\n# plt.legend(prop={'size': 12})\n# plt.show()\n\n# print(f'{col}, {[median2017, std2017, std20189, median20189]}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()\nwhat, means = fast_preprocess(train.copy(), isTrain=True)\nwhat.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"z,y = what.isna().sum(), what.isna().sum()\nz[z>0], y[y>0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg_cols = [\n    # Potential Overfitting:\n    'Season',\n    \n    'rusher_A', 'rusher_SX', 'avg_def9_dist_to_rusher',\n    'rusher_dist_scrimmage', 'rusher_S',\n    'avg_off5_dist_to_rusher', 'min_off5_dist_to_rusher',\n    'dist_next_point_time',\n    'min_def9_dist_to_rusher', 'std_off5_dist_to_rusher',\n    'defense_scrimmage_Y_std', 'std_def9_dist_to_rusher', 'Dis',\n    'closest_def_S', 'closest_def_A',\n    'defense_Y', 'InfluenceRusherX_flip',\n    'vArea', 'avg_def9_A', 'avg_def9_S', 'std_def9_S', 'min_def9_S',\n    # These are good, but we don't wanna insert 1000 of them:\n    # 'def9_dist_to_rusher_0',\n    # 'def9_dist_to_rusher_1', 'def9_dist_to_rusher_2',\n    # 'def9_dist_to_rusher_3', 'def9_dist_to_rusher_4',\n    # 'def9_dist_to_rusher_5', 'def9_dist_to_rusher_6',\n    # 'def9_dist_to_rusher_7', 'def9_dist_to_rusher_8',\n    \n    'RusherMaxObservedS',\n    'Mean_SX','Mean_AX','Mean_WX',\n    'Congestion', 'def_triCon',\n    'oline_length', 'oline_bline_area', 'oline_num_def_inbox', 'oline_avg_ol_dist',\n    \n    # New Experimental!!!:\n\n]\nlen(reg_cols)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# # ALWAYS DOUBLE CHECK:\n# if RUN_KAGGLE == False:\n#     for col in reg_cols:\n#         plt.title(col)\n#         plt.hist(what[col], 100)\n#         plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()\nX = what.copy()\n\n# transformed for mse, we should also standardize....\nymu = np.log1p(X.Yards.values - MIN_CLIP) # we zero it for sparse xent\n\nyent = np.zeros((X.Yards.shape[0], N_OUTPUTS))\nfor idx, target in enumerate(list(X.Yards.astype(int))):\n    yent[idx, target-MIN_CLIP] = 1\n    \nyent_NOCLIP_CSUM = np.zeros((X.Orig_Yards_NOTRAIN.shape[0], 199))\nfor idx, target in enumerate(list(X.Orig_Yards_NOTRAIN.astype(int))):\n    yent_NOCLIP_CSUM[idx, target+99] = 1\nyent_NOCLIP_CSUM = np.clip(np.cumsum(yent_NOCLIP_CSUM, axis=1), 0, 1)\n    \nstdscale = StandardScaler()\nstdscale.fit(X[reg_cols])","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"keys = X.Yards.unique()-MIN_CLIP\nvalues = class_weight.compute_class_weight('balanced', keys, X.Yards.values-MIN_CLIP)\ncweights = dict(zip(keys, values))\ncweights\n# cweights = None","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# UNDERSTAND OUR TARGETS\n\nidx = 1\norig_mu = X.Yards.values[idx] - MIN_CLIP\nplt.title(f'mu:{ymu[idx]:.2f}, omu:{orig_mu}')\nplt.plot(yent[idx])\nplt.show()\n\nplt.title('Yards')\nplt.hist(X.Yards,100)\nplt.show()\n\nprint(ymu.min(), ymu.max())\nplt.title('ymu')\nplt.hist(ymu,100)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import regularizers\n\ndef BuildModel(divisor=1):\n    global N_OUTPUTS\n    inp = Input(shape=(len(reg_cols),))\n    x = inp\n    x = GaussianNoise(0.0025)(x)\n    \n    x = Dense(1024//divisor, activation='elu')(x) \n    x = Dropout(0.25)(x)\n    x = Dense(256//divisor, activation='elu')(x)\n    x = Dense(128//divisor, activation='elu')(x)\n    x = Dense(64//divisor, activation='elu')(x)\n    x = BatchNormalization()(x)\n    \n    # mu in raw space [0,N_OUTPUTS)\n    # We don't have to add/sub Min_Clip because we're already in raw space starting from 0\n    # So we just apply the log1p transform\n    mu_raw = Dense(1, activation='relu')(x)\n    mu = Lambda(lambda x: K.log(1 + K.reshape(x[:,0],(-1,1))), name='mu')(mu_raw)\n    \n    ent = Dense(N_OUTPUTS, name='ent', activation='sigmoid')(\n        Concatenate()([\n            x, mu_raw\n        ])\n    )\n    reg_pass = Lambda(lambda x: x, name='reg_pass')(ent) # passthrough for different loss function...\n    \n    model = Model(inp, [ent,reg_pass,mu])\n    model.compile(\n        #optimizer=Nadam(learning_rate=0.0025, beta_1=0.9, beta_2=0.999),\n        optimizer=RAdam(weight_decay=0.0003),\n        loss={\n            'ent': 'categorical_crossentropy', # binary_crossentropy\n            'reg_pass': losses.mae,\n            'mu': losses.mae,\n        },\n        loss_weights={\n            'ent': 1,\n            'reg_pass': 1,\n            'mu': 1,\n        }\n    )\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# GYM"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Downconvert float64 to float32\nfor col, dt in zip(X.columns,X.dtypes):\n    if col in reg_cols and dt=='int64':\n        X[col] = X[col].astype(np.float32)\n        print('Categorical:', col)\n        \n    if dt!='float64': continue\n    X[col] = X[col].astype(np.float32)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"t = time()\nK.clear_session()\n\nif RUN_KAGGLE:\n    kf = KFold(n_splits=n_splits, random_state=42, shuffle=True)\nelse:\n    kf = GroupKFold(n_splits=n_splits)\n    \noof = np.zeros((X.Yards.shape[0], N_OUTPUTS))\noof_mu = np.zeros((X.Yards.shape[0]))\nlgb_oof = np.zeros((X.Yards.shape[0], N_OUTPUTS))\nreport = []\nuse_models = []\nperms = []\n\nfor fold_, (tdx, vdx) in enumerate(kf.split(X, yent, X.GameId)):\n    X_train, X_val = X.loc[tdx].copy(), X.loc[vdx].copy()\n    X_train_bound, X_val_bound = X_train[bounding_cols].copy(), X_val[bounding_cols].copy()\n    \n    y_train_ent, y_train_mu = yent[tdx], ymu[tdx]\n    y_val_ent, y_val_mu = yent[vdx], ymu[vdx]\n    y_train_act = X.Yards[tdx] - MIN_CLIP\n    y_val_act = X.Yards[vdx] - MIN_CLIP\n    y_val_ent = yent[vdx]\n    \n    if DO_LGBM:\n        # Add in a LGM model...\n        trn_data = lgb.Dataset(X_train[lgb_cols], label=y_train_act, categorical_feature=cat_feats, free_raw_data=False)\n        val_data = lgb.Dataset(X_val[lgb_cols], label=y_val_act, categorical_feature=cat_feats, free_raw_data=False)\n        \n        metric = Metric(None, [], []) # placeholder\n        yval199  = metric.convert_to_y199_cumsum(y_val_ent)\n        \n        model_name = f'LGB_{fold_}.lgb'\n            \n        lgb_model = lgb.train(\n            params,\n            trn_data,\n            num_rounds,\n            valid_sets = [trn_data, val_data],\n            verbose_eval=500,\n            early_stopping_rounds = 50,\n        )\n\n        lgb_preds = lgb_model.predict(X_val[lgb_cols], num_iteration=lgb_model.best_iteration)\n        lgb_model.save_model(model_name)\n\n        lgb_preds = metric.bound_prediction(lgb_preds, X_val)\n        lgb_oof[vdx] = lgb_preds.copy()\n        lgb_preds = metric.convert_to_y199_cumsum(lgb_preds)\n        lgb_score_ = crps(lgb_preds, yval199)\n        \n        print(f'(original) LGB RunScore: {lgb_score_}', end='\\n\\n')\n        del trn_data, val_data, yval199; gc.collect()\n        \n        \n    \n    # Transform for NNet: (note, for lgb don't do this)\n    X_train[reg_cols] = stdscale.transform(X_train[reg_cols])\n    X_val[reg_cols]   = stdscale.transform(X_val[reg_cols])\n    \n    # NNet Setup\n    y_train = {'ent':y_train_ent, 'reg_pass':y_train_ent, 'mu':y_train_mu}\n    y_val   = {'ent':y_val_ent,   'reg_pass':y_val_ent,   'mu':y_val_mu  }\n    y_val_act = X.Yards[vdx] - MIN_CLIP\n    \n    \n    \n    \n    \n    \n    \n    # For Drop Importance:\n    y_val_crps = None\n    y_val_crps_orig = yent_NOCLIP_CSUM[vdx]\n    \n    # I say blend the best 3 of 4 runs...\n    oof_mu_runs = np.zeros((n_runs, X_val.shape[0]))\n    oof_runs = np.zeros((n_runs, X_val.shape[0], N_OUTPUTS))\n    oof_run_scores = []\n    for run_ in range(n_runs):\n        print(f'Fold: {fold_}, Run: {run_}')\n\n        model = BuildModel(divisor=DIVISOR)\n        if fold_==0 and run_==0: model.summary()\n        es = EarlyStopping(\n            monitor='val_CRPS',\n            mode='min',\n            restore_best_weights=True, \n            verbose=2, \n            patience=25\n        )\n\n        es.set_model(model)\n        metric = Metric(model, [es], [(X_train[reg_cols],y_train), (X_val[reg_cols],y_val)])\n        metric.set_original_data(X_train_bound, X_val_bound)\n        hist = model.fit(X_train[reg_cols], y_train, class_weight=cweights, callbacks=[metric], epochs=400, batch_size=1024//2, verbose=False)\n        model.save(f'fold_{fold_}_{run_}.h5')\n        score_ = min(hist.history['val_CRPS'])\n        oof_run_scores.append(score_)\n        \n        # Stash OOF:\n        preds, _, preds_mu = model.predict(X_val[reg_cols], batch_size=1024)\n        preds = metric.bound_prediction(preds, X_val_bound)\n        oof_runs[run_] = preds.copy()\n        oof_mu_runs[run_] = preds_mu.flatten().copy()\n        preds = metric.convert_to_y199_cumsum(preds)\n        \n        \n        if y_val_crps is None:\n            y_val_crps = metric.convert_to_y199_cumsum(y_val_ent)\n        \n        orig_score_ = crps(preds, y_val_crps_orig)\n        print(f'RunScore: {score_}, (Original): {orig_score_}')\n\n        if True:#RUN_KAGGLE == False:\n            plt.title(str(score_))\n            plt.plot(hist.history['tr_CRPS'])\n            plt.plot(hist.history['val_CRPS'])\n            plt.show()\n                \n            # Then, we do perm-importance on all folds, not just the top-k\n            perm = {}\n            for idx, feature in enumerate(tqdm(reg_cols)):\n                backup = X_val[feature].values.copy()\n                X_val[feature] = 0\n\n                _y_pred_ent = model.predict(X_val[reg_cols], batch_size=1024)[0]\n                _y_pred_ent = metric.bound_prediction(_y_pred_ent, X_val_bound)\n                _y_pred_ent = metric.convert_to_y199_cumsum(_y_pred_ent)\n                perm[feature] = crps(_y_pred_ent, y_val_crps) - score_\n                \n                X_val[feature] = backup\n            perms.append(perm)\n\n    # Top-k, by minimizing score\n    worst_run = np.argmax(oof_run_scores)\n    bad_model = f'fold_{fold_}_{worst_run}.h5'\n    use_models += [f'fold_{fold_}_{run_}.h5' for run_ in range(n_runs) if run_ != worst_run]\n    \n    # Average the good runs\n    oof[vdx] = np.delete(oof_runs, worst_run, axis=0).mean(axis=0)\n    oof_mu[vdx] = np.delete(oof_mu_runs, worst_run, axis=0).mean(axis=0)\n    \n    # Transform to competition space, only for evaluation\n    preds = metric.convert_to_y199_cumsum(oof[vdx])\n    score_ = crps(preds, y_val_crps)\n    orig_score_ = crps(preds, y_val_crps_orig)\n    print(oof_run_scores, np.array(oof_run_scores).std(), 'STD')\n    report.append(f'Fold: {fold_}, TopK Mean Score: {score_}, (Original): {orig_score_}')\n    print(report[-1], end='\\n\\n')\n    print(''*100)\n    \noof = metric.convert_to_y199_cumsum(oof)\noof_yent = metric.convert_to_y199_cumsum(yent)\nscore_ = crps(oof, oof_yent)\norig_score_ = crps(oof, yent_NOCLIP_CSUM)\nreport.append(f'Final Model Score: {score_}, (Original): {orig_score_}')\n\nif DO_LGBM:\n    lgb_oof = metric.convert_to_y199_cumsum(lgb_oof)\n    total_val = metric.convert_to_y199_cumsum(yent)\n    lgb_score_ = crps(lgb_oof, total_val)\n    print('Final Model Score LGB', lgb_score_)\n\nprint(report[-1])\nprint(time() - t)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Looking at influence in the direction the user is moving rather than +x\n# Also bounding to +15 lookahead instead of |MIN_CLIP|\nreport","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"latest run:\n['Fold: 0, TopK Mean Score: 0.011965370656658841, (Original): 0.012045323200067763',\n 'Fold: 1, TopK Mean Score: 0.012407000801212644, (Original): 0.012463195754473574',\n 'Fold: 2, TopK Mean Score: 0.012510498826329663, (Original): 0.012570269999321933',\n 'Fold: 3, TopK Mean Score: 0.012645285175475428, (Original): 0.012712678162818718',\n 'Fold: 4, TopK Mean Score: 0.011831324078952689, (Original): 0.011899804051099784',\n 'Final Model Score: 0.012271693228860583, (Original): 0.012338055854888747']\n \nbefore deleting `defensive_fS_maxS`\n['Fold: 0, TopK Mean Score: 0.011943059015131515, (Original): 0.012023011561949988',\n 'Fold: 1, TopK Mean Score: 0.012397259426707278, (Original): 0.012453454379968207',\n 'Fold: 2, TopK Mean Score: 0.012501021827250765, (Original): 0.012560793000761227',\n 'Fold: 3, TopK Mean Score: 0.012640834343524985, (Original): 0.01270822733119565',\n 'Fold: 4, TopK Mean Score: 0.011813039764833024, (Original): 0.011881519736820988',\n 'Final Model Score: 0.01225883253833292, (Original): 0.012325195165182443']"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"report","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"testing 8 gkfold\n['Fold: 0, TopK Mean Score: 0.012463722499228199, (Original): 0.012519191270290417',\n 'Fold: 1, TopK Mean Score: 0.01213934659301469, (Original): 0.012210489598103553',\n 'Fold: 2, TopK Mean Score: 0.011721301219140638, (Original): 0.011797676179376862',\n 'Fold: 3, TopK Mean Score: 0.012219630007945197, (Original): 0.012261289080510092',\n 'Fold: 4, TopK Mean Score: 0.012917424462509217, (Original): 0.013054457570300236',\n 'Fold: 5, TopK Mean Score: 0.011798039759125498, (Original): 0.01188823875184244',\n 'Fold: 6, TopK Mean Score: 0.012114994859156996, (Original): 0.012137552398233595',\n 'Fold: 7, TopK Mean Score: 0.01267729378427158, (Original): 0.012713732885856853',\n 'Final Model Score: 0.012256529408419564, (Original): 0.012322892029865959']\n\n\ntesting 10 gkf\n['Fold: 0, TopK Mean Score: 0.011976455857112635, (Original): 0.012012978787960493',\n 'Fold: 1, TopK Mean Score: 0.012526913809200268, (Original): 0.012572557246206772',\n 'Fold: 2, TopK Mean Score: 0.011152096042416758, (Original): 0.01118252500147798',\n 'Fold: 3, TopK Mean Score: 0.012473215510765082, (Original): 0.012616666297376998',\n 'Fold: 4, TopK Mean Score: 0.012261935211583743, (Original): 0.012318446139412615',\n 'Fold: 5, TopK Mean Score: 0.012190408677963678, (Original): 0.01227082807599188',\n 'Fold: 6, TopK Mean Score: 0.012019706425567805, (Original): 0.01204144139032217',\n 'Fold: 7, TopK Mean Score: 0.013298529371208375, (Original): 0.013368111375724318',\n 'Fold: 8, TopK Mean Score: 0.011663117000981117, (Original): 0.011750094510771523',\n 'Fold: 9, TopK Mean Score: 0.012914115850562898, (Original): 0.013006536895049664',\n 'Final Model Score: 0.012248061078110849, (Original): 0.01231442370057316']\n\n956.0809223651886\n\nwould like 10f skf test...."},{"metadata":{},"cell_type":"raw","source":"original\n['Fold: 0, TopK Mean Score: 0.01100204416535311, (Original): 0.012048990356208487',\n 'Fold: 1, TopK Mean Score: 0.01131947137689635, (Original): 0.012513614133691075',\n 'Fold: 2, TopK Mean Score: 0.01113773279603027, (Original): 0.012611363927465967',\n 'Fold: 3, TopK Mean Score: 0.011276261087686533, (Original): 0.012712166550540515',\n 'Fold: 4, TopK Mean Score: 0.010801826234904922, (Original): 0.011923593409519066',\n 'Final Model Score: 0.011107588082379202, (Original): 0.01236175499392975']\n \ndouble maxclip\n['Fold: 0, TopK Mean Score: 0.011766389075786636, (Original): 0.012060268708307441',\n 'Fold: 1, TopK Mean Score: 0.012158071863926371, (Original): 0.012475789484286248',\n 'Fold: 2, TopK Mean Score: 0.012275394247154963, (Original): 0.012609026074109802',\n 'Fold: 3, TopK Mean Score: 0.012344500570671333, (Original): 0.012710814076335073',\n 'Fold: 4, TopK Mean Score: 0.011570760037013392, (Original): 0.011883811341556888',\n 'Final Model Score: 0.012022881297609625, (Original): 0.012347754552581558']\n \nprenorm\n['Fold: 0, TopK Mean Score: 0.011001027226974222, (Original): 0.012047973362748488',\n 'Fold: 1, TopK Mean Score: 0.011292735466690747, (Original): 0.012486878171037663',\n 'Fold: 2, TopK Mean Score: 0.011131376508041576, (Original): 0.012605007584445284',\n 'Fold: 3, TopK Mean Score: 0.011255890992370165, (Original): 0.012691796407860476',\n 'Fold: 4, TopK Mean Score: 0.010770617821669114, (Original): 0.011892384978463623',\n 'Final Model Score: 0.011090459314369578, (Original): 0.01234462618035132']\n \nprenorm double maxclip\n['Fold: 0, TopK Mean Score: 0.011748597668110422, (Original): 0.012042477292013772',\n 'Fold: 1, TopK Mean Score: 0.012173491461181348, (Original): 0.012491209052656504',\n 'Fold: 2, TopK Mean Score: 0.012261845162616272, (Original): 0.012595476955119256',\n 'Fold: 3, TopK Mean Score: 0.012344270883700376, (Original): 0.012710584371332385',\n 'Fold: 4, TopK Mean Score: 0.011558136664549256, (Original): 0.011871187960818132',\n 'Final Model Score: 0.012017136298665174, (Original): 0.012342009533986984']\n \nprenorm tripple maxclip \n['Fold: 0, TopK Mean Score: 0.011960840001576446, (Original): 0.012040792532441112',\n 'Fold: 1, TopK Mean Score: 0.012437348765628395, (Original): 0.012493543712419126',\n 'Fold: 2, TopK Mean Score: 0.012528519654399719, (Original): 0.012588290826471036',\n 'Fold: 3, TopK Mean Score: 0.012638391035275073, (Original): 0.012705784016718477',\n 'Fold: 4, TopK Mean Score: 0.011809672002924553, (Original): 0.011878151972204503',\n 'Final Model Score: 0.012274774887361162, (Original): 0.012341137507639967']\n \nprenorm tripple maxclip  shuffled kfold instead of groupkfold\n['Fold: 0, TopK Mean Score: 0.012197625084998158, (Original): 0.012340735446680899',\n 'Fold: 1, TopK Mean Score: 0.012160198228606403, (Original): 0.012208996384253002',\n 'Fold: 2, TopK Mean Score: 0.01230552853658252, (Original): 0.012326132202299972',\n 'Fold: 3, TopK Mean Score: 0.012250258574287893, (Original): 0.012312069558656697',\n 'Fold: 4, TopK Mean Score: 0.012483904048790294, (Original): 0.012541377422605962',\n 'Final Model Score: 0.012279499361020008, (Original): 0.012345861981641788']\n \nprenorm tripple maxclip gkf + DIR"},{"metadata":{"heading_collapsed":true},"cell_type":"markdown","source":"# Feature and Error Analysis"},{"metadata":{"hidden":true,"scrolled":false,"trusted":false},"cell_type":"code","source":"dfplt = pd.DataFrame({'y_true':X.Yards.values, 'y_pred':np.argmax(np.diff(oof), axis=1)+MIN_CLIP})\nsns.jointplot(x=dfplt.y_true, y=dfplt.y_pred, kind='kde')","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"trusted":false},"cell_type":"code","source":"# pidf = build_pidf(perms)\n# # pidf.to_csv('pidf50__{}.csv'.format(score_), index=False)\n\n# plt.title(f'Drop Importance, OOF CV {np.round(score_,8)}')\n# plt.plot(pidf.means.values)\n# plt.plot(pidf.medians.values)\n# plt.scatter(np.arange(pidf.shape[0]), pidf.means.values, s=10)\n\n# plt.show()\n\n# pidf","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"trusted":false},"cell_type":"code","source":"# plot_feature_importance(perms)#, cutoff=5)","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"trusted":false},"cell_type":"code","source":"# for season in [2017,2018]:\n#     look = train[(train.NflId==train.NflIdRusher) & (train.Season==season)].reset_index()\n#     look.sort_values('PlayId', ascending=False, inplace=True)\n#     look = look[look.index<look.shape[0]//3]\n#     print(look.shape)\n#     look.sort_values('PlayId', ascending=False, inplace=True)\n#     diffs = look.groupby('GameId').YardLine.diff().reset_index()\n#     diffs = diffs[~diffs.YardLine.isna() & diffs.YardLine.between(MIN_CLIP, MAX_CLIP)]\n\n#     plt.title(f'{season} Mean: {diffs.YardLine.mean()}, STD: {diffs.YardLine.std()}')\n#     plt.hist(diffs.YardLine, 100)\n#     plt.xlim(0,50)\n#     plt.show()\n    \n# look = test[test.NflId==test.NflIdRusher].copy()\n# print(look.shape)\n# look.sort_values('PlayId', ascending=False, inplace=True)\n# diffs = look.groupby('GameId').YardLine.diff().reset_index()\n# diffs = diffs[~diffs.YardLine.isna() & diffs.YardLine.between(MIN_CLIP, MAX_CLIP)]\n\n# plt.title(f'2019 Mean: {diffs.YardLine.mean()}, STD: {diffs.YardLine.std()}')\n# plt.hist(diffs.YardLine, 100)\n# plt.xlim(0,50)\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Inference"},{"metadata":{"trusted":false},"cell_type":"code","source":"models = []\nfor model_path in use_models:\n    models.append(load_model(model_path))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from kaggle.competitions import nflrush\nenv = nflrush.make_env()\niter_test = env.iter_test()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"for (test_df, sample_prediction_df) in iter_test:\n    basetable, _ = fast_preprocess(test_df, isTrain=False, means=means.copy())\n    basetable_bound = basetable[bounding_cols].copy()\n\n    y_pred = [\n        metric.bound_prediction(\n            lgb.Booster(model_file=f'LGB_{fold_}.lgb').predict(basetable[lgb_cols]),\n            basetable_bound\n        )\n        for fold_ in range(n_splits)\n    ]\n    \n    basetable[reg_cols] = stdscale.transform(basetable[reg_cols])\n    for model in models:\n        y_pred.append(\n            metric.bound_prediction(\n                model.predict(basetable[reg_cols], batch_size=1024)[0],\n                basetable_bound\n            )\n        )\n        \n    y_pred = np.mean(y_pred, axis=0)\n    y_pred = metric.convert_to_y199_cumsum(y_pred)\n    preds_df = pd.DataFrame(data=[y_pred.flatten()], columns=sample_prediction_df.columns)\n    env.predict(preds_df)\n    \nenv.write_submission_file()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LightGBM Version"},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"}},"nbformat":4,"nbformat_minor":1}