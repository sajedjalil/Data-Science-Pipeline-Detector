{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nimport pandas as pd\nfrom kaggle.competitions import nflrush\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt\nimport random\nfrom sklearn.model_selection import KFold\nimport lightgbm as lgb\nimport gc\nimport pickle\nimport tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env = nflrush.make_env()\ntrain_df = pd.read_csv('/kaggle/input/nfl-big-data-bowl-2020/train.csv', low_memory=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unused_columns = [\"GameId\",\"PlayId\",\"Team\",\"Yards\",\"TimeHandoff\",\"TimeSnap\"]\nunique_columns = []\nfor c in train_df.columns:\n    if c not in unused_columns+[\"PlayerBirthDate\"] and len(set(train_df[c][:11]))!= 1:\n        unique_columns.append(c)\n        print(c,\" is unique\")\n# unique_columns+=[\"BirthY\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ok = True\nfor i in range(0,509762,22):\n    p=train_df[\"PlayId\"][i]\n    for j in range(1,22):\n        if(p!=train_df[\"PlayId\"][i+j]):\n            ok=False\n            break\nprint(\"train data is sorted by PlayId.\" if ok else \"train data is not sorted by PlayId.\")\nok = True\nfor i in range(0,509762,11):\n    p=train_df[\"Team\"][i]\n    for j in range(1,11):\n        if(p!=train_df[\"Team\"][i+j]):\n            ok=False\n            break\nprint(\"train data is sorted by Team.\" if ok else \"train data is not sorted by Team.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_columns = []\nfor c in train_df.columns:\n    if c not in unique_columns + unused_columns+[\"DefensePersonnel\",\"GameClock\",\"PlayerBirthDate\"]:\n        all_columns.append(c)\nall_columns.append(\"DL\")\nall_columns.append(\"LB\")    \nall_columns.append(\"DB\")\nall_columns.append(\"GameHour\")   \nfor c in unique_columns:\n    for i in range(22):\n        all_columns.append(c+str(i))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lbl_dict = {}\nfor c in train_df.columns:\n    if c == \"DefensePersonnel\":\n        arr = [[int(s[0]) for s in t.split(\", \")] for t in train_df[\"DefensePersonnel\"]]\n        train_df[\"DL\"] = np.array([a[0] for a in arr])\n        train_df[\"LB\"] = np.array([a[1] for a in arr])\n        train_df[\"DB\"] = np.array([a[2] for a in arr])\n    elif c == \"GameClock\":\n        arr = [[int(s) for s in t.split(\":\")] for t in train_df[\"GameClock\"]]\n        train_df[\"GameHour\"] = pd.Series([a[0] for a in arr])\n    elif c == \"PlayerBirthDate\":\n        arr = [[int(s) for s in t.split(\"/\")] for t in train_df[\"PlayerBirthDate\"]]\n        train_df[\"BirthY\"] = pd.Series([a[2] for a in arr])\n    # elif c == \"PlayerHeight\":\n    #     arr = [float(s.split(\"-\")[0]) * 30.48 + float(s.split(\"-\")[1]) * 2.54\n    #         for s in list(train_df[\"PlayerHeight\"])]\n    #     train_df[\"PlayerHeight\"] = pd.Series(arr)\n    elif train_df[c].dtype=='object' and c not in unused_columns: \n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(train_df[c].values))\n        lbl_dict[c] = lbl\n        train_df[c] = lbl.transform(list(train_df[c].values))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data=np.zeros((509762//22,len(all_columns)))\nfor i in tqdm.tqdm(range(0,509762,22)):\n    count=0\n    for c in all_columns:\n        if c in train_df:\n            train_data[i//22][count] = train_df[c][i]\n            count+=1\n    for c in unique_columns:\n        for j in range(22):\n            train_data[i//22][count] = train_df[c][i+j]\n            count+=1      ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_ = np.array([train_df[\"Yards\"][i] for i in range(0,509762,22)])\nX_train = pd.DataFrame(data=train_data,columns=all_columns)\ndata = [0 for i in range(199)]\nfor y in y_train_:\n    data[int(y+99)]+=1\nplt.plot([i-99 for i in range(199)],data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# scaler = preprocessing.StandardScaler()\n# scaler.fit([[y] for y in y_train_])\n# y_train = np.array([y[0] for y in scaler.transform([[y] for y in y_train_])])\nscaler = preprocessing.StandardScaler()\nscaler.fit(y_train_.reshape(-1, 1))\ny_train = scaler.transform(y_train_.reshape(-1, 1)).flatten()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"train\nI used LGBMRegressor. I wanted to use multi-class classification, but the number of datasets was small and it was difficult to split them including all labels."},{"metadata":{"trusted":true},"cell_type":"code","source":"folds = 10\nseed = 222\nkf = KFold(n_splits = folds, shuffle = True, random_state=seed)\ny_valid_pred = np.zeros(X_train.shape[0])\nmodels = []\n\nfor tr_idx, val_idx in kf.split(X_train, y_train):\n    tr_x, tr_y = X_train.iloc[tr_idx,:], y_train[tr_idx]\n    vl_x, vl_y = X_train.iloc[val_idx,:], y_train[val_idx]\n            \n    print(len(tr_x),len(vl_x))\n    tr_data = lgb.Dataset(tr_x, label=tr_y)\n    vl_data = lgb.Dataset(vl_x, label=vl_y)  \n    clf = lgb.LGBMRegressor(n_estimators=200,learning_rate=0.01)\n    clf.fit(tr_x, tr_y,\n        eval_set=[(vl_x, vl_y)],\n        early_stopping_rounds=20,\n        verbose=False)\n    y_valid_pred[val_idx] += clf.predict(vl_x, num_iteration=clf.best_iteration_)\n    models.append(clf)\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"evaluation\nContinuous Ranked Probability Score (CRPS) is derived based on the predicted scalar value."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = np.zeros((509762//22,199))\ny_ans = np.zeros((509762//22,199))\n\nfor i,p in enumerate(np.round(scaler.inverse_transform(y_valid_pred))):\n    p+=99\n    for j in range(199):\n        if j>=p+10:\n            y_pred[i][j]=1.0\n        elif j>=p-10:\n            y_pred[i][j]=(j+10-p)*0.05\n\nfor i,p in enumerate(scaler.inverse_transform(y_train)):\n    p+=99\n    for j in range(199):\n        if j>=p:\n            y_ans[i][j]=1.0\n\nprint(\"validation score:\",np.sum(np.power(y_pred-y_ans,2))/(199*(509762//22)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"index = 0\nfor (test_df, sample_prediction_df) in tqdm.tqdm(env.iter_test()):\n    for c in test_df.columns:\n        if c == \"DefensePersonnel\":\n            try:\n                arr = [[int(s[0]) for s in t.split(\", \")] for t in test_df[\"DefensePersonnel\"]]\n                test_df[\"DL\"] = [a[0] for a in arr]\n                test_df[\"LB\"] = [a[1] for a in arr]\n                test_df[\"DB\"] = [a[2] for a in arr]\n            except:\n                test_df[\"DL\"] = [np.nan for i in range(22)]\n                test_df[\"LB\"] = [np.nan for i in range(22)]\n                test_df[\"DB\"] = [np.nan for i in range(22)]\n        elif c == \"GameClock\":\n            try:\n                arr = [[int(s) for s in t.split(\":\")] for t in test_df[\"GameClock\"]]\n                test_df[\"GameHour\"] = pd.Series([a[0] for a in arr])\n            except:\n                test_df[\"GameHour\"] = [np.nan for i in range(22)]\n        elif c == \"PlayerBirthDate\":\n            try:\n                arr = [[int(s) for s in t.split(\"/\")] for t in test_df[\"PlayerBirthDate\"]]\n                test_df[\"BirthY\"] = pd.Series([a[2] for a in arr])\n            except:\n                test_df[\"BirthY\"] = [np.nan for i in range(22)]\n        # elif c == \"PlayerHeight\":\n        #     try:\n        #         arr = [float(s.split(\"-\")[0]) * 30.48 + float(s.split(\"-\")[1]) * 2.54\n        #             for s in list(test_df[\"PlayerHeight\"])]\n        #         test_df[\"PlayerHeight\"] = pd.Series(arr)\n        #     except:\n        #         test_df[\"PlayerHeight\"] = [np.nan for i in range(22)]\n        elif c in lbl_dict and test_df[c].dtype=='object'and c not in unused_columns\\\n            and not pd.isnull(test_df[c]).any():\n            try:\n                test_df[c] = lbl_dict[c].transform(list(test_df[c].values))\n            except:\n                test_df[c] = [np.nan for i in range(22)]\n    count=0\n    test_data = np.zeros((1,len(all_columns)))\n\n    for c in all_columns:\n        if c in test_df:\n            try:\n                test_data[0][count] = test_df[c][index]\n            except:\n                test_data[0][count] = np.nan\n            count+=1\n    for c in unique_columns:\n        for j in range(22):\n            try:\n                test_data[0][count] = test_df[c][index + j]\n            except:\n                test_data[0][count] = np.nan\n            count+=1        \n    y_pred = np.zeros(199)        \n    y_pred_p = np.sum(np.round(scaler.inverse_transform(\n        [model.predict(test_data)[0] for model in models])))/folds\n    y_pred_p += 99\n    for j in range(199):\n        if j>=y_pred_p+10:\n            y_pred[j]=1.0\n        elif j>=y_pred_p-10:\n            y_pred[j]=(j+10-y_pred_p)*0.05\n    env.predict(pd.DataFrame(data=[y_pred],columns=sample_prediction_df.columns))\n    index += 22\nenv.write_submission_file()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}