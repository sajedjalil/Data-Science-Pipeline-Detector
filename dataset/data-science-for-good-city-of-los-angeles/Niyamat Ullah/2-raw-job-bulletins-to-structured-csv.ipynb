{"cells":[{"metadata":{},"cell_type":"markdown","source":"***\nThis notebook is a part of the solution for DSG: City of LA competition. The solution splited into 5 parts. Here is the list of notebook in correct order. The part of solution you are currently reading is highlighted in bold. \n\n[1. Introduction to the solution of DSG: City of LA](https://www.kaggle.com/niyamatalmass/1-introduction-to-the-solution-of-dsg-city-of-la)\n\n[**2. Raw Job Postings to structured CSV**](https://www.kaggle.com/niyamatalmass/2-raw-job-bulletins-to-structured-csv)\n\n[3. Identify biased language](https://www.kaggle.com/niyamatalmass/3-identify-biased-language)\n\n[4. Improve the diversity and quality](https://www.kaggle.com/niyamatalmass/4-improve-the-diversity-and-quality)\n\n[5. Jobs Promotional Pathway](https://www.kaggle.com/niyamatalmass/5-jobs-promotional-pathway)\n***\n<br/>"},{"metadata":{},"cell_type":"markdown","source":"<h1 align=\"center\"><font color=\"#5831bc\" face=\"Comic Sans MS\">Convert raw job bulletins to structured CSV file</font></h1> "},{"metadata":{},"cell_type":"markdown","source":"# <font color=\"#5831bc\" face=\"Comic Sans MS\">Notebook Overview</font> \nThis notebook successfully converts all the job bulletins text file into a beautiful CSV file. The technique used for doing that is **Regex and Custom Named Entity Recognition or NER model**. Please read the previous part of the solutions for easy understanding. All the descriptions, information and recommendations are provided with each step. \n\n\nThere are multiple steps for creating that beautiful CSV file.\n\n1. **Extract all main necessary section as whole text from job bulletins using regex**\n2. **Extract possible data field from those extracted texts using regex**\n3. **Build a custom NER model in spaCy**\n4. **Extract rest of the field using NER model**\n5. **Overview of structured CSV**\n\nEach section has rich documentations techniques used in that section. Let's get started! \n***"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport glob\nimport matplotlib.pyplot as plt\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <font color=\"#5831bc\" face=\"Comic Sans MS\">1. Extract all main sections as whole text using regex</font> \nIn this section, we will firstly create a pandas dataframe and store each job bulletins whole text as a row. We do this to easily apply our filter and preprocessing. The City of LA job postings has multiple main sections like ```ANNUAL SALARY, DUTIES, REQUIREMENTS, PROCESS NOTES, WHERE TO APPLY```. We first extract the whole section and store them as a column in our dataframe. For example, from ```ANNUAL SALARY``` we can extract ```salary field``` of our data dictionary very easily. Obviously, you can do it directly but I do that for modularity and easily manageable and customizable. Also, this makes our regex pattern more accurate. Without further do let's do it!"},{"metadata":{},"cell_type":"markdown","source":"> ### <font color=\"#5831bc\" face=\"Comic Sans MS\">Define all our function first</font>\n> In this section, I will define all the function needed for extracting each main section. These functions extremely modular and well coded. Also, the functions are well documented."},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_jobs_to_df(\n    path='../input/data-science-for-good-city-of-los-angeles/cityofla/CityofLA/Job Bulletins/*.txt',\n    raw_text_col_name='raw_job_text'):\n    \n    \"\"\"\n    Convert each text file in job bulletins to pandas dataframe\n    \n    -----------\n    Returns\n        Pandas Dataframe\n            ------------------------------------\n            |    index     |  raw_text         |\n            |-----------------------------------\n            |     0    |  raw_job_descriptions | \n            ____________________________________\n    \"\"\"\n    \n    \n    job_list = []\n    \n    files = glob.glob(path)\n    for file in files:\n        with open(file, 'r', errors='replace') as f:\n            content = f.read()\n            job_list.append(content)\n            \n    return pd.DataFrame({raw_text_col_name:job_list})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# all the method start with _ sign are used \n# in pandas apply function\n\ndef _class_code_apply(text):\n    \"\"\"\n    This class extract job class code\n    \"\"\"\n    match = re.search('Class Code: (\\d+)', text)\n    class_code = None\n    try:\n        class_code = match.group(1)\n    except:\n        class_code = None\n    return class_code\n        \n\ndef _open_date_apply(text):\n    \n    \"\"\"\n    Extract entire job open date section\n    \"\"\"\n    \n    open_date = ''\n    result= re.search(\n        \"(Class Code:|Class  Code:)(.*)(ANNUAL SALARY|ANNUALSALARY)\",\n        text)\n    \n    shortContent=''\n    if result:\n        shortContent=result.group(2).strip()\n        result= re.search(\n            \"Open Date:(.*)REVISED\",\n            shortContent,flags=re.IGNORECASE)\n        if result:\n            open_date=result.group(1).strip()\n        if open_date=='':\n            result= re.search(\n                \"Open Date:(.*)\\(Exam\",\n                shortContent,flags=re.IGNORECASE)\n            if result:\n                open_date=result.group(1).strip()\n        if open_date=='':\n            result= re.search(\n                \"Open Date:(.*)\",\n                shortContent,flags=re.IGNORECASE)\n            if result:\n                open_date=result.group(1).strip()\n    return open_date\n\n\ndef _exam_type_apply(text):\n    \n    \"\"\"\n    Extract entire exam type section\n    \"\"\"\n    \n    exam_type = \"\"\n    result= re.search(\n        \"(Class Code:|Class  Code:)(.*)(ANNUAL SALARY|ANNUALSALARY)\",\n        text)\n    \n    shortContent=''\n    if result:\n        shortContent=result.group(2).strip()\n        result= re.search(\n            \"\\(+(.*?)\\)\", shortContent,flags=re.IGNORECASE)\n        if result:\n            exam_type=result.group(1).strip()\n    return exam_type\n\n\ndef _salary_apply(text):\n    \"\"\"\n    Extract entire salary section\n    \"\"\"\n    salary = ''\n    salary_notes = ''\n    result=re.search(\n        \"(ANNUAL SALARY|ANNUALSALARY)(.*?)DUTIES\", text)\n    if result:\n        salContent= result.group(2).strip()\n        if \"NOTE:\" in salContent or \"NOTES:\" in salContent:\n            result=re.search(\n                \"(.*?)(NOTE:|NOTES:)\",\n                salContent,flags=re.IGNORECASE)\n            if result:\n                salary=result.group(1).strip()  \n            result= re.search(\n                \"(NOTE:|NOTES:)(.*)\",\n                salContent,flags=re.IGNORECASE)\n            if result:\n                salary_notes= result.group(2).strip()\n        else:\n            salary = salContent\n    else:\n        result=re.search(\n            \"(ANNUAL SALARY|ANNUALSALARY)(.*?)REQUIREMENT\",\n            text,flags=re.IGNORECASE)\n        if result:\n            salContent= result.group(2).strip()\n            if \"NOTE:\" in salContent or \"NOTES:\" in salContent:\n                result=re.search(\n                    \"(.*?)(NOTE:|NOTES:)\",\n                    salContent,flags=re.IGNORECASE)\n                if result:\n                    salary=result.group(1).strip()  \n                result= re.search(\n                    \"(NOTE:|NOTES:)(.*)\",\n                    salContent,flags=re.IGNORECASE)\n                if result:\n                    salary_notes= result.group(2).strip()\n            else:\n                salary= salContent\n    salary_text = \"|||||||||||||||\".join([salary, salary_notes])\n    return salary_text\n\n\ndef _duties_apply(text):\n    \"\"\"\n    Extract job duties section\n    \"\"\"\n    duties=''\n    result=duties= re.search(\"DUTIES(.*?)REQUIREMENT\", text)\n    if result:\n        duties= result.group(1).strip()\n    return duties\n\ndef _requirements_apply(text):\n    \"\"\"\n    Extract entire job requirements section\n    \"\"\"\n    req='|'.join([\"REQUIREMENT/MIMINUMUM QUALIFICATION\",\n                  \"REQUIREMENT/MINUMUM QUALIFICATION\",\n                  \"REQUIREMENT/MINIMUM QUALIFICATION\",\n                  \"REQUIREMENT/MINIMUM QUALIFICATIONS\",\n                  \"REQUIREMENT/ MINIMUM QUALIFICATION\",\n                  \"REQUIREMENTS/MINUMUM QUALIFICATIONS\",\n                  \"REQUIREMENTS/ MINIMUM QUALIFICATIONS\",\n                  \"REQUIREMENTS/MINIMUM QUALIFICATIONS\",\n                  \"REQUIREMENTS/MINIMUM REQUIREMENTS\",\n                  \"REQUIREMENTS/MINIMUM QUALIFCATIONS\",\n                  \"MINIMUM REQUIREMENTS:\",\n                  \"REQUIREMENTS\",\n                  \"REQUIREMENT\"])\n    \n    result= re.search(f\"({req})(.*)(WHERE TO APPLY|HOW TO APPLY)\", text)\n    requirements=''\n    if result:\n        requirements = result.group(2).strip()\n    return requirements\n\n\ndef _where_to_apply(text):\n    \n    \"\"\"\n    Extract entire 'WHERE TO APPLY' section\n    \"\"\"\n    \n    where_to_apply = ''\n    result= re.search(\n        \"(HOW TO APPLY|WHERE TO APPLY)(.*)(APPLICATION DEADLINE|APPLICATION PROCESS)\",\n        text)\n    if result:\n        where_to_apply= result.group(2).strip()\n    else:\n        result= re.search(\n            \"(HOW TO APPLY|WHERE TO APPLY)(.*)(SELECTION PROCESS|SELELCTION PROCESS)\",\n            text)\n        if result:\n            where_to_apply= result.group(2).strip()\n    return where_to_apply\n\ndef _deadline_apply(text):\n    \"\"\"\n    Extract entire deadline section\n    \"\"\"\n    \n    deadline=''\n    result= re.search(\n        \"(APPLICATION DEADLINE|APPLICATION PROCESS)(.*?)(SELECTION PROCESS|SELELCTION PROCESS)\",\n        text)\n    if result:\n        deadline= result.group(2).strip()\n    else:\n        result= re.search(\n            \"(APPLICATION DEADLINE|APPLICATION PROCESS)(.*?)(Examination Weight:)\",\n            text)\n        if result:\n            deadline= result.group(2).strip()\n            \n    return deadline\n\ndef _selection_process_apply(text):\n    \n    \"\"\"\n    Extract selectioin process section\n    \"\"\"\n    \n    selection_process=''\n    result=selection_process= re.search(\n        \"(SELECTION PROCESS|Examination Weight:)(.*)(APPOINTMENT|APPOINTMENT IS SUBJECT TO:)\",\n        text)\n    if result:\n        selection_process= result.group(2).strip()\n    else:\n        result=selection_process= re.search(\n            \"(SELECTION PROCESS|Examination Weight:)(.*)\",\n            text)\n        if result:\n            selection_process= result.group(2).strip()\n            \n    return selection_process","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def _whole_clean_text(text):\n    return text.replace(\"\\n\",\"\").replace(\"\\t\",\"\").strip()\n\ndef pre_processing(dataframe):\n    # remove all first new line charecters from text\n    dataframe['raw_job_text'] = dataframe['raw_job_text'].apply(\n        lambda x: x.lstrip())\n    return dataframe\n\ndef extract_job_title(dataframe):\n    # split at newline charecter, then grab first text\n    # and that is the title\n    dataframe['JOB_CLASS_TITLE'] = dataframe['raw_job_text'].apply(\n        lambda x: x.split('\\n', 1)[0])\n    dataframe['JOB_CLASS_TITLE'] = dataframe['JOB_CLASS_TITLE'].apply(\n        lambda x: _whole_clean_text(x))\n    return dataframe\n\ndef extract_class_code(dataframe):\n    # remove all extra white spaces\n    temp = dataframe['raw_job_text'].apply(lambda x: ' '.join(x.split()))\n    # find class code\n    dataframe['JOB_CLASS_NO'] = temp.apply(lambda x: _class_code_apply(x))\n    return dataframe\n\ndef extract_open_date(dataframe):\n    # remove all extra white spaces\n    temp = dataframe['raw_job_text'].apply(lambda x: ' '.join(x.split()))\n    \n    dataframe['OPEN_DATE'] = temp.apply(lambda x: _open_date_apply(x))\n    return dataframe\n\ndef extract_exam_type(dataframe):\n    # remove all extra white spaces\n    temp = dataframe['raw_job_text'].apply(lambda x: ' '.join(x.split()))\n    \n    dataframe['TEMP_EXAM_TYPE'] = temp.apply(lambda x: _exam_type_apply(x))\n    return dataframe\n\ndef extract_salary(dataframe):\n    # remove all extra white spaces\n    temp = dataframe['raw_job_text'].apply(lambda x: ' '.join(x.split()))\n    \n    dataframe['TEMP_SALARY'] = temp.apply(lambda x: _salary_apply(x))\n    return dataframe\n\n\ndef extract_duties(dataframe):\n    # remove all extra white spaces\n    temp = dataframe['raw_job_text'].apply(lambda x: ' '.join(x.split()))\n    \n    dataframe['JOB_DUTIES'] = temp.apply(lambda x: _duties_apply(x))\n    return dataframe\n\ndef extract_requirements(dataframe):\n    # remove all extra white spaces\n    temp = dataframe['raw_job_text'].apply(lambda x: ' '.join(x.split()))\n    \n    dataframe['TEMP_REQUIREMENTS'] = temp.apply(lambda x: _requirements_apply(x))\n    return dataframe\n\ndef extract_where_to_apply(dataframe):\n    # remove all extra white spaces\n    temp = dataframe['raw_job_text'].apply(lambda x: ' '.join(x.split()))\n    \n    dataframe['WHERE_TO_APPLY'] = temp.apply(lambda x: _where_to_apply(x))\n    return dataframe\n\ndef extract_deadline(dataframe):\n    # remove all extra white spaces\n    temp = dataframe['raw_job_text'].apply(lambda x: ' '.join(x.split()))\n    \n    dataframe['DEADLINE'] = temp.apply(lambda x: _deadline_apply(x))\n    return dataframe\n\ndef extract_selection_process(dataframe):\n    # remove all extra white spaces\n    temp = dataframe['raw_job_text'].apply(lambda x: ' '.join(x.split()))\n    \n    dataframe['SELECTION_PROCESS'] = temp.apply(lambda x: _selection_process_apply(x))\n    return dataframe\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### <font color=\"#5831bc\" face=\"Comic Sans MS\">Let's extract each main section of job bulletins!</font>\n> We already defined our necessary function. Now, all we have to is apply this function in pandas dataframe. We made our function in a way that we just of to pass the dataframe name to the function and function will do the rest and return a processed dataframe."},{"metadata":{"trusted":true},"cell_type":"code","source":"# first let's convert folder of raw text job bulletins\n# to pandas dataframe\ndf_jobs = convert_jobs_to_df()\n\n# do some initial text cleaning\ndf_jobs = pre_processing(df_jobs)\n\n###############################\n# Here is actual extraction of main section begin\n# we just call the function\n###############################\ndf_jobs = extract_job_title(df_jobs) # extract job title\n\ndf_jobs = extract_class_code(df_jobs) # extract class code\n\ndf_jobs = extract_open_date(df_jobs) # extract open date\n\ndf_jobs = extract_exam_type(df_jobs) # extract exam type section\n\ndf_jobs = extract_salary(df_jobs) # extract salary section\n\ndf_jobs = extract_duties(df_jobs) # extract duties section\n\ndf_jobs = extract_requirements(df_jobs) # extract requirements section\n\ndf_jobs = extract_where_to_apply(df_jobs) # extract where to apply section\n\ndf_jobs = extract_deadline(df_jobs) # extract deadline section\n\ndf_jobs = extract_selection_process(df_jobs) # extract selectin pro section\n\n# create a new column containing whole text but clean from new line and tab \ndf_jobs['raw_clean_job_text'] = df_jobs['raw_job_text'].apply(\n    lambda x: _whole_clean_text(x))\n\n# finally let's see what we have got\ndf_jobs.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nLooking very good. We have successfully extracted all the main section of each job bulletins. And stored them in pandas dataframe. It will be really helpful in the next section of solutions for **applying more regorious regex and function**.\n\n***"},{"metadata":{},"cell_type":"markdown","source":"# <font color=\"#5831bc\" face=\"Comic Sans MS\">2. Extract possible data fields using regex</font>\nIn the previous step, we extracted each** main section** of job bulletins. In this step, we are going to extract possible data field from those main sections. You may ask, why are you saying ***possible field***? Because as I said in my first notebook, that not every possible data fields is possible to extract using regex. Some field value pattern changes a lot with each job bulletins. But in this section, **I will be extracting those field value that can be extracted using regex**. \n\nI will be extracting these data field value: ```ENTRY_SALARY_GEN```, ```ENTRY_SALARY_DWP```, ```EXAM_TYPE```, ```DRIVERS_LICENSE_REQ```, ```DRIV_LIC_TYPE```. Without further talking, let's get into work! "},{"metadata":{},"cell_type":"markdown","source":"> ### <font color=\"#5831bc\" face=\"Comic Sans MS\">First Define our necessary function</font>\n> As always first, define all our functions first. It will help us reuse our code and make our code more moduler. We provided detailed documentation for each functions as comment."},{"metadata":{"trusted":true},"cell_type":"code","source":"################################\n# ENTRY_SALARY_GEN\n################################\ndef salary(content):   \n    try:\n        salary=re.compile(r'\\$(\\d+,\\d+)((\\s(to|and)\\s)(\\$\\d+,\\d+))?') #match salary\n        sal=re.search(salary,content)\n        if sal:\n            range1=sal.group(1)\n            if range1 and '$' not in range1:\n                range1='$'+range1\n            range2=sal.group(2)\n            if range2:\n                range2=sal.group(2).replace('to','')\n                range2=range2.replace('and','')\n            if range1 and range2:\n                return f\"{range1}-{range2.strip()}\"\n            elif range1:\n                return f\"{range1} (flat-rated)\"\n        else:\n            return ''\n    except Exception as e:\n        return ''\n\n################################\n# ENTRY_SALARY_DWP\n################################\ndef salaryDWP(content):\n    try:\n        result= re.search(\"(Department of Water and Power is)(.*)\", content)\n        if result:\n            salary=re.compile(r'\\$(\\d+,\\d+)((\\s(to|and)\\s)(\\$\\d+,\\d+))?') #match salary\n            sal=re.search(salary,result.group(2))\n            if sal:\n                range1=sal.group(1)\n                if range1 and '$' not in range1:\n                    range1='$'+range1\n                range2=sal.group(2)\n                if range2:\n                    range2=sal.group(2).replace('to','')\n                    range2=range2.replace('and','')\n                if range1 and range2:\n                    return f\"{range1}-{range2.strip()}\"\n                elif range1:\n                    return f\"{range1} (flat-rated)\"\n            else:\n                return ''\n    except Exception as e:\n        return ''  \n\n################################\n# DRIVERS_LICENSE_REQ\n# Whether a driver's license is required, \n# possibly required, or not required\n# (note: the job class will most likely not explicitly say if a license is not required)\n# P,R\n################################\ndef drivingLicenseReq(content):\n    try:\n        result= re.search(\n            \"(.*?)(California driver\\'s license|driver\\'s license)\",\n            content)\n        \n        if result:\n            exp=result.group(1).strip()\n            exp=' '.join(exp.split()[-10:]).lower()\n            if 'may require' in exp:\n                return 'P'\n            else:\n                return 'R'\n        else:\n            return ''\n    except Exception as e:\n        return '' \n\n################################\n#DRIV_LIC_TYPE\n################################\ndef drivingLicense(content):\n    driving_License=[]\n    result= re.search(\n        \"(valid California Class|valid Class|valid California Commercial Class)(.*?)(California driver\\'s license|driver\\'s license)\",\n        content)\n    if result:\n        dl=result.group(2).strip()\n        dl=dl.replace(\"Class\",\"\").replace(\"commercial\",\"\").replace(\"or\",\"\").replace(\"and\",\"\")\n        if 'A' in dl:\n            driving_License.append('A')\n        if 'B' in dl:\n            driving_License.append('B') \n        if 'C' in dl:\n            driving_License.append('C')  \n        if 'I' in dl:\n            driving_License.append('I')   \n        return ','.join(driving_License)\n    else:\n        return ''\n\n################################\n#EXAM_TYPE\n################################\ndef examType(content):\n    '''Code explanation:\n    OPEN: Exam open to anyone (pending other requirements)\n    INT_DEPT_PROM: Interdepartmental Promotional\n    DEPT_PROM: Departmental Promotional\n    OPEN_INT_PROM: Open or Competitive Interdepartmental Promotional\n    '''\n    exam_type=''\n    if 'INTERDEPARTMENTAL PROMOTIONAL AND AN OPEN COMPETITIVE BASIS' in content:\n        exam_type='OPEN_INT_PROM' \n    elif 'OPEN COMPETITIVE BASIS' in content:\n         exam_type='OPEN'\n    elif 'INTERDEPARTMENTAL PROMOTIONAL' or 'INTERDEPARMENTAL PROMOTIONAL' in content:\n        exam_type='INT_DEPT_PROM'\n    elif 'DEPARTMENTAL PROMOTIONAL' in content:\n        exam_type='DEPT_PROM' \n    return exam_type\n\ndef split_salary(text):\n    \"\"\"\n    When we extracted salary section, \n    we merge general and DWP salary together \n    with a seperator '|||||||||||||||'\n    This function split that\n    \"\"\"\n    return text.split('|||||||||||||||')[0]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### <font color=\"#5831bc\" face=\"Comic Sans MS\">Let's extract those data fields!</font>\n> We have already defined our functions. Now we just have to call them and store their value in pandas dataframe. Details documentation is provided with each line of code."},{"metadata":{"trusted":true},"cell_type":"code","source":"# split salary into general and DWP\ndf_jobs['TEMP_SALARY'] = df_jobs['TEMP_SALARY'].apply(lambda x: split_salary(x))\n\n# extract ENTRY_SALARY_GEN and ENTRY_SALARY_DWP\ndf_jobs['ENTRY_SALARY_GEN'] = df_jobs['TEMP_SALARY'].apply(lambda x: salary(x))\ndf_jobs['ENTRY_SALARY_DWP'] = df_jobs['TEMP_SALARY'].apply(lambda x: salaryDWP(x))\n\n# extract DRIVERS_LICENSE_REQ and DRIV_LIC_TYPE\ndf_jobs['DRIVERS_LICENSE_REQ'] = df_jobs['TEMP_REQUIREMENTS'].apply(\n    lambda x: drivingLicenseReq(x))\ndf_jobs['DRIV_LIC_TYPE'] = df_jobs['raw_clean_job_text'].apply(lambda x: drivingLicense(x))\n\n# extract EXAM_TYPE\ndf_jobs['EXAM_TYPE'] = df_jobs['raw_clean_job_text'].apply(lambda x: examType(x))\n\n# finally let's see what we have done\ndf_jobs[['ENTRY_SALARY_GEN', 'ENTRY_SALARY_DWP',\n         'DRIVERS_LICENSE_REQ', 'DRIV_LIC_TYPE', 'EXAM_TYPE']].head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Awesome! We have successfully **extracted 5 data fields column value** from each job bulletins. That's really awesome! We have progressed towards our goal a lot. In our next section, we will see how to extract those column value that are impossible to extract using regex!\n<br>\n\n***"},{"metadata":{},"cell_type":"markdown","source":"# <font color=\"#5831bc\" face=\"Comic Sans MS\">3. Build a custom NER model in spaCy</font>\nWe already extracted some of the important data field value. But there are some columns values that are **impossible to extract** using regex. We're not telling that it's will never be possible, but what we're saying that it's really hard. Because the pattern of the value is different in most of the jobs. The data field we can't extract using regex are \n\n```EDUCATION_YEARS, SCHOOL_TYPE ,EDUCATION_MAJOR, EXPERIENCE_LENGTH ,FULL_TIME_PART_TIME,EXP_JOB_CLASS_TITLE, EXP_JOB_CLASS_ALT_RESP,  EXP_JOB_CLASS_FUNCTION, EXP_JOB_CLASS_ADDITIONAL_FUNCTION,COURSE_COUNT, COURSE_LENGTH, COURSE_SUBJECT, MISC_COURSE_DETAILS, EXP_JOB_COMPANY, DEGREE NAME, EXP_JOB_CLASS_ALT_JOB_TITLE, REQUIRED_CERTIFICATE, CERTIFICATE_ISSUED_BY,COURSE_TITLE, REQUIRED_EXAM_PASS, EXPERIENCE_EXTRA_DETAILS```\n\nAs we see it's a lot of data field we shouldn't be using regex for extracting value. One interesting and useful fact is that **all of these data field value can be found** in one particular job description section. That is ```REQUIREMENTS``` section. It's a piece of very good news for us because do you remember we already extracted that section and store those in pandas column.\n\n\n## <font color=\"#5831bc\" face=\"Comic Sans MS\">So, what is a NER?</font>\nNamed entity recognition (NER), also known as entity chunking/extraction, is a popular technique used in information extraction to identify and segment the named entities and classify or categorize them under various predefined classes. That's we wanted! We have our ```REQUIREMENTS``` section text and we want to extract information from that into predefined data field or classes. Here is an example of NER in text:\n![](http://imanage.com/wp-content/uploads/2014/10/NER1.png)\n\n\n## <font color=\"#5831bc\" face=\"Comic Sans MS\">How NER works?</font>\nI don't want to go into mathematical details about NER, I just want to say that first, we will train a NER model using our hand labelling dataset. And then your model can predict new data's labelling. Description and example of traning data is provided below when we import it.\n\n\n## <font color=\"#5831bc\" face=\"Comic Sans MS\">How to train a custom NER?</font>\nNowadays, many states of art libraries are present that offer already trained NER model ready to use. But why don't we just use those? Well, that pre-trained NER model trained with a dataset that is totally different than ours. And we have our own custom labels. For that reason, we have to make our own model designed for our own use cases. \n\nFirstly, we need a custom hand labelled dataset. What does that mean? Our dataset consists of ```REQUIREMENTS``` section text and each text has some labelling identifying which word or phrase go to which data field. If we can train our custom model with that then our model can predict for other jobs as well.\n\nI have build custom hand label dataset with 70 jobs ```REQUIREMENTS``` labelled with the correct data field. We can use a different library for training our model, but I would like to use spaCy for that. It is a very popular and useful library for NLP. It makes really easy for working with NER. Without further do, let's begin! "},{"metadata":{},"cell_type":"markdown","source":"> ### <font color=\"#5831bc\" face=\"Comic Sans MS\">Some Preprocessing to our REQUIREMENTS section texts</font>\n> We previously extracted entire ```REQUIREMENTS``` section texts and store those in a column. But we don't need the entire text. There are texts about process notes and certainly, we don't need them.\n\n> **Also, ```REQUIREMENTS``` section has numbered list separated by or/and. We need to split them at ```or``` and store them as separate row because we need to extract data fields from each bullet points**.  So let's process our requirements section."},{"metadata":{"trusted":true},"cell_type":"code","source":"# function for seperating process notes\n# from requirements section\ndef _seperate_process_notes(text):\n    result = re.search('(.*)(PROCESS NOTES|NOTES)(.*)', text)\n    if result:\n        req = result.group(1)\n        process_notes = result.group(3)\n    else:\n        req = text \n        process_notes = None\n    return req, process_notes\n\ndef _split_requirements(text):\n    req_list = re.split('or \\d\\.', text)\n    return req_list\n\n\n# function for spliting requirements section \n# numbered points and store them as seperate rows\ndef split_list_to_rows(dataframe, col_name):\n    # here col_name is the name of the column which contains list\n    return pd.DataFrame({\n          col:np.repeat(dataframe[col].values, dataframe[col_name].str.len())\n          for col in dataframe.columns.drop(col_name)}\n        ).assign(**{col_name:np.concatenate(dataframe[col_name].values)})[dataframe.columns]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# seperate process notes\ndf_jobs['REQUIREMENTS'], df_jobs['REQUIREMENTS_PROCESS'] = \\\nzip(*df_jobs['TEMP_REQUIREMENTS'].apply(lambda x: _seperate_process_notes(x)))\n\n# remove some text\ndf_jobs['REQUIREMENTS'] = df_jobs['REQUIREMENTS'].str.replace(r'PROCESS', '')\n# df_jobs['REQUIREMENTS'].to_csv('./jobs_desc.csv', index=None)\n\n# split requirements and store them as seperate rows\ndf_jobs['req_list'] = df_jobs['REQUIREMENTS'].apply(lambda x: _split_requirements(x))\ndf_jobs = split_list_to_rows(df_jobs, 'req_list')\n\n\n# finally let's see what we did\ndf_jobs[['JOB_CLASS_TITLE', 'JOB_CLASS_NO', 'req_list']].head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wow! We successfully clean our requirements section and done some pre-processing. We can see the same job has multiple rows because we split by ```and/or``` in each ```REQUIREMENTS``` section and store each of them separate rows. Let's move on to our next section for build NER model in spaCy."},{"metadata":{},"cell_type":"markdown","source":"> ### <font color=\"#5831bc\" face=\"Comic Sans MS\">Import and Process our training data</font>\n> I build a hand-labelled custom dataset consist of only 70 job description requirements. The dataset is provided with this kernel as an external dataset. But in order to use this dataset for spaCy NER model, we need to do some pre-processing to our external data. Without further talking, let's do that!"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def convert_dataturks_to_spacy(dataturks_JSON_FilePath):\n    try:\n        training_data = []\n        lines=[]\n        with open(dataturks_JSON_FilePath, 'r') as f:\n            lines = f.readlines()\n\n        for line in lines:\n            data = json.loads(line)\n            text = data['content']\n            entities = []\n            for annotation in data['annotation']:\n                #only a single point in text annotation.\n                point = annotation['points'][0]\n                labels = annotation['label']\n                # handle both list of labels or a single label.\n                if not isinstance(labels, list):\n                    labels = [labels]\n\n                for label in labels:\n                    #dataturks indices are both inclusive [start, end] but spacy is not [start, end)\n                    entities.append((point['start'], point['end'] + 1 ,label))\n\n\n            training_data.append((text, {\"entities\" : entities}))\n\n        return training_data\n    except Exception as e:\n        logging.exception(\"Unable to process \" + dataturks_JSON_FilePath + \"\\n\" + \"error = \" + str(e))\n        return None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import logging\nimport json\nTRAIN_DATA = convert_dataturks_to_spacy('../input/ner-annotation-of-city-of-la-jobs/city_la_jobs_ner_labeling.json')\n\nTRAIN_DATA[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We successfully imported and processed our training data. It's a json file. In above, we can see a portion of of file. First we see our job requirements text. Then a bunch of entities. **Entities means our data field that we need**. For example first entity ```EXP_JOB_CLASS_FUNCTION``` denoted by ```118 and 229``` means **the value for ```EXP_JOB_CLASS_FUNCTION``` data field can be found in ```118 and 229``` number position in the requirements text of that job**. Now we understand our training data, let's move onto training our model!"},{"metadata":{},"cell_type":"markdown","source":"> ### <font color=\"#5831bc\" face=\"Comic Sans MS\">Building and Training NER model</font>\n> Building and training NER model in spaCy is pretty straightforward. Below I have provided informative comment for code. Also, the spaCy official website has an extensive guide on building a NER model. I highly recommend checking it out. After training completed, the model can be saved to disk for later use cases. But we didn't do it here, we don't need it right now. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from __future__ import unicode_literals, print_function\n\nimport plac\nimport random\nfrom pathlib import Path\nimport spacy\nfrom spacy.util import minibatch, compounding\nfrom tqdm import tqdm, tqdm_notebook\n\n\ndef ner_model(model=None, output_dir='./', n_iter=500):\n    print('Training started...')\n    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n    if model is not None:\n        nlp = spacy.load(model)  # load existing spaCy model\n        print(\"Loaded model '%s'\" % model)\n    else:\n        nlp = spacy.blank(\"en\")  # create blank Language class\n        print(\"Created blank 'en' model\")\n\n    # create the built-in pipeline components and add them to the pipeline\n    # nlp.create_pipe works for built-ins that are registered with spaCy\n    if \"ner\" not in nlp.pipe_names:\n        ner = nlp.create_pipe(\"ner\")\n        nlp.add_pipe(ner, last=True)\n    # otherwise, get it so we can add labels\n    else:\n        ner = nlp.get_pipe(\"ner\")\n\n    # add labels\n    for _, annotations in TRAIN_DATA:\n        for ent in annotations.get(\"entities\"):\n            ner.add_label(ent[2])\n\n    # get names of other pipes to disable them during training\n    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n    with nlp.disable_pipes(*other_pipes):  # only train NER\n        # reset and initialize the weights randomly â€“ but only if we're\n        # training a new model\n        if model is None:\n            nlp.begin_training()\n        for itn in tqdm_notebook(range(n_iter)):\n            random.shuffle(TRAIN_DATA)\n            losses = {}\n            # batch up the examples using spaCy's minibatch\n            batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n            for batch in batches:\n                texts, annotations = zip(*batch)\n                nlp.update(\n                    texts,  # batch of texts\n                    annotations,  # batch of annotations\n                    drop=0.5,  # dropout - make it harder to memorise data\n                    losses=losses,\n                )\n    print('Training completed.')\n    return nlp\n#             print(\"Losses\", losses)\n\n    # test the trained model\n#     for text, _ in TRAIN_DATA:\n#         doc = nlp(text)\n#         print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n# #         print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n\n    # save model to output directory\n#     if output_dir is not None:\n#         output_dir = Path(output_dir)\n#         if not output_dir.exists():\n#             output_dir.mkdir()\n#         nlp.to_disk(output_dir)\n#         print(\"Saved model to\", output_dir)\n\n\n# we have build our model class\n# let's train our model\n\nnlp = ner_model()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, we created and trained a custom Named Entity Recognition model. The model can be saved to disk so that we can reuse it later. But for now, let's use this model to extract our data field.\n***"},{"metadata":{},"cell_type":"markdown","source":"# <font color=\"#5831bc\" face=\"Comic Sans MS\">4. Extract rest of the fields using NER model</font>\nPreviously, we build our NER model. In this step, we are going to use our newly trained model to extract our data fields. First we will load the model from disk if we store the model in disk. But for now let's just directly use the model. Let's do that!"},{"metadata":{},"cell_type":"markdown","source":"> ### <font color=\"#5831bc\" face=\"Comic Sans MS\">Extract data fields and process</font>\n> In this section, we will do our most important and desired task, extracting necessary data field using custom build NER model. First, we will build a function to make code easy. Then apply that function to pandas requirement column. We have the ```nlp``` variable where our model is stored. We will use that. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# function for extracting data field value\ndef _ner_apply(text):\n    # pass our text to spacy\n    # it will return us doc (spacy doc)\n    doc = nlp(text)\n    # return list of tuples look like \n    # this [('four-year', 'EDUCATION_YEARS'), ('college or university', 'SCHOOL_TYPE')]\n    return [(ent.text, ent.label_) for ent in doc.ents]\n\n\n# apply the function and store the result in a new column \ndf_jobs['temp_entity'] = df_jobs['req_list'].apply(lambda x: _ner_apply(x))\n\n# finally look at our data\ndf_jobs[['req_list', 'temp_entity']].head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Okay, finally we extracted our data field. But it doesn't look clean and nice. Because the data our predict function returns are like a list of tuples. For example, ```[('four-year', 'EDUCATION_YEARS'), ('college or university', 'SCHOOL_TYPE')]``` for each requirements row. So we have to process it and store each of the data field value as separate column. Let's do that."},{"metadata":{"trusted":true},"cell_type":"code","source":"import itertools\n\n# process our data field column and seperate each column and store their value in their column\nflatter = sorted([list(x) + [idx] for idx, y in enumerate(df_jobs['temp_entity']) \n                  for x in y], key = lambda x: x[1]) \n\n# Find all of the values that will eventually go in each F column                \nfor key, group in itertools.groupby(flatter, lambda x: x[1]):\n    list_of_vals = [(val, idx) for val, _, idx in group]\n\n    # Add each value at the appropriate index and F column\n    for val, idx in list_of_vals:\n        df_jobs.loc[idx, key] = val\n        \ndf_jobs['REQUIREMENT_SET_ID'] = df_jobs.groupby('JOB_CLASS_NO').cumcount() + 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see what our final dataframe look like and what we extracted after clearing our fields. "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"COLUMNS_ORDER = ['JOB_CLASS_TITLE', 'JOB_CLASS_NO', \n                 'REQUIREMENT_SET_ID',\n                 'JOB_DUTIES', 'ENTRY_SALARY_GEN',\n                 'ENTRY_SALARY_DWP','OPEN_DATE',\n                 'EDUCATION_YEARS', 'SCHOOL_TYPE',\n                 'EDUCATION_MAJOR', 'DEGREE NAME','EXPERIENCE_LENGTH',\n                 'FULL_TIME_PART_TIME',\n                 'EXP_JOB_CLASS_TITLE', 'EXP_JOB_CLASS_FUNCTION',\n                 'EXP_JOB_COMPANY','EXP_JOB_CLASS_ALT_JOB_TITLE',\n                 'EXP_JOB_CLASS_ALT_RESP',\n                 'COURSE_LENGTH', 'COURSE_SUBJECT',\n                 'REQUIRED_CERTIFICATE','CERTIFICATE_ISSUED_BY',\n                 'DRIVERS_LICENSE_REQ', 'DRIV_LIC_TYPE',\n                 'EXAM_TYPE', 'req_list', 'raw_clean_job_text',\n                 'REQUIREMENTS']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# first order our column in desire way\ndf_jobs_clean_col = df_jobs[COLUMNS_ORDER]\n\n\ndf_jobs_clean_col = df_jobs_clean_col[df_jobs_clean_col['EDUCATION_YEARS'].str.contains(\n    \"year\", flags=re.IGNORECASE, na=True)]\n\ndf_jobs_clean_col = df_jobs_clean_col[df_jobs_clean_col['SCHOOL_TYPE'].str.contains(\n    \"school|college|university|apprenticeship|G.E.D\", flags=re.IGNORECASE, na=True)]\n\ndf_jobs_clean_col = df_jobs_clean_col[df_jobs_clean_col['EXPERIENCE_LENGTH'].str.contains(\n    \"year|month|hour\", flags=re.IGNORECASE, na=True)]\n\ndf_jobs_clean_col = df_jobs_clean_col[df_jobs_clean_col['FULL_TIME_PART_TIME'].str.contains(\n    \"time\", flags=re.IGNORECASE, na=True)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# save the structured csv for next solutions\ndf_jobs_clean_col.to_csv('./jobs.csv', index=None)\n# print the CSV\ndf_jobs_clean_col.head(25)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wow! That's look very good. If we scroll to right we can see our model extracted data fields value correctly. It's a pleasure to see all the structured version of job bulletins. The dataset is also saved to disk for later use in the solution. \n***"},{"metadata":{},"cell_type":"markdown","source":"# <font color=\"#5831bc\" face=\"Comic Sans MS\">5. Overview of structured CSV</font>"},{"metadata":{},"cell_type":"markdown","source":"We finally made a structured version of our job bulletins. After seeing the dataset, we can surely say that most of the data fields extracted correctly. Some data field may contain some outliers but we can ignore those. \n\n    - We didn't only use regex for extracting data fields. By using NER we have the opportunity to extract data fields from any random or new job information. If we only use regex, for new jobs regex pattern may not match and throw an error. But my solutions can extract data fields from any new job posting of the City of LA. \n\n    - Those data fields extracted using regex in my solution are proven that their pattern will not change in new jobs. So they will not break in new jobs. \n\n    - For all these reasons, We can say that this solution solves the first problem of this competition (converting jobs to structured CSV) very perfectly.\n***\n\nThe City of LA provided us with a data dictionary containing an example of possible data fields we can extract. We follow that example. And we made some new data fields and also delete some data fields. Let's talk about that. \n\nOur structured CSV has these fields name: \n\n    'JOB_CLASS_TITLE', 'JOB_CLASS_NO', \n     'REQUIREMENT_SET_ID',\n     'JOB_DUTIES', 'ENTRY_SALARY_GEN',\n     'ENTRY_SALARY_DWP','OPEN_DATE',\n     'EDUCATION_YEARS', 'SCHOOL_TYPE',\n     'EDUCATION_MAJOR', 'DEGREE NAME','EXPERIENCE_LENGTH',\n     'FULL_TIME_PART_TIME',\n     'EXP_JOB_CLASS_TITLE', 'EXP_JOB_CLASS_FUNCTION',\n     'EXP_JOB_COMPANY','EXP_JOB_CLASS_ALT_JOB_TITLE',\n     'EXP_JOB_CLASS_ALT_RESP',\n     'COURSE_LENGTH', 'COURSE_SUBJECT',\n     'REQUIRED_CERTIFICATE','CERTIFICATE_ISSUED_BY',\n     'DRIVERS_LICENSE_REQ', 'DRIV_LIC_TYPE',\n     'EXAM_TYPE', 'req_list', 'raw_clean_job_text'\n     \n     \nThe City of LA requested to provide information on newly created data fields. So let's do that. \n\n    - DEGREE NAME - Name of the degree (bachelor's, associate etc) required. It will helpful to do eda on educations and others. \n\n    - EXP_JOB_COMPANY - Experience in doing jobs in which company (e.g City of Los Angels). It will helpful to know the company employee must work for. Because most of the time, job bulletins says explicitly that they must experience with City of Los Angels. To identify this this data field will be very useful. \n\n    - EXP_JOB_CLASS_ALT_JOB_TITLE - Alternative job class title. We have alternate job class responsible class. But with this we can also have the title. This can enhance our explicit promotional pathway. \n\n    - REQUIRED_CERTIFICATE - Certificate required for this job. Some jobs wants different types of certificate. With this data field we can identify those. \n\n    - CERTIFICATE_ISSUED_BY - Certificate Issued by which organization. Required certificate is issued by which company? This column will answer that. \n\n    - req_list - REQUIREMENTS after splitting at and/or and stored in separate columns. Requirements sections number of requirements separeted by and/or. We splited at ```or``` and store them as seperate rows. This data field store that. \n\n    - raw_clean_job_text - cleaned whole raw job text. In the next part of the solution, we need whole raw job text. This data field will help us for that. \n***"},{"metadata":{},"cell_type":"markdown","source":"# <font color=\"#5831bc\" face=\"Comic Sans MS\">Conclusion</font>\nWe finally converted our raw job posting to structured CSV. This solved one of the hardest problems in this competition. The structured CSV saved to disk. Feel free download and check it's quality. We will also check some of its columns in part 4 of the solutions. Thank you very much for reading the notebook. See you in the next part of the solutions. "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}