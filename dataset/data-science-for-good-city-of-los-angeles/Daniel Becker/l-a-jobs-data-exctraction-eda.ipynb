{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Table of Content  \n1. [Introduction](#introduction)\n2. [Preparation](#preparation)\n3. [Data Extraction](#data_extraction)  \n4. [Submission](#submission)\n5. [Exploratory Data Analysis (EDA)](#eda)"},{"metadata":{},"cell_type":"markdown","source":"## 1. Introduction <a id=\"introduction\"></a> \nThis notebook uses regular expression to extract the relevant data we need for the submission. some usefull links to work with regular expression are:\n* [https://regexr.com/](https://regexr.com/) : A JavaScript Engine with explaination of the regular expressions.  \n* [https://pythex.org/](https://pythex.org/) : A implementation of the python `re.findall()` function, to test regular expression with the different flags.  \n* [https://docs.python.org/3/library/re.html](https://docs.python.org/3/library/re.html): Link to the python `re` library with the explanation of functions and parameters.  \n* [https://docs.python.org/3/howto/regex.html](https://docs.python.org/3/howto/regex.html): Informations about how to use the `re` library.  \n\nIn the section [Preparation](#preparation) we load all necesary libraries and files  \n* `job_titles`: List with all available job titles  \n* `data_dictionary`: Description how to create the submission file. We will use this later, to create a template for the `submission`.\n* `jobs`: DataFrame with the plain text of all job bulletins. This will be our working dataset.  \n* `submission`: Later we will create this DataFrame for the final submission file.  \n\nOne of the main tasks is to extract the data. This happens in the [Data Extraction](#data_extraction), where we will extract the data step by step using regular expressions. For example, in the first step, the `'Requirements'` section is extracted and then in the second step, the Details from this (`School type, Education years, ...`).  \nThe next Step in the [Submission](#submission) is to create the final submission file. We will use the `data_dictionary` to create a empty DataFrame with all necessary columns and fill them in the required format. In addition, there is a validation function that gives an overview of where potential errors exist.  \nAfter we have our data, we use [Exploratory Data Analysis (EDA)](#eda) to get some insight into the data. There we can see how the distribution of the salary is or which words occur most frequently in the 'Duties' or 'Requirements'."},{"metadata":{},"cell_type":"markdown","source":"## 2. Preparations <a id=\"preparation\"></a>"},{"metadata":{},"cell_type":"markdown","source":"### Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import os\nimport re\nimport math\nimport random\nimport warnings\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport spacy\nnlp = spacy.load('en')\nnlp.remove_pipe('parser')\nnlp.remove_pipe('ner')\n\nfrom wordcloud import WordCloud\nfrom collections import Counter","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Global Parameters"},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"warnings.filterwarnings('ignore')\npd.set_option('display.max_colwidth', -1)\n\nSEED = 13\nrandom.seed(SEED)\nnp.random.seed(SEED)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Load Data"},{"metadata":{},"cell_type":"markdown","source":"#### Job Titles"},{"metadata":{"trusted":true},"cell_type":"code","source":"file_job_titles = '../input/cityofla/CityofLA/Additional data/job_titles.csv'\njob_titles = pd.read_csv(file_job_titles, header=None, names=['job_title'])\njob_titles = job_titles['job_title'].tolist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Kaggle Data Dictionary"},{"metadata":{"trusted":true},"cell_type":"code","source":"file_data_dic = '../input/cityofla/CityofLA/Additional data/kaggle_data_dictionary.csv'\ndata_dictionary = pd.read_csv(file_data_dic)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Job Bulletins"},{"metadata":{"trusted":true},"cell_type":"code","source":"dir_job_bulletins = '../input/cityofla/CityofLA/Job Bulletins'\ndata_list = []\nfor filename in os.listdir(dir_job_bulletins):\n    with open(os.path.join(dir_job_bulletins, filename), 'r', errors='ignore') as f:\n        data_list.append([filename, ''.join(f.readlines())])\njobs = pd.DataFrame(data_list, columns=['file', 'job_description'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For now, we will remove the file `'Vocational Worker  DEPARTMENT OF PUBLIC WORKS.txt'`, because it contains a completely different format."},{"metadata":{"trusted":true},"cell_type":"code","source":"jobs = jobs[jobs['file'] != 'Vocational Worker  DEPARTMENT OF PUBLIC WORKS.txt']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Data Extraction <a id=\"data_extraction\"></a>  \nWe will use regular expressions to extract the relevant information.  \nWe will first divide the text into general parts (metadata, salary, ...) and then extract details from them (metadata -> job title, class code, open date, ...)."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def merge_jobs_data(jobs, extracted_data):\n    \"\"\" Add the extracted_data to the current jobs DataFrame\n\n        param jobs: Current jobs DataFrame\n        param extracted_data: Series with DataFrame inside to extract\n        return jobs: Merged DataFrame\n    \"\"\" \n    jobs['temp'] = extracted_data\n    for index, row in jobs.iterrows():\n        extracted_data = row['temp']\n        if isinstance(extracted_data, pd.DataFrame):\n            for c in extracted_data.columns:\n                jobs.loc[index, c] = extracted_data[c][0]\n    jobs = jobs.drop('temp', axis=1) \n    return jobs\n\ndef extract_text_by_regex(text, regex_dictionary, flags=re.DOTALL):\n    \"\"\" Extract values by regular expressions\n\n        param text: String to extract the values\n        param regex_dictionary: Dictionary with the names and regular expressions to extract\n        return result: Series with the first extracted values\n    \"\"\" \n    regex_dictionary = pd.DataFrame(regex_dictionary, columns=['name', 'regexpr'])\n    result = regex_dictionary.copy()\n    result['text'] = np.NaN\n    for index,row in regex_dictionary.iterrows():\n        find_reg = re.findall(row['regexpr'], text, flags)\n        extracted_text = find_reg[0].strip() if find_reg else np.NaN\n        result.loc[index, 'text'] = extracted_text\n    return result.set_index('name')[['text']].T \n\ndef extract_text_by_regex_index(text, regex_dictionary):\n    \"\"\" Extract values by regular expressions\n    \n        Search for the index of the first occurrence of the regular expression \n        and extract the text to the next regular expression.\n\n        param text: String to extract the values\n        param regex_dictionary: Dictionary with the names and regular expressions to extract\n        return result: Series with the first extracted values\n    \"\"\" \n    regex_dictionary = pd.DataFrame(regex_dictionary, columns=['name', 'regexpr'])\n\n    result = regex_dictionary.copy()\n    result['text'] = np.NaN\n    for index,row in regex_dictionary.iterrows():\n        find_text = re.search(row['regexpr'], text)\n        find_text = find_text.span(0)[0] if find_text else np.nan\n        result.loc[index, 'start'] = find_text\n    result.dropna(subset=['start'], inplace=True)\n    result['end'] = result['start'].apply(lambda x: np.min(result[result['start'] > x]['start'])).fillna(len(text))\n    \n    for index,row in result.iterrows():\n        extracted_text = text[int(row['start']):int(row['end'])]\n        find_reg = re.findall(row['regexpr']+'(.*)', extracted_text, re.DOTALL)\n        extracted_text = find_reg[0].strip() if find_reg else np.NaN\n        result.loc[index, 'text'] = extracted_text\n    return result.set_index('name')[['text']].T \n\ndef nlp_transformation(data, token_pos=None):\n    \"\"\" Use NLP to transform the text corpus to cleaned sentences and word tokens\n\n        param data: List with sentences, which should be processed.\n        param token_pos: List with the POS-Tags to filter (Default: None = All POS-Tags)\n        return processed_tokens: List with the cleaned and tokenized sentences\n    \"\"\"    \n    def token_filter(token):\n        \"\"\" Keep tokens who are alphapetic, in the pos (part-of-speech) list and not in stop list\n            \n        \"\"\"    \n        if token_pos:\n            return not token.is_stop and token.is_alpha and token.pos_ in token_pos\n        else:\n            return not token.is_stop and token.is_alpha\n    \n    data = [re.compile(r'<[^>]+>').sub('', x) for x in data] #Remove HTML-tags\n    processed_tokens = []\n    data_pipe = nlp.pipe(data)\n    for doc in data_pipe:\n        filtered_tokens = [token.lemma_.lower() for token in doc if token_filter(token)]\n        processed_tokens.append(filtered_tokens)\n    return processed_tokens","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Upper Sections  \n* Metadata\n* Salary\n* Duties\n* Requirements\n* Where to apply\n* Application deadline\n* Selection Process\n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"regex_dictionary = [('metadata', r''), \n                      ('salary', r'(?:ANNUAL SALARY|ANNUALSALARY)'),\n                      ('duties', r'(?:DUTIES)'),\n                      ('requirements', r'(?:REQUIREMENTS/MINIMUM QUALIFICATIONS|REQUIREMENT/MINIMUM QUALIFICATION|REQUIREMENT|REQUIREMENTS|REQUIREMENT/MIMINUMUM QUALIFICATION)'),\n                      ('where_to_apply', r'(?:WHERE TO APPLY|HOW TO APPLY)'),\n                      ('application_deadline', r'(?:APPLICATION DEADLINE|APPLICATION PROCESS)'),\n                      ('selection_process', r'(?:SELECTION PROCESS|SELELCTION PROCESS)'),\n                      ]\nextracted_data = jobs['job_description'].dropna().apply(lambda x: extract_text_by_regex_index(x, regex_dictionary))\njobs = merge_jobs_data(jobs, extracted_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Metadata  \n* Job title\n* Class code\n* Open Date\n* Revised"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"regex_dictionary = [('job_title', r'(.*?)(?=\\n)'), \n                      ('class_code', r'(?:Class Code:|Class  Code:)\\s*(\\d\\d\\d\\d)'),\n                      ('open_date', r'(?:Open Date:|Open date:)\\s*(\\d\\d-\\d\\d-\\d\\d)'),\n                      ('revised', r'(?:Revised:|Revised|REVISED:)\\s*(\\d\\d-\\d\\d-\\d\\d)')\n                      ]\nextracted_data = jobs['metadata'].dropna().apply(lambda x: extract_text_by_regex(x, regex_dictionary))\njobs = merge_jobs_data(jobs, extracted_data)\njobs['open_date'] = pd.to_datetime(jobs['open_date'], infer_datetime_format=True)\njobs['revised'] = pd.to_datetime(jobs['revised'], infer_datetime_format=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Salary  \n* Salary from\n* Salary to\n* Flat-rated\n* Additional informations\n* Notes"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Extract the first salary\nregex_dictionary = [('salary_first', r'(\\$(?:\\d{1,3})(?:\\,\\d{3})*(?:\\.\\d{2})* to \\$(?:\\d{1,3})(?:\\,\\d{3})*(?:\\.\\d{2})*|\\$(?:\\d{1,3})(?:\\,\\d{3})*(?:\\.\\d{2})* \\(flat-rated\\))'), \n                      ('salary_additional', r'(?:\\n)(.*)(?:NOTES)'),\n                      ('salary_notes', r'(?:NOTES:)(.*)'),\n                      ]\nextracted_data = jobs['salary'].dropna().apply(lambda x: extract_text_by_regex(x, regex_dictionary, re.DOTALL|re.IGNORECASE))\njobs = merge_jobs_data(jobs, extracted_data)\n# Extract from the first salary the values\nregex_dictionary = [('salary_from', r'\\$((?:\\d{1,3})(?:\\,\\d{3})*(?:\\.\\d{2})*).*'), \n                      ('salary_to', r'(?:to \\$)((?:\\d{1,3})(?:\\,\\d{3})*(?:\\.\\d{2})*).*'),\n                      ('salary_flatrated', r'(flat-rated)')\n                      ]\nextracted_data = jobs['salary_first'].dropna().apply(lambda x: extract_text_by_regex(x, regex_dictionary, re.DOTALL|re.IGNORECASE))\njobs = merge_jobs_data(jobs, extracted_data)\njobs['salary_from'] = jobs['salary_from'].dropna().apply(lambda x: float(x.replace(',', '')))\njobs['salary_to'] = jobs['salary_to'].dropna().apply(lambda x: float(x.replace(',', '')))\njobs['salary_flatrated'] = jobs['salary_flatrated'].dropna().apply(lambda x: True)\njobs.drop('salary_first', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Duties\n* Text\n* Notes"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"regex_dictionary = [('duties_text', r''), \n                      ('duties_notes', r'(?:NOTE:|NOTES:)'),\n                      ]\nextracted_data = jobs['duties'].dropna().apply(lambda x: extract_text_by_regex_index(x, regex_dictionary))\njobs = merge_jobs_data(jobs, extracted_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Requirements\n* Text\n* Notes\n* Certifications"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"regex_dictionary = [('requirements_text', r''), \n                         ('requirements_notes', r'(?:PROCESS NOTES|NOTES:|NOTE:|PROCESS NOTE)'),\n                         ('requirements_certifications', r'(?:SELECTIVE CERTIFICATION|SELECTIVE CERTIFICATION:)'),\n                      ]\nextracted_data = jobs['requirements'].dropna().apply(lambda x: extract_text_by_regex_index(x, regex_dictionary))\njobs = merge_jobs_data(jobs, extracted_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Where to apply\n* Text\n* Notes"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"regex_dictionary = [('where_to_apply_text', r''), \n                         ('where_to_apply_notes', r'(?:NOTE:)'),\n                      ]\nextracted_data = jobs['where_to_apply'].dropna().apply(lambda x: extract_text_by_regex_index(x, regex_dictionary))\njobs = merge_jobs_data(jobs, extracted_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Application deadline\n* Text\n* Notes\n* Review"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"regex_dictionary = [('application_deadline_text', r''), \n                         ('application_deadline_notes', r'(?:NOTE:)'),\n                         ('application_deadline_review', r'(?:QUALIFICATIONS REVIEW|EXPERT REVIEW COMMITTEE)'),\n                      ]\nextracted_data = jobs['application_deadline'].dropna().apply(lambda x: extract_text_by_regex_index(x, regex_dictionary))\njobs = merge_jobs_data(jobs, extracted_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Selection process\n* Text\n* Notes\n* Notice"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"regex_dictionary = [('selection_process_text', r''), \n                         ('selection_process_notes', r'(?:NOTES:)'),\n                         ('selection_process_notice', r'(?:NOTICE:|Notice:)'),\n                      ]\nextracted_data = jobs['selection_process'].dropna().apply(lambda x: extract_text_by_regex_index(x, regex_dictionary))\njobs = merge_jobs_data(jobs, extracted_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Other Details  \n* **Exam Type**"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def get_exam_type(text):\n    \"\"\" Extract the exam type from the text\n\n        param text: String to extract the values\n        return result: String with the exam_type code\n    \"\"\" \n    regex_dic = {'OPEN_INT_PROM':r'BOTH.*INTERDEPARTMENTAL.*PROMOTIONAL', \n                 'INT_DEPT_PROM':r'INTERDEPARTMENTAL.*PROMOTIONAL', \n                 'DEPT_PROM':r'DEPARTMENTAL.*PROMOTIONAL',\n                 'OPEN':r'OPEN.*COMPETITIVE.*BASIS'\n                }\n    result = np.nan\n    for key, value in regex_dic.items():\n        regex = value\n        regex_find = re.findall(regex, text, re.DOTALL)\n        if regex_find:\n            result = key\n            break\n    return result\n\njobs['exam_type'] = jobs['selection_process'].dropna().apply(get_exam_type)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Driver License**"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def get_driver_license(text):\n    \"\"\" Extract the driver license from the text\n\n        param text: String to extract the values\n        return result: String if a driver license is needed\n    \"\"\" \n    regex_dic = {'P':r'(may[^\\.]*requir[^\\.]*driver[^\\.]*license)', \n                 'R':r'(requir[^\\.]*driver[^\\.]*license)|(driver[^\\.]*license[^\\.]*requir)', \n                }\n    result = np.nan\n    for key, value in regex_dic.items():\n        regex = value\n        regex_find = re.findall(regex, text, re.IGNORECASE)\n        if regex_find:\n            result = key\n            break\n    return result\n\njobs['driver_license'] = jobs['job_description'].dropna().apply(get_driver_license)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Salary Informations**"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def extrac_salary(text):\n    reg_expr = r'(\\$(?:\\d{1,3})(?:\\,\\d{3})*(?:\\.\\d{2})* to \\$(?:\\d{1,3})(?:\\,\\d{3})*(?:\\.\\d{2})*|\\$(?:\\d{1,3})(?:\\,\\d{3})*(?:\\.\\d{2})* \\(flat-rated\\))'\n    result = re.findall(reg_expr, text, re.DOTALL|re.IGNORECASE)\n    return pd.Series(result)\n\nsalary = jobs['salary'].dropna().apply(extrac_salary)\nsalary.columns = ['salary_{}'.format(x) for x in salary.columns]\nfor col in salary.columns:\n    jobs[col] = salary[col]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Requirements Details \n* **Main requirements**  \n* **Secondary requirements**  \n\nExtracting the information from the requirements will create several rows per file. Because of this we first save them in a separate DataFrame `requirements`."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def extract_requirements(text):\n    \"\"\" Extract the main and seconday requirements from the text\n\n        param text: String to extract the values\n        return result: DataFrame with the main and seconday requirements\n    \"\"\" \n    reg_expr_set = r'^\\d(?=\\.)'\n    reg_expr_subset = r'^[a-z](?=\\.)'\n    find = re.finditer(reg_expr_set, text, re.MULTILINE|re.IGNORECASE)\n    result = [(x.group(0), x.span(0)[0]) for x in find]\n    result = pd.DataFrame(result, columns=['set', 'start'])\n    result['end'] = result['start'].apply(lambda x: np.min(result[result['start'] > x]['start'])).fillna(len(text))\n    for index,row in result.iterrows():\n        extracted_text = text[int(row['start']):int(row['end'])].strip()\n        result.loc[index, 'set_text'] = extracted_text\n\n        find_subset = re.finditer(reg_expr_subset, extracted_text, re.MULTILINE|re.IGNORECASE)\n        result_subset = [(x.group(0).lower(), x.span(0)[0]) for x in find_subset]\n        result_subset = pd.DataFrame(result_subset, columns=['subset', 'start'])\n        result_subset['end'] = result_subset['start'].apply(lambda x: np.min(result_subset[result_subset['start'] > x]['start'])).fillna(len(extracted_text))\n        for index_sub, row_sub in result_subset.iterrows():\n            extracted_sub_text = extracted_text[int(row_sub['start']):int(row_sub['end'])].strip()\n            result.loc[index, row_sub['subset']] = extracted_sub_text\n            result.loc[index, 'set_text'] = result.loc[index, 'set_text'].replace(extracted_sub_text, '').strip()\n    result.drop(['start', 'end'], axis=1, inplace=True) \n    return result\n\ntemp = jobs['requirements_text'].dropna().apply(extract_requirements)\n\nrequirements = pd.DataFrame()\nfor i, value in temp.iteritems(): \n    if isinstance(value, pd.DataFrame):\n        value['file'] = jobs.loc[i, 'file']\n        requirements = requirements.append(value, ignore_index=True)\n        \nrequirements = requirements.melt(id_vars=['file', 'set', 'set_text'], var_name='subset', value_name='subset_text')\nrequirements = requirements[(requirements['subset'] == 'a') | ((requirements['subset'] != 'a') & (~requirements['subset_text'].isnull()))]  \nrequirements['subset'] = requirements['subset'].apply(lambda x: x.upper())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **School Type**"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def get_school_type(text):\n    \"\"\" Extract the school type from the text\n\n        param text: String to extract the values\n        return result: String with the schol_type code\n    \"\"\" \n    regex_dic = {'COLLEGE OR UNIVERSITY':'college or university', \n                 'HIGH SCHOOL':'high school', \n                 'APPRENTICESHIP':'apprenticeship'\n                }\n    result = np.nan\n    for key, value in regex_dic.items():\n        regex = value\n        regex_find = re.findall(regex, text, re.DOTALL|re.IGNORECASE)\n        if regex_find:\n            result = key\n            break\n    return result\n\nrequirements['school_type'] = requirements['set_text'].dropna().apply(get_school_type)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Education Years**"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def get_education_years(text):\n    \"\"\" Extract the education years from the text\n\n        param text: String to extract the values\n        return result: String with the schol_type code\n    \"\"\" \n    regex_dic = {1:'one', 2:'two', 3:'three', 4:'four', 5:'five',\n                6:'six', 7:'seven', 8:'eight', 9:'nine'}\n    result = np.nan\n    for key, value in regex_dic.items():\n        regex = value+'[/s-]year.*(college or university|high school|apprenticeship)'\n        regex_find = re.findall(regex, text, re.DOTALL|re.IGNORECASE)\n        if regex_find:\n            result = key\n            break\n    return result\n\nrequirements['education_years'] = requirements['set_text'].dropna().apply(get_education_years)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Full Time / Part Time**"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def get_full_part_time(text):\n    \"\"\" Extract the full time / part time from the text\n\n        param text: String to extract the values\n        return result: String with the full time / part time\n    \"\"\" \n    regex_dic = {'FULL_TIME':'full-time', \n                 'PART_TIME':'part-time', \n                }\n    result = np.nan\n    for key, value in regex_dic.items():\n        regex = value\n        regex_find = re.findall(regex, text, re.DOTALL|re.IGNORECASE)\n        if regex_find:\n            result = key\n            break\n    return result\n\nrequirements['full_time_part_time'] = requirements['set_text'].dropna().apply(get_full_part_time)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Experience Length**"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def get_experience_length(text):\n    \"\"\" Extract the experience length from the text\n\n        param text: String to extract the values\n        return result: String with the experience length\n    \"\"\" \n    regex_dic = {1:'one', 2:'two', 3:'three', 4:'four', 5:'five',\n                6:'six', 7:'seven', 8:'eight', 9:'nine'}\n    result = np.nan\n    for key, value in regex_dic.items():\n        regex = r'(?!.*(college or university|high school|apprenticeship)).+'+value+'[\\s-]year.*(full[\\s-]time|part[\\s-]time)'\n        regex_find = re.findall(regex, text, re.DOTALL|re.IGNORECASE)\n        if regex_find:\n            result = key\n            break\n    return result\n\nrequirements['experience_length'] = requirements['set_text'].dropna().apply(get_experience_length)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Job class title of Internal City job**"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def get_exp_job_class_title(text, job_titles=job_titles):\n    \"\"\" Check if the text has a job from the job_titles list\n\n        param text: String to extract the values\n        param job_titles: List with possible job titles\n        return result: String with the job title\n    \"\"\" \n    result = np.nan\n    for x in job_titles:\n        if x.lower() in text.lower():\n            result = x\n            break\n    return result\n\nrequirements['exp_job_class_title'] = requirements['set_text'].dropna().apply(get_exp_job_class_title)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Print Example  \nLet's have a look on an example to see which information has been extracted.  \n(Expand the `output` to see the result)"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"drop_cols = ['job_description', 'metadata', 'salary', 'duties', \n        'requirements', 'where_to_apply', \n        'application_deadline', 'selection_process']\nexample = jobs[jobs['file'] == 'SYSTEMS ANALYST 1596 102717.txt'].drop(drop_cols, axis=1).iloc[0,:].dropna()\nfor idx in example.index:\n    print('\\033[42m'+idx+':'+'\\033[0m')\n    print(example[idx])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Submission <a id=\"submission\"></a>  \nHere we will create the submission file. \nFirst of all we can take a look off the definition from the Kaggle data dictionary.  \n(Expand the `output` to see the result)   \nThen we create an empty DataFrame `submission` based on the given columns and fill this with our extracted data.  \nFinally, we will display an example and run the validation."},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"data_dictionary","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Create Submission file**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize empty DataFrame with 'Field Name' as Columns\nsubmission = pd.DataFrame(columns=data_dictionary['Field Name'].values)\n\n# FILE_NAME\nsubmission['FILE_NAME'] = jobs['file']\n# JOB_CLASS_TITLE\nsubmission['JOB_CLASS_TITLE'] = jobs['job_title']\n# JOB_CLASS_NO\nsubmission['JOB_CLASS_NO'] = jobs['class_code']\n# EXAM_TYPE\nsubmission['EXAM_TYPE'] = jobs['exam_type']\n# OPEN_DATE\nsubmission['OPEN_DATE'] = jobs['open_date']\n# ENTRY_SALARY_GEN\nsubmission['ENTRY_SALARY_GEN'] = jobs['salary_0']\n# ENTRY_SALARY_DWP\nsubmission['ENTRY_SALARY_DWP'] = jobs['salary_1']\n# DRIVERS_LICENSE_REQ\nsubmission['DRIVERS_LICENSE_REQ'] = jobs['driver_license']\n# JOB_DUTIES\nsubmission['JOB_DUTIES'] = jobs['duties_text']\n\n# Merge with Requirements\nsubmission = pd.merge(submission, requirements, left_on='FILE_NAME', right_on='file', how='left')\nsubmission['REQUIREMENT_SET_ID'] = submission['set']\nsubmission['REQUIREMENT_SUBSET_ID'] = submission['subset']\nsubmission['EDUCATION_YEARS'] = submission['education_years']\nsubmission['SCHOOL_TYPE'] = submission['school_type']\nsubmission['EXP_JOB_CLASS_FUNCTION'] = submission['subset_text']\nsubmission['FULL_TIME_PART_TIME'] = submission['full_time_part_time']\nsubmission['EXPERIENCE_LENGTH'] = submission['experience_length']\nsubmission['EXP_JOB_CLASS_TITLE'] = submission['exp_job_class_title']\nsubmission.drop(requirements.columns, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Examples of the Submission file**  \nAs an example we use the 'SYSTEMS ANALYST 1596 102717.txt', which was also given by the Kaggle Team as an example submission. This allows us to check our result with their result.  \n(Expand the `output` to see the result)"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"submission[submission['FILE_NAME'] == 'SYSTEMS ANALYST 1596 102717.txt'].T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Validation of Submission**  \nA function has been defined, to validate out submission against the specifications and return a summary of the result.  \nThe following information are currently available:\n* **Index:** The Name of the Column in the `submission` DataFrame.\n* **Unique Values:** Number of unique values.  \n* **Values:** Number of filled values.  \n* **Null Values:** Number of null-values.  \n* **Accept Null Values:** Does the column accept null-values? (Yes/No).  \n* **Data Type:** Data type of the column.  \n* **Expected Data Type:** Expected data type of the column.  \n* **Check Values:** Check the null-value constraint.\n* **Check Data Type:** Check the data type constraint."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def validate_submission(submission, data_dictionary):\n    \"\"\" Makes some validations and creates a summary of the submission file\n\n        param submission: DataFrame ob the Submission file\n        param data_dictionary: DataFrame with the data dictionary to validate the submission file\n        return result: DataFrame with a summary of the validation\n    \"\"\" \n    result = pd.DataFrame(index=data_dictionary['Field Name'].values)\n    for col in data_dictionary['Field Name'].values:\n        result.loc[col, 'Unique Values'] = len(submission[col].dropna().unique())\n        result.loc[col, 'Values'] = len(submission[col].dropna())\n        result.loc[col, 'Null Values'] = len(submission[submission[col].isnull()][col])\n        result.loc[col, 'Accept Null Values'] = data_dictionary[data_dictionary['Field Name'] == col]['Accepts Null Values?'].values[0]\n        result.loc[col, 'Data Type'] = submission[col].dtype\n        result.loc[col, 'Expected Data Type'] = data_dictionary[data_dictionary['Field Name'] == col]['Data Type'].values[0]\n        result.loc[col, 'Check Values'] = ('Okay' if (result.loc[col, 'Accept Null Values'] == 'Yes') or (result.loc[col, 'Null Values'] == 0) else 'No Null Values allowed')\n        result.loc[col, 'Check Data Type'] = ('Okay' \n                                              if ((result.loc[col, 'Data Type'] == 'object') and (result.loc[col, 'Expected Data Type'] == 'String'))\n                                              or ((result.loc[col, 'Data Type'] == 'float64') and (result.loc[col, 'Expected Data Type'] == 'Float'))\n                                              or ((result.loc[col, 'Data Type'] == 'datetime64[ns]') and (result.loc[col, 'Expected Data Type'] == 'Date'))\n                                              or ((result.loc[col, 'Data Type'] == 'int64') and (result.loc[col, 'Expected Data Type'] == 'Integer'))\n                                              else 'Check Data Type')\n    return result\n\nvalidate_submission(submission, data_dictionary)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Exploratory Data Analysis (EDA) <a id=\"eda\"></a>"},{"metadata":{},"cell_type":"markdown","source":"### Missing Values  \nHere we can check where data is available and which information is less frequently available.  \nSince certain information needs to be checked and the data extraction may need to be adjusted, this plot may still change."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"temp = jobs.fillna('Missing')\ntemp = temp.applymap(lambda x: x if x == 'Missing' else 'Available')\nfigsize_width = 12\nfigsize_height = len(temp.columns)*0.5\nplt_data = pd.DataFrame()\nfor col in temp.columns:\n    temp_col = temp.groupby(col).size()/len(temp.index)\n    temp_col = pd.DataFrame({col:temp_col})\n    plt_data = pd.concat([plt_data, temp_col], axis=1)\n    \nax = plt_data.T.plot(kind='barh', stacked=True, figsize=(figsize_width, figsize_height))\n\n# Annotations\nlabels = []\nfor i in plt_data.index:\n    for j in plt_data.columns:\n        label = '{:.2%}'.format(plt_data.loc[i][j])\n        labels.append(label)\npatches = ax.patches\nfor label, rect in zip(labels, patches):\n    width = rect.get_width()\n    if width > 0:\n        x = rect.get_x()\n        y = rect.get_y()\n        height = rect.get_height()\n        ax.text(x + width/2., y + height/2., label, ha='center', va='center')\n\nplt.xlabel('Frequency')\nplt.title('Missing values')\nplt.xticks(np.arange(0, 1.05, 0.1))\nplt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Open Date  \nThis plot should show when new jobs should be occupied."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt_data = (jobs.groupby(jobs['open_date'].dropna().dt.strftime('%Y-%m')).size())\nplt_data = pd.DataFrame(plt_data)\nplt_data.plot(kind='bar', figsize=(15, 5))\nplt.xlabel('Month')\nplt.ylabel('Quantity')\nplt.title('Distribution over time')\nplt.legend('')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Salary  \nHere the distribution of the salary can be checked.  \n50% of jobs will earn at least \\$80,000.  \nThe maximum annual salary is currently \\$280,000."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt_data = jobs[['salary_from', 'salary_to']]\nplt_data.plot(kind='box', showfliers=True, vert=False, figsize=(12, 3), grid=True)\nplt.xticks(range(0, 300001, 25000))\nplt.xlabel('Salary')\nplt.title('Salary Distribution')\nplt.show()\nplt_data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt_data = jobs[['salary_from', 'salary_to']]\nplt_data.plot(kind='hist', bins=1000, density=True, histtype='step', cumulative=True, figsize=(15, 7), lw=2, grid=True)\nplt.xlabel('Salary')\nplt.ylabel('Cumulative')\nplt.title('Cumulative histogram for salary')\nplt.legend(loc='upper left')\nplt.xlim([25000, 200000])\nplt.xticks(range(25000, 200001, 10000))\nplt.yticks(np.arange(0, 1.05, 0.05))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Wordcount"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt_data_duties = jobs['duties_text'].astype(str).apply(lambda x: len(x.split()))\nplt_data_requirements = jobs['requirements_text'].astype(str).apply(lambda x: len(x.split()))\nplt_data = pd.DataFrame([plt_data_duties, plt_data_requirements]).T\n\nplt_data.plot(kind='box', showfliers=False, vert=False, figsize=(12, 3), grid=True)\nplt.xticks(range(0, 201, 10))\nplt.xlabel('Words')\nplt.title('Word count')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Exam Type  \nOPEN_INT_PROM = 'Open or Competitive Interdepartmental Promotional'  \nINT_DEPT_PROM = 'Interdepartmental Promotional'  \nDEPT_PROM = 'Departmental Promotional'  \nOPEN = 'Exam open to anyone'  \nNone = Not defined"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt_data = jobs['exam_type'].fillna('None')\nplt_data = plt_data.groupby(plt_data).size()\nplt_data.plot(kind='pie', figsize=(10, 5), autopct='%.2f')\nplt.title('Exam Type')\nplt.ylabel('')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Driver License \nP = 'Posible'  \nR = 'Required'  \nNone = Not defined"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt_data = jobs['driver_license'].fillna('None')\nplt_data = plt_data.groupby(plt_data).size()\nplt_data.plot(kind='pie', figsize=(10, 5), autopct='%.2f')\nplt.title('Driver License')\nplt.ylabel('')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Wordcloud (Duties)  \nThe most frequently found verbs in the duties text."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"pos_tags = ['NOUN', 'VERB', 'PROPN', 'ADJ']\nplot_cols = 2\nplot_rows = math.ceil(len(pos_tags) / plot_cols)\naxisNum = 0\nplt.figure(figsize=(7*plot_cols, 4*plot_rows))\nfor pos_tag in pos_tags:\n    plt_data = nlp_transformation(jobs['duties_text'].dropna(), [pos_tag])\n    plt_data = [j for i in plt_data for j in i]\n    plt_data=Counter(plt_data)\n    wordcloud = WordCloud(margin=0, max_words= 40, random_state=SEED).generate_from_frequencies(plt_data)\n    axisNum += 1\n    ax = plt.subplot(plot_rows, plot_cols, axisNum)\n    plt.imshow(wordcloud, interpolation='bilinear')\n    title = 'Duties ({})'.format(pos_tag)\n    plt.title(title)\n    plt.axis(\"off\")\nplt.show()    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Wordcloud (Requirements)  \nThe most frequently found noun, verbs, proper noun and adjective in the requirement text."},{"metadata":{"trusted":true},"cell_type":"code","source":"pos_tags = ['NOUN', 'VERB', 'PROPN', 'ADJ']\nplot_cols = 2\nplot_rows = math.ceil(len(pos_tags) / plot_cols)\naxisNum = 0\nplt.figure(figsize=(7*plot_cols, 4*plot_rows))\nfor pos_tag in pos_tags:\n    plt_data = nlp_transformation(jobs['requirements_text'].dropna(), [pos_tag])\n    plt_data = [j for i in plt_data for j in i]\n    plt_data=Counter(plt_data)\n    wordcloud = WordCloud(margin=0, max_words= 40, random_state=SEED).generate_from_frequencies(plt_data)\n    axisNum += 1\n    ax = plt.subplot(plot_rows, plot_cols, axisNum)\n    plt.imshow(wordcloud, interpolation='bilinear')\n    title = 'Requirements ({})'.format(pos_tag)\n    plt.title(title)\n    plt.axis(\"off\")\nplt.show()    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Wordcloud (Requirements - Job class title)  \nThe most frequently found job class titles in the requirement text."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt_data = requirements['exp_job_class_title'].dropna().tolist()\nplt_data=Counter(plt_data)\nplt.figure(figsize=(10, 10))\n\nwordcloud = WordCloud(margin=0, random_state=SEED).generate_from_frequencies(plt_data)\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.title('Requirements (Job class title)')\nplt.axis(\"off\")\nplt.show() ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}