{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1>Data Science for Good: City of Los Angeles</h1>\n\n<h2>Table of contents</h2>\n- [1. Overview](#1)\n    - [1.1 Related existing systems at City of LA](#1.1)\n    - [1.2 Problem Statement](#1.2)\n    - [1.3 Challenges](#1.3)\n    - [1.4 Ideal Solution](#1.4)\n- [2. What's in the Kernel](#2)\n- [3. Data Conversion](#3)\n    - [3.1 Functions & Logic Explanations](#3.1)\n- [4. Exploratory Data Analysis](#4)\n    - [4.1 Recommendation 1 (Reduce bias : competency analysis)](#4.1)\n    - [4.2 Recommendation 2 (Selection of job opening days)](#4.2)\n    - [4.3 Recommendation 3 (Engage quality applications)](#4.3)\n    - [4.4 Recommendation 4 (Increase applicant pool : content analysis)](#4.4)\n    - [4.5 Recommendation 5 (Readable to all : reach large audience)](#4.5)\n    - [4.6 Recommendation 6 (Attract diversity)](#4.6)\n    - [4.7 Recommendation 7 (HR Buzz words)](#4.7)\n    - [4.8 Recommendation 8 (Neutral message)](#4.8)\n    - [4.9 Recommendation 9 (Convice applicants)](#4.9)\n    - [4.10 Recommendation 10 (Find the match)](#4.10)\n- [5. Applications](#5)\n    - [5.1 Application 1 - Job Search](#5.1)\n    - [5.2 Application 2 - Readability Score](#5.2)\n    - [5.3 Application 3 - Gender Biasedness Check](#5.3)\n    - [5.4 Application 4 - Promotional Paths](#5.4)\n- [6. Extra Notes](#6)\n\n<a id='1'></a>    \n<h1><b><u>1. Overview</u></b></h1>\n\nThe City of Los Angeles faces a big hiring challenge - 1/3 of its 50,000 workers are eligible to retire by July of 2020. This is an issue of big concern.\n\nAs mentioned by one of the competition hosts, there are some job classes that will be challenging to fill with qualified candidates fullfilling the vision of City of LA.<br>\nTheir **vision** is to maintain excellence, provide consistent services to customers, and to meet the challenges of tomorrow through expertise, innovation, and cooperative partnerships. To acheive this, they aim to fill the vacancies with quality resources maintaining diversity.\n\nWe tried to parse the data for job bulletins and tried to simplify and highlight things to realise good and bad points to overcome this challenge. Here are few points we tried to focus on in the successive sections:\n- To get insights from the text.\n- To identify implicit bias in text that might be putting off people of specific gender, age, race from applying to a job class.\n- To achieve high quality and quantity of applications.\n- To identify and simplify the process of promotional paths.\n\n<a id='1.1'></a>\n<h3><b>1.1 Related Existing Systems at City of LA</b></h3>\n\n<h4>Job Match</h4>\nThe functionality is available at LA city [web portal.](https://personline.lacity.org/apps/jobmatch/)\n<br>Users can select multiple fields of their interest, education/training and experience, based on which jobs are searched and displayed.\n\n<h4>Job Analysis</h4>\nBelow is an excerpt from LA city [web portal.](http://per.lacity.org/index.cfm?content=jobanalyses)\n\nJob analysis is the systematic identification and documentation of the tasks performed on the job and the competencies required to perform the tasks based on information/data provided by a representative group of job experts. \n<br>Information in the job analysis must be sufficiently descriptive and detailed to provide an understanding of the job.\n\n- **Job** is a class in the City of Los Angeles Classification Plan.\n- **Tasks** are discrete, self-contained units of work. They reflect the specific behavior of individuals when performing the work.\n- **Competencies** are the knowledge, skill, ability, aptitude, capability, or other personal characteristics needed to perform the job.\n- **Representative** group refers to a sufficiently large sample that proportionally reflects the class.\n- **Job experts** are individuals with in-depth knowledge of the job and its requirements.\n\n<br>City of Los Angeles Personnel Department staff conducts job analyses by interviewing job experts, usually job incumbents and/or supervisors, and having them complete questionnaires. A formal observation of the job may also be conducted as part of the job analysis.\n\n<h4>Job Awareness</h4>\nFrom AMA discussion section: \n- Social media promotions\n- Word of mouth, general interest in government.\n- Job bulletins are available on current job openings [web page.](https://www.lacity.org/find-jobs/latest-job-opportunities)\n\n<a id='1.2'></a>\n<h3><b>1.2 Problem Statement</b></h3>\n\nKaggle describes it as:\n<br>For filling vacancies several job ads are posted which plays one of the vital role to attract job applications. The content, tone, and format of job bulletins can influence the quality of the applicant pool. Overly-specific job requirements may discourage diversity. The Los Angeles Mayor’s Office wants to reimagine the city’s job bulletins by using text analysis to identify needed improvements.\n\nThe goal is to convert a folder full of plain-text job postings into a single structured CSV file and then to use this data to:\n\n1. identify language that can negatively bias the pool of applicants;\n2. improve the diversity and quality of the applicant pool;\n3. make it easier to determine which promotions are available to employees in each job class.\n\n<h4>Data Description</h4>\nThe job bulletins are provided as a folder of plain-text files, one for each job classification.\n\nJob Bulletins Folder\n- 683 plain-text job postings\n\nInstructions and Additional Documents Folder\n- Job Bulletins with Annotations\n- Annotation Descriptions.docx\n- City Job Paths\n- PDFs\n- Description of promotions in job bulletins.docx\n- Job_titles.csv\n- Kaggle_data_dictionary.csv\n- Sample job class export template.csv\n\n<a id='1.3'></a>\n<h3><b>1.3 Challenges</b></h3>\n\nDealing with unstructured data is a real challenge yet the most important task. The plain text/data have a low degree of organization and is thus bit difficult to scan. It is on top priority to make this kind of data easily scannable to make it into a good use. \n\nHere as well, the Job bulletins have data which is in unstructured format. It has internal structure but is not structured via pre-defined data models or schema which makes it difficult to parse.<br>\n**Extracting information from text files, converting unstructured information into a structured format, combining the data, and ensuring data quality are a few roadblocks** faced when it comes to making sense of the data. \n<br>To get head around nondescript data and use it to validate assumptions and support decision-making ability, as well as integrate it with visualization tools is a challenge which needs to be overcome. \n<br>Finding an ideal data extraction solution required some time and effort, but it allows to **consolidate structured and unstructured data and give it a clear purpose for a more effective solution**.\n\n<a id='1.4'></a>\n<h3><b>1.4 Possible Solution Highlights</b></h3>\n\n<b>1. Be where your candidates are!</b>\n- <b>Professional Job Platforms (<u>LinkedIn</u>)</b>\n    <br>&emsp;City of LA web portal doesn't have link to the most used professionals' network portal.\n- <b>Social networking sites</b>\n<br>&emsp;Whether that’s Snapchat, Reddit, or LinkedIn. Use social media platform which helps to logon to most popular websites and reach large audience.\n- <b>Mobile App</b>\n<br>&emsp;According to Indeed, 65% of job seekers use their mobile devices to look for jobs. The ability to apply on mobile is especially important for hourly workers who might not have access to a desktop computer.\n\n<b>2. Deal with bias</b>\n- <b>Proactive measures</b>\n<br>&emsp;Understanding the hiring bias and how it works and by being actively alert to it creeping into your hiring process, you can nip it in the bud.\n- <b>Evaluate</b>\n<br>&emsp;Shortlist!! - Names, pictures and even post codes can influence the hiring decision. Ideally, candidates are judged on their qualifications and skills, not geography or the characteristics of their particular demographic. A **blind resume review** helps eliminate a number of unconscious biases from the process, while also help to avoid overcompensating for the same biases.\n<br><br>&emsp;Put an everyday problem to your candidates, and assess whether their solution aligns with your organisation’s values or ways of getting things done. This keeps everything professional, and avoids a number of common biases, such as age, gender, personality or appearance.\n<br><br>&emsp;Trainings to mitigate biases and increase cultural competency\n\n<b>3. Diversity Initiatives</b>\n- <b>For Female Employees</b>\n<br>&emsp;Show a viable path forward. Introduce them with females at senior levels or visible role models.\n<br>&emsp;Improve and showcase work life balance.\n<br>&emsp;Improve and showcase childcare and parental leaves.\n- <b>For General Diversity</b>\n<br>&emsp;**Standardize the interview process**. In order to get to the interview stage, you need to have an open minded diverse team of interviewee to ensure you’re not putting off great candidates.\n<br>&emsp;Morever most people like everything straight. Indeed found **42% of job seekers found lengthy applications the most frustrating part of the application process.** Try to cut-out on unwanted information. Keep it short: If possible, reduce candidate friction by creating a 1-click application process. If that doesn’t work for you, keep your qualification questions to a minimum (e.g., five and less)\n<br><br>&emsp;The job description is often the first insight candidates have into your organisation’s culture. **Be as neutral as possible**, and be aware of the message that certain language sends.\n<br>&emsp;You don’t have to write a completely bland job description devoid of any adjectives. However, if you seek to strike a balance, you may see changes in your pool of candidates. \n\n<a id='2'></a>\n<h1><b><u>2. What's in the Kernel</u></b></h1>\n\n<h3>Data Conversion</h3>\n\n<img src=\"https://i.imgur.com/ZSzPV5Q.jpg\" width='800' height='700'>\n\n<h3>Exploratory Data Analysis</h3>\n\n<img src=\"https://i.imgur.com/mCllTr3.jpg\" width='800' height='700'>\n\n<h3>Recommendations</h3>\n\n<img src=\"https://i.imgur.com/QdpKlhV.jpg\" width='800' height='700'>\n\n<h3>Applications</h3>\n<img src=\"https://i.imgur.com/iy67Vyc.jpg\" width='800' height='700'>\n<br><img src=\"https://i.imgur.com/3NgHseJ.jpg\" width='800' height='700'>\n\n<br><br><br><br>\n<a id='3'></a>\n<h1><b><u>3. Data Conversion</u></b></h1>\n\n**Loading libraries**"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"!pip install word2number\n!pip install textstat\n!pip install syllables\n\n# Import python packages\nimport os, sys\nimport pandas as pd,numpy as np\nimport re\nimport spacy\nfrom os import walk\nimport shutil\nfrom shutil import copytree, ignore_patterns\nfrom spacy import displacy\nfrom collections import Counter\nimport en_core_web_sm\nnlp = en_core_web_sm.load()\nimport xml.etree.cElementTree as ET\nfrom collections import OrderedDict\nimport json\nfrom __future__ import unicode_literals, print_function\nimport plac\nimport random\nfrom pathlib import Path\nfrom spacy.util import minibatch, compounding\nfrom spacy.matcher import Matcher\nfrom word2number import w2n\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime\nfrom datetime import date\nimport calendar\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom itertools import takewhile, tee\nimport itertools\nimport nltk, string\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tag import pos_tag\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.cluster.util import cosine_distance\nimport networkx as nx\nfrom PIL import Image,ImageFilter\nimport textstat\nfrom textstat.textstat import textstatistics, easy_word_set, legacy_round \nimport syllables\nfrom IPython.display import display, HTML, Javascript\n\nbulletin_dir = \"../input/data-science-for-good-city-of-los-angeles/cityofla/CityofLA/Job Bulletins\"\nadditional_data_dir = '../input/data-science-for-good-city-of-los-angeles/cityofla/CityofLA/Additional data'\nSTOP_WORDS = stopwords.words('english')\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Master configuration**\n<br>Read files (job bulletins, data dictionary & job titles)\n<br><br>Here, xml configuration is also created for few fixed/master data to be searched for in the bulletins.\n<br>(e.g. For SchoolType master values are 'College or University,High School,Apprenticeship'.\n<br>To add master data for the sections, just add it to the configuration."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dictionary = pd.read_csv(os.path.join(additional_data_dir,\"kaggle_data_dictionary.csv\"))\njobs_list = []\nfor file_name in os.listdir(bulletin_dir):\n    with open(os.path.join(bulletin_dir,file_name), encoding = \"ISO-8859-1\") as f:\n        content = f.read()\n        jobs_list.append([file_name, content])\njobs_df = pd.DataFrame(jobs_list)\njobs_df.columns = [\"FileName\", \"Content\"]\n\njob_titles = pd.read_csv(additional_data_dir+'/job_titles.csv', header=None)\n\njob_titles_str = ','.join(job_titles[0])\n\nconfigfile = r'''\n<Config-Specifications>\n<Term name=\"Requirements\">\n        <Method name=\"section_value_extractor\" section=\"RequirementSection\">\n            <SchoolType>College or University,High School,Apprenticeship</SchoolType>\n            <JobTitle>'''+job_titles_str+'''</JobTitle>\n        </Method>\n    </Term>\n</Config-Specifications>\n'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Read and preview a file**"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"print(jobs_df['Content'].values[0][:1000])","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print(jobs_df['Content'].values[1][:1000])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3><b>Interpretations</b></h3>\n- job bulletins follow a <b>generic format</b>. \n    - Job Title\n    - Class Code\n    - Open Date\n    - Salary details\n    - Duties\n    - Requirements\n    - continue...\n- <b>Headers are all in upper case letters.</b>\n<br>There are some guidelines that authors of the bulletins follow.\n<br><br>As mentioned by one of the competition hosts, in the AMA, the guidelines for the authors to follow for a job bulletin that \"The source document the City must use to guide all bulletin is Charter Section 1007. This states that the City has to notify applicants of the various elements: e.g., the time, place, and general scope of every exam. These are the absolute minimum.\"\n<br><br>Full rules are listed [here](https://personnel.lacity.org/documents/EEO%20Handbk%20CityComm.pdf)\n\n**Code Functions**\n<br>(Used in parsing files and creating structured data)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def createDataFrameAndDictionary(jobs_df, job_titles, data_dictionary):\n    headingsFrame = createHeadingsFrame()\n    jobClassDetailsFrame = createJobClassDetailsFrame()\n    requirementsFrame = createRequirementsFrame(headingsFrame)\n    df_salary_dwp,df_salary_gen = createSalaryFrame(headingsFrame)\n    \n    print('Adding EDUCATION_MAJOR')\n    requirementsFrame['EDUCATION_MAJOR']=requirementsFrame.apply(getEducationMajor, axis=1)\n    requirementsFrame['EDUCATION_MAJOR']=requirementsFrame.apply(getApprenticeshipMajor, axis=1)\n\n    print('Adding JOB_DUTIES')\n    df_duties = getValues('duties','JOB_DUTIES', headingsFrame)\n    \n    requirementsFrame = addColumnsFromConfig(df_requirements=requirementsFrame)\n    \n    requirementsFrame['EDUCATION_YEARS'] = requirementsFrame.apply(getEducationYears, axis=1)\n    print('Adding experience details...')\n    requirementsFrame['REQUIREMENT_TEXT_WITH_K1'] = requirementsFrame.apply(getRequirementTextWithK1, axis=1)\n    requirementsFrame['EXPERIENCE_TEXT'] = requirementsFrame.apply(getExperienceText, axis=1)\n    requirementsFrame['REQUIREMENT_WITHOUT_EXPERIENCE_TEXT'] = requirementsFrame.apply(getTextWithoutExperienceText, axis=1)\n    requirementsFrame['EXP_JOB_CLASS_ALT_RESP'] = requirementsFrame.apply(lambda x : getJobFunction(x,'K1b'), axis=1)\n    requirementsFrame['EXP_JOB_CLASS_FUNCTION'] = requirementsFrame.apply(lambda x : getJobFunction(x,'K2'), axis=1)\n    requirementsFrame['EXPERIENCE_LENGTH'],requirementsFrame['EXPERIENCE_LEN_UNIT'] = zip(*requirementsFrame.apply(getExperienceLength, axis=1))\n    requirementsFrame['FULL_TIME_PART_TIME'],requirementsFrame['PAID_VOLUNTEER'] = zip(*requirementsFrame.apply(getExperienceType, axis=1))\n    \n    requirementsFrame.loc[requirementsFrame['EXPERIENCE_LEN_UNIT']=='year','EXPERIENCE_LEN_UNIT']='years'\n    requirementsFrame.loc[requirementsFrame['EXPERIENCE_LEN_UNIT']=='month','EXPERIENCE_LEN_UNIT']='months'\n\n    print('Adding course details...')\n    requirementsFrame['COURSE_COUNT'] = requirementsFrame.apply(getCourseCount, axis=1)\n    requirementsFrame['COURSE_LENGTH'],requirementsFrame['COURSE_LENGTH_TEXT'] = zip(*requirementsFrame.apply(getCourseLength, axis=1))\n    requirementsFrame['COURSE_SUBJECT'] = requirementsFrame.apply(getCourseSubjects, axis=1)\n    requirementsFrame['MISC_COURSE_DETAILS']= requirementsFrame.apply(lambda x: x['REQUIREMENT_TEXT'] if x['COURSE_LENGTH']=='' and 'course' in x['REQUIREMENT_TEXT'] else '', axis=1)\n    \n    print('Adding license details...')\n    processNotesFrame = getValues('process note', 'PROCESS_NOTES', headingsFrame)\n    requirementsFrame = requirementsFrame.merge(right=processNotesFrame,how='left',left_on='FILE_NAME',right_on='FILE_NAME')\n    requirementsFrame['DRIVERS_LICENSE_REQ']=''\n    requirementsFrame['DRIV_LIC_TYPE']=''\n    requirementsFrame['DRIVERS_LICENSE_REQ'],requirementsFrame['DRIV_LIC_TYPE'] = zip(*requirementsFrame.apply(\n        lambda x : getLicenseRequired(x, 'PROCESS_NOTES'), axis=1))\n    requirementsFrame['DRIVERS_LICENSE_REQ'],requirementsFrame['DRIV_LIC_TYPE'] = zip(*requirementsFrame.apply(\n        lambda x : getLicenseRequired(x, 'REQUIREMENT_TEXT'), axis=1))\n    requirementsFrame['ADDTL_LIC'] = requirementsFrame.apply(lambda x: getAdditionalLic(x,'PROCESS_NOTES'), axis=1)\n    \n    jobs_df['ExamTypeContent'] = jobs_df.apply(getExamTypeContent, axis=1)\n    jobs_df['EXAM_TYPE'] = jobs_df.apply(setExamTypeContent, axis=1)\n    jobs_df.loc[jobs_df['EXAM_TYPE']=='','EXAM_TYPE']='OPEN'\n\n    appDeadlineFrame = getValues('deadline', 'APPLICATION_DEADLINE_TEXT', headingsFrame)\n    selectionProcessFrame = getValues(searchText='selection', COL_NAME='SELECTION_PROCESS', headingsFrame=headingsFrame)\n    whereToApplyFrame = getValues(COL_NAME='WHERE_TO_APPLY',searchText='where to', headingsFrame=headingsFrame)\n\n    appDeadlineFrame['APPLICATION_DEADLINE'] = appDeadlineFrame.apply(getAppDeadlineDate, axis=1)\n    \n    print('Adding selection criteria details...')\n    selectionProcessFrame['SELECTION_CRITERIA'] = selectionProcessFrame['SELECTION_PROCESS'].apply(getSelection)\n    whereToApplyFrame['WHERE_TO_APPLY'] = whereToApplyFrame['WHERE_TO_APPLY'].apply(getWhereToApplyUrl)\n    \n    #Merge all frames to create a single resultset for final submission\n    job_class_df = pd.merge(jobClassDetailsFrame, requirementsFrame, how='inner', left_on='FILE_NAME', right_on='FILE_NAME', sort=True)\n    job_class_df = pd.merge(job_class_df, df_salary_dwp, how='left', left_on='FILE_NAME', right_on='FILE_NAME', sort=True)\n    job_class_df = pd.merge(job_class_df, df_salary_gen, how='left', left_on='FILE_NAME', right_on='FILE_NAME', sort=True)\n    job_class_df = pd.merge(job_class_df, df_duties, how='left', left_on='FILE_NAME', right_on='FILE_NAME', sort=True)\n    job_class_df = pd.merge(job_class_df, jobs_df, how='left', left_on='FILE_NAME', right_on='FileName', sort=True)\n    job_class_df = pd.merge(job_class_df, appDeadlineFrame, how='left', left_on='FILE_NAME', right_on='FILE_NAME', sort=True)\n    job_class_df = pd.merge(job_class_df, selectionProcessFrame, how='left', left_on='FILE_NAME', right_on='FILE_NAME', sort=True)\n    job_class_df = pd.merge(job_class_df, whereToApplyFrame, how='left', left_on='FILE_NAME', right_on='FILE_NAME', sort=True)\n\n    job_class_df.drop(columns=['FileName', 'Content', 'ExamTypeContent','REQUIREMENT_TEXT_WITH_K1', 'EXPERIENCE_TEXT',\n                    'REQUIREMENT_WITHOUT_EXPERIENCE_TEXT','COURSE_LENGTH_TEXT','REQUIREMENT_TEXT','PROCESS_NOTES',\n                    'APPLICATION_DEADLINE_TEXT','SELECTION_PROCESS'],inplace=True)\n    \n    print('Updating data dictionary...\\n')\n    \n    #Update allowed values for DRIV_LIC_TYPE\n    data_dictionary.loc[data_dictionary['Field Name']=='DRIV_LIC_TYPE','Allowable Values'] = 'A,B,C,I'\n\n    data_dictionary.loc[-1] = ['EXPERIENCE_LEN_UNIT','','The unit of experience length(e.g hours, years)','String','','Yes','']\n    data_dictionary.index = data_dictionary.index+1\n\n    data_dictionary.loc[-1] = ['PAID_VOLUNTEER','','Whether the required experience is paid or volunteer or both.','String','PAID, VOLUNTEER','Yes','In case if both allowed, PAID|VOLUNTEER is used.']\n    data_dictionary.index = data_dictionary.index+1\n\n    data_dictionary.loc[-1] = ['APPLICATION_DEADLINE','','Date on which applications will be closed.','String','','Yes','']\n    data_dictionary.index = data_dictionary.index+1\n\n    data_dictionary.loc[-1] = ['SELECTION_CRITERIA','','Selection Criteria','String','','Yes','Some classes have multiple selection criteria, in that case criteria are separated by |.']\n    data_dictionary.index = data_dictionary.index+1\n\n    data_dictionary.loc[-1] = ['REQUIREMENT_CONJ','','Requirement/subrequirement conjuction','String','','Yes','If requirement_subset_id is blank then this will represent conjuction value for requirement else for sub requirement.']\n    data_dictionary.index = data_dictionary.index+1\n\n    data_dictionary.loc[-1] = ['WHERE_TO_APPLY','','Online Url for job application','String','','Yes','Any available url in where to apply section of the job class']\n    data_dictionary.index = data_dictionary.index+1\n    data_dictionary = data_dictionary.sort_index()\n\n    data_dictionary.loc[data_dictionary['Field Name']=='REQUIREMENT_SUBSET_ID','Accepts Null Values?'] = 'Yes'\n    data_dictionary.loc[data_dictionary['Field Name']=='REQUIREMENT_SET_ID','Accepts Null Values?'] = 'Yes'\n    data_dictionary.loc[data_dictionary['Field Name']=='JOB_CLASS_NO','Accepts Null Values?'] = 'Yes'\n\n    print('Final list of columns:\\n')\n    print(job_class_df.columns)\n    return job_class_df, data_dictionary","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='3.1'></a>\n**Functions (Logic & Explanations):**\n- createHeadingsFrame: to parse for headings in the text files.\n    - Upper Case text is considered as a heading\n- createJobClassDetailsFrame: to parse for FILE_NAME, JOB_CLASS_TITLE, JOB_CLASS_NO ,OPEN_DATE\n    - Searched for simple texts(e.g. Open Date, Class Code). Title is taken from first heading of the file.\n- getRequirements: get requirements section text\n    - get all text after requirements heading text till next heading is reached.\n- getRequirement_Conj: get conjunction from last word of requirement sets\n- createRequirementsFrame: to parse for REQUIREMENT_SET_ID,REQUIREMENT_SUBSET_ID\n    - get first letters from requirements sets\n    - (There are bulletins without requirement set ids. All the requirements section is written without set id. In that case '1' is used to represent requirement set id.\n- createSalaryFrame: to parse for ENTRY_SALARY_DWP, ENTRY_SALARY_GEN\n    - a regex search for amount in salary section\n    - an extra search for 'department of water' text for salary_dwp    "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def createHeadingsFrame():\n    headings = {}\n    for filename in os.listdir(bulletin_dir):\n        with open(bulletin_dir + \"/\" + filename, 'r', errors='ignore') as f:\n            for line in f.readlines():\n                line = line.replace(\"\\n\",\" \").replace(\"\\t\",\" \").replace(\":\",\"\").replace(\"  \",\" \").strip()\n\n                if line.isupper():\n                    if line not in headings.keys():\n                        headings[line] = 1\n                    else:\n                        count = int(headings[line])\n                        headings[line] = count+1\n\n    del headings['$103,606 TO $151,484'] #This is not a heading, it's an Annual Salary component\n    headingsFrame = []\n    for i,j in (sorted(headings.items(), key = lambda kv:(kv[1], kv[0]), reverse = True)):\n        headingsFrame.append([i,j])\n    headingsFrame = pd.DataFrame(headingsFrame)\n    headingsFrame.columns = [\"Heading\",\"Count\"]\n    return headingsFrame\n\ndef createJobClassDetailsFrame():\n    print('Adding FILE_NAME, JOB_CLASS_TITLE, JOB_CLASS_NO ,OPEN_DATE')\n    data_list = []\n    for filename in os.listdir(bulletin_dir):\n        with open(bulletin_dir + \"/\" + filename, 'r', errors='ignore') as f:\n            job_class_title = ''\n            job_class_no=''\n            job_bulletin_date=''\n            for line in f.readlines():\n                #Insert code to parse job bulletins\n                if \"Open Date:\" in line and job_bulletin_date=='':\n                    job_bulletin_date = line.split(\"Open Date:\")[1].split(\"(\")[0].strip()\n                    try:\n                        job_bulletin_date = pd.to_datetime(job_bulletin_date, format=\"%m-%d-%y\")\n                    except:\n                        job_bulletin_date = pd.to_datetime(job_bulletin_date, format=\"%m-%d-%Y\")\n                elif \"Open date:\" in line and job_bulletin_date=='':\n                    job_bulletin_date = line.split(\"Open date:\")[1].split(\"(\")[0].strip()\n                    try:\n                        job_bulletin_date = pd.to_datetime(job_bulletin_date, format=\"%m-%d-%y\")\n                    except:\n                        job_bulletin_date = pd.to_datetime(job_bulletin_date, format=\"%m-%d-%Y\")\n                if \"Class Code:\" in line and job_class_no=='':\n                    job_class_no = line.split(\"Class Code:\")[1].strip()[:4]\n                elif \"Class  Code:\" in line and job_class_no=='':\n                    job_class_no = line.split(\"Class  Code:\")[1].strip()[:4]\n                if len(job_class_title)<2 and len(line.strip())>1 and line.strip() != 'CAMPUS INTERVIEWS ONLY':\n                    if 'Class Code:' in line:\n                        job_class_title = line.split(\"Class Code:\")[0].strip()\n                    else:\n                        job_class_title = line.strip()\n            data_list.append([filename, job_bulletin_date, job_class_title, job_class_no])\n\n    df = pd.DataFrame(data_list)\n    df.columns = [\"FILE_NAME\", \"OPEN_DATE\", \"JOB_CLASS_TITLE\", \"JOB_CLASS_NO\"]\n    return df\n\ndef getRequirements(headingsFrame, COL_NAME='REQUIREMENT_TEXT'):\n    data_list = []\n    dataHeadings = [k for k in headingsFrame['Heading'].values if 'requirement' in k.lower()]\n\n    for filename in os.listdir(bulletin_dir):\n        with open(bulletin_dir + \"/\" + filename, 'r', errors='ignore') as f:\n            readNext = 0 \n            datatxt = ''\n            for line in f.readlines():\n                clean_line = line.replace(\"\\n\",\" \").replace(\"\\t\",\" \").replace(\":\",\"\").replace(\"  \",\" \").strip()\n                if readNext == 0:                         \n                    if clean_line in dataHeadings:\n                        readNext = 1\n                elif readNext == 1:\n                    if clean_line in headingsFrame['Heading'].values:\n                        break\n                    else:\n                         datatxt = datatxt + ' ' + line\n            data_list.append([filename,datatxt.strip()])\n    result = pd.DataFrame(data_list)\n    result.columns = ['FILE_NAME',COL_NAME]\n    return result\n\ndef getRequirement_Conj(txt):\n    \"\"\"Search for conjuction in the end of the passed text value\n    \n    Parameters\n    ----------\n    txt : str\n        text to be searched for conjuction\n    \"\"\"\n    rqmnt_conj = ''\n    allowed_conjs = ['or','and']\n\n    txt = txt.translate(str.maketrans('', '', string.punctuation))\n    li = list(map(str.strip,list(filter(None,txt.split()))))\n    if len(li)>0:\n        rqmnt_conj = li[-1].lower()\n    if rqmnt_conj not in allowed_conjs:\n        rqmnt_conj = ''\n    return rqmnt_conj\n\ndef createRequirementsFrame(headingsFrame):\n    print('Adding REQUIREMENT_SET_ID,REQUIREMENT_SUBSET_ID,REQUIREMENT_TEXT,REQUIREMENT_CONJ')\n    requirements = getRequirements(headingsFrame)\n\n    requirementsAll = []\n    for index,row in requirements.iterrows():\n        filename = row['FILE_NAME']\n        rqmntTxt=requirements[requirements['FILE_NAME']==filename]['REQUIREMENT_TEXT'].values\n        lines = rqmntTxt[0].split('\\n')\n        rid = ''\n        rsid = ''\n        rlineTxt = ''\n        rslineTxt = ''\n\n        for line in lines:\n            l = line.strip()\n            arr = re.split('\\. |\\)', l.replace(\"\\n\",\" \").replace(\"\\t\",\" \").replace(\":\",\"\").replace(\"  \",\" \").strip())\n            if l.replace(\"\\n\",\" \").replace(\"\\t\",\" \").strip() == '':\n                continue\n\n            if arr[0].isdigit():\n                if rlineTxt != '':\n                    if rslineTxt != '':\n                        requirementsAll.append([filename,rid,rsid,rlineTxt + ' - ' + rslineTxt])\n                    else:\n                        requirementsAll.append([filename,rid,rsid,rlineTxt])\n                rid = arr[0]\n                rlineTxt = l\n                rsid = ''\n                rslineTxt = ''\n            elif re.match('^[a-z]$',arr[0]):\n                if rlineTxt != '' and rslineTxt != '':\n                    if rid == '':\n                        rid = '1'\n                    requirementsAll.append([filename,rid,rsid,rlineTxt + ' - ' + rslineTxt])\n                rsid = arr[0]\n                rslineTxt = l\n            else:\n                if rsid == '':\n                    rlineTxt = rlineTxt + ' ' + l\n                else:\n                    rslineTxt = rslineTxt + ' ' + l\n                continue\n        if rid == '':\n            rid = '1'\n        if rslineTxt != '':\n            requirementsAll.append([filename,rid,rsid,rlineTxt + ' - ' + rslineTxt])\n        else:\n            requirementsAll.append([filename,rid,rsid,rlineTxt])\n\n    df_requirements = pd.DataFrame(requirementsAll)\n    df_requirements.columns = ['FILE_NAME','REQUIREMENT_SET_ID','REQUIREMENT_SUBSET_ID','REQUIREMENT_TEXT']\n\n    df_requirements['REQUIREMENT_CONJ'] = df_requirements['REQUIREMENT_TEXT'].apply(getRequirement_Conj)\n    \n    return df_requirements\n\ndef createSalaryFrame(headingsFrame):\n    #Check for salary components\n    salHeadings = [k for k in headingsFrame['Heading'].values if 'salary' in k.lower()]\n    sal_list = []\n    files = []\n    for filename in os.listdir(bulletin_dir):\n        files.append(filename)\n        with open(bulletin_dir + \"/\" + filename, 'r', errors='ignore') as f:\n            readNext = 0\n            for line in f.readlines():\n                clean_line = line.replace(\"\\n\",\" \").replace(\"\\t\",\" \").replace(\":\",\"\").replace(\"  \",\" \").strip()\n                if clean_line in salHeadings:\n                    readNext = 1\n                elif readNext == 1:\n                    if clean_line in headingsFrame['Heading'].values:\n                        break\n                    elif len(clean_line)<2:\n                        continue\n                    else:\n                        sal_list.append([filename, clean_line])\n\n    df_salary = pd.DataFrame(sal_list)\n    df_salary.columns = ['FILE_NAME','SALARY_TEXT']      \n\n    #Adding 'ENTRY_SALARY_GEN','ENTRY_SALARY_DWP'\n    pattern = r'\\$?([0-9]{1,3},([0-9]{3},)*[0-9]{3}|[0-9]+)(.[0-9][0-9])?'\n    dwp_salary_list = {}\n    gen_salary_list = {}\n    for filename in files:\n        for sal_text in df_salary.loc[df_salary['FILE_NAME']==filename]['SALARY_TEXT']:\n            if 'department of water' in sal_text.lower():\n                if filename in dwp_salary_list.keys():\n                    continue\n                matches = re.findall(pattern+' to '+pattern, sal_text) \n                if len(matches)>0:\n                    salary_dwp = ' - '.join([x for x in matches[0] if x and not x.endswith(',')])\n                else:\n                    matches = re.findall(pattern, sal_text)\n                    if len(matches)>0:\n                        salary_dwp = matches[0][0]\n                    else:\n                        salary_dwp = ''\n                dwp_salary_list[filename]= salary_dwp\n            else:\n                if filename in gen_salary_list.keys():\n                    continue\n                matches = re.findall(pattern+' to '+pattern, sal_text)\n                if len(matches)>0:\n                    salary_gen = ' - '.join([x for x in matches[0] if x and not x.endswith(',')])\n                else:\n                    matches = re.findall(pattern, sal_text)\n                    if len(matches)>0:\n                        salary_gen = matches[0][0]\n                    else:\n                        salary_gen = ''\n                if len(salary_gen)>1:\n                    gen_salary_list[filename]= salary_gen\n\n\n    df_salary_dwp = pd.DataFrame(list(dwp_salary_list.items()), columns=['FILE_NAME','ENTRY_SALARY_DWP'])\n    df_salary_gen = pd.DataFrame(list(gen_salary_list.items()), columns=['FILE_NAME','ENTRY_SALARY_GEN'])\n    return df_salary_dwp,df_salary_gen","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Functions (Logic & Explanations) continue...**\n- getEducationMajor: to parse for EDUCATION_MAJOR\n    - Used nltk regex parser to search for Nouns/pronouns part of speech tags following the words like major/majoring.\n- getApprenticeshipMajor: to parse for EDUCATION_MAJOR\n    - same as finding education major except for an additional search for words like apprenticeship\n- addColumnsFromConfig: to parse for columns defined in config section(master configuration)\n    - read master data from xml config & search for the data in text"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#Idea here is to first create part of speech tags\n#And then find Noun/Pronoun tags following the words like majoring/major/apprenticeship\n\ndef preprocess(txt):\n    txt = nltk.word_tokenize(txt)\n    txt = nltk.pos_tag(txt)\n    return txt\n\ndef getEducationMajor(row):\n    \"\"\"Returns education majors from the text supplied in the DataFrame row\"\"\"\n    \n    txt = row['REQUIREMENT_TEXT']\n    txtMajor = ''\n    if 'major in' not in txt.lower() and ' majoring ' not in txt.lower():\n        return txtMajor\n    result = []\n    \n    istart = txt.lower().find(' major in ')\n    if istart!=-1:\n        txt = txt[istart+10:]\n    else:\n        istart = txt.lower().find(' majoring ')\n        if istart==-1:\n            return txtMajor\n        txt = txt[istart+12:]\n    \n    txt = txt.replace(',',' or ').replace(' and/or ',' or ').replace(' a closely related field',' related field').replace(' a ',' ').replace(' an ',' ')\n    sent = preprocess(txt)\n    pattern = \"\"\"\n            NP: {<DT>? <JJ>* <NN.*>*}\n           BR: {<W.*>|<V.*>} \n        \"\"\"\n    cp = nltk.RegexpParser(pattern)\n    cs = cp.parse(sent)\n    #print(cs)\n    checkNext = 0\n    for subtree in cs.subtrees():\n        if subtree.label()=='NP':\n            result.append(' '.join([w for w, t in subtree.leaves()]))\n            checkNext=1\n        elif checkNext==1 and subtree.label()=='BR':\n            break\n    return '|'.join(result)\n\ndef getApprenticeshipMajor(row):\n    \"\"\"Returns education majors from the text supplied in the DataFrame row\n    \n    Checks for apprenticeship in the text and will process only if Education major is blank.\n    \"\"\"\n    \n    txt = row['REQUIREMENT_TEXT']\n    txtMajor = row['EDUCATION_MAJOR']\n    if 'apprenticeship' not in txt:\n        return txtMajor\n    if txtMajor != '':\n        return txtMajor\n    result = []\n    \n    istart = txt.lower().find(' apprenticeship program')\n    if istart!=-1:\n        txt = txt[istart+23:]\n    else:\n        istart = txt.lower().find(' apprenticeship ')\n        if istart==-1:\n            return txtMajor\n        txt = txt[istart+15:]\n    \n    txt = txt.replace(',',' or ').replace(' full-time ',' ').replace(' a ',' ').replace(' an ',' ')\n    sent = preprocess(txt)\n    pattern = \"\"\"\n            NP: {<DT>? <JJ>* <NN.*>*}\n           BR: {<W.*>|<V.*>} \n        \"\"\"\n    cp = nltk.RegexpParser(pattern)\n    cs = cp.parse(sent)\n    #print(cs)\n    checkNext = 0\n    for subtree in cs.subtrees():\n        if subtree.label()=='NP':\n            result.append(' '.join([w for w, t in subtree.leaves()]))\n            checkNext=1\n        elif checkNext==1 and subtree.label()=='BR':\n            break\n    return '|'.join(result)\n\n\ndef getValues(searchText, COL_NAME, headingsFrame):\n    \"\"\"Search for possible values for searchText in headings from job class file, and returns the content related to the heading\n    \n    Parameters\n    ----------\n    searchText : str\n        text value to be searched for in possible Headings list\n    COL_NAME : str\n        name of the column to represent the content in the resulting dataframe\n    \n    Returns\n    -------\n    DataFrame\n        dataframe with two columns ['FILE_NAME',COL_NAME].\n        FILE_NAME - name of the job class file\n        COL_NAME - content for the heading searched (searchText)\n    \"\"\"\n    \n    data_list = []\n    dataHeadings = [k for k in headingsFrame['Heading'].values if searchText in k.lower()]\n\n    for filename in os.listdir(bulletin_dir):\n        with open(bulletin_dir + \"/\" + filename, 'r', errors='ignore') as f:\n            readNext = 0 \n            datatxt = ''\n            for line in f.readlines():\n                clean_line = line.replace(\"\\n\",\" \").replace(\"\\t\",\" \").replace(\":\",\"\").replace(\"  \",\" \").strip()\n                if readNext == 0:                         \n                    if clean_line in dataHeadings:\n                        readNext = 1\n                elif readNext == 1:\n                    if clean_line in headingsFrame['Heading'].values:\n                        break\n                    else:\n                         datatxt = datatxt + ' ' + clean_line\n            data_list.append([filename,datatxt.strip()])\n    result = pd.DataFrame(data_list)\n    result.columns = ['FILE_NAME',COL_NAME]\n    return result\n\n#Functions to find out text from a set of pre-defined list\n#e.g. To search for 'High School' or 'Apprenticeship' or any job class title\ndef section_value_extractor( document, section, subterms_dict, parsed_items_dict ):\n    retval = OrderedDict()\n    single_section_lines = document.lower()\n    \n    for node_tag, pattern_string in subterms_dict.items():\n        pattern_list = re.split(r\",|:\", pattern_string[0])#.sort(key=len)\n        pattern_list=sorted(pattern_list, key=len, reverse=True)\n        #print (pattern_list)\n        matches=[]\n        for pattern in pattern_list:\n            if pattern.lower() in single_section_lines:\n                matches.append(pattern)\n                single_section_lines = single_section_lines.replace(pattern.lower(),'')\n                #print (single_section_lines)\n        #matches = [pattern for pattern in pattern_list if pattern.lower() in single_section_lines.lower()]\n        #print (matches)\n        if len(matches):\n            info_string = \", \".join(list(matches)) + \" \"\n            retval[node_tag] = info_string\n    return retval\n\ndef read_config( configfile ):\n\n    #tree = ET.parse(configfile) #Use this if configfile is path for xml\n    tree = ET.fromstring(configfile)\n    #root = tree.getroot()\n    root = tree\n    config = []\n    for child in root:\n        term = OrderedDict()\n        term[\"Term\"] = child.get('name', \"\")\n        for level1 in child:\n            term[\"Method\"] = level1.get('name', \"\")\n            term[\"Section\"] = level1.get('section', \"\")\n            for level2 in level1:\n                term[level2.tag] = term.get(level2.tag, []) + [level2.text]\n\n        config.append(term)\n    json_result = json.dumps(config, indent=4)\n    #print(\"Specifications:\\n {}\".format(json_result))\n    return config\n\ndef parse_document(document, config):\n    parsed_items_dict = OrderedDict()\n\n    for term in config:\n        term_name = term.get('Term')\n        extraction_method = term.get('Method')\n        extraction_method_ref = globals()[extraction_method]\n        section = term.get(\"Section\")\n        subterms_dict = OrderedDict()\n        \n        for node_tag, pattern_list in list(term.items())[3:]:\n            subterms_dict[node_tag] = pattern_list\n        parsed_items_dict[term_name] = extraction_method_ref(document, section, subterms_dict, parsed_items_dict)\n\n    return parsed_items_dict\n\ndef addColumnsFromConfig(df_requirements):\n    print('Adding columns from config.')\n    config = read_config(configfile.replace('&','&#38;').replace('\\'','&#39;'))\n    result = df_requirements['REQUIREMENT_TEXT'].apply(lambda k: parse_document(k,config))\n\n    i=0\n    df_requirements['EXP_JOB_CLASS_TITLE']=''\n    df_requirements['SCHOOL_TYPE']=''\n    for item in (result.values):\n        for requirement,dic in list(item.items()):        \n            if 'JobTitle' in dic.keys():\n                df_requirements.loc[i,'EXP_JOB_CLASS_TITLE'] = dic['JobTitle']\n            if 'SchoolType' in dic.keys():\n                df_requirements.loc[i,'SCHOOL_TYPE'] = dic['SchoolType']\n        i=i+1\n    return df_requirements","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Functions (Logic & Explanations) continue...**\n(Used spaCy pattern matchers in most of the cases)\n- getEducationYears: to parse for EDUCATION_YEARS\n    - search for part of speech tag NUM (i.e. education year count)\n- getExperienceType: to parse for FULL_TIME_PART_TIME, PAID_VOLUNTEER\n    - search for text like full time/ part time\n- getJobFunction: to parse for EXP_JOB_CLASS_ALT_RESP, EXP_JOB_CLASS_FUNCTION\n    - annotations are used (i.e. K1, K1b)\n    - EXP_JOB_CLASS_TITLE is replaced with K1 and then searched for K1b & K2\n    - if part of speech tags are matched with the likes of K1 class functions then tags after VERB is used to represent K1b else K2"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#Functions used for finding education & experience details for a job class\n\ndef getEducationYears(row):\n    txt = row['REQUIREMENT_TEXT'].lower()\n    txtSchoolType = row['SCHOOL_TYPE'].strip().lower()\n    \n    if txtSchoolType == '':\n        return ''\n    \n    txt = txt.replace(txtSchoolType,txtSchoolType.replace(' ','_'))\n    txtSchoolType = txtSchoolType.replace(' ','_')\n\n    doc = nlp(txt)\n    matchResult = ''\n    pattern = [{'POS': 'NUM'},\n               {'TEXT': {'IN':['-','year','years','month','months','of']},'OP': '*'},\n               {'TEXT': txtSchoolType}\n               ]\n    matcher = Matcher(nlp.vocab)\n\n    matcher.add(\"EducationYears\", None, pattern)\n    \n    matches = matcher(doc)\n    for match_id, start, end in matches:\n        string_id = nlp.vocab.strings[match_id]\n        span = doc[start:end]\n        matchResult = span.text\n        break\n    if matchResult!='':\n        matchResult = w2n.word_to_num(matchResult.split()[0])\n    \n    return matchResult\n\ndef getRequirementTextWithK1(row):\n    rqmntTxt = row['REQUIREMENT_TEXT']\n    jobClass = row['EXP_JOB_CLASS_TITLE']\n    rqmntTxt = rqmntTxt.replace('fulltime','full-time').replace('.','. ').replace('#','').replace('  ',' ')\n    if jobClass == '':\n        return rqmntTxt\n    else:\n        jobClass = jobClass.split(',')\n        for job in jobClass:\n            job = job.strip()\n            rqmntTxt = rqmntTxt.lower().replace(job.lower(),'K1 ')\n    return rqmntTxt.replace('  ',' ')\n\ndef getExperienceText(row):\n    txt = row['REQUIREMENT_TEXT_WITH_K1'].lower()\n    matcher = Matcher(nlp.vocab)\n    doc = nlp(txt)\n\n    pattern2 = [{'POS': 'NUM'},\n               {'POS': 'ADJ', 'OP': '?'},\n               {'IS_PUNCT': True, 'OP': '?'},\n               {'POS': 'NUM', 'OP': '?'},\n               {'IS_PUNCT': True, 'OP': '?'},\n               {'POS': 'NOUN'},\n               {'IS_STOP': True, 'OP': '?'},\n               {'LEMMA': {'IN': ['full','part']}, 'OP': '?'},\n               {'IS_PUNCT': True, 'OP': '?'},\n               {'LEMMA': 'time', 'OP': '?'},\n               {'IS_PUNCT': True, 'OP': '?'},\n               {'POS': 'NUM', 'OP': '*'},\n               {'LEMMA': 'pay', 'OP': '?'},\n               {'IS_STOP': True, 'OP': '?'},\n               {'LEMMA': 'volunteer', 'OP': '?'},\n               {'IS_PUNCT': True, 'OP': '?'},\n               {'POS': {'IN': ['NOUN','ADJ','VERB']}, 'OP': '*'},\n               {'ORTH': 'k1', 'OP': '?'},\n               {'ORTH': 'experience'}]\n    \n    pattern3 = [{'LEMMA': {'IN': ['full','part']}},\n               {'IS_PUNCT': True, 'OP': '?'},\n               {'LEMMA': 'time', 'OP': '?'},\n               {'IS_PUNCT': True, 'OP': '?'},\n               {'LEMMA': 'pay', 'OP': '?'},\n               {'IS_STOP': True, 'OP': '?'},\n               {'LEMMA': 'volunteer', 'OP': '?'},\n               {'IS_PUNCT': True, 'OP': '?'},\n               {'POS': {'IN': ['NOUN','ADJ','VERB']}, 'OP': '*'},\n               {'ORTH': 'k1', 'OP': '?'},\n               {'ORTH': 'experience'}]\n    \n    #matcher.add(\"Experience1\", None, pattern1)\n    matcher.add(\"Experience2\", None, pattern2)\n    matcher.add(\"Experience3\", None, pattern3)\n    \n    matches = matcher(doc)\n    i=0\n    result = ''\n    docs = {}\n    for match_id, start, end in matches:\n        string_id = nlp.vocab.strings[match_id]\n        span = doc[start:end] \n        if string_id not in docs.keys():\n            docs[string_id] = span.text\n        if ')' in span.text and i==0:\n            i=1\n            del docs[string_id]\n\n    if 'Experience2' in docs.keys():\n        result = docs['Experience2']\n    elif 'Experience3' in docs.keys():\n        result = docs['Experience3']\n        \n    return result\n\ndef getTextWithoutExperienceText(row):\n    rqmntTxt = row['REQUIREMENT_TEXT_WITH_K1'].lower()\n    expTxt = row['EXPERIENCE_TEXT'].lower()\n    if expTxt == '':\n        return rqmntTxt\n    istart = rqmntTxt.find(expTxt)\n    istart = istart + len(expTxt)\n    return rqmntTxt[istart:]\n\ndef getJobClassFunction(row, txt):\n    doc = nlp(txt)\n    verbTxt = ''\n    sentence = ''\n    for sent in doc.sents:\n        sentence = sent.text\n        for token in sent:\n            if token.pos_ == 'VERB' and not token.is_stop:\n                verbTxt = token.text\n                break\n        break\n    if verbTxt == '':\n        return ''\n    istart = sentence.find(verbTxt)\n    return sentence[istart:]\n\ndef getJobFunction(row, col):\n    if row['EXPERIENCE_TEXT'] == '':\n        return ''\n    txt = row['REQUIREMENT_TEXT_WITH_K1'].lower()\n    doc = nlp(txt)\n    pattern = [{'ORTH': 'k1'},\n               {'IS_STOP': True, 'OP': '*'},\n               {'IS_PUNCT': True, 'OP': '*'},\n               {'POS': 'NOUN', 'OP': '*'},\n               {'ORTH': 'or'},\n               {'IS_STOP': True, 'OP': '*'},\n               {'ORTH': 'class', 'OP': '?'},\n               {'IS_STOP': True, 'OP': '*'},\n               {'ORTH': 'level'}]\n\n    matcher = Matcher(nlp.vocab)\n\n    matcher.add(\"K1Class\", None, pattern)\n\n    matches = matcher(doc)\n    result = ''\n    \n    for match_id, start, end in matches:\n        string_id = nlp.vocab.strings[match_id]\n        span = doc[start:end] \n        result = span.text\n    if result != '':\n        if col == 'K1b':\n            istart = txt.find(result)\n            istart = istart + len(result)\n            result = txt[istart:]\n            return getJobClassFunction(row, result) #K1b\n        else:\n            return ''\n    elif col=='K2':\n        txt = row['REQUIREMENT_WITHOUT_EXPERIENCE_TEXT'].lower()\n        return getJobClassFunction(row, txt) #K2\n    else:\n        return ''\n    \ndef getExperienceLength(row):\n    expTxt = row['EXPERIENCE_TEXT'].lower()\n    if expTxt == '':\n        return expTxt,expTxt\n    matcher = Matcher(nlp.vocab)\n    doc = nlp(expTxt)\n\n    patExpLength = [{'POS': 'NUM'},\n               {'POS': 'ADJ', 'OP': '?'},\n               {'IS_PUNCT': True, 'OP': '?'},\n               {'POS': 'NUM', 'OP': '?'},\n               {'IS_PUNCT': True, 'OP': '?'},\n               {'POS': 'NOUN'},\n               ]\n    \n    matcher.add(\"ExperienceLength\", None, patExpLength)\n    \n    matches = matcher(doc)\n    matchResult=''\n    for match_id, start, end in matches:\n        string_id = nlp.vocab.strings[match_id]\n        span = doc[start:end]\n        matchResult = span.text\n        break\n    doc = nlp(matchResult)\n    expLength = ''\n    expUnit = ''\n    for token in doc:\n        if token.pos_ == 'NUM':\n            print\n            expLength = w2n.word_to_num(token.text.replace(',',''))\n        if token.pos_ == 'NOUN':\n            expUnit = token.text\n    return expLength,expUnit\n\ndef getExperienceType(row):\n    expTxt = row['EXPERIENCE_TEXT'].lower().replace('fulltime','full time').replace('-',' ')\n    if expTxt == '':\n        return expTxt,expTxt\n    paid_volu = ''\n    full_part = ''\n    if 'paid' in expTxt and 'volunteer' in expTxt:\n        paid_volu = 'PAID|VOLUNTEER'\n    elif 'paid' in expTxt:\n        paid_volu = 'PAID'\n    elif 'volunteer' in expTxt:\n        paid_volu = 'VOLUNTEER'\n    \n    if 'full time' in expTxt and 'part time' in expTxt:\n        full_part = 'FULL_TIME|PART_TIME'\n    elif 'full time' in expTxt:\n        full_part = 'FULL_TIME'\n    elif 'part time' in expTxt:\n        full_part = 'PART_TIME'\n    \n    return full_part,paid_volu","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Functions (Logic & Explanations) continue...**\n(Used spaCy's pattern matcher for most of the text search\n- getCourseCount: to parse for COURSE_COUNT \n    - search for part of speech tag NUM followed by course lemma, in requirement sets\n- getCourseLength: to parse for COURSE_LENGTH\n    - search for a few patterns to find course length number, in requirement sets\n- getAdditionalLic: to parse for ADDTL_LIC\n    - search for pos tags like NOUN/ADJ/PROPN between words valid & required\n- getWhereToApplyUrl:\n    - regex search for a url in where_to_apply section\n- getSelection: to parse for SELECTION_CRITERIA\n    - search for text between weight/weights and % in selection_criteria section\n- getAppDeadlineDate: to parse for APPLICATION_DEADLINE\n    - used nltk named entities to search for DATE entity in application deadline section"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def getCourseCount(row):\n    txt = row['REQUIREMENT_TEXT']\n    doc = nlp(txt.lower())\n    matchResult = ''\n    pattern = [{'POS': 'NUM'},\n                {'LEMMA': 'course'}\n               ]\n    matcher = Matcher(nlp.vocab)\n\n    matcher.add(\"CourseCount\", None, pattern)\n    \n    matches = matcher(doc)\n    for match_id, start, end in matches:\n        string_id = nlp.vocab.strings[match_id]\n        span = doc[start:end]\n        matchResult = span.text\n        break\n    if matchResult!='':\n        matchResult = w2n.word_to_num(matchResult.split()[0])\n    \n    return matchResult\n\ndef getCourseLength(row):\n    txt = row['REQUIREMENT_TEXT']\n    txt = txt.replace(')',') ').replace('-',' ').replace('  ',' ')\n    doc = nlp(txt.lower())\n    matchResult = ''\n    pattern = [{'POS': 'NUM', 'OP': '+'},\n               {'ORTH': '(', 'OP': '?'},\n               {'POS': 'NUM', 'OP': '?'},\n               {'ORTH': ')', 'OP': '?'},\n               {'LEMMA': 'semester', 'OP': '?'},\n               {'LEMMA': 'unit', 'OP': '?'},\n               {'POS': 'CCONJ', 'OP': '?'},\n               {'POS': 'NUM', 'OP': '*'},\n               {'ORTH': '(', 'OP': '?'},\n               {'POS': 'NUM', 'OP': '?'},\n               {'ORTH': ')', 'OP': '?'},\n               {'LEMMA': {'IN':['quarter','semester']}},\n               ]\n    matcher = Matcher(nlp.vocab)\n\n    matcher.add(\"CourseLength\", None, pattern)\n    \n    matches = matcher(doc)\n    index = -1\n    for match_id, start, end in matches:\n        string_id = nlp.vocab.strings[match_id]\n        span = doc[start:end]\n        matchResult = span.text\n\n    if matchResult=='':\n        return matchResult,matchResult\n    \n    doc = nlp(matchResult)\n    numSem = ''\n    numQtr = ''\n    txtSem = ''\n    txtQtr = ''\n    isAfterConj = False\n    readNext = True\n    for token in doc:\n        if readNext:\n            if token.pos_ == 'CCONJ':\n                isAfterConj = True\n            elif token.text == '(':\n                readNext = False\n            elif not isAfterConj:\n                if token.pos_ == 'NUM':\n                    numSem = numSem + ' ' + token.text\n                elif token.pos_ == 'NOUN' and token.lemma_.lower() != 'unit':\n                    txtSem = token.text[0].upper()\n            elif isAfterConj:\n                if token.pos_ == 'NUM':\n                    numQtr = numQtr + ' ' + token.text\n                elif token.pos_ == 'NOUN' and token.lemma_.lower() != 'unit':\n                    txtQtr = token.text[0].upper()\n        else:\n            if token.text == ')':\n                readNext = True\n    \n    if numSem != '':\n        numSem = w2n.word_to_num(numSem.strip())\n        txtSem = str(numSem) + ' ' + txtSem\n        if numQtr == '':\n            return txtSem,matchResult\n    if numQtr != '':\n        numQtr = w2n.word_to_num(numQtr.strip())\n        txtQtr = txtSem + '|' + str(numQtr) + ' ' + txtQtr\n        return txtQtr,matchResult\n\ndef getCourseSubjects(row):\n    txt = row['REQUIREMENT_TEXT']\n    txt = txt.replace(')',') ').replace('and/or','or').replace('/',' or ').replace('-',' ').replace('  ',' ').lower()\n    courseLengthTxt = row['COURSE_LENGTH_TEXT']\n    if courseLengthTxt=='':\n        return ''\n    \n    txt = txt[txt.find(courseLengthTxt)+len(courseLengthTxt)+1:]\n    doc = nlp(txt)\n\n    for sent in doc.sents:\n        docSent = nlp(sent.text)\n        isAfterAnd = False\n        sentText = ''\n        for token in docSent:\n            if token.text == 'and':\n                isAfterAnd = True\n                continue\n            else:\n                if isAfterAnd:\n                    if token.pos_ == 'NOUN' or token.pos_ == 'VERB':\n                        sentText = sentText + ' & ' + token.text\n                    else:\n                        sentText = sentText + ' and ' + token.text\n                else:\n                    sentText = sentText + ' ' + token.text\n                isAfterAnd = False\n        break\n\n    doc = nlp(sentText)\n\n    pattern2 = [{'ORTH': {'IN':['in','on','of']}},\n                {'POS': {'IN':['NOUN','VERB','ADJ','PUNCT','CCONJ','DET']}, 'OP': '*'},\n                {'ORTH': {'IN': ['of','in']}, 'OP': '*'},\n                {'POS': {'IN':['NOUN','VERB']}},\n                ]\n    matcher = Matcher(nlp.vocab)\n\n    matcher.add(\"CourseSubject2\", None, pattern2)\n\n    matches = matcher(doc)\n    result = ''\n    index = -1\n    prevMatch = ''\n    for match_id, start, end in matches:\n        string_id = nlp.vocab.strings[match_id]\n        span = doc[start:end]\n        if index == -1:\n            result = span.text\n            index = sentText.find(span.text)\n        elif index!=sentText.find(span.text):\n            continue\n        else:\n            result = span.text\n    \n    result = result.replace('an accredited college or university','')\n    \n    if len(result.split(' ', 1))>1:\n        result = result.split(' ', 1)[1]\n\n    char_list = [', or ', ' or ', ',', ';']\n    return re.sub(\"|\".join(char_list), \"|\", result)\n\n#Functions for getting license details\ndef getLicenseRequired(row, fromCol):\n    if row['DRIVERS_LICENSE_REQ'] != '':\n        return row['DRIVERS_LICENSE_REQ'],row['DRIV_LIC_TYPE']\n    blank = ''\n    txt = row[fromCol]\n    txt = re.sub('\\d\\.','',txt)\n    doc = nlp(txt)\n    matchResult = ''\n    licReq = ''\n    patLicenseP = [{'LOWER': 'may'},\n                   {'LOWER': 'require'},\n                   {'LOWER': 'a', 'OP': '?'},\n                   {'LOWER': 'class', 'OP': '?'},\n                    {'POS': {'IN': ['PROPN','NOUN','ADJ','PUNCT']}, 'OP': '*'},\n                    {'POS': 'PART','OP': '?'},\n                    {'POS': 'CCONJ', 'OP': '?'},\n                    {'LOWER': 'class', 'OP': '?'},\n                    {'POS': {'IN': ['PROPN','NOUN','ADJ','PUNCT']}},\n                    {'POS': 'PART','OP': '?'},\n                    {'LEMMA': 'driver'},\n                    {'POS': 'PART','OP': '?'},\n                    {'LOWER': 'license'}\n                   ]\n    patLicenseR = [{'LOWER': 'class', 'OP': '?'},\n                    {'POS': {'IN': ['PROPN','NOUN']}, 'OP': '?'},\n                    {'POS': 'PART','OP': '?'},\n                    {'POS': 'CCONJ', 'OP': '?'},\n                    {'LOWER': 'class', 'OP': '?'},\n                    {'POS': {'IN': ['PROPN','NOUN','ADJ']}},\n                    {'POS': 'PART','OP': '?'},\n                    {'LEMMA': 'driver'},\n                    {'POS': 'PART','OP': '?'},\n                    {'LOWER': 'license'}\n                   ]\n    matcher = Matcher(nlp.vocab)\n\n    matcher.add(\"patLicenseP\", None, patLicenseP)\n    matcher.add(\"patLicenseR\", None, patLicenseR)\n\n    pattern = ''\n    for sent in doc.sents:\n        docSent = nlp(sent.text)\n        matches = matcher(docSent)\n        for match_id, start, end in matches:\n            pattern = nlp.vocab.strings[match_id]\n            span = docSent[start:end]\n            matchResult = span.text\n            break\n\n        if matchResult != '':\n            if pattern == 'patLicenseP':\n                licReq = 'P'\n            else:\n                licReq = 'R'\n            doc1 = nlp(matchResult)\n            #print(matchResult)\n            matchResult = [y.text for y in doc1 if (y.pos_ == 'NOUN' or y.pos_=='PROPN') and \n                           (y.text.lower() not in ['class','driver','license','california','commercial',\n                                                   'state','drivers','firefighter'])]\n            matchResult = '|'.join(matchResult)\n            if matchResult != '':\n                break\n    if matchResult != '':       \n        return licReq,matchResult\n    elif licReq != '':\n        return licReq,blank\n    else:\n        return blank,blank\n    \ndef getAdditionalLic(row, fromCol):\n    txt = row[fromCol]\n    doc = nlp(txt)\n    matcher = Matcher(nlp.vocab)\n    \n    pattern = [{'LOWER': 'valid'},\n               {'POS': {'IN': ['PROPN','NOUN','ADJ','VERB']},'OP':'*'},\n               {'IS_STOP': True, 'OP':'*'},\n               {'LOWER': 'required'}]\n    matcher.add(\"pattern\", None, pattern)\n    \n    matchResult=''\n    for sent in doc.sents:\n        sentTxt = sent.text\n        if 'license' in sentTxt.lower():\n            sentTxt = sentTxt[sentTxt.lower().find('license'):]\n        docSent = nlp(sentTxt)\n        matches = matcher(docSent)\n        for match_id, start, end in matches:\n            pattern = nlp.vocab.strings[match_id]\n            span = docSent[start:end]\n            matchResult = span.text\n            break\n    return matchResult\n\ndef getExamTypeContent(row):\n    content = row['Content']\n    txtToMatch = 'THIS EXAMINATION IS TO BE GIVEN'\n    if txtToMatch not in content:\n        txtToMatch = 'THIS EXAM IS TO BE GIVEN'\n        if txtToMatch not in content:\n            return ''\n        else:\n            istart = content.find(txtToMatch)+25\n    else:\n        istart = content.find(txtToMatch)+32\n    content = content[istart:]\n    content = content.split('\\n')\n    content = list(filter(None, content))\n    content = ','.join(content[:2])\n    return content\n\ndef setExamTypeContent(row):\n    content = row['ExamTypeContent'].lower()\n    \n    if ('open' in content) and ('interdepartmental' in content):\n        return 'OPEN_INT_PROM'\n    elif 'open' in content:\n        return 'OPEN'\n    elif ('interdepartmental' in content) or ('interdeparmental' in content):\n        return 'INT_DEPT_PROM'\n    elif 'departmental' in content:\n        return 'DEPT_PROM'\n    else:\n        return ''\n\ndef getAppDeadlineDate(row):\n    \"\"\"Finds date pattern in application deadline text and returns date\n    \n    Parameters\n    ----------\n    row\n        row of the dataframe to be searched\n    \n    Returns\n    -------\n    date\n        1900-01-01 in case application will close without prior notice\n        1901-01-01 in case no application deadline found in text\n        1902-01-01 in case date found is not in correct format(i.e. %B %d %Y)\n        Actual date in %Y-%m%d format in case a date is found and is in correct format\n    \"\"\"\n    \n    txt = row['APPLICATION_DEADLINE_TEXT']\n    doc = nlp(txt)\n    days = ['Sunday','Monday','Tuesday','Wednesday','Thursday','Friday','Saturday']\n    \n    for sent in doc.sents:\n        txtSent = sent.text.title().replace('Am','a.m.').replace('Pm','p.m.').replace('A.M.','a.m.').replace('P.M.','p.m.').replace('Septemeber','September')\n        txtSent = re.sub(\"|\".join(days), \" \", txtSent)\n        if 'without prior notice' in txtSent.lower():\n            return pd.to_datetime('January 1, 1900', format=\"%B %d, %Y\")\n        docSent = nlp(txtSent)\n        for ent in docSent.ents:\n            if ent.label_ == \"DATE\":\n                try:\n                    return pd.to_datetime(ent.text.replace(',','').replace('  ',' '), format=\"%B %d %Y\")\n                except:\n                    try:\n                        toIndex = ent.text.find('To')\n                        return pd.to_datetime(ent.text[:toIndex-1], format=\"%B %d, %Y\")\n                    except:\n                        #print(ent.text)\n                        return pd.to_datetime('January 1, 1902', format=\"%B %d, %Y\")\n        \n    return pd.to_datetime('January 1, 1901', format=\"%B %d, %Y\")\n\ndef getSelection(txt):\n    matcher = Matcher(nlp.vocab)\n    doc = nlp(txt)\n    pattern = [{'LOWER': {'IN': ['weight','weights']}},\n               {\"TEXT\": {\"REGEX\": \".*\"},'OP':'*'},\n#                {'POS': 'NUM'},\n               {'TEXT': '%'}]\n\n    matcher.add(\"Selection\", None, pattern)\n\n    matches = matcher(doc)\n\n    spanTxt = ''\n    for match_id, start, end in matches:\n        string_id = nlp.vocab.strings[match_id]\n        span = doc[start:end]\n        spanTxt = span.text\n    if spanTxt=='':\n        return ''\n    txt = spanTxt.lower().replace('weights','').replace('weight','')\n    txt = txt.replace('. .','|').split('|')\n    txt = [k.replace('.','').strip() for k in txt if ('%' not in k.strip()) & (k.strip()!='')]\n\n    return '|'.join(txt) \n\ndef getWhereToApplyUrl(txt):\n    url = re.findall('http[s]?:?//(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', txt)\n    return url","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Generating DataFrames**"},{"metadata":{"trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"job_class_df, data_dictionary = createDataFrameAndDictionary(jobs_df, job_titles, data_dictionary)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Creating CSV**"},{"metadata":{"trusted":true},"cell_type":"code","source":"job_class_df.to_csv('job_class_submission.csv', index=False)\ndata_dictionary.to_csv('data_dictionary_submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='4'></a>\n<h1><b><u>4. Exploratory Data Analysis</u></b></h1>\n\n<h3>What matters most at City of LA</h3>\n**Competencies** are the knowledge, skill, ability, aptitude, capability, or other personal characteristics needed to perform the job.\n<br>Competencies for job classes are identified that best separate superior from satisfactory job performance in that class.\n<br><br>Data(pdfs) is collected from [here](http://personnel.lacity.org/index.cfm?content=jobanalyses) and then converted to csv and is available publicly [here](https://www.kaggle.com/tyagit3/job-title-wise-competency-list-cityofla)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"competency_df = pd.read_csv('../input/job-title-wise-competency-list-cityofla/df_competency.csv')\n\nprint('There are '+str(len(competency_df['job_class'].unique()))+' job classes for which competency data is collected and used in this analysis.')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"competency_df[competency_df['job_class']=='Accounting Clerk Competencies.pdf'][['job_class','SubCompetency','Competency']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Competencies here are divided into 7 major categories\n- **Personal**: Capability for effective performance\n- **Occupational:** Capability for effective performance in a specific job(s)\n- **Interpersonal:** Capability to interact effectively with others\n- **Communication:** Capability to exchange information effectively\n- **Organizational:** Capability to promote effectiveness of the organization\n- **External:** Capability to promote effective performance externally\n- **Future:** Capability to promote effective performance in the future\n\nThese competencies are then divided, which are labelled as 'SubCompetency' in the above dataframe."},{"metadata":{},"cell_type":"markdown","source":"**Check for competencies distribution over job classes**"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def group_lower_ranking_values(pie_raw, column):\n    \"\"\"Converts pie_raw dataframe with multiple categories to a dataframe with fewer categories\n    \n    Calculate the 75th quantile and group the lesser values together.\n    Lesser values will be labelled as 'Other'\n    \n    Parameters\n    ----------\n    pie_raw : DataFrame\n        dataframe with the data to be aggregated\n    column : str\n        name of the column based on which dataframe values will be aggregated\n    \"\"\"\n    pie_counts = pie_raw.groupby(column).agg('count')\n    pct_value = pie_counts[lambda df: df.columns[0]].quantile(.75)\n    values_below_pct_value = pie_counts[lambda df: df.columns[0]].loc[lambda s: s < pct_value].index.values\n    def fix_values(row):\n        if row[column] in values_below_pct_value:\n            row[column] = 'Other'\n        return row \n    pie_grouped = pie_raw.apply(fix_values, axis=1).groupby(column).agg('count')\n    return pie_grouped\n\nsubcompetency_df = group_lower_ranking_values(competency_df, 'SubCompetency')\n\nplt.figure(1, figsize=(10,10))\nplt.pie(subcompetency_df['Id'],labels=subcompetency_df.index, autopct='%1.1f%%', startangle=90)\nplt.axis('equal')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3><b>Interpretations:</b></h3>\n\nAs per above graph, **personal, organisational, external** traits are evenly distributed as performance measures for job classes.\n<br><b>Other</b> represents the values less than 75th quantile, grouped together.\n<br>Competency should be the sum total of skills, knowledge and attitudes, manifested in the employee's behaviour.\n\nEmployees who have the right attitude that translates to the best behaviour are said to be the more competent. <br>Competency is still equated or defined as skills, ability to perform, capacity, and knowledge. As such, the term has been used loosely. Competency takes more than skills and knowledge. It requires the right and appropriate attitude that eventually translates to behaviour which is correctly taken into consideration here.\n\n\n<a id='4.1'></a>\n<h3><b>4.1 Recommendation 1:</b></h3>\n\n- Add a new **Cultural Competency** in the list of **Competency Bank**.\n- Cultural Competency is linked with the diversity by encouraging the acknowledgement and acceptance of differences in behaviour, appearances and culture as a whole.\n\n<br><br><br><br>\n**Missing values** in structured output data."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(1,(13,8))\nplt.title('Missing Values in job_class file')\nsns.heatmap(job_class_df.replace(r'^\\s*$', np.nan, regex=True).isnull(), cbar=False)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def add_datepart(df, fldname, drop=True):\n    fld = df[fldname]\n    if not np.issubdtype(fld.dtype, np.datetime64):\n        df[fldname] = fld = pd.to_datetime(fld, infer_datetime_format=True)\n    targ_pre = re.sub('[Dd]ate$', '', fldname)\n    for n in ('Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear','Quarter','Weekday_name',\n            #'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start'\n             ):\n        df[targ_pre+n] = getattr(fld.dt,n.lower())\n    df[targ_pre+'Elapsed'] = fld.astype(np.int64) // 10**9\n    if drop: df.drop(fldname, axis=1, inplace=True)\n\ndef showHistogram(data, title):\n    \"\"\"Display Histogram\n    \n    Parameters\n    ----------\n    data (Series)\n        \n    title\n        title to dispay for the plot    \n    \"\"\"\n    plt.figure(1,(6,6))\n    data = data.value_counts(sort=False)\n    sns.barplot(y=data.values,x=data.index)\n    plt.gca().set_xticklabels(data.index,rotation='45')\n    plt.title(title)\n    plt.show()\n\njob_class_df.fillna('', inplace=True)\n\n#Adding some features using OPEN_DATE & APPLICATION_DEADLINE\n\njob_class_df['APPLICATION_DEADLINE'] = pd.to_datetime(job_class_df['APPLICATION_DEADLINE'], format=\"%Y-%m-%d\")\njob_class_df['OPEN_DATE'].fillna('',inplace=True)\njob_class_df['OPEN_DATE'] = pd.to_datetime(job_class_df['OPEN_DATE'], format=\"%Y-%m-%d\")\njob_class_df['no_of_days_to_apply'] = job_class_df.apply(lambda x: \n                                                         (x['APPLICATION_DEADLINE'] - x['OPEN_DATE']).days \n                                                         if (x['APPLICATION_DEADLINE'] - x['OPEN_DATE']).days >= 0 and \n                                                         x['APPLICATION_DEADLINE'].year>1903\n                                                         else 0, axis=1)\n\nopen_date_df=pd.DataFrame(job_class_df.groupby(['FILE_NAME','OPEN_DATE']).size().reset_index(name='Freq'))\n\n#Adding Date parts for OPEN_DATE\nadd_datepart(open_date_df, 'OPEN_DATE')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Graphs showing opening dates distributions over job classes.</b>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = plt.figure(4,(15,15))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\n\ndata = open_date_df['OPEN_DATEWeekday_name'].value_counts(sort=False)\nfig.add_subplot(2,2,1)\nsns.barplot(y=data.values,x=data.index)\nplt.title('Day wise job post openings')\n\ndata = open_date_df['OPEN_DATEMonth'].value_counts(sort=False)\nfig.add_subplot(2,2,2)\nsns.barplot(y=data.values,x=data.index)\nplt.title('Month wise job post openings')\n\ndata = open_date_df['OPEN_DATEQuarter'].value_counts(sort=False)\nfig.add_subplot(2,2,3)\nsns.barplot(y=data.values,x=data.index)\nplt.title('Quarter wise job post openings')\n\ndata = open_date_df['OPEN_DATEYear'].value_counts(sort=False)\nfig.add_subplot(2,2,4)\nsns.barplot(y=data.values,x=data.index)\nplt.title('Year wise job post openings')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3><b>Interpretations</b></h3>\n\nAs per above graphs it can be inferred that number of **vacancies are increasing per year** by a considerable number. Narrowing it to quarter wise job openings in an year, **last quarter of calander year has slightly more openings** in comparision to other quarters. After drilling down, month of March, October and December wins over others to create more vacancies for the applicants. Although there is no major gap in number of job postings among the other months.\nWhat was eye catching was that **almost all job openings are outset on a Friday**. \n\n<br>[An analysis on LinkedIn data.](https://business.linkedin.com/talent-solutions/blog/trends-and-research/2016/the-best-times-to-post-a-job-on-linkedin)\n<img src=\"https://i.imgur.com/Z8ALF3d.png\" width='800' height='700'>\n\n<a id='4.2'></a>\n<h3><b>4.2 Recommendation 2:</b></h3>\n\nTo figure out what are the popular days when candidate views and applies for a job, LinkedIn looked at the jobs posted by the top 15 European and US staffing agencies, based on market capitalization. **Monday is the most popular day to view and apply to jobs**. This may be because employees become sullen upon returning to the office after their weekend, Tuesday and Wednesday also over-index in terms of job views with a progressive drop off from Thursday through to Saturday.\nThis pattern suggests that the day you post your job matters. However, rest assured you will not need to start working on Saturdays to optimise your job posts.\n\nHence it is recommended to **post your job sunday evenings or monday mornings to prevent job post going down the list as old post**."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"app_deadline_df=pd.DataFrame(job_class_df.groupby(['FILE_NAME','APPLICATION_DEADLINE']).size().reset_index(name='Freq'))\nno_of_days_to_apply_df=pd.DataFrame(job_class_df.groupby(['FILE_NAME','no_of_days_to_apply']).size().reset_index(name='Freq'))\nno_of_days_to_apply_df = no_of_days_to_apply_df[no_of_days_to_apply_df['no_of_days_to_apply']<50] ##removing outliers to get a better view\n\n#Adding Date parts for APPLICATION_DEADLINE\nadd_datepart(app_deadline_df, 'APPLICATION_DEADLINE')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = plt.figure(2,(18,8))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\n\ndata = app_deadline_df['APPLICATION_DEADLINEWeekday_name'].value_counts(sort=False)\nfig.add_subplot(1,2,1)\nsns.barplot(y=data.values,x=data.index)\nplt.title('Day wise job application deadlines')\n\ndata = no_of_days_to_apply_df['no_of_days_to_apply'].value_counts(sort=False)\nfig.add_subplot(1,2,2)\nsns.barplot(y=data.values,x=data.index)\nplt.title('No of days to apply')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3><b>Interpretations</b></h3>\n\nSeeping into deadlines for job applications, it is derived via above graph that **fourth working day of the week ie; Thursday is commonly when the job posting is withdrawn**. No other day of the week has such large number of application deadline.\nMoreover, talking about duration of job openings it seems **window of 13 days is provided to candidates to apply** for the jobs.\n\n<a id='4.3'></a>\n<h3><b>4.3 Recommendation 3:</b></h3>\n\nAccording to a study there is a pattern with the way applications come in. When a job is first posted, there’s an initial rush of applications. These are in large part the people who are applying to everything they see for which they’re remotely qualified (and sometimes not even that) — **the resume-bombers**.  That tends to be true for roughly the **first week**. Conversely, of the **applications that come in toward the end of an application period, a much higher percentage of them are candidates who are very strongly matched** with the job qualifications. The reason for this may be because these are people who aren’t applying wildly every day — they’re being much more judicious in what they apply to, and they’re probably not looking as frequently because of that.\n<br>**Disclaimer!:** This is absolutely not to say that there aren’t strong candidates in the first week, or that everyone who applies toward the end is a strong match. These are just overall trends.\n\nAlso about **43 percent of job openings are filled within the first 30 days**, according to a new report from Indeed and the Centre for Economic and Business Research (CEBR).\nIt is possible if you’re closing jobs after one or two weeks, you’re probably missing out on some really strong candidates who aren’t checking ads as frequently so it is recommended that **minimum of three weeks, and four is better to close down the applications**.\n\n<br><br><br><br>\n**Let's analyse bulletins' content**"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def split_into_sentences(text):\n    alphabets= \"([A-Za-z])\"\n    prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n    suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n    starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n    acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n    websites = \"[.](com|net|org|io|gov)\"\n\n    text = \" \" + text + \"  \"\n    text = text.replace(\"\\n\",\" \")\n    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n    text = re.sub(websites,\"<prd>\\\\1\",text)\n    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n    text = text.replace(\".\",\".<stop>\")\n    text = text.replace(\"?\",\"?<stop>\")\n    text = text.replace(\"!\",\"!<stop>\")\n    text = text.replace(\"<prd>\",\".\")\n    sentences = text.split(\"<stop>\")\n    sentences = sentences[:-1]\n    sentences = [s.strip() for s in sentences]\n    return sentences\n\n# Returns Number of Words in the text \ndef word_count(text):\n    words = 0\n    sentences = split_into_sentences(text)\n    for sentence in sentences:\n        wordList = re.sub(\"[^\\w]\", \" \",  sentence).split()\n        words += len(wordList)\n    return words \n  \n# Returns the number of sentences in the text \ndef sentence_count(text):  \n    sentences = split_into_sentences(text)\n    return len(sentences)\n\ndef syllables_in_text(text):\n    sentences = split_into_sentences(text)\n    syllableCount = 0\n    for sentence in sentences:\n        wordList = re.sub(\"[^\\w]\", \" \",  sentence).split()\n        #syllables += sum([syllables_in_word(word.strip(string.punctuation)) for word in wordList])\n        syllableCount += sum([syllables.estimate(word) for word in wordList]) \n    return syllableCount\n\ndef avg_sentence_length(text): \n    words = word_count(text) \n    sentences = sentence_count(text) \n    average_sentence_length = float(words / sentences) \n    return average_sentence_length \n\ndef syllables_count(word):\n    #print(textstatistics().syllable_count(word))\n    return syllables_in_text(word)\n\n# Returns the average number of syllables per \n# word in the text \ndef avg_syllables_per_word(text): \n    syllable = syllables_count(text) \n    words = word_count(text)\n    ASPW = float(syllable) / float(words) \n    return legacy_round(ASPW, 1)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"job_class_df['title_len'] = job_class_df['JOB_CLASS_TITLE'].apply(lambda x: len(x))\njobs_df['content_word_len'] = jobs_df['Content'].apply(lambda x: word_count(x))\ndf_requirements = createRequirementsFrame(headingsFrame=createHeadingsFrame())\ndf_requirements['REQUIREMENT_TEXT_LEN'] = df_requirements['REQUIREMENT_TEXT'].apply(lambda x: len(x))\n\njob_title_df=pd.DataFrame(job_class_df.groupby(['FILE_NAME','title_len']).size().reset_index(name='Freq'))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = plt.figure(2,(15,15))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\n\nfig.add_subplot(3,1,1)\nsns.boxplot(x=job_title_df['title_len'])\nplt.title('Job Title Length(in characters)')\n\nfig.add_subplot(3,1,2)\nsns.boxplot(x=jobs_df['content_word_len'])\nplt.title('Job Class Content Length(in words)')\n\nfig.add_subplot(3,1,3)\nsns.boxplot(x=df_requirements['REQUIREMENT_TEXT_LEN'])\nplt.title('Requirement Text Length(in characters, set wise)')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3><b>Interpretations</b></h3>\n\n- <b>Job Title</b>\n    <br>The job title length in maximum job postings have median value of around **less than 25 characters** which is a plus point. According to studies, ideal length of job title should be less than 60 characters. Shorter job ad titles led to higher apply rates. [Recruitment Media Benchmark Report](https://info.appcast.io/2019-recruitment-media-benchmark-report-appcast)\n\n- <b>Job Content</b>\n    <br>The content of job description have median value of around **1400 words**. \n\n- <b>Requirement Text</b>\n    <br>The length of text specifying the requirement for a job comes out to be around **200 characters per set(i.e per REQUIREMENT_SET_ID)**.\n\n<a id='4.4'></a> \n<h3><b>4.4 Recommendation 4:</b></h3>\n\nAccording to studies, **job content should be 4,000-5000 characters** long. This is equal to around 500-600 words. \n<br>Scannable: Eye-tracking tests show that readers skim the job description portion of the posting. \n<br><br>Chunk the content to make it faster to scan or jump to the most relevant information for the reader. Appcast reports that **job descriptions with 300 to 800 words receive the highest apply rate**, according to its [Recruitment Media Benchmark Report](https://info.appcast.io/2019-recruitment-media-benchmark-report-appcast)\n\n\nTo further analyse the content of the bulletins' readability, a **readability score** is calculated for the bulletins.\nThanks for the share of a [kernel](https://www.kaggle.com/silverfoxdss/city-of-la-readability-and-promotion-nudges) highlighting this concept.\n\n<br><br><br><br>\n<h3>Readability Score</h3>\n\nAfter studying above data, we tried to deep-dive more into the text readaility factor.\nReadability is the ease with which a reader can understand a written text. **Readability takes into account length of words and sentences, number of syllables, use of pronouns and much more..** It focuses on the words we choose, and how we put them into sentences and paragraphs for the readers to comprehend. In readers with average or poor reading comprehension, raising the readability level of a text from mediocre to good can make the difference between success and failure of its communication goals.\n\nWriting readable copy means writing in Plain English as HR Tech Analyst Matt Charney wrote:\n\"No one makes an emotional connection with company acronyms, industry keywords and bulleted lists. Formal language like “ideal candidate\" or \"top performer\" can often hinder time to fill, time that could often be saved by simply addressing the person straight on, without all the fluff or purple prose.\"\n\nAdvantages of Readability Formulae:\n\n1. Readability formulas measure the grade-level readers must have to be to read a given text. Thus provides the writer of the text with much needed information to reach his target audience.\n2. Know Before hand if the target audience can understand your content.\n\n\nDisadvantages of Readability Formulae:\n\n1. Due to many readability formulas, there is an increasing chance of getting wide variations in results of a same text.\n2. Doesn't tell how and where to correct the text to improve readability.\n3. The two variables used in most formulas, a sentence and a vocabulary, are the ones most directly related to reading difficulty, but they are not the only ones.\n\nI've tried to use one of the different available methods to calculate and demonstrate the readability index in the Job bulletins.<br>\n<br>**Flesch reading ease**\n<br>In the Flesch reading-ease test, higher scores indicate material that is easier to read; lower numbers mark passages that are more difficult to read. \n<br>The formula for the Flesch reading-ease score (FRES) test is\n> **Reading Ease score = 206.835 - (1.015 × ASL) - (84.6 × ASW)**\n<br>ASL = average sentence length (number of words divided by number of sentences)\n<br>ASW = average word length in syllables (number of syllables divided by number of words)\n\nThere are other Readabilty Score Determination Methods/Formaulae that can be referred to [here](https://www.geeksforgeeks.org/readability-index-pythonnlp/)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#source : https://en.wikipedia.org/wiki/Flesch%E2%80%93Kincaid_readability_tests\ndef getGradeLevelText(flesch_reading_ease):\n    resultText = ''\n    if 90.00 < flesch_reading_ease <= 100.00:\n        resultText = '5th grade'\n    elif 80.0 < flesch_reading_ease <= 90.00:\n        resultText = '6th grade'\n    elif 70.0 < flesch_reading_ease <= 80.00:\n        resultText = '7th grade'\n    elif 60.0 < flesch_reading_ease <= 70.00:\n        resultText = '8th & 9th grade'\n    elif 50.0 < flesch_reading_ease <= 60.00:\n        resultText = '10th to 12th grade'\n    elif 30.0 < flesch_reading_ease <= 50.00:\n        resultText = 'College'\n    elif 0.0 < flesch_reading_ease <= 30.00:\n        resultText = 'College graduate'\n    else:\n        resultText = 'College graduate'\n    return resultText\n    \ndef flesch_reading_ease_grade(text): \n    \"\"\" \n        Implements Flesch Formula: \n        Reading Ease score = 206.835 - (1.015 × ASL) - (84.6 × ASW) \n        Here, \n          ASL = average sentence length (number of words  \n                divided by number of sentences) \n          ASW = average word length in syllables (number of syllables  \n                divided by number of words) \n    \"\"\"\n    ASL = avg_sentence_length(text)\n    ASW = avg_syllables_per_word(text)\n    FRE = 206.835 - float(1.015 * ASL) - float(84.6 * ASW) \n    FGL = (0.39 * ASL) + (11.8 * ASW) - 15.59\n    gradeText = getGradeLevelText(legacy_round(FRE, 2))\n    return legacy_round(FRE, 2), legacy_round(FGL, 2), gradeText","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"jobs_df['flesch_reading_ease_score'],jobs_df['flesch_grade_level'], jobs_df['grade_text'] = zip(*jobs_df['Content'].apply(flesch_reading_ease_grade))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(1,(10,3))\n\nsns.boxplot(x=jobs_df['flesch_reading_ease_score'])\nplt.title('Readability Score for Bulletins')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"jobs_grade_df = jobs_df['grade_text'].value_counts()\njobs_grade_df = pd.DataFrame({'code' : jobs_grade_df.index, 'values' : jobs_grade_df.values})\n\n#explode = (0.1, 0, 0, 0, 0.1)\nplt.figure(1,(8,8))\nplt.pie(jobs_grade_df['values'],labels=jobs_grade_df['code'],#explode=explode, \n        autopct='%1.1f%%', startangle=90)\nplt.axis('equal')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Legend explanations**\n- **5th grade**: Very easy to read. Easily understood by an average 11-year-old student.\n- **6th grade**: Easy to read. Conversational English for consumers.\n- **7th grade**: Fairly easy to read.\n- **8th & 9th grade**: Plain English. Easily understood by 13- to 15-year-old students.\n- **10th to 12th grade**: Fairly difficult to read.\n- **College**: Difficult to read.\n- **College graduate**: Very difficult to read. Best understood by university graduates.\n\n\n<a id='4.5'></a> \n<h3><b>4.5 Recommendation 5:</b></h3>\n\nMost job descriptions **(more than 89%) are written at an average college level instead of plain english**. However, they **should be written at a much lower grade level (8th grade or lower)** to attract busy candidates. \n\nThe best candidates are busy people and trying to get them to read college level text and then to stay awhile is not recommended. Most candidates spend less than <10 seconds reading a text-based job ad so job postings must be easy to read and simple to understand.\n\n<br><br><br><br>\n<h3>Salary Analysis</h3>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"salary_gen_df=pd.DataFrame(job_class_df.groupby(['FILE_NAME','ENTRY_SALARY_GEN']).size().reset_index(name='Freq'))\nsalary_gen_df['Is_Flat_Rated'] = salary_gen_df['ENTRY_SALARY_GEN'].apply(lambda x: 'No' if '-' in x else 'Yes')\nsalary_gen_df['Start_Range'] = salary_gen_df['ENTRY_SALARY_GEN'].apply(lambda x: 0 if x=='' else int(x.split('-')[0].replace(',','')))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = plt.figure(2,(15,8))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\n\ndata = salary_gen_df['Is_Flat_Rated'].value_counts(sort=False)\nfig.add_subplot(1,2,1)\nsns.barplot(y=data.values,x=data.index)\nplt.title('Is Flat Rated?')\n\nfig.add_subplot(1,2,2)\nsns.distplot(salary_gen_df['Start_Range'])\nplt.title('Salary Distribution Plot')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3><b>Interpretations</b></h3>\n\nIt can be concluded by analysing above graph that in most job ads salary range is specified in comparison to flat rated amount.\n\n\"When people are looking at job descriptions, they are looking for the details that drive their motivations when changing jobs,\" said Monica Lewis, head of product for LinkedIn Jobs. \nIn [a recent study](https://www.shrm.org/resourcesandtools/hr-topics/talent-acquisition/pages/salary-most-important-part-job-ad.aspx) on what candidates want during the job hunt, it was found that **over 70 percent of professionals want to hear about salary in the first message from a recruiter.** With 59 percent of candidates stating that salary was the leading factor that contributed to feeling fulfilled in their career, understanding pay and benefits is clearly top of mind during the job search.\n\n**This also helps prevent bias compensation of men and women.** Stating salary upfront will apply equally for all irrespective of gender,race or age.\n\n<br><br><br><br><h3>Exam Type Distribution</h3>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"exam_type_df=pd.DataFrame(job_class_df.groupby(['FILE_NAME','EXAM_TYPE']).size().reset_index(name='Freq'))\n\nshowHistogram(data=exam_type_df['EXAM_TYPE'], title='Exam Type Distribution')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Legend explanation:**\n\n- **OPEN:** Exam open to anyone (pending other requirements)\n- **INT_DEPT_PROM:** Interdepartmental Promotional\n- **DEPT_PROM:** Departmental Promotional\n- **OPEN_INT_PROM:** Open or Competitive Interdepartmental Promotional"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"selection_process_df = getValues(searchText='selection', COL_NAME='SELECTION_PROCESS', headingsFrame=createHeadingsFrame())\nselection_process_df['SELECTION_CRITERIA'] = selection_process_df['SELECTION_PROCESS'].apply(getSelection)\ns = selection_process_df[\"SELECTION_CRITERIA\"].str.split('|', expand=True).stack()\ni = s.index.get_level_values(0)\nselection_process_df = selection_process_df.loc[i].copy()\nselection_process_df[\"SELECTION_CRITERIA\"] = s.values\n\nselection_process_df = group_lower_ranking_values(selection_process_df, 'SELECTION_CRITERIA')\n\nplt.figure(1, figsize=(8,8))\nplt.pie(selection_process_df['SELECTION_PROCESS'],labels=selection_process_df.index, autopct='%1.1f%%', startangle=90)\nplt.axis('equal')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3><b>Interpretations</b></h3>\n\nAs per data, the major methods used for selection process are **Interviews, Essays and Written Tests**.\n<br>Interview being a major part of the selection, below are few recommendations.\n\n<a id='4.6'></a>\n<h3><b>4.6 Recommendation 6:</b></h3>\n\n- Research featured in Harvard Business Review found that when the final candidate pool has only one minority candidate, he or she has virtually no chances of being hired. If there are at least two female candidates in the final candidate pool, the odds of hiring a female candidate are 79 times greater. If there are at least two minority candidates in the final candidate pool, the odds of hiring a minority candidate are 194 times greater. Hence, the **“two in the pool effect” is recommended** to be followed. Avoid elimination of females/minorities in begining on a strict criteria in order to increase their chances of being selected.\n- Prioritize the skills you are looking for before you interview. Don’t use a laundry list, focus on **behavioural set of questions**. **Agree to criteria in advance of seeing candidates.** This helps you fairly and effectively evaluate candidates with different but equal experiences.\n- Employ a **diverse set of interviewers**. Women are much more likely to join a company when they can interact with women who are already there, and can testify to a company’s commitment to diversity. In fact, experts say, one of the biggest deciding factors on whether or not a female candidate accepts a job is if there was a woman on the interview panel.\n\n<br><br><br><br>\n<h2><b>Featured Words:</b></h2>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def getWordCloud(corpus):\n    wordcloud = WordCloud(\n                              background_color='white',\n                              stopwords=STOPWORDS,\n                              max_words=100,\n                              max_font_size=50, \n                              random_state=42\n                             ).generate(str(corpus))\n    return wordcloud","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"job_duties_df=pd.DataFrame(job_class_df.groupby(['FILE_NAME','JOB_DUTIES']).size().reset_index(name='Freq'))\ndf_processnotes = getValues('process note','PROCESS_NOTES', headingsFrame=createHeadingsFrame())","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = plt.figure(6,(15,15))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\n\nwordcloud = getWordCloud(' '.join(jobs_df['Content'].values))\nfig.add_subplot(3,2,1)\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.title('Job Class Content')\nplt.axis('off')\n\nwordcloud = getWordCloud(' '.join(job_duties_df['JOB_DUTIES'].values))\nfig.add_subplot(3,2,2)\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.title('Job Duties')\nplt.axis('off')\n\nwordcloud = getWordCloud(' '.join(df_requirements['REQUIREMENT_TEXT'].values))\nfig.add_subplot(3,2,3)\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.title('Requirements')\nplt.axis('off')\n\nwordcloud = getWordCloud(' '.join(job_class_df['EDUCATION_MAJOR'].values))\nfig.add_subplot(3,2,4)\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.title('Education Major')\nplt.axis('off')\n\nwordcloud = getWordCloud(' '.join(job_class_df['COURSE_SUBJECT'].values))\nfig.add_subplot(3,2,5)\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.title('Course Subject')\nplt.axis('off')\n\nwordcloud = getWordCloud(' '.join(df_processnotes['PROCESS_NOTES'].values))\nfig.add_subplot(3,2,6)\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.title('Process Notes')\nplt.axis('off')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3><b>Interpretations</b></h3>\n\n- **Job bulletins** reflects good to see words like 'Equal employment' and 'Disability accomodation'.\n- **Job duties** has positive message to be conveyed when digged, like to build and maintain a strong workforce/team and focus on equal employment.\n- **Requirements** implies that a candidate full time degree is peferred.\n- **Education major** shows the stream of study of an applicant. Candidate studied Science, Arts are required.\n- **Course subject** shows that City of LA is focusing majorly on engineering, science & accounting related courses.\n- **Process notes** shows that candidature disqualification can be done on basis of voilations found in the records.\n\n<br>Let's have a look at some phrases/words that are occuring more frequently in the bulletins.\n\n<br><br>\n<h3><b>Analyzing Phrases (n-grams):</b></h3>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def getCorpus(data, COL_NAME):\n    corpus = []\n    for i in range(0, data.shape[0]):\n        #Remove punctuations\n        text = re.sub('[^a-zA-Z]', ' ', data[COL_NAME][i])\n\n        #Convert to lowercase\n        text = text.lower()\n\n        #remove tags\n        text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n\n        # remove special characters and digits\n        text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n\n        ##Convert to list from string\n        text = text.split()\n\n        ##Stemming\n        ps=PorterStemmer()\n        #Lemmatisation\n        lem = WordNetLemmatizer()\n        text = [lem.lemmatize(word) for word in text if not word in  \n                set(stopwords.words(\"english\"))] \n        text = \" \".join(text)\n        #if len(text)>2:\n        corpus.append(text)\n    return corpus\n\ndef lambda_unpack(f):\n    return lambda args: f(*args)\ndef extract_key_words(text, good_tags=set(['JJ','JJR','JJS','NN','NNP','NNS','NNPS'])):\n\n    # exclude words that are stop words or entirely punctuation\n    punct = set(string.punctuation)\n    stop_words = set(nltk.corpus.stopwords.words('english'))\n    # tokenize and POS-tag words\n    tagged_words = itertools.chain.from_iterable(nltk.pos_tag_sents(nltk.word_tokenize(sent)\n                                                                    for sent in nltk.sent_tokenize(text)))\n    # filter on certain POS tags and lowercase all words\n    keywords = [word.lower() for word, tag in tagged_words\n                  if tag in good_tags and word.lower() not in stop_words\n                  and not all(char in punct for char in word)]\n\n    return keywords\n\ndef extract_key_chunks(text, grammar=r'KT: {(<JJ>* <NN.*>+ <IN>)? <JJ>* <NN.*>+}'):\n    \n    # exclude candidates that are stop words or entirely punctuation\n    punct = set(string.punctuation)\n    stop_words = set(nltk.corpus.stopwords.words('english'))\n    # tokenize, POS-tag, and chunk using regular expressions\n    chunker = nltk.chunk.regexp.RegexpParser(grammar)\n    tagged_sents = nltk.pos_tag_sents(nltk.word_tokenize(sent) for sent in nltk.sent_tokenize(text))\n    all_chunks = list(itertools.chain.from_iterable(nltk.chunk.tree2conlltags(chunker.parse(tagged_sent))\n                                                    for tagged_sent in tagged_sents))\n    # join constituent chunk words into a single chunked phrase\n    keychunks = [' '.join(word for word, pos, chunk in group).lower()\n                  for key, group in itertools.groupby(all_chunks, lambda_unpack(lambda word, pos, chunk: chunk != 'O')) if key]\n\n    return [cand for cand in keychunks\n            if cand not in stop_words and not all(char in punct for char in cand)]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"requirements = getRequirements(headingsFrame=createHeadingsFrame(),COL_NAME='REQUIREMENT_TEXT')\nrequirements['REQUIREMENT_WORDS'] = requirements['REQUIREMENT_TEXT'].apply(extract_key_words)\nrequirements['REQUIREMENT_CHUNKS'] = requirements['REQUIREMENT_TEXT'].apply(extract_key_chunks)\nrequirements['REQUIREMENT_SENTS'] = requirements['REQUIREMENT_WORDS'].apply(lambda x: ' '.join(x))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#Most frequently occuring words\ndef get_top_n_words(corpus, n=None):\n    vec = CountVectorizer().fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in      \n                   vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], \n                       reverse=True)\n    return words_freq[:n]\n\n#Most frequently occuring Bi-grams\ndef get_top_n3_words(corpus, n=None):\n    vec1 = CountVectorizer(ngram_range=(3,3),  \n            max_features=2000).fit(corpus)\n    bag_of_words = vec1.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in     \n                  vec1.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], \n                reverse=True)\n    return words_freq[:n]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"requirements_corpus = getCorpus(COL_NAME='REQUIREMENT_SENTS', data=requirements)\n\ntop3_words = get_top_n3_words(requirements_corpus, n=20)\ntop3_df = pd.DataFrame(top3_words)\ntop3_df.columns=[\"Requirement sentences(n-gram)\", \"Freq\"]\n\nsns.set(rc={'figure.figsize':(13,8)})\nh=sns.barplot(x=\"Requirement sentences(n-gram)\", y=\"Freq\", data=top3_df)\nh.set_xticklabels(h.get_xticklabels(), rotation=45)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df_processnotes['PROCESS_NOTE_WORDS'] = df_processnotes['PROCESS_NOTES'].apply(extract_key_words)\ndf_processnotes['PROCESS_NOTE_CHUNKS'] = df_processnotes['PROCESS_NOTES'].apply(extract_key_chunks)\ndf_processnotes['PROCESS_NOTE_SENTS'] = df_processnotes['PROCESS_NOTE_WORDS'].apply(lambda x: ' '.join(x))\n\nprocess_notes_corpus = getCorpus(COL_NAME='PROCESS_NOTE_SENTS', data=df_processnotes)\n\ntop3_words = get_top_n3_words(process_notes_corpus, n=20)\ntop3_df = pd.DataFrame(top3_words)\ntop3_df.columns=[\"Process notes(n-gram)\", \"Freq\"]\n\nsns.set(rc={'figure.figsize':(13,8)})\nh=sns.barplot(x=\"Process notes(n-gram)\", y=\"Freq\", data=top3_df)\nh.set_xticklabels(h.get_xticklabels(), rotation=45)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"job_duties_df['JOB_DUTIES_WORDS'] = job_duties_df['JOB_DUTIES'].apply(extract_key_words)\njob_duties_df['JOB_DUTIES_CHUNKS'] = job_duties_df['JOB_DUTIES'].apply(extract_key_chunks)\njob_duties_df['JOB_DUTIES_SENTS'] = job_duties_df['JOB_DUTIES_WORDS'].apply(lambda x: ' '.join(x))\n\njob_duties_corpus = getCorpus(COL_NAME='JOB_DUTIES_SENTS', data=job_duties_df)\n\ntop3_words = get_top_n3_words(job_duties_corpus, n=20)\ntop3_df = pd.DataFrame(top3_words)\ntop3_df.columns=[\"Job Duties(n-gram)\", \"Freq\"]\n\nsns.set(rc={'figure.figsize':(13,8)})\nh=sns.barplot(x=\"Job Duties(n-gram)\", y=\"Freq\", data=top3_df)\nh.set_xticklabels(h.get_xticklabels(), rotation=45)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3><b>Interpretation</b></h3>\n\nSimilar to inference derived from the word cloud, these graphs also depicts that **Full time - Four year college/university** education, **Full time paid** experience, **moving voilations** in past records, building **effecive workforce**, **equal employment** opportunities are some key phrases that dominates the job bulletins.\n<br><br>So equal employment is mentioned explicitly quite a number of times in the job class postings. **But what about implicit biasedness?**\n\nBefore moving to implicit biasedness, there is a point which highly influences diversity ie; **Work-Life Balance**. This is no where to be seen in featured words which plays a vital role to attract female candidates.\n**As per a study, job postings that talk about work-life balance attract far more women to apply.**\n\nBelow is an [analysis by textio.ai](https://textio.ai/hr-buzzword-bingo-e475e3fe22ab) about the importance of HR phrases in a job post.\n\n<img src='https://i.imgur.com/fpJJwxU.png' width='800' height='700'>\n\n<br>\n<a id='4.7'></a>\n<h3><b>4.7 Recommendation 7</b></h3>\n- It's often a challenge finding a full-timer with the diverse expertise, so it's worth considering hiring several **part-timers, each with a highly defined skill set**.\n- Divide the requirement section into two : \n    - **Minimally Required -** Identify those items that are minimally required to perform the essential duties of the role, not what the current incumbent may possess.\n    - **Preferred OR Specialized -** These are not required to perform the basic functions of the role.\n- Try to add **'WHY YOU SHOULD APPLY'** section in order to provide vision to  candidate and attract more candidates.\n- Add some key phrases like **'work life balance'** in bulletin text, that'll help attract **more women candidates** thereby increasing diversity.\n\n<br><br><br><br>\n<h2><b>Now let's check for the implicit biasedness in the text</b></h2>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"feminine_coded_words = [\n    \"agree\",\n    \"affectionate\",\n    \"child\",\n    \"cheer\",\n    \"collab\",\n    \"commit\",\n    \"communal\",\n    \"compassion\",\n    \"connect\",\n    \"considerate\",\n    \"cooperat\",\n    \"co-operat\",\n    \"depend\",\n    \"emotiona\",\n    \"empath\",\n    \"feel\",\n    \"flatterable\",\n    \"gentle\",\n    \"honest\",\n    \"interpersonal\",\n    \"interdependen\",\n    \"interpersona\",\n    \"inter-personal\",\n    \"inter-dependen\",\n    \"inter-persona\",\n    \"kind\",\n    \"kinship\",\n    \"loyal\",\n    \"modesty\",\n    \"nag\",\n    \"nurtur\",\n    \"pleasant\",\n    \"polite\",\n    \"quiet\",\n    \"respon\",\n    \"sensitiv\",\n    \"submissive\",\n    \"support\",\n    \"sympath\",\n    \"tender\",\n    \"together\",\n    \"trust\",\n    \"understand\",\n    \"warm\",\n    \"whin\",\n    \"enthusias\",\n    \"inclusive\",\n    \"yield\",\n    \"share\",\n    \"sharin\"\n]\n\nmasculine_coded_words = [\n    \"active\",\n    \"adventurous\",\n    \"aggress\",\n    \"ambitio\",\n    \"analy\",\n    \"assert\",\n    \"athlet\",\n    \"autonom\",\n    \"battle\",\n    \"boast\",\n    \"challeng\",\n    \"champion\",\n    \"compet\",\n    \"confident\",\n    \"courag\",\n    \"decid\",\n    \"decision\",\n    \"decisive\",\n    \"defend\",\n    \"determin\",\n    \"domina\",\n    \"dominant\",\n    \"driven\",\n    \"fearless\",\n    \"fight\",\n    \"force\",\n    \"greedy\",\n    \"head-strong\",\n    \"headstrong\",\n    \"hierarch\",\n    \"hostil\",\n    \"impulsive\",\n    \"independen\",\n    \"individual\",\n    \"intellect\",\n    \"lead\",\n    \"logic\",\n    \"objective\",\n    \"opinion\",\n    \"outspoken\",\n    \"persist\",\n    \"principle\",\n    \"reckless\",\n    \"self-confiden\",\n    \"self-relian\",\n    \"self-sufficien\",\n    \"selfconfiden\",\n    \"selfrelian\",\n    \"selfsufficien\",\n    \"stubborn\",\n    \"superior\",\n    \"unreasonab\"\n]\n\nhyphenated_coded_words = [\n    \"co-operat\",\n    \"inter-personal\",\n    \"inter-dependen\",\n    \"inter-persona\",\n    \"self-confiden\",\n    \"self-relian\",\n    \"self-sufficien\"\n]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def clean_up_word_list(ad_text):\n    cleaner_text = ''.join([i if ord(i) < 128 else ' '\n        for i in ad_text])\n    cleaner_text = re.sub(\"[\\\\s]\", \" \", cleaner_text, 0, 0)\n    cleaned_word_list = re.sub(u\"[\\.\\t\\,“”‘’<>\\*\\?\\!\\\"\\[\\]\\@\\':;\\(\\)\\./&]\",\n        \" \", cleaner_text, 0, 0).split(\" \")\n    word_list = [word.lower() for word in cleaned_word_list if word != \"\"]\n    return de_hyphen_non_coded_words(word_list)\n\ndef de_hyphen_non_coded_words(word_list):\n    for word in word_list:\n        if word.find(\"-\"):\n            is_coded_word = False\n            for coded_word in hyphenated_coded_words:\n                if word.startswith(coded_word):\n                    is_coded_word = True\n            if not is_coded_word:\n                word_index = word_list.index(word)\n                word_list.remove(word)\n                split_words = word.split(\"-\")\n                word_list = (word_list[:word_index] + split_words +\n                    word_list[word_index:])\n    return word_list\n\ndef assess_coding(row):\n    coding = ''\n    coding_score = row[\"feminine_ad_word_count\"] - row[\"masculine_ad_word_count\"]\n    if coding_score == 0:\n        if row[\"feminine_ad_word_count\"]>0:\n            coding = \"neutral\"\n        else:\n            coding = \"empty\"\n    elif coding_score > 3:\n        coding = \"strongly feminine-coded\"\n    elif coding_score > 0:\n        coding = \"feminine-coded\"\n    elif coding_score < -3:\n        coding = \"strongly masculine-coded\"\n    else:\n        coding = \"masculine-coded\"\n    return coding\n\ndef assess_coding_txt(fem_word_count, masc_word_count):\n    coding = ''\n    coding_score = fem_word_count - masc_word_count\n    if coding_score == 0:\n        if fem_word_count>0:\n            coding = \"neutral\"\n        else:\n            coding = \"empty\"\n    elif coding_score > 3:\n        coding = \"strongly feminine-coded\"\n    elif coding_score > 0:\n        coding = \"feminine-coded\"\n    elif coding_score < -3:\n        coding = \"strongly masculine-coded\"\n    else:\n        coding = \"masculine-coded\"\n    return coding\n\ndef find_and_count_coded_words(advert_word_list, gendered_word_list):\n    gender_coded_words = [word for word in advert_word_list\n        for coded_word in gendered_word_list\n        if word.startswith(coded_word)]\n    return (\",\").join(gender_coded_words), len(gender_coded_words)\n\ndef assessBias(txt):\n    words = clean_up_word_list(txt)\n    txt_masc_coded_words, masc_word_count = find_and_count_coded_words(words, masculine_coded_words)\n    txt_fem_coded_words, fem_word_count = find_and_count_coded_words(words, feminine_coded_words)\n    coding = assess_coding_txt(fem_word_count, masc_word_count)\n    print('List of masculine words found:')\n    print(txt_masc_coded_words)\n    print('\\nList of feminine words found:')\n    print(txt_fem_coded_words)\n    return coding","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print('Cleaning job bulletins words...')\njobs_df[\"clean_ad_words\"] = jobs_df[\"Content\"].apply(clean_up_word_list)\nprint('Finding masculine coded words...')\njobs_df[\"masculine_coded_ad_words\"], jobs_df[\"masculine_ad_word_count\"] = zip(*jobs_df[\"clean_ad_words\"].apply(\n    lambda x: find_and_count_coded_words(x,masculine_coded_words)))\nprint('Finding feminine coded words...')\njobs_df[\"feminine_coded_ad_words\"], jobs_df[\"feminine_ad_word_count\"] = zip(*jobs_df[\"clean_ad_words\"].apply(\n    lambda x: find_and_count_coded_words(x,feminine_coded_words)))\nprint('Assessing coding...')\njobs_df['coding'] = jobs_df.apply(assess_coding, axis=1)\nprint('Done')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"jobs_bias_df = jobs_df['coding'].value_counts()\njobs_bias_df = pd.DataFrame({'code' : jobs_bias_df.index, 'values' : jobs_bias_df.values})\n\nexplode = (0.1, 0, 0, 0, 0.1)\nplt.figure(1,(8,8))\nplt.pie(jobs_bias_df['values'],labels=jobs_bias_df['code'],explode=explode, autopct='%1.1f%%', startangle=90)\nplt.axis('equal')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Legend Explanations:</b>\n- **masculine-coded**\n    <br>This job ad uses more words that are subtly coded as feminine than words that are subtly coded as masculine (according to the research). Fortunately, the research suggests this will have only a slight effect on how appealing the job is to men, and will encourage women applicants.\n- **feminine-coded**\n    <br>This job ad uses more words that are subtly coded as masculine than words that are subtly coded as feminine (according to the research).It risks putting women off applying, but will probably encourage men to apply.\n- **strongly masculine-coded**\n    <br>This job ad uses more words that are subtly coded as feminine than words that are subtly coded as masculine (according to the research). Fortunately, the research suggests this will have only a slight effect on how appealing the job is to men, and will encourage women applicants.\n- **strongly feminine-coded**\n    <br>This job ad uses more words that are subtly coded as masculine than words that are subtly coded as feminine (according to the research). It risks putting women off applying, but will probably encourage men to apply.\n- **empty**\n    <br>This job ad doesn't use any words that are subtly coded as masculine or feminine (according to the research). It probably won't be off-putting to men or women applicants.\n- **neutral**\n    <br>This job ad uses an equal number of words that are subtly coded as masculine and feminine (according to the research). It probably won't be off-putting to men or women applicants.\n\n<br><br>This clearly reflects the problems in job bulletins. **Only 7% job bulletins' reflects neutrality** in the text. Rest all are either masculine or feminine.\n\nAbove analysis is based on the research that can be referred [here.](http://gender-decoder.katmatfield.com/about)\n<br>Research paper can be found [here.](http://gender-decoder.katmatfield.com/static/documents/Gaucher-Friesen-Kay-JPSP-Gendered-Wording-in-Job-ads.pdf)\n\n<br>\n<b>Gender coded words found in bulletins:</b>"},{"metadata":{"scrolled":true,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = plt.figure(2,(15,15))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\n\nwordcloud = getWordCloud(' '.join(jobs_df[\"masculine_coded_ad_words\"]))\nfig.add_subplot(1,2,1)\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.title('Masculine-Coded Words in Bulletins')\nplt.axis('off')\n\nwordcloud = getWordCloud(' '.join(jobs_df[\"feminine_coded_ad_words\"]))\nfig.add_subplot(1,2,2)\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.title('Feminine-Coded Words in Bulletins')\nplt.axis('off')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='4.8'></a>\n<h3><b>4.8 Recommendation 8:</b></h3>\n\n- Almost 90% of both women and men are open to hearing about new opportunities from recruiters and both genders do their homework. But, **women gets more affected by the langauage of JD**. 42% of working women have faced one of [8 types of discrimination at work](https://www.pewresearch.org/fact-tank/2017/12/14/gender-discrimination-comes-in-many-forms-for-todays-working-women/) so **using inclusive language** in job descriptions really matters.\n- It is suggested that **adding neutral, empty or few feminine coded words is more effective**. In addition to it, **prevent excessive use of superlatives**.This will have only a slight effect on how appealing the job advertised is to men, and will encourage women applicants. However, ads that have more stereotypically masculine words will risk putting women off applying.\n- As according to a study, women tend to apply for a post only if they match 100% of the criteria while men usually applies even after 60% of the requirements are fulfilled.\n<br>**Use softer language** in your requirements [like these from Hubspot](https://thinkgrowth.org/your-job-descriptions-are-hurting-your-hiring-pipeline-52b5a406fb8f) in order to attract more female candidates:\n    - familiarity with...\n    - bonus points for...\n    - working knowledge of...\n    - comfortable with...\n    - if you have any combination of these skills...\n    \n<br><br><br><br>\n<h3>Analysis on gender wise job applicants</h3>\n[Data source](https://catalog.data.gov/dataset/applicant-information-from-7-1-2014-to-9-30-2014-7835b)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"job_applicants = pd.read_csv('../input/la-applicants-gender-and-ethnicity/rows.csv')\njob_applicants.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data used here is available only for fiscal years 2013-2014 & 2014-2015\n<br>Only 185 job classes are present in the data\n<br>Statistical significance of the data is upon the users of the data.\n\nHere, I've tried to analyse the jobs that are female or male dominated.\n<br>If female applicant percentage is greater than 60% then it is considered as female dominant and if it is less than 30% then it is considered as male dominant."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"job_applicants['FemalePercentage'] = (job_applicants['Female'])*100/(job_applicants['Female']+job_applicants['Male'])\njob_applicants['job_title'] = job_applicants['Job Description'].apply(lambda x : x[:-4].strip())\n\njob_applicants = job_applicants.merge(job_class_df[['FILE_NAME','JOB_DUTIES','JOB_CLASS_TITLE']],\n                    how = 'left', left_on = 'job_title', right_on = 'JOB_CLASS_TITLE')\njob_applicants = job_applicants.drop_duplicates()\njob_applicants.fillna('',inplace=True)\n\njob_applicants['JOB_DUTIES_WORDS'] = job_applicants['JOB_DUTIES'].apply(extract_key_words)\njob_applicants['JOB_DUTIES_CHUNKS'] = job_applicants['JOB_DUTIES'].apply(extract_key_chunks)\njob_applicants['JOB_DUTIES_SENTS'] = job_applicants['JOB_DUTIES_WORDS'].apply(lambda x: ' '.join(x))\n\njob_applicants_female = job_applicants[job_applicants['FemalePercentage']>60]\njob_applicants_female.reset_index(inplace=True)\njob_applicants_male = job_applicants[job_applicants['FemalePercentage']<30]\njob_applicants_male.reset_index(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"female_duties_corpus = getCorpus(job_applicants_female,'JOB_DUTIES_SENTS')\nwordcloud = getWordCloud(female_duties_corpus)\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.title('Job Duties from female dominated jobs')\nplt.axis('off')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"male_duties_corpus = getCorpus(job_applicants_male,'JOB_DUTIES_SENTS')\nwordcloud = getWordCloud(male_duties_corpus)\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.title('Job Duties from male dominated jobs')\nplt.axis('off')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3><b>Interpretations</b></h3>\n\nThe above two word clouds display **words found in female dominating job duties and male dominating job duties respectively**.\n<br>Jobs in relation to Clerical, Art, Public facing, Accounting attracts more to a female applicant. Whereas job duties involving Equipments, Supervisory, Construction tends to deter females and interests more of male applicants.\n\n<a id='4.9'></a>\n<h3><b>4.9 Recommendation 9:</b></h3>\n\nJobs indicating equipment, supervisor can be thought of involing more males as these are some of the terms relating to core technical jobs which requires lot of time in field. This helps women form an opinion of these jobs as male dominating with risk of harrassment and putting deserving female candidates off from applying to these kinds of openings.\n<br>It is recommended to publish the job post involving these kind of work with **pictures/videos shouting out the importance of diversity for the organisation**, may be with names of female employees at senior position.\n\n<br><br><br><br>\n<h3><b>Finding Similar Jobs</b></h3>\n\n<b>Logic explanation</b>\n1. Bulletin context is selected(i.e. based on which similar jobs are to be searched, e.g. based on 'Requirements' section from a bulletin)\n2. Text from the selected context is extracted and converted into sentences.\n3. A sentence is then tagged with part of speech tags\n4. Words are extracted after identifying some good tags(like 'NN', 'NNP', ...)\n5. A similarity measure is identified.\n    - Some conditions a good similarity measure has to obey\n        - 0 <= similarity(A, B) <= 1\n        - similarity(A, A) = 1\n        - similarity(A, B) =/= 1 if A =/= B\n    - Similarity measure I've used here is cosine similarity.\n        - It's a widely used measure in Natural Language Processing. \n        - The Cosine Similarity computes the cosine of the angle between 2 vectors. If the vectors are identical, the cosine is 1.0. If the vectors are orthogonal, the cosine is 0.0.\n        - NLTK implements cosine_distance, which is 1 - cosine_similarity. The concept of distance is opposite to similarity. Two identical vectors are located at 0 distance and are 100% similar.\n6. A similarity matrix is build\n7. Similar jobs are extracted"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def sentence_similarity(sent1, sent2, stopwords=None):\n    if stopwords is None:\n        stopwords = []\n \n    sent1 = [w.lower() for w in sent1]\n    sent2 = [w.lower() for w in sent2]\n \n    all_words = list(set(sent1 + sent2))\n \n    vector1 = [0] * len(all_words)\n    vector2 = [0] * len(all_words)\n \n    # build the vector for the first sentence\n    for w in sent1:\n        if w in stopwords:\n            continue\n        vector1[all_words.index(w)] += 1\n \n    # build the vector for the second sentence\n    for w in sent2:\n        if w in stopwords:\n            continue\n        vector2[all_words.index(w)] += 1\n \n    return 1 - cosine_distance(vector1, vector2)\n\ndef build_similarity_matrix(sentences, stopwords=None):\n    # Create an empty similarity matrix\n    S = np.zeros((len(sentences), len(sentences)))\n \n \n    for idx1 in range(len(sentences)):\n        for idx2 in range(len(sentences)):\n            if idx1 == idx2:\n                continue\n \n            S[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stopwords) \n    return S\n\ndef showSimilarJobs(data, job_title, COL_NAME, similarity_matrix, n=3):\n    i = data[data[COL_NAME]==job_title].index.values.astype(int)\n    if len(i)>0:\n        i = i[0]\n        ind = np.argpartition(similarity_matrix[i], -n)[-n:]\n        print(data.loc[ind][COL_NAME].values)\n    else:\n        print('Job class not found in data.')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#Creating similarity matrix using words in requirements section of bulletins\nrequirements_s_matrix = build_similarity_matrix(requirements['REQUIREMENT_WORDS'], STOP_WORDS)\n\n#Removing bulletins where job duties section is missing\njob_duties_df['WORDS_LEN'] = job_duties_df['JOB_DUTIES_WORDS'].apply(lambda x: len(x))\njob_duties_words = job_duties_df[job_duties_df['WORDS_LEN']>0]\njob_duties_words = job_duties_words.reset_index()\n\n#Creating similarity matrix using words in job duties section of bulletins\njob_duties_s_matrix = build_similarity_matrix(job_duties_words['JOB_DUTIES_WORDS'], STOP_WORDS)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Show similar jobs based on Requirements**"},{"metadata":{"trusted":true},"cell_type":"code","source":"showSimilarJobs(requirements,'ANIMAL KEEPER 4304 083118.txt','FILE_NAME', requirements_s_matrix, 3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Show similar jobs based on Job Duties**"},{"metadata":{"trusted":true},"cell_type":"code","source":"showSimilarJobs(job_duties_words,'ANIMAL KEEPER 4304 083118.txt','FILE_NAME', job_duties_s_matrix, 3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='4.10'></a>\n<h3><b>4.10 Recommendation 10:</b></h3>\nPeople will only apply for a job post if they are aware of the job applications. If someone shows an interest for a job, then it is recommended to **display all the related/similar job titles**.\nThis give applicants a chance to look for similar jobs and thereby improving the count of applications received for a particular job.\n\n<br><br><br><br>\n<h3><b>Promotional Paths</b></h3>\n\nThis is an attempt for better visualization of different promotional paths available for a particular job title.\n\n**Promotional paths are displayed here as an interactive graph**\n<br>To achieve this, nodes & edges are created for the graph\n- Each node represents a job title\n- Edge is the connection between two job titles\n- Direction of the edge shows the next level job title\n- Clicking on a node will allow it to expand and display next level promotion paths, if available.\n\nIn order to check available promotions and visualise different paths for a job title, I've tried to create a sample application that is shared in [this kernel](https://www.kaggle.com/tyagit3/dsfg-cityofla-analysis-and-solution-application-4)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def createEdges (job_class_df):\n    edge_cols = ['JOB_CLASS_TITLE','EXP_JOB_CLASS_TITLE','EXPERIENCE_LENGTH','EXPERIENCE_LEN_UNIT']\n\n    df_edges = job_class_df[edge_cols]\n\n    #EXP_JOB_CLASS_TITLE consists of job classes separated with ',' (e.g. SYSTEMS ANALYST, MANAGEMENT ANALYST)\n    #Spliting these values and creating rows for each job class\n    s = df_edges[\"EXP_JOB_CLASS_TITLE\"].str.split(',', expand=True).stack()\n    i = s.index.get_level_values(0)\n    df_edges = df_edges.loc[i].copy()\n    df_edges[\"EXP_JOB_CLASS_TITLE\"] = s.values\n\n    df_edges['EXP_JOB_CLASS_TITLE'] = df_edges['EXP_JOB_CLASS_TITLE'].apply(lambda x: x.strip())\n\n    df_edges = pd.DataFrame(df_edges.groupby(edge_cols).size().reset_index(name='Freq'))\n    df_edges.drop(columns='Freq', inplace=True)\n\n    #remove exp_job_class_title if it is same as job_class_title\n    df_edges.loc[df_edges['JOB_CLASS_TITLE']==df_edges['EXP_JOB_CLASS_TITLE'],'EXP_JOB_CLASS_TITLE']=''\n\n    #To find implicit links\n    #For job classes like 'Senior Systems Analyst' - junior level job class will be like 'Systems Analyst', if the title exists\n    #Find senior job_class that doesn't have junior level exp_job_class_title \n    job_without_juniors = []\n    for job_class in df_edges['JOB_CLASS_TITLE'].unique():\n        if job_class.startswith('SENIOR'):\n            exist = False\n            junior_level = job_class.replace('SENIOR','').strip()\n            for exp_job in df_edges[df_edges['JOB_CLASS_TITLE']==job_class]['EXP_JOB_CLASS_TITLE']:\n                if exp_job.strip()==junior_level:\n                    exist = True\n                    break\n            if not exist:\n                #add only those titles that are actually job_titles\n                if junior_level in map(str.strip,job_titles[0].values):\n                    job_without_juniors.append([job_class,junior_level,'',''])\n\n    df_edges = df_edges.append(pd.DataFrame(job_without_juniors,\n                                           columns = edge_cols), ignore_index=True)\n\n    #df_edges.head()\n    return df_edges\n\ndef getLowerJobEdges(job_title, data):\n    \"\"\"Lower edges or the job titles that can be promoted to job_title.\n    \n    Parameters\n    ----------\n    job_title : str\n        job class title for which lower job classes are to be searched in data\n    data : DataFrame\n        dataframe used to search lower job classes\n    \n    Returns\n    -------\n    list : [tuple,str]\n        tuple - edge tuple (source, target)\n        str - edge label\n    \"\"\"\n    \n    result = []\n    for index,e in data.iterrows():\n        if e['JOB_CLASS_TITLE'] == job_title and e['EXP_JOB_CLASS_TITLE']!='':\n            result.append([(e['EXP_JOB_CLASS_TITLE'],e['JOB_CLASS_TITLE']),\n                          str(e['EXPERIENCE_LENGTH'])+' '+e['EXPERIENCE_LEN_UNIT']])\n            result.extend(getLowerJobEdges(e['EXP_JOB_CLASS_TITLE'], data))\n    return result\n\ndef getUpperJobEdges(job_title, data):\n    \"\"\"Upper edges or the job titles that a job_title can be promoted to.\n    \n    Parameters\n    ----------\n    job_title : str\n        job class title for which upper job classes are to be searched in data\n    data : DataFrame\n        dataframe used to search upper job classes\n    \n    Returns\n    -------\n    list : [tuple,str]\n        tuple - edge tuple (source, target)\n        str - edge label\n    \"\"\"\n    \n    result = []\n    for index,e in data.iterrows():\n        if e['EXP_JOB_CLASS_TITLE'] == job_title:\n            result.append([(e['EXP_JOB_CLASS_TITLE'],e['JOB_CLASS_TITLE']),\n                          str(e['EXPERIENCE_LENGTH'])+' '+e['EXPERIENCE_LEN_UNIT']])\n            result.extend(getUpperJobEdges(e['JOB_CLASS_TITLE'], data))\n    return result\n\ndef getEdges(job_title, data):\n    \"\"\"Edges or job titles that a job_title can be promoted to/from.\n    \n    Parameters\n    ----------\n    job_title : str\n        job title to search for edges\n    data : DataFrame\n        dataframe used to search upper job classes\n    \n    Returns\n    -------\n    list : [tuple,str]\n        tuple - edge tuple (source, target)\n        str - edge label\n    \"\"\"\n    \n    edges = []\n    edges = getLowerJobEdges(job_title, data)\n    edges.extend(getUpperJobEdges(job_title, data))\n    return edges\n\ndef plotGraph(G):\n    \"\"\"Plots a networkx graph\n    \n    Parameters\n    ----------\n    G : networkx Graph\n    \"\"\"\n    \n    plt.figure(figsize=(12, 12)) \n    plt.axis('off')\n    pos = nx.circular_layout(G)\n    nx.draw_networkx(G, pos, with_labels=True, \n                    node_color='blue', font_size=8, node_size=10000, width = 2)\n    \n    #Uncomment below to draw edge labels\n    #nx.draw_networkx_edge_labels(G, label_pos=.5, pos=pos)\n    \n    plt.show()\n\ndef showPromotionalPaths(df_edges, job_class_df, job_title, candidateDataDf=pd.DataFrame):\n    \"\"\"Displays eligible/proposed promotions. Future promotion paths are also displayed as a directed graph.\n    \n    Parameters\n    ----------\n    candidateDatadf : DataFrame\n        Dataframe consisting of data for a candidate to be searched for available promotions. Only first row will be considered.\n    df_edges : DataFrame\n        Dataframe consisting of all available data edges from all job classes\n    \"\"\"\n    \n    if 'JOB_CLASS_TITLE' in candidateDataDf.columns:\n        #only first row will be considered and searched for promotional paths\n        job_title = candidateDataDf['JOB_CLASS_TITLE'].iloc[0]\n    if job_title == '':\n        print('No job title assigned.')\n        return\n    \n    job_nodes = job_class_df[job_class_df['JOB_CLASS_TITLE']==job_title][['JOB_CLASS_TITLE',\n                                                                          'EXP_JOB_CLASS_TITLE']]\n    job_node_names = [n[0] for index,n in job_nodes.iterrows()]\n    job_edges = getEdges(job_title, df_edges)\n    \n    for edge in job_edges:\n        if edge[0][0] == job_title:\n            checkEligibility(job_title=edge[0][1], job_class_df = job_class_df, candidateDataDf=candidateDataDf) #check and print eligiblity for explicit promotions\n    \n    job_edge_names = [e[0] for e in job_edges]\n    \n    #set edge labels\n    experience_dict={}\n    for edge in job_edges:\n        experience_dict[edge[0]] = edge[1]\n\n    #networkx directed graph for promotion path visuals\n    G = nx.DiGraph()\n    G.add_nodes_from(job_node_names)\n    G.add_edges_from(job_edge_names)\n    nx.set_edge_attributes(G, experience_dict, 'Experience')\n\n    #print(nx.info(G))\n    plotGraph(G)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"class Experience:\n    \"\"\"A class to represent experience details\n    \n    Attributes\n    ----------\n    expLength : float\n        experience length\n    expLengthUnit : str\n        unit for experience length (e.g. year/month/hour)\n    fullTimePartTime : str\n        type of experience (FULL_TIME/PART_TIME)\n    paidVolunteer : str\n        type of experience (PAID/VOLUNTEER)\n    experience : str\n        Represents the experience text by concatenating other attributes\n    errMsg : str\n        Represents the errors/mismatch occured while comparing two experiences. Blank if matched.\n    \n    Methods\n    -------\n    compare(objCandidateExperience)\n        Compares self with another experience object(i.e. objCandidateExperience) and returns errMsg accordingly\n    getExperience\n        Outputs a string representing the experience text\n    getErrMsg\n        Outputs a message to be displayed in case of comparison mismatch\n    \"\"\"\n    \n    def __init__(self, dfJobClassRow):\n        \"\"\"\n        \n        Parameters\n        ----------\n        dfJobClassRow : Series\n            Row of Job Class dataframe, containing experience related columns\n        \"\"\"\n        \n        if dfJobClassRow['EXPERIENCE_LENGTH']=='':\n            self.expLength = 0\n        else:\n            self.expLength = float(dfJobClassRow['EXPERIENCE_LENGTH'])\n        self.expLengthUnit = dfJobClassRow['EXPERIENCE_LEN_UNIT']\n        self.fullTimePartTime = dfJobClassRow['FULL_TIME_PART_TIME']\n        self.paidVolunteer = dfJobClassRow['PAID_VOLUNTEER']\n        self.experience = self.getExperience()\n        self.errMsg = ''\n    \n    def compare(self, objCandidateExperience):\n        \"\"\"Compares self with another experience class object\n        \n        Parameters\n        ----------\n        objCandidateExperience : Experience\n            object for Experience class created for a candidate\n        \n        Returns\n        -------\n        errMsg : str\n            blank if matched, else mismatched experience requirement string\n        \"\"\"\n        \n        if self.experience == objCandidateExperience.experience:\n            self.errMsg = ''\n        else:\n            if self.expLength == objCandidateExperience.expLength and self.expLengthUnit == objCandidateExperience.expLengthUnit:\n                if self.fullTimePartTime == objCandidateExperience.fullTimePartTime:\n                    if objCandidateExperience.paidVolunteer in self.paidVolunteer:\n                        self.errMsg = ''\n                    else:\n                        self.errMsg = self.getErrorMsg()\n                else:\n                    if objCandidateExperience.fullTimePartTime in self.fullTimePartTime:\n                        if objCandidateExperience.paidVolunteer in self.paidVolunteer.contains:\n                            self.errMsg = ''\n                        else:\n                            self.errMsg = self.getErrorMsg()\n                    else:\n                        self.errMsg = self.getErrorMsg()\n            elif self.expLengthUnit == objCandidateExperience.expLengthUnit:\n                if self.expLength < objCandidateExperience.expLength:\n                    if self.fullTimePartTime == objCandidateExperience.fullTimePartTime:\n                        if objCandidateExperience.paidVolunteer in self.paidVolunteer:\n                            self.errMsg = ''\n                        else:\n                            self.errMsg = self.getErrorMsg()\n                    else:\n                        if objCandidateExperience.fullTimePartTime in self.fullTimePartTime:\n                            if objCandidateExperience.paidVolunteer in self.paidVolunteer.contains:\n                                self.errMsg = ''\n                            else:\n                                self.errMsg = self.getErrorMsg()\n                        else:\n                            self.errMsg = self.getErrorMsg()\n                else:\n                    self.errMsg = self.getErrorMsg()\n            else:\n                self.errMsg = self.getErrorMsg()\n        #print(self.experience, objCandidateExperience.experience)\n        return self.errMsg\n    \n    def getExperience(self):\n        \"\"\"Outputs a string representing the experience text\n        \n        Returns\n        -------\n        str - string representing the experience text\n        \"\"\"\n        \n        return ' '.join([str(float(self.expLength) if self.expLength != '' else self.expLength),\n                         self.expLengthUnit,\n                         self.fullTimePartTime.replace('|','/'),\n                         self.paidVolunteer.replace('|','/')])\n    \n    def getErrorMsg(self):\n        \"\"\"Outputs a message to be displayed in case of comparison mismatch\"\"\"\n        \n        return self.experience + ' experience is required for this job class.'\n\nclass License:\n    \"\"\"A class to represent license details\n    \n    Attributes\n    ----------\n    driverLicReq : str\n        whether driver's license is required for a job class or not. In case of candidate, it should be 'R' if one holds a license\n    driverLicType : str\n        license types hold by candidate/ license types required for a job class\n    additionalLic : str\n        additional licenses hold by candidate/ required for a job class\n    license : str\n        Represents the license text by concatenating other attributes\n    errMsg : str\n        Represents the errors/mismatch occured while comparing two licenses. Blank if matched.\n    \n    Methods\n    -------\n    compare(objCandidateLicense)\n        Compares self with another license object(i.e. objCandidateLicense) and returns errMsg accordingly\n    getLicense\n        Outputs a string representing the license text\n    getErrMsg\n        Outputs a message to be displayed in case of comparison mismatch\n    \"\"\"\n    \n    def __init__(self, dfJobClassRow):\n        \"\"\"\n        \n        Parameters\n        ----------\n        dfJobClassRow : Series\n            Row of Job Class dataframe, containing license related columns\n        \"\"\"\n        \n        if 'DRIVERS_LICENSE_REQ' in dfJobClassRow:\n            self.driverLicReq = dfJobClassRow['DRIVERS_LICENSE_REQ']\n        else:\n            self.driverLicReq = 'R'\n        self.driverLicType = dfJobClassRow['DRIV_LIC_TYPE']\n        self.additionalLic = dfJobClassRow['ADDTL_LIC']\n        self.license = self.getLicense()\n        self.errMsg = ''\n    \n    def compare(self, objCandidateLicense):\n        \"\"\"Compares self with another license class object\n        \n        If DRIVERS_LICENSE_REQ is 'P', then this will consider it as a match\n        Additional licenses are not compared in this method.\n        \n        Parameters\n        ----------\n        objCandidateLicense : License\n            object for License class created for a candidate\n        \n        Returns\n        -------\n        errMsg : str\n            blank if matched, else mismatched license requirement string\n        \"\"\"\n        \n        if self.driverLicReq == 'P' or self.driverLicReq == '':\n            self.errMsg = ''\n        else:\n            if self.driverLicType == '' and objCandidateLicense.driverLicReq == 'R':\n                self.errMsg = ''            \n            elif objCandidateLicense.driverLicType in self.driverLicType:\n                self.errMsg = ''\n            else:\n                self.errMsg = self.getErrorMsg()\n        \n        return self.errMsg\n    \n    def getLicense(self):\n        \"\"\"Outputs a string representing the license text\n        \n        Returns\n        -------\n        str - string representing the license text\n        \"\"\"\n        \n        return self.driverLicType\n    \n    def getErrorMsg(self):\n        \"\"\"Outputs a message to be displayed in case of comparison mismatch\"\"\"\n        \n        if self.driverLicType != '':\n            return self.driverLicType + ' license is required for this job class.'\n        else:\n            return 'A valid California driver\\'s license is required for this job class.'\n\nclass JobClass:\n    \"\"\"A class to represent job class details\n    \n    Attributes\n    ----------\n    jobClassTitle : str\n        job class title/ current job class of candidate\n    examType : str\n        OPEN, INT_DEPT_PROM, DEPT_PROM, OPEN_INT_PROM ('' if not provided)\n    selectionCriteria : str\n        selection criteria for a job class ('' if not provided)\n    requirementSetId : str\n        requirement set id ('' if not provided)\n    requirementSubSetId : str\n        requirement sub set id ('' if not provided)\n    experience : Experience\n        experience details for job class\n    license : License\n        license details for job class\n    errMsg : str\n        Represents the errors/mismatch occured while comparing two licenses. Blank if matched.\n    \n    Methods\n    -------\n    compare(candidateJobClass)\n        Compares self with another JobClass object(i.e. candidateJobClass) and returns errMsg accordingly\n    \"\"\"\n    \n    def __init__(self, dfJobClassRow):\n        \"\"\"\n        \n        Parameters\n        ----------\n        dfJobClassRow : Series\n            Row of Job Class dataframe, containing job class columns\n        \"\"\"\n        \n        if 'JOB_CLASS_TITLE' in dfJobClassRow:\n            self.jobClassTitle = dfJobClassRow['JOB_CLASS_TITLE']\n        else:\n            self.jobClassTitle = 'CandidateJobClass'\n        if 'EXAM_TYPE' in dfJobClassRow:\n            self.examType = dfJobClassRow['EXAM_TYPE']\n        else:\n            self.examType = ''\n        if 'SELECTION_CRITERIA' in dfJobClassRow:\n            self.selectionCriteria = dfJobClassRow['SELECTION_CRITERIA']\n        else:\n            self.selectionCriteria = ''\n        self.expJobClassTitle = dfJobClassRow['EXP_JOB_CLASS_TITLE']\n        if 'REQUIREMENT_SET_ID' in dfJobClassRow:\n            self.requirementSetId = dfJobClassRow['REQUIREMENT_SET_ID']\n        else:\n            self.requirementSetId = ''\n        if 'REQUIREMENT_SUBSET_ID' in dfJobClassRow:\n            self.requirementSubSetId = dfJobClassRow['REQUIREMENT_SUBSET_ID']\n        else:\n            self.requirementSubSetId = ''\n        self.experience = Experience(dfJobClassRow)\n        self.license = License(dfJobClassRow)\n        self.errMsg = ''\n    \n    def compare(self, candidateJobClass):\n        \"\"\"Compares education, experience, license details of a candidate job class with self\n        \n        Parameters\n        ----------\n        candidateJobClass : JobClass\n            JobClass object for candidate to be compared\n        Returns\n        -------\n        errMsg : str\n            blank if matched, else mismatched requirements string\n        \"\"\"\n        \n        self.errMsg = self.errMsg + ' ' + self.experience.compare(candidateJobClass.experience)\n        self.errMsg = self.errMsg + ' ' + self.license.compare(candidateJobClass.license)\n        return self.errMsg.strip()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def checkRequirements(data, candidateJobClass):\n    \"\"\"Matches the requirements of the job class with supplied candidate job class data\n    \n    Parameters\n    ----------\n    data : DataFrame\n        job class data with the data_dictionary fields (requirements to match to be eligible for the promotion)\n    candidateJobClass : JobClass\n        candidate data with which the requirements will be matched\n    \n    Returns\n    -------\n    list [errMsg, conj]\n        a row for each requirement in data\n        errMsg - blank if matches the requirements, else contains not matched requirement texts\n        conj - conjuction to be checked with other requirements (i.e.or/and)\n    \"\"\"\n    \n    conj = ''\n    result = []\n\n    for index,row in data.iterrows():\n        conj = row['REQUIREMENT_CONJ']\n        if conj == '':\n            conj = 'and'\n        \n        jobClass = JobClass(row)\n        errMsg = jobClass.compare(candidateJobClass)\n        result.append([errMsg, conj])\n    return result\n\ndef matchRequirements(requirementsResult):\n    \"\"\"Applies conjuctions on multiple requirements(checkRequirements result) \n    \n    Parameters\n    ----------\n    requirementsResult : list\n        [errMsg, conj]\n        errMsg - blank for matched requirements, else contains not matched requirement texts\n        conj - conjuction to be checked with other requirements (i.e.or/and)\n    \n    Returns\n    -------\n    errMsg, conj\n        errMsg - blank if all requirements matched, else ';' separated message for all unmatched requirements\n        conj - last conjuction to be matched with other requirements(if any). Will be used if first called for sub-requirements\n    \"\"\"\n    \n    resultErrList = [] \n    resultRequirementsMatch = False\n    conj = ''\n    for row in requirementsResult:\n        errMsg = row[0]\n        conj = row[1]\n        currentRequirementsMatch = False\n        if len(errMsg)==0:\n            currentRequirementsMatch = True\n        else:\n            resultErrList.append(errMsg)\n            currentRequirementsMatch = False\n        if conj=='or':\n            if not resultRequirementsMatch:\n                if currentRequirementsMatch:\n                    resultErrList = []\n                    resultRequirementsMatch = True\n        elif conj=='and':\n            if not currentRequirementsMatch:\n                resultRequirementsMatch = False\n        else:\n            resultRequirementsMatch = currentRequirementsMatch\n    return ';'.join(resultErrList),conj\n\ndef checkEligibility(job_title, job_class_df, candidateDataDf):\n    \"\"\"For a job title, compares all the requirements for all the explicitly mentioned promotions in job class dataframe.\n    \n    Prints messages based on requirements match/mismatch.\n    \n    Parameters\n    ----------\n    job_title : str\n        job title of the candidate to be searched and matched with the requirements\n    \"\"\"\n    \n    job_title = job_title\n    single_job_class_df = job_class_df[job_class_df['JOB_CLASS_TITLE'] == job_title]\n    single_job_class_df = single_job_class_df.iloc[::-1] #reverse the dataframe\n    candidate_job_class = JobClass(candidateDataDf.iloc[0])\n    prevRqmntId = ''\n    result = []\n    i = 0\n    for index,row in single_job_class_df.iterrows(): \n        rqmntId = ''\n        if row['REQUIREMENT_SUBSET_ID'] != '':\n            rqmntId = row['REQUIREMENT_SET_ID']\n            if prevRqmntId == '':\n                prevRqmntId = rqmntId\n                data = single_job_class_df[single_job_class_df['REQUIREMENT_SET_ID'] == rqmntId]\n                rqmntChk = checkRequirements(data, candidate_job_class)\n                errMsg,conj = matchRequirements(rqmntChk)\n                conj = data['REQUIREMENT_CONJ'].iloc[0]\n                result.append([errMsg,conj])\n            if rqmntId != prevRqmntId:\n                prevRqmntId = rqmntId  \n                data = single_job_class_df[single_job_class_df['REQUIREMENT_SET_ID'] == rqmntId]\n                rqmntChk = checkRequirements(data, candidate_job_class)\n                errMsg,conj = matchRequirements(rqmntChk)\n                conj = data['REQUIREMENT_CONJ'].iloc[0]\n                result.append([errMsg,conj])\n        else:\n            rqmntId = ''\n            data = pd.DataFrame(row).transpose()\n            rqmntChk = checkRequirements(data, candidate_job_class)\n            errMsg,conj = rqmntChk[0][0],rqmntChk[0][1]\n            result.append([errMsg,conj])\n    #print(result)\n    errMsg,conj = matchRequirements(result)\n    if errMsg.strip() == '':\n        print('Candidate is eligible for promotion for job class ' + job_title + '\\n')\n    else:\n        print('For job class '+ job_title +' you need to fulfill below requirements: ')\n        print(errMsg + '\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def getTreeFormattedChildren(df, title):\n    if title=='':\n        print('No job title provided')\n        return\n    resultList = []\n    result = {}\n    result['name'] = str(title.title())\n    children = df[df['name']==title]['children'].values\n    if len(children)>0:\n        for child in children:\n            resultList.append(getTreeFormattedChildren(df, child))\n        result['children'] = resultList\n    else:\n        return result\n    return result\n\ndef getTreeHTML(data, title):\n    treeHtml = \"\"\"\n    <!DOCTYPE html>\n    <meta charset=\"utf-8\">\n    <style>\n    .node circle {\n      fill: #fff;\n      stroke: steelblue;\n      stroke-width: 3px;\n    }\n\n    .node text { font: 8px sans-serif; }\n\n    .node--internal text {\n      text-shadow: 0 1px 0 #fff, 0 -1px 0 #fff, 1px 0 0 #fff, -1px 0 0 #fff;\n    }\n\n    .link {\n      fill: none;\n      stroke: #ccc;\n      stroke-width: 2px;\n    }\n    </style>\n    <body>\n    <div style=\"font: 30px sans-serif; color: #42374D\"><b>Promotional Paths</b></div>\n    <div id=\"fd\"></div>\n    <div style=\"font: 16px sans-serif; color: #42374D\"><b>Above graph is interacitve. Click on nodes to view more detailed paths\n    </b>\n    </div>\n    <script>\n    require.config({\n        paths: {\n            d3: \"https://d3js.org/d3.v4.min\"\n         }\n     });\n\n    require([\"d3\"], function(d3) {\n    var treeData =\n      \"\"\"+json.dumps(getTreeFormattedChildren(data, title))+\"\"\"\n\n    // Set the dimensions and margins of the diagram\n    var margin = {top: 20, right: 90, bottom: 30, left: 90},\n        width = 960 - margin.left - margin.right,\n        height = 700 - margin.top - margin.bottom;\n\n    // append the svg object to the body of the page\n    // appends a 'group' element to 'svg'\n    // moves the 'group' element to the top left margin\n    var svg = d3.select(\"#fd\").append(\"svg\")\n        .attr(\"width\", width + margin.right + margin.left)\n        .attr(\"height\", height + margin.top + margin.bottom)\n      .append(\"g\")\n        .attr(\"transform\", \"translate(\"\n              + margin.left + \",\" + margin.top + \")\");\n\n    var i = 0,\n        duration = 750,\n        root;\n\n    // declares a tree layout and assigns the size\n    var treemap = d3.tree().size([height, width]);\n\n    // Assigns parent, children, height, depth\n    root = d3.hierarchy(treeData, function(d) { return d.children; });\n    root.x0 = height / 2;\n    root.y0 = 0;\n\n    // Collapse after the second level\n    root.children.forEach(collapse);\n\n    update(root);\n\n    // Collapse the node and all it's children\n    function collapse(d) {\n      if(d.children) {\n        d._children = d.children\n        d._children.forEach(collapse)\n        d.children = null\n      }\n    }\n\n    function update(source) {\n\n      // Assigns the x and y position for the nodes\n      var treeData = treemap(root);\n\n      // Compute the new tree layout.\n      var nodes = treeData.descendants(),\n          links = treeData.descendants().slice(1);\n\n      // Normalize for fixed-depth.\n      nodes.forEach(function(d){ d.y = d.depth * 180});\n\n      // ****************** Nodes section ***************************\n\n      // Update the nodes...\n      var node = svg.selectAll('g.node')\n          .data(nodes, function(d) {return d.id || (d.id = ++i); });\n    \n      // Enter any new modes at the parent's previous position.\n      var nodeEnter = node.enter().append('g')\n          .attr('class', 'node')\n          .attr(\"transform\", function(d) {\n            return \"translate(\" + source.y0 + \",\" + source.x0 + \")\";\n        })\n        .on('click', click);\n\n      // Add Circle for the nodes\n      nodeEnter.append('circle')\n          .attr('class', 'node')\n          .attr('r', 1e-6)\n          .style(\"fill\", function(d) {\n              return d._children ? \"lightsteelblue\" : \"#fff\";\n          });\n\n      // Add labels for the nodes\n      nodeEnter.append('text')\n          .attr(\"dy\", \".35em\")\n          .attr(\"x\", function(d) {\n              return d.children || d._children ? -13 : 13;\n          })\n          .attr(\"text-anchor\", function(d) {\n              return d.children || d._children ? \"end\" : \"start\";\n          })\n          .text(function(d) { return d.data.name; });\n\n      // UPDATE\n      var nodeUpdate = nodeEnter.merge(node);\n\n      // Transition to the proper position for the node\n      nodeUpdate.transition()\n        .duration(duration)\n        .attr(\"transform\", function(d) { \n            return \"translate(\" + d.y + \",\" + d.x + \")\";\n         });\n\n      // Update the node attributes and style\n      nodeUpdate.select('circle.node')\n        .attr('r', 10)\n        .style(\"fill\", function(d) {\n            return d._children ? \"lightsteelblue\" : \"#fff\";\n        })\n        .attr('cursor', 'pointer');\n\n\n      // Remove any exiting nodes\n      var nodeExit = node.exit().transition()\n          .duration(duration)\n          .attr(\"transform\", function(d) {\n              return \"translate(\" + source.y + \",\" + source.x + \")\";\n          })\n          .remove();\n\n      // On exit reduce the node circles size to 0\n      nodeExit.select('circle')\n        .attr('r', 1e-6);\n\n      // On exit reduce the opacity of text labels\n      nodeExit.select('text')\n        .style('fill-opacity', 1e-6);\n\n      // ****************** links section ***************************\n\n      // Update the links...\n      var link = svg.selectAll('path.link')\n          .data(links, function(d) { return d.id; });\n\n      // Enter any new links at the parent's previous position.\n      var linkEnter = link.enter().insert('path', \"g\")\n          .attr(\"class\", \"link\")\n          .attr('d', function(d){\n            var o = {x: source.x0, y: source.y0}\n            return diagonal(o, o)\n          });\n\n      // UPDATE\n      var linkUpdate = linkEnter.merge(link);\n\n      // Transition back to the parent element position\n      linkUpdate.transition()\n          .duration(duration)\n          .attr('d', function(d){ return diagonal(d, d.parent) });\n\n      // Remove any exiting links\n      var linkExit = link.exit().transition()\n          .duration(duration)\n          .attr('d', function(d) {\n            var o = {x: source.x, y: source.y}\n            return diagonal(o, o)\n          })\n          .remove();\n\n      // Store the old positions for transition.\n      nodes.forEach(function(d){\n        d.x0 = d.x;\n        d.y0 = d.y;\n      });\n\n      // Creates a curved (diagonal) path from parent to the child nodes\n      function diagonal(s, d) {\n\n        path = `M ${s.y} ${s.x}\n                C ${(s.y + d.y) / 2} ${s.x},\n                  ${(s.y + d.y) / 2} ${d.x},\n                  ${d.y} ${d.x}`\n\n        return path\n      }\n\n      // Toggle children on click.\n      function click(d) {\n        if (d.children) {\n            d._children = d.children;\n            d.children = null;\n          } else {\n            d.children = d._children;\n            d._children = null;\n          }\n        update(d);\n      }\n    }\n    });\n    </script>\n    </body>\n    \"\"\"\n    return treeHtml\n\ndef showInteractivePromotionalPaths(df_edges, job_title):\n    treeEdges = getEdges(data=df_edges, job_title=job_title)\n    edges = []\n    for edge in treeEdges:\n        edges.append((edge[0][0],edge[0][1]))\n    \n    treeEdges = pd.DataFrame(edges, columns=['name','children'])\n    treeEdges = treeEdges.groupby(['name','children']).size().reset_index(name='Freq')\n    return (getTreeHTML(treeEdges, job_title))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_edges = createEdges(job_class_df)\nHTML(showInteractivePromotionalPaths(df_edges, 'WATER UTILITY WORKER'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Static graph visualisation and an attempt to match candidate's qualifications with the available promotional title's requirements.</b>\n\nFor now, only experience and license details are compared for a candidate with the available promotional title's respective details.\n\nExamples and logic for this application is explained in [this kernel](https://www.kaggle.com/tyagit3/dsfg-cityofla-analysis-and-solution-application-4)."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Test/Sample data for CandidateJobClass\ndata = {'JOB_CLASS_TITLE': 'WATER UTILITY WORKER',\n        'EXP_JOB_CLASS_TITLE': 'WATER UTILITY WORKER',\n        'EXPERIENCE_LENGTH': '3',\n        'EXPERIENCE_LEN_UNIT': 'years',\n        'FULL_TIME_PART_TIME': 'FULL_TIME',\n        'PAID_VOLUNTEER': 'PAID',\n        'DRIVERS_LICENSE_REQ': 'R',\n        'DRIV_LIC_TYPE': '',\n        'ADDTL_LIC': 'NA'}\ncandidate_job_class_df = pd.DataFrame(data=data, index=[0])\ndf_edges = createEdges(job_class_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"showPromotionalPaths(df_edges, job_class_df, '', candidate_job_class_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='5'></a>\n<h2><b>5. Applications</b></h2>\n\nBelow are the sample run for applications shown above.\n\n<a id='5.1'></a>\n<h3><b>5.1 Application 1</b></h3>\n<br>\n\nRecommends similar job titles based on requirements/job duties\n<br>Can be used to cluster jobs to help possible applicants to be able to view/apply for all applicable jobs\n<br><br>\n<b>Required configurations to execute</b>:<br>\n>--import libraries\n<br>--set master configurations\n<br>headingsFrame = createHeadingsFrame()\n<br>requirements = getRequirements(headingsFrame)\n<br>requirements_s_matrix = build_similarity_matrix(requirements['REQUIREMENT_WORDS'], STOP_WORDS)\n\n<br>\n<b>Command to execute</b>: \n>showSimilarJobs(requirements,'ANIMAL KEEPER 4304 083118.txt','FILE_NAME', requirements_s_matrix, 3)\n\n<br><br>\n<b>Sample Run</b>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Requirements based\nshowSimilarJobs(requirements,'ANIMAL KEEPER 4304 083118.txt','FILE_NAME', requirements_s_matrix, 3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='5.2'></a>\n<h3><b>5.2 Application 2</b></h3>\n<br>\nCalculates and returns <b>Flesch Reading Ease Score</b> for a given text.\n<br>Can be used to check readability levels for the bulletins and modify them accordingly.\n<br><br>\n<b>Required configurations to execute</b>:<br>\n\n>--import libraries\n<br>--set master configurations\n<br>text = 'sample text to get reading ease score'\n\n<br>\n<b>Command to execute</b>: <br>\n>ease_score, grade_level, ease_score_based_grade_level = flesch_reading_ease_grade(text)\n\n<br><br>\n<b>Sample Run</b>"},{"metadata":{"trusted":true},"cell_type":"code","source":"text = jobs_df['Content'].values[0] #sample bulletin text\nease_score, grade_level, ease_score_based_grade_level = flesch_reading_ease_grade(text)\nprint('Reading Ease Score for ' + jobs_df['FileName'].values[0] + ' bulletin is ' + str(ease_score))\nprint('Reading Ease Score based grade level for the bulletin is ' + ease_score_based_grade_level)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[An extension to Application 2 & 3](https://www.kaggle.com/tyagit3/dsfg-cityofla-biasedness-readability) that generates different reading scores and biasedness check for individual bulletins.\n\n<a id='5.3'></a>\n<h3><b>5.3 Application 3</b></h3>\n<br>\n    Check and print masculine and feminine coded words in a text.\n<br>Can be used to check existence of gender biasedness in a bulletin.\n<br><br>\n<b>Required configurations to execute</b>:<br>\n>--import libraries\n<br>--set master configurations, modify list of masculine,feminine coded words if required\n<br>text = 'sample text to check gender biasedness'\n\n<br>\n<b>Command to execute</b>: <br>\n>biasednessResultCode = assessBias(text)\n\n<br><br>\n<b>Sample run</b>"},{"metadata":{"trusted":true},"cell_type":"code","source":"text = jobs_df['Content'].values[0]\nbiasednessResultCode = assessBias(text)\nprint('\\nJob bulletin text for \"' + jobs_df['FileName'].values[0]+ '\" file is ' + biasednessResultCode)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='5.4'></a>\n<h3><b>5.4 Application 4</b></h3>\n<br>Check for available promotional paths for a candidate/job_title.\n<br>Prints messages for unfulfilled requirement sets for available promotion job titles.\n<br><br>\n<b>Required configurations to execute</b>:<br>\n>--import libraries\n<br>--set master configurations\n<br>data = {'JOB_CLASS_TITLE': 'WATER UTILITY WORKER',\n<br>&nbsp;&nbsp;&nbsp;&nbsp;'EXP_JOB_CLASS_TITLE': 'WATER UTILITY WORKER',\n<br>&nbsp;&nbsp;&nbsp;&nbsp;'EXPERIENCE_LENGTH': '1',\n<br>&nbsp;&nbsp;&nbsp;&nbsp;'EXPERIENCE_LEN_UNIT': 'years',\n<br>&nbsp;&nbsp;&nbsp;&nbsp;'FULL_TIME_PART_TIME': 'FULL_TIME',\n<br>&nbsp;&nbsp;&nbsp;&nbsp;'PAID_VOLUNTEER': 'PAID',\n<br>&nbsp;&nbsp;&nbsp;&nbsp;'DRIVERS_LICENSE_REQ': 'R',\n<br>&nbsp;&nbsp;&nbsp;&nbsp;'DRIV_LIC_TYPE': '',\n<br>&nbsp;&nbsp;&nbsp;&nbsp;'ADDTL_LIC': 'NA'}\n<br>--sample candidate job class data\n<br>candidate_job_class_df = pd.DataFrame(data=data, index=[0])   \n<br>edges for graph\n<br>df_edges = createEdges(job_class_df) \n\n\n<br>\n<b>Command to execute</b>:\n>showPromotionalPaths(df_edges, job_class_df, '', candidate_job_class_df)\n\n<br><br>\n<b>Sample run</b>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Test CandidateJobClass\ndata = {'JOB_CLASS_TITLE': 'WATER UTILITY WORKER',\n        'EXP_JOB_CLASS_TITLE': 'WATER UTILITY WORKER',\n        'EXPERIENCE_LENGTH': '1',\n        'EXPERIENCE_LEN_UNIT': 'years',\n        'FULL_TIME_PART_TIME': 'FULL_TIME',\n        'PAID_VOLUNTEER': 'PAID',\n        'DRIVERS_LICENSE_REQ': 'R',\n        'DRIV_LIC_TYPE': '',\n        'ADDTL_LIC': 'NA'}\ncandidate_job_class_df = pd.DataFrame(data=data, index=[0])\ndf_edges = createEdges(job_class_df)\nshowPromotionalPaths(df_edges, job_class_df, '', candidate_job_class_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<br>\n<a id='6'></a>\n<h2><b>6. Extra Notes</b></h2>\n- Good wages are important, but so are good benefits. Benefits aren't just desirable to employees. They can improve your business. Adding benefits can increase loyalty, focus and productivity, attendance, and recruiting.\n**Maximum job bulletins have a link to benefit section** but is not clearly visible in a job post. **Including/highlighting benefits delivered in the post itself** can highly boost diverse applicant pool. **Let candidates know what you offer.**\n\nHere is an [analysis by textio.ai](https://textio.ai/awesomeness-850c72d5cf8) done on serveral words affecting job posts for different cities (**City of LA** is one of them).\n\n<img src='https://i.imgur.com/8NMf3mH.png'>\n\nAlso, according to a research of textio <b>Los Angeles, CA</b> is one of the cities, where “super” in a job post predicts slow time to fill. <br>To read more about other cities [visit this link.](https://textio.ai/super-cities-3a38e1718d85)\n\n<br><br>\nThanks to **Kaggle** for giving the opportunity to participate in something **Good**. Hope City of LA find something helpful from this Kernel.\n\n[An extension to Application 2 & 3](https://www.kaggle.com/tyagit3/dsfg-cityofla-biasedness-readability) that generates different reading scores and checks for gender biasedness in bulletins text.<br>\n[An extension to Application 4](https://www.kaggle.com/tyagit3/dsfg-cityofla-analysis-and-solution-application-4) - An application to check promotions and view promotional paths."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}