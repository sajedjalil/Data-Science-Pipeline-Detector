{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Data Science for Good: City of Los Angeles"},{"metadata":{},"cell_type":"markdown","source":"## Problem statement\nThe goal is to convert a folder full of plain-text job postings into a single structured CSV file and then to use this data to:\n1.  identify language that can negatively bias the pool of applicants; \n2.  improve the diversity and quality of the applicant pool; and/or \n3.  make it easier to determine which promotions are available to employees in each job class."},{"metadata":{},"cell_type":"markdown","source":"## A General Guide to This Notebook\nThis notebook in organized as the following:\n* Import Libraries: All libraries required to run the notebook imports here.\n* Structured Dataset Building: The methodology and code to convert raw data into a stuctured dataset for following data analysis. The final output dataframe gives us a clear possibility to all promotion path of job classes. Each row in the dataframe refers to a set of minimun requirements need to be satisfied for a particular job class title.\n* Data Analysis & Stats: Explore the dataset we get from previous step, and try to see if we can get some interesting insights from it.\n* Text analysis for recommandation: Here we'll try to anwser the questions in our problem statement.\n* Summary & Recommendation"},{"metadata":{},"cell_type":"markdown","source":"## Importing Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom pandas import Series, DataFrame\nimport re\nimport os\nimport numpy as np\nfrom datetime import datetime\nfrom collections  import Counter\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.pyplot import figure\nimport math\nimport warnings\nwarnings.filterwarnings('ignore')# Don't show warnings\n%matplotlib inline ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Structured Dataset Building"},{"metadata":{},"cell_type":"markdown","source":"### We first take a look of all data in folder \"CityofLA\""},{"metadata":{"trusted":true},"cell_type":"code","source":"bulletins=os.listdir(\"../input/cityofla/CityofLA/Job Bulletins/\")\nadditional=os.listdir(\"../input/cityofla/CityofLA/Additional data/\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"bulletins","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"We have {len(bulletins)} .txt files in folder 'Job Bulletins'!\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"additional","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Raw data in hands\n- CityofLA/Job Bulletins: A folder that contains 683 job bulletins in text format.\n- CityofLA/Additional data/sample job class export template.csv: **Looks like our final structure dataset should look something like this.**\n- CityofLA/Additional data/kaggle_data_dictionary.csv: **Fields value decription of in the structure dataset.**\n- CityofLA/Additional data/job_titles.csv: **List of job class titles in LA City.**\n- CityofLA/Additional data/PDFs: A folder that contains job bulletins in PDF format organized by date subfolders.\n- CityofLA/Additional data/job bulletins with annotations: A folder that contains some detail decriptions of job bulletins.\n- CityofLA/Additional data/City Job Paths: A folder that contains 147 PDF files that decribes the promotion relationships between jobs and the type of jobs that are entry/open level."},{"metadata":{},"cell_type":"markdown","source":"### Output Example"},{"metadata":{"trusted":true},"cell_type":"code","source":"template = pd.read_csv(\"../input/cityofla/CityofLA/Additional data/sample job class export template.csv\")\nprint(template.shape)\ntemplate","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"template.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 25 features to be extract from the text file. Let's check the example text file \"SYSTEMS ANALYST 1596 102717.txt\" and see where is the information."},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"with open(\"../input/cityofla/CityofLA/Job Bulletins/SYSTEMS ANALYST 1596 102717.txt\") as f:\n    print(f.read())\n    pass","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like a text have serveral sections with a **all-uppercase-letter heading**, most field value could be captured by searching key words, but it seems that a lot of work needs to be done in the \"REQUIREMENT\" section."},{"metadata":{},"cell_type":"markdown","source":"### Structured Dataset Building"},{"metadata":{},"cell_type":"markdown","source":"Generally, we could use regex (re module in python) to help use extract the information we want from the text files. From the previous conclusion we see that all headings and job title in the raw text file are all uppercase letters. We could use this feature to help divide the text into subtext for more complex parsing or just search for the information we want. "},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"fail_list = []\nheading = set()\n# Check all files\nfor file in bulletins:\n    with open(\"../input/cityofla/CityofLA/Job Bulletins/\"+file) as f:\n        try:\n            #Clear all '\\t', '\\r', for better result visulization\n            text = f.read().replace('\\t','').replace('\\r', '')\n            lines = text.split('\\n')\n            while len(lines[0]) == 0:\n                # Exclude empty strings\n                lines.pop(0)\n            # We know the first line that is not emtpy string and all uppercase letter is the job title\n            # Exclude the job title to see the headings of each sections\n            lines.pop(0) \n            heads = set([head for head in lines if head.isupper()])\n            heading = heading.union(heads)\n            \n        except Exception as e:\n            fail_list.append(file)\n        \nheading","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Quick check of the headings, it shows that there are slightly different expressions of the equivalent meaning, such as extra spaces at the front, extra spaces at the end, w/ or w/o plural and different vocabulary expressions."},{"metadata":{"trusted":true},"cell_type":"code","source":"fail_list","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Among 683 files only one file has problem opening it, which sounds accetable for now."},{"metadata":{},"cell_type":"markdown","source":"### Determine the patterns to extract the feature values.\nAccording to the description in \"kaggle_data_dictionary.csv\", we need to extract information for **25 fields**: \n\nFILE_NAME, JOB_CLASS_TITLE, JOB_CLASS_NO, REQUIREMENT_SET_ID,  REQUIREMENT_SUBSET_ID, JOB_DUTIES, EDUCATION_YEARS, SCHOOL_TYPE, EDUCATION_MAJOR, EXPERIENCE_LENGTH, FULL_TIME_PART_TIME, EXP_JOB_CLASS_TITLE, EXP_JOB_CLASS_ALT_RESP, EXP_JOB_CLASS_FUNCTION, COURSE_COUNT, COURSE_LENGTH, COURSE_SUBJECT, MISC_COURSE_DETAILS, DRIVERS_LICENSE_REQ, DRIV_LIC_TYPE, ADDTL_LIC, EXAM_TYPE, ENTRY_SALARY_GEN, ENTRY_SALARY_DWP, OPEN_DATE.\n\nAmong the above **25 fields**, there are **14 fields** extract its' values from **\"REQUIREMENTS\" subtext**:\n\nREQUIREMENT_SET_ID,  REQUIREMENT_SUBSET_ID, EDUCATION_YEARS, SCHOOL_TYPE, EDUCATION_MAJOR, EXPERIENCE_LENGTH, FULL_TIME_PART_TIME, EXP_JOB_CLASS_TITLE, EXP_JOB_CLASS_ALT_RESP, EXP_JOB_CLASS_FUNCTION, COURSE_COUNT, COURSE_LENGTH, COURSE_SUBJECT, MISC_COURSE_DETAILS.\n\nAnd the **14 fields** related to REQUIRMENTS could be classified into three kinds: \n* **Education background:** EDUCATION_YEARS, SCHOOL_TYPE, EDUCATION_MAJOR.\n* **Previous job experience:** EXPERIENCE_LENGTH, FULL_TIME_PART_TIME, EXP_JOB_CLASS_TITLE, EXP_JOB_CLASS_ALT_RESP, EXP_JOB_CLASS_FUNCTION.\n* **Coueses Taken:** COURSE_COUNT, COURSE_LENGTH, COURSE_SUBJECT, MISC_COURSE_DETAILS.\n\nDue to the difficulty in extracting REQUIREMENTS related fields,there are serveral funtions written to help parse the subtext \"REQUIREMENTS\". **These functions are closely coupled with the main code**, they **utilize global variables df and i, which stands for the final output dataframe (a structed dataset) and the current row index of the dataframe.**"},{"metadata":{},"cell_type":"markdown","source":"### And the following code blocks is organized as:\n#### Functions that support parsing the REQUIREMENTS text\n- parse_course(text): Takes in a string and write COURSE_COUNT, COURSE_LENGTH, COURSE_SUBJECT and MISC_COURSE_DETAILS to the dataframe.\n- length_in_year(text): Takes in a string and returns a float that indicates the length in years. It is used in parse_job(text) and parse_edu(text).\n- def parse_job(text): Takes in a string and write EXPERIENCE_LENGTH, FULL_TIME_PART_TIME, EXP_JOB_CLASS_TITLE, EXP_JOB_CLASS_ALT_RESP and EXP_JOB_CLASS_FUNCTION to the dataframe.\n- parse_edu(text): Takes in a string and write EDUCATION_YEARS, SCHOOL_TYPE and EDUCATION_MAJOR to the dataframe.\n- parse_check(text): Takes in a string and returns a int that indicates whay which parse type function should be called to parse the text content. Is used in parse_helper().\n- parse_helper(text): Takes in a string and select a parse type function, either parse_edu(), parse_job() or parse_course() should be called to parse the text. Separate the text into piecies if multi types are detected.\n- parse_sub_req(text, is_and): Takes in a string text and boolean (indicates conjunction correlation with the previous item) and devide a requirement set into sub requirement sets. Determine the conjunciton (and/or) realtionship between sub sets. Assgin REQUIREMENT_SUBSET_ID value to dataframe.\n- parse_req(text): Takes in a string text and divide it into requirement sets and determine the parsing sequence of the sets, assign REQUIREMENT_SET_ID value to the dataframe.\n\n#### Main code\nThe code that generate a dataframe (variable name: df), the final structured data set for data analysis. Other field value is that is not in the REQUIREMENT text is determined here."},{"metadata":{},"cell_type":"markdown","source":"Functions that support parsing the REQUIREMENTS text. (These functions need to be run before running the main code block!)"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def parse_course(text):\n    '''\n    Takes in a string text and doesn't return any value. Complete the following tasks:\n    1. Get COURSE_COUNT value by calling length_in_year()\n    2. Get COURSE_LENGTH value\n    3. Get COURSE_SUBJECT value\n    4. Get MISC_COURSE_DETAILS value\n    '''\n\n    global df\n    global i\n    \n    # Get COURSE_COUNT\n    count_dict = {'one':1, 'two':2,'three':3, 'four':4, 'five':5,\n                 'six':6, 'seven':7 , 'eight':8, 'nine':9, 'ten':10}\n    count = re.compile(r\"(one|two|three|four|five|six|seven|eight|nine|ten) course\",re.I).search(text)\n    if count:\n        df.at[i,'COURSE_COUNT'] = count_dict[count.group(1).lower()]\n    else:\n        count = re.compile(r\"(\\d+) course\",re.I).search(text)\n        if count:\n            df.at[i,'COURSE_COUNT'] = count.group(1)\n            \n    # Get COURSE_LENGTH\n    semester = re.compile(r\" semesters?\\W\",re.I).search(text)\n    quarter  = re.compile(r\" quarters?\\W\",re.I).search(text)\n    course_lenth = ''\n    if quarter:\n        quarter = re.compile(r\"(one|two|three|four|five|six|seven|eight|nine|ten) quarter\",re.I).search(text)\n        if quarter:\n            course_lenth = str(count_dict[quarter.group(1).lower()])+'Q'\n        else:\n            quarter = re.compile(r\"(\\d+) quarters?\",re.I).search(text)\n            if quarter:\n                 course_lenth = str(quarter.group(1))+'Q'\n    if semester:\n        semester = re.compile(r\"(one|two|three|four|five|six|seven|eight|nine|ten) semester\",re.I).search(text)\n        if semester:\n            course_lenth += '|'+str(count_dict[semester.group(1).lower()])+'S'\n        else:\n            semester = re.compile(r\"(\\d+) semesters?\",re.I).search(text)\n            if semester:\n                 course_lenth += '|'+str(semester.group(1))+'S'\n                    \n    df.at[i,'COURSE_LENGTH'] = course_lenth\n    \n    # Get COURSE_SUBJECT\n    # Statements for requirement substitution will put into MISC_COURSE_DETAILS\n    check1 = re.compile(r\"course\\w* is equivalent to\").search(text)\n    if check1 or \"substitute\" in text:\n        course_text = re.compile(r\"\\s(\\w\\. )?([\\w, ()'/\\.-]+)\",re.I).search(text)\n        if course_text:\n            df.at[i,'MISC_COURSE_DETAILS'] = course_text.group(2)   \n            return\n    \n   \n    \n    # Keep updating possible endings\n    course_text = text.replace(', from', '.').replace(' from', '.').replace(', and', '.').replace('; or', '.')\n    course_text = course_text.replace(' and/or ', ', ').replace(', or ',' or ').replace(' or ', ', ') \n    # Refining text correlated to course subject\n    with_text = re.compile(r\" with .+\").search(course_text)\n    if with_text:\n        course_text = with_text.group()\n    course1 = re.compile(r\" follow[\\w ]+:\\s*([\\w\\(\\); ,'-]+)\").search(course_text)\n    course2 = re.compile(r\" in ([\\w\\(\\) ,'-]+)\").search(course_text)\n    course3 = re.compile(r\"units of ([\\w\\(\\) ,'-]+?) course\").search(course_text)\n\n    if course1:\n        course_text = course1.group(1).replace('; and ', ', ').replace('; ', ', ')\n    elif course2:\n        course_text = course2.group(1)\n    elif course3:\n        course3 = re.compile(r\" units of ([\\w\\(\\) ,'-]+?) course\").findall(course_text)\n        course_list = []\n        for x in course3:\n            course_list += x.split(\", \")\n        df.at[i,'COURSE_SUBJECT'] = \"|\".join([c.upper() for c in course_list])\n        return\n    \n    else:\n        course_text = re.compile(r\"\\s(\\w\\. )?([\\w, ()'/\\.-]+)\",re.I).search(text)\n        if course_text:\n            df.at[i,'MISC_COURSE_DETAILS'] = course_text.group(2)  \n        else:\n            print(\"ERROR! Need a better course subject pattern! File:\" ,file)\n            print(\"TEXT:\", text)\n        return\n    \n    # Remove strings inside paranthesis\n    while '(' in course_text and ')' in course_text :\n        course_text = course_text[:course_text.find('(')-1] + course_text[course_text.find(')')+1:]\n    \n    #subject = re.compile(r\"([A-Z][a-z]+( [aA]nd| [Oo]f)?( [A-Z][a-z]+)*)\").findall(text)\n    subject = re.compile(r\"([A-Z][a-z']+(( [aA]nd\\W| [Oo]f\\W)? ?[A-Z]+[a-z']*)*)\").findall(course_text)\n\n    subject_list = set() #to exclude duplicated records       \n    for x, y, z in subject:#two parenthesis indicates two group, but we only want the first one\n        if 'Los ' not in x:\n            subject_list.add(x)\n\n    if len(subject_list) != 0:\n        subject_list = list(subject_list)\n        df.at[i,'COURSE_SUBJECT'] = subject_list[0].upper() \n        for ind in range(1,len(subject_list)):\n            df.at[i,'COURSE_SUBJECT'] += '|'+ subject_list[ind].upper()\n    else:\n        subject_list = course_text.split(', ')\n        df.at[i,'COURSE_SUBJECT'] = subject_list[0]\n        for ind in range(1,len(subject_list)):\n            df.at[i,'COURSE_SUBJECT'] += '|' + subject_list[ind]\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def length_in_year(text):\n    '''\n    Takes in a string text, Returns a float that indicates the length of period in year, returns None if not found.\n    '''\n\n    num_word = {'one':1, 'two':2,'three':3, 'four':4, 'five':5,'six':6, 'seven':7 , 'eight':8, 'nine':9, 'ten':10,\n                'eleven':11,'twelve':12,'thirteen':13,'fourteen':14, 'fifteen':15, 'sixteen':16,'seventeen':17,\n                'eighteen':18, 'nineteen':19,'twenty':20,'thirty':30} \n    \n    # Try find word expression of year length first\n    year = re.compile(r\"(one|two|three|four|five|six|seven|eight|nine|ten|eleven|twelve|thirteen|fourteen|fifteen|sixteen|seventeen|eighteen|nineteen|twenty|thirty)[\\w\\(\\) -]*year\",re.I).search(text)\n    if year:\n        return float(num_word[year.group(1).lower()])\n    else:\n        # Try find digit expression of year\n        year = re.compile(r\"(\\d+(\\.\\d)?)[\\w ]*year\",re.I).search(text)\n        if year:\n            return float(year.group(1))\n        else:\n            # Try find digit expression of month\n            month = re.compile(r\"(\\d+(\\.\\d)?)[\\w ]*month\",re.I).search(text)\n            if month:\n                return float(month.group(1))/12\n            else:\n                # Try find word expression of month\n                month = re.compile(r\"(one|two|three|four|five|six|seven|eight|nine|ten|eighteen|thirty)[\\w\\(\\) -]*month\",re.I).search(text)\n                if month:\n                    return float(num_word[month.group(1).lower()])/12\n    \n    return None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Import the job title list needed for \"EXP_JOB_CLASS_TITLE\".\njob_title = pd.read_csv(\"../input/cityofla/CityofLA/Additional data/job_titles.csv\", header=None)\njob_titles = set(job_title[0])\n\n# General role name\njob_alt_class = ['accountant','advisor','administrator','advocate','agent','aide','analyst','aquarist','architect',\n                'archivist','assistant', 'associate','attendant','auditor','apprentice','biologist','blacksmith','boilermaker',\n                'buyer', 'captain','caretaker','chemist','chief','cleaner','commander','controller','coordinator',\n                'criminalist','curator','cabinetmaker','custodian','contractor','designer','detective','director', 'dispatcher','driver','educator',\n                'electrician','engineer','estimator','examiner','expert','fabricator','finisher','firefighter', 'gardener',\n                'geologist','guide','helper','hydrographer', 'hygienist','inspector','investigator', 'keeper','laborer',\n                'layer','librarian', 'lieutenant','locksmith','machinist','masonry','manager','mate','mechanic','microbiologist',\n                 'nurse','officer','operator','painter','photographer','pilot', 'pipefitter', 'planner','plumber', 'police',\n                'poster', 'preparator', 'programmer', 'provider', 'ranger',  'reader','registrar', 'repairer',\n                'representative','roofer', 'secretary','sergeant', 'setter','specialist', 'starter','storekeeper',\n                'superintendent','supervisor','surgeon','technician','tester','trainee','welder','wharfinger','worker']\n\ndef parse_job(text):\n    '''\n    Takes in a string text, No return value. Complete the following tasks:\n    1. Get EXPERIENCE_LENGTH value by calling length_in_year()\n    2. Get FULL_TIME_PART_TIME value: search for keywords like \"full-time\" or \"part-time\"\n    3. Get EXP_JOB_CLASS_TITLE value: search for proper nouns that matches the job title in \"job_titles.csv\"\n    4. Get EXP_JOB_CLASS_ALT_RESP value: search for proper nouns that is NOT in \"job_titles.csv\", or non-proper noun format forms.\n    5. Get EXP_JOB_CLASS_FUNCTION value: search for strings related to job function descriptions like \n       e.g., involved in the installation, modification, maintenance and repair of air conditioning,.....\n    \n    The relationship of getting values between EXP_JOB_CLASS_TITLE, EXP_JOB_CLASS_ALT_RESP, EXP_JOB_CLASS_FUNCTION:\n    \n    1. Generally this funciton first search for proper nouns that matches titles in the LA city \"job_titles.csv\", \n    if success, assign the matches to EXP_JOB_CLASS_TITLE with all UPPERCASE letters, here we assume for a single subitem \n    statement it is clear enough, so stop searching for alternative job title expression (EXP_JOB_CLASS_ALT_RESP) and\n    job function descriptions (EXP_JOB_CLASS_FUNCTION).\n    \n    2. If it can't find any EXP_JOB_CLASS_TITLE value, it will then try get EXP_JOB_CLASS_ALT_RESP\n    For EXP_JOB_CLASS_ALT_RESP, it first find proper noun format strings in particular positions, if NOT found, then \n    it tries to find lowercase letters in the correct position. \n    \n    3. If it can't find any EXP_JOB_CLASS_TITLE and EXP_JOB_CLASS_ALT_RESP values, it will also try get \n    EXP_JOB_CLASS_FUNCTION by seaching a particalur pattern expression.    \n    \n    '''\n    global df\n    global i\n\n    # Get EXPERIENCE_LENGTH value\n    if length_in_year(text):\n        df.at[i,'EXPERIENCE_LENGTH'] = length_in_year(text)\n        \n    # Get FULL_TIME_PART_TIME value. \n    ftpt = re.compile(r\"(part[- ]time|full[- ]time)\",re.I).search(text)\n    if ftpt:\n        if \"part\" in ftpt.group(1).lower():\n            df.at[i,'FULL_TIME_PART_TIME'] = \"PART-TIME\"\n        else:\n            df.at[i,'FULL_TIME_PART_TIME'] = \"FULL-TIME\"\n        \n\n    # Get EXP_JOB_CLASS_TITLE, EXP_JOB_CLASS_ALT_RESP, EXP_JOB_CLASS_FUNCTION value\n    # Words that likely to indicate job title ends. (Keep updating)\n    exp_text = text.replace(' with ', '.').replace(' or in ', '.').replace(' in ', '.').replace(' or an ', ', ')\n    exp_text = exp_text.replace(' perform', '.').replace(' engaged', '.').replace(' within ', '.')\n    exp_text = exp_text.replace('; and','.').replace('; or','.').replace('City of Los Angeles','city')\n\n    # Refine the text\n    exp_regex1 = re.compile(r\" at the level of (an? )?([\\w;,#/\\(\\) -]+)\", re.I).search(exp_text)\n    exp_regex2 =re.compile(r\" as (an? )?([\\w;,#/\\(\\) -]+)\", re.I).search(exp_text)\n\n    if exp_regex1:\n        exp_text = exp_regex1.group(2)\n\n    elif exp_regex2:\n        exp_text = exp_regex2.group(2)\n  \n    else:\n        # If no job title pattern match, get job function   \n        job_func1 = re.compile(r\" (in|in:) ([\\w,\\(\\)#/ -]+)[^; or\\s*][^; and\\s*]\", re.I).search(text)\n        job_func2 = re.compile(r\" (at|with) ([\\w,\\(\\)#/ -]+)[^; or\\s*][^; and\\s*]\", re.I).search(text)\n        job_func3 = re.compile(r\"([a-z]+ing [\\w,\\(\\)#/ -]+)[^; or\\s*][^; and\\s*]\", re.I).search(text)\n        if job_func1:\n            job_func_text = job_func1.group(2)\n        elif job_func2:\n            job_func_text = job_func2.group(2)\n        elif job_func3:\n            job_func_text = job_func3.group(1)\n        else:\n            # Grab the whole sentence\n            fulltext = re.compile(r\"\\n(\\w\\W) ([\\w, ()'/-]+)\",re.I).search(text)\n            if fulltext:# e.g, 1. either: or 2. One of the following:,....\n                df.at[i,'EXP_JOB_CLASS_FUNCTION'] = re.compile(r\"\\n(\\w\\W) ([\\w, ()'/-]+)\",re.I).search(text).group(2)\n            else:\n                while text[0] == '\\n' or text[0] == ' ':\n                    text = text[1:]\n                df.at[i,'EXP_JOB_CLASS_FUNCTION'] = text\n            return\n            \n        # If any job_func match, remove white spaces in front, assign value into dataframe\n        while job_func_text[0] == ' ':\n            job_func_text = job_func_text[1:]\n\n        df.at[i,'EXP_JOB_CLASS_FUNCTION'] = job_func_text\n        return\n\n    # If any exp_regex match, di the following\n    # Try grab proper noun format, e.g, \"Maintenance and Construction Helper\".\n    exp_title = re.compile(\"((Pre-)?[A-Z][a-z']+( [aA]nd\\W| [Oo]f\\W)?( ?(Pre-)?[A-Z]+[a-z']*)*)\").findall(exp_text)\n    \n    title_list = set()\n    for x,y,z,w,q in exp_title:\n\n        title_list.add(x)\n    \n    job_alt_list = set() # Holds possible alternative job titles if any\n    \n    if len(title_list) != 0:\n        assig_val = [] # this hold internal city job titles if any\n        for _ in title_list:\n            if _[-1] in ['I','X','V']: \n            # If it match something like Axxxx II, we assume it will be in the list. e.g. Assistant Inspector IV\n                assig_val.append(_.upper())   \n            # Check if it's in Internal City job class\n            elif _.upper() in job_titles: \n                assig_val.append(_.upper())\n            else:\n                # If the last word match in alternative job calss, add it in\n                if _.split()[-1].lower() in job_alt_class:\n                    job_alt_list.add(_)\n        \n        # Add all job class title match to EXP_JOB_CLASS_TITLE\n        if len(assig_val) != 0:    \n            df.at[i,'EXP_JOB_CLASS_TITLE'] = '|'.join(assig_val)\n\n\n    # If no proper noun found, try grab normal noun and regard it as an alternative title\n    job_atl_text = exp_text.replace(', or ', ', ').replace(' or ', ', ').replace(' and/or ', ', ').replace(', and', ', ')\n\n\n    # Check if the last word matches a role name\n    if len(job_alt_list)==0:\n        for ele in job_atl_text.split(', '):\n            if ele.split(' ')[-1] in job_alt_class: \n                job_alt_list.add(ele)\n\n    if len(job_alt_list) != 0:\n        df.at[i,'EXP_JOB_CLASS_ALT_RESP'] = '|'.join(list(job_alt_list))\n\n    # Get EXP_JOB_CLASS_FUNCTION, the same as above\n    # If no job title pattern match, get job function   \n    job_func1 = re.compile(r\" (in|in:) ([\\w,;\\(\\)#/ -]+)[^; or\\s*][^; and\\s*]\", re.I).search(text)\n    job_func2 = re.compile(r\" (at|with) ([\\w,\\(\\)#/; -]+)[^; or\\s*][^; and\\s*]\", re.I).search(text)\n    job_func3 = re.compile(r\"[a-z]+ing ([\\w,#\\(\\)/; -]+)[^; or\\s*][^; and\\s*]\", re.I).search(text)\n    if job_func1:\n        job_func_text = job_func1.group(2)\n    elif job_func2:\n        job_func_text = job_func2.group(2)\n    elif job_func3:\n        job_func_text = job_func3.group(1)\n    else:\n        if \" as a\" in \"text\":\n            return # Just a position with no job function description \n            \n        # Grab the whole sentence\n        fulltext = re.compile(r\"\\n(\\w\\W) ([\\w, ()'/-]+)\",re.I).search(text)\n        if fulltext:# e.g, 1. either: or 2. One of the following:,....\n            fulltext = fulltext.group(2)\n            while fulltext[0] == '\\n' or text[0] == ' ':\n                fulltext = fulltext[1:]\n            df.at[i,'EXP_JOB_CLASS_FUNCTION'] = fulltext\n        else:\n            while text[0] == '\\n' or text[0] == ' ':\n                text = text[1:]\n            df.at[i,'EXP_JOB_CLASS_FUNCTION'] = text\n        return","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def parse_edu(text):\n    '''\n    Takes in a string text and doesn't return any value. Complete the following tasks:\n    1. Get EDUCATION_YEARS value by calling length_in_year()\n    2. Get EDUCATION_MAJOR value\n    3. Get EDUCATION_MAJOR value\n    '''\n    global df\n    global i\n    \n    #print(\"parse_edu:\", text)\n    # Get EDUCATION_YEARS value\n    if length_in_year(text):\n        df.at[i,'EDUCATION_YEARS'] = length_in_year(text)\n        \n    # Get SCHOOL_TYPE value of either COLLEGE OR UNIVERSITY, HIGH SCHOOL, APPRENTICESHIP\n    if 'apprentice' in text.lower():\n        df.at[i,'SCHOOL_TYPE'] = 'APPRENTICESHIP'\n        \n    elif 'high school' in text.lower():\n        df.at[i,'SCHOOL_TYPE'] = 'HIGH SCHOOL'\n    else:\n        df.at[i,'SCHOOL_TYPE'] = 'COLLEGE OR UNIVERSITY'\n        \n    # Get EDUCATION_MAJOR value\n    # Formating text content and use key words to indicates list of major option ends (keep updating) \n    major_text = text.replace(' from', '.').replace(' with', '.').replace(', including', '.').replace('upon ', '.')\n    major_text = re.compile(r\"(degree)? in (an? )?([\\w/,\\(\\): &-]+)\").search(major_text)\n    \n    if major_text:\n        major_text = major_text.group(3)\n        # Handle expression like \"in the following major: CS; CE; SE.\" or \"Major in the following areas: \\nCS \\nCE \\nSE\"\n        if ':' in text:\n            # Replace '\\n' and ';' after ':' with ', ' \n            col_pos = text.find(\":\")\n            major_text = text[col_pos:].replace('\\n', ', ').replace('; ', ', ')\n            # Remove white spaces in the front\n            while ' ' in major_text[0]:\n                major_text = major_text[1:]\n\n        # Remove contents in the paranthesis\n        while '(' in major_text and ')' in major_text:\n            major_text = major_text[:major_text.find('(')-1] + major_text[major_text.find(')')+1:]\n\n        # Separating marks (keep updating)\n        major_text =  major_text.replace(', or ', ', ').replace(' or ', ', ').replace(' and/or ', ', ').replace(', and', ', ')\n        major_list = major_text.split(', ')\n        # Remove emtpy string\n        while '' in major_list:\n            major_list.remove('')\n        df.at[i,'EDUCATION_MAJOR'] = '|'.join([m.upper() for m in major_list if \"related\" not in m and \" a \" not in m ])        \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def parse_check(text):\n    '''\n    Takes in a string text and use regex to determine what basic type of the string is.\n    Basic type includes either of education, job expereince, course, or multi-type.\n    Return a int as a flag that indicates what basic type is the text.\n    flag value definition:\n    0: unknown structure, most likely be job funtion desciptions or course misc details, or non of the basic types.\n    1: information regard to education backgroud only.\n    2: information regard to previous job experience only.\n    3: information regard to course information only.\n    4: Double types information detected.\n    : Triple types information detected.\n    \n    '''\n    # Initialize default to unkown type\n    flag = 0 \n    count = 0 # match type count\n    \n    # Regex rules for determining basic type of statements\n    edu1 = re.compile(r\"degree from [\\w. -]*college\", re.I).search(text) \n    edu2 = re.compile(r\"graduation from [\\w. -]*(college|high school)\", re.I).search(text) \n    edu3 = re.compile(r\"completion [\\w' -]*apprenticeship\", re.I).search(text)\n    edu4 = re.compile(r\"(bachelor's|master's|Ph\\.D\\.) degree in\", re.I).search(text)\n    job1 = re.compile(r\"(full-time|part-time|paid) [\\w\\(\\)' -]*experience\",re.I).search(text)\n    job2 = re.compile(r\"current status as an? [\\w\\(\\)' -]\",re.I).search(text)\n    cour1 = re.compile(r\" semester\", re.I).search(text)\n    cour2 = re.compile(r\" quarter\", re.I).search(text)\n    cour3 = re.compile(r\"completion [\\w' -]*course\",re.I).search(text)\n    \n    if edu1 or edu2 or edu3 or edu4:\n        count += 1\n        flag = 1\n    if job1 or job2:\n        count += 1\n        flag = 2\n    if (cour1 and cour2) or cour3:\n        count += 1\n        flag = 3\n        \n    if count == 2:\n        flag = 4\n        \n    if count == 3:\n        flag = 5\n    \n    return flag\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def parse_helper(text):\n    '''\n    Takes in a string text, No return value. Complete the following tasks: \n    1. Call parse_check() to help select the next further parsing function.\n    2. If multiple types are found in the text, try to seperate the text by some key words. \n        For exmaple: Graduation in a four-year college and two year experience as a data scienctis.  \n        There are two type information in the text, education background and previous job experience requirements.\n        We could seperate it by \"and\" where left side is education background information and the right side is \n        previous job experience information, then call parse_edu() on left and parse_job() on the right.\n    3. Assign MISC_COURSE_DETAILS and EXP_JOB_CLASS_FUNCTION value if some conditions are met.\n    '''\n    global df\n    global i\n    \n    check_flag = parse_check(text)\n    # Unknow structure\n    if check_flag == 0:\n        if 'course' in text.lower() or 'program' in text.lower() or \"certificate\" in text.lower():\n            df.at[i,'MISC_COURSE_DETAILS'] = re.compile(r\"\\s(\\w\\. )?([\\w, ()'/-]+)\",re.I).search(text).group(2)\n            return\n        else:\n\n            main_item = re.compile(r\"\\n\\s*\\d\\W\").search(text)        \n            if main_item == None:# e.g, 1. either: or 2. One of the following:,....\n                df.at[i,'EXP_JOB_CLASS_FUNCTION'] = re.compile(r\"\\s*\\n(\\w\\W)? ?([\\w, ()'/-]+)\",re.I).search(text).group(2)\n                #print(\"df.at[i,'EXP_JOB_CLASS_FUNCTION']:\", df.at[i,'EXP_JOB_CLASS_FUNCTION'])\n            return\n    if check_flag == 1:\n        parse_edu(text)\n        \n    elif check_flag == 2:\n        parse_job(text)\n    \n    elif check_flag == 3:\n        parse_course(text)\n        \n    else:\n        # Try to separate the text by some key words to make each pieces just a single type\n        left = text\n        right = ''\n        \n        if check_flag == 4:\n            pivot_words = ['completion','AND', '; and',', and ',', which',' with ','including ',' and ', 'graduation', 'Both']\n\n            for p in pivot_words:\n                if p in text:\n                    left = text[:text.find(p)]\n                    right = text[text.find(p):]\n\n                if parse_check(left) != 4 and parse_check(right) != 4:\n                    parse_helper(left)\n                    parse_helper(right)\n                    return\n\n            print(\"Double type expression, NEED TO ADD A NEW PIVOT! File:\", file)\n            print(\"TEXT:\", text)\n            \n        else:\n            left = text\n            right = ''\n            pivot_words = ['; and', ' and ']\n            for p in pivot_words:\n                if p in text:\n                    left = text[:text.find(p)]\n                    right = text[text.find(p):]\n                if parse_check(left) != 5 and parse_check(right) != 5:\n                    parse_helper(left)\n                    parse_helper(right)\n                    return\n            \n            print(\"Triple type expression, NEED TO ADD A NEW PIVOT! File:\", file)\n            print(\"TEXT:\", text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def parse_sub_req(text, is_and):\n    '''\n    Takes in a string text and boolean (indicates conjunction correlation with the previous item), \n    and Complete the following tasks:\n    1. Assgin REQUIREMENT_SUBSET_ID value.\n    2. Divide the text into sub bullet items e.g., a. some statements b. some statements c...., etc.\n    3. Check the conjunction realationship between the sub bullet items. e.g., a. and b.; a. or b.\n    and then determine whether generates a new row or not. \n    For example, a. and b. stands for multi requiements need to satisfy, so one row generated. \n    On the other hand, a. or b. stands for optional requirement need to satisfy, so two rows are generated.\n      \n    '''\n    global df\n    global i\n    \n    alphabet = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n    subset_id_dict = { k:v for (k, v) in enumerate(alphabet)} #{0:'A', 1:'B', ....,etc.}\n    \n    # Find all position of the subitems\n    subitems = re.compile(r\"(\\s[a-z]\\. |\\n[a-z]\\W )\",re.I).finditer(text) # e.g., \\A., \\na., \\na), \n    items = []\n    for m in subitems:\n        items.append(m.start()) # record the position of a., b.,c. in the text.\n        \n    if len(items) < 2:#text only have one subitem 'a.', then parse the whole text\n\n        if is_and: # If the correlation with the previous parse item is AND, no need to overwrite the previous value.\n            parse_helper(text)\n        else:\n            df.at[i,'REQUIREMENT_SUBSET_ID'] = 'A'\n            parse_helper(text)\n        \n    else:\n\n        # Split the text into subtexts 1....; a....; b....; c..., etc.\n        subtext = [text[:items[0]]] # 1.....; \n        for x in range(len(items)-1):\n            subtext.append(text[items[x]:items[x+1]])\n        subtext.append(text[items[-1]:])\n        \n        # Check conjunction relationship\n        is_and = False\n        if \" and\" in subtext[0][-6:]:\n            is_and = True\n                    \n        parse_helper(subtext.pop(0)) # Parse 1....; first\n        row_copy = df.loc[i].copy() # Make a copy of the shared data\n        \n        subsetid_ind = 0\n        df.at[i,'REQUIREMENT_SUBSET_ID'] = subset_id_dict[subsetid_ind] # 0 for 'A', 1 for 'B'\n        while len(subtext) != 0:\n            nextitem = subtext.pop(0)\n            if is_and == False:\n                parse_helper(nextitem)\n                if len(subtext) != 0:\n                    i+=1\n                    subsetid_ind+=1\n                    df.loc[i] = row_copy\n                    df.at[i,'REQUIREMENT_SUBSET_ID'] = subset_id_dict[subsetid_ind]\n                    if ' and' in nextitem[-6:]:\n                        is_and = True\n            else:\n                parse_helper(nextitem)\n                if ' or' in nextitem[-6:]:\n                    is_and = False\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def parse_req(text):\n    '''\n    Takes in a string text and doesn't return any value. Complete the following tasks:\n    1. Assign REQUIREMENT_SET_ID value.\n    2. Divide the text into main bullet items e.g., 1. some statements 2. some statements 3. ...,etc.\n    3. Check the conjunction realationship between the bullet items. e.g., 1. and 2.; 1. or 2.\n    and then determine the further parsing sequence of the bullet items. \n        \n        For example, if the text structure looks somthing like this: \n        1. Master degree in either: \n        a. Computer Science; or \n        b. Computer Engineering; or \n        c. Software Engineering; and\n        2. Two years experience as a Data Scientist.\n\n        Then the final output should generate three rows: \n        1A: Master degree in CS plus 2 year experience as a DS\n        1B: Master degree in CE plus 2 year experience as a DS\n        1C: Master degree in SE plus 2 year experience as a DS\n        \n        Where information specified in #1 and #2 should all be included in #A, #B and #C. Hence, we parse #2 first, \n        and then copy all the information on the current row of the dataframe, then parse #1. Then when parsing each \n        subitem #A, #B and #C, we copy that information to every new generated row.\n    \n    On the other hand, if the structure looks somthing like this:\n        1. Two years experience as a Data Scientist.\n        1. Master degree in either: \n        a. Computer Science; or \n        b. Computer Engineering; or \n        c. Software Engineering; and\n        \n        The output should be the same, so instead, we parse #1 first, then #2.\n    \n    If no subitems in the text, we only need to parse the main items by original order. Generate a new row if the\n    realtion is \"or\". If subitems have an 'and' relationship then parse it by original order as well.\n    \n    In brief, always parse the main item first that has subitems with \"or\" relationship.\n    The conjunction relationship defualt will be 'AND' if not specified.\n    \n    '''\n    global df\n    global i\n    \n    row_count = 0\n    is_and = False\n    \n    # If there is only one requirement, assign it as 1A\n    if '\\n1.' not in text:\n\n        df.at[i,'REQUIREMENT_SET_ID'] = 1 \n        if '\\na.' not in text.lower() or ' a.' not in text.lower():\n            df.at[i,'REQUIREMENT_SUBSET_ID'] = 'A'\n            parse_helper(text) \n        else:\n            pass\n            parse_sub_req(text, False) # This funciton handle subitems' relationship\n    else:\n        # Find the position of the number. e.g., 1, 2, 3, ..., etc.\n        numlist = re.compile(r'\\n\\d\\W').finditer(text)\n        items = []\n        for m in numlist:\n            items.append(m.start())\n        \n        #  If there is only one main requirement, do subitem parsing. \n        if len(items) < 2: \n            df.at[i,'REQUIREMENT_SET_ID'] = 1\n            parse_sub_req(text, False)\n            \n        else:\n            # Split the text into subtexts e.g, 1. a...b...c.. 2. a...b..c.. 3. ...\n            subtext = []\n            for x in range(len(items)-1):\n                subtext.append(text[items[x]:items[x+1]])\n            subtext.append(text[items[-1]:])\n            \n            row_copy = df.loc[i].copy() # if a new row is generated we want values of other fields to be copied over\n            setid = 1\n            \n            subitem_cnt = 1\n            is_and = False\n            while len(subtext) != 0:\n                nextitem = subtext.pop(0)\n                if is_and == False:\n                    df.loc[i] = row_copy\n                    df.at[i,'REQUIREMENT_SET_ID'] = setid\n                    parse_sub_req(nextitem, is_and)\n                    if \" or\" not in nextitem[-6:]:\n                        is_and = True\n                    else:\n                        i+=1\n                        setid+=1\n                    subitem_cnt = len(re.compile(r\"(\\s[a-z]\\. |\\n[a-z]\\W )\",re.I).findall(nextitem))\n                    if subitem_cnt == 0: subitem_cnt = 1\n                        \n                else:\n                    k = 0\n                    while k < subitem_cnt:\n                        parse_sub_req(nextitem, is_and)\n                        i-=1\n                        k+=1\n                    i+=subitem_cnt\n                    if \" or\" in nextitem[-6:]:\n                        is_and = False\n                        i+=1\n                        setid+=1\n                        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Main code"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":false},"cell_type":"code","source":"# Initializations, df and i are global variables for supporting funcitons\ndf = pd.DataFrame() \ni = 0 # Row index of the dataframe\nk = 0\n\n# Debugging use. Fields that are not allowable of null values.\nfail_list = set() # unexpected issues during file processing goes here\nfail_job_title = set() # files that fails to match JOB_CLASS_TITLE regex goes here\nfail_class_code = set() # files that fails to match JOB_CLASS_NO regex goes here\nfail_duty = set() # files that fails to match JOB_DUTIES regex goes here\nfail_exam = set() # files that fails to match EXAM_TYPE regex goes here\nfail_salary = set() # files that fails to match ENTRY_SALARY_GEN regex goes here\nfail_req = set() # files that fails parse requiement text goes here\n\n# Open and read every text file under the bulletin directory\nfor file in bulletins:\n    with open(\"../input/cityofla/CityofLA/Job Bulletins/\"+file,encoding=\"ISO-8859-1\") as f:\n        try:\n            # Clear all '\\t', '\\r', for better result visulization\n            text = f.read().replace('\\t',' ').replace('\\r', '')\n            \n            # Get FILE_NAME\n            df.at[i,'FILE_NAME'] = file\n            \n            # Get JOB_CLASS_TITLE\n            # e.g, Grab the first line of words appear in the text that start with a capital letter\n            job_title = re.compile(r\"(CAMPUS INTERVIEWS ONLY\\s*)?([A-Z0-9][\\w'-]+( [\\w'-]+)*)\").search(text)\n            if job_title:\n                df.at[i,'JOB_CLASS_TITLE'] = ' '.join([w.capitalize() for w in job_title.group(2).lower().split(' ')])\n            else:\n                fail_job_title.add(file)\n            \n            # Get JOB_CLASS_NO\n            # e.g, Class Code: 0000\n            class_no = re.compile(r\"\\sClass *Code:\\s*([0-9]{4})\", re.I).search(text)\n            if class_no:\n                df.at[i,'JOB_CLASS_NO'] = class_no.group(1)\n            else:\n                fail_class_code.add(file)\n                \n            # Get JOB_DUTIES\n            # e.g, DUTIES..some string between...\\n\n            duty = re.compile(r\"\\n\\s*DUT[A-Z ]+\\s+(.+)\\n\").search(text)\n            if duty:\n                df.at[i,'JOB_DUTIES'] = duty.group(1)\n            else:\n                fail_duty.add(file)\n            \n            # Get DRIVERS_LICENSE_REQ: 'R' for required, 'P' for may required, leave it None if not mentioned\n            driver_req = re.compile(r\"valid [\\w' /()]*driver('s|s') license\", re.I).search(text)\n            driver_may = re.compile(r\"may require [\\w' /()]*driver('s|s') license\", re.I).search(text)\n            if driver_req: #driver_req patter covers driver_may\n                if driver_may:\n                    df.at[i,'DRIVERS_LICENSE_REQ'] = 'P'\n                else:\n                    df.at[i,'DRIVERS_LICENSE_REQ'] = 'R'\n\n            # Get DRIV_LIC_TYPE\n            # Narrow down our target, scrape a smaller piece of text first\n            driver_text = re.compile(r\"\\sclass([\\w' /(),]*)driver\", re.I).search(text)\n            if driver_text:\n                # In the narrow down text, find all capital letters that does not ajacent to a word character\n                # e.g, A, B, or C, \"A\", (B), etc...\n                driv_type_list = set()\n                driv_type = re.compile(r\"[^w]([A-Z])[^\\w]\").findall(driver_text.group(1))\n                for m in driv_type:\n                    driv_type_list.add(m)\n                # Assign all elements in the list to dataframe\n                driv_type_list = list(driv_type_list)\n                if len(driv_type_list) != 0:\n                    df.at[i,'DRIV_LIC_TYPE'] = driv_type_list[0]\n                    for x in range(1,len(driv_type_list)):\n                        df.at[i,'DRIV_LIC_TYPE'] += '|' + driv_type_list[x]\n                                    \n            # Get ADDTL_LIC\n            # Find all proper nouns follow with \"license\". e,g. \"PE license\", \"Professional Engineer license\", etc.\n            addtl = re.compile(r\"([A-Z]+([a-z']+|[A-Z]+)( and| or| and/or)?( [A-Z]+[a-z']*)*) [Ll]icense\\W\").findall(text)\n            addtl_list = set() # Exclude duplicates\n            \n            # This list come afterwards, exclude result which doesn't seem like a \"real\" license\n            bad_match = ['California', 'Special', 'This', 'The' ] \n  \n            for m,n,o,p in addtl:\n                # Exclude driver's license and bad matches\n                if (\"Driver\" not in m) and (m not in bad_match):\n                    # Accept some thing like \"Rubber Tired Tractor B Operator's license\" \n                    # but not \"Class A\" or \"California Class B\" license\n                    if m[:5] != \"Class\" and m[:16] != \"California Class\":\n                        addtl_list.add(m)\n            \n            addtl_list = list(addtl_list)\n            if len(addtl_list) != 0:\n                df.at[i,'ADDTL_LIC'] = addtl_list[0]\n                for x in range(1,len(addtl_list)):\n                    df.at[i,'ADDTL_LIC'] += \"|\" + addtl_list[x]\n            \n            # Get EXAM_TYPE, value should be either: OPEN, INT_DEPT_PROM, DEPT_PROM, OPEN_INT_PROM\n            exam_type1 = re.compile(r\"INTERDEPARTMENTAL PROMOTIONAL AND (AN )?OPEN COMPETITI?VE BASIS\").search(text)\n            exam_type2 = re.compile(r\"INTERDEPART?MENTAL PROMOTIONAL BASIS\").search(text)\n            exam_type3 = re.compile(r\"OPEN COMPETITIVE BASIS\").search(text)\n            exam_type4 = re.compile(r\" DEPARTMENTAL \").search(text)\n            if exam_type1 != None:\n                df.at[i,'EXAM_TYPE'] = 'OPEN_INT_PROM'    \n            elif exam_type2 != None:\n                df.at[i,'EXAM_TYPE'] = 'INT_DEPT_PROM'\n            elif exam_type3 != None:\n                df.at[i,'EXAM_TYPE'] = 'OPEN'\n            elif exam_type4 != None:\n                df.at[i,'EXAM_TYPE'] = 'DEPT_PROM'\n            else:\n                fail_exam.add(file)\n           \n            \n            #Get ENTRY_SALARY_GEN & ENTRY_SALARY_DWP:\n            '''\n            1. Scrape text patter. e.g., ANNUAL SALARY\\n $68,611 to $100,307 ...to four consecutive capital letters\n            2. From the scraped text, try find salary range pattern first. e.g., $68,611 to $100,307\n            3. If no match found, then try get flat-rated. e.g., $100,829 (flat-rated)\n            4. If multi salaries are specified, always grab the first one. e.g., $60,489 to $88,468 and $71,451 to $104,462\n            5. Assume normal salary always comes before DWP salary if both are specified.\n            6. If only DWP salary is specified than it will be the value of both ENTRY_SALARY_GEN & ENTRY_SALARY_DWP.\n            \n            *** we are going to ACCEPT typo expression like \"ANNUALSALARY\" here!***\n            '''\n            # Narrow down our target value position first\n            salary = re.compile(r\"ANNUAL\\s*SALARY\\s+(.|\\n)+?[A-Z]{4}\").search(text)\n            if salary:\n                sal_text = salary.group()\n                # Get ENTRY_SALARY_GEN\n                sal_range = re.compile(r\"\\$([\\d,]+) to *\\$([\\d,]+)\", re.I).search(sal_text)\n                sal_flat = re.compile(r\"\\$([\\d, ]+)\", re.I).search(sal_text)\n                if sal_range:\n                    df.at[i,'ENTRY_SALARY_GEN'] = sal_range.group(1).replace(',','') + '-' + sal_range.group(2).replace(',','')\n                else:\n                    if sal_flat:\n                        df.at[i,'ENTRY_SALARY_GEN'] = sal_flat.group(1).replace(',','') + ' (flat-rated)'\n                    else:\n                        fail_salary.add(file)\n                # Get ENTRY_SALARY_DWP\n                dwp_range = re.compile(r\"Department of Water and Power \\w*\\s*\\$([\\d, ]+) to *\\$([\\d,]+)\", re.I).search(sal_text)\n                dwp_flat = re.compile(r\"Department of Water and Power \\w*\\s*\\$([\\d, ]+)\", re.I).search(sal_text)\n                if dwp_range:\n                    df.at[i,'ENTRY_SALARY_DWP'] = dwp_range.group(1).replace(',','') + '-' + dwp_range.group(2).replace(',','')\n                else:\n                    if dwp_flat:\n                        df.at[i,'ENTRY_SALARY_DWP'] = dwp_flat.group(1).replace(',','') + ' (flat-rated)'       \n            else:\n                fail_salary.add(file)\n            \n           # Get OPEN_DATE \n            opendate1 = re.compile(r'Open [D,d]ate:\\s+(\\d?\\d-\\d?\\d-\\d\\d)').search(text)\n            opendate2 = re.compile(r'Open [D,d]ate:\\s+(\\d?\\d-\\d?\\d-\\d\\d\\d\\d)').search(text)\n            if opendate2:\n                df.at[i,'OPEN_DATE'] = datetime.strptime(opendate2.group(1),'%m-%d-%Y').date()\n            elif opendate1:\n                df.at[i,'OPEN_DATE'] = datetime.strptime(opendate1.group(1),'%m-%d-%y').date()\n            \n                \n            # Get REQUIREMENT related values \n            '''\n            Job requirement related values, which scrape information from \"REQUIREMENT/MINIMUM QUALIFICATION\" \n            section in the text such as:\n            \n            1. Item Labels: REQUIREMENT_SET_ID, REQUIREMENT_SUBSET_ID \n            2. Education background: EDUCATION_YEARS, SCHOOL_TYPE, EDUCATION_MAJOR\n            3. Previous job experience: EXPERIENCE_LENGTH, FULL_TIME_PART_TIME, EXP_JOB_CLASS_TITLE, \n            EXP_JOB_CLASS_ALT_RESP, EXP_JOB_CLASS_FUNCTION\n            4. Course taken: COURSE_COUNT, COURSE_LENGTH, COURSE_SUBJECT, MISC_COURSE_DETAILS\n            \n            Since this part is more complicated, there are closely couple funcitons (utilize global variables)\n            that handdle this part for readablility. The general method process are describe as follow:\n            \n            1. First scrape the text bellow REQUIREMENTS/MINIMUM QUALIFICATIONS (or simular meaning)\n            Most of the files follows the section PROCESS NOTES, but we're going to use a more generalize pattern,\n            that is, double new lines (w/ or w/o extra spaces between) plus four consecutive uppercase letters.\n            \n            2. Divide the requirement text into main bullets items in the form of \n                1. main statement, includes possible subitems like a...b...c..\n                2. main statement, includes possible subitems like a...b...c..\n                3. ................,etc\n            Determine the conjunction relationship between the items, such as 1. and 2., 1. or 2., etc.\n            If it multiple options (e.g., 1. or 2.), then they are different rows in the dataframe. \n            On the other hand, if they are multiple requirements to satisfy (e.g., 1. and 2.), \n            put all information in one row.\n            \n            parse_req() handlew part #2\n            \n            3. Then parse the subitems just like #2. \n                a. sub-statement.\n                b. sub-statement.\n                c. .............., etc.\n            Determine the conjunction relationship between the subitems and main items, just like #2.\n            \n            parse_sub_req() handles part #3\n            \n            4. Determine the type of the statement and select the right function to further parse the statement.\n                4.1 Use parse_check() to determine the statement type, e.g., education, job experience, or course taken\n                4.2 For education type statements, use parse_edu() to further parse it.\n                4.4 For previous job experience type statements, use parese_job to further parse it.\n                4.5 For course taken type statements, use, parse_course to further parse it.\n                4.6 If no specific stucture in the statement, then catergorize it to EXP_JOB_CLASS_FUNCTION or \n                MISC_COURSE_DETAILS, according to the example \"SYSTEMS ANALYST 1596 102717.txt\"\n                4.7 For multiple type statements such as education plus job experience, try to sperate the\n                sentence by using good pivot key words like \" and \", ';'\n                \n            parse_helper() handles part #4\n            \n            For more details about each functions, please see the description in function definition.\n\n            '''\n            # Narrow down the text\n            req = re.compile(r'(REQUIREMENT[A-Z /:]*)\\n((.|\\n)+?)\\n\\s*([A-Z ]*NOTE|\\n[A-Z]{4})').search(text)\n            if req:\n                req_text = req.group()\n                # Some files have paragraph at the end of the REQUIREMENT section \n                # for requirement substitution decription, which we're not handling it currently.\n                req_refine_text = re.compile(r\"\\n\\s*\\n?(.|\\n)+?\\n\\s*\\n\").search(req_text)\n                if req_refine_text:\n                    req_text = req_refine_text.group()\n\n                parse_req(req_text)\n                \n            else:\n                fail_req.add(file)\n            \n        except Exception as e:\n            fail_list.add(file)\n\n    i+=1\n\n# Rearrange column seqence\ndf = df[['FILE_NAME', 'JOB_CLASS_TITLE', 'JOB_CLASS_NO', 'REQUIREMENT_SET_ID',\n       'REQUIREMENT_SUBSET_ID', 'JOB_DUTIES', 'EDUCATION_YEARS', 'SCHOOL_TYPE',\n       'EDUCATION_MAJOR', 'EXPERIENCE_LENGTH', 'FULL_TIME_PART_TIME',\n       'EXP_JOB_CLASS_TITLE', 'EXP_JOB_CLASS_ALT_RESP',\n       'EXP_JOB_CLASS_FUNCTION', 'COURSE_COUNT', 'COURSE_LENGTH',\n       'COURSE_SUBJECT', 'MISC_COURSE_DETAILS', 'DRIVERS_LICENSE_REQ',\n       'DRIV_LIC_TYPE', 'ADDTL_LIC', 'EXAM_TYPE', 'ENTRY_SALARY_GEN',\n       'ENTRY_SALARY_DWP', 'OPEN_DATE']]\n    \n\n# \"The following code is for debug use\"\n# print(\"fail_list count:\",len(fail_list))\n# print(\"fail_list:\",fail_list)\n# print(\"fail_job_title count:\",len(fail_job_title))\n# print(\"fail_job_title:\",fail_job_title)\n# print(\"fail_class_code count:\",len(fail_class_code))\n# print(\"fail_class_code:\",fail_class_code)\n# print(\"fail_duty count:\",len(fail_duty))\n# print(\"fail_duty:\",fail_duty)\n# print(\"fail_exam count:\",len(fail_exam))\n# print(\"fail_exam:\",fail_exam)\n# print(\"fail_salary count:\",len(fail_salary))\n# print(\"fail_salary:\",fail_salary)\n# print(\"fail_req count:\",len(fail_req))\n# print(\"fail_req:\",fail_req)\nprint(\"Data set generated, please call df to view.\")\nprint(\"Fail parsing file:\")\nfail_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for Columns that are NOT allowable for null values\nprint(\"Files for no JOB_CLASS_TITLE:\",df[df['JOB_CLASS_TITLE'].isnull()]['FILE_NAME'].unique())\nprint(\"Files for no JOB_CLASS_NO:\",df[df['JOB_CLASS_NO'].isnull()]['FILE_NAME'].unique())\nprint(\"Files for no JOB_DUTIES:\",df[df['JOB_DUTIES'].isnull()]['FILE_NAME'].unique())\nprint(\"Files for no REQUIREMENT_SET_ID:\",df[df['REQUIREMENT_SET_ID'].isnull()]['FILE_NAME'].unique())\nprint(\"Files for no REQUIREMENT_SUBSET_ID:\",df[df['REQUIREMENT_SUBSET_ID'].isnull()]['FILE_NAME'].unique())\nprint(\"Files for no EXAM_TYPE:\",df[df['EXAM_TYPE'].isnull()]['FILE_NAME'].unique())\nprint(\"Files for no ENTRY_SALARY_GEN:\",df[df['ENTRY_SALARY_GEN'].isnull()]['FILE_NAME'].unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Summary text file parsing issues\nAfter examining all missing values in the fields that can't be a null value, we verify that all missing values are due to raw data issue, which means that they are, in fact not clearly specified in the plain text file, rather than problem due to our extract methods.\n\n- One file No Class code:\n\nVocational Worker  DEPARTMENT OF PUBLIC WORKS.txt\n\n- Six files No DUTY/DUTIES specification: \n\nAPPARATUS OPERATOR 2121 071417 (1).txt, ENGINEER OF FIRE DEPARTMENT 2131 111116.txt, FIRE ASSISTANT CHIEF 2166 011218.txt, FIRE BATTALION CHIEF 2152 030918.txt, FIRE HELICOPTER PILOT 3563 081415 REV. 081815.txt, FIRE INSPECTOR 2128 031717.txt\n\n- One file No Exam type:\n\nVocational Worker  DEPARTMENT OF PUBLIC WORKS.txt\n\n- Two files No Salary specification or \"the Salary is to be determined\"\n\nVocational Worker  DEPARTMENT OF PUBLIC WORKS.txt, AIRPORT POLICE SPECIALIST 3236 063017 (2).txt, \n\n- One file with a typo salary expression: extra spaces between digits\n\nEQUIPMENT REPAIR SUPERVISOR 3746 012717.txt\n\n- One file No Open Date: only mention \"DATE\" instead of \"Open Date\"\n\nVocational Worker  DEPARTMENT OF PUBLIC WORKS.txt\n"},{"metadata":{},"cell_type":"markdown","source":"### Our Final Sturctured Dataset is HERE!"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.to_csv(\"output.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Generate a data dictionary\nBasically it is the same as \"kaggle_data_dictionary.csv\", I follow it to generate the final output.csv file. Little changes are: \n1. Allow the JOB_DUTIES column to accepts null value, since there are 6 files did not specify this part.  \n2. Define OPEN_DATE should not allow null value.\n3. Drop Annotation Letter column, since I'm not planning to submit another word file or plain text."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dict = pd.read_csv(\"../input/cityofla/CityofLA/Additional data/kaggle_data_dictionary.csv\")\ndata_dict = data_dict.drop(\"Annotation Letter\",axis=1)\ndata_dict.at[19,\"Additional Notes\"] = '''A list of many class types  should appear as a single string separated by a \"|\"'''\ndata_dict.at[24,\"Accepts Null Values?\"] = \"No\"\n\ndata_dict.to_csv(\"data_dictionary.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### End of building structured dataset\nThere are a lot of work behind the screen, trying to find a good pattern to get the best result. Finally we could move on to some analysis!"},{"metadata":{},"cell_type":"markdown","source":"## Data Analysis & Stats"},{"metadata":{},"cell_type":"markdown","source":"### Data set feature expanding\n- From feature ENTRY_SALARY_GEN exapand MIN_SAL, SAL_DIFF, where MIN_SAL is the flat-rate salary or the starting salary of the range, and SAL_DIFF is the bias of the salary range (0 if for flat-rate expressions).\n- From feature OPEN_DATE expand YEAR, MONTH, DAY and WEEKDAY. "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"final_df = df.copy()\n\nsal_dif = []\nmin_sal = []\nfor sal in final_df['ENTRY_SALARY_GEN']:\n\n    sal = str(sal) # Make sure it's a string\n    sal_range = re.compile(r\"(\\d+)-(\\d+)\", re.I).search(sal)\n    sal_flat = re.compile(r\"(\\d+)\", re.I).search(sal)\n    \n    if sal_range:\n        # If salary range expression matches, grab the first digit expression as MIN_SAL, the difference as SAL_DIF\n        if int(sal_range.group(2)) > int(sal_range.group(1)): # if there is error negative expression, it will be detected here\n            sal_dif.append((int(sal_range.group(2))-int(sal_range.group(1))))\n        else:\n            # If some how the max salary is smaller than the min salary, set SAL_DIFF to 0.\n            sal_dif.append(0)\n        min_sal.append(int(sal_range.group(1)))\n    elif sal_flat:\n        # If flat-rate salay expression matches, set SAL_DIF to 0 and SAL_MIN as the digit exrpession.\n        sal_dif.append(0)\n        min_sal.append(int(sal_flat.group(1)))\n    else:\n        sal_dif.append(np.nan)\n        min_sal.append(np.nan)\n        \nfinal_df['SAL_DIF'] = sal_dif\nfinal_df['MIN_SAL'] = min_sal\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Drop row with null value in OPEN_DATE\nfinal_df = final_df[final_df['OPEN_DATE'].notnull()]\n# Expand year, month, day, weekday  \nweekday = ['MON','TUE', 'WED','THU','FRI','SAT','SUN'] # for weekday transform\nfinal_df['YEAR'] = [date.year for date in final_df['OPEN_DATE']]\nfinal_df['MONTH'] = [date.month for date in final_df['OPEN_DATE']]\nfinal_df['DAY'] = [date.day for date in final_df['OPEN_DATE']]\nfinal_df['DAY_OF_WEEK'] = [weekday[date.weekday()] for date in final_df['OPEN_DATE']]\nfinal_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"total_job_count = len(final_df['FILE_NAME'].unique())\ntotal_avg_sal = final_df[['FILE_NAME','MIN_SAL']].drop_duplicates(\"FILE_NAME\")['MIN_SAL'].mean()\nprint(f\"There are {total_job_count} job openings with {final_df.shape[0]} kinds of requirement sets in the structured data set\")\nprint(f\"The overall average starting salary is ${round(total_avg_sal,2)}.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"final_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Find some interesting insights from the data set\nSince we already put great effort building a structured data set, let's first see if we can get some insights or answer some questions from it before starting to focus on the problem statements."},{"metadata":{},"cell_type":"markdown","source":"#### What is the amount of jobs trend over the past years?"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Extract features\njob_date = final_df[[\"FILE_NAME\",\"JOB_CLASS_TITLE\",\"YEAR\",\"MONTH\",\"DAY_OF_WEEK\"]].drop_duplicates()\n# Check job release count by year\nyear = job_date['YEAR'].value_counts().sort_index().index.astype('str') # change index type to str for ploting\ncount = job_date['YEAR'].value_counts().sort_index().values\nplt.plot(year, count, color='g')\nplt.xlabel('Year')\nplt.ylabel('Count')\nplt.title('Jobs Release in LA Over Year')\nplt.xticks(rotation=50)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that job oppertunities starting to grow since then 2014, or a more likely explaination is that data records have been better preserved after 2014, since unlikely that there is nearly no hiring before 2013."},{"metadata":{},"cell_type":"markdown","source":"#### Does Job openings have a monthly preference?"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Check job release count by month\nfigure(num=None, figsize=(6, 4), dpi=80) # Set figure size\nplt.xlabel('Month')\nplt.ylabel('Count')\nplt.title('Jobs Count in LA Over Month')\njob_date['MONTH'].value_counts().sort_index().plot(kind='bar')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Possibly we have more job openings in March, April, October and December. Less opertunity in January, Febuary, May, July and Augest. "},{"metadata":{},"cell_type":"markdown","source":"#### Or maybe a weekday preference?"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Check job release count by weekday\nfigure(num=None, figsize=(6, 4), dpi=80) # Set figure size\njob_date['DAY_OF_WEEK'].value_counts().plot(kind='barh')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# What are the jobs open on thursday?\njob_date[job_date['DAY_OF_WEEK']=='THU'][['JOB_CLASS_TITLE']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Pretty solid that almost all jobs are opened on Friday, except four of those, which are \"police\" related jobs!"},{"metadata":{},"cell_type":"markdown","source":"#### What's the salary distribution in LA city?"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Generate a focus dataframe with the feature of interest\nsal = final_df[['FILE_NAME','JOB_CLASS_TITLE','ENTRY_SALARY_GEN', 'SAL_DIF', 'MIN_SAL']].drop_duplicates().dropna()\nsal = sal.drop('FILE_NAME', axis=1)\n# Plot distribution historgram\nsns.distplot(sal['MIN_SAL'],bins=25)\n#plt.hist(sal['MIN_SAL'], color = 'indianred', bins=25)\nplt.xlabel('US Dollar, $')\nplt.title('Salary Distribution in LA')\nplt.show()\nprint(f\"The average starting salary is ${int(sal['MIN_SAL'].mean())}. The std is {int(sal['MIN_SAL'].std())}.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like normal distribution in general."},{"metadata":{},"cell_type":"markdown","source":"#### What are the 15 best jobs?"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"best15 = sal[['JOB_CLASS_TITLE','MIN_SAL']].sort_values('MIN_SAL', ascending = False)[:15]\nsns.barplot(y=best15['JOB_CLASS_TITLE'],x=best15['MIN_SAL'])\nplt.xlabel('Salary in US dollar, $')\nplt.ylabel('')\nplt.title('Best Salary of Jobs in LA')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not surprise, \"Leader\" or \"Manager\" levels positions and polits."},{"metadata":{},"cell_type":"markdown","source":"#### What job has the biggest salary difference in percentage?"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"big_dif = sal[['JOB_CLASS_TITLE','SAL_DIF', 'MIN_SAL']]\nbig_dif['PERC_DIF'] = [ int(x)/int(y)*100 for x,y in zip(big_dif['SAL_DIF'], big_dif['MIN_SAL'])]\nbig_dif = big_dif.sort_values('PERC_DIF', ascending = False)[:15]\nsns.barplot(y=big_dif['JOB_CLASS_TITLE'],x=big_dif['PERC_DIF'])\nplt.xlabel('Salary difference in percentage, %')\nplt.ylabel('')\nplt.title('Biggest Salary Difference of Jobs in LA')\nplt.show()\nbig_dif[:1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some Port Police Officer might have up to 1.67 time salary than the starting colleagues. "},{"metadata":{},"cell_type":"markdown","source":"#### Is there a preference of exam type?"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Get data from the database (df)\npromo_type = final_df[['FILE_NAME','JOB_CLASS_TITLE','EXAM_TYPE', 'MIN_SAL']].drop_duplicates().dropna()\n# Check job release count by weekday\npromo_type['EXAM_TYPE'].value_counts().plot(kind='barh')\nplt.xlabel('Counts')\nplt.title('Counts for different exam type values')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"depart = promo_type[promo_type['EXAM_TYPE']==\"DEPT_PROM\"].drop(['FILE_NAME'], axis=1)\ndepart","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like \"police\" or \"fire\" related department prefer departmental promotional."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Check all job class titles include the word \"poice\" and \"fire\" \npolice_and_fire = []\nfor x in promo_type.index:\n    if 'police' in promo_type.loc[x][1].lower() or 'fire' in promo_type.loc[x][1].lower():\n        police_and_fire.append(promo_type.loc[x])\n        \npolice_and_fire = DataFrame(police_and_fire).drop('FILE_NAME',axis=1).sort_values('MIN_SAL', ascending = False)\npolice_and_fire.head()\n\npolice_and_fire.groupby('EXAM_TYPE').boxplot(layout=(1,4))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In \"police\" or \"fire\" related deparment, departmental exam type promotional leads to a higher salary position."},{"metadata":{},"cell_type":"markdown","source":"#### How many jobs mentioned additional license in the job bulletin? What's the salary difference of those jobs?"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Create dataframe from extracting rows that have values in column ADDTL_LIC\naddtl_df = final_df[['FILE_NAME','JOB_CLASS_TITLE' ,'ADDTL_LIC','MIN_SAL']][final_df['ADDTL_LIC'].notnull()].drop_duplicates()\n# Plot salary distribution\nsns.distplot(addtl_df['MIN_SAL'],bins=25)\n#plt.hist(addtl_df['MIN_SAL'], color = 'indianred', bins=10)\nplt.xlabel('Salary in US Dollar, $')\nplt.xticks(rotation=0)\nplt.title('Salary Distribution for Jobs that Ask for Additional License')\nplt.show()\nprint(f\"The average starting salary for jobs that ask for additinoal license is ${int(addtl_df['MIN_SAL'].mean())}.\")\nprint(f\"It's ${int(addtl_df['MIN_SAL'].mean()-total_avg_sal)} more than the overall average.\")\nprint(f\"{addtl_df.shape[0]} jobs mentioned additional license (not including driver's license), it's {round(addtl_df.shape[0]/total_job_count,3)*100}% of the job pool\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### How many jobs mentioned a valid driver's license? What about the types?"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"driv_df = final_df[['FILE_NAME','DRIVERS_LICENSE_REQ']].drop_duplicates().drop(\"FILE_NAME\", axis=1)\ndriv_df['DRIVERS_LICENSE_REQ'].replace(np.nan,'Not mentioned').value_counts().plot.pie(autopct='%.2f', fontsize=15, figsize=(6, 6))\nplt.ylabel('Percentage, %')\nplt.title(\"Valid Driver's License Mentioned in Job bulletin\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"drivtype_df = final_df[['FILE_NAME','DRIV_LIC_TYPE']].drop_duplicates().drop(\"FILE_NAME\", axis=1)\ntype_list = []\nfor x in drivtype_df['DRIV_LIC_TYPE'].dropna():\n    type_list += x.split(\"|\")\ncounter=Counter(type_list)\nSeries(counter).plot.pie(autopct='%.2f', fontsize=15, figsize=(6, 6))\nplt.ylabel('Percentage, %')\nplt.title(\"Driver's License Types Mentioned Job bulletin\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Over 80% of jobs mentioned driver's license, an type B is the most common one if specified."},{"metadata":{},"cell_type":"markdown","source":"#### How many jobs ask for course units to be taken in the requirements? What are the most popular ones they are looking for?"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Drop rows that have all null values in the following three columns: COURSE_COUNT, COURSE_LENGTH,COURSE_SUBJECT\ncourse_df = final_df.loc[final_df[['COURSE_COUNT','COURSE_LENGTH','COURSE_SUBJECT']].dropna(how='all').index]\n# Extract the features of interest\ncourse_df = course_df[['FILE_NAME','JOB_CLASS_TITLE','COURSE_COUNT','COURSE_LENGTH','COURSE_SUBJECT']]\ncourse_cnt = len(course_df['FILE_NAME'].unique())\nprint(f\"{course_cnt} jobs ask for specific course unit to be taken in the requirement. This is {round(course_cnt/total_job_count,4)*100}% of the job pool.\")","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Count all course subjects\ntemp = course_df[['COURSE_SUBJECT','FILE_NAME']].drop_duplicates()\ncourse_list = []\nfor x in temp['COURSE_SUBJECT'].dropna():\n    course_list += x.lower().split(\"|\")\ncourse_list\n\nSeries(Counter(course_list)).sort_values(ascending=False)[:15].plot(kind='barh')\nplt.title(\"Coures Subject Counts\")\nplt.gca().invert_yaxis()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### How many jobs ask for previous job experience and the experience length?"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Drop rows that have all null values in experience columns\nexp_df = final_df.loc[final_df[['EXPERIENCE_LENGTH','FULL_TIME_PART_TIME','EXP_JOB_CLASS_TITLE','EXP_JOB_CLASS_ALT_RESP']].dropna(how='all').index]\n# Grab the feature of interest\nfocus_columns = ['FILE_NAME','JOB_CLASS_TITLE','EXPERIENCE_LENGTH', 'FULL_TIME_PART_TIME',\n                 'EXP_JOB_CLASS_TITLE','EXP_JOB_CLASS_ALT_RESP','EXP_JOB_CLASS_FUNCTION']\nexp_df = exp_df[focus_columns]\nexp_cnt = len(exp_df['FILE_NAME'].unique())\n\nprint(f\"{exp_cnt} jobs mentioned previous job experience. this is {round(exp_cnt/total_job_count,4)*100}% of the job pool.\")","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Plot experience length distribution\nexp_leng = exp_df['EXPERIENCE_LENGTH'].dropna()#.value_counts().sort_index()\nexp_leng_avg = exp_leng.mean()\nsns.distplot(list(exp_leng.values),bins=8)\nplt.xlabel('Experience length, year')\nplt.ylabel('Ratio')\nplt.title('Experience Length Asked in Job Requirement')\nplt.show()\nprint(f\"The average of experience length per requirement set is {round(exp_leng_avg,2)} year.\")","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"position_list = []\n# We want one count of EXP_JOB_CLASS_TITLE for each job, so drop duplicates\nposition_df =  exp_df[['FILE_NAME','EXP_JOB_CLASS_TITLE']].drop_duplicates()\nfor x in position_df['EXP_JOB_CLASS_TITLE'].dropna():\n    position_list += x.split('|')\n\nSeries(Counter(position_list)).sort_values(ascending = False)[:15].plot(kind='barh')\nplt.title(\"LA Job Class Title Rank\")\nplt.gca().invert_yaxis()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Almost all jobs prefer applicants with job experience, most of those ask for at least two yearsof expereience. \"Analyst\" and \"Management\" are most frequently asked."},{"metadata":{},"cell_type":"markdown","source":"#### What the most popular school type mentioned in the job bullettin?"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Generate a dataframe with feartures of interest, drop rows with null value in SCHOOL_TYPE\nfocus_colum = ['FILE_NAME','JOB_CLASS_TITLE','EDUCATION_YEARS','SCHOOL_TYPE','EDUCATION_MAJOR','EXPERIENCE_LENGTH',\n               'EXP_JOB_CLASS_TITLE', 'EXP_JOB_CLASS_ALT_RESP','DRIVERS_LICENSE_REQ','ADDTL_LIC', 'MIN_SAL', 'YEAR', 'MONTH']\nedu_df = final_df[focus_colum][final_df['SCHOOL_TYPE'].notnull()]\n# Count the jobs that ask for apprenticeship, college and high school diploma\nedu_cnt = len(edu_df['FILE_NAME'].unique())\nappren_cnt = len(edu_df[edu_df['SCHOOL_TYPE']=='APPRENTICESHIP']['FILE_NAME'].unique())\nhigh_cnt = len(edu_df[edu_df['SCHOOL_TYPE']=='HIGH SCHOOL']['FILE_NAME'].unique())\ncollege_cnt = len(edu_df[edu_df['SCHOOL_TYPE']=='COLLEGE OR UNIVERSITY']['FILE_NAME'].unique())\nSeries({\"College\":college_cnt,\"Apprenticeship\":appren_cnt, \"High School\":high_cnt}).plot.pie(autopct='%.2f', fontsize=15, figsize=(6, 6))\nplt.ylabel(\"\")\nplt.title(\"Education Background Mentioned in Jobs, %\", fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most of them ask for college degree if mentioned."},{"metadata":{},"cell_type":"markdown","source":"#### What are the chances for ones that only have high school diploma?"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"high_cnt = len(edu_df[edu_df['SCHOOL_TYPE']=='HIGH SCHOOL'])\nprint(f\"Among {total_job_count} jobs, {high_cnt} jobs only ask for high school diploma.\")\nprint(f\"The portion is {round(high_cnt/total_job_count,4)*100}% of the job pool.\")\nhigh_df = edu_df[edu_df['SCHOOL_TYPE']=='HIGH SCHOOL'][['JOB_CLASS_TITLE','SCHOOL_TYPE','EXPERIENCE_LENGTH','DRIVERS_LICENSE_REQ','MIN_SAL']].sort_values('MIN_SAL', ascending=False)\nprint(f\"The best payment among those position is ${high_df['MIN_SAL'].max()}. The salary average is ${round(high_df['MIN_SAL'].mean(),2)}.\")\nhigh_df   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Best payment jobs for applicants that only have high school diploma are Firefighter and Police Officer. Most of those position required a valid driver's license."},{"metadata":{},"cell_type":"markdown","source":"#### What about chances for apprenticeship?"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Get more specialized worker list by searching apprenticeship value\nspecilized_worker = set()\nappren_df = edu_df[edu_df['SCHOOL_TYPE']=='APPRENTICESHIP'][[\"FILE_NAME\",\"JOB_CLASS_TITLE\",'MIN_SAL']].drop_duplicates()\nappren_cnt = len(appren_df)\nappren_sal = appren_df['MIN_SAL'].mean()\nfor role in appren_df:\n    specilized_worker.add(role.split(\" \")[-1])\n\n# Plot\nappren_df = appren_df.sort_values('MIN_SAL', ascending = False)[:15]\nsns.barplot(y=appren_df['JOB_CLASS_TITLE'],x=appren_df['MIN_SAL'])\nplt.xlabel('Salary US Dollar, $')\nplt.ylabel('')\nplt.title('Salary of Jobs That Ask for Apprenticeship')\nplt.show()\nprint(f\"Among {total_job_count} jobs, {appren_cnt} of those ask for apprenticeship.\")\nprint(f\"The portion is {round(appren_cnt/total_job_count,4)*100}% of the job pool.\")\nprint(f\"The average salary is ${round(appren_sal,2)}.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### How about chances for students who have college degree but with no work experience?"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Find all jobs that didn't required job expereience\nfocus_columns = ['FILE_NAME','JOB_CLASS_TITLE','MIN_SAL',\"MONTH\"]\ncollege_no_exp_df = edu_df[edu_df[\"EXPERIENCE_LENGTH\"].isnull()]\ncollege_no_exp_df = college_no_exp_df[college_no_exp_df[\"EXP_JOB_CLASS_TITLE\"].isnull()]\ncollege_no_exp_df = college_no_exp_df[college_no_exp_df[\"SCHOOL_TYPE\"]=='COLLEGE OR UNIVERSITY']\njob_no_exp = college_no_exp_df[focus_columns].drop_duplicates().drop(\"FILE_NAME\",axis=1).sort_values(\"MIN_SAL\", ascending=False)\n# Plot\nsns.barplot(y=job_no_exp[:15]['JOB_CLASS_TITLE'],x=job_no_exp[:15]['MIN_SAL'])\nplt.xlabel('Salary US Dollar, $')\nplt.ylabel('')\nplt.title('Salary of Jobs That Ask for College degree')\nplt.show()\nno_exp_cnt = len(job_no_exp)\nno_exp_sal = job_no_exp['MIN_SAL'].mean()\nprint(f\"Among {total_job_count} jobs, {no_exp_cnt} of those accepts college degree applicants with job experience.\")\nprint(f\"The portion is {round(no_exp_cnt/total_job_count,4)*100}% of the job pool.\")\nprint(f\"The average salary is ${round(no_exp_sal,2)}.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Last question for interest, if we try to group all job titles into three level by hierarchy, what are the changes over the past few years?"},{"metadata":{},"cell_type":"markdown","source":"I group job hierarchy by searching key words in the job title such as:\n- Top level: \"Director of\", \"Chief\", \"Principal\", \"Specialist\", \"Executive\", \"Captain\", \"Superintendent\", \"Curator\", \"Manager\", \"Division\", \"Engineer of\", \"Commander\".\n- Middle level: \"Senior\", \"Supervisor\", \"Sergeant\", \"Lieutenant\", \"Analyst\", \"Dispatcher\", \"Instructor\", \"Director\", \"Administrator\", \"Detective\".\n- Entry levle: Else."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"job_level = final_df[['FILE_NAME','JOB_CLASS_TITLE','MIN_SAL','YEAR']].drop_duplicates()\n\ntop_words = [\"director of\", \"chief\", \"principal\",\"specialist\", \"executive\", \"captain\",\n         \"superintendent\", \"curator\", \"manager\", \"division\",\"engineer of\", \"commander\"]\nmid_words = [\"senior\",\"supervisor\",\"sergeant\",\"lieutenant\",\"analyst\",\"dispatcher\",\"instructor\",\n             \"director\",\"administrator\", \"detective\"]\njob_level = job_level.reset_index()\nk = 0\nfor x in job_level[\"JOB_CLASS_TITLE\"]:  \n    if any(word in x.lower() for word in top_words):\n        job_level.at[k,'JOB_LV'] = \"T\"\n    elif any(word in x.lower() for word in mid_words):\n        job_level.at[k,'JOB_LV'] = \"M\"\n    else:\n        job_level.at[k,'JOB_LV'] = \"E\"\n    k+=1\njob_level = job_level[[\"JOB_LV\",\"MIN_SAL\",\"YEAR\"]]\n# Higher job level normaly should have higher grade, so we check if the average salary make sense\njob_level[[\"JOB_LV\",\"MIN_SAL\"]].groupby(\"JOB_LV\").mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The method probably work, because conventionally average starting salary should be like: top level > middle > entry level."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Plot the amount of job level trend over year\ntemp_dict = {}\nfor grade in [\"T\",\"M\",\"E\"]:\n    temp = job_level[job_level['JOB_LV']==grade]\n    temp_dict[grade] = temp[\"YEAR\"].value_counts()\nDataFrame(temp_dict).plot.bar(figsize=(8,5))\nplt.xlabel('Year')\nplt.ylabel('Job Opening Counts')\nplt.title('Job Level Trend Over Year')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We got normal structure after 2016, that is, opportunity for manager level are less than senior level and senior level are less than entry level. If not normal then if might indicates problem such as high job turnover rate. "},{"metadata":{},"cell_type":"markdown","source":"## Text analysis for recommandation\nIn this project we are ask to:\n1.  identify language that can negatively bias the pool of applicants; \n2.  improve the diversity and quality of the applicant pool; and/or \n3.  make it easier to determine which promotions are available to employees in each job class."},{"metadata":{},"cell_type":"markdown","source":"### Identify language that can negatively bias the pool of applicants\nTo identify language that can negatively bias the pool of applicants, we will first need a list of words that is to be considered as \"bias\" words, thus the following website provides us what we want:\n\nhttp://gender-decoder.katmatfield.com/about\n\nFor the following analysis, we are only going to count bias words in locate in the job duty and requirement section."},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"bias_words = set()\nt='''\nagree\naffectionate\nchild\ncheer\ncollab\ncommit\ncommunal\ncompassion\nconnect\nconsiderate\ncooperat\nco-operat\ndepend\nemotiona\nempath\nfeel\nflatterable\ngentle\nhonest\ninterpersonal\ninterdependen\ninterpersona\ninter-personal\ninter-dependen\ninter-persona\nkind\nkinship\nloyal\nmodesty\nnag\nnurtur\npleasant\npolite\nquiet\nrespon\nsensitiv\nsubmissive\nsupport\nsympath\ntender\ntogether\ntrust\nunderstand\nwarm\nwhin\nenthusias\ninclusive\nyield\nshare\nsharin\nactive\nadventurous\naggress\nambitio\nanaly\nassert\nathlet\nautonom\nbattle\nboast\nchalleng\nchampion\ncompet\nconfident\ncourag\ndecid\ndecision\ndecisive\ndefend\ndetermin\ndomina\ndominant\ndriven\nfearless\nfight\nforce\ngreedy\nhead-strong\nheadstrong\nhierarch\nhostil\nimpulsive\nindependen\nindividual\nintellect\nlead\nlogic\nobjective\nopinion\noutspoken\npersist\nprinciple\nreckless\nself-confiden\nself-relian\nself-sufficien\nselfconfiden\nselfrelian\nselfsufficien\nstubborn\nsuperior\nunreasonab'''\npattern = re.compile(r\"[a-z-]+\").findall(t)\nfor n in pattern:\n    bias_words.add(\" \"+n) # word should be a the starting characters\n\nprint(f\"We got {len(bias_words)} bias words in our list!\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"bias_check_df = DataFrame()\ni = 0\ntotal_cnt_list = [] # hold all bias word encountered\nfor file in bulletins:\n    with open(\"../input/cityofla/CityofLA/Job Bulletins/\"+file) as f:\n        try:\n            text = f.read()\n            # Extract job duty content\n            duty = re.compile(r\"DUT[A-Z ]+\\s+(.+)\\n\").search(text)\n            if duty:\n                duty_text = duty.group(1)\n            else:\n                continue\n            \n            # Extract requirement content\n            req = re.compile(r'(REQUIREMENT[A-Z /:]*)\\n((.|\\n)+?)\\n\\s*([A-Z ]*NOTE|\\n[A-Z]{4})').search(text)\n            if req:\n                req_text = req.group(2)\n            else: \n                continue\n            \n            # Put two content together\n            text = duty_text + req_text\n            \n            bias_check_df.at[i, \"FILE_NAME\"] = file\n            bias_w_cnt = 0 # count the number of all bias word in a text file\n            bias_record = [] # record the bias words \n            \n            for word in bias_words:\n                pattern = re.compile(word+\"[a-z]*\").findall(text)\n                for t in pattern:\n                    bias_w_cnt+=1\n                    bias_record.append(t)\n                    total_cnt_list.append(t)\n            bias_check_df.at[i, \"BIAS_COUNT\"] = bias_w_cnt\n            bias_check_df.at[i, \"BIAS_WORDS\"] = \"|\".join(list(set(bias_record)))\n        except:\n            pass\n        i+=1\ncounter = Counter(total_cnt_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bias_check_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"Series(counter).sort_values(ascending=False)\n# Plot\nbias_rank = Series(counter).sort_values(ascending=False)[:15]\nsns.barplot(y=bias_rank.index,x=bias_rank.values)\nplt.xlabel('Counts')\nplt.ylabel('')\nplt.title('Most Frequently Bias Words Appear in Job bulletin')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"bias_check_df.sort_values(\"BIAS_COUNT\", ascending=False)[:15]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Readability test\n- Score\tDifficulty\n- 90-100\tVery Easy\n- 80-89\tEasy\n- 70-79\tFairly Easy\n- 60-69\tStandard\n- 50-59\tFairly Difficult\n- 30-49\tDifficult\n- 0-29\tVery Confusing"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"read_df = DataFrame()\ni = 0\nfor file in bulletins:\n    with open(\"../input/cityofla/CityofLA/Job Bulletins/\"+file,encoding=\"ISO-8859-1\") as f:\n        try:\n            text = f.read()\n\n            read_df.at[i,\"FILE_NAME\"] = file\n            num_sen = text.count('.') + text.count('!') + text.count(';') + text.count(':') + text.count('?')\n\n            asl = len(text.split())/num_sen\n            syl_cnt = 0\n            for word in text.split():\n                for vowel in ['a','e','i','o','u']:\n                    syl_cnt += word.count(vowel)\n                for ending in ['es','ed','e']:\n                    if word.endswith(ending):\n                           syl_cnt -= 1\n                if word.endswith('le'):\n                    syl_cnt += 1\n                \n            asw = syl_cnt/len(text.split())\n            score = 206.835-(1.015*asl)-(84.6*asw)\n            read_df.at[i,\"F_SCORE\"] = score\n        except:\n            pass\n    i+=1\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Plot distribution\nsns.distplot(read_df[\"F_SCORE\"],bins=20)\nplt.xlabel('F-Score')\nplt.ylabel('')\nplt.title('F-Score distribution')\nplt.show()\nr_mean = read_df[\"F_SCORE\"].mean()\nr_std = read_df[\"F_SCORE\"].std()\nprint(f\"The average F-score is {round(r_mean,2)}. Std is {round(r_std,2)}.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Determine which promotions are available to employees in each job class\nIf the previous output data set (variable name: df) is well built then we can simply search that data set. Search column EXP_JOB_CLASS_TITLE and EXP_JOB_CLASS_ALT_RESP for a certain job class to see which promotions are avialable to it. "},{"metadata":{},"cell_type":"markdown","source":"Let's first see a general list for job classes that could be promoted."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def get_all_promotions(dataset=df):\n    '''\n    Takes a pandas DataFrame. Returns a set of job class titles.\n    **Notice**\n    DataFrame, need to include column EXP_JOB_CLASS_TITLE with data type object (string).\n    '''\n    promo_list = []\n    job_ser = dataset[\"EXP_JOB_CLASS_TITLE\"].dropna()\n    for title in job_ser:\n        promo_list += title.split(\"|\")\n\n    return set(promo_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Total of {len(get_all_promotions())} job titles have opportunities to be promoted.\")","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"get_all_promotions() # Run to see the list","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we want to know which promotions are available to employees in each job class. The following funciton help us get the result in the dataframe."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def promotion_search(job_class, dataset=df, vague=False):\n    '''\n    Take in a job class (string), a DataFrame, vague (boolean). Returns rows in the dataframe that match \n    the promotion for that job class. \n    \n    **Notice**\n    Dataset need to have a column name EXP_JOB_CLASS_TITLE and EXP_JOB_CLASS_ALT_RESP \n    with a data type of string (object).\n    \n    Parameters:\n    job_class, string data type, upper or lowercase. e.g, engineer, management analyst, etc.\n    dataset, pandas DataFrame data type, the default is set to the dataframe built previously.\n    vague, boolean data type, determines whether it should do a vague search or not. \n    For example:\n    1. job_class = \"senior management analyst\", vague=False. \n        Only search column EXP_JOB_CLASS_TITLE, returns any row that include \"senior management analyst\".\n    2. job_class = \"software engineer\", vague=True.\n        Search columns EXP_JOB_CLASS_TITLE and EXP_JOB_CLASS_ALT_RESP, return any row that include \n        \"software engineer\" in the two columns.\n\n    '''\n    \n    try:\n        if vague:\n            # Search column EXP_JOB_CLASS_TITLE\n            ret1 = dataset[dataset['EXP_JOB_CLASS_TITLE'].notnull()]\n            ret1 = ret1[ret1['EXP_JOB_CLASS_TITLE'].str.contains(job_class.upper())]\n            # Search column EXP_JOB_CLASS_ALT_RESP\n            ret2 = dataset[dataset['EXP_JOB_CLASS_ALT_RESP'].notnull()]\n            ret2 = ret2[ret2['EXP_JOB_CLASS_ALT_RESP'].str.contains(job_class.lower())]\n            \n            return pd.concat([ret1,ret2]) # Add both result together\n            \n        else:\n            ret = dataset[dataset['EXP_JOB_CLASS_TITLE'].notnull()]\n            return ret[ret['EXP_JOB_CLASS_TITLE'].str.contains(job_class.upper())]\n    except:\n        print(\"Invalid argument for promotion search!\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"# Function return match rows in dataframe, let's just present some particular columns \n# and show only the first five results \npromotion_search(\"management analyst\")[[\"JOB_CLASS_TITLE\",\"EXP_JOB_CLASS_TITLE\"]].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"# If we want to know opportunities for all job class and alternative class,\n# we could do vague search.\npromotion_search(\"architect\", vague=True)[[\"JOB_CLASS_TITLE\",\"EXP_JOB_CLASS_TITLE\",\"EXP_JOB_CLASS_ALT_RESP\"]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Summary & Recommendation\n\n1. **Identify language that can negatively bias the pool of applicants:** search the job description and reaplace all \"bias\" words if possible. A \"bias\" word list could be found on the internet and/or keep on updating a self-kept list. In this notebook we demostrate a rank of \"bias\" words found in 683 job bulletin text files, these bias words could either be masculine-coded or feminine-coded according to the website http://gender-decoder.katmatfield.com/about. The advatage of the method we use here is easy, we just simply search all \"bias\" words in all files and maintain a \"bias\" word list. The biggest disadvantage is that it may not be totally accurate, so following manual inspection is required. For example, in the result shown previously, \"principles\" is the most common word found in all files, but it is in fact often included in course subjects names. Nevertheless, this method could work as a filter for detecting possible bias languages in job descriptions. \n\n\n2. **Improve the diversity and quality of the applicant pool:** \n    * Identify and replacing \"bias\" words could imporve diversity and quality of the applicant pool, since some \"bais\" words many deter talent or potential applicants from applying the job. \n    * Some articles have pointed out that improving readability may improve the diversity of the applicant pool for some positions don't highly depend on readability, thus complex job decription might push away those acturally qualified applicants. In this notebook we demostrate using Flesch Score to test readabiity for all text files, but we are leaving the decision of what should be the proper readability level required for a position to the employer (unless we got a readability survey data for each positions).  \n\n\n3. **Make it easier to determine which promotions are available to employees in each job class:** This is going to be easy if we parse all text files into the final output data set correctly. Because each row in the output data set is a minimun requirement set for a position, hence for any job class appeared in EXP_JOB_CLASS_TITLE, there is a opportunity for that job class to be promoted. Here, we demostate the result by writing a function **promotion_search(\"job_class_name\")**, where we can pass is a job class name and it will return a dataframe with all job openings that match. We could also use **get_all_promotions()** to get a set of all potential job classes for promotion.  "},{"metadata":{},"cell_type":"markdown","source":"-----"},{"metadata":{},"cell_type":"markdown","source":"End of notebook, thanks for reading!"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}