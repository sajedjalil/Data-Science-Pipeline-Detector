{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Data Science for Good: City of Los Angeles"},{"metadata":{},"cell_type":"markdown","source":"This notebook analyses the data given in the \"Data Science for Good: City of Los Angeles\" competition. The problem statement given on the [competition website](https://www.kaggle.com/c/data-science-for-good-city-of-los-angeles) notes the following main challenge:\n\n> The content, tone, and format of job bulletins can influence the quality of the applicant pool. Overly-specific job requirements may discourage diversity. The Los Angeles Mayor’s Office wants to reimagine the city’s job bulletins by using text analysis to identify needed improvements.\n\nTo this end, the following goals are stated in the competition:\n> The goal is to convert a folder full of plain-text job postings into a single structured CSV file and then to use this data to: (1) identify language that can negatively bias the pool of applicants; (2) improve the diversity and quality of the applicant pool; and/or (3) make it easier to determine which promotions are available to employees in each job class.\n\nThis notebook is structured around the 4 parts mentioned in the problem statement above, i.e.,\n* Task 1: creation of a single structured CSV file\n* Task 2: identification of language that can negatively bias the pool of applicants\n* Task 3: improvement of diversity and quality of the applicant pool\n* Task 4: determining available promotions for employees in each job class"},{"metadata":{},"cell_type":"markdown","source":"### Preparatory statements\n\nInstallations and imports."},{"metadata":{"trusted":true},"cell_type":"code","source":"# installations\n!python -m spacy download en_core_web_md\n!pip install multidict\n!pip install vaderSentiment\n!pip install networkx","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# importing required (general) packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt \n\nimport os\n\nimport re\nimport string\n\nfrom graphviz import Digraph\n\nfrom IPython.display import HTML, display\nimport tabulate\nimport ipywidgets as widgets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# increase plot size\n%pylab inline\npylab.rcParams['figure.figsize'] = (15, 10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Task 1: Create a single structured CSV"},{"metadata":{},"cell_type":"markdown","source":"In the first part, we will create a single structured CSV file.\n\nFor the creation of the CSV, in this section, the followings steps are performed:\n\n* read all files\n* process all files and extract the required information in separate functions\n* create data frame\n\nThe following code follows these steps to create a homogenous CSV which follows the given data dictionary and the provided example (`sample job class export template.csv`)`"},{"metadata":{},"cell_type":"markdown","source":"### Read files\n\nMethods for reading files."},{"metadata":{"trusted":true},"cell_type":"code","source":"bulletin_path = \"../input/data-science-for-good-city-of-los-angeles/cityofla/CityofLA/Job Bulletins/\"\nfiles = os.listdir(bulletin_path)\n\nprint(\"There are \", len(files), \" job bulletins available.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def open_bulletin(file):\n    with open(bulletin_path + file, encoding='utf-8', errors='ignore') as f:\n        return \"\".join([line for line in f.readlines() if line.strip()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"titles_path = \"../input/data-science-for-good-city-of-los-angeles/cityofla/CityofLA/Additional data/job_titles.csv\"\nwith open(titles_path) as f:\n       job_titles = [line.lower().strip() for line in f.readlines() if line.strip()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Process files"},{"metadata":{},"cell_type":"markdown","source":"Methods for processing files and helper methods for extracting all the required information."},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_file(file, text):\n    '''parses text and returns an array containing multiple dictionnaries\n    as a result of normalizing the requirements attribute; the result belongs\n    to the text of one single file'''\n    \n    processed_data = process_file_compacted(file, text)\n    \n    requirements = processed_data[\"requirements\"]\n        \n    del processed_data[\"requirements\"]\n    \n    data = []\n    \n    # create separate entry in data for each requirement, where all the attributes\n    # are the same, except the requirement-related ones\n    for requirement in requirements:\n        split_data = processed_data.copy()\n        split_data.update(requirement)\n        data.append(split_data)\n    \n    return data\n\n\ndef process_file_compacted(file, text):\n    '''parses text and returns an array containing multiple dictionnaries\n    as a result of normalizing the requirements attribute; the result belongs\n    to the text of one single file'''\n    \n    data = {\n        \"FILE_NAME\" : get_file_name(file),\n        \"JOB_CLASS_TITLE\" : get_job_class_title(text),\n        \"JOB_CLASS_NO\" : get_job_class_no(text),\n        \"JOB_DUTIES\" : get_job_duties(text),\n        \"DRIVERS_LICENSE_REQ\" : get_drivers_license_requirement(text),\n        \"DRIVERS_LIC_TYPE\" : get_drivers_license_type(text),\n        \"ADDTL_LIC\" : get_additional_license(text),\n        \"EXAM_TYPE\" : get_exam_type(text),\n        \"ENTRY_SALARY_GEN\" : get_dwp_salary(text),\n        \"ENTRY_SALARY_DWP\" : get_gen_salary(text),\n        \"OPEN_DATE\" : get_open_date(text),\n        \"requirements\" : get_requirements(text),\n        #\"jobpost\" : text\n    }\n    return data\n\n\ndef get_file_name(file):\n    return file\n\n\ndef get_job_class_title(text):\n    # job class is given mostly as a upper-case letter string (on its own line)\n    # it appears at the beginning of the document, hence, only the first\n    # result is considered\n    match = re.search(\"^\\s*([A-Z0-9\\s'\\-a-z ()]*)(?: {5,}|\\t|\\n)\", text)\n    if match is None:\n        return None\n    job_class_title = match.group(0).replace(\"CAMPUS INTERVIEWS ONLY\", \"\").replace(\"\\n\", \"\").strip().lower()\n    return job_class_title\n\n\ndef get_job_class_no(text):\n    # job class is a four-digit code\n    # it appears at the beginning of the document, hence, only the first\n    # result is considered    \n    match = re.search(\"([0-9]{4,4})\", text)\n    if match is None:\n        return None\n    job_class_no = match.group(0)\n    return job_class_no\n\n\ndef get_job_duties(text):\n    # the job duties are listed after the title \"DUTIES\";\n    # all the text after the title belongs to this section, up until \n    # the next upper-case title\n    match = re.search(\"DUTIES(?:.*?RESPONSIBILITIES)?[:]?([\\s\\S]*?)(?=[A-Z]{3,})\", text)\n    if match is None:\n        return None\n    duties = match.group(1).strip() \n    return duties\n\n\ndef get_requirements(text):\n    # the requirements are listed after the title \"REQUIREMENT\";\n    # all the text after the title belongs to this section, up until \n    # the next upper-case title\n    # the requirements section is process further and the\n    # single parts of the requirements are analysed and split into\n    # sub-categories\n    match = re.search(\"REQUIREMENT.*\\n([\\s\\S]*?)[A-Z]{3,}\", text)\n    if match is None:\n        return None\n    requirement = match.group(1).strip()\n    requirements_raw = [item.strip() for item in re.split(r'[\\s](?=[0-9]+\\.)', requirement)]\n    \n    requirements = []\n    \n    for index, requirement_text in enumerate(requirements_raw):\n        requirement_set_id = index + 1\n        processed_sub_requirements = process_single_requirement(requirement_set_id, requirement_text)\n        \n        if processed_sub_requirements is not None:\n            for processed_sub_requirement in processed_sub_requirements:\n                requirements.append(processed_sub_requirement)\n        \n    return requirements\n\n\nSUBSET_ID = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\",\n            \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\", \"AA\", \"BB\", \"CC\", \"DD\", \"EE\",\n            \"FF\", \"GG\", \"HH\", \"II\", \"JJ\", \"KK\", \"LL\", \"MM\", \"NN\", \"OO\", \"PP\", \"QQ\", \"RR\", \n            \"SS\", \"TT\", \"UU\", \"VV\", \"WW\", \"XX\", \"YY\", \"ZZ\"]\n\n\ndef process_single_requirement(requirement_set_id, text, requirement_subset_id = \"\", parent_text = \"\", hint = \"\"):\n    # process a single requirement, split its parts and create\n    # a dictionnary reflecting the content of the requirement\n    requirement = {}\n    requirement[\"REQUIREMENTS_SET_ID\"] = requirement_set_id\n    requirement[\"REQUIREMENTS_SUBSET_ID\"] = requirement_subset_id\n\n    has_subrequirement = has_sub_requirements(text)\n    \n    if(has_subrequirement):\n        # the requirement has sub requirements, hence, the requirement\n        # receives only the main-text (and ignores the sub-requirements)\n        requirement_text = re.search(r'([\\s\\S\\n]+)(?:\\sa\\.)', text).group(1).strip()\n    else:\n        requirement_text = text.strip()\n    \n    # split requirement into its part (divided by \"and\" or \"or\")\n    parts_raw = get_requirement_parts(requirement_text)\n    parts = [{\"requirements_parts\" : item } for item in parts_raw]\n    for part in parts:\n        # get details for the requirement part (e.g., educational, course-related, experience-related)\n        part[\"details\"] = get_requirement_details(part[\"requirements_parts\"], hint)       \n    \n    # merge all the received details into the requirement dictionary for the given requirement\n    for part in parts:\n        for key, detail in part[\"details\"].items():\n            if(key not in requirement):\n                requirement[key] = detail\n    \n    processed_requirements = []\n    processed_requirements.append(requirement)\n    \n    # if requirement has sub requirements, these have to be processed as well; this is\n    # done separately\n    if(has_subrequirement):\n        sub_requirements_raw = re.split(r'\\n([a-z]\\..*)', text)\n        sub_requirements_raw = list(filter(None, sub_requirements_raw))\n        sub_requirements_raw = list(filter(lambda i : re.search(r'^[a-z]+\\.', i), sub_requirements_raw))\n        \n        sub_requirements = []\n        \n        # iterate over a. ... b. ... c. ...\n        for index, sub_requirement in enumerate(sub_requirements_raw):\n            sub_requirement_text = re.sub(r'^[a-z]+\\.', \"\", sub_requirement)\n            \n            # as sometimes, the sub-requirement does not contain enough information\n            # to classify a text (and its information) into one of the categories, i.e.,\n            # course-related, educational, experience-related, we pass on a \"hint\" which\n            # comes from the requirement's text, rather than the sub-requirements text\n            hint = \"\"\n            if(bool(re.search(r'experience\\s*in\\s*\\:', requirement_text))):\n                hint = \"experience\"\n            elif(is_course_related(requirement_text)):\n                hint = \"course\"\n            elif(is_educational(requirement_text)):\n                hint = \"education\"\n\n            sub_requirement = process_single_requirement(requirement_set_id,  sub_requirement_text, SUBSET_ID[index], requirement_text, hint)\n            sub_requirements.extend(sub_requirement)\n        \n        processed_requirements.extend(sub_requirements)\n        return processed_requirements \n    \n    return processed_requirements\n\n\ndef has_sub_requirements(text):\n    return bool(re.search(r'[\\n](?=[a-z]+\\.)', text))\n\n\ndef get_requirement_details(text, hint = \"\"):\n    # processes a requirement text based on whether it is course-related, educational\n    # or experience-related and searches for the information to add to\n    # the corresponding attributes\n    # a hint can be passed, to ensure that the right category is chosen\n    \n    use_hint = !(is_course_related(text) or is_educational(text) or is_experience_related(text))\n    \n    part = {}\n    \n    if(is_course_related(text) or (use_hint and hint == \"course\")):\n        part[\"COURSE_COUNT\"] = get_course_count(text)\n        part[\"COURSE_LENGTH\"] = get_course_length(text)\n        part[\"COURSE_SUBJECT\"] = get_course_subject(text)\n        part[\"MISC_COURSE_DETAILS\"] = text\n    elif(is_educational(text) or (use_hint and hint == \"education\")):\n        part[\"EDUCATION_YEARS\"] = get_education_years(text)\n        part[\"SCHOOL_TYPE\"] = get_school_type(text)\n        part[\"EDUCATION_MAJOR\"] = get_education_major(text)\n    elif(is_experience_related(text) or (use_hint and hint == \"experience\")):\n        part[\"EXPERIENCE_LENGTH\"] = get_experience_years(text)\n        part[\"FULL_TIME_PART_TIME\"] = get_full_time_part_time(text)\n        part[\"EXP_JOB_CLASS_TITLE\"] = get_exp_job_class_title(text)\n        part[\"EXP_JOB_CLASS_ALT_RESP\"] = get_exp_job_class_alt_resp(text)\n        part[\"EXP_JOB_CLASS_FUNCTION\"] = get_exp_job_class_function(text)        \n        \n    return part\n        \n\ndef check_text_for_catch_words(text, catch_words):\n    for word in catch_words:\n        if(word in text): \n            return True\n    return False\n    \ndef is_educational(text):\n    # check if a text is educational; this is done by checking for certain catch words\n    catch_words = [\"education\", \"university\", \"college\", \"high school\", \"apprenticeship\"]\n    return check_text_for_catch_words(text, catch_words)\n\n\ndef is_course_related(text):\n    # check if a text is course-related; this is done by checking for certain catch words\n    catch_words = [\"course\"]\n    return check_text_for_catch_words(text, catch_words)\n\ndef is_experience_related(text):    \n    # check if a text is course-related; this is done by checking for certain catch words \n    # or checking if a full-time/part-time information is given or a job experience class\n    catch_words = [\"experience\"]\n    result = check_text_for_catch_words(text, catch_words)\n\n    if(result):\n        return True\n    elif (get_experience_years(text) is not None or get_full_time_part_time(text) is not None or get_exp_job_class_title(text) is not None):\n        return True\n    \n    return False\n\n\ndef get_years(text):\n    # extracts from a text a year/month information, by checking for certain catch words \n    # (i.e., year and month)\n    # note that months are converted to years\n    years_match = re.search(\"([0-9A-Za-z]+)[\\- ]?year[s]?\", text)\n    if years_match is not None:\n        years =  years_match.group(1).strip().lower()\n        int_years = text_to_number(years)\n        \n        if int_years is not None:\n            return float(int_years)\n        else:\n            return None\n    \n    months_match = re.search(\"([0-9A-Za-z]+)[\\- ]?month[s]?\", text)\n    if months_match is not None:\n        months =  months_match.group(1).strip().lower()\n        int_months = text_to_number(months)\n        \n        if int_months is not None:\n            return round(float(int_months) / 12.0, 2)\n        else:\n            return None\n\n        \ndef get_experience_years(text):\n    return get_years(text)\n\n\ndef get_education_years(text):\n    return get_years(text)\n\n\ndef text_to_number(text):\n    # translates a textual information of a number into a integer\n    try:\n        i = int(text)\n        return i\n    except ValueError:\n        typical_numbers = {\"one\" : 1, \"two\" : 2, \"three\" : 3, \"four\" : 4, \"five\" : 5, \"six\" : 6, \n                          \"seven\" : 7, \"eight\" : 8, \"nine\" : 9, \"ten\" : 10, \"eleven\" : 11, \"twelve\" : 12,\n                          \"thirteen\" : 13, \"fourteen\" : 14, \"fifteen\" : 15, \"sixteen\" : 16, \"seventeen\" : 17,\n                          \"eighteen\" : 18, \"nineteen\" : 19, \"twenty\" : 20, \"thirty\": 30, \"fourty\" : 40, \"fifty\" : 50,\n                          \"sixty\" : 60, \"seventy\" : 70, \"eighty\" : 80, \"ninety\" : 90}\n        \n        if(text in typical_numbers):     \n            return typical_numbers[text]\n        else:\n            return None\n    \n    \ndef get_school_type(text):\n    # extracts school type information, i.e., college/university, high school, apprenticeship\n    if(\"college\" in text or \"university\" in text):\n        return \"COLLEGE OR UNIVERSITY\"\n    elif(\"high school\" in text):\n        return \"HIGH SCHOOL\"\n    elif(\"apprenticeship\" in text):\n        return \"APPRENTICESHIP\"\n    else:\n        None\n\n        \ndef get_education_major(text):\n    # extracts information on major/concentration in education-related texts\n    # it splitts the majors by or/and and joins them by a unified '|' symbol\n    match = re.search(r\"(?:major|concentration)\\s*in\\s*([\\s\\S]*)(?:[;]+|or a closely related field)\", text)\n    if match is not None:\n        majors_text = match.group(1).strip()\n        majors_text = re.sub(r'\\([^)]*\\)', '', majors_text) #remove information in brackets\n        majors_split = re.split(';|,|\\sor\\s|\\sand\\s', majors_text)\n        majors = [item.strip().lower() for item in majors_split]\n        majors = list(filter(None, majors))\n        return \"|\".join(majors)\n    return None\n\n\ndef get_requirement_parts(text):\n    # gets the single parts of a requirement (which are connected by an and or an or)\n    matches = re.split(r'\\b(?:and|or)(?=[\\s][A-Za-z0-9]*?[- ](?:year[s]|month[s]))', text)\n    matches = list(filter(None, matches))\n    if(len(matches) > 1):\n        return [item.strip() for item in matches]\n    return [text]\n\n\ndef get_full_time_part_time(text):\n    # extracts full-time or part-time information from text\n    if(\"full time\" in text or \"full-time\" in text):\n        return \"FULL_TIME\"\n    elif(\"part_time\" in text or \"part-time\" in text):\n        return \"PART_TIME\"\n    else:\n        return None\n    \n    \ndef get_exp_job_class_title(text):\n    # extracts the job class from a text by comparing it to a job titles dictionnary\n    for word in job_titles:\n        if(word in text.lower()): \n            return word\n    return None\n\ndef get_exp_job_class_alt_resp(text):\n    # extracts the alternative job class from a text by searching for \"in a class\"\n    match = re.search(r'(?:in a class)(.*)[.,;!\\n]', text)\n    if match is not None:\n        return match.group(1).strip()\n    return None   \n\ndef get_exp_job_class_function(text):\n    #extracts the job class function by searching for the string \"experience\"\n    match = re.search(r'(?:[0-9a-z]*\\s*)?(?:.*?)(?:[Ee]xperience\\s*\\S*?\\s*(?:an|a)?\\s*)((?:with)?.*?)(?:[;.\\n])', text)\n    if match is not None:\n        return match.group(1).strip()\n    return None   \n\ndef get_course_count(text):\n    # extracts the number of courses\n    match = re.search(r'([A-Za-z0-9]*)\\scourse[s]?', text)\n    if match is not None:\n        return text_to_number(match.group(1).strip())\n    return None    \n\n\ndef get_course_length(text):\n    # extracts the semester or quarters for a course, the information is \n    # returned in a unified schema with 'S' denoting semesters, 'Q' quarters and\n    # both information separated by a '|'\n    semester_match = re.search(r'([A-Za-z0-9]+)\\ssemester[s]?', text)\n    quarters_match = re.search(r'([A-Za-z0-9]+)\\squarter[s]?', text)\n    \n    if semester_match is not None:\n        semesters = text_to_number(semester_match.group(1).strip())\n    else:\n        semesters = None\n    \n    if quarters_match is not None:\n        quarters = text_to_number(quarters_match.group(1).strip())\n    else:\n        quarters = None\n\n    if semesters is not None and quarters is not None:\n        return str(quarters) + \"Q\" + \"|\"+ str(semesters) + \"S\"\n    elif semesters is not None:\n        return str(semesters) + \"S\"\n    elif quarters is not None:\n        return str(quarters) + \"Q\"\n    return None    \n\n\ndef get_course_subject(text):\n    # extracts the subject of the courses; the information is returned\n    # in a unified schema where the subjects are separated by a '|'\n    match = re.search(r'course[s]?(?:.*?)(?:in |:)(.*)', text)\n    if match is not None:\n        subjects_text = match.group(1).strip()\n        subjects_text = re.sub(r'\\([^)]*\\)', '', subjects_text) #remove information in brackets\n        subjects_splitted = re.split(\"[,;]\", subjects_text)\n        subjects = [re.sub(r'[\\.,;]', '', item) for item in subjects_splitted]\n        subjects = [item.strip().lower() for item in subjects]\n        subjects = [re.sub(r'^(or|and|either)\\s', '', item) for item in subjects] #remove leading or/and\n        subjects = [re.sub(r'\\s(or|and)$', '', item) for item in subjects] #remove ending or/and   \n        subjects = [re.sub(r'^\\s*(or|and)\\s*$', '', item) for item in subjects] #remove empty or/and  \n        subjects = list(filter(None, subjects))\n        return \"|\".join(subjects)\n    return None    \n\n\ndef get_drivers_license_requirement(text):\n    # extracts requirements for driver's license from the text; if a sentence mentioning\n    # the driver's license contains the words may/might, a \"P\" is returned (for possible),\n    # whereas if a driver's license is truly required, a \"R\" is returned\n    sentences = text.split(\".\")\n    requirements = list(filter(lambda x: re.search(r\"driver.* license\", x), sentences))\n    if(requirements):\n        for requirement in requirements:\n            match = re.search(\"\\s*(may|might)\\s*\", requirement, re.IGNORECASE)\n            if match is not None:\n                return \"P\"\n            else:\n                return \"R\"\n    else:\n        return \"\"\n\n    \ndef get_drivers_license_type(text):\n    # extracts requirements for driver's license type from the text\n    # the information is returned in a unified format\n    sentences = text.split(\".\")\n    requirements = list(filter(lambda x: re.search(r\"driver.* license\", x), sentences))\n    for requirement in requirements:\n        match = re.search(\"[Cc]lass\\s*([A-Z1-9\\/]{1,3}(?:(?:\\s*or\\s*|\\s*\\,\\s*)(?:[Cc]lass\\s*)?[A-Z1-9\\/]{1,3})*)\", requirement, re.IGNORECASE)\n        if match is not None:\n            return match.group(1).strip().replace(\"or\", \",\").replace(\"Class\", \"\").replace(\"class\", \"\").replace(\" \", \"\")\n    return None\n\n\ndef get_additional_license(text):\n    # extracts additional license requirements which do not involve the driver's license\n    match = re.search(\"REQUIREMENT([\\s\\S]*)\", text, re.IGNORECASE)\n    text = match.group(1)\n    sentences = [sentence.strip() for sentence in re.split(\"[\\.]\", text)]\n    return \"\\n\".join(list(filter(lambda x: re.search(r\"(?<!driver's) license\", x), sentences)))    \n\n\ndef get_exam_type(text):\n    # extracts information on the examination, 'EXAMINATION' is used as catch word as the category\n    # is decided based on the appearing word within the same sentence, i.e., \n    # \"INTERDEPARTMENTAL\" and \"OPEN\", \"INTERDEPARTMENTAL\", \"OPEN\", \"DEPARTEMENTAL\"\n    sentences = text.replace(\"\\n\", \"\").split(\".\")\n    examinations = list(filter(lambda x: re.search(r\"(?:THIS|THE)\\s*EXAMINATION\", x), sentences))\n    \n    if(examinations):\n        examination = re.search(r\"(([A-Z]*\\s)*)\", examinations[0]).group(0).replace('\\n', ' ').strip()\n        \n        if(\"INTERDEPARTMENTAL\" in examination and \"OPEN\" in examination):\n            return \"OPEN_INT_PROM\"\n        elif(\"INTERDEPARTMENTAL\" in examination):\n            return \"INT_DEP_PROM\"\n        elif(\"DEPARTMENTAL\" in examination):\n            return \"DEPT_PROM\"\n        elif(\"OPEN\" in examination):\n            return \"OPEN\"\n        else:\n            return None\n\n        \ndef get_dwp_salary(text):\n    # extracts the salary information at DWP; the information is searched by looking for\n    # lines which contain information with a dollar symbol ($)\n    # to consider it as a DWP salary, the word \"Department\" should appear in the same line\n    # the information is returned in a unified format: either as a single number or with a\n    # range separated by a '-'\n    lines = text.split(\"\\n\")\n    salary_lines = list(filter(lambda x: re.search(r\"\\$\", x), lines))\n\n    for salary_line in salary_lines:\n        if(\"Department\" in salary_line or \"department\" in salary_line):\n            match = re.search(r'\\$\\s?[0-9,]*(?:\\s*(?:to|-)\\s*\\$\\s?[0-9,]*)?', salary_line)\n            if match is not None:\n                salary = match.group(0).strip()\n                return re.sub(r\"\\s*to\\s*\", \"-\", salary)\n\n            \ndef get_gen_salary(text):\n    # extracts the salary information; the information is searched by looking for\n    # lines which contain information with a dollar symbol ($)\n    # the information is returned in a unified format: either as a single number or with a\n    # range separated by a '-'\n    lines = text.split(\"\\n\")\n    salary_lines = list(filter(lambda x: re.search(r\"\\$\", x), lines))\n\n    for salary_line in salary_lines:\n        if(\"Department\" not in salary_line and \"department\" not in salary_line):\n            match = re.search(r'\\$\\s?[0-9,]*(?:\\s*(?:to|-)\\s*\\$\\s?[0-9,]*)?', salary_line)\n            if match is not None:\n                salary = match.group(0).strip()\n                return re.sub(r\"\\s*to\\s*\", \"-\", salary)\n  \n            \ndef get_open_date(text):\n    # extracts the open date information by searching for the title \"Open Date\" and a \n    # text fromatted like a date\n    match = re.search(r'Open\\s*Date\\s*:\\s*([0-9]{1,2}\\-[0-9]{1,2}\\-[0-9]{2,4})', text, re.IGNORECASE)\n    if match is not None:\n        return match.group(1).strip()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Read all files, create data frame and export data frame as CSV"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_data():\n    '''reads the data from all files and creates a\n    data frame with all the extracted information\n    '''\n    data = []\n\n    for file in files:\n        text = open_bulletin(file)\n\n        results = process_file(file, text)\n\n        for result in results:\n            data.append(result)\n\n    df = pd.DataFrame.from_dict(data)\n    df.fillna(value=pd.np.nan, inplace=True)\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = get_data()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data is exported into `export.csv`. Note that `\"` is used as a text qualifier."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.to_csv(\"export.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Task 2: Identify Language that can Negatively Bias the Pool of Applicants"},{"metadata":{},"cell_type":"markdown","source":"### Method\n\nIn the following task, we only focus on the job duties (`JOB_DUTIES`) attribute for the analysis of the language, as it is possibly the most important field and where the language is comparably unconstrained. \n\nFor this task, we use the VADER sentiment analysis tools, as available on [GitHub](https://github.com/cjhutto/vaderSentiment) and as introduced in:\n\n> Hutto, Clayton J., and Eric Gilbert. \"Vader: A parsimonious rule-based model for sentiment analysis of social media text.\" Eighth international AAAI conference on weblogs and social media. 2014.\n\nThe VADER package is used to receive a valence score denoting how positive/negative the language used in the text is."},{"metadata":{},"cell_type":"markdown","source":"We first introduce the methods used for the analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Moreover, the spaCy package is used for tokenization, lemmatization and normalization of the sentences. spaCy is available on [Github](https://github.com/explosion/spaCy)."},{"metadata":{"trusted":true},"cell_type":"code","source":"import spacy\nfrom spacy.lang.en import English","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sm_parser = spacy.load(\"en_core_web_sm\")\nstopwords = spacy.lang.en.stop_words.STOP_WORDS\npunctuations = string.punctuation\n\nsm_parser.max_length = 51391693","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenize_clean_normalize(text):\n    '''tokenizes, cleans and normalizes a given text, i.e., the text is split/parsed, lower-cased, stripped from\n    any whitespace or other characters, words are lemmatized, and stopwords (as well as punctuations) are removed'''\n    \n    if(text is not None and len(str(text))) > 0:\n        tokens = sm_parser(str(text))\n        tokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in tokens ]\n        tokens = [ word for word in tokens if word not in stopwords and word not in punctuations ]\n        return tokens\n    else:\n        return []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"analyzer = SentimentIntensityAnalyzer()\n\ndef analyze_sentiment_document(text):\n    '''returns the valence for a document, i.e., the mean over the valence of the single words; \n    a positive valence denotes positive sentiment in the text (max: 1.0), \n    a negative valence denotes negative sentiment in the text (min: -1.0)'''\n    \n    try:\n        if(text is not None and len(str(text))) > 0:\n            document = sm_parser(str(text))\n            analysis = [analyzer.polarity_scores(sentence.text)[\"compound\"] for sentence in document.sents]\n            return np.mean(analysis)\n        else:\n            return None\n    except:\n        return 0.0\n\n    \ndef analyze_sentiment_word(word):\n    '''returns the valence for a single word; \n    a positive valence denotes positive sentiment in the text (max: 1.0), \n    a negative valence denotes negative sentiment in the text (min: -1.0)'''\n    \n    try:\n        analysis = analyzer.polarity_scores(word)\n        return analysis[\"compound\"]\n    except:\n        return 0.0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the following, we introduce a method to analyze the readability of a text (using the [Automated readability index](https://en.wikipedia.org/wiki/Automated_readability_index))."},{"metadata":{"trusted":true},"cell_type":"code","source":"def analyze_readability(text):\n    if(text is not None and len(str(text))) > 0:\n        text = str(text)\n        num_characters = len(re.sub(r\"[^a-zA-Z0-9]\",\"\", text))\n\n        tokens = list(sm_parser(str(text)))\n        tokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in tokens ]\n        tokens = [ word for word in tokens if word not in punctuations ]\n        num_words = len(tokens)\n\n        num_sentences = len(re.sub(r\"[^\\.;:?!]\",\"\", text))\n\n        score = 4.71 * (num_characters / max(num_words, 1)) + 0.5 * (num_words / max(num_sentences, 1)) - 21.43\n        return int(score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"job_posts = get_data()[[\"JOB_DUTIES\", \"FILE_NAME\"]].drop_duplicates()\njob_posts[\"valence\"] = job_posts[\"JOB_DUTIES\"].apply(analyze_sentiment_document)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the following, we plot the documents by their valence, where a valence of -1.0 denotes most extreme negative and +1.0 most extreme positive. As the plot shows, some job postings have a positive valence, whereas others contain negative language."},{"metadata":{"trusted":true},"cell_type":"code","source":"def valence_colors(lst):\n    cols = []\n    for l in lst:\n        if l < 0:\n            cols.append('red')\n        elif l > 0:\n            cols.append('green')\n        else:\n            cols.append('grey')\n    return cols\n\nplt.scatter(x = range(0, len(job_posts)), y = job_posts.sort_values([\"valence\"])[\"valence\"], c = valence_colors(job_posts.sort_values([\"valence\"])[\"valence\"]))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To exemplify, a document with a high positive valence, is the following one:"},{"metadata":{"trusted":true},"cell_type":"code","source":"high_valence_file = job_posts.iloc[job_posts[\"valence\"].idxmax()][\"FILE_NAME\"]\nhigh_valence_file","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pdf_file = \"../input/data-science-for-good-city-of-los-angeles/cityofla/CityofLA/Additional data/PDFs/2015/March 2015/03062015/SIGN SHOP SUPERVISOR 3419.pdf\"\nfrom wand.image import Image as WImage\nhigh_valence_post = WImage(filename=pdf_file)\nhigh_valence_post","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Methods for displayal of positive/negative wording."},{"metadata":{},"cell_type":"markdown","source":"For the following analysis, we use the WordCloud package which is also available on [Github](https://github.com/amueller/word_cloud)."},{"metadata":{"trusted":true},"cell_type":"code","source":"from PIL import Image\nfrom wordcloud import WordCloud, ImageColorGenerator\nimport multidict\nimport colorsys\nfrom gensim.corpora.dictionary import Dictionary\nimport matplotlib.colors as colors","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_color_for_word(word, font_size, position,orientation,random_state=None, **kwargs):\n    # returns a color value for a given word, where positive words are marked in green\n    # negative words are marked in red\n    valence = analyze_sentiment_word(word)\n    valence = (valence + 1.0) / 2.0\n    \n    cdict = {'red':   ((0.0, 0.0, 0.0), (0.5, 0.5, 0.5), (1.0, 1.0, 1.0)),\n             'green': ((0.0, 0.5, 0.5), (0.5, 0.5, 0.5), (1.0, 0.0, 0.0)),\n             'blue':  ((0.0, 0.0, 0.0), (0.5, 0.5, 0.5), (1.0, 0.0, 0.0))\n            }\n    \n    cmap = colors.LinearSegmentedColormap('GnRd', cdict)\n    rgba = cmap(1-valence)\n    color = \"rgb(\" + str(int(rgba[0] * 255)) + \",\" + str(int(rgba[1] * 255)) + \",\" + str(int(rgba[2] * 255)) + \")\"\n    return(color) \n        \n\ndef is_words_acceptable_for_wordcloud(word):\n    # checks if word is worthy to be considered in word cloud (e.g., we remove months,\n    # numbers)\n    months = ['january', 'february', 'march', 'april', 'may', 'june', 'july',\n          'august', 'september', 'october', 'november', 'december']\n    \n    if(word in months or is_number(word)):\n        return False\n    \n    return True\n\n\ndef is_number(word):\n    # checks if a word is a number\n    try:\n        float(word)\n        return True\n    except ValueError:\n        return False\n    \n    \ndef generate_word_cloud(documents, min_valence = -1.0, max_valence = 1.0, max_words = 20):\n    ''' generates a word cloud by processing the given documents; a min_valence and max_valence\n    can be passed to the method (e.g., to only display negatively connotated or positively connotated\n    words)'''\n    \n    tokenized_documents = [tokenize_clean_normalize(document) for document in documents]\n    \n    dictionary = Dictionary(tokenized_documents)\n    dictionary.compactify()\n\n    mdictionary = multidict.MultiDict()\n\n    for key, value in dictionary.dfs.items():\n        valence = analyze_sentiment_word(dictionary[key])\n        if(is_words_acceptable_for_wordcloud(dictionary[key]) and  valence > min_valence and valence < max_valence):\n            mdictionary.add(dictionary[key], value)\n        \n    wc = WordCloud(background_color=\"white\", max_words=max_words)\n    wc.generate_from_frequencies(mdictionary)\n    wc.recolor(color_func=get_color_for_word)\n\n    return (dictionary, wc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following word clouds display positive words used in the job posts in green, negative words in red."},{"metadata":{"trusted":true},"cell_type":"code","source":"(dictionary, wc_positive) = generate_word_cloud(job_posts[\"JOB_DUTIES\"], 0.0, 1.0)\nplt.imshow(wc_positive, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(dictionary, wc_negative) = generate_word_cloud(job_posts[\"JOB_DUTIES\"], -1.0, 0.0)\nplt.imshow(wc_negative, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To not limit the selection of words to the ones used already in the posts, we use an external dataset (from [Kaggle](https://www.kaggle.com/madhab/jobposts)) of 19'000 online job posts to analyse what other words might be used for the job postings and which ones should be used/avoided. The following plot shows the distribution of the valence value for the job posts found in the external dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"online_posts = pd.read_csv(\"../input/jobposts/data job posts.csv\").sample(1000)\nonline_posts[\"valence\"] = online_posts[\"JobDescription\"].apply(analyze_sentiment_document)\nplt.scatter(x = range(0, len(online_posts)), y = online_posts.sort_values([\"valence\"])[\"valence\"], c = valence_colors(online_posts.sort_values([\"valence\"])[\"valence\"]))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following table shows the distribution of the LA job posts in the different categories (positive / neutral / negative language) and the distribution of the categories in the external data sets. We can see that the LA data set has comparably more job posts with a negative connotated language than the external data set."},{"metadata":{"trusted":true},"cell_type":"code","source":"perc_pos_job_posts = job_posts[job_posts[\"valence\"] > 0].shape[0] / job_posts.shape[0]\nperc_neut_job_posts = job_posts[job_posts[\"valence\"] == 0].shape[0] / job_posts.shape[0]\nperc_neg_job_posts = job_posts[job_posts[\"valence\"] < 0].shape[0] / job_posts.shape[0]\n\n\nperc_pos_online_posts = online_posts[online_posts[\"valence\"] > 0].shape[0] / online_posts.shape[0]\nperc_neut_online_posts = online_posts[online_posts[\"valence\"] == 0].shape[0] / online_posts.shape[0]\nperc_neg_online_posts = online_posts[online_posts[\"valence\"] < 0].shape[0] / online_posts.shape[0]\n\ntable = [[\"\",\"LA job posts\",\"External dataset\"],\n         [\"positive\", str(round(perc_pos_job_posts * 100, 2)) + \"%\", str(round(perc_pos_online_posts * 100, 2)) + \"%\" ],\n         [\"neutral\",  str(round(perc_neut_job_posts * 100, 2)) + \"%\", str(round(perc_neut_online_posts * 100, 2)) + \"%\"],\n         [\"negative\", str(round(perc_neg_job_posts * 100, 2)) + \"%\", str(round(perc_neg_online_posts * 100, 2)) + \"%\"]]\ndisplay(HTML(tabulate.tabulate(table, tablefmt='html')))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the following word clouds, we mark with green the words that have a postive valence and are used in the external data sets. These word may be used as an inspiration to improve job posts."},{"metadata":{"trusted":true},"cell_type":"code","source":"(dictionary, wc_online_positive) = generate_word_cloud(online_posts[\"JobDescription\"], 0.0, 1.0)\nplt.imshow(wc_online_positive, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Similarly, in the following, we display words which should be avoided in job postings (from external data source)."},{"metadata":{"trusted":true},"cell_type":"code","source":"(dictionary, wc_online_negative) = generate_word_cloud(online_posts[\"JobDescription\"], -1.0, 0.0)\nplt.imshow(wc_online_negative, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In summary, here, we present a word list with positive words which should be used more (sorted by valence):"},{"metadata":{"trusted":true},"cell_type":"code","source":"positive_words = list(wc_positive.words_.keys()) + list(wc_online_positive.words_.keys())\npositive_dictionary = pd.DataFrame.from_dict([{\"word\" : word, \"valence\" : analyze_sentiment_word(word)} for word in positive_words]).drop_duplicates()\npositive_dictionary.sort_values(by=\"valence\", ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And similarly the words which should rather be avoided (again, sorted by valence):"},{"metadata":{"trusted":true},"cell_type":"code","source":"negative_words = list(wc_negative.words_.keys()) + list(wc_online_negative.words_.keys())\nnegative_dictionary = pd.DataFrame.from_dict([{\"word\" : word, \"valence\" : analyze_sentiment_word(word)} for word in negative_words]).drop_duplicates()\nnegative_dictionary.sort_values(by=\"valence\").head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Considering the complexity of the wording, we may report the following."},{"metadata":{"trusted":true},"cell_type":"code","source":"job_posts = get_data()[[\"JOB_DUTIES\", \"FILE_NAME\"]].drop_duplicates()\njob_posts[\"readability\"] = job_posts[\"JOB_DUTIES\"].apply(analyze_readability)\n\nplt.scatter(x = range(0, len(job_posts)), y = job_posts.sort_values([\"readability\"])[\"readability\"])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Recommendations and Evaluation\n\n* Improve the wording of the job posts, by using more positively connotated words. Positive words include words like \"successful\", \"effective\", \"motivated\", \"benefit\".\n* Of course, our analysis is based on the valence given by the VADER tool. A negative valence may result, however, due to the job's true nature; consider a job posting for a criminal prosecutor who is involved in researching \"fraud, violence, accidents, etc.\" - the negative valence cannot be avoided in such a case. We have, however, not considered this in our analysis.\n* Incrementing the existing job posting tool to suggest different wordings to avoid a negative language might be a useful approach."},{"metadata":{},"cell_type":"markdown","source":"## Task 3: Improve the Diversity and Quality of the Applicant Pool"},{"metadata":{},"cell_type":"markdown","source":"### Method\n\nWe consider the task of improving diversity and the quality of the applicant pool by looking at the aspect of gendered wording.\n\nTo approach the problem, we follow the ideas and the word list given in the following paper:\n \n> Gaucher, D., Friesen, J., & Kay, A. C. (2011). Evidence that gendered wording in job advertisements exists and sustains gender inequality. Journal of personality and social psychology, 101(1), 109.\n\nThe authors provide in this paper, a list of masculine related and feminine related words which influence whether a text is seen as being directed towards a male or a female person. The word lists are the following:"},{"metadata":{"trusted":true},"cell_type":"code","source":"masculine_words = ['active', 'adventurous', 'aggress', 'ambitio', 'analy', 'assert', 'athlet', 'autonom', 'battle', 'boast', 'challeng', 'champion', 'compet', 'confident', 'courag', 'decid', 'decision', 'decisive', 'defend', 'determin', 'domina', 'dominant', 'driven', 'fearless', 'fight', 'force', 'greedy', 'head-strong', 'headstrong', 'hierarch', 'hostil', 'impulsive', 'independen', 'individual', 'intellect', 'lead', 'logic', 'objective', 'opinion', 'outspoken', 'persist', 'principle', 'reckless', 'self-confiden', 'self-relian', 'self-sufficien', 'selfconfiden', 'selfrelian', 'selfsufficien', 'stubborn', 'superior', 'unreasonab']\nfeminine_words = ['agree', 'affectionate', 'child', 'cheer', 'collab', 'commit', 'communal', 'compassion', 'connect', 'considerate', 'cooperat', 'co-operat', 'depend', 'emotiona', 'empath', 'feel', 'flatterable', 'gentle', 'honest', 'interpersonal', 'interdependen', 'interpersona', 'inter-personal', 'inter-dependen', 'inter-persona', 'kind', 'kinship', 'loyal', 'modesty', 'nag', 'nurtur', 'pleasant', 'polite', 'quiet', 'respon', 'sensitiv', 'submissive', 'support', 'sympath', 'tender', 'together', 'trust', 'understand', 'warm', 'whin', 'enthusias', 'inclusive', 'yield', 'share', 'sharin']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the following, we define a function to compute the gender bias of a text towards one gender (in the following, we will denote the score as gender bias score). The gender bias score ranges from -1.0 denoting a strongly masculine-oriented language to 1.0 denoting a strongly feminine-oriented language."},{"metadata":{"trusted":true},"cell_type":"code","source":"def analyze_gendered_words(text):\n    '''returns a tuple denoting what percentage of the words of the full text is masculine,\n    and what percentage is feminine'''\n    \n    m_sum = 0\n    f_sum = 0\n    \n    nwords = len(str(text).split(\" \"))\n    \n    for word in masculine_words:\n        if(word in str(text)):\n            m_sum += 1\n\n    for word in feminine_words:\n        if(word in str(text)):\n            f_sum += 1\n            \n    return (m_sum / float(nwords), f_sum / float(nwords))\n\n\ndef analyze_gender_bias(text):\n    '''returns a bias score where -1.0 denotes a strongly masculine-oriented language\n    and 1.0 denotes a strongly feminine-oriented language'''  \n    \n    result = analyze_gendered_words(text)\n    return (result[1] - result[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"job_posts = get_data()[[\"FILE_NAME\", \"JOB_CLASS_TITLE\", \"JOB_DUTIES\", \"ENTRY_SALARY_GEN\"]].drop_duplicates()\njob_posts[\"gender_bias\"] = job_posts[\"JOB_DUTIES\"].apply(analyze_gender_bias)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following plot and the corresponding numbers below show that - although there is a large number of posts formulated in a gender-neutral language - a great part is gendered:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def gender_bias_colors(lst):\n    cols = []\n    for l in lst:\n        if l < 0:\n            cols.append('blue')\n        elif l > 0:\n            cols.append('red')\n        else:\n            cols.append('grey')\n    return cols\n\nplt.scatter(x = range(0, len(job_posts)), y = job_posts.sort_values([\"gender_bias\"])[\"gender_bias\"], c = gender_bias_colors(job_posts.sort_values([\"gender_bias\"])[\"gender_bias\"]))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"postings with male-oriented language:\", len(job_posts.loc[job_posts['gender_bias'] < 0]))\nprint(\"postings with female-oriented language:\", len(job_posts.loc[job_posts['gender_bias'] > 0]))\nprint(\"postings with neutral language:\", len(job_posts.loc[job_posts['gender_bias'] == 0]))\n\nprint(\"\\n\")\nprint(\"mean:\", job_posts['gender_bias'].mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the following, we analyse whether the language is related to salary or job class."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_ceil_entry_salary(text):\n    n = str(text).strip()\n    \n    if n is None or len(str(n)) == 0:\n        return None\n    else:\n        res = str(n).replace(\"$\",\"\").split(\"-\")\n        if(len(res) > 1):\n            return float(res[1].replace(\",\", \"\"))\n        elif(len(res[0]) > 0):\n            return float(res[0].replace(\",\", \"\"))\n        else:\n            return None","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The analysis finds no correlation between the salary and the gender bias score, as shown by the Pearson correlation coefficient:"},{"metadata":{"trusted":true},"cell_type":"code","source":"job_posts[\"CEIL_ENTRY_SALARY_GEN\"] = job_posts[\"ENTRY_SALARY_GEN\"].apply(get_ceil_entry_salary)\nfiltered_job_posts = job_posts[job_posts[\"CEIL_ENTRY_SALARY_GEN\"].notnull()]\nfiltered_job_posts = filtered_job_posts[job_posts[\"gender_bias\"].notnull()]\nfiltered_job_posts['CEIL_ENTRY_SALARY_GEN'].corr(filtered_job_posts['gender_bias'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the following, we analyse whether a gender biased language is related to the position. We compare the mean valence for positions where the class title contains certain words (e.g., \"senior\", \"supervisor\", \"head\" denoting senior-level positions; \"junior\", \"assistant\", \"secretary\", \"entry\" denoting entry-level positions)."},{"metadata":{"trusted":true},"cell_type":"code","source":"senior_level = pd.concat([\n    job_posts[job_posts[\"JOB_CLASS_TITLE\"].str.contains(\"supervisor\", case=False, na=False)],    \n    job_posts[job_posts[\"JOB_CLASS_TITLE\"].str.contains(\"head\", case=False, na=False)],    \n    job_posts[job_posts[\"JOB_CLASS_TITLE\"].str.contains(\"senior\", case=False, na=False)],    \n    job_posts[job_posts[\"JOB_CLASS_TITLE\"].str.contains(\"director\", case=False, na=False)],\n    job_posts[job_posts[\"JOB_CLASS_TITLE\"].str.contains(\"inspector\", case=False, na=False)],\n    job_posts[job_posts[\"JOB_CLASS_TITLE\"].str.contains(\"principal\", case=False, na=False)]],ignore_index=True).drop_duplicates().reset_index(drop=True)\n\n\nentry_level = pd.concat([\n    job_posts[job_posts[\"JOB_CLASS_TITLE\"].str.contains(\"junior\", case=False, na=False)],\n    job_posts[job_posts[\"JOB_CLASS_TITLE\"].str.contains(\"assistant\", case=False, na=False)],\n    job_posts[job_posts[\"JOB_CLASS_TITLE\"].str.contains(\"secretary\", case=False, na=False)]\n],ignore_index=True).drop_duplicates().reset_index(drop=True)\n\n\nprint(\"senior-level, mean:\", senior_level['gender_bias'].mean())\nprint(\"entry-level, mean:\", entry_level['gender_bias'].mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is a slight tendency for senior-level positions to have a male-oriented language, and entry-level positions a feminine-oriented language. While this tendency is only subtle, it should be considered further in more detail. The same is true for jobs with a technical orientation (i.e., where the job class title is \"engineering\", \"technic\", \"mechanical\") vs. job with a nursing focus (i.e., where the job class title is \"education\", \"nurse\"):"},{"metadata":{"trusted":true},"cell_type":"code","source":"technical = pd.concat([\n    job_posts[job_posts[\"JOB_CLASS_TITLE\"].str.contains(\"engineering\", case=False, na=False)],\n    job_posts[job_posts[\"JOB_CLASS_TITLE\"].str.contains(\"technical\", case=False, na=False)],\n    job_posts[job_posts[\"JOB_CLASS_TITLE\"].str.contains(\"mechanic\", case=False, na=False)]],ignore_index=True).drop_duplicates().reset_index(drop=True)\n\neducation = pd.concat([\n    job_posts[job_posts[\"JOB_CLASS_TITLE\"].str.contains(\"nurse\", case=False, na=False)],\n    job_posts[job_posts[\"JOB_CLASS_TITLE\"].str.contains(\"care\", case=False, na=False)]],ignore_index=True).drop_duplicates().reset_index(drop=True)\n\nprint(\"technical jobs, mean:\", technical['gender_bias'].mean())\nprint(\"education jobs, mean:\", education['gender_bias'].mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Recommendations and Evaluation\n\n* Ensure a balanced use of feminine and masculine words. Male-oriented words include words like \"adventurous\", \"aggressive\", \"challengin\", \"reckless\", \"selfconfident\", \"superior\"; female-oriented words include words like \"affectionate\", \"considerate\", \"compassion\", \"emotional\", \"trust\", \"understanding\", \"sharing\n* Following scientific practice, of course, more studies should be performed whether this list is complete and whether all words are truly biased towards one gender. In this task, we follow the results of the study by Gaucher et al..\n* Incrementing the existing job posting tool to suggest different wordings to avoid a gender-biased language might be a useful approach."},{"metadata":{},"cell_type":"markdown","source":"### Future Work\n\nTo improve the pool of applicants, our idea was to possibly increase in the first place the number of applicants. To this end, we analyzed the [WUZZUF Jobs Posts](https://www.kaggle.com/WUZZUF/wuzzuf-job-posts) dataset and built a machine learning pipeline to predict the number of views using the job post. However, the experiment failed (and is, hence, not listed here), as the training set was too small and for training a regression. Nevertheless, we add this idea here, as it might be used for future research."},{"metadata":{},"cell_type":"markdown","source":"## Task 4: Make it easier to Determine which Promotions are Available to Employees in each Job Class"},{"metadata":{},"cell_type":"markdown","source":"### Method\n\nTo answer the question of possible promotions for employees in each job class, the goal is to graphically display for each job the promotional possibilities."},{"metadata":{"trusted":true},"cell_type":"code","source":"job_posts = get_data()[[\"FILE_NAME\", \"JOB_CLASS_TITLE\", \"EXP_JOB_CLASS_TITLE\", \"EXPERIENCE_LENGTH\", \"FULL_TIME_PART_TIME\"]]\njob_posts_num_of_exp_job = job_posts[\"EXP_JOB_CLASS_TITLE\"].notnull().groupby([job_posts[\"FILE_NAME\"], job_posts[\"JOB_CLASS_TITLE\"]]).sum().reset_index(name ='num_of_exp_job_class_title')\n\n # leafs, i.e., entry jobs\njobs_without_exp_job = job_posts_num_of_exp_job[job_posts_num_of_exp_job[\"num_of_exp_job_class_title\"] == 0][\"JOB_CLASS_TITLE\"].drop_duplicates()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_promotions(job_class_title):\n    return job_posts[job_posts[\"EXP_JOB_CLASS_TITLE\"] == job_class_title][[\"JOB_CLASS_TITLE\", \"EXPERIENCE_LENGTH\", \"FULL_TIME_PART_TIME\"]].drop_duplicates().dropna().values.tolist()\n\n\ndef find_promotion_tree(job_class_title, experience_length = 0, full_time_part_time = \"\", depth = 10):\n    promotions = find_promotions(job_class_title)\n    \n    if(depth > 0):\n        promotions = [find_promotion_tree(job_class, experience_length, full_time_part_time, depth = depth - 1) \n                      for job_class, experience_length, full_time_part_time in promotions if job_class != job_class_title and job_class is not None]\n        \n    return {\"job_title\" : job_class_title, \"experience_length\" : experience_length, \"full_time_part_time\" : full_time_part_time, \"promotions\" : promotions}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_graph(promotion_tree):\n    dot = Digraph(comment='Promotions', strict = True)\n        \n    if(not promotion_tree or (len(promotion_tree) == 1 and not promotion_tree[0][\"promotions\"])):\n        dot.attr(label=r'\\nno promotions available')\n    \n    for job in promotion_tree:\n        if(job[\"promotions\"]): # only show the jobs which allow for promotions\n            dot.node(job[\"job_title\"].replace(\" \", \"_\"), job[\"job_title\"])\n\n            for promotion in job[\"promotions\"]:\n                dot.node(promotion[\"job_title\"].replace(\" \", \"_\"), promotion[\"job_title\"])\n                dot.edge(job[\"job_title\"].replace(\" \", \"_\"), promotion[\"job_title\"].replace(\" \", \"_\"), label=str(promotion[\"experience_length\"]) + \"years \\n\"+ str(promotion[\"full_time_part_time\"]))\n                create_graph(promotion[\"promotions\"])\n\n    return dot\n\n\ndef find_promotion_tree_create_graph(job_class_title):\n    promotion_tree = [find_promotion_tree(job_class_title)]\n    return create_graph(promotion_tree)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Analysis\n\nThe following figure displays the full promotional tree as given by the job bulletins."},{"metadata":{"trusted":true},"cell_type":"code","source":"full_promotion_tree = [find_promotion_tree(job) for job in jobs_without_exp_job]\ncreate_graph(full_promotion_tree)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the following, we supply a small widget which allows to select the current job class title and it displays possible promotions (with number of years and a note whether it is full-time or part-time experience)."},{"metadata":{"trusted":true},"cell_type":"code","source":"jobs = pd.concat([job_posts[\"JOB_CLASS_TITLE\"],job_posts[\"EXP_JOB_CLASS_TITLE\"]],ignore_index=True).drop_duplicates().reset_index(drop=True).sort_values().values.tolist()\n\nwidget_output = widgets.Output()\nw = widgets.Dropdown(options=jobs,description='Current job:',disabled=False)\n\n@widget_output.capture(clear_output=True)\ndef on_selection_change(change):\n    job_class_title = change[\"new\"]\n    graph = find_promotion_tree_create_graph(job_class_title)\n    with widget_output:\n        display(w)\n        display(graph)\n\nw.observe(on_selection_change, names='value')\n        \nwith widget_output:\n    graph = find_promotion_tree_create_graph(\"accountant\")\n    display(w)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Choose here the current position; it will display - if the data is available - possible promotions."},{"metadata":{"trusted":true},"cell_type":"code","source":"display(widget_output)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Recommendations"},{"metadata":{},"cell_type":"markdown","source":"* We have used the parsed data to display possible promotional paths. \n* A tool similar to the one prototyped in this notebook might be useful for employees."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}