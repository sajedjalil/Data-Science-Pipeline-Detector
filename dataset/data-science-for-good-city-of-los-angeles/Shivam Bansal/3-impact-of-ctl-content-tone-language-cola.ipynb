{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"<div align='center'><font size=\"6\" color=\"#ff3fb5\">Data Science For Good : CoLA</font></div>\n<div align='center'><font size=\"4\" color=\"#ff3fb5\">A Complete Pipeline for Structuring, Analysis and Recommendation</font></div>\n<div align='center'><font size=\"3\" color=\"#ff3fb5\">Improve Hiring Process and Decisions</font></div>\n<hr>\n\n<p style='text-align:justify'><b>Key Objectives:</b> Keeping these challenges in mind, an ideal solution for the City of Los Angeles has following key objectives: Develop an nlp framework to accurately structurize the job descriptions. Develop an analysis framework to identify the implict bias in the text and encourage diversity. Develop a system which can clearly identify the promotion pathways within the organization.</p>\n\n<b>My Submission:</b> Following are parts of Kernels Submissions in order:<br>\n\n<ul>\n    <li><a href=\"https://www.kaggle.com/shivamb/1-description-structuring-engine-cola\" target=\"_blank\">Part 1: Job Bulletin Structuring Engine - City of Los Angeles </a>  </li>\n    <li><a href=\"https://www.kaggle.com/shivamb/2-encourage-diversity-reduce-bias-cola\" target=\"_blank\">Part 2: Encourage Diversity and Remove Unconsious Bias from Job Bulletins - A Deep Analysis</a>  </li>\n    <li><a href=\"https://www.kaggle.com/shivamb/3-impact-of-ctl-content-tone-language-cola\" target=\"_blank\">Part 3: Impact of Content, Tone, and Language : CTL Analysis for CoLA</a>  </li> \n    <li><a href=\"https://www.kaggle.com/shivamb/4-promotional-pathway-discoverability-cola\" target=\"_blank\">Part 4: Increasing the Discoverability of Promotional Pathways (Explicit) </a></li>\n    <li><a href=\"https://www.kaggle.com/shivamb/5-implicit-promotional-pathways-discoverability/\" target=\"_blank\">Part 5: Implicit Promotional Pathways Discoverability</a></li></ul>\n\n<div align='center'><font size=\"5\" color=\"#ff3fb5\">Part 3: Impact of Content, Tone, and Language in Job Bulletins </font></div>\n<div align='center'>Other Parts: <a href='https://www.kaggle.com/shivamb/1-description-structuring-engine-cola'>Part 1</a> | <a href='https://www.kaggle.com/shivamb/2-encourage-diversity-reduce-bias-cola'>Part 2</a> | <a href='https://www.kaggle.com/shivamb/3-impact-of-ctl-content-tone-language-cola'>Part 3</a> | <a href='https://www.kaggle.com/shivamb/4-promotional-pathway-discoverability-cola'>Part 4</a> | <a href='https://www.kaggle.com/shivamb/5-implicit-promotional-pathways-discoverability/'>Part 5</a></div>\n<p style='text-align:justify'>The aim of this kernel is to provide an analysis framework which performs various types of text analysis on the job bulletins of City of Los Angeles. The goal of all types of analysis is to measure how well the job descriptions are written. The hypothesis is if the text is well written and fully serves its purpose, then it is likely to attract a good quality and quantity of applicants. To validate the hypothesis, I have also created a benchmark index using an indirect method, which can be used to compare CoLA insights with the market insights</p>       \n     \n### <font color=\"#ff3fb5\">Table of Contents</font>   \n\n<a href=\"#1\">1. Why Content, Tone, Format and Language Matters?</a>    \n<a href=\"#2\">2. Creating a Benchmark Index for Hypothesis Validation</a>         \n&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#2.1\">2.1 Analogy of using a Market Benchmark</a>        \n&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#2.2\">2.2 Analysis of Job Bulletins of Fortune 500 Companies</a>        \n<a href=\"#3\">3. Content: Is the Content of CoLA Job Descriptions Optimal?</a>      \n&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#3.1\">3.1 Are the Job Descriptions of City of LA too Lengthy?</a>        \n&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#3.2\">3.2 Are the Job Descriptions of City of LA too Wordy?</a>          \n&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#3.3\">3.3 Are the Job Bulletins difficult to read ?</a>           \n&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#3.4\">3.4 What does the Readability analysis of Bulletins suggest?</a>     \n&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#3.5\">3.5 Which Job Bulletins are too wordy?</a>     \n&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#3.6\">3.6 What are the outlier Job Bulletins - least wordy</a>     \n&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#3.7\">3.7 Outlier Job Bulletins - easier to read</a>     \n&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#3.8\">3.8 Relatively, which Job Bulletins are difficult to read</a>     \n<a href=\"#4\">4. Language: Is the Language of CoLA Job Descriptions Optimal?</a>     \n&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#4.1\">4.1 How well the Job Bulletins describes Duties?</a>           \n&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#4.2\">4.2 Does the Word Choice convey a lot of strictness?</a>            \n<a href=\"#5\">5. Is the format optimal? </a>         \n<a href=\"#6\">6. Tone: What does the Tone and Sentiment of CoLA Job Descriptions suggest?</a>    \n<a href=\"#7\">7. Key Recommendations</a>     \n\n<div id=\"1\"></div>\n## <font color=\"#ff3fb5\">1. Why Content, Tone, Language and Layout Matters ?</font>   \n\nA number of studies ([LinkedIn](https://business.linkedin.com/talent-solutions/blog/job-descriptions/2018/job-description-heatmap), [Forbes](https://www.forbes.com/sites/forbeshumanresourcescouncil/2018/03/28/need-to-attract-top-talent-try-these-12-smart-hiring-strategies/#78d24e616a69), and [Glassdoor](https://www.glassdoor.com/employers/blog/6-traits-great-job-descriptions/)) have suggested that both the quality and the quantity of applicant's pool can be significantly influenced by how a job description is written. \n\n*\"A well-written, complete, and insightful job description can result in attracting some of the top and diverse talents for the role\"*. On the other hand, a description which lacks key features (example - an optimal word limit, choice of the words, language used, overall tone) may result in attracting fewer candidates. It is important for City of Los Angeles to share the job descriptions with optimal and satisfactory content, language, tone, and the layout.     \n\nThe job descriptions are meant to effectively communicate about a particular job class, duties, responsibilities, requirements, and company to the audience. The point worth noting is that, the audience may have versatile backgrounds, demographics, different thought processes, intelligence, intellectuals, and different skills. With such diversity possible in applicants, it become important for the company to make sure that job descriptions are generic and not focussed to target a particular group. \n\n- **Content:** Content plays the most important role in describing the overall quality of a job description. Use of too many words, long paragraphs, and irrelevant verbose may not be perceived equally by all candidates, Especially women who according to [Harvard Business Review](https://hbr.org/2014/08/why-women-dont-apply-for-jobs-unless-theyre-100-qualified), tend to get 100% clarity about the job before applying. Another example from Linkedin study was related to length of job descriptions, Shorter job posts garnered a higher application rate than longer ones. Thus, keeping the job description text concise and optimal helps candidates immediately get the info they need. \n\n- **Tone:** For any big company it is important to express a professional tone in the job descriptions. An ideal text should be written in a formal tone and should reflect company's actual culture. The overall tone can be formal, generic or casual tones. Moreover, Use of particular wordings or phrases can also communicate emotional tones, which can have a direct impact on the applicant pool. For instance, more use of words with negative sentiment may reflect a pessimistic tone and may give a bad impression. It may result in only a fraction of potential applicants will apply. \n\n- **Language:** Choice of words and phrases is another important factor that makes up a good job description. Langauge analysis is about evaluating if the text conveys the proper message it is meant to be. One key example is about job duties which are meant to convey what the person will be doing if they are hired. It is possible that langauge or text of job duties section may not properly convey all those actions.    \n\n\n<div id=\"2\"></div>\n## <font color=\"#ff3fb5\">2. Creating A Benchmark Index for Hypothesis Validation </font>   \n\nThe problem statement of this challenge is very open ended and without actual applicant's response data, one cannot validate if all the intution and hypothesis are correct or not. Though I have used a number of other studies and researches conducted by many other companies as the reference to suggest key hypothesis but it is also important to develop a high level validation strategy. \n\n<div id=\"2.1\"></div>\n## <font color=\"#ff3fb5\">2.1 Analogy of using a Market Benchmark Index </font>   \n\n> - **Analogy:** In stock markets, investors refer to [S&P 500 index](https://en.wikipedia.org/wiki/S%26P_500_Index) which is an aggreagted stock index based on the market capitalizations of 500 large companies having common stock listed on the NYSE, NASDAQ, or the Cboe BZX Exchange. Investors use this index to understand how the overall market is performing and if their stock is also aligned (or below, or above) with it. Hence, it serve as a benchmark index.     \n> - In the similar manner, I created my own benchmark index by analysing the Job Descriptions of Fortune 500 Companies ([Forbes: Fortune 500 companies](http://fortune.com/fortune500/list/)). Use of this index, gives us a \"Industry\" benchmark (similar to \"market\" in S&P 500). Ofcourse, this index may not convey an exact reflection of metrics but this index definately gives the most ideal metrics which are used by some of the top and big companies having diverse sectors.   \n> - To state an example, it will be interesting to compare what is the average number of words used in City of Los Angeles job descriptions and that of Fortune 500 companies descriptions.   \n\nThe following diagram explains how I developed this index\n\n![](https://i.imgur.com/V73UmtU.png)  \n \n<br>\nI obtained the list of 2019 fortune 500 companies from [Forbes](http://fortune.com/fortune500/list/). I made different input queries (location=\"la\", role=\"CityofLA Job Roles\", company=\"One of the 500 companies\") and queried them in Free Public API of Indeed to obtain the job bulletins of these 500 companies. I obtained maximum three job descriptions per company, obtained the text data, and calculated all the relevant metrics to be used as a benchmark. The script of this process is shared in the next cell. The code is commented, it requires two arguments - API_KEY (which can be obtained from this link - https://opensource.indeedeng.io/api-documentation/) and a list of job roles to be searched.  \n\n*Adding this benchmark comparison gives a data-driven perspective to this problem along with our hypothesis.* "},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"from plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom plotly import tools\ninit_notebook_mode(connected=True)\n\nfrom nltk.corpus import stopwords \nfrom nltk.util import ngrams\nimport textstat, spacy, nltk \nimport pandas as pd, os\nimport string, re\nimport json\n\nbase_path = \"../input/data-science-for-good-city-of-los-angeles/cityofla/CityofLA/Job Bulletins/\"\next_path = \"../input/cityofla-external/\"\nstructured_path = \"../input/1-bulletin-structuring-engine-cola/structured_file.csv\"\nstructured_df = pd.read_csv(structured_path)\nstopwords = stopwords.words('english')\nnlp = spacy.load('en_core_web_sm')\n\n\n################### Code to Query Indeed API ############################\n\n\"\"\"\n:params: \n\"API_KEY\" : can be obtained from Indeed\n\"userip\" : Ip address of the user \n\"LIST_OF_JOB_ROLES\" : a file which contain job role titles to be searched\n\"\"\"\n\n\"\"\"\nfrom indeed import IndeedClient\nclient = IndeedClient('API_KEY')\nqueries = open('LIST_OF_JOB_ROLES.txt').read().split(\"\\n\")\n\nfor query in queries:\n    st, visited = 1, []\n    fout = open(query + \".txt\", \"w\")\n    for start in range(1, 5):\n        try:\n            params = {\n                'q' : query, 'l' : \"los angeles\", 'userip' : \"0.0.0.0\",\n                'useragent' : \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_2)\",\n                'start' : st, 'limit' : \"25\"\n            }\n            search_response = client.search(**params)\n            for a in search_response['results']:\n                key = a['company'] + \"_\" + a['jobtitle']\n                if key not in visited:\n                    visited.append(key)\n                    a['query'] = q\n                    fout.write(json.dumps(a) + \"\\n\")\n            st = start*25\n        except Exception as E:\n            continue\n\"\"\"\npass","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The compiled file of job bulletins text of these Fortune 500 companies is shared in external dataset used in this kernel, named as \"fortune_500_jds.csv\".    \n\n<div id=\"2.2\"></div>\n## <font color=\"#ff3fb5\">2.2 Fortune 500 Companies: Job Bulletins Analysis </font>   \n\nIn the next cell, I have analysed all the relevant metrics related to this kernel for the job bulletins of fortune 500 companies. The output of this analysis is the benchmark index."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\"\"\" function to cleanup the text \"\"\"\ndef _cleanup(text):\n    text = text.lower()\n    text = \" \".join([c for c in text.split() if c not in stopwords])\n    for c in string.punctuation:\n        text = text.replace(c, \" \")\n    text = \" \".join([c for c in text.split() if c not in stopwords])\n\n    words = []\n    ignorewords = []\n    for wrd in text.split():\n        if len(wrd) <= 2: \n            continue\n        if wrd in ignorewords:\n            continue\n        words.append(wrd)\n    text = \" \".join(words)    \n    return text\n\n## read the job descriptions of fortune 500 companies \nfort_df = pd.read_csv(ext_path + \"fortune_500_jds.csv\")\nfort_df['clean_text'] = fort_df['desc'].apply(lambda x : _cleanup(x))\n\n## meta features\nfort_df['lexicon_count'] = fort_df['desc'].apply(lambda x : textstat.lexicon_count(x))\nfort_df['sentence_count'] = fort_df['desc'].apply(lambda x : textstat.sentence_count(x))\nfort_df['avg_sentence_length'] = fort_df['desc'].apply(lambda x : textstat.avg_sentence_length(x))\n\n## readability \nfort_df['avg_syllables_per_word'] = fort_df['desc'].apply(lambda x : textstat.avg_syllables_per_word(x))\nfort_df['avg_character_per_word'] = fort_df['desc'].apply(lambda x : textstat.avg_character_per_word(x))\nfort_df['polysyllabcount'] = fort_df['desc'].apply(lambda x : textstat.polysyllabcount(x))\nfort_df['smog_index'] = fort_df['desc'].apply(lambda x : textstat.smog_index(x))\nfort_df['automated_readability_index'] = fort_df['desc'].apply(lambda x : textstat.automated_readability_index(x))\nfort_df['syllable_count'] = fort_df['desc'].apply(lambda x : textstat.syllable_count(x))\nfort_df['dale_chall_readability_score'] = fort_df['desc'].apply(lambda x : textstat.dale_chall_readability_score(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <font color=\"#ff3fb5\">Deep Text analysis of CoLA Job Descriptions</font>     \n\nIn the following sections, different types of text analysis focussing on three main features about the text - content, tone, langauge is shared. The insights are then compared with the benchmark metrics and the hypothesis. \n\n<div id=\"3\"></div>\n## <font color=\"#ff3fb5\">3. Is the Content of CoLA Job Descriptions Optimal?</font>   \n\nContent defines the overall structure of the bulletin. It involves the characteristics of words, phrases, sentences, and paragraphs. Key characteristics are:  \n\n**Meta Statistics of text-objects:** This consists of simple statistical values such as - Total Number of words, Total Number of sentences, Average length of the sentences     \n\n**Readability of words and document:** This involves the analysis of text object and identifying if the text is highly or poorly readable. This also consists of total number of difficult words used (words having more than three sounds - polysyllables), and an aggregated readability score of the text.   \n\nThe hypothesis is that the job descritions that should be easily perceived by everyone should have shorter lengths (words, sentences). The use of easy words should garner more responses as they must be grasped by a diverse set of people. "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"results = []\nfor fname in os.listdir(base_path):\n    if fname == \"POLICE COMMANDER 2251 092917.txt\":\n        continue\n\n    txt = open(base_path + fname).read()\n    lines = txt.split(\"\\n\")\n    txt = \" \".join(txt.split(\"\\n\"))\n    \n    file_df = structured_df[structured_df[\"FILE_NAME\"] == fname]\n    if len(file_df) > 0:\n        title = file_df['JOB_CLASS_TITLE'].iloc(0)[0]\n        duties = file_df['JOB_DUTIES'].iloc(0)[0]\n    else:\n        title = \"\"\n        \n    start = 0\n    rel_lines = []\n    for i, l in enumerate(lines):\n        if 'requirement' in l.lower():\n            start = i\n            break\n    for i, l in enumerate(lines[start+1:]):\n        if l.isupper():\n            break\n        rel_lines.append(l)\n    req = \"\\n\".join(rel_lines)\n    \n    \n    doc = {\n        \"title\" : title,\n        'filename' : fname,\n        'text' : txt,\n        'req' : req,\n        'duties' : duties\n    }\n    results.append(doc)\n    \nraw_df = pd.DataFrame(results)\n\n## meta features\nraw_df['lexicon_count'] = raw_df['text'].apply(lambda x : textstat.lexicon_count(x))\nraw_df['sentence_count'] = raw_df['text'].apply(lambda x : textstat.sentence_count(x))\nraw_df['avg_sentence_length'] = raw_df['text'].apply(lambda x : textstat.avg_sentence_length(x))\nraw_df['avg_syllables_per_word'] = raw_df['text'].apply(lambda x : textstat.avg_syllables_per_word(x))\n\nraw_df['avg_syllables_per_word'] = raw_df['text'].apply(lambda x : textstat.avg_syllables_per_word(x))\nraw_df['avg_character_per_word'] = raw_df['text'].apply(lambda x : textstat.avg_character_per_word(x))\nraw_df['polysyllabcount'] = raw_df['text'].apply(lambda x : textstat.polysyllabcount(x))\nraw_df['syllable_count'] = raw_df['text'].apply(lambda x : textstat.syllable_count(x))\n\nraw_df['smog_index'] = raw_df['text'].apply(lambda x : textstat.smog_index(x))\nraw_df['automated_readability_index'] = raw_df['text'].apply(lambda x : textstat.automated_readability_index(x))\nraw_df['dale_chall_readability_score'] = raw_df['text'].apply(lambda x : textstat.dale_chall_readability_score(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"<div id=\"3.1\"></div>\n## <font color=\"#ff3fb5\">3.1 Are the Job Descriptions of City of LA too Lengthy?</font>   "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np\nimport plotly.figure_factory as ff\n\ncolors = ['#333F44', '#37AA9C']\nhist_data = [fort_df['sentence_count'], raw_df['sentence_count']]\ngroup_labels = ['Fortune 500 Companies (Benchmark)', 'City of Los Angeles']\nfig = ff.create_distplot(hist_data, group_labels, colors=colors, bin_size=2)\nfig['layout'].update(title='Bulletins Length (Number of Sentences)', \n                     legend = dict(orientation=\"h\", x = 0.1, y = 1.09))\niplot(fig, filename='Distplot with Multiple Datasets')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> - At an overall level, bulletin lengths (total number of sentences) for City of LA  is skewed on the higher side. A large number of job bulletins have more than 30 total sentences in all the sections (duties, requirements, process, notes, selection etc.).   \n> - In comparison to Fortune 500 companies, their bulletin lengths is skewed on a much lower side.  \n> - The average number of sentences used in the job bulletins of fortune 500 companies is equal to 18 sentences while it is equal to 32 sentences for City of LA.   \n\n<div id=\"3.2\"></div>\n## <font color=\"#ff3fb5\">3.2 Are the Job Descriptions of City of LA too Wordy?</font>   "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"hist_data = [fort_df['lexicon_count'], raw_df['lexicon_count']]\ngroup_labels = ['Fortune 500 Companies (Benchmark)', 'City of Los Angeles']\nfig = ff.create_distplot(hist_data, group_labels, colors=colors, bin_size=12)\nfig['layout'].update(title='Bulletins Length (Number of Words)', \n                     legend = dict(orientation=\"h\", x = 0.1, y = 1.09))\niplot(fig, filename='Distplot with Multiple Datasets')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> - On an average, City of LA bulletins contain about 1352 words while fortune 500 companies bulletins contain only about 508, which is even lesser than half of what City of LA uses.  \n> - Very few City of LA bulletins contain less than 1000 words. Most of the bulletins are two page descriptions, there is a possiblity to cut down the noisy text which is irrelevant to the purpose.   "},{"metadata":{},"cell_type":"markdown","source":"<div id=\"3.3\"></div>\n## <font color=\"#ff3fb5\">3.3 Are the Job Bulletins difficult to read ?</font>   \n\nIn this section, the main focus is on evaluating the readability of job bulletins text. First let's focus on identifying the words which are difficult to read for all audience. For this purpose I am using my own readability calculation package. \n\nLink of my readability package - https://github.com/shivam5992/textstat \n\nEvery word is made up of multiple sounds which are called syllables. A word containing fewer number of syllables (sounds) is considered an easy word, while a word containing more than three syllables (sounds) are considered as difficult words. For example - "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"hist_data = [raw_df['polysyllabcount'], fort_df['polysyllabcount']]\ngroup_labels = ['City of Los Angeles', 'Fortune 500 Companies']\n\nfig = ff.create_distplot(hist_data, group_labels, colors = colors, show_hist=False, bin_size=1)\nfig['layout'].update(title='Number of Difficult Words (3+ sounds) used',\n                     legend = dict(orientation=\"h\", x = 0.1, y = 1.09))\niplot(fig, filename='Distplot with Multiple Datasets')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> - Their is a high useage of words having more than three sounds in the City of Los Angeles job bulletins, on an average every job description contain about 345 such keywords.  \n> - In comparison to benchmark, only about 135 keywords are used in every job bulletins.   \n\n<div id=\"3.4\"></div>\n## <font color=\"#ff3fb5\">3.4 Readability score of Bulletins</font>   \n\nI used the Dale Chall Readability score formula and the implementation from my own python [package](https://github.com/shivam5992/textstat) to calculate an overall readability score of job bulletins. Higher the score means higher readability of the text.   \n\nThe Dale Chall Readability score formula is inspired by Fleschâ€“Kincaid readability test which used word-length to determine how difficult a word was for readers to understand. DCR formula uses a list of 3000 very easy or basic words. "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fort_df['dale_chall_readability_score'] = fort_df['dale_chall_readability_score'].apply(lambda x : 15 if x >= 15 else x)\nhist_data = [raw_df['dale_chall_readability_score'], fort_df['dale_chall_readability_score']]\ngroup_labels = ['City of Los Angeles', 'Fortune 500 Companies']\n\nfig = ff.create_distplot(hist_data, group_labels, colors = colors, show_hist=False, bin_size=0.1)\nfig['layout'].update(title='Readability Score of Job Bulletins (Higher the better)',\n                     legend = dict(orientation=\"h\", x = 0.1, y = 1.09))\niplot(fig, filename='Distplot with Multiple Datasets')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> - On a normalized scale ( 0 - 15 ), benchmark companies have slightly higher readability scores in comparison to that of city of los angeles bulletins.  \n\nLet's compare all the metrics side by side. "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"cols = [\"lexicon_count\", \"sentence_count\", \"avg_sentence_length\", \"avg_syllables_per_word\", \n        \"avg_character_per_word\", \"polysyllabcount\", \"syllable_count\", \"dale_chall_readability_score\"]\nresults = []\nfor c in cols:\n    doc = {\n        \"label\" : c.replace(\"_\", \" \").title(),\n        \"Fortune 500\" : np.mean(fort_df[c]),\n        \"City of LA\" : np.mean(raw_df[c])        \n    }\n    results.append(doc)\n    \nnp.mean(raw_df['polysyllabcount']), np.mean(fort_df['polysyllabcount']) \ncom_df = pd.DataFrame(results)\ncom_df[['label', 'Fortune 500', 'City of LA']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can observe that CityofLA has lower (or higher in some case) values than the benchmark index. Following are some of the words having more number of syllables. Some of these words (if possible) can be replaced with some easy words having similar meanings. "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# import operator \n\n# results = []\n# html = \"<div><hr>\"\n# for i,f in raw_df.iterrows():\n#     words = f['req'].split()\n#     ps_doc = {}\n#     for w in words:\n#         w = \"\".join(c for c in w if c not in string.punctuation).lower()\n#         if w not in ps_doc:\n#             ps_doc[w] = 0\n#         ps_doc[w] = textstat.syllable_count(w)\n#     sorted_x = sorted(ps_doc.items(), key = operator.itemgetter(1), reverse = True)\n#     sorted_x = [x for x in sorted_x if x[1] >= 4]\n#     results += sorted_x\n\n# ignore = ['computerbased', 'closelyrelated', 'facility', 'application', 'administration', 'relationships', 'organization']\n# txt = \"\"\n# from collections import Counter \n# for i, w in enumerate(Counter(results).most_common(200)):\n#     if i > 0:\n#         if w[0][0] in ignore:\n#             continue\n#         doc = nlp(w[0][0])\n#         for w in doc:\n#             if w.tag_[0] == \"J\":                \n#                 print (w)\n        \n        \n# from wordcloud import WordCloud, STOPWORDS\n# import matplotlib.pyplot as plt\n\n# wordcloud = WordCloud(background_color='white').generate(txt)\n\n# plt.figure(figsize=(12, 6))\n# plt.imshow(wordcloud)\n# plt.axis('off')\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<table>\n    <tr>\n        <td>Modifiers</td>\n        <td>Verbs</td>\n        <td>Nouns</td>\n    </tr>\n    <tr>\n        <td><img src='https://i.imgur.com/YLIPikC.png'></td>\n        <td><img src='https://i.imgur.com/SPGiwCy.png'></td>\n        <td><img src='https://i.imgur.com/srlrdo3.png'></td>\n    </tr>\n</table>\n\n\n<br>\n<div id=\"3.5\"></div>\n## <font color=\"#ff3fb5\">3.5 Outlier Job Bulletins - too wordy </font>   \n\nLet's look at which roles have most number of the stats that we discussed in previous points. "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"t = raw_df.sort_values('lexicon_count', ascending = False).head(10)[['title', 'lexicon_count', 'polysyllabcount']]\n\ntrace1 = go.Bar(\n    y=t['title'][::-1],\n    x=t['lexicon_count'][::-1],\n    name='Total Words',\n    orientation = 'h',\n    marker = dict(\n        color = '#37AA9C', opacity=0.7,\n        line = dict(\n            color = '#37AA9C',\n            width = 3)\n    )\n)\ntrace2 = go.Bar(\n    y=t['title'][::-1],\n    x=t['polysyllabcount'][::-1],\n    name='Difficult Words (High Syllables)',\n    orientation = 'h',\n    marker = dict(\n        color = 'rgba(58, 71, 80, 0.4)',\n        line = dict(\n            color = 'rgba(58, 71, 80, 0.4)',\n            width = 3)\n    )\n)\n\ndata = [trace2, trace1]\nlayout = go.Layout(\n    title = \"Job Roles with most number of words (and difficult words)\",\n    legend = dict(orientation = \"h\", x = 0.1, y = 1.11),\n    barmode='group', margin = dict(l=300), width=800,\n)\n\nfig = go.Figure(data=data, layout=layout)\niplot(fig, filename='marker-h-bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div id=\"3.6\"></div>\n## <font color=\"#ff3fb5\">3.6 Outlier Job Bulletins - least wordy </font>   \n\nSimilarly, following plot shows the roles having least number of words. "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"t = raw_df.sort_values('lexicon_count', ascending = False).tail(10)[['title', 'lexicon_count', 'polysyllabcount']]\n\ntrace1 = go.Bar(\n    y=t['title'][::-1],\n    x=t['lexicon_count'][::-1],\n    name='Total Words',\n    orientation = 'h',\n    marker = dict(\n        color = '#f9bd63', opacity=0.7,\n        line = dict(\n            color = '#f9bd63',\n            width = 3)\n    )\n)\ntrace2 = go.Bar(\n    y=t['title'][::-1],\n    x=t['polysyllabcount'][::-1],\n    name='Difficult Words (High Syllables)',\n    orientation = 'h',\n    marker = dict(\n        color = 'rgba(58, 71, 80, 0.4)',\n        line = dict(\n            color = 'rgba(58, 71, 80, 0.4)',\n            width = 3)\n    )\n)\n\ndata = [trace2, trace1]\nlayout = go.Layout(\n    title = \"Job Roles with least number of words (and difficult words)\",\n    legend = dict(orientation = \"h\", x = 0.1, y = 1.11),\n    barmode='group', margin = dict(l=300), width=800,\n    xaxis=dict(range=(0,2500))\n)\n\nfig = go.Figure(data=data, layout=layout)\niplot(fig, filename='marker-h-bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div id=\"3.7\"></div>\n## <font color=\"#ff3fb5\">3.7 Outlier Job Bulletins - easier to read </font>   \n\nNow, let's identify which job roles have high readability scores, ie. they are relatively easier to read as compared to other bulletins. "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"t = raw_df.sort_values('automated_readability_index', ascending = False).head(10)[['title', 'automated_readability_index', 'syllable_count']]\n\ntrace0 = go.Scatter(\n    x=t.automated_readability_index[::-1],\n    y=t.title[::-1],\n    mode='markers',\n    name='Automated Readability Index (higher the better)',\n    marker=dict(\n        color='rgba(156, 165, 196, 0.95)',\n        line=dict(\n            color='rgba(156, 165, 196, 1.0)',\n            width=1,\n        ),\n        symbol='circle',\n        size=22,\n    )\n)\ntrace1 = go.Scatter(\n    x=t.automated_readability_index,\n    y=t.title,\n    mode='markers',\n    name='Average Syllables (Number of Sounds) per Word',\n    marker=dict(\n        color='rgba(204, 204, 204, 0.95)',\n        line=dict(\n            color='rgba(217, 217, 217, 1.0)',\n            width=1,\n        ),\n        symbol='circle',\n        size=16,\n    )\n)\n\ndata = [trace0]\nlayout = go.Layout(\n    title=\"Bulletins with highest Readability Scores\",\n    xaxis=dict(\n        showgrid=False,\n        showline=True,\n        linecolor='rgb(102, 102, 102)',\n        titlefont=dict(\n            color='rgb(204, 204, 204)'\n        ),\n        tickfont=dict(\n            color='rgb(102, 102, 102)',\n        ),\n        showticklabels=True,\n        ticks='outside',\n        tickcolor='rgb(102, 102, 102)',\n    ),\n    margin=dict(\n        l=400,\n        r=40,\n        b=50,\n        t=80\n    ),\n    legend=dict(\n        font=dict(\n            size=10,\n        ),\n        yanchor='middle',\n        xanchor='right',\n    ),\n    height=600,\n    hovermode='closest',\n)\nfig = go.Figure(data=data, layout=layout)\niplot(fig, filename='lowest-oecd-votes-cast')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div id=\"3.8\"></div>\n## <font color=\"#ff3fb5\">3.8 Outlier Job Bulletins - too difficult to read </font>   \n\nNext, let's look at which job role bulletins having least readability scores. "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"raw_df = raw_df[raw_df['title'] != \"\"]\nt = raw_df.sort_values('automated_readability_index', ascending = False).tail(10)[['title', 'automated_readability_index', 'syllable_count']]\n\ntrace0 = go.Scatter(\n    x=t.automated_readability_index[::-1],\n    y=t.title[::-1],\n    mode='markers',\n    name='Automated Readability Index (lower the bad)',\n    marker=dict(\n        color='rgba(156, 165, 196, 0.95)',\n        line=dict(\n            color='rgba(156, 165, 196, 1.0)',\n            width=1,\n        ),\n        symbol='circle',\n        size=22,\n    )\n)\ntrace1 = go.Scatter(\n    x=t.automated_readability_index,\n    y=t.title,\n    mode='markers',\n    name='Average Syllables (Number of Sounds) per Word',\n    marker=dict(\n        color='rgba(204, 204, 204, 0.95)',\n        line=dict(\n            color='rgba(217, 217, 217, 1.0)',\n            width=1,\n        ),\n        symbol='circle',\n        size=16,\n    )\n)\n\ndata = [trace0]\nlayout = go.Layout(\n    title=\"Bulletins with least Readability Scores\",\n    xaxis=dict(\n        showgrid=False,\n        showline=True,\n        linecolor='rgb(102, 102, 102)',\n        titlefont=dict(\n            color='rgb(204, 204, 204)'\n        ),\n        tickfont=dict(\n            color='rgb(102, 102, 102)',\n        ),\n        showticklabels=True,\n        ticks='outside',\n        tickcolor='rgb(102, 102, 102)',\n    ),\n    margin=dict(\n        l=400,\n        r=40,\n        b=50,\n        t=80\n    ),\n    legend=dict(\n        font=dict(\n            size=10,\n        ),\n        yanchor='middle',\n        xanchor='right',\n    ),\n    height=600,\n    hovermode='closest',\n)\nfig = go.Figure(data=data, layout=layout)\niplot(fig, filename='lowest-oecd-votes-cast')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div id=\"4\"></div>\n## <font color=\"#ff3fb5\">4. Analysis of Bulletins Language</font>   \n\nIn this section, we focus on analysing the language used in the job description of CoLA jobs. Mainly, I have focussed on two points:   \n- Do Duties section really convey the proper message.    \n- Does the Job Descriptions convey a very demanding/strict message?  \n\n<div id=\"4.1\"></div>\n## <font color=\"#ff3fb5\">4.1 How well the Job Bulletins describes Duties?</font>    \n\nThere is a very specific and and important section in a job bulletin called \"Job Duties\". The main goal of this section is to effectively communicate what does the employee of the particular job class do. This section describes all the actions or responsibilties to be performed by the employee. \n\nAny duties text which lacks this **action describing nature** is an example of **poorly written text**. A poorly written job duties section is likely to have following characteristics:\n\n- **Very few verbs:** this means that the duties section does not effectively describe what does the person will do or need to do in that job class.   \n- **High use of nouns:** this indicates that duties section talks a lot about entities or facts and less about the actions to be done.  \n\nAn example of a well written job duties section should have more usage of verbs that describe what does the person do and less usage of nouns. The main hypothesis is that the applicant should be well aware about all the possible responsibilties to be carried out in the job. The applicant should not bother about all the irrelevant or indirect details but should perceive the information about \"actions to do\". \n\nLet's look at how City of LA uses nouns and verbs in the job duties. "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"## more action or more verbs | specially in the duties\ndef _action_entity(txt):\n    doc = nlp(txt)\n    words = []\n    for token in doc:\n        words.append(token.lemma_)\n    words = \" \".join(words)\n    doc = nlp(words)\n\n    postags = {}\n    for token in doc:\n        if token.pos_ not in postags:\n            postags[token.pos_] = 0 \n        postags[token.pos_] += 1\n    return postags\n\nraw_df['pos'] = raw_df['duties'].apply(lambda x : _action_entity(str(x)))\nraw_df['verb'] = raw_df['pos'].apply(lambda x : x['VERB'] if 'VERB' in x else 0)\nraw_df['noun'] = raw_df['pos'].apply(lambda x : x['NOUN'] if 'NOUN' in x else 0)\n\nhist_data = [raw_df['verb'], raw_df['noun']]\ngroup_labels = ['Verbs usage in Duties Text', 'Nouns Usage in Duties Text']\nfig = ff.create_distplot(hist_data, group_labels, colors=['#8fddf7', '#ef8892'], bin_size=1)\nfig['layout'].update(title='', \n                     legend = dict(orientation=\"h\", x = 0.1, y = 1.09))\niplot(fig, filename='Distplot with Multiple Datasets')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can observe that, there is ofcourse higher usage of nouns and less of verbs which is fine. But the bad examples will be the one in which noun to verb ratio is very low. Let's look at such examples in which noun to verb ratio and the use of verbs in the text is low. \n\n## <font color=\"#ff3fb5\">Examples of Duties with Less Action Describing Nature</font>    "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from IPython.core.display import HTML\n\nraw_df['v_n_ratio'] = raw_df['verb'] / raw_df['noun']\nt = raw_df.sort_values('v_n_ratio')\nt = t[t['v_n_ratio']!=0]\nt = t[t['verb'] < 3]\n\ninterest = ['does', 'supervises', 'acts', 'works']\nhtml = \"<div><hr>\"\nfor i,f in t[1:4].iterrows():\n    html += \"<h2><font color='red'>Job: \" + f['title'] + \"</font></h2>\"\n    doc = nlp(f['duties'])\n    for token in doc:\n        if token.text in interest:\n            html += \"\"\"<span style='background-color:#f20909; padding:5px'>\n                     <font color='#000'>\"\"\"+token.text+\"\"\"</font></span>\"\"\"+\" \"\n        else:\n            html += token.text+\" \"\n\nhtml += \"<hr></div>\"\ndisplay(HTML(html))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can observe that these job descriptions talks very less about what the person needs to do. There is a very less usage of verbs in the text. Let's look at some examples of well written job duties text.\n\n## <font color=\"#ff3fb5\">Examples of Well Written Job Duties Text</font>    \n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"ignore = ['may', 'required', 'be', 'are']\nt = raw_df.sort_values('verb', ascending = False)\nhtml = \"<div><hr>\"\nfor i,f in t[:3].iterrows():\n    html += \"<h2><font color='green'>Job: \" + f['title'] + \"</font></h2>\"\n    doc = nlp(f['duties'])\n    for token in doc:\n        if token.text.endswith(\"d\"):\n            html += token.text+\" \"\n        elif token.text in ignore:\n            html += token.text+\" \"\n        elif token.pos_ == 'VERB':\n            html += \"\"\"<span style='background-color:#86f9c4; padding:5px'>\n                     <font color='#000'>\"\"\"+token.text+\"\"\"</font></span>\"\"\"+\" \"\n        else:\n            html += token.text+\" \"\n\nhtml += \"<hr></div>\"\ndisplay(HTML(html))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> - In these examples, we can clearly see that use of verbs and specifically those which describes the responsibilites and actions to do is very high. This makes them good examples of well written job duties text. \n\n<div id=\"4.2\"></div>\n## <font color=\"#ff3fb5\">4.2 Does the Word Choice convey a lot of strictness ?</font>    \n\nAnother part in the language analysis is measuring if the use of particular words or phrases convey a lot of strictness. A job description should not be very demanding in nature or tone as it may put off some applicants. Consider the applicants which do not qualify 100% for the job but about 70-80% which is still good for the company. However, due to the demanding or strictness conveyed from the job bulletin, those candidates may not apply for such jobs. Hence in order to encourage applications which is directly linked to getting more diverse applications, it is necessary to measure and rectify strictness conveyed due to use of some words. \n\nLet's look at en example in which more number of strict words or phrases is used. "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"class JD_Purifier():\n    def __init__(self):\n        self.lookups = {  \n                          \"strict_words\"   : ['disqualified', 'rejected', 'must', 'should' , 'required', 'banned', 'barred', 'disbarred', 'debarred', 'eliminated', 'precluded', 'disentitled', 'ineligible', 'unfit', 'unqualified', 'essential', 'desirable', 'desired'],\n                          \"strict_phrases\" : ['only be', 'who fail', 'not allowed', 'should have', 'must have' , 'subject to', 'will not be considered', 'cannot be appointed', 'who lack'],\n                }\n        \n    ## function to clean a text\n    def _cleanup(self, text):\n        text = text.lower()\n        text = \" \".join([c for c in text.split() if c not in stopwords])\n        for c in string.punctuation:\n            text = text.replace(c, \" \")\n        text = \" \".join([c for c in text.split() if c not in stopwords])\n\n        words = []\n        ignorewords = []\n        for wrd in text.split():\n            if len(wrd) <= 2: \n                continue\n            if wrd in ignorewords:\n                continue\n            words.append(wrd)\n        text = \" \".join(words)    \n        return text\n    \n    def _check_presence(self, flag, txt, condition):    \n        matched = {}\n        txt = \" \"+txt.lower()+\" \"\n        for wrd in self.lookups[flag]:\n            if condition == \"exact\":\n                cnt = txt.count(\" \"+wrd.lower()+\" \")\n                if cnt > 0:\n                    matched[wrd.lower()] = cnt\n            elif condition == \"startswith\":\n                cnt = txt.count(\" \"+wrd.lower())\n                if cnt > 0:\n                    matched[wrd.lower()] = cnt\n        return matched\n\n    def _strictness_bias(self, txt):\n        cln = self._cleanup(txt)\n\n        ## word choice\n        strict_usage = self._check_presence(\"strict_words\", txt, condition = \"exact\")\n\n        ## phrase choice\n        strict_phrases = self._check_presence(\"strict_phrases\", txt, condition = \"exact\")\n\n        doc = {\n            \"strict_wc\" : strict_usage,\n            \"strict_phrases\" : strict_phrases,\n            \"strict_wc_sum\" : sum(strict_usage.values()),\n            \"strict_phrases_sum\" : sum(strict_phrases.values())\n        }\n        return doc\n\njdp = JD_Purifier()\nraw_df['strict_mentions'] = raw_df['text'].apply(lambda x : jdp._strictness_bias(x))\nraw_df['strictness'] = raw_df['strict_mentions'].apply(lambda x : x['strict_phrases_sum']+x['strict_wc_sum'])\n\n\nfor i, r in raw_df.sort_values(\"strictness\", ascending = False).head().iterrows():\n    d = r['strict_mentions']['strict_wc'] \n    d.update(r['strict_mentions']['strict_phrases'])\n    htm = (r['text'].split(\"  4. \")[1].split(\"  5. \")[0])\n    title = r['title']\n    break\n    \nhtml = \"<div><hr>\"\nhtml += \"<h2><font color='#37AA9C'>Job: \" + title + \"</font></h2>\"\n\nwl1 = ['disqualified', 'rejected', 'must', 'should' , 'required', 'banned', 'barred', 'disbarred', 'debarred', 'eliminated', 'precluded', 'disentitled', 'ineligible', 'unfit', 'unqualified', 'essential', 'desirable', 'desired']\nwl2 = ['will not be processed', 'only be', 'who fail', 'not allowed', 'should have', 'must have' , 'subject to', 'will not be considered', 'cannot be appointed', 'who lack']\ninterest = wl1 + wl2\n\nfor w in interest:\n    if w in htm.lower():\n        if len(w.split()) > 1:\n            htm = htm.replace(w, \"\"\"<span style='background-color:#f20909; padding:5px'>\n                     <font color='#000'>\"\"\"+w+\"\"\"</font></span>\"\"\")\n        else:\n            htm = htm.replace(w, \"\"\"<span style='background-color:#f78f8f; padding:5px'>\n                     <font color='#000'>\"\"\"+w+\"\"\"</font></span>\"\"\")\nhtml += \"<p style='text-align:justify'>\" + htm + \"</p>\"\nhtml += \"<hr></div>\"\ndisplay(HTML(html))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> - Excessive use of phrases like \"who fail\", \"will not be considered\", \"must have\" etc gives an impression that job class is very demanding. Instead of using such phrases, one can use keywords like good to have, it will be an add-on. This is much nicer and gives an optimistic tone from the text. Thus, may encourage many more to apply. "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import operator \nfaculties = raw_df.sort_values(\"strictness\", ascending = False).head(4)['title'].values\n\ntraces, titles = [], []\nfor title in faculties:\n    fac_df = raw_df[raw_df['title'] == title]\n    \n    r = fac_df.iloc(0)[0]\n    d = r['strict_mentions']['strict_wc'] \n    d.update(r['strict_mentions']['strict_phrases'])\n    \n    d1 = sorted(d.items(), key=operator.itemgetter(1))\n    \n    xx, yy = [_[0] for _ in d1], [_[1] for _ in d1]\n    tracei = go.Bar(y=xx, x=yy, name=\"\", orientation=\"h\", marker=dict(color='green'), opacity=0.8)\n    traces.append(tracei)\n    titles.append(title)\n    \nk = 0\nfig = tools.make_subplots(rows=2, cols=2, subplot_titles=titles, print_grid=False)\nfor i in range(1,3):\n    for j in range(1,3):\n        fig.append_trace(traces[k], i, j)\n        k += 1\n\nfig['layout'].update(height=800, \n                     title='Use of strict phrases', \n                     showlegend=False,\n                     margin=dict(l=200))\nfor i in fig['layout']['annotations']:\n    i['font'] = dict(size=8, color='green')\n\niplot(fig, filename='grouped-bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"> - these words / phrases are some of the top phrases used in City of LA texts and can by replaced with much more friendly keywords. \n\n<div id=\"5\"></div>\n## <font color=\"#ff3fb5\">5. Is the format optimal ? </font>    \n\nOne of the key point I noticed that generally City's job bulletin's requirements have about 3 lines talking about job descriptions. The lines end with one of \"or\" /\"and\" which indicates that only one of the requirement set needs to be fulfilled. However, the use of \"or\" towards the very end is not immediately visible to the applicants and gives the first impression that the particular job class requires 3 different requirements. But in reality, they are only asking for 1. \n\nA simple solution for this issue is suggested in the following figure. Just by making a slight change, requirements can become more clear and explict. \n\n![](https://i.imgur.com/E2wG9FQ.png) \n\n\n<div id=\"6\"></div>\n## <font color=\"#ff3fb5\">6. What is the overall Tone / Sentiment of CoLA Job Bulletins</font>    \n\nSimilar to strictness tone discussed in section 4.1, it is also necessary that overall sentiment of the job bulletins should not be too negative. Use of sentences and words that convey moderate or high negative sentiments should be avoided. I used python's textblob library to caluclate the sentiment of every text in each of the document. \n\nLet's look at some examples of sentence with high negative sentences which can be avoided. "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from textblob import TextBlob\nsome_negatives = []\nother_negatives = []\nfor i,r in raw_df.iterrows():\n    blob = TextBlob(r['text'].lower())\n    for sent in blob.sentences:\n        senti = sent.sentiment.polarity\n        if senti < -0.2:\n            if \" who fail to \" in sent.lower():\n                some_negatives.append(str(sent))\n            else:\n                other_negatives.append(str(sent))\n\nends = []\nfor x in other_negatives:\n    if any(_ in x for _ in ['poor', 'unsafe']):\n        if x not in ends:\n            ends.append(x)\n\nnegs = ['unsafe', 'failed', 'disqualified', 'poor', 'affect']\nhtml = \"<div><hr>\"\nfor l in ends[:2]:\n    l = \" \".join([\"<span style='background-color:#f76276; padding:3px'>\"+w+\"</span>\" if w in negs else w for w in l.split()])\n    html += \"<b>Sentence:</b> \"+l\n    html += \"<br><br>\"\nhtml += \"<hr></div>\"\ndisplay(HTML(html))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And then there are examples of very generic sentences which convey a pessimistic tone. For example, one particular pattern repeats in most of the bulletins: \n\n>  candidates **who fail to** _____ **will not be considered** ______ **will not be processed**   "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"interest1 = ['who fail to']\ninterest2 = ['will not be considered', 'disqualified', 'will not be processed']\nhtml = \"<div><hr>\"\nfor l in some_negatives[:4]:\n    for phrase in interest1:\n        l = l.replace(phrase, \"\"\"<span style='background-color:#f76276; padding:3px'>\n                     <font color='#000'>\"\"\"+phrase+\"\"\"</font></span>\"\"\")\n    for phrase in interest2:\n        l = l.replace(phrase, \"\"\"<span style='background-color:#fc6267; padding:3px'>\n                     <font color='#000'>\"\"\"+phrase+\"\"\"</font></span>\"\"\")\n    html += \"&bull; \"+l\n    html += \"<br><br>\"\nhtml += \"<hr></div>\"\ndisplay(HTML(html))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This way of writing the text can be avoided and instead one can write, candidates need to ______ so that ___ can be considered.   \n\n<div id=\"7\"></div>\n## <font color=\"#ff3fb5\">7. Key Recommendations</font>    \n\n**Recommendation 1:** An Optimal Bulletin's Length \n> - The analysis performed in section 3 suggest that lenghts of City of LA's bulletin's is somewhat higher. It is recommended that unnecessary jargon, irrelevant details should be avoided. Bulletins should be made more crisper by cutting down atleast 30-40% of words used. On an average, City should try to use not more than 20 sentences and 700 keywords in the overall description. Additionally, as every career advisor suggest to every job candidate that their resume should be single page, similarl, for an organization, it will be really good to cut down the bulletin's to single page. This will help the candidates to grasp the information quickly. Reduce the average sentence length as well, number of words in a sentence should be as less possible. This is because the reader should get the main context of the different sections from the relevant information.  \n\n**Recommendation 2:** Easier to Read Job Bulletins \n> - Section 3 analysis also suggest that a large number of the job bulletin's consists of complex keywords. Mainly the words having more number of syllables (or sounds). It is obvious that most of the words cannot be ignored, however increased use of polysyllable (greater than three syllables) keywords reduces the readability of the overall text. Job bulletin's are meant to attract more and a variety of applicant's. To ensure that this happens, it is recommended to increase the use of more easier words and reducing the keywords having more than three sounds. \n\n**Recommendation 3:** Well Written Job Duties   \n> - The analysis performed in section 4 describes that Most of the job bulletin's duties section is not optimally written. An optimal duties section should clearly describe the actions, tasks, responsibilities of the job class. Hence, more use of verbs should be encouraged. Atleast 5 verbs that describe the actions should be used in the duties. Use of more verbs suggest that the duties section is more insightful and less factual. Hence, it makes it easier for the applicants or the readers to quickly get acquainted about the job duties. This will have a direct correlation on increase in number of applicants.    \n\n**Recommendation 4:** An Optimistic Tone of the Bulletin   \n> - Bulltein's should not sound pessimistic to the applicants. Minimize the use of keywords which gives a pessimistic or daunting tone. Instead of saying - X is must have, City can say, X is good to have. Also minimize the use of negative keywords such as - disqualified, fail etc. Instead use words which may convey the same message but in a much polite tone.     \n\n**Recommendation 5:** Less Cluttered Requirements Section    \n> - Make the requirement section more visible and less cluttered. Since, there is the use of logical operators (and, or) in the requirement pointers, hence it is recommended to remove the numbers (1, 2, 3) because their are only single independent sets not multiple sets. This is to ensure the applicant's to get the clear message in the first impression. Research suggested that female genders dont apply untill they feel that they meet 100% of the requirements. Thus, three points in the requirements should not give a wrong message.   \n\nCity of LA can use this kernel to keep track of these different metrics and compare them with a benchmark index. Keep a track on average readability score of the job bulletin, try to make the sentences as readable as possible. An ideal range of automated readability index is about 10.  \n\n### Next Kernels: \nFor next parts of my submission (analysis and recommendations), please visit next kernels of my Submission: \n<ul>\n    <li><a href=\"https://www.kaggle.com/shivamb/1-description-structuring-engine-cola\" target=\"_blank\">Part 1: Job Bulletin Structuring Engine - City of Los Angeles </a>  </li>\n    <li><a href=\"https://www.kaggle.com/shivamb/2-encourage-diversity-reduce-bias-cola\" target=\"_blank\">Part 2: Encourage Diversity and Remove Unconsious Bias from Job Bulletins - A Deep Analysis</a>  </li>\n    <li><a href=\"https://www.kaggle.com/shivamb/3-impact-of-ctl-content-tone-language-cola\" target=\"_blank\">Part 3: Impact of Content, Tone, and Language : CTL Analysis for CoLA</a>  </li> \n    <li><a href=\"https://www.kaggle.com/shivamb/4-promotional-pathway-discoverability-cola\" target=\"_blank\">Part 4: Increasing the Discoverability of Promotional Pathways (Explicit) </a></li>\n    <li><a href=\"https://www.kaggle.com/shivamb/5-implicit-promotional-pathways-discoverability/\" target=\"_blank\">Part 5: Implicit Promotional Pathways Discoverability</a></li></ul>\n\nNext <a href=\"https://www.kaggle.com/shivamb/4-promotional-pathway-discoverability-cola\" target=\"_blank\">Kernel</a> - Method to identify and visualize explict promotion pathways."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}