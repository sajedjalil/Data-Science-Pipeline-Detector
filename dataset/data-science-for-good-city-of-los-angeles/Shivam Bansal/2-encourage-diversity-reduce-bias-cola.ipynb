{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"<div align='center'><font size=\"6\" color=\"#ff3fb5\">Data Science For Good : CoLA</font></div>\n<div align='center'><font size=\"4\" color=\"#ff3fb5\">A Complete Pipeline for Structuring, Analysis and Recommendation</font></div>\n<div align='center'><font size=\"3\" color=\"#ff3fb5\">Improve Hiring Process and Decisions</font></div>\n<hr>\n\n<p style='text-align:justify'><b>Key Objectives:</b> Keeping these challenges in mind, an ideal solution for the City of Los Angeles has following key objectives: Develop an nlp framework to accurately structurize the job descriptions. Develop an analysis framework to identify the implict bias in the text and encourage diversity. Develop a system which can clearly identify the promotion pathways within the organization.</p>\n\n<b>My Submission:</b> Following are parts of Kernels Submissions in order:<br>\n\n<ul>\n    <li><a href=\"https://www.kaggle.com/shivamb/1-description-structuring-engine-cola\" target=\"_blank\">Part 1: Job Bulletin Structuring Engine - City of Los Angeles </a>  </li>\n    <li><a href=\"https://www.kaggle.com/shivamb/2-encourage-diversity-reduce-bias-cola\" target=\"_blank\">Part 2: Encourage Diversity and Remove Unconsious Bias from Job Bulletins - A Deep Analysis</a>  </li>\n    <li><a href=\"https://www.kaggle.com/shivamb/3-impact-of-ctl-content-tone-language-cola\" target=\"_blank\">Part 3: Impact of Content, Tone, and Language : CTL Analysis for CoLA</a>  </li> \n    <li><a href=\"https://www.kaggle.com/shivamb/4-promotional-pathway-discoverability-cola\" target=\"_blank\">Part 4: Increasing the Discoverability of Promotional Pathways (Explicit) </a></li>\n    <li><a href=\"https://www.kaggle.com/shivamb/5-implicit-promotional-pathways-discoverability/\" target=\"_blank\">Part 5: Implicit Promotional Pathways Discoverability</a></li></ul>\n\n\n<div align='center'><font size=\"5\" color=\"#ff3fb5\">Part 2: Encourage Diversity, Reduce Unconscious Bias - Deep Analysis </font></div>\n<div align='center'>Other Parts: <a href='https://www.kaggle.com/shivamb/1-description-structuring-engine-cola'>Part 1</a> | <a href='https://www.kaggle.com/shivamb/2-encourage-diversity-reduce-bias-cola'>Part 2</a> | <a href='https://www.kaggle.com/shivamb/3-impact-of-ctl-content-tone-language-cola'>Part 3</a> | <a href='https://www.kaggle.com/shivamb/4-promotional-pathway-discoverability-cola'>Part 4</a> | <a href='https://www.kaggle.com/shivamb/5-implicit-promotional-pathways-discoverability/'>Part 5</a></div>\n\n<p style='text-align:justify'>The aim of this kernel is to analyse, measure, and quantify unconscious (or implicit) bias present in the job bulletin's of City of Los Angeles. This kernel uses bag of words, dictionary approch to check the presence of certain category of keywords in the text, it then applies different normalization techniques in order to quantify the amount of unconscious bias present in the text. Different data visualizations are used to showcase the key insights and related trends. In the end, I perform a simulation experiment to validate the hypothesis. </p>       \n     \n### <font color=\"#ff3fb5\">Table of Contents</font>   \n\n<a href=\"#1\">1. What is Diversity Hiring? </a>    \n<a href=\"#2\">2. Unconscious Gender Bias in City's Bulletins </a>     \n&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#2.1\">2.1 Use of Gendered Keywords in City's Bulletins </a>     \n&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#2.2\">2.2 Measuring the gender bias in City's Bulletins </a>     \n&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#2.3\">2.3 Bulletins with High Masculine (or Feminine) Denotation </a>     \n&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#2.4\">2.4 Masculine and Feminine Words Usage </a>     \n&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#2.5\">2.5 Does (Masculine) Jobs are Offered Higher Salaries ? </a>     \n&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#2.6\">2.6 Does More Masculine Class means Higher Salary? </a>     \n&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#2.7\">2.7 Does Job Seniority Levels also show Gender Bias? </a>     \n&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#2.8\">2.8 Has the use of gendered language changed over time? </a>     \n<a href=\"#3\">3. Other forms of Unconscious Gender Bias </a>     \n&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#3.1\">3.1 Use of Superlatives Keywords </a>     \n&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#3.2\">3.2 Use of Master/Expert in Bulletins </a>     \n&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#3.3\">3.3 Describing the Requirements - Number of lines </a>     \n&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#3.4\">3.4 Keywords containing \"man\" </a>     \n&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#3.5\">3.5 Bad Assumption that there are only two genders </a>    \n<a href=\"#4\">4. Quantifying Unconscious Gender Bias </a>     \n<a href=\"#5\">5. Design Experiment for Validation : A Simple Simulation</a>     \n&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#5.1\">5.1 Analogy from Econometric Theory </a>     \n&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#5.2\">5.2 Using the Analogy to Design Experiment </a>     \n&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#5.3\">5.3 Creating Scenarios and Evaluating </a>     \n&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#5.4\">5.4 Analysis - Does reducing bias garners more applicants? </a>     \n&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#5.5\">5.5 Effect on Male Applicants </a>     \n&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#5.6\">5.6 Effect on Female Applicants </a>     \n<a href=\"#6\">6. Key Recommendations </a>     \n\n<div id=\"1\"></div>\n## <font color=\"#ff3fb5\">1. What is Diversity Hiring</font>   \n\n#### <font color=\"#ff3fb5\">A Gentle Introduction to Diversity Hiring</font> \n\n**\"Diversity is the collective strength of any successful organization\"**. Diversity hiring is the hiring process that is free from bias based on age, race, gender, religion, sexual orientation, and other personal characteristics. Diversity is very crutial for any company as it leads to creation of a culture that enables new ways of thinking, gives them an edge versus their competitors, and in turn, impacts the bottom line and overall success of the company (Source: [Forbes](https://www.forbes.com/sites/forbescommunicationscouncil/2019/01/04/three-practical-first-steps-to-successful-diversity-hiring/#221479b5f8cd)). \n\nHowever, Many companies struggle to ensure high levels of diversity among the employees and even the potential applicants for the open positions. In some cases, this is the due to explicit nature of companies to give less importance to diversity. But in large number of cases, it is due to the implicit Unconscious bias among different elements of the company such as hiring manager's mindset, company's actions, and even the job bulletins for the open positions. \n\n#### <font color=\"#ff3fb5\">Unconscious Bias : What it it, Why it occurs? </font> \n\nUnconscious bias is the set of assumptions directly linked to one's thinking, judgements, social background, personal values, etc. is often defined as prejudice or unsupported judgments in favor of or against one thing, person, or group as compared to another, in a way that is usually considered unfair. Most common example of Unconscious bias is the gender bias which exists in different cycles of hiring process. (Source: [Empiric](https://empiric.com/blog/unconscious-bias-in-job-descriptions/))    \n\n#### <font color=\"#ff3fb5\">Unconscious Bias in Job Bulletins</font> \n\nResearch shows that use of masculine language in bulletins such as including adjectives like “competitive” and “determined,” results in women’s “perceiving that they would not belong in the work environment.” On the other hand, use of words like “collaborative” and “cooperative” tend to draw more women than men. Use of language and word choice plays a key role in encourgaing applicant's diversity. Thus, it is important for the company to write job bulletins which are gendered-neutral or are free of any Unconscious bias. \n\nIn this kernel, I have performed different types of analysis which measure, quantify and analyse the Unconscious bias - specifically gender bias.  \n\n<div id=\"2\"></div>\n## <font color=\"#ff3fb5\">2. Unconscious Gender Bias in City of LA's Job Bulletins ? </font>   \n\nJob bulletins biased toward a specific gender can limit the candidate pool and diversity.    \n\n<img src='https://i.imgur.com/JLcqigL.png?1'>\n\nFollowing are some of the common traits of a gendered bias job bulletin.\n\n- **1. High Use of Genderded Keywords:** Words which are more ‘aggressive’, ‘assertive’ or ‘independent’ rather than things ‘conscientious’, ‘dedicated’ and ‘sociable’ will typically put off women. Using gender-charged words in job description can isolate a gender from applying\n- **2. High Use of Superlatives Usage:** Excessive use of superlatives such as “expert,” “superior,” “world class” can turn off female candidates who are more collaborative than competitive in nature   \n- **3. Less Use of Relationship/Family Keywords Usage:** Descriptions in which there is more usage of family oriented keywords tend to attract more female than men. \n- **4. Very Demanding Requirements:** Research shows that women are unlikely to apply for a position unless they meet 100 percent of the requirements, while men will apply if they meet 60 percent of the requirements.  \n- **5. Use of Gendered pronouns:** Using pronouns which are targeted for particular gender is not considered a good style of writing.  \n- **6. Use of Keywords containing \"man\":** Keywords containing \"man\" may sound that they are only meant for males.   \n\nI used following sources to obtain all the relevant information related to unconscious bias in job bulletins.  \n\nSources: [OnGig](https://blog.ongig.com/diversity-and-inclusion/unconscious-bias-in-job-descriptions), [LinkedIn](https://business.linkedin.com/talent-solutions/blog/job-descriptions/2018/5-must-dos-for-writing-inclusive-job-descriptions), [HarvardBusinessSchool](https://www.hbs.edu/recruiting/blog/post/simple-ways-to-take-gender-bias-out-of-your-jobs), [Lever](https://www.lever.co/blog/where-unconscious-bias-creeps-into-the-recruitment-process/), [SHRM](https://www.shrm.org/resourcesandtools/hr-topics/talent-acquisition/pages/7-practical-ways-to-reduce-bias-in-your-hiring-process.aspx), [Catalyst](https://www.catalyst.org/2015/05/07/can-you-spot-the-gender-bias-in-this-job-description/), [SheGeeksOut](https://www.shegeeksout.com/seven-tech-tools-to-mitigate-bias-in-hiring/), [Katmatfield](http://gender-decoder.katmatfield.com/), [ZipRecruiter](https://www.ziprecruiter.com/blog/are-you-limiting-candidates-with-biased-job-ads/), [BBC](http://www.bbc.com/capital/story/20180806-how-hidden-bias-can-stop-you-getting-a-job), [Textio](https://textio.com/)\n\nIn the following cell I have developed different functions that analyse these traits in the job bulletins. I curated several lists from multiple sources and complied them for different categories - masculine denotation, feminine denotation, superlatives, relationships etc. Following word clouds shows the top keywords related to feminine and masculine category. "},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"from plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom plotly import tools\ninit_notebook_mode(connected=True)\n\nfrom nltk.corpus import stopwords \nfrom nltk.util import ngrams\nimport textstat, spacy, nltk \nimport pandas as pd, os\nimport numpy as np \nimport string, re\nimport random\n\nbase_path = \"../input/data-science-for-good-city-of-los-angeles/cityofla/CityofLA/Job Bulletins/\"\nstructured_path = \"../input/1-bulletin-structuring-engine-cola/structured_file.csv\"\ntitles = open(base_path + \"../Additional data/job_titles.csv\").read().strip().split(\"\\n\")\nstructured_df = pd.read_csv(structured_path)\nstopwords = stopwords.words('english')\nnlp = spacy.load('en_core_web_sm')\ngenders = ['Male', 'Female']\nnp.random.seed(0)\n    \nclass JD_Purifier():\n    def __init__(self):\n        self.lookups = {  \n                          \"masculine\"      : ['active', 'adventurous', 'aggress', 'ambitio', 'analy', 'assert', 'athlet', 'autonom', 'battle', 'boast', 'challeng', 'champion', 'compet', 'confident', 'courag', 'decid', 'decision', 'decisive', 'defend', 'determin', 'domina', 'dominant', 'driven', 'fearless', 'fight', 'force', 'greedy', 'head strong', 'headstrong', 'hierarch', 'hostil', 'impulsive', 'independen', 'individual', 'intellect', 'lead', 'logic', 'objective', 'opinion', 'outspoken', 'persist', 'principle', 'reckless', 'self confiden', 'self relian', 'self sufficien', 'selfconfiden', 'selfrelian', 'selfsufficien', 'stubborn', 'superior', 'unreasonab', 'capable', 'certain', 'focus', 'benefit', 'trust ', 'trusting', 'acceptance ', 'accepting', 'appreciative ', 'appreciation', 'admire ', 'admiration', 'approval', 'encouragement', 'power', 'strenght', 'competency ', 'competence', 'efficient ', 'efficiency', 'achievement', 'honor', 'pride', 'dignity', 'solution ', 'solutions', 'success', 'skills', 'autonomy', 'love', 'serve', 'support', 'give ', 'giving', 'provide', 'devoted', 'fulfill', 'caretaker', 'space', 'useful', 'rational', 'strategy ', 'strategic', 'plan ', 'planning', 'analytic ', 'analytical', 'reasonable', 'consider', 'analyse ', 'analysing', 'believe', 'opinion', 'suggestion', 'think', 'prove themselves', 'achieve results', 'feel good about himself', 'doing things by himself', 'loving acceptance', 'feeling needed', 'someone to serve', 'good enough', 'fulfill others', 'silent acceptance', 'comforting love', 'common sense', 'point of view', 'active', 'adventurous', 'aggress', 'ambitio', 'analy', 'assert', 'athlet', 'autonom', 'boast', 'challeng', 'compet', 'confident', 'courag', 'decide', 'decisive', 'decision', 'determin', 'dominant', 'domina', 'force', 'greedy', 'headstrong', 'hierarch', 'hostil', 'implusive', 'independen', 'individual', 'intellect', 'lead', 'logic', 'masculine', 'objective', 'opinion', 'outspoken', 'persist', 'principle', 'reckless', 'stubborn', 'superior', 'self confiden', 'self sufficien', 'self relian'],\n                          \"feminine\"       : ['agree', 'affectionate', 'child', 'cheer', 'collab', 'commit', 'communal', 'compassion', 'connect', 'considerate', 'cooperat', 'co operat', 'depend', 'emotiona', 'empath', 'feel', 'flatterable', 'gentle', 'honest', 'interdependen', 'interpersona', 'inter personal', 'inter dependen', 'inter persona', 'kind', 'kinship', 'loyal', 'modesty', 'nag', 'nurtur', 'pleasant', 'polite', 'quiet', 'respon', 'sensitiv', 'submissive', 'support', 'sympath', 'tender', 'together', 'trust', 'understand', 'warm', 'whin', 'enthusias', 'inclusive', 'yield', 'share', 'sharin', 'affectionate', 'child', 'cheer', 'commit', 'communal', 'compassion', 'connect', 'considerate', 'cooperat', 'depend', 'emotiona', 'empath', 'feminine', 'flatterable', 'gentle', 'honest', 'interdependen', 'interpersona', 'kindkinship', 'loyal', 'modesty', 'nag', 'nurtur', 'pleasant', 'polite', 'quiet', 'respon', 'sensitiv', 'submissive', 'support', 'sympath', 'tender', 'together', 'trust', 'understand', 'warm', 'whin', 'yield', 'ease', 'permission', 'kindness', 'appreciation', 'caring', 'respect', 'devotion', 'validation', 'reassurance', 'respectful', 'love', 'communication', 'beauty', 'relationships', 'helping', 'sharing', 'relating', 'harmony', 'community', 'talking', 'intimate', 'life', 'healing', 'growth', 'intuitive', 'companionship', 'receive ', 'receiving', 'cherished', 'creativity', 'reassurance', 'worthy', 'supported ', 'supporting', 'nurture ', 'nurturing ', 'nurtured', 'feel', 'emotion'],\n                          \"superlatives\"   : ['expert', 'perfection', 'rockstar', 'specialist', 'authority', 'pundit', 'oracle', 'resource person', 'adept', 'maestro', 'virtuoso', 'master', 'past master', 'professional', 'genius', 'wizard', 'connoisseur', 'aficionado', 'cognoscenti', 'cognoscente', 'doyen', 'savant', 'ace', 'buff', 'ninja', 'pro', 'whizz', 'hotshot', 'old hand', 'alpha geek', 'dab hand', 'maven', 'crackerjack'],\n                          \"relationships\"  : ['family', 'child', 'parent', 'women', 'mother', 'father', 'son', 'daughter', 'kids', 'kid', 'married', 'household', 'home', 'sibling'],\n                          \"strict_words\"   : ['disqualified', 'rejected', 'must', 'should', 'required', 'banned', 'barred', 'disbarred', 'debarred', 'eliminated', 'precluded', 'disentitled', 'ineligible', 'unfit', 'unqualified', 'essential', 'desirable', 'desired'],\n                          \"strict_phrases\" : ['only be', 'who fail', 'not allowed', 'should have', 'is required', 'subject to', 'will not be considered', 'cannot be appointed', 'who lack'],\n                          \"mas_pronouns\"   : [\"he\", \"his\", \"him\", \"himself\"],\n                          \"fem_pronouns\"   : [\"she\", \"her\", \"herself\"],        \n                        }\n        \n    ## function to clean a text\n    def _cleanup(self, text):\n        text = text.lower()\n        text = \" \".join([c for c in text.split() if c not in stopwords])\n        for c in string.punctuation:\n            text = text.replace(c, \" \")\n        text = \" \".join([c for c in text.split() if c not in stopwords])\n\n        words = []\n        ignorewords = []\n        for wrd in text.split():\n            if len(wrd) <= 2: \n                continue\n            if wrd in ignorewords:\n                continue\n            words.append(wrd)\n        text = \" \".join(words)    \n        return text\n    \n    def _check_presence(self, flag, txt, condition):    \n        matched = {}\n        txt = \" \"+txt.lower()+\" \"\n        for wrd in self.lookups[flag]:\n            if condition == \"exact\":\n                cnt = txt.count(\" \"+wrd.lower()+\" \")\n                if cnt > 0:\n                    matched[wrd.lower()] = cnt\n            elif condition == \"startswith\":\n                cnt = txt.count(\" \"+wrd.lower())\n                if cnt > 0:\n                    matched[wrd.lower()] = cnt\n\n        return matched\n    \nimport numpy as np \n\ndef _gender_bias(txt, filename): \n    \n    ## create the object\n    jdp = JD_Purifier()\n    cln = jdp._cleanup(txt)\n\n    ## word choice\n    mas_words = jdp._check_presence(\"masculine\", cln, condition = 'startswith')\n    fem_words = jdp._check_presence(\"feminine\", cln, condition = 'startswith')\n    mas_wc, fem_wc = sum(mas_words.values()), sum(fem_words.values())\n\n    ## pronoun usage\n    mas_prns = jdp._check_presence(\"mas_pronouns\", cln, condition = 'exact')\n    fem_prns = jdp._check_presence(\"fem_pronouns\", cln, condition = 'exact')\n    masp_wc, femp_wc = sum(mas_prns.values()), sum(fem_prns.values())\n\n    ## relationships\n    relationships = jdp._check_presence(\"relationships\", cln, condition = 'exact')\n    relationships_wc = sum(relationships.values())\n\n    ## superlatives useage\n    superlatives = jdp._check_presence(\"superlatives\", cln, condition = 'exact')\n    superlatives_wc = len(superlatives.values())\n\n    # length of requirements\n    ## number of requirements\n\n    file_df = structured_df[structured_df[\"FILE_NAME\"] == filename]\n    if len(file_df) > 0:\n        salary = file_df['ENTRY_SALARY_GEN'].iloc(0)[0]\n        title = file_df['JOB_CLASS_TITLE'].iloc(0)[0]\n        date = file_df['OPEN_DATE'].iloc(0)[0]\n    else:\n        salary = \"\"\n        title = \"\"\n        date = \"\"\n    \n    doc = { \"mas_wc\"  : mas_wc,  \"fem_wc\" : fem_wc,\n            \"masp_wc\" : masp_wc, \"femp_wc\"   : femp_wc,\n            \"relationships_wc\" : relationships_wc, \n            \"superlatives_wc\"  : superlatives_wc,\n            \"superlatives_wrds\" : superlatives,\n            \"salary\" : salary,\n            \"title\" : title,\n            \"date\" : date,\n            \"mas_words\" : mas_words,\n            \"fem_words\" : fem_words}\n    return doc\n\n\nresults = []\nfor fname in os.listdir(base_path):\n    if fname == \"POLICE COMMANDER 2251 092917.txt\":\n        continue\n\n    txt = open(base_path + fname).read()\n    txt = \" \".join(txt.split(\"\\n\"))\n    doc = _gender_bias(txt, fname)\n    doc['filename'] = fname\n    results.append(doc)\n    \n    \ngender_df = pd.DataFrame(results)\n\n\ndef fix_sal(x):\n    if str(x).lower() == \"nan\":\n        x = \"\"\n    x = x.replace(\"$\", \"\").replace(\"\\t\",\"\")\n    x = x.split(\"flat\")[0].split(\"(\")[0].split(\", \")[0]\n    x = x.split(\"The\")[0].split(\"Some\")[0].replace(\"*\",\"\")\n    x = x.replace(\",\",\"\").replace(\".\",\"\").lower().strip()\n    \n    if \" to \" in x:\n        x = x.split(\" to \")\n        x = np.mean([float(_.strip()) for _ in x])\n    elif len(x) > 3:\n        x = float(x)\n    else:\n        x = 0.0\n\n    return x\n\ngender_df['_salary'] = gender_df['salary'].apply(lambda x : fix_sal(x))\n\n#### \nfemdict, masdict = {}, {}\nfor d in gender_df['fem_words']:\n    for k,v in d.items():\n        if k not in femdict:\n            femdict[k] = v\n        femdict[k] += v\nfor d in gender_df['mas_words']:\n    for k,v in d.items():\n        if k not in masdict:\n            masdict[k] = v\n        masdict[k] += v\n        \nfrom IPython.core.display import HTML\nhtml = HTML(\"\"\"<table align=\"center\">\n                    <tr><td><b>Feminine Denotation Words</b></td><td><b>Masculine Denotation Words</b></td></tr>\n                    <tr>\n                        <td><img src='https://i.imgur.com/W4ul7ii.png'></td>\n                        <td><img src='https://i.imgur.com/5nhWaoF.png'></td>\n                    </tr>\n                </table>\"\"\")\n\ndisplay(html)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div id=\"2.1\"></div>\n## <font color=\"#ff3fb5\">2.1 Use of Gendered Keywords in City of LA's Bulletins</font>   \n\nLet's look at how the gendered keywords are used in the job bulletins of City of Los Angeles bulletins. The following bar chart shows the frequency of different keywords in job bulletins.  "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import operator\nsorted_x = sorted(femdict.items(), key=operator.itemgetter(1), reverse = True)        \nyy = [_[0] for _ in sorted_x][:25][::-1]\nxx = [_[1] for _ in sorted_x][:25][::-1]\ntrace1 = go.Bar(y = yy, x= xx, width=0.6, \n    marker=dict(color='#ff77cd', line=dict(\n            color='#ff77cd',\n            width=1,\n        )),\n    orientation='h', name='')\n\nsorted_x = sorted(masdict.items(), key=operator.itemgetter(1), reverse = True)        \nyy = [_[0] for _ in sorted_x][:25][::-1]\nxx = [_[1] for _ in sorted_x][:25][::-1]\ntrace2 = go.Bar(y = yy, x= xx, width=0.6, \n    marker=dict(color='#54d1f7', line=dict(\n            color='#54d1f7',\n            width=1,\n        )),\n    orientation='h', name='')\n\nlayout = dict(\n    showlegend = False,\n    title='',\n    yaxis=dict(\n        showgrid=False,\n        showline=False,\n        showticklabels=True,\n    ),\n    xaxis=dict(\n        zeroline=False,\n        showline=False,\n        showticklabels=True,\n        showgrid=False,\n    ),\n    xaxis2=dict(\n        zeroline=False,\n        showline=False,\n        showticklabels=True,\n        showgrid=False,\n    ),\n    margin=dict( l=100, r=20, t=50, b=50),\n)\n\n# np.median(gender_df['fem_wc'])\n#np.median(gender_df['mas_wc'])\n\nfig = tools.make_subplots(rows=1, cols=2, shared_xaxes=True, print_grid = False,\n                          shared_yaxes=False, vertical_spacing=0.001, \n                          subplot_titles=[\"Keywords with Feminine Inclination\", \"Keywords with Masculine Inclination\"])\n\nfig.append_trace(trace1, 1, 1)\nfig.append_trace(trace2, 1, 2)\nfig['layout'].update(layout)\niplot(fig, filename='oecd-networth-saving-bar-line')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> - Keywords which depict feminine theme such as **\"respon*\" - responsible, respond, responsive** etc appears very often in the bulletins. One can observe the keywords related to life and relationships such as child, family, emotions, cooperation, respect, caring etc appear.     \n> - **On an average**, every job bulletin consists of about **8 feminine keywords** and about **24 masculine words**. This means that use of masculine keywords is three times as compared to that of feminine keywords in the City's bulletins.  \n> - The graphs shows that different keywords with masculine and feminine denotations appear in the City of Los Angeles Bulletins. By obtaining the frequency distribution of these words we can quantify and measure the gender bias. I have used a simple quantifying formula at this point (later improved in the kernel). \n\nGender Bias = Difference in Masculine and Feminine words used   \n\n<div id=\"2.2\"></div>\n## <font color=\"#ff3fb5\">2.2 Measuring the gender bias in City's Bulletins</font>  \n\nBased on the difference in masculine and femining keywords used in the bulletins, I have categorized them into one of the following categories: High/Medium/Low - Masculine/Feminine and Neutral. \n\nIf difference is higher than 35 keywords then it is high masculine, if it is lesser than -15 then it is high feminine. If the difference is between 10-35, then it is moderately masculine, if it is between -7 to -15 then it is moderately feminine, if it is between 3-10 then it has low masculine nature, if it is between -3 and -7 then it is classified as low feminine. All the remaining job descriptions are tagged as neutral ie. difference between -3 and 3. \n\nThe following plot shows the gender-bias distribution in City of LA jobs. "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"## Overall Distribution\ngender_df['difference'] = gender_df[\"fem_wc\"] - gender_df['mas_wc']\n\nvc = gender_df['difference'].value_counts().to_frame().reset_index()\nvc['index'] = vc['index'] * -1\ndef get_tag(x):\n    if x <= 3 and x >= -3:\n        return \"Neutral\"\n    elif x >=0 and x <= 10:\n        return \"Low Masculine\"\n    elif x >=0 and x <= 35:\n        return \"Moderate Masculine\"\n    elif x >=0 and x <= 75:\n        return \"High Masculine\"\n    elif x >= -7:\n        return \"Low Feminine\"\n    elif x >= -15:\n        return \"Moderate Feminine\"\n    else:\n        return \"High Feminine\"\n    return tag\n\nvc['bias_tag'] = vc['index'].apply(lambda x : get_tag(x))\ntemp = vc.groupby(\"bias_tag\").agg({\"difference\" : \"sum\"}).reset_index()\n\ntdoc = {}\nfor k,v in temp.iterrows():\n    tdoc[v['bias_tag']] = round(100*float(v['difference']) / 683, 2)\ntdoc['Low Feminine'] = 0\ntdoc['High Feminine'] = 0\n\n\norder = ['High Feminine', 'Moderate Feminine', 'Low Feminine', 'Neutral', 'Low Masculine', 'Moderate Masculine', 'High Masculine']\nsizes = [tdoc[i] for i in order]\n\nsizes = [5 if x < 2 else x for x in sizes]\nsizes = [10 if x < 3 else x for x in sizes]\nsizes = [14 if x < 4 else x for x in sizes]\n\ncols = [\"#ff77cd\", \"#ff77cd\", \"#ff77cd\", \"#72f9b1\", \"#42d4f4\", \"#54d1f7\", \"#54d1f7\", \"#54d1f7\"]\n\ndata = [go.Scatter(x = [_.replace(\" \",\"<br>\")+\"<br>(\"+str(tdoc[_])+\"%)\" for _ in order], \n                   y=[\"\",\"\",\"\",\"\",\"\",\"\", \"\"], mode='markers', name=\"\",  \n                   marker=dict(color=cols, opacity=1.0, size=sizes))]\nlayout = go.Layout(barmode='stack', height=300, margin=dict(l=120), \n#                    xaxis=dict(title='City of LA Job Bulletins'), \n                   legend = dict(orientation=\"h\", x=0.1, y=1.15), showlegend=False, \n                  xaxis=dict(showgrid= False, title='<br>City of LA Job Bulletins'), yaxis=dict(showgrid= False))\nfig = go.Figure(data=data, layout=layout)\ndisplay(HTML(\"<div align='center'><img src='https://i.imgur.com/kjdu1ah.png'></div>\"))\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> - A large proportion of jobs are tagged as moderate masculine while about 4% of the bulletins show very strong masculine. Only about 2% of the jobs are neutral.   \n> - At an overall level, more than 90% of the jobs have more masculine content, almost none of jobs have feminine bias.\n\n<div id=\"2.3\"></div>\n## <font color=\"#ff3fb5\">2.3 Bulletins with High Masculine (or Feminine) Denotation</font>  \n\nLet's now look at the jobs having high masculine tone (more usage of masculine gender words) or high feminine tone (more usage of feminine gender words). "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"temp1 = gender_df.sort_values('difference', ascending = True).head(20)\ntemp1 = temp1[temp1['title'] != \"\"]\ny1 = temp1['difference']* -1\nx1 = temp1['title']\n\ntrace1 = go.Bar(y=[x+\"&nbsp;&nbsp;&nbsp;&nbsp;\" for x in x1[::-1]], x=y1[::-1], width=0.6, \n    marker=dict(color='#54d1f7', line=dict(\n            color='#54d1f7',\n            width=1.5,\n        )),\n    orientation='h', name='High Masculine Bias')\ntrace2 = go.Scatter(x=[np.mean(gender_df['difference']*-1)]*20, y = [x+\"&nbsp;&nbsp;&nbsp;&nbsp;\" for x in x1[::-1]], mode='lines', name='Average Bias in All Descriptions')\n\nlayout = dict(\n    showlegend = False,\n    title='Jobs with High Masculine Bias',\n    yaxis=dict(\n        showgrid=False,\n        showline=False,\n        showticklabels=True,\n    ),\n    xaxis=dict(\n        title = \"Unconscious Gender Bias Score\",\n        zeroline=False,\n        showline=False,\n        showticklabels=True,\n        showgrid=False,\n    ),\n    margin=dict( l=300, r=20, t=50, b=50),\n)\n\nfig = go.Figure(data = [trace1, trace2], layout = layout)\niplot(fig)\n\n\n\n\n\n########## \n\n\ntemp1 = gender_df.sort_values('difference', ascending = False).head(1)\ny1 = temp1['difference']\nx1 = temp1['title']\n\ntrace1 = go.Bar(x=y1[::-1], y=[x+\"&nbsp;&nbsp;&nbsp;&nbsp;\" for x in x1[::-1]], width=0.3, \n    marker=dict(color='#ff77cd', line=dict(\n            color='#ff77cd',\n            width=1.5,\n        )),\n    orientation='h', name='Feminine Bias')\ntrace2 = go.Scatter(x=[np.mean(gender_df['difference']*-1)]*20, \n                    y = [x+\"&nbsp;&nbsp;&nbsp;&nbsp;\" for x in x1[::-1]], \n                    mode='lines', name='Average Bias in All Descriptions')\n\nlayout = dict(\n    showlegend = False,\n    title='Jobs with High Feminine Bias',\n    yaxis=dict(\n        showgrid=False,\n        showline=False,\n        showticklabels=True,\n    ),\n    xaxis=dict(\n        title = \"Unconscious Gender Bias Score\",\n        zeroline=False,\n        showline=False,\n        showticklabels=True,\n        showgrid=False,\n        range = (0,60)\n    ),\n    margin=dict( l=300, r=20, t=50, b=50),\n    height=170,\n)\n\nfig = go.Figure(data = [trace1], layout = layout)\niplot(fig)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> - **Average Line:** The orange line in the above graph represents the average difference in masculine and feminine words of all the job bulletins, which suggest that average is about 16.2. This means that on an average job bulletins uses 16 more masculine words than feminine words.     \n> - The job roles in this graph suggest that most of the **IT or Technology** realted jobs has very high levels of unconscious gender bias, For example - **Database Architect, System Programmer, Systems Analysts, and Programmer Analysts** are all one of the high masculine oriented jobs. When I tried to look for more information, I found this [Guardian article](https://www.theguardian.com/technology/2018/oct/11/tech-gender-problem-amazon-facebook-bias-women) which suggest a very important and interesting point. Now a days, many big companies want to use the data and AI to recruit good candidates, but since the job descriptions has the unconscious bias, AI systems will also fail.   \n> - \"**City Planner, Senior City Planner, City Planning Associate**\" : It was interesting to note that city planner jobs has high masculine bias. Infact, on gathering more information about these jobs, I realized that this is one big issue in the urban planning industry. Here is the quote from [NextCity](https://nextcity.org/features/view/urban-planning-sexism-problem)'s popular article: \"women remain seemingly consigned to supporting roles\", And from Forbes: [Why Women Matter In Urbanism And City Planning](https://www.forbes.com/sites/deborahtalbot/2018/05/08/why-women-matter-in-urbanism-and-city-planning/#6e4325d210cf))  \n> - Apart from this we also observe jobs related to Finance also shows high masculine bias. While only one job ie. **Community Affairs Advocate** has high feminine bias. This is due to the use of keywords like \"cooperation\", \"community\", and \"life\" which are more inclined towards feminine gender.     \n\n**Validation using External Data**  \n\nAdditionally, the fellow kagglers also found an external [data source](https://catalog.data.gov/dataset/applicant-information-from-7-1-2014-to-9-30-2014-7835b) about City's Job Applicant's by different roles. Though this data is from 2014, but it can be used to check if the high level insights are aligned. A quick look in this data suggest following: \n\n> - For City Planner Class, out of all the applicants (60) who applied, 2/3rd were males (36) which is aligned with gender bias insights from the graph.   \n> - For Financial Analyst role, more number of male candidates applied to the role than females.    \n\nThese examples suggest that unconscious bias in job description may have a causal effect on potential applicants.     \n\n\n<div id=\"2.4\"></div>\n## <font color=\"#ff3fb5\">2.4 Masculine and Feminine Words Usage</font>  \n\nLet's now look at the combined usage of masculine words and feminine words used in the job description text files. In the following graph, x-axis represents number of masculine words used, y-axis is number of feminine words used, and size of every node represents absolute difference in feminine and masculine words used. "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"trace = go.Scatter(x = gender_df['mas_wc'], y = gender_df['fem_wc'], mode = 'markers', text=gender_df['title'],\n                   marker=dict(opacity=1.0, size=np.abs(gender_df['difference']/2), \n                              colorscale=[\n        [0,\n          \"#ff77cd\"\n        ],\n        [\n          0.10,\n          \"#fc99d8\"\n        ],\n        [\n          0.25,\n          \"#f9c5e6\"\n        ],\n        [\n          0.40,\n          \"#fceaf5\"\n        ],\n        [\n          0.65,\n          \"#bce5f2\"\n        ],\n        [\n          0.85,\n          \"#8ddcf4\"\n        ],\n        [\n          1,\n          \"#54d1f7\"\n        ]], color=np.abs(gender_df['difference']),\n                              showscale=True), name=\"Every Point is a Bulletin\")\nlayout = go.Layout(yaxis=dict(range=[0,40], showgrid=False, title=\"Feminine Words Usage\"), showlegend = True,\n                   xaxis=dict(title=\"Masculine Words Usage\", range=[0,80], showgrid=False),\n                   title = \"Masculine and Feminine Words in Bulletins\",\n                  legend = dict(x=0.4, y=1.1))\nfig = go.Figure(data = [trace], layout = layout)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> - As every node is on job bulletin (Hover to view the job class), The graph suggest that it is skewed on the masculine side. Mainly the bigger blue nodes are the ones having more than 50 masculine keywords and less than 15 feminine keywords.   \n> - City' should try to use less masculine keywords so that this graph looks balanced from both sides.   \n\n<div id=\"2.5\"></div>\n## <font color=\"#ff3fb5\">2.5 Does More Masculine Class means Higher Salary?</font>  \n\nLet's check if high masculine job roles also have high salary offerings. "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"gender_df['title_word'] = gender_df['title'].apply(lambda x : str(x).split()[-1] if len(x.split())> 1 else str(x) )\ntemp = gender_df.groupby(\"title_word\").agg({'difference' : 'mean', 'title' : 'count', '_salary' : 'mean'})\ntemp['difference'] = temp['difference'] * -1\ntemp = temp[temp['title'] > 1]\ntemp = temp.sort_values('difference', ascending = [False]).reset_index()\ntemp1 = temp.head(10)\nx2 = temp1['_salary'] \nx1 = temp1['difference']\ny1 = temp1['title_word']\n\n\ntrace1 = go.Bar(y=[x+\"&nbsp;&nbsp;&nbsp;&nbsp;\" for x in y1[::-1]], x=x1[::-1], width=0.3, \n    marker=dict(color='#54d1f7', opacity=0.6),\n    orientation='h', name='Masculine Bias')\ntrace2 = go.Bar(y=[x+\"&nbsp;&nbsp;&nbsp;&nbsp;\" for x in y1[::-1]], x=x2[::-1], width=0.3, \n    marker=dict(color='red', opacity=0.6),\n    orientation='h', name='Salary')\ntrace6 = go.Scatter(y = [x+\"&nbsp;&nbsp;&nbsp;&nbsp;\" for x in y1[::-1]], x = [np.mean(gender_df['_salary'])]*10, mode = 'lines',\n                   name = 'Average of all Job Salaries')\n\ntemp = temp.sort_values('difference', ascending = [True]).reset_index()\ntemp = temp.head(10)\nx2 = temp['_salary'] \n\nx1 = temp['difference']\ny1 = temp['title_word']\n\ntrace3 = go.Bar(y=[x+\"&nbsp;&nbsp;&nbsp;&nbsp;\" for x in y1[::-1]], x=x1, width=0.3, \n    marker=dict(color='#54d1f7', opacity=0.6),\n    orientation='h', name='')\ntrace4 = go.Bar(y=[x+\"&nbsp;&nbsp;&nbsp;&nbsp;\" for x in y1[::-1]], x=x2, width=0.3, \n    marker=dict(color='red', opacity=0.6),\n    orientation='h', name='')\n\ntrace5 = go.Scatter(y = [x+\"&nbsp;&nbsp;&nbsp;&nbsp;\" for x in y1[::-1]], x = [np.mean(gender_df['_salary'])]*10, mode = 'lines',\n                   name = '')\n\n\nfig = tools.make_subplots(rows=1, cols=5, shared_xaxes=True, print_grid = False,\n                          shared_yaxes=False, vertical_spacing=0.001, \n                          subplot_titles=[\"More Masculine\", \"Salary\", \"\",\"Less Masculine\", \"Salary\"])\n\n\n\nlayout = dict(\n    yaxis=dict(\n        showgrid=False,\n        showline=False,\n        showticklabels=True,\n    ),\n    xaxis=dict(\n        zeroline=False,\n        showline=False,\n        showticklabels=True,\n        showgrid=False,\n    ),\n    xaxis2=dict(\n        zeroline=False,\n        showline=False,\n        showticklabels=True,\n        showgrid=False,\n    ),\n    yaxis2=dict(\n        zeroline=False,\n        showline=False,\n        showticklabels=False,\n        showgrid=False,\n    ),\n    yaxis5=dict(\n        zeroline=False,\n        showline=False,\n        showticklabels=False,\n        showgrid=False,\n    ),\n\n    xaxis4=dict(\n        range = (0,40),\n        zeroline=False,\n        showline=False,\n        showticklabels=True,\n        showgrid=False,\n    ),\n    xaxis5=dict(\n        zeroline=False,\n        showline=False,\n        showticklabels=True,\n        showgrid=False,\n    ),\n    margin=dict( l=100, t=50, b=50),\n    height=400,\n    showlegend=False,\n    title = \"\",\n    legend = dict(x = 0.4, y = 1.09, orientation = 'h')\n)\n\nfig.append_trace(trace1, 1, 1)\nfig.append_trace(trace2, 1, 2)\nfig.append_trace(trace6, 1, 2)\nfig.append_trace(trace3, 1, 4)\nfig.append_trace(trace4, 1, 5)\nfig.append_trace(trace5, 1, 5)\nfig['layout'].update(layout)\n\ndisplay(HTML(\"<div align='center'><img src='https://i.imgur.com/xjfP8x9.png'></div>\"))\n\niplot(fig, filename='oecd-networth-saving-bar-line')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> - We can observe that roles like \"Planner\", \"Analysts\", \"Architects\", \"Programmers\" shows high masculine bias in the job descriptions. And interesting to note that the salaries offered for these roles are all on the higher ends as well. The green line in this graph is the average salary of all the job roles. We can observe that most of the roles in the left graph have equal to or greater than average salaries.      \n> - The average salary of roles having high masculine bias is about 102.7K while it is about 71K for roles having low masculine bias.     \n> - In contrast to roles like Educator, Trainee, Instructor having less masculine bias but also the salaries are on lower end, even less than average.   \n\n<div id=\"2.6\"></div>\n## <font color=\"#ff3fb5\">2.6 Are (Masculine) Classes Offered Higher Salaries ?</font>  \n\nIs it true that the job roles which tend to attract more male applicants are also offered higher than average salaries. Let's plot the salary and gender bias distribution. "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"t = gender_df[gender_df[\"_salary\"] > 0]\ntrace = go.Scatter(x = t['difference']*-1, y = t['_salary'], text = t['title'],\n                   mode = 'markers', marker=dict(color=\"purple\", opacity=0.5), name='Bulletins')\n\nlayout = {\n    'xaxis' : {'title' : \"Difference in Masculine and Feminine Words Used\", 'showgrid' : False},\n    'yaxis' : {'title' : \"Average Salary\", 'showgrid' : False},\n    'title' : \"Gender Bias and Salary\",\n    'legend': {\"x\" : 0.4, \"y\" : 1.09, \"orientation\" : 'h'},\n    'shapes': [\n        {\n            'type': 'rect',\n            'xref': 'x',\n            'yref': 'paper',\n            'x0': 38,\n            'y0': 0.28,\n            'x1': 63,\n            'y1': 0.55,\n            'fillcolor': 'orange',\n            'opacity': 0.2,\n            'line': {\n                'width': 0,\n            }\n        }]}\n\n\ntrace1 = go.Scatter(x = t['difference']*-1, \n                    y = [np.mean(t['_salary'])]*len(gender_df), mode = 'lines',\n                    name = 'Average of all Job Salaries')\nfig = go.Figure(data = [trace, trace1], layout = layout)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> - The orange horizontal line is the average salary offered in all of the jobs and is equal to about USD 90,000.   \n> - From this graph, we observe that the jobs having higher value of gender bias (Orange Box) score have almost greater or equal to average salary offered. \n\n<div id=\"2.7\"></div>\n## <font color=\"#ff3fb5\">2.7 Does Job Seniority Levels also show Gender Bias?</font>  \n\nFrom the job titles, I identified the job seniority level. Let's plot the aggregated gender bias shown in their job descriptions. "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def get_seniority(x):\n    wrds = ['senior', 'junior', 'head', 'director', 'lead', 'president', 'chairmen', 'chief', 'assistant', 'worker']\n    for w in wrds:\n        if w in x.lower():\n            return w.title()\n    return \"\"\ngender_df['seniority'] = gender_df['title'].apply(lambda x : get_seniority(x) )\n\ntemp = gender_df.groupby(\"seniority\").agg({'difference' : 'mean', 'title' : 'count', '_salary' : 'mean'})\ntemp['difference'] = temp['difference'] * -1\ntemp = temp[temp['title'] > 1]\ntemp = temp.sort_values('difference', ascending = [False]).reset_index()\ntemp1 = temp.head(10)\ntemp1\n\n# Job titles that include the phrase 'assistant' strongly lean towards female-gendered language (28% male bias vs. 58% female bias)\n\n\ntdoc = {}\nsdoc = {}\nfor k,v in temp1.iterrows():\n    if not v['seniority']:\n        v['seniority'] = \"All Others\"\n    tdoc[v['seniority']] = round(v['difference'], 2)\n    sdoc[v['seniority']] = str(round(v['_salary']/1000))+\"K\"\n\ncols = [\"#ff77cd\", \"#ff77cd\", \"#ff77cd\", \"#72f9b1\", \"#42d4f4\", \"#42d4f4\", \"#42d4f4\", \"#42d4f4\"]\n\ndata = [go.Scatter(x = [_+\"<br>(Bias: \"+str(tdoc[_])+\")<br>$ \"+ str(sdoc[_]) for _ in list(tdoc.keys())], y=[\"\",\"\",\"\",\"\",\"\",\"\", \"\"], mode='markers', name=\"\",  \n                   marker=dict(symbol=\"circle\", \n                   color=list(reversed([52, 40, 25, 20,17,17])), colorscale='Blues', opacity = 1.0,\n                   size= [52, 40, 25, 20,17,17]  ))]\nlayout = go.Layout(barmode='stack', height=300, margin=dict(l=100), title='Gender Bias in Seniorty Levels', \n                   legend = dict(orientation=\"h\", x=0.1, y=1.15), showlegend=False, \n                  xaxis=dict(showgrid= False), yaxis=dict(showgrid= False))\nfig = go.Figure(data=data, layout=layout)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> - City of LA jobs which are more senior in position shows higher level of unconscious gender bias. As we can observe all the Director and Chief related toles have higher average bias. While this is not true for roles having \"Senior\" job position. \n\n<div id=\"2.8\"></div>\n## <font color=\"#ff3fb5\">2.8 Has the use of gendered language changed over time ?</font>  \n\nLet's look at has the use of gendered language changed over the years. "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# use date\ngender_df['year'] = pd.to_datetime(gender_df['date']).dt.year#.astype(str)# + \"-\" + pd.to_datetime(gender_df['date']).dt.month.astype(str)\ntemp = gender_df.groupby('year').agg({'difference' : 'mean', 'title_word' : 'count', 'mas_wc' : 'mean', 'fem_wc' : 'mean'})\ntemp = temp.reset_index()\ntemp['difference'] = temp['difference'] * -1\ntemp = temp[temp['year'] > 2013]\ntemp = temp[temp['year'] < 2019]\ntemp['year'] = \"Year: \" + temp['year'].astype(int).astype(str)\n\n\ntrace0 = go.Scatter( x = temp.year, y = temp.difference, name=\"Average Gender Bias\", marker = dict(color=\"green\", opacity = 0.6))\ntrace1 = go.Scatter( x = temp.year, y = temp.mas_wc, name=\"Usage: Masculine Words\", marker = dict(color=\"#42d4f4\"))\ntrace2 = go.Scatter( x = temp.year, y = temp.fem_wc, name=\"Usage: Feminine Words\", marker = dict(color=\"#ff77cd\"))\ndata = [trace0, trace1, trace2]\nlayout = go.Layout(title=\"Gender Bias over the Years\",\n                  yaxis = dict(showgrid = False, title = \"\"),\n                  xaxis = dict(showgrid = False), legend = dict(x = 0.1, y = 1.05, orientation = 'h'),\n                  )\nfig = go.Figure(data, layout)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div id=\"3\"></div>\n## <font color=\"#ff3fb5\">3. Other Forms of Gender Bias</font>   \n\nUnconscious Gender bias may not occur only due to use of masculine or feminine keywords, it can also take other forms. In this section, I have discussed and analyzed other forms of gender bias.  \n\n<div id=\"3.1\"></div>\n## <font color=\"#ff3fb5\">3.1 Use of Superlatives / Relationship keywords </font>   \n\nLess use of superlatives is better. According to [Glassdoor](https://www.glassdoor.com/employers/blog/10-ways-remove-gender-bias-job-listings/): Excessive use of superlatives such as “expert,” “superior,” “world class” can turn off female candidates who are more collaborative than competitive in nature. Research also shows that women are less likely than men to brag about their accomplishments. In addition, superlatives related to a candidate’s background can limit the pool of female applicants because there may be very few females currently in leading positions at “world class” firms.\n\nThe following plot shows which superlative keywords are mostly used in the CoLA job descriptions. \n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"supdict = {}\nfor d in gender_df['superlatives_wrds']:\n    for k,v in d.items():\n        if k not in supdict:\n            supdict[k] = v\n        supdict[k] += v\n\ndel supdict['pro']\ndel supdict['ace']\n\nimport squarify\ndef createhmap1(keys, vals):\n    x = 0.\n    y = 0.\n    width = 100.\n    height = 100.\n    colcnt = 0\n    values = vals\n\n    normed = squarify.normalize_sizes(values, width, height)\n    rects = squarify.squarify(normed, x, y, width, height)\n\n    color_brewer = ['#f4c242']\n    shapes = []\n    annotations = []\n    counter = 0\n\n    for r in rects:\n        shapes.append( \n            dict(\n                type = 'rect', \n                x0 = r['x'], \n                y0 = r['y'], \n                x1 = r['x']+r['dx'], \n                y1 = r['y']+r['dy'],\n                line = dict( width = 5, color=\"#fff\" ),\n                fillcolor = '#f4c242'\n            ) \n        )\n        annotations.append(\n            dict(\n                x = r['x']+(r['dx']/2),\n                y = r['y']+(r['dy']/2),\n                text = str(list(keys)[counter]) +\" (\"+ str(values[counter]) + \")\",\n                showarrow = False\n            )\n        )\n        counter = counter + 1\n        colcnt+=1\n        if colcnt >= len(color_brewer):\n            colcnt = 0\n\n    # For hover text\n    trace0 = go.Scatter(\n        x = [ r['x']+(r['dx']/2) for r in rects ], \n        y = [ r['y']+(r['dy']/2) for r in rects ],\n        text = [ str(v)+\" (\"+str(values[k])+\" )\" for k,v in enumerate(keys) ], \n        mode = 'text',\n    )\n\n    layout = dict(\n        height=500, width=800,\n        margin = dict(l=100),\n        xaxis=dict(\n                autorange=True,\n                showgrid=False,\n                zeroline=False,\n                showline=False,\n                ticks='',\n                showticklabels=False\n            ),\n        yaxis=dict(\n                autorange=True,\n                showgrid=False,\n                zeroline=False,\n                showline=False,\n                ticks='',\n                showticklabels=False\n            ),\n        shapes=shapes,\n        annotations=annotations,\n        #hovermode='closest',\n        title=\"Superlatives used in CoLA Jobs\"\n    )\n\n    figure = dict(data=[trace0], layout=layout)\n    iplot(figure, filename='squarify-treemap')\n\n\n\ncreatehmap1(list(supdict.keys()), list(supdict.values()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look at which job roles uses these keywords the most. Also, the job roles which uses the keywords related to relationships the most. The significance of relationship related keywords is that those wordings is highly preferred by women. Hence it is encouraged to use more such keywords while writing the job descriptions. "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"temp1 = gender_df.sort_values('superlatives_wc', ascending = False).head(13)\ntemp1 = temp1[temp1['title'] != \"\"]\ny1 = temp1['superlatives_wc']\nx1 = temp1['title']\n\ntrace1 = go.Bar(x=y1[::-1], y=[x+\"&nbsp;&nbsp;&nbsp;&nbsp;\" for x in x1[::-1]], width=0.3, \n    marker=dict(color='red', opacity=0.6),\n    orientation='h', name='')\n\n\ntemp1 = gender_df.sort_values('relationships_wc', ascending = False).head(12)\ny1 = temp1['relationships_wc']\nx1 = temp1['title']\n\ntrace2 = go.Bar(x=y1[::-1], y=[x+\"&nbsp;&nbsp;&nbsp;&nbsp;\" for x in x1[::-1]], width=0.3, \n    marker=dict(color='blue', opacity=0.6),\n    orientation='h', name='')\n\n\nfig = tools.make_subplots(rows=1, cols=3, shared_xaxes=True, print_grid = False,\n                          shared_yaxes=False, vertical_spacing=0.001, subplot_titles=[\"Superlative Words Usage\",\"\",\"Relationship Words Usage\"])\n\nfig.append_trace(trace1, 1, 1)\nfig.append_trace(trace2, 1, 3)\n\nlayout = dict(\n    title='',\n    yaxis=dict(\n        showgrid=False,\n        showline=False,\n        showticklabels=True,\n    ),\n    xaxis=dict(\n        range = (0,6),\n        title = \"\",\n        zeroline=False,\n        showline=False,\n        showticklabels=True,\n        showgrid=False,\n    ),\n    xaxis3=dict(\n        range = (0,5),\n        title = \"\",\n        zeroline=False,\n        showline=False,\n        showticklabels=True,\n        showgrid=False,\n    ),\n    margin=dict( l=300, t=50, b=50),\n    height=400,\n    width=1000,\n    showlegend=False\n)\n\nfig['layout'].update(layout)\niplot(fig, filename='oecd-networth-saving-bar-line')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div id=\"3.2\"></div>\n## <font color=\"#ff3fb5\">3.2 Use of \"Master\"/\"Expert\" keyword </font>   \nLet's look at some of the examples where these keywords, specifically \"Master\" and \"Expert\" are used in City's bulletins. Both of these keywords have a very strong masculine denotation and should be avoided in job class titles and job bulletin text."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import os\n\np = \"../input/data-science-for-good-city-of-los-angeles/cityofla/CityofLA/Job Bulletins/\"\n\ninterest = ['expert', 'master']\n\ninterest_titles = ['Senior Systems Analyst',\n 'Chief Port Pilot',\n 'Programmer Analyst',\n 'Systems Analyst',\n 'Systems Programmer',\n 'Port Pilot',\n 'Fingerprint Identification Expert']\n\nhtml = \"<div><hr>\"\nfor f in os.listdir(p):\n    try:\n        title = f.title()\n        ind = [i for i, j in enumerate(title) if j in \"01234567890\"][0]\n        title = title[:ind].strip()\n    except Exception as E:\n        continue\n    if title not in interest_titles:\n        continue\n    try:\n        txt = open(p + f).read()\n    except:\n        continue\n    words = txt.lower().split()\n    \n    is_rel = False\n    for w in words:    \n        if w in interest:\n            is_rel = True\n    \n    if is_rel:\n        try:\n            html += \"<h2><font color='Blue'>Job: \" + title + \"</font></h2>\"\n            for l in txt.split(\"\\n\"):\n                if any(w in l.lower() for w in interest):\n                    if any(x in l.lower() for x in ['panel', 'review', 'testimony']):\n                        continue\n                    l = l.replace(\"$\",\"USD \")\n                    words = l.split()\n\n                    l = \" \".join([\"<span style='background-color:Blue; padding:5px'><font color='#fff'>\"+w+\"</font></span>\" if w.lower() in interest else w for w in words])\n                    html += \"<p style='text-align:justify'>\" + l + \"</p>\"\n        except:\n            pass\nhtml += \"</div><hr>\"\n    \ndisplay(HTML(html))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div id=\"3.3.a\"></div>\n## <font color=\"#ff3fb5\">3.3 Lengthy Requirements ? </font>  \n\nRequirement section is one of the most important section of a job bulletin. It is very important to write the requirement text carefully. Lengthy requirements gives an impression that the particular job role demands many different minimum requirements. According to different research, it was observed that men like to apply for a job even if they met 60% of the requirements, but **women will only apply if they meet 100% of the requirements**. Hence it is advisable to use less number of requirements. [Harvard Business Review](https://www.hbs.edu/recruiting/blog/post/simple-ways-to-take-gender-bias-out-of-your-jobs) also suggest the same recommendation. The following graph shows how many lines are there in bulletins of City of LA.  "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"temp = structured_df.groupby(\"JOB_CLASS_TITLE\").agg({\"REQUIREMENT_SET_ID\" : \"max\"})\ntemp = temp.reset_index().sort_values(\"REQUIREMENT_SET_ID\", ascending = False)\ntemp = gender_df.merge(temp.rename(columns = {\"JOB_CLASS_TITLE\" : \"title\", \"REQUIREMENT_SET_ID\" : \"Req_Count\"}), on = 'title')\n\nvc = temp['Req_Count'].value_counts().to_frame().reset_index()\nvc['index_'] = vc['index'].apply(lambda x : \"4+\" if x >= 4 else x)\nt = vc.groupby(\"index_\").agg({\"Req_Count\" : \"sum\"}).reset_index()\n\n\n\ncolors = [ '#96D38C', '#D0F9B1', '#E1396C', 'red']\n\ntrace = go.Pie(labels=t['index_'], values=t['Req_Count'],\n               hoverinfo='label+percent', textinfo='value', \n               textfont=dict(size=20), hole = 0.77, \n               marker=dict(colors=colors, \n                           line=dict(color='#fff', width=10)))\nlayout = go.Layout(title = \"Number of Requirements in CoLA Jobs\", width=500)\nfig = go.Figure(data = [trace], layout = layout)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> - More than 100 job roles have more than 3 lines in requirements. These bullet points can be minimized. I have added another suggestion regarding the requirements in the next kernel. \n\n<div id=\"3.4\"></div>\n## <font color=\"#ff3fb5\">3.4 Keywords containing \"man\"</font>   \n\nSome keywords are so common that one hardly thinks about them. Some keywords like “workmanship”, \"wireman\", \"journeyman\" etc. suggest that company is biased towards hiring men (and not women). Let's look at some examples in City's bulletins.  \n\n"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import os\n\np = \"../input/data-science-for-good-city-of-los-angeles/cityofla/CityofLA/Job Bulletins/\"\ninterest = ['workmanship', 'wireman', 'seamanship', 'seaman', 'repairman', 'lineman', 'journeyman']\n\nhtml = \"<div><hr>\"\nfor f in os.listdir(p):\n    try:\n        txt = open(p + f).read()\n    except:\n        continue\n    words = txt.lower().split()\n    is_rel = False\n    for w in words:    \n        if w in interest:\n            is_rel = True\n    \n    if is_rel:\n        title = f.title()\n        ind = [i for i, j in enumerate(title) if j in \"1234567890\"][0]\n        title = title[:ind]\n        html += \"<h2><font color='red'>Job: \" + title + \"</font></h2>\"\n        for l in txt.split(\"\\n\"):\n            if any(w in l.lower() for w in interest):\n                words = l.split()[:85]\n                l = \" \".join([\"<span style='background-color:orange; padding:5px'><font color='#000'>\"+w.replace(\"'s\",\"\")+\"</font></span>\" \n                              if w.lower().replace(\"'s\",\"\") in interest else w for w in words])\n                html += \"<p style='text-align:justify'>\" + l + \"</p>\"\nhtml += \"<hr></div>\"\n    \ndisplay(HTML(html))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> - Instead of these words, City' can use different words which are common to both genders or are neutral. For example - \"Chairperson\" as compared to \"chairmen\" can be used. [Ongig Blog](https://blog.ongig.com/diversity-and-inclusion/gender-neutral-job-titles) lists what are some other alternatives. "},{"metadata":{},"cell_type":"markdown","source":"<div id=\"3.5\"></div>\n## <font color=\"#ff3fb5\">3.5 Bad Assumtion that there are only two genders </font>   \n\nThere is a big debate on internet about the numbers of genders. It is a bad assumption to only specify two genders as it is another form of unconscious gender bias. Typical example is the use of keyword - he/she. Let's see which roles in city's bulletins have the usage of he/she. "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"interest = ['he/she']\n\nhtml = \"<div><hr>\"\nfor f in os.listdir(p):\n    try:\n        txt = open(p + f).read()\n    except:\n        continue\n    words = txt.lower().split()\n    is_rel = False\n    for w in words:    \n        if w in interest:\n            is_rel = True\n    \n    if is_rel:\n        title = f.title()\n        ind = [i for i, j in enumerate(title) if j in \"1234567890\"][0]\n        title = title[:ind]\n        html += \"<h2><font color='purple'>Job: \" + title + \"</font></h2>\"\n        for l in txt.split(\"\\n\"):\n            if any(w in l.lower() for w in interest):\n                l = l.replace(\"$\",\"USD \")\n                words = l.split()\n                \n                l = \" \".join([\"<span style='background-color:purple; padding:5px'><font color='#fff'>\"+w+\"</font></span>\" if w.lower() in interest else w for w in words])\n                html += \"<p style='text-align:justify'>\" + l + \"</p>\"\nhtml += \"</hr></div>\"\n    \ndisplay(HTML(html))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> - A simple correction will be to use \"the applicant\" instead of \"he/she\".  \n\n<div id=\"4\"></div>\n## <font color=\"#ff3fb5\">4. Quantifying Unconscious Gender Bias in Job Bulletins </font>   \n\nIn this section, I have compiled all the components and metrics analyzed in previous sections to give an aggregated gender bias score for every bulletin. I have developed the following methodology. \n\n<span align='center'><br><img src='https://i.imgur.com/fzUtIfm.png' width=\"250\"></span>\n\nCity can ofcourse tweak this function and give weights according to their own hypothesis and some hit and trial. Following cell shows the weighing model and scores for every bulletin. "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def find_score(x):\n    d = x['difference'] * -1\n    if d > 50:\n        f = 10\n    elif d > 42:\n        f = 8.2\n    elif d > 33:\n        f = 6.5\n    elif d > 24:\n        f = 5.4\n    elif d > 15:\n        f = 3.8\n    elif d > 10:\n        f = 3.0\n    elif d > 5:\n        f = 1.5\n    elif d > 1:\n        f = 1\n    else:\n        f = 0\n        \n    d = x['superlatives_wc']\n    if d > 5:\n        f2 = 10\n    elif d > 3:\n        f2 = 7\n    elif d > 1:\n        f2 = 4\n    else:\n        f2 = 0\n\n    d = x['relationships_wc']\n    if d < 5:\n        f3 = 10\n    elif d < 3:\n        f3 = 7\n    elif d < 1:\n        f3 = 4\n    else:\n        f3 = 0\n    \n    score = f*0.70 + f2*0.20 + f3*0.10\n    return score\n\ngender_df['unconscious_bias'] = gender_df.apply(lambda x : find_score(x), axis = 1)\ngender_df.sort_values(\"unconscious_bias\", ascending = False)[['title', 'unconscious_bias']].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look at some bulletins and their aggregated scores. "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import matplotlib\nfrom matplotlib import cm\nfrom matplotlib import pyplot as plt\nfrom matplotlib.patches import Circle, Wedge, Rectangle\n\ndef degree_range(n): \n    start = np.linspace(0,180,n+1, endpoint=True)[0:-1]\n    end = np.linspace(0,180,n+1, endpoint=True)[1::]\n    mid_points = start + ((end-start)/2.)\n    return np.c_[start, end], mid_points\ndef rot_text(ang): \n    rotation = np.degrees(np.radians(ang) * np.pi / np.pi - np.radians(90))\n    return rotation\n\ndef gauge(labels=['LOW','MEDIUM','HIGH','VERY HIGH','EXTREME'], \\\n          colors='jet_r', arrow=1, title='', fname=False): \n    \n    \"\"\"\n    some sanity checks first\n    \n    \"\"\"\n    \n    N = len(labels)\n    \n    if arrow > N: \n        raise Exception(\"\\n\\nThe category ({}) is greated than \\\n        the length\\nof the labels ({})\".format(arrow, N))\n \n    \n    \"\"\"\n    if colors is a string, we assume it's a matplotlib colormap\n    and we discretize in N discrete colors \n    \"\"\"\n    \n    if isinstance(colors, str):\n        cmap = cm.get_cmap(colors, N)\n        cmap = cmap(np.arange(N))\n        colors = cmap[::-1,:].tolist()\n    if isinstance(colors, list): \n        if len(colors) == N:\n            colors = colors[::-1]\n        else: \n            raise Exception(\"\\n\\nnumber of colors {} not equal \\\n            to number of categories{}\\n\".format(len(colors), N))\n\n    \"\"\"\n    begins the plotting\n    \"\"\"\n    \n    fig, ax = plt.subplots()\n\n    ang_range, mid_points = degree_range(N)\n\n    labels = labels[::-1]\n    \n    \"\"\"\n    plots the sectors and the arcs\n    \"\"\"\n    patches = []\n    for ang, c in zip(ang_range, colors): \n        # sectors\n        patches.append(Wedge((0.,0.), .4, *ang, facecolor='w', lw=2))\n        # arcs\n        patches.append(Wedge((0.,0.), .4, *ang, width=0.10, facecolor=c, lw=2, alpha=0.5))\n    \n    [ax.add_patch(p) for p in patches]\n\n    \n    \"\"\"\n    set the labels (e.g. 'LOW','MEDIUM',...)\n    \"\"\"\n\n    for mid, lab in zip(mid_points, labels): \n\n        ax.text(0.35 * np.cos(np.radians(mid)), 0.35 * np.sin(np.radians(mid)), lab, \\\n            horizontalalignment='center', verticalalignment='center', fontsize=14, \\\n            fontweight='bold', rotation = rot_text(mid))\n\n    \"\"\"\n    set the bottom banner and the title\n    \"\"\"\n    r = Rectangle((-0.4,-0.1),0.8,0.1, facecolor='w', lw=2)\n    ax.add_patch(r)\n    \n    ax.text(0, -0.05, title, horizontalalignment='center', \\\n         verticalalignment='center', fontsize=12)\n\n    \"\"\"\n    plots the arrow now\n    \"\"\"\n    \n    pos = mid_points[abs(arrow - N)]\n    \n    ax.arrow(0, 0, 0.225 * np.cos(np.radians(pos)), 0.225 * np.sin(np.radians(pos)), \\\n                 width=0.01, head_width=0.02, head_length=0.1, fc='k', ec='k')\n    \n    ax.add_patch(Circle((0, 0), radius=0.02, facecolor='k'))\n    ax.add_patch(Circle((0, 0), radius=0.01, facecolor='w', zorder=11))\n\n    \"\"\"\n    removes frame and ticks, and makes axis equal and tight\n    \"\"\"\n    \n    ax.set_frame_on(False)\n    ax.axes.set_xticks([])\n    ax.axes.set_yticks([])\n    ax.axis('equal')\n\ncols = ['#007A00','#0063BF','#FFCC00','#e58722','#d6202f', '#007A00','#0063BF','#FFCC00','#e58722','#d6202f']\nlabels = ['1','2','3','4','5', '6','7', '8','9','10']\n\ngauge(labels=labels, colors=cols, arrow=10, title='Benefits Specialist (Bias: 8.8 / 10)')\ngauge(labels=labels, colors=cols, arrow=7, title='Auditor (Bias: 6.3 / 10)')\ngauge(labels=labels, colors=cols, arrow=5, title='Tree Surgeon (Bias: 4.7 / 10)')\ngauge(labels=labels, colors=cols, arrow=2, title='Art Instructor (Bias: 1.7 / 10)')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"<div id=\"5\"></div>\n## <font color=\"#ff3fb5\">5. A Design Experiment for Validation </font>   \n\n<div id=\"5.1\"></div>\n### <font color=\"#ff3fb5\">5.1 Analogy from Econometric Theory</font>      \n\nTo design the validation experiment, we need to consider one analogy from the econometric theory. A typical problem statement among the economists is to identify the **consumer demand function** This function suggests, how the consumer's willingness to pay for something changes as the market prices of goods or prices changes. Economists model the consumer preferences using a utility function. Here is a very simple example, it is not real world at all but it delivers some nice intuitive results:\n\n1. Suppose we are looking at a market populated by z individuals.   \n2. Each individual **i** in this market buys either 0 or 1 unit of some good produced by one or several firms    \n3. Each individual has a parameter **Bi** that measures perceived quality of the produced good (think of it as measuring the joy associated with consuming the good)   \n4. Let P be the price of goods in the market.   \n5. Consumer utility from consuming the good is defined simplistically as: **Ui = Bi - P**    \n6. Individual i will buy the good if and only if Ui > 0 i.e. if Bi > P     \n\nIn simple terms, if individual's preference parameter has higher value than the price of the good then individual will buy the goods else they will not. \n\nSource : [Advanced Industrial Organization: Identification of Demand Functions](http://www.soderbom.net/demand_final_slides11.pdf) by Mans Soderbom, University of Gothenburg\n\n<div id=\"5.2\"></div>\n### <font color=\"#ff3fb5\">5.2 Using the Analogy to Design a Validation Experiment</font>       \n\nLet's generate a scenario on the similar grounds using this analogy for the City of Los Angeles jobs and the applicants.   \n\n1. Consider a job market populated by z job applicants.    \n2. Each job applicant **i** in this market applies to a job either 0 or 1 in City of LA's job roles (assume they are qualified).   \n3. Each job applicant has a parameter **Bi** that measures how much is their \"willingness to apply\" if they realize their is some level of bias in the bulletin.  \n4. Let P be the measure of unconscious bias present in job bulletins of City of LA's jobs.   \n5. Applicant utility for applying to a particular job is defined simplistically as: **Ui = Bi - P**    \n6. Applicant i will apply to the job if and only if Ui > 0 i.e. if Bi > P     \n\nIn simple terms, if applicant's \"willingness to apply\" parameter has higher value than the unconscious bias in the bulletin then applicant will apply else they will not. \n\n### Generating the Applicant's Data \n\nLet's create a simulation, in which we simulate applicant's (id, gender, willingness to apply), and job roles with simulated unconscious bias score. Let's observe the applicant's response. "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def simulate_applicants_data():\n    applicant_rows = []\n    for i in range(1, 1000):\n        gender = random.choice(genders)\n        if gender == 'Male':\n            wta = random.randint(4, 10)\n        else:\n            wta = random.randint(2, 7)\n        applicant_row = {\"id\" : \"A\"+str(i), \"gender\" : gender, \"willingness_to_apply\" : wta}\n        applicant_rows.append(applicant_row)\n    return pd.DataFrame(applicant_rows)\n\napplicants_df = simulate_applicants_data()\napplicants_df[['id', 'gender', 'willingness_to_apply']].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div id=\"5.3\"></div>\n### <font color=\"#ff3fb5\">5.3 Creating Scenarios and Analysis</font>      \n\nNow, let's simulate two scenarios. First in which there are job bulletins which consists of high unconscious bias.  Second same scenario but it contains 40% less unconcsiouc bias as compared to first. For both the scenarios, there are same set of applicants have a particular gender and a fixed willingness_to_apply score. \n\n### Scenario 1 : Job Bulletins consists of high unconscious bias   \n### Scenario 2 : Job Bulletins consists of 40% less unconscious bias "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"bulletins_df = gender_df[['title', 'unconscious_bias']]\nbulletins_df = bulletins_df[bulletins_df['title'] != \"\"]\nbulletins_df = bulletins_df.sample(frac=1)\nbulletins_df = gender_df[['title', 'unconscious_bias']]\nselected_titles = bulletins_df['title'].head(10).values\n\ns1 = []\nfor title in selected_titles:\n    if title.strip() != \"\":\n        bias = bulletins_df[bulletins_df['title'] == title]['unconscious_bias'].iloc(0)[0]\n        doc = dict(applicants_df[applicants_df['willingness_to_apply'] > bias]['gender'].value_counts())\n        doc[\"title\"] = title\n        s1.append(doc)\n    \ns2 = []\nbulletins_df = gender_df[['title', 'unconscious_bias']]\nfor title in selected_titles:\n    if title.strip() != \"\":\n        bias = bulletins_df[bulletins_df['title'] == title]['unconscious_bias'].iloc(0)[0]\n        bias = bias - 0.40*bias\n        doc = dict(applicants_df[applicants_df['willingness_to_apply'] > bias]['gender'].value_counts())\n        doc[\"title\"] = title\n        s2.append(doc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <font color=\"#ff3fb5\">Analysis - Which Scenario garnered more applicants, and more diversity</font>       \n\n<div id=\"5.4\"></div>\n### <font color=\"#ff3fb5\">5.4 Applicant's response to Unconscious Bias Bulletins</font>      "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"x1 = [_['title'] for _ in s1]\ny1 = [_['Male']+_['Female'] for _ in s1]\n\nx2 = [_['title'] for _ in s2]\ny2 = [_['Male']+_['Female'] for _ in s2]\n\ntrace1 = go.Bar(x=y1, y=x1, width=0.3, \n    marker=dict(color='#ef708c', line=dict(\n            color='#ef708c',\n            width=1.5,\n        )), orientation='h', name='Scenario 1: More Unconscious Bias')\n\ntrace2 = go.Bar(x=y2, y=x1, width=0.3, \n    marker=dict(color='#42f4b3', line=dict(\n            color='#42f4b3',\n            width=1.5,\n        )),\n    orientation='h', name='Scenario 2: Less Unconscious Bias')\n\nlayout = dict(\n    showlegend = True,\n    title='Simulating Number of Applicants who will apply',\n    yaxis=dict(\n        showgrid=False,\n        showline=False,\n        showticklabels=True,\n    ),\n    xaxis=dict(\n        title = \"Number of Applicants\",\n        zeroline=False,\n        showline=False,\n        showticklabels=True,\n        showgrid=False,\n        range = (0,1000)\n    ),\n    barmode='group',\n    margin=dict( l=250, r=20, t=50, b=50),\n    height=600, width=900,\n)\n\nfig = go.Figure(data = [trace2, trace1], layout = layout)\niplot(fig)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> - In general, we observed that scenario 2 (bulletins with less unconscious bias) garnered more number of applicants for almost every role. \n\n<div id=\"5.5\"></div>\n### <font color=\"#ff3fb5\">5.5 Effect on Male Applicants</font>      \n\nLet's look at if reducing the bias from bulletins attracted more number of male applicants. "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"x1 = [_['title'] for _ in s1]\ny1 = [_['Male'] for _ in s1]\n\nx2 = [_['title'] for _ in s2]\ny2 = [_['Male'] for _ in s2]\n\ntrace1 = go.Bar(x=y1, y=x1, width=0.3, \n    marker=dict(color='#ef708c', line=dict(\n            color='#ef708c',\n            width=1.5,\n        )), orientation='h', name='Scenario 1: More Unconscious Bias')\n\ntrace2 = go.Bar(x=y2, y=x1, width=0.3, \n    marker=dict(color='#42f4b3', line=dict(\n            color='#42f4b3',\n            width=1.5,\n        )),\n    orientation='h', name='Scenario 2: Less Unconscious Bias')\n\nlayout = dict(\n    showlegend = True,\n    title='Effect on Male Applicants',\n    yaxis=dict(\n        showgrid=False,\n        showline=False,\n        showticklabels=True,\n    ),\n    xaxis=dict(\n        title = \"Number of Applicants\",\n        zeroline=False,\n        showline=False,\n        showticklabels=True,\n        showgrid=False,\n        range = (0,1000)\n    ),\n    barmode='group',\n    margin=dict( l=250, r=20, t=50, b=50),\n    height=600, width=900,\n)\n\nfig = go.Figure(data = [trace2, trace1], layout = layout)\niplot(fig)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> - We observe that, there are no significant difference in two scenarios. We observed that most of the job bulletins were masculine and tend to garner more males than women. It will be interesting to check the effect on female applicants. \n\n<div id=\"5.6\"></div>\n### <font color=\"#ff3fb5\">5.6 Does reducing the bias attracted more Female Applicants</font>      \n\nLet's look at the effect on female applicants. "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"x1 = [_['title'] for _ in s1]\ny1 = [_['Female'] for _ in s1]\n\nx2 = [_['title'] for _ in s2]\ny2 = [_['Female'] for _ in s2]\n\ntrace1 = go.Bar(x=y1, y=x1, width=0.3, \n    marker=dict(color='#ef708c', line=dict(\n            color='#ef708c',\n            width=1.5,\n        )), orientation='h', name='Scenario 1: More Unconscious Bias')\n\ntrace2 = go.Bar(x=y2, y=x1, width=0.3, \n    marker=dict(color='#42f4b3', line=dict(\n            color='#42f4b3',\n            width=1.5,\n        )),\n    orientation='h', name='Scenario 2: Less Unconscious Bias')\n\nlayout = dict(\n    showlegend = True,\n    title='Effect (Increase) in Female Applicants',\n    yaxis=dict(\n        showgrid=False,\n        showline=False,\n        showticklabels=True,\n    ),\n    xaxis=dict(\n        title = \"Number of Applicants\",\n        zeroline=False,\n        showline=False,\n        showticklabels=True,\n        showgrid=False,\n        range = (0,1000)\n    ),\n    barmode='group',\n    margin=dict( l=250, r=20, t=50, b=50),\n    height=600, width=900,\n)\n\nfig = go.Figure(data = [trace2, trace1], layout = layout)\niplot(fig)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> - Clearly, number of female applicants who can potentially apply to job bulletins increased in the second scenario in which unconscious bias was reduced by 40 %.\n\n### Experiment Summary  \n\n- Every applicant has a parameter called willingness to apply (similar to willingness to pay in economics), We used this parameter in a simulated job market with different roles.  \n- Two scenarios were created: One with normal job bulletins, other with 40% less unconcscious bias.  \n- This experiment suggest that if there is less unconscious bias in job bulletins, then number of applicant's increases to particular job roles. \n- More female applicants are likely to apply to job roles with less unconscious bias than males, thus there is a direct impact on applicant's diversity  \n\n<div id=\"6\"></div>\n## <font color=\"#ff3fb5\">6. Recommendation Summary </font>   \n\n> - **Recommendation 1:** Gendered-Neutral Text  \nThe analysis in this kernel suggest that about 70% of of the job bulletins have \"moderately masculine\" nature. We also observed that it is mainly because of high use of keywords which are associated with masculine stereotypes. On an average, masculine keywords are used 3 times as compared to feminine keywords. Thus, The first recommendation is related to word choice. City should encorage use of more gendered neutral language. Masculine keywords such as lead, independent, skill, determined, considered, competant, power, force, and support can be avoided. Instead, keywords with similar meanings can be used. City should encourage more use of feminine keywords for example - understanding, life, community, responsible, support etc. Though these keywords may just sound very normal english keywords but research shows that particular genders tend to associate these keywords to a particular category. Hence, it is the duty of job bulletin writers to carefully select the words to be used. In this kernel, I have shared a list of keywords belonging to both the categories, the bulletin writers can refer to those lists and consider using alternate similar meaning keywords. This will have a higher impact on encouraging diversity.  \n\n> - **Recommendation 2:** Focus on Bulletins independently    \nStart removing unconscious bias in a step by step manner. The analysis suggest that some jobs have very high levels of unconscious bias, for example - Database Architect, City Planner, and many IT or technical related jobs. City should target these classes independently and start replacing the keywords which show gender bias. On a normalized scale, different classes had different unconscious bias score ranging from as low as 1 to as high as 60. On an average, 683 job bulletins had the average bias score of 16.3 which is still on a higher side. By targeting the classes which are extereme outliers such as the ones related to technology, finance, benefits, urban industry etc. \n\n> - **Recommendation 3:** Minimising the use of Adjectives / Superlatives    \nAnalysis also suggest that many bulletins have high usage of superlatives. Most common ones were master, expert, professional, specialist. Excessive use of superlatives can turn off female candidates who are more collaborative than competitive in nature. Research also shows that women are less likely than men to brag about their accomplishments. In addition, superlatives related to a candidate’s background can limit the pool of female applicants. Hence it is recommended to minimize use of such keywords. Section 3.2 shows the bulletins which uses master and expert many times. Consider replacing such keywords with gender-neutral adjectives, such as excellent or exceptional. Not only such keywords are gender-neutral but also they are associated with a positive sentiment.   \n\n> - **Recommendation 4:** Replacing keywords containing \"man\"    \nA quote that I obtained from Ongig website - Gender-biased titles/language start at a very early age. BYU English professor Delys Snyder says: “When children hear a job title that has a…m-a-n ending, and you ask them to draw pictures or talk about who’s doing that job, they will pick the one that matches the gender of the word.” Hence, it is good to avoid the keywords that contain \"man\" because it may give an indirect associatation with the male gender. The analysis done in section 3.4 suggest that there are atleast 5 job classes which still uses keywords containing man. Either these keywords can be replaced or not used at all.   \n\n> - **Recommendation 5:** Do not assume that there are only two genders   \nOne good point about City of LA's job bulletin was that there is no use of gendered pronouns such as he, him, himself, his, her, herself, and she. However, atleast two job bulletins still uses the keyword \"he/she\". It is important to understand that many people do not want to specify their gender or consider that they do not belong to either of the two genders. Instead of using \"he/she\", try to use \"the applicant\" or \"they\"     \n\n> - **Recommendation 6:** Reduce the overall length of requirements portion    \n> Asking for too much (with unnecessary requirements) can chase away female applicants. One study reported that men apply for positions if they meet 60% of the qualifications, whereas women only apply if they meet 100% of them. Consider only including the necessary qualifications to do the job to encourage applications from both men and women. Ofcourse, in the next stages of the hiring, one can dig little deeper. Make the requirement section clear and crisp, ensure that it is bulleted, and if possible try to minimize the requirement. Not more than four lines should be used to describe the requirement text. It is also good to specify good to have and must have separately.   \n\n### Next Kernels: \nFor next parts of my submission (analysis and recommendations), please visit next kernels of my Submission: \n<ul>\n    <li><a href=\"https://www.kaggle.com/shivamb/1-description-structuring-engine-cola\" target=\"_blank\">Part 1: Job Bulletin Structuring Engine - City of Los Angeles </a>  </li>\n    <li><a href=\"https://www.kaggle.com/shivamb/2-encourage-diversity-reduce-bias-cola\" target=\"_blank\">Part 2: Encourage Diversity and Remove Unconsious Bias from Job Bulletins - A Deep Analysis</a>  </li>\n    <li><a href=\"https://www.kaggle.com/shivamb/3-impact-of-ctl-content-tone-language-cola\" target=\"_blank\">Part 3: Impact of Content, Tone, and Language : CTL Analysis for CoLA</a>  </li> \n    <li><a href=\"https://www.kaggle.com/shivamb/4-promotional-pathway-discoverability-cola\" target=\"_blank\">Part 4: Increasing the Discoverability of Promotional Pathways (Explicit) </a></li>\n    <li><a href=\"https://www.kaggle.com/shivamb/5-implicit-promotional-pathways-discoverability/\" target=\"_blank\">Part 5: Implicit Promotional Pathways Discoverability</a></li></ul>\n\nNext <a href=\"https://www.kaggle.com/shivamb/3-impact-of-ctl-content-tone-language-cola\" target=\"_blank\">Kernel</a> - Analysis of content, tone, and language used in job bulletins."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}