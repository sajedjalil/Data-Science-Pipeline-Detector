{"cells":[{"metadata":{},"cell_type":"markdown","source":"<div align='center'><font size=\"6\" color=\"#ff3fb5\">Data Science For Good : CoLA</font></div>\n<div align='center'><font size=\"4\" color=\"#ff3fb5\">A Complete Pipeline for Structuring, Analysis and Recommendation</font></div>\n<div align='center'><font size=\"3\" color=\"#ff3fb5\">Improve Hiring Process and Decisions</font></div>\n<hr>\n\n<br><p style='text-align:justify'>The workforce at The City of Los Angeles serve as the backbone for a number of services.  As an organization, they are in a unique hiring situation. One-third of their workers are retiring very soon and they have opened a number of job roles. The organization wants to improve the job bulletins that will fill all the open positions. Most of this data is present in unstructured form and it needs to be converted into a structured format before it can be analysed and obtain actionable insights. The content, tone, and format of job bulletins can strongly influence the quality of the applicants. Also, within such a huge organization, it also becomes difficult to clearly identify which promotions are available.</p>\n\n<p style='text-align:justify'><b>Key Objectives:</b> Keeping these challenges in mind, an ideal solution for the City of Los Angeles has following key objectives: Develop an nlp framework to accurately structurize the job descriptions. Develop an analysis framework to identify the implict bias in the text and encourage diversity. Develop a system which can clearly identify the promotion pathways within the organization.</p>\n\n<b>My Submission:</b> Following are parts of Kernels Submissions in order:<br><br>\n\n<ul>\n    <li><a href=\"https://www.kaggle.com/shivamb/1-description-structuring-engine-cola\" target=\"_blank\">Part 1: Job Bulletin Structuring Engine - City of Los Angeles </a>  </li>\n    <li><a href=\"https://www.kaggle.com/shivamb/2-encourage-diversity-reduce-bias-cola\" target=\"_blank\">Part 2: Encourage Diversity and Remove Unconsious Bias from Job Bulletins - A Deep Analysis</a>  </li>\n    <li><a href=\"https://www.kaggle.com/shivamb/3-impact-of-ctl-content-tone-language-cola\" target=\"_blank\">Part 3: Impact of Content, Tone, and Language : CTL Analysis for CoLA</a>  </li> \n    <li><a href=\"https://www.kaggle.com/shivamb/4-promotional-pathway-discoverability-cola\" target=\"_blank\">Part 4: Increasing the Discoverability of Promotional Pathways (Explicit) </a></li>\n    <li><a href=\"https://www.kaggle.com/shivamb/5-implicit-promotional-pathways-discoverability/\" target=\"_blank\">Part 5: Implicit Promotional Pathways Discoverability</a></li></ul>\n\n<br>\n<p style='text-align:justify'>In the first kernel, a complete job bulletin entity extraction and data structuring engine is developed. In the second kernel, a deep analysis of unconscious bias is performed and a framework is shared to validate and reproduce the results. In the third kernel, impact of content, tone, and language is measured. And finally, in the last two kernels, a methodology is shared to visualize the promotion pathways (both explict and implict) and identifying the possible promotions pathways for a role.</p> \n     \n<hr>\n<div align='center'><font size=\"5\" color=\"#ff3fb5\">Part 1: Job Bulletins Structuring Engine </font></div>\n<div align='center'>Other Parts: <a href='https://www.kaggle.com/shivamb/1-description-structuring-engine-cola'>Part 1</a> | <a href='https://www.kaggle.com/shivamb/2-encourage-diversity-reduce-bias-cola'>Part 2</a> | <a href='https://www.kaggle.com/shivamb/3-impact-of-ctl-content-tone-language-cola'>Part 3</a> | <a href='https://www.kaggle.com/shivamb/4-promotional-pathway-discoverability-cola'>Part 4</a> | <a href='https://www.kaggle.com/shivamb/5-implicit-promotional-pathways-discoverability'>Part 5</a> </div>\n\n### <font color=\"#ff3fb5\">Table of Contents</font>   \n<ul><li><a href=\"#1\">1. Approach Overview</a></li>\n<li><a href=\"#2\">2. Complete Code Implementation</a>\n<ul>\n    <li><a href=\"#2.1\">2.1 BulletinParser Class</a></li>\n    <li><a href=\"#2.2\">2.2 Formatter Class</a></li>\n    <li><a href=\"#2.3\">2.3 Extractor Class</a></li>\n    </ul>\n</li>\n<li><a href=\"#3\">3. Final Structured CSV</a></li>\n<li><a href=\"#4\">4. Code Documentation</a></li>\n<li><a href=\"#5\">5. Reusable Python Package</a></li>\n<li><a href=\"#6\">6. Key Highlights of the Approach</a></li>\n</ul> \n\n<div id=\"1\"></div>\n## <font color=\"#ff3fb5\">1. Approach Overview</font> \n\n<p style='text-align:justify'>In this challenge, a corpus text files is shared in which every file is a job bulletin of a job role. The task is to parse the text files, extract different entities, pieces of information and produce a well-defined structured file. This task is a typical example of a text parsing problem in which the end goal is to simplify the text data into meaningful entities.</p> \n\n<p style='text-align:justify'>I have developed a python based engine called Job Bulletin Structuring Engine which takes input as a corpus of text files and generates a structured file as the output. The overall architecture diagram is described below: </p>  \n<div align='center'><img src='https://i.imgur.com/oZ9csXm.png' width=\"550\"></div>\n<br>\nThis architcuture has three main classes - A. Bulletin Parsing Class, B. Formatter Class, and the C. Extractor Class. \n\nThe first class is the Bulletin Parsing Class is the core class which is used to extract different entities and different fields, It uses different techniques of text mining and information reterival to obtain the entities. \n\n- Layout based Pattern Identification   \n- Keyword based Patterns Identification      \n- Regular Expressions Matching    \n\nUsing these techniques, I have developed systematic and hierarichial parsing rules tailored for specific entity and general entities. Entities here mean different fields/information to be extracted from text files. The process of developing these nlp / text mining based rules was very iterative. I randomly read and explored more than 50 text files and observed the key patterns in them. Develop the parsing rule, test it, validate it, identify failure cases and iterated again. \n\nThe second class is the Formatter class in which results of different entities are passed as input, they are standardized, cleaned, formatted to desired format, and validated. The results are then stored in a csv file which is given as the output. Finally, The third class is the Extractor class which is used to execute the overall parsing and cleaning processes based on user inputs.   \n\n**Key features:**\n- Reusable and Generic Functions: The engine consists of code snippets which are generic codes for many fileds.   \n- Cleaning Functions : In many cases, extracted entities contain noise, hence many custom functions to clean the results are added.    \n- Code and Documentation : The comprehensive documentation of both the codes and the text parsing rules is also shared.   \n- Edge Case Handelling: Since this is a very unstructured problem, there were many edge cases in text parsing, I have incorporated as many as such cases possible to give best possible results.   \n\n\n<div id=\"2\"></div>\n## <font color=\"#ff3fb5\">2. Complete Code Implementation</font> (<a href='http://www.shivambansal.com/blog/network/cola/BulletinStructuringEngine.html'>DOCUMENTATION</a>)\n\nThe next section contains the entire well-documentated source code which follows pep8 guidelines and uses best coding practices. In the section <a href=\"#5\">5</a>, I have also shared a python package which can be used to produce the results with 2 lines of code.\n\nPackage link: https://pypi.org/project/pycola/\n\n<div id=\"2.1\"></div>\n### <font color=\"#ff3fb5\">2.1 BulletinParser Class</font>\n\nThe first component is BulletinParser Class whose role is to obtain entities and information using different text mining rules. "},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"\"\"\"\nBulletinParser: Framework to standardize the job descriptions present in text format \nto structured object. Uses reusable functions and no external library \n\nChallenge: Data Science for Good - City of Los Angeles\nKaggle: https://www.kaggle.com/c/data-science-for-good-city-of-los-angeles\nKernel: https://www.kaggle.com/shivamb/1-bulletin-structuring-engine-cola/ \n\n__author__  == 'shivam bansal'\n__email__   == 'shivam5992@gmail.com'\n__version__ == \"1.62\"\n\"\"\"\n\nimport re, string, os\n\nclass BulletinParser:\n\n    \"\"\"\n    python class to standardize the job descriptions present in text format \n    to structured object, the final results are produced in a csv file format\n    \n    inputs  : the input can be a single file, or a complete folder\n    outputs : single json object for one file, structured csv file for a folder\n    \"\"\"\n    \n    def __init__(self, config):\n        \n        \"\"\" config : root path of the file and name of the file \"\"\"\n        self.filename, self.path = config['filename'], config['path']      \n        with open(self.path + self.filename, errors='ignore') as file_object:\n            self.lines = file_object.readlines()\n            self.text = \"\\n\".join(self.lines)\n            \n        \"\"\" a global list of all pointers used in bulletins \"\"\"\n        self.chrs = \"abcdefghij\"\n        self.nums = \"0123456789\"\n        self.sybl = \"-*\"\n        \n        \"\"\" lookup dictionary for numbers text and values \"\"\"\n        self._ns = {\"one\":\"1\", \"two\":\"2\", \"three\":\"3\", \"four\":\"4\", \"five\":\"5\", \"six\":\"6\", \n        \"seven\":\"7\", \"eight\":\"8\", \"nine\":\"9\", \"ten\":\"10\", \"twelve\":\"12\", \"fourteen\":\"14\",\n        \"fifteen\":\"15\", 'sixteen':'16', \"eighteen\":\"18\", \"twenty\":\"20\",\"thirty-two\" : \"32\",\n        \"twenty-three\":\"23\",\"twenty-four\":\"24\",\"twenty-seven\":\"27\",\"thirty\":\"30\",\n        \"thirty-six\":\"36\", \"fourty-five\":\"45\", \"sixty\":\"60\", \"ninety\":\"90\", \"135\":\"135\"}\n\n        \"\"\" lookup dictionaries that contain key-value pair (keywords) for identification \"\"\"\n        self.lookups  = {\n        \"full_part\" : {\"full time\":[\"full time\",\"full-time\"],\n                       \"part time\":[\"part time\",\"part-time\"]},\n        \"education\" : [\"college or university\",\"education\",\"school\", \"semester\",\"quarter\", \n                       \"degree\",\"coursework\"],\n        \"experience\": [\"full time\", \"full-time\", \"experience\", \"part time\", \"part-time\"],\n        \"semester\"  : [\"semester\", \"quarter\", 'course'],\n        \"exam_type\" : [\"OPEN\", \"INT_DEPT_PROM\", \"DEPT_PROM\", \"OPEN_INT_PROM\"],\n        \"major\"     : ['degree in ', 'major in ', 'majoring'],\n        \"exp_flags\" : [\" as a \", \" at the level of \", \" as an \"],\n        \"school\" : {\"college or university\":[\"college or university\",\"university or college\"], \n        \"college\" : [\"college\"],\"high school\":[\"high school\"],\"trade school\":['trade school'], \n        \"apprenticeship\": [\"apprenticeship\"],\"law school\" : ['law school'], \n        \"technical school\":['technical school']}}\n        \n        \"\"\" common splitters used in text parsing and cleaning in different functions \"\"\"\n        self.split1, self.split2 = [\"\\n\", \"and\", \";\"], ['\\n', ';', '. ']\n        self.split3 = [\"; \",\" with \",\" and \",\" or \",\". \",\" in \",\" issued \",\"attached\",\"whose\"]\n        self.split4 = ['from an', 'from a', ' may ', ' for ', '; and']\n        self.split5 = [\"is required\",\"approved\",\"required\",\". \",\"may\",\"with\",\"prior\"]        \n        self.split5 += [\"upon\", \";\"]\n        \n        \"\"\" keywords required to clean / correct some entities \"\"\"\n        self.spellings = { \"COMPETITVE\" : \"COMPETITIVE\", \"PARMENTAL\" : \"PARTMENTAL\"}\n        spells = {\"CAMPUS INTERVIEWS ONLY\" : \"\", \"MINIMUM REQUIREMENTS\" : \"REQUIREMENTS\",\n                  \"REQUIREMENTS\\n\\n\\n\\nFive\" : \"REQUIREMENTS\\n0.Five years\"}\n        \n        \"\"\" file complete text and non empty lines \"\"\"\n        for k,v in spells.items():\n            self.text = self.text.replace(k, v)\n        self.lines = [l for l in self.text.split(\"\\n\") if l.strip()]\n        \n        \n        \n    \"\"\" utility function to find a portion of text using an identifier keyword \"\"\"\n    def portion(self, flag, txt=None, limit=10, next_word=True, indx=False):\n        \"\"\"\n        a generic python function to obtain the entites using an identifier flag \n        this function is used to extract class code, requirements, duties, date.\n\n        :params:\n        input : flag keyword (example - requirement, notes, duties), limit (optional) specify \n        number of characters to be extracted around the flag, next_word : True if only one \n        (immediate) word is output, indx : flag which identifies index of the relevant lines\n        output: the relevant portion related to the input flag keyword\n        \"\"\"\n        \n        \"\"\" create the list of lines variable \"\"\"\n        entity = \"\"\n        lines = self.lines\n        if txt != None:            \n            lines = txt.split(\"\\n\")\n        \n        \"\"\" filter the lines which contain the identifier flag \"\"\"\n        lines = [\" \".join(line.split()).lower() for line in lines]\n        lines = [line for line in lines if flag in line]\n        \n        \"\"\" find relevant index if indx is set True \"\"\"\n        if indx:\n            lines=[i for i,l in enumerate(self.lines) \n                   if l.strip().lower().startswith(flag.lower())]\n        if len(lines) == 0:\n            return entity \n        \n        \"\"\" find the required portion of text \"\"\"\n        start_ix = lines[0]    \n        if indx:\n            end = [i for i,l in enumerate(self.lines[start_ix+1:]) if l.isupper()][0]\n            entity = \"\\n\".join(self.lines[start_ix+1:start_ix+end+1])\n        else:\n            \"\"\" obtain the entity text till next words (limit parameter) \"\"\"\n            index = start_ix.find(flag)        \n            entity = start_ix[index:].replace(flag, \"\")\n            entity = entity.strip()[:limit]\n            if next_word:\n                entity = entity.split()[0]    \n            else:\n                for split in self.split2:\n                    entity = entity.split(split)[0]  \n        return entity\n    \n    \"\"\" similar function as portion, but it executes with multiple identifiers \"\"\"\n    def portions(self, flag, txt, limit=10, next_word=True):\n        \"\"\"\n        this function accept same parameters as portion, flags is a list.\n        used to extract major / degree related entities from the bulletins\n        \"\"\"\n        \n        entities = []\n        for flag in self.lookups[flag]:\n            entity = self.portion(flag, txt, limit, next_word)\n            if entity:\n                entities.append(entity)\n        return \", \".join(entities)\n\n    \"\"\" utility function to check the presence of an identifier and related keywords \"\"\"\n    def presence_check(self, identifier, txt):\n        \"\"\"\n        checks if a certain keyword or a list of keywords are present in the text, the function \n        performs exact and compact match, can be extend to perform more flexible text matching \n \n        :params:\n        input  : identifier and the relevant portion of the text to be searched\n        output : a string of identifiers which were found in the text \n        \"\"\"\n        \n        \"\"\" obtain the list/dict of relevant tokens from the lookup dictionary \"\"\"\n        lookup = self.lookups[identifier] \n        \n        \"\"\" iterate in lookup tokens and check if present \"\"\"\n        entities = []\n        if type(lookup) == dict:    \n            \"\"\" iterate key-value pairs, check if any value is present in the text \"\"\"\n            for key, values in lookup.items():    \n                for val in values:\n                    if val.lower() in txt.lower():\n                        entities.append(key)\n        else: \n            \"\"\" iterate in list, check if any element is present in the text \"\"\"\n            for val in lookup:\n                if val.lower() in txt.lower():\n                    entities.append(val)\n        \n        \"\"\" remove duplicates and join them by pipe \"\"\"\n        entities = \"|\".join(list(set(entities)))\n        return entities\n    \n    \"\"\" utility function to standardize the numbers text into a numerical value \"\"\"\n    def standardize_numbers(self, entity):\n        \"\"\"\n        it uses _ns lookup dictionary which defines the standardized form of a number\n\n        :params:\n        the input is uncleaned text which is probably about a number \n        the output is a numerical value\n        \"\"\"\n        \n        number = \"\"\n        if entity.lower() in self._ns:\n            number = self._ns[entity.lower()]\n        elif entity in list(self._ns.values()):\n            number = entity\n        return number\n\n    \"\"\" utility function to get the years/months associated with an identifer \"\"\"\n    def year_month(self, flag, req_text, span = 'year'):\n        \"\"\"\n        :params:\n        flag: key which is used to obtain the relevant values from the lookup dictionary \n        span: number context to be extracted, (year, month, semester, quarter)\n        output: years/month associated in the text; months are converted into floats.\n        \"\"\"\n        \n        \"\"\" obtain the list of related keywords from the lookup dictionary \"\"\"\n        lookup = self.lookups[flag]\n        \n        \"\"\" iterate by lines, check if the span-value is present \"\"\"\n        collected = []\n        for line in req_text.split(\"\\n\"):\n            line = line.replace(\"one-year\", \"one year\")\n            \"\"\" find index of all occurances of the span in the line \"\"\"\n            indexes = [m.start() for m in re.finditer(span, line)]            \n            \n            \"\"\" slice a portion around the index within lower and upper bound \"\"\"\n            lower_bound, upper_bound = 30, 40\n            for index in indexes:\n                if index-lower_bound < 0:\n                    portion = line[:index+upper_bound].lower()\n                else:\n                    portion = line[index-lower_bound:index+upper_bound].lower()\n                \n                \"\"\" next, identify if the portion of text is relevant of not, \"\"\"\n                \"\"\" four main conditions to identify not not important portion \"\"\"\n                is_relevant = False\n                for keyword in lookup:                    \n                    \"\"\" cond1 : keyword is not present in relevant portion \"\"\"\n                    if keyword not in portion:\n                        continue\n                    \n                    \"\"\" cond2 : span is mentioned after the lookup keyword \"\"\"\n                    yr_ix = portion.find(span)\n                    idf_ix = portion.find(keyword)\n                    if yr_ix > idf_ix:\n                        continue\n                    \n                    \"\"\" cond3 : presence of ignore words in the text portion \"\"\"\n                    ignore_words = [\"=\", \"equal\", \"equivalent\", 'lack', 'valent', 'ent to ']\n                    if any(eq in portion for eq in ignore_words):\n                        continue\n                    \n                    \"\"\" cond4 : presence of substitute in experience text \"\"\"\n                    if keyword == \"experience\":\n                        if \"titute\" in portion:\n                            continue\n                    \n                    \"\"\" for other cases, the portion is relevant \"\"\"\n                    is_relevant = True\n                \n                \"\"\" if relevant, then identify the numerical span value \"\"\"\n                if is_relevant:\n                    special_checks = [\"two or four\", \"two-year or four-year\"]\n                    if any(two_four in portion for two_four in special_checks):\n                        collected.append(\"4\")\n                    if \"two and one-half\" in portion:\n                        collected.append(\"2.5\")\n                    else:\n                        obtained = False\n                        \n                        \"\"\" check entities with two words: (ex - twenty-four etc.)\"\"\"\n                        for k,v in self._ns.items():\n                            if \"-\" in k:\n                                if k in portion.split(span)[0].replace(\" \", \"-\"):\n                                    collected.append(v)\n                                    obtained = True\n                        \n                        \"\"\" for other cases, obtain the immediate previous word \"\"\"\n                        if obtained == False:\n                            val = portion[:yr_ix].split()[-1]\n                            val = val.replace(\"-\",\"\").replace(\"(\",\"\").replace(\")\",\"\")\n                            val = self.standardize_numbers(val)\n                            if val != \"\":\n                                collected.append(val)\n\n        \"\"\" return the deduplicated list of month / year \"\"\"\n        collected = list(set(collected))\n        if span == 'month':\n            collected = [str(round(float(_)/12, 2)) for _ in collected]\n        if len(collected) > 0:\n            collected = [float(_) for _ in collected]\n        return collected\n\n    \"\"\" Custom function to identify exam type required in given job \"\"\"\n    def exam_type(self):\n        \"\"\"\n        identify and cleans the required text; tags the exam type according to rules\n        possible outputs for this function are open, open_int, int_dept, and dept \n        \"\"\"\n        \n        \"\"\" Meta data related to exam types \"\"\"\n        bad_entities = ['an', 'on', 'both', 'only', 'a', 'to', 'nvvc', 'basis']\n        bad_entities = [\" \"+_.upper()+\" \" for _ in bad_entities]\n        \n        \"\"\" identify and clean the main portion related to exam type \"\"\"\n        portion = \"\"\n        for i, line in enumerate(self.lines):\n            \"\"\" identify the relevant line \"\"\"\n            if \"THIS EXAM\" in line:\n                portion = line + \" \" + self.lines[i+1]\n                portion = \" \"+portion.split(\"GIVEN\")[1]\n                portion = portion.split(\"The City\")[0].strip()+\" \"\n                \n                \"\"\" cleanup bad entities and spelling mistakes \"\"\"\n                for ent in bad_entities:\n                    portion = portion.replace(ent, \" \")\n                for k,v in self.spellings.items():\n                    portion = portion.replace(k,v)\n                portion = \" \".join(portion.split()).split(\" AND \")              \n                break\n                \n        \"\"\" join and further standardize the exam type \"\"\"\n        cons = ['ONLY ', 'ON ', 'BOTH ']\n        exam_t = \" AND \".join(portion)\n        exam_t = \" \".join(w for w in exam_t.split() if w not in cons)\n        exam_t = exam_t.lower()\n        \n        \"\"\" generate the final tag of exam type \"\"\"\n        tag = \"OPEN\"\n        if \"open\" in exam_t and \"interdepartmental\" in exam_t:\n            tag = \"OPEN_INT_PROM\"\n        elif \"interdepartmental\" in exam_t:\n            tag = \"INT_DEPT_PROM\"\n        elif \"department\" in exam_t:\n            tag = \"DEPT_PROM\"\n        return tag\n    \n    \"\"\" Custom function to obtain course counts required for the job \"\"\"\n    def course_count(self, req_text, flag='course', limit=50):\n        \"\"\"\n        :params:\n        req_text: complete requirement text and output: number of courses required\n        output: number of courses required as the minimum requirement of the job\n        \"\"\"\n\n        \"\"\" find the numerical / textual span values in the text \"\"\"\n        spans = list(self._ns.keys()) + list(self._ns.values())\n        idx = [m.start() for m in re.finditer(flag, req_text.lower())]\n        \n        \"\"\" iterate and check if relevant \"\"\"\n        collect = []\n        for each in idx:\n            if each-limit < 0:\n                lines = req_text[:each+limit].split(\"\\n\")\n            else:\n                lines = req_text[each-limit:each+limit].split(\"\\n\")\n            \n            \"\"\" check which spans are mentioned in the text and store \"\"\"\n            for l in [l for l in lines if flag in l]: \n                for span in spans:\n                    if span+\" \"+flag in l:       \n                        if span in self._ns:\n                            span = self._ns[span]\n                        collect.append(int(span))\n\n        \"\"\" return the obtained value \"\"\"\n        if len(collect) == 0:\n            return \"\"\n        return max(collect)\n\n    \"\"\" Custom function to obtain salary amount and DWP salary amount \"\"\"\n    def salary_details(self):\n        \"\"\"\n        Identifies the salary amount mentioned in the text; also finds for DWP \n        for multiple salary amounts, only first salary is given as output\n        \"\"\"\n\n        \"\"\" first identify the relevant portion \"\"\"\n        identifier, next_chars = \"SALARY\", 250\n        ix = self.text.find(identifier)\n        portion_x = self.text[ix:ix+next_chars]\n        portion_x = portion_x.replace(identifier, \"\").strip()\n        \n        \"\"\" find the salary amount \"\"\"\n        salary = portion_x\n        for split in self.split1:\n            salary = salary.split(split)[0]\n        salary = \"$\" + \"$\".join(salary.split(\"$\")[1:])\n        salary = salary.split(\"(\")[0].split(\", $\")[0]\n        salary = salary.split(\"The\")[0].split(\". Some\")[0]\n        if salary.strip() == \"$\":\n            salary = \"\"\n        \n        \"\"\" find the DWP salary \"\"\"\n        dwp = \"\"\n        rep = [\"(\", \"flat\", \"$ \"]\n        identifier = \"Department of Water and Power is \"\n        for line in portion_x.split(\"\\n\"):\n            if identifier.lower() in line.lower():\n                dwp = line.lower().split(identifier.lower())[1]\n                \"\"\" basic cleanup \"\"\"\n                for split in self.split2 + [\". \"]:\n                    dwp = dwp.split(split)[0]\n                for r in rep:\n                    dwp = dwp.replace(r, \"\").rstrip(\".\")\n                dwp = dwp.replace(\"-rated)\",\"\").replace(\"-rated\",\"\")\n                dwp = dwp.rstrip(\".\").replace(\"rated)\",\"\").replace(\"at\",\"\")\n                dwp = dwp.split(\"and\")[0].strip()\n        return salary, dwp\n    \n    \"\"\" custom function to obtain the experience title \"\"\"\n    def experience_title(self, req_text):\n        \"\"\"\n        function to identify the experience title from the requirement text \n        input is only the requirement text of a job bulletin\n        \"\"\"\n\n        exp_title = []\n        possed = [] \n        \"\"\" iterate in experience flags \"\"\"\n        for identifier in self.lookups['exp_flags']:\n            for i,line in enumerate(req_text.split(\"\\n\")):\n                \n                \"\"\" clean and collect the relevant portions \"\"\"\n                if identifier in line:\n                    lx = line.split(identifier)[1]\n                    poss = lx\n                    possed.append(poss)\n                    for spliter in self.split3:\n                        if spliter in [\" or \", \" and \"]:\n                            \"\"\" special check for small lines \"\"\"\n                            if spliter in lx and len(lx) < 60:\n                                pass\n                            else:\n                                lx = lx.split(spliter)[0]\n                        else:\n                            lx = lx.split(spliter)[0]\n                    exp_title.append(lx)\n                    break\n        exp_title = \"; \".join(exp_title)\n        return exp_title\n    \n    \"\"\" Custom function to obtain class title from the job-description text \"\"\"\n    def class_title(self):\n        \"\"\"\n        extracts and cleans the job title from the bulletin text\n        \"\"\"\n        \n        entity = self.lines[0].strip().lower()\n        if \"revised\" in entity:\n            entity = entity.replace(\"revised\", \"\")\n        if \"class code\" in entity:\n            entity = entity.split(\"class code\")[0]\n        return entity.title()\n    \n    \"\"\" custom function to identify the course subjects \"\"\"\n    def course_subjects(self, req_text):\n        \"\"\"\n        this function identifes the relevant course subjects using a list of keywords \n        \"\"\"\n\n        kws = self.lookups[\"semester\"]\n\n        \"\"\" obtain the relevant lines \"\"\"\n        lines = req_text.split(\"\\n\")\n        relevant_lines = [l for l in lines if any(wd in l for wd in kws)]\n        relevant_lines = [l.lower() for l in relevant_lines]\n\n        \"\"\" iterate and locate the position of relevant text \"\"\"\n        courses = []\n        for each in relevant_lines:\n            course = \"\"\n            if all(k in each for k in kws):\n                ix = each.find(\"quarter\")\n            elif \"quarter\" in each:\n                ix = each.find(\"quarter\")\n            else:\n                ix = each.find(\"semester\")\n\n            \"\"\" using the obtained index, split the text \"\"\"\n            relt = each[ix:ix+300]\n            if \" units in \" in relt:\n                course = relt.split(\" units in \")[1]\n                course = course.split(\";\")[0].split(\" at \")[0]\n            elif \" courses in \" in relt:\n                course = relt.split(\" courses in \")[1]\n                course = course.split(\";\")[0].split(\" at \")[0]\n            elif \" units of \" in relt:\n                course = relt.split(\" units of \")[1]\n                course = course.split(\";\")[0].split(\" at \")[0]\n            elif \" units \" in relt:\n                if \" in \" in relt:\n                    right = relt.split(\" in \")[1]                    \n                    if right.startswith(\"requ\") or right.startswith(\"which\"):\n                        continue\n                    if right.startswith(\"the\"):                    \n                        right = right.replace(\"the production\", \"production\")\n                        right = right.replace(\"the areas of\", \"\")\n                    if right.startswith(\"the\"):\n                        continue\n                    course = right\n            else:\n                \"\"\" perform cleanup \"\"\"\n                for ent in ['university, in', 'or trade school']:\n                    relt = relt.replace(ent, \"\")\n\n                if \"university in \" in relt:\n                    course = relt.split(\"university in \")[1].split(\";\")[0]\n                    course = course.split(\" at \")[0]\n\n            \"\"\" more custom cleanup \"\"\"\n            for split in self.split4:\n                course = course.split(split)[0]\n            if course:\n                courses.append(course)\n        courses = list(set(courses))\n        courses = \" \".join(courses)\n        \n        \"\"\" further split by <num>_year \"\"\"\n        splitters = ['one', 'two', 'three', 'four', 'five', 'including']\n        for i,split in enumerate(splitters):\n            if i < 5:\n                split = split + \" year\"\n            courses = courses.split(split)[0]    \n        if \"coursework\" in courses:\n            courses = courses.split(\"coursework\")[0]\n        if courses.strip() == \"it\":\n            courses = \"IT\"\n        courses = courses.replace(\"the fields listed\", \"\")\n        courses = courses.replace(\"an accredited college or university.\",\"\")\n        courses = courses.split(\"or 12 \")[0].replace(\"the required coursework.\",\"\")\n        courses = courses.replace(\"any of the following areas: \",\"\")\n        return courses\n\n    \"\"\" function to check the driver license tag \"\"\"\n    def driver_license(self, limit = 50):\n        \"\"\"\n        this function finds if driving license is required or not\n        \n        :params:\n        limit: used to define the lower and upper bound set the text portion \n        \"\"\"\n        \n        \"\"\" find the index where driver keyword is mentioned \"\"\"\n        i = re.finditer(\"driver\", self.text.lower())\n        i = [m.start() for m in i]\n        \n        \"\"\" iterate and further check if 'license' is mentioned \"\"\"\n        tags = []\n        for each in i:\n            por = self.text[each-limit:each+limit*2].lower()\n            tag = \"\"\n            if \"license\" in por:\n                tag = \"R\"\n                if \" may \" in por:\n                    tag = \"P\"\n            tags.append(tag)\n        if len(tags) > 0:\n            return tags[0]\n        return \"\"\n\n    \"\"\"  function to fix breaks in requirments in which a line is broken \"\"\"\n    def fix_requirement_line_breaks(self, content):\n        \"\"\"\n        Sometimes the requirement text's flat lines are broken due to inaccuracy of \n        pdf-parsing. This function is used to fix such cases before parsing the text\n        \"\"\"\n\n        remove = []\n        points = [\"a.\",\"b.\",\"c.\",\"d.\",\"e.\", \"1.\", \"2.\", \"3.\", \"4.\", \"5.\"]\n        lines = content.split(\"\\n\")\n        \"\"\" first iterate and identify the lines having no pointer \"\"\"\n        for i,x in enumerate(lines):\n            x = x.strip()\n            if not any(x[:2] == p for p in points):\n                remove.append(i - 1)\n\n        \"\"\" identfiy the index of rows and join between consecutive lines \"\"\"\n        rremove = [i+1 for i in remove]\n        new_lines = []\n        covered = []\n        for j,x in enumerate(lines):\n            x = x.strip()\n            if j in covered:\n                continue\n            \"\"\" consecutive rows join operation \"\"\"\n            if j in remove:\n                if any(x[:2] == p for p in points):\n                    extra = lines[j+1].strip()\n                    if extra not in x:\n                        u_line = x + \" \" + extra\n                        new_lines.append (u_line)\n                        covered.append(j+1)\n            else:\n                new_lines.append(x)\n        return \"\\n\".join(new_lines)\n\n    \n    \"\"\" function to properly parse requirement set and subsets \"\"\"\n    def requirement_IDs(self, req_text):\n        \"\"\"\n        this function extracts out the pointer associated with ever requirment\n        1, 2, 3 .. are set_id, and set_text is the corresponding text \n        a, b, c .. are subset_id, and subset_text is the corresponding text\n        special symbols such as * or - are also tracked; the function formats the text \n        to generate required format; Also it identifies special cases with no pointers\n        \"\"\"\n        \n        \"\"\" special case 1: breakage in requirement lines pointers \"\"\"\n        obtained = []\n        exclude = ['POST-Certified Candidates', 'Under the policy']\n        for i, l1 in enumerate(req_text.split(\"\\n\")):\n            l1 = l1.strip()\n            if not l1:\n                continue\n            if any(l1.startswith(_) for _ in exclude):\n                break\n            if l1[0].lower() in \"12345678abcdefghi\":\n                obtained.append(i)\n        diff = [x - obtained[i - 1] for i, x in enumerate(obtained)][1:]\n        if 1 in diff:\n            diff.remove(1)\n        if any(x in diff for x in [2, 3, 4, 5, 6, 7, 8]):\n            for i in range(8):\n                content = self.fix_requirement_line_breaks(req_text)\n            lines = content.split(\"\\n\")\n        \n        \"\"\" step2: obtain the requirement text lines \"\"\"\n        lines_ = [l.strip().rstrip(\",\") for l in req_text.split(\"\\n\")]\n        \n        \"\"\" step3: custom break condition \"\"\"\n        lines = []\n        for l in lines_:\n            if \"substitute\" in l:\n                if l[0] not in [\"1\",\"2\",\"3\",\"4\",\"5\"]:\n                    break\n            elif l.startswith(exclude[0]):\n                break\n            lines.append(l)\n                    \n        \"\"\" sepcial case 2: no bullets  \"\"\"\n        flats = [l for l in lines if not any(l[0]==w and l[1]!=\",\" for w in self.nums)]\n        if len(lines) != 1 and len(lines) == len(flats):\n            lines = [\" \".join(lines)]\n\n        \"\"\" special case 3: single line requirement \"\"\"\n        ignore = False\n        if len(lines) == 1:\n            \"\"\" check the presense of numbers or alphabets \"\"\"\n            for x in self.nums+self.chrs:\n                starter = req_text[:2].strip().lower()\n                if any(patt == starter for patt in [x+\".\", x+\")\", \"(\"+x]):\n                    ignore = True\n                    \n            \"\"\" check the presense of symbols \"\"\"\n            for x in self.sybl:\n                starter = l[:1].strip().lower()\n                if x == starter:\n                    ignore = True\n                    \n            \"\"\" retun single line requirement, if of number / alphabet \"\"\"\n            if ignore == False:\n                return [\"1.\"], [\"\"], [req_text], [\"\"]\n            \n        \n        \"\"\" for all other cases, iterate and extarct pointer \"\"\"\n        set_id, subset_id, set_text, subset_text  = [], [], [], []\n        for i, l in enumerate(lines):\n            pointer = \"\"\n            \"\"\" check the presense of numbers or alphabets \"\"\"\n            for x in self.nums+self.chrs:\n                starter = l[:2].strip().lower()\n                if any(patt == starter for patt in [x+\".\", x+\")\", \"(\"+x]):\n                    pointer = x\n                    break\n                elif x == starter[0]:\n                    pointer = x \n                    break\n            \"\"\" check the presense of symbols \"\"\"\n            for x in self.sybl:\n                starter = l[:1].strip().lower()\n                if x == starter:\n                    pointer = x\n                    break\n            \n            \"\"\" if the pointer is obtained then slice it and the text \"\"\"\n            if pointer != \"\":             \n                if pointer.rstrip(\".\") in self.nums:\n                    sid, stext = pointer, l[2:]\n                    ssid, sstext = \"\", \"\"\n                else:\n                    pointer = \"\".join(c for c in pointer if c not in \".()\")\n                    if pointer in self.chrs:\n                        ssid, sstext = pointer, l[2:]\n\n                \"\"\" append ids and text for set and subset \"\"\"\n                set_id.append(sid)\n                subset_id.append(ssid)\n                set_text.append(stext)\n                subset_text.append(sstext)   \n        \n        \"\"\" following code formats the lists in the required format \"\"\"\n        count_dict, remove_ind = {}, []\n        for i in range(len(set_text)):\n            main = list(reversed(set_text))[i]\n            sub = list(reversed(subset_text))[i]\n            if main not in count_dict:\n                count_dict[main] = 0\n            count_dict[main] += 1\n        for i in range(len(set_text)):\n            main, sub = set_text[i], subset_text[i]\n            if count_dict[main] > 1 and sub == \"\":\n                remove_ind.append(i)\n                \n        \"\"\" cleanup and structure according to required format \"\"\"\n        uset_id, usubset_id, uset_text, usubset_text  = [], [], [], []\n        for i, val in enumerate(set_id):\n            if i not in remove_ind:\n                uset_id.append(set_id[i])\n                usubset_id.append(subset_id[i])\n                uset_text.append(set_text[i])\n                usubset_text.append(subset_text[i])   \n        \n        \"\"\" finally return the set_id, sub_ids along with their text \"\"\"\n        return uset_id, usubset_id, uset_text, usubset_text\n    \n    \"\"\" function to check the license type required \"\"\"\n    def license_type(self, flags = ['license']):\n        \"\"\"\n            this function serves two purposes : identify the type of license required, \n            and other licenses required for a particular job role. possible - A, B, C\n        \"\"\"\n        \n        \"\"\" iterate and find the portion where identifier is present \"\"\"\n        ltypes, DL_type = [], []\n        for flag in flags:\n            ix = re.finditer(flag, self.text.lower())\n            ix = [m.start() for m in ix]\n            \n            \"\"\" for search results check presense of driver keyword \"\"\"\n            is_dl = False\n            for i in ix:\n                lines = self.text[i-50:i+100].split(\"\\n\")\n                line = \" \".join([l for l in lines if flag in l.lower()])\n                \n                \"\"\" identify if driving license \"\"\"\n                if \"driver\" in line.lower():\n                    is_dl = True\n                if not line.strip():\n                    continue\n                    \n                \"\"\" identify the license class \"\"\"\n                if \"valid\" in line:\n                    iv = line.find(\"valid\")\n                    lx = line[iv:].replace(\"valid \",\"\").split(\"issued\")[0]\n                    for split in self.split5:\n                        lx = lx.split(split)[0]\n                    ltype = lx\n                else:\n                    words = line.split(flag)[0].split(flag.title())[0]\n                    words = words.split(flag.upper())[0].split()\n                    up_words = [w for w in words if w[0].isupper()]\n                    if len(up_words) == 0:\n                        continue\n                    \n                    \"\"\" basic cleaning of the relevant text \"\"\"\n                    types = []\n                    for x in reversed(words):\n                        if x.islower():\n                            break\n                        types.append(x)\n                    types = \" \".join(reversed(types))\n                    \n                    \"\"\" replace noisy entity \"\"\"\n                    for r in ['1. ', '2. ', '3. ']:\n                        types = types.replace(r, \"\").strip()\n                    if types in [\"A\", \"B\"]:\n                        types = \"\"\n                    ltype = types\n                \n                \"\"\" save the results - DL or other licenses \"\"\"\n                if is_dl == True:\n                    if ltype not in DL_type:\n                        DL_type.append(ltype)\n                else:\n                    if ltype not in ltypes:\n                        ltypes.append(ltype)\n        \n        \"\"\" deduplicate the obtained results \"\"\"\n        ltypes = list(set([x for x in ltypes if x.strip()]))\n        ltypes = [_.strip() for _ in ltypes]\n        \n        \"\"\" identify the class of the driving license \"\"\"\n        dl_type = []\n        classes = [\"A\", \"B\", \"C\"]\n        for l in DL_type:\n            l1 = \"\".join(_ for _ in l if _ not in string.punctuation)\n            for dl_class in classes:\n                if \" \"+dl_class+\" \" in l1:\n                    dl_type.append(dl_class)            \n            if l in ltypes:\n                ltypes.remove(l)\n        \n        \"\"\" deduplicate and combine multiple results \"\"\"\n        dl_type = sorted(list(set(dl_type)))\n        dl_type = \" OR \".join(dl_type)\n        return dl_type, ltypes\n\n    \"\"\" function to obtain the text related to alternate job class function \"\"\"\n    def job_class_alt_function(self, txt):\n        \"\"\"\n        function to obtain the text related to alternate job class function. \n        \"\"\"\n\n        _ = [\" or in a class\", \" or at the level\", \" or in a position\", \" or as a member\"]\n        lines = txt.split(\"\\n\")\n        for line in lines:\n            line = line.strip()\n            if \" or \" not in line:\n                continue\n            index = None\n            for connector in _:\n                if connector in line:\n                    index = connector\n                    break\n            if index != None:\n                return line.split(index)[1]\n        return \"\"\n    \n    \"\"\" function to extract the function of the job role \"\"\"\n    def get_function(self, l):\n        \"\"\"\n        function to obtain the text related to job class function. \n        \"\"\"\n        \n        functions = []\n\n        \"\"\" perform basic cleanup and proper splitting \"\"\"\n        l = l.split(\" or in a class \")[0].replace(\"with the City of Los Angeles\",\"\")\n        l = l.replace(\"; or\",\"\").strip().replace(\" .\",\"\")\n        \n        \"\"\" handle cases which contain experience keywords \"\"\"\n        if \"experience\" in l:\n            \n            \"\"\" handle cases which do not contain identfier of a job role \"\"\"\n            if not any(w in l for w in [\" as a \", \" at the level of \", \" as an \"]):\n                pr = l.split(\"experience\")[1].split(\" or in \")[0]\n                pr = pr.strip().lstrip(\".\").lstrip(\",\").strip().lstrip(\";\")\n                pr = pr.strip().lstrip(\",\").strip()\n                if pr:\n                    fw = pr.split()[0]\n                    if fw in ['in','at', 'with', 'within']:\n                        functions.append(\"experience \" + pr.strip())\n                    elif \"ing\" in fw:\n                        functions.append(pr)\n                    else:\n                        pr = pr.strip()\n                        if \"responsib\" in pr:\n                            functions.append(pr.replace(\"which include the \",\"\"))\n            else:\n                \"\"\" handle cases which contain experience and the identifier cases \"\"\"\n                if \" as a \" in l:\n                    l = l.split(\" as a \")[1]\n                    if len(l.split()) <= 8:\n                        pass\n                    elif \"experience\" in l:\n                        pr = l.split(\"experience\")[1].strip()\n                        pr = pr.lstrip(\"in the\").strip()\n                        pr = pr.lstrip(\"in \").strip()\n                        functions.append(\"experience in \" + pr)\n                    elif \"ing \" in l:\n                        got = \"\"\n                        for w in l.split():\n                            if w.endswith(\"ing\")and w[0].islower():\n                                got = w\n                                break\n                        if got != \"\":\n                            got = got + \"\" + l.split(got)[1]\n                            functions.append(got)\n                elif \" as an \" in l:\n                    l = l.split(\" as an \")[1]\n                    if len(l.split()) <= 8:\n                        pass \n                    elif \"experience\" in l:\n                        pr = l.split(\"experience\")[1].strip()\n                        pr = pr.lstrip(\"in the\").strip()\n                        pr = pr.lstrip(\"in \").strip()\n                        functions.append(\"experience in \" + pr)\n                    elif \"ing \" in l:\n                        got = \"\"\n                        for w in l.split():\n                            if w.endswith(\"ing\") and w[0].islower():\n                                got = w\n                                break\n                        if got != \"\":\n                            got = got + \"\" + l.split(got)[1]\n                            functions.append(got)\n                elif \" at the level of \" in l:\n                    l = l.split(\" at the level of \")[1]\n                    if len(l.split()) <= 8:\n                        pass # ignore\n                    elif \"experience\" in l:\n                        pr = l.split(\"experience\")[1].strip()\n                        pr = pr.lstrip(\"in the\").strip()\n                        pr = pr.lstrip(\"in \").strip()\n                        functions.append(\"experience in \" + pr)\n                    elif \"ing \" in l:\n                        got = \"\"\n                        for w in l.split():\n                            if w.endswith(\"ing\") and w[0].islower():\n                                got = w\n                                break\n                        if got != \"\":\n                            got = got + \"\" + l.split(got)[1]\n                            functions.append(got)\n        else:\n            \"\"\" alternate to experience, also handle cases for employment \"\"\"\n            if \"employment as a\" in l:\n                if \"position\" in l:\n                    functions.append(l.split(\"position\")[1])\n        \n        \"\"\" find final entity \"\"\"\n        func = \"\"\n        if len(functions) > 0:\n            func = functions[0].strip().rstrip(\" and\").rstrip(\" or\").rstrip(\":\").rstrip(\";\")        \n            words = func.split()\n            if \"following\" in words[-3:]:\n                func = \"\"\n        return func\n        \n    \"\"\" function to dig deeper into requirement text and obtain if any major is missed \"\"\"\n    def deep_major(self, txt):\n        \"\"\" function to dig deeper into requirement text and obtain if any \n            major related entity is missed from the first step \"\"\"\n\n        major = \"\"\n        for line in txt:\n            if \"university in \" not in line.lower():\n                continue\n            if \"in order\" in line.lower():\n                continue\n            if \"degree\" in line.lower():\n                major = line.split(\" in \")[1].split(\"related\")[0]\n        return major\n    \n    \"\"\" utility function to add up semester and quarter together \"\"\"\n    def add_course(self, sem, quar):\n        \"\"\"\n        this function is part of the restructuring component, it combines semester and quarter \n        together to give a combined value of course length in the format xSyQ\n        \"\"\"\n        \n        month, quarter = \"\", \"\"\n        if len(sem) > 0:\n            month = str(max(sem))+\"S\"\n        if len(quar) > 0:\n            quarter = str(max(quar))+\"Q\"\n        course_len = month + \",\" + quarter\n        if course_len == \",\":\n            course_len = \"\"\n        return course_len\n    \n    \"\"\" utility function to cleanup the final entities \"\"\"\n    def striptext(self, txt):\n        return \" \".join(str(txt).strip().split()).strip()    \n    \n    \"\"\" utility function to obtain the misc details about the course \"\"\"\n    def misc_details(self, txt):\n        \"\"\"\n        parses the requirement text, identifies additional course details \n        \n        :params: \n        txt: requirement text where misc details about course are captured\n        \"\"\"\n        \n        misc_details = \"\"\n        course_lists = self.lookups['education'] + self.lookups['semester']\n        ignore = ['experience', 'college', 'high school', 'trade school']\n        for l in txt.split(\"\\n\"):   \n            if any (w in l for w in ignore):\n                continue\n            if any(w in l.lower() for w in course_lists):\n                misc_details += l+\"\\n\"\n        misc_details = misc_details.strip().rstrip(\"or\").rstrip(\"and\")\n        return misc_details\n\n    \"\"\" master function to call all the other functions and generate the output \"\"\"\n    def standardize(self):\n        \"\"\"\n        master function 'standardize' is the main function which calls all the \n        other functions in order to obtain structured information\n        \"\"\"\n        \n        \"\"\" create an empty list to store the results \"\"\"\n        rows = []\n        form = Formatter()\n        \n        \"\"\" first obtain the requirement text \"\"\"\n        requirement_text = self.portion(\"requirement\", indx = True)  \n        requirement_list = self.requirement_IDs(requirement_text)\n        \n        \"\"\" iterate in every requirement line, and call the other functions \"\"\"\n        for j in range(0, len(requirement_list[0])):            \n            doc = {}\n            doc['FILE_NAME'] = \" \".join(self.filename.split())\n            \n            \"\"\" store the set-id, set-text, subset-id, subset-text \"\"\"\n            doc[\"REQUIREMENT_SET_ID\"] = requirement_list[0][j]\n            doc[\"REQUIREMENT_SET_TEXT\"] = requirement_list[2][j]\n            doc[\"REQUIREMENT_SUBSET_ID\"] = requirement_list[1][j]\n            doc[\"REQUIREMENT_SUBSET_TEXT\"] = requirement_list[3][j]\n            \n            \"\"\" requirement conjuction \"\"\"\n            conjunction = []\n            t1 = requirement_list[2][j].strip()\n            t2 = requirement_list[3][j].strip()\n            if t1.endswith(\"or\"):\n                conjunction.append(\"or\")\n            elif t1.endswith(\"and\"):\n                conjunction.append(\"and\")\n            elif t2.endswith(\"and\"):\n                conjunction.append(\"and\")\n            elif t2.endswith(\"or\"):\n                conjunction.append(\"or\")\n            doc['REQUIREMENT_CONJUNCTION'] = \";\".join(conjunction).rstrip(\";\")\n            \n            \"\"\" add classcode, duties, open date using generic functions \"\"\"\n            doc['JOB_CLASS_NO'] = self.portion(\"class code:\")\n            doc['JOB_DUTIES'] = self.portion(\"duties\", indx=True)        \n            doc['OPEN_DATE'] = self.portion(\"open date:\")\n\n            \"\"\" create a combined requirement text row, to be used for other entities \"\"\"\n            rtext = doc[\"REQUIREMENT_SET_TEXT\"] +\"\\n\"+ doc['REQUIREMENT_SUBSET_TEXT']\n            rtext = re.sub(r'\\([^)]*\\)', '', rtext)            \n\n            \"\"\" add schooltype, experience type date using generic functions \"\"\"\n            doc[\"SCHOOL_TYPE\"] = self.presence_check(\"school\", txt=rtext)\n            doc[\"FULL_TIME_PART_TIME\"] = self.presence_check(\"full_part\", txt=rtext)\n                        \n            \"\"\" custom functions that uses requirement text \"\"\"\n            doc[\"EXP_JOB_CLASS_ALT_RESP\"] = self.job_class_alt_function(rtext)\n            doc[\"EXP_JOB_CLASS_TITLE\"] = self.experience_title(rtext)\n            doc[\"COURSE_SUBJECT\"] = self.course_subjects(rtext) \n            doc[\"COURSE_COUNT\"] = self.course_count(rtext)\n            doc[\"ENTRY_SALARY_GEN\"] = self.salary_details()[0]\n            doc[\"ENTRY_SALARY_DWP\"] = self.salary_details()[1]\n            doc[\"DRIVERS_LICENSE_REQ\"] = self.driver_license()\n            doc[\"MISC_COURSE_DETAILS\"] = self.misc_details(rtext)\n            doc[\"EXP_JOB_CLASS_FUNCTION\"] = self.get_function(rtext)\n            \n            \"\"\" custom general functions \"\"\"\n            doc[\"DRIV_LIC_TYPE\"] = self.license_type()[0]\n            doc['JOB_CLASS_TITLE'] = self.class_title()\n            doc[\"ADDTL_LIC\"] = self.license_type()[1]\n            doc[\"EXAM_TYPE\"] = self.exam_type()      \n                        \n            \"\"\" identify year / months / number using generic functions \"\"\"\n            doc[\"Exp_y\"] = self.year_month(\"experience\", rtext, span = 'year')\n            doc[\"Exp_m\"] = self.year_month(\"experience\", rtext, span = 'month')\n            doc[\"Cor_s\"] = self.year_month(\"semester\", rtext, span='semester')\n            doc[\"Cor_q\"] = self.year_month(\"semester\", rtext, span='quarter')\n            doc[\"Edu_y\"] = self.year_month(\"education\", rtext, span = 'year')\n            \n            \"\"\" add course and experience length by aggregating years and months \"\"\"\n            doc[\"COURSE_LENGTH\"] = self.add_course(doc['Cor_s'], doc['Cor_q'])\n            doc[\"EDUCATION_MAJOR\"] = self.portions('major',rtext,limit=180,next_word=False)\n            doc[\"EXPERIENCE_LENGTH\"], doc[\"EDUCATION_YEARS\"]   = \"\", \"\"\n            \n            \"\"\" cleanup and restructuring \"\"\"\n            if len(doc[\"Exp_y\"] + doc[\"Exp_m\"]) > 0:\n                doc[\"EXPERIENCE_LENGTH\"] = max(doc[\"Exp_y\"] + doc[\"Exp_m\"])            \n            if len(doc['Edu_y']) > 0:\n                doc[\"EDUCATION_YEARS\"] = doc[\"Edu_y\"][0]\n            if doc['EDUCATION_MAJOR'] == \"\":\n                mix = requirement_list[2][j].split(\"\\n\")+requirement_list[3][j].split(\"\\n\")\n                doc['EDUCATION_MAJOR'] = self.deep_major(mix)\n            if \"college or university\" in doc[\"SCHOOL_TYPE\"].lower():\n                doc[\"SCHOOL_TYPE\"] = doc[\"SCHOOL_TYPE\"].replace(\"|college\",\"\")\n\n            \"\"\" perform further cleaning and standardizing on some fields \"\"\"\n            exp_title, alt_exp = doc[\"EXP_JOB_CLASS_TITLE\"], doc[\"EXP_JOB_CLASS_ALT_RESP\"]\n            doc[\"EXP_JOB_CLASS_ALT_RESP\"] = form.cleanup(alt_exp, tag='alt_exp')\n            doc[\"EXP_JOB_CLASS_TITLE\"] = form.cleanup(exp_title, tag='exp').title()\n            doc[\"EDUCATION_MAJOR\"] = form.cleanup(doc[\"EDUCATION_MAJOR\"], tag='major')  \n            doc[\"ADDTL_LIC\"] = form.cleanup(doc[\"ADDTL_LIC\"], tag='add_lic')\n            doc[\"OPEN_DATE\"] = form.cleanup(doc[\"OPEN_DATE\"], tag='date')  \n            \n            \"\"\" append the key-value pairs in the global list \"\"\"\n            rows.append({k:v for k,v in doc.items() if not k.startswith(\"_\")})\n        return rows","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div id=\"2.2\"></div>\n### <font color=\"#ff3fb5\">2.2 Formatter Class</font>\n\nThe second component is Formatter Class whose role is to remove the noise from the extracted entities and produce cleaned entities as output. "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"class Formatter:\n    \"\"\"\n    python class to format, restructure, and clean the extracted entities from different \n    functions of the BulletinParser class. \n    \"\"\"\n    \n    def __init__(self):\n        self.split6 = [\" and \", \"which\", \" with \", \", or any\", \"(as\",'such as', 'from ', \n        'may be', 'or a closely',  ', or upon', ', and one', ', which', ', or closely', '(as']\n        self.major_reps = [', , includi', ', or other', ', or in a closely related field', \n        'or a closely related field', 'a related field',', or a', ', or a related field'] \n        self.exp_starts = ['at the ', 'at that ', 'which is at least at the ', 'which is ',\n        'which is at that ', 'which is at the ', 'atleast at the ', 'at least at the ']\n        self.exp_starts1 = ['level', 'atleast', 'in', 'the', 'which', 'at', 'least', 'of','as ',\n                           'a ', 'that', 'with']\n        self.ignore_exp = [\"uniformed\", \"helper\", \"buyer\", \"civil\", \"engaged\", \"exempt\", \n        \"lead\", \"construction\", \"crime\", \"heating\", \"maintenance\", \"insulator\"]\n        self.exp_starts += ['atleast at that ', 'at least at that ']\n        self.major_reps += ['the text box which will appear']\n        \n    \"\"\" custom function used to cleanup the text/entites obtained from different functions \"\"\"\n    def cleanup(self, txt, tag):\n        \"\"\"\n        this function is used to clean the extracted according to different rules, this function\n        removes the noise captured along with the entity. Custom rules are used for this purpose. \n        \n        :params:\n        tag: identifies what type of cleaning is required\n        \"\"\"\n        \n        cleaned = \"\"\n        if tag == 'date':\n            \"\"\" cleaning the date \"\"\"\n            if \"-\" in txt:\n                txt = txt.split(\"-\")\n                m, d, y = txt[0], txt[1], txt[2]\n                if len(m) == 1:\n                    m = \"0\"+m\n                if len(y) == 4:\n                    y = y[2:]\n                cleaned = m+\"-\"+d+\"-\"+y\n                \n        elif tag == 'major':\n            \"\"\" cleaning the major \"\"\"\n            if not txt.startswith(\"the education section\"):\n                cleaned = txt.replace(\"major in \",\"\")\n                for split in self.split6:\n                    cleaned = cleaned.split(split)[0]\n                for r in self.major_reps:\n                    cleaned = cleaned.replace(r, \"\")\n                cleaned = cleaned.lstrip(\",\").rstrip(\",\").strip()\n                for st in ['such as', 'a ', 'an ']:\n                    if cleaned.startswith(st):\n                        cleaned = cleaned.replace(st, \"\")\n                for st in [', or', ', and']:\n                    if cleaned.endswith(st):\n                        cleaned = cleaned.split(st)[0]                    \n                if cleaned.startswith(\"in \"):\n                    cleaned = cleaned[3:].strip()\n                if cleaned.endswith(\" or\"):\n                    cleaned = cleaned[:-3]\n                cleaned = cleaned.strip().rstrip(\",\")\n        \n        elif tag == 'add_lic':\n            \"\"\" cleaning the additional license required \"\"\"\n            cleaned = []\n            for line in txt:\n                line = line.lower()\n                line = line.replace(\"'s\", \"\")\n                if line.startswith(\"a \"):\n                    line = line[2:].strip()\n                if line.endswith(\"license\"):\n                    line = line.replace(\"license\", \"\")\n                if \" as a \" in line:\n                    line = line.split(\" as a\")[1].strip()\n                line = line.replace(\" (pe)\",\"\")\n                if any(line == x for x in ['special', 'the', 'this']):\n                    line = \"\"\n                line = line.strip() + \" License\"\n                line = \" \"+line+\" \"\n                line = line.replace(\" pe \", \" PE \").strip()\n                if line == \"License\":\n                    line = \"\"\n                else:\n                    cleaned.append(line)\n            cleaned = \", \".join(cleaned)\n\n        elif tag == 'exp':\n            \"\"\" cleaning the experience \"\"\"\n            cleaned = []\n            txt = txt.rstrip(\",\").replace(\"engaged \",\"\").replace(\"either \", \"\")\n            for split in [\"which\", \"one year of\", \"for \"]:\n                txt = txt.split(split)[0]\n            if len(txt.split()) == 1:\n                if txt.lower().split()[0] in self.ignore_exp:\n                    txt = \"\"        \n            for i,_ in enumerate(txt.split()):\n                if _[0].isupper() == True:\n                    pass\n                elif i == 0:\n                    pass\n                elif _.lower()[:5] in [\"build\", \"plumb\", \"condi\", \"housi\"]:\n                    pass\n                elif _.replace(\",\",\"\").endswith(\"ing\"):\n                    break\n                cleaned.append(_)\n            cleaned = \" \".join(cleaned)\n            \n            \n            if cleaned.endswith(\"engaged\"):\n                cleaned = cleaned.replace(\"engaged\", \"\")\n            if \"(\" in cleaned:\n                cleaned = cleaned.split(\"(\")[0]\n            elif \" by \" in cleaned:\n                cleaned = cleaned.split(\" by \")[0]\n            elif cleaned.startswith(\"a \"):\n                cleaned = cleaned[2:].strip()\n            elif cleaned.startswith(\"an \"):\n                cleaned = cleaned[3:].strip()\n                \n            cleaned = cleaned.rstrip(\",\").rstrip(\".\").replace(\", two years of\",\"\")\n            cleaned = cleaned.split(\"Class Code\")[0]\n            if \" the level of \" in cleaned:\n                cleaned = cleaned.split(\" the level of \")[1]\n            cleaned = cleaned.replace(\", or\",\"\").split(\"within\")[0]\n            if cleaned.lower().endswith(\"construction\"):\n                cleaned = cleaned.lower().replace(\"construction\", \"\").title()\n            if cleaned.endswith(\"responsible\"):\n                cleaned = cleaned.replace(\"responsible\", \"\")\n            cleaned = cleaned.rstrip(\",\").rstrip(\".\")\n            if cleaned.strip().endswith(\" of\"):\n                cleaned = cleaned[:-2].strip()\n            if cleaned.strip().endswith(\" or\"):\n                cleaned = cleaned[:-2].strip()\n            if cleaned.strip().endswith(\" and\"):\n                cleaned = cleaned[:-3].strip()\n\n            ## more cleaning rules\n            cleaned = cleaned.split(\"may be sub\")[0]\n            cleaned = cleaned.replace(\"lead; \",\"\").replace(\"engaged; \",\"\")\n            if cleaned.startswith(\"exempt\"):\n                cleaned = cleaned.replace(\"exempt or\",\"\").replace(\"exempt\",\"\")\n            if cleaned.startswith(\"building, construction,\"):\n                cleaned = \"\"\n            if cleaned.startswith(\"housing, building, electrical\"):\n                cleaned = \"\"\n            if cleaned.endswith(\", building\"):\n                cleaned = cleaned.replace(\", building\",\"\")\n                \n\n        if tag == 'alt_exp':\n            \"\"\" cleaning the alternate experience \"\"\"\n            cleaned = txt.strip()\n            if cleaned:          \n                cleaned = cleaned.replace(\"with the City of Los Angeles;\",\"\")\n                for e in self.exp_starts:\n                    cleaned = cleaned.replace(e,\"\").strip()\n                for e in self.exp_starts1:\n                    if cleaned.startswith(e):\n                        cleaned = cleaned[len(e):].strip()\n                        cleaned = cleaned.lstrip(\",\").strip()\n                for e in self.exp_starts1:\n                    if cleaned.startswith(e):\n                        cleaned = cleaned[len(e):].strip()\n                        cleaned = cleaned.lstrip(\",\").strip()\n                if cleaned.endswith(\"or\"):\n                    cleaned = cleaned[:-2]\n                if cleaned.endswith(\"and\"):\n                    cleaned = cleaned[:-3]\n        return cleaned","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div id=\"2.3\"></div>\n### <font color=\"#ff3fb5\">2.3 Extractor Class</font>\n\nThe third component is the Extractor Class whose role is to exectute the entire code based on the user input config. This class act as the controller for the structuring engine."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import pandas as pd \nclass Extractor:\n    \"\"\"\n    Controller Class, which executes the key piece of code required to obtain entities and \n    information for different files, and producing a structured file as output\n    \"\"\"\n    \n    def __init__(self, path):\n        \"\"\"\n        python class to format, restructure and remove noise from obtained entities\n        most of the noise is handelled during the entity extraction but sometimes there is \n        still presence of noisy keywords. \n        \"\"\"\n        \n        self.path   = path\n        self.column = ['FILE_NAME', 'JOB_CLASS_TITLE', 'JOB_CLASS_NO', 'REQUIREMENT_SET_ID', \n        'REQUIREMENT_SUBSET_ID', 'JOB_DUTIES', 'EDUCATION_YEARS', 'SCHOOL_TYPE', \n        'EDUCATION_MAJOR', 'EXPERIENCE_LENGTH', 'FULL_TIME_PART_TIME', \n        'EXP_JOB_CLASS_TITLE', 'EXP_JOB_CLASS_ALT_RESP', 'EXP_JOB_CLASS_FUNCTION', \n        'COURSE_COUNT', 'COURSE_LENGTH', 'COURSE_SUBJECT', 'MISC_COURSE_DETAILS', \n        'DRIVERS_LICENSE_REQ', 'DRIV_LIC_TYPE', 'ADDTL_LIC', 'EXAM_TYPE', 'ENTRY_SALARY_GEN', \n        'ENTRY_SALARY_DWP', 'OPEN_DATE', 'REQUIREMENT_SET_TEXT', 'REQUIREMENT_SUBSET_TEXT', \n        'REQUIREMENT_CONJUNCTION']\n\n    \"\"\" function to iterate in files and generate a structured csv file \"\"\"\n    def extraction(self):\n        \"\"\" \n            master function that iterates in the root folder for every file \n            and obtains the structured information, final output is a csv file\n        \"\"\"\n        \n        print (\"> Starting Extraction for the Given Folder\")\n        rows = []\n        files = sorted(os.listdir(path))\n        for filename in files:\n            try:\n                config = {\"path\" : self.path, \"filename\" : filename}\n                bp = BulletinParser(config)\n                rows.extend(bp.standardize())\n            except Exception as E:\n                print (E)\n                continue\n\n        df = pd.DataFrame(rows)[self.column]\n        for c in df.columns:\n            df[c] = df[c].apply(lambda x : \"-\" if x == \"\" else x)\n            df[c] = df[c].apply(lambda x : bp.striptext(x))\n        df.to_csv(\"structured_file.csv\", index = False)\n        print (\">> Extraction Complete for Entire Folder\")\n        return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div id=\"3\"></div>\n## <font color=\"#ff3fb5\">3. Execute and Generating Final Structured CSV</font>    \n\nLet's now execute the program and generate final structured csv with just three lines of code. "},{"metadata":{"trusted":true},"cell_type":"code","source":"## define the input path\npath = \"../input/cityofla/CityofLA/Job Bulletins/\"\n\n## create the Extractor Class object\nextr = Extractor(path)\n\n## call the extraction function\ndata = extr.extraction()\ndata.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from IPython.display import HTML\nimport pandas as pd\nimport numpy as np\nimport base64\n\ndef create_download_link(df, title = \"Download Complete Structured CSV file\", filename = \"structured_file.csv\"):  \n    csv = df.to_csv()\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = \"\"\"<h1><a download=\"{filename}\" href=\"data:text/csv;base64,{payload}\" target=\"_blank\">{title}</a></h1>\n    \n    <h3><a href='https://docs.google.com/spreadsheets/d/1MuzQd4fuKvElQvVXCG68KOLSgoo0IBTEQseaW_Z7JCE/edit?usp=sharing'>Updated Data Dictionary</a></h3>\"\"\"\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\ncreate_download_link(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"<div id=\"4\"></div>\n## <font color=\"#ff3fb5\">4. Complete Documentation: </font>\n\nThe entire code is well-documented with docstrings and useful comments. These comments explain the process flow of a function and the use of certain code snippet.  \n\n<h3><a href='http://www.shivambansal.com/blog/network/cola/BulletinStructuringEngine.html'>VIEW COMPLETE CODE DOCUMENTATION HERE</a></h3>    \n\n\n<div id=\"5\"></div>\n## <font color=\"#ff3fb5\">5. Python Package</font>\n\nFor reusability, reproducibility and ease, I have wrapped the entire code in a python package callend \"pycola\". This package makes my solution fully automated and can be accessed anywhere. Following are the instructions to use the package:  \n\n#### Installation \n\n<code>pip install pycola</code>\n\n#### Usage \n```python\nfrom pycola.bulletin_parser import Extractor\n\n## define the input path\nconfig = {\n\t\"input_path\" : \"Bulletins/\",\n\t\"output_filename\" : \"structured_file.csv\"\n}\n\n## create the Extractor Class object\nextr = Extractor(config)\n\n## call the extraction function\nextr.extraction()\n```\n\nPackage link : https://pypi.org/project/pycola/\n\n<div id=\"6\"></div>\n## <font color=\"#ff3fb5\">6. Key Highlights of the Approach </font>\n\nIn the end, I would like to highlight specific cases and elements that I have incorporated. \n\n**<font color=\"#ff3fb5\">1. Edge Cases Handelling:</font>** I have incorporated many flexible text mining rules and handled different edge cases. For example: \n\n**Requirement of Zoo Creator Job Class (4297):**  \n\n>**Line1:** 1. Graduation from an accredited four-year college or university with a major in zoology, biology, or a closely related field; and  \n>**Line2:** 2. a.  Three years of full time paid professional zoo experience as an assistant or associate curator, curator, assistant  \n>**Line3:** zoo director, or zoo director; or  \n>**Line4:** b.  Three years of full time paid experience supervising the care, selection, or identification of exotic and wild animals as a   \n>**Line5:** Senior Animal Keeper, or in a class at that level, with the City of Los Angeles.     \n\nIn this requirement, we can observe that the single line text is broken into multiple lines and creates an edge cases. I observed that many other approaches used in the competition have ignore such edge cases and results differ. I added flexible rules to identify such cases and further recity them before information extraction. Following is the rectified result:   \n\n>**Line1:** 1. Graduation from an accredited four-year college or university with a major in zoology, biology, or a closely related field; and  \n>**Line2:** 2. a.  Three years of full time paid professional zoo experience as an assistant or associate curator, curator, assistant zoo director, or zoo director; or  \n>**Line3:** b.  Three years of full time paid experience supervising the care, selection, or identification of exotic and wild animals as a Senior Animal Keeper, or in a class at that level, with the City of Los Angeles.     \n\nAtleast 10 files had such problem. While other rules are also added for cases like: \n\n- single line requirement    \n- requirements with no bullets  \n- abrupt line breaks   \n\n**<font color=\"#ff3fb5\">2. Fully Automated:</font>** \n\nThe whole system is fully automated, it just needs an input and it produces the structured results. This is how one can use the code. The users can either choose to run this kernel to get the results, or they can use the package \"pycola\" which I have created by wrapping the code of this kernel in a module. Simply, install the package using the following command. \n\n<code>pip install pycola</code>\n\nand then use it as defined in point 5. Package is hosted at pypi (https://pypi.org/project/pycola/)   \n\n\n**Generic Functions:**    \n\nSame functions were used to extract different entities just by passing different arguments. For example - \n\n> - year_month() function was used to extract corresponding years/months/semesters/quarters using different input arguments.  \n> - presence_check() function was used to check if a group of keywords is present in the text, used for different fields.       \n> - portion() function was used to extract different portions of text from the bulletins by passing different input arguments.  \n\n**<font color=\"#ff3fb5\">3. Coding Practicies:</font>**   \n\nAll the snippets in all of my kernels uses PEP8 and coding conventions from the popular The [Hitchhikers Guide to Python](\"https://docs.python-guide.org/) by Kenneth Reitz.  \n\n**<font color=\"#ff3fb5\">4. Documentation, Comments and DocStrings:</font>** \n\nFunctions, methodology, and the classes are well-documented using docstrings and useful comments. It can be viewed <a href='http://www.shivambansal.com/blog/network/city_of_la/BulletinStructuringEngine.html'>here</a>\n\n**<font color=\"#ff3fb5\">5. Solution Evaluation:</font>**  \n\n> - For evaluation purposes, I focussed on two metrics - **Completeness** and **Correctness**. \n> - **Completeness:** Ability of the framework to give as many as possible results possbile both in terms of following:      \n> &nbsp;&nbsp;&nbsp;&nbsp; - completeness in terms of Files  \n> &nbsp;&nbsp;&nbsp;&nbsp; - completeness in terms of Columns / Fields   \n> - **Correctness:** Ability of the framework to produce as accurate as possible results    \n> - I developed this framework in a highly iterative process in which constant evaluation was part of every step.  \n\nThe iteration process was: \n\n> - Maintained a randomized sample of N files. Started small with N = 10 files only.  \n> - From the reminaing files, used M files (started small with M = 10) to develop the parts of the framework (example - writing text mining rules, expressions, and keyword dictionaries for relevant information extraction).   \n> - Manually evaluate the results for the M+N files both in terms of completeness and correctness.   \n> - In the cases in which the techniques were not giving the results, I tried to identified the root cause (different pattern, data format issue, different keywords used etc.)    \n> - Using the learnings from the evaluation, updated the techniques and rules to incorporate as much as and as correct as possible results. \n> - Iterated the process for bigger samples, N = 25, 50, and 100 and M = 50, 100, 500. \n> - In the end, I also did a manual quality check of the final structured file multiple times to make sure that system has produced the desired results. \n\n### Next Kernels: \nFor next parts of my submission (analysis and recommendations), please visit next kernels of my Submission: \n<ul>\n    <li><a href=\"https://www.kaggle.com/shivamb/1-description-structuring-engine-cola\" target=\"_blank\">Part 1: Job Bulletin Structuring Engine - City of Los Angeles </a>  </li>\n    <li><a href=\"https://www.kaggle.com/shivamb/2-encourage-diversity-reduce-bias-cola\" target=\"_blank\">Part 2: Encourage Diversity and Remove Unconsious Bias from Job Bulletins - A Deep Analysis</a>  </li>\n    <li><a href=\"https://www.kaggle.com/shivamb/3-impact-of-ctl-content-tone-language-cola\" target=\"_blank\">Part 3: Impact of Content, Tone, and Language : CTL Analysis for CoLA</a>  </li> \n    <li><a href=\"https://www.kaggle.com/shivamb/4-promotional-pathway-discoverability-cola\" target=\"_blank\">Part 4: Increasing the Discoverability of Promotional Pathways (Explicit) </a></li>\n    <li><a href=\"https://www.kaggle.com/shivamb/5-implicit-promotional-pathways-discoverability/\" target=\"_blank\">Part 5: Implicit Promotional Pathways Discoverability</a></li></ul>\n\nNext <a href=\"https://www.kaggle.com/shivamb/2-encourage-diversity-reduce-bias-cola\" target=\"_blank\">Kernel</a> - Analysis of Unconscious Bias in Job Bulletins."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"}},"nbformat":4,"nbformat_minor":1}