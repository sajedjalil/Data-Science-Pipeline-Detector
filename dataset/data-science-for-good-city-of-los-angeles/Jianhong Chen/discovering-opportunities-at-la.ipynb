{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Data Science for Good: City of Los Angeles\n\n# 0. Workflow stages\n\nThe competition solution workflow goes through the following stages.\n\n1. Question definition and goal.\n2. Prepare and cleanse the data.\n3. Analyze, identify patterns, and explore the data.\n4. Recommendations\n\n\n\n# 1. Question definition and goal\n\n**Question**\n\nThe content, tone, and format of job bulletins can influence the quality of the applicant pool. Overly-specific job requirements may discourage diversity. The Los Angeles Mayor’s Office wants to reimagine the city’s job bulletins by using text analysis to identify needed improvements.\n\n**Goal**\n\nThe goal is to convert a folder full of plain-text job postings into a single structured CSV file and then to use this data to:\n1. identify language that can negatively bias the pool of applicants;\n2. improve the diversity and quality of the applicant pool; \n3. make it easier to determine which promotions are available to employees in each job class.\n\n"},{"metadata":{},"cell_type":"markdown","source":"# 2. Prepare and cleanse the data\n**Objective**\n\nParse job bulletin text files and create output dataframe with the structure as mentioned in \"Sample job class export template.csv\". We have exported the output dataframe in \"job_bulletins.csv\" accordingly. \n\n## 2.1 Headings\nAfter looking at the data in text files, we can observe that there is some specefic pattern or format which is kept while writing job bulletins. We can use this to parse the text. For example, the headings are written in upper case letters and the order of the headings almost coincides with each other."},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import os, sys\nimport pandas as pd,numpy as np\nimport re\nimport spacy\nfrom spacy import displacy\nfrom collections import Counter\nimport en_core_web_sm\nnlp = en_core_web_sm.load()\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tag import pos_tag\nimport xml.etree.cElementTree as ET\nfrom collections import OrderedDict\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport json\nfrom datetime import datetime\nimport calendar\nfrom wordcloud import WordCloud ,STOPWORDS\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom gensim.models import word2vec\nfrom sklearn.manifold import TSNE\nfrom nltk import pos_tag\nfrom nltk.help import upenn_tagset\nimport gensim\nimport matplotlib.colors as mcolors\nplt.style.use('ggplot')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"'''\ndata directory \nJob Bulletins: This directory contains the job bulletins in text format.\nAdditional data: This directory contains additional data in pdf and csv format.\n'''\nbulletin_dir = \"../input/data-science-for-good-city-of-los-angeles/cityofla/CityofLA/Job Bulletins\"\nbulletins=os.listdir(bulletin_dir)\nadditional_data_dir = '../input/data-science-for-good-city-of-los-angeles/cityofla/CityofLA/Additional data'","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"headings = {}\nfor filename in os.listdir(bulletin_dir):\n    with open(bulletin_dir + \"/\" + filename, 'r', errors='ignore') as f:\n        for line in f.readlines():\n            line = line.replace(\"\\n\",\"\").replace(\"\\t\",\"\").replace(\":\",\"\").strip()\n            \n            if line.isupper():\n                if line not in headings.keys():\n                    headings[line] = 1  #add the heading as a new key into dictionary \n                else:\n                    count = int(headings[line])\n                    headings[line] = count+1  #add value 1 to the corresponding heading ","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"del headings['$103,606 TO $151,484'] #This is not a heading, it's an Annual Salary component\nheadingsFrame = [] \nfor i,j in (sorted(headings.items(), key = lambda kv:(kv[1], kv[0]), reverse = True)):\n    headingsFrame.append([i,j]) #convert the dictionary into dataframe\nheadingsFrame = pd.DataFrame(headingsFrame)\nheadingsFrame.columns = [\"Heading\",\"Count\"]\nheadingsFrame.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.2 Basic information\nWe extract the basic information of \"FILE_NAME\", \"OPEN_DATE\", \"POSITION\", \"JOB_CLASS_NO\" into a dataframe df here. "},{"metadata":{"trusted":false},"cell_type":"code","source":"#Add 'FILE_NAME', 'POSITION', 'JOB_CLASS_NO'\ndata_list = []\nfor filename in os.listdir(bulletin_dir):\n    with open(bulletin_dir + \"/\" + filename, 'r', errors='ignore') as f:\n        position=''\n        for line in f.readlines():\n            #Parse job bulletins\n            if \"Open Date:\" in line:\n                job_bulletin_date = line.split(\"Open Date:\")[1].split(\"(\")[0].strip()\n            if \"Class Code:\" in line:\n                job_class_no = line.split(\"Class Code:\")[1].split(\".\")[0].split(\"C\")[0].strip().split(\" \")[0]\n            if len(position)<2 and len(line.strip())>1:\n                position=line.split(\"Class Code:\")[0].strip().lower()\n        data_list.append([filename, job_bulletin_date, position, job_class_no])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df = pd.DataFrame(data_list)\ndf.columns = [\"FILE_NAME\", \"OPEN_DATE\", \"POSITION\", \"JOB_CLASS_NO\"]\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.3 Requirement\nWe extract the requirement of the jobs into a dataframe df_requirements here. "},{"metadata":{"_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"#Add 'REQUIREMENT_SET_ID','REQUIREMENT_SUBSET_ID','REQUIREMENT_TEXT'\nrequirements = []\nrequirementHeadings = [k for k in headingsFrame['Heading'].values if 'requirement' in k.lower()]\nfor filename in os.listdir(bulletin_dir):\n    with open(bulletin_dir + \"/\" + filename, 'r', errors='ignore') as f:\n        readNext = 0\n        isNumber=0\n        prevNumber=0\n        prevLine=''\n        \n        for line in f.readlines():\n            clean_line = line.replace(\"\\n\",\"\").replace(\"\\t\",\"\").replace(\":\",\"\").strip()   \n            if readNext == 0:                         \n                if clean_line in requirementHeadings:\n                    readNext = 1\n            elif readNext == 1:\n                if clean_line in headingsFrame['Heading'].values:\n                    if isNumber>0:\n                        requirements.append([filename,prevNumber,'',prevLine])\n                    break\n                elif len(clean_line)<2:\n                    continue\n                else:\n                    rqrmntText = clean_line.split('.')\n                    if len(rqrmntText)<2:\n                        requirements.append([filename,'','',clean_line])\n                    else:                        \n                        if rqrmntText[0].isdigit():\n                            if isNumber>0:\n                                requirements.append([filename,prevNumber,'',prevLine])\n                            isNumber=1\n                            prevNumber=rqrmntText[0]\n                            prevLine=clean_line\n                        elif re.match('^[a-z]$',rqrmntText[0]):\n                            requirements.append([filename,prevNumber,rqrmntText[0],prevLine+' - '+clean_line])\n                            isNumber=0\n                        else:\n                            requirements.append([filename,'','',clean_line])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_requirements = pd.DataFrame(requirements)\ndf_requirements.columns = ['FILE_NAME','REQUIREMENT_SET_ID','REQUIREMENT_SUBSET_ID','REQUIREMENT_TEXT']\ndf_requirements.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Check for one sample file \ndf_requirements.loc[df_requirements['FILE_NAME']=='SYSTEMS ANALYST 1596 102717.txt']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.4 Salary\nWe extract the salary of the jobs into a dataframe df_salary here. "},{"metadata":{"_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"#Check for salary components\nsalHeadings = [k for k in headingsFrame['Heading'].values if 'salary' in k.lower()]\nsal_list = []\nfor filename in os.listdir(bulletin_dir):\n    with open(bulletin_dir + \"/\" + filename, 'r', errors='ignore') as f:\n        readNext = 0\n        for line in f.readlines():\n            clean_line = line.replace(\"\\n\",\"\").replace(\"\\t\",\"\").replace(\":\",\"\").strip()  \n            if clean_line in salHeadings:\n                readNext = 1\n            elif readNext == 1:\n                if clean_line in headingsFrame['Heading'].values:\n                    break\n                elif len(clean_line)<2:\n                    continue\n                else:\n                    sal_list.append([filename, clean_line])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_salary = pd.DataFrame(sal_list)\ndf_salary.columns = ['FILE_NAME','SALARY_TEXT']\ndf_salary.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"However, the salary information contain different formats and it differs for workers in different departments. So we use the regular expressions to extract the salary numbers for city workers in departments other than DWP as df_salary_gen and for the Department of Water and Power workers as df_salary_dwp."},{"metadata":{"trusted":false},"cell_type":"code","source":"files = []\nfor filename in os.listdir(bulletin_dir):\n    files.append(filename)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"#Add 'ENTRY_SALARY_GEN','ENTRY_SALARY_DWP'\npattern = r'\\$?\\ ?([0-9]{1,3},\\ ?([0-9]{3},)*\\ ?[0-9]{3}|[0-9]+)(.[0-9][0-9])?\\*?'\ndf_salary_dwp=pd.DataFrame(columns=['FILE_NAME','ENTRY_SALARY_START_DWP','ENTRY_SALARY_END_DWP'])\ndf_salary_gen=pd.DataFrame(columns=['FILE_NAME','ENTRY_SALARY_START_GEN','ENTRY_SALARY_END_GEN'])\ndwp_salary_list = {}\ngen_salary_list = {}\nfor filename in files:\n    for sal_text in df_salary.loc[df_salary['FILE_NAME']==filename]['SALARY_TEXT']:\n        if 'department of water' in sal_text.lower():\n            if filename in dwp_salary_list.keys():\n                continue\n            matches = re.findall(pattern+' to '+pattern, sal_text) \n            if len(matches)>0:\n                salary_dwp = ' - '.join([x for x in matches[0] if x and not x.endswith(',')])\n                start=matches[0][0]\n                end=matches[0][3]\n            else:\n                matches = re.findall(pattern, sal_text)\n                if len(matches)>0:\n                    salary_dwp = matches[0][0]\n                else:\n                    salary_dwp = 0\n                start=salary_dwp\n                end=salary_dwp\n            dwp_salary_list[filename]= salary_dwp\n            start=int(start.split(',')[0].strip()+start.split(',')[1].strip() )\n            end=int(end.split(',')[0].strip()+end.split(',')[1].strip() ) \n            df_salary_dwp=df_salary_dwp.append({'FILE_NAME':filename,'ENTRY_SALARY_START_DWP':start,\n                               'ENTRY_SALARY_END_DWP':end},ignore_index=True)\n\n            \n        else:\n            if filename in gen_salary_list.keys():\n                continue\n            matches = re.findall(pattern+' to '+pattern, sal_text)\n            if len(matches)>0:\n                salary_gen = ' - '.join([x for x in matches[0] if x and not x.endswith(',')])\n                start=matches[0][0]\n                end=matches[0][3]\n            else:\n                matches = re.findall(pattern, sal_text)\n                if len(matches)>0:\n                    salary_gen = matches[0][0]\n                else:\n                    salary_gen = 0\n                start=salary_gen\n                end=salary_gen\n            gen_salary_list[filename]= salary_gen\n            if start!=0:\n                start=int(start.split(',')[0].strip()+start.split(',')[1].strip() )\n                end=int(end.split(',')[0].strip()+end.split(',')[1].strip() ) \n            df_salary_gen=df_salary_gen.append({'FILE_NAME':filename,'ENTRY_SALARY_START_GEN':start,\n                               'ENTRY_SALARY_END_GEN':end},ignore_index=True)\n\n\n        \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.5 Education major\nWe use the library of nltk to process the natural language and create a column of 'EDUCATION_MAJOR' in df_requirements. The main idea is to first create a part of speech tags, and then find Noun/Pronoun tags following the words majoring/major/apprenticeship."},{"metadata":{"_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"def preprocess(txt):\n    txt = nltk.word_tokenize(txt)\n    txt = nltk.pos_tag(txt)\n    return txt","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"def getEducationMajor(row):\n    txt = row['REQUIREMENT_TEXT']\n    txtMajor = ''\n    if 'major in' not in txt.lower() and ' majoring ' not in txt.lower():\n        return txtMajor\n    result = []\n    \n    istart = txt.lower().find(' major in ')\n    if istart!=-1:\n        txt = txt[istart+10:]\n    else:\n        istart = txt.lower().find(' majoring ')\n        if istart==-1:\n            return txtMajor\n        txt = txt[istart+12:]\n    \n    txt = txt.replace(',',' or ').replace(' and/or ',' or ').replace(' a closely related field',' related field')\n    sent = preprocess(txt)\n    pattern = \"\"\"\n            NP: {<DT>? <JJ>* <NN.*>*}\n           BR: {<W.*>|<V.*>} \n        \"\"\"\n    cp = nltk.RegexpParser(pattern)\n    cs = cp.parse(sent)\n    #print(cs)\n    checkNext = 0\n    for subtree in cs.subtrees():\n        if subtree.label()=='NP':\n            result.append(' '.join([w for w, t in subtree.leaves()]))\n            checkNext=1\n        elif checkNext==1 and subtree.label()=='BR':\n            break\n    return '|'.join(result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Add EDUCATION_MAJOR\ndf_requirements['EDUCATION_MAJOR']=df_requirements.apply(getEducationMajor, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_requirements.loc[df_requirements['EDUCATION_MAJOR']!=''].head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"#function to fill majors for apprenticeship programs\ndef getApprenticeshipMajor(row):\n    txt = row['REQUIREMENT_TEXT']\n    txtMajor = row['EDUCATION_MAJOR']\n    if 'apprenticeship' not in txt:\n        return txtMajor\n    if txtMajor != '':\n        return txtMajor\n    result = []\n    \n    istart = txt.lower().find(' apprenticeship program')\n    if istart!=-1:\n        txt = txt[istart+23:]\n    else:\n        istart = txt.lower().find(' apprenticeship ')\n        if istart==-1:\n            return txtMajor\n        txt = txt[istart+15:]\n    \n    txt = txt.replace(',',' or ').replace(' full-time ',' ')\n    sent = preprocess(txt)\n    pattern = \"\"\"\n            NP: {<DT>? <JJ>* <NN>*}\n           BR: {<W.*>|<V.*>} \n        \"\"\"\n    cp = nltk.RegexpParser(pattern)\n    cs = cp.parse(sent)\n    #print(cs)\n    checkNext = 0\n    for subtree in cs.subtrees():\n        if subtree.label()=='NP':\n            result.append(' '.join([w for w, t in subtree.leaves()]))\n            checkNext=1\n        elif checkNext==1 and subtree.label()=='BR':\n            break\n    return '|'.join(result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_requirements['EDUCATION_MAJOR']=df_requirements.apply(getApprenticeshipMajor, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_requirements[(df_requirements['EDUCATION_MAJOR']!='') & (df_requirements['REQUIREMENT_TEXT'].str.contains('apprentice'))].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.6 Duties\nWe extract the duties of the jobs into a dataframe df_duties here. "},{"metadata":{"_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"def getValues(searchText, COL_NAME):\n    data_list = []\n    dataHeadings = [k for k in headingsFrame['Heading'].values if searchText in k.lower()]\n\n    for filename in os.listdir(bulletin_dir):\n        with open(bulletin_dir + \"/\" + filename, 'r', errors='ignore') as f:\n            readNext = 0 \n            datatxt = ''\n            for line in f.readlines():\n                clean_line = line.replace(\"\\n\",\"\").replace(\"\\t\",\"\").replace(\":\",\"\").strip()   \n                if readNext == 0:                         \n                    if clean_line in dataHeadings:\n                        readNext = 1\n                elif readNext == 1:\n                    if clean_line in headingsFrame['Heading'].values:\n                        break\n                    else:\n                        datatxt = datatxt + ' ' + clean_line\n            data_list.append([filename,datatxt.strip()])\n    result = pd.DataFrame(data_list)\n    result.columns = ['FILE_NAME',COL_NAME]\n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Add JOB_DUTIES\ndf_duties = getValues('duties','JOB_DUTIES')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(df_duties['JOB_DUTIES'].loc[df_duties['FILE_NAME'] == 'AIRPORT POLICE SPECIALIST 3236 063017 (2).txt'].values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.7 Title and school\nWe update df_requirements with the information of EXP_POSITION' and 'SCHOOL_TYPE'."},{"metadata":{"_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"#Function to retrieve values that match with pre-defined values \ndef section_value_extractor( document, section, subterms_dict, parsed_items_dict ):\n    retval = OrderedDict()\n    single_section_lines = document.lower()\n    \n    for node_tag, pattern_string in subterms_dict.items():\n        pattern_list = re.split(r\",|:\", pattern_string[0])#.sort(key=len)\n        pattern_list=sorted(pattern_list, key=len, reverse=True)\n        #print (pattern_list)\n        matches=[]\n        for pattern in pattern_list:\n            if pattern.lower() in single_section_lines:\n                matches.append(pattern)\n                single_section_lines = single_section_lines.replace(pattern.lower(),'')\n        #print (matches)\n        if len(matches):\n            info_string = \", \".join(list(matches)) + \" \"\n            retval[node_tag] = info_string\n    return retval","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"#Function to read xml configuration to return json formatted string\ndef read_config( configfile ):\n    root = ET.fromstring(configfile)\n    config = []\n    for child in root:\n        term = OrderedDict()\n        term[\"Term\"] = child.get('name', \"\")\n        for level1 in child:\n            term[\"Method\"] = level1.get('name', \"\")\n            term[\"Section\"] = level1.get('section', \"\")\n            for level2 in level1:\n                term[level2.tag] = term.get(level2.tag, []) + [level2.text]\n\n        config.append(term)\n    json_result = json.dumps(config, indent=4)\n    return config","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"def parse_document(document, config):\n    parsed_items_dict = OrderedDict()\n\n    for term in config:\n        term_name = term.get('Term')\n        extraction_method = term.get('Method')\n        extraction_method_ref = globals()[extraction_method]\n        section = term.get(\"Section\")\n        subterms_dict = OrderedDict()\n        \n        for node_tag, pattern_list in list(term.items())[3:]:\n            subterms_dict[node_tag] = pattern_list\n        parsed_items_dict[term_name] = extraction_method_ref(document, section, subterms_dict, parsed_items_dict)\n\n    return parsed_items_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Read job_titles to use them to find patterns in the requirement text to extract POSITIONes\njob_titles = pd.read_csv(additional_data_dir+'/job_titles.csv', header=None)\n\njob_titles = ','.join(job_titles[0])\njob_titles = job_titles.replace('\\'','').replace('&','and')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"configfile = r'''\n<Config-Specifications>\n<Term name=\"Requirements\">\n        <Method name=\"section_value_extractor\" section=\"RequirementSection\">\n            <SchoolType>College or University,High School,Apprenticeship,Certificates</SchoolType>\n            <JobTitle>'''+job_titles+'''</JobTitle>\n        </Method>\n    </Term>\n</Config-Specifications>\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"config = read_config(configfile)\nresult = df_requirements['REQUIREMENT_TEXT'].apply(lambda k: parse_document(k,config))\ni=0\ndf_requirements['EXP_POSITION']=''\ndf_requirements['SCHOOL_TYPE']=''\nfor item in (result.values):\n    for requirement,dic in list(item.items()):        \n        if 'JobTitle' in dic.keys():\n            df_requirements.loc[i,'EXP_POSITION'] = dic['JobTitle']\n        if 'SchoolType' in dic.keys():\n            df_requirements.loc[i,'SCHOOL_TYPE'] = dic['SchoolType']\n    i=i+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Let's check the result for one sample file\ndf_requirements[df_requirements['FILE_NAME']=='SYSTEMS ANALYST 1596 102717.txt'][['FILE_NAME','EXP_POSITION','SCHOOL_TYPE']]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":false},"cell_type":"code","source":"#Combine all the results into one dataframe\nresult = pd.merge(df, df_requirements, how='inner', left_on='FILE_NAME', right_on='FILE_NAME', sort=True)\n\nresult = pd.merge(result, df_salary_dwp, how='left', left_on='FILE_NAME', right_on='FILE_NAME', sort=True)\n\nresult = pd.merge(result, df_salary_gen, how='left', left_on='FILE_NAME', right_on='FILE_NAME', sort=True)\n\nresult = pd.merge(result, df_duties, how='left', left_on='FILE_NAME', right_on='FILE_NAME', sort=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# result.drop(columns=['REQUIREMENT_TEXT'], inplace=True)\nresult[result['FILE_NAME']=='SYSTEMS ANALYST 1596 102717.txt']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.8 Data columns\n\n**Columns Added** \n>        'FILE_NAME', 'OPEN_DATE',  'POSITION', 'JOB_CLASS_NO', 'REQUIREMENT_SET_ID', \n       'REQUIREMENT_SUBSET_ID', 'REQUIREMENT_TEXT', 'EDUCATION_MAJOR','EXP_POSITION', \n       'SCHOOL_TYPE','ENTRY_SALARY_START_DWP','ENTRY_SALARY_END_DWP', \n       'ENTRY_SALARY_START_GEN','ENTRY_SALARY_END_GEN', 'JOB_DUTIES'\n      "},{"metadata":{"trusted":false},"cell_type":"code","source":"df=result\ndf.to_csv('job_bulletins.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Explore the data\n\n## 3.1 Jobs\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"print(\"There are %d text files in bulletin directory and there are %d different jobs available.\" %\n      (len(bulletins),df['POSITION'].nunique()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(8,5))\ntext=''.join(job for job in df['POSITION'])                                ##joining  data to form text\ntext=word_tokenize(text)\njobs=Counter(text)                                                         ##counting number of occurences\njobs_class=[job for job in jobs.most_common(12) if len(job[0])>3]          ##selecting most common words\na,b=map(list, zip(*jobs_class))\nsns.barplot(b,a,palette='rocket')                                           ##creating barplot\nplt.title('Job sectors')\nplt.xlabel(\"count\")\nplt.ylabel('sector')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that service sector dominates in creating opputunities.\n\n**Has job opportunities really increased recently?**\n\n\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"'''Extracting year out of opendate timestamp object and counting\n    the number of each occurence of each year using count_values() '''\ndf['OPEN_DATE']=pd.to_datetime(df['OPEN_DATE'])\ndf['YEAR_OF_OPEN']=[date.year for date in df['OPEN_DATE']]\n\ncount=df['YEAR_OF_OPEN'].value_counts(ascending=True)\nyears=['2018', '2017', '2016', '2015', '2014', '2019', '2012', '2013', '2008', '2006',\n           '2005', '2002', '1999']\nplt.figure(figsize=(7,5))\nplt.plot([z for z in reversed(years)],count.values,color='blue')\n\nplt.title('Oppurtunities over years')\nplt.xlabel('years')\nplt.ylabel('count')\nplt.gca().set_xticklabels([z for z in reversed(years)],rotation='45')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- It is evident from the above graph that job opportunities is constantly increasing after 2013. \n- Job opportunities have never decreased.\n\n**Which month of the year offers most opportunities?**"},{"metadata":{"trusted":false},"cell_type":"code","source":"'''Extracting month out of opendate timestamp object and counting\n    the number of each occurence of each months using count_values() '''\n\n\nplt.figure(figsize=(7,5))\ndf['OPEN_MONTH']=[z.month for z in df['OPEN_DATE']]\ncount=df['OPEN_MONTH'].value_counts(sort=False)\nsns.barplot(y=count.values,x=count.index,palette='rocket')\nplt.gca().set_xticklabels([calendar.month_name[x] for x in count.index],rotation='45')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there is more job opportunities created in the months of **March,October and December**\n\n\n## 3.2 Salary\n**What are the best paid jobs in LA?**"},{"metadata":{"trusted":false},"cell_type":"code","source":"'''finding the most paid 10 jobs at LA'''\ndf.ENTRY_SALARY_START_GEN=df.ENTRY_SALARY_START_GEN.astype(float)\nmost_paid=df[['POSITION','ENTRY_SALARY_START_GEN']].groupby('POSITION',as_index=False).mean()\nmost_paid=most_paid.sort_values(by='ENTRY_SALARY_START_GEN',ascending=False)[:10]\nplt.figure(figsize=(7,5))\nsns.barplot(y=most_paid['POSITION'],x=most_paid['ENTRY_SALARY_START_GEN'],palette='rocket')\nplt.title('Best paid jobs in LA')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**What are the jobs with highest salary deviation?**"},{"metadata":{"trusted":false},"cell_type":"code","source":"''''calculating salary start - salary end '''\ndf['SALARY_DIFF']=abs(df['ENTRY_SALARY_END_GEN']-df['ENTRY_SALARY_START_GEN']).astype(float)\nranges=df[['POSITION','SALARY_DIFF']].groupby('POSITION',as_index=False).mean().sort_values(by='SALARY_DIFF',ascending=False)[:10]\nplt.figure(figsize=(7,5))\nsns.barplot(y=ranges['POSITION'],x=ranges['SALARY_DIFF'],palette='RdBu')   ##plotting\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.3 Requirements\n### 3.3.1 Word cloud"},{"metadata":{"trusted":false},"cell_type":"code","source":"def show_wordcloud(data, title = None):\n    \n    \n    '''funtion to produce and display wordcloud\n        taken 2 arguments\n        1.data to produce wordcloud\n        2.title of wordcloud'''\n    \n    \n    wordcloud = WordCloud(\n        background_color='white',\n        stopwords=set(STOPWORDS),\n        max_words=250,\n        max_font_size=40, \n        scale=3,\n        random_state=1 # chosen at random by flipping a coin; it was heads\n    ).generate(str(data))\n\n    fig = plt.figure(1, figsize=(12, 12))\n    plt.axis('off')\n    if title: \n        fig.suptitle(title, fontsize=20)\n        fig.subplots_adjust(top=2.3)\n\n    plt.imshow(wordcloud)\n    plt.show()\nshow_wordcloud(text,'REQUIREMENTS')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.3.2 Most influential words in requirements"},{"metadata":{"trusted":false},"cell_type":"code","source":"req=' '.join(text for text in df['REQUIREMENT_TEXT'])\nlem=WordNetLemmatizer()\ntext=[lem.lemmatize(w) for w in word_tokenize(req)]\nvect=TfidfVectorizer(ngram_range=(1,3),max_features=100)\nvectorized_data=vect.fit_transform(text)\n#id_map=dict((v,k) for k,v in vect.vocabulary_.items())\nvect.vocabulary_.keys()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.3.3 Most common requirements"},{"metadata":{"trusted":false},"cell_type":"code","source":"token=word_tokenize(req)\ncounter=Counter(token)\ncount=[x[0] for x in counter.most_common(40) if len(x[0])>3]\nprint(\"Most common words in Requirement\")\nprint(count)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- It can be observed that companies prefer  \n- **experienced** \n- **educated professionals**  having **degree from an accredicted university**\n- also willing to work **full-time**"},{"metadata":{},"cell_type":"markdown","source":"### 3.3.4 Word 2 Vec and TSNE"},{"metadata":{"trusted":false},"cell_type":"code","source":"def build_corpus(df,col):\n    \n    '''function to build corpus from dataframe'''\n    lem=WordNetLemmatizer()\n    corpus= []\n    for x in df[col]:\n        \n        \n        words=word_tokenize(x)\n        corpus.append([lem.lemmatize(w) for w in words])\n    return corpus\n\ncorpus=build_corpus(df,'REQUIREMENT_TEXT')\nmodel = word2vec.Word2Vec(corpus, size=100, window=20, min_count=30, workers=4)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def tsne_plot(model,title='None'):\n    \"Creates and TSNE model and plots it\"\n    labels = []\n    tokens = []\n\n    for word in model.wv.vocab:\n        tokens.append(model[word])\n        labels.append(word)\n    \n    tsne_model = TSNE(perplexity=80, n_components=2, init='pca', n_iter=2500, random_state=23)\n    new_values = tsne_model.fit_transform(tokens)\n\n    x = []\n    y = []\n    for value in new_values:\n        x.append(value[0])\n        y.append(value[1])\n        \n    plt.figure(figsize=(12, 12)) \n    plt.title(title)\n    for i in range(len(x)):\n        plt.scatter(x[i],y[i])\n        plt.annotate(labels[i],\n                     xy=(x[i], y[i]),\n                     xytext=(5, 2),\n                     textcoords='offset points',\n                     ha='right',\n                     va='bottom')\n    plt.show()\n    \ntsne_plot(model,'Requirements')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.4 Duties\n### 3.4.1 Word cloud"},{"metadata":{"trusted":false},"cell_type":"code","source":"duties= ' '.join(d for d in df['JOB_DUTIES'])\nshow_wordcloud(duties,'Duties')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.4.2 Most influential words in duties"},{"metadata":{"trusted":false},"cell_type":"code","source":"lem=WordNetLemmatizer()\ntext=[lem.lemmatize(w) for w in word_tokenize(duties)]\nvect=TfidfVectorizer(ngram_range=(1,3),max_features=200)\nvectorized_data=vect.fit_transform(text)\n#id_map=dict((v,k) for k,v in vect.vocabulary_.items())\nvect.vocabulary_.keys()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.4.3 Most common words in duties"},{"metadata":{"trusted":false},"cell_type":"code","source":"token=word_tokenize(duties)\ncounter=Counter(token)\ncount=[x[0] for x in counter.most_common(40) if len(x[0])>3]\nprint(\"Most common words in Duties\")\nprint(count)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.4.4 Word 2 Vec and TSNE"},{"metadata":{"trusted":false},"cell_type":"code","source":"corpus=build_corpus(df,'JOB_DUTIES')\nmodel = word2vec.Word2Vec(corpus, size=100, window=20, min_count=40, workers=4)\n\ntsne_plot(model,'Duties')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.5 Gender bias\n\nIn the follow section I am trying to investigate if there is any gender biased term used in **Requirement** and **Duties** section of the job bulletin.   \nFor that I will pos tag all the text data in the requirement field and then,\n- Extract the words having pronoun tag.\n- check if any gender biased terms like he/she is used in the field.\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"def pronoun(data):\n    \n    '''function to tokenize data and perform pos_tagging.Returns tokens having \"PRP\" tag'''\n    \n    prn=[]\n    vrb=[]\n    token=word_tokenize(data)\n    pos=pos_tag(token)\n   \n    vrb=Counter([x[0] for x in pos if x[1]=='PRP'])\n    \n    return vrb\n    \n\n\nreq_prn=pronoun(req)\nduties_prn=pronoun(duties)\nprint('pronouns used in requirement section are')\nprint(req_prn.keys())\nprint('\\npronouns used in duties section are')\nprint(duties_prn.keys())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. Surprisingly, no gender biased or racist pronouns in **Requirement** or **Duties section** can be found\n2. All the pronouns used are neutral."},{"metadata":{},"cell_type":"markdown","source":"# 4. Recommendations \n\n## 4.1  First goal\n\n> identify language that can negatively bias the pool of applicants\n\nWe find there is a lot we can do in the job descriptions to reduce the bias in the applicants pool.\n\n1. The word grade levels are too high. There are not only too many high-syllable words, but also too many words in a sentence.\n - Our suggestion is to use simpler and lower level words in the descriptions.**\n\n\n2. The content might be overly-formal, reducing readability.\n - Our suggestion is to use less formal words in the descriptions so that more people can read and understand the job descriptions and apply afterwards.**\n\n\n3. The length of the postings is generally way too long and exceeds 700 word limit.\n - Our suggestion is to simplify or visually break up the description\n\n\n\n## 4.2 Second goal\n\n> improve the diversity and quality of the applicant pool\n\nWe believe a well standardized job description can significantly improve the diversity and quality of the applicant pool.\n\n1. Standardize the degrees and coursework selections (such as a drop-down) so that they can be more easily mapped\n2. Standardize the education type into a dropdown so that four-year-degree and Bachelor's are synched up. \n    -Some postings state four-year-college but not necessarily a degree. \n3. Standardize the job titles across the different data sources\n    -Ideally, job titles should match in the requirements and across documents and job graphs\n4. Standardize the certification, license, and training course names across descriptions\n \n \n\n\n## 4.3 Third goal\n\n> make it easier to determine which promotions are available to employees in each job class\n\nThe promotions are not shown clearly in the text job postings. We find it in the Job Paths in the format of PDF files. Extracting the tree relationships from the PDF files is extremely difficult. Moreover, it is already clear and easy enough to determine which promotions are available to employees in each job class. So we end up here. \n\n"},{"metadata":{},"cell_type":"markdown","source":"### Acknowledgement \nThanks to Sahil Tyagi's [Kernel](https://www.kaggle.com/tyagit3/starter-text-bulletins-to-dataframe) for instructions of extracting the information from text bulletins and Shahules786's [Kernel](https://www.kaggle.com/shahules/discovering-opportunities-at-la/log#Which-are-the-best-paid-jobs-in-LA?) for the inspiration of the investigated items. "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}