{"nbformat":4,"nbformat_minor":1,"cells":[{"source":"# New York City Taxi Trip Duration\n\n### ---my first Kaggle Kernel\n\nWhen travel, we always want to know the expected arriving time. It is espically true when we take taxi. The time it takes a taxi to arrive a destination can depend on many things, such as distance, local traffic, driver's skills, and etc. In this problem, we are going to explore the data collected from taxis in New York city and estimate the taxi trip duration there.\n\nThe training data has about 1.5 million samples and 11 features collected during the first half year of 2016. Testing data has 0.6 million samples and 9 feature and is from the same period of time. ","cell_type":"markdown","metadata":{"_cell_guid":"85db9bf1-8ac6-4b36-840d-d18e7460cdc8","_uuid":"8d0530bf6e68e4c36b2516639a3f30091feefc42"}},{"execution_count":null,"source":"# load python modules \nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom time import time\nfrom datetime import datetime\nimport pandas as pd\n\nfrom statsmodels.api import OLS as lm\nfrom sklearn.ensemble import RandomForestRegressor as rf\nfrom pandas.tseries.holiday import USFederalHolidayCalendar","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"ddbf3666-5919-4d97-9d2f-64edb27656c7","_uuid":"9bf069a4916b9ca34df76975c193855e4eb14c28","collapsed":true}},{"execution_count":null,"source":"# load data\ntrain = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"f1413d0e-2311-4bee-9bc8-a1633cca0ed7","_uuid":"68e406b07f9074ed749f2249f8c4c443a7224b25","collapsed":true}},{"execution_count":null,"source":"print(\"shape (train): {:d} obs. {:d} features in train\".format(*train.shape))\nprint(\"shape (test): {:d} obs. {:d} features in test\".format(*test.shape))","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"55a43674-10b4-474f-a370-7740f3e61f7b","_uuid":"512a397d72d578e2216e823b8abe691a8571ab75","collapsed":true}},{"execution_count":null,"source":"print(\"\\t\\t\\t\", \"train\\t\\t\\t\",\"test\")\nprint(\"starting time:\", pd.to_datetime(train['pickup_datetime']).min(), \\\n     pd.to_datetime(test['pickup_datetime']).min())\nprint(\"ending time:  \", pd.to_datetime(train['dropoff_datetime']).max())","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"3a8c6014-43f3-4e42-9b68-617bbe77531c","_uuid":"9dd121aaea8ed8b6ebb4d8c48a373612debaedc7","collapsed":true}},{"execution_count":null,"source":"# define some functions that will be used later\n\ndef Manhattan_distance(data, direct):\n    \"\"\" claculate direct distance (optional) and Manhattan distance\n        and add column log10(distance) to data \"\"\"\n    lat = (data['pickup_latitude']+data['dropoff_latitude'])/2\n    dx = (data['dropoff_longitude'] - data['pickup_longitude'])*np.cos(lat)\n    dy = data['dropoff_latitude'] - data['pickup_latitude']\n    theta = np.pi*30/180\n    rotate = np.array([[np.cos(theta), np.sin(theta)],[-np.sin(theta), np.cos(theta)]])\n    loc = np.dot(np.array([dx,dy]).T, rotate)\n    if direct:\n        data['distance'] = np.log10(np.sqrt((loc[:,0])**2 + (loc[:,1])**2)+1e-6)\n    data['Manhattan_dist'] = np.log10(np.abs(loc[:,0]) + np.abs(loc[:,1])+1e-6)\n    \ndef speed(data):\n    \"\"\" calculate average speed and add column log10(speed) to data \"\"\"\n    Manhattan_distance(data, False)\n    data['speed'] = data['Manhattan_dist']-np.log10(data['trip_duration'])\n    \ndef separate_time(data):\n    \"\"\" extract weekday, time from datetime and add corresponding columns to data \"\"\"\n    pickup_time = pd.DatetimeIndex(pd.to_datetime(data[\"pickup_datetime\"]))\n    data['pickup_date'] = pickup_time.weekday\n    data['pickup_hour'] = pickup_time.hour\n    try:\n        data['dropoff_hour'] = pd.DatetimeIndex(pd.to_datetime(data[\"dropoff_datetime\"])).hour\n    except KeyError:\n        pass\n    \ndef remove_exotic(data):\n    \"\"\" remove exotic data according to criteria in the section about outliers \"\"\"\n    Manhattan_distance(data, False)\n    select = (data['passenger_count']>0) & (data[\"Manhattan_dist\"]>-5.9)\n    try:\n        return data[(data['trip_duration']<3600*8) & select].copy()\n    except KeyError:\n        pass\n    return data[select].copy()\n\ndef preprocess(data):\n    \"\"\" remove exotic data, change data format and add new features necessary \"\"\"\n    Manhattan_distance(data, False)\n    data['store_and_fwd_flag'] = np.where(data['store_and_fwd_flag']==\"Y\", 1, 0).ravel()\n    separate_time(data)\n    try:\n        data['log_duration'] = np.log(data['trip_duration']+1)\n        data['speed'] = data['Manhattan_dist']-np.log10(data['trip_duration'])\n    except KeyError:\n        pass","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"492f7335-6e2f-49cb-b830-3d688a599a00","_uuid":"c7e376b0c46e963ea3bd827283ee944656df3804","collapsed":true}},{"source":"## data exploration -- a brief summary of each variable\n\nWe first explore the data provided. To estimate the trip duration, intuitively we need the time and location of pickup and dropoff. The training data provides a variable *trip_duration*, which is exactly *dropoff_datetime* minus *pickup_datetime* in unit of seconds. This information together with *dropoff_datetime* will not be available in the test data. In addition to location and time information, we are given variables vendor id, number of passengers and whether the trip is recorded automatically. Following is a brief summary of the three variables not related with location or time (we will look at them later).\n\n* ***vendor_id*** is a variable that has only two values: 1 and 2. The number of vendor 2 is slightly more than vendor 1.\n* ***passenger_count*** gives the number of passengers during the trip. One-passenger trip gets the most counts. Trips with more passengers happens fewer times. This is consistent with our commen sense. The training and testing data have the same distribution.\n* Most data have ***store_and_fwd_flag*** as N. It may not be a useful variable.\n\nThe codes generate these results are below.","cell_type":"markdown","metadata":{"_cell_guid":"d8aa273c-1c7c-43de-acf4-a43010536d88","_uuid":"187038791f0cd2f3983ac4bc2f0987c206bf8f37"}},{"execution_count":null,"source":"# first look at the training data\ntrain.head()","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"c68e7c68-d83a-4efd-876c-4aaa82df866c","_uuid":"dc38e2e9dab2e61b92c7616f74146e48c6855529","collapsed":true}},{"execution_count":null,"source":"test.head()","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"44a94474-58ea-49f1-973e-9dde9ea44378","_uuid":"f81d8df29d80fbf9a8ba2765365ea088300d4dd3","collapsed":true}},{"execution_count":null,"source":"# check vendor_id\ncounts = pd.DataFrame({\"train\": train['vendor_id'].value_counts(), \\\n                       \"test\": test['vendor_id'].value_counts()})\ncounts.plot.bar(stacked=True)\nplt.title(\"vendor_id\")\nplt.show()","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"3b40acd9-ac91-4d9d-973f-0ebe1604f1fe","_uuid":"d69ffbc6cbf4495c3463b2aad9555719cdb2984d","collapsed":true}},{"execution_count":null,"source":"# check passenger_count\ncounts = pd.DataFrame({\"train\": train['passenger_count'].value_counts(), \\\n                       \"test\": test['passenger_count'].value_counts()})\ncounts.plot.bar(stacked=True)\nfor i in range(counts.shape[0]):\n    plt.text(i, counts.sum(axis=1).iat[i], int(counts.sum(axis=1).iat[i]), \\\n             rotation=0, ha='center', va='bottom')\nplt.title(\"passenger_count\")\nplt.show()","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"fd298cbe-8545-4aa4-8bc0-165e75da0147","_uuid":"a6c233cff8a8df617d4a5e278cf37c56fa8db474","collapsed":true}},{"execution_count":null,"source":"# check store_and_fwd_flag\ncounts = pd.DataFrame({\"train\": train['store_and_fwd_flag'].value_counts(), \\\n                       \"test\": test['store_and_fwd_flag'].value_counts()})\ncounts.plot.bar(stacked=True)\nfor i in range(counts.shape[0]):\n    plt.text(i, counts.sum(axis=1).iat[i], int(counts.sum(axis=1).iat[i]), \\\n             ha='center', va='bottom')\nplt.title(\"store_and_fwd_flag\")\nplt.show()","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"61021697-534d-4fdd-8101-245e7ae25528","_uuid":"453c945f8b9152eefe831cff0fab126b3bb3981e","collapsed":true}},{"source":" ## data exploration -- duration, distance and speed\n\nNow we delve into two important pieces of information: time and location. Intuitively, we would expect travel duration and travel distance are related linearly. One major problem here is that we do not know the actual travel distance from given data. We can only extract the linear distance between the pickup and dropoff locations. One improvement we can do is to use Manhattan distance, which is the distance calculated with the assumption that all the roads form a square lattice structure. Since roads in Manhattan are not pointing North or Ease, we rotate the map by 30 degrees. The rotated map of Manhattan is shown below.","cell_type":"markdown","metadata":{"_cell_guid":"4684c82c-ab1d-4ad8-9d17-75d743941c46","_uuid":"0f56b65305e6ff2724fc474db8993fff84605f2f"}},{"execution_count":null,"source":"plt.figure(figsize=(4,8))\nplotdata = test[(-74.02<test['pickup_longitude']) &  (test['pickup_longitude']<-73.92) & \\\n                 (40.7<test['pickup_latitude']) & (test['pickup_latitude']<40.85)]\nlocy = np.array(plotdata['pickup_latitude'])\nlocx = (np.array(plotdata['pickup_longitude'])+74)*np.cos(locy*np.pi/180)\nloc = np.array([locx, locy]).T\ntheta = np.pi*30/180\nrotate = np.array([[np.cos(theta), np.sin(theta)],[-np.sin(theta), np.cos(theta)]])\nloc_new = np.dot(loc, rotate)\nplt.scatter(loc_new[:,0], loc_new[:,1], s=0.1, c='y')\nplt.xlim([-20.39,-20.34])\nplt.ylim([35.24,35.37])\nplt.xticks([])\nplt.yticks([])\nplt.show()","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"5383e56b-2ef1-4047-b400-13d4878e2726","_uuid":"1a1476072c7bd9e0b05fa7a015454b07287ba2fe","collapsed":true}},{"source":"An intuition about time is that travel speeds depend on when and where we travel. Therefore, we can estimate travel duration by estimating averate travel speed. We use Manhattan distances as true travel distances. On the other hand, after taking logarithm, the relationship between speed, duration and distance become linear. Therefore, as long as we are estimating values after taking lograrithm, we will get the same final results no matter whether we are estimating speed or duration.","cell_type":"markdown","metadata":{"_cell_guid":"271eaa2c-4e3f-46cf-a979-dce3bf40c6df","_uuid":"acad079340156602836e91740049fbb6d3690582"}},{"execution_count":null,"source":"# trip duration distribution\nplt.figure(figsize=(12,4))\nplt.title(\"distribution of log10(trip duration)\")\nplt.hist(np.log10(train['trip_duration']), bins=200)\nplt.show()","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"8ff2bd23-ca09-4916-a2b1-e3a59ed4c1d1","_uuid":"21276208e2c1a228862e4d71604bad3608b824a3","collapsed":true}},{"execution_count":null,"source":"# trip distance distribution\nManhattan_distance(train, True)\nplt.figure(figsize=(12,4))\nplt.title(\"distribution of log10(trip distance)\")\nplt.hist([train['distance']+np.log10(2)/2,train['Manhattan_dist']], bins=100, \\\n         label=['direct*sqrt(2)','Manhattan'])\nplt.legend(loc='best')\nplt.show()","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"f61ff8df-f920-4d55-9358-41d054f1324a","scrolled":true,"_uuid":"b0bf64f8cd92521c2fbd891144e03d50d1566cc0","collapsed":true}},{"source":"We plot the trip distance and trip duration after take logarithm of the values. The distributions are approximately Gaussian. We notice that the distribution of Manhattan distance differs with that of direct distance by a factor of $\\sqrt{2}$. We do not know which distance may give better prediction, but intuitively we expect Manhattan distance be closer to the true distance. We will not make comparison between the two in this report. Only Manhattan distance will be used in the rest part.","cell_type":"markdown","metadata":{"_cell_guid":"53e4a3c0-9a16-4698-936d-e601c826ec3e","_uuid":"b66df4cf45048cb159ed02a129af4debc1a408ea"}},{"execution_count":null,"source":"# trip speed distribution\nspeed(train)\nplt.figure(figsize=(12,4))\nplt.title(\"distribution of log10(trip speed)\")\nplt.hist(train['speed'], bins=300)\nplt.show()","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"4c1dd89c-a622-43fa-b7a5-73bbfd5fd4db","_uuid":"c4e48d23b8f2f53dd353cb7731e0821051818e58","collapsed":true}},{"source":"As mentioned before, we will focus on estimating  the average trip speed in this report. The distribution of logarithm of trip speed is a nice Gaussian distribution but with a small tail on the left. We will come back to this tail later. For now, we want to get some idea of how locations and time are related with this average speed.\n\nWe first plot the scatter plot of speed verses distance. Since we are in log scale, the difference between distance and speed gives duration (in log scale of course). The vertical line on the left is formed by data points with zero true distance. There is another line on the right. It corresponds to largest duration samples. The speed clearly has a upper bound representing the maximun speed possible. There are few extreme points with very high velocity. ","cell_type":"markdown","metadata":{"_cell_guid":"d3620886-94c8-49f3-aba8-f8b951eced31","_uuid":"317a69c01497604f16bc9bb1bf073a05e189528f"}},{"execution_count":null,"source":"train.sample(n=100000).plot.scatter(x='Manhattan_dist', y='speed', alpha=0.1)\nplt.show()","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"082d76b7-00c1-49e8-9a29-f05440a478ec","_uuid":"368cb649d87e126b1f073767f3dfe3cf7889be8f","collapsed":true}},{"source":"Next, we explore the effect of time. We group those trips by pickup and dropoff hours. For simplicity, we now only consider short trips that begin and end within the same hour. We plot the average speed of these short trips in polar axis below. The dot size represent percentage of travellers in that hour of the whole day. As can be seen the number of travellers remains unchanged during most time of the day. However, the average speed of trips plunges at 8 am in the morning and jumps up at 8 pm in the afternoon. ","cell_type":"markdown","metadata":{"_cell_guid":"e4547ce1-f35f-4c34-8d96-e4f0effc47e7","_uuid":"d0495809c34bcdfdcd97c2310002be5d31c45bcd"}},{"execution_count":null,"source":"separate_time(train)\ntemp = train.loc[train['pickup_hour']==train['dropoff_hour'], ['speed','pickup_hour']]\nplt.figure()\nax = plt.subplot(111, projection='polar')\nax.set_rticks([-4.4,-4.2])\nax.set_rlim([-4.6,-4.1])\nax.set_theta_offset(np.pi/2)\nax.set_theta_direction(-1)\nax.set_xticks(np.linspace(0,2*np.pi,6, endpoint=False))\nax.set_xticklabels(np.linspace(0,24,6, endpoint=False))\nx = np.linspace(0,2*np.pi, 25, endpoint=True)[:24]\ns = temp.groupby('pickup_hour').count().values.ravel()/temp.shape[0]*24*50\nax.scatter(x, temp.groupby('pickup_hour').mean().values.ravel(), s=s)\nax.set_title(\"average speed during a day\\n(shape represents counts)\\n\")\nplt.show()","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"1ad424c4-f3a8-4219-9837-754ce59ce1d9","_uuid":"3c8b398acd02c05e550806c35109879820253a01","collapsed":true}},{"source":"Another period in time is week. We plot distributions of trip speed in the following figure. The dotted line is median of all the dates. The short-duration trips and long-duration trips are plotted separately. Notice the sample sizes of the two cases differ by two orders. On Saturdays, the average trips have larger speed in general, but there are a lot of long-duration trips having low speed.","cell_type":"markdown","metadata":{"_cell_guid":"b637e3ba-1ba0-48f8-9239-f8b28a21158f","_uuid":"bb25e43cbea03d5935ac94ad855a4cc135b21b6f"}},{"execution_count":null,"source":"def plot(temp, title):\n    fig = plt.figure(figsize=(8,4))\n    ax = fig.add_axes([0,0,1,1])\n    ax.set_xlim([-0.5,6.5])\n    ax.set_ylim([-4.9,-3.7])\n    ax.set_xticks(np.arange(7))\n    ax.set_ylabel(\"log10(speed)\")\n    ax.set_xticklabels(['Sun','Mon','Tue','Wed','Thu','Fri','Sat'])\n    ax.axhline(temp['speed'].median(), linestyle='--', color='k')\n    ax.plot(np.arange(7), temp.groupby('pickup_date').median(), 'o-', label='Median')\n    ax.plot(np.arange(7), temp.groupby('pickup_date').mean(), 'o-', label='Mean')\n    ax.set_title(title)\n    ax.legend()\n    temp2 = temp[(temp['speed']<-3.7)&(temp['speed']>-4.9)]\n    for i in range(6,-1,-1):\n        ax = fig.add_axes([i/7,0,0.2,1])\n        ax.axis('off')\n        ax.patch.set_alpha(0)\n        ax.set_xlim([0,temp.shape[0]/7.0/10])\n        ax.set_ylim([-4.9,-3.7])\n        ax.hist(temp2.loc[temp2['pickup_date']==i, 'speed'], \\\n                          orientation='horizontal', bins=80, alpha=0.5)\n#         sns.kdeplot(temp.loc[temp['pickup_date']==i,'speed'], \\\n#                     shade=True, legend=False, vertical=True)\n    plt.show()\ntemp = train.loc[train['trip_duration']<3600, ['speed','pickup_date']]\nplot(temp, \"Trips of duration less than 1h (sample size {:d})\".format(temp.shape[0]))\ntemp = train.loc[train['trip_duration']>3600, ['speed','pickup_date']]\nplot(temp, \"Trips of duration more than 1h (sample size {:d})\".format(temp.shape[0]))","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"85a7002a-02c7-47f5-9b1f-2c21c2a0a99d","_uuid":"ab2fcfae78cb8b6ff80dc53eda49e9f10855f77c","collapsed":true}},{"source":"The effect of locations will not be addressed in this report.","cell_type":"markdown","metadata":{"_cell_guid":"ac47b08a-85c1-40ef-bda5-df3f8a0c932c","_uuid":"897075b3785b9277ac2717120791f87ce2769d79"}},{"source":"## data exploration -- outliers\n\nWe get some idea about the data from previous analysis. We notice that there are some data points with values that are against our commen sense. We will talk about these data in detail\n\n* ***passenger_count***: in the histogram of variable passenger_count, there is a category with zero passengers. We remove these data because a trip should not be valid without passengers.\n* ***trip_duration***: in the distribution of variable trip_duration, there is a small peak on the right at around 5, which corresponding to $10^5$ seconds or 27 hours. We do not expect a driver to drive for more than 8 hours, so we will remove data points with trip_duration more than $3600\\times8$ seconds\n* ***Manhattan_distance***: in the distribution of this variable, there is a small peak on the left at -6. These are data with zero distance. We assume these data are entered by accedent and remove them. \n\nWe apply these criteria on training and testing data and output the size of new data","cell_type":"markdown","metadata":{"_cell_guid":"992a21aa-5bf2-4575-93c6-bb31def07c97","_uuid":"3d534f8ec42c24b2b2c2f1ebb00a1a2755eade06"}},{"execution_count":null,"source":"new_train = remove_exotic(train)\nnew_test = remove_exotic(test)\nprint(\"size of new data v.s. original data\")\nprint(\"train: {}/{} \\t {:2f}%\".format(new_train.shape[0], train.shape[0], \\\n                                     100.0*new_train.shape[0]/train.shape[0]))\nprint(\"test:  {}/{} \\t {:2f}%\".format(new_test.shape[0], test.shape[0], \\\n                                     100.0*new_test.shape[0]/test.shape[0]))","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"f3a26fb5-90cc-47c4-adb3-021b76177a0c","_uuid":"0261f6a0e4f48018188af012555fc8ce6e178b45","collapsed":true}},{"source":"## estimation -- linear regression\n\nWe first use a linear model to do the estimation","cell_type":"markdown","metadata":{"_cell_guid":"ae909c74-6d17-4e06-aa2c-b788fe028b99","_uuid":"63b3dadd97dc0db077b04d9404ea6be29e766f7a"}},{"execution_count":null,"source":"preprocess(new_train)\npreprocess(test)","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"31987a30-7aee-4c12-89ff-30c5a6ef7f56","_uuid":"af251388975e26ca3a5fa0cbef885d5413da2661","collapsed":true}},{"execution_count":null,"source":"columns = ['vendor_id', 'passenger_count', 'store_and_fwd_flag', \\\n           'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', \\\n           'Manhattan_dist', 'pickup_date', 'pickup_hour']\nX = new_train[columns]\ny = new_train['log_duration']\nlm_model = lm(y, X)\nlm_result = lm_model.fit()\nlm_result.summary()","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"6c15b87b-9ba2-4b4f-aa38-d1a7f1495915","_uuid":"f53ea3a9fb47cc3d84cc43de7baf98d74d3ad8e4","collapsed":true}},{"source":"The R-square of linear model is very close to one (0.995), indicating a good fit. However, this is mainly because our data size is very large. The p-values of features are almost zero, indicating all features are necessary. The predicted value gives a score of 0.60486 on test data after submition, which is not too bad.","cell_type":"markdown","metadata":{"_cell_guid":"be4e1310-f9fc-4cb6-ace2-076d2607c101","_uuid":"6e945abc629bbedefc1fde6e0ec436e8a3b47d72","collapsed":true}},{"execution_count":null,"source":"# predicted values\nduration = np.exp(lm_result.predict(test[columns]))-1\nduration = np.where(duration<0, 0, duration)","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"6b64d031-54fe-4b11-acaa-0a8423e93059","_uuid":"a62ecf7665af0359c363ec8b4a7bb79cf09537f1","collapsed":true}},{"source":"# estimation -- random forest\n\nTo get the results quickly, we only use 250,000 samples as training data.","cell_type":"markdown","metadata":{"_cell_guid":"864c5522-f8ae-4fb2-82b6-cd0df1f1d18c","_uuid":"59352b7aa88474eacc954b750f3c476464a8aa95","collapsed":true}},{"execution_count":null,"source":"n = X.shape[0]\nshuffle = np.random.permutation(n)\n\nrf_model = rf(n_estimators=300)\nrf_model.fit(X.values[shuffle[:250000]], y.values[shuffle[:250000]])\nfeature_importance = rf_model.feature_importances_","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"275001c4-bdc8-4f8a-994c-8fb989435a11","_uuid":"6adfa14cd500bfb29b0f1963c593311e97d1a097","collapsed":true}},{"execution_count":null,"source":"plt.figure()\npos = np.arange(len(columns))\nplt.barh(pos, feature_importance)\nplt.yticks(pos, columns)\nplt.xlabel(\"feature importance\")\nplt.show()","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"d09e95b0-83b3-4662-a8e5-e9b863a33998","_uuid":"3cdc42d501b07b16410a5a834530a4037ecfce1c","collapsed":true}},{"source":"As expected, the Manhattan_dist have the highest importance value among all the features. We calculate the error using randomly chosen samples from the rest part of training data.","cell_type":"markdown","metadata":{"_cell_guid":"e9d47f49-ac54-40a0-91e1-4cac1e10632f","_uuid":"e02137eb333852a4645631af5c68cd51b0c8ef40"}},{"execution_count":null,"source":"pred = rf_model.predict(X.values[shuffle[100000:]])\nprint(\"Error is:\", np.sqrt(np.mean((y.values[shuffle][100000:]-pred)**2)))","cell_type":"code","outputs":[],"metadata":{"_cell_guid":"978e654c-638f-47b3-81c6-d8d833f2dd41","_uuid":"ab8606ec8ba7e9908529b1678c6ed3d0a5a6e668","collapsed":true}},{"source":"There are many other things we can do for data exploration and to improve estimation. I cannot exhaust all possibilities in this report. Hope this post can guide those not familar with data analysis and inspire those who are.","cell_type":"markdown","metadata":{"_cell_guid":"a085ebd4-84e1-4a6e-9655-747f5f982218","_uuid":"f5a5ac5956df2aacbf0090546705748829a07a25"}},{"source":"","cell_type":"markdown","metadata":{"_cell_guid":"c11b7562-56ff-43f5-ab41-5ac2593bed05","_uuid":"bd3606bea3d2f4bf4ed85c8c35dbfd370e9d995b"}}],"metadata":{"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","name":"python","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.3","mimetype":"text/x-python","file_extension":".py"},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"}}}