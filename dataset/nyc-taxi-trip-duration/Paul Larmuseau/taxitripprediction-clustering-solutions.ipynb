{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"879ce38f-d26c-4a80-84d3-d0173841b597","_execution_state":"busy","_uuid":"5536dbba67695acf82112e993e4bcf9c66343490"},"source":"comparing all clustering, regressing methods: the forecast errors on raw data (withoug logarithm), it's remarkably 50% high... then we refine to find a narrow foracasting error\n"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"f547012d-8676-4efe-84c9-607244d7d7ba","_execution_state":"busy","collapsed":true,"_uuid":"0d89983d2fc791d275c3b77dbf93d1c7576b3448"},"source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn import model_selection as b\nfrom haversine import haversine\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.cluster import KMeans"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"0540bea4-a0d1-46ee-a1fc-b9344a93f7b6","_execution_state":"busy","collapsed":true,"_uuid":"2e21b645a2e2a910beba92bb34b8ae9bbc93643f"},"source":"ds=pd.read_csv('../input/train.csv')[:10000]\ntest=pd.read_csv('../input/test.csv')[:10000]"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"93983c06-a18d-4154-a5ec-b09ac3aff4b1","_execution_state":"busy","collapsed":true,"_uuid":"f7a83e6046b020e5413ba9d62f68f0ea98dfb9aa"},"source":"def dddraw(X_reduced,name):\n    import matplotlib.pyplot as plt\n    from mpl_toolkits.mplot3d import Axes3D\n    # To getter a better understanding of interaction of the dimensions\n    # plot the first three PCA dimensions\n    fig = plt.figure(1, figsize=(8, 6))\n    ax = Axes3D(fig, elev=-150, azim=110)\n    ax.scatter(X_reduced[:, 0], X_reduced[:, 1], X_reduced[:, 2], c=Y,cmap=plt.cm.Paired)\n    titel=\"First three directions of \"+name \n    ax.set_title(titel)\n    ax.set_xlabel(\"1st eigenvector\")\n    ax.w_xaxis.set_ticklabels([])\n    ax.set_ylabel(\"2nd eigenvector\")\n    ax.w_yaxis.set_ticklabels([])\n    ax.set_zlabel(\"3rd eigenvector\")\n    ax.w_zaxis.set_ticklabels([])\n\n    plt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"64d41bd7-eb49-4673-9942-e77e51de23da","_uuid":"a352561b27bb8ff9ccd7e642b9af2aef61f5640d"},"source":"What do we see ?\n---\n\n* PCA shows 2x6 clusters grouped in two clusters, and forecast 7.9% error\n* FastICa shows 2 clusters, didn't converge\n* gauss is not so interesting 10% error\n* KMeans has also that same 2x6 clusters, 8.9% error\n* sparseRP separates in similar 2x6 clustersn, 7.9% error\n* Birch has the same separation as Kmeans, 9.7% error\n* NMF shows one big cloud, and evenhow gets 8.6%error\n\n"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"9dac59b9-bde7-4fa1-8bac-261b1794aa74","_execution_state":"busy","scrolled":false,"_uuid":"0d524c652c7d173aa4ead1490f365711a7fc95fc"},"source":"from sklearn.decomposition import PCA, FastICA,SparsePCA,NMF, LatentDirichletAllocation,FactorAnalysis\nfrom sklearn.random_projection import GaussianRandomProjection,SparseRandomProjection\nfrom sklearn.cluster import KMeans,Birch\nimport statsmodels.formula.api as sm\nfrom scipy import linalg\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import MinMaxScaler,PolynomialFeatures\ndef rmsle(y_predicted, y_real):\n    return np.sqrt(np.mean(np.power(np.log1p(y_predicted)-np.log1p(y_real), 2)))\ndef procenterror(y_predicted, y_real):\n     return ( np.mean(np.abs(y_predicted-y_real) )/ np.mean(y_real) *100 ).round()\n\nn_col=28\nX = ds.drop(['id','pickup_datetime','dropoff_datetime','store_and_fwd_flag','trip_duration'],axis=1) # we only take the first two features.\nle = preprocessing.LabelEncoder()\ndef rmsle(y_predicted, y_real):\n    return np.sqrt(np.mean(np.power(np.log1p(y_predicted)-np.log1p(y_real), 2)))\ndef procenterror(y_predicted, y_real):\n     return np.round( np.mean(np.abs(y_predicted-y_real) )/ np.mean(y_real) *100 ,1)\n\n    \n#le.fit(np.round(ds['trip_duration']/30))\n#print(list(le.classes_))\n#Y=le.transform(np.round(ds['trip_duration']/30))\nY=np.round(np.log(ds['trip_duration'])*25)\nscaler = MinMaxScaler()\nscaler.fit(X)\nX=scaler.transform(X)\npoly = PolynomialFeatures(2)\nX=poly.fit_transform(X)\n\n\nnames = [\n         'PCA',\n         'FastICA',\n         'Gauss',\n         'KMeans',\n         #'SparsePCA',\n         'SparseRP',\n         'Birch',\n         'NMF',    \n       #  'LatentDietrich',    \n        ]\n\nclassifiers = [\n    \n    PCA(n_components=n_col),\n    FastICA(n_components=n_col),\n    GaussianRandomProjection(n_components=3),\n    KMeans(n_clusters=10),\n    #SparsePCA(n_components=n_col),\n    SparseRandomProjection(n_components=n_col, dense_output=True),\n    Birch(branching_factor=10, n_clusters=3, threshold=0.5),\n    NMF(n_components=n_col),    \n  #  LatentDirichletAllocation(n_topics=n_col),\n    \n]\ncorrection= [1,1,0,0,0,0,0,0,0]\n\ntemp=zip(names,classifiers,correction)\nprint(temp)\n\nfor name, clf,correct in temp:\n    Xr=clf.fit_transform(X,Y)\n    dddraw(Xr,name)\n    res = sm.OLS(Y,Xr).fit()\n    #print(res.summary())  # show OLS regression\n    #print(res.predict(Xr).round()+correct)  #show OLS prediction\n    #print('Ypredict',res.predict(Xr).round()+correct)  #show OLS prediction\n    \n    print('Ypredict',res.predict(Xr).round()+correct*Y.mean())  #show OLS prediction\n    print(name,'%error',procenterror(res.predict(Xr)+correct*Y.mean(),Y),'rmsle',rmsle(res.predict(Xr)+correct*Y.mean(),Y)) #\n    \n    \n    \n    "},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"a1a4dd94-a0a4-43d5-9bfc-e3e8fce23607","_execution_state":"busy","collapsed":true,"_uuid":"40aacba6f7c4f0cec06ef6a7b2afd3aa4ccd23fc"},"source":"pl=np.array(ds['pickup_latitude'])\nplo=np.array(ds['pickup_longitude'])\ndl=np.array(ds['dropoff_latitude'])\ndlo=np.array(ds['dropoff_longitude'])\npl=pl.reshape(-1,1)\nplo=plo.reshape(-1,1)\ndl=dl.reshape(-1,1)\ndlo=dlo.reshape(-1,1)\nm = pl.size\nn = 4\nxk = np.empty([m,n], float)\nxk[:,0]=pl[:,0]\nxk[:,1]=plo[:,0]\nxk[:,2]=dl[:,0]\nxk[:,3]=dlo[:,0]"},{"cell_type":"markdown","metadata":{"_cell_guid":"90f39ac8-d1a8-467e-8574-9bc6e74555b3","_uuid":"108159c205db8ef4318a03b48056c03ae41bc946"},"source":"Lets cluster in 24 groups \n---\neven though there are 12 groups visible in the clusters it gives some redundancy. And we try to find a statistical tripduration/standardevation, and efficiency clustering\nIts simply a grouping of start-stop places and helps to forecast the typical speed for such a distance\n\n"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"547d1405-f144-42cf-aa66-14e45097ce6f","collapsed":true,"_uuid":"e5ea8085f6ed790f413fd0b8961420258b443a2d"},"source":"from geopy.distance import vincenty\nds['geo_distance']=ds.apply(lambda x: vincenty((x['pickup_latitude'] ,x['pickup_longitude']),(x['dropoff_latitude'], x['dropoff_longitude'])).miles,axis=1)\ntest['geo_distance']=test.apply(lambda x: vincenty((x['pickup_latitude'] ,x['pickup_longitude']),(x['dropoff_latitude'], x['dropoff_longitude'])).miles,axis=1)\nds['time_per_km']=np.log(ds['trip_duration']/(ds['geo_distance']/60+0.0001)+1)"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"6ce3afdb-1e83-4bcc-b0b0-50eb7fd8a361","collapsed":true,"_uuid":"59e1463c9d0bd506357dfa32c40c4661f1e79a12"},"source":"kmeans = KMeans(n_clusters=48, random_state=0).fit(xk)\nds['pickup_id']=kmeans.labels_"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"cdbe446f-fd80-4d1a-bc6c-724b6025267c","_execution_state":"busy","_uuid":"d45542adb07d46d29ad804a99b265d5d5dfa1773"},"source":"def clust(x):\n    kl=0\n    if x<0.3:\n        kl=1\n    if x>0.29 and x<0.6:\n        kl=2\n    if x>0.59:\n        kl=4\n    return kl\nds['log_trip_duration']=np.log(ds['trip_duration'])\nnew_col= ds[['time_per_km','log_trip_duration','pickup_id']].groupby('pickup_id').describe().fillna(method='bfill')\nnew_col.columns=['countt','meant','stdt','mint','p25t','p50t','p75t','maxt','countgt','meangt','stdgt','mingt','p25gt','p50gt','p75gt','maxgt']\nnew_col['efft']=new_col['stdt']/new_col['meant']\nnew_col['eff2t']=new_col['efft']*new_col['stdt']\nnew_col['clustt']=new_col['eff2t'].map(clust)\nprint(new_col.head())\nds=pd.merge(ds,new_col, how='outer', left_on='pickup_id',suffixes=('', '_c'), right_index=True)"},{"cell_type":"markdown","metadata":{"_cell_guid":"47976693-e717-44eb-bcd3-a3c720795e21","_uuid":"2072f716ef7fd5461b4680d2ad874b91fbd6c883"},"source":"HourofWeek\n----\nAnother grouping statistics. based upon the dayofweek and hour of day combination. There are 168hours per week. And each hour has his average speed and efficiency properties\n"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"9dc99c8e-897a-46d3-a39f-210e5f24ee48","_execution_state":"busy","_uuid":"14c939f60686cf787dea1e264881d2c65a29e971"},"source":"ds['pickup_datetime']=pd.to_datetime(ds['pickup_datetime'])\ntest['pickup_datetime']=pd.to_datetime(test['pickup_datetime'])\n\nds['pickup_weekday']=ds['pickup_datetime'].dt.weekday\nds['pickup_hour']=ds['pickup_datetime'].dt.hour\nds['pickup_month']=ds['pickup_datetime'].dt.month\nds['pickup_weekhour']=ds['pickup_weekday']*24+ds['pickup_hour']\n\nnew_col= ds[['log_trip_duration','time_per_km','pickup_weekhour']].groupby('pickup_weekhour').describe().fillna(method='bfill')\nnew_col.columns=['countt','meant','stdt','mint','p25t','p50t','p75t','maxt','countgt','meangt','stdgt','mingt','p25gt','p50gt','p75gt','maxgt']\nnew_col['efft']=new_col['stdt']/new_col['meant']\nnew_col['eff2t']=new_col['efft']*new_col['stdt']\nnew_col['clustt']=new_col['eff2t'].map(clust)\nprint(new_col.head())\nds=pd.merge(ds,new_col, how='outer', left_on='pickup_weekhour',suffixes=('', '_wh'), right_index=True)"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"ccef08e0-072f-4cf7-8ef4-f9058cb912a7","_execution_state":"busy","collapsed":true,"_uuid":"217de66e63c8946cbfeafd94f3e2c213cb31413a"},"source":"#ds['distance']=ds.apply(lambda x: haversine((x['pickup_latitude'] ,x['pickup_longitude']),(x['dropoff_latitude'], x['dropoff_longitude']),miles=True),axis=1)\n#test['distance']=test.apply(lambda x: haversine((x['pickup_latitude'] ,x['pickup_longitude']),(x['dropoff_latitude'], x['dropoff_longitude']),miles=True),axis=1)"},{"cell_type":"markdown","metadata":{"_cell_guid":"0007439b-3fd3-452e-8512-8a2e6d99aeeb","_uuid":"3d82462f183d3017a4ea2fed377f2b8ab9354ecc"},"source":"The distance and the average time per distance\n---"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"d8a63cbd-c2eb-4119-bc81-4818af861157","_execution_state":"busy","collapsed":true,"scrolled":true,"_uuid":"7387974e6332613c42a5252b64b35d1d6cb001d7"},"source":"loc_df = pd.DataFrame()\nloc_df['longitude'] = list(ds.pickup_longitude) + list(ds.dropoff_longitude)\nloc_df['latitude'] = list(ds.pickup_latitude) + list(ds.dropoff_latitude)\nkmeans = KMeans(n_clusters=24, random_state=2, n_init = 10).fit(loc_df)\nloc_df['label'] = kmeans.labels_\nds['pickup_cluster'] = kmeans.predict(ds[['pickup_longitude','pickup_latitude']])\nds['dropoff_cluster'] = kmeans.predict(ds[['dropoff_longitude','dropoff_latitude']])\ntest['pickup_cluster'] = kmeans.predict(test[['pickup_longitude','pickup_latitude']])\ntest['dropoff_cluster'] = kmeans.predict(test[['dropoff_longitude','dropoff_latitude']])"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"9f85482f-f2c4-4db7-a019-b3ce47a49cb9","_execution_state":"busy","collapsed":true,"_uuid":"22c6e8a720ba4f393a1c76887fe8d1a6cd582789"},"source":"test['pickup_weekday']=test['pickup_datetime'].dt.weekday\ntest['pickup_hour']=test['pickup_datetime'].dt.hour\ntest['pickup_month']=test['pickup_datetime'].dt.month\ntest['pickup_weekhour']=test['pickup_weekday']*24+test['pickup_hour']"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"4471d33b-59b5-4dda-a59a-238d6288b03c","_execution_state":"busy","collapsed":true,"_uuid":"d97ebe304cecb90985b84ecd5f3ce287665ec5b8"},"source":"test['isweekend']= test.apply(lambda x : (x['pickup_weekday']==6 | x['pickup_weekday']==5),axis=1)\ntest['isweekend']=test['isweekend'].map({True: 1, False:0})\ntest['store_and_fwd_flag']=test['store_and_fwd_flag'].map({'N': 1, 'Y':0})"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"26bdcb77-2a3b-4610-af79-9de7ad840cb1","_uuid":"0823d58f399c7920af14a383223f67528bbd205a"},"source":"Ypl = np.round(np.log(ds['trip_duration']))\nplt.figure()\nYpl.plot.hist(alpha=0.5)\n\n\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"487a36ed-d94b-4dc9-9ce1-0bc8e9268518","_uuid":"e69e8f56c6c43f436f553a6227ec308a5814070c"},"source":"Clustering again shows smaller error\n----\n* PCA and FastIca improves to 6.6% error\n* Gauss is worse\n* Kmeans, improves to 7%, SparseRP improves minimally\n\n"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"1ce99c62-70c9-49a2-9708-e99b41f04fea","_execution_state":"busy","_uuid":"681882b9372afceace8e098f5ba3ac436652f810"},"source":"from sklearn.decomposition import PCA, FastICA,SparsePCA,NMF, LatentDirichletAllocation,FactorAnalysis\nfrom sklearn.random_projection import GaussianRandomProjection,SparseRandomProjection\nfrom sklearn.cluster import KMeans,Birch\nimport statsmodels.formula.api as sm\nfrom scipy import linalg\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import MinMaxScaler,PolynomialFeatures\nimport matplotlib.pyplot as plt\n\ndef rmsle(y_predicted, y_real):\n    return np.sqrt(np.mean(np.power(np.log1p(y_predicted)-np.log1p(y_real), 2)))\ndef procenterror(y_predicted, y_real):\n     return ( np.mean(np.abs(y_predicted-y_real) )/ np.mean(y_real) *100 ).round()\n\nn_col=28\nX = ds.drop(['id','pickup_datetime','dropoff_datetime','store_and_fwd_flag','trip_duration','log_trip_duration','time_per_km'],axis=1) # we only take the first two features.\nle = preprocessing.LabelEncoder()\ndef rmsle(y_predicted, y_real):\n    return np.sqrt(np.mean(np.power(np.log1p(y_predicted)-np.log1p(y_real), 2)))\ndef procenterror(y_predicted, y_real):\n     return np.round( np.mean(np.abs(y_predicted-y_real) )/ np.mean(y_real) *100 ,1)\n\n    \nle.fit(np.round(np.log(ds['trip_duration'])*10))\n\nY=le.transform(np.round(np.log(ds['trip_duration'])*10))\n\nY=np.round(np.log(ds['trip_duration'])*10)\n               \nscaler = MinMaxScaler()\nscaler.fit(X)\nX=scaler.transform(X)\npoly = PolynomialFeatures(2)\nX=poly.fit_transform(X)\n\n\nnames = [\n         'PCA',\n         #'FastICA',\n         'Gauss',\n         'KMeans',\n          #'SparsePCA',\n         'SparseRP',\n         #'Birch',\n      #   'NMF',    \n      #   'LatentDietrich',    \n        ]\n\nclassifiers = [\n    \n    PCA(n_components=n_col),\n   # FastICA(n_components=n_col),\n    GaussianRandomProjection(n_components=3),\n    KMeans(n_clusters=24),\n #   SparsePCA(n_components=n_col),\n    SparseRandomProjection(n_components=n_col, dense_output=True),\n  #  Birch(branching_factor=10, n_clusters=12, threshold=0.5),\n  #  NMF(n_components=n_col),    \n #   LatentDirichletAllocation(n_topics=n_col),\n    \n]\ncorrection= [1,1,0,0,0,0,0,0,0]\n\ntemp=zip(names,classifiers,correction)\nprint(temp)\n\nfor name, clf,correct in temp:\n    Xr=clf.fit_transform(X,Y)\n    dddraw(Xr,name)\n    res = sm.OLS(Y,Xr).fit()\n    #print(res.summary())  # show OLS regression\n    #print(res.predict(Xr).round()+correct)  #show OLS prediction\n    print('Ypredict',res.predict(Xr).round()+correct)  #show OLS prediction\n\n    print('Ypredict *log_sec',res.predict(Xr).round()+correct*Y.mean())  #show OLS prediction\n    print(name,'%error',procenterror(res.predict(Xr)+correct*Y.mean(),Y),'rmsle',rmsle(res.predict(Xr)+correct*Y.mean(),Y))"},{"cell_type":"markdown","metadata":{"_cell_guid":"d1ee34fe-7b1f-4386-a480-d1389fa2f809","_uuid":"491b357b7420963bbc4973f2b262645bd4656969"},"source":"\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"96b33f20-c0f9-4dad-a90b-6d1eeae6613a","_uuid":"7815531676aa15416e3abb96493dd9ecb93bba11"},"source":"linear models forecast with lowest error for time being\n----\n\n* ElasticNet %error 6.24 rmsle 0.104493207131\n* HuberRegressor %error 6.06 rmsle 0.103178076065\n* Ridge %error 5.91 rmsle 0.0995130160778\n* Lasso %error 6.43 rmsle 0.107492482631\n* LassoCV %error 6.01 rmsle 0.100907758631\n* Lars %error 6.45 rmsle 0.107083059727\n* BayesianRidge %error 5.92 rmsle 0.0996487967579\n* SGDClassifier %error 10.1 rmsle 0.174949522616\n* RidgeClassifier %error 7.13 rmsle 0.123412139794\n* LogisticRegression %error 7.03 rmsle 0.116931520745\n* OrthogonalMatchingPursuit %error 6.31 rmsle 0.104854612413"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"1b79a30c-a4e6-4458-8c27-78621350c40a","_execution_state":"busy","_uuid":"6193cc69a7cdbcf59ddd483dd0075c663fb4ebba"},"source":"from sklearn.linear_model import OrthogonalMatchingPursuit,RANSACRegressor,LogisticRegression,ElasticNetCV,HuberRegressor, Ridge, Lasso,LassoCV,Lars,BayesianRidge,SGDClassifier,LogisticRegressionCV,RidgeClassifier\nfrom sklearn.preprocessing import MinMaxScaler\n\n# import some data to play with\nn_col=50\nX = ds.drop(['id','pickup_datetime','dropoff_datetime','store_and_fwd_flag','time_per_km','log_trip_duration','trip_duration'],axis=1) # we only take the first two features.\nY=np.round(ds['trip_duration']/30)\nY=np.round(np.log(ds['trip_duration'])*10)\n\nscaler = MinMaxScaler()\nscaler.fit(X)\nX=scaler.transform(X)\n\n\ndef rmsle(y_predicted, y_real):\n    return np.sqrt(np.mean(np.power(np.log1p(y_predicted)-np.log1p(y_real), 2)))\ndef procenterror(y_predicted, y_real):\n     return np.round( np.mean(np.abs(y_predicted-y_real) )/ np.mean(y_real) *100 ,2)\n\nnames = [\n         'ElasticNet',\n         'HuberRegressor',\n         'Ridge',\n         'Lasso',\n         'LassoCV',\n         'Lars',\n         'BayesianRidge',\n         'SGDClassifier',\n         'RidgeClassifier',\n         'LogisticRegression',\n         'OrthogonalMatchingPursuit',\n         #'RANSACRegressor',\n         ]\n\nclassifiers = [\n    ElasticNetCV(cv=10, random_state=0),\n    HuberRegressor(fit_intercept=True, alpha=0.0, max_iter=100,epsilon=2.95),\n    Ridge(fit_intercept=True, alpha=0.0, random_state=0, normalize=True),\n    Lasso(alpha=0.05),\n    LassoCV(),\n    Lars(n_nonzero_coefs=10),\n    BayesianRidge(),\n    SGDClassifier(),\n    RidgeClassifier(),\n    LogisticRegression(),\n    OrthogonalMatchingPursuit(),\n    #RANSACRegressor(),\n]\ncorrection= [0,0,0,0,0,0,0,0,0,0,0,0]\n\ntemp=zip(names,classifiers,correction)\nprint(temp)\n\nfor name, clf,correct in temp:\n    regr=clf.fit(X,Y)\n    #print( name,'% errors', abs(regr.predict(X)+correct-Y).sum()/(Y.sum())*100)\n    print(name,'%error',procenterror(regr.predict(X),Y),'rmsle',rmsle(regr.predict(X),Y))\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"23bbb3a6-8d0a-4f60-af86-a1a257dc2641","_uuid":"662d6c9d2a599dd0ab897ded1ba36b3b69a52841"},"source":"XGB superiority \n----\nerror 3.6%"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"d6f8d180-0757-4a73-b504-33653a2eef97","_uuid":"811b7dc2a9e1c36f3b43aa36b409c7c4fa97823e"},"source":"import xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import MiniBatchKMeans\n\n#train1=total[total['split']==0]\n#test1=total[total['split']==1]\nX = ds.drop(['id','pickup_datetime','dropoff_datetime','store_and_fwd_flag','time_per_km','log_trip_duration','trip_duration'],axis=1) # we only take the first two features.\nY=np.round(np.log(ds['trip_duration'])*10)\npoly = PolynomialFeatures(2)\nX=poly.fit_transform(X)\n\nXtr, Xv, ytr, yv = train_test_split(X, Y, test_size=0.2, random_state=2017)\ndtrain = xgb.DMatrix(Xtr, label=ytr)\ndvalid = xgb.DMatrix(Xv, label=yv)\n#dtest = xgb.DMatrix(test1[feature_names].values)\nwatchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n\n# Try different parameters! My favorite is random search :)\nxgb_pars = {'min_child_weight': 100, 'eta': 0.03, 'colsample_bytree': 0.3, 'max_depth': 10,\n            'subsample': 0.8, 'lambda': 1., 'nthread': -1, 'booster' : 'gbtree', 'silent': 1,\n            'eval_metric': 'rmse', 'objective': 'reg:linear'}\n\n# You could try to train with more epoch\nmodel = xgb.train(xgb_pars, dtrain, 2000, watchlist, early_stopping_rounds=25,\n                  maximize=False, verbose_eval=50)"},{"execution_count":null,"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"68eefde2-1050-4b38-af85-f0a167b2e564","_uuid":"a67562584a4346623ae6a84995c06af8c69f922c"},"source":"print('XGB %error',procenterror(model.predict(dtrain),ytr),'rmsle',rmsle(model.predict(dtrain),ytr))\n"}],"nbformat_minor":1,"nbformat":4,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"mimetype":"text/x-python","name":"python","codemirror_mode":{"version":3,"name":"ipython"},"version":"3.6.1","file_extension":".py","pygments_lexer":"ipython3","nbconvert_exporter":"python"}}}