{"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.1","file_extension":".py","pygments_lexer":"ipython3","mimetype":"text/x-python","nbconvert_exporter":"python"},"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"}},"nbformat":4,"cells":[{"source":"#  Be humble, sit down, and checkout the visualizations on folium \n#### Update - 23 Aug 2017 10PM IST \n- LB 0.37590 in first submission\n\n\n\n#### Updated documentation - 22 Aug 2017\n\n\n\n\n\n","metadata":{"_execution_state":"idle","_cell_guid":"7975d39a-fbee-42c9-9947-c3e64bccb5b1","_uuid":"647dd4f5d671ac5a39208df10af0450fa3450c0c"},"execution_count":null,"cell_type":"markdown","outputs":[]},{"source":"#### NOTE- This is my first kaggle kernel, Please do let me know by upvoting if it proves to be of any use to you or if you have any suggestion/feedback, please comment, I will try to include and make it better for learning\n\n# Preamble \nI am always told that there are no good visualization libraries in python and also felt the same way when I saw \"heads and tails\" R notebook with ggplot2 and tidy. A thought and wondered into my mind- is it really true ? are there no fancy visualization libaries in python? So I start exploring them and found that it's absolutely wrong. So **I will try to make this notebook as a visualization tutorial in python** and will be using data from NYC taxi competition to construct the visualizations and I will also be explaining in chronological order the way to interpret shown visualizations and how visualizations can provide beautiful ideas about new features. \n\nThis notebook may also be used by beginers to understand analytical thinking, how each step will be guiding us to next step will be explaining thoroughly. We will have interpretation of the plot right below it and ideas we get by that figure. I will try to make it as comprehensive as possible and will try to cover all the following packages -\n\n| Serial No.  | Package     | Plots used in this kernel                          | Remark         \n| :----------:|:----------: | :-------------------------------------------------:|:-----------------------------:|\n|1| **Matplotlib** |1. vendor_id histogram, 2. store and fwdflag histrogram |Matplotlib is oldest and most widely used python visalization package, its a decade old but stil its the first name come to our mind when it comes to plotting. Many libaries are build on top of it, and uses its functions in the backend. Its style is very simple and that's the reason plotting is fast in this. It is used to create axis and design the layout for plotting using other libraries like seaborn.| \n|2| **Seaborn** |1.Voilin plot (passenger count vs trip duration), 2. Boxplots( Weekday vs trip duration, 3. tsplot (hours, weekday vs avg trip duration), 4. distplots of lat-long, and trip_duration |Seaborn is my favorite plotting library (Not at all a fan of house greyjoy though :P) Plots from this package are soothing to eyes. Its build as a wrapper on matplotlib and many matplotlib's function are also work with it.colors are amazing in this package's plots|\n|3|**Pandas**  | 1. Paraller coordinates (for cluster characteristics) | Pandas also offer many plotting functions and its also a package built on matplotlib, so you need to know matplotlib to twick the defaults of pandas. Its offers Alluvial plots (which are nowhere near what R offers as alluvial plots) which is used in this notebbok to show cluster characteristics.|\n|4|**Bokeh** |1. Time series plot (day of year vs avg trip duration) | Bokeh is one great package which offers interactive plots, you can use bokeh with other libraries like seaborn, data-shadder or holoviews, but bokeh its offers various different kind of plots. zoom, axis and interactive legends makes bokeh different than others|\n|5|**Folium** | 1.pickup locations in manhattan, 2. cluster's location in USA, 3. clusters location in manhattan | This package offers geographical-maps and that to are interactive in nature. This package offers different kind of terrains for maps- stemmer terrain, open street maps to name a few. you can place bubble at the locations, shift the zoom, and scroll the plot left-right up-down and add interactions, for example - cluster plots shown in this notebook offers information about clusters like number of vehicles going out, most frequently visited clusters etc. *kaggle started supporting this package during this competetion only* |\n|6|**Pygmaps** | 1. location visualizations 2. cluster visualizations | Pygmaps is available as archive package and can't even be installed using pip install command, but this package was the predeccesor of gamps package but offers few great interactions which even gmaps does't offer. for example scattering of cluster can be plotting with this one better than with gmaps. This package was way underdeveloped and developed version of it is know as gmaps yet, it was able to generate beautiful vizs. plots made my this package are best viewed in browsers.|\n|7|**Plotly**| 1.bubble plot |This is another great package which offers colorful visualizations, but some of these beautiful plots require to interact with plotly server so you need API key and it will call the API.|\n|8|**Gmaps**|*To be updated*|gmaps provide great features like- route features, and we all are too used to gmaps, so feel like home.|\n|9|**Ggplot2** | 1. Weather plots of NYC for given period | gglots are now available in python as well, and its kind of in developing state and documentaion is less of this package which makes it a little difficult but at the same time it provides very beautiful plots, so there is a tradeoff ;)|\n|10|**Basemaps**| *Will not be added in this kernel*|As ong as you are not developing any maps related library, there is no benefits of using basemaps. They offere many options but using them is difficult, due to lots of arguments, differnet style and less documentaions, and many lines of codes sometimes will be required to plot a map.|\n|11|**No package**| 1. heatmaps of NYC taxi traffic |Instead of depending on data-shadder, I tried plotting the heatmap of traffic data with a row image, you will get to knoe the basics of image processing( reading image, color schemes that's all :P ) and how such basic exercise can result in traffic heatmap|\n\n\nfew of the above packages are not supported by kaggle kernels and you have to download the notebook or fork the notebook to get them working, I will put such plots in either appendix or will explicitly mention it above the celling creating such plots. After going through all these visualizations, use the etracted features to build a XGBoost model to do the prediction. The main motive of this motebook is to explore what is available, and how it can be used to generate visualize the data better.\n\n\n## About Competition \nIn this competition we are asked to build a model to predict trip duration for given pick and drop lat-long. We will keep on analysing the data and extracting features as we go forward with this notebook. Let's start get our hands dirty with data.We will be using following datasets in this competition to generate a model and prociding visuals -\n\n| Serial No.  | Datasets used | Description |\n| :----------:|:----------: |:-----------------------------:|\n|1|NYC taxi train-test| Datasets provided as standard data for this competition.\n|2|NYC OSRM dataset| Dataset contains, fatest route, and second fatest route and path, trip duration information.\n|3|Weather data| Dataset contain, precipitation, snowfall etc for given time duration of data.\n","metadata":{"_execution_state":"idle","_cell_guid":"a27209d6-8bc5-49ea-9189-d782e2b9c4f3","_uuid":"e92fdcb5637c0cc953be51957ed3bfcb2394d3f7"},"execution_count":null,"cell_type":"markdown","outputs":[]},{"source":"##### Importing packages for analysis","metadata":{"_cell_guid":"68911a30-9d36-4e4f-a4ed-560fac85826d","_uuid":"b5f740990545aa8ad4552e4af6b5629a07f87d31"},"execution_count":null,"cell_type":"markdown","outputs":[]},{"source":"import pandas as pd  #pandas for using dataframe and reading csv \nimport numpy as np   #numpy for vector operations and basic maths \n#import simplejson    #getting JSON in simplified format\nimport urllib        #for url stuff\n#import gmaps       #for using google maps to visulalize places on maps\nimport re            #for processing regular expressions\nimport datetime      #for datetime operations\nimport calendar      #for calendar for datetime operations\nimport time          #to get the system time\nimport scipy         #for other dependancies\nfrom sklearn.cluster import KMeans # for doing K-means clustering\nfrom haversine import haversine # for calculating haversine distance\nimport math          #for basic maths operations\nimport seaborn as sns #for making plots\nimport matplotlib.pyplot as plt # for plotting\nimport os  # for os commands\nfrom scipy.misc import imread, imresize, imsave  # for plots ","metadata":{"trusted":false,"_execution_state":"idle","collapsed":true,"_cell_guid":"7796c42c-c271-47af-a17a-492772e34075","_uuid":"f60942eb1d22ed39ab9ccef7e14a3f51d28d8a6d"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"### Reading and checking the head of training data and data from OSRM fastest route dataset","metadata":{"_cell_guid":"06b782a3-cade-47ab-af5d-4026ce11ac1d","_uuid":"10dff8114b6dde38350ed094699ef68f02e96d60"},"execution_count":null,"cell_type":"markdown","outputs":[]},{"source":"s = time.time()\ntrain_fr_1 = pd.read_csv('../input/new-york-city-taxi-with-osrm/fastest_routes_train_part_1.csv')\ntrain_fr_2 = pd.read_csv('../input/new-york-city-taxi-with-osrm/fastest_routes_train_part_2.csv')\ntrain_fr = pd.concat([train_fr_1, train_fr_2])\ntrain_fr_new = train_fr[['id', 'total_distance', 'total_travel_time', 'number_of_steps']]\ntrain_df = pd.read_csv('../input/nyc-taxi-trip-duration/train.csv')\ntrain = pd.merge(train_df, train_fr_new, on = 'id', how = 'left')\ntrain_df = train.copy()\nend = time.time()\nprint(\"Time taken by above cell is {}.\".format((end-s)))\ntrain_df.head()","metadata":{"trusted":false,"_execution_state":"idle","collapsed":true,"_cell_guid":"19a05f83-765f-4f15-afa8-b62a07244754","_uuid":"0ca716bfb4bd2e10cb9891a89d4ad126595da323"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"# checking if Ids are unique, \nstart = time.time()\ntrain_data = train_df.copy()\nstart = time.time()\nprint(\"Number of columns and rows and columns are {} and {} respectively.\".format(train_data.shape[1], train_data.shape[0]))\nif train_data.id.nunique() == train_data.shape[0]:\n    print(\"Train ids are unique\")\nprint(\"Number of Nulls - {}.\".format(train_data.isnull().sum().sum()))\nend = time.time()\nprint(\"Time taken by above cell is {}.\".format(end-start))","metadata":{"trusted":false,"collapsed":true,"_cell_guid":"25133c1e-576a-461a-967c-fd48d31f64c6","_uuid":"559656b78d38f1e137bf236b2565f79de41f575d"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"### Lets visualize the trip duration given using log-scale distplot in sns","metadata":{"_cell_guid":"e2f7093d-9a02-4e8a-81b2-4d112046e1e3","_uuid":"28856c384a3d78bcf06111c03e9b362cc886f059"},"execution_count":null,"cell_type":"markdown","outputs":[]},{"source":"%matplotlib inline\nstart = time.time()\nsns.set(style=\"white\", palette=\"muted\", color_codes=True)\nf, axes = plt.subplots(1, 1, figsize=(11, 7), sharex=True)\nsns.despine(left=True)\nsns.distplot(np.log(train_df['trip_duration'].values+1), axlabel = 'Log(trip_duration)', label = 'log(trip_duration)', bins = 50, color=\"r\")\nplt.setp(axes, yticks=[])\nplt.tight_layout()\nend = time.time()\nprint(\"Time taken by above cell is {}.\".format((end-start)))\nplt.show()","metadata":{"trusted":false,"collapsed":true,"_cell_guid":"5672b967-0e63-4aa4-a498-035d9cb66904","_uuid":"5270bc575832e1ad2d1570341eb270b210e95f43"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"**Findings** - It is clear with the above histrogram and kernel density plot that the trip-durations are like gaussian and few trips have very large duration, like ~350000 seconds which is 100 hours (which is weird, as long as it isn't a inter city taxi ride from NYC to SF or Alaska), while most of the trips are e^4 = 1 minute to e^8 ~ 60 minutes. and probably are taken inside manhattan or in new york only. Lets check the lat long distributions are then use them to have a heatmap kind of view of given lat-longs.","metadata":{"_cell_guid":"75ae720e-8826-430e-9d4d-368724e587ae","_uuid":"d39d7ea5be673d03afd85ff9b597e772439c1898"},"execution_count":null,"cell_type":"markdown","outputs":[]},{"source":"start = time.time()\nsns.set(style=\"white\", palette=\"muted\", color_codes=True)\nf, axes = plt.subplots(2,2,figsize=(10, 10), sharex=False, sharey = False)#\nsns.despine(left=True)\nsns.distplot(train_df['pickup_latitude'].values, label = 'pickup_latitude',color=\"m\",bins = 100, ax=axes[0,0])\nsns.distplot(train_df['pickup_longitude'].values, label = 'pickup_longitude',color=\"m\",bins =100, ax=axes[0,1])\nsns.distplot(train_df['dropoff_latitude'].values, label = 'dropoff_latitude',color=\"m\",bins =100, ax=axes[1, 0])\nsns.distplot(train_df['dropoff_longitude'].values, label = 'dropoff_longitude',color=\"m\",bins =100, ax=axes[1, 1])\nplt.setp(axes, yticks=[])\nplt.tight_layout()\nend = time.time()\nprint(\"Time taken by above cell is {}.\".format((end-start)))\nplt.show()","metadata":{"trusted":false,"collapsed":true,"_cell_guid":"77275f15-384e-42de-a817-3857e5fd9193","_uuid":"c19fb368721a5729135af5338fc836dac453bbc4"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"From the plot above it is clear that pick and drop latitude are centered around 40 to 41, and longitude are situated around -74 ton-73. we are not getting any histogram kind of plotswhen we are plotting lat- long as the distplot frunction of sns is getting affacted by outliers, trips which are very far from each other like lat 32 to lat 44, are taking very long time, and affacted this plot such that it is coming of as a spike. Let's remove those large duration trip by using a cap on lat-long and visulaize the distributions of latitude and longitude given to us.","metadata":{"_cell_guid":"eb9094ae-980e-47d8-a454-1afeaa0db043","_uuid":"8106eaa2e9733d5ab8acc51a16dd47bb359b3c15"},"execution_count":null,"cell_type":"markdown","outputs":[]},{"source":"start = time.time()\ndf = train_df.loc[(train_df.pickup_latitude > 40.6) & (train_df.pickup_latitude < 40.9)]\ndf = df.loc[(df.dropoff_latitude>40.6) & (df.dropoff_latitude < 40.9)]\ndf = df.loc[(df.dropoff_longitude > -74.05) & (df.dropoff_longitude < -73.7)]\ndf = df.loc[(df.pickup_longitude > -74.05) & (df.pickup_longitude < -73.7)]\ntrain_data_new = df.copy()\nsns.set(style=\"white\", palette=\"muted\", color_codes=True)\nf, axes = plt.subplots(2,2,figsize=(12, 12), sharex=False, sharey = False)#\nsns.despine(left=True)\nsns.distplot(train_data_new['pickup_latitude'].values, label = 'pickup_latitude',color=\"m\",bins = 100, ax=axes[0,0])\nsns.distplot(train_data_new['pickup_longitude'].values, label = 'pickup_longitude',color=\"g\",bins =100, ax=axes[0,1])\nsns.distplot(train_data_new['dropoff_latitude'].values, label = 'dropoff_latitude',color=\"m\",bins =100, ax=axes[1, 0])\nsns.distplot(train_data_new['dropoff_longitude'].values, label = 'dropoff_longitude',color=\"g\",bins =100, ax=axes[1, 1])\nplt.setp(axes, yticks=[])\nplt.tight_layout()\nend = time.time()\nprint(\"Time taken by above cell is {}.\".format((end-start)))\nprint(df.shape[0], train_data.shape[0])\nplt.show()","metadata":{"trusted":false,"collapsed":true,"_cell_guid":"baabcfd4-3b28-45e7-9a6f-193614146796","_uuid":"577dde4bbaa8ed22fe1c84bf7713247419bfaa72"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"As we put the following caps on lat-long -\n- latitude should be between 40.6 to 40.9\n- longitude should be between -74.05 to -73.70 \n\nWe get that the distribution spikes becomes as distribution in distplot (distplot is a histrogram plot in seaborn package), we can see that most of the trips are getting concentrated between these lat-long only. Lets plot them on an empty image and check what kind of a city map we are getting as we can't use gmaps and folium on kaggle kernel for visualizations. ","metadata":{"_cell_guid":"6fea7a83-ba17-413d-910d-8b65d5893ea9","_uuid":"f19141c2fa40eb62f8fb1863e3a382eb2e23462d"},"execution_count":null,"cell_type":"markdown","outputs":[]},{"source":"start = time.time()\ntemp = train_data.copy()\ntrain_data['pickup_datetime'] = pd.to_datetime(train_data.pickup_datetime)\ntrain_data.loc[:, 'pick_date'] = train_data['pickup_datetime'].dt.date\ntrain_data.head()\n\nts_v1 = pd.DataFrame(train_data.loc[train_data['vendor_id']==1].groupby('pick_date')['trip_duration'].mean())\nts_v1.reset_index(inplace = True)\nts_v2 = pd.DataFrame(train_data.loc[train_data.vendor_id==2].groupby('pick_date')['trip_duration'].mean())\nts_v2.reset_index(inplace = True)\n# we have two dataframes now, Lets see if there is any anomaly in given data as per trip time is concern\n\nfrom bokeh.palettes import Spectral4\nfrom bokeh.plotting import figure, output_notebook, show\n#from bokeh.sampledata.stocks import AAPL, IBM, MSFT, GOOG\noutput_notebook()\n\np = figure(plot_width=800, plot_height=250, x_axis_type=\"datetime\")\np.title.text = 'Click on legend entries to hide the corresponding lines'\n\nfor data, name, color in zip([ts_v1, ts_v2], [\"vendor 1\", \"vendor 2\"], Spectral4):\n    #df = pd.DataFrame(data)\n    #df['date'] = pd.to_datetime(df['date'])\n    df = data\n    p.line(df['pick_date'], df['trip_duration'], line_width=2, color=color, alpha=0.8, legend=name)\n\np.legend.location = \"top_left\"\np.legend.click_policy=\"hide\"\n#output_file(\"interactive_legend.html\", title=\"interactive_legend.py example\")\nshow(p)\nend = time.time()\ntrain_data = temp\nprint(end - start)","metadata":{"trusted":false,"collapsed":true,"_cell_guid":"2236331d-6e08-4bf0-8af3-630f0efd0731","_uuid":"94b520cc0ef394875bc764990e3d3cda55eaa0fa"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"## Heatmap of co-ordinates\n### Let's do basic image processing here \nwe taken an empty image and make it a color it black so that we can see colors where the lat-longs are falling. to visualize we need to consider each point of this image as a point represented by lat-long, to achieved that we will bring the lat-long to image coordinate range and then take a summary of lat-long and their count, assign different color for different count range. Running next cell will result in beautiful visualization shown below.","metadata":{"_cell_guid":"5cee0fb9-dc39-4d7d-b95a-ac09025086bd","_uuid":"88152d7bac3aa8c7c4a9477f4ae901698aba625d"},"execution_count":null,"cell_type":"markdown","outputs":[]},{"source":"start = time.time()\nrgb = np.zeros((3000, 3500, 3), dtype=np.uint8)\nrgb[..., 0] = 0\nrgb[..., 1] = 0\nrgb[..., 2] = 0\ntrain_data_new['pick_lat_new'] = list(map(int, (train_data_new['pickup_latitude'] - (40.6000))*10000))\ntrain_data_new['drop_lat_new'] = list(map(int, (train_data_new['dropoff_latitude'] - (40.6000))*10000))\ntrain_data_new['pick_lon_new'] = list(map(int, (train_data_new['pickup_longitude'] - (-74.050))*10000))\ntrain_data_new['drop_lon_new'] = list(map(int,(train_data_new['dropoff_longitude'] - (-74.050))*10000))\n\nsummary_plot = pd.DataFrame(train_data_new.groupby(['pick_lat_new', 'pick_lon_new'])['id'].count())\n\nsummary_plot.reset_index(inplace = True)\nsummary_plot.head(120)\nlat_list = summary_plot['pick_lat_new'].unique()\nfor i in lat_list:\n    #print(i)\n    lon_list = summary_plot.loc[summary_plot['pick_lat_new']==i]['pick_lon_new'].tolist()\n    unit = summary_plot.loc[summary_plot['pick_lat_new']==i]['id'].tolist()\n    for j in lon_list:\n        #j = int(j)\n        a = unit[lon_list.index(j)]\n        #print(a)\n        if (a//50) >0:\n            rgb[i][j][0] = 255\n            rgb[i,j, 1] = 255\n            rgb[i,j, 2] = 0\n        elif (a//10)>0:\n            rgb[i,j, 0] = 0\n            rgb[i,j, 1] = 255\n            rgb[i,j, 2] = 0\n        else:\n            rgb[i,j, 0] = 255\n            rgb[i,j, 1] = 0\n            rgb[i,j, 2] = 0\nfig, ax = plt.subplots(nrows=1,ncols=1,figsize=(14,20))\nend = time.time()\nprint(\"Time taken by above cell is {}.\".format((end-start)))\nax.imshow(rgb, cmap = 'hot')\nax.set_axis_off() ","metadata":{"trusted":false,"collapsed":true,"_cell_guid":"06c673ff-7a35-40b8-8ba9-569526ad6bdb","_uuid":"80e4445de060f92f57a7264713157484e9ffe08a"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"From the heatmap kind of image above -\n- Red points signifies that 1-10 trips in the given data have that point as pickup point\n- Green points signifies that more than 10-50 trips in the given data have that point as pickup point \n- Yellow points signifies that more than 50+ trips in the given data have that point as pickup point\n\nClearly the whole manhattan is yellow colored and with few green points as well, that shows that in manhatten most of the\ntrips are getting originated. We have to plot in this fasion as kaggle haven't yet support folium and gmaps. This is basic way in which you can plot large geospatial data in a empty image without being dependent on any package. BUt is you hate image processing, you can use **data shadder**, datashadder is a package which is used to show billions of datapoints on a image, they also uses the similar approach with diferrent color gradient.\nThought I will also show the same plot with a sample data of 1000 trips on pygmaps in next few cell. it will generate a HTML in output and user has to open that HTML in browser.","metadata":{"_cell_guid":"036e9d7b-f888-404b-b672-6971fe6026f4","_uuid":"3d5514e7e0f943b57300e8f51320c35c270ca63a"},"execution_count":null,"cell_type":"markdown","outputs":[]},{"source":"## Let's define few functions to unfold features in the given data \n\n","metadata":{"_cell_guid":"4aa49689-e4ad-442f-86b7-ed9ed715586b","_uuid":"fef671b57eac8bfed5835be27c89e12151398f2d"},"execution_count":null,"cell_type":"markdown","outputs":[]},{"source":"start = time.time()\ndef haversine_(lat1, lng1, lat2, lng2):\n    \"\"\"function to calculate haversine distance between two co-ordinates\"\"\"\n    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n    AVG_EARTH_RADIUS = 6371  # in km\n    lat = lat2 - lat1\n    lng = lng2 - lng1\n    d = np.sin(lat * 0.5) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(lng * 0.5) ** 2\n    h = 2 * AVG_EARTH_RADIUS * np.arcsin(np.sqrt(d))\n    return(h)\n\ndef manhattan_distance_pd(lat1, lng1, lat2, lng2):\n    \"\"\"function to calculate manhatten distance between pick_drop\"\"\"\n    a = haversine_(lat1, lng1, lat1, lng2)\n    b = haversine_(lat1, lng1, lat2, lng1)\n    return a + b\n\nimport math\ndef bearing_array(lat1, lng1, lat2, lng2):\n    \"\"\" function was taken from beluga's notebook as this function works on array\n    while my function used to work on individual elements and was noticably slow\"\"\"\n    AVG_EARTH_RADIUS = 6371  # in km\n    lng_delta_rad = np.radians(lng2 - lng1)\n    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n    y = np.sin(lng_delta_rad) * np.cos(lat2)\n    x = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(lng_delta_rad)\n    return np.degrees(np.arctan2(y, x))\n\nend = time.time()\nprint(\"Time taken by above cell is {}.\".format((end-start)))","metadata":{"trusted":false,"collapsed":true,"_cell_guid":"ff9b2a49-bbab-471a-9a59-acf6a88b8a7f","_uuid":"aebf77a330cee7fca11cf15cedb2dcf6aa61ab2a"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"## Feature extratction -  \n","metadata":{"_cell_guid":"c470141f-5c00-4d8a-92f4-48bb2a0f4ddc","_uuid":"3422dfc6534d02bdee5c5fc6fb35bc37b0442b95"},"execution_count":null,"cell_type":"markdown","outputs":[]},{"source":"start = time.time()\ntrain_data = temp.copy()\ntrain_data['pickup_datetime'] = pd.to_datetime(train_data.pickup_datetime)\ntrain_data.loc[:, 'pick_month'] = train_data['pickup_datetime'].dt.month\ntrain_data.loc[:, 'hour'] = train_data['pickup_datetime'].dt.hour\ntrain_data.loc[:, 'week_of_year'] = train_data['pickup_datetime'].dt.weekofyear\ntrain_data.loc[:, 'day_of_year'] = train_data['pickup_datetime'].dt.dayofyear\ntrain_data.loc[:, 'day_of_week'] = train_data['pickup_datetime'].dt.dayofweek\nend = time.time()\nprint(\"Time taken by above cell is {}.\".format(end-start))","metadata":{"trusted":false,"_execution_state":"idle","collapsed":true,"_cell_guid":"9feeabe5-5c88-4814-b235-a2103471336c","_uuid":"b587165e38bc01930bdea5e85cc511b74026205d"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"### Lets call the above defined functions and extracts the features  -","metadata":{"_cell_guid":"5cf8b243-ad46-417d-8ee6-c95f387f4bd6","_uuid":"f94713af786603a5dac6d59f5ba04d9aaa191cf5"},"execution_count":null,"cell_type":"markdown","outputs":[]},{"source":"start = time.time()\n#train_data = train_df\n\ntrain_data.loc[:,'hvsine_pick_drop'] = haversine_(train_data['pickup_latitude'].values, train_data['pickup_longitude'].values, train_data['dropoff_latitude'].values, train_data['dropoff_longitude'].values)\ntrain_data.loc[:,'manhtn_pick_drop'] = manhattan_distance_pd(train_data['pickup_latitude'].values, train_data['pickup_longitude'].values, train_data['dropoff_latitude'].values, train_data['dropoff_longitude'].values)\ntrain_data.loc[:,'bearing'] = bearing_array(train_data['pickup_latitude'].values, train_data['pickup_longitude'].values, train_data['dropoff_latitude'].values, train_data['dropoff_longitude'].values)\n\nend = time.time()\nprint(\"Time taken by above cell is {}.\".format((end-start)))\ntrain_data.head()","metadata":{"trusted":false,"collapsed":true,"_cell_guid":"5918ba49-9c89-4e30-ad2f-6b27baf0c980","_uuid":"077ee862708e9bcf6005cc007207c7f161c3b623"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"# Exploring new features -\n- Let's check the average time taken by two different vendors vs weekday","metadata":{"_execution_state":"idle","_cell_guid":"35702004-623c-4b24-8164-993966432ce2","_uuid":"b7d31da42b7ea3eddfaf593d1bd80481de783f2d"},"execution_count":null,"cell_type":"markdown","outputs":[]},{"source":"summary_wdays_avg_duration = pd.DataFrame(train_data.groupby(['vendor_id','day_of_week'])['trip_duration'].mean())\nsummary_wdays_avg_duration.reset_index(inplace = True)\n\nsummary_wdays_avg_duration['unit']=1\nsns.set(style=\"white\", palette=\"muted\", color_codes=True)\nsns.set_context(\"poster\")\nsns.tsplot(data=summary_wdays_avg_duration, time=\"day_of_week\", unit = \"unit\", condition=\"vendor_id\", value=\"trip_duration\")\nsns.despine(bottom = False)\nend = time.time()\nprint(end - start)\n","metadata":{"trusted":false,"collapsed":true,"_cell_guid":"d3e606c2-d35a-4d99-ac06-4f3cae5fdf70","_uuid":"41f0f75207b042ba6aa08e5820b772c020602e6a"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"**Findings** - it's clear that the vendor 1 is taking more time than vendor 2 on all the days of the week, we can also subset dataframe based on month and that will also give us the same results. the difference between average tiume taken by vendor 1 is ~250 seconds more than vendor 2. ","metadata":{"_cell_guid":"ec7cb6c5-e3ff-4468-948b-de01c9d5e7ec","_uuid":"85196694fd65ab699f65a394e1c7485dd1802006"},"execution_count":null,"cell_type":"markdown","outputs":[]},{"source":"### Violin Plot - \n- Voilin plot can be made using seaborn package in python and with split\n- here we are using them to check the distributions, and horizontal lines inside them shows the quartiles\n- green one is vendor 1 and red one is vendor 2 and trip_duration is plaotted on log scale","metadata":{"_cell_guid":"c766fca2-8704-48ae-912b-e8b56a2964a9","_uuid":"3bdb7425c64fabca58c3fb141be46645ffab5659"},"execution_count":null,"cell_type":"markdown","outputs":[]},{"source":"import seaborn as sns\nsns.set(style=\"whitegrid\", palette=\"pastel\", color_codes=True)\nsns.set_context(\"poster\")\ntrain_data2 = train_data.copy()\ntrain_data2['trip_duration']= np.log(train_data['trip_duration'])\nsns.violinplot(x=\"passenger_count\", y=\"trip_duration\", hue=\"vendor_id\", data=train_data2, split=True,\n               inner=\"quart\",palette={1: \"g\", 2: \"r\"})\n\nsns.despine(left=True)\nprint(df.shape[0])","metadata":{"trusted":false,"collapsed":true,"_cell_guid":"9b3c7a18-f67c-4b7f-bd6e-ed97aea1201d","_uuid":"9b8e1fd0465066aff04819a5625bd556732c2730"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"**Findings** -\n- There are trips for both the vendor with zeros passangers and few of these trips have negative time as well, I don't understand how can a taxi trip have negative time, possibly they aren't right datapoints to train the model, we would remove them before making model\n- Trips with zero passangers can be trips when taxi is called to a particulat location and the costomer is charged for getting the taxi there, that is one possible explaination.\n- Distributions are similar for both the vendors, but vendor one has more number of larger trips than vendor two for passanger count 2 and 3\n- There are very less number of trips with passanger count 7,8 and 9","metadata":{"_cell_guid":"fce82ff8-844f-44c0-9ee3-099fa8384285","_uuid":"6a833884c13d4cf9c0340357ceb1c7d28fd939b6"},"execution_count":null,"cell_type":"markdown","outputs":[]},{"source":"import seaborn as sns\nsns.set(style=\"ticks\")\nsns.set_context(\"poster\")\nsns.boxplot(x=\"day_of_week\", y=\"trip_duration\", hue=\"vendor_id\", data=train_data, palette=\"PRGn\")\nplt.ylim(0, 6000)\nsns.despine(offset=10, trim=True)\ntrain_data.trip_duration.max()","metadata":{"trusted":false,"collapsed":true,"_cell_guid":"626a0f85-f926-42b9-9eb0-e9e439ddbe0f","_uuid":"e8999d9207ffdd5635ba2b46dc150e44269f2bfb"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"**Findings** -\n- From the boxplot above we can see that 75%ile of avg trip duration on sunday(0) and saturday(6) is less than 2000 seconds. i.e. around 33 minutes\n- Time taken by Monday, Tuesday, Wednesday and Thursday are greater than rest of the days.","metadata":{"_cell_guid":"d63a59ad-5fc6-4488-9730-66a1c7ef6794","_uuid":"c00e51d8c81c31673bdae722798daf9a36c9329f"},"execution_count":null,"cell_type":"markdown","outputs":[]},{"source":"summary_hour_duration = pd.DataFrame(train_data.groupby(['day_of_week','hour'])['trip_duration'].mean())\nsummary_hour_duration.reset_index(inplace = True)\nsummary_hour_duration['unit']=1\nsns.set(style=\"white\", palette=\"muted\", color_codes=False)\nsns.set_context(\"poster\")\nsns.tsplot(data=summary_hour_duration, time=\"hour\", unit = \"unit\", condition=\"day_of_week\", value=\"trip_duration\")\nsns.despine(bottom = False)","metadata":{"trusted":false,"collapsed":true,"_cell_guid":"94b79a99-a20b-4757-9c16-21ac30a4e86e","_uuid":"e5628b1f0960d39e5a9b5c84741d92f07dac3524"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"**findings** - \n- Its clear from the above plot that on day 0, that is sunday and day day 6 that is saturday, the trip duration is very less that all the weekdays in 5AM to 15AM time. \n- See this, on  Saturday around midnight, the rides are taking far more than usual time, this is obvious thaough now varified  using given data","metadata":{"_cell_guid":"f75425a8-2f2d-428c-bc7a-6224bf8f115a","_uuid":"27fb3a8014aca2998ed684a9a6bff4ca4985b40e"},"execution_count":null,"cell_type":"markdown","outputs":[]},{"source":"# Cluster analysis and visualization\n\nAs now Kaggle has folium package installed, so I will be using folium package for visualizations and we will be visualizaing clusters on maps. Folium is map based interactive package and it's open source as well. Its based on leaflet js,it is great to see that finally python docker (kaggle) too have such nice map package available. It is  interactive in nature, balloons on the map can be click and they will show characteristics of that clusters.\n","metadata":{"_cell_guid":"ccd6e1bf-cd1f-4846-b8ac-224399754230","_uuid":"61fcefb7ba1ebab9f81b2577d0f5bb87e2e776ed"},"execution_count":null,"cell_type":"markdown","outputs":[]},{"source":"start = time.time()\ndef assign_cluster(df, k):\n    \"\"\"function to assign clusters \"\"\"\n    df_pick = df[['pickup_longitude','pickup_latitude']]\n    df_drop = df[['dropoff_longitude','dropoff_latitude']]\n    #df = df.dropna()\n    init = np.array([[ -73.98737616,   40.72981533],\n       [-121.93328857,   37.38933945],\n       [ -73.78423222,   40.64711269],\n       [ -73.9546417 ,   40.77377538],\n       [ -66.84140269,   36.64537175],\n       [ -73.87040541,   40.77016484],\n       [ -73.97316185,   40.75814346],\n       [ -73.98861094,   40.7527791 ],\n       [ -72.80966949,   51.88108444],\n       [ -76.99779701,   38.47370625],\n       [ -73.96975298,   40.69089596],\n       [ -74.00816622,   40.71414939],\n       [ -66.97216034,   44.37194443],\n       [ -61.33552933,   37.85105133],\n       [ -73.98001393,   40.7783577 ],\n       [ -72.00626526,   43.20296402],\n       [ -73.07618713,   35.03469086],\n       [ -73.95759366,   40.80316361],\n       [ -79.20167796,   41.04752096],\n       [ -74.00106031,   40.73867723]])\n    k_means_pick = KMeans(n_clusters=k, init=init, n_init=1)\n    k_means_pick.fit(df_pick)\n    clust_pick = k_means_pick.labels_\n    df['label_pick'] = clust_pick.tolist()\n    df['label_drop'] = k_means_pick.predict(df_drop)\n    return df, k_means_pick\n\nend = time.time()\nprint(\"time taken by thie script by now is {}.\".format(end-start))","metadata":{"trusted":false,"collapsed":true,"_cell_guid":"990ec289-afbc-471c-acd2-b8e4a6d81d7d","_uuid":"d87f7f0a84271a6afbdfb1d47a37f48a00ece1d8"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"start = time.time()\ntrain_cl, k_means = assign_cluster(train_data, 20)  # make it 100 when extracting features \ncentroid_pickups = pd.DataFrame(k_means.cluster_centers_, columns = ['centroid_pick_long', 'centroid_pick_lat'])\ncentroid_dropoff = pd.DataFrame(k_means.cluster_centers_, columns = ['centroid_drop_long', 'centroid_drop_lat'])\ncentroid_pickups['label_pick'] = centroid_pickups.index\ncentroid_dropoff['label_drop'] = centroid_dropoff.index\n#centroid_pickups.head()\ntrain_cl = pd.merge(train_cl, centroid_pickups, how='left', on=['label_pick'])\ntrain_cl = pd.merge(train_cl, centroid_dropoff, how='left', on=['label_drop'])\n#train_cl.head()\nend = time.time()\nprint(end - start)\ntrain_cl.head()","metadata":{"trusted":false,"collapsed":true,"_cell_guid":"c50de5dc-7c2f-44da-b7f2-53ae60cd501e","_uuid":"1ebf8e93142edb678f9d83c1a5ea94050d25af36"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"# Cluster related features\n","metadata":{"_cell_guid":"d9e5fc9c-f3b3-4442-9788-8c0c308d36dc","_uuid":"47124b0a1fe31ba1ef9223fd586754cc3732322b"},"execution_count":null,"cell_type":"markdown","outputs":[]},{"source":"start = time.time()\ntrain_cl.loc[:,'hvsine_pick_cent_p'] = haversine_(train_cl['pickup_latitude'].values, train_cl['pickup_longitude'].values, train_cl['centroid_pick_lat'].values, train_cl['centroid_pick_long'].values)\ntrain_cl.loc[:,'hvsine_drop_cent_d'] = haversine_(train_cl['dropoff_latitude'].values, train_cl['dropoff_longitude'].values, train_cl['centroid_drop_lat'].values, train_cl['centroid_drop_long'].values)\ntrain_cl.loc[:,'hvsine_cent_p_cent_d'] = haversine_(train_cl['centroid_pick_lat'].values, train_cl['centroid_pick_long'].values, train_cl['centroid_drop_lat'].values, train_cl['centroid_drop_long'].values)\ntrain_cl.loc[:,'manhtn_pick_cent_p'] = manhattan_distance_pd(train_cl['pickup_latitude'].values, train_cl['pickup_longitude'].values, train_cl['centroid_pick_lat'].values, train_cl['centroid_pick_long'].values)\ntrain_cl.loc[:,'manhtn_drop_cent_d'] = manhattan_distance_pd(train_cl['dropoff_latitude'].values, train_cl['dropoff_longitude'].values, train_cl['centroid_drop_lat'].values, train_cl['centroid_drop_long'].values)\ntrain_cl.loc[:,'manhtn_cent_p_cent_d'] = manhattan_distance_pd(train_cl['centroid_pick_lat'].values, train_cl['centroid_pick_long'].values, train_cl['centroid_drop_lat'].values, train_cl['centroid_drop_long'].values)\n\ntrain_cl.loc[:,'bearing_pick_cent_p'] = bearing_array(train_cl['pickup_latitude'].values, train_cl['pickup_longitude'].values, train_cl['centroid_pick_lat'].values, train_cl['centroid_pick_long'].values)\ntrain_cl.loc[:,'bearing_drop_cent_p'] = bearing_array(train_cl['dropoff_latitude'].values, train_cl['dropoff_longitude'].values, train_cl['centroid_drop_lat'].values, train_cl['centroid_drop_long'].values)\ntrain_cl.loc[:,'bearing_cent_p_cent_d'] = bearing_array(train_cl['centroid_pick_lat'].values, train_cl['centroid_pick_long'].values, train_cl['centroid_drop_lat'].values, train_cl['centroid_drop_long'].values)\ntrain_cl['speed_hvsn'] = train_cl.hvsine_pick_drop/train_cl.total_travel_time\ntrain_cl['speed_manhtn'] = train_cl.manhtn_pick_drop/train_cl.total_travel_time\nend = time.time()\nprint(\"Time Taken by above cell is {}.\".format(end-start))\ntrain_cl.head()","metadata":{"trusted":false,"collapsed":true,"_cell_guid":"f7ee54b2-32fd-47b8-9c05-6a105337eca4","_uuid":"471187d496fbcd59ccd6995bf673ef5932462ac8"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"start = time.time()\ndef cluster_summary(sum_df):\n    \"\"\"function to calculate summary of given list of clusters \"\"\"\n    #agg_func = {'trip_duration':'mean','label_drop':'count','bearing':'mean','id':'count'} # that's how you use agg function with groupby\n    summary_avg_time = pd.DataFrame(sum_df.groupby('label_pick')['trip_duration'].mean())\n    summary_avg_time.reset_index(inplace = True)\n    summary_pref_clus = pd.DataFrame(sum_df.groupby(['label_pick', 'label_drop'])['id'].count())\n    summary_pref_clus = summary_pref_clus.reset_index()\n    summary_pref_clus = summary_pref_clus.loc[summary_pref_clus.groupby('label_pick')['id'].idxmax()]\n    summary =pd.merge(summary_avg_time, summary_pref_clus, how = 'left', on = 'label_pick')\n    summary = summary.rename(columns={'trip_duration':'avg_triptime'})\n    return summary\nend = time.time()\nprint(\"Time Taken by above cell is {}.\".format(end-start))","metadata":{"trusted":false,"collapsed":true,"_cell_guid":"8dff96d6-650d-445d-939c-82526ce16e86","_uuid":"614a3310e314226a073abe9b2c29d92d2a8289cb"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"import folium\ndef show_fmaps(train_data, path=1):\n    \"\"\"function to generate map and add the pick up and drop coordinates\n    1. Path = 1 : Join pickup (blue) and drop(red) using a straight line\n    \"\"\"\n    full_data = train_data\n    summary_full_data = pd.DataFrame(full_data.groupby('label_pick')['id'].count())\n    summary_full_data.reset_index(inplace = True)\n    summary_full_data = summary_full_data.loc[summary_full_data['id']>70000]\n    map_1 = folium.Map(location=[40.767937, -73.982155], zoom_start=10,tiles='Stamen Toner') # manually added centre\n    new_df = train_data.loc[train_data['label_pick'].isin(summary_full_data.label_pick.tolist())].sample(50)\n    new_df.reset_index(inplace = True, drop = True)\n    for i in range(new_df.shape[0]):\n        pick_long = new_df.loc[new_df.index ==i]['pickup_longitude'].values[0]\n        pick_lat = new_df.loc[new_df.index ==i]['pickup_latitude'].values[0]\n        dest_long = new_df.loc[new_df.index ==i]['dropoff_longitude'].values[0]\n        dest_lat = new_df.loc[new_df.index ==i]['dropoff_latitude'].values[0]\n        folium.Marker([pick_lat, pick_long]).add_to(map_1)\n        folium.Marker([dest_lat, dest_long]).add_to(map_1)\n    return map_1","metadata":{"trusted":false,"collapsed":true,"_cell_guid":"e9f43aaf-a841-4461-8243-488ca175f868","_uuid":"5847cae12d6afaa38d696ccf1e5f1f13f784b8e7"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"def clusters_map(clus_data, full_data, tile = 'OpenStreetMap', sig = 0, zoom = 12, circle = 0, radius_ = 30):\n    \"\"\" function to plot clusters on map\"\"\"\n    map_1 = folium.Map(location=[40.767937, -73.982155], zoom_start=zoom,tiles= tile) # 'Mapbox' 'Stamen Toner'\n    summary_full_data = pd.DataFrame(full_data.groupby('label_pick')['id'].count())\n    summary_full_data.reset_index(inplace = True)\n    if sig == 1:\n        summary_full_data = summary_full_data.loc[summary_full_data['id']>70000]\n    sig_cluster = summary_full_data['label_pick'].tolist()\n    clus_summary = cluster_summary(full_data)\n    for i in sig_cluster:\n        pick_long = clus_data.loc[clus_data.index ==i]['centroid_pick_long'].values[0]\n        pick_lat = clus_data.loc[clus_data.index ==i]['centroid_pick_lat'].values[0]\n        clus_no = clus_data.loc[clus_data.index ==i]['label_pick'].values[0]\n        most_visited_clus = clus_summary.loc[clus_summary['label_pick']==i]['label_drop'].values[0]\n        avg_triptime = clus_summary.loc[clus_summary['label_pick']==i]['avg_triptime'].values[0]\n        pop = 'cluster = '+str(clus_no)+' & most visited cluster = ' +str(most_visited_clus) +' & avg triptime from this cluster =' + str(avg_triptime)\n        if circle == 1:\n            folium.CircleMarker(location=[pick_lat, pick_long], radius=radius_,\n                    color='#F08080',\n                    fill_color='#3186cc', popup=pop).add_to(map_1)\n        folium.Marker([pick_lat, pick_long], popup=pop).add_to(map_1)\n    return map_1","metadata":{"trusted":false,"collapsed":true,"_cell_guid":"00920f3e-f8cb-47bd-b643-b95ceb521392","_uuid":"aa7b5c26a499e527b30cc6a8811e2dbda8965cca"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"osm = show_fmaps(train_data, path=1)\nosm","metadata":{"trusted":false,"collapsed":true,"_cell_guid":"cdbafb09-e192-42c8-955b-4a81ded9e733","_uuid":"576da17fcfd4ea175afcc880141c85f9fb4e998c"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"# Findings - \n1. Clusters with more than 70k pickups are taken for plotting thismap, and they are covering the most of the rides, more than ~80%, so then plotting a sample of them on this maps shows that most of therdies are started from manhattan.","metadata":{"_cell_guid":"409e5b18-1452-45dc-b358-c1f99c390428","_uuid":"4dd881f91b4c60ed0001e12410eb1cd4069183ea"},"execution_count":null,"cell_type":"markdown","outputs":[]},{"source":"clus_map = clusters_map(centroid_pickups, train_cl, sig =0, zoom =3.2, circle =1, tile = 'Stamen Terrain')\nclus_map","metadata":{"trusted":false,"collapsed":true,"_cell_guid":"a1095ce4-86ac-4051-a582-49a9cde567a5","_uuid":"bf578923cbf04f12364651cb2ffdae7bcf0b422d"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"# Findings - \n1. One cluster is formed in aclifornia and and one is very far in north, so few people taking rides from california as well, and I guess that's why few rides have very long trip duration.\n2. Few clusters are getting centred in sea, its funny but few people are taking ride on the sea as well. :P ","metadata":{"_cell_guid":"bc2013ea-8ec3-4452-9b39-3933656c0cc2","_uuid":"f42402571dd4401b5f217560a9fbe46b46b5ec4a"},"execution_count":null,"cell_type":"markdown","outputs":[]},{"source":"clus_map_sig = clusters_map(centroid_pickups, train_cl, sig =1, circle =1)\nclus_map_sig","metadata":{"trusted":false,"collapsed":true,"_cell_guid":"afb975f3-13d8-4819-bfd1-c1ad999a4bef","_uuid":"5f728c8d32fd462320991af4eff6ccf0e1bbcbdf"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"**plots are interactive click on balloon as check out the characteristics of each cluster - **\n1. Cluster number \n2. Most frequently visited cluster from clicked cluster\n3. Avg triptime of rides started from this cluster","metadata":{"_cell_guid":"a35fdee4-a6ac-4479-8572-022d993e857f","_uuid":"f82b898c01aa52d6381e56f74dad2e5dc70e6029"},"execution_count":null,"cell_type":"markdown","outputs":[]},{"source":"from pandas.tools.plotting import parallel_coordinates\nparallel_coordinates(train_data.sample(1200)[['vendor_id','day_of_week', 'passenger_count', 'pick_month','label_pick', 'hour']], 'vendor_id', colormap='rainbow')\nplt.show()","metadata":{"trusted":false,"collapsed":true,"_cell_guid":"f52dffdf-821a-4a90-870f-e545f36fc29d","_uuid":"233b6cda4c36326001bef9ecac36d37ffd172046"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"#train_cl.to_csv(\"train_features_md.csv\")\n# Let's make test features as well \ntest_df = pd.read_csv('../input/nyc-taxi-trip-duration/test.csv')\ntest_fr = pd.read_csv('../input/new-york-city-taxi-with-osrm/fastest_routes_test.csv')\ntest_fr_new = test_fr[['id', 'total_distance', 'total_travel_time', 'number_of_steps']]\ntest_df = pd.merge(test_df, test_fr_new, on = 'id', how = 'left')\nend = time.time()\ntest_df.head()","metadata":{"trusted":false,"collapsed":true,"_cell_guid":"f705941a-7634-4595-b177-5887b98b013a","_uuid":"38df164eb72956f090ed2b0a3718acbdc1af9175"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"start = time.time()\ntest_data = test_df.copy()\ntest_data['pickup_datetime'] = pd.to_datetime(test_data.pickup_datetime)\ntest_data.loc[:, 'pick_month'] = test_data['pickup_datetime'].dt.month\ntest_data.loc[:, 'hour'] = test_data['pickup_datetime'].dt.hour\ntest_data.loc[:, 'week_of_year'] = test_data['pickup_datetime'].dt.weekofyear\ntest_data.loc[:, 'day_of_year'] = test_data['pickup_datetime'].dt.dayofyear\ntest_data.loc[:, 'day_of_week'] = test_data['pickup_datetime'].dt.dayofweek\nend = time.time()\nprint(\"Time taken by above cell is {}.\".format(end-start))\ntest_data.head()","metadata":{"trusted":false,"collapsed":true,"_cell_guid":"62f9167e-986a-4073-906f-80f01c17f94a","_uuid":"11884f428ed107ffc87c907cacf393d9b6c32979"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"strat = time.time()\ntest_data.loc[:,'hvsine_pick_drop'] = haversine_(test_data['pickup_latitude'].values, test_data['pickup_longitude'].values, test_data['dropoff_latitude'].values, test_data['dropoff_longitude'].values)\ntest_data.loc[:,'manhtn_pick_drop'] = manhattan_distance_pd(test_data['pickup_latitude'].values, test_data['pickup_longitude'].values, test_data['dropoff_latitude'].values, test_data['dropoff_longitude'].values)\ntest_data.loc[:,'bearing'] = bearing_array(test_data['pickup_latitude'].values, test_data['pickup_longitude'].values, test_data['dropoff_latitude'].values, test_data['dropoff_longitude'].values)\nend = time.time()\nprint(\"Time taken by above cell is {}.\".format(end-strat))\ntest_data.head()","metadata":{"trusted":false,"collapsed":true,"_cell_guid":"85f87cd6-89c4-42b5-9fde-679df43b3db1","_uuid":"925dbf7d02acf7094ad1ff579dc50038a4cbe05f"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"start = time.time()\ntest_data['label_pick'] = k_means.predict(test_data[['pickup_longitude','pickup_latitude']])\ntest_data['label_drop'] = k_means.predict(test_data[['dropoff_longitude','dropoff_latitude']])\ntest_cl = pd.merge(test_data, centroid_pickups, how='left', on=['label_pick'])\ntest_cl = pd.merge(test_cl, centroid_dropoff, how='left', on=['label_drop'])\n#test_cl.head()\nend = time.time()\nprint(end - start)\ntest_cl.head()","metadata":{"trusted":false,"collapsed":true,"_cell_guid":"d35b44de-0e36-4930-b93a-2ca5e93277b7","_uuid":"9220aceed9a2297bb48369781db2c395fcf794c6"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"start = time.time()\ntest_cl.loc[:,'hvsine_pick_cent_p'] = haversine_(test_cl['pickup_latitude'].values, test_cl['pickup_longitude'].values, test_cl['centroid_pick_lat'].values, test_cl['centroid_pick_long'].values)\ntest_cl.loc[:,'hvsine_drop_cent_d'] = haversine_(test_cl['dropoff_latitude'].values, test_cl['dropoff_longitude'].values, test_cl['centroid_drop_lat'].values, test_cl['centroid_drop_long'].values)\ntest_cl.loc[:,'hvsine_cent_p_cent_d'] = haversine_(test_cl['centroid_pick_lat'].values, test_cl['centroid_pick_long'].values, test_cl['centroid_drop_lat'].values, test_cl['centroid_drop_long'].values)\ntest_cl.loc[:,'manhtn_pick_cent_p'] = manhattan_distance_pd(test_cl['pickup_latitude'].values, test_cl['pickup_longitude'].values, test_cl['centroid_pick_lat'].values, test_cl['centroid_pick_long'].values)\ntest_cl.loc[:,'manhtn_drop_cent_d'] = manhattan_distance_pd(test_cl['dropoff_latitude'].values, test_cl['dropoff_longitude'].values, test_cl['centroid_drop_lat'].values, test_cl['centroid_drop_long'].values)\ntest_cl.loc[:,'manhtn_cent_p_cent_d'] = manhattan_distance_pd(test_cl['centroid_pick_lat'].values, test_cl['centroid_pick_long'].values, test_cl['centroid_drop_lat'].values, test_cl['centroid_drop_long'].values)\n\ntest_cl.loc[:,'bearing_pick_cent_p'] = bearing_array(test_cl['pickup_latitude'].values, test_cl['pickup_longitude'].values, test_cl['centroid_pick_lat'].values, test_cl['centroid_pick_long'].values)\ntest_cl.loc[:,'bearing_drop_cent_p'] = bearing_array(test_cl['dropoff_latitude'].values, test_cl['dropoff_longitude'].values, test_cl['centroid_drop_lat'].values, test_cl['centroid_drop_long'].values)\ntest_cl.loc[:,'bearing_cent_p_cent_d'] = bearing_array(test_cl['centroid_pick_lat'].values, test_cl['centroid_pick_long'].values, test_cl['centroid_drop_lat'].values, test_cl['centroid_drop_long'].values)\ntest_cl['speed_hvsn'] = test_cl.hvsine_pick_drop/test_cl.total_travel_time\ntest_cl['speed_manhtn'] = test_cl.manhtn_pick_drop/test_cl.total_travel_time\nend = time.time()\nprint(\"Time Taken by above cell is {}.\".format(end-start))\ntest_cl.head()","metadata":{"trusted":false,"collapsed":true,"_cell_guid":"9ae870fc-1e3e-4376-9852-97d22c0362ec","_uuid":"906d6e0346ae11b3094d528641ffd9c35263459b"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"#test_cl.to_csv('features_test_md.csv')\n# file names of files are - train_features_md.csv, and features_test_md.csv","metadata":{"trusted":false,"collapsed":true,"_cell_guid":"c3b7e072-fbd5-4ae1-8b0f-f49972b19328","_uuid":"8420ff864b283bd747821e16e43022be2a51cc97"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"# XGB Model","metadata":{"_cell_guid":"3736aba0-ee16-45fd-8357-51c31f753f96","_uuid":"833318e61b59683352bee0fcd9d7248b57a26b15"},"execution_count":null,"cell_type":"markdown","outputs":[]},{"source":"import xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import MiniBatchKMeans\nimport warnings","metadata":{"trusted":false,"collapsed":true,"_cell_guid":"7f0b69fa-f279-42fa-ab87-debb8574b2e5","_uuid":"e3f6d5449af413f035f579c06103a58099f03af9"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"# Lets Add PCA features in the model, reference Beluga's PCA\ntrain = train_cl\ntest = test_cl\nstart = time.time()\ncoords = np.vstack((train[['pickup_latitude', 'pickup_longitude']].values,\n                    train[['dropoff_latitude', 'dropoff_longitude']].values,\n                    test[['pickup_latitude', 'pickup_longitude']].values,\n                    test[['dropoff_latitude', 'dropoff_longitude']].values))\n\npca = PCA().fit(coords)\ntrain['pickup_pca0'] = pca.transform(train[['pickup_latitude', 'pickup_longitude']])[:, 0]\ntrain['pickup_pca1'] = pca.transform(train[['pickup_latitude', 'pickup_longitude']])[:, 1]\ntrain['dropoff_pca0'] = pca.transform(train[['dropoff_latitude', 'dropoff_longitude']])[:, 0]\ntrain['dropoff_pca1'] = pca.transform(train[['dropoff_latitude', 'dropoff_longitude']])[:, 1]\ntest['pickup_pca0'] = pca.transform(test[['pickup_latitude', 'pickup_longitude']])[:, 0]\ntest['pickup_pca1'] = pca.transform(test[['pickup_latitude', 'pickup_longitude']])[:, 1]\ntest['dropoff_pca0'] = pca.transform(test[['dropoff_latitude', 'dropoff_longitude']])[:, 0]\ntest['dropoff_pca1'] = pca.transform(test[['dropoff_latitude', 'dropoff_longitude']])[:, 1]\nend = time.time()\nprint(\"Time Taken by above cell is {}.\".format(end - start))","metadata":{"trusted":false,"collapsed":true,"_cell_guid":"561c76bb-88e4-496e-aadc-b7c03984dbb7","_uuid":"14f14b829e307978cb03fab5d00d3717273dfe97"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"train['store_and_fwd_flag_int'] = np.where(train['store_and_fwd_flag']=='N', 0, 1)\ntest['store_and_fwd_flag_int'] = np.where(test['store_and_fwd_flag']=='N', 0, 1)\ntrain.head()","metadata":{"trusted":false,"collapsed":true,"_cell_guid":"11563557-0c76-40b8-a5ee-d94a18067646","_uuid":"11a88ca70b58527394a3caa4e729f048ef33bd5b"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"feature_names = list(train.columns)\nprint(\"Difference of features in train and test are {}\".format(np.setdiff1d(train.columns, test.columns)))\nprint(\"\")\ndo_not_use_for_training = ['id', 'pickup_datetime', 'dropoff_datetime', 'trip_duration', 'store_and_fwd_flag']\nfeature_names = [f for f in train.columns if f not in do_not_use_for_training]\nprint(\"We will be using following features for training {}.\".format(feature_names))\nprint(\"\")\nprint(\"Total number of features are {}.\".format(len(feature_names)))\n","metadata":{"trusted":false,"collapsed":true,"_cell_guid":"1ee798f6-d550-45f5-93c9-c267d5220876","_uuid":"829dc3e69b454619f7a287b0b641170801d0c145"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"y = np.log(train['trip_duration'].values + 1)\n","metadata":{"trusted":false,"collapsed":true,"_cell_guid":"e46d9178-af6e-45aa-810b-ba649eca9b21","_uuid":"f972f3eda87f3676e8f2ebcb88fb6d3a640480cd"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"start = time.time()\nXtr, Xv, ytr, yv = train_test_split(train[feature_names].values, y, test_size=0.2, random_state=1987)\ndtrain = xgb.DMatrix(Xtr, label=ytr)\ndvalid = xgb.DMatrix(Xv, label=yv)\ndtest = xgb.DMatrix(test[feature_names].values)\nwatchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n\n# Try different parameters! My favorite is random search :)\nxgb_pars = {'min_child_weight': 50, 'eta': 0.3, 'colsample_bytree': 0.3, 'max_depth': 10,\n            'subsample': 0.8, 'lambda': 1., 'nthread': -1, 'booster' : 'gbtree', 'silent': 1,\n            'eval_metric': 'rmse', 'objective': 'reg:linear'}\n\n# You could try to train with more epoch\nmodel = xgb.train(xgb_pars, dtrain, 10, watchlist, early_stopping_rounds=50,\n                  maximize=False, verbose_eval=1)\nend = time.time()\nprint(\"Time taken by above cell is {}.\".format(end - start))\nprint('Modeling RMSLE %.5f' % model.best_score)","metadata":{"trusted":false,"collapsed":true,"_cell_guid":"e0da8416-d76f-4dc3-b8da-cbbed6269f72","_uuid":"f678acd8eeb6673edea5ce5a91f6010daf1bd302"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"## As we want to use Google maps for visualization, we use following code from pygmaps v0.1.1","metadata":{"_execution_state":"idle","_cell_guid":"17e518a6-ff0d-4cc1-b781-1aca2242ef74","_uuid":"f25b4a541bd3daed4ef166005969740caaec48f6"},"execution_count":null,"cell_type":"markdown","outputs":[]},{"source":"class maps:\n\n\tdef __init__(self, centerLat, centerLng, zoom ):\n\t\tself.center = (float(centerLat),float(centerLng))\n\t\tself.zoom = int(zoom)\n\t\tself.grids = None\n\t\tself.paths = []\n\t\tself.points = []\n\t\tself.radpoints = []\n\t\tself.gridsetting = None\n\t\tself.coloricon = 'http://chart.apis.google.com/chart?cht=mm&chs=12x16&chco=FFFFFF,XXXXXX,000000&ext=.png'\n\n\tdef setgrids(self,slat,elat,latin,slng,elng,lngin):\n\t\tself.gridsetting = [slat,elat,latin,slng,elng,lngin]\n\n\tdef addpoint(self, lat, lng, color = '#FF0000'):\n\t\tself.points.append((lat,lng,color[1:]))\n\n\t#def addpointcoord(self, coord):\n\t#\tself.points.append((coord[0],coord[1]))\n\n\tdef addradpoint(self, lat,lng,rad,color = '#0000FF'):\n\t\tself.radpoints.append((lat,lng,rad,color))\n\n\tdef addpath(self,path,color = '#FF0000'):\n\t\tpath.append(color)\n\t\tself.paths.append(path)\n\t\n\t#create the html file which inlcude one google map and all points and paths\n\tdef draw(self, htmlfile):\n\t\tf = open(htmlfile,'w')\n\t\tf.write('<html>\\n')\n\t\tf.write('<head>\\n')\n\t\tf.write('<meta name=\"viewport\" content=\"initial-scale=1.0, user-scalable=no\" />\\n')\n\t\tf.write('<meta http-equiv=\"content-type\" content=\"text/html; charset=UTF-8\"/>\\n')\n\t\tf.write('<title>Google Maps - pygmaps </title>\\n')\n\t\tf.write('<script type=\"text/javascript\" src=\"http://maps.google.com/maps/api/js?sensor=false\"></script>\\n')\n\t\tf.write('<script type=\"text/javascript\">\\n')\n\t\tf.write('\\tfunction initialize() {\\n')\n\t\tself.drawmap(f)\n\t\tself.drawgrids(f)\n\t\tself.drawpoints(f)\n\t\tself.drawradpoints(f)\n\t\tself.drawpaths(f,self.paths)\n\t\tf.write('\\t}\\n')\n\t\tf.write('</script>\\n')\n\t\tf.write('</head>\\n')\n\t\tf.write('<body style=\"margin:0px; padding:0px;\" onload=\"initialize()\">\\n')\n\t\tf.write('\\t<div id=\"map_canvas\" style=\"width: 100%; height: 100%;\"></div>\\n')\n\t\tf.write('</body>\\n')\n\t\tf.write('</html>\\n')\n\t\tf.close()\n\n\tdef drawgrids(self, f):\n\t\tif self.gridsetting == None:\n\t\t\treturn\n\t\tslat = self.gridsetting[0]\n\t\telat = self.gridsetting[1]\n\t\tlatin = self.gridsetting[2]\n\t\tslng = self.gridsetting[3]\n\t\telng = self.gridsetting[4]\n\t\tlngin = self.gridsetting[5]\n\t\tself.grids = []\n\n\t\tr = [slat+float(x)*latin for x in range(0, int((elat-slat)/latin))]\n\t\tfor lat in r:\n\t\t\tself.grids.append([(lat+latin/2.0,slng+lngin/2.0),(lat+latin/2.0,elng+lngin/2.0)])\n\n\t\tr = [slng+float(x)*lngin for x in range(0, int((elng-slng)/lngin))]\n\t\tfor lng in r:\n\t\t\tself.grids.append([(slat+latin/2.0,lng+lngin/2.0),(elat+latin/2.0,lng+lngin/2.0)])\n\t\t\n\t\tfor line in self.grids:\n\t\t\tself.drawPolyline(f,line,strokeColor = \"#000000\")\n\tdef drawpoints(self,f):\n\t\tfor point in  self.points:\n\t\t\tself.drawpoint(f,point[0],point[1],point[2])\n\n\tdef drawradpoints(self, f):\n\t\tfor rpoint in self.radpoints:\n\t\t\tpath = self.getcycle(rpoint[0:3])\n\t\t\tself.drawPolygon(f,path,strokeColor = rpoint[3])\n\n\tdef getcycle(self,rpoint):\n\t\tcycle = []\n\t\tlat = rpoint[0]\n\t\tlng = rpoint[1]\n\t\trad = rpoint[2] #unit: meter\n\t\td = (rad/1000.0)/6378.8;\n\t\tlat1 = (math.pi/180.0)* lat\n\t\tlng1 = (math.pi/180.0)* lng\n\n\t\tr = [x*30 for x in range(12)]\n\t\tfor a in r:\n\t\t\ttc = (math.pi/180.0)*a;\n\t\t\ty = math.asin(math.sin(lat1)*math.cos(d)+math.cos(lat1)*math.sin(d)*math.cos(tc))\n\t\t\tdlng = math.atan2(math.sin(tc)*math.sin(d)*math.cos(lat1),math.cos(d)-math.sin(lat1)*math.sin(y))\n\t\t\tx = ((lng1-dlng+math.pi) % (2.0*math.pi)) - math.pi \n\t\t\tcycle.append( ( float(y*(180.0/math.pi)),float(x*(180.0/math.pi)) ) )\n\t\treturn cycle\n\n\tdef drawpaths(self, f, paths):\n\t\tfor path in paths:\n\t\t\t#print path\n\t\t\tself.drawPolyline(f,path[:-1], strokeColor = path[-1])\n\n\t#############################################\n\t# # # # # # Low level Map Drawing # # # # # # \n\t#############################################\n\tdef drawmap(self, f):\n\t\tf.write('\\t\\tvar centerlatlng = new google.maps.LatLng(%f, %f);\\n' % (self.center[0],self.center[1]))\n\t\tf.write('\\t\\tvar myOptions = {\\n')\n\t\tf.write('\\t\\t\\tzoom: %d,\\n' % (self.zoom))\n\t\tf.write('\\t\\t\\tcenter: centerlatlng,\\n')\n\t\tf.write('\\t\\t\\tmapTypeId: google.maps.MapTypeId.ROADMAP\\n')\n\t\tf.write('\\t\\t};\\n')\n\t\tf.write('\\t\\tvar map = new google.maps.Map(document.getElementById(\"map_canvas\"), myOptions);\\n')\n\t\tf.write('\\n')\n\n\n\n\tdef drawpoint(self,f,lat,lon,color):\n\t\tf.write('\\t\\tvar latlng = new google.maps.LatLng(%f, %f);\\n'%(lat,lon))\n\t\tf.write('\\t\\tvar img = new google.maps.MarkerImage(\\'%s\\');\\n' % (self.coloricon.replace('XXXXXX',color)))\n\t\tf.write('\\t\\tvar marker = new google.maps.Marker({\\n')\n\t\tf.write('\\t\\ttitle: \"no implimentation\",\\n')\n\t\tf.write('\\t\\ticon: img,\\n')\n\t\tf.write('\\t\\tposition: latlng\\n')\n\t\tf.write('\\t\\t});\\n')\n\t\tf.write('\\t\\tmarker.setMap(map);\\n')\n\t\tf.write('\\n')\n\t\t\n\tdef drawPolyline(self,f,path,\\\n\t\t\tclickable = False, \\\n\t\t\tgeodesic = True,\\\n\t\t\tstrokeColor = \"#FF0000\",\\\n\t\t\tstrokeOpacity = 1.0,\\\n\t\t\tstrokeWeight = 2\n\t\t\t):\n\t\tf.write('var PolylineCoordinates = [\\n')\n\t\tfor coordinate in path:\n\t\t\tf.write('new google.maps.LatLng(%f, %f),\\n' % (coordinate[0],coordinate[1]))\n\t\tf.write('];\\n')\n\t\tf.write('\\n')\n\n\t\tf.write('var Path = new google.maps.Polyline({\\n')\n\t\tf.write('clickable: %s,\\n' % (str(clickable).lower()))\n\t\tf.write('geodesic: %s,\\n' % (str(geodesic).lower()))\n\t\tf.write('path: PolylineCoordinates,\\n')\n\t\tf.write('strokeColor: \"%s\",\\n' %(strokeColor))\n\t\tf.write('strokeOpacity: %f,\\n' % (strokeOpacity))\n\t\tf.write('strokeWeight: %d\\n' % (strokeWeight))\n\t\tf.write('});\\n')\n\t\tf.write('\\n')\n\t\tf.write('Path.setMap(map);\\n')\n\t\tf.write('\\n\\n')\n\n\tdef drawPolygon(self,f,path,\\\n\t\t\tclickable = False, \\\n\t\t\tgeodesic = True,\\\n\t\t\tfillColor = \"#000000\",\\\n\t\t\tfillOpacity = 0.0,\\\n\t\t\tstrokeColor = \"#FF0000\",\\\n\t\t\tstrokeOpacity = 1.0,\\\n\t\t\tstrokeWeight = 1\n\t\t\t):\n\t\tf.write('var coords = [\\n')\n\t\tfor coordinate in path:\n\t\t\tf.write('new google.maps.LatLng(%f, %f),\\n' % (coordinate[0],coordinate[1]))\n\t\tf.write('];\\n')\n\t\tf.write('\\n')\n\n\t\tf.write('var polygon = new google.maps.Polygon({\\n')\n\t\tf.write('clickable: %s,\\n' % (str(clickable).lower()))\n\t\tf.write('geodesic: %s,\\n' % (str(geodesic).lower()))\n\t\tf.write('fillColor: \"%s\",\\n' %(fillColor))\n\t\tf.write('fillOpacity: %f,\\n' % (fillOpacity))\n\t\tf.write('paths: coords,\\n')\n\t\tf.write('strokeColor: \"%s\",\\n' %(strokeColor))\n\t\tf.write('strokeOpacity: %f,\\n' % (strokeOpacity))\n\t\tf.write('strokeWeight: %d\\n' % (strokeWeight))\n\t\tf.write('});\\n')\n\t\tf.write('\\n')\n\t\tf.write('polygon.setMap(map);\\n')\n\t\tf.write('\\n\\n')\n\n\n","metadata":{"trusted":false,"_execution_state":"idle","collapsed":true,"_cell_guid":"7b5ff73f-7fba-456c-975e-d0dce3e687db","_uuid":"20291f9da059d147a6faee4b5740014c359c2071"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"def show_gmaps(train_data, path):\n    \"\"\"function to generate map and add the pick up and drop coordinates\n    1. Path = 1 : Join pickup (blue) and drop(red) using a straight line\n    \"\"\"\n    mymap = maps(40.767937, -73.982155, 12) # manually added centre\n    for i in range(train_data.shape[0]):\n        pick_long = train_data.loc[train_data.index ==i]['pickup_longitude'].values[0]\n        pick_lat = train_data.loc[train_data.index ==i]['pickup_latitude'].values[0]\n        dest_long = train_data.loc[train_data.index ==i]['dropoff_longitude'].values[0]\n        dest_lat = train_data.loc[train_data.index ==i]['dropoff_latitude'].values[0]\n        mymap.addpoint(pick_lat, pick_long, \"#FF0000\")\n        #mymap.addradpoint(dest_lat, dest_long, 50, \"#00FF00\")\n        #mymap.getcycle([dest_lat, dest_long, 0.01])\n        mymap.addpoint(dest_lat, dest_long, \"#0000FF\")\n        #if path == 1:\n        path = [(pick_lat, pick_long),(dest_lat, dest_long)]\n        mymap.addpath(path,\"#000000\")\n        if i%1000 == 0:\n            print(i, dest_lat, dest_long) #time.time(),\n    mymap.draw('./Google_map_showing_trips.txt')\n    return \n\nend = time.time()\nprint(\"time taken by thie script by now is {}.\".format(end-start))","metadata":{"trusted":false,"collapsed":true,"_cell_guid":"415b91f9-ff1c-4094-aa57-392038d493fb","_uuid":"9264b035ffef21153cb4f656609262160aae5992"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"# Lets visulalize a sample of 200 trips from all data\ntrain_sample_vis = train_data.loc[np.random.randint(1458644, size =200)]\ntrain_sample_vis.reset_index(drop = True, inplace = True)\nshow_gmaps(train_sample_vis, 1)\n#print( os.listdir('../input/'))\n\nfrom IPython.display import IFrame, HTML, display\nIFrame(HTML('../kaggle/working/Google_map_showing_trips.txt'), width=1000, height=500)\n\n\nend = time.time()\nprint(\"time taken by thie script by now is {}.\".format(end-start))\n\n\n","metadata":{"trusted":false,"_execution_state":"idle","collapsed":true,"_cell_guid":"429e5681-e352-4e1a-8a5f-b5b214280fd9","_uuid":"a6b8188619556c7e98557fa49d4b4497a5ed648d"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"From Above visualization on google maps, its clear that most of the cabs rides are situated in manhatten and few in the lesf of manhattan and few on the right. Few rides' pick drop are very far from manhattan, but there are suchs rides, like going to Airport JFK, and few rides locations are in SF, california. So, which brings us to idea of clsuteing them and then analysing where the clusters are falling. Expected is that most of the clusters will fall in manhattan and few will fall nearby manhattan.\n\nExcept it clustering is very advisible as total number of queries in train data is ~1.4M, which is very larger number , and using google map's distance matrix the pickup address, drop address, and shortest path distance for cars, and avg time taken from pickup tpo drop is not possible as free query limit is 2500 queries per day. so, if we do clustering and we reduce the number of queries, we ca use distance matrix's data for going from centroid of one cluster to another and then using normal, haversine or manhattan distance for going from centroid of cluster to destination. \n\n","metadata":{"_cell_guid":"93094f79-1c1f-4457-a106-169d18a964b4","_uuid":"f4732973690f9069b9d1f8120cd8130aa9ed6c60"},"execution_count":null,"cell_type":"markdown","outputs":[]},{"source":"df_cluster4 = train_cl","metadata":{"trusted":false,"_execution_state":"idle","collapsed":true,"_cell_guid":"53588cc4-f7c6-4d4d-b693-32f17d007573","_uuid":"38ed48b9b25a4d37654a982ca23b43b7c434b159"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"start = time.time()\ncentroid_drops = centroid_pickups.rename(columns={'centroid_pick_long':'centroid_drop_long', 'centroid_pick_lat':'centroid_drop_lat','label_pick':'label_drop'})\ncentroid_drops.head()\nclus5 = df_cluster4 # just to be safe side so store it\ndf_cluster4 = pd.merge(df_cluster4, centroid_drops, how='left', on=['label_drop'])\ndf_cluster4.head()\n\nend = time.time()\nprint(\"time taken by thie script by now is {}.\".format(end-start))","metadata":{"trusted":false,"_execution_state":"idle","collapsed":true,"_cell_guid":"84b4ffe3-6fc9-4f76-973e-6f8fb728b9ab","_uuid":"05daafb97d09836fd4281b98856682ea1a01aa30"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"#df_cluster4['manhtn_dist_pick_centroid'] = df_cluster4.apply(lambda row: manhattan_distance_pd(row, 'pick_cen'), axis =1)\n#df_cluster4['manhtn_drop_centroid'] = df_cluster4.apply(lambda row: manhattan_distance_pd(row, 'drop_cen'), axis =1)\n#df_cluster4['manhtn_pick_drop'] = df_cluster4.apply(lambda row: manhattan_distance_pd(row, 'pick_drop'), axis =1)\n#df_cluster4.head()","metadata":{"trusted":false,"_execution_state":"idle","collapsed":true,"_cell_guid":"4eab40d3-e381-4d17-aed9-f09e1c000e8a","_uuid":"d6fa4b6b36d03d0bc5242a41f41b60d17dfbbd55"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"#haversine_array(row, mode)\n#df_cluster4['hvsine_dist_pick_centroid'] = df_cluster4.apply(lambda row: haversine_array(row, 'pick_cen'), axis =1)\n#df_cluster4['hvsine_drop_centroid'] = df_cluster4.apply(lambda row: haversine_array(row, 'drop_cen'), axis =1)\n#df_cluster4['hvsine_pick_drop'] = df_cluster4.apply(lambda row: haversine_array(row, 'pick_drop'), axis =1)\n#df_cluster4.head()\n\nend = time.time()\nprint(\"time taken by thie script by now is {}.\".format(end-start))","metadata":{"trusted":false,"_execution_state":"idle","collapsed":true,"_cell_guid":"84c97c3a-6fea-41e4-892a-af22d8730132","_uuid":"89805580359af69aed4e5a16719bd6810c2da33c"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"#df_cluster4['speed_pick_drop_hvsine'] = df_cluster4['hvsine_pick_drop']/df_cluster4['trip_duration']*3600\n\n#end = time.time()\n#print(\"time taken by thie script by now is {}.\".format(end-start))","metadata":{"trusted":false,"_execution_state":"idle","collapsed":true,"_cell_guid":"f15f55e7-c3c0-4740-b64c-5ef38e784a92","_uuid":"ba5a8f791d2c436fed4a3ff1fd96b06f3d5dd02a"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"%matplotlib inline\ndef bar_plot(x_var, y_var):\n    \"\"\"function to show barplot between two variables\"\"\"\n    objects = x_var\n    performance = y_var\n \n    plt.bar(objects, performance, align='center', alpha=0.5)\n    plt.xlabel('class')\n    plt.ylabel('frequency')\n    plt.title('barplot')\n \n    plt.show()\n    return 0\n\nend = time.time()\nprint(\"time taken by thie script by now is {}.\".format(end-start))","metadata":{"trusted":false,"_execution_state":"idle","collapsed":true,"_cell_guid":"c07d2b6c-bdec-4c7a-bffa-d4dd0c10dafb","_uuid":"607bba6110fd7e3ce92c648c7529cd53c150649c"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"## Cluster processing and distance matrix query ","metadata":{"_cell_guid":"34d3be95-0193-497a-b7bc-b924d617efda","_uuid":"3c51143ac0fb83abd23ef3eedb341e6c52c59756"},"execution_count":null,"cell_type":"markdown","outputs":[]},{"source":"### cluster vs trip time\n##### Hypothesis is if trip starts and ends in same cluster, duration of that trip will be less than trips where pickup and dropoff are in different clusters","metadata":{"_cell_guid":"4dc3e538-4017-431a-8e04-477704de6c34","_uuid":"10bb2c08684d24e0b57d3beca8b7517a0fcc2e70"},"execution_count":null,"cell_type":"markdown","outputs":[]},{"source":"df_cluster3 = df_clusters2\ndf_cluster3['diff']= abs(df_clusters2['label_pick']- df_clusters2['label_drop'])\nsummary_s = pd.DataFrame(df_cluster3.groupby('diff')['trip_duration'].sum())\nsummary_c = pd.DataFrame(df_cluster3.groupby('diff')['trip_duration'].count())\nsummary_c.rename(columns={'trip_duration': 'count'}, inplace=True)\nsummary_pertrip = pd.concat([summary_s, summary_c], axis=1)\nsummary_pertrip.head()\nsummary_pertrip['time_pt'] = summary_pertrip['trip_duration']/summary_pertrip['count']\nbar_plot(summary_pertrip.index.values, summary_pertrip.time_pt.values)\n\nend = time.time()\nprint(\"time taken by thie script by now is {}.\".format(end-start))","metadata":{"_execution_state":"idle","_cell_guid":"54e17380-5a6f-4aad-aaa0-042a65bb9223","_uuid":"98f5a3094300ed76f59d654557d84412d93d3f06"},"execution_count":null,"cell_type":"markdown","outputs":[]},{"source":"### Clear from above plot that trips ending in same clusters have minimum time \n### Let's visualize location of clusters (All clusters) on google maps - Download HTML file from output and open in google chrome","metadata":{"_cell_guid":"fb5462ea-8740-49ae-a623-a3fbd0c3d587","_uuid":"cb2f2d863cd2d49e678b25bc150ad648a996db93"},"execution_count":null,"cell_type":"markdown","outputs":[]},{"source":"def show_cluster_gmaps(train_df, cluster_df, points_per_cluster, cluster_list, path, name):\n    \"\"\"function to vasualize cluster \n    1. train_df - df containg lat-long\n    2. cluster_df - df containing cluster centroid\n    3. points_per_cluster = number od coordniates shown per cluster\n    4. cluster_list - list of clusters you want to show on map\n    5. path - if 1, show paths to cluster centroid\"\"\"\n    cluster_df_new = cluster_df[cluster_df['label_pick'].isin(cluster_list)]\n    init_lat = cluster_df_new.centroid_pick_lat.mean()\n    init_long = cluster_df_new.centroid_pick_long.mean()\n    print(init_lat, init_long)\n    mymap = maps(init_lat,init_long, 12) # manually added centre\n    cluster_df_new = cluster_df_new.reset_index(drop = True)\n    #print(cluster_df_new)\n    for i in range(cluster_df_new.shape[0]):\n        pick_long = cluster_df_new.loc[cluster_df_new.index ==i]['centroid_pick_long'].values[0]\n        pick_lat = cluster_df_new.loc[cluster_df_new.index ==i]['centroid_pick_lat'].values[0]\n        mymap.addpoint(pick_lat, pick_long, \"#FF0000\")\n        mymap.addradpoint(pick_lat, pick_long, 750, \"#0000FF\")\n        #mymap.getcycle([pick_lat, pick_long, 0.01])\n    mymap.draw('./cluster_map_'+name+'.txt')\n    return\n\nend = time.time()\nprint(\"time taken by thie script by now is {}.\".format(end-start))","metadata":{"trusted":false,"_execution_state":"idle","collapsed":true,"_cell_guid":"87cb50a5-03e2-4e1b-8f9e-d8559a4fb6bb","_uuid":"42d54def3aa24fd877af7e38a5e1e5f8f14d0009"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"cluster_list = range(0,20)\n#summary_significant_clusters.tolist()\n#print(cluster_list)\nshow_cluster_gmaps(train_data.head(100), centroid_pickups, 30, cluster_list, 1, 'all clusters')\n\nfrom IPython.display import IFrame\nIFrame('cluster_map_all clusters.txt', width=1000, height=500)\n\nend = time.time()\nprint(\"time taken by thie script by now is {}.\".format(end-start))","metadata":{"trusted":false,"collapsed":true,"_cell_guid":"3304f01c-bcea-48b1-8df6-c258c1d680d5","scrolled":true,"_execution_state":"idle","_uuid":"96d9f9666c0ef10434728dfa14a80c04edab51f2"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"### Visualize clusters which are significant i.e have more than 50k pickups from training data ","metadata":{"_cell_guid":"e71ff6aa-bd64-456f-8959-c50962850a00","_uuid":"811979704722a13b66935cb8114b834eb882c2c4"},"execution_count":null,"cell_type":"markdown","outputs":[]},{"source":"summary_clusters_time = pd.DataFrame(df_cluster4.groupby('label_pick')['trip_duration'].count())\nsummary_significant_clusters = summary_clusters_time.loc[summary_clusters_time['trip_duration']>50000].index.values\nsummary_significant_clusters\ncluster_list = summary_significant_clusters.tolist()\n\nprint(\"list of clusters with 50k pick ups are - {}.\".format(cluster_list))\nshow_cluster_gmaps(train_data.head(100), centroid_pickups, 30, cluster_list, 1, 'significant')\n\nfrom IPython.display import IFrame\nIFrame('cluster_map_significant.txt', width=1000, height=500)\n\nend = time.time()\nprint(\"time taken by thie script by now is {}.\".format(end-start))","metadata":{"trusted":false,"_execution_state":"idle","collapsed":true,"_cell_guid":"66ff4825-c476-4835-a568-b47a49875e74","_uuid":"767c81a0f6fb199562ff809abc4910b1aee03dfa"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"index_rand = np.random.randint(len(cluster_list),size =4)\nprint(cluster_list, len(cluster_list))\ncluster_list4 = [cluster_list[x] for x in index_rand]\nprint(cluster_list4)\n\nend = time.time()\nprint(\"time taken by thie script by now is {}.\".format(end-start))","metadata":{"trusted":false,"_execution_state":"idle","collapsed":true,"_cell_guid":"ff5c46fe-daa5-4c5d-a981-553e95b13f6d","_uuid":"7edd06bf5362b8424c7161ef392db0fde247285c"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"#### We can easily check in last map that all the significant clusters are falling in manhattan","metadata":{"_cell_guid":"74a7b5ac-7ba8-4cd3-9294-79046feb944a","_uuid":"e6b20915e8bf35081ae53db5aa7be31a760fdced"},"execution_count":null,"cell_type":"markdown","outputs":[]},{"source":"### Travel visualization - cluster to cluster \n#### pickup ---------pick_cluster----------------------------------------dest_cluster--------destination","metadata":{"_cell_guid":"6b89056e-9fcf-46a9-b0f4-c9fc1f3253e9","_uuid":"838433297633005daf67650cc7cbcf10a0b94fb1"},"execution_count":null,"cell_type":"markdown","outputs":[]},{"source":"# cluster visualization - \ndef cluster_visualization(train_df, cluster_df, points_per_cluster, cluster_list, path_mode):\n    \"\"\"function to vasualize cluster \n    1. train_df - df containg lat-long\n    2. cluster_df - df containing cluster centroid\n    3. points_per_cluster = number od coordniates shown per cluster\n    4. cluster_list - list of clusters you want to show on map - only two clusters\n    5. path - if 1, show paths to cluster centroid\"\"\"\n    cluster_df_new = cluster_df[cluster_df['label_pick'].isin(cluster_list)]\n    init_lat = cluster_df_new.centroid_pick_lat.mean()\n    init_long = cluster_df_new.centroid_pick_long.mean()\n    print(init_lat, init_long)\n    train_df_new = train_df[train_df['label_pick'].isin(cluster_list)]\n    train_df_new = train_df_new.reset_index(drop = True)\n    sample_list = np.random.randint(train_df_new.shape[0], size = 100) # INITIAL 50\n    sample_df = train_df_new.loc[sample_list]\n    sample_df = sample_df.reset_index(drop = True)\n    #print(sample_df.head())\n    \n    mymap = maps(init_lat,init_long, 13.5) # manually added centre #INITIAL - 12\n    cluster_df_new = cluster_df_new.reset_index(drop = True)\n    #print(cluster_df_new)\n    for i in range(cluster_df_new.shape[0]):\n        pick_long = cluster_df_new.loc[cluster_df_new.index ==i]['centroid_pick_long'].values[0]\n        pick_lat = cluster_df_new.loc[cluster_df_new.index ==i]['centroid_pick_lat'].values[0]\n        mymap.addpoint(pick_lat, pick_long, \"#FF0000\")\n        mymap.addradpoint(pick_lat, pick_long, 750, \"#0000FF\")\n        sample_df_clus = sample_df.loc[sample_df['label_pick'] == cluster_df_new.loc[i]['label_pick']]\n        sample_df_clus = sample_df_clus.reset_index(drop = True)\n        for j in range(sample_df_clus.shape[0]):\n            sample_lat = sample_df_clus.loc[j]['pickup_latitude']\n            sample_long = sample_df_clus.loc[j]['pickup_longitude']\n            mymap.addpoint(sample_lat, sample_long, \"#FF0000\")\n            path = [(pick_lat, pick_long),(sample_lat, sample_long)]\n            mymap.addpath(path,\"#000000\")\n    if path_mode ==1:\n        for k in range(cluster_df_new.shape[0]):\n            ln1 = cluster_df_new.loc[cluster_df_new.index ==k]['centroid_pick_long'].values[0]\n            lt1 = cluster_df_new.loc[cluster_df_new.index ==k]['centroid_pick_lat'].values[0]\n            for l in range(cluster_df_new.shape[0]):\n                if k!=l:\n                    ln2 = cluster_df_new.loc[cluster_df_new.index ==l]['centroid_pick_long'].values[0]\n                    lt2 = cluster_df_new.loc[cluster_df_new.index ==l]['centroid_pick_lat'].values[0]\n                    path_c = [(lt1, ln1),(lt2, ln2)]\n                    mymap.addpath(path_c,\"#000000\")\n                    #print('path added')\n    mymap.draw('./multi_clusters_.txt')\n    return\n\nend = time.time()\nprint(\"time taken by thie script by now is {}.\".format(end-start))","metadata":{"trusted":false,"_execution_state":"idle","collapsed":true,"_cell_guid":"1ff3ebd0-c098-409d-88b3-117e1cc979f3","_uuid":"019d05d85289a577a7aadc44c9a9831e8bcd0f5e"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"cluster_list = cluster_list4#[0, 13, 26, 39] #INITIAL 9,23, 21,23, 33- 23,\nprint(cluster_list4)\n#summary_significant_clusters.tolist()\n#[ 0,  3,  7,  9, 12, 13, 15, 19, 21, 23, 25, 28, 33, 39, 41, 42]\nprint(\"clusters which are getting shown on Google maps are - {}.\".format(cluster_list))\ncluster_visualization(df_cluster4, centroid_pickups, 30, cluster_list, 1)\n\nfrom IPython.display import IFrame\nIFrame('multi_clusters_.txt', width=1000, height=500)\n\nend = time.time()\nprint(\"time taken by thie script by now is {}.\".format(end-start))","metadata":{"trusted":false,"_execution_state":"idle","collapsed":true,"_cell_guid":"a13a9bef-3e77-4b34-900f-8b8885b27a6d","_uuid":"f43d26f5ad692a9e46c555b920752c1ee12b28b9"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"### For distance matrix query, we create C(45,2) = 1980 pairs and find- 1. Shortest path for cars, 2. Avg. time for cars, 3. Origin address, and 4. destination address","metadata":{"_cell_guid":"7d47878d-56c5-4228-8d53-ec392740486a","_uuid":"2173f56d8b4a41cc674f4f8e1320323ff3dbfcef"},"execution_count":null,"cell_type":"markdown","outputs":[]},{"source":"# creating a df containing all such combinations\ndf_clus_pick_dest = pd.DataFrame(columns=('pick_long', 'pick_lat', 'pick_label','drop_long', 'drop_lat', 'drop_label'))\nlist_vars =[]\nk = centroid_pickups.shape[0]\nfor i in range(0,centroid_pickups.shape[0]):\n    for j in range(0,centroid_pickups.shape[0]):\n        if i !=j:\n            pick_long = centroid_pickups.loc[i]['centroid_pick_long']\n            pick_lat = centroid_pickups.loc[i]['centroid_pick_lat']\n            pick_label = centroid_pickups.loc[i]['label_pick']\n            drop_long = centroid_pickups.loc[j]['centroid_pick_long']\n            drop_lat = centroid_pickups.loc[j]['centroid_pick_lat']\n            drop_label = centroid_pickups.loc[j]['label_pick']\n            list_data = [pick_long, pick_lat, pick_label, drop_long, drop_lat, drop_label]\n            df_clus_pick_dest.loc[i*k+j] = list_data\n            \nprint(df_clus_pick_dest.shape[0])           \ndf_clus_pick_dest.head()\n\nend = time.time()\nprint(\"time taken by thie script by now is {}.\".format(end-start))","metadata":{"trusted":false,"_execution_state":"idle","collapsed":true,"_cell_guid":"785be715-31fc-454c-82d5-2b2c01a73b77","_uuid":"42722fc3113e6a6d5aab39f80700192500c1ce1c"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"\"\"\"piece of code to check if kaggle supports api queries\"\"\"\norig_lat = 40.729542\norig_lng = -73.984382\ndest_lat = 37.389339\ndest_lng = -121.933289\nprint(orig_lat, orig_lng)\n#url = \"\"\"http://maps.googleapis.com/maps/api/distancematrix/js?origins=%s,%s\"\"\"%(orig_lat, orig_lng)+ \"\"\"&destinations=%s,%s&mode=driving&language=en-EN&sensor=false\"\"\"% (dest_lat, dest_lng)\n#a = urllib.request.urlopen(url)\n#print(url)\n\n\n\n\n","metadata":{"trusted":false,"_execution_state":"idle","collapsed":true,"_cell_guid":"6eaed557-b3ba-469b-9c2b-1949d520d423","_uuid":"422c8651918e0828965f4841a9f4f97655ac2185"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"\n#df_clus_pick_dest['data_from_google']=''\nglobal count_hhh \ncount_hhh = 0\ndef google_maps_query(row):\n    \"\"\"\n    function to use google api on source and destination co-ordinates \n    returns following - \n    1. origin address \n    2. destination address\n    3. duration ~ duration from distance matrix api\n    4. distance of shrtest path\n    \"\"\"\n    orig_lat = row['pick_lat']\n    orig_lng = row['pick_long']\n    dest_lat = \trow['drop_lat']\n    dest_lng =  row['drop_long']\n    url = \"\"\"http://maps.googleapis.com/maps/api/distancematrix/json?origins=%s,%s\"\"\"%(orig_lat, orig_lng)+  \\\n    \"\"\"&destinations=%s,%s&mode=driving&language=en-EN&sensor=false\"\"\"% (dest_lat, dest_lng)\n    result= simplejson.load(urllib.urlopen(url))\n    global count_hhh\n    count_hhh = count_hhh + 1\n    if count_hhh % 198 ==0:\n        print(count_hhh//197, time.time())\n    return result\n\ndef origin_address(row):\n    return row['data_from_google']['origin_addresses']\n\ndef destination_address(row):\n    return row['data_from_google']['destination_addresses']\n\ndef gmaps_duration(row):\n    \"\"\"function to extract the duration of ride\"\"\"\n    if row['data_from_google']['status'] =='OK':\n        if len(row['data_from_google']['rows'][0]['elements'][0].keys())==3:\n            query_result = row['data_from_google']['rows'][0]['elements'][0]['duration']['value']\n        else:\n            query_result = np.nan\n    else:\n        query_result = np.nan\n    return(query_result)\n\ndef gmaps_distance(row):\n    \"\"\"function to extract the dstance of ride\"\"\"\n    if row['data_from_google']['status'] =='OK':\n        if len(row['data_from_google']['rows'][0]['elements'][0].keys())==3:\n            query_result = row['data_from_google']['rows'][0]['elements'][0]['distance']['value']\n        else:\n            query_result = np.nan\n    else:\n        query_result = np.nan\n    return(query_result)\n\nend = time.time()\nprint(\"time taken by thie script by now is {}.\".format(end-start))\n","metadata":{"trusted":false,"_execution_state":"idle","collapsed":true,"_cell_guid":"2ae5f32b-4c56-4ccd-9b81-ab3800f567a6","_uuid":"e142a4bb37363f5587b64b5e04087315c6c4d63b"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"#df_clus_pick_dest['data_from_google']=df_clus_pick_dest.apply(lambda row: google_maps_query(row), axis =1)\n\n","metadata":{"trusted":false,"_execution_state":"idle","collapsed":true,"_cell_guid":"a02ec1fc-c51c-4019-bcd5-a34fcb51f787","_uuid":"a8e207ae05a1e753479212ad1b108dd682fda6da"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"\"\"\"df_clus_pick_dest = df_clus_pick_dest.reset_index(drop = True)\ndf_clus_pick_dest['origin_address'] = df_clus_pick_dest.apply(lambda row: origin_address(row), axis =1)\ndf_clus_pick_dest['destination_address'] = df_clus_pick_dest.apply(lambda row: destination_address(row), axis =1)\ndf_clus_pick_dest['gmaps_duration'] = df_clus_pick_dest.apply(lambda row: gmaps_duration(row), axis =1)\ndf_clus_pick_dest['gmaps_distance'] = df_clus_pick_dest.apply(lambda row: gmaps_distance(row), axis =1)\n\"\"\"\n\nend = time.time()\nprint(\"time taken by thie script by now is {}.\".format(end-start))","metadata":{"trusted":false,"_execution_state":"idle","collapsed":true,"_cell_guid":"fe292e67-58af-4179-be1a-fdeed202369e","_uuid":"2273b44de54d4d7b8669cd0eb0965ee71ed12e96"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"e = time.time()\nprint(\"So we have {} seconds left and we will add many beautiful visualizations in this time.\".format(1200 -(e-s)))","metadata":{"trusted":false,"_execution_state":"idle","collapsed":true,"_cell_guid":"7397682f-58a5-47c9-aff2-f4a3a1e3c279","_uuid":"0d4769962acec62bee48a062b9e919ed1f8cde78"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"","metadata":{"trusted":false,"collapsed":true,"_cell_guid":"039d8307-2b8e-4536-b8fa-1393be92b823","_uuid":"edc1367da33468be6913ea1c006f8619d36d4845"},"execution_count":null,"cell_type":"code","outputs":[]}],"nbformat_minor":1}