{"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python","mimetype":"text/x-python","pygments_lexer":"ipython3","name":"python","file_extension":".py","version":"3.6.1"},"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"}},"nbformat":4,"nbformat_minor":0,"cells":[{"metadata":{"_uuid":"9dee9361a0206d40e33b95156ba37a417c697b65","_cell_guid":"0e8097e6-e2f0-487c-9e35-6e4cfaf9fa4d","collapsed":false,"_execution_state":"idle"},"source":"In this notebook I will begin by processing the data, specifically the datetime columns and from there conduct some basic EDA and then test different models such as Gradient Boosting (using XGBoost and Random Forests).","outputs":[],"execution_count":null,"cell_type":"markdown"},{"metadata":{"_uuid":"be7f926af830d6a5ad7a8a28e85c79ae0fb17568","trusted":false,"_cell_guid":"17c7863f-0708-4973-b85d-96f0c0132e9e","_execution_state":"idle"},"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"79674e3b680ef286f128871ba1bbea34286fdda2","_cell_guid":"8c6e66d9-a9f3-4314-8233-778a312b7f89","collapsed":false,"_execution_state":"idle"},"source":"# Step One: Processing the Data","outputs":[],"execution_count":null,"cell_type":"markdown"},{"metadata":{"_uuid":"c147e402ae9fb468a300568aa8f2e2a04f3769a1","_cell_guid":"aad78b43-52a6-42df-a31f-da844d5f59b5","collapsed":false,"_execution_state":"idle"},"source":"Let's begin by reading in the training data as a dataframe and then checking the head of the dataframe.","outputs":[],"execution_count":null,"cell_type":"markdown"},{"metadata":{"_uuid":"ab07de76074c12326c61c5a2bd6f47750804887d","_cell_guid":"9e9a1cd8-c269-4551-8b67-1e08a175f72f","trusted":false,"collapsed":false,"_execution_state":"idle"},"source":"train = pd.read_csv('../input/train.csv')","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"f846c59a0c0b1d9089a8dc759317e66cff0dd5e2","_cell_guid":"9f303afa-4f2f-4dd1-9f57-5731fa7378b9","trusted":false,"collapsed":false,"_execution_state":"idle"},"source":"train.head()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"ea6088d64718e6716492cfb14bd5417202e272ee","_cell_guid":"7118542b-72c2-41fe-a79d-6b7dbb66ec44","collapsed":false,"_execution_state":"idle"},"source":"Let's check out the type of data in each column","outputs":[],"execution_count":null,"cell_type":"markdown"},{"metadata":{"_uuid":"b63f2d506d10564bca9bdb4ef56227349827728c","_cell_guid":"85c0af32-82ac-4693-b360-ad5543e5c088","trusted":false,"collapsed":false,"_execution_state":"idle"},"source":"train.dtypes","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"c8e6f9b912f9b7155c864493e1a0b081d71aaed6","_cell_guid":"b1036555-5217-4185-8c6a-5d72380d2539","collapsed":false,"_execution_state":"idle"},"source":"We can see that most of the data is numerical except for the pickup_datetime, dropoff_datetime, and store_and_fwd_flag columns. You might notice that the pickup_datetime and dropoff_datetime columns are of type 'object' and we should convert the data in these columns to datetime objects using the pd.to_datetime method.","outputs":[],"execution_count":null,"cell_type":"markdown"},{"metadata":{"_uuid":"5ce001ac20953b403896f00c80157c20d7a573e8","_cell_guid":"1ad6ca5f-08d2-461f-b4c7-39db49b36799","trusted":false,"collapsed":false,"_execution_state":"idle"},"source":"train['pickup_datetime'] = pd.to_datetime(train['pickup_datetime'])","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"b25e79f3d90f54a3551332911be5a88473f6589c","_cell_guid":"7a3440c2-0cf4-4089-a754-db8ab5b88fb1","trusted":false,"collapsed":false,"_execution_state":"idle"},"source":"train['dropoff_datetime'] = pd.to_datetime(train['dropoff_datetime'])","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"a0564200f2ca67023480b7853f7b6cf3572f4b5b","_cell_guid":"2d10fa70-bfc8-471a-88ae-02c2e638c08f","collapsed":false,"_execution_state":"idle"},"source":"From here we can do some feature extraction on the pickup_datetime column by getting the hour, minute, day, and month of the pick up times.","outputs":[],"execution_count":null,"cell_type":"markdown"},{"metadata":{"_uuid":"853f780041786378fb879fe0c921f295bce695f8","_cell_guid":"f91110b9-f73a-46bc-8a82-22f1e643dae6","trusted":false,"collapsed":false,"_execution_state":"idle"},"source":"train['pickup_hour'] = train['pickup_datetime'].dt.hour\ntrain['pickup_minute'] = train['pickup_datetime'].dt.minute\ntrain['pickup_day'] = train['pickup_datetime'].dt.day\ntrain['pickup_month'] = train['pickup_datetime'].dt.month\n# Just in case, let's get the second of the pickup time as well\ntrain['pickup_second'] = train['pickup_datetime'].dt.second","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"6f684a2b0a848d6a9ee29b1ddc9c721bd298df64","_cell_guid":"916e45ce-9397-444c-9b45-f0e61255b052","trusted":false,"collapsed":false,"_execution_state":"idle"},"source":"train.head()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"aa6bda72b820c751078564c5777919494f7dbfd4","_cell_guid":"ee7378c2-74ad-4ffb-a317-e59121be4c26","collapsed":false,"_execution_state":"idle"},"source":"# Step Two : Exploratory Data Analysis","outputs":[],"execution_count":null,"cell_type":"markdown"},{"metadata":{"_uuid":"446021a7aa44bfa293a1fb9df59f32b644de0de4","_cell_guid":"016660e8-b005-4c18-93e0-cf93b7ea1d75","collapsed":false,"_execution_state":"idle"},"source":"Let's take a look at the distributions of the different time-based variables that we extracted in the previous step.","outputs":[],"execution_count":null,"cell_type":"markdown"},{"metadata":{"_uuid":"6e25b4d9d7eef6d4984d7359741292c40a7b45f0","_cell_guid":"3994e2d5-df67-4031-a782-663380c4eab3","trusted":false,"collapsed":false,"_execution_state":"idle"},"source":"sns.distplot(train['pickup_month'])","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"5037931b9cd9348b6a2b4230a5ea92a04095e701","_cell_guid":"631c6dd8-14f6-49ed-bcf8-a377b4a2200d","collapsed":false,"_execution_state":"idle"},"source":"So it seems that while March (corresponding to number 3) is the most common month for pickup times, the months for pick up times are almost evenly spread out.","outputs":[],"execution_count":null,"cell_type":"markdown"},{"metadata":{"_uuid":"62d6f295a84cb0546a2c923f7b82327aa6ca7fc5","_cell_guid":"16bfc78c-7601-45e1-8b11-f7bfb1dfc617","trusted":false,"collapsed":false,"_execution_state":"idle"},"source":"sns.distplot(train['pickup_day'])","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"43c1389d03f8531944d5e5c5b05a495734e74f71","_cell_guid":"cbdd3ee7-2321-47e6-80b8-80a80e2aae96","collapsed":false,"_execution_state":"idle"},"source":"Again, nothing too surprising here! The days for each pickup time are also almost evenly distributed.","outputs":[],"execution_count":null,"cell_type":"markdown"},{"metadata":{"_uuid":"de41b1c7e4037a6932c4ce3630317b94767696ba","_cell_guid":"01ce2527-8768-47f3-89c9-5c13787e92a8","trusted":false,"collapsed":false,"_execution_state":"idle"},"source":"sns.distplot(train['pickup_hour'])","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"f482d38057ad7ff71b9556593c86b9532cd004b4","_cell_guid":"54aa8c85-2a9f-47d6-b1ef-d446f8ffe210","collapsed":false,"_execution_state":"idle"},"source":"Based on the distribution above we can notice the following:\n\n- There are relatively few early morning pickups before 6 am.\n- From 6 am until around the early evening at 7 pm (19:00 in the 24-hour clock), the number of pickups start to rise and then peak at this evening time.","outputs":[],"execution_count":null,"cell_type":"markdown"},{"metadata":{"_uuid":"dde1ff639deb4724bdef7de140ba2b6955e5da88","_cell_guid":"29b19779-03ee-40ad-914f-a5330f30f300","trusted":false,"collapsed":false,"_execution_state":"idle"},"source":"sns.distplot(train['pickup_minute'])","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"26e1ba5c231faa1d0de426a2928ec64b18565c9b","_cell_guid":"5d37c472-96ef-44b9-b2f5-c657e8bab184","collapsed":false,"_execution_state":"idle"},"source":"Based on the above distribution, it seems that people usually get picked up most often in 5-6 minute time intervals throughout the hour. We can see a total of ten sudden peaks for the hour period. The rest of the distribution is relatively uniform.","outputs":[],"execution_count":null,"cell_type":"markdown"},{"metadata":{"_uuid":"51ee7e3a225e0d2f1a962c9af5cc0f15b401e06e","_cell_guid":"86e9f4fb-9d4c-466e-a684-6f63ef700f34","trusted":false,"collapsed":false,"_execution_state":"idle"},"source":"sns.distplot(train['pickup_second'])","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"c9e4debaf8ea9195e8cf2840a898bf05e45d3e79","_cell_guid":"c78edf7b-e32f-488e-838b-007d8d523dec","collapsed":false,"_execution_state":"idle"},"source":"The seconds follows a similar distribution as the minutes data but I am not sure if this data will be as significant. Nevertheless, for now let's keep this column and see what happens.","outputs":[],"execution_count":null,"cell_type":"markdown"},{"metadata":{"_uuid":"c863946841a51844cb1bc84ba3020f4fdb7c1ea5","_cell_guid":"e393c59f-60a0-4b76-8e48-63b6d8bbe8af","collapsed":false,"_execution_state":"idle"},"source":"Let's look at the distribution of the trip duration.","outputs":[],"execution_count":null,"cell_type":"markdown"},{"metadata":{"_uuid":"e1bd12587773c4c65a8cc680a2567b3835dc1ca8","_cell_guid":"2518bcae-d3bf-4089-b949-c058b76a7c7b","trusted":false,"collapsed":false,"_execution_state":"idle"},"source":"sns.distplot(train['trip_duration'])","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"cd135f033692dbfe0602f03cd3464e8de68f0abb","_cell_guid":"6210a34a-dba7-469e-af3f-d5a695f073c5","trusted":false,"collapsed":false,"_execution_state":"idle"},"source":"train['trip_duration'].describe()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"fd078b096b453a2978a4318e77a6ff1cd820bf53","_cell_guid":"289848a5-2163-42bc-a2e1-1cb1d460d103","collapsed":false,"_execution_state":"idle"},"source":"So it seems 75% of trips are under 1075 seconds (about 17.9 minutes) but the longest trip lasted 3,526,282 seconds (979.5 hours!). ","outputs":[],"execution_count":null,"cell_type":"markdown"},{"metadata":{"_uuid":"a1774e23a38a99909c53d1908fbca8d737a398a4","_cell_guid":"1cae6be7-87c5-4436-bd8c-4faf068bf6f5","trusted":false,"collapsed":false,"_execution_state":"idle"},"source":"sns.distplot(train['passenger_count'])","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"4bbc347a8804131bbb555854900899d4e5e6f1f3","_cell_guid":"1e454072-04c5-435f-9e01-49e988782b26","trusted":false,"collapsed":false,"_execution_state":"idle"},"source":"train['passenger_count'].describe()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"d316846a9aa4f3b0a126ac14ae682ddceae9ebc2","_cell_guid":"eb17e7a8-3b4e-4a81-ab88-1987f8f194f6","collapsed":false,"_execution_state":"idle"},"source":"So it seems no trip has more than nine passengers, which seems logical, with an overwhelming majority of trips having just one passenger.","outputs":[],"execution_count":null,"cell_type":"markdown"},{"metadata":{"_uuid":"779f6b49cbdde84af9a83be309f1f9235f6ae02f","_cell_guid":"6e124611-a581-46c9-9263-b0e9bc4ae880","collapsed":false,"_execution_state":"idle"},"source":"Let's remove outliers for the trip duration","outputs":[],"execution_count":null,"cell_type":"markdown"},{"metadata":{"_uuid":"5fe7852beae39522e88922969f903d060c8a39df","_cell_guid":"a2074681-3981-4361-adb9-90c41cfcef57","trusted":false,"collapsed":false,"_execution_state":"idle"},"source":"train = train[train['trip_duration'] < 500000]\nsns.distplot(train['trip_duration'])","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"86448a0f8cc31887b59feec53cf8f26f1d898588","_cell_guid":"101c439f-d61d-4c3b-8564-a92e2677fe7b","collapsed":false,"_execution_state":"idle"},"source":"Let's take a look at the distribution again with trip durations less than 5000","outputs":[],"execution_count":null,"cell_type":"markdown"},{"metadata":{"_uuid":"41479c4ef6dc86189baa023e4d3c0bf0711a9842","_cell_guid":"2d36b521-4b94-482a-bbe3-07514ce462eb","trusted":false,"collapsed":false,"_execution_state":"idle"},"source":"train_below_5000 = train[train['trip_duration'] < 5000]\nsns.distplot(train_below_5000['trip_duration'])","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"45bd86e3dadb0edc81ae701e54f4b60280cd5f2b","_cell_guid":"02a8609d-7b42-4202-9772-897f5908b78c","collapsed":false,"_execution_state":"idle"},"source":"Let's now look at a heatmap of the correlations.","outputs":[],"execution_count":null,"cell_type":"markdown"},{"metadata":{"_uuid":"905428698b9a79f8ef15a3b3ff5466e66c1425ed","_cell_guid":"a2ff2a92-3a6d-447d-8dfd-5d84145d18f3","trusted":false,"collapsed":false,"_execution_state":"idle"},"source":"sns.heatmap(train.drop(['id','dropoff_datetime', 'pickup_datetime', 'store_and_fwd_flag'], axis=1).corr())","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"17430f0cbe6984f943b159350b580e170d7abfb5","_cell_guid":"56c2dc2c-2bb6-42bf-8e83-5bfcdcf38532","collapsed":false,"_execution_state":"idle"},"source":"# Step Three: Model Building","outputs":[],"execution_count":null,"cell_type":"markdown"},{"metadata":{"_uuid":"4f10ddc72d82a8edd11cc33d1bc9641536d36630","_cell_guid":"2c9fd304-7e95-4460-9712-54d4aa65c679","collapsed":false,"_execution_state":"idle"},"source":"Let's try a simple Random Forest Regressor.","outputs":[],"execution_count":null,"cell_type":"markdown"},{"metadata":{"_uuid":"fbd27a7765f7827dd67baf4e43ebae9227628f1f","_cell_guid":"7005b05b-d2ad-4ad3-a8e0-c785a8a4b648","trusted":false,"collapsed":false,"_execution_state":"idle"},"source":"from sklearn.ensemble import RandomForestRegressor","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"4c329f1f8930708262e8f8ee34eb5e3fb7ca8119","collapsed":false,"trusted":false,"_cell_guid":"1e1c4db5-51e1-4ffd-81a5-e92297c2ee4b","_execution_state":"idle"},"source":"train.head()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"a900b8226a8d5c0fd187ca3f0d920e9e0210ab38","collapsed":false,"_cell_guid":"ea989e37-57b6-463f-99c2-cf73c63f36fe","_execution_state":"idle"},"source":"Let's go ahead and encode the binary store_and_fwd_flag categorical variable so our model can make use of it.","outputs":[],"execution_count":null,"cell_type":"markdown"},{"metadata":{"_uuid":"b0481ccec8826fbf98287bf628f4a700f3853550","_cell_guid":"8e02ae73-0014-4313-aaac-1221c0cdb2d0","trusted":false,"collapsed":false,"_execution_state":"idle"},"source":"\ndef encode(X):\n    if X == 'Y':\n        return 1\n    else:\n        return 0\n    \ntrain['store_and_fwd_flag'] = train['store_and_fwd_flag'].apply(lambda x: encode(x))\ntrain.head()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"85acdc38c5b07084fb669288e1d03e472953f999","_cell_guid":"09fcca53-28a8-4813-901c-3c487d0291d4","trusted":false,"collapsed":false,"_execution_state":"idle"},"source":"train_trim = train.drop(['id','dropoff_datetime', 'pickup_datetime', 'store_and_fwd_flag'], axis=1)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"e23efb23b20aae6485117c2d5b7f9842301b2168","collapsed":false,"_cell_guid":"46262bf3-30f8-4520-8746-7a39c8a2b4f8","_execution_state":"idle"},"source":"Now with a trimmed data frame with the columns that we really need, let's begin by testing a RandomForest Regressor. Please note that I have chosen a small value for the number of trees in order to fit the Kaggle limits for running time. In a practical scenario, you should probably use more trees in the random forest (I usually use at least 100). Since we have a very large dataset, I have chosen a small number of trees. ","outputs":[],"execution_count":null,"cell_type":"markdown"},{"metadata":{"_uuid":"b7bfa2b5283364fa737ef246f2b479e693571787","collapsed":false,"trusted":false,"_cell_guid":"5bb0f0b3-f213-4a2d-80ab-e8b195ce9851","_execution_state":"idle"},"source":"# I have chosen these parameters to fit the Kaggle limits on kernel run time\nrf_regressor = RandomForestRegressor(n_estimators=10, max_features='sqrt', n_jobs=-1)\nX = train_trim.drop('trip_duration', axis=1)\ny = train_trim['trip_duration']","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"dcc420a8edeafd4026ace047c580d57e2b5f9ad3","collapsed":false,"_cell_guid":"a3430d7a-f7f5-4058-8f6b-f7a5a0be0b04","_execution_state":"idle"},"source":"Now we can run a cross validation test where we split the data repeatedly into training and test sets to evaluate the performance of our model","outputs":[],"execution_count":null,"cell_type":"markdown"},{"metadata":{"_uuid":"2aad0a8bbff1502e67e42c49e4f21c401a63ab5a","collapsed":false,"trusted":false,"_cell_guid":"c03b83b7-ffc0-48f8-aa75-c04fdcfa0c17","_execution_state":"idle"},"source":"from sklearn.model_selection import train_test_split\nfrom random import randint\nfrom sklearn import metrics\nfor fold in range(3):\n    print('Testing model for fold {} ...'.format(fold + 1))\n    randnum = randint(1, 102)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=randnum)\n    rf_regressor.fit(X_train, y_train)\n    predictions = rf_regressor.predict(X_test)\n    print('Results for fold {}'.format(fold + 1))\n    print('Mean squared logarithmic error: {}'.format(metrics.mean_squared_log_error(predictions, y_test)))","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"4283048dfd200fc4d5f528abfda5a5db3545f4d9","collapsed":false,"trusted":false,"_cell_guid":"08948dab-8a1e-41e9-b675-ba111a6c3e12","_execution_state":"idle"},"source":"# More coming soon!","outputs":[],"execution_count":null,"cell_type":"markdown"}]}