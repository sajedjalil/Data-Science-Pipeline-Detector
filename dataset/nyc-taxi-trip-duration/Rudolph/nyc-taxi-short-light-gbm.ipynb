{"metadata":{"language_info":{"codemirror_mode":{"version":3,"name":"ipython"},"nbconvert_exporter":"python","file_extension":".py","version":"3.6.1","name":"python","pygments_lexer":"ipython3","mimetype":"text/x-python"},"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"}},"nbformat_minor":1,"cells":[{"metadata":{"_uuid":"9aef45736f1be9ee833b98cc0b8c4292db6c1adb","_cell_guid":"d06a7963-3fb4-4622-bae1-db7d3387491c"},"cell_type":"markdown","source":"This is a short code with the objective to better understand the main issues and produce a meaningfull score ( 0.3740 on LB - top 8%). I benefited from reading several of the more detailed kernels (eg beluga, and adapted some of the code) but wanted to have something easier using Light GBM.  "},{"metadata":{"_uuid":"dff9efb914a4485d481f2af33033cc893312342e","_cell_guid":"d147a64a-ad7e-456c-aaec-5fb3014316b2"},"outputs":[],"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pandas.tseries.holiday import USFederalHolidayCalendar as calendar\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import MiniBatchKMeans\nimport lightgbm as lgb\nimport gc\n\nmyfolder = '../input/'\nprint('loading files...')\ntrain = pd.read_csv(myfolder+'train.csv')\ntest = pd.read_csv(myfolder+'test.csv')\nprint(train.shape, test.shape)\n\n#remove outliers  (about an hour and half max to go to JFK )\ntrain = train[train['trip_duration'] <  train['trip_duration'].quantile(0.999)]\ntrain = train[train['trip_duration'] <= train['trip_duration'].mean() + 3*train['trip_duration'].std()]\nprint(' max time in min {:.2f} and mean time in sec {:.2f} '.format(\n    train['trip_duration'].max()/60, train['trip_duration'].mean()))\n\n# NYC longitude and latitude borders \nep = 0.0001      \n(lng1,lng2)=(-74.257*(1+ep), -73.699*(1-ep))\n(lat1,lat2)=(40.495*(1+ep), 40.915*(1-ep)) \ntrain = train[(train['pickup_longitude'] <=lng2)&(train['pickup_longitude'] >=lng1)]\ntrain = train[(train['pickup_latitude'] <=lat2) & (train['pickup_latitude'] >=lat1)]\ntrain = train[(train['dropoff_longitude'] <=lng2)&(train['dropoff_longitude'] >=lng1)]\ntrain = train[(train['dropoff_latitude'] <=lat2)&(train['dropoff_latitude'] >=lat1)]\n\n#  combine train and test into a single frame\ntrain['eval_set'] = 1\ntest['eval_set'] = 2\nData = pd.concat([train, test], axis=0)\nprint(train.shape,test.shape,len(test)+len(train),Data.shape)\n\ndel train, test\ngc.collect()\nData.head(2)","execution_count":20},{"metadata":{"_uuid":"3784457e1c53d468badf5e8610d54e5feed606c7","_cell_guid":"4bccbf3d-c0cb-4bd0-b678-df2d47bf7349"},"outputs":[],"cell_type":"code","source":"# transform the time variables, combine hours and minutes and add holidays\nData['log_trip_duration'] = np.log(Data['trip_duration'].values + 1)\nData['pickup_datetime'] = pd.to_datetime(Data.pickup_datetime)\nData['Month'] = Data['pickup_datetime'].dt.month\nData['DayofMonth'] = Data['pickup_datetime'].dt.day\nData['Timehm'] =Data['pickup_datetime'].dt.hour+Data['pickup_datetime'].dt.minute/60.\nData['dayofweek'] = Data['pickup_datetime'].dt.dayofweek\nholidays = calendar().holidays(start='2015-12-31', end='2016-07-01')\nData['Holiday']=Data['pickup_datetime'].dt.date.astype('datetime64[ns]').isin(holidays).astype(int)\nData.drop(['trip_duration','dropoff_datetime','pickup_datetime'], axis=1,inplace=True)\n# and replace the N,Y with 0,1\nData['store_and_fwd_flag'] = Data['store_and_fwd_flag'].map({'N': 0, 'Y':1}).astype(int)\nData.shape","execution_count":21},{"metadata":{"_uuid":"71d2d547b1d8739b6256bdd32ac1a9b72390982b","_cell_guid":"79374ca6-f2d6-47d2-ba93-de264028ec4f"},"outputs":[],"cell_type":"code","source":"# calculate the distance and direction between pickup and dropooff coordinates\n\ndef haversine_array(lat1, lng1, lat2, lng2):\n    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n    d = np.sin(lat2/2-lat1/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(lng2/2-lng1/2)**2\n    return 2 * 6371 * np.arcsin(np.sqrt(d))   # 6,371 km is the earth radius\n\ndef dummy_manhattan_distance(lat1, lng1, lat2, lng2):\n    return haversine_array(lat1,lng1,lat1,lng2)+haversine_array(lat1,lng1,lat2,lng1)\n\ndef bearing_array(lat1, lng1, lat2, lng2):\n    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n    y = np.sin(lng2 - lng1) * np.cos(lat2)\n    x = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(lng2 - lng1)\n    return np.degrees(np.arctan2(y, x))\n\nData['distance_haversine'] = haversine_array(Data['pickup_latitude'].values, Data['pickup_longitude'].values, \n                                          Data['dropoff_latitude'].values, Data['dropoff_longitude'].values)\nData['distance_dummy_manhattan'] =  dummy_manhattan_distance(Data['pickup_latitude'].values, \n            Data['pickup_longitude'].values, Data['dropoff_latitude'].values, Data['dropoff_longitude'].values)\nData['direction'] = bearing_array(Data['pickup_latitude'].values, Data['pickup_longitude'].values, \n                                      Data['dropoff_latitude'].values, Data['dropoff_longitude'].values)\nData.shape","execution_count":22},{"metadata":{"_uuid":"395a3708817a4f2220ce73486dfa0f955577b9a4","_cell_guid":"5b5a89a2-c4cf-4218-a72b-a010589124e2"},"outputs":[],"cell_type":"code","source":"#  replace latitudes and longitudes with 81 clusters\n\ncoords = np.vstack((Data[['pickup_latitude', 'pickup_longitude']].values,\n                    Data[['dropoff_latitude', 'dropoff_longitude']].values))\nsample_ind = np.random.permutation(len(coords))[:1000000]\nkmeans = MiniBatchKMeans(n_clusters=9**2, batch_size=36**3).fit(coords[sample_ind])\nData['pickup_cluster'] = kmeans.predict(Data[['pickup_latitude', 'pickup_longitude']])\nData['dropoff_cluster'] = kmeans.predict(Data[['dropoff_latitude', 'dropoff_longitude']])\nData.shape","execution_count":23},{"metadata":{"_uuid":"716ec3b187ef382904a58c79d58c602b03f6cc8b","_cell_guid":"b20cfa98-24fe-434b-8469-be11fef01b97"},"outputs":[],"cell_type":"code","source":"pca = PCA().fit(coords)\nData['pickup_pca0'] = pca.transform(Data[['pickup_latitude', 'pickup_longitude']])[:, 0]\nData['pickup_pca1'] = pca.transform(Data[['pickup_latitude', 'pickup_longitude']])[:, 1]\nData['dropoff_pca0'] = pca.transform(Data[['dropoff_latitude', 'dropoff_longitude']])[:, 0]\nData['dropoff_pca1'] = pca.transform(Data[['dropoff_latitude', 'dropoff_longitude']])[:, 1]\nData['pca_manhattan'] = np.abs(Data['dropoff_pca1']-Data['pickup_pca1']) +  np.abs(\n                                 Data['dropoff_pca0']-Data['pickup_pca0'])\nData['center_latitude'] = 0.5*Data['pickup_latitude']+0.5*Data['dropoff_latitude']\nData['center_longitude'] = 0.5*Data['pickup_longitude']+0.5*Data['dropoff_longitude']\ndel coords, sample_ind, kmeans\nData.shape","execution_count":24},{"metadata":{"_uuid":"3ebac32fc205dadc11402bed65c458c7dc0f8b20","_cell_guid":"adbeb79e-d538-4880-9e0a-a39db5751cda"},"outputs":[],"cell_type":"code","source":"# optional external data (OSRM features) from Oscarleo\n# https://www.kaggle.com/oscarleo/new-york-city-taxi-with-osrm/data\n\nflag=False\nif flag:\n    cols=['id', 'total_distance', 'total_travel_time',  'number_of_steps']\n    fr1 = pd.read_csv(myfolder+'fastest_routes_train_part_1.csv', usecols=cols)\n    fr2 = pd.read_csv(myfolder+'fastest_routes_train_part_2.csv', usecols=cols)\n    fr3 = pd.read_csv(myfolder+'fastest_routes_test.csv', usecols=cols)\n    tmp = pd.concat((fr1, fr2, fr3))\n    tmp.columns=['id', 'OSRM_distance', 'OSRM_time',  'OSRM_steps']\n    Data = Data.merge(tmp, how='left', on='id')\n    del fr1, fr2, fr3, tmp\n    gc.collect()\n    Data.shape\nprint(flag)  ","execution_count":25},{"metadata":{"_uuid":"c948e3ed6cc36583ed76ac90a0287ff9d67dffb0","_cell_guid":"219ce763-54ab-46b7-8b91-94e5c3bb4148"},"outputs":[],"cell_type":"code","source":"# split the data set back to train and test (for submission) and the train into \"train\" and \"eval\" sets for ML\n# save the ids for future\n\nTest_id = Data['id'].loc[Data['eval_set']==2].to_frame()\nData.drop('id',axis = 1, inplace=True)\ntrain = Data[Data['eval_set'] == 1]\ntest = Data[Data['eval_set'] == 2]\nprint(Data.shape, train.shape, test.shape)\n\nX=train.drop(['eval_set','log_trip_duration'],axis=1)\ny=train['log_trip_duration']\nfeatures=list(X.columns)\ncfeatures = list(X.select_dtypes(include = ['int64','int32']).columns)\n\nX_train, X_eval, y_train, y_eval = train_test_split(X,y, test_size=0.2, random_state=2)\n\ndel train, Data\ngc.collect()","execution_count":26},{"metadata":{"_uuid":"40acde3b9bd66426efdea2dde3a789be06c92ee2","_cell_guid":"5711dfa3-85f0-4ebd-85e3-1ff3a2ecb415"},"outputs":[],"cell_type":"code","source":"print('formatting and training LightGBM regression ...')\n# use higher num_boost_round eg 1000\n\nlgb_train = lgb.Dataset(X_train.values, y_train.values)\nlgb_eval = lgb.Dataset(X_eval.values, y_eval.values, reference = lgb_train)\nparams = {'metric': 'rmse', 'learning_rate' : 0.05, 'num_leaves': 512, \n         'feature_fraction': 0.9,'bagging_fraction':0.9,'bagging_freq':5,'min_data_in_leaf': 500}\nlgb_model = lgb.train(params, lgb_train, num_boost_round = 200, valid_sets = lgb_eval, \n             feature_name=features, early_stopping_rounds=10,  verbose_eval = 10)\n\ndel lgb_train\ngc.collect()","execution_count":27},{"metadata":{"_uuid":"1f5a7536f0fb0d8ecb922a960bc1a786018fdf28","_cell_guid":"25786ef4-ab5d-41c0-b6f7-deb195a71d30"},"outputs":[],"cell_type":"code","source":"#check feature importance\nlgb.plot_importance(lgb_model,  max_num_features=28, figsize=(7,9))\nplt.show()","execution_count":28},{"metadata":{"_uuid":"f9094f538b34e8f57b5ad74c84fe548376a979cc","_cell_guid":"5167055d-b93f-424b-b88a-92399c9b9966"},"outputs":[],"cell_type":"code","source":"pred = lgb_model.predict(test.drop(['eval_set','log_trip_duration'], axis=1).values, \n                         num_iteration = lgb_model.best_iteration)\nTest_id['trip_duration']=np.maximum(0,np.exp(pred) - 1)\nTest_id.to_csv('submission.csv', index=False)\nprint(Test_id['trip_duration'].mean())           # just a sanity check - it should be around 806","execution_count":29},{"metadata":{},"outputs":[],"cell_type":"code","source":"#check the score again\npred1 = lgb_model.predict(X_train.values, num_iteration = lgb_model.best_iteration)\npred2 = lgb_model.predict(X_eval.values, num_iteration = lgb_model.best_iteration)\nrmsle1= (((y_train-pred1)**2).mean())**0.5\nrmsle2 = (((y_eval-pred2)**2).mean())**0.5\nprint('train score: {:.4f}   eval score: {:.4f}'.format(rmsle1,rmsle2))","execution_count":30},{"metadata":{"_uuid":"ccb688a7c9a4b3be3bff8e5c9a7a29c6c42e441e","_cell_guid":"03166789-b79d-4ae7-b1ce-bfb2fd648b18"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"a2ef61bca36d9c5854714a1639df1adeafe93735","_cell_guid":"551e6a3a-a3b8-4ba1-8d4e-8c2ac39cfc08"},"cell_type":"markdown","source":""}],"nbformat":4}