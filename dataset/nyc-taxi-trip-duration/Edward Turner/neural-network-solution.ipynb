{"nbformat_minor":0,"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"file_extension":".py","version":"3.6.1","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"version":3,"name":"ipython"},"mimetype":"text/x-python","nbconvert_exporter":"python"}},"cells":[{"cell_type":"markdown","source":"In the previous kernel, I presented a solution using gradient boosting.  Now I present you with a solution using a neural network","metadata":{"_cell_guid":"e03fd7ac-2e83-4b5e-9734-0f6a1dd7e7bb","_uuid":"8aac61c763359e0212381029e718e9dab7ff9a2e","_execution_state":"idle","collapsed":false}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n#import modules for kernel\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import MiniBatchKMeans\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.regularizers import l2","metadata":{"_cell_guid":"fa566fcb-0a76-470a-bdb9-c1f6f97ce3a6","_uuid":"12fd359f55677a2e058575fe58d21f8318d69274","trusted":false,"_execution_state":"idle"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/nyc-taxi-trip-duration/train.csv')\ntest = pd.read_csv('../input/nyc-taxi-trip-duration/test.csv')","metadata":{"_cell_guid":"5f3dac82-7e3b-466f-a69d-52ea93976a9b","_uuid":"99a4289fd74a5011d75fb8b3bfd71ae836280e93","trusted":false,"_execution_state":"idle","collapsed":false},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Creation ##\n\nThese are our features.","metadata":{"_cell_guid":"d90ae594-7566-4483-bdf1-5c37669c2e88","_uuid":"7c6ee9b591870758d8639869654d55e7ab1cecd4","_execution_state":"idle","collapsed":false}},{"cell_type":"code","source":"coords = np.vstack((train[['pickup_latitude', 'pickup_longitude']].values,\n                    train[['dropoff_latitude', 'dropoff_longitude']].values,\n                    test[['pickup_latitude', 'pickup_longitude']].values,\n                    test[['dropoff_latitude', 'dropoff_longitude']].values))\n\npca = PCA().fit(coords)\n\nsample_ind = np.random.permutation(len(coords))[:500000]\nkmeans = MiniBatchKMeans(n_clusters=100, batch_size=10000).fit(coords[sample_ind])\n\ndef toDateTime( df ):\n    \n    df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n    \n    df['pickup_weekday'] = df['pickup_datetime'].dt.weekday_name\n    df['pickup_day'] = df['pickup_datetime'].dt.day\n    df['pickup_month'] = df['pickup_datetime'].dt.month.astype('object')\n    df['pickup_hour'] = df['pickup_datetime'].dt.hour\n    df['pickup_minute'] = df['pickup_datetime'].dt.minute\n    df['pickup_dt'] = (df['pickup_datetime'] - df['pickup_datetime'].min()).map(\n        lambda x: x.total_seconds())\n    \n    df.drop('pickup_datetime', axis = 1, inplace = True)\n\n    return df\n#get radical distince\ndef haversine_np(lon1, lat1, lon2, lat2):\n   \n    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n\n    dlon = lon2 - lon1\n    dlat = lat2 - lat1\n\n    a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2\n\n    c = 2 * np.arcsin(np.sqrt(a))\n    km = 6367 * c\n    return km\n\n#manhattan distance\ndef dummy_manhattan_distance(lat1, lng1, lat2, lng2):\n\n    a = haversine_np(lat1, lng1, lat1, lng2)\n    b = haversine_np(lat1, lng1, lat2, lng1)\n    return a + b\n\n#bearing direction\ndef bearing_array(lat1, lng1, lat2, lng2):\n    AVG_EARTH_RADIUS = 6371  # in km\n    lng_delta_rad = np.radians(lng2 - lng1)\n    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n    y = np.sin(lng_delta_rad) * np.cos(lat2)\n    x = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(lng_delta_rad)\n    return np.degrees(np.arctan2(y, x))\n\n#all distances\ndef locationFeatures( df ):\n    #displacement of degrees\n    df['up_town'] = np.sign( df['pickup_longitude'] - df['dropoff_longitude'] )\n    df['est_side'] = np.sign( df['pickup_latitude'] - df['dropoff_latitude'] )\n     \n    #radical distances\n    df['haversine_distance'] = haversine_np(\n        df['pickup_longitude'], df['pickup_latitude'], \n        df['dropoff_longitude'], df['dropoff_latitude']\n    )\n    \n    #log transform of the haversine distance\n    df['log_haversine_distance'] = np.log1p(df['haversine_distance']) \n    \n    #manhattan distances\n    df['distance_dummy_manhattan'] = dummy_manhattan_distance(\n        df['pickup_latitude'], df['pickup_longitude'],\n        df['dropoff_latitude'], df['dropoff_longitude']\n    )\n    \n    #log transform of the haversine distance\n    df['log_distance_dummy_manhattan'] = np.log1p(df['distance_dummy_manhattan']) \n    \n    #pca distances\n    df['pickup_pca0'] = pca.transform(df[['pickup_latitude', 'pickup_longitude']])[:, 0]\n    df['pickup_pca1'] = pca.transform(df[['pickup_latitude', 'pickup_longitude']])[:, 1]\n    df['dropoff_pca0'] = pca.transform(df[['dropoff_latitude', 'dropoff_longitude']])[:, 0]\n    df['dropoff_pca1'] = pca.transform(df[['dropoff_latitude', 'dropoff_longitude']])[:, 1]\n       \n    df.loc[:, 'pca_manhattan'] = ( np.abs(df['dropoff_pca1'] - df['pickup_pca1']) +\n    np.abs(df['dropoff_pca0'] - df['pickup_pca0']) )\n    \n    df.loc[:, 'pickup_cluster'] = kmeans.predict(df[['pickup_latitude', 'pickup_longitude']]).astype('object')\n    df.loc[:, 'dropoff_cluster'] = kmeans.predict(df[['dropoff_latitude', 'dropoff_longitude']]).astype('object')\n    \n    df.drop(['pickup_longitude', 'dropoff_longitude'], axis = 1, inplace = True)\n    df.drop(['pickup_latitude', 'dropoff_latitude'], axis = 1, inplace = True)\n    \n    return df\n\ndef featureCreate( df ):\n    print ('Date time features')\n    df = toDateTime( df )\n    print ('Location Features')\n    df = locationFeatures( df )\n    \n    return df","metadata":{"_cell_guid":"38416e01-1620-4451-8f6c-4f9ae533738d","_uuid":"1ad3d2547f9b78d80139fecfb4b59886f33eadbb","trusted":false,"_execution_state":"idle","collapsed":false},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we add all of our features together to the training dataset.","metadata":{"_cell_guid":"dac6fdca-3a17-4fb6-97cb-5eac4e0922fa","_uuid":"6db3e672daaf68a0edc774e036be30816b75e2b3","_execution_state":"idle","collapsed":false}},{"cell_type":"code","source":"train = featureCreate( train )\ntest = featureCreate( test )\n\n#log transform our trip duration\ntrain['trip_duration'] = np.log1p(train['trip_duration'])","metadata":{"_cell_guid":"60a7efd8-a010-4c0b-a80e-082a2ad66042","_uuid":"f40157b9fcb4132fd30ffa5c4476f7a8ad48b5c5","trusted":false,"_execution_state":"idle","collapsed":false},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We remove our outliers, as we have previously done.","metadata":{"_cell_guid":"c6d0ec3c-b0e0-4f44-a679-2109c0cf4c64","_uuid":"38f5925b9348691404f0c03538c6f8ef3021caed","_execution_state":"idle","collapsed":false}},{"cell_type":"code","source":"q1 = np.percentile(train['trip_duration'], 25)\nq3 = np.percentile(train['trip_duration'], 75)\n\niqr = q3 - q1\n\ntrain = train[ train['trip_duration'] <= q3 + 3.0*iqr]\n\ntrain = train[ q1 - 3.0*iqr <= train['trip_duration']]","metadata":{"_cell_guid":"9c065309-3edc-42a8-b5f1-00b340e0c99f","_uuid":"1a10ada30501ed8b86f3db735ad68c4b7c6d8f75","trusted":false,"_execution_state":"idle","collapsed":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = train.pop('trip_duration')\ntrain.drop([\"id\", \"dropoff_datetime\"], axis=1, inplace = True)\n\nsub = pd.DataFrame( columns = ['id', 'trip_duration'])\nsub['id'] = test.pop('id')","metadata":{"_cell_guid":"31e4ec55-9cd2-44fb-aec2-4ac1bc598a2a","_uuid":"97eed4353db3cfaabf631f5ca97e507822267939","trusted":false,"_execution_state":"idle","collapsed":false},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Change all categorial variables to one-hot encoded","metadata":{"_cell_guid":"283e0147-4896-4023-87d8-302c3a37db63","_uuid":"2cb8d80257ff3b67c73b1d808ac07ebe5ba25bef","_execution_state":"idle","collapsed":false}},{"cell_type":"code","source":"train = pd.get_dummies(train)\ntest = pd.get_dummies(test)","metadata":{"_cell_guid":"ea5d12ac-d5e0-45f2-8ae6-72e8761f7f1b","_uuid":"9b9c2eaf5995e8133e098369fece8e26572f0c92","trusted":false,"_execution_state":"idle","collapsed":false},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We need to scale our solutions","metadata":{"_cell_guid":"fe288eee-cf34-44ad-a59b-8c3f7a978969","_uuid":"dce9b76ee1532e77981b904f02b6ad2780b50ca9","_execution_state":"idle","collapsed":false}},{"cell_type":"code","source":"mmScale = MinMaxScaler()\n\n#input shape\nn = train.shape[1]\n\ntrain = mmScale.fit_transform(train)","metadata":{"_cell_guid":"aa44515f-196e-43b9-87bf-0a2b8b722bd8","_uuid":"4be87d0577a1b31f8a36d612c506dcbb47d7d99c","trusted":false,"_execution_state":"busy","collapsed":false},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Construction ##","metadata":{"_cell_guid":"7ba0578e-7369-4674-a104-3b9adfeace9c","_uuid":"1f9e81f78fd8ff8dff3947e0229181d68629b61e","_execution_state":"idle","collapsed":false}},{"cell_type":"code","source":"#deep learning\nmodel = Sequential()\n#Want to use an expotential linear unit instead of the usual relu\nmodel.add( Dense( n, activation='relu', input_shape=(n,) ) )\nmodel.add( Dense( int(0.5*n), activation='relu' ) )\nmodel.add(Dense(1, activation='linear'))\nmodel.compile(loss='mse', optimizer='rmsprop', metrics=['mae'])","metadata":{"_cell_guid":"1a93b3e8-0e0d-44ae-9f4e-c2983e6f6ebd","_uuid":"1418d999a3f0e1e91b82d6e1fc4bb6efd43d1bfd","trusted":false,"_execution_state":"busy","collapsed":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print ('On to the next one')\n\nmodel.fit(train, labels.values, epochs = 3)\n\nprint ('Finished')","metadata":{"_cell_guid":"2d755f9a-fe47-4445-9365-58898efe7bed","_uuid":"897ccf004e920f6a051fb2794b0cc8e901447640","trusted":false,"_execution_state":"busy","collapsed":false},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Error Analysis ##","metadata":{"_cell_guid":"fd32f03f-9c80-4601-997f-c212d2eb9392","_uuid":"01096fb727d5b79780c3d869524d86fe63f00f18","_execution_state":"idle","collapsed":false}},{"cell_type":"markdown","source":"Now lets analyze the results","metadata":{"_cell_guid":"eaacb1dd-e4fd-4e13-8462-bdf10824623b","_uuid":"4d32ed79e8dac66b515578d3af7622a79c3bea1b","_execution_state":"idle","collapsed":false}},{"cell_type":"code","source":"train_pred = model.predict(train)\ntest_pred = model.predict( mmScale.transform(test) )","metadata":{"_cell_guid":"a0078752-d0fc-44c2-8760-8ef7a5b1c7f6","_uuid":"677cac8a825253c8a86332427728cdbc14ce3f32","trusted":false,"_execution_state":"busy","collapsed":false},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we look at the predictions side by side.","metadata":{"_cell_guid":"655da3d7-c58c-4cbf-a138-cb4afffc90f4","_uuid":"c2e41e426b8de62c5864d6631ca66b9a4f3057c1","_execution_state":"idle","collapsed":false}},{"cell_type":"code","source":"fig = plt.figure()\nax = fig.add_subplot(111)\n\nax.scatter(train_pred, labels, c='b', marker=\"s\", label='pred')\n\nplt.legend()\n\nplt.show()","metadata":{"_cell_guid":"dc60e060-c738-4dee-863b-3e426bbe531f","_uuid":"d3736775dc9e13e25bba833902207775dbe88069","trusted":false,"_execution_state":"busy","collapsed":false},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Overall, the predictions seem tamed.  We will continue this and create polynomai features based on the prediction and create a model based on that, in hopes of generalizing the solution","metadata":{"_cell_guid":"ac0bfc8e-9a60-4823-80a9-d050971f09e5","_uuid":"aa5970feab29d6f0ce214b451c33f5c60d67b5d1","_execution_state":"idle","collapsed":false}},{"cell_type":"code","source":"sub['trip_duration'] = np.expm1(test_pred)\nsub.to_csv('submission.csv', index=False)","metadata":{"_cell_guid":"213a90cc-c126-4f07-be77-2e68180f4738","_uuid":"5f3b3b392f2c2f27338ee779e7a4a75652899b0b","trusted":false,"_execution_state":"busy","collapsed":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"collapsed":false,"_uuid":"9a5ba48d80b7de744b63030d491d06d2fba13a0a","_execution_state":"idle","_cell_guid":"839b031f-092f-4b7a-ac49-f9bbff562668"},"execution_count":null,"outputs":[]}],"nbformat":4}