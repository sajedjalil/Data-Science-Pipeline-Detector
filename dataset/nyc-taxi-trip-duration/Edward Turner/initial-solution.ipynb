{"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python","mimetype":"text/x-python","pygments_lexer":"ipython3","name":"python","file_extension":".py","version":"3.6.1"},"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"}},"nbformat":4,"nbformat_minor":0,"cells":[{"metadata":{"_uuid":"f9f843e8122cded2afa89f60a5e936f3c2e24dcf","collapsed":false,"_cell_guid":"9f207165-82b9-4514-8ed6-d0d2490a63d1","_execution_state":"idle"},"source":"From my previous kernel, and others, some features were engineered and created.  Lets create our model.","outputs":[],"execution_count":null,"cell_type":"markdown"},{"metadata":{"_uuid":"cbcfa52f318496e93522a4652f4d3aa7d91f4636","trusted":false,"_cell_guid":"86e2dfd0-2d34-4618-bd10-e3c399eda6cf","_execution_state":"idle"},"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"0da9c7b4bdec302be8a49aee5c6cb512f68f68d2","collapsed":false,"_cell_guid":"f2c8aedf-6aa2-4472-91e3-28183bd306e5","_execution_state":"idle"},"source":"## Feature Creation ##\n\nLets create our features to use.","outputs":[],"execution_count":null,"cell_type":"markdown"},{"metadata":{"_uuid":"d49cc7e9d6877eb5cc546829fa53548457f6caac","collapsed":false,"trusted":false,"_cell_guid":"c57237bb-b53a-4bc0-80b4-847794a4d475","_execution_state":"idle"},"source":"def toDateTime( df ):\n    \n    df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n    \n    df['pickup_weekday'] = df['pickup_datetime'].dt.weekday\n    df['pickup_day'] = df['pickup_datetime'].dt.day\n    df['pickup_month'] = df['pickup_datetime'].dt.month\n    df['pickup_hour'] = df['pickup_datetime'].dt.hour\n    df['pickup_minute'] = df['pickup_datetime'].dt.minute\n    df['pickup_dt'] = (df['pickup_datetime'] - df['pickup_datetime'].min()).map(\n        lambda x: x.total_seconds())\n    \n    df.drop('pickup_datetime', axis = 1, inplace = True)\n\n    return df\n\n#get radical distince\ndef haversine_np(lon1, lat1, lon2, lat2):\n   \n    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n\n    dlon = lon2 - lon1\n    dlat = lat2 - lat1\n\n    a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2\n\n    c = 2 * np.arcsin(np.sqrt(a))\n    km = 6367 * c\n    return km\n\n#manhattan distance\ndef dummy_manhattan_distance(lat1, lng1, lat2, lng2):\n\n    a = haversine_np(lat1, lng1, lat1, lng2)\n    b = haversine_np(lat1, lng1, lat2, lng1)\n    return a + b\n\n#all distances\ndef locationFeatures( df ):\n    #displacement of degrees\n    df['up_town'] = np.sign( df['pickup_longitude'] - df['dropoff_longitude'] )\n    df['est_side'] = np.sign( df['pickup_latitude'] - df['dropoff_latitude'] )\n     \n    #radical distances\n    df['haversine_distance'] = haversine_np(\n        df['pickup_longitude'], df['pickup_latitude'], \n        df['dropoff_longitude'], df['dropoff_latitude']\n    )\n    \n    #manhattan distances\n    df['distance_dummy_manhattan'] = dummy_manhattan_distance(\n        df['pickup_latitude'], df['pickup_longitude'],\n        df['dropoff_latitude'], df['dropoff_longitude']\n    )\n    \n    df.drop(['pickup_longitude', 'dropoff_longitude'], axis = 1, inplace = True)\n    df.drop(['pickup_latitude', 'dropoff_latitude'], axis = 1, inplace = True)\n    \n    return df\n\ndef featureCreate( df ):\n    df = toDateTime( df )\n    df = locationFeatures( df )\n    \n    return df","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"996dc47021694659f89e3bc2b8f550ceb13354f6","collapsed":false,"_cell_guid":"8b59c178-33d4-4247-b290-42f589f5aeee","_execution_state":"idle"},"source":"Lets load our data.","outputs":[],"execution_count":null,"cell_type":"markdown"},{"metadata":{"_uuid":"9d41bda49a641b847288d56c0d45623c4a9bf740","collapsed":false,"trusted":false,"_cell_guid":"ddff467e-5f92-47a4-a5e7-f0ec94924541","_execution_state":"busy"},"source":"train = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"6195df544343cb62df833a4384909c55b64db3ba","collapsed":false,"_cell_guid":"5b18ba0e-4738-4043-b807-15da3f60b843","_execution_state":"idle"},"source":"Lets add these features.","outputs":[],"execution_count":null,"cell_type":"markdown"},{"metadata":{"_uuid":"171da848782465842f94482a73d3d7a3c9c2b86c","collapsed":false,"trusted":false,"_cell_guid":"33651a0a-320d-490c-9756-14566b39669b","_execution_state":"busy"},"source":"train = featureCreate( train )\ntest = featureCreate( test )\n\n#log transform our trip duration\ntrain['trip_duration'] = np.log1p(train['trip_duration'])\n\n#log transform of the haversine distance\ntrain['haversine_distance'] = np.log1p(train['haversine_distance'])\ntest['haversine_distance'] = np.log1p(test['haversine_distance'])","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"5fd03f4052594df96f5a84dd5d19ac5af84e43a9","collapsed":false,"_cell_guid":"8292fac0-1476-442a-947c-40654f47d576","_execution_state":"idle"},"source":"This is an outlier remover in the target variable","outputs":[],"execution_count":null,"cell_type":"markdown"},{"metadata":{"_uuid":"3f5f5be3ab30f1537391bc02f266096a22ee113f","collapsed":false,"trusted":false,"_cell_guid":"7e96e654-44a1-4da8-b96f-b38d163e55ca","_execution_state":"busy"},"source":"q1 = np.percentile(train['trip_duration'], 25)\nq3 = np.percentile(train['trip_duration'], 75)\n\niqr = q3 - q1\n\ntrain = train[ train['trip_duration'] <= q3 + 3.0*iqr]\n\ntrain = train[ q1 - 3.0*iqr <= train['trip_duration']]","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"44559ecc1845842d9ebdd3f3bc39702fa030c769","collapsed":false,"_cell_guid":"76c5167b-67a4-433d-a16b-a31e6b1e05fb","_execution_state":"idle"},"source":"Lets set up our training set.","outputs":[],"execution_count":null,"cell_type":"markdown"},{"metadata":{"_uuid":"28651dbf5bdc50cf4ae3cddf8478c3d529c7d343","collapsed":false,"trusted":false,"_cell_guid":"89635806-b19a-40a9-8bbe-c4d76a8b4b8f","_execution_state":"busy"},"source":"labels = train.pop('trip_duration')\ntrain.drop([\"id\", \"dropoff_datetime\"], axis=1, inplace = True)\n\nsub = pd.DataFrame( columns = ['id', 'trip_duration'])\nsub['id'] = test.pop('id')","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"cf6537e464b9286609fac39e5a530f117cb05e87","collapsed":false,"_cell_guid":"7c54e309-d0bb-4825-89cc-016b5d45fbf8","_execution_state":"idle"},"source":"Label encode all objects.","outputs":[],"execution_count":null,"cell_type":"markdown"},{"metadata":{"_uuid":"358284cdb68f6270897e48b6a3cee50a98d24046","collapsed":false,"trusted":false,"_cell_guid":"2114595c-6509-48e1-9907-0244bc5babde","_execution_state":"busy"},"source":"from sklearn.preprocessing import LabelEncoder\nfor col in train.columns:\n    if train[col].dtype == 'object':\n        le = LabelEncoder()\n        \n        le.fit(train[col])\n        \n        train[col] = le.transform(train[col])\n        test[col] = le.transform(test[col])","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"419c6986dd8511f918d03e80559450e98dc4116b","collapsed":false,"_cell_guid":"8fc04580-9f54-41f3-a7de-1ba4fa93f167","_execution_state":"idle"},"source":"## Model Construction ##\n\nFirstly, since we will be using two models in particular, lets create a method to help us store our predictions.","outputs":[],"execution_count":null,"cell_type":"markdown"},{"metadata":{"_uuid":"a59e11e03c9a46f5f89b0a6ede1912e7741d5928","collapsed":false,"trusted":false,"_cell_guid":"faa37d35-36b0-4f5a-925a-a041bf8add64","_execution_state":"busy"},"source":"def modelPredict( est, train, test, labels):\n    from sklearn.model_selection import train_test_split\n    \n    #uncomment for model selection\n    \n    #x_train, x_val, y_train, y_val = train_test_split(train, labels)\n    \n    #eval_set = [(x_val, y_val)]\n    \n    #est.fit(x_train, y_train, eval_set = eval_set, early_stopping_rounds = 100)\n    \n    print ('Making model...')\n    \n    est.fit( train, labels )\n    \n    print ('Done!')\n    \n    return [est.predict( train ), est.predict( test ) ]","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"1e147e3a3aeb32a6e4be13cc394af790c3892c83","collapsed":false,"_cell_guid":"d1ca78d5-c1ca-43d1-b0b4-f7b9339cb51a","_execution_state":"idle"},"source":"These will be the models we will be using.","outputs":[],"execution_count":null,"cell_type":"markdown"},{"metadata":{"_uuid":"79e4876308f87accc2c6fd6e93b620cb29a1fb58","collapsed":false,"trusted":false,"_cell_guid":"4dc55be4-ed62-4f7b-b9d8-09154a9f63b1","_execution_state":"busy"},"source":"from xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\n#these were cross-validated using the current features and some intuition\nxgb = XGBRegressor(max_depth = 7, learning_rate = 1e-1, n_estimators = 479,\n                      subsample = 0.8, n_jobs = 4)\n\nlgbm = LGBMRegressor(max_depth = 7, learning_rate = 1e-1, n_estimators = 582,\n                      subsample = 0.8, nthread = 4)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"c040972597cfeeed68fecdde957eb0fa4a3d5994","collapsed":false,"_cell_guid":"099bfc63-e0ed-47ed-842c-37fb09fc84f6","_execution_state":"idle"},"source":"We need to store our results somewhere.","outputs":[],"execution_count":null,"cell_type":"markdown"},{"metadata":{"_uuid":"53737e72ed2c6c7846a2b28d4be929f8f6dcbc68","collapsed":false,"trusted":false,"_cell_guid":"048a3c88-86d6-4cd9-be9d-a787fa4dec38","_execution_state":"busy"},"source":"train_preds = pd.DataFrame( columns = ['xgb_pred', 'lgbm_pred'] )\ntest_preds = pd.DataFrame( columns = ['xgb_pred', 'lgbm_pred'] )","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"0266a7159db151b804c68e0a45268b702418ea2d","collapsed":false,"_cell_guid":"ac7cdb09-bb55-44ea-97c4-14adba7cce5d","_execution_state":"idle"},"source":"Now we are off to making our predictions.","outputs":[],"execution_count":null,"cell_type":"markdown"},{"metadata":{"_uuid":"ef59bfe925fb1013de0bf216d70e8c7727be432a","collapsed":false,"trusted":false,"_cell_guid":"a4c23b2c-4c7c-409b-9f8d-b08e32721605","_execution_state":"busy"},"source":"preds = modelPredict(xgb, train, test, labels)\n\ntrain_preds['xgb_pred'] = preds[0]\ntest_preds['xgb_pred'] = preds[1]\n\nprint ('On to the next one')\n\npreds = modelPredict(lgbm, train, test, labels)\n\ntrain_preds['lgbm_pred'] = preds[0]\ntest_preds['lgbm_pred'] = preds[1]\n\nprint ('Finished')","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"3d2698ac863dbb35073c5b36e3fc9eb191eed6b1","collapsed":false,"_cell_guid":"f5df73d2-9c40-48fe-9a2b-094a4af72295","_execution_state":"idle"},"source":"## Error Analysis ##\nLets create a 3D plot to visualize how linear, or nonlinear, our place of predictions to the target variable is.","outputs":[],"execution_count":null,"cell_type":"markdown"},{"metadata":{"_uuid":"db4a652e9bc68287353728b16f1ce58aecf2cd04","collapsed":false,"trusted":false,"_cell_guid":"58e2bf93-0f53-4737-aeb6-e0c8e770e138","_execution_state":"busy"},"source":"from mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.pyplot as plt\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\n\nax.scatter(train_preds['xgb_pred'], train_preds['lgbm_pred'], labels)\n\nplt.show()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"73beac766b32a6d1870f5ab26a8189530fb173f2","collapsed":false,"_cell_guid":"39b34217-95b0-4dfe-8fdd-64a3b32e293c","_execution_state":"idle"},"source":"The graph above does not show us any obvious skews in the predictions. Lets look at the plots per prediction now.","outputs":[],"execution_count":null,"cell_type":"markdown"},{"metadata":{"_uuid":"efef4885d4334b6d68a03994a85bafd248493201","collapsed":false,"trusted":false,"_cell_guid":"3ef69f42-ea48-4d22-8976-1c9639a52053","_execution_state":"busy"},"source":"fig = plt.figure()\nax = fig.add_subplot(111)\n\nax.scatter(train_preds['xgb_pred'], labels, c='b', marker=\"s\", label='xgb')\nax.scatter(train_preds['lgbm_pred'], labels, c='r', marker=\"o\", label='lbgm')\n\nplt.legend()\n\nplt.show()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"8017c7c6673ac009c5f14302241ca4a35b6b5098","collapsed":false,"_cell_guid":"028071ac-c063-4e89-9b65-d751e4452ce1","_execution_state":"idle"},"source":"It seems our predictions overlap each other. Lets plot the residuals side by side.","outputs":[],"execution_count":null,"cell_type":"markdown"},{"metadata":{"_uuid":"f1dc9d7b3a60127817568b7614d33f957df450ea","collapsed":false,"trusted":false,"_cell_guid":"33542fed-8be5-481d-85f6-e436a31d04f0","_execution_state":"busy"},"source":"fig = plt.figure()\nax = fig.add_subplot(111)\n\nres = train_preds['xgb_pred'].values - labels.values\nax.scatter(train_preds['xgb_pred'], res, c='b', marker=\"s\", label='xgb_res')\n\nres = train_preds['lgbm_pred'].values - labels.values\nax.scatter(train_preds['lgbm_pred'], res, c='r', marker=\"o\", label='lbgm_res')\n\nplt.legend()\n\nplt.show()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"a8c1e2abccb28342d02f9544eb1f69beaedce0e7","collapsed":false,"_cell_guid":"61c25766-3eec-4ab1-92ac-70940e83dc90","_execution_state":"idle"},"source":"There is an obvious bias according to the residual plot.  There should be no pattern present in the residual plot.  It seems there must include more features or feature engineering.\n\nFor now, we will simply average our predictions.","outputs":[],"execution_count":null,"cell_type":"markdown"},{"metadata":{"_uuid":"ecd27059358a2b956156799653554d27f3084533","collapsed":false,"trusted":false,"_cell_guid":"b69bc794-8ba4-4f35-8c52-19bd64aa921b","_execution_state":"busy"},"source":"preds = test_preds.mean(axis=1)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"297d4be76bc4c085b1dc4dfc3ec1064ee6f639aa","collapsed":false,"trusted":false,"_cell_guid":"4a7972c6-37d5-48b3-923b-26d903ac8761","_execution_state":"busy"},"source":"sub['trip_duration'] = np.expm1(preds)\nsub.to_csv('submission.csv', index=False)","outputs":[],"execution_count":null,"cell_type":"code"}]}