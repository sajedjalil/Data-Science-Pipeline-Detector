{"nbformat":4,"nbformat_minor":1,"cells":[{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_cell_guid":"b3d5bbb2-18e6-4c8f-bd87-43dcdecc9d3d","collapsed":true,"_uuid":"7dd7d54e32957387a59a1c80a19283bb5f8b4f2e"},"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n%matplotlib inline\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom datetime import timedelta\nimport datetime as dt\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = [16, 10]\nimport seaborn as sns\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import MiniBatchKMeans\n"},{"source":"Loading Data","cell_type":"markdown","metadata":{"_cell_guid":"d17f54ec-3a9e-4ea2-af83-f07d4df8278f","_uuid":"29ed5c1bd54a37d9a58c067fde68dd0166ba9f77"}},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_cell_guid":"d8ed3278-c4ce-411e-823e-8771922f3bf0","collapsed":true,"_uuid":"869d7d0bfbc389dcdefb545ef7bb3fb2c3ba5a8f"},"source":"t0 = dt.datetime.now()\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\nsample_submission = pd.read_csv('../input/sample_submission.csv')"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_cell_guid":"036f4ca1-7f7c-43d4-b281-b702c3a0ca5c","_uuid":"63c77417decc521826881b5bab28ed9c8ea05dfb"},"source":"train['pickup_datetime'] = pd.to_datetime(train.pickup_datetime)\ntest['pickup_datetime'] = pd.to_datetime(test.pickup_datetime)\ntrain.loc[:, 'pickup_date'] = train['pickup_datetime'].dt.date\ntest.loc[:, 'pickup_date'] = test['pickup_datetime'].dt.date\ntrain['dropoff_datetime'] = pd.to_datetime(train.dropoff_datetime)\ntrain['store_and_fwd_flag'] = 1 * (train.store_and_fwd_flag.values == 'Y')\ntest['store_and_fwd_flag'] = 1 * (test.store_and_fwd_flag.values == 'Y')\ntrain['check_trip_duration'] = (train['dropoff_datetime'] - train['pickup_datetime']).map(lambda x: x.total_seconds())\nduration_difference = train[np.abs(train['check_trip_duration'].values  - train['trip_duration'].values) > 1]\nprint('Trip_duration and datetimes are ok.') if len(duration_difference[['pickup_datetime', 'dropoff_datetime', 'trip_duration', 'check_trip_duration']]) == 0 else print('Ooops.')"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_cell_guid":"af9c0559-e4fa-4fe4-9867-a3f0f07cb77d","collapsed":true,"_uuid":"579dc3c06d4ba4f08b1bdc5a248aa4679d536cf4"},"source":"# Let's compute pickup hour for each ride\ntrain['pickup_datetime'] = pd.to_datetime(train['pickup_datetime'])\ntrain['pickup_hour'] = train.pickup_datetime.dt.hour\ntrain['day_week'] = train.pickup_datetime.dt.weekday\n# Get pick up hour for test data as well\ntest['pickup_datetime'] = pd.to_datetime(test['pickup_datetime'])\ntest['pickup_hour'] = test.pickup_datetime.dt.hour\ntest['day_week'] = test.pickup_datetime.dt.weekday"},{"source":"We could logtransform our target label and use RMSE during training.","cell_type":"markdown","metadata":{"_cell_guid":"f0a6e613-e014-471d-b018-6378c852a2b9","_uuid":"23197e2a09d584cca0b208f4dcc65374b73795d6"}},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_cell_guid":"465e9af2-9a25-4715-8ed8-24e10573dc52","_uuid":"8582ddbeed9b5863895cd3e2683ed6e81f8309e0"},"source":"train['log_trip_duration'] = np.log(train['trip_duration'].values + 1)\nsns.distplot(train['log_trip_duration'].values, bins=100)\nplt.xlabel('log(trip_duration)')\nplt.ylabel('number of train records')\nplt.show()"},{"source":"First let's check the train test split. It helps to decide our validation strategy and gives ideas about feature engineering.","cell_type":"markdown","metadata":{"_cell_guid":"c818a114-164d-41db-8cbb-713be6084897","_uuid":"0391f5ce0f2421c9b57995d78914635e3c0bf27e"}},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_cell_guid":"a6fc0163-5bf7-46d1-a872-7e6df80bb2a4","_uuid":"abc41ea3891d7e5282ef9b8bbd9c20c625d1dc1f"},"source":"#from pandas.plotting import parallel_coordinates\n#dct = {'training': train.groupby('pickup_date').count()[['id']], \n#       'testing': train.groupby('pickup_date').count()[['id']] }\n\n#df = pd.DataFrame.from_dict(dct)\n\n#parallel_coordinates(df, 'training')\n\n#df = pd.DataFrame( {train.groupby('pickup_date').count()[['id']] columns=['a', 'b', 'c', 'd']})\n\n#df.plot.area();\n#fig, ax = plt.subplots(ncols=2, sharex=True, sharey=True)\n\nplt.plot((train.groupby('pickup_date').count()[['id']]), label='train', color = 'g')\nplt.plot((test.groupby('pickup_date').count()[['id']]), label='train', color = 'r')\n\nplt.title('Train and test period complete overlap.')\nplt.legend(loc=0)\nplt.ylabel('number of records')\nplt.show()"},{"source":"\nremove obvious outliers and convert everything to sensible units","cell_type":"markdown","metadata":{"_cell_guid":"83bc3684-ba66-4d56-90aa-d0a44b13b28e","_uuid":"2270617390ce78847a5bba3a2783216ebf679570"}},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_cell_guid":"f43e8a52-6204-4c59-9ff5-fdab84b90fd5","collapsed":true,"_uuid":"26b8727bc64f1505ef5b2ed5504e1ef80bd0a3db"},"source":"def bearing_array(lat1, lng1, lat2, lng2):\n    AVG_EARTH_RADIUS = 6371  # in km\n    lng_delta_rad = np.radians(lng2 - lng1)\n    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n    y = np.sin(lng_delta_rad) * np.cos(lat2)\n    x = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(lng_delta_rad)\n    return np.degrees(np.arctan2(y, x))\n\ntrain.loc[:, 'direction'] = bearing_array(train['pickup_latitude'].values, train['pickup_longitude'].values, \n                                          train['dropoff_latitude'].values, train['dropoff_longitude'].values)\n\ntest.loc[:, 'direction'] = bearing_array(test['pickup_latitude'].values, test['pickup_longitude'].values, \n                                         test['dropoff_latitude'].values, test['dropoff_longitude'].values)"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_cell_guid":"546deafa-bdfe-4d12-86f0-1e2b3c0e716a","collapsed":true,"_uuid":"3fd8c179a334556d5096b5e86dc15727b9fd9dbf"},"source":"allLat  = np.array(list(train['pickup_latitude'])  + list(train['dropoff_latitude']))\nallLong = np.array(list(train['pickup_longitude']) + list(train['dropoff_longitude']))\n\nlongLimits = [np.percentile(allLong, 0.3), np.percentile(allLong, 99.7)]\nlatLimits  = [np.percentile(allLat , 0.3), np.percentile(allLat , 99.7)]\ndurLimits  = [np.percentile(train['trip_duration'], 0.4), np.percentile(train['trip_duration'], 99.7)]\n\ntrain = train[(train['pickup_latitude']   >= latLimits[0] ) & (train['pickup_latitude']   <= latLimits[1]) ]\ntrain = train[(train['dropoff_latitude']  >= latLimits[0] ) & (train['dropoff_latitude']  <= latLimits[1]) ]\ntrain = train[(train['pickup_longitude']  >= longLimits[0]) & (train['pickup_longitude']  <= longLimits[1])]\ntrain = train[(train['dropoff_longitude'] >= longLimits[0]) & (train['dropoff_longitude'] <= longLimits[1])]\ntrain = train[(train['trip_duration']     >= durLimits[0] ) & (train['trip_duration']     <= durLimits[1]) ]\ntrain = train.reset_index(drop=True)\n\nallLat  = np.array(list(train['pickup_latitude'])  + list(train['dropoff_latitude']))\nallLong = np.array(list(train['pickup_longitude']) + list(train['dropoff_longitude']))\n\n# convert fields to sensible units\nmedianLat  = np.percentile(allLat,50)\nmedianLong = np.percentile(allLong,50)\n\nlatMultiplier  = 111.32\nlongMultiplier = np.cos(medianLat*(np.pi/180.0)) * 111.32\n\ntrain['duration [min]'] = train['trip_duration']/60.0\ntrain['src lat [km]']   = latMultiplier  * (train['pickup_latitude']   - medianLat)\ntrain['src long [km]']  = longMultiplier * (train['pickup_longitude']  - medianLong)\ntrain['dst lat [km]']   = latMultiplier  * (train['dropoff_latitude']  - medianLat)\ntrain['dst long [km]']  = longMultiplier * (train['dropoff_longitude'] - medianLong)\n\nallLat  = np.array(list(train['src lat [km]'])  + list(train['dst lat [km]']))\nallLong = np.array(list(train['src long [km]']) + list(train['dst long [km]']))"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_cell_guid":"b9bc4e2f-6232-43be-aea4-d55991512b79","scrolled":true,"_uuid":"43001d6fd01a3695ad78e3d83b60516e4f17d38e"},"source":"# show the log density of pickup and dropoff locations\nimageSize = (700,700)\nlongRange = [-5,19]\nlatRange = [-13,11]\n\nallLatInds  = imageSize[0] - (imageSize[0] * (allLat  - latRange[0])  / (latRange[1]  - latRange[0]) ).astype(int)\nallLongInds =                (imageSize[1] * (allLong - longRange[0]) / (longRange[1] - longRange[0])).astype(int)\n\nlocationDensityImage = np.zeros(imageSize)\nfor latInd, longInd in zip(allLatInds,allLongInds):\n    locationDensityImage[latInd,longInd] += 1\n\nfig, ax = plt.subplots(nrows=1,ncols=1,figsize=(12,12))\nax.imshow(np.log(locationDensityImage+1),cmap='magma')\nax.set_axis_off()"},{"source":"Cluster The Trips and Look at their distribution\nevery trip is essentially made up of five major attributes: pickup and dropoff locations and the trip duration. We have also included direction as an additional measure to see its effect. \n\nlet's cluster all 1.4 million trips to 80 stereotypical template trips and then look at the distribution of this \"bag of trips\" and how it changes over time","cell_type":"markdown","metadata":{"_cell_guid":"a9ef0182-ac7c-4f25-90ec-1d0253d8476f","_uuid":"fbcda387dfd9d501ec36b88ce2a4b30586101a69"}},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_cell_guid":"ab00b3fa-36e3-49c2-9398-b66ce775b89a","_uuid":"482bb3c35b089b31186f1cb0ba47274cfa3252b8"},"source":"from sklearn import decomposition\nfrom scipy import stats\nfrom sklearn import cluster\n\n\ntripAttributes = np.array(train.loc[:,['src lat [km]','src long [km]','dst lat [km]','dst long [km]','duration [min]', 'direction']])\nmeanTripAttr = tripAttributes.mean(axis=0)\nstdTripAttr  = tripAttributes.std(axis=0)\ntripAttributes = stats.zscore(tripAttributes, axis=0)\n\n# choose number of clusters\n\n\nnumClusters = 60\nTripKmeansModel = cluster.MiniBatchKMeans(n_clusters=numClusters, batch_size=120000, n_init=100)\nclusterInds = TripKmeansModel.fit_predict(tripAttributes)\n\nclusterTotalCounts, _ = np.histogram(clusterInds, bins=numClusters)\nsortedClusterInds = np.flipud(np.argsort(clusterTotalCounts))\n\nplt.figure(figsize=(12,4)); plt.title('Cluster Histogram of all trip')\nplt.bar(range(1,numClusters+1),clusterTotalCounts[sortedClusterInds])\nplt.ylabel('Frequency [counts]'); plt.xlabel('Cluster index (sorted by cluster frequency)')\nplt.xlim(0,numClusters+1)"},{"source":"To visualize where the pick up and drop off locations are located, two circles are plotted. The yellow ones indicate the source coordinates an the green ones are the detinations. Credit goes to Selfish Gene 's Kernel for this visualization","cell_type":"markdown","metadata":{"_cell_guid":"05a7a544-0d0f-4810-9e65-37f1b195348e","_uuid":"0e7f35ca7d2eacab1717333f05caf676e78245b6"}},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_cell_guid":"686805b0-9998-4b47-9f20-103764badcae","_uuid":"69ef22d0b2f78285b71f5c41859752c01fed4223"},"source":"#%% show the templeate trips on the map \n\ndef ConvertToImageCoords(latCoord, longCoord, latRange, longRange, imageSize):\n    latInds  = imageSize[0] - (imageSize[0] * (latCoord  - latRange[0])  / (latRange[1]  - latRange[0]) ).astype(int)\n    longInds =                (imageSize[1] * (longCoord - longRange[0]) / (longRange[1] - longRange[0])).astype(int)\n\n    return latInds, longInds\n\ntemplateTrips = TripKmeansModel.cluster_centers_ * np.tile(stdTripAttr,(numClusters,1)) + np.tile(meanTripAttr,(numClusters,1))\n\nsrcCoords = templateTrips[:,:2]\ndstCoords = templateTrips[:,2:4]\n\nsrcImCoords = ConvertToImageCoords(srcCoords[:,0],srcCoords[:,1], latRange, longRange, imageSize)\ndstImCoords = ConvertToImageCoords(dstCoords[:,0],dstCoords[:,1], latRange, longRange, imageSize)\n\nplt.figure(figsize=(12,12))\nplt.imshow(np.log(locationDensityImage+1),cmap='magma'); plt.grid('off')\nplt.scatter(srcImCoords[1],srcImCoords[0],c='y',s=200,alpha=0.9)\nplt.scatter(dstImCoords[1],dstImCoords[0],c='g',s=200,alpha=0.9)\n\nfor i in range(len(srcImCoords[0])):\n    plt.arrow(srcImCoords[1][i],srcImCoords[0][i], dstImCoords[1][i]-srcImCoords[1][i], dstImCoords[0][i]-srcImCoords[0][i], \n              edgecolor='c', facecolor='c', width=2.4,alpha=0.6,head_width=10.0,head_length=10.0,length_includes_head=True)"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_cell_guid":"0666d7cc-ad3e-4983-b9a0-ab9b7b480301","_uuid":"88a9d30934154d4faf3157d2b4cba727404d271a"},"source":"#color = sns.color_palette()\n\ngrouped_df = train.groupby('pickup_hour')['trip_duration'].aggregate(np.mean).reset_index()\nplt.figure(figsize=(12,8))\nsns.pointplot(grouped_df.pickup_hour.values, grouped_df.trip_duration.values, alpha=0.8, \n              color='k' )\nplt.ylabel('Average trip duration', fontsize=12)\nplt.xlabel('Pickup Hour', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.show()"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_cell_guid":"357ab1d6-8c60-40ab-8a02-a9badd46dcc9","_uuid":"27401e2d930e309ed2fcb33cb4e2db86ddd4b252"},"source":"sns.set(style=\"ticks\")\nsns.set_context(\"poster\")\nsns.boxplot(x=\"day_week\", y=\"trip_duration\", hue=\"vendor_id\", data=train\n             )\nplt.ylim(0, 6000)\nsns.despine(offset=10, trim=True)\ntrain.trip_duration.max()"},{"source":"### Categorical Features Encoding","cell_type":"markdown","metadata":{"_cell_guid":"aeeb4252-8ec5-4bf8-8161-732aa4b379a2","_uuid":"7e7aa03c80de1d4f1887f69eff0452a74ecbccb1"}},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_cell_guid":"e43407e9-b927-4bed-8d84-882bb610990f","collapsed":true,"_uuid":"77d4ea548157e02c3756ef6797641ed48e87cd34"},"source":"from sklearn import model_selection, preprocessing\nimport xgboost as xgb\n\nfor f in train.columns:\n    if train[f].dtype=='object':\n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(train[f].values)) \n        train[f] = lbl.transform(list(train[f].values))\n"},{"source":"### Feature Importance Calculation","cell_type":"markdown","metadata":{"_cell_guid":"a34aea8b-74f5-4a65-84df-8691c084b1b9","_uuid":"aebf30f0b671e281472cd02398724de269fceffb"}},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_cell_guid":"273339e3-5e9a-4ffc-b1b2-ce31a27d3190","collapsed":true,"_uuid":"f7c07291ee1270fa2a2ef5db3ac7c797a0de0aff"},"source":"train_y = train.trip_duration.values\ntrain_X = train.drop([\"id\", \"dropoff_datetime\", \"pickup_datetime\", \"trip_duration\"], axis=1)\n"},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_cell_guid":"20ab5d60-8178-420d-be95-3de4c36ffb09","_uuid":"d3974e19d52652e7714f9b7348e7993adc704920"},"source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.grid_search import GridSearchCV\nimport matplotlib.pyplot as plt\n\nrf_clf = RandomForestRegressor(max_depth=15,n_estimators=100, min_samples_leaf=75,\n                                  min_samples_split=100, random_state=10)\n\n# Train the model\nrf_clf.fit(train_X, train_y)"},{"source":"Please keep checking back, It's a work in progress :)","cell_type":"markdown","metadata":{"_cell_guid":"4fcda865-db36-4419-b2f3-053ff73e37f0","_uuid":"26eb957a9e6dad715ddafad43edcb6e8702bdebf"}},{"execution_count":null,"outputs":[],"cell_type":"code","metadata":{"_cell_guid":"382bad18-d531-44a1-8228-b2f3699753a4","collapsed":true,"_uuid":"be96f6d54eccd83c7f52ba2af34707f0d194f9b9"},"source":""}],"metadata":{"language_info":{"mimetype":"text/x-python","codemirror_mode":{"version":3,"name":"ipython"},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py","version":"3.6.1","name":"python"},"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"}}}