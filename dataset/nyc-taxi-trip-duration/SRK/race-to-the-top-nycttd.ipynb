{"cells":[{"cell_type":"markdown","source":"The objective of the notebook is to try and get to the top of the LB (hopefully.!).\n\nWith just two weeks remaining for the competition to end, this will be a perfect start (as the competitions are generally more harder in the final days with all the forum informations, exploratory notebooks, high scoring kernels and so on).\n\nThe general flow is as follows:\n* Understanding the data\n* Validation Strategy\n* Create a baseline model with basic variables\n* Feature Engineering\n* Building various models and parameter tuning\n* Ensembling / stacking.\n\n**Competition Objective:**\n\nThe objective of the competition is to predict the trip time duration of the Taxis in New Tork City.\n\n**Understanding the data:**\n\nLet us import the dataset and have a sneak peak at what kind of data is present inside.\n","metadata":{"_cell_guid":"9a02f0d4-28b6-4ffe-9411-7de921443255","_uuid":"3b442fc985940c2f992c2ab74e89e42d2937987e"}},{"outputs":[],"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import model_selection, preprocessing, metrics\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom haversine import haversine\ncolor = sns.color_palette()\n\n%matplotlib inline\n\npd.options.mode.chained_assignment = None  # default='warn'\npd.set_option('display.max_columns', 500)","metadata":{"_cell_guid":"a1ca8b83-fb5c-4c5b-80ac-a796ef30cb43","_uuid":"a5c491f5b30e222e796a6c9252ffc25241418bde","collapsed":true},"execution_count":2},{"cell_type":"markdown","source":"Reading the dataset into pandas dataframe and looking at the top few rows.","metadata":{"_cell_guid":"a0988646-8018-4f27-8ad2-280bffe35484","_uuid":"e99a048fed1beeb78a3d51a6248600f75318bf89"}},{"outputs":[],"cell_type":"code","source":"train_df = pd.read_csv('../input/nyc-taxi-trip-duration/train.csv', parse_dates=['pickup_datetime'])\ntest_df = pd.read_csv('../input/nyc-taxi-trip-duration/test.csv', parse_dates=['pickup_datetime'])\nprint(\"Train dataframe shape : \",train_df.shape)\nprint(\"Test dataframe shape : \", test_df.shape)","metadata":{"_cell_guid":"b44c4969-632f-4cbd-a45b-15e56558a216","_uuid":"f5540243de6ee69edfbca9f24516bfab73e39e8f"},"execution_count":3},{"outputs":[],"cell_type":"code","source":"train_df.head()","metadata":{"_cell_guid":"3fe43711-80d6-44e7-a62c-4d4fe22e0d10","_uuid":"5e647522cb09d03b353ef1dc79ac15459953edf4"},"execution_count":4},{"outputs":[],"cell_type":"code","source":"test_df.head()","metadata":{"_cell_guid":"3e40d0f5-4997-48aa-952e-13463d253b51","_uuid":"34619f804ca63363e4039118e4f09769a7b2548a"},"execution_count":5},{"cell_type":"markdown","source":"The columns are self-explanatory and two columns 'dropoff_datetime' and 'trip_duration' are not present in the test set. \n\n'trip_duration' is the column to predict and Root Mean Square Logarithmic Error is our error metric. So let us look at the log distribution of the target variable.","metadata":{"_cell_guid":"8666afd1-ba50-4942-8b41-af019666bf47","_uuid":"30038ba73a09e2c07b11b43b4bd69e138bc96bfd"}},{"outputs":[],"cell_type":"code","source":"train_df['log_trip_duration'] = np.log1p(train_df['trip_duration'].values)\n\nplt.figure(figsize=(8,6))\nplt.scatter(range(train_df.shape[0]), np.sort(train_df.log_trip_duration.values))\nplt.xlabel('index', fontsize=12)\nplt.ylabel('log_trip_duration', fontsize=12)\nplt.show()","metadata":{"_cell_guid":"ed20cd36-79f5-4ead-9e63-b22d1e443488","_uuid":"016564f3deda81afe1fa43b14faaed41cfe31cde"},"execution_count":6},{"cell_type":"markdown","source":"Looks like there are few outiers in the data. Let us check  the actual count of it.","metadata":{"_cell_guid":"0679cc00-b022-4105-ba09-84ba4c4e1c54","_uuid":"924d164d7128b07648734c843319a9ca438e0d4b"}},{"outputs":[],"cell_type":"code","source":"(train_df['log_trip_duration'] > 12).sum()","metadata":{"_cell_guid":"fed5432c-2eee-44d5-a795-0db672c73f7e","_uuid":"6b0d0ca5061cef911172b0e6e45f39e5660ced1f"},"execution_count":7},{"cell_type":"markdown","source":"I think 4 is a smaller value and let us not worry about it now (but probably need to check later if needed).\n\nNext step is to check whether there are any null values in the data. ","metadata":{"_cell_guid":"dd312e03-c4bc-421e-961e-9bef74ef1208","_uuid":"9cc84ed972bc1a00bdd63841666e7a51b5e0edce"}},{"outputs":[],"cell_type":"code","source":"null_count_df = train_df.isnull().sum(axis=0).reset_index()\nnull_count_df.columns = ['col_name', 'null_count']\nnull_count_df","metadata":{"_cell_guid":"5d84cfea-e81a-45e4-9a8e-35e183df4500","_uuid":"91138503734a0c80322af1d133411fa456f5e948"},"execution_count":8},{"outputs":[],"cell_type":"code","source":"null_count_df = test_df.isnull().sum(axis=0).reset_index()\nnull_count_df.columns = ['col_name', 'null_count']\nnull_count_df","metadata":{"_cell_guid":"a69dd2c0-0988-4b88-bcce-516127a19788","_uuid":"b659f9b4a95d2386c49c169177f5ac9a409ef97f"},"execution_count":9},{"cell_type":"markdown","source":"There are no missing values :)\n\n**Validation Strategy:**\n\nValidation strategy is very important because without a proper validation starategy, it will be very hard to evaluate the models against each other. \n\nSince dates are given as part of the dataset, it is essential to check whether the train and test datasets are from the same time period or different time period.","metadata":{"_cell_guid":"d0a0b2ff-37b6-4c0b-981c-edfd3a6f6f8e","_uuid":"6d9cb1b6885480175e0f20921760b4de91abcfa7"}},{"outputs":[],"cell_type":"code","source":"train_df['pickup_date'] = train_df['pickup_datetime'].dt.date\ntest_df['pickup_date'] = test_df['pickup_datetime'].dt.date\n\ncnt_srs = train_df['pickup_date'].value_counts()\nplt.figure(figsize=(12,4))\nax = plt.subplot(111)\nax.bar(cnt_srs.index, cnt_srs.values, alpha=0.8)\nax.xaxis_date()\nplt.xticks(rotation='vertical')\nplt.show()","metadata":{"_cell_guid":"0fcda763-ca87-48ba-b6f4-102f37b6a5f3","_uuid":"a82c97ae9cce1f1a7a84bad3356122ed00fc778b"},"execution_count":10},{"outputs":[],"cell_type":"code","source":"cnt_srs = test_df['pickup_date'].value_counts()\nplt.figure(figsize=(12,4))\nax = plt.subplot(111)\nax.bar(cnt_srs.index, cnt_srs.values, alpha=0.8)\nax.xaxis_date()\nplt.xticks(rotation='vertical')\nplt.show()","metadata":{"_cell_guid":"e2268da6-6c24-4b60-90e0-dabd50894823","_uuid":"1e35fce4c861bd514421c58885160ec65c80cfd8"},"execution_count":11},{"cell_type":"markdown","source":"Wow. The distributions are very similar and so we could potentially use the K-fold cross validation for our dataset. Please note that if the train and test datasets are from different time frames, kindly use time based validation.","metadata":{"_cell_guid":"906c18f2-67f8-4cee-8fdc-d9bcaaf0d00c","_uuid":"ed8ccab4827f7a7ee3b54da694e2b68876e50eff"}},{"cell_type":"markdown","source":"**Baseline Model:**\n\nNow that we have got an idea about the dataset, let us buid a baseline model using xgboost and check the performance. \n\nWe could create few basic variables from datetime column and convert the store_and_forward_flag to numeric.","metadata":{"_cell_guid":"8108e117-7668-4115-94a1-c1aed2e5e6b9","_uuid":"7500f2fc0589cfbc431059b95e24211e92a2bd7e"}},{"outputs":[],"cell_type":"code","source":"# day of the month #\ntrain_df['pickup_day'] = train_df['pickup_datetime'].dt.day\ntest_df['pickup_day'] = test_df['pickup_datetime'].dt.day\n\n# month of the year #\ntrain_df['pickup_month'] = train_df['pickup_datetime'].dt.month\ntest_df['pickup_month'] = test_df['pickup_datetime'].dt.month\n\n# hour of the day #\ntrain_df['pickup_hour'] = train_df['pickup_datetime'].dt.hour\ntest_df['pickup_hour'] = test_df['pickup_datetime'].dt.hour\n\n# Week of year #\ntrain_df[\"week_of_year\"] = train_df[\"pickup_datetime\"].dt.weekofyear\ntest_df[\"week_of_year\"] = test_df[\"pickup_datetime\"].dt.weekofyear\n\n# Day of week #\ntrain_df[\"day_of_week\"] = train_df[\"pickup_datetime\"].dt.weekday\ntest_df[\"day_of_week\"] = test_df[\"pickup_datetime\"].dt.weekday\n\n# Convert to numeric #\nmap_dict = {'N':0, 'Y':1}\ntrain_df['store_and_fwd_flag'] = train_df['store_and_fwd_flag'].map(map_dict)\ntest_df['store_and_fwd_flag'] = test_df['store_and_fwd_flag'].map(map_dict)","metadata":{"_cell_guid":"7a66ff11-380e-4565-b7f3-13d7de26f6f9","_uuid":"1b8237c2d3379eae6231069a7ebb9032de08762d","collapsed":true},"execution_count":12},{"outputs":[],"cell_type":"code","source":"# drop off the variables which are not needed #\ncols_to_drop = ['id', 'pickup_datetime', 'pickup_date']\ntrain_id = train_df['id'].values\ntest_id = test_df['id'].values\ntrain_y = train_df.log_trip_duration.values\ntrain_X = train_df.drop(cols_to_drop + ['dropoff_datetime', 'trip_duration', 'log_trip_duration'], axis=1)\ntest_X = test_df.drop(cols_to_drop, axis=1)","metadata":{"_cell_guid":"e01cf615-64e9-4904-a72d-bd8e034e4932","_uuid":"fd173e82a8f7f8498af57d367486fc0004303fbc","collapsed":true},"execution_count":14},{"cell_type":"markdown","source":"Let us write a helper function to run the xgboost model and light gbm model.","metadata":{"_cell_guid":"4ff320c5-2473-4083-a848-577fe13e66ba","_uuid":"1af7fd1ae917d937c29cef84677af663ea4dec00"}},{"outputs":[],"cell_type":"code","source":"def runXGB(train_X, train_y, val_X, val_y, test_X, eta=0.05, max_depth=5, min_child_weight=1, subsample=0.8, colsample=0.7, num_rounds=8000, early_stopping_rounds=50, seed_val=2017):\n    params = {}\n    params[\"objective\"] = \"reg:linear\"\n    params['eval_metric'] = \"rmse\"\n    params[\"eta\"] = eta\n    params[\"min_child_weight\"] = min_child_weight\n    params[\"subsample\"] = subsample\n    params[\"colsample_bytree\"] = colsample\n    params[\"silent\"] = 1\n    params[\"max_depth\"] = max_depth\n    params[\"seed\"] = seed_val\n    params[\"nthread\"] = -1\n\n    plst = list(params.items())\n    xgtrain = xgb.DMatrix(train_X, label=train_y)\n    xgval = xgb.DMatrix(val_X, label = val_y)\n    xgtest = xgb.DMatrix(test_X)\n    watchlist = [ (xgtrain,'train'), (xgval, 'test') ]\n    model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=early_stopping_rounds, verbose_eval=20)\n\n    pred_val = model.predict(xgval, ntree_limit=model.best_ntree_limit)\n    pred_test = model.predict(xgtest, ntree_limit=model.best_ntree_limit)\n\n    return pred_val, pred_test\n\ndef runLGB(train_X, train_y, val_X, val_y, test_X, eta=0.05, max_depth=5, min_child_weight=1, subsample=0.8, colsample=0.7, num_rounds=8000, early_stopping_rounds=50, seed_val=2017):\n    params = {}\n    params[\"objective\"] = \"regression\"\n    params['metric'] = \"l2_root\"\n    params[\"learning_rate\"] = eta\n    params[\"min_child_weight\"] = min_child_weight\n    params[\"bagging_fraction\"] = subsample\n    params[\"bagging_seed\"] = seed_val\n    params[\"feature_fraction\"] = colsample\n    params[\"verbosity\"] = 0\n    params[\"max_depth\"] = max_depth\n    params[\"nthread\"] = -1\n\n    lgtrain = lgb.Dataset(train_X, label=train_y)\n    lgval = lgb.Dataset(val_X, label = val_y)\n    model = lgb.train(params, lgtrain, num_rounds, valid_sets=lgval, early_stopping_rounds=early_stopping_rounds, verbose_eval=20)\n\n    pred_val = model.predict(val_X, num_iteration=model.best_iteration)\n    pred_test = model.predict(test_X, num_iteration=model.best_iteration)\n\n    return pred_val, pred_test, model","metadata":{"_cell_guid":"129e6b38-058a-47e6-a582-3a3d49aa6d5c","_uuid":"36ac0b129246a8b1b8a94081b893b52942cffa10","collapsed":true},"execution_count":15},{"cell_type":"markdown","source":"Now let us build the baseline model using K-fold cross validation and save the scores in a csv file so as to build ensembles / stacking models later.\n\nPlease increase the number of rounds ('num_rounds') to a high value (1000) and then run the model in local. Using just 10 rounds here ","metadata":{"_cell_guid":"721477f4-70fe-49aa-ab48-b53859daecac","_uuid":"8a32febd905c631408e9deba932661db7ee07379"}},{"outputs":[],"cell_type":"code","source":"# Increase the num_rounds parameter to a higher value (1000) and run the model #\nkf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\ncv_scores = []\npred_test_full = 0\npred_val_full = np.zeros(train_df.shape[0])\nfor dev_index, val_index in kf.split(train_X):\n    dev_X, val_X = train_X.ix[dev_index], train_X.ix[val_index]\n    dev_y, val_y = train_y[dev_index], train_y[val_index]\n    pred_val, pred_test, model = runLGB(dev_X, dev_y, val_X, val_y, test_X, num_rounds=5, max_depth=8, eta=0.3)\n    pred_val_full[val_index] = pred_val\n    pred_test_full += pred_test\n    cv_scores.append(np.sqrt(metrics.mean_squared_error(val_y, pred_val)))\nprint(cv_scores)\nprint(\"Mean CV score : \",np.mean(cv_scores))\n\npred_test_full = pred_test_full / 5.\npred_test_full = np.expm1(pred_test_full)\npred_val_full = np.expm1(pred_val_full)\n\n# saving train predictions for ensemble #\ntrain_pred_df = pd.DataFrame({'id':train_id})\ntrain_pred_df['trip_duration'] = pred_val_full\ntrain_pred_df.to_csv(\"train_preds_lgb_baseline.csv\", index=False)\n\n# saving test predictions for ensemble #\ntest_pred_df = pd.DataFrame({'id':test_id})\ntest_pred_df['trip_duration'] = pred_test_full\ntest_pred_df.to_csv(\"test_preds_lgb_baseline.csv\", index=False)\n","metadata":{"_cell_guid":"705ffd18-1649-48d6-9a4d-535d293c54db","_uuid":"c4c215237c7a8086f8f1b8ae4c23fe66bc8e86e3"},"execution_count":16},{"cell_type":"markdown","source":"**The mean cv score (when trained offline with 1000 rounds) is 0.4147 and the LB score is 0.41412 (around rank 400 which is not bad for a baseline model)**. \n\nThis baseline model has helped us understand the following:\n1. Our overall setup is ready so that now we can make additional changes wherever needed\n2. The scores seem to be matching between cross validation and leaderboard and so we are probably good in that front\n\nSo some of the next steps are as follows:\n1. Feature engineering to create more useful variables.\n2. Ascertain that the cross validation and LB scores are in sync.\n3. Parameter tuning and building varied models.\n4. Ensembling / Stacking\n\n**Feature Engineering:**\n\nNow that we have our base model and the overall setup ready, let us dive into creating more variables (This is where I generally spend most of my time during competitions). It is a good idea to look at the feature importances of the model which we have built to understand what type of features are generally more predictive. So I got the feature importances from the light gbm model and is as follows:\n\n| Feature Name | Feature Importance |\n|:----------------|------------------------:|\n| dropoff_latitude | 0.1761 |\n| pickup_latitude | 0.1729 | \n| pickup_longitude | 0.172 |\n| dropoff_longitude | 0.1581 |\n| pickup_hour | 0.0999 | \n| pickup_day | 0.0611 |\n| week_of_year | 0.0538 |\n| day_of_week | 0.0499 | \n| pickup_month | 0.0203 | \n| passenger_count | 0.0203 | \n| vendor_id | 0.0132 |\n| store_and_fwd_flag | 0.0021 |\n\nThe important variables order seem to be the lat-lon co-ordinates followed by the time based variables. So some of my ideas to create new variables and the reasons are as follows\n1. Difference between pickup and dropoff latitude - will give an idea about the distance covered which could be predictive\n2. Difference between pickup and dropoff longitude - same reason as above\n3. Haversine distance between pickup and dropoff co-ordinates - to capture the actual distance travelled (commented out so as to use a faster function written by beluga)\n4. Pickup minute - since pickup hour is an important variable, the minute of pickup might well have been predictive\n5. Pickup day of year - same reason as above\n\nSo let us create these variables first and re-run it again.","metadata":{"_cell_guid":"7151a0d7-9825-45e3-a5c5-1f1280294d14","_uuid":"eae0e10201b25197b3dda281326bf53d4d2d8d4d","collapsed":true}},{"outputs":[],"cell_type":"code","source":"# difference between pickup and dropoff latitudes #\ntrain_df['lat_diff'] = train_df['pickup_latitude'] - train_df['dropoff_latitude']\ntest_df['lat_diff'] = test_df['pickup_latitude'] - test_df['dropoff_latitude']\n\n# difference between pickup and dropoff longitudes #\ntrain_df['lon_diff'] = train_df['pickup_longitude'] - train_df['dropoff_longitude']\ntest_df['lon_diff'] = test_df['pickup_longitude'] - test_df['dropoff_longitude']\n\n## compute the haversine distance ##\n#train_df['haversine_distance'] = train_df.apply(lambda row: haversine( (row['pickup_latitude'], row['pickup_longitude']), (row['dropoff_latitude'], row['dropoff_longitude']) ), axis=1)\n#test_df['haversine_distance'] = test_df.apply(lambda row: haversine( (row['pickup_latitude'], row['pickup_longitude']), (row['dropoff_latitude'], row['dropoff_longitude']) ), axis=1)\n\n# get the pickup minute of the trip #\ntrain_df['pickup_minute'] = train_df['pickup_datetime'].dt.minute\ntest_df['pickup_minute'] = test_df['pickup_datetime'].dt.minute\n\n# get the absolute value of time #\ntrain_df['pickup_dayofyear'] = train_df['pickup_datetime'].dt.dayofyear\ntest_df['pickup_dayofyear'] = test_df['pickup_datetime'].dt.dayofyear","metadata":{"_cell_guid":"8297ff05-10a4-4b9c-af0f-5a8b17338a77","_uuid":"dff776b1bd0727785b4337f04b419418bfda0c87","collapsed":true},"execution_count":17},{"cell_type":"markdown","source":"Now let us re-run the model again with these new variables and check the score.","metadata":{"_cell_guid":"df797b4b-b122-4df2-930f-78379cbd13c3","_uuid":"4530ed7731b4e3e0ea8a1d9481f5fe121d3f7f93"}},{"outputs":[],"cell_type":"code","source":"# drop off the variables which are not needed #\ncols_to_drop = ['id', 'pickup_datetime', 'pickup_date']\ntrain_id = train_df['id'].values\ntest_id = test_df['id'].values\ntrain_y = train_df.log_trip_duration.values\ntrain_X = train_df.drop(cols_to_drop + ['dropoff_datetime', 'trip_duration', 'log_trip_duration'], axis=1)\ntest_X = test_df.drop(cols_to_drop, axis=1)\n\n# Increase the num_rounds parameter to a higher value (1000) and run the model #\nkf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\ncv_scores = []\npred_test_full = 0\npred_val_full = np.zeros(train_df.shape[0])\nfor dev_index, val_index in kf.split(train_X):\n    dev_X, val_X = train_X.ix[dev_index], train_X.ix[val_index]\n    dev_y, val_y = train_y[dev_index], train_y[val_index]\n    pred_val, pred_test, model = runLGB(dev_X, dev_y, val_X, val_y, test_X, num_rounds=5, max_depth=8, eta=0.3)\n    pred_val_full[val_index] = pred_val\n    pred_test_full += pred_test\n    cv_scores.append(np.sqrt(metrics.mean_squared_error(val_y, pred_val)))\nprint(cv_scores)\nprint(\"Mean CV score : \",np.mean(cv_scores))\n\npred_test_full = pred_test_full / 5.\npred_test_full = np.expm1(pred_test_full)\npred_val_full = np.expm1(pred_val_full)\n\n# saving train predictions for ensemble #\ntrain_pred_df = pd.DataFrame({'id':train_id})\ntrain_pred_df['trip_duration'] = pred_val_full\ntrain_pred_df.to_csv(\"train_preds_lgb.csv\", index=False)\n\n# saving test predictions for ensemble #\ntest_pred_df = pd.DataFrame({'id':test_id})\ntest_pred_df['trip_duration'] = pred_test_full\ntest_pred_df.to_csv(\"test_preds_lgb.csv\", index=False)","metadata":{"_cell_guid":"fde08d71-3d49-4bf4-a203-759948f58eea","_uuid":"e16c980d0f2f5c714585bfc14093666f8771d03e","collapsed":true},"execution_count":16},{"cell_type":"markdown","source":"**The CV score of this new model is 0.3875 and the LB score is 0.38809. **\n\nSo the scores are pretty much in sync with each other. This is really a great news since in many competitions, they will be far away (in which cases, directional improvement can be looked at).\n\nOur new feature importances are as follows:\n\n|Feature name | Feature Importance|\n|:----------------|------------------:|\n| pickup_longitude | 0.1060 |\n| dropoff_latitude | 0.1055| \n| haversine_distance | 0.1007 | \n| dropoff_longitude | 0.0990 | \n| pickup_latitude | 0.0983 |\n| pickup_hour | 0.0890 |\n| lon_diff | 0.0860 | \n| lat_diff | 0.0815 | \n| pickup_dayofyear | 0.0560 | \n| pickup_minute | 0.0459 |\n| pickup_day | 0.0433 | \n| day_of_week | 0.0358 |\n| week_of_year | 0.0246 |\n| passenger_count | 0.0110 |\n| vendor_id | 0.0080 |\n| pickup_month | 0.0078 |\n| store_and_fwd_flag | 0.0005 |\n\nAs the next step, we can look into the forum posts / kernels  and see if there are any good feature ideas / implementations and try to add them as well into the features list.\n\n**More Features:**\n\nThis [excellent notebook](https://www.kaggle.com/gaborfodor/from-eda-to-the-top-lb-0-367) by beluga has a lot of wonderful features. I particularly liked the vectorized implementation of the distance  features. We shall also add them into our feature list.\n","metadata":{"_cell_guid":"607fdfb0-4995-453a-8269-44bec62b28af","_uuid":"c4a30c2e4e97e75fe18294bd5684994f49fc7343"}},{"outputs":[],"cell_type":"code","source":"def haversine_array(lat1, lng1, lat2, lng2):\n    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n    AVG_EARTH_RADIUS = 6371  # in km\n    lat = lat2 - lat1\n    lng = lng2 - lng1\n    d = np.sin(lat * 0.5) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(lng * 0.5) ** 2\n    h = 2 * AVG_EARTH_RADIUS * np.arcsin(np.sqrt(d))\n    return h\n\ndef bearing_array(lat1, lng1, lat2, lng2):\n    AVG_EARTH_RADIUS = 6371  # in km\n    lng_delta_rad = np.radians(lng2 - lng1)\n    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n    y = np.sin(lng_delta_rad) * np.cos(lat2)\n    x = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(lng_delta_rad)\n    return np.degrees(np.arctan2(y, x))\n\ntrain_df['haversine_distance'] = haversine_array(train_df['pickup_latitude'].values, \n                                                     train_df['pickup_longitude'].values, \n                                                     train_df['dropoff_latitude'].values, \n                                                     train_df['dropoff_longitude'].values)\ntest_df['haversine_distance'] = haversine_array(test_df['pickup_latitude'].values, \n                                                    test_df['pickup_longitude'].values, \n                                                    test_df['dropoff_latitude'].values, \n                                                    test_df['dropoff_longitude'].values)\n\ntrain_df['direction'] = bearing_array(train_df['pickup_latitude'].values, \n                                          train_df['pickup_longitude'].values, \n                                          train_df['dropoff_latitude'].values, \n                                          train_df['dropoff_longitude'].values)\ntest_df['direction'] = bearing_array(test_df['pickup_latitude'].values, \n                                         test_df['pickup_longitude'].values, \n                                         test_df['dropoff_latitude'].values, \n                                         test_df['dropoff_longitude'].values)","metadata":{"_cell_guid":"6e25fed8-173f-483a-8e67-ebd51a8293cd","_uuid":"12356d75e3423876a327004c754ebcd678cbabe5","collapsed":true},"execution_count":18},{"cell_type":"markdown","source":"Also we have this [wonderful osrm dataset](https://www.kaggle.com/oscarleo/new-york-city-taxi-with-osrm) published by oscarleo which gets the fastest and the second fastest routes for both train and test sets. Thank you oscarleo for this very helpful dataset. ","metadata":{"_cell_guid":"5d932952-bc8b-4253-90de-76615c6ab279","_uuid":"8fb88926f42e3a52b4ca73b79344f2df48d3d806"}},{"outputs":[],"cell_type":"code","source":"train_fr_part1 = pd.read_csv('../input/new-york-city-taxi-with-osrm/fastest_routes_train_part_1.csv', \n                             usecols=['id', 'total_distance', 'total_travel_time', 'number_of_steps'])\ntrain_fr_part2 = pd.read_csv('../input/new-york-city-taxi-with-osrm/fastest_routes_train_part_2.csv', \n                             usecols=['id', 'total_distance', 'total_travel_time', 'number_of_steps'])\ntest_fr = pd.read_csv('../input/new-york-city-taxi-with-osrm/fastest_routes_test.csv', \n                             usecols=['id', 'total_distance', 'total_travel_time', 'number_of_steps'])\ntrain_fr = pd.concat((train_fr_part1, train_fr_part2))\n\ntrain_df = train_df.merge(train_fr, how='left', on='id')\ntest_df = test_df.merge(test_fr, how='left', on='id')\n\ndel train_fr_part1, train_fr_part2, train_fr, test_fr\nimport gc; gc.collect()","metadata":{"_cell_guid":"9fed0b51-efdd-4816-8595-8c497d244d3a","_uuid":"7a314c2e229b508a81693001e9dbdd9605733d57"},"execution_count":20},{"cell_type":"markdown","source":"Why just the fastest route alone when the second fastest route is available? I tried adding the second fastest variables but it did not help much in the CV and so did not include them.\n\nLet us create few more variables by binning the latitudes and longitudes together.","metadata":{"_cell_guid":"bd85df18-adf4-4595-842b-687ceacd55f8","_uuid":"10bddd9644ff5f187e672a25e2775f5e17ba1bae","collapsed":true}},{"outputs":[],"cell_type":"code","source":"### some more new variables ###\ntrain_df['pickup_latitude_round3'] = np.round(train_df['pickup_latitude'],3)\ntest_df['pickup_latitude_round3'] = np.round(test_df['pickup_latitude'],3)\n\ntrain_df['pickup_longitude_round3'] = np.round(train_df['pickup_longitude'],3)\ntest_df['pickup_longitude_round3'] = np.round(test_df['pickup_longitude'],3)\n\ntrain_df['dropoff_latitude_round3'] = np.round(train_df['dropoff_latitude'],3)\ntest_df['dropoff_latitude_round3'] = np.round(test_df['dropoff_latitude'],3)\n\ntrain_df['dropoff_longitude_round3'] = np.round(train_df['dropoff_longitude'],3)\ntest_df['dropoff_longitude_round3'] = np.round(test_df['dropoff_longitude'],3)","metadata":{"collapsed":true},"execution_count":21},{"cell_type":"markdown","source":"Let us run our models again now to check the scores.","metadata":{}},{"outputs":[],"cell_type":"code","source":"# drop off the variables which are not needed #\ncols_to_drop = ['id', 'pickup_datetime', 'pickup_date']\ntrain_id = train_df['id'].values\ntest_id = test_df['id'].values\ntrain_y = train_df.log_trip_duration.values\ntrain_X = train_df.drop(cols_to_drop + ['dropoff_datetime', 'trip_duration', 'log_trip_duration'], axis=1)\ntest_X = test_df.drop(cols_to_drop, axis=1)\n\n# Increase the num_rounds parameter to a higher value (1000) and run the model #\nkf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\ncv_scores = []\npred_test_full = 0\npred_val_full = np.zeros(train_df.shape[0])\nfor dev_index, val_index in kf.split(train_X):\n    dev_X, val_X = train_X.ix[dev_index], train_X.ix[val_index]\n    dev_y, val_y = train_y[dev_index], train_y[val_index]\n    pred_val, pred_test, model = runLGB(dev_X, dev_y, val_X, val_y, test_X, num_rounds=5, max_depth=8, eta=0.3)\n    pred_val_full[val_index] = pred_val\n    pred_test_full += pred_test\n    cv_scores.append(np.sqrt(metrics.mean_squared_error(val_y, pred_val)))\nprint(cv_scores)\nprint(\"Mean CV score : \",np.mean(cv_scores))\n\npred_test_full = pred_test_full / 5.\npred_test_full = np.expm1(pred_test_full)\npred_val_full = np.expm1(pred_val_full)\n\n# saving train predictions for ensemble #\ntrain_pred_df = pd.DataFrame({'id':train_id})\ntrain_pred_df['trip_duration'] = pred_val_full\ntrain_pred_df.to_csv(\"train_preds_lgb.csv\", index=False)\n\n# saving test predictions for ensemble #\ntest_pred_df = pd.DataFrame({'id':test_id})\ntest_pred_df['trip_duration'] = pred_test_full\ntest_pred_df.to_csv(\"test_preds_lgb.csv\", index=False)","metadata":{},"execution_count":22},{"cell_type":"markdown","source":"**The CV score of this version is 0.3784 and the LB score is 0.3799. **\n\nThough the deviation between CV and LB deviated a little compared to previous submissions, I think it is okay since the deviation is not very high.\n\nNow that we are on the right path, let us try to create few more variables in the next version to further improve the score.","metadata":{}},{"cell_type":"markdown","source":"**More to come. Stay tuned.!**","metadata":{}}],"metadata":{"language_info":{"nbconvert_exporter":"python","version":"3.6.1","name":"python","pygments_lexer":"ipython3","file_extension":".py","codemirror_mode":{"version":3,"name":"ipython"},"mimetype":"text/x-python"},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"}},"nbformat_minor":1,"nbformat":4}