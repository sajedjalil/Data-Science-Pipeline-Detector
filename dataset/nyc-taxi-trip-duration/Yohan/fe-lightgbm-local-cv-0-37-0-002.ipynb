{"metadata":{"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"},"language_info":{"pygments_lexer":"ipython3","mimetype":"text/x-python","nbconvert_exporter":"python","version":"3.6.1","codemirror_mode":{"version":3,"name":"ipython"},"file_extension":".py","name":"python"}},"cells":[{"cell_type":"markdown","source":"Feature engineering and LightGBM framework.\nDatasets:\n\n - New York City Taxi Trip Duration (https://www.kaggle.com/c/nyc-taxi-trip-duration)\n - New York City Taxi with OSRM (https://www.kaggle.com/oscarleo/new-york-city-taxi-with-osrm)\n - Weather data in New York City - 2016 (https://www.kaggle.com/mathijs/weather-data-in-new-york-city-2016)\n - New York City Taxi Trip - Hourly Weather Data (https://www.kaggle.com/meinertsen/new-york-city-taxi-trip-hourly-weather-data)","metadata":{"_execution_state":"idle","_uuid":"1eae1a3c34c175a43f7bb3af33741102d07a63cc","_cell_guid":"b976893c-35bd-452c-be1e-e6f7212648ad"}},{"metadata":{"_execution_state":"idle","_uuid":"76715269bd776d2661cc2bcd1d3edf8dc14c4c8f","_cell_guid":"612bb5d3-1401-4cac-8fde-367c9b77132e","collapsed":true},"cell_type":"code","outputs":[],"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n# Plots\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n# Preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import MiniBatchKMeans\nimport gc \n# LightGBM framework\nimport lightgbm as lgb","execution_count":1},{"cell_type":"markdown","source":"## Data","metadata":{"_execution_state":"idle","_uuid":"1939bb61957ae2d666fc81f24a6abe1f2e229c3f","_cell_guid":"e2c5ecf6-d988-43b9-a599-154c0da2807d"}},{"metadata":{"_execution_state":"idle","_uuid":"abf982994a366fa6b9d6ef972546fd3993db1fba","_cell_guid":"6399403e-0535-477a-8c47-f72aee532313"},"cell_type":"code","outputs":[],"source":"### Get the data\n# Main dataset\ntrain = pd.read_csv('../input/nyc-taxi-trip-duration/train.csv')\ntest = pd.read_csv('../input/nyc-taxi-trip-duration/test.csv')\n# New York City Taxi with OSRM \n# (https://www.kaggle.com/mathijs/weather-data-in-new-york-city-2016)\ntrain_fastest_1 = pd.read_csv('../input/new-york-city-taxi-with-osrm/fastest_routes_train_part_1.csv')\ntrain_fastest_2 = pd.read_csv('../input/new-york-city-taxi-with-osrm/fastest_routes_train_part_2.csv')\ntest_fastest = pd.read_csv('../input/new-york-city-taxi-with-osrm/fastest_routes_test.csv')\n# Weather data in New York City - 2016\n# (https://www.kaggle.com/mathijs/weather-data-in-new-york-city-2016)\nweather = pd.read_csv('../input/weather-data-in-new-york-city-2016/weather_data_nyc_centralpark_2016.csv')\n# New York City Taxi Trip - Hourly Weather Data\n# (https://www.kaggle.com/meinertsen/new-york-city-taxi-trip-hourly-weather-data)\nweather_hour = pd.read_csv('../input/new-york-city-taxi-trip-hourly-weather-data/Weather.csv')\n# Train-validation split\ntrain, valid, _, _ = train_test_split(train, train.trip_duration, \n                                      test_size=0.2, random_state=2017)\n# Add set marker\ntrain['eval_set'] = 0; valid['eval_set'] = 1; test['eval_set'] = 2\ntest['trip_duration'] = np.nan; test['dropoff_datetime'] = np.nan\n# Glue tables\nframe = pd.concat([train, valid, test], axis=0)\nframe_fastest = pd.concat([train_fastest_1, train_fastest_2, test_fastest], axis = 0)\n\n### Memory optimization\n# Main dataframe\nframe.eval_set = frame.eval_set.astype(np.uint8)\nframe.passenger_count = frame.passenger_count.astype(np.int8)\nframe.store_and_fwd_flag = pd.get_dummies(frame['store_and_fwd_flag'], \n                                          prefix='store_and_fwd_flag', drop_first=True)\nframe.vendor_id = frame.vendor_id.astype(np.int8)\n# Weather dataframe\nweather.replace('T', 0.001, inplace=True)\nweather['date'] = pd.to_datetime(weather['date'], dayfirst=True).dt.date\nweather['average temperature'] = weather['average temperature'].astype(np.int64)\nweather['precipitation'] = weather['precipitation'].astype(np.float64)\nweather['snow fall'] = weather['snow fall'].astype(np.float64)\nweather['snow depth'] = weather['snow depth'].astype(np.float64)\n# Weather hourly dataframe\nweather_hour['Datetime'] = pd.to_datetime(weather_hour['pickup_datetime'], dayfirst=True)\nweather_hour['date'] = weather_hour.Datetime.dt.date\nweather_hour['hour'] = weather_hour.Datetime.dt.hour\nweather_hour['hour'] = weather_hour.hour.astype(np.int8)\nweather_hour['fog'] = weather_hour.fog.astype(np.int8)\nweather_hour = weather_hour[['date', 'hour', 'tempm', 'dewptm', 'hum', 'wspdm', \n                             'wdird', 'vism', 'pressurei', 'fog']]\ndel train, valid, test, train_fastest_1, train_fastest_2, test_fastest\ngc.collect();","execution_count":2},{"cell_type":"markdown","source":"## Adding features","metadata":{"_execution_state":"idle","_uuid":"c0ea4940e6140fccdf7c415f2b2767dc8f50707d","_cell_guid":"06d32536-3ae2-4443-89a9-8cd198729df0"}},{"metadata":{"_execution_state":"idle","_uuid":"e16fb1997acbb9a9e7e617adff5604ebc7b28cbe","_cell_guid":"599321d4-68a6-438f-a9ed-ae3cdcf2c6c7","collapsed":true},"cell_type":"code","outputs":[],"source":"# Define clusters\ndef clusters(df, pic=False):\n    coords = np.vstack((df[['pickup_longitude', 'pickup_latitude']].values,\n                        df[['dropoff_longitude', 'dropoff_latitude']].values))\n    sample_ind = np.random.permutation(len(coords))[:500000]\n    kmeans = MiniBatchKMeans(n_clusters=100, batch_size=10000).fit(coords[sample_ind])\n    cl_pickup = kmeans.predict(df[['pickup_longitude', 'pickup_latitude']])\n    cl_dropoff = kmeans.predict(df[['dropoff_longitude', 'dropoff_latitude']])\n    # If pic = True, show pictures\n    if pic:\n        plt.figure(figsize=(10,4))\n        plt.subplot(1,2,1)\n        plt.scatter(df.pickup_longitude.values, \n                    df.pickup_latitude.values, \n                    s=0.2, lw=0, c=cl_pickup, cmap='tab20', alpha=0.5)\n        plt.xlim(-74.03, -73.77); plt.ylim(40.63, 40.85)\n        plt.xlabel('pickup_longitude'); plt.ylabel('pickup_latitude')\n        plt.subplot(1,2,2)\n        plt.scatter(df.dropoff_longitude.values, \n                    df.dropoff_latitude.values, \n                    s=0.2, lw=0, c=cl_dropoff, cmap='tab20', alpha=0.5)\n        plt.xlim(-74.03, -73.77); plt.ylim(40.63, 40.85)\n        plt.xlabel('dropoff_longitude'); plt.ylabel('dropoff_latitude')\n        plt.show()\n    return cl_pickup, cl_dropoff\n\n# Rotate the map\ndef rotate_coords(df, col1, col2, pic=False):\n    alpha = 0.610865 # angle = 35 degrees\n    #alpha = 0.506145 # angle = 29 degrees\n    # Center of rotation\n    x_c = df[col1].mean()\n    y_c = df[col2].mean()\n    # Coordinates\n    C = df[[col1, col2]] - np.array([x_c, y_c])\n    # Rotation matrix\n    R = np.array([[np.cos(alpha), -np.sin(alpha)],\n                  [np.sin(alpha),  np.cos(alpha)]])\n    C_rot = np.matmul(R, C.transpose().values).transpose() + np.array([x_c, y_c])    \n    if pic:\n        plt.figure(figsize=(10,4))\n        plt.subplot(1,2,1)\n        plt.scatter(df[col1], df[col2], s=0.2, alpha=0.5)\n        plt.xlabel(col1); plt.ylabel(col2); \n        plt.ylim(40.6, 40.9); plt.xlim(-74.1, -73.7);\n        plt.subplot(1,2,2)\n        plt.scatter(C_rot[:, 0], C_rot[:, 1], s=0.2, alpha=0.5)\n        plt.xlabel(col1+'_rot'); plt.ylabel(col2+'_rot'); \n        plt.ylim(40.6, 40.9); plt.xlim(-74.1, -73.7);\n        plt.show()\n    return C_rot\n\n# Manhattan distances\ndef my_manhattan_distances(x1, x2, y1, y2):\n    return np.abs(x1 - x2) + np.abs(y1 - y2)\n# Euclidean distances\ndef my_euclidean_distances(x1, x2, y1, y2):\n    return np.square(x1 - x2) + np.square(y1 - y2)\nmy_manhattan_distances = np.vectorize(my_manhattan_distances)\nmy_euclidean_distances = np.vectorize(my_euclidean_distances)\n\n# Adding features\ndef add_features(df, predict=False):\n    # If predict = True, this function will prepare (add new features) \n    # train set (all train data) and test (all test data); else, \n    # if predict = False, the function will prepare train and validation datasets\n    if predict: \n        train_inds = df[(df.eval_set != 2)].index\n    else:\n        df = df[(df.eval_set != 2)].copy()\n        train_inds = df[df.eval_set != 1].index\n    \n    ### Log trip\n    print('Log trip duration')\n    df['trip_duration'] = df.trip_duration.apply(np.log)\n    \n    ### PCA transformation\n    print('Add PCA geo-coordinates')\n    coords = np.vstack((df[['pickup_latitude', 'pickup_longitude']], \n                        df[['dropoff_latitude', 'dropoff_longitude']]))\n    pca = PCA().fit(coords) # define 2 main axis\n    df['pickup_pca0'] = pca.transform(df[['pickup_longitude', 'pickup_latitude']])[:,0]\n    df['pickup_pca1'] = pca.transform(df[['pickup_longitude', 'pickup_latitude']])[:,1]\n    df['dropoff_pca0'] = pca.transform(df[['dropoff_longitude', 'dropoff_latitude']])[:,0]\n    df['dropoff_pca1'] = pca.transform(df[['dropoff_longitude', 'dropoff_latitude']])[:,1]\n    df['distance_pca0'] = np.abs(df.pickup_pca0-df.dropoff_pca0)\n    df['distance_pca1'] = np.abs(df.pickup_pca1-df.dropoff_pca1)\n    \n    print('Rorate geo-coordinates')\n    C_rot_pickup = rotate_coords(df, 'pickup_longitude', 'pickup_latitude', not predict)\n    C_rot_dropoff = rotate_coords(df, 'dropoff_longitude', 'dropoff_latitude', not predict)\n    df['pickup_longitude_rot'] = C_rot_pickup[:, 0]\n    df['pickup_latitude_rot'] = C_rot_pickup[:, 1]\n    df['dropoff_longitude_rot'] = C_rot_dropoff[:, 0]\n    df['dropoff_latitude_rot'] = C_rot_dropoff[:, 1]\n    \n    ### Add clusters\n    print('Add clusters')\n    cl_pu, cl_do = clusters(df, not predict)\n    df['pickup_clusters'] = cl_pu\n    df['dropoff_clusters'] = cl_do\n       \n    ### to DateTime\n    print('Convert to datetime format')\n    df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n    # Add weather info\n    df['date'] = df['pickup_datetime'].dt.date # adding date column\n    df['hour'] = df['pickup_datetime'].dt.hour # adding hour column\n    df = pd.merge(left=df, right=weather, on='date', how='left')\n    # Add weather hourly\n    df = pd.merge(left=df, right=weather_hour.drop_duplicates(subset=['date', 'hour']), \n                  on=['date', 'hour'], how='left')\n    df.drop(['date'], axis=1, inplace=True)\n    # Weather added\n    df['month'] = df['pickup_datetime'].dt.month\n    df['week_of_year'] = df['pickup_datetime'].dt.week\n    df['day'] = df['pickup_datetime'].dt.day\n    df['month_day'] = df['month'] + df['day']\n    df['day_of_year'] = df['pickup_datetime'].dt.dayofyear\n    #df['hour'] = df['pickup_datetime'].dt.hour\n    df['day_of_year_hour'] = 24*df['day_of_year'] + df['hour']\n    df['hour_minute'] = 60*df['hour'] + df['pickup_datetime'].dt.minute\n    df['day_week'] = df['pickup_datetime'].dt.weekday\n    df['month_day_hour'] = 31*24*df['month'] + 24*df['day'] + df['hour']\n    \n    ### Some usfull averages ###\n    print('Add averages')\n    train_info = df.iloc[train_inds].copy() # get only train data\n    # Month average \n    month_avg = train_info.groupby('month').trip_duration.mean()\n    month_avg = month_avg.reset_index(); month_avg.columns = ['month', 'month_avg']\n    df = pd.merge(left=df, right=month_avg, on='month', how='left')\n    \n    \n    # Week of year average\n    week_year_avg = train_info.groupby('week_of_year').trip_duration.mean()\n    week_year_avg = week_year_avg.reset_index()\n    week_year_avg.columns = ['week_of_year', 'week_of_year_avg']\n    df = pd.merge(left=df, right=week_year_avg, on='week_of_year', how='left')\n        \n    # Day of month average\n    day_month_avg = train_info.groupby('day').trip_duration.mean()\n    day_month_avg = day_month_avg.reset_index()\n    day_month_avg.columns = ['day', 'day_of_month_avg']\n    df = pd.merge(left=df, right=day_month_avg, on='day', how='left')\n        \n    # Day of year average\n    day_year_avg = train_info.groupby('day_of_year').trip_duration.mean()\n    day_year_avg = day_year_avg.reset_index()\n    day_year_avg.columns = ['day_of_year', 'day_of_year_avg']\n    df = pd.merge(left=df, right=day_year_avg, on='day_of_year', how='left')\n        \n    # Hour average\n    hour_avg = train_info.groupby('hour').trip_duration.mean()\n    hour_avg = hour_avg.reset_index(); hour_avg.columns = ['hour', 'hour_avg']\n    df = pd.merge(left=df, right=hour_avg, on='hour', how='left')\n        \n    # Day week average\n    day_week_avg = train_info.groupby('day_week').trip_duration.mean()\n    day_week_avg = day_week_avg.reset_index()\n    day_week_avg.columns = ['day_week', 'day_week_avg']\n    df = pd.merge(left=df, right=day_week_avg, on='day_week', how='left')  \n    \n    # Clusters\n    print('Pickup clusters')\n    cl_pu_avg = train_info.groupby('pickup_clusters').trip_duration.mean()\n    cl_pu_avg = cl_pu_avg.reset_index()\n    cl_pu_avg.columns = ['pickup_clusters', 'pickup_clusters_avg']\n    df = pd.merge(left=df, right=cl_pu_avg, on='pickup_clusters', how='left')\n    \n    print('Dropoff clusters')\n    cl_do_avg = train_info.groupby('dropoff_clusters').trip_duration.mean()\n    cl_do_avg = cl_do_avg.reset_index()\n    cl_do_avg.columns = ['dropoff_clusters', 'dropoff_clusters_avg']\n    df = pd.merge(left=df, right=cl_do_avg, on='dropoff_clusters', how='left')\n    \n    ### Distances ###\n    print('Add distances')\n    # Manhattan rot\n    df['distance_manhattan_rot'] = my_manhattan_distances(df.pickup_longitude_rot, \n                                                          df.dropoff_longitude_rot,\n                                                          df.pickup_latitude_rot, \n                                                          df.dropoff_latitude_rot)\n    # Manhattan pca\n    df['distance_manhattan_pca'] = df['distance_pca0'] + df['distance_pca1']\n    # Euclidean\n    df['distance_euclidean'] = my_euclidean_distances(df.pickup_latitude, \n                                                      df.dropoff_latitude,\n                                                      df.pickup_longitude, \n                                                      df.dropoff_longitude)\n    \n    # Fastest route\n    df = pd.merge(left=df,\n                  right=frame_fastest[['id', 'total_distance', \n                                       'total_travel_time', 'number_of_steps']],\n                  on='id', how='left')\n    \n    # Same destination\n    print('Add same destination marker')\n    df['same_destination'] = (df.distance_euclidean == 0).astype(np.uint)\n    \n    # Remove outliers\n    mask = (df.eval_set != 2) & (df.trip_duration > np.log(20*24*60*60))\n    print('Delete outliers:', mask.astype(np.uint).sum())\n    df = df[~mask]\n    return df","execution_count":3},{"cell_type":"markdown","source":"## Train-validation split","metadata":{"_execution_state":"idle","_uuid":"6bdfd7b4a68dc9711cc4f1c6a6382c83eedbe302","_cell_guid":"49813acd-d4d8-4a8e-ac7a-0884dbb1e7a5"}},{"metadata":{"_execution_state":"idle","_uuid":"bc93aef2e1eabc42b062d8dbe4256231fe0e5ecb","_cell_guid":"338cc15e-8d31-4424-a44b-9fbb83553c18"},"cell_type":"code","outputs":[],"source":"### Add features\nframe_augm = add_features(frame, predict=False)\n\ndrop_cols = ['dropoff_datetime', 'eval_set', 'pickup_datetime'] \\\n            + ['dropoff_pca1', 'month_avg', 'dropoff_longitude', \\\n               'dropoff_longitude_rot', 'pickup_longitude', \\\n               'pickup_longitude_rot', 'pickup_pca0', \\\n               'pickup_pca1', 'dropoff_pca0']\n# Train\nX_train = frame_augm[(frame_augm.eval_set==0)].copy()\ntrain_id = X_train.pop('id')\ny_train = X_train.pop('trip_duration')\nX_train.drop(drop_cols, axis=1, inplace=True)\n# Validation\nX_valid = frame_augm[frame_augm.eval_set==1].copy()\nvalid_id = X_valid.pop('id')\ny_valid = X_valid.pop('trip_duration')\nX_valid.drop(drop_cols, axis=1, inplace=True)\nprint('Train shape:', X_train.shape, '\\nTest shape:', X_valid.shape)","execution_count":4},{"cell_type":"markdown","source":"## LightDBM model\n\n### Important:\n\nSince kernels should work **under 1200 seconds** in the next cell some parameters of the model were changed. For better CV try, for example, **learning_rate = 0.3**. Some other parameters can be optimized too.","metadata":{"_execution_state":"idle","_uuid":"88be8bf2033834e2deadfe30c73dd8586b584ebd","_cell_guid":"a8a5570f-513c-4f69-9bdf-a38369eed2b8"}},{"metadata":{"_execution_state":"idle","_uuid":"9bbfce28d7e16111e2fd3204bbdea89fa5a99f4a","_cell_guid":"d4f471de-6910-4427-b82d-0fbe64c07f6b"},"cell_type":"code","outputs":[],"source":"def lgb_rmsle_score(preds, dtrain):\n    labels = np.exp(dtrain.get_label())\n    preds = np.exp(preds.clip(min=0))\n    return 'rmsle', np.sqrt(np.mean(np.square(np.log1p(preds)-np.log1p(labels)))), False\n\nd_train = lgb.Dataset(X_train, y_train)\n\nlgb_params = {\n    'learning_rate': 1.0, # try 0.2\n    'max_depth': 8,\n    'num_leaves': 55, \n    'objective': 'regression',\n    #'metric': {'rmse'},\n    'feature_fraction': 0.9,\n    'bagging_fraction': 0.5,\n    #'bagging_freq': 5,\n    'max_bin': 200}       # 1000\ncv_result_lgb = lgb.cv(lgb_params,\n                       d_train, \n                       num_boost_round=5000, \n                       nfold=3, \n                       feval=lgb_rmsle_score,\n                       early_stopping_rounds=50, \n                       verbose_eval=100, \n                       show_stdv=True)\nn_rounds = len(cv_result_lgb['rmsle-mean'])\nprint('num_boost_rounds_lgb=' + str(n_rounds))","execution_count":null},{"metadata":{"_execution_state":"idle","_uuid":"03951ddcaff665474a7f0b091b444a8810c1be87","_cell_guid":"7f9b8087-0fba-4553-b2fb-ffd51c2bbf4c","collapsed":true},"cell_type":"code","outputs":[],"source":"def dummy_rmsle_score(preds, y):\n    return np.sqrt(np.mean(np.square(np.log1p(np.exp(preds))-np.log1p(np.exp(y)))))\n\n# Train a model\nmodel_lgb = lgb.train(lgb_params, \n                      d_train, \n                      feval=lgb_rmsle_score, \n                      num_boost_round=n_rounds)\n# Predict on train\ny_train_pred = model_lgb.predict(X_train)\nprint('RMSLE on train = {}'.format(dummy_rmsle_score(y_train_pred, y_train)))\n# Predict on validation\ny_valid_pred = model_lgb.predict(X_valid)\nprint('RMSLE on valid = {}'.format(dummy_rmsle_score(y_valid_pred, y_valid)))","execution_count":null},{"metadata":{"_execution_state":"idle","_uuid":"6bee32e55e175baa239c086fba9fcce842e597e1","_cell_guid":"916275bf-071e-44c7-8dc6-1cbf162f2205","collapsed":true},"cell_type":"code","outputs":[],"source":"plt.figure(figsize=(10,5))\n# CV scores\nplt.subplot(1,2,1)\ntrain_scores = np.array(cv_result_lgb['rmsle-mean'])\ntrain_stds = np.array(cv_result_lgb['rmsle-stdv'])\nplt.plot(train_scores, color='green')\nplt.fill_between(range(len(cv_result_lgb['rmsle-mean'])), \n                 train_scores - train_stds, train_scores + train_stds, \n                 alpha=0.1, color='green')\nplt.title('LightGMB CV-results')\n#plt.ylim(0.34,0.40)\nplt.subplot(1,2,2)\nplt.scatter(y_valid, y_valid_pred, s=0.2, alpha=0.7)\nplt.plot([0,12], [0,12], color='g', alpha=0.3)\nplt.xlabel('True (log) validation set'); plt.xlim(0,12)\nplt.ylabel('Pred. (log) validation set'); plt.ylim(0,12)\nplt.show()","execution_count":null},{"cell_type":"markdown","source":"### Feature importance","metadata":{"_execution_state":"idle","_uuid":"305060a5de75eaa4e2e0b8aa9c335773a64529d2","_cell_guid":"4d3a17df-38fb-4baf-a2bb-2dd6358cb516"}},{"metadata":{"_execution_state":"idle","_uuid":"e5f3f3d81b6b0953ee87b4b33bae38ccbff80b4b","_cell_guid":"3353ec2d-a9b4-48f2-bbe0-ce8d884a0e46","collapsed":true},"cell_type":"code","outputs":[],"source":"feature_imp = pd.Series(dict(zip(X_train.columns, model_lgb.feature_importance()))) \\\n                    .sort_values(ascending=False)\nfeature_imp","execution_count":null},{"cell_type":"markdown","source":"### Predict","metadata":{"_execution_state":"idle","_uuid":"2c73dbcba0a128e1f0ebff4fb46fde78fb52ef1c","_cell_guid":"2bb20226-4c9f-45c4-b658-b23b07f149f1"}},{"metadata":{"_execution_state":"idle","_uuid":"530ca9115538f33c5d140d136d4acbd55611f7ec","_cell_guid":"470d0ba1-84c9-422f-a970-04ddd0248cbb","collapsed":true},"cell_type":"code","outputs":[],"source":"# Prediction on the test dataset\n# Clean training\ndel frame_augm; gc.collect()\n\ndef test_predict():\n    ### Add features\n    frame_augm = add_features(frame, True)\n    drop_cols = ['dropoff_datetime', 'eval_set', 'pickup_datetime'] \\\n                + ['dropoff_pca1', 'month_avg', 'dropoff_longitude', \\\n                   'dropoff_longitude_rot', 'pickup_longitude', \\\n                   'pickup_longitude_rot', 'pickup_pca0', \\\n                   'pickup_pca1', 'dropoff_pca0']\n    # Train\n    X_train = frame_augm[(frame_augm.eval_set!=2)].copy()\n    train_id = X_train.pop('id')\n    y_train = X_train.pop('trip_duration')\n    X_train.drop(drop_cols, axis=1, inplace=True)\n    # Test\n    X_test = frame_augm[frame_augm.eval_set==2].copy()\n    test_id = X_test.pop('id')\n    X_test.drop(drop_cols + ['trip_duration'], axis=1, inplace=True)\n    print('Train shape:', X_train.shape, 'Test shape', X_test.shape)\n\n    # Train a model\n    d_train = lgb.Dataset(X_train, y_train)\n    model_lgb = lgb.train(lgb_params, \n                          d_train, \n                          feval=lgb_rmsle_score, \n                          num_boost_round=n_rounds)\n    # Predict on validation\n    y_test_pred = model_lgb.predict(X_test)\n    return y_test_pred\n\n### Predict on TEST\n#y_test_pred = test_predict()\n### Submission\n#subm = pd.DataFrame()\n#subm['id'] = test_id\n#subm['trip_duration'] = np.exp(y_test_pred)\n#subm.to_csv('submission.csv', index=False)","execution_count":null}],"nbformat":4,"nbformat_minor":1}