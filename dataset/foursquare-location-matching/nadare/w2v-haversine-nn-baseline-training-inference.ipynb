{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install /kaggle/input/tensorflow-text-260/tensorflow_text-2.6.0-cp37-cp37m-manylinux1_x86_64.whl --no-deps\n!pip install /kaggle/input/tensorflowranking/tensorflow_ranking-0.5.0-py2.py3-none-any.whl --no-deps\n!pip install /kaggle/input/mojimoji/mojimoji-0.0.12-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl --no-deps","metadata":{"ExecuteTime":{"end_time":"2022-06-11T10:39:06.577816Z","start_time":"2022-06-11T10:38:58.230818Z"},"papermill":{"duration":53.552676,"end_time":"2022-05-04T10:20:54.774628","exception":false,"start_time":"2022-05-04T10:20:01.221952","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-12T02:03:08.75847Z","iopub.execute_input":"2022-06-12T02:03:08.758876Z","iopub.status.idle":"2022-06-12T02:04:18.220224Z","shell.execute_reply.started":"2022-06-12T02:03:08.758798Z","shell.execute_reply":"2022-06-12T02:04:18.218943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sentencepiece as spm\n\nimport tensorflow as tf\nimport tensorflow_text as tf_text\nimport tensorflow_ranking as tfr\n\nfrom tqdm.notebook import tqdm\n\nimport gc","metadata":{"execution":{"iopub.status.busy":"2022-06-12T02:04:21.824272Z","iopub.execute_input":"2022-06-12T02:04:21.824722Z","iopub.status.idle":"2022-06-12T02:04:28.867104Z","shell.execute_reply.started":"2022-06-12T02:04:21.824687Z","shell.execute_reply":"2022-06-12T02:04:28.866025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(\"../input/foursquare-location-matching/train.csv\")\ntest_df = pd.read_csv(\"../input/foursquare-location-matching/test.csv\")","metadata":{"ExecuteTime":{"end_time":"2022-06-11T10:39:09.254816Z","start_time":"2022-06-11T10:39:06.578816Z"},"papermill":{"duration":7.72778,"end_time":"2022-05-04T10:21:02.526592","exception":false,"start_time":"2022-05-04T10:20:54.798812","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-12T02:04:31.563656Z","iopub.execute_input":"2022-06-12T02:04:31.5643Z","iopub.status.idle":"2022-06-12T02:04:44.972971Z","shell.execute_reply.started":"2022-06-12T02:04:31.564269Z","shell.execute_reply":"2022-06-12T02:04:44.971904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_id_map = {v: i for i, v in enumerate(train_df[\"id\"].values)}\ntrain_df[\"ix\"] = train_df[\"id\"].map(train_id_map)\ntrain_df[\"categories\"] = train_df[\"categories\"].fillna(\"\")\ntrain_df[\"pid\"] = train_df[\"point_of_interest\"].map({v: i for i, v in enumerate(train_df[\"point_of_interest\"].unique())})\n\ntest_id_map = {v: i for i, v in enumerate(test_df[\"id\"].values)}\ntest_df[\"ix\"] = test_df[\"id\"].map(test_id_map)\ntest_df[\"categories\"] = test_df[\"categories\"].fillna(\"\")\ntest_df[\"pid\"] = -1","metadata":{"ExecuteTime":{"end_time":"2022-06-11T10:39:11.942817Z","start_time":"2022-06-11T10:39:10.705816Z"},"papermill":{"duration":2.936041,"end_time":"2022-05-04T10:21:31.818613","exception":false,"start_time":"2022-05-04T10:21:28.882572","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-12T02:04:48.194699Z","iopub.execute_input":"2022-06-12T02:04:48.195085Z","iopub.status.idle":"2022-06-12T02:04:53.848328Z","shell.execute_reply.started":"2022-06-12T02:04:48.195054Z","shell.execute_reply":"2022-06-12T02:04:53.847189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import KFold\ngroup = -np.ones(len(train_df), dtype=\"int32\")\ng = 0\nfor g, (dev_p_ids, val_p_ids) in enumerate(KFold(n_splits=5, shuffle=True, random_state=42).split(train_df[\"pid\"].unique())):\n    ix = train_df[\"pid\"].isin(val_p_ids)\n    group[ix] = g\ntrain_df[\"group\"] = group\ntest_df[\"group\"] = -1","metadata":{"ExecuteTime":{"end_time":"2022-06-11T10:39:12.926815Z","start_time":"2022-06-11T10:39:11.943817Z"},"execution":{"iopub.status.busy":"2022-06-12T02:04:56.137875Z","iopub.execute_input":"2022-06-12T02:04:56.138286Z","iopub.status.idle":"2022-06-12T02:04:56.790454Z","shell.execute_reply.started":"2022-06-12T02:04:56.138255Z","shell.execute_reply":"2022-06-12T02:04:56.789266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import mojimoji\ndef zenhan_normalize(x):\n    return mojimoji.han_to_zen(mojimoji.zen_to_han(x, kana=False), digit=False, ascii=False)\n\ntrain_df[\"name\"] = np.vectorize(zenhan_normalize)(train_df[\"name\"].fillna(\"\").str.lower())\ntest_df[\"name\"] = np.vectorize(zenhan_normalize)(test_df[\"name\"].fillna(\"\").str.lower())","metadata":{"ExecuteTime":{"end_time":"2022-06-11T10:39:14.258817Z","start_time":"2022-06-11T10:39:12.927816Z"},"execution":{"iopub.status.busy":"2022-06-12T02:04:58.877365Z","iopub.execute_input":"2022-06-12T02:04:58.878182Z","iopub.status.idle":"2022-06-12T02:05:08.112777Z","shell.execute_reply.started":"2022-06-12T02:04:58.878133Z","shell.execute_reply":"2022-06-12T02:05:08.111582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sentencepiece as spm\ntrain_df[[\"name\"]].to_csv(\"./name.txt\", index=None, encoding=\"utf-8\", header=None)\ntest_df[[\"name\"]].to_csv(\"./name.txt\", index=None, encoding=\"utf-8\", header=None, mode=\"a\")\n\nspm.SentencePieceTrainer.train(\n    input=\"./name.txt\",\n    model_type=\"unigram\",\n    split_by_whitespace=True,\n    model_prefix='./name_sp',\n    character_coverage=.9995,\n    vocab_size=32000,\n)\n\nsp = spm.SentencePieceProcessor()\nsp.load('./name_sp.model')\n\ntrain_df[\"sp_name\"] = np.vectorize(lambda x: \",\".join([y.replace(\"â–\", \"\") for y in sp.encode_as_pieces(x)]))(train_df[\"name\"])","metadata":{"ExecuteTime":{"end_time":"2022-06-11T10:39:47.934816Z","start_time":"2022-06-11T10:39:20.487818Z"},"execution":{"iopub.status.busy":"2022-06-12T02:05:10.550861Z","iopub.execute_input":"2022-06-12T02:05:10.551261Z","iopub.status.idle":"2022-06-12T02:07:51.380376Z","shell.execute_reply.started":"2022-06-12T02:05:10.551231Z","shell.execute_reply":"2022-06-12T02:07:51.37921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[[\"name\", \"sp_name\"]].sample(20)","metadata":{"ExecuteTime":{"end_time":"2022-06-11T10:39:48.220829Z","start_time":"2022-06-11T10:39:47.935818Z"},"execution":{"iopub.status.busy":"2022-06-12T02:08:14.852558Z","iopub.execute_input":"2022-06-12T02:08:14.853042Z","iopub.status.idle":"2022-06-12T02:08:15.617291Z","shell.execute_reply.started":"2022-06-12T02:08:14.85301Z","shell.execute_reply":"2022-06-12T02:08:15.616282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow_text import SentencepieceTokenizer\n\nclass SentencePieceEmbeddingLayer(tf.keras.layers.Layer):\n    \n    def __init__(self, vocab_size, out_dim, sp_model_path):\n        super(SentencePieceEmbeddingLayer, self).__init__()\n        self.vocab_size = vocab_size\n        self.out_dim = out_dim\n        model = open(sp_model_path, \"rb\").read()\n        self.tokenizer = SentencepieceTokenizer(model)\n        self.embedding = tf.keras.layers.Embedding(vocab_size, out_dim)\n    \n    def call(self, X):\n        token = self.tokenizer.tokenize(X)\n        X = self.embedding(token)\n        return X\n    \nclass SkipgramModel(tf.keras.Model):\n    \n    def __init__(self, spe):\n        super(SkipgramModel, self).__init__()\n        self.spe = spe\n        \n    @tf.function(experimental_relax_shapes=True)\n    def call(self, name):\n        X = self.spe(name)\n        name_skip_loss = self.skipgram_task(X)\n        return name_skip_loss\n    \n    @tf.function(experimental_relax_shapes=True)\n    def skipgram_task(self, X):\n        X_sum = tf.reduce_sum(X, axis=1)\n        X_sum_other = tf.nn.l2_normalize(tf.gather(X_sum, X.value_rowids()) - X.values, axis=1)\n\n        \n        X_sum_norm = tf.nn.l2_normalize(X_sum, axis=1)\n        X_sum_other_norm = tf.nn.l2_normalize(X_sum_other, axis=1)\n        X_value_norm = tf.nn.l2_normalize(X.values, axis=1)\n\n        correct_cossim = tf.expand_dims(tf.einsum(\"Vd,Vd->V\", X_value_norm, X_sum_other_norm), axis=1)\n        wrong_cossim = tf.einsum(\"Vd,Nd->VN\", X_value_norm, X_sum_norm)\n        cossim = tf.concat([correct_cossim, wrong_cossim], axis=1)\n\n        rowwise_mask = tf.expand_dims(tf.gather(X.row_lengths(), X.value_rowids()) > 1, axis=1)\n        sameid_mask = tf.expand_dims(tf.cast(X.value_rowids(), \"int32\"), axis=1) != tf.expand_dims(tf.range(tf.shape(X.row_lengths())[0] + 1)-1, axis=0)\n\n        mask = tf.math.logical_and(rowwise_mask, sameid_mask)\n        label = tf.concat([tf.ones(tf.shape(correct_cossim)), tf.zeros(tf.shape(wrong_cossim))], axis=1)\n        \n        # Fixed AdaCos paramet2r\n        scale = tf.sqrt(2.) * tf.math.log(tf.cast(tf.shape(X.row_lengths())[0], \"float32\") - 1.)\n\n        label = tf.where(mask, label, tf.zeros(tf.shape(mask)))\n        pred = tf.where(mask, scale * cossim, tf.float32.min)\n        loss = tf.math.reduce_mean(tf.keras.losses.categorical_crossentropy(label, pred, from_logits=True))\n        return loss        ","metadata":{"ExecuteTime":{"end_time":"2022-06-11T10:39:48.236822Z","start_time":"2022-06-11T10:39:48.223822Z"},"execution":{"iopub.status.busy":"2022-06-12T02:08:23.986536Z","iopub.execute_input":"2022-06-12T02:08:23.987212Z","iopub.status.idle":"2022-06-12T02:08:24.01388Z","shell.execute_reply.started":"2022-06-12T02:08:23.987176Z","shell.execute_reply":"2022-06-12T02:08:24.012733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DataContainer():\n    \n    def __init__(self, df, positive_ix, skip_group=-1):\n        self.positive_ix = positive_ix\n        self.skip_group = skip_group\n        self.name = tf.constant(df[\"name\"].values)\n        self.position = tf.expand_dims(tf.constant(np.deg2rad(df[[\"latitude\", \"longitude\"]].astype(np.float32).values), dtype=\"float32\"), axis=0)\n        self.pid = tf.constant(df[\"pid\"].astype(np.int32).values, dtype=\"int32\")\n        self.group = tf.constant(df[\"group\"].astype(np.int32).values, dtype=\"int32\")\n        \n    def get_position(self, ix):\n        return tf.gather(tf.gather(self.position, ix, axis=1), 0)\n\n    def call(self, ix):\n        name = tf.gather(self.name, ix)\n        position = self.get_position(ix)\n        return ix, name, position\n    \n    @tf.function(experimental_relax_shapes=True)\n    def log_haversine(self, X, Y):\n        delta = Y - X\n        x_lats = tf.gather(X, 0, axis=-1)\n        y_lats = tf.gather(Y, 0, axis=-1)\n        dlat = tf.gather(delta, 0, axis=-1)\n        dlon = tf.gather(delta, 1, axis=-1)\n\n        a = tf.sin(dlat/2) * tf.sin(dlat/2) + tf.cos(x_lats) * tf.cos(y_lats) * tf.sin(dlon/2) * tf.sin(dlon/2)\n        c = tf.math.log(2 * tf.math.atan2(tf.sqrt(a), tf.sqrt(1-a)) + tf.keras.backend.epsilon())\n        return c\n    \n    @tf.function(experimental_relax_shapes=True)\n    def negative_sample_by_dist_top_k(self, ix, k=128):\n        X = tf.expand_dims(self.get_position(ix), axis=1)\n        c = self.log_haversine(X, self.position)\n        pos_ix = tf.gather(self.positive_ix, tf.gather(self.pid, ix))\n        c = tf.tensor_scatter_nd_update(c, tf.stack([tf.cast(pos_ix.value_rowids(), \"int32\"), pos_ix.values], axis=1), tf.ones([tf.shape(pos_ix.values)[0]])*tf.float32.max/10.)\n        dist, neighbor = tf.math.top_k(-c, k=k)\n\n        dist = tf.reverse_sequence(-dist, \n                                  tf.ones([tf.shape(dist)[0]], dtype=\"int32\") * tf.shape(dist)[1],\n                                  seq_axis=1)\n        neighbor = tf.reverse_sequence(neighbor, \n                                  tf.ones([tf.shape(neighbor)[0]], dtype=\"int32\") * tf.shape(neighbor)[1],\n                                  seq_axis=1)\n        return dist, neighbor\n    \n    def get_positive_info(self, ix):\n        pos_ix = tf.gather(self.positive_ix, tf.gather(self.pid, ix))\n        Y = self.get_position(pos_ix)\n        X = self.get_position(tf.gather(ix, pos_ix.value_rowids()))\n        pos_dist = tf.RaggedTensor.from_value_rowids(self.log_haversine(X, Y.values), Y.value_rowids())\n        pos_ix = pos_ix.to_tensor(default_value=-1)\n        pos_ix = tf.where(tf.expand_dims(ix, axis=1) != pos_ix, pos_ix, -1)\n        pos_dist = pos_dist.to_tensor(default_value=0.)\n        return pos_dist, pos_ix","metadata":{"ExecuteTime":{"end_time":"2022-06-11T11:20:17.972955Z","start_time":"2022-06-11T11:20:17.954956Z"},"execution":{"iopub.status.busy":"2022-06-12T02:08:27.190264Z","iopub.execute_input":"2022-06-12T02:08:27.190765Z","iopub.status.idle":"2022-06-12T02:08:27.213907Z","shell.execute_reply.started":"2022-06-12T02:08:27.190734Z","shell.execute_reply":"2022-06-12T02:08:27.212572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DIMSIZE = 128\nspe = SentencePieceEmbeddingLayer(32000, DIMSIZE, \"./name_sp.model\")\nskipgram_model = SkipgramModel(spe)","metadata":{"ExecuteTime":{"end_time":"2022-06-11T11:09:55.564855Z","start_time":"2022-06-11T11:09:55.513236Z"},"execution":{"iopub.status.busy":"2022-06-12T02:08:41.543301Z","iopub.execute_input":"2022-06-12T02:08:41.544048Z","iopub.status.idle":"2022-06-12T02:08:45.116718Z","shell.execute_reply.started":"2022-06-12T02:08:41.544013Z","shell.execute_reply":"2022-06-12T02:08:45.115721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"container_cols = [\"name\", \"latitude\", \"longitude\", \"pid\", \"group\"] \nall_container = DataContainer(pd.concat([train_df[container_cols],\n                                         test_df[container_cols]],\n                                        axis=0),\n                              tf.cast(tf.ragged.constant(train_df.groupby(\"pid\")[\"ix\"].unique().tolist()), \"int32\"))","metadata":{"ExecuteTime":{"end_time":"2022-06-11T11:20:34.30015Z","start_time":"2022-06-11T11:20:20.011999Z"},"execution":{"iopub.status.busy":"2022-06-12T02:08:51.841108Z","iopub.execute_input":"2022-06-12T02:08:51.841503Z","iopub.status.idle":"2022-06-12T02:09:56.516697Z","shell.execute_reply.started":"2022-06-12T02:08:51.841473Z","shell.execute_reply":"2022-06-12T02:09:56.51554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = {\"skipgram_loss\": []}\nskipgram_loss = tf.keras.metrics.Mean(name='loss')\nNUM_EPOCH = 10\nskipgram_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\nall_ixs = tf.range(len(all_container.name))\nall_ds = tf.data.Dataset.from_tensor_slices(all_ixs)\\\n                .shuffle(len(all_ixs), reshuffle_each_iteration=True)\\\n                .batch(1024, drop_remainder=True)\\\n                .map(all_container.call)\n\n@tf.function(experimental_relax_shapes=True)\ndef skipgram_forward_step(name):\n    with tf.GradientTape() as tape:\n        loss = skipgram_model(name, training=True)\n    gradients = tape.gradient(loss, skipgram_model.trainable_variables)\n    skipgram_optimizer.apply_gradients(zip(gradients, skipgram_model.trainable_variables))\n    return loss\n\nwith tqdm(total=NUM_EPOCH) as pbar:\n    for _ in range(NUM_EPOCH):\n        step = 0\n        for ix, name, position in all_ds:\n            sl = skipgram_forward_step(name)\n            skipgram_loss(sl)\n            learning_text = \"[{}/{}] \".format(str(step).zfill(5), len(all_ds))\n            progress_text = \"skipgram_loss: {:.4f}\".format(skipgram_loss.result().numpy())\n            pbar.set_postfix_str(learning_text + progress_text)\n            step += 1\n        history[\"skipgram_loss\"].append(skipgram_loss.result().numpy())\n        \n        skipgram_loss.reset_states()\n        pbar.update(1)","metadata":{"ExecuteTime":{"end_time":"2022-06-11T11:11:20.72224Z","start_time":"2022-06-11T11:10:09.681357Z"},"execution":{"iopub.status.busy":"2022-06-12T02:10:28.818204Z","iopub.execute_input":"2022-06-12T02:10:28.819007Z","iopub.status.idle":"2022-06-12T02:16:13.029187Z","shell.execute_reply.started":"2022-06-12T02:10:28.818972Z","shell.execute_reply":"2022-06-12T02:16:13.025705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del all_container\ntrain_container = all_container = DataContainer(train_df[container_cols],\n                                                tf.cast(tf.ragged.constant(train_df.groupby(\"pid\")[\"ix\"].unique().tolist()), \"int32\"))\n\ntrain_ds = tf.data.Dataset.from_tensor_slices(tf.range(len(train_df)))\\\n                .batch(1024)\\\n                .map(train_container.call)\n\nembeddings = []\nfor ix, name, position in tqdm(train_ds):\n    embeddings.append(tf.nn.l2_normalize(tf.reduce_sum(spe(name), axis=1), axis=1))\nembeddings = tf.nn.l2_normalize(tf.concat(embeddings, axis=0), axis=1)","metadata":{"ExecuteTime":{"end_time":"2022-06-11T15:34:23.197921Z","start_time":"2022-06-11T15:34:19.224925Z"},"execution":{"iopub.status.busy":"2022-06-12T02:16:25.163614Z","iopub.execute_input":"2022-06-12T02:16:25.164484Z","iopub.status.idle":"2022-06-12T02:18:16.504794Z","shell.execute_reply.started":"2022-06-12T02:16:25.164418Z","shell.execute_reply":"2022-06-12T02:18:16.503774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow_ranking as tfr\nspe.embedding.trainable = False\n\nclass LogisticModel(tf.keras.Model):\n    def __init__(self, dim):\n        super(LogisticModel, self).__init__()\n        self.scale = tf.Variable(tf.zeros([dim], dtype=\"float32\"), trainable=True)\n        self.bias = tf.Variable(0., trainable=True)\n        #self.loss_func = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n        self.loss_func = tfr.keras.losses.ApproxNDCGLoss()\n        \n    def call(self, X, Y):\n        logit = tf.einsum(\"nmd,d->nm\", X, self.scale) + self.bias\n        loss = self.loss_func(Y, logit)\n        return loss    \n    \nlogistic_model = LogisticModel(2)","metadata":{"ExecuteTime":{"end_time":"2022-06-11T11:11:49.774201Z","start_time":"2022-06-11T11:11:49.705955Z"},"execution":{"iopub.status.busy":"2022-06-12T02:18:23.135325Z","iopub.execute_input":"2022-06-12T02:18:23.136047Z","iopub.status.idle":"2022-06-12T02:18:23.15269Z","shell.execute_reply.started":"2022-06-12T02:18:23.136016Z","shell.execute_reply":"2022-06-12T02:18:23.151233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nhistory = {\"logistic_loss\": []}\nlogistic_loss = tf.keras.metrics.Mean(name='loss')\nlogistic_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-2)\n\nall_ixs = tf.range(len(all_container.name))\ntrain_ds = tf.data.Dataset.from_tensor_slices(tf.range(len(train_df)))\\\n                .shuffle(len(all_ixs), reshuffle_each_iteration=True)\\\n                .batch(128, drop_remainder=True)\\\n                .map(all_container.call)\n\n@tf.function(experimental_relax_shapes=True)\ndef logistic_forward_step(X, Y):\n    with tf.GradientTape() as tape:\n        loss = logistic_model(X, Y, training=True)\n    gradients = tape.gradient(loss, logistic_model.trainable_variables)\n    if tf.reduce_all([tf.reduce_all(tf.math.is_finite(g)) for g in gradients]):\n        logistic_optimizer.apply_gradients(zip(gradients, logistic_model.trainable_variables))\n    return loss\n\nwith tqdm(total=len(train_ds)) as pbar:\n    step = 0\n    for ix, name, position in train_ds:\n        neg_dist, neg_ixs = all_container.negative_sample_by_dist_top_k(ix, k=1024)\n        pos_dist, pos_ixs = all_container.get_positive_info(ix)\n        pos_order = tf.slice(tf.argsort(pos_dist, axis=1), [0, 0], [-1, min(tf.shape(pos_dist)[1], 64)])\n        pos_dist = tf.gather(pos_dist, pos_order, batch_dims=1)\n        pos_ixs = tf.gather(pos_ixs, pos_order, batch_dims=1)\n        pos_ixs = tf.where(tf.expand_dims(ix, axis=1) != pos_ixs, pos_ixs, -1)\n\n        update_ixs = tf.where(pos_ixs >= 0)\n        target_ixs = tf.tensor_scatter_nd_update(neg_ixs, update_ixs, tf.gather_nd(pos_ixs, update_ixs))\n        target_label = tf.tensor_scatter_nd_update(tf.zeros(tf.shape(neg_dist)), update_ixs, tf.ones([tf.shape(update_ixs)[0]]))\n        target_dist = tf.tensor_scatter_nd_update(neg_dist, update_ixs, tf.gather_nd(pos_dist, update_ixs))\n        target_cossim = tf.einsum(\"nd,nmd->nm\", tf.gather(embeddings, ix), tf.gather(embeddings, target_ixs))\n        target_feat = tf.stack([target_dist, target_cossim], axis=-1)\n        loss = logistic_forward_step(target_feat, target_label)\n        if tf.math.is_finite(loss):\n            logistic_loss(loss)\n        progress_text = \"logistic_loss: {:.4f}\".format(logistic_loss.result().numpy())\n        pbar.set_postfix_str(progress_text)\n        pbar.update(1)\n        step += 1\n        if step == 1000:\n            break","metadata":{"ExecuteTime":{"end_time":"2022-06-11T11:22:36.452469Z","start_time":"2022-06-11T11:20:38.998143Z"},"execution":{"iopub.status.busy":"2022-06-12T02:19:44.474742Z","iopub.execute_input":"2022-06-12T02:19:44.475131Z","iopub.status.idle":"2022-06-12T02:24:04.129698Z","shell.execute_reply.started":"2022-06-12T02:19:44.4751Z","shell.execute_reply":"2022-06-12T02:24:04.128801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[\"true_count\"] = train_df.groupby(\"pid\")[\"ix\"].transform(\"count\")\nlogistic_model.trainable = False\nlogistic_model.compile()\nlogistic_model.scale\n","metadata":{"ExecuteTime":{"end_time":"2022-06-11T11:22:44.13183Z","start_time":"2022-06-11T11:22:44.121831Z"},"execution":{"iopub.status.busy":"2022-06-12T02:28:09.886943Z","iopub.execute_input":"2022-06-12T02:28:09.887289Z","iopub.status.idle":"2022-06-12T02:28:10.04158Z","shell.execute_reply.started":"2022-06-12T02:28:09.887261Z","shell.execute_reply":"2022-06-12T02:28:10.040442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# embedding only\n\"\"\"\nemb_ds = tf.data.Dataset.from_tensor_slices(embeddings)\\\n                .batch(128)\n\nneighbors = []\nfor emb in tqdm(emb_ds):\n    score, indices = tf.math.top_k(tf.einsum(\"md,nd->mn\", emb, embeddings), k=16)\n    neighbors.append(indices)\nneighbors = tf.minimum(tf.concat(neighbors, axis=0).numpy()[:len(train_df)], len(train_df)-1)\nprint(\"precision@16\", ((train_df[\"pid\"].values.reshape(-1, 1) == train_df[\"pid\"].values[neighbors]).sum(axis=1) / train_df[\"true_count\"]).mean())\n\"\"\"\nprint(\"precision@16\", 0.7658628703149348)","metadata":{"execution":{"iopub.status.busy":"2022-06-12T02:28:17.106156Z","iopub.execute_input":"2022-06-12T02:28:17.106714Z","iopub.status.idle":"2022-06-12T02:28:18.215537Z","shell.execute_reply.started":"2022-06-12T02:28:17.106671Z","shell.execute_reply":"2022-06-12T02:28:18.212685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# haversine\n\"\"\"\ntrain_ixs_ds = tf.data.Dataset.from_tensor_slices(tf.range(len(train_df)))\\\n                .batch(128)\n\nneighbors = []\nfor ix in tqdm(train_ixs_ds):\n    emb = tf.gather(embeddings, ix)\n    X = tf.expand_dims(train_container.get_position(ix), axis=1)\n    dist = train_container.log_haversine(X, train_container.position)\n    _, indices = tf.math.top_k(-dist, k=16)\n    neighbors.append(indices)\nneighbors = tf.minimum(tf.concat(neighbors, axis=0).numpy()[:len(train_df)], len(train_df)-1)\nprint(\"precision@16\", ((train_df[\"pid\"].values.reshape(-1, 1) == train_df[\"pid\"].values[neighbors]).sum(axis=1) / train_df[\"true_count\"]).mean())\n\"\"\"\nprint(\"precision@16\", 0.8945505050306388)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# embedding + haversine\n\"\"\"\ntrain_ixs_ds = tf.data.Dataset.from_tensor_slices(tf.range(len(train_df)))\\\n                .batch(128)\n\nneighbors = []\nfor ix in tqdm(train_ixs_ds):\n    emb = tf.gather(embeddings, ix)\n    X = tf.expand_dims(train_container.get_position(ix), axis=1)\n    dist = train_container.log_haversine(X, train_container.position)\n    cossim = tf.einsum(\"nd,md->nm\", emb, embeddings)\n    score = tf.einsum(\"d,nmd->nm\", logistic_model.scale, tf.stack([dist, cossim], axis=-1))\n    _, indices = tf.math.top_k(score, k=16)\n    neighbors.append(indices)\nneighbors = tf.minimum(tf.concat(neighbors, axis=0).numpy()[:len(train_df)], len(train_df)-1)\nprint(\"precision@16\", ((train_df[\"pid\"].values.reshape(-1, 1) == train_df[\"pid\"].values[neighbors]).sum(axis=1) / train_df[\"true_count\"]).mean())\n\"\"\"\nprint(\"precision@16\", 0.9550629119862758)","metadata":{"ExecuteTime":{"end_time":"2022-06-11T11:36:18.212335Z","start_time":"2022-06-11T11:23:42.366793Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DCNV2Classifier(tf.keras.models.Model):\n    \n    def __init__(self, num_cross, num_dense, hidden_dim, shrink_size=4):\n        super(DCNV2Classifier, self).__init__()\n        self.num_cross = num_cross\n        self.num_dense = num_dense\n        self.hidden_dim = hidden_dim\n        \n        self.norm_layers = [tf.keras.layers.LayerNormalization() for _ in range(self.num_cross+num_dense)]\n        self.dense_layers = [tf.keras.layers.Dense(hidden_dim, activation=\"gelu\") for _ in range(self.num_dense)]\n        self.pred_layer = tf.keras.layers.Dense(1)\n        self.built = False\n    \n    def build(self, input_shape):\n        last_dim = input_shape[-1]\n        self.filter_layers = [tf.keras.layers.Dense(hidden_dim//shrink_size) for _ in range(self.num_cross)]\n        self.out_layers = [tf.keras.layers.Dense(hidden_dim//shrink_size) for _ in range(self.num_cross)]        \n    \n    def call(self, X):\n        X0 = tf.identity(X)\n        for i in range(self.num_cross):\n            X = self.norm_layers[i](X + X0 * self.out_layers[i](self.filter_layers[i](X)))\n        \n        for i in range(self.num_dense):\n            X = self.norm_layers[i+self.num_cross](self.dense_layers[i](X))\n            \n        return self.pred_layer(X)\n","metadata":{"ExecuteTime":{"end_time":"2022-06-11T16:25:52.626797Z","start_time":"2022-06-11T16:25:52.6158Z"},"execution":{"iopub.status.busy":"2022-06-12T02:28:27.323723Z","iopub.execute_input":"2022-06-12T02:28:27.324131Z","iopub.status.idle":"2022-06-12T02:28:27.336347Z","shell.execute_reply.started":"2022-06-12T02:28:27.324102Z","shell.execute_reply":"2022-06-12T02:28:27.334873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classify_model = DCNV2Classifier(0, 2, 128)","metadata":{"ExecuteTime":{"end_time":"2022-06-11T18:01:27.173788Z","start_time":"2022-06-11T18:01:27.157899Z"},"execution":{"iopub.status.busy":"2022-06-12T02:28:27.900226Z","iopub.execute_input":"2022-06-12T02:28:27.901399Z","iopub.status.idle":"2022-06-12T02:28:27.922312Z","shell.execute_reply.started":"2022-06-12T02:28:27.901335Z","shell.execute_reply":"2022-06-12T02:28:27.921405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dev_ix = np.where(train_df[\"group\"] != 0)[0].astype(np.int32)\nval_ix = np.where(train_df[\"group\"] == 0)[0].astype(np.int32)\n\ndev_filter = tf.tensor_scatter_nd_update(tf.zeros(len(train_df)), \n                                         tf.expand_dims(val_ix, axis=1),\n                                         tf.ones(len(val_ix)) * tf.float32.min / 10.)\nval_filter = tf.zeros(len(train_df))\n","metadata":{"ExecuteTime":{"end_time":"2022-06-11T18:01:27.804741Z","start_time":"2022-06-11T18:01:27.774743Z"},"execution":{"iopub.status.busy":"2022-06-12T02:28:39.743548Z","iopub.execute_input":"2022-06-12T02:28:39.74395Z","iopub.status.idle":"2022-06-12T02:28:39.768805Z","shell.execute_reply.started":"2022-06-12T02:28:39.743905Z","shell.execute_reply":"2022-06-12T02:28:39.767491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dev_ds = tf.data.Dataset.from_tensor_slices(tf.constant(dev_ix))\\\n                .shuffle(len(dev_ix), reshuffle_each_iteration=True)\\\n                .batch(128)\nval_ds = tf.data.Dataset.from_tensor_slices(tf.constant(val_ix))\\\n                .batch(128)\n\nneighbors = []\nNUM_NEGATIVE = 32\n\n@tf.function(experimental_relax_shapes=True)\ndef make_feat_target(ix, retrieval_filter):\n    emb = tf.gather(embeddings, ix)\n    query_position = tf.expand_dims(train_container.get_position(ix), axis=1)\n    \n    pos_ix = tf.gather(train_container.positive_ix, tf.gather(train_container.pid, ix))\n    dist = train_container.log_haversine(query_position, train_container.position)\n    cossim = tf.einsum(\"nd,md->nm\", emb, embeddings)\n    score = tf.einsum(\"d,nmd->nm\", logistic_model.scale, tf.stack([dist, cossim], axis=-1))\n    score = tf.tensor_scatter_nd_update(score,\n                                        tf.stack([tf.cast(pos_ix.value_rowids(), \"int32\"), pos_ix.values], axis=1),\n                                        tf.ones([tf.shape(pos_ix.values)[0]])*tf.float32.min/10.)\n    score += retrieval_filter\n    _, neg_indices = tf.math.top_k(score, k=NUM_NEGATIVE)\n    neg_indices = tf.reverse_sequence(neg_indices, \n                              tf.ones([tf.shape(neg_indices)[0]], dtype=\"int32\") * tf.shape(neg_indices)[1],\n                              seq_axis=1)\n    \n    \n    pos_ix = pos_ix.to_tensor(default_value=-1)\n    pos_ix = tf.slice(pos_ix, [0, 0], [-1, tf.minimum(NUM_NEGATIVE, tf.shape(pos_ix)[1])])\n    pos_ix = tf.where(tf.expand_dims(ix, axis=1) != pos_ix, pos_ix, -1)\n    \n    \n    update_ixs = tf.where(pos_ix >= 0)\n    target_ixs = tf.tensor_scatter_nd_update(neg_indices, update_ixs, tf.gather_nd(pos_ix, update_ixs))\n    target_label = tf.tensor_scatter_nd_update(tf.zeros(tf.shape(target_ixs)), update_ixs, tf.ones([tf.shape(update_ixs)[0]]))\n    target_cossim = tf.gather(cossim, target_ixs, batch_dims=1)\n    target_dist = tf.gather(dist, target_ixs, batch_dims=1)\n    query_embedding = tf.repeat(tf.expand_dims(emb, axis=1), NUM_NEGATIVE, axis=1)\n    target_embedding = tf.gather(embeddings, target_ixs)\n    target_feat = tf.concat([query_embedding, target_embedding, tf.stack([target_dist, target_cossim], axis=-1)], axis=-1)\n    return target_ixs, target_feat, target_label\n    \nhistory = {\"classify_dev_loss\": [], \"classify_val_loss\": []}\nclassify_loss_func = tf.keras.losses.BinaryCrossentropy(from_logits=True)\nclassify_dev_loss = tf.keras.metrics.Mean(name='loss')\nclassify_val_loss = tf.keras.metrics.Mean(name='loss')\n\nclassify_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\nNUM_EPOCH = 1\n\n@tf.function(experimental_relax_shapes=True)\ndef classify_forward_step(X, y):\n    with tf.GradientTape() as tape:\n        pred = classify_model(X, training=True)\n        loss = classify_loss_func(y, pred)\n    gradients = tape.gradient(loss, classify_model.trainable_variables)\n    classify_optimizer.apply_gradients(zip(gradients, classify_model.trainable_variables))\n    return loss\n\n@tf.function(experimental_relax_shapes=True)\ndef classify_eval_step(X, y):\n    pred = classify_model(X, training=True)\n    loss = classify_loss_func(y, pred)\n    return loss\n\n\nfor i in range(NUM_EPOCH):\n    print(\"epoch \", i+1)\n    with tqdm(total=len(dev_ds)) as pbar:\n        step = 0\n        for ix in dev_ds:\n            tix, X, y = make_feat_target(ix, dev_filter)\n            y = tf.expand_dims(y, axis=-1)\n            loss = classify_forward_step(X, y)\n            classify_dev_loss(loss)\n            learning_text = \"[{}/{}] \".format(str(step).zfill(5), len(dev_ds))\n            progress_text = \"dev_loss: {:.4f} val_loss: {:.4f}\".format(classify_dev_loss.result().numpy(),\n                                                                       classify_val_loss.result().numpy(),\n                                                                       )\n            pbar.set_postfix_str(learning_text + progress_text)\n            step += 1\n            pbar.update(1)\n    with tqdm(total=len(val_ds)) as pbar:\n        step = 0\n        for ix in val_ds:\n            tix, X, y = make_feat_target(ix, val_filter)\n            y = tf.expand_dims(y, axis=-1)\n            loss = classify_eval_step(X, y)\n            classify_val_loss(loss)\n            learning_text = \"[{}/{}] \".format(str(step).zfill(5), len(val_ds))\n            progress_text = \"dev_loss: {:.4f} val_loss: {:.4f}\".format(classify_dev_loss.result().numpy(),\n                                                                       classify_val_loss.result().numpy(),\n                                                                       )\n            pbar.set_postfix_str(learning_text + progress_text)\n            step += 1\n            pbar.update(1)\n    classify_dev_loss.reset_states()\n    classify_val_loss.reset_states()\n    ","metadata":{"ExecuteTime":{"end_time":"2022-06-11T20:16:55.783119Z","start_time":"2022-06-11T18:01:28.264056Z"},"execution":{"iopub.status.busy":"2022-06-12T02:29:40.27418Z","iopub.execute_input":"2022-06-12T02:29:40.27463Z","iopub.status.idle":"2022-06-12T02:33:20.749572Z","shell.execute_reply.started":"2022-06-12T02:29:40.274594Z","shell.execute_reply":"2022-06-12T02:33:20.747721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@tf.function(experimental_relax_shapes=True)\ndef make_pred_feat(ix):\n    emb = tf.gather(embeddings, ix)\n    query_position = tf.expand_dims(train_container.get_position(ix), axis=1)\n    \n    dist = train_container.log_haversine(query_position, train_container.position)\n    cossim = tf.einsum(\"nd,md->nm\", emb, embeddings)\n    score = tf.einsum(\"d,nmd->nm\", logistic_model.scale, tf.stack([dist, cossim], axis=-1))\n    _, candidate_ixs = tf.math.top_k(score, k=NUM_NEGATIVE)\n    candidate_ixs  = tf.cast(candidate_ixs, \"int32\")\n    \n    candidate_cossim = tf.gather(cossim, candidate_ixs, batch_dims=1)\n    candidate_dist = tf.gather(dist, candidate_ixs, batch_dims=1)\n    query_embedding = tf.repeat(tf.expand_dims(emb, axis=1), NUM_NEGATIVE, axis=1)\n    candidate_embedding = tf.gather(embeddings, candidate_ixs)\n    candidate_feat = tf.concat([query_embedding, candidate_embedding, tf.stack([candidate_dist, candidate_cossim], axis=-1)], axis=-1)\n    return candidate_ixs, candidate_feat\n\npos_labels = []\ncandidate_ixs = []\ncandidate_scores = []\n\nfor ix in tqdm(val_ds):\n    candidate_ix, candidate_feat = make_pred_feat(ix)\n    candidate_score = tf.math.sigmoid(tf.reshape(classify_model(candidate_feat), tf.shape(candidate_ix)))\n    \n    pos_ix = tf.gather(train_container.positive_ix, tf.gather(train_container.pid, ix))\n    pos_ix = pos_ix.to_tensor(default_value=-2)\n    pos_ix = tf.slice(pos_ix, [0, 0], [-1, tf.minimum(NUM_NEGATIVE, tf.shape(pos_ix)[1])])\n    update_ix = tf.where(pos_ix >= 0)\n    target_ixs = tf.tensor_scatter_nd_update(tf.ones([len(pos_ix), NUM_NEGATIVE], dtype=\"int32\")*-2, update_ix, tf.gather_nd(pos_ix, update_ix))\n    pos_labels.append(target_ixs)\n    candidate_ixs.append(candidate_ix)\n    candidate_scores.append(candidate_score)","metadata":{"ExecuteTime":{"end_time":"2022-06-11T20:19:34.353734Z","start_time":"2022-06-11T20:16:55.784119Z"},"execution":{"iopub.status.busy":"2022-06-12T02:33:27.673568Z","iopub.execute_input":"2022-06-12T02:33:27.673958Z","iopub.status.idle":"2022-06-12T02:39:49.641263Z","shell.execute_reply.started":"2022-06-12T02:33:27.673927Z","shell.execute_reply":"2022-06-12T02:39:49.63998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pos_labels = tf.concat(pos_labels, axis=0)\ncandidate_ixs = tf.concat(candidate_ixs, axis=0)\ncandidate_scores = tf.concat(candidate_scores, axis=0)","metadata":{"ExecuteTime":{"end_time":"2022-06-11T20:19:34.369734Z","start_time":"2022-06-11T20:19:34.354734Z"},"execution":{"iopub.status.busy":"2022-06-12T02:39:55.058716Z","iopub.execute_input":"2022-06-12T02:39:55.059295Z","iopub.status.idle":"2022-06-12T02:39:55.131353Z","shell.execute_reply.started":"2022-06-12T02:39:55.05925Z","shell.execute_reply":"2022-06-12T02:39:55.130311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"candidate_correct = tf.reduce_any(tf.expand_dims(candidate_ixs, axis=2) == tf.expand_dims(pos_labels, axis=1), axis=2)","metadata":{"ExecuteTime":{"end_time":"2022-06-11T20:19:34.433734Z","start_time":"2022-06-11T20:19:34.370735Z"},"execution":{"iopub.status.busy":"2022-06-12T02:39:58.466366Z","iopub.execute_input":"2022-06-12T02:39:58.46679Z","iopub.status.idle":"2022-06-12T02:39:58.932818Z","shell.execute_reply.started":"2022-06-12T02:39:58.466759Z","shell.execute_reply":"2022-06-12T02:39:58.931694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_score = -1\nbest_threshold = 0.\nfor threshold in np.linspace(0, 1, 100):\n    true_count = tf.cast(tf.maximum(1, tf.math.count_nonzero(pos_labels >=0, axis=1)), \"float32\")\n    predict = candidate_scores > threshold\n    correct_count = tf.cast(tf.math.count_nonzero(tf.logical_and(candidate_correct, predict), axis=1), \"float32\")\n    predict_count = tf.cast(tf.math.count_nonzero(predict, axis=1), \"float32\")\n\n    score = tf.math.reduce_mean(correct_count / (true_count + predict_count - correct_count))\n    \n    if best_score < score:\n        best_score = score\n        best_threshold = threshold\n        \nprint(best_threshold, best_score)","metadata":{"ExecuteTime":{"end_time":"2022-06-12T01:16:56.165475Z","start_time":"2022-06-12T01:16:55.670477Z"},"execution":{"iopub.status.busy":"2022-06-12T02:40:06.350045Z","iopub.execute_input":"2022-06-12T02:40:06.350662Z","iopub.status.idle":"2022-06-12T02:40:07.242496Z","shell.execute_reply.started":"2022-06-12T02:40:06.350619Z","shell.execute_reply":"2022-06-12T02:40:07.241165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del all_container\ndel train_container\ndel embeddings\ngc.collect()\n\ntest_container = all_container = DataContainer(test_df[container_cols],\n                                                tf.cast(tf.ragged.constant(test_df.groupby(\"pid\")[\"ix\"].unique().tolist()), \"int32\"))\n\ntest_ds = tf.data.Dataset.from_tensor_slices(tf.range(len(test_df)))\\\n                .batch(1024)\\\n                .map(test_container.call)\n\nembeddings = []\nfor ix, name, position in tqdm(test_ds):\n    embeddings.append(tf.nn.l2_normalize(tf.reduce_sum(spe(name), axis=1), axis=1))\nembeddings = tf.nn.l2_normalize(tf.concat(embeddings, axis=0), axis=1)","metadata":{"ExecuteTime":{"end_time":"2022-06-12T01:49:12.224127Z","start_time":"2022-06-12T01:49:12.178981Z"},"execution":{"iopub.status.busy":"2022-06-12T02:40:41.488262Z","iopub.execute_input":"2022-06-12T02:40:41.488955Z","iopub.status.idle":"2022-06-12T02:41:12.133943Z","shell.execute_reply.started":"2022-06-12T02:40:41.488915Z","shell.execute_reply":"2022-06-12T02:41:12.132604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NUM_NEGATIVE = min(len(test_df), NUM_NEGATIVE)\ntest_ds = tf.data.Dataset.from_tensor_slices(tf.constant(tf.range(len(test_df))))\\\n                .batch(128)\n\n@tf.function(experimental_relax_shapes=True)\ndef make_pred_feat(ix):\n    emb = tf.gather(embeddings, ix)\n    query_position = tf.expand_dims(test_container.get_position(ix), axis=1)\n    \n    dist = test_container.log_haversine(query_position, test_container.position)\n    cossim = tf.einsum(\"nd,md->nm\", emb, embeddings)\n    score = tf.einsum(\"d,nmd->nm\", logistic_model.scale, tf.stack([dist, cossim], axis=-1))\n    _, candidate_ixs = tf.math.top_k(score, k=NUM_NEGATIVE)\n    candidate_ixs  = tf.cast(candidate_ixs, \"int32\")\n    \n    candidate_cossim = tf.gather(cossim, candidate_ixs, batch_dims=1)\n    candidate_dist = tf.gather(dist, candidate_ixs, batch_dims=1)\n    query_embedding = tf.repeat(tf.expand_dims(emb, axis=1), NUM_NEGATIVE, axis=1)\n    candidate_embedding = tf.gather(embeddings, candidate_ixs)\n    candidate_feat = tf.concat([query_embedding, candidate_embedding, tf.stack([candidate_dist, candidate_cossim], axis=-1)], axis=-1)\n    return candidate_ixs, candidate_feat\n\ntest_ids = test_df[\"id\"].values\nresults = []\n\nfor ix in tqdm(test_ds):\n    candidate_ix, candidate_feat = make_pred_feat(ix)\n    candidate_score = tf.math.sigmoid(tf.reshape(classify_model(candidate_feat), tf.shape(candidate_ix)))\n    candidate_score = tf.where(candidate_ix != tf.expand_dims(ix, axis=1), candidate_score, -1.)\n    pred_loc = tf.where(candidate_score > best_threshold)\n    res = [{\"id\": test_ids[i], \"matches\": test_ids[i]} for i in ix.numpy()]\n    for i, cix in zip(tf.gather(pred_loc, 0, axis=1).numpy(), tf.gather_nd(candidate_ix, pred_loc).numpy()):\n        res[i][\"matches\"] += \" \" + test_ids[cix] \n    results.extend(res)    ","metadata":{"ExecuteTime":{"end_time":"2022-06-12T01:52:05.264108Z","start_time":"2022-06-12T01:52:05.14011Z"},"execution":{"iopub.status.busy":"2022-06-12T02:41:29.831523Z","iopub.execute_input":"2022-06-12T02:41:29.831948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(results).to_csv(\"submission.csv\", index=None, mode=\"w\")","metadata":{"ExecuteTime":{"end_time":"2022-06-12T01:52:31.83766Z","start_time":"2022-06-12T01:52:31.818255Z"}},"execution_count":null,"outputs":[]}]}