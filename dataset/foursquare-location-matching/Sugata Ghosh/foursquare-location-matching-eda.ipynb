{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1><center> Foursquare Location Matching </center></h1>\n<h2><center> Exploratory Data Analysis </center></h2>\n<h2><center> Sugata Ghosh </center></h2>","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"### Contents\n\n- [Introduction](#1.-Introduction)\n- [Basic Data Exploration](#2.-Basic-Data-Exploration)\n- [Univariate Analysis - Training Set](#3.-Univariate-Analysis---Training-Set)\n- [Multivariate Analysis - Training Set](#4.-Multivariate-Analysis---Training-Set)\n- [Univariate Analysis - Pairs Set](#5.-Univariate-Analysis---Pairs-Set)\n- [Multivariate Analysis - Pairs Set](#6.-Multivariate-Analysis---Pairs-Set)\n- [Acknowledgements](#Acknowledgements)\n- [References](#References)","metadata":{}},{"cell_type":"markdown","source":"### Importing libraries","metadata":{}},{"cell_type":"code","source":"# File system manangement\nimport time, psutil, os, gc\n\n# Mathematical functions\nimport math\n\n# Data manipulation\nimport numpy as np\nimport pandas as pd\n\n# Plotting and visualization\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport seaborn as sns\nsns.set_theme()\nimport plotly\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\n\n# Others\nimport operator as op\nfrom functools import reduce, lru_cache\n\n# Suppress or allow warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:37:17.117407Z","iopub.execute_input":"2022-06-22T15:37:17.117913Z","iopub.status.idle":"2022-06-22T15:37:18.535595Z","shell.execute_reply.started":"2022-06-22T15:37:17.117815Z","shell.execute_reply":"2022-06-22T15:37:18.534381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Runtime and memory usage","metadata":{}},{"cell_type":"code","source":"# Recording the starting time, complemented with a stopping time check in the end to compute process runtime\nstart = time.time()\n\n# Class representing the OS process and having memory_info() method to compute process memory usage\nprocess = psutil.Process(os.getpid())","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:37:18.537619Z","iopub.execute_input":"2022-06-22T15:37:18.53815Z","iopub.status.idle":"2022-06-22T15:37:18.544118Z","shell.execute_reply.started":"2022-06-22T15:37:18.538098Z","shell.execute_reply":"2022-06-22T15:37:18.542957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Introduction\n\n- [Point of Interest (POI)](#Point-of-Interest-(POI))\n- [The Problem of POI Matching](#The-Problem-of-POI-Matching)\n- [About Foursquare](#About-Foursquare)\n- [Data](#Data)\n- [Project Objective](#Project-Objective)\n- [Evaluation Metric](#Evaluation-Metric)","metadata":{}},{"cell_type":"markdown","source":"## Point of Interest (POI)\n\nA [point of interest](https://en.wikipedia.org/wiki/Point_of_interest) (POI) is a specific point location that someone may find useful or interesting. An example is a point on the Earth representing the location of the Eiffel Tower, or a point on Mars representing the location of its highest mountain, [Olympus Mons](https://en.wikipedia.org/wiki/Olympus_Mons). Most consumers use the term when referring to hotels, campsites, fuel stations or any other categories used in modern automotive navigation systems. Users of a mobile device can be provided with geolocation and time aware POI service that recommends geolocations nearby and with a temporal relevance (e.g. POI to special services in a ski resort are available only in winter). The notion of POI is widely used in cartography, especially in electronic variants including GIS, and GPS navigation software.","metadata":{}},{"cell_type":"markdown","source":"## The Problem of POI Matching\n\nIt is useful to combine POI data obtained from multiple sources for effective reusability. One issue in merging such data is that different dataset may have variations in POI name, address, and other identifying information for the same POI. It is thus important to identify observations which refer to the same POI. The process of POI matching involves finding POI pairs that refer to the same real-world entity, which is the core issue in geospatial data integration and is perhaps the most technically difficult part of multi-source POI fusion. The raw location data can contain noise, unstructured information, and incomplete or inaccurate attributes, which makes the task even more difficult. Nonetheless, to maintain the highest level of accuracy, the data must be matched and duplicate POIs must be identified and merged with timely updates from multiple sources. A combination of machine-learning algorithms and rigorous human validation methods are optimal for effective de-duplication of such data.","metadata":{}},{"cell_type":"markdown","source":"## About Foursquare\n\n[Foursquare Labs Inc.](https://foursquare.com/), commonly known as Foursquare, is an American location technology company and data cloud platform. The company's location platform is the foundation of several business and consumer products, including the [Foursquare City Guide](https://en.wikipedia.org/wiki/Foursquare_City_Guide) and [Foursquare Swarm](https://en.wikipedia.org/wiki/Foursquare_Swarm) apps. Foursquare's products include Pilgrim SDK, Places, Visits, Attribution, Audience, Proximity, and Unfolded Studio. It is one of the leading independent providers of global POI data and is dedicated to building meaningful bridges between digital spaces and physical places. Trusted by leading enterprises like Apple, Microsoft, Samsung, and Uber, Foursquare's tech stack harnesses the power of places and movement to improve customer experiences and drive better business outcomes.","metadata":{}},{"cell_type":"markdown","source":"## Data\n\n**Source:** https://www.kaggle.com/competitions/foursquare-location-matching/data\n\nThe data considered in the competition comprises over one-and-a-half million place entries for hundreds of thousands of commercial Points-of-Interest (POIs) around the globe. Though the data entries may represent or resemble entries for real places, they may be contaminated with artificial information or additional noise.\n\nThe training data comprises eleven attribute fields for over one million place entries, together with:\n- `id` : A unique identifier for each entry.\n- `point_of_interest` : An identifier for the POI the entry represents. There may be one or many entries describing the same POI. Two entries *match* when they describe a common POI.","metadata":{}},{"cell_type":"code","source":"# Loading the training data\ndata_train = pd.read_csv('../input/foursquare-location-matching/train.csv')\nprint(pd.Series({\"Memory usage\": \"{:.2f} MB\".format(data_train.memory_usage().sum()/(1024*1024)),\n                 \"Dataset shape\": \"{}\".format(data_train.shape)}).to_string())\nprint(\" \")\ndata_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:37:18.54531Z","iopub.execute_input":"2022-06-22T15:37:18.546038Z","iopub.status.idle":"2022-06-22T15:37:25.855335Z","shell.execute_reply.started":"2022-06-22T15:37:18.545997Z","shell.execute_reply":"2022-06-22T15:37:25.854262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# A typical observation from the training set\ndata_train.iloc[0]","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:37:25.857461Z","iopub.execute_input":"2022-06-22T15:37:25.85788Z","iopub.status.idle":"2022-06-22T15:37:25.866423Z","shell.execute_reply.started":"2022-06-22T15:37:25.857846Z","shell.execute_reply":"2022-06-22T15:37:25.865259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The pairs data is a pregenerated set of pairs of place entries from the training data designed to improve detection of matches. It includes:\n- `match` : Boolean variables denoting whether or not the pair of entries describes a common POI.","metadata":{}},{"cell_type":"code","source":"# Loading pregenerated set of pairs of place entries from the training data\ndata_pairs = pd.read_csv('../input/foursquare-location-matching/pairs.csv')\nprint(pd.Series({\"Memory usage\": \"{:.2f} MB\".format(data_pairs.memory_usage().sum()/(1024*1024)),\n                 \"Dataset shape\": \"{}\".format(data_pairs.shape)}).to_string())\nprint(\" \")\ndata_pairs.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:37:25.867743Z","iopub.execute_input":"2022-06-22T15:37:25.868107Z","iopub.status.idle":"2022-06-22T15:37:32.749661Z","shell.execute_reply.started":"2022-06-22T15:37:25.868075Z","shell.execute_reply":"2022-06-22T15:37:32.748617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# A typical observation from the pregenerated set of pairs\ndata_pairs.iloc[0]","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:37:32.751393Z","iopub.execute_input":"2022-06-22T15:37:32.752365Z","iopub.status.idle":"2022-06-22T15:37:32.763849Z","shell.execute_reply.started":"2022-06-22T15:37:32.752313Z","shell.execute_reply":"2022-06-22T15:37:32.762634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The test data comprises a set of place entries with their recorded attribute fields, similar to the training set. The POIs in the test data are distinct from the POIs in the training data.","metadata":{}},{"cell_type":"code","source":"# Loading the test data\ndata_test = pd.read_csv('../input/foursquare-location-matching/test.csv')\nprint(pd.Series({\"Memory usage\": \"{:.5f} MB\".format(data_test.memory_usage().sum()/(1024*1024)),\n                 \"Dataset shape\": \"{}\".format(data_test.shape)}).to_string())\nprint(\" \")\ndata_test.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:37:32.76528Z","iopub.execute_input":"2022-06-22T15:37:32.765694Z","iopub.status.idle":"2022-06-22T15:37:32.799958Z","shell.execute_reply.started":"2022-06-22T15:37:32.765654Z","shell.execute_reply":"2022-06-22T15:37:32.799153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# A typical observation from the test set\ndata_test.iloc[0]","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:37:32.801539Z","iopub.execute_input":"2022-06-22T15:37:32.801915Z","iopub.status.idle":"2022-06-22T15:37:32.810322Z","shell.execute_reply.started":"2022-06-22T15:37:32.801881Z","shell.execute_reply":"2022-06-22T15:37:32.809205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Project Objective\n\nThe goal of the project is to match POIs together. Using the provided dataset of over one-and-a-half million places entries, heavily altered to include noise, duplications, extraneous, or incorrect information, the objective is to produce an algorithm that predicts which place entries represent the same POI. Each place entry in the data includes useful attributes like name, street address, and coordinates. Efficient and successful matching of POIs will make it easier to identify where new stores or businesses would benefit people the most.","metadata":{}},{"cell_type":"markdown","source":"## Evaluation Metric\n\n**[Jaccard index](https://en.wikipedia.org/wiki/Jaccard_index).** Also known as *Jaccard similarity coefficient*, it is a statistic used for gauging the similarity and diversity of sample sets. It was developed by [Grove Karl Gilbert](https://en.wikipedia.org/wiki/Grove_Karl_Gilbert) in 1884 as his *ratio of verification (v)* and now is frequently referred to as the *Critical Success Index* in meteorology. It was later developed independently by [Paul Jaccard](https://en.wikipedia.org/wiki/Paul_Jaccard), originally giving the French name *coefficient de communaut√©* and independently formulated again by T. T. Tanimoto. Thus, the *Tanimoto index* or *Tanimoto coefficient* are also used in some fields. However, they are identical in generally taking the ratio of Intersection over Union. The Jaccard coefficient measures similarity between finite sample sets, and is defined as the size of the intersection divided by the size of the union of the sample sets:\n\n$$ J(A, B) := \\frac{\\left\\vert A \\cap B \\right\\vert}{\\left\\vert A \\cup B \\right\\vert} = \\frac{\\left\\vert A \\cap B \\right\\vert}{\\left\\vert A \\right\\vert + \\left\\vert B \\right\\vert - \\left\\vert A \\cap B \\right\\vert}. $$\n\nNote that by design, $0\\leq J\\left(A, B\\right)\\leq 1$. If $A$ and $B$ are both empty, define $J(A, B) = 1$. The Jaccard coefficient is widely used in computer science, ecology, genomics, and other sciences, where binary or binarized data are used. Both the exact solution and approximation methods are available for hypothesis testing with the Jaccard coefficient. See [this paper](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-019-3118-5) ([arxiv version](https://arxiv.org/abs/1903.11372)) for details.\n\nLet us assume that for a specific `id` $a$, our algorithm produces three matches $a$, $b$ and $c$ whereas the true matches are $a$, $b$, $d$ and $e$. Then the Jaccard index for the prediction on this particular `id` will be\n\n$$ \\frac{\\left\\vert \\left\\{a, b, c\\right\\} \\cap \\left\\{a, b, d, e\\right\\} \\right\\vert}{\\left\\vert \\left\\{a, b, c\\right\\} \\cup \\left\\{a, b, d, e\\right\\} \\right\\vert} = \\frac{\\left\\vert \\left\\{a, b\\right\\} \\right\\vert}{\\left\\vert \\left\\{a, b, c, d, e\\right\\} \\right\\vert} = \\frac{2}{5}. $$\n\nThus, while correct matching predictions are rewarded, incorrect matching predictions are penalised by equal measure. The evaluation metric is simply the mean of Jaccard indices for each of the test observations, i.e. if the test data comprises $n_{\\text{test}}$ observations and $J_i$ denotes the Jaccard index corresponding to the $i$th test observation, $i = 1,2,\\cdots,n_{\\text{test}}$, then the final metric by which a model will be evaluated is:\n\n$$ \\frac{1}{n_{\\text{test}}} \\sum_{i=1}^{n_{\\text{test}}} J_i. $$","metadata":{}},{"cell_type":"markdown","source":"# 2. Basic Data Exploration","metadata":{}},{"cell_type":"code","source":"# Shape of the data\nprint(pd.Series({\"Shape of the training set\": data_train.shape,\n                 \"Shape of the pregenerated set of pairs\": data_pairs.shape,\n                 \"Shape of the test set\": data_test.shape}).to_string())","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:37:32.812033Z","iopub.execute_input":"2022-06-22T15:37:32.812984Z","iopub.status.idle":"2022-06-22T15:37:32.823474Z","shell.execute_reply.started":"2022-06-22T15:37:32.812946Z","shell.execute_reply":"2022-06-22T15:37:32.822389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Count of observations in the data\nprint(pd.Series({\"Number of observations in the training set\": len(data_train),\n                 \"Number of observations in the pregenerated set of pairs\": len(data_pairs),\n                 \"Number of observations in the test set\": len(data_test)}).to_string())","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:37:32.824917Z","iopub.execute_input":"2022-06-22T15:37:32.825475Z","iopub.status.idle":"2022-06-22T15:37:32.835671Z","shell.execute_reply.started":"2022-06-22T15:37:32.825436Z","shell.execute_reply":"2022-06-22T15:37:32.834568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is evident that the test set, while it does serve as a snapshot of the far larger test data used in the evaluation process of the submitted predictions in [the competition](https://www.kaggle.com/competitions/foursquare-location-matching), has too few observations to do anything with. We shall have to split the training set and use part of it for testing purpose.","metadata":{}},{"cell_type":"code","source":"# Count of columns in the data\nprint(pd.Series({\"Number of columns in the training set\": len(data_train.columns),\n                 \"Number of columns in the pregenerated set of pairs\": len(data_pairs.columns),\n                 \"Number of columns in the test set\": len(data_test.columns)}).to_string())","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:37:32.837307Z","iopub.execute_input":"2022-06-22T15:37:32.837991Z","iopub.status.idle":"2022-06-22T15:37:32.850145Z","shell.execute_reply.started":"2022-06-22T15:37:32.837949Z","shell.execute_reply":"2022-06-22T15:37:32.848973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Column names for the training set\ndata_train.columns.tolist()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:37:32.851726Z","iopub.execute_input":"2022-06-22T15:37:32.852116Z","iopub.status.idle":"2022-06-22T15:37:32.866368Z","shell.execute_reply.started":"2022-06-22T15:37:32.852077Z","shell.execute_reply":"2022-06-22T15:37:32.8655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Column names for the pregenerated set of pairs\ndata_pairs.columns.tolist()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:37:32.871101Z","iopub.execute_input":"2022-06-22T15:37:32.87182Z","iopub.status.idle":"2022-06-22T15:37:32.883848Z","shell.execute_reply.started":"2022-06-22T15:37:32.871763Z","shell.execute_reply":"2022-06-22T15:37:32.882698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Column names for the test set\ndata_test.columns.tolist()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:37:32.885584Z","iopub.execute_input":"2022-06-22T15:37:32.887013Z","iopub.status.idle":"2022-06-22T15:37:32.896686Z","shell.execute_reply.started":"2022-06-22T15:37:32.886965Z","shell.execute_reply":"2022-06-22T15:37:32.895792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Columns in the training set which are not in the test set\n[col for col in data_train.columns if col not in data_test.columns]","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:37:32.898292Z","iopub.execute_input":"2022-06-22T15:37:32.898919Z","iopub.status.idle":"2022-06-22T15:37:32.912542Z","shell.execute_reply.started":"2022-06-22T15:37:32.898882Z","shell.execute_reply":"2022-06-22T15:37:32.911543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Column datatypes for the training set\ndata_train.dtypes","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:37:32.913879Z","iopub.execute_input":"2022-06-22T15:37:32.91513Z","iopub.status.idle":"2022-06-22T15:37:32.934363Z","shell.execute_reply.started":"2022-06-22T15:37:32.915086Z","shell.execute_reply":"2022-06-22T15:37:32.933041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Count of column datatypes for the training set\nprint(pd.Series({\"Number of integer columns\": len(data_train.columns[data_train.dtypes == 'int64']),\n                 \"Number of float columns\": len(data_train.columns[data_train.dtypes == 'float64']),\n                 \"Number of object columns\": len(data_train.columns[data_train.dtypes == 'object'])}).to_string())","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:37:32.935976Z","iopub.execute_input":"2022-06-22T15:37:32.936376Z","iopub.status.idle":"2022-06-22T15:37:32.951044Z","shell.execute_reply.started":"2022-06-22T15:37:32.936341Z","shell.execute_reply":"2022-06-22T15:37:32.949934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Column datatypes for the pregenerated set of pairs\ndata_pairs.dtypes","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:37:32.952153Z","iopub.execute_input":"2022-06-22T15:37:32.953013Z","iopub.status.idle":"2022-06-22T15:37:32.966841Z","shell.execute_reply.started":"2022-06-22T15:37:32.952971Z","shell.execute_reply":"2022-06-22T15:37:32.965802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Count of column datatypes for the pregenerated set of pairs\nprint(pd.Series({\"Number of integer columns\": len(data_pairs.columns[data_pairs.dtypes == 'int64']),\n                 \"Number of float columns\": len(data_pairs.columns[data_pairs.dtypes == 'float64']),\n                 \"Number of object columns\": len(data_pairs.columns[data_pairs.dtypes == 'object']),\n                 \"Number of Boolean columns\": len(data_pairs.columns[data_pairs.dtypes == 'bool'])}).to_string())","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:37:32.968079Z","iopub.execute_input":"2022-06-22T15:37:32.968759Z","iopub.status.idle":"2022-06-22T15:37:32.985155Z","shell.execute_reply.started":"2022-06-22T15:37:32.96872Z","shell.execute_reply":"2022-06-22T15:37:32.983804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Column datatypes for the test set\ndata_test.dtypes","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:37:32.987475Z","iopub.execute_input":"2022-06-22T15:37:32.988085Z","iopub.status.idle":"2022-06-22T15:37:32.999379Z","shell.execute_reply.started":"2022-06-22T15:37:32.988004Z","shell.execute_reply":"2022-06-22T15:37:32.998299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Count of column datatypes for the test set\nprint(pd.Series({\"Number of integer columns\": len(data_test.columns[data_test.dtypes == 'int64']),\n                 \"Number of float columns\": len(data_test.columns[data_test.dtypes == 'float64']),\n                 \"Number of object columns\": len(data_test.columns[data_test.dtypes == 'object'])}).to_string())","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:37:33.00082Z","iopub.execute_input":"2022-06-22T15:37:33.001319Z","iopub.status.idle":"2022-06-22T15:37:33.015329Z","shell.execute_reply.started":"2022-06-22T15:37:33.001282Z","shell.execute_reply":"2022-06-22T15:37:33.014435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The columns of the test set are exactly the first $12$ columns of the training set, i.e. all columns except `point_of_interest`. The columns of the pairs set are two times replication of these $12$ columns, for two observations, plus the Boolean column `match` which indicates whether or not the two observations refer to the same real-world entity. Note that the columns `zip` and `phone` have object datatype in the training set and `float` datatype in the test set. The underlying reason for this may be the fact that the provided test set has only $5$ observations and hence does not capture the general picture.","metadata":{}},{"cell_type":"code","source":"# Number of unique values in the training set columns\ndata_train.nunique()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:37:33.016949Z","iopub.execute_input":"2022-06-22T15:37:33.018262Z","iopub.status.idle":"2022-06-22T15:37:36.563447Z","shell.execute_reply.started":"2022-06-22T15:37:33.018204Z","shell.execute_reply":"2022-06-22T15:37:36.562359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Number of unique values in the training set columns by percentage\nplt.figure(figsize = (9, 13 / 3))\ndata_temp = (data_train.nunique() / len(data_train)) * 100\ns = sns.barplot(x = data_temp.values, y = data_temp.index)\ns.set_xlim(0, 100)\n# s.bar_label(s.containers[0])\ns.set(xlabel = \"% of unique values\", ylabel = \"column\")\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:37:36.564843Z","iopub.execute_input":"2022-06-22T15:37:36.565201Z","iopub.status.idle":"2022-06-22T15:37:40.239573Z","shell.execute_reply.started":"2022-06-22T15:37:36.565155Z","shell.execute_reply":"2022-06-22T15:37:40.238248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Number of unique values in the columns of the pregenerated set of pairs\ndata_pairs.nunique()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:37:40.241419Z","iopub.execute_input":"2022-06-22T15:37:40.242167Z","iopub.status.idle":"2022-06-22T15:37:42.749369Z","shell.execute_reply.started":"2022-06-22T15:37:40.242122Z","shell.execute_reply":"2022-06-22T15:37:42.748266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Number of unique values in the columns of the pregenerated set of pairs by percentage\nplt.figure(figsize = (9, 25 / 3))\ndata_temp = (data_pairs.nunique() / len(data_pairs)) * 100\ns = sns.barplot(x = data_temp.values, y = data_temp.index)\ns.set_xlim(0, 100)\n# s.bar_label(s.containers[0])\ns.set(xlabel = \"% of unique values\", ylabel = \"column\")\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:37:42.751212Z","iopub.execute_input":"2022-06-22T15:37:42.751715Z","iopub.status.idle":"2022-06-22T15:37:45.465706Z","shell.execute_reply.started":"2022-06-22T15:37:42.751669Z","shell.execute_reply":"2022-06-22T15:37:45.464385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Number of unique values in the test set columns\ndata_test.nunique()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:37:45.467046Z","iopub.execute_input":"2022-06-22T15:37:45.467433Z","iopub.status.idle":"2022-06-22T15:37:45.478609Z","shell.execute_reply.started":"2022-06-22T15:37:45.467399Z","shell.execute_reply":"2022-06-22T15:37:45.477502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Number of unique values in the test set columns by percentage\nplt.figure(figsize = (9, 4))\ndata_temp = (data_test.nunique() / len(data_test)) * 100\ns = sns.barplot(x = data_temp.values, y = data_temp.index)\ns.set_xlim(0, 100)\n# s.bar_label(s.containers[0])\ns.set(xlabel = \"% of unique values\", ylabel = \"column\")\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:37:45.480114Z","iopub.execute_input":"2022-06-22T15:37:45.480859Z","iopub.status.idle":"2022-06-22T15:37:45.755045Z","shell.execute_reply.started":"2022-06-22T15:37:45.480816Z","shell.execute_reply":"2022-06-22T15:37:45.754277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Count of duplicate rows\nprint(pd.Series({\"Number of duplicate rows in the training set\": data_train.duplicated().sum(),\n                 \"Number of duplicate rows in the pregenerated set of pairs\": data_pairs.duplicated().sum(),\n                 \"Number of duplicate rows in the test set\": data_test.duplicated().sum()}).to_string())","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:37:45.756784Z","iopub.execute_input":"2022-06-22T15:37:45.757525Z","iopub.status.idle":"2022-06-22T15:37:52.890119Z","shell.execute_reply.started":"2022-06-22T15:37:45.757475Z","shell.execute_reply":"2022-06-22T15:37:52.889092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Constant columns in the training set\ncols_constant_train = data_train.columns[data_train.nunique() == 1].tolist()\nif len(cols_constant_train) == 0:\n    cols_constant_train = \"None\"\nprint(pd.Series({\"Constant columns in the training set\": cols_constant_train}).to_string())","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:37:52.891344Z","iopub.execute_input":"2022-06-22T15:37:52.891698Z","iopub.status.idle":"2022-06-22T15:37:56.179885Z","shell.execute_reply.started":"2022-06-22T15:37:52.891667Z","shell.execute_reply":"2022-06-22T15:37:56.179113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Constant columns in the pregenerated set of pairs\ncols_constant_pairs = data_pairs.columns[data_pairs.nunique() == 1].tolist()\nif len(cols_constant_pairs) == 0:\n    cols_constant_pairs = \"None\"\nprint(pd.Series({\"Constant columns in the pregenerated set of pairs\": cols_constant_pairs}).to_string())","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:37:56.181251Z","iopub.execute_input":"2022-06-22T15:37:56.181839Z","iopub.status.idle":"2022-06-22T15:37:58.498435Z","shell.execute_reply.started":"2022-06-22T15:37:56.181802Z","shell.execute_reply":"2022-06-22T15:37:58.497156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Constant columns in the test set\ncols_constant_test = data_test.columns[data_test.nunique() == 1].tolist()\nif len(cols_constant_test) == 0:\n    cols_constant_test = \"None\"\nprint(pd.Series({\"Constant columns in the test set\": cols_constant_test}).to_string())","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:37:58.500018Z","iopub.execute_input":"2022-06-22T15:37:58.500952Z","iopub.status.idle":"2022-06-22T15:37:58.512391Z","shell.execute_reply.started":"2022-06-22T15:37:58.500899Z","shell.execute_reply":"2022-06-22T15:37:58.511461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Count of columns with missing values\nprint(pd.Series({\"Number of columns with missing values in the training set\": len(data_train.isna().sum()[data_train.isna().sum() != 0]),\n                 \"Number of columns with missing values in the pregenerated set of pairs\": len(data_pairs.isna().sum()[data_pairs.isna().sum() != 0]),\n                 \"Number of columns with missing values in the test set\": len(data_test.isna().sum()[data_test.isna().sum() != 0])}).to_string())","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:37:58.51361Z","iopub.execute_input":"2022-06-22T15:37:58.514455Z","iopub.status.idle":"2022-06-22T15:38:03.068084Z","shell.execute_reply.started":"2022-06-22T15:37:58.514413Z","shell.execute_reply":"2022-06-22T15:38:03.066864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Columns with missing values in the training set with respective proportion of missing values\n(data_train.isna().sum()[data_train.isna().sum() != 0]/len(data_train)).sort_values(ascending = False)","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:38:03.069491Z","iopub.execute_input":"2022-06-22T15:38:03.069841Z","iopub.status.idle":"2022-06-22T15:38:05.428419Z","shell.execute_reply.started":"2022-06-22T15:38:03.069804Z","shell.execute_reply":"2022-06-22T15:38:05.427425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Missing values in the training set\nplt.figure(figsize = (9, 13 / 3))\ndata_temp = (data_train.isna().sum() * 100 / len(data_train)) #.sort_values()\ns = sns.barplot(x = data_temp.values, y = data_temp.index)\ns.set_xlim(0, 100)\n# s.bar_label(s.containers[0])\ns.set(xlabel = \"% of missing values\", ylabel = \"column\")\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:38:05.430041Z","iopub.execute_input":"2022-06-22T15:38:05.431762Z","iopub.status.idle":"2022-06-22T15:38:06.915267Z","shell.execute_reply.started":"2022-06-22T15:38:05.431694Z","shell.execute_reply":"2022-06-22T15:38:06.913712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Three columns `zip`, `phone` and `url` have over $50\\%$ values missing, while two more columns `address` and `state` have over $30\\%$ values missing.","metadata":{}},{"cell_type":"code","source":"# Columns with missing values in the pregenerated set of pairs with respective proportion of missing values\n(data_pairs.isna().sum()[data_pairs.isna().sum() != 0]/len(data_pairs)).sort_values(ascending = False)","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:38:06.91716Z","iopub.execute_input":"2022-06-22T15:38:06.917817Z","iopub.status.idle":"2022-06-22T15:38:09.079218Z","shell.execute_reply.started":"2022-06-22T15:38:06.917768Z","shell.execute_reply":"2022-06-22T15:38:09.077923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Missing values in the pregenerated set of pairs\nplt.figure(figsize = (9, 25 / 3))\ndata_pairs_missing = (data_pairs.isna().sum() * 100 / len(data_pairs)) #.sort_values()\ns = sns.barplot(x = data_pairs_missing.values, y = data_pairs_missing.index)\ns.set_xlim(0, 100)\n# s.bar_label(s.containers[0])\ns.set(xlabel = \"% of missing values\", ylabel = \"column\")\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:38:09.080439Z","iopub.execute_input":"2022-06-22T15:38:09.080786Z","iopub.status.idle":"2022-06-22T15:38:10.727877Z","shell.execute_reply.started":"2022-06-22T15:38:09.080754Z","shell.execute_reply":"2022-06-22T15:38:10.726307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Columns with missing values in the test set with respective proportion of missing values\n(data_test.isna().sum()[data_test.isna().sum() != 0]/len(data_test)).sort_values(ascending = False)","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:38:10.729776Z","iopub.execute_input":"2022-06-22T15:38:10.730273Z","iopub.status.idle":"2022-06-22T15:38:10.744698Z","shell.execute_reply.started":"2022-06-22T15:38:10.73023Z","shell.execute_reply":"2022-06-22T15:38:10.743655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Missing values in the test set\nplt.figure(figsize = (9, 4))\ndata_test_missing = (data_test.isna().sum() * 100 / len(data_test)) #.sort_values()\ns = sns.barplot(x = data_test_missing.values, y = data_test_missing.index)\ns.set_xlim(0, 100)\n# s.bar_label(s.containers[0])\ns.set(xlabel = \"% of missing values\", ylabel = \"column\")\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:38:10.745994Z","iopub.execute_input":"2022-06-22T15:38:10.747496Z","iopub.status.idle":"2022-06-22T15:38:11.062372Z","shell.execute_reply.started":"2022-06-22T15:38:10.747401Z","shell.execute_reply":"2022-06-22T15:38:11.060866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Statistical description of numerical variables in the training set\ndata_train.describe()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:38:11.064779Z","iopub.execute_input":"2022-06-22T15:38:11.06544Z","iopub.status.idle":"2022-06-22T15:38:11.23981Z","shell.execute_reply.started":"2022-06-22T15:38:11.065386Z","shell.execute_reply":"2022-06-22T15:38:11.238434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Statistical description of categorical variables in the training set\ndata_train.describe(include = ['O'])","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:38:11.248695Z","iopub.execute_input":"2022-06-22T15:38:11.249246Z","iopub.status.idle":"2022-06-22T15:38:19.291316Z","shell.execute_reply.started":"2022-06-22T15:38:11.249197Z","shell.execute_reply":"2022-06-22T15:38:19.289928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Statistical description of numerical variables in the pregenerated set of pairs\ndata_pairs.describe()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:38:19.29309Z","iopub.execute_input":"2022-06-22T15:38:19.293677Z","iopub.status.idle":"2022-06-22T15:38:19.443708Z","shell.execute_reply.started":"2022-06-22T15:38:19.293625Z","shell.execute_reply":"2022-06-22T15:38:19.442575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Statistical description of categorical variables in the pregenerated set of pairs\ndata_pairs.describe(include = ['O'])","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:38:19.445123Z","iopub.execute_input":"2022-06-22T15:38:19.445493Z","iopub.status.idle":"2022-06-22T15:38:25.514301Z","shell.execute_reply.started":"2022-06-22T15:38:19.445463Z","shell.execute_reply":"2022-06-22T15:38:25.513036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Statistical description of numerical variables in the test set\ndata_test.describe()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:38:25.516058Z","iopub.execute_input":"2022-06-22T15:38:25.516939Z","iopub.status.idle":"2022-06-22T15:38:25.54541Z","shell.execute_reply.started":"2022-06-22T15:38:25.516896Z","shell.execute_reply":"2022-06-22T15:38:25.544517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Statistical description of categorical variables in the test set\ndata_test.describe(include = ['O'])","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:38:25.546744Z","iopub.execute_input":"2022-06-22T15:38:25.547462Z","iopub.status.idle":"2022-06-22T15:38:25.585264Z","shell.execute_reply.started":"2022-06-22T15:38:25.547425Z","shell.execute_reply":"2022-06-22T15:38:25.584454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Training set synopsis:**\n\n- Number of observations: $1138812$\n- Number of columns: $13$\n- Number of integer columns: $0$\n- Number of float columns: $2$\n- Number of object columns: $11$\n- Number of duplicate observations: $0$\n- Constant columns: None\n- Number of columns with missing values: $9$\n- Memory Usage: $112.95$ MB\n\n**Pregenerated set of pairs synopsis:**\n\n- Number of observations: $578907$\n- Number of columns: $25$\n- Number of integer columns: $2$\n- Number of float columns: $30$\n- Number of object columns: $1$\n- Number of duplicate observations: $0$\n- Constant columns: None\n- Number of columns with missing values: $16$\n- Memory Usage: $106.55$ MB\n\n**Test set synopsis:**\n\n- Number of observations: $5$\n- Number of columns: $12$\n- Number of integer columns: $0$\n- Number of float columns: $4$\n- Number of object columns: $8$\n- Number of duplicate observations: $0$\n- Constant columns: `url`, `phone`\n- Number of columns with missing values: $6$\n- Memory Usage: $0.00058$ MB","metadata":{}},{"cell_type":"markdown","source":"# 3. Univariate Analysis - Training Set\n\n- [Point of Interest](#Point-of-Interest)\n- [Latitude and Longitude](#Latitude-and-Longitude)\n- [Country](#Country)\n- [State](#State)\n- [City](#City)\n- [Categories](#Categories)\n- [Name](#Name)\n- [Address](#Address)\n- [Phone](#Phone)\n- [URL](#URL)","metadata":{}},{"cell_type":"markdown","source":"## Point of Interest","metadata":{}},{"cell_type":"code","source":"# Horizontal countplot of state of training observations located in the United States of America\ncutoff = min(len(data_train['point_of_interest'].value_counts()), 50)\norder_descending = data_train.groupby('point_of_interest').size().sort_values().index[::-1][: cutoff].tolist()\nplt.figure(figsize = (15, cutoff / 5))\nsns.countplot(data = data_train, y = 'point_of_interest', order = order_descending)\nplt.title(f\"Top {cutoff} POIs in the training set with highest frequencies\", fontsize = 14)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:38:25.58676Z","iopub.execute_input":"2022-06-22T15:38:25.587552Z","iopub.status.idle":"2022-06-22T15:38:36.719677Z","shell.execute_reply.started":"2022-06-22T15:38:25.587507Z","shell.execute_reply":"2022-06-22T15:38:36.71847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We observe a big number of POI matching, with `P_fb339198a31db3` appearing as many as $332$ times.","metadata":{}},{"cell_type":"markdown","source":"## Latitude and Longitude","metadata":{}},{"cell_type":"code","source":"# Histograms of latitude and longitude of observations in the training set\nfig, ax = plt.subplots(1, 2, figsize = (15, 6), sharey = True)\nsns.histplot(data = data_train, x = 'latitude', bins = 30, ax = ax[0])\nsns.histplot(data = data_train, x = 'longitude', bins = 30, ax = ax[1])\nax[1].set_ylabel(\" \")\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:38:36.721154Z","iopub.execute_input":"2022-06-22T15:38:36.722257Z","iopub.status.idle":"2022-06-22T15:38:37.690803Z","shell.execute_reply.started":"2022-06-22T15:38:36.722214Z","shell.execute_reply":"2022-06-22T15:38:37.689568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We observe that a big chunk of observations fall inside the latitude interval $30$ to $60$, which covers much of `United States of America` as well as many countries from `Europe`. There are not many observations below $-50$ and above $70$ as these point to the two polar regions and their surrounding areas, which expectedly do not contain many POIs.\n\nAs for the longitude, we can see three separate groups. The first group is from $-125$ to $-25$, which covers `North America` and `South America`. The second group is from $-25$ to $75$, which covers `Europe` and `Africa`. The third and final group is from $75$ to $175$, which covers `Asia` and `Australia`. There four region of troughs in the histogram of longitudes. The first and the forth troughs (in the extreme left and the extreme right, which are joined because Earth is round) are due to the Pacific ocean, the second trough is due to the Atlantic ocean, where as the third trough is due to the Indian ocean, as well as lack of observations from some parts of `Russia` and the west part of `China`. A scatterplot of latitude and longitude, which is given in the next section, provides a clearer picture of the distribution of location of the training observations.\n\nNext, we present horizontal countplots of the object-type columns. We begin by presenting the top countries with most training observations. For labelling purpose, we use a .json file which contains the [ISO 3166-1 alpha-2](https://en.wikipedia.org/wiki/ISO_3166-1_alpha-2) codes for $246$ countries or territories.","metadata":{}},{"cell_type":"markdown","source":"## Country","metadata":{}},{"cell_type":"code","source":"# ISO 3166-1 alpha-2 country codes\nurl_iso_alpha_2 = \"https://raw.githubusercontent.com/sugatagh/Foursquare-Location-Matching/main/JSON/ISO_3166-1_alpha-2.json\"\ndict_iso_alpha_2 = pd.read_json(url_iso_alpha_2, typ = 'series')","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:38:37.692693Z","iopub.execute_input":"2022-06-22T15:38:37.693221Z","iopub.status.idle":"2022-06-22T15:38:37.931359Z","shell.execute_reply.started":"2022-06-22T15:38:37.693144Z","shell.execute_reply":"2022-06-22T15:38:37.930236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Horizontal countplot of country of observations in the training set\ndata_train_country = pd.DataFrame()\ndata_train_country['country'] = data_train['country'].map(dict_iso_alpha_2)\ncutoff = min(len(data_train_country['country'].value_counts()), 50)\norder_descending = data_train_country.groupby('country').size().sort_values().index[::-1][: cutoff].tolist()\nplt.figure(figsize = (15, cutoff / 5))\nsns.countplot(data = data_train_country, y = 'country', order = order_descending)\nplt.title(\"Top countries with most observations in the training set\", fontsize = 14)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:38:37.932792Z","iopub.execute_input":"2022-06-22T15:38:37.933333Z","iopub.status.idle":"2022-06-22T15:38:40.414232Z","shell.execute_reply.started":"2022-06-22T15:38:37.933282Z","shell.execute_reply":"2022-06-22T15:38:40.412923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We observe that `United States` has the highest number of training observations by a large margin, followed by `Turkey` and `Indonesia`.","metadata":{}},{"cell_type":"markdown","source":"## State","metadata":{}},{"cell_type":"code","source":"# Horizontal countplot of state of training observations\ncutoff = min(len(data_train['state'].value_counts()), 75)\norder_descending = data_train.groupby('state').size().sort_values().index[::-1][: cutoff].tolist()\nfig, ax = plt.subplots(1, 1, figsize = (15, cutoff / 5), sharex = True)\nsns.countplot(data = data_train, y = 'state', order = order_descending, ax = ax)\nax.set_title(\"Top states in the world with most training observations\", fontsize = 14)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:38:40.415706Z","iopub.execute_input":"2022-06-22T15:38:40.417571Z","iopub.status.idle":"2022-06-22T15:38:42.946317Z","shell.execute_reply.started":"2022-06-22T15:38:40.417503Z","shell.execute_reply":"2022-06-22T15:38:42.945289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The state of California (`CA`) comes out on top, followed by New York (`NY`) and Florida (`FL`). Often same state is reported in different names, caused by the variation in full name, short name, [postal abbreviations](https://en.wikipedia.org/wiki/List_of_U.S._state_and_territory_abbreviations), as well as uppercase and lowercase. For example, `CA`, `Calif`, `California`, `Ca` or `NY`, `New York`, `Ny`, `ny` or `Texas`, `Tx`, `tx`. It may be useful to match the state names with the respective postal abbreviations using a dictionary or otherwise. Also, to get rid of the variation due to capitalization of letters in general, we may convert all the reported object values (not just for the `state` column) to lowercase.","metadata":{}},{"cell_type":"markdown","source":"## City","metadata":{}},{"cell_type":"code","source":"# Horizontal countplot of city of training observations\ncutoff = min(len(data_train['city'].value_counts()), 50)\norder_descending = data_train.groupby('city').size().sort_values().index[::-1][: cutoff].tolist()\nfig, ax = plt.subplots(1, 1, figsize = (15, cutoff / 5), sharex = True)\nsns.countplot(data = data_train, y = 'city', order = order_descending, ax = ax)\nax.set_title(\"Top cities in the world with most observations\", fontsize = 14)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:38:42.947691Z","iopub.execute_input":"2022-06-22T15:38:42.948049Z","iopub.status.idle":"2022-06-22T15:38:45.901232Z","shell.execute_reply.started":"2022-06-22T15:38:42.948017Z","shell.execute_reply":"2022-06-22T15:38:45.89994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`Singapore` has most training observations, followed closely by `Mockba` and `Bandung`.","metadata":{}},{"cell_type":"markdown","source":"## ZIP Code","metadata":{}},{"cell_type":"code","source":"# Horizontal countplot of ZIP code of training observations\ncutoff = min(len(data_train['zip'].value_counts()), 50)\norder_descending = data_train.groupby('zip').size().sort_values().index[::-1][: cutoff].tolist()\nfig, ax = plt.subplots(1, 1, figsize = (15, cutoff / 5), sharex = True)\nsns.countplot(data = data_train, y = 'zip', order = order_descending, ax = ax)\nax.set_title(\"Top ZIP codes in the world with most training observations\", fontsize = 14)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:38:45.903116Z","iopub.execute_input":"2022-06-22T15:38:45.90354Z","iopub.status.idle":"2022-06-22T15:38:48.9429Z","shell.execute_reply.started":"2022-06-22T15:38:45.903505Z","shell.execute_reply":"2022-06-22T15:38:48.942021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We observe that the ZIP code with highest number of training observations is `9000`, followed by `10330` and `10110`. Note that postal/zip codes may not be unique globally. So the plot in the right side can be misleading.","metadata":{}},{"cell_type":"markdown","source":"## Categories","metadata":{}},{"cell_type":"code","source":"# Horizontal countplot of categories of training observations in the world\ncutoff = min(len(data_train['categories'].value_counts()), 50)\norder_descending = data_train.groupby('categories').size().sort_values().index[::-1][: cutoff].tolist()\nplt.figure(figsize = (15, cutoff / 5))\nsns.countplot(data = data_train, y = 'categories', order = order_descending)\nplt.title(\"Top categories in the world with most training observations\", fontsize = 14)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:38:48.944416Z","iopub.execute_input":"2022-06-22T15:38:48.945006Z","iopub.status.idle":"2022-06-22T15:38:52.131487Z","shell.execute_reply.started":"2022-06-22T15:38:48.94496Z","shell.execute_reply":"2022-06-22T15:38:52.130267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`Residential Buildings (Apartments / Condos)` has the highest number of training observations, followed by `Banks` and `College classrooms`.","metadata":{}},{"cell_type":"markdown","source":"## Name","metadata":{}},{"cell_type":"code","source":"# Horizontal countplots of name of training observations\ncutoff = min(len(data_train['name'].value_counts()), 50)\norder_descending = data_train.groupby('name').size().sort_values().index[::-1][: cutoff].tolist()\nplt.figure(figsize = (15, cutoff / 5))\nsns.countplot(data = data_train, y = 'name', order = order_descending)\nplt.title(\"Top names with most training observations\", fontsize = 14)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:38:52.133713Z","iopub.execute_input":"2022-06-22T15:38:52.134207Z","iopub.status.idle":"2022-06-22T15:39:05.048298Z","shell.execute_reply.started":"2022-06-22T15:38:52.134151Z","shell.execute_reply":"2022-06-22T15:39:05.047146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`Starbucks` tops the list with most highest of training observations, followed by `McDonald's` and `Redbox`.","metadata":{}},{"cell_type":"markdown","source":"## Address","metadata":{}},{"cell_type":"code","source":"# Horizontal countplots of address of training observations\ncutoff = min(len(data_train['address'].value_counts()), 50)\norder_descending = data_train.groupby('address').size().sort_values().index[::-1][: cutoff].tolist()\nplt.figure(figsize = (15, cutoff / 5))\nsns.countplot(data = data_train, y = 'address', order = order_descending)\nplt.title(\"Top addresses with most training observations\", fontsize = 14)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:39:05.049793Z","iopub.execute_input":"2022-06-22T15:39:05.0502Z","iopub.status.idle":"2022-06-22T15:39:13.86743Z","shell.execute_reply.started":"2022-06-22T15:39:05.050143Z","shell.execute_reply":"2022-06-22T15:39:13.866254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As expected, generic strings that are typically used as part of address feature as the top most addresses of the training observations, with `Terminal 1` having the highest frequency.","metadata":{}},{"cell_type":"markdown","source":"## Phone","metadata":{}},{"cell_type":"code","source":"# Horizontal countplots of phone numbers of training observations\ncutoff = min(len(data_train['phone'].value_counts()), 50)\norder_descending = data_train.groupby('phone').size().sort_values().index[::-1][: cutoff].tolist()\nplt.figure(figsize = (15, cutoff / 5))\nsns.countplot(data = data_train, y = 'phone', order = order_descending)\nplt.title(\"Top phone numbers with most training observations\", fontsize = 14)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:39:13.868725Z","iopub.execute_input":"2022-06-22T15:39:13.869074Z","iopub.status.idle":"2022-06-22T15:39:18.617929Z","shell.execute_reply.started":"2022-06-22T15:39:13.869043Z","shell.execute_reply":"2022-06-22T15:39:18.616897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The number `8667332693` leads by far, followed by itself with the extension `+1`. It shows how same phone number can have variation in the records due to addition or omission of extensions. Removing the extensions at the *data preprocessing* stage may be helpful to avoid same phone numbers from getting identified as different.","metadata":{}},{"cell_type":"markdown","source":"## URL","metadata":{}},{"cell_type":"code","source":"# Horizontal countplots of URL of training observations\ncutoff = min(len(data_train['url'].value_counts()), 50)\norder_descending = data_train.groupby('url').size().sort_values().index[::-1][: cutoff].tolist()\nplt.figure(figsize = (15, cutoff / 5))\nsns.countplot(data = data_train, y = 'url', order = order_descending)\nplt.title(\"Top URLs with most training observations\", fontsize = 14)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:39:18.619231Z","iopub.execute_input":"2022-06-22T15:39:18.619582Z","iopub.status.idle":"2022-06-22T15:39:22.40647Z","shell.execute_reply.started":"2022-06-22T15:39:18.619551Z","shell.execute_reply":"2022-06-22T15:39:22.405252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The website `https://www.sej.co.jp` is associated with the highest number of training observations, followed by `http://www.7eleven.co.th` and `http://www.payless.com/`. Observe the possibility of undesired variation due to the strings `https://`, `http://`, `www` or an appended frontslash at the end. It may be helpful to get rid of these strings and retain only the *core* part of the URLs at the *data preprocessing* stage.","metadata":{}},{"cell_type":"markdown","source":"# 4. Multivariate Analysis - Training Set\n\n- [Latitude and Longitude](#Latitude-and-Longitude)\n- [States of USA](#States-of-USA)\n- [Cities of California, USA](#Cities-of-California,-USA)\n- [ZIP codes of San Francisco, California, USA](#ZIP-codes-of-San-Francisco,-California,-USA)\n- [Categories of POIs in San Francisco - 94103, California, USA](#Categories-of-POIs-in-San-Francisco---94103,-California,-USA)\n- [Names of coffee shops in San Francisco - 94103, California, USA](#Names-of-coffee-shops-in-San-Francisco---94103,-California,-USA)\n- [Address, Phone and URL of Starbucks coffee shops in San Francisco - 94103, California, USA](#Address,-Phone-and-URL-of-Starbucks-coffee-shops-in-San-Francisco---94103,-California,-USA)","metadata":{}},{"cell_type":"markdown","source":"## Latitude and Longitude","metadata":{}},{"cell_type":"code","source":"# Scatterplot of latitude and longitude of observations in the training set\nplt.figure(figsize = (15, 9))\nsns.scatterplot(data = data_train, x = 'longitude', y = 'latitude')\nplt.title(\"Scatterplot of latitude and longitude of training observations\", fontsize = 14)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:39:22.40776Z","iopub.execute_input":"2022-06-22T15:39:22.408113Z","iopub.status.idle":"2022-06-22T15:39:24.938938Z","shell.execute_reply.started":"2022-06-22T15:39:22.408081Z","shell.execute_reply":"2022-06-22T15:39:24.938086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This scatterplot gives a clearer picture of the locations than the histograms for the latitude and the longitudes of POIs in the training data, as presented in the previous section, because they only captured marginal information on the location of the observations. The present plot roughly resembles the world map, although there are relatively less number of POIs from `Canada`, `Australia`, some parts of `Russia` and the countries from `Africa` (apart from the southeast part). Observations are dense and uniform over `United States of America`, `Mexico`, `New Zealand`, almost entirety of `India`, east coast of `South America`, the countries in `Europe` and `Southeast Asia`, as well as the east coast of `Australia`. Next, we present horizontal countplots of the object-type columns in a hierarchical manner:\n\n$$ \\text{World} \\mapsto \\text{USA} \\mapsto \\text{California} \\mapsto \\text{San Francisco} \\mapsto \\text{ZIP}\\,\\,94103 \\mapsto \\text{Coffee Shops} \\mapsto \\text{Starbucks}$$","metadata":{}},{"cell_type":"markdown","source":"## States of USA","metadata":{}},{"cell_type":"code","source":"# Horizontal countplot of state of training observations located in the United States of America\ndata_temp = data_train[data_train['country'] == 'US']\ncutoff = min(len(data_temp['state'].value_counts()), 50)\norder_descending_temp = data_temp.groupby('state').size().sort_values().index[::-1][: cutoff].tolist()\nfig, ax = plt.subplots(1, 1, figsize = (15, cutoff / 5), sharex = True)\nsns.countplot(data = data_temp, y = 'state', order = order_descending_temp, ax = ax)\nax.set_title(\"Top states in USA with most training observations\", fontsize = 14)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:39:24.940183Z","iopub.execute_input":"2022-06-22T15:39:24.940785Z","iopub.status.idle":"2022-06-22T15:39:26.388896Z","shell.execute_reply.started":"2022-06-22T15:39:24.940671Z","shell.execute_reply":"2022-06-22T15:39:26.387798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the USA, the state of California (`CA`) comes out on top, followed by New York (`NY`) and Florida (`FL`). In fact, seven of the top ten states in the world are from USA itself. Next we explore the top cities of the state of California, with most training observations.","metadata":{}},{"cell_type":"markdown","source":"## Cities of California, USA","metadata":{}},{"cell_type":"code","source":"# Horizontal countplot of city of training observations located in the state of California, USA\ndata_temp = data_train[(data_train['country'] == 'US') \n                       & (data_train['state'] == 'CA')]\ncutoff = min(len(data_temp['city'].value_counts()), 50)\norder_descending_temp = data_temp.groupby('city').size().sort_values().index[::-1][: cutoff].tolist()\nfig, ax = plt.subplots(1, 1, figsize = (15, cutoff / 5), sharex = True)\nsns.countplot(data = data_temp, y = 'city', order = order_descending_temp, ax = ax)\nax.set_title(\"Top cities in California, USA with most observations\", fontsize = 14)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:39:26.390157Z","iopub.execute_input":"2022-06-22T15:39:26.390584Z","iopub.status.idle":"2022-06-22T15:39:27.830921Z","shell.execute_reply.started":"2022-06-22T15:39:26.390542Z","shell.execute_reply":"2022-06-22T15:39:27.829946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the state of California, the city of `San Francisco` has most of the training observations, followed closely by `Los Angeles` and `San Diego`. Next we analyze the top ZIP codes in the city of `San Francisco`, covering most training observations.","metadata":{}},{"cell_type":"markdown","source":"## ZIP codes of San Francisco, California, USA","metadata":{}},{"cell_type":"code","source":"# Horizontal countplot of ZIP code of training observations in the city of San Francisco, state of California, USA\ndata_temp = data_train[(data_train['country'] == 'US') \n                       & (data_train['state'] == 'CA') \n                       & (data_train['city'] == 'San Francisco')]\ncutoff = min(len(data_temp['zip'].value_counts()), 50)\norder_descending_temp = data_temp.groupby('zip').size().sort_values().index[::-1][: cutoff].tolist()\nfig, ax = plt.subplots(1, 1, figsize = (15, cutoff / 5), sharex = True)\nsns.countplot(data = data_temp, y = 'zip', order = order_descending_temp, ax = ax)\nax.set_title(\"Top ZIP codes in San Francisco, California, USA with most training observations\", fontsize = 14)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:39:27.832289Z","iopub.execute_input":"2022-06-22T15:39:27.832656Z","iopub.status.idle":"2022-06-22T15:39:29.262138Z","shell.execute_reply.started":"2022-06-22T15:39:27.832622Z","shell.execute_reply":"2022-06-22T15:39:29.261126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We observe that in the city of `San Francisco`, the ZIP code with highest number of training observations is `94103`, followed by `94105`, `94107` and `94102`. We shall move onto the *categories* of the POIs and focus only on the observations from `San Francisco - 94103`.","metadata":{}},{"cell_type":"markdown","source":"## Categories of POIs in San Francisco - 94103, California, USA","metadata":{}},{"cell_type":"code","source":"# Horizontal countplot of categories of training observations in the city of San Francisco - 94103, state of California, USA\ndata_temp = data_train[(data_train['country'] == 'US') \n                       & (data_train['state'] == 'CA') \n                       & (data_train['city'] == 'San Francisco') \n                       & (data_train['zip'] == '94103')]\ncutoff = min(len(data_temp['categories'].value_counts()), 50)\norder_descending = data_temp.groupby('categories').size().sort_values().index[::-1][: cutoff].tolist()\nplt.figure(figsize = (15, cutoff / 5))\nsns.countplot(data = data_temp, y = 'categories', order = order_descending)\nplt.title(\"Top categories in San Francisco - 94103, California, USA with most training observations\", fontsize = 14)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:39:29.26344Z","iopub.execute_input":"2022-06-22T15:39:29.263811Z","iopub.status.idle":"2022-06-22T15:39:31.054567Z","shell.execute_reply.started":"2022-06-22T15:39:29.263777Z","shell.execute_reply":"2022-06-22T15:39:31.053572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Four categories have highest number of training observations in the specified ZIP code: `Food Trucks`, `Meeting Rooms`, `Offices` and `Coffee Shops`. Next we shall see the `name` of the `Coffee Shops` in `San Francisco - 94103, California, USA` region considered above.","metadata":{}},{"cell_type":"markdown","source":"## Names of coffee shops in San Francisco - 94103, California, USA","metadata":{}},{"cell_type":"code","source":"# Horizontal countplot of names of training observations which are coffee shops in the city of San Francisco - 94103, state of California, USA\ndata_temp = data_train[(data_train['country'] == 'US') \n                       & (data_train['state'] == 'CA') \n                       & (data_train['city'] == 'San Francisco') \n                       & (data_train['zip'] == '94103') \n                       & (data_train['categories'] == 'Coffee Shops')]\ncutoff = min(len(data_temp['name'].value_counts()), 75)\norder_descending = data_temp.groupby('name').size().sort_values().index[::-1][: cutoff].tolist()\nplt.figure(figsize = (15, cutoff / 2))\nsns.countplot(data = data_temp, y = 'name', order = order_descending)\nplt.title(\"Names of coffee shops in San Francisco - 94103, California, USA with most training observations\", fontsize = 14)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:39:31.055977Z","iopub.execute_input":"2022-06-22T15:39:31.056369Z","iopub.status.idle":"2022-06-22T15:39:32.123309Z","shell.execute_reply.started":"2022-06-22T15:39:31.056335Z","shell.execute_reply":"2022-06-22T15:39:32.122158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We observe that there are five`Starbucks` coffee shops in the region. We shall check `address`, `phone` and `url` to see if any pair of these shops are identical.","metadata":{}},{"cell_type":"markdown","source":"## Address, Phone and URL of Starbucks coffee shops in San Francisco - 94103, California, USA","metadata":{}},{"cell_type":"code","source":"# Horizontal countplots of address, phone and url of training observations which are coffee shops in the city of San Francisco - 94103, state of California, USA\ndata_temp = data_train[(data_train['country'] == 'US') \n                       & (data_train['state'] == 'CA') \n                       & (data_train['city'] == 'San Francisco') \n                       & (data_train['zip'] == '94103') \n                       & (data_train['categories'] == 'Coffee Shops')\n                       & (data_train['name'] == 'Starbucks')]\nfig, ax = plt.subplots(3, 1, figsize = (15, 9))\nsns.countplot(data = data_temp, y = 'address', ax = ax[0])\nax[0].set_title(\"Addresses of Starbucks coffee shops in San Francisco - 94103, California, USA with most training observations\", fontsize = 14)\nsns.countplot(data = data_temp, y = 'phone', ax = ax[1])\nax[1].set_title(\"Phone numbers of Starbucks coffee shops in San Francisco - 94103, California, USA with most training observations\", fontsize = 14)\nsns.countplot(data = data_temp, y = 'url', ax = ax[2])\nax[2].set_title(\"URLs of Starbucks coffee shops in San Francisco - 94103, California, USA with most training observations\", fontsize = 14)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:39:32.125153Z","iopub.execute_input":"2022-06-22T15:39:32.125689Z","iopub.status.idle":"2022-06-22T15:39:33.764342Z","shell.execute_reply.started":"2022-06-22T15:39:32.125639Z","shell.execute_reply":"2022-06-22T15:39:33.762983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Thus all the Starbucks coffee shops in the region have distinct addresses, indicating that there are no matches among them. Furthermore the available *phone* numbers (which is missing for one out of the five shops) are also distinct. Note that the available urls, though appear distinct, have many matches and are only distinct due to variations in *https*, *http*, *www* etc. One may get rid of strings like *https://*, *http://*, *www.*, *https://www.*, *http://www.* etc to overcome this issue.","metadata":{}},{"cell_type":"markdown","source":"# 5. Univariate Analysis - Pairs Set\n\n- [Match](#Match)\n- [Feature Based on Latitude and Longitude](#Feature-Based-on-Latitude-and-Longitude)\n- [Features Based on Levenshtein Distance](#Features-Based-on-Levenshtein-Distance)\n- [Features Based on Matching](#Features-Based-on-Matching)\n- [Distance between Locations](#Distance-between-Locations)\n- [Distance between Names](#Distance-between-Names)\n- [Distance between Addresses](#Distance-between-Addresses)\n- [Distance between URLs](#Distance-between-URLs)\n- [Distance between Phone Numbers](#Distance-between-Phone-Numbers)\n- [Matching of Countries](#Matching-of-Countries)\n- [Matching of States](#Matching-of-States)\n- [Matching of Cities](#Matching-of-Cities)\n- [Matching of ZIP Codes](#Matching-of-ZIP-Codes)\n- [Matching of Categories](#Matching-of-Categories)","metadata":{}},{"cell_type":"code","source":"# Barplot and donutplot of a dataframe column\ndef count(df, col):\n    fig = make_subplots(rows = 1, cols = 2, specs = [[{'type': 'xy'}, {'type': 'domain'}]])\n    x_val = df[col].value_counts(sort = False).index.tolist()\n    y_val = df[col].value_counts(sort = False).tolist()\n    fig.add_trace(go.Bar(x = x_val, y = y_val, text = y_val, textposition = 'auto'), row = 1, col = 1)\n    fig.add_trace(go.Pie(values = y_val, labels = x_val, hole = 0.5, textinfo = 'label+percent', title = f\"{col}\"), row = 1, col = 2)\n    fig.update_layout(height = 500, width = 800, showlegend = False, xaxis = dict(tickmode = 'linear', tick0 = 0, dtick = 1), title = dict(text = f\"Frequency distribution of {col}\", x = 0.5, y = 0.95)) \n    fig.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:39:33.765937Z","iopub.execute_input":"2022-06-22T15:39:33.766476Z","iopub.status.idle":"2022-06-22T15:39:33.778778Z","shell.execute_reply.started":"2022-06-22T15:39:33.766437Z","shell.execute_reply":"2022-06-22T15:39:33.777645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Match","metadata":{}},{"cell_type":"code","source":"# Donutplot of the 'match' column\ncount(data_pairs, 'match')","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:39:33.780508Z","iopub.execute_input":"2022-06-22T15:39:33.781251Z","iopub.status.idle":"2022-06-22T15:39:33.885274Z","shell.execute_reply.started":"2022-06-22T15:39:33.781211Z","shell.execute_reply":"2022-06-22T15:39:33.88442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We observe that $68.9\\%$ of the pairs in the pregenerated set of pairs match, while $31.1\\%$ of the pairs do not match. Next, we consider a typical observation from the pairs data.","metadata":{}},{"cell_type":"code","source":"# Typical observation from the pregenerated set of pairs\ndata_pairs.iloc[0]","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:39:33.886606Z","iopub.execute_input":"2022-06-22T15:39:33.887726Z","iopub.status.idle":"2022-06-22T15:39:33.896423Z","shell.execute_reply.started":"2022-06-22T15:39:33.887682Z","shell.execute_reply":"2022-06-22T15:39:33.895419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It contains a pair of observations. The *id* of the two observations are expectedly different. The *name* is slightly different, as are the *latitude* and *longitude*. *Country* and *category* are identical. Some of the attributes are missing in one of the observations, while some are missing in both. The target variable here is `match`, which is a Boolean variable taking the value `True` if the two observations refer to the same POI and `False` otherwise. We observe that the number of features can be greatly reduced if we focus on the information that are relevant in predicting `match`, and discard the rest. We initiate a dataframe to extract and store these relevant information out of the attributes in `data_pairs`.","metadata":{}},{"cell_type":"code","source":"# Dataframe initialization for new features based on the pregenerated set of pairs\ndata_pairs_feat = pd.DataFrame()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:39:33.89805Z","iopub.execute_input":"2022-06-22T15:39:33.898728Z","iopub.status.idle":"2022-06-22T15:39:33.913673Z","shell.execute_reply.started":"2022-06-22T15:39:33.898692Z","shell.execute_reply":"2022-06-22T15:39:33.912551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Based on Latitude and Longitude","metadata":{}},{"cell_type":"markdown","source":"The information that is relevant in predicting `match`, contained in `latitude_1`, `longitude_1`, `latitude_2` and `longitude_2`, can be encapsulted into a single variable, which is the [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance) `dist_loc` between the two locations (`latitude_1`, `longitude_1`) and (`latitude_2`, `longitude_2`), given by\n\n$$ \\text{dist_loc} = \\sqrt{\\left(\\text{latitude_1} - \\text{latitude_2}\\right)^2 + \\left(\\text{longitude_1} - \\text{longitude_2}\\right)^2}. $$","metadata":{}},{"cell_type":"code","source":"# Distance between locations\ndata_pairs_feat['dist_loc'] = np.sqrt(((data_pairs['latitude_1'] - data_pairs['latitude_2'])**2) + ((data_pairs['longitude_1'] - data_pairs['longitude_2'])**2))","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:39:33.915266Z","iopub.execute_input":"2022-06-22T15:39:33.915992Z","iopub.status.idle":"2022-06-22T15:39:34.012898Z","shell.execute_reply.started":"2022-06-22T15:39:33.915904Z","shell.execute_reply":"2022-06-22T15:39:34.01177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Features Based on Levenshtein Distance","metadata":{}},{"cell_type":"markdown","source":"In the typical observation printed at the beginning of the section, we see the name of the same POI is reported as `Caf√© Stad Oudenaarde` in one record, and `Caf√© Oudenaarde` in another. We use a quantification of distance between two strings to capture the extent of this difference between the reported names of a pair of observations. For example, we would expect the difference between `Caf√© Stad Oudenaarde` and `Caf√© Oudenaarde` to be lesser than the same between `Turkcell` and `Island Spa Theater`. Here we shall employ the [Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance), which measures the difference between two sequences. It is named after the Soviet mathematician [Vladimir Levenshtein](https://en.wikipedia.org/wiki/Vladimir_Levenshtein), who considered this distance in 1965. Informally, the Levenshtein distance between two words is the minimum number of single-character edits (insertions, deletions or substitutions) required to change one word into the other. Mathematically, the Levenshtein distance between two strings $a$ and $b$ (of length $\\left\\vert a \\right\\vert$ and $\\left\\vert b \\right\\vert$ respectively) is defined recursively as\n\n$$ \\text{lev(a, b)} =\n\\left\\{\n    \\begin{array}{ll}\n        \\left\\vert a \\right\\vert, & \\mbox{if } \\left\\vert b \\right\\vert == 0, \\\\\n        \\left\\vert b \\right\\vert, & \\mbox{if } \\left\\vert a \\right\\vert == 0, \\\\\n        \\text{lev(tail(a), tail(b))}, & \\mbox{if } a[0] == b[0], \\\\\n        1 + \\min \\left\\{\n                     \\begin{array}{l}\n                         \\text{lev(tail(a), b)} \\\\\n                         \\text{lev(a, tail(b))} \\\\\n                         \\text{lev(tail(a), tail(b))}\n                     \\end{array}\n                 \\right., & \\mbox{otherwise},\n    \\end{array}\n\\right. $$\n\nwhere the *tail* of some string $x$ is a string of all but the first character of $x$, for example `tail('Levenshtein') == 'evenshtein'`, and $x[n]$ is the $n$th character of the string $x$, counting from $0$, for example `'Levenshtein'[0] == 'L'`. Note that the first element in the minimum, `lev(tail(a), b)`, corresponds to *deletion* (from $a$ to $b$), the second `lev(a, tail(b))` to *insertion* and the third `lev(tail(a), tail(b))` to *replacement*.","metadata":{}},{"cell_type":"code","source":"# Recursive function to compute Levenshtein distance\ndef lev(a, b):\n    @lru_cache(None)\n    def min_dist(s1, s2):\n        if len(a) == s1:\n            return len(b) - s2\n        elif len(b) == s2:\n            return len(a) - s1\n        elif a[s1] == b[s2]:\n            return min_dist(s1 + 1, s2 + 1)\n        else:\n            return 1 + min(\n                min_dist(s1, s2 + 1),\n                min_dist(s1 + 1, s2),\n                min_dist(s1 + 1, s2 + 1)\n            )\n    return min_dist(0, 0)","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:39:34.014567Z","iopub.execute_input":"2022-06-22T15:39:34.015066Z","iopub.status.idle":"2022-06-22T15:39:34.023427Z","shell.execute_reply.started":"2022-06-22T15:39:34.01502Z","shell.execute_reply":"2022-06-22T15:39:34.022375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Computation of Levenshtein distance for two examples\nlev1 = lev('Caf√© Stad Oudenaarde', 'Caf√© Oudenaarde')\nlev2 = lev('Turkcell', 'Island Spa Theater')\nprint(pd.Series({\"Levenshtein distance between 'Caf√© Stad Oudenaarde' and 'Caf√© Oudenaarde'\": lev1,\n                 \"Levenshtein distance between 'Turkcell' and 'Island Spa Theater'\": lev2}).to_string())","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:39:34.025093Z","iopub.execute_input":"2022-06-22T15:39:34.025764Z","iopub.status.idle":"2022-06-22T15:39:34.039954Z","shell.execute_reply.started":"2022-06-22T15:39:34.025717Z","shell.execute_reply":"2022-06-22T15:39:34.038379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We observe in the pregenerated set of pairs that often `name`, `address`, `url`, `phone` for the same POI varies in different records for variety of reasons. Some records may contain shortened versions of the names. Addresses may vary in the depth of detailing. URLs may vary by presence or omission of strings like *http*, *https*, *www*. Phone numbers may vary due to extensions, brackets, space and symbols like $+$ or hyphen. For these reasons, we create distance features based on these attributes using the Levenshtein distance. Note that we have to convert some of the input data to string format before feeding it to the function that computes the distance.","metadata":{}},{"cell_type":"code","source":"# New features based on Levenshtein distance\ndist_name_list = []\ndist_address_list = []\ndist_url_list = []\ndist_phone_list = []\n\nfor i in range(len(data_pairs)):\n    dist_name_list.append(lev(data_pairs['name_1'][i], data_pairs['name_2'][i]))\n    dist_address_list.append(lev(str(data_pairs['address_1'][i]), str(data_pairs['address_2'][i])))\n    dist_url_list.append(lev(str(data_pairs['url_1'][i]), str(data_pairs['url_2'][i])))\n    dist_phone_list.append(lev(str(data_pairs['phone_1'][i]), str(data_pairs['phone_2'][i])))\n\ndata_pairs_feat['dist_name'] = dist_name_list\ndata_pairs_feat['dist_address'] = dist_address_list\ndata_pairs_feat['dist_url'] = dist_url_list\ndata_pairs_feat['dist_phone'] = dist_phone_list","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:39:34.041463Z","iopub.execute_input":"2022-06-22T15:39:34.041866Z","iopub.status.idle":"2022-06-22T15:46:33.426533Z","shell.execute_reply.started":"2022-06-22T15:39:34.041821Z","shell.execute_reply":"2022-06-22T15:46:33.425336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Features Based on Matching","metadata":{}},{"cell_type":"markdown","source":"As there are no scope of confusion about the country of a POI, the two features `country_1` and `country_2` can be replaced by a binary variable `country_match` which is `True` when `country_1 == country_2` and `False` otherwise.\n\n$$ \\text{match_country} =\n\\left\\{\n    \\begin{array}{ll}\n        \\text{True,}  & \\mbox{if country_1 == country_2,}\\\\\n        \\text{False,} & \\mbox{otherwise.}\n    \\end{array}\n\\right. $$\n\nIf `match_country == False` then it is a certain indication towards `match == False`. Similarly, we can construct `match_city`, `match_state`, `match_zip`, `match_categories`, respectively based on the columns on `city`, `state`, `zip`, `phone`, `categories`. We keep the `match` column as it is.","metadata":{}},{"cell_type":"code","source":"# New features based on matching\ndef condition(df, col1, col2, i):\n    return (str(df[col1][i]) == 'nan' or str(df[col2][i]) == 'nan')\ndef value(df, col1, col2, i):\n    return (df[col1][i] == df[col2][i])\ndef match_list(df, col1, col2):\n    return [np.nan if condition(df, col1, col2, i) else value(df, col1, col2, i) for i in range(len(df))]\n\ndata_pairs_feat['match_country'] = match_list(data_pairs, 'country_1', 'country_2')\ndata_pairs_feat['match_city'] = match_list(data_pairs, 'city_1', 'city_2')\ndata_pairs_feat['match_state'] = match_list(data_pairs, 'state_1', 'state_2')\ndata_pairs_feat['match_zip'] = match_list(data_pairs, 'zip_1', 'zip_2')\ndata_pairs_feat['match_categories'] = match_list(data_pairs, 'categories_1', 'categories_2')\ndata_pairs_feat['match'] = data_pairs['match']","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:46:33.428253Z","iopub.execute_input":"2022-06-22T15:46:33.429196Z","iopub.status.idle":"2022-06-22T15:47:45.180871Z","shell.execute_reply.started":"2022-06-22T15:46:33.429143Z","shell.execute_reply":"2022-06-22T15:47:45.17991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dataframe of new features and match, based on the pregenerated set of pairs\nprint(pd.Series({\"Memory usage\": \"{:.2f} MB\".format(data_pairs_feat.memory_usage().sum()/(1024*1024)),\n                 \"Dataframe shape\": \"{}\".format(data_pairs_feat.shape)}).to_string())\nprint(\" \")\ndata_pairs_feat.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:47:45.182158Z","iopub.execute_input":"2022-06-22T15:47:45.182946Z","iopub.status.idle":"2022-06-22T15:47:45.205915Z","shell.execute_reply.started":"2022-06-22T15:47:45.182903Z","shell.execute_reply":"2022-06-22T15:47:45.204677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Distance between Locations","metadata":{}},{"cell_type":"markdown","source":"This variable is extremely skewed. To deal with it, we have applied the following transformation: $x \\mapsto log(x+\\epsilon),$ where $\\epsilon$ is a very small positive real number. Here we have taken $\\epsilon = 0.00000001$. The reason behind making this small shift to the data is that the log function maps $0$ to $-\\infty$. The shift keeps the transformed data finite, and keeping $\\epsilon$ small ensures that the data points which were originally $0,$ stands out from the rest in the transformed setup. Visualizations of the distribution of both the original feature and the transformed feature have been shown.","metadata":{}},{"cell_type":"code","source":"# Histogram of distance between locations\nfig, ax = plt.subplots(1, 2, figsize = (15, 6), sharex = False, sharey = False)\ndata_temp = data_pairs_feat.copy(deep = True)\nepsilon = 0.00000001\ndata_temp['dist_loc_transformed'] = data_temp['dist_loc'].apply(lambda x: np.log(x + epsilon))\nsns.histplot(data = data_temp, x = 'dist_loc', bins = 30, hue = 'match', ax = ax[0])\nsns.histplot(data = data_temp, x = 'dist_loc_transformed', bins = 30, hue = 'match', ax = ax[1])\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:47:45.207076Z","iopub.execute_input":"2022-06-22T15:47:45.207449Z","iopub.status.idle":"2022-06-22T15:47:47.730362Z","shell.execute_reply.started":"2022-06-22T15:47:45.207416Z","shell.execute_reply":"2022-06-22T15:47:47.729255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Distance between Names","metadata":{}},{"cell_type":"code","source":"# Histogram of distance between names\nplt.figure(figsize = (9, 6))\nsns.histplot(data = data_pairs_feat, x = 'dist_name', bins = 30, hue = 'match')\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:47:47.731548Z","iopub.execute_input":"2022-06-22T15:47:47.731872Z","iopub.status.idle":"2022-06-22T15:47:48.413839Z","shell.execute_reply.started":"2022-06-22T15:47:47.731843Z","shell.execute_reply":"2022-06-22T15:47:48.412073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Distance between Addresses","metadata":{}},{"cell_type":"code","source":"# Histogram of distance between addresses\nplt.figure(figsize = (9, 6))\nsns.histplot(data = data_pairs_feat, x = 'dist_address', bins = 30, hue = 'match')\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:47:48.41506Z","iopub.execute_input":"2022-06-22T15:47:48.41542Z","iopub.status.idle":"2022-06-22T15:47:49.085037Z","shell.execute_reply.started":"2022-06-22T15:47:48.41539Z","shell.execute_reply":"2022-06-22T15:47:49.084046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Apart from the global peak at $0,$ we observe a noticeable local peak between $10$ and $15$. The reasons behind this local peak may be more than one. It may happen that the addresses are short, and hence the Leveshtein distance between the two addresses in a pair is small. It may also happen that the two observations in the pair report the same address, but at different level of detailing, which surely contributes to substantial amount of pairs with Leveshtein distance between addresses more than $0,$ but less than $15$.","metadata":{}},{"cell_type":"markdown","source":"## Distance between URLs","metadata":{}},{"cell_type":"code","source":"# Histogram of distance between URLs\nplt.figure(figsize = (9, 6))\nsns.histplot(data = data_pairs_feat, x = 'dist_url', bins = 30, hue = 'match')\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:47:49.086388Z","iopub.execute_input":"2022-06-22T15:47:49.086745Z","iopub.status.idle":"2022-06-22T15:47:51.482512Z","shell.execute_reply.started":"2022-06-22T15:47:49.086712Z","shell.execute_reply":"2022-06-22T15:47:51.481263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Distance between Phone Numbers","metadata":{}},{"cell_type":"code","source":"# Histogram of distance between phone numbers\nplt.figure(figsize = (9, 6))\nsns.histplot(data = data_pairs_feat, x = 'dist_phone', bins = 30, hue = 'match')\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:47:51.483743Z","iopub.execute_input":"2022-06-22T15:47:51.484087Z","iopub.status.idle":"2022-06-22T15:47:52.170164Z","shell.execute_reply.started":"2022-06-22T15:47:51.48405Z","shell.execute_reply":"2022-06-22T15:47:52.169107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The local peak around $10,$ which is expected, as most phone numbers have $10$ digits. Thus two different phone numbers of $10$ digits, for which none of the digits are same, has Leveshtein distance $10$ (as it requires $10$ substitutions to go from one number to the another). Of course, it can be more because country extension codes can be involved.","metadata":{}},{"cell_type":"markdown","source":"## Matching of Countries","metadata":{}},{"cell_type":"code","source":"# Frequency distribution of a feature\n\"\"\"\nInput: Dataframe <df>, categorical column <col>, Boolean target variable <boolean_target>\nOutput (top left): Barplot of df[col]\nOutput (top right): Donutplot of df[col]\nOutput (bottom left): Donutplot of df[col] when boolean_target == True\nOutput (bottom left): Donutplot of df[col] when boolean_target == False\n\"\"\"\ndef donut(df, col, boolean_target):\n    fig = make_subplots(rows = 1, cols = 2, specs = [[{'type': 'domain'}, {'type': 'domain'}]])\n    x_val_true = df[df[boolean_target] == True][col].value_counts(sort = False).index.tolist()\n    y_val_true = df[df[boolean_target] == True][col].value_counts(sort = False).tolist()\n    fig.add_trace(go.Pie(values = y_val_true, labels = x_val_true, hole = 0.5, textinfo = 'label+percent', title = f\"{boolean_target} = True\"), row = 1, col = 1)\n    x_val_false = df[df[boolean_target] == False][col].value_counts(sort = False).index.tolist()\n    y_val_false = df[df[boolean_target] == False][col].value_counts(sort = False).tolist()\n    fig.add_trace(go.Pie(values = y_val_false, labels = x_val_false, hole = 0.5, textinfo = 'label+percent', title = f\"{boolean_target} = False\"), row = 1, col = 2)\n    fig.update_layout(height = 500, width = 800, showlegend = False, xaxis = dict(tickmode = 'linear', tick0 = 0, dtick = 1), title = dict(text = f\"Frequency distribution of {col} by {boolean_target}\", x = 0.5, y = 0.95)) \n    fig.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:47:52.171567Z","iopub.execute_input":"2022-06-22T15:47:52.17193Z","iopub.status.idle":"2022-06-22T15:47:52.185112Z","shell.execute_reply.started":"2022-06-22T15:47:52.171896Z","shell.execute_reply":"2022-06-22T15:47:52.183884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Barplot and donutplot of 'match_country'\ncount(data_pairs_feat, 'match_country')","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:47:52.186733Z","iopub.execute_input":"2022-06-22T15:47:52.187257Z","iopub.status.idle":"2022-06-22T15:47:52.36436Z","shell.execute_reply.started":"2022-06-22T15:47:52.187208Z","shell.execute_reply":"2022-06-22T15:47:52.363261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Donutplots of 'match_country' for different target classes\ndonut(data_pairs_feat, 'match_country', 'match')","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:47:52.365903Z","iopub.execute_input":"2022-06-22T15:47:52.366398Z","iopub.status.idle":"2022-06-22T15:47:52.719121Z","shell.execute_reply.started":"2022-06-22T15:47:52.366353Z","shell.execute_reply":"2022-06-22T15:47:52.717984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Curiously, the pregenerated set of pairs predominantly $(99.7\\%)$ contains pairs of observations from same country. One reason perhaps is that if two observations are reported to be from different countries, then they certainly refer to different POIs (unless something is horrendously wrong). So those pairs can be classified as `match == False` with reasonable confidence from common sense itself, without requiring any classification algorithm. Although there may be errors due to faulty records, as seen by $0.4\\%$ observations in the `match == True` class, for which `match_country == False`. Nonetheless, for the modeling purpose, it maybe useful to focus mostly on the pairs that have observations from the same country.","metadata":{}},{"cell_type":"markdown","source":"## Matching of States","metadata":{}},{"cell_type":"code","source":"# Barplot and donutplot of 'match_state'\ncount(data_pairs_feat, 'match_state')","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:47:52.720445Z","iopub.execute_input":"2022-06-22T15:47:52.721323Z","iopub.status.idle":"2022-06-22T15:47:52.844204Z","shell.execute_reply.started":"2022-06-22T15:47:52.721286Z","shell.execute_reply":"2022-06-22T15:47:52.84318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Donutplots of 'match_state' for different target classes\ndonut(data_pairs_feat, 'match_state', 'match')","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:47:52.845595Z","iopub.execute_input":"2022-06-22T15:47:52.845958Z","iopub.status.idle":"2022-06-22T15:47:53.072351Z","shell.execute_reply.started":"2022-06-22T15:47:52.845926Z","shell.execute_reply":"2022-06-22T15:47:53.071023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Matching of Cities","metadata":{}},{"cell_type":"code","source":"# Barplot and donutplot of 'match_city'\ncount(data_pairs_feat, 'match_city')","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:47:53.073956Z","iopub.execute_input":"2022-06-22T15:47:53.0745Z","iopub.status.idle":"2022-06-22T15:47:53.212749Z","shell.execute_reply.started":"2022-06-22T15:47:53.074463Z","shell.execute_reply":"2022-06-22T15:47:53.211699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Donutplots of 'match_city' for different target classes\ndonut(data_pairs_feat, 'match_city', 'match')","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:47:53.21406Z","iopub.execute_input":"2022-06-22T15:47:53.2144Z","iopub.status.idle":"2022-06-22T15:47:53.452501Z","shell.execute_reply.started":"2022-06-22T15:47:53.21437Z","shell.execute_reply":"2022-06-22T15:47:53.451547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Matching of ZIP Codes","metadata":{}},{"cell_type":"code","source":"# Barplot and donutplot of 'match_zip'\ncount(data_pairs_feat, 'match_zip')","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:47:53.454111Z","iopub.execute_input":"2022-06-22T15:47:53.455279Z","iopub.status.idle":"2022-06-22T15:47:53.560934Z","shell.execute_reply.started":"2022-06-22T15:47:53.455223Z","shell.execute_reply":"2022-06-22T15:47:53.559889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Donutplots of 'match_zip' for different target classes\ndonut(data_pairs_feat, 'match_zip', 'match')","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:47:53.562509Z","iopub.execute_input":"2022-06-22T15:47:53.562889Z","iopub.status.idle":"2022-06-22T15:47:53.771054Z","shell.execute_reply.started":"2022-06-22T15:47:53.562855Z","shell.execute_reply":"2022-06-22T15:47:53.770033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Matching of Categories","metadata":{}},{"cell_type":"code","source":"# Barplot and donutplot of 'match_categories'\ncount(data_pairs_feat, 'match_categories')","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:47:53.772531Z","iopub.execute_input":"2022-06-22T15:47:53.773093Z","iopub.status.idle":"2022-06-22T15:47:53.935926Z","shell.execute_reply.started":"2022-06-22T15:47:53.77304Z","shell.execute_reply":"2022-06-22T15:47:53.934665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Donutplots of 'match_categories' for different target classes\ndonut(data_pairs_feat, 'match_categories', 'match')","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:47:53.937625Z","iopub.execute_input":"2022-06-22T15:47:53.938244Z","iopub.status.idle":"2022-06-22T15:47:54.201449Z","shell.execute_reply.started":"2022-06-22T15:47:53.938168Z","shell.execute_reply":"2022-06-22T15:47:54.200331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Unlike other matching attributes, the `matching_categories` is *false* for majority of the pairs in the pregenerated set, i.e. for majority of pairs, category of the two observations do not match.","metadata":{}},{"cell_type":"markdown","source":"# 6. Multivariate Analysis - Pairs Set\n\n- [Correlation structure of numerical features](#Correlation-structure-of-numerical-features)\n- [Bivariate scatterplots of numerical features](#Bivariate-scatterplots-of-numerical-features)\n- [Trivariate scatterplots of numerical features](#Trivariate-scatterplots-of-numerical-features)\n- [Contingency tables of categorical features](#Contingency-tables-of-categorical-features)\n- [Numerical features for different classes of categorical features](#Numerical-features-for-different-classes-of-categorical-features)","metadata":{}},{"cell_type":"markdown","source":"## Correlation structure of numerical features","metadata":{}},{"cell_type":"markdown","source":"[Correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) is a statistical measure of linear dependence between two variables. Extreme correlation gives an indication that the two variables are linearly related, however this does not prove any causal relationship between the said variables. The measure is defined as the covariance of the two variables, scaled by the product of respective standard deviations. Let $\\left\\{\\left(x_1, y_1\\right), \\left(x_2, y_2\\right), \\cdots, \\left(x_n, y_n\\right)\\right\\}$ be paired data on the variables $\\left(x, y\\right)$. Then the correlation coefficient of the two variables is given by\n$$ r_{xy} := \\frac{\\text{cov}\\left(x, y\\right)}{s_x s_y} = \\frac{\\frac{1}{n}\\sum_{i=1}^n\\left(x_i - \\bar{x}\\right)\\left(y_i - \\bar{y}\\right)}{\\sqrt{\\frac{1}{n}\\sum_{i=1}^n\\left(x_i - \\bar{x}\\right)^2} \\sqrt{\\frac{1}{n}\\sum_{i=1}^n\\left(y_i - \\bar{y}\\right)^2}},$$\n\nwhere $\\bar{x}$ and $\\bar{y}$ denote the respective sample means of the two variables, given by $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i$ and $\\bar{y} = \\frac{1}{n}\\sum_{i=1}^n y_i$.","metadata":{}},{"cell_type":"code","source":"# Correlation coefficients of pairs of numerical features\ndf_corr = pd.DataFrame(columns = ['feature_1', 'feature_2', 'all pairs', 'matched pairs', 'unmatched pairs'])\ncols_num = data_pairs_feat.columns[(data_pairs_feat.dtypes == 'int64') | (data_pairs_feat.dtypes == 'float64')].tolist()\ndata_true = data_pairs_feat[data_pairs_feat['match'] == True]\ndata_false = data_pairs_feat[data_pairs_feat['match'] == False]\nfor i in range(len(cols_num)):\n    for j in range(len(cols_num)):\n        if i < j:\n            df_corr.loc[len(df_corr.index)] = [cols_num[i], cols_num[j], data_pairs_feat[cols_num[i]].corr(data_pairs_feat[cols_num[j]]), data_true[cols_num[i]].corr(data_true[cols_num[j]]), data_false[cols_num[i]].corr(data_false[cols_num[j]])]\ndf_corr.sort_values(by = 'all pairs', ascending = False, inplace = True)\ndf_corr","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:47:54.202698Z","iopub.execute_input":"2022-06-22T15:47:54.203035Z","iopub.status.idle":"2022-06-22T15:47:54.508092Z","shell.execute_reply.started":"2022-06-22T15:47:54.203005Z","shell.execute_reply":"2022-06-22T15:47:54.506577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The correlation structure of the numerical features are more or less similar for pairs with `match == True` and pairs with `match == False`.","metadata":{}},{"cell_type":"code","source":"# Correlation heatmap of numerical features\nplt.figure(figsize = (10, 7.5))\nsns.heatmap(data_pairs_feat[cols_num].corr(), vmin = -1, vmax = 1, annot = True, cmap = plt.cm.CMRmap_r)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:47:54.510089Z","iopub.execute_input":"2022-06-22T15:47:54.510619Z","iopub.status.idle":"2022-06-22T15:47:55.070428Z","shell.execute_reply.started":"2022-06-22T15:47:54.510571Z","shell.execute_reply":"2022-06-22T15:47:55.069342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- `dist_loc` is approximately uncorrelated with each of the other numerical features\n- `dist_address` has slight positive correlation with `dist_name` and `dist_phone`\n- `dist_url` and `dist_phone` have moderate positive correlation","metadata":{}},{"cell_type":"markdown","source":"## Bivariate scatterplots of numerical features","metadata":{}},{"cell_type":"code","source":"# Bivariate scatterplots\npairs = [(cols_num[i], cols_num[j]) for i in range(len(cols_num)) for j in range(len(cols_num)) if i < j]\nfor z in pairs:\n    fig, ax = plt.subplots(1, 2, figsize = (15, 6), sharex = True, sharey = True)\n    sns.scatterplot(data = data_pairs_feat[data_pairs_feat['match'] == True], x = z[0], y = z[1], ax = ax[0])\n    ax[0].set_title(\"match = True\", fontsize = 14)\n    sns.scatterplot(data = data_pairs_feat[data_pairs_feat['match'] == False], x = z[0], y = z[1], ax = ax[1])\n    ax[1].set_title(\"match = False\", fontsize = 14)\n    plt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:47:55.071798Z","iopub.execute_input":"2022-06-22T15:47:55.072135Z","iopub.status.idle":"2022-06-22T15:48:11.670969Z","shell.execute_reply.started":"2022-06-22T15:47:55.072104Z","shell.execute_reply":"2022-06-22T15:48:11.669914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Trivariate scatterplots of numerical features","metadata":{}},{"cell_type":"code","source":"# Trivariate scatterplots\ntriples = [(cols_num[i], cols_num[j], cols_num[k]) for i in range(len(cols_num)) for j in range(len(cols_num)) for k in range(len(cols_num)) if i < j < k]\nfor z in triples:\n    fig = plt.figure(figsize = (15, 9))\n    ax = fig.add_subplot(1, 2, 1, projection = '3d')\n    x_true = data_pairs_feat[data_pairs_feat['match'] == True][z[0]]\n    y_true = data_pairs_feat[data_pairs_feat['match'] == True][z[1]]\n    z_true = data_pairs_feat[data_pairs_feat['match'] == True][z[2]]\n    s1 = ax.scatter(x_true, y_true, z_true, s = 40, marker = 'o', c = y_true, alpha = 1)\n    ax.set_title(\"match = True\", fontsize = 14)\n    ax.set_xlabel(z[0])\n    ax.set_ylabel(z[1]) # ax.set_zlabel(z[2])\n    ax = fig.add_subplot(1, 2, 2, projection = '3d')\n    x_false = data_pairs_feat[data_pairs_feat['match'] == False][z[0]]\n    y_false = data_pairs_feat[data_pairs_feat['match'] == False][z[1]]\n    z_false = data_pairs_feat[data_pairs_feat['match'] == False][z[2]]\n    s2 = ax.scatter(x_false, y_false, z_false, s = 40, marker = 'o', c = y_false, alpha = 1)\n    ax.set_title(\"match = False\", fontsize = 14)\n    ax.set_xlabel(z[0])\n    ax.set_ylabel(z[1])\n    ax.set_zlabel(z[2])\n    plt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:48:11.672619Z","iopub.execute_input":"2022-06-22T15:48:11.67301Z","iopub.status.idle":"2022-06-22T15:50:21.485265Z","shell.execute_reply.started":"2022-06-22T15:48:11.672974Z","shell.execute_reply":"2022-06-22T15:50:21.48415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Contingency tables of categorical features","metadata":{}},{"cell_type":"code","source":"# Function to compute contingency tables for pairs of binary features\ndef contingency_pairs(df, pairs, ncols = 3, figsize_multiplier = 4, update_ylabel = False):\n    nrows = math.ceil(len(pairs) / ncols)\n    figsize = (figsize_multiplier * ncols, 0.8 * figsize_multiplier * nrows)\n    fig, ax = plt.subplots(nrows, ncols, figsize = figsize, sharey = False)\n    labels = [True, False]\n    for i in range(len(pairs)):\n        contingency_mat = np.zeros(shape = (2, 2))\n        for j in range(2):\n            for k in range(2):\n                contingency_mat[j][k] = len([l for l in range(len(df)) if df[pairs[i][0]][l] == labels[j] and df[pairs[i][1]][l] == labels[k]])\n        contingency_df = pd.DataFrame(contingency_mat)\n        hm = sns.heatmap(contingency_df, annot = True, annot_kws = {\"size\": 16}, fmt = 'g', ax = ax[i // ncols, i % ncols])\n        hm.set_xlabel(f'{pairs[i][1]}', fontsize = 14)\n        hm.set_ylabel(f'{pairs[i][0]}', fontsize = 14)\n        hm.set_xticklabels(labels, fontdict = {'fontsize': 12}, rotation = 0, ha = \"right\")\n        hm.set_yticklabels(labels, fontdict = {'fontsize': 12}, rotation = 0, ha = \"right\")\n        if i % ncols != 0 and update_ylabel == True:\n                ax[i // ncols, i % ncols].set_ylabel(\" \")\n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:50:21.486796Z","iopub.execute_input":"2022-06-22T15:50:21.490732Z","iopub.status.idle":"2022-06-22T15:50:21.507846Z","shell.execute_reply.started":"2022-06-22T15:50:21.490657Z","shell.execute_reply":"2022-06-22T15:50:21.506275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Contingency tables for pairs of binary features\ncols_cat = [col for col in data_pairs_feat.columns if data_pairs_feat[col].nunique() == 2 and col != 'match']\npairs = [(cols_cat[i], cols_cat[j]) for i in range(len(cols_cat)) for j in range(len(cols_cat)) if i < j]\ncontingency_pairs(data_pairs_feat, pairs, ncols = 2, figsize_multiplier = 6)","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:50:21.510534Z","iopub.execute_input":"2022-06-22T15:50:21.511671Z","iopub.status.idle":"2022-06-22T15:54:24.680111Z","shell.execute_reply.started":"2022-06-22T15:50:21.511592Z","shell.execute_reply":"2022-06-22T15:54:24.679193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Numerical features for different classes of categorical features","metadata":{}},{"cell_type":"code","source":"# Numerical features for different classes of categorical features\npairs = [(cols_num[i], cols_cat[j]) for i in range(len(cols_num)) for j in range(len(cols_cat))]\nncols = 3\nnrows = math.ceil(len(pairs) / ncols)\nfig, ax = plt.subplots(nrows, ncols, figsize = (5 * ncols, 4.2 * nrows), sharey = False)\nfor i in range(len(pairs)):\n    sns.violinplot(data = data_pairs_feat, x = pairs[i][1], y = pairs[i][0], ax = ax[i // ncols, i % ncols])\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:54:24.681505Z","iopub.execute_input":"2022-06-22T15:54:24.682271Z","iopub.status.idle":"2022-06-22T15:55:19.1683Z","shell.execute_reply.started":"2022-06-22T15:54:24.682235Z","shell.execute_reply":"2022-06-22T15:55:19.166977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Expectedly, all the plots show global peak at $0$ and a local peak not far from it. This observation is consistent with the histograms of the numerical features in the univariate analysis. However, there are some finer details to check in the violinplots for the following features:\n- `dist_loc` for different classes of `match_country`, `match_state`, `match_city`, `match_zip`: There is a general dependence between the two features as `dist_loc` should be less in the situations where `match_country == True` than the situations where `match_country == False`. However, this does not hold in general as it is always possible that two POIs are located at the extremely opposite regions on the same country, and also that two POIs are located at nearby regions of two neighbor countries. Similar pattern is reflected in the violinplots of `dist_loc` for different classes of `match_state`, `match_city` and `match_zip`. Interestingly, the pattern does not replicate for `dist_loc` and `match_categories`.\n- `dist_name` for different classes of `match_country`, `match_state`, `match_city`, `match_zip`, `match_categories`: Though not as extreme as the cases with `dist_loc`, we observe that `dist_name` is more likely to be $0$ in the situations where the attributes `match_country`, `match_state`, `match_city`, `match_zip`, `match_categories` are *true*, than the situations where the same are *false*.\n- `dist_url` and `match_country`: The violinplot for `dist_url` is far less concentrated around $0$ if `match_country == False`, than `match_country == True`. However, the same pattern cannot be seen when `dist_url` is plotted against `match_state`, `match_city` or `match_zip`.\n- `dist_phone` and `match_country`: The distributions of `dist_phone` for `match_country == True` and `match_country == False` show considerable difference in concentration about $0$ and presence of outliers, however the distribution of `dist_phone` seems to be more or less unaffected by the status of `match_state`, `match_city`, `match_zip` and `match_categories`.","metadata":{}},{"cell_type":"markdown","source":"# Acknowledgements\n\n- [Foursquare - Location Matching](https://www.kaggle.com/competitions/foursquare-location-matching) competition","metadata":{}},{"cell_type":"markdown","source":"# References\n\n- [Correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient)\n- [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance)\n- [Foursquare City Guide](https://en.wikipedia.org/wiki/Foursquare_City_Guide)\n- [Foursquare Labs Inc.](https://foursquare.com/)\n- [Foursquare Swarm](https://en.wikipedia.org/wiki/Foursquare_Swarm)\n- [Grove Karl Gilbert](https://en.wikipedia.org/wiki/Grove_Karl_Gilbert)\n- [ISO 3166-1 alpha-2](https://en.wikipedia.org/wiki/ISO_3166-1_alpha-2)\n- [Jaccard index](https://en.wikipedia.org/wiki/Jaccard_index)\n- [Jaccard/Tanimoto similarity test and estimation methods for biological presence-absence data](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-019-3118-5)\n- [Jaccard/Tanimoto similarity test and estimation methods (arxiv version)](https://arxiv.org/abs/1903.11372)\n- [Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance)\n- [List of U.S. state and territory abbreviations](https://en.wikipedia.org/wiki/List_of_U.S._state_and_territory_abbreviations)\n- [Olympus Mons](https://en.wikipedia.org/wiki/Olympus_Mons)\n- [Paul Jaccard](https://en.wikipedia.org/wiki/Paul_Jaccard)\n- [Point of interest](https://en.wikipedia.org/wiki/Point_of_interest)\n- [Vladimir Levenshtein](https://en.wikipedia.org/wiki/Vladimir_Levenshtein)","metadata":{}},{"cell_type":"code","source":"# Runtime and memory usage\nstop = time.time()\nprint(pd.Series({\"Process runtime\": \"{:.2f} seconds\".format(float(stop - start)),\n                 \"Process memory usage\": \"{:.2f} MB\".format(float(process.memory_info()[0]/(1024*1024)))}).to_string())","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:55:19.170218Z","iopub.execute_input":"2022-06-22T15:55:19.170717Z","iopub.status.idle":"2022-06-22T15:55:19.179786Z","shell.execute_reply.started":"2022-06-22T15:55:19.17067Z","shell.execute_reply":"2022-06-22T15:55:19.179025Z"},"trusted":true},"execution_count":null,"outputs":[]}]}