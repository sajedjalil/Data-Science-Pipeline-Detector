{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1><center> Foursquare Location Matching </center></h1>\n<h2><center> Baseline Modeling </center></h2>\n<h2><center> Sugata Ghosh </center></h2>","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"### Competition: [Foursquare - Location Matching](https://www.kaggle.com/competitions/foursquare-location-matching)\n\n### Notebook on Exploratory Data Analysis: [Foursquare Location Matching - EDA](https://www.kaggle.com/code/sugataghosh/foursquare-location-matching-eda)","metadata":{}},{"cell_type":"markdown","source":"The present notebook compares several baseline models to classify whether two given [points of interest](https://en.wikipedia.org/wiki/Point_of_interest) (POI) match or not. We have considered the following features:\n- **Distance between locations** (in km), computed from the information on latitude and longitude of locations, using the [haversine formula](https://en.wikipedia.org/wiki/Haversine_formula)\n- **Similarity measures for several string-type features**, based on proportion of the length of **largest common substring** of two given strings, to the length of the shorter string\n- **Matching of countries**, based on [ISO 3166-1 alpha-2](https://en.wikipedia.org/wiki/ISO_3166-1_alpha-2) country codes","metadata":{}},{"cell_type":"markdown","source":"### Contents\n\n- [Introduction](#1.-Introduction)\n- [Train-Test Split](#2.-Train-Test-Split)\n- [Data Preprocessing](#3.-Data-Preprocessing)\n- [Feature Engineering](#4.-Feature-Engineering)\n- [Baseline Modeling](#5.-Baseline-Modeling)\n- [Acknowledgements](#Acknowledgements)\n- [References](#References)","metadata":{}},{"cell_type":"markdown","source":"### Importing libraries","metadata":{}},{"cell_type":"code","source":"# File system manangement\nimport time, psutil, os, gc\n\n# Mathematical functions\nimport math\nfrom math import cos, asin, sqrt, pi\n\n# Data manipulation\nimport numpy as np\nimport pandas as pd\n\n# Plotting and visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme()\nimport plotly\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\n\n# Train-test split and k-fold cross validation\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV, RepeatedStratifiedKFold\n\n# Classifiers\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import svm\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.linear_model import RidgeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\n\n# Others\nimport operator as op\nfrom functools import reduce, lru_cache","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:35:23.075034Z","iopub.execute_input":"2022-06-30T12:35:23.075431Z","iopub.status.idle":"2022-06-30T12:35:26.076434Z","shell.execute_reply.started":"2022-06-30T12:35:23.075348Z","shell.execute_reply":"2022-06-30T12:35:26.075338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Runtime and memory usage","metadata":{}},{"cell_type":"code","source":"# Recording the starting time, complemented with a stopping time check in the end to compute process runtime\nstart = time.time()\n\n# Class representing the OS process and having memory_info() method to compute process memory usage\nprocess = psutil.Process(os.getpid())","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:35:26.078329Z","iopub.execute_input":"2022-06-30T12:35:26.078655Z","iopub.status.idle":"2022-06-30T12:35:26.08334Z","shell.execute_reply.started":"2022-06-30T12:35:26.078626Z","shell.execute_reply":"2022-06-30T12:35:26.082601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Introduction\n\n- [Point of Interest](#Point-of-Interest)\n- [The Problem of POI Matching](#The-Problem-of-POI-Matching)\n- [About Foursquare](#About-Foursquare)\n- [Data](#Data)\n- [Project Objective](#Project-Objective)\n- [Evaluation Metric](#Evaluation-Metric)\n- [Working with Pairs Set](#Working-with-Pairs-Set)","metadata":{}},{"cell_type":"markdown","source":"**Note:** The introduction section from the [Foursquare Location Matching - EDA](https://www.kaggle.com/code/sugataghosh/foursquare-location-matching-eda) notebook is reproduced here to make the present notebook self-contained, so that one can understand the problem, as well as the objective of the competition, without requiring to go back to the EDA notebook.","metadata":{}},{"cell_type":"markdown","source":"## Point of Interest\n\nA point of interest (POI) is a specific point location that someone may find useful or interesting. An example is a point on the Earth representing the location of the Eiffel Tower, or a point on Mars representing the location of its highest mountain, [Olympus Mons](https://en.wikipedia.org/wiki/Olympus_Mons). Most consumers use the term when referring to hotels, campsites, fuel stations or any other categories used in modern automotive navigation systems. Users of a mobile device can be provided with geolocation and time aware POI service that recommends geolocations nearby and with a temporal relevance (e.g. POI to special services in a ski resort are available only in winter). The notion of POI is widely used in cartography, especially in electronic variants including GIS, and GPS navigation software.","metadata":{}},{"cell_type":"markdown","source":"## The Problem of POI Matching\n\nIt is useful to combine POI data obtained from multiple sources for effective reusability. One issue in merging such data is that different dataset may have variations in POI name, address, and other identifying information for the same POI. It is thus important to identify observations which refer to the same POI. The process of POI matching involves finding POI pairs that refer to the same real-world entity, which is the core issue in geospatial data integration and is perhaps the most technically difficult part of multi-source POI fusion. The raw location data can contain noise, unstructured information, and incomplete or inaccurate attributes, which makes the task even more difficult. Nonetheless, to maintain the highest level of accuracy, the data must be matched and duplicate POIs must be identified and merged with timely updates from multiple sources. A combination of machine-learning algorithms and rigorous human validation methods are optimal for effective de-duplication of such data.","metadata":{}},{"cell_type":"markdown","source":"## About Foursquare\n\n[Foursquare Labs Inc.](https://foursquare.com/), commonly known as Foursquare, is an American location technology company and data cloud platform. The company's location platform is the foundation of several business and consumer products, including the [Foursquare City Guide](https://en.wikipedia.org/wiki/Foursquare_City_Guide) and [Foursquare Swarm](https://en.wikipedia.org/wiki/Foursquare_Swarm) apps. Foursquare's products include Pilgrim SDK, Places, Visits, Attribution, Audience, Proximity, and Unfolded Studio. It is one of the leading independent providers of global POI data and is dedicated to building meaningful bridges between digital spaces and physical places. Trusted by leading enterprises like Apple, Microsoft, Samsung, and Uber, Foursquare's tech stack harnesses the power of places and movement to improve customer experiences and drive better business outcomes.","metadata":{}},{"cell_type":"markdown","source":"## Data\n\n**Source:** https://www.kaggle.com/competitions/foursquare-location-matching/data\n\nThe data considered in the competition comprises over one-and-a-half million place entries for hundreds of thousands of commercial Points-of-Interest (POIs) around the globe. Though the data entries may represent or resemble entries for real places, they may be contaminated with artificial information or additional noise.\n\nThe training data comprises eleven attribute fields for over one million place entries, together with:\n- `id` : A unique identifier for each entry.\n- `point_of_interest` : An identifier for the POI the entry represents. There may be one or many entries describing the same POI. Two entries *match* when they describe a common POI.","metadata":{}},{"cell_type":"code","source":"# Loading the training data\ndata_train = pd.read_csv('../input/foursquare-location-matching/train.csv')\nprint(pd.Series({\"Memory usage\": \"{:.2f} MB\".format(data_train.memory_usage().sum()/(1024*1024)),\n                 \"Dataset shape\": \"{}\".format(data_train.shape)}).to_string())\nprint(\" \")\ndata_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:35:26.084998Z","iopub.execute_input":"2022-06-30T12:35:26.085495Z","iopub.status.idle":"2022-06-30T12:35:34.916872Z","shell.execute_reply.started":"2022-06-30T12:35:26.085456Z","shell.execute_reply":"2022-06-30T12:35:34.915761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# A typical observation from the training set\ndata_train.iloc[0]","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:35:34.919675Z","iopub.execute_input":"2022-06-30T12:35:34.920339Z","iopub.status.idle":"2022-06-30T12:35:34.928576Z","shell.execute_reply.started":"2022-06-30T12:35:34.920291Z","shell.execute_reply":"2022-06-30T12:35:34.927341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The pairs data is a pregenerated set of pairs of place entries from the training data designed to improve detection of matches. It includes:\n- `match` : Boolean variables denoting whether or not the pair of entries describes a common POI.","metadata":{}},{"cell_type":"code","source":"# Loading pregenerated set of pairs of place entries from the training data\ndata_pairs = pd.read_csv('../input/foursquare-location-matching/pairs.csv')\nprint(pd.Series({\"Memory usage\": \"{:.2f} MB\".format(data_pairs.memory_usage().sum()/(1024*1024)),\n                 \"Dataset shape\": \"{}\".format(data_pairs.shape)}).to_string())\nprint(\" \")\ndata_pairs.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:35:34.930428Z","iopub.execute_input":"2022-06-30T12:35:34.93098Z","iopub.status.idle":"2022-06-30T12:35:43.123983Z","shell.execute_reply.started":"2022-06-30T12:35:34.930935Z","shell.execute_reply":"2022-06-30T12:35:43.122837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# A typical observation from the pregenerated set of pairs\ndata_pairs.iloc[0]","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:35:43.125867Z","iopub.execute_input":"2022-06-30T12:35:43.128945Z","iopub.status.idle":"2022-06-30T12:35:43.137372Z","shell.execute_reply.started":"2022-06-30T12:35:43.128889Z","shell.execute_reply":"2022-06-30T12:35:43.13652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The test data comprises a set of place entries with their recorded attribute fields, similar to the training set. The POIs in the test data are distinct from the POIs in the training data.","metadata":{}},{"cell_type":"code","source":"# Loading the test data\ndata_test = pd.read_csv('../input/foursquare-location-matching/test.csv')\nprint(pd.Series({\"Memory usage\": \"{:.5f} MB\".format(data_test.memory_usage().sum()/(1024*1024)),\n                 \"Dataset shape\": \"{}\".format(data_test.shape)}).to_string())\nprint(\" \")\ndata_test.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:35:43.138385Z","iopub.execute_input":"2022-06-30T12:35:43.139569Z","iopub.status.idle":"2022-06-30T12:35:43.174301Z","shell.execute_reply.started":"2022-06-30T12:35:43.139515Z","shell.execute_reply":"2022-06-30T12:35:43.173142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# A typical observation from the test set\ndata_test.iloc[0]","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:35:43.176286Z","iopub.execute_input":"2022-06-30T12:35:43.177141Z","iopub.status.idle":"2022-06-30T12:35:43.18717Z","shell.execute_reply.started":"2022-06-30T12:35:43.177093Z","shell.execute_reply":"2022-06-30T12:35:43.185913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Project Objective\n\nThe goal of the project is to match POIs together. Using the provided dataset of over one-and-a-half million places entries, heavily altered to include noise, duplications, extraneous, or incorrect information, the objective is to produce an algorithm that predicts which place entries represent the same POI. Each place entry in the data includes useful attributes like name, street address, and coordinates. Efficient and successful matching of POIs will make it easier to identify where new stores or businesses would benefit people the most.","metadata":{}},{"cell_type":"markdown","source":"## Evaluation Metric\n\n**[Jaccard index](https://en.wikipedia.org/wiki/Jaccard_index).** Also known as *Jaccard similarity coefficient*, it is a statistic used for gauging the similarity and diversity of sample sets. It was developed by [Grove Karl Gilbert](https://en.wikipedia.org/wiki/Grove_Karl_Gilbert) in 1884 as his *ratio of verification (v)* and now is frequently referred to as the *Critical Success Index* in meteorology. It was later developed independently by [Paul Jaccard](https://en.wikipedia.org/wiki/Paul_Jaccard), originally giving the French name *coefficient de communaut√©* and independently formulated again by T. T. Tanimoto. Thus, the *Tanimoto index* or *Tanimoto coefficient* are also used in some fields. However, they are identical in generally taking the ratio of Intersection over Union. The Jaccard coefficient measures similarity between finite sample sets, and is defined as the size of the intersection divided by the size of the union of the sample sets:\n\n$$ J(A, B) := \\frac{\\left\\vert A \\cap B \\right\\vert}{\\left\\vert A \\cup B \\right\\vert} = \\frac{\\left\\vert A \\cap B \\right\\vert}{\\left\\vert A \\right\\vert + \\left\\vert B \\right\\vert - \\left\\vert A \\cap B \\right\\vert}. $$\n\nNote that by design, $0\\leq J\\left(A, B\\right)\\leq 1$. If $A$ and $B$ are both empty, define $J(A, B) = 1$. The Jaccard coefficient is widely used in computer science, ecology, genomics, and other sciences, where binary or binarized data are used. Both the exact solution and approximation methods are available for hypothesis testing with the Jaccard coefficient. See [this paper](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-019-3118-5) ([arxiv version](https://arxiv.org/abs/1903.11372)) for details.\n\nLet us assume that for a specific `id` $a$, our algorithm produces three matches $a$, $b$ and $c$ whereas the true matches are $a$, $b$, $d$ and $e$. Then the Jaccard index for the prediction on this particular `id` will be\n\n$$ \\frac{\\left\\vert \\left\\{a, b, c\\right\\} \\cap \\left\\{a, b, d, e\\right\\} \\right\\vert}{\\left\\vert \\left\\{a, b, c\\right\\} \\cup \\left\\{a, b, d, e\\right\\} \\right\\vert} = \\frac{\\left\\vert \\left\\{a, b\\right\\} \\right\\vert}{\\left\\vert \\left\\{a, b, c, d, e\\right\\} \\right\\vert} = \\frac{2}{5}. $$\n\nThus, while correct matching predictions are rewarded, incorrect matching predictions are penalised by equal measure. The evaluation metric is simply the mean of Jaccard indices for each of the test observations, i.e. if the test data comprises $n_{\\text{test}}$ observations and $J_i$ denotes the Jaccard index corresponding to the $i$th test observation, $i = 1,2,\\cdots,n_{\\text{test}}$, then the final metric by which a model will be evaluated is:\n\n$$ \\frac{1}{n_{\\text{test}}} \\sum_{i=1}^{n_{\\text{test}}} J_i. $$","metadata":{}},{"cell_type":"markdown","source":"**Note:** In this notebook, we shall rely solely on the provided pairs set `data_pairs` to build the baseline models.","metadata":{}},{"cell_type":"markdown","source":"# 2. Train-Test Split","metadata":{}},{"cell_type":"markdown","source":"We split `data_pairs` into two parts:\n- `data_pairs_train`: The portion of data that we use to *train* the models\n- `data_pairs_test`: The portion of data that we use to *test* or evaluate the models\n\nWe shall keep `data_test`, which has only five observations, separate from the modeling procedure.","metadata":{}},{"cell_type":"code","source":"# Splitting data_train\ndata_pairs_train, data_pairs_test = train_test_split(data_pairs, test_size = 0.2, random_state = 40)\n\nlabels = ['Train','Test']\nvalues = [len(data_pairs_train), len(data_pairs_test)]\nfig_data = [go.Pie(values = values, labels = labels, hole = 0.0, textinfo = 'label+percent')]\nfig_title = dict(text = \"Train-test split\", x = 0.5, y = 0.95)\nfig = go.Figure(data = fig_data)\nfig.update_layout(height = 500, width = 800, showlegend = False, title = fig_title)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:35:43.188491Z","iopub.execute_input":"2022-06-30T12:35:43.188827Z","iopub.status.idle":"2022-06-30T12:35:44.196049Z","shell.execute_reply.started":"2022-06-30T12:35:43.1888Z","shell.execute_reply":"2022-06-30T12:35:44.194982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Donutplots of the 'match' column\nfig = make_subplots(rows = 1, cols = 2, specs = [[{'type': 'domain'}, {'type': 'domain'}]])\nx_val_train = data_pairs_train['match'].value_counts(sort = False).index.tolist()\ny_val_train = data_pairs_train['match'].value_counts(sort = False).tolist()\nx_val_test = data_pairs_test['match'].value_counts(sort = False).index.tolist()\ny_val_test = data_pairs_test['match'].value_counts(sort = False).tolist()\nfig.add_trace(go.Pie(values = y_val_train, labels = x_val_train, hole = 0.5, textinfo = 'label+percent', title = \"Train\"), row = 1, col = 1)\nfig.add_trace(go.Pie(values = y_val_test, labels = x_val_test, hole = 0.5, textinfo = 'label+percent', title = \"Test\"), row = 1, col = 2)\nfig.update_layout(height = 500, width = 800, showlegend = False, xaxis = dict(tickmode = 'linear', tick0 = 0, dtick = 1), title = dict(text = \"Frequency comparison of 'match'\", x = 0.5, y = 0.95)) \nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:35:44.201196Z","iopub.execute_input":"2022-06-30T12:35:44.201546Z","iopub.status.idle":"2022-06-30T12:35:44.394714Z","shell.execute_reply.started":"2022-06-30T12:35:44.201503Z","shell.execute_reply":"2022-06-30T12:35:44.393452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We observe that in both the `data_pairs_train` and the `data_pairs_test`, the count of pairs that match is higher than the count of pairs that do not match. However, the imbalance is not too big.","metadata":{}},{"cell_type":"markdown","source":"# 3. Data Preprocessing\n\n- [Decoding States Abbreviations](#Decoding-States-Abbreviations)\n- [Conversion to Lowercase](#Conversion-to-Lowercase)\n- [Missing Data Imputation](#Missing-Data-Imputation)","metadata":{}},{"cell_type":"markdown","source":"## Decoding States Abbreviations","metadata":{}},{"cell_type":"code","source":"# Dictionary of US states abbreviations and names\nurl_abbrev_to_name = \"https://raw.githubusercontent.com/sugatagh/Foursquare-Location-Matching/main/JSON/US_states_abbrev_to_name.json\"\ndict_abbrev_to_name = pd.read_json(url_abbrev_to_name, typ = 'series')\ndict_abbrev_to_name","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:35:44.396381Z","iopub.execute_input":"2022-06-30T12:35:44.397231Z","iopub.status.idle":"2022-06-30T12:35:44.71906Z","shell.execute_reply.started":"2022-06-30T12:35:44.397191Z","shell.execute_reply":"2022-06-30T12:35:44.718233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Converting US states abbreviations to names\ndata_pairs_train.replace({'state_1': dict_abbrev_to_name}, inplace = True)\ndata_pairs_train.replace({'state_2': dict_abbrev_to_name}, inplace = True)\ndata_pairs_test.replace({'state_1': dict_abbrev_to_name}, inplace = True)\ndata_pairs_test.replace({'state_2': dict_abbrev_to_name}, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:35:44.72007Z","iopub.execute_input":"2022-06-30T12:35:44.721081Z","iopub.status.idle":"2022-06-30T12:35:47.948246Z","shell.execute_reply.started":"2022-06-30T12:35:44.721031Z","shell.execute_reply":"2022-06-30T12:35:47.946966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conversion to Lowercase","metadata":{}},{"cell_type":"markdown","source":"We convert all alphabetical characters in the relevant object-type columns to lowercase so that the models do not differentiate identical words due to [case sensitivity](https://en.wikipedia.org/wiki/Case_sensitivity).","metadata":{}},{"cell_type":"code","source":"# Converting to lowercase\ndef convert_to_lowercase_skipna(x):\n    \"\"\"\n    Converts a given string to lowercase\n    \n    Arg:\n      x (string/NaN): input string (possibly NaN due to missing value)\n      \n    Returns:\n      y (string/NaN): x.lower() if x is not NaN, x otherwise\n    \"\"\"\n    if str(x) == 'nan':\n        y = x\n    else:\n        y = x.lower()\n    return y\n\ntext = \"Mobile Phone Shops\"\nprint(\"Input: {}\".format(text))\nprint(\"Output: {}\".format(convert_to_lowercase_skipna(text)))","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:35:47.949603Z","iopub.execute_input":"2022-06-30T12:35:47.949915Z","iopub.status.idle":"2022-06-30T12:35:47.957594Z","shell.execute_reply.started":"2022-06-30T12:35:47.949887Z","shell.execute_reply":"2022-06-30T12:35:47.95622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Before applying this conversion, we have to ensure that the argument is of `string` type. However, we do not want to convert the `nan` values to the string `'nan'`, as the converted string will no longer be identified as a missing value, and hence will go unaltered in the *missing value imputation* step.","metadata":{}},{"cell_type":"code","source":"# Converting to string, unless the argument is nan\ndef convert_to_string_skipna(x):\n    \"\"\"\n    Converts an input to string if it is not NaN, otherwise it is left unaltered\n    \n    Arg:\n      x (any python data type): input to be converted (possibly NaN due to missing value)\n      \n    Returns:\n      y (str/NaN): str(x) if x is not NaN, x otherwise\n    \"\"\"\n    if str(x) == 'nan':\n        y = x\n    else:\n        y = str(x)\n    return y\n\nnumber = 40\nprint(\"Input: {}\".format(number))\nprint(\"Output:\")\nconvert_to_string_skipna(number)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:35:47.958855Z","iopub.execute_input":"2022-06-30T12:35:47.959429Z","iopub.status.idle":"2022-06-30T12:35:47.981372Z","shell.execute_reply.started":"2022-06-30T12:35:47.959395Z","shell.execute_reply":"2022-06-30T12:35:47.980133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We convert the columns `name`, `address`, `city`, `state` and `categories`. We leave the column `url` as it is, for its case sensitivity.","metadata":{}},{"cell_type":"code","source":"# Applying the conversion to object columns\ncols_lower = ['name', 'address', 'city', 'state', 'categories']\ncols_lower_pairs = ['name_1', 'address_1', 'city_1', 'state_1', 'categories_1',\n                    'name_2', 'address_2', 'city_2', 'state_2', 'categories_2']\nfor col in cols_lower:\n    data_test[col] = data_test[col].apply(convert_to_string_skipna).apply(convert_to_lowercase_skipna)\nfor col in cols_lower_pairs:\n    data_pairs_train[col] = data_pairs_train[col].apply(convert_to_string_skipna).apply(convert_to_lowercase_skipna)\n    data_pairs_test[col] = data_pairs_test[col].apply(convert_to_string_skipna).apply(convert_to_lowercase_skipna)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:35:47.983048Z","iopub.execute_input":"2022-06-30T12:35:47.983378Z","iopub.status.idle":"2022-06-30T12:35:53.975337Z","shell.execute_reply.started":"2022-06-30T12:35:47.983349Z","shell.execute_reply":"2022-06-30T12:35:53.97428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Missing Data Imputation","metadata":{}},{"cell_type":"code","source":"# Columns with missing values in the training set with respective proportion of missing values\n(data_pairs_train.isna().sum()[data_pairs_train.isna().sum() != 0] / len(data_pairs_train)).sort_values(ascending = False)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:35:53.976819Z","iopub.execute_input":"2022-06-30T12:35:53.977148Z","iopub.status.idle":"2022-06-30T12:35:55.131581Z","shell.execute_reply.started":"2022-06-30T12:35:53.977085Z","shell.execute_reply":"2022-06-30T12:35:55.130344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Missing values in the training set\nplt.figure(figsize = (9, 6))\ndf_temp = data_pairs_train.isna().sum() * 100 / len(data_pairs_train)\ns = sns.barplot(x = df_temp.values, y = df_temp.index)\ns.set_xlim(0, 100)\ns.set_xlabel(\"% of missing values\", fontsize = 14)\ns.set_ylabel(\"column\", fontsize = 14)\nplt.axvline(x = 30)\nplt.axvline(x = 50)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:35:55.132826Z","iopub.execute_input":"2022-06-30T12:35:55.133114Z","iopub.status.idle":"2022-06-30T12:35:56.202264Z","shell.execute_reply.started":"2022-06-30T12:35:55.133088Z","shell.execute_reply":"2022-06-30T12:35:56.200968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Seven columns `zip_1`, `url_1`, `phone_1`, `address_2`, `zip_2`, `url_2` and `phone_2` have over $30\\%$ values missing. We shall drop these columns from the subsequent analysis.","metadata":{}},{"cell_type":"markdown","source":"#### Dropping columns with more than 30% missing values","metadata":{}},{"cell_type":"markdown","source":"Even though `address_1` has less than $30\\%$ missing values, `address_2` has a very high proportion of it. For a comparison purpose, `address_1` cannot contribute anything alone because *one hand cannot clap*. Thus we drop it alongside all the columns with more than $30\\%$ missing values.","metadata":{}},{"cell_type":"code","source":"# Dropping columns with more than 30% missing values in the training set\ncols_drop = ['address', 'zip', 'url', 'phone']\ncols_drop_pairs = ['address_1', 'zip_1', 'url_1', 'phone_1', 'address_2', 'zip_2', 'url_2', 'phone_2']\ndata_test.drop(cols_drop, axis = 1, inplace = True)\ndata_pairs_train.drop(cols_drop_pairs, axis = 1, inplace = True)\ndata_pairs_test.drop(cols_drop_pairs, axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:35:56.204306Z","iopub.execute_input":"2022-06-30T12:35:56.204642Z","iopub.status.idle":"2022-06-30T12:35:56.434339Z","shell.execute_reply.started":"2022-06-30T12:35:56.204613Z","shell.execute_reply":"2022-06-30T12:35:56.433208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Imputing missing values with 'unknown'","metadata":{}},{"cell_type":"code","source":"# Imputing missing names with 'unknown'\ncols_unknown = ['name', 'city', 'state', 'country']\nfor col in cols_unknown:\n    data_test[col].fillna('unknown', inplace = True)\ncols_unknown_pairs = ['name_1', 'city_1', 'state_1', 'country_1', 'name_2', 'city_2', 'state_2', 'country_2']\nfor col in cols_unknown_pairs:\n    data_pairs_train[col].fillna('unknown', inplace = True)\n    data_pairs_test[col].fillna('unknown', inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:35:56.435708Z","iopub.execute_input":"2022-06-30T12:35:56.436021Z","iopub.status.idle":"2022-06-30T12:35:56.700714Z","shell.execute_reply.started":"2022-06-30T12:35:56.435993Z","shell.execute_reply":"2022-06-30T12:35:56.69962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Mode imputation for categories","metadata":{}},{"cell_type":"code","source":"# Mode imputation for categories\ndata_test['categories'].fillna(data_test['categories'].mode()[0], inplace = True)\ndata_pairs_train['categories_1'].fillna(data_pairs_train['categories_1'].mode()[0], inplace = True)\ndata_pairs_train['categories_2'].fillna(data_pairs_train['categories_2'].mode()[0], inplace = True)\ndata_pairs_test['categories_1'].fillna(data_pairs_test['categories_1'].mode()[0], inplace = True)\ndata_pairs_test['categories_2'].fillna(data_pairs_test['categories_2'].mode()[0], inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:35:56.701831Z","iopub.execute_input":"2022-06-30T12:35:56.702128Z","iopub.status.idle":"2022-06-30T12:35:56.950302Z","shell.execute_reply.started":"2022-06-30T12:35:56.7021Z","shell.execute_reply":"2022-06-30T12:35:56.949075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Count of missing values in the 'data_pairs_train'\ndata_pairs_train.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:35:56.952091Z","iopub.execute_input":"2022-06-30T12:35:56.952578Z","iopub.status.idle":"2022-06-30T12:35:57.295592Z","shell.execute_reply.started":"2022-06-30T12:35:56.952516Z","shell.execute_reply":"2022-06-30T12:35:57.294625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Count of missing values in the 'data_pairs_test'\ndata_pairs_test.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:35:57.296784Z","iopub.execute_input":"2022-06-30T12:35:57.297072Z","iopub.status.idle":"2022-06-30T12:35:57.388332Z","shell.execute_reply.started":"2022-06-30T12:35:57.297047Z","shell.execute_reply":"2022-06-30T12:35:57.387225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Count of missing values in the true test set\ndata_test.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:35:57.390031Z","iopub.execute_input":"2022-06-30T12:35:57.390876Z","iopub.status.idle":"2022-06-30T12:35:57.401604Z","shell.execute_reply.started":"2022-06-30T12:35:57.390827Z","shell.execute_reply":"2022-06-30T12:35:57.400242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Feature Engineering\n\n- [Distance between Locations](#Distance-between-Locations)\n- [Features Based on Largest Common Substring](#Features-Based-on-Largest-Common-Substring)\n- [Matching of Countries](#Matching-of-Countries)\n- [The Target Variable](#The-Target-Variable)","metadata":{}},{"cell_type":"markdown","source":"Each row in `data_pairs_train` or `data_pairs_test` consists of two training observations and a `match` variable which indicates whether the two observations match or not. As we have seen in the previous section, an observation in the pairs set can be represented as:\n\n$$ \\left(a_1,a_2,\\ldots,a_n; b_1,b_2,\\ldots,b_n; y\\right), $$\n\nwhere $a_1,a_2,\\ldots,a_n$ is the observed features of the first observation, $b_1,b_2,\\ldots,b_n$ is the same for the second observation and $y$ is the `match` variable. Now, suppose that $\\left(c_1,c_2,\\ldots,c_n\\right)$ and $\\left(d_1,d_2,\\ldots,d_n\\right)$ are two observations from the `test set`. To predict whether these two observations refer to the same POI or not, we have to predict the corresponding `match` variable $y$. The general idea in this section is to construct a vector of features $\\left(z_1,z_2,\\ldots,z_m\\right)$, with $m \\leq n$, which captures the key information about $y$, contained in $\\left(c_1,c_2,\\ldots,c_n; d_1,d_2,\\ldots,d_n\\right)$. The goal of the current section is to produce a function that takes in a pair of observations, in the form of $\\left(a_1,a_2,\\ldots,a_n; b_1,b_2,\\ldots,b_n\\right)$, as an input and produce $\\left(z_1,z_2,\\ldots,z_m\\right)$ as an output, i.e.\n\n$$ \\left(a_1,a_2,\\ldots,a_n; b_1,b_2,\\ldots,b_n\\right) \\mapsto \\left(z_1,z_2,\\ldots,z_m\\right). $$","metadata":{}},{"cell_type":"code","source":"# Typical observation from the 'data_pairs_train'\ndata_pairs_train.iloc[0]","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:35:57.40488Z","iopub.execute_input":"2022-06-30T12:35:57.40547Z","iopub.status.idle":"2022-06-30T12:35:57.419393Z","shell.execute_reply.started":"2022-06-30T12:35:57.405438Z","shell.execute_reply":"2022-06-30T12:35:57.417964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It contains a pair of observations. The *id* of the two observations are expectedly different. The *name* is slightly different, as are the *latitude* and *longitude*. *Country* and *category* are identical. Some of the attributes are missing in one of the observations, while some are missing in both. The target variable here is `match`, which is a Boolean variable taking the value `True` if the two observations refer to the same POI and `False` otherwise. We observe that the number of features can be greatly reduced if we focus on the information that are relevant in predicting `match`, and discard the rest. We initiate a dataframe to extract and store these relevant information out of the attributes in `data_pairs`.","metadata":{}},{"cell_type":"code","source":"# Dataframe initialization for new features\ndf_train = pd.DataFrame()\ndf_test = pd.DataFrame()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:35:57.421192Z","iopub.execute_input":"2022-06-30T12:35:57.422157Z","iopub.status.idle":"2022-06-30T12:35:57.434041Z","shell.execute_reply.started":"2022-06-30T12:35:57.422109Z","shell.execute_reply":"2022-06-30T12:35:57.433209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Distance between Locations","metadata":{}},{"cell_type":"markdown","source":"The information that is relevant in predicting `match`, contained in `latitude_1`, `longitude_1`, `latitude_2` and `longitude_2`, can be encapsulted into a single variable, which is the distance `dist_loc` between the two locations (`latitude_1`, `longitude_1`) and (`latitude_2`, `longitude_2`), given by the haversine formula. Let $\\left(\\phi_1, \\lambda_1\\right)$ and $\\left(\\phi_2, \\lambda_2\\right)$ be the coordinates of two POIs. Then the distance between the two POIs is\n\n$$ d = 2r \\arcsin\\left(\\sqrt{\\text{hav}\\left(\\phi_1 - \\phi_2\\right) + \\text{cos}\\left(\\phi_1\\right) \\text{cos}\\left(\\phi_2\\right) \\text{hav}\\left(\\lambda_1 - \\lambda_2\\right)}\\right), $$\n\nwhere\n\n$$ \\text{hav}\\left(\\theta\\right) = \\sin^2\\left(\\frac{\\theta}{2}\\right) = \\frac{1-\\cos{\\theta}}{2}. $$","metadata":{}},{"cell_type":"code","source":"# Haversine formula\ndef dist(lat1, lon1, lat2, lon2):\n    \"\"\"\n    Computes the distance (over the surface) between two coordinates (lat1, lon1) and (lat2, lon2)\n    \n    Args:\n      lat1 (float): latitude of first location\n      lon1 (float): longitude of first location\n      lat2 (float): latitude of second location\n      lon2 (float): longitude of second location\n      \n    Returns:\n      d (float): distance between the two locations in km\n    \"\"\"\n    r = 6371\n    lat1, lon1, lat2, lon2 = np.radians(lat1), np.radians(lon1), np.radians(lat2), np.radians(lon2)\n    dlat, dlon = lat1 - lat2, lon1 - lon2\n    h = ((1 - cos(dlat)) / 2) + (cos(lat1) * cos(lat2) * ((1 - cos(dlon)) / 2))\n    d = 2 * r * asin(sqrt(h))\n    return d","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:35:57.435786Z","iopub.execute_input":"2022-06-30T12:35:57.436596Z","iopub.status.idle":"2022-06-30T12:35:57.449184Z","shell.execute_reply.started":"2022-06-30T12:35:57.436529Z","shell.execute_reply":"2022-06-30T12:35:57.448068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Distance between locations\ndist_loc_train = [dist(data_pairs_train['latitude_1'][i], data_pairs_train['longitude_1'][i], data_pairs_train['latitude_2'][i], data_pairs_train['longitude_2'][i]) for i in data_pairs_train.index]\ndist_loc_test = [dist(data_pairs_test['latitude_1'][i], data_pairs_test['longitude_1'][i], data_pairs_test['latitude_2'][i], data_pairs_test['longitude_2'][i]) for i in data_pairs_test.index]\ndf_train['dist_loc'] = dist_loc_train\ndf_test['dist_loc'] = dist_loc_test","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:35:57.450786Z","iopub.execute_input":"2022-06-30T12:35:57.451786Z","iopub.status.idle":"2022-06-30T12:36:15.597489Z","shell.execute_reply.started":"2022-06-30T12:35:57.451741Z","shell.execute_reply":"2022-06-30T12:36:15.596448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Histogram of 'dist_loc' for 'df_train' and 'df_test'\nfig, ax = plt.subplots(1, 2, figsize = (15, 6), sharey = True)\nsns.histplot(data = df_train, x = 'dist_loc', bins = 30, ax = ax[0])\nsns.histplot(data = df_test, x = 'dist_loc', bins = 30, ax = ax[1])\nax[0].set_title(\"df_train\", fontsize = 14)\nax[1].set_title(\"df_test\", fontsize = 14)\nax[1].set_ylabel(\"\")\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:36:15.603896Z","iopub.execute_input":"2022-06-30T12:36:15.604288Z","iopub.status.idle":"2022-06-30T12:36:16.224696Z","shell.execute_reply.started":"2022-06-30T12:36:15.604254Z","shell.execute_reply":"2022-06-30T12:36:16.223388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In both `df_train` and `df_test`, `dist_loc` is concentrated near $0$. It is likely that in both `df_train` and `df_test`, `dist_loc` is positively skewed to an extreme degree. To elaborate, [Skewness](https://en.wikipedia.org/wiki/Skewness) quantifies the asymmetry of a distribution about its mean. It is given by\n\n$$ g_1 := \\frac{\\frac{1}{n}\\sum_{i=1}^n\\left(x_i-\\bar{x}\\right)^3}{\\left[\\frac{1}{n}\\sum_{i=1}^n\\left(x_i-\\bar{x}\\right)^2\\right]^{3/2}}, $$\n\nwhere $\\bar{x}$ is the mean of the observations, given by $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i$. The measure $g_1$ can be negative, zero, positive. A value close to $0$ suggests that the distribution is more or less symmetric. However, as it deviates from $0$, it becomes more and more skewed (either positively or negatively). A positive skewness indicates that the distribution is concentrated towards the left side, with the longer tail being on the right side. A negative skewness indicates that the distribution is concentrated towards the right side, with the longer tail being on the left side. We back up the observation about skewness of `dist_loc` in `df_train` and `df_test` by computing the corresponding $g_1$ values.","metadata":{}},{"cell_type":"code","source":"# Skewness of 'dist_loc'\nprint(pd.Series({\"Skewness of 'dist_loc' in 'df_train'\": df_train['dist_loc'].skew(),\n                 \"Skewness of 'dist_loc' in 'df_test'\": df_test['dist_loc'].skew()}).to_string())","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:36:16.226347Z","iopub.execute_input":"2022-06-30T12:36:16.226711Z","iopub.status.idle":"2022-06-30T12:36:16.241143Z","shell.execute_reply.started":"2022-06-30T12:36:16.226678Z","shell.execute_reply":"2022-06-30T12:36:16.239913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To deal with the extreme skewness, we have applied the following transformation:\n\n$$ x \\mapsto log(x+\\epsilon), $$\n\nwhere $\\epsilon$ is a very small positive real number. Here we have taken $\\epsilon = 0.00000001$. The reason behind making this small shift to the data is that the log function maps $0$ to $-\\infty$. The shift keeps the transformed data finite, and keeping $\\epsilon$ small ensures that the data points which were originally $0$, stands out from the rest in the transformed setup. Visualizations of the distribution of both the original feature and the transformed feature have been shown.","metadata":{}},{"cell_type":"code","source":"# Applying the transformation\nepsilon = 0.00000001\ndf_train['dist_loc_transformed'] = df_train['dist_loc'].apply(lambda x: np.log(x + epsilon))\ndf_test['dist_loc_transformed'] = df_test['dist_loc'].apply(lambda x: np.log(x + epsilon))","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:36:16.243015Z","iopub.execute_input":"2022-06-30T12:36:16.244428Z","iopub.status.idle":"2022-06-30T12:36:16.927977Z","shell.execute_reply.started":"2022-06-30T12:36:16.244373Z","shell.execute_reply":"2022-06-30T12:36:16.926779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Deleting old features\ndf_train.drop('dist_loc', axis = 1, inplace = True)\ndf_test.drop('dist_loc', axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:36:16.929512Z","iopub.execute_input":"2022-06-30T12:36:16.929974Z","iopub.status.idle":"2022-06-30T12:36:16.944123Z","shell.execute_reply.started":"2022-06-30T12:36:16.929932Z","shell.execute_reply":"2022-06-30T12:36:16.94287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Histogram of 'dist_loc_transformed' for 'df_train' and 'df_test'\nfig, ax = plt.subplots(1, 2, figsize = (15, 6), sharey = True)\nsns.histplot(data = df_train, x = 'dist_loc_transformed', bins = 30, ax = ax[0])\nsns.histplot(data = df_test, x = 'dist_loc_transformed', bins = 30, ax = ax[1])\nax[0].set_title(\"df_train\", fontsize = 14)\nax[1].set_title(\"df_test\", fontsize = 14)\nax[1].set_ylabel(\"\")\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:36:16.947914Z","iopub.execute_input":"2022-06-30T12:36:16.948279Z","iopub.status.idle":"2022-06-30T12:36:17.516008Z","shell.execute_reply.started":"2022-06-30T12:36:16.948238Z","shell.execute_reply":"2022-06-30T12:36:17.514518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Features Based on Largest Common Substring","metadata":{}},{"cell_type":"markdown","source":"We observe that, in `data_pairs_train` and `data_pairs_test`, the name for the same POI varies in different records for variety of reasons. For example, some records may contain shortened versions of the names. Similar phenomenon is observed in the city, state and categories attribute. For these reasons, we create similarity features based on these attributes using the length of the largest common substring of two strings, relative to the length of the shorter string. Note that we have to convert some of the input data to string format before feeding it to the function that computes this quantity.","metadata":{}},{"cell_type":"code","source":"# Relative length proportion of largest common substring of two strings\ndef lcss(str1, str2):\n    \"\"\"\n    Computes length proportion of largest common substring of two strings, relative to the length of the shorter string\n\n    Args:\n      str1 (string): a general string\n      str2 (string): a general string\n\n    Returns:\n      prop (float): length of the largest common substring of two strings, scaled by length of the shorter string\n    \"\"\"\n    n1, n2 = len(str1), len(str2)\n    lc = [[0 for k in range(n2 + 1)] for l in range(n1 + 1)]\n    out = 0\n    for i in range(n1 + 1):\n        for j in range(n2 + 1):\n            if (i == 0 or j == 0):\n                lc[i][j] = 0\n            elif (str1[i-1] == str2[j-1]):\n                lc[i][j] = lc[i-1][j-1] + 1\n                out = max(out, lc[i][j])\n            else:\n                lc[i][j] = 0\n    prop = out / min(n1, n2)\n    return prop","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:36:17.517115Z","iopub.execute_input":"2022-06-30T12:36:17.517416Z","iopub.status.idle":"2022-06-30T12:36:17.525704Z","shell.execute_reply.started":"2022-06-30T12:36:17.517389Z","shell.execute_reply":"2022-06-30T12:36:17.524599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Relative length of largest common substring between names, cities, states and categories\nlcss_name_train, lcss_name_test = [], []\nlcss_city_train, lcss_city_test = [], []\nlcss_state_train, lcss_state_test = [], []\nlcss_categories_train, lcss_categories_test = [], []\nfor i in data_pairs_train.index:\n    lcss_name_train.append(lcss(data_pairs_train['name_1'][i], data_pairs_train['name_2'][i]))\n    lcss_city_train.append(lcss(data_pairs_train['city_1'][i], data_pairs_train['city_2'][i]))\n    lcss_state_train.append(lcss(data_pairs_train['state_1'][i], data_pairs_train['state_2'][i]))\n    lcss_categories_train.append(lcss(data_pairs_train['categories_1'][i], data_pairs_train['categories_2'][i]))\nfor i in data_pairs_test.index:\n    lcss_name_test.append(lcss(data_pairs_test['name_1'][i], data_pairs_test['name_2'][i]))\n    lcss_city_test.append(lcss(data_pairs_test['city_1'][i], data_pairs_test['city_2'][i]))\n    lcss_state_test.append(lcss(data_pairs_test['state_1'][i], data_pairs_test['state_2'][i]))\n    lcss_categories_test.append(lcss(data_pairs_test['categories_1'][i], data_pairs_test['categories_2'][i]))\ndf_train['lcss_name'], df_test['lcss_name'] = lcss_name_train, lcss_name_test\ndf_train['lcss_city'], df_test['lcss_city'] = lcss_city_train, lcss_city_test\ndf_train['lcss_state'], df_test['lcss_state'] = lcss_state_train, lcss_state_test\ndf_train['lcss_categories'], df_test['lcss_categories'] = lcss_categories_train, lcss_categories_test","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:36:17.527016Z","iopub.execute_input":"2022-06-30T12:36:17.527938Z","iopub.status.idle":"2022-06-30T12:39:36.143104Z","shell.execute_reply.started":"2022-06-30T12:36:17.5279Z","shell.execute_reply":"2022-06-30T12:39:36.141925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Histogram of 'lcss_name' for 'df_train' and 'df_test'\nfig, ax = plt.subplots(1, 2, figsize = (15, 6), sharey = True)\nsns.histplot(data = df_train, x = 'lcss_name', bins = 30, ax = ax[0])\nsns.histplot(data = df_test, x = 'lcss_name', bins = 30, ax = ax[1])\nax[0].set_title(\"df_train\", fontsize = 14)\nax[1].set_title(\"df_test\", fontsize = 14)\nax[1].set_ylabel(\"\")\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:39:36.14487Z","iopub.execute_input":"2022-06-30T12:39:36.145238Z","iopub.status.idle":"2022-06-30T12:39:36.859678Z","shell.execute_reply.started":"2022-06-30T12:39:36.145205Z","shell.execute_reply":"2022-06-30T12:39:36.858289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Histogram of 'lcss_city' for 'df_train' and 'df_test'\nfig, ax = plt.subplots(1, 2, figsize = (15, 6), sharey = True)\nsns.histplot(data = df_train, x = 'lcss_city', bins = 30, ax = ax[0])\nsns.histplot(data = df_test, x = 'lcss_city', bins = 30, ax = ax[1])\nax[0].set_title(\"df_train\", fontsize = 14)\nax[1].set_title(\"df_test\", fontsize = 14)\nax[1].set_ylabel(\"\")\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:39:36.862962Z","iopub.execute_input":"2022-06-30T12:39:36.863746Z","iopub.status.idle":"2022-06-30T12:39:37.425052Z","shell.execute_reply.started":"2022-06-30T12:39:36.863709Z","shell.execute_reply":"2022-06-30T12:39:37.424217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Histogram of 'lcss_state' for 'df_train' and 'df_test'\nfig, ax = plt.subplots(1, 2, figsize = (15, 6), sharey = True)\nsns.histplot(data = df_train, x = 'lcss_state', bins = 30, ax = ax[0])\nsns.histplot(data = df_test, x = 'lcss_state', bins = 30, ax = ax[1])\nax[0].set_title(\"df_train\", fontsize = 14)\nax[1].set_title(\"df_test\", fontsize = 14)\nax[1].set_ylabel(\"\")\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:39:37.426426Z","iopub.execute_input":"2022-06-30T12:39:37.426753Z","iopub.status.idle":"2022-06-30T12:39:37.981803Z","shell.execute_reply.started":"2022-06-30T12:39:37.426724Z","shell.execute_reply":"2022-06-30T12:39:37.980655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Histogram of 'lcss_categories' for 'df_train' and 'df_test'\nfig, ax = plt.subplots(1, 2, figsize = (15, 6), sharey = True)\nsns.histplot(data = df_train, x = 'lcss_categories', bins = 30, ax = ax[0])\nsns.histplot(data = df_test, x = 'lcss_categories', bins = 30, ax = ax[1])\nax[0].set_title(\"df_train\", fontsize = 14)\nax[1].set_title(\"df_test\", fontsize = 14)\nax[1].set_ylabel(\"\")\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:39:37.983422Z","iopub.execute_input":"2022-06-30T12:39:37.983892Z","iopub.status.idle":"2022-06-30T12:39:38.535228Z","shell.execute_reply.started":"2022-06-30T12:39:37.983845Z","shell.execute_reply":"2022-06-30T12:39:38.534151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Matching of Countries","metadata":{}},{"cell_type":"code","source":"# Matching of countries\ndf_train['match_country'] = [float(data_pairs_train['country_1'][i] == data_pairs_train['country_2'][i]) for i in data_pairs_train.index]\ndf_test['match_country'] = [float(data_pairs_test['country_1'][i] == data_pairs_test['country_2'][i]) for i in data_pairs_test.index]","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:39:38.53704Z","iopub.execute_input":"2022-06-30T12:39:38.537436Z","iopub.status.idle":"2022-06-30T12:39:45.724228Z","shell.execute_reply.started":"2022-06-30T12:39:38.537404Z","shell.execute_reply":"2022-06-30T12:39:45.723405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Donutplots of the 'match_country' column\nfig = make_subplots(rows = 1, cols = 2, specs = [[{'type': 'domain'}, {'type': 'domain'}]])\nx_val_train = df_train['match_country'].value_counts(sort = False).index.tolist()\ny_val_train = df_train['match_country'].value_counts(sort = False).tolist()\nx_val_test = df_test['match_country'].value_counts(sort = False).index.tolist()\ny_val_test = df_test['match_country'].value_counts(sort = False).tolist()\nfig.add_trace(go.Pie(values = y_val_train, labels = x_val_train, hole = 0.5, textinfo = 'label+percent', title = \"Train\"), row = 1, col = 1)\nfig.add_trace(go.Pie(values = y_val_test, labels = x_val_test, hole = 0.5, textinfo = 'label+percent', title = \"Test\"), row = 1, col = 2)\nfig.update_layout(height = 500, width = 800, showlegend = False, xaxis = dict(tickmode = 'linear', tick0 = 0, dtick = 1), title = dict(text = \"Frequency comparison of 'match_country'\", x = 0.5, y = 0.95)) \nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:39:45.725327Z","iopub.execute_input":"2022-06-30T12:39:45.726284Z","iopub.status.idle":"2022-06-30T12:39:45.771652Z","shell.execute_reply.started":"2022-06-30T12:39:45.726247Z","shell.execute_reply":"2022-06-30T12:39:45.770605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## The Target Variable","metadata":{}},{"cell_type":"code","source":"# 'match' column\ndf_train['match'] = [float(data_pairs_train['match'][i]) for i in data_pairs_train.index]\ndf_test['match'] = [float(data_pairs_test['match'][i]) for i in data_pairs_test.index]","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:39:45.773842Z","iopub.execute_input":"2022-06-30T12:39:45.774755Z","iopub.status.idle":"2022-06-30T12:39:49.571834Z","shell.execute_reply.started":"2022-06-30T12:39:45.774705Z","shell.execute_reply":"2022-06-30T12:39:49.570857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From this point on, we shall refer to `df_train` as the *effective training set*, or just `training set` and `df_test` as the *effective test set*, or just `test set`. We shall refer to the five-observations-strong `data_test` as the *true test set*.","metadata":{}},{"cell_type":"code","source":"# Effective training set\nprint(pd.Series({\"Memory usage\": \"{:.2f} MB\".format(df_train.memory_usage().sum()/(1024*1024)),\n                 \"Dataframe shape\": \"{}\".format(df_train.shape)}).to_string())\nprint(\" \")\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:39:49.573036Z","iopub.execute_input":"2022-06-30T12:39:49.574115Z","iopub.status.idle":"2022-06-30T12:39:49.593176Z","shell.execute_reply.started":"2022-06-30T12:39:49.574061Z","shell.execute_reply":"2022-06-30T12:39:49.592056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Effective test set\nprint(pd.Series({\"Memory usage\": \"{:.2f} MB\".format(df_test.memory_usage().sum()/(1024*1024)),\n                 \"Dataframe shape\": \"{}\".format(df_test.shape)}).to_string())\nprint(\" \")\ndf_test.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:39:49.594643Z","iopub.execute_input":"2022-06-30T12:39:49.595862Z","iopub.status.idle":"2022-06-30T12:39:49.622212Z","shell.execute_reply.started":"2022-06-30T12:39:49.595816Z","shell.execute_reply":"2022-06-30T12:39:49.621001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Baseline Modeling\n\n- [Logistic Regression](#Logistic-Regression)\n- [K-Nearest Neighbors (KNN)](#K-Nearest-Neighbors-(KNN))\n- [Decision Tree](#Decision-Tree)\n- [Naive Bayes](#Naive-Bayes)\n- [Random Forest](#Random-Forest)\n- [Linear Discriminant Analysis (LDA)](#Linear-Discriminant-Analysis-(LDA))\n- [Stochastic Gradient Descent (SGD)](#Stochastic-Gradient-Descent-(SGD))\n- [Ridge Classifier](#Ridge-Classifier)\n- [XGBoost](#XGBoost)\n- [AdaBoost](#AdaBoost)\n- [Summary](#Summary)","metadata":{}},{"cell_type":"markdown","source":"Each row in `df_train` or `df_test` are of the form $\\left(z_1,z_2,\\ldots,z_m\\right)$, which is essentially a vector representing the differences between two POI observations. Our accomplishment in this section can be summarized by the following transformation:\n\n$$ \\left(a_1,a_2,\\ldots,a_n; b_1,b_2,\\ldots,b_n; y\\right) \\mapsto \\left(z_1,z_2,\\ldots,z_m; y\\right). $$\n\nOnly for pairs generated from training observations, the `match` variable $y$ will be available. The task of modeling is to predict $y$ based on $\\left(z_1,z_2,\\ldots,z_m\\right)$, i.e.\n\n$$ \\left(z_1,z_2,\\ldots,z_m\\right) \\mapsto \\hat{y} \\,\\,(\\text{estimation of } y). $$\n\nThe general plan is to train models on `df_train`, evaluate them on `df_test` and finally apply the best one (according to appropriate evaluation metric) to each distinct, unordered pairs of observations from `data_test` to match the ids.","metadata":{}},{"cell_type":"code","source":"# Feature-target split\nX_train, y_train = df_train.drop('match', axis = 1), df_train['match']\nX_test, y_test = df_test.drop('match', axis = 1), df_test['match']","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:39:49.624736Z","iopub.execute_input":"2022-06-30T12:39:49.625045Z","iopub.status.idle":"2022-06-30T12:39:49.667499Z","shell.execute_reply.started":"2022-06-30T12:39:49.625017Z","shell.execute_reply":"2022-06-30T12:39:49.666465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to compute confusion matrix\ndef conf_mat(y_test, y_pred):\n    \"\"\"\n    Computes confusion matrix\n    \n    Args:\n      y_test (list/array/tuple/set/series): true binary (0/1) labels\n      y_pred (list/array/tuple/set/series): predicted binary (0/1) labels\n      \n    Returns:\n      confusion_mat (array): A 2D array representing a 2x2 confusion matrix\n    \"\"\"\n    y_test, y_pred = list(y_test), list(y_pred)\n    count, labels, confusion_mat = len(y_test), [0, 1], np.zeros(shape = (2, 2), dtype = int)\n    for i in range(2):\n        for j in range(2):\n            confusion_mat[i][j] = len([k for k in range(count) if y_test[k] == labels[i] and y_pred[k] == labels[j]])\n    return confusion_mat","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:39:49.66951Z","iopub.execute_input":"2022-06-30T12:39:49.670012Z","iopub.status.idle":"2022-06-30T12:39:49.677724Z","shell.execute_reply.started":"2022-06-30T12:39:49.669968Z","shell.execute_reply":"2022-06-30T12:39:49.6767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to print confusion matrix\ndef conf_mat_heatmap(y_test, y_pred):\n    \"\"\"\n    Prints confusion matrix\n    \n    Args:\n      y_test (list/array/tuple/set/series): true binary (0/1) labels\n      y_pred (list/array/tuple/set/series): predicted binary (0/1) labels\n      \n    Returns:\n      Nothing, prints a heatmap representing a 2x2 confusion matrix\n    \"\"\"\n    confusion_mat = conf_mat(y_test, y_pred)\n    labels, confusion_mat_df = [0, 1], pd.DataFrame(confusion_mat, range(2), range(2))\n    plt.figure(figsize = (6, 4.75))\n    # sns.set(font_scale = 1.4) # label size\n    sns.heatmap(confusion_mat_df, annot = True, annot_kws = {\"size\": 16}, fmt = 'd')\n    plt.xticks([0.5, 1.5], labels, rotation = 'horizontal')\n    plt.yticks([0.5, 1.5], labels, rotation = 'horizontal')\n    plt.xlabel(\"Predicted label\", fontsize = 14)\n    plt.ylabel(\"True label\", fontsize = 14)\n    plt.title(\"Confusion Matrix\", fontsize = 14)\n    plt.grid(False)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:39:49.679292Z","iopub.execute_input":"2022-06-30T12:39:49.679698Z","iopub.status.idle":"2022-06-30T12:39:49.696825Z","shell.execute_reply.started":"2022-06-30T12:39:49.679661Z","shell.execute_reply":"2022-06-30T12:39:49.695932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We shall use the *accuracy* measure as an evaluation metric to assess the baseline models. The measure is given by\n\n$$ \\text{Accuracy} = \\frac{\\text{Number of correct predictions}}{\\text{Number of total predictions}}. $$","metadata":{}},{"cell_type":"code","source":"# Function to compute accuracy\ndef accuracy(y_test, y_pred):\n    \"\"\"\n    Computes confusion matrix\n    \n    Args:\n      y_test (list/array/tuple/set/series): true binary (0/1) labels\n      y_pred (list/array/tuple/set/series): predicted binary (0/1) labels\n      \n    Returns:\n      acc (float): accuracy obtained from y_test and y_pred\n    \"\"\"\n    confusion_mat = conf_mat(y_test, y_pred)\n    num = confusion_mat[0, 0] + confusion_mat[1, 1]\n    denom = num + confusion_mat[0, 1] + confusion_mat[1, 0]\n    acc = num / denom\n    return acc","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:39:49.698315Z","iopub.execute_input":"2022-06-30T12:39:49.69878Z","iopub.status.idle":"2022-06-30T12:39:49.710828Z","shell.execute_reply.started":"2022-06-30T12:39:49.698738Z","shell.execute_reply":"2022-06-30T12:39:49.709371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initiating the summary dictionary\ndict_baseline = {}","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:39:49.712018Z","iopub.execute_input":"2022-06-30T12:39:49.712465Z","iopub.status.idle":"2022-06-30T12:39:49.729179Z","shell.execute_reply.started":"2022-06-30T12:39:49.712423Z","shell.execute_reply":"2022-06-30T12:39:49.727702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Logistic Regression","metadata":{}},{"cell_type":"code","source":"%%time\n# Logistic regression\nlogreg = LogisticRegression(max_iter = 1000)\nprint(f\"Model: {logreg}\")\ncv = RepeatedStratifiedKFold(n_splits = 6, n_repeats = 5, random_state = 1)\nprint(f\"Training accuracy (mean of cross validation scores): {cross_val_score(logreg, X_train, y_train, cv = cv, scoring = 'accuracy').mean()}\")\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_test)\nscore = accuracy(y_test, y_pred)\nprint(f\"Test accuracy: {score}\")\ndict_baseline['Logistic Regression'] = score\nconf_mat_heatmap(y_test, y_pred)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:39:49.730664Z","iopub.execute_input":"2022-06-30T12:39:49.731131Z","iopub.status.idle":"2022-06-30T12:40:52.281661Z","shell.execute_reply.started":"2022-06-30T12:39:49.731088Z","shell.execute_reply":"2022-06-30T12:40:52.280386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## K-Nearest Neighbors (KNN)","metadata":{}},{"cell_type":"code","source":"%%time\n# KNN\nk = math.floor(np.sqrt(len(y_test))) + 1\nknn = KNeighborsClassifier(n_neighbors = k, n_jobs = -1)\nprint(f\"Model: {knn}\")\ncv = RepeatedStratifiedKFold(n_splits = 6, n_repeats = 5, random_state = 1)\nprint(f\"Training accuracy (mean of cross validation scores): {cross_val_score(knn, X_train, y_train, cv = cv, scoring = 'accuracy').mean()}\")\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\nscore = accuracy(y_test, y_pred)\nprint(f\"Test accuracy: {score}\")\ndict_baseline['KNN'] = score\nconf_mat_heatmap(y_test, y_pred)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:40:52.284517Z","iopub.execute_input":"2022-06-30T12:40:52.285561Z","iopub.status.idle":"2022-06-30T12:50:35.416386Z","shell.execute_reply.started":"2022-06-30T12:40:52.285467Z","shell.execute_reply":"2022-06-30T12:50:35.414646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Decision Tree","metadata":{}},{"cell_type":"code","source":"%%time\n# Decision tree\ndt = DecisionTreeClassifier()\nprint(f\"Model: {dt}\")\ncv = RepeatedStratifiedKFold(n_splits = 6, n_repeats = 5, random_state = 1)\nprint(f\"Training accuracy (mean of cross validation scores): {cross_val_score(dt, X_train, y_train, cv = cv, scoring = 'accuracy').mean()}\")\ndt.fit(X_train, y_train)\ny_pred = dt.predict(X_test)\nscore = accuracy(y_test, y_pred)\nprint(f\"Test accuracy: {score}\")\ndict_baseline['Decision Tree'] = score\nconf_mat_heatmap(y_test, y_pred)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:50:35.419649Z","iopub.execute_input":"2022-06-30T12:50:35.420191Z","iopub.status.idle":"2022-06-30T12:52:09.415559Z","shell.execute_reply.started":"2022-06-30T12:50:35.420128Z","shell.execute_reply":"2022-06-30T12:52:09.414333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Naive Bayes","metadata":{}},{"cell_type":"code","source":"%%time\n# Naive Bayes\nnb = GaussianNB()\nprint(f\"Model: {nb}\")\ncv = RepeatedStratifiedKFold(n_splits = 6, n_repeats = 5, random_state = 1)\nprint(f\"Training accuracy (mean of cross validation scores): {cross_val_score(nb, X_train, y_train, cv = cv, scoring = 'accuracy').mean()}\")\nnb.fit(X_train, y_train)\ny_pred = nb.predict(X_test)\nscore = accuracy(y_test, y_pred)\nprint(f\"Test accuracy: {score}\")\ndict_baseline['Naive Bayes'] = score\nconf_mat_heatmap(y_test, y_pred)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:52:09.417404Z","iopub.execute_input":"2022-06-30T12:52:09.417969Z","iopub.status.idle":"2022-06-30T12:52:14.782303Z","shell.execute_reply.started":"2022-06-30T12:52:09.417922Z","shell.execute_reply":"2022-06-30T12:52:14.780954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Random Forest","metadata":{}},{"cell_type":"code","source":"%%time\n# Random forest\nrf = RandomForestClassifier(n_estimators = 100)\nprint(f\"Model: {rf}\")\ncv = RepeatedStratifiedKFold(n_splits = 6, n_repeats = 5, random_state = 1)\nprint(f\"Training accuracy (mean of cross validation scores): {cross_val_score(rf, X_train, y_train, cv = cv, scoring = 'accuracy').mean()}\")\nrf.fit(X_train, y_train)\ny_pred = rf.predict(X_test)\nscore = accuracy(y_test, y_pred)\nprint(f\"Test accuracy: {score}\")\ndict_baseline['Random Forest'] = score\nconf_mat_heatmap(y_test, y_pred)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:52:14.783742Z","iopub.execute_input":"2022-06-30T12:52:14.784056Z","iopub.status.idle":"2022-06-30T13:38:26.444059Z","shell.execute_reply.started":"2022-06-30T12:52:14.784024Z","shell.execute_reply":"2022-06-30T13:38:26.442751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Linear Discriminant Analysis (LDA)","metadata":{}},{"cell_type":"code","source":"%%time\n# Linear discriminant analysis\nlda = LinearDiscriminantAnalysis()\nprint(f\"Model: {lda}\")\ncv = RepeatedStratifiedKFold(n_splits = 6, n_repeats = 5, random_state = 1)\nprint(f\"Training accuracy (mean of cross validation scores): {cross_val_score(lda, X_train, y_train, cv = cv, scoring = 'accuracy').mean()}\")\nlda.fit(X_train, y_train)\ny_pred = lda.predict(X_test)\nscore = accuracy(y_test, y_pred)\nprint(f\"Test accuracy: {score}\")\ndict_baseline['LDA'] = score\nconf_mat_heatmap(y_test, y_pred)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:38:26.44632Z","iopub.execute_input":"2022-06-30T13:38:26.446787Z","iopub.status.idle":"2022-06-30T13:38:42.733438Z","shell.execute_reply.started":"2022-06-30T13:38:26.446741Z","shell.execute_reply":"2022-06-30T13:38:42.732163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Stochastic Gradient Descent (SGD)","metadata":{}},{"cell_type":"code","source":"%%time\n# Stochastic gradient descent\nsgd = SGDClassifier(loss = 'hinge')\nprint(f\"Model: {sgd}\")\ncv = RepeatedStratifiedKFold(n_splits = 6, n_repeats = 5, random_state = 1)\nprint(f\"Training accuracy (mean of cross validation scores): {cross_val_score(sgd, X_train, y_train, cv = cv, scoring = 'accuracy').mean()}\")\nsgd.fit(X_train, y_train)\ny_pred = sgd.predict(X_test)\nscore = accuracy(y_test, y_pred)\nprint(f\"Test accuracy: {score}\")\ndict_baseline['SGD'] = score\nconf_mat_heatmap(y_test, y_pred)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:38:42.735292Z","iopub.execute_input":"2022-06-30T13:38:42.735647Z","iopub.status.idle":"2022-06-30T13:39:19.4178Z","shell.execute_reply.started":"2022-06-30T13:38:42.735615Z","shell.execute_reply":"2022-06-30T13:39:19.4166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ridge Classifier","metadata":{}},{"cell_type":"code","source":"%%time\n# Ridge\nridge = RidgeClassifier()\nprint(f\"Model: {ridge}\")\ncv = RepeatedStratifiedKFold(n_splits = 6, n_repeats = 5, random_state = 1)\nprint(f\"Training accuracy (mean of cross validation scores): {cross_val_score(ridge, X_train, y_train, cv = cv, scoring = 'accuracy').mean()}\")\nridge.fit(X_train, y_train)\ny_pred = ridge.predict(X_test)\nscore = accuracy(y_test, y_pred)\nprint(f\"Test accuracy: {score}\")\ndict_baseline['Ridge'] = score\nconf_mat_heatmap(y_test, y_pred)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:39:19.419721Z","iopub.execute_input":"2022-06-30T13:39:19.420413Z","iopub.status.idle":"2022-06-30T13:39:27.886266Z","shell.execute_reply.started":"2022-06-30T13:39:19.42037Z","shell.execute_reply":"2022-06-30T13:39:27.885451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## XGBoost","metadata":{}},{"cell_type":"code","source":"%%time\n# XGBoost\nxgb = XGBClassifier()\nprint(f\"Model: {xgb}\")\ncv = RepeatedStratifiedKFold(n_splits = 6, n_repeats = 5, random_state = 1)\nprint(f\"Training accuracy (mean of cross validation scores): {cross_val_score(xgb, X_train, y_train, cv = cv, scoring = 'accuracy').mean()}\")\nxgb.fit(X_train, y_train)\ny_pred = xgb.predict(X_test)\nscore = accuracy(y_test, y_pred)\nprint(f\"Test accuracy: {score}\")\ndict_baseline['XGBoost'] = score\nconf_mat_heatmap(y_test, y_pred)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:39:27.887892Z","iopub.execute_input":"2022-06-30T13:39:27.888247Z","iopub.status.idle":"2022-06-30T13:50:13.104448Z","shell.execute_reply.started":"2022-06-30T13:39:27.888215Z","shell.execute_reply":"2022-06-30T13:50:13.103304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## AdaBoost","metadata":{}},{"cell_type":"code","source":"%%time\n# AdaBoost\nada = AdaBoostClassifier()\nprint(f\"Model: {ada}\")\ncv = RepeatedStratifiedKFold(n_splits = 6, n_repeats = 5, random_state = 1)\nprint(f\"Training accuracy (mean of cross validation scores): {cross_val_score(ada, X_train, y_train, cv = cv, scoring = 'accuracy').mean()}\")\nada.fit(X_train, y_train)\ny_pred = ada.predict(X_test)\nscore = accuracy(y_test, y_pred)\nprint(f\"Test accuracy: {score}\")\ndict_baseline['AdaBoost'] = score\nconf_mat_heatmap(y_test, y_pred)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:50:13.106107Z","iopub.execute_input":"2022-06-30T13:50:13.107149Z","iopub.status.idle":"2022-06-30T13:57:24.224779Z","shell.execute_reply.started":"2022-06-30T13:50:13.107114Z","shell.execute_reply":"2022-06-30T13:57:24.223519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Summary","metadata":{}},{"cell_type":"code","source":"# Summary of the baseline models\ndf_baseline = pd.DataFrame(dict_baseline.items(), columns = ['Classifier', 'Accuracy'])\ndf_baseline.sort_values(by = 'Accuracy', ascending = False)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:57:24.226687Z","iopub.execute_input":"2022-06-30T13:57:24.229009Z","iopub.status.idle":"2022-06-30T13:57:24.248715Z","shell.execute_reply.started":"2022-06-30T13:57:24.228953Z","shell.execute_reply":"2022-06-30T13:57:24.247154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- **XGBoost**, **AdaBoost** (*boosting algorithms*) and **KNN** perform relatively well.\n\n- **Decision Tree**, **Random Forest** (*tree-based algorithms*) and **Naive Bayes** do not work well, compared to the other models. However, these models actually do a better job than the other models in the `0`-class, i.e. the class of pairs for which the true value of `match` is `False` (or `0.0` when converted to float).","metadata":{}},{"cell_type":"markdown","source":"# Acknowledgements\n\n- [Foursquare - Location Matching](https://www.kaggle.com/competitions/foursquare-location-matching) competition","metadata":{}},{"cell_type":"markdown","source":"# References\n\n- [Case sensitivity](https://en.wikipedia.org/wiki/Case_sensitivity)\n- [Correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient)\n- [Foursquare City Guide](https://en.wikipedia.org/wiki/Foursquare_City_Guide)\n- [Foursquare Labs Inc.](https://foursquare.com/)\n- [Foursquare Swarm](https://en.wikipedia.org/wiki/Foursquare_Swarm)\n- [Grove Karl Gilbert](https://en.wikipedia.org/wiki/Grove_Karl_Gilbert)\n- [Haversine formula](https://en.wikipedia.org/wiki/Haversine_formula)\n- [ISO 3166-1 alpha-2](https://en.wikipedia.org/wiki/ISO_3166-1_alpha-2)\n- [Jaccard index](https://en.wikipedia.org/wiki/Jaccard_index)\n- [Jaccard/Tanimoto similarity test and estimation methods for biological presence-absence data](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-019-3118-5)\n- [Jaccard/Tanimoto similarity test and estimation methods (arxiv version)](https://arxiv.org/abs/1903.11372)\n- [Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance)\n- [List of U.S. state and territory abbreviations](https://en.wikipedia.org/wiki/List_of_U.S._state_and_territory_abbreviations)\n- [Olympus Mons](https://en.wikipedia.org/wiki/Olympus_Mons)\n- [Paul Jaccard](https://en.wikipedia.org/wiki/Paul_Jaccard)\n- [Point of Interest](https://en.wikipedia.org/wiki/Point_of_interest)\n- [Skewness](https://en.wikipedia.org/wiki/Skewness)\n- [Vladimir Levenshtein](https://en.wikipedia.org/wiki/Vladimir_Levenshtein)","metadata":{}},{"cell_type":"code","source":"# Runtime and memory usage\nstop = time.time()\nprint(pd.Series({\"Process runtime\": \"{:.2f} seconds\".format(float(stop - start)),\n                 \"Process memory usage\": \"{:.2f} MB\".format(float(process.memory_info()[0]/(1024*1024)))}).to_string())","metadata":{"execution":{"iopub.status.busy":"2022-06-30T13:57:24.254099Z","iopub.execute_input":"2022-06-30T13:57:24.254837Z","iopub.status.idle":"2022-06-30T13:57:24.267018Z","shell.execute_reply.started":"2022-06-30T13:57:24.254794Z","shell.execute_reply":"2022-06-30T13:57:24.265393Z"},"trusted":true},"execution_count":null,"outputs":[]}]}