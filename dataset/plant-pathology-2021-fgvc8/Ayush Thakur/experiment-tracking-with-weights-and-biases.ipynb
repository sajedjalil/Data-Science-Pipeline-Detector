{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<img src=\"https://i.imgur.com/gb6B4ig.png\" width=\"400\" alt=\"Weights & Biases\" />\n\n# What is W&B?\n\nWeights & Biases (W&B) is a set of machine learning tools that helps you build better models faster.\n\nI'll use this Knowledge-based competition ([Plant Pathology 2021 - FGVC8](https://www.kaggle.com/c/plant-pathology-2021-fgvc8)) to demonstrate some of its features: **Dashboard** (experiment tracking) and **Artifacts** (dataset and model versioning). \n\n## Why is W&B useful?\n\n**Kaggle competitions require fast-paced model development and evaluation**. There are a lot of components: exploring the training data, training different models, combining trained models in different combinations (ensembling), and so on.\n\n> ‚è≥ Lots of components = lots of places to go wrong = lots of time spent debugging \n\nYou might miss important details and have to retrain your model, or you might train on the wrong data (information leakage). Or, you might use the wrong model for generating submission. \n\nThis is where W&B comes in:\n* **Dashboard** (experiment tracking): Log and visualize experiments in real time = Keep data and results in one convenient place. Consider this as a repository of experiments. \n* **Artifacts** (dataset + model versioning): Store and version datasets, models, and results = Know exactly what data a model is being trained on. ","metadata":{}},{"cell_type":"markdown","source":"# üñ• Dashboard (experiment tracking)\n\nUse the Dashboard as a central place to organize and visualize results from your machine learning models.\n\n* Track metrics\n* Visualize results\n* Train anywhere\n* Stay organized\n\n> üôå **Save everything in one place and never lose your progress again** üôå\n\n## Track metrics\n\nTrack model performance in real time and identify problem areas immediately.\n\n## Visualize results\n\nW&B supports a large variety of media types - visualize graphs, images, videos, audio, 3D objects, and [more](https://docs.wandb.ai/guides/track/log#logging-objects).\n\nPlus, the dashboard is **interactive** - hover for more options and information.\n\n![Imgur](https://i.imgur.com/xW4cOSx.gif)\n\n## Train anywhere\n\nThe W&B dashboard is **centralized** -  whether you're training on a local machine, lab cluster, or spot instances in the cloud, all of your results get logged to a single place.\n\n![centralized dashboard](https://i.imgur.com/BGgfZj3.png)\n\n## Stay organized\n\nW&B logs data into powerful, querably tables that you can search, filter, sort, and group. This makes it easy to compare thousands of different models and find the best performing model for different tasks. \n\n![image.png](https://i.imgur.com/qPlykIn.png)\n\n# üóÇ Artifacts (dataset versioning)\n\nUse W&B Artifacts to track and version your datasets, models, dependencies, and results across machine learning pipelines.\n\nThink of an Artifact as a versioned folder of data. Once you've saved something as an Artifact, all modifications are automatically logged, giving you a complete history of changes.\n\n> üôå **Keep track of which model was trained on which data** üôå\n\n\n### Manage your pipeline / Track data lineage\n\nYou get a bird eye view on your entire machine learning workflow. Collaborating with your teammates is easier with such a holistic view of your pipeline.\n\n![img](https://i.imgur.com/dhntZxK.png)\n\n# ‚ö°Ô∏è TL;DR\n\nWeights & Biases helps you **build better models faster**.\n\n* Visualize results in real time in an interactive, centralized dashboard\n* Identify how changing data affects the resulting model\n\nPlus, W&B is fast ([get started with 6 lines of code](https://docs.wandb.ai/quickstart)) and flexible ([integrated with every major ML framework](https://docs.wandb.ai/guides/integrations)).\n\nHere's a quick (1:42) intro:\n\n[![thumbnail](https://i.imgur.com/oriwd9L.png)](https://www.youtube.com/watch?v=EIgoKitLUqM&t=92s)\n","metadata":{}},{"cell_type":"markdown","source":"# üß∞ 1. Set up W&B\n\nW&B is very easy to set up and integrate.","metadata":{}},{"cell_type":"markdown","source":"## üîµ 1a. Install `wandb`\n\n`wandb` (the W&B library), comes baked into your Kaggle kernels!\n\nHowever, since `wandb` is rapidly improving, I recommend `pip install`-ing the latest version with the `--upgrade` and `-q` flags.","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade -q wandb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## üîµ 1b. Import `wandb` and log in\n\nYou will need a unique API key to log in to Weights & Biases. \n\n1. If you don't have a Weights & Biases account, you can go to https://wandb.ai/site and create a FREE account.\n2. Access your API key: https://wandb.ai/authorize.\n\nThere are two ways you can login using a Kaggle kernel:\n\n1. Run a cell with `wandb.login()`. It will ask for the API key, which you can copy + paste in.\n2. You can also use Kaggle secrets to store your API key and use the code snippet below to login. Check out this [discussion post](https://www.kaggle.com/product-feedback/114053) to learn more about Kaggle secrets. \n\n```\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\n\n# I have saved my API token with \"wandb_api\" as Label. \n# If you use some other Label make sure to change the same below. \nwandb_api = user_secrets.get_secret(\"wandb_api\") \n\nwandb.login(key=wandb_api)\n```\nMore on W&B login [here](https://docs.wandb.ai/ref/cli/wandb-login).","metadata":{}},{"cell_type":"code","source":"import wandb\nfrom wandb.keras import WandbCallback\n\nwandb.login()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> üìå `WandbCallback` is a lightweight Keras callback that I'll use later on.","metadata":{}},{"cell_type":"code","source":"# 1. Import other dependencies\n\nimport tensorflow as tf\nprint(tf.__version__)\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import models\nimport tensorflow_addons as tfa\n\nimport os\nimport json\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 2. Set the random seeds\n\ndef seed_everything():\n    os.environ['TF_CUDNN_DETERMINISTIC'] = '1' \n    np.random.seed(hash(\"improves reproducibility\") % 2**32 - 1)\n    tf.random.set_seed(hash(\"by removing stochasticity\") % 2**32 - 1)\n    \nseed_everything()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 3. Set hyperparameters\n\nTRAIN_PATH = '../input/resized-plant2021/img_sz_256/'\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n\nCONFIG = dict (\n    num_labels = 6,\n    train_val_split = 0.2,\n    img_width = 224,\n    img_height = 224,\n    batch_size = 64,\n    epochs = 10,\n    learning_rate = 0.001,\n    architecture = \"CNN\",\n    infra = \"Kaggle\",\n    competition = 'plant-pathology',\n    _wandb_kernel = 'ayut'\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> üìå: Store your hyperparameters as a dictionary, because you can later directly log this config dict to W&B.","metadata":{}},{"cell_type":"markdown","source":"# üî® 2. Build Input Pipeline\n\nAfter importing `wandb` and other dependencies, you can set up an input pipeline as normal. The Plant Pathology 2021 competition is about multi-label classification. \n\n> The main objective of the competition is to develop machine learning-based models to accurately classify a given leaf image from the test dataset to a particular disease category, and to identify an individual disease from multiple disease symptoms on a single leaf image.\n\nTo build the input pipeline I have used `tf.data` API. ","metadata":{}},{"cell_type":"code","source":"# 4. Build input pipeline\n\n# Encode competition-provided labels \nlabel_to_id = {\n    'healthy': 0,\n    'scab': 1,\n    'frog_eye_leaf_spot': 2,\n    'rust': 3,\n    'complex': 4,\n    'powdery_mildew': 5\n}\n\nid_to_label = {value:key for key, value in label_to_id.items()} \n\n# Helper fu\ndef make_path(row):\n    return TRAIN_PATH+row.image\n\ndef parse_labels(row):\n    label_list = row.labels.split()\n    labels = []\n    for label in label_list:\n        labels.append(str(label_to_id[label]))\n    \n    return ' '.join(labels)\n\n# Read train.csv file\ndf = pd.read_csv('../input/plant-pathology-2021-fgvc8/train.csv')\n# Get absolute path\ndf['image'] = df.apply(lambda row: make_path(row), axis=1)\n# Parse labels\ndf['labels'] = df.apply(lambda row: parse_labels(row), axis=1)\n\n# Look at the dataframe\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 5. Training and validation split\n\ntrain_df, valid_df = train_test_split(df, test_size=CONFIG['train_val_split'])\nprint(f'Number of train images: {len(train_df)} and validation images: {len(valid_df)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 6. Helper functions for input pipeline\n\n@tf.function\ndef decode_image(image):\n    # Convert the compressed string to a 3D uint8 tensor\n    image = tf.image.decode_jpeg(image, channels=3)\n    \n    # Normalize image\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    \n    # Resize the image to the desired size\n    return image\n\n@tf.function\ndef load_image(df_dict):\n    # Load image\n    image = tf.io.read_file(df_dict['image'])\n    image = decode_image(image)\n    \n    # Resize image\n    image = tf.image.resize(image, (CONFIG['img_height'], CONFIG['img_width']))\n    \n    # Parse label\n    label = tf.strings.split(df_dict['labels'], sep='')\n    label = tf.strings.to_number(label, out_type=tf.int32)\n    label = tf.reduce_sum(tf.one_hot(indices=label, depth=CONFIG['num_labels']), axis=0)\n    \n    return image, label","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 7. Build data loaders\n\nAUTOTUNE = tf.data.AUTOTUNE\n\ntrainloader = tf.data.Dataset.from_tensor_slices(dict(train_df))\nvalidloader = tf.data.Dataset.from_tensor_slices(dict(valid_df))\n\ntrainloader = (\n    trainloader\n    .shuffle(1024)\n    .map(load_image, num_parallel_calls=AUTOTUNE)\n    .batch(CONFIG['batch_size'])\n    .prefetch(AUTOTUNE)\n)\n\nvalidloader = (\n    validloader\n    .map(load_image, num_parallel_calls=AUTOTUNE)\n    .batch(CONFIG['batch_size'])\n    .prefetch(AUTOTUNE)\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data loader sanity check\n\ndef show_batch(image_batch, label_batch):\n    plt.figure(figsize=(20,20))\n    for n in range(25):\n        ax = plt.subplot(5,5,n+1)\n        plt.imshow(image_batch[n])\n        plt.title(' '.join([id_to_label[i] for i, label in enumerate(label_batch[n].numpy()) if label==1.]))\n        plt.axis('off')\n\nimage_batch, label_batch = next(iter(trainloader))\nshow_batch(image_batch, label_batch)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üî® 3. Set up your model\n\nBuild your model definition next. Since it's a multi-label classification task the output activation is `sigmoid`. ","metadata":{}},{"cell_type":"code","source":"# 8. Define model: EfficientNetB0 trained on ImageNet as backbone\n\ndef get_model():\n    base_model = tf.keras.applications.EfficientNetB0(include_top=False, weights='imagenet')\n    base_model.trainabe = True\n\n    inputs = layers.Input((CONFIG['img_height'], CONFIG['img_width'], 3))\n    x = base_model(inputs, training=True)\n    x = layers.GlobalAveragePooling2D()(x)\n    x = layers.Dropout(0.5)(x)\n    outputs = layers.Dense(len(label_to_id), activation='sigmoid')(x)\n    \n    return models.Model(inputs, outputs)\n\n# Model sanity check\ntf.keras.backend.clear_session()\nmodel = get_model()\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" ","metadata":{}},{"cell_type":"markdown","source":"# üöÑ 3. Train and evaluate with W&B\n\nIn this section I'll use:\n\n* [`wandb.init()`](https://docs.wandb.ai/guides/track/launch): Initialize a new run.\n\n* [`wandb.finish()`](https://docs.wandb.ai/ref/python/finish): Finish and close a run.\n\n* [`wandb.config`](https://docs.wandb.ai/guides/track/config): An object that stores hyperparameters and settings related to a run.\n\n\nA run (or [wandb.Run object](https://docs.wandb.ai/ref/python/run)) is a W&B unit of computation, typically an ML experiment.","metadata":{}},{"cell_type":"code","source":"# Initialize model\ntf.keras.backend.clear_session()\nmodel = get_model()\n\n# Compile model\noptimizer = tf.keras.optimizers.Adam(learning_rate=CONFIG['learning_rate'])\nmodel.compile(optimizer, \n              loss=tfa.losses.SigmoidFocalCrossEntropy(), \n              metrics=[tf.keras.metrics.AUC(multi_label=True), tfa.metrics.F1Score(num_classes=6, average='micro')])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## üîµ 3. Use `wandb.init()` to initialize a new W&B run.\n\nIn an ML training pipeline, you could add `wandb.init()` to the beginning of your training script as well as your evaluation script, and each piece would be tracked as a run in W&B.\n\nTake a note of these arguments. \n\n* `entity`: An entity is a username or team name where you're sending runs. \n* `project`: The name of the project where you're sending the new run. If the project is not specified, the run is put in an \"Uncategorized\" project.\n* `config`: This sets wandb.config, a dictionary-like object for saving inputs to your job, like hyperparameters for a model or settings for a data preprocessing job. \n* `group`: Specify a group to organize individual runs into a larger experiment.This is a super handy feature. For example, you can create group for different model architecture names. \n* `job_type`: Specify the type of run, which is useful when you're grouping runs together into larger experiments using group. Typical job types are \"train\", \"evaluate\", etc. ","metadata":{}},{"cell_type":"code","source":"# Update CONFIG dict with the name of the model.\nCONFIG['model_name'] = 'efficientnetb0'\nprint('Training configuration: ', CONFIG)\n\n# Initialize W&B run\nrun = wandb.init(project='plant-pathology', \n                 config=CONFIG,\n                 group='EfficientNet', \n                 job_type='train')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I used these `wandb.init()` arguments:\n\n* `project`: This argument specifies the name of the W&B project where the run gets sent to. Here, I'm creating a new project called `'plant-pathology'` and sending the run there at the same time.\n\n* `config`: This argument sets `wandb.config`, a dictionary-like object that stores hyperparameters, input settings, and other independent variables.\n\n    As a reminder, here's what's currently in `CONFIG`:\n\n```python\nCONFIG = dict (\n    num_labels = 6,\n    train_val_split = 0.2,\n    img_width = 224,\n    img_height = 224,\n    batch_size = 64,\n    epochs = 10,\n    learning_rate = 0.001,\n    architecture = \"CNN\",\n    infra = \"Kaggle\",\n    model_name = \"efficientnetb0\"\n)\n```\n\n* `group`: This argument specifies a value to group individual runs by. Later on, I'll have a run that groups by `'EfficientNet'`, so that I can compare runs from the different architectures more easily.\n* `job_type`: This argument specifies a run type, for example `'train'`, or `'evaluate'`. Setting a run type makes it easier to later filter and group runs, for example if you want to compare multiple `'train'` runs.","metadata":{}},{"cell_type":"markdown","source":"## üîµ 3b. Update `wandb.config`\n\nSaving your training configuration is useful for analyzing your experiments and reproducing your work later. With W&B, you can also group by config values, meaning that you can compare the settings of different runs and see how they affect the output.\n\nThere are multiple ways to set up `wandb.config`:\n\n* Set `wandb.config` with the `wandb.init(config)` argument, as above in (3a). \n* Set `wandb.config` directly.\n* See more setup options in this [Colab](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/wandb-log/Configs_in_W%26B.ipynb#scrollTo=xFf3zjBSixC1).","metadata":{}},{"cell_type":"code","source":"# Add \"type\" and \"kaggle_competition\" to `wandb.config` directly\nwandb.config.type = 'baseline'\nwandb.config.kaggle_competition = 'Plant Pathology 2021 - FGVC8'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here's a gif showing how W&B logs and displays a run's training configuration:\n\n![img](https://i.imgur.com/WxLjIBx.gif)\n\nNow I'll use `WandbCallback()`, the lightweight Keras integration that I mentioned earlier.\n\nThis callback automatically saves all the metrics and the loss values tracked in `model.fit()`. Check out our [Keras integration docs](https://docs.wandb.ai/guides/integrations/keras) to learn more.","metadata":{}},{"cell_type":"markdown","source":"Weights and Biases comes with a light weight integration for Keras. We will be using W&B Keras integration (`WandbCallback()`) to automatically save all the metrics and the loss values tracked in `model.fit()`. Check out the [docs to learn more about this integration](https://docs.wandb.ai/guides/integrations/keras). ","metadata":{}},{"cell_type":"code","source":"earlystopper = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', patience=3, verbose=0, mode='min',\n    restore_best_weights=True\n)\n\n# Train\nmodel.fit(trainloader, \n          epochs=CONFIG['epochs'],\n          validation_data=validloader,\n          callbacks=[WandbCallback(),\n                     earlystopper])\n\n# Close W&B run\nrun.finish()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> üìå Pro tip: Head over to the W&B dashboard my clicking on the link generated above. \n\n> üìå Pro tip: If you want to silence W&B related logs use this code snippet `os.environ[WANDB_SILENT] = \"true\"` after `import os`. Check out this [Stackoverflow answer](https://stackoverflow.com/a/65997094/8663152) for more details. \n\n> üìå Pro tip: Use `run.finish()` to close the initialized W&B run after a `job_type` is finished. ","metadata":{}},{"cell_type":"markdown","source":"Each run gets its own Run Page, which has tabs that contain more information about the run.\n\nHere's a gif showing the [Charts Tab](https://docs.wandb.ai/ref/app/pages/run-page#charts-tab), [System Tab](https://docs.wandb.ai/ref/app/pages/run-page#system-tab), and [Model Tab](https://docs.wandb.ai/ref/app/pages/run-page#model-tab) from the run above:\n\n![img](https://i.imgur.com/Eq8X9RN.gif)\n\nYou can click through this Run Page [here](https://wandb.ai/ayush-thakur/plant-pathology/runs/5yq0kz7t).\n\n### Filtering and Grouping\n\nYou can use filter and group feature on W&B dashboard to either hide crashed run, group together multiple runs under one experiment, group according to job_type, select runs that satisfy a condition, etc. You can learn more about Group feature [here](https://docs.wandb.ai/guides/track/advanced/grouping).\n\n![img](https://i.imgur.com/BeYKbfS.gif)","metadata":{}},{"cell_type":"markdown","source":"## üîµ 3c. Log evaluation score with `wandb.log()`\n\n`WandbCallback()` can be used for `model.fit()`, but not for `model.evaluate()`. If you want to log metrics from evaluation, you should call `wandb.log()`.\n\n* [`wandb.log()`](https://docs.wandb.ai/guides/track/log): Log a dict of scalars (metrics like accuracy and loss) and any other type of [`wandb` object](https://docs.wandb.ai/ref/python/data-types).","metadata":{}},{"cell_type":"code","source":"# Initialize a run\nrun = wandb.init(project='plant-pathology', \n                 config=CONFIG,\n                 group='EfficientNet', \n                 job_type='evaluate') # Note the job_type\n\n# Update `wandb.config`\nwandb.config.type = 'baseline'\nwandb.config.kaggle_competition = 'Plant Pathology 2021 - FGVC8'\n\n# Evaluate model\nloss, auc, f1_score = model.evaluate(validloader)\n\n# Log scores using wandb.log()\nwandb.log({'val_AUC': auc, \n           'val_F1_score': f1_score})\n\n# Finish the run\nrun.finish()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![img](https://i.imgur.com/OBL9F1i.gif)\n\n> üìå Pro tip: Note that the bar chart apprears if there are more than one value for a key. ","metadata":{}},{"cell_type":"markdown","source":"# üíæ 4. Create a W&B Artifact\n\nW&B's Dashboard lets you log the model training **process**, things like output logs, code versions, configuration, hyperparameters, and metrics. W&B Artifacts lets you log the **data** that goes in (like a dataset) and out (like trained model weights) of these processes.\n\nIn other words, Artifacts are a way to save your datasets and models. You can use [this Colab](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/wandb-artifacts/Pipeline_Versioning_with_W%26B_Artifacts.ipynb) to learn more about Artifacts.\n\nIn this section I'll show you how to create a model Artifact. I also created some dataset Artifacts for this competition, and you can check them all out [here](https://wandb.ai/ayush-thakur/plant-pathology/artifacts).\n\n## üîµ 4a. Save your hard work with `wandb.log_artifact()`\n\nWithin a run, there are three steps for creating and saving a model Artifact.\n\n1. Create an empty Artifact with `wandb.Artifact()`.\n2. Add your model file to the Artifact with `wandb.add_file()`.\n3. Call `wandb.log_artifact()` to save the Artifact.","metadata":{}},{"cell_type":"code","source":"# Save model\nmodel.save('efficientnetb0-baseline.h5')\n\n# Initialize a new W&B run\nrun = wandb.init(project='plant-pathology', \n                 config=CONFIG,\n                 group='EfficientNet', \n                 job_type='save') # Note the job_type\n\n# Update `wandb.config`\nwandb.config.type = 'baseline'\nwandb.config.kaggle_competition = 'Plant Pathology 2021 - FGVC8'\n\n# Save model as Model Artifact\nartifact = wandb.Artifact(name='efficientnet-b0', type='model')\nartifact.add_file('efficientnetb0-baseline.h5')\nrun.log_artifact(artifact)\n\n# Finish W&B run\nrun.finish()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here's a gif that shows how an Artifact appears within a Run Page.\n\n![img](https://i.imgur.com/HjHDoSx.gif)\n\nAt the end of the gif, there's a graph that shows the lineage: the `deep-thunder-8` \"save\" run produced the `efficientnetb0-v0` model.\n\n![img](https://i.imgur.com/59IFYoT.png)","metadata":{}},{"cell_type":"markdown","source":"# ‚ùÑÔ∏è Resources\n\nI hope you find this kernel useful and I encourage you to try out Weights & Biases. Here are some relevant links that you might want to check out:\n\n* Check out the [official documentation](https://docs.wandb.ai/) to learn more about the best practices and advanced features. \n\n* Check out the [examples GitHub repository](https://github.com/wandb/examples) for curated and minimal examples. This can be a good starting point. \n\n* [Fully Connected](https://wandb.ai/fully-connected) is a home for curated tutorials, free-form dicussions, paper summaries, industry expert advices and more. \n\nHere are some other Kaggle kernels instrumented with Weights & Biases that you might find useful. \n\n* [EfficientNet+Mixup+K-Fold using TF and wandb](https://www.kaggle.com/ayuraj/efficientnet-mixup-k-fold-using-tf-and-wandb)\n\n* [HPA: Segmentation Mask Visualization with W&B](https://www.kaggle.com/ayuraj/hpa-segmentation-mask-visualization-with-w-b)\n\n* [HPA: Multi-Label Classification with TF and W&B](https://www.kaggle.com/ayuraj/hpa-multi-label-classification-with-tf-and-w-b)\n\n* [üê¶BirdCLEF: Quick EDA with W&B](https://www.kaggle.com/ayuraj/birdclef-quick-eda-with-w-b)","metadata":{}}]}