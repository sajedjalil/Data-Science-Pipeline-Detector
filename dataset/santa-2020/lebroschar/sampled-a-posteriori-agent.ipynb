{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Write Submission File"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile submission.py\n\n\"\"\"Agent that samples the estimated payout ratio distribution.\n\nThe a posteriori distribution of potential payout ratios for each machine\nis tracked and updated based on the results of each pull. The a priori\ndistribution for each machine is a uniform distribution from 0.0 to 1.0.\n\nWhen selecting a machine to pull, each distribution is sample a configurable\nnumber of times.  The machine with the sample(s) that generate the highest\nexpected reward is selected for the next pull.\n\n\"\"\"\nimport random\n\nimport numpy as np\n\n# Parameters\nNUM_SAMPLES = 5\nPRIOR_DISTRIBUTION = np.array([0.01] * 100)\n\n\nclass MonteCarloStragegy:\n    \"\"\"Implements strategy to maximize expected value\n\n    - Tracks estimated likelihood of payout ratio for each machine\n    - Tracks number of pulls on each machine\n    - Chooses machine based on maximum reward from a limited Monte-Carlo \n      simulation based on the estimated distribution of payout ratios\n    \n    \n    \"\"\"\n    def __init__(self, name, agent_num, initial_dist, ev_rounds, n_machines):\n        \"\"\"Initialize with simple distribution of payout probabilities\n\n        Args:\n           name (str):   Name for the agent\n           agent_num (int):   Assigned player number\n           initial_dist (np.array, (100,)):   a priori payout distribution for\n               each machine.\n           ev_rounds (int):   number of samples to average for monte-carlo\n               expected value calculation\n           n_machines (int):   number of machines in the game\n        \n        \"\"\"\n        # Record inputs\n        self.name = name\n        self.agent_num = agent_num\n        self.initial_dist = initial_dist\n        self.ev_rounds = ev_rounds  # Num rounds to base MC choice on\n        self.n_machines = n_machines\n        \n        # Initialize discrete set of payout ratios\n        self.p_ratios = np.linspace(0, 0.99, 100)\n        \n        # Initialize distributions for all machines\n        self.n_pulls = [0 for _ in range(n_machines)]\n        self.dist = [initial_dist for m_index in range(n_machines)]\n        self.cum_dist = [self.updateCumDist(m_index)\n                         for m_index in range(n_machines)]\n        \n        # Track winnings!\n        self.last_reward_count = 0\n\n    def __call__(self):\n        \"\"\"Choose machine based on maximum Monte-Carlo return\n\n        Returns:\n           <result> (int):  index of machine to pull\n        \n        \"\"\"\n        # Select machine with highest return on limited Monte Carlo\n        est_return = np.array([self.estimatedReturn(m_index)\n                               for m_index in range(self.n_machines)])\n        return int(np.argmax(est_return))\n\n    def samplePayoutRatio(self, m_index):\n        \"\"\"Pull a weighted sample from the distribution\"\"\"\n        x = random.random()\n        return self.p_ratios[np.where(x <= self.cum_dist[m_index])[0][0]]\n\n    def estimatedReturn(self, m_index):\n        \"\"\"Expected return from a Monte-Carlo sample of payout ratios\"\"\"\n        n_pulls = self.n_pulls[m_index]\n        est_p = sum([self.samplePayoutRatio(m_index)\n                     for ii in range(self.ev_rounds)])\n        return est_p / self.ev_rounds * 0.97**n_pulls\n        \n    def updateDist(self, curr_total_reward, last_m_indices):\n        \"\"\"Updates estimated distribution of payouts\"\"\"\n        # Compute last reward\n        last_reward = curr_total_reward - self.last_reward_count\n        self.last_reward_count = curr_total_reward\n\n        if len(last_m_indices) == 2:\n            # Update number of pulls for both machines\n            self.n_pulls[last_m_indices[0]] += 1\n            self.n_pulls[last_m_indices[1]] += 1\n\n            # Update estimated probabilities for this agent's pull\n            m_index = last_m_indices[self.agent_num]\n            n_pulls = self.n_pulls[m_index]\n            if last_reward == 1:\n                curr_prob = self.p_ratios * 0.97**n_pulls\n            else:\n                curr_prob = (1 - self.p_ratios * 0.97**n_pulls)\n\n            self.dist[m_index] = curr_prob * self.dist[m_index]\n            self.dist[m_index] = self.dist[m_index] / self.dist[m_index].sum()\n            self.cum_dist[m_index] = self.updateCumDist(m_index)\n\n    def updateCumDist(self, m_index):\n        \"\"\"Updates cumulative payout ratio distribution\"\"\"\n        return np.cumsum(self.dist[m_index])\n\n\n# DEFINE AGENT ----------------------------------------------------------------\n\ndef agent(observation, configuration):\n    global curr_agent\n    \n    if observation.step == 0:\n        # Initialize agent\n        curr_agent = MonteCarloStragegy(\n            'Mr. Agent %i' % observation['agentIndex'],\n            observation['agentIndex'],\n            PRIOR_DISTRIBUTION,\n            NUM_SAMPLES,\n            configuration['banditCount'])\n    \n    # Update payout ratio distribution with:\n    # - which machines were pulled by both players\n    # - result from previous pull\n    curr_agent.updateDist(observation['reward'], observation['lastActions'])\n\n    return curr_agent()\n    \n# -----------------------------------------------------------------------------\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Agent"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile random.py\n\n\"\"\"Implements an agent that selects machines randomly\n\n\"\"\"\nimport random\n\ndef agent(observation, configuration):\n    machine = random.randint(0, configuration['banditCount'] - 1)\n    return machine","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Simulation"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!pip install kaggle-environments --upgrade -q\nfrom kaggle_environments import make\n\nenv = make(\"mab\", debug=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nenv.reset()\nenv.configuration['episodeSteps'] = 1\nresult = env.run(['submission.py', 'random.py'])\n#env.render(mode=\"ipython\", width=800, height=500)\n\nprint('-------------------')\nprint('FINAL RESULTS')\nprint('-------------------')\nprint('Agent 0: %i rewards' % result[-1][0]['reward'])\nprint('Agent 1: %i rewards' % result[-1][1]['reward'])\nif result[-1][0]['reward'] > result[-1][1]['reward']:\n    print('\\nAgent 0 is the winner!!!')\nelif result[-1][0]['reward'] < result[-1][1]['reward']:\n    print('\\nAgent 1 is the winner!!!')\nelse:\n    print('\\nIts a tie!!!')\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}