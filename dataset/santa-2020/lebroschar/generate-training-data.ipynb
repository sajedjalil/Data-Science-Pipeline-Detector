{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Collect training data from episodes\n\nThe next step beyond heuristics and careful mathematics is to train a machine learning model to choose which lever we should pull.  This notebook shows how to pull some basic training features from matches run locally using the kaggle_environments package.\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom kaggle_environments import make","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Parameters\nrounds = 5\nagent_dir = '../input/sample-submissions/'\nagents = [\n    'bayesian_ucb_xxx.py', \n    'simple_mab_1045.py',\n    'thompson_xxx.py',\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def log_training(result, n_machines):\n    \"\"\"Records training data from each machine, each agent, each round\n    \n    Generates a training dataset to support prediction of the current\n    payout ratio for a given machine.\n    \n    Args:\n       result ([[dict]]) - output from all rounds provided as output of \n                           env.run([agent1, agent2])\n       n_machines (int) - number of machines\n                           \n    Returns:\n       training_data (pd.DataFrame) - training data, including:\n           \"round_num\"      : round number\n           \"machine_id\"     : machine data applies to\n           \"agent_id\"       : player data applies to (0 or 1)\n           \"n_pulls_self\"   : number of pulls on this machine so far by agent_id\n           \"n_success_self\" : number of rewards from this machine by agent_id\n           \"n_pulls_opp\"    : number of pulls on this machine by the other player\n           \"payout\"         : actual payout ratio for this machine\n    \n    \"\"\"\n    # Initialize machine and agent states\n    machine_state = [{'n_pulls_0': 0, 'n_success_0': 0,\n                      'n_pulls_1': 0, 'n_success_1': 0,\n                      'payout': None}\n                     for ii in range(n_machines)]\n    agent_state = {'reward_0': 0, 'reward_1': 0, 'last_reward_0': 0,\n                   'last_reward_1': 0}\n\n    # Initialize training dataframe\n    # - In the first round, store records for all n_machines\n    # - In subsequent rounds, just store the two machines that updated\n    training_data = pd.DataFrame(\n            index=range(n_machines + 4 * (len(result) - 1)),\n            columns=['round_num', 'machine_id', 'agent_id',\n                     'n_pulls_self', 'n_success_self',\n                     'n_pulls_opp', 'payout'])\n    \n    # Log training data from each round\n    for round_num, res in enumerate(result):\n        # Get current threshold values\n        thresholds = res[0]['observation']['thresholds']\n\n        # Update agent state\n        for agent_ii in range(2):\n            agent_state['last_reward_%i' % agent_ii] = (\n                res[agent_ii]['reward']\n                - agent_state['reward_%i' % agent_ii])\n            agent_state['reward_%i' % agent_ii] = res[agent_ii]['reward']        \n\n        # Update most recent machine state\n        if res[0]['observation']['lastActions']:\n            for agent_ii, r_obs in enumerate(res):\n                action = r_obs['action']\n                machine_state[action]['n_pulls_%i' % agent_ii] += 1\n                machine_state[action]['n_success_%i' % agent_ii] += \\\n                    agent_state['last_reward_%i' % agent_ii]\n                machine_state[action]['payout'] = thresholds[action]\n        else:\n            # Initialize machine states\n            for mach_ii in range(n_machines):\n                machine_state[mach_ii]['payout'] = thresholds[mach_ii]\n            \n        # Record training records\n        # -- Each record includes:\n        #       round_num, n_pulls_self, n_success_self, n_pulls_opp\n        if res[0]['observation']['lastActions']:\n            # Add results for most recent moves\n            for agent_ii, r_obs in enumerate(res):\n                action = r_obs['action']\n\n                # Add row for agent who acted\n                row_ii = n_machines + 4 * (round_num - 1) + 2 * agent_ii \n                training_data.at[row_ii, 'round_num'] = round_num\n                training_data.at[row_ii, 'machine_id'] = action\n                training_data.at[row_ii, 'agent_id'] = agent_ii\n                training_data.at[row_ii, 'n_pulls_self'] = (\n                    machine_state[action]['n_pulls_%i' % agent_ii])\n                training_data.at[row_ii, 'n_success_self'] = (\n                    machine_state[action]['n_success_%i' % agent_ii])\n                training_data.at[row_ii, 'n_pulls_opp'] = (\n                    machine_state[action]['n_pulls_%i' % (\n                        (agent_ii + 1) % 2)])\n                training_data.at[row_ii, 'payout'] = (\n                    machine_state[action]['payout'] / 100)\n\n                # Add row for other agent\n                row_ii = n_machines + 4 * (round_num - 1) + 2 * agent_ii + 1\n                other_agent = (agent_ii + 1) % 2\n                training_data.at[row_ii, 'round_num'] = round_num\n                training_data.at[row_ii, 'machine_id'] = action\n                training_data.at[row_ii, 'agent_id'] = other_agent\n                training_data.at[row_ii, 'n_pulls_self'] = (\n                    machine_state[action]['n_pulls_%i' % other_agent])\n                training_data.at[row_ii, 'n_success_self'] = (\n                    machine_state[action]['n_success_%i' % other_agent])\n                training_data.at[row_ii, 'n_pulls_opp'] = (\n                    machine_state[action]['n_pulls_%i' % agent_ii])\n                training_data.at[row_ii, 'payout'] = (\n                    machine_state[action]['payout'] / 100)\n                \n        else:\n            # Add initial data for all machines\n            for action in range(n_machines):\n                row_ii = action\n                training_data.at[row_ii, 'round_num'] = round_num\n                training_data.at[row_ii, 'machine_id'] = action\n                training_data.at[row_ii, 'agent_id'] = -1\n                training_data.at[row_ii, 'n_pulls_self'] = 0\n                training_data.at[row_ii, 'n_success_self'] = 0\n                training_data.at[row_ii, 'n_pulls_opp'] = 0\n                training_data.at[row_ii, 'payout'] = (\n                    machine_state[action]['payout'] / 100)\n            \n    return training_data\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Generate example data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initilize environment\nenv = make(\"mab\", debug=True)\n\n# Create results\ntraining_data = []\nfor r_num in range(rounds):\n    print('***Round %i of %i***' % (r_num + 1, rounds))\n    for ii, agent1 in enumerate(agents):\n        for agent2 in agents[(ii+1):]:\n            env.reset()\n            result = env.run([agent_dir + agent1, agent_dir + agent2])\n            training_data.append(log_training(\n                result, len(result[0][0]['observation']['thresholds'])))\n            print('agent 1 (%s): %i, agent 2 (%s): %i' % (\n                agent1, result[-1][0]['reward'], agent2,\n                result[-1][1]['reward']))\n\n# Save training data\ntraining_data = pd.concat(training_data, axis=0)\ntraining_data.to_csv('/kaggle/working/training_data.csv', index=False, header=True)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}