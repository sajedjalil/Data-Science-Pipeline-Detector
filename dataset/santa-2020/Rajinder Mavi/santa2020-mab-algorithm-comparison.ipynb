{"cells":[{"metadata":{},"cell_type":"markdown","source":"**A script evaluating several standard multi-arm bandit algorithms.**\n\nI'm completely new to the multi-armed bandit problem, so I coded up a bunch of the standard algorithms and compared each to a random algorithm. "},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\n# elf pulling at random \nclass basic_elf():\n    def __init__(self):\n        self.mu   = None\n        self.win  = None\n        self.loss = None\n        self.id   = None\n        self.tot_reward = 0 \n    def set_pars(self,obs,conf):\n        self.n_bandits = conf.banditCount \n        self.win  = np.ones(self.n_bandits)\n        self.loss = np.ones(self.n_bandits)\n        self.id   = obs.agentIndex\n    def pull(self,obs,config):\n        '''random elf'''\n        return int(np.random.choice(config.banditCount))\n\n# elf employing epsilon greedy algorithm\nclass epsilon_elf(basic_elf):\n    def __init__(self,epsilon):\n        super(epsilon_elf,self).__init__()\n        self.epsilon = epsilon\n    def pull(self,obs,conf):\n        '''epsilon elf'''\n        if obs.step == 0:\n            self.set_pars(obs,conf)\n        else:\n            r = obs.reward - self.tot_reward\n            self.tot_reward = obs.reward\n            \n            self.win[obs['lastActions'][self.id]]  += r\n            self.loss[obs['lastActions'][self.id]] += 1-r\n            \n        mu = self.win/(self.win+self.loss);\n        w=(self.epsilon/self.n_bandits)*np.ones(self.n_bandits)\n        w[np.argmax(mu)] = 1-self.epsilon+self.epsilon/self.n_bandits\n            \n        return int(np.random.choice(self.n_bandits,1,p=list(w)))    \n    \n\n    \n# elf employing Boltzmann algorithm\nclass boltzmann_elf(basic_elf):\n    def __init__(self,invT):\n        super(boltzmann_elf,self).__init__()\n        self.invT = invT\n    def pull(self,obs,conf):\n        '''boltzmann elf'''\n        if obs.step == 0:\n            self.set_pars(obs,conf)\n        else:\n            r = obs.reward - self.tot_reward\n            self.tot_reward = obs.reward\n            \n            self.win[obs['lastActions'][self.id]]  += r\n            self.loss[obs['lastActions'][self.id]] += 1-r\n            \n        mu = self.win/(self.win+self.loss);w=np.exp(self.invT*mu)\n            \n        return int(np.random.choice(self.n_bandits,1,p=list(w/w.sum())))\n    \n# elf employing pursuit algorithm    \nclass pursuit_elf(basic_elf):\n    def __init__(self,beta):\n        super(pursuit_elf,self).__init__()\n        self.beta = beta\n    def pull(self,obs,conf):\n        '''pursuit elf'''\n        if obs.step == 0:\n            self.set_pars(obs,conf)\n            self.pi = np.ones(self.n_bandits)/self.n_bandits\n        else:\n            r = obs.reward - self.tot_reward\n            self.tot_reward = obs.reward\n            \n            self.win[obs['lastActions'][self.id]]  += r\n            self.loss[obs['lastActions'][self.id]] += 1-r\n            \n            mu = self.win/(self.win+self.loss);mu_max = int(np.argmax(mu));\n            self.pi=self.pi- self.beta*self.pi; self.pi[mu_max]+=self.beta\n        \n        \n        return int(np.random.choice(np.arange(self.n_bandits), 1, p=list(self.pi) ))\n\n\n    \n# elf employing reinforcement comparison algorithm\nclass reinforcement_elf(basic_elf):\n    def __init__(self,alpha,beta):\n        super(reinforcement_elf,self).__init__()\n        self.alpha = alpha\n        self.beta  = beta\n        self.rbar  = 0.5\n    def pull(self,obs,conf):\n        '''reinforcement elf'''\n        if obs.step == 0:\n            self.set_pars(obs,conf)\n            self.pi = (1/self.n_bandits)*np.ones(self.n_bandits)\n        else:\n            r = obs.reward - self.tot_reward\n            self.tot_reward = obs.reward\n            \n            self.pi[obs['lastActions'][self.id]] +=self.beta*(r-self.rbar)      \n            self.rbar += self.alpha*(r - self.rbar)\n            \n        w=np.exp(self.pi)\n            \n        return int(np.random.choice(self.n_bandits,1,p=list(w/w.sum())))                   \n\n# elf employing UCB algorithm\nclass UCB_elf(basic_elf):\n    def __init__(self):\n        super(UCB_elf,self).__init__()\n    def pull(self,obs,conf):\n        '''boltzmann elf'''\n        if obs.step == 0:\n            self.set_pars(obs,conf)\n        else:\n            r = obs.reward - self.tot_reward\n            self.tot_reward = obs.reward\n            \n            self.win[obs['lastActions'][self.id]]  += r\n            self.loss[obs['lastActions'][self.id]] += 1-r\n            \n        pulls = self.win+self.loss           \n        mu = self.win/(self.win+self.loss);\n        \n        return int(np.argmax( mu + np.sqrt(2*np.log(1+obs.step)/pulls) ))\n                   \n                   ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"! pip install kaggle-environments --upgrade -q\nfrom kaggle_environments import evaluate, make, utils\n\nenv = make(\"mab\", debug=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef baseline(basic,name,agents,names,reps):\n    \n    if len(agents)!=len(names):\n        print('Number of agents do not correspond to number of names.')\n    \n    means = np.zeros([len(names),2])\n    mins = np.zeros([len(names),2])\n    maxes = np.zeros([len(names),2])\n    \n    for i, agenti in enumerate(agents):\n        results = np.zeros([reps,2])\n        for j in range(reps):\n            \n            env.reset()\n            #print('run agent')\n            env.run([basic, agenti])\n            #print('save agent')\n            json = env.toJSON()\n            rewards = json['rewards']\n            results[j,0] = rewards[0]\n            results[j,1] = rewards[1] \n        means[i] = np.mean(results,axis=0)\n        mins[i] = np.min(results,axis=0)\n        maxes[i] = np.max(results,axis = 0)\n    \n    plus = maxes -means;minus = means - mins\n\n    bars0 = np.vstack((minus.T[0],plus.T[0]))\n    bars1 = np.vstack((minus.T[1],plus.T[1]))\n \n    plt.errorbar(np.arange(len(names)),means.T[0],bars0,marker = 'o',ls = '',label = 'Random')\n    plt.errorbar(np.arange(len(names)),means.T[1],bars1,marker = 'o',ls = '',label = 'Algorithm')\n    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n    plt.xticks(np.arange(len(names)),names,rotation = 'vertical')\n    plt.title(f'Random selection vs each algorithm over {reps} runs')\n    plt.ylabel('Min/Mean/Max Reward')\n    \n    plt.show()\n\n        \n    return mins,maxes, means","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random = basic_elf()\nepsilon001 = epsilon_elf(0.01)\nepsilon01 = epsilon_elf(0.1)\nepsilon02 = epsilon_elf(0.2)\nboltzmann01 = boltzmann_elf(.1)\nboltzmann1 = boltzmann_elf(1)\nboltzmann2 = boltzmann_elf(2)\nboltzmann10 = boltzmann_elf(10) \npursuit001 = pursuit_elf(0.01)\npursuit01 = pursuit_elf(0.1)\npursuit02 = pursuit_elf(0.2)\nreinforcement001001 = reinforcement_elf(0.01,0.01)\nreinforcement01001 = reinforcement_elf(0.1,0.01)\nreinforcement00101 = reinforcement_elf(0.01,0.1)\nreinforcement0101 = reinforcement_elf(0.1,0.1)\nreinforcement0201 = reinforcement_elf(0.2,0.1)\nreinforcement0102 = reinforcement_elf(0.1,0.2)\nreinforcement0202 = reinforcement_elf(0.2,0.2)\nUCB = UCB_elf()\nagents = [\n          epsilon001.pull,epsilon01.pull, epsilon02.pull, \n          boltzmann01.pull,boltzmann1.pull,boltzmann2.pull,boltzmann10.pull,  \n          pursuit001.pull,pursuit01.pull,pursuit02.pull,\n          reinforcement001001.pull,\n          UCB.pull]\nnames = [\n         'greedy, (e = 0.01)','greedy, (e = 0.1)','greedy, (e = 0.2)',\n         'boltzmann, (invT = 0.1)','boltzmann, (invT = 1)','boltzmann, (invT = 2)','boltzmann, (invT = 10)', \n         'pursuit, (beta = 0.01)','pursuit, (beta = 0.1)','pursuit, (beta = 0.2)',\n         'reinforcement, (a=b=0.001)',\n         'UCB']\n\n\nmins,maxes,means = baseline(random.pull,'random',agents,names,20)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}