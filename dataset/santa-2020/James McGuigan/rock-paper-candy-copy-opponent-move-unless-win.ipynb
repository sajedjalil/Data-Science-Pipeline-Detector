{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Rock Paper Candy - Copy Opponent Move Unless Win\n\nThis follows on from the Copy Opponent Move strategy, but will repeatedly try any machine that\nproduces a win payout, and only copy the opponent move after a loss \n- https://www.kaggle.com/jamesmcguigan/rock-paper-candy-copy-opponent-move/"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install kaggle-environments --upgrade -q","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"051d70d956493feee0c6d64651c6a088724dca2a","_execution_state":"idle","trusted":true},"cell_type":"code","source":"%%writefile submission.py\n\nimport random\nimport sys\n\nclass CopyLastOpponentMoveUnlessWin:\n    def __init__(self, retry_winrate=5/6, retry_max_step=1000, verbose=True):\n        self.retry_winrate  = retry_winrate\n        self.retry_max_step = retry_max_step or sys.maxsize \n        self.verbose        = verbose\n        \n        self.last_reward = 0\n        self.last_action = 0\n        self.last_winrate = { \"count\": 0, \"reward\": 0 }\n\n    def __call__(self, obs, conf):\n        return self.agent(obs, conf)\n\n    def winrate(self) -> float:\n        winrate = self.last_winrate['reward'] / max(1, self.last_winrate['count'])\n        return winrate\n    \n    def print_winrate(self):\n        winrate = self.winrate()\n        print( f\"winrate {self.last_action:02d} = {self.last_winrate['reward']:2d} / {max(1, self.last_winrate['count']):2d} = {winrate:.2f}\" )\n        \n    \n    # observation   {'remainingOverageTime': 60, 'agentIndex': 1, 'reward': 0, 'step': 0, 'lastActions': []}\n    # configuration {'episodeSteps': 2000, 'actTimeout': 0.25, 'runTimeout': 1200, 'banditCount': 100, 'decayRate': 0.97, 'sampleResolution': 100}\n    def agent(self, obs, conf) -> int:\n        # print('observation', obs)\n        # print('configuration', conf)\n                \n        # First round doesn't have lastActions \n        if obs.step > 0 and len(obs.lastActions): \n            self.last_winrate['count'] += 1\n            \n            action = None\n            \n            # Stop doing retry near the end of the game\n            if obs.step <= self.retry_max_step:            \n                # Copy opponent move unless win\n                if obs.reward > self.last_reward:\n                    self.last_winrate['reward'] += 1\n                    action = self.last_action\n\n                # If we have found a bandit with a high winrate, keep trying it unless over retry_max_step\n                elif self.winrate() >= self.retry_winrate:\n                    action = self.last_action\n            \n            # Else copy opponent action and reset stats\n            if action is None:            \n                self.last_winrate = { \"count\": 0, \"reward\": 0 }\n                opponentIndex  = (obs.agentIndex + 1) % len(obs.lastActions)\n                opponentAction = obs.lastActions[opponentIndex]\n                action         = opponentAction\n        else:\n            # When in doubt, be random\n            action = random.randrange(conf.banditCount) \n\n        self.last_action = action = int(action or 0) % conf.banditCount\n        self.last_reward = obs.reward\n        \n        if self.verbose:\n            self.print_winrate()\n\n        return action\n\n    \ninstance = CopyLastOpponentMoveUnlessWin()\ndef kaggle_agent(obs, conf):\n    return instance.agent(obs, conf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%run submission.py","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\ndef random_agent(observation, configuration):\n    return random.randrange(configuration.banditCount)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from kaggle_environments import make, evaluate\n\n# env = make(\"mab\", debug=True, configuration={\"episodeSteps\": 50})\nenv = make(\"mab\", debug=True)\n\nenv.reset()\n# env.run([\n#     CopyLastOpponentMoveUnlessWin(retry_winrate=5/6, retry_max_step=1000), \n#     CopyLastOpponentMoveUnlessWin(retry_winrate=1,   retry_max_step=2000),\n# ])\nenv.run([\"submission.py\", random_agent])\nenv.render(mode=\"ipython\", width=500, height=500)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Turns out we can beat random bot!"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom joblib import Parallel, delayed\n\nresults = np.array(Parallel(-1)([\n    delayed(evaluate)(\"mab\", [\"submission.py\", random_agent])\n    for n in range(10)\n])).reshape(-1,2)\n\nprint('results\\n', results)\nprint('mean', np.mean(results, axis=0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Further Reading\n\nThis notebook is part of a series exploring the Santa2020 Candy Cane competition\n- [Rock Paper Candy - Copy Opponent Move](https://www.kaggle.com/jamesmcguigan/rock-paper-candy-copy-opponent-move)\n- [Rock Paper Candy - Copy Opponent Move Unless Win](https://www.kaggle.com/jamesmcguigan/rock-paper-candy-copy-opponent-move-unless-win)\n- [Candy Cane - Multi-Armed Bandit](https://www.kaggle.com/jamesmcguigan/candy-cane-multi-armed-bandit)\n- [Candy Cane - Optimized UCB](https://www.kaggle.com/jamesmcguigan/candy-cane-optimized-ucb)\n- [Candy Cane - Random Agent](https://www.kaggle.com/jamesmcguigan/candy-cane-random-agent)\n\nI also created an agents comparison notebook to compare the relative strengths of public agents:\n- [Santa 2020 - Agents Comparison](https://www.kaggle.com/jamesmcguigan/santa-2020-agents-comparison/)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}