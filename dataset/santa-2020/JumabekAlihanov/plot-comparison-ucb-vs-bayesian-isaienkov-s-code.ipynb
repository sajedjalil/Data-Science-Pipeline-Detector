{"cells":[{"metadata":{},"cell_type":"markdown","source":"Initial codes are from isaienkov"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"%%writefile random_agent.py\n\nimport random\n\ndef random_agent(observation, configuration):\n    return random.randrange(configuration.banditCount)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile ucb_agent.py\n\nimport math\n\nlast_bandit = -1\ntotal_reward = 0\n\nsums_of_reward = None\nnumbers_of_selections = None\nreward_hist = []\n\ndef ucb_agent(observation, configuration):    \n    global sums_of_reward, numbers_of_selections, last_bandit, total_reward\n    global reward_hist \n\n    if observation.step == 0:\n        numbers_of_selections = [0] * configuration[\"banditCount\"]\n        sums_of_reward = [0] * configuration[\"banditCount\"]\n\n    if last_bandit > -1:\n        reward = observation.reward - total_reward\n        sums_of_reward[last_bandit] += reward\n        total_reward += reward\n        \n        reward_hist.append(reward)\n\n    bandit = 0\n    max_upper_bound = 0\n    for i in range(0, configuration.banditCount):\n        if (numbers_of_selections[i] > 0):\n            average_reward = sums_of_reward[i] / numbers_of_selections[i]\n            delta_i = math.sqrt(2 * math.log(observation.step+1) / numbers_of_selections[i])\n            upper_bound = average_reward + delta_i\n        else:\n            upper_bound = 1e400\n        if upper_bound > max_upper_bound and last_bandit != i:\n            max_upper_bound = upper_bound\n            bandit = i\n            last_bandit = bandit\n\n    numbers_of_selections[bandit] += 1\n\n    if bandit is None:\n        bandit = 0\n\n    return bandit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install kaggle-environments --upgrade","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\ncolors_db = ['g','b']\ndef plot_final_rewards(hist):\n    num_episodes = 0\n    \n    plt.figure(figsize=(12,8))\n    for i,agent in enumerate(hist.keys()):\n        plt.plot(hist[agent], label=agent, color=colors_db[i])\n        num_episodes = len(hist[agent])\n        avg_final_reward = np.array(hist[agent]).mean()\n        plt.plot([0, num_episodes-1],[avg_final_reward, avg_final_reward], label=agent+' avg.', color=colors_db[i],linestyle='dashed')\n        \n    plt.legend(bbox_to_anchor=(1.2, 0.5))\n    plt.xlabel(\"Iterations\")\n    plt.ylabel(\"Final Reward\")\n    plt.title(\"Final Agent Rewards for \" \n              + str(num_episodes) + \" Episodes\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from kaggle_environments import make\nfrom collections import defaultdict\nfrom tqdm import tqdm\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hist = defaultdict(list)\nnum_trails = 10\n\nfor i in tqdm(range(num_trails)):\n    env = make(\"mab\")\n    env.run([\"random_agent.py\", \"ucb_agent.py\"])\n    hist['random_agent'].append(env.state[0]['reward'])\n    hist['ucb_agent'].append(env.state[1]['reward'])\n    \n\nplot_final_rewards(hist)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# UCB vs Bayesian"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile bayesian_sub.py\n\nimport numpy as np\nfrom scipy.stats import beta\n\nps_a = None\npost_b = None\nbandit = None\ntotal_reward = 0\n\n\ndef agent(observation, configuration):\n    global reward_sums, total_reward, bandit, post_a, post_b\n    \n    n_bandits = configuration.banditCount\n\n    if observation.step == 0:\n        post_a = np.ones(n_bandits)\n        post_b = np.ones(n_bandits)\n    else:\n        r = observation.reward - total_reward\n        total_reward = observation.reward\n\n        post_a[bandit] += r\n        post_b[bandit] += (1 - r)\n\n    \n    bound = post_a / (post_a + post_b).astype(float) + beta.std(post_a, post_b) * 21\n    bandit = int(np.argmax(bound))\n    \n    return bandit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hist = defaultdict(list)\nnum_trails = 10\n\nfor i in tqdm(range(num_trails)):\n    env = make(\"mab\")\n    env.run([\"bayesian_sub.py\", \"ucb_agent.py\"])\n    hist['bayesian_sub'].append(env.state[0]['reward'])\n    hist['ucb_agent'].append(env.state[1]['reward'])\n    \n\nplot_final_rewards(hist)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hist = defaultdict(list)\nnum_trails = 100\nenv = make(\"mab\")\nfor i in tqdm(range(num_trails)):\n    env.reset()\n    env.run([\"bayesian_sub.py\", \"ucb_agent.py\"])\n    hist['bayesian_sub'].append(env.state[0]['reward'])\n    hist['ucb_agent'].append(env.state[1]['reward'])\n    \n\nplot_final_rewards(hist)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Performance of both UCB & Bayesian are almost similar"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}