{"cells":[{"metadata":{},"cell_type":"markdown","source":"References:\n* [Santa 2020 starter](https://www.kaggle.com/isaienkov/santa-2020-starter/): Re-used writefile magic command and make_env function for creating a simulation.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install kaggle-environments --upgrade -q","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Thompson Sampling\n\nBased on Lilian's blog post: https://lilianweng.github.io/lil-log/2018/01/23/the-multi-armed-bandit-problem-and-its-solutions.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile thompson.py\n\nimport numpy as np\n\npost_a = None\npost_b = None\nbandit = None\ntotal_reward = 0\nc = 3 #3\n\n\ndef agent(observation, configuration):\n    global reward_sums, total_reward, bandit, post_a, post_b, c\n    \n    n_bandits = configuration.banditCount\n\n    if observation.step == 0:\n        post_a = np.ones(n_bandits)\n        post_b = np.ones(n_bandits)\n    else:\n        r = observation.reward - total_reward\n        total_reward = observation.reward\n\n        # Update Gaussian posterior\n        post_a[bandit] += r\n        post_b[bandit] += (1 - r)\n\n    samples = np.random.beta(post_a, post_b)\n    bandit = int(np.argmax(samples))\n    \n    return bandit","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Simulations"},{"metadata":{"trusted":true},"cell_type":"code","source":"from kaggle_environments import make\n\nenv = make(\"mab\", debug=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env.run([\"../input/santa-2020/submission.py\", \"thompson.py\"])\nenv.render(mode=\"ipython\", width=800, height=500)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile bayesian_ucb.py\nimport numpy as np\nfrom scipy.stats import beta\n\npost_a, post_b, bandit = [None] * 3\ntotal_reward = 0\nc = 3\n\ndef agent(observation, configuration):\n    global total_reward, bandit, post_a, post_b, c\n\n    if observation.step == 0:\n        post_a, post_b = np.ones((2, configuration.banditCount))\n    else:\n        r = observation.reward - total_reward\n        total_reward = observation.reward\n        # Update Gaussian posterior\n        post_a[bandit] += r\n        post_b[bandit] += 1 - r\n    \n    bound = post_a / (post_a + post_b) + beta.std(post_a, post_b) * c\n    bandit = int(np.argmax(bound))\n    \n    return bandit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile ucb_decay.py\nimport numpy as np\n\ndecay = 0.97\ntotal_reward = 0\nbandit = None\n\ndef agent(observation, configuration):\n    global reward_sums, n_selections, total_reward, bandit\n    \n    n_bandits = configuration.banditCount\n\n    if observation.step == 0:\n        n_selections, reward_sums = np.full((2, n_bandits), 1e-32)\n    else:\n        reward_sums[bandit] += decay * (observation.reward - total_reward)\n        total_reward = observation.reward\n\n    avg_reward = reward_sums / n_selections    \n    delta_i = np.sqrt(2 * np.log(observation.step + 1) / n_selections)\n    bandit = int(np.argmax(avg_reward + delta_i))\n\n    n_selections[bandit] += 1\n\n    return bandit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile epsilon_greedy_decay.py\nimport math\nimport random\n\nepsilon = 0.1 #0.1\n\nlast_bandit = -1\ntotal_reward = 0\n\nsums_of_reward = None\nnumbers_of_selections = None\nrandom.seed(42)\n\ndef agent(observation, configuration):    \n    global sums_of_reward, numbers_of_selections, last_bandit, total_reward    \n\n    if observation.step == 0:\n        numbers_of_selections = [0] * configuration.banditCount\n        sums_of_reward = [0] * configuration.banditCount\n\n    if last_bandit > -1:\n        reward = observation.reward - total_reward\n        sums_of_reward[last_bandit] += reward\n        total_reward += reward\n\n    if random.random() < epsilon:\n        bandit = random.randint(0, configuration.banditCount-1)\n        last_bandit = bandit\n    else:\n        bandit = 0\n        max_upper_bound = 0\n\n        for i in range(0, configuration.banditCount):\n            if numbers_of_selections[i] > 0:\n                decay = 0.94 ** numbers_of_selections[i] #0.97\n                upper_bound = decay * sums_of_reward[i] / numbers_of_selections[i]\n            else:\n                upper_bound = 1e300  #1e400\n            if upper_bound > max_upper_bound and last_bandit != i:\n                max_upper_bound = upper_bound\n                bandit = i\n                last_bandit = bandit\n\n    numbers_of_selections[bandit] += 1\n\n    if bandit is None:\n        bandit = 0\n\n    return bandit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile multi_armed.py\n\nimport json\nimport numpy as np\nimport pandas as pd\n\nbandit_state = None\ntotal_reward = 0\nlast_step = None\n    \ndef multi_armed_bandit_agent (observation, configuration):\n    global history, history_bandit\n\n    no_reward_step = 0.1 #0.3\n    decay_rate = 0.97 # how much do we decay the win count after each call #0.97\n    \n    global bandit_state,total_reward,last_step\n        \n    if observation.step == 0:\n        # initial bandit state\n        bandit_state = [[1,1] for i in range(configuration[\"banditCount\"])]\n    else:       \n        # updating bandit_state using the result of the previous step\n        last_reward = observation[\"reward\"] - total_reward\n        total_reward = observation[\"reward\"]\n        \n        # we need to understand who we are Player 1 or 2\n        player = int(last_step == observation.lastActions[1])\n        \n        if last_reward > 0:\n            bandit_state[observation.lastActions[player]][0] += last_reward\n        else:\n            bandit_state[observation.lastActions[player]][1] += no_reward_step\n        \n        bandit_state[observation.lastActions[0]][0] = (bandit_state[observation.lastActions[0]][0] - 1) * decay_rate + 1\n        bandit_state[observation.lastActions[1]][0] = (bandit_state[observation.lastActions[1]][0] - 1) * decay_rate + 1\n\n#     generate random number from Beta distribution for each agent and select the most lucky one\n    best_proba = -1\n    best_agent = None\n    for k in range(configuration[\"banditCount\"]):\n        proba = np.random.beta(bandit_state[k][0],bandit_state[k][1])\n        if proba > best_proba:\n            best_proba = proba\n            best_agent = k\n        \n    last_step = best_agent\n    return best_agent","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile agent2.py\nimport math, random\n\n\nhistory = {\n    \"turn\": 0,\n    \"cnts\": [0] * 100,\n    \"ocnts\": [0] * 100,\n    \"hits\": [0] * 100,\n    \"osteps\": [0] * 100,\n    \"la\": -1,\n}\n\ndef agent2(observation, configuration):\n    global history\n\n    N = 100\n    p = [4.60575, 0.000629018, 1.82229]\n    ti = observation[\"step\"]\n    if ti == 0:\n        pass\n    else:\n        la = history[\"la\"]\n        ola = sum(observation['lastActions']) - la\n        history[\"osteps\"][ola] = ti\n        if sum(history[\"hits\"]) < observation['reward']:\n            history[\"hits\"][la] += 1 / pow(0.97, history[\"cnts\"][la] + history[\"ocnts\"][la])\n        history[\"cnts\"][la] += 1\n        history[\"ocnts\"][ola] += 1\n\n    tau = p[0] / (ti + 1) + p[1]\n    ea = [0] * N\n    hits = history[\"hits\"]\n    cnts = history[\"cnts\"]\n    ocnts = history[\"ocnts\"]\n    osteps = history[\"osteps\"]\n\n    tv = sorted([(-ocnts[i], osteps[i], i) for i in range(N)])\n    ot = [0] * N\n    for i in range(N):\n        ot[tv[i][2]] = 99 - i\n\n    for i in range(N):\n        if cnts[i] == 0:\n            if ocnts[i] > 1:\n                ea[i] = math.exp(ot[i] / 100 * pow(0.97, ocnts[i]) / tau)\n            else:\n                ea[i] = math.exp(0.99 * pow(0.97, ocnts[i]) / tau)\n        else:\n            w = pow(cnts[i], p[2])\n            wo = ocnts[i]\n            if ocnts[i] < 2:\n                wo = 0\n            r = hits[i] / cnts[i]\n            ro = ot[i] / 100\n            ea[i] = math.exp((r * w + ro * wo) / (w + wo) * pow(0.97, cnts[i] + ocnts[i]) / tau)\n\n    se = sum(ea)\n    r = random.random() * se\n    t = 0\n    la = 99\n    for i in range(N):\n        t += ea[i]\n        if t >= r:\n            la = i\n            break\n\n    history[\"la\"] = la\n    return la","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile agent3.py\nimport math, random\n\n\nhistory = {\n    \"turn\": 0,\n    \"cnts\": [0] * 100,\n    \"ocnts\": [0] * 100,\n    \"hits\": [0] * 100,\n    \"osteps\": [0] * 100,\n    \"la\": -1,\n}\n\nNU=500 #500\nepson=0.97 #0.97\ndef agent3(observation, configuration):\n    global history\n\n    N = 100\n    p = [0.39918, 0.000138129, 1.23946]\n    #p = [5.60575, 0.000629018, 1.82229]\n    ti = observation[\"step\"]\n    if ti == 0:\n        pass\n    else:\n        la = history[\"la\"]\n        ola = sum(observation['lastActions']) - la\n        history[\"osteps\"][ola] = ti\n        if sum(history[\"hits\"]) < observation['reward']:\n            history[\"hits\"][la] += 1 / pow(epson, history[\"cnts\"][la] + history[\"ocnts\"][la])\n        history[\"cnts\"][la] += 1\n        history[\"ocnts\"][ola] += 1\n\n    tau = p[0] / (ti + 1) + p[1]\n    ea = [0] * N\n    hits = history[\"hits\"]\n    cnts = history[\"cnts\"]\n    ocnts = history[\"ocnts\"]\n    osteps = history[\"osteps\"]\n\n    tv = sorted([(-ocnts[i], osteps[i], i) for i in range(N)])\n    ot = [0] * N\n    for i in range(N):\n        ot[tv[i][2]] = 99 - i\n\n    for i in range(N):\n        if cnts[i] == 0:\n            if ocnts[i] > 1:\n                ea[i] = math.exp(min(NU, ot[i] / 100 * pow(epson, ocnts[i]) / tau)) #0.97\n            else:\n                ea[i] = math.exp(min(NU, 0.99 * pow(epson, ocnts[i]) / tau)) #0.99\n        else:\n            w = pow(cnts[i], p[2])\n            wo = ocnts[i]\n            if ocnts[i] < 1: #2\n                wo = 0\n            r = hits[i] / cnts[i]\n            ro = ot[i] / 100\n            ea[i] = math.exp(min(NU, (r * w + ro * wo) / (w + wo) * pow(epson, cnts[i] + ocnts[i]) / tau)) #0.97\n\n    se = sum(ea)\n    r = random.random() * se\n    t = 0\n    la = 99 #99\n    for i in range(N):\n        t += ea[i]\n        if t >= r:\n            la = i\n            break\n\n    history[\"la\"] = la\n    return la","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile fewa.py\nimport random\nimport numpy as np\n\n# seed\nrandom.seed(2020)\nnp.random.seed(2020)\n\n# global vars\ndecay = .97\nn_ag = None\nhistory = None\nlast_a_ag = None\nlast_a_op = None\nrewards = None\ntotal_reward = 0\nalpha = 0.1\ndelta0 = 1\n\n# filter\ndef filter_step(k, h, t, delta_t, rewards, sigma2=1):\n    # determine c\n    c = np.sqrt((2 * sigma2 / (h + 1)) * np.log(1 / delta_t))\n    # estimates\n    mu = np.mean(rewards[(t - h - 1):t, k], axis=0)\n    mu_max = np.max(mu)\n#     print('c', c, 'mu_max', mu_max, 'len(k)', len(k))\n    # filter\n    delta_i = mu_max - mu.reshape(-1,)\n    k_next = [i for i, di in zip(k, delta_i) if di <= 2 * c]\n    return k_next\n\n# agent\ndef agent(obs, conf):\n    global n_ag, last_a_ag, last_a_op, rewards, total_reward, history\n\n    # init\n    t = obs.step\n    if t == 0:\n        # init\n        n_ag = np.zeros(conf.banditCount, dtype=np.int)\n        history = np.zeros(conf.banditCount)\n        rewards = np.zeros(conf.banditCount)\n        # take action\n        action = int(obs.step)\n        # update history\n        hist_vector = 1 * (np.arange(conf.banditCount) == action)\n        history = np.vstack((history, hist_vector))\n    else:\n        # get opps last action\n        op_ix = (obs.agentIndex + 1) % len(obs.lastActions)\n        last_a_op = obs.lastActions[op_ix]\n        # update counts\n        n_ag[last_a_ag] += int(1)\n        #n_ag[last_a_op] += int(1)\n        #print(n_ag)\n        # update history\n        hist_vector = 1 * (np.arange(conf.banditCount) == last_a_op)\n        history = np.vstack((history, hist_vector))\n        # reward\n        r = (obs.reward - total_reward)\n        r_vector =  r * (np.arange(conf.banditCount) == last_a_op)\n        rewards = np.vstack((rewards, r_vector))\n        total_reward = obs.reward\n\n        # warmup\n        if t < conf.banditCount:\n            # take action\n            action = int(obs.step)\n        else:\n            # FEWA algorithm\n            # update delta\n            delta_t = delta0 / ((t+1) ** alpha)\n            # init\n            h = int(0)\n            k = list(range(conf.banditCount))\n            it = None\n            # loop\n            while it is None:\n                # filter\n                k_next = filter_step(k, h, t, delta_t, rewards)\n                k = k_next\n                # increment\n                h += int(1)\n                # there exists any bandit that number of selected times is h?\n                if any(n_ag[k] == h):\n                    if (n_ag[k] == h).sum() > 1:\n                        # breaks tie randomly\n                        it = int(np.random.choice(np.array(k)[n_ag[k] == h]))\n                    else:\n                        ix = np.argmin(n_ag[k])\n                        it = int(k[ix])\n#             print('potential bandits', len(k))\n#             print('action = ', it)\n            action = it\n    # update last action\n    last_a_ag = action\n    return action","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile agent4.py\nimport random\n\nmoves_stack = [x for x in range(100)]\noppo = []\nprev_reward = 0\nprev_action = 0\ntotal_bnd = [0 for x in range(100)]\nwon_bnd = [0 for x in range(100)]\n\n\ndef get_bandit():\n  best_bandit = 0\n  best_score = 0\n  for bnd in range(100):\n\n    if total_bnd[bnd] <= 3:\n      return bnd\n\n    this_score = (won_bnd[bnd] / total_bnd[bnd])\n    \n    if this_score > best_score:\n      best_score = this_score\n      best_bandit = bnd\n  \n  return best_bandit\n\n\ndef agent(obs, conf):\n    global moves_stack, oppo, prev_reward, prev_action, total_bnd, won_bnd\n\n    if obs.step == 1:\n        prev_action = moves_stack.pop(0)\n        \n        total_bnd[prev_action] += 1\n        return prev_action\n    \n    my_idx = obs['agentIndex']\n\n    if obs.step > 5:\n        oppo.append(obs['lastActions'][1-my_idx])\n\n    reward_this_time = obs.reward - prev_reward\n    prev_reward = obs.reward\n\n    if reward_this_time > 0:\n        moves_stack.insert(0, prev_action)\n        won_bnd[prev_action] += 1      \n\n    if len(oppo) >= 3:\n        if oppo[-1] == oppo[-2] and oppo[-1] == oppo[-3]:\n            moves_stack.insert(0, oppo[-1])\n                \n\n    if len(moves_stack) == 0:\n         moves_stack.insert(0, get_bandit())\n    \n    prev_action = moves_stack.pop(0)\n    \n    total_bnd[prev_action] += 1\n    return prev_action","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile sirish.py\nimport numpy as np\nimport pandas as pd\nimport random, os, datetime\n\ntotal_reward = 0\nbandit_dict = {}\nNUM=1.2 #1.5  #1.6_0.66  #1_0.46  #0.5_0.66   #0.25_0.73  #1.4_0.4  1.3_0.26\n\ndef set_seed(my_seed=44): #42  #44_0.33\n    os.environ['PYTHONHASHSEED'] = str(my_seed)\n    random.seed(my_seed)\n    np.random.seed(my_seed)\n\ndef get_next_bandit():\n    best_bandit = 0\n    best_bandit_expected = 0\n    for bnd in bandit_dict:\n        expect = (bandit_dict[bnd]['win'] - bandit_dict[bnd]['loss'] + bandit_dict[bnd]['opp'] - (bandit_dict[bnd]['opp']>0)*NUM) \\\n                 / (bandit_dict[bnd]['win'] + bandit_dict[bnd]['loss'] + bandit_dict[bnd]['opp'])\n        if expect > best_bandit_expected:\n            best_bandit_expected = expect\n            best_bandit = bnd\n    return best_bandit\n\ndef multi_armed_probabilities(observation, configuration):\n    global total_reward, bandit_dict\n\n    my_pull = random.randrange(configuration['banditCount'])\n    if 0 == observation['step']:\n        set_seed()\n        total_reward = 0\n        bandit_dict = {}\n        for i in range(configuration['banditCount']):\n            bandit_dict[i] = {'win': 1, 'loss': 0, 'opp': 0}\n    else:\n        last_reward = observation['reward'] - total_reward\n        total_reward = observation['reward']\n        \n        my_idx = observation['agentIndex']\n        if 0 < last_reward:\n            bandit_dict[observation['lastActions'][my_idx]]['win'] = bandit_dict[observation['lastActions'][my_idx]]['win'] +1\n        else:\n            bandit_dict[observation['lastActions'][my_idx]]['loss'] = bandit_dict[observation['lastActions'][my_idx]]['loss'] +1\n        bandit_dict[observation['lastActions'][1-my_idx]]['opp'] = bandit_dict[observation['lastActions'][1-my_idx]]['opp'] +1\n        my_pull = get_next_bandit()\n    \n    return my_pull","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile greedy_desion_tree.py\nimport pickle\nimport random\n\nimport numpy as np\nimport pandas as pd\nimport sklearn.tree as skt\n\n# Parameters\nFUDGE_FACTOR = 0.99\nVERBOSE = False\nDATA_FILE = '/kaggle/input/sample-training-data/training_data_201223.parquet'\nTRAIN_FEATS = ['round_num', 'n_pulls_self', 'n_success_self', 'n_pulls_opp']\nTARGET_COL = 'payout'\n\n\ndef make_model():\n    \"\"\"Builds a decision tree model based on stored trainingd data\"\"\"\n    data = pd.read_parquet(DATA_FILE)\n    model = skt.DecisionTreeRegressor(min_samples_leaf=40)\n    model.fit(data[TRAIN_FEATS], data[TARGET_COL])\n    return model\n\n\nclass GreedyStrategy:\n    \"\"\"Implements strategy to maximize expected value\n\n    - Tracks estimated likelihood of payout ratio for each machine\n    - Tracks number of pulls on each machine\n    - Chooses machine based on maximum expected value\n    \n    \n    \"\"\"\n    def __init__(self, name, agent_num, n_machines):\n        \"\"\"Initialize and train decision tree model\n\n        Args:\n           name (str):   Name for the agent\n           agent_num (int):   Assigned player number\n           n_machines (int):   number of machines in the game\n        \n        \"\"\"\n        # Record inputs\n        self.name = name\n        self.agent_num = agent_num\n        self.n_machines = n_machines\n        \n        # Initialize distributions for all machines\n        self.n_pulls_self = np.array([0 for _ in range(n_machines)])\n        self.n_success_self = np.array([0. for _ in range(n_machines)])\n        self.n_pulls_opp = np.array([0 for _ in range(n_machines)])\n\n        # Track other players moves\n        self.opp_moves = []\n        \n        # Track winnings\n        self.last_reward_count = 0\n\n        # Create model to predict expected reward\n        self.model = make_model()\n        \n        # Predict expected reward\n        features = np.zeros((self.n_machines, 4))\n        features[:, 0] = len(self.opp_moves)\n        features[:, 1] = self.n_pulls_self\n        features[:, 2] = self.n_success_self\n        features[:, 3] = self.n_pulls_opp\n        self.predicts = self.model.predict(features)\n        \n\n    def __call__(self):\n        \"\"\"Choose machine based on maximum expected payout\n\n        Returns:\n           <result> (int):  index of machine to pull\n        \n        \"\"\"\n        # Otherwise, use best available\n        est_return = self.predicts\n        max_return = np.max(est_return)\n        result = np.random.choice(np.where(\n            est_return >= FUDGE_FACTOR * max_return)[0])\n        \n        if VERBOSE:\n            print('  - Chose machine %i with expected return of %3.2f' % (\n                int(result), est_return[result]))\n\n        return int(result)\n    \n        \n    def updateDist(self, curr_total_reward, last_m_indices):\n        \"\"\"Updates estimated distribution of payouts\"\"\"\n        # Compute last reward\n        last_reward = curr_total_reward - self.last_reward_count\n        self.last_reward_count = curr_total_reward\n        if VERBOSE:\n            print('Last reward: %i' % last_reward)\n\n        if len(last_m_indices) == 2:\n            # Update number of pulls for both machines\n            m_index = last_m_indices[self.agent_num]\n            opp_index = last_m_indices[(self.agent_num + 1) % 2]\n            self.n_pulls_self[m_index] += 1\n            self.n_pulls_opp[opp_index] += 1\n\n            # Update number of successes\n            self.n_success_self[m_index] += last_reward\n            \n            # Update opponent activity\n            self.opp_moves.append(opp_index)\n\n            # Update predictions for chosen machines\n            self.predicts[[opp_index, m_index]] = self.model.predict([\n                [\n                    len(self.opp_moves),\n                    self.n_pulls_self[opp_index],\n                    self.n_success_self[opp_index],\n                    self.n_pulls_opp[opp_index]\n                ],\n                [\n                    len(self.opp_moves),\n                    self.n_pulls_self[m_index],\n                    self.n_success_self[m_index],\n                    self.n_pulls_opp[m_index]\n                ]])\n            \n\ndef agent(observation, configuration):\n    global curr_agent\n    \n    if observation.step == 0:\n        # Initialize agent\n        curr_agent = GreedyStrategy(\n            'Mr. Agent %i' % observation['agentIndex'],\n            observation['agentIndex'],\n            configuration['banditCount'])\n    \n    # Update payout ratio distribution with:\n    curr_agent.updateDist(observation['reward'], observation['lastActions'])\n\n    return curr_agent()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile santa3.py\n\nimport numpy as np\nfrom scipy.stats import beta\n\n# AGENT CONFIG\n\n\n# How do we evaluate the bandit performance ? \nest_method = ['mean','thompson','ucb'][2]      # choosing UCB here\nest_ucb_percentile = 0.75                      # percentile for UCB : higher is more optimistic\n\n# Having evaluated the bandits, how do we select a candidate (exploration / exploitation) ?\npick_method = ['epsilon','greedy','weighted','random','stupid'][0]    # choosing epsilon greedy here\npick_weighted_alpha = 1                                               # for the weighted sampling, higher is greedier\npick_epsilon = 0.5                                                    # for epsilon-greedy, higher is more exploration\npick_epsilon_decay = 0.997                                            # lower means we shift to exploitation faster\n\n# How can we use the information provided by the actions of the other / rival bot?\nmin_opp_quality = 0.2     # higher means we believe the other bot knows what they are doing, even if they seem to play poorly\nopp_retry_factor = 1.     # higher means we care about exploiting a bandit found by the other bot (stealing) more than a bandit we identified\n\n# GAME CONFIG\n\ndecay = 0.97\nn_levers = None\n\n# Global variables, will be explained below, as they are initialised\nmy_score = 0      # keep track of my score\nbeta_a = None\nbeta_b = None\nmy_pulls = None\nopp_pulls = None\nall_pulls = None\nopp_quality = 0   # this will reflect how well we believe the other bot is playing\n\n\n# EXECUTION\n\ndef logic(observation, configuration):\n    \n    global n_levers,my_score, beta_a, beta_b, my_pulls, opp_pulls,all_pulls, pick_epsilon\n    \n    # FIRST ROUND ?\n    \n    if observation.step == 0: \n        \n        # We initialise the global vars\n        \n        n_levers = configuration.banditCount   # Number of bandits\n        \n        beta_a = np.ones(n_levers)             # Beta distribution with parameters (1,1),\n        beta_b = np.ones(n_levers)             # means we have a uniform prior on the probability of each bandit\n        \n        my_pulls = np.zeros(n_levers)          # We keep track of how many times we pull each bandit\n        opp_pulls = np.zeros(n_levers)         # Same for the rival bot\n        all_pulls = np.zeros(n_levers)         # Same across both bots\n        \n    else:  \n\n        # We update our knowledge\n        \n        my_choice,opp_choice = get_actions(observation)  # what did we each play at the previous round ?\n        my_reward = compute_reward(observation)          # did I get a reward ?\n        \n        beta_a[my_choice] += my_reward                   # we compute the posterior distribution,\n        beta_b[my_choice] += 1 - my_reward               # ignoring the decay for now (dealt with later)\n        \n        my_pulls[my_choice] += 1                         # Update how many times the bandits were pulled\n        opp_pulls[opp_choice] += 1\n        all_pulls[my_choice] += 1\n        all_pulls[opp_choice] += 1\n        \n    a,b = merge_all_info()                               # What is the best estimate we can get for the distribution of each bandit\n                                                         # using both what we learnt from the rewards, but also the rival bot's actions\n    my_est = compute_est(a,b)                            # We sample an estimate for each bandit from these distributions\n    decayed_est = my_est * decay**all_pulls              # We decay the estimates based on how many times the bandits were used\n    my_choice = pick_bandit(decayed_est)                 # We pick one using the chosen strategy ()\n\n    pick_epsilon  *= pick_epsilon_decay                  # We progressively favour exploitation vs exploration    \n    \n    return int(my_choice)\n\n\n# MECHANICS    \n\n\ndef compute_est(a,b):\n    # Given some distributions for each bandit, how do we compute the estimate ?\n\n    if est_method == 'thompson':      # we sample from the distribution\n        return np.random.beta(a, b)\n    elif est_method == 'ucb':         # we pick the value at percentile X\n        \n        # Note : the Bayesian UCB sampler template in the competition is written as \n        # post_a / (post_a + post_b) + beta.std(post_a, post_b) * c\n        # which I don't understand (eg could give values >> 1 for c large enough)\n        # if anyone can explain, please let me know\n        # I am using the PPF here, as it makes more sense to me\n        \n        return beta.ppf(est_ucb_percentile,a,b)\n    elif est_method == 'mean':        # we pick the mean, this ignores the uncertainty\n        return a / (a + b)\n\n\ndef pick_bandit(est_prob,pick_method=pick_method):\n    # Given some estimes, how do we pick our candidate ?\n    \n    if pick_method == 'greedy':          # always pick the highest\n        return int(np.argmax(est_prob))\n    elif pick_method == 'epsilon':       # same, but sometimes explore\n        if np.random.random() < pick_epsilon:\n                                         # exploration is done via the weighted method\n            return pick_bandit(est_prob,pick_method='weighted')  \n        else:                            # default to greedy \n            return pick_bandit(est_prob,pick_method='greedy')\n    elif pick_method == 'weighted':      # we will pick high estimates more often than low ones\n        p = est_prob**pick_weighted_alpha\n        p = p / p.sum()\n        return np.random.choice(range(len(est_prob)),p=p)\n    elif pick_method == 'random':        # pure random\n        return np.random.choice(range(len(est_prob)))\n    elif pick_method == 'stupid':        # always pick the lowest / worst\n        return int(np.argmin(est_prob))\n\n\n\n    \n\n# INCORPORATE OPPONENT INFORMATION\n\ndef compute_opp_quality():\n    # How well is the opponent playing ?\n    # Should we use their choices to inform our knowledge ?\n    \n    global opp_quality\n    # What do we independly believe about the bandits, based on what we observed ?\n    indep_est = compute_est(beta_a,beta_b)\n    # How well do the rival's actions correlate with our knowledge ?\n    # Ie did they pull the right bandits ?\n    opp_quality = np.corrcoef(opp_pulls,indep_est)[0,1]\n    # Note : this can be improved, as it ignores\n    # - what they can not know, ie the bandits they never pulled\n    # - the decay of the bandits, ie they may have pulled lots from a bandit that is now very low probability\n\ndef merge_all_info():\n    # How do we bring together\n    # - what we observed\n    # - what we can infer from the rival's actions ?\n    \n    # How good / believable is the opponent ?\n    compute_opp_quality()\n    # We will use their information based on :\n    # - our estimate of the opponent quality\n    # - a minimum value (to give them the benefit of doubt, esp early in the game)\n    # - how much we prefer to steal / ruin their bandits vs exploiting the ones we found\n    opp_retry_value = max(min_opp_quality,opp_quality) * opp_retry_factor\n    \n    # The good bandits discovered by the rival are identified by them playing more than once\n    opp_wins = np.maximum(opp_pulls-1,0)\n    opp_losses = opp_pulls - opp_wins\n    \n    # we combine our estimate with the additional information\n    a = beta_a + opp_wins*opp_retry_value\n    b = beta_b + opp_losses*opp_retry_value\n    return a,b\n\n\n\n\n####### BORING\n\n\ndef compute_reward(observation):\n    global my_score\n    reward = observation.reward - my_score\n    my_score = observation.reward\n    return reward\n\ndef get_actions(obs):\n    opponentIndex = 1 - obs.agentIndex\n    oppAction = obs.lastActions[opponentIndex]\n    myAction = obs.lastActions[obs.agentIndex]\n    return myAction,oppAction\n\n\ndef agent(observation, configuration):\n    # just because this needs to be last\n    return logic(observation, configuration)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile oppon.py\nimport numpy as np\nimport pandas as pd\nimport random, os, datetime, math\n\ntotal_reward = 0\nbandit_dict = {}\n\ndef set_seed(my_seed=42):\n    os.environ['PYTHONHASHSEED'] = str(my_seed)\n    random.seed(my_seed)\n    np.random.seed(my_seed)\n\ndef get_next_bandit():\n    best_bandit = 0\n    best_bandit_expected = 0\n    for bnd in bandit_dict:\n        expect = (bandit_dict[bnd]['win'] - bandit_dict[bnd]['loss'] + bandit_dict[bnd]['opp'] - (bandit_dict[bnd]['opp']>0)*1.5) \\\n                 / (bandit_dict[bnd]['win'] + bandit_dict[bnd]['loss'] + bandit_dict[bnd]['opp']) \\\n                * math.pow(0.97, bandit_dict[bnd]['win'] + bandit_dict[bnd]['loss'] + bandit_dict[bnd]['opp'])#0.97\n        if expect > best_bandit_expected:\n            best_bandit_expected = expect\n            best_bandit = bnd\n    return best_bandit\n\nmy_action_list = []\nop_action_list = []\n\n\n\ndef multi_armed_probabilities(observation, configuration):\n    global total_reward, bandit_dict\n\n    my_pull = random.randrange(configuration['banditCount'])\n    if 0 == observation['step']:\n        set_seed()\n        total_reward = 0\n        bandit_dict = {}\n        for i in range(configuration['banditCount']):\n            bandit_dict[i] = {'win': 1, 'loss': 0, 'opp': 0}\n    else:\n        last_reward = observation['reward'] - total_reward\n        total_reward = observation['reward']\n        \n        my_idx = observation['agentIndex']\n        my_last_action = observation['lastActions'][my_idx]\n        op_last_action = observation['lastActions'][1-my_idx]\n        \n        my_action_list.append(my_last_action)\n        op_action_list.append(op_last_action)\n        \n        if 0 < last_reward:\n            bandit_dict[my_last_action]['win'] = bandit_dict[my_last_action]['win'] +1\n        else:\n            bandit_dict[my_last_action]['loss'] = bandit_dict[my_last_action]['loss'] +1\n        bandit_dict[op_last_action]['opp'] = bandit_dict[op_last_action]['opp'] +1\n        \n        if last_reward > 0:\n            my_pull = my_last_action\n        else:\n            if observation['step'] >= 4:   #4\n                if (my_action_list[-1] == my_action_list[-2]) and (my_action_list[-1] == my_action_list[-3]):\n                    if random.random() < 0.5: #0.5\n                        my_pull = my_action_list[-1]\n                    else:\n                        my_pull = get_next_bandit()\n                else:\n                    my_pull = get_next_bandit()\n            else:\n                my_pull = get_next_bandit()\n    \n    return my_pull","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile oppon1.py\nimport numpy as np\nimport pandas as pd\nimport random, os, datetime, math\n\ntotal_reward = 0\nbandit_dict = {}\n\nfac1=1.44#1.44,1.45,1.5\n\ndef set_seed(my_seed=42):\n    os.environ['PYTHONHASHSEED'] = str(my_seed)\n    random.seed(my_seed)\n    np.random.seed(my_seed)\n\ndef get_next_bandit():\n    best_bandit = 0\n    best_bandit_expected = 0\n    for bnd in bandit_dict:\n        expect = (bandit_dict[bnd]['win'] - bandit_dict[bnd]['loss'] + bandit_dict[bnd]['opp'] - (bandit_dict[bnd]['opp']>0)*fac1) \\\n                 / (bandit_dict[bnd]['win'] + bandit_dict[bnd]['loss'] + bandit_dict[bnd]['opp']) \\\n                * math.pow(0.98, bandit_dict[bnd]['win'] + bandit_dict[bnd]['loss'] + bandit_dict[bnd]['opp'])#0.96,0.97\n        if expect > best_bandit_expected:\n            best_bandit_expected = expect\n            best_bandit = bnd\n    return best_bandit\n\nmy_action_list = []\nop_action_list = []\n\n\n\ndef multi_armed_probabilities(observation, configuration):\n    global total_reward, bandit_dict\n\n    my_pull = random.randrange(configuration['banditCount'])\n    if 0 == observation['step']:\n        set_seed()\n        total_reward = 0\n        bandit_dict = {}\n        for i in range(configuration['banditCount']):\n            bandit_dict[i] = {'win': 1, 'loss': 0, 'opp': 0}\n    else:\n        last_reward = observation['reward'] - total_reward\n        total_reward = observation['reward']\n        \n        my_idx = observation['agentIndex']\n        my_last_action = observation['lastActions'][my_idx]\n        op_last_action = observation['lastActions'][1-my_idx]\n        \n        my_action_list.append(my_last_action)\n        op_action_list.append(op_last_action)\n        \n        if 0 < last_reward:\n            bandit_dict[my_last_action]['win'] = bandit_dict[my_last_action]['win'] +1\n        else:\n            bandit_dict[my_last_action]['loss'] = bandit_dict[my_last_action]['loss'] +1\n        bandit_dict[op_last_action]['opp'] = bandit_dict[op_last_action]['opp'] +1\n        \n        if last_reward > 0:\n            my_pull = my_last_action\n        else:\n            if observation['step'] >= 4:   #4\n                if (my_action_list[-1] == my_action_list[-2]) and (my_action_list[-1] == my_action_list[-3]):\n                    if random.random() < 0.54: #0.54, 0.53,0.5\n                        my_pull = my_action_list[-1]\n                    else:\n                        my_pull = get_next_bandit()\n                else:\n                    my_pull = get_next_bandit()\n            else:\n                my_pull = get_next_bandit()\n    \n    return my_pull","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile pull1.py\nimport numpy as np\nimport pandas as pd\nimport random, os, datetime, math\nfrom collections import defaultdict\n\ntotal_reward = 0\nbandit_dict = {}\n\ndef set_seed(my_seed=42):\n    os.environ['PYTHONHASHSEED'] = str(my_seed)\n    random.seed(my_seed)\n    np.random.seed(my_seed)\n\ndef get_next_bandit():\n    best_bandit = 0\n    best_bandit_expected = 0\n    for bnd in bandit_dict:\n        expect = (bandit_dict[bnd]['win'] - bandit_dict[bnd]['loss'] + bandit_dict[bnd]['opp'] - (bandit_dict[bnd]['opp']>0)*1.5 + bandit_dict[bnd]['op_continue']) \\\n                 / (bandit_dict[bnd]['win'] + bandit_dict[bnd]['loss'] + bandit_dict[bnd]['opp']) \\\n                * math.pow(0.97, bandit_dict[bnd]['win'] + bandit_dict[bnd]['loss'] + bandit_dict[bnd]['opp'])\n        if expect > best_bandit_expected:\n            best_bandit_expected = expect\n            best_bandit = bnd\n    return best_bandit\n\nmy_action_list = []\nop_action_list = []\n\nop_continue_cnt_dict = defaultdict(int)\n\ndef multi_armed_probabilities(observation, configuration):\n    global total_reward, bandit_dict\n\n    my_pull = random.randrange(configuration['banditCount'])\n    if 0 == observation['step']:\n        set_seed()\n        total_reward = 0\n        bandit_dict = {}\n        for i in range(configuration['banditCount']):\n            bandit_dict[i] = {'win': 1, 'loss': 0, 'opp': 0, 'my_continue': 0, 'op_continue': 0}\n    else:\n        last_reward = observation['reward'] - total_reward\n        total_reward = observation['reward']\n        \n        my_idx = observation['agentIndex']\n        my_last_action = observation['lastActions'][my_idx]\n        op_last_action = observation['lastActions'][1-my_idx]\n        \n        my_action_list.append(my_last_action)\n        op_action_list.append(op_last_action)\n        \n        if 0 < last_reward:\n            bandit_dict[my_last_action]['win'] = bandit_dict[my_last_action]['win'] +1\n        else:\n            bandit_dict[my_last_action]['loss'] = bandit_dict[my_last_action]['loss'] +1\n        bandit_dict[op_last_action]['opp'] = bandit_dict[op_last_action]['opp'] +1\n        \n        if observation['step'] >= 3:\n            if my_action_list[-1] == my_action_list[-2]:\n                bandit_dict[my_last_action]['my_continue'] += 1\n            else:\n                bandit_dict[my_last_action]['my_continue'] = 0\n            if op_action_list[-1] == op_action_list[-2]:\n                bandit_dict[op_last_action]['op_continue'] += 1\n            else:\n                bandit_dict[op_last_action]['op_continue'] = 0\n        \n        if last_reward > 0:\n            my_pull = my_last_action\n        else:\n            if observation['step'] >= 4:\n                if (my_action_list[-1] == my_action_list[-2]) and (my_action_list[-1] == my_action_list[-3]):\n                    if random.random() < 0.5:\n                        my_pull = my_action_list[-1]\n                    else:\n                        my_pull = get_next_bandit()\n                else:\n                    my_pull = get_next_bandit()\n            else:\n                my_pull = get_next_bandit()\n    \n    return my_pull","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile pull2.py\nimport numpy as np\nimport pandas as pd\nimport random, os, datetime, math\nfrom collections import defaultdict\n\ntotal_reward = 0\nbandit_dict = {}\n\ndef set_seed(my_seed=40):\n    os.environ['PYTHONHASHSEED'] = str(my_seed)\n    random.seed(my_seed)\n    np.random.seed(my_seed)\n\ndef get_next_bandit():\n    best_bandit = 0\n    best_bandit_expected = 0\n    for bnd in bandit_dict:\n        expect = (bandit_dict[bnd]['win'] - bandit_dict[bnd]['loss'] + bandit_dict[bnd]['opp'] - (bandit_dict[bnd]['opp']>0)*1.5 + bandit_dict[bnd]['op_continue']) \\\n                 / (bandit_dict[bnd]['win'] + bandit_dict[bnd]['loss'] + bandit_dict[bnd]['opp']) \\\n                * math.pow(0.97, bandit_dict[bnd]['win'] + bandit_dict[bnd]['loss'] + bandit_dict[bnd]['opp'])\n        if expect > best_bandit_expected:\n            best_bandit_expected = expect\n            best_bandit = bnd\n    return best_bandit\n\nmy_action_list = []\nop_action_list = []\n\nop_continue_cnt_dict = defaultdict(int)\n\ndef multi_armed_probabilities(observation, configuration):\n    global total_reward, bandit_dict\n\n    my_pull = random.randrange(configuration['banditCount'])\n    if 0 == observation['step']:\n        set_seed()\n        total_reward = 0\n        bandit_dict = {}\n        for i in range(configuration['banditCount']):\n            bandit_dict[i] = {'win': 1, 'loss': 0, 'opp': 0, 'my_continue': 0, 'op_continue': 0}\n    else:\n        last_reward = observation['reward'] - total_reward\n        total_reward = observation['reward']\n        \n        my_idx = observation['agentIndex']\n        my_last_action = observation['lastActions'][my_idx]\n        op_last_action = observation['lastActions'][1-my_idx]\n        \n        my_action_list.append(my_last_action)\n        op_action_list.append(op_last_action)\n        \n        if 0 < last_reward:\n            bandit_dict[my_last_action]['win'] = bandit_dict[my_last_action]['win'] +1\n        else:\n            bandit_dict[my_last_action]['loss'] = bandit_dict[my_last_action]['loss'] +1\n        bandit_dict[op_last_action]['opp'] = bandit_dict[op_last_action]['opp'] +1\n        \n        if observation['step'] >= 3: #3\n            if my_action_list[-1] == my_action_list[-2]:\n                bandit_dict[my_last_action]['my_continue'] += 1\n            else:\n                bandit_dict[my_last_action]['my_continue'] = 0\n            if op_action_list[-1] == op_action_list[-2]:\n                bandit_dict[op_last_action]['op_continue'] += 1\n            else:\n                bandit_dict[op_last_action]['op_continue'] = 0\n        \n        if last_reward > 0:\n            my_pull = my_last_action\n        else:\n            if observation['step'] >= 4: #4\n                if (my_action_list[-1] == my_action_list[-2]) and (my_action_list[-1] == my_action_list[-3]):\n                    if random.random() < 0.45: #0.5\n                        my_pull = my_action_list[-1]\n                    else:\n                        my_pull = get_next_bandit()\n                else:\n                    my_pull = get_next_bandit()\n            else:\n                my_pull = get_next_bandit()\n    \n    return my_pull","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile out2.py\nimport random\nimport numpy as np\n\nEXPLORE_STEPS = 23 # count of repeats of random selection before start of main algorithm\nFIRST_SELECTION = 2 \n\nSTART_TAU = 30.92623082874138 \nTAU_MULT = 0.9743893312107584\n\n\nROUNDS = 2000\n\n\nc_arr = np.empty(ROUNDS) # array of coefs 1, 0.97, 0.97^2, ...\nc_arr[0] = 1\nfor i in range(1, c_arr.size):\n    c_arr[i] = c_arr[i-1]*0.97\n\nx_arr = np.linspace(0, 100, 101) # net of predicted thresholds\n\n\ndef fast_choice(options, probs):\n    x = random.random()#np.random.rand()\n    cum = 0\n    for i, p in enumerate(probs):\n        cum += p\n        if x < cum:\n            return options[i]\n    return options[-1]\n\ndef probsnorm(x):\n    return x/x.sum()\n\ndef softmax(x, tau):\n    x2 = x/tau\n    e = np.exp(x2 - x2.max())\n    return e/e.sum()\n\n\n\n\nif True:   \n\n\n    ROUNDS = 2000\n\n\n    c_arr = np.empty(ROUNDS) # array of coefs 1, 0.97, 0.97^2, ...\n    c_arr[0] = 1\n    for i in range(1, c_arr.size):\n        c_arr[i] = c_arr[i-1]*0.97\n\n    x_arr = np.linspace(0, 100, 101) # net of predicted thresholds\n    tau = START_TAU\n \n    #@profile\n    def get_sample_probs(array, probs, best_of):\n\n        p = probsnorm(probs)# to probability form\n\n        args = np.argsort(p)[-best_of:] # select best_of values with biggest probs\n\n        # return array[np.random.choice(args, 1, p = softmax(p[args]))[0]]\n        return array[fast_choice(args, probsnorm(p[args]))]\n    \n    def get_sample_softmax(array, probs):\n        global tau\n        tau *= TAU_MULT\n        \n        p = softmax(probs, tau)# to probability form\n\n        # return array[np.random.choice(args, 1, p = softmax(p[args]))[0]]\n        return fast_choice(array, p)\n    \n\n    cached_x = {}\n    def get_floor_x(c):\n        global cached_x\n        if c in cached_x:\n            return cached_x[c]\n        \n        arr = np.floor(x_arr * c_arr[c])\n        cached_x[c] = arr\n        return arr\n\n\n\n    BANDITS = 100 # count of bandits\n\n    bandits_counts = np.zeros(BANDITS, dtype = np.int16) # choices count for each bandit\n\n    probs = np.ones((BANDITS, x_arr.size)) # matrix bandit*threshold probs\n\n    bandits_indexes = np.arange(BANDITS)\n\n    start_bandits = np.random.choice(bandits_indexes, int(BANDITS*EXPLORE_STEPS/3), replace = True) # just start random sequence of bandits selection before start of main algorithm\n\n\n\n    my_last_action = 0\n    #@profile\n    def update_counts(act1, act2, my_reward):\n        global bandits_counts, probs\n        opp = [act != my_last_action for act in (act1, act2)]\n        opp = (act1, act2)[opp[0]] if len(opp) > 0 else my_last_action\n\n        mlt = get_floor_x(bandits_counts[my_last_action])/100\n\n        if my_reward == 1:\n            probs[my_last_action, :] *= mlt\n        else:\n            probs[my_last_action, :] *= 1 - mlt\n\n        bandits_counts[my_last_action] += 1\n        bandits_counts[opp] += 1\n    #@profile\n    def get_best_action():\n\n        #inds = np.unravel_index(probs.argmax(), probs.shape)\n\n        #return inds[0] # select best bandit\n\n\n        #likeh = np.array([np.argmax(probs[i, :]) for i in range(BANDITS)])\n\n        #likeh = np.array([x_arr[ind]*c_arr[b]*probs[bandit, ind]/probs[bandit, :].sum() for bandit, (ind, b) in enumerate(zip(likeh, bandit_counts))])\n\n        likeh = np.array([get_sample_probs(get_floor_x(b), probs[bandit, :], FIRST_SELECTION) for bandit, b in enumerate(bandits_counts)])\n\n        return get_sample_softmax(bandits_indexes, likeh)# if random.random() < PROB else random.randrange(BANDITS)    \n\n\n\n    last_reward = 0\n    #@profile\n    def pasa_agent(observation, configuration):\n\n        global BANDITS, start_bandits, bandits_counts, probs, last_reward, bandits_indexes, my_last_action\n\n        if observation.step == 0:\n\n            BANDITS = configuration.banditCount\n            #print(f\"there are {BANDITS} bandits\")\n\n            bandits_indexes = np.arange(BANDITS, dtype = np.int16)   \n\n            start_bandits = np.random.choice(bandits_indexes, int(BANDITS*EXPLORE_STEPS/3), replace = True)\n\n            bandits_counts = np.zeros(BANDITS, dtype = np.int16)\n\n            probs = np.ones((BANDITS, x_arr.size))\n\n\n            my_last_action = start_bandits[0]\n\n        elif observation.step < start_bandits.size:\n\n            update_counts(int(observation.lastActions[0]), int(observation.lastActions[1]), observation.reward - last_reward)\n\n            my_last_action = start_bandits[observation.step]\n\n        else:\n\n            update_counts(int(observation.lastActions[0]), int(observation.lastActions[1]), observation.reward - last_reward)\n\n            my_last_action = get_best_action()\n\n\n        last_reward = observation.reward \n        my_last_action = int(my_last_action)\n\n        return my_last_action","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5-round comparison"},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_rounds(file1, file2, N=15):\n    env = make(\"mab\", debug=True)\n    per=0\n\n    for i in range(N):\n        env.run([file1, file2])\n        p1_score = env.steps[-1][0]['reward']\n        p2_score = env.steps[-1][1]['reward']\n        if (p1_score>p2_score):\n            per+=1\n        env.reset()\n        print(f\"Round {i+1}: {p1_score} - {p2_score}\")\n    print(f\"Percentage={per/N}\")   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Default vs Thompson Sampling')\n#print_rounds(\"../input/santa-2020/submission.py\", \"thompson.py\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Bayesian UCB vs Thompson Sampling')\n#print_rounds(\"../input/santa-2020-ucb-and-bayesian-ucb-starter/bayesian_ucb.py\", \"thompson.py\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('epsilon-greedy+decay vs Thompson Sampling')\n#print_rounds(\"../input/santa-2020-epsilon-greedy-starter/epsilon_greedy_decay.py\", \"thompson.py\")\n#print_rounds(\"epsilon_greedy_decay.py\", \"thompson.py\")\n#print_rounds(\"epsilon_greedy_decay.py\", \"../input/santa-2020/submission.py\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('epsilon-greedy+decay vs Thompson Sampling')\n#print_rounds(\"../input/santa-2020-epsilon-greedy-starter/epsilon_greedy_decay.py\", \"thompson.py\")\n#print_rounds(\"multi_armed.py\", \"thompson.py\")\n#print_rounds(\"multi_armed.py\", \"../input/santa-2020/submission.py\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('epsilon-greedy+decay vs Thompson Sampling')\n#print_rounds(\"../input/santa-2020-epsilon-greedy-starter/epsilon_greedy_decay.py\", \"thompson.py\")\n#print_rounds(\"agent2.py\", \"thompson.py\")\n#print_rounds(\"multi_armed.py\", \"../input/santa-2020/submission.py\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('epsilon-greedy+decay vs Thompson Sampling')\n#print_rounds(\"../input/santa-2020-epsilon-greedy-starter/epsilon_greedy_decay.py\", \"thompson.py\")\n#print_rounds(\"multi_armed.py\", \"agent3.py\")\n#print_rounds(\"agent3.py\", \"../input/santa-2020/submission.py\")\n#print_rounds(\"agent3.py\", \"thompson.py\")\n#print_rounds(\"oppon1.py\", \"oppon.py\")\n#print_rounds(\"oppon1.py\", \"pull1.py\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}