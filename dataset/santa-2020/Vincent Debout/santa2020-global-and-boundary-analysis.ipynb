{"cells":[{"metadata":{},"cell_type":"markdown","source":"These are global analysis to determine important boundaries and metrics on the Santa 2020 contest. This is no discussion on how to build the best AI for this."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# package loading\nimport sys\nimport numpy as np\nimport matplotlib.pyplot as pl\nfrom scipy.stats import linregress\n\nnp.random.seed(714)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# main dimensions\n\nNB_SLOT = 100\nDECAY = 0.97\nNB_TURN = 2000","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Initial probabilities\n\nInitially, all the 100 machines have a random probability to deliver a candy cane. Let's denote $P_i$ the initial delivering probability of the $i^{th}$ machine. The different machines probabilities are independant random variables uniformely picked on **[0, 1]**.\nSo every machine has an expected $P_i = 0.5$ with standard deviation $\\sigma = \\sqrt{\\frac{1}{12}} = 0.289$.\n\n* As the $P_i$ are independant, the 100 machines have an expected average of $P_{mean} = mean(P_i) = 0.5$ with a standard deviation of $\\sigma_{100} = 0.029$ (have a look at [Bates distribution](https://en.wikipedia.org/wiki/Bates_distribution) shapes)."},{"metadata":{},"cell_type":"markdown","source":"# Decay process"},{"metadata":{"trusted":true},"cell_type":"code","source":"# compute cumulative decaying\ndecaying = np.power(DECAY, np.arange(NB_TURN*2))\ndecaying_cumsum = np.cumsum(decaying)\n\n\n\n\nfig = pl.figure()\nax = fig.gca()\nax.grid()\nax.plot(decaying[:200])\nax.set_ylabel(\"decay\")\nax2 = ax.twinx()\nax2.plot(decaying_cumsum[:200], color=\"r\")\nax2.axhline(33.333333, color=\"g\")\nax2.set_ylabel(\"decay cumsum\")\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The convergence of the decay cumulative sum graph gives us an interesting information: the expectated total number of candy canes a machine may deliver, is $33.333 P_i$. It can be analitically calculated with the geometric series : $\\frac{1}{1-0.97} = 33.333$.\n\nThus, statiscally, in an average game ($P_{mean} = 0.5$) there is $1666.66$ available candy canes ($833.33$ per player). Note that this boundary assumes pulls are not simultaneous, which should not make big difference (as same pulling is quite rare). And we can estimate that $13000$ pulls would be"},{"metadata":{},"cell_type":"markdown","source":"# Depleting\n\nWhen probability is lower than $1 \\%$, it remains less than $0.333$ expected candy canes in the machine, which can reasonably considered as depleted (this is an arbitrary threshold)."},{"metadata":{"trusted":true},"cell_type":"code","source":"pull_to_deplete = np.zeros((101), dtype=\"i4\")\npull_to_deplete[1:] = len(decaying)-1-np.searchsorted(decaying[::-1], 1./np.arange(100,0,-1))[::-1]\nprint(\"decay 100% to 1%:\", pull_to_deplete[99])\nprint(\"decay 50% to 1%:\",pull_to_deplete[49])\nprint(\"decay 10% to 1%:\", pull_to_deplete[9])\nprint(\"mean %.2f std %2f\" %(pull_to_deplete.mean(), pull_to_deplete.std()))\n\n\npl.figure()\npl.grid()\npl.plot(range(101), pull_to_deplete)\npl.xlabel(\"$P_i$\")\npl.ylabel(\"pull to deplete\")\npl.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It takes $129$ and $152$ pulls to deplete respectively machines starting at $P_i = 1.0$ and $P_i = 0.5$. In average, $117.72$ pulls are necessary to deplete a machine, and so $11772$ pulls to deplete all machines. Thus a game of 2000 steps, so 4000 pulls is far from depleting all available candy canes."},{"metadata":{},"cell_type":"markdown","source":"# Game simulator"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Configuration(object):\n    \"\"\" configuration interaction object\"\"\"\n    \n    def __init__(self):\n        self.decayRate = DECAY\n        self.episodeSteps = NB_TURN\n        self.banditCount = NB_SLOT\n        self.actTimeout = 0.25\n        self.sampleResolution = 100\n\n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n\nclass Observation(object):\n    \"\"\" observation interaction object\"\"\"\n    \n    def __init__(self, step, reward, last_turn, probas):\n        self.step = step\n        self.reward = reward\n        self.last_turn = last_turn\n        self.probas = probas\n        self.remainingOverageTime = 60\n        \n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n\n\ndef one_game(configuration, bots, probas0=None):\n    \"\"\" launch a game \n    return scores\n    \"\"\"\n    \n    # initialize    \n    nb_player = len(bots)\n    probas0 = probas0 or np.round(np.random.rand(NB_SLOT), 2)    \n    probas = probas0.copy()\n    scores = [0]*nb_player\n    expected_scores = [0]*nb_player\n    last_pick = [None]*nb_player    \n    \n    plays = [[] for p in range(nb_player)]\n    \n    \n    def give_turn(p):\n        \"\"\" give turn to player p\"\"\"\n        \n        observation = Observation(turn, scores[p], last_pick, probas)\n        observation.magic = probas\n        ind = bots[p](observation, configuration)\n        plays[p].append(ind)\n        get = np.random.rand() < probas[ind]\n        scores[p] += int(get)\n        expected_scores[p] += probas[ind]\n        pick.append(ind)\n        return get\n\n    # loop on turns\n    for turn in range(NB_TURN):\n        pick = []\n        \n        for p in range(nb_player):\n            give_turn(p)\n        for ind in pick:\n            probas[ind] *= DECAY\n        last_pick = pick\n    \n    #print(\"BC0\", np.bincount(plays[0]))\n    #print(\"BC1\", np.bincount(plays[1]))\n    #print(\"scores\", scores)\n    return scores, expected_scores, probas0.mean(), np.sqrt(np.square(probas0).mean())         \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This simulator can handle any number of bots. Single game are useful to obtain some accurate metrics on a bot performance. It returns the result score but also tracks the expected score which is the sum of the probabilities when pulling. Now let's create some facilities to launch series of games and build statistics on it."},{"metadata":{"trusted":true},"cell_type":"code","source":"class Statistics(object):\n    \"\"\" Multi game statistics\"\"\"\n\n    def __init__(self, players, nb_run):\n        \n        self.players = players\n        self.nb_player = len(players)\n        self.nb_run = nb_run\n        \n        # arrays\n        self.scores = np.ones((self.nb_player, nb_run), \"i4\")*-1\n        self.exp_scores = np.ones((self.nb_player, nb_run), \"f8\")*-1\n        self.mean_probas = np.ones((nb_run), \"f8\")*-1\n        self.mean_probas2 = np.ones((nb_run), \"f8\")*-1        \n        self.bias = np.ones((self.nb_player, nb_run), \"f8\")*-1\n        \n    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#        \n    def __repr__(self):    \n        # statistics\n        txt = \"\"\n        for p in range(self.nb_player):\n            txt += \"%d) score    mean %.2f std %.2f\\n\" % (p, self.scores[p].mean(), self.scores[p].std())\n            txt += \"%d) expscore mean %.2f std %.2f\\n\" % (p, self.exp_scores[p].mean(), self.exp_scores[p].std()) \n            txt += \"%d) bias    mean %.2f std %.2f\\n\" % (p, self.bias[p].mean(), self.bias[p].std())\n        return txt\n        \n    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n        \n    def acquisition(self, ind, results, print_round=False):\n        \"\"\" acquire a one game result \"\"\"\n        \n        scores, exp_scores, gameval, gameval2 = results\n\n        self.mean_probas[ind] = gameval\n        self.mean_probas2[ind] = gameval2\n        for p in range(len(bots)):\n            self.scores[p, ind] = scores[p]\n            self.exp_scores[p, ind] = exp_scores[p]\n            self.bias[p, ind] = scores[p]-exp_scores[p]\n    \n        if print_round:\n            print(\"game %d / %d meanproba %.2f\" % (ind+1, self.nb_run, gameval), \"scores :\", scores, \"expscores\", np.round(exp_scores, 1))\n        \n        \n#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n\n\ndef multi_game(configuration, bots, N):\n    \"\"\" launch several games and collect statistics\"\"\"\n    \n    # initialization\n    nb_bot = len(bots)\n    stats = Statistics(bots, N)\n\n    # loop on games\n    for i in range(N):\n        results = one_game(configuration, bots)\n        print_round = i%(N//10) == 0\n        stats.acquisition(i, results, print_round)\n        \n    return stats","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Cheater bot with optimized exploitation\n\nOne can notice I added the current delivering probabilities to observation. This way, we can simulate a bot knowing current probabilities (like a cheater) to see how it performs. Using multi-armed bandit paradigm, this is like having exploration phase totally achieved and only exploitation to do. Well, in fact, it is an upper bound on how well can perform a bot as only luck can let another bot beat it.\n\nSo the bot code is quite simple isn't it ?"},{"metadata":{"trusted":true},"cell_type":"code","source":"def cheater(observation, configuration):\n    \"\"\" bot knowing exact current probabilities,\n    greedily pick the best.\n    \"\"\"\n    return np.argmax(observation.probas)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's run some games with a single cheater bot and a duel with two of them. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# single player games\nconfiguration = Configuration()\nbots = [cheater] \n\nsingle_stats = multi_game(configuration, bots, 1600)\nprint(\"SINGLE GAME\")\nprint(single_stats)\n\n\n# 2-players games\n\nconfiguration = Configuration()\nbots = [cheater, cheater] \n\nduel_stats = multi_game(configuration, bots, 1600)\nprint(\"DUEL GAME\")\nprint(duel_stats)\ndbias = duel_stats.bias[0]-duel_stats.bias[1]\n\nprint(\"bias1-bias2 mean %.2f std %.2f\" % (dbias.mean(), dbias.std()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, a single \"perfect\" bot gets on average $912$ candy canes. If they are two of them, they get each $652$ candy canes for a total of about $1300$ candy canes. The initial pool of $1666$ candy canes is indeed quite reduced. This also shows that the competition between the two players is really important. A given bot may perform very differently versus a strong and a weak opponent. I tested different bots and was surprised to discover that the ones performing well on single games were not necessary good in the duel. Furthermore, the duel performances do not let me establish a clear ranking (cheater bot always wins, don't worry !)."},{"metadata":{"trusted":true},"cell_type":"code","source":"pl.figure()\npl.grid()\npl.hist(dbias, 30, density=True, edgecolor=\"k\")\npl.xlabel(\"relative bias\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On the average of these many games, the bias between score and deserved score is very small but on a single game, the quadratic mean is about $20$ !!! The bias on the relative scores of the players reaches a quadratic mean of $27.4$ (about $\\sqrt 2$ times more), this is huge.\n\nJust keep it in mind when you run a single game to judge the efficiency of a new feature... Moreover, regular bots may also be misleaded on exploration phases. Thus, one should expect this bias to be even more important."},{"metadata":{},"cell_type":"markdown","source":"# Reduced scores ?\n\nThe global expected scores are very variable in the different games. It is evident that the initial probability distribution has an important role in this. Let's try to figure how. Hereinafter $P_{mean}$ is the mean initial probability (as before) and $P_{mean,2}$ is the quadratic mean.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = pl.subplots(2, 2, figsize=(12,12))\n\nX = np.array([0.4, 0.6])\nX2 = np.array([0.48, 0.67])\n\nslope, intercept, r_value, p_value, std_err = linregress(single_stats.mean_probas, single_stats.exp_scores[0])\naxs[0, 0].set_title(\"single $P_{mean}$ C=%.3f (a=%.1f, b=%.1f)\" % (r_value, slope, intercept))\naxs[0, 0].set_xlim(X)\naxs[0, 0].grid()\naxs[0, 0].scatter(single_stats.mean_probas, single_stats.exp_scores[0], marker=\"+\")\naxs[0, 0].plot(X, slope*X+intercept, \"r\")\n\nslope, intercept, r_value, p_value, std_err = linregress(single_stats.mean_probas2, single_stats.exp_scores[0])\naxs[0, 1].set_title(\"single $P_{mean, 2}$ C=%.3f (a=%.1f, b=%.1f)\" % (r_value, slope, intercept))\naxs[0, 1].set_xlim(X2)\naxs[0, 1].grid()\naxs[0, 1].scatter(single_stats.mean_probas2, single_stats.exp_scores[0], marker=\"+\")\naxs[0, 1].plot(X2, slope*X2+intercept, \"r\")\n\nslope, intercept, r_value, p_value, std_err = linregress(duel_stats.mean_probas, duel_stats.exp_scores[0])\naxs[1, 0].set_title(\"duel $P_{mean}$ C=%.3f (a=%.1f, b=%.1f)\" % (r_value, slope, intercept))\naxs[1, 0].grid()\naxs[1, 0].set_xlim(X)\naxs[1, 0].scatter(duel_stats.mean_probas, duel_stats.exp_scores[0], marker=\"+\")\naxs[1, 0].set_xlabel(\"$P_{mean}$\")\naxs[1, 0].plot(X, slope*X+intercept, \"r\")\n\nslope, intercept, r_value, p_value, std_err = linregress(duel_stats.mean_probas2, duel_stats.exp_scores[0])\naxs[1, 1].set_title(\"duel $P_{mean, 2}$ C=%.3f (a=%.1f, b=%.1f)\" % (r_value, slope, intercept))\naxs[1, 1].grid()\naxs[1, 1].set_xlim(X2)\naxs[1, 1].scatter(duel_stats.mean_probas2, duel_stats.exp_scores[0], marker=\"+\")\naxs[1, 1].set_xlabel(\"$P_{mean, 2}$\")\naxs[1, 1].plot(X2, slope*X2+intercept, \"r\")\n\npl.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"the global expected scores seem to be mainly driven by the quadratic mean. It indeed takes in acount that it is more efficient to exploit two machines $(P_i=1.0\\ \\&\\ P_j=0.0)$ than two machines $(P_i=0.5\\ \\&\\ P_j=0.5)$. Nevertheless, it is interesting to notice that the mean has significantly more influence on duel games than on single games. I assume that it is because depleting is more important and the bots have to use more often machines with low probability. If we imagine longer games of something like 5000 steps, nearly all the machines would be depleted at end and global score would be driven only by the mean."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}