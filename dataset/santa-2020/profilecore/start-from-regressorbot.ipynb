{"cells":[{"metadata":{},"cell_type":"markdown","source":"This notebook loads a dataset and trains a gradient boosting regressor to use to predict the thresholds.  The notebook plays a UCB and the vegas pull 2 bot.  It is not a very good bot, but with some work could be.  Commit the note and submit the submit.tar.gz file - it has the notebook and the model in the file."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/santa-train/train_santa.csv')\n\n#df = pd.read_csv('../input/santa-train/train_santa-1.csv')\n\n#df=df.sample(frac=0.15, replace=False, random_state=1)\n\n#df = df[df['pulls'] > 2]\n\ny = df['target'].values\n\n#df['step'] = df['step'].multiply(.0001)\n\nX = df.drop(['target','step','opp_pull'],axis=1).values\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.05,random_state=42)\n\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Setup a regressor\nhyper_params = {\n    'task': 'train',\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'metric': ['l2', 'auc'],\n    'learning_rate': 0.005,\n    'feature_fraction': 0.9,\n    'bagging_fraction': 0.7,\n    'bagging_freq': 10,\n    'verbose': 0,\n    \"max_depth\": 8,\n    \"num_leaves\": 128,  \n    \"max_bin\": 512,\n    \"num_iterations\": 100000,\n    \"n_estimators\": 1000\n}\n\n#reg = lgb.LGBMRegressor()\n\n#model=reg.fit(X_train, y_train)\n\n\n#reg = RandomForestRegressor()\nreg = GradientBoostingRegressor()\nmodel=reg.fit(X_train,y_train)\n\npred = model.predict(X_test)\n#Cost Function\nmse = mean_squared_error(y_test,pred)\nprint (reg.score(X_test,y_test))\nprint (mse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install kaggle-environments --upgrade\n\nfrom kaggle_environments import make\n\n!mkdir /kaggle_simulations\n!mkdir /kaggle_simulations/agent\n!mkdir /kaggle_simulations/agent/saved_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save the model to disk\nimport pickle\nfilename = '/kaggle_simulations/agent/saved_model/model.sav'\npickle.dump(model, open(filename, 'wb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile /kaggle_simulations/agent/main.py\n\nimport math\nfrom scipy.stats import beta\nimport numpy as np\nimport random\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n\n# load the model from disk\nimport pickle\nfilename = '/kaggle_simulations/agent/saved_model/model.sav'\nloaded_model = pickle.load(open(filename, 'rb'))\n\ntotal_reward = 0\n\ndef ucb_agent(observation, configuration):    \n    global wins, pulls, opp_pull, last_bandit, total_reward, opp_bandit\n    \n    if observation.step == 0:\n        wins = [0] * configuration[\"banditCount\"]\n        \n        pulls = [0] * configuration[\"banditCount\"]\n        \n        opp_pull = [0] * configuration[\"banditCount\"]\n        \n        opp_bandit=[]\n        \n        chosen_bandit = random.randint(0,99)\n        last_bandit = chosen_bandit\n        \n        return int(chosen_bandit)\n    \n    if observation.step > 0:\n        \n        player = int(last_bandit == observation.lastActions[1])\n        \n        if player==0:\n            opponent=1\n        else:\n            opponent=0\n        \n        reward = observation.reward - total_reward\n        total_reward += reward\n        \n        if reward == 1:\n            wins[last_bandit] += 1\n        pulls[last_bandit] += 1\n        opp_pull[observation['lastActions'][opponent]] += 1\n        \n        #if observation.step < 500 and reward ==1:\n            #return last_bandit\n        \n        opp_bandit.append(observation['lastActions'][opponent])\n        #'''\n        if len(opp_bandit)>15 and observation.step < 1500:\n            length=len(opp_bandit)\n            if opp_bandit[length-1] == opp_bandit[length-2] and opp_bandit[length-1] != opp_bandit[length-15]:\n                last_bandit=opp_bandit[length-1]\n                return opp_bandit[length-1]\n        #'''\n        best_proba = -1\n        best_agent = None\n        dat=[]\n        for k in range(configuration[\"banditCount\"]):\n            to_append = wins[k],pulls[k]#,opp_pull[k],observation.step\n            dat.append(to_append)\n            \n        proba = loaded_model.predict(dat)\n        \n        chosen_bandit = int(np.argmax(proba))\n        #'''\n        \n        if observation.step <= 800:\n            N = 10\n            res = sorted(range(len(proba)), key = lambda sub: proba[sub])[-N:]\n            chosen_bandit=random.choice(res)\n            \n        #''' \n        last_bandit = int(chosen_bandit)\n        return int(chosen_bandit)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile bayesian_ucb.py\n\nimport numpy as np\nfrom scipy.stats import beta\n\npost_a, post_b, bandit = [None] * 3\ntotal_reward = 0\nc = 3\n\ndef agent(observation, configuration):\n    global total_reward, bandit, post_a, post_b, c\n\n    if observation.step == 0:\n        post_a, post_b = np.ones((2, configuration.banditCount))\n    else:\n        r = (observation.reward - total_reward)\n        total_reward = observation.reward\n        # Update Gaussian posterior\n        post_a[bandit] += r\n        post_b[bandit] += 1 - r\n    \n    bound = post_a / (post_a + post_b) + beta.std(post_a, post_b) * c\n    bandit = int(np.argmax(bound))\n    \n    return bandit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile main.py\n\n\"\"\"Greedy agent that chooses machine based on maximum expected payout\n\nUses a trained decision tree model to consider the other player's movements\nin the expected payout.\n\nSee my other kernel for methodology for generating training data:\nhttps://www.kaggle.com/lebroschar/generate-training-data\n\n\"\"\"\nimport pickle\nimport base64\nimport random\nimport numpy as np\nimport pandas as pd\nimport sklearn.tree as skt\nimport sys\nimport os\n\nimport random, os, datetime, math\nfrom collections import defaultdict\n\n# Below is needed to submit tar.gz file to Kaggle.\nsys.path.append(\"/kaggle_simulations/agent\")\nworking_dir = \"/kaggle_simulations/agent\"\npath_to_model = os.path.join(working_dir,\"model.sav\")\n\n# Parameters\nFUDGE_FACTOR = 0.99\nVERBOSE = False\nDATA_FILE = '/kaggle/input/sample-training-data/training_data_201223.parquet'\nTRAIN_FEATS = ['round_num', 'n_pulls_self', 'n_success_self', 'n_pulls_opp']\nTARGET_COL = 'payout'\nfilename = 'model.sav'\n\n\nclass GreedyStrategy:\n    \"\"\"Implements strategy to maximize expected value\n\n    - Tracks estimated likelihood of payout ratio for each machine\n    - Tracks number of pulls on each machine\n    - Chooses machine based on maximum expected value\n    \n    \n    \"\"\"\n    def __init__(self, name, agent_num, n_machines):\n        \"\"\"Initialize and train decision tree model\n\n        Args:\n           name (str):   Name for the agent\n           agent_num (int):   Assigned player number\n           n_machines (int):   number of machines in the game\n        \n        \"\"\"\n        # Record inputs\n        self.name = name\n        self.agent_num = agent_num\n        self.n_machines = n_machines\n        \n        # Initialize distributions for all machines\n        self.n_pulls_self = np.array([0 for _ in range(n_machines)])\n        self.n_success_self = np.array([0. for _ in range(n_machines)])\n        self.n_pulls_opp = np.array([0 for _ in range(n_machines)])\n\n        # Track other players moves\n        self.opp_moves = []\n        \n        # Track winnings\n        self.last_reward_count = 0\n\n        # Load model from other file\n        self.model = pickle.load(open(path_to_model, 'rb'))\n        \n        # Predict expected reward\n        features = np.zeros((self.n_machines, 4))\n        features[:, 0] = len(self.opp_moves)\n        features[:, 1] = self.n_pulls_self\n        features[:, 2] = self.n_success_self\n        features[:, 3] = self.n_pulls_opp\n        self.predicts = self.model.predict(features)\n        \n\n    def __call__(self):\n        \"\"\"Choose machine based on maximum expected payout\n\n        Returns:\n           <result> (int):  index of machine to pull\n        \n        \"\"\"\n        # Otherwise, use best available\n        est_return_1 = self.predicts\n        \n        est_return_1=est_return_1/np.sum(est_return_1)\n        \n        est_return_2=[(bandit_dict[bnd]['win'] - bandit_dict[bnd]['loss'] + bandit_dict[bnd]['opp'] - (bandit_dict[bnd]['opp']>0)*1.5 + bandit_dict[bnd]['op_continue']) \\\n                     / (bandit_dict[bnd]['win'] + bandit_dict[bnd]['loss'] + bandit_dict[bnd]['opp']) \\\n                    * math.pow(0.97, bandit_dict[bnd]['win'] + bandit_dict[bnd]['loss'] + bandit_dict[bnd]['opp']) for bnd in bandit_dict]\n        est_return_2=est_return_2/np.sum(est_return_2)\n        \n        est_return=(2*est_return_1+est_return_2)/3\n        \n        \n        max_return = np.max(est_return)\n        result = np.random.choice(np.where(\n            est_return >= FUDGE_FACTOR * max_return)[0])\n        \n        if VERBOSE:\n            print('  - Chose machine %i with expected return of %3.2f' % (\n                int(result), est_return[result]))\n\n        return int(result)\n    \n        \n    def updateDist(self, curr_total_reward, last_m_indices):\n        \"\"\"Updates estimated distribution of payouts\"\"\"\n        # Compute last reward\n        last_reward = curr_total_reward - self.last_reward_count\n        self.last_reward_count = curr_total_reward\n        if VERBOSE:\n            print('Last reward: %i' % last_reward)\n\n        if len(last_m_indices) == 2:\n            # Update number of pulls for both machines\n            m_index = last_m_indices[self.agent_num]\n            opp_index = last_m_indices[(self.agent_num + 1) % 2]\n            self.n_pulls_self[m_index] += 1\n            self.n_pulls_opp[opp_index] += 1\n\n            # Update number of successes\n            self.n_success_self[m_index] += last_reward\n            \n            # Update opponent activity\n            self.opp_moves.append(opp_index)\n\n            # Update predictions for chosen machines\n            self.predicts[[opp_index, m_index]] = self.model.predict([\n                [\n                    len(self.opp_moves),\n                    self.n_pulls_self[opp_index],\n                    self.n_success_self[opp_index],\n                    self.n_pulls_opp[opp_index]\n                ],\n                [\n                    len(self.opp_moves),\n                    self.n_pulls_self[m_index],\n                    self.n_success_self[m_index],\n                    self.n_pulls_opp[m_index]\n                ]])\n            \n\ntotal_reward = 0\nbandit_dict = {}\nmy_action_list = []\nop_action_list = []\nop_continue_cnt_dict = defaultdict(int)\n\nreserve=0\ntrial=0\n\ndef agent(observation, configuration):\n    global total_reward, bandit_dict, curr_agent,reserve,trial\n    \n    if observation.step == 0:\n        # Initialize agent\n        curr_agent = GreedyStrategy('Mr. Agent %i' % observation['agentIndex'],\n            observation['agentIndex'],\n            configuration['banditCount'])\n    \n    # Update payout ratio distribution with:\n    curr_agent.updateDist(observation['reward'], observation['lastActions'])\n    \n    #pull vegas\n    my_pull = random.randrange(configuration['banditCount'])\n    if observation['step'] == 0:\n        total_reward = 0\n        bandit_dict = {}\n        for i in range(configuration['banditCount']):\n            bandit_dict[i] = {'win': 1, 'loss': 0, 'opp': 0, 'my_continue': 0, 'op_continue': 0}\n    else:\n        last_reward = observation['reward'] - total_reward\n        total_reward = observation['reward']\n        \n        my_idx = observation['agentIndex']\n        my_last_action = observation['lastActions'][my_idx]\n        op_last_action = observation['lastActions'][1-my_idx]\n        \n        my_action_list.append(my_last_action)\n        op_action_list.append(op_last_action)\n        \n        if last_reward > 0:\n            bandit_dict[my_last_action]['win'] += 1\n        else:\n            bandit_dict[my_last_action]['loss'] += 1\n        bandit_dict[op_last_action]['opp'] += 1\n        \n        if observation['step'] >= 3:\n            if my_action_list[-1] == my_action_list[-2]:\n                bandit_dict[my_last_action]['my_continue'] += 1\n            else:\n                bandit_dict[my_last_action]['my_continue'] = 0\n            if op_action_list[-1] == op_action_list[-2]:\n                bandit_dict[op_last_action]['op_continue'] += 1\n            else:\n                bandit_dict[op_last_action]['op_continue'] = 0\n        \n        if last_reward>0:\n            reserve+=1\n            trial+=1\n        elif reserve>0:\n            reserve-=1\n            trial+=1\n        \n        if (reserve/(trial+0.0000001)) > 0.8:\n            my_pull = my_last_action\n        else:\n            reserve=0\n            trial=0\n            if observation['step'] >= 4:\n                if (my_action_list[-1] == my_action_list[-2] == my_action_list[-3]):\n                    if random.random() < 0.5:\n                        my_pull = my_action_list[-1]\n                    else:\n                        my_pull = curr_agent()\n                else:\n                    my_pull = curr_agent()\n            else:\n                my_pull = curr_agent()\n    \n    return my_pull","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_rounds(file1, file2, N=3):\n    env = make(\"mab\", debug=True)\n    p1_count=0\n    p2_count=0\n    print ('simulating...',N,'games')\n    for i in range(N):\n        game=env.run([file1, file2])\n        p1_score = env.steps[-1][0]['reward']\n        p2_score = env.steps[-1][1]['reward']\n        if p1_score>p2_score:\n            p1_count+=1\n        else:\n            p2_count+=1\n        env.reset()\n        z=i+1\n        #print(f\"Round {i+1}: {p1_score} - {p2_score}\")\n    print (p1_count,'for',z,round(p1_count/z,3),'.vs',round(p2_count/z,3))\n    print ('complete')\n    points_est1=[]\n    points_est2=[]\n    \n    for x in range(2000):\n        #print (game[x][1]['reward'])\n        z=x+1\n        points_est1.append(game[x][0]['reward']/z)\n        points_est2.append(game[x][1]['reward']/z)\n        \n    plt.plot(points_est1,label='test_bot')\n    plt.plot(points_est2, label='baseline')\n    plt.legend()\n    plt.show()\n    print (sum(points_est2)/len(points_est2))\n    \n    #n=500\n    #chunked=[points_est2[i:i + n] for i in range(0, len(points_est2), n)]\n    #for i in range(len(chunked)):\n        #print (sum(chunked[i])/len(chunked[i]))\n        #print(len(chunked[i]))\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env = make(\"mab\", debug=True)\nenv.run([\"/kaggle_simulations/agent/main.py\", \"bayesian_ucb.py\"])\nenv.render(mode=\"ipython\", width=800, height=400)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env = make(\"mab\", debug=True)\nenv.run([\"/kaggle_simulations/agent/main.py\", \"vegas_pull2.py\"])\nenv.render(mode=\"ipython\", width=800, height=400)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('test bot vs bayesian_ucb')\nprint_rounds(\"/kaggle_simulations/agent/main.py\", \"bayesian_ucb.py\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('test bot vs vegas_pull2')\nprint_rounds(\"/kaggle_simulations/agent/main.py\", \"vegas_pull2.py\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!cd /kaggle_simulations/agent && tar -czvf /kaggle/working/submit.tar.gz main.py saved_model","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}