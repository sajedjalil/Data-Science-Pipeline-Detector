{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Performance Visualisation\n\nHow can we better understand _why_ an agent is failing? (or suceeding)\n\nWhy is an agent losing?\n- incorrect probabily predictions?\n- not quick enough to act on them? i.e. opponent stealing?\n- not enough exploring?\n\nFollowing are some of the visualisations I used to diagnose the performance of my agents throughout the competition."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from kaggle_environments import make, evaluate\n\n# Viz\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Data\nimport pandas as pd\nimport numpy as np\n\nimport random\n\n# Files\nfrom os import listdir\nfrom os.path import isfile, join\nimport json\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Test Agents"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%%writefile mab.py\n\n# from https://www.kaggle.com/demetrypascal/simple-multi-armed-bandit-ga\n\nimport numpy as np\n\nif True:\n\n    bandit_state = None\n    total_reward = 0\n    last_step = None\n    \n    def multi_armed_bandit_agent(observation, configuration):\n        \n        global bandit_state, total_reward, last_step\n    \n        step = 1# STEPstep #you can regulate exploration / exploitation balacne using this param\n        \n        decay_rate = 0.97 # how much do we decay the win count after each call\n        \n            \n        if observation.step == 0:\n            # initial bandit state\n            bandit_state = [[1,1] for i in range(configuration.banditCount)]\n        else:       \n            # updating bandit_state using the result of the previous step\n            last_reward = observation.reward - total_reward\n            total_reward = observation.reward\n            \n            # we need to understand who we are Player 1 or 2\n            player = int(last_step == observation.lastActions[1])\n            \n            if last_reward > 0:\n                bandit_state[observation.lastActions[player]][0] += step\n            else:\n                bandit_state[observation.lastActions[player]][1] += step\n            \n            bandit_state[observation.lastActions[0]][0] = (bandit_state[observation.lastActions[0]][0] - 1) * decay_rate + 1\n            bandit_state[observation.lastActions[1]][0] = (bandit_state[observation.lastActions[1]][0] - 1) * decay_rate + 1\n    \n    #     generate random number from Beta distribution for each agent and select the most lucky one\n        best_proba = -1\n        best_agent = None\n        for k in range(configuration.banditCount):\n            proba = np.random.beta(bandit_state[k][0],bandit_state[k][1])\n            if proba > best_proba:\n                best_proba = proba\n                best_agent = k\n            \n        last_step = best_agent\n        return best_agent","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%%writefile vegas.py\n\n# https://www.kaggle.com/jyesawtellrickson/pvsmawrc-commented-explained\n\nimport numpy as np\nimport pandas as pd\nimport random, os, datetime, math\nfrom collections import defaultdict\n\ntotal_reward = 0\nbandit_dict = {}\n\n\ndef get_next_bandit():\n    \"\"\"Get Next Bandit\n    \n    Choose the best bandit based on some logics. \n    \n    Honestly, don't really understand the logic T.T\n    \"\"\"\n    best_bandit = 0\n    best_bandit_expected = 0\n    \n    for bnd in bandit_dict:\n        # define some things\n        num_wins = bandit_dict[bnd]['win']\n        num_losses = bandit_dict[bnd]['loss']\n        num_opt_choices = bandit_dict[bnd]['opp']\n        num_opt_redraws = bandit_dict[bnd]['op_continue']\n        # calculate expectation\n        expect = (\n            num_wins - num_losses        # subtract the losses?! \n            + num_opt_choices            # add the num draws of opponent\n            - (num_opt_choices>0)*1.5    # subtract if opponent has ever drawn (rate up things you've never drawn)\n            + num_opt_redraws            # adding number of opt redraws (rate up something that's commonly drawn in a row)\n        ) / (\n            num_wins + num_losses + num_opt_choices  # divide by total plays\n        ) \\\n        * math.pow(0.97, num_wins + num_losses + num_opt_choices)  # decay\n        \n        \n        # find the best bandit\n        if expect > best_bandit_expected:\n            best_bandit_expected = expect\n            best_bandit = bnd\n            \n    return best_bandit\n\nmy_action_list = []\nop_action_list = []\n\nop_continue_cnt_dict = defaultdict(int)\n\ndef multi_armed_probabilities(observation, configuration):\n    \"\"\"Multi Armed Probabilities\n    \n    Track the moves and rewards of the game, as well as the repeated actions\n    by players.\n    \n    Logic: \n     1. If you have a successful pull, do that again. (over-exploit)\n     2. If you've drawn something 3 times in a row, redo that 50% of the time\n     3. Else choose agent based on best estimate of returns\n    \"\"\"\n    global total_reward, bandit_dict\n\n    # initialise randomly\n    my_pull = random.randrange(configuration['banditCount'])\n    \n    # update the internal data\n    if 0 == observation['step']:\n        total_reward = 0\n        bandit_dict = {}\n        for i in range(configuration['banditCount']):\n            bandit_dict[i] = {'win': 1, 'loss': 0, 'opp': 0, 'my_continue': 0, 'op_continue': 0}\n    else:\n        last_reward = observation['reward'] - total_reward\n        total_reward = observation['reward']\n        \n        my_idx = observation['agentIndex']\n        my_last_action = observation['lastActions'][my_idx]\n        op_last_action = observation['lastActions'][1-my_idx]\n        \n        my_action_list.append(my_last_action)\n        op_action_list.append(op_last_action)\n        \n        if 0 < last_reward:\n            bandit_dict[my_last_action]['win'] += 1\n        else:\n            bandit_dict[my_last_action]['loss'] += 1\n        bandit_dict[op_last_action]['opp'] += 1\n        \n        # if someone redraws the same, then increment\n        if observation['step'] >= 3:\n            if my_action_list[-1] == my_action_list[-2]:\n                bandit_dict[my_last_action]['my_continue'] += 1\n            else:\n                bandit_dict[my_last_action]['my_continue'] = 0\n            if op_action_list[-1] == op_action_list[-2]:\n                bandit_dict[op_last_action]['op_continue'] += 1\n            else:\n                bandit_dict[op_last_action]['op_continue'] = 0\n        \n        # if I had successful last pull, do that again\n        # this probably breaks various follow agents since it's not 'bayesian'\n        if last_reward > 0:\n            my_pull = my_last_action\n        # if I've mad three in a row the same, do it again 50% of the \n        # time, otherwise get the best bandit\n        elif observation['step'] >= 4 \\\n            and (my_action_list[-1] == my_action_list[-2]) and (my_action_list[-1] == my_action_list[-3]) \\\n            and random.random() < 0.5:\n            my_pull = my_action_list[-1]\n        # otherwise use bandit choice logics\n        else:\n            my_pull = get_next_bandit()\n    \n    return my_pull","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Move History\nAt each timestep, plot the 100 bandits probabilities, and the moves we played. This is good for seeing the general behaviour of your agents. You can see things like phases of exploration where it expands out to machines with lower thresholds. You can also observe things like following in detail.\n\nWe can observe local games (played live), or analyse downloaded games."},{"metadata":{"code_folding":[0,62,78],"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def plot_history(mygame, agent1_color='black', agent2_color='cyan'):\n    \"\"\"Plot moves\n    \n    Plot the moves that agents make across a game.\n    \n    Agent 1 shown in black, Agent 2 in cyan.\n    \n    Args:\n        mygame: a game file\n        agent1_color:\n        agent2_color:\n    \n    \"\"\"\n    data = []\n    for i in range(2000):\n        data += [mygame[i][0]['observation']['thresholds']]\n\n    data = np.array(data)\n    \n    # order it\n    data = sorted(np.concatenate((\n        np.transpose(data),\n        np.array(range(100)).reshape(-1,1)\n    ), axis=1), key=lambda x: x[0])\n\n\n    data = np.array(data)\n\n    d = dict(enumerate(data[:,2000:].reshape(-1).tolist()))\n    mapping = {int(v):k for k,v in d.items()}\n\n    # Make double\n    data = np.concatenate((data, data), axis=1).reshape(-1, 2001)\n\n    sns.set()\n    cmap = sns.light_palette(\"red\", as_cmap=True)\n    ax = sns.heatmap(data[:,:2000], vmin=20, vmax=100,cmap=cmap)\n    # vertical is num moves\n\n\n    fig = plt.gcf()\n    ax = plt.gca()\n\n    # Setup seaborn\n    fig.set_size_inches(12, 10)\n    ax.set_ylabel('Bandit')\n    ax.set_xlabel('Move')\n    ax.get_yaxis().set_visible(False)\n\n\n    # Plot moves\n    move_history = [mygame[i][1]['action'] for i in range(2000)]\n    ax.scatter(range(2000), [mapping[m]*2+1 for m in move_history],\n               color=agent2_color, label='moves', s=3)\n\n    # Plot moves\n    move_history = [mygame[i][0]['action'] for i in range(2000)]\n    ax.scatter(range(2000), [mapping[m]*2 for m in move_history],\n               color=agent1_color, label='moves', s=3)\n\n    plt.show()\n    \n    \ndef plot_history_agents(agent1, agent2):\n    \"\"\"Plot History Agents\n    \n    Run a live game between two saved agents.\n    \n    agent1: (str) location of agent 1\n    agent2: (str) location of agent 2\n    \"\"\"\n    # Prepare an environment\n    env = make(\"mab\", debug=True)\n    env.reset()\n    \n    mygame = env.run([agent1, agent2])\n    \n    plot_history(mygame)\n    \ndef plot_history_online(fname):\n    \n    with open(fname) as f:\n        mygame = json.load(f)\n\n    plot_history(mygame['steps'])\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot a local game\np1 = \"mab.py\"\np2 = \"vegas.py\"\n\nplot_history_agents(p1, p2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot an online game\nplot_history_online(\"../input/santa-2020-top-agents-dataset/episode/11933970.json\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Threshold Predictions\n\nHere we can look at the predicted thresholds of an agent and see how they vary from the actual thresholds.\n\nTo get this to work, the agent must:\n- be a class\n- have implemented method get_predictions() which returns an array with the predictions for each  bandit (100,)\n\n\nIn the example below, we see that it tends to overestimate the thresholds. This tells us our model needs some adjustments to better understand the actual thresholds (e.g. more decay!). It might also be that accurate predictions of the thresholds isn't required, as we've seen in some public notebook.s"},{"metadata":{"code_folding":[0],"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def run_game(agent, opponent):\n    \"\"\"Run Game\n    \n    Agent must have implemented method get_predictions() which returns an \n    array with the predictions for each  bandit (100,)\n    \n    agent: (obj) agent 1\n    opponent: (str) location of agent 2\n    \"\"\"\n    \n    env = make(\"mab\", debug=True)\n\n    trainer = env.train([None, opponent])\n    observation = trainer.reset()\n    configuration = env.configuration\n    done = False\n\n    preds = [[0.5 for i in range(100)]]\n    actions = []\n    opponent_actions = []\n    thresholds = [observation.thresholds]\n\n    while not done:\n        action = agent(observation, configuration)\n        preds += [agent.get_predictions()]\n        actions += [action]\n        # action = do_action(observation, configuration)\n        observation, reward, done, info = trainer.step(action)\n        thresholds += [observation.thresholds]\n        opponent_actions.append(observation.lastActions[1-observation.agentIndex])\n\n    assert len(thresholds) == len(preds)\n    assert len(preds) == 2000\n    \n    return preds, thresholds, actions, opponent_actions\n    \n\ndef plot_threshold_vars(agent, opponent, diffs=True):\n    \"\"\"Plot Treshold Variances\n    \n    Show heatmap of the variance from reality for each bandit\n    Put moves over the top.\n    \n    Args:\n        agent: (obj) agent 1\n        opponent:\n        diffs: (bool) if diffs = True, plot the variance from \n               actual threshold, otherwise plot predicted threshold\n    \"\"\"\n    # Run the game\n    (preds, thresholds, actions, opponent_actions) = run_game(agent, opponent)\n    \n    preds = np.array(preds) * 100\n    thresholds = np.array(thresholds)\n    if diffs:\n        var = preds - thresholds\n    else:\n        var = preds\n    # var = np.transpose(var)\n\n    \n    var = sorted(np.concatenate((\n        np.transpose(var),\n        np.array(range(100)).reshape(-1,1)\n    ), axis=1), key=lambda x: -x[0])\n\n    var = np.array(var)\n\n    # data = np.array(data)\n\n    d = dict(enumerate(var[:,2000:].reshape(-1).tolist()))\n    mapping = {int(v):k for k,v in d.items()}\n\n    # Make double\n    # data = np.concatenate((data, data), axis=1).reshape(-1, 2001)\n\n    sns.set()\n    cmap = sns.light_palette(\"red\", as_cmap=True)\n    # ax = sns.heatmap(data[:,:4000], vmin=20, vmax=100,cmap=cmap)\n    if diffs:\n        ax = sns.heatmap(var, vmin=-50, vmax=50, cmap=\"PiYG\")\n    else:\n        ax = sns.heatmap(var, vmin=0, vmax=100, cmap=\"PiYG\")\n    # vertical is num moves\n\n\n    fig = plt.gcf()\n    ax = plt.gca()\n\n    # Change seaborn plot size\n    fig.set_size_inches(12, 10)\n    ax.set_ylabel('Bandit')\n    ax.set_xlabel('Move')\n    # ax.set_title('Thresholds for Game A v B')\n\n    \n    ax.scatter(range(1, 2000), [mapping[m]*2 for m in opponent_actions],\n               color='cyan', label='moves', s=10)\n    \n    ax.scatter(range(1,2000), [mapping[m] for m in actions],\n               color='black', label='moves', s=10)\n\n        \n    # Agent 1 = black\n    # Opponent = cyan\n\n\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# %%writefile holmes.py\n\nimport numpy as np\nfrom scipy.stats import beta\nfrom random import choice, shuffle, random\nfrom collections import Counter\n\n# Helper functions\nclass HistoryCollectingAgent():\n    def __init__(self):\n        self.my_choices = []\n        self.my_rewards = []\n        self.opponent_choices = []\n        self.configuration = None\n    \n    def __call__(self, obs, conf):\n        self.configuration = conf\n        if obs.lastActions:\n            self.my_choices.append(obs.lastActions[obs.agentIndex])\n            self.opponent_choices.append(obs.lastActions[1 - obs.agentIndex])\n            self.my_rewards.append(obs.reward - sum(self.my_rewards))\n        return self.action(obs, conf)\n    \n    # abstract method to be implemented in inheriting classes\n    def action(self, obs, conf): raise NotImplementedError()\n\n\nclass CCHolmes(HistoryCollectingAgent):\n    def __init__(self):\n        # Collect history\n        HistoryCollectingAgent.__init__(self)\n\n        # Problem Info\n        self.decay_rate = 0.97\n\n        # Bayesian stuff\n        self.post_a = None\n        self.post_b = None\n        \n        # Behaviour monitoring\n        self.opponent_type = None\n        self.opponent_type_cd = 0\n        self.choice_types = []\n        self.opponent_types = []\n        self.opponent_post_ab = None\n        self.debug = False\n        \n        # Tuning constants\n        self.c = 1.5\n        self.opt_fact = 0\n    \n    def get_predictions(self):\n        \"\"\"\n        Bayesian agent\n        we estimate:\n            P(success) = a / (a+b)\n\n        with decay we know:\n            P(success, t) = P(success, t-1) * 0.97 if we drew at t-1\n\n        We should be more abusive of high success rates, we can dial down the \n        strength of the ucb to do this, or manually override.\n        \"\"\"\n        \n        decay_rate_mod = 1 #.005\n        \n        if len(self.post_a) > 0:\n            # Estimate the thresholds\n            threshold_pred = self.post_a / (self.post_a + self.post_b).astype(float)\n\n            # Perform decay\n            threshold_pred *= (self.decay_rate*decay_rate_mod) ** (\n                        self.post_a # + self.post_b + self.opponent_post_ab - 3\n                    ) \n\n            # Adjust for normal distribution\n            # threshold_pred = self.uniform_adjust_2(threshold_pred)\n            return threshold_pred\n        else:\n            return\n\n        \n    \n    @staticmethod\n    def get_maxes(l):\n        maxes = []\n        max_val = max(l)\n        # return choice([i for i in d if i==max_val])\n        for i, v in enumerate(l):\n            if v >= max_val:\n                maxes.append(i)\n        return max_val, maxes\n        \n        \n    def decision_logic(self, d):\n        (max_val, maxes) = self.get_maxes(d)\n        return choice(range(100))\n        if len(maxes) == 0:\n            return 1\n        return choice(maxes)\n\n    \n    def update_internals(self):\n        \"\"\"Update Internal Model\n        \n        Use Bayesian approach.\n        \n        B(a+1, b+1)\n        a = num wins\n        b = num losses\n        \"\"\"\n        if len(self.my_choices) == 0:\n            self.post_a = np.ones(self.configuration.banditCount)\n            self.post_b = np.ones(self.configuration.banditCount)\n            self.opponent_post_ab = np.ones(self.configuration.banditCount)\n            \n        else:\n            # get the latest reward\n            r = self.my_rewards[-1]\n            c = self.my_choices[-1]\n            # update distribution\n            self.post_a[c] += r\n            self.post_b[c] += (1 - r)\n            self.opponent_post_ab[c] += 1\n       \n    \n    def action(self, observation, configuration):\n        \"\"\"\n        \"\"\"\n        # Update any internals\n        self.update_internals()\n        \n        # Estimate the thresholds\n        threshold_pred = self.get_predictions()\n        \n        # Add in the bound (exploration)\n        bound =  threshold_pred \\\n            + beta.std(self.post_a, self.post_b) * self.c\n\n        bandit = int(self.decision_logic(bound))\n        \n        return bandit\n    \n    \nagent = CCHolmes()\n\ndef do_action(observation, configuration):\n    return agent(observation, configuration)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_threshold_vars(agent, opponent='vegas.py')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_threshold_vars(agent, opponent=agent, diffs=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Online Match Score Review\n\nSome agenst might be strongin the early game, and some stronger in the late game. It's instructive to check the rewards in time to see if you're lacking in a particular area."},{"metadata":{"code_folding":[0,39,58],"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"class Game():\n    def __init__(self, fname=''):\n        self.fname = fname\n        self.move_history = []  # 2000x2\n        self.reward_history = []  # 2000x2\n        self.threshold_history = []  # 2000x100\n        self.total_reward_history = []\n        self.teams = []\n        \n    def process_game(self):\n        # load the game\n        with open(self.fname) as f:\n            j = json.load(f)\n        self.teams = j['info']['TeamNames']\n        \n        move_history = []\n        total_reward_history = []\n        thresholds = []\n        for t in j['steps'][1:]:\n            move_history += [[t[0]['action'], t[1]['action']]]\n            total_reward_history += [[t[0]['reward'], t[1]['reward']]]\n            thresholds += [t[0]['observation']['thresholds']]\n        self.move_history = move_history\n        reward_history = [total_reward_history[0]]\n        for t in range(1, len(total_reward_history)):\n            reward_history += [[\n                total_reward_history[t][0] - total_reward_history[t-1][0],\n                total_reward_history[t][1] - total_reward_history[t-1][1],\n            ]]\n        \n        self.reward_history = reward_history\n        self.threshold_history = thresholds\n        self.total_reward_history = total_reward_history\n        \n        assert len(self.reward_history) == j['configuration']['episodeSteps']-1\n        assert len(self.move_history) == j['configuration']['episodeSteps']-1\n        \n        return\n\n    \ndef load_all_games(folder='../input/santa-2020-top-agents-dataset/episode/'):\n    \"\"\"Load All Games\n    \n    Load all the games from the games folder.\n    \"\"\"\n    mypath = '../input/santa-2020-top-agents-dataset/episode/'\n    game_dirs = [folder]\n    onlyfiles = []\n    for d in game_dirs:\n        onlyfiles += [join(d, f) for f in listdir(d) if isfile(join(d, f)) and f[-10:] != '_info.json']\n    return onlyfiles\n\n\ndef plot_reward_history(folder='../input/santa-2020-top-agents-dataset/episode/', diffs_trigger=True, lim=30):\n    \"\"\"\n    folder name must equal team name\n    \"\"\"\n\n    data = []\n    teams = []\n\n    games = load_all_games(folder=folder)[:lim]\n\n    for game in games:\n\n        g = Game(game)\n        g.fname\n        g.process_game()\n\n        data += [g.total_reward_history]\n        teams += [g.teams]\n\n    if diffs_trigger:\n        # Calculate differences\n        diffs = []\n        for i in range(len(teams)):\n            if teams[i][0] == folder:\n                # We are player one\n                diffs += [[p1-p2 for p1,p2 in data[i]]]\n            else:\n                diffs += [[p2-p1 for p1,p2 in data[i]]]\n    else:\n        diffs = []\n        for i in range(len(teams)):\n            if teams[i][0] == folder:\n                # We are player one\n                diffs += [[p1 for p1,p2 in data[i]]]\n            else:\n                diffs += [[p2 for p1,p2 in data[i]]]\n\n    # Plot each game\n    for i in range(len(diffs)):\n        plt.plot(range(1999), diffs[i], color='grey', linewidth=0.4)\n\n    fig = plt.gcf()\n    ax = plt.gca()\n\n    # Plot the avg reward on a separate axis\n    \n    if diffs_trigger:\n        ax2 = ax.twinx()\n        ax2.plot(range(1999), np.array(diffs).mean(axis=0), color='red')\n        # ax2.set_ylabel('Difference in Reward ({} - opponent)'.format(folder))\n        ax2.set_ylabel('Difference in Reward (mean)'.format(folder))\n    else:\n        ax.plot(range(1999), np.array(diffs).mean(axis=0), color='red')\n        \n    \n   \n\n    plt.xlim(0,2000)\n    # ax.set_ylabel('Difference in Reward ({} - opponent)'.format(folder))\n    ax.set_ylabel('Difference in Reward'.format(folder))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_reward_history(lim=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_reward_history(diffs_trigger=False, lim=100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Score vs. Board Strength\n\nIt's also interesting to consider if you perform better on certain boards, since not all are created equally. Some boards will have a higher threshold to begin with. Agents which favour exploration might be worse off when there are more resources to exploit."},{"metadata":{"code_folding":[0,39,58],"trusted":true},"cell_type":"code","source":"def plot_reward_vs_richness(folder='../input/santa-2020-top-agents-dataset/episode/', lim=30):\n    \"\"\"\n    folder name must equal team name\n    \"\"\"\n\n    data = []\n    teams = []\n    avg_thresholds = []\n\n    games = load_all_games(folder=folder)[:lim]\n\n    for game in games:\n        try:\n            g = Game(game)\n            g.fname\n            g.process_game()\n\n            data += [g.total_reward_history]\n            teams += [g.teams]\n            avg_thresholds += [sum(g.threshold_history[0])/100]\n        except:\n            True\n            # bad game\n\n\n    diffs = []\n    for i in range(len(teams)):\n        if teams[i][0] == folder:\n            # We are player one\n            diffs += [[p1-p2 for p1,p2 in data[i]][-1]]\n        else:\n            diffs += [[p2-p1 for p1,p2 in data[i]][-1]]\n\n\n    # print(len(diffs), len(avg_thresholds), diffs, avg_thresholds)\n    # Plot each game\n    plt.scatter(avg_thresholds, diffs)\n\n    fig = plt.gcf()\n    ax = plt.gca()\n\n    # Plot the avg reward on a separate axis\n    \n    # plt.xlim(0,2000)\n    ax.set_ylabel('Difference in Reward')\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_reward_vs_richness(lim=200)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}