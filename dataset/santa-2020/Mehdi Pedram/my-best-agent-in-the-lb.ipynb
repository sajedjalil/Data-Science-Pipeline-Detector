{"cells":[{"metadata":{},"cell_type":"markdown","source":"### The second cell of this notebook contains the code of my best agent in the leaderboard(1445.6) and rest cells are just some other of my agents and the code to compare them.\n### I have explained the strategy [here in the forum](https://www.kaggle.com/c/santa-2020/discussion/217537).\n\n### If you find it useful upvote is appreciated!"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!pip install kaggle-environments --upgrade\nfrom kaggle_environments import make, evaluate\nenv = make(\"mab\", debug=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile myBestAgentInLB.py\nimport numpy as np\n\ndecay_rate = 0.97\nn_rounds = 2000\nbandit_count = 100\nlast_reward = 0\ntotal_reward = None \nlast_bandit = None\nhis_hits = None\nhis_record = None\nmy_record = None\nmy_hits = None\nwins = None\nlosses = None\nbandits_record = None\nrecord_index = None\nbandit_last_step = None\n\nmax_depth = 2000\nnum_probabs = 1001\nk_decay1 = np.zeros([max_depth, num_probabs], 'float128')\nk_decay0 = np.zeros([max_depth, num_probabs], 'float128')\nprobabs = np.linspace(0, 1, num_probabs)\nfor i in range(k_decay1.shape[0]):\n    k_decay1[i] = probabs * 0.97 ** i\nk_decay0 = 1 - k_decay1\n\ndef estimate_probab(seq):\n    raw_probab1 = k_decay1[:seq.shape[0],:][seq == 1,:]\n    raw_probab0 = k_decay0[:seq.shape[0],:][seq == 0,:]\n    each_probab =  raw_probab0.prod(axis = 0) * raw_probab1.prod(axis = 0)\n    half_area = each_probab.sum()/ 2\n    cum_sum = np.cumsum(each_probab)\n    best_estimation =  np.argmin(np.abs(cum_sum - half_area))\n    return (probabs[best_estimation]  *.97**seq.shape[0])\n\n\nmyProbabs = None\nhis_bandit_rec = None\nhis_rec_index = None\nn_lookback = 60\nmax_hit = 66\nprobab_ratio = 1.5\nn_decision = 7\nprobab_step = [.47, .40, .35, .31, .26, .24, .21, .175, .15, .13]\navg_proabs = [ .5 * .97**(i* 0.02) for i in range(2000)]\nnu_my_bests = 70\nnu_my_best_best = 10\nmy_bests_index = 0\nchoicePath = None\ndef new_bandit(step):\n    global myProbabs, my_bests_index, choicePath\n    start = step - n_lookback if step >= n_lookback else 0\n    his_last_moves = his_record[start:step]\n    his_last_bandit = his_last_moves[-1]\n    n_his_last_move = (his_last_moves == his_last_bandit).sum()\n    myProbabs[last_bandit] = estimate_probab(bandits_record[last_bandit,:record_index[last_bandit]])\n    myProbabs[his_last_bandit] = estimate_probab(bandits_record[his_last_bandit,: record_index[his_last_bandit]])\n\n    # print(his_last_bandit)\n    # print('band rec',bandits_record[his_last_bandit,: record_index[his_last_bandit]])\n    # print(his_last_moves)\n    # print(n_his_last_move)\n    # print('------- Mine:')\n    # print(last_bandit)\n    # print('band rec:',bandits_record[last_bandit,:record_index[last_bandit]])\n    # print(myProbabs)\n    not_chosen = (my_hits == 0)\n    not_chosen_any = not_chosen.any()\n\n    if step == 1 :  choicePath = {'m1':0, 'm2':0, 'm3':0, 'm4':0,'m5':0, 'm6':0,}\n    # if step == 1999 : print(choicePath)\n    if  his_hits[his_last_bandit] > 1:\n        if my_hits[his_last_bandit] <= 1:\n            choicePath['m1'] += 1\n            return his_last_bandit\n    \n\n    if  not not_chosen_any :\n        if n_his_last_move > 1 and my_hits[his_last_bandit] <= 4:\n            choicePath['m2'] += 1\n            return his_last_bandit\n\n\n    my_bests = np.argsort(myProbabs)[-nu_my_bests:]\n    \n    if his_last_bandit in my_bests:\n        if my_hits[his_last_bandit] > 0:\n            choicePath['m3'] += 1\n            return his_last_bandit\n    #if np.random.rand() > .5 :\n    if not_chosen_any:\n        if ((not_chosen) & (his_hits==0)).any():\n            choicePath['m4'] += 1\n            return np.random.choice(np.where((not_chosen) & (his_hits==0))[0])\n        else:\n            choicePath['m5'] += 1\n            return np.random.choice(np.where(not_chosen)[0])\n\n    # winner = my_bests[-(my_bests_index % nu_my_best_best)-1]\n    #winner = np.random.choice( my_bests[-nu_my_best_best:])\n    my_best_bests = my_bests[-nu_my_best_best:]\n    steps = np.zeros(nu_my_best_best)\n    for i in range( nu_my_best_best):\n        steps[i] = bandit_last_step[my_best_bests[i]]\n    winner = my_best_bests[np.argmin(steps)]\n    choicePath['m6'] += 1\n    return winner\n\ndef agent(obs, conf):\n    global bandits_record, my_record, my_hits, his_hits, his_record, myProbabs, his_scores,last_reward\n    global last_bandit, total_reward,  record_index, wins, losses, his_rec_index, his_bandit_rec,bandit_last_step\n    if obs.step == 0:        \n        total_reward = 0 \n        his_record = np.zeros(n_rounds, 'int')\n        his_hits = np.zeros(conf.banditCount, 'int')\n        his_scores = np.zeros(bandit_count) + 0.5\n        his_bandit_rec = np.ones([bandit_count, 2000], 'int') * -1\n        his_rec_index = np.zeros(bandit_count, 'int')\n        \n        myProbabs = np.random.rand(bandit_count)* 0.001 + 0.5\n        my_record = np.zeros(n_rounds, 'int')\n        my_hits = np.zeros(conf.banditCount, 'int')\n        bandits_record = np.zeros([conf.banditCount, 2000], 'int')\n        record_index = np.zeros(conf.banditCount,'int')\n        wins = np.zeros(conf.banditCount, 'int')\n        losses = np.zeros(conf.banditCount, 'int')\n        bandit_last_step = np.zeros(conf.banditCount, 'int')\n\n        bandit = np.random.randint(conf.banditCount)\n        last_bandit = bandit\n        return bandit\n    if obs.lastActions[0] == last_bandit:\n        his_action = obs.lastActions[1]\n    else:\n        his_action = obs.lastActions[0]\n    his_record[obs.step-1] = his_action\n    his_hits[his_action] += 1\n    his_bandit_rec[his_action,his_rec_index[his_action]] = obs.step\n    his_rec_index[his_action] +=1\n    bandits_record[his_action, record_index[his_action]] = -1\n    record_index[his_action] += 1\n    my_hits[last_bandit] += 1\n    my_record[obs.step-1] = last_bandit\n    if obs.reward > total_reward:\n        total_reward += 1\n        bandits_record[last_bandit, record_index[last_bandit]] = 1\n        wins[last_bandit] += 1\n        last_reward = 1\n    else:\n        bandits_record[last_bandit, record_index[last_bandit]] = 0\n        losses[last_bandit] +=1\n        last_reward =0\n    record_index[last_bandit] += 1\n    bandit = int(new_bandit(obs.step))\n    last_bandit = bandit \n    bandit_last_step[bandit] = obs.step\n    return bandit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile random.py\nimport random\ndef agent(obs, conf):\n    return random.randrange(conf.banditCount)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile uniform.py\nimport random\ndef agent(obs, conf):\n    return obs.step % conf.banditCount","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile ucb_agent.py\nimport math\n\nlast_bandit = -1\ntotal_reward = 0\n\nsums_of_reward = None\nnumbers_of_selections = None\n\ndef ucb_agent(observation, configuration):    \n    global sums_of_reward, numbers_of_selections, last_bandit, total_reward\n\n    if observation.step == 0:\n        numbers_of_selections = [0] * configuration[\"banditCount\"]\n        sums_of_reward = [0] * configuration[\"banditCount\"]\n\n    if last_bandit > -1:\n        reward = observation.reward - total_reward\n        sums_of_reward[last_bandit] += reward\n        total_reward += reward\n\n    bandit = 0\n    max_upper_bound = 0\n    for i in range(0, configuration.banditCount):\n        if (numbers_of_selections[i] > 0):\n            average_reward = sums_of_reward[i] / numbers_of_selections[i]\n            delta_i = math.sqrt(2 * math.log(observation.step+1) / numbers_of_selections[i])\n            upper_bound = average_reward + delta_i\n        else:\n            upper_bound = 1e400\n        if upper_bound > max_upper_bound and last_bandit != i:\n            max_upper_bound = upper_bound\n            bandit = i\n            last_bandit = bandit\n\n    numbers_of_selections[bandit] += 1\n\n    if bandit is None:\n        bandit = 0\n\n    return bandit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile probabiliticRounds.py\nimport random\n\nimport numpy as np\nsp = np.linspace(0,1,1000)\nspp = 1 - sp\ndecay_rate = 0.97\n\ndef probab(n_ones,n_zeros):\n    global sp, spp\n    cdfBeta = np.cumsum(sp**n_ones * spp **n_zeros )\n    place = np.argmin(np.abs(cdfBeta - cdfBeta[999] / 2 ))\n    return sp[place]\ndef probab_with_decay(n_ones,n_zeros,n_opponent):\n    global sp, spp, decay_rate\n    cdfBeta = np.cumsum(sp**n_ones * spp **n_zeros )\n    place = np.argmin(np.abs(cdfBeta - cdfBeta[999] / 2 ))\n    return sp[place] * decay_rate**(n_ones + n_zeros + n_opponent)\ndef probab_decayed(n_ones, n_zeros, n_opponent):\n    global sp, spp\n    k_decay = n_ones + n_zeros + n_opponent \n    length = int(decay_rate**k_decay * 1000)\n    cdfBeta = np.cumsum(sp[:length]**n_ones * spp[:length]**n_zeros )\n    place = np.argmin(np.abs(cdfBeta - cdfBeta[length-1] / 2 ))\n    return sp[place]\n\nbandit_count = 100\nlast_result =0\ntotal_reward = 0\nlast_bandit = None\nlast_reward = None\nmy_win_record = np.zeros(bandit_count)\nmy_loss_record = np.zeros(bandit_count)\nmy_chances = np.ones(bandit_count) * 0.5\nhis_overall_record = np.zeros(bandit_count)\ntotal_steps = 2000\n\ndef new_bandit(step):\n    global my_chances, my_loss_record, my_win_record, his_overall_record\n    global last_bandit, total_reward, last_result, last_reward, decay_rate\n    progress = step/total_steps\n    #relative_decay = 1 - (1 - decay_rate) * progress**0.25\n    #decays = decay_rate**(my_win_record + my_loss_record + his_overall_record)\n    scores = my_chances * 1 #* decays\n    #scores[last_bandit] += (last_reward - .1) * ((1- progress)**0.5)\n    scores[last_bandit] += ((last_reward) * ((total_steps - step)/total_steps)**0.5) \n    winner = np.argmax(scores)\n    return int(winner)\n\ndef agent(obs, conf):\n    global my_chances, my_loss_record, my_win_record, his_overall_record\n    global last_bandit, total_reward, last_result, last_reward\n    if obs.step == 0:        \n        bandit = 0\n        last_bandit = bandit\n        return bandit\n    if obs.lastActions[0] == last_bandit:\n        his_action = obs.lastActions[1]\n    else:\n        his_action = obs.lastActions[0]\n    his_overall_record[his_action] += 1\n            \n    if obs.reward > total_reward:\n        total_reward += 1\n        last_reward = 1\n        my_win_record[last_bandit] += 1                        \n    else:        \n        last_reward = 0\n        my_loss_record[last_bandit] += 1\n    #my_chances[last_bandit] = my_win_record[last_bandit] / (my_win_record[last_bandit] + my_loss_record[last_bandit])\n    #my_chances[last_bandit] = probab(my_win_record[last_bandit], my_loss_record[last_bandit])\n    my_chances[last_bandit] = probab_decayed(my_win_record[last_bandit], my_loss_record[last_bandit],his_overall_record[last_bandit])\n    my_chances[his_action] = probab_decayed(my_win_record[his_action], my_loss_record[his_action],his_overall_record[his_action])\n    bandit = new_bandit(obs.step)\n    last_bandit = bandit \n    return bandit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile greedyMemory.py\nimport random\n\nlast_result =0\ntotal_reward = 0\nlast_bandit = None\nmy_win_record = None\nmy_loss_record = None\nmy_chances = None\ndef agent(obs, conf):\n    global my_chances, my_loss_record, my_win_record\n    global last_bandit, total_reward,last_result\n    if obs.step == 0:        \n        my_win_record = [8] * conf.banditCount\n        my_loss_record = [1] * conf.banditCount\n        my_chances = [9] * conf.banditCount\n        bandit = random.randrange(conf.banditCount)\n        last_bandit = bandit\n        return bandit\n    if obs.reward > total_reward:\n        total_reward += 1\n        my_win_record[last_bandit] += 1\n        my_chances[last_bandit] = my_win_record[last_bandit] / (my_win_record[last_bandit] + my_loss_record[last_bandit])\n        bandit = last_bandit        \n    else:\n        my_loss_record[last_bandit] += 1\n        my_chances[last_bandit] = my_win_record[last_bandit] / (my_win_record[last_bandit] + my_loss_record[last_bandit])\n        bandit = my_chances.index(max(my_chances))        \n        last_bandit = bandit\n    #print(my_chances)\n    #print(bandit)\n    return bandit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile recentEvents.py\nimport numpy as np\n\ndecay_rate = 0.97\n\ndef probab(n_ones,n_zeros):\n    global sp, spp\n    cdfBeta = np.cumsum(sp**n_ones * spp **n_zeros )\n    place = np.argmin(np.abs(cdfBeta - cdfBeta[999] / 2 ))\n    return sp[place]\n\nbandit_count = 100\ntotal_reward = 0\nlast_bandit = None\nhis_overall_record = np.zeros(bandit_count)\ntotal_steps = 2000\n\nrecord_size = 7\nbandits_record = np.zeros([record_size,bandit_count])\nrecord_index = np.zeros(bandit_count,'int')\nmy_hits_p_1 = np.ones(bandit_count)\n\ndef new_bandit(step):\n    global bandits_record, his_overall_record, last_bandit \n    exploration_score = 1 / my_hits_p_1\n    scores = bandits_record.sum(axis=0) + 2.5 * exploration_score\n    winner = np.argmax(scores)    \n    return int(winner)\n\ndef agent(obs, conf):\n    global bandits_record, his_overall_record\n    global last_bandit, total_reward\n    if obs.step == 0:        \n        bandit = np.random.randint(bandit_count)\n        last_bandit = bandit\n        return bandit\n    if obs.lastActions[0] == last_bandit:\n        his_action = obs.lastActions[1]\n    else:\n        his_action = obs.lastActions[0]\n    his_overall_record[his_action] += 1\n    my_hits_p_1[last_bandit] += 1\n    if obs.reward > total_reward:\n        total_reward += 1\n        bandits_record[record_index[last_bandit], last_bandit] = 1        \n    else:\n        bandits_record[record_index[last_bandit], last_bandit] = -1\n    record_index[last_bandit] = (record_index[last_bandit] + 1) % record_size \n    bandit = new_bandit(obs.step)\n    last_bandit = bandit \n    return bandit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile greedyMemoryWithExpolaration.py\nimport random\n\nlast_result =0\ntotal_reward = 0\nlast_bandit = None\nmy_win_record = None\nmy_loss_record = None\nmy_chances = None\nchances = None\ndef agent(obs, conf):\n    global my_chances, my_loss_record, my_win_record, chances\n    global last_bandit, total_reward,last_result\n    if obs.step == 0 or obs.step ==1:\n        print(obs)\n    if obs.step == 0:        \n        my_win_record = [8] * conf.banditCount\n        my_loss_record = [1] * conf.banditCount\n        my_chances = [8/9] * conf.banditCount\n        chances = [0] * conf.banditCount\n        bandit = random.randrange(conf.banditCount)\n        last_bandit = bandit\n        return bandit\n    if obs.reward > total_reward:\n        total_reward += 1\n        my_win_record[last_bandit] += 1\n        my_chances[last_bandit] = my_win_record[last_bandit] / (my_win_record[last_bandit] + my_loss_record[last_bandit])\n        bandit = last_bandit        \n    else:\n        my_loss_record[last_bandit] += 1\n        my_chances[last_bandit] = my_win_record[last_bandit] / (my_win_record[last_bandit] + my_loss_record[last_bandit])\n        for i in range(conf.banditCount):\n            chances[i] = my_chances[i] + 0.3/(my_win_record[i] + my_loss_record[i] + 1)\n        bandit = chances.index(max(chances))        \n        last_bandit = bandit\n    #print(my_chances)\n    #print(bandit)\n    return bandit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile ag_followHim.py\n\nimport numpy as np\n\ndecay_rate = 0.97\nn_rounds = 2000\nbandit_count = 100\n\ntotal_reward = None \nlast_bandit = None\nhis_hits = None\nhis_record = None\nmy_record = None\nmy_hits = None\nwins = None\nlosses = None\nbandits_record = None\nrecord_index = None\nx1 = None\nx2 = None\n\nsp = np.linspace(0,1,1000)\nspp = 1 - sp\ndef probab(n_ones,n_zeros):\n    global sp, spp\n    cdfBeta = np.cumsum(sp**n_ones * spp **n_zeros )\n    place = np.argmin(np.abs(cdfBeta - cdfBeta[999] / 2 ))\n    return sp[place] + np.random.rand() * 0.0001\n\ndef decayed_probab(n_ones, n_zeros, his_n):\n    global sp, spp\n    ps = sp**n_ones * spp **n_zeros\n    limit = int(1000 * decay_rate**(his_n + n_ones + n_zeros + 1 ) )+1\n    cdfBeta = np.cumsum(ps[:limit] )\n    place = np.argmin(np.abs(cdfBeta - cdfBeta[-1] / 2 ))\n    return sp[place] + np.random.rand() * 0.0001\n\nmyProbabs = None\nhis_scores = None\nhis_bandit_rec = None\nhis_rec_index = None\ndef new_bandit(step):\n    global myProbabs\n    n_lookback = 49\n    start = step - n_lookback if step >= 49 else 0\n    his_last_moves = his_record[start:step]\n    his_last_bandit = his_last_moves[-1]\n    n_last_move = (his_last_moves == his_last_bandit).sum()\n    \n    myProbabs[last_bandit] = decayed_probab(wins[last_bandit],losses[last_bandit],his_hits[last_bandit])\n    myProbabs[his_last_bandit] = decayed_probab(wins[his_last_bandit],losses[his_last_bandit],his_hits[his_last_bandit])\n    \n    if n_last_move > 1 and my_hits[his_last_bandit] <= 2:\n        return his_last_bandit\n    \n    if n_last_move == 1 and his_hits[his_last_bandit] > 1 :\n        myProbabs[his_last_bandit] -= .25\n    \n\n\n    exploratio_score = np.zeros(bandit_count)\n    exploratio_score[his_hits == 0] = -0.25\n    exploratio_score[his_hits == 1] = -0.5\n    exploratio_score += 1 /(my_hits + 1)\n    exploratio_score[my_hits>45] = -2\n    scores = myProbabs + 0.1 * exploratio_score\n    winner = int(np.argmax(scores))\n    return winner\n\ndef agent(obs, conf):\n    global bandits_record, my_record, my_hits, his_hits, his_record, myProbabs, his_scores\n    global last_bandit, total_reward,  record_index, wins, losses, his_rec_index, his_bandit_rec\n    if obs.step == 0:        \n        total_reward = 0 \n        his_record = np.zeros(n_rounds, 'int')\n        his_hits = np.zeros(conf.banditCount, 'int')\n        his_scores = np.zeros(bandit_count) + 0.5\n        his_bandit_rec = np.ones([bandit_count, 600], 'int') * -1\n        his_rec_index = np.zeros(bandit_count, 'int')\n        \n        myProbabs = np.random.rand(bandit_count)* 0.001 + 0.5\n        my_record = np.zeros(n_rounds, 'int')\n        my_hits = np.zeros(conf.banditCount, 'int')\n        bandits_record = np.zeros([conf.banditCount, 600], 'int')\n        record_index = np.zeros(conf.banditCount,'int')\n        wins = np.zeros(conf.banditCount, 'int')\n        losses = np.zeros(conf.banditCount, 'int')\n\n        bandit = np.random.randint(conf.banditCount)\n        last_bandit = bandit\n        return bandit\n    if obs.lastActions[0] == last_bandit:\n        his_action = obs.lastActions[1]\n    else:\n        his_action = obs.lastActions[0]\n    his_record[obs.step-1] = his_action\n    his_hits[his_action] += 1\n    his_bandit_rec[his_action,his_rec_index[his_action]] = obs.step\n    his_rec_index[his_action] +=1\n    my_hits[last_bandit] += 1\n    my_record[obs.step-1] = last_bandit\n    if obs.reward > total_reward:\n        total_reward += 1\n        bandits_record[last_bandit, record_index[last_bandit]] = 1\n        wins[last_bandit] += 1\n    else:\n        bandits_record[last_bandit, record_index[last_bandit]] = 0\n        losses[last_bandit] +=1\n    record_index[last_bandit] += 1\n    bandit = int(new_bandit(obs.step))\n    last_bandit = bandit \n    return bandit\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"agent1 = \"myBestAgentInLB.py\"\nn_rounds = 10\nagents = [\"uniform.py\", \"random.py\", \"ucb_agent.py\", \"greedyMemory.py\", \"probabiliticRounds.py\", \"recentEvents.py\", 'ag_followHim.py' ]\n\nwins1 = [0] * len(agents)\nwins2 = [0] * len(agents)\nsum1 = [0] * len(agents)\nsum2 = [0] * len(agents)\nfor agent in agents:\n    print(\"\\t\", agent ,end='')\nprint()\nfor i in range(n_rounds):\n    print(i+1, \":\\t\\t\" ,end='')\n    for j, agent in enumerate(agents):\n        mygame = env.run([agent1, agent])\n        #print(agent1, ':' , mygame[1999][0].observation.reward, end=\" \")\n        #print(agent2, ':' , mygame[1999][1].observation.reward, ,end='\\t')\n        #print(mygame[0][0].observation)\n        #print(mygame[1][0].observation)\n        sum1[j] += mygame[1999][0].observation.reward\n        sum2[j] += mygame[1999][1].observation.reward\n        if mygame[1999][0].observation.reward > mygame[1999][1].observation.reward:\n            wins1[j] +=1\n        if mygame[1999][0].observation.reward < mygame[1999][1].observation.reward:\n            wins2[j] +=1\n        print(wins1[j],wins2[j], end='\\t\\t')\n    print()\n\nfor j, agent in enumerate(agents):\n    print(f'{agent:<22}',wins1[j],wins2[j] ,sum1[j]/ n_rounds ,sum2[j] / n_rounds, sum1[j]/sum2[j], sep=\"\\t\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}