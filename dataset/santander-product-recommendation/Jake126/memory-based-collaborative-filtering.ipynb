{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Simple Memory-Based Collaborative Filtering Technique\n\n### Overview\n\nThe below code details our (after-the-fact) efforts to build a memory-based collaborative filtering (CF) recommendation system for the Santander Product Recommendation challenge, which ran a couple of years ago. The top submissions were based on classic Kaggle-winning algorithms like random forests and boosting algorithms, but we wanted to build something more in-line with the recommendation systems literature. The below can thus be viewed as a basic how-to on building a recommendation engine on difficultly-structured data; the main difficulties in this dataset arise from the fact that product information is sparse (in a traditional CF problem, we have ratings or purchase volumes attached to each product, rather than the binary data format we are faced with here). \n\nA good introduction to CF can be found at https://towardsdatascience.com/various-implementations-of-collaborative-filtering-100385c6dfe0\n\nThe presence of demographic data also posed a challenge: what is the best way to incorporate these informative variables in the CF model? We loosely followed the approach laid out in this paper: https://pdfs.semanticscholar.org/a621/441e7b688580707af3a4bf0ebff8c9e3d640.pdf\n\nWe approached the problem in 8 main steps:\n\n1. ** Data cleaning: ** \nthis took the form of dropping columns deemed surplus to requirements, imputing missing data.\n2. ** Feature engineering:**\nbinning continuous data into factor variables and mutating the product ownership variables to give an indication of purchase and ownership in the previous month.\n3. ** Data subsetting: **\nBased on other kernels and our own investigations, we only used June 2015 data (i.e. one year before the test data) as a predictor.\n4. ** De-duplicate the data: **\nthere are >900,000 test individuals, but most of these will have identical purchase histories given the data sparsity: we thus de-duplicate based on the variables that will be considered for CF, and create an index to map results back to the unique users. This de-duplication is employed in all the major algorithms we used, and substantially sped up processing time.\n5. ** Build a demographic-based and memory-based similarity matrix: **\nbecause of the large data size, we calculated the Manhattan distance of each test row to the training rows and used the inverse distance to derive similarities/purchase probabilities...\n6. ** ...combine demographic-based and memory-based probabilities: **\nwe combine the two models using a range of candidate weights, and nullified the probability of products each user already owns...\n7. ** ...and derive recommendations: **\nusing the derived probabilities, we output the top seven recommendations. We iterate steps 5-7 over each unique test user profile, and ascertain the optimal demographic/memory mixing parameter.\n8. ** Re-run model using all training data for optimal mixing parameter **\n\nOf course this solution is not exhaustive, and steps for algorithmic improvement are outlined at the end of the Kernel. We hope to be able to implement these improvements in-kernel at a future date!\n\nSpecial thanks to George Hartshorn for sharing the data-cleaning code this notebook is based on!"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#Import modules\n\nimport numpy as np\nimport pandas as pd\nimport csv\nimport datetime\nfrom operator import sub\nimport xgboost as xgb\nfrom sklearn import preprocessing, ensemble, metrics\nimport os\nimport gc\nimport psutil\nimport math\nfrom sklearn.metrics import roc_auc_score\nfrom collections import defaultdict\nfrom scipy.spatial.distance import pdist, wminkowski, squareform\n\npd.options.display.max_rows = 100\npd.options.display.max_columns = None\n\n# Check data library\n\nimport os\nprint(os.listdir(\"../input\"))\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#Import data\npath = '../input/'\ntraindat = pd.read_csv(path + 'train_ver2.csv', low_memory = True)\ntestdat = pd.read_csv(path + 'test_ver2.csv', low_memory = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 1: Data Cleaning\nNow that the data is read in, we can get going with everyone's favourite bit: ** data cleaning! **"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Define columns of interest, based on other kernels' output \n\ndemographic_cols = ['fecha_dato',\n 'ncodpers','ind_empleado','pais_residencia','sexo','age','fecha_alta','ind_nuevo','antiguedad','indrel',\n 'indrel_1mes','tiprel_1mes','indresi','indext','canal_entrada','indfall',\n 'tipodom','cod_prov','ind_actividad_cliente','renta','segmento']\n\nnotuse = [\"ult_fec_cli_1t\",\"nomprov\"]\n\nproduct_col = [\n 'ind_ahor_fin_ult1','ind_aval_fin_ult1','ind_cco_fin_ult1','ind_cder_fin_ult1','ind_cno_fin_ult1','ind_ctju_fin_ult1',\n 'ind_ctma_fin_ult1','ind_ctop_fin_ult1','ind_ctpp_fin_ult1','ind_deco_fin_ult1','ind_deme_fin_ult1',\n 'ind_dela_fin_ult1','ind_ecue_fin_ult1','ind_fond_fin_ult1','ind_hip_fin_ult1','ind_plan_fin_ult1',\n 'ind_pres_fin_ult1','ind_reca_fin_ult1','ind_tjcr_fin_ult1','ind_valo_fin_ult1','ind_viv_fin_ult1','ind_nomina_ult1',\n 'ind_nom_pens_ult1','ind_recibo_ult1']\n\ntrain_cols = demographic_cols + product_col\n\n# Create trimmed datasets\n\ntraindat = traindat.filter(train_cols)\ntestdat  = testdat.filter(train_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Identify columns with missing data\n\ntraindat.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Missing value variables can be broken down into three categories:\n1. Missing data for factor variables: we either impute the most common factor level, as the missing variables are a small subset of the total data, or set to a new 'missing' level if this will imbalance the factor classes.\n2. Missing data for numerical variables: we can use a more granular imputation, as other kernels have, by setting the missing value equal to the average for each province.\n3. Missing data for product variables: these are NA as the customers are not eligible to purchase the product. Thus they don't have the product, and their ownership status can be set to 0."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Impute training data\n\ntraindat.age = pd.to_numeric(traindat.age, errors='coerce')\ntraindat.renta = pd.to_numeric(traindat.renta, errors='coerce')\ntraindat.antiguedad = pd.to_numeric(traindat.antiguedad, errors='coerce')\n\ntraindat.loc[traindat['ind_empleado'].isnull(),'ind_empleado'] = 'N'\ntraindat.loc[traindat['pais_residencia'].isnull(),'pais_residencia'] = 'ES'\ntraindat.loc[traindat['sexo'].isnull(),'sexo'] = 'V'\ntraindat.fecha_alta = traindat.fecha_alta.astype('datetime64[ns]')\ntraindat.loc[traindat['fecha_alta'].isnull(), 'fecha_alta'] = pd.Timestamp(2011,9,1)\ntraindat.loc[traindat['ind_nuevo'].isnull(), 'ind_nuevo'] = 0\ntraindat.loc[traindat['indrel'].isnull(), 'indrel'] = 1\ntraindat.indrel_1mes = traindat.indrel_1mes.astype('str').str.slice(0,1)\ntraindat.loc[traindat['indrel_1mes'].isnull(), 'indrel_1mes'] = '1'\ntraindat.loc[traindat['tiprel_1mes'].isnull(), 'tiprel_1mes'] = 'I'\ntraindat.loc[traindat['indresi'].isnull(), 'indresi'] = 'S'\ntraindat.loc[traindat['indext'].isnull(), 'indext'] = 'N'\ntraindat.loc[traindat['canal_entrada'].isnull(), 'canal_entrada'] = 'MIS'\ntraindat.loc[traindat['indfall'].isnull(), 'indfall'] = 'N'\ntraindat.loc[traindat['tipodom'].isnull(), 'tipodom'] = 0.0\ntraindat.loc[traindat['cod_prov'].isnull(), 'cod_prov'] = 28.0\ntraindat.loc[traindat['ind_actividad_cliente'].isnull(), 'ind_actividad_cliente'] = 0.0\ntraindat[\"renta\"] = traindat[['renta','cod_prov']].groupby(\"cod_prov\").transform(lambda x: x.fillna(x.mean())) #Replace renta with provincial mean\ntraindat[\"age\"] = traindat[['age','cod_prov']].groupby(\"cod_prov\").transform(lambda x: x.fillna(x.mean())) #Replace age with provincial mean\ntraindat[\"antiguedad\"] = traindat[['antiguedad','cod_prov']].groupby(\"cod_prov\").transform(lambda x: x.fillna(x.mean())) #Replace antiguedad with provincial mean\ntraindat.loc[traindat['segmento'].isnull(), 'segmento'] = '02 - PARTICULARES'\ntraindat.loc[traindat['ind_nomina_ult1'].isnull(), 'ind_nomina_ult1'] = 0\ntraindat.loc[traindat['ind_nom_pens_ult1'].isnull(), 'ind_nom_pens_ult1'] = 0\n\n#Impute test data\n\ntestdat.age = pd.to_numeric(testdat.age, errors='coerce')\ntestdat.antiguedad = pd.to_numeric(testdat.antiguedad, errors='coerce')\ntestdat.renta = pd.to_numeric(testdat.renta, errors='coerce')\n\ntestdat.loc[testdat['sexo'].isnull(),'sexo'] = 'V'\ntestdat.indrel_1mes = testdat.indrel_1mes.astype('str').str.slice(0,1)\ntestdat.loc[testdat['indrel_1mes'].isnull(), 'indrel_1mes'] = '1'\ntestdat.loc[testdat['tiprel_1mes'].isnull(), 'tiprel_1mes'] = 'I'\ntestdat.loc[testdat['canal_entrada'].isnull(), 'canal_entrada'] = 'MIS'\ntestdat.loc[testdat['cod_prov'].isnull(), 'cod_prov'] = 28.0\ntestdat.loc[testdat['segmento'].isnull(), 'segmento'] = '02 - PARTICULARES'\ntestdat[\"renta\"] = testdat[['renta','cod_prov']].groupby(\"cod_prov\").transform(lambda x: x.fillna(x.mean())) #Replace renta with provincial mean\ntestdat[\"age\"] = testdat[['age','cod_prov']].groupby(\"cod_prov\").transform(lambda x: x.fillna(x.mean())) #Replace age with provincial mean\ntestdat[\"antiguedad\"] = testdat[['antiguedad','cod_prov']].groupby(\"cod_prov\").transform(lambda x: x.fillna(x.mean())) #Replace antiguedad with provincial mean","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check to make sure all missing data has been filled\ntraindat.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 2: Feature Engineering\n\nFeature engineering is one of the most important parts of good Kaggle performance, and in this competition it was vital. Our first step was to bin the continuous variables, so we could treat all variables as binary in the distance matrix, easily scaling demographic and ownership variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"# some more data cleaning\n\ntraindat[\"fecha_alta\"] = traindat[\"fecha_alta\"].astype(\"datetime64\")\ntestdat[\"fecha_alta\"] = testdat[\"fecha_alta\"].astype(\"datetime64\")\n\n# Observation: based on (omitted) EDA, a pre/post 2011 split would make sense for fecha_alta; as credit recovered following the 2008 crash, we may expect to see different user types\n\n# Observation: on a log scale, the salary data is broadly normal. We can take low-medium-high bounds using quartiles\n\ntraindat[\"renta\"] = np.log(traindat[\"renta\"])\ntestdat[\"renta\"] = np.log(testdat[\"renta\"])\n\n# bin the continuous variables\n\nbins_dt = pd.date_range('1994-01-01', freq='16Y', periods=3)\nbins_str = bins_dt.astype(str).values\nlabels = ['({}, {}]'.format(bins_str[i-1], bins_str[i]) for i in range(1, len(bins_str))]\n\ntraindat['fecha_alta'] = pd.cut(traindat.fecha_alta.astype(np.int64)//10**9,\n                   bins=bins_dt.astype(np.int64)//10**9,\n                   labels=labels)\n\ntestdat['fecha_alta'] = pd.cut(testdat.fecha_alta.astype(np.int64)//10**9,\n                   bins=bins_dt.astype(np.int64)//10**9,\n                   labels=labels)\n\n\nbins_renta = [0,np.percentile(traindat.renta, 25),np.percentile(traindat.renta, 75),25]\n\ntraindat['renta'] = pd.cut(traindat.renta,\n                   bins=bins_renta)\n\ntestdat['renta'] = pd.cut(testdat.renta,\n                   bins=bins_renta)\n\n\nbins_age = [0,25,42,60,1000]\nlabels_age = ['young','middle','older','old']\n\ntraindat['age'] = pd.cut(traindat.age,\n                   bins=bins_age,\n                   labels=labels_age)\n\ntestdat['age'] = pd.cut(testdat.age,\n                   bins=bins_age,\n                   labels=labels_age)\n\n\nbins_anti = [-1,220,300]\nlabels_anti = ['new','old']\n\n#remove negative antiguedad values\ntraindat.antiguedad[traindat.antiguedad<0] = 0\n\ntraindat['antiguedad'] = pd.cut(traindat.antiguedad,\n                   bins=bins_anti,\n                   labels=labels_anti)\n\ntestdat['antiguedad'] = pd.cut(testdat.antiguedad,\n                   bins=bins_anti,\n                   labels=labels_anti)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 3: Data Subsetting \nJumping quickly ahead to ** step 3 **: now we have dealt with continuous variables, we can subset the data to only the months of interest. We use June 2015 (and its associated lagged month) in the final model, as well as the lagged May 2016 for the response variable. The data merge that follows breaks the 16GB kernel RAM limit if we don't bring the date subsetting forward.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"traindat = traindat[traindat.fecha_dato.isin(['2015-05-28','2015-06-28','2016-05-28'])]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This frees up space, allowing us to finish our feature engineering. The data as presented to us was based on ownership in each month, whereas we are interested in purchase in each month: our second step was to create two new features for each variable, which were ownership in the *previous* month and whether the product was purchased in each month. We used the latter as our response variables, and the former as our main explanatory variables to construct the distance matrix."},{"metadata":{"trusted":true},"cell_type":"code","source":"# similar to a SQL window function, we want to join each user with itself in the previous month. We first sort data based on key columns...\ntraindat = traindat.sort_values(['ncodpers','fecha_dato'],ascending=[True,True]).reset_index(drop=True)\nprint('sort completed')\n\n# ...then create a new dataset where the index is incremented...\ntraindat['new'] = traindat.index\ntrain_index = traindat.copy()\ntrain_index['new'] += 1\n\n# ...then merge the dataset with itself to add each user's purchases in the previous month (there is definitely a quicker way of doing this - I am still relatively new to Python!)\n# we rename these new columns with a '_previous' suffix \nmerge_drop_cols = demographic_cols.copy()\nmerge_drop_cols.remove('ncodpers')\ntraindat_use = pd.merge(traindat,train_index.drop(merge_drop_cols,1), on=['new','ncodpers'],how='left',suffixes=['','_previous'])\nprint('merge completed')\n\n# replace current with (current - previous) to obtain what we want: purchase indicators\nfor i in product_col:\n    traindat_use[i] = traindat_use[i]-traindat_use[i+\"_previous\"]\n    # replace negative values with 0: if a user gets rid of a product from month x to month x+1, this registers as no purchase in the evaluation metric, so we also treat it as no purchase made\n    traindat_use[i][traindat_use[i] < 0] = 0\n\n# fill in na values created by merge\ntraindat_use[product_col] = traindat_use[product_col].fillna(0)\nnew_product_col = [i + \"_previous\" for i in product_col]\ntraindat_use[new_product_col] = traindat_use[new_product_col].fillna(0)\n\n# delete redundant objects to free up memory\ndel train_index\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We also want to add purchase history columns to the test data set, for the purposes of making predictions\n\ntest_col = product_col + ['ncodpers']\ntestdat_use = pd.merge(testdat,traindat[traindat.fecha_dato=='2016-05-28'][test_col],on='ncodpers',how='left',suffixes=['','_previous'])\n\ntestdat_use.rename(\n    columns={i:j for i,j in zip(product_col,new_product_col)}, inplace=True\n)\n\ntestdat_use[new_product_col] = testdat_use[new_product_col].fillna(0)\n\n# delete redundant objects to free up memory\ndel traindat, testdat\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 4: De-duplicate the Data\nBuilding a similarity matrix for all test users takes a very long time and repeats a lot of work. Instead, we work out the similarity of each ** unique ** test user with all the training users, speeding up computation time enormously. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# pull through variables for memory-based CF\n\ntraindat_purchases = traindat_use[traindat_use.fecha_dato == '2015-06-28'][product_col].copy()\ntraindat_final = traindat_use[traindat_use.fecha_dato == '2015-06-28'][new_product_col].copy()\n\n# pull through variables for demographic-based CF\n\ndemog_col = ['sexo','age','fecha_alta','ind_nuevo','indrel','indresi','indfall','tipodom','ind_actividad_cliente']\ntraindat_demog_final = traindat_use[traindat_use.fecha_dato == '2015-06-28'][demog_col].copy()\n\n# transform demographic factor variables into binary format\n\nsexo_map = {'V': 1,'H': 0}\nage_map = {'old': 1,'young': 0}\nfecha_alta_map = {'(1994-12-31, 2010-12-31]': 1,'(2010-12-31, 2026-12-31]': 0}\nindresi_map = {'S': 1,'N': 0}\nindfall_map = {'S': 1,'N': 0}\n\ntraindat_demog_final.loc[traindat_demog_final['age']=='older', 'age'] = 'old'\ntraindat_demog_final.loc[traindat_demog_final['age']=='middle', 'age'] = 'young'\ntraindat_demog_final.sexo = [sexo_map[item] for item in traindat_demog_final.sexo]\ntraindat_demog_final.age = [age_map[item] for item in traindat_demog_final.age]\ntraindat_demog_final.fecha_alta = [fecha_alta_map[item] for item in traindat_demog_final.fecha_alta]\ntraindat_demog_final.indresi = [indresi_map[item] for item in traindat_demog_final.indresi]\ntraindat_demog_final.indfall = [indfall_map[item] for item in traindat_demog_final.indfall]\n\n# we want all the observed combinations of purchase history\nnew_product_col_aug = new_product_col + ['ncodpers']\ntestdat_final = testdat_use[new_product_col_aug].copy()\ntestdat_final_unique = testdat_final.drop('ncodpers',1).drop_duplicates().copy().reset_index(drop=True)\n#testdat_final_unique.shape\n# 6510 unique combinations of purchase history\n\n# transform demographic factor variables into binary format\n\ndemog_col_aug = demog_col + ['ncodpers']\ntestdat_demog_final = testdat_use[demog_col_aug].copy()\n\ntestdat_demog_final.loc[testdat_demog_final['age']=='older', 'age'] = 'old'\ntestdat_demog_final.loc[testdat_demog_final['age']=='middle', 'age'] = 'young'\ntestdat_demog_final.sexo = [sexo_map[item] for item in testdat_demog_final.sexo]\ntestdat_demog_final.age = [age_map[item] for item in testdat_demog_final.age]\ntestdat_demog_final.fecha_alta = [fecha_alta_map[item] for item in testdat_demog_final.fecha_alta]\ntestdat_demog_final.indresi = [indresi_map[item] for item in testdat_demog_final.indresi]\ntestdat_demog_final.indfall = [indfall_map[item] for item in testdat_demog_final.indfall] \n\ntestdat_demog_final_unique = testdat_demog_final.drop('ncodpers',1).drop_duplicates().copy().reset_index(drop=True)\n\n#testdat_demog_final_unique.shape\n#114 unique combinations of demographics\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We now have everything we need to perform memory-based and demographic-based CF. If we want to run either of these models, we can use all the relevant data to get the strongest possible submission. However, if we want to combine these models, we need to use an evaluation metric to ascertain optimal combination weights. To do this, we can train a first-stage model using 80% of the training data as our 'training' and 20% as our 'test' data, find optimal weights, and run the final model on all the data using these weights."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split the training data into 'training' and 'test' sets\n\n# create 80% index\ntraindat_index = np.random.rand(len(traindat_final)) < 0.8\n# create traindat_train\ntraindat_train = traindat_final[traindat_index]\n# create traindat_test\ntraindat_test = traindat_final[~traindat_index]\n# make traindat_test unique\ntraindat_test_unique = traindat_test.drop_duplicates().copy().reset_index(drop=True)\n# create traindat_purchases\ntraindat_purchases_train = traindat_purchases[traindat_index]\n# create traindat_purchases_test for verification\ntraindat_purchases_test = traindat_purchases[~traindat_index]\n# create training ncodpers index\ntraindat_ncodpers = traindat_use[traindat_use.fecha_dato == '2015-06-28'][traindat_index][['fecha_dato','ncodpers']]\ntraindat_test_ncodpers = traindat_use[traindat_use.fecha_dato == '2015-06-28'][~traindat_index][['fecha_dato','ncodpers']]\n\n# repeat for demographic columns\n# create traindat_demog_train\ntraindat_demog_train = traindat_demog_final[traindat_index]\n# create traindat_demog_test\ntraindat_demog_test = traindat_demog_final[~traindat_index]\n# make traindat_demog_test unique\ntraindat_demog_test_unique = traindat_demog_test.drop_duplicates().copy().reset_index(drop=True)\n# purchase indices are the same as for memory-based model data\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 5: Build Demographic-Based and Memory-Based Similarity Matrices\nA similarity matrix, even on a few thousand unique data points, would be vast, so we iterated over each of the unique test profiles. The steps of the algorithm should be clear in the below code, but at a high level we calculated distances to each training point, weighted training purchases by the inverse distances to obtain a purchase probability, and nullified the purchase probability of owned items."},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_product_col = [i + \"_predict\" for i in new_product_col]\n\ndef probability_calculation(dataset,training,training_purchases,used_columns,metric,test_remap,print_option=False):\n    # 'dataset' takes the unique test data with purchase/demographic history; 'training' are the training data that we calculate distances to\n    n = dataset.shape[0]\n    for index, row in dataset.iterrows():\n        if print_option == True:\n            print(str(index) + '/' + str(n))\n        row_use = row.to_frame().T\n        #store purchase history for the test users\n        row_history = row_use[used_columns]\n        #calculate distances between the test point and each training point based on selected binary features\n        #use 'manhattan' when data was binary - when weighted against demographics, use Euclidean\n        distances = metrics.pairwise_distances(row_use,training,metric=metric) + 1e-6\n        #normalise distances: previously used 24-distances, and 1/(1+distances), but the asymptotic behaviour of 1/distances gives the most accurate predictions.\n        norm_distances = 1/distances\n        #take dot product between distance to training point and training point's purchase history to obtain ownership likelihood matrix\n        sim = pd.DataFrame(norm_distances.dot(training_purchases)/np.sum(norm_distances),columns = new_product_col)\n        if(index == 0):\n            probabilities = sim\n        else:\n            probabilities = probabilities.append(sim)\n    print(\"probabilities calculated\")\n    # reindex users for join\n    reindexed_output = probabilities.reset_index().drop('index',axis=1).copy()\n    indexed_unique_test = dataset.reset_index().drop('index',axis=1).copy()\n    output_unique = indexed_unique_test.join(reindexed_output,rsuffix='_predict')\n    output_final = pd.merge(test_remap,output_unique,on=used_columns,how='left')\n    # only select relevant products\n    output_final = output_final.drop(used_columns,1)\n    output_final.columns = output_final.columns.str.replace(\"_predict\", \"\")\n    output_final.columns = output_final.columns.str.replace(\"_previous\", \"_predict\")\n    # now we have all test probabilities - can average and compare with results\n    return output_final\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 6: Combine Demographic-Based and Memory-Based Probabilities\nUsing the probabilities derived in step 5, we can derive a weighted average (for 5 candidate weights, ranging from 0-1) to incorporate both data sources into our model. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate memory-based similarities\nprobabilities_memory = probability_calculation(traindat_test_unique,traindat_train,traindat_purchases_train,new_product_col,'manhattan',traindat_test)\n# calculate demographic-based similarities\nprobabilities_demog = probability_calculation(traindat_demog_test_unique,traindat_demog_train,traindat_purchases_train,demog_col,'manhattan',traindat_demog_test)\n# average predictions for a range of mixing probabilities\nprobabilities_avg_90 = 0.9*probabilities_memory + 0.1*probabilities_demog\nprobabilities_avg_70 = 0.7*probabilities_memory + 0.3*probabilities_demog\nprobabilities_avg_50 = 0.5*probabilities_memory + 0.5*probabilities_demog","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we have probabilities, we want to remove the possibility of predicting a product that a user already owns. Nullifying previous purchases can be done at step 5, but we do it here to avoid repeating work."},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_col = [i + \"_predict\" for i in product_col]\npredict_previous_col = predict_col + new_product_col\n\ndef purchase_nullifier(probabilities,purchase_history,print_option=False):\n    # function to 'nullify' any probabilities that would lead to an owned product being predicted\n    # probabilities should have 24 columns with suffix 'predict', purchase_history should have 24 columns with suffix 'previous'\n    # join two datasets together\n    purchase_history = purchase_history.reset_index(drop=True)\n    joined_data = purchase_history.join(probabilities)\n    # shrink dataset to deal with large-scale data\n    unique_data = joined_data.drop_duplicates().copy().reset_index(drop=True)\n    n = unique_data.shape[0]\n    print(\"data joined\")\n    for index,row in unique_data.iterrows():\n        if print_option == True:\n            print(str(index) + \"/\" + str(n))\n        row = row.to_frame().T\n        # subset dataframe and rename columns for nullification\n        row_purchases = row[new_product_col]\n        row_purchases.columns = row_purchases.columns.str.replace(\"_previous\",\"\")\n        row_probabilities = row[predict_col]\n        row_probabilities.columns = row_probabilities.columns.str.replace(\"_predict\",\"\")\n        prob_norm = (1-row_purchases).multiply(row_probabilities,axis=0)\n        if(index == 0):\n            output_norm = prob_norm\n        else:\n            output_norm = output_norm.append(prob_norm)\n    print(\"nullification complete\")\n    # duplicate back up to original dataset\n    # add columns to enable merge\n    output_index = output_norm.reset_index(drop=True)\n    prob_predict = output_index.join(unique_data)\n    scaled_predict = pd.merge(joined_data,prob_predict,how='left')\n    output = scaled_predict[product_col]\n    output.columns = output.columns.str.replace(\"ult1\",\"ult1_predict\")\n    return output\n\n# can output these probabilities for model averaging with other outputs or cast to predictions for a submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nulled_probabilities_100 = purchase_nullifier(probabilities_memory,traindat_test)\nnulled_probabilities_90 = purchase_nullifier(probabilities_avg_90,traindat_test)\nnulled_probabilities_70 = purchase_nullifier(probabilities_avg_70,traindat_test)\nnulled_probabilities_50 = purchase_nullifier(probabilities_avg_50,traindat_test)\nnulled_probabilities_0 = purchase_nullifier(probabilities_demog,traindat_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 7: Derive Recommendations\nKeeping purchase probabilities rather than simply outputting the recommendations is very useful for further analysis, e.g. model averaging. In this step we use the probabilities to derive recommendations."},{"metadata":{"trusted":true},"cell_type":"code","source":"def probabilities_to_predictions(probabilities,ncodpers,print_option=False):\n# ncodpers is a dataframe with two columns: fecha_dato and ncodpers (corresponding to probabilities order)    \n    # we make probabilities unique to speed upc calculations\n    unique_probabilities = probabilities.drop_duplicates().copy().reset_index(drop=True)\n    print(unique_probabilities.shape)\n    n = unique_probabilities.shape[0]\n    for index, row in unique_probabilities.iterrows():\n        if print_option == True:\n            print(str(index) + '/' + str(n))\n        row_use = row.to_frame().T\n        # rank list of product recommendations\n        arank = row_use.apply(np.argsort, axis=1)\n        ranked_cols = row_use.columns.to_series()[arank.values[:,::-1][:,:7]]\n        new_frame = pd.DataFrame(ranked_cols)\n        #concatenate all 7 predictions\n        recoms = new_frame[0] + ' ' + new_frame[1] + ' ' + new_frame[2] + ' ' + new_frame[3] + ' ' + new_frame[4] + ' ' + new_frame[5] + ' ' + new_frame[6]\n        recoms_final = recoms.str.replace('_predict', '', regex=True)\n        if(index == 0):\n            predictions = recoms_final\n        else:\n            predictions = predictions.append(recoms_final)\n    # merge predictions back to initial indices for full dataset\n    mapped_predictions = predictions.to_frame().rename(columns={0:'added_products'}).reset_index(drop=True)\n    output_unique = mapped_predictions.join(unique_probabilities)\n    output_final = pd.merge(probabilities,output_unique,on=predict_col,how='left')\n    # add ncodpers for final submission file\n    no_index_ncodpers = ncodpers.copy().reset_index(drop=True)\n    output_ncodpers = no_index_ncodpers.join(output_final['added_products']).drop('fecha_dato',axis=1)\n    return output_ncodpers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_output_100 = probabilities_to_predictions(nulled_probabilities_100,traindat_test_ncodpers)\npredictions_output_90 = probabilities_to_predictions(nulled_probabilities_90,traindat_test_ncodpers)\npredictions_output_70 = probabilities_to_predictions(nulled_probabilities_70,traindat_test_ncodpers)\npredictions_output_50 = probabilities_to_predictions(nulled_probabilities_50,traindat_test_ncodpers)\npredictions_output_0 = probabilities_to_predictions(nulled_probabilities_0,traindat_test_ncodpers)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The final piece of the puzzle is an evaluation metric, to ascertain the performance of each model (i.e. each mixing probability)."},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluation_col = product_col + ['added_products']\n\ndef evaluation_metric(predictions,reality,print_option=False):\n    # predictions is a list of the top seven purchase likelihood indicators; reality is the actual purchases\n    reality = reality.reset_index(drop=True)\n    # find unique combinations to speed up function: merge data, group_by, count (then multiply results at the end)\n    reality['added_products'] = predictions['added_products']\n    data_unique = reality.drop_duplicates().copy().reset_index(drop=True)\n    predictions_unique = data_unique['added_products'].to_frame()\n    reality_unique = data_unique.drop('added_products',1)\n    n = predictions_unique.shape[0]\n    for index, row in predictions_unique.iterrows():\n        if print_option == True:\n            print(str(index) + '/' + str(n))\n        prediction_use = row.to_frame().T['added_products'].str.split(' ',expand=True).T\n        prediction_use = prediction_use.rename(columns={list(prediction_use)[0]:'predict_products'})\n        #print(prediction_use)\n        # only take top 7 products purchased\n        reality_use = reality_unique.iloc[index].to_frame()\n        reality_use = reality_use.rename(columns={list(reality_use)[0]:'added_products'})\n        reality_use['product_name'] = reality_use.index\n        reality_use = reality_use[reality_use.added_products==1]\n        reality_use['ind'] = 1\n        #print(reality_use)\n        if reality_use.empty:\n            P = [0]\n        else:\n            # calculate precision @7: what average proportion of our predictions are purchased?\n            P = [precision_at_k(prediction_use,reality_use)]\n        if index == 0:\n            eval_sum = P\n        else:\n            eval_sum.extend(P)\n    # duplicate back up\n    print('precisions calculated')\n    data_unique['precision'] = eval_sum\n    reality_final = pd.merge(reality,data_unique,on=evaluation_col,how='left')\n    U = predictions.shape[0]\n    output = sum(reality_final.precision)/U\n    return output\n\ndef precision_at_k(prediction,reality):\n    # 'prediction' is a data frame with a column 'predict_products' containing our 7 predictions\n    # 'reality' is a data frame with a column 'added_products' containing any products purchased (always non-empty)\n    summand = min(prediction.shape[0],7)\n    sum_prec = 0\n    for k in range(summand):\n        # for each k, calculate precision at k (careful with 0 index)\n        top_k_predictions = prediction.head(k+1)\n        # join additions to reduced predictions\n        add_vs_pred = pd.merge(reality,top_k_predictions,left_on='product_name',right_on='predict_products',how='inner')\n        sum_prec = sum_prec + sum(add_vs_pred.ind)/top_k_predictions.shape[0]\n    denom = min(reality.shape[0],7)\n    # always defined as in evaluation_metric function 'reality_use' is always non-empty\n    output = sum_prec/denom\n    return output \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluation_100 = evaluation_metric(predictions_output_100,traindat_purchases_test)\nevaluation_90 = evaluation_metric(predictions_output_90,traindat_purchases_test)\nevaluation_70 = evaluation_metric(predictions_output_70,traindat_purchases_test)\nevaluation_50 = evaluation_metric(predictions_output_50,traindat_purchases_test)\nevaluation_0 = evaluation_metric(predictions_output_0,traindat_purchases_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"all memory: \" + str(evaluation_100) + '\\n' + \n      \"90% memory: \" + str(evaluation_90) + '\\n' + \n      \"70% memory: \" + str(evaluation_70) + '\\n' + \n      \"50% memory: \" + str(evaluation_50) + '\\n' + \n      \"all demographics: \" + str(evaluation_0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 8: Re-run model using all training data for optimal mixing parameter\n90% memory gives the strongest results: the optimum seems to lie somewhere around 85%. Now that we have an optimal mixing probability, we can iterate through the entire process, using all of the training data to return our optimal prediction that leverages both demographic and purchase data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate probabilities\nprobability_85_memory = probability_calculation(testdat_final_unique,traindat_final,traindat_purchases,new_product_col,'manhattan',testdat_final)\nprobability_85_demog = probability_calculation(testdat_demog_final_unique,traindat_demog_final,traindat_purchases,demog_col,'manhattan',testdat_demog_final)\n\n# average probabilities\nprobability_avg_85 = 0.85*probability_85_memory + 0.15*probability_85_demog\n\n# write csv of averaged probabilities\nprobability_avg_85.to_csv(\"probabilities_85_avg.csv\",index=False)\n\n# null previous ownership\nnulled_probability_85 = purchase_nullifier(probability_avg_85[predict_col],testdat_final[new_product_col])\n\n# map to predictions - check dimensions\ntestdat_ncodpers = testdat_use[['fecha_dato','ncodpers']]\npredictions_output_85 = probabilities_to_predictions(nulled_probability_85,testdat_ncodpers)\n\n# send predictions to csv\npredictions_output_85.to_csv('submission.csv',index=False)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The submission in its current form has a private score of 0.02644 and a public score of 0.02619, which is just outside the top 50% of scores. Not bad with no model-based input! This compares to respective scores of 0.02633 and 0.02610 when no mixing probabilities are used (i.e. we only use memory-based cf); a small improvement, but significant in a Kaggle context."},{"metadata":{},"cell_type":"markdown","source":"## Next Steps\nThis notebook is very much intended as an introduction to recommendation systems. There are several lines of analysis that could be pursued to improve upon this model's score:\n* ** Test different distance metrics and demographic weightings: ** in our final algorithm we use a very straightforward distance metric, and weight purchases by taking the inverse sum of this straightforward metric. There are improvements to be made by sharpening up this methodology.\n* ** Include item-based CF and average results: ** the products are real, and we can derive product features manually. This could help us to build an item-based CF approach, which could be combined with our algorithmically-defined memory-based probabilities to enhance to model. \n* ** Try model-based CF: ** memory-based CF is the most straightforward technique, and it will be interesting to compare the performance of these models with the performance of out memory-based algorithm.\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}