{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import Libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport sys\n\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pickle\nimport skimage\nfrom skimage.feature import greycomatrix, greycoprops\nfrom skimage.filters import sobel\nfrom skimage import color\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\n\nfrom keras import layers\nimport keras.backend as K\nfrom keras.models import Sequential, Model\nfrom keras.preprocessing import image\nfrom keras.layers import Input, Dense, Activation, Dropout\nfrom keras.layers import Flatten, BatchNormalization, Conv2D\nfrom keras.layers import MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D \nfrom keras.applications.imagenet_utils import preprocess_input\nfrom keras.applications.vgg16 import VGG16\n\nfrom PIL import Image\nfrom tqdm import tqdm\nimport random as rnd\nimport cv2\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom numpy import expand_dims\n\n!pip install livelossplot\nfrom livelossplot import PlotLossesKeras\n\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2022-03-24T04:13:37.940891Z","iopub.execute_input":"2022-03-24T04:13:37.941381Z","iopub.status.idle":"2022-03-24T04:13:54.390738Z","shell.execute_reply.started":"2022-03-24T04:13:37.941291Z","shell.execute_reply":"2022-03-24T04:13:54.389876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading Dataset \nWe'll use here the Pandas to load the dataset into memory","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('../input/happy-whale-and-dolphin/train.csv')\ntrain_df['path'] = '../input/happy-whale-and-dolphin/train_images/' + train_df['image']\n\npred_df = pd.read_csv('../input/happy-whale-and-dolphin/sample_submission.csv')\npred_df['path'] = '../input/happy-whale-and-dolphin/test_images/' + pred_df['image']","metadata":{"execution":{"iopub.status.busy":"2022-03-24T04:13:54.392649Z","iopub.execute_input":"2022-03-24T04:13:54.392893Z","iopub.status.idle":"2022-03-24T04:13:54.543507Z","shell.execute_reply.started":"2022-03-24T04:13:54.392864Z","shell.execute_reply":"2022-03-24T04:13:54.542793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Having two csv files\n* train.csv - contain image name,species and individual_id\n* sample_submission.csv - contain image name, dummy label for the images in the test folde\n## And two folders contain the images\n* train - having 51033 images of different type of whales and dolphins. There Labels have provided in the train.csv file\n* test - having 27956 images of different type of whales and dolphins. We need to predict their labels","metadata":{}},{"cell_type":"code","source":"train_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T04:13:54.546577Z","iopub.execute_input":"2022-03-24T04:13:54.546807Z","iopub.status.idle":"2022-03-24T04:13:54.565417Z","shell.execute_reply.started":"2022-03-24T04:13:54.54678Z","shell.execute_reply":"2022-03-24T04:13:54.564675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Train samples count: ', len(train_df))\ntrain_df.columns","metadata":{"execution":{"iopub.status.busy":"2022-03-24T04:13:54.568579Z","iopub.execute_input":"2022-03-24T04:13:54.56881Z","iopub.status.idle":"2022-03-24T04:13:54.576471Z","shell.execute_reply.started":"2022-03-24T04:13:54.568776Z","shell.execute_reply":"2022-03-24T04:13:54.575604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Species Count: ',len(train_df['species'].value_counts()))\ntrain_df['species'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-03-24T04:13:54.578321Z","iopub.execute_input":"2022-03-24T04:13:54.578844Z","iopub.status.idle":"2022-03-24T04:13:54.602454Z","shell.execute_reply.started":"2022-03-24T04:13:54.578804Z","shell.execute_reply":"2022-03-24T04:13:54.601734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Cleaning\n## Fixing Duplicate Labels\n* bottlenose_dolpin -> bottlenose_dolphin\n* kiler_whale -> killer_whale\n* beluga -> beluga_whale\n## Changing Label due to extreme similarities\n* globis & pilot_whale -> short_finned_pilot_whale","metadata":{}},{"cell_type":"code","source":"print('Before fixing duplicate labels : ')\nprint(\"Number of unique species : \", train_df['species'].nunique())\n\ntrain_df['species'].replace({\n    'bottlenose_dolpin' : 'bottlenose_dolphin',\n    'kiler_whale' : 'killer_whale',\n    'beluga' : 'beluga_whale',\n    'globis' : 'short_finned_pilot_whale',\n    'pilot_whale' : 'short_finned_pilot_whale'\n},inplace =True)\n\nprint('\\nAfter fixing duplicate labels : ')\nprint(\"Number of unique species : \", train_df['species'].nunique())\n\n\ntrain_df['class'] = train_df['species'].apply(lambda x: x.split('_')[-1])\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-24T04:13:54.603878Z","iopub.execute_input":"2022-03-24T04:13:54.604331Z","iopub.status.idle":"2022-03-24T04:13:54.667751Z","shell.execute_reply.started":"2022-03-24T04:13:54.604266Z","shell.execute_reply":"2022-03-24T04:13:54.667045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Checking missing data\nLets check if there is any missing values in our dataset","metadata":{}},{"cell_type":"code","source":"train_df.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2022-03-24T04:13:54.669245Z","iopub.execute_input":"2022-03-24T04:13:54.669741Z","iopub.status.idle":"2022-03-24T04:13:54.713794Z","shell.execute_reply.started":"2022-03-24T04:13:54.669703Z","shell.execute_reply":"2022-03-24T04:13:54.712682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(os.listdir('../input/happy-whale-and-dolphin/train_images'))","metadata":{"execution":{"iopub.status.busy":"2022-03-24T04:13:54.715418Z","iopub.execute_input":"2022-03-24T04:13:54.716029Z","iopub.status.idle":"2022-03-24T04:13:55.431759Z","shell.execute_reply.started":"2022-03-24T04:13:54.715987Z","shell.execute_reply":"2022-03-24T04:13:55.430947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualization\n### Looking at some random beauties \nIt's a great deal of fun to explore the data and play around with matplotlib","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (15,12))\nfor idx,i in enumerate(train_df.species.unique()):\n    plt.subplot(4,7,idx+1)\n    df = train_df[train_df['species'] ==i].reset_index(drop = True)\n    image_path = df.loc[rnd.randint(0, len(df))-1,'path']\n    img = Image.open(image_path)\n    img = img.resize((224,224))\n    plt.imshow(img)\n    plt.axis('off')\n    plt.title(i)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-24T04:13:55.433147Z","iopub.execute_input":"2022-03-24T04:13:55.433762Z","iopub.status.idle":"2022-03-24T04:14:00.132828Z","shell.execute_reply.started":"2022-03-24T04:13:55.433719Z","shell.execute_reply":"2022-03-24T04:14:00.130812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_species(df,species_name):\n    plt.figure(figsize = (12,12))\n    species_df = df[df['species'] ==species_name].reset_index(drop = True)\n    plt.suptitle(species_name)\n    for idx,i in enumerate(np.random.choice(species_df['path'],32)):\n        plt.subplot(8,8,idx+1)\n        image_path = i\n        img = Image.open(image_path)\n        img = img.resize((224,224))\n        plt.imshow(img)\n        plt.axis('off')\n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-24T04:14:00.13534Z","iopub.execute_input":"2022-03-24T04:14:00.13558Z","iopub.status.idle":"2022-03-24T04:14:00.144943Z","shell.execute_reply.started":"2022-03-24T04:14:00.13555Z","shell.execute_reply":"2022-03-24T04:14:00.143825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plotting more images from each species","metadata":{}},{"cell_type":"code","source":"for species in train_df['species'].unique():\n    #print('\\n\\n')\n    plot_species(train_df , species)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T04:14:00.14685Z","iopub.execute_input":"2022-03-24T04:14:00.147585Z","iopub.status.idle":"2022-03-24T04:16:03.911035Z","shell.execute_reply.started":"2022-03-24T04:14:00.147543Z","shell.execute_reply":"2022-03-24T04:16:03.909424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Lets see some image by individual_id\nWe have to predict individual_id from image. So lets see how each individual looks like.","metadata":{}},{"cell_type":"code","source":"def plot_individual(df,individual_id):\n    plt.figure(figsize = (12,12))\n    species_df = df[df['individual_id'] ==individual_id].reset_index(drop = True)\n    plt.suptitle(individual_id)\n    for idx,i in enumerate(np.random.choice(species_df['path'],24)):\n        plt.subplot(8,8,idx+1)\n        image_path = i\n        img = Image.open(image_path)\n        img = img.resize((224,224))\n        plt.imshow(img)\n        plt.axis('off')\n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-24T04:16:49.462773Z","iopub.execute_input":"2022-03-24T04:16:49.463136Z","iopub.status.idle":"2022-03-24T04:16:49.473522Z","shell.execute_reply.started":"2022-03-24T04:16:49.463094Z","shell.execute_reply":"2022-03-24T04:16:49.472645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Top 5 most frequent individual","metadata":{}},{"cell_type":"code","source":"top_5_ids = train_df.individual_id.value_counts().head(5)\nfor i in top_5_ids.index:\n    #print('\\n\\n')\n    plot_individual(train_df , i)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T04:16:49.475426Z","iopub.execute_input":"2022-03-24T04:16:49.475976Z","iopub.status.idle":"2022-03-24T04:17:16.593591Z","shell.execute_reply.started":"2022-03-24T04:16:49.475936Z","shell.execute_reply":"2022-03-24T04:17:16.592911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Top 5 least frequent individual\nWe will get duplicate images because many individual has only one training image.","metadata":{}},{"cell_type":"code","source":"last_5_ids = train_df.individual_id.value_counts().tail(5)\nfor i in last_5_ids.index:\n    #print('\\n\\n')\n    plot_individual(train_df , i)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T04:16:31.871563Z","iopub.execute_input":"2022-03-24T04:16:31.871991Z","iopub.status.idle":"2022-03-24T04:16:49.399092Z","shell.execute_reply.started":"2022-03-24T04:16:31.871953Z","shell.execute_reply":"2022-03-24T04:16:49.395518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets see some test images","metadata":{}},{"cell_type":"code","source":"t_df = pd.read_csv('../input/happy-whale-and-dolphin/sample_submission.csv')\nt_df['path'] = '../input/happy-whale-and-dolphin/test_images/' + t_df['image']\n\ndef plot_testimages(df):\n    plt.figure(figsize = (12,12))\n    plt.suptitle('Test Images')\n    for idx,i in enumerate(np.random.choice(df['path'],48)):\n        plt.subplot(8,8,idx+1)\n        image_path = i\n        img = Image.open(image_path)\n        img = img.resize((224,224))\n        plt.imshow(img)\n        plt.axis('off')\n    plt.tight_layout()\n    plt.show()\n\n# plot_testimages(t_df)\n# del t_df","metadata":{"execution":{"iopub.status.busy":"2022-03-24T04:16:49.401153Z","iopub.execute_input":"2022-03-24T04:16:49.401453Z","iopub.status.idle":"2022-03-24T04:16:49.459314Z","shell.execute_reply.started":"2022-03-24T04:16:49.401413Z","shell.execute_reply":"2022-03-24T04:16:49.458227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observations regarding handpicked images\n* There are some abnormal images in both train and test dataset\n* Some training images contains people, boats, birds, penguins etc\n* Many training images are cropped but some are not.\n* The uncropped images must be taken care of.\n* There are some images take from under water","metadata":{}},{"cell_type":"markdown","source":"## Class Distribution Analysis\nIn this section we will be analyzing the number of training and test samples in each class. It will give us a better understanding of our dataset and provide us the necessary information to preprocess our dataset before the training phase.","metadata":{}},{"cell_type":"code","source":"plot = sns.countplot(x = train_df['class'], color = '#2596be')\nsns.despine()\nplot.set_title('Class Distribution\\n', font = 'serif', x = 0.1, y=1, fontsize = 16);\nplot.set_ylabel(\"Count\", x = 0.02, font = 'serif', fontsize = 12)\nplot.set_xlabel(\"Specie\", fontsize = 12, font = 'serif')\n\nfor p in plot.patches:\n    plot.annotate(format(p.get_height(), '.0f'), (p.get_x() + p.get_width() / 2, p.get_height()), \n       ha = 'center', va = 'center', xytext = (0, -20),font = 'serif', textcoords = 'offset points', size = 15)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T04:17:16.594816Z","iopub.execute_input":"2022-03-24T04:17:16.595151Z","iopub.status.idle":"2022-03-24T04:17:16.844128Z","shell.execute_reply.started":"2022-03-24T04:17:16.595119Z","shell.execute_reply":"2022-03-24T04:17:16.843459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Percentage of images of whale and dolphin in the datasetÂ¶**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(5,5))\nclass_cnt = train_df.groupby(['class']).size().reset_index(name = 'counts')\ncolors = sns.color_palette('Paired')[0:9]\nplt.pie(class_cnt['counts'], labels=class_cnt['class'], colors=colors, autopct='%1.1f%%')\nplt.legend(loc='upper left')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-24T04:17:16.84548Z","iopub.execute_input":"2022-03-24T04:17:16.845742Z","iopub.status.idle":"2022-03-24T04:17:16.962449Z","shell.execute_reply.started":"2022-03-24T04:17:16.845706Z","shell.execute_reply":"2022-03-24T04:17:16.961626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Number of training images of each species**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(8,8))\nsns.countplot(data=train_df, y = 'species',  palette='crest', dodge=False)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-24T04:17:16.964006Z","iopub.execute_input":"2022-03-24T04:17:16.96425Z","iopub.status.idle":"2022-03-24T04:17:17.364316Z","shell.execute_reply.started":"2022-03-24T04:17:16.964215Z","shell.execute_reply":"2022-03-24T04:17:17.363542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Number of training images of each species of whale and dolphin**","metadata":{}},{"cell_type":"code","source":"fig,ax = plt.subplots(1,2,figsize=(10,5))\n\nwhales = train_df[train_df['class']=='whale']\ndolphins = train_df[train_df['class']!='whale']\n\nsns.countplot(y=\"species\", data=whales, order=whales.iloc[0:][\"species\"].value_counts().index, ax=ax[0], color = \"#0077b6\")\nax[0].set_title('Most frequent whales')\nax[0].set_ylabel(None)\n    \nsns.countplot(y=\"species\", data=dolphins,order=dolphins.iloc[0:][\"species\"].value_counts().index, ax=ax[1], color = \"#90e0ef\")\nax[1].set_title('Most frequent dolphins')\nax[1].set_ylabel(None)\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-24T04:17:17.365428Z","iopub.execute_input":"2022-03-24T04:17:17.365684Z","iopub.status.idle":"2022-03-24T04:17:17.893819Z","shell.execute_reply.started":"2022-03-24T04:17:17.365631Z","shell.execute_reply":"2022-03-24T04:17:17.892981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Number of training images of top 10 individuals**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,4))\ntop_ten_ids = train_df.individual_id.value_counts().head(24)\ntop_ten_ids = pd.DataFrame({'individual_id':top_ten_ids.index, 'frequency':top_ten_ids.values})\n\nplt.bar(top_ten_ids['individual_id'],top_ten_ids['frequency'],width = 0.8,color='c',zorder=4)\nplt.xticks(rotation=90)\nplt.ylabel(\"frequency\")\nplt.xlabel(\"Individual Ids\")\nplt.title(\"Top 10 Individual Ids used by frequency\")\nplt.grid(visible = True, color ='grey',linestyle ='-', linewidth = 0.9,alpha = 0.2, zorder=0)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-24T04:17:17.895074Z","iopub.execute_input":"2022-03-24T04:17:17.895625Z","iopub.status.idle":"2022-03-24T04:17:18.236031Z","shell.execute_reply.started":"2022-03-24T04:17:17.895585Z","shell.execute_reply":"2022-03-24T04:17:18.235302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Plot the value count graph of each individual**","metadata":{}},{"cell_type":"code","source":"train_df['individual_id'].value_counts().plot()\nplt.xticks(rotation=90)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-24T04:17:18.237097Z","iopub.execute_input":"2022-03-24T04:17:18.237462Z","iopub.status.idle":"2022-03-24T04:17:18.459984Z","shell.execute_reply.started":"2022-03-24T04:17:18.237424Z","shell.execute_reply":"2022-03-24T04:17:18.459301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Number of unique individuals in the dataset**","metadata":{}},{"cell_type":"code","source":"len(train_df.individual_id.unique())","metadata":{"execution":{"iopub.status.busy":"2022-03-24T04:17:18.461395Z","iopub.execute_input":"2022-03-24T04:17:18.461638Z","iopub.status.idle":"2022-03-24T04:17:18.47474Z","shell.execute_reply.started":"2022-03-24T04:17:18.461603Z","shell.execute_reply":"2022-03-24T04:17:18.473638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Image count of individuals**","metadata":{}},{"cell_type":"code","source":"train_df['count'] = train_df.groupby('individual_id',as_index=False)['individual_id'].transform(lambda x: x.count())\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-24T04:17:18.476045Z","iopub.execute_input":"2022-03-24T04:17:18.476389Z","iopub.status.idle":"2022-03-24T04:17:38.740984Z","shell.execute_reply.started":"2022-03-24T04:17:18.47635Z","shell.execute_reply":"2022-03-24T04:17:38.740318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Individuals with only one training image**","metadata":{}},{"cell_type":"code","source":"train_df[train_df['count']==1]","metadata":{"execution":{"iopub.status.busy":"2022-03-24T04:17:38.742141Z","iopub.execute_input":"2022-03-24T04:17:38.742461Z","iopub.status.idle":"2022-03-24T04:17:38.760604Z","shell.execute_reply.started":"2022-03-24T04:17:38.742423Z","shell.execute_reply":"2022-03-24T04:17:38.759809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Percentage of Individuals with less then 5 images**","metadata":{}},{"cell_type":"code","source":"tmp = train_df[train_df['count']<=4]\nlen(tmp)/len(train_df)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T04:17:38.761917Z","iopub.execute_input":"2022-03-24T04:17:38.762233Z","iopub.status.idle":"2022-03-24T04:17:38.772553Z","shell.execute_reply.started":"2022-03-24T04:17:38.762196Z","shell.execute_reply":"2022-03-24T04:17:38.771703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Percentage of Individuals with more then 20 images**","metadata":{}},{"cell_type":"code","source":"count = 0\nfor i in train_df['count']:\n    if(i > 21):\n        count += 1\nprint(count/len(train_df))","metadata":{"execution":{"iopub.status.busy":"2022-03-24T04:17:38.774Z","iopub.execute_input":"2022-03-24T04:17:38.774621Z","iopub.status.idle":"2022-03-24T04:17:38.792632Z","shell.execute_reply.started":"2022-03-24T04:17:38.774579Z","shell.execute_reply":"2022-03-24T04:17:38.79194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observation Regarding Class Distribution\nThere is a huge disbalance in the data. There are many classes with only one or several samples:\n\n* Total Number of individuals are 15587\n* 9258 individuals have just one image\n* Single whale with most images have 400 of them\n* Images dsitribution:\n    * almost 40% comes from whales with 4 or less images.\n    * almost 23% comes from whales with 5-20 images.\n    * rest 37% comes from individual with >20 images.","metadata":{}},{"cell_type":"markdown","source":"## Image Resolutions","metadata":{}},{"cell_type":"code","source":"widths, heights = [], []\n\nfor path in tqdm(train_df[\"path\"]):\n    width, height = Image.open(path).size\n    widths.append(width)\n    heights.append(height)\n    \ntrain_df[\"width\"] = widths\ntrain_df[\"height\"] = heights\ntrain_df[\"dimension\"] = train_df[\"width\"] * train_df[\"height\"]","metadata":{"execution":{"iopub.status.busy":"2022-03-24T04:17:38.794038Z","iopub.execute_input":"2022-03-24T04:17:38.794359Z","iopub.status.idle":"2022-03-24T04:25:19.84535Z","shell.execute_reply.started":"2022-03-24T04:17:38.794316Z","shell.execute_reply":"2022-03-24T04:25:19.843648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Lets see some small images**","metadata":{}},{"cell_type":"code","source":"train_df.sort_values('width').head(84)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T04:25:19.846678Z","iopub.execute_input":"2022-03-24T04:25:19.846948Z","iopub.status.idle":"2022-03-24T04:25:19.881862Z","shell.execute_reply.started":"2022-03-24T04:25:19.846913Z","shell.execute_reply":"2022-03-24T04:25:19.881126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Color Analysis\nWe need to do some color analysis to get an ida about the augmentation technique needed for this problem","metadata":{}},{"cell_type":"code","source":"def is_grey_scale(givenImage):\n    w,h = givenImage.size\n    for i in range(w):\n        for j in range(h):\n            r,g,b = givenImage.getpixel((i,j))\n            if r != g != b: return False\n    return True","metadata":{"execution":{"iopub.status.busy":"2022-03-24T04:25:19.885429Z","iopub.execute_input":"2022-03-24T04:25:19.886006Z","iopub.status.idle":"2022-03-24T04:25:19.891407Z","shell.execute_reply.started":"2022-03-24T04:25:19.885975Z","shell.execute_reply":"2022-03-24T04:25:19.890679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Check color scale of Train images**","metadata":{}},{"cell_type":"code","source":"sampleFrac = 0.1\n#get our sampled images\nisGreyList = []\nfor imageName in train_df['path'].sample(frac=sampleFrac):\n    val = Image.open(imageName).convert('RGB')\n    isGreyList.append(is_grey_scale(val))\nprint(np.sum(isGreyList) / len(isGreyList))\ndel isGreyList","metadata":{"execution":{"iopub.status.busy":"2022-03-24T04:25:19.892624Z","iopub.execute_input":"2022-03-24T04:25:19.893419Z","iopub.status.idle":"2022-03-24T04:37:11.124343Z","shell.execute_reply.started":"2022-03-24T04:25:19.893379Z","shell.execute_reply":"2022-03-24T04:37:11.123587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Check color scale of Test images**","metadata":{}},{"cell_type":"code","source":"sampleFrac = 0.1\n#get our sampled images\nisGreyList_test = []\nfor imageName in pred_df['path'].sample(frac=sampleFrac):\n    val = Image.open(imageName).convert('RGB')\n    isGreyList_test.append(is_grey_scale(val))\nprint(np.sum(isGreyList_test) / len(isGreyList_test))\ndel isGreyList_test","metadata":{"execution":{"iopub.status.busy":"2022-03-24T04:37:11.125483Z","iopub.execute_input":"2022-03-24T04:37:11.12683Z","iopub.status.idle":"2022-03-24T04:41:41.592758Z","shell.execute_reply.started":"2022-03-24T04:37:11.126782Z","shell.execute_reply":"2022-03-24T04:41:41.592005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Get mean intensity for each channel RGB**","metadata":{}},{"cell_type":"code","source":"def get_rgb_men(row):\n    img = cv2.imread(row['path'])\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    return np.sum(img[:,:,0]), np.sum(img[:,:,1]), np.sum(img[:,:,2])\n\ntqdm.pandas()\ntrain_df['R'], train_df['G'], train_df['B'] = zip(*train_df.progress_apply(lambda row: get_rgb_men(row), axis=1) )","metadata":{"execution":{"iopub.status.busy":"2022-03-24T04:41:41.594101Z","iopub.execute_input":"2022-03-24T04:41:41.594526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_color_dist(df, count):\n    fig, axr = plt.subplots(count,2,figsize=(15,15))\n    for idx, i in enumerate(np.random.choice(df['path'], count)):\n        img = cv2.imread(i)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        axr[idx,0].imshow(img)\n        axr[idx,0].axis('off')\n        axr[idx,1].set_title('R={:.0f}, G={:.0f}, B={:.0f} '.format(np.mean(img[:,:,0]), np.mean(img[:,:,1]), np.mean(img[:,:,2]))) \n        x, y = np.histogram(img[:,:,0], bins=255)\n        axr[idx,1].bar(y[:-1], x, label='R', alpha=0.8, color='red')\n        x, y = np.histogram(img[:,:,1], bins=255)\n        axr[idx,1].bar(y[:-1], x, label='G', alpha=0.8, color='green')\n        x, y = np.histogram(img[:,:,2], bins=255)\n        axr[idx,1].bar(y[:-1], x, label='B', alpha=0.8, color='blue')\n        axr[idx,1].legend()\n        axr[idx,1].axis('off')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Red images and their color distribution**<br>\nSince we are picking random images, some image may appear multiple times","metadata":{}},{"cell_type":"code","source":"df = train_df[((train_df['B']*1.05) < train_df['R']) & ((train_df['G']*1.05) < train_df['R'])]\nshow_color_dist(df, 8)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Blue images and their color distribution**","metadata":{}},{"cell_type":"code","source":"df = train_df[(train_df['B'] > 1.3*train_df['R']) & (train_df['B'] > 1.3*train_df['G'])]\nshow_color_dist(df, 8)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Green images and their color distribution**","metadata":{}},{"cell_type":"code","source":"df = train_df[(train_df['G'] > 1.05*train_df['R']) & (train_df['G'] > 1.05*train_df['B'])]\nshow_color_dist(df, 8)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Observation Regarding Color Distribution\n1. We see that around 3% of the images in the training set are greyscale. While 1% in the Test set are greyscale.\n2. Some whales have yellow spots and some images are reddish.This can happened due to sunset.\n3. This suggests that we need to create image transformations that are very agnostic to the RGB spectrum (i.e. bump up the number of greyscaled images in the smaller classes).","metadata":{}},{"cell_type":"markdown","source":"# Data for Analysis","metadata":{}},{"cell_type":"code","source":"def image_individual(df,individual_id):\n    species_df = df[df['individual_id'] ==individual_id].reset_index(drop = True)\n    return species_df['path']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_10_ids = train_df.individual_id.value_counts().head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bottom_10_ids = train_df.individual_id.value_counts().tail(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Analyzing Edges","metadata":{}},{"cell_type":"markdown","source":"A Sobel filter is one means of getting a basic edge magnitude/gradient image. Can be useful to threshold and find prominent linear features, etc. Several other similar filters in skimage.filters are also good edge detectors: roberts, scharr, etc. and you can control direction, i.e. use an anisotropic version.","metadata":{}},{"cell_type":"code","source":"for id in bottom_10_ids.index:\n    image = cv2.imread(image_individual(train_df,id)[0])\n    gray=cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    edges = sobel(image)\n    gray_edges=sobel(gray)\n    dimension = edges.shape\n    fig = plt.figure(figsize=(8, 8))\n    plt.suptitle(id)\n    plt.subplot(2,2,1)\n    plt.imshow(gray_edges)\n    plt.subplot(2,2,2)\n    plt.imshow(edges[:dimension[0],:dimension[1],0], cmap=\"gray\")\n    plt.subplot(2,2,3)\n    plt.imshow(edges[:dimension[0],:dimension[1],1], cmap='gray')\n    plt.subplot(2,2,4)\n    plt.imshow(edges[:dimension[0],:dimension[1],2], cmap='gray')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for id in bottom_10_ids.index:\n    image = cv2.imread(image_individual(train_df,id)[0])\n    edges = sobel(image)\n    gray=cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    gray_edges=sobel(gray)\n    dimension = edges.shape\n    fig = plt.figure(figsize=(8, 8))\n    plt.suptitle(id)\n    plt.subplot(2,2,1)\n    plt.imshow(gray_edges)\n    plt.subplot(2,2,2)\n    plt.imshow(edges[:dimension[0],:dimension[1],0], cmap=\"BuGn\")\n    plt.subplot(2,2,3)\n    plt.imshow(edges[:dimension[0],:dimension[1],1], cmap='BuGn')\n    plt.subplot(2,2,4)\n    plt.imshow(edges[:dimension[0],:dimension[1],2], cmap='BuGn')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for id in top_10_ids.index:\n    image = cv2.imread(image_individual(train_df,id)[0])\n    edges = sobel(image)\n    gray=cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    gray_edges=sobel(gray)\n    dimension = edges.shape\n    fig = plt.figure(figsize=(8, 8))\n    plt.suptitle(id)\n    plt.subplot(2,2,1)\n    plt.imshow(gray_edges)\n    plt.imshow(image)\n    plt.subplot(2,2,2)\n    plt.imshow(edges[:dimension[0],:dimension[1],0], cmap=\"gray\")\n    plt.subplot(2,2,3)\n    plt.imshow(edges[:dimension[0],:dimension[1],1], cmap='gray')\n    plt.subplot(2,2,4)\n    plt.imshow(edges[:dimension[0],:dimension[1],2], cmap='gray')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for id in top_10_ids.index:\n    image = cv2.imread(image_individual(train_df,id)[0])\n    edges = sobel(image)\n    gray=cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    gray_edges=sobel(gray)\n    dimension = edges.shape\n    fig = plt.figure(figsize=(8, 8))\n    plt.suptitle(id)\n    plt.subplot(2,2,1)\n    plt.imshow(gray_edges)\n    plt.subplot(2,2,2)\n    plt.imshow(edges[:dimension[0],:dimension[1],0], cmap=\"BuGn\")\n    plt.subplot(2,2,3)\n    plt.imshow(edges[:dimension[0],:dimension[1],1], cmap='BuGn')\n    plt.subplot(2,2,4)\n    plt.imshow(edges[:dimension[0],:dimension[1],2], cmap='BuGn')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Analysis\n\nWe need to train this images on a pre-trained model which has already known the features of fins of dolphins or our head of the model should be a good feature extractor of dolphin fins for the dataset.","metadata":{}},{"cell_type":"markdown","source":"# HSV Transform\nSince this contest is about time series ordering, I think it's possible there may be useful information in a transform to HSV color space. HSV is useful for identifying shadows and illumination, as well as giving us a means to identify similar objects that are distinct by color between scenes (hue), though there's no guarantee the hue will be stable.","metadata":{}},{"cell_type":"code","source":"for id in bottom_10_ids.index:\n    image = cv2.imread(image_individual(train_df,id)[0])\n    hsv = color.rgb2hsv(image)\n    dimension = hsv.shape\n    fig = plt.figure(figsize=(8, 8))\n    plt.suptitle(id)\n    plt.subplot(2,2,1)\n    plt.imshow(image)\n    plt.subplot(2,2,2)\n    plt.imshow(hsv[:dimension[0],:dimension[1],0], cmap=\"PuBuGn\")\n    plt.subplot(2,2,3)\n    plt.imshow(hsv[:dimension[0],:dimension[1],1], cmap='bone')\n    plt.subplot(2,2,4)\n    plt.imshow(hsv[:dimension[0],:dimension[1],2], cmap='bone')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for id in top_10_ids.index:\n    image = cv2.imread(image_individual(train_df,id)[0])\n    hsv = color.rgb2hsv(image)\n    dimension = hsv.shape\n    fig = plt.figure(figsize=(8, 8))\n    plt.suptitle(id)\n    plt.subplot(2,2,1)\n    plt.imshow(image)\n    plt.subplot(2,2,2)\n    plt.imshow(hsv[:dimension[0],:dimension[1],0], cmap=\"PuBuGn\")\n    plt.subplot(2,2,3)\n    plt.imshow(hsv[:dimension[0],:dimension[1],1], cmap='bone')\n    plt.subplot(2,2,4)\n    plt.imshow(hsv[:dimension[0],:dimension[1],2], cmap='bone')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Augmentations","metadata":{}},{"cell_type":"code","source":"def plot_augimages(paths, datagen):\n    plt.figure(figsize = (14,28))\n    plt.suptitle('Augmented Images')\n    \n    midx = 0\n    for path in paths:\n        data = Image.open(path)\n        data = data.resize((224,224))\n        samples = expand_dims(data, 0)\n        it = datagen.flow(samples, batch_size=1)\n    \n        # Show Original Image\n        plt.subplot(10,5, midx+1)\n        plt.imshow(data)\n        plt.axis('off')\n    \n        # Show Augmented Images\n        for idx, i in enumerate(range(4)):\n            midx += 1\n            plt.subplot(10,5, midx+1)\n            \n            batch = it.next()\n            image = batch[0].astype('uint8')\n            plt.imshow(image)\n            plt.axis('off')\n        midx += 1\n    \n    plt.tight_layout()\n    plt.show()\n\n    \ndatagen = ImageDataGenerator(\n    rotation_range=20,\n    zoom_range=0.10,\n    brightness_range=[0.6,1.4],\n    channel_shift_range=0.7,\n    width_shift_range=0.15,\n    height_shift_range=0.15,\n    shear_range=0.15,\n    horizontal_flip=True,\n    fill_mode='nearest'\n) \nplot_augimages(np.random.choice(train_df['path'],10), datagen)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load data in batches using data generators","metadata":{}},{"cell_type":"code","source":"val_datagen = ImageDataGenerator()","metadata":{"execution":{"iopub.status.busy":"2022-03-23T11:29:24.805616Z","iopub.execute_input":"2022-03-23T11:29:24.805893Z","iopub.status.idle":"2022-03-23T11:29:24.81238Z","shell.execute_reply.started":"2022-03-23T11:29:24.805844Z","shell.execute_reply":"2022-03-23T11:29:24.811647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Flow training images in batches of 32 using train_datagen generator\ntrain_generator = datagen.flow_from_dataframe(\n        train_df,  # This is the source directory for training images\n        x_col='path',\n        y_col='individual_id',\n        target_size=(32, 32),  # All images will be resized to 150x150\n        batch_size=32,\n        class_mode=\"categorical\",\n        shuffle=True,\n)","metadata":{"execution":{"iopub.status.busy":"2022-03-23T11:49:32.465474Z","iopub.execute_input":"2022-03-23T11:49:32.465721Z","iopub.status.idle":"2022-03-23T11:50:25.214457Z","shell.execute_reply.started":"2022-03-23T11:49:32.465694Z","shell.execute_reply":"2022-03-23T11:50:25.213702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing\nEncoding Labels","metadata":{}},{"cell_type":"code","source":"y = train_df.iloc[:, 2].values\n\nlabel_encoder = LabelEncoder()\ny = label_encoder.fit_transform(y)\nonehot_encoder = OneHotEncoder(sparse=False)\ny = y.reshape(len(y), 1)\ny = onehot_encoder.fit_transform(y)","metadata":{"execution":{"iopub.status.busy":"2022-03-23T11:54:54.532343Z","iopub.execute_input":"2022-03-23T11:54:54.533098Z","iopub.status.idle":"2022-03-23T11:54:54.800712Z","shell.execute_reply.started":"2022-03-23T11:54:54.533047Z","shell.execute_reply":"2022-03-23T11:54:54.799977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-23T11:54:55.849308Z","iopub.execute_input":"2022-03-23T11:54:55.84956Z","iopub.status.idle":"2022-03-23T11:54:55.858593Z","shell.execute_reply.started":"2022-03-23T11:54:55.849529Z","shell.execute_reply":"2022-03-23T11:54:55.85772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-03-23T11:54:56.897144Z","iopub.execute_input":"2022-03-23T11:54:56.897398Z","iopub.status.idle":"2022-03-23T11:55:06.560022Z","shell.execute_reply.started":"2022-03-23T11:54:56.897369Z","shell.execute_reply":"2022-03-23T11:55:06.559096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # include_top = False means that we doesnt include fully connected top layer we will add them accordingly\n# vgg16 = VGG16(include_top = False, input_shape = (224,224,3), weights = 'imagenet')\n\n# # training of all the convolution is set to false\n# for layer in vgg16.layers:\n#     layer.trainable = False\n\n# x = GlobalAveragePooling2D()(vgg16.output)\n# predictions = Dense(y.shape[1], activation='softmax')(x)\n\n# model = Model(inputs = vgg16.input, outputs = predictions)","metadata":{"execution":{"iopub.status.busy":"2022-03-23T11:55:06.562764Z","iopub.execute_input":"2022-03-23T11:55:06.565417Z","iopub.status.idle":"2022-03-23T11:55:06.686271Z","shell.execute_reply.started":"2022-03-23T11:55:06.565371Z","shell.execute_reply":"2022-03-23T11:55:06.685612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n# model.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# history = model.fit_generator(\n#       train_generator,\n#       steps_per_epoch=100,\n#       epochs=200,\n#       verbose=2)\n# model.save('./last.h5')","metadata":{"execution":{"iopub.status.busy":"2022-03-23T11:55:11.16039Z","iopub.execute_input":"2022-03-23T11:55:11.16064Z","iopub.status.idle":"2022-03-23T11:59:19.282728Z","shell.execute_reply.started":"2022-03-23T11:55:11.160611Z","shell.execute_reply":"2022-03-23T11:59:19.282021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.figure(figsize=(15,5))\n# plt.plot(history.history['accuracy'])\n# plt.title('Model accuracy')\n# plt.ylabel('Accuracy')\n# plt.xlabel('Epoch')\n# plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.figure(figsize=(15,5))\n# plt.plot(history.history['loss'])\n# plt.title('Model loss')\n# plt.ylabel('loss')\n# plt.xlabel('Epoch')\n# plt.show()","metadata":{},"execution_count":null,"outputs":[]}]}