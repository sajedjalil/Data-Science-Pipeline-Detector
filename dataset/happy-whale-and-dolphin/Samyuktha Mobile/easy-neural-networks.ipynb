{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n#/kaggle/input/happy-whale-and-dolphin/sample_submission.csv\n#/kaggle/input/happy-whale-and-dolphin/train.csv\n#/kaggle/input/happy-whale-and-dolphin/train_images/80b5373b87942b.jpg","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-13T19:13:10.848799Z","iopub.execute_input":"2022-03-13T19:13:10.84976Z","iopub.status.idle":"2022-03-13T19:13:10.855473Z","shell.execute_reply.started":"2022-03-13T19:13:10.849633Z","shell.execute_reply":"2022-03-13T19:13:10.85477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the train data\n\ntrain_df = pd.read_csv('../input/happy-whale-and-dolphin/train.csv')\ntrain_df.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-03-13T19:13:10.858617Z","iopub.execute_input":"2022-03-13T19:13:10.859199Z","iopub.status.idle":"2022-03-13T19:13:10.953673Z","shell.execute_reply.started":"2022-03-13T19:13:10.859169Z","shell.execute_reply":"2022-03-13T19:13:10.952848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# There are 51K images in train set\ntrain_df.info()","metadata":{"execution":{"iopub.status.busy":"2022-03-13T19:13:10.955166Z","iopub.execute_input":"2022-03-13T19:13:10.955423Z","iopub.status.idle":"2022-03-13T19:13:10.988399Z","shell.execute_reply.started":"2022-03-13T19:13:10.955388Z","shell.execute_reply":"2022-03-13T19:13:10.987707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sample submission\nsample_sub = pd.read_csv('../input/happy-whale-and-dolphin/sample_submission.csv')\nsample_sub.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-03-13T19:13:10.989632Z","iopub.execute_input":"2022-03-13T19:13:10.990029Z","iopub.status.idle":"2022-03-13T19:13:11.050559Z","shell.execute_reply.started":"2022-03-13T19:13:10.989993Z","shell.execute_reply":"2022-03-13T19:13:11.049916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Size of train set\ntrain_df.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-13T19:13:11.052271Z","iopub.execute_input":"2022-03-13T19:13:11.052472Z","iopub.status.idle":"2022-03-13T19:13:11.057971Z","shell.execute_reply.started":"2022-03-13T19:13:11.052449Z","shell.execute_reply":"2022-03-13T19:13:11.057097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Load some images\n# from tqdm.autonotebook import tqdm\n# from keras.preprocessing import image\n# from keras.applications.imagenet_utils import preprocess_input\n\n# # Loading images \n# # ds = training dataframe dataset , s = size of train_df, d = 'path to train_df'\n\n# def load_images(ds, s , d):\n#     x_train = np.zeros((s, 32, 32, ds.shape[1]))\n#     c = 0\n#     for fig in tqdm(ds['image']):\n#         img = image.load_img(\"../input/happy-whale-and-dolphin/\"+\n#                              d+\"/\"+fig, \n#                              target_size=(32, 32, 3))\n#         x = image.img_to_array(img)\n#         x = preprocess_input(x)\n#         x_train[c] = x\n#         c += 1\n#     return x_train\n# X = load_images(train_df, train_df.shape[0], 'train_images')\n# X /= 255\n                         ","metadata":{"execution":{"iopub.status.busy":"2022-03-13T19:13:11.05943Z","iopub.execute_input":"2022-03-13T19:13:11.05987Z","iopub.status.idle":"2022-03-13T19:13:11.065588Z","shell.execute_reply.started":"2022-03-13T19:13:11.059834Z","shell.execute_reply":"2022-03-13T19:13:11.064871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-03-13T19:13:11.066839Z","iopub.execute_input":"2022-03-13T19:13:11.067239Z","iopub.status.idle":"2022-03-13T19:13:11.083512Z","shell.execute_reply.started":"2022-03-13T19:13:11.067182Z","shell.execute_reply":"2022-03-13T19:13:11.082855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load images in training data\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ntrain_datagen = ImageDataGenerator(rescale = 1./255,\n                                   shear_range = 0.2,\n                                   zoom_range = 0.2,\n                                   height_shift_range = 0.1,\n                                   width_shift_range = 0.1,\n                                validation_split = 0.2,\n                                horizontal_flip = True)\n\nvalid_datagen = ImageDataGenerator(rescale = 1./255,\n                                  validation_split = 0.2)\n\ntrain_dir = '/kaggle/input/happy-whale-and-dolphin/train_images/'\n\ntrain_set = train_datagen.flow_from_dataframe(train_df, train_dir,\n                                              seed = 101,\n                                            target_size = (64,64),\n                                            batch_size = 32,\n                                            x_col='image',\n                                            y_col='individual_id',\n                                            class_mode = 'categorical',\n                                            subset = 'training')\n\nvalid_set = valid_datagen.flow_from_dataframe(train_df, \n                                              train_dir,\n                                                seed = 101,\n                                                target_size = (64,64),\n                                              x_col='image',\n                                                y_col='individual_id',\n                                                batch_size = 32,\n                                                class_mode = 'categorical',\n                                                subset = 'validation')","metadata":{"execution":{"iopub.status.busy":"2022-03-13T19:13:11.101542Z","iopub.execute_input":"2022-03-13T19:13:11.101844Z","iopub.status.idle":"2022-03-13T19:14:15.203925Z","shell.execute_reply.started":"2022-03-13T19:13:11.101816Z","shell.execute_reply":"2022-03-13T19:14:15.20311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# '''\n# Encode individual id's present in training data using \n# one hot encoding and label encoding\n# Writing a function to retrurn labels for individual image id's\n# ie: train_df['individual_id']\n\n# '''\n# from sklearn.preprocessing import LabelEncoder\n# from sklearn.preprocessing import OneHotEncoder\n\n# def labels(l):\n#     values = np.array(l)\n#     le = LabelEncoder()\n#     oe = OneHotEncoder(parse = False)\n#     le_values = le.fit_transform(values)\n#     oe_values = oe.fit_transform(values)\n#     return le_values, oe_values\n\n# y, le = labels(train_df['individual_id'])\n","metadata":{"execution":{"iopub.status.busy":"2022-03-13T19:14:15.20577Z","iopub.execute_input":"2022-03-13T19:14:15.206517Z","iopub.status.idle":"2022-03-13T19:14:15.210849Z","shell.execute_reply.started":"2022-03-13T19:14:15.206476Z","shell.execute_reply":"2022-03-13T19:14:15.210066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ref: https://www.kaggle.com/ammarnassanalhajali/cnn-with-keras-stater","metadata":{"execution":{"iopub.status.busy":"2022-03-13T19:14:15.212349Z","iopub.execute_input":"2022-03-13T19:14:15.212934Z","iopub.status.idle":"2022-03-13T19:14:15.222994Z","shell.execute_reply.started":"2022-03-13T19:14:15.212896Z","shell.execute_reply":"2022-03-13T19:14:15.222227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build the Model","metadata":{}},{"cell_type":"code","source":"from keras.layers import Input, Dense, Activation, BatchNormalization, Flatten, Conv2D\nfrom keras.layers import AveragePooling2D, MaxPooling2D, Dropout\nfrom tensorflow.keras import Sequential\n\nmodel = Sequential()\n\nmodel.add(Conv2D(32, (6,6), strides = (1,1), input_shape =\n                (64,64,3)))\nmodel.add(BatchNormalization(axis=3))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(2,2))\n\nmodel.add(Conv2D(32, (3,3), strides = (1,1)))\nmodel.add(BatchNormalization(axis=3))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(2,2))\nmodel.add(Flatten())\n\nmodel.add(Dense(256, activation = 'relu'))\nmodel.add(Dropout(0.85))\n\nmodel.add(Dense(15587, activation = 'softmax'))\nmodel.compile(loss = 'categorical_crossentropy',\n             optimizer = 'adam',\n             metrics = ['accuracy'])\nmodel.summary()\n","metadata":{"execution":{"iopub.status.busy":"2022-03-13T19:14:15.224488Z","iopub.execute_input":"2022-03-13T19:14:15.225047Z","iopub.status.idle":"2022-03-13T19:14:17.611279Z","shell.execute_reply.started":"2022-03-13T19:14:15.225011Z","shell.execute_reply":"2022-03-13T19:14:17.610564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping\n\ncallbacks = [EarlyStopping(patience = 3,\n                          monitor = 'accuracy',\n                          mode = 'max')]","metadata":{"execution":{"iopub.status.busy":"2022-03-13T19:14:17.613361Z","iopub.execute_input":"2022-03-13T19:14:17.613599Z","iopub.status.idle":"2022-03-13T19:14:17.619688Z","shell.execute_reply.started":"2022-03-13T19:14:17.613566Z","shell.execute_reply":"2022-03-13T19:14:17.618997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hist = model.fit(train_set,\n                 epochs = 10,\n                 validation_data = valid_set,\n                 batch_size = 40000,\n                 verbose = 1,\n                callbacks = callbacks)\n","metadata":{"execution":{"iopub.status.busy":"2022-03-13T19:14:17.621133Z","iopub.execute_input":"2022-03-13T19:14:17.62138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Evaluation","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize = (15,5))\nplt.plot(hist.history['accuracy'])\nplt.title('Model Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nplt.figure(figsize = (15,5))\nplt.plot(hist.history['loss'])\nplt.title('Model Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(hist.history['accuracy'])\nprint(hist.history['val_accuracy'])\nprint(hist.history['loss'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Leave an Upvoteüëç if you Like/Fork my work. It will help me Grow","metadata":{}}]}