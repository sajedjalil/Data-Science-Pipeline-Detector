{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# [Happywhale - Whale and Dolphin Identification](https://www.kaggle.com/c/happy-whale-and-dolphin/overview)\n## Identify whales and dolphins by unique characteristics\n\n<img src=\"https://storage.googleapis.com/kaggle-competitions/kaggle/22962/logos/header.png?t=2021-03-17-22-44-0\">","metadata":{}},{"cell_type":"markdown","source":"If you like this work, please upvote!\n\nPlease also this!\n\n[Happywhale - ðŸ‹ & ðŸ¬ ID [EDA] | Kaggle](https://www.kaggle.com/tomato0813/happywhale-identification-eda)\n\n# References for this notebook:\n\n## Code:\n[1] [HappyWhale ArcFace Baseline (TPU) | Kaggle](https://www.kaggle.com/ks2019/happywhale-arcface-baseline-tpu/notebook)\n\n[2] [[GLRec] ResNet50 ArcFace (TF2.2) | Kaggle](https://www.kaggle.com/akensert/glrec-resnet50-arcface-tf2-2)\n\n[3] [Explanation of MAP5 scoring metric | Kaggle](https://www.kaggle.com/pestipeti/explanation-of-map5-scoring-metric)\n\n## Discussions:\nNone\n\n## Others:\n[Load and preprocess images &nbsp;|&nbsp; TensorFlow Core](https://www.tensorflow.org/tutorials/load_data/images)\n\n[Transfer learning and fine-tuning &nbsp;|&nbsp; TensorFlow Core](https://www.tensorflow.org/tutorials/images/transfer_learning)\n\n[Data augmentation &nbsp;|&nbsp; TensorFlow Core](https://www.tensorflow.org/tutorials/images/data_augmentation)\n\n[arcface-pytorch/metrics.py at master Â· ronghuaiyang/arcface-pytorch](https://github.com/ronghuaiyang/arcface-pytorch)\n\n[TPUs in Colab](https://colab.research.google.com/notebooks/tpu.ipynb#scrollTo=ovFDeMgtjqW4)\n\nPlease Upvote these work too!\n\nThanks guys!","metadata":{"_kg_hide-input":true}},{"cell_type":"markdown","source":"# Setup","metadata":{}},{"cell_type":"code","source":"!pip install -q efficientnet","metadata":{"execution":{"iopub.status.busy":"2022-02-07T13:30:24.475575Z","iopub.execute_input":"2022-02-07T13:30:24.47592Z","iopub.status.idle":"2022-02-07T13:30:37.78172Z","shell.execute_reply.started":"2022-02-07T13:30:24.475835Z","shell.execute_reply":"2022-02-07T13:30:37.780758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import KFold\n\nimport PIL\nimport PIL.Image\n\nimport tensorflow as tf\nfrom tensorflow.keras.applications import MobileNetV2\nimport efficientnet.tfkeras as efn\n\nfrom kaggle_datasets import KaggleDatasets\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-07T13:30:37.78348Z","iopub.execute_input":"2022-02-07T13:30:37.783977Z","iopub.status.idle":"2022-02-07T13:30:45.556471Z","shell.execute_reply.started":"2022-02-07T13:30:37.783929Z","shell.execute_reply":"2022-02-07T13:30:45.555407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n    print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T13:30:45.557763Z","iopub.execute_input":"2022-02-07T13:30:45.558043Z","iopub.status.idle":"2022-02-07T13:30:45.576057Z","shell.execute_reply.started":"2022-02-07T13:30:45.558012Z","shell.execute_reply":"2022-02-07T13:30:45.575108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SEED = 42\n\nN_TRAIN_DATA = 51033\nN_TEST_DATA = 27956\nN_CLASSES = 15587\nNEW_INDIVIDUAL_CLASSE = 15587\n\n\nGCS_PATH = KaggleDatasets().get_gcs_path('happywhale-tfrecords-5743-v1')\n\nTRAIN_FILES = tf.io.gfile.glob(GCS_PATH + \"/happywhale-train-*.tfrecord\")\nTEST_FILES = tf.io.gfile.glob(GCS_PATH + \"/happywhale-test-*.tfrecord\")\n\n\nTRAIN_SIZE = int(0.8 * len(TRAIN_FILES))\nVALIDATION_SIZE = int(len(TRAIN_FILES) - TRAIN_SIZE)\n\nSHUFFLE_BUFFER_SIZE = 512\nBATCH_SIZE = 32 * strategy.num_replicas_in_sync\nIMAGE_SIZE = 512","metadata":{"execution":{"iopub.status.busy":"2022-02-07T13:30:45.577882Z","iopub.execute_input":"2022-02-07T13:30:45.578374Z","iopub.status.idle":"2022-02-07T13:30:49.000989Z","shell.execute_reply.started":"2022-02-07T13:30:45.578337Z","shell.execute_reply":"2022-02-07T13:30:48.999971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"GCS_PATH","metadata":{"execution":{"iopub.status.busy":"2022-02-07T03:48:24.876008Z","iopub.execute_input":"2022-02-07T03:48:24.876834Z","iopub.status.idle":"2022-02-07T03:48:24.88553Z","shell.execute_reply.started":"2022-02-07T03:48:24.876762Z","shell.execute_reply":"2022-02-07T03:48:24.884694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CSV","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('../input/happy-whale-and-dolphin/train.csv')\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-07T13:30:49.0023Z","iopub.execute_input":"2022-02-07T13:30:49.002563Z","iopub.status.idle":"2022-02-07T13:30:49.132307Z","shell.execute_reply.started":"2022-02-07T13:30:49.002528Z","shell.execute_reply":"2022-02-07T13:30:49.131729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv('../input/happy-whale-and-dolphin/sample_submission.csv')\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-07T13:30:49.133288Z","iopub.execute_input":"2022-02-07T13:30:49.133698Z","iopub.status.idle":"2022-02-07T13:30:49.213154Z","shell.execute_reply.started":"2022-02-07T13:30:49.13363Z","shell.execute_reply":"2022-02-07T13:30:49.212245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"concat_df = pd.concat([train_df['image'], test_df['image']])\nconcat_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-07T03:48:42.727929Z","iopub.execute_input":"2022-02-07T03:48:42.728187Z","iopub.status.idle":"2022-02-07T03:48:42.741346Z","shell.execute_reply.started":"2022-02-07T03:48:42.728158Z","shell.execute_reply":"2022-02-07T03:48:42.740826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_name_to_image_id = dict((image_name, index) for index, image_name in enumerate(concat_df.unique()))\nimage_id_to_image_name = {v: k for k, v in image_name_to_image_id.items()}","metadata":{"execution":{"iopub.status.busy":"2022-02-07T03:48:44.944549Z","iopub.execute_input":"2022-02-07T03:48:44.945021Z","iopub.status.idle":"2022-02-07T03:48:44.9995Z","shell.execute_reply.started":"2022-02-07T03:48:44.944993Z","shell.execute_reply":"2022-02-07T03:48:44.99868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_ids = [image_name_to_image_id[image_name] for image_name in train_df['image']]\ntrain_df['image_id'] = image_ids\ntrain_df.head(20)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T03:48:47.193325Z","iopub.execute_input":"2022-02-07T03:48:47.193729Z","iopub.status.idle":"2022-02-07T03:48:47.22895Z","shell.execute_reply.started":"2022-02-07T03:48:47.193698Z","shell.execute_reply":"2022-02-07T03:48:47.228014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"individual_id_to_label = dict((i_id, index) for index, i_id in enumerate(train_df['individual_id'].unique()))\nindividual_id_to_label['new_individual'] = NEW_INDIVIDUAL_CLASSE\n\nlabel_to_individual_id = {v: k for k, v in individual_id_to_label.items()}","metadata":{"execution":{"iopub.status.busy":"2022-02-07T03:48:49.94718Z","iopub.execute_input":"2022-02-07T03:48:49.947961Z","iopub.status.idle":"2022-02-07T03:48:49.964596Z","shell.execute_reply.started":"2022-02-07T03:48:49.947917Z","shell.execute_reply":"2022-02-07T03:48:49.963161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['label'] = [individual_id_to_label[i_id] for i_id in train_df['individual_id']]\ntrain_df.head(20)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T03:48:52.567028Z","iopub.execute_input":"2022-02-07T03:48:52.56734Z","iopub.status.idle":"2022-02-07T03:48:52.602492Z","shell.execute_reply.started":"2022-02-07T03:48:52.567307Z","shell.execute_reply":"2022-02-07T03:48:52.601516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Images","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create a dataset","metadata":{}},{"cell_type":"code","source":"def decode_image(image_raw):\n    image = tf.image.decode_jpeg(image_raw, channels=3)\n    image = tf.image.resize(image, [IMAGE_SIZE, IMAGE_SIZE])\n    image = tf.cast(image, tf.float32) / 255.0\n    \n    return image\n\n\ndef read_tfrecord(raw_image_dataset):\n    feature_description = {\n        \"image_id\": tf.io.FixedLenFeature([], tf.int64),\n        \"image_raw\": tf.io.FixedLenFeature([], tf.string),\n        \"label\": tf.io.FixedLenFeature([], tf.int64),\n    }\n\n    parsed_image_dataset = tf.io.parse_single_example(raw_image_dataset, feature_description)\n    image_id = tf.cast(parsed_image_dataset['image_id'], tf.int32)\n    image = decode_image(parsed_image_dataset['image_raw'])\n    label = tf.cast(parsed_image_dataset['label'], tf.int32)\n    \n    return image_id, image, label\n\n\ndef load_dataset(filenames):\n    raw_image_dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTOTUNE)\n    image_dataset = raw_image_dataset.map(read_tfrecord, num_parallel_calls=AUTOTUNE) \n    \n    return image_dataset\n\n\ndef check_format(image_id, image, label):\n    return image_id\n\n\ndef arcface_format(image_id, image, label):\n    return {'input/image': image, 'input/label': label}, label\n\n\ndef arcface_evaluation_format(image_id, image, label):\n    return image\n\ndef arcface_test_format(image_id, image, label):\n    return image_id, image\n\n\ndef augment(image):\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_hue(image, 0.01)\n    image = tf.image.random_saturation(image, 0.70, 1.30)\n    image = tf.image.random_contrast(image, 0.80, 1.20)\n    image = tf.image.random_brightness(image, 0.10)\n    \n    return image\n\n\ndef get_check_dataset(filenames):\n    ds = load_dataset(filenames)\n    ds = ds.map(check_format, num_parallel_calls=AUTOTUNE)\n    ds = ds.prefetch(buffer_size=AUTOTUNE)\n    \n    return ds\n\n    \ndef get_training_dataset(filenames):\n    ds = load_dataset(filenames)\n    ds = ds.map(lambda image_id, image, label: (image, label))\n    ds = ds.repeat()\n    ds = ds.shuffle(buffer_size=SHUFFLE_BUFFER_SIZE)\n    ds = ds.batch(BATCH_SIZE)\n    ds = ds.map(lambda image, label: (augment(image), label), num_parallel_calls=AUTOTUNE)\n    ds = ds.map(lambda image, label: ({'input/image': image, 'input/label': label}, label))\n    ds = ds.prefetch(buffer_size=AUTOTUNE)\n    \n    return ds\n    \n    \ndef get_validation_dataset(filenames):\n    ds = load_dataset(filenames)\n    ds = ds.map(arcface_format, num_parallel_calls=AUTOTUNE)\n    ds = ds.batch(BATCH_SIZE)\n    ds = ds.prefetch(buffer_size=AUTOTUNE)\n    \n    return ds\n    \n\ndef get_evaluation_dataset(filenames):\n    ds = load_dataset(filenames)\n    ds = ds.map(arcface_evaluation_format, num_parallel_calls=AUTOTUNE)\n    ds = ds.batch(BATCH_SIZE)\n    ds = ds.prefetch(buffer_size=AUTOTUNE)\n    \n    return ds\n\n\ndef get_test_dataset(filenames):\n    ds = load_dataset(filenames)\n    ds = ds.map(arcface_evaluation_format, num_parallel_calls=AUTOTUNE)\n    ds = ds.batch(BATCH_SIZE)\n    ds = ds.prefetch(buffer_size=AUTOTUNE)\n    \n    return ds","metadata":{"execution":{"iopub.status.busy":"2022-02-07T03:48:58.45929Z","iopub.execute_input":"2022-02-07T03:48:58.45973Z","iopub.status.idle":"2022-02-07T03:48:58.476521Z","shell.execute_reply.started":"2022-02-07T03:48:58.459702Z","shell.execute_reply":"2022-02-07T03:48:58.475178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kf = KFold(n_splits=5, random_state=42, shuffle=True)\n\ntrain_filenames = []\nval_filenames = []\n\nfor train_index, val_index in kf.split(TRAIN_FILES):\n    train_filenames = [TRAIN_FILES[i] for i in train_index]\n    val_filenames = [TRAIN_FILES[i] for i in val_index]","metadata":{"execution":{"iopub.status.busy":"2022-02-07T03:49:03.332251Z","iopub.execute_input":"2022-02-07T03:49:03.332847Z","iopub.status.idle":"2022-02-07T03:49:03.339331Z","shell.execute_reply.started":"2022-02-07T03:49:03.332804Z","shell.execute_reply":"2022-02-07T03:49:03.338717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_filenames","metadata":{"execution":{"iopub.status.busy":"2022-02-07T03:49:05.752638Z","iopub.execute_input":"2022-02-07T03:49:05.752977Z","iopub.status.idle":"2022-02-07T03:49:05.760129Z","shell.execute_reply.started":"2022-02-07T03:49:05.752941Z","shell.execute_reply":"2022-02-07T03:49:05.758804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_filenames","metadata":{"execution":{"iopub.status.busy":"2022-02-07T03:49:08.012907Z","iopub.execute_input":"2022-02-07T03:49:08.013963Z","iopub.status.idle":"2022-02-07T03:49:08.020748Z","shell.execute_reply.started":"2022-02-07T03:49:08.013908Z","shell.execute_reply":"2022-02-07T03:49:08.019996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"check_train_ds = get_check_dataset(train_filenames)\ncheck_val_ds = get_check_dataset(val_filenames)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T03:49:10.791359Z","iopub.execute_input":"2022-02-07T03:49:10.792175Z","iopub.status.idle":"2022-02-07T03:49:11.176205Z","shell.execute_reply.started":"2022-02-07T03:49:10.792135Z","shell.execute_reply":"2022-02-07T03:49:11.174945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrain_image_id = [i.numpy() for i in check_train_ds]","metadata":{"execution":{"iopub.status.busy":"2022-02-07T03:49:16.693568Z","iopub.execute_input":"2022-02-07T03:49:16.693899Z","iopub.status.idle":"2022-02-07T04:05:34.016879Z","shell.execute_reply.started":"2022-02-07T03:49:16.693862Z","shell.execute_reply":"2022-02-07T04:05:34.015197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds_df = train_df.iloc[train_image_id].copy()\ntrain_ds_df.head(20)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T04:05:42.592994Z","iopub.execute_input":"2022-02-07T04:05:42.593341Z","iopub.status.idle":"2022-02-07T04:05:42.631923Z","shell.execute_reply.started":"2022-02-07T04:05:42.593294Z","shell.execute_reply":"2022-02-07T04:05:42.631124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nval_image_id = [i.numpy() for i in check_val_ds]","metadata":{"execution":{"iopub.status.busy":"2022-02-06T18:30:43.404248Z","iopub.execute_input":"2022-02-06T18:30:43.404525Z","iopub.status.idle":"2022-02-06T18:33:16.785927Z","shell.execute_reply.started":"2022-02-06T18:30:43.404483Z","shell.execute_reply":"2022-02-06T18:33:16.784865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_ds_df = train_df.iloc[val_image_id].copy()\nval_ds_df.head(20)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T18:33:16.791683Z","iopub.execute_input":"2022-02-06T18:33:16.792005Z","iopub.status.idle":"2022-02-06T18:33:16.810755Z","shell.execute_reply.started":"2022-02-06T18:33:16.791974Z","shell.execute_reply":"2022-02-06T18:33:16.810016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds_count = len(train_ds_df)\nprint(\"train data count: \", train_ds_count)\n\nval_ds_count = len(val_ds_df)\nprint(\"valdation data count: \", val_ds_count)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T18:33:16.811872Z","iopub.execute_input":"2022-02-06T18:33:16.812112Z","iopub.status.idle":"2022-02-06T18:33:16.817119Z","shell.execute_reply.started":"2022-02-06T18:33:16.812085Z","shell.execute_reply":"2022-02-06T18:33:16.81634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds = get_training_dataset(train_filenames)\nval_ds = get_validation_dataset(val_filenames)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T18:33:16.818277Z","iopub.execute_input":"2022-02-06T18:33:16.818493Z","iopub.status.idle":"2022-02-06T18:33:17.085294Z","shell.execute_reply.started":"2022-02-06T18:33:16.818468Z","shell.execute_reply":"2022-02-06T18:33:17.084428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"row = 10; col = 8;\nrow = min(row,BATCH_SIZE//col)\n\nfor (sample,label) in train_ds:\n    img = sample['input/image']\n    plt.figure(figsize=(25,int(25*row/col)))\n    for j in range(row*col):\n        plt.subplot(row,col,j+1)\n        plt.title(label[j].numpy())\n        plt.axis('off')\n        plt.imshow(img[j,])\n    plt.show()\n    break\nprint(img.shape)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T18:33:17.09473Z","iopub.execute_input":"2022-02-06T18:33:17.09511Z","iopub.status.idle":"2022-02-06T18:33:51.159017Z","shell.execute_reply.started":"2022-02-06T18:33:17.095081Z","shell.execute_reply":"2022-02-06T18:33:51.158163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train a model","metadata":{}},{"cell_type":"code","source":"class ArcMarginProduct(tf.keras.layers.Layer):\n    '''\n    Implements large margin arc distance.\n\n    Reference:\n        https://arxiv.org/pdf/1801.07698.pdf\n        https://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution/\n            blob/master/src/modeling/metric_learning.py\n    '''\n    def __init__(self, n_classes, s=30, m=0.50, easy_margin=False, ls_eps=0.0, **kwargs):\n        super(ArcMarginProduct, self).__init__(**kwargs)\n\n        self.n_classes = n_classes\n        self.s = s\n        self.m = m\n        self.ls_eps = ls_eps\n        self.easy_margin = easy_margin\n        self.cos_m = tf.math.cos(m)\n        self.sin_m = tf.math.sin(m)\n        self.th = tf.math.cos(math.pi - m)\n        self.mm = tf.math.sin(math.pi - m) * m\n\n        \n    def build(self, input_shape):\n        super(ArcMarginProduct, self).build(input_shape[0])\n\n        self.W = self.add_weight(\n            name='W',\n            shape=(int(input_shape[0][-1]), self.n_classes),\n            initializer='glorot_uniform',\n            dtype='float32',\n            trainable=True,\n            regularizer=None)\n\n\n    def call(self, inputs):\n        X, y = inputs\n        y = tf.cast(y, dtype=tf.int32)\n        cosine = tf.matmul(tf.math.l2_normalize(X, axis=1), tf.math.l2_normalize(self.W, axis=0))\n        sine = tf.math.sqrt(1.0 - tf.math.pow(cosine, 2))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        \n        if self.easy_margin:\n            phi = tf.where(cosine > 0, phi, cosine)\n        else:\n            phi = tf.where(cosine > self.th, phi, cosine - self.mm)\n            \n        one_hot = tf.cast(tf.one_hot(y, depth=self.n_classes), dtype=cosine.dtype)\n        \n        if self.ls_eps > 0:\n            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.n_classes\n\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        output *= self.s\n        \n        return output","metadata":{"execution":{"iopub.status.busy":"2022-02-06T18:33:51.160428Z","iopub.execute_input":"2022-02-06T18:33:51.160764Z","iopub.status.idle":"2022-02-06T18:33:51.177525Z","shell.execute_reply.started":"2022-02-06T18:33:51.160713Z","shell.execute_reply":"2022-02-06T18:33:51.176857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_model(input_shape,\n                 n_classes,\n                 dense_units=512,\n                 dropout_rate=0.0,\n                 scale=30,\n                 margin=0.3):\n    \n    \"\"\"\n    preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n    backbone = tf.keras.applications.MobileNetV2(input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3), include_top=False, weights='imagenet')\n\n    preprocess_input = tf.keras.applications.efficientnet.preprocess_input\n    backbone = tf.keras.applications.efficientnet.EfficientNetB5(include_top=False,\n                                                                 weights='imagenet',\n                                                                 input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n                                                                 \n    x = preprocess_input(image)                                                             \n    \"\"\"\n    \n    backbone = efn.EfficientNetB5(include_top=False, weights='noisy-student')\n    \n    backbone.trainable=True\n\n    pooling = tf.keras.layers.GlobalAveragePooling2D(name='head/pooling')\n    dropout = tf.keras.layers.Dropout(dropout_rate, name='head/dropout')\n    dense = tf.keras.layers.Dense(dense_units, name='head/dense')\n\n    margin = ArcMarginProduct(n_classes=n_classes,\n                              s=scale,\n                              m=margin,\n                              name='head/arc_margin',\n                              dtype='float32')\n\n    softmax = tf.keras.layers.Softmax(dtype='float32')\n\n    image = tf.keras.layers.Input(input_shape, name='input/image')\n    label = tf.keras.layers.Input((), name='input/label')\n\n    \n    x = backbone(image)\n    x = pooling(x)\n    x = dropout(x)\n    embed = dense(x)\n    x = margin([embed, label])\n    outputs = softmax(x)\n    \n    model = tf.keras.Model(inputs=[image, label], outputs=outputs)\n    embed_model = tf.keras.Model(inputs=image, outputs=embed)\n    \n    model.compile(optimizer=tf.keras.optimizers.Adam(), \n              loss = [tf.keras.losses.SparseCategoricalCrossentropy()],\n              metrics = [tf.keras.metrics.SparseCategoricalAccuracy(), tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5)])\n\n    return model, embed_model","metadata":{"execution":{"iopub.status.busy":"2022-02-06T18:33:51.178519Z","iopub.execute_input":"2022-02-06T18:33:51.179186Z","iopub.status.idle":"2022-02-06T18:33:51.193772Z","shell.execute_reply.started":"2022-02-06T18:33:51.17915Z","shell.execute_reply":"2022-02-06T18:33:51.193112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope(): # creating the model in the TPUStrategy scope means we will train the model on the TPU\n    model, embed_model = create_model(input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3),\n                                      n_classes=N_CLASSES,\n                                      dense_units=512,\n                                      dropout_rate=0.2,\n                                      scale=30,\n                                      margin=0.3)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T18:33:51.194677Z","iopub.execute_input":"2022-02-06T18:33:51.19526Z","iopub.status.idle":"2022-02-06T18:34:02.245683Z","shell.execute_reply.started":"2022-02-06T18:33:51.195216Z","shell.execute_reply":"2022-02-06T18:34:02.245087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-02-06T18:34:02.247381Z","iopub.execute_input":"2022-02-06T18:34:02.247708Z","iopub.status.idle":"2022-02-06T18:34:02.301155Z","shell.execute_reply.started":"2022-02-06T18:34:02.247661Z","shell.execute_reply":"2022-02-06T18:34:02.300394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ExpDecayScheduler:\n    def __init__(self, warmup_lr_limit=0.001, warmup_epochs=4, lr_decay=0.9):\n        self.warmup_lr_limit = warmup_lr_limit\n        self.warmup_epochs = warmup_epochs\n        self.lr_decay = lr_decay\n        \n        \n    def __call__(self, epoch):\n        epoch = max(epoch, 1)\n        if epoch <= self.warmup_epochs:\n            return self.warmup_lr_limit * epoch / self.warmup_epochs\n        \n        return self.warmup_lr_limit * (self.lr_decay ** (epoch - self.warmup_epochs))\n\n    \nclass CosineDecayScheduler:\n    def __init__(self, max_epochs, warmup_lr_limit=0.001, warmup_epochs=4):\n        self.max_epochs = max_epochs\n        self.warmup_lr_limit = warmup_lr_limit\n        self.warmup_epochs = warmup_epochs\n\n\n    def __call__(self, epoch):\n        epoch = max(epoch, 1)\n        if epoch <= self.warmup_epochs:\n            return self.warmup_lr_limit * epoch / self.warmup_epochs\n        \n        epoch -= 1\n        rad = math.pi * epoch / self.max_epochs\n        weight = (math.cos(rad) + 1.0) / 2\n        return self.warmup_lr_limit * weight\n\n    \nscheduler = ExpDecayScheduler(warmup_lr_limit=0.000005*BATCH_SIZE, warmup_epochs=4, lr_decay=0.9)  \nlr_scheduler_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T18:34:02.302212Z","iopub.execute_input":"2022-02-06T18:34:02.302423Z","iopub.status.idle":"2022-02-06T18:34:02.313132Z","shell.execute_reply.started":"2022-02-06T18:34:02.302398Z","shell.execute_reply":"2022-02-06T18:34:02.312437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_checkpoint = tf.keras.callbacks.ModelCheckpoint(f'model_weights.h5', \n                                             monitor='val_loss', \n                                             verbose=0, \n                                             save_best_only=True,\n                                             save_weights_only=True, \n                                             mode='min', \n                                             save_freq='epoch')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(train_ds,\n          validation_data=val_ds,\n          epochs=1,\n          verbose=1,\n          callbacks=[lr_scheduler_callback, model_checkpoint],\n          steps_per_epoch=(train_ds_count//BATCH_SIZE),\n          validation_steps=(val_ds_count//BATCH_SIZE))","metadata":{"execution":{"iopub.status.busy":"2022-02-06T18:34:02.3141Z","iopub.execute_input":"2022-02-06T18:34:02.314688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load_weights(f'model_weights.h5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"eval_ds_df = val_ds_df.copy()\n\nids_set = set([i_id for i_id in train_ds_df['individual_id'].unique()])\neval_ds_df.loc[~eval_ds_df.individual_id.isin(ids_set),'individual_id'] = 'new_individual'\neval_ds_df.loc[~eval_ds_df.individual_id.isin(ids_set),'label'] = NEW_INDIVIDUAL_CLASSE\neval_ds_df.head(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eval_train_ds = get_evaluation_dataset(train_filenames)\neval_val_ds = get_evaluation_dataset(val_filenames)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_embeddings = embed_model.predict(eval_train_ds, verbose=1)\nval_embeddings = embed_model.predict(eval_val_ds, verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.neighbors import NearestNeighbors\neval_nn = NearestNeighbors(n_neighbors=150, metric='cosine')\neval_nn.fit(train_embeddings)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\neval_distances, eval_indices = eval_nn.kneighbors(val_embeddings, n_neighbors=150, return_distance=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eval_indices_list = eval_indices.tolist()\neval_confidences_list = (1 - eval_distances).tolist()\n\neval_ds_df['prediction_labels'] = None\neval_ds_df['confidence'] = None\n\nfor i, (eval_inds, confs) in enumerate(zip(eval_indices_list, eval_confidences_list)):\n    prediction_labels = []\n    confidence = []\n    \n    for eval_ind, conf in zip(eval_inds, confs):\n        pred = train_ds_df.at[train_ds_df.index[eval_ind], 'label']\n        \n        if not (pred in prediction_labels):\n            prediction_labels.append(pred)\n            confidence.append(conf)\n            \n            eval_ds_df.at[eval_ds_df.index[i], 'prediction_labels'] = prediction_labels\n            eval_ds_df.at[eval_ds_df.index[i], 'confidence'] = confidence\n            \n            if len(prediction_labels) == 5:\n                break","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eval_ds_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def map_per_image(label, predictions):\n    \"\"\"Computes the precision score of one image.\n\n    Parameters\n    ----------\n    label : string\n            The true label of the image\n    predictions : list\n            A list of predicted elements (order does matter, 5 predictions allowed per image)\n\n    Returns\n    -------\n    score : double\n    \"\"\"    \n    try:\n        return 1 / (predictions[:5].index(label) + 1)\n    except ValueError:\n        return 0.0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"thresholds = np.linspace(0, 1.0, 11)\nbest_threshold = 0\nbest_cv = 0\n\n\nfor th in thresholds:\n    temp_val = eval_ds_df.copy()\n    \n    for i, (confs, preds) in enumerate(zip(temp_val['confidence'], temp_val['prediction_labels'])):\n        temp_preds = preds\n        \n        for j, conf in enumerate(confs):\n            if conf <= th:\n                temp_preds.insert(j, NEW_INDIVIDUAL_CLASSE)\n                temp_preds.pop()\n                break\n            \n        temp_val.at[temp_val.index[i], 'prediction_labels'] = temp_preds\n        \n\n    eval_ds_df[f'threshold_{th}'] = None\n    for i, (label, preds) in enumerate(zip(temp_val['label'], temp_val['prediction_labels'])):\n        eval_ds_df.at[temp_val.index[i], f'threshold_{th}'] = map_per_image(label, preds)\n  \n    cv = eval_ds_df[f'threshold_{th}'].mean()\n    print(f\"Threshold: {th}, CV: {cv}\")\n    \n    if cv > best_cv:\n        best_threshold = th\n        best_cv = cv\n\n        \nprint(\"Best threshold\", best_threshold)\nprint(\"Best cv\", best_cv)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eval_ds_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Adjustment: Since Public lb has nearly 10% 'new_individual' (Be Careful for private LB)\nthreshold_df = eval_ds_df[[f'threshold_{th}' for th in thresholds]]\neval_ds_df['is_new_individual'] = eval_ds_df['individual_id']=='new_individual'\nprint(eval_ds_df.is_new_individual.value_counts().to_dict())\nthreshold_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eval_scores = pd.DataFrame(thresholds, columns=['threshold'])\neval_scores['adjusted_cv'] = 0\nadjusted_cv = threshold_df[eval_ds_df['is_new_individual']].mean() * 0.1 + threshold_df[~eval_ds_df['is_new_individual']].mean() * 0.9\neval_scores['adjusted_cv'] = [i for i in adjusted_cv]\nbest_threshold_adjusted = eval_scores['threshold'].iloc[eval_scores['adjusted_cv'].idxmax()]\nprint(\"best_threshold\",best_threshold_adjusted)\neval_scores","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"test_ds_df = pd.read_csv('../input/happy-whale-and-dolphin/sample_submission.csv')\ntest_ds_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"concat_train_embeddings = np.concatenate([train_embeddings, val_embeddings])\nconcat_train_embeddings.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"concat_train_ds_df = pd.concat([train_ds_df, val_ds_df])\nconcat_train_ds_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_nn = NearestNeighbors(n_neighbors=150, metric='cosine')\ntest_nn.fit(concat_train_embeddings)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ncheck_test_ds = get_check_dataset(TEST_FILES)\ntest_image_id = [i.numpy() - N_TRAIN_DATA for i in check_test_ds]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_image_id[:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_ds_df = test_ds_df.reindex(test_image_id)\ntest_ds_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_ds = get_test_dataset(TEST_FILES)\ntest_embeddings = embed_model.predict(test_ds, verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntest_distances, test_indices = test_nn.kneighbors(test_embeddings, n_neighbors=150, return_distance=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_indices_list = test_indices.tolist()\ntest_confidences_list = (1 - test_distances).tolist()\n\ntest_ds_df['prediction_labels'] = None\ntest_ds_df['confidence'] = None\n\nfor i, (test_inds, confs) in enumerate(zip(test_indices_list, test_confidences_list)):\n    prediction_labels = []\n    confidence = []\n    \n    for test_ind, conf in zip(test_inds, confs):\n        pred = concat_train_ds_df.at[concat_train_ds_df.index[test_ind], 'label']\n        \n        if not (pred in prediction_labels):\n            prediction_labels.append(pred)\n            confidence.append(conf)\n            \n            test_ds_df.at[test_ds_df.index[i], 'prediction_labels'] = prediction_labels\n            test_ds_df.at[test_ds_df.index[i], 'confidence'] = confidence\n            \n            if len(prediction_labels) == 5:\n                break","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_ds_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_ds_df = test_ds_df.reindex([i for i in range(N_TEST_DATA)])\ntest_ds_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i, (confs, preds) in enumerate(zip(test_ds_df['confidence'], test_ds_df['prediction_labels'])):\n        for j, conf in enumerate(confs):\n            if conf <= best_threshold_adjusted:\n                preds.insert(j, NEW_INDIVIDUAL_CLASSE)\n                preds.pop()\n                break\n            \n        test_ds_df.at[test_ds_df.index[i], 'prediction_labels'] = preds\n        \ntest_ds_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-02-07T13:33:39.095978Z","iopub.execute_input":"2022-02-07T13:33:39.096903Z","iopub.status.idle":"2022-02-07T13:33:39.100733Z","shell.execute_reply.started":"2022-02-07T13:33:39.096819Z","shell.execute_reply":"2022-02-07T13:33:39.099855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = [None] * N_TEST_DATA\n\nfor i, pred_labels in enumerate(test_ds_df['prediction_labels']):\n    row = []\n    \n    for pred_label in pred_labels:\n        row.append(label_to_individual_id[pred_label])\n        \n    predictions[i] = \" \".join(row)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_ds_df['predictions'] = predictions\n_ = test_ds_df.pop('prediction_labels')\n_ = test_ds_df.pop('confidence')\n\ntest_ds_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_ds_df.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}