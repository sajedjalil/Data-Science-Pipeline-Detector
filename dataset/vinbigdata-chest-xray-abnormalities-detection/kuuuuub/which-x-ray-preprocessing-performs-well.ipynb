{"cells":[{"metadata":{},"cell_type":"markdown","source":"## This notebook is quick save version, so if you want see the results, please see ver 9.\n"},{"metadata":{},"cell_type":"markdown","source":"# References\n\n1. https://www.kaggle.com/trungthanhnguyen0502/eda-vinbigdata-chest-x-ray-abnormalities\n2. https://www.kaggle.com/bhallaakshit/dicom-wrangling-and-enhancement\n3. https://www.kaggle.com/awsaf49/vinbigdata-cxr-ad-yolov5-14-class-train\n4. https://www.kaggle.com/kuuuuub/x-ray-image-enhancement-test/data\n\nThanks for above great works!\n\n"},{"metadata":{},"cell_type":"markdown","source":"# What to do?\n\n1. Introducing another way to enhance chest x-ray image (BCET) \n2. After training the images (Original, Clahe, Bcet, Bcet+Clahe) with Yolo v5 model, inference the result\n3. GT label vs prediction\n4. Compare with mAP score\n\n"},{"metadata":{},"cell_type":"markdown","source":"# Balance Contrast Enhancement Technique(BCET)\n\nPaper : LIU JIAN GUO (1991) Balance contrast enhancement technique and its application in image colour composition, International Journal of Remote Sensing, 12:10, 2133-2151, DOI: 10.1080/01431169108955241\n\nMatlab implemented code : https://www.imageeprocessing.com/2017/11/balance-contrast-enhancement-technique.html\n\nThe above code was modified and used."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport os\nimport shutil\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\nimport cv2\nimport matplotlib.pyplot as plt\nfrom skimage import exposure\nfrom glob import glob\n\nfrom scipy.io import wavfile\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.model_selection import GroupKFold","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"dataset_dir = '../input/vinbigdata-chest-xray-abnormalities-detection'","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def dicom2array(path, voi_lut=True, fix_monochrome=True):\n    dicom = pydicom.read_file(path)\n    # VOI LUT (if available by DICOM device) is used to\n    # transform raw DICOM data to \"human-friendly\" view\n    if voi_lut:\n        data = apply_voi_lut(dicom.pixel_array, dicom)\n    else:\n        data = dicom.pixel_array\n    # depending on this value, X-ray may look inverted - fix that:\n    if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n        data = np.amax(data) - data\n    data = data - np.min(data)\n    data = data / np.max(data)\n    data = (data * 255).astype(np.uint8)\n    return data\n        \n    \ndef plot_img(img, size=(7, 7), is_rgb=True, title=\"\", cmap='gray'):\n    plt.figure(figsize=size)\n    plt.imshow(img, cmap=cmap)\n    plt.suptitle(title)\n    plt.show()\n\n\ndef plot_imgs(imgs, cols=4, size=7, is_rgb=True, title=\"\", cmap='gray', img_size=(1000,1000)):\n    rows = len(imgs)//cols + 1\n    fig = plt.figure(figsize=(cols*size, rows*size))\n    for i, img in enumerate(imgs):\n        if img_size is not None:\n            img = cv2.resize(img, img_size)\n        fig.add_subplot(rows, cols, i+1)\n        plt.imshow(img, cmap=cmap)\n    plt.suptitle(title)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def clahe(image, clipLimit = 2., tileGridSize = (10,10)):\n    clahe = cv2.createCLAHE(\n        clipLimit, \n        tileGridSize\n    )\n    \n    image = clahe.apply(image) \n    #image = tf.expand_dims(image, axis = 2)\n    \n    return image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def bcet(img):\n    Lmin = np.min(img) # MINIMUM OF INPUT IMAGE\n    Lmax = np.max(img) # MAXIMUM OF INPUT IMAGE\n    Lmean = np.mean(img) #MEAN OF INPUT IMAGE\n    LMssum = np.mean(img * img) #MEAN SQUARE SUM OF INPUT IMAGE\n\n    Gmin = 0 #MINIMUM OF OUTPUT IMAGE\n    Gmax = 255 #MAXIMUM OF OUTPUT IMAGE\n    Gmean = 110 #MEAN OF OUTPUT IMAGE\n\n    bnum = Lmax * Lmax *(Gmean-Gmin) - LMssum*(Gmax-Gmin) + Lmin * Lmin *(Gmax-Gmean)\n    bden = 2*(Lmax*(Gmean-Gmin)-Lmean*(Gmax-Gmin)+Lmin*(Gmax-Gmean))\n\n    b = bnum/bden\n\n    a = (Gmax-Gmin)/((Lmax-Lmin)*(Lmax+Lmin-2*b))\n\n    c = Gmin - a*(Lmin-b) * (Lmin-b)\n\n    y = a*(img-b) * (img-b) +c #PARABOLIC FUNCTION\n    y = np.array(y, dtype=np.uint8)\n\n    return y","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Image comparison when seen with the eyes"},{"metadata":{"trusted":true},"cell_type":"code","source":"dicom_paths = glob(f'{dataset_dir}/train/*.dicom')\nimgs = [dicom2array(path) for path in dicom_paths[:4]]\nplot_imgs(imgs)\nplt.show()\nclahe_img = [clahe(img) for img in imgs]\nplot_imgs(clahe_img)\nplt.show()\nbcet_img = [bcet(img) for img in imgs]\nplot_imgs(bcet_img)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1st row : original images \\\n2nd row : clahe images \\\n3rd row : bcet images"},{"metadata":{},"cell_type":"markdown","source":"Clahe highlights the bones and blood vessels \\\nBcet emphasizes the information on the volume of the lungs."},{"metadata":{},"cell_type":"markdown","source":"# Histogram analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"hist_ori = cv2.calcHist(imgs[0],[0],None,[256],[0,256])\nhist_clahe = cv2.calcHist(clahe_img[0],[0],None,[256],[0,256])\nhist_bcet = cv2.calcHist(bcet_img[0],[0],None,[256],[0,256])\n\n\nplt.plot(hist_ori,color='r', label='original'), plt.legend()\nplt.plot(hist_clahe,color='g', label = 'clahe'), plt.legend()\nplt.plot(hist_bcet,color='b', label = 'bcet'), plt.legend()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Bcet emphasizes black area"},{"metadata":{"trusted":true},"cell_type":"code","source":"clahe_img = [clahe(img) for img in imgs]\nplot_imgs(clahe_img)\nplt.show()\nbcet_img = [bcet(img) for img in imgs]\nplot_imgs(bcet_img)\nplt.show()\nbcet_clahe = [clahe(img) for img in bcet_img]\nplot_imgs(bcet_clahe)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1st row : clahe images  \\\n2nd row : bcet images  \\\n3rd row : bcet -> clahe images"},{"metadata":{},"cell_type":"markdown","source":"How about vice versa??"},{"metadata":{"trusted":true},"cell_type":"code","source":"clahe_bcet = [bcet(img) for img in clahe_img]\nplot_imgs(clahe_bcet)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Terrible..."},{"metadata":{},"cell_type":"markdown","source":"# Train and compare performances"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"dim = 512 #1024, 256, 'original'\ntest_dir = f'/kaggle/input/vinbigdata-{dim}-image-dataset/vinbigdata/test'\nweights_dir = '/kaggle/input/vinbigdata-cxr-ad-yolov5-14-class-train/yolov5/runs/train/exp/weights/best.pt'\n\ntrain_df = pd.read_csv(f'../input/vinbigdata-{dim}-image-dataset/vinbigdata/train.csv')\ntrain_df['image_path'] = f'/kaggle/input/vinbigdata-{dim}-image-dataset/vinbigdata/train/'+train_df.image_id+('.png' if dim!='original' else '.jpg')\n\ntrain_df = train_df[train_df.class_id!=14].reset_index(drop = True)\n\n\nfold = 4\ngkf  = GroupKFold(n_splits = 5)\ntrain_df['fold'] = -1\nfor fold, (train_idx, val_idx) in enumerate(gkf.split(train_df, groups = train_df.image_id.tolist())):\n    train_df.loc[val_idx, 'fold'] = fold\nval_df = train_df[train_df['fold']==4]\n#val_df.head()\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"train_files = []\nval_files   = []\nval_files += list(train_df[train_df.fold==fold].image_path.unique())\ntrain_files += list(train_df[train_df.fold!=fold].image_path.unique())\nlen(train_files), len(val_files)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"os.makedirs('/kaggle/working/vinbigdata/labels/train', exist_ok = True)\nos.makedirs('/kaggle/working/vinbigdata/labels/val', exist_ok = True)\nos.makedirs('/kaggle/working/vinbigdata/images/train', exist_ok = True)\nos.makedirs('/kaggle/working/vinbigdata/images/val', exist_ok = True)\nlabel_dir = '/kaggle/input/vinbigdata-yolo-labels-dataset/labels'\nfor file in train_files:\n    shutil.copy(file, '/kaggle/working/vinbigdata/images/train')\n    filename = file.split('/')[-1].split('.')[0]\n    shutil.copy(os.path.join(label_dir, filename+'.txt'), '/kaggle/working/vinbigdata/labels/train')\n    \nfor file in val_files:\n    shutil.copy(file, '/kaggle/working/vinbigdata/images/val')\n    filename = file.split('/')[-1].split('.')[0]\n    shutil.copy(os.path.join(label_dir, filename+'.txt'), '/kaggle/working/vinbigdata/labels/val')\n    \nval_dir = f'/kaggle/working/vinbigdata/images/val'","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"os.makedirs('/kaggle/working/vinbigdata_clahe/images/train', exist_ok = True)\nos.makedirs('/kaggle/working/vinbigdata_clahe/images/val', exist_ok = True)\n\nos.makedirs('/kaggle/working/vinbigdata_bcet/images/train', exist_ok = True)\nos.makedirs('/kaggle/working/vinbigdata_bcet/images/val', exist_ok = True)\n\nos.makedirs('/kaggle/working/vinbigdata_bcet_clahe/images/train', exist_ok = True)\nos.makedirs('/kaggle/working/vinbigdata_bcet_clahe/images/val', exist_ok = True)\n\n\n# clahe\nshutil.copytree('/kaggle/working/vinbigdata/labels/train','/kaggle/working/vinbigdata_clahe/labels/train', )\nshutil.copytree('/kaggle/working/vinbigdata/labels/val','/kaggle/working/vinbigdata_clahe/labels/val')\n\n# bcet\nshutil.copytree('/kaggle/working/vinbigdata/labels/train','/kaggle/working/vinbigdata_bcet/labels/train')\nshutil.copytree('/kaggle/working/vinbigdata/labels/val','/kaggle/working/vinbigdata_bcet/labels/val')\n\n# bcet_clahe\nshutil.copytree('/kaggle/working/vinbigdata/labels/train','/kaggle/working/vinbigdata_bcet_clahe/labels/train')\nshutil.copytree('/kaggle/working/vinbigdata/labels/val','/kaggle/working/vinbigdata_bcet_clahe/labels/val')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# clahe\npath = '/kaggle/working/vinbigdata/images/train'\nsave_path = '/kaggle/working/vinbigdata_clahe/images/train'\nimages = os.listdir(path)\n\nfor image in images:\n    img_path = os.path.join(path, image)\n    ori_img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n    dst_img = clahe(ori_img)\n\n    cv2.imwrite(os.path.join(save_path, image), dst_img)\n    \nprint('clahe done')\n\n# bcet    \npath = '/kaggle/working/vinbigdata/images/train'\nsave_path_bcet = '/kaggle/working/vinbigdata_bcet/images/train'\nimages_bcet = os.listdir(path)\n\nfor image in images_bcet:\n    img_path = os.path.join(path, image)\n    ori_img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n    dst_img = bcet(ori_img)\n\n    cv2.imwrite(os.path.join(save_path_bcet, image), dst_img)\n    \nprint('bcet done')\n\n# bcet -> clahe\npath = '/kaggle/working/vinbigdata_bcet/images/train'\nsave_path_bclahe = '/kaggle/working/vinbigdata_bcet_clahe/images/train'\nimages_bcet_clahe = os.listdir(path)\n\nfor image in images_bcet_clahe:\n    img_path = os.path.join(path, image)\n    ori_img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n    dst_img = clahe(ori_img)\n\n    cv2.imwrite(os.path.join(save_path_bclahe, image), dst_img)\n    \nprint('bcet_clahe done')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check samples"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"print(len(os.listdir(save_path)))\nsample = cv2.imread(os.path.join(save_path, images[0]))\nplt.imshow(sample)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"print(len(os.listdir(save_path_bcet)))\nsample = cv2.imread(os.path.join(save_path_bcet, images[0]))\nplt.imshow(sample)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"print(len(os.listdir(save_path_bclahe)))\nsample = cv2.imread(os.path.join(save_path_bclahe, images[0]))\nplt.imshow(sample)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# clahe\nval_path = '/kaggle/working/vinbigdata/images/val'\nsave_path = '/kaggle/working/vinbigdata_clahe/images/val'\nimages = os.listdir(val_path)\n\nfor image in images:\n    img_path = os.path.join(val_path, image)\n    ori_img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n    dst_img = clahe(ori_img)\n\n    cv2.imwrite(os.path.join(save_path, image), dst_img)\n    \nprint('clahe done')\n\n# bcet    \nval_path = '/kaggle/working/vinbigdata/images/val'\nval_save_path_bcet = '/kaggle/working/vinbigdata_bcet/images/val'\nimages = os.listdir(val_path)\n\nfor image in images:\n    img_path = os.path.join(val_path, image)\n    ori_img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n    dst_img = bcet(ori_img)\n\n    cv2.imwrite(os.path.join(val_save_path_bcet, image), dst_img)\n    \nprint('bcet done')\n\n# bcet -> clahe\nval_path = '/kaggle/working/vinbigdata_bcet/images/val'\nval_save_path_bclahe = '/kaggle/working/vinbigdata_bcet_clahe/images/val'\nimages = os.listdir(val_path)\n\nfor image in images:\n    img_path = os.path.join(val_path, image)\n    ori_img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n    dst_img = clahe(ori_img)\n\n    cv2.imwrite(os.path.join(val_save_path_bclahe, image), dst_img)\n    \nprint('bcet_clahe done')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"print(len(os.listdir(save_path)))\nsample = cv2.imread(os.path.join(save_path, images[0]))\nplt.imshow(sample)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"print(len(os.listdir(val_save_path_bcet)))\nsample = cv2.imread(os.path.join(val_save_path_bcet, images[0]))\nplt.imshow(sample)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"print(len(os.listdir(save_path)))\nsample = cv2.imread(os.path.join(val_save_path_bclahe, images[0]))\nplt.imshow(sample)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Setups for training yolo v5\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"class_ids, class_names = list(zip(*set(zip(train_df.class_id, train_df.class_name))))\nclasses = list(np.array(class_names)[np.argsort(class_ids)])\nclasses = list(map(lambda x: str(x), classes))\nclasses","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# train with original image"},{"metadata":{"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"from os import listdir\nfrom os.path import isfile, join\nimport yaml\n\ncwd = '/kaggle/working/'\n\nwith open(join( cwd , 'train.txt'), 'w') as f:\n    for path in glob('/kaggle/working/vinbigdata/images/train/*'):\n        f.write(path+'\\n')\n            \nwith open(join( cwd , 'val.txt'), 'w') as f:\n    for path in glob('/kaggle/working/vinbigdata/images/val/*'):\n        f.write(path+'\\n')\n\ndata = dict(\n    train =  join( cwd , 'train.txt') ,\n    val   =  join( cwd , 'val.txt' ),\n    nc    = 14,\n    names = classes\n    )\n\nwith open(join( cwd , 'vinbigdata.yaml'), 'w') as outfile:\n    yaml.dump(data, outfile, default_flow_style=False)\n\nf = open(join( cwd , 'vinbigdata.yaml'), 'r')\nprint('\\nyaml:')\nprint(f.read())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shutil.copytree('/kaggle/input/yolov5-official-v31-dataset/yolov5', '/kaggle/working/yolov5')\nos.chdir('/kaggle/working/yolov5') # install dependencies\n\nimport torch\nfrom IPython.display import Image, clear_output  # to display images\n\nclear_output()\nprint('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"!WANDB_MODE=\"dryrun\" python train.py --img 640 --batch 16 --epochs 20 --data /kaggle/working/vinbigdata.yaml --weights yolov5s.pt --cache","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# train with clahe image"},{"metadata":{"trusted":true},"cell_type":"code","source":"from os import listdir\nfrom os.path import isfile, join\nimport yaml\n\ncwd = '/kaggle/working/'\n\nwith open(join( cwd , 'train_clahe.txt'), 'w') as f:\n    for path in glob('/kaggle/working/vinbigdata_clahe/images/train/*'):\n        f.write(path+'\\n')\n            \nwith open(join( cwd , 'val_clahe.txt'), 'w') as f:\n    for path in glob('/kaggle/working/vinbigdata_clahe/images/val/*'):\n        f.write(path+'\\n')\n\ndata = dict(\n    train =  join( cwd , 'train_clahe.txt') ,\n    val   =  join( cwd , 'val_clahe.txt' ),\n    nc    = 14,\n    names = classes\n    )\n\nwith open(join( cwd , 'vinbigdata_clahe.yaml'), 'w') as outfile:\n    yaml.dump(data, outfile, default_flow_style=False)\n\nf = open(join( cwd , 'vinbigdata_clahe.yaml'), 'r')\nprint('\\nyaml:')\nprint(f.read())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shutil.copytree('/kaggle/input/yolov5-official-v31-dataset/yolov5', '/kaggle/working/yolov5_clahe')\nos.chdir('/kaggle/working/yolov5_clahe') # install dependencies\n\nimport torch\nfrom IPython.display import Image, clear_output  # to display images\n\nclear_output()\nprint('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"!WANDB_MODE=\"dryrun\" python train.py --img 640 --batch 16 --epochs 20 --data /kaggle/working/vinbigdata_clahe.yaml --weights yolov5s.pt --cache\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train with bcet images"},{"metadata":{"trusted":true},"cell_type":"code","source":"from os import listdir\nfrom os.path import isfile, join\nimport yaml\n\ncwd = '/kaggle/working/'\n\nwith open(join( cwd , 'train_bcet.txt'), 'w') as f:\n    for path in glob('/kaggle/working/vinbigdata_bcet/images/train/*'):\n        f.write(path+'\\n')\n            \nwith open(join( cwd , 'val_bcet.txt'), 'w') as f:\n    for path in glob('/kaggle/working/vinbigdata_bcet/images/val/*'):\n        f.write(path+'\\n')\n\ndata = dict(\n    train =  join( cwd , 'train_bcet.txt') ,\n    val   =  join( cwd , 'val_bcet.txt' ),\n    nc    = 14,\n    names = classes\n    )\n\nwith open(join( cwd , 'vinbigdata_bcet.yaml'), 'w') as outfile:\n    yaml.dump(data, outfile, default_flow_style=False)\n\nf = open(join( cwd , 'vinbigdata_bcet.yaml'), 'r')\nprint('\\nyaml:')\nprint(f.read())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shutil.copytree('/kaggle/input/yolov5-official-v31-dataset/yolov5', '/kaggle/working/yolov5_bcet')\nos.chdir('/kaggle/working/yolov5_bcet') # install dependencies\n\nimport torch\nfrom IPython.display import Image, clear_output  # to display images\n\nclear_output()\nprint('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"!WANDB_MODE=\"dryrun\" python train.py --img 640 --batch 16 --epochs 20 --data /kaggle/working/vinbigdata_bcet.yaml --weights yolov5s.pt --cache\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train with bcet_clahe images"},{"metadata":{"trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"from os import listdir\nfrom os.path import isfile, join\nimport yaml\n\ncwd = '/kaggle/working/'\n\nwith open(join( cwd , 'train_bcet_clahe.txt'), 'w') as f:\n    for path in glob('/kaggle/working/vinbigdata_bcet_clahe/images/train/*'):\n        f.write(path+'\\n')\n            \nwith open(join( cwd , 'val_bcet_clahe.txt'), 'w') as f:\n    for path in glob('/kaggle/working/vinbigdata_bcet_clahe/images/val/*'):\n        f.write(path+'\\n')\n\ndata = dict(\n    train =  join( cwd , 'train_bcet_clahe.txt') ,\n    val   =  join( cwd , 'val_bcet_clahe.txt' ),\n    nc    = 14,\n    names = classes\n    )\n\nwith open(join( cwd , 'vinbigdata_bcet_clahe.yaml'), 'w') as outfile:\n    yaml.dump(data, outfile, default_flow_style=False)\n\nf = open(join( cwd , 'vinbigdata_bcet_clahe.yaml'), 'r')\nprint('\\nyaml:')\nprint(f.read())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shutil.copytree('/kaggle/input/yolov5-official-v31-dataset/yolov5', '/kaggle/working/yolov5_bcet_clahe')\nos.chdir('/kaggle/working/yolov5_bcet_clahe') # install dependencies\n\nimport torch\nfrom IPython.display import Image, clear_output  # to display images\n\nclear_output()\nprint('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"!WANDB_MODE=\"dryrun\" python train.py --img 640 --batch 16 --epochs 20 --data /kaggle/working/vinbigdata_bcet_clahe.yaml --weights yolov5s.pt --cache","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Inference"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"os.chdir('/kaggle/working/yolov5')\n!python detect.py --weights '/kaggle/working/yolov5/runs/train/exp/weights/best.pt'\\\n--img 640\\\n--conf 0.15\\\n--iou 0.5\\\n--source /kaggle/working/vinbigdata/images/val\\\n--exist-ok","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"os.chdir('/kaggle/working/yolov5_clahe')\n!python detect.py --weights '/kaggle/working/yolov5_clahe/runs/train/exp/weights/best.pt'\\\n--img 640\\\n--conf 0.15\\\n--iou 0.5\\\n--source /kaggle/working/vinbigdata_clahe/images/val\\\n--exist-ok","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"os.chdir('/kaggle/working/yolov5_bcet')\n!python detect.py --weights '/kaggle/working/yolov5_bcet/runs/train/exp/weights/best.pt'\\\n--img 640\\\n--conf 0.15\\\n--iou 0.5\\\n--source /kaggle/working/vinbigdata_bcet/images/val\\\n--exist-ok","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"os.chdir('/kaggle/working/yolov5_bcet_clahe')\n!python detect.py --weights '/kaggle/working/yolov5_bcet_clahe/runs/train/exp/weights/best.pt'\\\n--img 640\\\n--conf 0.15\\\n--iou 0.5\\\n--source /kaggle/working/vinbigdata_bcet_clahe/images/val\\\n--exist-ok","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Inference result (train with original images)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1 import ImageGrid\nimport numpy as np\nimport random\nimport cv2\nfrom glob import glob\nfrom tqdm import tqdm\n\nfiles = glob('/kaggle/working/yolov5/runs/detect/exp/*')\n\nfor _ in range(1):\n    row = 4\n    col = 4\n    grid_files = files[:16]\n    images     = []\n    for image_path in tqdm(grid_files):\n        img          = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n        images.append(img)\n\n    fig = plt.figure(figsize=(col*5, row*5))\n    grid = ImageGrid(fig, 111,  # similar to subplot(111)\n                     nrows_ncols=(col, row),  # creates 2x2 grid of axes\n                     axes_pad=0.05,  # pad between axes in inch.\n                     )\n\n    for ax, im in zip(grid, images):\n        # Iterating over the grid returns the Axes.\n        ax.imshow(im)\n        ax.set_xticks([])\n        ax.set_yticks([])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Inference result (train with clahe images)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1 import ImageGrid\nimport numpy as np\nimport random\nimport cv2\nfrom glob import glob\nfrom tqdm import tqdm\n\nfiles = glob('/kaggle/working/yolov5_clahe/runs/detect/exp/*')\n\nfor _ in range(1):\n    row = 4\n    col = 4\n    grid_files = files[:16]\n    images     = []\n    for image_path in tqdm(grid_files):\n        img          = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n        images.append(img)\n\n    fig = plt.figure(figsize=(col*5, row*5))\n    grid = ImageGrid(fig, 111,  # similar to subplot(111)\n                     nrows_ncols=(col, row),  # creates 2x2 grid of axes\n                     axes_pad=0.05,  # pad between axes in inch.\n                     )\n\n    for ax, im in zip(grid, images):\n        # Iterating over the grid returns the Axes.\n        ax.imshow(im)\n        ax.set_xticks([])\n        ax.set_yticks([])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Inference result (train with bcet images)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1 import ImageGrid\nimport numpy as np\nimport random\nimport cv2\nfrom glob import glob\nfrom tqdm import tqdm\n\nfiles = glob('/kaggle/working/yolov5_bcet/runs/detect/exp/*')\n\nfor _ in range(1):\n    row = 4\n    col = 4\n    grid_files = files[:16]\n    images     = []\n    for image_path in tqdm(grid_files):\n        img          = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n        images.append(img)\n\n    fig = plt.figure(figsize=(col*5, row*5))\n    grid = ImageGrid(fig, 111,  # similar to subplot(111)\n                     nrows_ncols=(col, row),  # creates 2x2 grid of axes\n                     axes_pad=0.05,  # pad between axes in inch.\n                     )\n\n    for ax, im in zip(grid, images):\n        # Iterating over the grid returns the Axes.\n        ax.imshow(im)\n        ax.set_xticks([])\n        ax.set_yticks([])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Inference result (train with bcet_clahe images)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1 import ImageGrid\nimport numpy as np\nimport random\nimport cv2\nfrom glob import glob\nfrom tqdm import tqdm\n\nfiles = glob('/kaggle/working/yolov5_bcet_clahe/runs/detect/exp/*')\n\nfor _ in range(1):\n    row = 4\n    col = 4\n    grid_files = files[:16]\n    images     = []\n    for image_path in tqdm(grid_files):\n        img          = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n        images.append(img)\n\n    fig = plt.figure(figsize=(col*5, row*5))\n    grid = ImageGrid(fig, 111,  # similar to subplot(111)\n                     nrows_ncols=(col, row),  # creates 2x2 grid of axes\n                     axes_pad=0.05,  # pad between axes in inch.\n                     )\n\n    for ax, im in zip(grid, images):\n        # Iterating over the grid returns the Axes.\n        ax.imshow(im)\n        ax.set_xticks([])\n        ax.set_yticks([])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Compare with GT labels"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import random\nfrom random import randint\n\nimgs = []\nfile = '/kaggle/working/yolov5/runs/detect/exp/'\noriginal_file = '/kaggle/working/vinbigdata/images/val/'\nimg_ids = os.listdir(file)\nclass_ids = val_df['class_id'].unique()\n\n# map label_id to specify color\nlabel2color = {class_id:[randint(0,255) for i in range(3)] for class_id in class_ids}\nthickness = 3\nscale = 5\n\n\nfor i in range(16):\n    img_id = img_ids[i][:-4]\n    img_png = img_ids[i]\n    #img_path = f'{dataset_dir}/train/{img_id}.dicom'\n    img = cv2.imread(os.path.join(original_file, img_png))\n    #img = cv2.resize(img, None, fx=1/scale, fy=1/scale)\n    #img = np.stack([img, img, img], axis=-1)\n    \n    boxes = val_df.loc[val_df['image_id'] == img_id, ['x_min', 'y_min', 'x_max', 'y_max']].values/scale\n    labels = val_df.loc[val_df['image_id'] == img_id, ['class_id']].values.squeeze()\n\n    \n    for label_id, box in zip(labels, boxes):\n        color = label2color[label_id]\n        img = cv2.rectangle(\n            img,\n            (int(box[0]), int(box[1])),\n            (int(box[2]), int(box[3])),\n            color, thickness\n    )\n    #img = cv2.resize(img, (500,500))\n    imgs.append(img)\n    \nplot_imgs(imgs, cmap=None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion"},{"metadata":{},"cell_type":"markdown","source":"> mAP compare\n\n\n1. Original\n    * mAP_0.5 0.30146\n    * mAP_0.5:0.95 0.12736\n\n1. Clahe\n    * mAP_0.5 0.29131  \n    * mAP_0.5:0.95 0.12017\n\n2. Bcet\n    * mAP_0.5 0.28734\n    * mAP_0.5:0.95 0.12204\n\n3. Bcet-clahe\n    * mAP_0.5 0.28066\n    * mAP_0.5:0.95 0.1193\n    \n    \ntrain(3515) 20 epoch and validate abnormal images(879) , 512x512, yolov5s"},{"metadata":{},"cell_type":"markdown","source":"### It seems without Clahe performs best for mAP score. \n#### But this experiment use abnormal images only, so I will experiment with normal images.\n### Bcet-clahe is the worst, so multiple preprocessing are not recommended."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}