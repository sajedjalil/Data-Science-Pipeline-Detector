{"cells":[{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://storage.googleapis.com/kaggle-competitions/kaggle/24800/logos/header.png?t=2020-12-17-19-26-15\">\n<center>\n    <h1 style=\"color:red;font-weight:900;font-size:2.5em\">VinBigData Chest X-ray Abnormalities Detection</h1>\n    <h3>Automatically localize and classify thoracic abnormalities from chest radiographs</h3>\n</center>\n<br>\n<br>\n<hr>\n<h2 style=\"color:blue;font-weight:600\"> About Competition </h2>\n<p>\n    Radiologists diagnose and treat medical conditions using imaging techniques like CT and PET scans, MRIs, and, of course, X-rays. Yet, as it happens when working with such a wide variety of medical tools, radiologists face many daily challenges, perhaps the most difficult being the chest radiograph. The interpretation of chest X-rays can lead to medical misdiagnosis, even for the best practicing doctor. Computer-aided detection and diagnosis systems (CADe/CADx) would help reduce the pressure on doctors at metropolitan hospitals and improve diagnostic quality in rural areas.\n</p>\n<p>\n    In this competition we are to predict the thoracic abnormalities in given X-Ray images and also locate those abnormalities. The data provided include:\n    <ul>\n    <li>Train and Test X-Ray images in folders <b style=\"font-weight:700\">Train</b> and <b style=\"font-weight:700\">Test</b>\n    <li> sample submission file in sample_submission.csv\n    <li> train dataframe in train.csv\n    </ul>\n</p>\n<hr>\n<br>\n<a id=\"home\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\" style=\"background: rgb(49,114,163);\nbackground: radial-gradient(circle, rgba(49,114,163,1) 0%, rgba(26,136,181,1) 15%, rgba(1,159,200,1) 52%, rgba(0,212,255,1) 60%, rgba(0,182,224,1) 64%, rgba(0,145,186,1) 69%, rgba(1,66,104,1) 82%, rgba(2,33,70,1) 95%, rgba(24,23,50,1) 100%);\">\n  <h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\"style=\"background: rgb(49,114,163);\nbackground: radial-gradient(circle, rgba(49,114,163,1) 0%, rgba(26,136,181,1) 15%, rgba(1,159,200,1) 52%, rgba(0,212,255,1) 60%, rgba(0,182,224,1) 64%, rgba(0,145,186,1) 69%, rgba(1,66,104,1) 82%, rgba(2,33,70,1) 95%, rgba(24,23,50,1) 100%);\">Table of Contents</h3>\n    <center>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#first\" role=\"tab\" aria-controls=\"profile\">First Look at the Data<span class=\"badge badge-primary badge-pill\">1</span></a>\n    <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#second\" role=\"tab\" aria-controls=\"profile\">EDA<span class=\"badge badge-primary badge-pill\">2</span></a>\n    <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#third\" role=\"tab\" aria-controls=\"profile\">An insight of the Data<span class=\"badge badge-primary badge-pill\">2</span></a>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#fourth\" role=\"tab\" aria-controls=\"messages\">Data Preparation<span class=\"badge badge-primary badge-pill\">3</span></a>\n    <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#fifth\" role=\"tab\" aria-controls=\"messages\">Model Building and training<span class=\"badge badge-primary badge-pill\">4</span></a>\n    </center>\n</div>\n<hr>\n<h1 style=\"color:red\">Note:</h1>\n<h5 style=\"color:red\">The utilities_x_ray module used here is a script that I have written(can be found <a href=\"https://www.kaggle.com/bibhash123/utilities-x-ray\">here</a>). It contains some functions for visualization of the X-Ray images. The dicom image reading pipeline is taken from <a href=\"https://www.kaggle.com/raddar/popular-x-ray-image-normalization-techniques\"> this Notebook</a> by <a href=\"https://www.kaggle.com/raddar\">@raddar</a></h5>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport random\nimport tensorflow as tf\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.backend as K\nimport seaborn as sns\nimport pandas as pd\nimport cv2\nimport os\nimport matplotlib.pyplot as plt\nfrom utilities_x_ray import read_xray,showXray\nfrom tqdm import tqdm\nimport pydicom\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom keras import preprocessing, layers\nfrom keras.models import Sequential\n\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.model_selection import train_test_split\nimport os\nimport cv2\nfrom PIL import Image\nimport pickle\nfrom pathlib import Path\n\nimport imageio\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\n\nimport os","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import Libraries\nimport matplotlib.pyplot as plt\nimport torch\nfrom torch import nn\nfrom torch import optim\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms, models\nfrom torch.autograd import Variable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### To test whether GPU instance is present in the system of not.\nuse_cuda = torch.cuda.is_available()\nprint('Using PyTorch version:', torch.__version__, 'CUDA:', use_cuda)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"display:inline\"> <a id=\"first\"> First Look at the data</a></h1>&emsp;&emsp;&emsp;&emsp;&emsp;<a href=\"#home\" style=\"color:blue\"><img src=\"https://toppng.com/uploads/preview/light-blue-up-arrow-11550117759k4je61afsa.png\" style=\"display:inline;width:2em;height:2em\"></a>"},{"metadata":{},"cell_type":"markdown","source":"## 1. DataFrames"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df_train = pd.read_csv('../input/vinbigdata-chest-xray-abnormalities-detection/train.csv')\nss = pd.read_csv('../input/vinbigdata-chest-xray-abnormalities-detection/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ids = df_train.image_id[:10]\nimgs = []\nlabels = []\nnps = []\nim_label_set = []\nn_images = 100\n\ntrain = os.listdir('../input/vinbigdata-chest-xray-abnormalities-detection/train')\ntest = os.listdir('../input/vinbigdata-chest-xray-abnormalities-detection/test')\n\ntest_img = ('../input/vinbigdata-chest-xray-abnormalities-detection/test/')\ntrain_img = ('../input/vinbigdata-chest-xray-abnormalities-detection/train/')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<ul>\n<li><code>image_id</code> - unique image identifier</li>\n<li><code>class_name</code>&nbsp;- the name of the class of detected object (or \"No finding\")</li>\n<li><code>class_id</code>&nbsp;- the ID of the class of detected object</li>\n<li><code>rad_id</code>&nbsp;- the ID of the radiologist that made the observation</li>\n<li><code>x_min</code>&nbsp;- minimum X coordinate of the object's bounding box</li>\n<li><code>y_min</code>&nbsp;- minimum Y coordinate of the object's bounding box</li>\n<li><code>x_max</code>&nbsp;- maximum X coordinate of the object's bounding box</li>\n<li><code>y_max</code>&nbsp;- maximum Y coordinate of the object's bounding box</li>\n</ul>"},{"metadata":{"trusted":true},"cell_type":"code","source":"ss.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The submission file must contain the image id and the prediction string in the format \"a b (c,d,e,f)\"<br>where\n<ul>\n    <li>a = predicted class ; 14 for no abnormality</li>\n    <li>b= confidence</li>\n    <li>(c,d,e,f) = (x_min,y_min,x_max,y_max)</li>\n</ul>"},{"metadata":{},"cell_type":"markdown","source":"## 2. Images"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,10))\nplt.imshow(read_xray('../input/vinbigdata-chest-xray-abnormalities-detection/train/0108949daa13dc94634a7d650a05c0bb.dicom'),cmap=plt.cm.bone)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"display:inline\"><a id=\"second\">EDA</a></h1>&emsp;&emsp;&emsp;&emsp;&emsp;<a href=\"#home\" style=\"color:blue\"><img src=\"https://toppng.com/uploads/preview/light-blue-up-arrow-11550117759k4je61afsa.png\" style=\"display:inline;width:2em;height:2em\"></a>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print(\"Number of rows in train dataframe: {}\".format(df_train.shape[0]))\nprint(\"Number of Unique images in train set: {}\".format(df_train.image_id.nunique()))\nprint(\"Number of Classes: {}\\n\".format(df_train.class_name.nunique()))\nprint(\"Class Names: {}\".format(list(df_train.class_name.unique())))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print(\"Null Values:\")\ndf_train.isna().sum().to_frame().rename(columns={0:'Null Value count'}).style.background_gradient('viridis')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The number of null values are same as the number of samples that do not have any abnormality"},{"metadata":{},"cell_type":"markdown","source":"### The Distribution of Classes\nWe can see there is a huge class imbalance. The number of negative examples are very high and a few abnormalities have very few examples "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(9,6))\nsns.countplot(df_train[\"class_id\"]);\nplt.title(\"Class Distributions\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Distribution of Radiologists"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(9,6))\nsns.countplot(df_train[\"rad_id\"]);\nplt.title(\"rad_id Distributions\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"display:inline\"><a id=\"third\"> An Intuition of the Data</a></h1>&emsp;&emsp;&emsp;&emsp;&emsp;<a href=\"#home\" style=\"color:blue\"><img src=\"https://toppng.com/uploads/preview/light-blue-up-arrow-11550117759k4je61afsa.png\" style=\"display:inline;width:2em;height:2em\"></a><br><br>\n<h5>Before proceeding further let us try and get an intuition of the data and what exactly we need to do.</h5>\n<h5> In this competition we have been given 15000 images for training. Parallelly we have a dataframe containing the ground truths for various abnormalities. Every sample in the datframe contains:</h5>\n  <ul>\n      <li>the image id</li><li>the id of the radiologist who annoted it</li><li>the name of the corresponding class</li><li>the class id</li><li>the bounding box coordinates</li>\n  </ul>\n<b style=\"font-weight:700\">Important points to be noted here are:</b>\n<ul>\n    <li>Each image may have multiple corresponding abnormalities. Therefore this is a multilabel prediction</li>\n    <li>Bounding boxes for each image have been annoted by multiple radiologists. Therefore for every sample we have multiple ground truths. A naive way to deal with this is to take mean of bounding box coordinates by every radiologists for a particular abnormality</li>\n    <li>There is a significant class imbalance which is likely to affect the performance of models a lot.</li>\n</ul>\n<h4 style=\"font-weight:700\">Information about dicom can be found: <a href=\"https://en.wikipedia.org/wiki/DICOM\" style=\"font-size:1em\">Here</a></h4>\n<h4 style=\"font-weight:700\">Procedure to extract DICOM metadata can be found in: <a href=\"https://www.kaggle.com/mrutyunjaybiswal/vbd-chest-x-ray-abnormalities-detection-eda\" style=\"font-size:1em\">this notebook</a></h4>"},{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"display:inline\"><a id=\"fourth\">Data Preparation</a></h1>&emsp;&emsp;&emsp;&emsp;&emsp;<a href=\"#home\" style=\"color:blue\"><img src=\"https://toppng.com/uploads/preview/light-blue-up-arrow-11550117759k4je61afsa.png\" style=\"display:inline;width:2em;height:2em\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"class_names = sorted(df_train.class_name.unique())\ndel class_names[class_names.index('No finding')]\nclass_names = class_names+['No finding']\nclasses = dict(zip(list(range(15)),class_names))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def prepareDataFrame(train_df= df_train):\n    train_df = train_df.fillna(0)\n    cols = ['image_id','label']+list(range(4*len(class_names[:-1])))\n    return_df = pd.DataFrame(columns=cols)\n    \n    for image in tqdm(train_df.image_id.unique()):\n        df = train_df.query(\"image_id==@image\")\n        label = np.zeros(15)\n        for cls in df.class_id.unique():\n            label[int(cls)]=1\n        bboxes_df = df.groupby('class_id')[['x_min','y_min','x_max','y_max']].mean().round()\n        \n        bboxes_list = [0 for i in range(60)]\n        for ind in list(bboxes_df.index):\n            bboxes_list[4*ind:4*ind+4] = list(bboxes_df.loc[ind,:].values)\n        return_df.loc[len(return_df),:] = [image]+[label]+bboxes_list[:-4]\n    return return_df\ntrain_df = prepareDataFrame()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp=train_df[train_df.image_id=='50a418190bc3fb1ef1633bf9678929b3']\nnp.array([temp.iloc[0,1],temp.iloc[0,2:].values])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### To test whether GPU instance is present in the system of not.\nuse_cuda = torch.cuda.is_available()\nprint('Using PyTorch version:', torch.__version__, 'CUDA:', use_cuda)                ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda\" if use_cuda else \"cpu\")\ndevice","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DataLoader:\n    def __init__(self,path = None,train_df=train_df):\n        self.path = path\n        self.files = os.listdir(self.path)\n        np.random.shuffle(self.files)\n        self.df = train_df\n    \n    def read_image(self):\n        for img in self.files:\n            im_name = img.split('.dicom')[0]\n            image = read_xray(self.path+img)\n            image = cv2.resize(image,(256,256),cv2.INTER_AREA)\n            image = np.expand_dims(image,axis=2)\n            temp = self.df[self.df.image_id==im_name]\n            c_label,bb = temp.iloc[0,1],temp.iloc[0,2:].values.astype('float')\n            yield image,c_label,bb\n    \n    \n    def batch_generator(self,items,batch_size):\n        a=[]\n        i=0\n        for item in items:\n            a.append(item)\n            i+=1\n\n            if i%batch_size==0:\n                yield a\n                a=[]\n        if len(a) is not 0:\n            yield a\n            \n    def flow(self,batch_size):\n        \"\"\"\n        flow from given directory in batches\n        ==========================================\n        batch_size: size of the batch\n        \"\"\"\n        while True:\n            for bat in self.batch_generator(self.read_image(),batch_size):\n                batch_images = []\n                batch_c_labels = []\n                batch_bb = []\n                for im,im_c_label,im_bb in bat:\n                    batch_images.append(im)\n                    batch_c_labels.append(im_c_label)\n                    batch_bb.append(im_bb)\n                batch_images = np.stack(batch_images,axis=0)\n                batch_labels =  (np.stack(batch_c_labels,axis=0),np.stack(batch_bb,axis=0))\n                yield batch_images,batch_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dl = DataLoader('../input/vinbigdata-chest-xray-abnormalities-detection/train/')\ntrain_set = dl.flow(batch_size=32)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"display:inline\"><a id=\"fifth\">Model Building and Training</a></h1>&emsp;&emsp;&emsp;&emsp;&emsp;<a href=\"#home\" style=\"color:blue\"><img src=\"https://toppng.com/uploads/preview/light-blue-up-arrow-11550117759k4je61afsa.png\" style=\"display:inline;width:2em;height:2em\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"for n, id_ in enumerate(ids):\n    dicom_path = train_img + id_ + '.dicom'\n    dicom = pydicom.read_file(dicom_path)\n    data = apply_voi_lut(dicom.pixel_array, dicom)\n    \n    data = data - np.min(data)\n    \n    data = data / np.max(data)\n    \n    data = (data * 255).astype(np.uint8)\n    new_shape = tuple([int(x / 3) for x in data.shape])\n    data = cv2.resize(data, (new_shape[1], new_shape[0]))\n    im = Image.fromarray(data)\n    \n    #imgs.append(im)\n    \n    new_im = im.resize((256,256))\n    npdata = np.asarray(new_im)\n    new = preprocessing.image.img_to_array(npdata)\n    nps.append(new)\n    \n    #im.save('test'+str(n)+'.jpg')\n    \n    df = df_train[df_train.image_id == id_]\n    label = df.iloc[0,2]\n    #label = tf.io.decode_raw(label, tf.uint8)\n    #label = tf.reshape(label, label.shape)\n    #label = tf.one_hot(label, 10)\n    labels.append(label) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# compile data into a keras readable format\n\nlabels = np.stack(labels)\ndata = np.stack(nps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dl = DataLoader('../input/vinbigdata-chest-xray-abnormalities-detection/train/')\ntrain_set = dl.flow(batch_size=32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_cnn():\n    model = tf.keras.models.Sequential()\n    model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu',strides=(2,2), input_shape=(256, 256, 3)))\n    model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n    model.add(tf.keras.layers.Conv2D(64, (3, 3), strides=(2,2),activation='relu'))\n    model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n    model.add(tf.keras.layers.Conv2D(128, (3, 3), strides=(2,2),activation='relu'))\n    model.add(tf.keras.layers.Flatten())\n    model.add(tf.keras.layers.Dense(128, activation='relu'))\n    model.add(tf.keras.layers.Dense(15, activation='softmax'))\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build CNN model\nmodel = build_cnn()\n#Compile the model with optimizer and loss function\nmodel.compile(optimizer='adam',loss='categorical_crossentropy',\nmetrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split data into train & test\n\nX_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=34)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 256\nepochs = 50\nhistory = model.fit(X_train, y_train,\n          batch_size=batch_size,\n          epochs=epochs,\n          verbose=0)\nscore = model.evaluate(X_test, y_test, verbose=0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create ohe of labels\n\ncomp_labels = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14]\n\nmlb = MultiLabelBinarizer()\n\nmlb.fit([labels])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# verify all classes are present\n\nmlb.classes_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# examine X_train shape before feeding model \n\nX_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# view model summary to get an idea of its parameters and steps\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save the CNN model to disk for later use.\nmodel_path = \"models/pneumiacnn\"\nmodel.save(filepath=model_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# evaluate model on the given input data from only 3000 of the training images\n\nmodel.evaluate(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualize model steps \n\ntf.keras.utils.plot_model(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.DataFrame(results, columns=['image_id', 'PredictionString'])\ntest_df.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}