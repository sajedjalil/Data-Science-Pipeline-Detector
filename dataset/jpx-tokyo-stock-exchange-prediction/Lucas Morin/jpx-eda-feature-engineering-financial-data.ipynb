{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Handling Financial Data\n\nThe goal of this notebook is to study and use Financial Data on corporations. \nThe approach make good use of a memory to keep track of last know data.","metadata":{"papermill":{"duration":0.029406,"end_time":"2021-12-27T23:07:23.934056","exception":false,"start_time":"2021-12-27T23:07:23.90465","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Features engineering techniques :\n\n- [Start with the end](#Start) \n- [Get Data](#Get_Data)\n- [Reorder Data](#Reorder_Data)\n- [Missing Values](#Missing_Values)\n- [Base Feature Engineering](#Base_FE)\n- [Market Features](#Market_Features)\n- [Time Features](#Time_Features)\n- [Running Moving Average](#RMA) (<- Magic)\n- [Moving Average Features](#MA_FE)\n- [Betas](#Betas)\n- [Putting it all together](#All) (<- All the features)\n- [Complete Feature Exploration](#FE_exploration)","metadata":{"papermill":{"duration":0.023612,"end_time":"2021-12-27T23:07:23.983008","exception":false,"start_time":"2021-12-27T23:07:23.959396","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import jpx_tokyo_market_prediction\nenv = jpx_tokyo_market_prediction.make_env()\niter_test = env.iter_test()\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom datetime import datetime\nimport pickle\nimport collections\n\ndef timestamp_to_date(timestamp):\n    return(datetime.fromtimestamp(timestamp))\n\nDEBUG = False","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.101102,"end_time":"2021-12-27T23:07:24.107841","exception":false,"start_time":"2021-12-27T23:07:24.006739","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-20T19:54:40.854632Z","iopub.execute_input":"2022-05-20T19:54:40.855476Z","iopub.status.idle":"2022-05-20T19:54:40.906485Z","shell.execute_reply.started":"2022-05-20T19:54:40.855372Z","shell.execute_reply":"2022-05-20T19:54:40.905788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"train_financials<a id='Start'></a>\n# Start with the end\nLooking at iterator submission data.","metadata":{"papermill":{"duration":0.024461,"end_time":"2021-12-27T23:07:24.157731","exception":false,"start_time":"2021-12-27T23:07:24.13327","status":"completed"},"tags":[]}},{"cell_type":"code","source":"(prices, options, financials, trades, secondary_prices, sample_prediction) = next(iter_test)\n\nfinancials","metadata":{"papermill":{"duration":0.0487,"end_time":"2021-12-27T23:07:24.231087","exception":false,"start_time":"2021-12-27T23:07:24.182387","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-20T19:54:41.299285Z","iopub.execute_input":"2022-05-20T19:54:41.299933Z","iopub.status.idle":"2022-05-20T19:54:41.595033Z","shell.execute_reply.started":"2022-05-20T19:54:41.299888Z","shell.execute_reply":"2022-05-20T19:54:41.593984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Only very small data. We just get the update. We will need an oline FE framework for this. ","metadata":{}},{"cell_type":"markdown","source":"<a id='Get_Data'></a>\n# Get Data","metadata":{"papermill":{"duration":0.024924,"end_time":"2021-12-27T23:07:24.281078","exception":false,"start_time":"2021-12-27T23:07:24.256154","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"We start with a pandas framework for EDA. ","metadata":{}},{"cell_type":"code","source":"# get raw data\ntrain = pd.read_csv('../input/jpx-tokyo-stock-exchange-prediction/train_files/stock_prices.csv')[['Date','SecuritiesCode','Volume','Close','Target','ExpectedDividend']]\ntrain_financials = pd.read_csv('../input/jpx-tokyo-stock-exchange-prediction/train_files/financials.csv')\n\nif DEBUG:\n    train_financials = train_financials[train_financials.Date.isin(train_financials.Date.unique()[-300:])]\n\n\ntrain_financials_sup = pd.read_csv('../input/jpx-tokyo-stock-exchange-prediction/supplemental_files/financials.csv')\n\n#transform to float\nbool_cols = ['ApplyingOfSpecificAccountingOfTheQuarterlyFinancialStatements',\n       'MaterialChangesInSubsidiaries',\n       'ChangesBasedOnRevisionsOfAccountingStandard',\n       'ChangesOtherThanOnesBasedOnRevisionsOfAccountingStandard',\n       'ChangesInAccountingEstimates', 'RetrospectiveRestatement']\n\nfloat_cols = ['NetSales',\n       'OperatingProfit', 'OrdinaryProfit', 'Profit', 'EarningsPerShare',\n       'TotalAssets', 'Equity', 'EquityToAssetRatio', 'BookValuePerShare',\n       'ResultDividendPerShare1stQuarter', 'ResultDividendPerShare2ndQuarter',\n       'ResultDividendPerShare3rdQuarter',\n       'ResultDividendPerShareFiscalYearEnd', 'ResultDividendPerShareAnnual',\n       'ForecastDividendPerShare1stQuarter',\n       'ForecastDividendPerShare2ndQuarter',\n       'ForecastDividendPerShare3rdQuarter',\n       'ForecastDividendPerShareFiscalYearEnd',\n       'ForecastDividendPerShareAnnual', 'ForecastNetSales',\n       'ForecastOperatingProfit', 'ForecastOrdinaryProfit', 'ForecastProfit',\n       'ForecastEarningsPerShare',\n       'NumberOfIssuedAndOutstandingSharesAtTheEndOfFiscalYearIncludingTreasuryStock',\n       'NumberOfTreasuryStockAtTheEndOfFiscalYear', 'AverageNumberOfShares']\n\n\nfeatures_to_lag = ['Date', 'DisclosedDate', 'TypeOfDocument', 'NetSales',\n       'OperatingProfit', 'OrdinaryProfit', 'Profit', 'EarningsPerShare',\n       'TotalAssets', 'Equity', 'EquityToAssetRatio', 'BookValuePerShare',\n       'ResultDividendPerShare1stQuarter', 'ResultDividendPerShare2ndQuarter',\n       'ResultDividendPerShare3rdQuarter',\n       'ResultDividendPerShareFiscalYearEnd', 'ResultDividendPerShareAnnual',\n       'ForecastDividendPerShare1stQuarter',\n       'ForecastDividendPerShare2ndQuarter',\n       'ForecastDividendPerShare3rdQuarter',\n       'ForecastDividendPerShareFiscalYearEnd',\n       'ForecastDividendPerShareAnnual', 'ForecastNetSales',\n       'ForecastOperatingProfit', 'ForecastOrdinaryProfit', 'ForecastProfit',\n       'ForecastEarningsPerShare',\n       'ApplyingOfSpecificAccountingOfTheQuarterlyFinancialStatements',\n       'MaterialChangesInSubsidiaries',\n       'ChangesBasedOnRevisionsOfAccountingStandard',\n       'ChangesOtherThanOnesBasedOnRevisionsOfAccountingStandard',\n       'ChangesInAccountingEstimates', 'RetrospectiveRestatement',\n       'NumberOfIssuedAndOutstandingSharesAtTheEndOfFiscalYearIncludingTreasuryStock',\n       'NumberOfTreasuryStockAtTheEndOfFiscalYear', 'AverageNumberOfShares']\n\ndtypes_dict = {}\n\nfor c in bool_cols:\n    dtypes_dict[c] = 'bool'\n\nfor c in float_cols:\n    dtypes_dict[c] = 'float32'\n\ndef clean_financials(df):\n    df = df.replace('-',np.nan).replace('ï¼',np.nan)\n    df = df.astype(dtypes_dict)\n    return df\n    \n# clean data\ntrain_financials = clean_financials(train_financials)\ntrain_financials_sup = clean_financials(train_financials_sup)","metadata":{"execution":{"iopub.status.busy":"2022-05-20T20:37:28.515996Z","iopub.execute_input":"2022-05-20T20:37:28.516603Z","iopub.status.idle":"2022-05-20T20:37:35.953706Z","shell.execute_reply.started":"2022-05-20T20:37:28.516549Z","shell.execute_reply":"2022-05-20T20:37:35.952829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature engineering:","metadata":{}},{"cell_type":"code","source":"%%time \n\ndf_result = []\ngrouped_train = train.groupby('SecuritiesCode')\ngrouped_financial = train_financials.groupby('SecuritiesCode')\nForecast_col = ['ForecastDividendPerShareFiscalYearEnd','ForecastDividendPerShareAnnual','ForecastNetSales',\n            'ForecastOperatingProfit','ForecastOrdinaryProfit','ForecastProfit','ForecastEarningsPerShare']\nForecast_surprise_col = [s+'_surprise_rel' for s in Forecast_col]\n\nfor code in train.SecuritiesCode.unique():\n    train_security = grouped_train.get_group(code)\n    financial_security = grouped_financial.get_group(code)\n    financial_security[Forecast_col] = financial_security[Forecast_col]\n    financial_security[Forecast_surprise_col] = financial_security[Forecast_col]/financial_security[Forecast_col].shift()\n    financial_security = financial_security.ffill()\n    train_security = train_security.merge(financial_security, on=['Date','SecuritiesCode'], how='left')\n    train_security = train_security.ffill()\n    train_security['ID_str'] = train_security.DateCode.str[-4:]\n    train_security['Days_Since_Disclosure'] = (pd.to_datetime(train_security['Date']) - pd.to_datetime(train_security['DisclosedDate'])).dt.days\n    df_result.append(train_security)\n    \ndf = pd.concat(df_result)","metadata":{"execution":{"iopub.status.busy":"2022-05-20T19:54:50.599279Z","iopub.execute_input":"2022-05-20T19:54:50.599532Z","iopub.status.idle":"2022-05-20T19:55:43.435585Z","shell.execute_reply.started":"2022-05-20T19:54:50.599504Z","shell.execute_reply":"2022-05-20T19:55:43.434715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# On-line Feature Engineering:\n\n","metadata":{}},{"cell_type":"markdown","source":"We need:\n\n    - data cleaning\n    - memory to keep track of current data \n    - memory to keep track of past data (growth)\n    - Featrue Engineering","metadata":{}},{"cell_type":"code","source":"Forecast_col = ['ForecastDividendPerShareFiscalYearEnd','ForecastDividendPerShareAnnual','ForecastNetSales','ForecastOperatingProfit','ForecastOrdinaryProfit','ForecastProfit','ForecastEarningsPerShare']\nForecast_surprise_col = [s+'_surprise_rel' for s in Forecast_col]\n\nAmounts = ['NetSales', 'OperatingProfit','OrdinaryProfit','Profit', 'TotalAssets', 'Equity']\nBases = ['Profit', 'TotalAssets', 'Equity']\nForecasts = ['ForecastDividend','ForecastNetSales','ForecastOperatingProfit','ForecastOrdinaryProfit','ForecastProfit','ForecastEarnings']\n\nhorizons = [250,60,20,5,1]\nsuffixes = ['_y_1','_q_1','_m_1','_w_1','_1']\n\n# lagger\ndict_lag = {}\nfor h in horizons:\n    dict_lag[h] = collections.deque(maxlen=h)\n\ndf_memory = pd.DataFrame(index=train.SecuritiesCode.unique(), columns = financials.columns).astype(dtypes_dict)\n\ndef update_fiancials(df, update, date):\n    df.Date = date\n    for i in range(len(group)):\n        row = group.iloc[i]\n        if row.SecuritiesCode in(df_memory.index):\n            df.loc[row.SecuritiesCode] = np.where(row.isna(),df.loc[row.SecuritiesCode],row)\n    return df\n\ndef build_base_finanacials_features(df):\n    # Days Since Disclosure\n    df['DaysSinceDisclosure'] = (pd.to_datetime(df['Date']) - pd.to_datetime(df['DisclosedDate'])).dt.days\n    # Base Amount Features\n    df['ForecastDividend'] =  df['AverageNumberOfShares']*df['ForecastDividendPerShareAnnual']\n    df['ForecastEarnings'] =  df['AverageNumberOfShares']*df['ForecastEarningsPerShare']\n    return df\n\ndef build_ratio_finanacials_features(df):\n    # ratios\n    for Am in Amounts:\n        for Base in Bases:\n            if Am!=Base:\n                df['r'+Am+'/'+Base] = df[Am]/df[Base]\n    # forecast ratios:\n    for For in Forecasts:\n        for Base in Bases:\n            df['r'+For+'/'+Base] = df[Am]/df[Base]  \n    return df\n\ndef build_growth_finanacials_features(df):\n    # Growth Features:\n    for Am in Amounts:\n        df[Am+'_YoY_growth'] = df[Am]/df[Am+'_y_1']\n        df[Am+'_QoQ_growth'] = df[Am]/df[Am+'_q_1']\n    return df\n\n\ndef Fundamental_Data_builder(group, df_memory):\n\n    #clean data\n    group = clean_financials(group)\n    \n    #update memory\n    df_memory = update_fiancials(df_memory.copy(), group, date).copy()\n    df_features = df_memory.copy()\n     \n    # build base features\n    df_features = build_base_finanacials_features(df_features)\n    \n    #create lagged features - TODO : lag only interesting Features\n    df_list = [df_features.copy()]\n    \n    for h,s in zip(horizons,suffixes):\n        dict_lag[h].append(df_memory[features_to_lag].copy())\n        df_lag = dict_lag[h][0]\n        df_lag.columns = [c + s for c in features_to_lag]\n        df_list.append(df_lag)\n    df_features_agg = pd.concat(df_list, axis=1)\n\n    # Build ratio / growth features\n    df_features_agg = build_ratio_finanacials_features(df_features_agg)\n    df_features_agg = build_growth_finanacials_features(df_features_agg)\n\n    return df_features_agg, df_memory","metadata":{"execution":{"iopub.status.busy":"2022-05-20T20:38:19.776815Z","iopub.execute_input":"2022-05-20T20:38:19.777141Z","iopub.status.idle":"2022-05-20T20:38:19.83687Z","shell.execute_reply.started":"2022-05-20T20:38:19.777105Z","shell.execute_reply":"2022-05-20T20:38:19.83594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = []\n\nfor date, group in train_financials.groupby('Date'):\n    features_agg, df_memory = Fundamental_Data_builder(group, df_memory.copy())\n    result.append(features_agg.copy())\n    \ndf_output = pd.concat(result).astype(dtypes_dict)\n\nDate_Names = df_output.columns[df_output.columns.str.startswith('Date')]\ndf_output[Date_Names] = df_output[Date_Names].astype(\"str\")\n\ndf_output.to_parquet('train_financials.parquet')\ndf_memory.astype({'Date':str}).to_parquet('memory.parquet')\npickle.dump( dict_lag, open('dict_lag_train.pkl', 'wb'))","metadata":{"execution":{"iopub.status.busy":"2022-05-20T20:50:38.360399Z","iopub.execute_input":"2022-05-20T20:50:38.360715Z","iopub.status.idle":"2022-05-20T20:52:37.954989Z","shell.execute_reply.started":"2022-05-20T20:50:38.360676Z","shell.execute_reply":"2022-05-20T20:52:37.954005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_sup = []\n\nfor date, group in train_financials_sup.groupby('Date'):\n    features_agg, df_memory = Fundamental_Data_builder(group, df_memory.copy())\n    result_sup.append(features_agg.copy())\n    \ndf_output_sup = pd.concat(result_sup).astype(dtypes_dict)\nDate_Names = df_output_sup.columns[df_output_sup.columns.str.startswith('Date')]\ndf_output_sup[Date_Names] = df_output_sup[Date_Names].astype(str)\n\ndf_output_sup.to_parquet('train_financials_sup.parquet')\ndf_memory.astype({'Date':str}).to_parquet('memory_sup.parquet')\npickle.dump(dict_lag, open('dict_lag_train_sup.pkl', 'wb'))","metadata":{"execution":{"iopub.status.busy":"2022-05-20T20:54:42.031319Z","iopub.execute_input":"2022-05-20T20:54:42.031637Z","iopub.status.idle":"2022-05-20T20:54:47.990938Z","shell.execute_reply.started":"2022-05-20T20:54:42.031606Z","shell.execute_reply":"2022-05-20T20:54:47.989096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA:\n\nReturn by quartile depending on the age of the news;\nWe mainly look at two things:\n- Long terms relations between features and target; We get some relationships between size / performance and target; \n- Short term impact of news (good or bad);","metadata":{}},{"cell_type":"code","source":"df_output = df_output.merge(train, on = ['Date','SecuritiesCode'], how='left')\n\nnumeric_features = df_output.columns[(df_output.dtypes=='float32')|(df_output.dtypes=='float64')]\n\ndata_change = df_output[(df_output['DaysSinceDisclosure']==1)][numeric_features]\ndata_change2 = df_output[(df_output['DaysSinceDisclosure']>1)][numeric_features]","metadata":{"execution":{"iopub.status.busy":"2022-05-20T21:02:49.891184Z","iopub.execute_input":"2022-05-20T21:02:49.891497Z","iopub.status.idle":"2022-05-20T21:02:57.84237Z","shell.execute_reply.started":"2022-05-20T21:02:49.891462Z","shell.execute_reply":"2022-05-20T21:02:57.841326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"error_plot = []\n\nfor c in numeric_features:\n    try:\n        print(c)\n        plt.scatter(data_change[c],data_change.Target)\n        plt.show()\n\n        n = 10\n        q = np.arange(n+1)/n\n\n        quant = data_change[c].quantile(q=q).values\n        quant[0] = -np.Inf\n        quant[n] = np.Inf\n\n        data_change['groups'] = pd.cut(data_change[c], quant, duplicates='drop')\n        data_change['avg_Target'] = data_change.groupby('groups')['Target'].transform('mean')\n        plt.plot(data_change.groupby('groups')['Target'].mean())\n\n        data_change2['groups'] = pd.cut(data_change2[c], quant,duplicates='drop')\n        data_change2['avg_Target'] = data_change2.groupby('groups')['Target'].transform('mean')\n        plt.plot(data_change2.groupby('groups')['Target'].mean())\n\n        plt.show()\n        \n    except:\n        error_plot.append(c)\n        \nprint(error_plot)","metadata":{"execution":{"iopub.status.busy":"2022-05-20T21:08:25.757689Z","iopub.execute_input":"2022-05-20T21:08:25.757985Z","iopub.status.idle":"2022-05-20T21:08:26.411249Z","shell.execute_reply.started":"2022-05-20T21:08:25.757936Z","shell.execute_reply":"2022-05-20T21:08:26.410455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}