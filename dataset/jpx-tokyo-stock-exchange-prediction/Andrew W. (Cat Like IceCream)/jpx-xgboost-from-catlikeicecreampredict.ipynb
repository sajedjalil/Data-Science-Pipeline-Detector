{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport xgboost as xgb\nimport jpx_tokyo_market_prediction\nimport warnings; warnings.filterwarnings(\"ignore\")\nfrom sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\nimport gc\nfrom decimal import ROUND_HALF_UP, Decimal\nfrom datetime import datetime, timedelta","metadata":{"execution":{"iopub.status.busy":"2022-06-28T14:57:29.044989Z","iopub.execute_input":"2022-06-28T14:57:29.045339Z","iopub.status.idle":"2022-06-28T14:57:30.510903Z","shell.execute_reply.started":"2022-06-28T14:57:29.045252Z","shell.execute_reply":"2022-06-28T14:57:30.510114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prices = pd.read_csv(\"../input/jpx-tokyo-stock-exchange-prediction/train_files/stock_prices.csv\")\nsprices = pd.read_csv(\"../input/jpx-tokyo-stock-exchange-prediction/train_files/secondary_stock_prices.csv\")\nstock_list = pd.read_csv(\"../input/jpx-tokyo-stock-exchange-prediction/stock_list.csv\")\nfinancials = pd.read_csv(\"../input/jpx-tokyo-stock-exchange-prediction/train_files/financials.csv\")\n#for final\nsupplemental_prices = pd.read_csv(\"../input/jpx-tokyo-stock-exchange-prediction/supplemental_files/stock_prices.csv\")\nsupplemental_sprices = pd.read_csv(\"../input/jpx-tokyo-stock-exchange-prediction/supplemental_files/secondary_stock_prices.csv\")\nsupplemental_financials = pd.read_csv(\"../input/jpx-tokyo-stock-exchange-prediction/supplemental_files/financials.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-06-28T14:57:30.51292Z","iopub.execute_input":"2022-06-28T14:57:30.513876Z","iopub.status.idle":"2022-06-28T14:57:47.628282Z","shell.execute_reply.started":"2022-06-28T14:57:30.513818Z","shell.execute_reply":"2022-06-28T14:57:47.627246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Combine files**","metadata":{}},{"cell_type":"code","source":"prices=prices.append(sprices,ignore_index=True)\n\n# for final\nprices=prices.append(supplemental_prices,ignore_index=True)\nprices=prices.append(supplemental_sprices,ignore_index=True)\nfinancials=financials.append(supplemental_financials,ignore_index=True)\n\ndel supplemental_financials\ndel supplemental_sprices\ndel supplemental_prices\ndel sprices\n\nprices=prices.drop(['RowId','ExpectedDividend'],axis=1)\nprices=prices.dropna()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-06-28T14:57:47.629489Z","iopub.execute_input":"2022-06-28T14:57:47.629722Z","iopub.status.idle":"2022-06-28T14:57:50.846705Z","shell.execute_reply.started":"2022-06-28T14:57:47.629693Z","shell.execute_reply":"2022-06-28T14:57:50.845729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def adjust_price(price):\n    \"\"\"\n    Args:\n        price (pd.DataFrame)  : pd.DataFrame include stock_price\n    Returns:\n        price DataFrame (pd.DataFrame): stock_price with generated AdjustedClose\n    \"\"\"\n    # transform Date column into datetime\n    price.loc[: ,\"Date\"] = pd.to_datetime(price.loc[: ,\"Date\"], format=\"%Y-%m-%d\")\n\n    def generate_adjusted_close(df):\n        \"\"\"\n        Args:\n            df (pd.DataFrame)  : stock_price for a single SecuritiesCode\n        Returns:\n            df (pd.DataFrame): stock_price with AdjustedClose for a single SecuritiesCode\n        \"\"\"\n        # sort data to generate CumulativeAdjustmentFactor\n        df = df.sort_values(\"Date\", ascending=False)\n        # generate CumulativeAdjustmentFactor\n        df.loc[:, \"CumulativeAdjustmentFactor\"] = df[\"AdjustmentFactor\"].cumprod()\n        # generate AdjustedClose\n        df.loc[:, \"AdjustedClose\"] = (\n            df[\"CumulativeAdjustmentFactor\"] * df[\"Close\"]\n        ).map(lambda x: float(\n            Decimal(str(x)).quantize(Decimal('0.1'), rounding=ROUND_HALF_UP)\n        ))\n        # reverse order\n        df = df.sort_values(\"Date\")\n        # to fill AdjustedClose, replace 0 into np.nan\n        df.loc[df[\"AdjustedClose\"] == 0, \"AdjustedClose\"] = np.nan\n        # forward fill AdjustedClose\n        df.loc[:, \"AdjustedClose\"] = df.loc[:, \"AdjustedClose\"].ffill()\n        return df\n\n    # generate AdjustedClose\n    price = price.sort_values([\"SecuritiesCode\", \"Date\"])\n    price = price.groupby(\"SecuritiesCode\").apply(generate_adjusted_close).reset_index(drop=True)\n    price['Date']= price['Date'].dt.strftime(\"%Y-%m-%d\")\n\n#     price.set_index(\"Date\", inplace=True)\n    return price","metadata":{"execution":{"iopub.status.busy":"2022-06-28T14:57:50.848584Z","iopub.execute_input":"2022-06-28T14:57:50.848863Z","iopub.status.idle":"2022-06-28T14:57:50.860996Z","shell.execute_reply.started":"2022-06-28T14:57:50.848833Z","shell.execute_reply":"2022-06-28T14:57:50.859918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**RSI**","metadata":{}},{"cell_type":"code","source":"#rsi\ndef RSI_create(df,period):\n    dfa=pd.DataFrame()\n    def RSI(series,period):\n        delta = series.diff().dropna()\n        u = delta * 0\n        d = u.copy()\n        u[delta > 0] = delta[delta > 0]\n        d[delta < 0] = -delta[delta < 0]\n        u[u.index[period-1]] = np.mean( u[:period] ) #first value is sum of avg gains\n        u = u.drop(u.index[:(period-1)])\n        d[d.index[period-1]] = np.mean( d[:period] ) #first value is sum of avg losses\n        d = d.drop(d.index[:(period-1)])\n        rs = pd.DataFrame.ewm(u, com=period-1, adjust=False).mean() / \\\n             pd.DataFrame.ewm(d, com=period-1, adjust=False).mean()\n        return 100 - 100 / (1 + rs)\n    for code in df['SecuritiesCode'].unique():\n        df_single=df[df['SecuritiesCode']==code][['Date','SecuritiesCode','AdjustedClose']]\n        try: df_single['rsi'] = RSI(df_single['AdjustedClose'],period)#5 19 best\n        except:\n            lst=[]\n            for i in range(len(df_single)):\n                lst.append(np.nan)\n            rsina=pd.Series(lst)\n            df_single['rsi']=rsina\n        df_single.drop(['AdjustedClose'],axis=1,inplace=True)\n        dfa=dfa.append(df_single)\n    return dfa\n\ndef rsi_class(x):\n    ret = \"low\"\n    if x < 50:\n        ret = \"low\"\n    if x > 50:\n        ret = \"med\"\n    if x > 70:\n        ret = \"hi\"\n    return(ret)","metadata":{"execution":{"iopub.status.busy":"2022-06-28T14:57:50.862716Z","iopub.execute_input":"2022-06-28T14:57:50.863214Z","iopub.status.idle":"2022-06-28T14:57:50.881045Z","shell.execute_reply.started":"2022-06-28T14:57:50.863169Z","shell.execute_reply":"2022-06-28T14:57:50.880085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reduce_memory(train_data):\n    start_mem = train_data.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    for col in train_data.columns:\n        col_type = train_data[col].dtype\n\n        if col_type != object and col_type != bool:\n            c_min = train_data[col].min()\n            c_max = train_data[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    train_data[col] = train_data[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    train_data[col] = train_data[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    train_data[col] = train_data[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    train_data[col] = train_data[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    train_data[col] = train_data[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    train_data[col] = train_data[col].astype(np.float32)\n                else:\n                    train_data[col] = train_data[col].astype(np.float64)\n        else:\n#             train_data[col] = train_data[col].astype('bool')\n            pass\n\n    end_mem = train_data.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n\n    return train_data","metadata":{"execution":{"iopub.status.busy":"2022-06-28T14:57:50.882416Z","iopub.execute_input":"2022-06-28T14:57:50.882675Z","iopub.status.idle":"2022-06-28T14:57:50.902309Z","shell.execute_reply.started":"2022-06-28T14:57:50.882644Z","shell.execute_reply":"2022-06-28T14:57:50.901516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_tables(df):\n    closes = pd.pivot_table(df, values = 'AdjustedClose', index = \"Date\", columns = \"SecuritiesCode\")\n    opens = pd.pivot_table(df, values = \"Open\", index = \"Date\", columns = \"SecuritiesCode\")\n    highs = pd.pivot_table(df, values = \"High\", index = \"Date\", columns = \"SecuritiesCode\")\n    lows = pd.pivot_table(df, values = \"Low\", index = \"Date\", columns = \"SecuritiesCode\")\n    volumes = pd.pivot_table(df, values = \"Volume\", index = \"Date\", columns = \"SecuritiesCode\")\n    return closes,opens,highs,lows,volumes\ndef atr(highs,lows,closes,i): #calculate average total cost\n    a = highs - lows\n    b = abs(highs - closes.shift(1))\n    c = abs(lows - closes.shift(1))\n    return pd.melt(pd.DataFrame(np.max([a,b,c], axis = 0), index = a.index, columns = a.columns).rolling(i).mean(),ignore_index=False).reset_index().rename(columns = {\"value\":f\"atr_{i}\"}).dropna()","metadata":{"execution":{"iopub.status.busy":"2022-06-28T14:57:50.903632Z","iopub.execute_input":"2022-06-28T14:57:50.903908Z","iopub.status.idle":"2022-06-28T14:57:50.922544Z","shell.execute_reply.started":"2022-06-28T14:57:50.903854Z","shell.execute_reply":"2022-06-28T14:57:50.921097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def feature_stock_list(df):\n    df=df[['SecuritiesCode','Section/Products','33SectorCode','17SectorCode','NewIndexSeriesSizeCode','IssuedShares','MarketCapitalization']]\n    df[df=='-']=np.nan\n    df['IssuedShares'].fillna(df['IssuedShares'].mean(),inplace=True)\n    df['MarketCapitalization'].fillna(df['MarketCapitalization'].mean(),inplace=True)\n    df.fillna(0,inplace=True)\n    df['Section/ProductsCode'] = LabelEncoder().fit_transform(df['Section/Products'])\n    df.drop('Section/Products',axis=1,inplace=True)\n    df[['33SectorCode','17SectorCode','NewIndexSeriesSizeCode']]=df[['33SectorCode','17SectorCode','NewIndexSeriesSizeCode']].astype(np.int64)\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-06-28T14:57:50.924607Z","iopub.execute_input":"2022-06-28T14:57:50.924855Z","iopub.status.idle":"2022-06-28T14:57:50.942912Z","shell.execute_reply.started":"2022-06-28T14:57:50.924827Z","shell.execute_reply":"2022-06-28T14:57:50.941849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def adjust_financial(df_financial):\n    df_financial['Date'] = pd.to_datetime(df_financial['Date'])\n    # Drop invalid row\n    df_financial = df_financial[~df_financial['DisclosedTime'].isna()]\n    \n    # Extract only consolidated financial reports\n    df_financial = df_financial[df_financial['TypeOfDocument'].str.contains('FinancialStatements_Consolidated')]\n\n    # If disclosure time >= 15:00, we can't use the information on the day\n    df_financial['DisclosedDateTime'] = pd.to_datetime(df_financial['DisclosedUnixTime'], unit = 's')\n    df_financial['DisclosedDateTime'] = df_financial['DisclosedDateTime'].dt.tz_localize('utc').dt.tz_convert('Asia/Tokyo')\n    df_financial['Date'] = df_financial['Date'] + (df_financial['DisclosedDateTime'].dt.hour >= 15) * timedelta(days = 1)\n\n    df_financial['EarningsPerShare'] = pd.to_numeric(df_financial[\"EarningsPerShare\"], errors = 'coerce').fillna(0)\n    df_financial['ForecastEarningsPerShare'] = pd.to_numeric(df_financial[\"ForecastEarningsPerShare\"], errors = 'coerce').fillna(0)\n    df_financial['NetSales'] = pd.to_numeric(df_financial[\"NetSales\"], errors = 'coerce').fillna(0)\n    df_financial['Profit'] = pd.to_numeric(df_financial[\"Profit\"], errors = 'coerce').fillna(0)\n    df_financial['ForecastProfit'] = pd.to_numeric(df_financial[\"ForecastProfit\"], errors = 'coerce').fillna(0)\n    df_financial['EquityToAssetRatio'] = pd.to_numeric(df_financial[\"EquityToAssetRatio\"], errors = 'coerce').fillna(0)\n    df_financial['ForecastDividendPerShareAnnual'] = pd.to_numeric(df_financial[\"ForecastDividendPerShareAnnual\"], errors = 'coerce').fillna(0)\n    adjust_list=['OrdinaryProfit', 'BookValuePerShare','Equity','TotalAssets','OperatingProfit','ForecastOperatingProfit','ForecastOrdinaryProfit','ForecastNetSales']\n    for ob in adjust_list:\n        df_financial[ob] = pd.to_numeric(df_financial[ob], errors = 'coerce').fillna(0)\n    df_financial['Date']= df_financial['Date'].dt.strftime(\"%Y-%m-%d\")\n    return df_financial[['Date', 'SecuritiesCode', 'EarningsPerShare', 'ForecastEarningsPerShare', 'NetSales', 'Profit', 'ForecastProfit', \n                         'EquityToAssetRatio', 'ForecastDividendPerShareAnnual',\n                         #need process,   TotalAssets-equirty= total liability\n                         'OrdinaryProfit', 'BookValuePerShare','Equity','TotalAssets','OperatingProfit',\n                         'ForecastOperatingProfit','ForecastOrdinaryProfit','ForecastNetSales','AverageNumberOfShares','DisclosedDate']]\ndef create_financial_features(df_financial):\n        # Days Since Disclosure\n    features = df_financial\n    features.drop(features[features['AverageNumberOfShares']=='ï¼'].index,inplace=True)\n    features['AverageNumberOfShares']=features['AverageNumberOfShares'].astype(float)\n    features['DaysSinceDisclosure'] = (pd.to_datetime(features['Date']) - pd.to_datetime(features['DisclosedDate'])).dt.days\n    # Base Amount Features\n    features = features.loc[:,~features.columns.duplicated()]\n    features['ForecastDividend'] =  features['AverageNumberOfShares']*features['ForecastDividendPerShareAnnual']\n    features['ForecastEarnings'] =  features['AverageNumberOfShares']*features['ForecastEarningsPerShare']\n    features.drop('DisclosedDate',inplace=True,axis=1)\n    features['eps_feps'] = (features['EarningsPerShare'] / features['ForecastEarningsPerShare']).replace([np.inf, -np.inf], np.nan).fillna(0)\n#     features['pmargin'] = (features['Profit'] / features['NetSales']).replace([np.inf, -np.inf], np.nan).fillna(0)\n    features['total_liability']= (features['TotalAssets']- features['Equity']).replace([np.inf, -np.inf], np.nan).fillna(0)\n    features[\"r_expense1\"] = (features[\"NetSales\"] - features[\"OperatingProfit\"]).replace([np.inf, -np.inf], np.nan).fillna(0)\n    features[\"r_expense2\"] = (features[\"OperatingProfit\"] - features[\"OrdinaryProfit\"]).replace([np.inf, -np.inf], np.nan).fillna(0)\n    features[\"r_expense3\"] = (features[\"OrdinaryProfit\"] - features[\"Profit\"]).replace([np.inf, -np.inf], np.nan).fillna(0)\n    \n    features[\"f_expense1\"] = (features[\"ForecastNetSales\"] - features[\"ForecastOperatingProfit\"]).replace([np.inf, -np.inf], np.nan).fillna(0)\n    features[\"f_expense2\"] = (features[\"ForecastOperatingProfit\"] - features[\"ForecastOrdinaryProfit\"]).replace([np.inf, -np.inf], np.nan).fillna(0)\n    features[\"f_expense3\"] = (features[\"ForecastOrdinaryProfit\"] - features[\"ForecastProfit\"]).replace([np.inf, -np.inf], np.nan).fillna(0)\n    \n    \n    features[\"r_pm1\"]  = (features[\"Profit\"]   / features[\"NetSales\"]).replace([np.inf, -np.inf], np.nan).fillna(0)\n    features[\"r_roe1\"] = (features[\"Profit\"]   / features[\"Equity\"]).replace([np.inf, -np.inf], np.nan).fillna(0)\n    features[\"r_roa1\"] = (features[\"Profit\"]   / features[\"TotalAssets\"]).replace([np.inf, -np.inf], np.nan).fillna(0)\n\n    features[\"f_pm1\"]  = (features[\"ForecastProfit\"] / features[\"ForecastNetSales\"]).replace([np.inf, -np.inf], np.nan).fillna(0)\n    features[\"f_roe1\"] = (features[\"ForecastProfit\"] / features[\"Equity\"]).replace([np.inf, -np.inf], np.nan).fillna(0)\n    features[\"f_roa1\"] = (features[\"ForecastProfit\"] / features[\"TotalAssets\"]).replace([np.inf, -np.inf], np.nan).fillna(0)\n\n    features[\"r_pm2\"]  = (features[\"OrdinaryProfit\"]   / features[\"NetSales\"]).replace([np.inf, -np.inf], np.nan).fillna(0)\n    features[\"r_roe2\"] = (features[\"OrdinaryProfit\"]   / features[\"Equity\"]).replace([np.inf, -np.inf], np.nan).fillna(0)\n    features[\"r_roa2\"] = (features[\"OrdinaryProfit\"]   / features[\"TotalAssets\"]).replace([np.inf, -np.inf], np.nan).fillna(0)\n\n    features[\"f_pm2\"]  = (features[\"ForecastOrdinaryProfit\"] / features[\"ForecastNetSales\"]).replace([np.inf, -np.inf], np.nan).fillna(0)\n    features[\"f_roe2\"] = (features[\"ForecastOrdinaryProfit\"] / features[\"Equity\"]).replace([np.inf, -np.inf], np.nan).fillna(0)\n    features[\"f_roa2\"] = (features[\"ForecastOrdinaryProfit\"] / features[\"TotalAssets\"]).replace([np.inf, -np.inf], np.nan).fillna(0)\n\n    features[\"r_pm3\"]  = (features[\"OperatingProfit\"]   / features[\"NetSales\"]).replace([np.inf, -np.inf], np.nan).fillna(0)\n    features[\"r_roe3\"] = (features[\"OperatingProfit\"]   / features[\"Equity\"]).replace([np.inf, -np.inf], np.nan).fillna(0)\n    features[\"r_roa3\"] = (features[\"OperatingProfit\"]   / features[\"TotalAssets\"]).replace([np.inf, -np.inf], np.nan).fillna(0)\n\n    features[\"f_pm3\"]  = (features[\"ForecastOperatingProfit\"] / features[\"ForecastNetSales\"]).replace([np.inf, -np.inf], np.nan).fillna(0)\n    features[\"f_roe3\"] = (features[\"ForecastOperatingProfit\"] / features[\"Equity\"]).replace([np.inf, -np.inf], np.nan).fillna(0)\n    features[\"f_roa3\"] = (features[\"ForecastOperatingProfit\"] / features[\"TotalAssets\"]).replace([np.inf, -np.inf], np.nan).fillna(0)\n\n    features[\"r_cost1\"] = (((features[\"NetSales\"] - features[\"OperatingProfit\"])/features[\"NetSales\"])).replace([np.inf, -np.inf], np.nan).fillna(0)\n    features[\"r_cost2\"] = (((features[\"OperatingProfit\"] - features[\"OrdinaryProfit\"])/features[\"NetSales\"])).replace([np.inf, -np.inf], np.nan).fillna(0)\n    features[\"r_cost3\"] = (((features[\"OrdinaryProfit\"] - features[\"Profit\"])/features[\"NetSales\"])).replace([np.inf, -np.inf], np.nan).fillna(0)\n\n    features[\"f_cost1\"] = (((features[\"ForecastNetSales\"] - features[\"ForecastOperatingProfit\"])/features[\"ForecastNetSales\"])).replace([np.inf, -np.inf], np.nan).fillna(0)\n    features[\"f_cost2\"] = (((features[\"ForecastOperatingProfit\"] - features[\"ForecastOrdinaryProfit\"])/features[\"ForecastNetSales\"])).replace([np.inf, -np.inf], np.nan).fillna(0)\n    features[\"f_cost3\"] = (((features[\"ForecastOrdinaryProfit\"] - features[\"ForecastProfit\"])/features[\"ForecastNetSales\"])).replace([np.inf, -np.inf], np.nan).fillna(0)\n\n    features[\"r_turn\"] = (features[\"NetSales\"] / features[\"TotalAssets\"]).replace([np.inf, -np.inf], np.nan).fillna(0)\n    features[\"f_turn\"] = (features[\"ForecastNetSales\"] / features[\"TotalAssets\"]).replace([np.inf, -np.inf], np.nan).fillna(0)\n\n    features[\"equity_ratio\"] = (features[\"Equity\"] / features[\"TotalAssets\"]).replace([np.inf, -np.inf], np.nan).fillna(0)\n    feat2 = [\"NetSales\", \"OperatingProfit\", \"OrdinaryProfit\", \"Profit\", \"ForecastNetSales\", \"ForecastOperatingProfit\", \"ForecastOrdinaryProfit\", \"ForecastProfit\",\n             \"r_expense1\", \"r_expense2\", \"r_expense3\", \"f_expense1\", \"f_expense2\", \"f_expense3\",\n             \"TotalAssets\", \"Equity\"]\n    feat3 = [\"r_pm1\", \"r_roe1\", \"r_roa1\", \"f_pm1\", \"f_roe1\", \"f_roa1\", \n                 \"r_pm2\", \"r_roe2\", \"r_roa2\", \"f_pm2\", \"f_roe2\", \"f_roa2\",\n                 \"r_pm3\", \"r_roe3\", \"r_roa3\", \"f_pm3\", \"f_roe3\", \"f_roa3\",\n                 \"r_cost1\", \"r_cost2\", \"r_cost3\", \"f_cost1\", \"f_cost2\", \"f_cost3\",\n                 \"r_turn\", \"f_turn\", \"equity_ratio\"\n                ]\n    d_feat2=[]\n    d_feat3 = []\n\n    features=reduce_memory(features)\n    dfuse=pd.DataFrame()\n\n    \n    for code in features['SecuritiesCode'].unique():\n        \n        dfsingle=features[features['SecuritiesCode']==code]\n        for f in feat2:\n            dfsingle[\"d_\"+f] = (dfsingle[f].diff(1)).replace([np.inf, -np.inf], np.nan).fillna(0)\n            d_feat2.append(\"d_\"+f)\n        for f in feat3:\n            dfsingle[\"d_\"+f] = (dfsingle[f].diff(1)).replace([np.inf, -np.inf], np.nan).fillna(0)\n            d_feat3.append(\"d_\"+f)\n        \n        dfsingle[\"m_sales\"] = dfsingle[\"NetSales\"] - dfsingle[\"ForecastNetSales\"].shift(1)\n        dfsingle[\"m_ope_income\"] = dfsingle[\"OperatingProfit\"] - dfsingle[\"ForecastOperatingProfit\"].shift(1)\n        dfsingle[\"m_ord_income\"] = dfsingle[\"OrdinaryProfit\"] - dfsingle[\"ForecastOrdinaryProfit\"].shift(1)\n        dfsingle[\"m_net_income\"] = dfsingle[\"Profit\"] - dfsingle[\"ForecastProfit\"].shift(1)\n        dfsingle[\"m_expense1\"] = dfsingle[\"r_expense1\"] - dfsingle[\"f_expense1\"].shift(1)\n        dfsingle[\"m_expense2\"] = dfsingle[\"r_expense2\"] - dfsingle[\"f_expense2\"].shift(1)\n        dfsingle[\"m_expense3\"] = dfsingle[\"r_expense3\"] - dfsingle[\"f_expense3\"].shift(1)\n\n        dfsingle[\"m_pm1\"] = dfsingle[\"r_pm1\"] - dfsingle[\"f_pm1\"].shift(1)\n        dfsingle[\"m_pm2\"] = dfsingle[\"r_pm2\"] - dfsingle[\"f_pm2\"].shift(1)\n        dfsingle[\"m_pm3\"] = dfsingle[\"r_pm3\"] - dfsingle[\"f_pm3\"].shift(1)\n        dfsingle[\"m_roe1\"] = dfsingle[\"r_roe1\"] - dfsingle[\"f_roe1\"].shift(1)\n        dfsingle[\"m_roe2\"] = dfsingle[\"r_roe2\"] - dfsingle[\"f_roe2\"].shift(1)\n        dfsingle[\"m_roe3\"] = dfsingle[\"r_roe3\"] - dfsingle[\"f_roe3\"].shift(1)\n        dfsingle[\"m_roa1\"] = dfsingle[\"r_roa1\"] - dfsingle[\"f_roa1\"].shift(1)\n        dfsingle[\"m_roa2\"] = dfsingle[\"r_roa2\"] - dfsingle[\"f_roa2\"].shift(1)\n        dfsingle[\"m_roa3\"] = dfsingle[\"r_roa3\"] - dfsingle[\"f_roa3\"].shift(1)\n        dfsingle[\"m_cost1\"] = dfsingle[\"r_cost1\"] - dfsingle[\"f_cost1\"].shift(1)\n        dfsingle[\"m_cost2\"] = dfsingle[\"r_cost2\"] - dfsingle[\"f_cost2\"].shift(1)\n        dfsingle[\"m_cost3\"] = dfsingle[\"r_cost3\"] - dfsingle[\"f_cost3\"].shift(1)\n        \n    \n        \n        dfuse= dfuse.append(dfsingle)\n    features=dfuse\n    \n    d_feat4 = [\"m_sales\", \"m_ope_income\", \"m_ord_income\", \"m_net_income\", \"m_expense1\", \"m_expense2\", \"m_expense3\",\n                   \"m_pm1\", \"m_pm2\", \"m_pm3\", \"m_roe1\", \"m_roe2\", \"m_roe3\", \"m_roa1\", \"m_roa2\", \"m_roa3\",\n                   \"m_cost1\", \"m_cost2\", \"m_cost3\"]\n    for deats in d_feat4:\n        features[deats]=features[deats].replace([np.inf, -np.inf], np.nan).fillna(0)\n    \n    return features","metadata":{"execution":{"iopub.status.busy":"2022-06-28T14:57:50.946156Z","iopub.execute_input":"2022-06-28T14:57:50.946609Z","iopub.status.idle":"2022-06-28T14:57:51.02194Z","shell.execute_reply.started":"2022-06-28T14:57:50.946563Z","shell.execute_reply":"2022-06-28T14:57:51.021127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def feature_create(df,financials,stock_list):\n    onl=['SecuritiesCode','Date']\n    \n    df= adjust_price(df)\n    df.drop(['Close','CumulativeAdjustmentFactor','AdjustmentFactor'],axis=1,inplace=True)\n    df = df.sort_values(by = [\"SecuritiesCode\",\"Date\"], ascending = True).reset_index(drop = True)\n\n    closes,opens,highs,lows,volumes= create_tables(df)\n    \n    #boarder bonds \n    df['Middle Band']= df.merge(pd.melt(closes.rolling(window=20).mean(),ignore_index=False).reset_index().dropna().rename(columns = {\"value\":f\"Middle Band\"}),on=onl)['Middle Band']\n    df['Upper Band'] = df['Middle Band'] + 1.96*df['AdjustedClose'].rolling(window=20).std()\n    df['Lower Band'] = df['Middle Band'] - 1.96*df['AdjustedClose'].rolling(window=20).std()\n    #time features             year, month, day, day of year\n    df['Year']=pd.to_numeric(df['Date'].str[0:4])\n    df['Month']=pd.to_numeric(df['Date'].str[5:7])\n    df['Day']=pd.to_numeric(df['Date'].str[8:10])\n    df['Date'] = pd.to_datetime(df['Date'])\n    df['Day_Of_Year'] = df['Date'].dt.dayofyear\n    df['Date']= df['Date'].dt.strftime(\"%Y-%m-%d\")\n    #single row features  \n    df['Close_Open_change_rate'] = (df['AdjustedClose'] - df['Open']) / df['AdjustedClose']\n    df['High_Low_change_rate'] = (df['High'] - df['Low']) / df['AdjustedClose']\n    #rsi and cat\n    dfa= RSI_create(df,5)\n    df=df.merge(dfa,how='left',on=['Date','SecuritiesCode'],suffixes=('', 'b')).set_axis(df.index)\n    df['rsicat'] = list(map(rsi_class, df['rsi']))\n    df['rsicat'] = LabelEncoder().fit_transform(df['rsicat'])\n    print('finish rsi!')\n    stock_list= feature_stock_list(stock_list)\n    df=df.merge(stock_list,on='SecuritiesCode',how='left')\n    financials=adjust_financial(financials)\n    financials=create_financial_features(financials).sort_values(['Date','SecuritiesCode'])\n    financials['SecuritiesCode']=financials['SecuritiesCode'].astype(int)\n    df=df.merge(financials,on=['Date','SecuritiesCode'],how='left')\n    \n    \n    for column_name in financials.columns:\n        df[column_name]=df.groupby(\"SecuritiesCode\")[column_name].ffill().reset_index(level=0, drop=True)\n        df[column_name].fillna(0, inplace = True)\n    df=reduce_memory(df)\n    print('finish finanicals!')\n    #volatility/ rolling features\n    period= [1, 5,10,20,30,50,100]\n    for i in period:\n        df[f\"{i}D-EMA\"]= df.merge(pd.melt(closes.ewm(span=i,adjust=False).mean(),ignore_index=False).reset_index().dropna().rename(columns = {\"value\":f\"{i}D-EMA\"}),on=onl)[f\"{i}D-EMA\"]\n        if i!=1:\n            df[\"Volatility_{}_Day\".format(i)] = np.log(df['AdjustedClose']).groupby(df[\"SecuritiesCode\"]).diff().rolling(i).std().reset_index(level=0, drop=True)\n        df[\"Return_{}_Day\".format(i)] = df.groupby(\"SecuritiesCode\")['AdjustedClose'].pct_change(i).reset_index(level=0, drop=True)\n        if i!=1:\n            df[\"vola_{}\".format(i)] = df.groupby(\"SecuritiesCode\")[\"Return_1_Day\"].rolling(i).std().reset_index(level=0, drop=True)\n        df['Volumn_{}_Day'.format(i)] = df.groupby(\"SecuritiesCode\")['Volume'].rolling(i).mean().reset_index(level=0, drop=True)\n        df['Close{}_Day'.format(i)] = df.groupby(\"SecuritiesCode\")['AdjustedClose'].rolling(i).mean().reset_index(level=0, drop=True)\n        df[f'atr_{i}']=df.merge(atr(highs,lows,closes,i),on=['SecuritiesCode','Date'])[f'atr_{i}']\n        \n        a = highs - lows\n        df[f\"atrday_{i}\"]=df.merge(pd.melt(a.rolling(i).mean(),ignore_index=False).reset_index().dropna().rename(columns = {\"value\":f\"atrday_{i}\"}),on=onl)[f\"atrday_{i}\"]\n        quote_volumes = (volumes * (closes + opens) / 2).rolling(i).sum()\n        df[f\"qvol_{i}\"]=df.merge(pd.melt(quote_volumes, ignore_index=False).reset_index().dropna().rename(columns = {\"value\": f\"qvol_{i}\"}),on=onl)[f\"qvol_{i}\"]\n        a = abs(highs - closes.shift(1))\n        df[f'atrgap_{i}']=df.merge(pd.melt(a.rolling(i).mean(),ignore_index=False).reset_index().dropna().rename(columns = {\"value\":f\"atrgap_{i}\"}),on=onl)[f'atrgap_{i}']\n\n        a = abs(lows - closes.shift(1))\n        df[f\"atrlow_{i}\"]=df.merge(pd.melt(a.rolling(i).mean(),ignore_index=False).reset_index().dropna().rename(columns = {\"value\":f\"atrhige_{i}\"}),on=onl)[f\"atrhige_{i}\"]\n        if i!=1:\n            df[f'variation_{i}']=df.merge(pd.melt((closes.diff()/closes.shift(1)).rolling(i).std(),ignore_index=False).reset_index().dropna().rename(columns = {\"value\":f\"variation_{i}\"}),on=onl)[f'variation_{i}']\n        df[f\"HL_{i}\"]=df.merge(pd.melt((highs.rolling(i).max()-lows.rolling(i).min()),ignore_index=False).reset_index().dropna().rename(columns = {\"value\":f\"HL_{i}\"}),on= onl)[f\"HL_{i}\"]\n        df[f\"market_impact_{i}\"]=df.merge(pd.melt((closes.diff()/volumes).rolling(i).mean(),ignore_index=False).reset_index().dropna().rename(columns = {\"value\":f\"market_impact_{i}\"}), on=onl)[f\"market_impact_{i}\"]\n    print(len(df.columns))\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-06-28T14:57:51.025302Z","iopub.execute_input":"2022-06-28T14:57:51.025675Z","iopub.status.idle":"2022-06-28T14:57:51.066435Z","shell.execute_reply.started":"2022-06-28T14:57:51.025627Z","shell.execute_reply":"2022-06-28T14:57:51.06518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"env = jpx_tokyo_market_prediction.make_env()\niter_test = env.iter_test()\ncols=['Date','SecuritiesCode','Open','High','Low','Close','Volume','AdjustmentFactor']\nmodel = xgb.Booster()\nmodel.load_model(\"../input/model-test/model.txt\")\nprices=reduce_memory(prices)\nfor (Testprices, options, Testfinancials, trades, secondary_prices, df_pred) in iter_test:\n    print('---------------------loop once-----------------------')\n    Testprices=Testprices.drop(['RowId','ExpectedDividend'],axis=1)\n\n#     finalfinancials= financials.append(Testfinancials,ignore_index=True)\n    finalfinancials=financials\n    \n    current_date=datetime.strptime(Testprices[\"Date\"].iloc[0],'%Y-%m-%d')\n    print(\"test date-------------------------->\",current_date)\n    totaldata = prices.loc[pd.to_datetime(prices[\"Date\"]) < current_date]\n    totaldata=pd.concat([totaldata, Testprices[cols]]).reset_index(drop=True)\n    totaldata.drop(\"SupervisionFlag\",axis=1,inplace=True)\n\n\n    totaldata= feature_create(totaldata,finalfinancials,stock_list)\n    totaldata=reduce_memory(totaldata)\n    X_test=totaldata[pd.to_datetime(totaldata[\"Date\"]) == current_date].drop('Date',axis=1)\n    X_test.drop(\"Target\",axis=1,inplace=True)\n    print(\"finish featuring\")\n\n#     del totaldata\n#     gc.collect()\n\n    xgtest = xgb.DMatrix(X_test.values)\n    y_pred = model.predict(xgtest)\n    df_pred['Target'] = y_pred\n    df_pred = df_pred.sort_values(by = \"Target\", ascending = False)\n    df_pred['Rank'] = np.arange(len(df_pred.index))\n    df_pred = df_pred.sort_values(by = \"SecuritiesCode\", ascending = True)\n    df_pred.drop([\"Target\"], axis = 1)\n    submission = df_pred[[\"Date\", \"SecuritiesCode\", \"Rank\"]]\n    print(submission.head(5))\n\n    assert sample_prediction.Rank.nunique() == 2000, 'duplicate rank'\n    assert sample_prediction[\"Rank\"].notna().all(), 'na value'\n    assert sample_prediction[\"Rank\"].min() == 0, 'rank below 0'\n    assert sample_prediction[\"Rank\"].max() == len(sample_prediction[\"Rank\"]) - 1, 'rank above 1999'\n\n    env.predict(submission)\n    print(\"finsih!!!!!!!!!!!!!!!\")","metadata":{"execution":{"iopub.status.busy":"2022-06-28T14:57:51.068285Z","iopub.execute_input":"2022-06-28T14:57:51.068621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prices.drop(\"SupervisionFlag\",axis=1,inplace=True)\n# prices= feature_create(prices,financials,stock_list)\n# prices=reduce_memory(prices)\n# prices.to_pickle(\"totaldatafinal.pkl\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# totaldata.drop('Date',inplace=True,axis=1)\n# totaldata.to_csv(\"totaldatafinal.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# percent_missing = prices.isnull().sum() * 100 / len(prices)\n# missing_value_df = pd.DataFrame({'column_name': prices.columns,\n#                                  'percent_missing': percent_missing})\n# missing_value_df.sort_values('percent_missing', inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# missing_value_df[200:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = xgb.XGBRegressor(\n#     n_estimators=1500,\n#     max_depth=6,#16\n#     learning_rate=0.01,#try 0.015\n# #     subsample=0.8,\n#     colsample_bytree=0.1,\n#     missing=-999,\n#     random_state=2020,\n#     min_child_weight=1 ,\n#     gamma=0,\n    \n#     tree_method='gpu_hist' # THE MAGICAL PARAMETER\n#     )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.model_selection import TimeSeriesSplit\n# from sklearn.metrics import mean_squared_error,mean_absolute_error","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**cross validation**","metadata":{}},{"cell_type":"code","source":"# ts_fold = TimeSeriesSplit(n_splits=10,gap=10000)\n# sharpe_ratio=[]\n    \n# for fold, (train_idx, val_idx) in enumerate(ts_fold.split(X, y)):\n    \n#     print(\"\\n========================== Fold {} ==========================\".format(fold+1))\n#     if (fold+1)>8:\n#         print(train_idx, val_idx)\n#         X_train, y_train = X.iloc[train_idx,:], y[train_idx]\n#         X_valid, y_val = X.iloc[val_idx,:], y[val_idx]\n\n#         print(\"Train Date range: {} to {}\".format(X_train.Date.min(),X_train.Date.max()))\n#         print(\"Valid Date range: {} to {}\".format(X_valid.Date.min(),X_valid.Date.max()))\n\n#         X_train.drop(['Date','SecuritiesCode'], axis=1, inplace=True)\n#         X_val=X_valid[X_valid.columns[~X_valid.columns.isin(['Date','SecuritiesCode'])]]\n#         val_dates=X_valid.Date.unique()[1:-1]\n#         print(\"\\nTrain Shape: {} {}, Valid Shape: {} {}\".format(X_train.shape, y_train.shape, X_val.shape, y_val.shape))\n\n#         model.fit(X_train, y_train, early_stopping_rounds=20, eval_set=[(X_train, y_train),(X_val, y_val)], verbose=300)\n#         gc.collect()\n#         y_pred = model.predict(X_val)\n#         rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n#         mae = mean_absolute_error(y_val, y_pred)\n\n#         rank=[]\n#         X_val_df=X_valid[X_valid.Date.isin(val_dates)]\n#         for i in X_val_df.Date.unique():\n#             temp_df = X_val_df[X_val_df.Date == i].drop(['Date','SecuritiesCode'],axis=1)\n#             temp_df[\"pred\"] = model.predict(temp_df)\n#             temp_df[\"Rank\"] = (temp_df[\"pred\"].rank(method=\"first\", ascending=False)-1).astype(int)\n#             rank.append(temp_df[\"Rank\"].values)\n\n#         stock_rank=pd.Series([x for y in rank for x in y], name=\"Rank\")\n#         df=pd.concat([X_val_df.reset_index(drop=True),stock_rank,prices[prices.Date.isin(val_dates)]['Target'].reset_index(drop=True)], axis=1)\n#         sharpe=calc_spread_return_sharpe(df)\n#         sharpe_ratio.append(sharpe)\n#         print(\"Valid Sharpe: {}, RMSE: {}, MAE: {}\".format(sharpe,rmse,mae))\n\n#         del X_train, y_train,  X_val, y_val\n#         gc.collect()\n#     else:\n#         continue\n# print(\"\\nAverage cross-validation Sharpe Ratio: {:.4f}, standard deviation = {:.2f}.\".format(np.mean(sharpe_ratio),np.std(sharpe_ratio)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}