{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Get started with TPU \n\n* Tensor Processing Units (TPUs) are Google’s custom-developed application-specific integrated circuits (ASICs) used to accelerate machine learning workloads. TPUs are designed from the ground up with the benefit of Google’s deep experience and leadership in machine learning.\n\n* TPUs minimize the time-to-accuracy when you train large, complex neural network models. \n\n### When to use TPUs\n* Models dominated by matrix computations\n* Models with no custom TensorFlow operations inside the main training loop\n* Models that train for weeks or months\n* Larger and very large models with very large effective batch sizes\n","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!pip install efficientnet --quiet\n\nimport os\nimport re\nimport time\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport math\nfrom kaggle_datasets import KaggleDatasets\nimport efficientnet.tfkeras as efficientnet\n\n\nsns.set()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#detect TPU Hardware\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() #TPU Detection\n    print(\"Running on TPU\",tpu.master())\nexcept ValueError:\n    tpu = None\n    \nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() #default strategy for cpu or single GPU\n    \nprint('Replicaes:', strategy.num_replicas_in_sync)\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Preparation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Classification Categories in our Dataset\nflower_categories = [\n    'pink primrose',     'hard-leaved pocket orchid', 'canterbury bells',\n    'sweet pea',         'wild geranium',             'tiger lily',\n    'moon orchid',       'bird of paradise',          'monkshood',\n    'globe thistle',     'snapdragon',                'colt\\'s foot',\n    'king protea',       'spear thistle',             'yellow iris',\n    'globe-flower',      'purple coneflower',         'peruvian lily',\n    'balloon flower',    'giant white arum lily',     'fire lily',\n    'pincushion flower', 'fritillary',                'red ginger',\n    'grape hyacinth',    'corn poppy',                'prince of wales feathers',\n    'stemless gentian',  'artichoke',                 'sweet william',\n    'carnation',         'garden phlox',              'love in the mist',\n    'cosmos',            'alpine sea holly',          'ruby-lipped cattleya',\n    'cape flower',       'great masterwort',          'siam tulip',\n    'lenten rose',       'barberton daisy',           'daffodil',\n    'sword lily',        'poinsettia',                'bolero deep blue',\n    'wallflower',        'marigold',                  'buttercup',\n    'daisy',             'common dandelion',          'petunia',\n    'wild pansy',        'primula',                   'sunflower',\n    'lilac hibiscus',    'bishop of llandaff',        'gaura',\n    'geranium',          'orange dahlia',             'pink-yellow dahlia',\n    'cautleya spicata',  'japanese anemone',          'black-eyed susan',\n    'silverbush',        'californian poppy',         'osteospermum',\n    'spring crocus',     'iris',                      'windflower',\n    'tree poppy',        'gazania',                   'azalea',\n    'water lily',        'rose',                      'thorn apple',\n    'morning glory',     'passion flower',            'lotus',\n    'toad lily',         'anthurium',                 'frangipani',\n    'clematis',          'hibiscus',                  'columbine',\n    'desert-rose',       'tree mallow',               'magnolia',\n    'cyclamen ',         'watercress',                'canna lily',\n    'hippeastrum ',      'bee balm',                  'pink quill',\n    'foxglove',          'bougainvillea',             'camellia',\n    'mallow',            'mexican petunia',           'bromelia',\n    'blanket flower',    'trumpet creeper',           'blackberry lily',\n    'common tulip',      'wild rose'\n]\n\nprint('Number of flower categories:', len(flower_categories))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_size = (224,224)\n# Data dirs\ndata_gcs = KaggleDatasets().get_gcs_path('tpu-getting-started')\next_gcs = KaggleDatasets().get_gcs_path('tf-flower-photo-tfrec')\n\ndata_dir_by_size = {\n     (512, 512): '/tfrecords-jpeg-512x512',\n    (331, 331): '/tfrecords-jpeg-331x331',\n    (224, 224): '/tfrecords-jpeg-224x224',\n    (192, 192): '/tfrecords-jpeg-192x192'\n}\n\nsubdir = data_dir_by_size[image_size]\n\ntrain_file_names = tf.io.gfile.glob(data_gcs + subdir+ '/train'+ '/*tfrec')\nval_file_names = tf.io.gfile.glob(data_gcs + subdir + '/val' + '/*tfrec')\ntest_file_names = tf.io.gfile.glob(data_gcs + subdir + '/test' + '/*tfrec')\n\n#Extending data with additional dataset \nimagenet_files = tf.io.gfile.glob(ext_gcs + '/imagenet' + subdir + '/*.tfrec')\ninaturelist_files = tf.io.gfile.glob(ext_gcs + '/inaturalist' + subdir + '/*.tfrec')\nopenimage_files = tf.io.gfile.glob(ext_gcs + '/openimage' + subdir + '/*.tfrec')\noxford_files = tf.io.gfile.glob(ext_gcs + '/oxford_102' + subdir + '/*.tfrec')\ntensorflow_files = tf.io.gfile.glob(ext_gcs + '/tf_flowers' + subdir + '/*.tfrec')\n\ntrain_file_names = train_file_names + imagenet_files + inaturelist_files + \\\n    openimage_files + oxford_files + tensorflow_files","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we just need to form a dataset out of the *.tfrec files. Below, we define some helper functions to combine them into a single memory map and parse the structure of our dataset into a more standard ```(image, label)``` format for classification. Please refer to [Tensorflow Data API](https://www.tensorflow.org/guide/data_performance) and [TFRecordDataset](https://www.tensorflow.org/api_docs/python/tf/data/TFRecordDataset) Documentation for more information.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def decode_image(image_data):\n    \"\"\"Decodes JPEG data and return a normalized image.\n    \n    WARNING: you may need a different normalization if you\n    use VGG-like networks.\n    \"\"\"\n    image = tf.image.decode_jpeg(image_data, channels =3)\n    image = tf.cast(image, tf.float32)/ 255.0\n    image = tf.reshape(image, [*image_size,3])\n    return image\n\ndef read_labeled_tfrecord(example):\n    \"\"\"\n    Converts a single record in labeled dataset (i.e. train and validation\n    sets) to the more convenient format (image, label)\n    \"\"\"\n    example = tf.io.parse_single_example(\n            serialized = example,\n            features = {\n                    'image' : tf.io.FixedLenFeature([], tf.string),\n                    'class' : tf.io.FixedLenFeature([], tf.int64)\n            }\n    )\n    \n    image = decode_image(example['image'])\n    label = tf.cast(example['class'], tf.int32)\n    return image, label\n\ndef read_unlabeled_tfrecord(example):\n    \"\"\"\n    Converts a single record in labeled dataset (i.e. test\n    set) to the more convenient format (image, id)\n    \"\"\"\n    example = tf.io.parse_single_example(\n        serialized=example,\n        features={\n            'image': tf.io.FixedLenFeature([], tf.string),\n            'id': tf.io.FixedLenFeature([], tf.string),\n        }\n    )\n    \n    image = decode_image(example['image'])\n    idnum = example['id']\n    return image, idnum\n\ndef load_dataset(filenames, labeled = True, ordered= False):\n    \"\"\"\n    Given a list of `*.tfrec` file names, converts them into a `tf.data.Dataset`\n    object that yields elements of format (image, label) or (image, id)\n    \n    # Arguments\n        filenames: list of paths to `*.tfrec` files\n        labeled: if True, the resulting dataset will yield data in format\n            (image, label). Otherwise it will yield in format (image, id)\n        ordered: whether to shuffle the dataset (not desirable for test/val)\n        \n    # Returns\n        a `tf.data.Dataset` object that holds memory map to `*.tfrec` files\n    \"\"\"\n    ignore_order = tf.data.Options()\n    ignore_order.experimental_deterministic = ordered\n    \n    dataset = tf.data.TFRecordDataset(\n        filenames, num_parallel_reads = tf.data.experimental.AUTOTUNE)\n    dataset = dataset.with_options(ignore_order)\n    dataset = dataset.map(\n        read_labeled_tfrecord if labeled else read_unlabeled_tfrecord,\n        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n    return dataset\n\ndef count_data_items(filenames):\n    \"\"\"\n    There's no way to obtain explicitly the number of elements in each dataset\n    (see: https://stackoverflow.com/questions/40472139/), but we can infer that\n    from file names, i.e. flowers00-230.tfrec = 230 data items\n    \"\"\"\n    return np.sum([\n        int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1))\n        for filename in filenames])\n\n\n# Import Datasets\ntrain_dataset = load_dataset(train_file_names, labeled=True, ordered=False)\nval_dataset = load_dataset(val_file_names, labeled=True, ordered=False)\ntest_dataset = load_dataset(test_file_names, labeled=False, ordered=True)\n\n# Calculate number of items\nnum_training_samples = count_data_items(train_file_names)\nnum_validation_samples = count_data_items(val_file_names)\nnum_testing_samples = count_data_items(test_file_names)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Train samples:', num_training_samples, end=', ')\nprint('Val samples:', num_validation_samples, end=', ')\nprint('Test samples:', num_testing_samples)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below are all available categories in the given dataset (we display a sample from each of 104 categories). As expected from flowers, they are quite symmetric, which is good - we can apply augmentations like flipping, shearing, and rotating. Since the images were taken in nature, the exposure varies, so we can use brightness augmentations also.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rows, cols = 13, 8\nplt.figure(figsize=(2.2 * cols, rows * 2))\n\nimage_dict = dict([(key, None) for key in range(len(flower_categories))])\n\ndef is_not_full_yet():\n    for key, value in image_dict.items():\n        if value is None:\n            return True\n    return False\n\nfor i, (image, label) in enumerate(train_dataset.as_numpy_iterator()):\n    if i >= num_training_samples:\n        break\n\n    if label in image_dict and image_dict[label] is None:\n        plt.subplot(rows, cols, label + 1)\n        plt.axis('off')\n        plt.imshow(image)\n        plt.title(flower_categories[label])\n        image_dict[label] = True\n    \n    if not is_not_full_yet():\n        break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data augmentation methods in GPU/TPU\n\nThe definition of our transformation object transformer is self explanatory. The augmentation method will be applied later by our training dataset (tf.data.Dataset object).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_mat(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n    # returns 3x3 transformmatrix which transforms indicies\n        \n    # CONVERT DEGREES TO RADIANS\n    rotation = math.pi * rotation / 180.\n    shear = math.pi * shear / 180.\n    \n    # ROTATION MATRIX\n    c1 = tf.math.cos(rotation)\n    s1 = tf.math.sin(rotation)\n    one = tf.constant([1],dtype='float32')\n    zero = tf.constant([0],dtype='float32')\n    rotation_matrix = tf.reshape( tf.concat([c1,s1,zero, -s1,c1,zero, zero,zero,one],axis=0),[3,3] )\n        \n    # SHEAR MATRIX\n    c2 = tf.math.cos(shear)\n    s2 = tf.math.sin(shear)\n    shear_matrix = tf.reshape( tf.concat([one,s2,zero, zero,c2,zero, zero,zero,one],axis=0),[3,3] )    \n    \n    # ZOOM MATRIX\n    zoom_matrix = tf.reshape( tf.concat([one/height_zoom,zero,zero, zero,one/width_zoom,zero, zero,zero,one],axis=0),[3,3] )\n    \n    # SHIFT MATRIX\n    shift_matrix = tf.reshape( tf.concat([one,zero,height_shift, zero,one,width_shift, zero,zero,one],axis=0),[3,3] )\n    \n    return K.dot(K.dot(rotation_matrix, shear_matrix), K.dot(zoom_matrix, shift_matrix))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def transform(image, label, DIM = image_size[0]):\n    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n    # output - image randomly rotated, sheared, zoomed, and shifted\n\n    XDIM = DIM % 2\n    \n    rot = 15. * tf.random.normal([1],dtype='float32')\n    shr = 5. * tf.random.normal([1],dtype='float32') \n    h_zoom = 1.0 + tf.random.normal([1],dtype='float32')/10.\n    w_zoom = 1.0 + tf.random.normal([1],dtype='float32')/10.\n    h_shift = 16. * tf.random.normal([1],dtype='float32') \n    w_shift = 16. * tf.random.normal([1],dtype='float32') \n  \n    # GET TRANSFORMATION MATRIX\n    m = get_mat(rot,shr,h_zoom,w_zoom,h_shift,w_shift) \n\n    # LIST DESTINATION PIXEL INDICES\n    x = tf.repeat( tf.range(DIM//2,-DIM//2,-1), DIM )\n    y = tf.tile( tf.range(-DIM//2,DIM//2),[DIM] )\n    z = tf.ones([DIM*DIM],dtype='int32')\n    idx = tf.stack( [x,y,z] )\n    \n    # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n    idx2 = K.dot(m,tf.cast(idx,dtype='float32'))\n    idx2 = K.cast(idx2,dtype='int32')\n    idx2 = K.clip(idx2,-DIM//2+XDIM+1,DIM//2)\n    \n    # FIND ORIGIN PIXEL VALUES           \n    idx3 = tf.stack( [DIM//2-idx2[0,], DIM//2-1+idx2[1,]] )\n    d = tf.gather_nd(image,tf.transpose(idx3))\n        \n    return tf.reshape(d,[DIM,DIM,3]),label","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below are examples of our data augmentation algorithm. There are several data augmentation methods being used here. These methods can be divided into \"Fast\" and \"Slow\" categories:\n\n1. Fast Augmentation Methods:\n    - Random horizontal/vertical flip\n    - Random brightness correction (up to certain gamma)\n\n2. Slow Augmentation Methods:\n    - Random Rotation (to the left or to the right, up to a certain degree)\n    - Random shear (to the left or to the right, up to a certain degree)\n    - Random zooming (in or out, horizontal or vertical, up to a certain value)\n    - Random shifting (up to a certain value)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rows, cols = 3, 6\nplt.figure(figsize=(2.9 * cols, rows * 2.9))\n\nfor row, element in zip(range(rows), train_dataset):\n    one_element = tf.data.Dataset.from_tensors(element).repeat()\n    for col, (image, _) in zip(range(cols), one_element.map(transform).as_numpy_iterator()):\n        plt.subplot(rows, cols, row * cols + col + 1)\n        plt.axis('off')\n        plt.imshow(image)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prepare model, train\nHere we define how the data will be feeded to our model during training:\n\n* For training dataset, we will apply random augmentations on each image, shuffle it, group into batches, and pre-fetch the data to our TPU (so that we don't have to wait for the data to be loaded each time). tf.data.experimental.AUTOTUNE will try to apply the most optimal fetching strategies, depending on our hardware configuration.\n* For validation and testing datasets, we don't augment or shuffle (obviously), but employ the same prefetch method. Also, since our validation set is small, we will try to cache it.\n\nAlso, we define some Keras Callbacks to improve our training:\n* early_stopping - performs early stopping, i.e. halts the training if the validation loss has not improved in several epochs\n* save_checkpoints - just save intermediate models, so that we don't lose good models\n* lr_callback - we will schedule our learning rate in such a way, that is suitable to fine-tuning from a pre-trained network","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training Parameters\nbatch_size = 16 * strategy.num_replicas_in_sync\nepochs = 25\n\n#prepare dataflow configuration\nbatched_train_dataset = train_dataset.map(transform)\nbatched_train_dataset = batched_train_dataset.repeat()\nbatched_train_dataset = batched_train_dataset.shuffle(buffer_size = 2048)\nbatched_train_dataset = batched_train_dataset.batch(batch_size)\nbatched_train_dataset = batched_train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n\n#Prepare validation config\nbatched_val_dataset = val_dataset.batch(batch_size)\nbatched_val_dataset = batched_val_dataset.cache()\nbatched_val_dataset = batched_val_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n\n#Prepare testing config\nbatched_test_dataset = test_dataset.batch(batch_size)\nbatched_test_dataset = batched_test_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n\n#Early stopping \nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor ='val_loss', patience= 3)\n\n#Saving checkpoint\nsave_checkpoints = tf.keras.callbacks.ModelCheckpoint('/kaggle/working/weights.{epoch:02d}-{val_loss:.2f}.hdf5',\n    save_freq='epoch', period=4)\n\n#define learning rate parameters\nLR_START = 0.00001\nLR_MAX = 0.00005 * strategy.num_replicas_in_sync\nLR_MIN = 0.00001\nLR_RAMPUP_EPOCHS = 5\nLR_SUSTAIN_EPOCHS = 0\nLR_DECAY = .8\n#define ramp up and decay\ndef lr_schedule(epoch):\n    if epoch < LR_RAMPUP_EPOCHS:\n        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n        lr = LR_MAX\n    else:\n        lr = (LR_MAX - LR_MIN) * LR_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\n    return lr\n\n# Learning Rate Scheduler for fine-tuning jobs (first increase lr, then decrease)\nlr_callback = tf.keras.callbacks.LearningRateScheduler(lr_schedule, verbose = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The ktu.LRSchedulers.FineTuningLR starts from a small learning rate (to preserve delicate pre-trained weights on Imagenet). Then, it gradually increases the learning rate and drops to minimum again at the end. The graph of the learning rate against batch number is shown below (you can see that lr changes only in the end of each epoch):","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#visualize learning rate schedule\nrng = [i for i in range(epochs)]\ny = [lr_schedule(x) for x in rng]\nplt.plot(rng, y)\nprint(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\".format(y[0], max(y), y[-1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we build our model, based on some pre-trained ImageNet feature extractor, and fine tune it for the new dataset. In this case, I chose EfficientNetB7, since it is shown by other competitors that this networks performs better on this competition than ResNets, DenseNets, VGGs, and other networks. Please notice that I am using weights='noisy-student' here - it obtained in this paper earlier this year which gives better results on ImageNet than the original one.\n\nTraining is quite straightforward. For more information please refer to the Keras Models API documentation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_model():\n    with strategy.scope():\n        feature_extractor = efficientnet.EfficientNetB7(\n                        weights = 'noisy-student', include_top = False, input_shape=[*image_size, 3])\n        feature_extractor.trainable = True\n        \n        model = tf.keras.Sequential([\n            feature_extractor,\n            tf.keras.layers.GlobalAveragePooling2D(),\n            tf.keras.layers.Dense(len(flower_categories), activation='softmax', dtype='float32')\n        ])\n        \n        model.compile(optimizer='adam',\n        loss = 'sparse_categorical_crossentropy',\n        metrics=['sparse_categorical_accuracy']\n        )\n        \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = get_model()\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train a model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train model\nhistory = model.fit(\n    batched_train_dataset,\n    steps_per_epoch=num_training_samples // batch_size,\n    epochs=epochs,\n    callbacks=[lr_callback, early_stopping, save_checkpoints],\n    validation_data=batched_val_dataset,\n    verbose=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Submission","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Creating submission\")\ntest_image_dataset = batched_test_dataset.map(lambda image,idnum: image)\npredictions = np.argmax(model.predict(test_image_dataset), axis = -1)\ntest_ids_dataset = batched_test_dataset.map(lambda image, idnum: idnum).unbatch()\ntest_ids = next(iter(test_ids_dataset.batch(num_testing_samples))).numpy().astype('U') # all in one batch\nnp.savetxt('submission.csv', np.rec.fromarrays([test_ids, predictions]), fmt=['%s', '%d'], delimiter=',', header='id,label', comments='')\nprint('Done!')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!head submission.csv","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### References:\n\n* [A beginner TPU kernel model](https://www.kaggle.com/chankhavu/a-beginner-s-tpu-kernel-single-model-0-97/data)\n* [Kfold Efficientnet](https://www.kaggle.com/tuckerarrants/kfold-efficientnet-augmentations-s)","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}