{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Home Credit Default Risk - Team 3 (Kahsai, Nichols, Pellerito)\nObjective: build a model that identifies consumer loan customers who are most likely to default on their loan obligations. We will consider model types: logistic regression, decision tree, random forest, extra trees, XG Boost and Light Gradient Boost Machine. (And maybe some ensemble models at the end.) This notebook imports the file train1205.pkl, produced by the Data Generation Notebook.","metadata":{}},{"cell_type":"markdown","source":"### Import packages","metadata":{}},{"cell_type":"code","source":"# standard Python tools\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# special tools for working in Kaggle\nimport joblib   # save and load ML models\nimport gc       # garbage collection\nimport os \nimport sklearn\nimport warnings\nfrom sklearn.utils.testing import ignore_warnings\nfrom sklearn.exceptions import ConvergenceWarning\n\n# preprocessing steps\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\n# machine learning models and tools\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix\nfrom xgboost import XGBClassifier, XGBRegressor\nfrom xgboost import plot_importance\nfrom sklearn.svm import SVR\nfrom sklearn import datasets\n\n# Optuna\nimport optuna as op\nfrom optuna import create_study\n\n# cross validation and metrics - remember this competition is scored as area under curve\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\n# clear out any old junk\ngc.collect()\n\n# Let's put this someplace where it's easy to find - the proportion split between training and validation data\ntrain_size = 0.75","metadata":{"execution":{"iopub.status.busy":"2021-12-08T02:51:26.464881Z","iopub.execute_input":"2021-12-08T02:51:26.46542Z","iopub.status.idle":"2021-12-08T02:51:26.708446Z","shell.execute_reply.started":"2021-12-08T02:51:26.465353Z","shell.execute_reply":"2021-12-08T02:51:26.707039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import the training data (already processed and merged)","metadata":{}},{"cell_type":"markdown","source":"### Read the training data","metadata":{}},{"cell_type":"code","source":"# Main table\npd.options.display.max_columns = None\ntrain = pd.read_pickle('../input/testandtrain1205/train1205.pkl')\ntrain.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T02:51:26.710912Z","iopub.execute_input":"2021-12-08T02:51:26.711178Z","iopub.status.idle":"2021-12-08T02:51:28.856576Z","shell.execute_reply.started":"2021-12-08T02:51:26.711147Z","shell.execute_reply":"2021-12-08T02:51:28.855584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Split the training and validation sets\nFor some early model runs, we used as little as 10% of the training data. By the end, we were using 75% for training and holding back 25% for validation. The parameter that controls the proportion of the train/test split is called test_size and can be found in the first code cell of this notebook.","metadata":{}},{"cell_type":"code","source":"y = train['TARGET'].values\nX_train, X_valid, y_train, y_valid = train_test_split(train.drop(['TARGET', 'SK_ID_CURR'], axis = 1), y, stratify = y, test_size=1 - train_size, random_state=1)\nprint('Shape of X_train:',X_train.shape)\nprint('Shape of y_train:',y_train.shape)\nprint('Shape of X_valid:',X_valid.shape)\nprint('Shape of y_valid:',y_valid.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T02:51:28.85841Z","iopub.execute_input":"2021-12-08T02:51:28.859392Z","iopub.status.idle":"2021-12-08T02:51:30.958021Z","shell.execute_reply.started":"2021-12-08T02:51:28.859311Z","shell.execute_reply":"2021-12-08T02:51:30.956718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### make lists of cat and num features for pipeline, based on dtype\nwe need to identify which factors are categorical and which are numeric, so that the pipeline knows how to process them.","metadata":{}},{"cell_type":"code","source":"types = np.array([z for z in X_train.dtypes])        # array([dtype('float64'), dtype('float64'), dtype('O'), dtype('O') ...])\nall_columns = X_train.columns.values                 # list of all column names\nis_num = types != 'object'                           # returns array([False, False, False, False,  True,  True, ...) where True is a numeric variable\nnum_features = all_columns[is_num].tolist()          # list of all numeric columns\ncat_features = all_columns[~is_num].tolist()         # list of all categorical columns\n\nprint(len(num_features), \"numeric features\")\nprint(len(cat_features), \"categorical features\")","metadata":{"execution":{"iopub.status.busy":"2021-12-08T02:51:30.96059Z","iopub.execute_input":"2021-12-08T02:51:30.960942Z","iopub.status.idle":"2021-12-08T02:51:30.972154Z","shell.execute_reply.started":"2021-12-08T02:51:30.960896Z","shell.execute_reply":"2021-12-08T02:51:30.970892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### build model pipeline based on num_cols and cat_cols lists","metadata":{}},{"cell_type":"code","source":"features = num_features + cat_features\n\nPipe_num = Pipeline(\n    steps=[\n    ('imputer', SimpleImputer(strategy = 'median')),        # tried median, mean, constant strategies\n    ('scaler', StandardScaler())       ])\n\nPipe_cat = Pipeline(\n    steps=[\n    ('imputer', SimpleImputer(strategy = 'constant', fill_value = 'Unknown')),\n    ('onehot', OneHotEncoder())        ])\n\npreprocessor = ColumnTransformer(\n    transformers = [\n        ('num', Pipe_num, num_features),\n        ('cat', Pipe_cat, cat_features)])\n\npreprocessor.fit(train[features])\nX_train = preprocessor.transform(X_train[features])\nX_valid = preprocessor.transform(X_valid[features])\n\nprint('Shape of X_train:',X_train.shape)\nprint('Shape of y_train:',y_train.shape)\n\ngc.collect()\n\ndel train            # yes, it is safe to delete this - from now on we are only working with X_train, X_valid, y_train, y_valid\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-12-08T02:51:30.974991Z","iopub.execute_input":"2021-12-08T02:51:30.975527Z","iopub.status.idle":"2021-12-08T02:53:27.192383Z","shell.execute_reply.started":"2021-12-08T02:51:30.975488Z","shell.execute_reply":"2021-12-08T02:53:27.191013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Build Model Scoreboard\nEach time we run a new model, we will append a new row to the scoreboard.","metadata":{}},{"cell_type":"code","source":"pd.set_option('display.max_colwidth', None)             # LGBM in particular has long hyperparameters and I want to see them all\nresults = pd.DataFrame(columns = ['Model Type','AUC - 10xv', 'AUC - Valid', 'Hyperparameters'])","metadata":{"execution":{"iopub.status.busy":"2021-12-08T02:53:27.193993Z","iopub.execute_input":"2021-12-08T02:53:27.194263Z","iopub.status.idle":"2021-12-08T02:53:27.203104Z","shell.execute_reply.started":"2021-12-08T02:53:27.194231Z","shell.execute_reply":"2021-12-08T02:53:27.202106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Memory Management\nThe only data frame open right now should be the model scoreboard 'results' that we just created. Even the train dataframe can be closed now because the information it contained is stored in arrays X_train, X_valid, etc. We should have about 15 GB free at this point.","metadata":{"execution":{"iopub.status.busy":"2021-11-24T01:42:29.954859Z","iopub.execute_input":"2021-11-24T01:42:29.955175Z","iopub.status.idle":"2021-11-24T01:42:29.958884Z","shell.execute_reply.started":"2021-11-24T01:42:29.955141Z","shell.execute_reply":"2021-11-24T01:42:29.957868Z"}}},{"cell_type":"code","source":"%who_ls DataFrame","metadata":{"execution":{"iopub.status.busy":"2021-12-08T02:53:27.204643Z","iopub.execute_input":"2021-12-08T02:53:27.20566Z","iopub.status.idle":"2021-12-08T02:53:27.218126Z","shell.execute_reply.started":"2021-12-08T02:53:27.205607Z","shell.execute_reply":"2021-12-08T02:53:27.217507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import psutil\nprint(psutil.virtual_memory()[1]/2**30, \"GB free\")","metadata":{"execution":{"iopub.status.busy":"2021-12-08T02:53:27.219647Z","iopub.execute_input":"2021-12-08T02:53:27.220347Z","iopub.status.idle":"2021-12-08T02:53:27.232915Z","shell.execute_reply.started":"2021-12-08T02:53:27.220298Z","shell.execute_reply":"2021-12-08T02:53:27.231929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Models","metadata":{}},{"cell_type":"markdown","source":"### Logistic Regression\nI tried both lbfgs and saga solvers, and got faster performance (and slightly higher scoring) out of lbfgs. I experimented with max_iter as low as 200, but did not see any degradation of accuracy below 400. L2 (ridge regression) regularization parameter easily outperformed L1. This model didn't outperform LightGBM or XGBoost, but it might be a candidate for an ensemble model.","metadata":{}},{"cell_type":"code","source":"%%time\nwarnings.filterwarnings(\"ignore\", category = ConvergenceWarning)    # seems like we are okay if we set the max_iter threshold around 400.\n\nlr_clf = LogisticRegression(max_iter = 400, solver = 'lbfgs')       # lbfgs is running rings around saga\n\nlr_parameters = {\"C\" : [0.005], \"penalty\" : ['l2']}\nlr_grid = GridSearchCV(lr_clf, lr_parameters, cv=10, refit='True', n_jobs=-1, verbose=1, scoring='roc_auc')\nlr_grid.fit(X_train, y_train)\n\nlr_model = lr_grid.best_estimator_\n\n# update model scoreboard\nresults = results.append({'Model Type' : 'Logistic Regression',\n                          'AUC - 10xv' : lr_grid.best_score_,\n                          'AUC - Valid' : roc_auc_score(y_valid, lr_model.predict_proba(X_valid)[:, 1]),\n                          'Hyperparameters' : lr_grid.best_params_},\n                        ignore_index=True)\nresults","metadata":{"execution":{"iopub.status.busy":"2021-12-08T02:53:27.23453Z","iopub.execute_input":"2021-12-08T02:53:27.234964Z","iopub.status.idle":"2021-12-08T02:56:50.291502Z","shell.execute_reply.started":"2021-12-08T02:53:27.23493Z","shell.execute_reply":"2021-12-08T02:56:50.290554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Decision Tree\nThe best parameters I could find for the decision tree were max_depth = 8 and min_samples_leaf = 10. This model is not going to be our best, but it provides a benchmark that our Random Forest and Extra Trees models should be able to beat. (Both those models are based on collections of decision trees.)","metadata":{}},{"cell_type":"code","source":"%%time\ndt_clf = DecisionTreeClassifier(random_state=1)\ndt_parameters = {'max_depth': [8], 'min_samples_leaf': [10]}\n\ndt_grid = GridSearchCV(dt_clf, dt_parameters, cv=10, refit='True', n_jobs=-1, verbose=0, scoring='roc_auc')\ndt_grid.fit(X_train, y_train)\ndt_model = dt_grid.best_estimator_\n\n# update model scoreboard\nresults = results.append({'Model Type' : 'Decision Tree',\n                          'AUC - 10xv' : dt_grid.best_score_,\n                          'AUC - Valid' : roc_auc_score(y_valid, dt_model.predict_proba(X_valid)[:,1]),\n                          'Hyperparameters' : dt_grid.best_params_},\n                        ignore_index=True)\ngc.collect()\nresults","metadata":{"execution":{"iopub.status.busy":"2021-12-08T02:56:50.293601Z","iopub.execute_input":"2021-12-08T02:56:50.2945Z","iopub.status.idle":"2021-12-08T02:59:29.856614Z","shell.execute_reply.started":"2021-12-08T02:56:50.29444Z","shell.execute_reply":"2021-12-08T02:59:29.855626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Random Forest\nWe are going to use Optuna to tune the hyperparameters (max_depth and min_samples_leaf) for the random forest model, and then run the full model based on the Optuna results. (See appendix for results.) I used 39 and 37 as my final hyperparameters, but the model doesn't seem to be very sensitive over a pretty wide range of choices. Basically any pair of numbers in the 30-40 range is giving us about the same results.","metadata":{}},{"cell_type":"code","source":"%%time\nrf_clf = RandomForestClassifier(random_state=1, n_estimators=100)\nrf_parameters = {'max_depth': [39],  'min_samples_leaf': [37]}        # results from Optuna study\nrf_grid = GridSearchCV(rf_clf, rf_parameters, cv=10, refit='True', n_jobs=-1, verbose=1, scoring='roc_auc')\nrf_grid.fit(X_train, y_train)\nrf_model = rf_grid.best_estimator_\n\n# update model scoreboard\nresults = results.append({'Model Type' : 'Random Forest',\n                          'AUC - 10xv' : rf_grid.best_score_,\n                          'AUC - Valid' : roc_auc_score(y_valid, rf_model.predict_proba(X_valid)[:, 1]),\n                          'Hyperparameters' : rf_grid.best_params_},\n                        ignore_index=True)\ngc.collect()\nresults","metadata":{"execution":{"iopub.status.busy":"2021-12-08T02:59:29.858461Z","iopub.execute_input":"2021-12-08T02:59:29.859235Z","iopub.status.idle":"2021-12-08T03:15:12.914081Z","shell.execute_reply.started":"2021-12-08T02:59:29.859159Z","shell.execute_reply":"2021-12-08T03:15:12.912525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Extra Trees\nNow with more trees! Similar to Random Forest in that it uses a collection of Decision Trees to make a classification. Main difference is that Extra Trees model uses the whole original sample, while Random Forest takes subsamples with replacement. Also, RF uses optimized split points, while XT uses random split points. Our Extra Trees model produced about the same performance as the Random Forest.\n\nhttps://towardsdatascience.com/extra-trees-please-cec916e24827","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesClassifier\n\nXT_clf = ExtraTreesClassifier(random_state = 1, class_weight = {0: 11.5, 1:1})                     \n\nXT_params={\n        'n_estimators': [300],\n        'min_samples_leaf': [18],\n        'min_samples_split': [8],\n    },\n\nXT_grid = GridSearchCV(XT_clf, XT_params, cv=5, refit='True', n_jobs=-1, verbose=1, scoring='roc_auc')\nXT_grid.fit(X_train, y_train)\n\nXT_model = XT_grid.best_estimator_\n\n# update model scoreboard\nresults = results.append({'Model Type' : 'Extra Trees',\n                          'AUC - 10xv' : XT_grid.best_score_,\n                          'AUC - Valid' : roc_auc_score(y_valid, XT_model.predict_proba(X_valid)[:, 1]),\n                          'Hyperparameters' : XT_grid.best_params_},\n                        ignore_index=True)\nresults","metadata":{"execution":{"iopub.status.busy":"2021-12-08T03:15:12.915774Z","iopub.execute_input":"2021-12-08T03:15:12.916086Z","iopub.status.idle":"2021-12-08T03:37:11.774892Z","shell.execute_reply.started":"2021-12-08T03:15:12.916045Z","shell.execute_reply":"2021-12-08T03:37:11.773895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### XG Boost\nXGBoost runs quickly if you have the GPU accelerator enabled, and produced better accuracy than our logistic regression model. It is also easier to tune than Light GBM, although it can be susceptible to overfitting if you set the max depth or learning rate parameters too high. This article was a major help in getting the best out of our XG Boost model: https://medium.com/@juniormiranda_23768/ensemble-methods-tuning-a-xgboost-model-with-scikit-learn-54ff669f988a","metadata":{}},{"cell_type":"code","source":"%%time\nimport xgboost as xgb\n\nXGB_clf = xgb.XGBClassifier() #tree_method = 'gpu_hist', gpu_id = -1)\nXGB_parameters = {'max_depth': [6], 'n_estimators' : [200], 'learning_rate' : [0.05], 'gamma' : [0]}\n\nXGB_grid = GridSearchCV(XGB_clf, XGB_parameters, cv=5, n_jobs=-1, verbose=0, scoring='roc_auc')\nXGB_grid.fit(X_train, y_train)\nXGB_model = XGB_grid.best_estimator_\n\n# update model scoreboard\nresults = results.append({'Model Type' : 'XG Boost',\n                          'AUC - 10xv' : XGB_grid.best_score_,\n                          'AUC - Valid' : roc_auc_score(y_valid, XGB_model.predict_proba(X_valid)[:,1]),\n                          'Hyperparameters' : XGB_grid.best_params_},\n                        ignore_index=True)\ngc.collect()\nresults","metadata":{"execution":{"iopub.status.busy":"2021-12-08T03:37:11.776524Z","iopub.execute_input":"2021-12-08T03:37:11.776779Z","iopub.status.idle":"2021-12-08T04:30:59.035506Z","shell.execute_reply.started":"2021-12-08T03:37:11.776748Z","shell.execute_reply":"2021-12-08T04:30:59.033499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Light GBM\nLight Gradient Boost Machine runs fast and produces our best predictive model. Performance starts to decline if the number of estimators is below 1400.\n\nhttps://neptune.ai/blog/lightgbm-parameters-guide\n\nThe optimized LGBM is our strongest model. Adjusting the regularization parameters alpha and gamma helped to boost the model's accuracy (default value is 20 for each.) The default learning rate is 0.10 but the sweet spot for this model seems to be closer to 0.02.","metadata":{}},{"cell_type":"code","source":"%%time\nimport lightgbm as lgb\n\nparams = {'boosting_type': 'gbdt', 'objective': 'binary', 'max_depth': 18,                        # choices for boosting type are 'gbdt', 'rf', 'dart', 'goss'\n          'nthread': -1, 'num_leaves': 30, 'learning_rate': 0.02, 'n_estimators' : 1600,          # default learning rate is 0.1 but 0.02 feels like the sweet spot.\n          'max_bin': 512, 'subsample_for_bin': 200, 'subsample': 0.8,\n          'subsample_freq': 1, 'colsample_bytree': 0.8, \n          'reg_alpha': 80, 'reg_lambda': 20,                                                      # bumping up the alpha parameter gave us a little boost\n          'min_split_gain': 0.5, 'min_child_weight': 1,\n          'min_child_samples': 10, 'scale_pos_weight': 11.5, 'num_class' : 1,                     # about 92% target=0 to 8% target=1 - ratio is about 11.5 to 1\n          'metric' : 'auc'\n          }\n    \nLGB_clf = lgb.LGBMClassifier(**params)\nLGB_parameters = {'learning_rate': [0.02]}\n\nLGB_grid = GridSearchCV(LGB_clf, LGB_parameters, cv=5, scoring= 'roc_auc')\nLGB_grid.fit(X_train, y_train)\nLGB_model = LGB_grid.best_estimator_\n\n# update model scoreboard\nresults = results.append({'Model Type' : 'Light GBM',\n                          'AUC - 10xv' : LGB_grid.best_score_,\n                          'AUC - Valid' : roc_auc_score(y_valid, LGB_model.predict_proba(X_valid)[:, 1]),\n                          'Hyperparameters' : LGB_grid.best_params_},\n                        ignore_index=True)\ngc.collect()\nresults","metadata":{"execution":{"iopub.status.busy":"2021-12-08T04:30:59.038928Z","iopub.execute_input":"2021-12-08T04:30:59.039224Z","iopub.status.idle":"2021-12-08T04:43:15.980654Z","shell.execute_reply.started":"2021-12-08T04:30:59.039189Z","shell.execute_reply":"2021-12-08T04:43:15.979227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Identifying Ensemble Model candidates\nThe Light GBM ended up being our best model and I submitted it to the Kaggle competition, scoring 78.36%. Let's see if we can identify an ensemble model that can do better. First, let's look at a correlation plot of the predictions made on the validation data for the five models - we want to find combinations of models that are high scoring, but which are also fairly low in correlation: if two models have the same strengths and weaknesses in terms of classification, they are not going to complement each other well in an ensemble. One thing that looks promising in the correlation plot is Light GBM and Logistic Regression - they were our first and third best models, but have a low correlation coefficient. XGB and LGB might be too similar to each other to make a good ensemble.","metadata":{}},{"cell_type":"code","source":"# First, let's capture the predicted probabilities on the validation data set from the models that we liked.\nEns1 = lr_model.predict_proba(X_valid)[:, 1]\nEns2 = dt_model.predict_proba(X_valid)[:, 1]\nEns3 = rf_model.predict_proba(X_valid)[:, 1]\nEns4 = XT_model.predict_proba(X_valid)[:, 1]\nEns5 = XGB_model.predict_proba(X_valid)[:, 1]\nEns6 = LGB_model.predict_proba(X_valid)[:, 1]\n\nEnsDF = pd.DataFrame({'LR' : Ens1, 'DT' : Ens2, 'RF' : Ens3, 'XT' : Ens4, 'XGB' : Ens5, 'LGB' : Ens6})\n\nsns.heatmap(EnsDF.corr(), annot = True, cmap = \"BuPu\", alpha = 0.5, fmt = \".4f\", cbar = False)\nplt.plot()","metadata":{"execution":{"iopub.status.busy":"2021-12-08T04:43:15.982269Z","iopub.execute_input":"2021-12-08T04:43:15.982574Z","iopub.status.idle":"2021-12-08T04:43:33.575492Z","shell.execute_reply.started":"2021-12-08T04:43:15.982541Z","shell.execute_reply":"2021-12-08T04:43:33.57459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here are some candidates for ensemble models. Some of them outscore the pure LGBM model on the validation data, even though they contain some components that are less accurate than the LGBM. The combination of LGBM and LR looks promising, as those were our first and third-best models but they have relatively low correlation to each other. In fact, combos heavy on LGBM and LR generally outperform combos of LGBM and XGB - the predictions of LGBM and LGB might just be too similar to each other to make a good ensemble.\n\nWe ended up submitting the 80% LGB, 15% LR and 5% RF blended model on the test data and achieved a public score of 78.737%, which does in fact beat the pure light GBM score.","metadata":{}},{"cell_type":"code","source":"# Candidates for ensemble models:\nScenarios = ('Equal weight', 'Top 5 equal', 'Top 4 equal', 'Top 3 equal', 'Top 2 equal', 'LGB+LR', 'Half LGB half mixed', \n             '70/20/10 blend', '80/15/5 blend', 'Pure LGB')\nLR_wt =   (1/6, 1/5, 1/4, 1/3,   0, 1/2,  0.1,  0.1, 0.15,  0)\nDT_wt  =  (1/6,   0,   0,   0,   0,   0,  0.1,    0,    0,  0)\nRF_wt  =  (1/6, 1/5, 1/4,   0,   0,   0,  0.1,    0, 0.05,  0)\nXT_wt  =  (1/6, 1/5,   0,   0,   0,   0,  0.1,  0.2,    0,  0)\nXGB_wt  = (1/6, 1/5, 1/4, 1/3, 1/2,   0,  0.1,    0,    0,  0) \nLGB_wt  = (1/6, 1/5, 1/4, 1/3, 1/2, 1/2,  0.5,  0.7,  0.8,  1) \n\ndef Ensemble(a, b, c, d, e, f):\n    E = a * Ens1 + b * Ens2 + c * Ens3 + d * Ens4 + e * Ens5 + f * Ens6\n    return(np.round(roc_auc_score(y_valid, E),6))\n\nEnsDF_wt = pd.DataFrame({'Scenario' : Scenarios, 'LR' : LR_wt, 'DT' : DT_wt, 'RF' : RF_wt, 'XT' : XT_wt,  'XGB' : XGB_wt, 'LGB' : LGB_wt})\nEnsDF_wt['Score'] = EnsDF_wt.apply(lambda row: Ensemble(row['LR'], row['DT'], row['RF'], row['XT'], row['XGB'], row['LGB']),axis = 1)\nEnsDF_wt.sort_values('Score', ascending = False)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T05:00:20.463267Z","iopub.execute_input":"2021-12-08T05:00:20.463948Z","iopub.status.idle":"2021-12-08T05:00:20.761728Z","shell.execute_reply.started":"2021-12-08T05:00:20.463908Z","shell.execute_reply":"2021-12-08T05:00:20.760897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualizing Model Outputs\nThis section has been moved to another notebook (see link below.) The code cell below this one creates a data frame that gets loaded into Final Model Evaluation Notebook, and the graphics are generated from this dataframe only. The model that I am sending to the Evaluation notebook is the pure Light GBM model.\n\nhttps://www.kaggle.com/cloycebox/final-model-evaluation-notebook-cjp-and-friends/edit/run/80947983","metadata":{}},{"cell_type":"code","source":"output_data = pd.DataFrame({'target': y_valid, 'prediction' : LGB_model.predict_proba(X_valid)[:,1]})\noutput_data['binary'] = np.floor(output_data['prediction'] + 0.5)     # convert probabilistic predictions to 0/1 binary predictions\noutput_data.head(5)\noutput_data.to_pickle(\"./output.pkl\", compression='infer', storage_options=None)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T04:43:33.857293Z","iopub.execute_input":"2021-12-08T04:43:33.857564Z","iopub.status.idle":"2021-12-08T04:43:39.145552Z","shell.execute_reply.started":"2021-12-08T04:43:33.857533Z","shell.execute_reply":"2021-12-08T04:43:39.144832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Final Model Selection - save data\nWe need to save the preprocessor, plus the models that we are going to consider for the ensemble model, to joblib.","metadata":{}},{"cell_type":"code","source":"gc.collect()\n\n# Logistic Regression\nparams_01 = {'C' : 0.005, 'penalty' : 'l2'}\nfinal_model_01 = LogisticRegression(**params_01, max_iter = 400, solver = 'lbfgs')\nfinal_model_01.fit(X_train, y_train)\njoblib.dump(final_model_01, 'default_model_final_LR.joblib')\n\n# Light GBM\nfinal_model_02 = lgb.LGBMClassifier(**params)\nfinal_model_02.fit(X_train, y_train)\njoblib.dump(final_model_02, 'default_model_final_LGB.joblib')\n\n# Random Forest\nparams_03 = {'max_depth' : 39, 'min_samples_leaf' : 37}\nfinal_model_03 = RandomForestClassifier(**params_03, random_state=1, n_estimators=100)\nfinal_model_03.fit(X_train, y_train)\njoblib.dump(final_model_03, 'default_model_final_RF.joblib')\n\njoblib.dump(preprocessor, 'default_preprocessor_final.joblib') ","metadata":{"execution":{"iopub.status.busy":"2021-12-08T04:43:39.147062Z","iopub.execute_input":"2021-12-08T04:43:39.147872Z","iopub.status.idle":"2021-12-08T04:49:39.897853Z","shell.execute_reply.started":"2021-12-08T04:43:39.147832Z","shell.execute_reply":"2021-12-08T04:49:39.896556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Appendix","metadata":{}},{"cell_type":"markdown","source":"### Feature Importance (Light GBM)\nAC_ratio (basically the term of the loan) is the most important feature, followed by age, length of time since ID changed, then the credit bureau scores. Creating the right engineered features was the key to a good performing model; the top 50 contains many of the features that we created.","metadata":{}},{"cell_type":"code","source":"LGB_model.fit(X_train, y_train)\nfeature_imp = pd.DataFrame(zip(LGB_model.feature_importances_, features), columns=['Value','Feature']).sort_values(by=\"Value\", ascending=False)\n\nplot_data = feature_imp.iloc[0:50,:]\nplt.figure(figsize=[6,10])\nsns.barplot(plot_data['Value'], plot_data['Feature'], orient = \"h\", color = \"lightsteelblue\")\nplt.title(\"Most important features (top 50)\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-08T04:49:39.899636Z","iopub.execute_input":"2021-12-08T04:49:39.9Z","iopub.status.idle":"2021-12-08T04:51:59.101907Z","shell.execute_reply.started":"2021-12-08T04:49:39.899954Z","shell.execute_reply":"2021-12-08T04:51:59.101285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Optuna study results for random forest model","metadata":{}},{"cell_type":"code","source":"import optuna as op\nfrom optuna import create_study\nfrom sklearn import datasets\n\ndef objective(trial):\n    maxD = trial.suggest_int('max_depth', 30, 40)                            # try integers in the range 30-40\n    minSL = trial.suggest_int('min_samples_leaf', 30, 40)\n    model = RandomForestClassifier(random_state=1, n_estimators=100, max_depth = maxD, min_samples_leaf = minSL)\n    model.fit(X_train[0:20000], y_train[0:20000])                            # for our study, limit the training data to 20,000\n    accuracy = roc_auc_score(y_valid, model.predict_proba(X_valid)[:,1])     # score on roc_auc for the validation data\n    return accuracy\n\nstudy = op.create_study(direction = \"maximize\")\nstudy.optimize(objective, n_trials = 50, n_jobs = -1)                        # select how many trials we want. Didn't see any improvement after 20 trials\ntrial = study.best_trial                                                     # ended up being 37 and 30\nprint(\"Best Tuning Parameters : {} \\n with roc_auc_score of : {:.4f} \".format(trial.params,trial.value))","metadata":{"execution":{"iopub.status.busy":"2021-12-08T04:51:59.103459Z","iopub.execute_input":"2021-12-08T04:51:59.103906Z","iopub.status.idle":"2021-12-08T04:56:06.576491Z","shell.execute_reply.started":"2021-12-08T04:51:59.103875Z","shell.execute_reply":"2021-12-08T04:56:06.575358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Optuna study results for LGBM","metadata":{}},{"cell_type":"code","source":"import lightgbm as lgb\nfrom lightgbm import LGBMClassifier\nimport optuna as op\nfrom optuna import create_study\nfrom sklearn import datasets\n\nparams = {'boosting_type': 'gbdt', 'objective': 'binary',                           # choices for boosting type are 'gbdt', 'rf', 'dart', 'goss'     \n          'n_jobs': -1, 'learning_rate': 0.02,                                      # default learning rate is 0.1 - lower numbers are working well\n          'n_estimators' : 1600, 'num_leaves' : 28, 'max_depth' : 18,               # these all seem pretty reasonable\n          'max_bin': 512, 'subsample_for_bin': 200, 'subsample': 0.8,\n          'subsample_freq': 1, 'colsample_bytree': 0.8,                             # 'reg_alpha': 20, 'reg_lambda': 20, \n          'min_split_gain': 0.5, 'min_child_weight': 1,\n          'min_child_samples': 10, 'scale_pos_weight': 11.5, 'num_class' : 1,       # about 92% target=0 to 8% target=1 - ratio is about 11.5 to 1\n          'metric' : 'auc'\n          }\n\ndef objective(trial):\n    l1 = trial.suggest_int('lambda_l1', 0,8)                              # try integers in the range 30-40\n    l2 = trial.suggest_int('lambda_l2', 0,8)\n    model = LGBMClassifier(**params, random_state=1, lambda_l1 = l1 * 5, lambda_l2 = l2 * 5)\n    model.fit(X_train[0:1000], y_train[0:1000])                                              # for our study, limit the training data to 20,000\n    accuracy = roc_auc_score(y_valid, model.predict_proba(X_valid)[:,1])     # score on roc_auc for the validation data\n    return accuracy\n\nstudy = op.create_study(direction = \"maximize\")\nstudy.optimize(objective, n_trials = 20, n_jobs = -1)                        # select how many trials we want. Didn't see any improvement after 20 trials\ntrial = study.best_trial                                                      # looks like good values are ~28 leaves, ~2400 estimators, max depth ~18\nprint(\"Best Tuning Parameters : {} \\n with roc_auc_score of : {:.4f} \".format(trial.params,trial.value))\n","metadata":{"execution":{"iopub.status.busy":"2021-12-08T04:56:06.578189Z","iopub.execute_input":"2021-12-08T04:56:06.578766Z","iopub.status.idle":"2021-12-08T04:57:15.964531Z","shell.execute_reply.started":"2021-12-08T04:56:06.578722Z","shell.execute_reply":"2021-12-08T04:57:15.96382Z"},"trusted":true},"execution_count":null,"outputs":[]}]}