{"cells":[{"metadata":{"_uuid":"e05884be89375f361c7c5024a00d4ed07c949497"},"cell_type":"markdown","source":"## Home Credit Default Risk\n##### Can you predict how capable each applicant is of repaying a loan?\n#### Overview \nThis project was inspired by that fact that many people who deserves loan do not get it and ends up in the hands of untrustworthy lenders.\nThis project is a competition from Kaggle. Below is the link: [Kaggle | Home Credit Default Risk Competition](https://www.kaggle.com/c/home-credit-default-risk)\n\n\nMany people struggle to get loans due to insufficient or non-existent credit histories. And, unfortunately, this population is often taken advantage of by untrustworthy lenders.\n\n![about homecredit](https://storage.googleapis.com/kaggle-media/competitions/home-credit/about-us-home-credit.jpg)       [Source : Kaggle](https://storage.googleapis.com/kaggle-media/competitions/home-credit/about-us-home-credit.jpg)\n\nHome Credit strives to broaden financial inclusion for the unbanked population by providing a positive and safe borrowing experience. In order to make sure this underserved population has a positive loan experience, Home Credit makes use of a\n \nvariety of alternative data--including telco and transactional information--to predict their clients' repayment abilities.\nWhile Home Credit is currently using various statistical and machine learning methods to make these predictions, they're challenging Kagglers to help them unlock the full potential of their data. Doing so will ensure that clients capable of repayment are not rejected and that loans are given with a principal, maturity, and repayment calendar that will empower their clients to be successful.\n"},{"metadata":{"_uuid":"dd2428d19fd71f1d5a27920bb33e4b71849cce67"},"cell_type":"markdown","source":"## Problem Statement.\n### Can you predict how capable each applicant is of repaying a loan ?\n- My analysis will be predicting how capable each applicant is at repaying a loan."},{"metadata":{"_uuid":"9b6ded79f21acb6b07c02c6a5fc9399c6f16bd1e"},"cell_type":"markdown","source":"### Datasets and Inputs.\nThe dataset for this project has been provided by Kaggle. <br>\nData description is below :\nThere are 7 different sources of data:\n+ **application_train/application_test**: the main training and testing data with information about each loan application at Home Credit. Every loan has its own row and is identified by the feature SK_ID_CURR. The training application data comes with the TARGET indicating **0** if the loan was repaid **`Repayers`** and **`1`** for default **`Defaulters`**\n+ **bureau**: data concerning client's previous credits from other financial institutions. Each previous credit has its own row in bureau, but one loan in the application data can have multiple previous credits.\n+ **bureau_balance**: monthly data about the previous credits in bureau. Each row is one month of a previous credit, and a single previous credit can have multiple rows, one for each month of the credit length.\n+ **previous_application**: previous applications for loans at Home Credit of clients who have loans in the application data. Each current loan in the application data can have multiple previous loans. Each previous application has one row and is identified by the feature SK_ID_PREV.\n+ **POS_CASH_BALANCE:** monthly data about previous point of sale or cash loans clients have had with Home Credit. Each row is one month of a previous point of sale or cash loan, and a single previous loan can have many rows.\n+ **credit_card_balance**: monthly data about previous credit cards clients have had with Home Credit. Each row is one month of a credit card balance, and a single credit card can have many rows.\n+ **installments_payment**: payment history for previous loans at Home Credit. There is one row for every made payment and one row for every missed payment.\nFor more information on what each data represents, please read the [PROPOSAL]('/Users/bhetey/version_control/machine-learning/projects/capstone/proposal.pdf'), or [Kaggle](https://www.kaggle.com/c/home-credit-default-risk) <br>\n- Below is a diagram of how the data are connected. \n![Data structure](https://storage.googleapis.com/kaggle-media/competitions/home-credit/home_credit.png)"},{"metadata":{"_uuid":"a57acc2fdc22226c3ca4341bff72130657f6809a"},"cell_type":"markdown","source":"### Loading Data "},{"metadata":{"trusted":true,"_uuid":"837b914aa817a0401b0a2b661925cab3626b2548","collapsed":true},"cell_type":"code","source":"from __future__ import division\nimport pandas as pd # this is to import the pandas module\nimport numpy as np # importing the numpy module \nimport os # file system management \nimport zipfile # module to read ZIP archive files.\nfrom glob import glob \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Figures inline and set visualization style\n%matplotlib inline\nsns.set()\n\n#print(os.listdir(\"../input/*.csv\"))\nfilenames = glob(\"../input/*.csv\")\nfilenames","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5808cd19933f1a304b3f20340f7a3e79d70c842d"},"cell_type":"code","source":"# reading the data with pandas \ndef reading_csv_file(filename):\n    return pd.read_csv(filename)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"67d7d8d3c30c9f84d4063eeae1bb02c68a2a993e","collapsed":true},"cell_type":"code","source":"app_train = reading_csv_file(filenames[2])\nprint ('Training data shape :{}'.format(app_train.shape))\napp_train.head()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6fb6ac3673d8644fa8c6d110df39ff6ff5fddf75"},"cell_type":"markdown","source":"Training data has 307511 rows (*each one represents separate loan*) and 112 featurees (columns) including the TARGET(What is to be predicted)"},{"metadata":{"trusted":true,"_uuid":"91a2d5a02aded7698377edb77486089a9f72ca66","collapsed":true},"cell_type":"code","source":"y = app_train.TARGET # y is going to be our target variable\ny.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6d8956410d4fda424f68c9d170d1e30831030771","collapsed":true},"cell_type":"code","source":"app_test = reading_csv_file(filenames[7])\nprint ('Testing test contains :{}'.format(app_test.shape))\napp_test.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"07826aab8bbe40869e6e8f420ad564b4c876b3cf"},"cell_type":"markdown","source":"The Testing set does not have target variable "},{"metadata":{"trusted":true,"_uuid":"e69511e20731dcabc9fd406de952f273e7579a82","collapsed":true},"cell_type":"code","source":"# traing and testing set do not have the same shape. \napp_train.shape == app_test.shape ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"55a2b6869444d7876be4b0314ce264de7ab728da"},"cell_type":"markdown","source":"### Data conversion in pandas DataFrame"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"2035c260f2e3631d1f6189837a45f14f2e1d423b"},"cell_type":"code","source":"# converted all csv in pandas dataframe. \npos_cash_balance = reading_csv_file(filenames[0])\nbureau_balance = reading_csv_file(filenames[1])\nprevious_application = reading_csv_file(filenames[3])\ninstallments_payments = reading_csv_file(filenames[4])\ncredit_card_balance = reading_csv_file(filenames[5])\nbureau = reading_csv_file(filenames[8])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3320e03a20afed238e27df5867baf72ee59ed8f1"},"cell_type":"markdown","source":"HomeCredit Columns Description gives us the details about each features in the dataset "},{"metadata":{"trusted":true,"_uuid":"070d58e7c43a8a3495f48e6d82e093387d6efa6c","collapsed":true},"cell_type":"code","source":"# this is done for indexing of the joint data later \ndata = (os.listdir(\"../input/\"))\ndata.remove('sample_submission.csv')\ndata","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fbb85ef1c90b0506b63272294a787000b74365d3"},"cell_type":"markdown","source":"### Visual Exploratory Data Analysis (EDA)"},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"d4e34f0d0f61784cf7f59fbd4863690823d82f51","collapsed":true},"cell_type":"code","source":"print (app_train['TARGET'].value_counts())\napp_train.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f54d180f16a79e8d10d8c2c1ea333182f35f1d31"},"cell_type":"markdown","source":"+ ### How many people repay loans : \n**Take away here is that**: Looking at the picture below, **`1`** for **Defaulter** and **`0`** for  **Repayers**. The image below shows that most applicant pay back the loan. This is what we called [Imbalanced Class Problem](http://www.chioka.in/class-imbalance-problem/). The differences between Repayer and Defaulter is too big "},{"metadata":{"trusted":true,"_uuid":"aeca524eae09e271ce230f38b2a2e1630aa84b83","collapsed":true},"cell_type":"code","source":"sns.countplot(x='TARGET',hue='TARGET', data=app_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cd672153f9b0fab6f5ec5e115330b1289f6316cd"},"cell_type":"markdown","source":"+ ### What is the family status of the applicant:\nIn the image, it is shown that more married candidates pay back thier loans"},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"08ce387f86860c87df30bac2f328ae62441eb68d","collapsed":true},"cell_type":"code","source":"plt.figure(figsize=(10,3))\nsns.countplot(x = \"NAME_FAMILY_STATUS\", hue = \"TARGET\", data = app_train)\nfam_stat_target = pd.crosstab(app_train['NAME_FAMILY_STATUS'], app_train['TARGET'])\nkind_of_applicant = (\"Repayers\", \"Defaulters\")\nfam_stat_target.columns = kind_of_applicant\nfam_stat_target","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dde0bf5479ec7f36d257ea020a36e3d05a4a35e6"},"cell_type":"markdown","source":"+ ### What is the Income Class and Family type that default the most :   \n**The Takeaway:** Most married and working class mostly default on loan payment "},{"metadata":{"trusted":true,"_uuid":"2a2985f317c0433e30d15404a91c5fda63aba704","collapsed":true},"cell_type":"code","source":"plt.figure(figsize= (10,5))# plot the figure \nplt.show(sns.countplot(x = \"NAME_FAMILY_STATUS\", \n                       hue = \"NAME_INCOME_TYPE\" , \n                       # filter the train set by using TARGET column == 1\n                       data= app_train.loc[app_train['TARGET'] == 1])) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b74071363bf9deda571179c2bd96ceab44ee7a16"},"cell_type":"markdown","source":"Checking for missing values."},{"metadata":{"trusted":true,"_uuid":"d6cd762e2580df05191a2797ffcc581cc65c2313","collapsed":true},"cell_type":"code","source":"# below is a function to check for missing values. \ndef check_missing_values(input):\n    # checking total missing values\n    total_miss_values = input.isnull().sum()\n    \n    # percentage of missing values. \n    miss_val_percent = total_miss_values/len(input)*100\n    \n    # table of total_miss_values and it's percentage\n    miss_val_percent_tab = pd.concat([total_miss_values, miss_val_percent], axis=1)\n    \n    # columns renamed\n    new_col_names = ('Missing values', 'Total missing values in %')\n    miss_val_percent_tab.columns = new_col_names\n    renamed_miss_val_percent_tab = miss_val_percent_tab\n    \n    # descending table sort \n    renamed_miss_val_percent_tab = renamed_miss_val_percent_tab[\n        renamed_miss_val_percent_tab.iloc[:,1] != 0\n    ].sort_values('Total missing values in %', ascending = False).round(1)\n    \n    # display information \n    print ('The selected dataframe has {} columns.\\n'.format(input.shape[1]))\n    print ('There are {} columns missing in the dataset'.format(renamed_miss_val_percent_tab.shape[0]))\n    \n    return renamed_miss_val_percent_tab","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dbd15ac1c231f78e2e197021dc75bc6793a42903","collapsed":true},"cell_type":"code","source":"missing_value = check_missing_values(app_train)\nmissing_value.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"627dc955720eadf6dd2c169a9cb89c276bb80702"},"cell_type":"markdown","source":"**Take away :** For some machine learning models, we have to deal with the missing values buy imputing or dropping either the roles or the columns with the highest percentage of missing values. However we might be loosing some data from them. We also do not know if the data removed will harm the analysis or help it ahead of time until we experiment on them. \nAlgorithmns like **XGBoost** can handle missing data without imputation. [It automatically learn how to deal with missing data point.](https://machinelearningmastery.com/data-preparation-gradient-boosting-xgboost-python/)\n+ [Additonal reading](https://arxiv.org/abs/1603.02754)"},{"metadata":{"_uuid":"755fab56cec27ccb1eb8ac3e1bf02e56e7ea15cf"},"cell_type":"markdown","source":"Dealing with features "},{"metadata":{"_uuid":"7fe50876c0e909f47cd14174920f9c23cab70ba4"},"cell_type":"markdown","source":"**Obviously we have 3 data types :** Numeric and Non-numeric (e.g Text ) called _object_.\n\nNumeric can be of discrete time or continuous time horizon. \nNon_numeric are [variables containing label values rather numeric values.](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/) They are sometimes called [nominal](https://en.wikipedia.org/wiki/Nominal_category)"},{"metadata":{"trusted":true,"_uuid":"a1c51a3c320107feb34736936a76546270a5452e","collapsed":true},"cell_type":"code","source":"app_train.get_dtype_counts() # Shows the numbers of types of values ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0fe17666789cec0edaad19359c93cbfe0550aef6"},"cell_type":"markdown","source":"Looking at the dataset with object type, below is the total number of object. However since we want to work with them we will need to hot encode them. \n\nHowever this depends on personal view. it depend on how big the categorical variables are. \n\nOne of the major problems with categorical data is that only few machine learning alogorithms works with them without any special form of implementation while others needs some implementation where the data needs to be encoded into numeric variables. \n\nHow to convert categorical data into numerical data: \n+ **Integer Encoding** _where integer values have a natural ordered relationship between each other and machine learning algorithms may be able to understand and harness this relationship._ \n\n\n+ **One-Hot Encoding** _where the integer encoded variable is removed and a new binary variable is added for each unique integer value._\n\n[Read more](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/)"},{"metadata":{"trusted":true,"_uuid":"9cb1e3838342a9a0043ae3011ab6fb7e89860c61","collapsed":true},"cell_type":"code","source":"print ('Total numbe of object type is : ', len(app_train.select_dtypes('object')))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e496a062d5e299ed0880f1384802f4e8f1821e19"},"cell_type":"markdown","source":"Checking the number of unique class in each object column "},{"metadata":{"trusted":true,"_uuid":"dcadee2ec9f49440284a9479dd17be71b19c99c8","collapsed":true},"cell_type":"code","source":"app_train.select_dtypes('object').apply(pd.Series.nunique, axis = 0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"00cc6f7b5be2f5d19724bf08da7ecc4f035461c8"},"cell_type":"markdown","source":"**.Describe** enerates descriptive statistics that summarize the central tendency, dispersion and shape of a datasetâ€™s distribution, excluding NaN values.\n\nDAYS_BIRTH was originally in days and now it will be converted to years. The columns has negative as they were recorded relative to the current loan application "},{"metadata":{"trusted":true,"_uuid":"7fb468d88b8e330a150374ece4e270e7cb5b95b6","collapsed":true},"cell_type":"code","source":"(app_train['DAYS_BIRTH']/-365).describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cc30ce338b4b90d9996a8788a29efc577422f167"},"cell_type":"markdown","source":"Looking at the result above everything seems okay. Cannot seems to find any outlier in this analysis "},{"metadata":{"_uuid":"ae3f060cea67a4ef02d1c5b1d656f40672a07377"},"cell_type":"markdown","source":"**DAYS_EMPOYED:** How many days before the application the person started current employment'\n\nThis is also relative to the current loan application "},{"metadata":{"trusted":true,"_uuid":"ea76ed38f8237fdbd0bc36d6a6ff08d03907428e","collapsed":true},"cell_type":"code","source":"(app_train['DAYS_EMPLOYED']/365).describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2947538d219dd736cb8156588e9138b88884bfd0","collapsed":true},"cell_type":"code","source":"(app_train['DAYS_EMPLOYED']/365).plot.hist(title = 'DAYS EMPLOYED');\nplt.xlabel('NO OF DAYS EMPLOYED BEFORE APPLICATION')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e578649937d56e3dfe9915b09e63b18e5fb65b51"},"cell_type":"markdown","source":"Looking at the image above, 1000 years does not seem right. \nWe will use imputation to solve this. "},{"metadata":{"_uuid":"4ef6e58f3b2b4b46bf6b120d6edc0f614d45d417"},"cell_type":"markdown","source":"### Checking correlation of the data.\nIt helps to show possible relationship within our data. \nThis article helps in interpreting [correlation](http://www.statstutor.ac.uk/resources/uploaded/pearsons.pdf), [How to interpret a Correlation Coefficient](https://www.dummies.com/education/math/statistics/how-to-interpret-a-correlation-coefficient-r/)"},{"metadata":{"trusted":true,"_uuid":"70b496129c32a0088425a943620dfc84093ee8d6","collapsed":true},"cell_type":"code","source":"data_corr = app_train.corr()['TARGET'].sort_values()\nprint ('These are samples of negative correlations : \\n',data_corr.head(20))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a5ea4e7e8155158c263bb9c4e41549f342b925ab","collapsed":true},"cell_type":"code","source":"print ('These are samples of positive correlations : \\n', data_corr.tail(20))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8dc058d2589b857c3344dfc1accbe8c9e49b5a04"},"cell_type":"markdown","source":"#### ONE-HOT ENCODING \nLet's **One-hot Encode** the categorical variable <br>\nWe need to import the module from scikit-learn library"},{"metadata":{"trusted":true,"_uuid":"6f6d49511da1e1be0e9c371d98b5a3ad178a7b64","collapsed":true},"cell_type":"code","source":"app_train.select_dtypes('object').columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"99dfa50850526df060a0151b799eba61117398a9","collapsed":true},"cell_type":"code","source":"len (app_train.columns) == len(app_train.select_dtypes('object').columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f7b88167e09fcfcb1fbf78a9d6e9cb61fb7715c1","collapsed":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\none_hot_encoded_app_train = pd.get_dummies(app_train)\none_hot_encoded_app_test = pd.get_dummies(app_test)\n\nprint ('Shape of the training set after one hot encoding {}'.format(one_hot_encoded_app_train.shape))\nprint ('Shape of the test set after one hot encoding {}'.format(one_hot_encoded_app_test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f67898e2882a7f0466392651e5de6ed4ca73b232","collapsed":true},"cell_type":"code","source":"app_train.shape == one_hot_encoded_app_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4ee120e41d529c30f6a388f3ca9bbf7f233644cc","collapsed":true},"cell_type":"code","source":"app_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1b13ab4f8e152b238d9bdffe0ece4a7d9ecf7fde","collapsed":true},"cell_type":"code","source":"app_test.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7bb1f1e0e0bef7744326b227b31d93083774503f"},"cell_type":"markdown","source":"Looking at the analysis above is obvious that **One-Hot Encoding** has added extra features to the original ones we have hereby leaving our data unaligned. \n\nWe need to have same features in both the training and testing data for our machine learning model to work if not we will get error when running the algorithm. \n\n**STEPS TAKING:** \n+ I decided to remove any column that is present on the training set but not on our testing set. \n+ Intuitively the **y** which is our **TARGET** is expected to  be removed as well but will add it back"},{"metadata":{"trusted":true,"_uuid":"8af82929386d2bfdb6d77e85655b97af4e7d4529","collapsed":true},"cell_type":"code","source":"#https://pandas.pydata.org/pandas-docs/version/0.21/generated/pandas.DataFrame.align.html\none_hot_encoded_app_train, one_hot_encoded_app_test = one_hot_encoded_app_train.align(one_hot_encoded_app_test,\n                                                                                      join='inner', axis=1)\n\nprint ('Shape of the training set after alignment {}'.format(one_hot_encoded_app_train.shape))\nprint ('Shape of the test set after alignment {}'.format(one_hot_encoded_app_test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"68a41e87373d64f4b237e74fda172537f3691554"},"cell_type":"code","source":"one_hot_encoded_app_train['TARGET'] = y # adding it back to the data ","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"collapsed":true,"_uuid":"45c0779e05b57c2201fb4fb31d512ac3003f441f"},"cell_type":"code","source":"# dropping the target to get our X \nX = one_hot_encoded_app_train.drop(['TARGET'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f1ac260b91df2c0af5c574deb348a899b790536e"},"cell_type":"markdown","source":"**Removing missing values**"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a4a296716ecb79c1851d581247e9d82ceaa7051a"},"cell_type":"code","source":"# function is to drop the missing values \ndef dropping_missing_columns(input_set):\n    \"\"\"this function removes the columns with missing values.\n    However input_set is the set you will put inside in the function \n    either the training set or the test set \n    \"\"\"\n    to_drop_missing_missing_values = [\n        col for col in input_set.columns if X[col].isnull().any()\n    ]\n    return input_set.drop(to_drop_missing_missing_values, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"fa4b15c562ebaa39c6ce349a3a19cd96162988c3"},"cell_type":"code","source":"#assigned a variable to data after dropping the missing values \nafter_removing_missing_values = dropping_missing_columns(X)\ntest_removing_missing_values = dropping_missing_columns(one_hot_encoded_app_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dc3f6622b20655efbaa2b9703e2b56ae43998fac","collapsed":true},"cell_type":"code","source":"print ('The shape of training set after removing missing values : {}'.format(after_removing_missing_values.shape))\nprint ('The shape of testing set after removing missing values :{}'.format(test_removing_missing_values.shape))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9b9fa69cc487f2192c5c3a5b7473e47c546e129b"},"cell_type":"markdown","source":"Looking at the dataset now after removing the **NaN** in the data, we have the columns reduced to **181 columns**"},{"metadata":{"_uuid":"e230d3b7481a1c1109297725d6fb7cbf238ff082"},"cell_type":"markdown","source":"**Imputing Nan values**"},{"metadata":{"trusted":true,"_uuid":"aef79ad8706369c7e2f6e97d6c2620e1e3219345","collapsed":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler, Imputer\n\n# making a copy for the data before imputing \ntrain_tobe_imputed = one_hot_encoded_app_train.copy() \ntest_tobe_imputed = one_hot_encoded_app_test.copy()\nnew_y = y.copy() # a copy of our target \n\n# dropping the target columns before imputing \nnew_X = train_tobe_imputed.drop(['TARGET'], axis=1)\n\n# calling imputer and transforming the data\nimputer = Imputer()\ntransformed_X = imputer.fit_transform(new_X)\ntransformed_test_X = imputer.fit_transform(test_tobe_imputed)\n\nprint ('Transformed training set :{}'.format(transformed_X.shape))\nprint ('Transformed testing set :{}'.format(transformed_test_X.shape))\nprint ('The data is back to the same shape we had during the Hot coding')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4f75113a4ca2fcaec9c1acaf41eeb810caadac0d"},"cell_type":"markdown","source":"## EVALUATION METRICS "},{"metadata":{"_uuid":"0f156cb5e6001288983f5bf7428922daee11261a"},"cell_type":"markdown","source":"There are different kind of evaluation metrics we can used since this is a **Classification problem.** <br>\nBelow are some of the metrics : \n\n+ Classification accuracy \n+ Logarithmic Loss \n+ Area Under ROC Curve \n+ Confusion Matrix \n+ Classification report\n\n[**Read more about them**](https://machinelearningmastery.com/metrics-evaluate-machine-learning-algorithms-python/)"},{"metadata":{"_uuid":"051374b620cb084f65efa37156de2cd823e4db7c"},"cell_type":"markdown","source":"### CLASSIFICATION MODELS \nClassification depends on whether the variables we are trying to predict are **Binary or Non-Binary**.\n\n**Binary variables** are those variables where the outcome we are looking are either 1 or 0, True or False. \n\n**Non-Binary variables** are those variables where the outcome we are looking are categorical. for example looking at the dataset and predicting where the color of the dress of a person will be `Yellow, Brown or Blue`\n\n#### Binary Classification Model : \n+ Logistic regression \n+ Decision Trees \n+ Support Vector Machine (SVM) : _good for anomaly detection especially in large feature sets_\n\n#### Non- Binary Classification Model: \n+ Adaboost \n+ Random Forest \n+ Decision Tree\n+ Neural Networks \n\n#### Considering choosing an algorithm, : \n+ Take note of the accuracy \n+ Training time \n+ Linearity \n+ Number of parameters \n+ Number of features \n\n[The Machine Learning Algorithm Cheat Sheet](https://docs.microsoft.com/en-us/azure/machine-learning/studio/algorithm-choice)\n\nHowever, I am going to try each of them and compare their performance."},{"metadata":{"_uuid":"74d2c33b90a74ecbbb356182b712253914f86cda"},"cell_type":"markdown","source":"### Logistic Regression Model \nThis is my first model. <br>\nC is used to control overfitting and a small tends to reduce overfitting "},{"metadata":{"_uuid":"100125aa4f03704b01dd0d336acb4ff0e11b2066"},"cell_type":"markdown","source":"Logistic regression using the data set with dropped missing values. \n#### Model Evaluation using a validation set"},{"metadata":{"trusted":true,"_uuid":"b39acddc50eb79eea93b218cd429818e0c307268","collapsed":true},"cell_type":"code","source":"# Import statements \nfrom sklearn.metrics import classification_report\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import r2_score\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn import metrics\n\nX_train, X_test, y_train, y_test = train_test_split(\n    after_removing_missing_values, y, test_size=0.25, random_state=42)\n\n# instantiate a logistic regression model, and fit with X and y\nmodel = LogisticRegression(C = 0.0001)\n\n# evaluate the model by splitting into train and test sets\nmodel.fit(X_train, y_train)\nprint ('This accuracy seems good {} but need to check further for the prediction and on testing set'.format(model.score(X_train, y_train)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"246a889c183f9edc151a2636abdc4fed6abb0f38","collapsed":true},"cell_type":"code","source":"predicted = model.predict(X_test)\npredicting_with_test = model.predict(test_removing_missing_values)\npredicting_probability = model.predict_proba(X_test)\nmatrix = confusion_matrix(y_test, predicted)\nscoring = accuracy_score(y_test, predicted)\nr_score = r2_score(y_test, predicted)\nreport = classification_report(y_test, predicted)\nprint (report)\nprint (matrix)\nprint ('Accuracy of the model :{}'.format(scoring))\nprint ('R2 Score for the prediction :'.format(r_score))\nprint(metrics.roc_auc_score(y_test, predicting_probability[:, 1]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ba5cb050c5f629877a893dfe2b31d9b0b8978f80"},"cell_type":"markdown","source":"#### Model Evaluation Using Cross-Validation\nNow let's try 10-fold cross-validation, to see if the accuracy holds up more rigorously."},{"metadata":{"trusted":true,"_uuid":"5f790265b501bb2663dc9620678ca80706185568","collapsed":true},"cell_type":"code","source":"# evaluate the model using 10-fold cross-validation\nfrom sklearn.cross_validation import cross_val_score\nscores = cross_val_score(LogisticRegression(), \n                         after_removing_missing_values, y, scoring='accuracy', cv=10)\nprint (scores)\nprint (scores.mean())\nprint (predicting_with_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"fbbbd23eb242991621935952ddc24a7b65b61522"},"cell_type":"code","source":"my_submission = pd.DataFrame({'SK_ID_CURR': one_hot_encoded_app_test.SK_ID_CURR, 'TARGET': predicting_with_test})\nmy_submission.to_csv('homecredit.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a87fd398fe5c10b6ac0f7571bc1fc456346f9659","collapsed":true},"cell_type":"code","source":"my_submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a69fe7e16a4f455feaf103e90b9f91c7bb03ab6d"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}