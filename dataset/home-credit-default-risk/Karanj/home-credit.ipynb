{"cells":[{"metadata":{"trusted":true,"_uuid":"c38854411ef0c3793d836926531610fe2e617ebb","collapsed":true},"cell_type":"code","source":"import pandas as pd # package for high-performance, easy-to-use data structures and data analysis\nimport numpy as np # fundamental package for scientific computing with Python\nimport matplotlib\nimport matplotlib.pyplot as plt # for plotting\nimport seaborn as sns # for making plots with seaborn\ncolor = sns.color_palette()\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.offline as offline\noffline.init_notebook_mode()\n\n# sklearn preprocessing for dealing with categorical variables\nfrom sklearn.preprocessing import LabelEncoder\n\n# from plotly import tools\n# import plotly.tools as tls\n# import squarify\n# from mpl_toolkits.basemap import Basemap\n# from numpy import array\n# from matplotlib import cm\n\n# import cufflinks and offline mode\nimport cufflinks as cf\ncf.go_offline()\n\n# from sklearn import preprocessing\n# # Supress unnecessary warnings so that presentation looks clean\n# import warnings\n# warnings.filterwarnings(\"ignore\")\n\n# # Print all rows and columns\n# pd.set_option('display.max_columns', None)\n# pd.set_option('display.max_rows', None)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n\n# This notebook is inspired by : https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d61a820d72dbb696274cd5b10300f056f69a6535"},"cell_type":"code","source":"# Import all data files\napplication_train = pd.read_csv('../input/application_train.csv')\nPOS_CASH_balance = pd.read_csv('../input/POS_CASH_balance.csv')\nbureau_balance = pd.read_csv('../input/bureau_balance.csv')\nprevious_application = pd.read_csv('../input/previous_application.csv')\ninstallments_payments = pd.read_csv('../input/installments_payments.csv')\ncredit_card_balance = pd.read_csv('../input/credit_card_balance.csv')\nbureau = pd.read_csv('../input/bureau.csv')\napplication_test = pd.read_csv('../input/application_test.csv')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"# Check size of all the given data-sets\nprint('Size of application_train data', application_train.shape)\nprint('Size of POS_CASH_balance data', POS_CASH_balance.shape)\nprint('Size of bureau_balance data', bureau_balance.shape)\nprint('Size of previous_application data', previous_application.shape)\nprint('Size of installments_payments data', installments_payments.shape)\nprint('Size of credit_card_balance data', credit_card_balance.shape)\nprint('Size of bureau data', bureau.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"01da0855f3efac79af1ef862497b16d87124aa2c","collapsed":true},"cell_type":"code","source":"application_train['TARGET'].value_counts()\nprint()\n\napplication_train['TARGET'].plot.hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b97886cfe854f77f3d4ae0201f1057703d62c285","collapsed":true},"cell_type":"code","source":"# Function to calculate missing values by column# Funct \ndef missing_values_table(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        #print(mis_val[:10])\n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() / len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        #print(mis_val_table[:20])\n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        #print(mis_val_table_ren_columns[:20])\n        \n        # Sort the table by percentage of missing descending\n        #iloc[:,1]!=0;  elements of 1st colomn(i.e. % of Total Values), which are not zero\n        #sort_values ; sort the elements of df in decending order according to elements of 1st coloumn\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        #print(mis_val_table_ren_columns[:20])\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns\n    \n    \n# Missing values statistics\nmissing_values = missing_values_table(application_train)\nmissing_values.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"392699e82655be5c8926c03237adfe44c2862b10","collapsed":true},"cell_type":"code","source":"# the number of columns of each data type. int64 and float64 are numeric variables (which can be either discrete or continuous). object columns contain strings and are categorical features. .\n# Number of each type of column\n#application_train.dtypes  # will show data types per column\n# https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.value_counts.html\napplication_train.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c1a78c511eea53e84ef1f6b4a81606f1c6d734e9","collapsed":true},"cell_type":"code","source":"# Number of unique classes in each object/feature column\n# https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.nunique.html\napplication_train.select_dtypes('object').apply(pd.Series.nunique, axis = 0)\n# nunique : Return number of unique elements in the object (here, in the column of dtype 'object').\n#application_train.select_dtypes('int64').apply(pd.Series.nunique, axis = 0)['SK_ID_CURR']  #dtype: int64, specific feature/column: SK_ID_CURR","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"90efac598203b7f2ea77d53dc03f2c5eb7243ecc"},"cell_type":"markdown","source":"Label Encoding and One-Hot Encoding\nLet's implement the policy described above: for any categorical variable (dtype == object) with 2 unique categories, we will use label encoding, and for any categorical variable with more than 2 unique categories, we will use one-hot encoding.\n\nFor label encoding, we use the Scikit-Learn LabelEncoder and for one-hot encoding, the pandas get_dummies(df) function."},{"metadata":{"trusted":true,"_uuid":"bf2e4954d0cef170eb7d143afd6a8b507d8365c1","collapsed":true},"cell_type":"code","source":"# Create a label encoder object\nle = LabelEncoder()\nle_count = 0\n\n# Iterate through the columns\nfor col in application_train:\n    if application_train[col].dtype == 'object':\n        #print(col)\n        # If 2 or fewer unique categories\n        if len(list(application_train[col].unique())) <= 2:\n            #print(col)\n            # Train on the training data\n            le.fit(application_train[col])\n            # Transform both training and testing data\n            application_train[col] = le.transform(application_train[col])\n            application_test[col] = le.transform(application_test[col])\n            \n            # Keep track of how many columns were label encoded\n            le_count += 1\n            \nprint('%d columns were label encoded.' % le_count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a07b81cfd80c6d87b6883aeadb78120270c2e67e","collapsed":true},"cell_type":"code","source":"# one-hot encoding of categorical variables\napp_train = pd.get_dummies(application_train)\napp_test = pd.get_dummies(application_test)\n\nprint('Training Features shape: ', app_train.shape)\nprint('Testing Features shape: ', app_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4f4c4dc46ef5f16a90bd8034c66263d64de72582","collapsed":true},"cell_type":"code","source":"# to make same features in training and testing data\n# inner join will only take columns present in both dataframes\n# we should take care of 'TARGET' column in training dataset\n\ntrain_labels = app_train['TARGET']\n\n# Align the training and testing data, keep only columns present in both dataframes\napp_train, app_test = app_train.align(app_test, join = 'inner', axis = 1)\n\n# Add the target back in\napp_train['TARGET'] = train_labels\n\nprint('Training Features shape: ', app_train.shape)\nprint('Testing Features shape: ', app_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ceb46d39253f6c0ede8e002853a3feb6e4c1438d"},"cell_type":"markdown","source":"**Anomalies**\nLookout for anomalies within data. It can be due to mistyped numbers, error in measuring equipmentrs or they can be valid but extreme measurements (super-outliers).\nOne way to support anomalies quantitatively is by looking at the statistics of a column using the describe method."},{"metadata":{"trusted":true,"_uuid":"5b6b5761832f2e5ae3394cdfbfe0bc39e03de5f1","collapsed":true},"cell_type":"code","source":"# The numbers in the DAYS_BIRTH column are negative because they are recorded relative to the current loan application.\n#To see these stats in years, we can mutliple by -1 and divide by the number of days in a year:\n(app_train['DAYS_BIRTH'] / -365).describe()\n\n#Those ages look reasonable. There are no outliers for the age on either the high or low end.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e5eadd31837301f30a742758e6dc2dd2e912d47e","collapsed":true},"cell_type":"code","source":"# lets look for anomalies in days of employment\n(app_train['DAYS_EMPLOYED']/365).describe() # divide by 365 to see numbers in years\n# That doesn't look right! The maximum value (besides being positive) is about 1000 years!","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8f4ef5de930945920c7bd78bfd7f9850ee7f53a7","collapsed":true},"cell_type":"code","source":"# Lets dig into \"DAYS_EMPLOYED\" using histogram\napp_train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram');\nplt.xlabel('Days Employment');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8d81099e52b022c5b7d8680d4ffa78ce12a658f2","collapsed":true},"cell_type":"code","source":"# let's subset the anomalous clients and see if they tend to have higher or low rates of default than the rest of the clients.\nanom = app_train[app_train['DAYS_EMPLOYED'] == 365243]\n#print(anom.head())\nnon_anom = app_train[app_train['DAYS_EMPLOYED'] != 365243]\nprint('The non-anomalies default on %0.2f%% of loans' % (100 * non_anom['TARGET'].mean()))\nprint('The anomalies default on %0.2f%% of loans' % (100 * anom['TARGET'].mean()))\nprint('There are %d anomalous entries in \"DAYS_EMPLOYED\" colomn' % len(anom))\n#print(len(non_anom))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"02c2f36ce4ced8ac9c03cdc27888e35cf17b4f09","collapsed":true},"cell_type":"code","source":"# Handling the anomalies depends on the exact situation, with no set rules.\n# One of the safest approaches is just to set the anomalies to a missing value and then have them filled in (using Imputation) before machine learning.\n\n# Create an anomalous flag column\napp_train['DAYS_EMPLOYED_ANOM'] = app_train[\"DAYS_EMPLOYED\"] == 365243\n#\n# Replace the anomalous values with nan\napp_train['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\n\napp_train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram');\nplt.xlabel('Days Employment')\n\n\n#The distribution looks to be much more in line with what we would expect, \n# and we also have created a new column to tell the model that these values were originally anomalous (becuase we will have to fill in the nans with some value, \n# probably the median of the column). The other columns with DAYS in the dataframe look to be about what we expect with no obvious outliers.\n\n\n# lets look at dataframe in days of employment again removal of anomalous records\n#(app_train['DAYS_EMPLOYED']/365).describe() # divide by 365 to see numbers in years\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e425ff1a9ae15a6797399cd24470ae7b13c40bab","collapsed":true},"cell_type":"code","source":"# As an extremely important note, anything we do to the training data we also have to do to the testing data. \n# Let's make sure to create the new column and fill in the existing column with np.nan in the testing data.\napp_test['DAYS_EMPLOYED_ANOM'] = app_test[\"DAYS_EMPLOYED\"] == 365243\napp_test[\"DAYS_EMPLOYED\"].replace({365243: np.nan}, inplace = True)\n\nprint('There are %d anomalies in the test data out of %d entries' % (app_test[\"DAYS_EMPLOYED_ANOM\"].sum(), len(app_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e4b744046f9b3a849bfa4eb9e723b2c53894ec70"},"cell_type":"markdown","source":"**Correlations**\n\nNow that we have dealt with the categorical variables and the outliers, let's continue with the EDA. One way to try and understand the data is by looking for correlations between the features and the target. We can calculate the Pearson correlation coefficient between every variable and the target using the .corr dataframe method.\n\nThe correlation coefficient is not the greatest method to represent \"relevance\" of a feature, but it does give us an idea of possible relationships within the data. Some general interpretations of the absolute value of the correlation coefficent are:\n\n    .00-.19 “very weak”\n    .20-.39 “weak”\n    .40-.59 “moderate”\n    .60-.79 “strong”\n    .80-1.0 “very strong”\n\n"},{"metadata":{"trusted":true,"_uuid":"3dc5c91c86b4716f1197cadeebd59ca67ac7fd72","collapsed":true},"cell_type":"code","source":"# Find correlations with the target and sort\ncorrelations = app_train.corr()['TARGET'].sort_values()\n\n# Display correlations\nprint('Most Positive Correlations:\\n', correlations.tail(15))\nprint('\\nMost Negative Correlations:\\n', correlations.head(15))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f2eba82605d72da5f9d5fc27f749e40b16b8344f"},"cell_type":"markdown","source":"Let's take a look at some of more significant correlations: the DAYS_BIRTH is the most positive correlation. (except for TARGET because the correlation of a variable with itself is always 1!) Looking at the documentation, DAYS_BIRTH is the age in days of the client at the time of the loan in negative days (for whatever reason!). The correlation is positive, but the value of this feature is actually negative, meaning that as the client gets older, they are less likely to default on their loan (ie the target == 0). That's a little confusing, so we will take the absolute value of the feature and then the correlation will be negative."},{"metadata":{"trusted":true,"_uuid":"1ad84c0268d387bfafb3038287573dac54a180ef","collapsed":true},"cell_type":"code","source":"# Find the correlation of the positive days since birth and target\napp_train['DAYS_BIRTH'] = abs(app_train['DAYS_BIRTH'])\napp_train['DAYS_BIRTH'].corr(app_train['TARGET'])\n\n# As the client gets older, there is a negative linear relationship with the target meaning that as clients get older, they tend to repay their loans on time more often.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0f7dd3534d97ba41830a3693af7f89951c15adcf"},"cell_type":"markdown","source":"To visualize the effect of the age on the target, we will next make a kernel density estimation plot (KDE) colored by the value of the target. A kernel density estimate plot shows the distribution of a single variable and can be thought of as a smoothed histogram (it is created by computing a kernel, usually a Gaussian, at each data point and then averaging all the individual kernels to develop a single smooth curve). We will use the seaborn kdeplot for this graph."},{"metadata":{"trusted":true,"_uuid":"0ed2f18638fca8b25dd2ec46c1444a44cb345b09","collapsed":true},"cell_type":"code","source":"plt.figure(figsize = (10, 8))\n\n# KDE plot of loans that were repaid on time  # https://stackoverflow.com/questions/31593201/pandas-iloc-vs-ix-vs-loc-explanation-how-are-they-different\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 0, 'DAYS_BIRTH'] / 365, label = 'target == 0')\n\n# KDE plot of loans which were not repaid on time  #loc[rows, columns] = loc[app_train['TARGET'] == 1, 'DAYS_BIRTH']\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 1, 'DAYS_BIRTH'] / 365, label = 'target == 1')\n\n# Labeling of plot\nplt.xlabel('Age (years)'); plt.ylabel('Density'); plt.title('Distribution of Ages');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c924d6e2661ca43058975365affee206a7ddce83"},"cell_type":"markdown","source":"The target == 1 curve skews towards the younger end of the range. Although this is not a significant correlation (-0.07 correlation coefficient), this variable is likely going to be useful in a machine learning model because it does affect the target. \n\nLet's look at this relationship in another way: average failure to repay loans by age bracket.\nTo make this graph, first we cut the age category into bins of 5 years each. Then, for each bin, we calculate the average value of the target, which tells us the ratio of loans that were not repaid in each age category.\n"},{"metadata":{"trusted":true,"_uuid":"8f7a7fa4ccb6fb7cbb39503223056e1d7aefeaf5","collapsed":true},"cell_type":"code","source":"# Age information into a separate dataframe\nage_data = app_train.loc[:,('TARGET', 'DAYS_BIRTH')]\n#age_data = app_train[['TARGET', 'DAYS_BIRTH']]  # this is slower option then above line #http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\nage_data['YEARS_BIRTH'] = age_data['DAYS_BIRTH'] / 365\n\n# Bin the age data\nage_data['YEARS_BINNED'] = pd.cut(age_data['YEARS_BIRTH'], bins = np.linspace(20, 70, num = 11)) #dividing range 20-70 into 10 bins of 5 years each\nage_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e865e77616783c32a6d35e385bd4f61e96026dd5","collapsed":true},"cell_type":"code","source":"# Group by the bin and calculate averages\nage_groups  = age_data.groupby('YEARS_BINNED').mean()\nage_groups","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7368a3a6d9a67aaefa80ccf236b539fab6170646","collapsed":true},"cell_type":"code","source":"plt.figure(figsize = (8, 8))\n\n# Graph the age bins and the average of the target as a bar plot\nplt.bar(age_groups.index.astype(str), 100 * age_groups['TARGET'])\n\n# Plot labeling\nplt.xticks(rotation = 75); plt.xlabel('Age Group (years)'); plt.ylabel('Failure to Repay (%)')\nplt.title('Failure to Repay by Age Group');\n\n# There is a clear trend: younger applicants are more likely to not repay the loan!\n#The rate of failure to repay is above 10% for the youngest three age groups and beolow 5% for the oldest age group.\n\n#This is information that could be directly used by the bank: because younger clients are less likely to repay the loan.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d312b9d7b9e157c929e80a8b027d2ed0d8686986"},"cell_type":"markdown","source":"**Exterior Sources**\n\nThe 3 variables with the strongest negative correlations with the target are EXT_SOURCE_1, EXT_SOURCE_2, and EXT_SOURCE_3. According to the documentation, these features represent a \"normalized score from external data source\". I'm not sure what this exactly means, but it may be a cumulative sort of credit rating made using numerous sources of data.\n\nLet's take a look at these variables.\n\nFirst, we can show the correlations of the EXT_SOURCE features with the target and with each other.\n"},{"metadata":{"trusted":true,"_uuid":"3e696d7d9f86144c1f9a2caa30c26a87a71a3594","collapsed":true},"cell_type":"code","source":"# Extract the EXT_SOURCE variables and show correlations\next_data = app_train[['TARGET', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\next_data_corrs = ext_data.corr()\next_data_corrs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"59787c28e95ba7a0ae685d2f006fc332ec108672","collapsed":true},"cell_type":"code","source":"plt.figure(figsize = (8, 6))\n\n# Heatmap of correlations\nsns.heatmap(ext_data_corrs, cmap = plt.cm.RdYlBu_r, vmin = -0.25, annot = True, vmax = 0.6)\nplt.title('Correlation Heatmap');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0dddfe3a85599999a4a9804bc87aed90d550ffc1"},"cell_type":"markdown","source":"All three EXT_SOURCE featureshave negative correlations with the target, indicating that as the value of the EXT_SOURCE increases, the client is more likely to repay the loan. We can also see that DAYS_BIRTH is positively correlated with EXT_SOURCE_1 indicating that maybe one of the factors in this score is the client age.\n\nNext we can look at the distribution of each of these features colored by the value of the target. This will let us visualize the effect of this variable on the target.\n"},{"metadata":{"trusted":true,"_uuid":"06221b4da750aee7ddf123a6daced5e3e9c4800e","collapsed":true},"cell_type":"code","source":"plt.figure(figsize = (10, 12))\n\n# iterate through the sources\nfor i, source in enumerate(['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']):\n    \n    # create a new subplot for each source\n    plt.subplot(3, 1, i + 1)\n    # plot repaid loans\n    sns.kdeplot(app_train.loc[app_train['TARGET'] == 0, source], label = 'target == 0')\n    # plot loans that were not repaid\n    sns.kdeplot(app_train.loc[app_train['TARGET'] == 1, source], label = 'target == 1')\n    \n    # Label the plots\n    plt.title('Distribution of %s by Target Value' % source)\n    plt.xlabel('%s' % source); plt.ylabel('Density');\n    \nplt.tight_layout(h_pad = 2.5)\n\n# EXT_SOURCE_3 displays the greatest difference between the values of the target.\n# We can clearly see that this feature has some relationship to the likelihood of an applicant to repay a loan.\n# The relationship is not very strong (in fact they are all considered very weak, but these variables will still be useful\n# for a machine learning model to predict whether or not an applicant will repay a loan on time.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2b883e972328cc7d8814f6f1c9f5976dd46827f5"},"cell_type":"markdown","source":"**Pairs Plot**\n\nAs a final exploratory plot, we can make a pairs plot of the EXT_SOURCE variables and the DAYS_BIRTH variable. The [Pairs Plot](https://towardsdatascience.com/visualizing-data-with-pair-plots-in-python-f228cf529166) is a great exploration tool because it lets us see relationships between multiple pairs of variables as well as distributions of single variables. Here we are using the seaborn visualization library and the PairGrid function to create a Pairs Plot with scatterplots on the upper triangle, histograms on the diagonal, and 2D kernel density plots and correlation coefficients on the lower triangle.\n\nIf you don't understand this code, that's all right! Plotting in Python can be overly complex, and for anything beyond the simplest graphs, I usually find an existing implementation and adapt the code (don't repeat yourself)!\n"},{"metadata":{"trusted":true,"_uuid":"4c98ecafd4f13f78164c3f136b841f078937fe72","collapsed":true},"cell_type":"code","source":"# Copy the data for plotting\nplot_data = ext_data.drop(columns = ['DAYS_BIRTH']).copy()\n\n# Add in the age of the client in years\nplot_data['YEARS_BIRTH'] = age_data['YEARS_BIRTH']\n\n# Drop na values and limit to first 100000 rows\nplot_data = plot_data.dropna().loc[:100000, :]\n\n# Function to calculate correlation coefficient between two columns\ndef corr_func(x, y, **kwargs):\n    r = np.corrcoef(x, y)[0][1]\n    ax = plt.gca()\n    ax.annotate(\"r = {:.2f}\".format(r),\n                xy=(.2, .8), xycoords=ax.transAxes,\n                size = 20)\n\n# Create the pairgrid object\ngrid = sns.PairGrid(data = plot_data, size = 3, diag_sharey=False,\n                    hue = 'TARGET', \n                    vars = [x for x in list(plot_data.columns) if x != 'TARGET'])\n\n# Upper is a scatter plot\ngrid.map_upper(plt.scatter, alpha = 0.2)\n\n# Diagonal is a histogram\ngrid.map_diag(sns.kdeplot)\n\n# Bottom is density plot\ngrid.map_lower(sns.kdeplot, cmap = plt.cm.OrRd_r);\n\nplt.suptitle('Ext Source and Age Features Pairs Plot', size = 32, y = 1.05);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"97d77cebd5a9cff106afe38e9c6c82798d7116a7"},"cell_type":"markdown","source":"**Feature Engineering**\n\nThis represents one of the patterns in machine learning: feature engineering has a greater return on investment than model building and hyperparameter tuning. This is a great article on the subject). As Andrew Ng is fond of saying: \"applied machine learning is basically feature engineering.\"\n\nWhile choosing the right model and optimal settings are important, the model can only learn from the data it is given. Making sure this data is as relevant to the task as possible is the job of the data scientist (and maybe some automated tools to help us out).\n\nFeature engineering refers to a geneal process and can involve both feature construction: adding new features from the existing data, and feature selection: choosing only the most important features or other methods of dimensionality reduction. There are many techniques we can use to both create features and select features.\n\nWe will do a lot of feature engineering when we start using the other data sources, but in this notebook we will try only two simple feature construction methods:\n\nPolynomial features\nDomain knowledge features"},{"metadata":{"_uuid":"666ab4948336033c59228e98d9de1870293a574f"},"cell_type":"markdown","source":"One simple feature construction method is called polynomial features. In this method, we make features that are powers of existing features as well as interaction terms between existing features. For example, we can create variables EXT_SOURCE_1^2 and EXT_SOURCE_2^2 and also variables such as EXT_SOURCE_1 x EXT_SOURCE_2, EXT_SOURCE_1 x EXT_SOURCE_2^2, EXT_SOURCE_1^2 x  EXT_SOURCE_2^2, and so on. These features that are a combination of multiple individual variables are called interaction terms because they capture the interactions between variables. In other words, while two variables by themselves may not have a strong influence on the target, combining them together into a single interaction variable might show a relationship with the target. Interaction terms are commonly used in statistical models to capture the effects of multiple variables, but I do not see them used as often in machine learning. Nonetheless, we can try out a few to see if they might help our model to predict whether or not a client will repay a loan.\n\nJake VanderPlas writes about polynomial features in his excellent book Python for Data Science for those who want more information.\n\nIn the following code, we create polynomial features using the EXT_SOURCE variables and the DAYS_BIRTH variable. Scikit-Learn has a useful class called PolynomialFeatures that creates the polynomials and the interaction terms up to a specified degree. We can use a degree of 3 to see the results (when we are creating polynomial features, we want to avoid using too high of a degree, both because the number of features scales exponentially with the degree, and because we can run into problems with overfitting)."},{"metadata":{"trusted":true,"_uuid":"0d46ead1503b78c3669f40cecd41e593a97d934e","collapsed":true},"cell_type":"code","source":"# Make a new dataframe for polynomial features\npoly_features = app_train[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH', 'TARGET']]\npoly_features_test = app_test[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\n\n# imputer for handling missing values\nfrom sklearn.preprocessing import Imputer\nimputer = Imputer(strategy = 'median')\n\npoly_target = poly_features['TARGET']\n\npoly_features = poly_features.drop(columns = ['TARGET'])\n\n# Need to impute missing values # Also this ll convert below two dataframes into ndarray\npoly_features = imputer.fit_transform(poly_features)  #it takes arguments as df, fill-ins the NaNs, and outputs numpy.ndarray\npoly_features_test = imputer.transform(poly_features_test)\n\nfrom sklearn.preprocessing import PolynomialFeatures\n                                  \n# Create the polynomial object with specified degree\npoly_transformer = PolynomialFeatures(degree = 3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ecb60f25f575dfea76d4abd892225950b916d210","collapsed":true},"cell_type":"code","source":"# Train the polynomial features\npoly_transformer.fit(poly_features)\n\n# Transform the features\npoly_features = poly_transformer.transform(poly_features)\npoly_features_test = poly_transformer.transform(poly_features_test)\nprint('Polynomial Features shape: ', poly_features.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9406e71a9cc35aa96a809e071fd88faa509dd700","collapsed":true},"cell_type":"code","source":"#poly_features.head() # error, because we got ndarray in above two blocks. Its not df anymore","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"105cb7766e54ac5797be78b4712d9eb892c9b758","collapsed":true},"cell_type":"code","source":"# This creates a considerable number of new features. To get the names we have to use the polynomial features get_feature_names method.\npoly_transformer.get_feature_names(input_features = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH'])[:15]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ba8b0bc6b95c2647eb508c98b878f8105a6d0763","collapsed":true},"cell_type":"code","source":"# There are 35 features with individual features raised to powers up to degree 3 and interaction terms.\n# Now, we can see whether any of these new features are correlated with the target.\n# Create a dataframe of the features  # create dataframe from ndarray\npoly_features = pd.DataFrame(poly_features, \n                             columns = poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2', \n                                                                           'EXT_SOURCE_3', 'DAYS_BIRTH']))\n\n#poly_features.head()\n# Add in the target\npoly_features['TARGET'] = poly_target\n\n# Find the correlations with the target\npoly_corrs = poly_features.corr()['TARGET'].sort_values()\n\n# Display most negative and most positive\nprint(poly_corrs.head(10))\nprint(poly_corrs.tail(5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"74bfe43bf9a815f77c6ad71f505a429e0d82f796","collapsed":true},"cell_type":"code","source":"# Put test features into dataframe #get back the dataframe from ndarray\npoly_features_test = pd.DataFrame(poly_features_test, \n                                  columns = poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2', \n                                                                                'EXT_SOURCE_3', 'DAYS_BIRTH']))\n\n# Merge polynomial features into training dataframe\npoly_features['SK_ID_CURR'] = app_train['SK_ID_CURR']\napp_train_poly = app_train.merge(poly_features, on = 'SK_ID_CURR', how = 'left')\n\n# Merge polnomial features into testing dataframe\npoly_features_test['SK_ID_CURR'] = app_test['SK_ID_CURR']\napp_test_poly = app_test.merge(poly_features_test, on = 'SK_ID_CURR', how = 'left')\n\n# Align the dataframes # but it ll delete target column from training data set\napp_train_poly, app_test_poly = app_train_poly.align(app_test_poly, join = 'inner', axis = 1)\n\n# Print out the new shapes\nprint('Training data with polynomial features shape: ', app_train_poly.shape)\nprint('Testing data with polynomial features shape:  ', app_test_poly.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"69909ba6095d2172e53daa1523c3e078375e1d53","collapsed":true},"cell_type":"code","source":"#app_train_poly.tail()\n\n# see the names of all the columns #missing target column due to alinging in above cell\nlist(app_train_poly)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4a721225a85057c395aa85d7f92f498eed4c43ba"},"cell_type":"markdown","source":"**Domain Knowledge Features**\n\nMaybe it's not entirely correct to call this \"domain knowledge\" because I'm not a credit expert, but perhaps we could call this \"attempts at applying limited financial knowledge\". In this frame of mind, we can make a couple features that attempt to capture what we think may be important for telling whether a client will default on a loan.\n\nCREDIT_INCOME_PERCENT: the percentage of the credit amount relative to a client's income\nANNUITY_INCOME_PERCENT: the percentage of the loan annuity relative to a client's income\nCREDIT_TERM: the length of the payment in months (since the annuity is the monthly amount due\nDAYS_EMPLOYED_PERCENT: the percentage of the days employed relative to the client's age"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e289b1e900de127a87a2bcd329954c5cac319175"},"cell_type":"code","source":"app_train_domain = app_train.copy()\napp_test_domain = app_test.copy()\n\napp_train_domain['CREDIT_INCOME_PERCENT'] = app_train_domain['AMT_CREDIT'] / app_train_domain['AMT_INCOME_TOTAL']\napp_train_domain['ANNUITY_INCOME_PERCENT'] = app_train_domain['AMT_ANNUITY'] / app_train_domain['AMT_INCOME_TOTAL']\napp_train_domain['CREDIT_TERM'] = app_train_domain['AMT_ANNUITY'] / app_train_domain['AMT_CREDIT']\napp_train_domain['DAYS_EMPLOYED_PERCENT'] = app_train_domain['DAYS_EMPLOYED'] / app_train_domain['DAYS_BIRTH']\n\napp_test_domain['CREDIT_INCOME_PERCENT'] = app_test_domain['AMT_CREDIT'] / app_test_domain['AMT_INCOME_TOTAL']\napp_test_domain['ANNUITY_INCOME_PERCENT'] = app_test_domain['AMT_ANNUITY'] / app_test_domain['AMT_INCOME_TOTAL']\napp_test_domain['CREDIT_TERM'] = app_test_domain['AMT_ANNUITY'] / app_test_domain['AMT_CREDIT']\napp_test_domain['DAYS_EMPLOYED_PERCENT'] = app_test_domain['DAYS_EMPLOYED'] / app_test_domain['DAYS_BIRTH']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"350b743830c3f4ca4d24bf25882c16c1f98057ee","collapsed":true},"cell_type":"code","source":"\"\"\"\nVisualize New Variables\nWe should explore these domain knowledge variables visually in a graph. For all of these, we will make the same KDE plot colored by the value of the TARGET.\n\"\"\"\n\nplt.figure(figsize = (12, 20))\n# iterate through the new features\nfor i, feature in enumerate(['CREDIT_INCOME_PERCENT', 'ANNUITY_INCOME_PERCENT', 'CREDIT_TERM', 'DAYS_EMPLOYED_PERCENT']):\n    \n    # create a new subplot for each source\n    plt.subplot(4, 1, i + 1)\n    # plot repaid loans\n    sns.kdeplot(app_train_domain.loc[app_train_domain['TARGET'] == 0, feature], label = 'target == 0')\n    # plot loans that were not repaid\n    sns.kdeplot(app_train_domain.loc[app_train_domain['TARGET'] == 1, feature], label = 'target == 1')\n    \n    # Label the plots\n    plt.title('Distribution of %s by Target Value' % feature)\n    plt.xlabel('%s' % feature); plt.ylabel('Density');\n    \nplt.tight_layout(h_pad = 2.5)\n\nprint(\"It's hard to say ahead of time if these new features will be useful. The only way to tell for sure is to try them out!\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"836ccebded8dccd0896b9d038b7b301955c21c36"},"cell_type":"markdown","source":"\n**Baseline**\n\nFor a naive baseline, we could guess the same value for all examples on the testing set. We are asked to predict the probability of not repaying the loan, so if we are entirely unsure, we would guess 0.5 for all observations on the test set. This will get us a Reciever Operating Characteristic Area Under the Curve (AUC ROC) of 0.5 in the competition (random guessing on a classification task will score a 0.5).\n\nSince we already know what score we are going to get, we don't really need to make a naive baseline guess. Let's use a slightly more sophisticated model for our actual baseline: Logistic Regression."},{"metadata":{"_uuid":"2dc5e7fa544d0530463bd62c7b31766389a0159a"},"cell_type":"markdown","source":"**Logistic Regression Implementation**\n\nHere I will focus on implementing the model rather than explaining the details, but for those who want to learn more about the theory of machine learning algorithms, I recommend both An Introduction to Statistical Learning and Hands-On Machine Learning with Scikit-Learn and TensorFlow. Both of these books present the theory and also the code needed to make the models (in R and Python respectively). They both teach with the mindset that the best way to learn is by doing, and they are very effective!\n\nTo get a baseline, we will use all of the features after encoding the categorical variables. We will preprocess the data by filling in the missing values ([imputation](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Imputer.html)) and normalizing the range of the features ([feature scaling](https://medium.com/@ian.dzindo01/feature-scaling-in-python-a59cc72147c1)). The following code performs both of these preprocessing steps."},{"metadata":{"trusted":true,"_uuid":"1e4e562de5d762da9f9ad699878d0031ec9304f2","collapsed":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler, Imputer\n\n# Drop the target from the training data\nif 'TARGET' in app_train:\n    train = app_train.drop(columns = ['TARGET'])\nelse:\n    train = app_train.copy()\n    \n# Feature names\nfeatures = list(train.columns)\n\n# Copy of the testing data\ntest = app_test.copy()\n\n# Median imputation of missing values\nimputer = Imputer(strategy = 'median')\n\n# Scale each feature to 0-1\nscaler = MinMaxScaler(feature_range = (0, 1))\n\n# Fit on the training data\nimputer.fit(train)\n\n# Transform both training and testing data\ntrain = imputer.transform(train)\ntest = imputer.transform(app_test)\n\n# Repeat with the scaler\nscaler.fit(train)\ntrain = scaler.transform(train)\ntest = scaler.transform(test)\n\nprint('Training data shape: ', train.shape)\nprint('Testing data shape: ', test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"47e397546365fdff39627a743585c1164855cb6d"},"cell_type":"markdown","source":"We will use [LogisticRegressionfrom](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) Scikit-Learn for our first model. The only change we will make from the default model settings is to lower the regularization parameter, C, which controls the amount of overfitting (a lower value should decrease overfitting). This will get us slightly better results than the default LogisticRegression, but it still will set a low bar for any future models.\n\nHere we use the familiar Scikit-Learn modeling syntax: we first create the model, then we train the model using .fit and then we make predictions on the testing data using .predict_proba (remember that we want probabilities and not a 0 or 1).\n\nAfter the model has been trained, we can use it to make predictions. We want to predict the probabilities of not paying a loan, so we use the model predict.proba method. This returns an m x 2 array where m is the number of observations. The first column is the probability of the target being 0 and the second column is the probability of the target being 1 (so for a single row, the two columns must sum to 1). We want the probability the loan is not repaid, so we will select the second column.\n\nThe following code makes the predictions and selects the correct column."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"504467b9fef891d8defde4f8f0d815b9dd489d2e"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\n# Make the model with the specified regularization parameter\nlog_reg = LogisticRegression(C = 0.0001)\n\n# Train on the training data\nlog_reg.fit(train, train_labels)\n\nlog_reg_pred = log_reg.predict_proba(test)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ce07668e8847c8d0972be73b175b2cc50090c984","collapsed":true},"cell_type":"code","source":"submit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = log_reg_pred\n\nsubmit.head()\n\n\"\"\"\nthe predictions represent a probability between 0 and 1 that the loan will not be repaid. \nIf we were using these predictions to classify applicants, we could set a probability threshold for determining that a loan is risky.\n\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f93cf78229a1de07bc564d734e8962bfdf06c5b2"},"cell_type":"code","source":"submit.to_csv('log_reg_baseline.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"850fe498806604f988454cf4542bbab509f8b66e"},"cell_type":"markdown","source":"The submission has now been saved to the virtual environment in which our notebook is running. To access the submission, at the end of the notebook, we will hit the blue Commit & Run button at the upper right of the kernel. This runs the entire notebook and then lets us download any files that are created during the run.\n\nOnce we run the notebook, the files created are available in the Versions tab under the Output sub-tab. From here, the submission files can be submitted to the competition or downloaded. Since there are several models in this notebook, there will be multiple output files.\n\nThe logistic regression baseline should score around 0.671 when submitted."},{"metadata":{"_uuid":"d1514584a16f0ac059649685026f977102ca0387"},"cell_type":"markdown","source":"**Improved Model: Random Forest**\n\nTo try and beat the poor performance of our baseline, we can update the algorithm. Let's try using a Random Forest on the same training data to see how that affects performance. The Random Forest is a much more powerful model especially when we use hundreds of trees. We will use 100 trees in the random forest"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"05ac7432cf13c7b07c46b64953c185dd3121673b"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\n# Make the random forest classifier\nrandom_forest = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fc62d0bab2c4a2a8c2bf7dcc172c92a13a9f6acf","collapsed":true},"cell_type":"code","source":"random_forest.fit(train, train_labels)\n\n# Extract feature importances\nfeature_importance_values = random_forest.feature_importances_\nfeature_importances = pd.DataFrame({'feature': features, 'importance': feature_importance_values})\n\n# Make predictions on the test data\npredictions = random_forest.predict_proba(test)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b43da68044eada2b58aef39c2762c87cce58f216","collapsed":true},"cell_type":"code","source":"submit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = predictions\n\n# Save the submission dataframe\nsubmit.to_csv('random_forest_baseline.csv', index = False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a870929d11c86e7cf2abbded68bf7f3fc900a989","collapsed":true},"cell_type":"code","source":"#These predictions will also be available when we run the entire notebook.\n\n#This model should score around 0.678 when submitted","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7961b98a654cb3a1acfe3764fca0ab07ff86b7eb"},"cell_type":"markdown","source":"**Make Predictions using Engineered Features**\n\nThe only way to see if the Polynomial Features and Domain knowledge improved the model is to train a test a model on these features! We can then compare the submission performance to that for the model without these features to gauge the effect of our feature engineering."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b6faf7be6dab00137d63ec3b54c900e0911199dc"},"cell_type":"code","source":"poly_features_names = list(app_train_poly.columns)\n\n# Impute the polynomial features\nimputer = Imputer(strategy = 'median')\n\npoly_features = imputer.fit_transform(app_train_poly)\npoly_features_test = imputer.transform(app_test_poly)\n\n# Scale the polynomial features\nscaler = MinMaxScaler(feature_range = (0, 1))\n\npoly_features = scaler.fit_transform(poly_features)\npoly_features_test = scaler.transform(poly_features_test)\n\nrandom_forest_poly = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea3574380af741cf60125f3373f7fa97115b8e0a","collapsed":true},"cell_type":"code","source":"random_forest_poly.fit(poly_features, train_labels)\n\n# Make predictions on the test data\npredictions = random_forest_poly.predict_proba(poly_features_test)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4864b4889a8b7fdbbbc87d880bfb2afb4d8b79c6","collapsed":true},"cell_type":"code","source":"submit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = predictions\n\n# Save the submission dataframe\nsubmit.to_csv('random_forest_baseline_engineered.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"62bc17511b9924ebed9c77a55bc46970267838a1"},"cell_type":"code","source":"#This model scored 0.678 when submitted to the competition, exactly the same as that without the engineered features. \n#Given these results, it does not appear that our feature construction helped in this case.\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"03a8427ac6d36da411689d874e8863aeec02d5a8"},"cell_type":"markdown","source":"**Testing Domain Features**\n\nNow we can test the domain features we made by hand."},{"metadata":{"trusted":true,"_uuid":"fd34c860bd8f6cb54f639e911dc570bc3acfea55","collapsed":true},"cell_type":"code","source":"app_train_domain = app_train_domain.drop(columns = 'TARGET')\n\ndomain_features_names = list(app_train_domain.columns)\n\n# Impute the domainnomial features\nimputer = Imputer(strategy = 'median')\n\ndomain_features = imputer.fit_transform(app_train_domain)\ndomain_features_test = imputer.transform(app_test_domain)\n\n# Scale the domainnomial features\nscaler = MinMaxScaler(feature_range = (0, 1))\n\ndomain_features = scaler.fit_transform(domain_features)\ndomain_features_test = scaler.transform(domain_features_test)\n\nrandom_forest_domain = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)\n\n# Train on the training data\nrandom_forest_domain.fit(domain_features, train_labels)\n\n# Extract feature importances\nfeature_importance_values_domain = random_forest_domain.feature_importances_\nfeature_importances_domain = pd.DataFrame({'feature': domain_features_names, 'importance': feature_importance_values_domain})\n\n# Make predictions on the test data\npredictions = random_forest_domain.predict_proba(domain_features_test)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"30ad59c0a3627195aff0c4a4fbd43be95f253dcf","collapsed":true},"cell_type":"code","source":"# Make a submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = predictions\n\n# Save the submission dataframe\nsubmit.to_csv('random_forest_baseline_domain.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"67540a63efe9650459122e00050f123c152c00b3"},"cell_type":"code","source":"#This scores 0.679 when submitted which probably shows that the engineered features do not help in this model (however they do help in the Gradient Boosting Model at the end of the notebook).","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8bc5ce894128a18b6abfe1174db2393bac804aca"},"cell_type":"markdown","source":"**Model Interpretation: Feature Importances**\nAs a simple method to see which variables are the most relevant, we can look at the feature importances of the random forest. Given the correlations we saw in the exploratory data analysis, we should expect that the most important features are the EXT_SOURCE and the DAYS_BIRTH. We may use these feature importances as a method of dimensionality reduction in future work."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"42f1114aa619a72f0f3a81f5a413536dcb29a904"},"cell_type":"code","source":"def plot_feature_importances(df):\n    \"\"\"\n    Plot importances returned by a model. This can work with any measure of\n    feature importance provided that higher importance is better. \n    \n    Args:\n        df (dataframe): feature importances. Must have the features in a column\n        called `features` and the importances in a column called `importance\n        \n    Returns:\n        shows a plot of the 15 most importance features\n        \n        df (dataframe): feature importances sorted by importance (highest to lowest) \n        with a column for normalized importance\n        \"\"\"\n    \n    # Sort features according to importance\n    df = df.sort_values('importance', ascending = False).reset_index()\n    \n    # Normalize the feature importances to add up to one\n    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n\n    # Make a horizontal bar chart of feature importances\n    plt.figure(figsize = (10, 6))\n    ax = plt.subplot()\n    \n    # Need to reverse the index to plot most important on top # https://matplotlib.org/api/_as_gen/matplotlib.pyplot.barh.html#matplotlib.pyplot.barh\n    # barh(y, width, *, align='center', **kwargs)\n    ax.barh(list(reversed(list(df.index[:15]))), \n            df['importance_normalized'].head(15), \n            align = 'center', edgecolor = 'r')\n    \n    # Set the yticks and labels\n    ax.set_yticks(list(reversed(list(df.index[:15]))))\n    ax.set_yticklabels(df['feature'].head(15))\n    \n    # Plot labeling\n    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n    plt.show()\n    \n    print(list(reversed(list(df.index[:15]))), \"\\n\\n\", df[['feature','importance_normalized']].head(15))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1d68feaaa480eab14ba418c3c3c36803a652b316","collapsed":true},"cell_type":"code","source":"#list(reversed(list(feature_importances.index[:15])))\n#feature_importances['importance'].head(15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"165867c104c0ffe3dcce06bee509b6f5eb235097","collapsed":true},"cell_type":"code","source":"# Show the feature importances for the default features\nfeature_importances_sorted = plot_feature_importances(feature_importances)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5c887688422dc703054fa6cba989d1b841a92c42","collapsed":true},"cell_type":"code","source":"feature_importances_domain_sorted = plot_feature_importances(feature_importances_domain)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f8f1a5cc2e9915c3be3816d8a65b271942aa32f0"},"cell_type":"markdown","source":"As expected, the most important features are those dealing with EXT_SOURCE and DAYS_BIRTH. We see that there are only a handful of features with a significant importance to the model, which suggests we may be able to drop many of the features without a decrease in performance (and we may even see an increase in performance.) Feature importances are not the most sophisticated method to interpret a model or perform dimensionality reduction, but they let us start to understand what factors our model takes into account when it makes predictions.\n\nWe see that all four of our hand-engineered features made it into the top 15 most important! This should give us confidence that our domain knowledge was at least partially on track."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ceddc03e1c43408816e31094761b78c228d75c74"},"cell_type":"markdown","source":"**Conclusions**\n\nIn this notebook, we saw how to get started with a Kaggle machine learning competition. We first made sure to understand the data, our task, and the metric by which our submissions will be judged. Then, we performed a fairly simple EDA to try and identify relationships, trends, or anomalies that may help our modeling. Along the way, we performed necessary preprocessing steps such as encoding categorical variables, imputing missing values, and scaling features to a range. Then, we constructed new features out of the existing data to see if doing so could help our model.\n\nOnce the data exploration, data preparation, and feature engineering was complete, we implemented a baseline model upon which we hope to improve. Then we built a second slightly more complicated model to beat our first score. We also carried out an experiment to determine the effect of adding the engineering variables.\n\nWe followed the general outline of a [machine learning project](https://towardsdatascience.com/a-complete-machine-learning-walk-through-in-python-part-one-c62152f39420):\n\nUnderstand the problem and the data\nData cleaning and formatting (this was mostly done for us)\nExploratory Data Analysis\nBaseline model\nImproved model\nModel interpretation (just a little)\nMachine learning competitions do differ slightly from typical data science problems in that we are concerned only with achieving the best performance on a single metric and do not care about the interpretation. However, by attempting to understand how our models make decisions, we can try to improve them or examine the mistakes in order to correct the errors."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2f71a231470bd641ff6f2a19450a65af44e52235"},"cell_type":"markdown","source":"**Light Gradient Boosting Machine**\n\nNow (if you want, this part is entirely optional) we can step off the deep end and use a real machine learning model: the[ gradient boosting machine](https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/) using the [LightGBM library](https://lightgbm.readthedocs.io/en/latest/Quick-Start.html)! The Gradient Boosting Machine is currently the leading model for learning on structured datasets (especially on Kaggle) and we will probably need some form of this model to do well in the competition. Don't worry, even if this code looks intimidating, it's just a series of small steps that build up to a complete model. I added this code just to show what may be in store for this project, and because it gets us a slightly better score on the leaderboard."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7cc9476e89dfc63f60714142031213f128a0da6e"},"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb\nimport gc\n\ndef model(features, test_features, encoding = 'ohe', n_folds = 5):\n    \n    \"\"\"Train and test a light gradient boosting model using\n    cross validation. \n    \n    Parameters\n    --------\n        features (pd.DataFrame): \n            dataframe of training features to use \n            for training a model. Must include the TARGET column.\n        test_features (pd.DataFrame): \n            dataframe of testing features to use\n            for making predictions with the model. \n        encoding (str, default = 'ohe'): \n            method for encoding categorical variables. Either 'ohe' for one-hot encoding or 'le' for integer label encoding\n            n_folds (int, default = 5): number of folds to use for cross validation\n        \n    Return\n    --------\n        submission (pd.DataFrame): \n            dataframe with `SK_ID_CURR` and `TARGET` probabilities\n            predicted by the model.\n        feature_importances (pd.DataFrame): \n            dataframe with the feature importances from the model.\n        valid_metrics (pd.DataFrame): \n            dataframe with training and validation metrics (ROC AUC) for each fold and overall.\n        \n    \"\"\"\n    \n    # Extract the ids\n    train_ids = features['SK_ID_CURR']\n    test_ids = test_features['SK_ID_CURR']\n    \n    # Extract the labels for training\n    labels = features['TARGET']\n    \n    # Remove the ids and target\n    features = features.drop(columns = ['SK_ID_CURR', 'TARGET'])\n    test_features = test_features.drop(columns = ['SK_ID_CURR'])\n    \n    \n    # One Hot Encoding\n    if encoding == 'ohe':\n        features = pd.get_dummies(features)\n        test_features = pd.get_dummies(test_features)\n        \n        # Align the dataframes by the columns\n        features, test_features = features.align(test_features, join = 'inner', axis = 1)\n        \n        # No categorical indices to record\n        cat_indices = 'auto'\n    \n    # Integer label encoding\n    elif encoding == 'le':\n        \n        # Create a label encoder\n        label_encoder = LabelEncoder()\n        \n        # List for storing categorical indices\n        cat_indices = []\n        \n        # Iterate through each column\n        for i, col in enumerate(features):\n            if features[col].dtype == 'object':\n                # Map the categorical features to integers\n                features[col] = label_encoder.fit_transform(np.array(features[col].astype(str)).reshape((-1,)))\n                test_features[col] = label_encoder.transform(np.array(test_features[col].astype(str)).reshape((-1,)))\n\n                # Record the categorical indices\n                cat_indices.append(i)\n    \n    # Catch error if label encoding scheme is not valid\n    else:\n        raise ValueError(\"Encoding must be either 'ohe' or 'le'\")\n        \n    print('Training Data Shape: ', features.shape)\n    print('Testing Data Shape: ', test_features.shape)\n    \n    # Extract feature names\n    feature_names = list(features.columns)\n    \n    # Convert to np arrays\n    features = np.array(features)\n    test_features = np.array(test_features)\n    \n    # Create the kfold object\n    k_fold = KFold(n_splits = n_folds, shuffle = True, random_state = 50)\n    \n    # Empty array for feature importances\n    feature_importance_values = np.zeros(len(feature_names))\n    \n    # Empty array for test predictions\n    test_predictions = np.zeros(test_features.shape[0])\n    \n    # Empty array for out of fold validation predictions\n    out_of_fold = np.zeros(features.shape[0])\n    \n    # Lists for recording validation and training scores\n    valid_scores = []\n    train_scores = []\n    \n    # Iterate through each fold\n    for train_indices, valid_indices in k_fold.split(features):\n        \n        # Training data for the fold\n        train_features, train_labels = features[train_indices], labels[train_indices]\n        # Validation data for the fold\n        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n        \n        # Create the model\n        model = lgb.LGBMClassifier(n_estimators=10000, objective = 'binary', \n                                   class_weight = 'balanced', learning_rate = 0.05, \n                                   reg_alpha = 0.1, reg_lambda = 0.1, \n                                   subsample = 0.8, n_jobs = -1, random_state = 50)\n        \n        # Train the model\n        model.fit(train_features, train_labels, eval_metric = 'auc',\n                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n                  eval_names = ['valid', 'train'], categorical_feature = cat_indices,\n                  early_stopping_rounds = 100, verbose = 200)\n        \n        # Record the best iteration\n        best_iteration = model.best_iteration_\n        \n        # Record the feature importances\n        feature_importance_values += model.feature_importances_ / k_fold.n_splits\n        \n        # Make predictions\n        test_predictions += model.predict_proba(test_features, num_iteration = best_iteration)[:, 1] / k_fold.n_splits\n        \n        # Record the out of fold predictions\n        out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]\n        \n        # Record the best score\n        valid_score = model.best_score_['valid']['auc']\n        train_score = model.best_score_['train']['auc']\n        \n        valid_scores.append(valid_score)\n        train_scores.append(train_score)\n        \n        # Clean up memory\n        gc.enable()\n        del model, train_features, valid_features\n        gc.collect()\n        \n    # Make the submission dataframe\n    submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_predictions})\n    \n    # Make the feature importance dataframe\n    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n    \n    # Overall validation score\n    valid_auc = roc_auc_score(labels, out_of_fold)\n    \n    # Add the overall scores to the metrics\n    valid_scores.append(valid_auc)\n    train_scores.append(np.mean(train_scores))\n    \n    # Needed for creating dataframe of validation scores\n    fold_names = list(range(n_folds))\n    fold_names.append('overall')\n    \n    # Dataframe of validation scores\n    metrics = pd.DataFrame({'fold': fold_names,\n                            'train': train_scores,\n                            'valid': valid_scores}) \n    \n    return submission, feature_importances, metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"707477d2be5dbc92cf393a3dd9649f529a91fad5","collapsed":true},"cell_type":"code","source":"submission, fi, metrics = model(app_train, app_test)\nprint('Baseline metrics')\nprint(metrics)\n\n#This model scores about 0.735","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1f58f00119042987bff688c822006abac12fb3d4","collapsed":true},"cell_type":"code","source":"fi_sorted = plot_feature_importances(fi)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"6d877e85f7003dbd433ecbb705b8435a5ea52867"},"cell_type":"code","source":"submission.to_csv('baseline_lgb.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2ee5f6a9833b7f1186efccac4f4b50af09c94cca","collapsed":true},"cell_type":"code","source":"app_train_domain['TARGET'] = train_labels\n\n# Test the domain knolwedge features\nsubmission_domain, fi_domain, metrics_domain = model(app_train_domain, app_test_domain)\nprint('Baseline with domain knowledge features metrics')\nprint(metrics_domain)\n\n#This model scores about 0.754","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a4290d62b0ae191d848028a51677afad9364e1e1","collapsed":true},"cell_type":"code","source":"fi_sorted = plot_feature_importances(fi_domain)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"71e157809396773782f3399100ccc5c2bc0e8b03"},"cell_type":"code","source":"submission_domain.to_csv('baseline_lgb_domain_features.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"08803c0e3a4a2191cbce5b68f293964496d32851"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}