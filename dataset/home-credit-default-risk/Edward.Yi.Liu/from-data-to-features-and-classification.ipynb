{"cells":[{"metadata":{"_uuid":"9d7057b0f4ce0d2bb50bfea37c29ce0927cdf53f"},"cell_type":"markdown","source":"> # From Data to Features & Classification\n\nIn this notebook, we will have a basic travel from data feature engineering to classification prediction for the Home Credit Default Risk competition. Machine learning automatically learns from data, and often performs better given more (useful/relevant) features. However, we still need to be careful with correlations between the features and the target. In this competition, several dataframes are available:\n![image.png](https://storage.googleapis.com/kaggle-media/competitions/home-credit/home_credit.png)\n\nNote that this is my first kaggle notebook, and I do refer hugely the work of [Will Koehrsen](https://www.kaggle.com/willkoehrsen) [here] (https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction). So this notebook is meant to get familiar with the competition data on top of his excellent work as a tutorial. \n\nIn addition to the `application` data, we also have more datasets such as the `bureau` and `bureau_balance` data:\n\n* **bureau**: data concerning client's previous credits from other financial institutions. Each previous credit has its own row in bureau and is identified by the SK_ID_BUREAU, Each loan in the application data can have multiple previous credits.\n* **bureau_balance**: monthly data about the previous credits in bureau. Each row is one month of a previous credit, and a single previous credit can have multiple rows, one for each month of the credit length.\n* **previous_application**: previous applications for loans at Home Credit of clients who have loans in the application data. Each current loan in the application data can have multiple previous loans. Each previous application has one row and is identified by the feature SK_ID_PREV.\n* **POS_CASH_BALANCE**: monthly data about previous point of sale or cash loans clients have had with Home Credit. Each row is one month of a previous point of sale or cash loan, and a single previous loan can have many rows.\n* **credit_card_balance**: monthly data about previous credit cards clients have had with Home Credit. Each row is one month of a credit card balance, and a single credit card can have many rows.\n* **installments_payment**: payment history for previous loans at Home Credit. There is one row for every made payment and one row for every missed payment.\n\nIf you don't have domain knowledge of loans and credits, no worries because me neither. In this notebook, we'll have a basic travel from data to features and classification. We'll also go through some basic concepts in regards during the travel. Note that this notebook also refers to several other notebooks, so we don't have to reinvent the wheel.\n\nNow let's begin."},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# File system manangement\nimport os\n\n# pandas and numpy for data manipulation\nimport pandas as pd\nimport numpy as np\n\n# matplotlib and seaborn for visualizing\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"># 1. Look at the data\nLet's have a look at where we are and what we have:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('where we are?')\n!pwd\n\nprint('\\nwhat we have/access to?')\n!ls ../ -l\n\nprint('\\nwhat data we can use?')\n! ls ../input -l","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Uncomment the following cell if you want to know the data items"},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Items of the data\n# HC_desc = pd.read_csv('../input/HomeCredit_columns_description.csv', encoding='latin')\n\n# # columns and rows\n# print(HC_desc.count())\n\n# app_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What the training data look like:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training data\napp_train = pd.read_csv('../input/application_train.csv')\nprint('Training data shape: ', app_train.shape, \\\n      '\\nTarget/Label:\\n', app_train['TARGET'].value_counts())\napp_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the `TARGET` include two classes {0,1}, which are the labels for training and machine learning model and to predict with. Following is the histogram of the clients who paied/defaulted their loans:"},{"metadata":{"trusted":true},"cell_type":"code","source":"app_train['TARGET'].astype(int).plot.hist(bins=app_train['TARGET'].value_counts().shape[0]+1, alpha=0.75 )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Testing data features\napp_test = pd.read_csv('../input/application_test.csv')\nprint('Testing data shape: ', app_test.shape, \\\n      '\\nTesting data example: \\n')\napp_test.head(5) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"># 2. Clean & prepare data\nBefore starting with machine learning that's really cool, we have to clean it up. Let's have a look at the data's `cleaness`, then prepare it ready for feature engineering and then machine learning ...\n\n> ### 2.1 Check missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to calculate missing values by column# Funct \ndef missing_values_table(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Total values\n        tot_val = df.count() # len(df)-mis_val\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() / len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, tot_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : 'Valid Values', 2 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,2] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns\n    \nmissing_values = missing_values_table(app_train)\nmissing_values.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### 2.2 Categorial variable encoding\nBriefly, we need to make the data \"computable\" by converting from category to numbers. \nLet's see the types of entries "},{"metadata":{"trusted":true},"cell_type":"code","source":"app_train.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`float` and `int64` are entries usable already wrt machine learning. Let's now look at the object entries of the columns, so that we can encode these categoriacal variables. In the following result, we can see that most of the variables have a small number of unique entries. By encoding, we basically use numbers to represent the variables so to be fed to machine learning algorithms:\n\n* label encoding for {2-category object}    - eg.{1,2,3,...}\n* one-hot encoding for {>2-category object} - eg.\n  - 1 0 0 0\n  - 0 1 0 0\n  - 0 0 1 0\n  - 0 0 0 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"app_train.select_dtypes('object').apply(pd.Series.nunique, axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sklearn preprocessing for dealing with categorical variables\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# new label encoder object\nle = LabelEncoder()\nle_count = 0\n\n# Iterate by object columns\nfor col in app_train:\n    if app_train[col].dtype == 'object':\n        # 2-category object\n        if len(list(app_train[col].unique())) <= 2:\n            # train on the column\n            le.fit(app_train[col])\n            \n            # transform both training & test data\n            # - note that this applies to the case where training data has equal or more categorial entries than the test data\n            app_train[col] = le.transform(app_train[col])\n            app_test[col] = le.transform(app_test[col])\n            \n            # count the number of 2-category objects\n            le_count += 1\n            \n            \nprint('%d columns were label-encoded.' % le_count)\n\n# one-hot encoding of categorical variables\napp_train = pd.get_dummies(app_train)\napp_test = pd.get_dummies(app_test)\n\nprint('Training Features shape: ', app_train.shape)\nprint('Testing Features shape: ', app_test.shape)\n            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# initialize list of lists \ndata_list = [['tom', 't'], ['peter', 'p'], ['paul', 'p']]\n  \n# Create the pandas DataFrame \ndataFrm = pd.DataFrame(data_list, columns = ['Name', 'alias']) \n\ndataFrm_ohe = pd.get_dummies(dataFrm)\n\n#\n\nprint('This is how one-hot encoding works:')\nprint('Originally: \\n', dataFrm.head())\n\nprint('\\nOne-Hot Encoded: \\n', dataFrm_ohe.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Align the dataframes, to make sure the columns in the training data are also in the testing data. We also keep the `target` column to be used as training labels."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels = app_train['TARGET']\n\n# align\napp_train, app_test = app_train.align(app_test, join='inner', axis = 1)\n\n# put back the TARGET column - creating a new column\napp_train['TARGET'] = train_labels\n\nprint('Training Features shape: ', app_train.shape)\nprint('Testing Features shape: ', app_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### 2.3 Check anomaly \nIt's almost impossible to record/document/collect perfect data. Anomalies/noise/dirts/,you name it, therefore, are quite common. Checking anomalies is highly necessary."},{"metadata":{},"cell_type":"markdown","source":"#### - Age (by years)"},{"metadata":{},"cell_type":"markdown","source":"# Ages of a person\nprint( (app_train['DAYS_BIRTH']/-365).describe() )\nprint('\\nAdults, between 20 and 69 - looks reasonable')\n\n(app_train['DAYS_BIRTH']/-365).hist(bins=20)"},{"metadata":{},"cell_type":"markdown","source":"#### - Years employed\n1000 years of employment is anomaly. Histogram shows that 1000 years (`365243`) is very likely the anomaly:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Years employed\n\nprint(  (app_train['DAYS_EMPLOYED']).describe() , '\\n')\n\nprint(  (app_train['DAYS_EMPLOYED']/365).describe()  )\n\nprint('\\nAnomalous!')\n\napp_train['DAYS_EMPLOYED'].hist(bins=55)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create an anomalous flag column\napp_train['DAYS_EMPLOYED_ANOM'] = app_train[\"DAYS_EMPLOYED\"] == 365243\n\n# Replace the anomalous values with nan\napp_train['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\n\n(app_train['DAYS_EMPLOYED']/-365).hist(bins=20)\nplt.title('DAYS_EMPLOYED Histogram')\nplt.xlabel('Years Employed')\nplt.ylabel('Years Employed')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Do the same change to the test data as well"},{"metadata":{"trusted":true},"cell_type":"code","source":"app_test['DAYS_EMPLOYED_ANOM'] = app_test[\"DAYS_EMPLOYED\"] == 365243\napp_test[\"DAYS_EMPLOYED\"].replace({365243: np.nan}, inplace = True)\n\nprint('There are %d anomalies in the test data out of %d entries' % (app_test[\"DAYS_EMPLOYED_ANOM\"].sum(), len(app_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">### 2.4 Correlation : feature - class ï¼ˆTARGET)\nThe correlation coefficient does not always represent \"relevance\" of a feature, but it does give us an idea of possible relationships within the data. Its value ranges from -1 to 1. Greater absolute value of the correlation coefficient, the higher \"relevance\" there is between two variables. In the following, we'll use several examples to show how we check and select features/variables from all with regards to correlation."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find correlations with the target and sort\ncorrelations = app_train.corr()['TARGET'].sort_values()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Most Positive Correlations:\\n', correlations.tail(15))\nprint('\\nMost Negative Correlations:\\n', correlations.head(15))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Negative corelation coefficient (CC) of, for the example of age (used negative values of DAYS_BIRTH so its CC is actually -0.78), means that the clients are less likely to default their loan as they get older. Let's visualize the effect of the age on the target with kernel density estimation (a smoothed histogram)."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('fivethirtyeight')\n\nplt.figure(figsize = (10, 8))\n\n# KDE plot of loans that were repaid on time\nsns.kdeplot(  abs(app_train.loc[app_train['TARGET'] == 0, 'DAYS_BIRTH'] / 365), label = 'target == 0 (repaid)', linewidth=3)\n\n# KDE plot of loans which were not repaid on time\nsns.kdeplot(  abs(app_train.loc[app_train['TARGET'] == 1, 'DAYS_BIRTH'] / 365), label = 'target == 1 (defaulted)', linewidth=3)\n\n# Labeling of plot\nplt.xlabel('Age (years)', fontsize=14); plt.ylabel('Density', fontsize=14); plt.title('Distribution of Ages', fontsize=14)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the group that client defaulted, there's a skewness towards the younger end by age, while it is fairly flat among different ages of the group who repaid their loans. To further illustrate how different age groups behave, we separate them into groups by 5 years from the youngest to the oldest and calculate their failure rate of repaying loans."},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\n\n# Age information into a separate dataframe\nage_data = app_train[['TARGET', 'DAYS_BIRTH']]\nage_data['YEARS_BIRTH'] = age_data['DAYS_BIRTH'] / -365\n\n# Bin the age data\nage_data['YEARS_BINNED'] = pd.cut(age_data['YEARS_BIRTH'], bins = np.linspace(20, 70, num = 11))\nage_data.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see how each age group behaved by calculating their mean default rate"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Group by the bin and calculate averages\nage_groups  = age_data.groupby('YEARS_BINNED').mean()\nprint(age_groups)\n\nplt.figure(figsize = (8, 8))\n\n# Graph the age bins and the average of the target as a bar plot\nplt.bar(age_groups.index.astype(str), 100 * age_groups['TARGET'])\n\n# Plot labeling\nplt.xticks(rotation = 75); plt.xlabel('Age Group (years)'); plt.ylabel('Failure to Repay (%)')\nplt.title('Failure to Repay by Age Group');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is a clear trend: younger applicants are more likely to not repay the loan! The rate of failure to repay is above 10% for the youngest three age groups and beolow 5% for the oldest age group.\n\nThis is information that could be directly used by the bank: because younger clients are less likely to repay the loan, maybe they should be provided with more guidance or financial planning tips. This doesn't mean that the bank should discriminize against younger clients by over-generalizing the above observation. In practice, there are many other factors and aspects that should be taken into account. Therefore, we should use more information/features than single ones. Machine learning is good at doing this organically."},{"metadata":{},"cell_type":"markdown","source":"Let's also have a look at Exterior Sources (`EXT_SOURCE_1`, `EXT_SOURCE_2`, and `EXT_SOURCE_3`), since they have the 'largest' correlation relevance (negative)\n\n> [(see this notebook)](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering) According to the documentation, these features represent a \"normalized score from external data source\". I'm not sure what this exactly means, but it may be a cumulative sort of credit rating made using numerous sources of data\n\nLike previous example of age, let's start with correlation\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extract the EXT_SOURCE variables and show correlations\next_data = app_train[['TARGET', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\next_data_corrs = ext_data.corr()\nprint(ext_data_corrs)\n\nplt.figure(figsize=(8,6))\n\n# Heatmap of correlations\nsns.heatmap(ext_data_corrs, cmap = plt.cm.RdYlBu_r, vmin = -0.25, annot = True, vmax = 0.6)\nplt.title('Correlation Heatmap')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Take a look at the first column of the heatmap, the color represents the correlation heat between EXT_SOURCE features and their loan behavior TARGET. All three features have negative correlation with the TARGET, suggesting that as their value increases, the client is more likely to repay the loan (1-default, 0-repaid). Let's furture visualize this information with KDE (smoothed histogram):"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10, 12))\n\n# iterate through the sources\nfor i, source in enumerate(['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']):\n    \n    # create a new subplot for each source\n    plt.subplot(3, 1, i + 1)\n    # plot repaid loans\n    sns.kdeplot(app_train.loc[app_train['TARGET'] == 0, source], label = 'target == 0')\n    # plot loans that were not repaid\n    sns.kdeplot(app_train.loc[app_train['TARGET'] == 1, source], label = 'target == 1')\n    \n    # Label the plots\n    plt.title('Distribution of %s by Target Value' % source)\n    plt.xlabel('%s' % source); plt.ylabel('Density');\n    \nplt.tight_layout(h_pad = 2.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From these three EXT_SOURCE_3 features, similar observations can be obtained compare with age feature. EXT_SOURCE_3 displays the greatest difference between its values and the target. \n\nSo far so good, or too much information? To be concise, we could use pairs plot to visualize all the above results:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Copy the data for plotting\nplot_data = ext_data.drop(columns = ['DAYS_BIRTH']).copy()\n\n# Add in the age of the client in years\nplot_data['YEARS_BIRTH'] = age_data['YEARS_BIRTH']\n\n# Drop na values and limit to first 10000 rows\nplot_data = plot_data.dropna().loc[:10000, :]\n\n# Function to calculate correlation coefficient between two columns\ndef corr_func(x, y, **kwargs):\n    r = np.corrcoef(x, y)[0][1]\n    ax = plt.gca()\n    ax.annotate(\"r = {:.2f}\".format(r),\n                xy=(.2, .8), xycoords=ax.transAxes,\n                size = 20)\n\n# Create the pairgrid object\ngrid = sns.PairGrid(data = plot_data, size = 3, diag_sharey=False,\n                    hue = 'TARGET', \n                    vars = [x for x in list(plot_data.columns) if x != 'TARGET'])\n\n# Upper is a scatter plot\ngrid.map_upper(plt.scatter, alpha = 0.2)\n\n# Diagonal is a histogram\ngrid.map_diag(sns.kdeplot)\n\n# Bottom is density plot\ngrid.map_lower(sns.kdeplot, cmap = plt.cm.OrRd_r);\n\nplt.suptitle('Ext Source and Age Features Pairs Plot', size = 32, y = 1.05);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One more thing to talk about\n> Should we always use as more features as we can/have? Well, not really. A good decision/judgement/plan is based on balanced consideration of multiple factors/aspects. Imagine that several features are representing the same factor/aspect, then these features may often play a dominant role in leading to a decision/jugement/plan. \n\nSo, how could we avoid this happening? \n> An easy way, usually useful but not always, is to also have a look at the correlations among your selected features.\n\nFor example, in the examples above, we don't see very strong positive/negative between the EXT_SOURCE features and age. It is very probably safe to use all of them in our machine learning model.\n\nAn advanced method is feature engineering. `Kaggle competitions are won by feature engineering: those win are those who can create the most useful features out of the data`, which is actually the case of many other competitions such as FlyAI, Tianchi.Aliyun."},{"metadata":{},"cell_type":"markdown","source":"> ### 2.5 Feature Engineering\nGiven the suitable model and optimized parameters, we still need to feed the model with data of good quality. This is where feature engineering plays a significant role. It takes a thick book to cover generally, not all, feature engineering. But don't panic, one major reason is due to the diversity of data and its application scenarios. Basically, feature engineering is a bunch of techniques that collect features out of the original data, which is often followed by feature selection (choosing from most useful {often subjective}, or dimension reducing). \n\n> Currently, many mature routines of features engineering are already available along with algorithms, methods and tricks.There are also some toolboxes for semi-/autonomous feature engineering out there. Some of them are open sourced for usage and development on top of, some change reasonable fees. It's flexible to choose a proper one depending on the case. \n\n> In this case, we use two simple but popular method of feature construction:\n  - Polynomial features\n  - Domain knowledge features\n \n#### 2.5.1 Polynomial Features\nLiterally by its name, polynomial features are polynomial combinations of existing features - polynomial calculation -[\\[involves only the operations of addition, subtraction, multiplication, and non-negative integer exponentiation of variables\\]](https://en.wikipedia.org/wiki/Polynomial). As the polynomial features capture the interaction between variables, they are called interaction terms. One useful observation - while two variables by themselves may not have a strong influence on the target, combining them together into a single interaction variable might show a relationship with the target.\n\nGood news is that Scikit-Learn library has a useful class called `PolynomialFeatures`, so we don't have to manually code our own function for this purpose. Try not to over use polynomial features because it may lead to overfitting (less generalizable capability, namely, less capable in predicting). \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make a new dataframe for polynomial features\npoly_features = app_train[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH', 'TARGET']]\npoly_features_test = app_test[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\n\n# imputer for handling missing values\nfrom sklearn.preprocessing import Imputer\nimputer = Imputer(strategy = 'median')\n\npoly_target = poly_features['TARGET']\n\npoly_features = poly_features.drop(columns = ['TARGET'])\n\n# Need to impute missing values\npoly_features = imputer.fit_transform(poly_features)\npoly_features_test = imputer.transform(poly_features_test)\n\nfrom sklearn.preprocessing import PolynomialFeatures\n                                  \n# Create the polynomial object with specified degree\npoly_transformer = PolynomialFeatures(degree = 3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train the polynomial features\npoly_transformer.fit(poly_features)\n\n# Transform the features\npoly_features = poly_transformer.transform(poly_features)\npoly_features_test = poly_transformer.transform(poly_features_test)\nprint('Polynomial Features shape: ', poly_features.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"poly_transformer.get_feature_names(input_features = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH'])[:27]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 35 features with individual features raised to powers up to degree 3 and interaction terms. Now, we can see whether any of these new features are correlated with the target."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a dataframe of the features \npoly_features = pd.DataFrame(poly_features, \n                             columns = poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2', \n                                                                           'EXT_SOURCE_3', 'DAYS_BIRTH']))\n\n# Add in the target\npoly_features['TARGET'] = poly_target\n\n# Find the correlations with the target\npoly_corrs = poly_features.corr()['TARGET'].sort_values()\n\n# Display most negative and most positive\nprint(poly_corrs.head(10), '\\n')\nprint(poly_corrs.tail(10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As seen in this result above, the combined features have higher correlation to the TARGET than original individual features. This does not directly lead to the final decision of which features to be used in machine learning, but provides clues instead. When we build machine learning models, we can try with and without these features to determine if they actually help the model learn. Often in machine learning, the only way to choose a better approach is to try it out.\n\nNow let's see what polynomial features we have for machine learning:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Put test features into dataframe\npoly_features_test = pd.DataFrame(poly_features_test, \n                                  columns = poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2', \n                                                                                'EXT_SOURCE_3', 'DAYS_BIRTH']))\n\n# Merge polynomial features into training dataframe\npoly_features['SK_ID_CURR'] = app_train['SK_ID_CURR']\napp_train_poly = app_train.merge(poly_features, on = 'SK_ID_CURR', how = 'left')\n\n# Merge polnomial features into testing dataframe\npoly_features_test['SK_ID_CURR'] = app_test['SK_ID_CURR']\napp_test_poly = app_test.merge(poly_features_test, on = 'SK_ID_CURR', how = 'left')\n\n# Align the dataframes\napp_train_poly, app_test_poly = app_train_poly.align(app_test_poly, join = 'inner', axis = 1)\n\n# Print out the new shapes\nprint('Training data with polynomial features shape: ', app_train_poly.shape)\nprint('Testing data with polynomial features shape:  ', app_test_poly.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2.5.2 Domain Knowledge Features\nNow let's go to the domain knowledge features. It really depends on the person who performs this work regarding their \"domain knowledge\" of finance. In this case, let's select a couple of features to slide to machine learning after the long journey. Five features are suggested by [this script](https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction/) and [this one](https://www.kaggle.com/jsaguiar/lightgbm-with-simple-features):\n>\n> - `CREDIT_INCOME_PERCENT`: the percentage of the credit amount relative to a client's income\n> - `ANNUITY_INCOME_PERCENT`: the percentage of the loan annuity relative to a client's income\n> - `CREDIT_TERM`: the length of the payment in months (since the annuity is the monthly amount due\n> - `DAYS_EMPLOYED_PERCENT`: the percentage of the days employed relative to the client's age\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"app_train_domain = app_train.copy()\napp_test_domain = app_test.copy()\n\napp_train_domain['CREDIT_INCOME_PERCENT'] = app_train_domain['AMT_CREDIT'] / app_train_domain['AMT_INCOME_TOTAL']\napp_train_domain['ANNUITY_INCOME_PERCENT'] = app_train_domain['AMT_ANNUITY'] / app_train_domain['AMT_INCOME_TOTAL']\napp_train_domain['CREDIT_TERM'] = app_train_domain['AMT_ANNUITY'] / app_train_domain['AMT_CREDIT']\napp_train_domain['DAYS_EMPLOYED_PERCENT'] = app_train_domain['DAYS_EMPLOYED'] / app_train_domain['DAYS_BIRTH']\n\n\napp_test_domain['CREDIT_INCOME_PERCENT'] = app_test_domain['AMT_CREDIT'] / app_test_domain['AMT_INCOME_TOTAL']\napp_test_domain['ANNUITY_INCOME_PERCENT'] = app_test_domain['AMT_ANNUITY'] / app_test_domain['AMT_INCOME_TOTAL']\napp_test_domain['CREDIT_TERM'] = app_test_domain['AMT_ANNUITY'] / app_test_domain['AMT_CREDIT']\napp_test_domain['DAYS_EMPLOYED_PERCENT'] = app_test_domain['DAYS_EMPLOYED'] / app_test_domain['DAYS_BIRTH']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Visualize New Variables**\nWe should explore these domain knowledge variables visually in a graph. For all of these, we will make the same KDE plot colored by the value of the TARGET"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (12, 20))\n# iterate through the new features\nfor i, feature in enumerate(['CREDIT_INCOME_PERCENT', 'ANNUITY_INCOME_PERCENT', 'CREDIT_TERM', 'DAYS_EMPLOYED_PERCENT']):\n    \n    # create a new subplot for each source\n    plt.subplot(4, 1, i + 1)\n    # plot repaid loans\n    sns.kdeplot(app_train_domain.loc[app_train_domain['TARGET'] == 0, feature], label = 'target == 0')\n    # plot loans that were not repaid\n    sns.kdeplot(app_train_domain.loc[app_train_domain['TARGET'] == 1, feature], label = 'target == 1')\n    \n    # Label the plots\n    plt.title('Distribution of %s by Target Value' % feature)\n    plt.xlabel('%s' % feature); plt.ylabel('Density');\n    \nplt.tight_layout(h_pad = 2.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ## 3. Machine learning - Classification\nFinally! Let's do the machine learning work ;). \n\n> ### 3.1 Machine learning using original features\nIn this section, we are going to use all the previous mentioned features, original and by feature engineering, to train a machine learning model. Several machine learning models will be tried, as said previously, to see which is the right one. "},{"metadata":{},"cell_type":"markdown","source":"#### * Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler, Imputer\n\n# Drop the target from the training data\nif 'TARGET' in app_train:\n    train = app_train.drop(columns = ['TARGET'])\nelse:\n    train = app_train.copy()\n    \n# Feature names\nfeatures = list(train.columns)\n\n# Copy of the testing data\ntest = app_test.copy()\n\n# Median imputation of missing values\nimputer = Imputer(strategy = 'median')\n\n# Scale each feature to 0-1\nscaler = MinMaxScaler(feature_range = (0, 1))\n\n# Fit on the training data\nimputer.fit(train)\n\n# Transform both training and testing data\ntrain = imputer.transform(train)\ntest = imputer.transform(app_test)\n\n# Repeat with the scaler\nscaler.fit(train)\ntrain = scaler.transform(train)\ntest = scaler.transform(test)\n\nprint('Training data shape: ', train.shape)\nprint('Testing data shape: ', test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\n# Make the model with the specified regularization parameter\nlog_reg = LogisticRegression(C = 0.0001)\n\n# Train on the training data\nlog_reg.fit(train, train_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"log_reg_pred = log_reg.predict_proba(test)[:, 1]\n\n# Submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = log_reg_pred\n\n# Save the submission to a csv file\nsubmit.to_csv('log_reg_baseline.csv', index = False)\n\nsubmit.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The logistic regression baseline should score around 0.671 when submitted"},{"metadata":{},"cell_type":"markdown","source":"#### * Random Forest\nThis is a more popular machine learning model for regression/classification problems in the recent two decades. The algorithm has been trying to be parameter friendly to users by simply setting the number of trees (that accumulate to form a 'forest'). Here we use 100 trees as the parameter.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\n# Make the random forest classifier\nrandom_forest = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train on the training data\nrandom_forest.fit(train, train_labels)\n\n# Extract feature importances\nfeature_importance_values = random_forest.feature_importances_\nfeature_importances = pd.DataFrame({'feature': features, 'importance': feature_importance_values})\n\n# Make predictions on the test data\npredictions = random_forest.predict_proba(test)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make a submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = predictions\n\n# Save the submission dataframe\nsubmit.to_csv('random_forest_baseline.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These predictions will also be available when we run the entire notebook.\n\nThis model should score around 0.678 when submitted."},{"metadata":{},"cell_type":"markdown","source":"#### * Support Vector Machine\nLike random forest, SVM is another popular machine learning model, invented a bit earlier in the 1990s. Both RF and SVM used to be the state-of-the-art models in many tasks before deep learning taking many of the fields. Still, RF, SVM and similar traditional models are of the best choices in specific tasks. Important parameters are the penalty coefficient C and standard deviation gamma (for kernel functions). [**](http://)"},{"metadata":{"trusted":true},"cell_type":"code","source":"## I commented this cell, guess why? it is slow if you directly train the data\n\n# from sklearn.svm import SVC\n# svm = SVC(C=1.0)\n# svm.fit(train, train_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# svm_pred = svm.predict_proba(test)[:, 1]\n\n# # Submission dataframe\n# submit = app_test[['SK_ID_CURR']]\n# submit['TARGET'] = log_reg_pred\n\n# # Save the submission to a csv file\n# submit.to_csv('svm_baseline.csv', index = False)\n\n# submit.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.2 Machie learning using engineered features \n\n#### * Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"poly_features_names = list(app_train_poly.columns)\n\n# Impute the polynomial features\nimputer = Imputer(strategy = 'median')\n\npoly_features = imputer.fit_transform(app_train_poly)\npoly_features_test = imputer.transform(app_test_poly)\n\n# Scale the polymnomial features\nscaler = MinMaxScaler(feature_range = (0, 1))\n\npoly_features = scaler.fit_transform(poly_features)\npoly_features_test = scaler.transform(poly_features_test)\n\nrandom_forest_poly = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train on the training data\nrandom_forest_poly.fit(poly_features, train_labels)\n\n# Make predictions on the test data\npredictions = random_forest_poly.predict_proba(poly_features_test)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make a submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = predictions\n\n# Save the submission dataframe\nsubmit.to_csv('random_forest_baseline_engineered.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This model scored 0.678 when submitted to the competition, exactly the same as that without the engineered features. Given these results, it does not appear that our feature construction helped in this case."},{"metadata":{},"cell_type":"markdown","source":"### 3.3 Machine learning with Domain Features"},{"metadata":{},"cell_type":"markdown","source":"#### * Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"app_train_domain = app_train_domain.drop(columns = 'TARGET')\n\ndomain_features_names = list(app_train_domain.columns)\n\n# Impute the domainnomial features\nimputer = Imputer(strategy = 'median')\n\ndomain_features = imputer.fit_transform(app_train_domain)\ndomain_features_test = imputer.transform(app_test_domain)\n\n# Scale the domainnomial features\nscaler = MinMaxScaler(feature_range = (0, 1))\n\ndomain_features = scaler.fit_transform(domain_features)\ndomain_features_test = scaler.transform(domain_features_test)\n\nrandom_forest_domain = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)\n\n# Train on the training data\nrandom_forest_domain.fit(domain_features, train_labels)\n\n# Extract feature importances\nfeature_importance_values_domain = random_forest_domain.feature_importances_\nfeature_importances_domain = pd.DataFrame({'feature': domain_features_names, 'importance': feature_importance_values_domain})\n\n# Make predictions on the test data\npredictions = random_forest_domain.predict_proba(domain_features_test)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make a submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = predictions\n\n# Save the submission dataframe\nsubmit.to_csv('random_forest_baseline_domain.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This scores 0.679 when submitted which probably shows that the engineered features do not help in this model (however they do help in the Gradient Boosting Model at the end of the notebook)."},{"metadata":{},"cell_type":"markdown","source":"> ## 4 Model Interpretation: Feature Importances\nAs a simple method to see which variables are the most relevant, we can look at the feature importances of the random forest. Given the correlations we saw in the exploratory data analysis, we should expect that the most important features are the EXT_SOURCE and the DAYS_BIRTH. We may use these feature importances as a method of dimensionality reduction in future work."},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_feature_importances(df):\n    \"\"\"\n    Plot importances returned by a model. This can work with any measure of\n    feature importance provided that higher importance is better. \n    \n    Args:\n        df (dataframe): feature importances. Must have the features in a column\n        called `features` and the importances in a column called `importance\n        \n    Returns:\n        shows a plot of the 15 most importance features\n        \n        df (dataframe): feature importances sorted by importance (highest to lowest) \n        with a column for normalized importance\n        \"\"\"\n    \n    # Sort features according to importance\n    df = df.sort_values('importance', ascending = False).reset_index()\n    \n    # Normalize the feature importances to add up to one\n    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n\n    # Make a horizontal bar chart of feature importances\n    plt.figure(figsize = (10, 6))\n    ax = plt.subplot()\n    \n    # Need to reverse the index to plot most important on top\n    ax.barh(list(reversed(list(df.index[:15]))), \n            df['importance_normalized'].head(15), \n            align = 'center', edgecolor = 'k')\n    \n    # Set the yticks and labels\n    ax.set_yticks(list(reversed(list(df.index[:15]))))\n    ax.set_yticklabels(df['feature'].head(15))\n    \n    # Plot labeling\n    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n    plt.show()\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show the feature importances (from random forest) for the default features\nfeature_importances_sorted = plot_feature_importances(feature_importances)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As expected, the most important features are those dealing with `EXT_SOURCE` and `DAYS_BIRTH`. I'll simply copy more from this [notebook](https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction/) since it's realy easily understandable. \n> We see that there are only a handful of features with a significant importance to the model, which suggests we may be able to drop many of the features without a decrease in performance (and we may even see an increase in performance.) Feature importances are not the most sophisticated method to interpret a model or perform dimensionality reduction, but they let us start to understand what factors our model takes into account when it makes predictions."},{"metadata":{},"cell_type":"markdown","source":"> ## 5. Feature Engineering does help\nIt is hereby definitely a good idea to show that feature engineering helps improve performance after talking so much about features, right? They do improve a lot the performance in predicting client behaviors with other classification models. "},{"metadata":{},"cell_type":"markdown","source":"#### * Light Gradient Boosting Machine - with original features"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb\nimport gc\n\ndef model(features, test_features, encoding = 'ohe', n_folds = 5):\n    \n    \"\"\"Train and test a light gradient boosting model using\n    cross validation. \n    \n    Parameters\n    --------\n        features (pd.DataFrame): \n            dataframe of training features to use \n            for training a model. Must include the TARGET column.\n        test_features (pd.DataFrame): \n            dataframe of testing features to use\n            for making predictions with the model. \n        encoding (str, default = 'ohe'): \n            method for encoding categorical variables. Either 'ohe' for one-hot encoding or 'le' for integer label encoding\n            n_folds (int, default = 5): number of folds to use for cross validation\n        \n    Return\n    --------\n        submission (pd.DataFrame): \n            dataframe with `SK_ID_CURR` and `TARGET` probabilities\n            predicted by the model.\n        feature_importances (pd.DataFrame): \n            dataframe with the feature importances from the model.\n        valid_metrics (pd.DataFrame): \n            dataframe with training and validation metrics (ROC AUC) for each fold and overall.\n        \n    \"\"\"\n    \n    # Extract the ids\n    train_ids = features['SK_ID_CURR']\n    test_ids = test_features['SK_ID_CURR']\n    \n    # Extract the labels for training\n    labels = features['TARGET']\n    \n    # Remove the ids and target\n    features = features.drop(columns = ['SK_ID_CURR', 'TARGET'])\n    test_features = test_features.drop(columns = ['SK_ID_CURR'])\n    \n    \n    # One Hot Encoding\n    if encoding == 'ohe':\n        features = pd.get_dummies(features)\n        test_features = pd.get_dummies(test_features)\n        \n        # Align the dataframes by the columns\n        features, test_features = features.align(test_features, join = 'inner', axis = 1)\n        \n        # No categorical indices to record\n        cat_indices = 'auto'\n    \n    # Integer label encoding\n    elif encoding == 'le':\n        \n        # Create a label encoder\n        label_encoder = LabelEncoder()\n        \n        # List for storing categorical indices\n        cat_indices = []\n        \n        # Iterate through each column\n        for i, col in enumerate(features):\n            if features[col].dtype == 'object':\n                # Map the categorical features to integers\n                features[col] =  label_encoder.fit_transform(np.array(features[col].astype(str)).reshape((-1,)))\n                test_features[col] = label_encoder.transform(np.array(test_features[col].astype(str)).reshape((-1,)))\n\n                # Record the categorical indices\n                cat_indices.append(i)\n    \n    # Catch error if label encoding scheme is not valid\n    else:\n        raise ValueError(\"Encoding must be either 'ohe' or 'le'\")\n        \n    print('Training Data Shape: ', features.shape)\n    print('Testing Data Shape: ', test_features.shape)\n    \n    # Extract feature names\n    feature_names = list(features.columns)\n    \n    # Convert to np arrays\n    features = np.array(features)\n    test_features = np.array(test_features)\n    \n    # Create the kfold object\n    k_fold = KFold(n_splits = n_folds, shuffle = True, random_state = 50)\n    \n    # Empty array for feature importances\n    feature_importance_values = np.zeros(len(feature_names))\n    \n    # Empty array for test predictions\n    test_predictions = np.zeros(test_features.shape[0])\n    \n    # Empty array for out of fold validation predictions\n    out_of_fold = np.zeros(features.shape[0])\n    \n    # Lists for recording validation and training scores\n    valid_scores = []\n    train_scores = []\n    \n    # Iterate through each fold\n    for train_indices, valid_indices in k_fold.split(features):\n        \n        # Training data for the fold\n        train_features, train_labels = features[train_indices], labels[train_indices]\n        # Validation data for the fold\n        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n        \n        # Create the model\n        model = lgb.LGBMClassifier(n_estimators=10000, objective = 'binary', \n                                   class_weight = 'balanced', learning_rate = 0.05, \n                                   reg_alpha = 0.1, reg_lambda = 0.1, \n                                   subsample = 0.8, n_jobs = -1, random_state = 50)\n        \n        # Train the model\n        model.fit(train_features, train_labels, eval_metric = 'auc',\n                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n                  eval_names = ['valid', 'train'], categorical_feature = cat_indices,\n                  early_stopping_rounds = 100, verbose = 200)\n        \n        # Record the best iteration\n        best_iteration = model.best_iteration_\n        \n        # Record the feature importances\n        feature_importance_values += model.feature_importances_ / k_fold.n_splits\n        \n        # Make predictions\n        test_predictions += model.predict_proba(test_features, num_iteration = best_iteration)[:, 1] / k_fold.n_splits\n        \n        # Record the out of fold predictions\n        out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]\n        \n        # Record the best score\n        valid_score = model.best_score_['valid']['auc']\n        train_score = model.best_score_['train']['auc']\n        \n        valid_scores.append(valid_score)\n        train_scores.append(train_score)\n        \n        # Clean up memory\n        gc.enable()\n        del model, train_features, valid_features\n        gc.collect()\n        \n    # Make the submission dataframe\n    submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_predictions})\n    \n    # Make the feature importance dataframe\n    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n    \n    # Overall validation score\n    valid_auc = roc_auc_score(labels, out_of_fold)\n    \n    # Add the overall scores to the metrics\n    valid_scores.append(valid_auc)\n    train_scores.append(np.mean(train_scores))\n    \n    # Needed for creating dataframe of validation scores\n    fold_names = list(range(n_folds))\n    fold_names.append('overall')\n    \n    # Dataframe of validation scores\n    metrics = pd.DataFrame({'fold': fold_names,\n                            'train': train_scores,\n                            'valid': valid_scores}) \n    \n    return submission, feature_importances, metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission, fi, metrics = model(app_train, app_test)\nprint('Baseline metrics')\nprint(metrics)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fi_sorted = plot_feature_importances(fi)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('baseline_lgb.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This submission should score about 0.735 on the leaderboard."},{"metadata":{},"cell_type":"markdown","source":"#### * Light Gradient Boosting Machine - with original features"},{"metadata":{"trusted":true},"cell_type":"code","source":"app_train_domain['TARGET'] = train_labels\n\n# Test the domain knolwedge features\nsubmission_domain, fi_domain, metrics_domain = model(app_train_domain, app_test_domain)\nprint('Baseline with domain knowledge features metrics')\nprint(metrics_domain)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fi_sorted = plot_feature_importances(fi_domain)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Again, we see that some of our features made it into the most important. Going forward, we will need to think about whatother domain knowledge features may be useful for this problem (or we should consult someone who knows more about the financial industry!)"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_domain.to_csv('baseline_lgb_domain_features.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">This model scores about 0.754 when submitted to the public leaderboard indicating that the domain features do improve the performance! Feature engineering is going to be a critical part of this competition (as it is for all machine learning problems)"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}