{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"![homecredit1.jpg](attachment:d674c640-78ef-41e4-854a-dc555ed1cf6c.jpg)\n\n<img src=\"https://storage.googleapis.com/kaggle-competitions/kaggle/9120/logos/thumb76_76.png?t=2018-04-02-23-45-04\" align=\"left\" width = \"100px\"/>\n\n<h1> Home Credit Default Risk</h1>","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Data Description\nMany people struggle to get loans due to insufficient or non-existent credit histories. And, unfortunately, this population is often taken advantage of by untrustworthy lenders.\n\nHome Credit strives to broaden financial inclusion for the unbanked population by providing a positive and safe borrowing experience. In order to make sure this underserved population has a positive loan experience, Home Credit makes use of a variety of alternative data--including telco and transactional information--to predict their clients' repayment abilities.\n\nWhile Home Credit is currently using various statistical and machine learning methods to make these predictions, they're challenging Kagglers to help them unlock the full potential of their data. Doing so will ensure that clients capable of repayment are not rejected and that loans are given with a principal, maturity, and repayment calendar that will empower their clients to be successful.\n\n## Datasets\n\n<center> <div style=\"width:70%\"><img src=\"https://storage.googleapis.com/kaggle-media/competitions/home-credit/home_credit.png\"></img></div></center>","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"######################################\n# Kütüphanelerin import edilmesi\n#####################################\n\nimport numpy as np\nimport pandas as pd\nimport gc\nimport time\nfrom contextlib import contextmanager\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.model_selection import KFold, StratifiedKFold\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nimport re\n\n\n@contextmanager\ndef timer(title):\n    t0 = time.time()\n    yield\n    print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))\n    \n    \n#######################\n# Fonksiyonlar\n#######################\n\n# veri setine genel bakış\ndef check_df(dataframe):\n    print(\"##################### Shape #####################\")\n    print(dataframe.shape)\n    print(\"##################### Types #####################\")\n    print(dataframe.dtypes)\n    \n# Değişkenlerin türlerinin belirlenmesi\ndef grab_col_names(dataframe, cat_th=10, car_th=20):\n    # cat_cols, cat_but_car\n    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and\n                   dataframe[col].dtypes != \"O\"]\n    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and\n                   dataframe[col].dtypes == \"O\"]\n    cat_cols = cat_cols + num_but_cat\n    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n\n    # num_cols\n    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]\n    num_cols = [col for col in num_cols if col not in num_but_cat]\n\n    #print(f\"Observations: {dataframe.shape[0]}\")\n    #print(f\"Variables: {dataframe.shape[1]}\")\n    #print(f'cat_cols: {len(cat_cols)}')\n    #print(f'num_cols: {len(num_cols)}')\n    #print(f'cat_but_car: {len(cat_but_car)}')\n    #print(f'num_but_cat: {len(num_but_cat)}')\n    return cat_cols, num_cols, cat_but_car\n\n# Eksik gözlemler\ndef missing_values_table(dataframe, na_name=False):\n    na_columns = [col for col in dataframe.columns if dataframe[col].isnull().sum() > 0]\n    n_miss = dataframe[na_columns].isnull().sum().sort_values(ascending=False)\n    ratio = (dataframe[na_columns].isnull().sum() / dataframe.shape[0] * 100).sort_values(ascending=False)\n    missing_df = pd.concat([n_miss, np.round(ratio, 2)], axis=1, keys=['n_miss', 'ratio'])\n    print(missing_df, end=\"\\n\")\n    if na_name:\n        return na_columns\n    \n# Kategorik değişkenlerin incelenmesi\ndef cat_summary(dataframe, col_name, plot=False):\n    print(pd.DataFrame({col_name: dataframe[col_name].value_counts(),\n                        \"Ratio\": 100 * dataframe[col_name].value_counts() / len(dataframe)}))\n    print(\"##########################################\")\n    if plot:\n        plt.figure(figsize=(10,5))\n        sns.countplot(x=dataframe[col_name], data=dataframe)\n        plt.show()\n        \n# Nümerik değişkenlerin incelenmesi\ndef num_summary(dataframe, numerical_col, plot=False):\n    quantiles = [0.05, 0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 0.90, 0.95, 0.99]\n    print(dataframe[numerical_col].describe(quantiles).T)\n    if plot:\n        dataframe[numerical_col].hist(bins=20)\n        plt.xlabel(numerical_col)\n        plt.title(numerical_col)\n        plt.show()\n        \n        \n# Korelasyonlar\ndef high_correlation(data, remove=['SK_ID_CURR', 'SK_ID_BUREAU'], corr_coef=\"pearson\", corr_value = 0.7):\n    if len(remove) > 0:\n        cols = [x for x in data.columns if (x not in remove)]\n        c = data[cols].corr(method=corr_coef)\n    else:\n        c = data.corr(method=corr_coef)\n\n    for i in c.columns:\n        cr = c.loc[i].loc[(c.loc[i] >= corr_value) | (c.loc[i] <= -corr_value)].drop(i)\n        if len(cr) > 0:\n            print(i)\n            print(\"-------------------------------\")\n            print(cr.sort_values(ascending=False))\n            print(\"\\n\")\n\n# Rare Encoding\ndef rare_encoder(dataframe, rare_perc, cat_cols):\n   \n    rare_columns = [col for col in cat_cols if\n                    (dataframe[col].value_counts() / len(dataframe) < rare_perc).sum() > 1]\n\n    for col in rare_columns:\n        tmp = dataframe[col].value_counts() / len(dataframe)\n        rare_labels = tmp[tmp < rare_perc].index\n        dataframe[col] = np.where(dataframe[col].isin(rare_labels), 'Rare', dataframe[col])\n\n    return dataframe\n\n# One-hot Encoding \ndef one_hot_encoder(df, nan_as_category = True):\n    original_columns = list(df.columns)\n    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n    df = pd.get_dummies(df, columns= categorical_columns, dummy_na= nan_as_category)\n    new_columns = [c for c in df.columns if c not in original_columns]\n    return df, new_columns\n#########################\n# Application_train_test\n#########################\n\ndef application_train_test(num_rows = None, nan_as_category = False):\n    # Veri setinin okutulması\n    df = pd.read_csv('../input/home-credit-default-risk/application_train.csv', nrows= num_rows)\n    test_df = pd.read_csv('../input/home-credit-default-risk/application_test.csv', nrows= num_rows)\n    print(\"Train samples: {}, test samples: {}\".format(len(df), len(test_df)))\n    df = df.append(test_df).reset_index()\n    \n    # Cinsiyeti belirtilmeyen 4 kişi var bunları çıkarıyoruz.\n    df = df[df['CODE_GENDER'] != 'XNA']\n    # Medeni durumu unknown olan 1 kişi var bunu dropladık.\n    df = df[df['NAME_FAMILY_STATUS'] != \"Unknown\" ]\n    # NaN values for DAYS_EMPLOYED: 365243 -> nan\n    df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace=True)\n    \n    ########\n    # RARE\n    ########\n    # NAME_INCOME_TYPE değişkeninin 4 sınıfının frekansı diğerlerine göre düşük olduğunu gözlemledik.\n    # Bu nedenle bu 4 sınıfı kendilerine en yakın olabilecek sınıfın için dahil ettik.\n    # Yani rare sınıfını diğerlerine eklemiş olduk.\n    df.loc[df['NAME_INCOME_TYPE'] == 'Businessman', 'NAME_INCOME_TYPE'] = 'Commercial associate'\n    df.loc[df['NAME_INCOME_TYPE'] == 'Maternity leave', 'NAME_INCOME_TYPE'] = 'Pensioner'\n    df.loc[df['NAME_INCOME_TYPE'] == 'Student', 'NAME_INCOME_TYPE'] = 'State servant'\n    df.loc[df['NAME_INCOME_TYPE'] == 'Unemployed', 'NAME_INCOME_TYPE'] = 'Pensioner'\n    \n    # ORGANIZATION_TYPE\n    df[\"ORGANIZATION_TYPE\"] = np.where(df[\"ORGANIZATION_TYPE\"].str.contains(\"Business Entity\"), \n                                       \"Business_Entity\", df[\"ORGANIZATION_TYPE\"])\n    df[\"ORGANIZATION_TYPE\"] = np.where(df[\"ORGANIZATION_TYPE\"].str.contains(\"Industry\"), \n                                       \"Industry\", df[\"ORGANIZATION_TYPE\"])\n    df[\"ORGANIZATION_TYPE\"] = np.where(df[\"ORGANIZATION_TYPE\"].str.contains(\"Trade\"),\n                                       \"Trade\", df[\"ORGANIZATION_TYPE\"])\n    df[\"ORGANIZATION_TYPE\"] = np.where(df[\"ORGANIZATION_TYPE\"].str.contains(\"Transport\"),\n                                       \"Transport\", df[\"ORGANIZATION_TYPE\"])\n    df[\"ORGANIZATION_TYPE\"] = np.where(df[\"ORGANIZATION_TYPE\"].isin([\"School\", \"Kindergarten\", \"University\"]),\n                                       \"Education\", df[\"ORGANIZATION_TYPE\"])\n    df[\"ORGANIZATION_TYPE\"] = np.where(df[\"ORGANIZATION_TYPE\"].isin([\"Emergency\",\"Police\", \"Medicine\",\"Goverment\", \"Postal\", \"Military\", \"Security Ministries\", \"Legal Services\"]), \"Official\", df[\"ORGANIZATION_TYPE\"])\n    df[\"ORGANIZATION_TYPE\"] = np.where(df[\"ORGANIZATION_TYPE\"].isin([\"Bank\", \"Insurance\"]),\n                                       \"Finance\", df[\"ORGANIZATION_TYPE\"])\n    df[\"ORGANIZATION_TYPE\"] = np.where(df[\"ORGANIZATION_TYPE\"].str.contains(\"Goverment\"), \n                                       \"Government\", df[\"ORGANIZATION_TYPE\"])\n    df[\"ORGANIZATION_TYPE\"] = np.where(df[\"ORGANIZATION_TYPE\"].isin([\"Realtor\", \"Housing\"]), \"Realty\", df[\"ORGANIZATION_TYPE\"])\n    df[\"ORGANIZATION_TYPE\"] = np.where(df[\"ORGANIZATION_TYPE\"].isin([\"Hotel\", \"Restaurant\",\"Services\"]), \"TourismFoodSector\", df[\"ORGANIZATION_TYPE\"])\n    df[\"ORGANIZATION_TYPE\"] = np.where(df[\"ORGANIZATION_TYPE\"].isin([\"Cleaning\",\"Electricity\", \"Telecom\", \"Mobile\", \"Advertising\", \"Religion\", \"Culture\"]), \"Other\", df[\"ORGANIZATION_TYPE\"])\n    \n    \n    # OCCUPATION_TYPE\n    df[\"OCCUPATION_TYPE\"] = np.where(df[\"OCCUPATION_TYPE\"].isin([\"Low-skill Laborers\", \"Cooking staff\", \"Security staff\", \"Private service staff\", \"Cleaning staff\", \"Waiters/barmen staff\"]), \"Low_skill_staff\", df[\"OCCUPATION_TYPE\"])\n    df[\"OCCUPATION_TYPE\"] = np.where(df[\"OCCUPATION_TYPE\"].isin([\"IT staff\", \"High skill tech staff\"]), \"High_skill_staff\", df[\"OCCUPATION_TYPE\"])\n    df[\"OCCUPATION_TYPE\"] = np.where(df[\"OCCUPATION_TYPE\"].isin([\"Secretaries\", \"HR staff\",\"Realty agents\"]), \"Others\", df[\"OCCUPATION_TYPE\"])\n\n    # NAME_TYPE_SUITE\n    rare_list = [\"NAME_TYPE_SUITE\"]\n    rare_encoder(df, 0.01, rare_list)\n    \n    # NAME_EDUCATION_TYPE\n    # Akademik derecenin frekansı az olduğu için bununla yüksek eğitimi aynı sınıfa aldık.\n    df[\"NAME_EDUCATION_TYPE\"] = np.where(df[\"NAME_EDUCATION_TYPE\"] == \"Academic degree\",\n                                         \"Higher education\", df[\"NAME_EDUCATION_TYPE\"])\n    \n    df[\"NAME_EDUCATION_TYPE\"] = np.where(df[\"NAME_EDUCATION_TYPE\"].str.contains(\"Secondary / secondary special\"),\n                                         \"Secondary_secondary_special\", df[\"ORGANIZATION_TYPE\"])\n    \n    # NAME_FAMILY_STATUS\n    df[\"NAME_FAMILY_STATUS\"] = np.where(df[\"NAME_FAMILY_STATUS\"].str.contains(\"Single / not married\"),\n                                        \"Single_not_married\", df[\"NAME_FAMILY_STATUS\"])\n    \n    \n#   # NAME_HOUSING_TYPE\n    df[\"NAME_HOUSING_TYPE\"] = np.where(df[\"NAME_HOUSING_TYPE\"].str.contains(\"House / apartment\"),\n                                       \"House_apartment\", df[\"NAME_HOUSING_TYPE\"])\n    \n    # NAME_TYPE_SUITE\n    df[\"NAME_TYPE_SUITE\"] = np.where(df[\"NAME_TYPE_SUITE\"].str.contains(\"Spouse, partner\"),\n                                       \"Spouse_partner\", df[\"NAME_TYPE_SUITE\"])\n    \n    \n    # NAME_CONTRACT_TYPE\n    # Kategorik olan ama cinsiyet gibi 0 ve 1 olarak kodlanacak değişkenlere binary encode yaptık.\n    for bin_feature in [\"NAME_CONTRACT_TYPE\", 'CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']:\n        df[bin_feature], uniques = pd.factorize(df[bin_feature])\n    \n    # WEEKDAY_APPR_PROCESS_START\n    # Gün isimlerini 1,2,3....,7 olarak değiştireceğiz.\n    # Daha sonra günler döngüsel yapıda oldukları için bunlara cycle encode uygulayacağız.\n    # Asıl değişkenleri silmedik feature importance da bak!\n    weekday_dict = {'MONDAY': 1, 'TUESDAY': 2, 'WEDNESDAY': 3, 'THURSDAY': 4, 'FRIDAY': 5, 'SATURDAY': 6, 'SUNDAY': 7}\n    df.replace({'WEEKDAY_APPR_PROCESS_START': weekday_dict}, inplace=True)\n    # Cycle encode\n    df['NEW_WEEKDAY_APPR_PROCESS_START' + \"_SIN\"] = np.sin(2 * np.pi * df[\"WEEKDAY_APPR_PROCESS_START\"]/7)\n    df[\"NEW_WEEKDAY_APPR_PROCESS_START\" + \"_COS\"] = np.cos(2 * np.pi * df[\"WEEKDAY_APPR_PROCESS_START\"]/7)\n    \n    # HOUR_APPR_PROCESS_START\n    # değişken müşterinin hangi saatte başvurduğu bilgisini veriyordu.\n    # Saat bilgisi de yine döngüsel olduğu için buna da cycle encode yapıyoruz.\n    df['NEW_HOUR_APPR_PROCESS_START' + \"_SIN\"] = np.sin(2 * np.pi * df[\"HOUR_APPR_PROCESS_START\"]/23)\n    df[\"NEW_HOUR_APPR_PROCESS_START\" + \"_COS\"] = np.cos(2 * np.pi * df[\"HOUR_APPR_PROCESS_START\"]/23)\n  \n    ###########\n    # DROP\n    ###########\n    # FLAG_MOBIL ve FLAG_CONT_MOBILE değişkenlerinde iki alt sınıfı var ve birinin frekansı çok az.\n    # Yani bilgi taşımayan değişken bu nedenle drop ediyoruz.\n    drop_cols = [\"FONDKAPREMONT_MODE\", \"WALLSMATERIAL_MODE\", \"HOUSETYPE_MODE\",\n                 \"EMERGENCYSTATE_MODE\",\"FLAG_MOBIL\", \"FLAG_EMP_PHONE\",\"FLAG_WORK_PHONE\", \"FLAG_CONT_MOBILE\", \"FLAG_PHONE\", \"FLAG_EMAIL\"]\n    df.drop(drop_cols, axis = 1, inplace = True)\n    \n    # OBS_30_CNT_SOCIAL_CIRCLE,OBS_60_CNT_SOCIAL_CIRCLE\n    df.drop(['OBS_30_CNT_SOCIAL_CIRCLE','OBS_60_CNT_SOCIAL_CIRCLE'], axis = 1, inplace = True)\n    \n    # REGION\n    # Bu değişkenler bölge ve şehir bazında ayrı ayrı puan veriyordu\n    # Biz bunları toplayarak tek bir değişken elde ettik ve diğerlerini dropladık.\n    cols = [\"REG_REGION_NOT_LIVE_REGION\",\"REG_REGION_NOT_WORK_REGION\", \"LIVE_REGION_NOT_WORK_REGION\", \n            \"REG_CITY_NOT_LIVE_CITY\",\"REG_CITY_NOT_WORK_CITY\",\"LIVE_CITY_NOT_WORK_CITY\"]\n    df[\"NEW_REGION\"] = df[cols].sum(axis = 1)\n    df.drop(cols, axis = 1, inplace = True)\n    \n    # Flag_DOCUMENT\n    # Bu değişkenler her dökümanın ayrı ayrı verilip verilmediği bilgisini veriyordu.\n    # Biz bunları toplayarak tek bir değişken elde ettik,yani totalde verilen belge sayısını hesapladık\n    # ve diğerlerini dropladık.\n    docs = [col for col in df.columns if 'FLAG_DOC' in col]\n    df['NEW_DOCUMENT'] = df[docs].sum(axis=1)\n    df.drop(docs, axis = 1, inplace = True)\n    \n    ##########################\n    # FEATURE ENGINEERING\n    ##########################\n    # 1. Müşteri başvurudan ne kadar önce işe başladı(gün) / müşterinin yaşı(gün)\n    df['NEW_DAYS_EMPLOYED_RATIO'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n\n    # 2. Müşterinin toplam geliri / kredi tutarı\n    df['NEW_INCOME_CREDIT_RATIO'] = df['AMT_INCOME_TOTAL'] / df['AMT_CREDIT']\n\n    # 3. Müşterinin toplam geliri / aile üyesi süresi\n    # Ailede kişi başına ne kadar gelir var bunu ortaya çıkarıyor.\n    df['NEW_INCOME_PER_RATIO'] = df['AMT_INCOME_TOTAL'] / df['CNT_FAM_MEMBERS']\n\n    # 4. Kredinin yıllık ödemesi / Müşterinin geliri\n    # Eğer bu değer 0-1 arasındaysa iyi yani geliri kredi ödemesinden fazla\n    # Eğer bu değer 1 ise kötü yani geliri kredi ödemesinden daha az.\n    df['NEW_ANNUITY_INCOME_RATIO'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']\n\n    # 5. Kredinin yıllık ödemesi / kredi tutarı\n    df['NEW_PAYMENT_RATIO'] = df['AMT_ANNUITY'] / df['AMT_CREDIT']\n    \n    # 6. EXT_SOURCE değişkenleri dışarıdan alınan puanlardı.Ortalamaları ile yeni bir değişken oluşturduk.\n    df[\"NEW_EXTSOURCE_MEAN\"] = df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis=1)\n\n    # 7. Bu değişkenleri çarparak ağırlıklı yeni bir değişken oluşturduk.\n    df['NEW_EXTSOURCES_WPOINT'] = df['EXT_SOURCE_1'] * df['EXT_SOURCE_2'] * df['EXT_SOURCE_3']\n    \n    # 8. Kredi ile satın alınacak malların fiyatı / toplam kredi tutarı\n    # Bu oran 0-1 arasında ise müşterinin alacağı mal fiyatından daha fazla kredi çekmesi demek.\n    # Bu oranın 1 olması demek müşterinin ihtiyacı kadar kredi çektiği anlamına gelir.\n    # Bu oran 1 den büykse müşteri ihtiyacından daha az kredi çekmiş demektir.\n    df[\"NEW_GOODS_CREDIT_RATIO\"] = df[\"AMT_GOODS_PRICE\"] / df[\"AMT_CREDIT\"]\n    \n    # 9.Yukarıdaki değişkenle bağlantılı olarak:\n    # Bu fark 0 dan büyükse ihtiyacından az kredi çekmiş\n    # 0 olursa ihtiyacı kadar çekmiş\n    # 0 dan küçükse ihtiyacından fazla çekmiş\n    df[\"NEW_GOODS_CREDIT_DIFF\"] = df[\"AMT_GOODS_PRICE\"] - df[\"AMT_CREDIT\"]\n    \n    # 10. (Kredi ile satın alınacak malların fiyatı / toplam kredi tutarı) / toplam gelir\n    df[\"NEW_GOODS_CREDIT_DIFF_RATIO\"] = (df[\"AMT_GOODS_PRICE\"] - df[\"AMT_CREDIT\"]) / df[\"AMT_INCOME_TOTAL\"]\n    \n    # 11. Toplam gelir / müşterinin yaşı(gün cinsinden)\n    df['NEW_INCOME_BIRTH_RATIO'] = df['AMT_INCOME_TOTAL'] / df['DAYS_BIRTH']\n    \n    # 12. Kişinin DAYS_BIRTH değişkeni gün cinsindn yaşını veriyordu.\n    # Ama değerler - (çünkü şu kadar gün önce doğmuş bilgisini veriyor.)\n    # Bu yüzden müşterinin yaşını bulmak için - ile çarpıp 360 a böleceğiz.\n    df[\"NEW_DAYS_BIRTH\"] = round(df[\"DAYS_BIRTH\"]* -1 / 365)\n    \n    # 13. Yaşlara göre müşterileri segmentlere ayırma\n    df.loc[df[\"NEW_DAYS_BIRTH\"] <= 34 ,\"NEW_SEGMENT_AGE\"] = \"Young\"\n    df.loc[(df[\"NEW_DAYS_BIRTH\"] > 34)&(df[\"NEW_DAYS_BIRTH\"] <= 54) ,\"NEW_SEGMENT_AGE\"] = \"Middle_Age\"\n    df.loc[(df[\"NEW_DAYS_BIRTH\"] > 54),\"NEW_SEGMENT_AGE\"] = \"Old\"\n    \n    # 14. Gelire göre müşterileri segmentlere ayırma\n    df.loc[df[\"AMT_INCOME_TOTAL\"] <= 112500 ,\"NEW_SEGMENT_INCOME\"] = \"Low_Income\"\n    df.loc[(df[\"AMT_INCOME_TOTAL\"] > 112500)&(df[\"AMT_INCOME_TOTAL\"] <= 225000) ,\"NEW_SEGMENT_INCOME\"] = \"Middle_Income\"\n    df.loc[(df[\"AMT_INCOME_TOTAL\"] > 225000),\"NEW_SEGMENT_INCOME\"] = \"High_Income\"\n    \n    # 15. Kişi başvuru yaparken kimle birlikteydi?\n    df.loc[df['NAME_TYPE_SUITE'] == 'Unaccompanied', 'NEW_TYPE_SUITE_CAT'] = 0\n    df.loc[df['NAME_TYPE_SUITE'] != 'Unaccompanied', 'NEW_TYPE_SUITE_CAT'] = 1\n    df.loc[df['NAME_TYPE_SUITE'].isnull(), 'NEW_TYPE_SUITE_CAT'] = np.nan\n    \n    # 16. Elimizde müşterinin çevresinde 30 ve 60 gün temerrüde düşmüş kişi sayısı bilgisini veren iki\n    # değişken vardı. Biz 30 ve 60 günü birleştirerek;\n    # müşterinin çevresinde temerrüde düşen varsa 1 etiketlesin\n    # temerrüde düşen yoksa 0 etiketlesin dedik.\n    df.loc[(df['DEF_30_CNT_SOCIAL_CIRCLE'] > 0) & (df['DEF_60_CNT_SOCIAL_CIRCLE'] > 0),\n           'NEW_DEF_30_60_SOCIAL_CIRCLE'] = 1\n    df.loc[(df['DEF_30_CNT_SOCIAL_CIRCLE'] > 0) & (df['DEF_60_CNT_SOCIAL_CIRCLE'] == 0),\n           'NEW_DEF_30_60_SOCIAL_CIRCLE'] = 1\n    df.loc[(df['DEF_30_CNT_SOCIAL_CIRCLE'] == 0) & (df['DEF_60_CNT_SOCIAL_CIRCLE'] > 0),\n           'NEW_DEF_30_60_SOCIAL_CIRCLE'] = 1\n    df.loc[(df['DEF_30_CNT_SOCIAL_CIRCLE'] == 0) & (df['DEF_60_CNT_SOCIAL_CIRCLE'] == 0),\n           'NEW_DEF_30_60_SOCIAL_CIRCLE'] = 0\n    \n    ########################\n    # One-Hot Encoding\n    ########################\n    df, cat_cols = one_hot_encoder(df, nan_as_category=False)\n    \n    # Dropping feature named index\n    df.drop('index', axis=1, inplace=True)\n    \n    del test_df\n    gc.collect()\n    #print(df.columns.tolist())\n    return df\n# application_train_test(num_rows = None, nan_as_category = False)\n###############################\n# Bureau and Bureau Balance\n##############################\n\ndef bureau_and_balance(num_rows = None, nan_as_category = True):\n    df = pd.read_csv(\"../input/home-credit-default-risk/bureau_balance.csv\", nrows = num_rows)\n    bureau = pd.read_csv('../input/home-credit-default-risk/bureau.csv')\n    #cat_cols, num_cols, cat_but_car = grab_col_names(df)\n    df,df_cat=one_hot_encoder(df)\n    \n    # Bureau balance: Perform aggregations and merge with bureau.csv\n    agg_list = {'MONTHS_BALANCE': ['min', 'max', 'size'] }\n    for col in df_cat:\n        agg_list[col] = ['mean','sum']\n\n    bb_agg = df.groupby(\"SK_ID_BUREAU\").agg(agg_list)\n    # Degisken isimlerinin yeniden adlandirilmasi \n    bb_agg.columns = pd.Index([col[0] + \"_\" + col[1].upper() for col in bb_agg.columns.tolist()])\n    # New feature\n    bb_agg['NEW_STATUS_SCORE'] = bb_agg['STATUS_1_SUM'] + bb_agg['STATUS_2_SUM']^2 + bb_agg['STATUS_3_SUM']^3 + bb_agg['STATUS_4_SUM']^4 + bb_agg['STATUS_5_SUM']^5\n    \n    bureau_and_bb = bureau.join(bb_agg, how='left', on='SK_ID_BUREAU')\n    bureau.drop(['SK_ID_BUREAU'], axis=1, inplace= True)\n    del df, bb_agg\n    gc.collect()\n\n    cat_cols, num_cols, cat_but_car = grab_col_names(bureau_and_bb)\n    rare_encoder(bureau_and_bb,0.2,cat_cols)\n    \n    #CREDIT_ACTIVE degiskeninin sinif sayisini 2'ye düşürdük (Closed-Active)\n    bureau_and_bb['CREDIT_ACTIVE'] = bureau_and_bb['CREDIT_ACTIVE'].replace(\"Rare\", 'Active')\n    #CREDIT_CURRENCY değişkenin 1 sınıfı, veri setinin %99'unu kapladığı için yani dengesiz veri olduğu için anlamsız bilgi taşır.Bu yüzden çıkaracağız.\n    bureau_and_bb.drop(\"CREDIT_CURRENCY\", inplace = True, axis = 1)\n    \n    #######################\n    # FEATURE ENGINEERING\n    #######################\n    # 1. Active ve Closed Krediler için kredi erken kapanmışmı? \n    # Eğer kredi durumu aktifse ve DAYS_CREDIT_ENDDATE değişkeni 0 dan küçükse(yani - olması\n    # o kadar gün önce sona erdiği anlamına gelir.) bu kişi kredisini erken kapatmıştır.\n    # Closed da da kredisi kapanmış ama erken ödemesi erken bitmiş olanlar var.\n    bureau_and_bb.loc[(bureau_and_bb['CREDIT_ACTIVE'] == 'Active') & (bureau_and_bb['DAYS_CREDIT_ENDDATE'] < 0), 'NEW_EARLY_ACTİVE'] = 1\n    bureau_and_bb.loc[(bureau_and_bb['CREDIT_ACTIVE'] == 'Closed') & (abs(bureau_and_bb['DAYS_CREDIT_ENDDATE']) < abs(bureau_and_bb['DAYS_ENDDATE_FACT']) ), 'NEW_EARLY_CLOSED'] = 1\n    \n    # 2. Uzatılmış Kredilerin 1 ile değiştirilmesi\n    bureau_and_bb[\"NEW_CNT_CREDIT_PROLONG_CAT\"] = bureau_and_bb.loc[:,'CNT_CREDIT_PROLONG']\n    prolong = [1,2,3,4,5,6,7,8,9]\n    bureau_and_bb[\"NEW_CNT_CREDIT_PROLONG_CAT\"] = bureau_and_bb['NEW_CNT_CREDIT_PROLONG_CAT'].replace(prolong, 1)\n    \n    # 3. Kişi Kaç farklı kredi tipi almış\n    temp_bu = bureau_and_bb[['SK_ID_CURR', 'CREDIT_TYPE']].groupby(by=['SK_ID_CURR'])['CREDIT_TYPE'].nunique().reset_index().rename(index=str, columns={'CREDIT_TYPE': 'NEW_BUREAU_LOAN_TYPES'})\n    bureau_and_bb = bureau_and_bb.merge(temp_bu, on=['SK_ID_CURR'], how='left')\n    \n    # 4. Borç Oranı\n    # Kredi Bürosuna olan mevcut borç / kredi bürosu için mevcut kredi \n    # 1 le toplamamızın sebebi tanımsızlık olmasın diye\n    bureau_and_bb['NEW_DEPT_RATIO'] = bureau_and_bb['AMT_CREDIT_SUM_DEBT'] / (bureau_and_bb['AMT_CREDIT_SUM']+1)\n\n    # 5.Kredi güncellenmesi yenimi ?\n    # 90 günü baz aldık. Çünkü bankalarda 3 ay gecikmeden sonra işlem başlatılıyor.\n    bureau_and_bb['NEWS_DAYS_CREDIT_UPDATE'] = bureau_and_bb['DAYS_CREDIT_UPDATE'].apply(lambda x : 'old' if x < -90 else 'new')\n    #cat_cols, num_cols, cat_but_car = grab_col_names(bureau_and_bb)\n    \n    ###########################\n    # One-Hot Encoding\n    ###########################\n    bureau_and_bb, bureau_and_bb_cat = one_hot_encoder(bureau_and_bb)\n    \n    # Bureau and bureau_balance numeric features\n    num_aggregations = {'DAYS_CREDIT': ['min', 'max', 'mean', 'var'],\n                        'DAYS_CREDIT_ENDDATE': ['min', 'max', 'mean'],\n                        'DAYS_CREDIT_UPDATE': ['mean'],\n                        'CREDIT_DAY_OVERDUE': ['max', 'mean'],\n                        'DAYS_ENDDATE_FACT':['min', 'max', 'mean'],\n                        'NEW_STATUS_SCORE':['min','mean','max'],\n                        'AMT_CREDIT_MAX_OVERDUE': ['mean'],\n                        'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n                        'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum'],\n                        'AMT_CREDIT_SUM_OVERDUE': ['mean'],\n                        'AMT_CREDIT_SUM_LIMIT': ['mean', 'sum'],\n                        'AMT_ANNUITY': ['max', 'mean'],\n                        'CNT_CREDIT_PROLONG': ['sum'],\n                        'MONTHS_BALANCE_MIN': ['min'],\n                        'MONTHS_BALANCE_MAX': ['max'],\n                        'MONTHS_BALANCE_SIZE': ['mean', 'sum'],\n                        'NEW_DEPT_RATIO': ['min','max','mean'] }\n\n    # Bureau and bureau_balance categorical features\n    cat_aggregations = {}\n    for cat in bureau_and_bb_cat: cat_aggregations[cat] = ['mean']\n    for cat in df_cat: cat_aggregations[cat + \"_MEAN\"] = ['mean']\n    \n    bureau_agg = bureau_and_bb.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n    bureau_agg.columns = pd.Index(['BURO_' + e[0] + \"_\" + e[1].upper() for e in bureau_agg.columns.tolist()])\n    \n    # Bureau: Active credits - using only numerical aggregations\n    active = bureau_and_bb[bureau_and_bb['CREDIT_ACTIVE_Active'] == 1]\n    active_agg = active.groupby('SK_ID_CURR').agg(num_aggregations)\n    active_agg.columns = pd.Index(['ACTIVE_' + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()])\n    bureau_agg = bureau_agg.join(active_agg, how='left', on='SK_ID_CURR')\n    del active, active_agg\n    \n    # Bureau: Closed credits - using only numerical aggregations\n    closed = bureau_and_bb[bureau_and_bb['CREDIT_ACTIVE_Closed'] == 1]\n    closed_agg = closed.groupby('SK_ID_CURR').agg(num_aggregations)\n    closed_agg.columns = pd.Index(['CLOSED_' + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()])\n    bureau_agg = bureau_agg.join(closed_agg, how='left', on='SK_ID_CURR')\n    del closed, closed_agg, bureau\n    gc.collect()\n    #print(bureau_agg.columns.tolist())\n    return bureau_agg\n# bureau_and_balance(num_rows = None, nan_as_category = True)\n###########################\n# Previous_application\n###########################\n\ndef previous_applications(num_rows = None, nan_as_category = True):\n    df = pd.read_csv(\"../input/home-credit-default-risk/previous_application.csv\", nrows = num_rows)\n    cat_cols, num_cols, cat_but_car = grab_col_names(df)\n    \n    \n    # Nümerik değişkenleri incelerken bazı değişkenlerin max değerinin gerçek dışı \n    # yani çok büyük olduğunu gördük.365 e böldük 1000 yıl gibi bir değer geldi.\n    # Bu sebeple bu kolondaki böyle büyük değerlere Nan değeri atayacağım\n    df['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace= True)\n    df['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace= True)\n    df['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace= True)\n    df['DAYS_LAST_DUE'].replace(365243, np.nan, inplace= True)\n    df['DAYS_TERMINATION'].replace(365243, np.nan, inplace= True)\n    \n    # Bazı kategorik değişkenlerin alt sınıfları XNA, XAP olarak yazılmış aslında bunlar nan değer.\n    # Bu yüzden bunlara nan atıyoruz.\n    na = ['XNA', 'XAP']\n    for col in cat_cols:\n        for n in na:\n            df.loc[df[col] == n, col] = np.nan\n\n    #########\n    # RARE\n    ########\n    rare_encoder(df, 0.01,cat_cols)\n    \n    # NAME_GOODS_CATEGORY\n    # Bu değişkenin bazı alt sınıflarını others da topladık.\n    a = ['Auto Accessories', 'Jewelry', 'Homewares', 'Medical Supplies', 'Vehicles', 'Sport and Leisure', \n         'Gardening', 'Other', 'Office Appliances', 'Tourism', 'Medicine', 'Direct Sales', 'Fitness', 'Additional Service', \n         'Education', 'Weapon', 'Insurance', 'House Construction', 'Animals'] \n\n    df[\"NAME_GOODS_CATEGORY\"] = df[\"NAME_GOODS_CATEGORY\"].replace(a, 'others')\n    \n    # \"NAME_SELLER_INDUSTRY\n    df[\"NAME_SELLER_INDUSTRY\"] = df[\"NAME_SELLER_INDUSTRY\"].replace(\"Rare\", 'others')\n    \n    # PREV_NAME_GOODS_CATEGORY\n    df[\"NAME_GOODS_CATEGORY\"] = np.where(df[\"NAME_GOODS_CATEGORY\"].str.contains(\"Photo / Cinema Equipment\"),\n                                       \"Photo_Cinema_Equipment\", df[\"NAME_GOODS_CATEGORY\"])\n    \n    # CHANNEL_TYPE\n    df[\"CHANNEL_TYPE\"] = np.where(df[\"CHANNEL_TYPE\"].str.contains(\"Regional / Local\"),\n                                       \"Regional_Local\", df[\"CHANNEL_TYPE\"])\n    \n    df[\"NAME_GOODS_CATEGORY\"] = np.where(df[\"NAME_GOODS_CATEGORY\"].str.contains(\"Audio/Video\"),\n                                       \"Audio_Video\", df[\"NAME_GOODS_CATEGORY\"])\n    \n    \n    \n    \n    \n    ############\n    # DROP\n    ############\n    # %99 eksik veri olan ve değişkenleri incelediğimizde bilgi taşımadığını düşündüğümüz \n    # değişkenleri verisetinden çıkarıyoruz.\n\n    del_cols = ['RATE_INTEREST_PRIMARY', 'RATE_INTEREST_PRIVILEGED', 'DAYS_FIRST_DRAWING',\n                'NAME_CASH_LOAN_PURPOSE', 'CODE_REJECT_REASON', 'FLAG_LAST_APPL_PER_CONTRACT',\n                'NFLAG_LAST_APPL_IN_DAY', 'SELLERPLACE_AREA']\n\n    df.drop(del_cols, axis=1, inplace=True)\n    \n    #########################\n    # FEATURE ENGINEERING\n    #########################\n    \n    # 1. \"HOUR_APPR_PROCESS_START\" (Müşteri önceki başvurusu için yaklaşık olarak hangi saatte başvurdu) \n    # değişkenin NEW_WORK_HOURS ve NEW_OFF_HOURS olarak iki sınıfa ayrılması\n    df[\"NEW_HOUR_APPR_PROCESS_START_CAT\"]= df.loc[:,\"HOUR_APPR_PROCESS_START\"]\n    a = [8,9,10,11,12,13,14,15,16,17]\n    df[\"NEW_HOUR_APPR_PROCESS_START_CAT\"] = df[\"NEW_HOUR_APPR_PROCESS_START_CAT\"].replace(a, \"WORK_HOURS\")\n    b = [18,19,20,21,22,23,0,1,2,3,4,5,6,7]\n    df[\"NEW_HOUR_APPR_PROCESS_START_CAT\"] = df[\"NEW_HOUR_APPR_PROCESS_START_CAT\"].replace(b, 'OFF_HOURS')\n    \n    # 2. \"WEEKDAY_APPR_PROCESS_START\"  değişkeninin  WEEK_DAY ve WEEKEND olarak iki sınıfa  indirdik\n    df[\"NEW_WEEKDAY_APPR_PROCESS_START_CAT\"] = df.loc[:,\"WEEKDAY_APPR_PROCESS_START\"]\n    df[\"NEW_WEEKDAY_APPR_PROCESS_START_CAT\"] = df[\"NEW_WEEKDAY_APPR_PROCESS_START_CAT\"].replace(['MONDAY','TUESDAY', 'WEDNESDAY','THURSDAY','FRIDAY'], 'WEEK_DAY')\n    df[\"NEW_WEEKDAY_APPR_PROCESS_START_CAT\"] = df[\"NEW_WEEKDAY_APPR_PROCESS_START_CAT\"].replace(['SATURDAY', 'SUNDAY'], 'WEEKEND')\n    \n    # 3. \"NAME_TYPE_SUITE\"  değişkeninin single ve multiple olarak iki kategoriye ayrılması\n    df[\"NAME_TYPE_SUITE\"] = df[\"NAME_TYPE_SUITE\"].replace('Unaccompanied', 'single')\n    b = ['Family', 'Spouse, partner', 'Children', 'Other_B', 'Other_A', 'Group of people']\n    df[\"NAME_TYPE_SUITE\"] = df[\"NAME_TYPE_SUITE\"].replace(b, 'multiple')\n    \n    # 4. Müşteri öncek başvuruda ne kadar kredi istedi / önceki başvurunun nihai kredi tutarı\n    df[\"NEW_AMT_CREDIT_RATIO\"] = df[\"AMT_APPLICATION\"]/df[\"AMT_CREDIT\"]\n    # 1 e yakın olması istediği krediden az almış olması\n    # 0 a yakın olması istediği krediden fazla almış olması\n    # 1 ise istenilen kredi ile verilen kredi miktarı aynı olması\n    \n    # 5. x <= 1 ise istediği krediyi almış veya daha fazlasını almış.\n    df[\"NEWX2_FLAG_AMT_CREDIT_RATIO\"] = df[\"NEW_AMT_CREDIT_RATIO\"].apply(lambda x: 1 if(x<=1) else 0)\n    \n    # 6. NFLAG_INSURED_ON_APPROVAL değişkeni yerine kullanılmak izere NEW_INSURANCE değişkeni tanımlandı.\n    # NFLAG_INSURED_ON_APPROVAL: Müşteri önceki başvuru sırasında sigorta talep etti mi?\n    # AMT_CREDIT: Bankanın verdiği nihai kredi tutarı\n    # AMT_GOOD_PRICE: Müşterinin(varsa) önceki uygulamada istediği malın fiyatı\n    ########### NAN SAYISINI AZALTMAK İÇİN YAPTIK ÖNEMLİİİ !!!!!!  ############\n    df[(df['AMT_CREDIT'] == 0) | (df['AMT_GOODS_PRICE'] == 0)]['NEW_INSURED_ON_APPROVAL'] = np.nan\n    df['INSURANCE_AMOUNT'] = df['AMT_CREDIT'] - df['AMT_GOODS_PRICE']\n    df['NEW_INSURED_ON_APPROVAL'] = df['INSURANCE_AMOUNT'].apply(lambda x: 1 if x > 0 else (0 if x <= 0 else np.nan))\n    df.drop('INSURANCE_AMOUNT', axis=1, inplace=True)\n    \n    # 7. Kaç yılda ödedi = Bankanın verdiği kredi / yıllık taksit tutarı\n    df['NEW_HOW_PAID_YEARS'] = df['AMT_CREDIT'] / df['AMT_ANNUITY']\n    \n    # 8. Başvurduğu kredi miktarı / almak istediği ürünün fiyatı \n    df['NEW_GOODS_RATIO'] = df['AMT_APPLICATION'] / df['AMT_GOODS_PRICE']\n    \n    #  Ödeme gününü geciktirmiş mi bunu gösteren değişken türetelim.\n    # 1= geciktirmiş, 0 = geciktirmemiş, NaN = boş değer\n    # (Mevcut başvurunun başvuru tarihine göre, bir önceki başvurunun ilk vadesi ne zamandı?) - \n    # (Mevcut başvurunun başvuru tarihine göre, bir önceki başvurunun ilk vadesi ne zaman olmalıydı?)\n    # 9. NÜMERİK OLAN\n    df['NEW_LATE_DAYS'] =  df['DAYS_LAST_DUE_1ST_VERSION'] - df['DAYS_FIRST_DUE'] \n    # 10 .SINIFLI OLAN\n    k = df[\"DAYS_LAST_DUE_1ST_VERSION\"] - df[\"DAYS_LAST_DUE\"]\n    df[\"NEW_FLAG_LATE_DAYS\"] = [1 if i >= 0 else (0 if i < 0  else \"NaN\") for i in k]\n    \n    # WEEKDAY_APPR_PROCESS_START_DIC\n    # Cycle encoding gün, ay, yıl gibi döngüsel değişkenlerde kullanılabilir.\n    df['WEEKDAY_APPR_PROCESS_START_DIC'] = df['WEEKDAY_APPR_PROCESS_START'].map({\n        'MONDAY': 1, 'TUESDAY': 2, 'WEDNESDAY': 3, 'THURSDAY': 4, 'FRIDAY': 5, 'SATURDAY': 6, 'SUNDAY': 7})\n    df['NEW_WEEKDAY_SIN'] = np.sin(2 * np.pi * df['WEEKDAY_APPR_PROCESS_START_DIC'] / 7)\n    df['NEW_WEEKDAY_COS'] = np.cos(2 * np.pi * df['WEEKDAY_APPR_PROCESS_START_DIC'] / 7)\n    # df.drop('WEEKDAY_APPR_PROCESS_START', axis=1, inplace=True) # feature imp bak!!!!!\n    \n    #######################\n    # One_Hot Encoding\n    #######################\n    df, cat_cols = one_hot_encoder(df, nan_as_category= True)\n\n    #################\n    # Aggregation\n    #################\n    col_list = df.columns.tolist()\n    id_list = [\"SK_ID_CURR\",\"SK_ID_PREV\"]\n    num_list = [col for col in col_list if col not in cat_cols + id_list]\n    \n    # Previous applications numeric features\n    agg_num_prev = {}\n    for num in num_list:\n        agg_num_prev[num] = ['min', 'max', 'mean', 'median']\n        \n    # Previous applications categorical features\n    agg_cat_prev = {}\n    for cat in cat_cols:\n        agg_cat_prev[cat] = ['mean']\n        \n    prev_agg = df.groupby('SK_ID_CURR').agg({**agg_num_prev, **agg_cat_prev})\n    prev_agg.columns = pd.Index(['PREV_' + e[0] + \"_\" + e[1].upper() for e in prev_agg.columns.tolist()])\n    # Previous Applications: Approved Applications - only numerical features\n    approved = df[df['NAME_CONTRACT_STATUS_Approved'] == 1]\n    approved_agg = approved.groupby('SK_ID_CURR').agg(agg_num_prev)\n    approved_agg.columns = pd.Index(['APPROVED_' + e[0] + \"_\" + e[1].upper() for e in approved_agg.columns.tolist()])\n    prev_agg = prev_agg.join(approved_agg, how='left', on='SK_ID_CURR')\n    # Previous Applications: Refused Applications - only numerical features\n    refused = df[df['NAME_CONTRACT_STATUS_Refused'] == 1]\n    refused_agg = refused.groupby('SK_ID_CURR').agg(agg_num_prev)\n    refused_agg.columns = pd.Index(['REFUSED_' + e[0] + \"_\" + e[1].upper() for e in refused_agg.columns.tolist()])\n    prev_agg = prev_agg.join(refused_agg, how='left', on='SK_ID_CURR')\n    \n    del refused, refused_agg, approved, approved_agg, df\n    gc.collect()\n    #print(prev_agg.columns.tolist())\n    return prev_agg","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##################\n# Pos_cash\n#################\n\ndef pos_cash(num_rows = None, nan_as_category = True):\n    df = pd.read_csv('../input/home-credit-default-risk/POS_CASH_balance.csv', nrows = num_rows)\n    cat_cols, num_cols, cat_but_car = grab_col_names(df)\n    \n    # Rare\n    rare_encoder(df, 0.01, cat_cols)\n    # One-Hot Encoding\n    df, cat_cols = one_hot_encoder(df, nan_as_category= True)\n    \n    # Numerical Features\n    aggregations = {'MONTHS_BALANCE': ['max', 'mean', 'size'],\n                    'CNT_INSTALMENT': ['max', 'mean', 'std', 'min', 'median'],\n                    'CNT_INSTALMENT_FUTURE': ['max', 'mean', 'sum', 'min', 'median', 'std'],\n                    'SK_DPD': ['max', 'mean'],\n                    'SK_DPD_DEF': ['max', 'mean']\n                   }\n    \n    # Categorical Features\n    original_columns = list(df.columns)\n    new_columns = [c for c in df.columns if c not in original_columns]\n    for cat in new_columns:\n        aggregations[cat] = ['mean']\n        \n    pos_agg = df.groupby('SK_ID_CURR').agg(aggregations)\n    pos_agg.columns = pd.Index(['POS_' + e[0] + \"_\" + e[1].upper() for e in pos_agg.columns.tolist()])\n    # Count pos cash accounts\n    pos_agg['POS_COUNT'] = df.groupby('SK_ID_CURR').size()\n    del df\n    gc.collect()\n    #print(pos_agg.columns.tolist())\n    return pos_agg\n# pos_cash(num_rows = None, nan_as_category = True)\n###############################\n# Installments_payments\n###############################\n\ndef installments_payments(num_rows = None, nan_as_category = True):\n    df = pd.read_csv('../input/home-credit-default-risk/installments_payments.csv', nrows = num_rows)\n    cat_cols, num_cols, cat_but_car = grab_col_names(df)\n    df, cat_cols = one_hot_encoder(df, nan_as_category= True) ### neden cat var?\n    \n    #########################\n    # Feature Engineering\n    #########################\n    # Her kredi taksidi ödemesinde ödediği miktarla aslı arasındaki fark ve bunun yüzdesi\n    df['PAYMENT_PERC'] = df['AMT_PAYMENT'] / df['AMT_INSTALMENT']\n    df['PAYMENT_DIFF'] = df['AMT_INSTALMENT'] - df['AMT_PAYMENT']\n\n    # Vadesi geçmiş günler ve vadesinden önceki günler -- sadece pozitif değerler alınır\n    df['DPD'] = df['DAYS_ENTRY_PAYMENT'] - df['DAYS_INSTALMENT']\n    df['DBD'] = df['DAYS_INSTALMENT'] - df['DAYS_ENTRY_PAYMENT']\n    df['DPD'] = df['DPD'].apply(lambda x: x if x > 0 else 0)\n    df['DBD'] = df['DBD'].apply(lambda x: x if x > 0 else 0)\n    \n    # Her bir taksit ödemesinin gec olup olmama durumu 1: gec ödedi 0: erken ödemeyi temsil eder\n    df['NEW_DAYS_PAID_EARLIER'] = df['DAYS_INSTALMENT'] - df['DAYS_ENTRY_PAYMENT']\n    df['NEW_NUM_PAID_LATER'] = df['NEW_DAYS_PAID_EARLIER'].map(lambda x: 1 if x<0 else 0)\n    \n    # Numeric Features\n    aggregations = {\n        'NUM_INSTALMENT_VERSION': ['nunique'],\n        'DPD': ['max', 'mean', 'sum'],\n        'DBD': ['max', 'mean', 'sum'],\n        'PAYMENT_PERC': ['max', 'mean', 'sum', 'var'],\n        'PAYMENT_DIFF': ['max', 'mean', 'sum', 'var'],\n        'AMT_INSTALMENT': ['max', 'mean', 'sum'],\n        'AMT_PAYMENT': ['min', 'max', 'mean', 'sum'],\n        'DAYS_ENTRY_PAYMENT': ['max', 'mean', 'sum']\n    }\n    \n    # Categorical Features\n    for cat in cat_cols:\n        aggregations[cat] = ['mean']\n    ins_agg = df.groupby('SK_ID_CURR').agg(aggregations)\n    ins_agg.columns = pd.Index(['INSTAL_' + e[0] + \"_\" + e[1].upper() for e in ins_agg.columns.tolist()])\n    # Count installments accounts\n    ins_agg['INSTAL_COUNT'] = df.groupby('SK_ID_CURR').size()\n    del df\n    gc.collect()\n    #print(ins_agg.columns.tolist())\n    return ins_agg","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#########################\n# Credit_card_balance\n########################\ndef credit_card_balance(num_rows = None, nan_as_category = True):\n    df = pd.read_csv('../input/home-credit-default-risk/credit_card_balance.csv', nrows = num_rows)\n    cat_cols, num_cols, cat_but_car = grab_col_names(df)\n    \n    # Rare\n    df[\"NAME_CONTRACT_STATUS\"] = np.where(~(df[\"NAME_CONTRACT_STATUS\"].isin([\"Active\", \"Completed\"])),\n                                          \"Rare\", df[\"NAME_CONTRACT_STATUS\"])\n    \n    # One Hot Encoder\n    df, cat_cols = one_hot_encoder(df, nan_as_category=False)\n    \n    ########################\n    # Feature Engineering\n    ########################\n    # ATM den cekilen tutar + mal satın alma miktari\n    df[\"TOTAL_SPENDING\"] = df[\"AMT_DRAWINGS_ATM_CURRENT\"] + df[\"AMT_DRAWINGS_POS_CURRENT\"]\n    # Müşterinin ay boyunca ödediği para - aylık asgari taksit\n    df[\"REGULARITY_PAYMENT\"] = df[\"AMT_INST_MIN_REGULARITY\"] - df[\"AMT_PAYMENT_TOTAL_CURRENT\"]\n    \n    # General aggregations\n    df.drop(['SK_ID_PREV'], axis= 1, inplace = True)\n    cc_agg = df.groupby('SK_ID_CURR').agg(['min', 'max', 'mean', 'sum', 'var'])\n    cc_agg.columns = pd.Index(['CC_' + e[0] + \"_\" + e[1].upper() for e in cc_agg.columns.tolist()])\n    # Count credit card lines\n    cc_agg['CC_COUNT'] = df.groupby('SK_ID_CURR').size()\n    del df\n    gc.collect()\n   #print(cc_agg.columns.tolist())\n    return cc_agg\n# credit_card_balance(num_rows = None, nan_as_category = True)\n# LightGBM GBDT with KFold or Stratified KFold\n# Parameters from Tilii kernel: https://www.kaggle.com/tilii7/olivier-lightgbm-parameters-by-bayesian-opt/code\ndef kfold_lightgbm(df, num_folds, stratified = False, debug= False):\n    df = df.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n    # Divide in training/validation and test data\n    train_df = df[df['TARGET'].notnull()]\n    test_df = df[df['TARGET'].isnull()]\n    print(\"Starting LightGBM. Train shape: {}, test shape: {}\".format(train_df.shape, test_df.shape))\n    del df\n    gc.collect()\n    # Cross validation model\n    if stratified:\n        folds = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=1001)\n    else:\n        folds = KFold(n_splits= num_folds, shuffle=True, random_state=1001)\n    # Create arrays and dataframes to store results\n    oof_preds = np.zeros(train_df.shape[0])\n    sub_preds = np.zeros(test_df.shape[0])\n    feature_importance_df = pd.DataFrame()\n    feats = [f for f in train_df.columns if f not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']]\n    \n    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['TARGET'])):\n        train_x, train_y = train_df[feats].iloc[train_idx], train_df['TARGET'].iloc[train_idx]\n        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['TARGET'].iloc[valid_idx]\n\n        # LightGBM parameters found by Bayesian optimization\n        clf = LGBMClassifier(\n            boosting_type= 'goss',\n            nthread=4,\n            n_estimators=10000,\n            learning_rate=0.005134,\n            num_leaves=54,\n            colsample_bytree=0.508716,\n            subsample=1,\n            subsample_for_bin= 240000,\n            max_depth=10,\n            reg_alpha=0.436193,\n            reg_lambda=0.436193,\n            min_split_gain=0.024766,\n            min_child_weight=39.3259775,\n            silent=-1,\n            verbose=-1, )\n        \n        clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], \n            eval_metric= 'auc', verbose= 200, early_stopping_rounds= 200)\n\n        oof_preds[valid_idx] = clf.predict_proba(valid_x, num_iteration=clf.best_iteration_)[:, 1]\n        sub_preds += clf.predict_proba(test_df[feats], num_iteration=clf.best_iteration_)[:, 1] / folds.n_splits\n\n        fold_importance_df = pd.DataFrame()\n        fold_importance_df[\"feature\"] = feats\n        fold_importance_df[\"importance\"] = clf.feature_importances_\n        fold_importance_df[\"fold\"] = n_fold + 1\n        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n        print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(valid_y, oof_preds[valid_idx])))\n        del clf, train_x, train_y, valid_x, valid_y\n        gc.collect()\n\n    print('Full AUC score %.6f' % roc_auc_score(train_df['TARGET'], oof_preds))\n    # Write submission file and plot feature importance\n    if not debug:\n        test_df['TARGET'] = sub_preds\n        test_df[['SK_ID_CURR', 'TARGET']].to_csv(submission_file_name, index= False)\n    display_importances(feature_importance_df)\n    return feature_importance_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display/plot feature importance\ndef display_importances(feature_importance_df_):\n    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:100].index\n    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n    plt.figure(figsize=(20, 25))\n    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n    plt.title('LightGBM Features (avg over folds)')\n    plt.tight_layout()\n    #plt.savefig('lgbm_importances01.png')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def main(debug = False):\n    num_rows = 10000 if debug else None\n    df = application_train_test(num_rows)\n    print(\"application shape:\", df.shape)\n    with timer(\"Process bureau and bureau_balance\"):\n        bureau = bureau_and_balance(num_rows)\n        print(\"Bureau df shape:\", bureau.shape)\n        df = df.join(bureau, how='left', on='SK_ID_CURR')\n        del bureau\n        gc.collect()\n    with timer(\"Process previous_applications\"):\n        prev = previous_applications(num_rows)\n        print(\"Previous applications df shape:\", prev.shape)\n        df = df.join(prev, how='left', on='SK_ID_CURR')\n        del prev\n        gc.collect()\n    with timer(\"Process POS-CASH balance\"):\n        pos = pos_cash(num_rows)\n        print(\"Pos-cash balance df shape:\", pos.shape)\n        df = df.join(pos, how='left', on='SK_ID_CURR')\n        del pos\n        gc.collect()\n    with timer(\"Process installments payments\"):\n        ins = installments_payments(num_rows)\n        print(\"Installments payments df shape:\", ins.shape)\n        df = df.join(ins, how='left', on='SK_ID_CURR')\n        del ins\n        gc.collect()\n    with timer(\"Process credit card balance\"):\n        cc = credit_card_balance(num_rows)\n        print(\"Credit card balance df shape:\", cc.shape)\n        df = df.join(cc, how='left', on='SK_ID_CURR')\n        del cc\n        gc.collect()\n    with timer(\"Run LightGBM with kfold\"):\n        feat_importance = kfold_lightgbm(df, num_folds= 10, stratified= False, debug= debug)\n\nif __name__ == \"__main__\":\n    submission_file_name = \"submission_kernel02.csv\"\n    with timer(\"Full model run\"):\n        main()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}