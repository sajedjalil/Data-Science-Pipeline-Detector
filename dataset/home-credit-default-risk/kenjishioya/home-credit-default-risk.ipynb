{"cells":[{"metadata":{"_uuid":"b5fb2cfd-2903-4723-b9a5-ee95b54120ff","_cell_guid":"51524f21-b1cb-4a11-aea0-1d2c45ec10b8","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\nimport gc\nimport re\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\n# for local\n# from utils.storage import get_storage\nimport optuna","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Home_Credit:\n    APPLICATION_TRAIN_PATH = '../input/home-credit-default-risk/application_train.csv'\n    APPLICATION_TEST_PATH = '../input/home-credit-default-risk/application_test.csv'\n    BUREAU_PATH = '../input/home-credit-default-risk/bureau.csv'\n    BUREAU_BALANCE_PATH = '../input/home-credit-default-risk/bureau_balance.csv'\n    PREVIOUS_APPLICATION_PATH = '../input/home-credit-default-risk/previous_application.csv'\n    CREDIT_CARD_PATH = '../input/home-credit-default-risk/credit_card_balance.csv'\n    INSTALLMENTS_PAYMENTS_PATH = '../input/home-credit-default-risk/installments_payments.csv'\n    POS_CASH_BALANCE_PATH = '../input/home-credit-default-risk/POS_CASH_balance.csv'\n\n    def __init__(self, debug=False):\n        self.debug = debug\n        self.nrows = 10000 if debug == True else None\n        self.var_list_to_clear = []\n    \n    def clear_memory(self):\n        for variable in self.var_list_to_clear:\n            del variable\n        self.var_list_to_clear = []\n        gc.collect()\n    \n    def one_hot_encoding(self, df):\n        cols = df.columns.tolist()\n        cat_cols = [col for col in df.columns if df[col].dtype == 'object']\n        df = pd.get_dummies(df, columns=cat_cols, dummy_na=True)\n        # new_cols = [new_col for new_col in df.columns if new_col not in cols]\n        return df #, new_cols\n    \n    def fill_zero_num_cols(self, df):\n        num_cols = [col for col in df.columns if df[col].dtype != 'object']\n        df.loc[:, num_cols] = df[num_cols].fillna(value=0)\n        return df\n    \n    def convert_float64_to_float32(self, df):\n        num_cols = [col for col in df.columns if df[col].dtype == 'float64']\n        df.loc[:, num_cols] = df[num_cols].astype('float32')\n        return df\n    \n    def get_num_columns_list(self, df, add_list=[]):\n        num_cols = [col for col in df.columns if df[col].dtype != 'object']\n        for elem in add_list:\n            num_cols.append(elem)\n        return num_cols\n\n    def preprocess_aplication(self):\n        train_df = pd.read_csv(self.APPLICATION_TRAIN_PATH, nrows=self.nrows)\n        test_df = pd.read_csv(self.APPLICATION_TEST_PATH, nrows=self.nrows)\n        all_df = train_df.append(test_df)\n        self.var_list_to_clear.extend([train_df, test_df])\n        all_df = all_df[all_df['CODE_GENDER'] != 'XNA']\n        for bin_feature in ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']:\n            all_df[bin_feature], uniques = pd.factorize(all_df[bin_feature])\n\n        encoded_df = self.one_hot_encoding(all_df)\n        self.var_list_to_clear.extend([all_df])\n        encoded_df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace= True)\n        encoded_df['DAYS_EMPLOYED_PERC'] = encoded_df['DAYS_EMPLOYED'] / encoded_df['DAYS_BIRTH']\n        encoded_df['INCOME_CREDIT_PERC'] = encoded_df['AMT_INCOME_TOTAL'] / encoded_df['AMT_CREDIT']\n        encoded_df['INCOME_PER_PERSON'] = encoded_df['AMT_INCOME_TOTAL'] / encoded_df['CNT_FAM_MEMBERS']\n        encoded_df['ANNUITY_INCOME_PERC'] = encoded_df['AMT_ANNUITY'] / encoded_df['AMT_INCOME_TOTAL']\n        encoded_df['PAYMENT_RATE'] = encoded_df['AMT_ANNUITY'] / encoded_df['AMT_CREDIT']\n        self.clear_memory()\n        return encoded_df\n    \n    def preprocess_breau(self):\n        bureau_df = pd.read_csv(self.BUREAU_PATH, nrows=self.nrows)\n        bureau_balance_df = pd.read_csv(self.BUREAU_BALANCE_PATH, nrows=self.nrows)\n        bureau_num_columns_list = self.get_num_columns_list(bureau_df)\n        encoded_bureau_df = self.one_hot_encoding(bureau_df)\n        encoded_bb_df = self.one_hot_encoding(bureau_balance_df)\n        self.var_list_to_clear.extend([bureau_df, bureau_balance_df])\n        # bureau_balance aggregate\n        bureau_balance_agg_df = encoded_bb_df.groupby('SK_ID_BUREAU').agg('mean')\n        bureau_merged_df = encoded_bureau_df.merge(bureau_balance_agg_df, how='left', on='SK_ID_BUREAU')\n        bureau_merged_df = bureau_merged_df.drop('SK_ID_BUREAU', axis='columns')\n        bureau_agg_df = bureau_merged_df.groupby('SK_ID_CURR').agg('mean')\n        self.var_list_to_clear.extend([bureau_balance_agg_df, bureau_merged_df])\n        # active\n        active = encoded_bureau_df[encoded_bureau_df['CREDIT_ACTIVE_Active'] == 1]\n        active_agg = active[bureau_num_columns_list].groupby('SK_ID_CURR').agg('mean')\n        active_agg = active_agg.drop('SK_ID_BUREAU', axis='columns')\n        active_agg.columns = pd.Index(['ACTIVE_' + col for col in active_agg.columns.tolist()])\n        bureau_agg_df = pd.merge(bureau_agg_df, active_agg, how='left', on='SK_ID_CURR')\n        self.var_list_to_clear.extend([active, active_agg])\n        # closed\n        closed = encoded_bureau_df[encoded_bureau_df['CREDIT_ACTIVE_Closed'] == 1]\n        closed_agg = closed[bureau_num_columns_list].groupby('SK_ID_CURR').agg('mean')\n        closed_agg = closed_agg.drop('SK_ID_BUREAU', axis='columns')\n        closed_agg.columns = pd.Index(['CLOSED_' + col for col in closed_agg.columns.tolist()])\n        bureau_merged_df = pd.merge(bureau_merged_df, closed_agg, how='left', on='SK_ID_CURR')\n        self.var_list_to_clear.extend([closed, closed_agg])\n        self.clear_memory()\n        return bureau_agg_df\n\n    def preprocess_prev_application(self):\n        prev_df = pd.read_csv(self.PREVIOUS_APPLICATION_PATH, nrows=self.nrows)\n        prev_num_columns_list = self.get_num_columns_list(prev_df)\n        encoded_prev_df = self.one_hot_encoding(prev_df)\n        self.var_list_to_clear.extend([prev_df])\n        encoded_prev_df['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace= True)\n        encoded_prev_df['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace= True)\n        encoded_prev_df['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace= True)\n        encoded_prev_df['DAYS_LAST_DUE'].replace(365243, np.nan, inplace= True)\n        encoded_prev_df['DAYS_TERMINATION'].replace(365243, np.nan, inplace= True)\n        # Add feature: value ask / value received percentage\n        encoded_prev_df['APP_CREDIT_PERC'] = encoded_prev_df['AMT_APPLICATION'] / encoded_prev_df['AMT_CREDIT']\n        prev_agg_df = encoded_prev_df.groupby('SK_ID_CURR').agg('mean')\n        self.var_list_to_clear.extend([encoded_prev_df])\n        prev_agg_df = prev_agg_df.drop('SK_ID_PREV', axis='columns')\n        # approved\n        approved = encoded_prev_df[encoded_prev_df['NAME_CONTRACT_STATUS_Approved'] == 1]\n        approved_agg = approved[prev_num_columns_list].groupby('SK_ID_CURR').agg('mean')\n        approved_agg = approved_agg.drop('SK_ID_PREV', axis='columns')\n        approved_agg.columns = pd.Index(['APPROVED_' + col for col in approved_agg.columns.tolist()])\n        prev_agg_df = pd.merge(prev_agg_df, approved_agg, how='left', on='SK_ID_CURR')\n        self.var_list_to_clear.extend([approved, approved_agg])\n        # refused\n        refused = encoded_prev_df[encoded_prev_df['NAME_CONTRACT_STATUS_Refused'] == 1]\n        refused_agg = refused[prev_num_columns_list].groupby('SK_ID_CURR').agg('mean')\n        refused_agg = refused_agg.drop('SK_ID_PREV', axis='columns')\n        refused_agg.columns = pd.Index(['REFUSED_' + col for col in refused_agg.columns.tolist()])\n        encoded_prev_df = pd.merge(encoded_prev_df, refused_agg, how='left', on='SK_ID_CURR')\n        self.var_list_to_clear.extend([refused, refused_agg])\n        \n        self.clear_memory()\n        return prev_agg_df\n\n    def preprocess_pos_cash(self):\n        pos_df = pd.read_csv(self.POS_CASH_BALANCE_PATH, nrows=self.nrows)\n        encoded_pos_df = self.one_hot_encoding(pos_df)\n        self.var_list_to_clear.extend([pos_df])\n        pos_agg_df = encoded_pos_df.groupby('SK_ID_CURR').agg('mean')\n        pos_agg_df['POS_COUNT'] = encoded_pos_df.groupby('SK_ID_CURR').size()\n        pos_agg_df = pos_agg_df.drop('SK_ID_PREV', axis='columns')\n        self.var_list_to_clear.extend([encoded_pos_df])\n        self.clear_memory()\n        return pos_agg_df\n\n    def preprocess_installments_df(self):\n        install_df = pd.read_csv(self.INSTALLMENTS_PAYMENTS_PATH, nrows=self.nrows)\n        encoded_install_df = self.one_hot_encoding(install_df)\n        self.var_list_to_clear.extend([install_df])\n        install_agg_df = encoded_install_df.groupby('SK_ID_CURR').agg('mean')\n        install_agg_df['INSTALL_COUNT'] = encoded_install_df.groupby('SK_ID_CURR').size()\n        install_agg_df = install_agg_df.drop('SK_ID_PREV', axis='columns')\n        self.var_list_to_clear.extend([encoded_install_df])\n        self.clear_memory()\n        return install_agg_df\n\n    def preprocess_credit_card_df(self):\n        credit_card_df = pd.read_csv(self.CREDIT_CARD_PATH, nrows=self.nrows)\n        encoded_credit_card_df = self.one_hot_encoding(credit_card_df)\n        self.var_list_to_clear.extend([credit_card_df])\n        credit_card_agg_df = encoded_credit_card_df.groupby('SK_ID_CURR').agg('mean')\n        credit_card_agg_df['CREDIT_COUNT'] = encoded_credit_card_df.groupby('SK_ID_CURR').size()\n        credit_card_agg_df = credit_card_agg_df.drop('SK_ID_PREV', axis='columns')\n        self.var_list_to_clear.extend([encoded_credit_card_df])\n        self.clear_memory()\n        return credit_card_agg_df\n\n    def objective_multi_classifiers(self, trial):\n        # search better model from RandomForestRegressor, XGBRegressor\n        classifier_name = trial.suggest_categorical('classifier', ['RandomForest', 'XGBoost', 'LGBM'])\n        # search better max_depth from 2 to 16\n        max_depth = trial.suggest_int('max_depth', 2, 16)\n        # search better n_estimators from 50 to 4000\n        n_estimators = trial.suggest_int('n_estimators', 50, 7000)\n        if classifier_name == 'RandomForest':\n            model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=1234)\n        elif classifier_name == 'XGBoost':\n            model = XGBClassifier(n_estimators=n_estimators, max_depth=max_depth, objective='binary:logistic', random_state=1234)\n        else:\n            model = LGBMClassifier(boosting_type='goss',n_estimators=n_estimators, max_depth=max_depth, objective='binary', num_leaves=34, random_state=1234)\n        \n        error_list = cross_val_score(model, self.X, self.y, cv=3, scoring='roc_auc')\n        gc.collect()\n        return error_list.mean()\n    \n    def objective_LGBM(self, trial):\n        # hyper parameters for tuning\n        n_estimators = trial.suggest_int('n_estimators', 50, 10000)\n        learning_rate = trial.suggest_float('learning_rate', 0.001, 0.5)\n        max_depth = trial.suggest_int('max_depth', 2, 16)\n        num_leaves = trial.suggest_int('num_leaves', 10, 50)\n        colsample_bytree = trial.suggest_float('colsample_bytree', 0.01, 1)\n        subsample = trial.suggest_float('subsample', 0.01, 1)\n        reg_alpha = trial.suggest_float('reg_alpha', 0.001, 0.1)\n        reg_lambda = trial.suggest_float('reg_lambda', 0.001, 0.1)\n        min_split_gain = trial.suggest_float('min_split_gain', 0.001, 0.1)\n        min_child_weight = trial.suggest_float('min_child_weight', 0.001, 50)\n        \n        model = LGBMClassifier(\n            boosting_type='goss',\n            n_estimators=n_estimators,\n            learning_rate=learning_rate,\n            max_depth=max_depth,\n            num_leaves=num_leaves,\n            colsample_bytree=colsample_bytree,\n            subsample=subsample,\n            reg_alpha=reg_alpha,\n            reg_lambda=reg_lambda,\n            min_split_gain=min_split_gain,\n            min_child_weight=min_child_weight,\n            objective='binary',\n            random_state=1234)\n        \n        error_list = cross_val_score(model, self.X, self.y, cv=3, scoring='roc_auc')\n        gc.collect()\n        return error_list.mean()\n\n    def preprocess_data(self):\n        # application\n        all_df = self.preprocess_aplication()\n        # bureau\n        bureau_df = self.preprocess_breau()\n        all_df = pd.merge(all_df, bureau_df, how='left', on='SK_ID_CURR')\n        self.var_list_to_clear.extend([bureau_df])\n        # previous application\n        prev_application_df = self.preprocess_prev_application()\n        all_df = pd.merge(all_df, prev_application_df, how='left', on='SK_ID_CURR')\n        self.var_list_to_clear.extend([prev_application_df])\n        # POS CASH\n        pos_cash_df = self.preprocess_credit_card_df()\n        all_df = pd.merge(all_df, pos_cash_df, how='left', on='SK_ID_CURR')\n        self.var_list_to_clear.extend([pos_cash_df])\n        # installments\n        installments_df = self.preprocess_installments_df()\n        all_df = pd.merge(all_df, installments_df, how='left', on='SK_ID_CURR')\n        self.var_list_to_clear.extend([installments_df])\n        # credit card\n        credit_card_df = self.preprocess_credit_card_df()\n        all_df = pd.merge(all_df, credit_card_df, how='left', on='SK_ID_CURR')\n        self.var_list_to_clear.extend([credit_card_df])\n        \n        all_df = all_df.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n        train_df = all_df[all_df['TARGET'].notnull()]\n        test_df = all_df[all_df['TARGET'].isnull()]\n        self.var_list_to_clear.extend([all_df])\n        \n        filled_train_df = self.fill_zero_num_cols(train_df)\n        filled_test_df = self.fill_zero_num_cols(test_df)\n        self.var_list_to_clear.extend([train_df, test_df])\n        \n        converted_train_df = self.convert_float64_to_float32(filled_train_df)\n        converted_test_df = self.convert_float64_to_float32(filled_test_df)\n        self.var_list_to_clear.extend([filled_train_df, filled_test_df])\n        \n        self.y = converted_train_df['TARGET']\n        self.predict = converted_test_df[['SK_ID_CURR']]\n        features = [feature for feature in converted_train_df.columns if feature not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV']]\n        self.X = converted_train_df[features]\n        self.test_X = converted_test_df[features]\n        print(f'shape of X: {self.X.shape}')\n        print(f'shape of test_X: {self.test_X.shape}')\n        self.clear_memory()\n\n    def get_objective(self, objective_name):\n        objective_dict = {\n            \"multi_classifiers\": self.objective_multi_classifiers,\n            \"LGBM\": self.objective_LGBM\n        }\n        return objective_dict[objective_name]\n        \n    def parameter_tuning_with_storage(self, study_name ,objective_name):\n        study = optuna.create_study(direction='maximize', study_name=study_name, storage=get_storage(), load_if_exists=True)  # Create a new study.\n        study.optimize(self.get_objective(objective_name), n_trials=10)\n        \n    def parameter_tuning(self, objective_name):\n        study = optuna.create_study(direction='maximize')  # Create a new study.\n        study.optimize(self.get_objective(objective_name), n_trials=10)\n        self.study = study\n    \n    def parameter_tuning_test(self, objective_name):\n        study = optuna.create_study(direction='maximize')  # Create a new study.\n        study.optimize(self.get_objective(objective_name), n_trials=4)\n        \n    def predict_with_best_parameter(self, study_name=None):\n        if study_name is not None:\n            loaded_study = optuna.load_study(study_name=study_name, storage=get_storage())\n        else:\n            loaded_study = self.study\n        best_params = loaded_study.best_trial.params\n        model = LGBMClassifier(\n            boosting_type='goss',\n            n_estimators=best_params['n_estimators'],\n            learning_rate=best_params['learning_rate'],\n            max_depth=best_params['max_depth'],\n            num_leaves=best_params['num_leaves'],\n            colsample_bytree=best_params['colsample_bytree'],\n            subsample=best_params['subsample'],\n            reg_alpha=best_params['reg_alpha'],\n            reg_lambda=best_params['reg_lambda'],\n            min_split_gain=best_params['min_split_gain'],\n            min_child_weight=best_params['min_child_weight'],\n            objective='binary',\n            random_state=1234)\n        model.fit(self.X, self.y)\n        probability = model.predict_proba(self.test_X)[:, 1]\n        self.predict['TARGET'] = probability\n        self.predict.to_csv('./submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"home_credit = Home_Credit()\nhome_credit.preprocess_data()\nhome_credit.parameter_tuning('LGBM')\nhome_credit.predict_with_best_parameter()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}