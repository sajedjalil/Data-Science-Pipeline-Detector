{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"markdown","source":"\n"},{"metadata":{"trusted":true,"_uuid":"ac9287c0f885d0af3236cb68e3773f86a1873590","collapsed":true},"cell_type":"markdown","source":"**Introduction**\n\nHome Credit strives to broaden financial inclusion for the unbanked population by providing a positive and safe borrowing experience. In order to make sure this underserved population has a positive loan experience, Home Credit makes use of a variety of alternative data--including telco and transactional information--to predict their clients' repayment abilities. While Home Credit is currently using various statistical and machine learning methods to make these predictions, they're challenging Kagglers to help them unlock the full potential of their data. Doing so will ensure that clients capable of repayment are not rejected and that loans are given with a principal, maturity, and repayment calendar that will empower their clients to be successful.\n\nData Engineering - \n\nImpute Data Missing Method - Imputing refers to using a model to replace missing values.\n\nThere are many options we could consider when replacing a missing value, for example:\n•A constant value that has meaning within the domain, such as 0, distinct from all other values.\n•A value from another randomly selected record.\n•A mean, median or mode value for the column.\n•A value estimated by another predictive model.\n\nAny imputing performed on the training dataset will have to be performed on new data in the future when predictions are needed from the finalized model. This needs to be taken into consideration when choosing how to impute the missing values. For example, if you choose to impute with mean column values, these mean column values will need to be stored to file for later use on new data that has missing values.\nPandas provides the fillna() function for replacing missing values with a specific value. It is a flexible class that allows you to specify the value to replace (it can be something other than NaN) and the technique used to replace it (such as mean, median, or mode). The Imputer class operates directly on the NumPy array instead of the DataFrame.\n\n**Feautre Selections - **\n\nFeature selection is a process where you automatically select those features in your data that contribute most to the prediction variable or output in which you are interested.\n\nHaving irrelevant features in your data can decrease the accuracy of many models, especially linear algorithms like linear and logistic regression.\n\nThree benefits of performing feature selection before modeling your data are:\n•Reduces Overfitting: Less redundant data means less opportunity to make decisions based on noise.\n•Improves Accuracy: Less misleading data means modeling accuracy improves.\n•Reduces Training Time: Less data means that algorithms train faster.\n\nFeature Selection for Machine Learning -\n\n**1. Univariate Selection**\n\nStatistical tests can be used to select those features that have the strongest relationship with the output variable. The scikit-learn library provides the SelectKBest class that can be used with a suite of different statistical tests to select a specific number of features.\n\n\n**2. Recursive Feature Elimination**\n\nThe Recursive Feature Elimination (or RFE) works by recursively removing attributes and building a model on those attributes that remain. It uses the model accuracy to identify which attributes (and combination of attributes) contribute the most to predicting the target attribute. \n\n**3.Principal Component Analysis\n**\nPrincipal Component Analysis (or PCA) uses linear algebra to transform the dataset into a compressed form. Generally this is called a data reduction technique. A property of PCA is that you can choose the number of dimensions or principal component in the transformed result.\n\n**4.Feature Importance**\n\nBagged decision trees like Random Forest and Extra Trees can be used to estimate the importance of features.\n\n\nBuild Models & Evaluate the Best - Choose the Best Optimized model for this Problem statement which will be derived from confusion martix -\n\n**Logistic Regression**  - Like all regression analyses, the logistic regression is a predictive analysis. Logistic regression is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables.Logistic regression is a statistical method for analysing a dataset in which there are one or more independent variables that determine an outcome. The outcome is measured with a dichotomous variable (in which there are only two possible outcomes). It is used to predict a binary outcome (1 / 0, Yes / No, True / False) given a set of independent variables. To represent binary / categorical outcome, we use dummy variables. You can also think of logistic regression as a special case of linear regression when the outcome variable is categorical, where we are using log of odds as dependent variable. In simple words, it predicts the probability of occurrence of an event by fitting data to a logit function. Logistic regression was developed by statistician David Cox in 1958. This binary logistic model is used to estimate the probability of a binary response based on one or more predictor (or independent) variables (features). It allows one to say that the presence of a risk factor increases the probability of a given outcome by a specific percentage. Binary logistic regression is estimated using Maximum Likelihood Estimation (MLE), unlike linear regression which uses the Ordinary Least Squares (OLS) approach. MLE is an iterative procedure, meaning that it starts with a guess as to the best weight for each predictor variable (that is, each coefficient in the model) and then adjusts these coefficients repeatedly until there is no additional improvement in the ability to predict the value of the outcome variable (either 0 or 1) for each case\n\nLike all regression analyses, the logistic regression is a predictive analysis. Logistic regression is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables. Most often, we would want to predict our outcomes as YES/NO (1/0).\n\nFor example:\nIs your favorite football team going to win the match today? — yes/no (0/1)\nDoes a student pass in exam? — yes/no (0/1)\nThe logistic function is given by:\nf(x) = L/(1+e ^-k(x-x0))\nwhere\nL – Curve’s maximum value\nk – Steepness of the curve\nx0 – x value of Sigmoid’s midpoint\nA standard logistic function is called sigmoid function (k=1,x0=0,L=1)\nS(x) = 1/1+ ( e ^ - x)\nThe sigmoid curve :\nThe sigmoid function gives an ‘S’ shaped curve.\nThis curve has a finite limit of:\n‘0’ as x approaches −∞\n‘1’ as x approaches +∞\nThe output of sigmoid function when x=0 is 0.5\nThus, if the output is more tan 0.5 , we can classify the outcome as 1 (or YES) and if it is less than 0.5 , we can classify it as 0(or NO) .\nFor example: If the output is 0.65, we can say in terms of probability as:\n“There is a 65 percent chance that your favorite foot ball team is going to win today ” . Historically, the first application of logistic regression was \"given a certain dose of poison, how likely is this pest to die?\"\n\nOther applications that are pretty common: Life insurance actuaries use logistic regression to predict, based on given data on a policy holder (e.g. age, gender, results from a physical examination) the chances that the policy holder will die before the term of the policy expires. Political campaigns try to predict the chances that a voter will vote for their candidate (or do something else desirable, such as donate to the campaign).\n\nThus the output of the sigmoid function can not be just used to classify YES/NO, it can also be used to determine the probability of YES/NO.\n\n![image.png](attachment:image.png)\n\n**Decision Tree****** - Dividing efficiently based on maximum information gain is key to decision tree classifier. However, in real world with millions of data dividing into pure class in practically not feasible (it may take longer training time) and so we stop at points in nodes of tree when fulfilled with certain parameters. Suppose we have multiple features to divide the current working set. What feature should we select for division? Perhaps one that gives us less impurity. Decision tree at every stage selects the one that gives best information gain. When information gain is 0 means the feature does not divide the working set at all. Entropy is degree of randomness of elements or in other words it is measure of impurity. Mathematically, it can be calculated with the help of probability of the items as: It is negative summation of probability times the log of probability of item x.\nFor example, \nif we have items as number of dice face occurrence in a throw event as 1123,\nthe entropy is\n   p(1) = 0.5\n   p(2) = 0.25\n   p(3) = 0.25\nentropy = - (0.5 * log(0.5)) - (0.25 * log(0.25)) -(0.25 * log(0.25)\n        = 0.45\n\n\nSuppose we divide the classes into multiple branches as follows, the information gain at any node is defined as\nInformation Gain (n) =\n  Entropy(x) — ([weighted average] * entropy(children for feature))\n\n\n**K-Nearest Neighbors** -  The model representation for KNN is the entire training dataset.\n\nIt is as simple as that. KNN has no model other than storing the entire dataset, so there is no learning required. Efficient implementations can store the data using complex data structures like k-d trees to make look-up and matching of new patterns during prediction efficient. Because the entire training dataset is stored, you may want to think carefully about the consistency of your training data. It might be a good idea to curate it, update it often as new data becomes available and remove erroneous and outlier data.\n\nKNN makes predictions using the training dataset directly.\n\nPredictions are made for a new instance (x) by searching through the entire training set for the K most similar instances (the neighbors) and summarizing the output variable for those K instances. For regression this might be the mean output variable, in classification this might be the mode (or most common) class value.\n\nTo determine which of the K instances in the training dataset are most similar to a new input a distance measure is used. For real-valued input variables, the most popular distance measure is Euclidean distance.\n\nEuclidean distance is calculated as the square root of the sum of the squared differences between a new point (x) and an existing point (xi) across all input attributes j.\n\nEuclideanDistance(x, xi) = sqrt( sum( (xj – xij)^2 ) )\n\nOther popular distance measures include:\n•Hamming Distance: Calculate the distance between binary vectors (more).\n•Manhattan Distance: Calculate the distance between real vectors using the sum of their absolute difference. Also called City Block Distance (more).\n•Minkowski Distance: Generalization of Euclidean and Manhattan distance (more).\n\nThere are many other distance measures that can be used, such as Tanimoto, Jaccard, Mahalanobis and cosine distance. You can choose the best distance metric based on the properties of your data. If you are unsure, you can experiment with different distance metrics and different values of K together and see which mix results in the most accurate models.\n\nEuclidean is a good distance measure to use if the input variables are similar in type (e.g. all measured widths and heights). Manhattan distance is a good measure to use if the input variables are not similar in type (such as age, gender, height, etc.).\n\nThe value for K can be found by algorithm tuning. It is a good idea to try many different values for K (e.g. values from 1 to 21) and see what works best for your problem.\n\nThe computational complexity of KNN increases with the size of the training dataset. For very large training sets, KNN can be made stochastic by taking a sample from the training dataset from which to calculate the K-most similar instances.\n\nKNN has been around for a long time and has been very well studied. As such, different disciplines have different names for it, for example:\n•Instance-Based Learning: The raw training instances are used to make predictions. As such KNN is often referred to as instance-based learning or a case-based learning (where each training instance is a case from the problem domain).\n•Lazy Learning: No learning of the model is required and all of the work happens at the time a prediction is requested. As such, KNN is often referred to as a lazy learning algorithm.\n•Non-Parametric: KNN makes no assumptions about the functional form of the problem being solved. As such KNN is referred to as a non-parametric machine learning algorithm.\n\nKNN can be used for regression and classification problems. KNN for Regression\n\nWhen KNN is used for regression problems the prediction is based on the mean or the median of the K-most similar instances.\n\nKNN for Classification\n\nWhen KNN is used for classification, the output can be calculated as the class with the highest frequency from the K-most similar instances. Each instance in essence votes for their class and the class with the most votes is taken as the prediction.\n\nClass probabilities can be calculated as the normalized frequency of samples that belong to each class in the set of K most similar instances for a new data instance. For example, in a binary classification problem (class is 0 or 1):\n\np(class=0) = count(class=0) / (count(class=0)+count(class=1))\n\nIf you are using K and you have an even number of classes (e.g. 2) it is a good idea to choose a K value with an odd number to avoid a tie. And the inverse, use an even number for K when you have an odd number of classes.\n\nTies can be broken consistently by expanding K by 1 and looking at the class of the next most similar instance in the training dataset.\n\n**Curse of Dimensionality**\n\nKNN works well with a small number of input variables (p), but struggles when the number of inputs is very large.\n\nEach input variable can be considered a dimension of a p-dimensional input space. For example, if you had two input variables x1 and x2, the input space would be 2-dimensional.\n\nAs the number of dimensions increases the volume of the input space increases at an exponential rate.\n\nIn high dimensions, points that may be similar may have very large distances. All points will be far away from each other and our intuition for distances in simple 2 and 3-dimensional spaces breaks down. This might feel unintuitive at first, but this general problem is called the “Curse of Dimensionality“.\n\nBest Prepare Data for KNN\n•Rescale Data: KNN performs much better if all of the data has the same scale. Normalizing your data to the range [0, 1] is a good idea. It may also be a good idea to standardize your data if it has a Gaussian distribution.\n•Address Missing Data: Missing data will mean that the distance between samples can not be calculated. These samples could be excluded or the missing values could be imputed.\n•Lower Dimensionality: KNN is suited for lower dimensional data. You can try it on high dimensional data (hundreds or thousands of input variables) but be aware that it may not perform as well as other techniques. KNN can benefit from feature selection that reduces the dimensionality of the input feature space.\n\n\n**Linear Discriminant Analysis** -  Limitations of Logistic Regression\n\nLogistic regression is a simple and powerful linear classification algorithm. It also has limitations that suggest at the need for alternate linear classification algorithms.\n•Two-Class Problems. Logistic regression is intended for two-class or binary classification problems. It can be extended for multi-class classification, but is rarely used for this purpose.\n•Unstable With Well Separated Classes. Logistic regression can become unstable when the classes are well separated.\n•Unstable With Few Examples. Logistic regression can become unstable when there are few examples from which to estimate the parameters.\n\nLinear Discriminant Analysis does address each of these points and is the go-to linear method for multi-class classification problems. Even with binary-classification problems, it is a good idea to try both logistic regression and linear discriminant analysis.\n\nRepresentation of LDA Models\n\nThe representation of LDA is straight forward.\n\nIt consists of statistical properties of your data, calculated for each class. For a single input variable (x) this is the mean and the variance of the variable for each class. For multiple variables, this is the same properties calculated over the multivariate Gaussian, namely the means and the covariance matrix.\n\nThese statistical properties are estimated from your data and plug into the LDA equation to make predictions. These are the model values that you would save to file for your model.\n\nLet’s look at how these parameters are estimated.\n\nLearning LDA Models\n\nLDA makes some simplifying assumptions about your data:\n1.That your data is Gaussian, that each variable is is shaped like a bell curve when plotted.\n2.That each attribute has the same variance, that values of each variable vary around the mean by the same amount on average.\n\nWith these assumptions, the LDA model estimates the mean and variance from your data for each class. It is easy to think about this in the univariate (single input variable) case with two classes.\n\nThe mean (mu) value of each input (x) for each class (k) can be estimated in the normal way by dividing the sum of values by the total number of values.\n\nmuk = 1/nk * sum(x)\n\nWhere muk is the mean value of x for the class k, nk is the number of instances with class k. The variance is calculated across all classes as the average squared difference of each value from the mean.\n\nsigma^2 = 1 / (n-K) * sum((x – mu)^2)\n\nWhere sigma^2 is the variance across all inputs (x), n is the number of instances, K is the number of classes and mu is the mean for input x.\n\nMaking Predictions with LDA\n\nLDA makes predictions by estimating the probability that a new set of inputs belongs to each class. The class that gets the highest probability is the output class and a prediction is made.\n\nThe model uses Bayes Theorem to estimate the probabilities. Briefly Bayes’ Theorem can be used to estimate the probability of the output class (k) given the input (x) using the probability of each class and the probability of the data belonging to each class:\n\nP(Y=x|X=x) = (PIk * fk(x)) / sum(PIl * fl(x))\n\nWhere PIk refers to the base probability of each class (k) observed in your training data (e.g. 0.5 for a 50-50 split in a two class problem). In Bayes’ Theorem this is called the prior probability.\n\nPIk = nk/n\n\nThe f(x) above is the estimated probability of x belonging to the class. A Gaussian distribution function is used for f(x). Plugging the Gaussian into the above equation and simplifying we end up with the equation below. This is called a discriminate function and the class is calculated as having the largest value will be the output classification (y):\n\nDk(x) = x * (muk/siga^2) – (muk^2/(2*sigma^2)) + ln(PIk)\n\nDk(x) is the discriminate function for class k given input x, the muk, sigma^2 and PIk are all estimated from your data.\n\nHow to Prepare Data for LDA\n\nThis section lists some suggestions you may consider when preparing your data for use with LDA.\n•Classification Problems. This might go without saying, but LDA is intended for classification problems where the output variable is categorical. LDA supports both binary and multi-class classification.\n•Gaussian Distribution. The standard implementation of the model assumes a Gaussian distribution of the input variables. Consider reviewing the univariate distributions of each attribute and using transforms to make them more Gaussian-looking (e.g. log and root for exponential distributions and Box-Cox for skewed distributions).\n•Remove Outliers. Consider removing outliers from your data. These can skew the basic statistics used to separate classes in LDA such the mean and the standard deviation.\n•Same Variance. LDA assumes that each input variable has the same variance. It is almost always a good idea to standardize your data before using LDA so that it has a mean of 0 and a standard deviation of 1.\n\nExtensions to LDA\n\nLinear Discriminant Analysis is a simple and effective method for classification. Because it is simple and so well understood, there are many extensions and variations to the method. Some popular extensions include:\n•Quadratic Discriminant Analysis (QDA): Each class uses its own estimate of variance (or covariance when there are multiple input variables).\n•Flexible Discriminant Analysis (FDA): Where non-linear combinations of inputs is used such as splines.\n•Regularized Discriminant Analysis (RDA): Introduces regularization into the estimate of the variance (actually covariance), moderating the influence of different variables on LDA.\n\nThe original development was called the Linear Discriminant or Fisher’s Discriminant Analysis. The multi-class version was referred to Multiple Discriminant Analysis. These are all simply referred to as Linear Discriminant Analysis now.\n\n**Support Vector Machine** - Maximal-Margin Classifier\n\nThe Maximal-Margin Classifier is a hypothetical classifier that best explains how SVM works in practice.\n\nThe numeric input variables (x) in your data (the columns) form an n-dimensional space. For example, if you had two input variables, this would form a two-dimensional space.\n\nA hyperplane is a line that splits the input variable space. In SVM, a hyperplane is selected to best separate the points in the input variable space by their class, either class 0 or class 1. In two-dimensions you can visualize this as a line and let’s assume that all of our input points can be completely separated by this line. For example:\n\nB0 + (B1 * X1) + (B2 * X2) = 0\n\nWhere the coefficients (B1 and B2) that determine the slope of the line and the intercept (B0) are found by the learning algorithm, and X1 and X2 are the two input variables.\n\nYou can make classifications using this line. By plugging in input values into the line equation, you can calculate whether a new point is above or below the line.\n•Above the line, the equation returns a value greater than 0 and the point belongs to the first class (class 0).\n•Below the line, the equation returns a value less than 0 and the point belongs to the second class (class 1).\n•A value close to the line returns a value close to zero and the point may be difficult to classify.\n•If the magnitude of the value is large, the model may have more confidence in the prediction.\n\nThe distance between the line and the closest data points is referred to as the margin. The best or optimal line that can separate the two classes is the line that as the largest margin. This is called the Maximal-Margin hyperplane.\n\nThe margin is calculated as the perpendicular distance from the line to only the closest points. Only these points are relevant in defining the line and in the construction of the classifier. These points are called the support vectors. They support or define the hyperplane.\n\nThe hyperplane is learned from training data using an optimization procedure that maximizes the margin. Soft Margin Classifier\n\nIn practice, real data is messy and cannot be separated perfectly with a hyperplane.\n\nThe constraint of maximizing the margin of the line that separates the classes must be relaxed. This is often called the soft margin classifier. This change allows some points in the training data to violate the separating line.\n\nAn additional set of coefficients are introduced that give the margin wiggle room in each dimension. These coefficients are sometimes called slack variables. This increases the complexity of the model as there are more parameters for the model to fit to the data to provide this complexity.\n\nA tuning parameter is introduced called simply C that defines the magnitude of the wiggle allowed across all dimensions. The C parameters defines the amount of violation of the margin allowed. A C=0 is no violation and we are back to the inflexible Maximal-Margin Classifier described above. The larger the value of C the more violations of the hyperplane are permitted.\n\nDuring the learning of the hyperplane from data, all training instances that lie within the distance of the margin will affect the placement of the hyperplane and are referred to as support vectors. And as C affects the number of instances that are allowed to fall within the margin, C influences the number of support vectors used by the model.\n•The smaller the value of C, the more sensitive the algorithm is to the training data (higher variance and lower bias).\n•The larger the value of C, the less sensitive the algorithm is to the training data (lower variance and higher bias).\n\nSupport Vector Machines (Kernels)\n\nThe SVM algorithm is implemented in practice using a kernel.\n\nThe learning of the hyperplane in linear SVM is done by transforming the problem using some linear algebra, which is out of the scope of this introduction to SVM.\n\nA powerful insight is that the linear SVM can be rephrased using the inner product of any two given observations, rather than the observations themselves. The inner product between two vectors is the sum of the multiplication of each pair of input values.\n\nFor example, the inner product of the vectors [2, 3] and [5, 6] is 2*5 + 3*6 or 28.\n\nThe equation for making a prediction for a new input using the dot product between the input (x) and each support vector (xi) is calculated as follows:\n\nf(x) = B0 + sum(ai * (x,xi))\n\nThis is an equation that involves calculating the inner products of a new input vector (x) with all support vectors in training data. The coefficients B0 and ai (for each input) must be estimated from the training data by the learning algorithm.\n\nLinear Kernel SVM\n\nThe dot-product is called the kernel and can be re-written as:\n\nK(x, xi) = sum(x * xi)\n\nThe kernel defines the similarity or a distance measure between new data and the support vectors. The dot product is the similarity measure used for linear SVM or a linear kernel because the distance is a linear combination of the inputs.\n\nOther kernels can be used that transform the input space into higher dimensions such as a Polynomial Kernel and a Radial Kernel. This is called the Kernel Trick.\n\nIt is desirable to use more complex kernels as it allows lines to separate the classes that are curved or even more complex. This in turn can lead to more accurate classifiers.\n\nPolynomial Kernel SVM\n\nInstead of the dot-product, we can use a polynomial kernel, for example:\n\nK(x,xi) = 1 + sum(x * xi)^d\n\nWhere the degree of the polynomial must be specified by hand to the learning algorithm. When d=1 this is the same as the linear kernel. The polynomial kernel allows for curved lines in the input space.\n\nRadial Kernel SVM\n\nFinally, we can also have a more complex radial kernel. For example:\n\nK(x,xi) = exp(-gamma * sum((x – xi^2))\n\nWhere gamma is a parameter that must be specified to the learning algorithm. A good default value for gamma is 0.1, where gamma is often 0 < gamma < 1. The radial kernel is very local and can create complex regions within the feature space, like closed polygons in two-dimensional space.\n\nHow to Learn a SVM Model\n\nThe SVM model needs to be solved using an optimization procedure.\n\nYou can use a numerical optimization procedure to search for the coefficients of the hyperplane. This is inefficient and is not the approach used in widely used SVM implementations like LIBSVM. If implementing the algorithm as an exercise, you could use stochastic gradient descent.\n\nThere are specialized optimization procedures that re-formulate the optimization problem to be a Quadratic Programming problem. The most popular method for fitting SVM is the Sequential Minimal Optimization (SMO) method that is very efficient. It breaks the problem down into sub-problems that can be solved analytically (by calculating) rather than numerically (by searching or optimizing).\n\nData Preparation for SVM\n\nThis section lists some suggestions for how to best prepare your training data when learning an SVM model.\n•Numerical Inputs: SVM assumes that your inputs are numeric. If you have categorical inputs you may need to covert them to binary dummy variables (one variable for each category).\n•Binary Classification: Basic SVM as described in this post is intended for binary (two-class) classification problems. Although, extensions have been developed for regression and multi-class classification.\n\n**Summary - Support Vector/Linear Discriminant Analysis giving best accuracy (92% accurate result).  We will also try with ANN method to see if accuracy gets improved.**\n\n**ANN Model **- An **Artificial neuron network** (ANN) is a computational model based on the structure and functions of biological neural networks. Information that flows through the network affects the structure of the ANN because a neural network changes - or learns, in a sense - based on that input and output. \n\nANNs are considered nonlinear statistical data modeling tools where the complex relationships between inputs and outputs are modeled or patterns are found. \n\nANN is also known as a neural network.  An ANN has several advantages but one of the most recognized of these is the fact that it can actually learn from observing data sets. In this way, ANN is used as a random function approximation tool. These types of tools help estimate the most cost-effective and ideal methods for arriving at solutions while defining computing functions or distributions. ANN takes data samples rather than entire data sets to arrive at solutions, which saves both time and money. ANNs are considered fairly simple mathematical models to enhance existing data analysis technologies. \n\nANNs have three layers that are interconnected. The first layer consists of input neurons. Those neurons send data on to the second layer, which in turn sends the output neurons to the third layer. \n\nTraining an artificial neural network involves choosing from allowed models for which there are several associated algorithms.  Artificial Neural networks (ANN) or neural networks are computational algorithms.\n\nIt intended to simulate the behavior of biological systems composed of “neurons”. ANNs are computational models inspired by an animal’s central nervous systems. It is capable of machine learning as well as pattern recognition. These presented as systems of interconnected “neurons” which can compute values from inputs.\n\nA neural network is an oriented graph. It consists of nodes which in the biological analogy represent neurons, connected by arcs. It corresponds to dendrites and synapses. Each arc associated with a weight while at each node. Apply the values received as input by the node and define Activation function along the incoming arcs, adjusted by the weights of the arcs.\n\n![image.png](attachment:image.png)\n\nA neural network is a machine learning algorithm based on the model of a human neuron. The human brain consists of millions of neurons. It sends and process signals in the form of electrical and chemical signals. These neurons are connected with a special structure known as synapses. Synapses allow neurons to pass signals. From large numbers of simulated neurons neural networks forms.\n\nAn Artificial Neural Network is an information processing technique. It works like the way human brain processes information. ANN includes a large number of connected processing units that work together to process information. They also generate meaningful results from it.\n\nWe can apply Neural network not only for classification. It can also apply for regression of continuous target attributes.\n\nNeural networks find great application in data mining used in sectors. For example economics, forensics, etc and for pattern recognition. It can be also used for data classification in a large amount of data after careful training.\n\nA neural network may contain the following 3 layers:\n•Input layer – The activity of the input units represents the raw information that can feed into the network.\n•Hidden layer – To determine the activity of each hidden unit. The activities of the input units and the weights on the connections between the input and the hidden units. There may be one or more hidden layers.\n•Output layer – The behavior of the output units depends on the activity of the hidden units and the weights between the hidden and output units.\n\nArtificial Neural Network Layers\n\nArtificial Neural network is typically organized in layers. Layers are being made up of many interconnected ‘nodes’ which contain an ‘activation function’. A neural network may contain the following 3 layers:\n\n![image.png](attachment:image.png)\n\nInput layer\n\nThe purpose of the input layer is to receive as input the values of the explanatory attributes for each observation. Usually, the number of input nodes in an input layer is equal to the number of explanatory variables. ‘input layer’ presents the patterns to the network, which communicates to one or more ‘hidden layers’.\n\nThe nodes of the input layer are passive, meaning they do not change the data. They receive a single value on their input and duplicate the value to their many outputs. From the input layer, it duplicates each value and sent to all the hidden nodes.\n\nb. Hidden layer\n\nThe Hidden layers apply given transformations to the input values inside the network. In this, incoming arcs that go from other hidden nodes or from input nodes connected to each node. It connects with outgoing arcs to output nodes or to other hidden nodes. In hidden layer, the actual processing is done via a system of weighted ‘connections’. There may be one or more hidden layers. The values entering a hidden node multiplied by weights, a set of predetermined numbers stored in the program. The weighted inputs are then added to produce a single number.\n\nc. Output layer\n\nThe hidden layers then link to an ‘output layer‘. Output layer receives connections from hidden layers or from input layer. It returns an output value that corresponds to the prediction of the response variable. In classification problems, there is usually only one output node. The active nodes of the output layer combine and change the data to produce the output values.\n\nThe ability of the neural network to provide useful data manipulation lies in the proper selection of the weights. This is different from conventional information processing.\n\n4. Structure of a Neural Network\n\nThe structure of a neural network also referred to as its ‘architecture’ or ‘topology’. It consists of the number of layers, Elementary units. It also consists of Interconchangend Weight adjustment mechanism. The choice of the structure determines the results which are going to obtain. It is the most critical part of the implementation of a neural network.\n\nThe simplest structure is the one in which units distributes in two layers: An input layer and an output layer. Each unit in the input layer has a single input and a single output which is equal to the input. The output unit has all the units of the input layer connected to its input, with a combination function and a transfer function. There may be more than 1 output unit. In this case, resulting model is a linear or logistic regression.This is depending on whether transfer function is linear or logistic. The weights of the network are regression coefficients.\n\nBy adding 1 or more hidden layers between the input and output layers and units in this layer the predictive power of neural network increases. But a number of hidden layers should be as small as possible. This ensures that neural network does not store all information from learning set but can generalize it to avoid overfitting.\n\nOverfitting can occur. It occurs when weights make the system learn details of learning set instead of discovering structures. This happens when size of learning set is too small in relation to the complexity of the model.\n\nA hidden layer is present or not, the output layer of the network can sometimes have many units, when there are many classes to predict.\n\n5. Advantages and Disadvantages of Neural Networks\n\nLet us see few advantages and disadvantages of neural networks:\n•Neural networks perform well with linear and nonlinear data but a common criticism of neural networks, particularly in robotics, is that they require a large diversity of training for real-world operation. This is so because any learning machine needs sufficient representative examples in order to capture the underlying structure that allows it to generalize to new cases.\n•Neural networks works even if one or few units fail to respond to network but to implement large and effective software neural networks, much processing and storage resources need to be committed. While the brain has hardware tailored to the task of processing signals through a graph of neurons, simulating even a most simplified form on Von Neumann technology may compel a neural network designer to fill millions of database rows for its connections – which can consume vast amounts of computer memory and hard disk space.\n•Neural network learns from the analyzed data and does not require to reprogramming but they are referred to as black box” models, and provide very little insight into what these models really do. The user just needs to feed it input and watch it train and await the output.\n\n\n\n\n\n\n\n\n\n\n\n"},{"metadata":{"trusted":true,"_uuid":"fc92ddc10cadc52361b41d558a6ca227195c3db9","collapsed":true},"cell_type":"code","source":"\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\ndef removing_specialchar(str):\n    return str.replace('?',0)\n    \n# Importing the dataset\napp_train = pd.read_csv('../input/application_train.csv')\napp_test=pd.read_csv('../input/application_test.csv')\napp_train = app_train.apply(removing_specialchar,axis=1)\napp_train.head(10)\napp_train.isnull().sum()\nprint('Training data shape: ', app_train.shape)\napp_train.head()\nprint('Testing data shape: ', app_test.shape)\napp_test.head()\napp_train['TARGET'].value_counts()\napp_train['TARGET'].astype(int).plot.hist();\n\ndef missing_values_table(df):\n    mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n    mis_val_percent = 100 * df.isnull().sum() / len(df)\n        \n        # Make a table with the results\n    mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n    mis_val_table_ren_columns = mis_val_table.rename(\n    columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n    mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n    print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        \n    return mis_val_table_ren_columns\n    \nmissing_values = missing_values_table(app_train)\nmissing_values.head(20)        \n\n#to get unique count of type of data types.\napp_train.dtypes.value_counts()\n#app_train.select_dtypes('object').apply(pd.Series.nunique, axis = 0)\n# to get distinct value of categorical variable\napp_train.loc[:,app_train.dtypes=='object'].apply(pd.Series.nunique, axis = 0)\n\n##data Analysis/data Distribution\nplt.title(\"Credit Amount of Loan Applicant\")\nplt.show(app_train['AMT_CREDIT'].plot.hist(bins=100))\nplt.title(\"Income Amt\")\nplt.show(app_train['AMT_INCOME_TOTAL'].plot.hist(bins=50))\nplt.title(\"Price AMT\")\nplt.show(app_train['AMT_GOODS_PRICE'].plot.hist(bins=50))\nplt.title(\"Type of Suite\")\nplt.show(app_train.groupby('NAME_TYPE_SUITE').size().plot(kind='bar'))\nplt.show(app_train.groupby('TARGET').size().plot(kind='pie'))\napp_train.groupby('NAME_FAMILY_STATUS').size().plot(kind='pie')\nplt.title(\"Income Type\")\napp_train.groupby('NAME_INCOME_TYPE').size().plot(kind='pie')\nplt.title(\"Making Own Car\")\napp_train.groupby('FLAG_OWN_CAR').size().plot(kind='pie')\nplt.title(\"Making Own Realty\")\napp_train.groupby('FLAG_OWN_REALTY').size().plot(kind='pie')\nplt.title(\"Loan Type\")\napp_train.groupby('NAME_CONTRACT_TYPE').size().plot(kind='bar')\nplt.title(\"Occupation of Applicant\")\napp_train.groupby('OCCUPATION_TYPE').size().plot(kind='bar')\nplt.title(\"Type of Organization who applied for Loan\")\napp_train.groupby('ORGANIZATION_TYPE').size().plot(kind='pie')\nprint('Dimension of data')\nprint(app_train.shape)\nprint('Peek at the data')\nprint(app_train.head(20))\nprint('Statistical Summary')\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle_count = 0\n\n#Label encoder & One hot encoder to get missing value replaced for categorical value\n# Iterate through the columns\nfor col in app_train:\n    if app_train[col].dtype == 'object':\n        # If 2 or fewer unique categories\n        if len(list(app_train[col].unique())) <= 2:\n            # Train on the training data\n            le.fit(app_train[col])\n            # Transform both training and testing data\n            app_train[col] = le.transform(app_train[col])\n            app_test[col] = le.transform(app_test[col])\n            \n            # Keep track of how many columns were label encoded\n            le_count += 1\n            \nprint('%d columns were label encoded.' % le_count)\n\napp_train = pd.get_dummies(app_train)\napp_test = pd.get_dummies(app_test)\nprint('Training Features shape: ', app_train.shape)\nprint('Testing Features shape: ', app_test.shape)\n\ntrain_labels = app_train['TARGET']\napp_train, app_test = app_train.align(app_test, join = 'inner', axis = 1)\napp_train['TARGET'] = train_labels\n#Finding the anamalies\n#AGE Column - Converting them into years\n(app_train['DAYS_BIRTH']/-365).describe()\n#Employement Days\napp_train['DAYS_EMPLOYED'].describe()\napp_train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram');\nplt.xlabel('Days Employment');\n\nanom = app_train[app_train['DAYS_EMPLOYED'] == 365243]\nnon_anom = app_train[app_train['DAYS_EMPLOYED'] != 365243]\nprint('The non-anomalies default on %0.2f%% of loans' % (100 * non_anom['TARGET'].mean()))\nprint('The anomalies default on %0.2f%% of loans' % (100 * anom['TARGET'].mean()))\nprint('There are %d anomalous days of employment' % len(anom))\n\napp_train['DAYS_EMPLOYED_ANOM'] = app_train[\"DAYS_EMPLOYED\"] == 365243\n# Replace the anomalous values with nan\napp_train['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\napp_train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram');\nplt.xlabel('Days Employment');\n#finding the correlation between variable with matching Target variable 1 or 0\nimport os\n# Suppress warnings \nimport warnings\nwarnings.filterwarnings('ignore')\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncorrelations = app_train.corr()['TARGET'].sort_values()\nprint('Most Positive Correlations:\\n', correlations.tail(15))\nprint('\\nMost Negative Correlations:\\n', correlations.head(15))\n\napp_train['DAYS_BIRTH'] = abs(app_train['DAYS_BIRTH'])\napp_train['DAYS_BIRTH'].corr(app_train['TARGET'])  \n\nplt.style.use('fivethirtyeight')\n# Plot the distribution of ages in years\nplt.hist(app_train['DAYS_BIRTH'] / 365, edgecolor = 'k', bins = 25)\nplt.title('Age of Client'); plt.xlabel('Age (years)'); plt.ylabel('Count');\n\nplt.figure(figsize = (10, 8))\n\n# KDE plot of loans that were repaid on time\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 0, 'DAYS_BIRTH'] / 365, label = 'target == 0')\n\n# KDE plot of loans which were not repaid on time\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 1, 'DAYS_BIRTH'] / 365, label = 'target == 1')\n\n# Labeling of plot\nplt.xlabel('Age (years)'); plt.ylabel('Density'); plt.title('Distribution of Ages'); \n\nage_data = app_train[['TARGET', 'DAYS_BIRTH']]\nage_data['YEARS_BIRTH'] = age_data['DAYS_BIRTH'] / 365\n\n# Bin the age data\nage_data['YEARS_BINNED'] = pd.cut(age_data['YEARS_BIRTH'], bins = np.linspace(20, 70, num = 11))\nage_data.head(10)   \n\nage_groups  = age_data.groupby('YEARS_BINNED').mean()\nage_groups       \n\nplt.figure(figsize = (8, 8))\n\n# Graph the age bins and the average of the target as a bar plot\nplt.bar(age_groups.index.astype(str), 100 * age_groups['TARGET'])\n\n# Plot labeling\nplt.xticks(rotation = 75); plt.xlabel('Age Group (years)'); plt.ylabel('Failure to Repay (%)')\nplt.title('Failure to Repay by Age Group');\n\n#finding the correlation between variable with matching Target variable 1 or 0\nimport os\n# Suppress warnings \nimport warnings\nwarnings.filterwarnings('ignore')\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncorrelations = app_train.corr()['TARGET'].sort_values()\nprint('Most Positive Correlations:\\n', correlations.tail(15))\nprint('\\nMost Negative Correlations:\\n', correlations.head(15))\n\napp_train['DAYS_BIRTH'] = abs(app_train['DAYS_BIRTH'])\napp_train['DAYS_BIRTH'].corr(app_train['TARGET'])  \n\nplt.style.use('fivethirtyeight')\n# Plot the distribution of ages in years\nplt.hist(app_train['DAYS_BIRTH'] / 365, edgecolor = 'k', bins = 25)\nplt.title('Age of Client'); plt.xlabel('Age (years)'); plt.ylabel('Count');\n\nplt.figure(figsize = (10, 8))\n\n# KDE plot of loans that were repaid on time\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 0, 'DAYS_BIRTH'] / 365, label = 'target == 0')\n\n# KDE plot of loans which were not repaid on time\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 1, 'DAYS_BIRTH'] / 365, label = 'target == 1')\n\n# Labeling of plot\nplt.xlabel('Age (years)'); plt.ylabel('Density'); plt.title('Distribution of Ages'); \n\nage_data = app_train[['TARGET', 'DAYS_BIRTH']]\nage_data['YEARS_BIRTH'] = age_data['DAYS_BIRTH'] / 365\n\n# Bin the age data\nage_data['YEARS_BINNED'] = pd.cut(age_data['YEARS_BIRTH'], bins = np.linspace(20, 70, num = 11))\nage_data.head(10)   \n\nage_groups  = age_data.groupby('YEARS_BINNED').mean()\nage_groups       \n\nplt.figure(figsize = (8, 8))\n\n# Graph the age bins and the average of the target as a bar plot\nplt.bar(age_groups.index.astype(str), 100 * age_groups['TARGET'])\n\n# Plot labeling\nplt.xticks(rotation = 75); plt.xlabel('Age Group (years)'); plt.ylabel('Failure to Repay (%)')\nplt.title('Failure to Repay by Age Group');\n\next_data = app_train[['TARGET', 'SK_ID_CURR','EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH','NAME_EDUCATION_TYPE_Higher education','CODE_GENDER_F','NAME_INCOME_TYPE_Pensioner','DAYS_EMPLOYED','ORGANIZATION_TYPE_XNA','FLOORSMAX_AVG','FLOORSMAX_MEDI','FLOORSMAX_MODE','EMERGENCYSTATE_MODE_No','HOUSETYPE_MODE_block of flats','AMT_GOODS_PRICE','REGION_POPULATION_RELATIVE','DAYS_BIRTH']]\next_data1 = app_train[['TARGET', 'SK_ID_CURR','EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\next_data_corrs = ext_data1.corr()\next_data_corrs   \n\next_test = app_test[['SK_ID_CURR','EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH','NAME_EDUCATION_TYPE_Higher education','CODE_GENDER_F','NAME_INCOME_TYPE_Pensioner','DAYS_EMPLOYED','ORGANIZATION_TYPE_XNA','FLOORSMAX_AVG','FLOORSMAX_MEDI','FLOORSMAX_MODE','EMERGENCYSTATE_MODE_No','HOUSETYPE_MODE_block of flats','AMT_GOODS_PRICE','REGION_POPULATION_RELATIVE','DAYS_BIRTH']]\n\ny = ext_data['TARGET']\ntrain = ext_data.drop('TARGET',1)   \n\nplt.figure(figsize = (8, 6))\n\n# Heatmap of correlations\nsns.heatmap(ext_data_corrs, cmap = plt.cm.RdYlBu_r, vmin = -0.25, annot = True, vmax = 0.6)\nplt.title('Correlation Heatmap');   \n\nplt.figure(figsize = (10, 12))\n\nfor i, source in enumerate(['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']):\n    \n    # create a new subplot for each source\n    plt.subplot(3, 1, i + 1)\n    # plot repaid loans\n    sns.kdeplot(app_train.loc[app_train['TARGET'] == 0, source], label = 'target == 0')\n    # plot loans that were not repaid\n    sns.kdeplot(app_train.loc[app_train['TARGET'] == 1, source], label = 'target == 1')\n    \n    # Label the plots\n    plt.title('Distribution of %s by Target Value' % source)\n    plt.xlabel('%s' % source); plt.ylabel('Density');\n    \nplt.tight_layout(h_pad = 2.5)    \n\npoly_features = app_train[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH', 'TARGET']]\npoly_features_test = app_test[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n#dataset = pd.read_csv('../input/application_train.csv')\ny = app_train['TARGET']\nfeature_names = app_train[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH','NAME_EDUCATION_TYPE_Higher education','CODE_GENDER_F','NAME_INCOME_TYPE_Pensioner','ORGANIZATION_TYPE_XNA','FLOORSMAX_AVG','FLOORSMAX_MEDI','EMERGENCYSTATE_MODE_No','HOUSETYPE_MODE_block of flats','AMT_GOODS_PRICE','REGION_POPULATION_RELATIVE','DAYS_EMPLOYED','AMT_ANNUITY','AMT_INCOME_TOTAL','DAYS_REGISTRATION','DAYS_LAST_PHONE_CHANGE','DAYS_ID_PUBLISH']]\nX = feature_names\n#X=app_train\n\n\nfrom sklearn.preprocessing import Imputer\nimputer = Imputer(missing_values = 'NaN', strategy = 'median', axis = 0)\nimputer = imputer.fit(X)\nX = imputer.transform(X)\n\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\ntest = SelectKBest(score_func=chi2, k=4)\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression()\nrfe = RFE(model, 10)\nfit = rfe.fit(X, y)\nprint(\"Num Features: -->\",fit.n_features_)\nprint(\"Selected Features: %s\",fit.support_)\nprint(\"Feature Ranking: %s\",fit.ranking_)\nfor idx,i in enumerate(fit.ranking_):\n    if i==1:\n        print (i,idx)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5e1ade7953f0bcfc490c76ced25531ce8f4d1d67"},"cell_type":"code","source":"from sklearn.preprocessing import Imputer\nimputer = Imputer(strategy = 'median')\ntrain = imputer.fit_transform(train)\nfrom sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(train, y, test_size = 0.2, random_state = 0)\n\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test) \n\next_test = app_test[['SK_ID_CURR','EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH','NAME_EDUCATION_TYPE_Higher education','CODE_GENDER_F','NAME_INCOME_TYPE_Pensioner','DAYS_EMPLOYED','ORGANIZATION_TYPE_XNA','FLOORSMAX_AVG','FLOORSMAX_MEDI','FLOORSMAX_MODE','EMERGENCYSTATE_MODE_No','HOUSETYPE_MODE_block of flats','AMT_GOODS_PRICE','REGION_POPULATION_RELATIVE','DAYS_BIRTH']]\nfrom sklearn.preprocessing import Imputer\nimputer = Imputer(strategy = 'median')\ntrain = imputer.fit_transform(train)\nimputer = Imputer(strategy = 'median')\ntest = imputer.fit_transform(ext_test)\n\nfrom sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\nprint('Logistic regression output :->')\nprint(\"\\n\")\nprint('Accuracy of Logistic regression classifier on training set: {:.2f}'\n     .format(logreg.score(X_train, y_train)))\n\ny_pred = logreg.predict(X_test)\nlog_reg_pred = logreg.predict_proba(X_test)[:, 1]\nsubmit = ext_test[['SK_ID_CURR']]\n#submit['TARGET'] = log_reg_pred*100\n#submit['TARGET_1']= np.where(log_reg_pred > .5,1,0)\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(\"\\n\")\nprint(\"Confusion metrics of Logistic regression- :\", cm)\nprint(\"\\n\")\nfrom sklearn.tree import DecisionTreeClassifier\nclf = DecisionTreeClassifier().fit(X_train, y_train)\nprint('Accuracy of Decision Tree classifier on training set: {:.2f}'\n     .format(clf.score(X_train, y_train)))\nprint('Accuracy of Decision Tree classifier on test set: {:.2f}'\n     .format(clf.score(X_test, y_test)))\nprint(\"\\n\")\ny_pred = clf.predict(X_test)\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(\"\\n\")\nprint(\"Confusion metrics of Decision Tree- :\", cm)\nprint(\"\\n\")\n\nprint('KNN regression output :->')\n\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier()\nknn.fit(X_train, y_train)\nprint('Accuracy of K-NN classifier on training set: {:.2f}'\n     .format(knn.score(X_train, y_train)))\nprint('Accuracy of K-NN classifier on test set: {:.2f}'\n     .format(knn.score(X_test, y_test)))\n\nprint(\"\\n\")\ny_pred = knn.predict(X_test)\nprint(\"\\n\")\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\n\nprint(cm)\nprint(\"\\n\")\n\nprint('LinearDiscriminat Analysis regression output :->')\nprint(\"\\n\")\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nlda = LinearDiscriminantAnalysis()\nlda.fit(X_train, y_train)\nprint('Accuracy of LDA classifier on training set: {:.2f}'\n     .format(lda.score(X_train, y_train)))\nprint('Accuracy of LDA classifier on test set: {:.2f}'\n     .format(lda.score(X_test, y_test)))\n\ny_pred = lda.predict(X_test)\n\n# Making the Confusion Matrix\nprint(\"\\n\")\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\n\nprint(\"\\n\")\nprint(cm)\nprint(\"\\n\")\n\nprint('Support Vector Classification output :->')\nprint(\"\\n\")\nfrom sklearn.svm import SVC\nsvm = SVC()\nsvm.fit(X_train, y_train)\nprint('Accuracy of SVM classifier on training set: {:.2f}'\n     .format(svm.score(X_train, y_train)))\nprint('Accuracy of SVM classifier on test set: {:.2f}'\n     .format(svm.score(X_test, y_test)))\n\ny_pred = svm.predict(X_test)\nprint(\"\\n\")\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\n\nprint(\"\\n\")\nprint(cm)\nprint(\"\\n\")\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\npred = clf.predict(X_test)\nprint(confusion_matrix(y_test, pred))\nprint(classification_report(y_test, pred))\n\nprint('KNN regression output :->')\n\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier()\nknn.fit(X_train, y_train)\nprint('Accuracy of K-NN classifier on training set: {:.2f}'\n     .format(knn.score(X_train, y_train)))\nprint('Accuracy of K-NN classifier on test set: {:.2f}'\n     .format(knn.score(X_test, y_test)))\n\nprint(\"\\n\")\ny_pred = knn.predict(X_test)\nprint(\"\\n\")\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\n\nprint(cm)\nprint(\"\\n\")\n\nfrom sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\n# Part 2 - Now let's make the ANN!\n\n# Importing the Keras libraries and packages\n#import keras\nfrom keras.models import Sequential\n\nfrom keras.layers import Dense\n\n# Initialising the ANN\nclassifier = Sequential()\n\n# Adding the input layer and the first hidden layer\nclassifier.add(Dense(output_dim = 11, init = 'uniform', activation = 'relu', input_dim = 20))\n\n# Adding the second hidden layer\nclassifier.add(Dense(output_dim = 11, init = 'uniform', activation = 'relu'))\n\n# Adding the output layer\nclassifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))\n\n#model.compile(loss='mean_squared_error', optimizer='adam')\n\n# Compiling the ANN\nclassifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n# Fitting the ANN to the Training set\nclassifier.fit(X_train, y_train, batch_size = 500, nb_epoch = 500)\n\n# Part 3 - Making the predictions and evaluating the model\n\n# Predicting the Test set results\ny_pred = classifier.predict(test)\n#log_reg_pred = logreg.predict_proba(test)[:, 1]\n#submit = ext_test[['SK_ID_CURR']]\n#submit['TARGET'] = log_reg_pred*100\n#submit['TARGET_1']= np.where(log_reg_pred > .5,1,0)\n\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a255300fa40d2273e2e39525722ef93ffd7b5101","collapsed":true},"cell_type":"code","source":"\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f47e262eb11595548252d51c3a78503f0b4a7a55","collapsed":true},"cell_type":"code","source":"\n\n\n        ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b81a785e26bb7d78be10b472e53254529c470c60","trusted":true,"collapsed":true},"cell_type":"code","source":"   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a019c7c863b930149ed4d85461d598950997e566","collapsed":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"af3211ad05ca8aab31881f5da44a1bb01e730cdb","collapsed":true},"cell_type":"code","source":"\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"33dea3d9b5943903ef0ce367aae81873277c64ea","collapsed":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eb90bc38d8d1f67d8004a55cb6e01fde0d6ea6ab","collapsed":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2ba9b0128e4a5e6a985fbbc59f1d2b261968211f","collapsed":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1cc998fac5e3df392a48967af3804d1275289298","collapsed":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}