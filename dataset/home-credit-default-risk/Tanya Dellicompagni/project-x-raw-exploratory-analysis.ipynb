{"cells":[{"metadata":{"trusted":true,"_uuid":"1c5ed7e406951970fb2cdc42a4187a0376a30308"},"cell_type":"markdown","source":"#Hi all I have created this so we can run exploratory analysis outside of the models and practice what is taught in labs. \n> #Feel free to add, edit & comment. \n"},{"metadata":{"trusted":true,"_uuid":"d76d5c7847d47ec047670911a6bef57e34e20b12"},"cell_type":"code","source":"#Lab Code for statistical analysis\n\nimport pandas as pd, numpy as np, os, gc, matplotlib.pyplot as plt, seaborn as sb, re, warnings, calendar, sys\nfrom numpy import arange\nget_ipython().run_line_magic('matplotlib', 'inline')\nwarnings.filterwarnings('ignore'); np.set_printoptions(suppress=True); pd.options.mode.chained_assignment = None\npd.set_option('display.float_format', lambda x: '%.3f' % x)\nglobal directory; directory = '../input'\n\ndef files(): return os.listdir(directory)\n\ndef read_clean(data):\n    data.columns = [str(x.lower().strip().replace(' ','_')) for x in data.columns]\n    seen = {}; columns = []; i = 0\n    for i,x in enumerate(data.columns):\n        if x in seen: columns.append(x+'_{}'.format(i))\n        else: columns.append(x)\n        seen[x] = None\n        \n    for x in data.columns[data.count()/len(data) < 0.0001]: del data[x];\n    gc.collect();\n    try: data = data.replace({'':np.nan,' ':np.nan});\n    except: pass;\n    \n    if len(data) < 10000: l = len(data);\n    else: l = 10000;\n    sample = data.sample(l);size = len(sample);\n    \n    for x in sample.columns:\n        ints = pd.to_numeric(sample[x], downcast = 'integer', errors = 'coerce')\n        if ints.count()/size > 0.97:\n            minimum = ints.min()\n            if minimum > 0: data[x] = pd.to_numeric(data[x], downcast = 'unsigned', errors = 'coerce')\n            else: data[x] = pd.to_numeric(data[x], downcast = 'integer', errors = 'coerce')\n        else:\n            floats = pd.to_numeric(sample[x], downcast = 'float', errors = 'coerce')\n            if floats.count()/size > 0.97: data[x] = pd.to_numeric(data[x], downcast = 'float', errors = 'coerce')\n            else:\n                dates = pd.to_datetime(sample[x], errors = 'coerce')\n                if dates.count()/size > 0.97: data[x] = pd.to_datetime(data[x], errors = 'coerce')\n    return data.reset_index(drop = True)\n\ndef read(x):\n    '''Kaggle Reading in CSV files.\n    Just type read('file.csv'), and you'll get back a Table.'''\n    \n    file = '{}/{}'.format(directory,x)\n    try:     data = pd.read_csv(file)\n    except:  data = pd.read_csv(file, encoding = 'latin-1')\n    return read_clean(data)\n\ndef tally(column, minimum = 0, top = None, graph = False, percent = False, multiple = False, lowercase = False, min_count = 1):\n    '''Provides a tally count of all values in a COLUMN.\n        1. minimum  =  (>0)          Least count of item to show.\n        2. top      =  (-1,>0)       Only show top N objects\n        3. graph    =  (False,True)  Show bar graph of results\n        4. percent  =  (False,>0)    Instead of showing counts, show percentages of total count\n        \n       multiple = False/True.\n       If True, counts and tallies objects in list of lists (Count Vectorizer)\n       \n       lowercase = True / False.\n       If True, lowers all text firsrt. So A == a\n       \n       min_count >= 1\n       If a column sum for tag has less than min_count, discard whole column\n    '''\n    if multiple == False:\n        counts = column.value_counts().astype('uint')\n        counts = counts[counts >= minimum][:top]\n        counts = pd.DataFrame(counts).reset_index()\n        counts.columns = [column.name, 'tally']\n        if percent: \n            counts['tally'] /= counts['tally'].sum()/100\n            counts['tally'] = counts['tally']\n        if graph:\n            C = counts[::-1]\n            C.plot.barh(x = column.name, y = 'tally', legend = False); plt.show();\n        return counts\n    else:\n        from sklearn.feature_extraction.text import CountVectorizer\n        column = column.fillna('<NAN>')\n        if type(column.iloc[0]) != list: column = column.apply(lambda x: [x])\n        counter = CountVectorizer(lowercase = lowercase, tokenizer = lambda x: x, dtype = np.uint32, min_df = min_count)\n        counter.fit(column)\n        counts = pd.DataFrame(counter.transform(column).toarray())\n        counts.columns = [column.name+'_('+str(x)+')' for x in counter.get_feature_names()]\n        return counts\n    \n    \ndef describe(data):\n    '''Provides an overview of your data\n        1. dtype    =  Column type\n        2. missing% =  % of the column that is missing\n        3. nunique  =  Number of unique values in column\n        4. top3     =  Top 3 most occuring items\n        5. min      =  Minimum value. If not a number column, then empty\n        6. mean     =  Average value. If not a number column, then empty\n        7. median   =  Middle value. So sort all numbers, and get middle. If not a number column, then empty\n        8. max      =  Maximum value. If not a number column, then empty\n        9. sample   =  Random 2 elements\n        10. name    =  Column Name\n    '''\n    dtypes = dtype(data)\n    length = len(data)\n    missing = ((length - data.count())/length*100)\n    \n    N = [];    most3 = []\n    for dt,col in zip(dtypes,data.columns):\n        if dt != 'datetime':\n            U = data[col].value_counts()\n            N.append(len(U))\n            if U.values[0] > 1: most3.append(U.index[:3].tolist())\n            else: most3.append([]);\n        else: N.append(0); most3.append([]);\n            \n    df = pd.concat([dtypes, missing], 1)\n    df.columns = ['dtype','missing%']\n    df['nunique'] = N; df['top3'] = most3\n    \n    numbers = list(data.columns[df['dtype'].isin(('uint','int','float'))])\n    df['min'] = data.min()\n    df['mean'] = data[numbers].mean()\n    df['median'] = data[numbers].median()\n    df['max'] = data.max()\n    df['sample'] = data.apply(lambda x : x.sample(2).values.tolist())\n    df['name'] = list(data.columns)\n    return df.sort_values(['missing%', 'nunique', 'dtype'], ascending = [False, False, True]).reset_index(drop = True)\n\n\ndef Checker(x):\n    if type(x) is pd.DataFrame: return 0\n    elif type(x) is pd.Series: return 1\n    else: return -1\n\ndef columns(data): return list(data.columns)\ndef rows(data): return list(data.index)\ndef index(data): return list(data.index)\ndef head(data, n = 10): return data.head(n)\ndef tail(data, n = 10): return data.tail(n)\ndef sample(data, n = 10): return data.sample(n)\n\ndef dtype(data):\n    what = Checker(data)\n    if what == 0:\n        dtypes = data.dtypes.astype('str')\n        dtypes = dtypes.str.split(r'\\d').str[0]\n    else:\n        dtypes = str(data.dtypes)\n        dtypes = re.split(r'\\d', dtypes)[0]\n    return dtypes\n\ndef mean(data):\n    what = Checker(data)\n    _dt = ('uint','int','float')\n    if what == 0:\n        dtypes = dtype(data)\n        numbers = data.columns[dtypes.isin(_dt)]\n        return data[numbers].mean()\n    elif what == 1:\n        dtypes = dtype(data)\n        if dtypes in _dt: return data.mean()\n        else: return np.nan\n    else:\n        try:     return np.nanmean(data)\n        except:  return np.nan\n        \ndef std(data):\n    what = Checker(data)\n    _dt = ('uint','int','float')\n    if what == 0:\n        dtypes = dtype(data)\n        numbers = data.columns[dtypes.isin(_dt)]\n        return data[numbers].std()\n    elif what == 1:\n        dtypes = dtype(data)\n        if dtypes in _dt: return data.std()\n        else: return np.nan\n    else:\n        try:     return np.nanstd(data)\n        except:  return np.nan\n        \ndef var(data):\n    what = Checker(data)\n    _dt = ('uint','int','float')\n    if what == 0:\n        dtypes = dtype(data)\n        numbers = data.columns[dtypes.isin(_dt)]\n        return data[numbers].var()\n    elif what == 1:\n        dtypes = dtype(data)\n        if dtypes in _dt: return data.var()\n        else: return np.nan\n    else:\n        try:     return np.nanvar(data)\n        except:  return np.nan\n        \ndef log(data):\n    what = Checker(data)\n    _dt = ('uint','int','float')\n    if what == 0:\n        dtypes = dtype(data)\n        numbers = data.columns[dtypes.isin(_dt)]\n        x = np.log(data[numbers])\n        x[np.isinf(x)] = np.nan\n        return pd.Series(x)\n    elif what == 1:\n        dtypes = dtype(data)\n        if dtypes in _dt:\n            x = np.log(data)\n            x[np.isinf(x)] = np.nan\n            return x\n        else: return np.nan\n    else:\n        try:\n            x = np.log(data)\n            x[np.isinf(x)] = np.nan\n            return x\n        except:  return np.nan\n        \ndef median(data):\n    what = Checker(data)\n    _dt = ('uint','int','float')\n    if what == 0:\n        dtypes = dtype(data)\n        numbers = data.columns[dtypes.isin(_dt)]\n        return data[numbers].median()\n    elif what == 1:\n        dtypes = dtype(data)\n        if dtypes in _dt: return data.median()\n        else: return np.nan\n    else:\n        try:     return np.nanmedian(data)\n        except:  return np.nan\n        \ndef minimum(data):\n    what = Checker(data)\n    if what == 0:      return data.min()\n    elif what == 1:    return data.min()\n    else:              return np.min(data)\n        \ndef maximum(data):\n    what = Checker(data)\n    if what == 0:      return data.max()\n    elif what == 1:    return data.max()\n    else:              return np.max(data)\n    \ndef missing(data):\n    what = Checker(data)\n    if what >= 0:      return pd.isnull(data)\n    else:              return np.isnan(data)\n    \ndef count(data):\n    what = Checker(data)\n    if what >= 0:      return data.count()\n    else:              return len(data)\n    \ndef nunique(data):\n    what = Checker(data)\n    if what >= 0:      return data.nunique()\n    else:              return len(np.unique(data))\n    \ndef unique(data):\n    if type(data) is pd.DataFrame:\n        uniques = []\n        for x in data.columns:\n            uniques.append(data[x].unique())\n        df = pd.Series(uniques)\n        df.index = data.columns\n        return df\n    elif type(data) is pd.Series: return data.unique()\n    else:              return np.unique(data)\n    \ndef total(data):\n    what = Checker(data)\n    _dt = ('uint','int','float')\n    if what == 0:\n        dtypes = dtype(data)\n        numbers = data.columns[dtypes.isin(_dt)]\n        return data[numbers].sum()\n    elif what == 1:\n        dtypes = dtype(data)\n        if dtypes in _dt: return data.sum()\n        else: return np.nan\n    else:\n        try:     return np.nansum(data)\n        except:  return np.nan\n        \ndef time_number(date): return hours(date)+minutes(date)/60+seconds(date)/60**2\ndef hours_minutes(date): return hours(date)+minutes(date)/60\ndef hours(date): return date.dt.hour\ndef minutes(date): return date.dt.minute\ndef seconds(date): return date.dt.second\ndef month(date): return date.dt.month\ndef year(date): return date.dt.year\ndef day(date): return date.dt.day\ndef weekday(date): return date.dt.weekday\ndef leap_year(date): return year(date).apply(calendar.isleap)\ndef date_number(date): return year(date)+month(date)/12+day(date)/(365+leap_year(date)*1)\ndef year_month(date): return year(date)+month(date)/12\n\ndef hcat(*columns):\n    cols = []\n    for c in columns:\n        if c is None: continue;\n        if type(c) in (list, tuple): \n            for i in c:\n                if type(i) not in (pd.DataFrame, pd.Series): cols.append(pd.Series(i))\n                else: cols.append(i)\n        elif type(c) not in (pd.DataFrame, pd.Series): cols.append(pd.Series(c))\n        else: cols.append(c)\n    return pd.concat(cols, 1)\n\ndef vcat(*columns):\n    cols = []\n    for c in columns:\n        if c is None: continue;\n        if type(c) in (list, tuple): \n            for i in c:\n                if type(i) not in (pd.DataFrame, pd.Series): cols.append(pd.Series(i))\n                else: cols.append(i)\n        elif type(c) not in (pd.DataFrame, pd.Series): cols.append(pd.Series(c))\n        else: cols.append(c)\n    return pd.concat(cols, 0)\n\ndef melt(data, columns):\n    '''Converts a dataset into long form'''\n    return data.melt(id_vars = columns)\n    \ndef tabulate(*columns, method = 'count'):\n    '''Splits columns into chunks, and counts the occurences in each group.\n        Remember - tabulate works on the LAST column passed.\n        Options:\n            1. count            = Pure Count in group\n            2. count_percent    = Percentage of Count in group\n            3. mean             = Mean in group\n            4. median           = Median in group\n            5. max              = Max in group\n            6. min              = Min in group\n            7. sum_percent      = Percentage of Sum in group\n        Eg:\n            Apple | 1\n            ---------\n            Orange| 3\n            ---------\n            Apple | 2\n            ---------\n        Becomes:\n            Apple | 1 | 1\n            -------------\n                  | 2 | 1\n            -------------\n            Orange| 3 | 1\n        \n        NOTE --------\n            method can be a list of multiple options.\n    '''\n    if type(method) in (list, tuple):\n        xs = []\n        for x in method:\n            g = tabulate(*columns, method = x)\n            xs.append(g)\n        xs = hcat(xs)\n        xs = xs.T.drop_duplicates().T\n        return read_clean(xs)        \n    else:\n        def percent(series):\n            counts = series.count()\n            return counts.sum()\n\n        data = hcat(*columns)\n        columns = data.columns.tolist()\n\n        if method in ('count', 'count_percent'):\n            groups = data.groupby(data.columns.tolist()).apply(lambda x: x[data.columns[-1]].count())\n\n            if method == 'count_percent':\n                groups = groups.reset_index()\n                groups.columns = list(groups.columns[:-1])+['Group_Count']\n                right = data.groupby(columns[:-1]).count().reset_index()\n                right.columns = list(right.columns[:-1])+['Group_Sum']\n\n                groups = pd.merge(left = groups, right = right, left_on = columns[:-1], right_on = columns[:-1])\n                groups['Percent%'] = groups['Group_Count']/groups['Group_Sum']*100\n                groups = groups[columns+['Percent%']]\n                return groups\n\n        elif method == 'mean': groups = data.groupby(data.columns.tolist()[:-1]).apply(lambda x: x[data.columns[-1]].mean())\n        elif method == 'median': groups = data.groupby(data.columns.tolist()[:-1]).apply(lambda x: x[data.columns[-1]].median())\n        elif method == 'max': groups = data.groupby(data.columns.tolist()[:-1]).apply(lambda x: x[data.columns[-1]].max())\n        elif method == 'min': groups = data.groupby(data.columns.tolist()[:-1]).apply(lambda x: x[data.columns[-1]].min())\n        elif method == 'sum_percent':\n            groups = data.groupby(data.columns.tolist()[:-1]).apply(lambda x: x[data.columns[-1]].sum()).reset_index()\n            groups.columns = list(groups.columns[:-1])+['Group_Count']\n            right = data.groupby(columns[:-1]).sum().reset_index()\n            right.columns = list(right.columns[:-1])+['Group_Sum']\n\n            groups = pd.merge(left = groups, right = right, left_on = columns[:-1], right_on = columns[:-1])\n            groups['Sum%'] = groups['Group_Count']/groups['Group_Sum']*100\n            groups = groups[cols+['Sum%']]\n            return groups\n        else:\n            print('Method does not exist. Please choose count, count_percent, mean, median, max, min, sum_percent.'); return None;\n        #except: print('Method = {}'.format(method)+' cannot work on Object, Non-Numerical data. Choose count.'); return None;\n\n        groups = pd.DataFrame(groups)\n        groups.columns = [method]\n        groups.reset_index(inplace = True)\n        return groups\n\n\ndef sort(data, by = None, how = 'ascending', inplace = False):\n    ''' how can be 'ascending' or 'descending' or 'a' or 'd'\n    It can also be a list for each sorted column.\n    '''\n    replacer = {'ascending':True,'a':True,'descending':False,'d':False}\n    if by is None and type(data) is pd.Series:\n        try:    x = replacer[how]\n        except: print(\"how can be 'ascending' or 'descending' or 'a' or 'd'\"); return None;\n        return data.sort_values(ascending = x, inplace = inplace)\n    elif type(how) is not list:\n        try:    how = replacer[how]\n        except: print(\"how can be 'ascending' or 'descending' or 'a' or 'd'\"); return None;\n    else:\n        for x in how: \n            try:    x = replacer[x]\n            except: print(\"how can be 'ascending' or 'descending' or 'a' or 'd'\"); return None;\n    return data.sort_values(by, ascending = how, inplace = inplace)\n\ndef keep(data, what, inplace = False):\n    '''Keeps data in a column if it's wanted.\n    Everything else is filled with NANs'''\n    if type(what) not in (list,tuple,np.array,np.ndarray): what = [what]\n    need = data.isin(what)\n    if inplace: \n        df = data\n        df.loc[~need] = np.nan\n    else: \n        df = data.copy()\n        df.loc[~need] = np.nan\n        return df\n\ndef remove(data, what, inplace = False):\n    '''Deletes data in a column if it's not wanted.\n    Everything else is filled with NANs'''\n    if type(what) not in (list,tuple): what = [what]\n    need = data.isin(what)\n    if inplace: \n        df = data\n        df.loc[need] = np.nan\n    else: \n        df = data.copy()\n        df.loc[need] = np.nan\n        return df\n    \n    \ndef ternary(data, condition, true, false = np.nan, inplace = False):\n    '''C style ternary operator on column.\n    Condition executes on column, and if true, is filled with some value.\n    If false, then replaced with other value. Default false is NAN.'''\n    try:\n        execute = 'data {}'.format(condition)\n        series = eval(execute)\n        try: series = series.map({True:true, False:false})\n        except: series = series.replace({True:true, False:false})\n        return series\n    except: print('Ternary accepts conditions where strings must be enclosed.\\nSo == USD not allowed. == \"USD\" allowed.'); return False;\n\n    \ndef locate(data, column):\n    '''Use ternary to get result and then filter with notnull'''\n    if dtype(column) == 'bool': return data.loc[column]\n    return data.loc[column.notnull()]\n    \ndef query(data, column = None, condition = None):\n    '''Querying data based on conditions'''\n    def Q(data, column, condition):\n        if column is not None:\n            if type(condition) in (np.array, np.ndarray, list, tuple, set):\n                cond = keep(data[column], tuple(condition))\n                cond = (cond.notnull())\n            else: cond = ternary(data[column], condition, True, False)\n            return data.loc[cond]\n        else:\n            if type(condition) in (np.array, np.ndarray, list, tuple, set):\n                cond = keep(data, tuple(condition))\n            else: cond = ternary(data, condition, True, False)\n            return data.loc[cond]\n    try:\n        return Q(data, column, condition)\n    except:\n        condition = condition.replace('=','==')\n        return Q(data, column, condition)\n        \ndef keep_top(x, n = 5):\n    '''Keeps top n (after tallying) in a column'''\n    df = keep(x, tally(x)[x.name][:n].values)\n    return df\n\ndef keep_bot(x, n = 5):\n    '''Keeps bottom n (after tallying) in a column'''\n    df = keep(x, tally(x)[x.name][:-n].values)\n    return df\n\n\ndef remove_outlier(x, method = 'iqr', range = 1.5):\n    '''Removes outliers in column with methods:\n        1. mean     =    meean+range (normally 3.5)\n        2. median   =    median+range (normally 3.5)\n        3. iqr      =    iqr+range (normally 1.5)\n    '''\n    i = x.copy()\n    if method == 'iqr':\n        first = np.nanpercentile(x, 0.25)\n        third = np.nanpercentile(x, 0.75)\n        iqr = third-first\n        i[(i > third+iqr*range) | (i < first-iqr*range)] = np.nan\n    else:\n        if method == 'mean': mu = np.nanmean(x)\n        else: mu = np.nanmedian(x)\n        std = np.nanstd(x)\n        i[(i > mu+std*range) | (i < mu-std*range)] = np.nan\n    return i\n\n\ndef cut(x, bins = 5, method = 'range'):\n    '''Cut continuous column into parts.\n        Method options:\n            1. range\n            2. quantile (number of quantile cuts)'''\n    if method == 'range': return pd.cut(x, bins = bins, duplicates = 'drop')\n    else: return pd.qcut(x, q = bins, duplicates = 'drop')\n    \n    \ndef plot(x, y = None, colour = None, column = None, data = None, size = 5, top = 10, wrap = 4, \n         subset = 5000, method = 'mean', quantile = True, bins = 10,\n         style = 'lineplot', logx = False, logy = False, logc = False, power = 1):\n    '''Plotting function using seaborn and matplotlib\n        Options:\n        x, y, colour, column, subset, style, method\n        \n        Plot styles:\n            1. boxplot\n            2. barplot\n            3. tallyplot (counting number of appearances)\n            4. violinplot (boxplot just fancier)\n            5. lineplot (mean line plot)\n            6. histogram\n            7. scatterplot (X, Y must be numeric --> dates will be converted)\n            8. bivariate (X, Y must be numeric --> dates will be converted)\n            9. heatmap (X, Y will be converted into categorical automatically --> bins)\n            10. regplot (X, Y must be numeric --> dates will be converted)\n    '''\n    if type(x) in (np.array,np.ndarray): x = pd.Series(x); x.name = 'x';\n    if type(y) in (np.array,np.ndarray): y = pd.Series(y); y.name = 'y';\n    if type(column) in (np.array,np.ndarray): column = pd.Series(column); column.name = 'column';\n    if type(colour) in (np.array,np.ndarray): colour = pd.Series(colour); colour.name = 'colour';\n        \n    if type(x) == pd.Series: \n        data = pd.DataFrame(x); x = x.name\n        if type(x) is not str:\n            data.columns = [str(x)]\n            x = str(x)\n    if method == 'mean': estimator = np.nanmean\n    elif method == 'median': estimator = np.nanmedian\n    elif method == 'min': estimator = np.min\n    elif method == 'max': estimator = np.max\n    else: print('Wrong method. Allowed = mean, median, min, max'); return False;\n    #----------------------------------------------------------\n    sb.set(rc={'figure.figsize':(size*1.75,size)})\n    dtypes = {'x':None,'y':None,'c':None,'col':None}\n    names = {'x':None,'y':None,'c':None,'col':None}\n    xlim = None\n    #----------------------------------------------------------\n    if data is not None:\n        if type(x) is str: x = data[x];\n        if type(y) is str: y = data[y]; \n        if type(colour) is str: colour = data[colour]; \n        if type(column) is str: column = data[column]; \n    if type(x) is str: print('Please specify data.'); return False;\n    #----------------------------------------------------------\n    if x is not None:\n        dtypes['x'] = dtype(x); names['x'] = x.name\n        if dtypes['x'] == 'object': x = keep_top(x, n = top)\n        elif dtypes['x'] == 'datetime': x = date_number(x)\n        if logx and dtype(x) != 'object': x = log(x)\n    if y is not None: \n        dtypes['y'] = dtype(y); names['y'] = y.name\n        if dtypes['y'] == 'object': y = keep_top(y, n = top)\n        elif dtypes['y'] == 'datetime': y = date_number(y)\n        if logy and dtype(y) != 'object': y = log(y)\n    if colour is not None:\n        dtypes['c'] = dtype(colour); names['c'] = colour.name\n        if dtypes['c'] == 'object': colour = keep_top(colour, n = top)\n        elif dtypes['c'] == 'datetime': colour = date_number(colour)\n        if logc and dtype(colour) != 'object': colour = log(colour)\n    if column is not None:\n        dtypes['col'] = dtype(column); names['col'] = column.name\n        if dtypes['col'] == 'object': column = keep_top(column, n = top)\n        elif dtypes['col'] == 'datetime': column = date_number(column)\n    #----------------------------------------------------------\n    df = hcat(x, y, colour, column)\n    if subset > len(df): subset = len(df)\n    df = sample(df, subset)\n    #----------------------------------------------------------\n    if column is not None:\n        if dtype(df[names['col']]) not in ('object', 'uint',' int') and nunique(df[names['col']]) > top: \n            if quantile: df[names['col']] = cut(df[names['col']], bins = bins, method = 'quantile')\n            else: df[names['col']] = cut(df[names['col']], bins = bins, method = 'range')\n    \n    try: df.sort_values(names['y'], inplace = True);\n    except: pass;\n    #----------------------------------------------------------\n    replace = {'boxplot':'box', 'barplot':'bar', 'tallyplot':'count', 'violinplot':'violin', \n               'lineplot': 'point', 'histogram':'lv'}\n    \n    if style == 'histogram' and y is None:\n        plot = sb.distplot(df[names['x']].loc[df[names['x']].notnull()], bins = bins)\n    elif style == 'lineplot' and y is None:\n        plot = plt.plot(df[names['x']]);\n        plt.show(); return;\n    elif style == 'barplot' and y is None:\n        plot = df.sort_values(names['x']).plot.bar();\n        plt.show(); return;\n    elif style in replace.keys():\n        if dtype(df[names['x']]) not in ('object', 'uint',' int') and nunique(df[names['x']]) > top: \n            if quantile: df[names['x']] = cut(df[names['x']], bins = bins, method = 'quantile')\n            else: df[names['x']] = cut(df[names['x']], bins = bins, method = 'range')\n        \n        if names['col'] is not None:\n            plot = sb.factorplot(x = names['x'], y = names['y'], hue = names['c'], data = df, kind = replace[style], col = names['col'],\n                             n_boot = 1, size = size, estimator = estimator, col_wrap = wrap)\n        else:\n            plot = sb.factorplot(x = names['x'], y = names['y'], hue = names['c'], data = df, kind = replace[style], col = names['col'],\n                             n_boot = 1, size = size, estimator = estimator)\n            \n        for ax in plot.axes.flatten(): \n            for tick in ax.get_xticklabels(): \n                tick.set(rotation=90)\n    \n    elif style == 'heatmap':\n        if dtype(df[names['x']]) != 'object'and nunique(df[names['x']]) > top:\n            if quantile: df[names['x']] = cut(df[names['x']], bins = bins, method = 'quantile')\n            else: df[names['x']] = cut(df[names['x']], bins = bins, method = 'range')\n                \n        if dtype(df[names['y']]) != 'object'and nunique(df[names['y']]) > top:\n            if quantile: df[names['y']] = cut(df[names['y']], bins = bins, method = 'quantile')\n            else: df[names['y']] = cut(df[names['y']], bins = bins, method = 'range')     \n\n        df = tabulate(df[names['x']], df[names['y']]).pivot(index = names['x'], columns = names['y'], values = 'count')\n        plot = sb.heatmap(df, cmap=\"YlGnBu\")\n\n        \n    elif dtype(df[names['x']]) == 'object' or dtype(df[names['y']]) == 'object':\n            print('{} can only take X = number and Y = number.'.format(style)); return False;\n        \n    elif style  in ('regplot', 'scatterplot'):\n        if column is None: col_wrap = None\n        else: col_wrap = wrap\n        if style == 'regplot': reg = True\n        else: reg = False\n        \n        plot = sb.lmplot(x = names['x'], y = names['y'], hue = names['c'], data = df, col = names['col'],\n                             n_boot = 2, size = size, ci = None, scatter_kws={\"s\": 50,'alpha':0.5},\n                        col_wrap = col_wrap, truncate = True, fit_reg = reg, order = power)\n        plot.set_xticklabels(rotation=90)\n        \n    elif style == 'bivariate':\n        plot = sb.jointplot(x = names['x'], y = names['y'], data = df, dropna = True, size = size, kind = 'reg',\n                           scatter_kws={\"s\": 50,'alpha':0.5}, space = 0)\n    plt.show()\n    \n    \ndef match_pattern(x, pattern, mode = 'find'):\n    '''Regex pattern finds in data and returns only match\n        \\d = digits\n        \\l = lowercase alphabet\n        \\p = uppercase alphabet\n        \\a = all alphabet\n        \\s = symbols and punctuation\n        \\e = end of sentence\n        \n        Modes =\n            1. find:   True/False if find or not\n            2. keep:   Output original string if match, else NAN\n            3. match:  Output only the matches in the string, else NAN\n        '''\n    pattern = pattern.replace('\\d','[0-9]').replace('\\l','[a-z]').replace('\\p','[A-Z]').replace('\\a','[a-zA-Z]')\\\n                .replace('\\s','[^0-9a-zA-Z]').replace('\\e', '(?:\\s|$)')\n    if dtype(x) != 'object': print('Data is not string. Convert first'); return False;\n\n    regex = re.compile(r'{}'.format(pattern))\n    \n    def patternFind(i):\n        try: j = re.match(regex, i).group(); return True\n        except: return False;\n    def patternKeep(i):\n        try: j = re.match(regex, i).group(); return i\n        except: return np.nan;\n    def patternMatch(i):\n        try: j = re.match(regex, i).group(); return j\n        except: return np.nan;\n    \n    if mode == 'find':        return x.apply(patternFind)\n    elif mode == 'keep':      return x.apply(patternKeep)\n    elif mode == 'match':     return x.apply(patternMatch)\n    \n    \ndef split(x, pattern):\n    '''Regex pattern finds in data and returns match. Then, it is splitted accordingly.\n        \\d = digits\n        \\l = lowercase alphabet\n        \\p = uppercase alphabet\n        \\a = all alphabet\n        \\s = symbols and punctuation\n        \\e = end of sentence\n        \\S = most symbols including spaces but not apostrophes\n        '''\n    pattern2 = pattern.replace('\\d','[0-9]').replace('\\l','[a-z]').replace('\\p','[A-Z]').replace('\\a','[a-zA-Z]')\\\n                .replace('\\s','[^0-9a-zA-Z]').replace('\\e', '(?:\\s|$)').replace('\\S','[.!, \"\\(\\)\\?\\*\\&\\^%$#@:/\\\\_;\\+\\-\\â€¦]')\n    \n    if dtype(x) != 'object': print('Data is not string. Convert first'); return False;\n    \n    regex = re.compile(r'{}'.format(pattern2))\n    if pattern == pattern2: return x.str.split(pattern)\n    else: return x.apply(lambda i: re.split(regex, i))\n    \ndef replace(x, pattern, with_ = None):\n    '''Regex pattern finds in data and returns match. Then, it is replaced accordingly.\n        \\d = digits\n        \\l = lowercase alphabet\n        \\p = uppercase alphabet\n        \\a = all alphabet\n        \\s = symbols and punctuation\n        \\e = end of sentence\n        '''\n    if type(pattern) is list:\n        d = {}\n        for l in pattern: d[l[0]] = l[1]\n        try:\n            return x.replace(d)\n        except:\n            return x.astype('str').replace(d)\n            \n    pattern2 = pattern.replace('\\d','[0-9]').replace('\\l','[a-z]').replace('\\p','[A-Z]').replace('\\a','[a-zA-Z]')\\\n                .replace('\\s','[^0-9a-zA-Z]').replace('\\e', '(?:\\s|$)')\n    \n    if dtype(x) != 'object': print('Data is not string. Convert first'); return False;\n    \n    regex = re.compile(r'{}'.format(pattern2))\n    if pattern == pattern2: return x.str.replace(pattern, with_)\n    else: return x.apply(lambda i: re.sub(regex, with_, i))\n    \ndef remove(x, what):\n    return replace(x, what, '')\n    \ndef notnull(data, loc = None):\n    '''Returns the items that are not null in a column / dataframe'''\n    if loc is not None:\n        return data.loc[loc.notnull()]\n    else:\n        return data.loc[data.notnull().sum(1) == data.shape[1]]\n    \n    \ndef exclude(data, col):\n    '''Only returns a dataframe where the columns in col are not included'''\n    if type(col) is str: col = [col]\n    columns = list(data.columns)\n    leave = list(set(columns) - set(col))\n    return data[leave]\n\n################### -----------------------------------------------------------------#######################\n#Recommendation Systems\ndef pivot(index, columns, values):\n    '''Creates a table where rows = users, columns = items, and cells = values / ratings'''\n    from scipy.sparse import dok_matrix\n    S = dok_matrix((nunique(index), nunique(columns)), dtype=np.float32)\n    \n    mins = np.abs(np.min(values))+1\n    indexM = {}\n    for i,x in enumerate(unique(index)): indexM[x] = i;\n    columnsM = {}\n    for i,x in enumerate(unique(columns)): columnsM[x] = i;\n        \n    for i,c,v in zip(index, columns, values+mins): S[indexM[i],columnsM[c]] = v;\n    \n    S = S.toarray(); S[S == 0] = np.nan; S -= mins\n    S = pd.DataFrame(S)\n    S.index = indexM.keys(); S.columns = columnsM.keys();\n    return S\n\ndef row_operation(data, method = 'sum'):\n    '''Apply a function to a row\n        Allowed functions:\n            1. sum\n            2. median\n            3. mean\n            4. max\n            5. min\n            6. count\n            7. count_percent\n            8. sum_percent\n            9. mean_zero         (Mean but zeroes arent counted)\n            10. count_zero       (Count but zeroes arent counted)\n        Own functions are also allowed.\n    '''\n    if method in ('sum','median','mean','max','min','count'):\n        x = eval('data.{}(1)'.format(method))\n    elif method in ('count_percent', 'sum_percent'):\n        x = eval('data.{}(1)'.format(method.split('_')[0]))\n        x /= x.sum()\n        x *= 100\n    elif method in ('mean_zero', 'count_zero'):\n        df = data.copy()\n        df[df == 0] = np.nan\n        x = eval('df.{}(1)'.format(method.split('_')[0]))\n    else: return data.apply(method, axis = 1)\n    x.name = 'row_operation'\n    return x\n\n\ndef col_operation(data, method = 'sum'):\n    '''Apply a function to a column\n        Allowed functions:\n            1. sum\n            2. median\n            3. mean\n            4. max\n            5. min\n            6. count\n            7. count_percent\n            8. sum_percent\n            9. mean_zero         (Mean but zeroes arent counted)\n            10. count_zero       (Count but zeroes arent counted)\n        Own functions are also allowed.\n        '''\n    if method in ('sum','median','mean','max','min','count'):\n        x = eval('data.{}(0)'.format(method))\n    elif method in ('count_percent', 'sum_percent'):\n        x = eval('data.{}(0)'.format(method.split('_')[0]))\n        x /= x.sum()\n        x *= 100\n    elif method in ('mean_zero', 'count_zero'):\n        df = data.copy()\n        df[df == 0] = np.nan\n        x = eval('df.{}(0)'.format(method.split('_')[0]))\n    else: return data.apply(method, axis = 0)\n    x.name = 'col_operation'\n    return x\n\n    \ndef random(obj, n = 1, p = None):\n    if p is not None:\n        if type(p) is pd.Series: p = p.values\n        if p.sum() > 2: p /= 100\n    return list(np.random.choice(obj, size = n, replace = False, p = p))\n\ndef row(data, n): return data.loc[n]\n\ndef distances(source, target):\n    '''Returns all distances between target and source (L2)'''\n    Y = np.tile(target.values, (source.shape[0],1))\n    nans = np.isnan(Y)\n    X = source.values; X[np.isnan(X)] = 0;\n    Y[nans] = 0;\n    diff = X - Y;\n    diff[nans] = 0;\n    d = np.linalg.norm(diff, axis = 1)\n    j = pd.Series(d)\n    j.index = source.index\n    return j\n\n################### -----------------------------------------------------------------#######################\n#Natural Language Processing & Machine Learning\n\ndef multiply(left, right):\n    ''' Multiplies 2 tables or columns together.\n        Will do automatic type casting'''\n\n    if len(left.shape) == 1:\n        try: return left.values.reshape(-1,1)*right\n        except: return left.reshape(-1,1)*right\n    elif len(right.shape) == 1:\n        try: return right.values.reshape(-1,1)*left\n        except: return right.reshape(-1,1)*left\n    else:\n        return left*right\n    \n    \ndef clean(data, missing = 'mean', remove_id = True):\n    '''Cleans entire dataset.\n    1. missing =\n        mean, max, median, min\n        Fills all missing values with column mean/median etc\n\n    2. remove_id = True/False\n        Checks data to see if theres an ID column.\n        Removes it (not perfect)\n    '''\n    x = data[data.columns[dtype(data) != 'object']].copy()\n    for c in x.columns[x.count()!=len(x)]:\n        x[c] = eval('x[c].fillna(x[c].{}())'.format(missing))\n    if remove_id:\n        for c in x.columns[(dtype(x) == 'int')|(dtype(x) == 'uint')]:\n            if x[c].min() >= 0:\n                j = (x[c] - x[c].min()).sort_values().diff().sum()\n                if j <= 1.001*len(x) and j >= len(x)-1: x.pop(c);\n    return x\n\n\ndef scale(data):\n    columns = data.columns\n    from sklearn.preprocessing import StandardScaler\n    scaler = StandardScaler().fit(data)\n    X = pd.DataFrame(scaler.transform(data))\n    X.columns = columns\n    return X\n\nfrom sklearn.base import BaseEstimator, RegressorMixin\nfrom keras.models import Sequential, load_model\nfrom sklearn.linear_model import LassoLarsIC\nfrom sklearn.linear_model import Ridge\nfrom sklearn.utils import class_weight\nfrom keras.layers import Dense, Activation, GaussianNoise, BatchNormalization, Dropout\nfrom keras.initializers import glorot_normal\nfrom keras.callbacks import *\nfrom keras.optimizers import Nadam, SGD\n\nclass LinearModel(BaseEstimator, RegressorMixin):\n\n    def __init__(self, lasso = False, scale = True, logistic = False, layers = 0, activation = 'tanh', epochs = 50,\n                    time = None, shift = 1, test_size = 0.2, early_stopping = 7, lr = 0.1):\n        self.scale = scale; self.logistic = logistic; self.lasso = lasso; self.layers = layers;\n        assert activation in ['tanh','relu','sigmoid','linear']\n        assert shift > 0;\n        self.activation = activation; self.epochs = epochs; self.time = time; self.shift = shift\n        if logistic or (self.logistic == False and self.layers > 0): self.model = Sequential()\n        elif lasso: self.model = LassoLarsIC()\n        else: self.model = Ridge()\n        self.mapping = {}; self.test_size = test_size; self.early_stopping = early_stopping\n        self.lr = lr\n\n    def fit(self, X, Y):\n        print('Model now fitting...')\n        X = self._process_X(X.copy())\n        X = self._time_transform(X)\n        \n        self.uniques, self.columns = X.apply(self._range_unique), list(X.columns)\n        if self.scale: X, self.means, self.stds = self._scaler(X)\n        self.uniques_scale = X.apply(self._range_unique)\n        \n        if self.logistic:\n            Y = self._process_Y(Y)\n            self._fit_keras(X, Y)\n        else:\n            try: \n                if self.layers == 0: self._fit_sklearn(X, Y)\n                else: \n                    self.out = 1\n                    if Y.min() >= 0:\n                        if Y.max() <= 1: self.activ = 'sigmoid'\n                        else: self.activ = 'relu'\n                    elif Y.min() >= -1 and Y.max() <= 1:\n                        self.activ = 'tanh'\n                    else: self.activ = 'linear'\n                    self.loss = 'mse'\n                    self._fit_keras(X, Y)\n            except: \n                print('Y is not numeric. Choose logistic = True for classification'); return None\n\n        self._store_coefficients()\n        self._df = self._store_analysis(X)\n        print('Model finished fitting')\n\n        \n    def predict(self, X):\n        X = self._process_X(X.copy())\n        X = self._time_transform(X)\n        X = self._transform(X)\n        if self.logistic:\n            prob = self.model.predict(X)\n            if self.activ == 'sigmoid': prob = prob.round().astype('int').flatten()\n            else: prob = prob.argmax(1).astype('int').flatten()\n            prob = pd.Series(prob).replace(self.mapping)\n            return prob\n        else: return self.model.predict(X).flatten()\n\n\n    def predict_proba(self, X):\n        if self.logistic:\n            X = self._process_X(X.copy())\n            X = self._time_transform(X)\n            X = self._transform(X)\n            prob = self.model.predict(X).flatten()\n            return prob\n        else: print('Predict Probabilities only works for logisitc models.'); return None;\n\n        \n    def coefficients(self, plot = False, top = None):\n        df = self.coef\n        if self.layers == 0:\n            if top is not None: df = df[:top]\n            if plot:\n                df = df.fillna('')\n                if len(self.mapping) > 2:\n                    df = df.style.bar(subset = [x for x in df.columns if 'Y=(' in x], align='mid', color=['#d65f5f', '#5fba7d'])\n                else:\n                    df = df.style.bar(subset = ['Coefficient'], align='mid', color=['#d65f5f', '#5fba7d'])\n        return df\n    \n    \n    def plot(self, predictions, real_Y):\n        if self.logistic:\n            from sklearn.metrics import confusion_matrix\n            conf = pd.DataFrame(confusion_matrix(real_Y, predictions))\n            try: \n                conf.index = [f'True({x}/{i})' for i,x in zip(self.mapping.keys(), self.mapping.values())]\n                conf.columns = [f'{x}/{i}' for i,x in zip(self.mapping.keys(), self.mapping.values())]\n            except: \n                conf.index = [f'True({x})' for x in range(nunique(real_Y))]\n            conf = conf.divide(conf.sum(1), axis = 'index')*100\n            return sb.heatmap(conf, cmap=\"YlGnBu\", vmin = 0, vmax = 100, annot = True)\n        else:\n            return plot(x = predictions, y = real_Y, style = 'regplot')\n    \n    \n    def score(self, predictions, real_Y):\n        if self.logistic:\n            from sklearn.metrics import matthews_corrcoef\n            coef = matthews_corrcoef(real_Y, predictions)\n            if np.abs(coef) < 0.3: print('Model is not good. Score is between (-0.3 and 0.3). A score larger than 0.3 is good, or smaller than -0.3 is good.') \n            else: print('Model is good.')\n            return coef\n        else:\n            from sklearn.metrics import mean_squared_error\n            error = np.abs(np.sqrt(mean_squared_error(real_Y, predictions))/np.mean(real_Y))\n            if error > 0.4: print('Model is not good. Score is larger than 40%. Smaller than 40% relative error is good.')\n            else: print('Model is good.')\n            return error\n    \n    \n    def analyse(self, column = None, plot = False, top = 20):\n        if self.layers == 0:\n            df = self._df.round(2)\n            if self.logistic:\n                if column is not None: df = df.loc[column]\n                else: df = df[:top]\n                def color_negative_red(val):\n                    color = 'lightgreen' if val == 'Add 1' else 'pink'\n                    return 'background-color: %s' % color\n\n                def highlight_max(s):\n                    is_max = s == s.max()\n                    return ['color: lightgreen' if v else '' for v in is_max]\n\n                if plot:\n                    df = df.fillna('')\n                    if self.activ == 'sigmoid':\n                        df = df.style.bar(align = 'mid', width = 75, color = ['gray'])\\\n                                    .applymap(color_negative_red, subset = ['Effect'])\n                    else:\n                        df = df.style.bar(align = 'mid', width = 75, color = ['gray'])\\\n                                    .applymap(color_negative_red, subset = ['Effect'])\\\n                                    .apply(highlight_max, subset = df.columns[2:], axis = 1)\n            else:\n                if column is not None: df = pd.DataFrame(df.loc[[column]])\n                else: df = df[:top]\n                if plot:\n                    cols = list(df.columns); cols.remove('If Stays'); cols.remove('Change if Removed')\n                    df[cols] = df[cols].fillna('')\n                    def color_negative_red(val):\n                        if val == True: color = 'cyan'\n                        elif val == False: color = 'pink'\n                        else: color = ''\n                        return 'background-color: %s' % color\n\n                    df = df.style.bar(subset = ['Coefficient','If Stays','Change if Removed','Best Addon',\n                                               'Worst Reduced'], align='mid', color=['#d65f5f', '#5fba7d'])\\\n                            .applymap(color_negative_red, subset = ['Stay'])\\\n                            .bar(subset = ['Best Contrib','Worst Contrib'], align='mid', color=['pink', 'cyan'])\n            return df\n        else:\n            print(\"Can't analyse since it's a neural network. I can only give you the model layout and loss graphs\")\n            print(self.model.summary()); history = self.history\n            plt.plot(history.history['loss']); plt.plot(history.history['val_loss'])\n            plt.title('model loss'); plt.ylabel('loss');plt.xlabel('epoch')\n            plt.legend(['train', 'test'], loc='upper left')\n            plt.show()\n        \n        \n    def degrees(self, prediction, real_Y):\n        '''The closer the degree of the fit line to 45*, the better!'''\n        if not self.logistic:\n            from sklearn.linear_model import Ridge as modeller\n            models = modeller().fit(prediction.reshape(-1,1),real_Y)\n            deg = np.round((np.arctan(models.coef_[0]))/np.pi*180, 3)\n            if deg <= 50 and deg > 45: print('Prediction seems good, but probably overpredicting')\n            elif deg > 50: print(\"Prediction doesn't seem good. It's overpredicting\")\n            elif deg == 45: print(\"Prediction looks ideal! It's quite smooth\")\n            elif deg <= 45 and deg > 40: print(\"Prediction seems good, but probably underpredicting\")\n            else: print(\"Prediction doesn't seem good. It's underpredicting\")\n            return deg\n        else: print('Model is not regression. Use score instead'); return None;\n        \n        \n    def _process_X(self, X):\n        try: X.shape[1]\n        except: X = X.reshape(-1,1)\n        if type(X) is not pd.DataFrame: X = pd.DataFrame(X)\n        try: X = X[self.columns]\n        except: pass\n        return X\n\n\n    def _process_Y(self, Y):\n        if type(Y) is not pd.Series: Y = pd.Series(Y)\n        n = nunique(Y); Y = Y.astype('category')\n        self.mapping = dict(enumerate(Y.cat.categories))\n        self.reverse_mapping = dict(zip(self.mapping.values(), self.mapping.keys()))\n        Y = Y.cat.codes\n        \n        class_weights = class_weight.compute_class_weight('balanced', list(self.mapping.keys()), Y)\n        self.class_weights = dict(enumerate(class_weights))\n        \n        if n == 2:\n            self.activ, self.loss, self.out = 'sigmoid', 'binary_crossentropy', 1\n        else:\n            self.activ, self.loss = 'softmax', 'categorical_crossentropy'\n            Y = pd.get_dummies(Y); self.out = Y.shape[1]\n        return Y\n    \n    \n    def _time_transform(self, X):\n        if self.time is not None:\n            X.sort_values(self.time, inplace = True)\n            alls = [X]\n            for s in range(1,self.shift+1):\n                ss = X.shift(s); ss.columns = [x+f'({-s})' for x in ss.columns]\n                alls.append(ss)\n            X = pd.concat(alls, 1)\n            X.fillna(method = 'backfill', inplace = True); X.sort_index(inplace = True)\n        return X\n    \n        \n    def _store_coefficients(self):\n        if self.logistic:\n            if self.layers == 0:\n                coefs = pd.DataFrame(self.coef_)\n                if len(self.mapping) > 2: coefs.columns, coefs.index = [f'Y=({x})' for x in self.mapping.values()], self.columns\n                else: coefs.columns, coefs.index = ['Coefficient'], self.columns\n                coefs['Abs'] = np.abs(coefs).sum(1)\n                coefs['Mean'], coefs['Std'], coefs['Range'], coefs['Scale'] = self.means, self.stds, self.uniques, self.uniques_scale\n                coefs.sort_values('Abs', inplace = True, ascending = False); coefs.pop('Abs');\n                self.coef = coefs\n            else: self.coef = self.coef_\n        else:\n            if self.layers == 0:\n                df = pd.DataFrame({'Coefficient':self.coef_ , 'Abs' : np.abs(self.coef_),\n                                    'Mean':self.means, 'Std':self.stds, 'Range':self.uniques, 'Scale':self.uniques_scale})\n                df.index = self.columns; df.sort_values('Abs', ascending = False, inplace = True)\n                df.pop('Abs');\n                self.coef = df\n            else: self.coef = self.coef_\n                \n\n    def _store_analysis(self, X):\n        if self.logistic:\n            if self.layers == 0:\n                coefs = pd.DataFrame(self.coef_)\n                if self.activ == 'sigmoid':\n                    col = 'Probability (Y={})'.format(max(list(self.mapping.values())))\n                    coefs.columns = [col]\n                    coefs.index = self.columns\n                    exponential = np.exp(1*coefs + self.bias_)\n                    exponential = exponential.divide(exponential + 1)*100\n                    exponential['Effect'] = 'Add 1'\n\n                    neg_exponential = np.exp(-1*coefs + self.bias_)\n                    neg_exponential = neg_exponential.divide(neg_exponential + 1)*100\n                    neg_exponential['Effect'] = 'Minus 1'\n\n                    coefs = pd.concat([exponential, neg_exponential]).round(2)\n                    coefs.reset_index(inplace = True); coefs.columns = ['Column']+list(coefs.columns[1:])\n                    coefs.sort_values(col, ascending = False, inplace = True)\n                else:\n                    coefs.columns, coefs.index = [f'Y=({x})' for x in self.mapping.values()], self.columns\n                    exponential = np.exp(1*coefs + self.bias_)\n                    exponential = exponential.divide(exponential.sum(1), axis = 0)*100\n                    exponential['Effect'] = 'Add 1'\n\n                    neg_exponential = np.exp(-1*coefs + self.bias_)\n                    neg_exponential = neg_exponential.divide(neg_exponential.sum(1), axis = 0)*100\n                    neg_exponential['Effect'] = 'Minus 1'\n\n                    coefs = pd.concat([exponential, neg_exponential])\n                    coefs.reset_index(inplace = True)\n                    coefs.columns = ['Column']+list(coefs.columns[1:])\n                    coefs = coefs[['Column','Effect']+list(coefs.columns)[1:-1]].round(2)\n\n                    coefs['Max'] = coefs.max(1); coefs.sort_values('Max', ascending = False, inplace = True); del coefs['Max'];\n                return coefs\n            else: return None\n        else:\n            if self.layers == 0:\n                full = X*self.coef_\n                transformed = full.sum(1) + self.bias_\n                selects, unselects, worst, best, W, B, L, original_G, original_B, overall = [],[],[],[],[],[],[],[],[],[]\n\n                for i, (col, mu) in enumerate(zip(self.columns, self.means)):\n                    if np.isnan(mu):\n                        cond = (X[col]!=0)\n                        select = transformed.loc[cond]\n                        unselect = transformed.loc[~cond]\n                        selects.append(select.mean())\n                        unselects.append(unselect.mean())\n\n                        original = X.loc[cond].mean(0)\n                        d = full.loc[cond].mean(0)\n                        dx = full.loc[~cond].mean(0)\n\n                        d = pd.DataFrame({col: d, 'Abs': np.abs(d)}).sort_values('Abs', ascending = False)[col]\n                        s = (d.index == col)\n                        d = d.loc[~s].sort_values(ascending = False)\n                        first = d.index[0]; end = d.index[-1]\n                        best.append(first)\n                        B.append(d[0]-dx.loc[first])\n                        worst.append(d.index[-1])\n                        W.append(d[-1]-dx.loc[end])\n                        L.append(len(select))\n\n                        original_G.append(original.loc[first])\n                        original_B.append(original.loc[end])\n                    else:\n                        selects.append(np.nan); unselects.append(np.nan); L.append(np.nan)\n\n                        gt = (full.gt(full[col], axis = 'index')*full)\n                        gt[gt == 0] = np.nan; gt_means = gt.mean(0).sort_values(ascending = False)\n                        changes = gt.subtract(full[col], axis = 'index').mean(0)\n                        b = gt_means.index[0]; b_add = changes.loc[b]; b_contrib = gt_means.iloc[0]\n                        best.append(b); B.append(b_add); original_G.append(b_contrib)\n\n                        lt = (full.lt(full[col], axis = 'index')*full)\n                        lt[lt == 0] = np.nan; lt_means = lt.mean(0).sort_values(ascending = True)\n                        changes = lt.subtract(full[col], axis = 'index').mean(0)\n                        w = lt_means.index[0]; w_add = changes.loc[w]; w_contrib = lt_means.iloc[0]\n                        worst.append(w); W.append(w_add); original_B.append(w_contrib)\n\n\n                df = pd.DataFrame({'Coefficient':self.coef_, 'N':L,'If Stays':selects, 'Removed':unselects, 'Change if Removed': 0, 'Stay' : 0,\n                                  'Best Combo':best, 'Best Addon':B,'Best Contrib':original_G,'Worst Combo':worst, 'Worst Reduced':W, 'Worst Contrib':original_B})\n\n                df['Change if Removed'] = df['Removed'] - df['If Stays']\n                df['Stay'] = (df['Change if Removed'] < 0); df['Abs'] = np.abs(df['Change if Removed'])\n                df.loc[df['N'].isnull(), 'Stay'] = np.nan\n                df['Abs_Coef'] = np.abs(df['Coefficient'])\n                df.index = self.columns\n                df.sort_values(['Abs','Abs_Coef'], ascending = [False,False], inplace = True)\n                df.pop('Abs'); df.pop('Removed'); df.pop('Abs_Coef');\n                return df\n            else: return None\n\n    def _fit_keras(self, X, Y):\n        self.model.add(GaussianNoise(0.01, input_shape = (X.shape[1],)))\n        \n        for l in range(self.layers):\n            self.model.add(Dense(X.shape[1], kernel_initializer = glorot_normal(seed = 0)))\n            self.model.add(Activation(self.activation))\n            self.model.add(BatchNormalization())\n            self.model.add(Dropout(0.15))\n            self.model.add(GaussianNoise(0.01))\n            \n        self.model.add(Dense(self.out, kernel_initializer = glorot_normal(seed = 0)))\n        self.model.add(Activation(self.activ))\n    \n        earlyStopping = EarlyStopping(monitor = 'val_loss', patience = int(self.early_stopping*(self.layers/2+1)), verbose = 0, mode = 'min')\n        reduce_lr_loss = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.1, patience = int(1*(self.layers/2+1)), verbose = 0, epsilon = 1e-4, mode = 'min')\n        cycle = CyclicLR(base_lr = 0.0005, max_lr = self.lr, step_size = 2000, mode = 'exp_range')\n        checkpoint = ModelCheckpoint('Best_Model.hdf5', save_best_only = True)\n        \n        self.metrics = ['acc']        \n        if not self.logistic: self.class_weights = None; self.metrics = None\n        \n        self.model.compile(optimizer = Nadam(), loss = self.loss, metrics = self.metrics)\n\n        if len(X) < 100: bs = 10\n        elif len(X) < 200: bs = 20\n        elif len(X) < 300: bs = 30\n        else: bs = 32\n\n        self.history = self.model.fit(X, Y, epochs = self.epochs, batch_size = bs, verbose = 2, validation_split = self.test_size, shuffle = True,\n                    callbacks = [earlyStopping, TerminateOnNaN(), reduce_lr_loss, cycle, checkpoint], \n                   class_weight = self.class_weights)\n        self.model = load_model('Best_Model.hdf5')\n        if self.layers == 0: self.coef_, self.bias_ = self.model.get_weights()\n        else: self.coef_ = self.model.get_weights()\n        self.lr = cycle\n        \n        \n    def _fit_sklearn(self, X, Y):\n        self.model.fit(X, Y)\n        self.coef_, self.bias_ = self.model.coef_, self.model.intercept_\n\n\n    def _range_unique(self, x):\n        s = x.sort_values(ascending = True).values\n\n        mins, maxs = np.round(s[0], 2), np.round(s[-1], 2)\n        length = len(s)/4\n        qtr1, qtr3 = np.round(s[int(length)], 2), np.round(s[int(3*length)], 2)\n        return sorted(set([mins, qtr1, qtr3, maxs]))\n\n\n    def _scaler(self, X):\n        result = []; means = []; stds = []\n        \n        for col in X.columns:\n            df = X[col]\n            if df.nunique() == 2 and df.min() == 0 and df.max() == 1:\n                result.append(df); means.append(np.nan); stds.append(np.nan)\n            else:\n                mu, std = df.mean(), df.std()\n                means.append(mu); stds.append(std)\n                result.append((df-mu)/std)\n        return pd.concat(result, 1), np.array(means), np.array(stds)\n\n\n    def _transform(self, X):\n        if self.scale:\n            final = []\n            for col, mu, std in zip(self.columns, self.means, self.stds):\n                if np.isnan(mu): final.append(X[col])\n                else: final.append((X[col]-mu)/std)\n            X = pd.concat(final, 1)\n        return X\n\n    \nclass CyclicLR(Callback):\n    \"\"\"(https://arxiv.org/abs/1506.01186). https://github.com/bckenstler/CLR/blob/master/clr_callback.py\"\"\"\n    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n                 gamma=0.9999, scale_fn=None, scale_mode='cycle'):\n        super(CyclicLR, self).__init__()\n        self.base_lr = base_lr; self.max_lr = max_lr\n        self.step_size = step_size; self.mode = mode; self.gamma = gamma\n\n        if scale_fn == None:\n            if self.mode == 'triangular':\n                self.scale_fn, self.scale_mode = lambda x: 1., 'cycle'\n            elif self.mode == 'triangular2':\n                self.scale_fn, self.scale_mode = lambda x: 1/(2.**(x-1)), 'cycle'\n            elif self.mode == 'exp_range':\n                self.scale_fn, self.scale_mode = lambda x: gamma**(x), 'iterations'\n        else:\n            self.scale_fn, self.scale_mode = scale_fn, scale_mode\n\n        self.clr_iterations = 0.; self.trn_iterations = 0.; self.history = {}\n        self._reset()\n\n    def _reset(self, new_base_lr=None, new_max_lr=None,new_step_size=None):\n        if new_base_lr != None: self.base_lr = new_base_lr\n        if new_max_lr != None: self.max_lr = new_max_lr\n        if new_step_size != None: self.step_size = new_step_size\n        self.clr_iterations = 0.\n\n    def clr(self):\n        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n        if self.scale_mode == 'cycle': return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n        else: return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n\n    def on_train_begin(self, logs={}):\n        logs = logs or {}\n        if self.clr_iterations == 0: K.set_value(self.model.optimizer.lr, self.base_lr)\n        else: K.set_value(self.model.optimizer.lr, self.clr())        \n\n    def on_batch_end(self, epoch, logs=None):\n        logs = logs or {}\n        self.trn_iterations += 1\n        self.clr_iterations += 1\n        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n        self.history.setdefault('iterations', []).append(self.trn_iterations)\n        for k, v in logs.items(): self.history.setdefault(k, []).append(v)\n        K.set_value(self.model.optimizer.lr, self.clr())\n        \n##----NATURAL LANG PROCESSING\ndef lower(x):\n    if type(x) is pd.Series: return x.str.lower()\n    else: return [y.lower() for y in x]\n\ndef upper(x):\n    if type(x) is pd.Series: return x.str.upper()\n    else: return [y.upper() for y in x]\n    \ndef remove_space(x):\n    '''Removes duplicate spaces'''\n    return x.str.replace('[\\s]{2,}', ' ')\n\ndef keep_length(x, length = 2):\n    '''Removes objects in lists spaces'''\n    return x.apply(lambda x: [y for y in x if len(re.sub(r'[0-9a-zA-Z\\']','',y)) > 0 or len(y) > length])\n\ndef clean_up(x):\n    '''Deletes \\n \\r'''\n    return x.str.replace('\\n',' ').str.replace('\\r',' ').str.lstrip().str.rstrip()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"380d54eaf927f5d405638d8ef4bb114470c2a99c"},"cell_type":"code","source":"#Lab 1 Code...\n\nfiles()\ndata = read('application_train.csv')\nhead (data,3)\ntail (data,3)\ncolumns (data)\nhead (data ['target'],5)\ndescribeData = describe (data)\ndescribeData.to_csv(\"homeCreditRawData_describe.csv\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"002edbea6f346a25b54557fc4338359544975ded"},"cell_type":"code","source":"head (data,3)\n\n#simplify names...\n\ngender = 'code_gender'\ncar = 'flag_own_car'\nchild = 'cnt_children'\ndebt = 'amt_credit'\njob = 'name_income_type'\nincome = 'amt_income_total'\neducation = 'name_education_type'\nurban_index = 'region_population_relative'\nage = 'days_birth'\nemployment = 'days_employed'\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2d8732ec233f6034527090c2b9a28a5b08d7c807"},"cell_type":"code","source":"# ***Error***\n\ntally_1 = print (tally(data[gender]))\ntally_2 = print (tally (data[car]))\ntally_3 = print (tally (data[child]))\ntally_4 = print (tally (data[job]))\ntally_5 = print (tally (data[education]))\n\ntally_output = tally_1\n\ntally_output.to_csv(\"homeCreditRawData_tally.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"54317816256e1c151e9176a8a5203d8f85480b13"},"cell_type":"code","source":"#help(tabulate)\n\ntabulate (data[customer],data[age],method='mean')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef1cbb40607dfdef7c58eb1f71480d4b3255e03a"},"cell_type":"code","source":"#help(pivot) https://pandas.pydata.org/pandas-docs/stable/generated/pandas.pivot_table.html\n\npivot(data[customer],data[age],data[income])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}