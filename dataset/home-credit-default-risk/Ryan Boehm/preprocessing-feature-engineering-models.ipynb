{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport os\nprint(os.listdir(\"../input\"))\n\n# Suppress warnings \nimport warnings\nwarnings.filterwarnings('ignore')\n\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1e0f8eecd4d178ac3f172d794eb0144cf64de555"},"cell_type":"markdown","source":"# Read in Training Data"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"scrolled":true},"cell_type":"code","source":"# Training data\napp_train = pd.read_csv('../input/application_train.csv')\nprint('Training data shape: ', app_train.shape)\napp_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"948a7c24b768076bda4c54bf7d4d1b144122b64f"},"cell_type":"markdown","source":"# Read in Testing Data"},{"metadata":{"trusted":true,"_uuid":"f1c0386650785947ea704ab5ac5785647f1db036"},"cell_type":"code","source":"# Testing data features\napp_test = pd.read_csv('../input/application_test.csv')\nprint('Testing data shape: ', app_test.shape)\napp_test.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5d2963af74ac4f4f22f2de9bdd30265207af84d6"},"cell_type":"markdown","source":"# Check if Dataset is Unbalanced"},{"metadata":{"trusted":true,"_uuid":"960631e68a3603d6f69658f36298157dbc796480","scrolled":true},"cell_type":"code","source":"app_train['TARGET'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eb18b148ffc3eccb97ce7b04a8115729f8ee9d78"},"cell_type":"code","source":"app_train['TARGET'].astype(int).plot.hist();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5aafd954a733e69a4e44d7ec10f9ec68ceb4d31e"},"cell_type":"markdown","source":"# Data Types"},{"metadata":{"trusted":true,"_uuid":"350d157c4d0ec3eb93daf4033b1b921e6e8fb550"},"cell_type":"code","source":"# Number of each type of column\napp_train.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8b3a69c7f94b2b6f2b21b07c429df983b7735940"},"cell_type":"code","source":"# Number of unique classes in each object column\napp_train.select_dtypes('object').apply(pd.Series.nunique, axis = 0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7bf371199c1c53ad7165fca031151102293af032"},"cell_type":"markdown","source":"# One-Hot Encoding"},{"metadata":{"trusted":true,"_uuid":"5d1c2c390862e4528ee30d5b5d05633418b8c8e3"},"cell_type":"code","source":"# one-hot encoding of categorical variables\napp_train = pd.get_dummies(app_train)\napp_test = pd.get_dummies(app_test)\n\nprint('Training Features shape: ', app_train.shape)\nprint('Testing Features shape: ', app_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"75cf0460a0b9154cfdc1f6cb30b8663da6b5f593"},"cell_type":"markdown","source":"# Align Training and Testing Data"},{"metadata":{"trusted":true,"_uuid":"21cf330bfee665c8dc4939a27e27683165077fe5"},"cell_type":"code","source":"train_labels = app_train['TARGET']\n\n# Align the training and testing data, keep only columns present in both dataframes\napp_train, app_test = app_train.align(app_test, join = 'inner', axis = 1)\n\n# Add the target back in\napp_train['TARGET'] = train_labels\n\nprint('Training Features shape: ', app_train.shape)\nprint('Testing Features shape: ', app_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"42f27122ff57b2464862abe29aaea22988446220"},"cell_type":"markdown","source":"# Preprocessing: Handle outliers and awkward data"},{"metadata":{"trusted":true,"_uuid":"22434b71643437230d94f16b9b93e4686e8123ed"},"cell_type":"code","source":"app_train['DAYS_BIRTH'] = abs(app_train['DAYS_BIRTH'])\napp_test[\"DAYS_EMPLOYED\"].replace({365243: np.nan}, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"629078a5d86acef6d80b57f3d85a4e5d2117594a"},"cell_type":"markdown","source":"# Helpful Functions"},{"metadata":{"_uuid":"ce70298fb4374519cbcd86a451a6569fa8328324"},"cell_type":"markdown","source":"## Function for Kernel Density Estimate Plots\nThe kernel density estimate plot shows the distribution of a single variable."},{"metadata":{"trusted":true,"_uuid":"6fdb20e11f5d24dbb98005ec5b7d0a213e388eaf"},"cell_type":"code","source":"# Plots the disribution of a variable colored by value of the target\ndef kde_target(var_name, df):\n    \n    # Calculate the correlation coefficient between the new variable and the target\n    corr = df['TARGET'].corr(df[var_name])\n    \n    # Calculate medians for repaid vs not repaid\n    avg_repaid = df.ix[df['TARGET'] == 0, var_name].median()\n    avg_not_repaid = df.ix[df['TARGET'] == 1, var_name].median()\n    \n    plt.figure(figsize = (12, 6))\n    \n    # Plot the distribution for target == 0 and target == 1\n    sns.kdeplot(df.ix[df['TARGET'] == 0, var_name], label = 'TARGET == 0')\n    sns.kdeplot(df.ix[df['TARGET'] == 1, var_name], label = 'TARGET == 1')\n    \n    # label the plot\n    plt.xlabel(var_name); plt.ylabel('Density'); plt.title('%s Distribution' % var_name)\n    plt.legend();\n    \n    # print out the correlation\n    print('The correlation between %s and the TARGET is %0.4f' % (var_name, corr))\n    # Print out average values\n    print('Median value for loan that was not repaid = %0.4f' % avg_not_repaid)\n    print('Median value for loan that was repaid =     %0.4f' % avg_repaid)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7f3babf2bc495c07a2658993e18002ca56aa4ede"},"cell_type":"markdown","source":"## Function for Numeric Aggregations\nAggregates numeric values for count, mean, max, min, sum"},{"metadata":{"trusted":true,"_uuid":"fec43c20d8b7a0c471a38cb3e23b61d6ac8518f1"},"cell_type":"code","source":"def agg_numeric(df, group_var, df_name):\n    \"\"\"Aggregates the numeric values in a dataframe. This can\n    be used to create features for each instance of the grouping variable.\n    \n    Parameters\n    --------\n        df (dataframe): \n            the dataframe to calculate the statistics on\n        group_var (string): \n            the variable by which to group df\n        df_name (string): \n            the variable used to rename the columns\n        \n    Return\n    --------\n        agg (dataframe): \n            a dataframe with the statistics aggregated for \n            all numeric columns. Each instance of the grouping variable will have \n            the statistics (mean, min, max, sum; currently supported) calculated. \n            The columns are also renamed to keep track of features created.\n    \n    \"\"\"\n    # Remove id variables other than grouping variable\n    for col in df:\n        if col != group_var and 'SK_ID' in col:\n            df = df.drop(columns = col)\n            \n    group_ids = df[group_var]\n    numeric_df = df.select_dtypes('number')\n    numeric_df[group_var] = group_ids\n\n    # Group by the specified variable and calculate the statistics\n    agg = numeric_df.groupby(group_var).agg(['count', 'mean', 'max', 'min', 'sum']).reset_index()\n\n    # Need to create new column names\n    columns = [group_var]\n\n    # Iterate through the variables names\n    for var in agg.columns.levels[0]:\n        # Skip the grouping variable\n        if var != group_var:\n            # Iterate through the stat names\n            for stat in agg.columns.levels[1][:-1]:\n                # Make a new column name for the variable and stat\n                columns.append('%s_%s_%s' % (df_name, var, stat))\n\n    agg.columns = columns\n    return agg","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"50c3d84dca185f579c7e8a1aa1634bc11ccf114a"},"cell_type":"markdown","source":"## Function to Calculate Correlation with Target\nFunction to calculate correlations with the target"},{"metadata":{"trusted":true,"_uuid":"b4109b84426bf019129d12e84dc2fa5ae841eac5"},"cell_type":"code","source":"def target_corrs(df):\n\n    # List of correlations\n    corrs = []\n\n    # Iterate through the columns \n    for col in df.columns:\n        print(col)\n        # Skip the target column\n        if col != 'TARGET':\n            # Calculate correlation with the target\n            corr = df['TARGET'].corr(df[col])\n\n            # Append the list as a tuple\n            corrs.append((col, corr))\n            \n    # Sort by absolute magnitude of correlations\n    corrs = sorted(corrs, key = lambda x: abs(x[1]), reverse = True)\n    \n    return corrs","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f7f0c8197b8d290be67bd240165a005cf6d934c8"},"cell_type":"markdown","source":"## Function to Handle Categorical Variables\nThis will calculate the counts and normalized counts of each category for all categorical variables in the dataframe."},{"metadata":{"trusted":true,"_uuid":"55e74729e76ce712b09f66e2041749818d8d9663"},"cell_type":"code","source":"def count_categorical(df, group_var, df_name):\n    \"\"\"Computes counts and normalized counts for each observation\n    of `group_var` of each unique category in every categorical variable\n    \n    Parameters\n    --------\n    df : dataframe \n        The dataframe to calculate the value counts for.\n        \n    group_var : string\n        The variable by which to group the dataframe. For each unique\n        value of this variable, the final dataframe will have one row\n        \n    df_name : string\n        Variable added to the front of column names to keep track of columns\n\n    \n    Return\n    --------\n    categorical : dataframe\n        A dataframe with counts and normalized counts of each unique category in every categorical variable\n        with one row for every unique value of the `group_var`.\n        \n    \"\"\"\n    \n    # Select the categorical columns\n    categorical = pd.get_dummies(df.select_dtypes('object'))\n\n    # Make sure to put the identifying id on the column\n    categorical[group_var] = df[group_var]\n\n    # Groupby the group var and calculate the sum and mean\n    categorical = categorical.groupby(group_var).agg(['sum', 'mean'])\n    \n    column_names = []\n    \n    # Iterate through the columns in level 0\n    for var in categorical.columns.levels[0]:\n        # Iterate through the stats in level 1\n        for stat in ['count', 'count_norm']:\n            # Make a new column name\n            column_names.append('%s_%s_%s' % (df_name, var, stat))\n    \n    categorical.columns = column_names\n    \n    return categorical","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"498ada6985eb710e375b708d24c6a0c74849ae84"},"cell_type":"code","source":"def aggregate_client(df, group_vars, df_names):\n    \"\"\"Aggregate a dataframe with data at the loan level \n    at the client level\n    \n    Args:\n        df (dataframe): data at the loan level\n        group_vars (list of two strings): grouping variables for the loan \n        and then the client (example ['SK_ID_PREV', 'SK_ID_CURR'])\n        names (list of two strings): names to call the resulting columns\n        (example ['cash', 'client'])\n        \n    Returns:\n        df_client (dataframe): aggregated numeric stats at the client level. \n        Each client will have a single row with all the numeric data aggregated\n    \"\"\"\n    \n    # Aggregate the numeric columns\n    df_agg = agg_numeric(df, group_var = group_vars[0], df_name = df_names[0])\n    \n    # If there are categorical variables\n    if any(df.dtypes == 'object'):\n    \n        # Count the categorical columns\n        df_counts = count_categorical(df, group_var = group_vars[0], df_name = df_names[0])\n\n        # Merge the numeric and categorical\n        df_by_loan = df_counts.merge(df_agg, on = group_vars[0], how = 'outer')\n\n        # Merge to get the client id in dataframe\n        df_by_loan = df_by_loan.merge(df[[group_vars[0], group_vars[1]]], on = group_vars[0], how = 'left')\n\n        # Remove the loan id\n        df_by_loan = df_by_loan.drop(columns = [group_vars[0]])\n\n        # Aggregate numeric stats by column\n        df_by_client = agg_numeric(df_by_loan, group_var = group_vars[1], df_name = df_names[1])\n\n        \n    # No categorical variables\n    else:\n        # Merge to get the client id in dataframe\n        df_by_loan = df_agg.merge(df[[group_vars[0], group_vars[1]]], on = group_vars[0], how = 'left')\n        \n        # Remove the loan id\n        df_by_loan = df_by_loan.drop(columns = [group_vars[0]])\n        \n        # Aggregate numeric stats by column\n        df_by_client = agg_numeric(df_by_loan, group_var = group_vars[1], df_name = df_names[1])\n\n    return df_by_client","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"03fd72edda14576d6527f598ae80d222eae5bc00"},"cell_type":"markdown","source":"# Preprocessing\nHandle missing values and outliers"},{"metadata":{"_uuid":"ef7de60b8830df224a17f7d102ac5afa2158092d"},"cell_type":"markdown","source":"## Outliers\nRemove or replace awkward values that were a mistake"},{"metadata":{"trusted":true,"_uuid":"d1da5cba4ed0e2aaeead22971b7acc967fcfdf0a"},"cell_type":"code","source":"app_train['DAYS_BIRTH'] = abs(app_train['DAYS_BIRTH'])\napp_test[\"DAYS_EMPLOYED\"].replace({365243: np.nan}, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3c74f78dffca0c6f24ccd05bff8b5bbdca4cd779"},"cell_type":"markdown","source":"## Missing Values\nRemove columns from training and testing datasets with greater than 50 percent of values missing."},{"metadata":{"trusted":true,"_uuid":"f836479e2375cd270f6bf838cd92b8ac9be5db30"},"cell_type":"code","source":"# Function to calculate missing values by column# Funct \ndef missing_values_table(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() / len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"97c69ef9db716bd6232baab5eed9023b01752777"},"cell_type":"code","source":"# Missing values statistics\nmissing_train = missing_values_table(app_train)\nmissing_train.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"82010cedde42cecd090b11305a7af271ae6d17e1"},"cell_type":"code","source":"missing_train_vars = list(missing_train.index[missing_train['% of Total Values'] > 80])\nlen(missing_train_vars)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1e658ea8785d5679953e2517a6d67f04f159606f"},"cell_type":"code","source":"missing_test = missing_values_table(app_test)\nmissing_test.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c429c07640fe593b54c896712dad76a94f0ba47f"},"cell_type":"code","source":"missing_test_vars = list(missing_test.index[missing_test['% of Total Values'] > 80])\nlen(missing_test_vars)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fd9c9ab4888d0d8dfb29ebc71daed49bc0eaa4d6"},"cell_type":"code","source":"missing_columns = list(set(missing_test_vars + missing_train_vars))\nprint('There are %d columns with more than 80%% missing in either the training or testing data.' % len(missing_columns))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"94a0180e06ec74c945b1de669100671633102413"},"cell_type":"code","source":"# Drop the missing columns\napp_train = app_train.drop(columns = missing_columns)\napp_test = app_test.drop(columns = missing_columns)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"720437fc20c42410d48afb60ec67abe175ba3e0a"},"cell_type":"markdown","source":"# Correlations\nExamine the correlations of the variables with the target. We can see in any of the variables we created have a greater correlation than those already present in the training data."},{"metadata":{"trusted":true,"_uuid":"3440007775752688962b894400058a7ad55ea711"},"cell_type":"code","source":"# Calculate all correlations in dataframe\ncorrs = app_train.corr()\n\ncorrs = corrs.sort_values('TARGET', ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9d61849c059858c5a62d60cfb35e31d6a5137a04"},"cell_type":"code","source":"# Ten most positive correlations\npd.DataFrame(corrs['TARGET'].head(10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"027e61b80f787682f5a8b43b862b6fcf484abaad"},"cell_type":"code","source":"# Ten most negative correlations\npd.DataFrame(corrs['TARGET'].dropna().tail(10))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3643a72dd8aa40bf42c1a043ace816533db7923b"},"cell_type":"markdown","source":"## Colinear Variables\nCalculate  the correlation of each variable with every other variable. This will allow us to see if there are highly collinear variables that should perhaps be removed from the data. Look for any variables that have a greather than 0.8 correlation with other variables."},{"metadata":{"trusted":true,"_uuid":"63fa9d97c5c477ff9d253716116ef46138fe74a5"},"cell_type":"code","source":"# Set the threshold\nthreshold = 0.8\n\n# Empty dictionary to hold correlated variables\nabove_threshold_vars = {}\n\n# For each column, record the variables that are above the threshold\nfor col in corrs:\n    above_threshold_vars[col] = list(corrs.index[corrs[col] > threshold])\n    \n# Track columns to remove and columns already examined\ncols_to_remove = []\ncols_seen = []\ncols_to_remove_pair = []\n\n# Iterate through columns and correlated columns\nfor key, value in above_threshold_vars.items():\n    # Keep track of columns already examined\n    cols_seen.append(key)\n    for x in value:\n        if x == key:\n            next\n        else:\n            # Only want to remove one in a pair\n            if x not in cols_seen:\n                cols_to_remove.append(x)\n                cols_to_remove_pair.append(key)\n            \ncols_to_remove = list(set(cols_to_remove))\nprint('Number of columns to remove: ', len(cols_to_remove))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"78e62f4fc4fdd6bc8ef2ec7c247d3375dc22c9e0"},"cell_type":"code","source":"# remove columns from training and testing sets\ntrain_corrs_removed = app_train.drop(columns = cols_to_remove)\ntest_corrs_removed = app_test.drop(columns = cols_to_remove)\n\nprint('Training Corrs Removed Shape: ', train_corrs_removed.shape)\nprint('Testing Corrs Removed Shape: ', test_corrs_removed.shape)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}