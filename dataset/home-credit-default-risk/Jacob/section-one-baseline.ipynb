{"cells":[{"metadata":{"_uuid":"7a2f7d90e73458354ca18a8d9a15cfec6b61c827"},"cell_type":"markdown","source":"<h1 style=\"text-align:center\">INFSCI 2595 Machine Learning Project</h1>\n<h2 style=\"text-align:center\">Home Credit Default Risk</h2>\n<h5 style=\"text-align:center\">Members: Chih Ying Chang, Xinghao Huang, Yuanyuan Zhang</h5>"},{"metadata":{"_uuid":"d4a1ef923e562e2bd693cfad874e1f9816582a42"},"cell_type":"markdown","source":"# Section One: Introduction and Baseline Model"},{"metadata":{"_uuid":"0bbe377baf4c566acf9cfaf01478779c9c86c7d6"},"cell_type":"markdown","source":"## 1. Abstract"},{"metadata":{"_uuid":"6895953afeb3d55800769eae53c3fb6b3d386f6f"},"cell_type":"markdown","source":"It is very important to evaluate the credibility of a person. In most cases, it is reliable to evaluate with complete credit records and files, however, some people do not have those records. So, it is hard to know if they will pay for the bill in the future. \n\nIn this project, we will use the historical loan application data provided by Home Credit to predict whether or not an applicant will be able to repay a loan. We first build a baseline model using simple data preprocessing and logistic regression model in section one. After that, in the second section, we add new datasets and take various feature engineering strategies to improve accuracy. In addition, we apply a brand new machine learning model - light GBM (Gradient Boosting Decision Tree (GBDT) implementation with GOSS and EFB), which is very effective and efficient for large size data. The result shows that our methods are effective. In the third section, we select the best features based on the results of the previous two sections. We remove the collinear, missing, and unimportant features. The result shows that the feature selection methods are effective. "},{"metadata":{"_uuid":"0573c2377b2b792dc04565db3a0eecf5004a9cde"},"cell_type":"markdown","source":"## 2. Introduction"},{"metadata":{"_uuid":"77ea9227ecadd482374e0ad4e1a0c01189013eb3"},"cell_type":"markdown","source":"There are tons of applicants want to make a loan. So, it is necessary to build an automatical system to evaluate the credit of people. A lot of related studies have been done in recent years.\n\nKhandani, A. E., Kim, A. J., & Lo, A. W. [1] applied machine learning models to forecast the consumer credit risk. They collected the customer transactions and credit bureau data from 2005 to 2009. They constructed out-of-sample forecasts that significantly improve the classification rates of credit-card-holder delinquencies and defaults, with linear regression R2â€™s of forecasted/realized delinquencies of 85%. \n\nGalindo, J., and Tamayo, P.[2] introduced a specific modeling methodology based on the study of error curves. The results show that CART decision-tree models provide the best estimation for default with an average 8.31% error rate for a training sample of 2,000 records. \n\nIn addition, Neural Networks provided the second best results with an average error of 11.00%. The K-Nearest Neighbor algorithm had an average error rate of 14.95%. These results outperformed the standard Probit algorithm which attained an average error rate of 15.13%. \n\nOne challenge for their study is the data size. If there is more data for training the model, the accuracy would be higher according to the experience. "},{"metadata":{"_uuid":"e1804b6ca2e43e93cff59a5686151fb1207c8a29"},"cell_type":"markdown","source":"## 3. Data Description"},{"metadata":{"_uuid":"03f65a90b4ce0b7d4be91cf6b6fc04a07a538ce0"},"cell_type":"markdown","source":"The data is provided by Home Credit, a service dedicated to provided lines of credit (loans) to the unbanked population. \n\nThere are three sources of data will be user in this section:\n1. application_train.csv\n2. application_test.csv\n3. codebook.csv\n\napplication_train/application_test data: the main training and testing data with information about each loan application at Home Credit. Every loan has its own row and is identified by the feature SK_ID_CURR. The training application data comes with the TARGET indicating 0: the loan was repaid or 1: the loan was not repaid.\n\ncodebook data: describe features in dataset including the full name and description of features.\n\nThere are some other data we will use in this project. They can connect with each other by particular feature."},{"metadata":{"_uuid":"0ffaf73ce2ee2ed1a07e3a06a07b1eda05d7101c"},"cell_type":"markdown","source":"![](https://i.postimg.cc/MH3Jjn5d/download.png)"},{"metadata":{"_uuid":"b9c1cc917ad94d54ec2ad5e467ef54d10c51c836"},"cell_type":"markdown","source":"## 4. Methods"},{"metadata":{"_uuid":"d6640f013dce0ede8f3f07e2febb5195b0e229c0"},"cell_type":"markdown","source":"Not everyone has a reliable credit history, which means it is really hard for banks or other institutions to evaluate his/her as a trustworthy lender or not. Due to insufficient or even no credit histories for those people, we can only utilize other records to evaluate their credibility. Home Credit is a company to take advantage of various data, like transactional information, to predict the reliability of those people, especially for their repayment abilities.\n\nIn this project, we will use the historical loan application data provided by Home Credit to predict whether or not an applicant will be able to repay a loan. The data size is very large, so it is not a good idea to use traditional machine learning models. Instead, we use Light GBM as the classifier [3]. \n\nLight GBM is a quick, distributed, efficient gradient boosting framework based on the decision tree algorithm. It splits the tree leaf wise with the best fit whereas other boosting algorithms split the tree depth wise or level wise rather than leaf-wise. So, when growing on the same leaf in Light GBM, the leaf-wise algorithm can reduce more loss than the level-wise algorithm and hence results in much better accuracy which can rarely be achieved by any of the existing boosting algorithms.\n\nLeaf wise splits lead to increase in complexity and may lead to overfitting and it can be overcome by specifying another parameter max-depth which specifies the depth to which splitting will occur. However, Light GBM is sensitive to overfitting and can easily overfit small data. So, it is only suitable for large data. \n![](https://i.postimg.cc/XqY0Jffr/leaf.png)\n![](https://i.postimg.cc/DwX38yXP/depth.png)"},{"metadata":{"_uuid":"2ea4574dc66ae52dcdad146b63f91d03289f568d"},"cell_type":"markdown","source":"In addition the Light GBM, we use several methods to make feature engineering and feature selection to improve the accuracy and efficiency. In the second section, we will use the two other data to improve the prediction accuracy. We will provide the detail information in section two. In section three, we try some methods to reduce the number of features, it will improve efficiency and accuracy. We will describe the methods in the third section. "},{"metadata":{"_uuid":"6343ad6b68d558ab2d98e4e34d9867ad8772e2fa"},"cell_type":"markdown","source":"<hr>\n## 5. Baseline Model"},{"metadata":{"_uuid":"ed94a70df30d39ec4bb1a2d097f58b3bd17e05a6"},"cell_type":"markdown","source":"# Import Packages"},{"metadata":{"_uuid":"0d1459c390309e70f83529756969a893154f526b","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn import linear_model\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import LeaveOneOut\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score,confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nimport gc\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.preprocessing import PolynomialFeatures\n\nimport time\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a0c070cfbccef68395932c9404f5b22d7fbb7ff5"},"cell_type":"markdown","source":"# Read Data"},{"metadata":{"_uuid":"62ca9ea8a62ac04c992912b83ec2558922a5c02b","trusted":true},"cell_type":"code","source":"app_train = pd.read_csv('../input/application_train.csv')\napp_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fbf6330668af77dc33fa277de38101d4a85ebad4","trusted":true},"cell_type":"code","source":"app_test = pd.read_csv('../input/application_test.csv')\napp_test.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7413e61187e560c36b4cd585ab9616c70f81c269"},"cell_type":"markdown","source":"# Training Data"},{"metadata":{"_uuid":"f240663daa4816771edab973044d5564ea0f7fde","trusted":true},"cell_type":"code","source":"target = app_train.loc[:, 'TARGET']\ncolumns_feature = app_train.columns.tolist()\ncolumns_feature.remove('TARGET')\nfeatures = app_train.loc[:, columns_feature]\nfeatures.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e2c4dcad7932d3fe0a86497454f7d2c68184eb4b"},"cell_type":"markdown","source":">Training Data Basic Information\n\n| Item         | Number           |\n| -------      | ---------------- |\n| Observation  | 307511           |\n| Features     | 121              |"},{"metadata":{"_uuid":"71d3638b7648dd82cf1a2206c83a891a99b6fa60"},"cell_type":"markdown","source":"# Testing Data"},{"metadata":{"_uuid":"3c030546d7fc0217610713e7097c89881fbd731d"},"cell_type":"markdown","source":"Comment: testing data does not have `TARGET` which is treat as the prediction result. However, we can still use the testing data.\n\nWe can upload the prediction result of the testing data using our model, and Kaggle will send the accuracy of our prediction back."},{"metadata":{"_uuid":"46ea680b9880d6512ca7ddafec264a391631cdfe","trusted":true},"cell_type":"code","source":"features = app_test\nfeatures.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ff1720885b5a4a277f5b01a611fc024ece1986db"},"cell_type":"markdown","source":">Testing Data Basic Information\n\n| Item         | Number           |\n| -------      | ---------------- |\n| Observation  | 48744           |\n| Features     | 121              |"},{"metadata":{"_uuid":"eb1414467800a1eea732bae8d47d554820cc396a"},"cell_type":"markdown","source":"# TARGET Distribution"},{"metadata":{"_uuid":"0410aab15625abae07c95d9c1933061e1cc2db10","trusted":true},"cell_type":"code","source":"df = pd.DataFrame(app_train['TARGET'].value_counts())\ndf['repaid'] = ['repaid', 'not repaid']\ndf.columns=['frequency', 'repaid']\ndf","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7f26b4989eab8a5574be76d55dcf8db8f869f36a"},"cell_type":"markdown","source":"0 means the loan was repaid on time; 1 means the loan was not repaid on time."},{"metadata":{"_uuid":"884e862b10beeceaa007c825e50eacab8686eff9","trusted":true},"cell_type":"code","source":"sns.set(style=\"whitegrid\")\nsize=(5, 5)\nfig, ax = plt.subplots(figsize=size)\nax.set_title('TARGET distribution')\nsns.barplot(x=\"repaid\", y=\"frequency\", data=df, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d670f6581e8cafd5dcfa7661e4f501e130594001"},"cell_type":"markdown","source":"It is obviously an imbalanced class problem, which means the not repaid people are far less than the paid people."},{"metadata":{"_uuid":"471c19cf5a452248e51a524c76d5c5e88ee1a721"},"cell_type":"markdown","source":"# Data Completeness"},{"metadata":{"_uuid":"9c9c2c148246d856d3edbc4743649edf01019e09","trusted":true},"cell_type":"code","source":"# calculate null function\ndef calCompleteness(data):\n    row_num = data.shape[0]\n\n    nul = data.isnull().sum()\n    nul = pd.DataFrame({'features': nul.index, 'null_number': nul.values})\n\n    comp = []\n    for index, row in nul.iterrows():\n        temp = row['null_number']\n        re = float(row_num - temp) / row_num * 100\n        comp.append(re)\n\n    nul['completeness'] = pd.to_numeric(comp, downcast='float')\n    nul = nul.sort_values(by=['completeness'], ascending=False)\n    nul = nul.reset_index(drop=True)\n    return nul\n\ncomplete = calCompleteness(features)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"27d60c8348673daf76f7ecf6f57ea34d13eee66c","trusted":true},"cell_type":"code","source":"sns.set(style=\"whitegrid\")\nsize=(20, 40)\nfig, ax = plt.subplots(figsize=size)\nax.set_title('Training Data Completeness')\nsns.barplot(x=\"completeness\", y=\"features\", data=complete, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ffdc69e2b68f808ec5da47a4ceca58558263f7b1"},"cell_type":"markdown","source":"# Data Type"},{"metadata":{"_uuid":"d6ad69fa1510c9c59437116ab7ad710302be1a28","trusted":true},"cell_type":"code","source":"app_train.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"444f5486033c1a760e5d5d42688c19012927da46"},"cell_type":"markdown","source":"# Check Object Type Data"},{"metadata":{"_uuid":"347676fcd0e514de83eb284270ab84342fc3ed7d","trusted":true},"cell_type":"code","source":"# pd.Series.nunique: Return number of unique elements in the object.\ntemp = app_train.select_dtypes('object').apply(pd.Series.nunique, axis = 0)\npd.DataFrame({'feature': temp.index, 'Number of Categories': temp.values})","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d4e63777ef9152c07201fabb0a9f4524d20af23d"},"cell_type":"markdown","source":"Most categorical variables have a small number of categories. "},{"metadata":{"_uuid":"98ad89e19ca76a7a36703fa7bd3c61af56293c37"},"cell_type":"markdown","source":"# Encode Categorical Variables"},{"metadata":{"_uuid":"04f7d2411eafd35c5e1790d9f59e2d631db974f6","trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle_count = 0\n\n# Iterate through the columns\nfor col in app_train:\n    if app_train[col].dtype == 'object':\n        # If 2 or fewer unique categories\n        if len(list(app_train[col].unique())) <= 2:\n            le.fit(app_train[col])\n            # Transform both training and testing data\n            app_train[col] = le.transform(app_train[col])\n            app_test[col] = le.transform(app_test[col])\n            \n            # Keep track of how many columns were label encoded\n            le_count += 1\n            \nprint('%d columns were label encoded.' % le_count)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bb2fae36956231e914afd6c1d071f524fd7c5c3d","trusted":true},"cell_type":"code","source":"# one-hot encoding\napp_train = pd.get_dummies(app_train)\napp_test = pd.get_dummies(app_test)\n\nprint('Training Features shape: ', app_train.shape)\nprint('Testing Features shape: ', app_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5b82f68b06133efffc9d4a8bb79e1328c46eece6"},"cell_type":"markdown","source":"# Aligning Training and Testing Data"},{"metadata":{"_uuid":"a4eb88f5b1d652b16fbd738e7808c4def3f4c7ab"},"cell_type":"markdown","source":"One-hot encoding created more columns in training data because there are some categorical variables not represents in testing data but represents in training data. So, it is necessary to remove the columns which are not in the testing data. "},{"metadata":{"_uuid":"976091196cc798d2d05432d1a2399b44c916cadd","trusted":true},"cell_type":"code","source":"train_labels = app_train['TARGET']\n# Align the training and testing data, keep only columns present in both dataframes\napp_train, app_test = app_train.align(app_test, join = 'inner', axis = 1)\n# Add the target back in\napp_train['TARGET'] = train_labels\n\nprint('Training Features shape: ', app_train.shape)\nprint('Testing Features shape: ', app_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"08c76f532090694c0c349e777dfd69d552c3cdfe"},"cell_type":"markdown","source":"# Anomalies Analysis"},{"metadata":{"_uuid":"a238ae0e4dd39409a2b5d876900ed8575fdc3e1e","trusted":true},"cell_type":"code","source":"# DAYS_BIRTH analysis\n# age description\n(app_train['DAYS_BIRTH'] / -365).describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4ec07af6f1199c2f50879fc7173107bb697d161f","trusted":true},"cell_type":"code","source":"# DAYS_EMPLOYED analysis\n# get employed years\n(app_train['DAYS_EMPLOYED'] / 365).describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cf4462ebdfa1dcdf23121731f0f0b13c52484fef","trusted":true},"cell_type":"code","source":"(app_train['DAYS_EMPLOYED']/356).plot.hist(title = 'Years Employment Histogram');\nplt.xlabel('Years Employment');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9c99f7156ce0f89e6a0da51131684223032b6fa3"},"cell_type":"markdown","source":"Some people are employed 1000 years, it is not correct. In addition, those anomalies values have the same value."},{"metadata":{"_uuid":"3430ebed72ee2e692df1c6864f4062a0bcaf093e","trusted":true},"cell_type":"code","source":"# Create an anomalous flag column\napp_train['DAYS_EMPLOYED_ANOM'] = app_train[\"DAYS_EMPLOYED\"] == 365243\n\n# Replace the anomalous values with nan\napp_train['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\n\napp_train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram');\nplt.xlabel('Days Employment');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"30bdca72e85755e483159c6388cac307b08deaf3","trusted":true},"cell_type":"code","source":"# do same thing on the testing data\napp_test['DAYS_EMPLOYED_ANOM'] = app_test[\"DAYS_EMPLOYED\"] == 365243\napp_test[\"DAYS_EMPLOYED\"].replace({365243: np.nan}, inplace = True)\n\nprint('There are %d anomalies in the test data out of %d entries' % (app_test[\"DAYS_EMPLOYED_ANOM\"].sum(), len(app_test)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8842a6d4e03a390447101c9b9d48295eb90716eb"},"cell_type":"markdown","source":"# Correlation Analysis"},{"metadata":{"_uuid":"5d9456b108d4ddbfa5b8105099e4033f96253f2b","trusted":true},"cell_type":"code","source":"# Find correlations with the target and sort\ncorr = app_train.corr()\ncorrelations = corr['TARGET'].sort_values()\n\n# Display correlations\nprint('Most Positive Correlations:\\n', correlations.tail(15))\nprint('\\nMost Negative Correlations:\\n', correlations.head(15))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"716de1759319b849c47502510a05e6e32453bf89"},"cell_type":"markdown","source":"# Baseling\n## Logistic Regression Implementation"},{"metadata":{"_uuid":"7a388d80ac05252e31d462171c0e21d88cfc5f21","trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import Imputer\n# Drop the target from the training data\nif 'TARGET' in app_train:\n    train = app_train.drop(columns = ['TARGET'])\nelse:\n    train = app_train.copy()\n    \n# Feature names\nfeatures = list(train.columns)\n\n# Copy of the testing data\ntest = app_test.copy()\n\n# Median imputation of missing values\nimputer = Imputer(strategy = 'median')\n\n# Scale each feature to 0-1\nscaler = MinMaxScaler(feature_range = (0, 1))\n\n# Fit on the training data\nimputer.fit(train)\n\n# Transform both training and testing data\ntrain = imputer.transform(train)\ntest = imputer.transform(app_test)\n\n# Repeat with the scaler\nscaler.fit(train)\ntrain = scaler.transform(train)\ntest = scaler.transform(test)\n\nprint('Training data shape: ', train.shape)\nprint('Testing data shape: ', test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e24a41e0d7efdc60d09501c917f13617311000ab","trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\n# Make the model with the specified regularization parameter\nlog_reg = LogisticRegression(C = 0.0001, solver='lbfgs')\n\n# Train on the training data\nlog_reg.fit(train, train_labels)\n\n# Make predictions\n# Make sure to select the second column only\nlog_reg_pred = log_reg.predict_proba(test)[:, 1]\n# Submission dataframe\nsubmit = app_test.loc[:, ['SK_ID_CURR']]\nsubmit['TARGET'] = log_reg_pred\n\nsubmit.head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"_kg_hide-output":false,"_uuid":"175dba0939f8aabed949f7e516ce4ccd87e772d4","trusted":true},"cell_type":"code","source":"# Save the submission to a csv file\nsubmit.to_csv('baseling_submission.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"58ef419fe36ca01cd3da91b97a5da37b6587fc72"},"cell_type":"markdown","source":"![](https://i.postimg.cc/3NRP7Khp/baseline.png)"},{"metadata":{"_uuid":"ac747e3fa1d366c36afb14777511f5ad1e0047d9"},"cell_type":"markdown","source":"## References:\n[1] Khandani, A. E., Kim, A. J., & Lo, A. W. (2010). Consumer credit-risk models via machine-learning algorithms. Journal of Banking & Finance, 34(11), 2767-2787.\n\n[2] Galindo, J., & Tamayo, P. (2000). Credit risk assessment using statistical and machine learning: basic methodology and risk modeling applications. Computational Economics, 15(1-2), 107-143.\n\n[3] Ke, G., Meng, Q., Finley, T., Wang, T., Chen, W., Ma, W., ... & Liu, T. Y. (2017). Lightgbm: A highly efficient gradient boosting decision tree. In Advances in Neural Information Processing Systems (pp. 3146-3154)."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}