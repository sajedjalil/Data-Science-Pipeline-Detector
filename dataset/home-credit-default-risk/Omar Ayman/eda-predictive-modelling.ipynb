{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Imports"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport gc\nimport time\nimport numpy as np\nimport pandas as pd\nfrom contextlib import contextmanager\nimport multiprocessing as mp\nfrom functools import partial\nfrom scipy.stats import kurtosis, iqr, skew\nimport lightgbm as lgb\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nimport warnings\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.impute import SimpleImputer\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Getting data"},{"metadata":{"trusted":true},"cell_type":"code","source":"def missing_values_table(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() / len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_stats(feature,label_rotation=False,horizontal_layout=True):\n    temp = df[feature].value_counts()\n    df1 = pd.DataFrame({feature: temp.index,'Number of contracts': temp.values})\n\n    # Calculate the percentage of target=1 per category value\n    cat_perc = df[[feature, 'TARGET']].groupby([feature],as_index=False).mean()\n    cat_perc.sort_values(by='TARGET', ascending=False, inplace=True)\n    \n    if(horizontal_layout):\n        fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12,6))\n    else:\n        fig, (ax1, ax2) = plt.subplots(nrows=2, figsize=(12,14))\n    sns.set_color_codes(\"pastel\")\n    s = sns.barplot(ax=ax1, x = feature, y=\"Number of contracts\",data=df1)\n    if(label_rotation):\n        s.set_xticklabels(s.get_xticklabels(),rotation=90)\n    \n    s = sns.barplot(ax=ax2, x = feature, y='TARGET', order=cat_perc[feature], data=cat_perc)\n    if(label_rotation):\n        s.set_xticklabels(s.get_xticklabels(),rotation=90)\n    plt.ylabel('Percent of target with value 1 [%]', fontsize=10)\n    plt.tick_params(axis='both', which='major', labelsize=10)\n\n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_distribution_comp(var,nrow=2):\n    \n    i = 0\n    t1 = df.loc[df['TARGET'] != 0]\n    t0 = df.loc[df['TARGET'] == 0]\n\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(nrow,2,figsize=(12,6*nrow))\n\n    for feature in var:\n        i += 1\n        plt.subplot(nrow,2,i)\n        sns.kdeplot(t1[feature], bw=0.5,label=\"TARGET = 1\")\n        sns.kdeplot(t0[feature], bw=0.5,label=\"TARGET = 0\")\n        plt.ylabel('Density plot', fontsize=12)\n        plt.xlabel(feature, fontsize=12)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='both', which='major', labelsize=12)\n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/home-credit-default-risk/application_train.csv')\ntest =  pd.read_csv('/kaggle/input/home-credit-default-risk/application_test.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_values_table(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['TARGET'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"data is imbalanced"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.select_dtypes('object').apply(pd.Series.nunique, axis = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most of the categorical variables are having 2 unique variables so its ok to use label encoding with columns which has 2 unique variables and one hot encoding with the rest."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.select_dtypes(['float64','int64']).describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"let's explore distribution for some appealing columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_stats('CODE_GENDER')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The number of female clients is almost double the number of male clients. Looking to the percent of defaulted credits, males have a higher chance of not returning their loans.\n\nalso there's a strange value 'XNA' , you will need to remove later"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_stats('FLAG_OWN_CAR')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"People who doesn't own a car are more likely to not return their loan"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_stats('FLAG_OWN_REALTY')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This feature is not actually interesting as it contibute the same towards return/not return the loan"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_stats('CNT_CHILDREN')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So more childern contibute alot in the way on not returning the loans"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_stats('NAME_INCOME_TYPE',True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that both unemployed and people who take Maternity leave are struggling in returning their loans, we can add a flag Unemployed/Maternity leave later in feature engineering part"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_stats('NAME_FAMILY_STATUS',True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can try to merge both Civil marriage and Married under on variable later"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_stats('NAME_HOUSING_TYPE',True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Both people with rented apartment and who lives with their parents are more likely to struggle in returning the loan"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_stats('OCCUPATION_TYPE',True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"low skill laborers flag can be added "},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_stats('CNT_FAM_MEMBERS',True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"More family memembers contibutes alot towards non returning of loans"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_stats('REG_CITY_NOT_LIVE_CITY')\nplot_stats('REG_CITY_NOT_WORK_CITY')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's explore some continuous features aganist their target"},{"metadata":{"trusted":true},"cell_type":"code","source":"var = ['AMT_ANNUITY','AMT_GOODS_PRICE','DAYS_EMPLOYED', 'DAYS_REGISTRATION','DAYS_BIRTH','DAYS_ID_PUBLISH']\nplot_distribution_comp(var,nrow=3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Those feature are somehow equally distributed for target = 0 or target =1"},{"metadata":{},"cell_type":"markdown","source":"## Anomality handling"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram');\nplt.xlabel('Days Employment');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['CNT_CHILDREN'].plot.hist(title = 'CNT_CHILDREN');\nplt.xlabel('CNT_CHILDREN');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['AMT_ANNUITY'].plot.hist(title = 'AMT_ANNUITY');\nplt.xlabel('AMT_ANNUITY');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['CNT_FAM_MEMBERS'].plot.hist(title = 'CNT_FAM_MEMBERS');\nplt.xlabel('CNT_FAM_MEMBERS');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df[df['DAYS_EMPLOYED'] != 365243]\ndf = df[df['CNT_CHILDREN']<4]\ndf = df[df['AMT_ANNUITY']<120000]\ndf = df[df['CNT_FAM_MEMBERS']<7.5]\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Correlations"},{"metadata":{"trusted":true},"cell_type":"code","source":"correlations = df.corr()['TARGET'].sort_values()\ncorrelations","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"strong negative correlation "},{"metadata":{},"cell_type":"markdown","source":"# Feature engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.NAME_INCOME_TYPE.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.append(test)\ndf['DAYS_BIRTH'] = abs(df['DAYS_BIRTH'])\ndf['family_members_more7']= np.where(df['CNT_FAM_MEMBERS']>7,1,0)\ndf['islowskilled_labour']= np.where(df['OCCUPATION_TYPE']=='Low-skill Laborers',1,0)\ndf['is_Maternity_leave']= np.where(df['NAME_INCOME_TYPE'] =='Maternity leave' ,1,0)\ndf['is_unemployed']= np.where(df['NAME_INCOME_TYPE'] =='Unemployed' ,1,0)\n\ndf['cnt_childern_more6']= np.where(df['CNT_CHILDREN'] > 6,1,0)\nplt.style.use('fivethirtyeight')\n\n# Plot the distribution of ages in years\nplt.hist(df['DAYS_BIRTH'] / 365, edgecolor = 'k', bins = 25)\nplt.title('Age of Client'); plt.xlabel('Age (years)'); plt.ylabel('Count');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.family_members_more7.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can try to bin the days birth to ranges maybe according to years so that it could be easier for the model we use"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_age_label(days_birth):\n    \"\"\" Return the age group label (int). \"\"\"\n    age_years = -days_birth / 365\n    if age_years < 27: return 1\n    elif age_years < 40: return 2\n    elif age_years < 50: return 3\n    elif age_years < 65: return 4\n    elif age_years < 99: return 5\n    else: return 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df[df['CODE_GENDER']!='XNA']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['AGE_RANGE'] = df['DAYS_BIRTH'].apply(lambda x: get_age_label(x))\ndf['CREDIT_INCOME_PERCENT'] = df['AMT_CREDIT'] / df['AMT_INCOME_TOTAL']\ndf['ANNUITY_INCOME_PERCENT'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']\ndf['CREDIT_TERM'] = df['AMT_ANNUITY'] / df['AMT_CREDIT']\ndf['DAYS_EMPLOYED_PERCENT'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\ndf['CREDIT_TO_GOODS_RATIO'] = df['AMT_CREDIT'] / df['AMT_GOODS_PRICE']\ndf['INCOME_TO_EMPLOYED_RATIO'] = df['AMT_INCOME_TOTAL'] / df['DAYS_EMPLOYED']\ndf['INCOME_TO_BIRTH_RATIO'] = df['AMT_INCOME_TOTAL'] / df['DAYS_BIRTH']\ndf['ID_TO_BIRTH_RATIO'] = df['DAYS_ID_PUBLISH'] / df['DAYS_BIRTH']\ndf['CAR_TO_BIRTH_RATIO'] = df['OWN_CAR_AGE'] / df['DAYS_BIRTH']\ndf['CAR_TO_EMPLOYED_RATIO'] = df['OWN_CAR_AGE'] / df['DAYS_EMPLOYED']\ndf['PHONE_TO_BIRTH_RATIO'] = df['DAYS_LAST_PHONE_CHANGE'] / df['DAYS_BIRTH']\nfor function_name in ['min', 'max', 'mean', 'nanmedian', 'var']:\n    feature_name = 'EXT_SOURCES_{}'.format(function_name.upper())\n    df[feature_name] = eval('np.{}'.format(function_name))(\n        df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']], axis=1)\ndf['EXT_SOURCES_PROD'] = df['EXT_SOURCE_1'] * df['EXT_SOURCE_2'] * df['EXT_SOURCE_3']\ndf['CREDIT_LENGTH'] = df['AMT_CREDIT'] / df['AMT_ANNUITY']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"external_sources = df[['EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3','DAYS_BIRTH']]\nimputer = SimpleImputer(strategy = 'median')\nexternal_sources = imputer.fit_transform(external_sources)\nexternal_sources= pd.DataFrame(external_sources, columns = ['EXT_SOURCE_1', 'EXT_SOURCE_2', \n                                                                           'EXT_SOURCE_3', 'DAYS_BIRTH'])\n\nexternal_sources['SK_ID_CURR'] = df['SK_ID_CURR'].tolist()\ndf = df.merge(external_sources, on = 'SK_ID_CURR', how = 'left')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_list = [\n        'CNT_CHILDREN', 'CNT_FAM_MEMBERS', 'HOUR_APPR_PROCESS_START',\n        'FLAG_EMP_PHONE', 'FLAG_MOBIL', 'FLAG_CONT_MOBILE', 'FLAG_EMAIL', 'FLAG_PHONE',\n        'FLAG_OWN_REALTY', 'REG_REGION_NOT_LIVE_REGION', 'REG_REGION_NOT_WORK_REGION',\n        'REG_CITY_NOT_WORK_CITY', 'OBS_30_CNT_SOCIAL_CIRCLE', 'OBS_60_CNT_SOCIAL_CIRCLE',\n        'AMT_REQ_CREDIT_BUREAU_DAY', 'AMT_REQ_CREDIT_BUREAU_MON', 'AMT_REQ_CREDIT_BUREAU_YEAR', \n        'COMMONAREA_MODE', 'NONLIVINGAREA_MODE', 'ELEVATORS_MODE', 'NONLIVINGAREA_AVG',\n        'FLOORSMIN_MEDI', 'LANDAREA_MODE', 'NONLIVINGAREA_MEDI', 'LIVINGAPARTMENTS_MODE',\n        'FLOORSMIN_AVG', 'LANDAREA_AVG', 'FLOORSMIN_MODE', 'LANDAREA_MEDI',\n        'COMMONAREA_MEDI', 'YEARS_BUILD_AVG', 'COMMONAREA_AVG', 'BASEMENTAREA_AVG',\n        'BASEMENTAREA_MODE', 'NONLIVINGAPARTMENTS_MEDI', 'BASEMENTAREA_MEDI', \n        'LIVINGAPARTMENTS_AVG', 'ELEVATORS_AVG', 'YEARS_BUILD_MEDI', 'ENTRANCES_MODE',\n        'NONLIVINGAPARTMENTS_MODE', 'LIVINGAREA_MODE', 'LIVINGAPARTMENTS_MEDI',\n        'YEARS_BUILD_MODE', 'YEARS_BEGINEXPLUATATION_AVG', 'ELEVATORS_MEDI', 'LIVINGAREA_MEDI',\n        'YEARS_BEGINEXPLUATATION_MODE', 'NONLIVINGAPARTMENTS_AVG', 'HOUSETYPE_MODE',\n        'FONDKAPREMONT_MODE', 'EMERGENCYSTATE_MODE'   ,'OWN_CAR_AGE',\n'CAR_TO_EMPLOYED_RATIO', \n'CAR_TO_BIRTH_RATIO', \n'EXT_SOURCES_PROD', \n'APARTMENTS_AVG', \n'APARTMENTS_MODE',\n'APARTMENTS_MEDI',\n'ENTRANCES_AVG', \n'ENTRANCES_MEDI', \n'LIVINGAREA_AVG', \n'FLOORSMAX_MEDI', \n'FLOORSMAX_AVG', \n'FLOORSMAX_MODE', \n'YEARS_BEGINEXPLUATATION_MEDI', \n'TOTALAREA_MODE'\n    ]\ndf.drop(drop_list,axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"le = LabelEncoder()\nfor col in df:\n    if df[col].dtype == 'object':\n        # If 2 or fewer unique categories\n        if len(list(df[col].unique())) <= 2:\n            # Train on the training data\n            le.fit_transform(df[col])\ndf = pd.get_dummies(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.replace([np.inf, -np.inf], np.nan)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modelling"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = df[df['TARGET'].notnull()]\ntest = df[df['TARGET'].isnull()]\nX,y = train.drop(['TARGET','SK_ID_CURR'],axis=1),train['TARGET'].tolist()\nprint(\"Train/valid shape: {}, test shape: {}\".format(train.shape, test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X.shape)\nprint(len(y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=314,stratify=y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fit_params={\"early_stopping_rounds\":30, \n            \"eval_metric\" : 'auc', \n            \"eval_set\" : [(X_test,y_test)],\n            'eval_names': ['valid'],\n            'verbose': 100,\n            'categorical_feature': 'auto'}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Learning rate callbacks"},{"metadata":{"trusted":true},"cell_type":"code","source":"def learning_rate_010_decay_power_099(current_iter):\n    base_learning_rate = 0.1\n    lr = base_learning_rate  * np.power(.99, current_iter)\n    return lr if lr > 1e-3 else 1e-3\n\ndef learning_rate_010_decay_power_0995(current_iter):\n    base_learning_rate = 0.1\n    lr = base_learning_rate  * np.power(.995, current_iter)\n    return lr if lr > 1e-3 else 1e-3\n\ndef learning_rate_005_decay_power_099(current_iter):\n    base_learning_rate = 0.05\n    lr = base_learning_rate  * np.power(.99, current_iter)\n    return lr if lr > 1e-3 else 1e-3","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n# Set up HyperParameter search space\n\nWe use random search, which is more flexible and more efficient than a grid search\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform\nparam_test ={'num_leaves': sp_randint(6, 50), \n             'min_child_samples': sp_randint(100, 500), \n             'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n             'subsample': sp_uniform(loc=0.2, scale=0.8), \n             'colsample_bytree': sp_uniform(loc=0.4, scale=0.6),\n             'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n             'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#This parameter defines the number of HP points to be tested\nn_HP_points_to_test = 100\n\n\n#n_estimators is set to a \"large value\". The actual number of trees build will depend on early stopping and 5000 define only the absolute maximum\nclf = lgb.LGBMClassifier(max_depth=-1, random_state=314, silent=True, metric='None', n_jobs=4, n_estimators=5000)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gs = RandomizedSearchCV(\n    estimator=clf, param_distributions=param_test, \n    n_iter=n_HP_points_to_test,\n    scoring='roc_auc',\n    cv=3,\n    refit=True,\n    random_state=314,\n    verbose=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"opt_parameters = {'colsample_bytree': 0.9234, 'min_child_samples': 399, 'min_child_weight': 0.1, 'num_leaves': 13, 'reg_alpha': 2, 'reg_lambda': 5, 'subsample': 0.855,'imbalanced':True}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nX_train.columns =  [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in X_train.columns]\nX_test.columns =  [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in X_test.columns]\n\nclf_final = lgb.LGBMClassifier(**clf.get_params())\n#set optimal parameters\nclf_final.set_params(**opt_parameters)\n\n#Train the final model with learning rate decay\nclf_final.fit(X_train, y_train, **fit_params, callbacks=[lgb.reset_parameter(learning_rate=learning_rate_010_decay_power_0995)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_imp = pd.Series(clf_final.feature_importances_, index=train.drop(['SK_ID_CURR', 'TARGET'], axis=1).columns)\nfeat_imp.nlargest(20).plot(kind='barh', figsize=(8,10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Trying Upsampling the minority class"},{"metadata":{"trusted":true},"cell_type":"code","source":"# X = X.replace([np.inf, -np.inf], np.nan)\n# X.fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from imblearn.over_sampling import SMOTE\n# X_resampled, y_resampled = SMOTE().fit_sample(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.20, random_state=314,stratify=y_resampled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_train.columns =  [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in X_train.columns]\n# X_test.columns =  [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in X_test.columns]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# clf_final = lgb.LGBMClassifier(**clf.get_params())\n# #set optimal parameters\n# clf_final.set_params(**opt_parameters)\n\n# #Train the final model with learning rate decay\n# clf_final.fit(X_train, y_train, **fit_params, callbacks=[lgb.reset_parameter(learning_rate=learning_rate_010_decay_power_0995)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"probabilities = clf_final.predict_proba(test.drop(['SK_ID_CURR','TARGET'], axis=1))\nsubmission = pd.DataFrame({\n    'SK_ID_CURR': test['SK_ID_CURR'],\n    'TARGET':     [ row[1] for row in probabilities]\n})\nsubmission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}