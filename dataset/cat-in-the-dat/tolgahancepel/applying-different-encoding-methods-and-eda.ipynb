{"cells":[{"metadata":{},"cell_type":"markdown","source":"<hr/>\n[**Tolgahan Cepel**](https://www.kaggle.com/tolgahancepel)\n<hr/>\n<font color=green>\n1. [Importing Libraries and Reading the Dataset](#1)\n2. [Exploratory Data Analysis (EDA)](#2)\n3. [Encoding the Features](#3)\n    * [Binary Features (Mapping)](#4) \n    * [Low and Medium Cardinality Features (Dummy Encoding)](#5)\n    * [High Cardinality Features (Target Encoding)](#6) \n4. [Model](#7)\n    * [LightGBM](#8) \n    * [Features Importance](#9)\n5. [Submission](#10)\n<hr/>"},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://docs.google.com/uc?id=1HKySMKDyZF6s7Lo62e8Df0s4B-xcpDyL\" width=\"750px\">\n"},{"metadata":{},"cell_type":"markdown","source":"## <span id=\"1\"></span> ** 1. Importing Libraries and Reading the Dataset **"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom scipy import stats\nfrom sklearn.model_selection import KFold\nimport lightgbm as lgb\nfrom sklearn.metrics import roc_auc_score\nimport gc\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('dark')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def resumetable(df):\n    #print(f\"Dataset Shape: {df.shape}\")\n    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])\n    summary = summary.reset_index()\n    summary['Name'] = summary['index']\n    summary = summary[['Name','dtypes']]\n    summary['Missing'] = df.isnull().sum().values    \n    summary['Uniques'] = df.nunique().values\n    summary['First Value'] = df.loc[0].values\n    summary['Second Value'] = df.loc[1].values\n    summary['Third Value'] = df.loc[2].values\n\n    for name in summary['Name'].value_counts().index:\n        summary.loc[summary['Name'] == name, 'Entropy'] = round(stats.entropy(df[name].value_counts(normalize=True), base=2),2) \n\n    return summary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/cat-in-the-dat/train.csv')\ntest = pd.read_csv('/kaggle/input/cat-in-the-dat/test.csv')\nsubmission = pd.read_csv('/kaggle/input/cat-in-the-dat/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Train shape: \", train.shape)\nprint(\"Test shape: \", test.shape)\nprint(\"Submission shape: \", submission.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <span id=\"2\"></span> ** 2. Exploratory Data Analysis (EDA) **"},{"metadata":{},"cell_type":"markdown","source":"Let's discover unique values of columns. As you can see, there are 23 columns without <b>id</b> and <b>target</b>."},{"metadata":{"trusted":true},"cell_type":"code","source":"summary = resumetable(train)\nsummary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"total = len(train)\nplt.figure(figsize=(10,6))\nflatui = [\"#e74c3c\", \"#34495e\"]\n\ng = sns.countplot(x='target', data=train, palette=flatui)\ng.set_title(\"TARGET DISTRIBUTION\", fontsize = 20)\ng.set_xlabel(\"Target Vaues\", fontsize = 15)\ng.set_ylabel(\"Count\", fontsize = 15)\n\n\nsizes=[] # Get highest values in y\nfor p in g.patches:\n    height = p.get_height()\n    sizes.append(height)\n    g.text(p.get_x()+p.get_width()/2.,\n            height + 3,\n            '{:1.2f}%'.format(height/total*100),\n            ha=\"center\", fontsize=14) \ng.set_ylim(0, max(sizes) * 1.15) # set y limit based on highest heights\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bin_cols = ['bin_0', 'bin_1', 'bin_2', 'bin_3', 'bin_4']","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import matplotlib.gridspec as gridspec\ngrid = gridspec.GridSpec(3, 2)\nplt.figure(figsize=(16,20))\n\nfor n, col in enumerate(train[bin_cols]): \n    ax = plt.subplot(grid[n]) # feeding the figure of grid\n    sns.countplot(x=col, data=train, hue='target', palette=flatui) \n    ax.set_ylabel('Count', fontsize=15) # y axis label\n    ax.set_title(f'{col} Distribution by Target', fontsize=18) # title label\n    ax.set_xlabel(f'{col} values', fontsize=15) # x axis label\n    sizes=[] # Get highest values in y\n    for p in ax.patches: # loop to all objects\n        height = p.get_height()\n        sizes.append(height)\n        ax.text(p.get_x()+p.get_width()/2.,\n                height + 3,\n                '{:1.2f}%'.format(height/total*100),\n                ha=\"center\", fontsize=14) \n    ax.set_ylim(0, max(sizes) * 1.15) # set y limit based on highest heights\n    \nplt.show()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <span id=\"4\"></span> **  **"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def ploting_cat_fet(df, cols, vis_row=5, vis_col=2):\n    \n    grid = gridspec.GridSpec(vis_row,vis_col) # The grid of chart\n    plt.figure(figsize=(17, 35)) # size of figure\n\n    # loop to get column and the count of plots\n    for n, col in enumerate(train[cols]): \n        tmp = pd.crosstab(train[col], train['target'], normalize='index') * 100\n        tmp = tmp.reset_index()\n        tmp.rename(columns={0:'No',1:'Yes'}, inplace=True)\n\n        ax = plt.subplot(grid[n]) # feeding the figure of grid\n        sns.countplot(x=col, data=train, order=list(tmp[col].values) , palette='Set2') \n        ax.set_ylabel('Count', fontsize=15) # y axis label\n        ax.set_title(f'{col} Distribution by Target', fontsize=18) # title label\n        ax.set_xlabel(f'{col} values', fontsize=15) # x axis label\n\n        # twinX - to build a second yaxis\n        gt = ax.twinx()\n        gt = sns.pointplot(x=col, y='Yes', data=tmp,\n                           order=list(tmp[col].values),\n                           color='black', legend=False)\n        gt.set_ylim(tmp['Yes'].min()-5,tmp['Yes'].max()*1.1)\n        gt.set_ylabel(\"Target %True(1)\", fontsize=16)\n        sizes=[] # Get highest values in y\n        for p in ax.patches: # loop to all objects\n            height = p.get_height()\n            sizes.append(height)\n            ax.text(p.get_x()+p.get_width()/2.,\n                    height + 3,\n                    '{:1.2f}%'.format(height/total*100),\n                    ha=\"center\", fontsize=14) \n        ax.set_ylim(0, max(sizes) * 1.15) # set y limit based on highest heights\n\n\n    plt.subplots_adjust(hspace = 0.5, wspace=.3)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nom_cols = ['nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ploting_cat_fet(train, nom_cols, vis_row=5, vis_col=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ord_cols = ['ord_0', 'ord_1', 'ord_2', 'ord_3']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ploting_cat_fet(train, ord_cols, vis_row=5, vis_col=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"tmp = pd.crosstab(train['ord_4'], train['target'], normalize='index') * 100\ntmp = tmp.reset_index()\ntmp.rename(columns={0:'No',1:'Yes'}, inplace=True)\nplt.figure(figsize=(15,12))\n\nplt.subplot(211)\nax = sns.countplot(x='ord_4', data=train, order=list(tmp['ord_4'].values), palette='Set2') \nax.set_ylabel('Count', fontsize=17) # y axis label\nax.set_title('ord_4 Distribution with Target %ratio', fontsize=20) # title label\nax.set_xlabel('ord_4 values', fontsize=17) # x axis label\n# twinX - to build a second yaxis\ngt = ax.twinx()\ngt = sns.pointplot(x='ord_4', y='Yes', data=tmp,\n                   order=list(tmp['ord_4'].values),\n                   color='black', legend=False)\ngt.set_ylim(tmp['Yes'].min()-5,tmp['Yes'].max()*1.1)\ngt.set_ylabel(\"Target %True(1)\", fontsize=16)\n\ngt = ax.twinx()\ngt = sns.pointplot(x='ord_4', y='Yes', data=tmp,\n                   order=list(tmp['ord_4'].values),\n                   color='black', legend=False)\ngt.set_ylim(tmp['Yes'].min()-5,tmp['Yes'].max()*1.1)\ngt.set_ylabel(\"Target %True(1)\", fontsize=16)\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <span id=\"4\"></span> ** Date Features **"},{"metadata":{"trusted":true},"cell_type":"code","source":"date_cols = ['day', 'month']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ploting_cat_fet(train, date_cols, vis_row=6, vis_col=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <span id=\"3\"></span> ** 3. Encoding the Features **"},{"metadata":{},"cell_type":"markdown","source":"### <span id=\"4\"></span> ** Binary Features (Mapping) **"},{"metadata":{"trusted":true},"cell_type":"code","source":"# dictionary to map the feature\nbin_dict = {'T':1, 'F':0, 'Y':1, 'N':0}\n\n# Maping the category values in our dict\ntrain['bin_3'] = train['bin_3'].map(bin_dict)\ntrain['bin_4'] = train['bin_4'].map(bin_dict)\ntest['bin_3'] = test['bin_3'].map(bin_dict)\ntest['bin_4'] = test['bin_4'].map(bin_dict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <span id=\"5\"></span> ** Low and Medium Cardinality Features (Dummy Encoding) **"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Concatenating train and test data\ntest['target'] = 'test'\ndf = pd.concat([train, test], axis=0, sort=False)\n\nprint(\"Data shape:\", df.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are converting categorical variables into dummy variables. Recap these nominal features:\n- <b> nom_0 : </b> 3 uniques\n- <b> nom_1 : </b> 6 uniques\n- <b> nom_2 : </b> 6 uniques\n- <b> nom_3 : </b> 6 uniques\n- <b> nom_4 : </b> 4 uniques"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Shape before dummy transformation: {df.shape}')\ndf = pd.get_dummies(df, columns=['nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4'],\\\n                          prefix=['nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4'], drop_first=True)\nprint(f'Shape after dummy transformation: {df.shape}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# Seperating dataset into train and test set\ntrain, test = df[df['target'] != 'test'], df[df['target'] == 'test'].drop('target', axis=1)\ntrain['target'] = train['target'].astype(int)\ndel df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ordinal features should be ordered before encoding. Recap these ordinal features:\n- <b> ord_0 : </b> 3 uniques (1,2,3) <- this is already ordered\n- <b> ord_1 : </b> 5 uniques (Novice, Contributor, Expert, Master, Grandmaster)\n- <b> ord_2 : </b> 6 uniques (Freezing, Cold, Warm, Hot, Boiling Hot, Lava Hot)\n- <b> ord_3 : </b> 15 uniques (a, b, c, ..., o)\n- <b> ord_4 : </b> 26 uniques (A, B, C, D, E, ..., Z)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing categorical options of pandas\nfrom pandas.api.types import CategoricalDtype \n\n# seting the orders of our ordinal features\nord_1 = CategoricalDtype(categories=['Novice', 'Contributor','Expert', \n                                     'Master', 'Grandmaster'], ordered=True)\nord_2 = CategoricalDtype(categories=['Freezing', 'Cold', 'Warm', 'Hot',\n                                     'Boiling Hot', 'Lava Hot'], ordered=True)\nord_3 = CategoricalDtype(categories=['a', 'b', 'c', 'd', 'e', 'f', 'g',\n                                     'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o'], ordered=True)\nord_4 = CategoricalDtype(categories=['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I',\n                                     'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R',\n                                     'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'], ordered=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transforming ordinal Features\ntrain.ord_1 = train.ord_1.astype(ord_1)\ntrain.ord_2 = train.ord_2.astype(ord_2)\ntrain.ord_3 = train.ord_3.astype(ord_3)\ntrain.ord_4 = train.ord_4.astype(ord_4)\n\n# test dataset\ntest.ord_1 = test.ord_1.astype(ord_1)\ntest.ord_2 = test.ord_2.astype(ord_2)\ntest.ord_3 = test.ord_3.astype(ord_3)\ntest.ord_4 = test.ord_4.astype(ord_4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Geting the codes of ordinal categoy's - train\ntrain.ord_1 = train.ord_1.cat.codes\ntrain.ord_2 = train.ord_2.cat.codes\ntrain.ord_3 = train.ord_3.cat.codes\ntrain.ord_4 = train.ord_4.cat.codes\n\n# Geting the codes of ordinal categoy's - test\ntest.ord_1 = test.ord_1.cat.codes\ntest.ord_2 = test.ord_2.cat.codes\ntest.ord_3 = test.ord_3.cat.codes\ntest.ord_4 = test.ord_4.cat.codes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[['ord_0', 'ord_1', 'ord_2', 'ord_3', 'ord_4']].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have 2 date features. These are <b>day</b> and <b>month</b>. Recap:\n- <b> day : </b> 7 uniques (1, 2, 3, 4, 5, 6, 7)\n- <b> month : </b> 12 uniques (1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transfer the cyclical features into two dimensional sin-cos features\n# https://www.kaggle.com/avanwyk/encoding-cyclical-features-for-deep-learning\n\ndef date_cyc_enc(df, col, max_vals):\n    df[col + '_sin'] = np.sin(2 * np.pi * df[col]/max_vals)\n    df[col + '_cos'] = np.cos(2 * np.pi * df[col]/max_vals)\n    return df\n\ntrain = date_cyc_enc(train, 'day', 7)\ntest = date_cyc_enc(test, 'day', 7) \n\ntrain = date_cyc_enc(train, 'month', 12)\ntest = date_cyc_enc(test, 'month', 12)\n\n# NOTE, I discovered it on: kaggle.com/gogo827jz/catboost-baseline-with-feature-importance","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Allright! There is only <b>ord_5</b> left. This feature is ordered by letters. Recap:\n- <b> ord_5 : </b> 192 uniques"},{"metadata":{"trusted":true},"cell_type":"code","source":"### Credit of this features to: \n## https://www.kaggle.com/gogo827jz/catboost-baseline-with-feature-importance\n\nimport string\n\n# Then encode 'ord_5' using ACSII values\n\n# Option 1: Add up the indices of two letters in string.ascii_letters\ntrain['ord_5_oe_add'] = train['ord_5'].apply(lambda x:sum([(string.ascii_letters.find(letter)+1) for letter in x]))\ntest['ord_5_oe_add'] = test['ord_5'].apply(lambda x:sum([(string.ascii_letters.find(letter)+1) for letter in x]))\n\n# Option 2: Join the indices of two letters in string.ascii_letters\ntrain['ord_5_oe_join'] = train['ord_5'].apply(lambda x:float(''.join(str(string.ascii_letters.find(letter)+1) for letter in x)))\ntest['ord_5_oe_join'] = test['ord_5'].apply(lambda x:float(''.join(str(string.ascii_letters.find(letter)+1) for letter in x)))\n\n# Option 3: Split 'ord_5' into two new columns using the indices of two letters in string.ascii_letters, separately\ntrain['ord_5_oe1'] = train['ord_5'].apply(lambda x:(string.ascii_letters.find(x[0])+1))\ntest['ord_5_oe1'] = test['ord_5'].apply(lambda x:(string.ascii_letters.find(x[0])+1))\n\ntrain['ord_5_oe2'] = train['ord_5'].apply(lambda x:(string.ascii_letters.find(x[1])+1))\ntest['ord_5_oe2'] = test['ord_5'].apply(lambda x:(string.ascii_letters.find(x[1])+1))\n\nfor col in ['ord_5_oe1', 'ord_5_oe2', 'ord_5_oe_add', 'ord_5_oe_join']:\n    train[col]= train[col].astype('float64')\n    test[col]= test[col].astype('float64')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <span id=\"6\"></span> ** High Cardinality Features (Target Encoding) **"},{"metadata":{},"cell_type":"markdown","source":"These features have tooooooo many uniqe values. Let's recap:\n- <b> nom_5 : </b> 222 uniques\n- <b> nom_6 : </b> 522 uniques\n- <b> nom_7 : </b> 1220 uniques\n- <b> nom_8 : </b> 2215 uniques\n- <b> nom_9 : </b> 11981 uniques\n- <b> ord_5 : </b> 192 uniques"},{"metadata":{"trusted":true},"cell_type":"code","source":"high_card = ['nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9', 'ord_5']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from category_encoders.target_encoder import TargetEncoder\nte = TargetEncoder()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[high_card] = te.fit_transform(X = train[high_card], y = train['target'])\ntest[high_card] = te.transform(X = test[high_card])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That's all! Here is the result:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <span id=\"7\"></span> ** 4. Model **"},{"metadata":{},"cell_type":"markdown","source":"### <span id=\"8\"></span> ** LightGBM **"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.drop(['id','target'], axis = 1)\ny = train['target']\nX_test = test.drop('id', axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"X:\", X.shape)\nprint(\"y:\", y.shape)\nprint(\"X_test:\", X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    'bagging_fraction': 0.52,\n    'boosting': 'goss',\n    'feature_fraction': 0.5,\n    'lambda_l1': 3.135897735211238,\n    'lambda_l2': 1.8097983046367754,\n    'learning_rate': 0.024895196236388753,\n    'max_bin': 64,\n    'max_depth': 2,\n    'metric': 'auc',\n    'min_data_in_bin': 176,\n    'min_data_in_leaf': 144,\n    'min_gain_to_split': 4.97,\n    'num_leaves': 1393,\n    'objective': 'binary',\n    'other_rate': 0.4399622643268988,\n    'top_rate': 0.1919072599467846,\n    'is_unbalance': True,\n    'random_state': 42\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nNFOLDS = 5\nfolds = KFold(n_splits=NFOLDS)\n\ncolumns = X.columns\nsplits = folds.split(X, y)\ny_preds = np.zeros(X_test.shape[0])\ny_oof = np.zeros(X.shape[0])\nscore = 0\n\nfeature_importances = pd.DataFrame()\nfeature_importances['feature'] = columns\n  \nfor fold_n, (train_index, valid_index) in enumerate(splits):\n    X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n    \n    dtrain = lgb.Dataset(X_train, label=y_train)\n    dvalid = lgb.Dataset(X_valid, label=y_valid)\n\n    clf = lgb.train(params, dtrain, 10000, valid_sets = [dtrain, dvalid], verbose_eval=200, early_stopping_rounds=500)\n    \n    feature_importances[f'fold_{fold_n + 1}'] = clf.feature_importance()\n    \n    y_pred_valid = clf.predict(X_valid)\n    y_oof[valid_index] = y_pred_valid\n    print(f\"Fold {fold_n + 1} | AUC: {roc_auc_score(y_valid, y_pred_valid)}\")\n    \n    score += roc_auc_score(y_valid, y_pred_valid) / NFOLDS\n    y_preds += clf.predict(X_test) / NFOLDS\n    \n    del X_train, X_valid, y_train, y_valid\n    gc.collect()\n    \nprint(f\"\\nMean AUC = {score}\")\nprint(f\"Out of folds AUC = {roc_auc_score(y, y_oof)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <span id=\"9\"></span> ** Feature Importance **"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importances['average'] = feature_importances[[f'fold_{fold_n + 1}' for fold_n in range(folds.n_splits)]].mean(axis=1)\nfeature_importances.to_csv('feature_importances.csv')\n\nplt.figure(figsize=(16, 16))\nsns.barplot(data=feature_importances.sort_values(by='average', ascending=False).head(50), x='average', y='feature');\nplt.title('50 TOP feature importance over {} folds average'.format(folds.n_splits));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <span id=\"10\"></span> ** 5. Submission **"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"submission = pd.read_csv('/kaggle/input/cat-in-the-dat/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission['target'] = y_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I have tried to apply different encoding methods and public kernels helped a lot while building this.\n\n<b>Please, <font color=\"red\">Don't forget to </font></b> <b><font color=\"green\">UPVOTE </font></b> if you liked this kernel, thank you. üôÇüëç"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}