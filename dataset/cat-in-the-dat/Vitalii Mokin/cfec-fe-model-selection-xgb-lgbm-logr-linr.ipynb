{"cells":[{"metadata":{},"cell_type":"markdown","source":"## This kernel basic on the kernels:\n\n* [Logistic Regression](https://www.kaggle.com/martin1234567890/logistic-regression)\n\n* [Feature importance - xgb, lgbm, logreg, linreg](https://www.kaggle.com/vbmokin/feature-importance-xgb-lgbm-logreg-linreg)\n\n* [One-Hot, Stratified, Logistic Regression](https://www.kaggle.com/bustam/one-hot-stratified-logistic-regression)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import roc_auc_score, r2_score\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np \nimport lightgbm as lgbm\nimport xgboost as xgb\nfrom sklearn import preprocessing\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(\"/kaggle/input/cat-in-the-dat/sample_submission.csv\")\ntrain = pd.read_csv(\"/kaggle/input/cat-in-the-dat/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/cat-in-the-dat/test.csv\")\n\nlabels = train.pop('target')\nlabels = labels.values\ntrain_id = train.pop(\"id\")\ntest_id = test.pop(\"id\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Thanks to https://www.kaggle.com/bustam/one-hot-stratified-logistic-regression\ndef ord_to_num(df, col):\n    keys=np.sort(df[col].unique())\n    values=np.arange(len(keys))\n    map = dict(zip(keys, values))\n    df[col] = df[col].replace(map)\nord_to_num(train, 'ord_4')\nord_to_num(test,'ord_4')\nord_to_num(train, 'ord_5')\nord_to_num(test,'ord_5')\n\n# ord_4\ntrain['ord_4_band'] = pd.qcut(train['ord_4'], 6)\nbands=train.ord_4_band.unique()\nkeys_bands=np.sort(bands)\nvalues_bands=np.arange(len(keys_bands))\nmap_bands = dict(zip(keys_bands, values_bands))\ntrain['ord_4_band'] = train['ord_4_band'].replace(map_bands)\ntest['ord_4_band']=pd.cut(test.ord_4,pd.IntervalIndex(keys_bands))\ntest['ord_4_band'] = test['ord_4_band'].replace(map_bands)\n\n# ord_5\ntrain['ord_5_band'] = pd.qcut(train['ord_5'], 6)\nbands=train.ord_5_band.unique()\nkeys_bands=np.sort(bands)\nvalues_bands=np.arange(len(keys_bands))\nmap_bands = dict(zip(keys_bands, values_bands))\ntrain['ord_5_band'] = train['ord_5_band'].replace(map_bands)\ntest['ord_5_band']=pd.cut(test.ord_5,pd.IntervalIndex(keys_bands))\ntest['ord_5_band'] = test['ord_5_band'].replace(map_bands)\n\n# \"nom_7\", \"nom_8\", \"nom_9\" - x for values is absent in both train and test\nfor col in [\"nom_7\", \"nom_8\", \"nom_9\"]:\n    train_vals = set(train[col].unique())\n    test_vals = set(test[col].unique())\n   \n    ex=train_vals ^ test_vals\n    if ex:\n        train.loc[train[col].isin(ex), col]=\"x\"\n        test.loc[test[col].isin(ex), col]=\"x\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preprocessing\nacc = []\ndata = pd.concat([train, test])\n# Determination categorical features\nnumerics = ['int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']\ncategorical_columns = []\nfeatures = data.columns.values.tolist()\nfor col in features:\n    if data[col].dtype in numerics: continue\n    categorical_columns.append(col)\n    \n# Encoding categorical features\ndata2 = data\nfor col in categorical_columns:\n    if col in data.columns:\n        le = LabelEncoder()\n        le.fit(list(data[col].astype(str).values))\n        data2[col] = le.transform(list(data[col].astype(str).values))\ntrain = data2.iloc[:train.shape[0], :]\ntest = data2.iloc[train.shape[0]:, :]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA & FE by the XGB, LGBM, Logistic Regression and Linear Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Building the feature importance diagrams and prediction by 4 models\n# Thanks to https://www.kaggle.com/vbmokin/feature-importance-xgb-lgbm-logreg-linreg\n  \n# LGBM\n#%% split training set to validation set\nXtrain, Xval, Ztrain, Zval = train_test_split(train, labels, test_size=0.2, random_state=0)\ntrain_set = lgbm.Dataset(Xtrain, Ztrain, silent=False)\nvalid_set = lgbm.Dataset(Xval, Zval, silent=False)\nparams = {\n        'boosting_type':'gbdt',\n        'objective': 'regression',\n        'num_leaves': 31,\n        'learning_rate': 0.05,\n        'max_depth': -1,\n        'subsample': 0.8,\n        'bagging_fraction' : 1,\n        'max_bin' : 5000 ,\n        'bagging_freq': 20,\n        'colsample_bytree': 0.6,\n        'metric': 'rmse',\n        'min_split_gain': 0.5,\n        'min_child_weight': 1,\n        'min_child_samples': 10,\n        'scale_pos_weight':1,\n        'zero_as_missing': True,\n        'seed':0,        \n    }\n\nmodelL = lgbm.train(params, train_set = train_set, num_boost_round=1000,\n                   early_stopping_rounds=50, verbose_eval=50, valid_sets=valid_set)\nfeature_score = pd.DataFrame(train.columns, columns = ['feature'])\npred_train = pd.DataFrame()\npred_test = pd.DataFrame()\nfeature_score['score_lgb'] = modelL.feature_importance()\npred_train['lgb_pred'] = modelL.predict(train)\npred_test['lgb_pred'] = modelL.predict(test)\nacc.append(round(r2_score(labels, pred_train['lgb_pred']) * 100, 2))\n\n#Plot FI\nfig =  plt.figure(figsize = (15,15))\naxes = fig.add_subplot(111)\nlgbm.plot_importance(modelL,ax = axes,height = 0.5)\nplt.show();plt.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#XGBM\n#%% split training set to validation set \ndata_tr  = xgb.DMatrix(Xtrain, label=Ztrain)\ndata_cv  = xgb.DMatrix(Xval   , label=Zval)\ndata_train = xgb.DMatrix(train)\ndata_test = xgb.DMatrix(test)\nevallist = [(data_tr, 'train'), (data_cv, 'valid')]\nparms = {'max_depth':8, #maximum depth of a tree\n         'objective':'reg:logistic',\n         'eta'      :0.3,\n         'subsample':0.8,#SGD will use this percentage of data\n         'lambda '  :4, #L2 regularization term,>1 more conservative \n         'colsample_bytree ':0.9,\n         'colsample_bylevel':1,\n         'min_child_weight': 10}\nmodelx = xgb.train(parms, data_tr, num_boost_round=200, evals = evallist,\n                  early_stopping_rounds=30, maximize=False, \n                  verbose_eval=10)\nfeature_score['score_xgb'] = feature_score['feature'].map(modelx.get_score(importance_type='weight'))\npred_train['xgb_pred'] = modelx.predict(data_train)\npred_test['xgb_pred'] = modelx.predict(data_test)\nacc.append(round(r2_score(labels, pred_train['xgb_pred']) * 100, 2))\n\n#Plot FI\nfig =  plt.figure(figsize = (15,15))\naxes = fig.add_subplot(111)\nxgb.plot_importance(modelx,ax = axes,height = 0.5)\nplt.show();plt.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Regression models\n# Standardization for regression models\ntrain2 = pd.DataFrame(preprocessing.MinMaxScaler().fit_transform(train),\n    columns=train.columns, index=train.index)\n# Logistic Regression\nlogreg = LogisticRegression()\nlogreg.fit(train2, labels)\ncoeff_logreg = pd.DataFrame(train2.columns.delete(0))\ncoeff_logreg.columns = ['feature']\ncoeff_logreg[\"score_logreg\"] = pd.Series(logreg.coef_[0])\npred_train['log_pred'] = logreg.predict(train)\npred_test['log_pred'] = logreg.predict(test)\nacc.append(round(r2_score(labels, pred_train['log_pred']) * 100, 2))\ncoeff_logreg.sort_values(by='score_logreg', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coeff_logreg[\"score_logreg\"] = coeff_logreg[\"score_logreg\"].abs() # the level of importance of features is not associated with the sign\nfeature_score = pd.merge(feature_score, coeff_logreg, on='feature')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Linear Regression\nlinreg = LinearRegression()\nlinreg.fit(train2, labels)\ncoeff_linreg = pd.DataFrame(train2.columns.delete(0))\ncoeff_linreg.columns = ['feature']\ncoeff_linreg[\"score_linreg\"] = pd.Series(linreg.coef_)\npred_train['lin_pred'] = linreg.predict(train)\npred_test['lin_pred'] = linreg.predict(test)\nacc.append(round(r2_score(labels, pred_train['lin_pred']) * 100, 2))\ncoeff_linreg.sort_values(by='score_linreg', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coeff_linreg[\"score_linreg\"] = coeff_linreg[\"score_linreg\"].abs() # the level of importance of features is not associated with the sign\nfeature_score = pd.merge(feature_score, coeff_linreg, on='feature')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Comparison of the all feature importance diagrams\nfeature_score = feature_score.fillna(0)\nfeature_score = feature_score.set_index('feature')\n\n# MinMax scale all importances\nfeature_score = pd.DataFrame(preprocessing.MinMaxScaler().fit_transform(feature_score),\n    columns=feature_score.columns, index=feature_score.index)\n\n# Create mean column\nfeature_score['mean'] = feature_score.mean(axis=1)\npred_train['mean'] = pred_train.mean(axis=1)\npred_test['mean'] = pred_test.mean(axis=1)\nacc.append(round(r2_score(labels, pred_train['mean']) * 100, 2))\n\n# Create total column with different weights\nfL = 0.2\nflog = 0.5\nflin = 0.15\nfx = 1-fL-flog-flin\nfeature_score['total'] = fL*feature_score['score_lgb'] + fx*feature_score['score_xgb'] \\\n                       + flog*feature_score['score_logreg'] + flin*feature_score['score_linreg']\npred_train['total'] = fL*pred_train['lgb_pred'] + fx*pred_train['xgb_pred'] + \\\n                    + flog*pred_train['log_pred'] + flin*pred_train['lin_pred']\npred_test['total'] = fL*pred_test['lgb_pred'] + fx*pred_test['xgb_pred'] + \\\n                    + flog*pred_test['log_pred'] + flin*pred_test['lin_pred']\nacc.append(round(r2_score(labels, pred_train['total']) * 100, 2))\n\n# Plot the feature importances\nfeature_score.sort_values('total', ascending=False).plot(kind='bar', figsize=(20, 10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_score.sort_values('score_logreg', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# to_drop = feature_score[(feature_score['score_logreg'] < 0.0025)].index.tolist()\n# to_drop","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data = data.drop(to_drop,axis=1)\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Apply Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = [i for i in data.columns]\n\ndummies = pd.get_dummies(data,\n                         columns=columns,\n                         drop_first=True,\n                         sparse=True)\n\ndel data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = dummies.iloc[:train.shape[0], :]\ntest = dummies.iloc[train.shape[0]:, :]\n\ndel dummies","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.sparse.to_coo().tocsr()\ntest = test.sparse.to_coo().tocsr()\n\ntrain = train.astype(\"float32\")\ntest = test.astype(\"float32\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression(C=0.12,\n                        solver=\"lbfgs\",\n                        tol=0.002,\n                        max_iter=10000)\n\nlr.fit(train, labels)\n\nlr_pred = lr.predict_proba(train)[:, 1]\nscore = roc_auc_score(labels, lr_pred)\n\nprint(\"score: \", score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission[\"id\"] = test_id\nsubmission[\"target\"] = lr.predict_proba(test)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}