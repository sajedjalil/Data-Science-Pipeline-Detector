{"cells":[{"metadata":{},"cell_type":"markdown","source":"This Playground competition will give you the opportunity to try different encoding schemes for different algorithms to compare how they perform.This competition mainly deals with encoding"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom catboost import CatBoostClassifier, Pool\nfrom catboost import CatBoostClassifier\nimport os\nfrom catboost import cv\nimport shap\n# load JS visualization code to notebook\nshap.initjs()\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"submission = pd.read_csv(\"../input/cat-in-the-dat/sample_submission.csv\")\ntrain = pd.read_csv(\"../input/cat-in-the-dat/train.csv\")\ntest = pd.read_csv(\"../input/cat-in-the-dat/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"##Data Prepare \n\n# dictionary to map the feature\nbin_dict = {'T':1, 'F':0, 'Y':1, 'N':0}\n\n# Maping the category values in our dict\ntrain['bin_3'] = train['bin_3'].map(bin_dict)\ntrain['bin_4'] = train['bin_4'].map(bin_dict)\ntest['bin_3'] = test['bin_3'].map(bin_dict)\ntest['bin_4'] = test['bin_4'].map(bin_dict)\n\n\n\ndummies = pd.concat([train, test], axis=0, sort=False)\nprint(f'Shape before dummy transformation: {dummies.shape}')\ndummies = pd.get_dummies(dummies, columns=['nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4'],\\\n                          prefix=['nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4'], drop_first=True)\nprint(f'Shape after dummy transformation: {dummies.shape}')\ntrain, test = dummies.iloc[:train.shape[0], :], dummies.iloc[train.shape[0]:, :]\ndel dummies\n#print(train.shape)\n#print(test.shape)\n\n\n# Importing categorical options of pandas\nfrom pandas.api.types import CategoricalDtype \n\n# seting the orders of our ordinal features\nord_1 = CategoricalDtype(categories=['Novice', 'Contributor','Expert', \n                                     'Master', 'Grandmaster'], ordered=True)\nord_2 = CategoricalDtype(categories=['Freezing', 'Cold', 'Warm', 'Hot',\n                                     'Boiling Hot', 'Lava Hot'], ordered=True)\nord_3 = CategoricalDtype(categories=['a', 'b', 'c', 'd', 'e', 'f', 'g',\n                                     'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o'], ordered=True)\nord_4 = CategoricalDtype(categories=['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I',\n                                     'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R',\n                                     'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'], ordered=True)\ndef transformingOrdinalFeatures(df, ord_1, ord_2, ord_3, ord_4):    \n    df.ord_1 = df.ord_1.astype(ord_1)\n    df.ord_2 = df.ord_2.astype(ord_2)\n    df.ord_3 = df.ord_3.astype(ord_3)\n    df.ord_4 = df.ord_4.astype(ord_4)\n\n    # Geting the codes of ordinal categoy's \n    df.ord_1 = df.ord_1.cat.codes\n    df.ord_2 = df.ord_2.cat.codes\n    df.ord_3 = df.ord_3.cat.codes\n    df.ord_4 = df.ord_4.cat.codes\n    \n    return df\ntrain = transformingOrdinalFeatures(train, ord_1, ord_2, ord_3, ord_4)\ntest = transformingOrdinalFeatures(test, ord_1, ord_2, ord_3, ord_4)\n\n#print(train.shape)\n#print(test.shape)\n\ndef date_cyc_enc(df, col, max_vals):\n    df[col + '_sin'] = np.sin(2 * np.pi * df[col]/max_vals)\n    df[col + '_cos'] = np.cos(2 * np.pi * df[col]/max_vals)\n    return df\n\ntrain = date_cyc_enc(train, 'day', 7)\ntest = date_cyc_enc(test, 'day', 7) \n\ntrain = date_cyc_enc(train, 'month', 12)\ntest = date_cyc_enc(test, 'month', 12)\n\n#print(train.shape)\n#print(test.shape)\n\n\nimport string\n\n# Then encode 'ord_5' using ACSII values\n\n# Option 1: Add up the indices of two letters in string.ascii_letters\ntrain['ord_5_oe_add'] = train['ord_5'].apply(lambda x:sum([(string.ascii_letters.find(letter)+1) for letter in x]))\ntest['ord_5_oe_add'] = test['ord_5'].apply(lambda x:sum([(string.ascii_letters.find(letter)+1) for letter in x]))\n\n# Option 2: Join the indices of two letters in string.ascii_letters\ntrain['ord_5_oe_join'] = train['ord_5'].apply(lambda x:float(''.join(str(string.ascii_letters.find(letter)+1) for letter in x)))\ntest['ord_5_oe_join'] = test['ord_5'].apply(lambda x:float(''.join(str(string.ascii_letters.find(letter)+1) for letter in x)))\n\n# Option 3: Split 'ord_5' into two new columns using the indices of two letters in string.ascii_letters, separately\ntrain['ord_5_oe1'] = train['ord_5'].apply(lambda x:(string.ascii_letters.find(x[0])+1))\ntest['ord_5_oe1'] = test['ord_5'].apply(lambda x:(string.ascii_letters.find(x[0])+1))\n\ntrain['ord_5_oe2'] = train['ord_5'].apply(lambda x:(string.ascii_letters.find(x[1])+1))\ntest['ord_5_oe2'] = test['ord_5'].apply(lambda x:(string.ascii_letters.find(x[1])+1))\n\nfor col in ['ord_5_oe1', 'ord_5_oe2', 'ord_5_oe_add', 'ord_5_oe_join']:\n    train[col]= train[col].astype('float64')\n    test[col]= test[col].astype('float64')\n\n#print(train.shape)\n#print(test.shape)\n\nfrom sklearn.preprocessing import LabelEncoder\n\nhigh_card_feats = ['nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9']\n\nfor col in high_card_feats:    \n    train[f'hash_{col}'] = train[col].apply( lambda x: hash(str(x)) % 5000 )\n    test[f'hash_{col}'] = test[col].apply( lambda x: hash(str(x)) % 5000 )\n                                               \n    if train[col].dtype == 'object' or test[col].dtype == 'object': \n        lbl = LabelEncoder()\n        lbl.fit(list(train[col].values) + list(test[col].values))\n        train[f'le_{col}'] = lbl.transform(list(train[col].values))\n        test[f'le_{col}'] = lbl.transform(list(test[col].values))\n\n#print(train.shape)\n#print(test.shape)\n\n\ncol = high_card_feats + ['ord_5','day', 'month']\n                    # + ['hash_nom_6', 'hash_nom_7', 'hash_nom_8', 'hash_nom_9']\n                    # + ['le_nom_5', 'le_nom_6', 'le_nom_7', 'le_nom_8', 'le_nom_9']\ntrain.drop(col, axis=1, inplace=True)\ntest.drop(col, axis=1, inplace=True)\n\n#print(train.shape)\n#print(test.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['target'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Objective Function\n### Logloss - Binary traget\n### CrossEntropy - Target probabilities "},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train.drop('target', axis=1).values\ny_train = train['target'].values\nX_test = test.values\n\nX_train_part, X_valid, y_train_part, y_valid = train_test_split(X_train, y_train, \n                                                                test_size=0.4, \n                                                                random_state=17,stratify=y_train)\n\nSEED = 17\n\nmodel = CatBoostClassifier (iterations=500,learning_rate=0.5, custom_loss=['AUC','Accuracy'],\n                            early_stopping_rounds=100,eval_metric='AUC')\n\nmodel.fit(X_train_part, y_train_part,\n        eval_set=(X_valid, y_valid),\n        use_best_model=False,\n        verbose=20,\n        plot=True,early_stopping_rounds=20);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Tree Count wiht early stopping \", (model.tree_count_))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Cross Validation "},{"metadata":{"trusted":true},"cell_type":"code","source":"train_pool = Pool(data=X_train,label=y_train)\nvalidation_pool = Pool(data=X_valid, label=y_valid)\n\nparams = {'loss_function' : 'Logloss',\n         'iterations': 100,\n         'custom_loss': ['AUC','Accuracy'],\n         'learning_rate': 0.5,\n         }\n\n\ncv_data = cv(params= params,\n             pool= train_pool,\n             shuffle=True,\n             fold_count=5,\n             partition_random_seed=0,\n             verbose=False,\n             plot=True,)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_value = np.min(cv_data['test-Logloss-mean'])\nbest_iter = np.argmin(cv_data['test-Logloss-mean'])\n\nprint('Best validation score without stratified: {:.4f} {:.4f} on step {} '\n      .format(best_value, cv_data['test-Logloss-std'][best_iter],best_iter))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Stratified = True"},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_data = cv(params= params,\n             pool= train_pool,\n             shuffle=True,\n             fold_count=5,\n             stratified=True,\n             partition_random_seed=0,\n             verbose=False,\n             plot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_value = np.min(cv_data['test-Logloss-mean'])\nbest_iter = np.argmin(cv_data['test-Logloss-mean'])\n\nprint('Best validation score with stratified: {:.4f} {:.4f} on step {} '\n      .format(best_value, cv_data['test-Logloss-std'][best_iter],best_iter)) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sklearn Grid Search"},{"metadata":{"trusted":true},"cell_type":"code","source":"'''from sklearn.model_selection import GridSearchCV\n\n\n#param_grid = {\"learning_rate\": [0.001,0.01,0.5],\"depth\":[3,1,2,6,4,5,7,8,9,10]}\n\nparam_grid = {\"depth\":[3,1,2,6,4,5,7,8,9,10],\n          \"iterations\":[50],\n          \"learning_rate\":[0.03,0.001,0.01,0.1,0.2,0.3], \n          \"l2_leaf_reg\":[3,1,5,10,100],\n          \"border_count\":[32,5,10,20,50,100,200],\n          #\"ctr_border_count\":[5,10,20,100,200],\n          \"thread_count\":[4]}\n\n\n\nclf  = CatBoostClassifier(\n            verbose=False,eval_metric='AUC',early_stopping_rounds=100)\n\ngrid_search = GridSearchCV(clf , param_grid=param_grid, cv = 3)\nresults = grid_search.fit(X_train,y_train)\nresults.best_estimator_.get_params()'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Overfiiting Detector with eval metric"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_with_early_stop = CatBoostClassifier(eval_metric='AUC',\n                                          iterations=100,learning_rate=0.5,\n                                          early_stopping_rounds=20)\n\nmodel_with_early_stop.fit(train_pool,eval_set = validation_pool,\n                         verbose = False, plot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(model_with_early_stop.tree_count_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sample Model Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = CatBoostClassifier(iterations=200,learning_rate=0.01)\nmodel.fit(train_pool,verbose=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(model.predict(X_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Probality of being 0 or 1\nprint(model.predict_proba(X_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_pred = model.predict(\n    X_test, prediction_type='RawFormulaVal'\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(raw_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Metric evaluation on a new dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics = model.eval_metrics(data= validation_pool,\n                            metrics = ['Logloss','AUC'],\n                            ntree_start = 0,\n                            ntree_end = 0,\n                            eval_period =1,\n                            plot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Hyperparameter tunning"},{"metadata":{"trusted":true},"cell_type":"code","source":"tunned_model = CatBoostClassifier(eval_metric='AUC',\niterations=500,\nlearning_rate=0.03,\ndepth=6,\nl2_leaf_reg=3,\nrandom_strength=1,\nbagging_temperature=1)\n\ntunned_model.fit(\ntrain_pool,verbose=False, eval_set = validation_pool,plot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_valid_predict = tunned_model.predict_proba(X_valid)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_auc_score(y_valid, model_valid_predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cb_model = CatBoostClassifier(iterations=500,\n                             learning_rate=0.05,\n                             depth=10,\n                             eval_metric='AUC',\n                             random_seed = 42,\n                             bagging_temperature = 0.2,\n                             od_type='Iter',\n                             metric_period = 50,\n                             od_wait=20,use_best_model=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cb_model.fit(\ntrain_pool,verbose=False, eval_set = validation_pool,plot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_valid_predict = cb_model.predict_proba(X_valid)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_auc_score(y_valid, model_valid_predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_test_predict = cb_model.predict(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv(\"../input/cat-in-the-dat/sample_submission.csv\")\nsubmission = pd.DataFrame({'id': sub[\"id\"], 'target': model_test_predict})\nsubmission.to_csv('submission_1.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}