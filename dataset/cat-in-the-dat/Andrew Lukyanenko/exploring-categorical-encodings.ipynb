{"cells":[{"metadata":{},"cell_type":"markdown","source":"## General information\n\nIn this kernel I work with data from Categorical Feature Encoding Challenge.\n\nThis is a playground competition, where all features are categorical.\n\nAs per data description:\n```\nThe data contains binary features (bin_*), nominal features (nom_*), ordinal features (ord_*) as well as (potentially cyclical) day (of the week) and month features. The string ordinal features ord_{3-5} are lexically ordered according to string.ascii_letters.\n```\n\nIn this kernel I'll write EDA and compare various categorical encoders.\n\n**The code for categorical encoding is heavily based on this great medium [article](https://towardsdatascience.com/benchmarking-categorical-encoders-9c322bd77ee8).**\n\n![](https://i.ytimg.com/vi/UCIFOCfYc6w/maxresdefault.jpg)"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"!pip install -U vega_datasets notebook vega","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Importing libraries"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom tqdm import tqdm_notebook\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import NuSVR, SVR\nfrom sklearn.metrics import mean_absolute_error\npd.options.display.precision = 15\n\nimport lightgbm as lgb\nimport xgboost as xgb\nimport time\nimport datetime\nfrom catboost import CatBoostRegressor\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GroupKFold, GridSearchCV, train_test_split, TimeSeriesSplit\nfrom sklearn import metrics\nfrom sklearn import linear_model\nimport gc\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom bayes_opt import BayesianOptimization\nimport eli5\nimport shap\nfrom IPython.display import HTML\nimport json\nimport altair as alt\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom category_encoders.ordinal import OrdinalEncoder\nfrom category_encoders.woe import WOEEncoder\nfrom category_encoders.target_encoder import TargetEncoder\nfrom category_encoders.sum_coding import SumEncoder\nfrom category_encoders.m_estimate import MEstimateEncoder\nfrom category_encoders.backward_difference import BackwardDifferenceEncoder\nfrom category_encoders.leave_one_out import LeaveOneOutEncoder\nfrom category_encoders.helmert import HelmertEncoder\nfrom category_encoders.cat_boost import CatBoostEncoder\nfrom category_encoders.james_stein import JamesSteinEncoder\nfrom category_encoders.one_hot import OneHotEncoder\nfrom typing import List\n\nimport os\nimport time\nimport datetime\nimport json\nimport gc\nfrom numba import jit\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm_notebook\n\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostRegressor, CatBoostClassifier\nfrom sklearn import metrics\n\nfrom itertools import product\n\nimport altair as alt\nfrom altair.vega import v5\nfrom IPython.display import HTML\n\nalt.renderers.enable('notebook')\n\n%env JOBLIB_TEMP_FOLDER=/tmp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import category_encoders\ncategory_encoders.__version__","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Helper functions and classes"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# using ideas from this kernel: https://www.kaggle.com/notslush/altair-visualization-2018-stackoverflow-survey\ndef prepare_altair():\n    \"\"\"\n    Helper function to prepare altair for working.\n    \"\"\"\n\n    vega_url = 'https://cdn.jsdelivr.net/npm/vega@' + v5.SCHEMA_VERSION\n    vega_lib_url = 'https://cdn.jsdelivr.net/npm/vega-lib'\n    vega_lite_url = 'https://cdn.jsdelivr.net/npm/vega-lite@' + alt.SCHEMA_VERSION\n    vega_embed_url = 'https://cdn.jsdelivr.net/npm/vega-embed@3'\n    noext = \"?noext\"\n    \n    paths = {\n        'vega': vega_url + noext,\n        'vega-lib': vega_lib_url + noext,\n        'vega-lite': vega_lite_url + noext,\n        'vega-embed': vega_embed_url + noext\n    }\n    \n    workaround = f\"\"\"    requirejs.config({{\n        baseUrl: 'https://cdn.jsdelivr.net/npm/',\n        paths: {paths}\n    }});\n    \"\"\"\n    \n    return workaround\n    \n\ndef add_autoincrement(render_func):\n    # Keep track of unique <div/> IDs\n    cache = {}\n    def wrapped(chart, id=\"vega-chart\", autoincrement=True):\n        if autoincrement:\n            if id in cache:\n                counter = 1 + cache[id]\n                cache[id] = counter\n            else:\n                cache[id] = 0\n            actual_id = id if cache[id] == 0 else id + '-' + str(cache[id])\n        else:\n            if id not in cache:\n                cache[id] = 0\n            actual_id = id\n        return render_func(chart, id=actual_id)\n    # Cache will stay outside and \n    return wrapped\n           \n\n@add_autoincrement\ndef render(chart, id=\"vega-chart\"):\n    \"\"\"\n    Helper function to plot altair visualizations.\n    \"\"\"\n    chart_str = \"\"\"\n    <div id=\"{id}\"></div><script>\n    require([\"vega-embed\"], function(vg_embed) {{\n        const spec = {chart};     \n        vg_embed(\"#{id}\", spec, {{defaultStyle: true}}).catch(console.warn);\n        console.log(\"anything?\");\n    }});\n    console.log(\"really...anything?\");\n    </script>\n    \"\"\"\n    return HTML(\n        chart_str.format(\n            id=id,\n            chart=json.dumps(chart) if isinstance(chart, dict) else chart.to_json(indent=None)\n        )\n    )\n    \n\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage(deep=True).sum() / 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                c_prec = df[col].apply(lambda x: np.finfo(x).precision).max()\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max and c_prec == np.finfo(np.float32).precision:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df\n    \n\n\ndef prepare_plot_dict(df, col):\n    \"\"\"\n    Preparing dictionary with data for plotting.\n    \n    I want to show how much higher/lower are the target rates for the current column comparing to base values (as described higher),\n    At first I calculate base rates, then for each category in the column I calculate target rate and find difference with the base rates.\n    \n    \"\"\"\n    main_count = train['target'].value_counts(normalize=True).sort_index()\n    main_count = dict(main_count)\n    plot_dict = {}\n    for i in df[col].unique():\n        val_count = dict(df.loc[df[col] == i, 'target'].value_counts().sort_index())\n\n        for k, v in main_count.items():\n            if k in val_count:\n                plot_dict[val_count[k]] = ((val_count[k] / sum(val_count.values())) / main_count[k]) * 100 - 100\n            else:\n                plot_dict[0] = 0\n\n    return plot_dict\n\ndef make_count_plot(df, x, hue='target', title=''):\n    \"\"\"\n    Plotting countplot with correct annotations.\n    \"\"\"\n    g = sns.countplot(x=x, data=df, hue=hue);\n    plt.title(f'Target {title}');\n    ax = g.axes\n\n    plot_dict = prepare_plot_dict(df, x)\n\n    for p in ax.patches:\n        h = p.get_height() if str(p.get_height()) != 'nan' else 0\n        text = f\"{plot_dict[h]:.0f}%\" if plot_dict[h] < 0 else f\"+{plot_dict[h]:.0f}%\"\n        ax.annotate(text, (p.get_x() + p.get_width() / 2., h),\n             ha='center', va='center', fontsize=11, color='green' if plot_dict[h] > 0 else 'red', rotation=0, xytext=(0, 10),\n             textcoords='offset points')  \n\ndef plot_two_graphs(col: str = '', top_n: int = None):\n    \"\"\"\n    Plotting four graphs:\n    - target by variable;\n    - counts of categories in the variable in train and test;\n    \"\"\"\n    data = train.copy()\n    all_data1 = all_data.copy()\n    if top_n:\n        top_cats = list(train[col].value_counts()[:5].index)\n        data = data.loc[data[col].isin(top_cats)]\n        all_data1 = all_data1.loc[all_data1[col].isin(top_cats)]\n        \n    plt.figure(figsize=(20, 12));\n    plt.subplot(2, 2, 1)\n    make_count_plot(df=data, x=col, title=f'and {col}')\n\n    plt.subplot(2, 2, 2)\n    sns.countplot(x='dataset_type', data=all_data1, hue=col);\n    plt.title(f'Count of samples in {col} in train and test data');\n\n    \n@jit\ndef fast_auc(y_true, y_prob):\n    \"\"\"\n    fast roc_auc computation: https://www.kaggle.com/c/microsoft-malware-prediction/discussion/76013\n    \"\"\"\n    y_true = np.asarray(y_true)\n    y_true = y_true[np.argsort(y_prob)]\n    nfalse = 0\n    auc = 0\n    n = len(y_true)\n    for i in range(n):\n        y_i = y_true[i]\n        nfalse += (1 - y_i)\n        auc += y_i * nfalse\n    auc /= (nfalse * (n - nfalse))\n    return auc\n\n\ndef eval_auc(y_true, y_pred):\n    \"\"\"\n    Fast auc eval function for lgb.\n    \"\"\"\n    return 'auc', fast_auc(y_true, y_pred), True\n\n\nclass DoubleValidationEncoderNumerical:\n    \"\"\"\n    Encoder with validation within\n    \"\"\"\n    def __init__(self, cols: List, encoder, folds):\n        \"\"\"\n        :param cols: Categorical columns\n        :param encoder: Encoder class\n        :param folds: Folds to split the data\n        \"\"\"\n        self.cols = cols\n        self.encoder = encoder\n        self.encoders_dict = {}\n        self.folds = folds\n\n    def fit_transform(self, X: pd.DataFrame, y: np.array) -> pd.DataFrame:\n        X = X.reset_index(drop=True)\n        y = y.reset_index(drop=True)\n        for n_fold, (train_idx, val_idx) in enumerate(self.folds.split(X, y)):\n            X_train, X_val = X.loc[train_idx].reset_index(drop=True), X.loc[val_idx].reset_index(drop=True)\n            y_train, y_val = y[train_idx], y[val_idx]\n            _ = self.encoder.fit_transform(X_train, y_train)\n\n            # transform validation part and get all necessary cols\n            val_t = self.encoder.transform(X_val)\n\n            if n_fold == 0:\n                cols_representation = np.zeros((X.shape[0], val_t.shape[1]))\n            \n            self.encoders_dict[n_fold] = self.encoder\n\n            cols_representation[val_idx, :] += val_t.values\n\n        cols_representation = pd.DataFrame(cols_representation, columns=X.columns)\n\n        return cols_representation\n\n    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n        X = X.reset_index(drop=True)\n\n        cols_representation = None\n\n        for encoder in self.encoders_dict.values():\n            test_tr = encoder.transform(X)\n\n            if cols_representation is None:\n                cols_representation = np.zeros(test_tr.shape)\n\n            cols_representation = cols_representation + test_tr / self.folds.n_splits\n\n        cols_representation = pd.DataFrame(cols_representation, columns=X.columns)\n        \n        return cols_representation\n\n\nclass FrequencyEncoder:\n    def __init__(self, cols):\n        self.cols = cols\n        self.counts_dict = None\n\n    def fit(self, X: pd.DataFrame, y=None) -> pd.DataFrame:\n        counts_dict = {}\n        for col in self.cols:\n            values, counts = np.unique(X[col], return_counts=True)\n            counts_dict[col] = dict(zip(values, counts))\n        self.counts_dict = counts_dict\n\n    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n        counts_dict_test = {}\n        res = []\n        for col in self.cols:\n            values, counts = np.unique(X[col], return_counts=True)\n            counts_dict_test[col] = dict(zip(values, counts))\n\n            # if value is in \"train\" keys - replace \"test\" counts with \"train\" counts\n            for k in [key for key in counts_dict_test[col].keys() if key in self.counts_dict[col].keys()]:\n                counts_dict_test[col][k] = self.counts_dict[col][k]\n\n            res.append(X[col].map(counts_dict_test[col]).values.reshape(-1, 1))\n        res = np.hstack(res)\n\n        X[self.cols] = res\n        return X\n\n    def fit_transform(self, X: pd.DataFrame, y=None) -> pd.DataFrame:\n        self.fit(X, y)\n        X = self.transform(X)\n        return X\n    \n\ndef train_model_classification(X, X_test, y, params, folds, model_type='lgb', eval_metric='auc', columns=None, plot_feature_importance=False, model=None,\n                               verbose=10000, early_stopping_rounds=200, n_estimators=50000, splits=None, n_folds=3, averaging='usual', n_jobs=-1, encoder=None, enc_val='single'):\n    \"\"\"\n    A function to train a variety of classification models.\n    Returns dictionary with oof predictions, test predictions, scores and, if necessary, feature importances.\n    \n    :params: X - training data, can be pd.DataFrame or np.ndarray (after normalizing)\n    :params: X_test - test data, can be pd.DataFrame or np.ndarray (after normalizing)\n    :params: y - target\n    :params: folds - folds to split data\n    :params: model_type - type of model to use\n    :params: eval_metric - metric to use\n    :params: columns - columns to use. If None - use all columns\n    :params: plot_feature_importance - whether to plot feature importance of LGB\n    :params: model - sklearn model, works only for \"sklearn\" model type\n    \n    \"\"\"\n    columns = X.columns if columns is None else columns\n    n_splits = folds.n_splits if splits is None else n_folds\n    X_test = X_test[columns]\n    \n    # to set up scoring parameters\n    metrics_dict = {'auc': {'lgb_metric_name': eval_auc,\n                        'catboost_metric_name': 'AUC',\n                        'sklearn_scoring_function': metrics.roc_auc_score},\n                    }\n    \n    result_dict = {}\n    if averaging == 'usual':\n        # out-of-fold predictions on train data\n        oof = np.zeros((len(X), 1))\n\n        # averaged predictions on train data\n        prediction = np.zeros((len(X_test), 1))\n        \n    elif averaging == 'rank':\n        # out-of-fold predictions on train data\n        oof = np.zeros((len(X), 1))\n\n        # averaged predictions on train data\n        prediction = np.zeros((len(X_test), 1))\n\n    \n    # list of scores on folds\n    scores = []\n    feature_importance = pd.DataFrame()\n        \n    # split and train on folds\n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n        if verbose:\n            print(f'Fold {fold_n + 1} started at {time.ctime()}')\n        if type(X) == np.ndarray:\n            X_train, X_valid = X[columns][train_index], X[columns][valid_index]\n            y_train, y_valid = y[train_index], y[valid_index]\n        else:\n            X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n            \n        \n        X_t = X_test.copy()\n            \n        if encoder and enc_val == 'single':\n            X_train = encoder.fit_transform(X_train, y_train)\n            X_valid = encoder.transform(X_valid)\n            X_t = encoder.transform(X_t)\n        elif encoder and enc_val == 'double':\n            encoder_double = DoubleValidationEncoderNumerical(cols=columns, encoder=encoder, folds=folds)\n            X_train = encoder_double.fit_transform(X_train, y_train)\n            X_valid = encoder_double.transform(X_valid)\n            X_t = encoder_double.transform(X_t)\n            \n            \n        if model_type == 'lgb':\n            model = lgb.LGBMClassifier(**params, n_estimators=n_estimators, n_jobs = n_jobs)\n            model.fit(X_train, y_train, \n                    eval_set=[(X_valid, y_valid)], eval_metric=metrics_dict[eval_metric]['lgb_metric_name'],\n                    verbose=verbose, early_stopping_rounds=early_stopping_rounds)\n            \n            y_pred_valid = model.predict_proba(X_valid)[:, 1]\n            y_pred = model.predict_proba(X_t, num_iteration=model.best_iteration_)[:, 1]\n            \n        if model_type == 'xgb':\n            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns)\n            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns)\n\n            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n            model = xgb.train(dtrain=train_data, num_boost_round=n_estimators, evals=watchlist, early_stopping_rounds=early_stopping_rounds, verbose_eval=verbose, params=params)\n            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n            y_pred = model.predict(xgb.DMatrix(X_t, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n        \n        if model_type == 'sklearn':\n            model = model\n            model.fit(X_train, y_train)\n            \n            y_pred_valid = model.predict(X_valid).reshape(-1,)\n            score = metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid)\n            print(f'Fold {fold_n}. {eval_metric}: {score:.4f}.')\n            print('')\n            \n            y_pred = model.predict_proba(X_t)[:, 1]\n        \n        if model_type == 'cat':\n            model = CatBoostClassifier(iterations=n_estimators, eval_metric=metrics_dict[eval_metric]['catboost_metric_name'], **params,\n                                      loss_function='Logloss')\n            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n\n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_t)\n        \n        if averaging == 'usual':\n            \n            oof[valid_index] = y_pred_valid.reshape(-1, 1)\n            scores.append(metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid))\n            \n            prediction += y_pred.reshape(-1, 1)\n\n        elif averaging == 'rank':\n                                  \n            oof[valid_index] = y_pred_valid.reshape(-1, 1)\n            scores.append(metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid))\n                                  \n            prediction += pd.Series(y_pred).rank().values.reshape(-1, 1)        \n        \n        if model_type == 'lgb':\n            # feature importance\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = columns\n            fold_importance[\"importance\"] = model.feature_importances_\n            fold_importance[\"fold\"] = fold_n + 1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n    prediction /= n_splits\n    if verbose:\n        print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    \n    result_dict['oof'] = oof\n    result_dict['prediction'] = prediction\n    result_dict['scores'] = scores\n    \n    if model_type == 'lgb':\n        feature_importance[\"importance\"] /= n_splits\n        cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n            by=\"importance\", ascending=False)[:50].index\n\n        best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n        result_dict['feature_importance'] = feature_importance\n        result_dict['top_columns'] = list(cols)\n        if plot_feature_importance:\n\n            plt.figure(figsize=(16, 12));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGB Features (avg over folds)');\n            \n        \n    return result_dict\n\n# setting up altair\nworkaround = prepare_altair()\nHTML(\"\".join((\n    \"<script>\",\n    workaround,\n    \"</script>\",\n)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data overview"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/cat-in-the-dat/train.csv')\ntest = pd.read_csv('/kaggle/input/cat-in-the-dat/test.csv')\nsample_submission = pd.read_csv('/kaggle/input/cat-in-the-dat/sample_submission.csv')\n\ntrain['dataset_type'] = 'train'\ntest['dataset_type'] = 'test'\nall_data = pd.concat([train, test])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().any().any(), test.isnull().any().any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in train.columns[1:-2]:\n    print(col, train[col].nunique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- we have 23 categorical columns;\n- train dataset is bigger than test dataset (300000 samples vs 200000);\n- there are no missing values;\n- some of nominal columns have a huge cardinality;\n- `ord_5` has quite a lot of unique values;\n\nLet's check whether there are some new categories in test features."},{"metadata":{"trusted":true},"cell_type":"code","source":"set(train['nom_7'].unique()) - set(test['nom_7'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"set(test['nom_8'].unique()) - set(train['nom_8'].unique()), set(train['nom_8'].unique()) - set(test['nom_8'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(set(test['nom_9'].unique()) - set(train['nom_9'].unique())), len(set(train['nom_9'].unique()) - set(test['nom_9'].unique()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are three features with new categories. They will be dealt with."},{"metadata":{"trusted":true},"cell_type":"code","source":"train['target'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Binary columns\n\nIn my visualizations below there are two plots:\n- first one shows counts of `0` and `1` for categories in train features. Also it shows how target rate in categories is different from \"base\" rate - target rate for the whole dataset;\n- second one shows counts of samples for categories in train and test data to compare their distributions.\n\nConsidering some features have high cardinaliry (nominal or ordinal features, I plot only top 5 categories)."},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in [col for col in train.columns if 'bin' in col]:\n    plot_two_graphs(col=col)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`bin_1` feature has different target rates for its values, it could be important."},{"metadata":{},"cell_type":"markdown","source":"## Nominal features"},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in [col for col in train.columns if 'nom' in col]:\n    plot_two_graphs(col=col, top_n=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ordinal features"},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in [col for col in train.columns if 'ord' in col]:\n    plot_two_graphs(col=col, top_n=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Other features"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_two_graphs(col='day')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_two_graphs(col='month')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Basic modelling\n\nLet's train a simple model at first. Will use label encoding and LGBM"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['ord_5_1'] = train['ord_5'].apply(lambda x: x[0])\ntrain['ord_5_2'] = train['ord_5'].apply(lambda x: x[1])\ntest['ord_5_1'] = test['ord_5'].apply(lambda x: x[0])\ntest['ord_5_2'] = test['ord_5'].apply(lambda x: x[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_columns = [col for col in train.columns if col not in ['id', 'target', 'dataset_type']]\nfor col in tqdm_notebook(cat_columns):\n    le = LabelEncoder()\n    le.fit(list(train[col].astype(str).values) + list(test[col].astype(str).values))\n    train[col] = le.transform(list(train[col].astype(str).values))\n    test[col] = le.transform(list(test[col].astype(str).values))   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.drop(['id', 'target', 'dataset_type'], axis=1)\ny = train['target']\nX_test = test.drop(['id', 'dataset_type'], axis=1)\ndel all_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_fold = 5\nfolds = StratifiedKFold(n_splits=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'num_leaves': 256,\n          'min_child_samples': 79,\n          'objective': 'binary',\n          'max_depth': -1,\n          'learning_rate': 0.03,\n          \"boosting_type\": \"gbdt\",\n          \"subsample_freq\": 3,\n          \"subsample\": 1.0,\n          \"bagging_seed\": 11,\n          \"metric\": 'auc',\n          \"verbosity\": -1,\n          'reg_alpha': 0.3,\n          'reg_lambda': 0.3,\n          'colsample_bytree': 1.0\n         }\nresult_dict_lgb = train_model_classification(X=X, X_test=X_test, y=y, params=params, folds=folds, model_type='lgb', eval_metric='auc', plot_feature_importance=True,\n                                                      verbose=500, early_stopping_rounds=200, n_estimators=100, averaging='usual', n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Categorical encoding.\n\nThere are many ways to do categorical encoding. This is a goog library, where many encoders are implemented: http://contrib.scikit-learn.org/categorical-encoding/index.html\n\nI'll do the following: while training models I'll fit a separate encoder on each fold to mininize leakage.\n\nTo compare multiple encoders I'll train them with 100 estimators, to make the process faster."},{"metadata":{"trusted":true},"cell_type":"code","source":"encoders_dict = {}\nencoders = [TargetEncoder(cols=cat_columns), TargetEncoder(cols=cat_columns, min_samples_leaf=5), TargetEncoder(cols=cat_columns, smoothing=5),\n            WOEEncoder(cols=cat_columns), WOEEncoder(cols=cat_columns, sigma=0.01), WOEEncoder(cols=cat_columns, sigma=0.5), WOEEncoder(cols=cat_columns, regularization=0.1), WOEEncoder(cols=cat_columns, regularization=5),\n           CatBoostEncoder(cols=cat_columns), CatBoostEncoder(cols=cat_columns, sigma=0.01), CatBoostEncoder(cols=cat_columns, sigma=0.5)]\nfor i, enc in tqdm_notebook(enumerate(encoders)):\n    encoder = enc\n    result_dict_lgb = train_model_classification(X=X, X_test=X_test, y=y, params=params, folds=folds, model_type='lgb', eval_metric='auc', plot_feature_importance=False,\n                                                      verbose=False, early_stopping_rounds=200, n_estimators=100, averaging='usual', n_jobs=-1,\n                                             encoder=encoder, enc_val='single')\n    encoders_dict[i] = [encoder, result_dict_lgb['scores']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoders_df = pd.DataFrame(encoders_dict.values(), columns=['encoder', 'scores'])\nencoders_df['mean'] = encoders_df['scores'].apply(lambda x: np.mean(x))\nfor i, row in encoders_df.sort_values('mean', ascending=False)[:5].iterrows():\n    print(row['encoder'])\n    print(f\"Mean score: {row['scores']}\")\n    print()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Double validation\n\nNow I'll use the idea of double validation from this article: https://towardsdatascience.com/benchmarking-categorical-encoders-9c322bd77ee8\n\nBasically, after we split data into folds, we split each fold into folds to train separate encoders. This should make the result more stable and better."},{"metadata":{"trusted":true},"cell_type":"code","source":"encoders_dict_double = {}\nfor i, enc in tqdm_notebook(enumerate(encoders)):\n    encoder = enc\n    result_dict_lgb = train_model_classification(X=X, X_test=X_test, y=y, params=params, folds=folds, model_type='lgb', eval_metric='auc', plot_feature_importance=False,\n                                                      verbose=False, early_stopping_rounds=200, n_estimators=100, averaging='usual', n_jobs=-1,\n                                             encoder=encoder, enc_val='double')\n    encoders_dict_double[i] = [encoder, result_dict_lgb['scores']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoders_df = pd.DataFrame(encoders_dict_double.values(), columns=['encoder', 'scores'])\nencoders_df['mean'] = encoders_df['scores'].apply(lambda x: np.mean(x))\nfor i, row in encoders_df.sort_values('mean', ascending=False)[:5].iterrows():\n    print(row['encoder'])\n    print(f\"Mean score: {row['mean']}\")\n    print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder = encoders_df.sort_values('mean', ascending=False)['encoder'].values[0]\nencoder","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predicting"},{"metadata":{"trusted":true},"cell_type":"code","source":"result_dict_lgb = train_model_classification(X=X, X_test=X_test, y=y, params=params, folds=folds, model_type='lgb', eval_metric='auc', plot_feature_importance=True,\n                                                      verbose=500, early_stopping_rounds=200, n_estimators=5000, averaging='usual', n_jobs=-1,\n                                             encoder=encoder, enc_val='double')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission['target'] = result_dict_lgb['prediction']\nsample_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature engineering\n\nLet's create new features as interactions of all features with top 5 features."},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/cat-in-the-dat/train.csv')\ntest = pd.read_csv('/kaggle/input/cat-in-the-dat/test.csv')\n\ntrain['ord_5_1'] = train['ord_5'].apply(lambda x: x[0])\ntrain['ord_5_2'] = train['ord_5'].apply(lambda x: x[1])\ntest['ord_5_1'] = test['ord_5'].apply(lambda x: x[0])\ntest['ord_5_2'] = test['ord_5'].apply(lambda x: x[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_columns = result_dict_lgb['top_columns'][:5]\nnew_cat_columns = []\nnew_cat_columns.extend(cat_columns)\nfor col1 in tqdm_notebook(top_columns):\n    for col2 in cat_columns:\n        if col1 != col2:\n            train[f'{col1}_{col2}'] = train[col1].astype(str) + '_' + train[col2].astype(str)\n            test[f'{col1}_{col2}'] = test[col1].astype(str) + '_' + test[col2].astype(str)\n            new_cat_columns.append(f'{col1}_{col2}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_data(df: pd.DataFrame):\n    data = df.copy()\n    # additional features based on ord_5\n    data['ord_5_1'] = data['ord_5'].apply(lambda x: x[0])\n    data['ord_5_2'] = data['ord_5'].apply(lambda x: x[1])\n    \n    # https://www.kaggle.com/gogo827jz/catboost-baseline-with-feature-importance\n    mapper_ord_1 = {'Novice': 1, \n                'Contributor': 2,\n                'Expert': 3, \n                'Master': 4, \n                'Grandmaster': 5}\n\n    mapper_ord_2 = {'Freezing': 1, \n                    'Cold': 2, \n                    'Warm': 3, \n                    'Hot': 4,\n                    'Boiling Hot': 5, \n                    'Lava Hot': 6}\n\n    mapper_ord_3 = {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, \n                    'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15}\n\n    mapper_ord_4 = {'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7, 'H': 8, \n                    'I': 9, 'J': 10, 'K': 11, 'L': 12, 'M': 13, 'N': 14, 'O': 15,\n                    'P': 16, 'Q': 17, 'R': 18, 'S': 19, 'T': 20, 'U': 21, 'V': 22, \n                    'W': 23, 'X': 24, 'Y': 25, 'Z': 26}\n    \n    for col, mapper in zip(['ord_1', 'ord_2', 'ord_3', 'ord_4'], [mapper_ord_1, mapper_ord_2, mapper_ord_3, mapper_ord_4]):\n        data[col] = data[col].replace(mapper)\n        \n    ord_5 = sorted(list(set(data['ord_5'].values)))\n    ord_5 = dict(zip(ord_5, range(len(ord_5))))\n    data.loc[:, 'ord_5'] = data['ord_5'].apply(lambda x: ord_5[x]).astype(float)\n    \n\n    data['bin_3'] = data['bin_3'].apply(lambda x: 1 if x == 'T' else 0)\n    data['bin_4'] = data['bin_4'].apply(lambda x: 1 if x == 'Y' else 0)\n    \n    # https://www.kaggle.com/avanwyk/encoding-cyclical-features-for-deep-learning\n\n    def date_cyc_enc(df, col, max_vals):\n        df[col + '_sin'] = np.sin(2 * np.pi * df[col]/max_vals)\n        df[col + '_cos'] = np.cos(2 * np.pi * df[col]/max_vals)\n        return df\n\n    data = date_cyc_enc(data, 'day', 7)\n    data = date_cyc_enc(data, 'month', 12)\n    \n    return data\n\ntrain = process_data(train)\ntest = process_data(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in tqdm_notebook(new_cat_columns):\n    le = LabelEncoder()\n    le.fit(list(train[col].astype(str).values) + list(test[col].astype(str).values))\n    train[col] = le.transform(list(train[col].astype(str).values))\n    test[col] = le.transform(list(test[col].astype(str).values))   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.drop(['id', 'target'], axis=1)\ny = train['target']\nX_test = test.drop(['id'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_dict_lgb = train_model_classification(X=X, X_test=X_test, y=y, params=params, folds=folds, model_type='lgb', eval_metric='auc', plot_feature_importance=True,\n                                                      verbose=100, early_stopping_rounds=200, n_estimators=5000, averaging='usual', n_jobs=-1,\n                                             encoder=CatBoostEncoder(cols=new_cat_columns, sigma=0.01), enc_val='double')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission['target'] = result_dict_lgb['prediction']\nsample_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}