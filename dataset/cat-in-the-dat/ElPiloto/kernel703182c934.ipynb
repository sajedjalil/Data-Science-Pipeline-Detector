{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# NOTE: The above was automatically generated by Kaggle when I generated this notebook.  It tells us where Kaggle stores our training and test data.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# *I'm including the Kaggle Data Description here as an accessible reminder of what we should expect to see in the data.*\n## Kaggle Data Description\n\n* In this competition, you will be predicting the probability [0, 1] of a binary target column.\n\n* The data contains binary features (bin_*), nominal features (nom_*), ordinal features (ord_*) as well as (potentially cyclical) day (of the week) and month features. The string ordinal features ord_{3-5} are lexically ordered according to string.ascii_letters.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Let's Inspect what is going on in the csv file by printing out the first 10 rows.\n# You can't load and manipulate data if you're not familiar with its contents, so this is always the first step.\nimport csv\ncsv_file = open('/kaggle/input/cat-in-the-dat/train.csv')\ncsv_reader = csv.reader(csv_file)\nfor _ in range(10):\n    row = next(csv_reader)\n    print(row)\ncsv_file.close()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Notice that csv_reader returns each row as a list of strings, we need to know this when it comes time to actually parse/convert the data. We can see the first row tells us the names of each column, subsequent rows are actual data. Let's read in the column names and first row.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"csv_file = open('/kaggle/input/cat-in-the-dat/train.csv')\ncsv_reader = csv.reader(csv_file)\nheader = next(csv_reader)\nrow = next(csv_reader)\ncsv_file.close()\n\nnum_columns = len(header)\n# print out name and value and type of data\nfor c in range(num_columns):\n    col = header[c]\n    value = row[c]\n    print('======================')\n    print('column name: {}'.format(col))\n    print('value: {}'.format(value))\n    # I'm printing out the type of the value here to emphasize that it csv loads everything as a string\n    # and that we'll have to convert it to the correct data format later if we want to feed this data\n    # into any algorithms.\n    print('type: {}'.format(type(value)))  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Next Steps\n\nThe big picture overview of this challenge is that most machine learning classifiers need numbers are inputs, but data doesn't always come as numbers.  And it turns out it makes a difference how you convert the data into numbers.  So this challenge is about exploring different ways of converting the same data into numbers to find the best way of doing so.  This can be broken into four steps:\n1. Load data into a pandas dataframe (without converting them into numbers).\n2. Convert data in your pandas dataframe to numbers.\n3. Feed your converted data to a machine learning classifier and see its test accuracy.\n4. Modify step 2 and try 3 again to see how your classification performance changes.\n\n**The most important part of data science is keeping a good record of the different things you tried and their outcomes.**\n\n## CSV Data --> Pandas DataFrame\n* This requires reading each row and parsing each column based on the type of column.\n  * The example row we printed above shows that `bin_*` columns can be coded in various ways: some are 0/1, some are `Y/N`, some are `T/F`. You will have to write a function that converts all binary values to python's True/False, regardless of what format it comes in.\n  * Similarly you need to write conversion functions for the rest of the different column types:\n    * nominal (which is like binary, except instead of only taking two values (True/False), it can take a range of values.)\n      * A challenge here is that depending on the specific column, you don't know the range of values it can take until you look through all the unique values in the dataset. Once you do have all the data loaded, `numpy` and `pandas.DataFrame` have `unique()` functions that tell you the unique values in a column or matrix - this should come in handy.\n    * ordinal (which is like nominal except there is an ordering to the values these variables can take)\n    * day/month: which is tricky because December (12th month) is closer to January (1st month) than it is to October (10th month) even though 12 is closer to 10 than it is to 1.\n* Each row in the data has a binary `target` column, which is used when you're training or evaluating the machine learning classifier.  This tells what \"class\" this row belongs to.\n\n    \n## Converting Panda Values To Numbers\n* This is where you get to try different things.\n  * Should binary values be converted to numbers as 0,1 or -0.5, 0.5?  Does it make a difference at all?  Does it make a difference for some classifiers but not others?\n  * If you convert a nominal category as `0, 1, 2`, should you convert a ordinal category also as `0, 1, 2` or would this confuse your classifier into thinking that the order of your nominal values matters just as much as the order of your ordinal values?\n* The output of this step should yield two numpy arrays that can plug very easily into classifier algorithms.\n  * `X`: This should have the shape: `[number_of_rows, number_of_columns`] and contain all of your rows and columns excluding the `target` column.\n  * `Y`: This should have the shape `[number_of_rows]` and this should contain the value of `target` for each row.  **Please Note:** Depending on the classifier you are using, this might need to have shape `[number_of_rows, 1]` or `[number_of_rows, 2]`.\n* You need to run these first two steps for both the training csv file and test csv file, so that ultimately you have: `X_train`, `Y_train`, `X_test`, `Y_test`\n\n## Train and Evaluate Classifiers\nFrom the previous steps you should have something that plugs neatly into this kind of template (try different classifiers of course).\n```\nfrom sklearn.linear_model import LogisticRegression\n# This is your training step\nclf = LogisticRegression(random_state=0).fit(X_train, Y_train)\nclf.score(X_test, Y_test)\n```\n\n## Record and reflect on what you tried in step 2 and the accuracy you got in step 3.  Form a hypothesis on a modification to step 2 that might improve or change your results and repeat the whole process.\n\nData science is really about using the scientific method to learn about your data and how it interacts with the algorithms you feed it into.  Even \"experiments\" where we get low classification accuracy are useful - learning about what does not work is still informative!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Begin Working Area","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Experiments\n1. Hypothesis: Using sin(days) only, not sin(days) cos(days), will lead to worse accuracy using Logistic Regression.\n- Result: Test Accuracy using sin(days): 78.% Test accuracy using sin and cos (days): 88%\n2. Question: what is the effect of removing order information from ordinal columns?\n\n3. Question: which single column, on its own, gives us the highest accuracy?\n- nom_1: Test accuracy 30%\n- nom_2: Test accuracy 31%\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# use easy import\ndf_train=pd.read_csv('../input/cat-in-the-dat/train.csv')\nprint(df_train.head())\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ordinal_columns = {}\nfor c in df_train.columns:\n    if 'ord' in c:\n        ordinal_columns[c] = df_train[c].unique()\nfrom pprint import pprint\npprint(ordinal_columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.keys()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\ndays = np.array(range(1,13))\n\nprint(np.sin(days), np.cos(days))\n\nplt.scatter(np.sin(days), np.cos(days))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}