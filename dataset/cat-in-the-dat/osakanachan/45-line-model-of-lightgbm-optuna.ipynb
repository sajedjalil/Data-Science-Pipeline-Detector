{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Preface\nThis notebook is a continuation of [25-line-model of LightGBM](https://www.kaggle.com/yutanakamura/25-line-model-of-lightgbm).\n\n- With the spirit of ZEN ......\n- The aim is to make LightGBM & Optuna beginners (like me) familiar with them, and to undergo a submission quickly."},{"metadata":{},"cell_type":"markdown","source":"# Code"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport copy\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb\nimport optuna\n\ndef convert(df_input):\n    df = copy.deepcopy(df_input)\n    df.loc[:, 'bin_3':'bin_4'] = df.loc[:, 'bin_3':'bin_4'].applymap(lambda x: 'FTNY'.find(x) % 2)\n    df = pd.get_dummies(df, columns=['nom_{}'.format(i) for i in range(5)])\n    df.loc[:, 'nom_5':'nom_9'] = df.loc[:, 'nom_5':'nom_9'].applymap(lambda x: int(x, 16))\n    df['ord_1'] = df['ord_1'].map(lambda x: ['Novice', 'Contributor', 'Expert', 'Master', 'Grandmaster'].index(x))\n    df['ord_2'] = df['ord_2'].map(lambda x: ['Freezing', 'Cold', 'Warm', 'Hot', 'Boiling Hot', 'Lava Hot'].index(x)) \n    df['ord_3'] = df['ord_3'].map(lambda x: (ord(x) - ord('a')))\n    df['ord_4'] = df['ord_4'].map(lambda x: (ord(x) - ord('A')))\n    df['ord_6'] = df['ord_5'].map(lambda x: (ord(x[1]) - ord('A')))\n    df['ord_5'] = df['ord_5'].map(lambda x: (ord(x[0]) - ord('A')))\n    return df\n\ndf_train, df_test = (convert(pd.read_csv('../input/cat-in-the-dat/train.csv')), convert(pd.read_csv('../input/cat-in-the-dat/test.csv')))\nX_train, X_valid, T_train, T_valid = train_test_split(df_train.drop(['id', 'target'], axis=1), df_train['target'], random_state=0)\nX_test = df_test.drop('id', axis=1)\nparams_fixed = {'objective':'binary', 'metric':'binary_logloss', 'boosting_type':'gbdt', 'num_iterations':10000, 'early_stopping_round':10,\\\n                'max_depth':7, 'max_bin':255, 'reg_alpha':0., 'min_split_gain':0., 'learning_rate':0.01, 'random_state':0}\nmodels = []\n\ndef objective(trial):\n    global params_fixed, models\n    params_tuning = {'num_leaves' : trial.suggest_int('num_leaves', 2, 100), \\\n                     'subsample' : trial.suggest_uniform('subsample', 0.5, 1.0), \\\n                     'subsample_freq' : trial.suggest_int('subsample_freq', 1, 20), \\\n                     'colsample_bytree' : trial.suggest_uniform('colsample_bytree', 0.01, 1.0), \\\n                     'min_child_samples' : trial.suggest_int('min_child_samples', 1, 50), \\\n                     'min_child_weight' : trial.suggest_loguniform('min_child_weight', 1e-3, 1e+1), \\\n                     'reg_lambda' : trial.suggest_loguniform('reg_lambda', 1e-2, 1e+3)}\n    model = lgb.train({**params_fixed, **params_tuning}, lgb.Dataset(X_train, T_train), valid_sets=lgb.Dataset(X_valid, T_valid), verbose_eval=100)\n    models.append(model)\n    score = roc_auc_score(T_valid, model.predict(X_valid))\n    return score\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=10)\npd.concat([df_test['id'], pd.Series(models[study.best_trial.number].predict(X_test), name='target')], axis=1).to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}