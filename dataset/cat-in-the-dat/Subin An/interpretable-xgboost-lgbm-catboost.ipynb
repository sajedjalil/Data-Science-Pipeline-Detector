{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Interpretable XGBoost / LGBM / Catboost\n\nWith SHAP, Let's interpret Tree-based Model.\n\n- XGBoost \n- LightGBM (TBD)\n- Catboost (TBD)\n\n### Reference\n\n- [Interpretable Machine Learning](https://christophm.github.io/interpretable-ml-book/index.html)"},{"metadata":{},"cell_type":"markdown","source":"## Read Files & Library"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%%time\ntrain = pd.read_csv('../input/cat-in-the-dat/train.csv')\ntest = pd.read_csv('../input/cat-in-the-dat/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First of all, I will separate the target value."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"target = train['target']\ntrain_id = train['id']\ntest_id = test['id']\n\ntrain.drop(['target', 'id'], axis=1, inplace=True)\ntest.drop('id', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To make encoding easier, let's connect to concat for a while."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df = pd.concat([train, test], axis=0, sort=False )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Categorical Encoding\n\n\n### Binary Feature (map/apply)\n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"bin_dict = {'T':1, 'F':0, 'Y':1, 'N':0}\ndf['bin_3'] = df['bin_3'].map(bin_dict)\ndf['bin_4'] = df['bin_4'].map(bin_dict)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Nominal Feature (One-Hot Encoding)"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(f'Shape before dummy transformation: {df.shape}')\ndf = pd.get_dummies(df, columns=['nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4'],\n                    prefix=['nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4'], \n                    drop_first=True)\nprint(f'Shape after dummy transformation: {df.shape}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ordinal Feature (~Label Encoding)"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from pandas.api.types import CategoricalDtype \n\nord_1 = CategoricalDtype(categories=['Novice', 'Contributor','Expert', \n                                     'Master', 'Grandmaster'], ordered=True)\nord_2 = CategoricalDtype(categories=['Freezing', 'Cold', 'Warm', 'Hot',\n                                     'Boiling Hot', 'Lava Hot'], ordered=True)\nord_3 = CategoricalDtype(categories=['a', 'b', 'c', 'd', 'e', 'f', 'g',\n                                     'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o'], ordered=True)\nord_4 = CategoricalDtype(categories=['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I',\n                                     'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R',\n                                     'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'], ordered=True)\n\ndf.ord_1 = df.ord_1.astype(ord_1)\ndf.ord_2 = df.ord_2.astype(ord_2)\ndf.ord_3 = df.ord_3.astype(ord_3)\ndf.ord_4 = df.ord_4.astype(ord_4)\n\ndf.ord_1 = df.ord_1.cat.codes\ndf.ord_2 = df.ord_2.cat.codes\ndf.ord_3 = df.ord_3.cat.codes\ndf.ord_4 = df.ord_4.cat.codes\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data (Cycle Encoding)"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def date_cyc_enc(df, col, max_vals):\n    df[col + '_sin'] = np.sin(2 * np.pi * df[col]/max_vals)\n    df[col + '_cos'] = np.cos(2 * np.pi * df[col]/max_vals)\n    return df\n\ndf = date_cyc_enc(df, 'day', 7)\ndf = date_cyc_enc(df, 'month', 12)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ETC (Label Encoding)"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%%time\nfrom sklearn.preprocessing import LabelEncoder\n\n# Label Encoding\nfor f in ['nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9', 'ord_5']:\n    lbl = LabelEncoder()\n    lbl.fit(df[f])\n    df[f'le_{f}'] = lbl.transform(df[f])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Drop 'object' features (Remaining features)"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\ndf.drop(['nom_5','nom_6','nom_7','nom_8','nom_9', 'ord_5'] , axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Reduce Memory Usage & Train_test split"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df = reduce_mem_usage(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train = df[:train.shape[0]]\ntest = df[train.shape[0]:]\n\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Models\n\nFor more parameters, I recommend the following site.\n\n- [Laurae++](https://sites.google.com/view/lauraepp/parameters)"},{"metadata":{},"cell_type":"markdown","source":"### Simple XGBoost\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(\n    train, target, test_size=0.2, random_state=2019\n)\n\nxgb_clf = XGBClassifier(learning_rate=0.05,\n    n_estimators=2000,\n    seed=42,\n    eval_metric='auc',\n)\n\nxgb_clf.fit(\n    X_train, \n    y_train, \n    eval_set=[(X_train, y_train), (X_val, y_val)],\n    early_stopping_rounds=20,\n    verbose=20\n)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = xgb_clf.evals_result()\nepochs = len(results['validation_0']['auc'])\nx_axis = range(0, epochs)\n\n# plot log loss\nplt.figure(figsize=(15, 7))\nplt.plot(x_axis, results['validation_0']['auc'], label='Train')\nplt.plot(x_axis, results['validation_1']['auc'], label='Val')\nplt.legend()\nplt.ylabel('AUC')\nplt.xlabel('# of iterations')\nplt.title('XGBoost AUC')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Importance\n\nYou can extract `Feature Importance` from boosting models\n\n**importance_type**\n\n- `weight` - the number of times a feature is used to split the data across all trees.\n- `gain` - the average gain across all splits the feature is used in.\n- `cover` - the average coverage across all splits the feature is used in.\n- `total_gain` - the total gain across all splits the feature is used in.\n- `total_cover` - the total coverage across all splits the feature is used in."},{"metadata":{},"cell_type":"markdown","source":"### SHAP Value\n\nThe main idea of SHAP value is :\n\n*How does this prediction $i$ change when variable $j$ is removed from this model*\n\n### Tree Explainer\n\n**TreeSHAP**, a variant of SHAP for tree-based machine learning models such as decision trees, random forests and gradient boosted trees. TreeSHAP is fast, computes exact Shapley values, and correctly estimates the Shapley values when features are dependent."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nimport shap\n\nshap.initjs()\n\nexplainer = shap.TreeExplainer(xgb_clf)\nshap_values = explainer.shap_values(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are many plot in `SHAP` package \n\n- summary plot \n- dependence plot\n- force plot\n- decision plot \n- etc\n\nIn this notebook, I will introduce `How to read a SHAP's plot`."},{"metadata":{},"cell_type":"markdown","source":"### SHAP Summary Plot\n\nThe **summary plot** combines feature importance with feature effects. \n\nEach point on the summary plot is a Shapley value for a feature and an instance. "},{"metadata":{},"cell_type":"markdown","source":"**Type 1 : bar plot**"},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.summary_plot(shap_values, train, plot_type=\"bar\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Type 2 : default**"},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.summary_plot(shap_values, train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- `Feature importance`: Variables are ranked in descending order.\n- `Impact`: The horizontal location shows whether the effect of that value is associated with a higher or lower prediction.\n- `Original value`: Color shows whether that variable is high (in red) or low (in blue) for that observation.\n- `Correlation`"},{"metadata":{},"cell_type":"markdown","source":"### SHAP Dependence Plot\n\nSHAP feature dependence might be the simplest global interpretation plot\n\nSHAP **dependence plots** are an alternative to partial dependence plots and accumulated local effects. "},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.dependence_plot(\"le_ord_5\", shap_values, train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.dependence_plot(\"ord_4\", shap_values, train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.dependence_plot(\"day\", shap_values, train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SHAP Force Plot \n\nYou can visualize feature attributions such as Shapley values as \"**forces**\". Each feature value is a force that either increases or decreases the prediction. \nThe prediction starts from the baseline. \nThe baseline for Shapley values is the average of all predictions. \n\nIn the plot, each Shapley value is an arrow that pushes to increase (positive value) or decrease (negative value) the prediction. These forces balance each other out at the actual prediction of the data instance."},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.force_plot(explainer.expected_value, shap_values[0,:], train.iloc[0,:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}