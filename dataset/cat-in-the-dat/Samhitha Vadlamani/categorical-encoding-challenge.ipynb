{"cells":[{"metadata":{},"cell_type":"markdown","source":"Here, I explore different encoding methods of categorical variables. Also the performances of logistic regression and random forest (with latter giving better accuracy). \n\nTypes of categorical variables:\n- Nominal: variables have no numerical importance. E.g: names of people, occupation.\n- Ordinal: variables have ordered ranking. E.g: rating of a movie\n- Binary: Data with only two categories. E.g: True or False\n\n\n1. Label encoding:\nconverts categorical variable to integers. Assumes an order for the categories (suits ordinal variables). Throws error when encounters unencoded variable in the test data that’s not present in train data. But doing combined fit on train+test together might leak test data into training and result in an overfitted model. An alternative could be to drop those rows in data with label not in train data label set\nEx: For example, “red” is 1, “green” is 2, and “blue” is 3.\n\n|       | enc |\n|-------|-----|\n| red   | 1   |\n| green | 2   |\n| blue  | 3   |\n\n\n2. One Hot encoding: \nConverts categorical values to a form where prediction can get better. Why?  Better than label coding because the model doesn’t have to consider higher categorical value to be a better category (suits nominal variables). Creates a separate column for each unique categorical data. Hence is better for categorical columns with low cardinality\nEx:\n\n| red | green | blue |\n|-----|-------|------|\n| 1   | 0     | 0    |\n| 0   | 1     | 0    |\n| 0   | 0     | 1    |\n\n\n3. Binary encoding:\nconvert categorical data to ordinal and the integers are then converted to binary and the binary number is split into columns. This way, fewer columns than in One hot encoding are created\nEx: \n\n|          | bin_0 | bin_1 |\n|----------|-------|-------|\n| red(1)   | 0     | 1     |\n| green(2) | 1     | 0     |\n| blue(3)  | 1     | 1     |\n\n\n4. Target encoding:\nconverting categorical data into a statistical function of true target variables occurrences (such as mean, probability, etc). Since there's a dependency on target itself to predict the target (target leakage), there is a chance of overfitting occuring here. k fold target encoding avoids this.\nEx: (probability)\n\n| Column | Target | encoded |\n|--------|--------|---------|\n| red    | 1      | 2/3     |\n| green  | 0      | 0       |\n| blue   | 1      | 1/2     |\n| red    | 0      | 2/3     |\n| red    | 1      | 2/3     |\n| blue   | 0      | 1/2     |"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# LIBRARIES \nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# DATA\nTRAIN=pd.read_csv('/kaggle/input/cat-in-the-dat/train.csv')\nTEST=pd.read_csv('/kaggle/input/cat-in-the-dat/test.csv')\nlabels=TRAIN[\"target\"]\nX_train=TRAIN.drop(\"target\", axis=1)\ndf=pd.concat([X_train, TEST])\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cat columns \ncat_col=[c for c in df.columns if df[c].dtypes=='object']\nn_levels=df[cat_col].nunique()\nprint(\"cardinality of categorical columns:\\n\",n_levels)\n\nhigh_cardinal=[c for c in cat_col if df[c].nunique()>100]\nlow_cardinal=list(set(cat_col)-set(high_cardinal))\n\n# # LABEL ENCODER (high cardinal)\n# from sklearn.preprocessing import LabelEncoder\n# enc=LabelEncoder()\n# for c in high_cardinal:\n# #     #drop rows from test that have unencoded labels different from train data\n# #     TEST.drop(TEST.loc[~TEST[c].isin(X_train[c])].index,inplace=True)\n# #     X_train=enc.fit_transform(X_train)\n# #     TEST=enc.transform(TEST)\n#     # full data encode since different levels in test\n#     df[c]=enc.fit_transform(df[c])\n\n# # split back to train and test\n# X_train=df.iloc[0:300000,:]\n# TEST=df.iloc[300000:,:]\n\n\n# OHE (low cardinal)\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Produces 1,0 data columns corresponding to all the unique categorical entries in low_cardinal columns list\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_X_train = pd.DataFrame(OH_encoder.fit_transform(X_train[low_cardinal]))\nOH_TEST = pd.DataFrame(OH_encoder.transform(TEST[low_cardinal]))\n\n#OH encoding removes index in the data set. Putting the index back again\nOH_X_train.index = X_train.index\nOH_TEST.index = TEST.index\n\n# Remove low cardinal columns (will replace with one-hot encoding)\nnum_X_train = X_train.drop(low_cardinal, axis=1)\nnum_TEST= TEST.drop(low_cardinal, axis=1)\nprint(num_X_train.shape)\n\n# Add one-hot encoded columns to the original data\nX_train = pd.concat([num_X_train, OH_X_train], axis=1)\nTEST = pd.concat([num_TEST, OH_TEST], axis=1)\n\n\n\n# # BINARY ENCODING \n# from category_encoders import BinaryEncoder\n# encoder = BinaryEncoder(cols=low_cardinal)\n# X_train = encoder.fit_transform(X_train)\n# TEST = encoder.transform(TEST)\n\n# # TARGET ENCODING (high cardinal)\n# from category_encoders import TargetEncoder\n# t_enc = TargetEncoder()\n# X_target=X_train.copy()\n# X_target['target']=labels\n# X_target[high_cardinal] = t_enc.fit_transform(X_target[high_cardinal], X_target['target'])\n# TEST[high_cardinal] = t_enc.transform(X_target[high_cardinal])\n# X_train=X_target.drop(['target'], axis=1)\n\n# K-FOLD TARGET ENCODING (high cardinal)\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.base import clone\nfrom sklearn.model_selection import check_cv, KFold\n\nfrom category_encoders import CatBoostEncoder\n\nclass TargetEncoderCV(BaseEstimator, TransformerMixin):\n\n    def __init__(self, cv, **cbe_params):\n        self.cv = cv\n        self.cbe_params = cbe_params\n\n    @property\n    def _n_splits(self):\n        return check_cv(self.cv).n_splits\n\n    def fit_transform(self, X: pd.DataFrame, y) -> pd.DataFrame:\n        self.cbe_ = []\n        cv = check_cv(self.cv)\n\n        cbe = CatBoostEncoder(\n            cols=X.columns.tolist(),\n            return_df=False,\n            **self.cbe_params\n        )\n\n        X_transformed = np.zeros_like(X, dtype=np.float64)\n        for train_idx, valid_idx in cv.split(X, y):\n            self.cbe_.append(\n                clone(cbe).fit(X.loc[train_idx], y[train_idx])\n            )\n            X_transformed[valid_idx] = self.cbe_[-1].transform(\n                X.loc[valid_idx]\n            )\n\n        return pd.DataFrame(X_transformed, columns=X.columns)\n\n    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n        X_transformed = np.zeros_like(X, dtype=np.float64)\n        for cbe in self.cbe_:\n            X_transformed += cbe.transform(X) / self._n_splits\n        return pd.DataFrame(X_transformed, columns=X.columns)\n\nte_cv = TargetEncoderCV(KFold(n_splits=5))\nX_train = te_cv.fit_transform(X_train, labels)\nTEST = te_cv.transform(TEST)\nprint(X_train.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# VIS \n\n# target variable distribution - bar plot\nlabels.value_counts().sort_index().plot.bar()\n\n#correlation between target and all other variables\nX_train[\"target\"]=labels\ncorr=X_train.corrwith(X_train[\"target\"])\nX_train=X_train.drop(\"target\", axis=1)\n\n# feature selection\nfeat=[c for c in corr.index if np.abs(corr[c])>0.05]\nfeat.remove('target')\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# MODEL\n#Logreg\n# from sklearn.linear_model import LogisticRegression\n# clf=LogisticRegression(C=1, solver=\"lbfgs\", max_iter=5000) \n\n# clf.fit(X_train, labels)\n# pred=clf.predict_proba(TEST)[:,1]\n# print(pred)\n\n#random forest (giving better accuracy than log reg)\nfrom sklearn.ensemble import RandomForestRegressor \nrfr = RandomForestRegressor(n_estimators = 100, random_state = 0) \nrfr.fit(X_train, labels)\npred=rfr.predict(TEST)\nprint(pred.shape)\n\nsubm_df = pd.read_csv('/kaggle/input/cat-in-the-dat/sample_submission.csv')\nsubm_df['target'] = pred\nsubm_df.to_csv('bakaito_submission.csv', index=False)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}