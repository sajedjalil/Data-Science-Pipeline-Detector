{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Welcome to my EDA and Modeling kernel ! \n\nIt's a very cool opportunity to practice and learn from other kagglers about interesting feature encoding techniques and modelling;\n\nI hope you enjoy my work and bring me your feedback. If this kernel is useful for you, please don't forget to <b>upvote</b> the kernel \n\nNOTE: English is not my native language, so sorry for any mistake. \n\n## <font color=\"red\">I'm near of Grandmaster tier, so please, if you find this kernel useful don't forget to UPVOTE!!!!  =) </font>"},{"metadata":{},"cell_type":"markdown","source":"# Competition description:\nIs there a cat in your dat?\n\nA common task in machine learning pipelines is encoding categorical variables for a given algorithm in a format that allows as much useful signal as possible to be captured.\n\nBecause this is such a common task and important skill to master, we've put together a dataset that contains only categorical features, and includes:\n\n- binary features\n- low- and high-cardinality nominal features\n- low- and high-cardinality ordinal features\n- (potentially) cyclical features\n\nThis Playground competition will give you the opportunity to try different encoding schemes for different algorithms to compare how they perform. We encourage you to share what you find with the community.\n\nIf you're not sure how to get started, you can check out the Categorical Variables section of Kaggle's Intermediate Machine Learning course.\n\n\nHave Fun!"},{"metadata":{},"cell_type":"markdown","source":"# Objective:\nI want to do complete exploration to understand the data and after it I will build a Machine Learning Model."},{"metadata":{},"cell_type":"markdown","source":"# Questions:\nBefore I start handling the data, I am thinking about what I want to find here, like:\n- What's the target distribution and what data type it is?\n- What is the type of features we have and the name?! These names can tell us anything?\n- What's the most common values in all cateogory's we have?\n- How many binary features we have?\n- How many missing values?\n- Which type of transformations we need to apply?! <br>\n\nAnd many more questions that will rise through the exploration;"},{"metadata":{},"cell_type":"markdown","source":"## Importing Library's"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport scipy as sp\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Preprocessing, modelling and evaluating\nfrom sklearn import preprocessing\nfrom sklearn.metrics import confusion_matrix, roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score, KFold\nfrom xgboost import XGBClassifier\nimport xgboost as xgb\n\n## Hyperopt modules\nfrom hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK, STATUS_RUNNING\nfrom functools import partial\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n# Any results you write to the current directory are saved as output.\n\nimport gc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Some utilities functions "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def resumetable(df):\n    print(f\"Dataset Shape: {df.shape}\")\n    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])\n    summary = summary.reset_index()\n    summary['Name'] = summary['index']\n    summary = summary[['Name','dtypes']]\n    summary['Missing'] = df.isnull().sum().values    \n    summary['Uniques'] = df.nunique().values\n    summary['First Value'] = df.loc[0].values\n    summary['Second Value'] = df.loc[1].values\n    summary['Third Value'] = df.loc[2].values\n\n    for name in summary['Name'].value_counts().index:\n        summary.loc[summary['Name'] == name, 'Entropy'] = round(stats.entropy(df[name].value_counts(normalize=True), base=2),2) \n\n    return summary\n\n## Function to reduce the DF size\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/cat-in-the-dat/train.csv')\ndf_test = pd.read_csv('/kaggle/input/cat-in-the-dat/test.csv')\nsubmission = pd.read_csv('/kaggle/input/cat-in-the-dat/sample_submission.csv', index_col='id')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Knowing our data"},{"metadata":{"trusted":true},"cell_type":"code","source":"summary = resumetable(df_train)\nsummary","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cool!! As we can note we don't have missing values and it's nice to us. <br>\nI will start exploring the Target and trying to find some patterns that could explain it;"},{"metadata":{},"cell_type":"markdown","source":"# Target Feature\n- Let's see the distribution and if we can identify what is the nature of this feature"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"total = len(df_train)\nplt.figure(figsize=(12,6))\n\ng = sns.countplot(x='target', data=df_train, color='green')\ng.set_title(\"TARGET DISTRIBUTION\", fontsize = 20)\ng.set_xlabel(\"Target Vaues\", fontsize = 15)\ng.set_ylabel(\"Count\", fontsize = 15)\nsizes=[] # Get highest values in y\nfor p in g.patches:\n    height = p.get_height()\n    sizes.append(height)\n    g.text(p.get_x()+p.get_width()/2.,\n            height + 3,\n            '{:1.2f}%'.format(height/total*100),\n            ha=\"center\", fontsize=14) \ng.set_ylim(0, max(sizes) * 1.15) # set y limit based on highest heights\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cool;  \nWe can see that our target is a binary feature and as it is 0 or 1 we can't know what is about.<br>\nAnother interesting thing to note is that isn't so imbalanced:<br>\n- Category 0 with 79.4% <br>\n- Category 1 with 30.6\n\nNow, as we have much of them, let's explore the patterns of other binary features"},{"metadata":{},"cell_type":"markdown","source":"# Binary Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"bin_cols = ['bin_0', 'bin_1', 'bin_2', 'bin_3', 'bin_4']","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#Looking the V's features\nimport matplotlib.gridspec as gridspec # to do the grid of plots\ngrid = gridspec.GridSpec(3, 2) # The grid of chart\nplt.figure(figsize=(16,20)) # size of figure\n\n# loop to get column and the count of plots\nfor n, col in enumerate(df_train[bin_cols]): \n    ax = plt.subplot(grid[n]) # feeding the figure of grid\n    sns.countplot(x=col, data=df_train, hue='target', palette='hls') \n    ax.set_ylabel('Count', fontsize=15) # y axis label\n    ax.set_title(f'{col} Distribution by Target', fontsize=18) # title label\n    ax.set_xlabel(f'{col} values', fontsize=15) # x axis label\n    sizes=[] # Get highest values in y\n    for p in ax.patches: # loop to all objects\n        height = p.get_height()\n        sizes.append(height)\n        ax.text(p.get_x()+p.get_width()/2.,\n                height + 3,\n                '{:1.2f}%'.format(height/total*100),\n                ha=\"center\", fontsize=14) \n    ax.set_ylim(0, max(sizes) * 1.15) # set y limit based on highest heights\n    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can't see any clear pattern to positive values in target. \n- It's interesting to see that in bin_3 the ratio of target true have the same size in both values; (~15%)\n- In the other binary features the pattern is very similar. "},{"metadata":{},"cell_type":"markdown","source":"# Working on binary Features\n- To work in machine learning models, we need numerical features. \n- As we can note, 3 of binary features are already integer features (0 or 1)\n- We will need to work on the features that isn't in numerical representation. \n\nI'm assuming that: \n- T is True and F is False\n- Y is Yes and N is No"},{"metadata":{"trusted":true},"cell_type":"code","source":"# dictionary to map the feature\nbin_dict = {'T':1, 'F':0, 'Y':1, 'N':0}\n\n# Maping the category values in our dict\ndf_train['bin_3'] = df_train['bin_3'].map(bin_dict)\ndf_train['bin_4'] = df_train['bin_4'].map(bin_dict)\ndf_test['bin_3'] = df_test['bin_3'].map(bin_dict)\ndf_test['bin_4'] = df_test['bin_4'].map(bin_dict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Checking the results"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[['bin_3', 'bin_4']].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cool! Now, it's ok to model this feature into a machine learning algorithmn. <br>\nLet's work in the other features"},{"metadata":{},"cell_type":"markdown","source":"# Nominal Features (with more than 2 and less than 15 values)\n- Let's see the distribution of the feature and target Ratio for each value in nominal features"},{"metadata":{"trusted":true},"cell_type":"code","source":"nom_cols = ['nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4']","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def ploting_cat_fet(df, cols, vis_row=5, vis_col=2):\n    \n    grid = gridspec.GridSpec(vis_row,vis_col) # The grid of chart\n    plt.figure(figsize=(17, 35)) # size of figure\n\n    # loop to get column and the count of plots\n    for n, col in enumerate(df_train[cols]): \n        tmp = pd.crosstab(df_train[col], df_train['target'], normalize='index') * 100\n        tmp = tmp.reset_index()\n        tmp.rename(columns={0:'No',1:'Yes'}, inplace=True)\n\n        ax = plt.subplot(grid[n]) # feeding the figure of grid\n        sns.countplot(x=col, data=df_train, order=list(tmp[col].values) , color='green') \n        ax.set_ylabel('Count', fontsize=15) # y axis label\n        ax.set_title(f'{col} Distribution by Target', fontsize=18) # title label\n        ax.set_xlabel(f'{col} values', fontsize=15) # x axis label\n\n        # twinX - to build a second yaxis\n        gt = ax.twinx()\n        gt = sns.pointplot(x=col, y='Yes', data=tmp,\n                           order=list(tmp[col].values),\n                           color='black', legend=False)\n        gt.set_ylim(tmp['Yes'].min()-5,tmp['Yes'].max()*1.1)\n        gt.set_ylabel(\"Target %True(1)\", fontsize=16)\n        sizes=[] # Get highest values in y\n        for p in ax.patches: # loop to all objects\n            height = p.get_height()\n            sizes.append(height)\n            ax.text(p.get_x()+p.get_width()/2.,\n                    height + 3,\n                    '{:1.2f}%'.format(height/total*100),\n                    ha=\"center\", fontsize=14) \n        ax.set_ylim(0, max(sizes) * 1.15) # set y limit based on highest heights\n\n\n    plt.subplots_adjust(hspace = 0.5, wspace=.3)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ploting_cat_fet(df_train, nom_cols, vis_row=5, vis_col=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nice!!! We can see clear different patterns between the nominal category values. \n\nSome summary of these features:\n- NOM_0 - \nRed (~35%) value have the highest % of positive values in the target;\n\n- NOM_1 -\nTriangle(~36%) value have the highest % of positive values in the target;\n\n- NOM_2 -\nHamster(~36%) value have the highest % of positive values in the target;\n\n- NOM_3 -\nIndia(~36%) value have the highest % of positive values in the target;\n\n- NOM_4 -\nTheremin(~36%) value have the highest % of positive values in the target;\n\nDo you noted something?! <br>\nAll the values with highest % of True values on target, are the category's with lowest frequency on the nominal category's. \nWe need to handle with all these features;<br>\nAs it is nominal and don't have so many values by category's we can transform it in dummy features;"},{"metadata":{},"cell_type":"markdown","source":"# Nominal Features Transformation"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test['target'] = 'test'\ndf = pd.concat([df_train, df_test], axis=0, sort=False )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Shape before dummy transformation: {df.shape}')\ndf = pd.get_dummies(df, columns=['nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4'],\\\n                          prefix=['nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4'], drop_first=True)\nprint(f'Shape after dummy transformation: {df.shape}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train, df_test = df[df['target'] != 'test'], df[df['target'] == 'test'].drop('target', axis=1)\ndel df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nice, we can see that it worked; <br>\nNow, let's see the remaining columns with few category values;"},{"metadata":{},"cell_type":"markdown","source":"# Ordinal Features (with more than 2 and less than 15 values)"},{"metadata":{"trusted":true},"cell_type":"code","source":"ord_cols = ['ord_0', 'ord_1', 'ord_2', 'ord_3']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ploting ordinal features"},{"metadata":{"trusted":true},"cell_type":"code","source":"ploting_cat_fet(df_train, ord_cols, vis_row=5, vis_col=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cool! In Ordinal features, the rule of nominal isn't real. <br>\nWe can see that only on ord_0 the highest ratio in target in the less common category;\n\nAs the \"ord_4\" and \"ord_5\" have highest cardinality I will plot it separated\n\n## Ord_4 and ord_5"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['ord_5_ot'] = 'Others'\ndf_train.loc[df_train['ord_5'].isin(df_train['ord_5'].value_counts()[:25].sort_index().index), 'ord_5_ot'] = df_train['ord_5']","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"tmp = pd.crosstab(df_train['ord_4'], df_train['target'], normalize='index') * 100\ntmp = tmp.reset_index()\ntmp.rename(columns={0:'No',1:'Yes'}, inplace=True)\nplt.figure(figsize=(15,12))\n\nplt.subplot(211)\nax = sns.countplot(x='ord_4', data=df_train, order=list(tmp['ord_4'].values) , color='green') \nax.set_ylabel('Count', fontsize=17) # y axis label\nax.set_title('ord_4 Distribution with Target %ratio', fontsize=20) # title label\nax.set_xlabel('ord_4 values', fontsize=17) # x axis label\n# twinX - to build a second yaxis\ngt = ax.twinx()\ngt = sns.pointplot(x='ord_4', y='Yes', data=tmp,\n                   order=list(tmp['ord_4'].values),\n                   color='black', legend=False)\ngt.set_ylim(tmp['Yes'].min()-5,tmp['Yes'].max()*1.1)\ngt.set_ylabel(\"Target %True(1)\", fontsize=16)\n\ntmp = pd.crosstab(df_train['ord_5_ot'], df_train['target'], normalize='index') * 100\ntmp = tmp.reset_index()\ntmp.rename(columns={0:'No',1:'Yes'}, inplace=True)\n\nplt.subplot(212)\nax1 = sns.countplot(x='ord_5_ot', data=df_train,\n                   order=list(df_train['ord_5_ot'].value_counts().sort_index().index) ,\n                   color='green') \nax1.set_ylabel('Count', fontsize=17) # y axis label\nax1.set_title('TOP 25 ord_5 and \"others\" Distribution with Target %ratio', fontsize=20) # title label\nax1.set_xlabel('ord_5 values', fontsize=17) # x axis label\n# twinX - to build a second yaxis\ngt = ax1.twinx()\ngt = sns.pointplot(x='ord_5_ot', y='Yes', data=tmp,\n                   order=list(df_train['ord_5_ot'].value_counts().sort_index().index),\n                   color='black', legend=False)\ngt.set_ylim(tmp['Yes'].min()-5,tmp['Yes'].max()*1.1)\ngt.set_ylabel(\"Target %True(1)\", fontsize=16)\n\nplt.subplots_adjust(hspace = 0.4, wspace=.3)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cool! <br> \n- In ord_4, as it is a ordinal feature, we can see a clearly linear relationship. As it is the alphabet order, it will be easy to encode\n- In ord_5 we have many category's, the top category is:<br>\n1 - \"od\" with 5019 entries, <br>\n2 - \"f0\" with 3435 entries, <br>\n3 - \"Zq\" with 2926<br>\n\nI will explore further the ord_5 Feature. I will explore the distribution of the values and how it is. "},{"metadata":{"trusted":true},"cell_type":"code","source":"ord_5_count = df_train['ord_5'].value_counts().reset_index()['ord_5'].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Distribution of ord_5 features"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,6))\n\ng = sns.distplot(ord_5_count, bins= 50)\ng.set_title(\"Frequency of ord_5 category values\", fontsize=22)\ng.set_xlabel(\"Total of entries in ord_5 category's\", fontsize=18)\ng.set_ylabel(\"Density\", fontsize=18)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cool!<br>\nWe can see that many values in ord_5 have ~2100 entries; <br>\nAlso, the data has many category's with ~100 entries; <br>"},{"metadata":{},"cell_type":"markdown","source":"## Ordinal Features Transformation\n- Only ord_0 is numerical values;\n- We need to transform ord_1, ord_2 and ord_3 to set it in the correctly order to feed the machine learning model;"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing categorical options of pandas\nfrom pandas.api.types import CategoricalDtype \n\n# seting the orders of our ordinal features\nord_1 = CategoricalDtype(categories=['Novice', 'Contributor','Expert', \n                                     'Master', 'Grandmaster'], ordered=True)\nord_2 = CategoricalDtype(categories=['Freezing', 'Cold', 'Warm', 'Hot',\n                                     'Boiling Hot', 'Lava Hot'], ordered=True)\nord_3 = CategoricalDtype(categories=['a', 'b', 'c', 'd', 'e', 'f', 'g',\n                                     'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o'], ordered=True)\nord_4 = CategoricalDtype(categories=['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I',\n                                     'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R',\n                                     'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'], ordered=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transforming ordinal Features\ndf_train.ord_1 = df_train.ord_1.astype(ord_1)\ndf_train.ord_2 = df_train.ord_2.astype(ord_2)\ndf_train.ord_3 = df_train.ord_3.astype(ord_3)\ndf_train.ord_4 = df_train.ord_4.astype(ord_4)\n\n# test dataset\ndf_test.ord_1 = df_test.ord_1.astype(ord_1)\ndf_test.ord_2 = df_test.ord_2.astype(ord_2)\ndf_test.ord_3 = df_test.ord_3.astype(ord_3)\ndf_test.ord_4 = df_test.ord_4.astype(ord_4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.ord_3.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ok, now it's ordered and we can get the codes that corresponds to each value in the nominal feature;\n\n### Geting codes of nominal feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Geting the codes of ordinal categoy's - train\ndf_train.ord_1 = df_train.ord_1.cat.codes\ndf_train.ord_2 = df_train.ord_2.cat.codes\ndf_train.ord_3 = df_train.ord_3.cat.codes\ndf_train.ord_4 = df_train.ord_4.cat.codes\n\n# Geting the codes of ordinal categoy's - test\ndf_test.ord_1 = df_test.ord_1.cat.codes\ndf_test.ord_2 = df_test.ord_2.cat.codes\ndf_test.ord_3 = df_test.ord_3.cat.codes\ndf_test.ord_4 = df_test.ord_4.cat.codes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[['ord_0', 'ord_1', 'ord_2', 'ord_3']].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Date Features\n- Let's see the date distributions\n- What's the target ratio of each value\n- Do we need do any transformation?"},{"metadata":{"trusted":true},"cell_type":"code","source":"date_cols = ['day', 'month']\n\n# Calling the plot function with date columns\nploting_cat_fet(df_train, date_cols, vis_row=5, vis_col=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Curiously, the data have two values that have few entries; \n- In the day column, the value 6 (maybe saturday?!) have less entries;\n- In the month column, the value 6 (maybe the holidays?!) have less entries;\n\nAt this momment, I will not transform it in dummy features, but could be a good solution to test; "},{"metadata":{},"cell_type":"markdown","source":"# Encoding Date features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transfer the cyclical features into two dimensional sin-cos features\n# https://www.kaggle.com/avanwyk/encoding-cyclical-features-for-deep-learning\n\ndef date_cyc_enc(df, col, max_vals):\n    df[col + '_sin'] = np.sin(2 * np.pi * df[col]/max_vals)\n    df[col + '_cos'] = np.cos(2 * np.pi * df[col]/max_vals)\n    return df\n\ndf_train = date_cyc_enc(df_train, 'day', 7)\ndf_test = date_cyc_enc(df_test, 'day', 7) \n\ndf_train = date_cyc_enc(df_train, 'month', 12)\ndf_test = date_cyc_enc(df_test, 'month', 12)\n\n# NOTE, I discovered it on: kaggle.com/gogo827jz/catboost-baseline-with-feature-importance","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ordinal Feature - High Cardinality Features\n- I will start by the ord_5 feature\n- It's an sparse ordinal feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"### Credit of this features to: \n## https://www.kaggle.com/gogo827jz/catboost-baseline-with-feature-importance\n\nimport string\n\n# Then encode 'ord_5' using ACSII values\n\n# Option 1: Add up the indices of two letters in string.ascii_letters\ndf_train['ord_5_oe_add'] = df_train['ord_5'].apply(lambda x:sum([(string.ascii_letters.find(letter)+1) for letter in x]))\ndf_test['ord_5_oe_add'] = df_test['ord_5'].apply(lambda x:sum([(string.ascii_letters.find(letter)+1) for letter in x]))\n\n# Option 2: Join the indices of two letters in string.ascii_letters\ndf_train['ord_5_oe_join'] = df_train['ord_5'].apply(lambda x:float(''.join(str(string.ascii_letters.find(letter)+1) for letter in x)))\ndf_test['ord_5_oe_join'] = df_test['ord_5'].apply(lambda x:float(''.join(str(string.ascii_letters.find(letter)+1) for letter in x)))\n\n# Option 3: Split 'ord_5' into two new columns using the indices of two letters in string.ascii_letters, separately\ndf_train['ord_5_oe1'] = df_train['ord_5'].apply(lambda x:(string.ascii_letters.find(x[0])+1))\ndf_test['ord_5_oe1'] = df_test['ord_5'].apply(lambda x:(string.ascii_letters.find(x[0])+1))\n\ndf_train['ord_5_oe2'] = df_train['ord_5'].apply(lambda x:(string.ascii_letters.find(x[1])+1))\ndf_test['ord_5_oe2'] = df_test['ord_5'].apply(lambda x:(string.ascii_letters.find(x[1])+1))\n\nfor col in ['ord_5_oe1', 'ord_5_oe2', 'ord_5_oe_add', 'ord_5_oe_join']:\n    df_train[col]= df_train[col].astype('float64')\n    df_test[col]= df_test[col].astype('float64')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[['ord_5', 'ord_5_oe_add', 'ord_5_oe_join', 'ord_5_oe1', 'ord_5_oe2']].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cool, now, let's work on nominal features"},{"metadata":{},"cell_type":"markdown","source":"# Nominal Features - High Cardinality Features\n\n## Feature Hasher"},{"metadata":{"trusted":true},"cell_type":"code","source":"high_card_feats = ['nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Nominal Features before hash transformation"},{"metadata":{"trusted":true},"cell_type":"code","source":"resumetable(df_train[high_card_feats])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see high-cardinality in all this columns. The fewer cardinality is 222 values and the highest is 11981 unique values;\n\nTo work with these features, I will try the Hash Trick Solution; Let's see if it give us good results."},{"metadata":{},"cell_type":"markdown","source":"## Hash Trick (Feature Hash)\n- I got this solution from @Giba"},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in high_card_feats:\n    df_train[f'hash_{col}'] = df_train[col].apply( lambda x: hash(str(x)) % 5000 )\n    df_test[f'hash_{col}'] = df_test[col].apply( lambda x: hash(str(x)) % 5000 )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"## Encoding with the Frequency"},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in high_card_feats:\n    enc_nom_1 = (df_train.groupby(col).size()) / len(df_train)\n    df_train[f'freq_{col}'] = df_train[col].apply(lambda x : enc_nom_1[x])\n    #df_test[f'enc_{col}'] = df_test[col].apply(lambda x : enc_nom_1[x])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"## Label Encoder"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n# Label Encoding\nfor f in ['nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9']:\n    if df_train[f].dtype=='object' or df_test[f].dtype=='object': \n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(df_train[f].values) + list(df_test[f].values))\n        df_train[f'le_{f}'] = lbl.transform(list(df_train[f].values))\n        df_test[f'le_{f}'] = lbl.transform(list(df_test[f].values))   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cool <br>\nAfter all transformations done, we can see the summary of the features and what type differences we got "},{"metadata":{"trusted":true},"cell_type":"code","source":"new_feat = ['hash_nom_5', 'hash_nom_6', 'hash_nom_7', 'hash_nom_8',\n            'hash_nom_9',  'freq_nom_5', 'freq_nom_6', 'freq_nom_7', \n            'freq_nom_8', 'freq_nom_9', 'le_nom_5', 'le_nom_6',\n            'le_nom_7', 'le_nom_8', 'le_nom_9']\n\nresumetable(df_train[high_card_feats + new_feat])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Looking nominal features after hash trick transformation"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[['nom_5', 'hash_nom_5', 'freq_nom_5', 'le_nom_5']].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cool! Now, We can run our model. Let's do it."},{"metadata":{},"cell_type":"markdown","source":"# Pre Processing\n- I will keep only the frequency encoding to evaluate the results.<br>\n\nFeel free to change if you would;"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.drop(['ord_5_ot', 'ord_5', \n                'hash_nom_6', 'hash_nom_7', 'hash_nom_8', 'hash_nom_9',\n               #'le_nom_5', 'le_nom_6', 'le_nom_7', 'le_nom_8', 'le_nom_9',\n                'freq_nom_5','freq_nom_6', 'freq_nom_7', 'freq_nom_8', 'freq_nom_9',\n                'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9'\n              ], axis=1, inplace=True)\n\ndf_test.drop(['ord_5',\n              'hash_nom_6', 'hash_nom_7', 'hash_nom_8', 'hash_nom_9', \n              #'le_nom_5', 'le_nom_6', 'le_nom_7', 'le_nom_8', 'le_nom_9',\n              #'freq_nom_5', 'freq_nom_6', 'freq_nom_7', 'freq_nom_8', 'freq_nom_9',\n              'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9',\n              ], axis=1, inplace=True)\n\n#'feq_nom_5', 'feq_nom_6', 'feq_nom_7', 'feq_nom_8', 'feq_nom_9', ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Seting X and Y"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = reduce_mem_usage(df_train)\ndf_test = reduce_mem_usage(df_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = df_train.drop([\"id\",\"target\"], axis=1)\ny_train = df_train[\"target\"]\ny_train = y_train.astype(bool)\nX_test = df_test.drop([\"id\"],axis=1)\n\nX_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Libraries to Machine Learning Modeling"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#Importing the auxiliar and preprocessing librarys \nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.model_selection import train_test_split, KFold, cross_validate\nfrom sklearn.metrics import accuracy_score\n\n#Models\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import RidgeClassifier, SGDClassifier, LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, BaggingClassifier, VotingClassifier, RandomTreesEmbedding","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating pipeline to evaluate different models"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"clfs = []\nseed = 42\n\nclfs.append((\"LogReg\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"LogReg\", LogisticRegression())])))\n\nclfs.append((\"XGBClassifier\", XGBClassifier()))\n\n# clfs.append((\"KNN\", \n#              Pipeline([(\"Scaler\", StandardScaler()),\n#                        (\"KNN\", KNeighborsClassifier(n_neighbors=5))])))\n\nclfs.append((\"DecisionTreeClassifier\", DecisionTreeClassifier()))\n\nclfs.append((\"RandomForestClassifier\", RandomForestClassifier(n_estimators=100)))\n\nclfs.append((\"GradientBoostingClassifier\", GradientBoostingClassifier(n_estimators=100)))\n\nclfs.append((\"RidgeClassifier\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"RidgeClassifier\", RidgeClassifier())])))\n\nclfs.append((\"BaggingClassifier\",\n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"BaggingClassifier\", BaggingClassifier())])))\n\nclfs.append((\"ExtraTreesClassifier\",ExtraTreesClassifier()))\n\n#'neg_mean_absolute_error', 'neg_mean_squared_error','r2'\nscoring = 'roc_auc'\nn_folds = 7\n\nresults, names  = [], [] \n\nfor name, model  in clfs:\n    kfold = KFold(n_splits=n_folds, shuffle=False, random_state=seed)\n    \n    cv_results = cross_val_score(model, \n                                 X_train.values, y_train, \n                                 cv= kfold, scoring=scoring,\n                                 n_jobs=-1)    \n    names.append(name)\n    results.append(cv_results)    \n    msg = \"%s: %f (+/- %f)\" % (name, cv_results.mean(),  \n                               cv_results.std())\n    print(msg)\n    \n# boxplot algorithm comparison\nfig = plt.figure(figsize=(15,6))\nfig.suptitle('Classifier Algorithm Comparison', fontsize=22)\nax = fig.add_subplot(111)\nsns.boxplot(x=names, y=results)\nax.set_xticklabels(names)\nax.set_xlabel(\"Algorithmn\", fontsize=20) \nax.set_ylabel(\"Accuracy of Models\", fontsize=18)\nax.set_xticklabels(ax.get_xticklabels(),rotation=45)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nice! We can see that Xgb is the best model. Let's use it with hyperopt"},{"metadata":{},"cell_type":"markdown","source":"# XGB - HyperOpt Optimization"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nfrom xgboost import plot_importance\nfrom sklearn.metrics import make_scorer\n\nimport time\ndef objective(params):\n    time1 = time.time()\n    params = {\n        'max_depth': int(params['max_depth']),\n        'gamma': \"{:.3f}\".format(params['gamma']),\n        'subsample': \"{:.2f}\".format(params['subsample']),\n        'reg_alpha': \"{:.3f}\".format(params['reg_alpha']),\n        'reg_lambda': \"{:.3f}\".format(params['reg_lambda']),\n        'learning_rate': \"{:.3f}\".format(params['learning_rate']),\n        'num_leaves': '{:.3f}'.format(params['num_leaves']),\n        'colsample_bytree': '{:.3f}'.format(params['colsample_bytree']),\n        'min_child_samples': '{:.3f}'.format(params['min_child_samples']),\n        'feature_fraction': '{:.3f}'.format(params['feature_fraction']),\n        'bagging_fraction': '{:.3f}'.format(params['bagging_fraction'])\n    }\n\n    print(\"\\n############## New Run ################\")\n    print(f\"params = {params}\")\n    FOLDS = 12\n    count=1\n    kf = KFold(n_splits=FOLDS, shuffle=False, random_state=42)\n\n    # tss = TimeSeriesSplit(n_splits=FOLDS)\n    y_preds = np.zeros(submission.shape[0])\n    # y_oof = np.zeros(X_train.shape[0])\n    score_mean = 0\n    for tr_idx, val_idx in kf.split(X_train, y_train):\n        clf = xgb.XGBClassifier(\n            n_estimators=500, random_state=4, \n            verbose=True, \n            tree_method='gpu_hist', \n            **params\n        )\n\n        X_tr, X_vl = X_train.iloc[tr_idx, :], X_train.iloc[val_idx, :]\n        y_tr, y_vl = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n        \n        clf.fit(X_tr, y_tr)\n        #y_pred_train = clf.predict_proba(X_vl)[:,1]\n        #print(y_pred_train)\n        score = make_scorer(roc_auc_score, needs_proba=True)(clf, X_vl, y_vl)\n        # plt.show()\n        score_mean += score\n        print(f'{count} CV - score: {round(score, 4)}')\n        count += 1\n    time2 = time.time() - time1\n    print(f\"Total Time Run: {round(time2 / 60,2)}\")\n    gc.collect()\n    print(f'Mean ROC_AUC: {score_mean / FOLDS}')\n    del X_tr, X_vl, y_tr, y_vl, clf, score\n    \n    return -(score_mean / FOLDS)\n\nspace = {\n    # The maximum depth of a tree, same as GBM.\n    # Used to control over-fitting as higher depth will allow model \n    # to learn relations very specific to a particular sample.\n    # Should be tuned using CV.\n    # Typical values: 3-10\n    'max_depth': hp.quniform('max_depth', 2, 8, 1),\n    \n    # reg_alpha: L1 regularization term. L1 regularization encourages sparsity \n    # (meaning pulling weights to 0). It can be more useful when the objective\n    # is logistic regression since you might need help with feature selection.\n    'reg_alpha':  hp.uniform('reg_alpha', 0.01, 0.4),\n    \n    # reg_lambda: L2 regularization term. L2 encourages smaller weights, this\n    # approach can be more useful in tree-models where zeroing \n    # features might not make much sense.\n    'reg_lambda': hp.uniform('reg_lambda', 0.01, .4),\n    \n    # eta: Analogous to learning rate in GBM\n    # Makes the model more robust by shrinking the weights on each step\n    # Typical final values to be used: 0.01-0.2\n    'learning_rate': hp.uniform('learning_rate', 0.01, 0.15),\n    \n    # colsample_bytree: Similar to max_features in GBM. Denotes the \n    # fraction of columns to be randomly samples for each tree.\n    # Typical values: 0.5-1\n    'colsample_bytree': hp.uniform('colsample_bytree', 0.3, 1),\n    \n    # A node is split only when the resulting split gives a positive\n    # reduction in the loss function. Gamma specifies the \n    # minimum loss reduction required to make a split.\n    # Makes the algorithm conservative. The values can vary depending on the loss function and should be tuned.\n    'gamma': hp.uniform('gamma', 0.01, .7),\n    \n    # more increases accuracy, but may lead to overfitting.\n    # num_leaves: the number of leaf nodes to use. Having a large number \n    # of leaves will improve accuracy, but will also lead to overfitting.\n    'num_leaves': hp.choice('num_leaves', list(range(20, 200, 5))),\n    \n    # specifies the minimum samples per leaf node.\n    # the minimum number of samples (data) to group into a leaf. \n    # The parameter can greatly assist with overfitting: larger sample\n    # sizes per leaf will reduce overfitting (but may lead to under-fitting).\n    'min_child_samples': hp.choice('min_child_samples', list(range(100, 250, 10))),\n    \n    # subsample: represents a fraction of the rows (observations) to be \n    # considered when building each subtree. Tianqi Chen and Carlos Guestrin\n    # in their paper A Scalable Tree Boosting System recommend \n    'subsample': hp.choice('subsample', [.5, 0.6, 0.7, .8]),\n    \n    # randomly select a fraction of the features.\n    # feature_fraction: controls the subsampling of features used\n    # for training (as opposed to subsampling the actual training data in \n    # the case of bagging). Smaller fractions reduce overfitting.\n    'feature_fraction': hp.uniform('feature_fraction', 0.4, .8),\n    \n    # randomly bag or subsample training data.\n    'bagging_fraction': hp.uniform('bagging_fraction', 0.4, .9)\n    \n    # bagging_fraction and bagging_freq: enables bagging (subsampling) \n    # of the training data. Both values need to be set for bagging to be used.\n    # The frequency controls how often (iteration) bagging is used. Smaller\n    # fractions and frequencies reduce overfitting.\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## HyperOpt Run"},{"metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"best = fmin(fn=objective,\n            space=space,\n            algo=tpe.suggest,\n            max_evals=30, \n            # trials=trials\n           )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Best params "},{"metadata":{"trusted":true},"cell_type":"code","source":"best_params = space_eval(space, best)\nbest_params['max_depth'] = int(best_params['max_depth'])\nbest_params","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predicting with best params Xgb"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = xgb.XGBClassifier(\n    n_estimators=500,\n    **best_params,\n    tree_method='gpu_hist'\n)\n\nclf.fit(X_train, y_train)\n\ny_preds = clf.predict_proba(X_test)[:,1] ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cool! Now, lets see what's the features that have more importance to the classification\n"},{"metadata":{},"cell_type":"markdown","source":"## Feature Importance of Xgb model\n# Weights"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"feature_important = clf.get_booster().get_score(importance_type=\"weight\")\nkeys = list(feature_important.keys())\nvalues = list(feature_important.values())\n\ndata = pd.DataFrame(data=values, index=keys, columns=[\"score\"]).sort_values(by = \"score\", ascending=False)\n\n# Top 10 features\ndata.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Seting submission CSV"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission['target'] = y_preds\nsubmission.to_csv('XGB_hypopt_model.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# I will keep working on this kernel; Stay tuned\n\n## <font color=\"red\">Don't forget to <b>UPVOTE</b> if you finded it useful, pleaase! =D <br></font>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}