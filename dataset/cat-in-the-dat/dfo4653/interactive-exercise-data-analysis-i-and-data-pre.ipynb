{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Interactive exercise: Data analysis I and data-preprocessing\n\n## 1. Data points, data features, and data types\n\nThe scikit-learn libary comes with a few small standard datasets. One of the dataset is *Boston house prices dataset*.\n\nIn this dataset, each **data point** describes the situation of a Boston suburb or town. The **input data** includes some **data attributes** related to the place, and the **output data** is the house price (median value of owner-occupied homes in the unit of 1000 USD) at the place.\n\nThe name for each attribute is abbreivated as several capital letters to save space. The following list describes the abbreviations:\n\n- CRIM: per capita crime rate by town\n\n- ZN: proportion of residential land zoned for lots over 25,000 sq.ft.\n\n- INDUS: proportion of non-retail business acres per town\n\n- CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n\n- NOX: nitric oxides concentration (parts per 10 million)\n\n- RM: average number of rooms per dwelling\n\n- AGE: proportion of owner-occupied units built prior to 1940\n\n- DIS: weighted distances to five Boston employment centres\n\n- RAD: index of accessibility to radial highways\n\n- TAX: full-value property-tax rate per $10,000\n\n- PTRATIO: pupil-teacher ratio by town\n\n- B: 1000(Bk - 0.63)^2 where Bk is the proportion of black people by town\n\n- LSTAT: % lower status of the population\n\nYou do not have to understand the specific meanings of each of the attributes. You only need to know that each of them represent some properties related to a place in Boston. Let's see what the dataset looks like!\n\nFirstly, let's look at the input data","metadata":{"_kg_hide-output":false}},{"cell_type":"code","source":"import numpy as np\nimport warnings\n#Please ignore the above line, as it is irrelevant for the purpose of the course\nfrom sklearn.datasets import load_boston\nwith warnings.catch_warnings():\n    warnings.filterwarnings(\"ignore\")\n    dataset= load_boston()\n#This above section basically loads the dataset from the scikit-learn library\n    \ndataset_input,dataset_output,dataset_feature_names=dataset[\"data\"],dataset[\"target\"],dataset[\"feature_names\"]\n\nprint(\"The type of the input data is:\",dataset_input.dtype,\"\\n\")\n\nprint(\"The input data is:\\n\", dataset_input,\"\\n\")\n\nprint(\"The shape of the input data is:\",dataset_input.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T22:33:45.654925Z","iopub.execute_input":"2022-05-09T22:33:45.655176Z","iopub.status.idle":"2022-05-09T22:33:45.675629Z","shell.execute_reply.started":"2022-05-09T22:33:45.655151Z","shell.execute_reply":"2022-05-09T22:33:45.674657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*float64* is a special data type from the numpy library, which is analogous to the built-in python data type *float*. For the purpose of this course, you can regard each element in the table (array), such as *6.3200e-03*, as a ordinary **float**. The *e* symbol means power. For example, *6.3200e-03* means ${6.32} \\times {10}^{-3}$.\n\nThere are some dots ($\\ldots$) inside the table. (add explanations about what the dots mean)\n\n**Problem 1:**\n\n> Each column of the table (each element along axis 1 of the array) represents a <u>____</u>. <br> A. Data type  B. Data attribute  C. Data point  D. Dataset\n\n**Problem 2:**\n\n> The shape of the array is (506,13), which correponds to \n> - 506 <u>____</u> \n> - and 13 <u>____</u>. \n> <br> A. Data type  B. Data attribute  C. Data point  D. Dataset <br> \n\n\nThe display of the table is simplied and you are unable to view the most of the elements in the table. You can view the hidden elements by specifying the row of the table (the index along axis 0 of the array). For example, to see the data point correponding to the 236th location:\n\n","metadata":{}},{"cell_type":"code","source":"print(dataset_input[235])","metadata":{"execution":{"iopub.status.busy":"2022-05-09T22:33:49.356868Z","iopub.execute_input":"2022-05-09T22:33:49.357177Z","iopub.status.idle":"2022-05-09T22:33:49.362398Z","shell.execute_reply.started":"2022-05-09T22:33:49.357144Z","shell.execute_reply":"2022-05-09T22:33:49.361364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Problem 3:**\n> What is the data point correponding to the 457th location?\n\nThe order of the data attributes (columns) are arranged in the following order:","metadata":{}},{"cell_type":"code","source":"print(\"The order of the attributes are: \\n\", dataset_feature_names)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T14:54:02.656912Z","iopub.execute_input":"2022-05-05T14:54:02.657747Z","iopub.status.idle":"2022-05-05T14:54:02.668165Z","shell.execute_reply.started":"2022-05-05T14:54:02.657701Z","shell.execute_reply":"2022-05-05T14:54:02.667121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For example\n\n- The 1st column of the table represents the attribute 'CRIM', which means per capita crime rate by town.\n- The 2nd column of the table repsesents the attribute 'ZN', which means proportion of residential land zoned for lots over 25,000 sq.ft.\n- The last column of the table represents the attribute 'LSTAT', which means % lower status of the population.\n\n**Problem 4:**\n> What does the 9th column of the table mean?\n\nWith this information, you can now understand what each element in the input data repesent. For example, to get the proportion of owner-occupied units built prior to 1940 at the 236th location, simply retrieve the 7th element of the data point. The value is:","metadata":{}},{"cell_type":"code","source":"print(dataset_input[236-1][7-1])","metadata":{"execution":{"iopub.status.busy":"2022-05-05T14:54:02.673362Z","iopub.execute_input":"2022-05-05T14:54:02.674366Z","iopub.status.idle":"2022-05-05T14:54:02.682298Z","shell.execute_reply.started":"2022-05-05T14:54:02.674313Z","shell.execute_reply":"2022-05-05T14:54:02.681267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Problem 5:**\n> What is the index of accessibility to radial highways at the 154th location?\n\nNow, let's move on the output data.","metadata":{}},{"cell_type":"code","source":"print(\"The type of the output data is:\",dataset_input.dtype,\"\\n\")\n\nprint(\"The output data is: \\n\", dataset_output,\"\\n\")\n\nprint(\"The shape of the output data is:\", dataset_output.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T14:54:02.916734Z","iopub.execute_input":"2022-05-05T14:54:02.917335Z","iopub.status.idle":"2022-05-05T14:54:02.929923Z","shell.execute_reply.started":"2022-05-05T14:54:02.917284Z","shell.execute_reply":"2022-05-05T14:54:02.929001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Similarly, the data type is also float.\n\nThe shape of the array is (506,), which is analagous to a 1-dimensional list of 506 elements.\n\nEach element of the list represents a data point.\n\nFor example, the house price (median value of owner-occupied homes in the unit of 1000 USD) at the 236th place is:","metadata":{}},{"cell_type":"code","source":"print(dataset_output[236-1])","metadata":{"execution":{"iopub.status.busy":"2022-05-05T14:54:02.931395Z","iopub.execute_input":"2022-05-05T14:54:02.931653Z","iopub.status.idle":"2022-05-05T14:54:02.937677Z","shell.execute_reply.started":"2022-05-05T14:54:02.931608Z","shell.execute_reply":"2022-05-05T14:54:02.936606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Problem 6:**\n> What is the house price at the 347th place?\n\n## 2. Statistical descriptions of data\n\nFor data preprocessing to be successful, it is essential to have an overall picture of your\ndata. Basic statistical descriptions can be used to identify properties of the data.\n\n### 2.1. Central tendency \n\nTo find the mean of each data attribute in the input data\n\n","metadata":{}},{"cell_type":"code","source":"print(np.mean(dataset_input, axis=0))","metadata":{"execution":{"iopub.status.busy":"2022-05-09T22:33:28.875046Z","iopub.execute_input":"2022-05-09T22:33:28.87532Z","iopub.status.idle":"2022-05-09T22:33:28.881315Z","shell.execute_reply.started":"2022-05-09T22:33:28.875277Z","shell.execute_reply":"2022-05-09T22:33:28.880237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Similarly, to find the mean of each data attribute in the input data","metadata":{}},{"cell_type":"code","source":"print(np.median(dataset_input, axis=0))","metadata":{"execution":{"iopub.status.busy":"2022-05-09T22:34:56.866461Z","iopub.execute_input":"2022-05-09T22:34:56.866729Z","iopub.status.idle":"2022-05-09T22:34:56.873693Z","shell.execute_reply.started":"2022-05-09T22:34:56.866703Z","shell.execute_reply":"2022-05-09T22:34:56.872762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.1. Dipersion\n\nTo find the variance of each data attribute in the input data","metadata":{}},{"cell_type":"code","source":"print(np.var(dataset_input, axis=0))","metadata":{"execution":{"iopub.status.busy":"2022-05-09T22:36:29.020829Z","iopub.execute_input":"2022-05-09T22:36:29.021099Z","iopub.status.idle":"2022-05-09T22:36:29.026217Z","shell.execute_reply.started":"2022-05-09T22:36:29.021071Z","shell.execute_reply":"2022-05-09T22:36:29.025213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Similarly, to find the standard deviation of each data attribute in the input data","metadata":{}},{"cell_type":"code","source":"print(np.std(dataset_input, axis=0))","metadata":{"execution":{"iopub.status.busy":"2022-05-09T22:37:18.264775Z","iopub.execute_input":"2022-05-09T22:37:18.265046Z","iopub.status.idle":"2022-05-09T22:37:18.271594Z","shell.execute_reply.started":"2022-05-09T22:37:18.265019Z","shell.execute_reply":"2022-05-09T22:37:18.270994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Encodindg\n\nThrough this section, you are going to learn and try some of the most commonly used encoding techniques.As Kaggle competition deals with encoding a lot it would be a great time to refresh some the most common and effective encoding techniques currently in use.\n\nFirst, import the dataset","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndf_train=pd.read_csv('../input/cat-in-the-dat/train.csv')\ndf_test=pd.read_csv('../input/cat-in-the-dat/test.csv')\n\nprint('train data set has got {} rows and {} columns'.format(df_train.shape[0],df_train.shape[1]))\nprint('test data set has got {} rows and {} columns'.format(df_test.shape[0],df_test.shape[1]))\n\nX=df_train.drop(['target'],axis=1)\ny=df_train['target']\n#X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=42,test_size=0.2)\n\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-09T22:16:44.395467Z","iopub.execute_input":"2022-05-09T22:16:44.39573Z","iopub.status.idle":"2022-05-09T22:16:46.038325Z","shell.execute_reply.started":"2022-05-09T22:16:44.395703Z","shell.execute_reply":"2022-05-09T22:16:46.03752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.1. Label encoding\n\nIn this method you change every categorical data to a number.That is each type will be subtuted by a number.for example we will substitute 1 for Grandmaster, 2 for master , 3 for expert etc..\n\nTo implement this:","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n\ntrain=pd.DataFrame()\nlabel=LabelEncoder()\nfor c in  X.columns:\n    if(X[c].dtype=='object'):\n        train[c]=label.fit_transform(X[c])\n    else:\n        train[c]=X[c]\n        \ntrain.head(3)    ","metadata":{"execution":{"iopub.status.busy":"2022-05-09T22:16:48.836211Z","iopub.execute_input":"2022-05-09T22:16:48.836781Z","iopub.status.idle":"2022-05-09T22:16:50.612892Z","shell.execute_reply.started":"2022-05-09T22:16:48.836735Z","shell.execute_reply":"2022-05-09T22:16:50.612357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2. One hot encoding\n\nThe second method is encoding each category as a one hot encoding vector (or dummy variables). It is is a representation method that takes each category value and turns it into a binary vector of size |i|(number of values in category i) where all columns are equal to zero besides the category column. Here is a little example:   \n\n![](https://miro.medium.com/max/878/1*WXpoiS7HXRC-uwJPYsy1Dg.png)\n\nTo implement this:","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\none=OneHotEncoder()\n\none.fit(X)\ntrain=one.transform(X)\n\nprint(train)\n\nprint('train data set has got {} rows and {} columns'.format(train.shape[0],train.shape[1]))","metadata":{"execution":{"iopub.status.busy":"2022-05-09T22:16:55.434879Z","iopub.execute_input":"2022-05-09T22:16:55.435405Z","iopub.status.idle":"2022-05-09T22:16:57.824488Z","shell.execute_reply.started":"2022-05-09T22:16:55.435372Z","shell.execute_reply":"2022-05-09T22:16:57.82379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.3 Target encoding\n\nTarget-based encoding is numerization of categorical variables via target. In this method, we replace the categorical variable with just one new numerical variable and replace each category of the categorical variable with its corresponding probability of the target (if categorical) or average of the target (if numerical). The main drawbacks of this method are its dependency to the distribution of the target, and its lower predictability power compare to the binary encoding method.\n\nfor example,\n<table style=\"width : 20%\">\n    <tr>\n    <th>Country</th>\n    <th>Target</th>\n    </tr>\n    <tr>\n    <td>India</td>\n    <td>1</td>\n    </tr>\n    <tr>\n    <td>China</td>\n    <td>0</td>\n    </tr>\n    <tr>\n    <td>India</td>\n    <td>0</td>\n    </tr>\n    <tr>\n    <td>China</td>\n    <td>1</td>\n    </tr>\n    </tr>\n    <tr>\n    <td>India</td>\n    <td>1</td>\n    </tr>\n</table>\n\n\nEncoding for India = [Number of true targets under the label India/ Total Number of targets under the label India] \nwhich is 2/3 = 0.66\n\n<table style=\"width : 20%\">\n    <tr>\n    <th>Country</th>\n    <th>Target</th>\n    </tr>\n    <tr>\n    <td>India</td>\n    <td>0.66</td>\n    </tr>\n    <tr>\n    <td>China</td>\n    <td>0.5</td>\n    </tr>\n</table>\n\n\nTo implement this:","metadata":{}},{"cell_type":"code","source":"X_target=df_train.copy()\nX_target['day']=X_target['day'].astype('object')\nX_target['month']=X_target['month'].astype('object')\nfor col in X_target.columns:\n    if (X_target[col].dtype=='object'):\n        target= dict ( X_target.groupby(col)['target'].agg('sum')/X_target.groupby(col)['target'].agg('count'))\n        X_target[col]=X_target[col].replace(target).values\n\nX_target.head(4)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T22:17:02.24449Z","iopub.execute_input":"2022-05-09T22:17:02.244974Z","iopub.status.idle":"2022-05-09T22:22:09.128608Z","shell.execute_reply.started":"2022-05-09T22:17:02.244943Z","shell.execute_reply":"2022-05-09T22:22:09.127636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Data cleaning\n\nIn this tutorial, you will learn three approaches to **dealing with missing values**. Then you'll compare the effectiveness of these approaches on a real-world dataset.\n\nThere are many ways data can end up with missing values. For example,\n- A 2 bedroom house won't include a value for the size of a third bedroom.\n- A survey respondent may choose not to share his income.\n\nMost machine learning libraries (including scikit-learn) give an error if you try to build a model using data with missing values. So you'll need to choose one of the strategies below.\n\nThe code below defines a dataset for this section:","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Load the data\ndata = pd.read_csv('../input/melbourne-housing-snapshot/melb_data.csv')\n\n# Select target\ny = data.Price\n\n# To keep things simple, we'll use only numerical predictors\nmelb_predictors = data.drop(['Price'], axis=1)\nX = melb_predictors.select_dtypes(exclude=['object'])\n\n# Divide data into training and validation subsets\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                      random_state=0)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T21:38:25.380301Z","iopub.execute_input":"2022-05-09T21:38:25.380596Z","iopub.status.idle":"2022-05-09T21:38:25.445213Z","shell.execute_reply.started":"2022-05-09T21:38:25.380565Z","shell.execute_reply":"2022-05-09T21:38:25.444443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then, letâ€™s define a metrics to measure the quality of data cleaning. Lower score represents better quality.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\n# Function for comparing different approaches\ndef score_dataset(X_train, X_valid, y_train, y_valid):\n    model = RandomForestRegressor(n_estimators=10, random_state=0)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_valid)\n    return mean_absolute_error(y_valid, preds)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T21:41:19.197073Z","iopub.execute_input":"2022-05-09T21:41:19.19738Z","iopub.status.idle":"2022-05-09T21:41:19.203623Z","shell.execute_reply.started":"2022-05-09T21:41:19.197347Z","shell.execute_reply":"2022-05-09T21:41:19.20266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.1 Deletion of data points\n\nThe simplest option is to drop columns with missing values. \n\n![tut2_approach1](https://i.imgur.com/Sax80za.png)\n\nUnless most values in the dropped columns are missing, the model loses access to a lot of (potentially useful!) information with this approach.  As an extreme example, consider a dataset with 10,000 rows, where one important column is missing a single entry. This approach would drop the column entirely!\n\nFor example, to delete the data points in the melbourne housing dataset:","metadata":{}},{"cell_type":"code","source":"# Get names of columns with missing values\ncols_with_missing = [col for col in X_train.columns\n                     if X_train[col].isnull().any()]\n\n# Drop columns in training and validation data\nreduced_X_train = X_train.drop(cols_with_missing, axis=1)\nreduced_X_valid = X_valid.drop(cols_with_missing, axis=1)\n\nprint(\"MAE from Approach 1 (Drop columns with missing values):\")\nprint(score_dataset(reduced_X_train, reduced_X_valid, y_train, y_valid))","metadata":{"execution":{"iopub.status.busy":"2022-05-09T21:41:23.13317Z","iopub.execute_input":"2022-05-09T21:41:23.134515Z","iopub.status.idle":"2022-05-09T21:41:23.628346Z","shell.execute_reply.started":"2022-05-09T21:41:23.134464Z","shell.execute_reply":"2022-05-09T21:41:23.62749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.2 Data imputation\n\n**Imputation** fills in the missing data points.  Common methods include filling out the missing data points with\n\n- Manully decided values\n- A global constant, such as 0 or 1\n- Central tendency\n\nFor instance, we can fill in the mean value along each column. \n\n![tut2_approach2](https://i.imgur.com/4BpnlPA.png)\n\nThe imputed value won't be exactly right in most cases, but it usually leads to more accurate models than you would get from dropping the column entirely.\n\nFor example, to fill in the missing data points in the melbourne housing dataset with mean values:","metadata":{}},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\n\n# Imputation\nmy_imputer = SimpleImputer()\nimputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))\nimputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))\n\n# Imputation removed column names; put them back\nimputed_X_train.columns = X_train.columns\nimputed_X_valid.columns = X_valid.columns\n\nprint(\"MAE from Approach 2 (Imputation):\")\nprint(score_dataset(imputed_X_train, imputed_X_valid, y_train, y_valid))","metadata":{"execution":{"iopub.status.busy":"2022-05-09T21:43:22.868348Z","iopub.execute_input":"2022-05-09T21:43:22.868871Z","iopub.status.idle":"2022-05-09T21:43:23.499579Z","shell.execute_reply.started":"2022-05-09T21:43:22.868823Z","shell.execute_reply":"2022-05-09T21:43:23.498946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. Balancing\n\nAn unbalanced dataset is one in which the data points has more entries in one specific class than the others in the output data. For example, an output data that describes the gender of the students in a class can be quite balanced, but an output data that describes the frauds in transations made by highly trusted agents cab be highly unbalanced:\n\n![bal vs unbal](https://miro.medium.com/max/1400/1*miAWYUJ7sgWaRHCZMdP2OQ.png)\n\nBalancing an unblanaced dataset would generate higher accuracy. For example, in the output data of the breast cancer dataset below, 0 represents that the breast cancer is benign and 1 represents the cancer is malignant \n\n","metadata":{}},{"cell_type":"code","source":"from sklearn.datasets import load_breast_cancer\n\ndataset_2=load_breast_cancer()\n\nprint(dataset_2.target)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T23:05:00.545788Z","iopub.execute_input":"2022-05-09T23:05:00.546418Z","iopub.status.idle":"2022-05-09T23:05:00.565689Z","shell.execute_reply.started":"2022-05-09T23:05:00.546376Z","shell.execute_reply":"2022-05-09T23:05:00.56459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To balance the dataset, first check the distribution of each class","metadata":{}},{"cell_type":"code","source":"values,counts = np.unique(dataset_2.target, return_counts=True)\n\nprint(np.asarray((values, counts)).T)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T23:11:33.294209Z","iopub.execute_input":"2022-05-09T23:11:33.294579Z","iopub.status.idle":"2022-05-09T23:11:33.302419Z","shell.execute_reply.started":"2022-05-09T23:11:33.294537Z","shell.execute_reply":"2022-05-09T23:11:33.30147Z"},"trusted":true},"execution_count":null,"outputs":[]}]}