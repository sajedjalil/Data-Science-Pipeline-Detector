{"cells":[{"metadata":{},"cell_type":"markdown","source":"### import packages"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import MinMaxScaler,LabelEncoder,OneHotEncoder\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import GridSearchCV,StratifiedKFold,KFold,train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as cbt\nimport datetime\nimport time\nimport warnings\nimport gc\nwarnings.filterwarnings(\"ignore\")\nsns.set(style=\"whitegrid\",color_codes=True)\nsns.set(font_scale=1)\n%time","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#reduce mem cost\n%time\ndef reduce_mem_cost(df,verbose = True):\n    num_type = ['int16','int32','int64','float32','float64','object']\n    start_mem = df.memory_usage().sum() / 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in num_type:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            elif str(col_type)[:5] == 'float':\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose:\n        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df       ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### load data "},{"metadata":{"trusted":true},"cell_type":"code","source":"%time\ntrain_data = pd.read_csv(\"/kaggle/input/cat-in-the-dat/train.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/cat-in-the-dat/test.csv\")\ntrain_target = train_data.pop(\"target\")\ntrain_data = reduce_mem_cost(train_data)\ntest_data = reduce_mem_cost(test_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"#查看数据\ntrain_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time\nlabel_encoder_cols = []\nno_label_encoder_cols = []\nfor col in train_data.columns:\n    if train_data[col].dtypes == \"int32\" or train_data[col].dtypes == \"int8\":\n        no_label_encoder_cols.append(col)\n    else:\n        label_encoder_cols.append(col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time\n# label encoder on some cols\ntrain_id = train_data.pop(\"id\")\ntest_id = test_data.pop(\"id\")\nall_data = pd.concat([train_data,test_data],axis=0)\nall_data[label_encoder_cols]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### feature engniees"},{"metadata":{"trusted":true},"cell_type":"code","source":"%time\n#generate  count feats\nbin_3_cnt = all_data[\"bin_3\"].value_counts().to_dict()\nall_data[\"bin_3_cnt\"] = all_data[\"bin_3\"].map(bin_3_cnt)\n\nbin_4_cnt = all_data[\"bin_4\"].value_counts().to_dict()\nall_data[\"bin_4_cnt\"] = all_data[\"bin_4\"].map(bin_4_cnt)\n\nnom_0_cnt = all_data[\"nom_0\"].value_counts().to_dict()\nall_data[\"nom_0_cnt\"] = all_data[\"nom_0\"].map(nom_0_cnt)\n\nnom_1_cnt = all_data[\"nom_1\"].value_counts().to_dict()\nall_data[\"nom_1_cnt\"] = all_data[\"nom_1\"].map(nom_1_cnt)\n\nnom_2_cnt = all_data[\"nom_2\"].value_counts().to_dict()\nall_data[\"nom_2_cnt\"] = all_data[\"nom_2\"].map(nom_2_cnt)\n\nnom_3_cnt = all_data[\"nom_3\"].value_counts().to_dict()\nall_data[\"nom_3_cnt\"] = all_data[\"nom_3\"].map(nom_3_cnt)\n\nnom_4_cnt = all_data[\"nom_4\"].value_counts().to_dict()\nall_data[\"nom_4_cnt\"] = all_data[\"nom_4\"].map(nom_4_cnt)\n\nnom_5_cnt = all_data[\"nom_5\"].value_counts().to_dict()\nall_data[\"nom_5_cnt\"] = all_data[\"nom_5\"].map(nom_5_cnt)\n\nnom_6_cnt = all_data[\"nom_6\"].value_counts().to_dict()\nall_data[\"nom_6_cnt\"] = all_data[\"nom_6\"].map(nom_6_cnt)\n\nnom_7_cnt = all_data[\"nom_7\"].value_counts().to_dict()\nall_data[\"nom_7_cnt\"] = all_data[\"nom_7\"].map(nom_7_cnt)\n\nnom_8_cnt = all_data[\"nom_8\"].value_counts().to_dict()\nall_data[\"nom_8_cnt\"] = all_data[\"nom_8\"].map(nom_8_cnt)\n\nnom_9_cnt = all_data[\"nom_9\"].value_counts().to_dict()\nall_data[\"nom_9_cnt\"] = all_data[\"nom_9\"].map(nom_9_cnt)\n\nord_1_cnt = all_data[\"ord_1\"].value_counts().to_dict()\nall_data[\"ord_1_cnt\"] = all_data[\"ord_1\"].map(ord_1_cnt)\n\nord_2_cnt = all_data[\"ord_2\"].value_counts().to_dict()\nall_data[\"ord_2_cnt\"] = all_data[\"ord_2\"].map(ord_2_cnt)\n\nord_3_cnt = all_data[\"ord_3\"].value_counts().to_dict()\nall_data[\"ord_3_cnt\"] = all_data[\"ord_3\"].map(ord_3_cnt)\n\nord_4_cnt = all_data[\"ord_4\"].value_counts().to_dict()\nall_data[\"ord_4_cnt\"] = all_data[\"ord_4\"].map(ord_4_cnt)\n\nord_5_cnt = all_data[\"ord_5\"].value_counts().to_dict()\nall_data[\"ord_5_cnt\"] = all_data[\"ord_5\"].map(ord_5_cnt)\n\nall_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time\n#label encoder on some cols\nfor le in label_encoder_cols:\n    le_feat = LabelEncoder()\n    le_feat.fit(all_data[le])\n    all_data[le] = le_feat.transform(all_data[le])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### split data into train data and test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#split all data into train and test\ntrain_data = all_data[:train_data.shape[0]]\ntest_data = all_data[train_data.shape[0]:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time\ntrain_x,train_y,testX = train_data.values,train_target.values,test_data.values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### model build"},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\nmodel = lgb.LGBMClassifier(boosting_type=\"gbdt\",num_leaves=48, max_depth=-1, learning_rate=0.05,\n                               n_estimators=3000, subsample_for_bin=50000,objective=\"binary\",min_split_gain=0, min_child_weight=5, min_child_samples=30, #10\n                               subsample=0.8,subsample_freq=1, colsample_bytree=1, reg_alpha=3,reg_lambda=5,\n                               feature_fraction= 0.9, bagging_fraction = 0.9,\n                               seed= 2019,n_jobs=10,slient=True,num_boost_round=3000)\nn_splits = 3\nrandom_seed = 2019\nskf = StratifiedKFold(shuffle = True, random_state = random_seed, n_splits = n_splits)\ncv_pred = []\nval_score = []\nfor idx, (tra_idx,val_idx) in enumerate(skf.split(train_x,train_y)):\n    startTime = time.time()\n    print(\"============================================fold_{}===================================================\".format(str(idx+1)))\n    X_train,Y_train = train_x[tra_idx],train_y[tra_idx]\n    X_val,Y_val = train_x[val_idx], train_y[val_idx]\n    lgb_model = model.fit(X_train,Y_train,eval_names=[\"train\",\"valid\"],eval_metric=[\"logloss\"],eval_set=[(X_train, Y_train),(X_val,Y_val)],early_stopping_rounds=200)\n    val_pred = lgb_model.predict(X_val,num_iteration = lgb_model.best_iteration_)\n    val_score.append(f1_score(Y_val,val_pred))\n    print(\"f1_score:\",f1_score(Y_val, val_pred))\n    test_pred = lgb_model.predict(testX, num_iteration = lgb_model.best_iteration_).astype(int)\n    cv_pred.append(test_pred)\n    endTime = time.time()\n    print(\"fold_{} finished in {}\".format(str(idx+1), datetime.timedelta(seconds= endTime-startTime)))\n    \nend = time.time()\nprint('-'*60)\nprint(\"Training has finished.\")\nprint(\"Total training time is {}\".format(str(datetime.timedelta(seconds=end-start))))\nprint(val_score)\nprint(\"mean f1:\",np.mean(val_score))\nprint('-'*60)\n\nsubmit = []\nfor line in np.array(cv_pred).transpose():\n    submit.append(np.argmax(np.bincount(line)))\nfinal_result = pd.DataFrame(columns=[\"id\",\"target\"])\nfinal_result[\"id\"] = list(test_id.unique())\nfinal_result[\"target\"] = submit\nfinal_result.to_csv(\"submitLGB{0}.csv\".format(datetime.datetime.now().strftime(\"%Y%m%d%H%M\")),index = False)\nprint(final_result.head())       ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}