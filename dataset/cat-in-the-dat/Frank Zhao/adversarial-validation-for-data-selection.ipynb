{"cells":[{"metadata":{},"cell_type":"markdown","source":"Sometimes, our LB score could suffer a lot when the train and test set have different distributions. It would be helpful if we could remove some rows to make the train set more similar to the test set. In other words, we need to classify rows in train which are 'abnormal'  ==> Use a binary classification algorithm.\n\n\n\n\nProecess: Combine train and test and set their target values as 1 and 0 seperately. Use the algorithm to predict **the target value of train**.\n- If the predict value is 0, it means the algorithm couldn't tell this one from test, which means it is usable.\n- If the predict value is 1, it means the algorithm finds the difference, indicating we should remove this row.\n\n\n![](https://i.imgur.com/cnQmn0W.jpg)\neditted from https://medium.com/crownsontop/are-you-the-black-sheep-in-a-black-family-5a3747d7dd0c"},{"metadata":{},"cell_type":"markdown","source":"# Note\nDon't use this method in real-life projects!!!!  It will cause the model **ovefit** the test dataset!Also, don't use it if public and private LB have differnent data distributions."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nfrom sklearn.model_selection import KFold, train_test_split\nfrom catboost import CatBoostClassifier, Pool\nfrom sklearn.metrics import roc_auc_score as auc\nimport plotly.graph_objects as go","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_raw=pd.read_csv(\"/kaggle/input/cat-in-the-dat/train.csv\")\ntest_raw=pd.read_csv(\"/kaggle/input/cat-in-the-dat/test.csv\")\n\ntrain = train_raw.drop(['id', 'target'], axis=1)\ntest = test_raw.drop(['id'], axis=1)\ny = train_raw.target\n\ncols = train.columns\ntrain_length = train.shape[0]\ntest_length = test.shape[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# BaseLine Model\n\nBasic CatboostClassifer without Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"# cbc = CatBoostClassifier(iterations = 300, learning_rate = 0.1, eval_metric = 'AUC', verbose = False)\n\n# tr_x, val_x, tr_y, val_y = train_test_split(train, y, test_size = 0.2, shuffle = True, random_state = 10)\n\n# cbc.fit(tr_x, tr_y, eval_set=(val_x, val_y), cat_features=cols)\n\n# y_pred = cbc.predict_proba(test)[:, 1]\n\n# submission = pd.DataFrame({'id': test_raw.id, 'target': y_pred})\n# submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Baseline Model Score on LB:\n\n![](https://i.imgur.com/C5EqLRS.png)"},{"metadata":{},"cell_type":"markdown","source":"# Use adversarial validation on raw train dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_splits = 5\n\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=10)\n\ny_pred_ad = np.zeros(train.shape[0]) ### Initialize an array to record the predicted result from adversarial validation\n\n\n### For all rows from train, no matter it is in tr_x or val_x, its taget value is setted as 1.\n### For rows from test, its taget value is setted as 0.\nfor tr_range, val_range in kf.split(train):\n    tr_x = train.loc[tr_range]\n    val_x = train.loc[val_range]\n    val_y = np.ones(val_x.shape[0]) \n    \n    tr_x_combined = pd.concat([tr_x, test], axis = 0)\n    tr_y_combined = np.append(np.ones(tr_x.shape[0]), np.zeros(test_length))\n    \n    cbc = CatBoostClassifier(iterations = 300, learning_rate = 0.1, eval_metric = 'AUC', verbose = False)\n    cbc.fit(tr_x_combined, tr_y_combined, eval_set=(val_x, val_y), cat_features = cols)\n    y_pred_ad[val_range] = cbc.predict_proba(val_x)[:, 1]  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I prefer using \"predict_proba\" because in this case, we could manually select a proper propability to decide how many rows we remove from train and how strict the selection is. It is a tradeoff between **quality** and **quantity**.\n\nIf you are familiar with \"the elbow method\", you would have an idea of choosing which point.  :)\n\nAlternatively, you could use \"predict\" to simplify the steps."},{"metadata":{"trusted":true},"cell_type":"code","source":"lox = []\nloy = []\nfor i in range(50, 60, 1): ### Proba from 0.5 to 0.6\n    threshold = i / 100\n    lox.append(threshold)\n    train_ad = train[y_pred_ad < threshold]\n    loy.append(train_ad.shape[0])\nfig = go.Figure(data=go.Scatter(x=lox, y=loy))\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this competition, it seems that there is no big differences between train and test since most predicted probabilities lie in the range [0.5, 0.6], i.e. the model couldn't confidently say whether one training example is normal or not.\n\nRemove the bad-performance rows and train a CatBoostClassifier of the same hyperparamters in baseline model to compare the performance."},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train[y_pred_ad < 0.56]\ny = y[y_pred_ad < 0.56]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cbc = CatBoostClassifier(iterations = 300, learning_rate = 0.1, eval_metric = 'AUC', verbose = False)\n\ntr_x, val_x, tr_y, val_y = train_test_split(train, y, test_size = 0.2, shuffle = True, random_state = 10)\n\ncbc.fit(tr_x, tr_y, eval_set=(val_x, val_y), cat_features=cols)\n\ny_pred = cbc.predict_proba(test)[:, 1]\n\nsubmission = pd.DataFrame({'id': test_raw.id, 'target': y_pred})\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"New Score:\n\n![](https://i.imgur.com/e8eMURx.png)\n\nThe score has been imporved from 0.79989 to 0.80099! Considering to the similarity of train and test in this competition which limits this method's performance, it is still a great improvement!"},{"metadata":{},"cell_type":"markdown","source":"If you like this kernel, please upvote it. Thanks!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}