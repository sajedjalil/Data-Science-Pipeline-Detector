{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"%%time\ntrain = pd.read_csv('../input/cat-in-the-dat/train.csv')\ntest = pd.read_csv('../input/cat-in-the-dat/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"target = train['target']\ntrain_id = train['id']\ntest_id = test['id']\n\ntrain.drop(['target', 'id'], axis=1, inplace=True)\ntest.drop('id', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"df = pd.concat([train, test], axis=0, sort=False )","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"bin_dict = {'T':1, 'F':0, 'Y':1, 'N':0}\ndf['bin_3'] = df['bin_3'].map(bin_dict)\ndf['bin_4'] = df['bin_4'].map(bin_dict)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"print(f'Shape before dummy transformation: {df.shape}')\ndf = pd.get_dummies(df, columns=['nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4'],\n                    prefix=['nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4'], \n                    drop_first=True)\nprint(f'Shape after dummy transformation: {df.shape}')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"from pandas.api.types import CategoricalDtype \n\nord_1 = CategoricalDtype(categories=['Novice', 'Contributor','Expert', \n                                     'Master', 'Grandmaster'], ordered=True)\nord_2 = CategoricalDtype(categories=['Freezing', 'Cold', 'Warm', 'Hot',\n                                     'Boiling Hot', 'Lava Hot'], ordered=True)\nord_3 = CategoricalDtype(categories=['a', 'b', 'c', 'd', 'e', 'f', 'g',\n                                     'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o'], ordered=True)\nord_4 = CategoricalDtype(categories=['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I',\n                                     'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R',\n                                     'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'], ordered=True)\n\ndf.ord_1 = df.ord_1.astype(ord_1)\ndf.ord_2 = df.ord_2.astype(ord_2)\ndf.ord_3 = df.ord_3.astype(ord_3)\ndf.ord_4 = df.ord_4.astype(ord_4)\n\ndf.ord_1 = df.ord_1.cat.codes\ndf.ord_2 = df.ord_2.cat.codes\ndf.ord_3 = df.ord_3.cat.codes\ndf.ord_4 = df.ord_4.cat.codes","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"def date_cyc_enc(df, col, max_vals):\n    df[col + '_sin'] = np.sin(2 * np.pi * df[col]/max_vals)\n    df[col + '_cos'] = np.cos(2 * np.pi * df[col]/max_vals)\n    return df\n\ndf = date_cyc_enc(df, 'day', 7)\ndf = date_cyc_enc(df, 'month', 12)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"%%time\nfrom sklearn.preprocessing import LabelEncoder\n\n# Label Encoding\nfor f in ['nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9', 'ord_5']:\n    lbl = LabelEncoder()\n    lbl.fit(df[f])\n    df[f'le_{f}'] = lbl.transform(df[f])","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"df.drop(['nom_5','nom_6','nom_7','nom_8','nom_9', 'ord_5'] , axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = reduce_mem_usage(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# SHAP"},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://slundberg.github.io/shap/notebooks/plots/decision_plot.html (Good !!)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nfrom matplotlib import pyplot as plt\nfrom pdpbox import pdp, get_dataset, info_plots\nimport shap","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = df[:train.shape[0]]\ntest = df[train.shape[0]:]\n\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(train, target, test_size = 0.2, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  RandomForest\nModel=LGBMClassifier(max_depth=10, n_estimators=1000, n_jobs=-1, num_leaves=45,  learning_rate=0.01)\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_val)\nprint(classification_report(y_pred,y_val))\n#print(confusion_matrix(y_pred,y_val))\n#Accuracy Score\nprint('Roc_Auc is ',roc_auc_score(y_pred,y_val))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* boosting_type (string, optional (default='gbdt')) – ‘gbdt’, traditional Gradient Boosting Decision Tree. ‘dart’, Dropouts meet Multiple Additive Regression Trees. ‘goss’, Gradient-based One-Side Sampling. ‘rf’, Random Forest.\n* num_leaves (int, optional (default=31)) – Maximum tree leaves for base learners.\n* max_depth (int, optional (default=-1)) – Maximum tree depth for base learners, <=0 means no limit.\n* learning_rate (float, optional (default=0.1)) – Boosting learning rate. You can use callbacks parameter of fit method to shrink/adapt learning rate in training using reset_parameter callback. Note, that this will ignore the learning_rate argument in training.\n* n_estimators (int, optional (default=100)) – Number of boosted trees to fit.\n* subsample_for_bin (int, optional (default=200000)) – Number of samples for constructing bins.\n* objective (string, callable or None, optional (default=None)) – Specify the learning task and the corresponding learning objective or a custom objective function to be used (see note below). Default: ‘regression’ for LGBMRegressor, ‘binary’ or ‘multiclass’ for LGBMClassifier, ‘lambdarank’ for LGBMRanker.\n* class_weight (dict, 'balanced' or None, optional (default=None)) – Weights associated with classes in the form {class_label: weight}. Use this parameter only for multi-class classification task; for binary classification task you may use is_unbalance or scale_pos_weight parameters. Note, that the usage of all these parameters will result in poor estimates of the individual class probabilities. You may want to consider performing probability calibration (https://scikit-learn.org/stable/modules/calibration.html) of your model. The ‘balanced’ mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y)). If None, all classes are supposed to have weight one. Note, that these weights will be multiplied with sample_weight (passed through the fit method) if sample_weight is specified.\n* min_split_gain (float, optional (default=0.)) – Minimum loss reduction required to make a further partition on a leaf node of the tree.\n* min_child_weight (float, optional (default=1e-3)) – Minimum sum of instance weight (hessian) needed in a child (leaf).\n* min_child_samples (int, optional (default=20)) – Minimum number of data needed in a child (leaf).\n* subsample (float, optional (default=1.)) – Subsample ratio of the training instance.\n* subsample_freq (int, optional (default=0)) – Frequence of subsample, <=0 means no enable.\n* colsample_bytree (float, optional (default=1.)) – Subsample ratio of columns when constructing each tree.\n* reg_alpha (float, optional (default=0.)) – L1 regularization term on weights.\n* reg_lambda (float, optional (default=0.)) – L2 regularization term on weights.\n* random_state (int or None, optional (default=None)) – Random number seed. If None, default seeds in C++ code will be used.\n* n_jobs (int, optional (default=-1)) – Number of parallel threads.\n* silent (bool, optional (default=True)) – Whether to print messages while running boosting.\n* importance_type (string, optional (default='split')) – The type of feature importance to be filled into feature_importances_. If ‘split’, result contains numbers of times the feature is used in a model. If ‘gain’, result contains total gains of splits which use the feature.\n* **kwargs –\n* Other parameters for the model. Check http://lightgbm.readthedocs.io/en/latest/Parameters.html for more parameters."},{"metadata":{"trusted":true},"cell_type":"code","source":"perm = PermutationImportance(Model, random_state=1).fit(X_val, y_val)\neli5.show_weights(perm, feature_names = X_val.columns.tolist(),top=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"https://eli5.readthedocs.io/en/latest/autodocs/eli5.html"},{"metadata":{},"cell_type":"markdown","source":"* As you move down the top of the graph, the importance of the feature decreases.\n* The features that are shown in green indicate that they have a positive impact on our prediction\n* The features that are shown in white indicate that they have no effect on our prediction\n* The features shown in red indicate that they have a negative impact on our prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"explainer = shap.TreeExplainer(Model)\nexpected_value = explainer.expected_value\nif isinstance(expected_value, list):\n    expected_value = expected_value[1]\nprint(f\"Explainer expected value: {expected_value}\")\n\nselect = range(20)\nfeatures = X_val.iloc[select]\n#features_display = features.columns\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    shap_values = explainer.shap_values(features)[1]\n    shap_interaction_values = explainer.shap_interaction_values(features)\nif isinstance(shap_interaction_values, list):\n    shap_interaction_values = shap_interaction_values[1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Refer to the decision plot of the 20 test observations below. _Note: This plot isn't informative by itself; we use it only to illustrate the primary concepts._\n\n* The x-axis represents the model's output. In this case, the units are log odds.\n* The plot is centered on the x-axis at explainer.expected_value. All SHAP values are relative to the model's expected value like a linear model's effects are relative to the intercept.\n* The y-axis lists the model's features. By default, the features are ordered by descending importance. The importance is calculated over the observations plotted. _This is usually different than the importance ordering for the entire dataset._ In addition to feature importance ordering, the decision plot also supports hierarchical cluster feature ordering and user-defined feature ordering.\n* Each observation's prediction is represented by a colored line. At the top of the plot, each line strikes the x-axis at its corresponding observation's predicted value. This value determines the color of the line on a spectrum.\n* Moving from the bottom of the plot to the top, SHAP values for each feature are added to the model's base value. This shows how each feature contributes to the overall prediction.\n* At the bottom of the plot, the observations converge at explainer.expected_value."},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.decision_plot(expected_value, shap_values, features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Like the force plot, the decision plot supports link='logit' to transform log odds to probabilities."},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.decision_plot(expected_value, shap_values, features,link='logit')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observations can be highlighted using a dotted line style. Here, we highlight a misclassified observation."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Our naive cutoff point is zero log odds (probability 0.5).\ny_pred = (shap_values.sum(1) + expected_value) > 0\nmisclassified = y_pred != y_val.iloc[select]\nshap.decision_plot(expected_value, shap_values, features, highlight=misclassified, link='logit')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.decision_plot(expected_value, shap_values[misclassified], features[misclassified],\n                    link='logit', highlight=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.initjs()\nshap.force_plot(expected_value, shap_values[misclassified], features[misclassified])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# PDP"},{"metadata":{},"cell_type":"markdown","source":"Here is the code to create the Partial Dependence Plot using the PDPBox library."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the data that we will plot\npdp_goals = pdp.pdp_isolate(model=Model, dataset=X_val, model_features=features.columns, feature='le_ord_5')\n\n# plot it\npdp.pdp_plot(pdp_goals, 'le_ord_5')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A few items are worth pointing out as you interpret this plot\n\n* The y axis is interpreted as change in the prediction from what it would be predicted at the baseline or leftmost value.\n* A blue shaded area indicates level of confidence"},{"metadata":{},"cell_type":"markdown","source":"In a nutshell, the larger the value of le_ord_5, the more likely it is to hit the target, and that data does not seem to require interpretation."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the data that we will plot\npdp_goals = pdp.pdp_isolate(model=Model, dataset=X_val, model_features=features.columns, feature='ord_4')\n\n# plot it\npdp.pdp_plot(pdp_goals, 'ord_4')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2D PDP"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Similar to previous PDP plot except we use pdp_interact instead of pdp_isolate and pdp_interact_plot instead of pdp_isolate_plot\nfeatures_to_plot = ['le_ord_5', 'ord_4']\ninter1  =  pdp.pdp_interact(model=Model, dataset=X_val, model_features=features.columns, features=features_to_plot)\n\npdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=features_to_plot, plot_type='contour', plot_pdp=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How do we interpret it in this model of category variables? <br>\nI'd appreciate it if you could share your knowledge."},{"metadata":{},"cell_type":"markdown","source":"**TO BE Continue**"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}