{"cells":[{"metadata":{},"cell_type":"markdown","source":"----------\n**IEEE Fraud Detection - Bayesian optimization - LGB**\n=====================================\n\n***Vincent Lugat***\n\n*July 2019*\n\n----------"},{"metadata":{},"cell_type":"markdown","source":"![](https://image.noelshack.com/fichiers/2019/29/2/1563297157-cis-logo.png)"},{"metadata":{},"cell_type":"markdown","source":"- <a href='#1'>1. Libraries and Data</a>  \n- <a href='#2'>2. Bayesian Optimisation </a> \n- <a href='#3'>3. LGB + best hyperparameters</a>\n- <a href='#4'>4. Features importance</a>\n- <a href='#4'>5. Submission</a>"},{"metadata":{},"cell_type":"markdown","source":"# <a id='1'>1. Librairies and data</a> "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#  Libraries\nimport numpy as np \nimport pandas as pd \n# Data processing, metrics and modeling\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom bayes_opt import BayesianOptimization\nfrom datetime import datetime\nfrom sklearn.metrics import precision_score, recall_score, confusion_matrix, accuracy_score, roc_auc_score, f1_score, roc_curve, auc\nfrom sklearn import metrics\nfrom sklearn import preprocessing\n\nfrom sklearn.decomposition import PCA, FastICA,SparsePCA,KernelPCA\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.manifold import TSNE\nfrom sklearn.random_projection import GaussianRandomProjection\nfrom sklearn.random_projection import SparseRandomProjection\n\n# Lgbm\nimport lightgbm as lgb\n# Suppr warning\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport itertools\nfrom scipy import interp\n\n# Plots\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## DATASETS"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntrain_transaction = pd.read_csv('../input/train_transaction.csv', index_col='TransactionID')\ntest_transaction = pd.read_csv('../input/test_transaction.csv', index_col='TransactionID')\ntrain_identity = pd.read_csv('../input/train_identity.csv', index_col='TransactionID')\ntest_identity = pd.read_csv('../input/test_identity.csv', index_col='TransactionID')\nsample_submission = pd.read_csv('../input/sample_submission.csv', index_col='TransactionID')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## MERGE, MISSING VALUE, FILL NA"},{"metadata":{"trusted":true},"cell_type":"code","source":"# merge \ntrain_df = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\ntest_df = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)\n\nprint(\"Train shape : \"+str(train_df.shape))\nprint(\"Test shape  : \"+str(test_df.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_columns', 500)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# GPreda, missing data\ndef missing_data(data):\n    total = data.isnull().sum()\n    percent = (data.isnull().sum()/data.isnull().count()*100)\n    tt = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    types = []\n    for col in data.columns:\n        dtype = str(data[col].dtype)\n        types.append(dtype)\n    tt['Types'] = types\n    return(np.transpose(tt))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(missing_data(train_df), missing_data(test_df))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fillna\ntrain_df = train_df.fillna(-999)\ntest_df = test_df.fillna(-999)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_transaction, train_identity, test_transaction, test_identity","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ENCODING"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Label Encoding\nfor f in train_df.columns:\n    if  train_df[f].dtype=='object': \n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(train_df[f].values) + list(test_df[f].values))\n        train_df[f] = lbl.transform(list(train_df[f].values))\n        test_df[f] = lbl.transform(list(test_df[f].values))  \ntrain_df = train_df.reset_index()\ntest_df = test_df.reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#PCA/ICA for dimensionality reduction\n\nn_comp = 12\n\n# tSVD\ntsvd = TruncatedSVD(n_components=n_comp, random_state=420)\ntsvd_results_train = tsvd.fit_transform(train_df.drop([\"isFraud\"], axis=1))\ntsvd_results_test = tsvd.transform(test_df)\n\n# # PCA\n# pca = PCA(n_components=n_comp, random_state=420)\n# pca2_results_train = pca.fit_transform(train.drop([\"y\"], axis=1))\n# pca2_results_test = pca.transform(test)\n#\n\n#Polynomial features\n# poly = PolynomialFeatures(degree=1)\n# poly_results_train = poly.fit_transform(train.drop([\"y\"], axis=1))\n# poly_results_test = poly.transform(test)\n\n#sparse PCA\nspca = SparsePCA(n_components=n_comp, random_state=420)\nspca2_results_train = spca.fit_transform(train_df.drop([\"isFraud\"], axis=1))\nspca2_results_test = spca.transform(test_df)\n\n#Kernel PCA\n# kpca = KernelPCA(n_components=n_comp, random_state=420)\n# kpca2_results_train = kpca.fit_transform(train)\n# kpca2_results_test = kpca.transform(test)\n\n# ICA\nica = FastICA(n_components=n_comp, random_state=420)\nica2_results_train = ica.fit_transform(train_df.drop([\"isFraud\"], axis=1))\nica2_results_test = ica.transform(test_df)\n\n# GRP\ngrp = GaussianRandomProjection(n_components=n_comp, eps=0.1, random_state=420)\ngrp_results_train = grp.fit_transform(train_df.drop([\"isFraud\"], axis=1))\ngrp_results_test = grp.transform(test_df)\n\n# SRP\nsrp = SparseRandomProjection(n_components=n_comp, dense_output=True, random_state=420)\nsrp_results_train = srp.fit_transform(train_df.drop([\"isFraud\"], axis=1))\nsrp_results_test = srp.transform(test_df)\n\n\n\n# Append decomposition components to datasets\nfor i in range(1, n_comp + 1):\n    # train['pca_' + str(i)] = pca2_results_train[:, i - 1]\n    # test['pca_' + str(i)] = pca2_results_test[:, i - 1]\n\n    # train['poly_' + str(i)] = poly_results_train[:, i - 1]\n    # test['poly_' + str(i)] = poly_results_test[:, i - 1]\n\n    train_df['spca_' + str(i)] = spca2_results_train[:, i - 1]\n    test_df['spca_' + str(i)] = spca2_results_test[:, i - 1]\n\n#     train['kpca_' + str(i)] = kpca2_results_train[:, i - 1]\n#     test['kpca_' + str(i)] = kpca2_results_test[:, i - 1]\n\n    train_df['ica_' + str(i)] = ica2_results_train[:, i - 1]\n    test_df['ica_' + str(i)] = ica2_results_test[:, i - 1]\n\n    train_df['grp_' + str(i)] = grp_results_train[:, i - 1]\n    test_df['grp_' + str(i)] = grp_results_test[:, i - 1]\n\n    train_df['srp_' + str(i)] = srp_results_train[:, i - 1]\n    test_df['srp_' + str(i)] = srp_results_test[:, i - 1]\n\n\nprint(\"After PCA/ICA\")\nprint (len(list(train_df)))\nprint (len(list(test_df)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = list(train_df)\nfeatures.remove('isFraud')\ntarget = 'isFraud'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='2'>2. Bayesian Optimisation</a> "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#cut tr and val\nbayesian_tr_idx, bayesian_val_idx = train_test_split(train_df, test_size = 0.5, random_state = 42, stratify = train_df[target])\nbayesian_tr_idx = bayesian_tr_idx.index\nbayesian_val_idx = bayesian_val_idx.index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#black box LGBM \ndef LGB_bayesian(\n    learning_rate,\n    num_leaves, \n    bagging_fraction,\n    feature_fraction,\n    min_child_samples, \n    min_child_weight,\n    subsample, \n    min_data_in_leaf,\n    max_depth,\n    colsample_bytree,\n    reg_alpha,\n    reg_lambda\n     ):\n    \n    # LightGBM expects next three parameters need to be integer. \n    num_leaves = int(num_leaves)\n    min_data_in_leaf = int(min_data_in_leaf)\n    max_depth = int(max_depth)\n\n    assert type(num_leaves) == int\n    assert type(min_data_in_leaf) == int\n    assert type(max_depth) == int\n    \n\n    param = {\n              'num_leaves': num_leaves, \n              'min_child_samples': min_child_samples, \n              'min_data_in_leaf': min_data_in_leaf,\n              'min_child_weight': min_child_weight,\n              'bagging_fraction' : bagging_fraction,\n              'feature_fraction' : feature_fraction,\n              'subsample': subsample, \n              'max_depth': max_depth,\n              'colsample_bytree': colsample_bytree,\n              'reg_alpha': reg_alpha,\n              'reg_lambda': reg_lambda,\n              'objective': 'binary',\n              'save_binary': True,\n              'seed': 1337,\n              'feature_fraction_seed': 1337,\n              'bagging_seed': 1337,\n              'drop_seed': 1337,\n              'data_random_seed': 1337,\n              'boosting_type': 'gbdt',\n              'verbose': 1,\n              'is_unbalance': False,\n              'boost_from_average': True,\n              'metric':'auc'}    \n    \n    oof = np.zeros(len(train_df))\n    trn_data= lgb.Dataset(train_df.iloc[bayesian_tr_idx][features].values, label=train_df.iloc[bayesian_tr_idx][target].values)\n    val_data= lgb.Dataset(train_df.iloc[bayesian_val_idx][features].values, label=train_df.iloc[bayesian_val_idx][target].values)\n\n    clf = lgb.train(param, trn_data,  num_boost_round=50, valid_sets = [trn_data, val_data], verbose_eval=50, early_stopping_rounds = 50)\n    \n    oof[bayesian_val_idx]  = clf.predict(train_df.iloc[bayesian_val_idx][features].values, num_iteration=clf.best_iteration)  \n    \n    score = roc_auc_score(train_df.iloc[bayesian_val_idx][target].values, oof[bayesian_val_idx])\n\n    return score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Bounded region of parameter space\nbounds_LGB = {\n    'num_leaves': (5, 200), \n    'min_data_in_leaf': (5, 200),\n    'bagging_fraction' : (0.1,0.9),\n    'feature_fraction' : (0.1,0.9),\n    'learning_rate': (0.01, 0.3),\n    'min_child_weight': (0.00001, 0.01),   \n    'min_child_samples':(100, 500), \n    'subsample': (0.2, 0.8),\n    'colsample_bytree': (0.4, 0.6), \n    'reg_alpha': (1, 2), \n    'reg_lambda': (1, 2),\n    'max_depth':(-1,15),\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LGB_BO = BayesianOptimization(LGB_bayesian, bounds_LGB, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(LGB_BO.space.keys)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"init_points = 30\nn_iter = 3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('-' * 130)\n\nwith warnings.catch_warnings():\n    warnings.filterwarnings('ignore')\n    LGB_BO.maximize(init_points=init_points, n_iter=n_iter, acq='ucb', xi=0.0, alpha=1e-6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LGB_BO.max['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LGB_BO.max['params']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## CONFUSION MATRIX"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Confusion matrix \ndef plot_confusion_matrix(cm, classes,\n                          normalize = False,\n                          title = 'Confusion matrix\"',\n                          cmap = plt.cm.Blues) :\n    plt.imshow(cm, interpolation = 'nearest', cmap = cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation = 0)\n    plt.yticks(tick_marks, classes)\n\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])) :\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment = 'center',\n                 color = 'white' if cm[i, j] > thresh else 'black')\n \n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='3'>3. LGB + best hyperparameters</a> "},{"metadata":{"trusted":true},"cell_type":"code","source":"param_lgb = {\n        'min_data_in_leaf': int(LGB_BO.max['params']['min_data_in_leaf']), \n        'num_leaves': int(LGB_BO.max['params']['num_leaves']), \n        'learning_rate': LGB_BO.max['params']['learning_rate'],\n        'min_child_weight': LGB_BO.max['params']['min_child_weight'],\n        'colsample_bytree' : LGB_BO.max['params']['colsample_bytree'],\n        'bagging_fraction': LGB_BO.max['params']['bagging_fraction'], \n        'min_child_samples': LGB_BO.max['params']['min_child_samples'],\n        'subsample': LGB_BO.max['params']['subsample'],\n        'reg_lambda': LGB_BO.max['params']['reg_lambda'],\n        'reg_alpha': LGB_BO.max['params']['reg_alpha'],\n        'max_depth': int(LGB_BO.max['params']['max_depth']), \n        'objective': 'binary',\n        'save_binary': True,\n        'seed': 1337,\n        'feature_fraction_seed': 1337,\n        'bagging_seed': 1337,\n        'drop_seed': 1337,\n        'data_random_seed': 1337,\n        'boosting_type': 'gbdt',\n        'verbose': 1,\n        'is_unbalance': False,\n        'boost_from_average': True,\n        'metric':'auc'\n    }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"nfold = 5\nskf = StratifiedKFold(n_splits=nfold, shuffle=True, random_state=42)\n\noof = np.zeros(len(train_df))\nmean_fpr = np.linspace(0,1,100)\ncms= []\ntprs = []\naucs = []\nrecalls = []\nroc_aucs = []\nf1_scores = []\naccuracies = []\nprecisions = []\npredictions = np.zeros(len(test_df))\nfeature_importance_df = pd.DataFrame()\n\ni = 1\nfor train_idx, valid_idx in skf.split(train_df, train_df.isFraud.values):\n    print(\"\\nfold {}\".format(i))\n    trn_data = lgb.Dataset(train_df.iloc[train_idx][features].values,\n                                   label=train_df.iloc[train_idx][target].values\n                                   )\n    val_data = lgb.Dataset(train_df.iloc[valid_idx][features].values,\n                                   label=train_df.iloc[valid_idx][target].values\n                                   )   \n    \n    clf = lgb.train(param_lgb, trn_data, num_boost_round=800, valid_sets = [trn_data, val_data], verbose_eval=100, early_stopping_rounds = 100)\n    oof[valid_idx] = clf.predict(train_df.iloc[valid_idx][features].values) \n    \n    predictions += clf.predict(test_df[features]) / nfold\n    \n    # Scores \n    roc_aucs.append(roc_auc_score(train_df.iloc[valid_idx][target].values, oof[valid_idx]))\n    accuracies.append(accuracy_score(train_df.iloc[valid_idx][target].values, oof[valid_idx].round()))\n    recalls.append(recall_score(train_df.iloc[valid_idx][target].values, oof[valid_idx].round()))\n    precisions.append(precision_score(train_df.iloc[valid_idx][target].values ,oof[valid_idx].round()))\n    f1_scores.append(f1_score(train_df.iloc[valid_idx][target].values, oof[valid_idx].round()))\n    \n    # Roc curve by fold\n    fpr, tpr, t = roc_curve(train_df.iloc[valid_idx][target].values, oof[valid_idx])\n    tprs.append(interp(mean_fpr, fpr, tpr))\n    roc_auc = auc(fpr, tpr)\n    aucs.append(roc_auc)\n    plt.plot(fpr, tpr, lw=2, alpha=0.3, label='ROC fold %d (AUC = %0.4f)' % (i,roc_auc))\n    i= i+1\n    \n    # Confusion matrix by folds\n    cms.append(confusion_matrix(train_df.iloc[valid_idx][target].values, oof[valid_idx].round()))\n    \n    # Features imp\n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = features\n    fold_importance_df[\"importance\"] = clf.feature_importance()\n    fold_importance_df[\"fold\"] = nfold + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n\n# Metrics\n\nroc_score  = np.mean(roc_aucs)\nprint(\n        '\\nCV roc score        : {0:.4f}, std: {1:.4f}.'.format(roc_score, np.std(roc_aucs)),\n        '\\nCV accuracy score   : {0:.4f}, std: {1:.4f}.'.format(np.mean(accuracies), np.std(accuracies)),\n        '\\nCV recall score     : {0:.4f}, std: {1:.4f}.'.format(np.mean(recalls), np.std(recalls)),\n        '\\nCV precision score  : {0:.4f}, std: {1:.4f}.'.format(np.mean(precisions), np.std(precisions)),\n        '\\nCV f1 score         : {0:.4f}, std: {1:.4f}.'.format(np.mean(f1_scores), np.std(f1_scores))\n)\n\nplt.plot([0,1],[0,1],linestyle = '--',lw = 2,color = 'grey')\nmean_tpr = np.mean(tprs, axis=0)\nmean_auc = auc(mean_fpr, mean_tpr)\nplt.plot(mean_fpr, mean_tpr, color='blue',\n         label=r'Mean ROC (AUC = %0.4f)' % (np.mean(roc_aucs)),lw=2, alpha=1)\n\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('LGB ROC curve by folds')\nplt.legend(loc=\"lower right\")\nplt.show()\n\n# Confusion maxtrix & metrics\ncm = np.average(cms, axis=0)\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(cm, \n                      classes=class_names, \n                      title= 'LGB Confusion matrix [averaged/folds]')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='4'>4. Features importance</a> "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.style.use('dark_background')\ncols = (feature_importance_df[[\"Feature\", \"importance\"]]\n    .groupby(\"Feature\")\n    .mean()\n    .sort_values(by=\"importance\", ascending=False)[:30].index)\nbest_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n\nplt.figure(figsize=(10,10))\nsns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False),\n        edgecolor=('white'), linewidth=2, palette=\"rocket\")\nplt.title('LGB Features importance (averaged/folds)', fontsize=18)\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='5'>5. Submission</a> "},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission['isFraud'] = predictions\nsample_submission.to_csv('LGB_Bayesian_PCA_{}.csv'.format(roc_score))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}