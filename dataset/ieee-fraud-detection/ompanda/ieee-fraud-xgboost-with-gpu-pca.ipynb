{"cells":[{"metadata":{},"cell_type":"markdown","source":"# About this kernel\n\nBefore I get started, I just wanted to say: huge props to Inversion! The official starter kernel is **AWESOME**; it's so simple, clean, straightforward, and pragmatic. It certainly saved me a lot of time wrangling with data, so that I can directly start tuning my models (real data scientists will call me lazy, but hey I'm an engineer I just want my stuff to work).\n\nI noticed two tiny problems with it:\n* It takes a lot of RAM to run, which means that if you are using a GPU, it might crash as you try to fill missing values.\n* It takes a while to run (roughly 3500 seconds, which is more than an hour; again, I'm a lazy guy and I don't like waiting).\n\nWith this kernel, I bring some small changes:\n* Decrease RAM usage, so that it won't crash when you change it to GPU. I simply changed when we are deleting unused variables.\n* Decrease **running time from ~3500s to ~40s** (yes, that's almost 90x faster), at the cost of a slight decrease in score. This is done by adding a single argument.\n\nAgain, my changes are super minimal (cause Inversion's kernel was already so awesome), but I hope it will save you some time and trouble (so that you can start working on cool stuff).\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\nimport xgboost as xgb\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA, FastICA,SparsePCA,KernelPCA\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.manifold import TSNE\nfrom sklearn.random_projection import GaussianRandomProjection\nfrom sklearn.random_projection import SparseRandomProjection","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Efficient Preprocessing\n\nThis preprocessing method is more careful with RAM usage, which avoids crashing the kernel when you switch from CPU to GPU."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transaction = pd.read_csv('../input/train_transaction.csv', index_col='TransactionID')\ntest_transaction = pd.read_csv('../input/test_transaction.csv', index_col='TransactionID')\n\ntrain_identity = pd.read_csv('../input/train_identity.csv', index_col='TransactionID')\ntest_identity = pd.read_csv('../input/test_identity.csv', index_col='TransactionID')\n\nsample_submission = pd.read_csv('../input/sample_submission.csv', index_col='TransactionID')\n\ntrain = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\ntest = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)\n\nprint(train.shape)\nprint(test.shape)\n\ny = train['isFraud'].copy()\ndel train_transaction, train_identity, test_transaction, test_identity\n\n# Drop target, fill in NaNs\ntrain = train.drop('isFraud', axis=1)\n\ntrain = train.fillna(-999)\ntest = test.fillna(-999)\n\n# Label Encoding\nfor f in train.columns:\n    if train[f].dtype=='object' or test[f].dtype=='object': \n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(train[f].values) + list(test[f].values))\n        train[f] = lbl.transform(list(train[f].values))\n        test[f] = lbl.transform(list(test[f].values))   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#PCA/ICA for dimensionality reduction\n\nn_comp = 12\n\n# tSVD\ntsvd = TruncatedSVD(n_components=n_comp, random_state=420)\ntsvd_results_train = tsvd.fit_transform(train)\ntsvd_results_test = tsvd.transform(test)\n\n# # PCA\n# pca = PCA(n_components=n_comp, random_state=420)\n# pca2_results_train = pca.fit_transform(train.drop([\"y\"], axis=1))\n# pca2_results_test = pca.transform(test)\n#\n\n#Polynomial features\n# poly = PolynomialFeatures(degree=1)\n# poly_results_train = poly.fit_transform(train.drop([\"y\"], axis=1))\n# poly_results_test = poly.transform(test)\n\n#sparse PCA\nspca = SparsePCA(n_components=n_comp, random_state=420)\nspca2_results_train = spca.fit_transform(train)\nspca2_results_test = spca.transform(test)\n\n#Kernel PCA\n# kpca = KernelPCA(n_components=n_comp, random_state=420)\n# kpca2_results_train = kpca.fit_transform(train)\n# kpca2_results_test = kpca.transform(test)\n\n# ICA\nica = FastICA(n_components=n_comp, random_state=420)\nica2_results_train = ica.fit_transform(train)\nica2_results_test = ica.transform(test)\n\n# GRP\ngrp = GaussianRandomProjection(n_components=n_comp, eps=0.1, random_state=420)\ngrp_results_train = grp.fit_transform(train)\ngrp_results_test = grp.transform(test)\n\n# SRP\nsrp = SparseRandomProjection(n_components=n_comp, dense_output=True, random_state=420)\nsrp_results_train = srp.fit_transform(train)\nsrp_results_test = srp.transform(test)\n\n\n\n# Append decomposition components to datasets\nfor i in range(1, n_comp + 1):\n    # train['pca_' + str(i)] = pca2_results_train[:, i - 1]\n    # test['pca_' + str(i)] = pca2_results_test[:, i - 1]\n\n    # train['poly_' + str(i)] = poly_results_train[:, i - 1]\n    # test['poly_' + str(i)] = poly_results_test[:, i - 1]\n\n    train['spca_' + str(i)] = spca2_results_train[:, i - 1]\n    test['spca_' + str(i)] = spca2_results_test[:, i - 1]\n\n#     train['kpca_' + str(i)] = kpca2_results_train[:, i - 1]\n#     test['kpca_' + str(i)] = kpca2_results_test[:, i - 1]\n\n    train['ica_' + str(i)] = ica2_results_train[:, i - 1]\n    test['ica_' + str(i)] = ica2_results_test[:, i - 1]\n\n    train['grp_' + str(i)] = grp_results_train[:, i - 1]\n    test['grp_' + str(i)] = grp_results_test[:, i - 1]\n\n    train['srp_' + str(i)] = srp_results_train[:, i - 1]\n    test['srp_' + str(i)] = srp_results_test[:, i - 1]\n\n\nprint(\"After PCA/ICA\")\nprint (len(list(train)))\nprint (len(list(test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(train, y, test_size=0.33, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training\n\nDAYS OF RESEARCH BROUGHT ME TO THE CONCLUSION THAT I SHOULD SIMPLY SPECIFY `tree_method='gpu_hist'` IN ORDER TO ACTIVATE GPU (okay jk, took me an hour to figure out, but I wish XGBoost documentation was more clear about that)."},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = xgb.XGBClassifier(\n    n_estimators=500,\n    max_depth=9,\n    learning_rate=0.05,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    missing=-999,\n    tree_method='gpu_hist'  # THE MAGICAL PARAMETER\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time clf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_pred = clf.predict(X_test)\n\nscore = roc_auc_score(y_test,y_test_pred )\nprint   (\"score is {} \".format(score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del X_train, X_test, y_train, y_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some of you must be wondering how we were able to decrease the fitting time by that much. The reason for that is not only we are running on gpu, but we are also computing an approximation of the real underlying algorithm (which is a greedy algorithm). This hurts your score slightly, but as a result is much faster.\n\nSo why am I not using CPU with `tree_method='hist'`? If you try it out yourself, you'll realize it'll take ~ 7 min, which is still far from the GPU fitting time. Similarly, `tree_method='gpu_exact'` will take ~ 4 min, but likely yields better accuracy than `gpu_hist` or `hist`.\n\nThe [docs on parameters](https://xgboost.readthedocs.io/en/latest/parameter.html) has a section on `tree_method`, and it goes over the details of each option."},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission['isFraud'] = clf.predict_proba(test)[:,1]\nsample_submission.to_csv('xgboost_v1_{}.csv'.format(score))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}