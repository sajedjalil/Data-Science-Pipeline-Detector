{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Anomaly Detection with AutoEncoder (pytorch)\nHi! I'm new to kaggle, and this is my first competition in my life. \nIf you notice the better way about my implementation please tell me! :)\n\nIn past fraud detection competition, some people used auto encoder approach to detect anomalous for fraud data.\nThere are categorical variables in the case, however, I thought the same approach can be applied to the task.\nSo I just tried it.\n\nTODO:\n- Current implementation is incorrect. training should be done by using only non fraud data.\n\nhttps://www.kaggle.com/jdoz22/detecting-fraud-using-an-autoencoder-and-pytorch?scriptVersionId=2802227\nhttps://www.kaggle.com/artgor/eda-and-models/notebook"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\nimport xgboost as xgb\nimport seaborn as sns\nimport collections\n\nfrom tqdm import tqdm, tqdm_notebook\n\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import (confusion_matrix, precision_recall_curve, auc,\n                             roc_curve, recall_score, classification_report, f1_score)\nfrom sklearn.metrics import accuracy_score, precision_score\n                            \nimport torch.nn as nn\nfrom torch.autograd import Variable as V\nimport torch.nn.functional as F\nimport torch\nfrom torch.utils.data import DataLoader\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom bokeh.plotting import figure, output_notebook, show, ColumnDataSource\nfrom bokeh.models import HoverTool, NumeralTickFormatter\nfrom bokeh.palettes import Set3_12\nfrom bokeh.transform import jitter\n\nimport gc\ngc.enable()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training epochs\nepochs=10","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transaction = pd.read_csv('../input/train_transaction.csv', index_col='TransactionID')\ntest_transaction = pd.read_csv('../input/test_transaction.csv', index_col='TransactionID')\n\ntrain_identity = pd.read_csv('../input/train_identity.csv', index_col='TransactionID')\ntest_identity = pd.read_csv('../input/test_identity.csv', index_col='TransactionID')\n\nsample_submission = pd.read_csv('../input/sample_submission.csv', index_col='TransactionID')\n\ntrain = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\ntest = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)\n\nprint(train.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Drop columns and Standard Scaling"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop columns\ndef dropper(column_name, train, test):\n    train = train.drop(column_name, axis=1)\n    test = test.drop(column_name, axis=1)\n    return train, test\n\ndel_columns = ['TransactionDT']\nfor col in del_columns:\n    train, test = dropper(col, train, test)\n\ndef scaler(scl, column_name, data):\n    data[column_name] = scl.fit_transform(data[column_name].values.reshape(-1,1))\n    return data\n\nscl_columns = ['TransactionAmt', 'card1', 'card3', 'card5', 'addr1', 'addr2']\nfor col in scl_columns:\n    train = scaler(StandardScaler(), col, train)\n    test = scaler(StandardScaler(), col, test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n#TODO: Learning should be done by using non fraud data\ntrain = train[train['isFraud'] == 0]\ntrain_fraud = train[train['isFraud'] == 1].copy()\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# AutoEncoder\n## Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = train['isFraud'].copy()\ndel train_transaction, train_identity, test_transaction, test_identity\n\n# Drop target\nX_train = train.drop('isFraud', axis=1)\n#X_train_fraud = train_fraud.drop('isFraud', axis=1)\nX_test = test.copy()\n\ndel train, test\n    \n# TODO: change methods\n# Fill in NaNs\nX_train = X_train.fillna(-999)\n#X_train_fraud = X_train_fraud.fillna(-999)\nX_test = X_test.fillna(-999)\n\n# TODO: change to Label Count Endocing\n# Label Encoding\nfor f in X_train.columns:\n    if X_train[f].dtype=='object' or X_test[f].dtype=='object': \n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(X_train[f].values) + list(X_test[f].values)) #+ list(X_train_fraud[f].values))\n        X_train[f] = lbl.transform(list(X_train[f].values))\n        #X_train_fraud[f] = lbl.transform(list(X_train_fraud[f].values)) \n        X_test[f] = lbl.transform(list(X_test[f].values)) \n        \ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train.head())\n#print(X_train_fraud.head())\nprint(X_test.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n    params:\n        data : data desired to be split\n        ratio : validation ratio for split\n        \n    output:\n        train_data, validation_data\n\"\"\"\n\ndef splitter(data, ratio=0.2):\n    num = int(ratio*len(data))\n    return data[num:], data[:num]\n\nX_train, X_val = splitter(X_train)\ny_train, y_val = splitter(y_train)\n\n# Check number of data\nprint(len(X_train), len(X_val), len(y_train), len(y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xtr = torch.FloatTensor(X_train.values)\nxts = torch.FloatTensor(X_test.values)\n# X_val: validation data for isFraud == 0\nxvl = torch.FloatTensor(X_val.values) \n# X_train_fraud: validation data for isFraud == 1\n#xvt = torch.FloatTensor(X_train_fraud.values)\n\nxdl = DataLoader(xtr,batch_size=1000)\ntdl = DataLoader(xts,batch_size=1000)\nvdl = DataLoader(xvl,batch_size=1000)\n#fdl = DataLoader(xvt,batch_size=1000)\n\nprint(len(X_train.values), len(X_test.values), len(X_val.values)) #, len(X_train_fraud))\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class AutoEncoder(nn.Module):\n    def __init__(self, length):\n        super().__init__()\n        self.lin1 = nn.Linear(length,20)\n        self.lin2 = nn.Linear(20,10)\n        self.lin7 = nn.Linear(10,20)\n        self.lin8 = nn.Linear(20,length)\n        \n        self.drop2 = nn.Dropout(0.05)\n        \n        self.lin1.weight.data.uniform_(-2,2)\n        self.lin2.weight.data.uniform_(-2,2)\n        self.lin7.weight.data.uniform_(-2,2)\n        self.lin8.weight.data.uniform_(-2,2)\n\n    def forward(self, data):\n        x = F.tanh(self.lin1(data))\n        x = self.drop2(F.tanh(self.lin2(x)))\n        x = F.tanh(self.lin7(x))\n        x = self.lin8(x)\n        return x\n    \ndef score(x):\n    y_pred = model(V(x))\n    x1 = V(x)\n    return loss(y_pred,x1).item()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = AutoEncoder(len(X_train.columns))\nloss=nn.MSELoss()\nlearning_rate = 1e-2\noptimizer=torch.optim.Adam(model.parameters(), lr=learning_rate)\nprint(model)\n\n# Utilize a named tuple to keep track of scores at each epoch\nmodel_hist = collections.namedtuple('Model','epoch loss val_loss')\nmodel_loss = model_hist(epoch = [], loss = [], val_loss = [])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(epochs, model, model_loss):\n    try: c = model_loss.epoch[-1]\n    except: c = 0\n    for epoch in tqdm_notebook(range(epochs),position=0, total = epochs):\n        losses=[]\n        dl = iter(xdl)\n        for t in range(len(dl)):\n            # Forward pass: compute predicted y and loss by passing x to the model.\n            xt = next(dl)\n            y_pred = model(V(xt))\n            \n            l = loss(y_pred,V(xt))\n            losses.append(l)\n            optimizer.zero_grad()\n\n            # Backward pass: compute gradient of the loss with respect to model parameters\n            l.backward()\n\n            # Calling the step function on an Optimizer makes an update to its parameters\n            optimizer.step()\n            \n        val_dl = iter(tdl)\n        val_scores = [score(next(val_dl)) for i in range(len(val_dl))]\n        \n        model_loss.epoch.append(c+epoch)\n        model_loss.loss.append(l.item())\n        model_loss.val_loss.append(np.mean(val_scores))\n        print(f'Epoch: {epoch}   Loss: {l.item():.4f}    Val_Loss: {np.mean(val_scores):.4f}')\n\ntrain(model=model, epochs=epochs, model_loss=model_loss)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Check Loss/Validation Loss"},{"metadata":{"trusted":true},"cell_type":"code","source":"x = np.linspace(0, epochs-1, epochs)\nprint(model_loss.loss)\nprint(model_loss.val_loss)\nprint(x)\nplt.plot(x, model_loss.loss, label=\"loss\")\nplt.legend()\nplt.show()\n\nplt.plot(x, model_loss.val_loss, label=\"val_loss\")\nplt.legend()\nplt.show()\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Iterate through the dataloader and get predictions for each batch of the test set.\np = iter(vdl)\npreds = np.vstack([model(V(next(p))).cpu().data.numpy() for i in range(len(p))])\n\n# Create a pandas DF that shows the Autoencoder MSE vs True Labels\nerror_nonfraud = np.mean(np.power((X_val-preds),2), axis=1)\n\"\"\"\np = iter(fdl)\npreds = np.vstack([model(V(next(p))).cpu().data.numpy() for i in range(len(p))])\nerror_fraud = np.mean(np.power((X_train_fraud-preds),2), axis=1)\n\npd.DataFrame(error_fraud)\n\"\"\"\nerror_df = pd.DataFrame(data = {'error':error_nonfraud,'true':y_val})\n\nerror_df.groupby('true')['error'].describe().reset_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ROC_AUC"},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr, tpr, thresholds = roc_curve(error_df.true, error_df.error)\nroc_auc = auc(fpr, tpr)\n\nplt.plot(fpr, tpr, label='ROC curve (area = {})'.format(roc_auc))\nplt.legend()\nplt.title('ROC curve')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.grid(True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_df = error_df[error_df['true'] == 0]\nthreshold = temp_df['error'].mean() + temp_df['error'].std()\nprint(f'Threshold: {threshold:.3f}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Precision Recall F1-Score"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = [1 if e > threshold else 0 for e in error_df.error.values]\nprint(classification_report(error_df.true.values,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plot Precision Recall"},{"metadata":{"trusted":true},"cell_type":"code","source":"conf_matrix = confusion_matrix(error_df.true, y_pred)\n\nsns.set(font_scale = 1.2)\nplt.figure(figsize=(10, 10))\nsns.heatmap(conf_matrix, xticklabels=['Not Fraud','Fraud'], yticklabels=['Not Fraud','Fraud'], annot=True, fmt=\"d\");\nplt.title(\"Confusion matrix\")\nplt.ylabel('True class')\nplt.xlabel('Predicted class')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plot Precision Recall for each thresholds\nChanging the threshold from threshold_min to threshold_max."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 12))\nm = []\nthreshold_min = threshold * 0.99\nthreshold_max = threshold * 1.01\n\nfor thresh in np.linspace(threshold_min, threshold_max):\n    y_pred = [1 if e > thresh else 0 for e in error_df.error.values]\n    conf_matrix = confusion_matrix(error_df.true, y_pred)\n    m.append((conf_matrix,thresh))\n    \ncount = 0\nfor i in range(3):\n    for j in range(3):\n        plt.subplot2grid((3, 3), (i, j))\n        sns.heatmap(m[count][0], xticklabels=['Not Fraud','Fraud'], yticklabels=['Not Fraud','Fraud'], annot=True, fmt=\"d\");\n        plt.title(f\"Threshold - {m[count][1]:.3f}\")\n        plt.ylabel('True class')\n        plt.xlabel('Predicted class')\n        plt.tight_layout()\n        count += 1\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Iterate through the dataloader and get predictions for each batch of the test set.\np = iter(tdl)\npreds = np.vstack([model(V(next(p))).cpu().data.numpy() for i in range(len(p))])\n\n# Create a pandas DF that shows the Autoencoder MSE vs True Labels\nerror = np.mean(np.power((X_test-preds),2), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def min_max_normalization(x):\n    x_min = x.min()\n    x_max = x.max()\n    x_norm = (x-x_min) / (x_max-x_min)\n    return x_norm\n\n# min max normalization\n#error_df = pd.DataFrame(data={'isFraud':min_max_normalization(error)})\nerror_df = pd.DataFrame(data={'isFraud':error})\n\nprint(\"Num data: \" + str(len(error_df)))\nprint(\"Beyond threshold num data: \" + str(len(error_df[error_df['isFraud'] > threshold])))\n#error_df[error_df['isFraud'] > threshold]\n\nx_min = 3600000\nx_max = 4200000\nplt.hlines(threshold, x_min, x_max, \"black\")\nplt.plot(error_df, alpha=0.3)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"error_df = pd.DataFrame(data={'isFraud':min_max_normalization(error)})\nerror_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission['isFraud'] = error_df\nsample_submission.to_csv('simple_autoencoder.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# To be continued\n\n# Feature Engineering\n## Day/Hour features from TransactionDT"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making day/hour features. See the kernel below:\n# https://www.kaggle.com/c/ieee-fraud-detection/discussion/100400#latest-579480\n\"\"\"\ndef make_day_feature(df, offset=0, tname='TransactionDT'):\n    # found a good offset is 0.58\n    days = df[tname] / (3600*24)        \n    encoded_days = np.floor(days-1+offset) % 7\n    return encoded_days\n\ndef make_hour_feature(df, tname='TransactionDT'):\n    hours = df[tname] / (3600)        \n    encoded_hours = np.floor(hours) % 24\n    return encoded_hours\n\n# check train\nprint(make_day_feature(train, offset=0.58).value_counts().head(10))\nprint(make_hour_feature(train).value_counts().head(10))\n\ntrain['TransactionDay'] = make_day_feature(train, offset=0.58)\ntrain['TransactionHour'] = make_hour_feature(train)\ntest['TransactionDay'] = make_day_feature(test, offset=0.58)\ntest['TransactionHour'] = make_hour_feature(test)\n\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n# Delete columns\ndrop_true = False\nif(drop_true):\n    drop_col = ['V300', 'V309', 'V111', 'C3', 'V124', 'V106', 'V125', 'V315', 'V134', 'V102', 'V123', 'V316', 'V113', 'V136', 'V305', 'V110', 'V299', 'V289', 'V286', 'V318', 'V103', 'V304', 'V116', 'V298', 'V284', 'V293', 'V137', 'V295', 'V301', 'V104', 'V311', 'V115', 'V109', 'V119', 'V321', 'V114', 'V133', 'V122', 'V319', 'V105', 'V112', 'V118', 'V117', 'V121', 'V108', 'V135', 'V320', 'V303', 'V297', 'V120']\n    X_train.drop(drop_col,axis=1, inplace=True)\n    X_test.drop(drop_col, axis=1, inplace=True)\n    X_train.head()\n    \n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\ncorrelation_matrix = X_train.corr()\nfig = plt.figure(figsize=(12,9))\nsns.heatmap(correlation_matrix,vmax=0.8,square = True)\n\nplt.show()\n\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training\n\nDAYS OF RESEARCH BROUGHT ME TO THE CONCLUSION THAT I SHOULD SIMPLY SPECIFY `tree_method='gpu_hist'` IN ORDER TO ACTIVATE GPU (okay jk, took me an hour to figure out, but I wish XGBoost documentation was more clear about that)."},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nfrom sklearn.cluster import KMeans\nKMeans(n_clusters=2).fit(X_train)\n\nclf = xgb.XGBClassifier(\n    n_estimators=500,\n    max_depth=9,\n    learning_rate=0.05,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    missing=-999,\n    tree_method='gpu_hist'  # THE MAGICAL PARAMETER\n)\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## KFold and validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\ndef get_avg_auc_kfold(X_train, y_train, X_test, clf, NFOLDS=2, shuffle=True):\n    kf = KFold(n_splits=NFOLDS, shuffle=shuffle)\n    y_preds = np.zeros(X_test.shape[0])\n    y_oof = np.zeros(X_train.shape[0])\n    score = 0\n  \n    for fold, (tr_idx, val_idx) in enumerate(kf.split(X_train, y_train)):    \n        X_tr, X_vl = X_train.iloc[tr_idx, :], X_train.iloc[val_idx, :]\n        y_tr, y_vl = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n        clf.fit(X_tr, y_tr)\n        y_pred_train = clf.predict_proba(X_vl)[:,1]\n        y_oof[val_idx] = y_pred_train\n        print(\"FOLD: \",fold,' AUC {}'.format(roc_auc_score(y_vl, y_pred_train)))\n        score += roc_auc_score(y_vl, y_pred_train) / NFOLDS\n        y_preds+= clf.predict_proba(X_test)[:,1] / NFOLDS\n    \n        del X_tr, X_vl, y_tr, y_vl\n        gc.collect()\n    print (\">> Avg AUC: \", score)\n    return score\n\nexecuteKFold = False\nif(executeKFold):\n    print(get_avg_auc_kfold(X_train, y_train, X_test, clf))\n    \n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train\n## training and visualizing all importances of futures"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n# Training\n%time clf.fit(X_train, y_train)\n\n# Get xgBoost importances\nimportance_dict = {}\nfor import_type in ['weight', 'gain', 'cover']:\n    importance_dict['xgBoost-'+import_type] = clf.get_booster().get_score(importance_type=import_type)\n    \n# MinMax scale all importances\nimportance_df = pd.DataFrame(importance_dict).fillna(0)\nimportance_df = pd.DataFrame(\n    preprocessing.MinMaxScaler().fit_transform(importance_df),\n    columns=importance_df.columns,\n    index=importance_df.index\n)\n\n# Create mean column\nimportance_df['mean'] = importance_df.mean(axis=1)\n\n# Plot the feature importances\nimportance_df.sort_values('mean').head(50).plot(kind='bar', figsize=(20, 7))\n\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nsample_submission['isFraud'] = clf.predict_proba(X_test)[:,1]\nsample_submission.to_csv('simple_xgboost.csv')\n\"\"\"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}