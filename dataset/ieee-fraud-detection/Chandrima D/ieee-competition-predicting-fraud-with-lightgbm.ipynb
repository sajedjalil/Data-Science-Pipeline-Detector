{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\ntrain_transact = pd.read_csv('/kaggle/input/ieee-fraud-detection/train_transaction.csv', index_col='TransactionID')\ntrain_id = pd.read_csv('/kaggle/input/ieee-fraud-detection/train_identity.csv', index_col='TransactionID')\ntest_transact = pd.read_csv('/kaggle/input/ieee-fraud-detection/test_transaction.csv', index_col='TransactionID')\ntest_id = pd.read_csv('/kaggle/input/ieee-fraud-detection/test_identity.csv', index_col='TransactionID')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Step 1:** \nMerging transaction data set with identification data set to form complete (workable) data sets","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = train_transact.merge(train_id,how='left',left_index=True,right_index=True)\ntest = test_transact.merge(test_id,how='left',left_index=True,right_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()\ntest.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We observe that the size of both train and test datasets are considerably large (1.9+ GB and 1.7+ GB respectively). So we need to take care of memory allocation and will have to keep on deleting redundant (non-required) datasets time-to-time while running the script.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Freeing up some space\ndel train_transact, train_id, test_transact, test_id","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Step 2: ** \nWe need to compress the working data sets (train and test) so that we do not encounter any interruption due to shortage of memory while working with the large data sets.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Taken from https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\nimport numpy as np\ndef reduce_mem_usage(df):\n   \n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reduce_mem_usage(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reduce_mem_usage(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visibly, the sizes of datasets have been reduced from 1.9 GB to around 500 MB, and from 1.7 GB to 480 MB respectively. The compression function has worked very fine. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# **Step 3:**\nNow we will proceed to check whether the training data set is imbalanced. If the number of instances having fraud label positive is very low in number (rare event), then we need to balance the data first. This step is highly recommended to judge the sensitivity of a classifier to the training data set. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfig = plt.figure(figsize = (8,5))\ntrain.isFraud.value_counts(normalize = True).plot(kind='bar', color= ['skyblue','red'], alpha = 0.8)\nplt.title('Fraud and Non-Fraud (Imbalanced Dataset)')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The training data set is clearly imbalanced having only about 2% of fraudulent transactions and rest 98% legit transactions respectively. We will follow **\"Undersampling Strategy\"** to balance the data set. Since the data set is very large, we avoided using oversampling.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Undersampling majority to remove Class Imbalance\nimport pandas as pds\nfrom sklearn.utils import resample\n\nfraud = train[train.isFraud==1]\nnot_fraud = train[train.isFraud==0]\n\nnot_fraud_undsamp = resample(not_fraud,replace = False,n_samples = len(fraud),random_state = 42)\nundsamp_train = pds.concat([not_fraud_undsamp, fraud])\n\nfig = plt.figure(figsize = (8,5))\nundsamp_train.isFraud.value_counts(normalize = True).plot(kind='bar', color= ['skyblue','red'], alpha = 0.8)\nplt.title('Fraud and Non-Fraud after Undersampling (Balanced Dataset)')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After undersampling majority, the data set has now become perfectly balanced having 50% fraud and 50% legit transaction instances.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# **Step 4:**\nWe will first discard the features having more than 40% missing values. For remaining features, we will impute categorical features with mode (followed by label encoding to convert them into continuous variables). Afterwards, we will impute all continuous variables using **MICE (Multiple Imputation by Chained Equations)**.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop features having >40% missing values in Train Dataset\n\npct_null = undsamp_train.isnull().sum() / len(undsamp_train)\nmissing_features = pct_null[pct_null > 0.40].index\nundsamp_train.drop(missing_features, axis=1, inplace=True)\nundsamp_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train, fraud, not_fraud #free space","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"undsamp_train.select_dtypes(include=['category']).columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Impute categorical var with Mode\nundsamp_train['ProductCD'] = undsamp_train['ProductCD'].fillna(undsamp_train['ProductCD'].mode()[0])\nundsamp_train['card4'] = undsamp_train['card4'].fillna(undsamp_train['card4'].mode()[0])\nundsamp_train['card6'] = undsamp_train['card6'].fillna(undsamp_train['card6'].mode()[0])\nundsamp_train['P_emaildomain'] = undsamp_train['P_emaildomain'].fillna(undsamp_train['P_emaildomain'].mode()[0])\nundsamp_train['M4'] = undsamp_train['M4'].fillna(undsamp_train['M4'].mode()[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert categorical features to continuous features with Label Encoding\nfrom sklearn.preprocessing import LabelEncoder\nlencoders = {}\nfor col in undsamp_train.select_dtypes(include=['category']).columns:\n    lencoders[col] = LabelEncoder()\n    undsamp_train[col] = lencoders[col].fit_transform(undsamp_train[col])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Multiple Imputation by Chained Equations\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\ntrain_MiceImputed = undsamp_train.copy(deep=True) \nmice_imputer = IterativeImputer()\ntrain_MiceImputed.iloc[:, :] = mice_imputer.fit_transform(undsamp_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_MiceImputed.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Step 5:**\nWe are now done with data pre-processing, and now, we will move forward carrying on with **Feature Selection** step. Here, we have used Extra Trees Classifier and Random Forest Classifier for selecting the most relevant features having high relative feature importance w.r.t. the target variable 'isFraud'. Finally, we will merge the resultant features and will use that as our workable training data set. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature Selection using Extra Trees Classifier (max_features='log2')\n\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\n\nx_t = train_MiceImputed.drop('isFraud', axis=1)\ny_t = train_MiceImputed['isFraud']\nclf_1 = SelectFromModel(ExtraTreesClassifier(max_features='log2'))\nclf_1.fit(x_t, y_t)\nselect_feats_1 = x_t.columns[(clf_1.get_support())]\nprint(select_feats_1)\n\n# Feature Selection using Extra Trees Classifier (max_features = 'auto')\n\nclf_2 = SelectFromModel(ExtraTreesClassifier(max_features='auto'))\nclf_2.fit(x_t, y_t)\nselect_feats_2 = x_t.columns[(clf_2.get_support())]\nprint(select_feats_2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature Selection using Random Forest (max_features='log2')\n\nfrom sklearn.ensemble import RandomForestClassifier\nclf_3 = SelectFromModel(RandomForestClassifier(max_features='log2'))\nclf_3.fit(x_t, y_t)\nselect_feats_3 = x_t.columns[(clf_3.get_support())]\nprint(select_feats_3)\n\n# Feature Selection using Random Forest (max_features 'auto') \n\nclf_4 = SelectFromModel(RandomForestClassifier(max_features='auto'))\nclf_4.fit(x_t, y_t)\nselect_feats_4 = x_t.columns[(clf_4.get_support())]\nprint(select_feats_4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Combine all selected features\nx_train = train_MiceImputed[['TransactionDT', 'TransactionAmt', 'ProductCD', 'card1', 'card2','card3', 'card4', 'card5', \n                             'card6','addr1', 'addr2', 'P_emaildomain', 'C1', 'C2', 'C6', 'C7','C8', 'C9','C10','C11',\n                             'C12', 'C13', 'C14','D1', 'D4', 'D10', 'D15','M4','V12', 'V13', 'V15', 'V22','V29', 'V30',\n                             'V34', 'V35','V36', 'V37','V38','V42', 'V43', 'V44','V45', 'V47', 'V48','V49','V50', 'V51', \n                             'V52', 'V53', 'V54','V57','V69', 'V70','V71', 'V74','V75', 'V76', 'V78', 'V81','V82','V83', \n                             'V84', 'V85', 'V86','V87', 'V90', 'V91', 'V92','V93', 'V94', 'V102', 'V127', 'V128', 'V133', \n                             'V280','V282','V283', 'V285', 'V294','V295', 'V306', 'V307', 'V308', 'V310', 'V312', 'V313',\n                             'V314','V315', 'V317', 'V318']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = train_MiceImputed['isFraud']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Though we have combined all important features, we will further check if these selected features are very heavily skewed or not. If heavily skewed, they will deviate from normality and might require log-transformation to reduce their skewness.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"skewness_of_features=[]\nfor col in x_train:\n        skewness_of_features.append(x_train[col].skew())\nprint(skewness_of_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will spot the highly skewed variables from selected features now and then we will analyse their feature importance further using **\"Earth Package\"**.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"skewed_vars = x_train[['TransactionAmt','C2','C6','C7','C8','C9','C10','C11','C12','C13','C14','V29','V37','V38','V44',\n                       'V45','V47','V52','V53','V69','V75','V78','V81','V82','V86','V87','V91','V102','V127','V128','V133',\n                       'V280','V283','V285','V294','V295','V306','V307','V308','V310','V312','V313','V314','V315','V317',\n                       'V318']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Step 6:**\nNow we will check feature importance of these highly skewed selected variables using **Earth Package**. The earth package implements variable importance based on 3 criteria: **(1)** **RSS (Residual Sum of Squares)**; **(2)** **GCV (Generalized Cross Validation)** and **(3)** **Number of subset models where a variable occurs (nb_subsets)**. If considering all criteria, any highly skewed variable shows high feature importance, we will consider that variable into our working model.   ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy\nimport matplotlib.pyplot as plt\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom pyearth import Earth\n\n# Fit an Earth model (Multi Variate Adaptive Regression Splines)\n# Residual Sum of Squares (RSS), Generalized Cross Validation (GCV), No. of Subsets of MARS Model Terms \ncriteria = ('rss', 'gcv', 'nb_subsets')  \nmodel = Earth(max_degree=3,\n              max_terms=10,\n              minspan_alpha=.5,\n              feature_importance_type=criteria,\n              verbose=True)\nmodel.fit(skewed_vars, y_train)\nrf = RandomForestRegressor()\nrf.fit(skewed_vars, y_train)\nprint(model.trace())\nprint(model.summary())\nprint(model.summary_feature_importances(sort_by='gcv'))\n\n# Plot the feature importances\nimportances = model.feature_importances_\nimportances['random_forest'] = rf.feature_importances_\ncriteria = criteria + ('random_forest',)\nidx = 1\n\nfig = plt.figure(figsize=(30, 12))\nlabels = [i for i in range(len(skewed_vars.columns))]\nfor crit in criteria:\n    plt.subplot(2, 2, idx)\n    plt.bar(numpy.arange(len(labels)),\n            importances[crit],\n            align='center',\n            color='blue')\n    plt.xticks(numpy.arange(len(labels)), labels)\n    plt.title(crit)\n    plt.ylabel('importances')\n    idx += 1\ntitle = 'Feature Importance Plots using Earth Package'\nfig.suptitle(title, fontsize=\"18\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Visibly, from first three plots, it is evident that 0th (TransactionAmt), 17th (V52), 25th (V87), 26th (V91), 38th (V308) indexed variables are having high feature importance. Whereas, last plot shows that 0th, 11th (C14), 17th, 25th, 44th (V315) indexed variables are having high feature importance. So we need to consider them. \n\n* Also, features V29, V53, V47, V45, V44, V38, V37 are ranked higher than C14 in the exhaustive list. They can be considered. Features V75, C13, C12, C11, C10, C9, C8, C7, C6, C2, V69, V318, V78, V294 are ranked higher than V315 in the exhaustive list. They can be considered as well.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Log-Transformation :** It is interesting to note that we can apply log-transformation to 'TransactionAmt' only (among these considerable set of highly skewed variables). We cannot apply log-transformation on 'C14','V52','V87','V91','V308','V315' since they have many zero values. Converting them into log will give raise to \"inf\" values. 'TransactionAmt' does not have any zero value.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train['TransactionAmt']=np.log(x_train['TransactionAmt'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check skewness again after log-transormation\nprint(x_train['TransactionAmt'].skew())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Please observe that the skewness has now got reduced.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Keeping only the important features\nx_train_work = x_train[['TransactionDT', 'TransactionAmt', 'ProductCD', 'card1', 'card2','card3', 'card4', 'card5', \n                        'card6','addr1', 'addr2', 'P_emaildomain', 'D1','D4', 'D10', 'D15','M4','C2','C6','C7','C8',\n                        'C9','C10','C11','C12','C13','C14','V12', 'V13', 'V15', 'V22','V29','V30','V34', 'V35','V36',\n                        'V37','V38','V42', 'V43','V44','V45','V47','V48','V49', 'V50', 'V51', 'V52','V53','V54','V57',\n                        'V69','V70','V71', 'V74','V75','V76','V78','V83', 'V84', 'V85', 'V87','V90','V91','V92','V93',\n                        'V94', 'V282','V294','V308','V315','V318']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above one is our final working data set for training.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check correlation matrix for the selected important features\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nmask = np.zeros_like(x_train_work.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nplt.figure(figsize=(20,20))\nsns.heatmap(x_train_work.corr(), mask = mask, vmin = -1, annot = False, cmap = 'RdYlGn')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are very high correlation among the following group of features: \n* C6, C7, C8\n* C10, C11, C12, C13, C14\n* V90, V91\n* V92, V93\n* V308, V318\nBut the correlations are not equal to 1 in any of these cases. So we are not discarding any feature on the ground of multicollinearity.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# **Step 7:**\nWe will use Light GBM model for our binary classification. **\"Hyperopt\"** library has been used to tune the LGBM hyper-parameters and to select the best set of hyper-parameters into a separate dataframe for future use.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\nfrom hyperopt import hp, tpe, fmin\nfrom sklearn.model_selection import cross_val_score\n\nvalgrid = {'n_estimators':hp.quniform('n_estimators', 1000, 5000, 50),\n         'subsample_for_bin':hp.uniform('subsample_for_bin',10,300000),\n         'learning_rate':hp.uniform('learning_rate', 0.00001, 0.03),\n         'max_depth':hp.quniform('max_depth', 3,8,1),\n         'num_leaves':hp.quniform('num_leaves', 7,256,1),\n         'subsample':hp.uniform('subsample', 0.60, 0.95),\n         'colsample_bytree':hp.uniform('colsample_bytree', 0.60, 0.95),\n         'min_child_samples':hp.quniform('min_child_samples', 1, 500,1),\n         'min_child_weight':hp.uniform('min_child_weight', 0.60, 0.95),\n         'min_split_gain':hp.uniform('min_split_gain', 0.60, 0.95),  \n         'reg_lambda':hp.uniform('reg_lambda', 1, 25)\n         #'reg_alpha':hp.uniform('reg_alpha', 1, 25)  \n        }\n\ndef objective(params):\n    params = {'n_estimators': int(params['n_estimators']),\n              'subsample_for_bin': int(params['subsample_for_bin']),\n              'learning_rate': params['learning_rate'],\n              'max_depth': int(params['max_depth']),\n              'num_leaves': int(params['num_leaves']),\n              'subsample': params['subsample'],\n              'colsample_bytree': params['colsample_bytree'],\n              'min_child_samples': int(params['min_child_samples']),\n              'min_child_weight': params['min_child_weight'],\n              'min_split_gain': params['min_split_gain'],\n              'reg_lambda': params['reg_lambda']}\n              #'reg_alpha': params['reg_alpha']}\n    \n    lgb_a = lgb.LGBMClassifier(**params)\n    score = cross_val_score(lgb_a, x_train_work, y_train, cv=5, n_jobs=-1).mean()\n    return score\n\nbestP = fmin(fn= objective, space= valgrid, max_evals=20, rstate=np.random.RandomState(123), algo=tpe.suggest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(bestP)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Step 8:**\nNow, we have to repeat the similar steps for test data set as well to form the working data set for testing.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Missing value analysis for the selected feature set in Test Data\ntotal = test[['TransactionDT', 'TransactionAmt', 'ProductCD', 'card1', 'card2','card3', 'card4', 'card5', \n                        'card6','addr1', 'addr2', 'P_emaildomain', 'D1','D4', 'D10', 'D15','M4','C2','C6','C7','C8',\n                        'C9','C10','C11','C12','C13','C14','V12', 'V13', 'V15', 'V22','V29','V30','V34', 'V35','V36',\n                        'V37','V38','V42', 'V43','V44','V45','V47','V48','V49', 'V50', 'V51', 'V52','V53','V54','V57',\n                        'V69','V70','V71', 'V74','V75','V76','V78','V83', 'V84', 'V85', 'V87','V90','V91','V92','V93',\n                        'V94', 'V282','V294','V308','V315','V318']].isnull().sum().sort_values(ascending=False)\n\npercent = (test.isnull().sum()/test.isnull().count()).sort_values(ascending=False)\nmissing_test = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_test.head(25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Impute categorical vars with Mode\ntest['ProductCD']=test['ProductCD'].fillna(test['ProductCD'].mode()[0])\ntest['card4']=test['card4'].fillna(test['card4'].mode()[0])\ntest['card6']=test['card6'].fillna(test['card6'].mode()[0])\ntest['P_emaildomain']=test['P_emaildomain'].fillna(test['P_emaildomain'].mode()[0])\ntest['M4']=test['M4'].fillna(test['M4'].mode()[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert categorical features to continuous features with Label Encoding\nfrom sklearn.preprocessing import LabelEncoder\nlencoders_te = {}\nfor col in test[['ProductCD','card4','card6','P_emaildomain','M4']]:\n    lencoders_te[col] = LabelEncoder()\n    test[col] = lencoders_te[col].fit_transform(test[col])  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_working = test[['TransactionDT', 'TransactionAmt', 'ProductCD', 'card1', 'card2','card3', 'card4', 'card5', \n                        'card6','addr1', 'addr2', 'P_emaildomain', 'D1','D4', 'D10', 'D15','M4','C2','C6','C7','C8',\n                        'C9','C10','C11','C12','C13','C14','V12', 'V13', 'V15', 'V22','V29','V30','V34', 'V35','V36',\n                        'V37','V38','V42', 'V43','V44','V45','V47','V48','V49', 'V50', 'V51', 'V52','V53','V54','V57',\n                        'V69','V70','V71', 'V74','V75','V76','V78','V83', 'V84', 'V85', 'V87','V90','V91','V92','V93',\n                        'V94', 'V282','V294','V308','V315','V318']]\ndel test #free space","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# MICE Imputation for test data set\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\ntest_MiceImputed = test_working.copy(deep=True) \nmice_imputer_te = IterativeImputer()\ntest_MiceImputed.iloc[:, :] = mice_imputer_te.fit_transform(test_working)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_MiceImputed.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_MiceImputed['TransactionAmt']=np.log(test_MiceImputed['TransactionAmt'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_working_final = test_MiceImputed[['TransactionDT', 'TransactionAmt', 'ProductCD', 'card1', 'card2','card3', 'card4', 'card5', \n                        'card6','addr1', 'addr2', 'P_emaildomain', 'D1','D4', 'D10', 'D15','M4','C2','C6','C7','C8',\n                        'C9','C10','C11','C12','C13','C14','V12', 'V13', 'V15', 'V22','V29','V30','V34', 'V35','V36',\n                        'V37','V38','V42', 'V43','V44','V45','V47','V48','V49', 'V50', 'V51', 'V52','V53','V54','V57',\n                        'V69','V70','V71', 'V74','V75','V76','V78','V83', 'V84', 'V85', 'V87','V90','V91','V92','V93',\n                        'V94', 'V282','V294','V308','V315','V318']]\ndel test_MiceImputed #free space","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Step 9:**\nFinally, we will apply Light GBM classifier model having the best set of hyper-parameters on test data to get final prediction whether a transaction in test data set is fraudulent or legit.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prediction using Light GBM with Best Hyperparameters \nimport pandas as pd_out\nimport lightgbm \nfrom sklearn.model_selection import train_test_split\nimport sklearn.metrics \nimport numpy as np\n\nclf_final = lightgbm.LGBMClassifier(n_estimators = int(bestP['n_estimators']),\n                                    subsample_for_bin = int(bestP['subsample_for_bin']),\n                                    learning_rate = bestP['learning_rate'],\n                                    max_depth = int(bestP['max_depth']),\n                                    num_leaves = int(bestP['num_leaves']),\n                                    subsample = bestP['subsample'],\n                                    colsample_bytree = bestP['colsample_bytree'],\n                                    min_child_samples = int(bestP['min_child_samples']),\n                                    min_child_weight = bestP['min_child_weight'],\n                                    min_split_gain = bestP['min_split_gain'],\n                                    reg_lambda = bestP['reg_lambda'],\n                                    #reg_alpha = bestP['reg_alpha'], \n                                    random_state = 123)\n\nX_train, X_test, Y_train, Y_test = train_test_split(x_train, y_train, test_size=0.2, random_state=123, stratify=y_train)\nclf_final.fit(x_train_work,y_train)\ny_pred = clf_final.predict(test_working_final)\n#print(accuracy_score(Y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before submitting our final result, we will do some analysis of key metrics generated for the final LGBM classifier model we applied on the test dataset. These key metrics include:\n* Calculation of Area under ROC Curve\n* Plotting ROC Curve (False Positive Rate vs True Positive Rate)\n* A detailed classification report with Precision, Recall, F1-Score information\n* Plotting Confusion Matrix indicating TPs, TNs, FPs, FNs\n\n**Note:-** **\"F1-Score\"** is a better metric compared to **\"Accuracy\"** for this kind of problems where the original data set is imbalanced (in case of rare events).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_prob = clf_final.predict_proba(x_train_work)[:,-1] # Positive class prediction probabilities  \ny_thresh = np.where(y_prob > 0.5, 1, 0) # Threshold the probabilities to give class predictions\nclf_final.score(x_train_work, y_thresh)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_proba = clf_final.predict_proba(x_train_work)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, predicted_proba[:,-1]) \nroc_auc = auc(false_positive_rate, true_positive_rate)\nprint(roc_auc) # Checking area under ROC Curve","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting ROC Curve\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10,10))\nplt.title('Receiver Operating Characteristic')\nplt.plot(false_positive_rate,true_positive_rate, color='red',label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],linestyle='--')\nplt.axis('tight')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Detailed Classification Report\nfrom sklearn.metrics import classification_report\nprint(classification_report(clf_final.predict(x_train_work),y_train,digits=4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nconfusion_matrix=metrics.confusion_matrix(clf_final.predict(x_train_work),y_train)\nprint(confusion_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import plot_confusion_matrix\nplot_confusion_matrix(clf_final, x_train_work, y_train,cmap=plt.cm.Blues, normalize = 'all')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Step 10:**\nExport binary classification result to an excel file and complete submission. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd_out.read_csv('/kaggle/input/ieee-fraud-detection/sample_submission.csv')\nsubmission['isFraud'] = y_pred\nsubmission.head()\nsubmission.to_csv('IEEEfraud_submission.csv', index=False)\nprint(\"Submission successful\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}