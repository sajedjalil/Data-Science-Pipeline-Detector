{"cells":[{"metadata":{},"cell_type":"markdown","source":"I wanted to mix it up a little bit and create my own T-SNE using Keras."},{"metadata":{"trusted":true},"cell_type":"code","source":"from __future__ import print_function\nimport os\nos.environ['TF_C_API_GRAPH_CONSTRUCTION']='0'\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n#general packages\nimport time\nimport datetime\nimport gc\n\nimport numpy as np\nimport pandas as pd\n\n# plotting\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\nfrom matplotlib.lines import Line2D\n%matplotlib inline\n\n#metrics\nfrom sklearn.model_selection import StratifiedKFold, RandomizedSearchCV, train_test_split\nfrom sklearn.metrics import mean_squared_error, roc_auc_score\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\nfrom sklearn.model_selection import StratifiedKFold, RandomizedSearchCV, train_test_split, KFold\nfrom sklearn.metrics import mean_squared_error, roc_auc_score\n\nfrom scipy.stats import norm, rankdata\nfrom scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform\n\n#modeling\nfrom bayes_opt import BayesianOptimization\nimport lightgbm as lgb\nimport tensorflow as tf\n\nimport keras\nfrom keras import regularizers\nfrom keras.layers import Input,Dropout,BatchNormalization,Activation,Add,PReLU, LSTM\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Activation, Reshape, Flatten, Conv2D, MaxPooling2D, Conv1D, MaxPool1D, GlobalMaxPooling1D\nfrom keras.optimizers import SGD\nfrom keras import backend as K\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, LearningRateScheduler\n\n# module loading settings\n%load_ext autoreload\n%autoreload 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reduce memory\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Set working directory"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(os.listdir('../input/ieee-fraud-detection/'))\nWDR = '../input/ieee-fraud-detection/'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Load data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transaction = reduce_mem_usage(pd.read_csv(WDR +'train_transaction.csv'))\nprint(train_transaction.shape)\ntest_transaction = reduce_mem_usage(pd.read_csv(WDR +'test_transaction.csv'))\nprint(test_transaction.shape)\ntrain_identity = reduce_mem_usage(pd.read_csv(WDR +'train_identity.csv'))\nprint(train_identity.shape)\ntest_identity = reduce_mem_usage(pd.read_csv(WDR +'test_identity.csv'))\nprint(test_identity.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Join Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train_transaction.merge(train_identity, how='left', on='TransactionID')\ntest = test_transaction.merge(test_identity, how='left', on='TransactionID')\nprint(\"shape of train is .....\"+str(train.shape))\nprint(\"shape of test is .....\"+str(test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_original = pd.concat([train, test],axis=0,sort=False)\ndf_original = df_original.rename(columns={\"isFraud\": \"target\"})\nprint(\"shape of df is .....\"+str(df_original.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Clean ID columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"def minify_identity_df(df):\n\n    df['id_12'] = df['id_12'].map({'Found':1, 'NotFound':0})\n    df['id_15'] = df['id_15'].map({'New':2, 'Found':1, 'Unknown':0})\n    df['id_16'] = df['id_16'].map({'Found':1, 'NotFound':0})\n\n    df['id_23'] = df['id_23'].map({'TRANSPARENT':4, 'IP_PROXY':3, 'IP_PROXY:ANONYMOUS':2, 'IP_PROXY:HIDDEN':1})\n\n    df['id_27'] = df['id_27'].map({'Found':1, 'NotFound':0})\n    df['id_28'] = df['id_28'].map({'New':2, 'Found':1})\n\n    df['id_29'] = df['id_29'].map({'Found':1, 'NotFound':0})\n\n    df['id_35'] = df['id_35'].map({'T':1, 'F':0})\n    df['id_36'] = df['id_36'].map({'T':1, 'F':0})\n    df['id_37'] = df['id_37'].map({'T':1, 'F':0})\n    df['id_38'] = df['id_38'].map({'T':1, 'F':0})\n\n    df['id_34'] = df['id_34'].fillna(':0')\n    df['id_34'] = df['id_34'].apply(lambda x: x.split(':')[1]).astype(np.int8)\n    df['id_34'] = np.where(df['id_34']==0, np.nan, df['id_34'])\n    \n    df['id_33'] = df['id_33'].fillna('0x0')\n    df['id_33_0'] = df['id_33'].apply(lambda x: x.split('x')[0]).astype(int)\n    df['id_33_1'] = df['id_33'].apply(lambda x: x.split('x')[1]).astype(int)\n    df['id_33'] = np.where(df['id_33']=='0x0', np.nan, df['id_33'])\n\n    df['id_33'] = df['id_33'].fillna('unseen_before_label')  \n    df['DeviceType'].map({'desktop':1, 'mobile':0})\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#minify\nminify_identity_df(df_original)\n\n#change missing\n#id_var = [col for col in df_original if col.startswith('id')]\n#for col in id_var:\n#    df_original[col] = df_original[col].fillna(df_original[col].mode()[0])\n#    df_original[col] = df_original[col].astype('category')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Filter out columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Filter out columns\ntarget = df_original['target']\ndf_original = df_original.loc[:, df_original.isnull().mean() < .05]\ndf_original['target'] = target.values\nprint(\"shape of filtered df is .....\"+str(df_original.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Define Card_ID"},{"metadata":{"trusted":true},"cell_type":"code","source":"def corret_card_id(x):\n    x=x.replace('.0','')\n    x=x.replace('-999','nan')\n    return x\n\ndef define_indices(df):\n    # create date column\n    START_DATE = '2017-12-01'\n    startdate = datetime.datetime.strptime(START_DATE, '%Y-%m-%d')\n    df['date'] = df['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds = x)))\n    df['year'] = df['date'].dt.year\n    df['month'] = df['date'].dt.month\n    df['dow'] = df['date'].dt.dayofweek\n    df['day'] = df['date'].dt.day\n    df['hour'] = df['date'].dt.hour\n    # create card ID \n    cards_cols= ['card1', 'card2', 'card3', 'card5']\n    df['Card_ID'] = df[cards_cols].astype(str).sum(axis=1)\n    \n    # sort train data by Card_ID and then by transaction date \n    df= df.sort_values(['Card_ID', 'date'], ascending=[True, True])\n    \n    # small correction of the Card_ID\n    df['Card_ID']=df['Card_ID'].apply(corret_card_id)\n    \n    # set indexes\n    #df = df.reset_index()\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create Card_ID\ndf_original = define_indices(df_original)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Encode"},{"metadata":{"trusted":true},"cell_type":"code","source":"def encoder(df, numerics_only, categorical_only):\n    \n    #Numeric Encoding\n    for col in numerics_only:\n        df[col] = (df[col].fillna(-99))\n        df[col] = df[col].astype(float)\n        df[col] = df[col].fillna(df[col].mean())\n        \n    # Label Encoding\n    for col in categorical_only:\n        df[col] = df[col].fillna(df[col].mode()[0])\n        df[col] = df[col].astype('category')\n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(df[col].values))\n        df[col] = lbl.transform(list(df[col].values))\n        \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#subset variables\n#all usuable features\nfeatures = [f for f in df_original if f not in ['target','TransactionID','TransactionDT','date',\n                                                'Card_ID','year','month','dow','day','hour']]\n#numeric\ndf_numerics_only = [col for col in df_original[features].select_dtypes(include=[np.number]) \n                    if not col.startswith('id')]\n#cat\ndf_categorical_only = [col for col in df_original[features].columns if col not in df_numerics_only]\n\n#encode df\ndf_original[features] = encoder(df_original[features], df_numerics_only, df_categorical_only)\ndf = df_original","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Clean up workspace"},{"metadata":{"trusted":true},"cell_type":"code","source":"del df_original, train, test, test_identity, train_identity, train_transaction, test_transaction\ngc.collect()\n%whos DataFrame","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Generate counts"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_categorical_only.append('Card_ID')\nfor var in (df[df_categorical_only]):\n    print(var)\n    df['weight_' + var] = df.groupby([var])[var].transform('count')\n    df['binary_' + var] = df['weight_' + var].apply(lambda x: 1 if x > 1 else 0) * df[var]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Select features\ndf_numerics_only = [col for col in df[features].select_dtypes(include=[np.number]) if not col.startswith('id')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_numerics_only","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Scale data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Guass transformation\nfrom scipy.special import erfinv\nfor col in df_numerics_only:\n    values = sorted(set(df[col]))\n    # Because erfinv(1) is inf, we shrink the range into (-0.9, 0.9)\n    f = pd.Series(np.linspace(-0.9, 0.9, len(values)), index=values)\n    f = np.sqrt(2) * erfinv(f)\n    f -= f.mean()\n    df[col] = df[col].map(f)\n    df[col] = df[col].round(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df.head(10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Define T-SNE"},{"metadata":{"trusted":true},"cell_type":"code","source":"#define uniform batch size\nfrom fractions import gcd\ngcb = []\n\nfor i in range(32,128):\n    gcb.append(gcd(df[df_numerics_only].shape[0],i))\n    \ngcb.sort(reverse=True)\ngcb = [nom for nom in gcb if nom != df[df_numerics_only].shape[0]]\nbatch = gcb[0]\nprint(batch)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#set params\nlen_input_columns, len_data = df[df_numerics_only].shape[1], df[df_numerics_only].shape[0]\nkernel_initializer=keras.initializers.lecun_normal()\nNUM_GPUS=1\nlearning_rate = 0.001\nnb_epoch = int(10)\ndcy = learning_rate / nb_epoch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create probability function\ndef Hbeta(D, beta):\n    P = np.exp(-D * beta)\n    sumP = np.sum(P)\n    H = np.log(sumP) + beta * np.sum(np.multiply(D, P)) / sumP\n    P = P / sumP\n    return H, P\n\ndef x2p(X, u=15, tol=1e-4, print_iter=500, max_tries=50, verbose=0):\n    # Initialize some variables\n    n = X.shape[0]                     # number of instances\n    P = np.zeros((n, n))               # empty probability matrix\n    beta = np.ones(n)                  # empty precision vector\n    logU = np.log(u)                   # log of perplexity (= entropy)\n    \n    # Compute pairwise distances\n    if verbose > 0: print('Computing pairwise distances...')\n    sum_X = np.sum(np.square(X), axis=1)\n    # note: translating sum_X' from matlab to numpy means using reshape to add a dimension\n    D = sum_X + sum_X[:,None] + -2 * X.dot(X.T)\n\n    # Run over all datapoints\n    if verbose > 0: print('Computing P-values...')\n    for i in range(n):\n        \n        if verbose > 1 and print_iter and i % print_iter == 0:\n            print('Computed P-values {} of {} datapoints...'.format(i, n))\n        \n        # Set minimum and maximum values for precision\n        betamin = float('-inf')\n        betamax = float('+inf')\n        \n        # Compute the Gaussian kernel and entropy for the current precision\n        indices = np.concatenate((np.arange(0, i), np.arange(i + 1, n)))\n        Di = D[i, indices]\n        H, thisP = Hbeta(Di, beta[i])\n        \n        # Evaluate whether the perplexity is within tolerance\n        Hdiff = H - logU\n        tries = 0\n        while abs(Hdiff) > tol and tries < max_tries:\n            \n            # If not, increase or decrease precision\n            if Hdiff > 0:\n                betamin = beta[i]\n                if np.isinf(betamax):\n                    beta[i] *= 2\n                else:\n                    beta[i] = (beta[i] + betamax) / 2\n            else:\n                betamax = beta[i]\n                if np.isinf(betamin):\n                    beta[i] /= 2\n                else:\n                    beta[i] = (beta[i] + betamin) / 2\n            \n            # Recompute the values\n            H, thisP = Hbeta(Di, beta[i])\n            Hdiff = H - logU\n            tries += 1\n        \n        # Set the final row of P\n        P[i, indices] = thisP\n        \n    if verbose > 0: \n        print('Mean value of sigma: {}'.format(np.mean(np.sqrt(1 / beta))))\n        print('Minimum value of sigma: {}'.format(np.min(np.sqrt(1 / beta))))\n        print('Maximum value of sigma: {}'.format(np.max(np.sqrt(1 / beta))))\n    \n    return P, beta\n\ndef compute_joint_probabilities(samples, batch_size=5000, d=2, perplexity=30, tol=1e-5, verbose=0):\n    v = d - 1\n    \n    # Initialize some variables\n    n = samples.shape[0]\n    batch_size = min(batch_size, n)\n    \n    # Precompute joint probabilities for all batches\n    if verbose > 0: print('Precomputing P-values...')\n    batch_count = int(n / batch_size)\n    P = np.zeros((batch_count, batch_size, batch_size))\n    for i, start in enumerate(range(0, n - batch_size + 1, batch_size)):   \n        curX = samples[start:start+batch_size]                   # select batch\n        P[i], beta = x2p(curX, perplexity, tol, verbose=verbose) # compute affinities using fixed perplexity\n        P[i][np.isnan(P[i])] = 0                                 # make sure we don't have NaN's\n        P[i] = (P[i] + P[i].T) # / 2                             # make symmetric\n        P[i] = P[i] / P[i].sum()                                 # obtain estimation of joint probabilities\n        P[i] = np.maximum(P[i], np.finfo(P[i].dtype).eps)\n\n    return P","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# P is the joint probabilities for this batch (Keras loss functions call this y_true)\n# activations is the low-dimensional output (Keras loss functions call this y_pred)\ndef tsne(P, activations):\n    d = int(model.get_layer('Output').get_output_at(0).get_shape().as_list()[1])#need to find a way to automate\n    d = 5 \n    n = batch \n    v = d - 1.\n    eps = K.variable(10e-15) # needs to be at least 10e-8 to get anything after Q /= K.sum(Q)\n    sum_act = K.sum(K.square(activations), axis=1)\n    Q = K.reshape(sum_act, [-1, 1]) + -2 * K.dot(activations, K.transpose(activations))\n    Q = (sum_act + Q) / v\n    Q = K.pow(1 + Q, -(v + 1) / 2)\n    Q *= K.variable(1 - np.eye(n))\n    Q /= K.sum(Q)\n    Q = K.maximum(Q, eps)\n    C = K.log((P + eps) / (Q + eps))\n    C = K.sum(P * C)\n    return C","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.callbacks import LearningRateScheduler\nimport math\nfrom math import exp\n\ndef step_decay(epoch):\n    initial_lrate = 0.1\n    drop = 0.5\n    epochs_drop = 10.0\n    lrate = initial_lrate * math.pow(drop,  \n           math.floor((1+epoch)/epochs_drop))\n    return lrate\n#lrate = LearningRateScheduler(step_decay)\n        \n#def exp_decay(epoch):\n#    initial_lrate = 0.1\n#    k = 0.1\n#    t = epoch\n#    lrate = initial_lrate * exp(-k*t)\n#    return lrate\n#lrate = LearningRateScheduler(exp_decay)\n\nclass LossHistory(keras.callbacks.Callback):\n    def on_train_begin(self, logs={}):\n        self.losses = []\n        self.lr = []\n \n    def on_epoch_end(self, batch, logs={}):\n        self.losses.append(logs.get('loss'))\n#        self.lr.append(exp_decay(len(self.losses)))\n        self.lr.append(step_decay(len(self.losses)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Build model"},{"metadata":{"trusted":true},"cell_type":"code","source":"#build model\nmodel = Sequential()\nmodel.add(Dense(units=len_input_columns, dtype='float32', name='Input', input_shape=(len_input_columns,)))\nmodel.add(Dense(units=500, activation='relu', dtype='float32', name='Hidden1', kernel_initializer=kernel_initializer))\nmodel.add(Dense(units=500, activation='relu', dtype='float32', name='Hidden2', kernel_initializer=kernel_initializer))\nmodel.add(Dense(units=2000, activation='relu', dtype='float32', name='Hidden3', kernel_initializer=kernel_initializer))\nmodel.add(Dense(units=5, activation='linear', dtype='float32', name='Output', kernel_initializer=kernel_initializer))\n\nsgd = SGD(lr=0.1)\nmodel.compile(metrics=['accuracy'], loss=tsne, optimizer=sgd)\n\nprint(\"GPU MODEL\")\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#define callbacks\ncp = ModelCheckpoint(filepath=\"tsne_0.h5\",save_best_only=True,verbose=0)\ncpnn = ModelCheckpoint(filepath=\"ae_0.h5\",save_best_only=True,verbose=0)\ntb = TensorBoard(log_dir='./logs',histogram_freq=0,write_graph=True,write_images=True)\nrl = tf.keras.callbacks.ReduceLROnPlateau(monitor='acc', patience=1, verbose=1)\nes = tf.keras.callbacks.EarlyStopping(monitor='acc', patience=5, verbose=1)\nloss_history = LossHistory()\nlrate = LearningRateScheduler(step_decay)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#compute probability\nP = compute_joint_probabilities(df[df_numerics_only].values, batch_size=batch, d = int(model.get_layer('Output').get_output_at(0).get_shape().as_list()[1]), verbose=0)\nY_train = P.reshape(df[df_numerics_only].shape[0], -1)\nY_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train T-SNE"},{"metadata":{"trusted":true},"cell_type":"code","source":"#train model\nmodel.fit(df[df_numerics_only].values, Y_train, batch_size=batch, shuffle=False, epochs=nb_epoch,\nverbose=1, callbacks=[cpnn,tb,es,loss_history,lrate])\n\n#plot weights\nplt.hist(model.get_weights(), bins = 100)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#make prediction\nreconstruction = pd.DataFrame(model.predict(df[df_numerics_only].values))\nreconstruction = reconstruction.reset_index(drop=True)\nprint(\"reconstruction values .....\")\nprint(reconstruction.head(10))\n\n#join data\ntarget = df['target']\ndf = df.reset_index(drop=True)\ndf = pd.concat([df[features], reconstruction], axis=1)\ndf = df.rename(index=str, columns={0: \"tsne_0\", 1: \"tsne_1\", 2: \"tsne_2\", 3: \"tsne_3\", 4: \"tsne_4\",})\ndf['target'] = target.values\nprint(\"df shape .....\"+str(df.shape))\n\n#del recon\ndel reconstruction\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Generate more features"},{"metadata":{"trusted":true},"cell_type":"code","source":"for feature in df.filter(like='tsne').columns:\n    df['mean_'+feature] = (df[feature].mean()-df[feature])\n    df['zscore_'+feature] = (df[feature] - df[feature].mean())/df[feature].std(ddof=0)\n    df['sq_'+feature] = (df[feature])**2\n    df['sqrt_'+feature] = np.abs(df[feature])**(1/2)\n    \n    #these need work\n    #df['cp_'+feature] = pd.DataFrame(rankdata(df[feature]))\n    #df['cnp_'+feature] = pd.DataFrame((norm.cdf(df[feature])))\n    #df['zscorecountarctan'+feature]=(np.arctan(df['weight_'+features])2/np.pi)df['zscore_'+feature]\n    #df['zscorecount' +feature] = df['zscore_'+feature] * ((df['weight_'+feature]-df['weight_'+feature].min())/((df['weight_'+feature].max()-df['weight_'+feature].min())*8+1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Select features\ntraining_features = [f for f in df if f not in ['target', 'TransactionID', 'TransactionDT', 'date', 'Card_ID']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Split into train/test"},{"metadata":{"trusted":true},"cell_type":"code","source":"#split data\ntrain = df[df['target'].notnull()]\ntrain['target'] = train['target'].astype(int)\ntest = df[df['target'].isnull()]\n\n#check data\nprint(\"df shape .....\"+str(df.shape))\nprint(\"train shape .....\"+str(train.shape))\nprint(\"test shape .....\"+str(test.shape))\nprint(\"df .....\")\nprint(df.head(5))\nprint(\"train .....\")\nprint(train.head(5))\n\ndel df\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plot T-SNE"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_model(embedding, labels):\n    fig = plt.figure(figsize=(8,8))\n    plt.scatter(embedding[:,0], embedding[:,1], marker='o', s=1, edgecolor='', c=labels)\n    fig.tight_layout()\n    \nfrom matplotlib.lines import Line2D\ndef plot_differences(embedding, actual, lim=1000):\n    fig = plt.figure(figsize=(12,12))\n    ax = fig.gca()\n    for a, b in zip(embedding, actual)[:lim]:\n        ax.add_line(Line2D((a[0], b[0]), (a[1], b[1]), linewidth=1))\n    ax.autoscale_view()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#make prediction on train\ntrain_reconstruction = pd.DataFrame(model.predict(train[df_numerics_only].values))\ntrain_reconstruction = train_reconstruction.reset_index(drop=True)\nprint(train_reconstruction.head(10))\n\n#plot reconstruction\nplot_model(train_reconstruction.values, train['target'].values)\n\ndel train_reconstruction\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Define grid architecture"},{"metadata":{"trusted":true},"cell_type":"code","source":"#set archtype\nnb_folds = 2\nruns_per_fold = 2 #bagging on each fold\nnb_epoch = int(100)\nkfolds = StratifiedKFold(n_splits=nb_folds, shuffle=True, random_state=42)\n#kfolds = TimeSeriesSplit(n_splits=nb_folds)\n#kfolds = BlockingTimeSeriesSplit(n_splits=nb_folds)\n\n#create empty dataframes\nfeatures_importance = pd.DataFrame({'Feature':[], 'Importance':[], 'Fold':[], 'Aug':[]})\noof = np.zeros(shape=(np.shape(train)[0], 1))\npredict = np.zeros(shape=(np.shape(test)[0], 1))\npredictions = np.zeros(shape=(np.shape(test)[0], nb_folds))\noof_lgbm = np.zeros(len(train))\navg_roc = np.zeros(1)\ncv_roc = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Bounded region of parameter space\nbounds_LGB = {\n    'num_leaves': (2, 300), \n    'min_data_in_leaf': (2, 300),\n    'bagging_fraction' : (0.1,0.9),\n    'feature_fraction' : (0.1,0.9),\n    'learning_rate': (0.003, 0.01),\n    'min_child_weight': (0.00001, 0.01),   \n    'min_child_samples':(100, 500), \n    'subsample': (0.2, 0.8),\n    'colsample_bytree': (0.4, 0.6), \n    'reg_alpha': (0, 2), \n    'reg_lambda': (0, 2),\n    'max_depth':(1,100)\n    }","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's Create the functions we will use for the grid"},{"metadata":{"trusted":true},"cell_type":"code","source":"# define roc_callback, inspired by https://github.com/keras-team/keras/issues/6050#issuecomment-329996505\ndef auc_roc(y_true, y_pred):\n    # any tensorflow metric\n    value, update_op = tf.contrib.metrics.streaming_auc(y_pred, y_true)\n\n    # find all variables created for this metric\n    metric_vars = [i for i in tf.local_variables() if 'auc_roc' in i.name.split('/')[1]]\n\n    # Add metric variables to GLOBAL_VARIABLES collection.\n    # They will be initialized for new session.\n    for v in metric_vars:\n        tf.add_to_collection(tf.GraphKeys.GLOBAL_VARIABLES, v)\n\n    # force to update metric values\n    with tf.control_dependencies([update_op]):\n        value = tf.identity(value)\n        return value\n    \ndef gini(actual, pred):\n    assert (len(actual) == len(pred))\n    all = np.asarray(np.c_[actual, pred, np.arange(len(actual))], dtype=np.float)\n    all = all[np.lexsort((all[:, 2], -1 * all[:, 1]))]\n    totalLosses = all[:, 0].sum()\n    giniSum = all[:, 0].cumsum().sum() / totalLosses\n\n    giniSum -= (len(actual) + 1) / 2.\n    return giniSum / len(actual)\n\ndef gini_normalizedc(actual, pred):\n    return gini(actual, pred) / gini(actual, actual)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def augment(x,y,t=2):\n    xs,xn = [],[]\n    for i in range(t):\n        mask = y>0\n        x1 = x[mask].copy()\n        ids = np.arange(x1.shape[0])\n        for c in range(x1.shape[1]):\n            np.random.shuffle(ids)\n            x1[:,c] = x1[ids][:,c]\n        xs.append(x1)\n\n    for i in range(t//2):\n        mask = y==0\n        x1 = x[mask].copy()\n        ids = np.arange(x1.shape[0])\n        for c in range(x1.shape[1]):\n            np.random.shuffle(ids)\n            x1[:,c] = x1[ids][:,c]\n        xn.append(x1)\n\n    xs = np.vstack(xs)\n    xn = np.vstack(xn)\n    ys = np.ones(xs.shape[0])\n    yn = np.zeros(xn.shape[0])\n    x = np.vstack([x,xs,xn])\n    y = np.concatenate([y,ys,yn])\n    return x,y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def LGB_Grid(trn_data, dev_data, X_dev,  y_dev):\n    #base LGBM \n    \"\"\"Apply Bayesian Optimization to LGBM parameters.\"\"\"\n    \n    init_points = 3\n    n_iter = 3\n    score = np.zeros(1)\n    num_round = 100\n    oof_lgb = np.zeros(len(X_dev))\n    \n    def LGB_bayesian(\n        learning_rate,\n        num_leaves, \n        bagging_fraction,\n        feature_fraction,\n        min_child_samples, \n        min_child_weight,\n        subsample, \n        min_data_in_leaf,\n        max_depth,\n        colsample_bytree,\n        reg_alpha,\n        reg_lambda\n         ):\n\n        # LightGBM expects next three parameters need to be integer. \n        num_leaves = int(num_leaves)\n        min_data_in_leaf = int(min_data_in_leaf)\n        max_depth = int(max_depth)\n\n        assert type(num_leaves) == int\n        assert type(min_data_in_leaf) == int\n        assert type(max_depth) == int\n\n        param = {\n                  'num_leaves': num_leaves, \n                  'min_child_samples': min_child_samples, \n                  'min_data_in_leaf': min_data_in_leaf,\n                  'min_child_weight': min_child_weight,\n                  'bagging_fraction' : bagging_fraction,\n                  'feature_fraction' : feature_fraction,\n                  'learning_rate' : learning_rate,\n                  'subsample': subsample, \n                  'max_depth': max_depth,\n                  'colsample_bytree': colsample_bytree,\n                  'reg_alpha': reg_alpha,\n                  'reg_lambda': reg_lambda,\n                  'objective': 'binary',\n                  'save_binary': True,\n                  'seed': 420,\n                  'feature_fraction_seed': 420,\n                  'bagging_seed': 420,\n                  'drop_seed': 420,\n                  'data_random_seed': 420,\n                  'boosting_type': 'gbdt',\n                  'verbose': 1,\n                  'is_unbalance': True,\n                  'boost_from_average': False,\n                  'metric':'auc'\n                  }   \n\n        clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, dev_data], verbose_eval=False, early_stopping_rounds=200)\n        oof_lgb = clf.predict(X_dev, num_iteration=clf.best_iteration)\n        score = gini_normalizedc(y_dev, oof_lgb)\n        #score = auc_roc(y_dev, oof_lgb)\n        #print(score)\n        auc_score = score\n        return auc_score\n    \n    LGB_BO = BayesianOptimization(LGB_bayesian, bounds_LGB)\n    print(LGB_BO.space.keys)\n    \n    print('-' * 130)\n    with warnings.catch_warnings():\n        warnings.filterwarnings('ignore')\n        LGB_BO.maximize(init_points=init_points, n_iter=n_iter, acq='ucb', xi=0.0, alpha=1e-6)\n        \n    print(LGB_BO.max['target'])\n    return(LGB_BO.max['params'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Run the grid!"},{"metadata":{"trusted":true},"cell_type":"code","source":"#run grid\nfor fold_, (trn_idx, dev_idx) in enumerate(kfolds.split(train[training_features], train['target'])):  \n    \n    #manipulate data\n    #train_df = pd.concat([aggregate_transactions(time_train.iloc[trn_idx]), aggregate_transactions(time_train.iloc[dev_idx])])\n    trn = train[training_features].iloc[trn_idx]\n    dev = train[training_features].iloc[dev_idx]\n    \n    #grab targets\n    trn_target = train['target'].iloc[trn_idx]\n    dev_target = train['target'].iloc[dev_idx]\n    \n    #encode data    \n    #trn[encoder_features] = encoder(trn)\n    #dev[encoder_features] = encoder(dev)\n    \n    #create train/dev splits\n    #trn_data = lgb.Dataset(trn, label=trn_target)\n    #dev_data = lgb.Dataset(dev, label=dev_target)\n    X_train, y_train = trn, trn_target\n    X_dev, y_dev = dev, dev_target\n    \n    #upsampling adapted from kernel: \n    #https://www.kaggle.com/ogrellier/xgb-classifier-upsampling-lb-0-283\n    pos = (pd.Series(y_train == 0))\n    \n    # Add positive examples\n    X_train = pd.concat([X_train, X_train.loc[pos]], axis=0)\n    y_train = pd.concat([y_train, y_train.loc[pos]], axis=0)\n    \n    # Shuffle data\n    idx = np.arange(len(X_train))\n    np.random.shuffle(idx)\n    X_train = X_train.iloc[idx]\n    y_train = y_train.iloc[idx]\n \n    #run augmentation\n    predictions_lgb = np.zeros(len(test))\n    roc = np.zeros(1)\n    p_valid,yp = 0,0\n    \n    for i in range(runs_per_fold):\n        print('-')\n        print(\"Fold {}\".format(fold_ + 1))\n        print(\"Augment {}\".format(i + 1))\n        X_t, y_t = augment(X_train.values, y_train.values)\n        X_t = pd.DataFrame(X_t)\n        X_t = X_t.add_prefix('var_')\n    \n        #run probability grid\n        trn_data = lgb.Dataset(X_t, label=y_t)\n        dev_data = lgb.Dataset(X_dev, label=y_dev)\n        best_param =  LGB_Grid(trn_data, dev_data, X_dev,  y_dev)\n\n        param_lgb = {\n            'min_data_in_leaf': int(best_param['min_data_in_leaf']), \n            'num_leaves': int(best_param['num_leaves']), \n            'learning_rate': best_param['learning_rate'],\n            'min_child_weight': best_param['min_child_weight'],\n            'colsample_bytree' : best_param['colsample_bytree'],\n            'bagging_fraction': best_param['bagging_fraction'], \n            'min_child_samples': best_param['min_child_samples'],\n            'subsample': best_param['subsample'],\n            'reg_lambda': best_param['reg_lambda'],\n            'reg_alpha': best_param['reg_alpha'],\n            'max_depth': int(best_param['max_depth']), \n            'objective': 'binary',\n            'save_binary': True,\n            'seed': 420,\n            'feature_fraction_seed': 420,\n            'bagging_seed': 420,\n            'drop_seed': 420,\n            'data_random_seed': 420,\n            'boosting_type': 'gbdt',\n            'verbose': 1,\n            'is_unbalance': True,\n            'boost_from_average': False,\n            'metric':'auc'\n        }\n        #train best model and predict\n        clf = lgb.train(param_lgb, trn_data, nb_epoch, valid_sets = [trn_data, dev_data], verbose_eval=100, early_stopping_rounds=200)\n        oof_lgbm[dev_idx] += clf.predict(X_dev, num_iteration=clf.best_iteration) / runs_per_fold\n        predictions[:, fold_] += clf.predict(test[training_features ], num_iteration=clf.best_iteration) / runs_per_fold\n        #roc += roc_auc_score(dev_target.values, oof_lgbm[dev_idx]) / runs_per_fold\n        roc += gini_normalizedc(dev_target.values, oof_lgbm[dev_idx]) / runs_per_fold\n\n        #pull best features\n        fold_importance = pd.DataFrame({'Feature':[], 'Importance':[]})\n        fold_importance['Feature']= training_features\n        fold_importance['Importance']= clf.feature_importance()\n        fold_importance[\"Fold\"] = fold_ + 1\n        fold_importance[\"Aug\"] = i + 1\n        features_importance = pd.concat([features_importance, fold_importance], axis=0)\n    \n    #aggregate all folds into final scores\n    print('Round AUC score %.6f' % roc)\n    avg_roc += roc / nb_folds\n\n#average out features\nfeatures_importance = features_importance.groupby('Feature').mean().sort_values(by = ['Importance'], ascending=False).drop(columns=['Fold', 'Aug'])\nfeatures_importance = features_importance.reset_index()\nfeatures_importance = features_importance[features_importance.Importance >= np.percentile(features_importance.Importance, 50)]\n\n#save predictions\ntest_predictions = np.mean(predictions, axis=1)\nnp.save('predictions', test_predictions)\nprint('Full AUC score %.6f' % avg_roc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plot the features"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nfrom matplotlib import pyplot as plt\ndef display_importances(feature_importance_df_):\n    cols = feature_importance_df_[[\"Feature\", \"Importance\"]]\n    best_features = feature_importance_df_[[\"Feature\", \"Importance\"]][:100]\n    best_features.reset_index(inplace=True)\n    print(best_features.dtypes)\n    plt.figure(figsize=(8, 10))\n    sns.barplot(x=\"Importance\", y=\"Feature\", data=best_features)\n    plt.title('LightGBM Features (avg over folds)')\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display_importances(features_importance)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Submit predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = pd.DataFrame()\nsample_submission['TransactionID'] = test['TransactionID']\nsample_submission['isFraud'] = test_predictions\nsample_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"nbformat":4,"nbformat_minor":1}