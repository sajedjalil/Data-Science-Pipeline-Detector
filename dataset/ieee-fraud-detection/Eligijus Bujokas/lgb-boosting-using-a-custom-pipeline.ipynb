{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Package imports"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Date wrangling\nimport datetime\n\n# Data wrangling\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport gc\n\n# Package to track the optimizing of parameters \nimport time\n\n# Status tracker\nfrom tqdm import tqdm\n\n# xgboost library\nimport xgboost as xgb\nfrom xgboost import plot_importance\n\n# gbm library\nimport lightgbm as lgb\n\n# One hot encoding \nfrom sklearn import preprocessing\n\n# Dimensionality reduction\nfrom sklearn.decomposition import PCA\n\n# Hyper parameter optimization\nfrom sklearn.model_selection import KFold, TimeSeriesSplit, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import make_scorer\nfrom hyperopt import fmin, tpe, hp, space_eval\n\n#TODO move analyzing and ploting to a class \nanalyze_data = False\nplot_data = False","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reading data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Reading the training data\ntrain_identity = pd.read_csv('../input/ieee-fraud-detection/train_identity.csv')\ntrain_transaction = pd.read_csv('../input/ieee-fraud-detection/train_transaction.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Seeing the columns of the data\nprint(\"Identity columns:\", train_identity.columns)\nprint(\"Transaction columns:\", train_transaction.columns)\n\n# Shapes\nprint(\"Identity shape:\", train_identity.shape)\nprint(\"Transaction shape\", train_transaction.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merging the two data sets together\nd = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')\nprint(\"Shape of merged data\", d.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reading the test set\ntest_identity = pd.read_csv('../input/ieee-fraud-detection/test_identity.csv')\ntest_transaction = pd.read_csv('../input/ieee-fraud-detection/test_transaction.csv')\n\n# Merging to one data frame\nd_test = pd.merge(test_transaction, test_identity, on='TransactionID', how='left')\nprint(f'Test data shape: {d_test.shape}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Freeing up memory\ndel test_identity, test_transaction, train_identity, train_transaction\ngc.collect()\n\n# Convergin inf to NA\nd = d.replace([np.inf, -np.inf], np.nan)\nd_test = d_test.replace([np.inf, -np.inf], np.nan)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Short EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(d.head(10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see from the head of the data, the features are messy with lots of missing values. Some of them are numeric while others seems to be factorial."},{"metadata":{},"cell_type":"markdown","source":"## Distribution of Y"},{"metadata":{},"cell_type":"markdown","source":"The variable 'isFraud' is binary indicating whether the transaction if fraudulent or not. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Balance of data\nprint('Percent of fraud transactions:', d[d['isFraud']==1].shape[0] * 100 /d.shape[0])\nprint('Percent of good transactions:', d[d['isFraud']==0].shape[0] * 100 /d.shape[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see the data is unbalanced where more than 90 percent of the data falls in one class."},{"metadata":{},"cell_type":"markdown","source":"## Feature analysis"},{"metadata":{},"cell_type":"markdown","source":"We will create a dict with the information about every feature. "},{"metadata":{"trusted":true},"cell_type":"code","source":"if analyze_data:\n    features = d.columns\n    feature_info = {}\n    row_count = d.shape[0]\n\n    for feature in tqdm(features):\n        NA_count = d[d[feature].isna()].shape[0]\n        NA_share = round(NA_count * 100/row_count, 4)\n        unique_values_count = len(d[feature].unique())\n        coltype = d[feature].dtype.kind\n\n        feature_info.update({\n            feature: {\n                'NA_count': NA_count,\n                'NA_share': NA_share,\n                'unique_values_count': unique_values_count,\n                'coltype': coltype\n            }\n        })\n    \n    feature_df = pd.DataFrame({\n        'NA_share': [feature_info.get(x).get('NA_share') for x in feature_info],\n        'NA_count': [feature_info.get(x).get('NA_count') for x in feature_info],\n        'unique_values_count': [feature_info.get(x).get('unique_values_count') for x in feature_info],\n        'coltype': [feature_info.get(x).get('coltype') for x in feature_info],\n        'feature': [x for x in feature_info.keys()]\n    }).sort_values('NA_share')\n    print(feature_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if analyze_data:\n    na_shares = np.array([feature_info.get(x).get('NA_share') for x in feature_info])\n    labels, counts = np.unique(na_shares, return_counts=True)\n    count_df = pd.DataFrame({\n        'na_share': labels, \n        'count': counts\n    }).sort_values('na_share', ascending=False)\n    print(count_df.head(20))\n    plt.hist(na_shares)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if analyze_data:\n    # Getting the columns for each of the data type\n    numeric_cols = [x for x in feature_info if feature_info.get(x).get('coltype')=='f']\n    numeric_cols += [x for x in feature_info if feature_info.get(x).get('coltype')=='i']\n\n    categorical_cols = [x for x in feature_info if (feature_info.get(x).get('coltype')=='O')]\n\n    # Removing unwanted features\n    numeric_cols = [x for x in numeric_cols if x not in ['isFraud', 'TransactionID']]\n\n    # Getting the datatypes\n    height = [len(numeric_cols), len(categorical_cols)]\n    bars = ('numeric', 'categorical')\n    y_pos = np.arange(len(bars))\n\n    # Drawing the distribution\n    plt.bar(y_pos, height)\n    plt.xticks(y_pos, bars)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ploting numeric features"},{"metadata":{"trusted":true},"cell_type":"code","source":"if analyze_data and plot_data:\n    for i, float_col in enumerate(numeric_cols):\n        plt.figure(figsize=(16, 8))\n        sns.violinplot(x='isFraud', y=float_col, data=d, hue='isFraud').set_title(float_col)\n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature engineering class"},{"metadata":{},"cell_type":"markdown","source":"A class to create new features from existing one. Because xgboost and lightgbm treat missing values as part of the features we do not need to worry about missing values too much. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining a feature engineering class\nclass featureEngineer:\n    \n    def __init__(self):\n        self.email_map = {\n        'gmail': 'google', \n        'att.net': 'att', \n        'twc.com': 'spectrum', \n        'scranton.edu': 'other', \n        'optonline.net': 'other', \n        'hotmail.co.uk': 'microsoft',\n        'comcast.net': 'other', \n        'yahoo.com.mx': 'yahoo', \n        'yahoo.fr': 'yahoo',\n        'yahoo.es': 'yahoo', \n        'charter.net': 'spectrum', \n        'live.com': 'microsoft', \n        'aim.com': 'aol', \n        'hotmail.de': 'microsoft', \n        'centurylink.net': 'centurylink',\n        'gmail.com': 'google', \n        'me.com': 'apple', \n        'earthlink.net': 'other', \n        'gmx.de': 'other',\n        'web.de': 'other', \n        'cfl.rr.com': 'other', \n        'hotmail.com': 'microsoft', \n        'protonmail.com': 'other', \n        'hotmail.fr': 'microsoft', \n        'windstream.net': 'other', \n        'outlook.es': 'microsoft', \n        'yahoo.co.jp': 'yahoo', \n        'yahoo.de': 'yahoo',\n        'servicios-ta.com': 'other', \n        'netzero.net': 'other', \n        'suddenlink.net': 'other',\n        'roadrunner.com': 'other', \n        'sc.rr.com': 'other', \n        'live.fr': 'microsoft',\n        'verizon.net': 'yahoo', \n        'msn.com': 'microsoft', \n        'q.com': 'centurylink', \n        'prodigy.net.mx': 'att', \n        'frontier.com': 'yahoo', \n        'anonymous.com': 'other', \n        'rocketmail.com': 'yahoo', \n        'sbcglobal.net': 'att', \n        'frontiernet.net': 'yahoo', \n        'ymail.com': 'yahoo', \n        'outlook.com': 'microsoft', \n        'mail.com': 'other', \n        'bellsouth.net': 'other', \n        'embarqmail.com': 'centurylink', \n        'cableone.net': 'other', \n        'hotmail.es': 'microsoft', \n        'mac.com': 'apple', \n        'yahoo.co.uk': 'yahoo',\n        'netzero.com': 'other', \n        'yahoo.com': 'yahoo', \n        'live.com.mx': 'microsoft', \n        'ptd.net': 'other', \n        'cox.net': 'other',\n        'aol.com': 'aol', \n        'juno.com': 'other', \n        'icloud.com': 'apple'\n        }\n\n        self.minmaxcols = ['TransactionAmt', 'card1', 'card2', 'card3', 'card5', 'D1', 'D15', 'dist1']\n        \n        self.logcols = ['TransactionAmt']\n\n    def feature_engineer(self, X):\n        \"\"\"\n        A function that transforms features in a given dataset by certain rules\n        \"\"\"\n        for col in X.columns:\n\n            if col in self.logcols:\n                X[col + '_log'] = np.log(X[col])\n            \n            if col in self.minmaxcols:\n                normalizer = preprocessing.MinMaxScaler()\n                X[col + '_scaled'] = normalizer.fit_transform(X[col].values.reshape(-1, 1)) \n\n            if col in ['P_emaildomain', 'R_emaildomain']:\n                X[col + '_provider'] = X[col].map(self.email_map)\n                X[col + '_suffix'] = X[col].map(lambda x: str(x).split('.')[-1])\n\n            if col == 'TransactionDT':\n                START_DATE = '2017-12-01'\n                startdate = datetime.datetime.strptime(START_DATE, \"%Y-%m-%d\")\n                X[\"Date\"] = X[col].apply(lambda x: (startdate + datetime.timedelta(seconds=x)))\n                X['Weekdays'] = X['Date'].dt.dayofweek\n                X['Hours'] = X['Date'].dt.hour\n                X['Days'] = X['Date'].dt.day\n                X['Weekdays_Hours'] = X['Weekdays'] + X['Hours']\n\n            if col == 'id_30':\n                X[col + '_cleaned'] = X[col]\n                for ops in ['windows', 'ios', 'mac', 'android', 'linux']:\n                    X.loc[X[col].str.lower().str.contains(ops, na=False), col + '_cleaned'] = ops\n\n            if col == 'id_31':\n                X[col + '_cleaned'] = np.nan\n                for ops in ['chrome', 'firefox', 'edge', 'ie', \n                            'android', 'opera', 'safari', 'samsung', \n                            'google', 'blackberry']:\n                    X.loc[X[col].str.lower().str.contains(ops, na=False), col + '_cleaned'] = ops\n                # Creating a column to check if the device is a mobile\n                X.loc[X[col].str.lower().str.contains('mobile', na=False), col + '_cleaned_mobile'] = \"1\"\n        \n        # Crude aggregations\n        X['P_emaildomain__addr1'] = X['P_emaildomain'] + '__' + X['addr1'].astype(str)\n        X['card1__card2'] = X['card1'].astype(str) + '__' + X['card2'].astype(str)\n        X['card1__addr1'] = X['card1'].astype(str) + '__' + X['addr1'].astype(str)\n        X['card2__addr1'] = X['card2'].astype(str) + '__' + X['addr1'].astype(str)\n        X['card12__addr1'] = X['card1__card2'] + '__' + X['addr1'].astype(str)    \n\n        X['TransactionAmt_to_mean_card1'] = X['TransactionAmt'] / X.groupby(['card1'])['TransactionAmt'].transform('mean')\n        X['TransactionAmt_to_mean_card2'] = X['TransactionAmt'] / X.groupby(['card2'])['TransactionAmt'].transform('mean')\n        X['TransactionAmt_to_mean_card3'] = X['TransactionAmt'] / X.groupby(['card3'])['TransactionAmt'].transform('mean')\n        X['TransactionAmt_to_mean_card5'] = X['TransactionAmt'] / X.groupby(['card5'])['TransactionAmt'].transform('mean')\n        \n        X['TransactionAmt_to_std_card1'] = X['TransactionAmt'] / X.groupby(['card1'])['TransactionAmt'].transform('std')\n        X['TransactionAmt_to_std_card2'] = X['TransactionAmt'] / X.groupby(['card2'])['TransactionAmt'].transform('std')\n        X['TransactionAmt_to_std_card3'] = X['TransactionAmt'] / X.groupby(['card3'])['TransactionAmt'].transform('std')\n        X['TransactionAmt_to_std_card5'] = X['TransactionAmt'] / X.groupby(['card5'])['TransactionAmt'].transform('std')\n        \n        return X","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocesing class"},{"metadata":{},"cell_type":"markdown","source":"A class to preproces data. The main tasks is to encode categorical features for the models."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining a class to preproces data \nclass preprocesingEngineer:\n    \"\"\"\n    Class that preprocess data before modeling\n    \"\"\"\n    def __init__(self):\n        self.numeric_cols = [\n            'TransactionAmt_log', 'card1', 'card2', 'card3', 'card5', \n            'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14',\n            'D1', 'Hours', 'dist1', 'D15', \n            'TransactionAmt_to_mean_card1', 'TransactionAmt_to_mean_card2', 'TransactionAmt_to_mean_card3', 'TransactionAmt_to_mean_card5', \n            'TransactionAmt_to_std_card1', 'TransactionAmt_to_std_card2', 'TransactionAmt_to_std_card3', 'TransactionAmt_to_std_card5',\n            'id_02'\n        ]\n        \n        self.categorical_cols = [\n            'ProductCD', \n            'P_emaildomain_provider', 'P_emaildomain_suffix', \n            'R_emaildomain_provider', 'R_emaildomain_suffix',\n            'id_30_cleaned', 'id_31_cleaned',\n            'addr1', 'addr2', \n            'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9',\n            'Weekdays', 'Days', 'Weekdays_Hours',\n            \"id_12\", \"id_13\",\"id_14\",\"id_15\",\"id_16\",\"id_17\",\"id_18\",\"id_19\",\"id_20\",\"id_21\",\"id_22\",\"id_23\",\"id_24\",\n            \"id_25\",\"id_26\",\"id_27\",\"id_28\",\"id_29\",\"id_30\",\"id_32\",\"id_34\", 'id_36', \"id_37\", \"id_38\",\n            'P_emaildomain__addr1', 'card1__card2', 'card1__addr1', 'card2__addr1',\n            'card12__addr1',\n        ]\n        \n        self.target_col = 'isFraud'\n        \n    def to_string(self, X):\n        \"\"\"\n        A method to convert categorical columns to strings\n        \"\"\"\n        for feature in self.categorical_cols:\n            X[feature] = [str(x) for x in X[feature].values]\n            \n        return X    \n    \n    def encode_labels(self, X, X_test=pd.DataFrame({})):\n        \"\"\"\n        A method to label encode categorical columns\n        \"\"\"\n        for feature in self.categorical_cols:\n            # Initiating the label encoder\n            lbl = preprocessing.LabelEncoder()\n            \n            # Getting unique column values\n            features = list(set(X[feature].values))\n            if not X_test.empty:\n                features += list(set(X[feature].values))\n            \n            # Fitting the label encoder    \n            lbl.fit(features)\n            \n            # Transforming the original feature\n            X[feature] = lbl.transform(list(X[feature].values))\n            \n        return X      \n    \n    def leave_final_cols(self, X, leave_target=False):\n        \"\"\"\n        A method to leave only the columns which will be used in modeling\n        \"\"\"\n        cols = self.numeric_cols + self.categorical_cols\n        \n        if leave_target:\n            return X[self.target_col].values, X[cols]\n        else:\n            return X[cols]            ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modelling class"},{"metadata":{},"cell_type":"markdown","source":"Class that handles hyper parameter tuning and fitting the model. If we use GPU then the model of choice is xgboost, otherwise we use lightgbm."},{"metadata":{"trusted":true},"cell_type":"code","source":"class modelingEngineer:\n    \"\"\"\n    Class for creating and using boosting models\n    \"\"\"\n    \n    def __init__(self):\n        self.NFOLDS = 5\n        \n        self.top_ft = 50\n        \n        self.lgb_parameters={\n            'objective': 'binary',\n            'metric': 'auc',\n            'num_leaves': 600,\n            'min_child_weight': 0.03,\n            'feature_fraction': 0.27,\n            'bagging_fraction': 0.31,\n            'min_data_in_leaf': 106,\n            'n_jobs': -1,\n            'learning_rate': 0.005,\n            'max_depth': -1,\n            'tree_learner': 'serial',\n            'colsample_bytree': 0.5,\n            'subsample_freq': 1,\n            'subsample': 0.7,\n            'max_bin': 300,\n            'verbose': -1,\n            'early_stopping_rounds': 200,\n        }\n        \n    def fit_model(self, X_train, Y_train, X_test):\n        \"\"\"\n        Fits the model using cross-validation and early stopping\n        \"\"\"\n        folds = KFold(n_splits=self.NFOLDS)\n        splits = folds.split(X_train, Y_train)\n\n        feature_importances = pd.DataFrame()\n        feature_importances['feature'] = X_train.columns\n        \n        y_preds = np.zeros(X_test.shape[0])\n        \n        for fold_n, (train_index, valid_index) in enumerate(splits):\n            x_train, x_valid = X_train.iloc[train_index], X_train.iloc[valid_index]\n            y_train, y_valid = Y_train[train_index], Y_train[valid_index]\n\n            dtrain = lgb.Dataset(x_train, label=y_train)\n            dvalid = lgb.Dataset(x_valid, label=y_valid)\n\n            clf = lgb.train(self.lgb_parameters, dtrain, 10000, valid_sets = [dvalid], verbose_eval=100)\n            \n            feature_importances[f'fold_{fold_n + 1}'] = clf.feature_importance()\n\n            # Predicting the true values of Y\n            y_preds += clf.predict(X_test) / self.NFOLDS\n            \n            del x_train, x_valid, y_train, y_valid\n            gc.collect()\n       \n        return y_preds, feature_importances\n    \n    def plot_feature_importance(self, feature_importances):\n        feature_importances['average'] = feature_importances[[f'fold_{fold_n + 1}' for fold_n in range(self.NFOLDS)]].mean(axis=1)\n        sns.barplot(data=feature_importances.sort_values(by='average', ascending=False).head(self.top_ft), x='average', y='feature')\n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pipeline class"},{"metadata":{},"cell_type":"markdown","source":"The pipeline is as follows: \nFeature engineering -> feature preprocesing -> data modelling. \n\nThe output of the pipeline is the .csv file to upload to the competition."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining the global pipeline parameters\noptimize_params = True\nN_evals = 10\nuse_gpu = False\n\nclass pipeline(featureEngineer, preprocesingEngineer, modelingEngineer):\n    \n    def __init__(self):\n        self.d_train = d\n        self.d_test = d_test\n        \n    def pipeline(self):\n        \"\"\"\n        Pipeline that creates new features, preproces and models data\n        \"\"\"\n        # Creating new features\n        fe = featureEngineer()\n        X_train = fe.feature_engineer(self.d_train)\n        X_test = fe.feature_engineer(self.d_test)\n        \n        # Preprocesing data\n        pre = preprocesingEngineer()\n        X_train = pre.to_string(X_train)\n        X_test = pre.to_string(X_test)\n\n        X_train = pre.encode_labels(X_train, X_test)\n        X_test = pre.encode_labels(X_test, X_train)\n\n        Y_train, X_train = pre.leave_final_cols(X_train, leave_target=True)\n        X_test = pre.leave_final_cols(X_test)\n        \n        # Modeling data \n        me = modelingEngineer()\n\n        # Fitting the model and getting predictions\n        y_preds, feature_importances = me.fit_model(X_train, Y_train, X_test)\n        \n        # Ploting the most important features\n        fig, ax = plt.subplots(figsize=(15, 20))\n        me.plot_feature_importance(feature_importances)\n    \n        # Forecasting the test set and creating file for submission\n        d_upload = pd.DataFrame({\n            'TransactionID': self.d_test['TransactionID'],\n            'isFraud': y_preds})\n        \n        return d_upload","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Running the pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting the pipeline\npipe = pipeline()\nd_upload = pipe.pipeline()\n\n# Saving for uplaod\nd_upload.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}