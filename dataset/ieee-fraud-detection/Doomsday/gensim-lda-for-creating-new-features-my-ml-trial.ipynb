{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering \n##### As this competition my first ever Machine Learning problem solving ,I started learning a bit with various pandas method for data processing , read kernels about interesting features and correlations between them.\n\n##### I learnt a bit about how xgboost , LGBM work . How to increase speed , how to increase accuracy . Various kind of splits . I have worked on them on various versions of the below Kernel\nhttps://www.kaggle.com/phoenix9032/ieee-fraud-my-first-ml-trial\n\n##### In this comp I have learnt that people can download high score excel and blend them and can score even higher . Some of them were really smart combinations. Looks like its a legitimate thing. Therefore learnt a bit about blending and stacking for future use .\n\n##### I have since tried to learn feature engineering . Looks like we can add new columns from the existing ones , drop columns that has very high correlation with each other or of lesser importance from a model perspective . Few are present in my original kernel . \n\n#### This method below , I was not sure to put in my existing note, so tried with a new Kernel . "},{"metadata":{},"cell_type":"markdown","source":"\nGot this idea from the 1st place ad-click Fraud Solution by talking data  . Since I am trying it out without much prior knowledge , I am open for review and correction . \n\n We can run this process to all or most important categorical columns and create new 5 features for each of them . Here I have shown one example for P_EMAIL_DOMAIN\n\n I accidentaly found a Kernel on this while viewing the OPs profile . The below Kernel is modified for our problem \n\nhttps://www.kaggle.com/izmaylov/topic-lda-modelling"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import os\nimport gc\ngc.enable()\n\nimport logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.DEBUG)\nimport os\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\nimport xgboost as xgb\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nimport catboost\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import roc_auc_score, roc_curve\nimport matplotlib.gridspec as gridspec\n%matplotlib inline\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Going to use these 5 base models for the stacking\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n                              GradientBoostingClassifier, ExtraTreesClassifier)\nfrom sklearn import metrics\n\nfrom sklearn.svm import SVC\nimport time\nimport seaborn as sns\nimport json\nfrom tqdm import tqdm_notebook\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import mean_absolute_error\nfrom scipy import sparse\nimport pyLDAvis.gensim\nimport gensim\nfrom gensim.matutils  import Sparse2Corpus\nfrom gensim.corpora import Dictionary\nfrom gensim.models import LdaModel\nfrom sklearn.linear_model import Ridge","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntrain_transaction = pd.read_csv('../input/train_transaction.csv', index_col='TransactionID')\ntest_transaction = pd.read_csv('../input/test_transaction.csv', index_col='TransactionID')\n\ntrain_identity = pd.read_csv('../input/train_identity.csv', index_col='TransactionID')\ntest_identity = pd.read_csv('../input/test_identity.csv', index_col='TransactionID')\n\nsample_submission = pd.read_csv('../input/sample_submission.csv', index_col='TransactionID')\n\ntrain = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\ntest = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)\n\nprint(train.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_ltrain = train['P_emaildomain'] ## Taking only one categorical column sample . We can clean up this code by using smart loops \ndf_ltest =  test ['P_emaildomain']## Taking only one categorical column sample . We can clean up this code by using smart loops ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_ltrain.fillna('X',inplace = True) ## Filling the NaN with some value else the next operations will give error\ndf_ltest.fillna('X',inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\"Text data requires special preparation before you can start using it for predictive modeling.\n\nThe text must be parsed to remove words, called tokenization. Then the words need to be encoded as integers or floating point values for use as input to a machine learning algorithm, called feature extraction (or vectorization).\n\nThe scikit-learn library offers easy-to-use tools to perform both tokenization and feature extraction of your text data.\"\n\nTherefore \" I will write a new Kernel\" becomes [I],[will],[write],[a], [new],[kernel]. "},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = CountVectorizer(max_features=10000, min_df = 0.1, max_df = 0.8) ###This just breaks\n                                                                     ### the documents into various tokens \nsparse_train = cv.fit_transform(df_ltrain)\nsparse_test  = cv.transform(df_ltest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sparse_data_train =  sparse.vstack([sparse_train, sparse_test]) ## original implementation had their train and test data together ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Transform our sparse_data to corpus for gensim\ncorpus_data_gensim = gensim.matutils.Sparse2Corpus(sparse_train, documents_columns=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create dictionary for LDA model\nvocabulary_gensim = {}\nfor key, val in cv.vocabulary_.items():\n    vocabulary_gensim[val] = key\n    \ndict = Dictionary()\ndict.merge_with(vocabulary_gensim)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(vocabulary_gensim)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nlda = LdaModel(corpus_data_gensim, num_topics = 5 ) ## Here we are creating 5 new features, so topic is 5.\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def document_to_lda_features(lda_model, document):\n    topic_importances = lda.get_document_topics(document, minimum_probability=0)\n    topic_importances = np.array(topic_importances)\n    return topic_importances[:,1]\n\nlda_features = list(map(lambda doc:document_to_lda_features(lda, doc),corpus_data_gensim))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_pd_lda_features = pd.DataFrame(lda_features)\ndata_pd_lda_features.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_pd_lda_features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## lets find out the correlation between newly created features\nfig, ax = plt.subplots()\n# the size of A4 paper\nfig.set_size_inches(20.7, 8.27)\nsns.heatmap(data_pd_lda_features.corr(method = 'spearman'), cmap=\"coolwarm\", ax = ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Except for between 0 and 3 looks like features are not much correlated and can be added as new features . We can try the same method for test set and then pass it to our XGBoost or LGBM model with new features . \n\n##### TO be continued and will be reused to my original kernel ."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}