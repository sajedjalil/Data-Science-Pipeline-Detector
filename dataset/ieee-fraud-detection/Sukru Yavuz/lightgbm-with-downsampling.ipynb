{"cells":[{"metadata":{},"cell_type":"markdown","source":"This competition was a fantastic journey for me and my teammate. Mostly, we learned the importance of selecting right Cross Validation strategy. I will share our approach (mostly we utilized kagglers' code). If I forget to mention you or your kernel, it's not on purpose. Please warn me on comments. Hope you will like it!"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport scipy as sp\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime\n# Standard plotly imports\n\n\n\n# Preprocessing, modelling and evaluating\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn import preprocessing\nfrom sklearn.metrics import confusion_matrix, roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score, KFold\nfrom xgboost import XGBClassifier\nimport xgboost as xgb\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nimport lightgbm as lgb\n\nimport os\nimport gc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reading and Merging Identity and Transaction Datasets"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_identity = pd.read_csv('../input/ieee-fraud-detection/train_identity.csv')\ntrain_transaction = pd.read_csv('../input/ieee-fraud-detection/train_transaction.csv')\ntest_identity = pd.read_csv('../input/ieee-fraud-detection/test_identity.csv')\ntest_transaction = pd.read_csv('../input/ieee-fraud-detection/test_transaction.csv')\nsub = pd.read_csv('../input/ieee-fraud-detection/sample_submission.csv')\n\ntrain = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')\ntest = pd.merge(test_transaction, test_identity, on='TransactionID', how='left')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fraud data is usually imbalanced. Let's check how this one is."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isFraud.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data that we are working on is imbalanced. There are many ways to deal with imbalanced data. Me and my teammate decided to use undersampling the majority class. And, we would like to thank Shahules786 for this very informative kernel. You can check his kernel [here](https://www.kaggle.com/shahules/tackling-class-imbalance). And, don't forget to upvote. We decided do downsample majority class to 400k. We decided it by trial and error."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils import resample\nnot_fraud=train[train.isFraud==0]\nfraud=train[train.isFraud==1]\n\nnot_fraud_downsampled = resample(not_fraud,\n                                replace = False, # sample without replacement\n                                n_samples = 400000, # match minority n\n                                random_state = 27) # reproducible results\n\n# combine minority and downsampled majority\ndownsampled = pd.concat([not_fraud_downsampled, fraud])\n\n# checking counts\ndownsampled.isFraud.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = downsampled.copy()\ndel not_fraud_downsampled, downsampled, train_identity, test_identity, train_transaction, test_transaction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The functions which are named as setBrowser, setDevice and setTimer are taken from this amazing [kernel](https://www.kaggle.com/tolgahancepel/lightgbm-single-model-and-feature-engineering). Go and upvote him!"},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"lastest_browser\"] = np.zeros(train.shape[0])\ntest[\"lastest_browser\"] = np.zeros(test.shape[0])\n\ndef setBrowser(df):\n    df.loc[df[\"id_31\"]==\"samsung browser 7.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"opera 53.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"mobile safari 10.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"google search application 49.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"firefox 60.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"edge 17.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 69.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 67.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 63.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 63.0 for ios\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 64.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 64.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 64.0 for ios\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 65.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 65.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 65.0 for ios\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 66.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 66.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 66.0 for ios\",'lastest_browser']=1\n    return df\n\ntrain=setBrowser(train)\ntest=setBrowser(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def setDevice(df):\n    df['DeviceInfo'] = df['DeviceInfo'].cat.add_categories('unknown_device')\n    df['DeviceInfo'] = df['DeviceInfo'].fillna('unknown_device').str.lower()\n    df['device_name'] = df['DeviceInfo'].str.split('/', expand=True)[0]\n\n    df.loc[df['device_name'].str.contains('SM', na=False), 'device_name'] = 'Samsung'\n    df.loc[df['device_name'].str.contains('SAMSUNG', na=False), 'device_name'] = 'Samsung'\n    df.loc[df['device_name'].str.contains('GT-', na=False), 'device_name'] = 'Samsung'\n    df.loc[df['device_name'].str.contains('Moto G', na=False), 'device_name'] = 'Motorola'\n    df.loc[df['device_name'].str.contains('Moto', na=False), 'device_name'] = 'Motorola'\n    df.loc[df['device_name'].str.contains('moto', na=False), 'device_name'] = 'Motorola'\n    df.loc[df['device_name'].str.contains('LG-', na=False), 'device_name'] = 'LG'\n    df.loc[df['device_name'].str.contains('rv:', na=False), 'device_name'] = 'RV'\n    df.loc[df['device_name'].str.contains('HUAWEI', na=False), 'device_name'] = 'Huawei'\n    df.loc[df['device_name'].str.contains('ALE-', na=False), 'device_name'] = 'Huawei'\n    df.loc[df['device_name'].str.contains('-L', na=False), 'device_name'] = 'Huawei'\n    df.loc[df['device_name'].str.contains('Blade', na=False), 'device_name'] = 'ZTE'\n    df.loc[df['device_name'].str.contains('BLADE', na=False), 'device_name'] = 'ZTE'\n    df.loc[df['device_name'].str.contains('Linux', na=False), 'device_name'] = 'Linux'\n    df.loc[df['device_name'].str.contains('XT', na=False), 'device_name'] = 'Sony'\n    df.loc[df['device_name'].str.contains('HTC', na=False), 'device_name'] = 'HTC'\n    df.loc[df['device_name'].str.contains('ASUS', na=False), 'device_name'] = 'Asus'\n\n    #df.loc[df.device_name.isin(df.device_name.value_counts()[df.device_name.value_counts() < 200].index), 'device_name'] = \"Others\"\n    df['had_id'] = 1\n    gc.collect()\n    \n    return df\n\ntrain=setDevice(train)\ntest=setDevice(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm_notebook\nfor col in tqdm_notebook(train.columns):\n    if train[col].dtype == 'object':\n        le = LabelEncoder()\n        le.fit(list(train[col].astype(str).values) + list(test[col].astype(str).values))\n        train[col] = le.transform(list(train[col].astype(str).values))\n        test[col] = le.transform(list(test[col].astype(str).values))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Combining Two or More Columns"},{"metadata":{},"cell_type":"markdown","source":"Two (string or numeric) columns can be combined into one column. If you think that you should improve your feature engineering skills, you definitely should check this [discussion topic](https://www.kaggle.com/c/ieee-fraud-detection/discussion/108575#latest-643395). Chris is an amazing Kaggler."},{"metadata":{"trusted":true},"cell_type":"code","source":"train['d2_d4_comb'] = train['D2'].astype(str)+ '_' +train['D4'].astype(str)\ntest['d2_d4_comb'] = test['D2'].astype(str)+ '_' +test['D4'].astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for feature in ['id_02__id_20', 'id_02__D8', 'D11__DeviceInfo', 'DeviceInfo__P_emaildomain', 'P_emaildomain__C2', \n                'card2__dist1', 'card1__card5', 'card2__id_20', 'card5__P_emaildomain', 'addr1__card1']:\n\n    f1, f2 = feature.split('__')\n    train[feature] = train[f1].astype(str) + '_' + train[f2].astype(str)\n    test[feature] = test[f1].astype(str) + '_' + test[f2].astype(str)\n    \n    le = LabelEncoder()\n    le.fit(list(train[feature].astype(str).values) + list(test[feature].astype(str).values))\n    train[feature] = le.transform(list(train[feature].astype(str).values))\n    test[feature] = le.transform(list(test[feature].astype(str).values))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating unique IDs by combining two columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"def uid_fe(df):\n    df['uid'] = df['card1'].astype(str)+'_'+df['card2'].astype(str)\n    df['uid2'] = df['uid'].astype(str)+'_'+df['card3'].astype(str)+'_'+df['card5'].astype(str)\n    df['uid3'] = df['uid2'].astype(str)+'_'+df['addr1'].astype(str)+'_'+df['addr2'].astype(str)\n    df['uid4'] = df['uid3'].astype(str)+'_'+df['P_emaildomain'].astype(str)\n    df['uid5'] = df['uid3'].astype(str)+'_'+df['R_emaildomain'].astype(str)\n    return df\ntrain = uid_fe(train)\ntest = uid_fe(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering with GroupBy"},{"metadata":{},"cell_type":"markdown","source":"The code and features down below belongs to Andrew Lukyanenko. Thanks for his amazing kernel. And down forget to upvote his kernel [here](https://www.kaggle.com/artgor/eda-and-models)."},{"metadata":{"trusted":true},"cell_type":"code","source":"def andrew_fe(df):\n    df['TransactionAmt_to_mean_card1'] = df['TransactionAmt'] / df.groupby(['card1'])['TransactionAmt'].transform('mean')\n    df['TransactionAmt_to_mean_card4'] = df['TransactionAmt'] / df.groupby(['card4'])['TransactionAmt'].transform('mean')\n    df['TransactionAmt_to_std_card1'] = df['TransactionAmt'] / df.groupby(['card1'])['TransactionAmt'].transform('std')\n    df['TransactionAmt_to_std_card4'] = df['TransactionAmt'] / df.groupby(['card4'])['TransactionAmt'].transform('std')\n\n    df['id_02_to_mean_card1'] = df['id_02'] / df.groupby(['card1'])['id_02'].transform('mean')\n    df['id_02_to_mean_card4'] = df['id_02'] / df.groupby(['card4'])['id_02'].transform('mean')\n    df['id_02_to_std_card1'] = df['id_02'] / df.groupby(['card1'])['id_02'].transform('std')\n    df['id_02_to_std_card4'] = df['id_02'] / df.groupby(['card4'])['id_02'].transform('std')\n\n    df['D15_to_mean_card1'] = train['D15'] / df.groupby(['card1'])['D15'].transform('mean')\n    df['D15_to_mean_card4'] = train['D15'] / df.groupby(['card4'])['D15'].transform('mean')\n    df['D15_to_std_card1'] = train['D15'] / df.groupby(['card1'])['D15'].transform('std')\n    df['D15_to_std_card4'] = train['D15'] / df.groupby(['card4'])['D15'].transform('std')\n\n    df['D15_to_mean_addr1'] = df['D15'] / df.groupby(['addr1'])['D15'].transform('mean')\n    df['D15_to_mean_addr2'] = df['D15'] / df.groupby(['addr2'])['D15'].transform('mean')\n    df['D15_to_std_addr1'] = df['D15'] / df.groupby(['addr1'])['D15'].transform('std')\n    df['D15_to_std_addr2'] = df['D15'] / df.groupby(['addr2'])['D15'].transform('std')\n    return df\n\ntrain = andrew_fe(train)\ntest = andrew_fe(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering Based on ProductCD"},{"metadata":{},"cell_type":"markdown","source":"ProductCD is one of the most important features in this dataset. Let's check it. The code below belongs to this extensive [kernel](https://www.kaggle.com/kabure/extensive-eda-and-modeling-xgb-hyperopt). Please, go and upvote it!"},{"metadata":{"trusted":true},"cell_type":"code","source":"g1 = sns.countplot(x='ProductCD', hue='isFraud', data=train)\nplt.legend(title='Fraud', loc='best', labels=['No', 'Yes'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def product_fe(df):\n    df[\"card1_count\"] = df.groupby([\"card1\"])[\"ProductCD\"].transform(\"size\")\n    df[\"card2_count\"] = df.groupby([\"card2\"])[\"ProductCD\"].transform(\"size\")\n    df[\"card5_count\"] = df.groupby([\"card5\"])[\"ProductCD\"].transform(\"size\")\n    df[\"R_email_count\"] = df.groupby([\"R_emaildomain\"])[\"ProductCD\"].transform(\"size\")\n    df[\"P_email_count\"] = df.groupby([\"P_emaildomain\"])[\"ProductCD\"].transform(\"size\")\n    df[\"P_R_emails_count\"] = df.groupby([\"P_emaildomain\",\"R_emaildomain\"])[\"ProductCD\"].transform(\"size\")\n    df[\"addr1_count\"] = df.groupby([\"addr1\"])[\"ProductCD\"].transform(\"size\")\n    df[\"addr2_count\"] = df.groupby([\"addr2\"])[\"ProductCD\"].transform(\"size\")\n    df[\"joint_addr1_card1_count\"] = df.groupby([\"addr1\",\"card1\"])[\"ProductCD\"].transform(\"size\") #our MAGICAL FEATURE\n    df[\"joint_matches_count\"] = df.groupby([\"M4\",\"M5\", \"M6\"])[\"ProductCD\"].transform(\"size\")\n    df[\"joint_d2_d4_count\"] = df.groupby(['d2_d4_comb'])[\"ProductCD\"].transform(\"size\")\n    df[\"joint_card1_d2_d4_count\"] = df.groupby([\"card1\",\"d2_d4_comb\"])[\"ProductCD\"].transform(\"size\")\n    df[\"uid_pd5_count\"] = df.groupby([\"uid5\"])[\"ProductCD\"].transform(\"size\")\n    df[\"uid_pd5_count\"] = df.groupby([\"uid5\"])[\"ProductCD\"].transform(\"size\")\n    \n    return df\n\ntrain = product_fe(train)\ntest = product_fe(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## New Features with Transaction Amounts' Decimal Part with GroupBy"},{"metadata":{"trusted":true},"cell_type":"code","source":"def decimal_fe(df):\n    df['TransactionAmt_decimal'] = ((df['TransactionAmt'] - df['TransactionAmt'].astype(int)) * 1000).astype(int)\n    df['TransactionAmt_decimal_to_mean_card1'] = df['TransactionAmt_decimal'] / train.groupby(['card1'])['TransactionAmt_decimal'].transform('mean')\n    df['TransactionAmt_decimal_to_mean_card2'] = df['TransactionAmt_decimal'] / train.groupby(['card2'])['TransactionAmt_decimal'].transform('mean')\n    df['TransactionAmt_decimal_to_mean_card4'] = df['TransactionAmt_decimal'] / train.groupby(['card4'])['TransactionAmt_decimal'].transform('mean')\n    df['TransactionAmt_decimal_to_std_card1'] = df['TransactionAmt_decimal'] / train.groupby(['card1'])['TransactionAmt_decimal'].transform('std')\n    df['TransactionAmt_decimal_to_std_card2'] = df['TransactionAmt_decimal'] / train.groupby(['card2'])['TransactionAmt_decimal'].transform('std')\n    df['TransactionAmt_decimal_to_std_card4'] = df['TransactionAmt_decimal'] / train.groupby(['card4'])['TransactionAmt_decimal'].transform('std')\n    \n    return df\n\ntrain = decimal_fe(train)\ntest = decimal_fe(test)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After doing some tests, we found out that TransactionAmt_decimal column was increasing the gap between training and validation set. But the features that are made with this column was useful. Thus, we decided to stick with the new features and remove the TransactionAmt_decimal column itself."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop(labels=['TransactionAmt_decimal'],axis=1,inplace=True)\ntest.drop(labels=['TransactionAmt_decimal'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### New Features with E-Mail Domains"},{"metadata":{"trusted":true},"cell_type":"code","source":"emails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', 'scranton.edu': 'other', 'optonline.net': 'other', 'hotmail.co.uk': 'microsoft', 'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo', 'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', 'aim.com': 'aol', 'hotmail.de': 'microsoft', 'centurylink.net': 'centurylink', 'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', 'gmx.de': 'other', 'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', 'protonmail.com': 'other', 'hotmail.fr': 'microsoft', 'windstream.net': 'other', 'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo', 'yahoo.de': 'yahoo', 'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other', 'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft', 'verizon.net': 'yahoo', 'msn.com': 'microsoft', 'q.com': 'centurylink', 'prodigy.net.mx': 'att', 'frontier.com': 'yahoo', 'anonymous.com': 'other', 'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo', 'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', 'bellsouth.net': 'other', 'embarqmail.com': 'centurylink', 'cableone.net': 'other', 'hotmail.es': 'microsoft', 'mac.com': 'apple', 'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', 'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other', 'cox.net': 'other', 'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\nus_emails = ['gmail', 'net', 'edu']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for c in ['P_emaildomain', 'R_emaildomain']:\n    train[c + '_bin'] = train[c].map(emails)\n    test[c + '_bin'] = test[c].map(emails)\n    \n    train[c + '_suffix'] = train[c].map(lambda x: str(x).split('.')[-1])\n    test[c + '_suffix'] = test[c].map(lambda x: str(x).split('.')[-1])\n    \n    train[c + '_suffix'] = train[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n    test[c + '_suffix'] = test[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Some Frequency Encoding and GroupBy for Transaction Day of the Week, Transaction Hour and Unique IDs."},{"metadata":{"trusted":true},"cell_type":"code","source":"START_DATE = datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def setTime(df):\n    df['DT'] = df['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\n    df['DT_M'] = (df['DT'].dt.year-2017)*12 + df['DT'].dt.month\n    df['DT_W'] = (df['DT'].dt.year-2017)*52 + df['DT'].dt.weekofyear\n    df['DT_D'] = (df['DT'].dt.year-2017)*365 + df['DT'].dt.dayofyear\n    \n    df['DT_hour'] = df['DT'].dt.hour\n    df['DT_day_week'] = df['DT'].dt.dayofweek\n    df['DT_day'] = df['DT'].dt.day\n\n    df['TransactionAmt_to_mean_hour'] = df['TransactionAmt'] / df.groupby(['DT_hour'])['TransactionAmt'].transform('mean')\n    df['TransactionAmt_to_mean_dow'] = df['TransactionAmt'] / df.groupby(['DT_day_week'])['TransactionAmt'].transform('mean')\n    df['TransactionAmt_to_std_dow'] = df['TransactionAmt'] / df.groupby(['DT_day_week'])['TransactionAmt'].transform('std')\n    df['TransactionAmt_to_std_hour'] = df['TransactionAmt'] / df.groupby(['DT_hour'])['TransactionAmt'].transform('std')\n    return df\ntrain = setTime(train)\ntest= setTime(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in ['DT_M','DT_W','DT_D']:\n    temp_df = pd.concat([train[[col]], test[[col]]])\n    fq_encode = temp_df[col].value_counts().to_dict()\n            \n    train[col+'_total'] = train[col].map(fq_encode)\n    test[col+'_total']  = test[col].map(fq_encode)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"periods = ['DT_M','DT_W','DT_D']\ni_cols = ['uid']\nfor period in periods:\n    for col in i_cols:\n        new_column = col + '_' + period\n            \n        temp_df = pd.concat([train[[col,period]], test[[col,period]]])\n        temp_df[new_column] = temp_df[col].astype(str) + '_' + (temp_df[period]).astype(str)\n        fq_encode = temp_df[new_column].value_counts().to_dict()\n            \n        train[new_column] = (train[col].astype(str) + '_' + train[period].astype(str)).map(fq_encode)\n        test[new_column]  = (test[col].astype(str) + '_' + test[period].astype(str)).map(fq_encode)\n        \n        train[new_column] /= train[period+'_total']\n        test[new_column]  /= test[period+'_total']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"i_cols = ['card1','card2','card3','card5',\n          'C1','C2','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14',\n          'D1','D2','D3','D4','D5','D6','D7','D8',\n          'addr1','addr2',\n          'dist1',\n          'P_emaildomain', 'R_emaildomain',\n          'DeviceInfo',\n          'id_30','id_33',\n          'uid','uid2','uid3','uid4','uid5'\n         ]\n\nfor col in i_cols:\n    temp_df = pd.concat([train[[col]], test[[col]]])\n    fq_encode = temp_df[col].value_counts(dropna=False).to_dict()   \n    train[col+'_fq_enc'] = train[col].map(fq_encode)\n    test[col+'_fq_enc']  = test[col].map(fq_encode)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"i_cols = ['M1','M2','M3','M5','M6','M7','M8','M9']\n\nfor df in [train, test]:\n    df['M_sum'] = df[i_cols].sum(axis=1).astype(np.int8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in ['ProductCD','M4']:\n    temp_dict = train.groupby([col])['isFraud'].agg(['mean']).reset_index().rename(\n                                                        columns={'mean': col+'_target_mean'})\n    temp_dict.index = temp_dict[col].values\n    temp_dict = temp_dict[col+'_target_mean'].to_dict()\n\n    train[col+'_target_mean'] = train[col].map(temp_dict)\n    test[col+'_target_mean'] = test[col].map(temp_dict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Count Encoding for Training and Test Set"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encoding - count encoding for both train and test\nfor feature in ['card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'id_36']:\n    train[feature + '_count_full'] = train[feature].map(pd.concat([train[feature], test[feature]], ignore_index=True).value_counts(dropna=False))\n    test[feature + '_count_full'] = test[feature].map(pd.concat([train[feature], test[feature]], ignore_index=True).value_counts(dropna=False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#After some tests, we decided to drop uid column since it increases the gap between training and validation.\ntrain.drop(labels=['uid'],axis=1,inplace=True)\ntest.drop(labels=['uid'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for feature in ['d2_d4_comb', 'uid2', 'uid3', 'uid4', 'uid5', 'P_emaildomain_bin', 'P_emaildomain_suffix', 'R_emaildomain_bin', 'R_emaildomain_suffix']:\n\n    le = LabelEncoder()\n    le.fit(list(train[feature].astype(str).values) + list(test[feature].astype(str).values))\n    train[feature] = le.transform(list(train[feature].astype(str).values))\n    test[feature] = le.transform(list(test[feature].astype(str).values))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.sort_values('TransactionDT').drop(['isFraud', 'DT','TransactionDT','TransactionID'], axis=1)\nY = train.sort_values('TransactionDT')['isFraud']\nX_test = test.drop(['TransactionDT', 'DT','TransactionID'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train, test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"While we were testing our new features with the given parameters below, we decided to play with max_depth a bit to see if we are overfitting. We changed it as 3, and the difference between validation and training were decreased. It might be a good idea to optimize it. But in my opinion, optimizing it at the last day of the competition was our biggest mistake. Changing max depth parameter caused underfitting and huge shakeup."},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'num_leaves': 493,\n          'min_child_weight': 0.03454472573214212,\n          'feature_fraction': 0.3797454081646243,\n          'bagging_fraction': 0.4181193142567742,\n          'min_data_in_leaf': 106,\n          'objective': 'binary',\n          'max_depth': -1,\n          'learning_rate': 0.006883242363721497,\n          \"boosting_type\": \"gbdt\",\n          \"bagging_seed\": 11,\n          \"metric\": 'auc',\n          \"verbosity\": -1,\n          'reg_alpha': 0.3899927210061127,\n          'reg_lambda': 0.6485237330340494,\n          'random_state': 47\n         }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import datetime\nfrom time import time\nfrom sklearn.model_selection import KFold, TimeSeriesSplit, StratifiedKFold\nfolds = StratifiedKFold(n_splits=5)\n\nevals_result = {}\ntrainings = list()\naucs = list()\nfeature_importances = pd.DataFrame()\nfeature_importances['feature'] = X.columns\n\ntraining_start_time = time()\nfor fold, (trn_idx, test_idx) in enumerate(folds.split(X, Y)):\n    start_time = time()\n    print('Training on fold {}'.format(fold + 1))\n    \n    trn_data = lgb.Dataset(X.iloc[trn_idx], label=Y.iloc[trn_idx])\n    val_data = lgb.Dataset(X.iloc[test_idx], label=Y.iloc[test_idx])\n    clf = lgb.train(params, trn_data, 10000, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds=500, evals_result=evals_result)\n    \n    feature_importances['fold_{}'.format(fold + 1)] = clf.feature_importance()\n    aucs.append(clf.best_score['valid_1']['auc'])\n    trainings.append(clf.best_score['training']['auc'])\n\n    print('Fold {} finished in {}'.format(fold + 1, str(datetime.timedelta(seconds=time() - start_time))))\nprint('-' * 30)\nprint('Training has finished.')\nprint('Total training time is {}'.format(str(datetime.timedelta(seconds=time() - training_start_time))))\nprint('Mean AUC:', np.mean(aucs))\nprint('STD AUC:', np.std(aucs))\nprint('Training AUC:', np.mean(trainings))\nprint('Difference is:', np.mean(trainings)-np.mean(aucs))\nprint('-' * 30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = lgb.plot_metric(evals_result, metric='auc')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importances['average'] = feature_importances[['fold_{}'.format(fold + 1) for fold in range(folds.n_splits)]].mean(axis=1)\n\nplt.figure(figsize=(16, 16))\nsns.barplot(data=feature_importances.sort_values(by='average', ascending=False).head(50), x='average', y='feature');\nplt.title('50 TOP feature importance over {} folds average'.format(folds.n_splits));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_iter = clf.best_iteration","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = lgb.LGBMClassifier(**params, num_boost_round=best_iter)\nclf.fit(X, Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub['isFraud'] = clf.predict_proba(X_test)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv('lightgbmdownsampled400k.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I would like to thank my teammate Arda for his contributions, and all the competitors who shared kernels and their ideas. Any comment is much appreciated. If you own any codeblocks in this kernel, please don't hesitate to tell it in comments."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}