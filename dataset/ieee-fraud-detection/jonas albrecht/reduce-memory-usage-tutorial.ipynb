{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Hello and welcome to my memory usage reduction tutorial!\n\n\n**In this very short and simple tutorial I will show some insanely easy methods to reduce the memory/RAM usage of pandas dataframes.**\n\n**I will not do any stuff like imputing, encoding, handling missing values, feature selection,  NONE of that stuff, ONLY memory usage reduction.**\n\n**For this tutorial I will use the data of the IEEE fraud detection competition:**  https://www.kaggle.com/c/ieee-fraud-detection/submissions\n\n**If you are interested in a complete and detailed tutorial for this competition, feel free to have a look at this:** https://www.kaggle.com/jonas0/ieee-fraud-detection","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Overview\n \n## '1'. Load the data\n\n## '2'. Convert numerical datatypes to smaller ones\n\n## '3'. Convert categorical features to 'category' datatype\n\n## '4'. Summary","metadata":{}},{"cell_type":"markdown","source":"# 1. Load the data\n\n**For this short and simple tutorial we will only load the train_transaction data.**","metadata":{}},{"cell_type":"code","source":"print(\"loading data takes about 1 minute....\")\n\ntrain_transaction = pd.read_csv('/kaggle/input/ieee-fraud-detection/train_transaction.csv', index_col='TransactionID')\n#test_transaction = pd.read_csv('/kaggle/input/ieee-fraud-detection/test_transaction.csv', index_col='TransactionID')\n\n#train_identity = pd.read_csv('/kaggle/input/ieee-fraud-detection/train_identity.csv', index_col='TransactionID')\n#test_identity = pd.read_csv('/kaggle/input/ieee-fraud-detection/test_identity.csv', index_col='TransactionID')\n\n#sample_submission = pd.read_csv('/kaggle/input/ieee-fraud-detection/sample_submission.csv', index_col='TransactionID')\n\nprint(\"loading successful!\")","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's have a look at our data:**","metadata":{}},{"cell_type":"code","source":"print(\"shape of train_transaction: \", train_transaction.shape, \"\\n\")\n\nprint(\"info of train_transaction: \\n\")\n\nprint(train_transaction.info())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**As we can see this dataframe has 590540 rows and 393 columns.**\n\n**The .info() function of pandas is quite helpful, it shows us what datatypes the 393 columns have: float64(376), int64(3), object(14).**\n\n**And in the last line we can see that our dataframe uses about 1.7 GB of memory/RAM, let's see if we can reduce it :)**\n\n**Before we can reduce the memory usage of our dataframe, we have to detect the numerical and the categorical features first:**","metadata":{}},{"cell_type":"code","source":"# lets generate some useful lists of columns,\n# we want a list of numerical features\n# and a list of categorical features\n\nc = (train_transaction.dtypes == 'object')\nn = (train_transaction.dtypes != 'object')\ncat_cols = list(c[c].index)\nnum_cols = list(n[n].index) \n\nprint(cat_cols, \"\\n\")\nprint(\"number categorical features: \", len(cat_cols), \"\\n\\n\")\nprint(num_cols, \"\\n\")\nprint(\"number numerical features: \", len(num_cols))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Convert numerical datatypes to smaller ones","metadata":{}},{"cell_type":"markdown","source":"**Dataframes can contain two types of numerical values: integers and floats. These integers and floats have a certain datatype called int8/int16/int32/int64  or float16/float32/float64. (float8 does not exist)**\n\n**As you maybe already know, the higher the number in the datatype, the more memory it consumes. Down below is a chart listing all numerical datatypes together with their minimum and maximum value displayable and the maximum range which this datatype can cover.**\n\n**When you load a dataframe with pandas all numerical features are given by int64/float64 by default.**\n\n**If all values of a numerical feature do not exceed this range or the minimum or maximum value, you can convert this int64/float64  number down to a smaller datatype. No information is lost, but we can save a lot of memory.** ","metadata":{}},{"cell_type":"code","source":"# the int/float datatypes have the following ranges:\n\n#   int8:  -128 to 127, range = 255  \n\n#  int16:  -32,768 to 32,767, range = 65,535\n\n#  int32:  -2,147,483,648 to 2,147,483,647, range = 4,294,967,295\n\n#  int64:  -9,223,372,036,854,775,808 to 9,223,372,036,854,775,807,\n#           range = 18,446,744,073,709,551,615\n\n\n#  These ranges are the same for all float datatypes.\n#  By default all numerical columns in pandas are in int64 or float64.\n#  This means that when we find a numerical integer column whose \n#  values do not exceed one of the ranges shown above, we can then\n#  convert this datatype down to a smaller one. ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's look at our dataframe again before we start reducing the memory usage:**","metadata":{}},{"cell_type":"code","source":"print(\"train_transaction.info(): \\n\")\n\nprint(train_transaction.info())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The dataframes uses 1.7 GB and has 376 columns in float64, 3 columns in int64 and 14 columns in object.**\n\n**Let's write some code that checks if we can convert a numerical feature to a smaller datatype:**","metadata":{}},{"cell_type":"code","source":"#  this function detects all the numerical columns,\n#  that can be converted to a smaller datatype.\n\ndef detect_num_cols_to_shrink(list_of_num_cols, dataframe):\n \n    convert_to_int8 = []\n    convert_to_int16 = []\n    convert_to_int32 = []\n    \n    #  sadly the datatype float8 does not exist\n    convert_to_float16 = []\n    convert_to_float32 = []\n    \n    for col in list_of_num_cols:\n        \n        if dataframe[col].dtype in ['int', 'int8', 'int32', 'int64']:\n            describe_object = dataframe[col].describe()\n            minimum = describe_object[3]\n            maximum = describe_object[7]\n            diff = abs(maximum - minimum)\n\n            if diff < 255:\n                convert_to_int8.append(col)\n            elif diff < 65535:\n                convert_to_int16.append(col)\n            elif diff < 4294967295:\n                convert_to_int32.append(col)   \n                \n        elif dataframe[col].dtype in ['float', 'float16', 'float32', 'float64']:\n            describe_object = dataframe[col].describe()\n            minimum = describe_object[3]\n            maximum = describe_object[7]\n            diff = abs(maximum - minimum)\n\n            if diff < 65535:\n                convert_to_float16.append(col)\n            elif diff < 4294967295:\n                convert_to_float32.append(col) \n        \n    list_of_lists = []\n    list_of_lists.append(convert_to_int8)\n    list_of_lists.append(convert_to_int16)\n    list_of_lists.append(convert_to_int32)\n    list_of_lists.append(convert_to_float16)\n    list_of_lists.append(convert_to_float32)\n    \n    return list_of_lists","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's call the function and print all the numerical features we can convert:**","metadata":{}},{"cell_type":"code","source":"num_cols_to_shrink_trans = detect_num_cols_to_shrink(num_cols, train_transaction)\n\nconvert_to_int8 = num_cols_to_shrink_trans[0]\nconvert_to_int16 = num_cols_to_shrink_trans[1]\nconvert_to_int32 = num_cols_to_shrink_trans[2]\n\nconvert_to_float16 = num_cols_to_shrink_trans[3]\nconvert_to_float32 = num_cols_to_shrink_trans[4]\n\nprint(\"convert_to_int8 :\", convert_to_int8, \"\\n\")\nprint(\"convert_to_int16 :\", convert_to_int16, \"\\n\")\nprint(\"convert_to_int32 :\", convert_to_int32, \"\\n\")\n\nprint(\"convert_to_float16 :\", convert_to_float16, \"\\n\")\nprint(\"convert_to_float32 :\", convert_to_float32, \"\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**As we can see a lot of features can be converted,  most of them to float16, some more of them to float32.** ","metadata":{}},{"cell_type":"code","source":"print(\"starting with converting process....\")\n\n# convert the datatypes with .astype() \n\nfor col in convert_to_int16:\n    train_transaction[col] = train_transaction[col].astype('int16')  \n    \nfor col in convert_to_int32:\n    train_transaction[col] = train_transaction[col].astype('int32') \n\nfor col in convert_to_float16:\n    train_transaction[col] = train_transaction[col].astype('float16')\n    \nfor col in convert_to_float32:\n    train_transaction[col] = train_transaction[col].astype('float32')\n    \nprint(\"successfully converted!\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"train_transaction.info(): \\n\")   # now uses 548 MB\n\nprint(train_transaction.info(), \"\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Wow, the memory usage went down from 1.7 GB to 548 MB, so converting the numerical datatypes can really be worth it.**\n\n**In the next and final chapter we will transform the categorical columns from 'object' datatype to 'category'.**","metadata":{}},{"cell_type":"markdown","source":"# 3. Convert categorical features to 'category' datatype","metadata":{}},{"cell_type":"markdown","source":"**We have already created our list 'cat_cols' containing all categorical features.**\n\n**We can simply convert all of the 14 categorical features from 'object' datatype to 'category', we do not have to check for any conditions.**","metadata":{}},{"cell_type":"code","source":"for i in cat_cols:\n    \n    train_transaction[i] = train_transaction[i].astype('category')\n    \nprint(\"successfully converted all categorical features!\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Now we can check if the memory usage went down again:**","metadata":{}},{"cell_type":"code","source":"print(\"train_transaction.info(): \\n\")\n\nprint(train_transaction.info(), \"\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Well ok, converting the 14 categorical features from 'object' to 'category' only saved us about 55 MB, but there were only 14 categorical features.**","metadata":{}},{"cell_type":"markdown","source":"# 4. Summary\n\n\n**Converting the 379 numerical features saved about 1.1 GB, while converting the 14 categorical columns only saved about 55 MB.**\n\n**Considering the few lines of code and the low effort this whole procedure takes, it can really be worth it, since you can save Gigabytes of RAM.**","metadata":{}},{"cell_type":"markdown","source":"# Thank you for reading this tutorial :)","metadata":{}}]}