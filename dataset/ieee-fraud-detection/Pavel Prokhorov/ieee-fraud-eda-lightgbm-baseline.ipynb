{"cells":[{"metadata":{},"cell_type":"markdown","source":"# EDA, feature engineering, LightGBM baseline\n\n**Feature engineering:**\n* select ~280 features from 432 (-)\n* count 'null' values per row (-)\n* fill 'null' values with constant (+)\n* remove features that have more than 90% of same values (-)\n* make 'os', 'browser', 'device' from 'id_30', 'id_31' (-)\n* transform 'TransactionAmt' (+)\n* feature aggregates (-)\n* 'card1', ... count (+)\n* features interaction (+)\n* make 'day', 'hour' features (+)\n* make 'vendor', 'suffix', 'us' from email features (-)\n* remove timestamp (+)\n\n**Model:**\n* LightGBM\n* Optuna to get optimal parameters\n* 5 fold cross-validation"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport warnings\nwarnings.simplefilter('ignore')\n\nimport gc, sys\ngc.enable()\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ## Load data"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"%%time\ntrain_transaction = pd.read_csv('../input/train_transaction.csv', index_col='TransactionID')\ntrain_identity = pd.read_csv('../input/train_identity.csv', index_col='TransactionID')\ntrain = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\ndel train_transaction, train_identity\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntest_transaction = pd.read_csv('../input/test_transaction.csv', index_col='TransactionID')\ntest_identity = pd.read_csv('../input/test_identity.csv', index_col='TransactionID')\ntest = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)\ndel test_transaction, test_identity\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Keep selected features only"},{"metadata":{"trusted":true},"cell_type":"code","source":"# From https://www.kaggle.com/nroman/lgb-single-model-lb-0-9419\n\nselected_features = [\n    'TransactionAmt', 'ProductCD',\n    'card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'addr1', 'addr2', 'dist1',\n    'P_emaildomain', 'R_emaildomain',\n    'C1', 'C2', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14',\n    'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D8', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15',\n    'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9',\n    'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12',\n    'V13', 'V17', 'V19', 'V20', 'V29', 'V30', 'V33', 'V34', 'V35', 'V36',\n    'V37', 'V38', 'V40', 'V44', 'V45', 'V46', 'V47', 'V48', 'V49', 'V51',\n    'V52', 'V53', 'V54', 'V56', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63',\n    'V64', 'V69', 'V70', 'V71', 'V72', 'V73', 'V74', 'V75', 'V76', 'V78',\n    'V80', 'V81', 'V82', 'V83', 'V84', 'V85', 'V87', 'V90', 'V91', 'V92',\n    'V93', 'V94', 'V95', 'V96', 'V97', 'V99', 'V100', 'V126', 'V127', 'V128',\n    'V130', 'V131', 'V138', 'V139', 'V140', 'V143', 'V145', 'V146', 'V147', 'V149',\n    'V150', 'V151', 'V152', 'V154', 'V156', 'V158', 'V159', 'V160', 'V161', 'V162',\n    'V163', 'V164', 'V165', 'V166', 'V167', 'V169', 'V170', 'V171', 'V172', 'V173',\n    'V175', 'V176', 'V177', 'V178', 'V180', 'V182', 'V184', 'V187', 'V188', 'V189',\n    'V195', 'V197', 'V200', 'V201', 'V202', 'V203', 'V204', 'V205', 'V206', 'V207',\n    'V208', 'V209', 'V210', 'V212', 'V213', 'V214', 'V215', 'V216', 'V217', 'V219',\n    'V220', 'V221', 'V222', 'V223', 'V224', 'V225', 'V226', 'V227', 'V228', 'V229',\n    'V231', 'V233', 'V234', 'V238', 'V239', 'V242', 'V243', 'V244', 'V245', 'V246',\n    'V247', 'V249', 'V251', 'V253', 'V256', 'V257', 'V258', 'V259', 'V261', 'V262',\n    'V263', 'V264', 'V265', 'V266', 'V267', 'V268', 'V270', 'V271', 'V272', 'V273',\n    'V274', 'V275', 'V276', 'V277', 'V278', 'V279', 'V280', 'V282', 'V283', 'V285',\n    'V287', 'V288', 'V289', 'V291', 'V292', 'V294', 'V303', 'V304', 'V306', 'V307',\n    'V308', 'V310', 'V312', 'V313', 'V314', 'V315', 'V317', 'V322', 'V323', 'V324',\n    'V326', 'V329', 'V331', 'V332', 'V333', 'V335', 'V336', 'V338',\n    'id_01', 'id_02', 'id_03', 'id_05', 'id_06', 'id_09', 'id_11', 'id_12', 'id_13', 'id_14',\n    'id_15', 'id_17', 'id_19', 'id_20', 'id_30', 'id_31', 'id_32', 'id_33', 'id_36', 'id_37',\n    'id_38', 'DeviceType', 'DeviceInfo'\n]\n\nlen(selected_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns_to_drop = list(set(train.columns) - set(selected_features) - set(['isFraud', 'TransactionDT']))\nlen(columns_to_drop)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train = train.drop(columns_to_drop, axis=1)\n# test = test.drop(columns_to_drop, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Count 'null' values per row"},{"metadata":{"trusted":true},"cell_type":"code","source":"# train['null_columns'] = train.isna().sum(axis=1)\n# test['null_columns'] = test.isna().sum(axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fill 'null' values with constant"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.fillna(-999)\ntest = test.fillna(-999)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Remove features that have more than 90% of same values"},{"metadata":{"trusted":true},"cell_type":"code","source":"# columns with 90% null\n# many_null_columns_train = [c for c in train.columns if train[c].isnull().sum() / train.shape[0] > 0.9]\n\nmany_same_values_columns_train = [c for c in train.columns if train[c].value_counts(normalize=True).values[0] > 0.9]\n\n# value_counts(dropna=False, normalize=True)  ^^^\n# commented code not needed because of 'fillna(-999)'\n\nmany_same_values_columns_test = [c for c in test.columns if test[c].value_counts(normalize=True).values[0] > 0.9]\n\ncolumns_to_drop = list(set(many_same_values_columns_train + many_same_values_columns_test))\ncolumns_to_drop.remove('isFraud')\nlen(columns_to_drop)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train = train.drop(columns_to_drop, axis=1)\n# test = test.drop(columns_to_drop, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Transform id_30, id_31"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\nos = ['Windows', 'iOS', 'Android', 'Mac OS', 'Linux']\nbrowser = ['chrome', 'mobile safari', 'ie', 'safari', 'edge', 'firefox']\ndevice = ['Windows', 'iOS', 'MacOS', 'SM', 'SAMSUNG', 'Moto', 'LG']\n\ndef to_pattern(x: str, patterns):\n    for p in patterns:\n        t = re.compile('^(' + p + ').*')\n        if t.match(x):\n            return p\n    return 'other'\n\ndef make_os_feature(df):\n    return df['id_30'].map(lambda x: to_pattern(str(x), os))\n\ndef make_browser_feature(df):\n    return df['id_31'].map(lambda x: to_pattern(str(x), browser))\n\ndef make_device_feature(df):\n    return df['DeviceInfo'].map(lambda x: to_pattern(str(x), device))\n\n# train['os'] = make_os_feature(train)\n# train['browser'] = make_browser_feature(train)\n# train['device'] = make_device_feature(train)\n\n# test['os'] = make_os_feature(test)\n# test['browser'] = make_browser_feature(test)\n# test['device'] = make_device_feature(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Transform 'TransactionAmt'"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['TransactionAmt_decimal'] = ((train['TransactionAmt'] - train['TransactionAmt'].astype(int)) * 1000).astype(int)\ntest['TransactionAmt_decimal'] = ((test['TransactionAmt'] - test['TransactionAmt'].astype(int)) * 1000).astype(int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature aggregates"},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_aggregates(df, feature_to_group_by, feature):\n    out = pd.DataFrame(index=df.index)\n    \n    # filter for -999 is needed because NaN values were filled by this constant\n    agg = df[df[feature] != -999].groupby([feature_to_group_by])[feature]\n    \n    new_feature = feature + '_' + feature_to_group_by\n    out[new_feature + '_mean'] = df[feature] / agg.transform('mean')\n    out[new_feature + '_std' ] = df[feature] / agg.transform('std')\n    \n    return out\n\ndef merge_aggregates(df, feature_to_group_by, feature):\n    return df.merge(make_aggregates(df, feature_to_group_by, feature), how='left', left_index=True, right_index=True)\n\n\n# for pair in [('card1', 'TransactionAmt'), ('card1', 'D15'), ('addr1', 'TransactionAmt')]:\n#    train = merge_aggregates(train, pair[0], pair[1])\n#    test = merge_aggregates(test, pair[0], pair[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 'card1', ... count"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Based on https://www.kaggle.com/nroman/eda-for-cis-fraud-detection\n\ndef make_count_1(feature, df):\n    temp = df[feature].value_counts(dropna=False)\n    new_feature = feature + '_count'\n    df[new_feature] = df[feature].map(temp)\n\ndef make_count_2(feature, train, test):\n    temp = pd.concat([train[feature], test[feature]], ignore_index=True).value_counts(dropna=False)\n    new_feature = feature + '_count'\n    train[new_feature] = train[feature].map(temp)\n    test[new_feature] = test[feature].map(temp)\n\n\nfor feature in ['id_01', 'id_31', 'id_33', 'id_36']:\n    make_count_1(feature, train)\n    make_count_1(feature, test)\n    \nfor feature in ['card1', 'id_36']:\n    make_count_2(feature, train, test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Features interaction"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Based on https://www.kaggle.com/nroman/lgb-single-model-lb-0-9419\n\ndef features_interaction(df, feature_1, feature_2):\n    return df[feature_1].astype(str) + '_' + df[feature_2].astype(str)\n\nfeatures_interactions = [\n    'id_02__id_20',\n    'id_02__D8',\n    'D11__DeviceInfo',\n    'DeviceInfo__P_emaildomain',\n    'P_emaildomain__C2',\n    'card2__dist1',\n    'card1__card5',\n    'card2__id_20',\n    'card5__P_emaildomain',\n    'addr1__card1'\n]\n\nfor new_feature in features_interactions:\n    feature_1, feature_2 = new_feature.split('__')\n    train[new_feature] = features_interaction(train, feature_1, feature_2)\n    test[new_feature] = features_interaction(test, feature_1, feature_2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Make day and hour features"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# From https://www.kaggle.com/fchmiel/day-and-time-powerful-predictive-feature\n\ndef make_day_feature(df, offset=0.58, tname='TransactionDT'):\n    \"\"\"\n    Creates a day of the week feature, encoded as 0-6.\n    \"\"\"\n    days = df[tname] / (3600 * 24)\n    encoded_days = np.floor(days - 1 + offset) % 7\n    return encoded_days\n\ndef make_hour_feature(df, tname='TransactionDT'):\n    \"\"\"\n    Creates an hour of the day feature, encoded as 0-23.\n    \"\"\"\n    hours = df[tname] / (3600)\n    encoded_hours = np.floor(hours) % 24\n    return encoded_hours","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(train['TransactionDT'] / (3600 * 24), bins=1800)\nplt.xlim(70, 78)\nplt.xlabel('Days')\nplt.ylabel('Number of transactions')\nplt.ylim(0,1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Weekday'] = make_day_feature(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(train.groupby('Weekday').mean()['isFraud'])\nplt.ylim(0, 0.04)\nplt.xlabel('Encoded day')\nplt.ylabel('Fraction of fraudulent transactions')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Hour'] = make_hour_feature(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(train.groupby('Hour').mean()['isFraud'])\nplt.xlabel('Encoded hour')\nplt.ylabel('Fraction of fraudulent transactions')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['Weekday'] = make_day_feature(test)\ntest['Hour'] = make_hour_feature(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Transform emails"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Based on https://www.kaggle.com/c/ieee-fraud-detection/discussion/100499\n\nemail_vendors = {\n    'gmail': 'google',\n    'att.net': 'att',\n    'twc.com': 'spectrum',\n    'scranton.edu': 'other',\n    'optonline.net': 'other',\n    'hotmail.co.uk': 'microsoft',\n    'comcast.net': 'other',\n    'yahoo.com.mx': 'yahoo',\n    'yahoo.fr': 'yahoo',\n    'yahoo.es': 'yahoo',\n    'charter.net': 'spectrum',\n    'live.com': 'microsoft',\n    'aim.com': 'aol',\n    'hotmail.de': 'microsoft',\n    'centurylink.net': 'centurylink',\n    'gmail.com': 'google',\n    'me.com': 'apple',\n    'earthlink.net': 'other',\n    'gmx.de': 'other',\n    'web.de': 'other',\n    'cfl.rr.com': 'other',\n    'hotmail.com': 'microsoft',\n    'protonmail.com': 'other',\n    'hotmail.fr': 'microsoft',\n    'windstream.net': 'other',\n    'outlook.es': 'microsoft',\n    'yahoo.co.jp': 'yahoo',\n    'yahoo.de': 'yahoo',\n    'servicios-ta.com': 'other',\n    'netzero.net': 'other',\n    'suddenlink.net': 'other',\n    'roadrunner.com': 'other',\n    'sc.rr.com': 'other',\n    'live.fr': 'microsoft',\n    'verizon.net': 'yahoo',\n    'msn.com': 'microsoft',\n    'q.com': 'centurylink',\n    'prodigy.net.mx': 'att',\n    'frontier.com': 'yahoo',\n    'anonymous.com': 'other',\n    'rocketmail.com': 'yahoo',\n    'sbcglobal.net': 'att',\n    'frontiernet.net': 'yahoo',\n    'ymail.com': 'yahoo',\n    'outlook.com': 'microsoft',\n    'mail.com': 'other',\n    'bellsouth.net': 'other',\n    'embarqmail.com': 'centurylink',\n    'cableone.net': 'other',\n    'hotmail.es': 'microsoft',\n    'mac.com': 'apple',\n    'yahoo.co.uk': 'yahoo',\n    'netzero.com': 'other',\n    'yahoo.com': 'yahoo',\n    'live.com.mx': 'microsoft',\n    'ptd.net': 'other',\n    'cox.net': 'other',\n    'aol.com': 'aol',\n    'juno.com': 'other',\n    'icloud.com': 'apple',\n    -999: 'undefined'\n}\n\nus_emails = ['gmail', 'net', 'edu']\n\ndef transform_email(df, column):\n    out = pd.DataFrame(index=df.index)\n    \n    # vendor\n    out[column + '_vendor'] = df[column].map(email_vendors)\n    \n    # suffix\n    out[column + '_suffix'] = df[column].map(lambda x: str(x).split('.')[-1])\n    \n    # US\n    out[column + '_us'] = out[column + '_suffix'].map(lambda x: 'us' if str(x) in us_emails else 'other')\n    \n    return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# temp = transform_email(train, 'P_emaildomain')\n# train = train.merge(temp, how='left', left_index=True, right_index=True)\n# temp = transform_email(train, 'R_emaildomain')\n# train = train.merge(temp, how='left', left_index=True, right_index=True)\n# del temp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# temp = transform_email(test, 'P_emaildomain')\n# test = test.merge(temp, how='left', left_index=True, right_index=True)\n# temp = transform_email(test, 'R_emaildomain')\n# test = test.merge(temp, how='left', left_index=True, right_index=True)\n# del temp","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Remove timestamp"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.sort_values('TransactionDT').drop('TransactionDT', axis=1)\ntest = test.sort_values('TransactionDT').drop('TransactionDT', axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Encode categorial features"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n\ndef encode_categorial_features_fit(df, columns_to_encode):\n    encoders = {}\n    for c in columns_to_encode:\n        if c in df.columns:\n            encoder = LabelEncoder()\n            encoder.fit(df[c].astype(str).values)\n            encoders[c] = encoder\n    return encoders\n\ndef encode_categorial_features_transform(df, encoders):\n    out = pd.DataFrame(index=df.index)\n    for c in encoders.keys():\n        if c in df.columns:\n            out[c] = encoders[c].transform(df[c].astype(str).values)\n    return out\n\n\ncategorial_features_columns = [\n    'id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18', 'id_19', 'id_20', 'id_21',\n    'id_22', 'id_23', 'id_24', 'id_25', 'id_26', 'id_27', 'id_28', 'id_29', 'id_30', 'id_31',\n    'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38',\n    'DeviceType', 'DeviceInfo', 'ProductCD', 'P_emaildomain', 'R_emaildomain',\n    'card1', 'card2', 'card3', 'card4', 'card5', 'card6',\n    'addr1', 'addr2',\n    'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9',\n    'P_emaildomain_vendor', 'P_emaildomain_suffix', 'P_emaildomain_us',\n    'R_emaildomain_vendor', 'R_emaildomain_suffix', 'R_emaildomain_us' # ,\n    # 'os', 'browser', 'device'\n] + features_interactions\n\ncategorial_features_encoders = encode_categorial_features_fit(\n    pd.concat([train, test], join='outer', sort=False),\n    categorial_features_columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = encode_categorial_features_transform(train, categorial_features_encoders)\ncolumns_to_drop = list(set(categorial_features_columns) & set(train.columns))\ntrain = train.drop(columns_to_drop, axis=1).merge(temp, how='left', left_index=True, right_index=True)\ndel temp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = encode_categorial_features_transform(test, categorial_features_encoders)\ncolumns_to_drop = list(set(categorial_features_columns) & set(test.columns))\ntest = test.drop(columns_to_drop, axis=1).merge(temp, how='left', left_index=True, right_index=True)\ndel temp","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Free memory"},{"metadata":{"trusted":true},"cell_type":"code","source":"# From https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\n\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n#        else:\n#            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Extract target variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = train['isFraud'].copy()\nx_train = train.drop('isFraud', axis=1)\ndel train\n\nx_test = test.copy()\ndel test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ## LightGBM"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold, train_test_split\nimport lightgbm as lgb\nfrom sklearn.metrics import roc_auc_score\nimport optuna","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def objective(trial):\n    \n    params = {\n        'objective': 'binary',\n        'metric': 'binary_logloss',\n        'boosting_type': 'gbdt',\n        'is_unbalance': False,\n        'boost_from_average': True,\n        'num_threads': 4,\n        \n        'num_leaves': trial.suggest_int('num_leaves', 10, 1000),\n        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 200),\n        'min_child_weight': trial.suggest_loguniform('min_child_weight', 0.001, 0.1),\n        'max_depth': trial.suggest_int('max_depth', 1, 100),\n        'bagging_fraction' : trial.suggest_loguniform('bagging_fraction', .5, .99),\n        'feature_fraction' : trial.suggest_loguniform('feature_fraction', .5, .99),\n        'lambda_l1': trial.suggest_loguniform('lambda_l1', 0.1, 2),\n        'lambda_l2': trial.suggest_loguniform('lambda_l2', 0.1, 2)\n    }\n    \n    scores = []\n    \n    cv = KFold(n_splits=5)\n    for train_idx, valid_idx in cv.split(x_train, y_train):\n        \n        x_train_train = x_train.iloc[train_idx]\n        y_train_train = y_train.iloc[train_idx]\n        x_train_valid = x_train.iloc[valid_idx]\n        y_train_valid = y_train.iloc[valid_idx]\n        \n        lgb_train = lgb.Dataset(data=x_train_train.astype('float32'), label=y_train_train.astype('float32'))\n        lgb_valid = lgb.Dataset(data=x_train_valid.astype('float32'), label=y_train_valid.astype('float32'))\n        \n        lgb_model = lgb.train(params, lgb_train, valid_sets=lgb_valid, verbose_eval=100)\n        y = lgb_model.predict(x_train_valid.astype('float32'), num_iteration=lgb_model.best_iteration)\n        \n        score = roc_auc_score(y_train_valid.astype('float32'), y)\n        print('Fold score:', score)\n        scores.append(score)\n    \n    average_score = sum(scores) / len(scores)\n    print('Average score:', average_score)\n    return average_score\n\n\n# study = optuna.create_study(direction='maximize')\n# study.optimize(objective, n_trials=40)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# study.best_trial","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    'objective': 'binary',\n    'metric': 'auc',\n    'boosting_type': 'gbdt',\n    'is_unbalance': False,\n    'boost_from_average': True,\n    'num_threads': 4,\n    \n#     'num_leaves': study.best_trial.params['num_leaves'],\n#     'min_data_in_leaf': study.best_trial.params['min_data_in_leaf'],\n#     'min_child_weight': study.best_trial.params['min_child_weight'],\n#     'max_depth': study.best_trial.params['max_depth'],\n#     'bagging_fraction' : study.best_trial.params['bagging_fraction'],\n#     'feature_fraction' : study.best_trial.params['feature_fraction'],\n#     'lambda_l1': study.best_trial.params['lambda_l1'],\n#     'lambda_l2': study.best_trial.params['lambda_l2']\n    \n    'num_iterations': 5000,\n    'learning_rate': 0.006,\n    'early_stopping_round': 100,\n    \n    'num_leaves': 396,\n    'min_data_in_leaf': 39,\n    'min_child_weight': 0.06046322469906681,\n    'max_depth': 95,\n    'bagging_fraction' : 0.7119300452429695,\n    'feature_fraction' : 0.6121880522551549,\n    'lambda_l1': 0.30642889371923837,\n    'lambda_l2': 0.6028242575804484\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_splits=5\n\ny = np.zeros(x_test.shape[0])\n\nfeature_importances = []\n\ncv = KFold(n_splits=n_splits)\nfor train_idx, valid_idx in cv.split(x_train, y_train):\n    \n    x_train_train = x_train.iloc[train_idx]\n    y_train_train = y_train.iloc[train_idx]\n    x_train_valid = x_train.iloc[valid_idx]\n    y_train_valid = y_train.iloc[valid_idx]\n    \n    lgb_train = lgb.Dataset(data=x_train_train.astype('float32'), label=y_train_train.astype('float32'))\n    lgb_valid = lgb.Dataset(data=x_train_valid.astype('float32'), label=y_train_valid.astype('float32'))\n    \n    lgb_model = lgb.train(params, lgb_train, valid_sets=lgb_valid, verbose_eval=100, early_stopping_rounds=100)\n    \n    y_part = lgb_model.predict(x_test.astype('float32'), num_iteration=lgb_model.best_iteration)\n    y += y_part / n_splits\n    \n    feature_importances.append(lgb_model.feature_importance())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\n\nfeature_importance_df = pd.concat([\n    pd.Series(x_train.columns),\n    pd.Series(np.mean(feature_importances, axis=0))], axis=1)\nfeature_importance_df.columns = ['featureName', 'importance']\n\n# get top 100 features sorted by importance descending\ntemp = feature_importance_df.sort_values(by=['importance'], ascending=False).head(100)\n\nplt.figure(figsize=(20, 20))\nsns.barplot(x=\"importance\", y=\"featureName\", data=temp)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submit predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/sample_submission.csv', index_col='TransactionID')\nsubmission['isFraud'] = y\nsubmission.to_csv('lightgbm.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}