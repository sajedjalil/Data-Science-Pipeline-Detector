{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\nimport os","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"train_transaction = pd.read_csv('../input/train_transaction.csv', index_col='TransactionID')\ntest_transaction = pd.read_csv('../input/test_transaction.csv', index_col='TransactionID')\n\ntrain_identity = pd.read_csv('../input/train_identity.csv', index_col='TransactionID')\ntest_identity = pd.read_csv('../input/test_identity.csv', index_col='TransactionID')\n\nsample_submission = pd.read_csv('../input/sample_submission.csv', index_col='TransactionID')\n\ntrain = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\ntest = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)\ndel train_transaction, test_transaction, train_identity, test_identity","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true},"cell_type":"markdown","source":"# Counting 'duplicate' transactions\n\nIt's been noted [publically](https://www.kaggle.com/c/ieee-fraud-detection/discussion/105261#latest-605153) that there are some duplicate transactions in the databases. In this kernel I show how you can make a feature which counts the number of duplicate transactions in a given time window. \n\n**Notes:**\n- This is not a fast implementation, some vectorized method using pandas methods would be better. Please share if you are able to speed up this implementation. \n- I haven't tested this feature yet. Still struggling with a CV mechanism to use! Please share if you find it improves your CV.\n- You can tune the window_size."},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_duplicate_feature(df, cols_to_match, window_size=3):\n    \"\"\"\n    Counts the number of duplicate transactions in a temporal window\n    of width 2*window_size.\n    \n    Parameters:\n    -----------\n    df : pd.DataFrame, \n        The dataset.\n        \n    cols_to_match : list,\n        Columns which are required to be equal to be considered a match. \n        \n    window_size : (float, int)\n        Controls the size of the window (in minutes) to search for\n        duplicates.\n\n    Returns:\n    --------\n    duplicate_counts : list,\n        List of length df.shape[0] which counts the duplicate transactions.\n    \"\"\"\n    window_size = window_size * 60 # convert to seconds\n    df['TransactionDT'] = df['TransactionDT']\n    # Thanks to WeNYoBen on SO for speeding this bit up: https://stackoverflow.com/questions/57101482/counting-duplicate-row-within-a-rolling-window-of-a-pandas-df\n    s = pd.Series(df[cols_to_match].apply(tuple,1).map(hash).values,\n                  index=df.TransactionDT)\n    duplicate_count = [sum(s.loc[x-window_size:x+window_size]==y)-1 for x ,y in zip(s.index,s)]\n    \n    return duplicate_count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = ['TransactionAmt','ProductCD','card1','card2','card3',\n        'card4','card5','card6','addr1','addr2']\n\ntest['duplicate_count'] = make_duplicate_feature(test, cols_to_match=cols)\ntrain['duplicate_count'] = make_duplicate_feature(train, cols_to_match=cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Quick analysis of the new feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train['duplicate_count']>0].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Percentage of transactions which have duplicates\n\nA significant number of transactions have duplicates. There is a large difference between train and test sets."},{"metadata":{"trusted":true},"cell_type":"code","source":"for df, name in zip([train,test], ['train','test']):\n    num_duplicates = df[df['duplicate_count']>0].shape[0] / df.shape[0]\n    print(f'{num_duplicates*100:.2f} % transactions have duplicates in the {name} set.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### How fraudlent are duplicate transactions\n\nFeatures with a duplicate appear to be more likely to be a fraudulent. Some binning of this feature may help."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.groupby('duplicate_count').mean()['isFraud']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# its important to look at the count, small counts are unreliable\ntrain.groupby('duplicate_count').count()['isFraud']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### And a graph of this table to help with visualisation"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(train.groupby('duplicate_count').mean()['isFraud'], color='k')\nplt.ylabel('Fraction fraudulent')\nplt.xlabel('Duplicate count')\nplt.xlim(0, 40)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}