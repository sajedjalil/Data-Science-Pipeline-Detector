{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# General imports\nimport numpy as np\nimport pandas as pd\nimport os, sys, gc, warnings, random, datetime, math\n\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom scipy.stats import ks_2samp\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"########################### Helpers\n#################################################################################\n## Seeder\n# :seed to make all processes deterministic     # type: int\ndef seed_everything(seed=0):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    \n## Memory Reducer\n# :df pandas dataframe to reduce size             # type: pd.DataFrame()\n# :verbose                                        # type: bool\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### Vars\n#################################################################################\nSEED = 42\nseed_everything(SEED)\nLOCAL_TEST = True\nTARGET = 'isFraud'\nSTART_DATE = datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"----"},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### DATA LOAD\n#################################################################################\nprint('Load Data')\ntrain_df = pd.read_pickle('../input/ieee-data-minification/train_transaction.pkl')\n\nif LOCAL_TEST:\n    \n    # Convert TransactionDT to \"Month\" time-period. \n    # We will also drop penultimate block \n    # to \"simulate\" test set values difference\n    train_df['DT_M'] = train_df['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\n    train_df['DT_M'] = (train_df['DT_M'].dt.year-2017)*12 + train_df['DT_M'].dt.month \n    test_df = train_df[train_df['DT_M']==train_df['DT_M'].max()].reset_index(drop=True)\n    train_df = train_df[train_df['DT_M']<(train_df['DT_M'].max()-1)].reset_index(drop=True)\n    del train_df['DT_M'], test_df['DT_M']\n    \nelse:\n    test_df = pd.read_pickle('../input/ieee-data-minification/test_transaction.pkl')\n    \nprint('Shape control:', train_df.shape, test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### We need some time features for our experiments\nfor df in [train_df, test_df]:\n    \n    # Temporary variables for aggregation\n    df['DT'] = df['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\n    df['DT_M'] = ((df['DT'].dt.year-2017)*12 + df['DT'].dt.month).astype(np.int8)\n    df['DT_W'] = ((df['DT'].dt.year-2017)*52 + df['DT'].dt.weekofyear).astype(np.int8)\n    df['DT_D'] = ((df['DT'].dt.year-2017)*365 + df['DT'].dt.dayofyear).astype(np.int16)\n    \n    df['DT_hour'] = (df['DT'].dt.hour).astype(np.int8)\n    df['DT_day_week'] = (df['DT'].dt.dayofweek).astype(np.int8)\n    df['DT_day_month'] = (df['DT'].dt.day).astype(np.int8)\n\n# Total transactions per timeblock\nfor col in ['DT_M','DT_W','DT_D']:\n    temp_df = pd.concat([train_df[[col]], test_df[[col]]])\n    fq_encode = temp_df[col].value_counts().to_dict()\n            \n    train_df[col+'_total'] = train_df[col].map(fq_encode)\n    test_df[col+'_total']  = test_df[col].map(fq_encode)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### I would like to check DT_hour and D9 (that is also hour of transaction)\ntest_group = ['DT_hour','D9']\nperiods = ['TransactionDT']\ntemp_df = pd.concat([train_df[test_group+periods], test_df[test_group+periods]])\nfor period in periods:\n    for col in test_group:\n        for df in [temp_df]:\n            df.set_index(period)[col].plot(style='.', title=col, figsize=(15, 3))\n            plt.show()\n            \n# Seems ok\n# What mean nans in D9 - I don't know\n# Is it Server time or local time - I don't know\n# But it doesn't metter for us now","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### We wanted to check mean hour for normal transactions and for fraud\ntest_group = ['TransactionID','TransactionDT','DT_hour','D9','DT_M','DT_W','DT_D','isFraud']\n\ndf = pd.concat([train_df[test_group], test_df[test_group]])\n\ndf_fraud = df[df['isFraud']==1]\ndf_not_fraud = df[df['isFraud']==0]\n\nprint('#'*10)\nprint('Mean hour by fraud group')\nprint('Fraud mean hour', df_fraud['DT_hour'].mean())\nprint('Not Fraud mean hour', df_not_fraud['DT_hour'].mean())\n\nprint('#'*10)\nprint('Mean fraud by hour')\nprint(df.groupby(['DT_hour'])['isFraud'].agg(['mean']).sort_values(by='mean', ascending=False))\n\nprint('#'*10)\nprint('Mean fraud:')\n\n\ncmap = sns.cubehelix_palette(rot=-.2, as_cmap=True)\nfor time_block in ['DT_M','DT_W','DT_D']:\n    plt.figure(figsize=(16, 6))\n    for period in list(df[time_block].unique()):\n        data = df[df[time_block]==period].groupby(['DT_hour'])['isFraud'].agg(['mean']).reset_index()\n        ax = sns.scatterplot(x=\"DT_hour\", y=\"mean\", palette=cmap,data=data)            \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}