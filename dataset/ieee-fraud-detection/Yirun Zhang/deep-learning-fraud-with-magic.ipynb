{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Deep Learning Fraud with Magic\n  \nIn this kernel, we build a Deep Learning model for fraud deteciton with magic features introduced in Chris's notebook.\n\n## Reference\n\nChris's [XGB Fraud with Magic - [0.9600]][1] and [RAPIDS - Feature Engineering - Fraud - [0.96]][2]\n\n\n[1]: https://www.kaggle.com/cdeotte/xgb-fraud-with-magic-0-9600\n[2]: https://www.kaggle.com/cdeotte/rapids-feature-engineering-fraud-0-96/data?","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Load Data\nWe will load all the data except 219 V columns that were determined redundant by correlation analysis [here][1]\n\n[1]: https://www.kaggle.com/cdeotte/eda-for-columns-v-and-id","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"FEATURE = True\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport datetime\nimport numpy as np, pandas as pd, os, gc\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import roc_auc_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport xgboost as xgb\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras import callbacks\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras import utils\nfrom tensorflow.keras.utils import plot_model\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nprint(\"XGBoost version:\", xgb.__version__)\n\nprint(\"Tensorflow version \" + tf.__version__)\nAUTO = tf.data.experimental.AUTOTUNE\n\n# COLUMNS WITH STRINGS\nstr_type = ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain','M1', 'M2', 'M3', 'M4','M5',\n            'M6', 'M7', 'M8', 'M9', 'id_12', 'id_15', 'id_16', 'id_23', 'id_27', 'id_28', 'id_29', 'id_30', \n            'id_31', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38', 'DeviceType', 'DeviceInfo']\nstr_type += ['id-12', 'id-15', 'id-16', 'id-23', 'id-27', 'id-28', 'id-29', 'id-30', \n            'id-31', 'id-33', 'id-34', 'id-35', 'id-36', 'id-37', 'id-38']\n\n# FIRST 53 COLUMNS\ncols = ['TransactionID', 'TransactionDT', 'TransactionAmt',\n       'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6',\n       'addr1', 'addr2', 'dist1', 'dist2', 'P_emaildomain', 'R_emaildomain',\n       'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11',\n       'C12', 'C13', 'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8',\n       'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'M1', 'M2', 'M3', 'M4',\n       'M5', 'M6', 'M7', 'M8', 'M9']\n\n# V COLUMNS TO LOAD DECIDED BY CORRELATION EDA\n# https://www.kaggle.com/cdeotte/eda-for-columns-v-and-id\nv =  [1, 3, 4, 6, 8, 11]\nv += [13, 14, 17, 20, 23, 26, 27, 30]\nv += [36, 37, 40, 41, 44, 47, 48]\nv += [54, 56, 59, 62, 65, 67, 68, 70]\nv += [76, 78, 80, 82, 86, 88, 89, 91]\n\n#v += [96, 98, 99, 104] #relates to groups, no NAN \nv += [107, 108, 111, 115, 117, 120, 121, 123] # maybe group, no NAN\nv += [124, 127, 129, 130, 136] # relates to groups, no NAN\n\n# LOTS OF NAN BELOW\nv += [138, 139, 142, 147, 156, 162] #b1\nv += [165, 160, 166] #b1\nv += [178, 176, 173, 182] #b2\nv += [187, 203, 205, 207, 215] #b2\nv += [169, 171, 175, 180, 185, 188, 198, 210, 209] #b2\nv += [218, 223, 224, 226, 228, 229, 235] #b3\nv += [240, 258, 257, 253, 252, 260, 261] #b3\nv += [264, 266, 267, 274, 277] #b3\nv += [220, 221, 234, 238, 250, 271] #b3\n\nv += [294, 284, 285, 286, 291, 297] # relates to grous, no NAN\nv += [303, 305, 307, 309, 310, 320] # relates to groups, no NAN\nv += [281, 283, 289, 296, 301, 314] # relates to groups, no NAN\n#v += [332, 325, 335, 338] # b4 lots NAN\n\ncols += ['V'+str(x) for x in v]\ndtypes = {}\nfor c in cols+['id_0'+str(x) for x in range(1,10)]+['id_'+str(x) for x in range(10,34)]+\\\n    ['id-0'+str(x) for x in range(1,10)]+['id-'+str(x) for x in range(10,34)]:\n        dtypes[c] = 'float32'\nfor c in str_type: dtypes[c] = 'category'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Detect hardware, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MIXED_PRECISION = False\nXLA_ACCELERATE = True\n\nif MIXED_PRECISION:\n    from tensorflow.keras.mixed_precision import experimental as mixed_precision\n    if tpu: policy = tf.keras.mixed_precision.experimental.Policy('mixed_bfloat16')\n    else: policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n    mixed_precision.set_policy(policy)\n    print('Mixed precision enabled')\n\nif XLA_ACCELERATE:\n    tf.config.optimizer.set_jit(True)\n    print('Accelerated Linear Algebra enabled')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nif FEATURE:\n    \n    # LOAD TRAIN\n    X_train = pd.read_csv('../input/ieee-with-feature/IEEE_Train.csv', index_col = 'TransactionID')\n    # LOAD TEST\n    X_test = pd.read_csv('../input/ieee-with-feature/IEEE_Test.csv', index_col = 'TransactionID')\n    # LOAD TARGET\n    y_train = pd.read_csv('../input/ieee-with-feature/IEEE_Target.csv', index_col = 'TransactionID')\n    \nelse:\n\n    # LOAD TRAIN\n    X_train = pd.read_csv('../input/ieee-fraud-detection/train_transaction.csv',index_col='TransactionID', dtype=dtypes, usecols=cols+['isFraud'])\n    train_id = pd.read_csv('../input/ieee-fraud-detection/train_identity.csv',index_col='TransactionID', dtype=dtypes)\n    X_train = X_train.merge(train_id, how='left', left_index=True, right_index=True)\n    # LOAD TEST\n    X_test = pd.read_csv('../input/ieee-fraud-detection/test_transaction.csv',index_col='TransactionID', dtype=dtypes, usecols=cols)\n    test_id = pd.read_csv('../input/ieee-fraud-detection/test_identity.csv',index_col='TransactionID', dtype=dtypes)\n    fix = {o:n for o, n in zip(test_id.columns, train_id.columns)}\n    test_id.rename(columns=fix, inplace=True)\n    X_test = X_test.merge(test_id, how='left', left_index=True, right_index=True)\n    # TARGET\n    y_train = X_train['isFraud'].copy()\n    del train_id, test_id, X_train['isFraud']; x = gc.collect()\n    # PRINT STATUS\n    print('Train shape',X_train.shape,'test shape',X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n\n# for c in X_train.columns:\n    \n#     if X_train[c].dtype == 'float64':\n    \n#         X_train[c] = X_train[c].astype('float32')\n#         X_test[c] = X_test[c].astype('float32')\n        \n#     if X_train[c].dtype == 'int64':\n    \n#         X_train[c] = X_train[c].astype('int32')\n#         X_test[c] = X_test[c].astype('int32')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for c in X_train.columns:\n    \n    print(X_train[c].dtype)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Normalize D Columns\nThe D Columns are \"time deltas\" from some point in the past. We will transform the D Columns into their point in the past. This will stop the D columns from increasing with time. The formula is `D15n = Transaction_Day - D15` and `Transaction_Day = TransactionDT/(24*60*60)`. Afterward we multiple this number by negative one.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"if not FEATURE:\n\n    # PLOT ORIGINAL D\n    plt.figure(figsize=(15,5))\n    plt.scatter(X_train.TransactionDT,X_train.D15)\n    plt.title('Original D15')\n    plt.xlabel('Time')\n    plt.ylabel('D15')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if not FEATURE:\n\n    # NORMALIZE D COLUMNS\n    for i in range(1,16):\n        if i in [1,2,3,5,9]: continue\n        X_train['D'+str(i)] =  X_train['D'+str(i)] - X_train.TransactionDT/np.float32(24*60*60)\n        X_test['D'+str(i)] = X_test['D'+str(i)] - X_test.TransactionDT/np.float32(24*60*60) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if not FEATURE:\n\n    # PLOT TRANSFORMED D\n    plt.figure(figsize=(15,5))\n    plt.scatter(X_train.TransactionDT,X_train.D15)\n    plt.title('Transformed D15')\n    plt.xlabel('Time')\n    plt.ylabel('D15n')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%%time\n\nif not FEATURE:\n\n    # LABEL ENCODE AND MEMORY REDUCE\n    for i,f in enumerate(X_train.columns):\n        # FACTORIZE CATEGORICAL VARIABLES\n        if (np.str(X_train[f].dtype)=='category')|(X_train[f].dtype=='object'): \n            df_comb = pd.concat([X_train[f],X_test[f]],axis=0)\n            df_comb,_ = df_comb.factorize(sort=True)\n            if df_comb.max()>32000: print(f,'needs int32')\n            X_train[f] = df_comb[:len(X_train)].astype('int16')\n            X_test[f] = df_comb[len(X_train):].astype('int16')\n        # SHIFT ALL NUMERICS POSITIVE. SET NAN to -1\n        elif f not in ['TransactionAmt','TransactionDT']:\n            mn = np.min((X_train[f].min(),X_test[f].min()))\n            X_train[f] -= np.float32(mn)\n            X_test[f] -= np.float32(mn)\n            X_train[f].fillna(-1,inplace=True)\n            X_test[f].fillna(-1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Encoding Functions\nBelow are 5 encoding functions. (1) `encode_FE` does frequency encoding where it combines train and test first and then encodes. (2) `encode_LE` is a label encoded for categorical features (3) `encode_AG` makes aggregated features such as aggregated mean and std (4) `encode_CB` combines two columns (5) `encode_AG2` makes aggregated features where it counts how many unique values of one feature is within a group. For more explanation about feature engineering, see the discussion [here][1]\n\n[1]: https://www.kaggle.com/c/ieee-fraud-detection/discussion/108575#latest-641841","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# FREQUENCY ENCODE TOGETHER\ndef encode_FE(df1, df2, cols):\n    for col in cols:\n        df = pd.concat([df1[col],df2[col]])\n        vc = df.value_counts(dropna=True, normalize=True).to_dict()\n        vc[-1] = -1\n        nm = col+'_FE'\n        df1[nm] = df1[col].map(vc)\n        df1[nm] = df1[nm].astype('float32')\n        df2[nm] = df2[col].map(vc)\n        df2[nm] = df2[nm].astype('float32')\n        print(nm,', ',end='')\n        \n# LABEL ENCODE\ndef encode_LE(col,train=X_train,test=X_test,verbose=True):\n    df_comb = pd.concat([train[col],test[col]],axis=0)\n    df_comb,_ = df_comb.factorize(sort=True)\n    nm = col\n    if df_comb.max()>32000: \n        train[nm] = df_comb[:len(train)].astype('int32')\n        test[nm] = df_comb[len(train):].astype('int32')\n    else:\n        train[nm] = df_comb[:len(train)].astype('int16')\n        test[nm] = df_comb[len(train):].astype('int16')\n    del df_comb; x=gc.collect()\n    if verbose: print(nm,', ',end='')\n        \n# GROUP AGGREGATION MEAN AND STD\n# https://www.kaggle.com/kyakovlev/ieee-fe-with-some-eda\ndef encode_AG(main_columns, uids, aggregations=['mean'], train_df=X_train, test_df=X_test, \n              fillna=True, usena=False):\n    # AGGREGATION OF MAIN WITH UID FOR GIVEN STATISTICS\n    for main_column in main_columns:  \n        for col in uids:\n            for agg_type in aggregations:\n                new_col_name = main_column+'_'+col+'_'+agg_type\n                temp_df = pd.concat([train_df[[col, main_column]], test_df[[col,main_column]]])\n                if usena: temp_df.loc[temp_df[main_column]==-1,main_column] = np.nan\n                temp_df = temp_df.groupby([col])[main_column].agg([agg_type]).reset_index().rename(\n                                                        columns={agg_type: new_col_name})\n\n                temp_df.index = list(temp_df[col])\n                temp_df = temp_df[new_col_name].to_dict()   \n\n                train_df[new_col_name] = train_df[col].map(temp_df).astype('float32')\n                test_df[new_col_name]  = test_df[col].map(temp_df).astype('float32')\n                \n                if fillna:\n                    train_df[new_col_name].fillna(-1,inplace=True)\n                    test_df[new_col_name].fillna(-1,inplace=True)\n                \n                print(\"'\"+new_col_name+\"'\",', ',end='')\n                \n# COMBINE FEATURES\ndef encode_CB(col1,col2,df1=X_train,df2=X_test):\n    nm = col1+'_'+col2\n    df1[nm] = df1[col1].astype(str)+'_'+df1[col2].astype(str)\n    df2[nm] = df2[col1].astype(str)+'_'+df2[col2].astype(str) \n    encode_LE(nm,verbose=False)\n    print(nm,', ',end='')\n    \n# GROUP AGGREGATION NUNIQUE\ndef encode_AG2(main_columns, uids, train_df=X_train, test_df=X_test):\n    for main_column in main_columns:  \n        for col in uids:\n            comb = pd.concat([train_df[[col]+[main_column]],test_df[[col]+[main_column]]],axis=0)\n            mp = comb.groupby(col)[main_column].agg(['nunique'])['nunique'].to_dict()\n            train_df[col+'_'+main_column+'_ct'] = train_df[col].map(mp).astype('float32')\n            test_df[col+'_'+main_column+'_ct'] = test_df[col].map(mp).astype('float32')\n            print(col+'_'+main_column+'_ct, ',end='')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering\nWe will now engineer features. All of these features where chosen because each increases local validation. The procedure for engineering features is as follows. First you think of an idea and create a new feature. Then you add it to your model and evaluate whether local validation AUC increases or decreases. If AUC increases keep the feature, otherwise discard the feature.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nif not FEATURE:\n\n    # TRANSACTION AMT CENTS\n    X_train['cents'] = (X_train['TransactionAmt'] - np.floor(X_train['TransactionAmt'])).astype('float32')\n    X_test['cents'] = (X_test['TransactionAmt'] - np.floor(X_test['TransactionAmt'])).astype('float32')\n    print('cents, ', end='')\n    # FREQUENCY ENCODE: ADDR1, CARD1, CARD2, CARD3, P_EMAILDOMAIN\n    encode_FE(X_train,X_test,['addr1','card1','card2','card3','P_emaildomain'])\n    # COMBINE COLUMNS CARD1+ADDR1, CARD1+ADDR1+P_EMAILDOMAIN\n    encode_CB('card1','addr1')\n    encode_CB('card1_addr1','P_emaildomain')\n    # FREQUENCY ENOCDE\n    encode_FE(X_train,X_test,['card1_addr1','card1_addr1_P_emaildomain'])\n    # GROUP AGGREGATE\n    encode_AG(['TransactionAmt','D9','D11'],['card1','card1_addr1','card1_addr1_P_emaildomain'],['mean','std'],usena=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Selection - Time Consistency\nWe added 28 new feature above. We have already removed 219 V Columns from correlation analysis done [here][1]. So we currently have 242 features now. We will now check each of our 242 for \"time consistency\". We will build 242 models. Each model will be trained on the first month of the training data and will only use one feature. We will then predict the last month of the training data. We want both training AUC and validation AUC to be above `AUC = 0.5`. It turns out that 19 features fail this test so we will remove them. Additionally we will remove 7 D columns that are mostly NAN. More techniques for feature selection are listed [here][2]\n\n[1]: https://www.kaggle.com/cdeotte/eda-for-columns-v-and-id\n[2]: https://www.kaggle.com/c/ieee-fraud-detection/discussion/111308","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"if not FEATURE:\n\n    cols = list( X_train.columns )\n    cols.remove('TransactionDT')\n    for c in ['D6','D7','D8','D9','D12','D13','D14']:\n        cols.remove(c)\n\n    # FAILED TIME CONSISTENCY TEST\n    for c in ['C3','M5','id_08','id_33']:\n        cols.remove(c)\n    for c in ['card4','id_07','id_14','id_21','id_30','id_32','id_34']:\n        cols.remove(c)\n    for c in ['id_'+str(x) for x in range(22,28)]:\n        cols.remove(c)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if not FEATURE:\n    \n    print('NOW USING THE FOLLOWING',len(cols),'FEATURES.')\n    np.array(cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if not FEATURE:\n\n    START_DATE = datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')\n    X_train['DT_M'] = X_train['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\n    X_train['DT_M'] = (X_train['DT_M'].dt.year-2017)*12 + X_train['DT_M'].dt.month \n\n    X_test['DT_M'] = X_test['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\n    X_test['DT_M'] = (X_test['DT_M'].dt.year-2017)*12 + X_test['DT_M'].dt.month ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The Magic Feature - UID\nWe will now create and use the MAGIC FEATURES. First we create a UID which will help our model find clients (credit cards). This UID isn't perfect. Many UID values contain 2 or more clients inside. However our model will detect this and by adding more splits with its trees, it will split these UIDs and find the single clients (credit cards).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"if not FEATURE:\n\n    X_train['day'] = X_train.TransactionDT / (24*60*60)\n    X_train['uid'] = X_train.card1_addr1.astype(str)+'_'+np.floor(X_train.day-X_train.D1).astype(str)\n\n    X_test['day'] = X_test.TransactionDT / (24*60*60)\n    X_test['uid'] = X_test.card1_addr1.astype(str)+'_'+np.floor(X_test.day-X_test.D1).astype(str)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Group Aggregation Features\nFor our model to use the new UID, we need to make lots of aggregated group features. We will add 47 new features! The pictures in the introduction to this notebook explain why this works. Note that after aggregation, we remove UID from our model. We don't use UID directly.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nif not FEATURE:\n\n    # FREQUENCY ENCODE UID\n    encode_FE(X_train,X_test,['uid'])\n    # AGGREGATE \n    encode_AG(['TransactionAmt','D4','D9','D10','D15'],['uid'],['mean','std'],fillna=True,usena=True)\n    # AGGREGATE\n    encode_AG(['C'+str(x) for x in range(1,15) if x!=3],['uid'],['mean'],X_train,X_test,fillna=True,usena=True)\n    # AGGREGATE\n    encode_AG(['M'+str(x) for x in range(1,10)],['uid'],['mean'],fillna=True,usena=True)\n    # AGGREGATE\n    encode_AG2(['P_emaildomain','dist1','DT_M','id_02','cents'], ['uid'], train_df=X_train, test_df=X_test)\n    # AGGREGATE\n    encode_AG(['C14'],['uid'],['std'],X_train,X_test,fillna=True,usena=True)\n    # AGGREGATE \n    encode_AG2(['C13','V314'], ['uid'], train_df=X_train, test_df=X_test)\n    # AGGREATE \n    encode_AG2(['V127','V136','V309','V307','V320'], ['uid'], train_df=X_train, test_df=X_test)\n    # NEW FEATURE\n    X_train['outsider15'] = (np.abs(X_train.D1-X_train.D15)>3).astype('int8')\n    X_test['outsider15'] = (np.abs(X_test.D1-X_test.D15)>3).astype('int8')\n    print('outsider15')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"cols = list( X_train.columns )\ncols.remove('TransactionDT')\nfor c in ['D6','D7','D8','D9','D12','D13','D14']:\n    cols.remove(c)\nfor c in ['DT_M','day','uid']:\n    cols.remove(c)\n\n# FAILED TIME CONSISTENCY TEST\nfor c in ['C3','M5','id_08','id_33']:\n    cols.remove(c)\nfor c in ['card4','id_07','id_14','id_21','id_30','id_32','id_34']:\n    cols.remove(c)\nfor c in ['id_'+str(x) for x in range(22,28)]:\n    cols.remove(c)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('NOW USING THE FOLLOWING',len(cols),'FEATURES.')\nnp.array(cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define Deep Learning Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model(data, catcols, numcols, hidden_units, spatial_dropout_rate, dropout_rate):    \n    inp_cat = []\n    outputs = []\n        \n    for c in catcols: \n            \n        num_unique_values = int(data[c].nunique())\n        embed_dim = int(min(np.ceil((num_unique_values) / 2), 50))\n        inp = layers.Input(shape = (1, ))\n        out = layers.Embedding(num_unique_values + 1, embed_dim, name = c)(inp)\n        out = layers.SpatialDropout1D(spatial_dropout_rate)(out)\n        out = layers.Reshape((embed_dim, ))(out)\n        inp_cat.append(inp)\n        outputs.append(out)\n        \n    inp_num = layers.Input(shape = (len(numcols),))\n    outputs.append(inp_num)\n\n    x = layers.Concatenate()(outputs)\n    x = layers.BatchNormalization()(x)\n    \n    for units in hidden_units:\n        \n        x = layers.Dense(units, activation = \"relu\")(x)\n        x = layers.Dropout(dropout_rate)(x)\n        x = layers.BatchNormalization()(x)\n    \n    y = layers.Dense(1, activation = 'sigmoid')(x)\n\n    model = Model(inputs = [inp_cat, inp_num], outputs = y)\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"catcols = []\nnumcols = []\n\nfor c in X_train[cols].columns:\n    if (X_train[c].dtype == 'int64') or (X_train[c].dtype == 'int32'):\n        catcols.append(c)\n    else:\n        numcols.append(c)\n        \nprint(catcols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for c in catcols:\n\n    le = LabelEncoder()\n    \n    le.fit(np.concatenate([X_train[c], X_test[c]]))\n    X_train[c] = le.transform(X_train[c])\n    X_test[c] = le.transform(X_test[c])\n    \ndel le\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nscaler = StandardScaler()\n\nscaler.fit(pd.concat([X_train[numcols], X_test[numcols]]))\nX_train[numcols] = scaler.transform(X_train[numcols])\nX_test[numcols] = scaler.transform(X_test[numcols])\n\ndel scaler\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hidden_units = [1024, 512, 256]\nspatial_dropout_rate = 0.3\ndropout_rate = 0.5\n\ndata = pd.concat([X_train[cols], X_test[cols]])\n\nwith strategy.scope():\n\n    model = create_model(data, catcols, numcols, hidden_units, spatial_dropout_rate, dropout_rate)\n    model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = [tf.metrics.AUC(name = 'AUC')])\n\nmodel.summary()\n\ndel data\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Local Validation\nWe will now perform local validation with the new magic features included. Chris' local validation now achieves AUC = 0.9472 and Konstantin's local validation achieves AUC = 0.9343. Note that without the magic features we achieved AUC = 0.9363 and AUC = 0.9241. We gained AUC 0.01 in both validations therefore our LB should increase from 0.95 to 0.96. Konstantin's LGBM with magic scores Konstantin local validation AUC = 0.9377 [here][1]\n\n[1]: https://www.kaggle.com/kyakovlev/ieee-basic-fe-part-1","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# # CHRIS - TRAIN 75% PREDICT 25%\n# idxT = X_train.index[:3*len(X_train)//4]\n# idxV = X_train.index[3*len(X_train)//4:]\n\n# # KONSTANTIN - TRAIN 4 SKIP 1 PREDICT 1 MONTH\n# #idxT = X_train.index[:417559]\n# #idxV = X_train.index[-89326:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\n\nclass CosineAnnealingScheduler(callbacks.Callback):\n    \"\"\"Cosine annealing scheduler.\n    \"\"\"\n\n    def __init__(self, T_max, eta_max, eta_min = 0, verbose = 0):\n        super(CosineAnnealingScheduler, self).__init__()\n        self.T_max = T_max\n        self.eta_max = eta_max\n        self.eta_min = eta_min\n        self.verbose = verbose\n\n    def on_epoch_begin(self, epoch, logs = None):\n        if not hasattr(self.model.optimizer, 'lr'):\n            raise ValueError('Optimizer must have a \"lr\" attribute.')\n        lr = self.eta_min + (self.eta_max - self.eta_min) * (1 + math.cos(math.pi * epoch / self.T_max)) / 2\n        K.set_value(self.model.optimizer.lr, lr)\n        if self.verbose > 0:\n            print('\\nEpoch %05d: CosineAnnealingScheduler setting learning '\n                  'rate to %s.' % (epoch + 1, lr))\n\n    def on_epoch_end(self, epoch, logs = None):\n        logs = logs or {}\n        logs['lr'] = K.get_value(self.model.optimizer.lr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# EPOCHS = 100\n# ES_P = 10\n# LR_MAX = 1e-3\n# LR_MIN = 1e-5\n# BATCH_SIZE = 4096\n\n# es = callbacks.EarlyStopping(monitor = 'val_AUC', min_delta = 0.0001, patience = ES_P, mode = 'max', \n#                              baseline = None, restore_best_weights = True, verbose = 0)\n\n# cas = CosineAnnealingScheduler(EPOCHS, LR_MAX, LR_MIN)\n\n# ckp = callbacks.ModelCheckpoint('best_model.h5', monitor = 'val_AUC', verbose = 0, \n#                                 save_best_only = True, save_weights_only = True, mode = 'max')\n\n# x_tr = [X_train.loc[idxT, c] for c in catcols] + [X_train.loc[idxT, numcols]]\n# y_tr = y_train.loc[idxT]\n\n# x_val = [X_train.loc[idxV, c] for c in catcols] + [X_train.loc[idxV, numcols]]\n# y_val = y_train.loc[idxV]\n\n# history = model.fit(x_tr, y_tr, validation_data = (x_val, y_val), callbacks = [ckp, cas, es], \n#                     epochs = EPOCHS, batch_size = BATCH_SIZE, verbose = 1)\n\n# hist = pd.DataFrame(history.history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# clf = xgb.XGBClassifier( \n#     n_estimators=2000,\n#     max_depth=12, \n#     learning_rate=0.02, \n#     subsample=0.8,\n#     colsample_bytree=0.4, \n#     missing=-1, \n#     eval_metric='auc',\n#     #nthread=4,\n#     #tree_method='hist' \n#     tree_method='gpu_hist' \n# )\n# h = clf.fit(X_train.loc[idxT,cols], y_train.loc[idxT], \n#     eval_set=[(X_train.loc[idxV,cols],y_train.loc[idxV])],\n#     verbose=50, early_stopping_rounds=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# feature_imp = pd.DataFrame(sorted(zip(clf.feature_importances_,cols)), columns=['Value','Feature'])\n\n# plt.figure(figsize=(20, 10))\n# sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False).iloc[:50])\n# plt.title('XGB96 Most Important')\n# plt.tight_layout()\n# plt.show()\n\n# del clf, h; x=gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predict test.csv","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"oof = np.zeros(len(X_train))\npreds = np.zeros(len(X_test))\n\nskf = GroupKFold(n_splits=6)\nfor i, (idxT0, idxV0) in enumerate( skf.split(X_train, y_train, groups=X_train['DT_M']) ):\n    month = X_train.iloc[idxV0]['DT_M'].iloc[0]\n    print('Fold',i,'withholding month',month)\n    print('rows of train =',len(idxT0),'rows of holdout =',len(idxV0))\n    \n    idxT = X_train.index[idxT0]\n    idxV = X_train.index[idxV0]\n    \n    hidden_units = [1024, 512, 256]\n    spatial_dropout_rate = 0.3\n    dropout_rate = 0.5\n    \n    data = pd.concat([X_train[cols], X_test[cols]])\n\n    with strategy.scope():\n\n        clf = create_model(data, catcols, numcols, hidden_units, spatial_dropout_rate, dropout_rate)\n        clf.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = [tf.metrics.AUC(name = 'AUC')])\n\n    del data\n    x=gc.collect()\n    \n    EPOCHS = 100\n    ES_P = 10\n    RLR_P = 5\n#     LR_MAX = 1e-3\n#     LR_MIN = 1e-5\n    BATCH_SIZE = 4096\n\n    es = callbacks.EarlyStopping(monitor = 'val_AUC', min_delta = 0.0001, patience = ES_P, mode = 'max', \n                                 baseline = None, restore_best_weights = True, verbose = 0)\n    \n    rlr = callbacks.ReduceLROnPlateau(monitor = 'val_AUC', patience = RLR_P, min_lr = 1e-5, verbose = 0, mode = 'max')\n\n#     cas = CosineAnnealingScheduler(EPOCHS, LR_MAX, LR_MIN)\n\n    ckp = callbacks.ModelCheckpoint(f'best_model_{i}.h5', monitor = 'val_AUC', verbose = 0, \n                                    save_best_only = True, save_weights_only = True, mode = 'max')\n\n    x_tr = [X_train.loc[idxT, c] for c in catcols] + [X_train.loc[idxT, numcols]]\n    y_tr = y_train.loc[idxT]\n\n    x_val = [X_train.loc[idxV, c] for c in catcols] + [X_train.loc[idxV, numcols]]\n    y_val = y_train.loc[idxV]\n    \n    x_tt = [X_test[c] for c in catcols] + [X_test[numcols]]\n    \n    h = clf.fit(x_tr, y_tr, validation_data = (x_val, y_val), callbacks = [ckp, rlr, es], \n                epochs = EPOCHS, batch_size = BATCH_SIZE, verbose = 2)\n\n    oof[idxV0] += clf.predict(x_val)[:, 0]\n    preds += clf.predict(x_tt)[:, 0] / skf.n_splits\n    \n    K.clear_session()\n    \n    del h, clf\n    x=gc.collect()\n    \nprint('#'*20)\nprint ('DL OOF CV=',roc_auc_score(y_train,oof))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# oof = np.zeros(len(X_train))\n# preds = np.zeros(len(X_test))\n\n# skf = GroupKFold(n_splits=6)\n# for i, (idxT, idxV) in enumerate( skf.split(X_train, y_train, groups=X_train['DT_M']) ):\n#     month = X_train.iloc[idxV]['DT_M'].iloc[0]\n#     print('Fold',i,'withholding month',month)\n#     print(' rows of train =',len(idxT),'rows of holdout =',len(idxV))\n#     clf = xgb.XGBClassifier(\n#         n_estimators=5000,\n#         max_depth=12,\n#         learning_rate=0.02,\n#         subsample=0.8,\n#         colsample_bytree=0.4,\n#         missing=-1,\n#         eval_metric='auc',\n#         # USE CPU\n#         #nthread=4,\n#         #tree_method='hist'\n#         # USE GPU\n#         tree_method='gpu_hist' \n#     )        \n#     h = clf.fit(X_train[cols].iloc[idxT], y_train.iloc[idxT], \n#             eval_set=[(X_train[cols].iloc[idxV],y_train.iloc[idxV])],\n#             verbose=100, early_stopping_rounds=200)\n\n#     oof[idxV] += clf.predict_proba(X_train[cols].iloc[idxV])[:,1]\n#     preds += clf.predict_proba(X_test[cols])[:,1]/skf.n_splits\n#     del h, clf\n#     x=gc.collect()\n# print('#'*20)\n# print ('XGB96 OOF CV=',roc_auc_score(y_train,oof))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plt.hist(oof,bins=100)\n# plt.ylim((0,5000))\n# plt.title('XGB OOF')\n# plt.show()\n\n# X_train['oof'] = oof\n# X_train.reset_index(inplace=True)\n# X_train[['TransactionID','oof']].to_csv('oof_xgb_96.csv')\n# X_train.set_index('TransactionID',drop=True,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Kaggle Submission File XGB_96","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = pd.read_csv('../input/ieee-fraud-detection/sample_submission.csv')\nsample_submission.isFraud = preds\nsample_submission.to_csv('sub_dl.csv',index=False)\n\nplt.hist(sample_submission.isFraud,bins=100)\nplt.ylim((0,5000))\nplt.title('DL Submission')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sample_submission = pd.read_csv('../input/ieee-fraud-detection/sample_submission.csv')\n# sample_submission.isFraud = preds\n# sample_submission.to_csv('sub_xgb_96.csv',index=False)\n\n# plt.hist(sample_submission.isFraud,bins=100)\n# plt.ylim((0,5000))\n# plt.title('XGB96 Submission')\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Post Process File XGB_96_PP\nOur final submission is an ensemble of XGB, CatBoost, and LGBM. Then we post process the ensemble. We will not load the CatBoost and LGBM here, but we will show you the post process. Konstantin wrote a script [here][1] that finds precise UIDs (more precise than `card1_addr1_D1n`). We believe each to be an individual client (credit card). Analysis shows us that all transactions from a single client (one of Konstantin's UIDs) are either all `isFraud=0` or all `isFraud=1`. In other words, all their predictions are the same. Therefore our post process is to replace all predictions from one client with their average prediction including the `isFraud` values from the train dataset. We have two slightly different versions so we apply them sequentially.\n\nApplying post process on our XGB model increases its Public LB to 0.9618 from LB 0.9602. And increases its Private LB to 0.9341 from LB 0.9324. This is an improvement of LB 0.0016 !!\n\n[1]: https://www.kaggle.com/kyakovlev/ieee-uid-detection-v6","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test['isFraud'] = sample_submission.isFraud.values\nX_train['isFraud'] = y_train.values\ncomb = pd.concat([X_train[['isFraud']],X_test[['isFraud']]],axis=0)\n\nuids = pd.read_csv('/kaggle/input/ieee-submissions-and-uids/uids_v4_no_multiuid_cleaning..csv',usecols=['TransactionID','uid']).rename({'uid':'uid2'},axis=1)\ncomb = comb.merge(uids,on='TransactionID',how='left')\nmp = comb.groupby('uid2').isFraud.agg(['mean'])\ncomb.loc[comb.uid2>0,'isFraud'] = comb.loc[comb.uid2>0].uid2.map(mp['mean'])\n\nuids = pd.read_csv('/kaggle/input/ieee-submissions-and-uids/uids_v1_no_multiuid_cleaning.csv',usecols=['TransactionID','uid']).rename({'uid':'uid3'},axis=1)\ncomb = comb.merge(uids,on='TransactionID',how='left')\nmp = comb.groupby('uid3').isFraud.agg(['mean'])\ncomb.loc[comb.uid3>0,'isFraud'] = comb.loc[comb.uid3>0].uid3.map(mp['mean'])\n\nsample_submission.isFraud = comb.iloc[len(X_train):].isFraud.values\nsample_submission.to_csv('sub_dl_PP.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_test['isFraud'] = sample_submission.isFraud.values\n# X_train['isFraud'] = y_train.values\n# comb = pd.concat([X_train[['isFraud']],X_test[['isFraud']]],axis=0)\n\n# uids = pd.read_csv('/kaggle/input/ieee-submissions-and-uids/uids_v4_no_multiuid_cleaning..csv',usecols=['TransactionID','uid']).rename({'uid':'uid2'},axis=1)\n# comb = comb.merge(uids,on='TransactionID',how='left')\n# mp = comb.groupby('uid2').isFraud.agg(['mean'])\n# comb.loc[comb.uid2>0,'isFraud'] = comb.loc[comb.uid2>0].uid2.map(mp['mean'])\n\n# uids = pd.read_csv('/kaggle/input/ieee-submissions-and-uids/uids_v1_no_multiuid_cleaning.csv',usecols=['TransactionID','uid']).rename({'uid':'uid3'},axis=1)\n# comb = comb.merge(uids,on='TransactionID',how='left')\n# mp = comb.groupby('uid3').isFraud.agg(['mean'])\n# comb.loc[comb.uid3>0,'isFraud'] = comb.loc[comb.uid3>0].uid3.map(mp['mean'])\n\n# sample_submission.isFraud = comb.iloc[len(X_train):].isFraud.values\n# sample_submission.to_csv('sub_xgb_96_PP.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}