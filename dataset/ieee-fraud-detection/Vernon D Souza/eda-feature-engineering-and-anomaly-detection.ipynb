{"cells":[{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://i2.wp.com/computopedia.com/wp-content/uploads/2020/01/online-fraud-image.png?fit=399%2C300&ssl=1\">","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Detection of Anomalies in the transactions\n\nThe prediction is based on the column 'isFraud' which contains binary values of 0 and 1.\nOur task is to predict the probability of a fraudulent transaction.\n\nThe data consists of 2 files. One of which contains identity information i.e train_identitiy.csv and the other contains transaction information i.e train_transaction.csv. The similar equivalent is also available in the testing set.\n\nAvailable details of the columns are as mentioned below:\n\n- Transaction Table \n  - TransactionDT: timedelta from a given reference datetime (not an actual timestamp)\n  - TransactionAMT: transaction payment amount in USD\n  - ProductCD: product code, the product for each transaction\n  - card1 - card6: payment card information, such as card type, card category, issue bank, country, etc.\n  - addr: address\n  - dist: distance\n  - P_ and (R__) emaildomain: purchaser and recipient email domain\n  - C1-C14: counting, such as how many addresses are found to be associated with the payment card, etc. The actual meaning is masked.\n  - D1-D15: timedelta, such as days between previous transaction, etc.\n  - M1-M9: match, such as names on card and address, etc.\n  - Vxxx: Vesta engineered rich features, including ranking, counting, and other entity relations.\n  \n\n- Identity Table \n  - DeviceType\n  - DeviceInfo\n  - id12 - id38\n  \n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"- Our First approach would be to perform an exploratory data analysis to better understand the features\n- Then using the EDA, we can perform Feature Engineering\n- Then search for the best hyper-parameters in LightGBM model\n- Then implement our features and target values in LightGBM model\n- Search for feature importances","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Libraries","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \n\nimport pandas as pd \n\nimport os\n\nimport seaborn as sns\n\nfrom scipy.stats import chi2_contingency\nfrom scipy.stats import chisquare\nimport scipy.stats as ss\nfrom scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform\n\nimport matplotlib.pyplot as plt\nplt.style.use('tableau-colorblind10')\ncolor_pal = [x['color'] for x in plt.rcParams['axes.prop_cycle']]\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.preprocessing import minmax_scale\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score,roc_auc_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import preprocessing \nfrom sklearn.model_selection import GridSearchCV,RandomizedSearchCV\nfrom sklearn.model_selection import KFold\n\nimport datetime\n\nfrom lightgbm import LGBMClassifier\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Reference\n  - https://www.kaggle.com/kabure/extensive-eda-and-modeling-xgb-hyperopt\n  - https://mlfromscratch.com/gridsearch-keras-sklearn/#/\n  - https://stackoverflow.com/questions/53413701/feature-importance-using-lightgbm\n  - https://towardsdatascience.com/the-search-for-categorical-correlation-a1cf7f1888c9\n  - https://chrisalbon.com/machine_learning/feature_selection/drop_highly_correlated_features/\n  ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Read Data and Data Memory Reduction","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## Function to reduce the DF size\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_transaction = pd.read_csv(\"../input/ieee-fraud-detection/train_transaction.csv\")\n\n# Check for Missing  data\ntotal = df_transaction.isnull().sum()\nprint(\"NaN values in Transaction database\",total)\n\n# Unique Values\nUniqueID = df_transaction['TransactionID'].nunique()\nprint(\"No of unique Transaction ID's\",UniqueID)\n\n# Columns in dataframe\nprint(\"Columns in dataframe\",df_transaction.columns)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Columns datatypes\nstring_columns = df_transaction.select_dtypes('object').columns.tolist()\nprint(\"Object Columns\",string_columns)\n\nnumeric_columns = df_transaction.select_dtypes(include=np.number).columns.tolist()\nprint(\"Numerical columns \",numeric_columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Memory Reduction\ndf_transaction = reduce_mem_usage(df_transaction, verbose=True)\nprint(df_transaction.head(10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis of Transaction Database","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Categorical Columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for Missing Data\n\ndf_transaction = df_transaction.replace(np.nan, 'NaN', regex=True)\n\nlst = string_columns\ndata = df_transaction[df_transaction.columns.intersection(lst)]\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for M variables since they have a high number of NaN vales\nM = data.filter(regex='^M',axis=1)\nM.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = M.columns\n\n#df_trans = df_transaction.copy()\n\nfor col in columns:   \n    \n        \n    df_trans = df_transaction[df_transaction.isFraud==1][col]  \n    \n    \n    plt.figure(figsize=(14,10))\n    \n    g1 = sns.countplot(x=df_trans)\n    \n    g1.set_title(\"Frequeny of \" + str(col)+ \" by Fraud Values\", fontsize=19)\n \n    g1.set_xticklabels(g1.get_xticklabels(),rotation=45)\n    plt.show()\n  \n    del g1\n    \n    \n    \n    \ndel df_trans \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- High Frequency of Fraud values among 'M' variables which have missing values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Columns ProductCD, card4, card6\ncolumns = ['ProductCD', 'card4', 'card6']\n\nfor col in columns:\n    \n    df_trans = df_transaction[df_transaction.isFraud==1][col]  \n    \n    \n    plt.figure(figsize=(14,10))\n    \n    g1 = sns.countplot(x=df_trans)\n    \n    g1.set_title(\"Frequeny of \" + str(col)+ \" by Fraud Values\", fontsize=19)\n \n    g1.set_xticklabels(g1.get_xticklabels(),rotation=45)\n    plt.show()\n  \n    del g1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Interesting Observations\n  - High Frequency of Fraud in W in ProductCD\n  - High Frequency of Fraud in visa\n  - High Frequency of Fraud among debit cards","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Columns other than M\ncolumns = ['P_emaildomain', 'R_emaildomain']\n\n\nfor col in columns:\n    \n    df_trans = df_transaction[df_transaction.isFraud==1][col]  \n    \n    \n    plt.figure(figsize=(14,10))\n    \n    g1 = sns.countplot(x=df_trans)\n    \n    g1.set_title(\"Frequeny of \" + str(col)+ \" by Fraud Values\", fontsize=19)\n \n    g1.set_xticklabels(g1.get_xticklabels(),rotation=45)\n    plt.show()\n  \n    del g1\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Among P email domain, gmail.com has a high fraud frequency\n- Among R email domain, gmail.com has a high fraud frequency","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cramers V for categorical correlations\ndef cramers_v(x, y):\n    x = np.array(x)\n    y = np.array(y)\n    confusion_matrix = pd.crosstab(x, y)\n    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n    n = confusion_matrix.sum().sum()\n    phi2 = chi2/n\n    r,k = confusion_matrix.shape\n    phi2corr = max(0, phi2-((k-1)*(r-1))/(n-1))\n    rcorr = r-((r-1)**2)/(n-1)\n    kcorr = k-((k-1)**2)/(n-1)\n    return np.sqrt(phi2corr/min((kcorr-1),(rcorr-1)))\n\n\ncramersv = pd.DataFrame(index=data.columns,columns=data.columns)\ncolumns = data.columns\n\nfor i in range(0,len(columns)):\n    for j in range(0,len(columns)):\n        #print(data[columns[i]].tolist())\n        u = cramers_v(data[columns[i]].tolist(),data[columns[j]].tolist())\n        cramersv.loc[columns[i],columns[j]] = u\n        \ncramersv.fillna(value=np.nan,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,15))\nsns.heatmap(cramersv, annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reduce Memory\ndel cramersv,data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Columns M2 and M3 have a very strong correlation of 0.85.\n- Also from the line plots M2 and M3 share a similar pattern of fraud correlation\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Non Categorical Columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_transaction.drop(columns=M.columns,inplace=True)\nprint(df_transaction.head(10))\ndel M","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_transaction.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Too many V variables. We can do a correlation analysis and remove columns that are highly correlated\nV = df_transaction.filter(regex='^V',axis=1)\n\nprint(len(V))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Drop V columns which have high correlation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create correlation matrix\n\nV = V.applymap(float)\ncorr_matrix = V.corr().abs()\n\n\n# Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n# Find index of feature columns with correlation greater than 0.95\nto_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n\nprint(to_drop)\nHighCorrVDrop = to_drop\n\ndel corr_matrix, upper","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's look at other columns\ndf_transaction.drop(columns = V.columns,inplace=True)\ndel V","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's look at transaction amt\n\ndf_transaction['TransactionAmt'] = df_transaction['TransactionAmt'].apply(pd.to_numeric)\nf, ax = plt.subplots(1, 1,figsize=(15,15))\n\nsns.distplot(ax= ax,a = df_transaction[df_transaction['isFraud']==0]['TransactionAmt'], color=\"skyblue\", label=\"is Not Fraud\")\n\n\nsns.distplot(ax=ax,a = df_transaction[df_transaction['isFraud']==1]['TransactionAmt'], color=\"red\", label=\"is Fraud\")\nplt.legend(labels=['is Not Fraud', 'is Fraud'])\nax.set_xlabel(\"Transaction Amt\")\n\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Applying log transformation on data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data heavily skewed. Apply log of the data\n\nf, ax = plt.subplots(1, 1,figsize=(15,15))\n\nsns.distplot(ax= ax,a = np.log(df_transaction[df_transaction['isFraud']==0]['TransactionAmt']), color=\"skyblue\", label=\"is Not Fraud\")\n\n\nsns.distplot(ax=ax,a = np.log(df_transaction[df_transaction['isFraud']==1]['TransactionAmt']), color=\"red\", label=\"is Fraud\")\nplt.legend(labels=['is Not Fraud', 'is Fraud'])\nax.set_xlabel(\"Log of Transaction Amt\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- It would be better if we scale the values of the transaction amount while training","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"- C subset columns\n- Due to high presence of many features. Let's pick only the top features by frquency for plotting","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find which of the columns have the highest frquency among fake transactions\n\ndf_trans = df_transaction[df_transaction['isFraud']==1]\ndf_tran = df_transaction.copy()\nC = df_transaction.filter(regex='^C',axis=1)\n\n\n\nfor col in C.columns:\n   \n    index_list = df_trans[col].value_counts(ascending=False)[:15].index.to_list()\n    df_tran.loc[df_tran[(~df_tran[col].isin(index_list))].index, col] = \"Others\"\n    \n    \ndel df_trans","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = C.columns\n\n\nfor col in columns:\n    \n    df_trans = df_tran[df_tran.isFraud==1][col]  \n    \n    \n    plt.figure(figsize=(14,10))\n    \n    g1 = sns.countplot(x=df_trans)\n    \n    g1.set_title(\"Frequeny of \" + str(col)+ \" by Fraud Values\", fontsize=19)\n \n    g1.set_xticklabels(g1.get_xticklabels(),rotation=45)\n    plt.show()\n  \n    del g1\n    \ndel C","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Some interesting Features from the graphs regarding the fraud for various features. Now let's look at D i.e days between transactions\n  ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find which of the columns have the highest frquency among fake transactions\n# Due to high presence of many features. Let's pick only the top features by frequency for plotting\n\ndf_trans = df_transaction[df_transaction['isFraud']==1]\ndf_tran = df_transaction.copy()\nD = df_transaction.filter(regex='^D',axis=1)\n\n\n\nfor col in D.columns:\n   \n    index_list = df_trans[col].value_counts(ascending=False)[:15].index.to_list()\n    df_tran.loc[df_tran[(~df_tran[col].isin(index_list))].index, col] = \"Others\"\n    \n    \ndel df_trans","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = D.columns\n\n\nfor col in columns:\n    \n    df_trans = df_tran[df_tran.isFraud==1][col]  \n    \n    \n    plt.figure(figsize=(14,10))\n    \n    g1 = sns.countplot(x=df_trans)\n    \n    g1.set_title(\"Frequeny of \" + str(col)+ \" by Fraud Values\", fontsize=19)\n \n    g1.set_xticklabels(g1.get_xticklabels(),rotation=45)\n    plt.show()\n  \n    del g1\n    \ndel D\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Not sure of the meaning between the different TimeDelta's, but there seems to be a high frequency for NaN values","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"- Distance and address columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find which of the columns have the highest frquency among fake transactions\n# Due to high presence of many features. Let's pick only the top features by frquency for plotting\n\ndf_trans = df_transaction[df_transaction['isFraud']==1]\ndf_tran = df_transaction.copy()\nD = df_transaction[['addr1','addr2','dist1','dist2']]\n\n\n\nfor col in D.columns:\n   \n    index_list = df_trans[col].value_counts(ascending=False)[:15].index.to_list()\n    df_tran.loc[df_tran[(~df_tran[col].isin(index_list))].index, col] = \"Others\"\n    \n    \ndel df_trans","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = D.columns\n\nfor col in columns:\n    \n    df_trans = df_tran[df_tran.isFraud==1][col]  \n    \n    \n    plt.figure(figsize=(14,10))\n    \n    g1 = sns.countplot(x=df_trans)\n    \n    g1.set_title(\"Frequeny of \" + str(col)+ \" by Fraud Values\", fontsize=19)\n \n    g1.set_xticklabels(g1.get_xticklabels(),rotation=45)\n    plt.show()\n  \n    del g1\n    \ndel D","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Missing values in addr1, addr2, dist1,dist2 have a higher fraud rate as well as in dist 1\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"- For cards 1 - 6 except 4  and 6","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find which of the columns have the highest frquency among fake transactions\n# Due to high presence of many features. Let's pick only the top features by frquency for plotting\n\ndf_trans = df_transaction[df_transaction['isFraud']==1]\ndf_tran = df_transaction.copy()\nD = df_transaction[['card1','card2','card3','card5']]\n\n\n\nfor col in D.columns:\n   \n    index_list = df_trans[col].value_counts(ascending=False)[:15].index.to_list()\n    df_tran.loc[df_tran[(~df_tran[col].isin(index_list))].index, col] = \"Others\"\n    \n    \ndel df_trans","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = D.columns\n\nfor col in columns:\n    \n    df_trans = df_tran[df_tran.isFraud==1][col]  \n    \n    \n    plt.figure(figsize=(14,10))\n    \n    g1 = sns.countplot(x=df_trans)\n    \n    g1.set_title(\"Frequeny of \" + str(col)+ \" by Fraud Values\", fontsize=19)\n \n    g1.set_xticklabels(g1.get_xticklabels(),rotation=45)\n    plt.show()\n  \n    del g1\n    \ndel D","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- High number of low frequency values in card1, card2\n- 150 and 185 have a high frequency for card 3 \n- 226 have a high frequency for card 3","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"- TransactionDT\n  - Break datetime into weekdays, hours and days\n  - https://www.kaggle.com/c/ieee-fraud-detection/discussion/100400#latest-579480","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"START_DATE = '2017-12-01'\nstartdate = datetime.datetime.strptime(START_DATE, \"%Y-%m-%d\")\ndf_transaction[\"date\"] = df_transaction['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds=x)))\n\ndf_transaction['Weekdays'] = df_transaction['date'].dt.dayofweek\ndf_transaction['Hours'] = df_transaction['date'].dt.hour\ndf_transaction['Days'] = df_transaction['date'].dt.day\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = ['Weekdays','Hours','Days']\n\nfor col in columns:\n    \n    df_trans = df_transaction[df_transaction.isFraud==1][col]  \n    \n    \n    plt.figure(figsize=(14,10))\n    \n    g1 = sns.countplot(x=df_trans)\n    \n    g1.set_title(\"Frequeny of \" + str(col)+ \" by Fraud Values\", fontsize=19)\n \n    g1.set_xticklabels(g1.get_xticklabels(),rotation=45)\n    plt.show()\n  \n    del g1\n    \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Fraud rate has a count on 5th working day and in the evening","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"truncated_df = df_transaction[['TransactionID','isFraud']]\ndel df_transaction","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis of Identity Database\n- Here I will be performing EDA only on select columns with understandable data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_identity = pd.read_csv(\"../input/ieee-fraud-detection/train_identity.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Name of columns and null values among them\n\nprint(df_identity.columns)\n\nprint(\"NaN values : \",df_identity.isna().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's look at the data in the ID subset folder\n\nID = df_identity.filter(regex='^id',axis=1)\nID.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(ID.iloc[:,0:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(ID.iloc[:,10:20])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(ID.iloc[:,20:30])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(ID.iloc[:,30:40])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merge the targets and identity database for plotting purpose\n\ndf_identity = pd.merge(truncated_df,df_identity, how='left',on='TransactionID')\ndf_identity.reset_index(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = ['id_34','id_35','id_36','id_37','id_38']\n\nfor col in columns:\n    \n    df_iden = df_identity[df_identity.isFraud==1][col]  \n    \n    \n    plt.figure(figsize=(14,10))\n    \n    g1 = sns.countplot(x=df_iden)\n    \n    g1.set_title(\"Frequeny of \" + str(col)+ \" by Fraud Values\", fontsize=19)\n \n    g1.set_xticklabels(g1.get_xticklabels(),rotation=45)\n    plt.show()\n  \n    del g1\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = ['id_12','id_15','id_16','id_28','id_29']\n\nfor col in columns:\n    \n    df_iden = df_identity[df_identity.isFraud==1][col]  \n    \n    \n    plt.figure(figsize=(14,10))\n    \n    g1 = sns.countplot(x=df_iden)\n    \n    g1.set_title(\"Frequeny of \" + str(col)+ \" by Fraud Values\", fontsize=19)\n \n    g1.set_xticklabels(g1.get_xticklabels(),rotation=45)\n    plt.show()\n  \n    del g1\n    \n \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ID columns with high number of NaN values\n\ncolumns =['id_07', 'id_08', 'id_21', 'id_22', 'id_23', 'id_24', 'id_25', 'id_26', 'id_27']\n\n\nfor col in columns:\n    \n    df_iden = df_identity[df_identity.isFraud==1][col]  \n    \n    \n    plt.figure(figsize=(14,10))\n    \n    g1 = sns.countplot(x=df_iden)\n    \n    g1.set_title(\"Frequeny of \" + str(col)+ \" by Fraud Values\", fontsize=19)\n \n    g1.set_xticklabels(g1.get_xticklabels(),rotation=45)\n    plt.show()\n  \n    del g1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Interesting result in id_23 and id_27\n- id_23 has dist of IP address. There are high frequency values in anonymous (not higher than transparent)\n- id_27 shows 'Found' throughout\n- It would be valid to keep the High NaN columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Further analysis of id_23\n\n  \nplt.figure(figsize=(14,10))\n    \ng1 = sns.countplot(x='id_23',hue='isFraud',data=df_identity)\n    \ng1.set_title(\"Frequeny of \" + \"id_23\" + \" by Fraud Values\", fontsize=19)\n \ng1.set_xticklabels(g1.get_xticklabels(),rotation=45)\nplt.show()\n\ndel g1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Higher tedency for anonymous IP ratio wise","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find which of the columns have the highest frquency among fake transactions\n# Due to high presence of many features. Let's pick only the top features by frquency for plotting\n\ndf_ident =df_identity[df_identity['isFraud']==1]\ndf_iden = df_identity.copy()\nD = df_identity[['id_30','id_31','id_33','DeviceInfo']]\n\n\n\nfor col in D.columns:\n   \n    index_list = df_ident[col].value_counts(ascending=False)[:15].index.to_list()\n    df_iden.loc[df_iden[(~df_iden[col].isin(index_list))].index, col] = \"Others\"\n    \n    \ndel df_ident","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = D.columns\n\n\nfor col in columns:\n    \n    df_ident = df_iden[df_iden.isFraud==1][col]  \n    \n    \n    plt.figure(figsize=(14,10))\n    \n    g1 = sns.countplot(x=df_ident)\n    \n    g1.set_title(\"Frequeny of \" + str(col)+ \" by Fraud Values\", fontsize=19)\n \n    g1.set_xticklabels(g1.get_xticklabels(),rotation=45)\n    plt.show()\n  \n    del g1\n    \ndel df_iden\n\n  \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- High amount of Fraud among low frequency values\n- It would be advisable to take a higher size for prediction\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Applying information we have learnt and implementing LightGBM on the training set first","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_trans = pd.read_csv(\"../input/ieee-fraud-detection/train_transaction.csv\")\ndf_identity = pd.read_csv(\"../input/ieee-fraud-detection/train_identity.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data Merging\ndf = pd.merge(df_trans,df_identity, how='left',on='TransactionID')\ndf.reset_index(inplace=True)\ndel df_trans,df_identity","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(columns=['index'],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We then pick the values in the corresponding columns with high frequency from Fraudelent and non Fraudenlent case.\n- This ensures that the prediction works well for the most frequent values in both cases","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def pick_high_freq_only(df,df2 = pd.DataFrame(),second=False):\n    \n    # Find which of the columns have the highest frquency among fake transactions\n    \n    # Device Info\n    d = df[df['isFraud']==1]\n    dnf = df[df['isFraud']==0]\n    columns  = ['DeviceInfo']\n\n\n    for col in columns:\n   \n        index_list1 = d[col].value_counts(ascending=False)[:100].index.to_list()\n        index_list2 = dnf[col].value_counts(ascending=False)[:100].index.to_list()\n    \n        index_list = index_list1 + index_list2\n        df.loc[df[(~df[col].isin(index_list))].index, col] = \"Others\"\n    \n        if second == True:\n            df2.loc[df2[(~df2[col].isin(index_list))].index, col] = \"Others\"\n    \n    \n    del d,dnf\n    \n    # Email Domains\n    d = df[df['isFraud']==1]\n    dnf = df[df['isFraud']==0]\n    columns  = ['P_emaildomain', 'R_emaildomain']\n\n\n    for col in columns:\n   \n        index_list1 = d[col].value_counts(ascending=False)[:20].index.to_list()\n        index_list2 = dnf[col].value_counts(ascending=False)[:20].index.to_list()\n    \n        index_list = index_list1 + index_list2\n        df.loc[df[(~df[col].isin(index_list))].index, col] = \"Others\"\n        \n        if second == True:\n            df2.loc[df2[(~df2[col].isin(index_list))].index, col] = \"Others\"\n            \n    \n    \n    \n    del d,dnf\n    \n    # Selected ID's\n    d = df[df['isFraud']==1]\n    dnf = df[df['isFraud']==0]\n    columns = ['id_30','id_31','id_33']\n\n    for col in columns:\n   \n        index_list1 = d[col].value_counts(ascending=False)[:100].index.to_list()\n        index_list2 = dnf[col].value_counts(ascending=False)[:100].index.to_list()\n    \n        index_list = index_list1 + index_list2\n        df.loc[df[(~df[col].isin(index_list))].index, col] = \"Others\"\n        \n        if second == True:\n            df2.loc[df2[(~df2[col].isin(index_list))].index, col] = \"Others\"\n    \n    \n    del d,dnf\n    \n    return df,df2\n\n\n\n\ndf,_ = pick_high_freq_only(df)\n    \n#print(df)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Seggregate into features and target\n\nFeatures = df\nFeatures = Features.drop(columns=['isFraud','TransactionID'])\nTarget = df['isFraud'].astype(float)\nTarget = Target.values\nTarget = Target.reshape((len(Target), 1))\n\ndel df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Feature Engineering with TransactionDT","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def FETransactionDT(Features):\n    \n    #print(Features)\n    START_DATE = '2017-12-01'\n    startdate = datetime.datetime.strptime(START_DATE, \"%Y-%m-%d\")\n    Features[\"date\"] = Features['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds=x)))\n\n    Features['Weekdays'] = Features['date'].dt.dayofweek\n    Features['Hours'] = Features['date'].dt.hour\n    Features['Days'] = Features['date'].dt.day\n\n    Features['Weekdays'] = pd.to_numeric(Features['Weekdays'])\n    Features['Hours'] = pd.to_numeric(Features['Hours'])\n    Features['Days'] = pd.to_numeric(Features['Days'])\n\n    Features.drop(columns=[\"date\"],inplace=True)\n    \n    return Features\n    \nFeatures = FETransactionDT(Features)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Drop columns with high number of NaN values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def HighNaNcol(Features):\n    \n    HighNaN = []\n    for col in Features.columns:\n    \n        no = Features[col].isna().sum()\n    \n        if no > 0.95 * len(Features):\n            HighNaN.append(col)\n            \n    print(HighNaN)\n    return HighNaN\n\n#HighNaN = HighNaNcol(Features)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Features to drop \ncolumns_to_drop = HighCorrVDrop\n\n\n#Features.drop(columns=HighNaN,inplace=True)\nFeatures.drop(columns = columns_to_drop,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_size = 0.2\nX_train, X_test, y_train, y_test = train_test_split(Features, Target, test_size=test_size,random_state=32)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"-  Scale the Categorical and Numerical columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def ScaleColumns(X_train,X_test):\n    for col in X_train.columns:\n        if X_train[col].dtype=='object' or X_test[col].dtype=='object': \n            print(col)\n            lbl = preprocessing.LabelEncoder()\n        \n            X_train[col] = X_train[col].astype(str)\n            X_test[col] = X_test[col].astype(str)\n        \n            X_train[col] = lbl.fit_transform(X_train[col])\n            X_test[col] = lbl.transform(X_test[col])  \n        \n        if col == 'TransactionAmt':\n            \n            print(col)\n            SS = preprocessing.StandardScaler()\n        \n            X_train_arr = X_train[col].astype(float).values\n            X_test_arr = X_test[col].astype(float).values   \n        \n            \n            X_train_arr = SS.fit_transform(X_train_arr.reshape(-1,1))\n            X_test_arr = SS.transform(X_test_arr.reshape(-1,1))\n            \n            X_train[col]  = X_train_arr \n            X_test[col]   = X_test_arr\n            \n    return X_train, X_test\n            \nX_train, X_test = ScaleColumns(X_train,X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- RandomSearchCV Search for LightGBM","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#fit_params={\"early_stopping_rounds\":50, \n           # \"eval_metric\" : 'auc', \n           #\"eval_set\" : [(X_test,y_test)],\n           #'eval_names': ['valid'],\n           # 'verbose': 100,\n           #'categorical_feature': 'auto'}\n\n#param_test ={  'n_estimators': [400, 700, 1000],\n  #'colsample_bytree': [0.7, 0.8],\n  # 'max_depth': [15,20,25],\n   #'num_leaves': [50, 100, 200],\n  # 'reg_alpha': [1.1, 1.2, 1.3],\n # 'reg_lambda': [1.1, 1.2, 1.3],\n # 'min_split_gain': [0.3, 0.4],\n #'subsample': [0.7, 0.8, 0.9],\n  # 'subsample_freq': [20]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#clf = LGBMClassifier(random_state=314, silent=True, metric='None', n_jobs=2)\n#model = RandomizedSearchCV(\n  # estimator=clf, param_distributions=param_test, \n   # scoring='roc_auc',\n   # cv=3,\n    #refit=True,\n   # random_state=314,\n   # verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model.fit(X_train, y_train, **fit_params)\n#print('Best score reached: {} with params: {} '.format(model.best_score_, model.best_params_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Model fitting using LGBM classifier\n- The hyperparameters are found using a RandomizedSearchCV (commented above)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmodel = LGBMClassifier(subsample_freq= 20,subsample= 0.7, reg_lambda= 1.2, reg_alpha = 1.1, \n                       num_leaves = 200, n_estimators = 700, \n                       min_split_gain = 0.3, max_depth = 25, colsample_bytre = 0.8,random_state=314,n_jobs=2)\n\nmodel.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make predictions for test data\ny_pred = model.predict_proba(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# evaluate predictions\nscore = roc_auc_score(y_test, y_pred[:,1])\nprint(\"ROC_AUC score: %.2f%%\" % (score * 100.0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Implement model on test set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_trans = pd.read_csv(\"../input/ieee-fraud-detection/train_transaction.csv\")\ndf_train_identity = pd.read_csv(\"../input/ieee-fraud-detection/train_identity.csv\")\n\n# Data Merging\ndf_train = pd.merge(df_train_trans,df_train_identity , how='left',on='TransactionID')\ndf_train.reset_index(inplace=True)\ndel df_train_trans,df_train_identity","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test_trans = pd.read_csv(\"../input/ieee-fraud-detection/test_transaction.csv\")\ndf_test_identity = pd.read_csv(\"../input/ieee-fraud-detection/test_identity.csv\")\n\n# Data Merging\ndf_test = pd.merge(df_test_trans,df_test_identity , how='left',on='TransactionID')\ndf_test.reset_index(inplace=True)\ndel df_test_trans,df_test_identity","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.drop(columns=['index'],inplace=True)\ndf_test.drop(columns=['index'],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Feature Engineering\n   - Replace values with low value count with 'Others'\n   - Create columns splitting datetime into components\n   - Drop highly correlatted V columns\n   - Drop columns with high NaN values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.columns = df_test.columns.str.replace(\"-\", \"_\")\nprint(df_test.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train,df_test = pick_high_freq_only(df_train,df_test,second=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Seggregate into features and target\n\nFeatures_train = df_train\nFeatures_train = Features_train.drop(columns=['isFraud'])\nTarget_train = df_train['isFraud']\nTarget_train = Target_train.values\nTarget_train = Target_train.reshape((len(Target_train), 1))\n\ndel df_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Features_test = df_test\n\ndel df_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Features_train = FETransactionDT(Features_train)\nFeatures_test = FETransactionDT(Features_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"HighNaN = HighNaNcol(Features_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Features to drop \ncolumns_to_drop = HighCorrVDrop\n\n#Features_train.drop(columns=HighNaN,inplace=True)\nFeatures_train.drop(columns = columns_to_drop,inplace=True)\n\n#Features_test.drop(columns=HighNaN,inplace=True)\nFeatures_test.drop(columns = columns_to_drop,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Features_train, Features_test = ScaleColumns(Features_train, Features_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Features_train = Features_train.drop(columns=['TransactionID'])\nFeatures_test_without_ID = Features_test.drop(columns=['TransactionID'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmodel = LGBMClassifier(subsample_freq= 20,subsample= 0.7, reg_lambda= 1.2, reg_alpha = 1.1, \n                       num_leaves = 200, n_estimators = 700, \n                       min_split_gain = 0.3, max_depth = 25, colsample_bytre = 0.8,random_state=314,n_jobs=2)\n\nmodel.fit(Features_train, Target_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make predictions for test data\ny_pred = model.predict_proba(Features_test_without_ID)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame()\nsubmission['TransactionID'] = Features_test['TransactionID']\nsubmission['isFraud'] = y_pred[:,1]\n\nprint(submission)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Importance Chart","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def plotImp(model, X , num = 20):\n    feature_imp = pd.DataFrame({'Value':model.feature_importances_,'Feature':X.columns})\n    plt.figure(figsize=(40, 20))\n    sns.set(font_scale = 5)\n    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", \n                                                        ascending=False)[0:num])\n    plt.title('LightGBM Feature imprtances')\n    plt.tight_layout()\n    \n    plt.show()\n    \nplotImp(model, Features_train , num =20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Summary\n- Card1,Card2 TransactionDT and TransactionAmt seem to hold a major importance in the model\n- Still scope for Feature Engineering by adjusting the value counts of the categorical variables we would like to hold","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"- The notebook helped me learn and practice Feature Engineering and Visualization.\n- Let me know if there is any further scope of improvement!","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}