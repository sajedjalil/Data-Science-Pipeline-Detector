{"cells":[{"metadata":{"trusted":false},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport datetime\nimport gc\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn import preprocessing\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import minmax_scale\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom hyperopt import hp, tpe\nfrom hyperopt.fmin import fmin","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#!pip install xgboost","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Preprocessing"},{"metadata":{"trusted":false},"cell_type":"code","source":"# read in files\nraw_train_identity  = pd.read_csv(\"../input/ieee-fraud-detection/train_identity.csv\")\nraw_train_transaction = pd.read_csv(\"../input/ieee-fraud-detection/train_transaction.csv\")\nraw_test_identity  = pd.read_csv(\"../input/ieee-fraud-detection/test_identity.csv\")\nraw_test_transaction = pd.read_csv(\"../input/ieee-fraud-detection/test_transaction.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# reduce file memory of train_identity, train_transaction, test_identity, and test_transaction \n# credit to @Tharindu Gangoda: https://www.kaggle.com/tharug/ieee-fraud-detection\ndef memory_reduction(df):\n    # check original dataframe usage\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    # reduce all numeric columns to a smaller data type\n    for col in df.columns:\n        col_type = df[col].dtype.name\n        if col_type not in ('object', 'category'):\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n        else:\n            pass\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# reduce original data memory\nraw_test_identity = memory_reduction(raw_test_identity)\nraw_test_transaction = memory_reduction(raw_test_transaction)\nraw_train_transaction = memory_reduction(raw_train_transaction)\nraw_train_identity = memory_reduction(raw_train_identity)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"<h3><center>Transaction Table</center></h3> \n<center>reference from https://www.kaggle.com/c/ieee-fraud-detection/discussion/101203</center>\n<center>It contains money transfer and also other gifting goods and service, like you booked a ticket for others, etc</center>\n\n- <em>TransactionDT</em>: timedelta from a given reference datetime (not an actual timestamp). TransactionDT first value is 86400, which corresponds to the number of seconds in a day (60 * 60 * 24 = 86400) so I think the unit is seconds. Using this, we know the data spans 6 months, as the maximum value is 15811131, which would correspond to day 183\n\n- <em>TransactionAMT</em>:  transaction payment amount in USD\n\n- <em>ProductCD (Categorical)</em>: product code, the product for each transaction \n\n- <em>card1 - card6 (Categorical)</em>: payment card information, such as card type, card category, issue bank, country, etc.\n\n- <em>addr (Categorical)</em>: purchaser address\naddr1 as billing region\naddr2 as billing country\n\n- <em>dist</em>: distances between (not limited) billing address, mailing address, zip code, IP address, phone area, etc.”\n\n- <em>P_ and R_emaildomain (Categorical)</em>: purchaser and recipient email domain. Certain transactions don't need recipient, so R_emaildomain is null\n\n- <em>C1-C14 </em>: counting, such as how many addresses are found to be associated with the payment card, etc. The actual meaning is masked\n\n- <em>D1-D15</em>: timedelta, such as days between previous transaction, etc\n\n- <em>M1-M9 (Categorical)</em>: match, such as names on card and address, etc\n\n- <em>Vxxx</em>: Vesta engineered rich features, including ranking, counting, and other entity relations\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"# preview raw_train_transaction \nraw_train_transaction.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# create a bar graph to visualize the distribution of fraud vs. non fraud transactions\nplt.figure(figsize=(12,6))\nplt.title('Fraud Transaction Distribution')\nax=sns.countplot(x='isFraud', data =raw_train_transaction)\nplt.ylabel(\"Transaction Count\")\nfor p in ax.patches:\n             ax.annotate(str(round(p.get_height()/len(raw_train_transaction)*100,2))+\"%\", (p.get_x() + p.get_width() / 2., p.get_height()),\n                 ha='center', va='center', fontsize=11, color='black', xytext=(0, 5),\n                 textcoords='offset points')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# we examine whether the continous variable TransactionAMT has normal distribution or not by plotting a histogram \nplt.figure(figsize=(12,6))\nsns.distplot(raw_train_transaction['TransactionAmt'])\nplt.title(\"TransactionAmt Distribution \")\nplt.ylabel(\"Probability Density\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(12,6))\nsns.distplot(raw_train_transaction['TransactionAmt'].apply(np.log))\nplt.title(\"Log Transformation of TransactionAmt Distribution \")\nplt.ylabel(\"Probability Density\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# defind a function to visualize the percentage of fraud transactions in each categorical variables \ndef find_percentage(df,variable_name,fig_size,rotate):\n    # create empty list to sore percentage \n    find_percent=[]\n    # go through all the columns \n    for i in variable_name:\n        # create a dictionary\n        percent=dict()\n        # fill na with 'NA'\n        df[i] = df[i].fillna(\"NA\")\n        # go through each category\n        for j in sorted(df[i].unique()):\n            # fraud number\n            fraud_num = len(df[(df[i]==j) & (df['isFraud']==1)])\n            # not fraud number\n            total_num = len(df[df[i]==j])\n            # round to 2 decimal\n            temp = dict([(j,round((fraud_num/total_num*100),2))])\n            # add new key &value\n            percent.update(temp)\n        plt.figure(figsize=fig_size)\n        plt.title(\"Number of Transactions for \" + str(i)) \n        ax=sns.countplot(x=i,data=df,order=sorted(df[i].unique()))\n        plt.ylabel(\"Transaction Count\")\n        # add value label\n        for p in ax.patches:\n            ax.annotate(str(round(p.get_height()/len(raw_train_transaction)*100,2))+\"%\", (p.get_x() + p.get_width() / 2., p.get_height()),\n            ha='center', va='center', fontsize=11, color='black', xytext=(0, 5), textcoords='offset points')\n        plt.xticks(rotation=rotate)\n        plt.figure(figsize=fig_size)\n        plt.title(\"Fraud Transaction Percentage for \" + str(i))\n        ax=sns.barplot(x=list(percent.keys()),\n            y=list(percent.values()))        \n        plt.xticks(range(len(percent)), list(percent.keys()))\n        plt.xticks(rotation=rotate)\n        plt.xlabel(i)\n        plt.ylabel(\"Fraud Transaction Percentage of Each Category\")\n        # add value label\n        for p in ax.patches:\n            ax.annotate(str(p.get_height())+'%', (p.get_x() + p.get_width() / 2., p.get_height()),\n            ha='center', va='center', fontsize=11, color='black', xytext=(0, 5),\n            textcoords='offset points')\n        find_percent.append(percent)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# find out the number of transactions for each category of ProductCD variable\n# find out the fraud transaction percentage of total transactions for each category of ProductCD variable\nfig_size =(12,6)\nrotation=0\nfind_percentage(raw_train_transaction,['ProductCD'],fig_size,rotation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# check the distribution for continuous card1 variable \nplt.figure(figsize=(12,6))\nsns.distplot(raw_train_transaction[raw_train_transaction['isFraud']==0]['card1'])\nsns.distplot(raw_train_transaction[raw_train_transaction['isFraud']==1]['card1'])\nplt.legend(labels=['Not Fraud','Fraud'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# find out the number of transactions for each category of card4 variable\n# find out the fraud transaction percentage of total transactions for each category of card4 variable\nfind_percentage(raw_train_transaction,['card4'],fig_size,rotation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# find out the number of transactions for each category of card6 variable\n# find out the fraud transaction percentage of total transactions for each category of card6 variable\nfind_percentage(raw_train_transaction,['card6'],fig_size,rotation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# create list to store M1-M9 variable names\nM_cols = [\"M\"+str(i) for i in np.arange(1, 10, 1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# find out the number of transactions for each category of M1 to M9 variables\n# find out the fraud transaction percentage of total transactions for each category of M1 to M9 variables\nfind_percentage(raw_train_transaction,M_cols,fig_size,rotation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# visualize the distribution of each addr1 category\nplt.figure(figsize=(12,6))\nraw_train_transaction['addr1'].plot(kind='hist',bins=80)\nplt.xticks(np.arange(min(raw_train_transaction['addr1']), max(raw_train_transaction['addr1'])+1, 20))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# visualize the distribution of each addr2 category\nplt.figure(figsize=(12,6))\nraw_train_transaction['addr2'].plot(kind='hist',bins=80)\nplt.xticks(np.arange(min(raw_train_transaction['addr2']), max(raw_train_transaction['addr2'])+1, 5))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# In order to visualize more deailed insights of add1 variable, we select the top 20 transaction count add1 categories \ntop20_addr1 = raw_train_transaction[\"addr1\"].value_counts().head(20).index\ntop20_addr1 = raw_train_transaction[raw_train_transaction['addr1'].isin(top20_addr1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# find out the number of transactions for addr1 categories with the top 20 transaction count \n# find out the fraud transaction percentage of total transactions for addr1 categories with the top 20 transaction count\nfig_size=(22,6)\nfind_percentage(top20_addr1,[\"addr1\"],fig_size,rotation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# In order to visualize more deailed insights of add1 variable, we selecte the top 20 transaction count add2 categories \ntop20_addr2 = raw_train_transaction[\"addr2\"].value_counts().head(20)\ntop20_addr2 = raw_train_transaction[raw_train_transaction['addr2'].isin(top20_addr2)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# find out the number of transactions for addr2 categories with the top 20 transaction count \n# find out the fraud transaction percentage of total transactions for addr2 categories with the top 20 transaction count \nfind_percentage(top20_addr2,[\"addr2\"],fig_size,rotation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# select the P_emaildomain categories with the top 20 transaction count\ntop20_P_email = raw_train_transaction[\"P_emaildomain\"].value_counts().head(20).index\ntop20_P_email = raw_train_transaction[raw_train_transaction['P_emaildomain'].isin(top20_P_email)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# find out the number of transactions for P_emaildomain categories with the top 20 transaction count \n# find out the fraud transaction percentage of total transactions for P_emaildomain categories with the top 20 transaction count \nfig_size=(24,6)\nfind_percentage(top20_P_email,['P_emaildomain'],fig_size,rotation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# select the R_emaildomain categories with the top 20 transaction count\ntop20_R_email = raw_train_transaction[\"R_emaildomain\"].value_counts().head(20).index\ntop20_R_email = raw_train_transaction[raw_train_transaction['R_emaildomain'].isin(top20_R_email)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# find out the number of transactions for R_emaildomain categories with the top 20 transaction count \n# find out the fraud transaction percentage of total transactions for R_emaildomain categories with the top 20 transaction count \nfind_percentage(top20_R_email,['R_emaildomain'],fig_size,rotation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# create a function to convert TransactionDT variable into a time format\ndef convert_time (df):\n    START_DATE = '2017-01-01'\n    startdate = datetime.datetime.strptime(START_DATE, \"%Y-%m-%d\")\n    df[\"Date\"] = df['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds=x)))\n    df['Weekdays'] = df['Date'].dt.dayofweek\n    df['Hours'] = df['Date'].dt.hour\n    df['Days'] = df['Date'].dt.day\n    df['Month'] = df['Date'].dt.month\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# convert both train and test transaction's TransactionDT variable into a time format\nraw_train_transaction = convert_time(raw_train_transaction)\nraw_test_transaction =  convert_time(raw_test_transaction)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# plot bar graphs to visualize the number of transaction and fraud transaction percentage for each hour\nfig_size=(16,6)\nfind_percentage(raw_train_transaction,['Hours'],fig_size,rotation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# plot bar graphs to visualize the number of transaction and fraud transaction percentage for each week day\nfind_percentage(raw_train_transaction,['Weekdays'],fig_size,rotation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# plot bar graphs to visualize the number of transaction and fraud transaction percentage for each month\nfind_percentage(raw_train_transaction,['Month'],fig_size,rotation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# create a new dataframe to store the number of fraud for each day\nfraud_record=raw_train_transaction[raw_train_transaction['isFraud']==1]\ndaily_fraud=fraud_record.groupby(by=fraud_record['Date'].dt.date).size().reset_index()\ndaily_fraud.columns=['Date','Number of Fraud']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# create a line graph to visualize the changes in the number of fraud transaction for each day from 2017-01-01 to 2017-07-02\nplt.figure(figsize=(16,6))\nplt.title(\"Number of Fraud Transaction per Day\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Transaction Count\")\nplt.plot(daily_fraud['Date'],daily_fraud['Number of Fraud'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3><center>Identity Table</center></h3> \n<center>reference from https://www.kaggle.com/c/ieee-fraud-detection/discussion/101203</center>\n\nVariables in this table are identity information – network connection information (IP, ISP, Proxy, etc) and digital signature (UA/browser/os/version, etc) associated with transactions.\n\n- <em>id01 - id38</em>: features for identity, which is collected by Vesta and security partners such as device rating, ip_domain rating, proxy rating, etc. Also it recorded behavioral fingerprint like account login times/failed to login times, how long an account stayed on the page, etc. All of these are not able to elaborate due to security partner T&C\n- <em>id12 - id38 (Categorical)</em>\n \n- <em>DeviceType (Categorical)</em>: device type of the transaction occurred\n- <em>DeviceInfo (Categorical)</em>: More detailed device type of the transaction occurred  \n"},{"metadata":{"trusted":false},"cell_type":"code","source":"# preview of identify file\nraw_train_identity.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"merged_df = raw_train_transaction.merge(raw_train_identity,how='left', left_index=True, right_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# select the devices with top 10 transaction count \ntop10_Device_Type = merged_df[\"DeviceType\"].value_counts().index\ntop10_Device_Type  = merged_df[merged_df['DeviceType'].isin(top10_Device_Type)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# select more detailed devices with top 10 transaction count \ntop10_Device_Info = merged_df[\"DeviceInfo\"].value_counts().head(10).index\ntop10_Device_Info = merged_df[merged_df['DeviceInfo'].isin(top10_Device_Info)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# plot bar graphs to visualize the number of transaction and fraud transaction percentage for each device  \nfind_percentage(top10_Device_Type,['DeviceType'],fig_size,rotation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# plot bar graphs to visualize the number of transaction and fraud transaction percentage for the top 10 devices \nrotation=45\nfind_percentage(top10_Device_Info,['DeviceInfo'],fig_size,rotation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# plot bar graphs to visualize other categorical variables in the identity table\nrotation=0\nid_list= ['id_12','id_15','id_16','id_23','id_27','id_28','id_29','DeviceType']\nfind_percentage(merged_df,id_list,fig_size,rotation)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3><center>Key findings through exploratory data analysis:</center></h3>\n\n- 96.5% of transactions were not fraud and 3.5% of transactions were fraud transactions\n- The values in TransactionAmt variable were heavily skewed to the right, therefore apply log transformation on TransactionAMT column will give the values a normal distribution\n- In variable ProductCD category W had the most transactions, but category C had the highest chance to be a fraud transaction\n- Transactions with card1 value around 10000 were more likely to have fraud transactions, whereas card1 value around 8000, and 12500 were more likely to have non fraud transactions\n- 65.16% of the transactions were from visa credit card company, but discover credit card company was more likely to have a fraud transaction than other credit card types\n- 74.5% of the transactions were debit type, however credit transactions were more likely to have fraud transactions\n- The top 3 transactions were from 200, 300, and 320 addr1 variable\n- Most of the transactions were from 87 addr2 variable\n- Although there were only 1.4% of transactions were from 512 add1 variable, it had the highest chance to have a fraud transaction \n- Purchaser and receipt’s email domain from outlook had the highest chance to be a fraud transaction\n- Around 30% of fraud transactions occurred during 07:00 – 09:00\n- Most of the transactions occurred in early March (March break), and transactions in July had the highest chance to be fraud transactions\n- The most used device was on Windows but the device with the highest probabilty to have a fraud transaction was SM-G531H Build/LMY48B\n"},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering"},{"metadata":{"trusted":false},"cell_type":"code","source":"# store raw files into a new variable for modification\ntrain_transaction = raw_train_transaction\ntest_transaction = raw_test_transaction","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# log transformation on TransactionAMT variable \ntrain_transaction['TransactionAmt'] = np.log(raw_train_transaction['TransactionAmt'])\ntest_transaction['TransactionAmt'] = np.log(raw_test_transaction['TransactionAmt'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# merge transaction file with identity file \nmerged_train_df = train_transaction.merge(raw_train_identity,how='left', left_index=True, right_index=True)\nmerged_test_df = test_transaction.merge(raw_test_identity,how='left', left_index=True, right_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# create lists to store categorical variables\nv_features = [\"V\"+str(i) for i in np.arange(1, 340, 1)]\nC_cols = [\"C\"+str(i) for i in np.arange(1, 15, 1)]\ncard_cols = [\"card\"+str(i) for i in np.arange(1, 7, 1)]\nD_cols = [\"D\"+str(i) for i in np.arange(1, 16, 1)]\naddr_cols = [\"addr\"+str(i) for i in np.arange(1, 3, 1)]\nid_cols = [\"id_\"+str(i) for i in np.arange(12, 39, 1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# create a function to perform pca transformation to reduce the number of variables\ndef PCA_transform(df, cols,prefix, n_features):\n    pca = PCA(n_components = n_features, random_state=101)\n    pca_model = pca.fit_transform(df[cols])\n    pca_df = pd.DataFrame(pca_model)\n    df.drop(cols, axis=1, inplace=True)\n    pca_df.rename(columns=lambda x: str(prefix)+str(x), inplace=True)\n    df = pd.concat([df, pca_df], axis=1)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# since pca does not accept NA values, we will fill na with -1 \n# before pca transformation the data need to be scaled from 0 to 1 \ndef fill_na_features (df,features):\n    for col in features:\n        df[col] = df[col].fillna((df[col].min() - 1))\n        df[col] = (minmax_scale(df[col], feature_range=(0,1)))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# prepare pca transformation\nmerged_train_df = fill_na_features(merged_train_df,v_features)\nmerged_test_df = fill_na_features(merged_test_df,v_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# perform pca transformation which holds 95% of variance of v_features\nmerged_train_df = PCA_transform(merged_train_df, v_features, 'PCA_V',20)\nmerged_test_df = PCA_transform(merged_test_df, v_features, 'PCA_V',20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"cat_cols1 = [card_cols,addr_cols,M_cols,id_cols]\ncat_cols2 = ['ProductCD','P_emaildomain','R_emaildomain','DeviceType','DeviceInfo']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# create a function to convert the categorical variable's categories into numbers\ndef convert_cat_label1(df):\n    for i in range(len(cat_cols1)):\n        for col in cat_cols1[i]:\n            # avoid nan\n            if df[col].dtype=='object':\n                le = preprocessing.LabelEncoder()\n                le.fit(list(df[col].values) + list(df[col].values))\n                df[col] = le.transform(list(df[col].values))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# create a function to convert the categorical variable's categories into numbers\ndef convert_cat_label2(df):\n    for col in cat_cols2:\n        if col in df.columns:\n            le = preprocessing.LabelEncoder()\n            le.fit(list(df[col].values) + list(df[col].values))\n            df[col] = le.transform(list(df[col].values))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# convert categorical variables's categories into numbers \nmerged_train_df = convert_cat_label1(merged_train_df)\nmerged_train_df = convert_cat_label2(merged_train_df)\nmerged_test_df = convert_cat_label1(merged_test_df)\nmerged_test_df = convert_cat_label2(merged_test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# collect all the garbabge variables to reduce memory\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prediction Model"},{"metadata":{"trusted":false},"cell_type":"code","source":"# assign indedepnt variables to X, and depdent variable isFraud to y\nX= merged_train_df.drop(['TransactionID_x','TransactionDT','isFraud','Date'],axis=1)\ny=merged_train_df['isFraud']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\nX_submission = merged_test_df.drop(['TransactionID_x','TransactionDT','Date'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# use hypteropt to optimize the parameters of xgb classifier \n'''\ndef objective(params):\n    params = {\n        'max_depth': int(params['max_depth']),\n        'subsample':'{:.3f}'.format(params['subsample']),\n        'colsample_bytree': '{:.3f}'.format(params['colsample_bytree']),\n    }\n    \n    clf = xgb.XGBClassifier(\n        n_estimators=500,\n        learning_rate=0.05,\n        n_jobs=4,\n        random_state=101\n        **params\n    )\n\n    #score = cross_val_score(clf, X_test, y_test, scoring='roc_auc', cv=3)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n    clf.fit(X_train, y_train)\n    auc = roc_auc_score(y_test, clf.predict_proba(X_test)[:,1])\n\n    print(\"auc {:.3f} params {}\".format(auc, params))\n    return auc\n\nspace = {\n    'max_depth': hp.quniform('max_depth', 3, 8, 1),\n    'subsample': hp.uniform('subsample', 0.6, 0.9),\n    'colsample_bytree': hp.uniform('colsample_bytree', 0.6, 0.9),\n}\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"'''\n%%time\nbest = fmin(fn=objective,\n            space=space,\n            algo=tpe.suggest,\n            max_evals=5)\nprint(\"Hyperopt estimated optimum {}\".format(best))\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# use xgboost to classify whether each transaction is fraud or not\nimport xgboost as xgb\nclf = xgb.XGBClassifier( n_estimators=500,\n    max_depth=7,\n    learning_rate=0.05,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    random_state=101)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# fit the model with x label and y label and predict X_test\nclf.fit(X_train,y_train)\ny_preds = clf.predict_proba(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# accuracy on y_test \nauc = roc_auc_score(y_test, y_preds[:,1])\nprint('AUC: %.3f' % auc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# predict the probability of each transaction is fraud or not\ny_preds = clf.predict_proba(X_submission)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# merge prediction results with test transactions\nsample_submission = pd.read_csv(\"../input/ieee-fraud-detection/sample_submission.csv\")\nsample_submission['isFraud']=y_preds[:,1]\nsample_submission.head(10)\nsample_submission.to_csv('final_result.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# display the first 10 rows\nsample_submission.head(10)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}