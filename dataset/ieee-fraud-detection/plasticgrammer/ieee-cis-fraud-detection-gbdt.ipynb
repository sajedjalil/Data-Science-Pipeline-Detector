{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport gc,os,sys\nimport re\nimport random\n\nfrom sklearn import metrics, preprocessing\nfrom sklearn.model_selection import KFold, StratifiedKFold, GroupKFold, LeaveOneGroupOut\nfrom sklearn.decomposition import PCA, KernelPCA, NMF\nfrom sklearn.cluster import KMeans\nfrom tqdm import tqdm\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nsns.set_style('darkgrid')\n\npd.options.display.float_format = '{:,.3f}'.format\n\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed=0):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n\nseed_everything(42)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Memory saving function credit to https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.\n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        #else:\n            #df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB --> {:.2f} MB (Decreased by {:.1f}%)'.format(\n        start_mem, end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load data"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"%%time\ntrain_id = pd.read_csv('../input/train_identity.csv')\ntrain_trn = pd.read_csv('../input/train_transaction.csv')\ntest_id = pd.read_csv('../input/test_identity.csv')\ntest_trn = pd.read_csv('../input/test_transaction.csv')\n\nprint(train_id.shape, test_id.shape)\nprint(train_trn.shape, test_trn.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature engineering"},{"metadata":{},"cell_type":"markdown","source":"## Prepare"},{"metadata":{"trusted":true},"cell_type":"code","source":"id_cols = list(train_id.columns.values)\ntrn_cols = list(train_trn.drop('isFraud', axis=1).columns.values)\n\nX_train = pd.merge(train_trn[trn_cols + ['isFraud']], train_id[id_cols], how='left')\nX_train = reduce_mem_usage(X_train)\nX_test = pd.merge(test_trn[trn_cols], test_id[id_cols], how='left')\nX_test = reduce_mem_usage(X_test)\n\nX_train_id = X_train.pop('TransactionID')\nX_test_id = X_test.pop('TransactionID')\ndel train_id,train_trn,test_id,test_trn\n\nall_data = X_train.append(X_test, sort=False).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_='''\ncorr_matrix = all_data.corr().abs()\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\ncolumns_to_drop = [c for c in upper.columns if any(upper[c] > 0.98)]\ndel upper\n\nprint('drop columns:', columns_to_drop)\nall_data.drop(columns_to_drop, axis=1, inplace=True)\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode_loop(df, col, drop=True):\n    df[col + '_cos'] = np.cos(2 * np.pi * df[col] / df[col].max())\n    df[col + '_sin'] = np.sin(2 * np.pi * df[col] / df[col].max())\n    if drop:\n        df.drop(col, axis=1, inplace=True)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ccols = [f'C{i}' for i in range(1,15)]\ndcols = [f'D{i}' for i in range(1,16)]\nmcols = ['M1','M2','M3','M5','M6','M7','M8','M9']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data['_log_dist_1_2'] = np.log1p(np.where(all_data['dist1'].isna(), all_data['dist2'], all_data['dist1']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## New date feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"import datetime\n\nSTART_DATE = '2017-11-30'\n#START_DATE = '2017-12-01'\nstartdate = datetime.datetime.strptime(START_DATE, \"%Y-%m-%d\")\ntrandate = all_data['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds=x)))\n\nall_data['_days'] = all_data['TransactionDT'] // (24*60*60)\n#all_data['_weekday'] = trandate.dt.dayofweek.astype(str)\nall_data['_hour'] = trandate.dt.hour\nall_data = encode_loop(all_data, '_hour')\n#all_data['_day'] = trandate.dt.day\n#all_data['_year_month'] = trandate.dt.year.astype(str) + '_' + trandate.dt.month.astype(str)\nall_data['_weekday__hour'] = trandate.dt.dayofweek.astype(str) + '_' + trandate.dt.hour.astype(str)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Combine feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data['_P_emaildomain__ProductCD'] = all_data['P_emaildomain'] + '__' + all_data['ProductCD']\nall_data['_card3__card5'] = all_data['card3'].astype(str) + '__' + all_data['card5'].astype(str)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Id feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data['_uid1'] = (all_data['_days'] - all_data['D1']).astype(str) + '__' + all_data['P_emaildomain'].astype(str)\nall_data['_uid2'] = all_data['card1'].astype(str) + '__' + all_data['addr1'].astype(str) + '__' + all_data['_uid1']\n\n# lag previous transaction\ngroup_key = ['_uid2']\nall_data = all_data.assign(\n        _day_lag_uid2 = all_data['TransactionDT'] - all_data.groupby(group_key)['TransactionDT'].shift(1)\n        #,_amount_lag_uid2 = all_data['TransactionAmt'] - all_data.groupby(group_key)['TransactionAmt'].shift(1)\n        #,_amount_ema5_uid2 = all_data.groupby(group_key)['TransactionAmt'].apply(lambda x: x.ewm(span=5).mean())\n        ,_amount_lag_pct_uid2 = np.abs(all_data.groupby(group_key)['TransactionAmt'].pct_change())\n)\n_='''\ngroup_key = ['_uid1']\nall_data = all_data.assign(\n        _day_lag_uid1 = all_data['TransactionDT'] - all_data.groupby(group_key)['TransactionDT'].shift(1)\n        ,_amount_lag_uid1 = all_data['TransactionAmt'] - all_data.groupby(group_key)['TransactionAmt'].shift(1)\n        ,_amount_ema5_uid1 = all_data.groupby(group_key)['TransactionAmt'].apply(lambda x: x.ewm(span=5).mean())\n        ,_amount_lag_pct_uid1 = np.abs(all_data.groupby(group_key)['TransactionAmt'].pct_change())\n)\n'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## New amount feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data['_amount_decimal'] = ((all_data['TransactionAmt'] - all_data['TransactionAmt'].astype(int)) * 1000).astype(int)\nall_data['_amount_decimal_len'] = all_data['TransactionAmt'].apply(lambda x: len(re.sub('0+$', '', str(x)).split('.')[1]))\nall_data['_amount_fraction'] = all_data['TransactionAmt'].apply(lambda x: float('0.'+re.sub('^[0-9]|\\.|0+$', '', str(x))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Aggregate feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"def values_agg(df, periods, columns, aggs=['max']):\n    for period in periods:\n        for col in columns:\n            if col in df.columns:\n                new_col = f'{col}_{period}'\n                grouped_col = df.groupby([period])[col]\n                for a in aggs:\n                    df[f'_{a}_{new_col}'] = df[period].map(grouped_col.agg(a).to_dict())\n    return df\n\nall_data = values_agg(all_data, ['_uid2'], ccols)\n#all_data = values_agg(all_data, ['_uid2'], dcols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"amt_cols = ['_uid2','_P_emaildomain__ProductCD']\n\nall_data = values_agg(all_data, amt_cols, ['TransactionAmt'], aggs=['max','mean','var'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Count encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"for f in ccols + amt_cols:\n    vc = all_data[f].value_counts(dropna=False)\n    all_data[f'_count_full_{f}'] = all_data[f].map(vc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data['_all_na'] = all_data.isna().sum(axis=1).astype(np.int8)\n#all_data['_addr_na'] = all_data[['addr1','addr2','dist1','dist2']].isna().sum(axis=1).astype(np.int8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cx feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data['_ccols_nonzero'] = all_data[ccols].apply(lambda x: len(x.to_numpy().nonzero()[0]), axis=1)\nall_data['_ccols_sum'] = all_data[ccols].sum(axis=1).astype(np.int8)\nall_data['_ccols_0_bin'] = ''\nfor c in ccols:\n    all_data['_ccols_0_bin'] += (all_data[c] == 0).astype(int).astype(str)\n\nall_data.drop(ccols, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dx feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"#all_data['_D1_eq_D2'] = np.where(all_data['D1'] == all_data['D2'].fillna(0),'1','0')\n#all_data['_D3_eq_D5'] = np.where(all_data['D3'] == all_data['D5'],'1','0')\n#all_data['_D8_na'] = np.where(np.isnan(all_data['D8']),'1','0')\n\nall_data['_dcol_na'] = all_data[dcols].isna().sum(axis=1).astype(np.int8)\nall_data['_dcols_na_bin'] = ''\nfor c in dcols:\n    all_data['_dcols_na_bin'] += all_data[c].isna().astype(int).astype(str)\n\n# diff date threshold\nfor f in ['D1','D2']:\n    #all_data[f] = all_data[f].fillna(0) - all_data['_days'].apply(lambda x: np.min([154,x]))\n    all_data[f] = all_data[f].fillna(0) - all_data['_days']\n\nfor f in ['D3','D4','D5','D6','D7','D10','D11','D12','D13','D14','D15']:\n    all_data[f] = all_data[f].fillna(0) - all_data['_days']\n\n#all_data['_dcol_max'] = all_data[dcols].fillna(0).max(axis=1).astype(np.int8)\n    \n# time feature\n#all_data['D9'] = (all_data['D9'] * 24)\n#all_data['_D9_na'] = all_data['D9'].isna().astype(np.int8)\n\n#all_data.drop(dcols, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Mx feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"#all_data['_mcol_sum'] = all_data[mcols].sum(axis=1).astype(np.int8)\n#all_data['_mcol_na'] = all_data[mcols].isna().sum(axis=1).astype(np.int8)\nall_data['_mcols_na_bin'] = ''\nfor c in mcols:\n    all_data['_mcols_na_bin'] += all_data[c].isna().astype(int).astype(str)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Vx feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"vcols = [f'V{i}' for i in range(1,340)]\n\nsc = preprocessing.MinMaxScaler()\n\ndec = PCA(n_components=2, random_state=42) #0.99\nvcol_dec = dec.fit_transform(sc.fit_transform(all_data[vcols].fillna(-1)))\n\nall_data['_vcols_dec0'] = vcol_dec[:,0]\nall_data['_vcols_dec1'] = vcol_dec[:,1]\nall_data['_vcols_na'] = all_data[vcols].isna().sum(axis=1).astype(np.int8)\n\nfor f in ['V144','V145','V150','V151','V159','V160','V307']:\n    vcols.remove(f)\n\nall_data['_vcols_sum'] = all_data[vcols].sum(axis=1).astype(np.int8)\n\nall_data.drop(vcols, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_='''\ncnt_day = cnt_day / cnt_day.mean()\nall_data['_pct_trns_day'] = all_data['_days'].map(cnt_day.to_dict())\n'''\ncnt_day = all_data['_days'].value_counts()\nall_data['_count_trns_day'] = all_data['_days'].map(cnt_day.to_dict())\n\n#daily_cols = ['C1','C13'] # + amt_cols + dcols\ndaily_cols = ccols\nfor f in daily_cols:\n    if f in all_data.columns:\n        val_day = all_data[f].astype(str) + '__' + all_data['_days'].astype(str)\n        vc = val_day.value_counts(dropna=True)\n        all_data[f'_count_day_{f}'] = val_day.map(vc)\n        all_data[f'_count_pct_day_{f}'] = all_data[f'_count_day_{f}'] / all_data['_count_trns_day']\n\nall_data.drop('_count_trns_day', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Drop feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop low impotance features\n_='''\n'''\ncolumns_to_drop = ['id_07','id_08','id_10','id_12','id_16',\n                   'id_21','id_22','id_23','id_24','id_25',\n                   'id_26','id_27','id_28','id_29','id_32',\n                   'id_34','id_35','id_36','id_37']\nall_data.drop(columns_to_drop, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"many_same_values_columns = [c for c in all_data.drop('isFraud', axis=1).columns if all_data[c].value_counts(normalize=True).values[0] >= 0.98]\ncolumns_to_drop = list(many_same_values_columns)\nprint('drop columns:', columns_to_drop)\nall_data.drop(columns_to_drop, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop feature\nall_data.drop(['_days'], axis=1, inplace=True)\nall_data.drop(['TransactionDT'], axis=1, inplace=True) #,'card3','card5'\n\nuid = all_data['_uid2']\n\ndrop_card_cols = ['_uid1','_uid2','card1','card2']\n#all_data.drop(drop_card_cols, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Encode category feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"_='''\n'''\ncat_cols = ['ProductCD','card1','card2','card3','card4','card5','card6','addr1','addr2','P_emaildomain','R_emaildomain',\n            'M1','M2','M3','M4','M5','M6','M7','M8','M9','DeviceType','DeviceInfo'] + [f'id_{i}' for i in range(12,39)]\n\n# to str type\nfor i in cat_cols:\n    if i in all_data.columns:\n        all_data[i] = all_data[i].astype(str)\n        #all_data[i].fillna('unknown', inplace=True) # need for category-type\n\n# factorize\nenc_cols = []\nfor i, t in all_data.loc[:, all_data.columns != 'isFraud'].dtypes.iteritems():\n    if t == object:\n        enc_cols.append(i)\n        all_data[i] = pd.factorize(all_data[i])[0]\n        #all_data[i] = all_data[i].astype('category')\nprint(enc_cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Total feature count"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('features:', all_data.shape[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predict"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = all_data[all_data['isFraud'].notnull()]\nX_test = all_data[all_data['isFraud'].isnull()].drop('isFraud', axis=1)\nY_train = X_train.pop('isFraud')\n\nuid_train = uid[all_data['isFraud'].notnull()]\nuid_fraud = uid_train[Y_train == 1]\nuid_test = uid[all_data['isFraud'].isnull()]\n\ntrain_group = trandate[:len(X_train)].dt.month\n\ndel uid\ndel all_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_='''\npseudo = X_test[uid_test.isin(uid_fraud).values]\npseudo['isFraud'] = 1\n\nY_train = pd.concat([Y_train, pseudo.pop('isFraud')], axis=0)\nX_train = pd.concat([X_train, pseudo], axis=0)\np_group = pd.Series([train_group[0]] * len(pseudo))\ntrain_group = pd.concat([train_group, p_group], axis=0)\nprint(len(pseudo))\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_cols = ['card1','card2','card3','card5','addr1','_uid1','_uid2','_P_emaildomain__ProductCD','_card3__card5']\nfor f in cat_cols:\n    if f in X_train.columns:\n        train_set = set(X_train[f])\n        test_set = set(X_test[f])\n        tt = train_set.intersection(test_set)\n        print(f, '-', 'train:%.3f'%(len(tt)/len(train_set)), ',test:%.3f'%(len(tt)/len(test_set)))\n        X_train[f] = X_train[f].map(lambda x: -999 if x not in tt else x)\n        X_test[f] = X_test[f].map(lambda x: -999 if x not in tt else x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\nfrom imblearn.datasets import make_imbalance\n\ndef scale_minmax(preds):\n    return (preds - preds.min()) / (preds.max() - preds.min())\n\ndef pred_lgb(X_train, Y_train, X_test, params, num_iterations=1000):\n    feature_importances = pd.DataFrame()\n    feature_importances['feature'] = X_train.columns\n \n    oof_preds = np.zeros(X_train.shape[0])\n    sub_preds = np.zeros(X_test.shape[0])\n\n    dtrain = lgb.Dataset(X_train[:312574], label=Y_train.iloc[:312574])\n    dvalid = lgb.Dataset(X_train[-175998:], label=Y_train[-175998:])\n\n    clf = lgb.train(params, dtrain, 5000, \n                    valid_sets=[dtrain, dvalid], valid_names=['train', 'valid'],\n                    verbose_eval=1000, early_stopping_rounds=200)\n    feature_importances['fold_0'] = clf.feature_importance()\n    oof_preds = clf.predict(X_train)\n    sub_preds = clf.predict(X_test)\n    \n    return oof_preds, sub_preds, feature_importances\n\n# predict with lightgbm, KFold \ndef pred_lgb_kfold(X_train, Y_train, X_test, params, nfolds=5):\n    feature_importances = pd.DataFrame()\n    feature_importances['feature'] = X_train.columns\n\n    oof_preds = np.zeros(X_train.shape[0])\n    sub_preds = np.zeros(X_test.shape[0])\n\n    kf = KFold(n_splits=nfolds, shuffle=False, random_state=42)\n    for fold, (train_index, test_index) in enumerate(kf.split(X_train, Y_train)):\n        dtrain = lgb.Dataset(X_train.iloc[train_index], label=Y_train.iloc[train_index])\n        dvalid = lgb.Dataset(X_train.iloc[test_index], label=Y_train.iloc[test_index])\n\n        clf = lgb.train(params, dtrain, 5000, \n                        valid_sets=[dtrain, dvalid], valid_names=['train', 'valid'],\n                        verbose_eval=1000, early_stopping_rounds=200)\n        feature_importances['fold_{}'.format(fold + 1)] = clf.feature_importance()\n        oof_preds[test_index] = clf.predict(X_train.iloc[test_index])\n        sub_preds += clf.predict(X_test) / nfolds\n    \n    return oof_preds, sub_preds, feature_importances","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict with lightgbm, LeaveOneGroupOut \ndef pred_lgb_LOGO(X_train, Y_train, X_test, groups, params, undersampling=False):\n    feature_importances = pd.DataFrame()\n    feature_importances['feature'] = X_train.columns\n\n    oof_preds = np.zeros(X_train.shape[0])\n    sub_preds = np.zeros(X_test.shape[0])\n\n    nfolds = groups.nunique()\n\n    kf = LeaveOneGroupOut()\n    for fold, (train_index, test_index) in enumerate(kf.split(X_train, Y_train, groups)):\n        if undersampling:\n            size1 = sum(Y_train.iloc[train_index]==1)\n            X_train_us, Y_train_us = make_imbalance(\n                    X_train.iloc[train_index].fillna(-9999), Y_train.iloc[train_index], \n                    sampling_strategy={0:size1*5, 1:size1}, random_state=42)\n            X_train_us = pd.DataFrame(X_train_us, columns=X_train.columns)\n            dtrain = lgb.Dataset(X_train_us, label=Y_train_us)\n        else:\n            dtrain = lgb.Dataset(X_train.iloc[train_index], label=Y_train.iloc[train_index])\n\n        dvalid = lgb.Dataset(X_train.iloc[test_index], label=Y_train.iloc[test_index])\n\n        clf = lgb.train(params, dtrain, 5000, \n                        valid_sets=[dtrain, dvalid], valid_names=['train', 'valid'],\n                        verbose_eval=1000, early_stopping_rounds=200)\n        feature_importances['fold_{}'.format(fold + 1)] = clf.feature_importance()\n        oof_preds[test_index] = clf.predict(X_train.iloc[test_index])\n        sub_preds += clf.predict(X_test) / nfolds\n    \n    return oof_preds, sub_preds, feature_importances","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict with lightgbm, LeaveOneGroupOut \ndef pred_lgb_LOGO_2(X_train, Y_train, X_test, groups, params):\n    feature_importances = pd.DataFrame()\n    feature_importances['feature'] = X_train.columns\n\n    oof_preds = np.zeros(X_train.shape[0])\n    sub_preds = np.zeros(X_test.shape[0])\n\n    nfolds = groups.nunique()\n    print('groups:', nfolds)\n\n    kf = LeaveOneGroupOut()\n    pred_count = 0\n    for fold, (tt_index, out_index) in enumerate(kf.split(X_train, Y_train, groups)):\n        train_index = tt_index[tt_index < out_index[0]]\n        test_index = tt_index[tt_index > out_index[len(out_index) - 1]]\n        print(f'fold[{fold+1}] - {len(tt_index)} (train:{len(train_index)}, valid:{len(test_index)})')\n        if len(train_index) == 0 or len(test_index) == 0: # or len(train_index) < len(test_index)\n            print('- skip')\n            continue\n        \n        pred_count += 1\n        \n        dtrain = lgb.Dataset(X_train.iloc[train_index], label=Y_train.iloc[train_index])\n        dvalid = lgb.Dataset(X_train.iloc[test_index], label=Y_train.iloc[test_index])\n\n        clf = lgb.train(params, dtrain, 5000, \n                        valid_sets=[dtrain, dvalid], valid_names=['train', 'valid'],\n                        verbose_eval=500, early_stopping_rounds=200)\n        feature_importances['fold_{}'.format(pred_count)] = clf.feature_importance()\n        oof_preds += clf.predict(X_train)\n        sub_preds += clf.predict(X_test) \n\n    oof_preds = oof_preds / pred_count\n    sub_preds = sub_preds / pred_count \n        \n    return oof_preds, sub_preds, feature_importances","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nparams={\n        #'boosting':'dart', # dart (drop out trees)\n        'learning_rate': 0.01,\n        'objective': 'binary',\n        'boost_from_average': False,\n        'is_unbalance': False,\n        'metric': 'auc',\n        'num_threads': -1,\n        'num_leaves': 256,\n        'max_bin': 256,\n        'verbose': 1,\n        'random_state': 42,\n        'bagging_fraction': 0.85,\n        'bagging_freq': 1,\n        'feature_fraction': 0.60\n    }\n\n#oof_preds, sub_preds, feature_importances = pred_lgb(X_train, Y_train, X_test, params)\n#oof_preds, sub_preds, feature_importances = pred_lgb_kfold(X_train, Y_train, X_test, params, nfolds=3)\noof_preds, sub_preds, feature_importances = pred_lgb_LOGO(X_train, Y_train, X_test, train_group, params) #, undersampling=True\n#oof_preds, sub_preds, feature_importances = pred_lgb_LOGO_2(X_train, Y_train, X_test, train_group, params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr, tpr, thresholds = metrics.roc_curve(Y_train, oof_preds)\nauc = metrics.auc(fpr, tpr)\n\nplt.plot(fpr, tpr, label='ROC curve (area = %.3f)'%auc)\nplt.legend()\nplt.title('ROC curve')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.grid(True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add pseudo labeled data\n_='''\n#X_test_p1 = X_test[(sub_preds <= 0.00001)].copy()\n#X_test_p1['isFraud'] = 0\n#print(X_test_p1.shape)\n#Y_train = pd.concat([Y_train, X_test_p1.pop('isFraud')], axis=0)\n#X_train = pd.concat([X_train, X_test_p1], axis=0)\n\nX_test_p2 = X_test[(sub_preds >= 0.99)].copy()\nX_test_p2['isFraud'] = 1\nprint(X_test_p2.shape)\nY_train = pd.concat([Y_train, X_test_p2.pop('isFraud')], axis=0)\nX_train = pd.concat([X_train, X_test_p2], axis=0)\n\nY_train.reset_index(drop=True, inplace=True)\nX_train.reset_index(drop=True, inplace=True)\n\noof_preds, sub_preds, feature_importances = pred_lgb_kfold(X_train, Y_train, X_test, params, nfolds=3)\n\nfpr, tpr, thresholds = metrics.roc_curve(Y_train, oof_preds)\nauc = metrics.auc(fpr, tpr)\n\nplt.plot(fpr, tpr, label='ROC curve (area = %.3f)'%auc)\nplt.legend()\nplt.title('ROC curve')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.grid(True)\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"folds = [c for c in feature_importances.columns if c.startswith('fold')]\nfeature_importances['average'] = feature_importances[folds].mean(axis=1)\nfeature_importances.sort_values(by='average', ascending=False, inplace=True)\n#feature_importances.to_csv('feature_importances.csv')\n\nplt.figure(figsize=(13, 13))\nsns.barplot(data=feature_importances.head(50), x='average', y='feature');\nplt.title('50 TOP feature importance');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importances.sort_values(by='average', ascending=False)['feature'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame()\nsubmission['TransactionID'] = X_test_id\nsubmission['isFraud'] = sub_preds\n\nsubmission.loc[uid_test.isin(uid_fraud).values, 'isFraud'] = 1\n\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.mean(sub_preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}