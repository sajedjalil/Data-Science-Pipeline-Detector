{"cells":[{"metadata":{},"cell_type":"markdown","source":"* Model code is from the baseline + [gpu/Xhulu's kernel](https://www.kaggle.com/xhlulu/ieee-fraud-xgboost-with-gpu-fit-in-40s)\n* I've added a few simple & generic fraud/anomaly features. Surprisingly, they don't seem to have aneffect here, possibly due to artifacts in the anonymization process, or other reasons. the kernel will be updated with additional features.\n* Isolation forest for anomaly detection feature taken from here: https://www.kaggle.com/danofer/anomaly-detection-for-feature-engineering-v2\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\nimport xgboost as xgb\n\nfrom sklearn.ensemble import IsolationForest","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"%%time\ntrain_transaction = pd.read_csv('../input/train_transaction.csv', index_col='TransactionID') # ,nrows=42345\ntest_transaction = pd.read_csv('../input/test_transaction.csv', index_col='TransactionID') #,nrows=12345\n\ntrain_identity = pd.read_csv('../input/train_identity.csv', index_col='TransactionID')\ntest_identity = pd.read_csv('../input/test_identity.csv', index_col='TransactionID')\n\nsample_submission = pd.read_csv('../input/sample_submission.csv', index_col='TransactionID')\n\ntrain = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\ntest = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)\n\nprint(train.shape)\nprint(test.shape)\n\ny_train = train['isFraud'].copy()\ndel train_transaction, train_identity, test_transaction, test_identity\n\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ## join train+test for easier feature engineering:\n# df = pd.concat([train,test],sort=False)\n# print(df.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Add some features\n* missing values count\n    * TODO: nans per cattegory/group (e..g V columns)\n    * Could be more efficient with this code, but that's aimed at columnar, not row level summation: https://stackoverflow.com/questions/54207038/groupby-columns-on-column-header-prefix\n* Add some of the time series identified in external platform\n* ToDo: anomaly detection features. \n* proxy for lack of an identifier, duplicate values. \n    * TODO: try to understand what could be a proxy for a key/customer/card identifier (then apply features based on that).\n    \n    \n* ToDo: readd feature of identical transactions: this is typically a strong feature, but (surprisingly) gave no signal in this dataset. Both with and without transaction amount (and with transaction time removed ofc)."},{"metadata":{"trusted":true},"cell_type":"code","source":"list(train.columns)\n\n# COLUMN_GROUP_PREFIXES = [\"card\",\"C\",\"D\",\"M\",\"V\",\"id\"]\nCOLUMN_GROUP_PREFIXES = [\"card\",\"D\",\"M\",\"id\"] # \"C\" , \"V\" # V has many values, slow, \n\ndef column_group_features(df):\n    \"\"\"\n    Note: surprisingly slow! \n    TODO: Check speed, e.g. with `$ pip install line_profiler`\n    \"\"\"\n    df[\"total_missing\"] = df.isna().sum(axis=1)\n    print(\"total_missing\",df[\"total_missing\"].describe(percentiles=[0.5]))\n    df[\"total_unique_values\"] = df.nunique(axis=1)\n    print(\"total_unique_values\",df[\"total_unique_values\"].describe(percentiles=[0.5]))\n    \n    for p in COLUMN_GROUP_PREFIXES:\n        col_group = [col for col in df if col.startswith(p)]\n        print(\"total cols in subset:\", p ,len(col_group))\n        df[p+\"_missing_count\"] = df[col_group].isna().sum(axis=1)\n        print(p+\"_missing_count\", \"mean:\",df[p+\"_missing_count\"].describe(percentiles=[]))\n        df[p+\"_uniques_count\"] = df[col_group].nunique(axis=1)\n        print(p+\"_uniques_count\", \"mean:\",df[p+\"_uniques_count\"].describe(percentiles=[]))\n#         df[p+\"_max_val\"] = df[col_group].max(axis=1)\n#         df[p+\"_min_val\"] = df[col_group].min(axis=1)\n    print(\"done \\n\")\n    return df\n\n\n# From kernel https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\n# WARNING! THIS CAN DAMAGE THE DATA \ndef reduce_mem_usage(df,do_categoricals=False):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            if do_categoricals==True:\n                df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n\ntrain = reduce_mem_usage(train,do_categoricals=False)\ntest = reduce_mem_usage(test,do_categoricals=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ntrain = column_group_features(train)\nprint(\"train features generated\")\n\ntest = column_group_features(test)\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## datetime features\n* try to guess date and datetime delta unit, then add features\n* TODO: strong features potential already found offline, need to validate\n* Try 01.12.2017 as start date: https://www.kaggle.com/kevinbonnes/transactiondt-starting-at-2017-12-01"},{"metadata":{"trusted":true},"cell_type":"code","source":"import datetime\n\nSTART_DATE = '2017-12-01'\n\n# Preprocess date column\nstartdate = datetime.datetime.strptime(START_DATE, '%Y-%m-%d')\ntrain['time'] = train['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds = x)))\ntest['time'] = test['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds = x)))\n\n## check if time of day is morning/early night, and/or weekend/holiday:\ntrain[\"hour_of_day\"] = train['time'].dt.hour\ntest[\"hour_of_day\"] = test['time'].dt.hour\n\n## check if time of day is morning/early night, and/or weekend/holiday: (day of the week with Monday=0, Sunday=6.)\ntrain[\"day_of_week\"] = train['time'].dt.dayofweek\ntest[\"day_of_week\"] = test['time'].dt.dayofweek\n\nprint(train['time'].describe())\nprint(test['time'].describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## no clear correlation, but we expect any such features to be categorical in nature, not ordinal/continous. the model can findi t\ntrain[[\"isFraud\",\"hour_of_day\",\"day_of_week\"]].sample(frac=0.1).corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### label-encode & model build\n* TODO: compare to OHE? +- other encoding/embedding methods"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop target, fill in NaNs ?\n# consider dropping the TransactionDT column as well...\nX_train = train.drop(['isFraud',\"time\"], axis=1)\nX_test = test.drop([\"time\"], axis=1).copy()\n\ndel train, test\n\n# Label Encoding\nfor f in X_train.columns:\n    if X_train[f].dtype=='object' or X_test[f].dtype=='object': \n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(X_train[f].values) + list(X_test[f].values))\n        X_train[f] = lbl.transform(list(X_train[f].values))\n        X_test[f] = lbl.transform(list(X_test[f].values))   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nX_train = reduce_mem_usage(X_train,do_categoricals=True)\nX_test = reduce_mem_usage(X_test,do_categoricals=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Anomaly detection features\n* Isolation forest approach for now, can easily be improved with semisupervised approach, additional models, TSNE etc'\n* Based on this kernel: https://www.kaggle.com/danofer/anomaly-detection-for-feature-engineering-v2\n\n* Note: potential improvement: train additional model on only positive (non fraud) samples on concatenated train+test. \n\n\n\n##### Isolation forest (anomaly detection)\n* https://www.kaggle.io/svf/1100683/56c8356ed1b0a6efccea8371bc791ba7/__results__.html#Tree-based-techniques )\n* contamination = % of anomalies expected  (fraud class % in our case)\n\n* isolation forest doesn't work on nan values!\n    * TODO: model +- transaction amount. +- nan imputation (at least/especially for important columns)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all = pd.concat([X_train.dropna(axis=1),X_test.dropna(axis=1)]).drop([\"TransactionDT\"],axis=1).dropna(axis=1)\nTR_ROWS = X_train.shape[0]\nNO_NAN_COLS = df_all.columns\nprint(\"num of no nan cols\",len(NO_NAN_COLS))\nprint(df_all.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nclf = IsolationForest(random_state=42,  max_samples=0.7, bootstrap=True,n_jobs=2,\n                          n_estimators=120,max_features=0.9,behaviour=\"new\",contamination= 0.035)\nclf.fit(df_all)\ndel (df_all)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## add anomalous feature.\n## Warning! this is brittle! be careful with the columns!!\n\nX_train[\"isolation_overall_score\"] =clf.decision_function(X_train[NO_NAN_COLS])\nX_test[\"isolation_overall_score\"] =clf.decision_function(X_test[NO_NAN_COLS])\n\nprint(\"Fraud only mean anomaly score\",X_train.loc[y_train==1][\"isolation_overall_score\"].mean())\nprint(\"Non-Fraud only mean anomaly score\",X_train.loc[y_train==0][\"isolation_overall_score\"].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train only on non fraud samples\n\nclf = IsolationForest(random_state=42,  bootstrap=False,  max_samples=0.85,\n                          n_estimators=100,max_features=0.8,behaviour=\"new\",n_jobs=1)\nclf.fit(X_train[NO_NAN_COLS].loc[y_train==1].values)\n\nX_train[\"isolation_pos_score\"] =clf.decision_function(X_train[NO_NAN_COLS])\nX_test[\"isolation_pos_score\"] =clf.decision_function(X_test[NO_NAN_COLS])\n\ndel (clf)\n\nprint(\"Fraud only mean pos-anomaly score\",X_train.loc[y_train==1][\"isolation_pos_score\"].mean())\nprint(\"Non-Fraud only mean pos-anomaly score\",X_train.loc[y_train==0][\"isolation_pos_score\"].mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Model training\n\n* todo: do cross_val_predict (sklearn) using sklearn api for convenience\n* Temporal split :  use sklearn's TimeSeriesSplit (or manual) for early stopping/validation + validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom sklearn.model_selection import KFold, TimeSeriesSplit\nfrom sklearn.metrics import roc_auc_score\nEPOCHS = 4\nkf = KFold(n_splits = EPOCHS, shuffle = True)\n# kf = TimeSeriesSplit(n_splits = EPOCHS) # temporal validation. use this to evaluate performance better , not necessarily as good for OOV ensembling though!\n\ny_preds = np.zeros(sample_submission.shape[0])\ny_oof = np.zeros(X_train.shape[0])\nfor tr_idx, val_idx in kf.split(X_train, y_train):\n    clf = xgb.XGBClassifier(#n_jobs=2,\n        n_estimators=500,  # 500 default\n        max_depth=9, # 9\n        learning_rate=0.05,\n        subsample=0.9,\n        colsample_bytree=0.9,\n#         tree_method='gpu_hist' # #'gpu_hist', - faster, less exact , \"gpu_exact\" - better perf\n#         ,min_child_weight=2 # 1 by default\n    )\n    \n    X_tr, X_vl = X_train.iloc[tr_idx, :], X_train.iloc[val_idx, :]\n    y_tr, y_vl = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n    clf.fit(X_tr, y_tr)\n    y_pred_train = clf.predict_proba(X_vl)[:,1]\n    y_oof[val_idx] = y_pred_train\n    print('ROC AUC {}'.format(roc_auc_score(y_vl, y_pred_train)))\n    \n    y_preds+= clf.predict_proba(X_test)[:,1] / EPOCHS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make submissions\nsample_submission['isFraud'] = y_preds\nsample_submission.to_csv('dan_xgboost.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Simple model based feature importance plot\n* TODO: shapley, interactions\n\n* It looks like our grouped missing values are **valuable**, although the datetime features seemingly didn't (likely, some of the anonymized variables already capture them). They may have some marginal contribution.\n    * toDo: check that run models with and without them"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# fi = pd.DataFrame(index=clf.feature_names_)\nfi = pd.DataFrame(index=X_train.columns)\nfi['importance'] = clf.feature_importances_\nfi.loc[fi['importance'] > 0.0005].sort_values('importance').head(50).plot(kind='barh', figsize=(14, 28), title='Feature Importance')\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}