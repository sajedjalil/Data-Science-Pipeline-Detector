{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Predicting frauds step by step","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 1 - Introduction","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Hi! My name is Yuri and I'm studying data science and machine learning. After taking courses on this subject and practicing with some getting started competitions, I decided to take a look on how a **real** Kaggle competition looks like. In this notebook, my idea is to present my workflow sequentially, without changing the order of the steps that I took, and writing down everything that was important to my decisions. The goal here is not to provide a detailed analysis concisely written, but rather show how my understanding of the problem evolved with data exploration and by reading other notebooks and discussions made by the Kaggle community. I think this notebook will be very useful to me in future projects, and I hope it may be useful to other beginners like me when facing such a challenging competition.\n\n**Finally, I'm still learning from this and other competitions, so, any comments or constructive criticism will be helpful :)**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 2 - A simple data analysis and modelling","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#Importing libraries.\n\nimport os\nimport gc\nimport time\nimport datetime\n\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport lightgbm as lgb\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, TimeSeriesSplit, GridSearchCV, train_test_split\nfrom sklearn import metrics\n\n\n#Reading the data.\n\ntrain_identity = pd.read_csv('/kaggle/input/ieee-fraud-detection/train_identity.csv')\ntrain_transaction = pd.read_csv('/kaggle/input/ieee-fraud-detection/train_transaction.csv')\ntest_identity = pd.read_csv('/kaggle/input/ieee-fraud-detection/test_identity.csv')\ntest_transaction = pd.read_csv('/kaggle/input/ieee-fraud-detection/test_transaction.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First things first! Let's take a look at the training data and discover some important information about it, like the number of columns in the identity and transaction data, the number of frauds, and how much missing information there is in each column.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('train_identity has {} columns'.format(len(train_identity.columns)))\ntrain_identity.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('train_transaction has {} columns'.format(len(train_transaction.columns)))\ntrain_transaction.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"There are {} fraudulent transactions out of {}\".format(train_transaction['isFraud'].sum(),train_transaction.shape[0]))\n_ = sns.countplot(x=\"isFraud\", data=train_transaction)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"identity_cols_withNA = 100*train_identity.isnull().sum().sort_values(ascending=False)/train_identity.shape[0]\nidentity_cols_withNA = identity_cols_withNA[identity_cols_withNA > 0]\nprint('train_identity has {} columns with null entries. The 10 most empty columns are shown below'.format(identity_cols_withNA.shape[0]))\nidentity_cols_withNA[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transaction_cols_withNA = 100*train_transaction.isnull().sum().sort_values(ascending=False)/train_transaction.shape[0]\ntransaction_cols_withNA = transaction_cols_withNA[transaction_cols_withNA > 0]\nprint('train_transaction has {} columns with null entries. The 10 most empty columns are shown below'.format(transaction_cols_withNA.shape[0]))\ntransaction_cols_withNA[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are many problems already in the beginning. We have a very large dataset, with many missing values, and the columns does not have a clear meaning. The data description only tells us what are the categorical features and states that the column \"isFraud\" is the target variable, \"TransactionDT\" is a timedelta, and \"TransactionID\" is the column that joins the identity and transaction files. By reading [the post made by the competition host](https://www.kaggle.com/c/ieee-fraud-detection/discussion/101203) we discover that the field names are purposefully masked for privacy protection and contract agreement. We also are able to understand what are the features related to, but not what many of them actually are. Hence, it is really hard to try things like manually impute the data by using domain knowledge or grasp meaningful information with exploratory data analysis (of course I already took a look at the notebooks that did it, but they did not helped me very much at this point). \n\nWe shall keep it simple then. Let's explore the categorical columns and, to gain further insights over the dataset, also make a simple model without much data cleaning and feature engineering.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"identity_cat_cols = ['DeviceType','DeviceInfo']\nfor i in range(12,39):\n    identity_cat_cols.append('id_'+str(i))\n\ntransaction_cat_cols = ['ProductCD','addr1','addr2','P_emaildomain','R_emaildomain']\nfor i in range(1,7):\n    transaction_cat_cols.append('card'+str(i))\n    \nfor i in range(1,10):\n    transaction_cat_cols.append('M'+str(i))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Number of unique entries per categorical column (including nulls) in train_identity:\\n')\nfor col in identity_cat_cols:\n    cat_number = len(train_identity[col].unique())\n    print(col+': '+str(cat_number))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Number of unique entries per categorical column (including nulls) in train_transaction:\\n')\nfor col in transaction_cat_cols:\n    cat_number = len(train_transaction[col].unique())\n    print(col+': '+str(cat_number))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are a lot of different categories in some of the features. Card1 has astounding 13553 different categories. DeviceInfo also has over a thousand categories (1787). We must deal with that later.\n\nBefore starting modelling, notice that such a large dataset consume a lot of RAM (over 3GB).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_identity_mem_usage = train_identity.memory_usage(index = True).sum()/(1024**2)\ntrain_transaction_mem_usage = train_transaction.memory_usage(index = True).sum()/(1024**2)\ntest_identity_mem_usage = test_identity.memory_usage(index = True).sum()/(1024**2)\ntest_transaction_mem_usage = test_transaction.memory_usage(index = True).sum()/(1024**2)\nprint('Memory usage of train_identity is {:.2f} MB'.format(train_identity_mem_usage))\nprint('Memory usage of train_transaction is {:.2f} MB'.format(train_transaction_mem_usage))\nprint('Memory usage of test_identity is {:.2f} MB'.format(test_identity_mem_usage))\nprint('Memory usage of test_transaction is {:.2f} MB'.format(test_transaction_mem_usage))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Unfortunately, I only realized that after trying to run some code that crashed my kernel, but if you are reading this, you won't make the same mistake! There is a way to reduce the memory of a dataset, which consists of converting every categorical column data type to category and the numerical columns to the type that ocuppies the minimum amount of memory possible while also preserving all the information in that column. The code below is inspired in what I found in [other kernel](https://www.kaggle.com/artgor/eda-and-models) and at https://www.mikulskibartosz.name/how-to-reduce-memory-usage-in-pandas/.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df,cat_cols):\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in cat_cols:\n        df[col] = df[col].astype('category')\n        \n    num_cols = list(set(df.columns) - set(cat_cols))\n    for col in num_cols:\n        col_type = df[col].dtype        \n        c_min = df[col].min()\n        c_max = df[col].max()\n        if str(col_type)[:3] == 'int':\n            if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                df[col] = df[col].astype(np.int8)\n            elif c_min > np.iinfo(np.uint8).min and c_max < np.iinfo(np.uint8).max:\n                df[col] = df[col].astype(np.uint8)\n            elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                df[col] = df[col].astype(np.int16)\n            elif c_min > np.iinfo(np.uint16).min and c_max < np.iinfo(np.uint16).max:\n                df[col] = df[col].astype(np.uint16)\n            elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                df[col] = df[col].astype(np.int32)\n            elif c_min > np.iinfo(np.uint32).min and c_max < np.iinfo(np.uint32).max:\n                df[col] = df[col].astype(np.uint32)                    \n            elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                df[col] = df[col].astype(np.int64)\n            elif c_min > np.iinfo(np.uint64).min and c_max < np.iinfo(np.uint64).max:\n                df[col] = df[col].astype(np.uint64)  \n        else:\n            if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                df[col] = df[col].astype(np.float16)\n            elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                df[col] = df[col].astype(np.float32)\n            else:\n                df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is important to understand what is being done in this function, which can be accomplished by reading the [numpy documentation on its basic data types](https://numpy.org/doc/stable/user/basics.types.html). Now we apply this function to the training data set and merge the identity and transaction dataframes.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Reducing memory usage of train_identity\\n')\ntrain_identity = reduce_mem_usage(train_identity,identity_cat_cols)\nprint('\\nReducing memory usage of train_transaction\\n')\ntrain_transaction = reduce_mem_usage(train_transaction,transaction_cat_cols)\n\ntrain = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')\ntrain_mem_usage = train.memory_usage(index = True).sum()/(1024**2)\nprint('\\nMemory usage of train is {:.2f} MB'.format(train_mem_usage))\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's do the same with the test set. The test dataset has some columns named slightly different from the training dataset. For convenience, we shall rename them. After merging identity and transaction, it is good to delete the heavy variables and use the gc python module to collect the garbage and free the RAM for further computation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_to_rename = {}\nfor i in range(10):    \n    cols_to_rename['id-0'+str(i)] = 'id_0'+str(i)\nfor i in range(10,39):    \n    cols_to_rename['id-'+str(i)] = 'id_'+str(i)\n    \ntest_identity = test_identity.rename(columns=cols_to_rename)\n\nprint('Reducing memory usage of test_identity\\n')\ntest_identity = reduce_mem_usage(test_identity,identity_cat_cols)\nprint('\\nReducing memory usage of test_transaction\\n')\ntest_transaction = reduce_mem_usage(test_transaction,transaction_cat_cols)\n\ntest = pd.merge(test_transaction, test_identity, on='TransactionID', how='left')\ntest_mem_usage = test.memory_usage(index = True).sum()/(1024**2)\nprint('\\nMemory usage of test is {:.2f} MB'.format(test_mem_usage))\n\ndel train_transaction\ndel train_identity\ndel test_identity\ndel test_transaction\ngc.collect()\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we have our data in good shape let's start modelling. But first we need to decide how to impute the missing values, right? Well, not necessarily. Gradient boosting models like XGBoost and LightGBM are actually [able to handle missing data](http://mlexplained.com/2018/01/05/lightgbm-and-xgboost-explained/), and since we already discussed that imputing nulls in this dataset is not easy we will not bother with that now. Since [LightGBM is faster than XGBoost](https://towardsdatascience.com/lightgbm-vs-xgboost-which-algorithm-win-the-race-1ff7dd4917d), we will prefer this model here (like the majority of public kernels in this competition). The steps for modelling are the following:\n\n\n1. **Encode the categorical features -**  This is a tricky part. One-hot-encoding the categorical features allows us to to handle unseen categories, but it is [not optimal for LightGBM](https://lightgbm.readthedocs.io/en/latest/Advanced-Topics.html), which performs better when categories are encoded as integers. LabelEncoder is able to encode categorical data in integers, however, [it is meant to encode the target variable, not the features](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html?highlight=labelencoder#sklearn.preprocessing.LabelEncoder). This means that it only accepts a single column as the input and it cannot handle unseen categories. Since in this competition we only care about the training and test sets, one way to surpass this problem is to use LabelEncoder to encode the categorical features of the training and test set together, as done in https://www.kaggle.com/artgor/eda-and-models.\n\n\n2. **Search for the best hyperparameters -** this is a necessary step for achieving better scores. However, since the code takes some minutes to run and this is only a simple model made to gain insights over the dataset, [we will not bother with that now](https://www.kaggle.com/c/ieee-fraud-detection/discussion/103109). Instead, we shall use the hyperparameters that were already tuned in other notebooks.\n\n\n3. **Kfold cross validate the model -** we must do cross validation to see how well our model is able to perform on unseen data that is in the training set and to be able to obtain a better estimation of the feature importances. We evaluate the model using the AUC metric.\n\n\n4. **Predict the target variable on the test set -** we shall submit our result and see how well our model can perform on unseen data outside the training set. It will be good to compare this score with the mean score on the cross validation sets to draw important conclusions that will guide our future work.\n\nWith that in mind, let's start!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Step 1\n\ncat_cols = transaction_cat_cols + identity_cat_cols\nnum_cols = list(set(train.drop(['TransactionID','isFraud','TransactionDT'],axis=1).columns) - set(cat_cols))\n\ntrain_copy = train.copy()\ntest_copy = test.copy()\nfor col in cat_cols:\n    le = LabelEncoder()\n    le.fit(list(train_copy[col].astype(str).values) + list(test_copy[col].astype(str).values))\n    train_copy[col] = le.transform(list(train_copy[col].astype(str).values))\n    test_copy[col] = le.transform(list(test_copy[col].astype(str).values))\n\nX = train_copy[cat_cols + num_cols]\ny = train_copy['isFraud'] \nX_test = test_copy[cat_cols + num_cols]\n\ndel train_copy\ndel test_copy\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Step 2\nparams = {'num_leaves': 256,\n          'min_child_samples': 79,\n          'objective': 'binary',\n          'max_depth': 8,\n          'learning_rate': 0.02,\n          \"boosting_type\": \"gbdt\",\n          \"subsample_freq\": 3,\n          \"subsample\": 0.9,\n          \"bagging_seed\": 11,\n          \"metric\": 'auc',\n          \"verbosity\": -1,\n          'reg_alpha': 0.3,\n          'reg_lambda': 0.3,\n          'colsample_bytree': 0.9,\n         }\n\n#Step 3\nn_splits = 4\nskf = StratifiedKFold(n_splits=n_splits,shuffle=True)\nskf.get_n_splits(X, y)\n\nscores = []\ni = 1\ny_pred = np.zeros(len(X_test.iloc[:,0]))\nfeat_imp = pd.Series(0, index=X.columns)\nfor train_index, valid_index in skf.split(X, y):\n    print('Training the model using fold number {}...'.format(i))\n    X_train, X_valid = X.iloc[train_index,:], X.iloc[valid_index,:]\n    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n\n    model = lgb.LGBMClassifier(**params, n_estimators=500, n_jobs = -1)\n    model.fit(X_train, y_train,eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='auc', early_stopping_rounds=200,verbose=False)\n    y_pred_valid = model.predict_proba(X_valid)[:, 1]\n    y_pred += model.predict_proba(X_test)[:, 1]/n_splits\n    score = metrics.roc_auc_score(y_valid, y_pred_valid)\n    scores.append(score)\n    feat_imp += pd.Series(model.feature_importances_, index=X.columns)/n_splits\n    print('AUC score: {:.3f}'.format(score))\n    i += 1\n    \nprint('mean AUC score over the validation sets: {:.3f}\\n'.format(np.mean(scores)))    \nfig,ax = plt.subplots(figsize=(10,10))\n_ = feat_imp.nlargest(20).sort_values().plot(kind='barh',ax=ax)\n_ = ax.set_xlabel('Average importance (a.u.)')\n_ = ax.set_ylabel('Features')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Step 4\noutput = pd.DataFrame({'TransactionID': test.TransactionID, 'isFraud': y_pred})\noutput.to_csv('simple_lightgbm_submission.csv', index=False)\nprint(\"Submission successfully saved!\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Time to think before taking the next step!** \n\nThe model performance on the test set was about 0.91 (private score, 80% of the data), while the mean score on the cross validation sets is 0.964. This means that our model is probably overfitting the training set. However, it is still surprising that with no work on the data LightGBM was able to achieve this score.\n\n* **Possible reasons for overfitting:** from the feature importance plot, we are able to see that features that are likely to identify the client, such as card1, card2, addr1, dist1, ..., were very important to the model. But what if the test set has unseen clients? Indeed, this is exactly what happens, as explained in the [1st post of the competition's winner](https://www.kaggle.com/c/ieee-fraud-detection/discussion/111284). Another reason is that we used Kfold cross validation, which shuffles the data from different times of the dataset. When applying Kfold cross validation we still use all clients of the training set to train the model, which in turn tend to overfit the training set. Since the clients change over time, a better validation scheme would be to train the model with data in a given interval of time and try to predict frauds in the future. This can be accomplished by using TimeSeriesSplit together with the \"TransactionDT\" feature. As also discussed in the winner's post, once a client card has fraud, any other future transaction will be caracterized as fraudulent. Hence, we can take advantage of it to post-process the predctions and achieve better results. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 3 - Data manipulation and feature engineering","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now it's time to do some data cleaning and manipulation, and some feature engineering to try to improve the model. One important part of the workflow that helps finding good ways of doing this is data visualization. I will not focus into it very much here since there are some great kernels out there with many nice visualizations (like https://www.kaggle.com/ysjf13/cis-fraud-detection-visualize-feature-engineering). Let's start by some basic data cleaning. The first thing that we shall do is to get rid of highly correlated features (most V features are correlated). Chris Deotte, one of the winners of the competition, excluded many V features in a very interesting way, which is presented [in this notebook](https://www.kaggle.com/cdeotte/eda-for-columns-v-and-id). Here I will prefer a rather simple approach, based on computing the correlation matrix and recording the correlated columns (which I found [here](https://chrisalbon.com/machine_learning/feature_selection/drop_highly_correlated_features/))\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Cleaning the mess\ndel X\ndel X_test\ndel y\ndel X_train\ndel X_valid\ndel y_train\ndel y_valid\ndel y_pred\ndel y_pred_valid\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Sorting the numerical columns by name\nnum_cols.sort()\n\n# Create correlation matrix\ncorr_matrix = train[num_cols].corr().abs()\n\n# Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n# Find index of feature columns with correlation greater than 0.95\nto_drop = [column for column in upper.columns if any(upper[column] > 0.9)]\n\nprint('Showing the first 15 elements of to_drop (out of {}):\\n{}'.format(len(to_drop),to_drop[:15]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As expected, there are many highly correlated V columns. We shall drop all of them (except one for each group of correlated features, of course). There are C and D columns that are also correlated. For those, we shall exclude only the ones with many missing values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"to_drop_notV = to_drop[:14]\nto_drop_V = to_drop[14:]\n\nfor col in to_drop_notV:\n    na_proportion = train[col].isna().sum()/train.shape[0]\n    if na_proportion >= 0.4:\n        train = train.drop(col,axis=1)\n        test = test.drop(col,axis=1)\n        print('Column {} was dropped'.format(col))\n        \ntrain = train.drop(columns = to_drop_V)\ntest = test.drop(columns = to_drop_V)\nprint('\\n{} V columns were dropped'.format(len(to_drop_V)))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we deal with the missing values. The idea here is not to impute them with statistics, but just assign them to a value so that LGBM treat them separately (to understand why you must read how LGBM works).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Excluding the removed features from the definition of num_cols\nnum_cols = list(set(num_cols) - set(['D2','D6','D7']) - set(to_drop_V))\n\ntrain[cat_cols] = train[cat_cols].astype('str').replace('nan','unknown').astype('category')\ntest[cat_cols] = test[cat_cols].astype('str').replace('nan','unknown').astype('category')\ntrain[num_cols] = train[num_cols].fillna(-999)\ntest[num_cols] = test[num_cols].fillna(-999)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now we shall do some feature engineering**. Feature engineering in this problem seemed very difficult to me, and I took a while reading discussion posts and notebooks trying to understand what techniques could be used in this problem. As already mentioned, we do not have much information about the dataset, which makes things even more difficult. Nonetheless, there are many ways to do feature engineering that does not require a deep understanding of the dataset, just a try and error approach. The many ways to do that are very well explained in [this other topic written by Chris Deotte](https://www.kaggle.com/c/ieee-fraud-detection/discussion/108575), and one good approach to this specific competition can be found in [this notebook by Konstantin Yakovlev](https://www.kaggle.com/kyakovlev/ieee-basic-fe-part-1). Indeed, these guys did an amazing (and hard) work on this problem, and reading their notebooks and discussion posts were by far the moments when I learned the most. A good summary of important posts and notebooks about this competition can be found [here](https://www.kaggle.com/c/ieee-fraud-detection/discussion/107697).\n\nSoo, back to this notebook! To start, let's analyze the categorical features and see what can be done. I analyzed them one by one, trying to find some room for feature engineering. Below I present only the features that I found interesting.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Showing the first 10 unique elements of DeviceInfo (out of {}):\\n{}'\\\n      .format(len(list(train['DeviceInfo'].unique())),\\\n              list(train['DeviceInfo'].unique())[:10]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This column has many categories, many of them share some common information, like the name of the device (samsung, motorola etc). One thing that can be done to help the model is to create another column that contains this information (or at least most of them).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def setDevice(df):\n    df['DeviceInfo'] = df['DeviceInfo'].str.lower()   \n    df['device_name'] = df['DeviceInfo'].str.split('/', expand=True)[0]\n    \n    df.loc[df['device_name'].str.contains('sm', na=False), 'device_name'] = 'samsung'\n    df.loc[df['device_name'].str.contains('samsung', na=False), 'device_name'] = 'samsung'\n    df.loc[df['device_name'].str.contains('gt-', na=False), 'device_name'] = 'samsung'\n    df.loc[df['device_name'].str.contains('moto', na=False), 'device_name'] = 'motorola'\n    df.loc[df['device_name'].str.contains('lg', na=False), 'device_name'] = 'lg'\n    df.loc[df['device_name'].str.contains('rv:', na=False), 'device_name'] = 'rv'\n    df.loc[df['device_name'].str.contains('huawei', na=False), 'device_name'] = 'huawei'\n    df.loc[df['device_name'].str.contains('ale-', na=False), 'device_name'] = 'huawei'\n    df.loc[df['device_name'].str.contains('-l', na=False), 'device_name'] = 'huawei'\n    df.loc[df['device_name'].str.contains('blade', na=False), 'device_name'] = 'zte'\n    df.loc[df['device_name'].str.contains('linux', na=False), 'device_name'] = 'linux'\n    df.loc[df['device_name'].str.contains('xt', na=False), 'device_name'] = 'sony'\n    df.loc[df['device_name'].str.contains('htc', na=False), 'device_name'] = 'htc'\n    df.loc[df['device_name'].str.contains('asus', na=False), 'device_name'] = 'asus'\n    df.loc[df['device_name'].str.contains('lenovo', na=False), 'device_name'] = 'lenovo'\n    df.loc[df['device_name'].str.contains('pixel', na=False), 'device_name'] = 'google'\n    df.loc[df['device_name'].str.contains('redmi', na=False), 'device_name'] = 'xiaomi'\n    df.loc[df['device_name'].str.contains('windows', na=False), 'device_name'] = 'microsoft'\n    df.loc[df['device_name'].str.contains('microsoft', na=False), 'device_name'] = 'microsoft'\n    df.loc[df['device_name'].str.contains('nexus', na=False), 'device_name'] = 'google'\n    df.loc[df['device_name'].str.contains('ilium', na=False), 'device_name'] = 'lanix'\n    df.loc[df['device_name'].str.contains('lumia', na=False), 'device_name'] = 'nokia'\n    \n    df['DeviceInfo'] = df['DeviceInfo'].astype('category')\n    df['device_name'] = df['device_name'].astype('category')\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = setDevice(train)\ntest = setDevice(test)\nprint('The new column, device_name, has {} unique values'\\\n      .format(len(list(train['device_name'].unique()))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Showing the first 10 unique elements of id_31 (out of {}):\\n{}'\\\n      .format(len(list(train['id_31'].unique())),\\\n              list(train['id_31'].unique())[:10]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Same thing could be done here, but since is mixed and there are only 131 categories, I prefer not to create another column.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train['id_31'] = train['id_31'].str.lower().astype('category')\ntest['id_31'] = test['id_31'].str.lower().astype('category')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Showing the first 10 unique elements of id_30 (out of {}):\\n{}'\\\n      .format(len(list(train['id_30'].unique())),\\\n              list(train['id_30'].unique())[:10]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here it is easy to get the information about the operating system. Let's do it.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train['id_30'] = train['id_30'].str.lower()\ntest['id_30'] = test['id_30'].str.lower()\ntrain['id_30_word1'] = train['id_30'].str.split(' ', expand=True)[0].astype('category')\ntest['id_30_word1'] = test['id_30'].str.split(' ', expand=True)[0].astype('category')\ntrain['id_30'] = train['id_30'].astype('category')\ntest['id_30'] = test['id_30'].astype('category')\n\nprint('The new column, id_30_word1, has {} unique values'\\\n      .format(len(list(train['id_30_word1'].unique()))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Showing the unique elements of id_15:\\n{}'\\\n      .format(list(train['id_15'].unique())))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This column already had an entry called 'Unknown', which has the same meaning as a null value. We must join these two categories.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train['id_15'] = train['id_15'].str.lower().astype('category')\ntest['id_15'] = test['id_15'].str.lower().astype('category')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Showing the first 10 unique elements of P_emaildomain (out of {}):\\n{}'\\\n      .format(len(list(train['P_emaildomain'].unique())),\\\n              list(train['P_emaildomain'].unique())[:10]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For this column, we can separate the prefix and the suffix to get new information.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def SplitEmailDomain(df):\n    \n    df['P_emaildomain_prefix'] = df['P_emaildomain'].str.split('.',1, expand=True)[0].astype('category')\n    df['P_emaildomain_sufix'] = df['P_emaildomain'].str.split('.',1, expand=True)[1].fillna('unknown').astype('category')\n    df['R_emaildomain_prefix'] = df['R_emaildomain'].str.split('.',1, expand=True)[0].astype('category')\n    df['R_emaildomain_sufix'] = df['R_emaildomain'].str.split('.',1, expand=True)[1].fillna('unknown').astype('category')\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = SplitEmailDomain(train)\ntest = SplitEmailDomain(test)\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's move to the D columns. As explained in the data description post, these columns are information abou the number of days since something happened (like the day which the credit card was created). Hence, these features depend not only on the day that 'something happened', but also on the transaction day. To get rid of this mixed information and keep only the information about when something happened, we can subtract the information about the transaction. To do that, it is important to first notice that the column TransactionDT is in seconds and the training set covers 6 months of transactions.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Total number of days:',round((train.TransactionDT.max() - train.TransactionDT.min())/(24*60*60),3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# NORMALIZE D COLUMNS\nfor i in range(1,16):\n    if i not in [2,6,7]:\n        train['D'+str(i)] = train['D'+str(i)] - train.TransactionDT/np.float32(24*60*60)\n        test['D'+str(i)] = test['D'+str(i)] - test.TransactionDT/np.float32(24*60*60)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now our D features do not contain information about the transaction day.\n\nOk, we can continue doing feature engineering forever, but it is always a good idea to not do it blindly, but frequently stop and test if the changes we made have a positive or negative impact on our ML model. This is what we will do now. Below, the only difference from what I did in the last section is the cross validation scheme. As explained by Chris Deotte and others, a good way to do cross validation is to train the model on the first few months of transactions (like the first 4 or 5 months) and validate on the last month.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_cols = cat_cols + ['device_name','id_30_word1','P_emaildomain_prefix','P_emaildomain_sufix',\\\n                      'R_emaildomain_prefix','R_emaildomain_sufix']\n\ntrain_copy = train.copy()\ntest_copy = test.copy()\nfor col in cat_cols:\n    le = LabelEncoder()\n    le.fit(list(train_copy[col].astype(str).values) + list(test_copy[col].astype(str).values))\n    train_copy[col] = le.transform(list(train_copy[col].astype(str).values))\n    test_copy[col] = le.transform(list(test_copy[col].astype(str).values))\n\nfirst_day = train_copy['TransactionDT'].min()\nfive_months = first_day + 5*30*24*3600\nX_train = train_copy.loc[train_copy['TransactionDT'] <= five_months,cat_cols + num_cols]\ny_train = train_copy.loc[train_copy['TransactionDT'] <= five_months,'isFraud']\nX_valid = train_copy.loc[train_copy['TransactionDT'] > five_months,cat_cols + num_cols]\ny_valid = train_copy.loc[train_copy['TransactionDT'] > five_months,'isFraud'] \nX_test = test_copy[cat_cols + num_cols]\n\ndel train_copy\ndel test_copy\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'num_leaves': 256,\n          'min_child_samples': 79,\n          'objective': 'binary',\n          'max_depth': 8,\n          'learning_rate': 0.02,\n          \"boosting_type\": \"gbdt\",\n          \"subsample_freq\": 3,\n          \"subsample\": 0.9,\n          \"bagging_seed\": 11,\n          \"metric\": 'auc',\n          'reg_alpha': 0.3,\n          'reg_lambda': 0.3,\n          'colsample_bytree': 0.9,\n         }\n\n\nmodel = lgb.LGBMClassifier(**params, n_estimators=2000, n_jobs = -1)\nmodel.fit(X_train, y_train,eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='auc', early_stopping_rounds=100,verbose=50)\ny_pred_valid = model.predict_proba(X_valid)[:, 1]\ny_pred = model.predict_proba(X_test)[:, 1]\nscore = metrics.roc_auc_score(y_valid, y_pred_valid)\nfeat_imp = pd.Series(model.feature_importances_, index=X_train.columns)\n\nprint('AUC score: {:.3f}'.format(score))\n  \nfig,ax = plt.subplots(figsize=(10,10))\n_ = feat_imp.nlargest(20).sort_values().plot(kind='barh',ax=ax)\n_ = ax.set_xlabel('Average importance (a.u.)')\n_ = ax.set_ylabel('Features')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output = pd.DataFrame({'TransactionID': test.TransactionID, 'isFraud': y_pred})\noutput.to_csv('lightgbm_submission.csv', index=False)\nprint(\"Submission successfully saved!\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Time to think before proceeding again!**\n\nThe model did not perform better than before. Actually, it performed a little bit worse. We can also see from the AUC score on the training  and validation set that the model is still overfitting the training set, and it is not very good to generalize to unseen data (unseen clients here). \n\nWe can continue doing feature engineering to try to improve the model, but, for now, I will stop here. The feature engineering that won the competition was already very well explained by Chris Deotte. It consists, basically, on [finding features that uniquely indentify the clients](https://www.kaggle.com/c/ieee-fraud-detection/discussion/111510) (the UIDs), [create aggregated statistics on top of these UIDs](https://www.kaggle.com/c/ieee-fraud-detection/discussion/111453) and then remove the UIDs columns to prevent the model to overfit. To me, the best notebook to follow from this point of the analysis is also one made by Chris Deotte, which can be found [here](https://www.kaggle.com/cdeotte/xgb-fraud-with-magic-0-9600). A lot can be learned by just understanding the meaning of each transformation and the methods used to accomplish that.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}