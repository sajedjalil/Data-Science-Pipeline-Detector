{"cells":[{"metadata":{},"cell_type":"markdown","source":"# FRAUD DETECTION\n\nThis problem is a matter of guessing whether transaction is fraudulent or not through a number of pieces of data. \n\nLet's see data more detail.\n(Below code is the default settings and code for read data.)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport time\nimport datetime\nimport json\nimport gc\nfrom numba import jit\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm_notebook\n\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostRegressor, CatBoostClassifier\nfrom sklearn import metrics\n\nfrom itertools import product\n\nimport altair as alt\nfrom altair.vega import v5\nfrom IPython.display import HTML\n\n# using ideas from this kernel: https://www.kaggle.com/notslush/altair-visualization-2018-stackoverflow-survey\ndef prepare_altair():\n    \"\"\"\n    Helper function to prepare altair for working.\n    \"\"\"\n\n    vega_url = 'https://cdn.jsdelivr.net/npm/vega@' + v5.SCHEMA_VERSION\n    vega_lib_url = 'https://cdn.jsdelivr.net/npm/vega-lib'\n    vega_lite_url = 'https://cdn.jsdelivr.net/npm/vega-lite@' + alt.SCHEMA_VERSION\n    vega_embed_url = 'https://cdn.jsdelivr.net/npm/vega-embed@3'\n    noext = \"?noext\"\n    \n    paths = {\n        'vega': vega_url + noext,\n        'vega-lib': vega_lib_url + noext,\n        'vega-lite': vega_lite_url + noext,\n        'vega-embed': vega_embed_url + noext\n    }\n    \n    workaround = f\"\"\"    requirejs.config({{\n        baseUrl: 'https://cdn.jsdelivr.net/npm/',\n        paths: {paths}\n    }});\n    \"\"\"\n    \n    return workaround\n    \n\ndef add_autoincrement(render_func):\n    # Keep track of unique <div/> IDs\n    cache = {}\n    def wrapped(chart, id=\"vega-chart\", autoincrement=True):\n        if autoincrement:\n            if id in cache:\n                counter = 1 + cache[id]\n                cache[id] = counter\n            else:\n                cache[id] = 0\n            actual_id = id if cache[id] == 0 else id + '-' + str(cache[id])\n        else:\n            if id not in cache:\n                cache[id] = 0\n            actual_id = id\n        return render_func(chart, id=actual_id)\n    # Cache will stay outside and \n    return wrapped\n           \n\n@add_autoincrement\ndef render(chart, id=\"vega-chart\"):\n    \"\"\"\n    Helper function to plot altair visualizations.\n    \"\"\"\n    chart_str = \"\"\"\n    <div id=\"{id}\"></div><script>\n    require([\"vega-embed\"], function(vg_embed) {{\n        const spec = {chart};     \n        vg_embed(\"#{id}\", spec, {{defaultStyle: true}}).catch(console.warn);\n        console.log(\"anything?\");\n    }});\n    console.log(\"really...anything?\");\n    </script>\n    \"\"\"\n    return HTML(\n        chart_str.format(\n            id=id,\n            chart=json.dumps(chart) if isinstance(chart, dict) else chart.to_json(indent=None)\n        )\n    )\n    \n\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage(deep=True).sum() / 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                c_prec = df[col].apply(lambda x: np.finfo(x).precision).max()\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max and c_prec == np.finfo(np.float32).precision:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage(deep=True).sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df\n    \n\n@jit\ndef fast_auc(y_true, y_prob):\n    \"\"\"\n    fast roc_auc computation: https://www.kaggle.com/c/microsoft-malware-prediction/discussion/76013\n    \"\"\"\n    y_true = np.asarray(y_true)\n    y_true = y_true[np.argsort(y_prob)]\n    nfalse = 0\n    auc = 0\n    n = len(y_true)\n    for i in range(n):\n        y_i = y_true[i]\n        nfalse += (1 - y_i)\n        auc += y_i * nfalse\n    auc /= (nfalse * (n - nfalse))\n    return auc\n\n\ndef eval_auc(y_true, y_pred):\n    \"\"\"\n    Fast auc eval function for lgb.\n    \"\"\"\n    return 'auc', fast_auc(y_true, y_pred), True\n\n\ndef group_mean_log_mae(y_true, y_pred, types, floor=1e-9):\n    \"\"\"\n    Fast metric computation for this competition: https://www.kaggle.com/c/champs-scalar-coupling\n    Code is from this kernel: https://www.kaggle.com/uberkinder/efficient-metric\n    \"\"\"\n    maes = (y_true-y_pred).abs().groupby(types).mean()\n    return np.log(maes.map(lambda x: max(x, floor))).mean()\n    \n\ndef train_model_regression(X, X_test, y, params, folds=None, model_type='lgb', eval_metric='mae', columns=None, plot_feature_importance=False, model=None,\n                               verbose=10000, early_stopping_rounds=200, n_estimators=50000, splits=None, n_folds=3):\n    \"\"\"\n    A function to train a variety of regression models.\n    Returns dictionary with oof predictions, test predictions, scores and, if necessary, feature importances.\n    \n    :params: X - training data, can be pd.DataFrame or np.ndarray (after normalizing)\n    :params: X_test - test data, can be pd.DataFrame or np.ndarray (after normalizing)\n    :params: y - target\n    :params: folds - folds to split data\n    :params: model_type - type of model to use\n    :params: eval_metric - metric to use\n    :params: columns - columns to use. If None - use all columns\n    :params: plot_feature_importance - whether to plot feature importance of LGB\n    :params: model - sklearn model, works only for \"sklearn\" model type\n    \n    \"\"\"\n    columns = X.columns if columns is None else columns\n    X_test = X_test[columns]\n    splits = folds.split(X) if splits is None else splits\n    n_splits = folds.n_splits if splits is None else n_folds\n    \n    # to set up scoring parameters\n    metrics_dict = {'mae': {'lgb_metric_name': 'mae',\n                        'catboost_metric_name': 'MAE',\n                        'sklearn_scoring_function': metrics.mean_absolute_error},\n                    'group_mae': {'lgb_metric_name': 'mae',\n                        'catboost_metric_name': 'MAE',\n                        'scoring_function': group_mean_log_mae},\n                    'mse': {'lgb_metric_name': 'mse',\n                        'catboost_metric_name': 'MSE',\n                        'sklearn_scoring_function': metrics.mean_squared_error}\n                    }\n\n    \n    result_dict = {}\n    \n    # out-of-fold predictions on train data\n    oof = np.zeros(len(X))\n    \n    # averaged predictions on train data\n    prediction = np.zeros(len(X_test))\n    \n    # list of scores on folds\n    scores = []\n    feature_importance = pd.DataFrame()\n    \n    # split and train on folds\n    for fold_n, (train_index, valid_index) in enumerate(splits):\n        if verbose:\n            print(f'Fold {fold_n + 1} started at {time.ctime()}')\n        if type(X) == np.ndarray:\n            X_train, X_valid = X[columns][train_index], X[columns][valid_index]\n            y_train, y_valid = y[train_index], y[valid_index]\n        else:\n            X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n            \n        if model_type == 'lgb':\n            model = lgb.LGBMRegressor(**params, n_estimators = n_estimators, n_jobs = -1)\n            model.fit(X_train, y_train, \n                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric=metrics_dict[eval_metric]['lgb_metric_name'],\n                    verbose=verbose, early_stopping_rounds=early_stopping_rounds)\n            \n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n            \n        if model_type == 'xgb':\n            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns)\n            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns)\n\n            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=verbose, params=params)\n            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n        \n        if model_type == 'sklearn':\n            model = model\n            model.fit(X_train, y_train)\n            \n            y_pred_valid = model.predict(X_valid).reshape(-1,)\n            score = metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid)\n            print(f'Fold {fold_n}. {eval_metric}: {score:.4f}.')\n            print('')\n            \n            y_pred = model.predict(X_test).reshape(-1,)\n        \n        if model_type == 'cat':\n            model = CatBoostRegressor(iterations=20000,  eval_metric=metrics_dict[eval_metric]['catboost_metric_name'], **params,\n                                      loss_function=metrics_dict[eval_metric]['catboost_metric_name'])\n            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n\n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test)\n        \n        oof[valid_index] = y_pred_valid.reshape(-1,)\n        if eval_metric != 'group_mae':\n            scores.append(metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid))\n        else:\n            scores.append(metrics_dict[eval_metric]['scoring_function'](y_valid, y_pred_valid, X_valid['type']))\n\n        prediction += y_pred    \n        \n        if model_type == 'lgb' and plot_feature_importance:\n            # feature importance\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = columns\n            fold_importance[\"importance\"] = model.feature_importances_\n            fold_importance[\"fold\"] = fold_n + 1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n    prediction /= n_splits\n    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    \n    result_dict['oof'] = oof\n    result_dict['prediction'] = prediction\n    result_dict['scores'] = scores\n    \n    if model_type == 'lgb':\n        if plot_feature_importance:\n            feature_importance[\"importance\"] /= n_splits\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:50].index\n\n            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n            plt.figure(figsize=(16, 12));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGB Features (avg over folds)');\n            \n            result_dict['feature_importance'] = feature_importance\n        \n    return result_dict\n    \n\n\ndef train_model_classification(X, X_test, y, params, folds, model_type='lgb', eval_metric='auc', columns=None, plot_feature_importance=False, model=None,\n                               verbose=10000, early_stopping_rounds=200, n_estimators=50000, splits=None, n_folds=3, averaging='usual', n_jobs=-1):\n    \"\"\"\n    A function to train a variety of classification models.\n    Returns dictionary with oof predictions, test predictions, scores and, if necessary, feature importances.\n    \n    :params: X - training data, can be pd.DataFrame or np.ndarray (after normalizing)\n    :params: X_test - test data, can be pd.DataFrame or np.ndarray (after normalizing)\n    :params: y - target\n    :params: folds - folds to split data\n    :params: model_type - type of model to use\n    :params: eval_metric - metric to use\n    :params: columns - columns to use. If None - use all columns\n    :params: plot_feature_importance - whether to plot feature importance of LGB\n    :params: model - sklearn model, works only for \"sklearn\" model type\n    \n    \"\"\"\n    columns = X.columns if columns is None else columns\n    n_splits = folds.n_splits if splits is None else n_folds\n    X_test = X_test[columns]\n    \n    # to set up scoring parameters\n    metrics_dict = {'auc': {'lgb_metric_name': eval_auc,\n                        'catboost_metric_name': 'AUC',\n                        'sklearn_scoring_function': metrics.roc_auc_score},\n                    }\n    \n    result_dict = {}\n    if averaging == 'usual':\n        # out-of-fold predictions on train data\n        oof = np.zeros((len(X), 1))\n\n        # averaged predictions on train data\n        prediction = np.zeros((len(X_test), 1))\n        \n    elif averaging == 'rank':\n        # out-of-fold predictions on train data\n        oof = np.zeros((len(X), 1))\n\n        # averaged predictions on train data\n        prediction = np.zeros((len(X_test), 1))\n\n    \n    # list of scores on folds\n    scores = []\n    feature_importance = pd.DataFrame()\n    \n    # split and train on folds\n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n        print(f'Fold {fold_n + 1} started at {time.ctime()}')\n        if type(X) == np.ndarray:\n            X_train, X_valid = X[columns][train_index], X[columns][valid_index]\n            y_train, y_valid = y[train_index], y[valid_index]\n        else:\n            X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n            \n        if model_type == 'lgb':\n            model = lgb.LGBMClassifier(**params, n_estimators=n_estimators, n_jobs = n_jobs)\n            model.fit(X_train, y_train, \n                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric=metrics_dict[eval_metric]['lgb_metric_name'],\n                    verbose=verbose, early_stopping_rounds=early_stopping_rounds)\n            \n            y_pred_valid = model.predict_proba(X_valid)[:, 1]\n            y_pred = model.predict_proba(X_test, num_iteration=model.best_iteration_)[:, 1]\n            \n        if model_type == 'xgb':\n            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns)\n            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns)\n\n            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n            model = xgb.train(dtrain=train_data, num_boost_round=n_estimators, evals=watchlist, early_stopping_rounds=early_stopping_rounds, verbose_eval=verbose, params=params)\n            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n        \n        if model_type == 'sklearn':\n            model = model\n            model.fit(X_train, y_train)\n            \n            y_pred_valid = model.predict(X_valid).reshape(-1,)\n            score = metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid)\n            print(f'Fold {fold_n}. {eval_metric}: {score:.4f}.')\n            print('')\n            \n            y_pred = model.predict_proba(X_test)\n        \n        if model_type == 'cat':\n            model = CatBoostClassifier(iterations=n_estimators, eval_metric=metrics_dict[eval_metric]['catboost_metric_name'], **params,\n                                      loss_function=Logloss)\n            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n\n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test)\n        \n        if averaging == 'usual':\n            \n            oof[valid_index] = y_pred_valid.reshape(-1, 1)\n            scores.append(metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid))\n            \n            prediction += y_pred.reshape(-1, 1)\n\n        elif averaging == 'rank':\n                                  \n            oof[valid_index] = y_pred_valid.reshape(-1, 1)\n            scores.append(metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid))\n                                  \n            prediction += pd.Series(y_pred).rank().values.reshape(-1, 1)        \n        \n        if model_type == 'lgb' and plot_feature_importance:\n            # feature importance\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = columns\n            fold_importance[\"importance\"] = model.feature_importances_\n            fold_importance[\"fold\"] = fold_n + 1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n    prediction /= n_splits\n    \n    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    \n    result_dict['oof'] = oof\n    result_dict['prediction'] = prediction\n    result_dict['scores'] = scores\n    \n    if model_type == 'lgb':\n        if plot_feature_importance:\n            feature_importance[\"importance\"] /= n_splits\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:50].index\n\n            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n            plt.figure(figsize=(16, 12));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGB Features (avg over folds)');\n            \n            result_dict['feature_importance'] = feature_importance\n            result_dict['top_columns'] = cols\n        \n    return result_dict\n\n# setting up altair\nworkaround = prepare_altair()\nHTML(\"\".join((\n    \"<script>\",\n    workaround,\n    \"</script>\",\n)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# read data\ntrain_transaction = pd.read_csv('/kaggle/input/train_transaction.csv')\ntrain_identity = pd.read_csv('/kaggle/input/train_identity.csv')\ntest_transaction = pd.read_csv('/kaggle/input/test_transaction.csv')\ntest_identity = pd.read_csv('/kaggle/input/test_identity.csv')\n\ntrain = pd.merge(train_transaction, train_identity, on = 'TransactionID', how='left')\ntest = pd.merge(test_transaction, test_identity, on = 'TransactionID', how='left')\n\nprint(f'Train dataset shape : {train.shape[0]} * {train.shape[1]}')\nprint(f'Test dataset shape : {test.shape[0]} * {test.shape[1]}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering\n\n(I got a lot of help from [here](https://www.kaggle.com/artgor/eda-and-models).     ([artgor](https://www.kaggle.com/artgor)'s kernel))\n\nWe have 393 data per each case, and have about a quarter of datail human's data.\nIn transaction table, let's see one by one. \n\n[detail information in here](https://www.kaggle.com/c/ieee-fraud-detection/discussion/101203)\n* TransactionDT : timedelta from a given reference datetime (not an actual timestamp)\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# show TransactionDT data\n\nprint(train['TransactionDT'].head())\nprint(train['TransactionDT'].tail())\nprint(test['TransactionDT'].head())\nprint(test['TransactionDT'].tail())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"86400 is 24 * 3600. It's assumed to one day. I wonder how many days the total time is. "},{"metadata":{"trusted":true},"cell_type":"code","source":"print(((15811131 - 86400) + (34214345 - 18403224)) / 86400)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TransactionDT graph\n\nplt.hist(train['TransactionDT'], label='train');\nplt.hist(test['TransactionDT'], label='test');\nplt.legend();\nplt.title('Distribution of transactiond dates');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can assume it's one year. Maybe train data's end time will be 15811200 ( = 183\\* 86400), test data's start time will be 18403200 ( = 213 \\* 86400), test data's end time will be ( = 396 \\* 86400). I think that the empty time in the middle does not exist actually."},{"metadata":{"trusted":true},"cell_type":"code","source":"train['TransactionDT'] = train['TransactionDT'].apply(lambda x : x/86400)\ntest['TransactionDT'] = test['TransactionDT'].apply(lambda x : (x - 30 * 86400)/86400)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TransactionDT graph\n\nplt.hist(train['TransactionDT'], label='train');\nplt.hist(test['TransactionDT'], label='test');\nplt.legend();\nplt.title('Distribution of transactiond dates');\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TransactionDT to date\n\nimport datetime\nimport seaborn as sns\n\ngenesis = datetime.datetime.strptime('2018-01-01', '%Y-%m-%d')\ntrain['Date'] = train['TransactionDT'].apply(lambda x : genesis+datetime.timedelta(days=x))\n\ntrain['Month'] = train['Date'].dt.month\ntrain['Weekdays'] = train['Date'].dt.dayofweek\ntrain['Days'] = train['Date'].dt.day\ntrain['Hours'] = train['Date'].dt.hour\n\ntest['Date'] = test['TransactionDT'].apply(lambda x : genesis+datetime.timedelta(days=x))\n\ntest['Month'] = test['Date'].dt.month\ntest['Weekdays'] = test['Date'].dt.dayofweek\ntest['Days'] = test['Date'].dt.day\ntest['Hours'] = test['Date'].dt.hour\n\nfig, ax = plt.subplots(1, 4, figsize=(20, 5))\n\ng = sns.barplot(train.Month, train.isFraud, ax=ax[0])\nax[0].set_title('Fraud Charges by Month')\ng = sns.barplot(train.Weekdays, train.isFraud, ax=ax[1])\nax[1].set_title('Fraud Charges by Weekdays')\ng = sns.barplot(train.Days, train.isFraud, ax=ax[2])\nax[2].set_title('Fraud Charges by Days')\ng = sns.barplot(train.Hours, train.isFraud, ax=ax[3])\nax[3].set_title('Fraud Charges by Hours')\n\nplt.show()\n\nfig, ax = plt.subplots(1, 3, figsize=(15, 5))\n\ng = sns.countplot(train.Weekdays, ax=ax[0])\nax[0].set_title('Train Charges by Weekdays')\ng = sns.countplot(train.Days, ax=ax[1])\nax[1].set_title('Train Charges by Days')\ng = sns.countplot(train.Hours, ax=ax[2])\nax[2].set_title('Train Charges by Hours')\n\nplt.show()\n\nfig, ax = plt.subplots(1, 3, figsize=(15, 5))\n\ng = sns.countplot(test.Weekdays, ax=ax[0])\nax[0].set_title('Test Charges by Weekdays')\ng = sns.countplot(test.Days, ax=ax[1])\nax[1].set_title('Test Charges by Days')\ng = sns.countplot(test.Hours, ax=ax[2])\nax[2].set_title('Test Charges by Hours')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[Reference](https://www.kaggle.com/cybercat/naive-modeling-using-minimum-analysis)\n\nIn the first low, only \"Fraud Charges by Hours\" graph is meaningful. Furthermore 'Train Charges by Hours', 'Test Charges by Hours' this two graph is too similar. We can see from these 10 graphs that the rate of fraud is influenced hour and month, but train data is a half of year's data and test data is the rest of data, so the month cannot be considered. So, we will consider only hours."},{"metadata":{},"cell_type":"markdown","source":"* TransactionAMT : transaction payment amount in USD\n\nSome data has three decimal places. Furthermore, only that data's ProductCD is 'C', and addr1, addr2 is blank. I predict that data is International payment history, because this data doesn't have address data, and TransactionAmt has three decial places. It will be the original price multiplied by the exchange rate. "},{"metadata":{"trusted":true},"cell_type":"code","source":"train['decimal_three_places'] = train['TransactionAmt'].apply(lambda x : 1000*x%10 == 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.barplot(train.decimal_three_places, train.isFraud)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This graph means international payment is more likely to be a scam."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train['decimal_three_places'].groupby(train['ProductCD']).mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_rows', 20000)\ntrain['isFraud'].groupby(train['TransactionAmt']).mean().head(20000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* card (payment card information, such as card type, card category, issue bank, country, etc.)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = ['card1', 'card2', 'card3', 'card4', 'card5', 'card6']\nfor i in range(len(columns)):\n    print(columns[i]+' : '+str(train[columns[i]].isnull().sum())+'/'+str(train[columns[i]].count()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in columns:\n    print (\"Unique \",i, \" = \",train[i].nunique())\n    if train[i].nunique() <= 120 and isinstance(train[i][0], np.float64):\n        arr = train[i].unique()\n        arr = arr[~np.isnan(arr)]\n        arr.sort()\n        print(arr)\n    elif train[i].nunique() <= 120 and isinstance(train[i][0], str):\n        print(train[i].unique())\n        \nfor i in columns:\n    print (\"Unique \",i, \" = \",test[i].nunique())\n    if test[i].nunique() <= 120 and isinstance(test[i][0], np.float64):\n        arr = test[i].unique()\n        arr = arr[~np.isnan(arr)]\n        arr.sort()\n        print(arr)\n    elif test[i].nunique() <= 120 and isinstance(test[i][0], str):\n        print(test[i].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in columns:\n    \n    print('-' * 50)\n    print('# col : ', col)\n    #print(df[df.index < 590540]['isFraud'].groupby(df[col]).sum(),df[df.index < 590540]['isFraud'].groupby(df[col]).count())\n    print(100*train['isFraud'].groupby(train[col]).sum()/\n          train['isFraud'].groupby(train[col]).count(), 100 * train['isFraud'].groupby(train[col]).count()/train['isFraud'].count()) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(train['card3'])\nplt.show()\nsns.countplot(train['card4'])\nplt.show()\nsns.countplot(train['card5'])\nplt.show()\nsns.countplot(train['card6'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train['card1'].groupby(train['isFraud']).mean())\nprint(train['card2'].groupby(train['isFraud']).mean())\nprint(train['card3'].groupby(train['isFraud']).mean())\nprint(train['card5'].groupby(train['isFraud']).mean())\nprint(train['card1'].groupby(train['ProductCD']).mean())\nprint(train['card2'].groupby(train['ProductCD']).mean())\nprint(train['card3'].groupby(train['ProductCD']).mean())\nprint(train['card5'].groupby(train['ProductCD']).mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint(train['Hours'].groupby(train['card4']).mean())\nprint(train['Hours'].groupby(train['card6']).mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1, 4, figsize=(25,5))\n\nsns.countplot(x=\"card4\", ax=ax[0], data=train.loc[train['isFraud'] == 0])\nax[0].set_title('card4 isFraud=0', fontsize=14)\nsns.countplot(x=\"card4\", ax=ax[1], data=train.loc[train['isFraud'] == 1])\nax[1].set_title('card4 isFraud=1', fontsize=14)\nsns.countplot(x=\"card6\", ax=ax[2], data=train.loc[train['isFraud'] == 0])\nax[2].set_title('card6 isFraud=0', fontsize=14)\nsns.countplot(x=\"card6\", ax=ax[3], data=train.loc[train['isFraud'] == 1])\nax[3].set_title('card6 isFraud=1', fontsize=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train['TransactionAmt'].groupby(train['card1']).mean())\nprint(train['TransactionAmt'].groupby(train['card2']).mean())\nprint(train['TransactionAmt'].groupby(train['card3']).mean())\nprint(train['TransactionAmt'].groupby(train['card4']).mean())\nprint(train['TransactionAmt'].groupby(train['card5']).mean())\nprint(train['TransactionAmt'].groupby(train['card6']).mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* productCD"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['ProductCD'].unique()\nprint(train['isFraud'].groupby(train['ProductCD']).mean())\nsns.countplot(train['ProductCD'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['card3_is_150'] =  train['card3'].apply(lambda x : 1 if x == 150 else 0)\nprint(train['card3_is_150'].groupby(train['ProductCD']).mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_rows', 500)\nfilter = train['isFraud'].isin([1])\ntrain[train.columns[0:20]][filter].head(20000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* addr"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['isFraud'].groupby(train['addr1']).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['addr2'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analyse Correlation\n\nfor column in train.columns:\n    print(column, type(train[column][0]), type(column))\n    if isinstance(train[column][0], np.int64) or isinstance(train[column][0], np.float64):\n        data = train['isFraud'].corr(train[column])\n        if data > 0.3 or data < -0.3:\n            print(\"corr  :   \" + str(data))\n    else:\n        if column == \"DeviceInfo\" or column == \"Date\":\n            pass\n        else:\n            print(pd.crosstab(train['isFraud'], train[column]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* dist"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_rows',500)\nprint(train['dist1'].value_counts())\nprint(train['dist2'].value_counts())\nprint(len(train[train['dist1'].isnull()]))\nprint(len(train[train['dist2'].isnull()]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train[train['dist1'].notnull()]['ProductCD'].value_counts())\nprint(train[train['dist2'].notnull()]['ProductCD'].value_counts())\nprint(train[train['dist1'].notnull()]['card4'].value_counts())\nprint(train[train['dist2'].notnull()]['card4'].value_counts())\nprint(train[train['dist1'].notnull()]['card6'].value_counts())\nprint(train[train['dist2'].notnull()]['card6'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['TransactionAmt_to_mean_card1'] = train['TransactionAmt'] / train.groupby(['card1'])['TransactionAmt'].transform('mean')\ntrain['TransactionAmt_to_mean_card4'] = train['TransactionAmt'] / train.groupby(['card4'])['TransactionAmt'].transform('mean')\ntrain['TransactionAmt_to_std_card1'] = train['TransactionAmt'] / train.groupby(['card1'])['TransactionAmt'].transform('std')\ntrain['TransactionAmt_to_std_card4'] = train['TransactionAmt'] / train.groupby(['card4'])['TransactionAmt'].transform('std')\n\ntest['TransactionAmt_to_mean_card1'] = test['TransactionAmt'] / test.groupby(['card1'])['TransactionAmt'].transform('mean')\ntest['TransactionAmt_to_mean_card4'] = test['TransactionAmt'] / test.groupby(['card4'])['TransactionAmt'].transform('mean')\ntest['TransactionAmt_to_std_card1'] = test['TransactionAmt'] / test.groupby(['card1'])['TransactionAmt'].transform('std')\ntest['TransactionAmt_to_std_card4'] = test['TransactionAmt'] / test.groupby(['card4'])['TransactionAmt'].transform('std')\n\ntrain['id_02_to_mean_card1'] = train['id_02'] / train.groupby(['card1'])['id_02'].transform('mean')\ntrain['id_02_to_mean_card4'] = train['id_02'] / train.groupby(['card4'])['id_02'].transform('mean')\ntrain['id_02_to_std_card1'] = train['id_02'] / train.groupby(['card1'])['id_02'].transform('std')\ntrain['id_02_to_std_card4'] = train['id_02'] / train.groupby(['card4'])['id_02'].transform('std')\n\ntest['id_02_to_mean_card1'] = test['id_02'] / test.groupby(['card1'])['id_02'].transform('mean')\ntest['id_02_to_mean_card4'] = test['id_02'] / test.groupby(['card4'])['id_02'].transform('mean')\ntest['id_02_to_std_card1'] = test['id_02'] / test.groupby(['card1'])['id_02'].transform('std')\ntest['id_02_to_std_card4'] = test['id_02'] / test.groupby(['card4'])['id_02'].transform('std')\n\ntrain['D15_to_mean_card1'] = train['D15'] / train.groupby(['card1'])['D15'].transform('mean')\ntrain['D15_to_mean_card4'] = train['D15'] / train.groupby(['card4'])['D15'].transform('mean')\ntrain['D15_to_std_card1'] = train['D15'] / train.groupby(['card1'])['D15'].transform('std')\ntrain['D15_to_std_card4'] = train['D15'] / train.groupby(['card4'])['D15'].transform('std')\n\ntest['D15_to_mean_card1'] = test['D15'] / test.groupby(['card1'])['D15'].transform('mean')\ntest['D15_to_mean_card4'] = test['D15'] / test.groupby(['card4'])['D15'].transform('mean')\ntest['D15_to_std_card1'] = test['D15'] / test.groupby(['card1'])['D15'].transform('std')\ntest['D15_to_std_card4'] = test['D15'] / test.groupby(['card4'])['D15'].transform('std')\n\ntrain['D15_to_mean_addr1'] = train['D15'] / train.groupby(['addr1'])['D15'].transform('mean')\ntrain['D15_to_mean_addr2'] = train['D15'] / train.groupby(['addr2'])['D15'].transform('mean')\ntrain['D15_to_std_addr1'] = train['D15'] / train.groupby(['addr1'])['D15'].transform('std')\ntrain['D15_to_std_addr2'] = train['D15'] / train.groupby(['addr2'])['D15'].transform('std')\n\ntest['D15_to_mean_addr1'] = test['D15'] / test.groupby(['addr1'])['D15'].transform('mean')\ntest['D15_to_mean_addr2'] = test['D15'] / test.groupby(['addr2'])['D15'].transform('mean')\ntest['D15_to_std_addr1'] = test['D15'] / test.groupby(['addr1'])['D15'].transform('std')\ntest['D15_to_std_addr2'] = test['D15'] / test.groupby(['addr2'])['D15'].transform('std')\n\ntrain[['P_emaildomain_1', 'P_emaildomain_2', 'P_emaildomain_3']] = train['P_emaildomain'].str.split('.', expand=True)\ntrain[['R_emaildomain_1', 'R_emaildomain_2', 'R_emaildomain_3']] = train['R_emaildomain'].str.split('.', expand=True)\ntest[['P_emaildomain_1', 'P_emaildomain_2', 'P_emaildomain_3']] = test['P_emaildomain'].str.split('.', expand=True)\ntest[['R_emaildomain_1', 'R_emaildomain_2', 'R_emaildomain_3']] = test['R_emaildomain'].str.split('.', expand=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"many_null_cols = [col for col in train.columns if train[col].isnull().sum() / train.shape[0] > 0.9]\nmany_null_cols_test = [col for col in test.columns if test[col].isnull().sum() / test.shape[0] > 0.9]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"big_top_value_cols = [col for col in train.columns if train[col].value_counts(dropna=False, normalize=True).values[0] > 0.9]\nbig_top_value_cols_test = [col for col in test.columns if test[col].value_counts(dropna=False, normalize=True).values[0] > 0.9]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"one_value_cols = [col for col in train.columns if train[col].nunique() <= 1]\none_value_cols_test = [col for col in test.columns if test[col].nunique() <= 1]\none_value_cols == one_value_cols_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_to_drop = list(set(many_null_cols + many_null_cols_test + big_top_value_cols + big_top_value_cols_test + one_value_cols+ one_value_cols_test))\ncols_to_drop.remove('isFraud')\nlen(cols_to_drop)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\ncat_cols = ['id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18', 'id_19', 'id_20', 'id_21', 'id_22', 'id_23', 'id_24', 'id_25', 'id_26', 'id_27', 'id_28', 'id_29',\n            'id_30', 'id_31', 'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38', 'DeviceType', 'DeviceInfo', 'ProductCD', 'card4', 'card6', 'M4','P_emaildomain',\n            'R_emaildomain', 'card1', 'card2', 'card3',  'card5', 'addr1', 'addr2', 'M1', 'M2', 'M3', 'M5', 'M6', 'M7', 'M8', 'M9',\n            'P_emaildomain_1', 'P_emaildomain_2', 'P_emaildomain_3', 'R_emaildomain_1', 'R_emaildomain_2', 'R_emaildomain_3']\nfor col in cat_cols:\n    if col in train.columns:\n        le = LabelEncoder()\n        le.fit(list(train[col].astype(str).values) + list(test[col].astype(str).values))\n        train[col] = le.transform(list(train[col].astype(str).values))\n        test[col] = le.transform(list(test[col].astype(str).values))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.columns)\nX = train.sort_values('TransactionDT').drop(['isFraud', 'TransactionDT', 'TransactionID'], axis=1)\nY = train.sort_values('TransactionDT')['isFraud']\n\nX_test = test.drop(['TransactionDT', 'TransactionID'], axis=1)\ndel train\ntest = test[['TransactionDT', 'TransactionID']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# by https://www.kaggle.com/dimartinot\ndef clean_inf_nan(df):\n    return df.replace([np.inf, -np.inf], np.nan)   \n\n# Cleaning infinite values to NaN\nX = clean_inf_nan(X)\nX_test = clean_inf_nan(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_fold = 5\nfolds = TimeSeriesSplit(n_splits=n_fold)\nfolds = KFold(n_splits=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'num_leaves': 256,\n          'min_child_samples': 79,\n          'objective': 'binary',\n          'max_depth': 3,\n          'learning_rate': 0.03,\n          \"boosting_type\": \"gbdt\",\n          \"subsample_freq\": 3,\n          \"subsample\": 0.9,\n          \"bagging_seed\": 11,\n          \"metric\": 'auc',\n          \"verbosity\": -1,\n          'reg_alpha': 0.3,\n          'reg_lambda': 0.3,\n          'colsample_bytree': 0.9,\n          #'categorical_feature': cat_cols\n         }\nresult_dict_lgb = train_model_classification(X=X, X_test=X_test, y=y, params=params, folds=folds, model_type='lgb', eval_metric='auc', plot_feature_importance=True,\n                                                      verbose=500, early_stopping_rounds=200, n_estimators=5000, averaging='usual', n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub['isFraud'] = result_dict_lgb['prediction']\nsub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}