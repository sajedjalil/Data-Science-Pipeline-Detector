{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"print(wrong)\nimport os\nimport time\nos.system('pip install catboost==0.15.2')\nos.system('pip install xlrd')\nos.system('pip install lightgbm')\nos.system('pip install tqdm')\nos.system('pip install gensim')\n\nimport pandas as pd\nimport numpy as np\n\nfrom tqdm import tqdm_notebook\nfrom sklearn.metrics import roc_auc_score\nimport gc\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom tqdm import tqdm\n\nimport datetime\ndef tiem_deta_date(data,new_data_col):\n    START_DATE = '2017-12-01'\n    startdate = datetime.datetime.strptime(START_DATE, '%Y-%m-%d')\n    data[new_data_col] = startdate + data['TransactionDT'].map(lambda x:datetime.timedelta(seconds = x))\n    return data\n    \ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df\n\n\nimport os\nos.chdir('/cos_person/IEEE/data/')\n\n\n\n\n\nfolder_path = '../data/'\n\nprint('Loading data...')\n\ntrain_identity = pd.read_csv('{}train_identity.csv'.format(folder_path))\nprint('\\tSuccessfully loaded train_identity!')\n\ntrain_transaction = pd.read_csv('{}train_transaction.csv'.format(folder_path))\nprint('\\tSuccessfully loaded train_transaction!')\n\ntest_identity = pd.read_csv('{}test_identity.csv'.format(folder_path))\nprint('\\tSuccessfully loaded test_identity!')\n\ntest_transaction = pd.read_csv('{}test_transaction.csv'.format(folder_path))\nprint('\\tSuccessfully loaded test_transaction!')\n\nsub = pd.read_csv('{}sample_submission.csv'.format(folder_path))\nprint('\\tSuccessfully loaded sample_submission!')\n\nprint('Data was successfully loaded!\\n')    \n\n\n\ndef id_split(dataframe):\n    dataframe['device_name'] = dataframe['DeviceInfo'].str.split('/', expand=True)[0]\n    dataframe['device_version'] = dataframe['DeviceInfo'].str.split('/', expand=True)[1]\n\n    dataframe['OS_id_30'] = dataframe['id_30'].str.split(' ', expand=True)[0]\n    dataframe['version_id_30'] = dataframe['id_30'].str.split(' ', expand=True)[1]\n\n    dataframe['browser_id_31'] = dataframe['id_31'].str.split(' ', expand=True)[0]\n    dataframe['version_id_31'] = dataframe['id_31'].str.split(' ', expand=True)[1]\n\n    dataframe['screen_width'] = dataframe['id_33'].str.split('x', expand=True)[0]\n    dataframe['screen_height'] = dataframe['id_33'].str.split('x', expand=True)[1]\n\n    dataframe['id_34'] = dataframe['id_34'].str.split(':', expand=True)[1]\n    dataframe['id_23'] = dataframe['id_23'].str.split(':', expand=True)[1]\n\n    dataframe.loc[dataframe['device_name'].str.contains('SM', na=False), 'device_name'] = 'Samsung'\n    dataframe.loc[dataframe['device_name'].str.contains('SAMSUNG', na=False), 'device_name'] = 'Samsung'\n    dataframe.loc[dataframe['device_name'].str.contains('GT-', na=False), 'device_name'] = 'Samsung'\n    dataframe.loc[dataframe['device_name'].str.contains('Moto G', na=False), 'device_name'] = 'Motorola'\n    dataframe.loc[dataframe['device_name'].str.contains('Moto', na=False), 'device_name'] = 'Motorola'\n    dataframe.loc[dataframe['device_name'].str.contains('moto', na=False), 'device_name'] = 'Motorola'\n    dataframe.loc[dataframe['device_name'].str.contains('LG-', na=False), 'device_name'] = 'LG'\n    dataframe.loc[dataframe['device_name'].str.contains('rv:', na=False), 'device_name'] = 'RV'\n    dataframe.loc[dataframe['device_name'].str.contains('HUAWEI', na=False), 'device_name'] = 'Huawei'\n    dataframe.loc[dataframe['device_name'].str.contains('ALE-', na=False), 'device_name'] = 'Huawei'\n    dataframe.loc[dataframe['device_name'].str.contains('-L', na=False), 'device_name'] = 'Huawei'\n    dataframe.loc[dataframe['device_name'].str.contains('Blade', na=False), 'device_name'] = 'ZTE'\n    dataframe.loc[dataframe['device_name'].str.contains('BLADE', na=False), 'device_name'] = 'ZTE'\n    dataframe.loc[dataframe['device_name'].str.contains('Linux', na=False), 'device_name'] = 'Linux'\n    dataframe.loc[dataframe['device_name'].str.contains('XT', na=False), 'device_name'] = 'Sony'\n    dataframe.loc[dataframe['device_name'].str.contains('HTC', na=False), 'device_name'] = 'HTC'\n    dataframe.loc[dataframe['device_name'].str.contains('ASUS', na=False), 'device_name'] = 'Asus'\n\n    dataframe.loc[dataframe.device_name.isin(dataframe.device_name.value_counts()[dataframe.device_name.value_counts() < 200].index), 'device_name'] = \"Others\"\n    dataframe['had_id'] = 1\n    gc.collect()\n    \n    return dataframe\n\n\ntrain_identity = id_split(train_identity)\ntest_identity = id_split(test_identity)\n# had_idã€\nsome_feat_ = []\nfor i in range(4):\n    for df_data in [train_identity,test_identity]:\n        df_data['id_31_back_' + str(i)] = df_data['id_31'].str.split(' ').str[-(i + 1)]\n        some_feat_.append('id_31_back_' + str(i))\n# some_feat_ = list(set(some_feat_))\n\nprint('Merging data...')\ntrain = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')\ntest = pd.merge(test_transaction, test_identity, on='TransactionID', how='left')\n\nprint('Data was successfully merged!\\n')\n\n# del train_identity, train_transaction, test_identity, test_transaction\nprint('Train dataset has {} rows and {} columns.'.format(train.shape[0],train.shape[1]))\nprint('Test dataset has {} rows and {} columns.\\n'.format(test.shape[0],test.shape[1]))\n\n\ngc.collect()\n\n\n###########public kernels************************ \n\n# 'V_NULL','V_NULL_count',\nuseful_features = ['TransactionAmt', 'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'addr1', 'addr2', 'dist1',\n                   'P_emaildomain', 'R_emaildomain', 'C1', 'C2','C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13',\n                   'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D8', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'M2', 'M3',\n                   'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V17',\n                   'V19', 'V20', 'V29', 'V30', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V40', 'V44', 'V45', 'V46', 'V47', 'V48',\n                   'V49', 'V51', 'V52', 'V53', 'V54', 'V56', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64', 'V69', 'V70', 'V71',\n                   'V72', 'V73', 'V74', 'V75', 'V76', 'V78', 'V80', 'V81', 'V82', 'V83', 'V84', 'V85', 'V87', 'V90', 'V91', 'V92',\n                   'V93', 'V94', 'V95', 'V96', 'V97', 'V99', 'V100', 'V126', 'V127', 'V128', 'V130', 'V131', 'V138', 'V139', 'V140',\n                   'V143', 'V145', 'V146', 'V147', 'V149', 'V150', 'V151', 'V152', 'V154', 'V156', 'V158', 'V159', 'V160', 'V161',\n                   'V162', 'V163', 'V164', 'V165', 'V166', 'V167', 'V169', 'V170', 'V171', 'V172', 'V173', 'V175', 'V176', 'V177',\n                   'V178', 'V180', 'V182', 'V184', 'V187', 'V188', 'V189', 'V195', 'V197', 'V200', 'V201', 'V202', 'V203', 'V204',\n                   'V205', 'V206', 'V207', 'V208', 'V209', 'V210', 'V212', 'V213', 'V214', 'V215', 'V216', 'V217', 'V219', 'V220',\n                   'V221', 'V222', 'V223', 'V224', 'V225', 'V226', 'V227', 'V228', 'V229', 'V231', 'V233', 'V234', 'V238', 'V239',\n                   'V242', 'V243', 'V244', 'V245', 'V246', 'V247', 'V249', 'V251', 'V253', 'V256', 'V257', 'V258', 'V259', 'V261',\n                   'V262', 'V263', 'V264', 'V265', 'V266', 'V267', 'V268', 'V270', 'V271', 'V272', 'V273', 'V274', 'V275', 'V276',\n                   'V277', 'V278', 'V279', 'V280', 'V282', 'V283', 'V285', 'V287', 'V288', 'V289', 'V291', 'V292', 'V294', 'V303',\n                   'V304', 'V306', 'V307', 'V308', 'V310', 'V312', 'V313', 'V314', 'V315', 'V317', 'V322', 'V323', 'V324', 'V326',\n                   'V329', 'V331', 'V332', 'V333', 'V335', 'V336', 'V338', 'id_01', 'id_02', 'id_03', 'id_05', 'id_06', 'id_09',\n                   'id_11', 'id_12', 'id_13', 'id_14', 'id_15', 'id_17', 'id_19', 'id_20', 'id_30', 'id_31', 'id_32', 'id_33',\n                   'id_36', 'id_37', 'id_38', 'DeviceType', 'DeviceInfo', 'device_name', 'device_version', 'OS_id_30', 'version_id_30',\n                   'browser_id_31', 'version_id_31', 'screen_width', 'screen_height', 'had_id']\n\n\n\ncols_to_drop = [col for col in train.columns if col not in useful_features]\ncols_to_drop.remove('isFraud')\ncols_to_drop.remove('TransactionDT')\ncols_to_drop.remove('TransactionID')\n\n\n\ntrain = train.drop(cols_to_drop, axis=1)\ntest = test.drop(cols_to_drop, axis=1)\n\ni_cols = [c for c in train.columns if c.startswith('M')]\nfor df in [train, test]:\n    df['M_sum'] = df[i_cols].sum(axis=1).astype(np.int8)\n    df['M_na'] = df[i_cols].isna().sum(axis=1).astype(np.int8)\n\n\na = np.zeros(train.shape[0])\ntrain[\"lastest_browser\"] = a\na = np.zeros(test.shape[0])\ntest[\"lastest_browser\"] = a\ndef setbrowser(df):\n    df.loc[df[\"id_31\"]==\"samsung browser 7.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"opera 53.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"mobile safari 10.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"google search application 49.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"firefox 60.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"edge 17.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 69.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 67.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 63.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 63.0 for ios\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 64.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 64.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 64.0 for ios\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 65.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 65.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 65.0 for ios\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 66.0\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 66.0 for android\",'lastest_browser']=1\n    df.loc[df[\"id_31\"]==\"chrome 66.0 for ios\",'lastest_browser']=1\n    return df\ntrain=setbrowser(train)\ntest=setbrowser(test)                               \n\n\n\n\ntrain['TransactionAmt_to_mean_card1'] = train['TransactionAmt'] / train.groupby(['card1'])['TransactionAmt'].transform('mean')\ntrain['TransactionAmt_to_mean_card4'] = train['TransactionAmt'] / train.groupby(['card4'])['TransactionAmt'].transform('mean')\ntrain['TransactionAmt_to_std_card1'] = train['TransactionAmt'] / train.groupby(['card1'])['TransactionAmt'].transform('std')\ntrain['TransactionAmt_to_std_card4'] = train['TransactionAmt'] / train.groupby(['card4'])['TransactionAmt'].transform('std')\n\ntest['TransactionAmt_to_mean_card1'] = test['TransactionAmt'] / test.groupby(['card1'])['TransactionAmt'].transform('mean')\ntest['TransactionAmt_to_mean_card4'] = test['TransactionAmt'] / test.groupby(['card4'])['TransactionAmt'].transform('mean')\ntest['TransactionAmt_to_std_card1'] = test['TransactionAmt'] / test.groupby(['card1'])['TransactionAmt'].transform('std')\ntest['TransactionAmt_to_std_card4'] = test['TransactionAmt'] / test.groupby(['card4'])['TransactionAmt'].transform('std')\n\ntrain['id_02_to_mean_card1'] = train['id_02'] / train.groupby(['card1'])['id_02'].transform('mean')\ntrain['id_02_to_mean_card4'] = train['id_02'] / train.groupby(['card4'])['id_02'].transform('mean')\ntrain['id_02_to_std_card1'] = train['id_02'] / train.groupby(['card1'])['id_02'].transform('std')\ntrain['id_02_to_std_card4'] = train['id_02'] / train.groupby(['card4'])['id_02'].transform('std')\n\ntest['id_02_to_mean_card1'] = test['id_02'] / test.groupby(['card1'])['id_02'].transform('mean')\ntest['id_02_to_mean_card4'] = test['id_02'] / test.groupby(['card4'])['id_02'].transform('mean')\ntest['id_02_to_std_card1'] = test['id_02'] / test.groupby(['card1'])['id_02'].transform('std')\ntest['id_02_to_std_card4'] = test['id_02'] / test.groupby(['card4'])['id_02'].transform('std')\n\ntrain['D15_to_mean_card1'] = train['D15'] / train.groupby(['card1'])['D15'].transform('mean')\ntrain['D15_to_mean_card4'] = train['D15'] / train.groupby(['card4'])['D15'].transform('mean')\ntrain['D15_to_std_card1'] = train['D15'] / train.groupby(['card1'])['D15'].transform('std')\ntrain['D15_to_std_card4'] = train['D15'] / train.groupby(['card4'])['D15'].transform('std')\n\ntest['D15_to_mean_card1'] = test['D15'] / test.groupby(['card1'])['D15'].transform('mean')\ntest['D15_to_mean_card4'] = test['D15'] / test.groupby(['card4'])['D15'].transform('mean')\ntest['D15_to_std_card1'] = test['D15'] / test.groupby(['card1'])['D15'].transform('std')\ntest['D15_to_std_card4'] = test['D15'] / test.groupby(['card4'])['D15'].transform('std')\n\ntrain['D15_to_mean_addr1'] = train['D15'] / train.groupby(['addr1'])['D15'].transform('mean')\ntrain['D15_to_mean_card4'] = train['D15'] / train.groupby(['card4'])['D15'].transform('mean')\ntrain['D15_to_std_addr1'] = train['D15'] / train.groupby(['addr1'])['D15'].transform('std')\ntrain['D15_to_std_card4'] = train['D15'] / train.groupby(['card4'])['D15'].transform('std')\n\ntest['D15_to_mean_addr1'] = test['D15'] / test.groupby(['addr1'])['D15'].transform('mean')\ntest['D15_to_mean_card4'] = test['D15'] / test.groupby(['card4'])['D15'].transform('mean')\ntest['D15_to_std_addr1'] = test['D15'] / test.groupby(['addr1'])['D15'].transform('std')\ntest['D15_to_std_card4'] = test['D15'] / test.groupby(['card4'])['D15'].transform('std')\n\n\n\n\n# New feature - log of transaction amount. ()\ntrain['TransactionAmt_Log'] = np.log(train['TransactionAmt'])\ntest['TransactionAmt_Log'] = np.log(test['TransactionAmt'])\n\n# New feature - decimal part of the transaction amount.\ntrain['TransactionAmt_decimal'] = ((train['TransactionAmt'] - train['TransactionAmt'].astype(int)) * 1000).astype(int)\ntest['TransactionAmt_decimal'] = ((test['TransactionAmt'] - test['TransactionAmt'].astype(int)) * 1000).astype(int)\n\n# New feature - day of week in which a transaction happened.\ntrain['Transaction_day_of_week'] = np.floor((train['TransactionDT'] / (3600 * 24) - 1) % 7)\ntest['Transaction_day_of_week'] = np.floor((test['TransactionDT'] / (3600 * 24) - 1) % 7)\n\n# New feature - hour of the day in which a transaction happened.\ntrain['Transaction_hour'] = np.floor(train['TransactionDT'] / 3600) % 24\ntest['Transaction_hour'] = np.floor(test['TransactionDT'] / 3600) % 24\n\nfor feature in ['id_02__id_20', 'id_02__D8', 'D11__DeviceInfo', 'DeviceInfo__P_emaildomain', 'P_emaildomain__C2', \n                'card2__dist1', 'card1__card5', 'card2__id_20', 'card5__P_emaildomain', 'addr1__card1']:\n\n    f1, f2 = feature.split('__')\n    train[feature] = train[f1].astype(str) + '_' + train[f2].astype(str)\n    test[feature] = test[f1].astype(str) + '_' + test[f2].astype(str)\n\n    le = LabelEncoder()\n    le.fit(list(train[feature].astype(str).values) + list(test[feature].astype(str).values))\n    train[feature] = le.transform(list(train[feature].astype(str).values))\n    test[feature] = le.transform(list(test[feature].astype(str).values))\n\n\n\nsep_count = ['id_01', 'id_31', 'id_33', 'id_36']\ncateCOLS = ['card1', 'card2', 'card3', 'card4', 'card5', 'card6','id_30','id_31','addr1','addr2','P_emaildomain','R_emaildomain','ProductCD','DeviceType', 'DeviceInfo', 'device_name', 'device_version']\n_count_full = [c for c in cateCOLS if c not in sep_count]\n# for feature in ['card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'id_36','addr1','addr2']:\n# testing adding more full count\nfor feature in _count_full:    \n    train[feature + '_count_full'] = train[feature].map(pd.concat([train[feature], test[feature]], ignore_index=True).value_counts(dropna=False))\n    test[feature + '_count_full'] = test[feature].map(pd.concat([train[feature], test[feature]], ignore_index=True).value_counts(dropna=False))\n\n# Encoding - count encoding separately for train and test\nfor feature in ['id_01', 'id_31', 'id_33', 'id_36']:\n    train[feature + '_count_dist'] = train[feature].map(train[feature].value_counts(dropna=False))\n    test[feature + '_count_dist'] = test[feature].map(test[feature].value_counts(dropna=False))\n\n###########public kernels************************\n\n\n\n\ndf_data = pd.concat([train,test])\n\ndef agg_mode(dataDetails, gpID, modeCol, modeReCol ):\n    df = dataDetails.groupby( [gpID, modeCol] )[ gpID ].agg({ 'cnt':'count' }).reset_index()\n    df = df.sort_values( 'cnt' )\n    df = df.groupby( gpID ).tail(1)\n    df.drop( 'cnt',axis = 1 ,inplace = True)\n    df.columns = [ gpID,modeReCol]\n    return df\n    \n#############boost 0.5k feats   fill card2~5 by mod of card1\nfor car in ['card{}'.format(i) for i in range(2,7)]:\n    gp = agg_mode(df_data,'card1',car,'tmpcard')\n    gp = gp.groupby(['card1'])['tmpcard'].first()\n    df_data['tmpcard'] = df_data['card1'].map(gp)\n    df_data[car] = df_data[car].fillna(df_data['tmpcard'])\n    del df_data['tmpcard']\n    \n\ntrain = df_data[df_data.isFraud.isnull()==False]\ntest = df_data[df_data.isFraud.isnull()==True]\ndel df_data\n\n\n\nemails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', 'scranton.edu': 'other', 'optonline.net': 'other', 'hotmail.co.uk': 'microsoft', 'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo', 'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', 'aim.com': 'aol', 'hotmail.de': 'microsoft', 'centurylink.net': 'centurylink', 'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', 'gmx.de': 'other', 'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', 'protonmail.com': 'other', 'hotmail.fr': 'microsoft', 'windstream.net': 'other', 'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo', 'yahoo.de': 'yahoo', 'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other', 'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft', 'verizon.net': 'yahoo', 'msn.com': 'microsoft', 'q.com': 'centurylink', 'prodigy.net.mx': 'att', 'frontier.com': 'yahoo', 'anonymous.com': 'other', 'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo', 'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', 'bellsouth.net': 'other', 'embarqmail.com': 'centurylink', 'cableone.net': 'other', 'hotmail.es': 'microsoft', 'mac.com': 'apple', 'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', 'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other', 'cox.net': 'other', 'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\nus_emails = ['gmail', 'net', 'edu']\n\nfor c in ['P_emaildomain', 'R_emaildomain']:\n    train[c + '_bin'] = train[c].map(emails)\n    test[c + '_bin'] = test[c].map(emails)\n    \n    train[c + '_suffix'] = train[c].map(lambda x: str(x).split('.')[-1])\n    test[c + '_suffix'] = test[c].map(lambda x: str(x).split('.')[-1])\n    \n    train[c + '_suffix'] = train[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n    test[c + '_suffix'] = test[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')    \n\n\n#############boost 1k+ feats  making uid\ntrain['uid'] = train['card1'].astype(str)+'_'+train['card2'].astype(str)+'_'+train['card3'].astype(str)+'_'+train['card4'].astype(str)\ntest['uid'] = test['card1'].astype(str)+'_'+test['card2'].astype(str)+'_'+test['card3'].astype(str)+'_'+test['card4'].astype(str)\n\ntrain['uid2'] = train['uid'].astype(str)+'_'+train['addr1'].astype(str)+'_'+train['addr2'].astype(str)\ntest['uid2'] = test['uid'].astype(str)+'_'+test['addr1'].astype(str)+'_'+test['addr2'].astype(str)\n\ntrain['uid3'] = train['card1'].astype(str)+'_'+train['addr1'].astype(str)\ntest['uid3'] = test['card1'].astype(str)+'_'+test['addr1'].astype(str)\n\n\n\n# summary on total data \n#########################################################################\ndf_data = pd.concat([train,test])\ncate_cols = ['ProductCD','card4','card6']\nfor col in cate_cols:\n    agg_data = train.groupby([col])['isFraud'].agg(['mean','sum']).reset_index()\n    agg_data.columns = [col,col+'_label_mean',col+'_label_sum']\n    df_data = pd.merge(df_data,agg_data,'left',col)\n    \n\ndf_data = tiem_deta_date(df_data,'Transaction_dt')\ndf_data['TransDateStr'] = df_data['Transaction_dt'].map(lambda x:str(x)[:10])\nalldays = sorted(df_data['TransDateStr'].unique().tolist())\nallDay_dic = dict( zip( alldays,range( len(alldays) ) ) )\ndf_data['TransDateStr'] = df_data['TransDateStr'].map(allDay_dic )\ndf_data['TransDateStr'] = df_data['TransDateStr']//120\n\n\n# 0.5k rank of count by 120days\ncards = ['card1','card2','card3','card5']\nfor c in cards:\n    col = '{}_120days'.format(c)\n    print('adding card ranking col {}'.format(col))\n    df_data['{}_120days'.format(c)] = df_data[c].map(str) + '_days_'+df_data['TransDateStr'].map(str)\n    df_data[col+'_rank_count'] = df_data[col].map( df_data[col].value_counts() )\n    df_data[col+'_rank_count'] = df_data.groupby( [ 'TransDateStr'] )[ col+'_rank_count' ].rank( ascending = False)\n    del df_data[col]\n\n\n\n\n#############boost 2k+ feats  making uid\n#**************so called magic here*********\n## *** features :first by make uid like V307 which is cumsum of user payment\n# than do some fe base on it\n# Magic one \ndef _V307_series(AMT_V307):\n    AMT = AMT_V307[0]\n    V307 = AMT_V307[1]   \n    a =  pd.Series(V307)\n    zero_index = np.where(a == 0)[0]\n    _len = len(AMT)\n    series = np.zeros(_len)\n    if _len==1:\n        return series\n    flag = 0 \n    for idx in zero_index:\n        flag += 1\n        series[idx] = flag\n        cursor = AMT[idx ]\n        for j in range( idx+1,_len ):\n            if abs(V307[j]-cursor)<=0.01:\n                cursor = AMT[j] + cursor\n                series[j] = flag\n    return series\ndef _join(AMT,V307):\n    result = [AMT,V307]\n    return result\n\n\n\n\n\n\ncards = ['card1','card2','card3','card4','card5','card6','addr1']\ncards =['card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'addr1']\ndf_data['cards'] = ''\nfor ca in cards:\n    df_data['cards'] = df_data['cards'] + '-' + df_data[ca].map(str)\n    \ndf_data = df_data.sort_values(['card1','TransactionDT']).reset_index(drop=True)\n# 2 k\nD2    = df_data.groupby(['card1']).apply(lambda x: _join( x['TransactionAmt'].values.tolist(),x['V307'].values.tolist() )).reset_index()\nD2.columns = ['cards','joinlists']\nD2['V307_series'] = D2['joinlists'].map(_V307_series)\nD2_series = []\nfor val in tqdm(D2['V307_series'].values):\n    D2_series.extend(val)\ndf_data['V307_series'] = D2_series\n\ndf_data['card1_V307_series'] =df_data['card1'].map(str) +'_V307_'+ df_data['V307_series'].map(str)\n\ndf_data['cards_V307_series'] =df_data['cards'].map(str) +'_V307_'+ df_data['V307_series'].map(str)\n\ndf_data['card1_addr1_V307_series'] =df_data['card1'].map(str) +df_data['addr1'].map(str) +'_V307_'+ df_data['V307_series'].map(str)\n\ndel df_data['V307_series']\n\nfrom tqdm import tqdm\n\n\n#############boost 2k+ feats  making uid\n#**************so called magic here*********\n## *** features : first by make uid like D1/D2/D4  which is reference day for\n# some events ,thanks to @tuttifrutti; https://www.kaggle.com/tuttifrutti/creating-features-from-d-columns-guessing-userid\n# Magic two\n\n\ncards = ['card1','card2','card3','card4','card5','card6','addr1']\ndf_data['cards'] = ''\nfor ca in cards:\n    df_data['cards'] = df_data['cards'] + '-' + df_data[ca].map(str)\n\n\ndf_data['DaysFromStart'] = np.round(df_data['TransactionDT'] / (60 * 60 * 24), 0)\n\n\n\ncards = ['card1','card2','card3','card4','card5','card6','addr1']\nDids = []\nfor d in ['D1','D4','D15']:\n    df_data['{}_resDay'.format(d)] = df_data['DaysFromStart'] - df_data[d]\n    df_data['card1_{}_resDay'.format(d)] =df_data['card1'].map(str) +'_V307_'+ df_data['{}_resDay'.format(d)].map(str)\n    df_data['cards_{}_resDay'.format(d)] =df_data['cards'].map(str) +'_V307_'+ df_data['{}_resDay'.format(d)].map(str)\n    # df_data['card1_P_emaildomain_{}_resDay'.format(d)] =df_data['card1'].map(str) +df_data['P_emaildomain'].map(str) +'_V307_'+ df_data['{}_resDay'.format(d)].map(str)\n    df_data['card1_addr1_{}_resDay'.format(d)] =df_data['card1'].map(str) +df_data['addr1'].map(str) +'_V307_'+ df_data['{}_resDay'.format(d)].map(str)\n    # df_data['card1_addr1_P_emaildomain_{}_resDay'.format(d)] =df_data['card1'].map(str) +df_data['addr1'].map(str) +df_data['P_emaildomain'].map(str) +'_V307_'+ df_data['{}_resDay'.format(d)].map(str)\n    #,'card1_P_emaildomain_{}_resDay'.format(d),'card1_addr1_P_emaildomain_{}_resDay'.format(d) \n    del df_data['{}_resDay'.format(d)]\n    _add = ['card1_{}_resDay'.format(d),'cards_{}_resDay'.format(d),'card1_addr1_{}_resDay'.format(d)]\n    Dids.extend(_add)\n\n\n\ndel df_data['DaysFromStart']\ndel df_data['cards']\n# del df_data['D1_resDay']\n# del df_data['D4_resDay']\n# del df_data['D3_cumsum']\n\n#*********************************\n# add make uid\n#*********************************\n\n\n\n\n\nlist_count = []\ndef Count_engineering(all_data, name):\n\n    all_name = 'count_' + name\n    all_data[all_name] = all_data.groupby([name])[name].transform('count')\n    list_count.append(all_name)\n\n    return all_data\n\nx_list = ['TransactionAmt']\n\nfor name in tqdm(x_list):\n    df_data = Count_engineering(df_data, name)\n\nlist_num = []\ndef Num_engineering(all_data, x_name, y_name):\n\n    x_name = [x_name]\n\n    all_name = ''\n\n    for name in x_name:\n        \n        if all_data[name].nunique() >= all_data[y_name].nunique():\n            return all_data\n        \n        all_name += (name + '_')\n\n    all_name += ('num_' + y_name)\n    if y_name=='D1':\n        all_data[all_name + '_mean'] = all_data.groupby(x_name)[y_name].transform('mean')\n    else:\n        all_data[all_name + '_max'] = all_data.groupby(x_name)[y_name].transform('max')\n        all_data[all_name + '_min'] = all_data.groupby(x_name)[y_name].transform('min')\n        all_data[all_name + '_mean'] = all_data.groupby(x_name)[y_name].transform('mean')\n        all_data[all_name + '_std'] = all_data.groupby(x_name)[y_name].transform('std')\n\n    list_num.append(all_name)\n    \n    return all_data\n\n\n# 'card1__D8', 'card1__id_02', 'addr1__D8', 'addr1__id_02', 'card1__dist1', 'card1__dist2', 'card1__V332', 'card1__V323'\nxy_list = ['card1__TransactionAmt', 'card1__D8',\n'uid__TransactionAmt','uid2__TransactionAmt','uid3__TransactionAmt',\n'card2__TransactionAmt','card3__TransactionAmt','card5__TransactionAmt', 'card1__D8',\n'card3__D8','addr1__id_02', 'card1__dist1', 'card1__dist2','card1__V332', 'addr1__id_02','card1__V99', 'addr1__V99',\n# 'make_uid_first__TransactionAmt','make_uid_first__dist1',\n'cards_D1_resDay__TransactionAmt',\n# 'cards_D2_resDay__TransactionAmt',\n'cards_D4_resDay__TransactionAmt',\n# 'card1_V307_series__TransactionAmt',\n'cards_V307_series__TransactionAmt',\n# 'card1_addr1_V307_series__TransactionAmt',\n'card1__D1','card2__D1','card3__D1','card5__D1','addr1__D1',\n'card1__D3','card2__D3','card3__D3','card5__D3','addr1__D3',\n'card1__V307','card2__V307','card3__V307','card5__V307','addr1__V307',\n\n'card1__V130','card2__V130','card3__V130','card5__V130','addr1__V130',\n'card1__V127','card2__V127','card3__V127','card5__V127','addr1__V127'\n# 'cards_D3CUM_MinsDate__TransactionAmt'\n# 'D1_PART_ID__TransactionAmt','D1_PART_ID__dist1',\n# 'card1__D1','card2__D1','card3__D1','card5__D1','addr1__D1'\n\n] ### new adding pair\n# 'card3__D8','addr1__id_02', 'card1__dist1', 'card1__dist2','card1__V332', 'addr1__id_02','card1__V99', 'addr1__V99',\n\n\n\n# 'card1__V99', 'addr1__V99', 'card1__V100', 'addr1__V100', 'card1__V139', 'addr1__V139'\n#  dels\n\n\n\n\nfor xy_name in tqdm(xy_list):\n    if 'dist2' in xy_name:\n        continue\n    x_name, y_name = xy_name.split('__')\n    if y_name not in df_data.columns:\n        continue\n    df_data = Num_engineering(df_data, x_name, y_name)\n    \n\nuids = Dids + ['card1_V307_series','cards_V307_series','card1_addr1_V307_series']\nfor col in uids:\n    ct = df_data[col].value_counts()\n    df_data[col+'_cnt'] = df_data[col].map(ct)\n\ncats = ['id_01', 'id_02', 'id_03', 'id_05', 'id_06', 'id_09',\n                   'id_11', 'id_12', 'id_13', 'id_14', 'id_15', 'id_17', 'id_19', 'id_20', 'id_30', 'id_31', 'id_32', 'id_33',\n                   'id_36', 'id_37', 'id_38', 'DeviceType', 'DeviceInfo', 'device_name', 'device_version', 'OS_id_30', 'version_id_30',\n                   'browser_id_31', 'version_id_31', ]\n\ncats = cats+uids\ncats = cats + ['uid','uid2','uid3']\n\ndf_data['tmpid'] = df_data['card1'].map(str) + '-' + df_data['addr1'].map(str)\nfor col in ['C1', 'C5', 'C6','V53', 'V138', 'C9']:\n    new_id = 'card1_addr1_' + col\n    new_id_count = 'card1_addr1_' + col +'_cnt'\n    df_data[ new_id ] = df_data['tmpid'] + '-' + df_data[col].map(str)\n    cats.append(new_id)\n    df_data[ new_id_count ] = df_data[ new_id ].map(df_data[ new_id ].value_counts())\ndel df_data['tmpid']\n\n\ntrain = df_data[df_data.isFraud.isnull()==False]\ntest = df_data[df_data.isFraud.isnull()==True]\ndel df_data\n\n\n\n# combinations count of cates\ncateCOLS = ['card1', 'card2', 'card3', 'card4', 'card5', 'card6','id_30','id_31','addr1','addr2','P_emaildomain','R_emaildomain','ProductCD','DeviceType', 'DeviceInfo', 'device_name', 'device_version']\ncats = cats+cateCOLS\n\nuse_join_nnq = []\nfor k,v in dict(train[cateCOLS].nunique()).items():\n    if v>50:\n        use_join_nnq.append(k)\n        print(k)\nfor from_col in use_join_nnq:\n    for to_col in use_join_nnq:\n        if from_col == to_col:\n            continue\n        gp = pd.concat([ train[ [from_col,to_col] ],test[ [from_col,to_col] ] ]).groupby([from_col])[to_col].nunique()\n        lennnq = len(set(gp.values.tolist()))\n        if lennnq <10:\n            print('from_col:{}, to_col : {} creating new col only {} values less than 10 ,will not use'.format(from_col,to_col,lennnq  ))\n        else:\n            train[ '{}_nnqof_{}'.format( from_col,to_col )] = train[from_col].map(gp)\n            test[ '{}_nnqof_{}'.format( from_col,to_col )] = test[from_col].map(gp)\n\n#to avoid overfit\ntonan_theadhold_dict = {'C1':2000,'C2':2000,'C4':1000,'C6':1000,'C8':800,'C9':240,\n                       'C14':240,'C10':520,'C11':520,'C12':520,'C13':1000}\n\nfor col,thod in tonan_theadhold_dict.items():\n    train.loc[train[col]>= thod,col] = np.nan\n    test.loc[test[col]>= thod,col] = np.nan\n\nimport datetime\ndef tiem_deta_date(data,new_data_col):\n    START_DATE = '2017-12-01'\n    startdate = datetime.datetime.strptime(START_DATE, '%Y-%m-%d')\n    data[new_data_col] = startdate + data['TransactionDT'].map(lambda x:datetime.timedelta(seconds = x))\n    return data\n\n\ntrain = tiem_deta_date(train,'Transaction_dt')\ntest = tiem_deta_date(test,'Transaction_dt')\n\ntrain['TransDateStr'] = train['Transaction_dt'].map(lambda x:str(x)[:10])\ntest['TransDateStr'] = test['Transaction_dt'].map(lambda x:str(x)[:10])\ndel train['Transaction_dt']\ndel test['Transaction_dt']\n\n\n# for col in [c for c in data.columns if '_days_nunique' in c]:\n#     del train[col]\n#     del test[col]\n\n\n    #*************************************\n    #to avoid overfit\n    #*************************************\n\nfor col in ['id_01','id_06','id_11','id_13','id_21','id_25','addr1__card1','card1']:\n    if col in train.columns:\n        print('deal with col {}'.format(col))\n        thr = 7\n        if 'card1' in col:\n            thr = 1\n        for data in [train,test]:\n            data['value_happend_days'] = data[col].map(data.groupby([col])['TransDateStr'].nunique() )\n            ## adding day nunique here\n#             data[col +'_days_nunique'] = data[col].map(  (pd.concat( [train[[col,'TransDateStr' ]],test[[col,'TransDateStr' ]]] ) ).groupby([col])['TransDateStr'].nunique()  )\n            data.loc[ data['value_happend_days']<=thr,col ]= np.nan\n            \n            print('deleted {} of {} of data to nan '.format(len(data[ data['value_happend_days']<=thr]),col))  \n            del data['value_happend_days']            \n    else:\n        print('!!! not exists,col {} had been deleted '.format(col))\n#to avoid overfit        \nfor col in ['addr1__card1','card1']:\n    train_col = train[col]\n    test_col = test[col]\n    print('train: deleting un overlap data :{}'.format( len(train.loc[train[col].isin( test_col ) == False,col  ]) ))\n    print('test: deleting un overlap data :{}'.format( len(test.loc[test[col].isin( train_col ) == False,col  ] ) ))\n    train.loc[train[col].isin( test_col ) == False,col  ] = np.nan\n    test.loc[test[col].isin( train_col ) == False,col  ] = np.nan\n\n\n\n\n# dealing with time based cols\n\ntest = tiem_deta_date(test,'Transaction_dt')\ntest['TransDateStr'] = test['Transaction_dt'].map(lambda x:str(x)[:10])\ntrain = tiem_deta_date(train,'Transaction_dt')\ntrain['TransDateStr'] = train['Transaction_dt'].map(lambda x:str(x)[:10])\ndel train['Transaction_dt']\ndel test['Transaction_dt']\n\n\n\ntrain['weekofyear'] = pd.to_datetime(train['TransDateStr']).dt.weekofyear\ntrain['Year'] = pd.to_datetime(train['TransDateStr']).dt.year\ntrain['Year-weekofyear'] = train['Year'].map(str) + \"|\" + train['weekofyear'].map(str)\n\n\ntest['weekofyear'] = pd.to_datetime(test['TransDateStr']).dt.weekofyear\ntest['Year'] = pd.to_datetime(test['TransDateStr']).dt.year\ntest['Year-weekofyear'] = test['Year'].map(str) + \"|\" + test['weekofyear'].map(str)\n\ndel train['weekofyear']\ndel train['Year']\ndel test['weekofyear']\ndel test['Year']\ndel train['TransDateStr']\ndel test['TransDateStr']\n\n\n# D7\nDs = ['D3','D4','D5','D6','D8','D10','D11','D12','D13','D14','D15']\n\n\n\ndef out_lier_remove(df,col):\n    gp =  df.groupby(['Year-weekofyear'])[col].quantile(.995)\n    df['qt'] = df['Year-weekofyear'].map(gp)\n    df['new_' + col] = df[col].copy()\n    df.loc[ df[col] >=df['qt'],'new_' + col]=np.nan\n    del df['qt']\n    return df\n\nfor col in Ds:\n    train = out_lier_remove(train,col)\n    test = out_lier_remove(test,col)\n\nbase_on = 'Year-weekofyear'    \nnew_DS = ['new_' +col for col in Ds]    \ndf = pd.concat([train[[base_on] + new_DS],test[[base_on] + new_DS]])\naggs = {}\nfor d in new_DS:\n    aggs[d] = ['mean']\n    \ngp = df.groupby([ base_on ]).agg(aggs)\ngp.columns = pd.Index([ e[0]+\"_\"+e[1] for e in gp.columns.tolist()])\ngp.reset_index(inplace=True)\ngp = gp.sort_values([base_on]).reset_index(drop = True)\n\n\n    \n\ntrain = train.merge(gp[[base_on]+[c for c in gp.columns if 'mean' in c]],on=[base_on],how='left')\ntest = test.merge(gp[[base_on]+[c for c in gp.columns if 'mean' in c]],on=[base_on],how='left')\ndel gp\ndel df\nfor col in Ds:\n    train[col ] = (train[col]-train['new_'+col + '_mean'])/train['new_'+col + '_mean']\n    test[col] = (test[col]-test['new_'+col + '_mean'])/test['new_'+col + '_mean']\n    del train['new_'+col + '_mean']\n    del test['new_'+col + '_mean']\n    \n\n    #*************************************\n    #to avoid overfit\n    #*************************************\n\n\n\n# del train[base_on]    \n# del test[base_on]\nle = LabelEncoder()\nle.fit(list(train[base_on].astype(str).values) + list(test[base_on].astype(str).values))\ntrain[base_on] = le.transform(list(train[base_on].astype(str).values))\ntest[base_on] = le.transform(list(test[base_on].astype(str).values))    \n\n#########################################\n# bacily data cleaning\n#########################################\n#*******************************end*******************************\n\nfor col in train.columns:\n    if train[col].dtype == 'object':\n        if col == 'TransactionID':\n            continue\n            \n        le = LabelEncoder()\n        le.fit(list(train[col].astype(str).values) + list(test[col].astype(str).values))\n        train[col] = le.transform(list(train[col].astype(str).values))\n        test[col] = le.transform(list(test[col].astype(str).values))\n\n\n\n\n\n# drop corr counters\n\ngp = train[[c for c in train.columns if 'count' in c]].corr()\ngp[gp>0.99]\ncor_dic = {}\nfor col in gp.columns:\n    cells = gp[gp[col]>=0.999].index.values\n    cor_dic[col] = cells\ndrop_cols = []\n\nfor k,v in cor_dic.items():\n    if k in drop_cols:\n        continue\n    dropEmp = []\n    if len(v)==1:\n        continue\n    for c in v:\n        if c==k:\n            pass\n        else:\n            dropEmp.append( c )\n    drop_cols = drop_cols+dropEmp    \n    \n# catboost model..............\n\nfrom sklearn.model_selection import KFold\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold,StratifiedKFold\nimport catboost as cbt\n\n\n\n# w2v_exception = [c for c in train.columns if 'domain_Sep_w2v' in c]\n\n# 'addr1__card1']+time_base_col\nfeats = [c for c in train.columns if c not in ['isFraud', 'TransactionDT','TransactionID','TransDateStr','Year-weekofyear'] \n        and 'new' not  in c and c not in drop_cols]\n\ncat_list = [c for c in feats if c in cats]\n        # \n\nfor cat in cat_list:\n    train[cat] = train[cat].map(str)\n    test[cat] = test[cat].map(str)\n\n\n\nX_train = train[feats]\ny = train['isFraud']\nX_test = test[feats]\nid_test = test['TransactionID'].values\n\nprint(X_train.shape,X_test.shape)\noof = np.zeros(X_train.shape[0])\n\n\n\nprediction = np.zeros(X_test.shape[0])\nseeds = [ 2048, 1024]\nnum_model_seed = 2\nfor model_seed in range(num_model_seed):\n    oof_cat = np.zeros(X_train.shape[0])\n    prediction_cat=np.zeros(X_test.shape[0])\n    skf = StratifiedKFold(n_splits=5, random_state=seeds[model_seed], shuffle=True)\n    for index, (train_index, test_index) in enumerate(skf.split(X_train, y)):\n        print(index)\n        print('on folder tracking ........')\n        # print('*********[{}]*********'.format(folder_track_log_dir))\n        print('on folder tracking ........')\n        \n        \n        # fold_pred_file_ = folder_track_log_dir + '/model_seed_{}_Fold_{}_Done.npy'.format( model_seed,index )\n        # if os.path.exists( fold_pred_file_ ):\n        #     continue\n        # # boosting_type='Plain', \n        train_x, test_x, train_y, test_y = X_train[feats].iloc[train_index], X_train[feats].iloc[test_index], y.iloc[train_index], y.iloc[test_index]\n        cbt_model = cbt.CatBoostClassifier(iterations=10000,learning_rate=0.1,max_depth=7,verbose=100,\n                                      early_stopping_rounds=500,task_type='GPU',eval_metric='AUC',\n                                      cat_features=cat_list)\n        cbt_model.fit(train_x[feats], train_y,eval_set=(test_x[feats],test_y))\n        \n        oof_cat[test_index] += cbt_model.predict_proba(test_x)[:,1]\n        prediction_cat += cbt_model.predict_proba(X_test[feats])[:,1]/5\n        del cbt_model\n        del train_x, test_x, train_y, test_y\n\n        \n    print('AUC',roc_auc_score(y,oof_cat))    \n    oof += oof_cat / num_model_seed\n    prediction += prediction_cat / num_model_seed\nprint('score',roc_auc_score(y,oof)) \n\n# roc_auc_score(y_valid, y_pred_valid)\n\n\n\nsub.columns = ['TransactionID','old']\ntest['isFraud'] = prediction\nsub = sub.merge(test[['TransactionID','isFraud']],on=['TransactionID'])\ndel sub['old']\n\n# add D2\nsub.to_csv('../submission/cat_submit_549_1001_2seed.csv',index=False","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}