{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# Introduction"},{"metadata":{},"cell_type":"markdown","source":"Hi there! I'm kind of new to Kaggle but would like to use this competition as an opportunity to demonstrate the Automunge tool for automated data-wrangling. This entry is possibly in the realm of Automated ML Tool territory, but to be fair Automunge isn't really a turnkey machine learning replacement, it's intended use is primarily for the preparation of tabular data for machine learning (basically performing the data preparation pipelines in the steps immediately preceding training a machine learning model or the subsequent consistent processing to prepare data to generate predictions), and the application of predictive algorithms are intended to be conducted seperately. That being said, the tool does make use of some predictive algorithms along the way, including the optional use of machine learning to predict infill to missing or improperly formatted data, what we call ML infill (more on that later).\n\nIn short, Automunge prepares tabular data intended for training a machine learning model, and enables consistent processing of subsequently available data for generating predictions from that same model. Through preparation, numerical data is normalized, categorical data is encoded, and time-series data is also encoded. A user may defer to automated methods where the tool infers properties of each column to assign a processing method, or alternately assign custom processign methods to distinct columns from our library of feature engineering transformations. \n\nA user may also consider automunge a platform for data wrangling, and may pass their own processing functions incorproating simple data structures such that through the incorproation of their trasnforms into the tool they can make use of extremely useful methods such as machine learning derived infill to missing or improperly formatted data (ML infill), feature importance evaluation, automated dimensionality reduction via feature importance results or Principle Components Analysis (PCA), and perthaps most importantly the simplest means for consistent processing of subsequently available data with just the simplest of function calls. In short, we make machine learning easy."},{"metadata":{},"cell_type":"markdown","source":"# Prerequisites"},{"metadata":{},"cell_type":"markdown","source":"Before proceeding with the demonstration we'll conduct a few data preparations. Note that Automunge needs following prerequisites to operate:\n\n- tabular data in Pandas dataframe or Numpy array format\n- \"tidy data\" (meaning one feature per column and one observation per row)\n- if available label column included in the set with column name passed to function as string\n- a \"train\" data set intended to train a machine learning model and if available a \"test\" set intended to generate predictions from the same model\n- the train and test data must have consistently formatted data and consistent column headers\n\nOk well introductions complete let's go ahead and manually munge to meet these requirements."},{"metadata":{},"cell_type":"markdown","source":"# Data imports and preliminary munging"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#First we'll grab the file paths\ntrain_identify_filepath = \"../input/ieee-fraud-detection/train_identity.csv\"\ntrain_transaction_filepath = \"../input/ieee-fraud-detection/train_transaction.csv\"\ntest_identity_filepath = \"../input/ieee-fraud-detection/test_identity.csv\"\ntest_transaction_filepath = \"../input/ieee-fraud-detection/test_transaction.csv\"\nsample_submission_filepath = \"../input/ieee-fraud-detection/sample_submission.csv\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now let's import them as dataframes. Note both the identify and transaction sets include \n#a single common column, TransactionID, so we'll use that as an index column to merge\n\ntrain_identify = pd.read_csv(train_identify_filepath, error_bad_lines=False, index_col=\"TransactionID\")\ntrain_transaction = pd.read_csv(train_transaction_filepath, error_bad_lines=False, index_col=\"TransactionID\")\ntest_identity = pd.read_csv(test_identity_filepath, error_bad_lines=False, index_col=\"TransactionID\")\ntest_transaction = pd.read_csv(test_transaction_filepath, error_bad_lines=False, index_col=\"TransactionID\")\nsample_submission = pd.read_csv(sample_submission_filepath, error_bad_lines=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Note that by inspection we end up with the identify columns on about a quarter \n#of the rows of the transaction data\nprint(\"   train_identify.shape = \", train_identify.shape)\nprint(\"train_transaction.shape = \", train_transaction.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Here we'll concatinate the two sets based on the common index points\nmaster_train = pd.concat([train_transaction, train_identify], axis=1, sort=False)\nmaster_test = pd.concat([test_transaction, test_identity], axis=1, sort=False)\n\nprint(\"master_train.shape = \", master_train.shape)\nprint(\"master_test.shape = \", master_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Because I'm going to be doing a whole bunch of demonstrations \n#in this notebook, I'm going to carve out a much smaller set \n#to speed up the writing.\n\ncolumns_subset = list(master_train)[:15]\n#(test columns won't have the label column)\ntest_columns_subset = list(master_test)[:14]\n\nsmall_train = master_train[columns_subset]\nsmall_test = master_test[test_columns_subset]\n\nfrom sklearn.model_selection import train_test_split\nbig_train, tiny_train = train_test_split(small_train, test_size=0.002, random_state=42)\nbig_test, tiny_test = train_test_split(small_test, test_size=0.002, random_state=42)\n\n\nprint(list(tiny_train))\nprint(\"\")\nprint(\"tiny_train.shape = \", tiny_train.shape)\nprint(\"big_train.shape = \", big_train.shape)\nprint(\"\")\nprint(\"tiny_test.shape = \", tiny_test.shape)\nprint(\"big_test.shape = \", big_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Automunge install and initialize"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Ok here's where we import our tool with pip install. Note that this step requires  \n#access to the internet. (Note this import procedure changed with version 2.58.)\n\n! pip install Automunge\n\n# #or to upgrade (we currently roll out upgrades pretty frequently)\n# ! pip install Automunge --upgrade","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#And then we initialize the class.\n\nfrom Automunge import Automunger\nam = Automunger.AutoMunge()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ok let's give it a shot"},{"metadata":{},"cell_type":"markdown","source":"Well at the risk of overwhelming the reader I'm just going to throw out a full application. Basically, we pass the train set and if available a consistently formatted test set to the function and it returns normalized and numerically encoded sets suitable for the direct application of machine learning. The function returns a series of sets (which based on the options selected may be empty), I find it helps to just copy and paste the full range of arguments and returned sets from the documentation for each application. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#So first let's just try a generic application with our tiny_train set. Note tiny_train here\n#represents our train set. If a labels column is available we should include and designate, \n#and any columns we want to exclude from processing we can designate as \"ID columns\" which\n#will be carved out and consitnelty shuffled and partitioned.\n\n#Note here we're only demonstrating on the set with the reduced number of features to save time.\n\n\ntrain, trainID, labels, \\\nvalidation1, validationID1, validationlabels1, \\\nvalidation2, validationID2, validationlabels2, \\\ntest, testID, testlabels, \\\nlabelsencoding_dict, finalcolumns_train, finalcolumns_test, \\\nfeatureimportance, postprocess_dict = \\\nam.automunge(tiny_train, df_test = False, labels_column = 'isFraud', trainID_column = False, \\\n            testID_column = False, valpercent1=0.0, valpercent2 = 0.0, \\\n            shuffletrain = False, TrainLabelFreqLevel = False, powertransform = False, \\\n            binstransform = False, MLinfill = False, infilliterate=1, randomseed = 42, \\\n            numbercategoryheuristic = 15, pandasoutput = True, NArw_marker = False, \\\n            featureselection = False, featurepct = 1.0, featuremetric = .02, \\\n            featuremethod = 'pct', PCAn_components = None, PCAexcl = [], \\\n            ML_cmnd = {'MLinfill_type':'default', \\\n                       'MLinfill_cmnd':{'RandomForestClassifier':{}, 'RandomForestRegressor':{}}, \\\n                       'PCA_type':'default', \\\n                       'PCA_cmnd':{}}, \\\n            assigncat = {'mnmx':[], 'mnm2':[], 'mnm3':[], 'mnm4':[], 'mnm5':[], 'mnm6':[], \\\n                         'nmbr':[], 'nbr2':[], 'nbr3':[], 'MADn':[], 'MAD2':[], 'MAD3':[], \\\n                         'bins':[], 'bint':[], \\\n                         'bxcx':[], 'bxc2':[], 'bxc3':[], 'bxc4':[], \\\n                         'log0':[], 'log1':[], 'pwrs':[], \\\n                         'bnry':[], 'text':[], 'ordl':[], 'ord2':[], \\\n                         'date':[], 'dat2':[], 'wkdy':[], 'bshr':[], 'hldy':[], \\\n                         'excl':[], 'exc2':[], 'exc3':[], 'null':[], 'eval':[]}, \\\n            assigninfill = {'stdrdinfill':[], 'MLinfill':[], 'zeroinfill':[], 'oneinfill':[], \\\n                            'adjinfill':[], 'meaninfill':[], 'medianinfill':[]}, \\\n            transformdict = {}, processdict = {}, \\\n            printstatus = True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So what's going on here is we're calling the function am.automunge and pass the returned sets to a series of objects:\n```\ntrain, trainID, labels, \\\nvalidation1, validationID1, validationlabels1, \\\nvalidation2, validationID2, validationlabels2, \\\ntest, testID, testlabels, \\\nlabelsencoding_dict, finalcolumns_train, finalcolumns_test, \\\nfeatureimportance, postprocess_dict = \\\n```\n\nAgain we don't have to include all of the parameters when calling the function, but I find it helpful just to copy and paste them all. For example if we just wanted to defer to defaults we could just call:\n```\ntrain, trainID, labels, \\\nvalidation1, validationID1, validationlabels1, \\\nvalidation2, validationID2, validationlabels2, \\\ntest, testID, testlabels, \\\nlabelsencoding_dict, finalcolumns_train, finalcolumns_test, \\\nfeatureimportance, postprocess_dict = \\\nam.automunge(tiny_train)\n```\n\nThose sets returned from the function call are as follows:\n\n- __train, trainID, labels__ : these are the sets intended to train a machine learning model. (The ID set is simply any columns we wanted to exclude from transformations comparably partitioned and shuffled)\n- __validation1, validationID1, validationlabels1__ : these are sets carved out from the train set intended for hyperparameter tuning validation based on the designated validation1 ratio (defaults to 0.0)\n- __validation2, validationID2, validationlabels2__ : these are sets carved out from the train set intended for final model validation based on the designated validation2 ratio (defaults to 0.0)\n- __test, testID, testlabels__ : these are the sets derived from any passed test set intended to generate predictions from the machine learning model trained form the train set, consistently processed as the train set\n- __labelsencoding_dict__ : this is a dictionary which may prove useful for reverse encoding predictions generated from the machine learning model to be trained from the train set\n- __finalcolumns_train, finalcolumns_test__ : a list of the columns returned from the transformation, may prove useful in case one wants to ensure consistent column labeling which is required for subsequent processing of any future test data\n- __featureimportance__ : this stores the results of the feature importance evaluation if user elects to conduct\n- __postprocess_dict__ : this dictionary should be saved as it may be used as an input to the postmunge funciton to consistently process any subsequently available test data"},{"metadata":{},"cell_type":"markdown","source":"Let's take a look at a few items of interest from the returned sets.\n\nNotice that the returned sets now include a suffix appended to column name. These suffixes identify what type of transformation were performed. Here we see a few different types of suffixes:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#sffixes identifying steps of transformation\nlist(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#And here's what the returned data looks like.\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Upon inspection:\n- addr2, card4, and ProductCD both have a series of suffixes which represent the different categories derived from a one-hot-encoding of a categorical set\n- each of TransactionDT, TransactionAmt, card1, card2, card3, card5, addr1, addr2, dist1 have the suffix 'nmbr' which represents a z-score normalization\n- card6 has the suffix 'bnry' which represents a binary (0/1) encoding\n- P_emaildomain has the suffix 'ordl' which represents an ordinal (integer) encoding"},{"metadata":{},"cell_type":"markdown","source":"Automunge uses suffix appenders to track the steps of transformations. For example, one could assign transformations to a column which resulted in multiple suffix appenders, such as say:\ncolumn1_bxcx_nmbr\nWhich would represent a column with original header 'column1' upon which was performed two steps of transformation, a box-cox power law transform followed by a z-score normalization."},{"metadata":{},"cell_type":"markdown","source":"# Labels\n\nWhen we conducted the transfomation we also desiganted a label column which was included in the set, so let's take a peek at the returned labels."},{"metadata":{"trusted":true},"cell_type":"code","source":"list(labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#as you can see the returned values on the labels column are consistently encoded\n#as were passed\nlabels['isFraud_bnry'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Note that if or original labels weren't yet binary encoded, we could inspect the \n#returned labelsencoding_dict object to determine the basis of encoding.\n\n#Here we just see that the 1 value originated from values 1, and the 0 value\n#originated from values 0 - a trivial example, but this could be helpful if\n#we had passed a column containing values ['cat', 'dog'] for instance.\n\nlabelsencoding_dict","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Subsequent consistent processing with postmunge(.)\n\nAnother important object returned form the automunge application is what we call the \"postprocess_dict\". In fact, good practice is that we should always save externally any postprocess_dict returned from the application of automunge whose output was used to train a machine learning model. Why? Well using this postprocess_dict object, we can then pass any subsequently available \"test\" data that we want to use to generate predictions from that machine learning model giving fully consistent processing and encoding. Let's demonstrate. \n\nWhen we performed a train_test_split above to derive the \"tiny_train\" set, we also ended up with a bigger set called \"tiny_train_bigger\". Let's try applying the postmunge function to consistently process.\n\nNote a few pre-requisites for the appplication of postmunge:\n\n- requires passing a postprocess_dict that was dervied from the application of automunge\n- consistently formatted data as the train set used in the application of automunge from which the postprocess_dict was derived\n- consistent column labeling as the train set used in the application of automunge from which the postprocess_dict was derived\n\nAnd there we have it, let's demonstrate the postmunge function on the set \"tiny_test\" we prepared above."},{"metadata":{"trusted":true},"cell_type":"code","source":"test, testID, testlabels, \\\nlabelsencoding_dict, finalcolumns_test = \\\nam.postmunge(postprocess_dict, tiny_test, testID_column = False, \\\n             labelscolumn = False, pandasoutput=True, printstatus=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#And if we're doing our job right then this set should be formatted exaclty like that returned\n#from automunge, let's take a look.\n\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Looks good! \n\n#So if we wanted to generate predictions from a machine learning model trained \n#on a train set processed with automunge, we now have a way to consistently \n#prepare data with postmunge.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's explore a (few) of the automunge parameters"},{"metadata":{},"cell_type":"markdown","source":"Ok let's take a look at a few of the optional methods available here. First here again is what a full automunge call looks like:\n\n```\ntrain, trainID, labels, \\\nvalidation1, validationID1, validationlabels1, \\\nvalidation2, validationID2, validationlabels2, \\\ntest, testID, testlabels, \\\nlabelsencoding_dict, finalcolumns_train, finalcolumns_test, \\\nfeatureimportance, postprocess_dict = \\\nam.automunge(df_train, df_test = False, labels_column = False, trainID_column = False, \\\n            testID_column = False, valpercent1=0.0, valpercent2 = 0.0, \\\n            shuffletrain = False, TrainLabelFreqLevel = False, powertransform = False, \\\n            binstransform = False, MLinfill = False, infilliterate=1, randomseed = 42, \\\n            numbercategoryheuristic = 15, pandasoutput = True, NArw_marker = True, \\\n            featureselection = False, featurepct = 1.0, featuremetric = .02, \\\n            featuremethod = 'pct', PCAn_components = None, PCAexcl = [], \\\n            ML_cmnd = {'MLinfill_type':'default', \\\n                       'MLinfill_cmnd':{'RandomForestClassifier':{}, 'RandomForestRegressor':{}}, \\\n                       'PCA_type':'default', \\\n                       'PCA_cmnd':{}}, \\\n            assigncat = {'mnmx':[], 'mnm2':[], 'mnm3':[], 'mnm4':[], 'mnm5':[], 'mnm6':[], \\\n                         'nmbr':[], 'nbr2':[], 'nbr3':[], 'MADn':[], 'MAD2':[], 'MAD3':[], \\\n                         'bins':[], 'bint':[], \\\n                         'bxcx':[], 'bxc2':[], 'bxc3':[], 'bxc4':[], \\\n                         'log0':[], 'log1':[], 'pwrs':[], \\\n                         'bnry':[], 'text':[], 'ordl':[], 'ord2':[], \\\n                         'date':[], 'dat2':[], 'wkdy':[], 'bshr':[], 'hldy':[], \\\n                         'excl':[], 'exc2':[], 'exc3':[], 'null':[], 'eval':[]}\n            assigninfill = {'stdrdinfill':[], 'MLinfill':[], 'zeroinfill':[], 'oneinfill':[], \\\n                            'adjinfill':[], 'meaninfill':[], 'medianinfill':[]}, \\\n            transformdict = {}, processdict = {}, \\\n            printstatus = True)\n```"},{"metadata":{},"cell_type":"markdown","source":"So let's just go through these one by one. (This section is kind of diving into the weeds, not required reading)\n\n__df_train__ and __df_test__ First note that if we want we can pass two different pandas dataframe sets to automunge, such as might be beneficial if we have one set with labels (a \"train\" set) and one set without (a \"test\" set). Note that the normalization parameters are all derived just from the train set, and applied for consistent processing of any test set if included. Again a prerequisite is that any train and test set must have consistently labeled columns and consistent formated data, with the exception of any designated \"ID\" columns or \"label\" columns which will be carved out and consistenlty shuffled and partitioned. Note too that we can pass these sets with non-integer-range index or even multi column indexes, such that such index columns will be carved out and returned as part of the ID sets, consistently shuffled and partitioned. If we only want to process a train set we can pass the test set as \"False\".\n\n__labels_column__ is intended for passing string identifiers of a column that will be treated as labels. Note that some of the methods require the inclusion of labels, such as feature importance evaluation or label frequency levelizer (for oversampling rows with lower frequency labels).\n\n__trainID_column__ and testID_column are intended for passing strings or lists of strings identifying columns that will be carved out before processing and consistently shuffled and partitioned. \n\n__valpercent1__ and __valpercent2__ parameters are intended as floats between 0-1 that indicate the ratio of the sets that will be carved out for the two validation sets. If shuffle train is activated then the sets will be carved out randomly, else they will be taken from the bottom sequnetial rows of the train set and randomly partitioned between the two validaiton sets. Note that these values default to 0.\n\n__shuffletrain__ parameter indicates whether the train set will be (can you guess?) yep you were right the answer is shuffled.\n\n__TrainLabelFreqLevel__ parameter indicates whether the train set will have the oversampling method applied where rows with lower frequency labels are copied for more equal distribution of labels, such as might be beneficial for oversampling in the training operation.\n\n__powertransform__ parameter indicates whether default numerical coloumn evluation will include an inference of distribution properties to assign between z-score normalization, min-max scaling, or box-cox power law trasnformation. Note this one is still somewhat rough around the edges and we will continue to refine mewthods going forward.\n\n__binstransform__ indicates whether defauilt z-score normalizaiton applicaiton will include the develoipment of bins sets identifying a point's placement with respect to number of standard deviations from the mean.\n\n__MLinfill__ indicates whether default infill methods will predict infill for missing points using machine learning models trained on the rest of the set in generalizaed and automated fashion. Note that this method benefits from increased scale of data in teh train set, and mmodels derbvied from the train set are used for consistent prediction methods for the test set.\n\n__infilliterate__ indicates whether the predictive methods for MLinfill will be iterated by this integer such as may be beneficial for particularily messy data.\n\n__randomseed__ seed for randomness for all of the random seeded methods such as predcitive algorithms for ML infill, feature importance, PCA, shuffling, etc\n\n__numbercategoryheuristic__ an integer indicating for categorical sets the threshold between processing with one-hot-encoding vs ordinal methods\n\n__pandasoutput__ quite simply True means returned sets are pandas dataframes, False means Numpy arrays (defaults to Numpy arrays)\n\n__NArw_marker__ indicates whether returned columns will include a derived column indicating rows that were subject to infill (can be identified with the suffix \"NArw\")\n\n__featureselection__ indicated whether a feature importance evlauation will be performed (using then shuffle permeation method), note this requires the inclusion of a designated loabels column in the train set. Results are presented in the returned object \"featureimportance\"\n\n__featurepct__ if feature selection performed and featuremethod == 'pct', indicates what percent of columns will be retained from the feature importance dimensionality reduction (columns are ranked by importance and the low percent are trimmed). Note that a value of 1.0 means no trimming will be done.\n\n__featuremetric__ if feature selection performed and featuremethod == 'metric', indicates what threshold of importance metric will be required for retained columns from the feature importance dimensionality reduction (columns feature importance metrics are derived and those below this threshold are trimmed). Note that a value of 0.0 means no trimming will be done.\n\n__feteaturemethod__ accepts values of 'pct' or 'metric' indicates method used for any feature importance dimensionality reduction\n\n__PCAn_components__ Triggers PCA dimensionality reduction when != None. Can be a float indicating percent of columns to retain in PCA or an integer indicated number of columns to retain. The tool evaluates whether set is suitable for kernel PCA, sparse PCA, or PCA. Alternatively, a user can assign a desired PCA method in the ML-cmnd['PCA_type']. Note that a value of None means no PCA dimensionality reduction will be performed unless the scale of data is below a heuristic based on the number of features. (A user can also just turn off default PCA with ML-cmnd['PCA_type'])\n\n__PCAexcl__ a list of any columns to be excluded from PCA trasnformations\n\n__ML_cmnd__ allows a user to pass parameters to the predictive algorithms used in ML infill, feature importance, and PCA (I won't go into full detail here, although note one handy feature is we can tell the algorithm to exlcude boolean columns form PCA which is useful)\n\n__assigncat__ allows a user to assign distinct columns to different processing methods, for those columns that they don't want to defer to default automated processing. For example a user could designate columns for min-max scaling instead of z-score, or box-cox power law trasnform, or you know we've got a whole library of methods that we're continueing to build out. These are defined in our READ ME. Simply pass the column header string identifier to the list associated with any of these root categories.\n\n__assigninfill__ allows a user to assign disinct columns to different infill methods for missing or improperly formatted data, for those columns that they don't want to defer to default automated infill whi ch could be either standard infill (mean to numerical sets, most common to binary, and boolean identifier to categorical), or ML infill if it was selected. \n\n__transformdict__ and __processdict__ allows a user to design custom trees or trasnformations or even custom processing functions such as documented in our essays that no one reads. Once defined a column can be assigned to these methods in the assigncat.\n\n__printstatus__ You know, like, prints the status during operation. Self-explanatory!\n\n\nNow we'll demonstrate a few."},{"metadata":{},"cell_type":"markdown","source":"# trainID_column, shuffletrain, valpercent1"},{"metadata":{"trusted":true},"cell_type":"code","source":"#great well let's try a few of these out. How about the ID columns, let's see what happens when we pass one.\n#Let's just pick an arbitrary one, TransactionDT\n\ntrain, trainID, labels, \\\nvalidation1, validationID1, validationlabels1, \\\nvalidation2, validationID2, validationlabels2, \\\ntest, testID, testlabels, \\\nlabelsencoding_dict, finalcolumns_train, finalcolumns_test, \\\nfeatureimportance, postprocess_dict = \\\nam.automunge(tiny_train, df_test = False, labels_column = 'isFraud', trainID_column = 'TransactionDT', \\\n             valpercent1=0.20, shuffletrain = True, pandasoutput=True, printstatus = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now we'll find that the TransactionDT column is missing from the train set, left \n#unaltered instead in the ID set, paired with the Transaction ID which was put\n#in the ID set because it was a non-integer range index column (thus if we wanted\n#to reassign the original index column we could simply copy the TransactionID column\n#from the ID set back to the processed train set)\n\ntrainID.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#note that since our automunge call included a validation ratio, we'll find \n#a portion of the sets partitioned in the validation sets, here for instance\n#is the validaiton ID sets \n\n#(we'll also find returned sets in the validation1, and validationlabels1)\n\n#note that since we activated the shuffletrain option these are randomly\n#selected from the train set\n\nvalidationID1.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TrainLabelFreqLevel"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's take a look at TrainLabelFreqLevel, which serves to copy rows such as to\n#(approximately) levelize the frequency of labels found in the set.\n\n#First let's look at the shape of a train set returtned from an automunge\n#applicaiton without this option selected\n\ntrain, trainID, labels, \\\nvalidation1, validationID1, validationlabels1, \\\nvalidation2, validationID2, validationlabels2, \\\ntest, testID, testlabels, \\\nlabelsencoding_dict, finalcolumns_train, finalcolumns_test, \\\nfeatureimportance, postprocess_dict = \\\nam.automunge(tiny_train, df_test = False, labels_column = 'isFraud', TrainLabelFreqLevel=False, \\\n             pandasoutput=True, printstatus=False)\n\nprint(\"train.shape = \", train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#OK now let's try again with the option selected. If there was a material discrepency in label frequency\n#we should see more rows included in the returned set\n\ntrain, trainID, labels, \\\nvalidation1, validationID1, validationlabels1, \\\nvalidation2, validationID2, validationlabels2, \\\ntest, testID, testlabels, \\\nlabelsencoding_dict, finalcolumns_train, finalcolumns_test, \\\nfeatureimportance, postprocess_dict = \\\nam.automunge(tiny_train, df_test = False, labels_column = 'isFraud', TrainLabelFreqLevel=True, \\\n             pandasoutput=True, printstatus=False)\n\nprint(\"train.shape = \", train.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# binstransform"},{"metadata":{"trusted":true},"cell_type":"code","source":"#binstransform just means that default numerical sets will include an additional set of bins identifying\n#number of standard deviations from the mean. We have to be careful with this one if we don't have a lot\n#of data as it adds a fair bit of dimensionality\n\ntrain, trainID, labels, \\\nvalidation1, validationID1, validationlabels1, \\\nvalidation2, validationID2, validationlabels2, \\\ntest, testID, testlabels, \\\nlabelsencoding_dict, finalcolumns_train, finalcolumns_test, \\\nfeatureimportance, postprocess_dict = \\\nam.automunge(tiny_train, df_test = False, labels_column = 'isFraud', binstransform=True, \\\n             pandasoutput=True, printstatus=False)\n\nprint(\"list(train):\")\nlist(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#so the interpretation should be for columns with suffix including \"bint\" that indicates \n#bins for number fo standard deviations from the mean. For example, nmbr_bint_t+01\n#would indicated values between mean to +1 standard deviation.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# MLinfill"},{"metadata":{"trusted":true},"cell_type":"code","source":"#So MLinfill changes the default infill method from standardinfill (which means mean for \n#numerical sets, most common for binary, and boolean marker for categorical), to a predictive\n#method in which a machine learning model is trained for each column to predict infill based\n#on properties of the rest of the set. This one's pretty neat, but caution that it performs \n#better with more data as you would expect.\n\n#Let's demonstrate, first here's an applicaiton without MLinfill, we'll turn on the NArws option\n#to output an identifier of rows subject to infill\n\ntrain, trainID, labels, \\\nvalidation1, validationID1, validationlabels1, \\\nvalidation2, validationID2, validationlabels2, \\\ntest, testID, testlabels, \\\nlabelsencoding_dict, finalcolumns_train, finalcolumns_test, \\\nfeatureimportance, postprocess_dict = \\\nam.automunge(tiny_train, df_test = False, labels_column = 'isFraud', MLinfill=False, \\\n             NArw_marker=True, pandasoutput=True, printstatus=False)\n\nprint(\"train.head()\")\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#So upon inspection it looks like we had a few infill points on\n#columns originating from dist1 (as identified by the NArw columns)\n#so let's focus on that\n\n#As you can see the plug value here is just the mean which for a \n#z-score normalized set is 0\n\ncolumns = ['dist1_nmbr', 'dist1_NArw']\ntrain[columns].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now let's try with MLinfill\n\ntrain, trainID, labels, \\\nvalidation1, validationID1, validationlabels1, \\\nvalidation2, validationID2, validationlabels2, \\\ntest, testID, testlabels, \\\nlabelsencoding_dict, finalcolumns_train, finalcolumns_test, \\\nfeatureimportance, postprocess_dict = \\\nam.automunge(tiny_train, df_test = False, labels_column = 'isFraud', MLinfill=True, \\\n             NArw_marker=True, pandasoutput=True, printstatus=False)\n\nprint(\"train[columns].head()\")\ntrain[columns].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#As you can see the method predicted a unique infill value to each row subject to infill\n#(as identified by the NArw column). We didn't include a lot of data with this small demonstration\n#set, so I expect the accuracy of this method would improve with a bigger set","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# numbercategoryheuristic"},{"metadata":{"trusted":true},"cell_type":"code","source":"# numbercategoryheuristic just changes the threshold for number of unique values in a categorical set\n#between processing a categorical set via one-hot encoding or ordinal processing (sequential integer encoding)\n\n#for example consiter the returned column for the email domain set in the data, if we look above we see the\n#set was processed as ordinal, let's see why\n\nprint(\"number of unique values in P_emaildomain column pre-processing\")\nprint(len(train['P_emaildomain_ordl']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#So yeah looks like that entry has a unique entry per row, so really not really a good candidate for inclusion at\n#all, this might be better served carved out into the ID set until such time as we can extract some info from it\n#prior to processing. But the poitn is if we had set numbercategoryheuristic to 1478 instead of 15 we would have \n#derived 1477 one-hot-encoded columns from this set which obviosuly would be an issue for this scale of data.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# pandasoutput"},{"metadata":{"trusted":true},"cell_type":"code","source":"#pandasoutput just tells whether to return pandas dataframe or numpy arrays (defaults to numpy which\n#is a more universal elligible input to the different machine learning frameworks)\n\ntrain, trainID, labels, \\\nvalidation1, validationID1, validationlabels1, \\\nvalidation2, validationID2, validationlabels2, \\\ntest, testID, testlabels, \\\nlabelsencoding_dict, finalcolumns_train, finalcolumns_test, \\\nfeatureimportance, postprocess_dict = \\\nam.automunge(tiny_train, df_test = False, labels_column = 'isFraud',  \\\n             pandasoutput=False, NArw_marker = False, printstatus=False)\n\nprint(\"type(train)\")\nprint(type(train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#note that if we return numpy arrays and want to view the column headers \n#(which remember track the steps of transofmations in their suffix appenders)\n#good news that's available in the returned finalcolumns_train\nprint(\"finalcolumns_train\")\nfinalcolumns_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#or with pandasoutput = True\n\ntrain, trainID, labels, \\\nvalidation1, validationID1, validationlabels1, \\\nvalidation2, validationID2, validationlabels2, \\\ntest, testID, testlabels, \\\nlabelsencoding_dict, finalcolumns_train, finalcolumns_test, \\\nfeatureimportance, postprocess_dict = \\\nam.automunge(tiny_train, df_test = False, labels_column = 'isFraud',  \\\n             pandasoutput=True, NArw_marker = True, printstatus=False)\n\nprint(\"type(train)\")\nprint(type(train))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# NArw_marker"},{"metadata":{"trusted":true},"cell_type":"code","source":"#The NArw marker helpfully outputs from each column a marker indicating what rows were\n#subject to infill. Let's quickly demonstrate. First here again are the returned columns\n#without this feature activated.\n\ntrain, trainID, labels, \\\nvalidation1, validationID1, validationlabels1, \\\nvalidation2, validationID2, validationlabels2, \\\ntest, testID, testlabels, \\\nlabelsencoding_dict, finalcolumns_train, finalcolumns_test, \\\nfeatureimportance, postprocess_dict = \\\nam.automunge(tiny_train, df_test = False, labels_column = 'isFraud', \\\n             NArw_marker=False, pandasoutput=True, printstatus=False)\n\nprint(\"list(train)\")\nlist(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now with NArw_marker turned on.\n\ntrain, trainID, labels, \\\nvalidation1, validationID1, validationlabels1, \\\nvalidation2, validationID2, validationlabels2, \\\ntest, testID, testlabels, \\\nlabelsencoding_dict, finalcolumns_train, finalcolumns_test, \\\nfeatureimportance, postprocess_dict = \\\nam.automunge(tiny_train, df_test = False, labels_column = 'isFraud', \\\n             NArw_marker=True, pandasoutput=True, printstatus=False)\n\nprint(\"list(train)\")\nlist(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#If we inspect one of these we'll see a marker for what rows were subject to infill\n#(actually already did this a few cells ago but just to be complete)\n\ncolumns = ['dist1_nmbr', 'dist1_NArw']\ntrain[columns].head()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# featureselection"},{"metadata":{"trusted":true},"cell_type":"code","source":"#featureselection performs a feature importance evaluation with the permutaion method. \n#(basically trains a machine learning model, and then measures impact to accuaracy \n#after randomly shuffling each feature)\n\n#Let's try it out. Note that this method requires the inclusion of a labels column.\n\ntrain, trainID, labels, \\\nvalidation1, validationID1, validationlabels1, \\\nvalidation2, validationID2, validationlabels2, \\\ntest, testID, testlabels, \\\nlabelsencoding_dict, finalcolumns_train, finalcolumns_test, \\\nfeatureimportance, postprocess_dict = \\\nam.automunge(tiny_train, df_test = False, labels_column = 'isFraud', NArw_marker=False, \\\n             featureselection=True, pandasoutput=True, printstatus=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now we can view the results like so.\n#(a future iteration of tool will improve the reporting method, for now this works)\nfor keys,values in featureimportance.items():\n    print(keys)\n    print('shuffleaccuracy = ', values['shuffleaccuracy'])\n    print('baseaccuracy = ', values['baseaccuracy'])\n    print('metric = ', values['metric'])\n    print('metric2 = ', values['metric2'])\n    print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#I suspect the small size of this demonstration set impacted these results.\n\n#Note that for interpretting these the \"metric\" represents the impact\n#after shuffling the entire set originating from same feature and larger\n#metric implies more importance\n#and metric2 is derived after shuffling all but the current column originating from same\n#feature and smaller metric2 implies greater relative importance in that set of\n#derived features. In case you were wondering.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# PCAn_components, PCAexcl"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now if we want to apply some kind of dimensionality reduction, we can conduct \n#via Principle Component Analysis (PCA), a type of unsupervised learning.\n\n#a few defaults here is PCA is automatically performed if number of features > 50% number of rows\n#(can be turned off via ML_cmnd)\n#also the PCA type defaults to kernel PCA for all non-negative sets, sparse PCA otherwise, or regular\n#PCA if PCAn_components pass as a percent. (All via scikit PCA methods)\n\n#If there are any columns we want to exclude from PCA, we can specify in PCAexcl\n\n#We can also pass parameters to the PCA call via the ML_cmnd\n\n#Let's demosntrate, here we'll reduce to four PCA derived sets, arbitrarily excluding \n#from the transofrmation columns derived from dist1\n\n\ntrain, trainID, labels, \\\nvalidation1, validationID1, validationlabels1, \\\nvalidation2, validationID2, validationlabels2, \\\ntest, testID, testlabels, \\\nlabelsencoding_dict, finalcolumns_train, finalcolumns_test, \\\nfeatureimportance, postprocess_dict = \\\nam.automunge(tiny_train, df_test = False, labels_column = 'isFraud', NArw_marker=False, \\\n             PCAn_components=4, PCAexcl=['dist1'], \\\n             pandasoutput=True, printstatus=False)\n\nprint(\"derived columns\")\nlist(train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Noting that any subsequently available data can easily be consistently prepared as follows\n#with postmunge (by simply passing the postprocess_dict object returned from automunge, which\n#you did remember to save, right? If not no worries it's also possible to consistnelty process\n#by passing the test set with the exact saem original train set to automunge)\n\ntest, testID, testlabels, \\\nlabelsencoding_dict, finalcolumns_test = \\\nam.postmunge(postprocess_dict, tiny_test, testID_column = False, \\\n             labelscolumn = False, pandasoutput=True, printstatus=False)\n\nlist(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Another useful method might be to exclude any boolean columns from the PCA\n#dimensionality reduction. We can do that with ML_cmnd by passing following:\n\ntrain, trainID, labels, \\\nvalidation1, validationID1, validationlabels1, \\\nvalidation2, validationID2, validationlabels2, \\\ntest, testID, testlabels, \\\nlabelsencoding_dict, finalcolumns_train, finalcolumns_test, \\\nfeatureimportance, postprocess_dict = \\\nam.automunge(tiny_train, df_test = False, labels_column = 'isFraud', NArw_marker=False, \\\n             PCAn_components=4, PCAexcl=['dist1'], \\\n             pandasoutput=True, printstatus=False, \\\n             ML_cmnd = {'MLinfill_type':'default', \\\n                        'MLinfill_cmnd':{'RandomForestClassifier':{}, \\\n                                         'RandomForestRegressor':{}}, \\\n                        'PCA_type':'default', \\\n                        'PCA_cmnd':{'bool_PCA_excl':True}})\n\nprint(\"derived columns\")\nlist(train)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# assigncat"},{"metadata":{"trusted":true},"cell_type":"code","source":"#A really important part is that we don't have to defer to the automated evaluation of\n#column properties to determine processing methods, we can also assign distinct processing\n#methods to specific columns.\n\n#Now let's try assigning a few different methods to the numerical sets:\n\n#remember we're assigninbg based on the original column names before the appended suffixes\n\n#How about let's arbitrily select min-max scaling to these columns \nminmax_list = ['card1', 'card2', 'card3']\n\n#And since we previously saw that Transaction_Amt might have some skewness based on our\n#prior powertrasnform evaluation, let's set that to 'pwrs' which puts it into bins\n#based on powers of 10\npwrs_list = ['TransactionAmt']\n\n#Let's say we don't feel the P_emaildomain is very useful, we can just delete it with null\nnull_list = ['P_emaildomain']\n\n#and if there's a column we want to exclude from processiong, we can exclude with excl\n#note that any column we exclude from processing needs to be already numerically encoded\n#if we want to use any of our predictive methods like MLinfill, feature improtance, PCA\n#on other columns. (excl just passes data untouched, exc2 performs a modeinfill just in \n#case some missing points are found.)\nexc2_list = ['card5']\n\n#and we'll leave the rest to default methods\n\ntrain, trainID, labels, \\\nvalidation1, validationID1, validationlabels1, \\\nvalidation2, validationID2, validationlabels2, \\\ntest, testID, testlabels, \\\nlabelsencoding_dict, finalcolumns_train, finalcolumns_test, \\\nfeatureimportance, postprocess_dict = \\\nam.automunge(tiny_train, df_test = False, labels_column = 'isFraud', NArw_marker=False, \\\n             pandasoutput=True, printstatus=False, \\\n             assigncat = {'mnmx':minmax_list, 'mnm2':[], 'mnm3':[], 'mnm4':[], 'mnm5':[], 'mnm6':[], \\\n                         'nmbr':[], 'nbr2':[], 'nbr3':[], 'MADn':[], 'MAD2':[], \\\n                         'bins':[], 'bint':[], \\\n                         'bxcx':[], 'bxc2':[], 'bxc3':[], 'bxc4':[], \\\n                         'log0':[], 'log1':[], 'pwrs':pwrs_list, \\\n                         'bnry':[], 'text':[], 'ordl':[], 'ord2':[], \\\n                         'date':[], 'dat2':[], 'wkdy':[], 'bshr':[], 'hldy':[], \\\n                         'excl':[], 'exc2':exc2_list, 'exc3':[], 'null':null_list, 'eval':[]})\n\nprint(\"derived columns\")\nlist(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Here's what the resulting derivations look like\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# assigninfill"},{"metadata":{"trusted":true},"cell_type":"code","source":"#We can also assign distinct infill methods to each column. Let's demonstrate. \n#I remember when we were looking at MLinfill that one of our columns had a few NArw\n#(rows subject to infill), let's try a different infill method on those \n\n#how about we try adjinfill which carries the value from an adjacent row\n\n#remember we're assigning columns based on their title prior to the suffix appendings\n\ntrain, trainID, labels, \\\nvalidation1, validationID1, validationlabels1, \\\nvalidation2, validationID2, validationlabels2, \\\ntest, testID, testlabels, \\\nlabelsencoding_dict, finalcolumns_train, finalcolumns_test, \\\nfeatureimportance, postprocess_dict = \\\nam.automunge(tiny_train, df_test = False, labels_column = 'isFraud', \\\n             NArw_marker=True, pandasoutput=True, printstatus=False, \\\n             assigninfill = {'adjinfill':['dist1']})\n\ncolumns = ['dist1_nmbr', 'dist1_NArw']\ntrain[columns].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# transformdict and processdict"},{"metadata":{"trusted":true},"cell_type":"code","source":"#trasnformdict and processdict are for more advanced users. They allow the user to design\n#custom compositions of transformations, or even incorporate their own custom defined\n#trasnformation functions into use on the platform. I won't go into full detail on these methods\n#here, I documented these a bunch in the essays which I'll link to below, but here's a taste.\n\n#Say that we have a numerical set that we want to use to apply multiple trasnformations. Let's just\n#make a few up, say that we have a set with fat tail characteristics, and we want to do multiple\n#trasnformions including a bocx-cox trasnformation, a z-score trasnformation on that output, as\n#well as a set of bins for powers of 10. Well our 'TransactionAmt' column might be a good candiate\n#for that. Let's show how.\n\n#Here we define our cusotm trasnform dict using our \"family tree primitives\"\n#Note that we always need to uyse at least one replacement primitive, if a column is intended to be left\n#intact we can include a excl trasnfo0rm as a replacement primitive.\n\n#here are the primitive definitions\n# 'parents' :           upstream / first generation / replaces column / with offspring\n# 'siblings':           upstream / first generation / supplements column / with offspring\n# 'auntsuncles' :       upstream / first generation / replaces column / no offspring\n# 'cousins' :           upstream / first generation / supplements column / no offspring\n# 'children' :          downstream parents / offspring generations / replaces column / with offspring\n# 'niecesnephews' :     downstream siblings / offspring generations / supplements column / with offspring\n# 'coworkers' :         downstream auntsuncles / offspring generations / replaces column / no offspring\n# 'friends' :           downstream cousins / offspring generations / supplements column / no offspring\n\n#So let's define our custom trasnformdict for a new root category we'll call 'cstm'\ntransformdict = {'cstm' : {'parents' : ['bxcx'], \\\n                           'siblings': [], \\\n                           'auntsuncles' : [], \\\n                           'cousins' : ['pwrs'], \\\n                           'children' : [], \\\n                           'niecesnephews' : [], \\\n                           'coworkers' : [], \\\n                           'friends' : []}}\n\n#Note that since bxcx is a parent category, it will look for offspring in the primitives associated\n#with bxcx root cateogry in the library, and find there a downstream nmbr category\n\n#Note that since we are defining a new root category, we also have to define a few parameters for it\n#demonstrate here. Further detail on thsi step available in documentation. If you're not sure you might\n#want to try just copying an entry in the READ ME.\n\n#Note that since cstm is only a root cateogry and not included in the family tree primitives we don't have to\n#define a processing funciton (for the dualprocess/singleprocess/postprocess entries), we can just enter None\n\nprocessdict = {'cstm' : {'dualprocess' : None, \\\n                         'singleprocess' : None, \\\n                         'postprocess' : None, \\\n                         'NArowtype' : 'numeric', \\\n                         'MLinfilltype' : 'numeric', \\\n                         'labelctgy' : 'nmbr'}}\n\n#We can then pass this trasnformdict to the automunge call and assign the intended column in assigncat\ntrain, trainID, labels, \\\nvalidation1, validationID1, validationlabels1, \\\nvalidation2, validationID2, validationlabels2, \\\ntest, testID, testlabels, \\\nlabelsencoding_dict, finalcolumns_train, finalcolumns_test, \\\nfeatureimportance, postprocess_dict = \\\nam.automunge(tiny_train, df_test = False, labels_column = 'isFraud', \\\n             NArw_marker=True, pandasoutput=True, printstatus=False, \\\n             assigncat = {'cstm':['TransactionAmt']}, \\\n             transformdict = transformdict, processdict = processdict)\n\nprint(\"list(train)\")\nlist(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#and then of course use also has the ability to define their own trasnformation functions to\n#incorproate into the platform, I'll defer to the essays for that bit in the interest of brevity","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# postmunge"},{"metadata":{"trusted":true},"cell_type":"code","source":"#And the final bit which I'll just reiterate here is that automunge facilitates the simplest means\n#for consistent processing of subsequently available data with just a single function call\n#all you need is the postprocess_dict object returned form the original automunge call\n\n#This even works when we passed custom trasnformdict entries as was case with last postprocess_dict\n#derived in last example, however if you're defining custom trasfnormation functions for now you\n#need to save those custom function definitions are redefine in the new notewbook when applying postmunge\n\n#Here again is a demosntration of postmunge. Since the last postprocess_dict we returned\n#was with our custom transfomrations in preceding excample, the 'TransactionAmt' column will\n#be processed consistently\n\ntest, testID, testlabels, \\\nlabelsencoding_dict, finalcolumns_test = \\\nam.postmunge(postprocess_dict, tiny_test, testID_column = False, \\\n             labelscolumn = False, pandasoutput=True, printstatus=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Closing thoughts"},{"metadata":{},"cell_type":"markdown","source":"Great well certainly appreciate your attention and opportunity to share. I suppose next step for me is to try and hone in on my entry and perhaps get on the leaderboard. That'd be cool. \n\nOh before I go if you'd like to see more I recently published my first collection of essays titled \"From the Diaries of John Henry\", which a big chunk included the documentation through the development of Automunge. Check it out it's all online.\n\n[turingsquared.com](http://turingsquared.com)\n\nOr for more on Automunge our website and contact info is available at \n\n[automunge.com](https://www.automunge.com)"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}