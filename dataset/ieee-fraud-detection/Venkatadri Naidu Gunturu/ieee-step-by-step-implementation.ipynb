{"cells":[{"metadata":{},"cell_type":"markdown","source":"Here I am explaing small steps even I know that every one knows these in this platform. Very basic steps, but I am feeling beginers can use/check this to learn"},{"metadata":{},"cell_type":"markdown","source":"Few steps to solve,\n\n1. Data analysing\n2. Data cleaning\n3. Feature engineering\n4. Feature filtering by identifying important features\n5. Algorithm selection\n6. Parameter selection/fine tune\n7. Train & predict\n8. Cross Validation\n\nAs a initial step, I am covering few steps(2,5,6,7,8) which are needed to create baseline notebook as complete cycle. Later I will other steps(1,3,4,6-fine tune) one by one as update\n\nIn addition these I am adding few which are needed like libraries, seed, memory, etc.\n\nNext step I will add Feature engineering(3rd step) "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Libraries - We need to import the libraries for the classes, methods which we may using in any notebook - this is general concept like any oops language \n\nimport numpy as np \nimport pandas as pd \nimport os\nimport gc\n\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom lightgbm import LGBMClassifier\n\nfrom sklearn import preprocessing\n\n# To list the directories in ../input, using this we can use the directory/file name(s) to read the train,test and any additional data\nprint(os.listdir('../input'))\n\n# Pandas's dispaly  settings - to view all column data without \"...\" when we are using pandas library to read and dispaly data to anlayse \npd.set_option('display.max_colwidth', -1)\npd.set_option('display.max_rows', 100)  \npd.set_option('display.max_columns', 500)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Seed - to maintian uniqueness and stability of predictions throughout the developement\n# Without seed settings we may getting different results each time when we train and predict\nrandom_state = 1337\nnp.random.seed(random_state)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Read Data using pandas\n# Competietion having 2 types(trasaction and identities) data for train and test. \n# After read both files for each we need to merge the data by using join.\n\ntrain_ts = pd.read_csv(\"../input/ieee-fraud-detection/train_transaction.csv\", index_col='TransactionID')\ntrain_id = pd.read_csv(\"../input/ieee-fraud-detection/train_identity.csv\", index_col='TransactionID')\n\ntest_ts = pd.read_csv(\"../input/ieee-fraud-detection/test_transaction.csv\", index_col='TransactionID')\ntest_id = pd.read_csv(\"../input/ieee-fraud-detection/test_identity.csv\", index_col='TransactionID')\n\nsubmission = pd.read_csv(\"../input/ieee-fraud-detection/sample_submission.csv\")\n\nprint(train_ts.shape,train_id.shape,test_ts.shape,test_id.shape,submission.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# trasactions and identities data mearging using join\n\ntrain = train_ts.merge(train_id, how='left', left_index=True, right_index=True)\ntest = test_ts.merge(test_id, how='left', left_index=True, right_index=True)\nprint(train.shape,test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data having huge size, to run the notebook without intereptions we need to clean unused datasets to release memory\n# Here we created new datasets, so old datasets need to dispose\n\ndel(train_ts, train_id, test_ts, test_id)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Memory reduction function - Memory is high for original data. We need to reduce memory to process features. \ndef reduce_mem_usage(df):\n    start_mem_usg = df.memory_usage().sum() / 1024**2 \n    print(\"Memory usage of properties dataframe is :\",start_mem_usg,\" MB\")\n    NAlist = [] # Keeps track of columns that have missing values filled in. \n    for col in df.columns:\n        if df[col].dtype != object:  # Exclude strings                      \n            # make variables for Int, max and min\n            IsInt = False\n            mx = df[col].max()\n            mn = df[col].min()\n            # Integer does not support NA, therefore, NA needs to be filled\n            if not np.isfinite(df[col]).all(): \n                NAlist.append(col)\n                df[col].fillna(0,inplace=True) \n                   \n            # test if column can be converted to an integer\n            asint = df[col].fillna(0).astype(np.int64)\n            result = (df[col] - asint)\n            result = result.sum()\n            if result > -0.01 and result < 0.01:\n                IsInt = True            \n            # Make Integer/unsigned Integer datatypes\n            if IsInt:\n                if mn >= 0:\n                    if mx < 255:\n                        df[col] = df[col].astype(np.uint8)\n                    elif mx < 65535:\n                        df[col] = df[col].astype(np.uint16)\n                    elif mx < 4294967295:\n                        df[col] = df[col].astype(np.uint32)\n                    else:\n                        df[col] = df[col].astype(np.uint64)\n                else:\n                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n                        df[col] = df[col].astype(np.int8)\n                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n                        df[col] = df[col].astype(np.int16)\n                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n                        df[col] = df[col].astype(np.int32)\n                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n                        df[col] = df[col].astype(np.int64)    \n            # Make float datatypes 32 bit\n            else:\n                df[col] = df[col].astype(np.float32)\n            \n    # Print final result\n    print(\"___MEMORY USAGE AFTER COMPLETION:___\")\n    mem_usg = df.memory_usage().sum() / 1024**2 \n    print(\"Memory usage is: \",mem_usg,\" MB\")\n    print(\"This is \",100*mem_usg/start_mem_usg,\"% of the initial size\")\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Memory reducing for train and test datasets\ntrain = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# In both train and test datasets having few categorical columns. We need to convert any strain data into numericl as best practice\n# and also we can process numerical categorical columns also. \n# Here I listed out the categorical columns(string and numerical) seperately\ncat_cols_str=['ProductCD', 'card4', 'card6','P_emaildomain', 'R_emaildomain', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'id_12', 'id_15', 'id_16', 'id_23', 'id_27', 'id_28', 'id_29', 'id_30', 'id_31', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38', 'DeviceType', 'DeviceInfo']\ncat_cols_int=[ 'card1','card2','card3','card5','addr1', 'addr2']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Processing categorical data function - I will convert categorical columns in to sequesntial numerical data\n# Filled 0 or NA for empty data values\n\ndef process_categorical_columns(temp_ds):\n    for col in cat_cols_str: \n        temp_ds[col] = temp_ds[col].fillna('NA')\n        temp_ds[col]= temp_ds[col].replace(0,'NA')\n        le = preprocessing.LabelEncoder()\n        le.fit(temp_ds[col])\n        temp_ds[col] = le.transform(temp_ds[col]) \n\n    for col in cat_cols_int: \n        temp_ds[col] = temp_ds[col].fillna(0) \n        temp_ds[col]= temp_ds[col].replace('NA',0)\n        le = preprocessing.LabelEncoder()\n        le.fit(temp_ds[col])\n        temp_ds[col] = le.transform(temp_ds[col]) \n    return temp_ds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Processing train and test categorical data\ntrain = process_categorical_columns(train)\ntest = process_categorical_columns(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prepare dataset to train \n# In this problem we need to predict probabillity in between 0 and 1. \n# So, this is clasifier problem and I am using LGBM clasifier wiht Kfold validation. And using predict_proba() method\n# For LGBM we need to provide  Train data(X_data), Train target(y_data) and test data(X_test)\n\nX_data = train.drop('isFraud',axis=1)\ny_data = train['isFraud'].values\nX_test = test\nprint(X_data.shape, y_data.shape, X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Clean unused datasets to release memory\ndel(train, test)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# LGB Parameters - initial - need to fine tune\nlgb_params = {\n    \"objective\" : \"binary\",\n    \"metric\" : \"auc\",\n    \"boosting\": 'gbdt',\n    \"max_depth\" : -1,\n    \"num_leaves\" : 13,\n    \"learning_rate\" : 0.0085,\n    \"bagging_freq\": 5,\n    \"bagging_fraction\" : 0.4,\n    \"feature_fraction\" : 0.05,\n    \"min_data_in_leaf\": 80,\n    \"min_sum_heassian_in_leaf\": 10,\n    \"tree_learner\": \"serial\",\n    \"boost_from_average\": \"false\",\n    \"bagging_seed\" : random_state,\n    \"verbosity\" : 1,\n    \"seed\": random_state\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# LGB training method\ndef train_and_predict(n_splits=5, n_estimators=100):\n    y_pred = np.zeros(X_test.shape[0], dtype='float32')\n    cv_score = 0\n\n    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n\n    for fold, (train_index, val_index) in enumerate(kfold.split(X_data, y_data)):\n        if fold >= n_splits:\n            break\n        \n        # Split the data for train and validation in each fold\n        X_train, X_val = X_data.iloc[train_index, :], X_data.iloc[val_index, :]\n        y_train, y_val = y_data[train_index], y_data[val_index]\n        \n        # Using LGBM classifier algorthm\n        model = LGBMClassifier(**lgb_params, n_estimators=n_estimators, n_jobs = -1)\n        model.fit(X_train, y_train, \n            eval_set=[(X_train, y_train), (X_val, y_val)], eval_metric='auc',\n            verbose=50, early_stopping_rounds=200)\n\n        # in predict_proba we will get 2 values for each prediction, so we need to use 2nd value\n        y_val_pred = model.predict_proba(X_val)[:,1]\n        \n        # Calcualte cross validation score using metrics\n        val_score = roc_auc_score(y_val, y_val_pred)\n        print(f'Fold {fold}, AUC {val_score}')\n        \n        # Averaging cross validation score and test predictions for all folds\n        cv_score += val_score / n_splits\n        y_pred += model.predict_proba(X_test)[:,1] / n_splits\n    \n    # Assign final test predictions to submission dataset\n    submission['isFraud'] = y_pred\n    return cv_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setting folds and start training - as I said I am using KFold, in this we need to set number of folds.\n# Based on the folds count full dataset splited into smaller sets \n# and each set used for training seperately and predict the results. \n# And by averaging the results we can get final results with better accuracy\n# I am using 5000 iterations,verbore as 50 and early stopping rounds as 200 (in above funtion)\n# Verbose - cross validate for each 50 iterations \n# Early stopping rounds - Stop training early if results not improve for last 200 iteration\n\nN_FOLDS = 5\nn_estimators = 5000\ncv_score = train_and_predict(n_splits=N_FOLDS, n_estimators=n_estimators)\nprint(cv_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission['isFraud'].sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create the output file for submission\nsubmission.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# list the files in output folder to download the output file if we run the notebook as interactive session instead of commit\nfrom IPython.display import FileLink, FileLinks\nFileLinks('.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" LB : 0.9121\n \n We can slightly improve by increaseing estimators to 10K or 20K\n "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}