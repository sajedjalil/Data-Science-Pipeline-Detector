{"cells":[{"metadata":{},"cell_type":"markdown","source":"# This test is just a preliminary imitation of LB , CV, PB.\n    Because of the distribution of data, the imitation effect is not good.\n    del some feature corresponding to time like id31 may can do better."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# From kernel https://www.kaggle.com/mpearmain/extended-timeseriessplitter\n\"\"\"\nThis module provides a class to split time-series data for back-testing and evaluation.\nThe aim was to extend the current sklearn implementation and extend it's uses.\n\nMight be useful for some ;)\n\"\"\"\n\nimport logging\nfrom typing import Optional\n\nimport numpy as np\nfrom sklearn.model_selection._split import _BaseKFold\nfrom sklearn.utils import indexable\nfrom sklearn.utils.validation import _num_samples\n\nLOGGER = logging.getLogger(__name__)\n\n\nclass TimeSeriesSplit_(_BaseKFold):  # pylint: disable=abstract-method\n    \"\"\"Time Series cross-validator\n\n    Provides train/test indices to split time series data samples that are observed at fixed time intervals,\n    in train/test sets. In each split, test indices must be higher than before, and thus shuffling in cross validator is\n    inappropriate.\n\n    This cross_validation object is a variation of :class:`TimeSeriesSplit` from the popular scikit-learn package.\n    It extends its base functionality to allow for expanding windows, and rolling windows with configurable train and\n    test sizes and delays between each. i.e. train on weeks 1-8, skip week 9, predict week 10-11.\n\n    In this implementation we specifically force the test size to be equal across all splits.\n\n    Expanding Window:\n\n            Idx / Time  0..............................................n\n            1           |  train  | delay |  test  |                   |\n            2           |       train     | delay  |  test  |          |\n            ...         |                                              |\n            last        |            train            | delay |  test  |\n\n    Rolling Windows:\n            Idx / Time  0..............................................n\n            1           | train   | delay |  test  |                   |\n            2           | step |  train  | delay |  test  |            |\n            ...         |                                              |\n            last        | step | ... | step |  train  | delay |  test  |\n\n    Parameters:\n        n_splits : int, default=5\n            Number of splits. Must be at least 4.\n\n        train_size : int, optional\n            Size for a single training set.\n\n        test_size : int, optional, must be positive\n            Size of a single testing set\n\n        delay : int, default=0, must be positive\n            Number of index shifts to make between train and test sets\n            e.g,\n            delay=0\n                TRAIN: [0 1 2 3] TEST: [4]\n            delay=1\n                TRAIN: [0 1 2 3] TEST: [5]\n            delay=2\n                TRAIN: [0 1 2 3] TEST: [6]\n\n        force_step_size : int, optional\n            Ignore split logic and force the training data to shift by the step size forward for n_splits\n            e.g\n            TRAIN: [ 0  1  2  3] TEST: [4]\n            TRAIN: [ 0  1  2  3  4] TEST: [5]\n            TRAIN: [ 0  1  2  3  4  5] TEST: [6]\n            TRAIN: [ 0  1  2  3  4  5  6] TEST: [7]\n\n    Examples\n    --------\n    >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])\n    >>> y = np.array([1, 2, 3, 4, 5, 6])\n    >>> tscv = TimeSeriesSplit(n_splits=5)\n    >>> print(tscv)  # doctest: +NORMALIZE_WHITESPACE\n    TimeSeriesSplit(train_size=None, n_splits=5)\n    >>> for train_index, test_index in tscv.split(X):\n    ...    print('TRAIN:', train_index, 'TEST:', test_index)\n    ...    X_train, X_test = X[train_index], X[test_index]\n    ...    y_train, y_test = y[train_index], y[test_index]\n    TRAIN: [0] TEST: [1]\n    TRAIN: [0 1] TEST: [2]\n    TRAIN: [0 1 2] TEST: [3]\n    TRAIN: [0 1 2 3] TEST: [4]\n    TRAIN: [0 1 2 3 4] TEST: [5]\n    \"\"\"\n\n    def __init__(self,\n                 n_splits: Optional[int] = 5,\n                 train_size: Optional[int] = None,\n                 test_size: Optional[int] = None,\n                 delay: int = 0,\n                 force_step_size: Optional[int] = None):\n\n        if n_splits and n_splits < 5:\n            raise ValueError(f'Cannot have n_splits less than 5 (n_splits={n_splits})')\n        super().__init__(n_splits, shuffle=False, random_state=None)\n\n        self.train_size = train_size\n\n        if test_size and test_size < 0:\n            raise ValueError(f'Cannot have negative values of test_size (test_size={test_size})')\n        self.test_size = test_size\n\n        if delay < 0:\n            raise ValueError(f'Cannot have negative values of delay (delay={delay})')\n        self.delay = delay\n\n        if force_step_size and force_step_size < 1:\n            raise ValueError(f'Cannot have zero or negative values of force_step_size '\n                             f'(force_step_size={force_step_size}).')\n\n        self.force_step_size = force_step_size\n\n    def split(self, X, y=None, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n\n        Parameters:\n            X : array-like, shape (n_samples, n_features)\n                Training data, where n_samples is the number of samples  and n_features is the number of features.\n\n            y : array-like, shape (n_samples,)\n                Always ignored, exists for compatibility.\n\n            groups : array-like, with shape (n_samples,), optional\n                Always ignored, exists for compatibility.\n\n        Yields:\n            train : ndarray\n                The training set indices for that split.\n\n            test : ndarray\n                The testing set indices for that split.\n        \"\"\"\n        X, y, groups = indexable(X, y, groups)  # pylint: disable=unbalanced-tuple-unpacking\n        n_samples = _num_samples(X)\n        n_splits = self.n_splits\n        n_folds = n_splits + 1\n        delay = self.delay\n\n        if n_folds > n_samples:\n            raise ValueError(f'Cannot have number of folds={n_folds} greater than the number of samples: {n_samples}.')\n\n        indices = np.arange(n_samples)\n        split_size = n_samples // n_folds\n\n        train_size = self.train_size or split_size * self.n_splits\n        test_size = self.test_size or n_samples // n_folds\n        full_test = test_size + delay\n\n        if full_test + n_splits > n_samples:\n            raise ValueError(f'test_size\\\\({test_size}\\\\) + delay\\\\({delay}\\\\) = {test_size + delay} + '\n                             f'n_splits={n_splits} \\n'\n                             f' greater than the number of samples: {n_samples}. Cannot create fold logic.')\n\n        # Generate logic for splits.\n        # Overwrite fold test_starts ranges if force_step_size is specified.\n        if self.force_step_size:\n            step_size = self.force_step_size\n            final_fold_start = n_samples - (train_size + full_test)\n            range_start = (final_fold_start % step_size) + train_size\n\n            test_starts = range(range_start, n_samples, step_size)\n\n        else:\n            if not self.train_size:\n                step_size = split_size\n                range_start = (split_size - full_test) + split_size + (n_samples % n_folds)\n            else:\n                step_size = (n_samples - (train_size + full_test)) // n_folds\n                final_fold_start = n_samples - (train_size + full_test)\n                range_start = (final_fold_start - (step_size * (n_splits - 1))) + train_size\n\n            test_starts = range(range_start, n_samples, step_size)\n\n        # Generate data splits.\n        for test_start in test_starts:\n            idx_start = test_start - train_size if self.train_size is not None else 0\n            # Ensure we always return a test set of the same size\n            if indices[test_start:test_start + full_test].size < full_test:\n                continue\n            yield (indices[idx_start:test_start],\n                   indices[test_start + delay:test_start + full_test])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# From kernel https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\n# WARNING! THIS CAN DAMAGE THE DATA \ndef reduce_mem_usage2(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport gc\n# Any results you write to the current directory are saved as output.\nimport imblearn\nfrom imblearn.under_sampling import RandomUnderSampler,TomekLinks\nimport datetime\nimport lightgbm as lgb\nimport sklearn as skl\nfrom sklearn.model_selection import *\nfrom sklearn.metrics import *\nimport matplotlib.pyplot as plt\nfrom imblearn.over_sampling import *\nfrom sklearn.decomposition import PCA, TruncatedSVD, FastICA\nfrom sklearn.preprocessing import LabelEncoder,MinMaxScaler,StandardScaler\nfrom sklearn.utils import resample\nfrom dateutil import relativedelta","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfolder_path = '../input/ieee-fraud-detection/'\ntrain_id = pd.read_csv(f'{folder_path}train_identity.csv')\ntrain_tr = pd.read_csv(f'{folder_path}train_transaction.csv')\ntest_id = pd.read_csv(f'{folder_path}test_identity.csv')\ntest_tr = pd.read_csv(f'{folder_path}test_transaction.csv')\nsub = pd.read_csv(f'{folder_path}sample_submission.csv')\ntrain = pd.merge(train_tr, train_id, on='TransactionID', how='left')\ntest = pd.merge(test_tr, test_id, on='TransactionID', how='left')\ndel train_id,test_id,train_tr,test_tr\ngc.collect()\ntrain = train.drop('id_31',axis=1)\ntest = test.drop('id_31',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_time(df):\n    START_DATE = '2017-11-30'\n    startdate = datetime.datetime.strptime(START_DATE, '%Y-%m-%d')\n    df['TransactionDT'] = df['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds = x)))\n\n    #df['year'] = df['TransactionDT'].dt.year\n    #df['month'] = df['TransactionDT'].dt.month\n    df['dow'] = df['TransactionDT'].dt.dayofweek\n    df['hour'] = df['TransactionDT'].dt.hour\n    df['day'] = df['TransactionDT'].dt.day\n    #df = df.drop(['TransactionDT','TransactionID'],axis=1)\n    return df\n\ndef split_pseudo(df,gap=15):\n    split_time_head = df['TransactionDT'][0]+relativedelta.relativedelta(days=+180/2-int(gap/2))\n    split_time_tail = df['TransactionDT'][0]+relativedelta.relativedelta(days=+180/2+int(gap/2))\n    train = df[df['TransactionDT']<split_time_head]\n    test = df[df['TransactionDT']>split_time_tail]\n    train = train.drop('TransactionDT',axis=1)\n    test = test.drop('TransactionDT',axis=1)\n    del df \n    gc.collect()\n    return train,test\ndef clean_inf_nan(df):\n    return df.replace([np.inf, -np.inf], np.nan)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train =make_time(train)\ntrain = train.drop('TransactionID',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test =make_time(test)\ntest = test.drop('TransactionID',axis=1)\ntest = test.drop('TransactionDT',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train,pseudo_test =  split_pseudo(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntrain = clean_inf_nan(train)\npseudo_test = clean_inf_nan(pseudo_test)\ntest = clean_inf_nan(test)\ntrain = reduce_mem_usage2(train)\npseudo_test = reduce_mem_usage2(pseudo_test)\ntest = reduce_mem_usage2(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train['isFraud']\ntrain = train.drop(['isFraud'],axis=1)\ny_pseudo = pseudo_test['isFraud']\npseudo_test = pseudo_test.drop(['isFraud'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pseudo_test_LB = pseudo_test.iloc[0:int(0.2*len(pseudo_test)),:]\ny_LB = y_pseudo.iloc[0:int(0.2*len(pseudo_test))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape[1],pseudo_test.shape[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params  = {\n          'num_leaves': 190,\n          'num_boost_round':5000,\n          'min_child_samples': 79,\n          'objective': 'binary',\n          'max_depth': 13,\n          'learning_rate': 0.01,\n          \"boosting_type\": \"gbdt\",\n          \"subsample_freq\": 1,\n          \"subsample\": 0.6,\n          \"metric\": 'auc',\n          #\"verbosity\": -1,\n          'reg_alpha': 0.4,\n          'reg_lambda': 0.4,\n          'colsample_bytree': 0.3,\n          #'categorical_feature': cat_cols\n         }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub['isFraud'] = 0.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_cv = 0.\ntotal_p_cv = 0.\ntotal_lb = 0.\nn_folds=5\nfolds = TimeSeriesSplit_(delay=30,train_size=int(len(train)/2),test_size=int(len(train)/6)-100)\n\nfor fold_n,(train_idx,val_idx) in enumerate(folds.split(train)):\n    tr_X,val_X = train.iloc[train_idx],train.iloc[val_idx]\n    tr_y,val_y = y.iloc[train_idx],y.iloc[val_idx]\n    \n    lgtrain = lgb.Dataset(tr_X,label=tr_y)\n    lgval = lgb.Dataset(val_X,label=val_y)\n    \n    print('fold: %d  len_train:%d len_val:%d '%(fold_n+1,len(train_idx),len(val_idx)))\n    model = lgb.train(params,lgtrain,valid_sets=[lgtrain,lgval],\n                      early_stopping_rounds=70,verbose_eval=200)\n    ##########################\n    print('#'*20)\n    cv_predict = model.predict(val_X)\n    cv_score = roc_auc_score(y_true=val_y,y_score=cv_predict)\n    total_cv +=cv_score\n    \n    print('AUC on cv: %f'%(cv_score))\n    ########################################\n    p_cv_predict = model.predict(pseudo_test)\n    p_cv_score = roc_auc_score(y_true=y_pseudo,y_score=p_cv_predict)\n    total_p_cv+=p_cv_score\n    print('AUC on p_cv: %f'%(p_cv_score))\n    ##########################################\n    \n    LB_pre = model.predict(pseudo_test_LB)\n    LB_score = roc_auc_score(y_true=y_LB,y_score=LB_pre)\n    total_lb += LB_score\n    print('pseudo LB auc score : %f'%(LB_score))\n    print('#'*20)\n    ########################################   \n    pre = model.predict(test)\n    print(pre.shape)\n    sub['isFraud'] += pre\n    ##################################\nprint('*'*20)\nprint('total_cv: %f'%(total_cv/n_folds))\nprint('total_p_cv: %f'%(total_p_cv/n_folds))\nprint('total_LB_auc: %f'%(total_lb/n_folds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub['isFraud'] /=n_folds\nsub.to_csv('sub.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}