{"cells":[{"metadata":{},"cell_type":"markdown","source":"> # Introduction <br>\nHi everyone! In this kernel I'd like to share some ideas about NN's and loss functions. <br>\nCore points of this kernel: \n* Preparing tabular data for NN's\n* Handling skewed continuous features\n* Implementing custom loss function\n* Which models are good for ensembling"},{"metadata":{},"cell_type":"markdown","source":"# Data Loading and Feature Selection <br>\nIn this particular kernel we will use only features from transaction table, 'cause with NN after a brief investigation I didn't get any significant improvement by using all features."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\ntrain = pd.read_csv('../input/train_transaction.csv')\ntest = pd.read_csv('../input/test_transaction.csv')\nsub = pd.read_csv('../input/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"useful_features = list(train.iloc[:, 3:55].columns)\n\ny = train.sort_values('TransactionDT')['isFraud']\nX = train.sort_values('TransactionDT')[useful_features]\nX_test = test[useful_features]\ndel train, test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_features = [\n    'ProductCD',\n    'card1', 'card2', 'card3', 'card4', 'card5', 'card6',\n    'addr1', 'addr2',\n    'P_emaildomain',\n    'R_emaildomain',\n    'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9'\n]\n\ncontinuous_features = list(filter(lambda x: x not in categorical_features, X))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Processing\nFor continuous right-skewed features we wil apply log-transform, so that will make them look more like normal distributed."},{"metadata":{"trusted":true},"cell_type":"code","source":"class ContinuousFeatureConverter:\n    def __init__(self, name, feature, log_transform):\n        self.name = name\n        self.skew = feature.skew()\n        self.log_transform = log_transform\n        \n    def transform(self, feature):\n        if self.skew > 1:\n            feature = self.log_transform(feature)\n        \n        mean = feature.mean()\n        std = feature.std()\n        return (feature - mean)/(std + 1e-6)        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm.autonotebook import tqdm\n\nfeature_converters = {}\ncontinuous_features_processed = []\ncontinuous_features_processed_test = []\n\nfor f in tqdm(continuous_features):\n    feature = X[f]\n    feature_test = X_test[f]\n    log = lambda x: np.log10(x + 1 - min(0, x.min()))\n    converter = ContinuousFeatureConverter(f, feature, log)\n    feature_converters[f] = converter\n    continuous_features_processed.append(converter.transform(feature))\n    continuous_features_processed_test.append(converter.transform(feature_test))\n    \ncontinuous_train = pd.DataFrame({s.name: s for s in continuous_features_processed}).astype(np.float32)\ncontinuous_test = pd.DataFrame({s.name: s for s in continuous_features_processed_test}).astype(np.float32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"continuous_train['isna_sum'] = continuous_train.isna().sum(axis=1)\ncontinuous_test['isna_sum'] = continuous_test.isna().sum(axis=1)\n\ncontinuous_train['isna_sum'] = (continuous_train['isna_sum'] - continuous_train['isna_sum'].mean())/continuous_train['isna_sum'].std()\ncontinuous_test['isna_sum'] = (continuous_test['isna_sum'] - continuous_test['isna_sum'].mean())/continuous_test['isna_sum'].std()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"isna_columns = []\nfor column in tqdm(continuous_features):\n    isna = continuous_train[column].isna()\n    if isna.mean() > 0.:\n        continuous_train[column + '_isna'] = isna.astype(int)\n        continuous_test[column + '_isna'] = continuous_test[column].isna().astype(int)\n        isna_columns.append(column)\n        \ncontinuous_train = continuous_train.fillna(0.)\ncontinuous_test = continuous_test.fillna(0.)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For categorical features we will apply OneHot transformation, but only for most common values for each feature to reduce sparsity. <br>\nAlso there is an embedding approach for categorical features transformation. It was implemented in this kernel https://www.kaggle.com/ryches/keras-nn-starter-w-time-series-split <br>\nWith embedding approach I didn't get any significant improvement comparing to this."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom tqdm.autonotebook import tqdm\n\ndef categorical_encode(df_train, df_test, categorical_features, n_values=50):\n    df_train = df_train[categorical_features].astype(str)\n    df_test = df_test[categorical_features].astype(str)\n    \n    categories = []\n    for column in tqdm(categorical_features):\n        categories.append(list(df_train[column].value_counts().iloc[: n_values - 1].index) + ['Other'])\n        values2use = categories[-1]\n        df_train[column] = df_train[column].apply(lambda x: x if x in values2use else 'Other')\n        df_test[column] = df_test[column].apply(lambda x: x if x in values2use else 'Other')\n        \n    \n    ohe = OneHotEncoder(categories=categories)\n    ohe.fit(pd.concat([df_train, df_test]))\n    df_train = pd.DataFrame(ohe.transform(df_train).toarray()).astype(np.float16)\n    df_test = pd.DataFrame(ohe.transform(df_test).toarray()).astype(np.float16)\n    return df_train, df_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_categorical, test_categorical = categorical_encode(X, X_test, categorical_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = pd.concat([continuous_train, train_categorical], axis=1)\ndel continuous_train, train_categorical\nX_test = pd.concat([continuous_test, test_categorical], axis=1)\ndel continuous_test, test_categorical","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Validation\nFor validation I use time-based holdout. For these and other models it has a good correlation between val and lb."},{"metadata":{"trusted":true},"cell_type":"code","source":"split_ind = int(X.shape[0]*0.8)\n\nX_tr = X.iloc[:split_ind]\nX_val = X.iloc[split_ind:]\n\ny_tr = y.iloc[:split_ind]\ny_val = y.iloc[split_ind:]\n\ndel X","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nimport random\nimport tensorflow as tf\nimport keras.backend as K\n\nfrom keras.models import Model\nfrom keras.layers import Dense, Input, Dropout, BatchNormalization, Activation\nfrom keras.utils.generic_utils import get_custom_objects\nfrom keras.optimizers import Adam, Nadam\nfrom keras.callbacks import Callback\nfrom sklearn.metrics import roc_auc_score\n\nnp.random.seed(42) # NumPy\nrandom.seed(42) # Python\ntf.set_random_seed(42) # Tensorflow","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compatible with tensorflow backend\nclass roc_callback(Callback):\n    def __init__(self,training_data,validation_data):\n        self.x = training_data[0]\n        self.y = training_data[1]\n        self.x_val = validation_data[0]\n        self.y_val = validation_data[1]\n\n\n    def on_train_begin(self, logs={}):\n        return\n\n    def on_train_end(self, logs={}):\n        return\n\n    def on_epoch_begin(self, epoch, logs={}):\n        return\n\n    def on_epoch_end(self, epoch, logs={}):\n        y_pred_val = self.model.predict(self.x_val)\n        roc_val = roc_auc_score(self.y_val, y_pred_val)\n        print('\\rroc-auc_val: %s' % (str(round(roc_val,4))),end=100*' '+'\\n')\n        return\n\n    def on_batch_begin(self, batch, logs={}):\n        return\n\n    def on_batch_end(self, batch, logs={}):\n        return\n    \ndef focal_loss(gamma=2., alpha=.25):\n    def focal_loss_fixed(y_true, y_pred):\n        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n        return -K.mean(alpha * K.pow(1. - pt_1, gamma) * K.log(K.epsilon()+pt_1))-K.mean((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0 + K.epsilon()))\n    return focal_loss_fixed\n\ndef custom_gelu(x):\n    return 0.5 * x * (1 + tf.tanh(tf.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3))))\n\nget_custom_objects().update({'custom_gelu': Activation(custom_gelu)})\nget_custom_objects().update({'focal_loss_fn': focal_loss()})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model(loss_fn):\n    inps = Input(shape=(X_tr.shape[1],))\n    x = Dense(512, activation=custom_gelu)(inps)\n    x = BatchNormalization()(x)\n    x = Dropout(0.3)(x)\n    x = Dense(256, activation=custom_gelu)(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.2)(x)\n    x = Dense(1, activation='sigmoid')(x)\n    model = Model(inputs=inps, outputs=x)\n    model.compile(\n        optimizer=Nadam(),\n        loss=[loss_fn]\n    )\n    #model.summary()\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_focal = create_model('focal_loss_fn')\nmodel_bce = create_model('binary_crossentropy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_bce.fit(\n    X_tr, y_tr, epochs=8, batch_size=2048, validation_data=(X_val, y_val), verbose=True, \n    callbacks=[roc_callback(training_data=(X_val, y_tr), validation_data=(X_val, y_val))]\n)\nmodel_focal.fit(\n    X_tr, y_tr, epochs=8, batch_size=2048, validation_data=(X_val, y_val), verbose=True, \n    callbacks=[roc_callback(training_data=(X_val, y_tr), validation_data=(X_val, y_val))]\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_preds_bce = model_bce.predict(X_val).flatten()\nval_preds_focal = model_focal.predict(X_val).flatten()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import rankdata, spearmanr\n\nprint('BCE preds: ', roc_auc_score(y_val, val_preds_bce))\nprint('Focal preds: ',roc_auc_score(y_val, val_preds_focal))\nprint('Correlation matrix: ')\nprint(np.corrcoef(val_preds_bce, val_preds_focal))\nprint(\"Spearman's correlation: \", spearmanr(val_preds_bce, val_preds_focal).correlation)\nprint('Averaging: ', roc_auc_score(y_val, val_preds_bce + val_preds_focal))\nprint('Rank averaging: ', roc_auc_score(y_val, rankdata(val_preds_bce, method='dense') + rankdata(val_preds_focal, method='dense')))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, we've got a pretty interesting results. The same models with almost the same validation scores give predictions with correlation ~80% by optimizing different loss functions, so that makes them good for ensembling."},{"metadata":{},"cell_type":"markdown","source":"# Fine-tuning and Predicting"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_bce.fit(X_val, y_val, epochs=2, batch_size=2048, verbose=True)\nmodel_focal.fit(X_val, y_val, epochs=2, batch_size=2048, verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.isFraud = rankdata(model_bce.predict(X_test).flatten(), method='dense') + rankdata(model_focal.predict(X_test).flatten(), method='dense')\nsub.isFraud = sub.isFraud/sub.isFraud.max()\nsub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Final thoughts <br>\nObviously as a single model NN doesn't perform as well as Gradient Boosting and to addition NN requires more sophisticated data processing approaches. But I'm sure, that it will be good for stacking as one of the first-level models. <br>\nI also think, that using focal loss with gradient boosting can improve score.<br>\nMaybe, It is a good idea to research different loss functions, 'cause the same model can give uncorrelated predicts by optimizing different loss functions.\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}