{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#standard python packages\nimport os \nimport sys\nimport random\nimport warnings\n\n#data science packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import image\nfrom PIL import Image #we will need this for conversion from images (.png) into numpy arrays\n%matplotlib inline\n\n# import cv2\n\nfrom tqdm import tqdm_notebook, tnrange #progress bar\nfrom itertools import chain # chain('ABC', 'DEF') --> A B C D E F\nfrom skimage.io import imread, imshow, concatenate_images\nfrom skimage.transform import resize\n\n#tensorflow\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.layers import Input, Lambda, concatenate\nfrom tensorflow.keras.layers import Conv2D, Conv2DTranspose\nfrom tensorflow.keras.layers import MaxPooling2D\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n\n# from tensorflow.keras import backend as K\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Unzip files\nHere, we unzip the contetns of test.zip and test.zip from the \"/kaggle/input\" folder into the \"/kaggle/working\" folder.\nAfter unzipping, we print out the contents of the \"/kaggle/working\" folder for inspection","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"___/kaggle/input folder___\")\nfor dirname, _, filenames in os.walk('/kaggle/input/tgs-salt-identification-challenge/'):\n    for filename in filenames:\n        print(filename)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# #unzip contents\n! unzip /kaggle/input/tgs-salt-identification-challenge/train.zip -d /kaggle/working/train\n! unzip /kaggle/input/tgs-salt-identification-challenge/test.zip -d /kaggle/working/test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis (EDA)\n\n> Keep your friends close but your data closer.\n\nResources: https://towardsdatascience.com/exploratory-data-analysis-8fc1cb20fd15\n\nHere, we perform EDA to understand our data better. First, we can display some images to ensure that we have extracted our raw data correctly.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"path_train = '/kaggle/working/train/'\npath_test = '/kaggle/working/test/'\n\n#specifying index of 2 will only retrieve the list of filenames, and not the absolute paths.\ntrain_image_ids = next(os.walk(path_train+\"images\"))[2]\ntrain_mask_ids = next(os.walk(path_train+\"masks\"))[2] \ntest_image_ids = next(os.walk(path_test+\"images\"))[2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now let's inspect the data visually by extracting some random samples\n\ndef visualize_raw_data(n): #n, number of examples to display\n\n    # plt.figure(figsize = (20,10))\n    fig, axs = plt.subplots(2, n)\n    fig.suptitle('Random samples of training images and masks', va = 'baseline' ) #va = 'baseline' prevents intersection with the sub titles.\n\n    for i in range(n):\n        ran_index = np.random.randint(0, len(train_image_ids))\n\n        img = image.imread(path_train + 'images/' + train_image_ids[ran_index] )\n        img_mask = image.imread(path_train + 'masks/' + train_mask_ids[ran_index] )\n        \n        axs[0, i].set_title(\"train_id: {}\".format(train_image_ids[ran_index]),fontsize = 'small', pad=2)\n        axs[0, i].imshow(img)\n        axs[1, i].set_title(\"corresponding mask\",fontsize = 'small', pad=2)\n        axs[1, i].imshow(img_mask)\n\n    plt.tight_layout()\n    plt.show()\n\nvisualize_raw_data(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Pre-processing\n\n1. We resize both the train and the test images\n2. We normalize the data:\n2a. The two most discussed data scaling methods are Normalization and Standardization: Normalization typically means rescales the values into a range of [0,1]. Standardization typically means rescales data to have a mean of 0 and a standard deviation of 1 (unit variance). Here, we choose to normalize the data as standardization is normally used when there are many features of different units we want to put onto the same scale.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"path_train = '/kaggle/working/train/'\npath_test = '/kaggle/working/test/'\n\n# Further preprocessing of data\n# 1. resize images to input size and convert to numpy arrays\n# 2. Normalize the data to a range between 0 and 1\ndef preprocess_data(inp_w, inp_h, inp_c, path_train, path_test):\n\n    # PART 1: Resize train, mask and test images\n    X_train_ = np.zeros((len(train_image_ids), inp_w, inp_h, inp_c), dtype=np.uint8)\n    Y_train_ = np.zeros((len(train_image_ids), inp_w, inp_h, inp_c), dtype=np.bool)\n    X_test_ = np.zeros((len(test_image_ids), inp_w, inp_h, inp_c), dtype=np.uint8)\n\n    sys.stdout.flush() #write everything in the buffer to the terminal, even if normally it would wait before doing so. \n\n    # Iterate through array of train_ids\n    for n, id_ in tqdm_notebook(enumerate(train_image_ids), total = len(train_image_ids)):\n        #load the images\n        #we extract the second channel from the 3 channel image as our numpy array\n        img_array = img_to_array(load_img(path_train + '/images/' + id_ ))[:,:,1]\n        mask_array = img_to_array(load_img(path_train + '/masks/' + id_ ))[:,:,1]\n\n        #resize the arrays. \n        #anti_aliasing: Set anti_aliasing to true, thereby applying a Gaussian filter to smooth the image \n        #prior to down-scaling. It is crucial to filter when down-sampling the image to avoid aliasing artifacts. \n        #preserve_range: Keep the original range of values. \n        X_train_[n] = resize(img_array, (inp_w, inp_h, inp_c), mode='constant', anti_aliasing=True, preserve_range=True) \n        Y_train_[n] = resize(mask_array, (inp_w, inp_h, inp_c), mode='constant', anti_aliasing=True, preserve_range=True)\n\n    sizes_test = []\n    # Iterate through array of test_ids\n    for n, id_ in tqdm_notebook(enumerate(test_image_ids), total = len(test_image_ids)):\n        test_array = img_to_array(load_img(path_test + '/images/' + id_))[:,:,1]\n        \n        sizes_test.append([test_array.shape[0], test_array.shape[1]])\n        X_test_[n] = resize(test_array, (inp_w, inp_h, inp_c), mode='constant', anti_aliasing=True, preserve_range=True)\n    \n    # PART 2A: Normalize the arrays\n#     X_train_ = X_train_/255 \n#     X_test_ = X_test_/255\n    \n    return X_train_, Y_train_, X_test_, sizes_test\n\nX_train_, Y_train_, X_test, sizes_test = preprocess_data(128, 128, 1, '/kaggle/working/train/', '/kaggle/working/test/')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Train Test Split\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_val, Y_train, Y_val = train_test_split(X_train_, Y_train_, test_size=0.1)\ndel X_train_\ndel Y_train_\n\nprint(\"x_train shape: {}, y_train shape: {}\".format(X_train.shape, Y_train.shape))\nprint(\"x_val shape: {}, y_val shape: {}\".format(X_val.shape, Y_val.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The below cell takes up alot of RAM, need to rewrite without the function","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# #n, number of examples to display\n# def visualize_resized_data(n, X_train_, Y_train_): \n\n#     # plt.figure(figsize = (20,10))\n#     fig, axs = plt.subplots(2, n)\n#     fig.suptitle('Random samples of training images and masks', va = 'baseline' ) #va = 'baseline' prevents intersection with the sub titles.\n\n#     for i in range(n):\n\n#         ran_index = np.random.randint(0, X_train_.shape[0])\n        \n#         X = X_train_[ran_index]\n#         # Because Y_train datatype is bool, we need to convert it to float32\n#         # np.squeeze removes single-dimensional entries from the shape of an array.\n#         Y = Y_train_[ran_index].astype(np.float32) \n\n#         axs[0, i].set_title(\"train_id: {}\".format(train_image_ids[ran_index]),fontsize = 'small', pad=2)\n#         axs[0, i].imshow(np.dstack((X, X, X)) )\n        \n#         axs[1, i].set_title(\"corresponding mask\",fontsize = 'small', pad=2)\n#         axs[1, i].imshow(np.dstack((Y, Y, Y)) )\n\n#     plt.tight_layout()\n#     plt.show()\n    \n# visualize_resized_data(3, X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Image augmentation\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\ndatagen = ImageDataGenerator(featurewise_center = False, \n                             samplewise_center = False,\n                             featurewise_std_normalization = False,\n                             samplewise_std_normalization = False,\n                             zca_whitening = False,\n#                              rotation_range = 45, #perhaps to change this is 90?\n#                              zoom_range = 0.1,\n#                              width_shift_range = 0.15,\n#                              height_shift_range = 0.15,\n                             horizontal_flip = True,\n                             vertical_flip = True)\ndatagen.fit(X_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Training","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### From tensorflow documentation\n> Mean IoU is a common evaluation metric for semantic image segmentation, which first computes the IOU for each semantic class and then computes the average over classes. IOU is defined as follows:\nIOU =  true_positive / (true_positive+ false_positive + false_negative)\nThe predictions are accumulated in a confusion matrix, weighted by sample_weight and the metric is then calculated from it.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# #Encoder\n# def EncoderUnit(filters, kernel_size, activation, padding, input_layer):\n#     x1 = Conv2D(filters=filters, kernel_size=kernel_size, activation=activation, padding=padding)(input_layer)\n#     x2 = Conv2D(filters=filters, kernel_size=kernel_size, activation=activation, padding=padding)(x1)\n#     p1 = MaxPooling2D((2,2))(x2)\n#     return p1, x2\n\n# def ConvUnit(filters, kernel_size, activation, padding, input_layer):\n#     x1 = Conv2D(filters=filters, kernel_size=kernel_size, activation=activation, padding=padding)(input_layer)\n#     x2 = Conv2D(filters=filters, kernel_size=kernel_size, activation=activation, padding=padding)(x1)\n#     return x2\n\n# def Decoder(filters, kernel_size, strides, padding, input_layer, skip_layer):\n#     u1 = Conv2DTranspose(filters=filters, kernel_size=kernel_size, strides=strides, padding=padding)(input_layer)\n#     concat = concatenate([u1, skip_layer]) #shortcut\n#     return concat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #Build U-Net Model \n# inputs = Input(shape=(128, 128, 1))\n# # s = Lambda(lambda x: x/255) (inputs)\n\n# Encoder1, Conv1 = EncoderUnit(8, (3,3), 'relu', 'same', inputs)\n# Encoder2, Conv2 = EncoderUnit(16, (3,3), 'relu', 'same', Encoder1)\n# Encoder3, Conv3 = EncoderUnit(32, (3,3), 'relu', 'same', Encoder2)\n# Encoder4, Conv4 = EncoderUnit(64, (3,3), 'relu', 'same', Encoder3)\n\n# Conv5 = ConvUnit(8, (3,3), 'relu', 'same', Encoder4)\n\n# Decoder6 = Decoder(64, (2,2), (2,2), 'same', input_layer=Conv5, skip_layer=Conv4)\n# Conv6 = ConvUnit(8, (3,3), 'relu', 'same', Decoder6)\n\n# Decoder7 = Decoder(64, (2,2), (2,2), 'same', input_layer=Conv6, skip_layer=Conv3)\n# Conv7 = ConvUnit(8, (3,3), 'relu', 'same', Decoder7)\n\n# Decoder8 = Decoder(64, (2,2), (2,2), 'same', input_layer=Conv7, skip_layer=Conv2)\n# Conv8 = ConvUnit(8, (3,3), 'relu', 'same', Decoder8)\n\n# Decoder9 = Decoder(64, (2,2), (2,2), 'same', input_layer=Conv8, skip_layer=Conv1)\n# Conv9 = ConvUnit(8, (3,3), 'relu', 'same', Decoder9)\n\n# outputs=Conv2D(1, (1,1), activation='sigmoid')(Conv9)\n\n# model = Model(inputs=[inputs], outputs=[outputs])\n# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[tf.keras.metrics.MeanIoU(num_classes=2)])\n# # model.summary()\n\n# # tf.keras.utils.plot_model(model, show_shapes=True, show_layer_names=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inputs = Input((128, 128, 1))\ns = Lambda(lambda x: x / 255) (inputs)\n\nc1 = Conv2D(8, (3, 3), activation='relu', padding='same') (s)\nc1 = Conv2D(8, (3, 3), activation='relu', padding='same') (c1)\np1 = MaxPooling2D((2, 2)) (c1)\n\nc2 = Conv2D(16, (3, 3), activation='relu', padding='same') (p1)\nc2 = Conv2D(16, (3, 3), activation='relu', padding='same') (c2)\np2 = MaxPooling2D((2, 2)) (c2)\n\nc3 = Conv2D(32, (3, 3), activation='relu', padding='same') (p2)\nc3 = Conv2D(32, (3, 3), activation='relu', padding='same') (c3)\np3 = MaxPooling2D((2, 2)) (c3)\n\nc4 = Conv2D(64, (3, 3), activation='relu', padding='same') (p3)\nc4 = Conv2D(64, (3, 3), activation='relu', padding='same') (c4)\np4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n\nc5 = Conv2D(128, (3, 3), activation='relu', padding='same') (p4)\nc5 = Conv2D(128, (3, 3), activation='relu', padding='same') (c5)\n\nu6 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (c5)\nu6 = concatenate([u6, c4]) #shortcut\nc6 = Conv2D(64, (3, 3), activation='relu', padding='same') (u6)\nc6 = Conv2D(64, (3, 3), activation='relu', padding='same') (c6)\n\nu7 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (c6)\nu7 = concatenate([u7, c3]) #shortcut\nc7 = Conv2D(32, (3, 3), activation='relu', padding='same') (u7)\nc7 = Conv2D(32, (3, 3), activation='relu', padding='same') (c7)\n\nu8 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same') (c7)\nu8 = concatenate([u8, c2]) #shortcut\nc8 = Conv2D(16, (3, 3), activation='relu', padding='same') (u8)\nc8 = Conv2D(16, (3, 3), activation='relu', padding='same') (c8)\n\nu9 = Conv2DTranspose(8, (2, 2), strides=(2, 2), padding='same') (c8)\nu9 = concatenate([u9, c1], axis=3)\nc9 = Conv2D(8, (3, 3), activation='relu', padding='same') (u9)\nc9 = Conv2D(8, (3, 3), activation='relu', padding='same') (c9)\n\noutputs = Conv2D(1, (1, 1), activation='sigmoid') (c9)\n\nmodel = Model(inputs=[inputs], outputs=[outputs])\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=[tf.keras.metrics.MeanIoU(num_classes=2)])\n# model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#define callbacks\n#earlystopper stops the the training after the metrics stop improving for a number of epochs(defined by patience).\n#checkpointer saves the model after each epoch\n\nearlystopper= EarlyStopping(patience=5, verbose=1)\n\ncheckpointer = ModelCheckpoint('/kaggle/working/model-tgs-salt-1.h5', monitor='val_loss', verbose=1, save_best_only=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# hyperparameters\nbatch_size = 8\nepochs = 15\n\nhistory = model.fit(datagen.flow(X_train, Y_train, batch_size = batch_size),\n                   epochs=epochs,\n                   validation_data = (X_val, Y_val),\n                    verbose = 1,\n#                     steps_per_epoch=steps_per_epoch,\n                    callbacks=[earlystopper, checkpointer])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Predict on train, val and test set\nmodel = load_model('/kaggle/working/model-tgs-salt-1.h5', custom_objects={'mean_iou': tf.keras.metrics.MeanIoU(num_classes=2)})\npreds_train = model.predict(X_train, verbose=1)\npreds_val = model.predict(X_val, verbose=1)\npreds_test = model.predict(X_test, verbose=1)\n\n#threshold predictions\npreds_train_t = (preds_train > 0.5).astype(np.uint8)\npreds_val_t = (preds_val > 0.5).astype(np.uint8)\npreds_test_t = (preds_test > 0.5).astype(np.uint8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create list of upsampled test masks\npreds_test_upsampled = []\nfor i in tnrange(len(preds_test)): # tnrange is a shortcut for tqdm_notebook(xrange(*args), **kwargs).\n    preds_test_upsampled.append(resize(\n                                    np.squeeze(preds_test[i]), \n                                    (sizes_test[i][0], sizes_test[i][0]), \n                                    mode='constant', \n                                    preserve_range=True))\npreds_test_upsampled[0].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Perform a sanity check on some random training samples\nix = random.randint(0, len(preds_train_t))\n# plt.imshow(np.dstack((X_train[ix],X_train[ix],X_train[ix])))\n# plt.show()\n# tmp = np.squeeze(Y_train[ix]).astype(np.float32)\n# plt.imshow(np.dstack((tmp,tmp,tmp)))\n# plt.show()\n\n\ntmp = X_test[ix]\nplt.imshow(np.dstack((tmp,tmp,tmp)))\nplt.show()\ntmp = np.squeeze(preds_train_t[ix]).astype(np.float32)\nplt.imshow(np.dstack((tmp,tmp,tmp)))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def RLenc(img, order='F', format=True):\n    \"\"\"\n    img is binary mask image, shape (r,c)\n    order is down-then-right, i.e. Fortran\n    format determines if the order needs to be preformatted (according to submission rules) or not\n\n    returns run length as an array or string (if format is True)\n    \"\"\"\n    bytes = img.reshape(img.shape[0] * img.shape[1], order=order)\n    runs = []  ## list of run lengths\n    r = 0  ## the current run length\n    pos = 1  ## count starts from 1 per WK\n    for c in bytes:\n        if (c == 0):\n            if r != 0:\n                runs.append((pos, r))\n                pos += r\n                r = 0\n            pos += 1\n        else:\n            r += 1\n\n    # if last run is unsaved (i.e. data ends with 1)\n    if r != 0:\n        runs.append((pos, r))\n        pos += r\n        r = 0\n\n    if format:\n        z = ''\n\n        for rr in runs:\n            z += '{} {} '.format(rr[0], rr[1])\n        return z[:-1]\n    else:\n        return runs\n    \npred_dict = {fn[:-4]:RLenc(np.round(preds_test_upsampled[i])) for i,fn in tqdm_notebook(enumerate(test_image_ids))}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.DataFrame.from_dict(pred_dict,orient='index')\nsub.index.names = ['id']\nsub.columns = ['rle_mask']\nsub.to_csv('submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}