{"cells":[{"metadata":{"_uuid":"c1cef6a380dc56b81baea085638967ce9aa5e50a"},"cell_type":"markdown","source":"\n# Testing image equalization options\n\nTry out different ways to do image equalization. See scikit-image's Histogram Equalization documentatin at http://scikit-image.org/docs/dev/auto_examples/color_exposure/plot_equalize.html\n<BR>\nTesting out:\n- Contrast Stretching\n- Equalization\n- Adaptive Equalization\n- Local Equalization\n\n### TBDR - None of these options improved results\n\n### This was origianlly based on \"U-net, dropout, augmentation, stratification\" by Peter HÃ¶nigschmid \nhttps://www.kaggle.com/phoenigs/u-net-dropout-augmentation-stratification\n<br>\nRather than resizing the images from 101x101 to 128x128, this kernel adjusts the padding on U-net \n"},{"metadata":{"_uuid":"9547b0e07fc0dc3f75e974cc6d07a01ddf34fa02"},"cell_type":"markdown","source":"## Issues and suggestions\nI am relatively new to python and these techniques, so suggestions or error corrections are welcome. Some issues I am still aware of:\n<br>\n- Some of the equalization methods generate this error, \"Possible precision loss when converting from float64 to uint16\" but I don't think this affects the results.\n- There should be some easy way to apply the equalizaiton functions to the training/testing arrays, but I was lazy and just re-ran the train_test_split function each time.\n- I kept the network simple with no dropout or batch normalization layers. While these would improve the results, I don't think they will change the relative performance of the methods.\n- Are there some other image adjustement techniques I should be testing?"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":false},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom random import randint\n\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-white')\nimport seaborn as sns\nsns.set_style(\"white\")\n\nfrom sklearn.model_selection import train_test_split\n\nfrom skimage.transform import resize\nfrom skimage import exposure\nfrom skimage.filters import rank\nfrom skimage.morphology import disk\n\nfrom keras.preprocessing.image import load_img\nfrom keras import Model\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, Callback\nfrom keras.models import load_model\nfrom keras.optimizers import Adam, RMSprop\nfrom keras.utils.vis_utils import plot_model\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, Dropout, AveragePooling2D\n\nfrom tqdm import tqdm_notebook","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"962c2c6775b5fcf605df8e7c59cbcabe6ba9ceaa"},"cell_type":"markdown","source":"# Params and helpers"},{"metadata":{"_uuid":"e54e151245d665e42bb95d9cf2e1a33cb9440e48","trusted":false},"cell_type":"code","source":"img_size_target = 101\n\n# removed upsample and downsample code, since resizing is not used","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"24d7f3d982bfa582b222f012129acdda55282b6d"},"cell_type":"markdown","source":"# Read images and masks\nLoad the images and masks into the DataFrame and divide the pixel values by 255."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/train.csv\", index_col=\"id\", usecols=[0])\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b18c1f50cefd7504eae7e7b9605be3814c7cad6d","trusted":false},"cell_type":"code","source":"train_df[\"images\"] = [np.array(load_img(\"../input/train/images/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(train_df.index)]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"86620c6a070571895f4f36ec050a25803915ed74","trusted":false},"cell_type":"code","source":"train_df[\"masks\"] = [np.array(load_img(\"../input/train/masks/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(train_df.index)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"4301bb4f52aa9a4b4ee3e49b0d768d234d5dd1a5"},"cell_type":"code","source":"# Simple split of images into training and testing sets\nids_train, ids_valid, x_train, x_valid, y_train, y_valid = train_test_split(\n    train_df.index.values,\n    np.array(train_df.images.tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    np.array(train_df.masks.tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"63ac58ab47921b4e4f54102e2c8b85fa318225f1"},"cell_type":"markdown","source":"# Build model"},{"metadata":{"trusted":false,"_uuid":"aebfbf19f66365a0d87e2721b286a7b4e635f6e1"},"cell_type":"code","source":"# Nikhil Tomar -- Unet with layer concatenation in downblock\n# https://www.kaggle.com/nikhilroxtomar/unet-with-layer-concatenation-in-downblock\n\ndef inception(input_layer, base_name, num_filters=16):\n    # Inception module\n    incep_1x1 = Conv2D(num_filters, (1,1), padding='same', activation='relu', name=base_name + 'incep_1x1')(input_layer)\n    incep_3x3_reduce = Conv2D(int(num_filters*1.5), (1,1), padding='same', activation='relu', name=base_name + 'incep_3x3_reduce')(input_layer)\n    incep_3x3 = Conv2D(num_filters, (3,3), padding='same', activation='relu', name=base_name + 'incep_3x3')(incep_3x3_reduce)\n    incep_5x5_reduce = Conv2D(int(num_filters/4), (1,1), padding='same', activation='relu', name=base_name + 'incep_5x5_reduce')(input_layer)\n    incep_5x5 = Conv2D(int(num_filters/2), (5,5), padding='same', activation='relu', name=base_name + 'incep_5x5')(incep_5x5_reduce)\n    incep_pool = AveragePooling2D(pool_size=(3,3), strides=(1,1), padding='same', name=base_name + 'incep_pool')(input_layer)\n    incep_pool_proj = Conv2D(int(num_filters/2), (1,1), padding='same', activation='relu', name=base_name + 'incep_pool_proj')(incep_pool)\n    incep_output = concatenate([incep_1x1, incep_3x3, incep_5x5, incep_pool_proj], axis = 3, name=base_name + 'incep_output')\n    return incep_output\n\ndef down_block(input_layer, base_name,  num_filters=16, padding='same', dropout=0.25):\n    conv1 = Conv2D(num_filters, (3, 3), activation=\"relu\", padding=padding, name=base_name + 'conv1')(input_layer)\n    conv2 = Conv2D(num_filters, (3, 3), activation=\"relu\", padding=padding, name=base_name + 'conv2')(conv1)\n    pool = MaxPooling2D((2, 2), name=base_name + 'pool')(conv2)\n    output = Dropout(dropout)(pool)\n    return output, conv2\n\ndef down_inception(input_layer, base_name, num_filters=16, padding='same', dropout=0.25):\n    #First Inception module\n    incep_1_output = inception(input_layer, base_name+\"incep1\", num_filters)\n    #Second Inception module\n    incep_2_output = inception(incep_1_output, base_name+\"incep2\", num_filters)\n    pool_incep = MaxPooling2D((2, 2), name=base_name + 'pool')(incep_2_output)\n    output_layer = Dropout(dropout, name=base_name + 'drop')(pool_incep)\n    return output_layer, incep_2_output\n\n\ndef down_block_resnet(x, num_filters=16, kernel_size=(3, 3), padding='same', activation='relu', pool_size=(2, 2), dropout=0.25):\n    conv = resnet_block(x, num_filters=num_filters, kernel_size=kernel_size, padding=padding,\n    activation=activation)\n    pool = conv\n    if pool_size != None:\n        #pool = MaxPooling2D(pool_size) (conv)\n        pool = Conv2D(num_filters, kernel_size, padding='same', strides=pool_size, activation='tanh') (conv)\n        #pool = Conv2D(num_filters, kernel_size, padding='same', activation='tanh') (pool)\n    if dropout != None:\n        pool = Dropout(dropout) (pool)\n    return pool, conv\n\ndef up_block(uconv_input, conv_input, base_name, num_filters=16, padding='same', dropout=0.25):\n    deconv = Conv2DTranspose(num_filters, (3, 3), strides=(2, 2), padding=padding, name=base_name + 'deconv')(uconv_input)\n    uconv1 = concatenate([deconv, conv_input], name=base_name + 'concate')\n    uconv2 = Dropout(dropout, name=base_name + 'drop')(uconv1)\n    uconv3 = Conv2D(num_filters, (3, 3), padding=\"same\", name=base_name + 'conv1')(uconv2)\n    uconv4 = Conv2D(num_filters, (3, 3), padding=\"same\", name=base_name + 'conv2')(uconv3)\n    return uconv4\n\ndef resnet_block(x, num_filters=16, kernel_size=(3, 3), strides=(2, 2), padding='same', activation='relu'):\n    conv1 = Conv2D(num_filters, (7, 7), padding=padding) (x)\n    conv1 = _activation(activation, conv1)\n\n    conv2 = Conv2D(num_filters, (5, 5), padding=padding) (x)\n    conv2 = _activation(activation, conv2)\n\n    conv3 = Conv2D(num_filters, (3, 3), padding=padding) (x)\n    conv3 = _activation(activation, conv3)\n\n    return concatenate([conv1, conv2, conv3, x])\n\ndef Unet_standard(num_filters=16,):\n    input_img = Input((img_size_target, img_size_target, 1), name='img')\n    input_features = Input((1, ), name='feat')\n\n    pool1, conv1 = down_block(input_img, \"down1\", num_filters * 1, dropout=0.3)\n    pool2, conv2 = down_block(pool1, \"down2\", num_filters * 2, dropout=0.5)\n    pool3, conv3 = down_block(pool2, \"down3\", num_filters * 4, dropout=0.5)\n    pool4, conv4 = down_block(pool3, \"down4\", num_filters * 8, dropout=0.5)\n\n    # Middle\n    middle1 = Conv2D(num_filters * 16, (3, 3), activation=\"relu\", padding=\"same\")(pool4)\n    middle2 = Conv2D(num_filters * 16, (3, 3), activation=\"relu\", padding=\"same\")(middle1)\n\n    deconv4 = up_block(middle2, conv4, \"up4\", num_filters * 8, dropout=0.5)\n    deconv3 = up_block(deconv4, conv3, \"up3\", num_filters * 4, padding='valid', dropout=0.5)\n    deconv2 = up_block(deconv3, conv2, \"up2\", num_filters * 2, dropout=0.5)\n    deconv1 = up_block(deconv2, conv1, \"up1\", num_filters * 1, padding='valid', dropout=0.5)\n    deconv1 = Dropout(0.5) (deconv1)\n    output_layer = Conv2D(1, (1, 1), padding='same', activation='sigmoid') (deconv1)\n\n    return Model(inputs=[input_img], outputs=[output_layer])\n\ndef Unet_incept(num_filters=16,):\n    input_img = Input((img_size_target, img_size_target, 1), name='img')\n    input_features = Input((1, ), name='feat')\n\n    pool1, conv1 = down_inception(input_img, \"down1\", num_filters * 1, dropout=0.3)\n    pool2, conv2 = down_inception(pool1, \"down2\", num_filters * 2, dropout=0.5)\n    pool3, conv3 = down_inception(pool2, \"down3\", num_filters * 4, dropout=0.5)\n    pool4, conv4 = down_inception(pool3, \"down4\", num_filters * 8, dropout=0.5)\n\n    # Middle\n    middle1 = Conv2D(num_filters * 16, (3, 3), activation=\"relu\", padding=\"same\")(pool4)\n    middle2 = Conv2D(num_filters * 16, (3, 3), activation=\"relu\", padding=\"same\")(middle1)\n\n    deconv4 = up_block(middle2, conv4, \"up4\", num_filters * 8, dropout=0.5)\n    deconv3 = up_block(deconv4, conv3, \"up3\", num_filters * 4, padding='valid', dropout=0.5)\n    deconv2 = up_block(deconv3, conv2, \"up2\", num_filters * 2, dropout=0.5)\n    deconv1 = up_block(deconv2, conv1, \"up1\", num_filters * 1, padding='valid', dropout=0.5)\n    deconv1 = Dropout(0.5) (deconv1)\n    output_layer = Conv2D(1, (1, 1), padding='same', activation='sigmoid') (deconv1)\n\n    return Model(inputs=[input_img], outputs=[output_layer])\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"0a5883e1c34cd5ce6932d09062edc63f610c3944"},"cell_type":"code","source":"model = Unet_standard()\n#model = Unet_incept()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3399029adb039b049e3d6ca01fef30ed8653482b","trusted":false},"cell_type":"code","source":"optimizer_Adam = Adam(lr=0.0001)\nmodel.compile(loss=\"binary_crossentropy\", optimizer=optimizer_Adam, metrics=['accuracy'])\nmodel.summary()\nmodel.save_weights('imageWeights.h5')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c7ded4adc1757c88a1bea59ea36b1a9f7941bd28","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"3bc2bb6abb19dbee4b769e1f63ae277409621046"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f5a6b1abaa4681cba3b608bc5f33cf260370d82a"},"cell_type":"markdown","source":"# Training with basic images"},{"metadata":{"_uuid":"f1773642758da7b4480e0e48c045bd01ea3684ae","scrolled":true,"trusted":false},"cell_type":"code","source":"reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1, min_lr=0.000001)\nepochs = 5          # REDUCE FROM 100 FOR KAGGLE RUNTIME LIMIT\nbatch_size = 16     # REDUCED TO 16 FOR MEMORY USE\n\nhistory = model.fit(x_train, y_train,\n                    validation_data=[x_valid, y_valid], \n                    epochs=epochs,\n                    batch_size=batch_size,\n                    verbose=1,\n                    callbacks=[reduce_lr])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"61cef294cf0f7dbb91896648a8190e2de85981be"},"cell_type":"code","source":"def plot_training_results(history):\n    fig, (ax_loss, ax_acc) = plt.subplots(1, 2, figsize=(15,5))\n    ax_loss.plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\n    ax_loss.plot(history.epoch, history.history[\"val_loss\"], label=\"Validation loss\")\n    ax_acc.plot(history.epoch, history.history[\"acc\"], label=\"Train accuracy\")\n    ax_acc.plot(history.epoch, history.history[\"val_acc\"], label=\"Validation accuracy\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"42e9ef3c4e0a2bb2539e5e51740ba6bfc092d37c","trusted":false},"cell_type":"code","source":"plot_training_results(history)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ab2d162ccbff0bcd654364be4610b4b20d8f8f7d"},"cell_type":"markdown","source":"# Try out differen versions of image equalization\n\nFor details see http://scikit-image.org/docs/dev/auto_examples/color_exposure/plot_equalize.html "},{"metadata":{"trusted":false,"_uuid":"3d7cc96b87fc5183e4c7c78d253adb83bf03b643"},"cell_type":"code","source":"\n\ndef contrast_stretch(img):\n    # Contrast stretching\n    p2, p98 = np.percentile(img, (2, 98))\n    if (p2==p98):\n        return img      # some images are just one color, so they gerenate an divide by zero error, so return original image\n    img_contrast_stretch = exposure.rescale_intensity(img, in_range=(p2, p98))\n    return img_contrast_stretch\n\ndef equalization(img):\n    # some images are just one color, so they gerenate a divide by zero error\n    #     so return original image if the min and max values are the same\n    if (np.max(img) == np.min(img) ):\n        return img      \n    # Equalization\n    img_equalized = exposure.equalize_hist(img)\n    return img_equalized\n\ndef adaptive_equalization(img):\n    # some images are just one color, so they gerenate a divide by zero error\n    #     so return original image if the min and max values are the same\n    if (np.max(img) == np.min(img) ):\n        return img      \n    # Adaptive Equalization\n    img_adaptive_equalized = exposure.equalize_adapthist(img, clip_limit=0.03)\n    return img_adaptive_equalized\n\ndef local_equalization(img):\n    # some images are just one color, so they gerenate a divide by zero error\n    #     so return original image if the min and max values are the same\n    if (np.max(img) == np.min(img) ):\n        return img      \n    # Local Equalization--for details see http://scikit-image.org/docs/dev/auto_examples/color_exposure/plot_local_equalize.html\n    selem = disk(30)\n    img_local_equal = rank.equalize(img, selem=selem)\n    return img_local_equal\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bc2e040971fa691661b5f813c16fd7bee81b7c4d"},"cell_type":"markdown","source":"# Display some sample images"},{"metadata":{"trusted":false,"_uuid":"efddd129e883c58617bdeff8b6e75bc0c307487b"},"cell_type":"code","source":"def display_equalizations(img):\n    fix, axs = plt.subplots(1, 5, figsize=(15,5))\n    axs[0].imshow(img, cmap=\"Greys\")\n    axs[0].set_title(\"Original image\")\n\n    axs[1].imshow(contrast_stretch(img), cmap=\"Greys\")\n    axs[1].set_title(\"Contrast stretching\")\n\n    axs[2].imshow(equalization(img), cmap=\"Greys\")\n    axs[2].set_title(\"Equalized image\")\n\n    axs[3].imshow(adaptive_equalization(img), cmap=\"Greys\")\n    axs[3].set_title(\"Adaptive Equalization image\")\n\n    axs[4].imshow(local_equalization(img), cmap=\"Greys\")\n    axs[4].set_title(\"Local Equalization image\")\n\nimg = train_df.images.loc[ids_train[11]]\ndisplay_equalizations(img)\nimg = train_df.images.loc[ids_train[14]]\ndisplay_equalizations(img)\nimg = train_df.images.loc[ids_train[27]]\ndisplay_equalizations(img)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b7e786c76849e62dfe387f3c23be435bb1debe59"},"cell_type":"markdown","source":"# Contrast stretching"},{"metadata":{"trusted":false,"_uuid":"03294fadcf6cb9238de4087ce250c46654d4b27a"},"cell_type":"code","source":"# Redo the train/test split with the contrast_stretch added in\n# --- TODO: there should be a better way to do this directly on train/test arrays\nids_train, ids_valid, x_train, x_valid, y_train, y_valid = train_test_split(\n    train_df.index.values,\n    np.array(train_df.images.map(contrast_stretch).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    np.array(train_df.masks.tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    test_size=0.2, random_state=77777)\n\n# Run the training \nmodel.load_weights('imageWeights.h5')       # reload the initial, untrained wieghts to keep each trial the same\noptimizer_Adam = Adam(lr=0.0001)            # reset the learning rate back to the starting value \nmodel.compile(loss=\"binary_crossentropy\", optimizer=optimizer_Adam, metrics=['accuracy'])\nhistory_contrast_stretch = model.fit(x_train, y_train,\n                    validation_data=[x_valid, y_valid], \n                    epochs=epochs,\n                    batch_size=batch_size,\n                    verbose=1,\n                    callbacks=[reduce_lr])\n\nplot_training_results(history_contrast_stretch)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3e7a9845651fc270115b05511f0a8f950e8a52ee"},"cell_type":"markdown","source":"# Equalization"},{"metadata":{"trusted":false,"_uuid":"180a7dc2f029dd57a39f00da1295ac7a002ad461"},"cell_type":"code","source":"# Redo the train/test split with the contrast_stretch added in\n# --- TODO: there should be a better way to do this directly on train/test arrays\nids_train, ids_valid, x_train, x_valid, y_train, y_valid = train_test_split(\n    train_df.index.values,\n    np.array(train_df.images.map(equalization).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    np.array(train_df.masks.tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    test_size=0.2, random_state=77777)\n\n# Run the training \nmodel.load_weights('imageWeights.h5')       # reload the initial, untrained wieghts to keep each trial the same\noptimizer_Adam = Adam(lr=0.0001)            # reset the learning rate back to the starting value \nmodel.compile(loss=\"binary_crossentropy\", optimizer=optimizer_Adam, metrics=['accuracy'])\nhistory_equalization = model.fit(x_train, y_train,\n                    validation_data=[x_valid, y_valid], \n                    epochs=epochs,\n                    batch_size=batch_size,\n                    verbose=1,\n                    callbacks=[reduce_lr])\n\nplot_training_results(history_equalization)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b1547f9bbab8c888020f3d8fd6c1fdca54936586"},"cell_type":"markdown","source":"#  adaptive_equalized"},{"metadata":{"trusted":false,"_uuid":"05c997a171f9194749d3e0cc9d9f366d57b5b0e7"},"cell_type":"code","source":"# Redo the train/test split with the contrast_stretch added in\n# --- TODO: there should be a better way to do this directly on train/test arrays\nids_train, ids_valid, x_train, x_valid, y_train, y_valid = train_test_split(\n    train_df.index.values,\n    np.array(train_df.images.map(adaptive_equalization).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    np.array(train_df.masks.tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    test_size=0.2, random_state=77777)\n\n# Run the training \nmodel.load_weights('imageWeights.h5')       # reload the initial, untrained wieghts to keep each trial the same\noptimizer_Adam = Adam(lr=0.0001)            # reset the learning rate back to the starting value \nmodel.compile(loss=\"binary_crossentropy\", optimizer=optimizer_Adam, metrics=['accuracy'])\nhistory_adaptive_equalization = model.fit(x_train, y_train,\n                    validation_data=[x_valid, y_valid], \n                    epochs=epochs,\n                    batch_size=batch_size,\n                    verbose=1,\n                    callbacks=[reduce_lr])\n\nplot_training_results(history_adaptive_equalization)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8336177aa44cb3e6247dff87a3f8a3e8a1ccdefc"},"cell_type":"markdown","source":"# Local Equalization"},{"metadata":{"trusted":false,"_uuid":"8b53347d81e913fbde8689fc7261460f12367353"},"cell_type":"code","source":"# Redo the train/test split with the contrast_stretch added in\n# --- TODO: there should be a better way to do this directly on train/test arrays\nids_train, ids_valid, x_train, x_valid, y_train, y_valid = train_test_split(\n    train_df.index.values,\n    np.array(train_df.images.map(local_equalization).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    np.array(train_df.masks.tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    test_size=0.2, random_state=77777)\n\n# Run the training \nmodel.load_weights('imageWeights.h5')       # reload the initial, untrained wieghts to keep each trial the same\noptimizer_Adam = Adam(lr=0.0001)            # reset the learning rate back to the starting value \nmodel.compile(loss=\"binary_crossentropy\", optimizer=optimizer_Adam, metrics=['accuracy'])\n\nhistory_local_equalization = model.fit(x_train, y_train,\n                    validation_data=[x_valid, y_valid], \n                    epochs=epochs,\n                    batch_size=batch_size,\n                    verbose=1,\n                    callbacks=[reduce_lr])\n\nplot_training_results(history_local_equalization)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"119590fef4fd601776cfb28d39c4eb16cf8a04f8"},"cell_type":"markdown","source":"# Results Graph\nStock images perform better than any equalization technique."},{"metadata":{"trusted":false,"_uuid":"1affc58086ce2c20595ca122386c33affb337539"},"cell_type":"code","source":"\n# Plot the loss and accuracy curves for training and validation \nfig, ax = plt.subplots(2,1, figsize=(15,15))\n#ax[0].title = \"Loss\"\nax[0].plot(history.history['loss'], color='b', label=\"Stock image\")\nax[0].plot(history_contrast_stretch.history['loss'], color='g', label=\"contrast_stretch \")\nax[0].plot(history_equalization.history['loss'], color='y', label=\"equalization \")\nax[0].plot(history_adaptive_equalization.history['loss'], color='r', label=\"adaptive_equalization \")\nax[0].plot(history_local_equalization.history['loss'], color='c', label=\"local_equalization\")\n\nax[0].plot(history.history['val_loss'], color='b', linestyle=':')\nax[0].plot(history_contrast_stretch.history['val_loss'], color='g', linestyle=':')\nax[0].plot(history_equalization.history['val_loss'], color='y', linestyle=':')\nax[0].plot(history_adaptive_equalization.history['val_loss'], color='r', linestyle=':')\nax[0].plot(history_local_equalization.history['val_loss'], color='c', linestyle=':')\nlinestyle=':'\nlegend = ax[0].legend(loc='best', shadow=True)\n#plt.ylim(0,1)\n\n#ax[0].title = \"Accuracy\"\nax[1].plot(history.history['acc'], color='b', label=\"Stock Image\")\nax[1].plot(history_contrast_stretch.history['acc'], color='g', label=\"contrast_stretch\")\nax[1].plot(history_equalization.history['acc'], color='y', label=\"equalization\")\nax[1].plot(history_adaptive_equalization.history['acc'], color='r', label=\"adaptive_equalization\")\nax[1].plot(history_local_equalization.history['acc'], color='c', label=\"local_equalization\")\n\nax[1].plot(history.history['val_acc'], color='b', linestyle=':')\nax[1].plot(history_contrast_stretch.history['val_acc'], color='g', linestyle=':')\nax[1].plot(history_equalization.history['val_acc'], color='y', linestyle=':')\nax[1].plot(history_adaptive_equalization.history['val_acc'], color='r', linestyle=':')\nax[1].plot(history_local_equalization.history['val_acc'], color='c', linestyle=':')\nlegend = ax[1].legend(loc='best', shadow=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b9b1aa36a6e12da6826a155479e66ace6dd5a345"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b40f21e0c9de1f8f8b221f33fef559eb1d8220d5"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}